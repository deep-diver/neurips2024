[{"type": "text", "text": "Stable Minima Cannot Overfit in Univariate ReLU Networks: Generalization by Large Step Sizes ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Dan Qiao Kaiqi Zhang Esha Singh CSE, UC San Diego CS, UC Santa Barbara CSE, UC San Diego d2qiao@ucsd.edu kzhang70@ucsb.edu e3singh@ucsd.edu ", "page_idx": 0}, {"type": "text", "text": "Daniel Soudry Yu-Xiang Wang Technion - Israel Institute of Technology Halicioglu Data Science Institute, UC San Diego daniel.soudry@gmail.com yuxiangw@ucsd.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study the generalization of two-layer ReLU neural networks in a univariate nonparametric regression problem with noisy labels. This is a problem where kernels (e.g. NTK) are provably sub-optimal and benign overfitting does not happen, thus disqualifying existing theory for interpolating (O-loss, global optimal) solutions. We present a new theory of generalization for local minima that gradient descent with a constant learning rate can stably converge to. We show that gradient descent with a fixed learning rate $\\eta$ can only find local minima that represent smooth functions with a certain weighted first order total variation bounded by ", "page_idx": 0}, {"type": "text", "text": "$1/\\eta-1/2+\\widetilde O(\\sigma+\\sqrt{\\mathrm{MSE}})$ where $\\sigma$ is the label noise level, MSE is short for mean squared error against the ground truth, and $\\widetilde O(\\cdot)$ hides a logarithmic factor. Under mild assumptions, we also prove a nearly-optimal MSE bound of $\\widetilde{O}(n^{-4/5})$ within the strict interior of the support of the $n$ data points. Our theoretical results are validated by extensive simulation that demonstrates large learning rate training induces sparse linear spline fits. To the best of our knowledge, we are the first to obtain generalization bound via minima stability in the non-interpolation case and the first to show ReLU NNs without regularization can achieve near-optimal rates in nonparametric regression. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "How do gradient descent-trained neural networks work? It is an intriguing question that depends on model architecture, data distribution, and optimization algorithms used for training [Zhang et al. 2021]. Specifically, in the overparameterized regime with specific random initialization of the weights, it was shown that gradient descent finds global optimal (O-loss or interpolating) solutions despite the non-convex objective function [Jacot et al., 2018, Du et al., 2018, Liu et al., 2022]. It was also shown that among the (many) global optimal solutions, the particular solutions that are selected by gradient descent often do not overfit despite having O training error [Chizat et al., 2019, Arora et al., 2019, Mei et al., 2019], sometimes even if the data is noisy \u2014- a phenomenon known as \u201c\"benign overfitting' [e.g., Belkin et al., 2019, Bartlett et al., 2020, Frei et al., 2022]. ", "page_idx": 0}, {"type": "image", "img_path": "7Swrtm9Qsp/tmp/955c88281c68d16ecdd26e519c7e2a86e6df6214fcdd567900ed8826c24303ac.jpg", "img_caption": ["Figure 1: We show that \"Large step size selects simple functions that generalize.\" "], "img_footnote": [], "page_idx": 0}, {"type": "image", "img_path": "7Swrtm9Qsp/tmp/590df71ec27965ed9ceaa12d8393891b15b4b3e9f8d37733d6c6d750f48c31fd.jpg", "img_caption": ["Figure 2: Empirical evidence of our claim. Constant step size gradient descent-trained two-layer ReLU neural networks generalize because of minima stability. The left panel shows that with increasing step size, gradient descent finds smoother solutions (linear splines) with a smaller number of knots. The middle panel illustrates our theoretical result with a numerically accurate upper bound using $1/\\eta+O(1)$ of the curvature and TV1-complexity of the smooth solution. The right panel showsthat tuning $\\eta$ gives the classical U-shape bias-variance tradeoff for overparameterized NN. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "What is less well-known is that interpolating solutions do overfit for ReLU neural networks (ranging from tempered to catastrophic) [Mallinar et al., 2022, Joshi et al., 2023, Haas et al., 2023] and the generalization bounds in the kernel regime are provably suboptimal for certain univariate nonparametric regression problems [Suzuki, 2018, Zhang and Wang, 2022]. These \u201cexceptions\" significantly limit the abilities of the kernel theory or \u201cbenign overfitting\u201d theory in predicting the performance of an overparameterized neural network in practice. ", "page_idx": 1}, {"type": "text", "text": "In many learning problems with noisy data, the best solutions are simply not among those that interpolate the data. For example, no interpolating solutions can be consistent in a fixed-design nonparametric regression problem. Even if interpolating solutions that satisfy benign overfitting can be found, they could be undesirable due to their \u201cspikiness\" [Haas et al., 2023] and lack of robustness [Hao and Zhang, 2024]. In addition, it was reported that when the label is noisy, it takes much longer for gradient descent to overfit [Zhang et al., 2021]. Most practical NN training would have entered the Edge-of-Stability regime [Cohen et al., 2020] or stopped before the interpolation regime kicks in. ", "page_idx": 1}, {"type": "text", "text": "These observations motivate us to come up with an alternative theory for gradient-descent training of overparameterized neural networks that do not require interpolation. ", "page_idx": 1}, {"type": "text", "text": "1.1  Summary of Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this paper, we present a new theory of generalization for solutions that gradient descent (GD) with a fixed learning rate can stably converge to. Specifically: ", "page_idx": 1}, {"type": "text", "text": "1. We show that for 1D nonparametric regression ( $n$ data points with noisy labels), the solutions that GD can stably converge to must be regular functions with small (weighted) first-order total variation (Theorem 4.1 and Corollary 4.2), thus promoting sparsity in the number of linear pieces. This generalizes the result of Mulayoff et al. [2021] by removing the \u201cinterpolation\u201d assumption. We also show that in the noisy case, there is no \u201cflat' interpolating solution and gradient descent cannot converge to them unless the learning rate is $O(1/n^{2})$ no matter how overparameterized the two-layer ReLU network is (Theorem 3.1).   \n2. We show that such solutions (stable local minima that GD converges to) cannot overfit, in the sense that the generalization gap vanishes as $n\\to\\infty$ inside the strict interior of the data support (Theorem 4.3). Moreover, under a mild additional assumption on gradient descent finding solutions with training loss smaller than $\\sigma^{2}$ , we prove that these solutions achieve near-optimal rate for estimating (the strict interior of) first-order bounded variation functions (Theorem 4.4) provably faster than any kernel ridge regression estimators, including neural networks in the \"kernel regime.   \n3. We conduct extensive numerical experiments to demonstrate our theoretical predictions, validate ", "page_idx": 1}, {"type": "text", "text": "our technical assumptions, and illustrate the functional form of the ReLU NNs as well as the learned basis functions that gradient descent finds with different step sizes. These results reveal new insights into how gradient descent training aggressively learns representation and induces implicit sparsity. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "To the best of our knowledge, these results are new to this paper. We emphasize that (1) the training objective function is not explicitly regularized; (2) we do not early-stop training in favor of algorithmic-stability; and (3) the solutions that gradient descent converges to are not global optimal (interpolating) solutions unless the label noise is O. Our approach is new in that we directly analyze the complexity of a superset of solutions that gradient descent can stably converge to, which enables us to prove end-to-end generalization bounds that are near-optimal in nonparametric regression tasks. ", "page_idx": 2}, {"type": "text", "text": "Our analysis for gradient descent-training is categorically different from those in the \u201ckernel\" (a.k.a. \"lazy\") regime about interpolating solutions. Instead, we rigorously prove (and empirically demonstrate) that large step-size gradient descent do not behave this way and it does not converge to interpolating solutions. Our results fall into the non-kernel regime of neural network learning known as the \u201crich\" (a.k.a. the \u201cfeature learning\" or \u201csparse\") regime [Chizat et al., 2019, Woodworth et al., 2020], in which the weights and biases can move arbitrarily far away from their initialization. ", "page_idx": 2}, {"type": "text", "text": "Technical novelty. The main technical innovation in our analysis is in handling the $\\textstyle{\\frac{1}{n}}\\sum_{i}(y_{i}\\,-$ $f_{\\theta}(x_{i}))\\nabla_{\\theta}^{2}f_{\\theta}(x_{i})$ term that arises in the minima stability analysis when it was previously handled by Mulayoff et al. [2021] using interpolation, i.e., $y_{i}=\\dot{f}(x_{i})\\;\\dot{\\forall}\\;i\\in[n]$ . It turns out in the noisy-label case, we can decompose the term into a certain Gaussian complexity measure and a self-bounding style MSE of that $f_{\\theta}$ . A non-trivial step is to bound the largest eigenvalue of $\\nabla^{2}f_{\\theta}(\\cdot)$ by a constant which results in a uniform bound of both the Gaussian complexity and the MSE. These bounds themselves are vacuous in terms of the implied generalization error, but plugging them into the function-space constraint imposed by the noisy minima stability bound restricts the learned ReLU NN to be inside a weighted TV1 function class (Details in Theorem 4.1). This, in turn, allows us to amplify a vacuous MSE bound into a new MSE bound that is nearly optimal in the strict interior of the data support. The main technique in the last step involves bounding the metric entropy of the weighted TV1 class and then carefully working out a self-bounding (square loss) version of the Dudley's chaining argument [Wainwright, 2019]. ", "page_idx": 2}, {"type": "text", "text": "Disclaimers and limitations. It is important to note that while we analyze gradient descent training of overparameterized neural networks, the computational claim is very different from those in the kernel regime. The analysis in the kernel regime ensures that gradient descent finds interpolating solutions efficiently. We do not have a comparable efficiency claim. While computational guarantees on stationary point (and local minima) convergence in non-convex optimization problems are wellunderstood [Ghadimi and Lan, 2013, Jin et al., 2017], we do not have guarantees on whether the solution that gradient descent finds satisfies our assumption on the training loss being \u201coptimized\" (smaller than label noise $\\sigma^{2}$ ). Instead, our results provide a generalization gap bound for any stable solutions (Theorem 4.3) and a near-optimal excess risk (MSE vs ground truth) bound (Theorem 4.4) when the solution that GD finds happens to satisfy the assumption (empirically it does!). This is a meaningful middle ground between classical learning theory which does not concern optimization at all and modern theory that is fully optimization-dependent. ", "page_idx": 2}, {"type": "text", "text": "While we focus on (full batch) gradient descent for a clean presentation, the minima stability for stochastic gradient descent is immediate under the stochastic definition of minima stability [Mulayoff et al., 2021]. The same reason applies to us focusing on univariate nonparametric regression. Our technique can be used to generalize the multivariate function space interpretation of minima stability from Nacson et al. [2022] to the noisy case, but it will take substantial effort to formalize the corresponding generalization bounds in the multivariate case, which we leave as a future work. ", "page_idx": 2}, {"type": "text", "text": "1.2 Related Work and Implications of Our Results ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Generalization in Overparameterized NNs, Interpolation, and Benign Overfitting. Most existing theoretical work on understanding the generalization of overparameterized neural networks focuses on the interpolation regime [Ca0 and Gu, 2019, Frei et al., 2022, Kou et al., 2023, Buzaglo et al., 2024]. Handling label noise either requires explicit regularization [Hu et al., 2020, Zhang and Wang, 2022], algorithmic stability through early stopping [Hardt et al., 2016, Richards and Kuzborskij, 2021], or carefully crafted data distribution that leads to a phenomenon known as benign overfitting [Bartlett et al., 2020, Frei et al., 2022, Kou et al., 2023]. Benign overfitting could happen for nonparametric regression tasks [Belkin et al., 2019], but there is well-documented empirical and theoretical evidence that benign overfitting does not occur for regression tasks with ReLU activation [Mallinar et al., 2022, Haas et al., 2023, Joshi et al., 2023] and that the excess risk is required to be proportional to the standard deviation of the label noise [Kornowski et al., 2024]. We are the first to go beyond the interpolation regime and show that gradient descent-trained neural networks generalize in noisy regression tasks without explicit regularization. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Implicit bias of gradient descent. The implicit bias of gradient descent training of overparameterized NN is well-studied. It was shown that among the many globally optimal (interpolating) solutions, gradient descent finds the ones with the smallest norm in certain Hilbert spaces [Arora et al., 2019, Mei et al., 2019], classifiers with largest-margin [Chizat and Bach, 2020], or the smoothest cubic spline interpolation [Jin and Montufar, 2023]. None of these results, however, imply generalization bounds when the labels are noisy. Interestingly, our results show that gradient descent with a large step size induces an implicit bias that resembles sparse L1-regularization rather than the dense L2 regularization from gradient fow [Jin and Montufar, 2023]. ", "page_idx": 3}, {"type": "text", "text": "Implicit bias of minima stability. The closest to our work is the line of work on the implicit bias of minima stability [Ma and Ying, 2021, Mulayoff et al., 2021, Wu and Su, 2023]. We build directly on top of the function-space interpretation of minima stability established by Mulayoff et al. [2021]. However, these works critically rely on the minima interpolating the data, which makes their results inapplicable to settings with label noise. Mulayoff et al. [2021] also did not establish formal generalization bounds. Ma and Ying [2021], Wu and Su [2023] do have generalization bounds, but (again) their results require interpolation and thus do not apply to our settings. ", "page_idx": 3}, {"type": "text", "text": "Flat/Sharp Minima and generalization. Our work is also connected to the body of work on the hypothesis that \u201cfat local minima generalize better'\". Despite compelling empirical evidence [Hochreiter and Schmidhuber, 1997, Keskar et al., 2017], rigorous theoretical understanding of this hypothesis is still lacking [see, e.g., Wu and Su, 2023, and the references therein]. Our work contributes to this literature by formally proving that the hypothesis is real for two-layer ReLU NNs in a noisy regression task. ", "page_idx": 3}, {"type": "text", "text": "Edge-of-Stability and Catapults. Empirical observations on how large learning rate training of NN finds solutions with Hessian's largest eigenvalue dancing around $2/\\eta$ i.e., \u201cedge of stability\u201d regime [Cohen et al., 2020]; and that the loss may go up first before going down to a good solution (\"catapult\") [Lewkowycz et al., 2020]. Existing theoretical understanding of these curious behaviors of GD training is stillimited to toy-scale settings (e.g., Arora et al. [2022], Ahn et al. [2023], Kreisler et al. [2023]). Our work is complementary in that we provide generalization bound to the final solution GD stabilizes on no matter how GD gets there. Outside the context of GD and neural networks, \u201cedge of stability\u201d and the implicit bias of large step-size were observed for forward stagewise regression [see, e.g. Tibshirani, 2015, 2014, Section 4.4 and Page 42] albeit only empirically. Our results may provide a theoretical handle in formally analyzing these observations. ", "page_idx": 3}, {"type": "text", "text": "Optimal rates of NNs in nonparametric regression. Finally, it was previously known that neural networks can achieve optimal rates for estimating TV1 functions [Suzuki, 2018, Liu et al., 2021, Parhi and Nowak, 2021, Zhang and Wang, 2022]. Specifically, Savarese et al. [2019], Ongie et al. [2020], Parhi and Nowak [2021], Zhang and Wang [2022] show that weight decay in ReLU networks is connected to total variation regularization. However, these works assume one can solve an appropriately constrained or regularized empirical risk minimization problem. Our work is the first to show that the optimal rate is achievable with gradient descent without weight decay. In fact, it was a pleasant surprise to us that both weight decay and large learning rate induce total variation-like implicit regularization in the function space. ", "page_idx": 3}, {"type": "text", "text": "2  Notations and Problem Setup ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Let us set up the problem formally. Throughout the paper, we use $O(\\cdot),\\Omega(\\cdot)$ to absorb constants while $\\widetilde O(\\cdot)$ suppresses logarithmic factors. Meanwhile, $[n]=\\{1,2,\\cdots\\,,n\\}$ ", "page_idx": 3}, {"type": "text", "text": "Two-layer neural network. We consider two-layer (i.e. one-hidden-layer) univariate ReLU networks, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{F}=\\left\\{f:\\mathbb{R}\\rightarrow\\mathbb{R}\\ \\bigg|\\ f(x)=\\sum_{i=1}^{k}w_{i}^{(2)}\\phi\\left(w_{i}^{(1)}x+b_{i}^{(1)}\\right)+b^{(2)}\\right\\},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the network consists of $k$ hidden neurons and $\\phi(\\cdot)$ denotes the ReLU activation function. ", "page_idx": 4}, {"type": "text", "text": "Training data and loss function. The training dataset is denoted by $\\mathcal{D}=\\left\\{(x_{i},y_{i})\\in\\mathbb{R}\\times\\mathbb{R},i\\in[n]\\right\\}$ $\\{x_{i}\\}_{i=1}^{n}$ is assumed to be supported by $[-x_{\\mathrm{max}},x_{\\mathrm{max}}]$ for some constant $x_{\\operatorname*{max}}>0$ . We focus on regression problems with square loss $\\ell(f,(x,y))={\\textstyle\\frac{1}{2}}(f(x)-y)^{2}$ . The training loss is defined as $\\begin{array}{r}{\\mathcal{L}(f)\\,=\\,\\frac{1}{2n}\\sum_{i=1}^{n}\\left(f(x_{i})-y_{i}\\right)^{2}}\\end{array}$ $f$ $\\theta:=\\big[w_{1:k}^{(1)},b_{1:k}^{(1)},w_{1:k}^{(2)},b^{(2)}\\big]\\in$ $\\mathbb{R}^{3k+1}$ As a short hand, we define $\\ell_{i}(\\theta):=\\ell(f_{\\theta},(x_{i},y_{i}))$ and $\\begin{array}{r}{\\mathcal{L}(\\theta):=\\frac{1}{n}\\sum_{i\\in[n]}\\ell_{i}(\\theta)}\\end{array}$ ", "page_idx": 4}, {"type": "text", "text": "Gradient descent. We focus on the Gradient descent (GD) learner, which iteratively updates $\\theta$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\theta_{t+1}=\\theta_{t}-\\eta\\nabla\\mathcal{L}(\\theta_{t}),\\;t\\geq0,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\eta>0$ is the step size (a.k.a. learning rate) and $\\theta_{0}$ is the initial parameter. Detailed calculation of gradient for two-layer ReLU networks is deferred to Appendix E. Below we define stability for local minima and discuss the conditions for a minimum to be stable. ", "page_idx": 4}, {"type": "text", "text": "Twice differentiable stable local minima. Similar to Mulayoff et al. [2021], we consider twice differentiable minima. According to Taylor's expansion around a twice differentiable minimum $\\theta^{\\star}$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\theta)\\approx\\mathcal{L}(\\theta^{\\star})+(\\theta-\\theta^{\\star})^{T}\\nabla\\mathcal{L}(\\theta^{\\star})+\\frac{1}{2}(\\theta-\\theta^{\\star})^{T}\\nabla^{2}\\mathcal{L}(\\theta^{\\star})(\\theta-\\theta^{\\star}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\nabla^{2}{\\mathcal{L}}$ denotes the Hessian matrix and $\\nabla{\\mathcal{L}}(\\theta^{\\star})\\,=\\,0$ . Therefore, as $\\theta_{t}$ gets close to $\\theta^{\\star}$ , the update rule for GD (2) can be approximated as $\\theta_{t+1}\\approx\\theta_{t}-\\eta\\left(\\nabla\\mathcal{L}(\\theta^{\\star})+\\nabla^{2}\\mathcal{L}(\\theta^{\\star})(\\theta_{t}-\\theta^{\\star})\\right)$ . Such approximation motivates the definition of linear stability, which is first stated in Wu et al. [2018]. ", "page_idx": 4}, {"type": "text", "text": "Definition 2.1 (Linear stability). With the update rule $\\theta_{t+1}=\\theta_{t}-\\eta\\left(\\nabla\\mathcal{L}(\\theta^{\\star})+\\nabla^{2}\\mathcal{L}(\\theta^{\\star})(\\theta_{t}-\\theta^{\\star})\\right)$ a twice differentiable local minimum $\\theta^{\\star}$ of $\\mathcal{L}$ is said to be $\\epsilon$ linearly stable if for any $\\theta_{0}$ in the $\\epsilon$ -ball $B_{\\epsilon}(\\theta^{\\star})$ , it holds that lim $\\operatorname*{sup}_{t\\to\\infty}\\|\\theta_{t}-\\theta^{\\star}\\|\\leq\\epsilon$ ", "page_idx": 4}, {"type": "text", "text": "Note that different from previous works [Wu et al., 2018, Mulayoff et al., 2021], we remove the expectationbefore $\\lVert\\boldsymbol{\\theta}_{t}-\\boldsymbol{\\theta}^{\\star}\\rVert$ since under GD everything is deterministic. Intuitively speaking, linear stability requires that once we have arrived at a distance of $\\epsilon$ from $\\theta^{\\star}$ , we end up staying in the $\\epsilon$ -ball $B_{\\epsilon}(\\theta^{\\star})$ . It is known that linear stability is connected to the flatness of the local minima. ", "page_idx": 4}, {"type": "text", "text": "Lemma 2.2. Consider the update rule in Definition 2.1, for any $\\epsilon>0$ alocalminimum $\\theta^{\\star}$ is an $\\epsilon$ linearlystable minimum of C if and only if >max(2C(0)2 ", "page_idx": 4}, {"type": "text", "text": "The implication is that the set of stable minima is equivalent to the set of fat local minima whose largest eigenvalue of Hessian is smaller than $2/\\eta$ . The proof is adapted from Mulayoff et al. [2021] and we state the proof in Appendix $\\mathbf{C}$ for completeness. When the result does not depend on $\\epsilon$ (as above), we simply say \u201clinearly stable\". Throughout the paper, we overload the notation by calling a function $f=f_{\\theta}$ linearly stable function if $\\theta$ is linearly stable. ", "page_idx": 4}, {"type": "text", "text": "\"Edge of Stability\u201d regime. Extensive empirical and theoretical evidence (Cohen et al. [2020], Damian et al. [2024], and see Section 1.2) have shown that the threshold of linear stability (from Lemma 2.2) is quite significant in GD dynamics: GD iterations initially tend to exhibit \u201cprogressive sharpening\", where $\\bar{\\lambda_{\\operatorname*{max}}}(\\nabla^{2}\\mathcal{L}(\\theta_{t}))$ is increasing, until finally GD reaches the \u201cEdge of Stability\u201d, where $\\bar{\\lambda_{\\operatorname*{max}}}\\bar{(\\nabla^{2}\\mathcal{L}(\\theta_{t}))}\\approx\\dot{2}/\\eta$ We capture this phenomenon with the following definition. ", "page_idx": 4}, {"type": "text", "text": "Definition 2.3 (Below Edge of Stability). We say that a sequence of parameters $\\{\\theta_{t}\\}_{t=1,2,\\cdots}$ generated   \nby gradient descent with step-size $\\eta$ is $\\epsilon$ -approximately Below-Edge-of-Stability (BEoS) for $\\epsilon>0$ if $t^{*}>0$ $\\begin{array}{r}{\\lambda_{\\mathrm{max}}\\big(\\nabla^{2}\\mathcal{L}(\\bar{\\theta_{t}})\\big)\\leq\\frac{2e^{\\epsilon}}{\\eta}}\\end{array}$ foral $t\\geq t^{*}$ Aniy $\\theta_{t}$ with $t\\geq t^{*}$ isrfteredto $\\epsilon$ ", "page_idx": 4}, {"type": "text", "text": "The BEoS regime provides a strong justification for the connection between the step size $\\eta$ and the largest eigenvalue of the Hessian matrix. It holds for all twice-differentiable solutions GD finds along the way \u2014\u2014- even if the GD does not converge to a (local or global) minimum. Empirically, BEoS is valid for both the \u201cprogressive sharpening\u201d phase and the oscillating EoS phase for a small constant $\\epsilon$ ", "page_idx": 4}, {"type": "text", "text": "Our goal in this paper is to understand generalization for both (twice differentiable) stable local minima (Definition 2.1) and any other solutions satisfying $\\epsilon$ -BEoS (Definition 2.3), which are both subsetsof ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{F}(\\eta,\\epsilon,\\mathcal{D}):=\\left\\{f_{\\theta}\\,\\,\\bigg|\\,\\,\\lambda_{\\mathrm{max}}(\\nabla^{2}\\mathcal{L}(\\theta))\\leq\\frac{2e^{\\epsilon}}{\\eta}\\right\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "To simplify the presentation, we focus on the case with $\\epsilon=0$ w.1.o.g.1 and unless otherwise specified, a \u201cstable solution\" refers to an element of $\\mathcal{F}(\\eta,0,\\mathcal{D})$ in the remainder of the paper. ", "page_idx": 5}, {"type": "text", "text": "For the data generation process, we will consider two settings of interest: (1) the fixed design nonparametric regression setting (with noisy labels) (2) the agnostic statistical learning setting. They have different data assumptions and performance metrics to capture \u201cgeneralization\". ", "page_idx": 5}, {"type": "text", "text": "Nonparametric Regression with Noisy labels. In this setting, we assume fixed input $x_{1},\\cdot\\cdot\\cdot\\,,x_{n}$ and $y_{i}=f_{0}(x_{i})+\\epsilon_{i}$ for $i\\in[n]$ where $f_{0}:\\mathbb{R}\\rightarrow\\mathbb{R}$ is the ground-truth target) function and $\\{\\epsilon_{i}\\}_{i=1}^{n}$ are independent Gaussian noises ${\\mathcal{N}}(0,\\sigma^{2})$ . Our goal is find a ReLU NN $f$ using the dataset to minimize the mean squared error (MSE): ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{MSE}(f)=\\frac{1}{n}\\sum_{i=1}^{n}\\left(f(x_{i})-f_{0}(x_{i})\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "It is nonparametric because we do not require $f_{0}$ to be described by a smaller number of parameters, but rather satisfy certain regularity conditions. Specifically, we focus on estimating target functions inside the first order bounded variation class ", "page_idx": 5}, {"type": "equation", "text": "$$\nf_{0}\\in\\mathbf{B}\\mathbf{V}^{(1)}(B,C_{n}):=\\left\\{f:\\left[-x_{\\operatorname*{max}},x_{\\operatorname*{max}}\\right]\\to\\mathbb{R}\\;\\bigg|\\operatorname*{max}_{x}|f(x)|\\leq B,\\int_{-x_{\\operatorname*{max}}}^{x_{\\operatorname*{max}}}|f^{\\prime\\prime}(x)|d x\\leq C_{n}\\right\\},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $f^{\\prime\\prime}$ denotes the second-order weak derivative of $f$ and we define a short hand $\\mathrm{TV}^{(1)}(f):=$ $\\int_{-x_{\\mathrm{max}}}^{x_{\\mathrm{max}}}|f^{\\prime\\prime}(x)|d x$ which we refer to as the $\\mathrm{TV}^{(1)}$ (semi)norm of $f$ throughout the paper. We refer readers to a recent paper [Hu et al., 2022, Section 1.2] for the historical importance and the challenges in estimating the BV functions. The complexity of such function class is discussed in Appendix D. ", "page_idx": 5}, {"type": "text", "text": "Agnostic statistical learning and generalization gap. In this setting, we assume the $n$ data points $\\{(\\bar{x}_{i},y_{i})\\}_{i=1}^{n}$ are drawn i.i.d. from an unknown distribution $\\mathcal{P}$ defined on $[-x_{\\mathrm{max}},x_{\\mathrm{max}}]\\times[-D,D]$ The expected performance on new data points is called \u201cRisk\", $R(f)=\\mathbb{E}_{(x,y)\\sim\\mathcal{P}}[\\ell\\left(f,(x,y)\\right)]$ .We define the absolute difference between training loss and the risk: ", "page_idx": 5}, {"type": "equation", "text": "$$\n{\\mathrm{Gen}}(f):={\\mathrm{GeneralizationGap}}(f)=|R(f)-{\\mathcal{L}}(f)|.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We say that $f$ generalizes if Generalization $\\mathrm{Gap}(f)\\to0$ as $n\\to\\infty$ with high probability. ", "page_idx": 5}, {"type": "text", "text": "3 Stable Solutions Cannot Interpolate Noisy Labels ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "A large portion of previous works studying minima stability assume the learned function interpolates the data. However, for various optimization problems, it is unclear whether there exists such an interpolating solution that is stable, especially when the number of samples $n$ becomes large. ", "page_idx": 5}, {"type": "text", "text": "For the nonparametric regression problem with noisy labels, we design an example where any interpolating function can not be stable. Before presenting the example, we first define the $g$ function, which will be the weight function of the weighted $\\bar{\\mathrm{TV}}^{(1)}$ norm throughout the paper:for $x\\in[-x_{\\mathrm{max}},x_{\\mathrm{max}}],g(x)=\\mathrm{min}\\{g^{-}(x),g^{+}(x)\\}$ With ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g^{-}(x)=\\mathbb{P}^{2}(X<x)\\mathbb{E}[x-X|X<x]\\sqrt{1+(\\mathbb{E}[X|X<x])^{2}},}\\\\ &{g^{+}(x)=\\mathbb{P}^{2}(X>x)\\mathbb{E}[X-x|X>x]\\sqrt{1+(\\mathbb{E}[X|X>x])^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $X$ is drawn from the empirical distribution of the data (a sample chosen uniformly from $\\{x_{j}\\}$ ", "page_idx": 5}, {"type": "text", "text": "For various distributions of training data (e.g. Gaussian distribution, uniform distribution), most of $g$ 's mass is located at the center while $g$ decays towards the extreme data points. The same $g(x)$ is also applied as the weight function in Mulayoff et al. [2021], where they derived an upper bound $\\begin{array}{r}{\\int_{-x_{\\mathrm{max}}}^{x_{\\mathrm{max}}}|f^{\\prime\\prime}(x)|g(x)d x\\leq\\frac{1}{\\eta}-\\frac{1}{2}}\\end{array}$ asstming $f$ lating. We generalize the same upper bound to all stable solutions in $\\mathcal{F}(\\eta,0,\\mathcal{D})$ as in Theorem C.2. Below we construct a counter-example, where we can prove a contradicting lower bound of $\\begin{array}{r}{\\int_{-x_{\\mathrm{max}}}^{x_{\\mathrm{max}}}|f^{\\prime\\prime}(x)|g(x)d x}\\end{array}$ for any interpolatng $f$ thus disprovingth asumptonof interpolaton ", "page_idx": 5}, {"type": "text", "text": "Counter-example. We fix 2mxi - (m+1)mx fori E [n] and fo(z) = 0for any x, which implies that $y_{i}$ 's are independent random variables from ${\\mathcal{N}}(0,\\sigma^{2})$ ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.1. For the counter-example, with probability $1-\\delta$ for any interpolating function $f$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\int_{-x_{\\mathrm{max}}}^{x_{\\mathrm{max}}}|f^{\\prime\\prime}(x)|g(x)d x=\\Omega\\left(\\sigma n\\left[n-24\\log\\left(\\frac{1}{\\delta}\\right)\\right]\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where the randomness comes from the noises $\\{\\epsilon_{i}\\}$ . Under this high-probability event, when $n\\geq$ $\\begin{array}{r}{\\Omega\\left(\\sqrt{\\frac{1}{\\sigma\\eta}}\\log\\left(\\frac{1}{\\delta}\\right)\\right)}\\end{array}$ any stabe soluion $f$ for GD with sep size $\\eta$ wil ot interpolatethe da ie. ", "page_idx": 6}, {"type": "equation", "text": "$$\n{\\mathcal{F}}(\\eta,0,{\\mathcal{D}})\\cap\\{f\\mid f(x_{i})=y_{i},\\,\\forall\\,i\\in[n]\\}=\\emptyset.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The proof of Theorem 3.1 is deferred to Appendix $\\boldsymbol{\\mathrm{F}}$ due to space limit. This result, together with Mulayoff et al. [2021, Theorem 1], implies that gradient descent cannot converge to interpolating solutions unless $\\eta={\\cal O}(1/n^{2})$ . It also implies (when combined with Theorem 4.1) an intriguing geometric insight that all twice-differentiable interpolating solutions must be very sharp, i.e., its largest eigenvalue is larger than $\\Omega(n^{2})$ (see details in Appendix J ). Moreover, we highlight that the conclusion of Theorem 3.1 is consistent with our observation in Figure 2(a), where the learned function tends to be smoother and would not interpolate the data as $\\eta$ becomes larger. Therefore, in the following discussion, we consider the case without assuming interpolation. ", "page_idx": 6}, {"type": "text", "text": "4 Main Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we present the main results about stable solutions for GD (functions in $\\mathcal{F}(\\eta,0,\\mathcal{D}))$ from three aspects. Section 4.1 describes the implicit bias of stable solutions of gradient descent with large learning rate in the function space. Section 4.2 and 4.3 derive concrete generalization bounds that leverage the implicit biases in the distribution-free statistical learning setting and the non-parametric regression setting respectively. An outline of the proof of our main theorems is given in Appendix B. The full proof details are deferred to the appendix. ", "page_idx": 6}, {"type": "text", "text": "4.1 Implicit Bias of Stable Solutions in the Function Space ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We begin with characterizing the stable solutions for GD with step size $\\eta$ without the assumption of interpolation (there can be $i\\in[n]$ such that $f(x_{i})\\neq y_{i})$ . Similar to the interpolating case, the learned stable function $f$ enjoys a (weighted) $\\mathrm{TV}^{(1)}$ bound as below. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.1. For a function $f=f_{\\theta}$ where the training (square) loss $\\mathcal{L}$ is twice differentiable at $\\theta$ 2\uff0c ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\int_{-x_{\\mathrm{max}}}^{x_{\\mathrm{max}}}|f^{\\prime\\prime}(x)|g(x)d x\\leq\\frac{\\lambda_{\\mathrm{max}}(\\nabla_{\\theta}^{2}\\mathcal{L}(\\theta))}{2}-\\frac{1}{2}+x_{\\mathrm{max}}\\sqrt{2\\mathcal{L}(\\theta)},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $g(x)$ is defined as (6). Moreover, $i f$ we assume $y_{i}\\,=\\,f_{0}(x_{i})+\\epsilon_{i}$ for independent noise $\\epsilon_{i}\\sim\\mathcal{N}(0,\\sigma^{2})$ ,thenwithprobability $1-\\delta$ where the randomness is over the noises $\\{\\epsilon_{i}\\}$ ", "page_idx": 6}, {"type": "text", "text": "$\\int_{-x_{\\operatorname*{max}}}^{x_{\\operatorname*{max}}}|f^{\\prime\\prime}(x)|g(x)d x\\leq\\frac{\\lambda_{\\operatorname*{max}}(\\nabla_{\\theta}^{2}\\mathcal{L}(\\theta))}{2}-\\frac{1}{2}+\\widetilde O\\left(\\sigma x_{\\operatorname*{max}}\\cdot\\operatorname*{min}\\left\\{1,\\sqrt{\\frac{k}{n}}\\right\\}\\right)+x_{\\operatorname*{max}}\\sqrt{\\operatorname{MSE}(f)}.$ (10 ", "page_idx": 6}, {"type": "text", "text": "In addition,if $f=f_{\\theta}$ is $a$ stablesolutionof $G D$ with step size $\\eta$ on dataset $\\mathcal{D}$ i.e., $f_{\\theta}\\in\\mathcal{F}(\\eta,0,\\mathcal{D})$ as in (4),thenwecanreplace $\\frac{\\lambda_{\\operatorname*{max}}(\\nabla_{\\theta}^{2}\\mathcal{L}(\\theta))}{2}$ with $\\begin{array}{r}{\\frac{1}{\\eta}}\\end{array}$ in (9) and (10). ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.1, which we prove in Appendix G, associates the local curvature of the loss landscape at $\\theta$ with the smoothness of the function $f_{\\theta}$ it represents as measured in a weighted $\\mathrm{TV}^{(1)}$ norm. In short, it says that flat solutions are simple. The result is a strict generalization of Theorem 1 in Mulayoff et al. [2021] which requires interpolation, i.e., $\\mathcal{L}(\\theta)=0$ . Observe that the number of neurons $k$ does not appear in (9) and have no effect in (10) when $k>n$ , thus the result applies to arbitrarily overparameterized two-layer NNs. Under the standard nonparametric regression assumption, (10) is a stronger bound that asymptotically matches the bound under interpolation [Mulayoff et al., 2021, Theorem 1] when $k=o(n)$ and $\\mathrm{MSE}(f_{\\theta})=o(1)$ as the number of data points $n\\to\\infty$ ", "page_idx": 6}, {"type": "text", "text": "Another interesting observation when combining (10) with Theorem 3.1 is that for all interpolating solutions (observe that $\\mathrm{MSE}(f_{\\theta})=\\widetilde{O}(\\sigma^{2})$ W.h.p.) ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{max}}(\\nabla_{\\theta}^{2}\\mathcal{L}(\\theta))\\ge\\Omega(n^{2}\\sigma)-\\widetilde O(\\sigma).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "To say it differently, all interpolating solutions are very sharp minima when the labels are noisy. This provides theoretical explanation of the empirical observation that noisy labels are harder to overfit using gradient training [Zhang et al., 2021]. ", "page_idx": 7}, {"type": "text", "text": "Note that we leave the term ${\\mathcal{L}}(\\theta)$ (or $\\mathrm{MSE}(f))$ in the $\\mathrm{TV}^{(1)}$ bound. Therefore, we can plug any upper bound for these terms into Theorem 4.1 for a concrete result, and below we instantiate the $\\bar{\\mathrm{TV}}^{(1)}$ bound with a crude MSE bound under the assumption that $f$ is \u201coptimized'. ", "page_idx": 7}, {"type": "text", "text": "Corollary 4.2. In the nonparametric regression problem with ground-truth function $f_{0}$ for a stable solution $f=f_{\\theta}$ of $G D$ with step size $\\eta$ where $\\mathcal{L}$ is twice differentiableat $\\theta$ assume that $f$ is optimized, i.e,the empirical los of $f$ is smallerthan $\\begin{array}{r}{f_{0}\\colon\\frac{1}{2n}\\sum_{i=1}^{n}\\left(f(x_{i})-y_{i}\\right)^{2}\\le\\frac{1}{2n}\\sum_{i=1}^{n}\\left(f_{0}(x_{i})-y_{i}\\right)^{2},}\\end{array}$ thenwithprobability $1-\\delta$ the function $f$ satisfies ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\int_{-x_{\\mathrm{max}}}^{x_{\\mathrm{max}}}|f^{\\prime\\prime}(x)|g(x)d x\\leq\\frac{1}{\\eta}-\\frac{1}{2}+\\widetilde O\\left(\\sigma x_{\\mathrm{max}}\\right),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where the randomness is over the noises $\\{\\epsilon_{i}\\}$ and $\\widetilde O$ suppresses logarithmic terms of $n,1/\\delta$ ", "page_idx": 7}, {"type": "text", "text": "The assumption of optimized $f$ is rather mild since, in practice, gradient-based optimizers are commonly quite effective in loss minimization (see also our experiments). With such assumption, we can derive an MSE upper bound (with high probability) of order $\\widetilde O(\\sigma^{2})$ (details in Lemma G.5), and thusthe $\\mathrm{TV}^{(1)}$ bound above. ", "page_idx": 7}, {"type": "text", "text": "In some cases, we can decrease the MSE upper bound we assumed in Corollary 4.2, and use this to improve the resulting $\\mathrm{TV}^{(1)}$ bound (11). For instance, if the neural network is under-parameterized (i.e. $k$ is smaller than $n$ ), in Appendix G.2 we derive a (high-probability) bound for MSE of order ${\\widetilde{O}}(k/n)$ , which implies that the last term in (11) becomes $\\tilde{O}(\\sigma x_{\\mathrm{max}}\\sqrt{k/n})$ . The term vanishes if $n/k$ is large enough, where the $\\mathrm{TV}^{(1)}$ bound reduces to the noiseless and interpolating case. ", "page_idx": 7}, {"type": "text", "text": "4.2 GD on ReLU NN Does Not Overfit ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Why would anyone care about the function space implications of stable solutions? The next theorem shows that these solutions cannot overfit (in the strict interior of the data support) without making strong assumptions on the shape of the data distribution. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.3. Let $\\mathcal{P}$ be a joint distribution of $(x,y)$ supported on $[-\\underline{{x}}_{\\mathrm{max}},x_{\\mathrm{max}}]\\times[-D,D]$ Assume the dataset $\\mathcal{D}\\sim\\mathcal{P}^{n}$ i.i.d. For any fixed interval $T\\subset[-x_{\\mathrm{max}},x_{\\mathrm{max}}]$ anda universal constant $c>0$ such that with probability $1-\\delta/2,g(x)\\geq c$ for all $x\\in{\\mathcal{Z}}$ if thefunction $f=f_{\\theta}$ is a stable solution of $G D$ with step size  such that $\\mathcal{L}$ is twice differentiable at $\\theta$ and $\\|f\\|_{\\infty}\\leq D$ with probability $1-\\delta$ (randomness over the dataset), the generalization gap restricted to $\\mathcal{T}$ satisfies ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathsf{Z e n}_{\\mathbb{Z}}(f):=\\left|\\mathbb{E}_{\\mathbb{Z}}\\left[\\left(f(x)-y\\right)^{2}\\right]-\\frac{1}{n_{\\mathbb{Z}}}\\sum_{x_{i}\\in\\mathbb{Z}}\\left(f(x_{i})-y_{i}\\right)^{2}\\right|\\leq\\widetilde{O}\\left(D^{\\frac{9}{8}}\\left[\\frac{x_{\\operatorname*{max}}\\left(\\frac{1}{\\eta}-\\frac{1}{2}+2x_{\\operatorname*{max}}D\\right)}{n_{\\mathbb{Z}}^{2}}\\right]^{\\frac{1}{8}}\\right),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\mathbb{E}_{\\mathcal{L}}$ means that $(x,y)$ is a new sample from the data distribution conditioned on $x\\in\\mathcal{Z}$ and $n_{\\mathcal{L}}$ is the number of data points in $\\mathcal{D}$ suchthat $x_{i}\\in{\\mathcal{T}}$ ", "page_idx": 7}, {"type": "text", "text": "The proof of Theorem 4.3 is deferred to Appendix H. Briefly speaking, we show that in the strict interior of the data support, the generalization gap will vanish as the number of data points $n$ increases. This vanishing generalization gap further implies that the expected performance on new data points is close to the (observable) training loss, i.e., the output stable solutions do not overfit. ", "page_idx": 7}, {"type": "text", "text": "Regarding our assumptions, in addition to standard boundedness assumptions, we focus on the strict interior of the domain where $g$ can be lower bounded (i.e. interval $\\mathcal{T}$ ). This is because for extreme data points, $g(x)$ decays and thus imposes little constraint on the output function $f$ . In Appendix H.1, we show that if the marginal distribution of $x$ is the uniform distribution on $[-x_{\\mathrm{max}},x_{\\mathrm{max}}]$ ,when $n$ is suffciently large, $\\mathcal{T}$ can be chosen as $\\left[-{\\frac{2x_{\\operatorname*{max}}}{3}},{\\frac{2x_{\\operatorname*{max}}}{3}}\\right]$ In this case, with high probability, $\\mathcal{T}$ incorporates a large portion of the data points and $\\begin{array}{r}{n_{\\mathbb{Z}}=\\check{\\Omega}(n)}\\end{array}$ . More illustrations about the choice of $\\mathcal{T}$ under various data distributions are deferred to Appendix H.2. ", "page_idx": 7}, {"type": "text", "text": "Meanwhile, the generalization gap bound has dependence $\\begin{array}{r}{{\\frac{1}{\\eta}}}\\end{array}$ on the learning rate $\\eta$ .Therefore,aswe increase the learning rate (in a reasonable range), the learned stable solution tends to be smoother, which further implies better generalization performances. ", "page_idx": 8}, {"type": "text", "text": "4.3GD on ReLU NN Achieves Optimal Rate for Estimating BV(1) Functions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Finally, we zoom into the nonparametric regression task that we described in Section 2 where $x_{1},\\cdot\\cdot\\cdot\\,,x_{n}$ are fixed and the noisy labels $y_{i}\\ \\overset{\\cdot}{=}f_{0}(x_{i})+\\mathcal{N}(0,\\sigma^{2})$ independently for $i\\in[n]$ for a ground truth function $f_{0}$ in the first order bounded variation class (see Section 2 for details). Similar to Theorem 4.3, we focus on the strict interior $\\mathcal{T}$ of $[-x_{\\mathrm{max}},x_{\\mathrm{max}}]$ , but instead of a generalization gap bound, we prove an MSE bound against $f_{0}$ on $\\mathcal{T}$ that nearly matches the theoretical limit. ", "page_idx": 8}, {"type": "text", "text": "Theorem 4.4. Under the same conditions in Corollary 4.2, for any interval $\\mathcal{I}\\subset[-x_{\\mathrm{max}},x_{\\mathrm{max}}]$ andauniversalconstant $c>0$ such that $g(x)\\geq c$ for all $x\\in\\mathcal{T}$ and $f$ isoptimizedover $\\mathcal{T}$ ,i.e. $\\begin{array}{r}{\\sum_{x_{i}\\in\\mathbb Z}(f(x_{i})-y_{i})^{2}\\le\\sum_{x_{i}\\in\\mathbb Z}(f_{0}(x_{i})-y_{i})^{2}}\\end{array}$ iftheoutputstablesolution $\\theta$ satisfies $\\|\\theta\\|_{\\infty}\\leq\\rho$ (for some constant $\\rho>0$ ) and the ground truth $f_{0}\\in\\mathrm{BV}^{(1)}(k\\rho^{2},\\frac{1}{c}\\widetilde O(\\frac{1}{\\eta}+\\sigma x_{\\mathrm{max}}))$ ,then with probability $1-\\delta$ (overtherandomnoises $\\{\\epsilon_{i}\\}_{.}$ ), the function $f=f_{\\theta}$ satisfies ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathrm{MSE}_{\\mathbb{Z}}(f)=\\frac{1}{n_{T}}\\sum_{x_{i}\\in\\mathbb{Z}}\\left(f(x_{i})-f_{0}(x_{i})\\right)^{2}\\leq\\widetilde{O}\\left(\\left(\\frac{\\sigma^{2}}{n_{T}}\\right)^{\\frac{4}{5}}\\left(\\frac{x_{\\operatorname*{max}}}{\\eta}+\\sigma x_{\\operatorname*{max}}^{2}\\right)^{\\frac{2}{5}}\\right),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $n\\tau$ is the number of data points in $\\mathcal{D}$ suchthat $x_{i}\\in\\mathcal{T}$ ", "page_idx": 8}, {"type": "text", "text": "The proof of Theorem 4.4 is deferred to Appendix I. Below we discuss some interesting aspects of the result. First of all, Theorem 4.4 focuses on an interval $\\mathcal{T}$ where $g(x)$ can be lower bounded. In this way, we ignore the extreme data points and derive an MSE upper bound (restricted to $\\mathcal{T}$ of order O(n-4/5) , which matches the minimax optimal rate for estimating $\\mathrm{BV}^{(1)}$ functions [see, e.g., Donoho and Johnstone, 1998, Theorem 1]. In contrast, it is well-known that neural networks in the kernel regime (and any other \u201clinear smoothers\") must incur a strictly suboptimal worst-case $\\mathrm{MSE}=\\Omega(n_{\\mathbb{Z}}{-}2/3)$ [Donoho et al., 1990, Suzuki, 2018]. According to the discussions in Appendix H.1, if the data follows a uniform distribution, the interval $\\mathcal{T}$ can incorporate most of the data points where $n_{\\mathcal{T}}=\\Omega(n)$ ", "page_idx": 8}, {"type": "text", "text": "Meanwhile, the MSE bound has dependence $\\eta^{-2/5}$ on the learning rate $\\eta$ Such dependence is because with a larger learning rate, the learned function $f$ will have smaller $\\mathrm{TV}^{(1)}$ bound, and therefore the set of possible output functions will contain fewer non-smooth functions, which implies a tighter MSE bound. However, this does not mean that a larger learning rate is always better. When $\\eta$ is too large, GD may diverge, and even if it does not, the set of stable solutions cannot approximate the ground truth fo wellif Jmax $\\begin{array}{r}{\\int_{-x_{\\mathrm{max}}}^{x_{\\mathrm{max}}}|f_{0}^{\\prime\\prime}(x)|d x\\gg\\frac{1}{c\\eta}+\\frac{\\sigma x_{\\mathrm{max}}}{c}}\\end{array}$ , thus failing to satisfy the \u201coptimized\" assumption. In our experiments, we verify the \u201coptimized\u201d\u2019 assumption numerically for a wide range of $\\eta$ and demonstrate that by tuning learning rate $\\eta$ , we are adapting to the unknown $\\mathrm{TV}^{(1)}(f_{0})$ ", "page_idx": 8}, {"type": "text", "text": "Lastly, we remark that Theorem 4.4 holds for arbitrary $k$ , even if the neural network is heavily over-parameterized $(k\\gg n)$ . The dependence $\\begin{array}{r}{\\frac{1}{\\eta}+\\sigma x_{\\mathrm{max}}}\\end{array}$ on $\\eta$ results from the $\\mathrm{TV}^{(1)}$ bound in Corollry 4.2, wherete term $\\begin{array}{r}{{\\frac{1}{\\eta}}}\\end{array}$ will dominate if $\\begin{array}{r}{\\eta\\leq\\frac{1}{\\sigma x_{\\mathrm{max}}}}\\end{array}$ , Which is the case if the step size is not large. Furthermore, if $\\textstyle{\\frac{n}{k}}$ is suffciently large, we can improve the $\\mathrm{TV}^{(1)}$ bound in Corollary 4.2 as in Apendpq $\\mathrm{MSE}_{\\mathbb{Z}}(f)$ 10 $\\begin{array}{r}{\\widetilde{O}\\left(\\left(\\frac{\\sigma^{2}}{n_{\\mathcal{Z}}}\\right)^{\\frac{4}{5}}\\left(\\frac{x_{\\mathrm{max}}}{\\eta}\\right)^{\\frac{2}{5}}\\right)}\\end{array}$ accordingly. The assumptions and detailed statements for the improved MSE are deferred to Appendix 1.1. ", "page_idx": 8}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we empirically validate our claims by training a two-layer fully connected neural network with ReLU activation using gradient descent (GD) with varying step sizes. We focus on fitting a mildly overparameterized ReLU network to a simple nonparametric regression problem. The input dataset comprises of 30 equally spaced fixed design points $\\{x_{i}\\}_{i=1}^{n}$ : where each $x_{i}\\in[-0.5,0.5]$ $n=$ 30). Label $\\bar{y_{i}}=f_{0}(x_{i})\\substack{+\\bar{\\mathcal{N}}(0,\\,\\sigma^{2})}$ with $\\sigma=0.5$ and $f_{0}(x)={\\bar{(2x+1)}}\\mathbf{1}(x\\leq0)+(-{\\bar{2}}x+1)\\mathbf{1}({\\bar{x}}>0)$ ", "page_idx": 8}, {"type": "image", "img_path": "7Swrtm9Qsp/tmp/112c442b726048a348563a18a3ea119bb888252c871bc7556a0667cdf38a4308.jpg", "img_caption": ["Figure 3: Highlights of our numerical simulation for large step size $\\eta=0.4$ , first row) and small stepsize $(\\eta=0.01$ , second row) gradient descent training of a univariate ReLU NN with $n=30$ noisy observations and $k=100$ hidden neurons. From left to right, the three columns illustrate (a) Trained NN function (b) Learning curves (c) Learned basis functions (each of the 100 neurons). "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "The two-layer ReLU network is parameterized by $\\theta$ (seeSection 2) with $k=100$ neuronsper layer. The network uses standard parameterization (scale factor of 1) and parameters are initialized randomly (see Figure 7 for the initial basis functions). ", "page_idx": 9}, {"type": "text", "text": "Figure 2 (in the introduction) illustrates how changing the learning rate affects the learned ReLU NN that GD-training stabilizes on. The main take-aways are (a) large learning rate learns flatter minima which represent more regular functions $(\\mathrm{in~TV}^{(1)})$ ; (b) Our bound from Theorem 4.1 is a very accurate description of the curvature of the Hessian as well as a valid upper bound of the $\\mathrm{TV}^{\\mathrm{(1)}}$ -(pseudo) norm;(c) When we tune learning rate $\\eta$ i is implitlyregularizing the complexity, which provides a satisfying variance tradeoff explanation to how GD-training works. ", "page_idx": 9}, {"type": "text", "text": "Figure 3 provides further details on the learning curves and representation learning. We note that the learned representation is very different from the initialization, thus our experiments are clearly describing phenomena not covered by the \u201ckernel' regime. In addition, it seems that all solutions that GD finds after a small number of iterations satisfy the \u201coptimized\"\u2019 assumption as required in Theorem 4.4. In the appendix (Figure 8), we provide empirical justification for the other assumption we make about the twice-differentiability of the solutions. More experiments can be found in the appendix with more learning rate choices, as well as a discussion on the catastrophic and tempered overfitting of interpolating solutions when we adjust $k$ ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we took a new look into how gradient descent-trained two-layer ReLU neural networks generalize from a lens of minima stability (and the closely related Edge-of-Stability phenomena). We focused on univariate inputs with noisy labels and showed GD with typical choice of learning rate cannot interpolate the data. We also established that local smoothness of the training loss functions implies a first order total variation constraint on the function the neural network represents, hence proving that all such solutions have a vanishing generalization gap inside the strict interior of the data support. In addition, under a mild assumption, we prove that these stable solutions achieve near-optimal rate for estimating first-order bounded variation functions. Future work includes generalization beyond 1D input, two-hidden layers, and understanding the choice of optimization allgorithms. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The research is partially supported by NSF #2134214. DQ, KZ, ES and YW's work was partially completed while they were with the Department of Computer Science at UCSB. The research of DS was Funded by the European Union (ERC, A-B-C-Deep, 101039436). Views and opinions expressed are however those of the author only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency (ERCEA). Neither the European Union nor the granting authority can be held responsible for them. DS also acknowledges the support of the Schmidt Career Advancement Chair in AI. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Kwangjun Ahn, Sebastien Bubeck, Sinho Chewi, Yin Tat Lee, Felipe Suarez, and Yi Zhang. Learning threshold neurons via edge of stability. Advances in Neural Information Processing Systems, 36, 2023. ", "page_idx": 10}, {"type": "text", "text": "Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. In International Conference on Machine Learning, pages 322-332. PMLR, 2019.   \nSanjeev Arora, Zhiyuan Li, and Abhishek Panigrahi. Understanding gradient descent on the edge of stability in deep learning. In International Conference on Machine Learning, pages 948-1024. PMLR,2022.   \nPeter L Bartlett, Philip M Long, Gabor Lugosi, and Alexander Tsigler. Benign overfitting in linear regression. Proceedings of the National Academy of Sciences, 117(48):30063-30070, 2020.   \nMikhail Belkin, Alexander Rakhlin, and Alexandre B Tsybakov. Does data interpolation contradict statistical optimality? In The 22nd International Conference on Artifcial Intelligence and Statistics, pages 1611-1619. PMLR, 2019.   \nGon Buzaglo, Itamar Harel, Mor Shpigel Nacson, Alon Brutzkus, Nathan Srebro, and Daniel Soudry. How uniform random weights induce non-uniform bias: Typical interpolating neural networks generalize with narrow teachers. In International Conference on Machine Learning, 2024.   \nYuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and deep neural networks. Advances in neural information processing systems, 32, 2019.   \nHerman Chernoff. A measure of asymptotic efficiency for tests of a hypothesis based on the sum of observations. The Annals of Mathematical Statistics, pages 493-507, 1952.   \nLenaic Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss. In Conference on learning theory, pages 1305-1338. PMLR, 2020.   \nLenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming. Advances in neural information processing systems, 32, 2019.   \nJeremy Cohen, Simran Kaur, Yuanzhi Li, J Zico Kolter, and Ameet Talwalkar. Gradient descent on neural networks typically occurs at the edge of stability. In International Conference on Learning Representations,2020.   \nAlex Damian, Eshaan Nichani, and Jason D Lee. Self-stabilization: The implicit bias of gradient descent at the edge of stability. In International Conference on Learning Representations, 2024.   \nChristoph Dann, Tor Lattimore, and Emma Brunskill. Unifying pac and regret: Uniform pac bounds for episodic reinforcement learning. Advances in Neural Information Processing Systems, 30, 2017.   \nRonald A DeVore and George G Lorentz. Constructive approximation, volume 303. Springer Science & Business Media, 1993.   \nDavid L Donoho and Iain M Johnstone. Minimax estimation via wavelet shrinkage. The annals of Statistics, 26(3):879-921,1998.   \nDavid L Donoho, Richard C Liu, and Brenda MacGibbon. Minimax risk over hyperrectangles, and implications. The Annals of Statistics, pages 1416-1437, 1990.   \nSimon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-parameterized neural networks. In International Conference on Learning Representations, 2018.   \nDavid Eric Edmunds and Hans Triebel. Function spaces, entropy numbers, differential operators. (No Title), 1996.   \nSpencer Frei, Niladri S Chatterji, and Peter Bartlett. Benign overfitting without linearity: Neural network classifiers trained by gradient descent for noisy linear data. In Conference on Learning Theory, pages 2668-2703. PMLR, 2022.   \nSaeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic programming. SIAM journal on optimization, 23(4):2341-2368, 2013.   \nMoritz Haas, David Holzmiller, Ulrike Luxburg, and Ingo Steinwart. Mind the spikes: Benign overfitting of kernels and neural networks in fixed dimension. Advances in Neural Information Processing Systems, 36, 2023.   \nYifan Hao and Tong Zhang. The surprising harmfulness of benign overfiting for adversarial robustness. arXiv preprint arXiv:2401.12236, 2024.   \nMoritz Hardt, Ben Recht, and Yoram Singer Train faster, generalize better: Stability of stochastic gradient descent. In International conference on machine learning, pages 1225-1234. PMLR, 2016.   \nSepp Hochreiter and Jurgen Schmidhuber. Flat minima. Neural computation, 9(1):1-42, 1997.   \nAddison J Hu, Alden Green, and Ryan J Tibshirani. The voronoigram: Minimax estimation of bounded variation functions from scattered data. arXiv preprint arXiv:2212.14514, 2022.   \nWei Hu, Zhiyuan Li, and Dingli Yu. Simple and effective regularization methods for training on noisily labeled data with generalization guarantee. In International Conference on Learning Representations, 2020.   \nArthur Jacot, Franck Gabriel, and Cl\u00e9ment Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Advances in neural information processing systems, 31, 2018.   \nChi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape saddle points effciently. In International conference on machine learning, pages 1724-1732. PMLR, 2017.   \nHui Jin and Guido Montifar. Implicit bias of gradient descent for mean squared error regression with two-layer wide neural networks. Journal of Machine Learning Research, 24(137):1-97, 2023.   \nNirmit Joshi, Gal Vardi, and Nathan Srebro. Noisy interpolation learning with shallw univariate relu networks. In International Conference on Learning Representations, 2023.   \nNitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In International Conference on Learning Representations, 2017.   \nGuy Kornowski, Gilad Yehudai, and Ohad Shamir. From tempered to benign overfitting in relu neural networks. Advances in Neural Information Processing Systems, 36, 2024.   \nYiwen Kou, Zixiang Chen, Yuanzhou Chen, and Quanquan Gu. Benign overfitting in two-layer relu convolutional neural networks. In International Conference on Machine Learning, pages 17615-17659. PMLR, 2023.   \nItai Kreisler, Mor Shpigel Nacson, Daniel Soudry, and Yair Carmon. Gradient descent monotonically decreases the sharpness of gradient flow solutions in scalar networks and beyond. In International Conference on Machine Learning, pages 17684-17744. PMLR, 2023.   \nAitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy Gur-Ari. The large learning rate phase of deep learning: the catapult mechanism. arXiv preprint arXiv:2003.02218, 2020.   \nChaoyue Liu, Libin Zhu, and Mikhail Belkin. Loss landscapes and optimization in over-parameterized non-linear systems and neural networks. Applied and Computational Harmonic Analysis, 59: 85-116, 2022.   \nHao Liu, Minshuo Chen, Tuo Zhao, and Wenjing Liao. Besov function approximation and binary classification on low-dimensional manifolds using convolutional residual networks. In International Conference on Machine Learning, pages 6770-6780. PMLR, 2021.   \nChao Ma and Lexing Ying. On linear stability of sgd and input-smoothness of neural networks. Advances in Neural Information Processing Systems, 34:16805-16817, 2021.   \nNeil Mallinar, James Simon, Amirhesam Abedsoltan, Parthe Pandit, Misha Belkin, and Preetum Nakkiran. Benign,tempered, or catastrophic: Toward a refined taxonomy of overfitting. Advances in Neural Information Processing Systems, 35:1182-1195, 2022.   \nSong Mei, Theodor Misiakiewicz, and Andrea Montanari. Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit. In Conference on Learning Theory, pages 2388-2464. PMLR, 2019.   \nRotem Mulayoff, Tomer Michaeli, and Daniel Soudry. The implicit bias of minima stability: A view from function space. Advances in Neural Information Processing Systems, 34:17749-17761, 2021.   \nMor Shpigel Nacson, Rotem Mulayoff Greg Ongie, Tomer Michaeli, and Daniel Soudry The imliit bias of minima stability in multivariate shallow relu networks. In The Eleventh International Conference on Learning Representations, 2022.   \nRichard Nickl and Benedikt M Potscher. Bracketing metric entropy rates and empirical central limit theorems for function classes of besov-and sobolev-type. Journal of Theoretical Probability, 20: 177-199, 2007.   \nGreg Ongie, Rebecca Willett, Daniel Soudry, and Nathan Srebro. A function space view of bounded norm infinite width relu nets: The multivariate case. In ICLR, 2020.   \nRahul Parhi and Robert D Nowak. Banach space representer theorems for neural networks and ridge splines. Journal of Machine Learning Research, 22(43): 1-40, 2021.   \nDan Qiao and Yu-Xiang Wang. Near-optimal deployment efciency in reward-free reinforcement learning with linear function approximation. In The Eleventh International Conference on Learning Representations, 2023a.   \nDan Qiao and Yu-Xiang Wang. Near-optimal differentially private reinforcement learning. In International Conference on Artificial Intelligence and Statistics, pages 9914-9940. PMLR, 2023b.   \nDan Qiao and Yu-Xiang Wang. Offine reinforcement learning with differential privacy. Advances in Neural Information Processing Systems, 36, 2023c.   \nDan Qiao and Yu-Xiang Wang. Near-optimal reinforcement learning with self-play under adaptivity constraints. In International Conference on Machine Learning, 2024.   \nDan Qiao, Ming Yin, Ming Min, and Yu-Xiang Wang. Sample-efficient reinforcement learning with loglog (t) switching cost. In International Conference on Machine Learning, pages 18031-18061. PMLR, 2022.   \nDan Qiao, Ming Yin, and Yu-Xiang Wang. Logarithmic switching cost in reinforcement learning beyond linear mdps. arXiv preprint arXiv:2302.12456, 2023.   \nDominic Richards and Ija Kuzborskij. Stability & generalisation of gradient descent for shallow neural networks without the neural tangent kernel. Advances in neural information processing systems, 34:8609-8621, 2021.   \nPedro Savarese, Itay Evron, Daniel Soudry, and Nathan Srebro. How do infinite width bounded norm networks look in function space? In Conference on Learning Theory, pages 2667-2690. PMLR, 2019.   \nKarthik Sridharan. A gentle introduction to concentration inequalities. Dept. Comput. Sci., Cornell Univ., Tech.Rep, 2002.   \nTaiji Suzuki. Adaptivity of deep relu network for learning in besov and mixed smooth besov spaces: optimal rate and curse of dimensionality. arXiv preprint arXiv:1810.08033, 2018.   \nRyan J Tibshirani. Fast stagewise algorithms for approximate regularization paths. https : / /www . stat.berkeley.edu/\\~ryantibs/talks/stagewise-2014.pdf, 2014.   \nRyan J Tibshirani. A general framework for fast stagewise algorithms. J. Mach. Learn. Res., 16(1): 2543-2588,2015.   \nMartin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cambridge university press, 2019.   \nBlake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pedro Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models. In Conference on Learning Theory, pages 3635-3673. PMLR, 2020.   \nLei Wu and Weijie J Su. The implicit regularization of dynamical stability in stochastic gradient descent. In International Conference on Machine Learning, pages 37656-37684. PMLR, 2023.   \nLei Wu, Chao Ma, and Weinan E. How sgd selects the global minima in over-parameterized learning: A dynamical stability perspective. Advances in Neural Information Processing Systems, 31, 2018.   \nJianyu Xu, Dan Qiao, and Yu-Xiang Wang. Doubly fair dynamic pricing. In International Conference on Artijficial Intelligence and Statistics, pages 9941-9975. PMLR, 2023.   \nChiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107-115, 2021.   \nKaiqi Zhang and Yu-Xiang Wang. Deep learning meets nonparametric regression: Are weightdecayed dnns locally adaptive? In International Conference on Learning Representations, 2022.   \nFuheng Zhao, Dan Qiao, Rachel Redberg, Divyakant Agrawal, Amr El Abbadi, and Yu-Xiang Wang. Differentially private linear sketches: Effcient implementations and applications. Advances in Neural Information Processing Systems, 35:12691-12704, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A  Full Experimental Results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1  Stable Minima GD Converges to and Learning Curves ", "text_level": 1, "page_idx": 14}, {"type": "image", "img_path": "7Swrtm9Qsp/tmp/40d421085a6053bce93b066ddad0075db07afef47ff7f54af2e9ea9d89da0ddd.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "", "img_caption": ["Figure 4: Illustration of the solutions gradient descent with learning rate $\\eta$ converges to (Part I). As $\\eta$ decreases, the fitted function goes from simple to complex. Any line below the $\\bar{\\sigma}^{2}$ line satisfies the \"optimized\u2019\u201d' assumption from Corollary 4.2 and Theorem 4.4. "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "7Swrtm9Qsp/tmp/3bcb9e2cf55a5cbd282f0af0376df579eb523cc5b4ceab2f0ad44456c88db831.jpg", "img_caption": ["Figure 5: Illustration of the solutions gradient descent with learning rate $\\eta$ converges to (Part II). As $\\eta$ decreases further, the fitted function starts to overfit to the noisy label. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.2Interpolating Solutions as the Number of Hidden Neurons Increases ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we illustrate a sequence of interpolating solutions that are the global optimal solutions, which is also the kernel limit in the \u201clazy\u201d regime. The results are obtained by randomly initializing ", "page_idx": 15}, {"type": "text", "text": "the weights, but solve the minimum norm solution by directly solving the least square problem (optimizing only the second layer weights.) ", "page_idx": 16}, {"type": "image", "img_path": "7Swrtm9Qsp/tmp/a12cff4c4cd3d9871a49a0a53b5c195696dd98edcd780a6d2100e658fd1c2346.jpg", "img_caption": ["Figure 6: Examples of global optimal (interpolating) solutions (fitting only second layer weights). Notice that the number of data points $n=30$ . When the model is barely able to interpolate $[k=30]$ the fitted function experiences the catastrophic overfitting. When the number of neurons $k$ increases the interpolation solution becomes smoother and enters the tempered overfitting regime. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "A.3 Representation Learning of Large Learning Rate: Visualizing Learned Basis Functions. ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we visualize the basis functions at initialization and after training with different learning rate. ", "page_idx": 17}, {"type": "image", "img_path": "7Swrtm9Qsp/tmp/55fa947f17734a211e4e047ff39479056c430a67f471a4261727338684f301b8.jpg", "img_caption": ["Figure 7: Illustration of the learned basis function with learning rate $\\eta$ It is clear from the figures that there were substantial representation learning and the number of active basis functions gets smaller as the learning rate $\\eta$ gets bigger. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "We make several observations about Figure 7. First, the learned basis functions are very different from the initialization, so a lot of representation learning is happening, in comparison to the \u201ckernel\" regime in which nearly no representation learning is happening. Second, as $\\eta$ gets smaller, the number of learned basis functions increases, hence increasing the number of knots in the fitted function. Third, the learned basis function displays a strong \u201cclustering\" effect in the sense that despite overparameterization, many learned basis functions end up being the same on the data support. Interestingly, they are not the same on $\\mathbb{R}$ , we verified that they are still different outside the data support, e.g., one of the learned basis function has a knot at $x<-800$ ", "page_idx": 17}, {"type": "text", "text": "A.4 Knots of the Learned ReLU NN (aka Linear Splines) and Their Coefficients ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Recall that a linear spline is a continuous piecewise linear function and a two-layer ReLU NN with $k$ neurons span the class of all linear splines with at most $k$ knots. In Figure 8, we visualize the locations of the knots of the linear spline that the learned ReLU neural networks represent. ", "page_idx": 17}, {"type": "image", "img_path": "7Swrtm9Qsp/tmp/1439de0203400f4d0169927477312978a764fbb233df50bdea9a15feb53363ff.jpg", "img_caption": ["Quantiles of the Learned knots ", "Learned knots to the closest input knot "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "7Swrtm9Qsp/tmp/91a4071c6df7abd2c9bd03383324085894870a0d9fa86322f00a8f4d7e1d72a0.jpg", "img_caption": ["Figure 8: Mllustration of the learned (steady-state) ReLU NN with different learning rate. Recall that all ReLU NNs are linear splines, therefore the location of the knots (i.e., the change points of the linear pieces) describes the representation learning that happens. Each basis function is a ReLU function at the knot. The final ReLU NN is a linear combination of these learned basis functions. In the first panel, we plot the quantiles of the locations of the learned knots as the function of $1/\\eta$ .In the second panel, we plot the sparsity of the learned coefficients in sparse $L_{1}$ and $L_{p}$ norm as the function of $1/\\eta$ . The third panel plots the distance of the learned knots from the closest input data points. This empirically verifies that the solution that gradient descent finds at the end is a twice differentiable function w.r.t. the parameters in the sense that not a single learned knot is exactly at the input data point, thus ensuring the applicability of our theorems. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "In the first panel of Figure 8, we find for large learning rate, most of the location of the learned knot is actually outside the data support on $[-0.5,0.5]$ . This is a somewhat surprising finding in that the mechanism of neural network learning may actually be \u201cpushing the knot outside the data support'\" so they become inactive on the training data (and only the ReLU truncated Os are active). This is a new (and very interesting) way to understand how sparsity arrives in gradient descent learning. ", "page_idx": 18}, {"type": "text", "text": "The second panel describes the sparsification effect of the implicit biases from large learning rate, which again, indicates that the weighted TV1 constraint is indeed making the learned function sparse (in the coefficient vector). The third panel shows that despite that the learning rate gets as small as $1e-2$ , none of the learned basis function actually have knots coinciding with any of the input data, thus empirically justifying our assumptions on the twice-differentiability of the solutions GD finds. ", "page_idx": 18}, {"type": "text", "text": "B Proof Overview ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we outline proof ideas for the main theorems in Section 4. ", "page_idx": 19}, {"type": "text", "text": "Proof overview of Theorem 4.1. According to direct calculation, the Hessian is given by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}^{2}\\mathcal{L}(\\theta)=\\frac{1}{n}\\sum_{i=1}^{n}(\\nabla_{\\theta}f_{\\theta}(x_{i}))(\\nabla_{\\theta}f_{\\theta}(x_{i}))^{T}+\\frac{1}{n}\\sum_{i=1}^{n}(f_{\\theta}(x_{i})-y_{i})\\nabla_{\\theta}^{2}f(x_{i}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Let $v$ denote the unit eigenvector $(\\|v\\|_{2}=1)$ of $\\begin{array}{r}{\\frac{1}{n}\\sum_{i=1}^{n}(\\nabla_{\\theta}f_{\\theta}(x_{i}))(\\nabla_{\\theta}f_{\\theta}(x_{i}))^{T}}\\end{array}$ with respect to the largest eigenvalue, then it holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\lambda_{\\operatorname*{max}}(\\nabla_{\\theta}^{2}\\mathcal{L}(\\theta))\\geq v^{T}\\nabla_{\\theta}^{2}\\mathcal{L}(\\theta)v}\\\\ &{=\\underbrace{\\lambda_{\\operatorname*{max}}\\left(\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}(\\nabla_{\\theta}f_{\\theta}(x_{i}))(\\nabla_{\\theta}f_{\\theta}(x_{i}))^{T}\\right)}_{(\\star)}+\\underbrace{\\frac{1}{n}\\sum_{i=1}^{n}(f_{\\theta}(x_{i})-y_{i})v^{T}\\nabla_{\\theta}^{2}f(x_{i})v}_{(\\#)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For the term $(\\star)$ , we connect the maximal eigenvalue at $\\theta$ to the (weighted) $\\mathrm{TV}^{(1)}$ norm of the corresponding $f\\,=\\,f_{\\theta}$ . Let $g(x)$ be defined as (6), Mulayoff et al. [2021, Lemma 4] shows that (details in Lemma G.1): ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left(\\star\\right)=\\lambda_{\\operatorname*{max}}\\left(\\frac{1}{n}\\sum_{i=1}^{n}(\\nabla_{\\theta}f_{\\theta}(x_{i}))(\\nabla_{\\theta}f_{\\theta}(x_{i}))^{T}\\right)\\geq1+2\\int_{-x_{\\operatorname*{max}}}^{x_{\\operatorname*{max}}}|f^{\\prime\\prime}(x)|g(x)d x.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For the term $(\\#)$ , we can bound it by the training loss ${\\mathcal{L}}(\\theta)$ using Cauchy-Schwarz inequality and a somewhat surprising uniform upper bound of $v^{\\check{T}}\\nabla_{\\theta}^{2}f(x_{i})\\check{v}$ in Lemma E.1: ", "page_idx": 19}, {"type": "equation", "text": "$$\n|(\\#)|\\leq\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}\\left(f_{\\theta}(x_{i})-y_{i}\\right)^{2}}\\cdot\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}\\left(v^{T}\\nabla_{\\theta}^{2}f(x_{i})v\\right)^{2}}\\leq2x_{\\operatorname*{max}}\\sqrt{2\\mathcal{L}(\\theta)}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The first conclusion (9) of Theorem 4.1 is derived by combining the inequalities above. For the second conclusion (10), note that the term $(\\#)$ can be further decomposed as: ", "page_idx": 19}, {"type": "equation", "text": "$$\n(\\#)=\\underbrace{\\frac{1}{n}\\sum_{i=1}^{n}(f_{0}(x_{i})-y_{i})v^{T}\\nabla_{\\theta}^{2}f(x_{i})v}_{(i)}+\\underbrace{\\frac{1}{n}\\sum_{i=1}^{n}(f_{\\theta}(x_{i})-f_{0}(x_{i}))v^{T}\\nabla_{\\theta}^{2}f(x_{i})v}_{(i i)}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Similar to $(\\#)$ , the term $|(i i)|$ can be bounded by $2x_{\\mathrm{max}}{\\sqrt{\\mathrm{MSE}(f_{\\theta})}}$ using Cauchy-Schwarz inequality. Under the data-generating assumption $y_{i}-f_{0}(x_{i})=\\epsilon_{i}\\sim\\mathcal{N}(0,\\sigma^{2})$ i.i.d., $|(i)|$ can be bounded by a certain empirical Gaussian complexity term $\\begin{array}{r}{\\operatorname*{sup}_{\\theta}\\frac{1}{n}\\sum_{i}\\epsilon_{i}h_{\\theta}(x_{i})}\\end{array}$ with $h_{\\theta}(x_{i})\\stackrel{}{=}v^{T}\\nabla_{\\theta}^{2}f(x_{i})v$ . The proof is complete by Lemma G.2 which bounds this Gaussian complexity term (w.h.p.) in two ways: (1) a dimension-free bound of $\\widetilde{O}(\\sigma x_{\\mathrm{max}})$ and (2) $\\widetilde{O}(\\sigma x_{\\operatorname*{max}}\\sqrt{k/n})$ for the under-parameterized case. ", "page_idx": 19}, {"type": "text", "text": "Proof overview of Theorem 4.3. Under the boundedness assumption in Theorem 4.3, we can prove aconstatuppr bound for $\\begin{array}{r}{\\int_{-x_{\\mathrm{max}}}^{x_{\\mathrm{max}}}|f^{\\prime\\prime}(x)|g(x)d x}\\end{array}$ Lmmawhfrtheripan constant upper bound for $\\textstyle\\int_{\\mathcal{T}}|f^{\\prime\\prime}(x)|{\\widehat{d x}}$ . Therefore, the metric entropy (logarithmic of $\\epsilon_{}$ -covering number $N_{\\epsilon}$ ) of the possible output function class is of order $\\epsilon^{-1/2}$ (Details in Lemma H.2). ", "page_idx": 19}, {"type": "text", "text": "For a fixed $\\epsilon$ -cover of the possible output function class, according to Hoeffding's inequality and a union bound, the uniform upper bound of ${\\mathrm{Gen}}_{\\mathbb{Z}}(f)$ can be bounded by $\\tilde{O}(\\sqrt{\\log N_{\\epsilon}/n_{\\mathcal{T}}})\\;=$ $\\widetilde{\\cal O}(\\epsilon^{-\\frac{1}{4}}n_{\\mathbb{Z}}^{-\\frac{1}{2}})$ wihhibabtaxiois $\\widetilde O(\\epsilon)$ therefore ${\\mathrm{Gen}}_{\\mathbb{Z}}(f)$ can be uniformly bounded over the possible output function class by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\widetilde{\\cal O}(\\epsilon)+\\widetilde{\\cal O}(\\epsilon^{-\\frac{1}{4}}n_{\\mathcal{T}}^{-\\frac{1}{2}})=\\widetilde{\\cal O}(n_{\\mathcal{T}}^{-\\frac{2}{5}}),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\epsilon$ is chosen to minimize the bound. More details can be found in the proof of Theorem H.3. ", "page_idx": 19}, {"type": "text", "text": "Proof overview of Theorem 4.4. Similar to Theorem 4.3, we can prove a constant upper bound for $\\textstyle\\int_{\\mathcal{T}}|f^{\\prime\\prime}(x)|d x$ , which implies that the metric entropy of the possible output function class (4) is of order $\\epsilon^{-1/2}$ Details in Lmma I.2). Therefore, the critical radus $r$ is of order ${\\widetilde O}(n_{\\mathbb Z}^{-\\frac{2}{5}})$ , which leads to a (high probability) MSE upper bound of order ${\\widetilde O}(n_{\\mathbb Z}^{-\\frac{4}{5}})$ using a self-bounding technique. More details about handling other parameters can be found in Appendix I. ", "page_idx": 19}, {"type": "text", "text": "C Some Optimization Results ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Lemma C.1 (Restate Lemma 2.2). Consider the update rule in Definition 2.1, for any $\\epsilon>0$ alocal minimum o\\* is an inearly sable minimum of Lif and only if >max(2 C(0\\*)\u2264 2 ", "page_idx": 20}, {"type": "text", "text": "Proof of Lemma C.1. It holds that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\theta_{t+1}-\\theta^{\\star}=\\theta_{t}-\\theta^{\\star}-\\eta\\left(\\nabla\\mathcal{L}(\\theta^{\\star})+\\nabla^{2}\\mathcal{L}(\\theta^{\\star})(\\theta_{t}-\\theta^{\\star})\\right)}\\\\ &{\\quad\\quad\\quad\\quad=\\theta_{t}-\\theta^{\\star}-\\eta\\nabla^{2}\\mathcal{L}(\\theta^{\\star})(\\theta_{t}-\\theta^{\\star})}\\\\ &{\\quad\\quad\\quad\\quad=\\left(I-\\eta\\nabla^{2}\\mathcal{L}(\\theta^{\\star})\\right)(\\theta_{t}-\\theta^{\\star}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the first equation is from the update rule in Definition 2.1. The second equation holds because $\\theta^{\\star}$ is a local minimum and therefore $\\nabla{\\mathcal{L}}(\\theta^{\\star})=0$ .As a result, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\theta_{t}-\\theta^{\\star}=\\left(I-\\eta\\nabla^{2}\\mathcal{L}(\\theta^{\\star})\\right)^{t}(\\theta_{0}-\\theta^{\\star}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "On one hand, if $\\begin{array}{r}{\\lambda_{\\mathrm{max}}(\\nabla^{2}\\mathcal{L}(\\theta^{\\star}))\\leq\\frac{2}{\\eta}}\\end{array}$ , it holds that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\theta_{t}-\\theta^{\\star}\\|\\leq\\big\\|I-\\eta\\nabla^{2}\\mathcal{L}(\\theta^{\\star})\\big\\|_{2}^{t}\\cdot\\|\\theta_{0}-\\theta^{\\star}\\|\\leq\\|\\theta_{0}-\\theta^{\\star}\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the second inequality is because all the eigenvalues of $I-\\eta\\nabla^{2}\\mathcal{L}(\\theta^{\\star})$ is bounded between $[-1,1]$ . Therefore, $\\theta^{\\star}$ $\\epsilon$ linearly stable for any $\\epsilon$ ", "page_idx": 20}, {"type": "text", "text": "On the other hand, if $\\theta^{\\star}$ is $\\epsilon$ linearly stable, we choose $\\theta_{0}$ such that $\\frac{\\theta_{0}\\!-\\!\\theta^{\\star}}{\\|\\theta_{0}\\!-\\!\\theta^{\\star}\\|}$ is the top eigenvector of $\\nabla^{2}{\\mathcal{L}}(\\theta^{\\star})$ and $\\lVert{\\boldsymbol{\\theta}}_{0}-{\\boldsymbol{\\theta}}^{\\star}\\rVert=\\epsilon$ Then we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\|\\theta_{t}-\\theta^{\\star}\\right\\|=\\left|1-\\eta\\lambda_{\\operatorname*{max}}\\left(\\nabla^{2}\\mathcal{L}(\\theta^{\\star})\\right)\\right|^{t}\\cdot\\epsilon,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which implies that lin $\\begin{array}{r}{\\operatorname*{nsup}_{t\\to\\infty}\\left|1-\\eta\\lambda_{\\mathrm{max}}\\left(\\nabla^{2}\\mathcal{L}(\\theta^{\\star})\\right)\\right|^{t}\\leq1}\\end{array}$ and therefore $\\begin{array}{r}{\\lambda_{\\mathrm{max}}(\\nabla^{2}\\mathcal{L}(\\theta^{\\star}))\\leq\\frac{2}{\\eta}}\\\\ {\\lambda}\\end{array}$ which finishes the proof. ", "page_idx": 20}, {"type": "text", "text": "The following Theorem C.2 is an extension of the main result in Mulayoff et al. [2021]. Recall that stable solutions refer to the functions in $\\mathcal{F}(\\eta,0,\\mathcal{D})$ as defined in Section 2. ", "page_idx": 20}, {"type": "text", "text": "Theorem C.2 (Extension of Theorem 1 in Mulayoff et al. [2021]). Let $f=f_{\\theta}$ be a stable solution for $G D$ with step size $\\eta$ where the training loss $\\mathcal{L}(f)=0$ and $\\mathcal{L}$ istwicedifferentiableat $\\theta$ Then ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\int_{-x_{\\mathrm{max}}}^{x_{\\mathrm{max}}}|f^{\\prime\\prime}(x)|g(x)d x\\leq\\frac{1}{\\eta}-\\frac{1}{2},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $g(x)=\\operatorname*{min}\\{g^{-}(x),g^{+}(x)\\}$ for $x\\in[-x_{\\operatorname*{max}},x_{\\operatorname*{max}}]\\;w i t h$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g^{-}(x)=\\mathbb{P}^{2}(X<x)\\mathbb{E}[x-X|X<x]\\sqrt{1+(\\mathbb{E}[X|X<x])^{2}},}\\\\ &{g^{+}(x)=\\mathbb{P}^{2}(X>x)\\mathbb{E}[X-x|X>x]\\sqrt{1+(\\mathbb{E}[X|X>x])^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Here $X$ is drawn from the empirical distribution of the data (a sample chosen uniformly from $\\{x_{j}\\},$ ", "page_idx": 20}, {"type": "text", "text": "ProofofTheorem C.2. The proof of Theorem 1 in Mulayoff et al. [2021] first proves that $\\begin{array}{r}{\\lambda_{\\mathrm{max}}(\\bar{\\nabla}^{2}\\mathcal{L}(\\theta))\\;\\leq\\;\\frac{2}{\\eta}}\\end{array}$ according to the assumption that $f$ is linearly stable, and then proves the conclusion above. Therefore, the same conclusion directly follows for the stable solutions $f=f_{\\theta}$ in $\\mathcal{F}(\\eta,0,\\mathcal{D})$ satisfying $\\begin{array}{r}{\\lambda_{\\operatorname*{max}}(\\nabla^{2}\\mathcal{L}(\\theta))\\le\\frac{2}{\\eta}}\\end{array}$ \u53e3 ", "page_idx": 20}, {"type": "text", "text": "Some discussions about the result. Mulayoff et al. [2021] studied the problem assuming that the optimization converges to a global minimum and the learned function interpolates the training data, i.e\u3002 $\\textstyle{\\mathcal{L}}(f)=0$ . In this way, they link the Hessian matrix to properties of the learned function $f$ and show that stable solutions of GD correspond to functions whose second order derivative has a bounded weighted norm. Moreover, as the learning rate increases, the set of stable solutions contains less and less non-smooth functions. For a fixed learning rate, according to the curve of $g(x)$ ,stable solutions tend to be smoother for instances near the center of the data distribution, and less smooth for instances near the edges. More discussions can be found in Mulayoff et al. [2021]. ", "page_idx": 20}, {"type": "text", "text": "D   Bounded Variation Function Class and its Metric entropy ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we first define the Besov function class. Then we recall the definition of bounded variation function class and discuss the connection between these two classes. Finally we bound the metric entropy of bounded variation function class using analysis about Besov class. ", "page_idx": 21}, {"type": "text", "text": "D.1Definition of Besov Class ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Let $\\Omega$ be the domain of the function class (which we omit in the definition) and $\\|\\cdot\\|_{p}$ denote the $\\ell_{p}$ norm. We first define the modulus of smoothness. ", "page_idx": 21}, {"type": "text", "text": "Definition D.1. For a function $f\\in L^{p}(\\Omega)$ where $1\\leq p\\leq\\infty$ the $r$ -th modulus of smoothness is defined by ", "page_idx": 21}, {"type": "equation", "text": "$$\nw_{r,p}(f,t)=\\operatorname*{sup}_{h\\in\\mathbb{R}^{d}:\\|h\\|_{2}\\leq t}\\|\\Delta_{h}^{r}(f)\\|_{p},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\Delta$ is defined as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Delta_{h}^{r}(f):=\\left\\{\\begin{array}{l l}{\\sum_{j=0}^{r}\\binom{r}{j}\\,(-1)^{r-j}f(x+j h),\\quad\\mathrm{if}\\;x\\in\\Omega,\\ x+r h\\in\\Omega,}\\\\ {0,\\quad\\quad\\mathrm{otherwise}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then the norm of Besov space is defined as below. ", "page_idx": 21}, {"type": "text", "text": "Definition D.2. For $1\\leq p,q\\leq\\infty,\\alpha>0,r:=\\lceil\\alpha\\rceil+1$ define ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|f|_{B_{p,q}^{\\alpha}}=\\left\\{\\begin{array}{l l}{\\left(\\displaystyle\\int_{t=0}^{\\infty}\\left(t^{-\\alpha}w_{r,p}(f,t)\\right)^{q}\\frac{d t}{t}\\right)^{\\frac{1}{q}},\\qquad q<\\infty,}\\\\ {\\operatorname*{sup}_{t>0}t^{-\\alpha}w_{r,p}(f,t),\\qquad q=\\infty.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then the norm of Besov space is defined as: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|f\\|_{B_{p,q}^{\\alpha}}=\\|f\\|_{p}+|f|_{B_{p,q}^{\\alpha}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Finally,a function $f$ is in the Besov class $B_{p,q}^{\\alpha}$ $\\|f\\|_{B_{p,q}^{\\alpha}}$ is finite. For more discussions and properties of Besov class, we refer interesting readers to Edmunds and Triebel [1996]. ", "page_idx": 21}, {"type": "text", "text": "D.2Definition of Bounded Variation Class and the Connection ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "For the same domain $\\Omega$ , recall that the bounded variation function class is defined as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{BV}^{(1)}(B,C_{n}):=\\left\\{f:\\Omega\\to\\mathbb{R}\\left|\\operatorname*{max}_{x}|f(x)|\\leq B,\\int_{-x_{\\operatorname*{max}}}^{x_{\\operatorname*{max}}}|f^{\\prime\\prime}(x)|d x\\leq C_{n}\\right.\\right\\}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "According to De Vore and Lorentz [1993], bounded total variation class is closely connected to the Besov class. Specifically, for any constant $B,C_{n}$ , it holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{BV}^{(1)}(B,C_{n})\\subset B_{1,\\infty}^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "D.3Metric Entropy of Bounded Total Variation Function Class ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Now we bound the complexity of $\\mathbf{B}\\mathbf{V}^{(1)}(1,1)$ , which will be helpful for bounding the complexity of $\\mathbf{B}\\mathbf{V}^{(1)}(B,C_{n})$ in the future. We first define the metric entropy of a metric space $(\\mathbb{T},\\rho)$ ", "page_idx": 21}, {"type": "text", "text": "Definition D.3. For a set $\\mathbb{T}$ with a corresponding metric $\\rho(\\cdot,\\cdot)$ , let $N(\\epsilon,\\mathbb{T},\\rho)$ denote the $\\epsilon$ -covering number of $\\mathbb{T}$ under metric $\\rho$ . Then the metric entropy of $\\mathbb{T}$ with respect to $\\rho$ is $\\log N(\\epsilon,\\mathbb{T},\\rho)$ ", "page_idx": 21}, {"type": "text", "text": "More details and examples about covering and metric entropy can be found in Chapter 5 of Wainwright [2019]. Next we bound the metric entropy of a bounded subset of $B V(1)$ . Note that the $\\ell_{\\infty}$ metric over domain $\\Omega$ is $\\rho_{\\infty}(f,g)=\\operatorname*{sup}_{x\\in\\Omega}|{\\bar{f}}(x)-g(x)|$ , which we denote by $\\|\\cdot\\|_{\\infty}$ for short. ", "page_idx": 21}, {"type": "text", "text": "Lemma D.4.Assume the set $\\begin{array}{r}{\\mathbb{T}_{1}=\\bigg\\{f:[-1,1]\\rightarrow\\mathbb{R}\\;\\bigg|\\;\\int_{-1}^{1}|f^{\\prime\\prime}(x)|d x\\leq1,\\;|f(x)|\\leq1\\bigg\\}}\\end{array}$ and the metric is the $\\ell_{\\infty}$ distance $\\|\\cdot\\|_{\\infty}$ ,thenthereexistsauniversalconstant $C_{1}>0$ such that for any $\\epsilon>0$ the metric entropy of $\\lvert\\Dot{\\mathbb{T}}_{1},\\rvert\\rvert\\cdot\\lvert\\rvert_{\\infty})$ satisfies ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\log N(\\epsilon,\\mathbb{T}_{1},\\|\\cdot\\|_{\\infty})\\leq C_{1}\\epsilon^{-\\frac{1}{2}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof of Lemma D.4. First of all, the domain $\\Omega=[-1,1]$ is a bounded set in $\\mathbb{R}$ . Moreover, according to DeVore and Lorentz [1993], we have $B V(1)\\,\\subset\\,B_{1,\\infty}^{2}$ . Therefore, $\\mathbb{T}_{1}$ is a bounded subset of $B_{1,\\infty}^{2}(\\Omega)$ with both $B_{1,\\infty}^{2}$ and $\\ell_{\\infty}$ norm bounded by a universal constant. ", "page_idx": 22}, {"type": "text", "text": "Therefore, the metric space $\\left(\\mathbb{T}_{1},\\Vert\\cdot\\Vert_{\\infty}\\right)$ satisfies the assumptions in (the second point) of Corollary 2 in Nickl and Potscher [2007] with $r=\\infty,\\ d=1,\\ s=2$ . Then combining the conclusion of Corollary 2 in Nickl and Potscher [2007] and the fact that the metric entropy is upper bounded by bracketing metric entropy (Definition 4 in Nickl and Potscher [2007]), we finish the proof. \u53e3 Remark D.5. For our purpose of bounding the metric entropy of $\\mathrm{\\mathbf{B}V}(1)$ , we only consider the case where $\\Omega$ is bounded. For more results regarding the metric entropy of (weighted) Besov space, please refer to Nickl and Potscher [2007]. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "E  Calculation of Gradient and Hessian Matrix ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we calculate the gradient and Hessian matrix of $f_{\\boldsymbol{\\theta}}(\\boldsymbol{x})$ with respect to $\\theta$ .Recall that $\\begin{array}{r}{f_{\\theta}(x)\\ =\\ \\sum_{i=1}^{k}w_{i}^{(2)}\\phi\\left(w_{i}^{(1)}x+b_{i}^{(1)}\\right)\\,+\\,b^{(2)}}\\end{array}$ where $\\phi(x)~=~\\mathrm{max}\\{x,0\\}$ .We denote $\\boldsymbol{\\theta}=(w_{1}^{(1)},\\cdot\\cdot\\cdot\\,,w_{k}^{(1)},b_{1}^{(1)},\\cdot\\cdot\\cdot\\,,b_{k}^{(1)},w_{1}^{(2)},\\cdot\\cdot\\cdot\\,,w_{k}^{(2)},b^{(2)})^{T}$ ", "page_idx": 22}, {"type": "text", "text": "E.1  Calculation of Gradient ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "According to direct calculation, for a given $x\\in[-x_{\\mathrm{max}},x_{\\mathrm{max}}]$ wehave ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\nabla_{w_{i}^{(1)}}f_{\\theta}(x)=x w_{i}^{(2)}\\mathbb{1}\\left(w_{i}^{(1)}x+b_{i}^{(1)}>0\\right),\\ \\forall\\,i\\in[k]}\\\\ {\\nabla_{b_{i}^{(1)}}f_{\\theta}(x)=w_{i}^{(2)}\\mathbb{1}\\left(w_{i}^{(1)}x+b_{i}^{(1)}>0\\right),\\ \\forall\\,i\\in[k]}\\\\ {\\nabla_{w_{i}^{(2)}}f_{\\theta}(x)=\\phi\\left(w_{i}^{(1)}x+b_{i}^{(1)}\\right)=\\left(w_{i}^{(1)}x+b_{i}^{(1)}\\right)\\mathbb{1}\\left(w_{i}^{(1)}x+b_{i}^{(1)}>0\\right),\\ \\forall\\,i\\in[k]}\\\\ {\\nabla_{b^{(2)}}f_{\\theta}(x)=1}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "E.2 Calculation of the Hessian Matrix ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this part, we calculate $\\nabla_{\\theta}^{2}f_{\\theta}(x)$ for a given $x\\in[-x_{\\mathrm{max}},x_{\\mathrm{max}}]$ Below we alculate $\\frac{\\partial^{2}f_{\\theta}(\\boldsymbol{x})}{\\partial\\theta_{i}\\partial\\theta_{j}}$ $\\theta_{i}\\,=\\,b^{(2)}$ $\\theta_{j}\\:=\\:b^{(2)}$ \uff0c $\\frac{\\partial^{2}f_{\\theta}(x)}{\\partial\\theta_{i}\\partial\\theta_{j}}\\,=\\,0$ $\\frac{\\partial^{2}f_{\\theta}(x)}{\\partial\\theta_{i}\\partial\\theta_{j}^{\\prime}}$ where $i,j\\in[k]$ $\\theta,\\theta^{\\prime}\\in\\{w^{(1)},b^{(1)},w^{(2)}\\}$ $i\\neq j$ $\\frac{\\partial^{2}f_{\\theta}(x)}{\\partial\\theta_{i}\\partial\\theta_{j}^{\\prime}}=0$ Therefoe we nly calculatethecasewhen $j=i$ . Let $\\delta$ denote the Dirac function, it holds that: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\begin{array}{l}{\\frac{\\partial^{2}f_{\\theta}(x)}{\\partial w_{i}^{(1)}\\partial w_{i}^{(1)}}=w_{i}^{(2)}x^{2}\\delta\\left(w_{i}^{(1)}x+b_{i}^{(1)}\\right),\\ \\ \\forall\\ i\\in[k]}\\\\ {\\frac{\\partial^{2}f_{\\theta}(x)}{\\partial w_{i}^{(1)}\\partial b_{i}^{(1)}}=\\frac{\\partial^{2}f_{\\theta}(x)}{\\partial b_{i}^{(1)}\\partial w_{i}^{(1)}}=x w_{i}^{(2)}\\delta\\left(w_{i}^{(1)}x+b_{i}^{(1)}\\right),\\ \\ \\forall\\ i\\in[k]}\\\\ {\\frac{\\partial^{2}f_{\\theta}(x)}{\\partial b_{i}^{(1)}\\partial b_{i}^{(1)}}=w_{i}^{(2)}\\delta\\left(w_{i}^{(1)}x+b_{i}^{(1)}\\right),\\ \\ \\forall\\ i\\in[k]}\\end{array}\\right)}\\\\ &{\\left(\\begin{array}{l}{\\frac{\\partial^{2}f_{\\theta}(x)}{\\partial w_{i}^{(2)}\\partial w_{i}^{(2)}}=0,\\ \\ \\forall\\ i\\in[k]}\\\\ {\\frac{\\partial^{2}f_{\\theta}(x)}{\\partial w_{i}^{(1)}\\partial w_{i}^{(2)}}=\\frac{\\partial^{2}f_{\\theta}(x)}{\\partial w_{i}^{(2)}\\partial w_{i}^{(1)}}=x\\mathbb{I}\\left(w_{i}^{(1)}x+b_{i}^{(1)}>0\\right),\\ \\ \\forall\\ i\\in[k]}\\\\ {\\frac{\\partial^{2}f_{\\theta}(x)}{\\partial b_{i}^{(1)}\\partial w_{i}^{(2)}}=\\frac{\\partial^{2}f_{\\theta}(x)}{\\partial w_{i}^{(2)}\\partial b_{i}^{(1)}}=\\mathbb{I}\\left(w_{i}^{(1)}x+b_{i}^{(1)}>0\\right),\\ \\ \\forall\\ i\\in[k]}\\end{array}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The gradient is generally not well-defined according to the existence of the Dirac function. However, under the assumption that $f_{\\theta}$ is twice differentiable with respect to $\\theta$ (i.e. the knots of $f$ donot coincide with $x$ ), all the Dirac functions take the value O. In this case, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\frac{\\partial^{2}f_{\\theta}(x)}{\\partial w_{i}^{(1)}\\partial w_{i}^{(1)}}=0,\\ \\ \\forall\\,i\\in[k]}\\\\ {\\frac{\\partial^{2}f_{\\theta}(x)}{\\partial w_{i}^{(1)}\\partial b_{i}^{(1)}}=\\frac{\\partial^{2}f_{\\theta}(x)}{\\partial b_{i}^{(1)}\\partial w_{i}^{(1)}}=0,\\ \\ \\forall\\,i\\in[k]}\\\\ {\\frac{\\partial^{2}f_{\\theta}(x)}{\\partial b_{i}^{(1)}\\partial b_{i}^{(1)}}=0,\\ \\ \\forall\\,i\\in[k]}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "E.3Upper Bound of Operator Norm ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this part, we upper bound the operator norm of the Hessian matrix. Equivalently, we upper bound $|v^{T}\\nabla^{2}\\b{f}_{\\theta}(x)v|$ under the constraint that $\\|v\\|_{2}=1$ . We have the following lemma. ", "page_idx": 23}, {"type": "text", "text": "Lemma E.1. Assume that $f_{\\boldsymbol{\\theta}}(\\boldsymbol{x})$ is twice differentiable with respect to $\\theta$ and $x\\in[-x_{\\operatorname*{max}},x_{\\operatorname*{max}}],$ for any $v$ such that $\\|v\\|_{2}=1.$ it holds that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left|v^{T}\\nabla^{2}f_{\\theta}(x)v\\right|\\leq2\\operatorname*{max}\\{x_{\\operatorname*{max}},1\\}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof of Lemma E.1. Assume $v\\;=\\;\\left(\\alpha_{1},\\cdots,\\alpha_{k},\\beta_{1},\\cdots,\\beta_{k},\\gamma_{1},\\cdots,\\gamma_{k},\\iota\\right)^{T}\\;\\in\\;\\mathbb{R}^{3k+1}$ such that $\\begin{array}{r}{\\sum_{i=1}^{k}(\\alpha_{i}^{2}+\\beta_{i}^{2}+\\gamma_{i}^{2})+\\iota^{2}=1}\\end{array}$ Note that the Hessian matrix $\\nabla_{\\theta}^{2}f_{\\theta}(x)$ follows the structure: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}^{2}f_{\\theta}(x)={\\left(\\!\\!\\begin{array}{l l l l}{A_{w^{(1)}w^{(1)}}}&{A_{w^{(1)}b^{(1)}}}&{A_{w^{(1)}w^{(2)}}}&{A_{w^{(1)}b^{(2)}}}\\\\ {A_{b^{(1)}w^{(1)}}}&{A_{b^{(1)}b^{(1)}}}&{A_{b^{(1)}w^{(2)}}}&{A_{b^{(1)}b^{(2)}}}\\\\ {A_{w^{(2)}w^{(1)}}}&{A_{w^{(2)}b^{(1)}}}&{A_{w^{(2)}w^{(2)}}}&{A_{w^{(2)}b^{(2)}}}\\\\ {A_{b^{(2)}w^{(1)}}}&{A_{b^{(2)}b^{(1)}}}&{A_{b^{(2)}w^{(2)}}}&{A_{b^{(2)}b^{(2)}}}\\end{array}\\!\\!\\right)}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $A_{w^{(1)}w^{(1)}},A_{w^{(1)}b^{(1)}},A_{b^{(1)}w^{(1)}},A_{b^{(1)}b^{(1)}},A_{w^{(2)}w^{(2)}}\\in\\mathbb{R}^{k\\times k},$ $A_{w^{(1)}b^{(2)}},A_{b^{(1)}b^{(2)}},A_{w^{(2)}b^{(2)}}\\;\\in$ $\\mathbb{R}^{k\\times1}$ \uff0c $A_{b^{(2)}w^{(1)}},A_{b^{(2)}b^{(1)}},A_{b^{(2)}w^{(2)}}\\,\\in\\,\\mathbb{R}^{1\\times k}$ and $A_{b^{(2)}b^{(2)}}\\in\\mathbb{R}$ areallzero matrices.Meanwhile, $A_{w^{(1)}w^{(2)}}$ $\\mathit{\\Pi}_{:(2)}\\,,\\mathit{A}_{b^{(1)}w^{(2)}},\\mathit{A}_{w^{(2)}w^{(1)}},\\mathit{A}_{w^{(2)}b^{(1)}}\\in\\mathbb{R}^{k\\times k}$ are all diagonal matrices whose non-zero elements are between $[-\\operatorname*{max}\\{x_{\\operatorname*{max}},1\\},\\operatorname*{max}\\{x_{\\operatorname*{max}},1\\}].$ Therefore, it holds that: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|v^{T}\\nabla^{2}f_{\\theta}(x)v\\right|\\le2\\operatorname*{max}\\{x_{\\operatorname*{max}},1\\}\\displaystyle\\sum_{i=1}^{k}\\left(\\lvert\\alpha_{i}\\gamma_{i}\\rvert+\\lvert\\beta_{i}\\gamma_{i}\\rvert\\right)}\\\\ &{\\qquad\\qquad\\le2\\operatorname*{max}\\{x_{\\operatorname*{max}},1\\}\\left(\\sqrt{\\displaystyle\\sum_{i=1}^{k}\\alpha_{i}^{2}\\cdot\\displaystyle\\sum_{i=1}^{k}\\gamma_{i}^{2}}+\\sqrt{\\displaystyle\\sum_{i=1}^{k}\\beta_{i}^{2}\\cdot\\displaystyle\\sum_{i=1}^{k}\\gamma_{i}^{2}}\\right)}\\\\ &{\\qquad\\qquad\\le2\\operatorname*{max}\\{x_{\\operatorname*{max}},1\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the second inequality holds because of Cauchy-Schwarz inequality. The last inequality results from $x(1-x)\\leq\\frac{1}{4}$ \u53e3 ", "page_idx": 23}, {"type": "text", "text": "F Proof for the Counter-Example (Theorem 3.1) ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Theorem F.1 (Restate Theorem 3.1). For the counter-example, with probability $1-\\delta$ forany interpolatingfunction $f$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\int_{-x_{\\mathrm{max}}}^{x_{\\mathrm{max}}}|f^{\\prime\\prime}(x)|g(x)d x=\\Omega\\left(\\sigma n\\left[n-24\\log\\left(\\frac{1}{\\delta}\\right)\\right]\\right),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the randomness comes from the noises $\\{\\epsilon_{i}\\}$ . Under this high-probability event, when $n\\geq$ $\\Omega\\left(\\sqrt{\\frac{1}{\\sigma\\eta}}\\log\\left(\\frac{1}{\\delta}\\right)\\right)$ any stable solution $f$ for $G D$ with step size $\\eta$ will not interpolate the data, i.e. ", "page_idx": 23}, {"type": "equation", "text": "$$\n{\\mathcal{F}}(\\eta,0,{\\mathcal{D}})\\cap\\{f\\mid f(x_{i})=y_{i},\\,\\forall\\,i\\in[n]\\}=\\emptyset.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof of Theorem $F.l$ . Consider any three consecutive points $x_{j},x_{j+1},x_{j+2}$ where $j\\in[n-2]$ , note that their corresponding $y$ 's are $y_{j},y_{j+1},y_{j+2}$ which are ii.d. Gaussian random variables ${\\mathcal{N}}(0,\\sigma^{2})$ Then according to Mean Value Theorem, there exists $a\\in[x_{j},x_{j+1}]$ and $b\\in[x_{j+1},x_{j+2}]$ such that ", "page_idx": 24}, {"type": "equation", "text": "$$\nf^{\\prime}(a)={\\frac{y_{j+1}-y_{j}}{x_{j+1}-x_{j}}}={\\frac{n-1}{2x_{\\operatorname*{max}}}}(y_{j+1}-y_{j}),\\quad\\;f^{\\prime}(b)={\\frac{y_{j+2}-y_{j+1}}{x_{j+2}-x_{j+1}}}={\\frac{n-1}{2x_{\\operatorname*{max}}}}(y_{j+2}-y_{j+1}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, it holds that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\int_{x_{i}}^{x_{j+2}}|f^{\\prime\\prime}(x)|d x\\geq|f^{\\prime}(b)-f^{\\prime}(a)|=\\frac{n-1}{2x_{\\operatorname*{max}}}|y_{j+2}-2y_{j+1}+y_{j}|\\sim\\frac{n-1}{2x_{\\operatorname*{max}}}\\cdot|N(0,6\\sigma^{2})|,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the last equation means that $y_{j+2}-2y_{j+1}+y_{j}$ follows the distribution $\\mathcal{N}(0,6\\sigma^{2})$ ", "page_idx": 24}, {"type": "text", "text": "We focus on the interval in the middle. For any $x\\in[x_{n/4},x_{3n/4}]$ we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}^{2}(X<x)\\geq\\displaystyle\\frac{1}{16},\\quad\\mathbb{E}[x-X|X<x]\\geq\\displaystyle\\frac{x_{\\operatorname*{max}}}{4},}\\\\ &{\\mathbb{P}^{2}(X>x)\\geq\\displaystyle\\frac{1}{16},\\quad\\mathbb{E}[X-x|X>x]\\geq\\displaystyle\\frac{x_{\\operatorname*{max}}}{4}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Together with the definition of $g(x)$ (6), we have for any $\\begin{array}{r}{x\\in[x_{n/4},x_{3n/4}],g(x)\\geq\\frac{x_{\\operatorname*{max}}}{64}}\\end{array}$ Therefore, for any interpolating solutions $f$ , it holds that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\int_{-x_{\\operatorname*{max}}}^{x_{\\operatorname*{max}}}|f^{\\prime\\prime}(x)|g(x)d x\\geq\\frac{x_{\\operatorname*{max}}}{64}\\int_{x_{n/4}}^{x_{3n/4}}|f^{\\prime\\prime}(x)|d x\\geq\\frac{x_{\\operatorname*{max}}}{64}\\cdot\\frac{n-1}{2x_{\\operatorname*{max}}}\\sum_{i=1}^{n/6}|G_{i}|,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $G_{i}$ 's are i.i.d samples from $\\mathcal{N}(0,6\\sigma^{2})$ ", "page_idx": 24}, {"type": "text", "text": "Assume the median of $|\\mathcal{N}(0,1)|$ is $c>0$ , which is a universal constant. For any $i\\in[\\frac{n}{6}]$ ,define ", "page_idx": 24}, {"type": "equation", "text": "$$\nH_{i}=\\left\\{\\begin{array}{l l}{\\sqrt{6}c\\sigma,\\ }&{\\mathrm{if}\\ |G_{i}|\\geq\\sqrt{6}c\\sigma,}\\\\ {0,\\ }&{\\mathrm{otherwise}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then we have $H_{i}={\\sqrt{6}}c\\sigma$ with probability $\\frac{1}{2}$ . In addition, $|G_{i}|\\geq H_{i}$ . According to Lemma K.1, with probability at least $1-\\delta$ \uff0c ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\int_{-x_{\\mathrm{max}}}^{x_{\\mathrm{max}}}\\left|f^{\\prime\\prime}(x)\\right|g(x)d x\\ge\\frac{n-1}{128}\\sum_{i=1}^{n/6}H_{i}\\ge\\frac{n-1}{128}\\cdot\\sqrt{6}c\\sigma\\cdot\\left(\\frac{n}{24}-\\log\\left(\\frac{1}{\\delta}\\right)\\right)=c^{\\prime}\\sigma n\\left(n-24\\log\\left(\\frac{1}{\\delta}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for some universal constant $c^{\\prime}$ ", "page_idx": 24}, {"type": "text", "text": "Together with the conclusion in Theorem C.2, for any interpolating and stable solution $f$ ,with probability $1-\\delta$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{1}{\\eta}-\\frac{1}{2}\\geq c^{\\prime}\\sigma n\\left(n-24\\log\\left(\\frac{1}{\\delta}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Which does not hold when $\\begin{array}{r}{n\\geq\\Omega\\left(\\sqrt{\\frac{1}{\\sigma\\eta}}\\log\\left(\\frac{1}{\\delta}\\right)\\right)}\\end{array}$ ", "page_idx": 24}, {"type": "text", "text": "G  Proof for the $\\mathrm{TV}^{(1)}$ Bound (Theorem 4.1) ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We begin by calculating the gradient of empirical loss $\\mathcal{L}$ at $\\theta$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}\\mathcal{L}(\\theta)=\\frac{1}{n}\\sum_{i=1}^{n}(f_{\\theta}(x_{i})-y_{i})\\nabla_{\\theta}f_{\\theta}(x_{i}),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the detailed calculation of $\\nabla_{\\theta}f_{\\theta}(x)$ can be found in Appendix E. Then the Hessian is given by ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}^{2}\\mathcal{L}(\\theta)=\\frac{1}{n}\\sum_{i=1}^{n}(\\nabla_{\\theta}f_{\\theta}(x_{i}))(\\nabla_{\\theta}f_{\\theta}(x_{i}))^{T}+\\frac{1}{n}\\sum_{i=1}^{n}(f_{\\theta}(x_{i})-y_{i})\\nabla_{\\theta}^{2}f(x_{i}),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\nabla_{\\theta}^{2}f(x)$ is calculated in Appendix E. Let $v$ denote the unit eigenvector $(\\|v\\|_{2}\\;=\\;1)$ of $\\begin{array}{r}{\\frac{1}{n}\\sum_{i=1}^{n}(\\dot{\\nabla}_{\\theta}f_{\\theta}(x_{i}))(\\nabla_{\\theta}f_{\\theta}(x_{i}))^{T}}\\end{array}$ with respect to the largest eigenvalue, it holds that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\lambda_{\\operatorname*{max}}(\\nabla_{\\theta}^{2}G(\\theta))\\geq v^{T}\\nabla_{\\theta}^{2}\\mathcal{L}(\\theta)v}\\\\ &{=\\underbrace{\\lambda_{\\operatorname*{max}}\\left(\\frac{1}{n}\\underbrace{\\sum_{i=1}^{n}(\\nabla_{\\theta}f_{\\theta}(x_{i}))(\\nabla_{\\theta}f_{\\theta}(x_{i}))^{T}}_{(*)}\\right)}_{(*)}+\\underbrace{\\frac{1}{n}\\sum_{i=1}^{n}(f_{\\theta}(x_{i})-y_{i})v^{T}\\nabla_{\\theta}^{2}f(x_{i})v}_{(*)}}\\\\ &{=\\underbrace{\\lambda_{\\operatorname*{max}}\\left(\\frac{1}{n}\\underbrace{\\sum_{i=1}^{n}(\\nabla_{\\theta}f_{\\theta}(x_{i}))(\\nabla_{\\theta}f_{\\theta}(x_{i}))^{T}}_{(*)}\\right)}_{(*)}+\\underbrace{\\frac{1}{n}\\underbrace{\\sum_{i=1}^{n}(f_{\\theta}(x_{i})-y_{i})v^{T}\\nabla_{\\theta}^{2}f(x_{i})v}_{(*)}}_{(*)}}\\\\ &{\\quad+\\underbrace{\\frac{1}{n}\\underbrace{\\sum_{i=1}^{n}(f_{\\theta}(x_{i})-f_{0}(x_{i}))v^{T}\\nabla_{\\theta}^{2}f(x_{i})v}_{(*)}}_{(*)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For term $(\\star)$ , we connect the maximal eigenvalue at $\\theta$ to the (weighted) $\\mathrm{TV}^{(1)}$ norm of the corresponding $f=f_{\\theta}$ . Let the weight function $g(x)$ be defined as (6), then the lemma below holds. ", "page_idx": 25}, {"type": "text", "text": "Lemma G.1. Assume $\\mathcal{L}$ is twice differentiable at $\\theta$ and the corresponding function of $\\theta$ is $f$ then ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{max}}\\left(\\frac{1}{n}\\sum_{i=1}^{n}(\\nabla_{\\theta}f_{\\theta}(x_{i}))(\\nabla_{\\theta}f_{\\theta}(x_{i}))^{T}\\right)\\geq1+2\\int_{-x_{\\operatorname*{max}}}^{x_{\\operatorname*{max}}}|f^{\\prime\\prime}(x)|g(x)d x,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $g(x)$ is defined as (6). ", "page_idx": 25}, {"type": "text", "text": "Proof of Lemma G.1. The proof of Lemma 4 in Mulayoff et al. [2021] directly proves the result for $\\begin{array}{r}{\\lambda_{\\operatorname*{max}}\\left(\\frac{1}{n}\\sum_{i=1}^{n}(\\nabla_{\\theta}f_{\\theta}(x_{i}))\\mathring(\\nabla_{\\theta}f_{\\theta}(x_{i}))^{T}\\right)}\\end{array}$ ,which is the $\\lambda_{\\operatorname*{max}}(\\nabla_{\\theta}^{2}\\mathcal{L})$ in Lemma 4 of Mulayoff et al. [2021] when $f$ is an interpolating solution. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "For the first inequality of Theorem 4.1, we directly handle the term $(\\#)$ as below. ", "page_idx": 25}, {"type": "equation", "text": "$$\n|(\\#)|\\leq\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(f_{\\theta}(x_{i})-y_{i})^{2}}\\cdot\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(v^{T}\\nabla_{\\theta}^{2}f(x_{i})v)^{2}}\\leq2x_{\\operatorname*{max}}\\sqrt{2\\mathcal{L}(\\theta)},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the first inequality results from Cauchy-Schwarz inequality. The second inequality is because of the uniform upper bound of $v^{T}\\nabla_{\\theta}^{2}f(x_{i})v$ (Lemma E.1). ", "page_idx": 25}, {"type": "text", "text": "For the second inequality of Theorem 4.1, we bound the two terms (i) and (i). We begin with $\\begin{array}{r}{|(i)|=|\\frac{1}{n}\\sum_{i=1}^{n}v^{T}\\overleftarrow{\\nabla_{\\theta}^{2}}f(\\bar{x}_{i})v\\cdot\\epsilon_{i}|}\\end{array}$ which is a weighted sum of noises $\\{\\epsilon_{i}\\}$ ", "page_idx": 25}, {"type": "text", "text": "Lemma G.2. Assume $\\epsilon_{i}$ 's are independently sampled from ${\\mathcal{N}}(0,\\sigma^{2})$ for some $\\sigma>0$ with probability at least $1-\\delta$ uniformly over all $\\theta,v$ such that $\\mathcal{L}$ is twice differentiable at $\\theta$ and $\\|\\boldsymbol{v}\\|_{2}=1$ \uff0c ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left\\vert\\frac{1}{n}\\sum_{i=1}^{n}v^{T}\\nabla_{\\theta}^{2}f(x_{i})v\\cdot\\epsilon_{i}\\right\\vert\\leq\\sigma\\operatorname*{max}\\{x_{\\operatorname*{max}},1\\}\\cdot\\operatorname*{min}\\left\\{4\\sqrt{\\log\\left(\\frac{4n}{\\delta}\\right)},14\\sqrt{\\frac{k\\log\\left(\\frac{13n}{\\delta}\\right)}{n}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof of Lemma G.2. For the first part, according to Lemma E.1, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left|\\frac{1}{n}\\sum_{i=1}^{n}v^{T}\\nabla_{\\theta}^{2}f(x_{i})v\\cdot\\epsilon_{i}\\right|\\leq2\\operatorname*{max}\\{x_{\\operatorname*{max}},1\\}\\cdot\\operatorname*{max}_{i}\\{|\\epsilon_{i}|\\}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Since $\\epsilon_{i}$ 's are independently sampled from Gaussian distribution ${\\mathcal{N}}(0,\\sigma^{2})$ , according to concentration of Gaussian distribution and a union bound, with probability $\\textstyle1-{\\frac{\\delta}{2}}$ ,it holds that: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i}\\{|\\epsilon_{i}|\\}\\leq2\\sigma\\sqrt{\\log\\left(\\frac{4n}{\\delta}\\right)}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Under this high-probability event, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n|(i)|=\\left|\\frac{1}{n}\\sum_{i=1}^{n}v^{T}\\nabla_{\\theta}^{2}f(x_{i})v\\cdot\\epsilon_{i}\\right|\\leq4\\sigma\\operatorname*{max}\\{x_{\\mathrm{max}},1\\}\\sqrt{\\log\\left(\\frac{4n}{\\delta}\\right)}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "For the second part, we bound the complexity of $\\left\\{v^{T}\\nabla_{\\theta_{.}}^{2}f(x_{i})v\\right\\}_{i=1}^{n}$ Notice that $\\theta$ is a function of the dataset, thus not independent to $\\epsilon_{i}$ . Also, $v$ is a function of the dataset and $\\theta$ . Our strategy is to apply an $\\epsilon$ -net argument for both $v$ and $\\nabla_{\\theta}^{2}f(x_{i})$ for $i=1,\\cdots,n$ ", "page_idx": 26}, {"type": "text", "text": "We begin  with considering the possibilities of $\\{\\nabla_{\\theta}^{2}f(x_{i})\\}_{i=1}^{n}$ : According to the detailed form  of $\\nabla_{\\theta}^{2}f_{\\theta}(x)$ in Appendix E, we have the set $\\{\\nabla_{\\theta}^{2}f(x_{i})\\}_{i=1}^{n}$ is fully  determined by $\\left\\{\\mathbb{1}\\left(w_{j}^{(1)}x_{i}+b_{j}^{(1)}>0\\right)\\right\\}_{i,j=1,1}^{n,k}$ Therefore it suffices to cover all the possibilities of $\\left\\{\\mathbb{1}\\left(w_{j}^{(1)}x_{i}+b_{j}^{(1)}>0\\right)\\right\\}_{i=1}^{n}$ for al $j~\\in~[k]$ Wthout lsfal wana $x_{1}\\,<\\,x_{2}\\,<\\,\\cdot\\,\\cdot\\,<\\,x_{n}$ , and then $\\left\\{w_{j}^{(1)}x_{i}+b_{j}^{(1)}\\right\\}_{i=1}^{n}$ is also monotonic, which implies that there $2(n+1)$ $\\left\\{\\mathbb{1}\\left(w_{j}^{(1)}x_{i}+b_{j}^{(1)}>0\\right)\\right\\}_{i=1}^{n}$ in total As aresult te produetspace $\\left\\{\\mathbb{1}\\left(w_{j}^{(1)}x_{i}+b_{j}^{(1)}>0\\right)\\right\\}_{i,j=1,1}^{n,k}$ (and also $\\{\\nabla_{\\theta}^{2}f(x_{i})\\}_{i=1}^{n})$ has $N_{1}=(2n+2)^{k}$ possibilities. For a fixed matrix $M=\\nabla_{\\theta}^{2}f(x)$ for some $\\theta,x$ and $v,v^{\\prime}$ such that $\\|v\\|_{2}\\leq1,\\|v^{\\prime}\\|_{2}\\leq1,\\|v-v^{\\prime}\\|_{2}\\leq\\epsilon$ with $\\epsilon\\in(0,1)$ . it holds that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left|v^{T}M v-(v^{\\prime})^{T}M v^{\\prime}\\right|\\leq2\\left|(v-v^{\\prime})^{T}M v^{\\prime}\\right|+\\left|(v-v^{\\prime})^{T}M(v-v^{\\prime})\\right|}&{}\\\\ {\\leq\\!2\\|v-v^{\\prime}\\|_{2}\\|M\\|_{2}\\|v^{\\prime}\\|_{2}+\\|v-v^{\\prime}\\|_{2}\\|M\\|_{2}\\|v-v^{\\prime}\\|_{2}}&{}\\\\ {\\leq\\!4\\operatorname*{max}\\{x_{\\operatorname*{max}},1\\}\\epsilon+2\\operatorname*{max}\\{x_{\\operatorname*{max}},1\\}\\epsilon^{2}}&{}\\\\ {\\leq\\!6\\operatorname*{max}\\{x_{\\operatorname*{max}},1\\}\\epsilon,}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the third inequality results from the upper bound of operator norm of $M$ (Lemma E.1). Therefore, the exact covern set of $\\{\\nabla_{\\theta}^{2}f(x_{i})\\}_{i=1}^{n}$ andan $\\frac{\\epsilon}{6\\operatorname*{max}\\{x_{\\operatorname*{max}},1\\}}$ covering st of the unit Euclidean Ball with dimension $3k+1$ (which is exactly the domain of ) together provides an $\\epsilon$ cover of $\\left\\{\\left(v^{T}\\nabla_{\\theta}^{2}f(x_{i})v\\right)_{i=1}^{n}\\right\\}$ with respect to $\\|\\cdot\\|_{\\infty}$ $\\frac{\\epsilon}{6\\operatorname*{max}\\{x_{\\operatorname*{max}},1\\}}$ -cover with respect to $\\Vert\\cdot\\Vert_{2}$ of the unit Euclidean Ball with dimension $3k+1$ has cardinality bounded by $\\begin{array}{r}{N_{2}=\\left(1+\\frac{12\\operatorname*{max}\\{x_{\\operatorname*{max}},1\\}}{\\epsilon}\\right)^{3k+1}}\\end{array}$ according to Lemma K.2. Combining the two covering arguments, the $\\epsilon$ -covering set of $\\left\\{\\left(v^{T}\\nabla_{\\theta}^{2}f(x_{i})v\\right)_{i=1}^{n}\\right\\}$ with respect to $\\|\\cdot\\|_{\\infty}$ has cardinality $N_{\\epsilon}$ satisfying: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\log N_{\\epsilon}\\leq\\log N_{1}+\\log N_{2}\\leq4k\\log\\left(\\frac{13n\\operatorname*{max}\\{x_{\\operatorname*{max}},1\\}}{\\epsilon}\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Consider a fixed stream $(a_{i})_{i=1}^{n}$ in the covering set, we have $|a_{i}|\\leq2\\operatorname*{max}\\{x_{\\operatorname*{max}},1\\}$ for all $i\\in[n]$ Then according to the concentration result of Gaussian distribution, with probability $1-\\delta$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left|\\frac{1}{n}\\sum_{i=1}^{n}a_{i}\\epsilon_{i}\\right|\\leq4\\sigma\\operatorname*{max}\\{x_{\\mathrm{max}},1\\}\\sqrt{\\frac{\\log(2/\\delta)}{n}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "With a union bound over the $\\epsilon_{}$ -covering set of $\\left\\{\\left(v^{T}\\nabla_{\\theta}^{2}f(x_{i})v\\right)_{i=1}^{n}\\right\\}$ and conditioned on the high probability event of (55), with probability $1-\\delta$ , uniformly over all possible $\\|v\\|_{2}=1$ and $\\theta$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left\\lvert\\frac{1}{n}\\sum_{i=1}^{n}v^{T}\\nabla_{\\theta}^{2}f(x_{i})v\\cdot\\epsilon_{i}\\right\\rvert\\leq2\\sigma\\epsilon\\sqrt{\\log{\\left(\\frac{4n}{\\delta}\\right)}}+4\\sigma\\operatorname*{max}\\{x_{\\operatorname*{max}},1\\}\\sqrt{\\frac{\\log(4N_{\\epsilon}/\\delta)}{n}}}\\\\ {\\displaystyle\\leq2\\sigma\\epsilon\\sqrt{\\log{\\left(\\frac{4n}{\\delta}\\right)}}+4\\sigma\\operatorname*{max}\\{x_{\\operatorname*{max}},1\\}\\sqrt{\\frac{4k\\log{\\left(\\frac{13n\\operatorname*{max}\\{x_{\\operatorname*{max}},1\\}}{\\epsilon\\delta}\\right)}}{n}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Finally,by choosing $\\begin{array}{r}{\\epsilon=\\frac{\\operatorname*{max}\\{x_{\\operatorname*{max}},1\\}}{\\sqrt{n}}}\\end{array}$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left|\\frac{1}{n}\\sum_{i=1}^{n}v^{T}\\nabla_{\\theta}^{2}f(x_{i})v\\cdot\\epsilon_{i}\\right|\\leq14\\sigma\\operatorname*{max}\\{x_{\\mathrm{max}},1\\}\\sqrt{\\frac{k\\log\\left(\\frac{13n}{\\delta}\\right)}{n}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Remark G.3. According to the Lemma G.2 above, there are two cases. When the neural network is over-parameterized, we can derive a constant upper bound for term (i). Meanwhile, if the number of neurons is smaller than the sample complexity, we can derive a tighter bound for (i) by covering $\\left\\{v^{T}\\nabla_{\\theta}^{2}f(x_{i})v\\right\\}_{i=1}^{n}$ For $k,n$ such that $k$ is polynomially smaller than $n$ (e.g. $k=n^{1-\\alpha}$ for some positive $\\alpha$ ), the term (i) will vanish if $n$ converges to infinity. ", "page_idx": 27}, {"type": "text", "text": "Meanwhile, the term (i) can be bounded by the mean squared error $\\mathrm{MSE}(f_{\\theta})$ using Cauchy-Schwarz inequality and the uniform upper bound of $v^{T}\\nabla_{\\theta}^{2}f(x_{i}){\\dot{v}}$ (Lemma E.1). ", "page_idx": 27}, {"type": "equation", "text": "$$\n|(i i)|\\leq\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}\\left(f_{\\theta}(x_{i})-f_{0}(x_{i})\\right)^{2}}\\cdot\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}\\left(v^{T}\\nabla_{\\theta}^{2}f(x_{i})v\\right)^{2}}\\leq2\\operatorname*{max}\\{x_{\\mathrm{max}},1\\}\\sqrt{\\mathrm{MSE}(f_{\\theta})}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Combining the results above, we state the following weighted $\\mathrm{TV}^{(1)}$ bound of the learned function.   \nWe leave the training loss and mean squared error (MSE) in the bound, which will be handled later. ", "page_idx": 27}, {"type": "text", "text": "Theorem G.4 (Restate Theorem 4.1). For a function $f=f_{\\theta}$ where the training (square) loss $\\mathcal{L}$ is twice differentiable at $\\theta$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\int_{-x_{\\mathrm{max}}}^{x_{\\mathrm{max}}}|f^{\\prime\\prime}(x)|g(x)d x\\leq\\frac{\\lambda_{\\mathrm{max}}(\\nabla_{\\theta}^{2}\\mathcal{L}(\\theta))}{2}-\\frac{1}{2}+x_{\\mathrm{max}}\\sqrt{2\\mathcal{L}(\\theta)},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $g(x)$ is defined as (6). Moreover, $i f$ we assume $y_{i}\\,=\\,f_{0}(x_{i})+\\epsilon_{i}$ for independent noise $\\epsilon_{i}\\sim\\mathcal{N}(0,\\sigma^{2})$ ,then with probability $1-\\delta$ where the randomness is over the noises $\\{\\epsilon_{i}\\}$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\int_{-x_{\\operatorname*{max}}}^{x_{\\operatorname*{max}}}|f^{\\prime\\prime}(x)|g(x)d x\\leq\\frac{\\lambda_{\\operatorname*{max}}(\\nabla_{\\theta}^{2}\\mathcal{L}(\\theta))}{2}-\\frac{1}{2}+\\widetilde O\\left(\\sigma x_{\\operatorname*{max}}\\cdot\\operatorname*{min}\\left\\{1,\\sqrt{\\frac{k}{n}}\\right\\}\\right)+x_{\\operatorname*{max}}\\sqrt{\\operatorname{MSE}(f)}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "In addition, if $f=f_{\\theta}$ is $a$ stable solution of $G D$ with step size n on dataset $\\mathcal{D}$ i.e., $f_{\\theta}\\in\\mathcal{F}(\\eta,0,\\mathcal{D})$ as in (4),then we can replace max(VC(0) with $\\begin{array}{r}{\\frac{1}{\\eta}}\\end{array}$ in (9) and (10). ", "page_idx": 27}, {"type": "text", "text": "Proof of Theorem G.4. The first inequality results from plugging Lemma G.1 and (52) into (50). The second inequality holds by plugging Lemma G.1, Lemma G.2 and inequality (62) into (50). For the instantiation of $\\dot{\\lambda}_{\\mathrm{max}}(\\nabla_{\\theta}^{2}\\dot{\\mathcal{L}}(\\theta))$ , the replacement holds due to the definition of $\\mathcal{F}(\\eta,0,\\mathcal{D})$ in (4).\u53e3 ", "page_idx": 27}, {"type": "text", "text": "G.1  A Crude Bound for MSE and the Instantiated $\\mathrm{TV}^{(1)}$ Bound (Corollary 4.2) ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Now we prove a crude upper bound for MSE, which could instantiate a weighted $\\mathrm{TV}^{(1)}$ bound. Lemma G.5. Assume that the function $f$ is optimized, then with probability $1-\\delta$ wehave ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname{MSE}(f)={\\frac{1}{n}}\\sum_{i=1}^{n}\\left(f(x_{i})-f_{0}(x_{i})\\right)^{2}\\leq16\\sigma^{2}\\log\\left({\\frac{2n}{\\delta}}\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof of Lemma G.5. According to the assumption that $f$ is optimized, it holds that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\left(f(x_{i})-y_{i}\\right)^{2}\\leq\\sum_{i=1}^{n}{\\big(}f_{0}(x_{i})-y_{i}{\\big)}^{2}=\\sum_{i=1}^{n}\\epsilon_{i}^{2}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then since $(a+b)^{2}\\leq2a^{2}+2b^{2}$ always holds (AM-GM inequality), we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\left(f(x_{i})-f_{0}(x_{i})\\right)^{2}=\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\left(f(x_{i})-y_{i}+y_{i}-f_{0}(x_{i})\\right)^{2}}\\\\ {\\displaystyle\\leq\\!\\frac{2}{n}\\left[\\sum_{i=1}^{n}\\left(f(x_{i})-y_{i}\\right)^{2}+\\displaystyle\\sum_{i=1}^{n}\\left(f_{0}(x_{i})-y_{i}\\right)^{2}\\right]}\\\\ {\\displaystyle\\leq\\!\\frac{4}{n}\\sum_{i=1}^{n}\\epsilon_{i}^{2}\\leq4\\operatorname*{max}_{i}\\epsilon_{i}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Recall that $\\epsilon_{i}$ 's are independently sampled from ${\\mathcal{N}}(0,\\sigma^{2})$ , then according to the concentration of Gaussian distribution and a union bound, with probability $1-\\delta$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i}\\epsilon_{i}^{2}\\leq4\\sigma^{2}\\log\\left(\\frac{2n}{\\delta}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Combining the two results, with probability $1-\\delta$ where the randomness is over the noises $\\{\\epsilon_{i}\\}$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname{MSE}(f)={\\frac{1}{n}}\\sum_{i=1}^{n}\\left(f(x_{i})-f_{0}(x_{i})\\right)^{2}\\leq16\\sigma^{2}\\log\\left({\\frac{2n}{\\delta}}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Finally, Corollary 4.2 results from directly plugging Lemma G.5 into Theorem 4.1. ", "page_idx": 28}, {"type": "text", "text": "G.2  An Improved MSE Bound and the Corresponding $\\mathrm{TV}^{(1)}$ Bound ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In this part, we provide an improved upper bound of the mean squared error $\\mathrm{MSE}(f_{\\theta})$ and also the term (ii). We first make the assumption below that the parameters are from a bounded space. ", "page_idx": 28}, {"type": "text", "text": "Assumption G.6. There exists some constant $\\rho>0$ such that gradient descent converges to some local minimum $\\theta$ with $\\|\\theta\\|_{\\infty}\\leq\\rho$ . In addition, we assume that $\\|f_{0}\\|_{\\infty}\\leq D$ where $D>0$ is some universal constant satisfying that $D\\leq k\\rho^{2}(x_{\\mathrm{max}}+1)$ ", "page_idx": 28}, {"type": "text", "text": "Assumption G.6 ensures that the parameter space is bounded while the ground truth function $f_{0}$ can be approximated well by some possible output function. The assumption will surely hold for some large enough constants $\\rho,D$ , which is without loss of generality. In the following analysis, we will replace $D$ with its upper bound $k\\rho^{2}(x_{\\mathrm{max}}+1)$ to reduce the parameters in the logarithmic terms. To handle the mean squared error $\\mathrm{MSE}(f_{\\theta})$ , we begin with an analysis on the complexity of the function class of two-layer ReLU networks with bounded parameters $\\begin{array}{r l}{\\mathcal{F}_{\\rho}}&{{}=}\\end{array}$ $\\begin{array}{r}{\\left\\{f:[-x_{\\operatorname*{max}},x_{\\operatorname*{max}}]\\to\\mathbb{R}\\ \\middle|\\ f(x)=\\sum_{i=1}^{\\cdot}w_{i}^{(2)}\\phi\\left(w_{i}^{(1)}x+b_{i}^{(1)}\\right)+b^{(2)},\\|\\theta\\|_{\\infty}\\stackrel{\\cdot}{\\leq}\\rho\\right\\}}\\end{array}$ Note that here we assume that the input is from $[-x_{\\mathrm{max}},x_{\\mathrm{max}}]$ and there exists an upper bound $\\rho\\:>\\:0$ on the parameter 0 = (wi,\u00b7 , wk\u201d,b $\\begin{array}{r l}&{\\theta=(w_{1}^{(1)},\\cdot\\cdot\\cdot,w_{k}^{(1)},b_{1}^{(\\bar{1})},\\cdot\\cdot\\cdot,b_{k}^{(1)},w_{1}^{(2)},\\cdot\\cdot\\cdot,w_{k}^{(2)},b^{(2)})^{T}\\in\\mathbb{R}^{3k+1}.}\\end{array}$ ", "page_idx": 28}, {"type": "text", "text": "Lemma G.7. The e-covering number $N_{\\epsilon}$ of function class ${\\mathcal F}_{\\rho}$ with respect to $\\|\\cdot\\|_{\\infty}$ satisfies ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\log N_{\\epsilon}\\leq4k\\log\\left(\\frac{11\\operatorname*{max}\\{x_{\\operatorname*{max}},1\\}k\\rho^{2}}{\\epsilon}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof of Lemma G.7. We consider the discrete function class below ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{\\mathcal{F}}_{\\bar{\\epsilon}}=\\left\\{f:[-x_{\\mathrm{max}},x_{\\mathrm{max}}]\\rightarrow\\mathbb{R}\\,\\bigg|\\,\\begin{array}{c}{f(x){=}\\sum_{i=1}^{k}\\bar{w}_{i}^{(2)}\\phi\\Big(\\bar{w}_{i}^{(1)}x{+}\\bar{b}_{i}^{(1)}\\Big){+}\\bar{b}^{(2)}}\\\\ {\\mathrm{~s.t.~}\\theta_{j}\\in\\bar{\\epsilon}\\cdot\\mathbb{Z}\\cap[-\\rho,\\rho],\\,\\forall\\,j\\in[3k+1]}\\end{array}\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Wwhere $\\theta_{j}$ isthe $j$ th elementof $(\\bar{w}_{1}^{(1)},\\cdot\\cdot\\cdot,\\bar{w}_{k}^{(1)},\\bar{b}_{1}^{(1)},\\cdot\\cdot\\cdot\\cdot,\\bar{b}_{k}^{(1)},\\bar{w}_{1}^{(2)},\\cdot\\cdot\\cdot\\cdot,\\bar{w}_{k}^{(2)},\\bar{b}^{(2)})^{T}\\in\\mathbb{R}^{3k+1}$ and $\\bar{\\epsilon}\\cdot\\mathbb{Z}$ is the set $\\{\\bar{\\epsilon}\\cdot i,\\ i\\in\\mathbb{Z}\\}$ . Since for each element, the number of choices is bounded by $\\begin{array}{r}{\\frac{2\\rho}{\\bar{\\epsilon}}+1}\\end{array}$ the total cardinality of $\\bar{\\mathcal{F}}_{\\bar{\\epsilon}}$ is bounded by $\\begin{array}{r}{\\left(\\frac{2\\rho}{\\bar{\\epsilon}}+1\\right)^{3k+1}}\\end{array}$ ", "page_idx": 28}, {"type": "text", "text": "For each function $f\\in\\mathcal{F}_{\\rho}$ with corresponding parameter $\\theta$ $(\\|\\theta\\|_{\\infty}\\le\\rho)$ , we choose function $\\bar{f}$ from $\\bar{\\mathcal{F}}_{\\bar{\\epsilon}}$ with corresponding parameter $\\bar{\\theta}$ such that $\\lvert\\bar{\\bar{\\theta}}_{j}-\\theta_{j}\\rvert$ is minimized for all $j\\in[3k+1]$ . According to our definition of $\\bar{\\mathcal{F}}_{\\bar{\\epsilon}}$ , we have for all $j\\in[3k+1]$ \uff0c $|\\bar{\\theta}_{j}-\\theta_{j}|\\leq\\bar{\\epsilon}$ . Therefore, it holds that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left\\lvert f-\\bar{f}\\right\\rvert\\|_{\\infty}\\leq\\sum_{i=1}^{k}\\left\\lVert w_{i}^{(2)}\\phi\\left(w_{i}^{(1)}x+b_{i}^{(1)}\\right)-\\bar{w}_{i}^{(2)}\\phi\\left(\\bar{w}_{i}^{(1)}x+\\bar{b}_{i}^{(1)}\\right)\\right\\rVert_{\\infty}+\\left\\lvert b^{(2)}-\\bar{b}^{(2)}\\right\\rvert}\\\\ {\\leq\\displaystyle\\sum_{i=1}^{k}\\left\\lVert w_{i}^{(2)}\\phi\\left(w_{i}^{(1)}x+b_{i}^{(1)}\\right)-\\bar{w}_{i}^{(2)}\\phi\\left(w_{i}^{(1)}x+b_{i}^{(1)}\\right)\\right\\rVert_{\\infty}}\\\\ {\\displaystyle\\qquad+\\sum_{i=1}^{k}\\left\\lVert\\bar{w}_{i}^{(2)}\\phi\\left(w_{i}^{(1)}x+b_{i}^{(1)}\\right)-\\bar{w}_{i}^{(2)}\\phi\\left(\\bar{w}_{i}^{(1)}x+\\bar{b}_{i}^{(1)}\\right)\\right\\rVert_{\\infty}+\\bar{\\epsilon}}\\\\ {\\displaystyle\\qquad\\leq k\\bar{\\epsilon}\\cdot\\rho(x_{\\operatorname*{max}}+1)+k\\rho(x_{\\operatorname*{max}}+1)\\bar{\\epsilon}+\\bar{\\epsilon}}\\\\ {\\displaystyle\\qquad\\leq5\\operatorname*{max}\\{x_{\\operatorname*{max}},1\\}k\\bar{\\epsilon}\\leq\\epsilon,}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the last inequalityis by choosing = 5max(max,l}ke ", "page_idx": 29}, {"type": "text", "text": "Therefore, the $\\epsilon$ -covering number $N_{\\epsilon}$ of function class ${\\mathcal{F}}_{\\rho}$ with respect to $\\|\\cdot\\|_{\\infty}$ satisfies ", "page_idx": 29}, {"type": "equation", "text": "$$\nN_{\\epsilon}\\leq\\left(1+\\frac{10\\operatorname*{max}\\{x_{\\operatorname*{max}},1\\}k\\rho^{2}}{\\epsilon}\\right)^{3k+1},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "which implies that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\log N_{\\epsilon}\\leq4k\\log\\left(\\frac{11\\operatorname*{max}\\{x_{\\operatorname*{max}},1\\}k\\rho^{2}}{\\epsilon}\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Now we provide an upper bound of $\\mathrm{MSE}(f_{\\theta})$ under the assumption that $\\theta$ is optimized, which means that the empirical error of $f_{\\theta}$ is smaller than that of $f_{0}$ , i.e. ", "page_idx": 29}, {"type": "equation", "text": "$$\n{\\frac{1}{2n}}\\sum_{i=1}^{n}\\left(f_{\\theta}(x_{i})-y_{i}\\right)^{2}\\leq{\\frac{1}{2n}}\\sum_{i=1}^{n}\\left(f_{0}(x_{i})-y_{i}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Below we state the improved upper bound of mean squared error under such assumptions. ", "page_idx": 29}, {"type": "text", "text": "Lemma G.8. Assume $\\epsilon_{i}$ 's are independently sampled from ${\\mathcal{N}}(0,\\sigma^{2})$ for some $\\sigma>0$ ifAssumption $G.6$ holds and $f_{\\theta}\\in\\mathcal{F}_{\\rho}$ is optimized, then with probability at least $1-\\delta$ it holds that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{MSE}(f_{\\theta})\\le O\\left(\\frac{\\sigma^{2}k\\log\\big(\\frac{\\operatorname*{max}\\{x_{\\mathrm{max}},1\\}k n\\rho}{\\delta}\\big)}{n}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof of Lemma G.8. Since $\\begin{array}{r}{\\frac{1}{2n}\\sum_{i=1}^{n}\\left(f_{\\theta}(x_{i})-y_{i}\\right)^{2}\\leq\\frac{1}{2n}\\sum_{i=1}^{n}\\left(f_{0}(x_{i})-y_{i}\\right)^{2}}\\end{array}$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\mathrm{MSE}(f_{\\theta})=\\frac{1}{2n}\\sum_{i=1}^{n}\\left(f_{\\theta}(x_{i})-f_{0}(x_{i})\\right)^{2}\\leq\\frac{1}{n}\\sum_{i=1}^{n}\\epsilon_{i}\\left(f_{\\theta}(x_{i})-f_{0}(x_{i})\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "For the optimized function $f_{\\theta}$ , we choose a function $\\bar{f}$ from the $\\epsilon$ -covering set in Lemma G.7 such that $\\|\\bar{f}-f_{\\theta}\\|_{\\infty}\\leq\\epsilon$ Due to identical analysis as (55), with probability $1\\!-\\!\\frac{\\delta}{2}$ $\\begin{array}{r}{,\\operatorname*{max}_{i}\\{|\\epsilon_{i}|\\}\\leq2\\sigma\\sqrt{\\log\\left(\\frac{4n}{\\delta}\\right)}}\\end{array}$ Under this high-probability event, it holds that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{i=1}^{n}\\epsilon_{i}\\left(f_{\\theta}(x_{i})-f_{0}(x_{i})\\right)\\leq\\frac{1}{n}\\sum_{i=1}^{n}\\epsilon_{i}\\left(\\bar{f}(x_{i})-f_{0}(x_{i})\\right)+2\\sigma\\epsilon\\sqrt{\\log\\left(\\frac{4n}{\\delta}\\right)}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "According to Lemma G.7, the $\\epsilon$ -covering number $N_{\\epsilon}$ of function class ${\\mathcal F}_{\\rho}$ with respect to $\\|\\cdot\\|_{\\infty}$ satisfies $\\begin{array}{r}{\\log N_{\\epsilon}\\leq4k\\log\\left(\\frac{11\\operatorname*{max}\\{x_{\\operatorname*{max}},1\\}k\\rho^{2}}{\\epsilon}\\right)}\\end{array}$ (1l maxmax\uff09.Due to theconcentrationofGaussiandistribtion and ", "page_idx": 29}, {"type": "text", "text": "a union bound over the covering set, with probability $\\textstyle1-{\\frac{\\delta}{2}}$ , for all $\\bar{f}$ in the covering set, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\epsilon_{i}\\left(\\bar{f}(x_{i})-f_{0}(x_{i})\\right)\\leq2\\sigma\\sqrt{\\sum_{i=1}^{n}\\left(\\bar{f}(x_{i})-f_{0}(x_{i})\\right)^{2}\\cdot\\log{\\left(\\frac{4N_{\\epsilon}}{\\delta}\\right)}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Combining the two high-probability events, with probability $1-\\delta$ wehave ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{2}\\operatorname{BSL}[\\rho_{\\xi}]\\leq\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\varepsilon_{i}(f(x_{i})-f_{n}(x_{i}))+2\\pi\\nu_{\\uparrow}\\Big\\{h\\alpha\\Big(\\frac{4n}{\\xi}\\Big)}\\\\ &{\\leq\\frac{2\\pi}{n}\\sqrt{\\frac{\\sum_{i=1}^{n}\\big(f(x_{i})-f_{n}(x_{i})\\big)^{2}}{\\varepsilon_{i}}-\\frac{1}{n}\\big(\\frac{\\ln\\big(X_{\\xi}\\big)}{\\xi}\\big)+2\\pi\\nu_{\\uparrow}\\Big\\lvert\\log\\Big(\\frac{4n}{\\xi}\\Big)}}\\\\ &{\\leq\\frac{2\\pi}{n}\\sqrt{\\frac{\\sum_{i=1}^{n}\\big(f(x_{i})-f_{n}(x_{i})\\big)^{2}}{\\varepsilon_{i}}-\\frac{4\\pi\\nu_{\\uparrow}}{n}\\big(\\frac{1}{n}\\ln\\big(\\frac{1-\\ln\\big(X_{\\xi}\\big)-1}{\\xi_{i}}\\big)+2\\pi\\nu_{\\uparrow}\\Big\\lvert\\log\\Big(\\frac{4n}{\\xi}\\Big)}}\\\\ &{\\leq\\frac{2\\pi}{n}\\sqrt{\\sum_{i=1}^{n}\\big(f(x_{i})-f_{n}(x_{i})\\big)^{2}+10\\ln\\mu^{2}\\operatorname*{max}(x_{n-1},1)\\big\\rangle}\\cdot4\\hbar\\nu_{\\downarrow}\\Big(\\frac{11\\operatorname*{max}\\{x_{i},1\\}\\varepsilon_{i}^{2}}{\\varepsilon_{i}}\\Big)}\\\\ &{\\quad+2\\pi\\nu_{\\uparrow}\\sqrt{\\log\\Big(\\frac{4n}{\\xi}\\Big)}}\\\\ &{\\leq O\\left(\\nu\\|\\operatorname*{Bet}{\\frac{(n\\alpha(x_{i},1)\\varepsilon_{j})}{\\varepsilon_{i}}-\\frac{15\\pi\\mathsf{E}(f_{n})}{n})}\\cdot\\frac{18\\mathbf{E}(f_{n})}{n}\\right)+O\\left(\\nu h\\rho\\sqrt{\\operatorname*{max}(x_{i},1)+\\log\\Big(\\frac{8\\pi\\nu_{\\downarrow}}{n}\\Big)}\\right.}\\\\ &{\\quad+O\\left(\\nu\\sqrt{\\log\\Big(\\frac{4n}{\\xi}\\Big)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the forth inequalityrslts from Asumtion G.6. Seleting $\\begin{array}{r}{\\epsilon\\,=\\,\\frac{1}{k^{2}\\rho^{2}n^{2}\\operatorname*{max}\\left\\{x_{\\mathrm{max}},1\\right\\}}}\\end{array}$ and solving the second order inequality, it holds that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{MSE}(f_{\\theta})\\le O\\left(\\frac{\\sigma^{2}k\\log\\big(\\frac{\\operatorname*{max}\\{x_{\\mathrm{max}},1\\}k n\\rho}{\\delta}\\big)}{n}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Plugging in the upper bound for MSE (Lemma G.8) to Theorem 4.1, we have the corollary below. ", "page_idx": 30}, {"type": "text", "text": "Corollary G.9 (Improved version of Corollary 4.2). For a stable solution $f=f_{\\theta}$ off $G D$ with step size n where $\\mathcal{L}$ is twice differentiable at $\\theta$ assumeAssumption $G.6$ holds and $f\\in\\mathcal{F}_{\\rho}$ isoptimized, withprobability $1-\\delta$ the function $f$ satisfies ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\int_{-x_{\\operatorname*{max}}}^{x_{\\operatorname*{max}}}|f^{\\prime\\prime}(x)|g(x)d x\\leq\\frac{1}{\\eta}-\\frac{1}{2}+O\\left(\\sigma x_{\\operatorname*{max}}\\sqrt{\\frac{k\\log\\left(\\frac{\\operatorname*{max}\\{x_{\\operatorname*{max}},1\\}k n\\rho}{\\delta}\\right)}{n}}\\right),\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the randomness is over the noises $\\{\\epsilon_{i}\\}$ and $g(x)$ is defined as (6). ", "page_idx": 30}, {"type": "text", "text": "Remark G.10. The additional term here $\\tilde{O}\\left(\\sigma x_{\\operatorname*{max}}\\sqrt{\\frac{k}{n}}\\right)$ improves over the constant additional term in Corollary 4.2 if $k<n$ . In addition, if $\\textstyle{\\frac{n}{k}}$ converges to infinity (e.g. $k=n^{1-\\alpha}$ for some $\\alpha>0$ and $n$ is sufficiently large), the additional term could vanish. ", "page_idx": 30}, {"type": "text", "text": "H Proof for the Generalization Gap Bound (Theorem 4.3) ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Recall that the generalization gap of function $f$ is defined as ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\operatorname{Gen}(f):=\\left|\\mathbb{E}\\left[\\left(f(x)-y\\right)^{2}\\right]-\\frac{1}{n}\\sum_{i=1}^{n}\\left(f(x_{i})-y_{i}\\right)^{2}\\right|,\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $(x,y)$ is a new sample from the data distribution. The generalization gap measures the difference of the expected testing error and the training loss, and a small generalization gap implies that the model is not overfitting. ", "page_idx": 31}, {"type": "text", "text": "For a stable solution $f=f_{\\theta}$ of GD with step size $\\eta$ where $\\mathcal{L}$ is twice differentiable at $\\theta$ , we first derive a corresponding analysis for the weighted $\\mathrm{TV}^{(1)}$ bound. Recall that the empirical loss is still defined as $\\begin{array}{r}{\\mathcal{L}(\\dot{f})=\\frac{1}{2n}^{*}\\!\\sum_{i=1}^{n}\\!\\!\\!\\!\\!\\!(f(x_{i})-y_{i})^{2}}\\end{array}$ . Then using the same calculation as (50), we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac2\\eta\\ge\\lambda_{\\operatorname*{max}}(\\nabla_{\\theta}^{2}\\mathcal{L}(\\theta))\\ge v^{T}\\nabla_{\\theta}^{2}\\mathcal{L}(\\theta)v}\\\\ &{\\quad=\\lambda_{\\operatorname*{max}}\\left(\\displaystyle\\frac1n\\sum_{i=1}^{n}(\\nabla_{\\theta}f_{\\theta}(x_{i}))(\\nabla_{\\theta}f_{\\theta}(x_{i}))^{T}\\right)+\\displaystyle\\frac1n\\sum_{i=1}^{n}(f_{\\theta}(x_{i})-y_{i})v^{T}\\nabla_{\\theta}^{2}f(x_{i})v}\\\\ &{\\quad\\ge\\underbrace{\\lambda_{\\operatorname*{max}}\\left(\\displaystyle\\frac1n\\sum_{i=1}^{n}(\\nabla_{\\theta}f_{\\theta}(x_{i}))(\\nabla_{\\theta}f_{\\theta}(x_{i}))^{T}\\right)}_{(\\star)}-\\underbrace{\\sqrt{\\displaystyle\\frac1n\\sum_{i=1}^{n}(f_{\\theta}(x_{i})-y_{i})^{2}}\\cdot\\sqrt{\\displaystyle\\frac1n\\sum_{i=1}^{n}(v^{T}\\nabla_{\\theta}^{2}f(x_{i})v)^{2}}}_{(\\#)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where the last inequality results from Cauchy-Schwarz inequality. ", "page_idx": 31}, {"type": "text", "text": "According to Lemma G.1, the term $(\\star)$ satisfies ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\left(\\star\\right)=\\lambda_{\\operatorname*{max}}\\left(\\frac{1}{n}\\sum_{i=1}^{n}(\\nabla_{\\theta}f_{\\theta}(x_{i}))(\\nabla_{\\theta}f_{\\theta}(x_{i}))^{T}\\right)\\geq1+2\\int_{-x_{\\operatorname*{max}}}^{x_{\\operatorname*{max}}}|f^{\\prime\\prime}(x)|g(x)d x.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "In addition, the term $(\\#)$ satisfies (w.l.o.g, we assume $x_{\\operatorname*{max}}\\geq1$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n|(\\#)|\\leq2x_{\\operatorname*{max}}\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(f_{\\theta}(x_{i})-y_{i})^{2}}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Under the assumption that the learned function $f=f_{\\theta}$ satisfies $\\|f\\|_{\\infty}\\leq D$ and $|y_{i}|\\leq D$ for all $i\\in[n]$ , it further implies that ", "page_idx": 31}, {"type": "equation", "text": "$$\n|(\\#)|\\leq4x_{\\operatorname*{max}}D.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Combining the results, the following $\\mathrm{TV}^{(1)}$ bound holds. ", "page_idx": 31}, {"type": "text", "text": "Lemma H.1. Assume the data distribution satisfies that for all possible $(x,y)$ fromthedistribution, $|x|\\leq x_{\\mathrm{max}}$ and $\\vert y\\vert\\le D$ if the function $f=f_{\\theta}$ isastablesolution of $G D$ with step size $\\eta$ such that $\\mathcal{L}$ istwice differentiableat $\\theta$ and $\\|f\\|_{\\infty}\\leq D$ then it holds that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\int_{-x_{\\mathrm{max}}}^{x_{\\mathrm{max}}}|f^{\\prime\\prime}(x)|g(x)d x\\leq\\frac{1}{\\eta}-\\frac{1}{2}+2x_{\\mathrm{max}}D,\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $g(x)$ is defined as (6). ", "page_idx": 31}, {"type": "text", "text": "Note that we assume there exists some interval $\\mathcal{T}\\subset[-x_{\\mathrm{max}},x_{\\mathrm{max}}]$ and a universal constant $c>0$ such that with probability $1-\\delta/2$ (randomness over the dataset), $g(x)\\geq c$ for all $x\\in\\mathcal{Z}$ (w.1.o.g. we assume $c<1$ ), which further implies that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\int_{\\mathbb{Z}}|f^{\\prime\\prime}(x)|d x\\leq\\frac{1}{c}\\left(\\frac{1}{\\eta}-\\frac{1}{2}+2x_{\\mathrm{max}}D\\right).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We base on the high-probability event above in the following discussions. Next we bound the metric entropy of the possible output function class. ", "page_idx": 31}, {"type": "text", "text": "Lemma H.2. Define the set $\\begin{array}{r}{\\mathbb{T}_{3}=\\Big\\{f:\\mathcal{T}\\rightarrow\\mathbb{R}\\;\\big|\\;\\|f\\|_{\\infty}\\leq D,\\;\\int_{\\mathcal{T}}|f^{\\prime\\prime}(x)|d x\\leq\\frac{1}{c}\\left(\\frac{1}{\\eta}-\\frac{1}{2}+2x_{\\operatorname*{max}}D\\right)\\Big\\},}\\end{array}$ then the metric entropy with respect to $\\|\\cdot\\|_{\\infty}$ satisfies that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\log N(\\epsilon,\\mathbb{T}_{3},\\|\\cdot\\|_{\\infty})\\leq O\\left(\\sqrt{\\frac{x_{\\operatorname*{max}}\\left(\\frac{1}{\\eta}-\\frac{1}{2}+2x_{\\operatorname*{max}}D\\right)}{\\epsilon}}\\right),\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $O$ also absorbs the constant c. ", "page_idx": 32}, {"type": "text", "text": "Proof of Lemma $H.2$ . Define the set $\\mathbb{T}_{4}$ as: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{T}_{4}=\\left\\{f:[-x_{\\operatorname*{max}},x_{\\operatorname*{max}}]\\to\\mathbb{R}\\ |\\ \\|f\\|_{\\infty}\\leq D,\\ \\int_{-x_{\\operatorname*{max}}}^{x_{\\operatorname*{max}}}|f^{\\prime\\prime}(x)|d x\\leq\\frac{1}{c}\\left(\\frac{1}{\\eta}-\\frac{1}{2}+2x_{\\operatorname*{max}}D\\right)\\right\\}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Note that the metric entropy of $\\mathbb{T}_{3}$ is bounded by that of $\\mathbb{T}_{4}$ , therefore we directly prove the upper bound of $\\log N(\\epsilon,\\mathbb{T}_{4},\\|\\cdot\\|_{\\infty})$ ", "page_idx": 32}, {"type": "text", "text": "Let the set $\\begin{array}{r}{\\mathbb{T}_{1}=\\bigg\\{f:[-1,1]\\rightarrow\\mathbb{R}\\bigg|\\;\\int_{-1}^{1}|f^{\\prime\\prime}(x)|d x\\leq1,\\;|f(x)|\\leq1\\bigg\\}}\\end{array}$ (as in Lemma D.4). For a fixed $\\epsilon>0$ according to Lemma D.4, there exists a $\\frac{\\epsilon}{\\frac{1}{c}\\left(\\frac{1}{\\eta}-\\frac{1}{2}\\!+\\!2x_{\\mathrm{max}}D\\right)x_{\\mathrm{max}}}$ -covering set of $\\mathbb{T}_{1}$ wich respect to $\\|\\cdot\\|_{\\infty}$ , denoted as $\\{h_{i}(x)\\}_{i\\in[N]}$ , whose cardinality $N$ satisfies ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\log N\\leq C_{1}{\\sqrt{\\frac{{\\frac{1}{c}}\\left({\\frac{1}{\\eta}}-{\\frac{1}{2}}+2x_{\\operatorname*{max}}D\\right)x_{\\operatorname*{max}}}{\\epsilon}}}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We define $\\begin{array}{r}{g_{i}(x)=\\frac{1}{c}\\left(\\frac{1}{\\eta}-\\frac{1}{2}+2x_{\\mathrm{max}}D\\right)x_{\\mathrm{max}}h_{i}(\\frac{x}{x_{\\mathrm{max}}})}\\end{array}$ forall $i\\in[N]$ Then $g_{i}$ 's are al defined on $[-x_{\\mathrm{max}},x_{\\mathrm{max}}]$ . Obviously, we have $\\{g_{i}(x)\\}_{i\\in[N]}$ also has cardinality $N$ ", "page_idx": 32}, {"type": "text", "text": "Forany $f(x)\\in\\mathbb{T}_{4}$ .we deine $\\begin{array}{r}{g(x)=\\frac{1}{\\frac{1}{c}\\left(\\frac{1}{\\eta}-\\frac{1}{2}+2x_{\\mathrm{max}}D\\right)x_{\\mathrm{max}}}f(x\\cdot x_{\\mathrm{max}})}\\end{array}$ which s defned on $[-1,1]$ We now show that $g(x)\\in\\mathbb{T}_{1}$ . First of all, for any $x\\in[-x_{\\mathrm{max}},x_{\\mathrm{max}}]$ , we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n|g(x)|\\leq\\frac{D}{\\frac{1}{c}\\left(\\frac{1}{\\eta}-\\frac{1}{2}+2x_{\\mathrm{max}}D\\right)x_{\\mathrm{max}}}<1.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Meanwhile, it holds that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\int_{-1}^{1}|g^{\\prime\\prime}(x)|d x=\\int_{-1}^{1}\\frac{1}{\\frac{1}{c}\\left(\\frac{1}{\\eta}-\\frac{1}{2}+2x_{\\mathrm{max}}D\\right)x_{\\mathrm{max}}}\\cdot x_{\\mathrm{max}}^{2}|f^{\\prime\\prime}(x\\cdot x_{\\mathrm{max}})|d x}\\\\ {\\displaystyle\\leq\\frac{1}{\\frac{1}{c}\\left(\\frac{1}{\\eta}-\\frac{1}{2}+2x_{\\mathrm{max}}D\\right)}\\int_{-x_{\\mathrm{max}}}^{x_{\\mathrm{max}}}|f^{\\prime\\prime}(x)|d x\\leq1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Combining the two results, we have $g\\ \\in\\ \\mathbb{T}_{1}$ .Therefore, there exists some $h_{i}$ such that $\\parallel g\\mathrm{~-~}$ $\\begin{array}{r l}&{\\nu_{i}\\|_{\\infty}\\leq\\frac{\\epsilon}{\\frac{1}{c}\\left(\\frac{1}{\\eta}-\\frac{1}{2}+2x_{\\mathrm{max}}D\\right)x_{\\mathrm{max}}},\\ \\mathrm{Since}\\ f(x)=\\frac{1}{c}\\left(\\frac{1}{\\eta}-\\frac{1}{2}+2x_{\\mathrm{max}}D\\right)x_{\\mathrm{max}}g(\\frac{x}{x_{\\mathrm{max}}}),\\|g_{i}-f\\|_{\\infty}=}\\\\ &{\\ z\\left(\\frac{1}{\\eta}-\\frac{1}{2}+2x_{\\mathrm{max}}D\\right)x_{\\mathrm{max}}\\|h_{i}-g\\|_{\\infty}\\leq\\epsilon.}\\end{array}$ ", "page_idx": 32}, {"type": "text", "text": "In conclusion, $\\{g_{i}\\}_{i\\in[N]}$ is an $\\epsilon$ -covering of $\\mathbb{T}_{4}$ with respect to $\\|\\cdot\\|_{\\infty}$ . Moreover, the cardinality of $\\{g_{i}\\}_{i\\in[N]}$ is $N$ , which finishes the proof. \u53e3 ", "page_idx": 32}, {"type": "text", "text": "Now we provide our main result about the generalization gap (restricted to $\\mathcal{T}$ ). Below we define the generalization gap restricted to $\\mathcal{T}$ ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\operatorname{Gen}_{\\mathbb{Z}}(f):=\\left|\\mathbb{E}_{\\mathbb{Z}}\\left[\\left(f(x)-y\\right)^{2}\\right]-\\frac{1}{n_{\\mathbb{Z}}}\\sum_{x_{i}\\in\\mathbb{Z}}\\left(f(x_{i})-y_{i}\\right)^{2}\\right|,\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $\\mathbb{E}_{\\mathcal{L}}$ means that $(x,y)$ is a new sample from the data distribution conditioned on $x\\in\\mathcal{T}$ and $n_{\\mathcal{L}}$ is the number of data points in $\\mathcal{D}$ such that $x_{i}\\in\\mathbb{Z}$ ", "page_idx": 32}, {"type": "text", "text": "Theorem H.3 (Restate Theorem 4.3). Let $\\mathcal{P}$ be a joint distribution of $(x,y)$ supported on $[-x_{\\mathrm{max}},x_{\\mathrm{max}}]\\,\\stackrel{.}{\\times}\\,[-D,D]$ .Assume the dataset $\\mathcal{D}\\stackrel{{}}{\\sim}\\mathcal{P}^{n}$ i.i.d. For any fixed interval $\\mathcal{Z}\\subset$ $\\left\\vert-x_{\\operatorname*{max}},x_{\\operatorname*{max}}\\right\\vert$ and a universal constant $c~>~0$ such that with probability $\\begin{array}{r}{\\dot{1}\\,-\\,\\delta/2,\\;g(x)\\,\\geq\\,c}\\end{array}$ for all $x\\in\\mathcal{T}$ .if the function $f\\,=\\,f_{\\theta}$ is a stable solution of $G D$ with step size $\\eta$ such that $\\mathcal{L}$ is twice differentiable at $\\theta$ and $\\|f\\|_{\\infty}\\overset{\\cdot}{\\leq}D$ ,with probability $1-\\delta$ (randomness over the dataset), the generalization gaprestricted to $\\mathcal{T}$ satisfies ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathsf{2e n}_{\\mathbb{Z}}(f):=\\left|\\mathbb{E}_{\\mathbb{Z}}\\left[\\left(f(x)-y\\right)^{2}\\right]-\\frac{1}{n_{\\mathbb{Z}}}\\sum_{x_{i}\\in\\mathbb{Z}}\\left(f(x_{i})-y_{i}\\right)^{2}\\right|\\leq\\tilde{O}\\left(D^{\\frac{9}{8}}\\left[\\frac{x_{\\operatorname*{max}}\\left(\\frac{1}{\\eta}-\\frac{1}{2}+2x_{\\operatorname*{max}}D\\right)}{n_{\\mathbb{Z}}^{2}}\\right]^{\\frac{1}{5}}\\right),\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $\\mathbb{E}_{\\mathcal{L}}$ means that $(x,y)$ is a new samplefrom the data distribution conditioned on $x\\in\\mathcal{Z}$ and $n_{\\mathcal{L}}$ is the number of data points in $\\mathcal{D}$ suchthat $x_{i}\\in\\mathcal{T}$ ", "page_idx": 33}, {"type": "text", "text": "Proof of Theorem H.3. We base on the following event that holds with probability $1-\\delta/2$ ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\int_{\\mathbb{Z}}|f^{\\prime\\prime}(x)|d x\\leq\\frac{1}{c}\\left(\\frac{1}{\\eta}-\\frac{1}{2}+2x_{\\mathrm{max}}D\\right).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Then the output function $f\\in\\mathbb{T}_{3}$ defined in Lemma H.2. ", "page_idx": 33}, {"type": "text", "text": "For a fixed $\\epsilon>0$ , according to Lemma H.2, there exists an $\\epsilon$ -covering set of $\\mathbb{T}_{3}$ (with respect to $\\|\\cdot\\|_{\\infty})$ whose cardinality $N$ satisfies that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\log N\\leq O\\left(\\sqrt{\\frac{x_{\\operatorname*{max}}\\left(\\frac{1}{\\eta}-\\frac{1}{2}+2x_{\\operatorname*{max}}D\\right)}{\\epsilon}}\\right).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "For a fixed function $\\bar{f}$ in the covering set, since the data set $\\{(x_{i},y_{i})\\}_{x_{i}\\in\\mathbb{Z}}$ is still i.d. from the data distribution conditioned on $x\\in\\mathcal{Z}$ , Hoeffding's inequality (Lemma K.3) implies that with probability $1-\\delta$ , it holds that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}_{\\mathbb{Z}}\\left[\\left(\\bar{f}(x)-y\\right)^{2}\\right]-\\frac{1}{n_{\\mathbb{Z}}}\\sum_{x_{i}\\in\\mathbb{Z}}\\left(\\bar{f}(x_{i})-y_{i}\\right)^{2}\\right|\\leq4D^{2}\\cdot\\sqrt{\\frac{\\log(2/\\delta)}{n_{\\mathbb{Z}}}}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Together with a union bound over the covering set, we have with probability $1-\\delta/2$ , for all $\\bar{f}$ in the covering set, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\mathbb{E}_{\\mathcal{Z}}\\left[\\left(\\bar{f}(x)-y\\right)^{2}\\right]-\\frac{1}{n_{\\mathcal{Z}}}\\sum_{x_{i}\\in\\mathcal{Z}}\\left(\\bar{f}(x_{i})-y_{i}\\right)^{2}\\right|\\le4D^{2}\\cdot\\sqrt{\\frac{\\log(4N/\\delta)}{n_{\\mathcal{Z}}}}}\\\\ &{\\le\\!O\\left(D^{2}\\cdot\\frac{\\left[x_{\\operatorname*{max}}\\left(\\frac{1}{\\eta}-\\frac{1}{2}+2x_{\\operatorname*{max}}D\\right)\\right]^{\\frac{1}{4}}\\log\\left(1/\\delta\\right)^{\\frac{1}{2}}}{n_{\\mathcal{Z}}^{\\frac{1}{2}}\\epsilon^{\\frac{1}{4}}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Under such high probability event, for any $f\\in\\mathbb{T}_{3}$ , let $\\bar{f}$ be a function in the covering set such that $\\|f-\\bar{f}\\|_{\\infty}\\leq\\bar{\\epsilon}$ . Then it holds that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\mathbb{E}_{\\mathbb{Z}}\\left[(f(x)-y)^{2}\\right]-\\frac{1}{n\\chi}\\sum_{i\\in\\mathbb{Z}}\\left(f(x_{i})-y_{i}\\right)^{2}\\right|}\\\\ &{\\leq\\left|\\mathbb{E}_{\\mathbb{Z}}\\left[(\\bar{f}(x)-y)^{2}\\right]-\\frac{1}{n\\chi}\\sum_{i=1}^{n}\\frac{\\left(\\bar{f}_{i}(x_{i})-y_{i}\\right)^{2}}{n\\chi}\\right|+O(D\\epsilon)}\\\\ &{\\leq O(D\\epsilon)+O\\left(D^{2}\\cdot\\frac{\\left[x_{\\operatorname*{max}}\\left(\\frac{1}{\\eta}-\\frac{1}{2}+2x_{\\operatorname*{max}}D\\right)\\right]^{\\frac{1}{4}}\\log(1/\\delta)^{\\frac{1}{2}}}{n\\frac{1}{L}\\epsilon^{\\frac{1}{4}}}\\right)}\\\\ &{\\leq O\\left(D^{5}\\left[\\frac{x_{\\operatorname*{max}}\\left(\\frac{1}{\\eta}-\\frac{1}{2}+2x_{\\operatorname*{max}}D\\right)\\log(1/\\delta)^{2}}{n\\frac{1}{L^{2}}}\\right]^{\\frac{1}{8}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "H.1 Choice of the Interval Under Uniform Distribution ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "In this part, we discuss the choice of the interval $\\mathcal{T}$ under the case that the marginal distribution of $x$ is the uniform distribution on $[-x_{\\mathrm{max}},x_{\\mathrm{max}}]$ . For simplicity, we assume that $x_{\\operatorname*{max}}=1$ ", "page_idx": 34}, {"type": "text", "text": "Lemma H.4. Assume that $x\\sim\\operatorname{Unif}([-1,1])$ , then we can choose $\\mathcal{T}$ to be $[-\\textstyle{\\frac{2}{3}},\\,\\textstyle{\\frac{2}{3}}]$ . In this way, with probability $1-24e^{-\\frac{n}{96}}$ , for any $x\\in{\\mathcal{Z}}$ it holds that ", "page_idx": 34}, {"type": "equation", "text": "$$\ng(x)\\geq{\\frac{1}{4320}}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "As a result, when $\\begin{array}{r}{n\\geq96\\log\\left(\\frac{48}{\\delta}\\right)}\\end{array}$ , we can choose $\\mathcal{T}=[-\\frac{2}{3},\\frac{2}{3}]$ and $\\begin{array}{r}{c=\\frac{1}{4320}}\\end{array}$ ", "page_idx": 34}, {"type": "text", "text": "Proof of Lemma H.4. Let the intervals $A_{i}$ be defined as below: for all $i\\in[12]$ ", "page_idx": 34}, {"type": "equation", "text": "$$\nA_{i}=\\left[\\frac{i-7}{6},\\frac{i-6}{6}\\right].\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "For a fixed $n$ , let $P_{i}$ denote the number of data points in $A_{i}$ , which follows Binomial distribution with $\\textstyle p={\\frac{1}{12}}$ . Then for a fixed $i\\in[12]$ , according to Multiplicative Chernoff bound (Lemma K.4), with probability $1-2e^{-\\frac{n}{96}}$ , it holds that ", "page_idx": 34}, {"type": "equation", "text": "$$\n{\\frac{1}{2}}\\cdot{\\frac{n}{12}}\\leq P_{i}\\leq2\\cdot{\\frac{n}{12}}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Then according to a union bound, with probability $1-24e^{-{\\frac{n}{96}}}$ , the above inequality holds for all $i\\in[12]$ . Under the case above, we prove that $\\begin{array}{r}{g(x)\\overset{\\bullet}{=}\\operatorname*{min}\\{g^{-}(x),g^{+}(x)\\}\\ge\\frac{\\ddagger}{4320}}\\end{array}$ for all $x\\in\\mathcal{T}$ ", "page_idx": 34}, {"type": "text", "text": "Recall that $g^{-}(x)=\\mathbb{P}^{2}(X<x)\\mathbb{E}[x-X|X<x]\\sqrt{1+(\\mathbb{E}[X|X<x])^{2}}$ where $X$ is drawn from the empirical distribution of the data (a sample chosen uniformly from $\\{x_{j}\\},$ . Then for any $x\\geq-{\\frac{2}{3}}$ ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}^{2}(X<x)\\ge\\left(\\displaystyle\\frac{P_{1}+P_{2}}{n}\\right)^{2}\\ge\\frac{1}{144},}\\\\ &{\\mathbb{E}[x-X|X<x]\\ge\\frac{\\frac{n}{24}\\cdot\\frac{1}{6}}{\\frac{n}{24}+\\frac{n}{6}}=\\frac{1}{30},}\\\\ &{\\sqrt{1+(\\mathbb{E}[X|X<x])^{2}}\\ge1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Combining thequalites, we ha $\\begin{array}{r}{g^{-}(x)\\geq\\frac{1}{4320}}\\end{array}$ forall $x\\in\\mathcal{Z}$ The result for $g^{+}(x)$ can be proven silaly $1-24e^{-\\frac{n}{96}}$ \uff0c $\\begin{array}{r}{g(x)\\geq\\frac{1}{4320}}\\end{array}$ for all $x\\in\\mathcal{Z}$ ", "page_idx": 34}, {"type": "text", "text": "The implication can be proven by direct calculation. ", "page_idx": 34}, {"type": "text", "text": "Remark H.5. We only consider the case where the data is sampled from uniform distribution, while we remark that for various distributions that are not heavy-tailed (e.g. Gaussian distribution, Laplace distribution), a similar result can be derived. Some empirical illustrations are shown in Appendix H.2 below. ", "page_idx": 34}, {"type": "text", "text": "H.2 More Mlustrations of the Choice of the Interval ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "In this part, we consider the choice of $\\mathcal{T}$ and $c$ under different data distributions. More specifically, we consider the following four distributions of $x$ ", "page_idx": 34}, {"type": "text", "text": "Uniform distribution: $x\\sim\\operatorname{Unif}([-1,1])$ ", "page_idx": 34}, {"type": "text", "text": "Normal distribution: $x\\sim\\mathcal{N}(0,1)$ ", "page_idx": 34}, {"type": "text", "text": "Laplace distribution: $x\\sim\\mathrm{Laplace}(0,1)$ ", "page_idx": 34}, {"type": "equation", "text": "$$\nx\\sim\\left\\{\\begin{array}{l l}{{\\mathcal{N}}(-0.5,0.25)}&{{\\mathrm{~with~probability~}}\\frac{1}{2},}\\\\ {{\\mathcal{N}}(0.5,0.25)}&{{\\mathrm{~with~probability~}}\\frac{1}{2}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "For each distribution, we sample $n=1000$ data points from the distribution (conditional on $x\\in$ $[-1,1])$ and construct the $g(x)$ function. Then we choose the interval $\\mathcal{T}$ and the corresponding lower bound $c$ of $g(x)$ over $\\mathcal{T}$ . From Figure 9, we find that for all of the four distributions, with a constant $c\\ge0.002$ , the interval $\\mathcal{T}$ can be chosen to incorporate a large portion of the data $(n\\tau\\ge0.65n)$ ", "page_idx": 34}, {"type": "image", "img_path": "7Swrtm9Qsp/tmp/b737069a11114ed63179faa89c26275bf12765812e97c6529cf484e3bcf853d6.jpg", "img_caption": ["Figure 9: Illustration of the choice of interval $\\mathcal{T}$ and the corresponding lower bound $c$ for $g(x)$ "], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "I Proof for the Refined MSE Bound (Theorem 4.4) ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "In this part, we base on the same conditions in Corollary 4.2, which is the weighted $\\mathrm{TV}^{(1)}$ upper bound. For an output stable solution $f$ satisfying the conclusion of Corollary 4.2, we have $\\begin{array}{r}{\\int_{-x_{\\mathrm{max}}}^{x_{\\mathrm{max}}}|f^{\\prime\\prime}(x)|g(x)d x\\overset{\\cdot}{\\leq}\\frac{1}{\\eta}-\\frac{1}{2}+\\widetilde{O}(\\sigma x_{\\mathrm{max}})}\\end{array}$ andwedete tergthadsi $S$ In adition, according to the assumption that $g(x)\\geq c$ for any $x\\in\\mathcal{Z}$ , we further have $\\begin{array}{r}{\\int_{\\mathcal{Z}}|f^{\\prime\\prime}(x)|d x\\leq\\frac{S}{c}.}\\end{array}$ Now we bound the complexity of the possible output function class. ", "page_idx": 35}, {"type": "text", "text": "According to the definition of two-layer ReLU network and our assumption that $\\|\\theta\\|_{\\infty}\\leq\\rho$ it holds that: ", "page_idx": 35}, {"type": "equation", "text": "$$\n|f(0)|=\\left|\\sum_{i=1}^{k}w_{i}^{(2)}\\phi\\left(b_{i}^{(1)}\\right)+b^{(2)}\\right|\\leq k\\rho^{2}+\\rho.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "equation", "text": "$$\n|f^{\\prime}(0)|\\leq\\sum_{i=1}^{k}\\left|w_{i}^{(2)}w_{i}^{(1)}\\right|\\leq k\\rho^{2}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Define the set $\\begin{array}{r}{\\mathbb{T}=\\big\\{f:\\!Z\\rightarrow\\mathbb{R}\\;\\big|\\;|f(0)|\\leq k\\rho^{2}+\\rho,\\;|f^{\\prime}(0)|\\leq k\\rho^{2},\\;\\int_{\\mathbb{Z}}|f^{\\prime\\prime}(x)|d x\\leq\\frac{S}{c}\\big\\}.}\\end{array}$ According to the inequalities above, the possible output function (if restricted to $\\mathcal{T}$ )belongsto $\\bar{\\mathbb T}$ Webegin with an analysis of the metric entropy of the intermediate function set $\\mathbb{T}_{2}$ ", "page_idx": 35}, {"type": "text", "text": "Lemma I.1. Assume the st $\\begin{array}{r}{\\mathbb{T}_{2}=\\left\\{f:[-x_{\\operatorname*{max}},x_{\\operatorname*{max}}]\\to\\mathbb{R}\\ |\\ f(0)=f^{\\prime}(0)=0,\\ \\int_{-x_{\\operatorname*{max}}}^{x_{\\operatorname*{max}}}|f^{\\prime\\prime}(x)|d x\\le C_{2}\\right\\}}\\end{array}$ for some constant $C_{2}>0$ , and the metric is $\\ell_{\\infty}$ distance $\\|\\cdot\\|_{\\infty}$ , then there exists a universal constant ", "page_idx": 35}, {"type": "text", "text": "$C_{1}>0$ such that for any $\\epsilon>0$ the metric entropy of $\\left(\\mathbb{T}_{2},\\Vert\\cdot\\Vert_{\\infty}\\right)$ satisfies ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\log N(\\epsilon,\\mathbb{T}_{2},\\|\\cdot\\|_{\\infty})\\leq C_{1}\\sqrt{\\frac{C_{2}x_{\\operatorname*{max}}}{\\epsilon}},\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $C_{1}$ can be chosen as the same $C_{1}$ inLemma $D.4$ ", "page_idx": 36}, {"type": "text", "text": "Proof of Lemma I.1. Let the set $\\begin{array}{r}{\\mathbb{T}_{1}=\\bigg\\{f:[-1,1]\\rightarrow\\mathbb{R}\\;\\bigg|\\;\\int_{-1}^{1}|f^{\\prime\\prime}(x)|d x\\leq1,\\;|f(x)|\\leq1\\bigg\\}}\\end{array}$ (as in Lemma D.4) Forafxd e >,according to LmmaD.4, there exists a -covering set of $\\mathbb{T}_{1}$ with respect to $\\|\\cdot\\|_{\\infty}$ , denoted as $\\{h_{i}(x)\\}_{i\\in[N]}$ , whose cardinality $N$ satisfies ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\log N\\leq C_{1}{\\sqrt{\\frac{C_{2}x_{\\operatorname*{max}}}{\\epsilon}}}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "We define $\\begin{array}{r}{g_{i}(x)=C_{2}x_{\\mathrm{max}}h_{i}\\big(\\frac{x}{x_{\\mathrm{max}}}\\big)}\\end{array}$ for all $i\\in[N]$ Then $g_{i}$ 's are ll defined on $[-x_{\\mathrm{max}},x_{\\mathrm{max}}]$ Obviously, we have $\\{g_{i}(x)\\}_{i\\in[N]}$ also has cardinality $N$ ", "page_idx": 36}, {"type": "text", "text": "For any $f(x)\\in\\mathbb{T}_{2}$ $\\begin{array}{r}{g(x)=\\frac{1}{C_{2}x_{\\mathrm{max}}}f(x\\cdot x_{\\mathrm{max}})}\\end{array}$ Wwhich sdefned on $[-1,1]$ .Wwe now show that $g(x)\\in\\mathbb{T}_{1}$ . First of all, for any $x\\in[-x_{\\mathrm{max}},x_{\\mathrm{max}}]$ , we have $\\begin{array}{r}{|f^{\\prime}(x)|\\leq\\int_{-x_{\\mathrm{max}}}^{x_{\\mathrm{max}}}|f^{\\prime\\prime}(x)|d x\\leq C_{2}}\\end{array}$ Therefore, for any $x\\in\\ [-x_{\\mathrm{max}},x_{\\mathrm{max}}]$ $|f(x)|\\leq C_{2}x_{\\mathrm{max}}$ , which implies that $|g(x)|\\leq1$ for any $x\\in[-1,1]$ . Meanwhile, it holds that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l l}{\\displaystyle\\int_{-1}^{1}|g^{\\prime\\prime}(x)|d x=\\displaystyle\\int_{-1}^{1}\\frac{1}{C_{2}x_{\\mathrm{max}}}\\cdot x_{\\mathrm{max}}^{2}|f^{\\prime\\prime}(x\\cdot x_{\\mathrm{max}})|d x}\\\\ {\\displaystyle\\leq\\frac{1}{C_{2}}\\int_{-x_{\\mathrm{max}}}^{x_{\\mathrm{max}}}|f^{\\prime\\prime}(x)|d x\\leq1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Combining the two results, we have $g\\in\\mathbb{T}_{1}$ . Therefore, there exists some $h_{i}$ such that $\\begin{array}{r}{\\|g-h_{i}\\|_{\\infty}\\leq}\\end{array}$ $\\frac{\\epsilon}{C_{2}x_{\\mathrm{max}}}$ Since $\\begin{array}{r}{f(x)=C_{2}x_{\\mathrm{max}}g\\big(\\frac{x}{x_{\\mathrm{max}}}\\big)}\\end{array}$ $\\lVert g_{i}-f\\rVert_{\\infty}=C_{2}x_{\\mathrm{max}}\\lVert h_{i}-g\\rVert_{\\infty}\\leq\\epsilon.$ ", "page_idx": 36}, {"type": "text", "text": "In conclusion, $\\{g_{i}\\}_{i\\in[N]}$ is an $\\epsilon$ -covering of $\\mathbb{T}_{2}$ with respect to $\\|\\cdot\\|_{\\infty}$ . Moreover, the cardinality of $\\{g_{i}\\}_{i\\in[N]}$ is $N$ , which finishes the proof. \u53e3 ", "page_idx": 36}, {"type": "text", "text": "With Lemma I.1, we are ready to bound the metric entropy of $\\mathbb{T}$ ", "page_idx": 36}, {"type": "text", "text": "Lemma I.2. Assume the metric is $\\ell_{\\infty}$ distance $\\|\\cdot\\|_{\\infty}$ , then the metric entropy of $(\\mathbb{T},\\|\\cdot\\|_{\\infty})$ satisfies ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\log N(\\epsilon,\\mathbb{T},\\|\\cdot\\|_{\\infty})\\leq O\\left(\\sqrt{\\frac{x_{\\operatorname*{max}}S}{\\epsilon}}\\right),\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $S$ is the right-hand side of Corollary 4.2 and $O$ also absorbs the constant $c$ ", "page_idx": 36}, {"type": "text", "text": "Proof of Lemma I.2. For any function $f\\in\\mathbb{T}$ , it can be written as below: ", "page_idx": 36}, {"type": "equation", "text": "$$\nf(x)=f(0)+f^{\\prime}(0)x+g(x),\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $g(x)=f(x)-f(0)-f^{\\prime}(0)x$ satisfies that $g(0)=g^{\\prime}(0)=0$ and $g^{\\prime\\prime}(x)=f^{\\prime\\prime}(x)$ . Therefore, to cover $\\mathbb{T}$ to $\\epsilon$ accuracy, it suffices to cover the three parts to $\\frac{\\epsilon}{3}$ accuracy with respect to $\\|\\cdot\\|_{\\infty}$ \uff0c respectively. ", "page_idx": 36}, {"type": "text", "text": "For $f(0)$ ,since $|f(0)|\\le k\\rho^{2}+\\rho$ , the covering number is bounded by ", "page_idx": 36}, {"type": "equation", "text": "$$\nN_{1}\\leq\\frac{6(k\\rho^{2}+\\rho)}{\\epsilon}\\leq\\frac{8k\\rho^{2}}{\\epsilon}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "For $f^{\\prime}(0)x$ since $|f^{\\prime}(0)|\\le k\\rho^{2}$ , the covering number is bounded by ", "page_idx": 36}, {"type": "equation", "text": "$$\nN_{2}\\leq\\frac{6k\\rho^{2}x_{\\mathrm{max}}}{\\epsilon}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Finally, for $g(x)$ , since $g(x)\\in\\mathbb{T}_{2}$ with $\\begin{array}{r}{C_{2}=\\frac{S}{c}}\\end{array}$ $g$ is extended linearly beyond the interval $\\mathcal{T}$ ), the covering number is bounded according to Lemma I.1 above. ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\log N_{3}\\leq C_{1}\\sqrt{\\frac{3x_{\\operatorname*{max}}S}{c\\epsilon}}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Combining the three parts, the metric entropy is bounded by ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\log N(\\epsilon,\\mathbb{T},\\|\\cdot\\|_{\\infty})\\leq\\log N_{1}+\\log N_{2}+\\log N_{3}\\leq O\\left(\\sqrt{\\frac{x_{\\operatorname*{max}}S}{\\epsilon}}\\right),\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $O$ also absorbs $c$ , which is the constant lower bound of $g(x)$ ", "page_idx": 37}, {"type": "text", "text": "According to the metric entropy above, we are ready to provide a refined (high probability) bound for mean squared error (restricted to $\\mathcal{T}$ ). Note that we assume that the ground-truth function $f_{0}\\in\\mathbb{T}$ which is necessary for the mean squared error to vanish. ", "page_idx": 37}, {"type": "text", "text": "Lemma I.3. Under the same conditions in Corollary 4.2, for any interval $\\mathcal{T}\\,\\subset\\,[-x_{\\mathrm{max}},x_{\\mathrm{max}}]$ andauniversalconstant $c>0$ such that $g(x)\\geq c$ for all $x\\in\\mathcal{T}$ and $f$ is optimized over $\\mathcal{T}$ ,i.e. $\\begin{array}{r}{\\sum_{x_{i}\\in\\mathbb{Z}}(f(x_{i})-y_{i})^{2}\\le\\sum_{x_{i}\\in\\mathbb{Z}}(f_{0}(x_{i})-\\bar{y}_{i})^{2}}\\end{array}$ iftheuut stablelin $\\theta$ satisfies $\\|\\theta\\|_{\\infty}\\leq\\rho$ and the ground truth $f_{0}\\in\\mathbb{T}$ then with probability $1-\\delta$ (over the random noises $\\{\\epsilon_{i}\\}$ ), the function $f=f_{\\theta}$ satisfies ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathrm{MSE}_{\\mathbb{Z}}(f)=\\frac{1}{n_{T}}\\sum_{x_{i}\\in\\mathbb{Z}}\\left(f(x_{i})-f_{0}(x_{i})\\right)^{2}\\leq O\\left(\\left(\\frac{\\sigma^{2}}{n_{T}}\\right)^{\\frac{4}{5}}(x_{\\mathrm{max}}S)^{\\frac{2}{5}}\\log\\left(\\frac{1}{\\delta}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $n\\tau$ is the number of data points in $\\mathcal{D}$ such that $x_{i}\\in{\\mathcal{T}}$ ", "page_idx": 37}, {"type": "text", "text": "Proof of Lemma I.3. According to the assumption that $f$ is optimized over $\\mathcal{T}$ wehave ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\sum_{x_{i}\\in\\mathbb{Z}}(f(x_{i})-y_{i})^{2}\\leq\\sum_{x_{i}\\in\\mathbb{Z}}(f_{0}(x_{i})-y_{i})^{2}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Similar to the calculation in Lemma G.8, it holds that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\mathrm{MSE}_{\\mathbb{Z}}(f)=\\frac{1}{2n_{\\mathbb{Z}}}\\sum_{x_{i}\\in\\mathbb{Z}}\\left(f(x_{i})-f_{0}(x_{i})\\right)^{2}\\leq\\frac{1}{n_{\\mathbb{Z}}}\\sum_{x_{i}\\in\\mathbb{Z}}\\epsilon_{i}\\left(f(x_{i})-f_{0}(x_{i})\\right).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "It is obvious that the function class $\\mathbb{T}$ is convex, together with the assumption that $f_{0}\\in\\mathbb{T}$ we have the function set $\\mathbb{T}^{\\star}:=\\mathbb{T}-\\{f_{0}\\}$ is star-shaped (details in Section 13 of Wainwright [2019]). Note that the metric entropy of $\\mathbb{T}$ satisfies that $\\begin{array}{r}{\\log N(\\epsilon,\\mathbb{T},\\|\\cdot\\|_{\\infty})\\leq O\\left(\\sqrt{\\frac{x_{\\operatorname*{max}}S}{\\epsilon}}\\right)}\\end{array}$ According to Corollary 13.7 of Wainwright [2019], the critical radius $r$ satisfies that ", "page_idx": 37}, {"type": "equation", "text": "$$\nr^{2}\\leq O\\left(\\left({\\frac{\\sigma^{2}}{n_{\\mathcal{T}}}}\\right)^{\\frac{4}{5}}\\left(x_{\\mathrm{max}}S\\right)^{\\frac{2}{5}}\\right).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Finally, according to Theorem 13.5 of Wainwright [2019], we have with probability $1-\\delta$ ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathrm{MSE}_{\\mathbb{Z}}(f)\\leq O\\left(\\left(\\frac{\\sigma^{2}}{n_{\\mathbb{Z}}}\\right)^{\\frac{4}{5}}(x_{\\mathrm{max}}S)^{\\frac{2}{5}}\\log\\left(\\frac{1}{\\delta}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "which finishes the proof. ", "page_idx": 37}, {"type": "text", "text": "Finally, Theorem 4.4 is derived by plugging in the definition of $S$ ", "page_idx": 37}, {"type": "text", "text": "Theorem I.4 (Restate Theorem 4.4). Under the same conditions in Corollary 4.2, for any interval $\\mathcal{T}\\subset$ $[-x_{\\mathrm{max}},x_{\\mathrm{max}}]$ andauniversalconstant $c>0$ such that $g(x)\\geq c$ for all $x\\in\\mathcal{Z}$ and $f$ is optimized over $\\mathcal{T}$ i.e. $\\begin{array}{r}{\\sum_{x_{i}\\in\\mathbb Z}(f(x_{i})-y_{i})^{2}\\le\\sum_{x_{i}\\in\\mathbb Z}(f_{0}(x_{i})-y_{i})^{2}}\\end{array}$ ,if theoutput stablesolution $\\theta$ satisfies $\\|\\theta\\|_{\\infty}\\leq\\rho$ (for some constant $\\rho>0$ )and theground truth $f_{0}\\in\\mathrm{BV}^{(1)}(k\\rho^{2},\\frac{1}{c}\\widetilde O(\\frac{1}{\\eta}+\\sigma x_{\\mathrm{max}}))$ , then with probability $1-\\delta$ (overtherandomnoises $\\{\\epsilon_{i}\\}_{,}$ ),the function $f=f_{\\theta}$ satisfies ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathrm{MSE}_{\\mathbb{Z}}(f)=\\frac{1}{n_{T}}\\sum_{x_{i}\\in\\mathbb{Z}}\\left(f(x_{i})-f_{0}(x_{i})\\right)^{2}\\leq\\widetilde{O}\\left(\\left(\\frac{\\sigma^{2}}{n_{T}}\\right)^{\\frac{4}{5}}\\left(\\frac{x_{\\operatorname*{max}}}{\\eta}+\\sigma x_{\\operatorname*{max}}^{2}\\right)^{\\frac{2}{5}}\\right),\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $n\\tau$ is the number of data points in $\\mathcal{D}$ such that $x_{i}\\in\\mathcal{T}$ ", "page_idx": 38}, {"type": "text", "text": "Proof of Theorem I.4. Note that $\\mathrm{BV}^{(1)}(k\\rho^{2},\\frac{1}{c}\\widetilde O(\\frac{1}{\\eta}+\\sigma x_{\\mathrm{max}}))$ is a subset of $\\mathbb{T}$ . Then the proof directly resultsfrom Lemma I3 and $\\begin{array}{r}{S=\\widetilde{O}\\left(\\frac{1}{\\eta}+\\sigma x_{\\mathrm{max}}\\right)}\\end{array}$ \u53e3 ", "page_idx": 38}, {"type": "text", "text": "1.1  The Improved Results for the Under-parameterized Case ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "We assume that $n/k$ is large enough such that the additional term in the $\\mathrm{TV}^{(1)}$ bound vanishes. ", "page_idx": 38}, {"type": "text", "text": "Assumption I.5. We assume that $\\frac{n}{k}$ is large enough such that the last term in (82) $\\widetilde{O}(\\sigma x_{\\mathrm{max}}\\sqrt{k/n})\\le$ $\\frac{1}{2}$ Wwhichfrtherimpie that $\\begin{array}{r}{\\int_{-x_{\\mathrm{max}}}^{x_{\\mathrm{max}}}|f^{\\prime\\prime}(x)|g(x)d x\\leq\\frac{1}{\\eta}}\\end{array}$ Wwhere $g(x)$ isdefnedas (6). ", "page_idx": 38}, {"type": "text", "text": "Assumption I.5 requires that $\\frac{n}{k}$ is larger than some constant, which naturally holds if $k=n^{1-\\alpha}$ for some $\\alpha>0$ and $n$ is sufficiently large. Under such assumption, we improve the MSE upper bound. ", "page_idx": 38}, {"type": "text", "text": "Theorem I.6. Under the same conditions in Corollary G.9, assume that Assumption I.5 holds. For any interval $\\mathcal{T}\\subset[-x_{\\mathrm{max}},x_{\\mathrm{max}}]$ and a universal constant $c>0$ such that $g(x)\\geq c$ for all $x\\in\\mathcal{Z}$ and $f$ is optimized over $\\mathcal{T}$ i.e. $\\begin{array}{r}{\\sum_{x_{i}\\in\\mathbb Z}(f(x_{i})-y_{i})^{2}\\le\\sum_{x_{i}\\in\\mathbb Z}(f_{0}(x_{i})-y_{i})^{2}}\\end{array}$ , if the output stable solution $\\theta$ satisfies $\\|\\theta\\|_{\\infty}\\leq\\rho$ (for some constant $\\rho>0$ ) and the ground truth $f_{0}\\in\\mathrm{BV}^{(1)}(k\\rho^{2},\\frac{1}{c\\eta})$ then with probability $1-\\delta$ (over the random noises $\\{\\epsilon_{i}\\}_{.}$ ), the function $f=f_{\\theta}$ satisfies ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathrm{MSE}_{\\mathbb{Z}}(f)=\\frac{1}{n_{\\mathbb{Z}}}\\sum_{x_{i}\\in\\mathbb{Z}}\\left(f(x_{i})-f_{0}(x_{i})\\right)^{2}\\leq\\widetilde{O}\\left(\\left(\\frac{\\sigma^{2}}{n_{\\mathbb{Z}}}\\right)^{\\frac{4}{5}}\\left(\\frac{x_{\\operatorname*{max}}}{\\eta}\\right)^{\\frac{2}{5}}\\right),\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $n\\tau$ is the number of data points in $\\mathcal{D}$ suchthat $x_{i}\\in\\mathcal{T}$ ", "page_idx": 38}, {"type": "text", "text": "Proof of Theorem I.6. The proof is identical to Theorem I.4, with $S$ replaced by $\\begin{array}{r}{\\frac{1}{\\eta}}\\end{array}$ ", "page_idx": 38}, {"type": "text", "text": "Remark I.7. Compared to Theorem 4.4, Theorem I.6 is better on the dependence of $\\eta$ by removing the additional term $\\sigma x_{\\mathrm{max}}^{2}$ . Such improvement results from the improved $\\mathrm{TV}^{(1)}$ bound (Corollary G.9) and the fact that $n/k$ is sufficiently large. ", "page_idx": 38}, {"type": "text", "text": "J  Twice-Differentiable Interpolating Solution with Noisy Labels must be \"Sharp\" ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Recall that in the counter-example, we fix 2@mxi - (n+1)max for i E [n] and fo(x) = 0 for any $x$ , which implies that $y_{i}$ 's are independent random variables from ${\\mathcal{N}}(0,\\sigma^{2})$ ", "page_idx": 38}, {"type": "text", "text": "Proposition J.1. In the example above, assume $f=f_{\\theta}$ is an interpolating solution where $\\mathcal{L}$ is twice differentiable at $\\theta$ thenwithprobability $1-\\delta$ wehave ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{max}}(\\nabla_{\\theta}^{2}\\mathcal{L}(\\theta))=\\Omega\\left(\\sigma n\\left[n-24\\log\\left(\\frac{1}{\\delta}\\right)\\right]\\right).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Proof of Proposition J.1. According to Theorem 3.1, with probability $1-\\delta$ wehave ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\int_{-x_{\\mathrm{max}}}^{x_{\\mathrm{max}}}|f_{\\theta}^{\\prime\\prime}(x)|g(x)d x=\\Omega\\left(\\sigma n\\left[n-24\\log\\left(\\frac{1}{\\delta}\\right)\\right]\\right),\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $g(x)$ is defined in (6). Meanwhile, note that $f_{\\theta}$ is an interpolating solution, and therefore ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\nabla_{\\theta}^{2}\\mathcal{L}(\\theta)=\\ \\!\\frac{1}{n}\\sum_{i=1}^{n}\\!(\\nabla_{\\theta}f_{\\theta}(x_{i}))(\\nabla_{\\theta}f_{\\theta}(x_{i}))^{T}+\\frac{1}{n}\\sum_{i=1}^{n}\\!(f_{\\theta}(x_{i})-y_{i})\\nabla_{\\theta}^{2}f(x_{i})}}\\\\ {{\\displaystyle=\\!\\frac{1}{n}\\sum_{i=1}^{n}\\!(\\nabla_{\\theta}f_{\\theta}(x_{i}))(\\nabla_{\\theta}f_{\\theta}(x_{i}))^{T}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Finally, combining the results and applying Lemma G.1, it holds that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\lambda_{\\operatorname*{max}}(\\nabla_{\\theta}^{2}\\mathcal{L}(\\theta))=\\lambda_{\\operatorname*{max}}\\left(\\frac{1}{n}\\sum_{i=1}^{n}(\\nabla_{\\theta}f_{\\theta}(x_{i}))(\\nabla_{\\theta}f_{\\theta}(x_{i}))^{T}\\right)}\\\\ {\\displaystyle\\geq1+2\\int_{-x_{\\operatorname*{max}}}^{x_{\\operatorname*{max}}}|f_{\\theta}^{\\prime\\prime}(x)|g(x)d x\\geq\\Omega\\left(\\sigma n\\left[n-24\\log\\left(\\frac{1}{\\delta}\\right)\\right]\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "which finishes the proof. ", "page_idx": 39}, {"type": "text", "text": "K Technical Lemmas ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Lemma K.1 (Lemma F.4 in Dann et al. [2017]). Let $F_{i}$ for $i=1,\\cdot\\cdot\\cdot$ be a filtration and $X_{1},\\cdot\\cdot\\cdot,X_{n}$ be a sequence of Bernoulli random variables with $\\mathbb{P}(X_{i}\\,=\\,1|F_{i-1})\\,=\\,P_{i}$ with $P_{i}$ being $F_{i-1}$ measurableand $X_{i}$ being $F_{i}$ measurable.It holdsthat ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\exists\\,n:\\sum_{t=1}^{n}X_{t}<\\sum_{t=1}^{n}P_{t}/2-W\\right]\\leq e^{-W}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Lemma K.2 (Covering Number of Euclidean Ball [Wainwright, 2019]). For any $\\epsilon>0$ the $\\epsilon$ -covering numberoftheEuclideanball in $\\mathbb{R}^{d}$ with radius $R>0$ is upper bounded by $\\begin{array}{r}{(1+\\frac{2R}{\\epsilon})^{d}}\\end{array}$ ", "page_idx": 39}, {"type": "text", "text": "Lemma K.3 (Hoeffding's inequality [Sridharan, 2002]). Suppose $X_{1},X_{2},\\cdot\\cdot\\cdot,X_{n}$ are a sequence of independent, identically distributed (i.i.d.) random variables with mean O. Let $\\textstyle{\\bar{X}}={\\frac{1}{n}}\\sum_{i=1}^{\\hat{n}}X_{i}$ Suppose that $X_{i}\\in[-b,b]$ with probability $^{\\,l}$ then with probability $1-\\delta$ ", "page_idx": 39}, {"type": "equation", "text": "$$\n|{\\bar{X}}|\\leq b\\cdot{\\sqrt{\\frac{2\\log(2/\\delta)}{n}}}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Lemma K.4 (Multiplicative Chernoff bound [Chernoff, 1952]). Let $X$ beaBinomialrandomvariable withparameters $p,n$ Thenforany $\\delta\\in[0,1]$ itholdsthat: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}[X>(1+\\delta)p n]<e^{-\\frac{\\delta^{2}p n}{3}},}\\\\ {\\mathbb{P}[X<(1-\\delta)p n]<e^{-\\frac{\\delta^{2}p n}{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "The lemmas above are also applied in Qiao et al. [2022], Qiao and Wang [2023a,c], Qiao et al. [2023], Qia0 and Wang [2023b, 2024], Zha0 et al. [2022], $\\mathrm{Xu}$ et al.[2023]. ", "page_idx": 39}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect the paper's contributions and scope. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 40}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Justification: Please refer to the \\*\u201cDisclaimers and limitations\"\u2019 part in page 3 ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should refect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should refect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 40}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: The assumptions are stated in the theorems while the proof is stated in the appendix. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 41}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: The details can be found in Section 5. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 41}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: We will release the code. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : / /nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 42}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: The details can be found in Section 5. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 42}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 42}, {"type": "text", "text": "Answer: [No] ", "page_idx": 42}, {"type": "text", "text": "Justification: Since we use gradient descent, everything is deterministic once the initialization of parameters is finished. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 42}, {"type": "text", "text": "", "page_idx": 43}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce theexperiments? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: All experiments are run on a mac book. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 43}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: The research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 43}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: This is a paper regarding theory of neural networks, where we believe there is no possible negative societal impact. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 43}, {"type": "text", "text": "", "page_idx": 44}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 44}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 44}, {"type": "text", "text": "\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 45}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 45}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 45}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 45}]