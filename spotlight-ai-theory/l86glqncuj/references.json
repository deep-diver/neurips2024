{"references": [{"fullname_first_author": "Luigi Ambrosio", "paper_title": "Gradient Flows in Metric Spaces and in the Space of Probability Measures", "publication_date": "2008-01-01", "reason": "This paper provides the foundational mathematical framework for Wasserstein Gradient Flows, which are central to the mean-field analysis of neural networks in this work."}, {"fullname_first_author": "L\u00e9na\u00efc Chizat", "paper_title": "On the global convergence of gradient descent for over-parameterized models using optimal transport", "publication_date": "2018-01-01", "reason": "This paper establishes the connection between the training dynamics of overparameterized neural networks and Wasserstein gradient flows, providing a crucial theoretical foundation for the mean-field analysis."}, {"fullname_first_author": "Song Mei", "paper_title": "A mean field view of the landscape of two-layer neural networks", "publication_date": "2018-01-01", "reason": "This paper pioneers the application of mean-field theory to understand the training dynamics of neural networks, laying the groundwork for the present paper's extension to the case of symmetric data and symmetry-leveraging techniques."}, {"fullname_first_author": "Justin Sirignano", "paper_title": "Mean field analysis of neural networks: A central limit theorem", "publication_date": "2020-01-01", "reason": "This paper provides a rigorous mathematical justification of the mean-field limit, providing a precise asymptotic characterization of stochastic gradient descent training dynamics that underpins the present study."}, {"fullname_first_author": "Dmitry Yarotsky", "paper_title": "Universal approximations of invariant maps by neural networks", "publication_date": "2022-01-01", "reason": "This work addresses the crucial question of whether neural networks can efficiently approximate invariant functions and provides valuable theoretical insights into the capacity of equivariant architectures."}]}