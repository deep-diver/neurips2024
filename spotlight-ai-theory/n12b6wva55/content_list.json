[{"type": "text", "text": "Mirror and Preconditioned Gradient Descent in Wasserstein Space ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Cl\u00e9ment Bonet Th\u00e9o Uscidda Adam David   \nCREST, ENSAE, IP Paris CREST, ENSAE, IP Paris Institute of Mathematics   \nclement.bonet@ensae.fr theo.uscidda@ensae.fr Technische Universit\u00e4t Berlin david@math.tu-berlin.de ", "page_idx": 0}, {"type": "text", "text": "Pierre-Cyril Aubin-Frankowski Anna Korba TU Wien CREST, ENSAE, IP Pari pierre-cyril.aubin@tuwien.ac.at anna.korba@ensae.fr ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "As the problem of minimizing functionals on the Wasserstein space encompasses many applications in machine learning, different optimization algorithms on $\\mathbb{R}^{d}$ have received their counterpart analog on the Wasserstein space. We focus here on lifting two explicit algorithms: mirror descent and preconditioned gradient descent. These algorithms have been introduced to better capture the geometry of the function to minimize and are provably convergent under appropriate (namely relative) smoothness and convexity conditions. Adapting these notions to the Wasserstein space, we prove guarantees of convergence of some Wassersteingradient-based discrete-time schemes for new pairings of objective functionals and regularizers. The difficulty here is to carefully select along which curves the functionals should be smooth and convex. We illustrate the advantages of adapting the geometry induced by the regularizer on ill-conditioned optimization tasks, and showcase the improvement of choosing different discrepancies and geometries in a computational biology task of aligning single-cells. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Minimizing functionals on the space of probability distributions has become ubiquitous in machine learning for e.g. sampling [13, 129], generative modeling [53, 86], learning neural networks [36, 90, 107], dataset transformation [4, 63], or modeling population dynamics [23, 120]. It is a challenging task as it is an infinite-dimensional problem. Wasserstein gradient flows [5] provide an elegant way to solve such problems on the Wasserstein space, i.e., the space of probability distributions with bounded second moment, equipped with the Wasserstein-2 distance from optimal transport (OT). These flows provide continuous paths of distributions decreasing the objective functional and can be seen as analog to Euclidean gradient flows [111]. Their implicit time discretization, referred to as the JKO scheme [66], has been studied in depth [1, 26, 95, 111]. In contrast, explicit schemes, despite being easier to implement, have been less investigated. Most previous works focus on the optimization of a specific objective functional with a time-discretation of its gradient flow with the Wasserstein-2 metrics. For instance, the forward Euler discretization leads to the Wasserstein gradient descent. The latter takes the form of gradient descent (GD) on the position of particles for functionals with a closed-form over discrete measures, e.g. Maximum Mean Discrepancy (MMD), which can be of interest to train neural networks [7, 30]. For objectives involving absolutely continuous measures, such as the Kullback-Leibler (KL) divergence for sampling, other discretizations can be easily computed such as the Unadjusted Langevin Algorithm (ULA) [106]. This leaves the question open of assessing the theoretical and empirical performance of other optimization algorithms relying on alternative geometries and time-discretizations. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In the optimization community, a recent line of works has focused on extending the methods and convergence theory beyond the Euclidean setting by using more general costs for the gradient descent scheme [77]. For instance, mirror descent (MD), originally introduced by Nemirovskij and Yudin [92] to solve constrained convex problems, uses a cost that is a divergence defined by a Bregman potential [12]. Mirror descent benefits from convergence guarantees for objective functions that are relatively smooth in the geometry induced by the (Bregman) divergence [88], even if they do not have a Lipschitz gradient, i.e., are not smooth in the Euclidean sense. More recently, a closely related scheme, namely preconditioned gradient descent, was introduced in [89]. It can be seen as a dual version of the mirror descent algorithm, where the role of the objective function and Bregman potential are exchanged. In particular, its convergence guarantees can be obtained under relative smoothness and convexity of the Fenchel transform of the potential, with respect to the objective. This algorithm appears more efficient to minimize the gradient magnitude than mirror descent [68]. The flexible choice of the Bregman divergence used by these two schemes enables to design or discover geometries that are potentially more efficient. ", "page_idx": 1}, {"type": "text", "text": "Mirror descent has already attracted attention in the sampling community, and some popular algorithms have been extended in this direction. For instance, ULA was adapted into the Mirror Langevin algorithm [3, 32, 62, 64, 79, 121, 132]. Other sampling algorithms have received their counterpart mirror versions such as the Metropolis Adjusted Langevin Algorithm [116], diffusion models [82], Stein Variational Gradient Descent (SVGD) [114], or even Wasserstein gradient descent [113]. Preconditioned Wasserstein gradient descent has been also recently proposed for specific geometries in [31, 44] to minimize the KL in a more efficient way, but without an analysis in discrete time. All the previous references focus on optimizing the KL as an objective, while Wasserstein gradient flows have been studied in machine learning for different functionals such as more general $f$ -divergences [6, 93], interaction energies [19, 78], MMDs [7, 30, 59, 60, 72] or Sliced-Wasserstein (SW) distances [15, 18, 45, 86]. In this work, we propose to bridge this gap by providing a general convergence theory of both mirror and preconditioned gradient descent schemes for general target functionals, and investigate as well empirical beneftis of alternative transport geometries for optimizing functionals on the Wasserstein space. We emphasize that the latter is different from [9, 67], wherein mirror descent is defined in the Radon space of probability distributions, using the flat geometry defined by TV or $L^{2}$ norms on measures, see Appendix A for more details. ", "page_idx": 1}, {"type": "text", "text": "Contributions. We are interested in minimizing a functional $\\mathcal{F}:\\mathcal{P}_{2}(\\mathbb{R}^{d})\\to\\mathbb{R}\\cup\\{+\\infty\\}$ over probability distributions, through schemes of the form, for $\\tau>0,k\\geq0$ , ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathrm{T}_{k+1}=\\underset{\\mathrm{T}\\in L^{2}(\\mu_{k})}{\\mathrm{argmin}}\\ \\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k}),\\mathrm{T}-\\mathrm{Id}\\rangle_{L^{2}(\\mu_{k})}+\\frac{1}{\\tau}\\mathrm{d}(\\mathrm{T},\\mathrm{Id}),\\quad\\mu_{k+1}=(\\mathrm{T}_{k+1})_{\\#}\\mu_{k},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "with different costs $\\mathrm{d}:L^{2}(\\mu_{k})\\times L^{2}(\\mu_{k})\\to\\mathbb{R}_{+}$ , and in providing convergence conditions. While we can recover a map $\\mathrm{T}=\\mathrm{T}_{k}\\circ\\mathrm{T}_{k-1}\\cdot\\cdot\\cdot\\circ\\mathrm{T}_{1}$ such that $\\mu_{k}=\\bar{\\mathrm{T}}_{\\#}\\bar{\\mu}_{0}$ , the scheme (1) proceeds by successive regularized linearizations retaining the Wasserstein structure, since the tangent space to $\\mathcal{P}_{2}(\\mathbb{R}^{d})$ at $\\mu$ is a subset of $L^{2}(\\mu)$ [96]. This paper is organized as follows. In Section 2, we provide some background on Bregman divergences, as well as on differentiability and convexity over the Wasserstein space. In Section 3, we consider Bregman divergences on $\\dot{L}^{2}(\\mu)$ for the cost in (1), generalizing the mirror descent scheme to the Wasserstein space. We study this new scheme by discussing its implementation, and proving its convergence under relative smoothness and convexity assumptions. In Section 4, we consider alternative costs in (1), that are analogous to OT distances with translation-invariant cost, extending the dual space preconditioning scheme to the latter space. Finally, in Section 5, we apply the two schemes to different objective functionals, including standard free energy functionals such as interaction energies and KL divergence, but also to Sinkhorn divergences [50] or SW [16, 102] with polynomial preconditioners on single-cell datasets. ", "page_idx": 1}, {"type": "text", "text": "Notations. Consider the set $\\mathcal{P}_{2}(\\mathbb{R}^{d})$ of probability measures $\\mu$ on $\\mathbb{R}^{d}$ with finite second moment and $\\mathcal{P}_{\\mathrm{2,ac}}(\\mathbb{R}^{d})\\subset\\mathcal{P}_{\\mathrm{2}}(\\mathbb{R}^{d})$ its subset of absolutely continuous probability measures with respect to the Lebesgue measure. For any $\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ , we denote by $L^{2}(\\mu)$ the Hilbert space of functions $f:\\mathbb R^{d}\\to\\mathbb R^{d}$ such that $\\int\\|f\\|^{2}\\mathrm{d}\\mu<\\infty$ equipped with the norm $\\|\\cdot\\|_{L^{2}(\\mu)}$ and inner product $\\langle\\cdot,\\cdot\\rangle_{L^{2}(\\mu)}$ For a Hilbert space $X$ , the Fenchel transform of $f:X\\to\\mathbb{R}$ is $f^{*}(y)=\\operatorname*{sup}_{x\\in X}\\,\\langle x,y\\rangle-f(x)$ Given a measurable map $\\mathrm{T}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d}$ and $\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ , $\\mathrm{T}_{\\#}\\mu$ is the pushforward measure of $\\mu$ by $\\mathrm{T}$ ; and $\\begin{array}{r}{\\mathrm{T}\\star\\mu=\\int\\mathrm{T}(\\cdot-x)\\mathrm{d}\\mu(x)}\\end{array}$ . For $\\mu,\\nu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ , the Wasserstein-2 distance is $\\mathrm{W}_{2}^{2}(\\mu,\\nu)=$ $\\operatorname*{inf}_{\\gamma\\in\\Pi(\\mu,\\nu)}\\int\\|x-y\\|^{2}\\,\\mathrm{d}\\gamma(x,y)$ , where $\\Pi(\\mu,\\nu)=\\{\\gamma\\in\\mathcal{P}(\\mathbb{R}^{d}\\times\\mathbb{R}^{d})$ , $\\pi_{\\#}^{1}\\gamma=\\mu$ , $\\pi_{\\#}^{2}\\gamma=\\nu\\}$ with $\\pi^{i}(x_{1},x_{2})\\,=\\,x_{i}$ , is the set of couplings between $\\mu$ and $\\nu$ , and we denote by $\\Pi_{o}(\\mu,\\nu)$ the set of optimal couplings. When the optimal coupling is of the form $\\gamma=(\\mathrm{Id},\\mathrm{T}_{\\mu}^{\\nu}){\\#}\\mu$ with ${\\mathrm{Id}}:x\\mapsto x$ and $\\mathrm{T}_{\\mu}^{\\nu}\\in\\Lt L^{2}(\\mu)$ satisfying $(\\mathrm{T}_{\\mu}^{\\nu}){\\#}\\mu=\\nu$ , we call $\\mathrm{T}_{\\mu}^{\\nu}$ the OT map. We refer to the metric space $(\\mathcal{P}_{2}(\\mathbb{R}^{d}),\\mathrm{W}_{2})$ as the Wasserstein space. We note $S_{d}^{++}(\\mathbb{R})$ the space of symmetric positive definite matrices, and for $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ , $\\Sigma\\in S_{d}^{++}(\\mathbb{R})$ , $\\|\\boldsymbol{x}\\|_{\\Sigma}^{2}=x^{T}\\Sigma\\boldsymbol{x}$ . ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we fix $\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ and introduce first the Bregman divergence on $L^{2}(\\mu)$ along with the notions of relative convexity and smoothness that will be crucial in the analysis of the optimization schemes. Then, we introduce the differential structure and computation rules for differentiating a functional $\\mathcal{F}:\\mathcal{P}_{2}(\\mathbb{R}^{d})\\to\\mathbb{R}$ along curves and discuss notions of convexity on $\\mathcal{P}_{2}(\\mathbb{R}^{d})$ . We refer the reader to Appendix B and Appendix C for more details on $L^{2}(\\mu)$ and the Wasserstein space respectively. Finally, we introduce the mirror descent and preconditioned gradient descent on $\\mathbb{R}^{d}$ . ", "page_idx": 2}, {"type": "text", "text": "Bregman divergence on $L^{2}(\\mu)$ . Frigyik et al. [54, Definition 2.1] defined the Bregman divergence of Fr\u00e9chet differentiable functionals. In our case, we only need G\u00e2teaux differentiability. In this paper, $\\nabla$ refers to the G\u00e2teaux differential, which coincides with the Fr\u00e9chet derivative if the latter exists. ", "page_idx": 2}, {"type": "text", "text": "Definition 1. Let $\\phi_{\\mu}:L^{2}(\\mu)\\to\\mathbb{R}$ be convex and continuously G\u00e2teaux differentiable. The Bregman divergence is defined for all $\\mathrm{T},\\mathrm{S}\\in L^{2}(\\mu)$ as $\\mathrm{d}_{\\phi_{\\mu}}(\\mathrm{T},\\mathrm{S})=\\phi_{\\mu}(\\mathrm{T})-\\phi_{\\mu}(\\mathrm{S})-\\langle\\nabla\\phi_{\\mu}(\\mathrm{S}),\\mathrm{T}-\\mathrm{S}\\rangle_{L^{2}(\\mu)}.$ . ", "page_idx": 2}, {"type": "text", "text": "We use the same definition on $\\mathbb{R}^{d}$ . The map $\\phi_{\\mu}$ (respectively $\\nabla\\phi_{\\mu})$ in the definition of $\\mathrm{d}_{\\phi_{\\mu}}$ above is referred to as the Bregman potential (respectively mirror map). If $\\phi_{\\mu}$ is strictly convex, then $\\mathrm{d}_{\\phi_{\\mu}}$ is a valid Bregman divergence, i.e. it is positive and separates maps $\\mu$ -almost everywhere (a.e.). In particular, for $\\begin{array}{r}{\\check{\\phi}_{\\mu}(\\mathrm{T})=\\frac{1}{2}\\|\\mathrm{T}\\|_{L^{2}(\\mu)}^{2}}\\end{array}$ , we recover the $L^{2}$ norm as a divergence $\\mathrm{d}_{\\phi_{\\mu}}(\\bar{\\mathrm{T}},\\mathrm{S})=\\textstyle{\\frac{1}{2}}\\|\\mathrm{T}-\\mathrm{S}\\|_{L^{2}(\\mu)}^{2}$ Bregman divergences have received a lot of attention as they allow to define provably convergent schemes for functions which are not smooth in the standard (e.g. Euclidean) sense [11, 88], and thus for which gradient descent is not appropriate. These guarantees rely on the notion of relative smoothness and relative convexity [88, 89], which we introduce now on $\\dot{L}^{2}(\\mu)$ . ", "page_idx": 2}, {"type": "text", "text": "Definition 2 (Relative smoothness and convexity). Let $\\psi_{\\mu},\\phi_{\\mu}:L^{2}(\\mu)\\to\\mathbb{R}$ be convex and continuously G\u00e2teaux differentiable. We say that $\\psi_{\\mu}$ is $\\beta$ -smooth (respectively $\\alpha$ -convex) relative to $\\phi_{\\mu}$ if and only if for all $\\Gamma,\\mathrm{S}\\in L^{2}(\\mu)$ , $\\mathrm{d}_{\\psi_{\\mu}}(\\mathrm{T},\\mathrm{S})\\leq\\beta\\mathrm{d}_{\\phi_{\\mu}}(\\mathrm{T},\\mathrm{S})$ (respectively $\\mathrm{d}_{\\psi_{\\mu}}(\\mathrm{T},\\mathrm{S})\\geq\\alpha\\mathrm{d}_{\\phi_{\\mu}}(\\mathrm{T},\\mathrm{S})\\,$ . ", "page_idx": 2}, {"type": "text", "text": "Similarly to the Euclidean case [88], relative smoothness and convexity are equivalent to respectively $\\beta\\phi_{\\mu}-\\psi_{\\mu}$ and $\\psi_{\\mu}-\\alpha\\phi_{\\mu}$ being convex (see Appendix B.2). Yet, proving the convergence of (1) requires only that these properties hold at specific functions (directions), a fact we will soon exploit. ", "page_idx": 2}, {"type": "text", "text": "In some situations, we need the $L^{2}$ Fenchel transform $\\phi_{\\mu}^{\\ast}$ of $\\phi_{\\mu}$ to be differentiable, e.g. to compute its Bregman divergence $\\mathrm{d}_{\\phi_{\\mu}^{*}}$ . We show in Lemma 18 that a sufficient condition to satisfy this property is for $\\phi_{\\mu}$ to be strictly convex, lower semicontinuous and superlinear, i.e. $\\begin{array}{r}{\\operatorname*{lim}_{\\|\\mathrm{T}\\|\\rightarrow\\infty}\\phi_{\\mu}(\\mathrm{T})/\\|\\mathrm{T}\\|_{L^{2}(\\mu)}=+\\infty}\\end{array}$ . Moreover, in this case, $(\\nabla\\phi_{\\mu})^{-1}=\\nabla\\phi_{\\mu}^{*}$ . When needed, we will suppose that $\\phi_{\\mu}$ satisfies this assumption. ", "page_idx": 2}, {"type": "text", "text": "Differentiability on $(\\mathcal{P}_{2}(\\mathbb{R}^{d}),\\mathrm{W}_{2})$ . Let $\\mathcal{F}:\\mathcal{P}_{2}(\\mathbb{R}^{d})\\to\\mathbb{R}\\cup\\{+\\infty\\}$ , and denote $D({\\mathcal{F}})=\\{\\mu\\in{\\mathfrak{~}}$ $\\mathcal{P}_{2}(\\mathbb{R}^{d})$ , $\\mathcal F(\\mu)<+\\infty\\}$ the domain of $\\mathcal{F}$ and $D(\\tilde{\\mathcal{F}}_{\\mu})=\\{\\mathrm{T}\\in L^{2}(\\mu)$ , $\\mathrm{T}_{\\#}\\mu\\in D(\\mathcal{F})\\}$ the domain of ${\\tilde{\\mathcal{F}}}_{\\mu}$ defined as $\\tilde{\\mathcal{F}}_{\\mu}(\\mathrm{T}):=\\mathcal{F}(\\mathrm{T}_{\\#}\\mu)$ for all $\\mathrm{T}\\in L^{2}(\\mu)$ . In the following, we use the differential structure of $(\\mathcal{P}_{2}(\\mathbb{R}^{d}),\\mathrm{W}_{2})$ introduced in [17, Definition 2.8], and we say that $\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu)$ is a Wasserstein gradient of $\\mathcal{F}$ at $\\mu\\in D({\\mathcal{F}})$ if for any $\\nu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ and any optimal coupling $\\gamma\\in\\Pi_{o}(\\mu,\\nu)$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{F}(\\nu)=\\mathcal{F}(\\mu)+\\int\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu)(x),y-x\\rangle\\;\\mathrm{d}\\gamma(x,y)+o\\big(\\mathrm{W}_{2}(\\mu,\\nu)\\big).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "If such a gradient exists, then we say that $\\mathcal{F}$ is Wasserstein differentiable at $\\mu$ [17, 74]. Moreover there is a unique gradient belonging to the tangent space of $\\mathcal{P}_{2}(\\mathbb{R}^{d})$ verifying (2) [74, Proposition ", "page_idx": 2}, {"type": "text", "text": "2.5], and we will always restrict ourselves without loss of generality to this particular gradient, see Appendix C.1. The differentiability of $\\mathcal{F}$ and ${\\tilde{\\mathcal{F}}}_{\\mu}$ are very related, as described in the following proposition. ", "page_idx": 3}, {"type": "text", "text": "Proposition 1. Let $\\mathcal{F}:\\mathcal{P}_{2}(\\mathbb{R}^{d})\\to\\mathbb{R}\\cup\\{+\\infty\\}$ be a Wasserstein differentiable functional on $D({\\mathcal{F}})$ . Let $\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ and $\\tilde{\\mathcal{F}}_{\\mu}(\\mathrm{T})=\\mathcal{F}(\\mathrm{T}_{\\#}\\mu).$ for all $\\mathrm{T}\\in D(\\tilde{\\mathcal{F}}_{\\mu})$ . Then, ${\\tilde{\\mathcal{F}}}_{\\mu}$ is Fr\u00e9chet differentiable, and for all $\\mathrm{S}\\in D(\\tilde{\\mathcal{F}}_{\\mu})$ , $\\nabla\\tilde{\\mathcal{F}}_{\\mu}(\\mathrm{S})=\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mathrm{S}_{\\#}\\mu)\\circ\\mathrm{S}.$ . ", "page_idx": 3}, {"type": "text", "text": "The Wasserstein differentiable functionals include $c$ -Wasserstein costs on $\\mathcal{P}_{2,\\mathrm{ac}}(\\mathbb{R}^{d})$ [74, Proposition 2.10 and 2.11], potential energies $\\textstyle\\mathcal{V}(\\mu)~=~\\int V\\mathrm{d}\\mu$ or interaction energies $\\mathcal{W}(\\mu)~{}=\\,$ $\\textstyle{\\frac{1}{2}}\\int\\!\\!\\int W(x-y)\\ \\mathrm{d}\\mu(x)\\mathrm{d}\\mu(y)$ for $V:\\ensuremath{\\mathbb{R}^{d}}\\to\\ensuremath{\\mathbb{R}}$ and $W:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ differentiable and with bounded Hessian [74, Section 2.4]. In particular, their Wasserstein gradients read as $\\nabla_{\\mathrm{W}_{2}}\\mathcal{V}(\\mu)\\,=\\,\\nabla V$ and $\\nabla_{\\mathrm{W}_{2}}\\mathcal{W}(\\mu)\\,=\\,\\nabla W\\star\\mu$ . However, entropy functionals, e.g. the negative entropy defined as $\\begin{array}{r}{\\mathcal{H}(\\mu)=\\int\\log\\big(\\rho(x)\\big)\\mathrm{d}\\mu(x)}\\end{array}$ for distributions $\\mu$ admitting a density $\\rho\\,w.r.t.$ . the Lebesgue measure, are not Wasserstein differentiable. In this case, we can consider subgradients $\\nabla_{\\mathrm{W_{2}}}\\mathcal{F}(\\mu)$ at $\\mu$ for which (2) becomes an inequality. To guarantee that the Wasserstein subgradient is not empty, we need $\\rho$ to satisfy some Sobolev regularity, see e.g. [5, Theorem 10.4.13] or [108]. Then, if $\\nabla\\log\\rho\\in L^{2}(\\mu)$ , the only subgradient of $\\mathcal{H}$ in the tangent space is $\\nabla_{\\mathrm{W_{2}}}\\mathcal{H}(\\mu)=\\nabla\\log\\rho.$ , see [5, Theorem 10.4.17] and [47, Proposition 4.3]. Then, free energies are functionals that write as sums of potential, interaction and entropy terms [110, Chapter 7]. It is notably the case for the KL to a fixed target distribution, that is the sum of a potential and entropy term [129], or the MMD as a sum of a potential and interaction term [7]. ", "page_idx": 3}, {"type": "text", "text": "Examples of functionals. The definitions of Bregman divergences on $L^{2}(\\mu)$ and of Wasserstein differentiability enable us to consider alternative Bregman potentials than the $L^{2}(\\mu)$ -norm mentioned above. For instance, for $V$ convex, differentiable and $L$ -smooth, we can use potential energies $\\phi_{\\mu}^{V}(\\mathrm{T}):=\\mathcal{V}(\\mathrm{T}_{\\#}\\mu)$ , for which $\\begin{array}{r}{\\mathrm{d}_{\\phi_{\\mu}^{V}}(\\mathrm{T},\\mathrm{S})\\,=\\,\\int\\mathrm{d}_{V}\\big(\\mathrm{T}(x),\\mathrm{S}(x)\\big)\\mathrm{d}\\mu(x)}\\end{array}$ where $\\mathrm{d}_{V}$ is the Bregman divergence of $V$ on $\\mathbb{R}^{d}$ . Notice that $\\begin{array}{r}{\\phi_{\\mu}(\\mathrm{T})=\\frac{1}{2}\\|\\mathrm{T}\\|_{L^{2}(\\mu)}^{2}}\\end{array}$ is a specific example of a potential energy where $V\\,=\\,\\textstyle{\\frac{1}{2}}\\|\\,\\cdot\\,\\|^{2}$ . Moreover, we will consider interaction energies $\\phi_{\\mu}^{W}(\\mathrm{T}):=\\mathcal{W}(\\mathrm{T}_{\\#}\\mu)$ with $W$ convex, differentiable, $L$ -smooth, and satisfying $W(-x)\\,=\\,W(x)$ ; for which $\\mathrm{d}_{\\phi_{\\mu}^{W}}(\\mathrm{T},\\mathrm{S})\\,=$ $\\begin{array}{r}{\\frac{1}{2}\\iint\\mathrm{d}_{W}\\big(\\mathrm{T}(x)-\\mathrm{T}(x^{\\prime}),\\mathrm{S}(x)-\\mathrm{S}(x^{\\prime})\\big)\\mathrm{d}\\mu(x)\\mathrm{d}\\mu(x^{\\prime})}\\end{array}$ (see Appendix I.3). We will also use $\\phi_{\\mu}^{\\mathcal{H}}(\\mathrm{T})=$ ${\\mathcal{H}}(\\mathrm{T}_{\\#}\\mu)$ with $\\mathcal{H}$ the negative entropy. Note that Bregman divergences on the Wasserstein space using these functionals were proposed by Li [80], but only for $\\mathrm{S}=\\mathrm{Id}$ and optimal transport maps $\\mathrm{T}$ . ", "page_idx": 3}, {"type": "text", "text": "Convexity and smoothness in $(\\mathcal{P}_{2}(\\mathbb{R}^{d}),\\mathrm{W}_{2})$ . In order to study the convergence of gradient flows and their discrete-time counterparts, it is important to have suitable notions of convexity and smoothness. On $(\\mathcal{P}_{2}(\\mathbb{R}^{d}),\\mathrm{W}_{2})$ , different such notions have been proposed based on specific choices of curves. The most popular one is to require the functional $\\mathcal{F}$ to be $\\alpha$ -convex along geodesics (see Definition 10), which are of the form $\\mu_{t}\\,=\\,\\bigl((1\\,-\\,t)\\mathrm{Id}+t\\mathrm{T}_{\\mu_{0}}^{\\mu_{1}}\\bigr)_{\\#}\\mu_{0}$ if $\\mu_{0}\\in\\bar{\\mathcal{P}_{\\mathrm{2,ac}}(\\mathbb{R}^{d})}$ and $\\mu_{1}\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ , with $\\mathrm{T}_{\\mu_{0}}^{\\mu_{1}}$ the OT map between them. In that setting, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{\\alpha}{2}\\mathrm{W}_{2}^{2}(\\mu_{0},\\mu_{1})=\\frac{\\alpha}{2}\\|\\mathrm{T}_{\\mu_{0}}^{\\mu_{1}}-\\mathrm{Id}\\|_{L^{2}(\\mu_{0})}^{2}\\le\\mathcal{F}(\\mu_{1})-\\mathcal{F}(\\mu_{0})-\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{0}),\\mathrm{T}_{\\mu_{0}}^{\\mu_{1}}-\\mathrm{Id}\\rangle_{L^{2}(\\mu_{0})}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For instance, free energies such as potential or interaction energies with convex $V$ or $W$ , or the negative entropy, are convex along geodesics [110, Section 7.3]. However, some popular functionals, such as the Wasserstein-2 distance $\\begin{array}{r}{\\mu\\mapsto\\frac{1}{2}\\mathrm{W}_{2}^{2}(\\mu,\\eta)}\\end{array}$ itself, for a given $\\eta\\in\\mathcal{P}_{2}(\\bar{\\mathbb{R}}^{d})$ , are not convex along geodesics. Instead Ambrosio et al. [5, Theorem 4.0.4] showed that it was sufficient for the convergence of the gradient flow to be convex along other curves, e.g. along particular generalized geodesics for the Wasserstein-2 distance [5, Lemma 9.2.7], which, for $\\mu,\\bar{\\nu}\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ , are of the form $\\mu_{t}\\,=\\,\\bigl((1\\,-\\,t)\\mathrm{T}_{\\eta}^{\\mu}+t\\mathrm{T}_{\\eta}^{\\nu}\\bigr)_{\\#}\\eta$ for $\\mathrm{T}_{\\eta}^{\\mu}$ , $T_{\\eta}^{\\nu}$ OT maps from $\\eta$ to $\\mu$ and $\\nu$ . Observing that for n\u2019 $\\begin{array}{r}{\\phi_{\\mu}(\\mathrm{T})\\,=\\,\\frac{1}{2}\\|\\mathrm{T}\\|_{L^{2}(\\mu)}^{2}}\\end{array}$ , we can rewrite (3) as $\\alpha\\mathrm{d}_{\\phi_{\\mu_{0}}}(\\mathrm{T}_{\\mu_{0}}^{\\mu_{1}},\\mathrm{Id})\\leq\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu_{0}}}(\\mathrm{T}_{\\mu_{0}}^{\\mu_{1}},\\mathrm{Id})$ and see that being convex along geodesics boils down to being convex in the $L^{2}$ sense for $\\mathrm{S}=\\mathrm{Id}$ and $\\mathrm{T}$ chosen as an OT map. This observation motivates us to consider a more refined notion of convexity along curves. ", "page_idx": 3}, {"type": "text", "text": "Definition 3. Let $\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ , T $\\therefore\\mathrm{S}\\in L^{2}(\\mu)$ and for all $t\\in[0,1]$ , $\\mu_{t}=(\\mathrm{T}_{t})_{\\#}\\mu$ with $\\mathrm{T}_{t}=(1\\mathrm{-}t)\\mathrm{S+}$ $t\\mathrm{T}$ . We say that $\\mathcal{F}:\\mathcal{P}_{2}(\\mathbb{R}^{d})\\to\\mathbb{R}$ is $\\alpha$ -convex (resp. $\\beta$ -smooth) relative to $\\mathcal{G}:\\mathcal{P}_{2}(\\mathbb{R}^{d})\\to\\mathbb{R}$ along $t\\mapsto\\mu_{t}$ if for all $s,t\\in[0,1]$ , $\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu}}(\\mathrm{T}_{s},\\mathrm{T}_{t})\\geq\\alpha\\mathrm{d}_{\\tilde{\\mathcal{G}}_{\\mu}}(\\mathrm{T}_{s},\\mathrm{T}_{t})$ (resp. $\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu}}(\\mathrm{T}_{s},\\mathrm{T}_{t})\\leq\\beta\\mathrm{d}_{\\tilde{\\mathcal{G}}_{\\mu}}(\\mathrm{T}_{s},\\mathrm{T}_{t})\\jmath$ . ", "page_idx": 3}, {"type": "text", "text": "Notice that in contrast with Definition 2, Definition 3 is stated for a fixed distribution $\\mu$ and directions $(\\mathrm{S},\\mathrm{T})$ , and involves comparisons between Bregman divergences depending on $\\mu$ and curves $(\\mathrm{T}_{s})_{s\\in[0,1]}$ depending on S, T. The larger family of S and $\\mathrm{T}$ for which Definition 3 holds, the more restricted is the notion of convexity of $\\mathcal{F}\\mathrm{-}\\alpha\\mathcal{G}$ (resp. of $\\beta\\mathcal{G}-\\mathcal{F})$ on $\\mathcal{P}_{2}(\\mathbb{R}^{d})$ . For instance, Wasserstein-2 generalized geodesics with anchor $\\eta\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ correspond to considering S, T as all the OT maps originating from $\\eta$ , among which geodesics are particular cases when taking $\\eta=\\mu$ (hence $\\mathrm{S}=\\mathrm{Id}$ ). If we furthermore ask for $\\alpha$ -convexity to hold for all $\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ and T $\\therefore\\mathrm{S}\\in L^{2}(\\mu)$ (i.e., not only OT maps), then we recover the convexity along acceleration free-curves as introduced in [28, 98, 117]. Our motivation behind introducing Definition 3 is that the convergence proofs of MD and preconditioned GD require relative smoothness and convexity properties to hold only along specific curves. ", "page_idx": 4}, {"type": "text", "text": "Mirror descent and preconditioned gradient descent on $\\mathbb{R}^{d}$ . These schemes read respectively as $\\nabla\\phi(x_{k+1})-\\nabla\\phi(x_{k})=-\\tau\\nabla f(x_{k})$ [12] and $y_{k+1}-y_{k}=-\\tau\\nabla h^{*}\\big(\\nabla g(y_{k})\\big)$ [89], where the objectives $f,g$ and the regularizers $h,\\phi$ are convex $C^{1}$ functions from $\\mathbb{R}^{d}$ to $\\mathbb{R}$ . The algorithms are closely related since, using the Fenchel transform and setting $g=\\phi^{*}$ and $h^{*}=f$ , we see that, for $\\boldsymbol{y}=\\nabla\\phi(\\boldsymbol{x})$ , the two schemes are equivalent when permuting the roles of the objective and of the regularizer. For MD, convergence of $f$ is ensured if $f$ is both $^1\\!/\\!\\tau$ -smooth and $\\alpha$ -convex relative to $\\phi$ [88, Theorem 3.1]. Concerning preconditioned GD, assuming that $h,g$ are Legendre, $\\left(g(y_{k})\\right)_{k}$ converges to the minimum of $g$ if $h^{*}$ is both $^1\\!/\\!\\tau$ -smooth and $\\alpha$ -convex relative to $g^{*}$ with $\\alpha>0$ [89, Theorem 3.9]. ", "page_idx": 4}, {"type": "text", "text": "3 Mirror descent ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "For every $\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ , let $\\phi_{\\mu}:L^{2}(\\mu)\\to\\mathbb{R}$ be strictly convex, proper and differentiable and assume that the (sub)gradient $\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu)\\in L^{2}(\\mu)$ exists. In this section, we are interested in analyzing the scheme (1) where the cost d is chosen as a Bregman divergence, i.e. $\\mathrm{d}_{\\phi_{\\mu}}$ as defined in Definition 1. This corresponds to a mirror descent scheme in $\\mathcal{P}_{2}(\\mathbb{R}^{d})$ . For $\\tau>0$ and $k\\geq0$ , it writes: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{T}_{k+1}=\\operatorname*{argmin}_{\\mathrm{T}\\in L^{2}(\\mu_{k})}\\ \\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{T},\\mathrm{Id})+\\tau\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k}),\\mathrm{T}-\\mathrm{Id}\\rangle_{L^{2}(\\mu_{k})},\\ \\ \\ \\mu_{k+1}=(\\mathrm{T}_{k+1})_{\\#}\\mu_{k}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Iterates of mirror descent. In all that follows, we assume that the iterates (4) exist, which is true e.g. for a superlinear $\\phi_{\\mu_{k}}$ , since the objective is a sum of linear functions and of the continuous $\\phi_{\\mu_{k}}$ . In the previous section, we have seen that the second term in the proximal scheme (4) can be interpreted as a linearization of the functional $\\mathcal{F}$ at $\\mu_{k}$ for Wasserstein (sub)differentiable functionals. Now define for all $\\mathrm{T}\\in L^{2}(\\mu_{k}),\\mathrm{J}(\\mathrm{T})=\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{T},\\mathrm{Id})+\\tau\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k}),\\mathrm{T}-\\mathrm{Id}\\rangle_{L^{2}(\\mu_{k})}.$ . Then, deriving the first order conditions of (4) as $\\nabla\\mathrm{J}(\\mathrm{T}_{k+1})=0$ , we obtain $\\mu_{k}$ -a.e., ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\nabla\\phi_{\\mu_{k}}\\bigl(\\mathrm{T}_{k+1}\\bigr)=\\nabla\\phi_{\\mu_{k}}(\\mathrm{Id})-\\tau\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k})\\Longleftrightarrow\\mathrm{T}_{k+1}=\\nabla\\phi_{\\mu_{k}}^{*}\\bigl(\\nabla\\phi_{\\mu_{k}}(\\mathrm{Id})-\\tau\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k})\\bigr).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Note that for $\\begin{array}{r}{\\phi_{\\mu}(\\mathrm{T})\\,=\\,\\frac{1}{2}\\|\\mathrm{T}\\|_{L^{2}(\\mu)}^{2}}\\end{array}$ , the update (5) translates as $\\mathrm{T}_{k+1}\\,=\\,\\mathrm{Id}\\,-\\,\\tau\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k})$ , and our scheme recovers Wasserstein gradient descent [35, 91]. This is analogous to mirror descent recovering GD when the Bregman potential is chosen as the Euclidean squared norm in $\\mathbb{R}^{d}$ [12]. We discuss in Appendix D.2 the continuous formulation of (4), showing it coincides with the gradient flow of the mirror Langevin [3, 130], the limit of the JKO scheme with Bregman groundcosts [104], Information Newton\u2019s flows [126], or Sinkhorn\u2019s flow [41] for specific choices of $\\phi$ and $\\mathcal{F}$ . ", "page_idx": 4}, {"type": "text", "text": "Our proof of convergence of the mirror descent algorithm will require the Bregman divergence to satisfy the following property, which is reminiscent of conditions of optimality for couplings in OT. ", "page_idx": 4}, {"type": "text", "text": "Assumption 1. For $\\mu,\\rho\\in\\mathcal{P}_{2,\\mathrm{ac}}(\\mathbb{R}^{d})$ and $\\nu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ , setting T\u03d5\u00b5,\u00b5\u03bd = argminT#\u00b5=\u03bd d\u03d5\u00b5(T, Id), $\\mathrm{U}_{\\phi_{\\rho}}^{\\rho,\\nu}\\,=\\,\\mathrm{argmin}_{\\mathrm{U}_{\\#}\\rho=\\nu}\\,\\mathrm{~d}_{\\phi_{\\rho}}\\bigl(\\mathrm{U},\\mathrm{Id}$ , the functional $\\phi_{\\mu}$ is such that, for any $\\mathrm{~S~}\\in\\,L^{2}(\\mu)$ satisfying $\\mathrm{S}_{\\#}\\mu=\\rho$ , we have $\\mathrm{d}_{\\phi_{\\mu}}(\\mathrm{T}_{\\phi_{\\mu}}^{\\mu,\\nu},\\mathrm{S})\\geq\\mathrm{d}_{\\phi_{\\rho}}(\\mathrm{U}_{\\phi_{\\rho}}^{\\rho,\\nu},\\mathrm{Id}).$ . ", "page_idx": 4}, {"type": "text", "text": "The inequality in Assumption 1 can be interpreted as follows: the \u201cdistance\u201d between $\\rho$ and $\\nu$ is greater when observed from an anchor $\\mu$ that differs from $\\rho$ and $\\nu$ . We demonstrate that Bregman divergences satisfy this assumption under the following conditions on the Bregman potential $\\phi$ . ", "page_idx": 4}, {"type": "text", "text": "Proposition 2. Let $\\mu,\\rho\\,\\in\\,\\mathcal{P}_{2,\\mathrm{{ac}}}(\\mathbb{R}^{d})$ and $\\nu\\,\\in\\,\\mathcal{P}_{2}(\\mathbb{R}^{d})$ . Let $\\phi_{\\mu}$ be a pushforward compatible functional, i.e. there exists $\\phi\\,:\\,\\mathcal{P}_{2}(\\mathbb{R}^{d})\\,\\to\\,\\mathbb{R}$ such that for all $\\mathrm{~T~}\\in\\mathrm{~}L^{2}(\\mu)$ , $\\phi_{\\mu}(\\mathrm{T})\\,=\\,\\phi(\\mathrm{T}_{\\#}\\mu)$ . Assume furthermore $\\nabla_{\\mathrm{W}_{2}}\\phi(\\mu)$ and $\\nabla_{\\mathrm{W_{2}}}\\phi(\\rho)$ invertible $(o n\\mathbb{R}^{d},$ ). Then, $\\phi_{\\mu}$ satisfies Assumption $^{\\,l}$ . ", "page_idx": 4}, {"type": "text", "text": "All the maps $\\phi_{\\mu}^{V},\\phi_{\\mu}^{W}$ and $\\phi_{\\mu}^{\\mathcal{H}}$ defined in Section 2 satisfy the assumptions of Proposition 2 under mild requirements, see Appendix D.1. The proof of Proposition 2 is given in Appendix H.2. It relies on the definition of an appropriate optimal transport problem ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{W}_{\\phi}(\\nu,\\mu)=\\operatorname*{inf}_{\\gamma\\in\\Pi(\\nu,\\mu)}\\,\\phi(\\nu)-\\phi(\\mu)-\\int\\langle\\nabla_{\\mathrm{W}_{2}}\\phi(\\mu)(y),x-y\\rangle\\;\\mathrm{d}\\gamma(x,y),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and on the proof of existence of OT maps for absolutely continuous measures (see Proposition 15), which implies $\\mathrm{W}_{\\phi}(\\nu,\\mu)=\\mathrm{d}_{\\phi_{\\mu}}(\\mathrm{T}_{\\phi_{\\mu}}^{\\mu,\\nu},\\mathrm{I}\\bar{\\mathrm{d}})$ with $\\mathrm{T}_{\\phi_{\\mu}}^{\\mu,\\nu}$ defined as in Assumption 1. From there, we can conclude that $\\phi_{\\mu}$ satisfies Assumption 1. We notice that the corresponding transport problem recovers previously considered objects such as OT problems with Bregman divergence costs [25, 103], but is strictly more general (as our results pertain to the existence of OT maps), as detailed in Appendix D.1. ", "page_idx": 5}, {"type": "text", "text": "We now analyze the convergence of the MD scheme. Under a relative smoothness condition along curves generated by $\\mathrm{S}=\\mathrm{Id}$ and $\\mathrm{T}=\\mathrm{T}_{k+1}$ solutions of (4) for all $k\\geq0$ , we derive the following descent lemma, which ensures that $\\left(\\mathcal{F}(\\mu_{k})\\right)_{k}$ is non-increasing. Its proof can be found in Appendix H.3 and relies on the three-point inequality [29], which we extended to $L^{2}(\\mu)$ in Lemma 29. ", "page_idx": 5}, {"type": "text", "text": "Proposition 3. Let $\\beta\\,>\\,0$ , $\\begin{array}{r}{\\tau\\,\\leq\\,\\frac{1}{\\beta}}\\end{array}$ . Assume for all $k\\,\\geq\\,0$ , $\\mathcal{F}$ is $\\beta$ -smooth relative to $\\phi$ along $t\\mapsto\\left((1-t)\\mathrm{Id}+t\\mathrm{T}_{k+1}\\right)_{\\#}\\mu_{k}$ , which implies $\\beta\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{T}_{k+1},\\mathrm{Id})\\geq\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu_{k}}}(\\mathrm{T}_{k+1},\\mathrm{Id})$ . Then, for all $k\\geq0$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{F}(\\mu_{k+1})\\leq\\mathcal{F}(\\mu_{k})-\\frac{1}{\\tau}\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{Id},\\mathrm{T}_{k+1}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Assuming additionally the convexity of $\\mathcal{F}$ along the curves $\\mu_{t}=\\big((1-t)\\mathrm{Id}+t\\mathrm{T}_{\\phi_{\\mu}}^{\\mu,\\nu}\\big){\\#}\\mu,t\\in[0,$ and that $\\phi$ satisfies Assumption 1, we can obtain global convergence. ", "page_idx": 5}, {"type": "text", "text": "Proposition 4. Let $\\nu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ , $\\alpha\\geq0$ . Suppose Assumption 1 and the conditions of Proposition 3 hold, and that $\\mathcal{F}$ is $\\alpha$ -convex relative to $\\phi$ along the curves $t\\mapsto\\big((1-t)\\mathrm{Id}+t\\mathrm{T}_{\\phi_{\\mu_{k}}}^{\\mu_{k},\\nu}\\big)_{\\#}\\mu_{k}$ T\u03d5\u00b5k\u00b5,k\u03bd )#\u00b5k. Then, for all $k\\geq1$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{F}(\\mu_{k})-\\mathcal{F}(\\nu)\\leq\\frac{\\alpha}{\\left(1-\\tau\\alpha\\right)^{-k}-1}\\mathrm{W}_{\\phi}(\\nu,\\mu_{0})\\leq\\frac{1-\\alpha\\tau}{k\\tau}\\mathrm{W}_{\\phi}(\\nu,\\mu_{0}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Moreover, if $\\alpha\\,>\\,0$ , taking $\\nu\\,=\\,\\mu^{*}$ the minimizer of $\\mathcal{F}$ , we obtain a linear rate: for all $k\\,\\geq\\,0$ , $\\mathrm{W}_{\\phi}(\\mu^{\\ast},\\mu_{k})\\leq(1-\\tau\\alpha)^{k}\\,\\mathrm{W}_{\\phi}(\\mu^{\\ast},\\mu_{0})$ . ", "page_idx": 5}, {"type": "text", "text": "The proof of Proposition 4 can be found in Appendix H.4, and requires Assumption 1 to hold so that consecutive distances between iterates and the global minimizer telescope. This is not as direct as in the proofs of [88] over $\\mathbb{R}^{d}$ , because the minimization problem of each iteration (4) happens in a different space $L^{2}(\\mu_{k})$ . We discuss in Section 5 how to verify the relative smoothness and convexity on some examples. In particular, when both $\\mathcal{F}$ and $\\phi$ are potential energies, it is inherited from the relative smoothness and convexity on $\\mathbb{R}^{d}$ , and the conditions are similar with those for MD on $\\mathbb{R}^{d}$ . We also note that relative smoothness assumptions along descent directions as stated in Proposition 3 and relative strong convexity along optimal curves between the iterates and a minimizer as stated in Proposition 4 have been used already in the literature of optimization over measures in very specific cases, e.g. for descent results for the KL along SVGD [71] or for Sinkhorn convergence in [9]. We further analyze in Appendix F the convergence of Bregman proximal gradient scheme [11, 123] for objectives of the form $\\mathcal F(\\mu)=\\mathcal G(\\mu)+\\mathcal H(\\mu)$ with $\\mathcal{H}$ non smooth; which includes the KL divergence decomposed as a potential energy plus the negative entropy. ", "page_idx": 5}, {"type": "text", "text": "Implementation. We now discuss the practical implementation of MD on $(\\mathcal{P}_{2}(\\mathbb{R}^{d}),\\mathrm{W}_{2})$ as written in (5). If $\\phi_{\\mu}$ is pushforward compatible, we have $\\nabla\\phi_{\\mu_{k}}(\\mathrm{T}_{k+1})=\\nabla_{\\mathrm{W}_{2}}\\phi\\big((\\mathrm{T}_{k+1})_{\\#}\\mu_{k}\\big)\\circ\\mathrm{T}_{k+1}$ ; but if $\\nabla\\phi_{\\mu_{k}}^{*}$ is unknown, the scheme is implicit in $\\mathrm{T}_{k+1}$ . A possible solution is to rely on a root finding algorithm such as Newton\u2019s method to find the zero of $\\nabla\\mathrm{J}$ at each step, which we use in Section 5 for $\\phi_{\\mu}^{\\breve{W}}$ as Bregman potential. However, this procedure may be computationally costly and scale badly $w.r t$ . the dimension and the number of samples, see Appendix G.1. Nonetheless, in the special case $\\begin{array}{r}{\\phi_{\\mu}^{V}(\\mathrm{T})=\\int V\\circ\\mathrm{T}\\,\\mathrm{d}\\mu}\\end{array}$ with $V$ differentiable, strongly convex and $L$ -smooth, since $\\nabla_{\\mathrm{W}_{2}}\\Bar{\\mathcal{V}}(\\mu)=\\nabla V$ and $(\\nabla V)^{-1}=\\nabla V^{*}$ , the scheme reads as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\forall k\\geq0,\\;\\mathrm{T}_{k+1}=\\nabla V^{*}\\circ\\big(\\nabla V-\\tau\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k})\\big),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and can be implemented on particles, i.e. for $\\begin{array}{r}{\\hat{\\mu}_{k}\\ =\\ \\frac1n\\sum_{i=1}^{n}\\delta_{x_{i}^{k}}}\\end{array}$ , xik+1 = \u2207V \u2217 \u2207V (xik ) \u2212 $\\tau\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\hat{\\mu}_{k})(x_{i}^{k}))$ for all $k\\,\\geq\\,0$ , $i\\,\\in\\,\\{1,\\ldots,n\\}$ . This scheme is analogous to MD in $\\mathbb{R}^{d}$ [12] and has been introduced as the mirror Wasserstein gradient descent [113]. Moreover, for $V=\\frac{1}{2}\\|\\cdot\\|_{2}^{2}$ , as observed earlier, we recover the usual Wasserstein gradient descent, i.e. $\\mathrm{T}_{k+1}=\\mathrm{Id}\\!-\\!\\tau\\nabla_{\\mathrm{W}_{2}}\\tilde{\\mathcal{F}}(\\mu_{k})$ . The scheme can also be implemented for Bregman potentials that are not pushforward compatible. For specific $\\phi$ , it recovers notably SVGD and its variants [83, 84, 114, 131] or the Kalman-Wasserstein gradient descent [56]. We refer to Appendix D.4 for more details. ", "page_idx": 6}, {"type": "text", "text": "4 Preconditioned gradient descent ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "As seen in Section 2, preconditioned gradient descent on $\\mathbb{R}^{d}$ has dual convergence conditions compared to mirror descent. Our goal is to extend these to (1) and $\\mathcal{P}_{2}(\\mathbb{R}^{d})$ . Let $\\tau>0$ , $\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ and $h:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ proper and strictly convex on $\\mathbb{R}^{d}$ . We consider in this section $\\begin{array}{r}{\\phi_{\\mu}^{h}(\\mathrm{T})=\\int h\\circ\\mathrm{T}\\,\\mathrm{d}\\mu}\\end{array}$ and $\\begin{array}{r}{\\mathrm{d}(\\mathrm{T},\\mathrm{Id})=\\phi_{\\mu_{k}}^{h}\\big((\\mathrm{Id}-\\mathrm{T})/\\tau\\big)\\tau=\\int h\\big((x-\\mathrm{T}(x))/\\tau\\big)\\tau\\,\\mathrm{d}\\mu_{k}(x)}\\end{array}$ . This type of discrepancy is analogous to OT costs with translation-invariant ground cost $c(x,y)=h(x-y)$ , which have been popular as they induce an OT map [110, Box 1.12]. Such costs have been introduced e.g. in [39, 70] to promote sparse transport maps. More generally, for $\\phi_{\\mu}$ strictly convex, proper, differentiable and superlinear, we have $(\\nabla\\phi_{\\mu})^{-1}=\\nabla\\phi_{\\mu}^{\\ast}$ and the following theory is still valid. For simplicity, we leave studying more general $\\phi$ for future works. Here, the scheme (1) results in: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{{\\mit\\Gamma}}_{k+1}=\\operatorname*{argmin}_{\\mathrm{T}\\in L^{2}(\\mu_{k})}\\,\\int h\\left(\\frac{x-\\mathrm{T}(x)}{\\tau}\\right)\\tau\\,\\mathrm{d}\\mu_{k}(x)+\\left<\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k}),\\mathrm{T}\\mathrm{-}\\mathrm{Id}\\right>_{L^{2}(\\mu_{k})},\\;\\;\\mu_{k+1}=(\\mathrm{T}_{k+1})_{\\#}\\mu_{k}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Deriving the first order conditions similarly to (5) in Section 3, we obtain the following update: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\forall k\\geq0,\\,\\,\\mathrm{T}_{k+1}=\\mathrm{Id}-\\tau(\\nabla\\phi_{\\mu_{k}}^{h})^{-1}\\big(\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k})\\big)=\\mathrm{Id}-\\tau\\nabla h^{*}\\circ\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k}).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Notice that for $h=\\textstyle\\frac{1}{2}\\|\\cdot\\|_{2}^{2}$ the squared Euclidean norm, $\\phi_{\\mu}^{h}$ and $\\phi_{\\mu}^{h^{*}}$ recover the squared $L^{2}(\\mu)$ norm, and schemes (4) and (10) coincide. The scheme (10) is analogous to preconditioned gradient descent [68, 75, 76, 89, 119], which provides a dual alternative to mirror descent. For the latter, the goal is to find a suitable preconditioner $h^{*}$ allowing to have convergence guarantees, or to speed-up the convergence for ill-conditioned problems. It was recently considered on the Wasserstein space by Cheng et al. [31] and Dong et al. [44] with a focus on the KL divergence as objective $\\mathcal{F}$ and for $h=\\|\\cdot\\|p$ with $p>1$ [31] or $h$ quadratic [44]. Moreover, their theoretical analysis [1] was mostly done using the continuous formulation. Instead we focus on deriving conditions for the convergence of the discrete-time scheme (11) for more general functionals objectives. ", "page_idx": 6}, {"type": "text", "text": "Convergence guarantees. Inspired by [89], we now provide a descent lemma on $\\left(\\phi_{\\mu_{k}}^{h^{\\ast}}(\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k}))\\right)_{k}$ under a technical inequality between the Bregman divergences of $\\phi_{\\mu_{k}}^{h^{*}}$ and $\\tilde{\\mathcal{F}}_{\\mu_{k}}$ for all $k\\,\\geq\\,0$ . Additionally, we also suppose that $\\mathcal{F}$ is convex along the curves generated by $\\mathrm{S}=\\mathrm{T}_{k+1}$ and $\\mathrm{T}=\\mathrm{Id}$ . This last hypothesis ensures that $\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu_{k}}}(\\mathrm{T}_{k+1},\\mathrm{Id})\\geq0$ , and thus that $\\left(\\phi_{\\mu_{k}}^{h^{\\ast}}(\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k}))\\right)_{k}$ is non-increasing. Analogously to the Euclidean case, $\\phi_{\\mu}^{h^{*}}$ quantifies the magnitude of the gradient, and provides a second quantifier of convergence leading to possibly different efficient methods compared to mirror descent [68]. The proof relies mainly on the three-point identity (see e.g. [54, Appendix B.7] or Lemma 28) and algebra with the definition of Bregman divergences. ", "page_idx": 6}, {"type": "text", "text": "Proposition 5. Let $\\beta>0$ . Assume $\\begin{array}{r}{\\tau\\le\\frac{1}{\\beta}}\\end{array}$ , and for all $k\\geq0$ , $\\mathcal{F}$ convex along $t\\mapsto\\left((1-t)\\mathrm{T}_{k+1}+\\right.$ $t{\\mathrm{Id}}\\rangle_{\\#}\\mu_{k}$ and $\\begin{array}{r}{\\mathrm{d}_{\\phi_{\\mu_{k}}^{h^{*}}}\\bigl(\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k+1})\\circ\\mathrm{T}_{k+1},\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k})\\bigr)\\leq\\beta\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu_{k}}}(\\mathrm{Id},\\mathrm{T}_{k+1}).}\\end{array}$ . Then, for all $k\\geq0,$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\phi_{\\mu_{k+1}}^{h^{\\ast}}\\bigl(\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k+1})\\bigr)\\leq\\phi_{\\mu_{k}}^{h^{\\ast}}\\bigl(\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k})\\bigr)-\\frac{1}{\\tau}\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu_{k}}}\\bigl(\\mathrm{T}_{k+1},\\mathrm{Id}\\bigr).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Under an additional assumption of a reverse inequality between the Bregman divergences of $\\phi_{\\mu_{k}}^{h^{*}}$ and $\\tilde{\\mathcal{F}}_{\\mu_{k}}$ , and assuming that $\\phi_{\\mu}^{h^{*}}$ attains its minimum in 0, we can show the convergence of the gradient quantified by $\\phi_{\\mu}^{h^{*}}$ (see Lemma 21), and the convergence of $\\left(\\mathcal{F}(\\mu_{k})\\right)_{k}$ towards the minimum of $\\mathcal{F}$ . ", "page_idx": 6}, {"type": "text", "text": "Proposition 6. Let $\\alpha~\\geq~0$ and $\\mu^{*}\\;\\in\\;\\mathcal{P}_{2}(\\mathbb{R}^{d})$ be the minimizer of $\\mathcal{F}$ . Assume the conditions of Proposition $5$ hold, and that for $\\bar{\\mathrm{~T~}}=\\mathrm{\\argmin_{T,T_{\\neq}\\mu_{k}=\\mu^{*}}}\\ \\mathrm{~d}_{\\tilde{\\mathcal{F}}_{\\mu_{k}}}(\\mathrm{Id},\\mathrm{T})$ , $\\alpha\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu_{k}}}(\\mathrm{Id},\\bar{\\mathrm{T}})\\ \\leq$ ", "page_idx": 6}, {"type": "text", "text": "$\\begin{array}{r l}&{\\mathrm{d}_{\\phi_{\\mu_{k}}^{h^{\\ast}}}\\bigl(\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\bar{\\mathrm{T}}_{\\#}\\mu_{k})\\,\\circ\\,\\bar{\\mathrm{T}},\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k})\\bigr)}\\\\ &{\\phi_{\\mu_{k}}^{h^{\\ast}}(0)=h^{\\ast}(0),}\\end{array}$ . Then, for all $k~\\ge~1$ , since $\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu^{*})~=~0$ and ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\phi_{\\mu_{k}}^{h^{\\ast}}\\bigl(\\nabla_{\\mathsf{W}_{2}}\\mathcal{F}(\\mu_{k})\\bigr)-h^{\\ast}(0)\\leq\\frac{\\alpha}{\\left(1-\\tau\\alpha\\right)^{-k}-1}\\bigl(\\mathcal{F}(\\mu_{0})-\\mathcal{F}(\\mu^{\\ast})\\bigr)\\leq\\frac{1-\\tau\\alpha}{\\tau k}\\bigl(\\mathcal{F}(\\mu_{0})-\\mathcal{F}(\\mu^{\\ast})\\bigr).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Moreover, assuming that $h^{*}$ attains its minimum at 0 and $\\alpha>0$ , $\\mathcal{F}$ converges towards its minimum at a linear rate, i.e. for all $k\\geq0$ , $\\mathcal{F}(\\mu_{k})-\\mathcal{F}(\\mu^{*})\\leq\\left(1-\\tau\\alpha\\right)^{k}\\left(\\mathcal{F}(\\mu_{0})-\\mathcal{F}(\\mu^{*})\\right)$ . ", "page_idx": 7}, {"type": "text", "text": "The proofs of Proposition 5 and Proposition 6 can be found respectively in Appendix H.5 and Appendix H.6. ", "page_idx": 7}, {"type": "text", "text": "We now discuss sufficient conditions to obtain the inequalities between the Bregman divergences required in Proposition 5 and Proposition 6. Maddison et al. [89] showed on $\\mathbb{R}^{d}$ for a cost $h$ and an objective function $g$ , that these conditions were equivalent to $\\beta$ -smoothness and $\\alpha$ -convexity of the preconditioner $h^{*}$ (analogous to $\\phi_{\\mu}^{\\ast},$ ) relative to the convex conjugate of the objective $g^{*}$ (analogous to $\\tilde{\\mathcal{F}}_{\\mu}^{*})$ . To write the inequalities we assumed as a relative smoothness/convexity property of $\\phi_{\\mu_{k}}^{h^{*}}$ dwi.vr.et.r ${\\tilde{\\mathcal{F}}}_{\\mu_{k}}^{*}$ ,e  waec cwoorduilnd gn teoe dD aet flieniatsito tno  1e.n sTuhries  tchaatn ${\\tilde{\\mathcal{F}}}_{\\mu_{k}}^{*}$ iosn dei fef.egr.e bntyi aabslseu, imni nogr o  sdtreifcitnley  itcso nBvreexg manadn $\\tilde{\\mathcal{F}}_{\\mu_{k}}$ superlinear (see Lemma 18). The latter is true for several examples of functionals $\\mathcal{F}$ we already mentioned, such as potential or interaction energies with strongly convex potentials. ", "page_idx": 7}, {"type": "text", "text": "Under this assumption, we can show that the inequalities in Proposition 5 and Proposition 6 are implied by relative smoothness and convexity along suitable curves. ", "page_idx": 7}, {"type": "text", "text": "Proposition 7. Let $\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ and $\\mathrm{T}\\in L^{2}(\\mu)$ . Assume $\\tilde{\\mathcal{F}}_{\\mu}^{*}$ is G\u00e2teaux differentiable and define ${\\mathcal{F}}_{\\mu}^{*}$ on $t\\mapsto\\mu_{t}=(\\mathrm{T}_{t})_{\\#}\\mu$ as $\\mathcal{F}_{\\mu}^{*}(\\mu_{t})=\\tilde{\\mathcal{F}}_{\\mu}^{*}(\\mathrm{T}_{t})$ for $\\mathrm{T}_{t}=(1-t)\\mathrm{U}+t\\mathrm{S}$ for all $t\\in[0,1].$ , $\\mathrm{S},\\mathrm{U}\\in L^{2}(\\mu)$ . $I f\\,\\phi^{h^{*}}$ is $\\beta$ -smooth relative to $\\mathcal{F}_{\\mu}^{*}$ along $t\\mapsto\\big((1-t)\\nabla_{\\mathrm{W}_{2}}\\mathcal F(\\mu)+t\\nabla_{\\mathrm{W}_{2}}\\mathcal F(\\mathrm{T}_{\\#}\\mu)\\circ\\mathrm{T}\\big)_{\\#}\\mu.$ . Then, ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathrm{d}_{\\phi_{\\mu}^{h^{*}}}\\left(\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mathrm{T}_{\\#}\\mu)\\circ\\mathrm{T},\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu)\\right)\\leq\\beta\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu}}(\\mathrm{Id},\\mathrm{T}).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Likewise, $i f\\,\\phi^{h^{*}}$ is $\\alpha$ -convex relative to ${\\mathcal{F}}_{\\mu}^{*}$ along $t\\mapsto\\big((1-t)\\nabla_{\\mathrm{W}_{2}}\\mathcal F(\\mu)+t\\nabla_{\\mathrm{W}_{2}}\\mathcal F(\\mathrm{T}_{\\#}\\mu)\\circ\\mathrm{T}\\big)_{\\#}\\mu,$ then ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\alpha\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu}}(\\mathrm{Id},\\mathrm{T})\\leq\\mathrm{d}_{\\phi_{\\mu}^{h^{\\ast}}}\\big(\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mathrm{T}_{\\#}\\mu)\\circ\\mathrm{T},\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu)\\big).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "In particular, for $\\mathcal{F}$ a potential energy, the conditions coincide with those of [89] in $\\mathbb{R}^{d}$ . We refer to Appendix E.1 for more details. ", "page_idx": 7}, {"type": "text", "text": "5 Applications and Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we first discuss how to verify the relative convexity and smoothness between functionals in practice. Then, we provide some examples of mirror descent and preconditioned gradient descent on different objectives. We refer to Appendix G for more details on the experiments1. ", "page_idx": 7}, {"type": "text", "text": "Relative convexity of functionals. To assess relative convexity or smoothness as stated in Definition 3, we need to compare the Bregman divergences along the right curves. When both functionals $\\phi$ and $\\mathcal{F}$ are of the same type, for example potential (respectively interaction) energies, this property is lifted from the convexity and smoothness on $\\mathbb{R}^{d}$ of the underlying potential functions (respectively interaction kernels) to $\\bar{\\mathcal{P}_{2}}(\\mathbb{R}^{d})$ , see Appendix E.2 for more details. When both $\\phi$ and $\\mathcal{F}$ are potential energies, the schemes (4) and (10) are equivalent to parallel MD and preconditioned GD since there are no interactions between the particles. The conditions of convergence then coincide with the ones obtained for MD and preconditioned GD on $\\mathbb{R}^{d}$ [88, 89]. In other cases, (4) and (10) provide schemes that are novel to the best of our knowledge. ", "page_idx": 7}, {"type": "text", "text": "For functionals which are not of the same type, it is less straightforward. Using equivalent notions of convexity (see Proposition 13), we may instead compare their Hessians along the right curves, see Appendix E.2 for an example between an interaction and a potential energy. We note also that for the particular case of a functional obtained as a sum $\\mathcal{F}=\\mathcal{G}+\\mathcal{H}$ with $\\tilde{\\mathcal{G}}_{\\mu}$ and $\\tilde{\\mathcal{H}}_{\\mu}$ convex, since $\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu}}\\,=\\,\\mathrm{d}_{\\tilde{\\mathcal{G}}_{\\mu}}\\,+\\,\\mathrm{d}_{\\tilde{\\mathcal{H}}_{\\mu}}$ , $\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu}}\\,\\geq\\,\\operatorname*{max}\\{\\mathrm{d}_{\\tilde{\\mathcal{G}}_{\\mu}},\\mathrm{d}_{\\tilde{\\mathcal{H}}_{\\mu}}\\}$ , and thus $\\mathcal{F}$ is 1-convex relative to $\\mathcal{G}$ and $\\mathcal{H}$ . This includes e.g. the KL divergence which is convex relative to the potential and the negative entropy. ", "page_idx": 7}, {"type": "image", "img_path": "N12B6wvA55/tmp/76f8bebd0d97d2ca4c1f46da5021e6fa4a53d607c133f92f28e162fcc3d55a45.jpg", "img_caption": ["Figure 1: (Left) Value of $\\mathcal{W}$ along the flow for two Figure 2: Convergence towards Gaussians difference interaction Bregman potentials, (Middle $\\bar{\\mathcal{N}}(0,U D U^{T})$ averaged over 20 covariances, and Right) Trajectories of particles to minimize $\\mathcal{W}$ . with $U\\sim\\mathrm{Unif}\\big(O_{10}(\\mathbb{R})\\big)$ and $D$ fixed. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "MD on interaction energies. We first focus on minimizing interaction energies $\\mathcal{W}(\\mu)~=$ ${\\textstyle\\frac{1}{2}}\\int\\!\\!\\int W(x-y)\\ \\mathrm{d}\\mu(x)\\mathrm{d}\\mu(y)$ with kernel $W(z)\\;=\\;{\\textstyle{\\frac{1}{4}}}\\|z\\|_{\\Sigma^{-1}}^{4}\\,-\\,{\\textstyle{\\frac{1}{2}}}\\|z\\|_{\\Sigma^{-1}}^{2}$ , $\\Sigma\\,\\in\\,S_{d}^{++}(\\mathbb{R})$ , whose minimizer is an ellipsoid [27]. Since the Hessian norm of $W$ can be bounded by a polynomial of degree 2, following [88, Section 2], $W$ is $\\beta.$ -smooth relative to $\\begin{array}{r}{K_{4}(z)\\,=\\,\\frac{1}{4}\\|z\\|_{2}^{\\tilde{4}}+\\frac{\\}{2}\\|\\dot{z}\\|_{2}^{2}}\\end{array}$ with $\\beta=4$ , and $\\mathcal{W}$ is $\\beta$ -smooth relative to $\\begin{array}{r}{\\phi_{\\mu}(\\mathrm{T})=\\frac12\\int\\!\\!\\int K_{4}\\big(\\mathrm{T}(x)-\\mathrm{T}(y)\\big)\\ \\mathrm{d}\\mu(\\dot{x})\\mathrm{d}\\mu(y)}\\end{array}$ . Supposing additionally that the distributions are compactly supported, we can show that $\\mathcal{W}$ is smooth relative to the interaction energy with $\\begin{array}{r}{K_{2}(z)=\\frac{1}{2}\\|\\dot{z}\\|_{2}^{2}}\\end{array}$ . For ill-conditioned $\\Sigma$ , i.e. for which the ratio between the largest and smallest eigenvalues is large, the convergence can be slow. Thus, we also propose to use $\\begin{array}{r}{\\check{K}_{2}^{\\Sigma}(z)=\\frac{1}{2}\\|z\\|_{\\Sigma^{-1}}^{2}}\\end{array}$ and $\\begin{array}{r}{K_{4}^{\\Sigma}(z)=\\frac{\\check{1}}{4}\\|z\\|_{\\Sigma^{-1}}^{4}+\\frac{1}{2}\\|z\\|_{\\Sigma^{-1}}^{2}}\\end{array}$ . We illustrate these mirror descent schemes on Figure 1 and observe the convergence we expect for the ones taking into account In practice, since $\\nabla\\phi_{\\mu}(\\mathrm{T})\\,=\\,(\\nabla K\\star\\mathrm{T}_{\\#}\\mu)\\ \\bar{\\circ}\\ \\mathrm{T}$ , the scheme (5) needs to be approximated using Newton\u2019s algorithm which can be computationally heavy. Using $\\begin{array}{r}{\\phi_{\\mu}^{V}(\\mathrm{T})=\\int V\\circ\\mathrm{T}\\,\\mathrm{d}\\mu}\\end{array}$ with $V=K_{2}^{\\Sigma}$ , we obtain a more computationally friendly scheme with the same convergence, see Appendix G.2, but for which the smoothness is trickier to show. ", "page_idx": 8}, {"type": "text", "text": "MD on KL. We now focus on minimizing $\\begin{array}{r}{\\mathcal{F}(\\mu)=\\int V\\mathrm{d}\\mu+\\mathcal{H}(\\mu)}\\end{array}$ for $V(x)={\\textstyle{\\frac{1}{2}}}x^{T}\\Sigma^{-1}x$ with $\\Sigma$ possibly ill-conditioned, whose minimizer is the Gaussian $\\nu=\\mathcal{N}(0,\\Sigma)$ , and for which Wasserstein gradient descent is slow to converge. We study the MD scheme in (4) with negative entropy $\\mathcal{H}$ as the Bregman potential (NEM), and compare it on Figure 2 with the Forward-Backward (FB) scheme studied in [43] and the ideally preconditioned Forward-Backward scheme (PFB) with Bregman potential $\\phi_{\\mu}^{\\bar{V}}$ (see (116) in Appendix F). For computational purpose, we restrain the minimization in (4) over affine maps, which can be seen as taking the gradient over the submanifold of Gaussians [43, 73]. Starting from $\\mathcal{N}(0,\\Sigma_{0})$ , the distributions stay Gaussian over the flow, and their closed-form is reported in (62) (Appendix D.3). We note that this might not be the case for the scheme (4), and thus that this scheme does not enter into the framework developed in the previous sections. Nonetheless, it demonstrates the benefits of using different Bregman potentials. We generate 20 Gaussian targets $\\nu$ on $\\mathbb{R}^{10}$ with $\\Sigma=U D U^{T}$ , $D$ diagonal and scaled in log space between 1 and 100, and $U$ a uniformly sampled orthogonal matrix, and we report the averaged KL over time. Surprisingly, NEM, which does not require an ideal (and not available in general) preconditioner, is almost as fast to converge as the ideal PFB, and much faster than the FB scheme. ", "page_idx": 8}, {"type": "text", "text": "Preconditioned GD for single-cells. Predicting the response of cells to a perturbation is a central question in biology. In this context, as the measuring process is destructive, feature descriptions of control and treated cells must be dealt with as (unpaired) source $\\mu$ and target distributions $\\nu$ . Following [112], OT theory to recover a mapping $\\mathrm{T}$ between these two populations has been used in [21, 22, 23, 39, 48, 69, 122]. Inspired by the recent success of iterative refinement in generative modeling, through diffusion [61, 115] or flow-based models [81, 85], our scheme (1) follows the idea of transporting $\\mu$ to $\\nu$ via successive and dynamic displacements instead of, directly, with a static map T\u00af. We model the transition from unperturbed to perturbed states through the (preconditioned) ", "page_idx": 8}, {"type": "image", "img_path": "N12B6wvA55/tmp/76f2783b73264cd530a2e60137ad3b26df30f2fb6b4489ec1812c7f25be69e13.jpg", "img_caption": ["Figure 3: Preconditioned GD vs. (vanilla) GD to predict the responses of cell populations to cancer treatment on 4i (Upper row) and scRNAseq (Lower row) datasets. For each treatment, starting from the untreated cells $\\mu_{i}$ , we minimize ${\\mathcal F}(\\mu)=D(\\mu,\\nu_{i})$ with $\\nu_{i}$ the treated cells. The plot is organized as pairs of columns, each corresponding to optimizing a specific metric, with two scatter plots displaying points $z_{i}=(x_{i},y_{i})$ where (First column) $y_{i}$ is the attained minima $\\mathcal{F}(\\hat{\\mu})=D(\\bar{\\hat{\\mu}},\\nu_{i})$ with preconditioning and $x_{i}$ that without preconditioning, and (Second column) $y_{i}$ is the number of iterations to reach convergence with preconditioning and $x_{i}$ that without preconditioning. A point below the diagonal $y=x$ then refers to an experiment in which preconditioning provides (First column) a better minima or (Second column) faster convergence. We assign a color to each treatment and plot three runs, obtained with three different initializations, along with their mean (brighter point). "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "gradient flow of a functional ${\\mathcal F}(\\mu)\\,=\\,D(\\mu,\\nu)$ initialized at $\\mu_{0}\\,=\\,\\mu$ , where $D$ is a distributional metric, and predict the perturbed population via ${\\hat{\\mu}}=\\operatorname*{min}_{\\mu}{\\mathcal{F}}(\\mu)$ . We focus on the datasets used in [21], consisting of cell lines analyzed using (i) 4i [58], and (ii) scRNA sequencing [118]. For each profiling technology, the response to respectively (i) 34 and (ii) 9 treatments are provided. As in [21], training is performed in data space for the 4i data and in a latent space learned by the scGen autoencoder [87] for the scRNA data. We use three metrics: the Sliced-Wasserstein distance $\\mathrm{SW_{2}^{2}}$ [16], the Sinkhorn divergence $\\mathrm{S}_{\\varepsilon,2}^{2}$ [50] and the energy distance ED [59, 60, 105], and we compare the performances when minimizing this functional via preconditioned GD vs. (vanilla) GD. We measure the convergence speed when using a fixed relative tolerance $\\mathrm{[ol=10^{-3}}$ , as well as the attained optimal value $\\mathcal{F}(\\hat{\\mu})$ . Note that we follow [21] and additionally consider $40\\%$ of unseen (test) target cells for evaluation, i.e., for computing $\\mathcal{F}(\\hat{\\mu})=D(\\hat{\\mu},\\nu)$ . As preconditioner, we use the one induced by $h^{*}(x)=(\\|x\\|_{2}^{a}+1)^{1/a}-1$ with $a>0$ , which is well suited to minimize functionals which grow in $\\|x-x^{*}\\|^{a/(a-1)}$ near their minimum [119]. We set the step size $\\tau=1$ for all the experiments. Then, we tune the parameter $a$ very simply: for a given metric $D$ and a profiling technology, we pick a random treatment and select $a\\in\\{1.25,1.5,1.75\\}$ by grid search, and we generalize the selected $a$ for all the other treatments. Results are described in Figure 3: Preconditioned GD significantly outperforms GD over the 43 datasets, in terms of convergence speed and optimal value $\\mathcal{F}(\\hat{\\mu})$ . For instance, for $D=\\mathrm{S}_{2,\\varepsilon}^{2}$ , we converge in 10 times less iterations while providing, on average, a better estimate of the treated population. We also compare our iterative (non parametric) approach with the use of a static (non parametric) map in Appendix G.4. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we extended two non-Euclidean optimization methods on $\\mathbb{R}^{d}$ to the Wasserstein space, generalizing $\\mathrm{W_{2}}$ -gradient descent to alternative geometries. We investigated the practical beneftis of these schemes, and provided rates of convergences for pairs of objectives and Bregman potentials satisfying assumptions of relative smoothness and convexity along specific curves. While these assumptions can be easily checked is some cases (e.g. potential or interaction energies) by comparing the Bregman divergences or Hessian operators in the Wasserstein geometry, they may be hard to verify in general. Different objectives such as the Sliced-Wasserstein distance or the Sinkhorn divergence, or alternative geometries to the Wasserstein-2 as studied in this work, require to derive specific computations on a case-by-case basis. We leave this investigation for future work. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Cl\u00e9ment Bonet acknowledges the support of the center Hi! PARIS and of ANR PEPR PDE-AI. Adam David gratefully acknowledges funding by the BMBF 01|S20053B project SALE. Pierre-Cyril Aubin-Frankowski was funded by the FWF project P 36344-N. Anna Korba acknowledges the support of ANR-22-CE23-0030. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Martial Marie-Paul Agueh. Existence of Solutions to Degenerate Parabolic Equations via the Monge-Kantorovich Theory. Georgia Institute of Technology, 2002. (Cited on p. 1, 7)   \n[2] Byeongkeun Ahn, Chiyoon Kim, Youngjoon Hong, and Hyunwoo J Kim. Invertible Monotone Operators for Normalizing Flows. Advances in Neural Information Processing Systems, 35: 16836\u201316848, 2022. (Cited on p. 28)   \n[3] Kwangjun Ahn and Sinho Chewi. Efficient Constrained Sampling via the Mirror-Langevin Algorithm. Advances in Neural Information Processing Systems, 34:28405\u201328418, 2021. (Cited on p. 2, 5, 21, 29, 44)   \n[4] David Alvarez-Melis and Nicol\u00f2 Fusi. Dataset Dynamics via Gradient Flows in Probability Space. In International conference on machine learning, pages 219\u2013230. PMLR, 2021. (Cited on p. 1)   \n[5] Luigi Ambrosio, Nicola Gigli, and Giuseppe Savar\u00e9. Gradient Flows: in Metric Spaces and in the Space of Probability Measures. Springer Science & Business Media, 2005. (Cited on p. 1, 4, 23, 25, 38, 53)   \n[6] Abdul Fatir Ansari, Ming Liang Ang, and Harold Soh. Refining Deep Generative Models via Discriminator Gradient Flow. In 9th International Conference on Learning Representations, ICLR, 2021. (Cited on p. 2, 20, 44)   \n[7] Michael Arbel, Anna Korba, Adil Salim, and Arthur Gretton. Maximum Mean Discrepancy Gradient Flow. Advances in Neural Information Processing Systems, 32, 2019. (Cited on p. 1, 2, 4, 24) [8] Hedy Attouch, Giuseppe Buttazzo, and G\u00e9rard Michaille. Variational Analysis in Sobolev and BV Spaces. Society for Industrial and Applied Mathematics, 2014. (Cited on p. 33, 35)   \n[9] Pierre-Cyril Aubin-Frankowski, Anna Korba, and Flavien L\u00e9ger. Mirror Descent with Relative Smoothness in Measure Spaces, with Application to Sinkhorn and EM. Advances in Neural Information Processing Systems, 35:17263\u201317275, 2022. (Cited on p. 2, 6, 21)   \n[10] Heinz H Bauschke and Patrick L Combettes. Convex Analysis and Monotone Operator Theory in Hilbert Spaces. Springer, 2017. (Cited on p. 33)   \n[11] Heinz H Bauschke, J\u00e9r\u00f4me Bolte, and Marc Teboulle. A descent lemma beyond Lipschitz gradient continuity: first-order methods revisited and applications. Mathematics of Operations Research, 42(2):330\u2013348, 2017. (Cited on p. 3, 6, 20, 37)   \n[12] Amir Beck and Marc Teboulle. Mirror Descent and Nonlinear Projected Subgradient Methods for Convex Optimization. Operations Research Letters, 31(3):167\u2013175, 2003. (Cited on p. 2, 5, 7, 20)   \n[13] David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational Inference: A Review for Statisticians. Journal of the American statistical Association, 112(518):859\u2013877, 2017. (Cited on p. 1)   \n[14] Cl\u00e9ment Bonet, Nicolas Courty, Fran\u00e7ois Septier, and Lucas Drumetz. Efficient Gradient Flows in Sliced-Wasserstein Space. Transactions on Machine Learning Research, 2022. (Cited on p. 20)   \n[15] Cl\u00e9ment Bonet, Lucas Drumetz, and Nicolas Courty. Sliced-Wasserstein Distances and Flows on Cartan-Hadamard Manifolds. arXiv preprint arXiv:2403.06560, 2024. (Cited on p. 2)   \n[16] Nicolas Bonneel, Julien Rabin, Gabriel Peyr\u00e9, and Hanspeter Pfister. Sliced and Radon Wasserstein Barycenters of Measures. Journal of Mathematical Imaging and Vision, 51:22\u201345, 2015. (Cited on p. 2, 10, 43)   \n[17] Beno\u00eet Bonnet. A Pontryagin Maximum Principle in Wasserstein Spaces for Constrained Optimal Control Problems. ESAIM: Control, Optimisation and Calculus of Variations, 25:52, 2019. (Cited on p. 3, 23)   \n[18] Nicolas Bonnotte. Unidimensional and Evolution Methods for Optimal Transportation. PhD thesis, Universit\u00e9 Paris Sud-Paris XI; Scuola normale superiore (Pise, Italie), 2013. (Cited on p. 2, 43)   \n[19] Siwan Boufad\u00e8ne and Fran\u00e7ois-Xavier Vialard. On the global convergence of Wasserstein gradient flow of the Coulomb discrepancy. arXiv preprint arXiv:2312.00800, 2023. (Cited on p. 2)   \n[20] Yann Brenier. Polar Factorization and Monotone Rearrangement of Vector-Valued Functions. Communications on pure and applied mathematics, 44(4):375\u2013417, 1991. (Cited on p. 27)   \n[21] Charlotte Bunne, Stefan G Stark, Gabriele Gut, Jacobo Sarabia del Castillo, Kjong-Van Lehmann, Lucas Pelkmans, Andreas Krause, and Gunnar Ratsch. Learning Single-Cell Perturbation Responses using Neural Optimal Transport. bioRxiv, 2021. (Cited on p. 9, 10, 43)   \n[22] Charlotte Bunne, Andreas Krause, and Marco Cuturi. Supervised Training of Conditional Monge Maps. Advances in Neural Information Processing Systems, 35:6859\u20136872, 2022. (Cited on p. 9)   \n[23] Charlotte Bunne, Laetitia Papaxanthos, Andreas Krause, and Marco Cuturi. Proximal Optimal Transport Modeling of Population Dynamics. In International Conference on Artificial Intelligence and Statistics, pages 6511\u20136528. PMLR, 2022. (Cited on p. 1, 9)   \n[24] Martin Burger, Matthias Erbar, Franca Hoffmann, Daniel Matthes, and Andr\u00e9 Schlichting. Covariance-modulated optimal transport and gradient flows. arXiv preprint arXiv:2302.07773, 2023. (Cited on p. 21)   \n[25] Guillaume Carlier and Chlo\u00e9 Jimenez. On Monge\u2019s Problem for Bregman-like Cost Functions. Journal of Convex Analysis, 14(3):647, 2007. (Cited on p. 6, 21)   \n[26] J. A. Carrillo, M. DiFrancesco, A. Figalli, T. Laurent, and D. Slep\u02c7cev. Global-in-time weak measure solutions and finite-time aggregation for nonlocal interaction equations. Duke Mathematical Journal, 156(2):229 \u2013 271, 2011. (Cited on p. 1)   \n[27] Jos\u00e9 A Carrillo, Katy Craig, Li Wang, and Chaozhen Wei. Primal dual methods for Wasserstein gradient flows. Foundations of Computational Mathematics, pages 1\u201355, 2022. (Cited on p. 9, 40)   \n[28] Giulia Cavagnari, Giuseppe Savar\u00e9, and Giacomo Enrico Sodini. A Lagrangian approach to totally dissipative evolutions in Wasserstein spaces. arXiv preprint arXiv:2305.05211, 2023. (Cited on p. 5, 26)   \n[29] Gong Chen and Marc Teboulle. Convergence Analysis of a Proximal-Like Minimization Algorithm Using Bregman Functions. SIAM Journal on Optimization, 3(3):538\u2013543, 1993. (Cited on p. 6)   \n[30] Zonghao Chen, Aratrika Mustaf,i Pierre Glaser, Anna Korba, Arthur Gretton, and Bharath K Sriperumbudur. (De)-regularized Maximum Mean Discrepancy Gradient Flow. arXiv preprint arXiv:2409.14980, 2024. (Cited on p. 1, 2)   \n[31] Ziheng Cheng, Shiyue Zhang, Longlin Yu, and Cheng Zhang. Particle-based Variational Inference with Generalized Wasserstein Gradient Flow. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. (Cited on p. 2, 7, 21)   \n[32] Sinho Chewi, Thibaut Le Gouic, Chen Lu, Tyler Maunu, Philippe Rigollet, and Austin Stromme. Exponential Ergodicity of Mirror-Langevin Diffusions. Advances in Neural Information Processing Systems, 33:19573\u201319585, 2020. (Cited on p. 2, 21, 44)   \n[33] Sinho Chewi, Tyler Maunu, Philippe Rigollet, and Austin J Stromme. Gradient descent algorithms for Bures-Wasserstein barycenters. In Conference on Learning Theory, pages 1276\u20131304. PMLR, 2020. (Cited on p. 53)   \n[34] Sinho Chewi, Jonathan Niles-Weed, and Philippe Rigollet. Statistical Optimal Transport. arXiv preprint arXiv:2407.18163, 2024. (Cited on p. 23)   \n[35] Lenaic Chizat. Sparse Optimization on Measures with Over-parameterized Gradient Descent. Mathematical Programming, 194(1):487\u2013532, 2022. (Cited on p. 5)   \n[36] Lenaic Chizat and Francis Bach. On the Global Convergence of Gradient Descent for Overparameterized Models using Optimal Transport. Advances in neural information processing systems, 31, 2018. (Cited on p. 1)   \n[37] Dario Cordero-Erausquin. Transport inequalities for log-concave measures, quantitative forms, and applications. Canadian Journal of Mathematics, 69(3):481\u2013501, 2017. (Cited on p. 21)   \n[38] Marco Cuturi, Laetitia Meng-Papaxanthos, Yingtao Tian, Charlotte Bunne, Geoff Davis, and Olivier Teboul. Optimal Transport Tools (OTT): A JAX Toolbox for all things Wasserstein. arXiv Preprint arXiv:2201.12324, 2022. (Cited on p. 43)   \n[39] Marco Cuturi, Michal Klein, and Pierre Ablin. Monge, Bregman and Occam: Interpretable Optimal Transport in High-Dimensions with Feature-Sparse Maps. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 6671\u20136682. PMLR, 23\u201329 Jul 2023. (Cited on p. 7, 9)   \n[40] Mathieu Dagr\u00e9ou, Pierre Ablin, Samuel Vaiter, and Thomas Moreau. How to compute Hessian-vector products? In ICLR Blogposts 2024, 2024. (Cited on p. 39)   \n[41] Nabarun Deb, Young-Heon Kim, Soumik Pal, and Geoffrey Schiebinger. Wasserstein Mirror Gradient Flow as the Limit of the Sinkhorn Algorithm. arXiv preprint arXiv:2307.16421, 2023. (Cited on p. 5, 21, 29, 56)   \n[42] Sylvain Delattre and Nicolas Fournier. On the Kozachenko\u2013Leonenko entropy estimator. Journal of Statistical Planning and Inference, 185:69\u201393, 2017. (Cited on p. 44)   \n[43] Michael Ziyang Diao, Krishna Balasubramanian, Sinho Chewi, and Adil Salim. Forwardbackward Gaussian variational inference via JKO in the Bures-Wasserstein Space. In International Conference on Machine Learning, pages 7960\u20137991. PMLR, 2023. (Cited on p. 9, 29, 37, 41, 42, 53)   \n[44] Hanze Dong, Xi Wang, Lin Yong, and Tong Zhang. Particle-based Variational Inference with Preconditioned Functional Gradient Flow. In The Eleventh International Conference on Learning Representations, 2023. (Cited on p. 2, 7, 21)   \n[45] Chao Du, Tianbo Li, Tianyu Pang, Shuicheng Yan, and Min Lin. Nonparametric Generative Modeling with Conditional Sliced-Wasserstein Flows. In International Conference on Machine Learning (ICML), 2023. (Cited on p. 2)   \n[46] Andrew Duncan, Nikolas N\u00fcsken, and Lukasz Szpruch. On the geometry of Stein variational gradient descent. Journal of Machine Learning Research, 24(56):1\u201339, 2023. (Cited on p. 21, 32)   \n[47] Matthias Erbar. The heat equation on manifolds as a gradient flow in the Wasserstein space. Annales de l\u2019Institut Henri Poincar\u00e9, Probabilit\u00e9s et Statistiques, 46(1), February 2010. (Cited on p. 4)   \n[48] Luca Eyring, Dominik Klein, Th\u00e9o Uscidda, Giovanni Palla, Niki Kilbertus, Zeynep Akata, and Fabian J Theis. Unbalancedness in Neural Monge Maps Improves Unpaired Domain Translation. In The Twelfth International Conference on Learning Representations, 2024. (Cited on p. 9)   \n[49] Xingdong Feng, Yuan Gao, Jian Huang, Yuling Jiao, and Xu Liu. Relative Entropy Gradient Sampler for Unnormalized Distribution. Journal of Computational and Graphical Statistics, pages 1\u201316, 2024. (Cited on p. 20, 44)   \n[50] Jean Feydy, Thibault S\u00e9journ\u00e9, Fran\u00e7ois-Xavier Vialard, Shun-ichi Amari, Alain Trouv\u00e9, and Gabriel Peyr\u00e9. Interpolating between Optimal Transport and MMD using Sinkhorn Divergences. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 2681\u20132690. PMLR, 2019. (Cited on p. 2, 10, 43)   \n[51] Nic Fishman, Leo Klarner, Valentin De Bortoli, Emile Mathieu, and Michael John Hutchinson. Diffusion Models for Constrained Domains. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. (Cited on p. 44)   \n[52] Nic Fishman, Leo Klarner, Emile Mathieu, Michael Hutchinson, and Valentin De Bortoli. Metropolis Sampling for Constrained Diffusion Models. Advances in Neural Information Processing Systems, 36, 2024. (Cited on p. 44)   \n[53] Jean-Yves Franceschi, Mike Gartrell, Ludovic Dos Santos, Thibaut Issenhuth, Emmanuel de B\u00e9zenac, Micka\u00ebl Chen, and Alain Rakotomamonjy. Unifying GANs and Score-Based Diffusion as Generative Particle Models. Advances in Neural Information Processing Systems, 36, 2023. (Cited on p. 1)   \n[54] Bela A Frigyik, Santosh Srivastava, and Maya R Gupta. Functional Bregman divergence. In 2008 IEEE International Symposium on Information Theory, pages 1681\u20131685. IEEE, 2008. (Cited on p. 3, 7)   \n[55] Wilfrid Gangbo and Adrian Tudorascu. On differentiability in the Wasserstein space and wellposedness for Hamilton\u2013Jacobi equations. Journal de Math\u00e9matiques Pures et Appliqu\u00e9es, 125:119\u2013174, 2019. (Cited on p. 24)   \n[56] Alfredo Garbuno-Inigo, Franca Hoffmann, Wuchen Li, and Andrew M Stuart. Interacting Langevin Diffusions: Gradient Structure and Ensemble Kalman Sampler. SIAM Journal on Applied Dynamical Systems, 19(1):412\u2013441, 2020. (Cited on p. 7, 21, 32)   \n[57] Xin Guo, Johnny Hong, and Nan Yang. Ambiguity set and learning via Bregman and Wasserstein. arXiv preprint arXiv:1705.08056, 2017. (Cited on p. 21)   \n[58] Gabriele Gut, Markus Herrmann, and Lucas Pelkmans. Multiplexed protein maps link subcellular organization to cellular state. Science (New York, N.Y.), 361, 08 2018. (Cited on p. 10)   \n[59] Johannes Hertrich, Manuel Gr\u00e4f, Robert Beinert, and Gabriele Steidl. Wasserstein steepest descent flows of discrepancies with Riesz kernels. Journal of Mathematical Analysis and Applications, 531(1):127829, March 2024. (Cited on p. 2, 10)   \n[60] Johannes Hertrich, Christian Wald, Fabian Altekr\u00fcger, and Paul Hagemann. Generative Sliced MMD Flows with Riesz Kernels. In The Twelfth International Conference on Learning Representations, 2024. (Cited on p. 2, 10, 43)   \n[61] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models. Advances in neural information processing systems, 33:6840\u20136851, 2020. (Cited on p. 9)   \n[62] Ya-Ping Hsieh, Ali Kavis, Paul Rolland, and Volkan Cevher. Mirrored Langevin Dynamics. Advances in Neural Information Processing Systems, 31, 2018. (Cited on p. 2)   \n[63] Xinru Hua, Truyen Nguyen, Tam Le, Jose Blanchet, and Viet Anh Nguyen. Dynamic Flows on Curved Space Generated by Labeled Data. In Edith Elkind, editor, Proceedings of the ThirtySecond International Joint Conference on Artificial Intelligence, IJCAI-23, pages 3803\u20133811. International Joint Conferences on Artificial Intelligence Organization, 8 2023. Main Track. (Cited on p. 1)   \n[64] Qijia Jiang. Mirror Langevin Monte Carlo: the Case Under Isoperimetry. Advances in Neural Information Processing Systems, 34:715\u2013725, 2021. (Cited on p. 2)   \n[65] Yiheng Jiang, Sinho Chewi, and Aram-Alexandre Pooladian. Algorithms for mean-field variational inference via polyhedral optimization in the Wasserstein space. arXiv preprint arXiv:2312.02849, 2023. (Cited on p. 37)   \n[66] Richard Jordan, David Kinderlehrer, and Felix Otto. The Variational Formulation of the Fokker\u2013Planck Equation. SIAM journal on mathematical analysis, 29(1):1\u201317, 1998. (Cited on p. 1)   \n[67] Mohammad Reza Karimi, Ya-Ping Hsieh, and Andreas Krause. Sinkhorn Flow: A ContinuousTime Framework for Understanding and Generalizing the Sinkhorn Algorithm. arXiv preprint arXiv:2311.16706, 2023. (Cited on p. 2, 21)   \n[68] Jaeyeon Kim, Chanwoo Park, Asuman Ozdaglar, Jelena Diakonikolas, and Ernest K Ryu. Mirror Duality in Convex Optimization. arXiv preprint arXiv:2311.17296, 2023. (Cited on p. 2, 7, 20)   \n[69] Dominik Klein, Th\u00e9o Uscidda, Fabian Theis, and Marco Cuturi. Entropic (Gromov) Wasserstein Flow Matching with GENOT. arXiv preprint arXiv:2310.09254, 2023. (Cited on p. 9)   \n[70] Michal Klein, Aram-Alexandre Pooladian, Pierre Ablin, Eug\u00e8ne Ndiaye, Jonathan Niles-Weed, and Marco Cuturi. Learning elastic costs to shape monge displacements, 2023. (Cited on p. 7)   \n[71] Anna Korba, Adil Salim, Michael Arbel, Giulia Luise, and Arthur Gretton. A Non-Asymptotic Analysis for Stein Variational Gradient Descent. Advances in Neural Information Processing Systems, 33:4672\u20134682, 2020. (Cited on p. 6, 32)   \n[72] Anna Korba, Pierre-Cyril Aubin-Frankowski, Szymon Majewski, and Pierre Ablin. Kernel Stein Discrepancy Descent. In International Conference on Machine Learning, pages 5719\u2013 5730. PMLR, 2021. (Cited on p. 2, 24)   \n[73] Marc Lambert, Sinho Chewi, Francis Bach, Silv\u00e8re Bonnabel, and Philippe Rigollet. Variational inference via Wasserstein gradient flows. Advances in Neural Information Processing Systems, 35:14434\u201314447, 2022. (Cited on p. 9, 42)   \n[74] Nicolas Lanzetti, Saverio Bolognani, and Florian D\u00f6rfler. First-Order Conditions for Optimization in the Wasserstein Space. arXiv preprint arXiv:2209.12197, 2022. (Cited on p. 3, 4, 23, 48)   \n[75] Emanuel Laude and Panagiotis Patrinos. Anisotropic Proximal Gradient. arXiv preprint arXiv:2210.15531, 2022. (Cited on p. 7, 20)   \n[76] Emanuel Laude, Andreas Themelis, and Panagiotis Patrinos. Dualities for Non-Euclidean Smoothness and Strong Convexity under the Light of Generalized Conjugacy. SIAM Journal on Optimization, 33(4):2721\u20132749, 2023. (Cited on p. 7, 20)   \n[77] Flavien L\u00e9ger and Pierre-Cyril Aubin-Frankowski. Gradient Descent with a General Cost. arXiv preprint arXiv:2305.04917, 2023. (Cited on p. 2)   \n[78] Lingxiao Li, Qiang Liu, Anna Korba, Mikhail Yurochkin, and Justin Solomon. Sampling with Mollified Interaction Energy Descent. In The Eleventh International Conference on Learning Representations, 2023. (Cited on p. 2)   \n[79] Ruilin Li, Molei Tao, Santosh S Vempala, and Andre Wibisono. The Mirror Langevin Algorithm Converges with Vanishing Bias. In International Conference on Algorithmic Learning Theory, pages 718\u2013742. PMLR, 2022. (Cited on p. 2)   \n[80] Wuchen Li. Transport Information Bregman Divergences. Information Geometry, 4(2): 435\u2013470, 2021. (Cited on p. 4, 21, 55, 56)   \n[81] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow Matching for Generative Modeling. In The Eleventh International Conference on Learning Representations, 2023. (Cited on p. 9)   \n[82] Guan-Horng Liu, Tianrong Chen, Evangelos Theodorou, and Molei Tao. Mirror Diffusion Models for Constrained and Watermarked Generation. Advances in Neural Information Processing Systems, 36, 2024. (Cited on p. 2, 44)   \n[83] Qiang Liu. Stein Variational Gradient Descent as Gradient Flow. Advances in neural information processing systems, 30, 2017. (Cited on p. 7, 20, 32)   \n[84] Qiang Liu and Dilin Wang. Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm. Advances in neural information processing systems, 29, 2016. (Cited on p. 7, 20, 32)   \n[85] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow. In The Eleventh International Conference on Learning Representations, 2023. (Cited on p. 9)   \n[86] Antoine Liutkus, Umut Simsekli, Szymon Majewski, Alain Durmus, and Fabian-Robert St\u00f6ter. Sliced-Wasserstein Flows: Nonparametric Generative Modeling via Optimal Transport and Diffusions. In International Conference on Machine Learning, pages 4104\u20134113. PMLR, 2019. (Cited on p. 1, 2)   \n[87] Mohammad Lotfollahi, F Alexander Wolf, and Fabian J Theis. scGen predicts single-cell perturbation responses. Nature methods, 16(8):715\u2013721, 2019. (Cited on p. 10)   \n[88] Haihao Lu, Robert M Freund, and Yurii Nesterov. Relatively Smooth Convex Optimization by First-Order Methods, and Applications. SIAM Journal on Optimization, 28(1):333\u2013354, 2018. (Cited on p. 2, 3, 5, 6, 8, 9, 20, 22, 35, 36, 40, 55)   \n[89] Chris J Maddison, Daniel Paulin, Yee Whye Teh, and Arnaud Doucet. Dual Space Preconditioning for Gradient Descent. SIAM Journal on Optimization, 31(1):991\u20131016, 2021. (Cited on p. 2, 3, 5, 7, 8, 20)   \n[90] Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A Mean Field View of the Landscape of Two-Layer Neural Networks. Proceedings of the National Academy of Sciences, 115(33): E7665\u2013E7671, 2018. (Cited on p. 1)   \n[91] Pierre Monmarch\u00e9 and Julien Reygner. Local convergence rates for Wasserstein gradient flows and McKean-Vlasov equations with multiple stationary solutions. arXiv preprint arXiv:2404.15725, 2024. (Cited on p. 5)   \n[92] Arkadij Semenovic\u02c7 Nemirovskij and David Borisovich Yudin. Problem complexity and method efficiency in optimization. Wiley, 1983. (Cited on p. 2, 20)   \n[93] Sebastian Neumayer, Viktor Stein, and Gabriele Steidl. Wasserstein Gradient Flows for Moreau Envelopes of f-Divergences in Reproducing Kernel Hilbert Spaces. arXiv preprint arXiv:2402.04613, 2024. (Cited on p. 2)   \n[94] Maxence Noble, Valentin De Bortoli, and Alain Durmus. Unbiased constrained sampling with Self-Concordant Barrier Hamiltonian Monte Carlo. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. (Cited on p. 44)   \n[95] Felix Otto. Double degenerate diffusion equations as steepest descent. Citeseer, 1996. (Cited on p. 1)   \n[96] Felix Otto. The Geometry of Dissipative Evolution Equations: the Porous Medium Equation. Communications in Partial Differential Equations, 26(1-2):101\u2013174, 2001. (Cited on p. 2)   \n[97] Felix Otto and C\u00e9dric Villani. Generalization of an inequality by Talagrand and links with the logarithmic Sobolev inequality. Journal of Functional Analysis, 173(2):361\u2013400, 2000. (Cited on p. 24)   \n[98] Guy Parker. Some Convexity Criteria for Differentiable Functions on the 2-Wasserstein Space. arXiv preprint arXiv:2306.09120, 2023. (Cited on p. 5, 26)   \n[99] Juan Peypouquet. Convex Optimization in Normed Spaces: Theory, Methods and Examples. Springer, 2015. (Cited on p. 22, 33, 54, 55)   \n[100] Gabriel Peyr\u00e9. Entropic approximation of Wasserstein gradient flows. SIAM Journal on Imaging Sciences, 8(4):2323\u20132351, 2015. (Cited on p. 20)   \n[101] Aram-Alexandre Pooladian and Jonathan Niles-Weed. Entropic estimation of optimal transport maps. arXiv preprint arXiv:2109.12004, 2021. (Cited on p. 42, 43)   \n[102] Julien Rabin, Gabriel Peyr\u00e9, Julie Delon, and Marc Bernot. Wasserstein Barycenter and its Application to Texture Mixing. In Scale Space and Variational Methods in Computer Vision: Third International Conference, SSVM 2011, Ein-Gedi, Israel, May 29\u2013June 2, 2011, Revised Selected Papers 3, pages 435\u2013446. Springer, 2012. (Cited on p. 2, 43)   \n[103] Cale Rankin and Ting-Kam Leonard Wong. Bregman-Wasserstein Divergence: Geometry and Applications. arXiv preprint arXiv:2302.05833, 2023. (Cited on p. 6, 21, 29)   \n[104] Cale Rankin and Ting-Kam Leonard Wong. JKO schemes with general transport costs. arXiv preprint arXiv:2402.17681, 2024. (Cited on p. 5, 20, 21, 29)   \n[105] Maria L. Rizzo and G\u00e1bor J. Sz\u00e9kely. Energy Distance. WIREs Computational Statistics, 8(1): 27\u201338, 2016. (Cited on p. 10)   \n[106] Gareth O Roberts and Richard L Tweedie. Exponential convergence of Langevin distributions and their discrete approximations. Bernoulli, pages 341\u2013363, 1996. (Cited on p. 1)   \n[107] Grant M Rotskoff and Eric Vanden-Eijnden. Neural networks as interacting particle systems: Asymptotic convexity of the loss landscape and universal scaling of the approximation error. stat, 1050:22, 2018. (Cited on p. 1)   \n[108] Adil Salim and Peter Richt\u00e1rik. Primal dual interpretation of the proximal stochastic gradient Langevin algorithm. Advances in Neural Information Processing Systems, 33:3786\u20133796, 2020. (Cited on p. 4)   \n[109] Adil Salim, Anna Korba, and Giulia Luise. The Wasserstein Proximal Gradient Algorithm. Advances in Neural Information Processing Systems, 33:12356\u201312366, 2020. (Cited on p. 37, 38, 42)   \n[110] Filippo Santambrogio. Optimal Transport for Applied Mathematicians, volume 55. Springer, 2015. (Cited on p. 4, 7, 23)   \n[111] Filippo Santambrogio. Euclidean, metric, and Wasserstein gradient flows: an overview. Bulletin of Mathematical Sciences, 7:87\u2013154, 2017. (Cited on p. 1)   \n[112] Geoffrey Schiebinger, Jian Shu, Marcin Tabaka, Brian Cleary, Vidya Subramanian, Aryeh Solomon, Joshua Gould, Siyan Liu, Stacie Lin, Peter Berube, et al. Optimal-Transport Analysis of Single-Cell Gene Expression Identifies Developmental Trajectories in Reprogramming. Cell, 176(4), 2019. (Cited on p. 9)   \n[113] Louis Sharrock, Lester Mackey, and Christopher Nemeth. Learning Rate Free Bayesian Inference in Constrained Domains. Advances in Neural Information Processing Systems, 36, 2024. (Cited on p. 2, 7, 32, 44)   \n[114] Jiaxin Shi, Chang Liu, and Lester Mackey. Sampling with Mirrored Stein Operators. In International Conference on Learning Representations, 2022. (Cited on p. 2, 7, 32, 44)   \n[115] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-Based Generative Modeling through Stochastic Differential Equations. In International Conference on Learning Representations, 2021. (Cited on p. 9)   \n[116] Vishwak Srinivasan, Andre Wibisono, and Ashia Wilson. Fast sampling from constrained spaces using the Metropolis-adjusted Mirror Langevin algorithm. In Proceedings of Thirty Seventh Conference on Learning Theory, volume 247, pages 4593\u20134635, 2024. (Cited on p. 2, 44)   \n[117] Ken\u2019ichiro Tanaka. Accelerated gradient descent method for functionals of probability measures by new convexity and smoothness based on transport maps. arXiv preprint arXiv:2305.05127, 2023. (Cited on p. 5, 26, 38)   \n[118] Fuchou Tang, Catalin Barbacioru, Yangzhou Wang, Ellen Nordman, Clarence Lee, Nanlan Xu, Xiaohui Wang, John Bodeau, Brian B Tuch, Asim Siddiqui, et al. mRNA-Seq wholetranscriptome analysis of a single cell. Nature methods, 6(5):377\u2013382, 2009. (Cited on p. 10)   \n[119] Salma Tarmoun, Stewart Slocum, Benjamin David Haeffele, and Rene Vidal. Gradient Preconditioning for Non-Lipschitz smooth Nonconvex Optimization. 2022. (Cited on p. 7, 10, 20)   \n[120] Antonio Terpin, Nicolas Lanzetti, and Florian D\u00f6rfler. Learning Diffusion at Lightspeed. Advances in Neural Information Processing Systems, 2024. (Cited on p. 1)   \n[121] Belinda Tzen, Anant Raj, Maxim Raginsky, and Francis Bach. Variational Principles for Mirror Descent and Mirror Langevin Dynamics. IEEE Control Systems Letters, 7:1542\u20131547, 2023. (Cited on p. 2)   \n[122] Th\u00e9o Uscidda and Marco Cuturi. The Monge Gap: A Regularizer to Learn All Transport Maps. In International Conference on Machine Learning, pages 34709\u201334733. PMLR, 2023. (Cited on p. 9)   \n[123] Quang Van Nguyen. Forward-backward splitting with Bregman distances. Vietnam Journal of Mathematics, 45(3):519\u2013539, 2017. (Cited on p. 6)   \n[124] C\u00e9dric Villani. Topics in Optimal Transportation, volume 58. American Mathematical Soc., 2003. (Cited on p. 24)   \n[125] C\u00e9dric Villani. Optimal Transport: Old and New, volume 338. Springer, 2009. (Cited on p. 51)   \n[126] Yifei Wang and Wuchen Li. Information Newton\u2019s Flow: Second-Order Optimization Method in Probability Space. arXiv preprint arXiv:2001.04341, 2020. (Cited on p. 5, 21, 24, 25, 29)   \n[127] Yifei Wang, Peng Chen, and Wuchen Li. Projected Wasserstein gradient descent for highdimensional Bayesian inference. SIAM/ASA Journal on Uncertainty Quantification, 10(4): 1513\u20131532, 2022. (Cited on p. 20, 44)   \n[128] Yifei Wang, Peng Chen, Mert Pilanci, and Wuchen Li. Optimal Neural Network Approximation of Wasserstein Gradient Direction via Convex Optimization. arXiv preprint arXiv:2205.13098, 2022. (Cited on p. 20, 44)   \n[129] Andre Wibisono. Sampling as optimization in the space of measures: The Langevin dynamics as a composite optimization problem. In Conference on Learning Theory, pages 2093\u20133027. PMLR, 2018. (Cited on p. 1, 4, 31, 42)   \n[130] Andre Wibisono. Proximal Langevin Algorithm: Rapid Convergence under Isoperimetry. arXiv preprint arXiv:1911.01469, 2019. (Cited on p. 5, 29)   \n[131] Lantian Xu, Anna Korba, and Dejan Slepcev. Accurate Quantization of Measures via Interacting Particle-based Optimization. In International Conference on Machine Learning, pages 24576\u201324595. PMLR, 2022. (Cited on p. 7)   \n[132] Kelvin Shuangjian Zhang, Gabriel Peyr\u00e9, Jalal Fadili, and Marcelo Pereyra. Wasserstein Control of Mirror Langevin Monte Carlo. In Conference on Learning Theory, pages 3814\u2013 3841. PMLR, 2020. (Cited on p. 2)   \n[133] Zico Zolter, David Duvenaud, and Matt Johnson. Deep Implicit Layers - Neural ODEs, Deep Equilibirum Models, and Beyond. Neurips 2020 Tutorial, 2020. (Cited on p. 39) ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The appendix is organized as follows. In Appendix A, we discuss related works. In Appendix B and Appendix C, we provide mathematical background respectively on $L^{2}$ and on the Wasserstein space. In Appendix D, we provide complementary results for the mirror descent scheme on the Wasserstein space. In Appendix E, we discuss the relative convexity and smoothness between functionals. In Appendix F, we study the Bregman proximal gradient scheme, deriving a convergence result and closed-form updates for the Gaussian case. In Appendix G, we provide more details on the experiments. In Appendix H, we report the proofs of our results. Finally, auxiliary results are given in Appendix I. ", "page_idx": 18}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "A Related works 20 ", "page_idx": 18}, {"type": "text", "text": "B Background on $L^{2}(\\mu)$ 21 ", "page_idx": 18}, {"type": "text", "text": "B.1 Differential calculus on $L^{2}(\\mu)$ 21   \nB.2 Convexity on $L^{2}(\\mu)$ . . . 22 ", "page_idx": 18}, {"type": "text", "text": "C Background on Wasserstein space 23 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "C.1 Wasserstein differentials 23   \nC.2 Wasserstein Hessians 24   \nC.3 Convexity in Wasserstein space . . 25 ", "page_idx": 18}, {"type": "text", "text": "D Additional results on mirror descent 27 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "D.1 Optimal transport maps for mirror descent 27   \nD.2 Continuous formulation . . . 29   \nD.3 Derivation in specific settings . . . 29   \nD.4 Mirror scheme with non-pushforward compatible Bregman potentials 32 ", "page_idx": 18}, {"type": "text", "text": "E Relative convexity and smoothness 33 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "E.1 Relative convexity and smoothness between Fenchel transforms 33   \nE.2 Relative convexity and smoothness between functionals . . 35 ", "page_idx": 18}, {"type": "text", "text": "F Bregman proximal gradient scheme 37 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "G Additional details on experiments 39 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "G.1 Implementing the schemes . . 39   \nG.2 Mirror descent of interaction energies 40   \nG.3 Mirror descent on Gaussians . . 41   \nG.4 Single-cell experiments . . . 42   \nG.5 Mirror descent on the simplex 44 ", "page_idx": 18}, {"type": "text", "text": "H Proofs 44 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "H.1 Proof of Proposition 1 44 ", "page_idx": 18}, {"type": "text", "text": "H.2 Proof of Proposition 2 . . . . 45   \nH.3 Proof of Proposition 3 . . . . 45   \nH.4 Proof of Proposition 4 . . . . 46   \nH.5 Proof of Proposition 5 . . . . 47   \nH.6 Proof of Proposition 6 . . . . 48   \nH.7 Proof of Proposition 7 . . . . 48   \nH.8 Proof of Lemma 11 . . 49   \nH.9 Proof of Proposition 12 . . . 50   \nH.10 Proof of Proposition 13 . . . 50   \nH.11 Proof of Proposition 24 . . . 51   \nH.12 Proof of Proposition 25 . . 52   \nH.13 Proof of Lemma 26 . . 53   \nH.14 Proof of Proposition 27 . . 53 ", "page_idx": 19}, {"type": "text", "text": "I Additional results 54 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "I.1 Three-point identity and inequality 54   \nI.2 Convergence lemma . . 55   \nI.3 Some properties of Bregman divergences 55 ", "page_idx": 19}, {"type": "text", "text": "J NeurIPS Paper Checklist 58 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "A Related works ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Mirror descent on $\\mathbb{R}^{d}$ . Mirror descent has been introduced by Nemirovskij and Yudin [92] to solve convex optimization problems. Its convergence has been first studied under the assumption that the objective has a Lipschitz gradient, see e.g. [12]. More recently, Bauschke et al. [11], Lu et al. [88] provided convergence guarantees by assuming relative smoothness and convexity. ", "page_idx": 19}, {"type": "text", "text": "Preconditioned gradient descent on $\\mathbb{R}^{d}$ . The preconditioned gradient descent has first been studied by Maddison et al. [89], providing convergence guarantees under assumptions on the smoothness and convexity of the preconditioner relative to the Legendre transform of the objective. Kim et al. [68] underlined connections with the mirror descent, and introduced an accelerated version of the preconditioned gradient descent. Laude and Patrinos [75], Laude et al. [76] studied a generalized version of this algorithm by minimizing an anisotropic upper bound and supposing anisotropic smoothness of the objective. In particular, their analysis for the descent lemma is also valid for a non-convex smooth objective. Tarmoun et al. [119] also studied preconditioned gradient descent for non-Lipschitz smooth non-convex problems. ", "page_idx": 19}, {"type": "text", "text": "Wasserstein Gradient flows with respect to non-Euclidean geometries. Several existing schemes are based on time-discretizations of gradient flows with respect to optimal transport metrics, but different than the Wasserstein-2 distance. ", "page_idx": 19}, {"type": "text", "text": "To simplify the computation of the backward scheme, Peyr\u00e9 [100] added an entropic regularization into the JKO scheme while Bonet et al. [14] considered using the Sliced-Wasserstein distance instead. More recently, Rankin and Wong [104] suggested using Bregman divergences e.g. when geodesic distances are not known in closed-forms. ", "page_idx": 19}, {"type": "text", "text": "The most popular objective in Wasserstein gradient flows is the KL. However, this can be intricate to compute as it requires the evaluation of the density at each step, which is not known for particles, and thus requires approximations using kernel density estimators [127] or density ratio estimators [6, 49, 128]. Restricting the velocity field to a reproducing kernel Hilbert space (RKHS), an update in closed-form can be obtained, which is given by the SVGD algorithm [83, 84]. This algorithm can also be seen as using an alternative Wasserstein metric [46]. However, the restriction to RKHS can hinder the flexibility of the method. This motivated the introduction of new schemes based on using the Wasserstein distance with a convex translation invariant cost [31, 44]. Particle systems preconditioned by they empirical covariance matrix have also been recently considered, and can be seen as discretization of the Kalman-Wasserstein or Covariance Modulated gradient flow [24, 56]. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "Mirror descent with flat geometry. The space of probability distributions can be endowed with different metrics. When endowed with the Fisher-Rao metric instead of the Wasserstein distance, the geometry becomes very different. Notably, the shortest path between the two distributions is now a mixture between them. In this situation, the gradient is the first variation. Aubin-Frankowski et al. [9] studied the mirror descent in this space and notably showed connections with the Sinkhorn algorithm when the mirror map and the optimized functionals are KL divergences. Karimi et al. [67] extended the mirror descent algorithm for more general time steps, and notably recovered the \u201cWasserstein Mirror Flow\u201d proposed by Deb et al. [41] as a special case. ", "page_idx": 20}, {"type": "text", "text": "Bregman divergence on $\\mathcal{P}_{2}(\\mathbb{R}^{d})$ . Several works introduced Bregman divergences on $\\mathcal{P}_{2}(\\mathbb{R}^{d})$ . Carlier and Jimenez [25] first studied the existence of Monge maps for the OT problem with Bregman costs $c(x,y)\\,=\\,\\mathrm{d}_{V}(x,y)$ and symmetrized Bregman costs $\\bar{c(x,y)}\\,=\\,\\mathrm{d}_{V}(\\bar{x},y)+\\mathrm{d}_{V}(y,x)$ . For Bregman costs, the resulting OT problem was named the Bregman-Wasserstein divergence and its properties were studied in [37, 57, 103]. The Bregman-Wasserstein divergence has also been used by Ahn and Chewi [3] to show the convergence of the Mirror Langevin algorithm while Rankin and Wong [104] studied its JKO scheme with KL objective. Li [80] introduced the notion of Bregman divergence on Wasserstein space for a geodesically strictly convex $\\mathcal{F}:\\mathcal{P}_{2}(\\mathbb{R}^{d})\\to\\mathbb{R}$ as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\forall\\mu,\\nu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d}),\\;\\mathrm{d}_{\\mathcal{F}}(\\mu,\\nu)=\\mathcal{F}(\\mu)-\\mathcal{F}(\\nu)-\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\nu),\\mathrm{T}_{\\nu}^{\\mu}-\\mathrm{Id}\\rangle_{L^{2}(\\nu)},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\mathrm{T}_{\\nu}^{\\mu}$ is the OT map between $\\nu$ and $\\mu\\ w.r.t\\ \\mathrm{W_{2}}$ . The Bregman divergence used in our work and as defined in Definition 1 is more general as it allows using more general maps and contains as special case (16). Li [80] studied properties of this Bregman divergence for different functionals $\\mathcal{F}$ and provided closed-forms for one-dimensional distributions or Gaussian, but did not use it to define a mirror scheme. ", "page_idx": 20}, {"type": "text", "text": "Mirror descent on $\\mathcal{P}_{2}(\\mathbb{R}^{d})$ . Deb et al. [41] defined a mirror flow by using the continuous formulation. They focused on KL objectives with Bregman potential $\\begin{array}{r}{\\phi(\\bar{\\mu})=\\frac{1}{2}\\mathrm{\\bar{W}}_{2}^{2}(\\mu,\\nu)}\\end{array}$ with some reference measure $\\nu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ , and defined the flow as the solution of ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\varphi(\\mu_{t})=\\nabla_{\\mathrm{W}_{2}}\\phi(\\mu_{t})}\\\\ {\\frac{\\mathrm{d}}{\\mathrm{d}t}\\varphi(\\mu_{t})=-\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{t}).}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We note that $\\phi$ is pushforward compatible and hence enters our framework. Also related to our work, Wang and Li [126] studied a Wasserstein Newton\u2019s flow, which, analogously to the relation between Newton\u2019s method and mirror descent [32], is another discretization of our scheme for $\\phi={\\mathcal{F}}$ . We clarify the link with the Mirror Descent algorithm we define in this work with the previous continuous formulation above in Appendix D.2. ", "page_idx": 20}, {"type": "text", "text": "B Background on $L^{2}(\\mu)$ ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "B.1 Differential calculus on $L^{2}(\\mu)$ ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We recall some differentiability definitions on the Hilbert space $L^{2}(\\mu)$ for $\\mu\\ \\in\\ \\mathcal{P}_{2}(\\mathbb{R}^{d})$ . Let $\\phi:L^{2}(\\mu)\\to\\mathbb{R}$ . We start by recalling the notions of G\u00e2teaux and Fr\u00e9chet derivatives. ", "page_idx": 20}, {"type": "text", "text": "Definition 4. A function $\\phi:L^{2}(\\mu)\\to\\mathbb{R}$ is said to be G\u00e2teaux differentiable at $T$ if there exists an operator $\\phi^{\\prime}(\\mathrm{T}):L^{2}(\\mu)\\rightarrow\\mathbb{R}$ such that for any direction $h\\in L^{2}(\\mu)$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\phi^{\\prime}(\\mathrm{T})(h)=\\operatorname*{lim}_{t\\rightarrow0}\\ {\\frac{\\phi(\\mathrm{T}+t h)-\\phi(\\mathrm{T})}{t}},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and $\\phi^{\\prime}(\\mathrm{T})$ is a linear function. The operator $\\phi^{\\prime}(\\mathrm{T})$ is called the G\u00e2teaux derivative of $\\phi$ at $\\mathrm{T}$ and if it exists, it is unique. ", "page_idx": 20}, {"type": "text", "text": "Definition 5. The Fr\u00e9chet derivative of $\\dot{\\phi}$ at $\\mathrm{T}\\in L^{2}(\\mu)$ in the direction $h\\in L^{2}(\\mu)$ , denoted $\\delta\\phi(\\mathrm{T},h)$ , is defined implicitly by ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\phi(\\mathrm{T}+t h)=\\phi(\\mathrm{T})+t\\delta\\phi(\\mathrm{T},h)+t o(\\|h\\|).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "If $\\phi$ is Fr\u00e9chet differentiable, then it is also G\u00e2teaux differentiable, and both derivatives agree, i.e. for all $\\mathrm{T},h\\in L^{2}(\\mu)$ , $\\delta\\phi(\\mathrm{T},h)=\\phi^{\\prime}(\\mathrm{T})(h)$ [99, Proposition 1.26]. ", "page_idx": 21}, {"type": "text", "text": "Moreover, since $L^{2}(\\mu)$ is a Hilbert space, and $\\delta\\phi(\\mathrm{{T},\\cdot})$ and $\\phi^{\\prime}(\\mathrm{T})$ are linear and continuous, if $\\phi$ is Fr\u00e9chet (resp. G\u00e2teaux) differentiable, by the Riesz representation theorem, there exists $\\nabla\\phi\\in L^{2}(\\mu)$ such that for all $h\\in L^{2}(\\mu),\\delta\\phi(\\mathrm{T},h)=\\langle\\nabla\\phi(\\mathrm{T}),h\\rangle\\dot{\\Sigma}^{2}(\\mu)$ (resp. $\\phi^{\\prime}(\\mathrm{T})(h)=\\langle\\nabla\\phi(\\mathrm{T}),h\\rangle_{L^{2}(\\mu)})$ . ", "page_idx": 21}, {"type": "text", "text": "As a brief comment on these notions in the context of convexity, if the subdifferential of a convex $f$ at $x$ contains a single element then it is the G\u00e2teaux derivative and we have an inequality $f(y)\\geq$ $f(x)+\\langle\\nabla f(x),y-x\\rangle$ . Instead Fr\u00e9chet diff\u00e9rentiability gives an equality (19) corresponding to a series expansion. ", "page_idx": 21}, {"type": "text", "text": "B.2 Convexity on $L^{2}(\\mu)$ ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Let $\\phi\\,:\\,L^{2}(\\mu)\\,\\to\\,\\mathbb{R}$ be G\u00e2teaux differentiable. We recall that $\\phi$ is convex if for all $t\\,\\in\\,[0,1]$ , $\\mathrm{T},\\mathrm{S}\\in L^{2}(\\mu)$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\phi\\big((1-t)\\mathrm{T}+t\\mathrm{S}\\big)\\leq(1-t)\\phi(\\mathrm{T})+t\\phi(\\mathrm{S}),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which is equivalent by [99, Proposition 3.10] with ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\forall{\\mathrm{T}},{\\mathrm{S}}\\in L^{2}(\\mu),\\;\\phi({\\mathrm{T}})\\geq\\phi({\\mathrm{S}})+\\langle\\nabla\\phi({\\mathrm{S}}),{\\mathrm{T}}-{\\mathrm{S}}\\rangle\\rangle_{L^{2}(\\mu)}\\iff\\mathrm{d}_{\\phi}({\\mathrm{T}},{\\mathrm{S}})\\geq0.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We now present equivalent definitions of the relative smoothness and relative convexity, which is the equivalent of [88, Proposition 1.1]. ", "page_idx": 21}, {"type": "text", "text": "Proposition 8. Let $\\psi,\\phi:L^{2}(\\mu)\\to\\mathbb{R}$ be convex and G\u00e2teaux differentiable functions. The following conditions are equivalent: ", "page_idx": 21}, {"type": "text", "text": "(a1) $\\psi$ is $\\beta$ -smooth relative to $\\phi$ ", "page_idx": 21}, {"type": "text", "text": "$\\mathbf{\\sigma}(a2)$ $\\beta\\phi-\\psi$ is a convex function on $L^{2}(\\mu)$ ", "page_idx": 21}, {"type": "text", "text": "$\\left(a3\\right)$ If twice G\u00e2teaux differentiable, $\\langle\\nabla^{2}\\psi(\\mathrm{T})\\mathrm{S},\\mathrm{S}\\rangle_{L^{2}(\\mu)}\\leq\\beta\\langle\\nabla^{2}\\phi(\\mathrm{T})\\mathrm{S},\\mathrm{S}\\rangle_{L^{2}(\\mu)}$ for all $\\mathrm{T},\\mathrm{S}\\in$ $L^{2}(\\mu)$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\langle\\nabla\\psi({\\mathrm{T}})-\\nabla\\psi({\\mathrm{S}}),{\\mathrm{T}}-\\mathrm{S}\\rangle_{L^{2}(\\mu)}\\leq\\beta\\langle\\nabla\\phi({\\mathrm{T}})-\\nabla\\phi({\\mathrm{S}}),{\\mathrm{T}}-\\mathrm{S}\\rangle_{L^{2}(\\mu)}\\,f o r\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The following conditions are equivalent: ", "page_idx": 21}, {"type": "text", "text": "(b1) $\\psi$ is $\\alpha$ -convex relative to $\\phi$ ", "page_idx": 21}, {"type": "text", "text": "(b2) $\\psi-\\alpha\\phi$ is a convex function on $L^{2}(\\mu)$ ", "page_idx": 21}, {"type": "text", "text": "(b3) If twice differentiable, $\\langle\\nabla^{2}\\psi(\\mathrm{T})\\mathrm{S},\\mathrm{S}\\rangle_{L^{2}(\\mu)}\\ge\\alpha\\langle\\nabla^{2}\\phi(\\mathrm{T})\\mathrm{S},\\mathrm{S}\\rangle_{L^{2}(\\mu)}.$ for all T, S \u2208L2(\u00b5) ", "page_idx": 21}, {"type": "equation", "text": "$$\n(b\\mathcal{A})\\ \\langle\\nabla\\psi(\\mathrm{T})-\\nabla\\psi(\\mathrm{S}),\\mathrm{T}-\\mathrm{S}\\rangle_{L^{2}(\\mu)}\\ge\\alpha\\langle\\nabla\\phi(\\mathrm{T})-\\nabla\\phi(\\mathrm{S}),\\mathrm{T}-\\mathrm{S}\\rangle_{L^{2}(\\mu)}\\,f o r\\,a l l\\,\\mathrm{T},\\mathrm{S}\\in L^{2}(\\mu).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. We do it only for the smoothness. It holds likewise for the convexity. ", "page_idx": 21}, {"type": "text", "text": "(a1) $\\Longleftrightarrow(\\mathbf{a}2)$ : ", "text_level": 1, "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall\\mathrm{T},\\mathrm{S}\\in L^{2}(\\mu),\\;\\mathrm{d}_{\\psi}(\\mathrm{T},\\mathrm{S})\\leq\\beta\\mathrm{d}_{\\phi}(\\mathrm{T},\\mathrm{S})}\\\\ &{\\iff\\forall\\mathrm{T},\\mathrm{S}\\in L^{2}(\\mu),\\;\\psi(\\mathrm{T})-\\psi(\\mathrm{S})-\\langle\\nabla\\psi(\\mathrm{S}),\\mathrm{T}-\\mathrm{S}\\rangle_{L^{2}(\\mu)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\beta\\big(\\phi(\\mathrm{T})-\\phi(\\mathrm{S})-\\langle\\nabla\\phi(\\mathrm{S}),\\mathrm{T}-\\mathrm{S}\\rangle_{L^{2}(\\mu)}\\big)}\\\\ &{\\iff\\forall\\mathrm{T},\\mathrm{S}\\in L^{2}(\\mu),\\;(\\beta\\phi-\\psi)(\\mathrm{S})-\\langle\\nabla(\\beta\\phi-\\psi)(\\mathrm{S}),\\mathrm{T}-\\mathrm{S}\\rangle_{L^{2}(\\mu)}\\leq(\\beta\\phi-\\psi)(\\mathrm{T}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For the rest of the equivalences, we apply [99, Proposition 3.10]. Indeed, $\\beta\\phi-\\psi$ convex is equivalent to ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall\\mathrm{T},\\mathrm{S}\\in L^{2}(\\mu),\\ \\langle\\nabla(\\beta\\phi-\\psi)(\\mathrm{T})-\\nabla(\\beta\\phi-\\psi)(\\mathrm{S}),\\mathrm{T}-\\mathrm{S}\\rangle_{L^{2}(\\mu)}\\geq0}\\\\ &{\\iff\\forall\\mathrm{T},\\mathrm{S}\\in L^{2}(\\mu),\\ \\beta\\langle\\nabla\\phi(\\mathrm{T})-\\nabla\\phi(\\mathrm{S}),\\mathrm{T}-\\mathrm{S}\\rangle_{L^{2}(\\mu)}\\geq\\langle\\nabla\\psi(\\mathrm{T})-\\nabla\\psi(\\mathrm{S}),\\mathrm{T}-\\mathrm{S}\\rangle_{L^{2}(\\mu)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which gives the equivalence between (a2) and (a4). And if $\\psi$ and $\\phi$ are twice differentiables, it is also equivalent to ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall\\mathrm{T},\\mathrm{S}\\in L^{2}(\\mu),\\;\\langle\\nabla^{2}(\\beta\\phi-\\psi)(\\mathrm{T})\\mathrm{S},\\mathrm{S}\\rangle_{L^{2}(\\mu)}\\geq0}\\\\ &{\\qquad\\qquad\\iff\\forall\\mathrm{T},\\mathrm{S}\\in L^{2}(\\mu),\\;\\beta\\langle\\nabla^{2}\\phi(\\mathrm{T})\\mathrm{S},\\mathrm{S}\\rangle_{L^{2}(\\mu)}\\geq\\langle\\nabla^{2}\\psi(\\mathrm{T})\\mathrm{S},\\mathrm{S}\\rangle_{L^{2}(\\mu)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which gives the equivalence between (a2) and (a3). ", "page_idx": 22}, {"type": "text", "text": "C Background on Wasserstein space ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "C.1 Wasserstein differentials ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We recall the notion of Wasserstein differentiability introduced in [5, 17, 74]. First, we introduce suband super-differentials. ", "page_idx": 22}, {"type": "text", "text": "Definition 6 (Wasserstein sub- and super-differential [17, 74]). Let $\\mathcal{F}:\\mathcal{P}_{2}(\\mathbb{R}^{d})\\,\\rightarrow\\,(-\\infty,+\\infty]$ lower semicontinuous and denote $D(\\mathcal{F})=\\{\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ , $\\mathcal F(\\mu)<\\infty\\}$ . Let $\\mu\\in D(\\mathcal{F})$ . Then, $a$ map $\\xi\\in L^{2}(\\mu)$ belongs to the subdifferential $\\partial^{-}\\mathcal{F}(\\mu)$ of $\\mathcal{F}$ at $\\mu$ if for all $\\nu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathscr{F}(\\nu)\\geq\\mathscr{F}(\\mu)+\\operatorname*{sup}_{\\gamma\\in\\Pi_{o}(\\mu,\\nu)}\\int\\langle\\xi(x),y-x\\rangle\\;\\mathrm{d}\\gamma(x,y)+o\\bigl(\\mathrm{W}_{2}(\\mu,\\nu)\\bigr).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Similarly, $\\xi\\in L^{2}(\\mu)$ belongs to the superdifferential $\\partial^{+}\\mathcal{F}(\\mu)$ of $\\mathcal{F}\\,a t\\,\\mu\\,i f\\!-\\!\\xi\\in\\partial^{-}(-\\mathcal{F})(\\mu)$ . ", "page_idx": 22}, {"type": "text", "text": "Then, we say that a functional is Wasserstein differentiable if it admits sub and super differentials which coincide. ", "page_idx": 22}, {"type": "text", "text": "Definition 7 (Wasserstein differentiability, Definition 2.3 in [74]). A functional $\\mathcal{F}:\\mathcal{P}_{2}(\\mathbb{R}^{d})\\to\\mathbb{R}$ is Wasserstein differentiable at $\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ if $\\partial^{-}{\\mathcal{F}}(\\mu)\\cap\\partial^{+}{\\mathcal{F}}(\\mu)\\neq\\emptyset$ . In this case, we say that $\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu)\\in\\partial^{-}\\mathcal{F}(\\mu)\\cap\\partial^{+}\\mathcal{F}(\\mu)$ is $a$ Wasserstein gradient of $\\mathcal{F}$ at $\\mu_{\\cdot}$ , satisfying for any $\\nu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ , $\\gamma\\in\\Pi_{o}(\\mu,\\nu)$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{F}(\\nu)=\\mathcal{F}(\\mu)+\\int\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu)(x),y-x\\rangle\\;\\mathrm{d}\\gamma(x,y)+o\\big(\\mathrm{W}_{2}(\\mu,\\nu)\\big).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Recall that the tangent space of $\\mathcal{P}_{2}(\\mathbb{R}^{d})$ at $\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ is defined as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{T}_{\\mu}\\mathcal{P}_{2}(\\mathbb{R}^{d})=\\overline{{\\{\\nabla\\psi,~\\psi\\in\\mathcal{C}_{c}^{\\infty}(\\mathbb{R}^{d})\\}}}\\subset L^{2}(\\mu),}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the closure is taken in $L^{2}(\\mu)$ , see Ambrosio et al. [5, Definition 8.4.1]. Lanzetti et al. [74, Proposition 2.5] showed that if $\\mathcal{F}$ is Wasserstein differentiable, then there is always a unique gradient living in the tangent space, and we can restrict ourselves without loss of generality to this gradient. ", "page_idx": 22}, {"type": "text", "text": "Lanzetti et al. [74] further showed that Wasserstein gradients provide linear approximations even if the perturbations are not induced by OT plans, i.e. differentials are \u201cstrong Fr\u00e9chet differentials\u201d. ", "page_idx": 22}, {"type": "text", "text": "Proposition 9 (Proposition 2.6 in [74]). Let $\\mu,\\nu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d}).$ , $\\gamma\\in\\Pi(\\mu,\\nu)$ any coupling and let ${\\mathcal{F}}:$ $\\mathcal{P}_{2}(\\mathbb{R}^{d})\\rightarrow\\mathbb{R}$ be Wasserstein differentiable at $\\mu$ with Wasserstein gradient $\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu)\\in T_{\\mu}\\mathcal{P}_{2}(\\mathbb{R}^{d})$ . Then, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{F}(\\nu)=\\mathcal{F}(\\mu)+\\int\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu)(x),y-x\\rangle\\;\\mathrm{d}\\gamma(x,y)+o\\left(\\sqrt{\\int\\|x-y\\|_{2}^{2}\\;\\mathrm{d}\\gamma(x,y)}\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Under regularity assumptions, the Wasserstein gradient of $\\mathcal{F}$ can be computed in practice using the first variatio  \u03b4\u03b4F\u00b5 n [110, Definition 7.12], which is defined, if it exists, as the unique function (up to a constant) such that, for $\\chi$ satisfying $\\textstyle\\int\\mathrm{d}\\chi=0$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\mathcal{F}(\\mu+t\\chi)\\Big|_{t=0}=\\operatorname*{lim}_{t\\to0}\\,\\frac{\\mathcal{F}(\\mu+t\\chi)-\\mathcal{F}(\\mu)}{t}=\\int\\frac{\\delta\\mathcal{F}}{\\delta\\mu}(\\mu)\\;\\mathrm{d}\\chi.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then the Wasserstein gradient can be computed as $\\begin{array}{r}{\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu)=\\nabla\\frac{\\delta\\mathcal{F}}{\\delta\\mu}(\\mu)}\\end{array}$ , see e.g. [34, Proposition 5.10]. ", "page_idx": 22}, {"type": "text", "text": "We now show that we can relate the Fr\u00e9chet derivative of $\\tilde{\\mathcal{F}}_{\\mu}(\\mathrm{T}):=\\mathcal{F}(\\mathrm{T}_{\\#}\\mu)$ with the Wasserstein gradient of $\\mathcal{F}$ belonging to the tangent space of $\\mathcal{P}_{2}(\\mathbb{R}^{d})$ at $\\mu$ . ", "page_idx": 22}, {"type": "text", "text": "Proposition 10. Let $\\mathcal{F}:\\mathcal{P}_{2}(\\mathbb{R}^{d})\\to\\mathbb{R}\\cup\\{+\\infty\\}$ be a Wasserstein differentiable functional on $D({\\mathcal{F}})$ . Let $\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ and $\\tilde{\\mathcal{F}}_{\\mu}(\\mathrm{T})=\\mathcal{F}(\\mathrm{T}_{\\#}\\mu)$ for all $\\mathrm{T}\\in D(\\tilde{\\mathcal{F}}_{\\mu})$ . Then, ${\\tilde{\\mathcal{F}}}_{\\mu}$ is Fr\u00e9chet differentiable, and for all $\\mathrm{S}\\in D(\\tilde{\\mathcal{F}}_{\\mu})$ $\\tilde{\\mathcal{F}}_{\\mu}),\\,\\nabla\\tilde{\\mathcal{F}}_{\\mu}(\\mathrm{S})=\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mathrm{S}_{\\#}\\mu)\\circ\\mathrm{S}.$ . ", "page_idx": 23}, {"type": "text", "text": "Proof. See Appendix H.1. ", "page_idx": 23}, {"type": "text", "text": "A similar formula can be found in Gangbo and Tudorascu [55, Corollary 3.22], however the space $H$ used there is not $L^{2}(\\mu)$ but a lifting $\\mathbf{\\bar{\\boldsymbol{L}}}^{2}(\\Omega;\\mathbb{R}^{d})$ of measures on random variables. They should not be confused. ", "page_idx": 23}, {"type": "text", "text": "C.2 Wasserstein Hessians ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "A natural object of interest in the context of optimization over the Wasserstein space is the Hessian of the objective $\\mathcal{F}$ , which we define below, according to the original definitions of [97, Section 3] and [124, Chapter 8]. This notion is usually defined along Wasserstein geodesics. ", "page_idx": 23}, {"type": "text", "text": "Definition 8 (Chapter 8 in [124]). The Wasserstein Hessian of $\\mathcal{F}$ , denoted $\\mathrm{H}\\mathcal{F}_{\\mu}$ is an operator over $\\mathcal{T}_{\\mu}\\mathcal{P}_{2}(\\mathbb{R}^{d})$ verifying $\\begin{array}{r}{\\langle\\mathrm{H}\\mathcal{F}_{\\mu}v_{0},v_{0}\\rangle_{L^{2}(\\mu)}=\\frac{\\mathrm{d}^{2}}{\\mathrm{d}t^{2}}\\mathcal{F}(\\rho_{t})\\big\\vert_{t=0}\\,i f\\,(\\rho_{t},v_{t})_{t\\in[0,1]}}\\end{array}$ is a Wasserstein geodesic starting at $\\mu$ . ", "page_idx": 23}, {"type": "text", "text": "If $\\mu$ is absolutely continuous, Wassertein geodesics starting from $\\mu$ are curves of the form $\\rho_{t}=$ $(\\mathrm{Id}+t\\nabla\\psi)_{\\#}\\mu$ for $\\psi\\,\\in\\,\\mathcal{C}_{c}^{\\infty}(\\mathbb{R}^{d})$ . Using this fact, one can compute the Wasserstein Hessians of Kullback\u2013Leibler divergence [97], Maximum Mean Discrepancy [7] or Kernel Stein Discrepancy [72] and many other functionals. ", "page_idx": 23}, {"type": "text", "text": "However in this work, we are interested in more general curves, which we call acceleration free, i.e.   \n$\\mu_{t}=(\\mathrm{S}+t v)_{\\#}\\mu$ with $\\mathrm{S},v\\in L^{2}(\\mu)$ . Thus, we define analogously the Hessian along such curves. ", "page_idx": 23}, {"type": "text", "text": "Definition 9. We define the Hessian operator $\\mathrm{H}\\mathscr{F}_{\\mu,t}:L^{2}(\\mu)\\to L^{2}(\\mu)$ as the operator satisfying for all $t\\in[0,1]$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}^{2}}{\\mathrm{d}t^{2}}\\mathcal{F}(\\mu_{t})=\\langle\\mathrm{H}\\mathcal{F}_{\\mu,t}v,v\\rangle_{L^{2}(\\mu)},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $t\\mapsto\\mu_{t}=(\\mathrm{S}+t v)_{\\#}\\mu$ for $\\mathrm{S},v\\in L^{2}(\\mu)$ . ", "page_idx": 23}, {"type": "text", "text": "Note that the Hessian $\\mathrm{H}\\mathcal{F}_{\\mu,t}$ is taken at time $t$ but that the vector field $v\\in L^{2}(\\mu)$ is in the tangent space at $t=0$ , hence the discrepancy with Definition 8 besides the fact that we can have $S\\neq\\mathrm{Id}$ . ", "page_idx": 23}, {"type": "text", "text": "Wang and Li [126] derived a general closed form of the Wasserstein Hessian on tangent spaces through the first variation of $\\mathcal{F}$ . Here, we extend their formula along any curve $\\mu_{t}=\\bar{(\\mathrm{S}+t v)}_{\\#}\\mu$ with S, $v\\in L^{2}(\\mu)$ . We first provide a lemma computing the derivative of the Wasserstein gradient. ", "page_idx": 23}, {"type": "text", "text": "Lemma 11. Let $\\mathcal{F}~:~\\mathcal{P}_{2}(\\mathbb{R}^{d})~\\to~\\mathbb{R}$ be twice continuously differentiable and assume that $\\begin{array}{r}{\\frac{\\delta}{\\delta\\mu}\\nabla\\frac{\\delta\\mathcal{F}}{\\delta\\mu}(\\mu)=\\nabla\\frac{\\delta^{2}\\mathcal{F}}{\\delta\\mu^{2}}(\\mu)}\\end{array}$ for all $\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ . Let $\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ and for all $t\\in[0,1]$ , $\\mu_{t}=(\\mathrm{T}_{t})_{\\#}\\mu$ where $\\mathrm{T}_{t}$ is differentiable w.r.t. $t$ with $\\begin{array}{r}{\\frac{\\mathrm{d}\\mathrm{T}_{t}}{\\mathrm{d}t}\\in L^{2}(\\mu)}\\end{array}$ . Then, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\mathrm{d}}{\\mathrm{d}t}(\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{t})\\circ\\mathrm{T}_{t})(x)=\\displaystyle\\int\\big[\\nabla_{y}\\nabla_{x}\\frac{\\delta^{2}\\mathcal{F}}{\\delta\\mu^{2}}\\big((\\mathrm{T}_{t})_{\\#}\\mu\\big)\\big(\\mathrm{T}_{t}(x),\\mathrm{T}_{t}(y)\\big)\\frac{\\mathrm{d}\\mathrm{T}_{t}}{\\mathrm{d}t}(y)\\big]\\;\\mathrm{d}\\mu(y)\\qquad}\\\\ {+\\,\\nabla^{2}\\frac{\\delta\\mathcal{F}}{\\delta\\mu}\\big((\\mathrm{T}_{t})_{\\#}\\mu\\big)\\big(\\mathrm{T}_{t}(x)\\big)\\frac{\\mathrm{d}\\mathrm{T}_{t}}{\\mathrm{d}t}(x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. See Appendix H.8. ", "page_idx": 23}, {"type": "text", "text": "This allows us to define a closed-form for HF\u00b5,t. ", "page_idx": 23}, {"type": "text", "text": "Proposition 12. Under the same assumptions as in Lemma $I I$ , let $\\mu_{t}=(\\mathrm{T}_{t})_{\\#}\\mu$ with $\\mathrm{T}_{t}=\\mathrm{S}+t v$ , $\\mathrm{S},v\\in L^{2}(\\mu)$ , then $\\mathrm{H}\\mathscr{F}_{\\mu,t}:L^{2}(\\mu)\\to L^{2}(\\mu)$ (as defined in Definition 9) is defined for all $v\\in L^{2}(\\mu)$ , $x\\in\\mathbb{R}^{d}\\:a s$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{1}\\mathcal{F}_{\\mu,t}[v](x)=\\int\\nabla_{y}\\nabla_{x}\\frac{\\delta^{2}\\mathcal{F}}{\\delta\\mu^{2}}\\big((\\mathrm{T}_{t})_{\\#}\\mu\\big)\\big(\\mathrm{T}_{t}(x),\\mathrm{T}_{t}(y)\\big)v(y)\\,\\mathrm{d}\\mu(y)+\\nabla^{2}\\frac{\\delta\\mathcal{F}}{\\delta\\mu}\\big((\\mathrm{T}_{t})_{\\#}\\mu\\big)\\big(\\mathrm{T}_{t}(x)\\big)v(x).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "While $\\mathrm{H}\\mathcal{F}_{\\mu,t}$ and $\\mathrm{H}\\mathcal{F}_{\\mu_{t}}$ differ in general, in some simple cases their relation boils down to composition with a pushforward. For instance, if S is invertible, we can write $\\mu_{t}$ as a curve starting from $\\mathrm{S}_{\\#}\\mu$ with a velocity field $v\\circ\\mathrm{S}^{-1}$ , i.e. $\\mu_{t}=(\\mathrm{Id}+t v\\circ\\mathrm{S}^{-1})_{\\#}\\mathrm{S}_{\\#}\\mu.$ Thus, we recover the original definition of the Wasserstein Hessian at $t=0$ as $\\mathrm{H}\\mathscr{F}_{\\mu,0}=\\mathrm{H}\\mathscr{F}_{\\mathrm{S}_{\\#}\\mu}$ . However, in general, this does not need to be the case. ", "page_idx": 24}, {"type": "text", "text": "Similarly, if $\\mathrm{T}_{t}$ is invertible for all $t$ , setting $v_{t}=v\\circ\\mathrm{T}_{t}^{-1}$ , we can write ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\mathrm{d}^{2}}{\\mathrm{d}t^{2}}\\mathcal{F}(\\mu_{t})=\\langle\\mathrm{H}\\mathcal{F}_{\\mu,t}v,v\\rangle_{L^{2}(\\mu)}}\\\\ &{\\phantom{\\frac{1}{\\mathrm{d}t^{2}}\\mathcal{F}(\\mu_{t})=}=\\int\\langle\\mathrm{H}\\mathcal{F}_{\\mu,t}[v](x),v(x)\\rangle\\;\\mathrm{d}\\mu(x)}\\\\ &{\\phantom{\\frac{1}{\\mathrm{d}t^{2}}\\mathcal{F}(\\mu_{t})}=\\int\\langle\\mathrm{H}\\mathcal{F}_{\\mu,t}[v]\\bigl(\\mathrm{T}_{t}^{-1}(x_{t})\\bigr),v_{t}(x_{t})\\rangle\\;\\mathrm{d}\\mu_{t}(x_{t})}\\\\ &{\\phantom{\\frac{1}{\\mathrm{d}t^{2}}\\mathcal{F}(\\mu_{t})}=\\langle\\mathrm{H}\\mathcal{F}_{\\mu_{t}}v_{t},v_{t}\\rangle_{L^{2}(\\mu_{t})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The last line is obtained by leveraging the closed form in Proposition 12 and that $\\mu_{t}=(\\mathrm{T}_{t})_{\\#}\\mu$ , as for all $x\\in\\operatorname{supp}(\\mu)$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{H}\\mathcal{F}_{\\mu,t}[v](x)=\\displaystyle\\int\\nabla_{y}\\nabla_{x}\\frac{\\delta^{2}\\mathcal{F}}{\\delta\\mu^{2}}(\\mu_{t})\\big(\\mathrm{T}_{t}(x),\\mathrm{T}_{t}(y)\\big)v(y)\\;\\mathrm{d}\\mu(y)+\\nabla^{2}\\frac{\\delta\\mathcal{F}}{\\delta\\mu}(\\mu_{t})\\big(\\mathrm{T}_{t}(x)\\big)v(x)}\\\\ &{\\qquad\\qquad=\\displaystyle\\int\\nabla_{y}\\nabla_{x}\\frac{\\delta^{2}\\mathcal{F}}{\\delta\\mu^{2}}(\\mu_{t})(\\mathrm{T}_{t}(x),y_{t})v_{t}(y_{t})\\;\\mathrm{d}\\mu_{t}(y)+\\nabla^{2}\\frac{\\delta\\mathcal{F}}{\\delta\\mu}(\\mu_{t})\\big(\\mathrm{T}_{t}(x)\\big)v(x)}\\\\ &{\\qquad\\qquad=\\mathrm{H}\\mathcal{F}_{\\mu_{t}}[v_{t}]\\big(\\mathrm{T}_{t}(x)\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and thus $\\mathrm{H}\\mathcal{F}_{\\mu,t}[v]\\bigl(\\mathrm{T}_{t}^{-1}(x)\\bigr)=\\mathrm{H}\\mathcal{F}_{\\mu_{t}}[v_{t}](x).$ . ", "page_idx": 24}, {"type": "text", "text": "Here are two examples of $\\mathcal{F}$ satisfying $\\begin{array}{r}{\\frac{\\delta}{\\delta\\mu}\\nabla\\frac{\\delta\\mathcal{F}}{\\delta\\mu}\\,=\\,\\nabla\\frac{\\delta^{2}\\mathcal{F}}{\\delta\\mu^{2}}}\\end{array}$ for which Proposition 12 provides an expression of the Wasserstein Hessian. ", "page_idx": 24}, {"type": "text", "text": "Example 1 (Potential energy). Let $\\textstyle\\gamma(\\mu)=\\int V\\,\\mathrm{d}\\mu$ with $V$ twice differentiable with bounded Hessian. Then, we have $\\begin{array}{r}{\\frac{\\delta\\mathcal{V}}{\\delta\\mu}(\\mu)=V}\\end{array}$ and $\\begin{array}{r}{\\frac{\\delta^{2}\\mathcal{V}}{\\delta\\mu^{2}}(\\mu)=0}\\end{array}$ (using (29)). Thus, applying Proposition $I2$ , we recover for $\\mu_{t}=(\\mathrm{T}_{t})_{\\#}\\dot{\\mu},$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}^{2}}{\\mathrm{d}t^{2}}\\mathscr{V}(\\mu_{t})=\\int\\big\\langle\\nabla^{2}V\\big(\\mathrm{T}_{t}(x)\\big)v(x),v(x)\\big\\rangle\\,\\,\\mathrm{d}\\mu(x).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Example 2 (Interaction energy). Let $\\begin{array}{r}{\\mathcal{W}(\\mu)=\\frac{1}{2}\\iint W(x-y)\\;\\mathrm{d}\\mu(x)\\mathrm{d}\\mu(y)}\\end{array}$ with $W$ symmetric, twice differentiable and with bounded Hessian. Then, we have for all $x,y\\in\\mathbb{R}^{d}$ , ${\\frac{\\delta\\mathrm{W}}{\\delta\\mu}}(\\mu)(x)=(W\\star\\mu)(x)$ and ${\\frac{\\delta^{2}\\mathrm{W}}{\\delta\\mu^{2}}}(\\mu)(x,y)=W(x-y)$ (see e.g. [126, Example 7]), and thus applying Proposition $^{12}$ , for $\\mu_{t}=(\\mathrm{T}_{t})_{\\#}\\mu$ , the operator is ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathrm{H}{\\mathcal{W}}_{\\mu,t}[v](x)=-\\int\\nabla^{2}W\\bigl(\\mathrm{T}_{t}(x)-\\mathrm{T}_{t}(y)\\bigr)v(y)\\;\\mathrm{d}\\mu(y)+(\\nabla^{2}W\\star(\\mathrm{T}_{t})_{\\#}\\mu)(\\mathrm{T}_{t}(x))v(x),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}^{2}}{\\mathrm{d}t^{2}}\\mathcal{W}(\\mu_{t})=\\iint\\langle\\nabla^{2}W\\big(\\mathrm{T}_{t}(x)-\\mathrm{T}_{t}(y)\\big)\\big(v(x)-v(y)\\big),v(x)\\rangle\\;\\mathrm{d}\\mu(y)\\mathrm{d}\\mu(x).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "C.3 Convexity in Wasserstein space ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We first recall the definition of $\\alpha$ -convex functionals [5, Definition 9.1.1]. ", "page_idx": 24}, {"type": "text", "text": "Definition 10. $\\mathcal{F}$ is $\\alpha$ -convex along geodesics if for all $\\mu_{0},\\mu_{1}\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\forall t\\in[0,1],\\ \\mathcal{F}(\\mu_{t})\\leq(1-t)\\mathcal{F}(\\mu_{0})+t\\mathcal{F}(\\mu_{1})-\\alpha\\frac{t(1-t)}{2}\\mathrm{W}_{2}^{2}(\\mu_{0},\\mu_{1}),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $(\\mu_{t})_{t\\in[0,1]}$ is $a$ Wasserstein geodesic between $\\mu_{0}$ and $\\mu_{1}$ . ", "page_idx": 24}, {"type": "text", "text": "If we want to derive the minimal set of assumptions for the convergence of the gradient descent algorithms on Wasserstein space, we can actually restrict the smoothness and convexity to specific curves. In the next proposition, we characterize the convexity along one curve. The relative smoothness or convexity follows by considering the convexity of respectively $\\beta\\mathcal{G}-\\mathcal{F}$ or ${\\mathcal{F}}-\\alpha{\\mathcal{G}}$ . ", "page_idx": 25}, {"type": "text", "text": "Proposition 13. Let $\\mathcal{F}:\\,\\mathcal{P}_{2}(\\mathbb{R}^{d})\\,\\to\\,\\mathbb{R}$ be twice continuously differentiable. Let $\\mu\\,\\in\\,\\mathcal{P}_{2}(\\mathbb{R}^{d})$ , $\\Gamma,\\mathrm{S}\\in L^{2}(\\mu)$ , $\\mu_{t}=\\left(\\mathrm{T}_{t}\\right)_{\\#}\\mu$ for all $t\\in[0,1]$ where $\\mathrm{T}_{t}=(1-t)\\mathrm{S}+t\\mathrm{T}$ . Furthermore, denote for $t_{1},t_{2}\\in[0,1]$ , $\\tilde{\\mu}_{t}^{t_{1}\\rightarrow t_{2}}=\\left((1-t)\\mathrm{T}_{t_{1}}+t\\mathrm{T}_{t_{2}}\\right)_{\\#}\\mu$ . Then, the following statement are equivalent: ", "page_idx": 25}, {"type": "text", "text": "(c1) For all $t_{1},t_{2},t\\in[0,1],\\mathscr{F}(\\tilde{\\mu}_{t}^{t_{1}\\to t_{2}})\\leq(1-t)\\mathscr{F}\\big((\\mathrm{T}_{t_{1}})_{\\#}\\mu\\big)+t\\mathscr{F}\\big((\\mathrm{T}_{t_{2}})_{\\#}\\mu\\big),$ , i.e. $\\mathcal{F}$ is convex along t  \u2192\u00b5t. ", "page_idx": 25}, {"type": "text", "text": "(c2) For all $t_{1},t_{2}\\in[0,1]$ , we have $\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu}}(\\mathrm{T}_{t_{2}},\\mathrm{T}_{t_{1}})\\geq0,$ , i.e. ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{F}\\big((\\mathrm{T}_{t_{2}})_{\\#}\\mu\\big)-\\mathcal{F}\\big((\\mathrm{T}_{t_{1}})_{\\#}\\mu\\big)-\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}\\big((\\mathrm{T}_{t_{1}})_{\\#}\\mu\\big)\\circ\\mathrm{T}_{t_{1}},\\mathrm{T}_{t_{2}}-\\mathrm{T}_{t_{1}}\\rangle_{L^{2}(\\mu)}\\geq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "(c3) For all $t_{1},t_{2}\\in[0,1]$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}\\big((\\mathrm{T}_{t_{2}})_{\\#}\\mu\\big)\\circ\\mathrm{T}_{t_{2}}-\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}\\big((\\mathrm{T}_{t_{1}})_{\\#}\\mu\\big)\\circ\\mathrm{T}_{t_{1}},\\mathrm{T}_{t_{2}}-\\mathrm{T}_{t_{1}}\\big)_{L^{2}(\\mu)}\\geq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. See Appendix H.10. ", "page_idx": 25}, {"type": "text", "text": "As stated in Section 2, if we require the convexity to hold along all curves with $\\mathrm{S}=\\mathrm{Id}$ and $\\mathrm{T}$ the gradient of some convex function, i.e. an OT map, then $\\mathcal{F}$ is convex along geodesics. Likewise, if the convexity holds for all S, T that are gradients of convex functions, then we obtain the convexity along generalized geodesics. ", "page_idx": 25}, {"type": "text", "text": "If we require the convexity and the smoothness to hold along any curve of the form $\\mu_{t}\\,=\\,\\bigl((1\\,-\\,$ $t)\\mathrm{S}+t\\mathrm{T})_{\\#}\\mu$ for $\\mu\\,\\in\\,\\mathcal{P}_{2}(\\mathbb{R}^{d})$ and $\\mathrm{T},\\mathrm{S}\\,\\in\\,L^{2}(\\mu)$ , then it coincides with the transport convexity and smoothness recently introduced by Tanaka [117, Definitions 4.1 and 4.5]. As by Proposition 1, $\\delta\\tilde{\\mathcal{F}}_{\\mu}(\\mathrm{S},\\mathrm{T}-\\mathrm{S})=\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mathrm{S}_{\\#}\\mu)\\circ\\mathrm{S},\\mathrm{T}-\\mathrm{S}\\rangle_{L^{2}(\\mu)}$ , and thus the convexity of $\\mathcal{F}$ on such a curve reads as follows ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu}}(\\mathrm{T},\\mathrm{S})=\\mathcal{F}(\\mathrm{T}_{\\#}\\mu)-\\mathcal{F}(\\mathrm{S}_{\\#}\\mu)-\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mathrm{S}_{\\#}\\mu)\\circ\\mathrm{S},\\mathrm{T}-\\mathrm{S}\\rangle_{L^{2}(\\mu)}\\geq0.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "And for $\\begin{array}{r}{\\tilde{\\mathcal{G}}_{\\mu}(\\mathrm{T})=\\frac{1}{2}\\|\\mathrm{T}\\|_{L^{2}(\\mu)}}\\end{array}$ , the $\\beta.$ -smoothness of $\\mathcal{F}$ relative to $\\mathcal{G}$ expresses as ", "page_idx": 25}, {"type": "equation", "text": "$$\n1_{\\tilde{\\mathcal{F}}_{\\mu}}(\\mathrm{T},\\mathrm{S})=\\mathcal{F}(\\mathrm{T}_{\\#}\\mu)-\\mathcal{F}(\\mathrm{S}_{\\#}\\mu)-\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mathrm{S}_{\\#}\\mu)\\mathrm{oS},\\mathrm{T}-\\mathrm{S}\\rangle_{L^{2}(\\mu)}\\leq\\frac{\\beta}{2}\\|\\mathrm{T}-\\mathrm{S}\\|_{L^{2}(\\mu)}=\\beta\\mathrm{d}_{\\tilde{\\mathcal{G}}_{\\mu}}(\\mathrm{T},\\mathrm{S}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This type of convexity is actually a particular case of the notion of convexity along acceleration-free curves introduced by Parker [98] (also introduced by Cavagnari et al. [28] under the name of total convexity). The latter requires convexity to hold along any curve of the form $\\mu_{t}=\\left((1-t)\\pi^{1}+t\\pi^{2}\\right)_{\\#}\\gamma$ with $\\gamma\\in\\,\\Pi(\\mu,\\nu)$ , $\\mu,\\nu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ and $\\pi^{1}(x,y)\\,=\\,x$ , $\\pi^{2}(x,y)\\,=\\,y$ . The transport convexity of Tanaka [117] is thus a particular case for couplings obtained through maps, i.e. $\\gamma\\,=\\,(\\mathrm{T},\\mathrm{S})_{\\#}\\mu$ . Parker [98] notably showed that this notion of convexity is equivalent to the geodesic convexity for Wasserstein differentiable functionals. ", "page_idx": 25}, {"type": "text", "text": "We can also define the strict convexity using strict inequalities in Proposition 13-(c1)-(c2)-(c3), but not in (c4) as there are counter examples already for real functions. For instance, $f:\\mathbb{R}\\,\\rightarrow\\,\\mathbb{R}$ , defined as $f(x)=x^{4}$ for all $x\\in\\mathbb R$ , is strictly convex but $f^{\\prime\\prime}(0)=0$ . Thus, for $\\textstyle{\\mathcal{F}}(\\mu)=\\int f\\;\\mathrm{d}\\mu$ , choosing $\\mu=\\delta_{0}$ and $\\mathrm{T_{0}}=\\mathrm{Id}$ , by Example 1, we have that $\\begin{array}{r}{\\left.\\frac{\\mathrm{d}^{2}}{\\mathrm{d}t^{2}}\\mathcal{F}(\\mu_{t})\\right|_{t=0}=f^{\\prime\\prime}(0)v(0)^{2}=0.}\\end{array}$ . But $\\mathcal{F}(\\mu_{t})=f\\big(t\\mathrm{T}_{1}(0)\\big)<t f\\big(\\mathrm{T}_{1}(0)\\big)$ since $f$ is strictly convex and thus $\\mathcal{F}$ is well strictly convex. ", "page_idx": 25}, {"type": "text", "text": "Finally, as we defined the relative $\\alpha$ -convexity and $\\beta$ -smoothness of $\\mathcal{F}$ relative to $\\mathcal{G}$ using Bregman divergences in Definition 3, we can show that it is equivalent to ${\\mathcal{F}}-\\alpha{\\mathcal{G}}$ and $\\beta\\mathcal{G}-\\mathcal{F}$ being convex. ", "page_idx": 25}, {"type": "text", "text": "Proposition 14. Let $\\mathcal{F},\\mathcal{G}\\,:\\,\\mathcal{P}_{2}(\\mathbb{R}^{d})\\,\\to\\,\\mathbb{R}$ be two differentiable functionals. Let $\\mu\\,\\in\\,\\mathcal{P}_{2}(\\mathbb{R}^{d})$ , T, $\\bar{\\mathrm{S}}\\in L^{2}(\\mu)$ and for all $t\\in[0,1]$ , $\\mu_{t}=(\\mathrm{T}_{t})_{\\#}\\mu$ with $\\mathrm{T}_{t}=(1-t)\\mathrm{S}+t\\mathrm{T}$ . Then, $\\mathcal{F}$ is $\\alpha$ -convex (resp. $\\beta$ -smooth) relative to $\\mathcal{G}$ along $t\\mapsto\\mu_{t}$ if and only if ${\\mathcal{F}}-\\alpha{\\mathcal{G}}$ (resp. $\\beta\\mathcal{G}-\\mathcal{F})$ is convex along $t\\mapsto\\mu_{t}$ . ", "page_idx": 26}, {"type": "text", "text": "Proof. By Definition 3, $\\mathcal{F}$ is $\\alpha$ -convex relative to $\\mathcal{G}$ along $t~\\mapsto~\\mu_{t}$ if for all $s,t~\\in~[0,1]$ , $\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu}}(\\mathrm{T}_{s},\\mathrm{\\dot{T}}_{t})\\geq\\alpha\\mathrm{d}_{\\tilde{\\mathcal{G}}_{\\mu}}(\\mathrm{T}_{s},\\mathrm{T}_{t})$ . This is equivalent to $\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu}-\\alpha\\tilde{\\mathcal{G}}_{\\mu}}(\\mathrm{T}_{s},\\mathrm{T}_{t})\\geq0$ , which is equivalent by Proposition 13 (c2) (and using Proposition 1) to ${\\mathcal{F}}-{\\overset{\\cdot}{\\alpha}}{\\mathcal{G}}$ convex along $t\\mapsto\\mu_{t}$ . The result for the $\\beta$ -smoothness follows likewise. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "D Additional results on mirror descent ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "D.1 Optimal transport maps for mirror descent ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Let $\\phi:\\mathcal{P}_{2}(\\mathbb{R}^{d})\\to\\mathbb{R}$ be a strictly convex functional along all acceleration-free curves $\\mu_{t}=(\\mathrm{T}_{t})_{\\#}\\mu$ , $t\\in[0,1]$ with $\\mathrm{T}_{t}=(1-t)\\mathrm{S}+t\\mathrm{T}$ for $\\mathrm{T},\\mathrm{S}\\in L^{2}(\\mu)$ . Denote for $\\mu\\in L^{2}(\\mu)$ , $\\phi_{\\mu}(\\mathrm{T})=\\phi(\\mathrm{T}_{\\#}\\mu)$ . Since $\\phi$ is strictly convex along all acceleration-free curves, by Proposition 13, for all $\\mathrm{T}\\neq\\mathrm{S}\\in L^{2}(\\mu)$ , $\\mathrm{d}_{\\phi_{\\mu}}(\\mathrm{T},\\mathrm{S})>0$ and thus $\\phi_{\\mu}$ is strictly convex. Indeed, recall that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall\\mathrm{T},\\mathrm{S}\\in L^{2}(\\mu),\\ \\mathrm{d}_{\\phi_{\\mu}}(\\mathrm{T},\\mathrm{S})=\\phi_{\\mu}(\\mathrm{T})-\\phi_{\\mu}(\\mathrm{S})-\\langle\\nabla\\phi_{\\mu}(\\mathrm{S}),\\mathrm{T}-\\mathrm{S}\\rangle_{L^{2}(\\mu)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\phi(\\mathrm{T}_{\\#}\\mu)-\\phi(\\mathrm{S}_{\\#}\\mu)-\\langle\\nabla_{\\mathrm{W}_{2}}\\phi(\\mathrm{S}_{\\#}\\mu)\\circ\\mathrm{S},\\mathrm{T}-\\mathrm{S}\\rangle_{L^{2}(\\mu)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where we used Proposition 1 for the computation of the gradient. ", "page_idx": 26}, {"type": "text", "text": "Let us now define for all $\\mu,\\nu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathrm{W}_{\\phi}(\\nu,\\mu)=\\operatorname*{inf}_{\\gamma\\in\\Pi(\\nu,\\mu)}\\,\\phi(\\nu)-\\phi(\\mu)-\\int\\langle\\nabla_{\\mathrm{W}_{2}}\\phi(\\mu)(y),x-y\\rangle\\,\\mathrm{d}\\gamma(x,y).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "This problem encompasses several previously considered objects, as discussed in more detail in Remark 1. Our motivation for introducing Equation (42) is to prove that for $\\phi_{\\mu}$ verifying the assumptions of Proposition 2, its associated Bregman divergence $\\mathrm{d}_{\\phi_{\\mu}}$ satisfies the property given in Assumption 1. First, we can observe that as $\\gamma=(\\mathrm{T},\\mathrm{S})_{\\#}\\mu\\in\\Pi(\\mathrm{T}_{\\#}\\mu,\\mathrm{S}_{\\#}\\mu)$ , we have $\\mathrm{d}_{\\phi_{\\mu}}(\\mathrm{T},\\mathrm{S})\\geq$ $\\mathrm{W}_{\\phi}(\\mathrm{T}_{\\#}\\mu,\\mathrm{S}_{\\#}\\mu)$ . Then, for $\\mu\\,\\in\\,\\mathcal{P}_{2,\\mathrm{ac}}(\\mathbb{R}^{d})$ , assuming that $\\nabla_{\\mathrm{W}_{2}}\\phi(\\mu)\\,=\\,\\nabla\\phi_{\\mu}(\\mathrm{Id})$ is invertible, we can leverage Brenier\u2019s theorem [20], and show in Proposition 15 that the optimal coupling of Equation (42) is of the form $(\\mathrm{T}_{\\phi_{\\mu}}^{\\mu,\\nu},\\mathrm{Id})_{\\#}\\mu$ with $\\mathrm{T}_{\\phi_{\\mu}}^{\\mu,\\nu}=\\bar{\\mathrm{argmin}}_{\\mathrm{T}_{\\#}\\mu=\\nu}\\ \\mathrm{d}_{\\phi_{\\mu}}(\\mathrm{T},\\mathrm{Id})$ . Moreover, if $\\nu\\in\\mathcal{P}_{2,\\mathrm{ac}}(\\mathbb{R}^{d})$ , we also have that $\\mathrm{T}_{\\phi_{\\mu}}^{\\mu,\\nu}$ is invertible with inverse $\\bar{\\mathrm{T}}_{\\phi_{\\nu}}^{\\nu,\\mu}=\\operatorname{argmin}_{\\mathrm{T}_{\\#}\\nu=\\mu}\\mathrm{d}_{\\phi_{\\nu}}(\\mathrm{Id},\\mathrm{T})$ . ", "page_idx": 26}, {"type": "text", "text": "Proposition 15. Let $\\mu\\in\\mathcal{P}_{2,\\mathrm{ac}}(\\mathbb{R}^{d})$ , $\\nu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ and assume $\\nabla_{\\mathrm{W}_{2}}\\phi(\\mu)$ invertible. Then, ", "page_idx": 26}, {"type": "text", "text": "1. There exists a unique minimizer $\\gamma$ of (42). Besides, there exists a uniquely determined $\\mu$ -almost everywhere (a.e.) map $\\dot{\\mathrm{T}}_{\\phi_{\\mu}}^{\\mu,\\nu}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d}$ such that $\\boldsymbol{\\gamma}=(\\mathrm{T}_{\\phi_{\\mu}}^{\\mu,\\bar{\\nu}},\\mathrm{Id})_{\\#}\\mu$ . Finally, there exists a convex function $u:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ such that $\\mathrm{T}_{\\phi_{\\mu}}^{\\mu,\\nu}=\\nabla u\\circ\\nabla_{\\mathrm{W}_{2}}\\phi(\\mu)\\;\\mu{-}a.e.$   \n2. Assume further that $\\nu\\in\\mathcal{P}_{2,\\mathrm{ac}}(\\mathbb{R}^{d})$ . Then there exists a uniquely determined $\\nu$ -a.e. map $\\bar{\\mathrm{T}}_{\\phi_{\\nu}}^{\\nu,\\mu}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d}$ such that $\\gamma=(\\mathrm{Id},\\bar{\\mathrm{T}}_{\\phi_{\\nu}}^{\\nu,\\mu})_{\\#}\\nu$ . Moreover, there exists a convex function $v:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ such that $\\bar{\\mathrm{T}}_{\\phi_{\\nu}}^{\\nu,\\mu}=\\nabla_{\\mathrm{W}_{2}}\\phi(\\mu)^{-1}\\circ\\nabla v\\ \\nu\\!-\\!a.e.$ , and $\\mathrm{T}_{\\phi_{\\mu}}^{\\mu,\\nu}\\circ\\bar{\\mathrm{T}}_{\\phi_{\\nu}}^{\\nu,\\mu}=\\mathrm{Id}\\;\\nu\\cdot$ -a.e. and $\\bar{\\mathrm{T}}_{\\phi_{\\nu}}^{\\nu,\\mu}\\circ\\mathrm{T}_{\\phi_{\\mu}}^{\\mu,\\nu}=\\mathrm{Id}\\;\\mu{-}a.e$   \n3. As a corollary, $\\begin{array}{r}{\\mathrm{W}_{\\phi}(\\nu,\\mu)=\\operatorname*{min}_{\\mathrm{T}_{\\#}\\mu=\\nu}\\;\\mathrm{d}_{\\phi_{\\mu}}(\\mathrm{T},\\mathrm{Id})=\\operatorname*{min}_{\\mathrm{T}_{\\#}\\nu=\\mu}\\;\\mathrm{d}_{\\phi_{\\nu}}(\\mathrm{Id},\\mathrm{T}).}\\end{array}$ ", "page_idx": 26}, {"type": "text", "text": "Proof. 1. Observe that problem (42) is equivalent to ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\gamma\\in\\Pi(\\nu,\\mu)}\\;\\int\\|x-\\nabla_{\\mathrm{W}_{2}}\\phi(\\mu)(y)\\|_{2}^{2}\\;\\mathrm{d}\\gamma(x,y).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Then, since for any $\\gamma\\in\\Pi(\\nu,\\mu)$ , $\\big(\\mathrm{Id},\\nabla_{\\mathrm{W}_{2}}\\phi(\\mu)\\big)_{\\#}\\gamma\\in\\Pi\\big(\\nu,\\nabla_{\\mathrm{W}_{2}}\\phi(\\mu)_{\\#}\\mu\\big)$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\gamma\\in\\Pi(\\nu,\\mu)}\\;\\int\\|x-\\nabla_{\\mathrm{W}_{2}}\\phi(\\mu)(y)\\|_{2}^{2}\\;\\mathrm{d}\\gamma(x,y)\\geq\\operatorname*{inf}_{\\widetilde{\\gamma}\\in\\Pi\\big(\\nu,\\nabla_{\\mathrm{W}_{2}}\\phi(\\mu)_{\\#}\\mu\\big)}\\;\\int\\|x-z\\|_{2}^{2}\\;\\mathrm{d}\\widetilde{\\gamma}(x,z).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Let $\\mu\\in\\mathcal{P}_{2,\\mathrm{ac}}(\\mathbb{R}^{d})$ . Since $\\nabla_{\\mathrm{W}_{2}}\\phi(\\mu)$ is invertible, $\\nabla_{\\mathrm{W}_{2}}\\phi(\\mu)_{\\#}\\mu\\in\\mathcal{P}_{\\mathrm{2,ac}}(\\mathbb{R}^{d})$ . By Brenier\u2019s theorem, there exists a convex function $u$ such that $(\\nabla u)_{\\#}(\\nabla_{\\mathrm{W}_{2}}\\phi(\\mu))_{\\#}\\mu=\\nu$ and the optimal coupling is of the form $\\tilde{\\gamma}^{*}=(\\nabla u,\\mathrm{Id})_{\\#}\\nabla_{\\mathrm{W}_{2}}\\phi(\\mu)_{\\#}\\mu$ . Let $\\gamma=(\\nabla u\\circ\\nabla_{\\mathrm{W}_{2}}\\phi(\\mu),\\mathrm{Id})_{\\#}\\mu\\in\\Pi(\\nu,\\mu)$ , then ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\int\\|z-\\tilde{y}\\|_{2}^{2}\\,\\mathrm{d}\\tilde{\\gamma}^{*}(z,\\tilde{y})=\\int\\|\\nabla u\\big(\\nabla_{\\mathrm{W}_{2}}\\phi(\\mu)(y)\\big)-\\nabla_{\\mathrm{W}_{2}}\\phi(\\mu)(y)\\|_{2}^{2}\\,\\mathrm{d}\\mu(y)}}\\\\ {{=\\displaystyle\\int\\|x-\\nabla_{\\mathrm{W}_{2}}\\phi(\\mu)(y)\\|_{2}^{2}\\,\\mathrm{d}\\gamma(x,y).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus, since $\\gamma\\in\\Pi(\\nu,\\mu)$ , $\\gamma$ is an optimal coupling for (42). ", "page_idx": 27}, {"type": "text", "text": "2. We symmetrize the arguments. Assuming $\\nu\\in\\mathcal{P}_{2,\\mathrm{ac}}(\\mathbb{R}^{d})$ and $\\nabla\\phi_{\\mu}(\\mathrm{Id})=\\nabla_{\\mathrm{W_{2}}}\\phi(\\mu)$ invertible, by Brenier\u2019s theorem, there exists a convex function $v$ such that $(\\nabla v)_{\\#}\\nu=\\nabla_{\\mathrm{W}_{2}}\\phi(\\mu)_{\\#}\\mu$ (and such that $\\nabla\\boldsymbol{u}\\circ\\nabla\\boldsymbol{v}=\\mathrm{Id}$ $\\nu$ -a.e. and $\\nabla v\\circ\\nabla u=\\operatorname{Id}~\\nabla_{\\operatorname{W}_{2}}\\phi(\\mu)_{\\#}\\mu.$ -a.e.) and the optimal coupling is of the form $\\tilde{\\gamma}^{*}=(\\mathrm{Id},\\nabla v)_{\\#}\\nu$ . Let $\\gamma=(\\mathrm{Id},\\nabla_{\\mathrm{W}_{2}}\\phi(\\mu)^{-1}\\circ\\nabla v)_{\\#}\\nu\\in\\Pi(\\nu,\\mu)$ , then ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\int\\|x-z\\|_{2}^{2}\\,\\mathrm{d}\\tilde{\\gamma}^{*}(x,z)=\\displaystyle\\int\\|x-\\nabla v(x)\\|_{2}^{2}\\,\\mathrm{d}\\nu(x)}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\qquad=\\int\\|x-\\nabla\\mathrm{w}_{2}\\phi(\\mu)\\big((\\nabla\\mathrm{w}_{2}\\phi(\\mu))^{-1}(\\nabla v(x))\\big)\\|_{2}^{2}\\,\\mathrm{d}\\nu(x)}\\\\ {\\displaystyle\\qquad\\qquad\\qquad=\\int\\|x-\\nabla\\mathrm{w}_{2}\\phi(\\mu)(y)\\|_{2}^{2}\\,\\mathrm{d}\\gamma(x,y).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "aaTnnhddu $\\gamma\\in\\Pi(\\nu,\\mu)$ $\\gamma$ $\\mathrm{T}_{\\phi_{\\mu}}^{\\mu,\\nu}=\\nabla u\\circ\\nabla_{\\mathrm{W}_{2}}\\phi(\\mu)$$\\bar{\\mathrm{T}}_{\\phi_{\\nu}}^{\\nu,\\mu}=\\nabla_{\\mathrm{W}_{2}}\\phi(\\mu)^{-1}\\circ\\nabla v$ $\\mu$ $\\bar{\\mathrm{T}}_{\\phi_{\\nu}}^{\\nu,\\mu}\\mathrm{oT}_{\\phi_{\\mu}}^{\\mu,\\nu}=\\nabla_{\\mathrm{W}_{2}}\\phi(\\mu)^{-1}\\mathrm{o}\\nabla v\\mathrm{o}\\bar{\\nabla}u\\mathrm{o}\\nabla_{\\mathrm{W}_{2}}\\phi(\\mu)=\\mathrm{Id}$$\\nu$ $\\mathrm{T}_{\\phi_{\\mu}}^{\\mu,\\nu}\\circ\\bar{\\mathrm{T}}_{\\phi_{\\nu}}^{\\nu,\\mu}\\,=\\,\\nabla u\\circ\\nabla_{\\mathrm{W}_{2}}\\phi(\\mu)\\circ\\nabla_{\\mathrm{W}_{2}}\\phi(\\mu)^{-1}\\circ\\nabla v\\,=\\,\\mathrm{Id}$consequences of Brenier\u2019s theorem. \u53e3", "page_idx": 27}, {"type": "text", "text": "We continue this section with additional results relative to the invertibility of mirror maps, which are required in Proposition 2. For a potential energy $\\textstyle\\gamma(\\mu)=\\int V\\mathrm{d}\\mu$ , since $\\nabla_{\\mathrm{W}_{2}}\\gamma=\\nabla V$ , then $\\nabla_{\\mathrm{W}_{2}}\\nu}$ is invertible provided $\\nabla V$ is. This is the case e.g. for $V$ strictly convex. We now state in the two next lemmas conditions for an interaction energy and for the negative entropy to satisfy the invertibility requirements. ", "page_idx": 27}, {"type": "text", "text": "Lemma 16. Let $\\mu\\,\\in\\,\\mathcal{P}_{2}(\\mathbb{R}^{d})$ and let $W:\\mathbb{R}^{d}\\,\\rightarrow\\,\\mathbb{R}$ be even, $\\epsilon$ -strongly convex for $\\epsilon\\,>\\,0$ and differentiable. Then, for $\\begin{array}{r}{\\mathcal{W}(\\mu)=\\iint W(x-y)\\;\\mathrm{d}\\mu(x)\\mathrm{d}\\mu(y),\\nabla_{\\mathrm{W}_{2}}\\mathcal{W}(\\mu)}\\end{array}$ is invertible. ", "page_idx": 27}, {"type": "text", "text": "Proof. On one hand, $\\nabla_{\\mathrm{W}_{2}}\\mathcal{W}(\\mu)=\\nabla W\\star\\mu$ . Moreover, $W\\,\\epsilon\\cdot$ -strongly convex is equivalent to ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\forall x,y\\in\\mathbb{R}^{d},\\;x\\neq y,\\;\\langle\\nabla W(x)-\\nabla W(y),x-y\\rangle\\geq\\epsilon\\|x-y\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which implies for all $x,y,z\\in\\mathbb{R}^{d}$ , $\\langle\\nabla W(x-z)-\\nabla W(y-z),x-y\\rangle\\geq\\epsilon\\|x-y\\|_{2}^{2}$ . By integrating with respect to $\\mu$ , it implies ", "page_idx": 27}, {"type": "equation", "text": "$$\n(\\nabla W\\star\\mu)(x)-(\\nabla W\\star\\mu)(y),x-y\\rangle=\\int\\langle\\nabla W(x-z)-\\nabla W(y-z),x-y\\rangle\\,\\mathrm{d}\\mu(z)\\geq\\epsilon\\|x-y\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus, $\\nabla W\\star\\mu$ is $\\epsilon$ -strongly monotone, and in particular invertible [2, Theorem 1]. ", "page_idx": 27}, {"type": "text", "text": "Lemma 17. Let $\\mu\\in\\mathcal{P}_{2,\\mathrm{ac}}(\\mathbb{R}^{d})$ such that its density is of the form $\\rho\\,\\propto\\,e^{-V}$ with $V:\\mathbb R^{d}\\rightarrow\\mathbb R$ $\\epsilon$ -strongly convex for $\\epsilon>0$ . Then, for $\\begin{array}{r}{\\mathcal{H}(\\mu)=\\int\\log\\left(\\rho(x)\\right)\\,\\mathrm{d}\\mu(x)}\\end{array}$ with $\\rho$ the density of $\\mu$ w.r.t the Lebesgue measure, $\\nabla_{\\mathrm{W_{2}}}\\mathcal{H}(\\mu)$ is invertible. ", "page_idx": 27}, {"type": "text", "text": "Proof. Let $\\mu$ such distribution. Then, $\\nabla_{\\mathrm{W}_{2}}\\mathcal{H}(\\mu)=\\nabla\\log\\rho=-\\nabla V$ . Since $V$ is $\\epsilon$ -strongly convex, then $\\nabla V$ is $\\epsilon$ -strongly monotone and in particular invertible [2, Theorem 1]. \u53e3 ", "page_idx": 27}, {"type": "text", "text": "We conclude this section with a discussion of (42) with respect to related work. ", "page_idx": 27}, {"type": "text", "text": "Remark 1. The OT problem (42) recovers other OT costs for specific choices of $\\phi$ . For instance, for $\\begin{array}{r}{\\phi_{\\mu}(\\mathrm{T})=\\frac{1}{2}\\|\\mathrm{T}\\|_{L^{2}(\\mu)}^{2}.}\\end{array}$ , it coincides with the squared Wasserstein-2 distance. And more generally, for $\\begin{array}{r}{\\phi_{\\mu}^{V}(\\mathrm{T})=\\int V\\circ\\mathrm{T}\\,\\mathrm{d}\\mu_{\\mathrm{,~}}}\\end{array}$ , since by Lemma 31, for all $\\mathrm{T},\\mathrm{S}\\in L^{2}(\\mu)$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathrm{d}_{\\phi_{\\mu}^{V}}(\\mathrm{T},\\mathrm{S})=\\int\\mathrm{d}_{V}\\big(\\mathrm{T}(x),\\mathrm{S}(x)\\big)\\ \\mathrm{d}\\mu(x),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\mathrm{d}_{V}$ is the Euclidean Bregman divergence, i.e. for all x $;y\\in\\mathbb{R}^{d},\\,\\mathrm{d}_{V}(x,y)=V(x)-V(y)\\mathrm{~-~}$ $\\langle\\nabla V(y),x-y\\rangle$ , $\\mathrm{W}_{\\phi}$ coincides with the Bregman-Wasserstein divergence [103] ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathcal{B}_{V}(\\mu,\\nu)=\\operatorname*{inf}_{\\gamma\\in\\Pi(\\mu,\\nu)}\\int\\mathrm{d}_{V}(x,y)\\;\\mathrm{d}\\gamma(x,y).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "D.2 Continuous formulation ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Let $\\phi\\,:\\,L^{2}(\\mu)\\,\\to\\,\\mathbb{R}$ be pushforward compatible and superlinear. Introducing the (mirror) map $\\varphi(\\mu)=\\nabla_{\\mathrm{W_{2}}}\\phi(\\mu)$ , we can write informally the mirror descent scheme (4) and its continuous-time counterpart when $\\tau\\rightarrow0$ as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\varphi(\\mu_{k})=\\nabla_{\\mathrm{W}_{2}}\\phi(\\mu_{k})}\\\\ {\\varphi(\\mu_{k+1})\\circ\\mathrm{T}_{k+1}=\\varphi(\\mu_{k})-\\tau\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k})}\\end{array}\\right.\\qquad\\overset{\\longrightarrow}{\\tau\\to0}\\quad\\left\\{\\begin{array}{l l}{\\varphi(\\mu_{t})=\\nabla_{\\mathrm{W}_{2}}\\phi(\\mu_{t})}\\\\ {\\overset{\\mathrm{d}}{\\mathrm{d}t}\\varphi(\\mu_{t})=-\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{t}).}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "However, $\\begin{array}{r}{\\frac{\\mathrm{d}}{\\mathrm{d}t}\\varphi(\\mu_{t})\\,=\\,\\frac{\\mathrm{d}}{\\mathrm{d}t}\\nabla_{\\mathrm{W}_{2}}\\phi(\\mu_{t})\\,=\\,\\mathrm{H}\\phi_{\\mu_{t}}(v_{t})}\\end{array}$ where $\\mathrm{H}\\phi_{\\mu_{t}}:L^{2}(\\mu_{t})\\to L^{2}(\\mu_{t})$ is the Hessian operator (defined in Appendix C.2) such that $\\begin{array}{r}{\\frac{\\mathrm{d}^{2}}{\\mathrm{d}t^{2}}\\phi(\\mu_{t})=\\langle\\mathrm{H}\\phi_{\\mu_{t}}(v_{t}),v_{t}\\rangle_{L^{2}(\\mu_{t})}}\\end{array}$ and $v_{t}\\in L^{2}(\\mu_{t})$ is a velocity field satisfying $\\partial_{t}\\mu_{t}+\\mathrm{div}(\\mu_{t}v_{t})=\\tilde{0}$ . Thus, the continuity equation corresponding to the Mirror Flow is given by ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\partial_{t}\\mu_{t}-\\mathrm{div}\\left(\\mu_{t}(\\mathrm{H}\\phi_{\\mu_{t}})^{-1}\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{t})\\right)=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "For $\\phi_{\\mu}^{V}$ as Bregman potential, since $\\mathrm{H}\\phi_{\\mu}^{V}(v)=(\\nabla^{2}V)v$ (see Appendix C.2), the flow is a solution of $\\partial_{t}\\dot{\\mu}_{t}-\\mathrm{div}\\bigl(\\mu_{t}(\\nabla^{2}V)^{-1}\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{t})\\bigr)^{\\!}=0$ . For ${\\mathcal{F}}(\\mu)=\\operatorname{KL}(\\mu||\\nu)$ with $\\nu\\propto e^{-U}$ , this coincides with the gradient flow of the mirror Langevin [3, 130] and with the continuity equation obtained in [104] as the limit of the JKO scheme with Bregman groundcosts. For $\\phi={\\mathcal{F}}$ , this coincides with Information Newton\u2019s flows [126]. Note also that Deb et al. [41] defined mirror flows through the scheme $\\tau\\rightarrow0$ of (51), but focused on ${\\mathcal{F}}(\\mu)=\\mathrm{KL}(\\mu||\\nu)$ and $\\begin{array}{r}{\\phi(\\bar{\\mu})=\\frac{1}{2}\\mathrm{W}_{2}^{2}(\\mu,\\eta)}\\end{array}$ . ", "page_idx": 28}, {"type": "text", "text": "D.3 Derivation in specific settings ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In this section, we analyze several novel mirror schemes obtained through the use of different Bregman potential maps in (4), and used in various applications in Section 5. We start by discussing the scheme with an interaction energy as Bregman potential. Next, we study mirror descent with negative entropy or KL divergence as Bregman potential. For the last two, we derive closed-forms for the case where every distribution is Gaussian, which is equivalent to working on the Bures-Wasserstein space, and to use the gradient on the Bures-Wasserstein space [43]. In particular, this space is a submanifold of $\\mathcal{P}_{\\mathrm{2,ac}}(\\mathbb{R}^{\\bar{d}})$ and the tangent space is the space of affine maps with symmetric linear term, i.e. of the form $\\dot{T}(\\dot{x})=b+S(\\dot{x^{\\prime}}-m)\\dot{\\,}$ with $S\\in S_{d}(\\mathbb{R})$ . ", "page_idx": 28}, {"type": "text", "text": "Interaction mirror scheme. Consider as Bregman potential an interaction energy $\\phi_{\\mu}(\\mathrm{T})\\;=\\;$ $\\begin{array}{r}{\\frac{1}{2}\\iint W\\big((T(x)-T(x^{\\prime})\\big)\\;\\mathrm{d}\\mu(x)\\mathrm{d}\\mu(x^{\\prime})}\\end{array}$ . The mirror descent scheme (4) is given by ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\forall k\\geq0,\\;(\\nabla W\\star\\mu_{k+1})\\circ\\mathrm{T}_{k+1}=\\nabla W\\star\\mu_{k}-\\tau\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k}).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "For the particular case $W(x)={\\textstyle{\\frac{1}{2}}}\\|x\\|_{2}^{2}$ , the scheme can be made more explicit as $\\nabla W\\star\\mu(x)=$ $\\begin{array}{r}{\\int\\nabla W(x-y)\\;\\mathrm{d}\\mu(y)=\\int(x-y)\\;\\mathrm{d}\\mu(y)=x-m(\\mu)}\\end{array}$ with $\\begin{array}{r}{m(\\mu)=\\int y\\mathrm{d}\\mu(y)}\\end{array}$ the expectation, and thus (53) translates as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\forall k\\ge0,\\ x_{k+1}-m(\\mu_{k+1})=x_{k}-m(\\mu_{k})-\\tau\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k}),\\ x_{k}\\sim\\mu_{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "On one hand, recall from Example 2 that the Hessian of $\\phi$ is given, for $\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d}),v\\in L^{2}(\\mu)$ , by ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\forall x\\in\\mathbb{R}^{d},\\;\\mathrm{H}\\phi_{\\mu}[v](x)=-\\int v(y)\\;\\mathrm{d}\\mu(y)+v(x),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "since $\\nabla^{2}W=I_{d}$ . On the other hand, the mirror descent scheme (54) can be written as, for all $k\\geq0$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\ny_{k+1}=y_{k}-\\tau\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k})(x_{k}),\\;y_{k}=x_{k}-m(\\mu_{k}),\\;x_{k}\\sim\\mu_{k}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Passing to the limit $\\tau\\rightarrow0$ , we get ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}y_{t}}{\\mathrm{d}t}=-\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{t})(x_{t}),\\;y_{t}=x_{t}-m(\\mu_{t}),\\;x_{t}\\sim\\mu_{t},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\begin{array}{r}{\\frac{\\mathrm{d}y_{t}}{\\mathrm{d}t}=\\frac{\\mathrm{d}x_{t}}{\\mathrm{d}t}-\\frac{\\mathrm{d}m\\left(\\mu_{t}\\right)}{\\mathrm{d}t}}\\end{array}$ . Now, by setting $\\textstyle v_{t}(x)={\\frac{\\mathrm{d}x_{t}}{\\mathrm{d}t}}$ , by integration by part, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n{\\frac{\\mathrm{d}}{\\mathrm{d}t}}m(\\mu_{t})=\\int x\\,\\partial_{t}\\mu_{t}=-\\int x\\cdot\\mathrm{div}(\\mu_{t}v_{t})=\\int v_{t}(y)\\,\\mathrm{d}\\mu_{t}(y).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Combining the latter equation with (55), we obtain as expected that $\\begin{array}{r}{\\frac{\\mathrm{d}y_{t}}{\\mathrm{d}t}=\\mathrm{H}\\phi_{\\mu_{t}}\\lbrack v_{t}\\rbrack(x)}\\end{array}$ . ", "page_idx": 29}, {"type": "text", "text": "Negative entropy mirror scheme. Consider the negative entropy $\\begin{array}{r}{\\phi(\\mu)\\,=\\,\\int\\log\\left(\\rho(x)\\right)\\,\\mathrm{d}\\mu(x)}\\end{array}$ where $\\mathrm{d}\\mu(x)=\\rho(x)\\mathrm{d}x$ and $\\phi_{\\mu}(\\mathrm{T})=\\phi(\\mathrm{T}_{\\#}\\mu)$ . For such Bregman potential, the mirror scheme (4) can be written for all $k\\geq0$ as ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\nabla\\log\\rho_{k+1}\\circ\\mathrm{T}_{k+1}=\\nabla\\log\\rho_{k}-\\tau\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k}).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "In general, this scheme is not tractable. Nonetheless, supposing that $\\mu_{k}=\\mathcal{N}(m_{k},\\Sigma_{k})$ for all $k\\geq0$ , the scheme translates as ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{-\\Sigma_{k+1}^{-1}(\\mathrm{T}_{k+1}(x_{k})-m_{k+1})=-\\Sigma_{k}^{-1}(x_{k}-m_{k})-\\tau\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k}),\\ x_{k}\\sim\\mu_{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "\u2022 For an objective functional $\\mathcal F(\\mu)=\\mathcal H(\\mu)+\\mathcal V(\\mu)$ with $V(x)={\\textstyle{\\frac{1}{2}}}x^{T}\\Sigma^{-1}x$ , the scheme is ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\Sigma_{k+1}^{-1}(x_{k+1}-m_{k+1})=-\\Sigma_{k}^{-1}(x_{k}-m_{k})-\\tau\\big(-\\Sigma_{k}^{-1}(x_{k}-m_{k})+\\Sigma^{-1}x_{k}\\big)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad=-(1-\\tau)\\Sigma_{k}^{-1}(x_{k}-m_{k})-\\tau\\Sigma^{-1}x_{k}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad=-\\big((1-\\tau)\\Sigma_{k}^{-1}+\\tau\\Sigma^{-1}\\big)x_{k}+(1-\\tau)\\Sigma_{k}^{-1}m_{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Assuming $m_{k}=0$ for all $k$ , we obtain the following update rule for the covariance matrices: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\Sigma_{k+1}^{-1}=\\left((1-\\tau)\\Sigma_{k}^{-1}+\\tau\\Sigma^{-1}\\right)^{T}\\Sigma_{k}\\bigl((1-\\tau)\\Sigma_{k}^{-1}+\\tau\\Sigma^{-1}\\bigr).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We illustrate this scheme in Figure 2. ", "page_idx": 29}, {"type": "text", "text": "\u2022 For $\\mathcal{F}(\\mu)=\\mathcal{H}(\\mu)$ , we obtain ", "page_idx": 29}, {"type": "equation", "text": "$$\n-\\Sigma_{k+1}^{-1}(\\mathrm{T}_{k+1}(x_{k})-m_{k+1})=-(1-\\tau)\\Sigma_{k}^{-1}(x_{k}-m_{k}),\\;x_{k}\\sim\\mu_{k}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Assuming $m_{k}\\,=\\,0$ for all $k$ , for $\\tau<1$ , we obtain the following update rule for the covariance matrices: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\Sigma_{k+1}^{-1}=(1-\\tau)^{2}\\Sigma_{k}^{-1},\\mathrm{~i.e.,}}}\\\\ {{\\Sigma_{k+1}=\\displaystyle\\frac{1}{(1-\\tau)^{2}}\\Sigma_{k}=\\displaystyle\\frac{1}{(1-\\tau)^{2k}}\\Sigma_{0}\\displaystyle\\sim\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\,\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\,\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\,\\!\\!\\!\\!\\!\\!\\!\\!\\!\\,\\!\\!\\!\\!\\!\\!\\,\\!\\!\\!\\!\\!\\!\\!\\!\\,\\!\\!\\,\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\,\\,\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\,\\,\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\,\\,\\!\\!\\!\\,\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "The continuous time analog of this scheme is thus $\\mu_{t}:t\\mapsto\\mathcal{N}(0,\\Sigma_{t})$ with $\\Sigma_{t}\\,=\\,e^{2t}\\Sigma_{0}$ and the negative entropy decreases along this curve as ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{H}(\\mu_{t})=\\displaystyle\\int\\log\\left(\\rho_{t}(x)\\right)\\,\\mathrm{d}\\mu_{t}(x)}\\\\ &{\\qquad=\\displaystyle\\int\\log\\left(\\frac{1}{(2\\pi)^{\\frac{d}{2}}\\sqrt{\\operatorname*{det}\\Sigma_{t}}t}e^{-\\frac{1}{2}x^{T}\\Sigma_{t}^{-1}x}\\right)\\,\\mathrm{d}\\mu_{t}(x)}\\\\ &{\\qquad=\\displaystyle-\\frac{d}{2}\\log(2\\pi)-\\frac{1}{2}\\log\\operatorname*{det}(e^{2t}\\Sigma_{0})-\\frac{1}{2}\\mathrm{Tr}\\left(\\Sigma_{t}^{-1}\\int x x^{T}\\mathrm{d}\\mu_{t}(x)\\right)}\\\\ &{\\qquad=\\displaystyle-\\frac{d}{2}\\log(2\\pi e)-d t-\\frac{1}{2}\\sum_{i=1}^{d}\\log(\\lambda_{i}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $(\\lambda_{i})_{i}$ denote the eigenvalues of $\\Sigma_{0}$ . This is much faster than the heat flow for which the negative entropy decreases as [129, Appendix E.2] ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathcal{H}(\\rho_{t})=-\\frac{d}{2}\\log(2\\pi e)-\\frac{1}{2}\\sum_{i=1}^{d}\\log(\\lambda_{i}+2t),\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "with the scheme given by [129, Example 6] ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\forall k\\geq0,\\;\\left\\{{\\underset{\\Sigma_{k+1}}{m_{k+1}}}=m_{0}\\right.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "With our notations, the heat flow is the continuous time limit of the scheme (4) for the same objective $\\mathcal{F}$ but for a quadratic Bregman potential $\\begin{array}{r}{\\phi_{\\mu}(\\mathrm{T})=\\frac{1}{2}\\|\\mathrm{T}\\|_{L^{2}(\\mu)}^{2}}\\end{array}$ (which recovers the Wasserstein-2 geometry, hence Wasserstein-2 gradient flows). ", "page_idx": 30}, {"type": "text", "text": "KL mirror scheme. Suppose we want to optimize the KL divergence, i.e. a functional of the form $\\mathcal F(\\mu)=\\mathcal G(\\mu)+\\mathcal H(\\mu)$ where $\\begin{array}{r}{\\mathcal{G}(\\mu)=\\int U\\mathrm{d}\\bar{\\mu}}\\end{array}$ . Then, a natural choice of Bregman potential is also a functional of the form $\\phi(\\mu)=\\Psi(\\mu)+\\mathcal{H}(\\mu)$ with $\\textstyle\\Psi(\\mu)=\\int V\\mathrm{d}\\mu$ , with $U\\alpha$ -convex and $\\beta$ -smooth relative to $V$ . ", "page_idx": 30}, {"type": "text", "text": "In that case, we obtain the smoothness of $\\mathcal{F}$ relative to $\\phi$ . Recall we denote $\\tilde{\\mathcal{F}}_{\\mu}(\\mathrm{T})=\\mathcal{F}(\\mathrm{T}_{\\#}\\mu)$ for $\\mathrm{T}\\in L^{2}(\\mu)$ . Then for all $\\mathrm{T},\\mathrm{S}\\in L^{2}(\\mu)$ , we have $\\alpha\\mathrm{d}_{\\tilde{\\Psi}_{\\mu}}(\\mathrm{T},\\mathrm{S})\\leq\\mathrm{d}_{\\tilde{\\mathcal{G}}_{\\mu}}(\\mathrm{T},\\mathrm{S})\\leq\\beta\\mathrm{d}_{\\tilde{\\Psi}_{\\mu}}(\\mathrm{T},\\mathrm{S})$ , hence $\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu}}(\\mathrm{T},\\mathrm{S})=\\mathrm{d}_{\\tilde{\\mathcal{H}}_{\\mu}}(\\mathrm{T},\\mathrm{S})+\\mathrm{d}_{\\tilde{\\mathcal{G}}_{\\mu}}(\\mathrm{T},\\mathrm{S})\\le\\mathrm{d}_{\\tilde{\\mathcal{H}}_{\\mu}}(\\mathrm{T},\\mathrm{S})+\\beta\\mathrm{d}_{\\tilde{\\Psi}_{\\mu}}(\\mathrm{T},\\mathrm{S})\\le\\operatorname*{max}(1,\\beta)\\mathrm{d}_{\\phi_{\\mu}}(\\mathrm{T},\\mathrm{S}).$ (69) Similarly, $\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu}}(\\mathrm{T},\\mathrm{S})\\geq\\operatorname*{min}(1,\\alpha)\\mathrm{d}_{\\phi_{\\mu}}(\\mathrm{T},\\mathrm{S})$ . ", "page_idx": 30}, {"type": "text", "text": "We now focus on the case where all measures are Gaussian in order to be able to compute a closedform, i.e. $U(x)={\\textstyle{\\frac{1}{2}}}(x-m)^{T}\\Sigma^{-1}(x-m),V(x)={\\textstyle{\\frac{1}{2}}}x^{T}\\Lambda^{-1}x$ and for all $k\\geq0$ , $\\mu_{k}=\\mathcal{N}(m_{k},\\Sigma_{k})$ . In this case, recall that $\\nabla\\log\\mu_{k}(x)=-\\Sigma_{k}^{-1}(x-m_{k})$ . Then, at each step, the mirror descent scheme (4) writes for $x_{k}\\sim\\mu_{k},\\ k\\geq0$ as ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\nabla V(x_{k+1})+\\nabla\\log\\big(\\mu_{k+1}(x_{k+1})\\big)=\\nabla V(x_{k})+\\nabla\\log\\big(\\mu_{k}(x_{k})\\big)-\\tau\\big(\\nabla U(x_{k})+\\nabla\\log\\big(\\mu_{k}(x_{k})\\big)\\big)}}\\\\ &{\\Longleftrightarrow\\ \\Lambda^{-1}x_{k+1}-\\Sigma_{k+1}^{-1}(x_{k+1}-m_{k+1})}\\\\ &{}&{=\\Lambda^{-1}x_{k}-\\Sigma_{k}^{-1}(x_{k}-m_{k})-\\tau\\big(\\Sigma^{-1}(x_{k}-m)-\\Sigma_{k}^{-1}(x_{k}-m_{k})\\big)}\\\\ &{\\Longleftrightarrow\\ (\\Lambda^{-1}-\\Sigma_{k+1}^{-1})x_{k+1}+\\Sigma_{k+1}^{-1}m_{k+1}}\\\\ &{}&{=\\big(\\Lambda^{-1}-(1-\\tau)\\Sigma_{k}^{-1}-\\tau\\Sigma^{-1}\\big)x_{k}+(1-\\tau)\\Sigma_{k}^{-1}m_{k}+\\tau\\Sigma^{-1}m.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Thus, we get for the expectation that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\Lambda^{-1}-\\Sigma_{k+1}^{-1})m_{k+1}+\\Sigma_{k+1}^{-1}m_{k+1}=\\big(\\Lambda^{-1}-(1-\\tau)\\Sigma_{k}^{-1}-\\tau\\Sigma^{-1}\\big)m_{k}(1-\\tau)\\Sigma_{k}^{-1}m_{k}+\\tau\\Sigma^{-1}m}\\\\ &{\\Longleftrightarrow\\ \\Lambda^{-1}m_{k+1}=(\\Lambda^{-1}-\\tau\\Sigma^{-1})m_{k}+\\tau\\Sigma^{-1}m}\\\\ &{\\Longleftrightarrow\\ m_{k+1}=(I_{d}-\\tau\\Lambda\\Sigma^{-1})m_{k}+\\tau\\Lambda\\Sigma^{-1}m.\\quad}&{(71)}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We note that the latter update on the means coincides with the forward Euler method in the forwardbackward scheme, see (116) in Appendix F, which uses as Bregman potential $\\phi=\\Psi$ . Thus, the entropy does not affect the convergence towards the mean, which can be done simply by (preconditioned) gradient descent. ", "page_idx": 30}, {"type": "text", "text": "For the covariance part, we get ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\big(\\Lambda^{-1}-\\Sigma_{k+1}^{-1}\\big)^{T}\\Sigma_{k+1}\\big(\\Lambda^{-1}-\\Sigma_{k+1}^{-1}\\big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\big(\\Lambda^{-1}-\\tau\\Sigma^{-1}-(1-\\tau)\\Sigma_{k}^{-1}\\big)^{T}\\Sigma_{k}\\big(\\Lambda^{-1}-\\tau\\Sigma^{-1}-(1-\\tau)\\Sigma_{k}^{-1}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Now, supposing that all matrices commute, we get ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Lambda^{-2}\\Sigma_{k+1}-2\\Lambda^{-1}+\\Sigma_{k+1}^{-1}=(\\Lambda^{-1}-\\tau\\Sigma^{-1})^{2}\\Sigma_{k}-2(1-\\tau)\\Lambda^{-1}+2\\tau(1-\\tau)\\Sigma^{-1}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\,(1-\\tau)^{2}\\Sigma_{k}^{-1}}\\\\ &{\\iff\\Lambda^{-2}\\Sigma_{k+1}+\\Sigma_{k+1}^{-1}=(\\Lambda^{-1}-\\tau\\Sigma^{-1})^{2}\\Sigma_{k}+2\\tau\\Lambda^{-1}+2\\tau(1-\\tau)\\Sigma^{-1}+(1-\\tau)^{2}\\Sigma_{k}^{-1}}\\\\ &{\\iff\\Sigma_{k+1}+\\Lambda^{2}\\Sigma_{k+1}^{-1}=(I_{d}-\\tau\\Lambda\\Sigma^{-1})^{2}\\Sigma_{k}+2\\tau\\Lambda+2\\tau(1-\\tau)\\Lambda^{2}\\Sigma^{-1}+(1-\\tau)^{2}\\Lambda^{2}\\Sigma_{k}^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Denoting ", "page_idx": 31}, {"type": "equation", "text": "$$\nC=(I_{d}-\\tau\\Lambda\\Sigma^{-1})^{2}\\Sigma_{k}+2\\tau\\Lambda+2\\tau(1-\\tau)\\Lambda^{2}\\Sigma^{-1}+(1-\\tau)^{2}\\Lambda^{2}\\Sigma_{k}^{-1},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "the update on covariances is equivalent to ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\Sigma_{k+1}^{2}-C\\Sigma_{k+1}+\\Lambda^{2}=0.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Thus, $\\Sigma_{k+1}={\\textstyle\\frac{1}{2}}\\big(C\\pm(C^{2}-4\\Lambda^{2})^{\\frac{1}{2}}\\big)$ . ", "page_idx": 31}, {"type": "text", "text": "D.4 Mirror scheme with non-pushforward compatible Bregman potentials ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "We study in this Section schemes for which the Bregman potential $\\phi_{\\mu}$ is not pushforward compatible, and thus for which we cannot apply Proposition 2 and thus Assumption 1 may not hold a priori. An example of such potential is $\\bar{\\phi}_{\\mu}\\mathrm{(T)}\\bar{=}\\,\\langle\\mathrm{T},P_{\\mu}\\mathrm{T}\\rangle_{L^{2}(\\mu)}$ where $P_{\\mu}^{\\bar{\\mathbf{\\alpha}}}\\colon L^{2}(\\mu)\\ \\bar{\\rightarrow}\\ L^{2}(\\mu)$ is a linear autoadjoint and invertible operator. Since $\\nabla\\phi_{\\mu}(\\mathrm{T})\\,=\\,P_{\\mu}\\mathrm{T}$ , taking the first order conditions, we obtain the following scheme: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\forall k\\geq0,\\;\\mathrm{T}_{k+1}=\\mathrm{Id}-P_{\\mu_{k}}^{-1}\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k}).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "In particular, this includes SVGD [71, 83, 84] if we pose $P_{\\mu}^{-1}\\mathrm{T}\\,=\\,\\iota S_{\\mu}\\mathrm{T}$ with $S_{\\mu}\\,:\\,L^{2}(\\mu)\\,\\to$ $\\mathcal{H}$ defined as ${\\cal S}_{\\mu}\\mathrm{T}\\,=\\,\\int k(x,\\cdot)\\mathrm{T}(x)\\mathrm{d}\\mu(x)$ which maps functions from $L^{2}(\\mu)$ to the reproducing kernel Hilbert space $\\mathcal{H}$ with kernel $k$ , and with $\\iota:\\mathscr{H}\\,\\to\\,L^{2}(\\mu)$ the inclusion operator that is the adjoint of $S_{\\mu}$ [71]. It also includes the Kalman-Wasserstein gradient descent [56] for which $\\begin{array}{r}{P_{\\mu}^{-1}=\\int\\left(x-\\dot{m}(\\mu)\\right)\\otimes\\left(x-m(\\mu)\\right)\\,\\mathrm{d}\\mu(x)}\\end{array}$ is the covariance matrix, where $m(\\mu)=\\textstyle\\int x\\,\\mathrm{d}\\mu(x)$ . ", "page_idx": 31}, {"type": "text", "text": "More generally, for $\\begin{array}{r}{\\phi_{\\mu}(\\mathrm{T})\\,=\\,\\int P_{\\mu}(V\\circ\\mathrm{T})\\mathrm{d}\\mu.}\\end{array}$ , we can recover their mirrored version, including mirrored SVGD [113, 114], i.e. $\\mathrm{T}_{k+1}=\\nabla V^{*}\\circ\\left(\\nabla V-\\tau P_{\\mu_{k}}^{-1}\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k})\\right)$ . ", "page_idx": 31}, {"type": "text", "text": "Kalman-Wasserstein. We focus now on a particular choice of linear operator $P_{\\mu}$ . Namely, we take $P_{\\mu}\\mathrm{T}=C(\\mu)\\mathrm{T}$ with $\\begin{array}{r}{C(\\mu)=\\left(\\int\\left(x-m(\\mu)\\right)\\otimes\\left(x-m(\\mu)\\right)\\mathrm{d}\\mu(x)\\right)^{-1}}\\end{array}$ the inverse of the covariance matrix. In this case, (76) corresponds to the discretization of the Kalman-Wasserstein gradient flow [56]. We now show that it satisfies Assumption 1. First, let us compute the Bregman divergence associated to $\\phi$ : ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall\\mathrm{T},\\mathrm{S}\\in L^{2}(\\boldsymbol{\\mu}),\\ \\mathrm{d}_{\\phi_{\\boldsymbol{\\mu}}}(\\mathrm{T},\\mathrm{S})=\\frac{1}{2}\\langle\\mathrm{T},C(\\boldsymbol{\\mu})\\mathrm{T}\\rangle_{L^{2}(\\boldsymbol{\\mu})}+\\frac{1}{2}\\langle\\mathrm{S},C(\\boldsymbol{\\mu})\\mathrm{S}\\rangle_{L^{2}(\\boldsymbol{\\mu})}-\\langle C(\\boldsymbol{\\mu})\\mathrm{S},\\mathrm{T}\\rangle_{L^{2}(\\boldsymbol{\\mu})}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=\\frac{1}{2}\\big(\\langle\\mathrm{T},C(\\boldsymbol{\\mu})(\\mathrm{T}-\\mathrm{S})\\rangle_{L^{2}(\\boldsymbol{\\mu})}+\\langle\\mathrm{S}-\\mathrm{T},C(\\boldsymbol{\\mu})\\mathrm{S}\\rangle_{L^{2}(\\boldsymbol{\\mu})}\\big)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=\\frac{1}{2}\\|C(\\boldsymbol{\\mu})^{\\frac{1}{2}}(\\mathrm{T}-\\mathrm{S})\\|_{L^{2}(\\boldsymbol{\\mu})}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "For $\\gamma=(\\mathrm{T},\\mathrm{S})_{\\#}\\mu$ , we can write ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathrm{d}_{\\phi_{\\mu}}(\\mathrm{T},\\mathrm{S})=\\frac{1}{2}\\int\\|C(\\mu)^{\\frac{1}{2}}(x-y)\\|_{2}^{2}\\,\\mathrm{d}\\gamma(x,y).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Moreover, the problem $\\begin{array}{r}{\\operatorname*{inf}_{\\gamma\\in\\Pi(\\alpha,\\beta)}\\;\\int\\|C(\\mu)^{\\frac{1}{2}}(x-y)\\|_{2}^{2}\\,\\mathrm{d}\\gamma(x,y)}\\end{array}$ is equivalent to ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\gamma\\in\\Pi(\\alpha,\\beta)}\\;-\\int x^{T}C(\\mu)y\\;\\mathrm{d}\\gamma(x,y),\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "which is a squared OT problem. Thus, it admits an OT map if $C(\\mu)$ is invertible and $\\mu$ or $\\nu$ is absolutely continuous. ", "page_idx": 31}, {"type": "text", "text": "Second point of view. Another point of view would be to use the linearization with the gradient corresponding to the associated generalized Wasserstein distance, which is of the form $\\nabla_{\\mathrm{W}}\\mathcal{F}(\\mu)=$ $P_{\\mu}^{-1}\\dot{\\nabla}_{\\mathrm{W_{2}}}\\mathcal{F}(\\bar{\\mu})$ [46, 56], i.e. considering ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathrm{T}_{k+1}=\\underset{\\mathrm{T}\\in L^{2}(\\mu)}{\\mathrm{argmin}}\\,\\;\\mathrm{d}_{\\phi_{\\mu}}(\\mathrm{T},\\mathrm{Id})+\\langle\\nabla_{\\mathrm{W}}\\mathcal{F}(\\mu),\\mathrm{T}-\\mathrm{Id}\\rangle_{L^{2}(\\mu)},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where we assume that $\\nabla_{\\mathrm{W}}\\mathcal{F}(\\mu)\\in L^{2}(\\mu)$ . In that case, using the first order conditions, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\nabla\\mathrm{J}(\\mathrm{T}_{k+1})=0\\iff\\nabla_{\\mathrm{W}_{2}}\\phi\\big((\\mathrm{T}_{k+1})_{\\#}\\mu_{k}\\big)\\circ\\mathrm{T}_{k+1}=\\nabla_{\\mathrm{W}_{2}}\\phi(\\mu_{k})-\\tau P_{\\mu_{k}}^{-1}\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k}).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Then, for $\\phi_{\\mu}$ satisfying Assumption 1, the convergence will hold under relative smoothness and convexity assumptions similarly as for the analysis derived in Section 3. ", "page_idx": 31}, {"type": "text", "text": "E Relative convexity and smoothness ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "E.1 Relative convexity and smoothness between Fenchel transforms ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "In this Section, we show sufficient conditions to satisfy the inequalities assumed in Proposition 5 and Proposition 6 under the additional assumption that, for all $k\\,\\geq\\,0$ , $\\tilde{\\mathcal{F}}_{\\mu_{k}}$ is superlinear, lower semicontinuous and strictly convex. In this case, we can show that ${\\tilde{\\mathcal{F}}}_{\\mu k}^{*}$ is G\u00e2teaux differentiable, and thus we can use the Bregman divergence of ${\\tilde{\\mathcal{F}}}_{\\mu_{k}}^{*}$ . ", "page_idx": 32}, {"type": "text", "text": "Lemma 18. Let $\\phi:L^{2}(\\mu)\\to\\mathbb{R}$ be a superlinear, lower semicontinuous and strictly convex function.   \nThen, $\\phi^{*}$ is G\u00e2teaux differentiable. ", "page_idx": 32}, {"type": "text", "text": "Proof. Fix $g\\in L^{2}(\\mu)$ . Notice that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\bar{f}\\in\\partial\\phi^{*}(g)\\iff\\phi^{*}(g)=\\langle\\bar{f},g\\rangle-\\phi(\\bar{f})=\\operatorname*{sup}_{f\\in L^{2}(\\mu)}\\langle f,g\\rangle-\\phi(f).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "So to prove there is a unique element in $\\partial\\phi^{*}(g)$ , we just need to show that, setting $\\phi_{g}(f):=-\\langle f,g\\rangle+$ $\\phi(f)$ , the problem $\\operatorname*{inf}_{f\\in L^{2}(\\mu)}\\phi_{g}(f)$ has a unique solution. Under our assumptions, $\\phi_{g}$ is lower semicontinuous and strictly convex. Since $\\phi$ is superlinear, $\\phi_{g}$ is coercive, i.e. $\\begin{array}{r l}{\\operatorname*{lim}_{\\parallel f\\parallel\\to\\infty}\\phi_{g}(f)=}&{{}}\\end{array}$ $+\\infty$ . There thus exists a solution [8, Theorem 3.3.4], which is unique by strict convexity. Hence $\\partial\\phi^{*}(g)$ is reduced to a point, which is necessarily the G\u00e2teaux derivative of $\\phi^{*}$ at $g$ . \u53e3 ", "page_idx": 32}, {"type": "text", "text": "This allows us to relate the Bregman divergence of $\\phi^{*}$ to the Bregman divergence of $\\phi$ . ", "page_idx": 32}, {"type": "text", "text": "Lemma 19. Let $\\phi:L^{2}(\\mu)\\to\\mathbb{R}$ be a proper superlinear and strictly convex differentiable function, then for all $\\mathrm{T},\\mathrm{S}\\in L^{2}(\\mu)$ $\\boldsymbol{u}),\\,\\mathrm{d}_{\\phi^{*}}\\bigl(\\nabla\\phi(\\mathrm{T}),\\nabla\\phi(\\mathrm{S})\\bigr)=\\mathrm{d}_{\\phi}(\\mathrm{S},\\mathrm{T}).$ . ", "page_idx": 32}, {"type": "text", "text": "Proof. By [99, Corollary 3.44], we have $\\phi^{*}\\big(\\nabla\\phi(\\mathrm{T})\\big)=\\langle\\mathrm{T},\\nabla\\phi(\\mathrm{T})\\rangle_{L^{2}(\\mu)}-\\phi(\\mathrm{T})$ for all $\\mathrm{T}\\in L^{2}(\\mu)$ since $\\phi$ is convex and differentiable. By Lemma 18, $\\phi^{*}$ is invertible and by [10, Corollary 16.24], since $\\phi$ is proper, lower semicontinuous and convex, then $(\\nabla\\phi)^{-1}=\\nabla\\phi^{*}$ . ", "page_idx": 32}, {"type": "text", "text": "Thus, for all $\\mathrm{T},\\mathrm{S}\\in L^{2}(\\mu)$ , ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ensuremath{\\mathrm{d}}_{\\phi^{*}}\\left(\\nabla\\phi(\\mathrm{T}),\\nabla\\phi(\\mathrm{S})\\right)=\\phi^{*}\\left(\\nabla\\phi(\\mathrm{T})\\right)-\\phi^{*}\\left(\\nabla\\phi(\\mathrm{S})\\right)-\\left\\langle\\nabla\\phi^{*}\\left(\\nabla\\phi(\\mathrm{S})\\right),\\nabla\\phi(\\mathrm{T})-\\nabla\\phi(\\mathrm{S})\\right\\rangle_{L^{2}(\\mu)}}\\\\ &{=\\phi^{*}\\left(\\nabla\\phi(\\mathrm{T})\\right)-\\phi^{*}\\left(\\nabla\\phi(\\mathrm{S})\\right)-\\left\\langle\\mathrm{S},\\nabla\\phi(\\mathrm{T})-\\nabla\\phi(\\mathrm{S})\\right\\rangle_{L^{2}(\\mu)}}\\\\ &{=\\left\\langle\\nabla\\phi(\\mathrm{T}),\\mathrm{T}\\right\\rangle_{L^{2}(\\mu)}-\\phi(\\mathrm{T})-\\left\\langle\\nabla\\phi(\\mathrm{S}),\\mathrm{S}\\right\\rangle_{L^{2}(\\mu)}+\\phi(\\mathrm{S})}\\\\ &{\\quad-\\left\\langle\\mathrm{S},\\nabla\\phi(\\mathrm{T})-\\nabla\\phi(\\mathrm{S})\\right\\rangle_{L^{2}(\\mu)}}\\\\ &{=\\phi(\\mathrm{S})-\\phi(\\mathrm{T})-\\left\\langle\\nabla\\phi(\\mathrm{T}),\\mathrm{S}-\\mathrm{T}\\right\\rangle_{L^{2}(\\mu)}}\\\\ &{=\\ensuremath{\\mathrm{d}}_{\\phi}(\\mathrm{S},\\mathrm{T}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Finally, we can relate the relative convexity of $\\phi$ relative to $\\psi^{*}$ by using an inequality between the Bregman divergences of $\\phi$ and $\\psi$ . In particular, we recover the assumptions of Propositions 5 and 6 for $\\phi_{\\mu_{k}}^{h^{*}}$ that is $\\beta$ -smooth and $\\alpha$ -convex relative to ${\\tilde{\\mathcal{F}}}_{\\mu_{k}}^{*}$ . ", "page_idx": 32}, {"type": "text", "text": "Proposition 20. Let $\\phi,\\psi:L^{2}(\\mu)\\to\\mathbb{R}$ proper, superlinear, strictly convex and differentiable. $\\phi$ is $\\beta$ -smooth (resp. $\\alpha$ -convex) relative to $\\psi^{*}$ if and only $i f\\forall\\mathrm{T},\\mathrm{S}\\in L^{2}(\\mu)$ , $\\mathrm{d}_{\\phi}\\big(\\nabla\\psi(\\mathrm{T}),\\nabla\\psi(\\mathrm{S})\\big)\\leq$ $\\beta\\mathrm{d}_{\\psi}(\\mathrm{S},\\mathrm{T})$ (resp. $\\mathrm{d}_{\\phi}\\bigl(\\nabla\\psi(\\mathrm{T}),\\nabla\\psi(\\mathrm{S})\\bigr)\\geq\\alpha\\mathrm{d}_{\\psi}(\\mathrm{S},\\mathrm{T})\\,\\!$ . ", "page_idx": 32}, {"type": "text", "text": "Proof of Proposition 20. First, suppose that $\\phi$ is $\\beta$ -smooth relative to $\\psi^{*}$ . Then, by definition, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\forall\\mathrm{T},\\mathrm{S}\\in L^{2}(\\mu),\\;\\mathrm{d}_{\\phi}(\\mathrm{T},\\mathrm{S})\\leq\\beta\\mathrm{d}_{\\psi^{*}}(\\mathrm{T},\\mathrm{S}).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "In particular, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}_{\\phi}\\big(\\nabla\\psi(\\mathrm{T}),\\nabla\\psi(\\mathrm{S})\\big)\\leq\\beta\\mathrm{d}_{\\psi^{*}}\\big(\\nabla\\psi(\\mathrm{T}),\\nabla\\psi(\\mathrm{S})\\big)=\\beta\\mathrm{d}_{\\psi}(\\mathrm{S},\\mathrm{T}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "using Lemma 19. ", "page_idx": 32}, {"type": "text", "text": "On the other hand, suppose for all $\\mathrm{T},\\mathrm{S}\\in L^{2}(\\mu),\\mathrm{d}_{\\phi}\\bigl(\\nabla\\psi(\\mathrm{T}),\\nabla\\psi(\\mathrm{S})\\bigr)\\leq\\beta\\mathrm{d}_{\\psi}(\\mathrm{S},\\mathrm{T})$ . Then, by first using Lemma 19 and then the supposed inequality, we have for all $\\mathrm{T},\\mathrm{S}\\in L^{2}(\\mu)$ , ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\beta\\mathrm{d}_{\\psi^{*}}\\bigl(\\nabla\\psi(\\mathrm{T}),\\nabla\\psi(\\mathrm{S})\\bigr)=\\beta\\mathrm{d}_{\\psi}(\\mathrm{S},\\mathrm{T})\\geq\\mathrm{d}_{\\phi}\\bigl(\\nabla\\psi(\\mathrm{T}),\\nabla\\psi(\\mathrm{S})\\bigr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Likewise, we can show that $\\phi$ is $\\alpha$ -convex relative to $\\psi$ if and only if $\\begin{array}{r l}{\\mathrm{d}_{\\phi}\\big(\\nabla\\psi(\\mathrm{T}),\\nabla\\psi(\\mathrm{S})\\big)}&{{}\\ge}\\end{array}$ $\\alpha\\mathrm{d}_{\\psi}(\\mathrm{S},\\mathrm{T})$ for all $\\mathrm{T},\\mathrm{S}\\in L^{2}(\\mu)$ . ", "page_idx": 33}, {"type": "text", "text": "Links with the conditions of Proposition 5 and Proposition 6. Proposition 20 allows to translate the inequality hypothesis of Proposition 5 and Proposition 6. Assume that for all $k$ , $\\tilde{\\mathcal{F}}_{\\mu_{k}}$ is strictly convex, differentiable and superlinear. We first note that it implies that ${\\mathcal{F}}_{\\mu_{k}}$ is convex along $t\\mapsto$ $\\left((1-t)\\mathrm{T}_{k+1}+t\\mathrm{Id}\\right)_{\\#}\\mu_{k}$ . Moreover, by Lemma 18, $\\nabla\\tilde{\\mathcal{F}}_{\\mu_{k}}^{*}$ is differentiable. ", "page_idx": 33}, {"type": "text", "text": "Note that this assumption is satisfied, e.g. by $\\textstyle\\phi_{\\mu}(\\mathrm{T})=\\int V\\circ\\mathrm{T}\\;\\mathrm{d}\\mu$ for $V\\;\\eta$ -strongly convex and differentiable. Indeed, in this case, $\\phi_{\\mu}$ is also $\\eta$ -strongly convex, and satisfies for all $\\mathrm{T},\\mathrm{S}\\in L^{2}(\\mu)$ , ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathrm{d}_{\\phi_{\\mu}}(\\mathrm{T},\\mathrm{S})=\\phi_{\\mu}(\\mathrm{T})-\\phi_{\\mu}(\\mathrm{S})-\\langle\\nabla\\phi_{\\mu}(\\mathrm{S}),\\mathrm{T}-\\mathrm{S}\\rangle_{L^{2}(\\mu)}\\ge\\frac{\\eta}{2}\\|\\mathrm{T}-\\mathrm{S}\\|_{L^{2}(\\mu)}^{2}}\\\\ {\\displaystyle\\iff\\phi_{\\mu}(\\mathrm{T})\\ge\\phi_{\\mu}(\\mathrm{S})+\\langle\\nabla\\phi_{\\mu}(\\mathrm{S}),\\mathrm{T}-\\mathrm{S}\\rangle_{L^{2}(\\mu)}+\\frac{\\eta}{2}\\|\\mathrm{T}-\\mathrm{S}\\|_{L^{2}(\\mu)}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "For $\\mathrm{S}=0$ , and dividing by $\\|\\mathrm{T}\\|_{L^{2}(\\mu)}$ the right term diverges to $+\\infty$ when $\\|\\mathrm{T}\\|_{L^{2}(\\mu)}\\to+\\infty$ , and thus $\\begin{array}{r}{\\operatorname*{lim}_{\\|\\mathrm{T}\\|_{L^{2}(\\mu)}\\to\\infty}\\phi_{\\mu}(\\mathrm{T})/\\|\\mathrm{T}\\|_{L^{2}(\\mu)}=+\\infty}\\end{array}$ , and $\\phi_{\\mu}$ is superlinear. ", "page_idx": 33}, {"type": "text", "text": "This assumption is also satisfied for interaction energies $\\begin{array}{r}{\\phi_{\\mu}^{W}(\\mathrm{T})=\\iint W\\big(\\mathrm{T}(x)-\\mathrm{T}(y)\\big)\\;\\mathrm{d}\\mu(x)\\mathrm{d}\\mu(y)}\\end{array}$ with $W\\;\\eta$ -strongly convex, even and differentiable. Indeed, by strong convexity of $W$ in 0, we have for all $\\boldsymbol{x},\\dot{\\boldsymbol{y}}\\in\\mathbb{R}^{\\bar{d}}$ , ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W\\big(\\mathrm{T}(x)-\\mathrm{T}(y)\\big)-W(0)-\\langle\\nabla W(0),\\mathrm{T}(x)-\\mathrm{T}(y)\\rangle\\geq\\frac{\\eta}{2}\\|\\mathrm{T}(x)-\\mathrm{T}(y)\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\geq\\frac{\\eta}{2}\\underset{z\\in\\mathbb{R}^{d}}{\\operatorname*{inf}}\\ \\|\\mathrm{T}(x)-z\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Integrating w.r.t. $\\mu\\otimes\\mu$ , we get ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\phi_{\\mu}^{W}(\\mathrm{T})-W(0)\\geq\\frac{\\eta}{2}\\operatorname*{inf}_{z\\in\\mathbb{R}^{d}}\\,\\int\\|\\mathrm{T}(x)-z\\|_{2}^{2}\\,\\mathrm{d}\\mu(x),\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and dividing by $\\|\\mathrm{T}\\|_{L^{2}(\\mu)}$ , we get that $\\phi_{\\mu}^{W}$ is superlinear. ", "page_idx": 33}, {"type": "text", "text": "For a curve $t\\,\\mapsto\\,\\mu_{t}$ , we define ${\\mathcal{F}}_{\\mu}^{*}$ on $\\mu_{t}$ as $\\mathcal{F}_{\\mu}^{\\ast}(\\mu_{t})\\,:=\\,\\tilde{\\mathcal{F}}_{\\mu}^{\\ast}(\\mathrm{T}_{t})$ with $\\tilde{\\mathcal{F}}_{\\mu}^{*}$ the convex conjugate of ${\\tilde{\\mathcal{F}}}_{\\mu}$ in the $L^{2}(\\mu)$ sense. Then, we can apply Proposition 20, and we obtain that the inequality hypothesis of Proposition 5 is implied by the $\\beta$ -smoothness of $\\phi^{h^{*}}$ relative to $\\mathcal{F}_{\\mu_{k}}^{*}$ along $t\\mapsto$ $\\big((1-t)\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k})+t\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k+1})\\circ\\mathrm{T}_{k+1}\\big)_{\\#}\\mu_{k}$ since ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{d}_{\\phi_{\\mu_{k}}^{h^{*}}}\\bigl(\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k+1})\\circ\\mathrm{T}_{k+1},\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k})\\bigr)\\leq\\beta\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu_{k}}}(\\mathrm{Id},\\mathrm{T}_{k+1})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\beta\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu_{k}}^{*}}\\bigl(\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k+1})\\circ\\mathrm{T}_{k+1},\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k})\\bigr),}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where we used Proposition 1 to compute the gradient $\\nabla\\tilde{\\mathcal{F}}_{\\mu_{k}}(\\mathrm{T}_{k+1})=\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k+1})\\circ\\mathrm{T}_{k+1}.$ . Similarly, the condition of Proposition 6 ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{d}_{\\phi_{\\mu_{k}}^{h^{*}}}\\bigl(\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mathrm{T}_{\\#}\\mu_{k})\\circ\\mathrm{T},\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k})\\bigr)\\geq\\alpha\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu_{k}}}(\\mathrm{Id},\\mathrm{T})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\alpha\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu_{k}}^{*}}\\bigl(\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mathrm{T}_{\\#}\\mu_{k})\\circ\\mathrm{T},\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k})\\bigr)}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "is implied by the $\\alpha$ -convexity of $\\phi^{h^{*}}$ relative to $\\mathcal{F}_{\\mu_{k}}^{*}$ along $t~\\;\\mapsto\\;~\\big((1~-~t)\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k})~+$ $t\\nabla_{\\mathrm{W_{2}}}\\mathcal{F}(\\mathrm{T}_{\\#}\\mu_{k})\\circ\\mathrm{T}\\big)_{\\#}\\mu_{k}.$ . ", "page_idx": 33}, {"type": "text", "text": "These results are summarized in Proposition 7 and shown formally in Appendix H.7. ", "page_idx": 33}, {"type": "text", "text": "Convergence towards the minimizer in Proposition 6. We add an additional result justifying the convergence towards the minimizer in Proposition 6. ", "page_idx": 34}, {"type": "text", "text": "Lemma 21. Let $(X,\\tau)$ be a metrizable topological space, and $f:X\\to\\mathbb{R}\\cup\\{+\\infty\\}$ be strictly convex, $\\tau$ -lower semicontinuous and with one $\\tau$ -compact sublevel set. Let $x_{0}\\in X$ be the minimizer of $f$ and take a sequence $(x_{n})_{n\\in\\mathbb{N}}$ such that $f(x_{n})\\bar{\\to}\\,f(x_{0})$ . Then, $(x_{n})_{n\\in\\mathbb{N}}$ $\\tau$ -converges to $x_{0}$ . ", "page_idx": 34}, {"type": "text", "text": "Proof. The existence of the minimum is given by [8, Theorem 3.2.2]. For $N$ large enough, $(x_{n})_{n\\geq N}$ lives in the $\\tau$ -compact sublevel set of $f$ , since $x_{0}$ belongs to it and $f(x_{0})$ is minimal. We can then consider a subsequence $\\tau$ -converging to some $x^{*}$ . By $\\tau$ -lower semicontinuity, we have $f(x_{0})\\leq$ f(x\u2217) \u2264lim inf $\\bar{f}(x_{\\sigma(n)})\\,=\\,f(\\bar{x_{0}})$ , so $f(x_{0})\\,=\\,f(\\bar{x}^{*})$ and by strict convexity $x_{0}\\,=\\,x^{*}$ . Since all subsequences of $(x_{n})_{n\\geq N}$ converge to $x^{*}$ and the space is metrizable, $(x_{n})_{n\\in\\mathbb{N}}$ $\\tau$ -converges to $x_{0}$ . \u53e3 ", "page_idx": 34}, {"type": "text", "text": "The typical case is when $X$ is a Hilbert space and $\\tau$ is the weak topology. One could wish to have strong convergence under a coercivity assumption, however \u201cIn infinite dimensional spaces, the topologies which are directly related to coercivity are the weak topologies\u201d [8, p86]. Nevertheless G\u00e2teaux differentiability implies continuity, which paired with convexity gives weak lower semicontinuity [8, Theorem 3.3.3]. We cannot hope for convergence of the norm of $x_{n}$ to come for free, as the weak convergence would then imply the strong convergence. ", "page_idx": 34}, {"type": "text", "text": "E.2 Relative convexity and smoothness between functionals ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Let $U,V:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ be differentiable and convex functions. We recall that $V$ is $\\alpha$ -convex relative to $U$ if [88] ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\forall x,y\\in\\mathbb{R}^{d},\\;\\mathrm{d}_{V}(x,y)\\geq\\alpha\\mathrm{d}_{U}(x,y).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Likewise, $V$ is $\\beta$ -smooth relative to $U$ if ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathrm{d}_{V}(x,y)\\leq\\beta\\mathrm{d}_{U}(x,y).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Relative convexity and smoothness between potential energies. By Lemma 31, for Bregman potentials of the form $\\begin{array}{r}{\\phi_{\\mu}(\\mathrm{T})=\\int V\\circ\\mathrm{T}\\,\\mathrm{d}\\mu_{:}}\\end{array}$ , the Bregman divergence can be written as ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\forall\\mathrm{T},\\mathrm{S}\\in L^{2}(\\mu),\\;\\mathrm{d}_{\\phi_{\\mu}}(\\mathrm{T},\\mathrm{S})=\\int\\mathrm{d}_{V}\\big(\\mathrm{T}(x),\\mathrm{S}(x)\\big)\\;\\mathrm{d}\\mu(x).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Thus, leveraging this result, we can show that relative convexity and smoothness of $\\phi_{\\mu}^{V}$ relative to $\\phi_{\\mu}^{U}$ is inherited by the relative convexity and smoothness of $V$ relative to $U$ . ", "page_idx": 34}, {"type": "text", "text": "Proposition 22. Let $\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ , $\\textstyle\\phi_{\\mu}(\\mathrm{T})=\\int V\\circ\\mathrm{T}\\;\\mathrm{d}\\mu$ and $\\begin{array}{r}{\\psi_{\\mu}(\\mathrm{T})=\\int U\\circ\\mathrm{T}\\;\\mathrm{d}\\mu\\;w h e r e\\;V,U:}\\end{array}$ $\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ are $C^{1}$ . If $V$ is $\\alpha$ -convex (resp. $\\beta$ -smooth) relative to $U:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ , then $\\phi_{\\mu}$ is $\\alpha$ -convex (resp $\\beta$ -smooth) relative to $\\psi_{\\mu}$ . ", "page_idx": 34}, {"type": "text", "text": "Proof. First, observe (Lemma 31) that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\forall\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d}),\\mathrm{T},\\mathrm{S}\\in L^{2}(\\mu),\\;\\mathrm{d}_{\\phi_{\\mu}}(\\mathrm{T},\\mathrm{S})=\\int\\mathrm{d}_{V}\\big(\\mathrm{T}(x),\\mathrm{S}(x)\\big)\\;\\mathrm{d}\\mu(x).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Let $\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d}),\\mathrm{T},\\mathrm{S}\\in L^{2}(\\mu)$ . If $V$ is $\\alpha$ -convex relatively to $U$ , we have for all $x,y\\in\\mathbb{R}^{d}$ , ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathrm{d}_{V}\\big(\\mathrm{T}(x),\\mathrm{S}(y)\\big)\\geq\\alpha\\mathrm{d}_{U}\\big(\\mathrm{T}(x),\\mathrm{S}(y)\\big),\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and hence by integrating on both sides with respect to $\\mu$ , ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathrm{d}_{\\phi_{\\mu}}(\\mathrm{T},\\mathrm{S})\\geq\\alpha\\mathrm{d}_{\\psi_{\\mu}}(\\mathrm{T},\\mathrm{S}).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Likewise, we have the result for the $\\beta$ -smoothness. ", "page_idx": 34}, {"type": "text", "text": "Relative convexity and smoothness between interaction energies. Similarly, by Lemma 32, for Bregman potentials obtained through interaction energies, i.e. $\\phi_{\\mu}(\\mathrm{T})\\;=\\;{\\textstyle\\frac{\\circ}{2}}\\int\\!\\!\\!\\!\\int W\\big(\\mathrm{T}(x)\\,-\\,$ $\\mathrm{T}(\\boldsymbol{x}^{\\prime}))\\;\\mathrm{d}\\mu(\\boldsymbol{x})\\mathrm{d}\\mu(\\boldsymbol{x}^{\\prime})$ , then ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\forall{\\mathrm{T}},{\\mathrm{S}}\\in L^{2}(\\mu),\\ \\mathrm{d}_{\\phi_{\\mu}}({\\mathrm{T}},{\\mathrm{S}})=\\frac{1}{2}\\iint\\mathrm{d}_{W}\\big({\\mathrm{T}}(x)-{\\mathrm{T}}(x^{\\prime}),{\\mathrm{S}}(x)-{\\mathrm{S}}(x^{\\prime})\\big)\\ \\mathrm{d}\\mu(x)\\mathrm{d}\\mu(x^{\\prime}).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "It also allows to inherit the relative convexity and smoothness results from $\\mathbb{R}^{d}$ . ", "page_idx": 35}, {"type": "text", "text": "Proposition 23. Let $\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ , $W,K:\\mathbb{R}^{d}\\to\\mathbb{R}$ be symmetric, $C^{1}$ and convex. Let $\\phi_{\\mu}(\\mathrm{T})=$ $\\begin{array}{r}{\\frac{1}{2}\\iint W\\big(\\mathrm{T}(x)-\\mathrm{T}(x^{\\prime})\\big)\\;\\mathrm{d}\\mu(x)\\mathrm{d}\\mu(x^{\\prime})}\\end{array}$ and $\\begin{array}{r}{\\psi_{\\mu}(\\mathrm{T})=\\frac{1}{2}\\int\\!\\!\\int K\\big(\\mathrm{T}(x)-\\mathrm{T}(x^{\\prime})\\big)\\;\\mathrm{d}\\mu(x)\\mathrm{d}\\mu(x^{\\prime})}\\end{array}$ . If $W$ is $\\alpha$ -convex relative to $K$ , then $\\phi_{\\mu}$ is $\\alpha$ -convex relatively to $\\psi_{\\mu}$ . Likewise, $i f W$ is $\\beta$ -smooth relatively to $K$ , then $\\phi_{\\mu}$ is $\\beta$ -smooth relatively to $\\psi_{\\mu}$ . ", "page_idx": 35}, {"type": "text", "text": "Proof. We use first Lemma 32 and then that $W$ is $\\alpha$ -convex relatively to $K$ : ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathrm{d}_{\\phi_{\\mu}}(\\mathrm{T},\\mathrm{S})=\\frac{1}{2}\\iint\\mathrm{d}_{W}\\bigl(\\mathrm{T}(x)-\\mathrm{T}(x^{\\prime}),\\mathrm{S}(x)-\\mathrm{S}(x^{\\prime})\\bigr)\\;\\mathrm{d}\\mu(x)\\mathrm{d}\\mu(x^{\\prime})}\\\\ &{\\displaystyle\\qquad\\qquad\\ge\\frac{\\alpha}{2}\\iint\\mathrm{d}_{K}\\bigl(\\mathrm{T}(x)-\\mathrm{T}(x^{\\prime}),\\mathrm{S}(x)-\\mathrm{S}(x^{\\prime})\\bigr)\\;\\mathrm{d}\\mu(x)\\mathrm{d}\\mu(x^{\\prime})}\\\\ &{\\displaystyle\\qquad\\qquad=\\alpha\\mathrm{d}_{\\psi_{\\mu}}(\\mathrm{T},\\mathrm{S}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Likewise, we have the result for the $\\beta$ -smoothness. ", "page_idx": 35}, {"type": "text", "text": "Thus, in situations where the objective functional and the Bregman potential are of the same type and either potential energies or interaction energies, we only need to show the convexity and smoothness of the underlying potentials or interaction kernels. For instance, let $V:\\mathbb R^{d}\\ \\dot{\\rightarrow}\\ \\mathbb R$ be a twicedifferentiable convex function, such that $\\|\\nabla^{2}V\\|_{\\mathrm{op}}\\leq p_{r}(\\|x\\|_{2})$ with $p_{r}$ a polynomial function of degree $r$ and $\\|\\cdot\\|_{\\mathrm{op}}$ the operator norm. Then, by [88, Proposition 2.1], $V$ is $\\beta$ -smooth relative to $h$ where for all $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ , $\\begin{array}{r}{h(x)=\\frac{1}{r+2}\\|x\\|_{2}^{r+2}+\\frac{1}{2}\\|x\\|_{2}^{2}}\\end{array}$ . ", "page_idx": 35}, {"type": "text", "text": "Relative convexity and smoothness between functionals of different types. When the functionals do not belong to the same type, comparing directly the Bregman divergences is less straightforward in general. In that case, one might instead leverage the equivalence relations given by Proposition 13 and Proposition 14, and show that $\\beta\\mathcal{G}-\\mathcal{F}$ or ${\\mathcal{F}}-\\alpha{\\mathcal{G}}$ is convex in order to show respectively the $\\beta$ -smoothness and $\\alpha$ -convexity of $\\mathcal{F}$ relative to $\\mathcal{G}$ . For instance, we can use the characterization through Hessians, and thus we would aim at showing ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}^{2}}{\\mathrm{d}t^{2}}\\mathcal{F}({\\mu_{t}})\\leq\\beta\\frac{\\mathrm{d}^{2}}{\\mathrm{d}t^{2}}\\mathcal{G}({\\mu_{t}}),\\quad\\frac{\\mathrm{d}^{2}}{\\mathrm{d}t^{2}}\\mathcal{F}({\\mu_{t}})\\geq\\alpha\\frac{\\mathrm{d}^{2}}{\\mathrm{d}t^{2}}\\mathcal{G}({\\mu_{t}}),\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "along the right curve $t\\mapsto\\mu_{t}$ . ", "page_idx": 35}, {"type": "text", "text": "For instance, consider an objective functional $\\begin{array}{r}{\\mathcal{F}(\\mu)\\,=\\,\\frac{1}{2}\\iint W(x-y)\\;\\mathrm{d}\\mu(x)\\mathrm{d}\\mu(x^{\\prime})}\\end{array}$ and another functional $\\textstyle{\\mathcal{G}}(\\mu)=\\int V\\mathrm{d}\\mu$ . Then, by Example 1 and Example 2, we have, for $\\mu_{t}=(\\mathrm{T}_{t})_{\\#}\\mu$ and $\\mathrm{T}_{t}=\\mathrm{S}+t v$ , ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}^{2}}{\\mathrm{d}t^{2}}\\mathscr{G}(\\mu_{t})=\\int\\langle\\nabla^{2}V\\big(\\mathrm{T}_{t}(x)\\big)v(x),v(x)\\rangle\\;\\mathrm{d}\\mu(x),\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "and ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}^{2}}{\\mathrm{d}t^{2}}\\mathscr{F}(\\mu_{t})=\\iint\\langle\\nabla^{2}W\\bigl(\\mathrm{T}_{t}(x)-\\mathrm{T}_{t}(y)\\bigr)\\bigl(v(x)-v(y)\\bigr),v(x)\\rangle\\;\\mathrm{d}\\mu(x)\\mathrm{d}\\mu(y).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "To show the conditions of Proposition 3, we need to take $\\mathrm{S}=\\mathrm{Id}$ and $v=\\mathrm{T}_{k+1}-\\mathrm{Id}$ , and to verify for $t=s\\in[0,1]$ the inequality, i.e. ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\mathrm{d}^{2}}{\\mathrm{d}t^{2}}\\mathcal{F}(\\mu_{t})\\Big\\vert_{t=s}\\leq\\beta\\frac{\\mathrm{d}^{2}}{\\mathrm{d}t^{2}}\\mathcal{G}(\\mu_{t})\\Big\\vert_{t=s}}\\\\ &{\\iff}\\\\ &{\\iint\\langle\\nabla^{2}W(\\Gamma_{s}(x)-\\mathrm{T}_{s}(y))\\big(v(x)-v(y)\\big),v(x)\\rangle\\;\\mathrm{d}\\mu_{k}(x)\\mathrm{d}\\mu_{k}(y)}\\\\ &{\\leq\\beta\\int\\langle\\nabla^{2}V(\\Gamma_{s}(x))v(x),v(x)\\rangle\\;\\mathrm{d}\\mu_{k}(x)}\\\\ &{\\iff}\\\\ &{\\int\\Big\\langle v(x)\\,\\Big\\rangle\\Big(\\big(\\nabla^{2}W\\big(\\mathrm{T}_{s}(x)-\\mathrm{T}_{s}(y)\\big)-\\beta\\nabla^{2}V\\big(\\mathrm{T}_{s}(x)\\big)\\big)v(x)}\\\\ &{\\quad-\\nabla^{2}W\\big(\\mathrm{T}_{s}(x)-\\mathrm{T}_{s}(y)\\big)v(y)\\Big)\\mathrm{d}\\mu_{k}(y)\\Big\\rangle\\mathrm{d}\\mu_{k}(x)\\leq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "For example, choosing $W(x)={\\textstyle{\\frac{1}{2}}}\\|x\\|_{2}^{2}$ , then $\\nabla^{2}W=I_{d}$ and $\\mathcal{F}$ is $\\beta$ -smooth relative to $\\mathcal{G}$ as long as $\\begin{array}{r}{\\nabla^{2}V\\circ\\mathrm{T}_{s}\\succeq\\frac{1}{\\beta}I_{d}}\\end{array}$ for any $s\\in[0,1]$ . ", "page_idx": 36}, {"type": "text", "text": "F Bregman proximal gradient scheme ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "In this section, we are interested into minimizing a functional $\\mathcal{F}$ of the form $\\mathcal F(\\mu)=\\mathcal G(\\mu)+\\mathcal H(\\mu)$ where $\\mathcal{G}$ is smooth relative to some function $\\phi$ and $\\mathcal{H}$ is convex on $L^{2}(\\mu)$ . Different strategies can be used to tackle this problem. For instance, Jiang et al. [65] restrict the space to particular directions along which $\\mathcal{H}$ is smooth while Diao et al. [43], Salim et al. [109] use Proximal Gradient algorithms. We focus here on the latter and generalize the Bregman Proximal Gradient algorithm [11], also known as the Forward-Backward scheme. It consists of alternating a forward step on $\\mathcal{G}$ and then a backward step on $\\mathcal{H}$ , i.e. for $k\\geq0$ , ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left[\\begin{array}{l l}{\\mathrm{S}_{k+1}=\\mathrm{argmin}_{\\mathrm{S}\\in L^{2}(\\mu_{k})}\\,\\,\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{S},\\mathrm{Id})+\\tau\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{G}(\\mu_{k}),\\mathrm{S}-\\mathrm{Id}\\rangle_{L^{2}(\\mu_{k})},}&{\\nu_{k+1}=(\\mathrm{S}_{k+1})_{\\#}\\mu_{k}}\\\\ {\\mathrm{T}_{k+1}=\\mathrm{argmin}_{\\mathrm{T}\\in L^{2}(\\nu_{k+1})}\\,\\,\\mathrm{d}_{\\phi_{\\nu_{k+1}}}(\\mathrm{T},\\mathrm{Id})+\\tau\\mathcal{H}(\\mathrm{T}_{\\#}\\nu_{k+1}),}&{\\mu_{k+1}=(\\mathrm{T}_{k+1})_{\\#}\\nu_{k+1}.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "The first step of our analysis is to show that this scheme is equivalent to ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int\\tilde{\\mathrm{T}}_{k+1}=\\operatorname*{argmin}_{\\mathrm{T}\\in L^{2}(\\mu_{k})}~\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{T},\\mathrm{Id})+\\tau\\big(\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{G}(\\mu_{k}),\\mathrm{T}-\\mathrm{Id}\\rangle_{L^{2}(\\mu_{k})}+\\mathcal{H}(\\mathrm{T}_{\\#}\\mu_{k})\\big)}\\\\ &{\\binom{\\tilde{\\mathrm{T}}_{k+1}}{\\mu_{k+1}}\\#\\mu_{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "This is true under the condition that $\\mu_{k}\\in\\mathcal{P}_{2,\\mathrm{ac}}(\\mathbb{R}^{d})$ implies that $\\nu_{k+1}\\in\\mathscr{P}_{2,\\mathrm{{ac}}}(\\mathbb{R}^{d})$ . ", "page_idx": 36}, {"type": "text", "text": "Proposition 24. Let $\\phi_{\\mu}$ be pushforward compatible, $\\mu_{0}\\,\\in\\,\\mathcal{P}_{\\mathrm{2,ac}}(\\mathbb{R}^{d})$ and assume that if $\\mu_{k}~\\in$ $\\mathcal{P}_{\\mathrm{2,ac}}(\\mathbb{R}^{d})$ then $\\nu_{k+1}\\in\\mathscr{P}_{2,\\mathrm{{ac}}}(\\mathbb{R}^{d})$ . Then the schemes (104) and (105) are equivalent. ", "page_idx": 36}, {"type": "text", "text": "Proof. See Appendix H.11. ", "page_idx": 36}, {"type": "text", "text": "We are now ready to state the convergence results for the proximal gradient scheme. ", "page_idx": 36}, {"type": "text", "text": "Proposition 25. Let $\\begin{array}{r}{\\mu_{0}\\,\\in\\,\\mathcal{P}_{\\mathrm{2,ac}}(\\mathbb{R}^{d}),\\,\\tau\\,\\leq\\,\\frac{1}{\\beta}}\\end{array}$ and $\\mathcal F(\\mu)\\,=\\,\\mathcal G(\\mu)\\,+\\,\\mathcal H(\\mu)$ . Consider the iterates of the Bregman proximal gradient scheme (104), equivalently (105). Let $k\\ge0$ . Assume $\\tilde{\\mathcal{H}}_{\\mu_{k}}$ is convex on $L^{2}(\\mu_{k})$ and $\\mathscr{G}\\ \\beta$ -smooth relative to $\\phi$ along $t\\mapsto\\left((1-t)\\mathrm{Id}+t\\tilde{\\mathrm{T}}_{k+1}\\right)_{\\#}\\mu_{k}$ . Then, for all $\\mathrm{T}\\in L^{2}(\\mu_{k})$ , ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\tau(\\mu_{k+1})\\leq\\mathcal{H}(\\mathrm{T}_{\\#}\\mu_{k})+\\mathcal{G}(\\mu_{k})+\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{G}(\\mu_{k}),\\mathrm{T}-\\mathrm{Id}\\rangle_{L^{2}(\\mu_{k})}+\\frac{1}{\\tau}\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{T},\\mathrm{Id})-\\frac{1}{\\tau}\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{T},\\tilde{\\mathrm{T}}_{k+1}).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Moreover, for $\\mathrm{T}=\\mathrm{Id},$ , ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathcal{F}(\\mu_{k+1})\\leq\\mathcal{F}(\\mu_{k})-\\frac{1}{\\tau}\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{Id},\\tilde{\\mathrm{T}}_{k+1}),\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "i.e., the scheme decreases the objective at each iteration. Additionally, let $\\alpha\\,\\geq\\,0$ , $\\nu\\,\\in\\,\\mathcal{P}_{2}(\\mathbb{R}^{d})$ and suppose that $\\phi_{\\mu}$ satisfies Assumption $^{\\,l}$ . If $\\mathcal{G}$ is $\\alpha$ -convex relative to $\\phi$ along $t\\mapsto\\left((1-t)\\mathrm{Id}+\\right.$ $t\\mathrm{T}_{\\phi_{\\mu_{k}}}^{\\mu_{k},\\nu})_{\\#}\\mu_{k}$ , then for all $k\\geq1$ , ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathcal{F}(\\mu_{k})-\\mathcal{F}(\\nu)\\leq\\frac{\\alpha}{\\left(1-\\tau\\alpha\\right)^{-k}-1}\\mathrm{W}_{\\phi}(\\nu,\\mu_{0})\\leq\\frac{1-\\alpha\\tau}{k\\tau}\\mathrm{W}_{\\phi}(\\nu,\\mu_{0}).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof. See Appendix H.12. ", "page_idx": 37}, {"type": "text", "text": "We verify now that Proposition 24 can be applied for mirror schemes of interest. Salim et al. [109, Lemma 2] showed that it holds for the Wasserstein proximal gradient when using some potential energies, more precisely with $\\begin{array}{r}{\\phi(\\mu)=\\int\\frac{1}{2}\\|\\cdot\\|_{2}^{2}\\,\\mathrm{d}\\mu}\\end{array}$ and $\\textstyle{\\mathcal{G}}(\\mu)={\\overline{{\\int U\\,\\mathrm{d}\\mu}}}$ with $U$ (strictly) convex. We extend their result for $\\textstyle\\mathcal{G}(\\mu)=\\int U\\,\\mathrm{d}\\mu$ and $\\textstyle\\phi(\\mu)=\\int V\\,\\mathrm{d}\\mu$ for $V$ strictly convex and $U\\;\\beta\\cdot$ -smooth relative to $V$ . ", "page_idx": 37}, {"type": "text", "text": "Lemma 26. Let $\\mu\\in\\mathcal{P}_{2,\\mathrm{ac}}(\\mathbb{R}^{d})$ , $\\begin{array}{r}{\\mathcal{G}(\\mu)=\\int U\\,\\mathrm{d}\\mu,\\,\\phi_{\\mu}(\\mathrm{T})=\\int V\\circ\\mathrm{T}\\,\\mathrm{d}\\mu}\\end{array}$ with $V$ strongly convex and $U\\beta$ -smooth relative to $V$ , and $\\mathrm{T}=\\nabla V^{*}\\circ(\\nabla V-\\tau\\nabla U)$ . Assume $\\begin{array}{r}{\\tau<\\frac{1}{\\beta}}\\end{array}$ , then $\\mathrm{T}_{\\#}\\mu\\in\\mathcal{P}_{2,\\mathrm{ac}}(\\mathbb{R}^{d})$ . ", "page_idx": 37}, {"type": "text", "text": "Sketch of the proof. The proof of the lemma is inspired from [109, Lemma 2]. We apply [5, Lemma 5.5.3], which requires to show that $\\mathrm{T}$ is injective almost everywhere and that $|\\operatorname*{det}\\nabla\\mathrm{T}|>0$ almost everywhere. See Appendix H.13 for the full proof. \u53e3 ", "page_idx": 37}, {"type": "text", "text": "To apply Proposition 25, we need $\\mathcal{H}$ to be convex along some curve. We discuss here the convexity of the negative entropy along acceleration free curves. Let $\\mu\\in\\mathcal{P}_{2,\\mathrm{ac}}(\\mathbb{R}^{d})$ , and denote $\\rho$ its density $w.r t$ the Lebesgue measure. For $\\begin{array}{r}{\\mathcal{H}(\\mu)=\\int f\\big(\\rho(x)\\big)\\;\\mathrm{d}x}\\end{array}$ where $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ is $C^{1}$ and satisfies $f(0)=0$ , $\\textstyle\\operatorname*{lim}_{x\\to0}x f^{\\prime}(x)=0$ and $x\\mapsto f(x^{-d})x^{d}$ is convex and non-increasing on $\\mathbb{R}_{+}$ , then by [117, Theorem 4.2], $\\mathcal{H}$ is convex along curves $\\mu_{t}=\\big((1-t)\\mathrm{S}+t\\mathrm{T}\\big)_{\\#}\\mu$ obtained with S and $\\mathrm{T}$ with positive definite Jacobians. This is the case e.g. for $f(x)=x\\log x$ , for which $\\mathcal{H}$ corresponds to the negative entropy. By Remark 3, to be able to apply the three-point inequality (that is necessary to obtain the descent lemma), we actually only need $\\mathcal{H}$ to be convex along $\\left((1-t)\\tilde{\\mathrm{T}}_{k+1}+t\\mathrm{Id}\\right)_{\\#}\\mu_{k}$ and along $\\left[1-\\right.$ $t)\\tilde{\\mathrm{T}}_{k+1}+t\\mathrm{T}_{\\phi_{\\mu_{k}}}^{\\mu_{k},\\nu})_{\\#}\\mu_{k}$ T\u03d5\u00b5k\u00b5,\u03bd #\u00b5k for the convergence. ", "page_idx": 37}, {"type": "text", "text": "Gaussian target. In what follows, we focus on $\\textstyle{\\mathcal{G}}(\\mu)=\\int U\\mathrm{d}\\mu$ with $\\begin{array}{r}{U(x)=\\frac{1}{2}(x-m)^{T}\\Sigma^{-1}(x-1)^{-\\frac{1}{2}}}\\end{array}$ $m)$ for $\\Sigma\\in S_{d}^{++}(\\mathbb{R})$ , $\\mathcal{H}$ the negative entropy and with a Bregman potential of the form $\\phi(\\mu)=$ $\\int V\\mathrm{d}\\mu$ with $\\hat{V(x)}={\\textstyle{\\frac{1}{2}}}x^{T}\\Lambda^{-1}x$ . Moreover, we suppose $\\mu_{0}=\\mathcal{N}(m_{0},\\Sigma_{0})$ . In this situation, each distribution $\\mu_{k}$ is also Gaussian, as the forward and backward steps are affine operations. ", "page_idx": 37}, {"type": "text", "text": "Assuming the covariances matrices are full rank, $\\tilde{\\mathrm{T}}_{k+1}$ is affine and its gradient is invertible. Moreover, by Proposition 15, $\\mathrm{T}_{\\phi_{\\mu_{k}}}^{\\mu_{k},\\nu}=\\nabla u\\circ\\nabla_{\\mathrm{W}_{2}}\\phi(\\mu_{k})$ for $\\nabla u$ an OT map between $\\nabla_{\\mathrm{W_{2}}}\\phi(\\mu_{k})_{\\#}\\mu_{k}$ and $\\nu$ . Since each distribution is Gaussian, and $\\nabla_{\\mathrm{W}_{2}}\\phi(\\mu_{k})(x)=\\Lambda^{-1}x$ is affine, it has a positive definite Jacobian. Thus, using [117, Theorem 4.2], we can conclude that we can apply Proposition 25. ", "page_idx": 37}, {"type": "text", "text": "Closed-form for Gaussians. Let $\\textstyle{\\mathcal{G}}(\\mu)\\,=\\,\\int U\\mathrm{d}\\mu$ with $U(x)\\,=\\,{\\textstyle{\\frac{1}{2}}}(x-m)^{T}\\Sigma^{-1}(x-m),$ , $\\Sigma\\in$ $S_{d}^{++}(\\mathbb{R})$ , $m\\in\\mathbb{R}^{d}$ , and $\\begin{array}{r}{\\mathcal{H}(\\mu)=\\int\\log\\left(\\rho(x)\\right)\\,\\mathrm{d}\\mu(x)}\\end{array}$ for $\\mathrm{d}\\mu=\\rho(x)\\mathrm{d}x$ . For the Bregman potential, we will choose $\\textstyle\\phi(\\mu)=\\int V\\mathrm{d}\\mu$ for $V(x)=\\textstyle{\\frac{1}{2}}\\langle x,\\Lambda^{-1}x\\rangle$ . Recall that the forward step reads as ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathrm{S}_{k+1}=\\nabla V^{\\ast}\\circ\\left(\\nabla V-\\tau\\nabla_{\\mathrm{W}_{2}}\\mathcal{G}(\\mu_{k})\\right),\\quad\\nu_{k+1}=(\\mathrm{S}_{k+1})_{\\#}\\mu_{k}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Since $\\nabla V(x)=\\Lambda^{-1}x$ , and $\\mu_{k}=\\mathcal{N}(m_{k},\\Sigma_{k})$ , we obtain for $x_{k}\\sim\\mu_{k}$ , ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathrm{S}_{k+1}(x_{k})=\\Lambda\\bigl(\\Lambda^{-1}x_{k}-\\tau\\Sigma^{-1}(x_{k}-m)\\bigr)=x_{k}-\\tau\\Lambda\\Sigma^{-1}(x_{k}-m).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Thus, the output of the forward step is still a Gaussian of the form $\\nu_{k+1}=\\mathcal{N}(m_{k+\\frac{1}{2}},\\Sigma_{k+\\frac{1}{2}})$ with ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{m_{k+\\frac{1}{2}}=(I_{d}-\\tau\\Lambda\\Sigma^{-1})m_{k}+\\tau\\Lambda\\Sigma^{-1}m}\\\\ {\\Sigma_{k+\\frac{1}{2}}=(I_{d}-\\tau\\Lambda\\Sigma^{-1})^{T}\\Sigma_{k}(I_{d}-\\tau\\Lambda\\Sigma^{-1}).}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Since $\\nabla V$ is linear, the output of the backward step stays Gaussian. Moreover, the first order conditions give ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla V\\circ\\mathrm{T}_{k+1}+\\tau\\nabla\\log(\\rho_{k+1}\\circ\\mathrm{T}_{k+1})=\\nabla V}\\\\ &{\\qquad\\qquad\\iff\\forall x,\\,\\Lambda^{-1}x=\\Lambda^{-1}\\mathrm{T}_{k+1}(x)-\\tau\\Sigma_{k+1}^{-1}(\\mathrm{T}_{k+1}(x)-m_{k+1})}\\\\ &{\\qquad\\qquad\\qquad\\iff\\forall x,\\,x=\\mathrm{T}_{k+1}(x)-\\tau\\Lambda\\Sigma_{k+1}^{-1}(\\mathrm{T}_{k+1}(x)-m_{k+1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Thus, the output is a Gaussian $\\mathcal{N}(m_{k+1},\\Sigma_{k+1})$ with $(m_{k+1},\\Sigma_{k+1})$ satisfying ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{m_{k+1}=m_{k+\\frac{1}{2}}}\\\\ {\\Sigma_{k+\\frac{1}{2}}=(I_{d}-\\tau\\Lambda\\Sigma_{k+1}^{-1})^{T}\\Sigma_{k+1}(I_{d}-\\tau\\Lambda\\Sigma_{k+1}^{-1}).}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Moreover, if $\\Lambda$ and $\\Sigma_{k+1}$ commute, this is equivalent to ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\Sigma_{k+1}^{2}-(2\\tau\\Lambda+\\Sigma_{k+\\frac{1}{2}})\\Sigma_{k+1}+\\tau^{2}\\Lambda^{2}=0,\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "which solution is given by ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\Sigma_{k+1}={\\frac{1}{2}}\\bigl(\\Sigma_{k+\\frac{1}{2}}+2\\tau\\Lambda+\\bigl(\\Sigma_{k+\\frac{1}{2}}(4\\tau\\Lambda+\\Sigma_{k+\\frac{1}{2}})\\bigr)^{\\frac{1}{2}}\\bigr).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "To sum up, the update is ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\nu_{k+1}=\\mathcal{N}\\big((I_{d}-\\tau\\Lambda\\Sigma^{-1})m_{k}+\\tau\\Lambda\\Sigma^{-1}m,(I_{d}-\\tau\\Lambda\\Sigma^{-1})^{T}\\Sigma_{k}(I_{d}-\\tau\\Lambda\\Sigma^{-1})\\big)}\\\\ {\\mu_{k+1}=\\mathcal{N}\\big(m_{k+\\frac{1}{2}},\\frac{1}{2}\\big(\\Sigma_{k+\\frac{1}{2}}+2\\tau\\Lambda+(\\Sigma_{k+\\frac{1}{2}}(4\\tau\\Lambda+\\Sigma_{k+\\frac{1}{2}}))^{\\frac{1}{2}}\\big)\\big).}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "For $\\Lambda=\\Sigma$ , we call it the ideally preconditioned Forward-Backward scheme (PFB). ", "page_idx": 38}, {"type": "text", "text": "G Additional details on experiments ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "G.1 Implementing the schemes ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "In this subsection, we sum up how to implement the different schemes in practice, given a finite number of particles. In all cases, we first sample x(10 ), . . . , x(n0) , then we apply the scheme to $\\begin{array}{r}{\\hat{\\mu}_{n}^{(k)}=\\frac{1}{n}\\,\\bar{\\sum_{i=1}^{n}\\delta_{x_{i}^{(k)}}}}\\end{array}$ . ", "page_idx": 38}, {"type": "text", "text": "Mirror descent. In general, for $\\phi$ pushforward compatible, one needs to solve at each iteration $k\\geq0$ , ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\nabla_{\\mathrm{W}_{2}}\\phi(\\mu_{k+1})\\circ\\mathrm{T}_{k+1}=\\nabla_{\\mathrm{W}_{2}}\\phi(\\mu_{k})-\\tau\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k}).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "If $\\textstyle\\phi(\\mu)=\\int V\\,\\mathrm{d}\\mu$ with $\\nabla V$ having an analytical inverse, the scheme can be implemented as ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\forall k\\geq0,\\ \\forall i\\in\\{1,\\ldots,n\\},\\ x_{i}^{(k+1)}=\\nabla V^{*}\\big(\\nabla V(x_{i}^{(k)})-\\tau\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\hat{\\mu}_{n}^{(k)})(x_{i}^{(k)})\\big).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Except for this case, one cannot in general invert $\\nabla_{\\mathrm{W}_{2}}\\phi(\\mu_{k+1})\\,\\circ\\,\\mathrm{T}_{k+1}$ directly. A practical workaround is to solve an implicit problem, see e.g. [133]. Here, we use the Newton-Raphson algorithm. Suppose we have \u00b5k = n1 in=1 \u03b4 and we are looking for $\\begin{array}{r}{\\mu_{k+1}=\\frac{1}{n}\\sum_{i=1}^{n}\\delta_{x_{i}}}\\end{array}$ . Then, the scheme is equivalent to ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\forall j\\in\\{1,\\ldots,n\\},\\:G_{j}(x_{1},\\ldots,x_{n})=0,\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "for ", "page_idx": 38}, {"type": "equation", "text": "$$\nG_{j}(x_{1},\\ldots,x_{n})=\\nabla_{\\mathbf{W}_{2}\\phi}\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\delta_{x_{i}}\\right)(x_{j})-\\nabla_{\\mathbf{W}_{2}}\\phi(\\mu_{k})(x_{j}^{(k)})+\\tau\\nabla_{\\mathbf{W}_{2}}\\mathcal{F}(\\mu_{k})(x_{j}^{(k)}).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Write $\\mathcal{G}(x_{1},\\ldots,x_{n})=\\left(G_{1}(x_{1},\\ldots,x_{n}),\\ldots,G_{n}(x_{1},\\ldots,x_{n})\\right)$ . Then, at each step $k$ , we perform the following Newton iterations, starting from $(x_{1}^{(k)},\\ldots,x_{n}^{(k)})$ : ", "page_idx": 38}, {"type": "equation", "text": "$$\n(x_{1}^{(k_{\\ell+1})},\\ldots,x_{n}^{(k_{\\ell+1})})=(x_{1}^{(k_{\\ell})},\\ldots,x_{n}^{(k_{\\ell})})-\\gamma\\big(J_{\\mathcal{G}}(x_{1}^{(k_{\\ell})},\\ldots,x_{n}^{(k_{\\ell})})\\big)^{-1}\\mathcal{G}(x_{1}^{(k_{\\ell})},\\ldots,x_{n}^{(k_{\\ell})}).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "The Jacobian is of size $n d\\times n d$ , which does not scale well with the dimension and the number of samples. We can reduce the complexity of the algorithm by relying on inverse Hessian vector products, see e.g. [40]. ", "page_idx": 38}, {"type": "image", "img_path": "N12B6wvA55/tmp/be493ad64220139d7a19224a1e94a88288fc10979cb7848bc47893d10dcaf0c2.jpg", "img_caption": ["Figure 4: (Left) Value of $\\mathcal{W}$ along the flow for two difference interaction Bregman potentials, (Right) Trajectories of particles to minimize $\\mathcal{W}$ . "], "img_footnote": [], "page_idx": 39}, {"type": "image", "img_path": "N12B6wvA55/tmp/2273b4a94be42b7e8bc44bdc89162569eac71f824b057f637703fe36b90a87e9.jpg", "img_caption": ["Figure 5: Convergence towards Gaussian ${\\mathcal{N}}(0,D)$ with $D$ diagonal and uniformly sampled on $[0,50]^{10}$ . "], "img_footnote": [], "page_idx": 39}, {"type": "text", "text": "Preconditioned gradient descent. Plugging the empirical measure in (11), the preconditioned scheme can be implemented as ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\forall k\\geq0,\\forall i\\in\\{1,\\ldots,n\\},\\ x_{i}^{(k+1)}=x_{i}^{(k)}-\\tau\\nabla h^{*}\\big(\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\hat{\\mu}_{n}^{(k)})(x_{i}^{(k)})\\big).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "G.2 Mirror descent of interaction energies ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Details of Section 5. We detail in this Section the first experiment of Section 5. We aim at minimizing the interaction energy $\\begin{array}{r}{\\mathcal{W}(\\mu)=\\frac{1}{2}\\iint W(x-y)\\,\\mathrm{d}\\mu(\\dot{x})\\mathrm{d}\\mu(y)}\\end{array}$ for $\\begin{array}{r}{W(z)=\\frac{1}{4}\\|z\\|_{2}^{4}\\!-\\!\\frac{1}{2}\\|z\\|_{2}^{2}}\\end{array}$ . It is well-known that the stationary solution of its gradient flow is a Dirac ring [27]. Since the stationary solution is translation invariant, we project the measures to be centered. ", "page_idx": 39}, {"type": "text", "text": "We study here two Bregman potentials which are also interaction energies. First, observing that $\\nabla^{2}W(z)=2z z^{T}+\\bigl(\\|\\bar{z}\\|_{2}^{2}-\\bar{1}\\bigr)I_{d}$ , we have for all $z$ , ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\|\\nabla^{2}W\\|_{\\mathrm{op}}\\leq2\\|z\\|_{2}^{2}+\\|z\\|_{2}^{2}+1=3\\|z\\|_{2}^{2}+1=p_{2}(\\|z\\|_{2}),\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "with $p_{2}(t)=3t^{2}+1$ . Thus, by [88, Remark 2], $W$ is $\\beta$ -smooth relative to $\\begin{array}{r}{K_{4}(z)=\\frac{1}{4}\\|z\\|_{2}^{4}+\\frac{1}{2}\\|z\\|_{2}^{2}}\\end{array}$ with $\\beta\\,=\\,4$ . Thus, using Proposition 23, $\\tilde{\\mathcal{W}}_{\\mu}$ is $\\beta.$ -smooth relative to $\\begin{array}{r}{\\phi_{\\mu}(\\mathrm{T})\\,=\\,\\frac12\\int\\!\\!\\int K\\big(\\mathrm{T}(x)\\,-\\,}\\end{array}$ $\\mathrm{T}(\\boldsymbol{x}^{\\prime}))\\;\\mathrm{d}\\mu(\\boldsymbol{x})\\mathrm{d}\\mu(\\boldsymbol{x}^{\\prime})$ for all $\\mu$ , and we can apply Proposition 3. ", "page_idx": 39}, {"type": "text", "text": "Under the additional hypothesis that the measures are compactly supported, and thus there exists $M>0$ such that $\\|x\\|_{2}^{2}\\leq M$ for $\\mu$ -almost every $x$ , we can also show that $W$ is $\\beta$ -smooth relative to $\\begin{array}{r}{K_{2}(z)=\\frac{1}{2}\\|z\\|_{2}^{2}}\\end{array}$ . Indeed, on one hand, $\\nabla^{2}K=I_{d}$ and $\\nabla^{2}W(z)=2z z^{T}+\\bigl(\\|z\\|_{2}^{2}-1\\bigr)I_{d}$ . Thus, for all $\\boldsymbol{v},\\boldsymbol{z}\\in\\mathbb{R}^{d}$ , ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{v^{T}\\nabla^{2}W(z)v=2\\langle z,v\\rangle^{2}+(\\|z\\|_{2}^{2}-1)\\|v\\|_{2}^{2}\\leq3\\|z\\|_{2}^{2}\\|v\\|_{2}^{2}\\leq3M\\|v\\|_{2}^{2}=3M v^{T}\\nabla^{2}K(z)v.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "In Figure 4, we plot the evolution of $\\mathcal{W}$ along the flows obtained with these two Bregman potential, starting from $\\bar{\\mu_{0}^{-}}\\!=\\!\\mathcal{N}(0,0.25^{2}I_{2})$ for $n=100$ particles, with a step size of $\\tau=0.1$ for 120 epochs. ", "page_idx": 39}, {"type": "text", "text": "Ill-conditioned interaction energy. We also study the minimization of an interaction energy with an ill-conditioned kernel ${\\cal W}(z)\\stackrel{\\smile}{=}{\\textstyle{\\frac{1}{4}}}(z^{T}\\Sigma^{-1}z)^{2}-{\\textstyle{\\frac{\\circ}{2}}}z^{T}\\Sigma^{-1}z$ where $\\Sigma\\in S_{d}^{++}(\\mathbb{R})$ but is possibly badly conditioned, i.e. the ratio between the largest and smallest eigenvalues is large. In this case, the stationary solution becomes an ellipsoid instead of a ring. In our experiments, we take $\\Sigma=\\mathrm{diag}(100,0.1)$ . For each scheme, we use $\\mu_{0}=\\mathcal{N}(0,0.25^{2}I_{2})$ , $n=100$ particles and a step size of $\\tau=0.1$ . ", "page_idx": 39}, {"type": "text", "text": "$\\begin{array}{r}{K_{2}^{\\Sigma}(z)=\\frac{1}{2}z^{T}\\Sigma^{-1}z}\\end{array}$ aBnrde ${\\check{K}}_{4}^{\\Sigma}(\\dot{z^{\\prime}})={\\textstyle{\\frac{1}{4}}}(z^{T}\\Sigma^{-1}z)^{2}\\!-\\!{\\textstyle{\\frac{1}{2}}}\\big(z^{T}\\Sigma^{-1}z\\big)$ , nat ntdh iws ec oobnsdeitrivoen tihnagt, t nhea cmoenlyv erwgee nucsee is much faster compared to the same kernels without preconditioning. For $\\begin{array}{r}{K_{2}^{\\Sigma}(z)=\\frac{1}{2}z^{T}\\Sigma^{-1}z}\\end{array}$ , the scheme becomes ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\nabla K\\star\\mu_{k+1})\\circ\\mathrm{T}_{k+1}=\\nabla K\\star\\mu_{k}-\\gamma\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k})}\\\\ &{\\iff\\Sigma^{-1}\\big(\\mathrm{T}_{k+1}-m(\\mu_{k+1})\\big)=\\Sigma^{-1}\\big(\\mathrm{Id}-m(\\mu_{k})\\big)-\\gamma\\Sigma^{-1}(\\mathrm{Id}^{T}\\Sigma^{-1}\\mathrm{Id}-1)\\mathrm{Id}}\\\\ &{\\iff\\mathrm{T}_{k+1}-m(\\mu_{k+1})=\\mathrm{Id}-m(\\mu_{k})-\\gamma(\\mathrm{Id}^{T}\\Sigma^{-1}\\mathrm{Id}-1)\\mathrm{Id}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "image", "img_path": "N12B6wvA55/tmp/fccb9054381328f54c476ff616f0d9e0fe2cdf2d5677a7246ed3ed4d67ec3354.jpg", "img_caption": ["Figure 6: (Left) Value of $\\mathcal{W}$ over time and trajectory of particles using $K_{4}$ and $K_{4}^{\\Sigma}$ as interaction kernels. (Right) Value of $\\mathcal{W}$ over time and trajectory of particles for the Wasserstein gradient descent and preconditioned Wasserstein gradient descent (with the ideal preconditioner $h^{*}(\\check{x})=\\textstyle\\frac{1}{2}x^{T}\\Sigma x)$ . "], "img_footnote": [], "page_idx": 40}, {"type": "text", "text": "Thus, we see that $\\Sigma^{-1}$ has less influence which might explain the faster convergence. ", "page_idx": 40}, {"type": "text", "text": "Similarly as in the case without preconditioning, using that $\\nabla^{2}W(z)=2\\Sigma^{-1}z z^{T}\\Sigma^{-1}+(z^{T}\\Sigma^{-1}z-$ $1)\\Sigma^{-1}$ , we can show that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{v^{T}\\nabla^{2}W(z)v=2\\langle z,v\\rangle_{\\Sigma^{-1}}^{2}+(\\|z\\|_{\\Sigma^{-1}}^{2}-1)\\|v\\|_{\\Sigma^{-1}}^{2}\\leq3M\\|v\\|_{\\Sigma^{-1}}^{2}=3M v^{T}\\nabla^{2}K(z)v.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "For the sake of comparison, we also report on Figure 6 the trajectories of particles for the use of $K_{4}$ and $K_{4}^{\\Sigma}$ , as well as of the usual Wasserstein gradient descent and the preconditioned Wasserstein gradient descent obtained with $h^{*}(x)={\\textstyle\\frac{1}{2}}x^{\\check{T}}\\Sigma x$ (which is equivalent to the Mirror Descent with $\\phi_{\\mu}^{V}$ as Bregman potential and $V(x)={\\textstyle\\frac{1}{2}}x^{\\bar{T}}\\Sigma^{-1}x)$ . We observe almost the same trajectories as $K_{2}$ which would indicate that the target is also smooth compared to $\\phi_{\\mu}^{V}$ . ", "page_idx": 40}, {"type": "text", "text": "Runtime. These experiments were run on a personal Laptop with a CPU Intel Core i5-9300H. For the interaction energy as Bregman potential, running the algorithm with Newton\u2019s method for $n=100$ particles in dimension $d=2$ for 120 epochs took about $5\\mathrm{mn}$ for $K_{2}$ and $K_{2}^{\\Sigma}$ , and about 1h for $K_{4}$ and $K_{4}^{\\Sigma}$ . ", "page_idx": 40}, {"type": "text", "text": "G.3 Mirror descent on Gaussians ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "As the mirror descent scheme cannot be computed in closed-form for Bregman potentials which are not potential energies, and thus are computationally costly, we propose here to restrain ourselves to the Gaussian setting. ", "page_idx": 40}, {"type": "text", "text": "We choose as target distribution $\\boldsymbol{\\nu}=\\mathcal{N}(\\boldsymbol{0},\\boldsymbol{\\Sigma})$ for $\\Sigma$ a symmetric positive definite matrix in $\\mathbb{R}^{10\\times10}$ , and the functional to be optimized is $\\begin{array}{r}{\\mathcal{F}(\\mu)=\\int V\\mathrm{d}\\mu+\\mathcal{H}(\\mu)}\\end{array}$ with $V(x)={\\textstyle{\\frac{1}{2}}}x^{T}\\Sigma^{-1}x$ . The initial distribution is always chosen as $\\mu_{0}=\\mathcal{N}(0,I_{d})$ . In all cases, the step size is chosen as $\\tau=0.01$ , and we run the scheme for 1500 iterations. For the target distributions, we sample 20 random covariances of the form $\\Sigma=U D U^{T}$ with $D$ evenly spaced in log scale between 1 and 100, and $U\\in\\mathbb{R}^{10\\times10}$ chosen as a uniformly random orthogonal matrix, as in [43], and we report the averaged KL divergence over iterations in Figure 2. We add on Figure 5 the same experiments with targets of the form ${\\mathcal{N}}(0,D)$ where $D$ is a diagonal matrix on $\\bar{\\mathbb{R}}^{10\\times10}$ sampled uniformly over $[0,5\\bar{0}]^{10}$ . We compare here the Forward-Backward (FB) scheme of [43], the ideally preconditioned ForwardBackward scheme (PFB), which uses the closed-form (116) derived in Appendix $\\mathrm{F}$ with $\\Lambda=\\Sigma$ , and the Mirror Descent with negative entropy Bregman potential (NEM), whose closed-form was derived in Appendix D.3, and which we recall: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\forall k\\geq0,\\ \\Sigma_{k+1}^{-1}=\\big((1-\\tau)\\Sigma_{k}^{-1}+\\tau\\Sigma^{-1}\\big)^{T}\\Sigma_{k}\\big((1-\\tau)\\Sigma_{k}^{-1}+\\tau\\Sigma^{-1}\\big).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "We also experiment with the KL divergence as Bregman potential (KLM) and the ideally preconditioned KL divergence (PKLM). We observe that, even though the objective is convex relative to the Bregman potential, this scheme does not always converge. It might be due to its gradient which might not always be invertible. We leave further investigations for future works. ", "page_idx": 40}, {"type": "text", "text": "Remark 2. We note that using as Bregman potential $\\begin{array}{r}{\\phi_{\\mu}(\\mathrm{T})=\\int\\psi\\circ\\mathrm{Td}\\mu_{\\mathcal{I}}}\\end{array}$ for $\\psi(x)=\\textstyle{\\frac{1}{2}}x^{T}\\Lambda^{-1}x$ is equivalent to using a preconditioner with $h^{*}(x)=\\textstyle{\\frac{1}{2}}x^{T}\\Lambda x$ . ", "page_idx": 40}, {"type": "text", "text": "Analysis of the convergence. It is well-known that along the Wasserstein gradient flow of the KL divergence starting from a Gaussian and with a Gaussian target (Ornstein-Uhlenbeck process), the measures stay Gaussian [129]. Thus, the Forward-Backward scheme has Gaussian iterates at each step [43, 109]. In this work, we also use a linearly preconditioned Forward-Backward scheme, whose closed-form is derived in (116) (Appendix F). For the Bregman potential, we choose $\\textstyle\\phi_{\\mu}(\\mathrm{T})=\\int\\psi\\circ\\mathrm{T}\\,\\mathrm{d}\\mu$ for $\\psi(x)={\\textstyle\\frac{1}{2}}x^{T}\\Sigma x$ . In this situation, $\\textstyle{\\mathcal{G}}(\\mu)=\\int V\\mathrm{d}\\bar{\\mu}$ is 1-smooth and 1-convex relative to $\\phi$ . Thus, we can apply Proposition 25. We refer to Appendix F for more details on the convexity of $\\mathcal{H}$ . ", "page_idx": 40}, {"type": "image", "img_path": "N12B6wvA55/tmp/13b43d832cf863755dccb8122f54ec5f3ba3de53112bd446bcffe5f801764176.jpg", "img_caption": ["Figure 7: Preconditioned GD and (vanilla) GD vs. the entropic map $T_{\\varepsilon}$ [101] to predict the responses of cell populations to cancer treatments on 4i and scRNAseq datasets, providing respectively 34 and 9 treatment responses. For each profiling technology and each treatment, we have a pair $(\\mu_{i},\\nu_{i})$ of source (untreated) cells and target (treated) cells. For each pair $(\\mu_{i},\\nu_{i})$ , with both preconditioned GD and vanilla GD, we minimize the functional $\\mathcal{F}(\\mu)=D(\\mu,\\nu_{i}).$ \u2013with D a metric\u2013to recover the effect of the perturbation. In both cases, the prediction is obtained by $\\hat{\\mu_{i}}=\\operatorname*{min}_{\\mu}\\mathcal{F}(\\mu)$ . We then fit an entropic map $T_{\\varepsilon}$ and predict $T_{\\varepsilon}\\sharp\\mu_{i}$ . We then compare the objective function values $\\mathcal{F}(\\hat{\\mu_{i}})$ and $\\mathcal{F}(T_{\\varepsilon}\\sharp\\hat{\\mu_{i}})$ . A point below the diagonal $y=x$ then refers to an experiment in which (preconditioned) WGD provides a better estimate of the perturbed population. "], "img_footnote": [], "page_idx": 41}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "For Bregman potentials whose gradient is not affine, the distributions do not necessarily stay Gaussian along the flows. Thus, we work on the Bures-Wasserstein space and use the Bures-Wasserstein gradient, i.e. we project the gradient on the space of affine maps with symmetric linear term, i.e. of the form $\\operatorname{T}(x)=b+S(x-m)$ with $S\\in S_{d}(\\mathbb{R})$ [43]. We refer to [43, 73] for more details on this submanifold. This can be seen as performing Variational Inference. We derive the closed-form of the different schemes in Appendix D.3. ", "page_idx": 41}, {"type": "text", "text": "Even though these procedures do not fti exactly the theory developed in this work, we show the relative smoothness of $\\mathcal{F}$ relative to $\\mathcal{H}$ along the curve $\\mu_{t}=\\left(({\\dot{1}}-t)\\mathrm{Id}+t\\mathrm{T}_{k+1}\\right)_{\\#}\\mu_{k}$ under the hypothesis that the covariances matrices have bounded eigenvalues. Moreover, since $\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu}}^{\\mathrm{\"}}=\\mathrm{d}_{\\phi_{\\mu}^{V}}+\\mathrm{d}_{\\tilde{\\mathcal{H}}_{\\mu}}\\geq\\mathrm{d}_{\\tilde{\\mathcal{H}}_{\\mu}}$ , $\\mathcal{F}$ is also 1-convex relative to $\\mathcal{H}$ . ", "page_idx": 41}, {"type": "text", "text": "Proposition 27. Let $\\lambda>0$ , $\\begin{array}{r}{\\mathcal{F}(\\mu)=\\int V\\mathrm{d}\\mu+\\mathcal{H}(\\mu)}\\end{array}$ with $V(x)={\\textstyle{\\frac{1}{2}}}x^{T}\\Sigma^{-1}x$ where $\\Sigma\\in S_{d}^{++}(\\mathbb{R})$ and $\\Sigma\\preceq\\lambda I_{d}$ . Suppose that for all $k\\geq0,$ , $(1-\\tau)\\Sigma_{k+1}\\Sigma_{k}^{-1}+\\tau\\Sigma_{k+1}\\Sigma^{-1}\\succeq0.$ . Then, $\\mathcal{F}$ is smooth relative to $\\mathcal{H}$ along $\\mu_{t}\\,=\\,\\bigl((1\\,-\\,t)\\mathrm{Id}+t\\mathrm{T}_{k+1}\\bigr)_{\\#}\\mu_{k}$ where $\\mu_{k}\\,=\\mathcal{N}(0,\\Sigma_{k})$ with $\\Sigma_{k}\\,\\in\\,S_{d}^{++}(\\mathbb{R})$ , $\\Sigma_{k}\\preceq\\lambda I_{d}$ . ", "page_idx": 41}, {"type": "text", "text": "G.4 Single-cell experiments ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "First, we provide more details on the experiment on single cells of Section 5. Then, we detail a second experiment comparing the method with using a static map. ", "page_idx": 41}, {"type": "text", "text": "Details on the metrics. We show the benefits of using the polynomial preconditioner over the single-cell datasets for different metrics. ", "page_idx": 42}, {"type": "text", "text": "The first one considered is the Sliced-Wasserstein distance [16, 102], defined as ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\forall\\mu,\\nu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d}),\\;\\mathrm{SW}_{2}^{2}(\\mu,\\nu)=\\int_{S^{d-1}}\\mathrm{W}_{2}^{2}(P_{\\#}^{\\theta}\\mu,P_{\\#}^{\\theta}\\nu)\\;\\mathrm{d}\\lambda(\\theta),\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where $S^{d-1}\\,=\\,\\{\\theta\\,\\in\\,\\mathbb{R}^{d}$ , $\\|\\theta\\|_{2}\\,=\\,1\\}$ , $\\lambda$ denotes the uniform distribution on $S^{d-1}$ and for all $\\theta\\,\\in\\,S^{d-1}$ , $x\\in\\mathbb{R}^{d}$ , $P^{\\theta}(x)\\,=\\,\\langle x,\\theta\\rangle$ . For $\\begin{array}{r}{\\mathcal{F}(\\mu)\\,=\\,\\frac{1}{2}\\mathrm{SW}_{2}^{2}(\\mu,\\nu)}\\end{array}$ , the Wasserstein gradient can be computed as [18] ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\nabla_{\\mathrm{W}_{2}}\\mathcal F(\\mu)=\\int_{S^{d-1}}\\psi_{\\theta}^{\\prime}\\big(P^{\\theta}(x)\\big)\\theta\\;\\mathrm{d}\\lambda(\\theta),\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where, for $t\\in\\mathbb R$ , $\\psi_{\\theta}^{\\prime}(t)=t-F_{P_{\\#}^{\\theta}\\nu}^{-1}\\big(F_{P_{\\#}^{\\theta}\\mu}(t)\\big)$ with $F_{P_{\\#}^{\\theta}\\mu}$ the cumulative distribution function of $P_{\\#}^{\\theta}\\mu$ . In practice, we compute SW and its gradient using a Monte-Carlo approximation by first drawing $L$ uniform random directions $\\theta_{1},\\ldots,\\theta_{L}$ . ", "page_idx": 42}, {"type": "text", "text": "\u2022 The second one considered is the Sinkhorn divergence [50] defined as ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\forall\\mu,\\nu\\in\\mathscr{P}_{2}(\\mathbb{R}^{d}),\\ \\mathrm{S}_{\\varepsilon,2}^{2}(\\mu,\\nu)=\\mathrm{OT}_{\\varepsilon}(\\mu,\\nu)-\\frac{1}{2}\\mathrm{OT}_{\\varepsilon}(\\mu,\\mu)-\\frac{1}{2}\\mathrm{OT}_{\\varepsilon}(\\nu,\\nu),\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "with ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathrm{OT}_{\\varepsilon}(\\mu,\\nu)=\\operatorname*{inf}_{\\gamma\\in\\Pi(\\mu,\\nu)}\\,\\int\\|x-y\\|_{2}^{2}\\;\\mathrm{d}\\gamma(x,y)+\\varepsilon\\mathrm{KL}(\\gamma||\\mu\\otimes\\nu),\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "the entropic regularized OT. The Wasserstein gradient of $\\mathrm{S}_{\\varepsilon,2}^{2}$ is simply obtained as the potential [50]. ", "page_idx": 42}, {"type": "text", "text": "\u2022 Finally, we also consider the energy distance, defined as ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\forall\\mu,\\nu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d}),\\ \\mathrm{ED}(\\mu,\\nu)=-\\iint\\|x-y\\|_{2}\\,\\mathrm{d}(\\mu-\\nu)(x)\\mathrm{d}(\\mu-\\nu)(y).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "To compute its Wasserstein gradient, we use the sliced procedure of [60]. ", "page_idx": 42}, {"type": "text", "text": "Parameters chosen. For all the metrics, we fixed the step size at $\\tau=1$ . To choose the parameter $a$ of the preconditioner $h^{*}(x)=(\\|x\\|_{2}^{a}+1)^{1/a}-1$ , we ran a grid search over $a\\in\\{1.25,1.5,1.75\\}$ for a random treatment, and used it for all the others. In particular, we used for the dataset 4i $a=1.5$ for the Sinkhorn divergence and for SW, and $a=1.75$ for the energy distance. For the scRNAseq dataset, we used $a=1.25$ for the Sinkhorn divergence and SW, and $a=1.5$ for the energy distance. We note that for the dataset 4i, the data lie in dimension $d=48$ and $d=50$ for scRNAseq. For all the metrics, we first sampled 4096 particles from the source (untreated) dataset, and used in average between 2000 and 3000 samples from the target dataset. For the test value, we also added $40\\%$ of unseen cells following [21]. Note that we reported the results in Figure 3 for 3 different initializations for each treatment, and reported these results with their mean. We report the results using a fixed relative tolerance $\\mathrm{{tol}=10^{-3}}$ , i.e. at the first iteration where $|\\mathcal{F}(\\mu_{k})-\\mathcal{F}(\\mu_{k-1})|/\\mathcal{F}(\\mu_{k-\\bar{1}})\\leq$ tol, with a maximum value of iterations of $10^{4}$ . For the Sinkhorn divergence, we chose $\\varepsilon$ as $10^{-1}$ time the variance of the target. Finally, for SW and the computation of the gradient of the energy distance, we used a Monte-Carlo approximation with $L=1024$ projections. ", "page_idx": 42}, {"type": "text", "text": "Comparison to an OT static map. We now compare the prediction of the response of cells to a perturbation using Wasserstein gradient descent, with and without preconditioning, to the one provided by a static estimator, the entropic map $T_{\\varepsilon}$ [101]. This experiment motivates the use of a dynamic procedure, iterating multiple steps to map the unperturbed population $\\mu$ to the perturbed population $\\nu$ , instead of a unique static step. We use the proteomic dataset [21] as the one considered in 3. We use the default OTT-JAX [38] of $T_{\\varepsilon}$ . The results are shown in Figure 7. ", "page_idx": 42}, {"type": "text", "text": "Runtime. For this experiment, we used a GPU Tesla P100-PCIE-16GB. Depending on the convergence and on the metric considered, each run took in between 30s and $10\\mathrm{mn}$ . So in total, it took a few hundred of hours of computation time. ", "page_idx": 42}, {"type": "image", "img_path": "N12B6wvA55/tmp/bc56ac25478e46a1d8d1fb537f54779e5649107926011e5c64a59857547dfb1a.jpg", "img_caption": ["Figure 8: (Left) Samples from a Dirichlet posterior distribution for Mirror Descent (MD) and Mirror Langevin (MLD). (Right) Evolution of the objective averaged over 20 different initialisations. "], "img_footnote": [], "page_idx": 43}, {"type": "text", "text": "G.5 Mirror descent on the simplex ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "We can also leverage the mirror map to perform sampling in constrained spaces. This has received a lot of attention recently either through mirror Langevin methods [3, 32, 116], diffusion methods [51, 82], mirror SVGD [113, 114] or other MCMC algorithms [52, 94]. ", "page_idx": 43}, {"type": "text", "text": "The goal here is to sample from a Dirichlet distribution, i.e. from a distribution $\\nu\\propto e^{-V}$ where $\\begin{array}{r}{V(x)\\,=\\,-\\sum_{i=1}^{d}a_{i}\\log(x_{i})\\,-a_{d+1}\\log\\left(1-\\sum_{i=1}^{d}x_{i}\\right)}\\end{array}$ . To sample from such a distribution, we minimize the Kullback-Leibler divergence, i.e. $\\begin{array}{r}{\\mathcal{F}(\\mu)\\stackrel{}{=}\\mathrm{KL}(\\mu||\\nu)=\\int V\\,\\mathrm{d}\\mu+\\mathcal{H}(\\mu)}\\end{array}$ . To stay on the (open) simplex $\\Delta_{d}=\\left\\{x\\in\\mathbb{R}^{d+1},x_{i}>0,\\sum_{i=1}^{d+1}x_{i}<1\\right\\}$ , we use the mirror map $\\textstyle\\phi(\\mu)=\\int\\psi\\mathrm{d}\\mu$ with $\\begin{array}{r}{\\psi(x)=\\sum_{i=1}^{d}x_{i}\\log(x_{i})+(1-\\sum_{i}x_{i})\\log(1-\\sum_{i}x_{i})}\\end{array}$ for which ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\nabla\\psi(x)=\\left(\\log x_{i}-\\log\\left(1-\\sum_{j}x_{j}\\right)\\right)_{i},\\quad\\nabla\\psi^{*}(y)=\\left(\\frac{e^{y_{i}}}{1+\\sum_{j}e^{y_{j}}}\\right)_{i}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "The scheme here is given by $\\mathrm{T}_{k+1}=\\nabla\\psi^{*}\\circ\\left(\\nabla\\psi-\\gamma\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k})\\right)$ , where $\\nabla_{\\mathrm{W_{2}}}\\mathcal{F}(\\mu_{k})=\\nabla V+$ $\\nabla\\log\\mu_{k}$ , with the density of $\\mu_{k}$ estimated through a Kernel Density Estimator (KDE). We plot on Figure 8a the results obtained for $d=2$ , $a_{1}=a_{2}=a_{3}=6$ and 100 samples. We also report the results for the Mirror Langevin Dynamic (MLD) algorithm, which provide iid samples, which are thus less ordered. We plot the evolution of the KL over iterations on Figure 8b (where the entropy is estimated using the Kozachenko-Leonenko estimator [42]). ", "page_idx": 43}, {"type": "text", "text": "The KDE used here will not scale well with the dimension, however, different methods have been recently propose to overcome this issue, such as using projection on lower dimensional subspaces [127], or using neural networks to learn ratio density estimators [6, 49, 128]. ", "page_idx": 43}, {"type": "text", "text": "H Proofs ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "H.1 Proof of Proposition 1 ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Let $\\mu\\,\\in\\,\\mathcal{P}_{2}(\\mathbb{R}^{d}),\\,\\mathrm{S},\\mathrm{T}\\,\\in\\,D(\\tilde{\\mathcal{F}}_{\\mu}),\\,\\epsilon\\,>\\,0.$ Since $\\mathcal{F}$ is Wasserstein differentiable at $\\mathrm{S}_{\\#}\\mu$ , applying Proposition 9 at $\\mathrm{S}_{\\#}\\mu$ with $\\nu=\\left(\\mathrm{S}+\\epsilon(\\mathrm{T}-\\mathrm{S})\\right)_{\\#}\\mu$ and $\\gamma=\\bigl(\\mathrm{S},\\mathrm{S}+\\epsilon(\\mathrm{T}-\\mathrm{S})\\bigr)_{\\#}\\mu\\in\\Pi\\bigl(\\mathrm{S}_{\\#}\\mu,\\nu\\bigr)$ , we ", "page_idx": 43}, {"type": "text", "text": "obtain, ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\mathcal{F}}_{\\mu}\\big(\\mathrm{S}+\\epsilon(\\mathrm{T}-\\mathrm{S})\\big)=\\mathcal{F}\\big((\\mathrm{S}+\\epsilon(\\mathrm{T}-\\mathrm{S}))_{\\#}\\mu\\big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\mathcal{F}(\\mathrm{S}_{\\#}\\mu)+\\int\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mathrm{S}_{\\#}\\mu)(x),y-x\\rangle\\,\\mathrm{d}\\gamma(x,y)}\\\\ &{\\qquad\\qquad\\quad+\\ o\\left(\\sqrt{\\int\\|x-y\\|_{2}^{2}}\\,\\mathrm{d}\\gamma(x,y)\\right)}\\\\ &{\\qquad\\qquad\\qquad=\\tilde{\\mathcal{F}}_{\\mu}(\\mathrm{S})+\\epsilon\\int\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mathrm{S}_{\\#}\\mu)\\big(\\mathrm{S}(x)\\big),\\mathrm{T}(x)-\\mathrm{S}(x)\\rangle\\,\\mathrm{d}\\mu(x)}\\\\ &{\\qquad\\qquad\\qquad\\qquad+o\\left(\\epsilon\\sqrt{\\int\\|\\mathrm{T}(x)-\\mathrm{S}(x)\\|_{2}^{2}}\\,\\mathrm{d}\\mu(x)\\right)}\\\\ &{\\qquad\\qquad\\qquad=\\tilde{\\mathcal{F}}_{\\mu}(\\mathrm{S})+\\epsilon\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mathrm{S}_{\\#}\\mu)\\,\\mathrm{o},\\mathrm{T}-\\mathrm{S}\\rangle_{L^{2}(\\mu)}+\\epsilon o(\\|\\mathrm{T}-\\mathrm{S}\\|_{L^{2}(\\mu)}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Thus, $\\delta\\tilde{\\mathcal{F}}_{\\mu}(\\mathrm{S},\\mathrm{T}-\\mathrm{S})=\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mathrm{S}_{\\#}\\mu)\\circ\\mathrm{S},\\mathrm{T}-\\mathrm{S}\\rangle_{L^{2}(\\mu)}.$ . Note that in the third equality we used that $\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu)\\in L^{2}(\\mu)$ . \u53e3 ", "page_idx": 44}, {"type": "text", "text": "H.2 Proof of Proposition 2 ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Let $\\mu,\\rho\\;\\in\\;\\mathcal{P}_{2,\\mathrm{{ac}}}(\\mathbb{R}^{d})$ and $\\nu\\;\\in\\;{\\mathcal{P}}_{2}(\\mathbb{R}^{d})$ . Define $\\mathrm{T}_{\\phi_{\\mu}}^{\\mu,\\nu}\\ =\\ \\mathrm{argmin}_{\\mathrm{T}_{\\#}\\mu=\\nu}\\ \\mathrm{d}_{\\phi_{\\mu}}(\\mathrm{T},\\mathrm{Id}),\\ \\mathrm{U}_{\\phi_{\\rho}}^{\\rho,\\nu}\\ =$ $\\mathrm{argmin}_{\\mathrm{U}_{\\#}\\rho=\\nu}\\ \\mathrm{d}_{\\phi_{\\rho}}(\\mathrm{U},\\mathrm{Id})$ and let $\\mathrm{~S~}\\in\\mathrm{~}L^{2}(\\mu)$ such that $\\mathrm{S}_{\\#}\\mu\\;=\\;\\rho$ . Then, noticing that $\\gamma=$ $(\\mathrm{T}_{\\phi_{\\mu}}^{\\mu,\\nu},\\mathrm{S})_{\\#}\\mu\\in\\Pi(\\nu,\\rho)$ , we have ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{d}_{\\phi_{\\mu}}(\\mathrm{T}_{\\phi_{\\mu}}^{\\mu,\\nu},\\mathrm{S})=\\phi\\big((\\mathrm{T}_{\\phi_{\\mu}}^{\\mu,\\nu})_{\\#}\\mu\\big)-\\phi(\\mathrm{S}_{\\#}\\nu)-\\displaystyle\\int\\langle\\nabla_{\\mathrm{W}_{2}}\\phi(\\mathrm{S}_{\\#}\\mu)(y),x-y\\rangle\\;\\mathrm{d}(\\mathrm{T}_{\\phi_{\\mu}}^{\\mu,\\nu},\\mathrm{S})_{\\#}\\mu(x,y)}\\\\ &{\\qquad\\qquad\\qquad=\\phi(\\nu)-\\phi(\\rho)-\\displaystyle\\int\\langle\\nabla_{\\mathrm{W}_{2}}\\phi(\\rho)(y),x-y\\rangle\\;\\mathrm{d}\\gamma(x,y)}\\\\ &{\\qquad\\qquad\\qquad\\geq\\mathrm{W}_{\\phi}(\\nu,\\rho)=\\mathrm{d}_{\\phi_{\\rho}}(\\mathrm{U}_{\\phi_{\\rho}}^{\\rho,\\nu},\\mathrm{Id}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "In the last line, we used Proposition 15, i.e. that the optimal coupling is of the form $(\\mathrm{U}_{\\phi_{\\rho}}^{\\rho,\\nu},\\mathrm{Id})_{\\#}\\rho$ . ", "page_idx": 44}, {"type": "text", "text": "H.3 Proof of Proposition 3 ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Let Tk+1 = argminT\u2208L2(\u00b5k) $\\tau\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k}),\\mathrm{T}-\\mathrm{Id}\\rangle_{L^{2}(\\mu_{k})}\\!+\\!\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{T},\\mathrm{Id})$ . Applying the three-point inequality (Lemma 29) with $\\psi(\\mathrm{T})=\\tau\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k}),\\mathrm{T}-\\mathrm{Id}\\rangle_{L^{2}(\\dot{\\mu}_{k})}$ which is convex, $\\mathrm{T_{0}}=\\mathrm{Id}$ and $\\mathrm{T}^{*}=\\mathrm{T}_{k+1}$ , we get for all $\\mathrm{T}\\in L^{2}(\\mu_{k})$ , ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tau\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k}),\\mathrm{T}-\\mathrm{Id}\\rangle_{L^{2}(\\mu_{k})}+\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{T},\\mathrm{Id})}\\\\ &{\\geq\\tau\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k}),\\mathrm{T}_{k+1}-\\mathrm{Id}\\rangle_{L^{2}(\\mu_{k})}+\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{T}_{k+1},\\mathrm{Id})+\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{T},\\mathrm{T}_{k+1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "which is equivalent to ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k}),\\mathrm{T}_{k+1}-\\mathrm{Id}\\rangle_{L^{2}(\\mu_{k})}+\\displaystyle\\frac{1}{\\tau}\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{T}_{k+1},\\mathrm{Id})}\\\\ &{\\leq\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k}),\\mathrm{T}-\\mathrm{Id}\\rangle_{L^{2}(\\mu_{k})}+\\displaystyle\\frac{1}{\\tau}\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{T},\\mathrm{Id})-\\frac{1}{\\tau}\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{T},\\mathrm{T}_{k+1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "By the $\\beta$ -smoothness of $\\tilde{\\mathcal{F}}_{\\mu_{k}}$ relative to $\\phi_{\\mu_{k}}$ , we also have ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{1}_{\\tilde{\\mathcal{F}}_{\\mu_{k}}}(\\mathrm{T}_{k+1},\\mathrm{Id})=\\tilde{\\mathcal{F}}_{\\mu_{k}}(\\mathrm{T}_{k+1})-\\tilde{\\mathcal{F}}_{\\mu_{k}}(\\mathrm{Id})-\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k}),\\mathrm{T}_{k+1}-\\mathrm{Id}\\rangle_{L^{2}(\\mu_{k})}\\leq\\beta\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{T}_{k+1},\\mathrm{Id})}\\\\ &{\\iff\\tilde{\\mathcal{F}}_{\\mu_{k}}(\\mathrm{T}_{k+1})\\leq\\tilde{\\mathcal{F}}_{\\mu_{k}}(\\mathrm{Id})+\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k}),\\mathrm{T}_{k+1}-\\mathrm{Id}\\rangle_{L^{2}(\\mu_{k})}+\\beta\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{T}_{k+1},\\mathrm{Id}).\\qquad\\mathrm{(138)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Moreover, since $\\beta\\leq\\frac{1}{\\tau}$ , this inequality implies (by non-negativity of $\\mathrm{d}_{\\phi_{\\mu_{k}}}$ ), ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{F}}_{\\mu_{k}}(\\mathrm{T}_{k+1})\\leq\\tilde{\\mathcal{F}}_{\\mu_{k}}(\\mathrm{Id})+\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k}),\\mathrm{T}_{k+1}-\\mathrm{Id}\\rangle_{L^{2}(\\mu_{k})}+\\frac{1}{\\tau}\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{T}_{k+1},\\mathrm{Id}).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Then, using the inequality (137), we obtain for all $\\mathrm{T}\\in L^{2}(\\mu_{k})$ , ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{F}}_{\\mu_{k}}(\\mathrm{T}_{k+1})\\leq\\tilde{\\mathcal{F}}_{\\mu_{k}}(\\mathrm{Id})+\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k}),\\mathrm{T}-\\mathrm{Id}\\rangle_{L^{2}(\\mu_{k})}+\\frac{1}{\\tau}\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{T},\\mathrm{Id})-\\frac{1}{\\tau}\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{T},\\mathrm{T}_{k+1}).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Observing that $\\tilde{\\mathcal{F}}_{\\mu_{k}}(\\mathrm{T}_{k+1})=\\mathcal{F}(\\mu_{k+1})$ and $\\tilde{\\mathcal{F}}_{\\mu_{k}}(\\mathrm{Id})=\\mathcal{F}(\\mu_{k})$ , we get ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\mathcal{F}(\\mu_{k+1})\\leq\\mathcal{F}(\\mu_{k})+\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k}),\\mathrm{T}-\\mathrm{Id}\\rangle_{L^{2}(\\mu_{k})}+\\frac{1}{\\tau}\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{T},\\mathrm{Id})-\\frac{1}{\\tau}\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{T},\\mathrm{T}_{k+1}).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Finally, setting $\\mathrm{T}=\\mathrm{Id}$ , we obtain the result: ", "page_idx": 45}, {"type": "equation", "text": "$$\n{\\mathcal{F}}(\\mu_{k+1})\\leq{\\mathcal{F}}(\\mu_{k})-{\\frac{1}{\\tau}}\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{Id},\\mathrm{T}_{k+1}).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "H.4 Proof of Proposition 4 ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Let $\\nu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ , and $\\mathrm{T}=\\mathrm{argmin}_{\\mathrm{T},\\mathrm{T}_{\\#}\\mu_{k}=\\nu}\\ \\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{T},\\mathrm{Id})$ . From the relative convexity hypothesis, we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu_{k}}}(\\mathrm{T},\\mathrm{Id})\\geq\\alpha\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{T},\\mathrm{Id})}\\\\ &{\\qquad\\qquad\\Longleftrightarrow\\tilde{\\mathcal{F}}_{\\mu_{k}}(\\mathrm{T})-\\tilde{\\mathcal{F}}_{\\mu_{k}}(\\mathrm{Id})-\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k}),\\mathrm{T}-\\mathrm{Id}\\rangle_{L^{2}(\\mu_{k})}\\geq\\alpha\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{T},\\mathrm{Id})}\\\\ &{\\qquad\\quad\\Longleftrightarrow\\tilde{\\mathcal{F}}_{\\mu_{k}}(\\mathrm{T})-\\alpha\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{T},\\mathrm{Id})\\geq\\tilde{\\mathcal{F}}_{\\mu_{k}}(\\mathrm{Id})+\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k}),\\mathrm{T}-\\mathrm{Id}\\rangle_{L^{2}(\\mu_{k})}}\\\\ &{\\qquad\\quad\\Longleftrightarrow\\mathcal{F}(\\nu)-\\alpha\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{T},\\mathrm{Id})\\geq\\mathcal{F}(\\mu_{k})+\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k}),\\mathrm{T}-\\mathrm{Id}\\rangle_{L^{2}(\\mu_{k})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Plugging this into (140), we get ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\mathcal{F}(\\mu_{k+1})\\leq\\mathcal{F}(\\nu)+\\frac{1}{\\tau}\\big(\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{T},\\mathrm{Id})-\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{T},\\mathrm{T}_{k+1})\\big)-\\alpha\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{T},\\mathrm{Id}).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Then, by definition of $\\mathrm{T}$ , note that $\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{T},\\mathrm{Id})\\,=\\,\\mathrm{W}_{\\phi}(\\nu,\\mu_{k})$ , and by Assumption 1, we have $\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{T},\\mathrm{T}_{k+1})\\geq\\mathrm{W}_{\\phi}(\\nu,\\mu_{k+1})$ , since $\\mathrm{T}_{\\#}\\mu_{k}=\\nu$ and $(\\mathrm{T}_{k+1})_{\\#}\\mu_{k}=\\mu_{k+1}$ . Thus, ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\mathcal{F}(\\mu_{k+1})-\\mathcal{F}(\\nu)\\leq\\left(\\frac{1}{\\tau}-\\alpha\\right)\\mathrm{W}_{\\phi}(\\nu,\\mu_{k})-\\frac{1}{\\tau}\\mathrm{W}_{\\phi}(\\nu,\\mu_{k+1}).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Observing that ${\\mathcal{F}}(\\mu_{k})\\,\\leq\\,{\\mathcal{F}}(\\mu_{\\ell})$ for all $\\ell\\,\\leq\\,k$ (by Proposition 3 and non-negativity of $\\mathrm{d}_{\\phi}$ for $\\phi$ convex) and that $\\mathrm{W}_{\\phi}(\\nu,\\mu)\\geq0$ , we can apply Lemma 30 with $f={\\mathcal{F}}$ , $c={\\mathcal{F}}(\\nu)$ and $g=\\mathrm{W}_{\\phi}(\\nu,\\cdot)$ , and we obtain ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\forall k\\geq1,\\,\\mathcal{F}(\\mu_{k})-\\mathcal{F}(\\nu)\\leq\\frac{\\alpha}{\\left(\\frac{\\frac{1}{\\tau}}{\\frac{1}{\\tau}-\\alpha}\\right)^{k}-1}\\mathrm{W}_{\\phi}(\\nu,\\mu_{0})\\leq\\frac{\\frac{1}{\\tau}-\\alpha}{k}\\mathrm{W}_{\\phi}(\\nu,\\mu_{0}).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "For the second result, from (145), we get for $\\nu=\\mu^{*}$ the minimizer of $\\mathcal{F}$ , since $\\mathcal{F}(\\mu_{k+1})\\!-\\!\\mathcal{F}(\\mu^{*})\\geq0$ , ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\mathrm{W}_{\\phi}(\\mu^{*},\\mu_{k+1})\\leq(1-\\alpha\\tau)\\,\\mathrm{W}_{\\phi}(\\mu^{*},\\mu_{k})\\leq(1-\\alpha\\tau)^{k+1}\\,\\mathrm{W}_{\\phi}(\\mu^{*},\\mu_{0}).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "H.5 Proof of Proposition 5 ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Let $k\\geq0$ , by the definition of $\\mathrm{d}_{\\phi_{\\mu_{k}}^{h^{\\ast}}}$ and the hypothesis $\\mathrm{d}_{\\phi_{\\mu_{k}}^{h^{\\ast}}}\\bigl(\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k+1})\\circ\\mathrm{T}_{k+1},\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k})\\bigr)\\leq$ $\\beta\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu_{k}}}(\\mathrm{Id},\\mathrm{T}_{k+1})$ , we have ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\phi_{\\mu_{k+1}}^{h^{\\ast}}\\big(\\nabla_{\\mathbf{W}_{2}}\\mathcal{F}(\\mu_{k+1})\\big)=\\phi_{\\mu_{k}}^{h^{\\ast}}\\big(\\nabla_{\\mathbf{W}_{2}}\\mathcal{F}(\\mu_{k})\\big)}&{}\\\\ {\\quad}&{\\quad+\\left\\langle\\nabla{h^{\\ast}}\\circ\\nabla_{\\mathbf{W}_{2}}\\mathcal{F}(\\mu_{k}),\\nabla_{\\mathbf{W}_{2}}\\mathcal{F}(\\mu_{k+1})\\circ\\nabla_{k+1}-\\nabla_{\\mathbf{W}_{2}}\\mathcal{F}(\\mu_{k})\\right\\rangle_{L^{2}(\\mu_{k})}}\\\\ {\\quad}&{\\quad+\\underbrace{d_{\\phi_{k}^{h^{\\ast}}}\\big(\\nabla_{\\mathbf{W}_{2}}\\mathcal{F}((\\Gamma_{k+1})_{\\#\\mu^{k}})\\circ\\nabla_{k+1},\\nabla_{\\mathbf{W}_{2}}\\mathcal{F}(\\mu_{k})\\big)}_{\\leq\\phi_{\\mu_{k}^{h^{\\ast}}}\\big(\\nabla_{\\mathbf{W}_{2}}\\mathcal{F}(\\mu_{k})\\big)}}\\\\ {\\quad}&{\\quad\\leq\\phi_{\\mu_{k}}^{h^{\\ast}}\\big(\\nabla_{\\mathbf{W}_{2}}\\mathcal{F}(\\mu_{k})\\big)}\\\\ {\\quad}&{\\quad+\\left\\langle\\nabla{h^{\\ast}}\\circ\\nabla_{\\mathbf{W}_{2}}\\mathcal{F}(\\mu_{k}),\\nabla_{\\mathbf{W}_{2}}\\mathcal{F}(\\mu_{k+1})\\circ\\nabla_{k+1}-\\nabla_{\\mathbf{W}_{2}}\\mathcal{F}(\\mu_{k})\\right\\rangle_{L^{2}(\\mu_{k})}}\\\\ {\\quad}&{\\quad+\\beta\\mathrm{d}_{\\hat{\\mathcal{F}}_{\\mu_{k}}}\\big(\\mathrm{H},\\mathrm{T}_{k+1}\\big)}\\\\ {\\quad}&{\\quad\\leq\\phi_{\\mu_{k}^{h^{\\ast}}}\\big(\\nabla_{\\mathbf{W}_{2}}\\mathcal{F}(\\mu_{k})\\big)}\\\\ {\\quad}&{\\quad+\\left\\langle\\nabla{h^{\\ast}}\\circ\\nabla_{\\mathbf{W}_{2}}\\mathcal{F}(\\mu_{k}),\\nabla_{\\mathbf{W}_{2}}\\mathcal{F}(\\mu_{k+1})\\circ\\nabla_{k+1}-\\nabla_{\\mathbf{W}_{2}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where we used in the last line that $\\begin{array}{r}{\\tau\\le\\frac{1}{\\beta}}\\end{array}$ and the non-negativity of the Bregman divergence since $\\mathcal{F}$ is convex along $t\\mapsto\\left((1-t)\\mathrm{T}_{k+1}+t\\mathrm{Id}\\right)_{\\#}\\mu_{k}$ and thus by Proposition 13, $\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu_{k}}}(\\mathrm{Id},\\mathrm{T}_{k+1})\\geq0$ . ", "page_idx": 46}, {"type": "text", "text": "Let $\\mathrm{~T~}\\in\\mathrm{~\\cal~L}^{2}(\\mu_{k})$ . Then, using the three-point identity (Lemma 28) (with $\\mathrm{~S~}=\\mathrm{~Id},\\mathrm{~U~}=\\mathrm{~T~}$ and $\\mathrm{T}=\\mathrm{T}_{k+1}$ ), and remembering that $\\mathrm{T}_{k+1}=\\mathrm{Id}-\\tau\\nabla h^{*}\\circ\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k})$ , we get ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu_{k}}}(\\mathrm{H},\\mathrm{T}_{k+1})=\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu_{k}}}(\\mathrm{H},\\mathrm{T})-\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu_{k}}}(\\mathrm{T}_{k+1},\\mathrm{T})}\\\\ &{\\qquad\\qquad\\quad-\\left\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}\\big((\\mathrm{T}_{k+1})_{\\#}\\mu_{k}\\big)\\circ\\mathrm{T}_{k+1},\\mathrm{Id}-\\mathrm{T}_{k+1}\\right\\rangle_{L^{2}(\\mu_{k})}}\\\\ &{\\qquad\\qquad\\quad+\\left\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mathrm{T}_{\\#}\\mu_{k})\\circ\\mathrm{T},\\mathrm{H}-\\mathrm{T}_{k+1}\\right\\rangle_{L^{2}(\\mu_{k})}}\\\\ &{\\quad=\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu_{k}}}(\\mathrm{H},\\mathrm{T})-\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu_{k}}}(\\mathrm{T}_{k+1},\\mathrm{T})}\\\\ &{\\qquad\\quad+\\left\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mathrm{T}_{\\#}\\mu_{k})\\circ\\mathrm{T}-\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k+1})\\circ\\mathrm{T}_{k+1},\\mathrm{Id}-\\mathrm{T}_{k+1}\\right\\rangle_{L^{2}(\\mu_{k})}}\\\\ &{\\quad=\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu_{k}}}(\\mathrm{H},\\mathrm{T})-\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu_{k}}}(\\mathrm{T}_{k+1},\\mathrm{T})}\\\\ &{\\qquad\\quad+\\tau\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mathrm{T}_{\\#}\\mu_{k})\\circ\\mathrm{T}-\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k+1})\\circ\\mathrm{T}_{k+1},\\nabla h^{*}\\circ\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k})\\rangle_{L^{2}(\\mu_{k})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "This is equivalent to ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\nabla h^{*}\\circ\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k}),\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k+1})\\circ\\mathrm{T}_{k+1}-\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k})\\rangle_{L^{2}(\\mu_{k})}+\\frac{1}{\\tau}\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu_{k}}}(\\mathrm{Id},\\mathrm{T}_{k+1})}\\\\ &{\\quad\\quad\\quad=\\frac{1}{\\tau}\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu_{k}}}(\\mathrm{Id},\\mathrm{T})-\\frac{1}{\\tau}\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu_{k}}}(\\mathrm{T}_{k+1},\\mathrm{T})}\\\\ &{\\quad\\quad\\quad\\quad+\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mathrm{T}_{\\#}\\mu_{k})\\circ\\mathrm{T}-\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k}),\\nabla h^{*}\\circ\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k})\\rangle_{L^{2}(\\mu_{k})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Then, using the definition of $\\mathrm{d}_{\\phi_{\\mu_{k}}^{h^{\\ast}}}\\big(\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mathrm{T}_{\\#}\\mu_{k})\\circ\\mathrm{T},\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k})\\big)$ , we obtain ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\nabla h^{*}\\circ\\nabla_{\\mathbf{W}_{2}}\\mathcal{F}(\\mu_{k}),\\nabla_{\\mathbf{W}_{2}}\\mathcal{F}(\\mu_{k+1})\\circ\\mathrm{T}_{k+1}-\\nabla_{\\mathbf{W}_{2}}\\mathcal{F}(\\mu_{k})\\rangle_{L^{2}(\\mu_{k})}+\\frac{1}{7}\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu_{k}}}(\\mathrm{Id},\\mathrm{T}_{k+1})}\\\\ &{=\\frac{1}{\\tau}\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu_{k}}}(\\mathrm{Id},\\mathrm{T})-\\frac{1}{\\tau}\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu_{k}}}(\\mathrm{T}_{k+1},\\mathrm{T})}\\\\ &{\\phantom{\\hat{\\mathcal{F}}_{\\mu_{k}}}-\\mathrm{d}_{\\phi_{\\mu_{k}}^{h*}}\\bigl(\\nabla_{\\mathbf{W}_{2}}\\mathcal{F}(\\mathrm{T}_{\\#\\mu_{k}})\\circ\\mathrm{T},\\nabla_{\\mathbf{W}_{2}}\\mathcal{F}(\\mu_{k})\\bigr)+\\phi_{\\mu_{k}}^{h^{*}}\\bigl(\\nabla_{\\mathbf{W}_{2}}\\mathcal{F}(\\mathrm{T}_{\\#\\mu_{k}})\\circ\\mathrm{T}\\bigr)-\\phi_{\\mu_{k}}^{h^{*}}\\bigl(\\nabla_{\\mathbf{W}_{2}}\\mathcal{F}(\\mu_{k})\\bigr)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Plugging this into (148), we get ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\phi_{\\mu_{k+1}}^{h^{*}}\\!\\left(\\nabla_{\\mathbf{W}_{2}}\\mathcal{F}(\\mu_{k+1})\\right)\\leq\\phi_{\\mu_{k}}^{h^{*}}\\!\\left(\\nabla_{\\mathbf{W}_{2}}\\mathcal{F}(\\mathrm{T}_{\\#}\\mu_{k})\\circ\\mathrm{T}\\right)+\\frac{1}{\\tau}\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu_{k}}}(\\mathrm{Id},\\mathrm{T})-\\frac{1}{\\tau}\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu_{k}}}(\\mathrm{T}_{k+1},\\mathrm{T})}\\\\ {-\\:\\mathrm{d}_{\\phi_{\\mu_{k}}^{h^{*}}}\\!\\left(\\nabla_{\\mathbf{W}_{2}}\\mathcal{F}(\\mathrm{T}_{\\#}\\mu_{k})\\circ\\mathrm{T},\\nabla_{\\mathbf{W}_{2}}\\mathcal{F}(\\mu_{k})\\right)\\!.}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "For $\\mathrm{T}=\\mathrm{Id}$ , we get ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\phi_{\\mu_{k+1}}^{h^{\\ast}}\\mathopen{}\\mathclose\\bgroup\\left(\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k+1})\\aftergroup\\egroup\\right)\\leq\\phi_{\\mu_{k}}^{h^{\\ast}}\\mathopen{}\\mathclose\\bgroup\\left(\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k})\\aftergroup\\egroup\\right)-\\frac1{\\tau}\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu_{k}}}\\mathopen{}\\mathclose\\bgroup\\left(\\mathrm{T}_{k+1},\\mathrm{Id}\\aftergroup\\egroup\\right).\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "H.6 Proof of Proposition 6 ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Let $\\mu^{*}\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ be the minimizer of $\\mathcal{F}$ , $k\\geq0$ and $\\mathrm{T}=\\mathrm{argmin}_{\\mathrm{T}\\in L^{2}(\\mu_{k}),\\mathrm{T}_{\\#}\\mu_{k}=\\mu^{*}}\\ \\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu_{k}}}(\\mathrm{Id},\\mathrm{T})$ First, observe that since $\\mu^{*}$ is the minimizer of $\\mathcal{F}$ , then $\\nabla_{\\mathrm{W_{2}}}\\mathcal{F}(\\mu^{*})=0$ (see e.g. [74, Theorem 3.1]), and thus $\\phi_{\\mu_{k}}^{h^{\\ast}}(0)\\,=\\,h^{\\ast}(0)$ . Moreover, it induces that $\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu_{k}}}(\\mathrm{Id},\\mathrm{T})\\,=\\,\\mathcal{F}(\\mu_{k})\\,-\\,\\mathcal{F}(\\mu^{*})$ and $\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu_{k}}}(\\mathrm{T}_{k+1},\\mathrm{T})=\\mathcal{F}(\\mu_{k+1})-\\mathcal{F}(\\mu^{*})$ . ", "page_idx": 47}, {"type": "text", "text": "Therefore, using (152) and the hypothesis $\\alpha\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu_{k}}}(\\mathrm{Id},\\mathrm{T})\\leq\\mathrm{d}_{\\phi_{\\mu_{k}}^{h^{\\ast}}}\\big(0,\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k})\\big)$ , we get ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phi_{\\mu_{k+1}}^{h^{\\ast}}\\!\\left(\\nabla_{\\mathbb{W}_{2}}\\mathcal{F}(\\mu_{k+1})\\right)-h^{\\ast}(0)\\leq\\displaystyle\\frac{1}{\\tau}\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu_{k}}}(\\mathrm{Id},\\mathrm{T})-\\frac{1}{\\tau}\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu_{k}}}(\\mathrm{T}_{k+1},\\mathrm{T})-\\mathrm{d}_{\\phi_{\\mu_{k}}^{h^{\\ast}}}\\!\\left(0,\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k})\\right)}\\\\ &{\\phantom{\\phi_{\\mu_{k+1}}^{h^{\\ast}}\\!\\left(\\nabla_{\\mathbb{W}_{2}}\\mathcal{F}(\\mu_{k+1})\\right)-}\\leq\\displaystyle\\frac{1}{\\tau}\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu_{k}}}(\\mathrm{Id},\\mathrm{T})-\\frac{1}{\\tau}\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu_{k}}}(\\mathrm{T}_{k+1},\\mathrm{T})-\\alpha\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu_{k}}}(\\mathrm{Id},\\mathrm{T})}\\\\ &{\\phantom{\\phi_{\\mu_{k+1}}^{h^{\\ast}}\\!\\left(\\nabla_{\\mathbb{W}_{2}}\\mathcal{F}(\\mu_{k})\\right)-}=\\left(\\frac{1}{\\tau}-\\alpha\\right)\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu_{k}}}(\\mathrm{Id},\\mathrm{T})-\\displaystyle\\frac{1}{\\tau}\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu_{k}}}(\\mathrm{T}_{k+1},\\mathrm{T})}\\\\ &{\\phantom{\\phi_{\\mu_{k+1}}^{h^{\\ast}}\\!\\left(0,\\nabla_{\\mathbb{W}_{2}}\\mathcal{F}(\\mu_{k})\\right)-}=\\left(\\frac{1}{\\tau}-\\alpha\\right)\\left(\\mathcal{F}(\\mu_{k})-\\mathcal{F}(\\mu^{\\ast})\\right)-\\displaystyle\\frac{1}{\\tau}\\big(\\mathcal{F}(\\mu_{k+1})-\\mathcal{F}(\\mu^{\\ast})\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Then, applying Lemma 30 with $f\\ =\\ \\phi_{.}^{h^{\\ast}}\\circ\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}$ (which satisfies $\\phi_{\\mu_{k+1}}^{h^{\\ast}}\\left(\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k+1})\\right)\\ \\leq$ $\\phi_{\\mu_{k}}^{h^{\\ast}}\\left(\\nabla_{\\mathrm{W_{2}}}\\mathcal{F}(\\mu_{k})\\right)$ by Proposition 5), $c=h^{*}(0)$ and $g=\\mathcal{F}(\\cdot)-\\mathcal{F}(\\mu^{*})\\geq0$ , we get ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\phi_{\\mu_{k}}^{h^{\\ast}}\\bigl(\\nabla_{\\mathsf{W}_{2}}\\mathcal{F}(\\mu_{k})\\bigr)-h^{\\ast}(0)\\leq\\frac{\\alpha}{\\Bigl(\\frac{\\frac{1}{\\tau}}{\\frac{1}{\\tau}-\\alpha}\\Bigr)^{k}-1}\\bigl(\\mathcal{F}(\\mu_{0})-\\mathcal{F}(\\mu^{\\ast})\\bigr)\\leq\\frac{\\frac{1}{\\tau}-\\alpha}{k}\\bigl(\\mathcal{F}(\\mu_{0})-\\mathcal{F}(\\mu^{\\ast})\\bigr).\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Concerning the convergence of $\\mathcal{F}(\\mu_{k})$ , if $\\alpha>0$ and $h^{*}$ attains its minimum in 0, then necessarily $\\phi_{\\mu}^{h^{\\ast}}(\\mathrm{T})\\geq h^{\\ast}(0)$ for all $\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ and $\\mathrm{T}\\in L^{2}(\\mu)$ . Thus, using (152), we get ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle)\\le\\phi_{\\mu_{k+1}}^{h^{*}}\\left(\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k+1})\\right)-h^{*}(0)\\le\\frac{1}{\\tau}\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu_{k}}}(\\mathrm{Id},\\mathrm{T})-\\frac{1}{\\tau}\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu_{k}}}(\\mathrm{T}_{k+1},\\mathrm{T})-\\mathrm{d}_{\\phi_{\\mu_{k}}^{h*}}\\bigl(0,\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k})\\bigr)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\frac{1}{\\tau}\\bigl(\\mathcal{F}(\\mu_{k})-\\mathcal{F}(\\mu^{*})\\bigr)-\\frac{1}{\\tau}\\bigl(\\mathcal{F}(\\mu_{k+1})-\\mathcal{F}(\\mu^{*})\\bigr)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad-\\alpha\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu_{k}}}(\\mathrm{Id},\\mathrm{T})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\left(\\frac{1}{\\tau}-\\alpha\\right)\\bigl(\\mathcal{F}(\\mu_{k})-\\mathcal{F}(\\mu^{*})\\bigr)-\\frac{1}{\\tau}\\bigl(\\mathcal{F}(\\mu_{k+1})-\\mathcal{F}(\\mu^{*})\\bigr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Thus, for all $k\\geq0$ , ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathcal{F}(\\mu_{k+1})-\\mathcal{F}(\\mu^{*})=\\left(1-\\tau\\alpha\\right)\\left(\\mathcal{F}(\\mu_{k})-\\mathcal{F}(\\mu^{*})\\right)}&{}\\\\ {\\leq\\left(1-\\tau\\alpha\\right)^{k+1}\\left(\\mathcal{F}(\\mu_{0})-\\mathcal{F}(\\mu^{*})\\right)\\!.}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "H.7 Proof of Proposition 7 ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Let $\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ . Since $\\tilde{\\mathcal{F}}_{\\mu}^{*}$ is G\u00e2teaux differentiable, we can define its Bregman divergence. ", "page_idx": 47}, {"type": "text", "text": "For the first point, $\\phi^{h^{*}}$ is $\\beta$ -smooth relative to ${\\mathcal{F}}_{\\mu}^{*}$ along $t\\mapsto\\left((1-t)\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu)+t\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mathrm{T}_{\\#}\\mu)\\circ\\right.$ $\\operatorname{T})_{\\#}\\mu$ . Thus, by applying Definition 3 for $s=1$ and $t=0$ , we have ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\mathrm{d}_{\\phi_{\\mu}^{h^{\\ast}}}\\bigl(\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mathrm{T}_{\\#}\\mu)\\circ\\mathrm{T},\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu)\\bigr)\\leq\\beta\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu}^{\\ast}}\\bigl(\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mathrm{T}_{\\#}\\mu)\\circ\\mathrm{T},\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu)\\bigr).\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Using Lemma 19, we finally obtain ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{d}_{\\phi_{\\mu}^{h^{\\ast}}}\\left(\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mathrm{T}_{\\#}\\mu)\\circ\\mathrm{T},\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{k})\\right)\\leq\\beta\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu}}\\big(\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mathrm{T}_{\\#}\\mu)\\circ\\mathrm{T},\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu)\\big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\beta\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu}}\\big(\\mathrm{Id},\\mathrm{T}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "which is the desired inequality. ", "page_idx": 48}, {"type": "text", "text": "The second point follows similarly. ", "page_idx": 48}, {"type": "text", "text": "H.8 Proof of Lemma 11 ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Let us define $\\tilde{\\mathcal{G}}_{\\mu}:L^{2}(\\mu)\\times\\mathbb{R}^{d}\\to\\mathbb{R}^{d}$ as for all $\\mathrm{T}\\in L^{2}(\\mu),x\\in\\mathbb{R}^{d}$ , ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{G}}_{\\mu}(\\mathrm{T},x)=\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mathrm{T}_{\\#}\\mu)(x)=\\left(\\begin{array}{c}{\\frac{\\partial}{\\partial x_{1}}\\frac{\\delta\\mathcal{F}}{\\delta\\mu}(\\mathrm{T}_{\\#}\\mu)(x)}\\\\ {\\vdots}\\\\ {\\frac{\\partial}{\\partial x_{d}}\\frac{\\delta\\mathcal{F}}{\\delta\\mu}(\\mathrm{T}_{\\#}\\mu)(x)}\\end{array}\\right)=\\left(\\begin{array}{c}{\\tilde{G}_{\\mu}^{1}(\\mathrm{T},x)}\\\\ {\\vdots}\\\\ {\\tilde{G}_{\\mu}^{d}(\\mathrm{T},x)}\\end{array}\\right),\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "with for all $i$ , $\\tilde{G}_{\\mu}^{i}:L^{2}(\\mu)\\times\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ , $\\begin{array}{r}{\\tilde{G}_{\\mu}^{i}(\\mathrm{T},x)=\\frac{\\partial}{\\partial x_{i}}\\frac{\\delta\\mathcal{F}}{\\delta\\mu}(\\mathrm{T}_{\\#}\\mu)(x)}\\end{array}$ . Using the chain rule, for all $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ , ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\tilde{G}_{\\mu}^{i}}{\\mathrm{d}s}\\bigl(\\mathrm{T}_{s},\\mathrm{T}_{s}(x)\\bigr)=\\biggl\\langle\\nabla_{1}\\tilde{G}_{\\mu}^{i}\\bigl(\\mathrm{T}_{s},\\mathrm{T}_{s}(x)\\bigr),\\frac{\\mathrm{d}\\mathrm{T}_{s}}{\\mathrm{d}s}\\biggr\\rangle_{L^{2}(\\mu)}+\\biggl\\langle\\nabla_{2}\\tilde{G}_{\\mu}^{i}\\bigl(\\mathrm{T}_{s},\\mathrm{T}_{s}(x)\\bigr),\\frac{\\mathrm{d}\\mathrm{T}_{s}}{\\mathrm{d}s}(x)\\biggr\\rangle.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "On one hand, we have $\\begin{array}{r}{\\nabla_{2}\\tilde{G}_{\\mu}^{i}\\bigl(\\mathrm{T}_{s},\\mathrm{T}_{s}(x)\\bigr)\\,=\\,\\nabla\\frac{\\partial}{\\partial x_{i}}\\frac{\\delta\\mathcal{F}}{\\delta\\mu}\\bigl((\\mathrm{T}_{s})_{\\#}\\mu\\bigr)\\bigl(\\mathrm{T}_{s}(x)\\bigr)}\\end{array}$ . On the other hand, let us compute $\\nabla_{1}\\tilde{G}_{\\mu}^{i}(\\mathrm{T},x)$ . First, we define the shorthands $\\begin{array}{r}{\\tilde{g}_{\\mu}^{x,i}(\\mathrm{T})=\\tilde{G}_{\\mu}^{i}(\\mathrm{T},x)=\\frac{\\partial}{\\partial x_{i}}\\frac{\\delta\\mathcal{F}}{\\delta\\mu}(\\mathrm{T}_{\\#}\\mu)(x)}\\end{array}$ and $\\begin{array}{r}{g^{x,i}(\\nu)\\,=\\,\\frac{\\partial}{\\partial x_{i}}\\frac{\\delta\\mathcal{F}}{\\delta\\mu}(\\nu)(x)}\\end{array}$ . Since $\\tilde{g}_{\\mu}^{x,i}(\\mathrm{T})\\,=\\,g^{x,i}(\\mathrm{T}_{\\#}\\mu)$ , applying Proposition 1, iwe know that $\\nabla_{1}\\tilde{G}_{\\mu}(\\mathrm{T},x)=\\nabla\\tilde{g}_{\\mu}^{x,i}(\\mathrm{T})=\\nabla_{\\mathrm{W}_{2}}g^{x,i}(\\mathrm{T}_{\\#}\\mu)\\circ\\mathrm{T}.$ ", "page_idx": 48}, {"type": "text", "text": "Now, let us compute $\\begin{array}{r}{\\nabla_{\\mathrm{W}_{2}}g^{x,i}(\\nu)\\,=\\,\\nabla\\frac{\\delta g^{x,i}}{\\delta\\mu}(\\nu)}\\end{array}$ . Let $\\chi$ be such that $\\int\\mathrm{d}\\chi\\,=\\,0$ , then using the hypothesis that $\\begin{array}{r}{\\frac{\\delta}{\\delta\\mu}\\nabla\\frac{\\delta\\mathcal{F}}{\\delta\\mu}=\\nabla\\frac{\\delta^{2}\\mathcal{F}}{\\delta\\mu^{2}}}\\end{array}$ and the definition of $g^{x,i}$ , ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\int\\frac{\\delta g^{x,i}}{\\delta\\mu}(\\nu)\\ \\mathrm{d}\\chi=\\int\\frac{\\partial}{\\partial x_{i}}\\frac{\\delta^{2}\\mathcal F}{\\delta\\mu^{2}}(\\nu)(x,y)\\ \\mathrm{d}\\chi(y).\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "$\\begin{array}{r}{\\nabla_{\\mathrm{W}_{2}}g^{x,i}(\\nu)=\\nabla_{y}\\frac{\\partial}{\\partial x_{i}}\\frac{\\delta^{2}\\mathcal{F}}{\\delta\\mu^{2}}(\\nu)(x,y).}\\end{array}$ ", "page_idx": 48}, {"type": "text", "text": "Putting everything together, we obtain ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\mathrm{d}\\tilde{G}_{\\mu}^{i}}{\\mathrm{d}s}\\big(\\mathrm{T}_{s},\\mathrm{T}_{s}(x)\\big)=\\bigg\\langle\\nabla_{y}\\frac{\\partial}{\\partial x_{i}}\\frac{\\delta^{2}\\mathcal{F}}{\\delta\\mu^{2}}\\big((\\mathrm{T}_{s})_{\\#}\\mu\\big)\\big(\\mathrm{T}_{s}(x),\\mathrm{T}_{s}(\\cdot)\\big),\\frac{\\mathrm{d}\\mathrm{T}_{s}}{\\mathrm{d}s}\\bigg\\rangle_{L^{2}(\\mu)}}\\\\ {\\displaystyle\\quad\\quad\\quad\\quad\\quad+\\left\\langle\\nabla\\frac{\\partial}{\\partial x_{i}}\\frac{\\delta\\mathcal{F}}{\\delta\\mu}\\big((\\mathrm{T}_{s})_{\\#}\\mu\\big)\\big(\\mathrm{T}_{s}(x)\\big),\\frac{\\mathrm{d}\\mathrm{T}_{s}}{\\mathrm{d}s}(x)\\right\\rangle}\\\\ {\\displaystyle\\quad\\quad\\quad\\quad\\quad=\\int\\bigg\\langle\\nabla_{y}\\frac{\\partial}{\\partial x_{i}}\\frac{\\delta^{2}\\mathcal{F}}{\\delta\\mu^{2}}\\big((\\mathrm{T}_{s})_{\\#}\\mu\\big)\\big(\\mathrm{T}_{s}(x),\\mathrm{T}_{s}(y)\\big),\\frac{\\mathrm{d}\\mathrm{T}_{s}}{\\mathrm{d}s}(y)\\bigg\\rangle\\;\\mathrm{d}\\mu(y)}\\\\ {\\displaystyle\\quad\\quad\\quad\\quad\\quad+\\left\\langle\\nabla\\frac{\\partial}{\\partial x_{i}}\\frac{\\delta\\mathcal{F}}{\\delta\\mu}\\big((\\mathrm{T}_{s})_{\\#}\\mu\\big)\\big(\\mathrm{T}_{s}(x)\\big),\\frac{\\mathrm{d}\\mathrm{T}_{s}}{\\mathrm{d}s}(x)\\right\\rangle,}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "and thus ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\mathrm{d}}{\\mathrm{d}s}\\tilde{g}_{\\mu}\\big(\\mathrm{T}_{s},\\mathrm{T}_{s}(x)\\big)=\\int\\nabla_{y}\\nabla_{x}\\frac{\\delta^{2}\\mathcal{F}}{\\delta\\mu^{2}}\\big((\\mathrm{T}_{s})_{\\#}\\mu\\big)\\big(\\mathrm{T}_{s}(x),\\mathrm{T}_{s}(y)\\big)\\frac{\\mathrm{d}\\mathrm{T}_{s}}{\\mathrm{d}s}(y)\\,\\mathrm{d}\\mu(y)\\qquad\\qquad}\\\\ {+\\,\\nabla^{2}\\frac{\\delta\\mathcal{F}}{\\delta\\mu}\\big((\\mathrm{T}_{s})_{\\#}\\mu\\big)\\big(\\mathrm{T}_{s}(x)\\big)\\frac{\\mathrm{d}\\mathrm{T}_{s}}{\\mathrm{d}s}(x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "H.9 Proof of Proposition 12 ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "First, recall that by using the chain rule and Proposition 1, $\\begin{array}{r}{\\frac{\\mathrm{d}}{\\mathrm{d}t}\\mathcal{F}(\\mu_{t})=\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{t})\\circ\\mathrm{T}_{t},\\frac{\\mathrm{d}\\mathrm{T}_{t}}{\\mathrm{d}t}\\rangle_{L^{2}(\\mu)}}\\end{array}$ . Thus, since $\\begin{array}{r}{\\frac{\\mathrm{d}^{2}\\mathrm{T}_{t}}{\\mathrm{d}t^{2}}=0}\\end{array}$ , ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\mathrm{d}^{2}}{\\mathrm{d}t^{2}}\\mathcal{F}(\\mu_{t})=\\frac{\\mathrm{d}}{\\mathrm{d}t}\\left<\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{t})\\circ\\mathrm{T}_{t},\\frac{\\mathrm{d}\\mathrm{T}_{t}}{\\mathrm{d}t}\\right>_{L^{2}(\\mu)}}\\\\ &{\\displaystyle\\qquad\\qquad\\quad=\\left<\\frac{\\mathrm{d}}{\\mathrm{d}t}\\left(\\nabla_{\\mathrm{W}_{2}}\\mathcal{F}(\\mu_{t})\\circ\\mathrm{T}_{t}\\right),\\frac{\\mathrm{d}\\mathrm{T}_{t}}{\\mathrm{d}t}\\right>_{L^{2}(\\mu)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "By Lemma 11, ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{\\mathrm{d}^{2}}{\\mathrm{d}t^{2}}\\mathcal{F}(\\mu_{t})=\\int\\!\\!\\int\\!\\left\\langle\\nabla_{y}\\nabla_{x}\\frac{\\delta^{2}\\mathcal{F}}{\\delta\\mu^{2}}((\\Gamma_{t})_{\\#}\\mu)\\left(\\Gamma_{t}(x),\\Gamma_{t}(y)\\right)\\frac{\\mathrm{d}\\Gamma_{t}}{\\mathrm{d}t}(y),\\frac{\\mathrm{d}\\Gamma_{t}}{\\mathrm{d}t}(x)\\right\\rangle\\,\\mathrm{d}\\mu(y)\\mathrm{d}\\mu(x)}\\\\ &{\\quad+\\int\\!\\!\\left\\langle\\nabla^{2}\\frac{\\delta\\mathcal{F}}{\\delta\\mu}((\\Gamma_{t})_{\\#}\\mu)\\left(\\Gamma_{t}(x)\\right)\\frac{\\mathrm{d}\\Gamma_{t}}{\\mathrm{d}t}(x),\\frac{\\mathrm{d}\\Gamma_{t}}{\\mathrm{d}t}(x)\\right\\rangle\\,\\mathrm{d}\\mu(x)}\\\\ &{=\\int\\!\\!\\!\\int\\!\\left\\langle\\nabla_{y}\\nabla_{x}\\frac{\\delta^{2}\\mathcal{F}}{\\delta\\mu^{2}}((\\Gamma_{t})_{\\#}\\mu)\\left(\\Gamma_{t}(x),\\Gamma_{t}(y)\\right)v(y),v(x)\\right\\rangle\\,\\mathrm{d}\\mu(y)\\mathrm{d}\\mu(x)}\\\\ &{\\quad+\\int\\!\\!\\left\\langle\\nabla^{2}\\frac{\\delta\\mathcal{F}}{\\delta\\mu}((\\Gamma_{t})_{\\#}\\mu)\\left(\\Gamma_{t}(x)\\right)v(x),v(x)\\right\\rangle\\,\\mathrm{d}\\mu(x)}\\\\ &{=\\int\\!\\left\\langle\\int\\nabla_{y}\\nabla_{x}\\frac{\\delta^{2}\\mathcal{F}}{\\delta\\mu^{2}}((\\Gamma_{t})_{\\#}\\mu)\\left(\\Gamma_{t}(x),\\Gamma_{t}(y)\\right)v(y)\\,\\mathrm{d}\\mu(y)}\\\\ &{\\quad+\\nabla^{2}\\frac{\\delta\\mathcal{F}}{\\delta\\mu}((\\Gamma_{t})_{\\#}\\mu)\\left(\\Gamma_{t}(x)\\right)v(x),v(x)\\right\\rangle\\,\\mathrm{d}\\mu(x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "H.10 Proof of Proposition 13 ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "1. (c1) $\\mathbf{\\nabla}\\mapsto\\mathbf{\\nabla}(\\mathbf{c}2)$ . Let $t>0$ , $t_{1},t_{2}\\in[0,1]$ , ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal F(\\tilde{\\mu}_{t}^{t_{1}\\rightarrow t_{2}})\\leq(1-t)\\mathcal F\\big((\\mathrm T_{t_{1}})_{\\#}\\mu\\big)+t\\mathcal F\\big((\\mathrm T_{t_{2}})_{\\#}\\mu\\big)}\\\\ &{\\qquad\\qquad\\iff\\frac{\\mathcal F\\big(\\tilde{\\mu}_{t}^{t_{1}\\rightarrow t_{2}}\\big)-\\mathcal F\\big((\\mathrm T_{t_{1}})_{\\#}\\mu\\big)}t\\leq\\mathcal F\\big((\\mathrm T_{t_{2}})_{\\#}\\mu\\big)-\\mathcal F\\big((\\mathrm T_{t_{1}})_{\\#}\\mu\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Passing to the limit $t\\rightarrow0$ and using Proposition 1, we get $\\langle\\nabla_{W_{2}}\\mathcal{F}\\big((\\mathrm{T}_{t_{1}})_{\\#}\\mu\\big)\\circ\\mathrm{T}_{t_{1}},\\mathrm{T}_{t_{2}}\\mathrm{~-~}$ $\\begin{array}{r}{\\mathrm{T}_{t_{1}}\\big\\rangle_{L^{2}(\\mu)}\\leq\\mathcal{F}\\big((\\mathrm{T}_{t_{2}})_{\\#}\\mu\\big)-\\mathcal{F}\\big((\\mathrm{T}_{t_{1}})_{\\#}\\mu\\big).}\\end{array}$ ", "page_idx": 49}, {"type": "text", "text": "2. $(\\mathbf{c}2)\\implies$ (c3). Let $t_{1},t_{2}\\in[0,1]$ , then by hypothesis, ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\int\\langle\\nabla_{W_{2}}\\mathcal{F}\\big((\\mathrm{T}_{t_{1}})_{\\#}\\mu\\big)\\circ\\mathrm{T}_{t_{1}},\\mathrm{T}_{t_{2}}-\\mathrm{T}_{t_{1}}\\big\\rangle_{L^{2}(\\mu)}\\leq\\mathcal{F}\\big((\\mathrm{T}_{t_{2}})_{\\#}\\mu\\big)-\\mathcal{F}\\big((\\mathrm{T}_{t_{1}})_{\\#}\\mu\\big)}\\\\ {\\big\\{\\langle\\nabla_{W_{2}}\\mathcal{F}\\big((\\mathrm{T}_{t_{2}})_{\\#}\\mu\\big)\\circ\\mathrm{T}_{t_{2}},\\mathrm{T}_{t_{1}}-\\mathrm{T}_{t_{2}}\\big\\rangle_{L^{2}(\\mu)}\\leq\\mathcal{F}\\big((\\mathrm{T}_{t_{1}})_{\\#}\\mu\\big)-\\mathcal{F}\\big((\\mathrm{T}_{t_{2}})_{\\#}\\mu\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Summing the two inequalities, we get ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\langle\\nabla_{W_{2}}\\mathcal{F}\\big((\\mathrm{T}_{t_{2}})_{\\#}\\mu\\big)\\circ\\mathrm{T}_{t_{2}}-\\nabla_{W_{2}}\\mathcal{F}\\big((\\mathrm{T}_{t_{1}})_{\\#}\\mu\\big)\\circ\\mathrm{T}_{t_{1}},\\mathrm{T}_{t_{2}}-\\mathrm{T}_{t_{1}}\\big)_{L^{2}(\\mu)}\\geq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "3. $(\\mathbf{c}3)\\implies(\\mathbf{c}4)$ . Let $t_{1},t_{2}\\in[0,1]$ . First, we have, ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\int_{0}^{1}\\frac{\\mathrm{d}^{2}}{\\mathrm{d}t^{2}}\\mathcal{F}(\\tilde{\\mu}_{t}^{t_{1}\\to t_{2}})\\;\\mathrm{d}t=\\frac{\\mathrm{d}}{\\mathrm{d}t}\\mathcal{F}(\\tilde{\\mu}_{t}^{t_{1}\\to t_{2}})\\Big|_{t=1}-\\frac{\\mathrm{d}}{\\mathrm{d}t}\\mathcal{F}(\\tilde{\\mu}_{t}^{t_{1}\\to t_{2}})\\Big|_{t=0}}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad=\\langle\\nabla_{W_{2}}\\mathcal{F}\\big((\\mathrm{T}_{t_{2}})_{\\#}\\mu\\big)\\circ\\mathrm{T}_{t_{2}}-\\nabla_{W_{2}}\\mathcal{F}\\big((\\mathrm{T}_{t_{1}})_{\\#}\\mu\\big)\\circ\\mathrm{T}_{t_{1}},}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\quad\\mathrm{T}_{t_{2}}-\\mathrm{T}_{t_{1}}\\big\\rangle_{L^{2}(\\mu)}}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\geq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Let $\\epsilon\\,\\in\\,(0,1)$ and define $t\\,\\mapsto\\,\\nu_{t}^{\\epsilon}\\,=\\,\\tilde{\\mu}_{\\epsilon t}^{t_{1}\\to1}$ the interpolation curve between $(\\mathrm{T}_{t_{1}})_{\\#}\\mu$ and $\\left(\\mathrm{T}_{t_{1}}\\,+\\,\\epsilon(\\mathrm{T}\\,-\\,\\mathrm{T}_{t_{1}})\\right)_{\\#}\\mu$ . Then, noting that $\\mathrm{T}_{t_{1}}\\,+\\,\\epsilon(\\mathrm{T}\\,-\\,\\mathrm{T}_{t_{1}})\\;=\\;\\mathrm{T}_{t_{1}+\\epsilon(1-t_{1})}$ , so ", "page_idx": 49}, {"type": "text", "text": "$\\nu_{t}^{\\epsilon}=\\tilde{\\mu}_{\\epsilon t}^{t_{1}\\rightarrow1}=\\tilde{\\mu}_{t}^{t_{1}\\rightarrow t_{1}+\\epsilon(1-t_{1})}$ and we have that ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\int_{0}^{1}\\frac{\\mathrm{d}^{2}}{\\mathrm{d}t^{2}}\\mathcal{F}(\\nu_{t}^{\\epsilon})\\;\\mathrm{d}t\\geq0.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Moreover, by continuity, $\\begin{array}{r}{\\frac{\\,\\mathrm{d}^{2}}{\\,\\mathrm{d}t^{2}}\\mathcal{F}(\\nu_{t}^{\\epsilon})\\;\\xrightarrow[\\epsilon\\to0]{}\\;\\frac{\\,\\mathrm{d}^{2}}{\\,\\mathrm{d}t^{2}}\\mathcal{F}\\big((\\mathrm{T}_{t_{1}})_{\\#}\\mu\\big)\\;=\\;\\frac{\\,\\mathrm{d}^{2}}{\\,\\mathrm{d}t^{2}}\\mathcal{F}(\\mu_{t_{1}})}\\end{array}$ . Then, since $\\begin{array}{r}{t\\;\\mapsto\\;\\frac{\\mathrm{d}^{2}}{\\mathrm{d}t^{2}}\\mathcal{F}(\\nu_{t}^{\\epsilon})}\\end{array}$ is continuous on $[0,1]$ , it is bounded, and we can apply the dominated convergence theorem. This implies that for all , ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\operatorname{Hess}_{\\mu_{t_{1}}}\\mathcal{F}=\\frac{\\mathrm{d}^{2}}{\\mathrm{d}t^{2}}\\mathcal{F}(\\mu_{t})\\Big|_{t=t_{1}}=\\operatorname*{lim}_{\\epsilon\\to0}\\int_{0}^{1}\\frac{\\mathrm{d}^{2}}{\\mathrm{d}t^{2}}\\mathcal{F}(\\nu_{t}^{\\epsilon})\\;\\mathrm{d}t\\geq0.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "4. $(\\mathbf{c}4)\\implies$ (c1). Let $t_{1},t_{2}\\,\\in\\,[0,1]$ and $\\varphi(t)\\,=\\,\\mathcal{F}(\\tilde{\\mu}_{t}^{t_{1}\\to t_{2}})$ for all $t\\in[0,1]$ . From [125, Equation 16.5], ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\forall t\\in[0,1],\\ \\varphi(t)=(1-t)\\varphi(0)+t\\varphi(1)-\\int_{0}^{1}\\frac{\\mathrm{d}^{2}}{\\mathrm{d}t^{2}}\\varphi(s)G(s,t)\\ \\mathrm{d}s,\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "where $G$ is the Green function defined as $G(s,t)=s(1-t)\\mathbb{1}_{\\{s\\leq t\\}}+t(1-s)\\mathbb{1}_{\\{t\\leq s\\}}\\geq0$ [125, Equation 16.6]. Then, $\\begin{array}{r}{\\frac{\\mathrm{d}^{2}}{\\mathrm{d}t^{2}}\\mathcal{F}(\\mu_{t})\\geq0}\\end{array}$ implies that $\\begin{array}{r}{\\int_{0}^{1}\\frac{\\mathrm{d}^{2}}{\\mathrm{d}t^{2}}\\varphi(s)G(s,t)\\,\\mathrm{d}s\\geq0}\\end{array}$ , and thus $\\varphi(t)=\\mathcal{F}(\\tilde{\\mu}_{t}^{t_{1}\\to t_{2}})\\leq(1-t)\\varphi(0)+t\\varphi(1)=(1-t)\\mathcal{F}\\big((\\mathrm{T}_{t_{1}})_{\\#}\\mu\\big)+t\\mathcal{F}\\big((\\mathrm{T}_{t_{2}})_{\\#}\\mu\\big).$ (174) ", "page_idx": 50}, {"type": "text", "text": "H.11 Proof of Proposition 24 ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Let $\\mathrm{J(T)}=\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{T},\\mathrm{Id})+\\tau\\big(\\nabla_{\\mathrm{W}_{2}}\\mathcal{G}(\\mu_{k}),\\mathrm{T}-\\mathrm{Id}\\big)_{L^{2}(\\mu_{k})}+\\mathcal{H}(\\mathrm{T}_{\\#}\\mu_{k})\\big)$ . Taking the first variation, we get ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\nabla\\mathrm{J}(\\tilde{\\mathrm{T}}_{k+1})=\\nabla\\phi_{\\mu_{k}}(\\tilde{\\mathrm{T}}_{k+1})-\\nabla\\phi_{\\mu_{k}}(\\mathrm{Id})+\\tau\\big(\\nabla_{\\mathrm{W}_{2}}\\mathcal{G}(\\mu_{k})+\\nabla_{\\mathrm{W}_{2}}\\mathcal{H}\\big((\\tilde{\\mathrm{T}}_{k+1})_{\\#}\\mu_{k}\\big)\\circ\\tilde{\\mathrm{T}}_{k+1}\\big)}}\\\\ &{}&{=\\nabla\\phi_{\\mu_{k}}(\\tilde{\\mathrm{T}}_{k+1})+\\tau\\nabla_{\\mathrm{W}_{2}}\\mathcal{H}\\big((\\tilde{\\mathrm{T}}_{k+1})_{\\#}\\mu_{k}\\big)\\circ\\tilde{\\mathrm{T}}_{k+1}-\\big(\\nabla\\phi_{\\mu_{k}}(\\mathrm{Id})-\\tau\\nabla_{\\mathrm{W}_{2}}\\mathcal{G}(\\mu_{k})\\big)}\\\\ &{}&{=\\nabla\\phi_{\\mu_{k}}(\\tilde{\\mathrm{T}}_{k+1})+\\tau\\nabla_{\\mathrm{W}_{2}}\\mathcal{H}\\big((\\tilde{\\mathrm{T}}_{k+1})_{\\#}\\mu_{k}\\big)\\circ\\tilde{\\mathrm{T}}_{k+1}-\\nabla\\phi_{\\mu_{k}}(\\mathrm{S}_{k+1}).\\qquad\\qquad\\qquad(1^{\\prime})}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Thus, ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\nabla\\mathrm{J}(\\tilde{\\mathrm{T}}_{k+1})=0\\iff\\tilde{\\mathrm{T}}_{k+1}\\in\\underset{\\mathrm{T}\\in L^{2}(\\mu_{k})}{\\mathrm{argmin}}\\ \\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{T},\\mathrm{S}_{k+1})+\\tau\\mathcal{H}(\\mathrm{T}_{\\#}\\mu_{k}).\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Now, we aim at showing that $\\tilde{\\mathrm{T}}_{k+1}=\\mathrm{T}_{k+1}\\circ\\mathrm{S}_{k+1}$ or ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathrm{T}\\in L^{2}(\\mu_{k})}\\;\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{T},\\mathrm{S}_{k+1})+\\tau\\mathcal{H}(\\mathrm{T}_{\\#}\\mu_{k})=\\operatorname*{min}_{\\mathrm{T}\\in L^{2}(\\nu_{k+1})}\\;\\mathrm{d}_{\\phi_{\\nu_{k+1}}}(\\mathrm{T},\\mathrm{Id})+\\tau\\mathcal{H}(\\mathrm{T}_{\\#}\\nu_{k+1}).\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "First, by the change of variable formula, since $\\phi_{\\mu}$ is pushforward compatible, observe that for $\\begin{array}{r}{\\mathrm{T}\\in L^{2}(\\nu_{k+1}),\\mathrm{d}_{\\phi_{\\nu_{k+1}}}(\\mathrm{T},\\mathrm{Id})+\\tau\\mathcal{H}(\\mathrm{T}_{\\#}\\nu_{k+1})=\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{T}\\circ\\mathrm{S}_{k+1},\\mathrm{S}_{k+1})+\\tau\\mathcal{H}\\big((\\mathrm{T}\\circ\\mathrm{S}_{k+1})_{\\#}\\mu_{k}\\big)}\\end{array}$ . Since $\\{\\mathrm{T}\\circ\\mathrm{S}_{k+1}\\mid\\mathrm{T}\\in L^{2}(\\nu_{k+1})\\}\\subset L^{2}(\\mu_{k})$ , we have ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathrm{T}\\in L^{2}(\\nu_{k+1})}\\;\\mathrm{d}_{\\phi_{\\nu_{k+1}}}(\\mathrm{T},\\mathrm{Id})+\\tau\\mathcal{H}(\\mathrm{T}_{\\#}\\nu_{k+1})\\geq\\operatorname*{min}_{\\mathrm{T}\\in L^{2}(\\mu_{k})}\\;\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{T},\\mathrm{S}_{k+1})+\\tau\\mathcal{H}(\\mathrm{T}_{\\#}\\mu_{k}).\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "By assumption, $\\nu_{k+1}\\,\\in\\,\\mathcal{P}_{2,\\mathrm{{ac}}}(\\mathbb{R}^{d})$ . Thus, applying Proposition 15, there exists $\\mathrm{T}_{\\phi_{\\nu_{k+1}}}^{\\nu_{k+1},\\mu_{k+1}}$ T\u03bdk+1,\u00b5k+1such that $(\\mathrm{T}_{\\phi_{\\nu_{k+1}}}^{\\nu_{k+1},\\mu_{k+1}}){\\_}\\nu_{k+1}=\\mu_{k+1}$ and $\\begin{array}{r}{\\mathrm{T}_{\\phi_{\\nu_{k+1}}}^{\\nu_{k+1},\\mu_{k+1}}=\\operatorname{argmin}_{\\mathrm{T},\\mathrm{T}_{\\#}\\nu_{k+1}=\\mu_{k+1}}\\,\\mathrm{d}_{\\phi_{\\nu_{k+1}}}(\\mathrm{T},\\mathrm{Id}),}\\end{array}$ and thus $\\mathrm{d}_{\\phi_{\\nu_{k+1}}}(\\mathrm{T}_{\\phi_{\\nu_{k+1}}}^{\\nu_{k+1},\\mu_{k+1}},\\mathrm{Id})=\\mathrm{W}_{\\phi}(\\mu_{k+1},\\nu_{k+1}).$ ", "page_idx": 50}, {"type": "text", "text": "By contradiction, we suppose that ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbb{T}\\in L^{2}(\\nu_{k+1})}\\;\\mathrm{d}_{\\phi_{\\nu_{k+1}}}(\\mathrm{T},\\mathrm{Id})+\\tau\\mathcal{H}(\\mathrm{T}_{\\#}\\nu_{k+1})>\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\tilde{\\mathrm{T}}_{k+1},\\mathrm{S}_{k+1})+\\tau\\mathcal{H}\\big((\\tilde{\\mathrm{T}}_{k+1})_{\\#}\\mu_{k}\\big).\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "$(\\mathsf{T}_{\\phi_{\\nu_{k+1}}}^{\\nu_{k+1},\\mu_{k+1}}\\circ\\mathsf{S}_{k+1})_{\\#}\\mu_{k}\\ =\\ (\\mathsf{T}_{\\phi_{\\nu_{k+1}}}^{\\nu_{k+1},\\mu_{k+1}})_{\\#}\\nu_{k+1}\\ =\\ \\mu_{k+1}$ , and therefore $\\begin{array}{r c l}{\\mathcal{H}\\big((\\mathrm{T}_{\\phi_{\\nu_{k+1}}}^{\\nu_{k+1},\\mu_{k+1}}\\circ\\mathrm{~S}_{k+1})_{\\#}\\mu_{k}\\big)}&{=}&{\\mathcal{H}(\\mu_{k+1})\\!}&{=\\!}&{\\mathcal{H}\\big((\\widetilde{\\mathrm{T}}_{k+1})_{\\#}\\mu_{k}\\big)}\\end{array}$ . On the other hand, $(\\tilde{\\mathrm{T}}_{k+1},\\mathrm{S}_{k+1})_{\\#}\\mu_{k}\\in\\Pi(\\mu_{k+1},\\nu_{k+1})$ , and thus ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\tilde{\\mathrm{T}}_{k+1},\\mathrm{S}_{k+1})\\geq\\mathrm{W}_{\\phi}(\\mu_{k+1},\\nu_{k+1})=\\mathrm{d}_{\\phi_{\\nu_{k+1}}}(\\mathrm{T}_{\\phi_{\\nu_{k+1}}}^{\\nu_{k+1},\\mu_{k+1}},\\mathrm{Id}).\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Thus, ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\underset{^{\\vee}\\in L^{2}(\\nu_{k+1})}{\\operatorname*{min}}\\,\\,\\mathrm{d}_{\\phi_{\\nu_{k+1}}}(\\mathrm{T},\\mathrm{Id})+\\tau\\mathcal{H}(\\mathrm{T}_{\\#}\\nu_{k+1})>\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\tilde{\\mathrm{T}}_{k+1},\\mathrm{S}_{k+1})+\\tau\\mathcal{H}\\big((\\tilde{\\mathrm{T}}_{k+1})_{\\#}\\mu_{k}\\big)\\qquad\\qquad(181)}\\\\ &{}&{\\geq\\mathrm{d}_{\\phi_{\\nu_{k+1}}}\\big(\\mathrm T_{\\phi_{\\nu_{k+1}}}^{\\nu_{k+1},\\mu_{k+1}},\\mathrm{Id}\\big)+\\tau\\mathcal{H}\\big(\\big(\\mathrm T_{\\phi_{\\nu_{k+1}}}^{\\nu_{k+1},\\mu_{k+1}}\\big)_{\\#}\\nu_{k+1}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "But $\\mathrm{T}_{\\phi_{\\nu_{k+1}}}^{\\nu_{k+1},\\mu_{k+1}}\\in L^{2}(\\nu_{k+1})$ , so this is a contradiction. So, we can conclude that the two schemes are equivalent, and moreover, $\\tilde{\\mathrm{T}}_{k+1}=\\mathrm{T}_{\\phi_{\\nu_{k+1}}}^{\\nu_{k+1},\\mu_{k+1}}\\circ\\mathrm{S}_{k+1}$ . \u53e3 ", "page_idx": 51}, {"type": "text", "text": "H.12 Proof of Proposition 25 ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Let $\\psi(\\mathrm{T})=\\tau\\big(\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{G}(\\mu_{k}),\\mathrm{T}-\\mathrm{Id}\\rangle_{L^{2}(\\mu_{k})}+\\mathcal{H}(\\mathrm{T}_{\\#}\\mu_{k})\\big)$ . Since $\\tilde{\\mathcal{H}}_{\\mu_{k}}$ is convex on $L^{2}(\\mu_{k})$ , $\\psi$ is convex, and we can apply the three-point inequality (Lemma 29) and for all $\\mathrm{T}\\in L^{2}(\\mu_{k})$ , ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tau\\big(\\mathcal{H}(\\mathrm{T}_{\\#}\\mu_{k})+\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{G}(\\mu_{k}),\\mathrm{T}-\\mathrm{Id}\\rangle_{L^{2}(\\mu_{k})}\\big)+\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{T},\\mathrm{Id})}\\\\ &{\\geq\\tau\\big(\\mathcal{H}(\\mu_{k+1})+\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{G}(\\mu_{k}),\\tilde{\\mathrm{T}}_{k+1}-\\mathrm{Id}\\rangle_{L^{2}(\\mu_{k})}\\big)+\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\tilde{\\mathrm{T}}_{k+1},\\mathrm{Id})+\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{T},\\tilde{\\mathrm{T}}_{k+1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "(182) which is equivalent to ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathcal{H}(\\mu_{k+1})+\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{G}(\\mu_{k}),\\tilde{\\mathrm{T}}_{k+1}-\\mathrm{Id}\\rangle_{L^{2}(\\mu_{k})}+\\frac{1}{\\tau}\\mathrm{d}_{\\phi_{\\mu}}(\\tilde{\\mathrm{T}}_{k+1},\\mathrm{Id})}}\\\\ &{}&{\\leq\\mathcal{H}(\\mathrm{T}_{\\#}\\mu_{k})+\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{G}(\\mu_{k}),\\mathrm{T}-\\mathrm{Id}\\rangle_{L^{2}(\\mu_{k})}+\\frac{1}{\\tau}\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{T},\\mathrm{Id})-\\frac{1}{\\tau}\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{T},\\tilde{\\mathrm{T}}_{k+1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Since $\\tilde{\\mathcal{G}}_{\\mu_{k}}$ is $\\beta.$ -smooth relatively to $\\phi_{\\mu_{k}}$ along $t\\mapsto\\big((1-t)\\mathrm{Id}+t\\tilde{\\mathrm{T}}_{k+1}\\big)_{\\#}\\mu_{k}$ , and $\\begin{array}{r}{\\tau\\le\\frac{1}{\\beta}}\\end{array}$ , we also have ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{G}(\\mu_{k+1})\\leq\\mathcal{G}(\\mu_{k})+\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{G}(\\mu_{k}),\\tilde{\\mathrm{T}}_{k+1}-\\mathrm{Id}\\rangle_{L^{2}(\\mu_{k})}+\\beta\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\tilde{\\mathrm{T}}_{k+1},\\mathrm{Id})}\\\\ &{\\qquad\\qquad\\leq\\mathcal{G}(\\mu_{k})+\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{G}(\\mu_{k}),\\tilde{\\mathrm{T}}_{k+1}-\\mathrm{Id}\\rangle_{L^{2}(\\mu_{k})}+\\frac{1}{T}\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\tilde{\\mathrm{T}}_{k+1},\\mathrm{Id}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Thus, applying first the smoothness of $\\mathcal{G}$ and then the three-point inequality, we get for all $\\mathrm{~T~}\\in$ $L^{2}(\\mu_{k})$ , ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\forall(\\mu_{k+1})+\\mathcal{G}(\\mu_{k+1})\\leq\\mathcal{H}(\\mu_{k+1})+\\mathcal{G}(\\mu_{k})+\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{G}(\\mu_{k}),\\tilde{\\mathrm{T}}_{k+1}-\\mathrm{Id}\\rangle_{L^{2}(\\mu_{k})}+\\frac{1}{7}\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\tilde{\\mathrm{T}}_{k+1},\\mathrm{Id})}}\\\\ &{}&{\\leq\\mathcal{H}(\\mathrm{T}_{\\#\\mu_{k}})+\\mathcal{G}(\\mu_{k})+\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{G}(\\mu_{k}),\\mathrm{T}-\\mathrm{Id}\\rangle_{L^{2}(\\mu_{k})}+\\frac{1}{7}\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{T},\\mathrm{Id})}\\\\ &{}&{\\:-\\,\\frac{1}{7}\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{T},\\tilde{\\mathrm{T}}_{k+1}).~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~(185)}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Now, let \u03bd \u2208P2(Rd) and T\u03d5\u00b5k,\u03bd $\\begin{array}{r}{\\mathrm{T}_{\\phi_{\\mu_{k}}}^{\\mu_{k},\\nu}=\\mathrm{argmin}_{\\mathrm{T},\\mathrm{T}\\#\\mu_{k}=\\nu}\\ \\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{T},\\mathrm{Id})}\\end{array}$ , and suppose that $\\tilde{\\mathcal{G}}_{\\mu_{k}}$ is $\\alpha$ -convex relative to $\\phi_{\\mu_{k}}$ along $t\\mapsto\\big((1-t)\\mathrm{Id}+t T_{\\phi_{\\mu_{k}}}^{\\mu_{k},\\nu}\\big)_{\\#}\\mu_{k}$ T \u03d5\u00b5kk  #\u00b5k. Thus, ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{d}_{\\tilde{\\mathcal{G}}_{\\mu_{k}}}(\\mathrm{T}_{\\phi_{\\mu_{k}}}^{\\mu_{k},\\nu},\\mathrm{Id})\\geq\\alpha\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{T}_{\\phi_{\\mu_{k}}}^{\\mu_{k},\\nu},\\mathrm{Id})}\\\\ &{\\qquad\\qquad\\Longleftrightarrow\\ \\mathcal{G}(\\nu)-\\alpha\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{T}_{\\phi_{\\mu_{k}}}^{\\mu_{k},\\nu},\\mathrm{Id})\\geq\\mathcal{G}(\\mu_{k})+\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{G}(\\mu_{k}),\\mathrm{T}_{\\phi_{\\mu_{k}}}^{\\mu_{k},\\nu}-\\mathrm{Id}\\rangle_{L^{2}(\\mu_{k})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Plugging this into (185), we get ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\mathcal{F}(\\mu_{k+1})\\leq\\mathcal{H}(\\nu)+\\mathcal{G}(\\nu)-\\alpha\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{T}_{\\phi_{\\mu_{k}}}^{\\mu_{k},\\nu},\\mathrm{Id})+\\frac{1}{\\tau}\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{T}_{\\phi_{\\mu_{k}}}^{\\mu_{k},\\nu},\\mathrm{Id})-\\frac{1}{\\tau}\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{T}_{\\phi_{\\mu_{k}}}^{\\mu_{k},\\nu},\\tilde{\\mathrm{T}}_{k+1}).\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Now, note that d\u03d5\u00b5k (T $\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{T}_{\\phi_{\\mu_{k}}}^{\\mu_{k},\\nu},\\mathrm{Id})\\ =\\ \\mathrm{W}_{\\phi}(\\nu,\\mu_{k})$ and by Assumption 1, $\\mathrm{d}_{\\phi_{\\mu_{k}}}(\\mathrm{T}_{\\phi_{\\mu_{k}}}^{\\mu_{k},\\nu},\\tilde{\\mathrm{T}}_{k+1})\\;\\geq$ $\\mathrm{W}_{\\phi}(\\nu,\\mu_{k+1})$ . Thus, ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\mathcal{F}(\\mu_{k+1})-\\mathcal{F}(\\nu)\\leq\\left(\\frac{1}{\\tau}-\\alpha\\right)\\mathrm{W}_{\\phi}(\\nu,\\mu_{k})-\\frac{1}{\\tau}\\mathrm{W}_{\\phi}(\\nu,\\mu_{k+1}).\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Using $\\mathrm{T}=\\mathrm{Id}$ in (185), we observe that $\\mathcal{F}(\\mu_{k})\\leq\\mathcal{F}(\\mu_{\\ell})$ for all $\\ell\\leq k$ . Moreover, $\\mathrm{W}_{\\phi}(\\nu,\\mu_{k})\\geq0$ . Thus, applying Lemma 30 with $f={\\mathcal{F}}$ , $c={\\mathcal{F}}(\\nu)$ and $\\boldsymbol{g}=\\mathrm{W}_{\\phi}(\\nu,\\cdot)$ , we obtain ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\forall k\\geq1,\\,\\mathcal{F}(\\mu_{k})-\\mathcal{F}(\\nu)\\leq\\frac{\\alpha}{\\left(\\frac{\\frac{1}{\\tau}}{\\frac{1}{\\tau}-\\alpha}\\right)^{k}-1}\\mathrm{W}_{\\phi}(\\nu,\\mu_{0})\\leq\\frac{\\frac{1}{\\tau}-\\alpha}{k}\\mathrm{W}_{\\phi}(\\nu,\\mu_{0}).\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "H.13 Proof of Lemma 26 ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "First, $\\nabla V^{*}$ is bijective. Thus, we only need to show that $\\boldsymbol{h}\\,=\\,\\boldsymbol{\\nabla}V\\,-\\,\\tau\\boldsymbol{\\nabla}U$ is injective. Take $u=V-\\tau U$ . ", "page_idx": 52}, {"type": "text", "text": "Since $U$ is $\\beta$ -smooth relative to $V$ , we have for all $x,y$ , ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r}{U(x)\\le U(y)+\\langle\\nabla U(y),x-y\\rangle+\\beta\\mathrm{d}_{V}(x,y),}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "which is equivalent to ", "page_idx": 52}, {"type": "equation", "text": "$$\n-U(y)\\leq-U(x)+\\langle\\nabla U(y),x-y\\rangle+\\beta\\mathrm{d}_{V}(x,y).\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Moreover, by definition of $\\mathrm{d}_{V}$ , ", "page_idx": 52}, {"type": "equation", "text": "$$\nV(y)=V(x)-\\langle\\nabla V(y),x-y\\rangle-\\mathrm{d}_{V}(x,y).\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Summing the two inequalities, we get ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V(y)-\\tau U(y)\\leq V(x)-\\langle\\nabla V(y),x-y\\rangle-\\mathrm{d}_{V}(x,y)-\\tau U(x)+\\tau\\langle\\nabla U(y),x-y\\rangle}\\\\ &{\\qquad\\qquad\\qquad+\\tau\\beta\\mathrm{d}_{V}(x,y)}\\\\ &{\\qquad\\qquad\\qquad=V(x)-\\tau U(x)-\\langle\\nabla V(y)-\\tau\\nabla U(y),x-y\\rangle-(1-\\tau\\beta)\\mathrm{d}_{V}(x,y).}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "This is equivalent to ", "page_idx": 52}, {"type": "equation", "text": "$$\nu(y)\\leq u(x)-\\langle\\nabla u(y),x-y\\rangle-(1-\\tau\\beta)\\mathrm{d}_{V}(x,y),\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "and thus with $u$ being $(1-\\tau\\beta)$ -convex relative to $V$ (for $\\tau\\beta\\leq1$ ). For $\\tau\\beta<1$ , it is equivalent to $u-(1-\\tau\\beta)V$ convex, i.e. $\\langle\\nabla u(x)-\\nabla u(y),x-y\\rangle\\geq(1-\\tau\\beta)\\langle\\nabla V(x)-\\nabla V(y),x-y\\rangle\\geq0$ . Since $V$ is strictly convex, $\\nabla u$ is injective. ", "page_idx": 52}, {"type": "text", "text": "Moreover, $|\\operatorname*{det}\\nabla\\mathrm{T}|\\,=\\,|\\operatorname*{det}\\left(\\nabla^{2}V^{*}\\circ(\\nabla V-\\tau\\nabla U)\\right)\\operatorname*{det}\\nabla^{2}u|\\,>\\,0$ because on one hand $u$ is $(1-\\beta\\tau)$ -convex relative to $V$ which is strictly convex, and on the other hand, $V^{*}$ is also strictly convex. ", "page_idx": 52}, {"type": "text", "text": "To conclude, applying [5, Lemma 5.5.3], $\\mathrm{T}_{\\#}\\mu$ is absolutely continuous with respect to the Lebesgue measure. ", "page_idx": 52}, {"type": "text", "text": "H.14 Proof of Proposition 27 ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "On one hand, $\\mathcal{H}$ is 1-smooth relative to $\\mathcal{H}$ , thus we only need to show that $\\textstyle\\mu\\mapsto\\int V\\mathrm{d}\\mu$ is smooth relative to $\\mathcal{H}$ . Using Proposition 13, we need to show that ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}^{2}}{\\mathrm{d}t^{2}}\\mathscr{V}(\\mu_{t})=\\frac{1}{2}\\int(\\mathrm{T}_{k+1}-\\mathrm{Id})^{T}\\nabla^{2}V(\\mathrm{T}_{k+1}-\\mathrm{Id})\\;\\mathrm{d}\\mu_{k}\\leq\\beta\\frac{\\mathrm{d}^{2}}{\\mathrm{d}t^{2}}\\mathscr{H}(\\mu_{t}).\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Recall from (61) that ${\\mathrm{T}}_{k+1}(x)=\\big((1-\\tau)\\Sigma_{k+1}\\Sigma_{k}^{-1}\\!+\\!\\tau\\Sigma_{k+1}\\Sigma^{-1}\\big)x\\!+\\!c s t$ , thus $\\nabla\\mathrm{T}_{k+1}$ is a constant. Using the computations of [43, Appendix B.2], ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}^{2}}{\\mathrm{d}t^{2}}\\mathcal{H}(\\mu_{t})=\\langle[\\nabla\\mathrm{T}_{t}]^{-2},\\nabla\\mathrm{T}_{k+1}-I_{d}\\rangle.\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Assuming $(1-\\tau)\\Sigma_{k+1}\\Sigma_{k}^{-1}+\\tau\\Sigma_{k+1}\\Sigma^{-1}\\succeq0,\\mathrm{T}_{k+1}$ is the gradient of a convex function and $\\mu_{t}$ is a Wasserstein geodesic. Thus, by [43], ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}^{2}}{\\mathrm{d}t^{2}}\\mathcal{H}(\\mu_{t})\\geq\\frac{1}{\\|\\Sigma_{\\mu_{t}}\\|_{\\mathrm{op}}}\\|\\mathrm{T}_{k+1}-\\mathrm{Id}\\|_{L^{2}(\\mu_{k})}^{2}.\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Moreover, by [33, Lemma 10], $\\mu\\,\\mapsto\\,\\|\\Sigma_{\\mu}\\|_{\\mathrm{op}}$ is convex along generalized geodesics, and thus $\\Sigma_{\\mu_{t}}\\preceq\\lambda I_{d}$ and $\\|\\Sigma_{\\mu_{t}}\\|_{\\mathrm{op}}\\leq\\lambda$ [43]. Hence, noting $\\sigma_{\\mathrm{max}}(M)$ the largest eigenvalue of some matrix $M$ , ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\mathrm{d}^{2}}{\\mathrm{d}t^{2}}\\mathcal{H}(\\mu_{t})\\geq\\frac{1}{\\lambda}\\Vert\\mathrm{T}_{k+1}-\\mathrm{Id}\\Vert_{L^{2}(\\mu_{k})}^{2}\\geq\\frac{1}{\\lambda\\sigma_{\\operatorname*{max}}(\\nabla^{2}V)}\\int(\\mathrm{T}_{k+1}-\\mathrm{Id})^{T}\\nabla^{2}V(T_{k+1}-\\mathrm{Id})\\mathrm{d}\\mu_{k}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\frac{2}{\\lambda\\sigma_{\\operatorname*{max}}(\\nabla^{2}V)}\\frac{\\mathrm{d}^{2}}{\\mathrm{d}t^{2}}\\mathcal{V}(\\mu_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "From this inequality, we deduce that ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\lambda\\sigma_{\\operatorname*{max}}\\left(\\nabla^{2}V\\right)}{2}\\mathrm{d}_{\\tilde{\\mathcal{H}}_{\\mu_{k}}}(\\mathrm{T}_{k+1},\\mathrm{H})=\\frac{\\lambda\\sigma_{\\operatorname*{max}}\\left(\\nabla^{2}V\\right)}{2}(\\mathcal{H}(\\mu_{k+1})-\\mathcal{H}(\\mu_{k})}\\\\ &{\\phantom{\\frac{...}{2}}-\\langle\\nabla_{\\mathrm{W}_{2}}\\mathcal{H}(\\mu_{k}),\\mathrm{T}_{k+1}-\\mathrm{H}\\rangle_{L^{2}(\\mu_{k})})}\\\\ &{\\phantom{\\frac{...}{2}}=\\frac{\\lambda\\sigma_{\\operatorname*{max}}\\left(\\nabla^{2}V\\right)}{2}\\int(1-t)\\frac{\\mathrm{d}^{2}}{\\mathrm{d}t^{2}}\\mathcal{H}(\\mu_{t})\\;\\mathrm{d}t}\\\\ &{\\phantom{\\frac{...}{2}}\\geq\\int\\frac{\\mathrm{d}^{2}}{\\mathrm{d}t^{2}}\\mathcal{V}(\\mu_{t})(1-t)\\;\\mathrm{d}t}\\\\ &{\\phantom{\\frac{...}{2}}=\\mathrm{d}_{\\tilde{\\mathcal{V}}_{\\mu_{k}}}(\\mathrm{T}_{k+1},\\mathrm{H}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "So, ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{d}_{\\tilde{\\mathcal{F}}_{\\mu_{k}}}(\\mathrm{T}_{k+1},\\mathrm{Id})=\\mathrm{d}_{\\tilde{\\mathcal{V}}_{\\mu_{k}}}(\\mathrm{T}_{k+1},\\mathrm{Id})+\\mathrm{d}_{\\tilde{\\mathcal{H}}_{\\mu_{k}}}(\\mathrm{T}_{k+1},\\mathrm{Id})}\\\\ &{\\qquad\\qquad\\qquad\\leq\\bigg(1+\\frac{\\lambda\\sigma_{\\operatorname*{max}}\\left(\\nabla^{2}V\\right)}{2}\\bigg)\\,\\mathrm{d}_{\\tilde{\\mathcal{H}}_{\\mu_{k}}}(\\mathrm{T}_{k+1},\\mathrm{Id}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "I Additional results ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "I.1 Three-point identity and inequality ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "In this Section, we derive results which are useful to show the convergence of mirror descent or preconditioned schemes. Namely, we first derive the three-point identity which we use to show the convergence of the preconditioned scheme in Proposition 5 as well as the three-point inequality, which we use for the convergence of the mirror descent scheme in Proposition 3. ", "page_idx": 53}, {"type": "text", "text": "Lemma 28 (Three-Point Identity). Let $\\phi:L^{2}(\\mu)\\to\\mathbb{R}$ be G\u00e2teaux differentiable. For all $\\mathrm{S},\\mathrm{T},\\mathrm{U}\\in$ $L^{2}(\\mu)$ , we have ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\mathrm{d}_{\\phi}(\\mathrm{S},\\mathrm{U})=\\mathrm{d}_{\\phi}(\\mathrm{S},\\mathrm{T})+\\mathrm{d}_{\\phi}(\\mathrm{T},\\mathrm{U})+\\langle\\nabla\\phi(\\mathrm{T}),\\mathrm{S}-\\mathrm{T}\\rangle_{L^{2}(\\mu)}-\\langle\\nabla\\phi(\\mathrm{U}),\\mathrm{S}-\\mathrm{T}\\rangle_{L^{2}(\\mu)}.\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Proof. Let $\\mathrm{S},\\mathrm{T},\\mathrm{U}\\in L^{2}(\\mu)$ , then using the linearity of the G\u00e2teaux differential, ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{d}_{\\phi}(\\mathrm{S},\\mathrm{U})-\\mathrm{d}_{\\phi}(\\mathrm{S},\\mathrm{T})-\\mathrm{d}_{\\phi}(\\mathrm{T},\\mathrm{U})=\\phi(\\mathrm{S})-\\phi(\\mathrm{U})-\\langle\\nabla\\phi(\\mathrm{U}),\\mathrm{S}-\\mathrm{U}\\rangle_{L^{2}(\\mu)}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad-\\phi(\\mathrm{S})+\\phi(\\mathrm{T})+\\langle\\nabla\\phi(\\mathrm{T}),\\mathrm{S}-\\mathrm{T}\\rangle_{L^{2}(\\mu)}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad-\\phi(\\mathrm{T})+\\phi(\\mathrm{U})+\\langle\\nabla\\phi(\\mathrm{U}),\\mathrm{T}-\\mathrm{U}\\rangle_{L^{2}(\\mu)}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=-\\langle\\nabla\\phi(\\mathrm{U}),\\mathrm{S}-\\mathrm{U}\\rangle_{L^{2}(\\mu)}+\\langle\\nabla\\phi(\\mathrm{T}),\\mathrm{S}-\\mathrm{T}\\rangle_{L^{2}(\\mu)}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\langle\\nabla\\phi(\\mathrm{U}),\\mathrm{T}-\\mathrm{U}\\rangle_{L^{2}(\\mu)}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=\\langle\\nabla\\phi(\\mathrm{T}),\\mathrm{S}-\\mathrm{T}\\rangle_{L^{2}(\\mu)}-\\langle\\nabla\\phi(\\mathrm{U}),\\mathrm{S}-\\mathrm{T}\\rangle_{L^{2}(\\mu)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Lemma 29 (Three-Point Inequality). Let $\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ , $\\mathrm{T}_{0}\\in L^{2}(\\mu)$ and $\\phi_{\\mu}:L^{2}(\\mu)\\to\\mathbb{R}$ convex, and G\u00e2teaux differentiable. Let $\\psi\\,:\\,L^{2}(\\mu)\\,\\to\\,\\mathbb{R}$ be convex, proper and lower semicontinuous. Assume there exists $\\mathrm{T}^{*}=\\mathrm{argmin}_{\\mathrm{T}\\in L^{2}(\\mu)}$ $\\mathrm{d}_{\\phi_{\\mu}}(\\mathrm{T},\\mathrm{T}_{0})+\\psi(\\mathrm{T})$ . Then, for all $\\mathrm{T}\\in L^{2}(\\mu)$ , ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\psi(\\mathrm{T})+\\mathrm{d}_{\\phi_{\\mu}}(\\mathrm{T},\\mathrm{T}_{0})\\geq\\psi(\\mathrm{T}^{*})+\\mathrm{d}_{\\phi_{\\mu}}(\\mathrm{T}^{*},\\mathrm{T}_{0})+\\mathrm{d}_{\\phi_{\\mu}}(\\mathrm{T},\\mathrm{T}^{*}).\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Proof. Denote $\\mathrm{J(T)}=\\mathrm{d}_{\\phi_{\\mu}}(\\mathrm{T},\\mathrm{T}_{0})+\\psi(\\mathrm{T})$ . Let $\\mathrm{T}^{*}=\\mathrm{argmin}_{\\mathrm{T}\\in L^{2}(\\mu)}\\ \\mathrm{J(T)}$ , hence $0\\in\\partial\\mathrm{J}(\\mathrm{T}^{*})$ . Since $\\phi$ and $\\psi$ are proper, convex and lower semicontinuous, and $\\mathrm{T}\\mapsto\\mathrm{d}_{\\phi_{\\mu}}(\\mathrm{T},\\mathrm{T}_{0})$ is continuous (since $\\phi_{\\mu}$ is continuous), thus by [99, Theorem 3.30], $\\partial\\mathrm{J(T^{*})}=\\partial\\psi(\\mathrm{T^{*}})+\\partial\\mathrm{d}_{\\phi_{\\mu}}(\\cdot,\\mathrm{T_{0}})(\\mathrm{T^{*}})$ . Moreover, since $\\phi_{\\mu}$ is differentiable, $\\partial\\mathrm{d}_{\\phi_{\\mu}}(\\cdot,\\mathrm{T}_{0})(\\mathrm{T}^{*})\\;=\\;\\{\\nabla_{\\mathrm{T}}\\mathrm{d}_{\\phi_{\\mu}}(\\mathrm{T}^{*},\\mathrm{T}_{0})\\}\\;=\\;\\{\\nabla\\phi_{\\mu}(\\mathrm{T}^{*})\\;-\\;\\phi_{\\mu}(\\mathrm{T}^{*})\\}\\;.$ $\\nabla\\phi_{\\mu}(\\mathrm{T}_{0})\\}$ , and thus $\\nabla\\phi_{\\mu}(\\mathrm{T}_{0})-\\nabla\\phi_{\\mu}(\\mathrm{T}^{*})\\overset{\\cdot}{\\in}\\partial\\psi(\\mathrm{T}^{*})$ Finally, by definition of subgradients and by applying Lemma 28, we get for all $\\mathrm{T}\\in L^{2}(\\mu)$ , ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\psi(\\mathrm{T})\\geq\\psi(\\mathrm{T}^{*})-\\left(\\langle\\nabla\\phi_{\\mu}(\\mathrm{T}^{*}),\\mathrm{T}-\\mathrm{T}^{*}\\rangle_{L^{2}(\\mu)}-\\langle\\nabla\\phi_{\\mu}(\\mathrm{T}_{0}),\\mathrm{T}-\\mathrm{T}^{*}\\rangle_{L^{2}(\\mu)}\\right)}\\\\ &{\\quad\\quad=\\psi(\\mathrm{T}^{*})-\\mathrm{d}_{\\phi_{\\mu}}(\\mathrm{T},\\mathrm{T}_{0})+\\mathrm{d}_{\\phi_{\\mu}}(\\mathrm{T},\\mathrm{T}^{*})+\\mathrm{d}_{\\phi_{\\mu}}(\\mathrm{T}^{*},\\mathrm{T}_{0}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Remark 3. Actually we can restrict $\\psi$ to be convex along $\\left((1-t)\\mathrm{T}^{*}+t\\mathrm{T}\\right)_{\\#}\\mu.$ . In that case, $\\mathrm{d}_{\\psi}(\\mathrm{T},\\mathrm{T^{*}})=\\psi(\\mathrm{T})-\\psi(\\mathrm{T^{*}})-\\langle\\varphi,\\mathrm{T}-\\mathrm{T^{*}}\\rangle_{L^{2}(\\mu)}\\geq0$ for $\\varphi\\in\\partial\\psi(\\mathrm{T}^{*})$ (by Proposition $^{13}$ ) and we still have $\\partial\\psi(\\mathrm{T}^{*})+\\partial\\mathrm{d}_{\\phi_{\\mu}}(\\cdot,\\mathrm{T}_{0})(\\mathrm{T}^{*})\\subset\\partial\\mathrm{J(T^{*})}$ (see [99, Theorem 3.30]) so that we can conclude. ", "page_idx": 54}, {"type": "text", "text": "I.2 Convergence lemma ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "We first provide a Lemma which follows from [88, Theorem 3.1], and which is useful for the proofs of Propositions 4, 6 and 25. ", "page_idx": 54}, {"type": "text", "text": "Lemma 30. Let $f\\;:\\;X\\;\\rightarrow\\;\\mathbb{R},$ , $g\\;:\\;X\\;\\rightarrow\\;\\mathbb{R}_{+}$ and $(x_{k})_{k\\in\\mathbb{N}}$ a sequence in $X$ such that for all $k\\geq1$ , $f(x_{k})\\,\\leq\\,f(x_{k-1})$ . Assume that there exists $\\beta\\,>\\,\\alpha\\,\\geq\\,0,$ , $c\\in\\mathbb{R}$ such that for all $k\\,\\geq\\,0$ , $f(x_{k+1})-c\\leq(\\beta-\\alpha)g(x_{k})-\\beta g(x_{k+1})$ , then ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\forall k\\geq1,\\ f(x_{k})-c\\leq{\\frac{\\alpha}{\\left({\\frac{\\beta}{\\beta-\\alpha}}\\right)^{k}-1}}g(x_{0})\\leq{\\frac{\\beta-\\alpha}{k}}g(x_{0}).\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Proof. First, observe the $f(x_{k})\\leq f(x_{\\ell})$ for all $\\ell\\leq k$ . Thus, for all $k\\geq1$ , ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{\\ell=1}^{k}\\left(\\frac{\\beta}{\\beta-\\alpha}\\right)^{\\ell}\\cdot\\left(f(x_{k})-c\\right)\\le\\displaystyle\\sum_{\\ell=1}^{k}\\left(\\frac{\\beta}{\\beta-\\alpha}\\right)^{\\ell}\\left(f(x_{\\ell})-c\\right))}&{}\\\\ {\\displaystyle\\le\\sum_{\\ell=1}^{k}\\left(\\frac{\\beta}{\\beta-\\alpha}\\right)^{\\ell}\\left((\\beta-\\alpha)g(x_{\\ell-1})-\\beta g(x_{\\ell})\\right)}&{}\\\\ {\\displaystyle=\\beta\\displaystyle\\sum_{\\ell=0}^{k-1}\\left(\\frac{\\beta}{\\beta-\\alpha}\\right)^{\\ell}g(x_{\\ell})-\\beta\\displaystyle\\sum_{\\ell=1}^{k}\\left(\\frac{\\beta}{\\beta-\\alpha}\\right)^{\\ell}g(x_{\\ell})}&{}\\\\ {\\displaystyle=\\beta g(x_{0})-\\beta\\left(\\frac{\\beta}{\\beta-\\alpha}\\right)^{k}g(x_{k})}&{}\\\\ {\\displaystyle\\le\\beta g(x_{0})~~~\\mathrm{since}\\,g\\ge0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Now, note that =(\u03b2\u2212\u03b2\u03b1\u03b1)k\u22121 =(1+\u03b2\u2212\u03b1\u03b1\u03b1)k\u22121 \u2264\u03b2\u2212k\u03b1since 1 +\u03b2\u2212\u03b1\u03b1 k $\\begin{array}{r}{\\left(1+\\frac{\\alpha}{\\beta-\\alpha}\\right)^{k}\\geq1+k\\frac{\\alpha}{\\beta-\\alpha}}\\end{array}$ (by convexity on $\\mathbb{R}_{+}$ of $x\\mapsto(1+x)^{k})$ . Thus, ", "page_idx": 54}, {"type": "equation", "text": "$$\nf(x_{k})-c\\leq{\\frac{\\beta}{\\sum_{\\ell=1}^{k}\\left({\\frac{\\beta}{\\beta-\\alpha}}\\right)^{\\ell}}}g(x_{0})={\\frac{\\alpha}{\\left({\\frac{\\beta}{\\beta-\\alpha}}\\right)^{k}-1}}g(x_{0})\\leq{\\frac{\\beta-\\alpha}{k}}g(x_{0}).\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "I.3 Some properties of Bregman divergences ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "We provide in this Section additional results on the Bregman divergences introduced in Section 3. First, we focus on $\\textstyle\\phi_{\\mu}(\\mathrm{T})=\\int V\\circ\\mathrm{T}\\,\\mathrm{d}\\mu$ . The following Lemma is akin to [80, Proposition 4] which shows it only for OT maps. ", "page_idx": 54}, {"type": "text", "text": "Lemma 31. Let $V:\\ensuremath{\\mathbb{R}^{d}}\\to\\ensuremath{\\mathbb{R}}$ convex and $\\begin{array}{r}{\\phi_{\\mu}(\\mathrm{T})=\\int V\\circ\\mathrm{T}\\,\\mathrm{d}\\mu.}\\end{array}$ . Then, ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\forall\\mathrm{T},\\mathrm{S}\\in L^{2}(\\mu),\\;\\mathrm{d}_{\\phi_{\\mu}}(\\mathrm{T},\\mathrm{S})=\\int\\mathrm{d}_{V}\\big(\\mathrm{T}(x),\\mathrm{S}(x)\\big)\\;\\mathrm{d}\\mu(x).\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Proof. Let $\\mathrm{T},\\mathrm{S}\\in L^{2}(\\mu)$ , then remembering that $\\nabla_{\\mathrm{W}_{2}}\\mathcal{V}(\\mu)=\\nabla V$ , we have ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{\\mathrm{d}_{\\phi_{\\mu}}(\\mathrm{T},\\mathrm{S})=\\phi_{\\mu}(\\mathrm{T})-\\phi_{\\mu}(\\mathrm{S})-\\langle\\nabla V\\circ\\mathrm{S},\\mathrm{T}-\\mathrm{S}\\rangle_{L^{2}(\\mu)}}}\\\\ {\\displaystyle{\\qquad\\qquad=\\int V\\circ\\mathrm{T}-V\\circ\\mathrm{S}-\\langle\\nabla V\\circ\\mathrm{S},\\mathrm{T}-\\mathrm{S}\\rangle\\;\\mathrm{d}\\mu}}\\\\ {\\displaystyle{\\qquad=\\int\\mathrm{d}_{V}\\big(\\mathrm{T}(x),\\mathrm{S}(x)\\big)\\;\\mathrm{d}\\mu(x).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Next, we focus on $\\begin{array}{r}{\\phi_{\\mu}(\\mathrm{T})\\,=\\,\\frac12\\int\\!\\int W\\big(\\mathrm{T}(x)-\\mathrm{T}(x^{\\prime})\\big)\\ \\mathrm{d}\\mu(x)\\mathrm{d}\\mu(x^{\\prime})}\\end{array}$ , and we generalize the result from [80, Proposition 4]. ", "page_idx": 55}, {"type": "text", "text": "Lemma 32. Let $W:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ even $(W(x)=W(-x))$ , convex and differentiable. Let $\\phi_{\\mu}(\\mathrm{T})=$ $\\begin{array}{r}{\\frac{1}{2}\\iint W\\big(\\mathrm{T}(x)-\\mathrm{T}(x^{\\prime})\\big)\\;\\mathrm{d}\\mu(x)\\mathrm{d}\\mu(x^{\\prime})}\\end{array}$ . Then, ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\forall{\\mathrm{T}},{\\mathrm{S}}\\in L^{2}(\\mu),\\ \\mathrm{d}_{\\phi_{\\mu}}({\\mathrm{T}},{\\mathrm{S}})=\\frac{1}{2}\\iint\\mathrm{d}_{W}\\left({\\mathrm{T}}(x)-{\\mathrm{T}}(x^{\\prime}),{\\mathrm{S}}(x)-{\\mathrm{S}}(x^{\\prime})\\right)\\,\\mathrm{d}\\mu(x)\\mathrm{d}\\mu(x^{\\prime}).\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Proof. Let $\\mathrm{T},\\mathrm{S}\\,\\in\\,L^{2}(\\mu)$ , remember that $\\nabla_{\\mathrm{W}_{2}}\\mathcal{W}(\\mu)=\\nabla W\\star\\mu$ , and thus $\\nabla_{\\mathrm{W}_{2}}\\mathcal{W}(\\mathrm{S}_{\\#}\\mu)\\circ\\mathrm{S}=$ $(\\nabla W\\star\\ensuremath{\\mathrm{S}}_{\\#}\\mu)\\circ\\ensuremath{\\mathrm{S}}$ . Thus, ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathrm{d}_{\\phi_{\\mu}}(\\mathrm{T},\\mathrm{S})=\\phi_{\\mu}(\\mathrm{T})-\\phi_{\\mu}(\\mathrm{S})-\\langle(\\nabla W\\star\\mathrm{S}_{\\#}\\mu)\\circ\\mathrm{S},\\mathrm{T}-\\mathrm{S}\\rangle_{L^{2}(\\mu)}}\\\\ {\\displaystyle\\qquad=\\frac{1}{2}\\iint W\\big(\\mathrm{T}(x)-\\mathrm{T}(x^{\\prime})\\big)\\;\\mathrm{d}\\mu(x)\\mathrm{d}\\mu(x^{\\prime})-\\frac{1}{2}\\iint W\\big(\\mathrm{S}(x)-\\mathrm{S}(x^{\\prime})\\big)\\;\\mathrm{d}\\mu(x)\\mathrm{d}\\mu(x^{\\prime})}\\\\ {\\displaystyle\\qquad\\qquad-\\int\\langle(\\nabla W\\star\\mathrm{S}_{\\#}\\mu)(\\mathrm{S}(x)),\\mathrm{T}(x)-\\mathrm{S}(x)\\rangle\\;\\mathrm{d}\\mu(x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Then, note that $\\nabla W(-x)=-\\nabla W(x)$ and thus the last term can be written as: ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int\\langle(\\nabla W\\star\\mathbf{S}_{q}\\mu)(\\nabla(x)),\\nabla(x)-\\mathbf{S}(x)\\rangle\\;\\mathrm{d}\\mu(x)}\\\\ &{=\\int\\!\\!\\int\\langle\\nabla W(\\mathbf{S}(x)-\\mathbf{S}(x^{\\prime})),\\nabla(x)-\\mathbf{S}(x^{\\prime})\\rangle\\;\\mathrm{d}\\mu(x)\\mathrm{d}\\mu(x^{\\prime})}\\\\ &{=\\frac{1}{2}\\int\\!\\!\\int\\langle\\nabla W(\\mathbf{S}(x)-\\mathbf{S}(x^{\\prime})),\\nabla(x)-\\mathbf{S}(x)\\rangle\\;\\mathrm{d}\\mu(x)\\mathrm{d}\\mu(x^{\\prime})}\\\\ &{\\quad+\\frac{1}{2}\\langle\\nabla W(\\mathbf{S}(x^{\\prime})-\\mathbf{S}(x)),\\nabla(y)-\\mathbf{S}(y)\\rangle\\;\\mathrm{d}\\mu(x)\\mathrm{d}\\mu(x^{\\prime})}\\\\ &{=\\frac{1}{2}\\int\\!\\!\\int\\langle\\nabla W(\\mathbf{S}(x)-\\mathbf{S}(x^{\\prime})),\\nabla(x)-\\mathbf{S}(x)\\rangle\\;\\mathrm{d}\\mu(x)\\mathrm{d}\\mu(x^{\\prime})}\\\\ &{\\quad-\\frac{1}{2}\\langle\\nabla W(\\mathbf{S}(x)-\\mathbf{S}(x^{\\prime})),\\nabla(x^{\\prime})-\\mathbf{S}(x^{\\prime})\\rangle\\;\\mathrm{d}\\mu(x)\\mathrm{d}\\mu(x^{\\prime})}\\\\ &{=\\frac{1}{2}\\int\\!\\!\\int\\!\\langle\\nabla W(\\mathbf{S}(x)-\\mathbf{S}(x^{\\prime})),\\nabla(x)-\\mathbf{T}(x^{\\prime})-(\\mathbf{S}(x)-\\mathbf{S}(x^{\\prime}))\\rangle\\;\\mathrm{d}\\mu(x)\\mathrm{d}\\mu(x^{\\prime})}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Finally, we get ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathrm{d}_{\\phi_{\\mu}}(\\mathrm{T},\\mathrm{S})=\\frac{1}{2}\\iint\\Big(W\\big(\\mathrm{T}(x)-\\mathrm{T}(x^{\\prime})\\big)-W\\big(\\mathrm{S}(x)-\\mathrm{S}(x^{\\prime})\\big)}\\\\ &{\\qquad\\qquad\\quad-\\,\\langle\\nabla W\\big(\\mathrm{S}(x)-\\mathrm{S}(x^{\\prime})\\big),\\mathrm{T}(x)-\\mathrm{T}(x^{\\prime})-\\big(\\mathrm{S}(x)-\\mathrm{S}(x^{\\prime})\\big)\\rangle\\Big)\\;\\mathrm{d}\\mu(x)\\mathrm{d}\\mu(x^{\\prime})}\\\\ &{\\qquad\\quad=\\displaystyle\\frac{1}{2}\\iint\\mathrm{d}_{W}\\big(\\mathrm{T}(x)-\\mathrm{T}(x^{\\prime}),\\mathrm{S}(x)-\\mathrm{S}(x^{\\prime})\\big)\\;\\mathrm{d}\\mu(x)\\mathrm{d}\\mu(x^{\\prime}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Now, we make the connection with the mirror map used by Deb et al. [41] and derive the related Bregman divergence. ", "page_idx": 55}, {"type": "text", "text": "Lemma 33. Let $\\begin{array}{r}{\\phi_{\\mu}(\\mathrm{T})=\\frac{1}{2}\\mathrm{W}_{2}^{2}(\\mathrm{T}_{\\#}\\mu,\\rho)}\\end{array}$ for $\\mu,\\rho\\in\\mathcal P_{2,\\mathrm{ac}}(\\mathbb{R}^{d})$ . Then, for all $\\mathrm{T},\\mathrm{S}\\in L^{2}(\\mu)$ , such that $\\mathrm{T}_{\\#}\\mu,\\mathrm{S}_{\\#}\\mu\\in\\mathcal{P}_{\\mathrm{2,ac}}(\\mathbb{R}^{d})$ , ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\mathsf{l}_{\\phi_{\\mu}}(\\mathrm{T},\\mathrm{S})=\\frac{1}{2}\\|\\boldsymbol{\\Gamma}_{\\mathrm{T}_{\\#}\\mu}^{\\rho}\\circ\\mathrm{T}-\\boldsymbol{\\Gamma}_{\\mathrm{S}_{\\#}\\mu}^{\\rho}\\circ\\mathrm{S}-(\\mathrm{T}-\\mathrm{S})\\|_{L^{2}(\\mu)}^{2}+\\left\\langle\\boldsymbol{\\Gamma}_{\\mathrm{S}_{\\#}\\mu}^{\\rho}\\circ\\mathrm{S}-\\mathrm{S},\\boldsymbol{\\Gamma}_{\\mathrm{T}_{\\#}\\mu}^{\\rho}\\circ\\mathrm{T}-\\boldsymbol{\\Gamma}_{\\mathrm{S}_{\\#}\\mu}^{\\rho}\\circ\\mathrm{S}\\right\\rangle_{L^{2}(\\mu)},\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "where T\u03c1T#\u00b5 denotes the OT map between $\\mathrm{T}_{\\#}\\mu$ and $\\rho$ . ", "page_idx": 55}, {"type": "text", "text": "Proof. Let $\\mathrm{T},\\mathrm{S}\\,\\in\\,L^{2}(\\mu)$ such that $\\mathrm{T}_{\\#}\\mu,\\mathrm{S}_{\\#}\\mu\\;\\in\\;\\mathcal{P}_{\\mathrm{2,ac}}(\\mathbb{R}^{d})$ . Remember that $\\nabla_{\\mathrm{W_{2}}}\\mathrm{W}_{2}^{2}(\\cdot,\\rho)\\,=$ $\\mathrm{Id}-\\mathrm{T}_{\\cdot}^{\\rho}$ , then ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\bar{u}_{\\theta\\theta}(\\bar{T},\\theta)=\\bar{\\theta}_{\\theta}(\\bar{T})-\\theta_{\\theta}(\\bar{T})-\\bar{\\theta}(\\pi_{\\theta},\\theta)\\theta\\theta>\\bar{\\theta}_{\\theta}(\\bar{T}),}&{}\\\\ {=\\frac{1}{2}\\overline{{\\Pi_{\\theta}}}[\\mathcal{T}_{i}\\pi_{\\theta},\\theta]-\\frac{1}{2}\\overline{{\\Pi_{\\theta}}}[\\mathcal{T}_{i}\\theta_{\\theta},\\theta]-(\\overline{{\\Pi_{\\theta}}}-\\pi_{\\theta}^{\\theta})\\theta\\cdot\\bar{\\Pi_{\\theta}}[\\mathcal{T}-\\mathcal{S}]_{i}\\pi_{\\theta}\\theta}\\\\ &{=\\frac{1}{2}\\overline{{\\|\\Gamma_{\\theta}\\pi_{\\theta}\\theta\\|^{2}}}-\\frac{1}{2}\\overline{{\\Pi_{\\theta}}}[\\mathcal{T}_{i}\\theta_{\\theta},\\theta]-8\\overline{{\\Pi_{\\theta}}}[\\mathcal{S}_{i}\\theta_{\\theta},0]+(\\overline{{\\Pi_{\\theta}}}\\pi_{\\theta}^{\\theta}-8.\\nabla_{\\theta}T-8.\\nabla_{\\theta}T_{i}\\pi_{\\theta})}\\\\ &{=\\frac{1}{2}\\overline{{\\|\\Gamma_{\\theta}\\pi_{\\theta}\\theta^{2}\\|^{2}}},}\\\\ &{\\quad+(\\overline{{\\Pi_{\\theta}}}\\pi_{\\theta}^{\\theta}-8.\\nabla_{\\theta}T-\\overline{{\\Pi_{\\theta}}}\\pi_{\\theta}^{\\theta}+8\\overline{{\\Pi_{\\theta}}}\\theta)+(\\overline{{\\Pi_{\\theta}}}\\pi_{\\theta}^{\\theta}-8.8T_{i}\\pi_{\\theta}^{\\theta}\\otimes8-S_{i}\\overline{{\\mathcal{X}}})\\pi_{\\theta}\\theta}\\\\ &{=\\frac{1}{2}\\overline{{\\|\\Gamma_{\\theta}\\pi_{\\theta}\\theta^{2}\\|^{2}}}-\\overline{{\\Pi_{\\theta}^{2}}}[\\mathcal{T}_{i}\\theta_{\\theta},\\theta]+8\\overline{{\\Pi_{\\theta}}}[\\mathcal{R}_{i}\\theta_{\\theta},0]}\\\\ &{\\quad-(\\overline{{\\Pi_{\\theta}}}\\pi_{\\theta}^{\\theta}-8.5T_{i}\\pi_{\\theta}^{\\theta}\\otimes-{\\Pi_{\\theta}}\\pi_{\\theta}^{\\theta})}\\\\ &{ \n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "J NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 57}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 57}, {"type": "text", "text": "Justification: In the abstract and introduction, we claim that we adapt mirror descent and the preconditioned gradient descent to the Wasserstein space, and that we provide guarantees of convergence. These results are presented in Section 3 and Section 4 for respectively mirror descent and preconditioned gradient descent. We also claim that we illustrate advantages of such schemes on ill-conditioned optimization tasks and to minimize discrepancies in order to align single-cells, which we do in Section 5. ", "page_idx": 57}, {"type": "text", "text": "Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 57}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 57}, {"type": "text", "text": "Justification: On the two theoretical sections (Section 3 and Section 4), we state all the hypotheses to obtain our convergence results. Then, in Section 5, we discuss some couples of target functional $\\mathcal{F}$ and Bregman potential/preconditioner $\\phi$ for which we can verify these assumptions. However, in general, verifying these assumptions is a hard problem, as stated in the Conclusion, and we leave for future works these investigations. ", "page_idx": 57}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best ", "page_idx": 57}, {"type": "text", "text": "judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 58}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 58}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 58}, {"type": "text", "text": "Justification: The full set of assumptions are provided for each theoretical result, along with a complete proof in Appendix. ", "page_idx": 58}, {"type": "text", "text": "Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 58}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 58}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 58}, {"type": "text", "text": "Justification: The information to reproduce the main experimental results are described in Section 5 and Appendix G. ", "page_idx": 58}, {"type": "text", "text": "Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 58}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 59}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 59}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 59}, {"type": "text", "text": "Justification: We provide a part of the code to reproduce the experiment on Gaussians and on interaction functionals in supplementary materials and in https://github.com/clbonet/Mirror_and_Preconditioned_Gradient_ Descent_in_Wasserstein_Space. ", "page_idx": 59}, {"type": "text", "text": "Guidelines: ", "page_idx": 59}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 59}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 59}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Justification: All the training and test details for the experiments are provided in Appendix G. Guidelines: ", "page_idx": 59}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 59}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 59}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 59}, {"type": "text", "text": "Justification: The experiment on Gaussians in Figure 2 is run over 20 randomly sampled objective covariances, and the results are plotted with the standard deviation. For the single-cell experiment of Figure 3, we reported the results with 3 different initialization for each treatment, along their mean. For the mirror interaction experiment, the results are deterministic. ", "page_idx": 60}, {"type": "text", "text": "Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 60}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 60}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 60}, {"type": "text", "text": "Justification: We report in Appendix G the computer resources and approximate runtime for each experiment. Namely, the Gaussian and mirror interaction experiments were done on a personal laptop on CPU, and took only few hours to run. The single-cell experiment was performed on GPU as the data are of higher dimension with about 4000 samples, and took a few hundred of hours of computational time. ", "page_idx": 60}, {"type": "text", "text": "Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 60}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 60}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 60}, {"type": "text", "text": "Justification: The research conducted in this paper is conform with the NeurIPS Code of Ethics. ", "page_idx": 60}, {"type": "text", "text": "Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 61}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 61}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 61}, {"type": "text", "text": "Justification: This work is mostly theoretical and not tied to particular applications. ", "page_idx": 61}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 61}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 61}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 61}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 61}, {"type": "text", "text": "Guidelines: ", "page_idx": 61}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 61}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 62}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 62}, {"type": "text", "text": "Justification: The datasets used are properly cited. Guidelines: ", "page_idx": 62}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 62}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 62}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 62}, {"type": "text", "text": "Justification: [NA] Guidelines: ", "page_idx": 62}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 62}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 62}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 62}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 62}, {"type": "text", "text": "Guidelines: ", "page_idx": 62}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 62}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 62}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 63}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 63}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 63}]