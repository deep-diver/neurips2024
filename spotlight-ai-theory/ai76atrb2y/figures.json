[{"figure_path": "Ai76ATrb2y/figures/figures_7_1.jpg", "caption": "Figure 1: Prior-posterior scatter plots for LLP, RR, and LLP+Geom from two synthetic datasets (where the prior \u03b7(x) is drawn) and the two real-world datasets (where \u03b7(x) is approximated). The colors of the dots correspond to different parameter values for the PETs. For each bag size k and distribution, we did 1000 independent runs. The further a point is from the y = x dotted line, the more is revealed about its label as a result of the PET.", "description": "This figure presents prior-posterior scatter plots that visualize the privacy leakage of three different privacy-enhancing technologies (PETs): Label Proportions (LLP), Randomized Response (RR), and LLP+Geometric Noise.  The plots compare the attacker's prior belief about a label (before seeing the PET output) against their posterior belief (after seeing the PET output) for different parameter settings (bag size k for LLP and epsilon for RR) on various datasets. The further a data point deviates from the y=x line (representing no information gain from the PET), the higher the privacy leakage.  The plots help to illustrate how different PETs affect the relationship between the prior and posterior beliefs and the level of information leakage.", "section": "4 Experiments"}, {"figure_path": "Ai76ATrb2y/figures/figures_8_1.jpg", "caption": "Figure 1: Prior-posterior scatter plots for LLP, RR, and LLP+Geom from two synthetic datasets (where the prior \u03b7(x) is drawn) and the two real-world datasets (where \u03b7(x) is approximated). The colors of the dots correspond to different parameter values for the PETs. For each bag size k and distribution, we did 1000 independent runs. The further a point is from the y=x dotted line, the more is revealed about its label as a result of the PET.", "description": "This figure shows prior-posterior scatter plots for three different privacy-enhancing technologies (PETs): Label Proportions (LLP), Randomized Response (RR), and LLP with Geometric Noise (LLP+Geom).  The plots visualize the relationship between an adversary's prior belief about a label and their posterior belief after observing the output of each PET.  Four datasets are used: two synthetic datasets with different prior distributions (Beta(2,30) and Uniform([0,1])), and two real-world datasets (Higgs and KDD12). Different colors represent different parameter settings for each PET, revealing how varying parameters influence the amount of information leaked about the labels.", "section": "4 Experiments"}, {"figure_path": "Ai76ATrb2y/figures/figures_9_1.jpg", "caption": "Figure 3: Privacy vs utility tradeoff curves for the various PETs on the Higgs (left) and KDD12 (right) datasets. Utility is measured by AUC on test set, while privacy is either the additive measure (bottom row) or the 98th-percentile of the multiplicative measure (so as to rule out the infinite multiplicative advantage cases that can occur for LLP). Each point corresponds to a setting of the privacy parameter for the PET (e for RR, k for LLP, and both for LLP+Geom). The x-coordinate is the advantage (either additive or multiplicative) value for that PET, while the y-coordinate is the test AUC of a model trained from the output of that PET. The AUC of the model trained without a PET roughly corresponds to the top value achieved by these curves.", "description": "This figure shows the trade-off between privacy and utility for different privacy-enhancing technologies (PETs). The x-axis represents the privacy risk (measured using either additive or multiplicative advantage), while the y-axis represents the utility (measured by the Area Under the Curve (AUC) of a model trained on the privatized data).  Each line represents a different PET with varying privacy parameters.  The figure demonstrates how the choice of PET and its parameters impact the balance between privacy and utility, showing that differentially private schemes often offer a better privacy-utility tradeoff.", "section": "4 Experiments"}, {"figure_path": "Ai76ATrb2y/figures/figures_29_1.jpg", "caption": "Figure 4: Prior-posterior scatter plots for LLP+Geom and LLP+Lap on two synthetic datasets and the two real-world datasets. The two synthetic datasets have been generated by drawing \u03b7(x) from a Beta(2,30) distribution and a uniform distribution on [0, 1]. The colors of the dots correspond to different parameter values for the PETs. For each bag size k and distribution, we did 1000 independent runs. The further a point is from the y = x dotted line, the more is revealed about its label as a result of the PET.", "description": "This figure presents prior-posterior scatter plots that visualize the privacy leakage of two label privatization mechanisms (LLP+Geom and LLP+Lap).  Two synthetic datasets and two real-world datasets (Higgs and KDD12) are used to assess how much each mechanism reveals about true labels when provided with a privatized version.  The plots show the prior probability P(y=1|x) versus the posterior probability P(y=1|x,M(x,y)=z) for each data point, where M represents the privatization mechanism.  Points near the diagonal (y=x) indicate low privacy leakage, while points far from the diagonal indicate high leakage. Different colors represent different parameter settings (bag size k and privacy parameter epsilon).", "section": "4 Experiments"}, {"figure_path": "Ai76ATrb2y/figures/figures_29_2.jpg", "caption": "Figure 3: Privacy vs utility tradeoff curves for the various PETs on the Higgs (left) and KDD12 (right) datasets. Utility is measured by AUC on test set, while privacy is either the additive measure (bottom row) or the 98th-percentile of the multiplicative measure (so as to rule out the infinite multiplicative advantage cases that can occur for LLP). Each point corresponds to a setting of the privacy parameter for the PET (\u20ac for RR, k for LLP, and both for LLP+Geom). The x-coordinate is the advantage (either additive or multiplicative) value for that PET, while the y-coordinate is the test AUC of a model trained from the output of that PET. The AUC of the model trained without a PET roughly corresponds to the top value achieved by these curves.", "description": "This figure displays the trade-off between privacy and utility for different privacy-enhancing technologies (PETs) applied to the Higgs and KDD12 datasets.  The x-axis represents the privacy level, measured using both additive and multiplicative advantage metrics.  The y-axis shows the utility, measured by the Area Under the Curve (AUC) of a model trained on data processed by each PET.  Different colored lines represent different PETs and their parameter settings.  The figure shows that differentially private mechanisms generally offer a better privacy-utility trade-off compared to more heuristic approaches.", "section": "4 Experiments"}]