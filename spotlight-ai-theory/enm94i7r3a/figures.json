[{"figure_path": "eNM94i7R3A/figures/figures_1_1.jpg", "caption": "Figure 1: Unbalanced initializations lead to rapid rich learning and generalization. We follow the experimental setup used in Fig. 1 of Chizat et al. [8] \u2013 a wide two-layer student ReLU network f(x; \u03b8) = \u03a3h=1 ai max(0, wTix) trained on a dataset generated from a narrow two-layer teacher ReLU network. The student parameters are initialized as w\u2081 ~ Unif(Sd\u22121(+)) and a\u2081 = \u00b1ar, such that \u03c4 > 0 controls the overall scale of the function, while \u03b1 > 0 controls the relative scale of the first and second layers through the conserved quantity \u03b4 = \u03c4\u00b2(\u03b1\u00b2 \u2013 \u03b1\u00af\u00b2). (a) Shows the training trajectories of |ai|wi (color denotes sgn(ai)) when \u03b4 = 2 for four different settings of \u03c4, \u03b4. The left plot confirms that small overall scale leads to rich and large overall scale to lazy. The right plot shows that even at small overall scale, the relative scale can move the network between rich and lazy as well. Here an upstream initialization \u03b4 > 0 shows striking alignment to the teacher (dotted lines), while a downstream initialization \u03b4 < 0 shows no alignment. (b) Shows the test loss and kernel distance from initialization computed through training over a sweep of \u03c4 and \u03b4 when \u03b4 = 100. Lazy learning happens when \u03c4 is large, rich learning happens when \u03c4 is small, and rapid rich learning happens when both \u03c4 is small and \u03b4 is large \u2013 an upstream initialization. This initialization also leads to the smallest test loss. See Fig. 10 in Appendix D.1 for supporting figures.", "description": "This figure demonstrates the impact of initialization on the learning regime of a two-layer ReLU network.  Panel (a) shows training trajectories for different overall and relative scales, highlighting how unbalanced initializations (\u03b4 \u2260 0) can lead to rich learning (rapid feature learning) and good generalization. Panel (b) shows the effect of these parameters on test loss and kernel distance, further illustrating the superiority of unbalanced upstream initializations (\u03b4 > 0) for achieving rapid rich learning.", "section": "1 Introduction"}, {"figure_path": "eNM94i7R3A/figures/figures_1_2.jpg", "caption": "Figure 1: Unbalanced initializations lead to rapid rich learning and generalization. We follow the experimental setup used in Fig. 1 of Chizat et al. [8] \u2013 a wide two-layer student ReLU network f(x; \u03b8) = \u03a3h=1 ai max(0, wTix) trained on a dataset generated from a narrow two-layer teacher ReLU network. The student parameters are initialized as w\u2081 ~ Unif(Sd\u22121(+)) and a\u2081 = \u00b1ar, such that \u03c4 > 0 controls the overall scale of the function, while \u03b4 > 0 controls the relative scale of the first and second layers through the conserved quantity \u03b4 = \u03c4\u00b2(\u03b1\u00b2 \u2212 \u03b1\u2212\u00b2). (a) Shows the training trajectories of |ai|wi (color denotes sgn(ai)) when \u03b4 = 2 for four different settings of \u03c4, \u03b4. The left plot confirms that small overall scale leads to rich and large overall scale to lazy. The right plot shows that even at small overall scale, the relative scale can move the network between rich and lazy as well. Here an upstream initialization \u03b4 > 0 shows striking alignment to the teacher (dotted lines), while a downstream initialization \u03b4 < 0 shows no alignment. (b) Shows the test loss and kernel distance from initialization computed through training over a sweep of \u03c4 and \u03b4 when \u03b4 = 100. Lazy learning happens when \u03c4 is large, rich learning happens when \u03c4 is small, and rapid rich learning happens when both \u03c4 is small and \u03b4 is large \u2013 an upstream initialization. This initialization also leads to the smallest test loss. See Fig. 10 in Appendix D.1 for supporting figures.", "description": "This figure demonstrates how unbalanced initializations affect the learning regime of a two-layer neural network.  Panel (a) shows training trajectories for different overall and relative scales, highlighting how upstream initializations lead to faster learning and better alignment with a teacher network. Panel (b) shows a complex phase portrait of learning regimes in parameter space, illustrating the transition from lazy to rich learning driven by overall scale and the directionality of relative scale (upstream vs. downstream).", "section": "1 Introduction"}, {"figure_path": "eNM94i7R3A/figures/figures_3_1.jpg", "caption": "Figure 2: Balance determines geometry of trajectory. The quantity \u03b4 = nwa\u00b2 \u2013 Na||w||2 is conserved through gradient flow, which constrains the trajectory to: (a) a one-sheeted hyperboloid for downstream initializations, (b) a double cone for balanced initializations, and (c) a two-sheeted hyperboloid for upstream initializations. Gradient flow dynamics for three different initializations ao, wo with the same product \u03b2\u03bf = \u03b1\u03c1wo are shown. The minima manifold is shown in red and the manifold of equivalent \u03b2\u03bf initializations in gray. The surface is colored according to training loss, with blue representing higher loss and red representing lower loss.", "description": "This figure shows how the conserved quantity delta (\u03b4) influences the geometry of the training trajectory in parameter space for a two-layer single-neuron linear network.  The conserved quantity \u03b4 = nwa\u00b2 \u2013 Na||w||2 constrains the trajectory to different shapes depending on its value:\n\n- **\u03b4 < 0 (Downstream):** The trajectory is constrained to a one-sheeted hyperboloid.\n- **\u03b4 = 0 (Balanced):** The trajectory follows a double cone shape.\n- **\u03b4 > 0 (Upstream):** The trajectory is constrained to a two-sheeted hyperboloid.\n\nThe figure illustrates these trajectories for three different initializations (a0, w0) that have the same product (a0*w0),  highlighting how the conserved quantity \u03b4 shapes the geometry of the optimization landscape.", "section": "A Minimal Model of Lazy and Rich Learning with Exact Solutions"}, {"figure_path": "eNM94i7R3A/figures/figures_4_1.jpg", "caption": "Figure 3: Exact solutions for the single hidden neuron model. Our theoretical predictions (black dashed lines) agree with gradient flow simulations (solid lines, color-coded based on \u03b4 values), shown here for three key metrics: \u03bc (left), \u03c6 (middle), and S(0, t) (right). Each metric starts at the same value for all \u03b4, but varying \u03b4 has a pronounced effect on the metric's dynamics. For upstream initializations (\u03b4 > 0), \u03bc changes only slightly, exponentially aligns, and S remains near zero, indicative of the lazy regime. For balanced initializations (\u03b4 = 0), both \u03bc and \u03c6 change significantly and S quickly moves away from zero, indicative of the rich regime. For downstream initializations (\u03b4 < 0), \u03bc quickly drops to zero, then \u03bc and \u03c6 slowly climb back to one. Similarly, S remains small before a sudden transition towards one, indicative of a delayed rich regime. See Appendix A.2 for further details.", "description": "This figure shows the comparison between theoretical predictions and gradient flow simulations for a single hidden neuron model across different values of delta (\u03b4).  The metrics plotted are the signed magnitude \u03bc, alignment \u03c6, and kernel distance S(0, t). The results demonstrate distinct learning dynamics across different \u03b4 regimes, showcasing lazy, rich, and delayed rich learning behaviors.", "section": "A Minimal Model of Lazy and Rich Learning with Exact Solutions"}, {"figure_path": "eNM94i7R3A/figures/figures_5_1.jpg", "caption": "Figure 4: Balance modulates \u03b2 dynamics and implicit bias. Here we show the dynamics of \u03b2 = aw with different values of \u03b4, but the same initial \u03b20. When XTX is whitened (left), we can solve for the dynamics exactly using our expressions for \u03bc, \u03c6 (black dashed lines). Upstream initializations follow the trajectory of gradient flow on \u03b2, downstream initializations first move in the direction of \u03b20 before sweeping around towards \u03b2*, and balanced initializations take an intermediate trajectory between these two. When XTX is low-rank (right), then we can only predict the trajectories in the limit of \u03b4 = \u00b1\u221e. If the interpolating manifold is one-dimensional, then we can solve for the solution in terms of \u03b4 exactly (black dots). See Appendix A.4 for details.", "description": "This figure shows how the conserved quantity  \u03b4 influences the learning trajectories in function space.  The left panel shows the trajectories for a whitened input matrix (XTX = I), where exact solutions can be derived. The right panel shows the trajectories for a low-rank input matrix, where exact solutions are only possible in the limit of  \u03b4  approaching positive or negative infinity.  Different initializations (upstream, downstream, and balanced) lead to distinct learning behaviors.", "section": "A Minimal Model of Lazy and Rich Learning with Exact Solutions"}, {"figure_path": "eNM94i7R3A/figures/figures_8_1.jpg", "caption": "Figure 5: Rapid feature learning is caused by large activation changes with minimal parameter movement. (a) We show the surface of a two-layer ReLU network trained on an XOR-like task, starting with a near-zero input-output map, f(x; \u03b80) \u2248 0. The surface consists of convex conic regions, each with a distinct activation pattern, colored by the parity of active neurons. A lazy initialization (bottom) maintains a fixed activation partition throughout training, reweighting the hidden neurons to fit the data. In contrast, a rich balanced or upstream initialization (top) features an initial alignment phase where the partition map changes rapidly while the input-output map remains close to zero, followed by a data-fitting phase. (b) We show the evolution of Hamming distance in activation patterns and parameter distance, relative to t = 0, as a function of overall and relative scales (same experiments as in Fig. 1(b)). Rapid feature learning occurs from a small-\u03c4 upstream initialization that promotes faster learning in early layers, driving a large change in Hamming distance, but a small change in parameter space.", "description": "This figure demonstrates the difference between lazy and rich learning regimes in a two-layer ReLU network trained on an XOR-like dataset.  Panel (a) visually shows how the input-output map's surface changes over time, revealing different activation patterns. Lazy learning retains the same activation pattern, while rich learning shows dynamic changes in activation patterns. Panel (b) quantitatively supports these observations by showing that rich learning involves large changes in activation patterns (Hamming distance) with only small changes in parameters, while lazy learning shows smaller changes in both.", "section": "5 Piecewise Linear Networks"}, {"figure_path": "eNM94i7R3A/figures/figures_9_1.jpg", "caption": "Figure 6: Impact of upstream initializations in practice. Here we provide evidence that an upstream initialization (a) drives feature learning through changing activation patterns, (b) promotes interpretability of early layers in CNNs, (c) reduces the sample complexity of learning hierarchical data, and (d) decreases the time to grokking in modular arithmetic. In these experiments, we regulate the first layer's learning speed relative to the rest of the network by dividing its initialization by a. For models without normalization layers, we also scale the last layer's initialization by a to preserve the input-output map. a = 1 represents standard parameterization, while a \u226b 1 and a \u226a 1 correspond to upstream and downstream initializations, respectively. See Appendix D.3 for details.", "description": "This figure shows experimental results supporting the claim that upstream initializations (where the first layer learns faster than subsequent layers) improve feature learning in various deep learning models.  Subfigure (a) shows kernel distance, measuring the change in the model's learned kernel, demonstrating that upstream initialization leads to more significant changes. Subfigure (b) presents convolutional filter visualizations, showing that upstream initialization leads to simpler, more interpretable filters in the early layers of a CNN. Subfigure (c) illustrates that upstream initialization reduces the sample complexity needed to learn hierarchical data, while subfigure (d) shows that it reduces the time until successful generalization is achieved (a phenomenon known as \"grokking\").", "section": "Unbalanced initializations drive rapid feature learning in practice"}, {"figure_path": "eNM94i7R3A/figures/figures_19_1.jpg", "caption": "Figure 2: Balance determines geometry of trajectory. The quantity \u03b4 = nwa\u00b2 \u2013 Na||w||2 is conserved through gradient flow, which constrains the trajectory to: (a) a one-sheeted hyperboloid for downstream initializations, (b) a double cone for balanced initializations, and (c) a two-sheeted hyperboloid for upstream initializations. Gradient flow dynamics for three different initializations ao, wo with the same product \u03b2\u03bf = \u03b1\u03c1wo are shown. The minima manifold is shown in red and the manifold of equivalent \u03b2\u03bf initializations in gray. The surface is colored according to training loss, with blue representing higher loss and red representing lower loss.", "description": "This figure shows how the conserved quantity delta (\u03b4) influences the geometry of the training trajectory in parameter space.  Different values of \u03b4 correspond to different geometric shapes (hyperboloids or cones), each constraining the learning dynamics to specific regions. The color-coding of the surface indicates the training loss at different points in parameter space, showing how different initializations and trajectories lead to varying levels of loss. The figure visually represents the transition between lazy and rich learning regimes by showing how the geometry of the trajectory changes based on the initial conditions. This is a key illustration of how the conserved quantity affects the training dynamics. ", "section": "A Minimal Model of Lazy and Rich Learning with Exact Solutions"}, {"figure_path": "eNM94i7R3A/figures/figures_20_1.jpg", "caption": "Figure 3: Exact solutions for the single hidden neuron model. Our theoretical predictions (black dashed lines) agree with gradient flow simulations (solid lines, color-coded based on \u03b4 values), shown here for three key metrics: \u03bc (left), \u03c6 (middle), and S(0, t) (right). Each metric starts at the same value for all \u03b4, but varying \u03b4 has a pronounced effect on the metric's dynamics. For upstream initializations (\u03b4 > 0), \u03bc changes only slightly, exponentially aligns, and S remains near zero, indicative of the lazy regime. For balanced initializations (\u03b4 = 0), both \u03bc and \u03c6 change significantly and S quickly moves away from zero, indicative of the rich regime. For downstream initializations (\u03b4 < 0), \u03bc quickly drops to zero, then \u03bc and \u03c6 slowly climb back to one. Similarly, S remains small before a sudden transition towards one, indicative of a delayed rich regime. See Appendix A.2 for further details.", "description": "This figure shows the comparison between theoretical predictions and gradient flow simulations for three key metrics (\u03bc, \u03c6, and S(0,t)).  The dynamics of these metrics are shown for different values of \u03b4 (relative scale), which determines the learning regime (lazy, rich, or delayed rich). Upstream initializations (\u03b4>0) show minimal changes in \u03bc, rapid alignment in \u03c6, and S remains near zero (lazy). Balanced initializations (\u03b4=0) exhibit significant changes in both \u03bc and \u03c6, and S quickly increases (rich). Downstream initializations (\u03b4<0) initially show a rapid decrease in \u03bc, followed by slow increases in both \u03bc and \u03c6, and a delayed increase in S (delayed rich).", "section": "A Minimal Model of Lazy and Rich Learning with Exact Solutions"}, {"figure_path": "eNM94i7R3A/figures/figures_34_1.jpg", "caption": "Figure 5: Rapid feature learning is caused by large activation changes with minimal parameter movement. (a) We show the surface of a two-layer ReLU network trained on an XOR-like task, starting with a near-zero input-output map, f(x;\u03b80) \u2248 0. The surface consists of convex conic regions, each with a distinct activation pattern, colored by the parity of active neurons. A lazy initialization (bottom) maintains a fixed activation partition throughout training, reweighting the hidden neurons to fit the data. In contrast, a rich balanced or upstream initialization (top) features an initial alignment phase where the partition map changes rapidly while the input-output map remains close to zero, followed by a data-fitting phase. (b) We show the evolution of Hamming distance in activation patterns and parameter distance, relative to t = 0, as a function of overall and relative scales (same experiments as in Fig. 1(b)). Rapid feature learning occurs from a small-\u03c4 upstream initialization that promotes faster learning in early layers, driving a large change in Hamming distance, but a small change in parameter space.", "description": "This figure shows that rapid feature learning is driven by large changes in the activation patterns with minimal parameter changes.  Two-layer ReLU networks are trained on an XOR-like task, starting from an almost-zero input-output map.  The top row shows the impact of a balanced or upstream initialization; an initial rapid change in activation patterns with minimal changes to the input-output map, followed by a data-fitting phase. The bottom row illustrates a lazy initialization, where the activation partition remains static throughout training, and the network simply reweights the neurons to fit the data. The right panel (b) further quantifies this by plotting the Hamming distance (changes in activation patterns) and parameter distance as a function of overall and relative scales, demonstrating that unbalanced initializations, especially upstream, lead to fast rich learning.", "section": "5 Piecewise Linear Networks"}, {"figure_path": "eNM94i7R3A/figures/figures_37_1.jpg", "caption": "Figure 1: Unbalanced initializations lead to rapid rich learning and generalization. We follow the experimental setup used in Fig. 1 of Chizat et al. [8] \u2013 a wide two-layer student ReLU network f(x; \u03b8) = \u03a3h=1 ai max(0, wix) trained on a dataset generated from a narrow two-layer teacher ReLU network. The student parameters are initialized as w\u2081 ~ Unif(Sd\u22121(+)) and a\u2081 = \u00b1ar, such that \u03c4 > 0 controls the overall scale of the function, while \u03b1 > 0 controls the relative scale of the first and second layers through the conserved quantity \u03b4 = \u03c4\u00b2(\u03b1\u00b2 \u2013 \u03b1\u00af\u00b2). (a) Shows the training trajectories of |ai|wi (color denotes sgn(ai)) when \u03b4 = 2 for four different settings of \u03c4, \u03b4. The left plot confirms that small overall scale leads to rich and large overall scale to lazy. The right plot shows that even at small overall scale, the relative scale can move the network between rich and lazy as well. Here an upstream initialization \u03b4 > 0 shows striking alignment to the teacher (dotted lines), while a downstream initialization \u03b4 < 0 shows no alignment. (b) Shows the test loss and kernel distance from initialization computed through training over a sweep of \u03c4 and \u03b4 when \u03b4 = 100. Lazy learning happens when \u03c4 is large, rich learning happens when \u03c4 is small, and rapid rich learning happens when both \u03c4 is small and \u03b4 is large \u2013 an upstream initialization. This initialization also leads to the smallest test loss. See Fig. 10 in Appendix D.1 for supporting figures.", "description": "This figure demonstrates the impact of unbalanced initializations on the learning dynamics of a two-layer neural network. Panel (a) shows how different overall scales and relative scales between layers affect the training trajectories, leading to either lazy or rich learning regimes. Panel (b) illustrates the relationship between overall scale, relative scale, test loss, and kernel distance, revealing that a small overall scale and large relative scale (upstream initialization) lead to rapid rich learning and good generalization.", "section": "1 Introduction"}, {"figure_path": "eNM94i7R3A/figures/figures_38_1.jpg", "caption": "Figure 6: Impact of upstream initializations in practice. Here we provide evidence that an upstream initialization (a) drives feature learning through changing activation patterns, (b) promotes interpretability of early layers in CNNs, (c) reduces the sample complexity of learning hierarchical data, and (d) decreases the time to grokking in modular arithmetic. In these experiments, we regulate the first layer's learning speed relative to the rest of the network by dividing its initialization by a. For models without normalization layers, we also scale the last layer's initialization by a to preserve the input-output map. a = 1 represents standard parameterization, while a \u226b 1 and a \u226a 1 correspond to upstream and downstream initializations, respectively. See Appendix D.3 for details.", "description": "This figure shows experimental results supporting the claim that upstream initialization promotes rapid feature learning.  It demonstrates the impact of unbalanced initializations on four different aspects: feature learning via activation pattern changes, interpretability of early convolutional layers, sample complexity in hierarchical data learning, and the speed of \"grokking\" in modular arithmetic.  In all cases, the effect is achieved by modifying the initialization variance of early layers relative to later ones. ", "section": "Unbalanced initializations drive rapid feature learning in practice"}, {"figure_path": "eNM94i7R3A/figures/figures_39_1.jpg", "caption": "Figure 6: Impact of upstream initializations in practice. Here we provide evidence that an upstream initialization (a) drives feature learning through changing activation patterns, (b) promotes interpretability of early layers in CNNs, (c) reduces the sample complexity of learning hierarchical data, and (d) decreases the time to grokking in modular arithmetic. In these experiments, we regulate the first layer's learning speed relative to the rest of the network by dividing its initialization by a. For models without normalization layers, we also scale the last layer's initialization by a to preserve the input-output map. a = 1 represents standard parameterization, while a \u226b 1 and a \u226a 1 correspond to upstream and downstream initializations, respectively. See Appendix D.3 for details.", "description": "This figure demonstrates the impact of upstream initializations on various aspects of deep learning.  It shows that upstream initializations (where the first layer learns faster than subsequent layers) lead to faster feature learning, improved interpretability of early layers in convolutional neural networks (CNNs), reduced sample complexity in learning hierarchical data, and faster 'grokking' (sudden generalization) in modular arithmetic tasks.  The experiments involve scaling the initialization of the first (and last) layer to control the relative learning speed.  Subfigures illustrate these impacts using different metrics: kernel distance, convolutional filter visualizations, random hierarchy model results, and transformer grokking results.", "section": "Unbalanced initializations drive rapid feature learning in practice"}]