[{"type": "text", "text": "Dimension-free deterministic equivalents and scaling laws for random feature regression ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Leonardo Defilippis ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Bruno Loureiro ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "D\u00e9partement d\u2019Informatique \u00c9cole Normale Sup\u00e9rieure - PSL & CNRS leonardo.defilippis@ens.psl.eu ", "page_idx": 0}, {"type": "text", "text": "D\u00e9partement d\u2019Informatique \u00c9cole Normale Sup\u00e9rieure - PSL & CNRS bruno.loureiro@ens.psl.eu ", "page_idx": 0}, {"type": "text", "text": "Theodor Misiakiewicz Department of Statistics and Data Science Yale University theodor.misiakiewicz@yale.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this work we investigate the generalization performance of random feature ridge regression (RFRR). Our main contribution is a general deterministic equivalent for the test error of RFRR. Specifically, under a certain concentration property, we show that the test error is well approximated by a closed-form expression that only depends on the feature map eigenvalues. Notably, our approximation guarantee is non-asymptotic, multiplicative, and independent of the feature map dimension\u2014 allowing for infinite-dimensional features. We expect this deterministic equivalent to hold broadly beyond our theoretical analysis, and we empirically validate its predictions on various real and synthetic datasets. As an application, we derive sharp excess error rates under standard power-law assumptions of the spectrum and target decay. In particular, we provide a tight result for the smallest number of features achieving optimal minimax error rate. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "At odds with classical statistical intuition, overparametrized neural networks are able to generalize while perfectly interpolating the training data. This observation, which defies the canonical mathematical understanding of generalization based on complexity measures and uniform convergence, appears surprising at first [Zhang et al., 2017]. However, recent progress in our mathematical understanding of generalization has taught us that this benign overftiting property of overparametrized neural networks is shared by a plethora of simpler learning tasks [Bartlett et al., 2021, Belkin, 2021]. Among them, the investigation of the following class of random feature models has been at the forefront of this progress: ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\mathcal{F}_{\\mathrm{RF}}=\\Big\\{\\hat{f}(\\pmb{x};\\pmb{a})=\\frac{1}{\\sqrt{p}}\\sum_{j\\in[p]}a_{j}\\varphi(\\pmb{x},\\pmb{w}_{j})\\ :\\ \\pmb{a}=(a_{j})_{j\\in[p]}\\in\\mathbb{R}^{p}\\Big\\}.\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "Here $\\textbf{\\em x}\\in\\mathcal{X}$ denotes the inputs and $W=({\\pmb w}_{j})_{j\\in[p]}$ a set of weight vectors which are taken to be random $\\pmb{w}_{j}\\;\\in\\;\\mathcal{W}\\;\\subseteq\\;\\mathbb{R}^{d}\\;\\sim_{\\mathrm{iid}}\\;\\mu_{w}$ . Hence, as the name suggests the feature map $\\varphi:\\mathcal{X}\\times$ $\\mathcal{W}\\to\\mathbb{R}$ defines a random function. A few examples of random feature maps include the fully connected neural network features $\\varphi(\\pmb{x},\\pmb{w})\\;=\\;\\sigma(\\bar{\\langle}\\pmb{w},\\pmb{x}))$ , $\\sigma:\\mathbb{R}\\,\\rightarrow\\,\\mathbb{R}$ , and the convolutional features with global average pooling $\\begin{array}{r}{\\varphi(\\pmb{x},\\pmb{w})\\,=\\,1/{d}\\sum_{\\ell=1}^{d}{\\sigma(\\langle\\pmb{w},g_{\\ell}\\cdot\\pmb{x}\\rangle)}}\\end{array}$ where $\\sigma:\\mathbb{R}\\rightarrow\\mathbb{R}$ and $g_{\\ell}\\cdot\\pmb{x}=(x_{\\ell+1},\\ldots,x_{d},x_{1},\\ldots,x_{\\ell})$ is the $\\ell$ -shift operator with cyclic boundary conditions (both with $\\boldsymbol{\\mathcal{X}}=\\mathbb{R}^{d}$ ). ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Random features [Balcan et al., 2006, Rahimi and Recht, 2007] were originally introduced as a computationally efficient approximation for the limiting kernel: ", "page_idx": 1}, {"type": "equation", "text": "$$\nK(\\pmb{x},\\pmb{x}^{\\prime})=\\mathbb{E}_{\\pmb{w}\\sim\\mu_{w}}\\left[\\varphi(\\pmb{x},\\pmb{w})\\varphi(\\pmb{x}^{\\prime},\\pmb{w})\\right].\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Although this can reduce the computational cost of kernel methods, it introduces an approximation error. Rahimi and Recht [2008] showed that in a supervised setting with $n$ samples, $p\\,=\\,O(n)$ features are sufficient to achieve an excess risk $O(n^{-1/2})$ . Rudi and Rosasco [2017] improved this result under standard power-law assumptions on the asymptotic kernel spectrum, showing that for fast decays, less features are needed to achieve the minimax rate. In Section 4 we will revisit this question, where we will derive a tight result for the minimum number of features. ", "page_idx": 1}, {"type": "text", "text": "More recently, the random feature model has gained in popularity as a proxy model for studying the generalization properties of two-layer neural networks in the lazy regime of training [Jacot et al., 2018, Chizat et al., 2019]. Indeed, for particular choices of feature maps such as $\\varphi(\\bar{\\boldsymbol{\\mathbf{x}}},\\boldsymbol{\\mathbf{w}})=\\sigma(\\langle\\boldsymbol{\\mathbf{w}},\\boldsymbol{\\mathbf{x}}\\rangle)$ , it can also be seen as a two-layer neural network with fixed first-layer weights. Exact asymptotic results for the generalization error of eq. (1) were derived for different supervised learning tasks under the proportional scaling regime $n,p=\\Theta(d)$ in [Mei and Montanari, 2022, Gerace et al., 2021, Dhifallah and Lu, 2020, Hu and Lu, 2023, Goldt et al., 2022, Loureiro et al., 2022, Bosch et al., 2023b,a, Schr\u00f6der et al., 2023, 2024] and under more general polynomial scaling $n,p=\\Theta(d^{\\kappa})$ in [Simon et al., 2023b, Aguirre-L\u00f3pez et al., 2024, Hu et al., 2024]. As discussed above, these works played a fundamental role in our current mathematical understanding of the relationship between overparametrization and generalization, demystifying different phenomena such as double descent Belkin et al. [2019] and benign overftiting Bartlett et al. [2020]. It also led to fundamental separation results between lazy and trained networks [Ghorbani et al., 2019, 2020, Mei et al., 2022], recently motivating the investigation of corrections to the random limit [Ba et al., 2022, Dandi et al., 2023, Moniri et al., 2024, Cui et al., 2024]. ", "page_idx": 1}, {"type": "text", "text": "With the exception of [Simon et al., 2023b], which is based on non-rigorous arguments, the results in all the works cited above are derived in the asymptotic limit of large data dimension. However, the relative scaling of the $n,p,d$ is fundamentally artificial, and in practice it is hard to unambiguously define the regime of interest. Our main goal in this manuscript is to provide a dimension-free characterization of the generalization error allowing us to give tight answers to questions which cannot be addressed asymptotically. More precisely, our main contributions are: ", "page_idx": 1}, {"type": "text", "text": "(i) Under a concentration assumption on the feature map eigenfunctions, we prove a non-asymptotic deterministic approximation for the RFRR risk $\\mathcal{R}_{\\mathrm{test}}\\approx\\mathsf{R}_{n,p}$ which is independent of the feature map dimension. More precisely, with high-probability over the input data and random weights: ", "page_idx": 1}, {"type": "equation", "text": "$$\n|\\mathcal{R}_{\\mathrm{test}}-\\mathsf{R}_{n,p}|\\leq\\tilde{O}(p^{-1/2}+n^{-1/2})\\cdot\\mathsf{R}_{n,p}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where the deterministic equivalent $\\mathsf{R}_{n,p}$ can be computed by solving a set of self-consistent equations of the type $\\ x\\,=\\,f(x)$ , with $f$ a contractive map. This result unifies the long list of asymptotic formulas in the RFRR literature, and proves a conjecture by Simon et al. [2023b]. We numerically validate the results on various real and synthetic datasets. The precise statement of the theorem and the assumptions are discussed in Section 3. ", "page_idx": 1}, {"type": "text", "text": "(ii) Leveraging our formula, we investigate the error scaling laws in a setting where the target function and feature spectrum decay as a power-law, also known as source and capacity conditions. We provide a full picture of the different scaling regimes and the cross-overs between them, summarized in Figure 2. Our result is closely related to the neural scaling laws literature [Kaplan et al., 2020], and provides the first rigorous, non-linear extension of [Bahri et al., 2024, Maloney et al., 2022]. ", "page_idx": 1}, {"type": "text", "text": "(iii) We provide a sharp expression for the minimum number of features required to achieve the minimax optimal decay rate of Caponnetto and De Vito [2007], closing the gap of previous lower-bounds in the literature [Rudi and Rosasco, 2017]. ", "page_idx": 1}, {"type": "text", "text": "Further related works \u2014 Deterministic equivalents have been derived for a wide range of learning problems, such as ridge regression [Dobriban and Wager, 2018, Hastie et al., 2022, Cheng and Montanari, 2022, Wei et al., 2022], kernel regression [Misiakiewicz and Saeed, 2024], shallow [Liao and Couillet, 2018, Mei and Montanari, 2022, Chouard, 2022, Bach, 2024, Atanasov et al., 2024] and deep random feature regression [Fan and Wang, 2020, Schr\u00f6der et al., 2023, 2024, Chouard, 2023] and spiked random features [Wang et al., 2024]. Scaling laws under source and capacity conditions were studied by several authors in the context of kernel ridge regression [Bordelon et al., 2020, Spigler et al., 2020, Cui et al., 2022, Simon et al., 2023a, Li et al., 2023, Misiakiewicz and Mei, 2022, Favero et al., 2021, Cagnetta et al., 2023, Dohmatob et al., 2024] and classification [Cui et al., 2023]. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Setting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this work, we focus on the generalization properties of the random feature class ${\\mathcal{F}}_{\\mathrm{RF}}$ defined in eq. (1) in a supervised regression setting. More precisely, consider a data set $\\boldsymbol{\\mathcal{D}}=\\{(\\boldsymbol{x}_{i},y_{i})_{i\\in[n]}\\}$ composed of $n$ independent and identically distributed samples from a joint distribution $\\mu_{x,y}$ on $\\mathcal{X}\\times\\mathbb{R}$ . Let $f_{\\star}({\\pmb x})=\\mathbb{E}[y|{\\pmb x}]$ denote the target function. We assume $f_{\\star}\\in L_{2}(\\mu_{x})$ , where $\\mu_{x}$ is the marginal distribution over $\\mathcal{X}$ . Moreover, we assume the noise $\\varepsilon:=y-f_{\\star}(x)$ has zero mean and finite variance $\\mathbb{E}[\\varepsilon^{2}]=\\sigma_{\\varepsilon}^{2}<\\infty$ . Note this is equivalent to: ", "page_idx": 2}, {"type": "equation", "text": "$$\ny_{i}=f_{\\star}(\\pmb{x}_{i})+\\varepsilon_{i},\\qquad\\quad f_{\\star}\\in L_{2}(\\mu_{x}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Given the training data, we are interested in the properties of the minimiser: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{a}_{\\lambda}(Z,y):=\\underset{a\\in\\mathbb{R}^{p}}{\\operatorname{arg\\,min}}\\,\\Big\\lbrace\\,\\sum_{i\\in[n]}\\Big(y_{i}-\\hat{f}(\\pmb{x}_{i};\\pmb{a})\\Big)^{2}+\\lambda\\|\\pmb{a}\\|_{2}^{2}\\Big\\rbrace=(Z^{\\top}Z+\\lambda I_{p})Z^{\\top}y,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where we have defined the feature matrix $Z_{i j}=p^{-1/2}\\varphi(\\pmb{x}_{i};\\pmb{w}_{j})$ and the label vectors $\\pmb{y}=(y_{i})_{i\\in[n]}$ In particular, we are interested in its capacity of generalising to unseen data, as quantified by the excess population risk: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{R}(f_{\\star},X,W,\\varepsilon,\\lambda):=\\mathbb{E}_{x\\sim\\mu_{x}}\\Big[\\Big(f_{\\star}(x)-\\hat{f}(x;\\hat{a}_{\\lambda})\\Big)^{2}\\Big].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "It will be convenient to decompose the excess risk above in terms of the standard bias and variance: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{R}(f_{\\star};X,W,\\lambda):=\\mathbb{E}_{\\varepsilon}\\left[\\mathcal{R}(f_{\\star};X,W,\\varepsilon,\\lambda)\\right]=\\mathcal{B}(f_{\\star};X,W,\\lambda)+\\mathcal{V}(X,W,\\lambda),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathcal{B}}(f_{\\star};X,W,\\lambda):=\\mathbb{E}_{x\\sim\\mu_{x}}\\left[\\left(f_{\\star}(x)-\\mathbb{E}_{\\varepsilon}[\\hat{f}(x;\\hat{a}_{\\lambda})]\\right)^{2}\\right],}\\\\ &{\\quad\\ V(X,W,\\lambda):=\\mathbb{E}_{x\\sim\\mu_{x}}\\left[\\mathrm{Var}_{\\varepsilon}(\\hat{f}(x;\\hat{a}_{\\lambda}))\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Note that to simplify the exposition, we have explicitly taken an expectation over the training data noise $\\boldsymbol{\\varepsilon}=(\\varepsilon_{i})_{i\\in[n]}$ . Indeed, it can be shown that the excess risk eq. (6) concentrates on its expectation over $\\varepsilon$ under mild assumptions (see for example Misiakiewicz and Saeed [2024]). ", "page_idx": 2}, {"type": "text", "text": "3 Deterministic equivalents ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The excess risk eq. (6) is a function of the covariates $\\mathbf{\\deltaX}$ and the weights $W$ , and therefore it is a random quantity. Our main result in what follows is a sharp characterization of the bias and variance in terms of a deterministic equivalent depending only on the model parameters and spectral properties of the features. Consider a square-integrable $\\varphi\\in\\dot{L}_{2}(\\mathcal{X}\\times\\mathcal{W})$ , and define the Fredholm integral operator $\\mathbb{T}:L_{2}(\\mathscr{X})\\to\\mathscr{V}\\subseteq L_{2}(\\mathscr{W})$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{T}h(\\pmb{w}):=\\int_{\\mathcal{X}}\\varphi(\\pmb{x};\\pmb{w})h(\\pmb{x})\\mu_{x}(\\mathrm{d}\\pmb{x}),\\qquad\\quad\\forall h\\in L_{2}(\\mathcal{X}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where we define $\\mathcal{V}=\\mathrm{Im}(\\mathbb{T})$ . This is a compact operator, and therefore can be diagonalized: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{T}=\\sum_{k=1}^{\\infty}\\xi_{k}\\psi_{k}\\boldsymbol{\\phi}_{k}^{*},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $(\\xi_{k})_{k\\geq1}\\subseteq\\mathbb{R}$ are the eigenvalues and $(\\psi_{k})_{k\\geq1}$ and $(\\phi_{k})_{k\\geq1}$ are orthonormal bases of $L_{2}(\\mathcal{X})$ and $\\nu$ respectively: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\langle\\psi_{k},\\psi_{k^{\\prime}}\\rangle_{L_{2}(\\mathcal{X})}=\\delta_{k k^{\\prime}},\\qquad\\quad\\langle\\phi_{k},\\phi_{k^{\\prime}}\\rangle_{L_{2}(\\mathcal{W})}=\\delta_{k k^{\\prime}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Without loss of generality, we assume the eigenvalues are ordered in non-increasing absolute values $|\\xi_{1}|\\geq|\\xi_{2}|\\geq\\ldots$ , and for simplicity of presentation we assume that all eigenvalues are non-zero, i.e., $\\ker(\\mathbb{T})\\,=\\,\\{0\\}$ . Denote $\\begin{array}{r}{\\bar{\\Sigma^{\\dag}}=\\operatorname{diag}(\\bar{\\xi}_{1}^{2},\\xi_{2}^{2},\\dots)\\in\\mathbb{R}^{\\infty\\times\\infty}}\\end{array}$ the diagonal matrix of the squared eigenvalues. Similarly, since $f_{\\star}\\in L_{2}(\\mu_{x})$ , it admits the following decomposition in $(\\psi)_{k\\geq1}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\nf_{\\star}=\\sum_{k\\ge1}\\beta_{\\star,k}\\psi_{k}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Our formal results will assume the following concentration property over the eigenfunctions. ", "page_idx": 3}, {"type": "text", "text": "Assumption 3.1 (Concentration of the eigenfunctions). Denote the (infinite-dimensional) random vectors1 $\\pmb{\\psi}:=(\\xi_{k}\\psi_{k}(\\pmb{x}))_{k\\geq1}$ and $\\phi:=(\\bar{\\xi_{k}}\\phi_{k}({\\pmb w}))_{k\\geq1}$ . There exists a constant $\\mathsf C_{x}>0$ such that for any deterministic p.s.d. matrix $\\boldsymbol{A}\\in\\mathbb{R}^{\\infty\\times\\infty}$ , i.e. a linear operator acting on an infinite-dimensional Hilbert space, with $\\mathrm{Tr}({\\pmb{\\Sigma}}{\\pmb{A}})<\\infty$ , we have ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb P\\left(\\left|\\psi^{\\top}A\\psi-\\operatorname{Tr}(\\Sigma A)\\right|\\geq t\\cdot\\|\\Sigma^{1/2}A\\Sigma^{1/2}\\|_{F}\\right)\\leq\\mathsf C_{x}\\exp\\left\\{-t/\\mathsf C_{x}\\right\\},}\\\\ &{\\mathbb P\\left(\\left|\\phi^{\\top}A\\phi-\\operatorname{Tr}(\\Sigma A)\\right|\\geq t\\cdot\\|\\Sigma^{1/2}A\\Sigma^{1/2}\\|_{F}\\right)\\leq\\mathsf C_{x}\\exp\\left\\{-t/\\mathsf C_{x}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "While Assumption 3.1 is restrictive and will not be satisfied by many non-linear settings, it covers a number of popular theoretical models studied in the literature: 1) independent sub-Gaussian entries, 2) verifying a log-Sobolev inequality or convex Lipschitz concentration (see Cheng and Montanari [2022]). We expect that Assumption 3.1 can be relaxed using the same procedure as in Misiakiewicz and Saeed [2024] to cover classical examples such as data and weights uniformly distributed on the sphere or hypercube. Such a relaxation is involved and we leave it to future work. We will further assume that: ", "page_idx": 3}, {"type": "text", "text": "Assumption 3.2. There exists $\\mathsf{m}\\in\\mathbb{N}$ such that ", "page_idx": 3}, {"type": "equation", "text": "$$\np\\xi_{\\mathfrak{m}+1}^{2}\\leq\\frac{\\lambda}{n}\\sum_{k=\\mathfrak{m}+1}^{\\infty}\\xi_{k}^{2}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Furthermore, we will assume that for some $\\mathsf C_{*}>0$ that we have ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{Tr}(\\Sigma(\\Sigma+\\nu_{2})^{-1})}{\\mathrm{Tr}(\\Sigma^{2}(\\Sigma+\\nu_{2})^{-2})}\\leq\\mathsf C_{*},\\qquad\\quad\\frac{\\langle\\beta_{\\star},(\\Sigma+\\nu_{2})^{-1}\\beta_{\\star}\\rangle}{\\nu_{2}\\langle\\beta_{\\star},(\\Sigma+\\nu_{2})^{-2}\\beta_{\\star}\\rangle}\\leq\\mathsf C_{*}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Assumption 3.2 is technical, and we believe it can be removed at the cost of a more involved analysis. For instance, eq. (16) is always satisfied for $\\xi_{k}^{2}\\propto k^{-\\alpha}$ if we take $\\mathsf{m}=O(p^{2})$ . Condition (17) was also considered in Cheng and Montanari [2022], and is satisfied in many settings of interest, for example under source and capacity conditions $\\bar{\\beta_{k}}\\asymp k^{-\\beta}$ and $\\xi_{k}^{2}\\asymp k^{-\\alpha}$ considered in Section 4. ", "page_idx": 3}, {"type": "text", "text": "Main result \u2014 Our main result concerns a dimension-free characterization of the risk eq. (6) in terms of deterministic equivalents. We start by defining them. ", "page_idx": 3}, {"type": "text", "text": "Definition 1 (Deterministic equivalents). Given integers $n,p$ , covariance matrix $\\Sigma$ and regularization parameter $\\lambda\\geq0$ . Consider the parameter $\\nu_{2}\\,\\in\\,\\mathbb{R}_{>0}$ defined as the unique solution of the selfconsistent equation: ", "page_idx": 3}, {"type": "equation", "text": "$$\n1+\\frac{n}{p}-\\sqrt{\\left(1-\\frac{n}{p}\\right)^{2}+4\\frac{\\lambda}{p\\nu_{2}}}=\\frac{2}{p}\\mathrm{Tr}\\left(\\Sigma(\\Sigma+\\nu_{2})^{-1}\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and $\\nu_{1}\\in\\mathbb{R}_{>0}$ is given by: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\nu_{1}:=\\frac{\\nu_{2}}{2}\\left[1-\\frac{n}{p}+\\sqrt{\\left(1-\\frac{n}{p}\\right)^{2}+4\\frac{\\lambda}{p\\nu_{2}}}\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We introduce the short-hand: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Upsilon(\\nu_{1},\\nu_{2}):=\\frac{p}{n}\\left[\\left(1-\\frac{\\nu_{1}}{\\nu_{2}}\\right)^{2}+\\left(\\frac{\\nu_{1}}{\\nu_{2}}\\right)^{2}\\frac{\\mathrm{Tr}(\\Sigma^{2}(\\Sigma+\\nu_{2})^{-2})}{p-\\mathrm{Tr}(\\Sigma^{2}(\\Sigma+\\nu_{2})^{-2})}\\right],}\\\\ &{\\quad\\quad\\chi(\\nu_{2}):=\\frac{\\mathrm{Tr}(\\Sigma(\\Sigma+\\nu_{2})^{-2})}{p-\\mathrm{Tr}(\\Sigma^{2}(\\Sigma+\\nu_{2})^{-2})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Then, the deterministic equivalents for the bias, variance and test error are defined as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{B}_{n,p}(\\beta_{*},\\lambda):=\\displaystyle\\frac{\\nu_{2}^{2}}{1-\\Upsilon(\\nu_{1},\\nu_{2})}\\Big[\\langle\\beta_{*},(\\Sigma+\\nu_{2})^{-2}\\beta_{*}\\rangle+\\chi(\\nu_{2})\\langle\\beta_{*},\\Sigma(\\Sigma+\\nu_{2})^{-2}\\beta_{*}\\rangle\\Big],}\\\\ &{\\quad\\vee_{n,p}(\\lambda):=\\sigma_{\\varepsilon}^{2}\\displaystyle\\frac{\\Upsilon(\\nu_{1},\\nu_{2})}{1-\\Upsilon(\\nu_{1},\\nu_{2})},}\\\\ &{\\mathsf{R}_{n,p}(\\beta_{*},\\lambda):=\\mathsf{B}_{n,p}(\\beta_{*},\\lambda)+\\mathsf{V}_{n,p}(\\lambda).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Our main result provides precise conditions for when the deterministic equivalents defined in definition 1 are a good approximation for the test error eq. (6), as a function of the dimensions $n,p$ , feature covariance $\\Sigma$ , and regularization $\\lambda>0$ . More precisely, the approximation rates will depend on them through the following quantities: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{r_{\\Sigma}(k):=\\frac{\\sum_{j\\geq k}\\xi_{j}^{2}}{\\xi_{k}^{2}},\\quad M_{\\Sigma}(k):=1+\\frac{r_{\\Sigma}\\big(\\big\\lfloor\\eta_{*}\\cdot k\\big\\rfloor\\big)\\vee k}{k}\\log\\left(r_{\\Sigma}(\\lfloor\\eta_{*}\\cdot k\\rfloor)\\vee k\\right),}\\\\ &{\\rho_{\\kappa}(p):=1+\\frac{p\\cdot\\xi_{\\lfloor\\eta_{*}\\cdot p\\rfloor}^{2}}{\\kappa}M_{\\Sigma}(p),}\\\\ &{\\tilde{\\rho}_{\\kappa}(n,p):=1+\\mathbb{1}[n\\leq p/\\eta_{*}]\\cdot\\left\\{\\frac{n\\xi_{\\lfloor\\eta_{*}\\cdot n\\rfloor}^{2}}{\\kappa}+\\frac{n}{p}\\cdot\\rho_{\\kappa}(p)\\right\\}M_{\\Sigma}(n),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Below we denote $C_{a_{1},\\ldots,a_{k}}$ constants that only depend on the values of $\\{a_{i}\\}_{i\\in[k]}$ . We use $a_{i}=\\,^{\\circ}\\ast^{\\prime}$ to denote the dependency on the constants in Assumptions 3.1 and 3.2. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.3 (Test error of RFRR). Under Assumptions 3.1, 3.2 and for any $D,K>0,$ , there exist constants $\\eta_{*}\\,\\in\\,(0,1/2)$ and $C_{*,D,K}\\,>\\,0$ such that the following holds. For any $n,p\\ge C_{*,D,K}$ , regularization $\\lambda>0$ , and target function $f_{\\star}\\in L_{2}(\\mu_{x}),\\,i$ f ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lambda\\geq n^{-K},}&{\\quad\\gamma_{\\lambda}\\geq p^{-K},}&{\\qquad}&{\\widetilde{\\rho}_{\\lambda}(n,p)^{5/2}\\cdot\\log^{3/2}(n)\\leq K\\sqrt{n},}\\\\ &{}&{\\quad\\widetilde{\\rho}_{\\lambda}(n,p)^{2}\\cdot\\rho_{\\gamma_{+}}(p)^{8}\\cdot\\log^{4}(p)\\leq K\\sqrt{p},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "then with probability at least $1-n^{-D}-p^{-D}$ , we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\mathcal{R}(f_{\\star};X,W,\\lambda)-\\mathsf{R}_{n,p}(\\beta_{\\ast},\\lambda)|\\leq C_{\\ast,D,K}\\cdot\\mathcal{E}(n,p)\\cdot\\mathsf{R}_{n,p}(\\beta_{\\ast},\\lambda),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathsf{R}_{n,p}(\\beta_{*},\\lambda)$ has been defined in eq. (24) and: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\gamma_{\\lambda}=\\frac{p\\lambda}{n}+\\sum_{k=\\mathsf{m}+1}^{\\infty}\\xi_{k}^{2},\\qquad\\qquad\\gamma_{+}=p\\nu_{1}+\\sum_{k=\\mathsf{m}+1}^{\\infty}\\xi_{k}^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and the approximation rate is given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal E(n,p):=\\frac{\\widetilde{\\rho}_{\\lambda}(n,p)^{6}\\log^{7/2}(n)}{\\sqrt{n}}+\\frac{\\widetilde{\\rho}_{\\lambda}(n,p)^{2}\\cdot\\rho_{\\gamma_{+}}(p)^{8}\\log^{7/2}(p)}{\\sqrt{p}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For typical settings, with regularly varying spectrum, $\\rho_{\\kappa}(p)\\lesssim\\log(p)^{C}/\\kappa$ and $\\widetilde{\\rho}_{\\kappa}(n,p)\\lesssim\\log(n\\wedge$ $p)^{C}/\\kappa$ . In this case, the approximation rate scales as $\\mathcal{E}(n,p)=\\tilde{O}(n^{-1/2}+p^{-1/2})$ , which matches the optimal rates expected from local law fluctuations. A few remarks on this theorem are in order: ", "page_idx": 4}, {"type": "text", "text": "(a) Theorem 3.3 provides fully non-asymptotic approximation bounds for the population risk and its deterministic equivalent. They hold pointwise and for a large class of functions. In particular, they do not require probabilistic assumptions over the target function coefficients $\\beta_{\\star}$ , as for instance in [Dobriban and Wager, 2018, Richards et al., 2021, Wu and Xu, 2020]. ", "page_idx": 4}, {"type": "image", "img_path": "FBLJIfW64D/tmp/1b8a079afdc488686864254d3d4cfb644be5c820d129f7f95dcf06f3358f0cc2.jpg", "img_caption": ["Figure 1: Excess risk eq. (6) of RFRR as a function of the number of features $p$ for a fixed number of samples $n$ . Solid lines are obtained from the deterministic equivalent in Theorem 3.3, and points are numerical simulations, with the different curves denoting different regularization strengths $\\lambda\\geq0$ . (Left) Training data $(x_{i},y_{i})_{i\\in[n]}$ , $n=500$ , sampled from a teacher-student model $y_{i}=\\operatorname{erf}(\\langle\\beta,x_{i}\\rangle)+$ $\\varepsilon_{i}$ , $\\sigma_{\\varepsilon}^{2}=0.1$ , $\\mathbf{\\boldsymbol{x}}_{i}\\sim_{\\mathrm{i.i.d.}}$ $\\mathcal{N}(0,\\pmb{I}_{d})$ , with a spiked random feature map $\\varphi(\\pmb{x},\\pmb{w})=\\operatorname{tanh}(\\langle\\pmb{w}+\\boldsymbol{u}\\pmb{v},\\pmb{x}\\rangle)$ where $\\pmb{v}\\in\\mathbb{R}^{d}$ has a fixed overlap $\\boldsymbol{\\gamma}\\,=\\,\\langle\\boldsymbol{v},\\beta\\rangle$ with the teacher vector, $\\pmb{w}\\sim\\mathcal{N}(0,d^{-1}\\pmb{I}_{d})$ , $u\\sim$ $\\mathcal{N}(0,1)$ . (Right) Training data $(x_{i},y_{i})_{i\\in[n]}$ , $n=300$ , sub-sampled from the FashionMNIST data set [Xiao et al., 2017], with feature map given by $\\varphi(\\pmb{x};\\pmb{w})=\\operatorname{erf}(\\langle\\pmb{w},\\pmb{x}\\rangle)$ and $\\mu_{w}=\\mathcal{N}(0,d^{-1}I_{d})$ . "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "(b) They are not explicitly dependent on the feature map dimension. ", "page_idx": 5}, {"type": "text", "text": "(c) They are multiplicative, and therefore relative to the scale of the risk. In particular, they hold even if $\\mathcal{R}\\asymp n^{-\\gamma}$ , which will be crucial to the discussion in section 4. ", "page_idx": 5}, {"type": "text", "text": "(d) Theorem 3.3 is considerably more general than previous results. First, it extends the dimension-free results of Cheng and Montanari [2022] for well-specified ridge regression and Misiakiewicz and Saeed [2024] for KRR (see $p\\rightarrow\\infty$ discussion below) to the case of feature maps $\\varphi:\\mathcal{X}\\times\\mathcal{W}\\to\\mathbb{R}$ , which, as discussed in Section 2, comprises several cases of interest in machine learning. Moreover, the deterministic equivalent recovers as particular cases the asymptotic results derived under proportional $n,p=\\Theta(d)$ [Mei and Montanari, 2022, Loureiro et al., 2022, Schr\u00f6der et al., 2023] and polynomial $n,p=\\Theta(d^{\\kappa})$ [Xiao et al., 2022, Hu et al., 2024, Aguirre-L\u00f3pez et al., 2024] scaling. ", "page_idx": 5}, {"type": "text", "text": "(e) The bounds depend on $\\lambda^{-1}$ and $\\lambda_{>\\mathsf{m}}^{-1}$ . Following similar arguments as in Cheng and Montanari [2022], Misiakiewicz and Saeed [2024], this assumption could be removed at the cost of a lengthier analysis and worse rates $n^{-C}+\\bar{p}^{-C}$ with $C<1/2$ . ", "page_idx": 5}, {"type": "text", "text": "Figure 1 illustrates Theorem 3.3 in two different settings with real and synthetic data. On the left, we show the population risk of learning a single-index target function with a spiked random features model. This model was recently shown to be equivalent to the first-step of training in a fully-connected two-layer network [Ba et al., 2022], and it was recently studied by several authors [Moniri et al., 2024, Cui et al., 2024, Wang et al., 2024]. On the right, we apply our formulas directly to a real data set. In both cases, the theoretical curves show excellent agreement with the numerical simulations. In Appendix C we present additional plots, together with a discussion of how these plots were generated. ", "page_idx": 5}, {"type": "text", "text": "Particular limits \u2014 We now discuss some particular limits of interest of the deterministic equ\u221aivalent eq. (24). First, note that at the interpolation threshold $n=p$ , we have $1-\\Upsilon(\\nu_{1},\\nu_{2})\\sim\\sqrt{\\lambda}$ . Therefore, the risk $\\mathsf{R}_{n,p}\\,\\sim\\,\\lambda^{-1/2}$ diverges as $\\lambda\\,\\rightarrow\\,0^{+}$ , a well-known behaviour known as the interpolation peak in the random feature literature [Hastie et al., 2022, Mei and Montanari, 2022, Gerace et al., 2021] and observed in neural networks [Spigler et al., 2019, Nakkiran et al., 2021]. ", "page_idx": 5}, {"type": "text", "text": "Another limit of interest is $p\\rightarrow\\infty$ where, in the generic case, the features span an infinite-dimensional RKHS. Typically, the resulting kernel will be universal, implying it can approximate any function in $L_{2}(\\mu_{x})$ . In this limit, the risk bottleneck is given by the finite amount of data $n$ . ", "page_idx": 5}, {"type": "text", "text": "Corollary 3.4 (Kernel limit). In the $p\\rightarrow\\infty$ limit both $\\nu_{1}$ and $\\nu_{2}$ converge to a single \u03bdK which is the unique positive solution to the following self-consistent equation ", "page_idx": 6}, {"type": "equation", "text": "$$\nn-\\frac{\\lambda}{\\nu_{\\mathsf{K}}}=\\mathrm{Tr}\\bigl(\\Sigma(\\Sigma+\\nu_{\\mathsf{K}})^{-1}\\bigr).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Moreover, the bias eq. (22) and variance eq. (23) terms simplify to: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathsf{B}_{\\mathsf{K},n}(\\beta_{*},\\lambda)=\\frac{\\nu_{\\mathsf{K}}^{2}\\langle\\beta_{*},(\\Sigma+\\nu_{\\mathsf{K}})^{-2}\\beta_{*}\\rangle}{1-\\frac{1}{n}\\mathrm{Tr}\\big(\\Sigma^{2}(\\Sigma+\\nu_{\\mathsf{K}})^{-2}\\big)},\\quad\\mathsf{V}_{\\mathsf{K},n}(\\lambda)=\\sigma_{\\varepsilon}^{2}\\frac{\\mathrm{Tr}(\\Sigma^{2}(\\Sigma+\\nu_{\\mathsf{K}})^{-2})}{n-\\mathrm{Tr}\\big(\\Sigma^{2}(\\Sigma+\\nu_{\\mathsf{K}})^{-2}\\big)}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We denote the corresponding test error $\\mathsf{R}_{\\mathsf{K},n}(\\beta_{*},\\lambda)=\\mathsf{B}_{\\mathsf{K},n}(\\beta_{*},\\lambda)+\\mathsf{V}_{\\mathsf{K},n}(\\lambda).$ ", "page_idx": 6}, {"type": "text", "text": "Note that eq. (23) exactly agrees with the dimension-free deterministic equivalents for kernel methods in Cheng and Montanari [2022], Misiakiewicz and Saeed [2024]. Finally, the third limit of interest is the $n\\to\\infty$ where data is abundant. In this case, the empirical risk eq. (5) converge to the population risk, and therefore the bottleneck in the risk is given by the capacity of the random feature class $\\mathcal{F}_{\\mathrm{RF}}$ eq. (1) to approximate the target $f_{\\star}$ . ", "page_idx": 6}, {"type": "text", "text": "Corollary 3.5 (Approximation limit). In the $n\\to\\infty$ limit, we have $\\nu_{1}\\to0$ and $\\nu_{2}\\to\\nu_{\\mathsf{A}}$ satisfying the following simplified self-consistent equation: ", "page_idx": 6}, {"type": "equation", "text": "$$\np=\\mathrm{Tr}\\bigl(\\Sigma(\\Sigma+\\nu_{\\mathsf{A}})^{-1}\\bigr).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Moreover, the bias eq. (22) and variance eq. (23) terms simplify $t o$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathsf{B}_{\\mathsf{A},p}(\\beta_{*})=\\nu_{\\mathsf{A}}\\langle\\beta_{*},(\\Sigma+\\nu_{\\mathsf{A}})^{-1}\\beta_{*}\\rangle,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathsf{V}_{\\mathsf{A},n}=0.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We denote the risk in this case $\\mathsf{R}_{\\mathsf{A},p}(\\beta_{*})=\\mathsf{B}_{\\mathsf{A},p}(\\beta_{*})$ , which as expected does not depend on $\\lambda$ . ", "page_idx": 6}, {"type": "text", "text": "4 Scaling laws ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Our exact characterization of the excess risk in Theorem 3.3 shows that the bottleneck in the model performance stems either from its approximation capacity (as measured by the \u201cwidth\u201d $p$ ) and the availability of data (as measured by the number of samples $n$ ). In other words, for a fixed data budget $n$ , increasing $p$ might not improve the error besides a certain point, yielding a waste of computational resources. This raises an important question: given a fixed data budged $n$ , what is the optimal choice of model size $p_{\\star}$ ? ", "page_idx": 6}, {"type": "text", "text": "Context \u2014 This is a fundamental question in the random feature literature, and was investigated already in the pioneering works of Rahimi and Recht [2007, 2008], who showed that to achieve an excess risk of $O(n^{-1/2})$ requires at most $p=O(n)$ features. This upper bound was considerably refined by Rudi and Rosasco [2017] under classical power law scaling assumptions, also known as source and capacity conditions in the kernel literature: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{Tr}\\Sigma^{1/\\alpha}<\\infty,\\qquad\\qquad\\qquad||\\Sigma^{-r}\\beta_{\\star}||_{2}<\\infty.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\alpha\\,\\in\\,(1,\\infty)$ and $r\\,\\in\\,(0,\\infty)$ , with the case $r\\,=\\,^{1}/2$ corresponding to $f_{\\star}$ belonging to the RKHS of the asymptotic random feature kernel eq. (2). The optimal minmax rate $O\\left(n^{-\\frac{2\\alpha r}{2\\alpha r+1}}\\right)$ for ridge regression under source and capacity conditions were obtained by Caponnetto and De Vito [2007]. Rudi and Rosasco [2017] showed that this optimal rate can be attained by the random feature hypothesis eq. (1) with $p>p_{0}=O\\left(n^{\\frac{\\alpha-1+2r}{1+2\\alpha r}}\\right)$ features. However, this is only an upper bound, and understanding how tight it is, as well as the full picture in the hard regime $r\\in(0,1/2)$ , remains an open question. In this section, we leverage our tight characterization of the excess risk in Theorem 3.3 to provide a sharp answer to this question. ", "page_idx": 6}, {"type": "text", "text": "Results \u2014 Without loss of generality, we can assume the covariance is a diagonal matrix $\\pmb{\\Sigma}=$ $\\mathrm{diag}(\\xi_{k}^{2})_{k\\geq1}$ , and we consider the case where the exponents exactly saturate the source and capacity conditions eq. (37): ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\xi_{k}^{2}=k^{-\\alpha},~~~~~~~~~~~~~~~~~~~~~~~~~\\beta_{*,k}=k^{-\\frac{1+2\\alpha r}{2}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Further, we assume a relative scaling of the number of features $p$ and the regularization $\\lambda$ with the number of samples $n$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\np=n^{q},\\qquad\\qquad\\qquad\\qquad\\lambda=n^{-(\\ell-1)}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "with $q\\geq0$ and $\\ell\\geq0$ . ", "page_idx": 6}, {"type": "image", "img_path": "FBLJIfW64D/tmp/e9556fe6641560c7044a8deb23533676fd1792e85b258a3852587bb10901048e.jpg", "img_caption": ["Figure 2: Excess error rate $\\gamma$ in the regime $n\\gg\\sigma_{\\varepsilon}^{-1/(\\gamma_{\\beta}(\\ell,q)-\\gamma_{\\gamma}(\\ell,q))}$ as a function of $(\\ell,q)$ , defined in eq. (40) and eq. (39) for $r\\geq\\,^{1}/2$ (Left) and $r\\,\\in\\,[0,{^1}/{2})$ (Right). The explicit crossover points $\\ell_{\\star},q_{\\star},\\hat{q}$ are defined in eq. (43) as a function of the source $r$ and capacity $\\alpha$ exponents. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Theorem 4.1 (Excess risk rates). Under source and capacity conditions eq. (38) and scaling assumptions eq. (39), the deterministic equivalent eq. (24) rate is given by: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathsf{R}_{n,p}(\\beta_{*},\\lambda)=\\Theta\\left(n^{-\\gamma_{B}(\\ell,q)}+\\sigma_{\\varepsilon}^{2}n^{-\\gamma\\nu(\\ell,q)}\\right)=\\Theta\\left(n^{-\\gamma(\\ell,q)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\gamma(\\ell,q):=\\gamma_{B}(\\ell,q)\\land\\gamma_{\\mathscr{V}}(\\ell,q)$ for non-zero noise variance $\\sigma_{\\varepsilon}^{2}\\neq0,$ , otherwise $\\gamma(\\ell,q)=\\gamma_{B}(\\ell,q)$ . The exponents $\\gamma_{B}$ and $\\gamma_{\\mathcal{V}}$ are respectively the decay rates of the bias and variance terms eqs. (22) and (23), and are explicitly given by ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\gamma_{B}:=\\left[2\\alpha\\left(\\frac{\\ell}{\\alpha}\\wedge q\\wedge1\\right)\\left(r\\wedge1\\right)\\right]\\wedge\\left[\\left(2\\alpha\\left(r\\wedge\\frac{1}{2}\\right)-1\\right)\\left(\\frac{\\ell}{\\alpha}\\wedge q\\wedge1\\right)+q\\right],}\\\\ {\\displaystyle\\gamma_{V}:=1-\\left(\\frac{\\ell}{\\alpha}\\wedge q\\wedge1\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Remark 4.1. Under the scaling in eqs. (38) and (39), one can check that the approximation rates $\\mathcal{E}(n,p)$ in Theorem 3.3 are vanishing for $\\ell\\leq\\alpha\\,+\\,^{1}\\!/12\\,i f q\\geq1,$ , and for $\\ell\\leq q((\\alpha+1/16)\\vee^{1}/16(\\alpha-1))$ if $q\\,<\\,1$ , which includes the optimal vertical line $\\ell\\,=\\,\\ell_{*}$ . Hence, for these regions of scaling, Theorem 3.3 readily implies that the excess risk eq. (6) indeed has the decay rates described in Theorem 4.1. As discussed in the previous section, we expect that these approximation guarantees can be improved to include a larger region of decay rates, but we leave it to future work. ", "page_idx": 7}, {"type": "text", "text": "A detailed derivation of the result above from the deterministic equivalent characterization from Theorem 3.3 is discussed in Appendix D. The expressions in eq. (41) are easier to visualise in a diagram. Figure 2 shows the excess risk exponent $\\bar{\\gamma}(\\ell,q)$ as a function of the parameters $\\ell$ and $q$ , in the case where $\\sigma_{\\varepsilon}^{2}\\neq0$ for $r\\geq1/2$ (left) and $r<1/2$ (right). Note that the key difference between the diagrams is the presence of an additional region for $r\\geq1/2$ .2 Defining the following shorthand: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\ell_{\\star}:=\\frac{\\alpha}{2\\alpha(r\\wedge1)+1},\\qquad q_{\\star}:=1-\\ell_{\\star}(2r\\wedge1),\\qquad\\widehat{q}:=\\frac{1}{\\alpha(2r\\wedge1)+1}=q_{\\star}\\vee\\frac{1}{\\alpha+1}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "we can identify two main regions in the $(\\ell,q)$ plane, corresponding to a trade-off between the bias $\\gamma_{B}$ and variance $\\gamma_{\\mathcal{V}}$ terms: ", "page_idx": 7}, {"type": "text", "text": "(a) Variance dominated region $(\\gamma\\nu<\\gamma_{B})$ : if $\\ell>\\ell_{\\star}$ , $q>\\hat{q}$ and $p>\\lambda$ , the excess risk is dominated by the variance term, provided the number of samples is large enough \u22121/(\u03b3B(\u2113,q)\u2212\u03b3V (\u2113,q)).3 Inside this region it is possible to further distinguish between two regimes: ", "page_idx": 7}, {"type": "text", "text": "\u2022 slow decay regime (orange and brown): for $\\ell<\\alpha$ and $q<1\\;(p\\ll n)$ , $\\gamma\\nu=1-(\\ell/\\alpha\\wedge q)$ , hence the decay depends on the interplay between regularization strength and number of random features and it is slower as $(\\ell/\\alpha\\wedge q)$ increases; ", "page_idx": 8}, {"type": "text", "text": "\u2022 plateau regime (red): for $\\ell\\geq\\alpha$ and $q\\geq1\\left(p\\geq n\\right)$ the excess risk converges to a constant value and does not decay as $n$ increases. ", "page_idx": 8}, {"type": "text", "text": "(b) Bias dominated region $(\\gamma\\nu>\\gamma_{B})$ : if $\\ell<\\ell_{\\star}$ , $q<\\hat{q}$ and $p<\\lambda$ , the excess risk is dominated by the bias term, whose decay is faster as $(\\ell/\\alpha\\wedge q)$ increases (cyan, emerald and teal). ", "page_idx": 8}, {"type": "text", "text": "Note that in the limit of large number of random features $p\\rightarrow\\infty$ , we recover the same rates found by Cui et al. [2022] for kernel ridge regression. Of particular interest is the rate for which the excess risk decays the fastest with the number of samples $n$ , and what is the minimum number of random features $p_{\\star}$ required to achieve this rate. ", "page_idx": 8}, {"type": "text", "text": "Corollary 4.2 (Optimal rates). The optimal excess risk rate achieved by the random features hypothesis eq. (1) under source and capacity conditions eq. (38) and scaling assumptions eq. (39): ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\gamma_{\\star}=\\operatorname*{max}_{\\ell,q}\\gamma(\\ell,q)=\\frac{2\\alpha(r\\wedge1)}{2\\alpha(r\\wedge1)+1},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "and it is attained for: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{\\left\\{\\lambda=\\lambda_{\\star}:=n^{-(\\ell_{\\star}-1)}\\right.}\\\\ &{\\left\\{p\\ge p_{\\star}:=n^{q_{\\star}}=\\lambda_{\\star}\\right.}\\\\ &{\\left.\\left\\{\\lambda=\\lambda_{\\star}\\right.\\right.}\\\\ &{\\left.p\\ge p_{\\star}=\\left(\\lambda_{\\star}^{-1}n\\right)^{1/\\alpha}\\right.}&{\\ o r\\,\\left\\{p=p_{\\star}=\\left(\\lambda_{\\star}^{-1}n\\right)^{1/\\alpha}\\right.}\\end{array}\\qquad\\qquad}&&{f o r\\,r\\le1/2,}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "corresponding to the bold red line $(-)$ in Fig. 2. In particular, the minimal number of random features $p_{\\star}=n^{q_{\\star}}$ required to achieve the optimal rate $\\gamma_{\\star}$ is given by: ", "page_idx": 8}, {"type": "equation", "text": "$$\nq_{\\star}=1-\\frac{\\alpha(2r\\wedge1)}{2\\alpha(r\\wedge1)+1}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "and corresponds to the bold red dot (\u2022) in Fig. 2. ", "page_idx": 8}, {"type": "text", "text": "A few comments on Corollary 4.2 are in place. 1) The optimal excess error rate eq. (44) is consistent with the minimax optimal rates for ridge regression from Caponnetto and De Vito [2007], as also discussed by Rudi and Rosasco [2017]. 2) The minimal number of random features $p_{\\star}=n^{q_{\\star}}$ in eq. (47) achieving the optimal rate eq. (44) in the $r\\geq1/2$ regime is strictly smaller than the lower bound $p>p_{0}$ of Rudi and Rosasco [2017]. More precisely, letting $p_{0}=n^{q_{0}}$ , for $r\\in[1/2,1)$ : ", "page_idx": 8}, {"type": "equation", "text": "$$\nq_{0}-q_{\\star}=\\frac{2(1-r)(\\alpha-1)}{2\\alpha r+1}>0,\\qquad\\mathrm{for}\\,\\mathrm{all}\\,\\alpha>1.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Relationship to scaling laws \u2014 The empirical observation that the performance of large scale neural networks decreases as a power law with respect to the number of samples, parameter and computing time has sparked a renewed wave of interest in the theoretical investigation of power laws [Kaplan et al., 2020]. Despite being a mature topic in the statistical learning literature, different recent works have turned to the study of linear models under source and capacity conditions as a playground to understand the emergence of different bottlenecks in the excess error rates [Bahri et al., 2024, Maloney et al., 2022]. ", "page_idx": 8}, {"type": "text", "text": "The model studied in these works is given by ridge regression on data $y_{i}\\,=\\,\\langle\\beta_{\\star},{\\pmb x}_{i}\\rangle$ with $\\textbf{\\em x}\\sim$ ${\\mathcal{N}}(\\mathbf{0},\\mathrm{diag}((d/k)^{\\alpha}))$ and $\\beta_{\\star}\\,\\sim\\mathcal{N}(\\mathbf{0},{^1/d}I_{d})$ with a linear projection model $\\hat{f}({\\pmb x},{\\pmb a})\\,=\\,\\langle{\\pmb a},W{\\pmb x}\\rangle$ , where $W$ is an i.i.d. Gaussian matrix. Note this model is a particular case of the one studied here, corresponding to a linear feature map and random target function. Moreover, since the variance of the target is constant, the source is entirely determined by the capacity $\\alpha$ of the asymptotic kernel, here controlled by the decay of the covariance of the input data. ", "page_idx": 8}, {"type": "text", "text": "The approximation limit from Corollary 3.5 and the kernel limit from Corollary 3.4 are known in this literature as Variance and Resolution limited regimes, respectively [Bahri et al., 2024]. They correspond precisely to the bottlenecks in the excess risk arising from the limited approximation capacity of the random feature model or the limited availability of training data. As this model is a particular case of ours, the rates in the variance limited regime can also be obtained from Theorem 4.1, and correspond to particular cases in Fig. 2, see Appendix E for a detailed discussion. Contemporary to our work, Atanasov et al. [2024] has extended the analysis in this linear model to the case where $\\beta_{\\star}$ also has a power-law decay, and provided a comprehensive discussion of the different scaling regimes for this model. Their rates can be put in a one-to-one correspondence with the rates derived in section 4. We refer the interested reader to Section VI.6 of Atanasov et al. [2024] for a detailed discussion of this relationship. We stress, however, that beside being rigorous, our results hold for features in infinite-dimensional Hilbert spaces and are not restricted to a particular asymptotic limit in the dimensions. ", "page_idx": 8}, {"type": "image", "img_path": "FBLJIfW64D/tmp/5fc882307d59fbcc857eca5f7942f881a258921a25f16095f880e69d6a75f79f.jpg", "img_caption": ["Figure 3: Excess risk eq. (6) of RFRR as a function of the number of samples $n$ under source and capacity conditions eq. (37) and power-law assumptions $\\lambda=n^{-(\\ell-1)}$ , $p=n^{q}$ , with noise variance $\\sigma_{\\varepsilon}^{2}=0.1$ . Solid lines are obtained from the deterministic equivalent Theorem 3.3. In the figure on the left, points are finite size numerical experiments. Dashed and dotted lines are the analytical rates from Theorem 4.1, stated in the legend. The colour scheme corresponds to the regions of Fig. 2. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Complementary to the sample and model complexity bottlenecks, Kaplan et al. [2020] also observed the emergence of computational scaling laws in the risk as a function of flops used in training. A recent line of work has investigated this question on the aforementioned linear random feature model under different training algorithms, such as gradient flow [Bordelon et al., 2024] and SGD [Paquette et al., 2024, Lin et al., 2024]. Due to the simplicity of this setting, the risk of ridge regression with a particular choice of regularization $\\lambda$ is closely related to the risk of different descent algorithms for least-squares at a fixed running horizon [Ali et al., 2019, 2020, Sonthalia et al., 2024]. A similar analogy allows us to compare our results to the ones obtained in [Paquette et al., 2024, Lin et al., 2024]. In particular, our setting cover three of the phases identified by Paquette et al. [2024], and correspond to the result in Theorem 4.1 with $\\lambda=1$ $\\mathcal{\\ell}=1$ ). Similarly, the rates of Lin et al. [2024] are obtained by taking $\\lambda$ to be the inverse of the learning rate. A detailed connection to this line of work is discussed in Appendix E. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we have investigated the generalization properties of random feature models, deriving a non-asymptotic deterministic equivalent for the risk of random feature ridge regression\u2014which recovers (and unifies) previous asymptotic findings as special limits. Our results provide a rigorous multiplicative approximation rate, enabling us to analyze error scaling laws under source and capacity conditions, and offers a complete view of the different scaling regimes and their cross-overs. Our analysis relies on Assumption 3.1 which, while popular in theoretical investigations, excludes more realistic random feature models, such as $\\varphi(\\pmb{x},\\pmb{w})^{\\pmb{\\cdot}}{=}\\,\\sigma(\\pmb{x}^{\\top}\\pmb{w})$ with $\\mathbf{\\Delta}x,w$ Gaussian vectors and non-linear $\\sigma$ . Although restrictive, this assumption allowed us to derive tight multiplicative approximation bounds for a generic random feature model with infinite-dimensional features\u2014which was essential for obtaining the rigorous excess risk rates that are the primary motivation of our work. We further note that numerical simulations in Figure 1 and Appendix C suggest that the predictions of Theorem 3.3 remain accurate much beyond Assumption 3.1. We consider lifting this technical condition\u2014e.g., by following the approach in Misiakiewicz and Saeed [2024]\u2014to be an important direction for future research. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We would like to thank Yasaman Bahri, Hugo Cui and Florent Krzakala for stimulating discussions.   \nBL & LD acknowledges funding from the Choose France - CNRS AI Rising Talents program. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Fabi\u00e1n Aguirre-L\u00f3pez, Silvio Franz, and Mauro Pastore. Random features and polynomial rules. arXiv preprint arXiv:2402.10164, 2024. ", "page_idx": 10}, {"type": "text", "text": "Alnur Ali, J Zico Kolter, and Ryan J Tibshirani. A continuous-time view of early stopping for least squares regression. In The 22nd international conference on artificial intelligence and statistics, pages 1370\u20131378. PMLR, 2019.   \nAlnur Ali, Edgar Dobriban, and Ryan Tibshirani. The implicit regularization of stochastic gradient flow for least squares. In International conference on machine learning, pages 233\u2013244. PMLR, 2020.   \nAlexander B. Atanasov, Jacob A. Zavatone-Veth, and Cengiz Pehlevan. Scaling and renormalization in high-dimensional regression. arXiv preprint arXiv:2405.00592, 2024.   \nJimmy Ba, Murat A Erdogdu, Taiji Suzuki, Zhichao Wang, Denny Wu, and Greg Yang. Highdimensional asymptotics of feature learning: How one gradient step improves the representation. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 37932\u201337946. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/ f7e7fabd73b3df96c54a320862afcb78-Paper-Conference.pdf.   \nFrancis Bach. High-dimensional analysis of double descent for linear regression with random projections. SIAM Journal on Mathematics of Data Science, 6(1):26\u201350, 2024. doi: 10.1137/ 23M1558781. URL https://doi.org/10.1137/23M1558781.   \nYasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma. Explaining neural scaling laws. Proceedings of the National Academy of Sciences, 121(27):e2311878121, July 2024. doi: 10.1073/pnas.2311878121. URL https://www.pnas.org/doi/abs/10.1073/ pnas.2311878121.   \nMaria-Florina Balcan, Avrim Blum, and Santosh Vempala. Kernels as features: On kernels, margins, and low-dimensional mappings. Machine Learning, 65(1):79\u201394, 2006.   \nPeter L. Bartlett, Philip M. Long, G\u00e1bor Lugosi, and Alexander Tsigler. Benign overfitting in linear regression. Proceedings of the National Academy of Sciences, 117(48):30063\u201330070, 2020. doi: 10.1073/pnas.1907378117. URL https://www.pnas.org/doi/abs/10.1073/ pnas.1907378117.   \nPeter L. Bartlett, Andrea Montanari, and Alexander Rakhlin. Deep learning: a statistical viewpoint. Acta Numerica, 30:87\u2013201, 2021. doi: 10.1017/S0962492921000027.   \nMikhail Belkin. Fit without fear: remarkable mathematical phenomena of deep learning through the prism of interpolation. Acta Numerica, 30:203\u2013248, 2021. doi: 10.1017/S0962492921000039.   \nMikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning practice and the classical bias\u2013variance trade-off. Proceedings of the National Academy of Sciences, 116(32):15849\u201315854, August 2019. doi: 10.1073/pnas.1903070116. URL https: //www.pnas.org/doi/full/10.1073/pnas.1903070116.   \nBlake Bordelon, Abdulkadir Canatar, and Cengiz Pehlevan. Spectrum dependent learning curves in kernel regression and wide neural networks. In Hal Daum\u00e9 III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 1024\u20131034. PMLR, 13\u201318 Jul 2020. URL https://proceedings. mlr.press/v119/bordelon20a.html.   \nBlake Bordelon, Alexander Atanasov, and Cengiz Pehlevan. A dynamical model of neural scaling laws. arXiv preprint arXiv:2402.01092, 2024. ", "page_idx": 10}, {"type": "text", "text": "David Bosch, Ashkan Panahi, and Babak Hassibi. Precise asymptotic analysis of deep random feature models. In Gergely Neu and Lorenzo Rosasco, editors, Proceedings of Thirty Sixth Conference on Learning Theory, volume 195 of Proceedings of Machine Learning Research, pages 4132\u20134179. PMLR, 12\u201315 Jul 2023a. URL https://proceedings.mlr.press/v195/bosch23a.html. ", "page_idx": 11}, {"type": "text", "text": "David Bosch, Ashkan Panahi, Ayca Ozcelikkale, and Devdatt Dubhashi. Random features model with general convex regularization: A fine grained analysis with precise asymptotic learning curves. In Francisco Ruiz, Jennifer Dy, and Jan-Willem van de Meent, editors, Proceedings of The 26th International Conference on Artificial Intelligence and Statistics, volume 206 of Proceedings of Machine Learning Research, pages 11371\u201311414. PMLR, 25\u201327 Apr 2023b. URL https://proceedings.mlr.press/v206/bosch23a.html.   \nFrancesco Cagnetta, Alessandro Favero, and Matthieu Wyart. What can be learnt with wide convolutional neural networks? In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 3347\u20133379. PMLR, 23\u201329 Jul 2023. URL https://proceedings.mlr.press/v202/cagnetta23a.html.   \nAndrea Caponnetto and Ernesto De Vito. Optimal rates for the regularized least-squares algorithm. Foundations of Computational Mathematics, 7(3):331\u2013368, 2007.   \nChen Cheng and Andrea Montanari. Dimension free ridge regression. arXiv preprint arXiv:2210.08571, 2022.   \nL\u00e9na\u00efc Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/file/ ae614c557843b1df326cb29c57225459-Paper.pdf.   \nCl\u00e9ment Chouard. Quantitative deterministic equivalent of sample covariance matrices with a general dependence structure. arXiv preprint arXiv:2211.13044, 2022.   \nCl\u00e9ment Chouard. Deterministic equivalent of the conjugate kernel matrix associated to artificial neural networks. arXiv preprint arXiv:2306.05850, 2023.   \nHugo Cui, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborov\u00e1. Generalization error rates in kernel regression: the crossover from the noiseless to noisy regime. Journal of Statistical Mechanics: Theory and Experiment, 2022(11):114004, nov 2022. doi: 10.1088/1742-5468/ac9829. URL https://dx.doi.org/10.1088/1742-5468/ac9829.   \nHugo Cui, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborov\u00e1. Error scaling laws for kernel classification under source and capacity conditions. Machine Learning: Science and Technology, 4 (3):035033, aug 2023. doi: 10.1088/2632-2153/acf041. URL https://dx.doi.org/10.1088/ 2632-2153/acf041.   \nHugo Cui, Luca Pesce, Yatin Dandi, Florent Krzakala, Yue Lu, Lenka Zdeborova, and Bruno Loureiro. Asymptotics of feature learning in two-layer networks after one gradient-step. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 9662\u20139695. PMLR, 21\u201327 Jul 2024. URL https://proceedings.mlr.press/v235/cui24d.html.   \nYatin Dandi, Florent Krzakala, Bruno Loureiro, Luca Pesce, and Ludovic Stephan. How two-layer neural networks learn, one (giant) step at a time. arXiv preprint arXiv:2305.18270, 2023.   \nOussama Dhifallah and Yue M. Lu. A precise performance analysis of learning with random features. arXiv preprint arXiv:2008.11904, 2020.   \nEdgar Dobriban and Stefan Wager. High-dimensional asymptotics of prediction: Ridge regression and classification. The Annals of Statistics, 46(1):247\u2013279, 2018. ISSN 00905364, 21688966. URL https://www.jstor.org/stable/26542784. ", "page_idx": 11}, {"type": "text", "text": "Elvis Dohmatob, Yunzhen Feng, and Julia Kempe. Model collapse demystified: The case of regression. arXiv preprint arXiv:2402.07712, 2024. ", "page_idx": 12}, {"type": "text", "text": "Zhou Fan and Zhichao Wang. Spectra of the conjugate kernel and neural tangent kernel for linearwidth neural networks. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 7710\u20137721. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/ file/572201a4497b0b9f02d4f279b09ec30d-Paper.pdf.   \nAlessandro Favero, Francesco Cagnetta, and Matthieu Wyart. Locality defeats the curse of dimensionality in convolutional teacher-student scenarios. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 9456\u20139467. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/ 4e8eaf897c638d519710b1691121f8cb-Paper.pdf.   \nFederica Gerace, Bruno Loureiro, Florent Krzakala, Marc M\u00e9zard, and Lenka Zdeborov\u00e1. Generalisation error in learning with random features and the hidden manifold model. Journal of Statistical Mechanics: Theory and Experiment, 2021(12):124013, dec 2021. doi: 10.1088/1742-5468/ac3ae6. URL https://dx.doi.org/10.1088/1742-5468/ac3ae6.   \nBehrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Limitations of lazy training of two-layers neural network. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/   \npaper_files/paper/2019/file/c133fb1bb634af68c5088f3438848bfd-Paper.pdf.   \nBehrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. When do neural networks outperform kernel methods? In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 14820\u201314830. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/ paper_files/paper/2020/file/a9df2255ad642b923d95503b9a7958d8-Paper.pdf.   \nSebastian Goldt, Bruno Loureiro, Galen Reeves, Florent Krzakala, Marc Mezard, and Lenka Zdeborova. The gaussian equivalence of generative models for learning with shallow neural networks. In Joan Bruna, Jan Hesthaven, and Lenka Zdeborova, editors, Proceedings of the 2nd Mathematical and Scientific Machine Learning Conference, volume 145 of Proceedings of Machine Learning Research, pages 426\u2013471. PMLR, 16\u201319 Aug 2022. URL https://proceedings.mlr.press/v145/goldt22a.html.   \nTrevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J. Tibshirani. Surprises in highdimensional ridgeless least squares interpolation. The Annals of Statistics, 50(2):949 \u2013 986, 2022. doi: 10.1214/21-AOS2133. URL https://doi.org/10.1214/21-AOS2133.   \nHong Hu and Yue M. Lu. Universality laws for high-dimensional learning with random features. IEEE Transactions on Information Theory, 69(3):1932\u20131964, 2023. doi: 10.1109/TIT.2022.3217698.   \nHong Hu, Yue M. Lu, and Theodor Misiakiewicz. Asymptotics of random feature regression beyond the linear scaling regime. arXiv preprint arXiv:2403.08160, 2024.   \nArthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper_files/paper/ 2018/file/5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf.   \nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.   \nYan Lecun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998. doi: 10.1109/5.726791.   \nYicheng Li, haobo Zhang, and Qian Lin. On the asymptotic learning curves of kernel ridge regression under power-law decay. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 49341\u201349364. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/ 2023/file/9adc8ada9183f4b9a007a02773fd8114-Paper-Conference.pdf.   \nZhenyu Liao and Romain Couillet. On the spectrum of random features maps of high dimensional data. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 3063\u20133071. PMLR, 10\u201315 Jul 2018. URL https://proceedings.mlr.press/v80/liao18a.html.   \nLicong Lin, Jingfeng Wu, Sham M Kakade, Peter L Bartlett, and Jason D Lee. Scaling laws in linear regression: Compute, parameters, and data. arXiv preprint arXiv:2406.08466, 2024.   \nBruno Loureiro, C\u00e9dric Gerbelot, Hugo Cui, Sebastian Goldt, Florent Krzakala, Marc M\u00e9zard, and Lenka Zdeborov\u00e1. Learning curves of generic features maps for realistic datasets with a teacherstudent model. Journal of Statistical Mechanics: Theory and Experiment, 2022(11):114001, nov 2022. doi: 10.1088/1742-5468/ac9825. URL https://dx.doi.org/10.1088/1742-5468/ ac9825.   \nAlexander Maloney, Daniel A. Roberts, and James Sully. A solvable model of neural scaling laws. arXiv preprint arXiv:2210.16859, 2022.   \nSong Mei and Andrea Montanari. The generalization error of random features regression: Precise asymptotics and the double descent curve. Communications on Pure and Applied Mathematics, 75(4):667\u2013766, 2022. doi: https://doi.org/10.1002/cpa.22008. URL https://onlinelibrary. wiley.com/doi/abs/10.1002/cpa.22008.   \nSong Mei, Theodor Misiakiewicz, and Andrea Montanari. Generalization error of random feature and kernel methods: Hypercontractivity and kernel matrix concentration. Applied and Computational Harmonic Analysis, 59:3\u201384, 2022. ISSN 1063-5203. doi: https://doi.org/10. 1016/j.acha.2021.12.003. URL https://www.sciencedirect.com/science/article/pii/ S1063520321001044. Special Issue on Harmonic Analysis and Machine Learning.   \nTheodor Misiakiewicz and Song Mei. Learning with convolution and pooling operations in kernel methods. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 29014\u201329025. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/ file/ba8aee784ffe0813890288b334444eda-Paper-Conference.pdf.   \nTheodor Misiakiewicz and Basil Saeed. A non-asymptotic theory of kernel ridge regression: deterministic equivalents, test error, and gcv estimator. arXiv preprint arXiv:2403.08938, 2024.   \nBehrad Moniri, Donghwan Lee, Hamed Hassani, and Edgar Dobriban. A theory of non-linear feature learning with one gradient step in two-layer neural networks. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 36106\u201336159. PMLR, 21\u201327 Jul 2024. URL https://proceedings.mlr.press/v235/moniri24a.html.   \nPreetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep double descent: where bigger models and more data hurt. Journal of Statistical Mechanics: Theory and Experiment, 2021(12):124003, dec 2021. doi: 10.1088/1742-5468/ac3a74. URL https://dx.doi.org/10.1088/1742-5468/ac3a74.   \nElliot Paquette, Courtney Paquette, Lechao Xiao, and Jeffrey Pennington. $_{4+3}$ phases of computeoptimal neural scaling laws. arXiv preprint arXiv:2405.15074, 2024.   \nAli Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In J. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems, volume 20. Curran Associates, Inc., 2007. URL https://proceedings.neurips.cc/paper_ files/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Ali Rahimi and Benjamin Recht. Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems, volume 21. Curran Associates, Inc., 2008. URL https://proceedings.neurips.cc/paper_files/paper/2008/file/ 0efe32849d230d7f53049ddc4a4b0c60-Paper.pdf. ", "page_idx": 14}, {"type": "text", "text": "Dominic Richards, Jaouad Mourtada, and Lorenzo Rosasco. Asymptotics of ridge(less) regression under general source condition. In Arindam Banerjee and Kenji Fukumizu, editors, Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, volume 130 of Proceedings of Machine Learning Research, pages 3889\u20133897. PMLR, 13\u201315 Apr 2021. URL https://proceedings.mlr.press/v130/richards21b.html. ", "page_idx": 14}, {"type": "text", "text": "Alessandro Rudi and Lorenzo Rosasco. Generalization properties of learning with random features. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/ file/61b1fb3f59e28c67f3925f3c79be81a1-Paper.pdf. ", "page_idx": 14}, {"type": "text", "text": "Dominik Schr\u00f6der, Hugo Cui, Daniil Dmitriev, and Bruno Loureiro. Deterministic equivalent and error universality of deep random features learning. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 30285\u201330320. PMLR, 23\u201329 Jul 2023. URL https://proceedings. mlr.press/v202/schroder23a.html. ", "page_idx": 14}, {"type": "text", "text": "Dominik Schr\u00f6der, Daniil Dmitriev, Hugo Cui, and Bruno Loureiro. Asymptotics of learning with deep structured (Random) features. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 43862\u201343894. PMLR, 21\u201327 Jul 2024. URL https://proceedings.mlr. press/v235/schroder24a.html. ", "page_idx": 14}, {"type": "text", "text": "James B. Simon, Madeline Dickens, Dhruva Karkada, and Michael R. DeWeese. The eigenlearning framework: A conservation law perspective on kernel regression and wide neural networks. arXiv preprint arXiv:2110.03922, 2023a. ", "page_idx": 14}, {"type": "text", "text": "James B. Simon, Dhruva Karkada, Nikhil Ghosh, and Mikhail Belkin. More is better in modern machine learning: when infinite overparameterization is optimal and overftiting is obligatory. arXiv preprint arXiv:2311.14646, 2023b. ", "page_idx": 14}, {"type": "text", "text": "Rishi Sonthalia, Jackie Lok, and Elizaveta Rebrova. On regularization via early stopping for least squares regression. arXiv preprint arXiv:2406.04425, 2024. ", "page_idx": 14}, {"type": "text", "text": "S Spigler, M Geiger, S d\u2019Ascoli, L Sagun, G Biroli, and M Wyart. A jamming transition from under- to over-parametrization affects generalization in deep learning. Journal of Physics A: Mathematical and Theoretical, 52(47):474001, oct 2019. doi: 10.1088/1751-8121/ab4c8b. URL https://dx.doi.org/10.1088/1751-8121/ab4c8b. ", "page_idx": 14}, {"type": "text", "text": "Stefano Spigler, Mario Geiger, and Matthieu Wyart. Asymptotic learning curves of kernel methods: empirical data versus teacher\u2013student paradigm. Journal of Statistical Mechanics: Theory and Experiment, 2020(12):124001, dec 2020. doi: 10.1088/1742-5468/abc61d. URL https://dx. doi.org/10.1088/1742-5468/abc61d. ", "page_idx": 14}, {"type": "text", "text": "Joel A Tropp et al. An introduction to matrix concentration inequalities. Foundations and Trends\u00ae in Machine Learning, 8(1-2):1\u2013230, 2015. ", "page_idx": 14}, {"type": "text", "text": "Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint arXiv:1011.3027, 2010. ", "page_idx": 14}, {"type": "text", "text": "Zhichao Wang, Denny Wu, and Zhou Fan. Nonlinear spiked covariance matrices and signal propagation in deep neural networks. arXiv preprint arXiv:2402.10127, 2024. ", "page_idx": 14}, {"type": "text", "text": "Alexander Wei, Wei Hu, and Jacob Steinhardt. More than a toy: Random matrix models predict how real-world neural representations generalize. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 23549\u201323588. PMLR, 17\u201323 Jul 2022. URL https://proceedings.mlr.press/v162/ wei22a.html.   \nDenny Wu and Ji Xu. On the optimal weighted \\ell_2 regularization in overparameterized linear regression. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 10112\u201310123. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/ file/72e6d3238361fe70f22fb0ac624a7072-Paper.pdf.   \nHan Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.   \nLechao Xiao, Hong Hu, Theodor Misiakiewicz, Yue Lu, and Jeffrey Pennington. Precise learning curves and higher-order scalings for dot-product kernel regression. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 4558\u20134570. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/ 1d3591b6746204b332acb464b775d38d-Paper-Conference.pdf.   \nChiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview.net/forum?id=Sy8gdB9xx. ", "page_idx": 15}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A Background on deterministic equivalents ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We consider a feature vector $\\pmb{f}\\in\\mathbb{R}^{q}$ , $q\\in\\mathbb{N}\\cup\\{\\infty\\}$ , with covariance matrix $\\Sigma=\\mathbb{E}[f f^{\\top}]$ . We denote $\\gamma_{1}^{2}\\geq\\gamma_{2}^{2}\\geq\\gamma_{3}^{2}\\geq\\cdot\\cdot\\cdot$ the eigenvalues of $\\Sigma$ in non-increasing order. In the case of infinite-dimensional features $q=\\infty$ , we will further assume that $\\mathrm{Tr}(\\Sigma)<\\infty$ , i.e., we consider $\\Sigma$ to be a trace-class self-adjoint operator. ", "page_idx": 16}, {"type": "text", "text": "We assume that the feature $\\pmb{f}$ satisfies the following assumption. ", "page_idx": 16}, {"type": "text", "text": "Assumption A.1 (Feature concentration). There exist $\\mathsf{C}_{*}$ $\\mathbf{\\Psi}_{:}\\int\\mathsf{C}_{*}\\mathbf{\\Psi}>0$ such that for any p.s.d. matrix $A\\in\\mathbb{R}^{\\bar{q}\\times q}$ with $\\mathrm{Tr}({\\pmb{\\Sigma}}{\\pmb{A}})<\\infty$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(\\left|f^{\\top}A f-\\operatorname{Tr}(\\Sigma A)\\right|\\geq t\\cdot\\|\\Sigma^{1/2}A\\Sigma^{1/2}\\|_{F}\\right)\\leq\\mathsf C_{*}\\exp\\left\\{-\\mathsf c_{*}t\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Recall that we denote $C_{a_{1},\\ldots,a_{k}}$ constants that only depend on the values of $\\{a_{i}\\}_{i\\in[k]}$ . We use $a_{i}={}^{\\prime}{}*$ to denote the dependency on the constants $\\mathsf{C}_{*},\\mathsf{C}_{*}$ from Assumption A.1. ", "page_idx": 16}, {"type": "text", "text": "We will further define the following two quantities. ", "page_idx": 16}, {"type": "text", "text": "Definition 2 (Effective regularization). For an integer $n$ , covariance $\\Sigma$ , and regularization $\\lambda\\geq0$ , we define the effective regularization $\\lambda_{*}$ associated to model $(n,\\Sigma,\\lambda)$ to be the unique non-negative solution to the equation ", "page_idx": 16}, {"type": "equation", "text": "$$\nn-\\frac{\\lambda}{\\lambda_{*}}=\\mathrm{Tr}\\bigl(\\Sigma(\\Sigma+\\lambda_{*})^{-1}\\bigr).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Throughout this appendix, we assume that $\\lambda>0$ . The existence and uniqueness follow from noticing that the left-hand side is monotonically increasing in $\\lambda_{*}$ while the right-hand side is monotonically decreasing. We consider the change of variable $\\bar{\\mu}_{*}:=\\mu_{*}(\\lambda)=\\lambda/\\lambda_{*}$ , such that $\\mu_{*}$ is the unique non-negative solution of ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mu_{*}=\\frac{n}{1+\\mathrm{Tr}(\\Sigma(\\mu_{*}\\Sigma+\\lambda)^{-1})}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Both $\\mu_{*}$ and $\\lambda_{*}$ are increasing functions with $\\lambda$ . ", "page_idx": 16}, {"type": "text", "text": "Definition 3 (Intrinsic dimension). For a covariance matrix $\\Sigma\\in\\mathbb{R}^{q\\times q}$ with eigenvalues in nonincreasing order $\\gamma_{1}^{2}\\geq\\gamma_{2}^{2}\\geq\\gamma_{3}^{2}\\geq\\cdots$ , we define the intrinsic dimension $r_{\\Sigma}(k)$ at level $k\\in\\mathbb{N}$ of $\\Sigma$ to be the intrinsic dimension of the covariance matrix \u03a3\u2265k = diag(\u03b3k2, \u03b3k2+1, . . .), i.e., the covariance matrix projected orthogonally to the top $k-1$ eigenspaces, which is given by ", "page_idx": 16}, {"type": "equation", "text": "$$\nr_{\\Sigma}(k):=\\frac{\\mathrm{Tr}(\\Sigma_{\\geq k})}{\\|\\Sigma_{\\geq k}\\|_{\\mathrm{op}}}=\\frac{\\sum_{j=k}^{q}\\gamma_{j}^{2}}{\\gamma_{k}^{2}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The intrinsic dimension of $\\Sigma_{\\geq k}$ captures the number of dimensions of $\\Sigma_{\\geq k}$ that have significant spectral content, i.e., $\\gamma_{j}^{2}\\approx\\|\\bar{\\boldsymbol{\\Sigma}_{\\geq k}}\\|_{\\mathrm{op}}$ (see [Tropp et al., 2015, Chapter 7] for further background). ", "page_idx": 16}, {"type": "text", "text": "We are given $n$ i.i.d. features $(\\pmb{f}_{i})_{i\\in[n]}$ , and we denote $\\pmb{F}\\,=\\,[\\pmb{f}_{1},\\dots,\\pmb{f}_{n}]^{\\intercal}\\,\\in\\,\\mathbb{R}^{n\\times q}$ the feature matrix. The train and test errors are functionals of the feature matrix $\\pmb{F}$ . In particular, they depend on the following resolvent matrix ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\pmb{R}=(\\pmb{F}^{\\top}\\pmb{F}+\\lambda\\mathbf{I}_{q})^{-1}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In this section we consider functionals that depend on products of $F,R$ and deterministic matrices. For a general p.s.d. matrix $A\\in\\mathbb{R}^{q\\times q}$ , define the functionals ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Phi_{1}(\\boldsymbol{F};\\boldsymbol{A},\\lambda):=\\mathrm{Tr}\\left(A\\boldsymbol{\\Sigma}^{1/2}(\\boldsymbol{F}^{\\top}\\boldsymbol{F}+\\lambda)^{-1}\\boldsymbol{\\Sigma}^{1/2}\\right),}\\\\ &{\\quad\\Phi_{2}(\\boldsymbol{F};\\lambda):=\\mathrm{Tr}\\left(\\frac{\\boldsymbol{F}^{\\top}\\boldsymbol{F}}{n}(\\boldsymbol{F}^{\\top}\\boldsymbol{F}+\\lambda)^{-1}\\right),}\\\\ &{\\Phi_{3}(\\boldsymbol{F};\\boldsymbol{A},\\lambda):=\\mathrm{Tr}\\left(A\\boldsymbol{\\Sigma}^{1/2}(\\boldsymbol{F}^{\\top}\\boldsymbol{F}+\\lambda)^{-1}\\boldsymbol{\\Sigma}(\\boldsymbol{F}^{\\top}\\boldsymbol{F}+\\lambda)^{-1}\\boldsymbol{\\Sigma}^{1/2}\\right),}\\\\ &{\\Phi_{4}(\\boldsymbol{F};\\boldsymbol{A},\\lambda):=\\mathrm{Tr}\\left(A\\boldsymbol{\\Sigma}^{1/2}(\\boldsymbol{F}^{\\top}\\boldsymbol{F}+\\lambda)^{-1}\\frac{\\boldsymbol{F}^{\\top}\\boldsymbol{F}}{n}(\\boldsymbol{F}^{\\top}\\boldsymbol{F}+\\lambda)^{-1}\\boldsymbol{\\Sigma}^{1/2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "These functionals are well approximated by quantities proportional to ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Psi_{1}(\\lambda_{*};A):=\\mathrm{Tr}\\left(A\\Sigma(\\Sigma+\\lambda_{*})^{-1}\\right),}\\\\ &{\\quad\\Psi_{2}(\\lambda_{*}):=\\frac{1}{n}\\mathrm{Tr}\\left(\\Sigma(\\Sigma+\\lambda_{*})^{-1}\\right),}\\\\ &{\\quad\\Psi_{3}(\\lambda_{*};A):=\\frac{1}{n}\\cdot\\frac{\\mathrm{Tr}(A\\Sigma^{2}(\\Sigma+\\lambda_{*})^{-2})}{n-\\mathrm{Tr}\\left(\\Sigma^{2}(\\Sigma+\\lambda_{*})^{-2}\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Without loss of generality, we can assume that $\\operatorname{Tr}(A\\Sigma)<\\infty$ for $\\Phi_{1}$ , as otherwise $\\Phi_{1}(F;A,\\lambda)=$ $\\Psi_{1}(\\mu_{*};A,\\lambda)=\\infty$ almost surely, and $\\operatorname{Tr}(A\\Sigma^{2})<\\infty$ for $\\Phi_{3}$ and $\\Phi_{4}$ , as otherwise $\\Phi_{j}(F;A,\\lambda)=$ $\\Psi_{j}(\\mu_{*};A,\\lambda)=\\infty$ , $j=3,4$ , almost surely. ", "page_idx": 17}, {"type": "text", "text": "Our relative approximation bound will depend on the covariance matrix $\\Sigma$ through ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\rho_{\\lambda}(n)=1+\\frac{n\\gamma_{\\lfloor\\eta_{*}\\cdot n\\rfloor}^{2}}{\\lambda}\\left\\{1+\\frac{r_{\\Sigma}(\\lfloor\\eta_{*}\\cdot n\\rfloor)\\vee n}{n}\\log\\left(r_{\\Sigma}(\\lfloor\\eta_{*}\\cdot n\\rfloor)\\vee n\\right)\\right\\},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\eta_{*}\\in(0,1/2)$ is a constant that will only depend on $\\mathsf{C}_{*},\\mathsf{C}_{*}$ and we used the convention that $\\gamma_{\\lfloor\\eta_{*}\\cdot n\\rfloor}^{2}=0$ if $\\lfloor\\eta_{*}\\cdot n\\rfloor>q$ . ", "page_idx": 17}, {"type": "text", "text": "The following theorem gathers the approximation guarantees for the different functionals stated above, and is obtained by modifying [Misiakiewicz and Saeed, 2024, Theorem 4]. ", "page_idx": 17}, {"type": "text", "text": "Theorem A.2 (Dimension-free deterministic equivalents). Assume the features $(\\pmb{f}_{i})_{i\\in[n]}$ satisfy Assumption $A.I$ with some constants $\\mathsf{c}_{x},\\mathsf{C}_{x},\\beta\\;>\\;0$ . For any $D,K\\,>\\,0,$ , there exist constants $\\eta\\,:=\\,\\eta_{x}\\;\\in\\;(0,1/2)$ (only depending on $\\mathsf C_{x},\\mathsf C_{x},\\beta)$ , $C_{D,K}\\,>\\,0$ (only depending on $K,D)$ , and $C_{x,D,K}>0$ (only depending on $\\mathsf{c}_{x},\\mathsf{C}_{x},\\beta,D,K)$ , such that the following holds. For all $n\\geq C_{D,K}$ and $\\lambda>0$ , if it holds that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\lambda\\cdot\\rho_{\\lambda}(n)\\geq\\|\\Sigma\\|_{\\mathrm{op}}\\cdot n^{-K},\\qquad\\rho_{\\lambda}(n)^{5/2}\\log^{3/2}(n)\\leq K\\sqrt{n},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "then for any p.s.d. matrix $\\pmb{A}$ , we have with probability at least $1-n^{-D}$ that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\left\\vert\\Phi_{1}(F;A,\\lambda)-\\frac{\\lambda_{*}}{\\lambda}\\Psi_{1}(\\lambda_{*};A)\\right\\vert\\leq C_{x,D,K}\\frac{\\rho_{\\lambda}(n)^{5/2}\\log^{3/2}(n)}{\\sqrt{n}}\\cdot\\frac{\\lambda_{*}}{\\lambda}\\Psi_{1}(\\lambda_{*};A),}&{}&{\\mathrm{(54)}}\\\\ {\\left\\vert\\Phi_{2}(F;\\lambda)-\\Psi_{2}(\\lambda_{*})\\right\\vert\\leq C_{x,D,K}\\frac{\\rho_{\\lambda}(n)^{5/2}\\log^{3/2}(n)}{\\sqrt{n}}\\Psi_{2}(\\lambda_{*}),}&{}&{\\mathrm{(53)}}\\\\ {\\left\\vert\\Phi_{3}(F;A,\\lambda)-\\left(\\frac{n\\lambda_{*}}{\\lambda}\\right)^{2}\\Psi_{3}(\\mu_{*};A,\\lambda)\\right\\vert\\leq C_{x,D,K}\\frac{\\rho_{\\lambda}(n)^{6}\\log^{5/2}(n)}{\\sqrt{n}}\\cdot\\left(\\frac{n\\lambda_{*}}{\\lambda}\\right)^{2}\\Psi_{3}(\\mu_{*};A,\\lambda),}&{}&{\\mathrm{(54)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left|\\Phi_{4}(F;A,\\lambda)-\\Psi_{3}(\\lambda_{*};A)\\right|\\leq C_{x,D,K}\\frac{\\rho_{\\lambda}(n)^{6}\\log^{3/2}(n)}{\\sqrt{n}}\\Psi_{3}(\\lambda_{*};A).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof of Theorem A.2. The only difference between this theorem and [Misiakiewicz and Saeed, 2024, Theorem 4] comes from the definition of $\\rho_{\\lambda}(n)$ . This new definition is obtained by slightly modifying the proof bounding the operator norm of $\\Sigma^{1/2}R\\Sigma^{1/2}$ from [Cheng and Montanari, 2022, Lemma 7.2] and [Misiakiewicz and Saeed, 2024, Lemma 1]. In particular, we will simply modify step 2 in the proof of [Misiakiewicz and Saeed, 2024, Lemma 1]. Consider $\\mathbf{F}_{+}\\,=\\,[\\pmb{f}_{+,1},\\hdots,\\pmb{f}_{+,n}]^{\\mathsf{T}}\\,\\in\\,\\mathbb{R}^{n\\times(q-k_{*})}$ where $\\pmb{f}_{+,i}$ correspond to the projection orthogonal to the top $k_{*}:=\\lfloor\\eta_{*}n\\rfloor-1$ eigenspaces with covariance matrix ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Sigma_{+}:=\\mathbb E[f_{+,i}f_{+,i}^{\\top}]=\\mathrm{diag}(\\gamma_{k_{*}}^{2},\\gamma_{k_{*}+1}^{2},\\ldots,\\gamma_{q}^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then, denoting $\\begin{array}{r}{{\\pmb S}=\\sum_{i\\in[n]}{\\pmb S}_{i}}\\end{array}$ with $S_{i}:=\\pmb{f}_{+,i}\\pmb{f}_{i,+}^{\\top}$ , we have with probability at least $1-n^{-D}$ , for any $i\\in[n]$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|S_{i}\\|_{\\mathrm{op}}\\leq\\mathrm{Tr}(\\Sigma_{+})+C_{*,D}\\cdot\\log(n)\\sqrt{\\gamma_{k_{*}}^{2}\\mathrm{Tr}(\\Sigma_{+})}\\leq\\mathrm{Tr}(\\Sigma_{+})\\left(1+C_{*,D}\\frac{\\log(n)}{\\sqrt{r_{\\Sigma}(k_{*})}}\\right)=:L_{n}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Denoting $\\widetilde{\\pmb S}\\,=\\,\\textstyle\\sum_{i\\in[n]}\\widetilde{\\pmb S}_{i}$ with $\\widetilde{\\pmb{S}}_{i}\\;:=\\;{\\pmb{S}}_{i}\\mathbb{1}_{\\|{\\pmb{S}}_{i}\\|_{\\mathrm{op}}\\leq L_{n}}$ , so that $\\widetilde{\\boldsymbol{S}}\\,=\\,\\boldsymbol{S}$ with probability at least $1-n^{-D}$ , we obtain ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\widetilde{\\boldsymbol{S}}\\|_{\\mathrm{op}}\\leq n L_{n}\\gamma_{k_{*}}^{2}=:v_{n},\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\|\\mathbb{E}[\\widetilde{\\boldsymbol{S}}]\\|_{\\mathrm{op}}\\leq n\\|\\mathbb{E}[\\boldsymbol{S}_{i}]\\|_{\\mathrm{op}}=n\\gamma_{k_{*}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, applying the matrix Bernstein\u2019s inequality with intrinsic dimension [Tropp et al., 2015, Theorem 7.3.1] to $\\widetilde{S}$ gives that with probability at least $1-n^{-D}$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\widetilde{S}\\|_{\\mathrm{op}}\\leq n\\gamma_{k_{*}}^{2}+C_{D}\\left(\\sqrt{v_{n}}+L_{n}\\right)\\sqrt{\\log(r_{\\Sigma}(k_{*})n)}}\\\\ &{\\qquad\\leq n\\gamma_{k_{*}}^{2}+C_{D}L_{n}\\log(r_{\\Sigma}(k_{*})n)}\\\\ &{\\qquad\\leq n\\gamma_{k_{*}}^{2}\\left\\{1+\\frac{r_{\\Sigma}\\left(k_{*}\\right)}{n}\\left(1+C_{*,D}\\frac{\\log(n)}{\\sqrt{r_{\\Sigma}\\left(k_{*}\\right)}}\\right)\\log(r_{\\Sigma}(k_{*})n)\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that by the condition of our theorem, $\\log(n)\\leq K{\\sqrt{n}}$ , and therefore ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|\\widetilde{\\mathbf{S}}\\|_{\\mathrm{op}}\\leq C_{*,D,K}\\cdot n\\gamma_{k_{*}}^{2}\\left\\{1+\\frac{r_{\\Sigma}(k_{*})}{n}\\left(1+\\sqrt{\\frac{n}{r_{\\Sigma}(k_{*})}}\\right)\\log(r_{\\Sigma}(k_{*})n)\\right\\}}\\\\ {\\displaystyle\\leq C_{*,D,K}\\cdot n\\gamma_{k_{*}}^{2}\\left\\{1+\\frac{r_{\\Sigma}(k_{*})\\vee n}{n}\\log(r_{\\Sigma}(k_{*})\\vee n)\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Following the rest of the argument in [Misiakiewicz and Saeed, 2024, Lemma 1] we obtain $\\rho_{\\lambda}(n)$ in Eq. (52). \u53e3 ", "page_idx": 18}, {"type": "text", "text": "B Proof of the deterministic equivalent for RFRR ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this appendix, we prove the approximation guarantees stated in Theorem 3.3 between the test error of RFRR and its deterministic equivalent. We start in Section B.1 by introducing background and notations that we will use throughout the proof. Section B.2 introduces key results on the covariance matrix and the fixed points. We then leverage these results to prove deterministic equivalents for different functionals of $Z=(\\sigma(\\langle\\pmb{x}_{i},\\pmb{w}_{j}\\rangle))_{i\\in[n],j\\in[p]}\\in\\mathbb{R}^{n\\times p}$ conditional on $(w_{j})_{j\\in[p]}$ in Section B.3, and functionals of $\\pmb{F}=(\\xi_{k}\\phi_{k}(\\pmb{w}_{j}))_{j\\in[p],k\\geq1}\\in\\mathbb{R}^{p\\times\\infty}$ in Section B.4. Given these deterministic equivalents, we prove our approximation guarantees for the variance term in Section B.5, and for the bias term in B.6. Finally, we deffer the proof of some technical results to Section B.7. ", "page_idx": 18}, {"type": "text", "text": "B.1 Preliminaries ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Recall that throughout the paper, we will keep track of the parameters of the problem $(n,p,\\Sigma,\\lambda,\\sigma_{\\varepsilon}^{2})$ . For the other constants $\\mathsf{C}_{*},K,D$ , we will denote $C_{a_{1},a_{2},\\ldots,a_{k}}$ constants that only depend on the values of $\\{a_{i}\\}_{i\\in[k]}$ . We use $a_{i}=\\sqrt[\\object Object] $ to denote the dependency on the constant $\\mathsf C_{*}$ appearing in Assumption 3.1 and Assumption 3.2. ", "page_idx": 18}, {"type": "text", "text": "Throughout this appendix, we will directly work in the \u2018feature space\u2019 ", "page_idx": 18}, {"type": "equation", "text": "$$\ng_{i}:=(\\psi_{k}(\\pmb{x}_{i}))_{k\\geq1},\\qquad\\quad\\mathrm{and}\\qquad\\quad\\pmb{f}_{j}:=(\\xi_{k}\\phi_{k}(\\pmb{w}_{j}))_{k\\geq1},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "with distribution induced by $x_{i}\\sim\\mu_{x}$ and $w_{j}\\sim\\mu_{w}$ . We will denote the covariate feature and weight feature matrices by ", "page_idx": 18}, {"type": "equation", "text": "$$\nG:=[g_{1},\\ldots,g_{n}]^{\\mathsf{T}}\\in\\mathbb{R}^{n\\times\\infty},\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ F:=[f_{1},\\ldots,f_{p}]^{\\mathsf{T}}\\in\\mathbb{R}^{p\\times\\infty}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We denote the random feature weight vector ", "page_idx": 18}, {"type": "equation", "text": "$$\nz_{i}:=\\frac{1}{\\sqrt{p}}[\\sigma(\\langle{\\pmb w}_{1},{\\pmb x}_{i}\\rangle),\\dots,\\sigma(\\langle{\\pmb w}_{p},{\\pmb x}_{i}\\rangle)]=\\frac{1}{\\sqrt{p}}{\\pmb F}{\\pmb g}_{i}\\in\\mathbb{R}^{p},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and the associated feature matrix ", "page_idx": 18}, {"type": "equation", "text": "$$\nZ=[z_{1},\\ldots,z_{n}]^{\\mathsf{T}}={\\frac{1}{\\sqrt{p}}}G F^{\\mathsf{T}}\\in\\mathbb{R}^{n\\times p}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that $\\pmb{f}$ has covariance matrix ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Sigma:=\\mathbb{E}[f f^{\\top}]=\\mathrm{diag}(\\xi_{1}^{2},\\xi_{2}^{2},\\xi_{3}^{2},\\cdot\\cdot).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We will further introduce the covariance matrix of $_{\\textit{z}}$ conditional on the weight feature matrix $\\pmb{F}$ (i.e., conditional on $(w_{j})_{j\\in[p]},$ ) ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\widehat{\\Sigma}_{F}:=\\mathbb{E}_{z}\\left[z z^{\\mathsf{T}}\\Big|F\\right]=\\frac{1}{p}\\mathbf{F}\\mathbf{F}^{\\mathsf{T}}\\in\\mathbb{R}^{p\\times p}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that under Assumption 3.1, the features $_{z}$ and $\\pmb{f}$ satisfy the following assumption. ", "page_idx": 19}, {"type": "text", "text": "Assumption B.1 (Concentration of the features $_{z}$ and $\\boldsymbol{\\textbf{\\textit{f}}}$ ). There exists a constant $\\mathsf{C}_{*}\\,>\\,0$ such that for any weight feature matrix $\\pmb{F}\\,\\in\\,\\mathbb{R}^{p\\times\\infty}$ and deterministic $p$ .s.d. matrix $\\boldsymbol{A}\\,\\in\\,\\mathbb{R}^{p\\times p}$ with $\\mathrm{Tr}(\\dot{\\widehat{\\Sigma}}_{F}A)<\\infty$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}_{z|F}\\left(\\left|z^{\\top}A z-\\mathrm{Tr}(\\widehat{\\Sigma}_{F}A)\\right|\\geq t\\cdot\\left\\|\\widehat{\\Sigma}_{F}^{1/2}A\\widehat{\\Sigma}_{F}^{1/2}\\right\\|_{F}\\right)\\leq\\mathsf{C}_{*}\\exp\\left\\{-t/\\mathsf{C}_{x}\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and for any deterministic p.s.d. matrix $\\b{B}\\in\\mathbb{R}^{\\infty\\times\\infty}$ with $\\mathrm{Tr}(\\Sigma B)<\\infty,$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}_{f}\\left(\\left|f^{\\top}B f-\\operatorname{Tr}(\\Sigma B)\\right|\\geq t\\cdot\\left\\|\\Sigma^{1/2}B\\Sigma^{1/2}\\right\\|_{F}\\right)\\leq\\mathsf{C}_{*}\\exp\\left\\{-t/\\mathsf{C}_{x}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We will assume in the rest of this appendix that Assumption B.1 holds. Using the notations introduced above, we restate our setting below. Recall that we consider learning a target function $h_{*}(g):=g^{\\mathsf{T}}\\beta_{*}$ from i.i.d. samples $(y_{i},\\pmb{g}_{i})_{i\\in[n]}$ with ", "page_idx": 19}, {"type": "equation", "text": "$$\ny_{i}=\\pmb{g}_{i}^{\\top}\\beta_{*}+\\varepsilon_{i},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\varepsilon_{i}$ are independent noise with $\\mathbb{E}[\\varepsilon_{i}]=0$ and $\\mathbb{E}[\\varepsilon_{i}^{2}]=\\sigma_{\\varepsilon}^{2}$ . Denote $\\pmb{y}=(y_{1},\\dots,y_{n})$ the vector containing the labels. We fti this data using a random feature model with i.i.d. random weight features $({\\pmb{f}}_{j})_{j\\in[p]}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\hat{f}(\\pmb{g})=\\frac{1}{\\sqrt{p}}\\pmb{g}^{\\top}\\pmb{F}^{\\top}\\pmb{a},\\qquad\\quad\\pmb{a}\\in\\mathbb{R}^{p}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We fit the parameter $\\textbf{\\em a}$ using random feature ridge regression (RFRR) ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\hat{\\pmb{a}}_{\\lambda}=\\underset{\\pmb{a}\\in\\mathbb{R}^{p}}{\\arg\\operatorname*{min}}\\left\\lbrace\\|\\pmb{y}-\\pmb{Z}\\pmb{a}\\|_{2}^{2}+\\lambda\\|\\pmb{a}\\|_{2}^{2}\\right\\rbrace=(\\pmb{Z}^{\\mathsf{T}}\\pmb{Z}+\\lambda)^{-1}\\pmb{Z}^{\\mathsf{T}}\\pmb{y}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The test error is then given by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\mathcal{R}_{\\mathrm{test}}(h_{*};G,F,\\lambda):=\\mathbb{E}_{\\varepsilon}\\left\\{\\mathbb{E}_{g}\\left[\\left(g^{\\mathsf{T}}\\beta_{*}-\\frac{1}{\\sqrt{p}}g^{\\mathsf{T}}F^{\\mathsf{T}}\\hat{a}_{\\lambda}\\right)^{2}\\right]\\right\\}}}\\\\ {{=\\mathcal{B}(\\beta_{*};G,F,\\lambda)+\\mathcal{V}(G,F,\\lambda),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the bias and variance terms are given explicitly by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{B}(\\beta_{*};G,F,\\lambda)=\\|\\beta_{*}-p^{-1/2}F^{\\mathsf{T}}(Z^{\\mathsf{T}}Z+\\lambda)^{-1}Z^{\\mathsf{T}}G\\beta_{*}\\|_{2}^{2},}\\\\ {\\quad\\quad\\mathcal{V}(G,F,\\lambda)=\\sigma_{\\varepsilon}^{2}\\cdot\\mathrm{Tr}\\big(\\widehat{\\Sigma}_{F}Z^{\\mathsf{T}}Z(Z^{\\mathsf{T}}Z+\\lambda)^{-2}\\big).\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that both the bias and variance terms are random quantities that depend on the random matrices $G,F$ . The goal of this appendix is to prove non-asymptotic and multiplicative approximation guarantees between these two terms and deterministic quantities that only depend on the parameters of the model $(n,p,\\Sigma,\\beta_{*},\\lambda,\\sigma_{\\varepsilon}^{2})$ , i.e., we will show that with high probability ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\mathcal{B}(\\beta_{*};G,F,\\lambda)-\\mathsf{B}_{n,p}(\\beta_{*},\\lambda)|=\\widetilde{O}\\left(n^{-1/2}+p^{-1/2}\\right)\\cdot\\mathsf{B}_{n,p}(\\beta_{*},\\lambda),}\\\\ &{\\qquad\\quad|\\mathcal{V}(G,F,\\lambda)-\\mathsf{V}_{n,p}(\\lambda)|=\\widetilde{O}\\left(n^{-1/2}+p^{-1/2}\\right)\\cdot\\mathsf{V}_{n,p}(\\lambda),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\mathsf{B}_{n,p}(\\beta_{*},\\lambda)$ and $\\mathsf{V}_{n,p}(\\lambda)$ are defined in Eqs (22) and (23), and the approximation rates $\\widetilde O(\\cdot)$ are explicit in terms of the model parameters. ", "page_idx": 19}, {"type": "text", "text": "The proof of these approximation guarantees will proceed in two steps. We first show that the bias and variance terms conditional on $\\pmb{F}$ are well approximated by functionals that only depend on $\\pmb{F}$ , i.e., ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\mathcal{B}(\\beta_{*};G,F,\\lambda)-\\widetilde{\\mathcal{B}}(\\beta_{*};F,\\lambda)\\right|=\\widetilde{O}\\left(n^{-1/2}\\right)\\cdot\\widetilde{\\mathcal{B}}(\\beta_{*};F,\\lambda),}\\\\ &{\\qquad\\qquad\\left|\\gamma(G,F,\\lambda)-\\widetilde{\\mathcal{V}}(F,\\lambda)\\right|=\\widetilde{O}\\left(n^{-1/2}\\right)\\cdot\\widetilde{\\mathcal{V}}(F,\\lambda).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We then show that $\\widetilde{\\cal B}(\\beta_{*};{\\cal F},\\lambda)$ and $\\widetilde{\\mathcal{V}}(\\boldsymbol{F},\\lambda)$ are well approximated by $\\mathsf{B}_{n,p}(\\beta_{*},\\lambda)$ and $\\mathsf{V}_{n,p}(\\lambda)$ with ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left|{\\widetilde{\\beta}}(\\beta_{*};F,\\lambda)-{\\mathsf B}_{n,p}(\\beta_{*},\\lambda)\\right|=\\widetilde{O}\\left(p^{-1/2}\\right)\\cdot{\\mathsf B}_{n,p}(\\beta_{*},\\lambda),}&{}\\\\ {\\left|\\widetilde{\\gamma}(F,\\lambda)-{\\mathsf V}_{n,p}(\\lambda)\\right|=\\widetilde{O}\\left(p^{-1/2}\\right)\\cdot{\\mathsf V}_{n,p}(\\lambda).}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For each of these two steps, we will apply general results showing deterministic equivalents for functionals of (possibly infinite-dimensional) random matrices proved in Misiakiewicz and Saeed [2024] (see Appendix A and in particular Theorem A.2 for some background). Note that when writing the proof, we will directly show Eq. (66) assuming that $\\pmb{F}$ is in some good event $F\\in\\mathcal A_{\\mathcal F}$ , where $\\mathbb{P}_{F}(\\lambda_{\\mathcal{F}})\\geq1-p^{-D}$ , so that we can immediately write functionals with regularization parameter that does not depend on $\\widehat{\\Sigma}_{F}$ , and therefore the rate will be directly $\\widetilde{O}\\left(n^{-1/2}+p^{-1/2}\\right)$ . However, we can reorganize the proof to indeed get the separate contributions (66) and (67) to the approximation error rate. ", "page_idx": 20}, {"type": "text", "text": "The rest of Appendix B is devoted to implementing this proof strategy. We start in the next three sections by introducing key technical results which we will use in the analysis of the bias and variance terms. ", "page_idx": 20}, {"type": "text", "text": "B.2 Fixed points, feature covariance matrix, and tail rank ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Recall that our deterministic equivalents will depend on the fixed points $(\\nu_{1},\\nu_{2})\\in\\mathbb{R}_{>0}^{2}$ stated in Definition 1. Furthermore, our approximation guarantees will depend on the covariance matrix $\\Sigma$ through $\\rho_{\\kappa}(p)$ and $\\widetilde{\\rho}_{\\kappa}(n,p)$ which we restate below for convenience ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~M_{\\Sigma}(k)=1+\\frac{r_{\\Sigma}(\\lfloor\\eta_{*}\\cdot k\\rfloor)\\vee k}{k}\\log\\left(r_{\\Sigma}(\\lfloor\\eta_{*}\\cdot k\\rfloor)\\vee k\\right),}\\\\ &{~~~\\rho_{\\kappa}(p)=1+\\frac{p\\cdot\\xi_{\\lfloor\\eta_{*}\\cdot p\\rfloor}^{2}}{\\kappa}M_{\\Sigma}(p),}\\\\ &{~~~\\widetilde{\\rho}_{\\kappa}(n,p)=1+\\mathbb{1}\\left[n\\leq p/\\eta_{*}\\right]\\cdot\\left\\{\\frac{n\\xi_{\\lfloor\\eta_{*}\\cdot n\\rfloor}^{2}}{\\kappa}+\\frac{n}{p}\\cdot\\rho_{\\kappa}(p)\\right\\}M_{\\Sigma}(n),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\eta_{*}\\in(0,1/4)$ is a constant that only depends on ${\\mathsf{C}}_{*}$ appearing in Assumption B.1, and $r_{\\Sigma}(k)$ is the intrinsic dimension of $\\Sigma$ at level $k$ (see Definition 3) ", "page_idx": 20}, {"type": "equation", "text": "$$\nr_{\\Sigma}(k)=\\frac{\\sum_{j=k}^{p}\\xi_{j}^{2}}{\\xi_{k}^{2}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Observe that $\\widetilde{\\rho}_{\\kappa}(\\boldsymbol{n},\\boldsymbol{p})$ is well defined for $n\\to\\infty$ while $p$ stays constant with $\\widetilde{\\rho}_{\\kappa}(\\infty,p)=1$ , and for $p\\rightarrow\\infty$ w h ile $n$ stays constant with $\\widetilde{\\rho}_{\\kappa}(n,\\infty)=\\rho_{\\kappa}(n)$ (under the condition s  in our setting that $\\rho_{\\kappa}(p)\\leq K\\sqrt{p}$ for some constant $K$ ). ", "page_idx": 20}, {"type": "text", "text": "In this section, we introduce and prove properties on the fixed point $\\left(\\nu_{1},\\nu_{2}\\right)\\in\\mathbb{R}_{>0}^{2}$ and the weight feature matrix $\\pmb{F}$ which we will use to prove deterministic equivalents. ", "page_idx": 20}, {"type": "text", "text": "Feature covariance matrix. The features $z_{i}\\in\\mathbb{R}^{p}$ conditional on $\\pmb{F}$ are i.i.d. random vectors with covariance $\\widehat{\\Sigma}_{F}=F F^{\\top}/p$ . We first show that with high probability over $\\pmb{F}$ , this feature covariance matrix has  eigenvalues and intrinsic dimensions bounded by the ones of $\\Sigma$ . ", "page_idx": 20}, {"type": "text", "text": "Denote $(\\hat{\\xi}_{1}^{2},\\hat{\\xi}_{2}^{2},\\dots\\hat{\\xi}_{p}^{2})$ the $p$ eigenvalues of $\\widehat{\\Sigma}_{F}$ in nonincreasing order. Applying the definition of the intrinsic dimension at level $k$ to $\\widehat{\\Sigma}_{F}$ , we have for any $k=1,\\hdots,p$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\nr_{\\widehat{\\Sigma}_{F}}(k)=\\frac{\\sum_{j=k}^{p}\\hat{\\xi}_{j}^{2}}{\\hat{\\xi}_{k}^{2}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Applying Theorem A.2 directly to functionals of $Z$ conditional on $\\pmb{F}$ , the approximation guarantees depend on ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\widehat{\\rho}_{\\lambda}(n)=1+\\frac{n\\cdot\\widehat{\\xi}_{[\\eta_{*}\\cdot n]}^{2}}{\\lambda}\\left\\{1+\\frac{r_{\\widehat{\\Sigma}_{F}}(\\lfloor\\eta_{*}\\cdot n\\rfloor)\\vee n}{n}\\log\\left(r_{\\widehat{\\Sigma}_{F}}(\\lfloor\\eta_{*}\\cdot n\\rfloor)\\vee n\\right)\\right\\},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which simply corresponds to $\\rho_{\\lambda}$ defined in Eq. (52) applied to $\\widehat{\\Sigma}_{F}$ where we recall that $\\hat{\\xi}_{\\lfloor\\eta_{*}\\cdot n\\rfloor}^{2}=0$ if . The next lemma shows that with high probability over , we have $\\widehat{\\rho}_{\\lambda}(n)\\overset{=}{\\underset{\\sim}{\\sim}}\\widetilde{\\rho}_{\\lambda}\\overset{\\cdot}{(n,p)}$ for all $n\\in\\mathbb{N}$ . ", "page_idx": 21}, {"type": "text", "text": "Lemma B.2 (Feature covariance matrix). Assume the feature vectors $\\{f_{j}\\}_{j\\in[p]}$ satisfy Assumption B.1. Then for any $D,K>0,$ , there exist constants $\\eta_{*}\\,\\in\\,(0,1/4)$ and $C_{*,D,K}\\,>\\,0$ such that the following holds. For any $p\\geq C_{*,D,K}.$ , the event ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\widetilde{A}_{\\mathcal{F}}=\\left\\{F\\in\\mathbb{R}^{p\\times\\infty}\\,:\\,\\,||\\widehat{\\Sigma}_{F}||_{\\mathrm{op}}\\geq\\frac{1}{2},\\,\\,\\,\\,\\widehat{\\rho}_{\\lambda}(n)\\leq C_{*,D,K}\\cdot\\widetilde{\\rho}_{\\lambda}(n,p),\\,\\,\\,\\forall n\\in\\mathbb{N},\\lambda\\in\\mathbb{R}\\right\\}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "holds with probability at least $1-p^{-D}$ . ", "page_idx": 21}, {"type": "text", "text": "We defer the proof of this lemma to Section B.7.1. ", "page_idx": 21}, {"type": "text", "text": "High-degree part of the feature matrix $\\pmb{F}$ . Recall that $(\\nu_{1},\\nu_{2})\\in\\mathbb{R}_{>0}^{2}$ are the solutions to the fixed point equations stated in Definition 1. In order to get approximation guarantees when $p\\nu_{1}\\to0$ as $n\\to\\infty$ , we will analyze separately the top eigenspaces of $\\pmb{F}$ from the rest. For an integer $\\mathsf{m}\\in\\mathbb{N}$ , we split the feature vector $\\pmb{f}_{j}=[\\pmb{f}_{0,j},\\pmb{f}_{+,j}]$ where $\\pmb{f}_{0,j}\\in\\mathbb{R}^{m}$ corresponds to the top $\\mathsf{m}$ coordinates with covariance ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Sigma_{0}:=\\mathbb{E}[f_{0,j}f_{0,j}^{\\top}]=\\mathrm{diag}(\\xi_{1}^{2},\\xi_{2}^{2},\\ldots,\\xi_{\\mathsf{m}}^{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and $\\pmb{f}_{+,j}\\in\\mathbb{R}^{\\infty}$ corresponds to the high degree features orthogonal to the top $\\mathsf{m}$ eigenspaces. We denote their covariance ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Sigma_{+}:=\\mathbb{E}[f_{+,j}f_{+,j}^{\\top}]=\\mathrm{diag}(\\xi_{\\mathfrak{m}+1}^{2},\\xi_{\\mathfrak{m}+2}^{2},\\cdot\\cdot).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We split the weight feature matrix intro $\\pmb{F}=[\\pmb{F}_{0},\\pmb{F}_{+}]$ , where ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\pmb{F}_{0}=[\\pmb{f}_{0,1},\\ldots,\\pmb{f}_{0,p}]^{\\mathsf{T}}\\in\\mathbb{R}^{p\\times m},\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\pmb{F}_{+}=[\\pmb{f}_{+,1},\\ldots,\\pmb{f}_{+,p}]^{\\mathsf{T}}\\in\\mathbb{R}^{p\\times\\infty}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We will use that for $\\mathsf{m}$ chosen such that $p\\cdot\\xi_{\\mathfrak{m}+1}\\ll\\operatorname{Tr}(\\Sigma_{+})$ , we have with high probability ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|F_{+}F_{+}^{\\mathsf{T}}-\\operatorname{Tr}(\\Sigma_{+})\\cdot\\mathbf{I}_{p}\\|_{\\mathrm{op}}\\lesssim\\sqrt{p\\cdot\\xi_{\\mathsf{m}+1}\\mathrm{Tr}(\\Sigma_{+})}\\ll\\operatorname{Tr}(\\Sigma_{+}),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and therefore ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{F F^{\\top}+\\kappa\\approx F_{0}F_{0}^{\\top}+\\gamma(\\kappa),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where we defined the function ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\gamma(\\kappa):=\\kappa+\\operatorname{Tr}(\\Sigma_{+}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "To simplify the final statement of our results, we assume that we can choose m such that $p^{2}\\xi_{m+1}^{2}\\leq$ $\\gamma(p\\lambda/\\bar{n})$ . Note that $\\nu_{1}\\,\\geq\\,\\lambda/n$ from the fixed point equations and $\\gamma(p\\lambda/n)\\,\\leq\\,\\gamma(p\\nu_{1})$ (e.g., see Equation (76)). For convenience, we will further denote ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\gamma_{+}:=\\gamma(p\\nu_{1}),\\qquad\\qquad\\gamma_{\\lambda}:=\\gamma(p\\lambda/n).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Lemma B.3 (Concentration of high-degree part of $\\pmb{F}$ ). Assume that $(f_{+,j})_{j\\in[p]}$ satisfy Assumption B.1 and $\\mathsf{m}\\in\\mathbb{N}$ is chosen such that $p^{2}\\xi_{\\mathfrak{m}+1}^{2}\\leq\\gamma(p\\lambda/n)$ . Then for any $D>0$ , there exists a constant $C_{*,D}>0$ such that with probability at least $1-p^{-D}$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|F_{+}F_{+}^{\\top}-\\operatorname{Tr}(\\Sigma_{+})\\cdot\\mathbf{I}_{p}\\|_{\\mathrm{op}}\\leq C_{*,D}\\frac{\\log^{3}(p)}{\\sqrt{p}}\\gamma_{\\lambda}\\leq C_{*,D}\\frac{\\log^{3}(p)}{\\sqrt{p}}\\gamma_{+}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This lemma follows directly from [Misiakiewicz and Saeed, 2024, Proposition 9]. ", "page_idx": 21}, {"type": "text", "text": "Effective regularization and fixed points. Conditional on $\\pmb{F}$ , the bias and variance are functionals of the random matrix $_{z}$ which has $n$ i.i.d. rows with regularization parameter $\\lambda$ and covariance $\\widehat{\\Sigma}_{F}$ . The deterministic approximations to these functionals depend on an \u201ceffective regularization\u201d $\\tilde{\\nu}_{1}$ associated to $(n,\\widehat{\\Sigma}_{F}^{^{-}},\\lambda)$ (see Definition 2 in Appendix A for background) which is given as the unique non-negative solution to the equation ", "page_idx": 21}, {"type": "equation", "text": "$$\nn-\\frac{\\lambda}{\\widetilde{\\nu}_{1}}=\\mathrm{Tr}\\big(\\widehat{\\Sigma}_{F}\\big(\\widehat{\\Sigma}_{F}+\\widetilde{\\nu}_{1}\\big)^{-1}\\big).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Note that the right-hand side can be rewritten as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{Tr}\\big(\\widehat{\\Sigma}_{\\boldsymbol{F}}\\big(\\widehat{\\Sigma}_{\\boldsymbol{F}}+\\tilde{\\nu}_{1}\\big)^{-1}\\big)=\\mathrm{Tr}\\big(\\boldsymbol{F}\\boldsymbol{F}^{\\intercal}(\\boldsymbol{F}\\boldsymbol{F}^{\\intercal}+p\\tilde{\\nu}_{1})^{-1}\\big),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which is itself of functional of the random matrix $\\pmb{F}$ which has $p$ i.i.d. rows with regularization parameter $p\\tilde{\\nu}_{1}$ and covariance $\\Sigma$ . Note that $\\tilde{\\nu}_{1}$ is a random variable depending itself on $\\pmb{F}$ . However we will show that it concentrates on a deterministic value $\\nu_{1}$ . We therefore introduce a second effective regularization $\\nu_{2}$ associated to $(p,\\Sigma,p\\nu_{1})$ given as the unique non-negatice solution of the equation ", "page_idx": 22}, {"type": "equation", "text": "$$\np-{\\frac{p\\nu_{1}}{\\nu_{2}}}=\\mathrm{Tr}\\left(\\Sigma(\\Sigma+\\nu_{2})^{-1}\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The functional (75) is then well approximated by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{Tr}\\big(F F^{\\top}(F F^{\\top}+p\\tilde{\\nu}_{1})^{-1}\\big)=\\big(1+o_{p,\\mathbb{P}}(1)\\big)\\cdot\\mathrm{Tr}\\big(\\Sigma(\\Sigma+\\nu_{2})^{-1}\\big).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This motivates to define $\\left(\\nu_{1},\\nu_{2}\\right)\\in\\mathbb{R}_{>0}^{2}$ as the unique non-negative solutions to the coupled fixed point equations ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{n-\\displaystyle\\frac{\\lambda}{\\nu_{1}}=\\mathrm{Tr}\\left(\\Sigma(\\Sigma+\\nu_{2})^{-1}\\right),}}\\\\ {{p-\\displaystyle\\frac{p\\nu_{1}}{\\nu_{2}}=\\mathrm{Tr}\\left(\\Sigma(\\Sigma+\\nu_{2})^{-1}\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Writing $\\nu_{1}$ as a function of $\\nu_{2}$ indeed produces the equations stated in Definition 1. ", "page_idx": 22}, {"type": "text", "text": "To show that $\\tilde{\\nu}_{1}$ concentrates on $\\nu_{1}$ , we define the following fixed points $(\\tilde{\\nu}_{1},\\tilde{\\nu}_{2})\\in\\mathbb{R}_{>0}^{2}$ to be the unique positive solutions to the random equations ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{n-\\displaystyle\\frac{\\lambda}{\\widetilde{\\nu}_{1}}=\\mathrm{Tr}\\big(\\widehat{\\Sigma}_{F}\\big(\\widehat{\\Sigma}_{F}+\\widetilde{\\nu}_{1}\\big)^{-1}\\big),}}\\\\ {{p-\\displaystyle\\frac{p\\widetilde{\\nu}_{1}}{\\widetilde{\\nu}_{2}}=\\mathrm{Tr}\\left(\\Sigma(\\Sigma+\\widetilde{\\nu}_{2})^{-1}\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The following proposition shows that $(\\tilde{\\nu}_{1},\\tilde{\\nu}_{2})$ is well approximated by $\\left(\\nu_{1},\\nu_{2}\\right)$ with high probability. ", "page_idx": 22}, {"type": "text", "text": "Proposition B.4 (Concentration of the fixed points). Assume that $({\\pmb{f}}_{j})_{j\\in[p]}$ satisfy Assumption B.1. Then for any $D,K>0,$ , there exist constants $\\eta_{*}\\in(0,1/4)$ and $C_{*,D,K}>0$ such that the following holds. Let $\\rho_{\\kappa}(p)$ and $\\widetilde{\\rho}_{\\kappa}(n,p)$ be defined as per Eqs. (26) and (25), and $\\gamma_{\\lambda}$ and $\\gamma_{+}$ as per Eq. (74). For any $p\\geq C_{*,D,K}$ a nd $\\lambda>0$ , if it holds that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\gamma_{\\lambda}\\geq p^{-K},\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\widetilde{\\rho}_{\\lambda}(n,p)\\cdot\\rho_{\\gamma_{\\lambda}}(p)^{5/2}\\log^{4}(p)\\leq K\\sqrt{p},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "then with probability at least $1-p^{-D}$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\left\\{\\frac{|\\tilde{\\nu}_{2}-\\nu_{2}|}{\\nu_{2}},\\frac{|\\tilde{\\nu}_{1}-\\nu_{1}|}{\\nu_{1}}\\right\\}\\leq C_{*,D,K}\\cdot\\mathcal{E}_{\\nu}(p),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where we defined ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{E}_{\\nu}(p):=\\frac{\\widetilde{\\rho}_{\\lambda}(n,p)\\cdot\\rho_{\\gamma_{+}}(p)^{5/2}\\log^{3}(p)}{\\sqrt{p}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The proof of this proposition can be found in Section B.7.2. ", "page_idx": 22}, {"type": "text", "text": "To study functionals of $_{z}$ conditional on $\\pmb{F}$ , we will assume that $\\pmb{F}$ belonds to the good event ", "page_idx": 22}, {"type": "equation", "text": "$$\nA_{\\mathcal{F}}:=\\widetilde{A}_{\\mathcal{F}}\\cap\\left\\{\\pmb{F}\\in\\mathbb{R}^{p\\times\\infty}\\enspace:\\enspace|\\tilde{\\nu}_{1}-\\nu_{1}|\\leq C_{*,D,K}\\cdot\\mathcal{E}_{\\nu}(p)\\cdot\\nu_{1}\\right\\},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\widetilde{\\mathcal{A}}_{\\mathcal{F}}$ is defined in Lemma B.2. In particular, as long as $p\\geq C_{*,D,K}$ and condition (78) hold, then $\\mathbb{P}(\\boldsymbol{A}_{\\mathcal{F}})\\geq1-2p^{-D}$ by Lemma B.2 and Proposition B.4. ", "page_idx": 22}, {"type": "text", "text": "Truncated fixed point. As mentioned above, we will separate the analysis of the low-degree part of the feature matrix $\\scriptstyle{F_{0}}$ and the high-degree part $F_{+}$ . For the high-degree part, we will simply use the concentration stated in Lemma B.3. For the low degree part, we will study functional of $\\scriptstyle{F_{0}}$ with regularization $\\gamma_{+}=p\\nu_{1}+\\mathrm{Tr}(\\Sigma_{+})$ . We therefore introduce an effective regularization $\\nu_{2,0}$ associated to the model $(p,\\Sigma_{0},\\gamma_{+})$ , i.e., the unique positive solution to the equation ", "page_idx": 23}, {"type": "equation", "text": "$$\np-\\frac{\\gamma_{+}}{\\nu_{2,0}}=\\mathrm{Tr}\\bigl(\\Sigma_{0}(\\Sigma_{0}+\\nu_{2,0})^{-1}\\bigr).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Intuitively, $\\nu_{2,0}$ will be closed to $\\nu_{2}$ as soon as $\\lambda_{\\operatorname*{max}}(\\Sigma_{+})\\ll\\nu_{2}$ as ", "page_idx": 23}, {"type": "equation", "text": "$$\nn-\\frac{p\\nu_{1}}{\\nu_{2}}\\approx\\mathrm{Tr}\\left(\\Sigma_{0}(\\Sigma_{0}+\\nu_{2})^{-1}\\right)+\\frac{\\mathrm{Tr}(\\Sigma_{+})}{\\nu_{2}},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and uniqueness of the positive solution $\\nu_{2,0}$ . This is formalized in the following lemma. ", "page_idx": 23}, {"type": "text", "text": "Lemma B.5 (Truncated fixed point). Let m be chosen such that $p^{2}\\xi_{\\mathsf{m}+1}^{2}\\leq\\gamma_{\\lambda}$ , then ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{|\\nu_{2,0}-\\nu_{2}|}{\\nu_{2}}\\leq\\frac{1}{p}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Furthermore, there exists an absolute constant $C>0$ such that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\Biggl|\\mathrm{Tr}\\bigl(\\Sigma_{0}(\\Sigma_{0}+\\nu_{2,0})^{-1}\\bigr)+\\frac{\\mathrm{Tr}(\\Sigma_{+})}{\\nu_{2,0}}-\\mathrm{Tr}\\bigl(\\Sigma(\\Sigma+\\nu_{2})^{-1}\\bigr)\\Biggr|\\leq\\frac{C}{p}\\mathrm{Tr}\\bigl(\\Sigma(\\Sigma+\\nu_{2})^{-1}\\bigr).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof of Lemma B.5. The first bound (81) follows directly from [Misiakiewicz and Saeed, 2024, Lemma 6]. For the second inequality, we decompose this difference into ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left|\\mathrm{Tr}\\big(\\Sigma_{0}(\\Sigma_{0}+\\nu_{2,0})^{-1}\\big)+\\frac{\\mathrm{Tr}\\big(\\Sigma_{+}\\big)}{\\nu_{2,0}}-\\mathrm{Tr}\\big(\\Sigma(\\Sigma+\\nu_{2})^{-1}\\big)\\right|}\\\\ &{\\leq|\\nu_{2,0}-\\nu_{2}|\\,\\mathrm{Tr}\\big(\\Sigma_{0}(\\Sigma_{0}+\\nu_{2,0})^{-1}(\\Sigma_{0}+\\nu_{2})^{-1}\\big)+\\frac{\\xi_{\\mathfrak m+1}^{2}+|\\nu_{2,0}-\\nu_{2}|}{\\nu_{2,0}}\\mathrm{Tr}\\big(\\Sigma_{+}(\\Sigma_{+}+\\nu_{2})^{-1}\\big)}\\\\ &{\\leq\\left\\{\\frac{\\xi_{\\mathfrak m+1}^{2}}{\\nu_{2,0}}+\\frac{|\\nu_{2,0}-\\nu_{2}|}{\\nu_{2,0}}\\right\\}\\mathrm{Tr}\\big(\\Sigma(\\Sigma+\\nu_{2})^{-1}\\big)}\\\\ &{\\leq\\frac{3}{\\alpha}\\mathrm{Tr}\\big(\\Sigma(\\Sigma+\\nu_{2})^{-1}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where we used that $\\xi_{\\mathfrak{m}+1}^{2}/\\nu_{2,0}\\leq\\gamma_{+}/(p^{2}\\nu_{2,0})\\leq1/p$ by the assumption on m and identity (80). ", "page_idx": 23}, {"type": "text", "text": "B.3 Deterministic equivalents for functionals of $\\mathbf{Z}$ conditional on F ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "As mentioned in Section B.1, we will first show that the test error concentrates over $Z$ conditional on $\\pmb{F}$ on some quantity that only depends on $\\widehat{\\Sigma}_{F}$ . The bias and variance terms can be written in terms of the following three functionals of the fe ature matrix $Z$ : for a general p.s.d. matrix $A\\in\\mathbb{R}^{p\\times p}$ and positive scalar $\\kappa>0$ , define ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\Phi_{2}(Z;\\kappa):=\\mathrm{Tr}\\left(\\frac{Z^{\\top}Z}{n}(Z^{\\top}Z+\\kappa)^{-1}\\right),}\\\\ &{\\quad\\Phi_{3}(Z;A,\\kappa):=\\mathrm{Tr}\\left(A\\widehat{\\Sigma}_{F}^{1/2}(Z^{\\top}Z+\\kappa)^{-1}\\widehat{\\Sigma}_{F}(Z^{\\top}Z+\\kappa)^{-1}\\widehat{\\Sigma}_{F}^{1/2}\\right),}\\\\ &{\\quad\\Phi_{4}(Z;A,\\kappa):=\\left(A\\widehat{\\Sigma}_{F}^{1/2}(Z^{\\top}Z+\\kappa)^{-1}\\frac{Z^{\\top}Z}{n}(Z^{\\top}Z+\\kappa)^{-1}\\widehat{\\Sigma}_{F}^{1/2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where we recall that $_{z}$ has i.i.d. rows with covariance $\\widehat{\\Sigma}_{F}=F F^{\\top}/p$ . We show that these functional are well approximated by functionals of $\\pmb{F}$ that can be written in terms of the following functionals: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\~~~\\widetilde{\\Phi}_{2}(F;\\kappa):=\\mathrm{Tr}\\left(\\frac{F^{\\top}F}{p}(F^{\\top}F+\\kappa)^{-1}\\right),}\\\\ &{\\quad}\\\\ &{\\widetilde{\\Phi}_{5}(F;A,\\kappa):=\\displaystyle\\frac{1}{n}\\cdot\\frac{\\widetilde{\\Phi}_{6}(F;A,\\kappa)}{n-\\widetilde{\\Phi}_{6}(F;\\mathbf{I},\\kappa)},}\\\\ &{\\widetilde{\\Phi}_{6}(F;A,\\kappa):=\\mathrm{Tr}\\left(A(F F^{\\top})^{2}(F F^{\\top}+\\kappa)^{-2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The following proposition gather the approximation guarantees for $\\Phi_{2},\\Phi_{3},\\Phi_{4}$ listed in Eq. (83). ", "page_idx": 24}, {"type": "text", "text": "Proposition B.6 (Deterministic equivalents for $\\Phi(Z)$ conditional on $\\pmb{F}$ ). Under Assumption B.1 and assuming that $F\\in\\mathcal{A}_{\\mathcal{F}}$ defined in Eq. (79), for any $D,K>0,$ , there exist constants $\\eta_{*}\\in(0,1/4)$ , $C_{D,K}>0,$ , and $C_{*,D,K}>0$ such that the followings holds. Let $\\rho_{\\kappa}(p)$ and $\\widetilde{\\rho}_{\\kappa}(n,p)$ be defined as per Eqs. (26) and (25). For any $n\\geq C_{D,K}$ and regularization parameter $\\lambda>0$ , if it holds that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lambda\\geq n^{-K},}&{}&{\\widetilde{\\rho}_{\\lambda}(n,p)^{5/2}\\log^{3/2}(n)\\leq K\\sqrt{n},}\\\\ &{}&{\\widetilde{\\rho}_{\\lambda}(n,p)^{2}\\cdot\\rho_{\\gamma_{+}}(p)^{5/2}\\log^{3}(p)\\leq K\\sqrt{p},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "then for any p.s.d. matrix $A\\in\\mathbb{R}^{p\\times p}$ (independent of $Z|F)$ , with probability at least $1-n^{-D}$ on $_{z}$ conditional on $\\pmb{F}$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\Phi_{2}(Z;\\lambda)-\\frac{p}{n}\\widetilde{\\Phi}_{2}(F;p\\nu_{1})\\right|\\leq C_{*,D,K}\\cdot\\mathcal{E}_{1}(n,p)\\cdot\\frac{p}{n}\\widetilde{\\Phi}_{2}(F;p\\nu_{1}),}\\\\ &{\\qquad\\qquad\\Big|\\Phi_{3}(Z;A,\\lambda)-\\left(\\frac{n\\nu_{1}}{\\lambda}\\right)^{2}\\widetilde{\\Phi}_{5}(F;A,p\\nu_{1})\\Big|\\leq C_{*,D,K}\\cdot\\mathcal{E}_{1}(n,p)\\cdot\\left(\\frac{n\\nu_{1}}{\\lambda}\\right)^{2}\\widetilde{\\Phi}_{5}(F;A,p\\nu_{1}),}\\\\ &{\\qquad\\qquad\\Big|\\Phi_{4}(Z;A,\\lambda)-\\widetilde{\\Phi}_{5}(F;A,p\\nu_{1})\\Big|\\leq C_{*,D,K}\\cdot\\mathcal{E}_{1}(n,p)\\cdot\\widetilde{\\Phi}_{5}(F;A,p\\nu_{1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the approximation rate is given by ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{E}_{1}(n,p):=\\frac{\\widetilde{\\rho}_{\\lambda}(n,p)^{6}\\log^{5/2}(n)}{\\sqrt{n}}+\\frac{\\widetilde{\\rho}_{\\lambda}(n,p)^{2}\\cdot\\rho_{\\gamma_{+}}(p)^{5/2}\\log^{3}(p)}{\\sqrt{p}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proposition B.6 is a consequence of [Misiakiewicz and Saeed, 2024, Theorem 4] (see Appendix A and Theorem A.2 for background) and Proposition B.4. We defer its proof to Section B.7.3. Note that the term $\\widetilde{O}(p^{-1/2})$ in the approximation rate $\\mathcal{E}_{1}(n,p)$ defined in Eq. (89) comes from comparing $p\\nu_{1}$ with $p\\tilde{\\nu}_{1}$ and is equal to $\\widetilde{\\rho}_{\\lambda}^{-}(n,p)\\cdot\\mathcal{E}_{\\nu}(n,p)$ where $\\mathcal{E}_{\\nu}(n,p)$ is defined in Proposition B.4. If instead, we compared to functionals with regularization $p\\tilde{\\nu}_{1}$ , then the approximation rate in Proposition B.6 would scale $\\widetilde{O}(n^{-1/2})$ as expected. ", "page_idx": 24}, {"type": "text", "text": "In the analysis of the bias term, we will further need to show deterministic equivalents in the case where $\\pmb{A}$ is itself a random matrix uncorrelated (but not independent) to $Z|F$ . The following proposition gather these approximation guarantees and is a consequence of [Misiakiewicz and Saeed, 2024, Lemma 10] and Proposition B.4. ", "page_idx": 24}, {"type": "text", "text": "Proposition B.7 (Deterministic equivalents for $\\Phi(Z)$ , uncorrelated numerator). Assume the same setting as Proposition B.6 and the same conditions (85). Consider a deterministic vector $\\pmb{v}\\in\\mathbb{R}^{p}$ and a random vector $\\pmb{u}=(u_{i})_{i\\in[n]}$ with i.i.d. entries and $\\mathbb{E}[u_{i}]=0,\\,\\mathbb{E}[u_{i}^{2}]=1,$ , and $\\mathbb{E}[z_{i}u_{i}|F]=\\mathbf{0}$ . Then with probability at least $1-n^{-D}$ on $Z$ conditional on $\\pmb{F}$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\langle u,Z(Z^{\\top}Z+\\lambda)^{-1}\\widehat{\\Sigma}_{F}(Z^{\\top}Z+\\lambda)^{-1}Z^{\\top}u\\rangle-n\\widetilde{\\Phi}_{5}(F;\\mathbf{I},p\\nu_{1})\\right|\\leq C_{*,D,K}\\cdot\\mathcal{E}_{2}(n,p),}\\\\ &{\\left|\\langle u,Z(Z^{\\top}Z+\\lambda)^{-1}\\widehat{\\Sigma}_{F}(Z^{\\top}Z+\\lambda)^{-1}v\\rangle\\right|\\leq C_{*,D,K}\\cdot\\mathcal{E}_{2}(n,p)\\cdot\\frac{n\\nu_{1}}{\\lambda}\\sqrt{\\widetilde{\\Phi}_{5}(F;\\overline{{v\\upnu}}^{\\top},p\\nu_{1})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where we denoted $\\overline{{\\boldsymbol{v}}}:=\\widehat{\\boldsymbol{\\Sigma}}_{F}^{-1/2}\\boldsymbol{v}$ and the approximation rate is given by ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{E}_{2}(n,p):=\\frac{\\widetilde{\\rho}_{\\lambda}(n,p)^{6}\\log^{7/2}(n)}{\\sqrt{n}}+\\frac{\\widetilde{\\rho}_{\\lambda}(n,p)^{2}\\cdot\\rho_{\\gamma_{+}}(p)^{5/2}\\log^{3}(p)}{\\sqrt{p}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We defer the proof of Proposition B.7 to Section B.7.3. ", "page_idx": 24}, {"type": "text", "text": "B.4 Deterministic equivalents for functionals of $\\mathbf{F}$ ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "After replacing the bias and variance terms by their deterministic equivalents over the randomness in $Z|F$ , we obtain functionals in terms of $\\widetilde{\\Phi}_{2}(\\pmb{F};p\\nu_{1})$ and $\\widetilde{\\Phi}_{5}({\\cal F};{\\cal A},p\\nu_{1})$ listed in Eq. (84). As mentioned in Section B.2, we will analyze the low-degree and high degree part of the feature matrix separately. Using Lemma B.3, we can replace the high-degree part $F_{+}F_{+}^{\\top}$ by a deterministic matrix, which results in a regularization parameter $\\gamma(p\\nu_{1})=p\\nu_{1}+\\operatorname{Tr}(\\Sigma_{+})$ . ", "page_idx": 24}, {"type": "text", "text": "The functionals of $\\pmb{F}_{0}$ can be written in terms of the following quantities: for any deterministic matrix $B\\in\\mathbb{R}^{m\\times m}$ , define ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{\\Phi}_{1}(F_{0};B,\\kappa)=\\mathrm{Tr}\\left(B\\Sigma_{0}^{1/2}(F_{0}^{\\top}F_{0}+\\gamma(\\kappa))^{-1}\\Sigma_{0}^{1/2}\\right),}\\\\ &{\\quad\\widetilde{\\Phi}_{2}(F_{0};\\kappa)=\\mathrm{Tr}\\left(\\frac{F_{0}^{\\top}F_{0}}{p}(F_{0}^{\\top}F_{0}+\\gamma(\\kappa))^{-1}\\right),}\\\\ &{\\widetilde{\\Phi}_{3}(F_{0};B,\\kappa)=\\mathrm{Tr}\\left(B_{0}\\Sigma_{0}^{1/2}(F_{0}^{\\top}F_{0}+\\gamma(\\kappa))^{-1}\\Sigma_{0}(F_{0}^{\\top}F_{0}+\\gamma(\\kappa))^{-1}\\Sigma_{0}^{1/2}\\right),}\\\\ &{\\widetilde{\\Phi}_{4}(F_{0};B,\\kappa)=\\left(B\\Sigma_{0}^{1/2}(F_{0}^{\\top}F_{0}+\\gamma(\\kappa))^{-1}\\frac{F_{0}^{\\top}F_{0}}{p}(F_{0}^{\\top}F_{0}+\\gamma(\\kappa))^{-1}\\Sigma^{1/2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We show that these functionals can be well approximated by the deterministic functions that can be written in terms of ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Psi_{1}(\\nu;B)=\\mathrm{Tr}\\left(B\\Sigma_{0}(\\Sigma_{0}+\\nu)^{-1}\\right),}\\\\ &{\\quad\\Psi_{2}(\\nu)=\\frac{1}{p}\\mathrm{Tr}\\left(\\Sigma_{0}(\\Sigma_{0}+\\nu)^{-1}\\right),}\\\\ &{\\Psi_{3}(\\nu;B)=\\frac{1}{p}\\cdot\\frac{\\mathrm{Tr}\\left(B\\Sigma_{0}^{2}(\\Sigma_{0}+\\nu)^{-2}\\right)}{p-\\mathrm{Tr}\\left(\\Sigma_{0}^{2}\\left(\\Sigma_{0}+\\nu\\right)^{-2}\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Recall that for $\\kappa\\,=\\,p\\nu_{1}$ , we denote $\\gamma_{+}\\,:=\\,\\gamma(p\\nu_{1})$ and $\\nu_{2,0}$ the effective regularization associated to model $(p,\\Sigma_{0},\\gamma_{+})$ . The following proposition gather the approximation guarantees for $\\widetilde{\\Phi}_{1},\\widetilde{\\Phi}_{2},\\widetilde{\\Phi}_{3},\\widetilde{\\Phi}_{4}$ listed in Eq. (93). ", "page_idx": 25}, {"type": "text", "text": "Proposition B.8 (Deterministic equivalents for $\\scriptstyle{F_{0}}$ ). Under Assumption B.1, for any $D,K>0,$ , there exist constants $\\eta_{*}\\in(0,1/4),\\,{\\cal C}_{D,K}^{\\,\\,\\,\\,\\,}>0,$ , and $C_{*,D,K}>0$ such that the followings holds. Let $\\rho_{\\kappa}(p)$ be defined as per Eq. (26). For any $p\\geq C_{D,K}$ and $\\lambda>0$ , if it holds that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\gamma_{+}\\geq p^{-K},\\qquad\\qquad\\rho_{\\gamma_{+}}(p)^{5/2}\\log^{3/2}(p)\\leq K\\sqrt{p},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "then for any deterministic $p$ .s.d. matrix $B\\in\\mathbb{R}^{m\\times m}$ , with probability at least $1-p^{-D}$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad~\\bigg|\\widetilde{\\Phi}_{1}(F_{0};B,p\\nu_{1})-\\frac{\\nu_{2,0}}{\\gamma_{+}}\\Psi_{1}(\\nu_{2,0};B)\\bigg|\\leq C_{*,D,K}\\cdot\\mathcal{E}_{3}(p)\\cdot\\frac{\\nu_{2,0}}{\\gamma_{+}}\\Psi_{1}(\\nu_{2,0};B),}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\left|\\widetilde{\\Phi}_{2}(F_{0};p\\nu_{1})-\\Psi_{2}(\\nu_{2,0})\\right|\\leq C_{*,D,K}\\cdot\\mathcal{E}_{3}(p)\\cdot\\Psi_{2}(\\nu_{2,0}),}\\\\ &{\\bigg|\\widetilde{\\Phi}_{3}(F_{0};B,p\\nu_{1})-\\left(\\frac{p\\nu_{2,0}}{\\gamma_{+}}\\right)^{2}\\Psi_{3}(\\nu_{2,0};B)\\bigg|\\leq C_{*,D,K}\\cdot\\mathcal{E}_{3}(p)\\cdot\\left(\\frac{p\\nu_{2,0}}{\\gamma_{+}}\\right)^{2}\\Psi_{3}(\\nu_{2,0};B),}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\bigg|\\widetilde{\\Phi}_{4}(F_{0};B,p\\nu_{1})-\\Psi_{3}(\\nu_{2,0};B)\\bigg|\\leq C_{*,D,K}\\cdot\\mathcal{E}_{3}(p)\\cdot\\Psi_{3}(\\nu_{2,0};B),}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the approximation rate is given by ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathcal{E}_{3}(p):=\\frac{\\rho_{\\gamma_{+}}(p)^{6}\\log^{3}(p)}{\\sqrt{p}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This proposition is obtained by directly applying Theorem A.2 with no modifications. ", "page_idx": 25}, {"type": "text", "text": "Again, in the analysis of the bias term, because we separated the analysis of the low-degree and highdegree parts $\\pmb{F}_{0}$ and $\\pmb{F}_{+}$ , we will further need deterministic equivalents when $_B$ is itself a random matrix uncorrelated but not independent to $\\pmb{F}$ . We gather the associated deterministic equivalents in the following proposition. ", "page_idx": 25}, {"type": "text", "text": "Proposition B.9 (Deterministic equivalents for $\\pmb{F}_{0}$ , uncorrelated numerator). Assume the same setting as Proposition $B.8$ and the same conditions (95). Consider a deterministic vector $\\pmb{v}\\in\\mathbb{R}^{m}$ and a random vector $\\pmb{u}=(u_{j})_{j\\in[p]}$ with i.i.d. entries and $\\mathbb{E}[u_{j}]=0$ , $\\mathbb{E}[u_{j}^{2}]=1$ , and $\\mathbb{E}[f_{0,j}u_{j}]=\\mathbf{0}$ . ", "page_idx": 25}, {"type": "text", "text": "Then with probability at least $1-p^{-D}$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left|\\frac{1}{p}\\langle u,(F_{0}F_{0}^{\\mathsf{T}}+\\gamma_{+})^{-2}u\\rangle-\\frac{1}{(p\\nu_{2,0})^{2}}\\frac{1}{1-\\frac{1}{p}\\mathrm{Tr}(\\Sigma_{0}^{2}(\\Sigma_{0}+\\nu_{2,0})^{-2})}\\right|\\leq C_{*,D,K}\\cdot\\mathcal{E}_{4}(p)\\cdot\\frac{1}{\\gamma_{+}^{2}},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left|\\frac{1}{p}\\langle\\mathbf{\\boldsymbol{u}},(\\mathbf{\\boldsymbol{F}}_{0}\\mathbf{\\boldsymbol{F}}_{0}^{\\top}+\\boldsymbol{\\gamma}_{+})^{-2}\\mathbf{\\boldsymbol{F}}_{0}\\boldsymbol{\\boldsymbol{v}}\\rangle\\right|\\leq C_{*,D,K}\\cdot\\mathcal{E}_{4}(p)\\frac{p\\nu_{2,0}}{\\gamma_{+}^{2}}\\sqrt{\\Psi_{3}(\\nu_{2,0};\\overline{{\\boldsymbol{v}\\boldsymbol{v}}}^{\\top})},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where we denoted $\\overline{{\\boldsymbol{v}}}:=\\Sigma_{0}^{-1/2}\\boldsymbol{v}$ and the approximation rate is given by ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathcal{E}_{4}(p):=\\frac{\\rho_{\\gamma_{+}}(p)^{6}\\log^{7/2}(p)}{\\sqrt{p}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "This proposition is a direct consequence of [Misiakiewicz and Saeed, 2024, Lemma 10]. ", "page_idx": 26}, {"type": "text", "text": "Throughout the proofs, with a slight abuse of notations, we will denote functionals $\\widetilde{\\Phi}_{i}(\\boldsymbol{F}_{0};\\boldsymbol{B},\\kappa),$ $i\\in$ [4], the functionals listed in Eq. (93) applied to the truncated feature matrix $\\pmb{F}_{0}$ , with covariance $\\Sigma_{0}$ , regularization $\\gamma(\\kappa)$ , and deterministic matrix $B\\in\\mathbb{R}^{m\\times m}$ , and $\\widetilde{\\Phi}_{i}(\\boldsymbol{F};\\boldsymbol{B},\\kappa)$ , $i\\in$ [4], the functionals applied to the full feature matrix $\\b{F}\\in\\mathbb{R}^{p\\times\\infty}$ , where $\\Sigma_{0}$ is replac ed by $\\Sigma$ , the regularization parameter is $\\kappa$ , and the deterministic matrix $\\b{B}\\in\\mathbb{R}^{\\infty\\times\\infty}$ . Similarly, we will us the notation $\\Psi_{i}(\\nu_{2,0};B),i\\in[3]$ for the truncated deterministic functionals (94), and the notation $\\Psi_{i}(\\nu_{2};B),i\\in[3]$ ] to denote the full functionals with $\\Sigma_{0}$ replaced by $\\Sigma$ and $B\\in\\mathbb{R}^{\\infty\\times\\infty}$ . ", "page_idx": 26}, {"type": "text", "text": "B.5 Approximation guarantee for the variance term ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Recall the expressions for the variance term ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{V}(G,F,\\lambda)=\\sigma_{\\varepsilon}^{2}\\cdot\\mathrm{Tr}\\big(\\widehat{\\Sigma}_{F}Z^{\\top}Z(Z^{\\top}Z+\\lambda)^{-2}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and its associated deterministic equivalent ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathsf{V}_{n,p}(\\lambda)=\\sigma_{\\varepsilon}^{2}\\frac{\\Upsilon(\\nu_{1},\\nu_{2})}{1-\\Upsilon(\\nu_{1},\\nu_{2})},}}\\\\ {{\\displaystyle\\Upsilon(\\nu_{1},\\nu_{2})=\\frac{p}{n}\\left[\\left(1-\\frac{\\nu_{1}}{\\nu_{2}}\\right)^{2}+\\left(\\frac{\\nu_{1}}{\\nu_{2}}\\right)^{2}\\frac{\\mathrm{Tr}(\\Sigma^{2}(\\Sigma+\\nu_{2})^{-2})}{p-\\mathrm{Tr}(\\Sigma^{2}(\\Sigma+\\nu_{2})^{-2})}\\right].}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We prove in this section an approximation guarantee between $\\mathcal{V}(G,F,\\lambda)$ and $\\mathsf{V}_{n,p}(\\lambda)$ . For convenience, we state a separate theorem for this term. ", "page_idx": 26}, {"type": "text", "text": "Theorem B.10 (Deterministic equivalent for the variance term). Assume the features $(z_{i})_{i\\in[n]}$ and $({\\pmb{f}}_{j})_{j\\in[p]}$ satisfy Assumption B.1, and the covariance $\\Sigma$ and target coefficients $\\beta_{*}$ satisfy Assumption 3.2. Then, for any $D,K>0,$ , there exist constants $\\eta_{*}\\,\\in\\,(0,1/2)$ and $C_{*,D,K}>0$ such that the following holds. Let $\\rho_{\\kappa}$ and $\\widetilde{\\rho}_{\\kappa}$ be defined as per Eqs. (26) and (25). For any $n,p\\ge C_{*,D,K}$ and $\\lambda>0$ , if it holds that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lambda\\geq n^{-K},}&{\\quad\\gamma_{\\lambda}\\geq p^{-K},}&{\\qquad}&{\\widetilde{\\rho}_{\\lambda}(n,p)^{5/2}\\cdot\\log^{3/2}(n)\\leq K\\sqrt{n},}\\\\ &{}&{\\quad}&{\\widetilde{\\rho}_{\\lambda}(n,p)^{2}\\cdot\\rho_{\\gamma_{+}}(p)^{7}\\cdot\\log^{4}(p)\\leq K\\sqrt{p},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "then with probability at least $1-n^{-D}-p^{-D}$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n|\\mathcal{V}(G,F,\\lambda)-\\mathsf{V}_{n,p}(\\lambda)|\\leq C_{*,D,K}\\cdot\\mathcal{E}_{V}(n,p)\\cdot\\mathsf{V}_{n,p}(\\lambda),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the approximation rate is given by ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathcal{E}_{V}(n,p):=\\frac{\\widetilde{\\rho}_{\\lambda}(n,p)^{6}\\log^{5/2}(n)}{\\sqrt{n}}+\\frac{\\widetilde{\\rho}_{\\lambda}(n,p)^{2}\\cdot\\rho_{\\gamma_{+}}(p)^{7}\\log^{3}(p)}{\\sqrt{p}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof of Theorem B.10. First, note that $\\mathcal{V}(G,F,\\lambda)$ can be written in terms of the functional $\\Phi_{4}$ defined in Eq. (83): ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{V}(G,F,\\lambda)=\\sigma_{\\varepsilon}^{2}\\cdot n\\Phi_{4}(Z;\\mathbf{I},\\lambda).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Recall that $A_{\\mathcal{F}}$ is the event defined in Eq. (79). Under the assumptions of Theorem B.10, we can apply Lemma B.2 and Proposition B.4 to obtain ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}(\\mathcal{A}_{\\mathcal{F}})\\geq1-p^{-D}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Hence, applying Proposition B.6 for $F\\in\\mathcal A_{\\mathcal F}$ and via union bound, we obtain that with probability at least $1-\\dot{p}^{-}\\dot{p}-\\bar{n}^{-}\\bar{D}$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|n\\Phi_{4}(Z;\\mathbf{I},\\lambda)-n\\widetilde{\\Phi}_{5}(F;\\mathbf{I},p\\nu_{1})\\right|\\leq C_{*,D,K}\\cdot\\mathcal{E}_{1}(p,n)\\cdot n\\widetilde{\\Phi}_{5}(F;\\mathbf{I},p\\nu_{1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\mathcal{E}_{1}(n,p)$ is defined in Eq. (89) and we recall the expressions ", "page_idx": 27}, {"type": "equation", "text": "$$\nn\\widetilde{\\Phi}_{5}(F;\\mathbf{I},p\\nu_{1})=\\frac{\\widetilde{\\Phi}_{6}(F;\\mathbf{I},p\\nu_{1})}{n-\\widetilde{\\Phi}_{6}(F;\\mathbf{I},p\\nu_{1})},\\qquad\\quad\\widetilde{\\Phi}_{6}(F;\\mathbf{I},p\\nu_{1})=\\mathrm{Tr}\\big((F F^{\\top})^{2}(F F^{\\top}+p\\nu_{1})^{-2}\\big).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Let us decompose $\\widetilde{\\Phi}_{6}(\\pmb{F};\\mathbf{I},p\\nu_{1})$ into ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\widetilde{\\Phi}_{6}({\\pmb F};{\\mathbf{I}},{p}{\\nu}_{1})=\\mathrm{Tr}({\\pmb F}{\\pmb F}^{\\top}({\\pmb F}{\\pmb F}^{\\top}+p{\\nu}_{1})^{-1})-p{\\nu}_{1}\\mathrm{Tr}({\\pmb F}{\\pmb F}^{\\top}({\\pmb F}{\\pmb F}^{\\top}+p{\\nu}_{1})^{-2}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "From Lemma B.11 stated below, we have with probability at least $1-p^{-D}$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\frac{1}{p}\\mathrm{Tr}(F F^{\\mathsf{T}}(F F^{\\mathsf{T}}(F F^{\\mathsf{T}}+p\\nu_{1})^{-1})-\\Psi_{2}(\\nu_{2})\\right|\\leq C_{\\ast,D,K}\\cdot\\mathcal{E}_{3}(p)\\cdot\\Psi_{2}(\\nu_{2}),}\\\\ &{\\left|\\frac{1}{p}\\mathrm{Tr}(F F^{\\mathsf{T}}(F F^{\\mathsf{T}}+p\\nu_{1})^{-2})-\\Psi_{3}(\\nu_{2};\\Sigma^{-1})\\right|\\leq C_{\\ast,D,K}\\cdot\\rho_{\\gamma_{+}}(p)\\mathcal{E}_{3}(p)\\cdot\\Psi_{3}(\\nu_{2};\\Sigma^{-1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\mathcal{E}_{3}(p)$ is the approximation rate defined in Eq. (100). Note that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p\\Psi_{2}(\\nu_{2})-p^{2}\\nu_{1}\\Psi_{3}(\\nu_{2};\\mathbf{Z}^{-1})=\\mathrm{Tr}(\\bar{\\mathbf{X}}(\\mathbf{\\Sigma}+\\nu_{2})^{-1})-\\frac{\\nu_{1}\\mathrm{Tr}(\\mathbf{Z}(\\mathbf{Z}+\\nu_{2})^{-2})}{1-\\frac{1}{p}\\mathrm{Tr}(\\mathbf{Z}^{2}(\\mathbf{Z}+\\nu_{2})^{-2})}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=p\\left\\{1-\\frac{\\nu_{1}}{\\nu_{2}}-\\frac{\\nu_{1}}{\\nu_{2}}\\frac{1-\\frac{\\nu_{1}}{\\nu_{2}}-\\frac{1}{p}\\mathrm{Tr}(\\mathbf{Z}^{2}(\\mathbf{Z}+\\nu_{2})^{-2})}{1-\\frac{1}{p}\\mathrm{Tr}(\\mathbf{Z}^{2}(\\mathbf{Z}+\\nu_{2})^{-2})}\\right\\}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=p\\left\\{\\left(1-\\frac{\\nu_{1}}{\\nu_{2}}\\right)^{2}+\\left(\\frac{\\nu_{1}}{\\nu_{2}}\\right)^{2}\\frac{\\mathrm{Tr}(\\mathbf{Z}^{2}(\\mathbf{Z}+\\nu_{2})^{-2})}{p-\\mathrm{Tr}(\\mathbf{Z}^{2}(\\mathbf{Z}+\\nu_{2})^{-2})}\\right\\}}\\\\ &{\\qquad\\qquad\\qquad=n\\Upsilon(\\nu_{1},\\nu_{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Combining the above displays, we deduce that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\widetilde{\\Phi}_{6}(F;\\mathbf{I},p\\nu_{1})-n\\Upsilon(\\nu_{1},\\nu_{2})\\right|\\leq C_{*,D,K}\\cdot\\rho_{\\gamma_{+}}(p)\\mathcal{E}_{3}(p)\\cdot\\left[p\\Psi_{2}(\\nu_{2})+p^{2}\\nu_{1}\\Psi_{3}(\\nu_{2};\\mathbf{Z}^{-1})\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq C_{*,D,K}\\cdot\\rho_{\\gamma_{+}}(p)\\mathcal{E}_{3}(p)\\cdot\\left[n\\Upsilon(\\nu_{1},\\nu_{2})+2p^{2}\\nu_{1}\\Psi_{3}(\\nu_{2};\\mathbf{Z}^{-1})\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Using Eq. (120) in Lemma B.14 stated in Section B.7, we conclude that with probability at least 1 \u2212p\u2212D, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\Big|\\widetilde{\\Phi}_{6}(F;\\mathbf{I},p\\nu_{1})-n\\Upsilon(\\nu_{1},\\nu_{2})\\Big|\\le C_{*,D,K}\\cdot\\rho_{\\gamma_{+}}(p)\\mathcal{E}_{3}(p)\\cdot n\\Upsilon(\\nu_{1},\\nu_{2}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Finally, by simple algebra, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\left|n\\widetilde{\\Phi}_{5}(F;\\mathbf{I},p\\nu_{1})-\\frac{\\Upsilon(\\nu_{1},\\nu_{2})}{1-\\Upsilon(\\nu_{1},\\nu_{2})}\\right|}\\\\ &{\\le\\left\\{n\\widetilde{\\Phi}_{5}(F;\\mathbf{I},p\\nu_{1})+1\\right\\}\\frac{\\left|\\widetilde{\\Phi}_{6}(F;\\mathbf{I},p\\nu_{1})-n\\Upsilon(\\nu_{1},\\nu_{2})\\right|}{n-\\Upsilon(\\nu_{1},\\nu_{2})}}\\\\ &{\\le\\left\\{\\left|n\\widetilde{\\Phi}_{5}(F;\\mathbf{I},p\\nu_{1})-\\frac{\\Upsilon(\\nu_{1},\\nu_{2})}{1-\\Upsilon(\\nu_{1},\\nu_{2})}\\right|+\\frac{1}{1-\\Upsilon(\\nu_{1},\\nu_{2})}\\right\\}\\cdot C_{*,D,K}\\cdot\\rho_{\\gamma_{+}}(p)\\mathcal{E}_{3}(p)\\frac{\\Upsilon(\\nu_{1},\\nu_{2})}{1-\\Upsilon(\\nu_{1},\\nu_{2})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Note that by Eq. (119) in Lemma B.14, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{\\Upsilon(\\nu_{1},\\nu_{2})}{1-\\Upsilon(\\nu_{1},\\nu_{2})}\\leq(1-\\Upsilon(\\nu_{1},\\nu_{2}))^{-1}\\leq C_{*}\\cdot\\widetilde{\\rho}_{\\lambda}(n,p).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Hence rearranging the terms in Eq. (110) and using from conditions (106) and that $p\\geq C_{*,D,K}$ , we obtain with probability at least $1-p^{-D}$ that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left|n\\widetilde{\\Phi}_{5}(F;\\mathbf{I},p\\nu_{1})-\\frac{\\Upsilon(\\nu_{1},\\nu_{2})}{1-\\Upsilon(\\nu_{1},\\nu_{2})}\\right|\\le C_{\\ast,D,K}\\cdot\\widetilde{\\rho}_{\\lambda}(n,p)\\rho_{\\gamma_{+}}(p)\\mathcal{E}_{3}(p)\\frac{\\Upsilon(\\nu_{1},\\nu_{2})}{1-\\Upsilon(\\nu_{1},\\nu_{2})}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Using an union bound and combining bounds Eqs. (108) and (111), we obtain with probability at least $\\bar{1}-n^{-D}-p^{-D}$ , that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad|\\mathcal{V}(G,F,\\lambda)-\\nabla_{n,p}(\\lambda)|}\\\\ &{\\le\\left|\\gamma(G,F,\\lambda)-\\sigma_{\\varepsilon}^{2}\\cdot n\\widetilde{\\Phi}_{5}(F;\\mathbf{I},p\\nu_{1})\\right|+\\sigma_{\\varepsilon}^{2}\\left|n\\widetilde{\\Phi}_{5}(F;\\mathbf{I},p\\nu_{1})-\\frac{\\Upsilon(\\nu_{1},\\nu_{2})}{1-\\Upsilon(\\nu_{1},\\nu_{2})}\\right|}\\\\ &{\\le C_{\\ast,D,K}\\cdot\\left\\{\\mathcal{E}_{1}(p,n)+\\widetilde{\\rho}_{\\lambda}(n,p)\\rho_{\\gamma_{+}}(p)\\mathcal{E}_{3}(p)\\right\\}\\cdot\\left[\\sigma_{\\varepsilon}^{2}\\cdot n\\widetilde{\\Phi}_{5}(F;\\mathbf{I},p\\nu_{1})+\\nabla_{n,p}(\\lambda)\\right]}\\\\ &{\\le C_{\\ast,D,K}\\cdot\\left\\{\\mathcal{E}_{1}(p,n)+\\widetilde{\\rho}_{\\lambda}(n,p)\\rho_{\\gamma_{+}}(p)\\mathcal{E}_{3}(p)\\right\\}\\cdot\\nabla_{n,p}(\\lambda),}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where we used Eq. (111) and conditions (106) in the last line. Replacing the rates $\\mathscr{E}_{j}$ by their expressions conclude the proof of this theorem. \u53e3 ", "page_idx": 28}, {"type": "text", "text": "Lemma B.11. Under the setting of Theorem B.10 and assuming the same conditions (106), we have with probability at least $1-p^{-D}$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\frac{1}{p}\\mathrm{Tr}(F F^{\\top}(F F^{\\top}+p\\nu_{1})^{-1})-\\Psi_{2}(\\nu_{2})\\right|\\leq C_{\\ast,D,K}\\cdot\\mathcal{E}_{3}(p)\\cdot\\Psi_{2}(\\nu_{2}),}\\\\ &{\\left|\\frac{1}{p}\\mathrm{Tr}(F F^{\\top}(F F^{\\top}+p\\nu_{1})^{-2})-\\Psi_{3}(\\nu_{2};\\Sigma^{-1})\\right|\\leq C_{\\ast,D,K}\\cdot\\rho_{\\gamma_{+}}(p)\\mathcal{E}_{3}(p)\\cdot\\Psi_{3}(\\nu_{2};\\Sigma^{-1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\mathcal{E}_{3}(p)$ is the approximation rate defined in Eq. (100). ", "page_idx": 28}, {"type": "text", "text": "The proof of this lemma can be found in Section B.7.4. ", "page_idx": 28}, {"type": "text", "text": "B.6 Approximation guarantee for the bias term ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Recall the expression for the bias term ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathcal{B}(\\boldsymbol{\\beta}_{*};\\boldsymbol{G},\\boldsymbol{F},\\lambda)=\\|\\boldsymbol{\\beta}_{*}-{p^{-1/2}}\\boldsymbol{F}^{\\mathsf{T}}(\\boldsymbol{Z}^{\\mathsf{T}}\\boldsymbol{Z}+\\lambda)^{-1}\\boldsymbol{Z}^{\\mathsf{T}}\\boldsymbol{G}\\boldsymbol{\\beta}_{*}\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and its associated deterministic equivalent ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\chi(\\nu_{2})=\\frac{\\mathrm{Tr}(\\Sigma(\\Sigma+\\nu_{2})^{-2})}{p-\\mathrm{Tr}(\\Sigma^{2}(\\Sigma+\\nu_{2})^{-2})},}}\\\\ &{}&{\\Xi_{n,p}(\\beta_{*},\\lambda)=\\frac{\\nu_{2}^{2}}{1-\\mathrm{T}(\\nu_{1},\\nu_{2})}\\Big[\\langle\\beta_{*},(\\Sigma+\\nu_{2})^{-2}\\beta_{*}\\rangle+\\chi(\\nu_{2})\\langle\\beta_{*},\\Sigma(\\Sigma+\\nu_{2})^{-2}\\beta_{*}\\rangle\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We prove in this section an approximation guarantee between $B(\\beta_{*};G,F,\\lambda)$ and $\\mathsf{B}_{n,p}(\\beta_{*},\\lambda)$ . For convenience, we state a separate theorem for this term. ", "page_idx": 28}, {"type": "text", "text": "Theorem B.12 (Deterministic equivalent for the bias term). Assume the features $(z_{i})_{i\\in[n]}$ and $({\\pmb{f}}_{j})_{j\\in[p]}$ satisfy Assumption B.1, and that there exists $\\mathsf{m}\\in\\mathbb{N}$ such that $p^{2}\\xi_{\\mathsf{m}}^{2}\\leq\\gamma_{\\mathsf{m}}(n\\lambda/p)$ . Then, for any $D,K>0,$ , there exist constants $\\eta_{*}\\in(0,1/2)$ and $C_{*,D,K}>0$ such that the following holds. Let $\\rho_{\\kappa}$ and $\\widetilde{\\rho}_{\\kappa}$ be defined as per Eqs. (26) and (25), and recall that $\\gamma_{\\lambda}:=\\gamma_{\\mathsf{m}}(n\\lambda/p)$ and $\\gamma_{+}:=\\gamma_{\\mathsf{m}}(p\\nu_{1})$ . For any $n,p\\ge C_{*,D,K}$ and $\\lambda>0$ , if it holds that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lambda\\geq n^{-K},}&{\\quad\\gamma_{\\lambda}\\geq p^{-K},}&{\\qquad}&{\\widetilde{\\rho}_{\\lambda}(n,p)^{5/2}\\cdot\\log^{3/2}(n)\\leq K\\sqrt{n},}\\\\ &{}&{\\quad\\widetilde{\\rho}_{\\lambda}(n,p)^{2}\\cdot\\rho_{\\gamma_{+}}(p)^{8}\\cdot\\log^{4}(p)\\leq K\\sqrt{p},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "then with probability at least $1-n^{-D}-p^{-D}$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n|\\mathcal{B}(\\beta_{*};G,F,\\lambda)-\\mathsf{B}_{n,p}(\\beta_{*},\\lambda)|\\leq C_{*,D,K}\\cdot\\mathcal{E}_{B}(n,p)\\cdot\\mathsf{B}_{n,p}(\\beta_{*},\\lambda),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the approximation rate is given by ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathcal{E}_{B}(n,p):=\\frac{\\widetilde{\\rho}_{\\lambda}(n,p)^{6}\\log^{7/2}(n)}{\\sqrt{n}}+\\frac{\\widetilde{\\rho}_{\\lambda}(n,p)^{2}\\cdot\\rho_{\\gamma_{+}}(p)^{8}\\log^{7/2}(p)}{\\sqrt{p}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Before starting the proof, let us introduce some notations. First, define $\\mathsf{P}_{F}$ the projection onto the span of $\\pmb{F}$ , and $\\mathsf{P}_{\\perp,F}$ the projection orthogonal to the span of $\\pmb{F}$ , i.e., ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathsf{P}_{F}:=(F^{\\top}F)^{\\dagger}F^{\\top}F,\\qquad\\quad\\mathsf{P}_{\\perp,F}=\\mathbf{I}-\\mathsf{P}_{F}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We can decompose the feature $\\textbf{\\textit{g}}$ with respect to the orthogonal sum of these two subspaces ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{g}=\\mathsf{P}_{\\pmb{F}}\\pmb{g}+\\mathsf{P}_{\\perp,\\pmb{F}}\\pmb{g}=\\sqrt{p}(\\pmb{F}^{\\top}\\pmb{F})^{\\dagger}\\pmb{F}^{\\top}\\pmb{z}+\\mathsf{P}_{\\perp,\\pmb{F}}\\pmb{g}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We define $r:=\\mathsf{P}_{\\bot,F}g$ and $\\pmb{R}=[\\pmb{r}_{1},\\dots,\\pmb{r}_{n}]^{\\mathsf{T}}\\in\\mathbb{R}^{n\\times\\infty}$ . Similarly, we can decompose the target function ", "page_idx": 29}, {"type": "equation", "text": "$$\nh_{\\ast}(\\pmb{g})=\\langle\\beta_{\\ast},\\pmb{g}\\rangle=\\langle\\beta_{F},z\\rangle+\\langle\\beta_{\\perp,F},r\\rangle,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where we introduced ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\beta_{F}:=\\sqrt{p}F(F^{\\top}F)^{\\dagger}\\beta_{*},\\qquad\\qquad\\beta_{\\perp,F}=\\mathsf{P}_{\\perp,F}\\beta_{*}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Note that in particular $\\mathbb{E}[z\\langle r,\\beta_{\\perp,F}\\rangle]=\\mathbf{0}$ by orthogonality. ", "page_idx": 29}, {"type": "text", "text": "Proof of Theorem B.12. Step 0: Decomposing the bias term. ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Note that using the notations introduced above, we can decompose the bias term into ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{B}(\\beta_{*};G,F,\\lambda)=\\|\\beta_{*}-p^{-1/2}F^{\\top}(Z^{\\top}Z+\\lambda)^{-1}Z^{\\top}G\\beta_{*}\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad=\\cfrac{1}{p}\\,\\Big\\|F^{\\top}\\left(\\beta_{F}-(Z^{\\top}Z+\\lambda)^{-1}Z^{\\top}(Z\\beta_{F}+R\\beta_{\\perp,F})\\right)\\Big\\|_{2}^{2}+\\|\\beta_{\\perp,F}\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad=T_{1}-2T_{2}+T_{3}+\\|\\beta_{\\perp,F}\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where we denoted ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{1}:=\\lambda^{2}\\langle\\beta_{F},(\\pmb{Z}^{\\top}\\pmb{Z}+\\lambda)^{-1}\\widehat{\\Sigma}_{F}(\\pmb{Z}^{\\top}\\pmb{Z}+\\lambda)^{-1}\\beta_{F}\\rangle,}\\\\ &{T_{2}:=\\lambda\\langle\\beta_{F},(\\pmb{Z}^{\\top}\\pmb{Z}+\\lambda)^{-1}\\widehat{\\Sigma}_{F}(\\pmb{Z}^{\\top}\\pmb{Z}+\\lambda)^{-1}\\pmb{Z}^{\\top}\\pmb{R}\\beta_{\\bot,F}\\rangle,}\\\\ &{T_{3}:=\\langle\\beta_{\\bot,F},\\pmb{R}^{\\top}\\pmb{Z}(\\pmb{Z}^{\\top}\\pmb{Z}+\\lambda)^{-1}\\widehat{\\Sigma}_{F}(\\pmb{Z}^{\\top}\\pmb{Z}+\\lambda)^{-1}\\pmb{Z}^{\\top}\\pmb{R}\\beta_{\\bot,F}\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We proceed similarly to the proof for the variance term, by first considering the deterministic equivalent over $_{z}$ conditional on $\\pmb{F}$ , and then over $\\pmb{F}$ . We omit some repetitive details for the sake of brevity. ", "page_idx": 29}, {"type": "text", "text": "Step 1: Deterministic equivalent over $_{z}$ conditional on $\\pmb{F}$ . ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "First note that, denoting A\u02dc\u2217= \u03a3 F\u22121/2\u03b2F \u03b2TF\u03a3 F\u22121/2, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\nT_{1}=\\lambda^{2}\\Phi_{3}(Z;\\tilde{A}_{*},\\lambda).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Furthermore, $T_{3}$ and $T_{2}$ correspond respectively to the terms (90) and (91) in Proposition B.7 with $v=\\beta_{F}$ and $u=R\\beta_{\\perp,F}$ where ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[z_{i}u_{i}]=\\mathbb{E}[z_{i}\\langle r_{i},\\beta_{\\perp,F}\\rangle]=0,\\qquad\\quad\\mathbb{E}[u_{i}^{2}]=\\|\\beta_{\\perp,F}\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Thus, under the assumptions of Theorem B.12, we can apply Propositions B.8 and B.7 to obtain (via union bound) that with probability at least $1-n^{-D}-\\dot{p}^{-\\dot{D}}$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left|T_{1}-(n\\nu_{1})^{2}\\widetilde{\\Phi}_{5}(F;\\widetilde{\\boldsymbol{A}}_{*},p\\nu_{1})\\right|\\leq C_{*,D,K}\\cdot\\mathcal{E}_{1}(p,n)\\cdot(n\\nu_{1})^{2}\\widetilde{\\Phi}_{5}(F;\\widetilde{\\boldsymbol{A}}_{*},p\\nu_{1}),}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\left|T_{2}\\right|\\leq C_{*,D,K}\\cdot\\mathcal{E}_{2}(p,n)\\cdot\\sqrt{\\|\\beta_{\\perp,F}\\|_{2}^{2}\\cdot(n\\nu_{1})^{2}\\widetilde{\\Phi}_{5}(F;\\widetilde{\\boldsymbol{A}}_{*},p\\nu_{1})},}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\left|T_{3}-\\|\\beta_{\\perp,F}\\|_{2}^{2}\\cdot n\\widetilde{\\Phi}_{5}(F;\\mathbf{I},p\\nu_{1})\\right|\\leq C_{*,D,K}\\cdot\\mathcal{E}_{2}(p,n)\\cdot\\|\\beta_{\\perp,F}\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Hence we deduce that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left|{\\mathcal{B}}(\\beta_{*};G,F,\\lambda)-(n\\nu_{1})^{2}\\widetilde{\\Phi}_{5}(F;\\widetilde{A}_{*},p\\nu_{1})-\\|\\beta_{\\perp,F}\\|_{2}^{2}\\cdot n\\widetilde{\\Phi}_{5}(F;\\mathbf{I},p\\nu_{1})-\\|\\beta_{\\perp,F}\\|_{2}^{2}\\right|}\\\\ &{\\leq C_{*,D,K}\\cdot\\{\\mathcal{E}_{1}(n,p)+\\mathcal{E}_{2}(n,p)\\}\\cdot\\left[(n\\nu_{1})^{2}\\widetilde{\\Phi}_{5}(F;\\widetilde{A}_{*},p\\nu_{1})+\\|\\beta_{\\perp,F}\\|_{2}^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Let us simplify these terms. Recall that ", "page_idx": 30}, {"type": "equation", "text": "$$\nn\\widetilde{\\Phi}_{5}(F;\\tilde{\\mathbf{A}}_{*},p\\nu_{1})=\\frac{\\widetilde{\\Phi}_{6}(F;\\tilde{\\mathbf{A}}_{*},p\\nu_{1})}{n-\\widetilde{\\Phi}_{6}(F;\\mathbf{I},p\\nu_{1})},\\qquad\\quad n\\widetilde{\\Phi}_{5}(F;\\mathbf{I},p\\nu_{1})\\frac{\\widetilde{\\Phi}_{6}(F;\\mathbf{I},p\\nu_{1})}{n-\\widetilde{\\Phi}_{6}(F;\\mathbf{I},p\\nu_{1})}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "For the term involving $\\tilde{\\boldsymbol{A}}_{*}$ , we can rewrite it as ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nu_{1}^{2}\\widetilde{\\Phi}_{6}(F;\\widetilde{A}_{*},p\\nu_{1})=\\nu_{1}^{2}\\langle\\beta_{F},\\widehat{\\Sigma}_{F}^{-1}(F^{\\top}F)^{2}(F^{\\top}F+p\\nu_{1})^{-2}\\beta_{F}\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad=(p\\nu_{1})^{2}\\langle\\beta_{*},(F^{\\top}F)^{\\dagger}(F^{\\top}F)(F^{\\top}F+p\\nu_{1})^{-2}(F^{\\top}F)(F^{\\top}F)^{\\dagger}\\beta_{*}\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad=(p\\nu_{1})^{2}\\langle\\beta_{*},(F^{\\top}F+p\\nu_{1})^{-2}\\beta_{*}\\rangle-\\|\\beta_{\\bot,F}\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Hence the terms involving $\\|\\beta_{\\perp,F}\\|_{2}^{2}$ cancel out and we obtain ", "page_idx": 30}, {"type": "equation", "text": "$$\nn\\nu_{1})^{2}\\widetilde{\\Phi}_{5}(F;\\widetilde{A}_{*},p\\nu_{1})+\\Vert\\beta_{\\perp,F}\\Vert_{2}^{2}\\cdot n\\widetilde{\\Phi}_{5}(F;{\\mathbf I},p\\nu_{1})+\\Vert\\beta_{\\perp,F}\\Vert_{2}^{2}=(p\\nu_{1})^{2}\\frac{\\langle\\beta_{*},(F^{\\top}F+p\\nu_{1})^{-2}\\beta_{*}\\rangle}{1-\\frac{1}{n}\\widetilde{\\Phi}_{6}(F;{\\mathbf I},p\\nu_{1})}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Combining the above displays, we deduce that with probability at least $1-n^{-D}-p^{-D}$ , ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left|{\\mathcal B}(\\beta_{*};G,F,\\lambda)-(p\\nu_{1})^{2}\\frac{\\langle\\beta_{*},(F^{\\top}F+p\\nu_{1})^{-2}\\beta_{*}\\rangle}{1-\\frac{1}{n}\\widetilde{\\Phi}_{6}(F;\\mathbf{I},p\\nu_{1})}\\right|}\\\\ &{\\le C_{*,D,K}\\cdot\\{\\mathcal E_{1}(n,p)+\\mathcal E_{2}(n,p)\\}\\cdot(p\\nu_{1})^{2}\\frac{\\langle\\beta_{*},(F^{\\top}F+p\\nu_{1})^{-2}\\beta_{*}\\rangle}{1-\\frac{1}{n}\\widetilde{\\Phi}_{6}(F;\\mathbf{I},p\\nu_{1})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Step 2: Deterministic equivalents over $\\pmb{F}$ . ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Following the same steps as Eq. (110) in the proof of Theorem B.10, we have with probability at least $1-\\overline{{p}}^{-D}$ , ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(1-n^{-1}\\widetilde{\\Phi}_{6}(F;\\mathbf{I},p\\nu_{1}))^{-1}-(1-\\Upsilon(\\nu_{1},\\nu_{2}))^{-1}\\Big|\\leq C_{\\ast,D,K}\\cdot\\widetilde{\\rho}_{\\lambda}(n,p)\\rho_{\\gamma_{+}}(p)\\mathcal{E}_{3}(p)\\cdot(1-\\Upsilon(\\nu_{1},\\nu_{2}))^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Furthermore, from Lemma B.13 stated below, with probability at least $1-p^{-D}$ , ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|(p\\nu_{1})^{2}\\langle\\beta_{*},(F^{\\top}F+p\\nu_{1})^{-2}\\beta_{*}\\rangle-\\widetilde{\\mathsf{B}}_{n,p}(\\beta_{*},\\lambda)\\right|\\leq C_{*,D,K}\\cdot\\rho_{\\gamma_{+}}(p)^{2}\\mathcal{E}_{4}(p)\\cdot\\widetilde{\\mathsf{B}}_{n,p}(\\beta_{*},\\lambda),}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $\\widetilde{\\mathsf{B}}_{n,p}(\\beta_{*},\\lambda)$ is defined in Eq. (118). Combining these two bounds and recalling conditions (114),  we obtain ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\bigg|(p\\nu_{1})^{2}\\frac{\\langle\\beta_{*},(\\pmb{F}^{\\top}\\pmb{F}+p\\nu_{1})^{-2}\\beta_{*}\\rangle}{1-\\frac{1}{n}\\widetilde{\\Phi}_{6}(\\pmb{F};\\mathbf{I},p\\nu_{1})}-\\frac{\\widetilde{\\mathsf{B}}_{n,p}(\\beta_{*},\\lambda)}{1-\\Upsilon(\\nu_{1},\\nu_{2})}\\bigg|}\\\\ &{\\le C_{*,D,K}\\,\\Big\\{\\widetilde{\\rho}_{\\lambda}(n,p)\\rho_{\\gamma_{+}}(p)\\mathcal{E}_{3}(p)+\\rho_{\\gamma_{+}}(p)^{2}\\mathcal{E}_{4}(p)\\Big\\}\\cdot\\frac{\\widetilde{\\mathsf{B}}_{n,p}(\\beta_{*},\\lambda)}{1-\\Upsilon(\\nu_{1},\\nu_{2})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Noting that $\\mathsf{B}_{n,p}(\\beta_{*},\\lambda)=\\widetilde{\\mathsf{B}}_{n,p}(\\beta_{*},\\lambda)/(1-\\Upsilon(\\nu_{1},\\nu_{2}))$ , we can combine this bound with Eq. (116) to obtain via union bound t h at with probability at least $1-n^{-D}-p^{-D}$ , ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad|\\mathcal{B}(\\beta_{*};G,F,\\lambda)-\\mathsf{B}_{n,p}(\\beta_{*},\\lambda)|}\\\\ &{\\leq C_{*,D,K}\\left\\{\\mathcal{E}_{1}(n,p)+\\mathcal{E}_{2}(n,p)+\\widetilde{\\rho}_{\\lambda}(n,p)\\rho_{\\gamma_{+}}(p)\\mathcal{E}_{3}(p)+\\rho_{\\gamma_{+}}(p)^{2}\\mathcal{E}_{4}(p)\\right\\}\\cdot\\mathsf{B}_{n,p}(\\beta_{*},\\lambda).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Replacing the rates $\\mathscr{E}_{j}$ by their expressions conclude the proof of this theorem. ", "page_idx": 30}, {"type": "text", "text": "Lemma B.13. Under the setting of Theorem B.12 and assuming the same conditions (114), we have with probability at least $1-p^{-D}$ ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|(p\\nu_{1})^{2}\\langle\\beta_{*},(F^{\\top}F+p\\nu_{1})^{-2}\\beta_{*}\\rangle-\\widetilde{\\mathsf{B}}_{n,p}(\\beta_{*},\\lambda)\\right|\\leq C_{*,D,K}\\cdot\\rho_{\\gamma_{+}}(p)^{2}\\mathcal{E}_{4}(p)\\cdot\\widetilde{\\mathsf{B}}_{n,p}(\\beta_{*},\\lambda),}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $\\mathcal{E}_{3}(p)$ and $\\mathcal{E}_{4}(p)$ are the approximation rates defined in Eqs. (100) and (103), and we denoted ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\widetilde{\\mathsf{B}}_{n,p}(\\beta_{*},\\lambda):=\\nu_{2}^{2}\\Big[\\langle\\beta_{*},(\\Sigma+\\nu_{2})^{-2}\\beta_{*}\\rangle+\\chi(\\nu_{2})\\langle\\beta_{*},\\Sigma(\\Sigma+\\nu_{2})^{-2}\\beta_{*}\\rangle\\Big].\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "The proof of Lemma B.13 can be found in Section B.7.5. ", "page_idx": 30}, {"type": "text", "text": "B.7 Technical results ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In this section, we prove the technical results that were deferred from the previous sections. We start with a lemma that gathers useful bounds on the deterministic functionals. ", "page_idx": 31}, {"type": "text", "text": "Lemma B.14. There exists a constant $C_{*}>0$ such that ", "page_idx": 31}, {"type": "equation", "text": "$$\n(1-\\Upsilon(\\nu_{1},\\nu_{2}))^{-1}\\leq C_{*}\\cdot\\widetilde{\\rho}_{\\lambda}(n,p),\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where we recall that $\\Upsilon(\\nu_{1},\\nu_{2})$ is defined as per Eq. (105). Furthermore, under Assumption 3.2, we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p\\nu_{1}\\frac{\\mathrm{Tr}\\left(\\Sigma\\left(\\Sigma+\\nu_{2}\\right)^{-2}\\right)}{p-\\mathrm{Tr}\\left(\\Sigma^{2}\\left(\\Sigma+\\nu_{2}\\right)^{-2}\\right)}\\leq C_{*}\\cdot n\\Upsilon(\\nu_{1},\\nu_{2}),}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad p\\nu_{1}\\frac{\\left\\langle\\beta_{*},\\,\\Sigma\\left(\\Sigma+\\nu_{2}\\right)^{-2}\\beta_{*}\\right\\rangle}{p-\\mathrm{Tr}\\left(\\Sigma^{2}\\left(\\Sigma+\\nu_{2}\\right)^{-2}\\right)}\\leq C_{*}\\cdot\\widetilde{\\mathsf{B}}_{n,p}(\\beta_{*},\\lambda),}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $\\widetilde{\\mathsf{B}}_{n,p}(\\beta_{*},\\lambda)$ is defined as per Eq. (118) in Lemma B.13. ", "page_idx": 31}, {"type": "text", "text": "Proof of Lemma B.14. Step 1: Equation (119). ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Note that we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\Upsilon(\\nu_{1},\\nu_{2})\\leq\\frac{1}{n}\\mathrm{Tr}(\\Sigma(\\Sigma+\\nu_{2})^{-1})\\leq\\frac{p}{n}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "In particular, if $n\\geq p/\\eta_{*}$ , then we can simply write ", "page_idx": 31}, {"type": "equation", "text": "$$\n(1-\\Upsilon(\\nu_{1},\\nu_{2}))^{-1}\\leq\\frac{1}{1-\\eta_{*}}=C_{*}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "For $n\\leq p/\\eta_{*}$ , note that using the first identity in Eq. (76), we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n(1-\\Upsilon(\\nu_{1},\\nu_{2}))^{-1}\\leq\\left(1-\\frac{1}{n}\\mathrm{Tr}(\\Sigma(\\Sigma+\\nu_{2})^{-1})\\right)^{-1}\\leq\\frac{n\\nu_{1}}{\\lambda}\\leq C_{*}\\rho_{\\lambda}(n).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Combining the previous two displays, we obtain Eq. (119). ", "page_idx": 31}, {"type": "text", "text": "Step 2: Equation (120). ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "We rewrite the left-hand side as ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p\\nu_{1}\\frac{\\mathrm{Tr}(\\Sigma(\\Sigma+\\nu_{2})^{-2})}{p-\\mathrm{Tr}(\\Sigma^{2}(\\Sigma+\\nu_{2})^{-2})}\\leq\\frac{p\\nu_{1}}{p-\\mathrm{Tr}(\\Sigma(\\Sigma+\\nu_{2})^{-1})}\\mathrm{Tr}(\\Sigma(\\Sigma+\\nu_{2})^{-2})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\mathrm{Tr}(\\Sigma(\\Sigma+\\nu_{2})^{-1})-\\mathrm{Tr}(\\Sigma^{2}(\\Sigma+\\nu_{2})^{-2})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\mathrm{\\Gamma}\\left(1-\\frac{1}{C_{*}}\\right)\\mathrm{Tr}(\\Sigma(\\Sigma+\\nu_{2})^{-1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where we uses the second of the identities (76) in the second line, and Assumption 3.2 in the third line. Hence, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p\\nu_{1}\\displaystyle\\frac{\\mathrm{Tr}(\\Sigma(\\Sigma+\\nu_{2})^{-2})}{p-\\mathrm{Tr}(\\Sigma^{2}(\\Sigma+\\nu_{2})^{-2})}\\le(\\mathbb{C}_{*}-1)\\cdot\\left\\{\\mathrm{Tr}(\\Sigma(\\Sigma+\\nu_{2})^{-1})-p\\nu_{1}\\displaystyle\\frac{\\mathrm{Tr}(\\Sigma(\\Sigma+\\nu_{2})^{-2})}{p-\\mathrm{Tr}(\\Sigma^{2}(\\Sigma+\\nu_{2})^{-2})}\\right\\}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=(\\mathbb{C}_{*}-1)\\cdot n\\Upsilon(\\nu_{1},\\nu_{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Step 3: Equation (121). ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Similarly, we get again using Eq. (76) and Assumption 3.2 that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p\\nu_{1}\\frac{\\langle\\beta_{*},\\,\\Sigma(\\Sigma+\\nu_{2})^{-2}\\beta_{*}\\rangle}{p-\\mathrm{Tr}(\\Sigma^{2}(\\Sigma+\\nu_{2})^{-2})}\\leq\\nu_{2}\\left\\{\\langle\\beta_{*},(\\Sigma+\\nu_{2})^{-1}\\beta_{*}\\rangle-\\nu_{2}\\langle\\beta_{*},\\Sigma^{2}(\\Sigma+\\nu_{2})^{-2}\\beta_{*}\\rangle\\right\\}}\\\\ &{\\phantom{p s p a c e{2p c}}p\\leq\\left(1-\\frac{1}{C_{*}}\\right)\\cdot\\nu_{2}\\langle\\beta_{*},(\\Sigma+\\nu_{2})^{-1}\\beta_{*}\\rangle,}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and therefore ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nu_{1}\\frac{\\langle\\beta_{*},\\,\\Sigma(\\pmb{\\Sigma}+\\nu_{2})^{-2}\\beta_{*}\\rangle}{p-\\mathrm{Tr}(\\mathbf{\\Sigma}^{2}(\\pmb{\\Sigma}+\\nu_{2})^{-2})}\\leq(C_{*}-1)\\left\\{\\nu_{2}\\langle\\beta_{*},(\\mathbf{\\Sigma}+\\nu_{2})^{-1}\\beta_{*}\\rangle-p\\nu_{1}\\frac{\\langle\\beta_{*},\\,\\Sigma(\\pmb{\\Sigma}+\\nu_{2})^{-2}\\beta_{*}\\rangle}{p-\\mathrm{Tr}(\\mathbf{\\Sigma}^{2}(\\pmb{\\Sigma}+\\nu_{2})^{-2})}\\right\\}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\widetilde{\\mathsf{B}}_{n,p}(\\beta_{*},\\lambda),}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "which concludes the proof of this lemma. ", "page_idx": 31}, {"type": "text", "text": "B.7.1 Feature covariance matrix ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Proof of Lemma B.2. Recall that we consider $\\widehat{\\boldsymbol{\\Sigma}}_{F}=p^{-1}\\boldsymbol{F}\\boldsymbol{F}^{\\top}$ , with $\\pmb{F}=[\\pmb{f}_{1},\\dots,\\pmb{f}_{p}]^{\\mathsf{T}}\\in\\mathbb{R}^{p\\times\\infty}$ and the $\\pmb{f}_{i}$ are i.i.d. random vectors satisfying Assumption B.1. For any integers $k_{2}\\geq k_{1}\\geq1$ , we split the weight feature matrix into $\\pmb{F}=[\\pmb{\\dot{F}}_{1},\\pmb{\\bar{F}}_{2},\\pmb{F}_{3}]$ , where $\\pmb{F}_{1}=[\\pmb{f}_{1,1},\\dots,\\pmb{f}_{1,p}]^{\\mathsf{T}}\\in\\mathbb{R}^{p\\times k_{1}}$ with ${\\pmb f}_{1,j}$ the first $k_{1}$ coordinates of the feature vector $\\pmb{f}_{j}$ , $\\pmb{F}_{2}=[\\pmb{f}_{2,1},\\ldots,\\pmb{f}_{2,p}]^{\\mathsf{T}}\\in\\mathbb{R}^{p\\times(k_{2}-k_{1})}$ with ${\\pmb f}_{2,j}$ the next $k_{2}-k_{1}$ coordinates of $\\pmb{f}_{j}$ , and $\\pmb{F}_{3}=[\\pmb{f}_{3,1},\\..\\ .\\ ,\\pmb{f}_{3,p}]^{\\mathsf{T}}\\in\\mathbb{R}^{p\\times\\infty}$ contains the rest of the coordinates. In other words, we split the feature vector into $\\pmb{f}_{j}=[\\pmb{f}_{1,j},\\pmb{f}_{2,j},\\pmb{f}_{3,j}]$ . Denote ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Sigma_{1}=\\mathbb{E}[f_{1,j}f_{1,j}^{\\top}]=\\mathrm{diag}(\\xi_{1}^{2},\\ldots,\\xi_{k_{1}}^{2})\\in\\mathbb{R}^{k_{1}\\times k_{1}},}\\\\ &{\\Sigma_{2}=\\mathbb{E}[f_{2,j}f_{2,j}^{\\top}]=\\mathrm{diag}(\\xi_{k_{1}+1}^{2},\\ldots,\\xi_{k_{2}}^{2})\\in\\mathbb{R}^{(k_{2}-k_{1})\\times(k_{2}-k_{1})},}\\\\ &{\\Sigma_{3}=\\mathbb{E}[f_{3,j}f_{3,j}^{\\top}]=\\mathrm{diag}(\\xi_{k_{2}+1}^{2},\\xi_{k_{2}+2}^{2},\\ldots)\\in\\mathbb{R}^{\\infty\\times\\infty}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We decompose the feature covariance matrix into ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\frac{\\boldsymbol{F}\\boldsymbol{F}^{\\top}}{p}=\\frac{\\boldsymbol{F}_{1}\\boldsymbol{F}_{1}^{\\top}}{p}+\\frac{\\boldsymbol{F}_{2}\\boldsymbol{F}_{2}^{\\top}}{p}+\\frac{\\boldsymbol{F}_{3}\\boldsymbol{F}_{3}^{\\top}}{p}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Step 1: Bounding the eigenvalues of $\\pmb{F}_{1}$ and $F_{2}$ . ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Introduce the whitened matrices ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overline{{\\mathbf{F}}}_{1}=F_{1}\\boldsymbol{\\Sigma}_{1}^{-1/2}=[\\overline{{\\mathbf{f}}}_{1,1},\\ldots,\\overline{{\\mathbf{f}}}_{1,p}]^{\\mathsf{T}},\\qquad\\quad\\overline{{\\mathbf{F}}}_{2}=F_{2}\\boldsymbol{\\Sigma}_{2}^{-1/2}=[\\overline{{\\mathbf{f}}}_{2,1},\\ldots,\\overline{{\\mathbf{f}}}_{2,p}]^{\\mathsf{T}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "so that the feature vectors $\\overline{{\\pmb{f}}}_{1,j}$ and $\\overline{{\\pmb{f}}}_{2,j}$ have covariance $\\mathbf{I}_{k_{1}}$ and $\\mathbf{I}_{k_{2}-k_{1}}$ respectively. We have ", "page_idx": 32}, {"type": "equation", "text": "$$\n||\\overline{{F}}_{1}^{\\mathsf{T}}\\overline{{F}}_{1}/p-\\mathbf{I}_{k_{1}}||_{\\mathrm{op}}=\\operatorname*{sup}_{\\substack{v\\in\\mathbb{R}^{k_{1}},\\|v\\|_{2}=1}}\\left|\\frac{1}{p}\\sum_{j\\in[p]}\\langle v,\\overline{{f}}_{1,j}\\rangle^{2}-1\\right|\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Denote $Z_{j}:=\\langle\\pmb{v},\\overline{{\\pmb{f}}}_{1,j}\\rangle^{2}-1$ . Then we have from Equation (60) applied to $\\pmb{B}=\\pmb{\\Sigma}_{1}^{-1/2}\\pmb{v}\\pmb{v}^{\\top}\\pmb{\\Sigma}_{1}^{-1/2}$ , for any integer $q\\geq1$ , ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{E}[|Z_{j}|^{q}]\\leq q\\mathsf{C}_{*}\\int_{0}^{\\infty}t^{q-1}e^{-\\mathsf{c}_{x}t}\\mathrm{d}t=\\frac{\\mathsf{C}_{*}q!}{\\mathsf{c}_{*}^{q}}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Hence we can apply Bernstein\u2019s inequality for centered sub-exponential random variable and obtain ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left|\\frac{1}{p}\\sum_{j\\in[p]}Z_{j}\\right|\\geq\\varepsilon/2\\right)\\leq2\\exp\\left\\{-c_{*}p\\cdot\\operatorname*{min}(\\varepsilon^{2},\\varepsilon)\\right\\}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Following the proof of [Vershynin, 2010, Theorem 5.39], we deduce that there exist constants $C_{*},C_{*,D}>0$ such that with probability at least $1-p^{-D}$ , ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\|\\overline{{F}}_{1}^{\\mathsf{T}}\\overline{{F}}_{1}/p-\\mathbf{I}_{k_{1}}\\|_{\\mathrm{op}}\\leq C_{*}\\sqrt{\\frac{k_{1}}{p}}+C_{*,D}\\sqrt{\\frac{\\log(p)}{p}}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "In particular, there exists $\\eta_{*}\\in(0,1/4)$ and $C_{*,D}>0$ such that for $p\\geq C_{*,D}$ and via union bound, we have with probability at least $1-p^{-D}$ (reparametrizing $D$ ), that for any $k_{1}\\leq\\lfloor\\eta_{*}\\cdot p\\rfloor$ , ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\|\\overline{{F}}_{1}^{\\mathsf{T}}\\overline{{F}}_{1}/p-\\mathbf{I}_{k_{1}}\\|_{\\mathrm{op}}\\leq1/2.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "From Eq. (122), we deduce that the $k_{1}$ -th eigenvalue of $\\widehat{\\Sigma}_{F}$ for $k_{1}<\\lfloor\\eta_{*}\\cdot p\\rfloor$ is lower bounded by ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\hat{\\xi}_{k_{1}}^{2}\\geq\\lambda_{\\operatorname*{min}}\\left(\\frac{F_{1}F_{1}^{\\mathsf{T}}}{p}\\right)\\geq\\xi_{k_{1}}^{2}\\cdot\\lambda_{\\operatorname*{min}}\\left(\\frac{\\overline{{F}}_{1}\\overline{{F}}_{1}^{\\mathsf{T}}}{p}\\right)\\geq\\frac{\\xi_{k_{1}}^{2}}{2},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "and ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{max}}\\left(\\frac{F_{1}F_{1}^{\\top}}{p}\\right)\\leq\\frac{3}{2}\\xi_{1}^{2}=\\frac{3}{2}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Similarly for $\\overline{{F}}_{2}$ , we have with probability at least $1-p^{-D}$ that for any $k_{1}<k_{2}\\leq\\lfloor\\eta_{*}\\cdot p\\rfloor$ , ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\|\\overline{{F}}_{2}^{\\mathsf{T}}\\overline{{F}}_{2}/p-\\mathbf{I}_{k_{2}-k_{1}}\\|_{\\mathrm{op}}\\leq1/2,\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and therefore ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{max}}\\left(\\frac{F_{2}F_{2}^{\\mathsf{T}}}{p}\\right)\\leq\\frac{3}{2}\\xi_{k_{1}+1}^{2}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Step 2: Bounding the eigenvalues of $\\pmb{F}$ . ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Equation (124) provides a lower bound on the eigenvalues $\\hat{\\xi}_{k}^{2}$ of $\\widehat{\\Sigma}_{F}$ up to $k<\\lfloor\\eta_{*}\\cdot p\\rfloor$ . Let\u2019s upper bound the $p$ eigenvalues of $\\widehat{\\Sigma}_{F}$ . From now on, set $k_{2}=p_{*}-1$ where $p_{*}:=\\lfloor\\eta_{*}\\cdot p\\rfloor$ . ", "page_idx": 33}, {"type": "text", "text": "For the contribution of $\\|\\pmb{F}_{+}\\|$ , we use the matrix Bernstein\u2019s inequality as in [Misiakiewicz and Saeed, 2024, Lemma 1] (see proof of Theorem A.2 in Appendix A) and obtain that for $p\\geq C_{K}$ , with probability at least $1-\\bar{p}^{-D}$ , ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\frac{1}{p}\\|F_{3}F_{3}^{\\mathsf{T}}\\|_{\\mathrm{op}}\\leq C_{*,D,K}\\cdot\\xi_{p_{*}}^{2}\\left\\{1+\\frac{r_{\\Sigma}(p_{*})\\vee p}{p}\\log\\left(r_{\\Sigma}(p_{*})\\vee p\\right)\\right\\}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "By the min-max theorem, we have with probability at least $1-p^{-D}$ for all $k_{1}<p_{*}-1$ , ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\overset{\\because}{\\underset{k_{1}+1}{2}}\\leq\\lambda_{\\operatorname*{max}}\\left(\\frac{F_{2}F_{2}^{\\top}}{p}+\\frac{F_{3}F_{3}^{\\top}}{p}\\right)\\leq\\frac{3}{2}\\xi_{k_{1}+1}^{2}+C_{*,D,K}\\cdot\\xi_{p_{*}}^{2}\\left\\{1+\\frac{r_{\\Sigma}(p_{*})\\vee p}{p}\\log\\left(r_{\\Sigma}(p_{*})\\vee p\\right)\\right\\},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where we used Eq. (125). For $k\\geq p_{*}$ , we simply use that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\hat{\\xi}_{k}^{2}\\leq\\frac{1}{p}\\|F_{3}F_{3}^{\\mathsf{T}}\\|_{\\mathrm{op}}\\leq\\frac{3}{2}\\xi_{k}^{2}+C_{*,D,K}\\cdot\\xi_{p_{*}}^{2}\\left\\{1+\\frac{r_{\\Sigma}(p_{*})\\vee p}{p}\\log\\left(r_{\\Sigma}(p_{*})\\vee p\\right)\\right\\}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We deduce from the above two displays that with probability at least $1-p^{-D}$ , we have for any $k\\leq p$ ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\hat{\\xi}_{k}^{2}\\le C_{*,K,D}\\cdot\\left\\{\\xi_{k}^{2}+\\xi_{p_{*}}^{2}\\cdot M_{\\Sigma}(p)\\right\\},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where we recall that we defined ", "page_idx": 33}, {"type": "equation", "text": "$$\nM_{\\Sigma}(k):=1+\\frac{r_{\\Sigma}(\\lfloor\\eta_{*}\\cdot k\\rfloor)\\vee k}{k}\\log\\left(r_{\\Sigma}(\\lfloor\\eta_{*}\\cdot k\\rfloor)\\vee k\\right).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Step 3: Bounding $r_{\\widehat{\\Sigma}_{F}}(k)$ for $k\\leq p$ . ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Recall that the intrinsic dimension $r_{\\widehat{\\Sigma}_{F}}(k)$ at level $k\\leq p$ is given by ", "page_idx": 33}, {"type": "equation", "text": "$$\nr_{\\widehat{\\Sigma}_{F}}(k)=\\frac{\\sum_{j=k}^{p}\\hat{\\xi}_{j}^{2}}{\\hat{\\xi}_{k}^{2}}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "First note that for $p\\geq k\\geq\\lfloor\\eta_{*}\\cdot p\\rfloor$ , we simply use that and the eigenvalues are nonincreasing to get ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\frac{\\sum_{j=k}^{p}\\hat{\\xi}_{j}^{2}}{\\hat{\\xi}_{k}^{2}}\\le(p+1-k)\\le C(1-\\eta_{*})p\\le C\\frac{1-\\eta_{*}}{\\eta_{*}}k.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "For $k\\leq p_{*}-1$ , we use that from Eq. (124) with probability at least $1-p^{-D}$ , ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\frac{\\sum_{j=k}^{p}\\hat{\\xi}_{j}^{2}}{\\hat{\\xi}_{k}^{2}}\\le\\frac{2}{\\xi_{k}^{2}}\\left(\\frac32\\sum_{j=k}^{\\lfloor\\eta_{*}\\cdot p\\rfloor-1}\\xi_{k}^{2}+\\sum_{j=\\lfloor\\eta_{*}\\cdot p\\rfloor}^{p}\\hat{\\xi}_{j}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Let\u2019s bound the second term on the right-hand side. Notice that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\sum_{j=\\lfloor\\eta_{*}\\cdot p\\rfloor+1}^{p}\\hat{\\xi}_{j}^{2}=\\operatorname*{min}_{V}\\mathrm{Tr}(\\widehat{\\Sigma}_{F}(\\mathbf{I}-V V^{\\top}))\\leq\\frac{1}{p}\\mathrm{Tr}(F_{3}F_{3}^{\\top}),\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where the minimization is over $V\\in\\mathbb{R}^{p\\times(\\lfloor\\eta_{*}\\cdot p\\rfloor-1)}$ with $V^{\\top}V=\\mathbf{I}_{\\lfloor\\eta_{*}\\cdot p\\rfloor-1}$ and the second inequality is obtained by taking $V$ orthogonal to the matrix $[F_{1},F_{2}]$ . We can rewrite ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{1}{p}\\mathrm{Tr}({\\cal F}_{3}{\\cal F}_{3}^{\\top})-\\mathrm{Tr}(\\Sigma_{3})=\\frac{1}{p}\\sum_{j\\in[p]}\\|{\\pmb{f}}_{3,j}\\|_{2}^{2}-\\mathrm{Tr}(\\Sigma_{3}).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Introduce $Z_{j}:=\\|f_{3,j}\\|_{2}^{2}-\\mathrm{Tr}(\\Sigma_{3})$ . By Assumption B.1 with $\\boldsymbol{B}\\,=\\,\\mathrm{diag}(\\mathbf{0,I})$ (identity on the subspace $\\Sigma_{3}$ and 0 otherwise), we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{E}[|Z_{j}|^{q}]\\leq q\\mathsf{C}_{x}\\|\\boldsymbol{\\Sigma}_{3}\\|_{F}^{q}\\int_{0}^{\\infty}t^{q-1}e^{-\\mathsf{c}_{x}t}\\mathrm{d}t\\leq\\mathsf{C}_{x}\\cdot\\mathrm{Tr}(\\boldsymbol{\\Sigma}_{3})^{q}\\frac{q!}{\\mathsf{c}_{x}^{q}}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We therefore we can apply Bernstein\u2019s inequality (123) again and we get ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left|\\frac{1}{p}\\sum_{j\\in[p]}Z_{j}\\right|\\geq t\\cdot\\mathrm{Tr}(\\Sigma_{3})\\right)\\leq2\\exp(-c_{*}\\operatorname*{min}(p t^{2},p t)).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Hence, for any $p\\geq C_{*,D}$ with probability at least $1-p^{-D}$ , ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{1}{p}\\mathrm{Tr}(F_{3}F_{3}^{\\top})\\leq2\\mathrm{Tr}(\\Sigma_{3}).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Combining the above display with the previous inequalities yields with probability at least $1-p^{-D}$ that for any k \u2264\u230a\u03b7\u2217\u00b7 p\u230b, ", "page_idx": 34}, {"type": "equation", "text": "$$\nr_{\\widehat{\\Sigma}_{F}}(k)=\\frac{\\sum_{j=k}^{p}\\hat{\\xi}_{j}^{2}}{\\hat{\\xi}_{k}^{2}}\\leq C\\frac{\\sum_{j=k}^{\\infty}\\xi_{j}^{2}}{\\xi_{k}^{2}}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We deduce that there exist a constant $C_{*}>0$ such that with probability at least $1-p^{-D}$ , we have for any $n_{*}:=\\lfloor\\eta_{*}\\cdot n\\rfloor\\le p$ ", "page_idx": 34}, {"type": "equation", "text": "$$\nr_{\\widehat{\\Sigma}_{F}}(n_{*})\\vee n\\le C_{*}\\cdot r_{\\Sigma}(n_{*})\\vee n,\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $r_{\\Sigma}(n)$ is the effective rank associated to $\\Sigma$ . ", "page_idx": 34}, {"type": "text", "text": "Step 4: Concluding the proof. ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "For any $n_{*}=\\lfloor\\eta_{*}\\cdot n\\rfloor>p$ , we have $\\widehat{\\rho}_{\\lambda}(n)=\\widetilde{\\rho}_{\\lambda}(n,p)=1$ . Hence, we only need to consider the case $n_{*}\\leq p$ . Using Eqs. (126) and (1 2 7), we g e t ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\rho}_{\\lambda}(n)=1+\\frac{n\\widehat{\\xi}_{n_{*}}^{2}}{\\lambda}\\left\\{1+\\frac{r_{\\widehat{\\Sigma}_{F}}(n_{*})\\vee n}{n}\\log\\left(r_{\\widehat{\\Sigma}_{F}}(n_{*})\\vee n\\right)\\right\\}}\\\\ &{\\qquad\\leq1+C_{*,D,K}\\cdot\\left\\{\\frac{n\\widehat{\\xi}_{n_{*}}^{2}}{\\lambda}+\\frac{n}{p}\\cdot\\frac{p\\cdot\\xi_{p_{*}}}{\\lambda}M_{\\Sigma}(p)\\right\\}M_{\\Sigma}(n)}\\\\ &{\\qquad\\leq1+C_{*,D,K}\\cdot\\left\\{\\frac{n\\widehat{\\xi}_{n_{*}}^{2}}{\\lambda}+\\frac{n}{p}\\cdot\\rho_{\\lambda}(p)\\right\\}M_{\\Sigma}(n),}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "which concludes the proof. ", "page_idx": 34}, {"type": "text", "text": "B.7.2 Concentration of the fixed points ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Proof of Proposition B.4. Recall that $(\\tilde{\\nu}_{1},\\tilde{\\nu}_{2})\\in\\mathbb{R}_{>0}^{2}$ are the unique solution to the random fixed point equations (77). From the first equation, we have the following bounds on $\\tilde{\\nu}_{1}$ : ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{p\\lambda}{n}\\leq p\\tilde{\\nu}_{1}\\leq\\frac{p\\|\\widehat{\\pmb{\\Sigma}}_{F}\\|_{\\mathrm{op}}+p\\lambda}{n}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "From Lemma B.2, we have with probability at least $1-p^{-D}$ that $\\|\\widehat{\\Sigma}_{F}\\|_{\\mathrm{op}}\\leq C_{*,D,K}\\leq p$ . Hence, by the uniform concentration over $\\kappa\\in[p\\lambda/n,(p^{3}+p\\lambda)/n]_{,}$ in Lemma B.15 stated below and an union bound, we deduce that with probability at least $1-\\dot{p}^{-}\\dot{D}$ ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Tr}\\big(F F^{\\top}(F F^{\\top}+p\\tilde{\\nu}_{1})^{-1}\\big)-\\mathrm{Tr}\\big(\\pmb{\\Sigma}(\\pmb{\\Sigma}+\\tilde{\\nu}_{2})^{-1}\\big)\\Big|\\leq C_{\\ast,K,D}\\frac{\\rho_{\\gamma_{+}}(p)^{5/2}\\log^{3}(p)}{\\sqrt{p}}\\mathrm{Tr}\\big(\\pmb{\\Sigma}(\\pmb{\\Sigma}+\\tilde{\\nu}_{2})^{-1}\\big)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=:\\tilde{\\mathcal{E}}_{2}\\cdot\\mathrm{Tr}\\big(\\pmb{\\Sigma}(\\pmb{\\Sigma}+\\tilde{\\nu}_{2})^{-1}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Therefore, we can rewrite the fixed equations (77) as ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{n-\\displaystyle\\frac{\\lambda}{\\widetilde{\\nu}_{1}}=\\mathrm{Tr}\\big(\\Sigma\\big(\\Sigma+\\widetilde{\\nu}_{2}\\big)^{-1}\\big)\\cdot(1+\\delta({\\cal F})),}}\\\\ {{p-\\displaystyle\\frac{p\\widetilde{\\nu}_{1}}{\\widetilde{\\nu}_{2}}=\\mathrm{Tr}\\left(\\Sigma(\\Sigma+\\widetilde{\\nu}_{2})^{-1}\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where with probability at least $1-p^{-D}$ , we have $|\\delta(\\boldsymbol{F})|\\leq\\widetilde{\\mathcal{E}}(\\boldsymbol{p})$ . Therefore, by condition (78) and $p\\geq C_{*,D,K}$ ", "page_idx": 35}, {"type": "equation", "text": "$$\nC_{*}\\widetilde{\\mathcal{E}}(p)\\cdot\\widetilde{\\rho}_{\\lambda}(n,p)\\leq\\frac{C_{*,D,K}}{\\log(p)}\\leq\\frac{1}{2},\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "and we can directly use Lemma B.17 stated below to obtain with probability at least $1-p^{-D}$ , ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\frac{|\\widetilde{\\nu}_{1}-\\nu_{1}|}{\\nu_{1}}\\leq C_{*}\\cdot\\mathcal{E}_{2}(p)\\cdot\\widetilde{\\rho}_{\\lambda}(n,p),\\qquad\\qquad\\frac{|\\widetilde{\\nu}_{2}-\\nu_{2}|}{\\nu_{2}}\\leq C_{*}\\cdot\\mathcal{E}_{2}(p)\\cdot\\widetilde{\\rho}_{\\lambda}(n,p).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "This concludes the proof of this proposition. ", "page_idx": 35}, {"type": "text", "text": "For any $\\kappa\\geq0$ , denote $\\nu_{2}(\\kappa)\\in\\mathbb{R}_{>0}$ the unique positive solution to ", "page_idx": 35}, {"type": "equation", "text": "$$\np-{\\frac{\\kappa}{\\nu_{2}(\\kappa)}}=\\operatorname{Tr}\\bigl(\\Sigma(\\Sigma+\\nu_{2}(\\kappa))^{-1}\\bigr).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "We will further define analogously to Eq. (80) the truncated fixed point ", "page_idx": 35}, {"type": "equation", "text": "$$\np-\\frac{\\gamma(\\kappa)}{\\nu_{2,0}(\\kappa)}=\\mathrm{Tr}\\bigl(\\Sigma_{0}(\\Sigma_{0}+\\nu_{2,0}(\\kappa))^{-1}\\bigr),\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where we recall that we denoted $\\gamma(\\kappa)=\\kappa+\\operatorname{Tr}(\\Sigma_{+})$ . It will be convenient to recall the notations ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\widetilde{\\Phi}_{2}(\\boldsymbol{F};\\kappa)=\\frac{1}{p}\\mathrm{Tr}\\big(\\boldsymbol{F}\\boldsymbol{F}^{\\intercal}(\\boldsymbol{F}\\boldsymbol{F}^{\\intercal}+\\kappa)^{-1}\\big),}\\\\ &{\\displaystyle\\widetilde{\\Phi}_{2}(\\boldsymbol{F}_{0};\\gamma(\\kappa))=\\frac{1}{p}\\mathrm{Tr}\\big(\\boldsymbol{F}_{0}\\boldsymbol{F}_{0}^{\\intercal}(\\boldsymbol{F}_{0}\\boldsymbol{F}_{0}^{\\intercal}+\\gamma(\\kappa))^{-1}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "and the deterministic equivalents ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\Psi_{2}(\\nu_{2}(\\kappa))=\\frac{1}{p}\\mathrm{Tr}\\big({\\bf\\Sigma}[{\\bf\\Sigma}({\\bf\\Sigma}+\\nu_{2}(\\kappa))^{-1}\\big),}}\\\\ {{\\displaystyle\\Psi_{2}(\\nu_{2,0}(\\kappa))=\\frac{1}{p}\\mathrm{Tr}\\big({\\bf\\Sigma}_{0}({\\bf\\Sigma}_{0}+\\nu_{2,0}(\\kappa))^{-1}\\big).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "The next lemma show that $\\widetilde{\\Phi}_{2}(\\pmb{F},\\kappa)$ concentrates on $\\Psi_{2}\\big(\\nu_{2}(\\kappa)\\big)$ uniformly on an interval of $\\kappa$ . ", "page_idx": 35}, {"type": "text", "text": "Lemma B.15. Under the setting of Proposition $B.4$ and for any $D,K\\ge0,$ , there exist constants $\\eta_{*}\\in(0,1/4)$ and $C_{*,D,K}>0$ such that the following holds. For any $p\\geq C_{*,D,K}$ and $\\lambda>0$ , if it holds that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\gamma_{\\lambda}=\\gamma(p\\lambda/n)\\ge p^{-K},\\qquad\\qquad\\rho_{\\gamma_{\\lambda}}(p)^{5/2}\\log^{3/2}(p)\\le K\\sqrt{p},\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "then with probability at least $1-p^{-D}$ , we have for any $\\kappa\\in[p\\lambda/n,p(p^{2}+\\lambda)/n]$ , ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left|\\widetilde{\\Phi}_{2}(F;\\kappa)-\\Psi_{2}(\\nu_{2}(\\kappa))\\right|\\leq C_{\\ast,D,K}\\frac{\\rho_{\\gamma(\\kappa)}(p)^{5/2}\\log^{3}(p)}{\\sqrt{p}}\\Psi_{2}(\\nu_{2}(\\kappa)).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof of Lemma B.15. Throughout the proof we assume that we are working on the event ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\|F_{+}F_{+}^{\\top}-\\gamma(0)\\mathbf{I}_{p}\\|_{\\mathrm{op}}\\leq C_{*,D}\\frac{\\log^{3}(p)}{\\sqrt{p}}\\gamma(p\\lambda/n),\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "which happens with probability at least $1\\mathrm{~-~}p^{-D}$ by Lemma B.3 via union bound. Note that $\\gamma(\\kappa)\\,\\geq\\,\\bar{\\gamma}(p\\lambda/n)$ for all the $\\kappa\\,\\stackrel{.}{\\in}\\,[p\\lambda/n,p(p^{2}+\\lambda)/\\bar{n}]$ . Furthermore, we assume that $\\textit{p}\\geq C_{*,D}$ chosen large enough so that $\\|F_{+}F_{+}^{\\top}-\\gamma(0)\\mathbf{I}_{p}\\|_{\\mathrm{op}}\\leq1/2\\cdot\\gamma(p\\lambda/n)$ so that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\boldsymbol{F}\\boldsymbol{F}^{\\intercal}+\\kappa\\succeq\\frac{1}{2}\\gamma(\\kappa).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "The proof will proceed via a standard union bound argument over $\\kappa$ in the interval $[p\\lambda/n,p(p^{2}\\!+\\!\\lambda)/n]$ . Let us first prove Eq. (131) for a fixed $\\kappa$ . We first simplify the functional by rewriting it as ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\widetilde{\\Phi}_{2}(\\pmb{F};\\kappa)=1-\\frac{\\kappa}{p}\\mathrm{Tr}\\big((\\pmb{F}\\pmb{F}^{\\top}+\\kappa)^{-1}\\big)}\\\\ {=1-\\frac{\\kappa}{p}\\mathrm{Tr}\\big((\\pmb{F}_{0}\\pmb{F}_{0}^{\\top}+\\gamma(\\kappa))^{-1}\\big)+\\frac{\\kappa}{p}\\Delta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where we denoted ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\Delta|=\\left|\\mathrm{Tr}\\big((\\mathbf{}F F^{\\mathsf{T}}+\\kappa)^{-1}\\big)-\\mathrm{Tr}\\big((F_{0}\\pmb{F}_{0}^{\\mathsf{T}}+\\gamma(\\kappa))^{-1}\\big)\\right|}\\\\ &{\\quad=\\left|\\mathrm{Tr}\\big((\\pmb{F}F^{\\mathsf{T}}+\\kappa)^{-1}(F_{1}\\pmb{F}_{1}^{\\mathsf{T}}-\\gamma(0)\\mathbf{I}_{p})(F_{0}\\pmb{F}_{0}^{\\mathsf{T}}+\\gamma(\\kappa))^{-1}\\big)\\right|}\\\\ &{\\quad\\leq C_{*,D}\\frac{\\log^{3}(p)}{\\sqrt{p}}\\left|\\mathrm{Tr}\\big((\\pmb{F}_{0}\\pmb{F}_{0}^{\\mathsf{T}}+\\gamma(\\kappa))^{-1}\\big)\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Using again the above identity, we rewrite ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\frac{1}{p}\\mathrm{Tr}\\bigl((F_{0}F_{0}^{\\mathsf{T}}+\\gamma(\\kappa))^{-1}\\bigr)=\\frac{1}{\\gamma(\\kappa)}-\\frac{1}{\\gamma(\\kappa)}\\widetilde{\\Phi}_{2}(F_{0};\\gamma(\\kappa)).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "We can now apply Eq. (55) in Theorem A.2: under the assumption of Proposition B.4 and recalling the conditions (130), we have with probability at least $1-p^{-\\overset{}{D}}$ , ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\left\\vert\\widetilde{\\Phi}_{2}(F_{0};\\gamma(\\kappa))-\\Psi_{2}(\\nu_{2,0}(\\kappa))\\right\\vert\\leq C_{\\ast,D,K}\\frac{\\rho_{\\gamma(\\kappa)}(p)^{5/2}\\log^{3/2}(p)}{\\sqrt{p}}\\Psi_{2}(\\nu_{2,0}(\\kappa)).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Combining the above displays, we obtain that with probability at least $1-p^{-D}$ ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left|\\widetilde{\\Phi}_{2}(F;\\kappa)-\\Psi_{2}(\\nu_{2,0}(\\kappa))-\\frac{\\mathrm{Tr}(\\Sigma_{+})}{p\\nu_{2,0}}\\right|}\\\\ &{\\le\\displaystyle\\frac{\\kappa}{\\gamma(\\kappa)}\\left|\\widetilde{\\Phi}_{2}(F_{0};\\gamma(\\kappa))-\\Psi_{2}(\\nu_{2,0}(\\kappa))\\right|+\\kappa|\\Delta|}\\\\ &{\\le C_{*,D,K}\\displaystyle\\frac{\\rho_{\\gamma(\\kappa)}(p)^{5/2}\\log^{3/2}(p)}{\\sqrt{p}}\\Psi_{2}(\\nu_{2,0}(\\kappa))+C_{*,D}\\displaystyle\\frac{\\log^{3}(p)}{\\sqrt{p}}\\left|\\frac{\\kappa}{p}\\mathrm{Tr}\\big((F_{0}F_{0}^{\\intercal}+\\gamma(\\kappa))^{-1}\\big)\\right|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where we used in the first inequality identity (129) to get ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\left(1-\\frac{\\kappa}{\\gamma(\\kappa)}\\right)\\left(1-\\frac{1}{p}\\mathrm{Tr}(\\Sigma_{0}(\\Sigma_{0}+\\nu_{2,0})^{-1})\\right)=\\frac{\\mathrm{Tr}(\\Sigma_{+})}{p\\nu_{2,0}}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Further note that using condition (130), we can simplify the right-hand side ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\kappa\\mathrm{Tr}\\big((F_{0}F_{0}^{\\intercal}+\\gamma(\\kappa))^{-1}\\big)\\right|\\leq\\cfrac{\\kappa}{\\gamma(\\kappa)}\\left|1-\\Psi_{2}(\\nu_{2,0}(\\kappa))\\right|+C_{*,D,K}\\cdot K\\cdot\\Psi_{2}(\\nu_{2,0}(\\kappa))}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq C_{*,D,K}\\left\\{\\Psi_{2}(\\nu_{2,0}(\\kappa))+\\cfrac{\\mathrm{Tr}(\\Sigma_{+})}{p\\nu_{2,0}}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "We can now use Eq. (82) in Lemma B.5 applied to $\\nu_{2}(\\kappa)$ and $\\nu_{2,0}(\\kappa)$ to concluded that with probability at least $1-p^{-D}$ , ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\Big|\\widetilde{\\Phi}_{2}(F;\\kappa)-\\Psi_{2}(\\nu_{2}(\\kappa))\\Big|\\le\\widetilde{\\mathcal{E}}(p)\\cdot\\Psi_{2}(\\nu_{2}(\\kappa)),\\qquad\\widetilde{\\mathcal{E}}(p):=C_{*,D,K}\\frac{\\rho_{\\gamma(\\kappa)}(p)^{5/2}\\log^{3}(p)}{\\sqrt{p}}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "We now consider a $p^{-P}$ -grid ${\\mathcal{P}}_{n}$ of the interval $\\kappa\\in[p\\lambda/n,p(p^{2}+\\lambda)/n]$ , which contains at most $p^{P+3}$ points. We can use a union bound over $\\kappa\\in\\mathcal{P}_{n}$ and reparametrizing $D^{\\prime}=D+P+3$ , so that with probability at least $1-p^{-D}$ , Equation (132) holds for any $\\kappa\\in\\mathcal{P}_{n}$ . Then for any point $\\kappa_{1}\\in[p\\lambda/n,p(K p^{2}+\\lambda)/n]$ , denote $\\kappa_{0}\\in\\mathcal{P}_{n}$ its closest point. Then by Lemma B.16 stated below, we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left|\\widetilde{\\Phi}_{2}(F;\\kappa_{1})-\\Psi_{2}(\\nu_{2}(\\kappa_{1}))\\right|}\\\\ &{\\le\\Big|\\widetilde{\\Phi}_{2}(F;\\kappa_{1})-\\widetilde{\\Phi}_{2}(F;\\kappa_{0})\\Big|+\\Big|\\widetilde{\\Phi}_{2}(F;\\kappa_{0})-\\Psi_{2}(\\nu_{2}(\\kappa_{0}))\\Big|+|\\Psi_{2}(\\nu_{2}(\\kappa_{0}))-\\Psi_{2}(\\nu_{2}(\\kappa_{1}))|}\\\\ &{\\le p^{C K}|\\kappa_{1}-\\kappa_{0}|\\widetilde{\\Phi}_{2}(F;\\kappa_{0})+\\widetilde{\\mathcal{E}}(p)\\cdot\\Psi_{2}(\\nu_{2}(\\kappa_{0}))+p^{C K}|\\kappa_{1}-\\kappa_{0}|\\Psi_{2}(\\nu_{2}(\\kappa_{1}))}\\\\ &{\\le\\Big(p^{C K-P}+\\widetilde{\\mathcal{E}}(p)\\Big)\\left(1+p^{C K-P}+\\widetilde{\\mathcal{E}}(p)\\right)\\cdot\\Psi_{2}(\\nu_{2}(\\kappa_{1})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Lemma B.16. Under the setting of Proposition $B.4$ and for any $D,K\\ge0,$ , there exist constants $\\eta_{*}\\in(0,1/4)$ , $C>0$ and $C_{*,D}>0$ such that the following holds. For any $p\\geq1$ and $\\lambda>0$ , it holds that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\gamma(p\\lambda/n)=\\frac{p\\lambda}{n}+\\mathrm{Tr}(\\Sigma_{+})\\geq p^{-K},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "then for any $\\kappa_{0},\\kappa_{1}\\geq p\\lambda/n$ , we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\left|\\frac{\\Psi_{2}(\\nu_{2}(\\kappa_{1}))}{\\Psi_{2}(\\nu_{2}(\\kappa_{0}))}-1\\right|\\leq p^{C K}|\\kappa_{1}-\\kappa_{0}|.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Furthermore, if $\\boldsymbol{p}\\ge C_{*,D}$ , then we have with probability $1-p^{-D}$ that for any $\\kappa_{1},\\kappa_{2}\\geq p\\lambda/n,$ , ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\left|\\frac{\\widetilde{\\Phi}_{2}({\\cal F};\\kappa_{1})}{\\widetilde{\\Phi}_{2}({\\cal F};\\kappa_{0})}-1\\right|\\leq p^{C K}|\\kappa_{1}-\\kappa_{0}|.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof of Lemma B.16. Using the identity (128), we can decompose the first difference into ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\frac{\\Psi_{2}\\left(\\nu_{2}\\left(\\kappa_{1}\\right)\\right)}{\\Psi_{2}\\left(\\nu_{2}\\left(\\kappa_{0}\\right)\\right)}-1\\right|=\\frac{\\left|\\frac{\\kappa_{0}}{\\nu_{2}\\left(\\kappa_{0}\\right)}-\\frac{\\kappa_{1}}{\\nu_{2}\\left(\\kappa_{1}\\right)}\\right|}{p\\Psi_{2}\\left(\\nu_{2}\\left(\\kappa_{0}\\right)\\right)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\frac{1}{\\nu_{2}\\left(\\kappa_{0}\\right)\\cdot p\\Psi_{2}\\left(\\nu_{2}\\left(\\kappa_{0}\\right)\\right)}\\left\\{\\left|\\kappa_{1}-\\kappa_{0}\\right|+\\left|\\frac{\\nu_{2}\\left(\\kappa_{0}\\right)}{\\nu_{2}\\left(\\kappa_{1}\\right)}-1\\right|\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "From condition (133) and the proof of [Misiakiewicz and Saeed, 2024, Lemma 11], we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{\\nu_{2}(\\kappa_{0})\\cdot p\\Psi_{2}(\\nu_{2}(\\kappa_{0}))}\\leq p^{3+2K},}\\\\ &{\\qquad\\qquad\\Big|\\frac{\\nu_{2}(\\kappa_{0})}{\\nu_{2}(\\kappa_{1})}-1\\Big|\\leq p^{2+2K}|\\kappa_{1}-\\kappa_{0}|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Combining the above inequalities, we deduce that there exists a constant $C>0$ such that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\left|\\frac{\\Psi_{2}(\\nu_{2}(\\kappa_{1}))}{\\Psi_{2}(\\nu_{2}(\\kappa_{0}))}-1\\right|\\leq p^{C K}|\\kappa_{1}-\\kappa_{0}|.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "For the second inequality (135), we rewrite the difference as ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\frac{\\widetilde{\\Phi}_{2}(\\boldsymbol{F};\\kappa_{1})}{\\widetilde{\\Phi}_{2}(\\boldsymbol{F};\\kappa_{0})}-1\\right|=\\frac{\\mathrm{Tr}(\\boldsymbol{F}\\boldsymbol{F}^{\\intercal}(\\boldsymbol{F}\\boldsymbol{F}^{\\intercal}+\\kappa_{1})^{-1}(\\boldsymbol{F}\\boldsymbol{F}^{\\intercal}+\\kappa_{0})^{-1})}{\\mathrm{Tr}(\\boldsymbol{F}\\boldsymbol{F}^{\\intercal}(\\boldsymbol{F}\\boldsymbol{F}^{\\intercal}+\\kappa_{0})^{-1})}|\\kappa_{1}-\\kappa_{0}|}\\\\ {\\leq\\|(\\boldsymbol{F}\\boldsymbol{F}^{\\intercal}+\\kappa_{1})^{-1}\\|_{\\mathrm{op}}|\\kappa_{1}-\\kappa_{0}|.\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Hence, recalling Lemma B.3 and condition (133), for any $p\\geq C_{*,D}$ , we get with probability at least $1-p^{-D}$ and for all $\\kappa_{1},\\kappa_{2}\\geq p\\lambda/n$ , ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\left|\\frac{\\widetilde{\\Phi}_{2}(\\pmb{F};\\kappa_{1})}{\\widetilde{\\Phi}_{2}(\\pmb{F};\\kappa_{0})}-1\\right|\\leq\\frac{2}{\\gamma(\\kappa_{1})}|\\kappa_{1}-\\kappa_{0}|\\leq n^{K}|\\kappa_{1}-\\kappa_{0}|,\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "which concludes the proof of this lemma. ", "page_idx": 37}, {"type": "text", "text": "We consider $\\left(\\nu_{1}^{\\varepsilon},\\nu_{2}^{\\varepsilon}\\right)\\in\\mathbb{R}_{\\ge0}$ the unique positive solutions of the perturbed equations: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle n-\\frac{\\lambda}{\\nu_{1}^{\\varepsilon}}=\\mathrm{Tr}\\left(\\Sigma(\\Sigma+\\nu_{2}^{\\varepsilon})^{-1}\\right)(1+\\varepsilon),}}\\\\ {{\\displaystyle p-\\frac{p\\nu_{1}^{\\varepsilon}}{\\nu_{2}^{\\varepsilon}}=\\mathrm{Tr}\\left(\\Sigma(\\Sigma+\\nu_{2}^{\\varepsilon})^{-1}\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Lemma B.17. Let $\\eta_{*}\\in(0,1/4)$ be chosen as in Proposition B.4. Then there exists $C_{*},C_{*}^{\\prime}>0$ such that for any $\\varepsilon\\in\\mathbb R$ with ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\begin{array}{c}{{\\left|\\varepsilon\\right|\\cdot C_{*}\\cdot\\widetilde{\\rho}_{\\lambda}(n,p)\\leq\\frac{1}{2},}}\\\\ {{\\left|\\frac{\\nu_{1}^{\\varepsilon}-\\nu_{1}^{0}}{\\nu_{1}^{0}}\\right|\\leq C_{*}^{\\prime}\\cdot\\widetilde{\\rho}_{\\lambda}(n,p)\\cdot\\left|\\varepsilon\\right|,}}\\end{array}}}\\\\ {{\\begin{array}{c}{{\\left|\\frac{\\nu_{2}^{\\varepsilon}-\\nu_{2}^{0}}{\\nu_{1}^{0}}\\right|\\leq C_{*}^{\\prime}\\cdot\\widetilde{\\rho}_{\\lambda}(n,p)\\cdot\\left|\\varepsilon\\right|,}}\\\\ {{\\left|\\frac{\\nu_{2}^{0}}{\\nu_{1}^{0}}\\right|\\leq C_{*}^{\\prime}\\cdot\\widetilde{\\rho}_{\\lambda}(n,p)\\cdot\\left|\\varepsilon\\right|.}}\\end{array}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "then ", "page_idx": 38}, {"type": "text", "text": "Proof of Lemma B.17. For convenience, we introduce the notations ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\delta_{1}:=\\frac{\\nu_{1}^{\\varepsilon}-\\nu_{1}^{0}}{\\nu_{1}^{\\varepsilon}},\\qquad\\qquad\\delta_{2}:=\\frac{\\nu_{2}^{\\varepsilon}-\\nu_{2}^{0}}{\\nu_{2}^{\\varepsilon}}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "We first consider the second equation (137) and subtract the identities for $(\\nu_{1}^{\\varepsilon},\\nu_{2}^{\\varepsilon})$ and $(\\nu_{1}^{0},\\nu_{2}^{0})$ to obtain ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad p\\left(\\frac{\\nu_{1}^{0}}{\\nu_{2}^{0}}-\\frac{\\nu_{1}^{\\varepsilon}}{\\nu_{2}^{\\varepsilon}}\\right)+\\mathrm{Tr}\\left(\\Sigma(\\Sigma+\\nu_{2}^{0})^{-1}\\right)-\\mathrm{Tr}\\left(\\Sigma(\\Sigma+\\nu_{2}^{\\varepsilon})^{-1}\\right)}\\\\ &{=p\\left[\\frac{\\nu_{1}^{0}}{\\nu_{2}^{0}}\\delta_{2}-\\frac{\\nu_{1}^{\\varepsilon}}{\\nu_{2}^{\\varepsilon}}\\delta_{1}\\right]+\\delta_{2}\\cdot\\nu_{2}^{\\varepsilon}\\mathrm{Tr}(\\Sigma(\\Sigma+\\nu_{2}^{0})^{-1}(\\Sigma+\\nu_{2}^{\\varepsilon})^{-1})=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Hence, we obtain the first identity ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\delta_{2}=\\delta_{1}\\frac{(\\nu_{1}^{\\varepsilon}/\\nu_{2}^{\\varepsilon})}{(\\nu_{1}^{0}/\\nu_{2}^{0})+\\frac{\\nu_{2}^{\\varepsilon}}{p}\\mathrm{Tr}(\\Sigma(\\Sigma+\\nu_{2}^{0})^{-1}(\\Sigma+\\nu_{2}^{\\varepsilon})^{-1})}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "We now turn to the first equation (136): we obtain similarly ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\lambda\\left(\\cfrac{1}{\\nu_{1}^{0}}-\\cfrac{1}{\\nu_{1}^{\\varepsilon}}\\right)=\\operatorname{Tr}\\left(\\Sigma(\\Sigma+\\nu_{2}^{\\varepsilon})^{-1}\\right)(1+\\varepsilon)-\\operatorname{Tr}\\left(\\Sigma(\\Sigma+\\nu_{2}^{0})^{-1}\\right)}\\\\ &{\\Longrightarrow\\,\\frac{\\lambda}{\\nu_{1}^{0}}\\delta_{1}=\\delta_{2}\\cdot\\nu_{2}^{\\varepsilon}\\operatorname{Tr}(\\Sigma(\\Sigma+\\nu_{2}^{0})^{-1}(\\Sigma+\\nu_{2}^{\\varepsilon})^{-1})(1+\\varepsilon)+\\varepsilon\\operatorname{Tr}\\left(\\Sigma(\\Sigma+\\nu_{2}^{0})^{-1}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Hence, rearranging the term and recalling the first identity (136), we get the second identity ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\delta_{1}\\left[\\frac{\\lambda}{\\nu_{1}^{0}}+(1+\\varepsilon)\\frac{\\nu_{2}^{\\varepsilon}\\mathrm{Tr}(\\Sigma(\\Sigma+\\nu_{2}^{0})^{-1}(\\Sigma+\\nu_{2}^{\\varepsilon})^{-1})\\cdot(\\nu_{1}^{\\varepsilon}/\\nu_{2}^{\\varepsilon})}{(\\nu_{1}^{0}/\\nu_{2}^{0})+\\frac{\\nu_{2}^{\\varepsilon}}{p}\\mathrm{Tr}(\\Sigma(\\Sigma+\\nu_{2}^{0})^{-1}(\\Sigma+\\nu_{2}^{\\varepsilon})^{-1})}\\right]=\\varepsilon\\mathrm{Tr}\\left(\\Sigma(\\Sigma+\\nu_{2}^{0})^{-1}\\right).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "From this identity, we directly have ", "page_idx": 38}, {"type": "equation", "text": "$$\n|\\delta_{1}|\\leq|\\varepsilon|\\cdot\\frac{\\nu_{1}^{0}}{\\lambda}\\mathrm{Tr}(\\Sigma(\\Sigma+\\nu_{2}^{0})^{-1}).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Note that for $n\\geq p/\\eta_{*}$ , we simply have by Eq. (136) that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\frac{\\nu_{1}^{0}}{\\lambda}\\mathrm{Tr}(\\Sigma(\\Sigma+\\nu_{2}^{0})^{-1})=\\frac{\\mathrm{Tr}(\\Sigma(\\Sigma+\\nu_{2}^{0})^{-1})}{n-\\mathrm{Tr}(\\Sigma(\\Sigma+\\nu_{2}^{0})^{-1})}\\leq\\frac{p}{n-p}\\leq\\frac{\\eta_{*}}{1-\\eta_{*}},\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where we use that $\\mathrm{Tr}(\\Sigma(\\Sigma+\\nu_{2}^{0})^{-1})\\leq p$ by Eq. (137). For $n\\leq p/\\eta_{*}$ , let\u2019s denote $\\mu_{1}^{0}=\\lambda/\\nu_{1}^{0}$ . Rewriting Eq. (136), we get that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mu_{*}=\\frac{n}{1+\\mathrm{Tr}(\\Sigma(\\mu_{*}\\Sigma+\\mu_{*}\\nu_{2}^{0})^{-1})}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Hence ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\slash_{1}^{0}}{\\lambda}\\mathrm{Tr}(\\Sigma(\\Sigma+\\nu_{2}^{0})^{-1})=\\mathrm{Tr}(\\Sigma_{<\\lfloor\\eta_{*}\\cdot n\\rfloor}(\\mu_{1}^{0}\\Sigma_{<\\lfloor\\eta_{*}\\cdot n\\rfloor}+\\mu_{1}^{0}\\nu_{2}^{0})^{-1})+\\mathrm{Tr}(\\Sigma_{\\geq\\lfloor\\eta_{*}\\cdot n\\rfloor}(\\mu_{1}^{0}\\Sigma_{\\geq\\lfloor\\eta_{*}\\cdot n\\rfloor}+\\mu_{1}^{0}\\nu_{2}^{0})^{-1})}\\\\ &{\\leq\\frac{\\eta_{*}\\cdot n}{\\mu_{1}^{0}}+\\frac{\\mathrm{Tr}(\\Sigma_{\\geq\\lfloor\\eta_{*}\\cdot n\\rfloor})}{\\mu_{1}^{0}\\nu_{2}^{0}}}\\\\ &{\\leq\\eta_{*}\\left\\{1+\\mathrm{Tr}(\\Sigma(\\mu_{*}\\Sigma+\\mu_{*}\\nu_{2}^{0})^{-1})\\right\\}+\\frac{\\mathrm{Tr}(\\Sigma_{\\geq\\lfloor\\eta_{*}\\cdot n\\rfloor})}{\\lambda},}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where we use in the last inequality that $\\nu_{1}^{0}/\\nu_{2}^{0}\\leq1$ and the definition of the effective rank. Rearranging the terms we obtain ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\frac{\\nu_{1}^{0}}{\\lambda}\\mathrm{Tr}(\\Sigma(\\Sigma+\\nu_{2}^{0})^{-1})\\leq\\frac{1}{1-\\eta_{*}}\\left\\{1+\\frac{\\mathrm{Tr}(\\Sigma_{\\geq\\lfloor\\eta_{*}\\cdot n\\rfloor})}{\\lambda}\\right\\}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Combining the above bounds we deduce that ", "page_idx": 39}, {"type": "equation", "text": "$$\n|\\delta_{1}|\\leq C_{*}|\\varepsilon|\\cdot\\left\\{1+\\mathbb{1}_{n\\leq p/\\eta_{*}}\\frac{\\mathrm{Tr}(\\Sigma_{\\geq\\lfloor\\eta_{*}\\cdot n\\rfloor})}{\\lambda}\\right\\}\\leq C_{*}\\cdot\\widetilde{\\rho}_{\\lambda}(n,p)\\cdot|\\varepsilon|.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "By assumption $C_{*}\\cdot\\widetilde{\\rho}_{\\lambda}(n,p)\\cdot|\\varepsilon|\\leq1/2$ and therefore $\\nu_{1}^{\\varepsilon}\\leq2\\nu_{1}^{0}$ . We conclude the first inequality ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\left|\\frac{\\nu_{1}^{\\varepsilon}-\\nu_{1}^{0}}{\\nu_{1}^{0}}\\right|\\leq\\frac{\\nu_{1}^{\\varepsilon}}{\\nu_{1}^{0}}|\\delta_{1}|\\leq C_{*}\\cdot\\widetilde{\\rho}_{\\lambda}(n,p)\\cdot|\\varepsilon|.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Recalling Eq. (138), we have directly ", "page_idx": 39}, {"type": "equation", "text": "$$\n|\\delta_{2}|\\leq|\\delta_{1}|\\frac{\\nu_{1}^{\\varepsilon}\\nu_{2}^{0}}{\\nu_{1}^{0}\\nu_{2}^{\\varepsilon}}\\quad\\Longrightarrow\\quad\\left|\\frac{\\nu_{2}^{\\varepsilon}-\\nu_{2}^{0}}{\\nu_{2}^{0}}\\right|\\leq\\left|\\frac{\\nu_{1}^{\\varepsilon}-\\nu_{1}^{0}}{\\nu_{1}^{0}}\\right|,\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "which concludes the proof. ", "page_idx": 39}, {"type": "text", "text": "B.7.3 Proof of deterministic equivalents for functionals on Z ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Proof of Proposition B.6. Recall that $\\widehat{\\rho}_{\\lambda}(n)$ is defined in Eq. (72) and that for $F\\in\\mathcal{A}_{\\mathcal{F}}$ , we have $\\widehat{\\rho}_{\\lambda}(n)\\leq C_{*,D,K}\\widetilde{\\rho}_{\\lambda}(n,p)$ for all $n\\in\\mathbb N$ . Under the assumptions of Proposition B.6, we can apply Theorem A.2 to $Z$ conditional on $\\pmb{F}$ to obtain that with probability at least $1-n^{-D}$ , ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\Big\\vert\\Phi_{2}(Z;\\lambda)-\\frac{p}{n}\\widetilde{\\Phi}_{2}(F;p\\widetilde{\\nu}_{1})\\Big\\vert\\leq C_{\\ast,D,K}\\frac{\\widetilde{\\rho}_{\\lambda}(n,p)^{5/2}\\log^{3/2}(n)}{\\sqrt{n}}\\cdot\\frac{p}{n}\\widetilde{\\Phi}_{2}(F;p\\widetilde{\\nu}_{1}),}}\\\\ &{}&{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\Phi_{3}(Z;A,\\lambda)-\\bigg(\\frac{n\\widetilde{\\nu}_{1}}{\\lambda}\\bigg)^{2}\\widetilde{\\Phi}_{5}(F;A,p\\widetilde{\\nu}_{1})\\Bigg\\vert\\leq C_{\\ast,D,K}\\frac{\\widetilde{\\rho}_{\\lambda}(n,p)^{6}\\log^{5/2}(n)}{\\sqrt{n}}\\cdot\\bigg(\\frac{n\\widetilde{\\nu}_{1}}{\\lambda}\\bigg)^{2}\\widetilde{\\Phi}_{5}(F;A,p\\widetilde{\\nu}_{1}),}\\\\ &{}&{\\quad\\quad\\quad\\quad\\quad\\quad\\Big\\vert\\Phi_{4}(Z;A,\\lambda)-\\widetilde{\\Phi}_{5}(F;A,p\\widetilde{\\nu}_{1})\\Big\\vert\\leq C_{\\ast,D,K}\\frac{\\widetilde{\\rho}_{\\lambda}(n,p)^{6}\\log^{3/2}(n)}{\\sqrt{n}}\\cdot\\widetilde{\\Phi}_{5}(F;A,p\\widetilde{\\nu}_{1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where $\\tilde{\\nu}_{1}$ is the solution of the fixed point equation (77). We conclude the proof using Lemma B.18 stated below and that by condition (85), we have $C_{*,D,K}\\cdot\\widetilde{\\rho}_{\\lambda}(n,p)\\mathcal{E}_{\\nu}(p)\\leq C_{*,D,K}K.$ \u53e3 ", "page_idx": 39}, {"type": "text", "text": "Proof of Proposition B.7. Again, under the assumptions of the proposition and for $F\\in\\mathcal{A}_{\\mathcal{F}}$ , we can apply [Misiakiewicz and Saeed, 2024, Lemma 10] to get ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left|\\langle u,Z(Z^{\\mathsf{T}}Z+\\lambda)^{-1}\\widehat{\\Sigma}_{F}(Z^{\\mathsf{T}}Z+\\lambda)^{-1}Z^{\\mathsf{T}}u\\rangle-n\\widetilde{\\Phi}_{5}(F;\\mathbf{I},p\\widetilde{\\nu}_{1})\\right|\\leq C_{*,D,K}\\frac{\\widetilde{\\rho}_{\\lambda}(n,p)^{6}\\log^{7/2}(n)}{\\sqrt{n}},}\\\\ &{Z(Z^{\\mathsf{T}}Z+\\lambda)^{-1}\\widehat{\\Sigma}_{F}(Z^{\\mathsf{T}}Z+\\lambda)^{-1}Z^{\\mathsf{T}}v\\rangle\\Big|\\leq C_{*,D,K}\\frac{\\widetilde{\\rho}_{\\lambda}(n,p)^{6}\\log^{7/2}(n)}{\\sqrt{n}}\\cdot\\frac{n\\nu_{1}}{\\lambda}\\sqrt{\\widetilde{\\Phi}_{5}(F;\\overline{{v\\circ}}^{\\mathsf{T}},p\\widetilde{\\nu}_{1})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "We conclude by combining these inequalities with Lemma B.18. ", "page_idx": 39}, {"type": "text", "text": "Lemma B.18. For $F\\in\\mathcal{A}_{\\mathcal{F}}$ , we have ", "text_level": 1, "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\widetilde{\\Phi}_{2}(F;p\\widetilde{\\nu}_{1})-\\widetilde{\\Phi}_{2}(F;p\\nu_{1})\\right|\\leq C_{*,D,K}\\cdot\\mathcal{E}_{\\nu}(p)\\cdot\\widetilde{\\Phi}_{2}(F;p\\nu_{1}),}\\\\ &{\\left|\\widetilde{\\Phi}_{5}(F;A,p\\widetilde{\\nu}_{1})-\\widetilde{\\Phi}_{5}(F;A,p\\nu_{1})\\right|\\leq C_{*,D,K}\\cdot\\widetilde{\\rho}_{\\lambda}(n,p)\\mathcal{E}_{\\nu}(p)\\cdot\\widetilde{\\Phi}_{5}(F;A,p\\nu_{1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "and ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left|\\left(\\frac{n\\tilde{\\nu}_{1}}{\\lambda}\\right)^{2}\\widetilde{\\Phi}_{5}(\\pmb{F};\\pmb{A},p\\tilde{\\nu}_{1})-\\left(\\frac{n\\nu_{1}}{\\lambda}\\right)^{2}\\widetilde{\\Phi}_{5}(\\pmb{F};\\pmb{A},p\\nu_{1})\\right|}\\\\ {\\displaystyle\\leq C_{*,D,K}\\cdot\\widetilde{\\rho}_{\\lambda}(n,p)\\mathcal{E}_{\\nu}(p)\\cdot\\left(\\frac{n\\nu_{1}}{\\lambda}\\right)^{2}\\widetilde{\\Phi}_{5}(\\pmb{F};\\pmb{A},p\\nu_{1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Proof of Lemma B.18. For convenience, we denote $\\kappa:=\\nu_{1}$ , $\\kappa^{\\prime}:=\\tilde{\\nu}_{1}$ , and $\\mathbf{T}:=\\widehat{\\boldsymbol{\\Sigma}}_{F}$ . For Eq. (140), we simply use that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|p\\widetilde{\\Phi}_{2}(F;p\\widetilde{\\nu}_{1})-p\\widetilde{\\Phi}_{2}(F;p\\nu_{1})\\right|=\\left|\\mathrm{Tr}\\left(\\mathbf{\\Gamma}(\\mathbf{\\Gamma}(\\mathbf{\\Gamma}\\mathbf{\\Gamma}+\\kappa^{\\prime})^{-1}\\right)-\\mathrm{Tr}\\left(\\mathbf{\\Gamma}(\\mathbf{\\Gamma}+\\kappa)^{-1}\\right)\\right|}\\\\ &{\\qquad\\qquad\\qquad\\qquad=|\\kappa-\\kappa^{\\prime}|\\mathrm{Tr}\\left(\\mathbf{\\Gamma}(\\mathbf{\\Gamma}(\\mathbf{\\Gamma}+\\kappa^{\\prime})^{-1}(\\mathbf{\\Gamma}+\\kappa)^{-1}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\frac{|\\kappa^{\\prime}-\\kappa|}{\\kappa^{\\prime}}\\mathrm{Tr}\\left(\\mathbf{\\Gamma}(\\mathbf{\\Gamma}(\\mathbf{\\Gamma}+\\kappa)^{-1}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq C_{\\ast,D,K}\\cdot\\mathcal{E}_{\\nu}(p)\\cdot p\\widetilde{\\Phi}_{2}(F;p\\nu_{1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Similarly, by simple algebra, we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\Big|\\widetilde{\\Phi}_{6}(F;A,p\\widetilde{\\nu}_{1})-\\widetilde{\\Phi}_{6}(F;A,p\\nu_{1})\\Big|}\\\\ &{=\\left|\\mathrm{Tr}\\big(A\\mathbf{F}^{2}(\\mathbf{F}+\\kappa^{\\prime})^{-2}\\big)-\\mathrm{Tr}\\big(A\\mathbf{F}^{2}(\\mathbf{F}+\\kappa)^{-2}\\big)\\right|}\\\\ &{\\le2|\\kappa-\\kappa^{\\prime}|\\mathrm{Tr}\\big(A\\mathbf{F}^{3}(\\mathbf{F}+\\kappa^{\\prime})^{-2}(\\mathbf{F}+\\kappa)^{-2}\\big)+|\\kappa^{2}-(\\kappa^{\\prime})^{2}|\\mathrm{Tr}\\big(A\\mathbf{F}^{3}(\\mathbf{F}+\\kappa^{\\prime})^{-2}(\\mathbf{F}+\\kappa)^{-2}\\big)}\\\\ &{\\le\\frac{|\\kappa-\\kappa^{\\prime}|}{\\kappa^{\\prime}}\\left\\{2+\\frac{|\\kappa+\\kappa^{\\prime}|}{\\kappa^{\\prime}}\\right\\}\\mathrm{Tr}\\big(A\\mathbf{F}^{2}(\\mathbf{F}+\\kappa)^{-2}\\big)}\\\\ &{\\le C_{*,D,K}\\cdot\\mathcal{E}_{\\nu}(p)\\cdot\\widetilde{\\Phi}_{6}(F;A,p\\nu_{1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Furthermore, note that by the identity (77), ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\widetilde{\\Phi}_{6}(F;\\mathbf{I},p\\widetilde{\\nu}_{1})\\left(n-\\widetilde{\\Phi}_{6}(F;\\mathbf{I},p\\widetilde{\\nu}_{1})\\right)^{-1}\\leq\\widetilde{\\Phi}_{2}(F;p\\widetilde{\\nu}_{1})\\left(n-\\widetilde{\\Phi}_{2}(F;p\\widetilde{\\nu}_{1})\\right)^{-1}=\\frac{\\widetilde{\\nu}_{1}}{\\lambda}\\widetilde{\\Phi}_{2}(F;p\\widetilde{\\nu}_{1}).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Furthermore, we have from the previous computation and simple algebra that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\frac{\\tilde{\\nu}_{1}}{\\lambda}\\widetilde{\\Phi}_{2}(F;p\\tilde{\\nu}_{1})\\leq(1+C_{*,D,K}\\mathscr{E}_{\\nu}(p))\\cdot\\frac{\\nu_{1}}{\\lambda}\\mathrm{Tr}(\\Sigma(\\Sigma+\\nu_{2})^{-1})\\leq C_{*,D,K}\\cdot\\widetilde{\\rho}_{\\lambda}(n,p),\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where we used the same argument as in the proof of Lemma B.17 in the last inequality as well as condition (85). The proof of Eqs. (142) and (141) from simple algebra from the above displays and (143). \u53e3 ", "page_idx": 40}, {"type": "text", "text": "B.7.4 Proof of Lemma B.11 ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Proof of Lemma B.11. For convenience, we introduce the notations ", "page_idx": 40}, {"type": "equation", "text": "$$\nG_{F}:=(F F^{\\mathsf{T}}+p\\nu_{1})^{-1},\\qquad G_{0}:=(F_{0}F_{0}^{\\mathsf{T}}+\\gamma_{+})^{-1},\\qquad R_{0}:=(F_{0}^{\\mathsf{T}}F_{0}+\\gamma_{+})^{-1},\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where we recall that we denoted $\\gamma_{+}=\\gamma(p\\nu_{1})=p\\nu_{1}\\!+\\!\\mathrm{Tr}(\\Sigma_{+})$ . Recall that for the first approximation guarantee, we denote ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\widetilde{\\Phi}_{2}(\\boldsymbol{F};p\\nu_{1})=\\frac{1}{p}\\mathrm{Tr}(\\boldsymbol{F}\\boldsymbol{F}^{\\intercal}\\boldsymbol{G}_{F}),\\qquad\\qquad\\widetilde{\\Phi}_{2}(\\boldsymbol{F}_{0};p\\nu_{1})=\\frac{1}{p}\\mathrm{Tr}(\\boldsymbol{F}_{0}\\boldsymbol{F}_{0}^{\\intercal}\\boldsymbol{G}_{0}),\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "and the deterministic equivalents ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\Psi_{2}(\\nu_{2})=\\frac{1}{p}\\mathrm{Tr}(\\Sigma(\\Sigma+\\nu_{2})^{-1}),~~~~~~~~~\\Psi_{2}(\\nu_{2,0})=\\frac{1}{p}\\mathrm{Tr}(\\Sigma_{0}(\\Sigma_{0}+\\nu_{2,0})^{-1}).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "For the second approximation guarantee, recall that we denote ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\widetilde{\\Phi}_{4}(\\boldsymbol{F};\\boldsymbol{\\Sigma}^{-1},p\\nu_{1})=\\frac{1}{p}\\mathrm{Tr}(\\boldsymbol{F}\\boldsymbol{F}^{\\intercal}\\boldsymbol{G}_{F}^{2}),\\qquad\\qquad\\widetilde{\\Phi}_{4}(\\boldsymbol{F};\\boldsymbol{\\Sigma}_{0}^{-1},p\\nu_{1})=\\frac{1}{p}\\mathrm{Tr}(\\boldsymbol{F}_{0}\\boldsymbol{F}_{0}^{\\intercal}\\boldsymbol{G}_{0}^{2}),\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "and their associated deterministic equivalents ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathbb{I}_{3}\\big(\\nu_{2};\\,\\mathbf{\\DeltaE}^{-1}\\big)=\\frac{1}{p}\\cdot\\frac{\\mathrm{Tr}(\\mathbf{\\DeltaE}(\\mathbf{\\Sigma}\\mathbf{}\\mathbf{\\Sigma}+\\nu_{2})^{-2})}{p-\\mathrm{Tr}(\\mathbf{\\Sigma}\\mathbf{\\Sigma}^{2}(\\mathbf{\\Sigma}\\mathbf{}^{2}+\\nu_{2})^{-2})},\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\Psi_{3}(\\nu_{2,0};\\,\\mathbf{\\DeltaE}_{0}^{-1})=\\frac{1}{p}\\cdot\\frac{\\mathrm{Tr}(\\Sigma_{0}(\\Sigma_{0}+\\nu_{2,0})^{-2})}{p-\\mathrm{Tr}(\\Sigma_{0}^{2}(\\Sigma_{0}^{2}+\\nu_{2,0})^{-2})}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "We separate the analysis of the low-degree part $\\pmb{F}_{0}$ from the high-degree part $\\pmb{F}_{+}$ . To remove the dependency on the high-degree part $\\pmb{F}_{+}$ , we recall that by Lemma B.3, we have with probability at least $1-\\dot{p^{-}}^{D}$ ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\|\\Delta\\|_{\\mathrm{op}}\\leq C_{*,D}\\frac{\\log^{3}(p)}{\\sqrt{p}}\\gamma_{+},\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where we denoted $\\Delta:=F_{+}F_{+}-\\operatorname{Tr}(\\Sigma_{+})\\cdot{\\bf I}_{p}$ and we recall that $\\gamma_{+}=\\gamma(p\\nu_{1})=p\\nu_{1}+\\mathrm{Tr}(\\Sigma_{+})$ . In particular, taking $p\\geq C_{*,D}$ , we have $\\begin{array}{r}{\\|\\Delta\\|_{\\mathrm{op}}\\leq\\frac{1}{2}\\gamma_{+}}\\end{array}$ and therefore ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\|G_{F}\\|_{\\mathrm{op}}\\leq2\\|G_{0}\\|_{\\mathrm{op}}\\leq\\frac{2}{\\gamma_{+}}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Step 1: Bound on $|\\widetilde{\\Phi}_{2}(F;p\\nu_{1})-\\Psi_{2}(\\nu_{2})|$ . ", "page_idx": 41}, {"type": "text", "text": "First note that we have the identity ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\widetilde{\\Phi}_{2}(F;p\\nu_{1})=1-\\nu_{1}\\mathrm{Tr}(G_{F})=1-\\nu_{1}\\mathrm{Tr}(G_{0})+\\nu_{1}\\Theta,\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where we denoted $\\Theta=\\mathrm{Tr}({\\cal G}_{0})-\\mathrm{Tr}({\\cal G}_{F})$ . By Eqs. (145) and (146), we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n|\\Theta|=|\\mathrm{Tr}(G_{F}\\Delta G_{0})|\\leq C_{*,D}\\frac{\\log^{3}(p)}{\\sqrt{p}}\\cdot\\mathrm{Tr}(G_{0}).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Using again the above identity, we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\frac{1}{p}\\mathrm{Tr}({\\cal G}_{0})=\\frac{1}{\\gamma_{+}}-\\frac{1}{\\gamma_{+}}\\widetilde{\\Phi}_{2}({\\cal F}_{0};p\\nu_{1}).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Under the assumption of the lemma, we can apply Proposition B.8 and obtain with probability at least $1-p^{-D}$ that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\widetilde{\\Phi}_{2}(\\pmb{F}_{0};p\\nu_{1})-\\Psi_{2}(\\nu_{2,0})\\right|\\leq C_{*,D,K}\\cdot\\mathcal{E}_{3}(p)\\cdot\\Psi_{2}(\\nu_{2,0}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where $\\mathcal{E}_{3}(p)$ is defined in Eq. (100). Furthermore note that using identity (80), we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{Tr}({\\Sigma}_{+})}{\\gamma_{+}}+\\frac{p\\nu_{1}}{\\gamma_{+}}\\Psi_{2}(\\nu_{2,0})=\\Psi_{2}(\\nu_{2,0})+\\frac{\\mathrm{Tr}({\\Sigma}_{+})}{p\\nu_{2,0}},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "and that by Eq. (82) in Lemma B.5, ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\bigg|\\Psi_{2}(\\nu_{2,0})+\\frac{\\mathrm{Tr}(\\Sigma_{+})}{p\\nu_{2,0}}-\\Psi_{2}(\\nu_{2})\\bigg|\\leq\\frac{C}{p}\\Psi_{2}(\\nu_{2}).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Thus combining Eqs. (147), (149) and (150), we obtain ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widetilde{\\Phi}_{2}(F;p\\nu_{1})-\\Psi_{2}(\\nu_{2})|\\leq\\nu_{1}|\\Theta|+\\frac{p\\nu_{1}}{\\gamma_{+}}\\left|\\widetilde{\\Phi}_{2}(F_{0};p\\nu_{1})-\\Psi_{2}(\\nu_{2,0})\\right|+\\bigg|\\Psi_{2}(\\nu_{2,0})+\\frac{\\mathrm{Tr}(\\Sigma_{+})}{p\\nu_{2,0}}-\\Psi_{2}(\\nu_{2})}\\\\ {\\leq C_{\\ast,D,K}\\left\\{\\displaystyle\\frac{\\log^{3}(p)}{\\sqrt{p}}+\\mathcal{E}_{3}(p)+\\frac{1}{p}\\right\\}\\left[\\nu_{1}\\mathrm{Tr}(G_{0})+\\frac{p\\nu_{1}}{\\gamma_{+}}\\Psi_{2}(\\nu_{2,0})+\\Psi_{2}(\\nu_{2})\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "with probability at least $1-p^{-D}$ via union bound. Using condition (106), we can simplify the right-hand side using identity (148) and bounds (149) and (150) to get ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\kappa_{1}\\mathrm{Tr}(G_{0})\\leq\\displaystyle\\frac{p\\nu_{1}}{\\gamma_{+}}\\left|1-\\Psi_{2}(\\nu_{2,0})\\right|+C_{*,D,K}K\\Psi_{2}(\\nu_{2,0})}&{}\\\\ {\\leq C_{*,D,K}\\left\\{\\Psi_{2}(\\nu_{2,0})+\\displaystyle\\frac{\\mathrm{Tr}(\\Sigma_{+})}{p\\nu_{2,0}}\\right\\}\\leq C_{*,D,K}\\cdot\\Psi_{2}(\\nu_{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "This concludes the proof of the first part of this lemma. ", "page_idx": 41}, {"type": "text", "text": "Step 2: Bound on $|\\widetilde{\\Phi}_{4}({\\pmb F};{\\pmb\\Sigma}^{-1},p\\nu_{1})-\\Psi_{3}(\\nu_{2};{\\pmb\\Sigma}^{-1})|.$ ", "page_idx": 41}, {"type": "text", "text": "We proceed similarly as in the first part and omit some repetitive details. First note that we can rewrite ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{\\Phi}_{4}(F;\\Sigma^{-1},p\\nu_{1})=\\displaystyle\\frac{1}{p}\\mathrm{Tr}(G_{F})-\\nu_{1}\\mathrm{Tr}(G_{F}^{2})}\\\\ &{\\qquad\\qquad\\qquad=\\displaystyle\\frac{1}{p}\\mathrm{Tr}(G_{0})-\\nu_{1}\\mathrm{Tr}(G_{0}^{2})+\\Theta}\\\\ &{\\qquad\\qquad\\qquad=\\displaystyle\\frac{\\mathrm{Tr}(\\Sigma_{+})}{\\gamma_{+}^{2}}\\left(1-\\widetilde{\\Phi}_{2}(F_{0};p\\nu_{1})\\right)+\\frac{p\\nu_{1}}{\\gamma_{+}}\\widetilde{\\Phi}_{4}(F_{0};\\Sigma_{0}^{-1},p\\nu_{1})+\\Theta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\Theta|=\\displaystyle\\frac{1}{p}\\left|\\mathrm{Tr}(G_{F})-\\mathrm{Tr}(G_{0})+p\\nu_{1}\\mathrm{Tr}(G_{0}^{2})-p\\nu_{1}\\mathrm{Tr}(G_{F}^{2})\\right|}\\\\ &{\\quad\\leq C_{*,D}\\displaystyle\\frac{\\log^{3}(p)}{\\sqrt{p}}\\cdot\\left[\\frac{1}{p}\\mathrm{Tr}(G_{0})+\\nu_{1}\\mathrm{Tr}(G_{0}^{2})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Again, by Proposition B.8, we get that with probability at least $1-p^{-D}$ that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\qquad\\Big|\\widetilde{\\Phi}_{2}(F_{0};p\\nu_{1})-\\Psi_{2}(\\nu_{2,0})\\Big|\\leq C_{*,D,K}\\cdot\\mathcal{E}_{3}(p)\\cdot\\Psi_{2}(\\nu_{2,0}),}\\\\ &{\\qquad\\qquad\\quad\\Big|\\widetilde{\\Phi}_{4}(F_{0};\\Sigma_{0}^{-1},p\\nu_{1})-\\Psi_{3}(\\nu_{2,0};\\Sigma_{0}^{-1})\\Big|\\leq C_{*,D,K}\\cdot\\mathcal{E}_{3}(p)\\cdot\\Psi_{3}(\\nu_{2,0};\\Sigma_{0}^{-1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Furthermore, note that by Lemma B.19 stated below, we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\left|\\frac{\\mathrm{Tr}(\\Sigma_{+})}{\\gamma_{+}^{2}}\\left(1-\\Psi_{2}(\\nu_{2,0})\\right)+\\frac{p\\nu_{1}}{\\gamma_{+}}\\Psi_{3}(\\nu_{2,0};\\Sigma_{0}^{-1})-\\Psi_{3}(\\nu_{2};\\Sigma^{-1})\\right|\\leq C\\frac{\\rho_{\\gamma_{+}}(p)}{p}\\Psi_{3}(\\nu_{2};\\Sigma^{-1}).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Combining Eqs. (151), (152) and (153), we deduce via union bound that with probability at least 1 \u2212p\u2212D ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ |\\widetilde{\\Phi}_{4}(F;\\Sigma^{-1},p\\nu_{1})-\\Psi_{3}(\\nu_{2};\\Sigma^{-1})|}\\\\ &{\\le C_{*,D,K}\\cdot\\mathcal{E}_{3}(p)\\left[\\frac{1}{p}\\mathrm{Tr}(G_{0})+\\nu_{1}\\mathrm{Tr}(G_{0}^{2})+\\frac{\\mathrm{Tr}(\\Sigma_{+})^{2}}{\\gamma_{+}^{2}}\\Psi_{2}(\\nu_{2,0})+\\frac{p\\nu_{1}}{\\gamma_{+}}\\Psi_{3}(\\nu_{2,0};\\Sigma_{0}^{-1})+\\Psi_{3}(\\nu_{2};\\Sigma^{-1}(\\nu_{2,0})+\\frac{1}{\\gamma_{+}}\\Psi_{2}(\\nu_{2,0}))\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Let us simplify the right hand side. First, from the proof of Lemma B.19, we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{Tr}(\\Sigma_{+})^{2}}{\\gamma_{+}^{2}}\\Psi_{2}(\\nu_{2,0})+\\frac{p\\nu_{1}}{\\gamma_{+}}\\Psi_{3}(\\nu_{2,0};\\Sigma_{0}^{-1})\\leq C\\rho_{\\gamma_{+}}(p)\\cdot\\Psi_{3}(\\nu_{2};\\Sigma^{-1}).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Combining the above two displays with $\\mathcal{E}_{3}(p)\\leq K$ from conditions (106) yields ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\frac{1}{p}\\mathrm{Tr}(G_{0})-\\nu_{1}\\mathrm{Tr}(G_{0}^{2})\\leq C_{*,D,K}\\cdot\\rho_{\\gamma_{+}}(p)\\cdot\\Psi_{3}(\\nu_{2};\\Sigma^{-1}).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Finally, note that using Eq. (152) and again $\\mathcal{E}_{3}(p)\\leq K$ that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\nu_{1}\\mathrm{Tr}(G_{0}^{2})\\leq\\frac{p\\nu_{1}}{\\gamma_{+}}\\widetilde{\\Phi}_{4}(F;\\Sigma_{0}^{-1/2},p\\nu_{1})\\leq C_{*,D,K}\\Psi_{3}(\\nu_{2};\\Sigma^{-1}),\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "which concludes the proof of this lemma. ", "page_idx": 42}, {"type": "text", "text": "Lemma B.19. Assuming that $p^{2}\\xi_{\\mathfrak{m}}^{2}\\leq\\gamma_{+}$ , we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\left|\\frac{\\mathrm{Tr}(\\Sigma_{+})}{\\gamma_{+}^{2}}\\left(1-\\Psi_{2}(\\nu_{2,0})\\right)+\\frac{p\\nu_{1}}{\\gamma_{+}}\\Psi_{3}(\\nu_{2,0};\\Sigma_{0}^{-1})-\\Psi_{3}(\\nu_{2};\\Sigma^{-1})\\right|\\leq C\\frac{\\rho_{\\gamma_{+}}(p)}{p}\\Psi_{3}(\\nu_{2};\\Sigma^{-1}).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Proof of Lemma B.19. For convenience, we introduce ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\Upsilon(\\nu_{2,0}):=\\frac{1}{p}\\mathrm{Tr}(\\Sigma_{0}^{2}(\\Sigma_{0}+\\nu_{2,0})^{-2}),\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\Upsilon(\\nu_{2}):=\\frac{1}{p}\\mathrm{Tr}(\\Sigma^{2}(\\Sigma+\\nu_{2})^{-2}).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Using that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\frac{1}{p}\\mathrm{Tr}(\\Sigma_{0}(\\Sigma_{0}+\\nu_{2,0})^{-2})=\\frac{1}{\\nu_{2,0}}\\left\\{\\Psi_{2}(\\nu_{2,0})-\\Upsilon(\\nu_{2,0})\\right\\},\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "we obtain by simple algebra and using identity (80) that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{Tr}(\\Sigma_{+})}{\\gamma_{+}^{2}}\\left(1-\\Psi_{2}(\\nu_{2,0})\\right)+\\frac{p\\nu_{1}}{\\gamma_{+}}\\Psi_{3}(\\nu_{2,0};\\Sigma_{0}^{-1})=\\frac{1}{\\nu_{2,0}}\\frac{\\Psi_{2}(\\nu_{2,0})+\\frac{\\mathrm{Tr}(\\Sigma_{+})}{p\\nu_{2,0}}-\\Upsilon(\\nu_{2,0})}{p(1-\\Upsilon(\\nu_{2,0}))}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Following the proof of [Misiakiewicz and Saeed, 2024, Lemma 7], we get that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\Upsilon(\\nu_{2,0})-\\Upsilon(\\nu_{2})|\\leq10\\frac{p\\xi_{\\mathfrak{m}}^{2}}{\\gamma_{+}}\\leq\\frac{C}{p},}\\\\ {(1-\\Upsilon(\\nu_{2,0}))^{-1}\\leq(1-\\Psi_{2}(\\nu_{2,0}))^{-1}=\\frac{p\\nu_{2,0}}{\\gamma_{+}}\\leq C\\rho_{\\gamma_{+}}(p).}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Recalling Eq. (82) in Lemma B.5, we can conclude the proof using simple algebraic manipulations similarly to [Misiakiewicz and Saeed, 2024, Lemma 7]. \u53e3 ", "page_idx": 42}, {"type": "text", "text": "B.7.5 Proof of Lemma B.13 ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Proof of Lemma B.13. We will follow similar steps as in the proof of Lemma B.11. For the sake of brevity, we omit some repetitive details. Recall that we introduced the notations (144). We decompose the coefficient vector $\\bar{\\beta_{*}}=[\\beta_{0},\\beta_{+}]$ with $\\beta_{0}\\in\\mathbb{R}^{{\\mathfrak{m}}}$ the first m coordinates of $\\beta_{\\ast}$ (aligned with the top m eigenspaces), while $\\beta_{+}\\in\\mathbb{R}^{\\infty}$ corresponds to the rest of the coordinates. Further introduce the matrices ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A_{*}:=\\Sigma^{-1/2}\\beta_{*}\\beta_{*}^{\\top}\\Sigma^{-1/2},\\qquad\\qquad A_{0}:=\\Sigma_{0}^{-1/2}\\beta_{0}\\beta_{0}^{\\top}\\Sigma_{0}^{-1/2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "and recall the expressions of the functionals ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\widetilde{\\Phi}_{1}(F;A_{*},p\\nu_{1})=\\mathrm{Tr}\\big(A_{*}\\Sigma^{1/2}R\\Sigma^{1/2}\\big),\\qquad}&{\\widetilde{\\Phi}_{1}(F_{0};A_{0},p\\nu_{1})=\\mathrm{Tr}\\big(A_{0}\\Sigma_{0}^{1/2}R_{0}\\Sigma_{0}^{1/2}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "as well as ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{\\Phi}_{4}(\\boldsymbol{F};\\boldsymbol{A}_{*},p\\nu_{1})=\\frac{1}{p}\\mathrm{Tr}\\big(\\boldsymbol{A}_{*}\\boldsymbol{\\Sigma}^{1/2}\\boldsymbol{R}\\boldsymbol{F}^{\\intercal}\\boldsymbol{F}\\boldsymbol{R}\\boldsymbol{\\Sigma}^{1/2}\\big),}\\\\ &{\\widetilde{\\Phi}_{4}(\\boldsymbol{F}_{0};\\boldsymbol{A}_{0},p\\nu_{1})=\\frac{1}{p}\\mathrm{Tr}\\big(\\boldsymbol{A}_{0}\\boldsymbol{\\Sigma}_{0}^{1/2}\\boldsymbol{R}_{0}\\boldsymbol{F}_{0}^{\\intercal}\\boldsymbol{F}_{0}\\boldsymbol{R}_{0}\\boldsymbol{\\Sigma}_{0}^{1/2}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "We further recall the expressions of the deterministic equivalents ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\Psi_{1}(\\nu_{2};A_{*})=\\mathrm{Tr}\\big(A_{*}\\Sigma(\\Sigma+\\nu_{2})^{-1}\\big),\\qquad\\quad\\Psi_{1}(\\nu_{2,0};A_{0})=\\mathrm{Tr}\\big(A_{0}\\Sigma_{0}(\\Sigma_{0}+\\nu_{2,0})^{-1}\\big),\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "and ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\psi_{3}(\\nu_{2};A_{*})=\\frac{1}{p}\\cdot\\frac{\\mathrm{Tr}(A_{*}\\Sigma^{2}(\\Sigma+\\nu_{2})^{-2})}{p-\\mathrm{Tr}(\\Sigma^{2}(\\Sigma+\\nu_{2})^{-2})},\\quad}\\\\ {\\psi_{3}(\\nu_{2,0};A_{0})=\\frac{1}{p}\\cdot\\frac{\\mathrm{Tr}(A_{0}\\Sigma_{0}^{2}(\\Sigma_{0}+\\nu_{2,0})^{-2})}{p-\\mathrm{Tr}(\\Sigma_{0}^{2}(\\Sigma_{0}+\\nu_{2,0})^{-2})}.\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "We first rewrite our functionals into ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(p\\nu_{1})^{2}\\langle\\beta_{*},R^{2}\\beta_{*}\\rangle=(p\\nu_{1})\\langle\\beta_{*},R\\beta_{*}\\rangle-(p\\nu_{1})\\langle\\beta_{*},R F^{\\top}F R\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad=(p\\nu_{1})\\left[\\widetilde{\\Phi}_{1}(F;A_{*},p\\nu_{1})-p\\widetilde{\\Phi}_{4}(F;A_{*},p\\nu_{1})\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "and study each term separately. ", "page_idx": 43}, {"type": "text", "text": "Step 1: Bounding term $\\widetilde{\\Phi}_{1}(\\boldsymbol{F};A_{*},p\\nu_{1}).$ . ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Let us start by removing the dependency on the high-degree part $F_{+}$ in the denominator. Note that we can rewrite the matrix $\\boldsymbol{R}$ in block matrix form ", "page_idx": 43}, {"type": "equation", "text": "$$\n(p\\nu_{1})R=(p\\nu_{1})\\left(\\begin{array}{c c}{{F_{0}^{\\top}F_{0}+p\\nu_{1}}}&{{F_{0}^{\\top}F_{+}}}\\\\ {{F_{+}^{\\top}F_{0}}}&{{F_{+}^{\\top}F_{+}+p\\nu_{1}}}\\end{array}\\right)^{-1}=:\\left(\\boldsymbol{\\widetilde{R}}_{00}\\quad\\boldsymbol{\\widetilde{R}}_{0+}\\right),\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "so that ", "page_idx": 43}, {"type": "equation", "text": "$$\n(p\\nu_{1})\\widetilde{\\Phi}_{1}(F;A_{*},p\\nu_{1})=\\beta_{0}^{\\top}\\widetilde{\\pmb{R}}_{00}\\beta_{0}+2\\beta_{0}\\widetilde{\\pmb{R}}_{0+}\\beta_{+}+\\beta_{+}^{\\top}\\widetilde{\\pmb{R}}_{++}\\beta_{+}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Let us study each of these terms separately. Denote $\\widetilde{G}_{+}:=F_{+}F_{+}^{\\mathsf{T}}+p\\nu_{1}$ . By simple algebra, we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\beta_{0}^{\\top}\\widetilde{R}_{00}\\beta_{0}=\\beta_{0}^{\\top}\\left(F_{0}^{\\top}\\widetilde{G}_{+}F_{0}+1\\right)^{-1}\\beta_{0}=\\gamma_{+}\\widetilde{\\Phi}_{1}(F_{0};A_{0},p\\nu_{1})+\\Theta_{00},}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\Theta_{00}|=\\left.\\left|\\gamma_{+}\\beta_{0}^{\\mathsf{T}}R_{0}\\beta_{0}-\\beta_{0}^{\\mathsf{T}}\\left(F_{0}^{\\mathsf{T}}\\widetilde{G}_{+}F_{0}+1\\right)^{-1}\\beta_{0}\\right|}\\\\ &{\\qquad\\leq C\\left\\|R_{0}^{1/2}F_{0}^{\\mathsf{T}}(\\mathbf{I}-\\gamma_{+}\\widetilde{G}_{+})F_{0}R_{0}^{1/2}\\right\\|_{\\mathrm{op}}\\cdot\\gamma_{+}\\beta_{0}^{\\mathsf{T}}R_{0}\\beta_{0}}\\\\ &{\\qquad\\leq C_{*,D}\\frac{\\log^{3}(p)}{\\sqrt{p}}\\gamma_{+}\\widetilde{\\Phi}_{1}(F_{0};A_{0},p\\nu_{1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Furthermore, from Proposition B.8, we have with probability at least $1-p^{-D}$ that ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\gamma_{+}\\widetilde{\\Phi}_{1}(F_{0};A_{0},p\\nu_{1})-\\nu_{2,0}\\Psi_{1}(\\nu_{2,0};A_{0})\\right|\\leq C_{*,D,K}\\cdot\\mathcal{E}_{3}(p)\\cdot\\nu_{2,0}\\Psi_{1}(\\nu_{2,0};A_{0}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Hence, combining the previous two displays and using that $\\mathcal{E}_{3}(p)\\leq K$ by conditions (114), we have ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\left\\vert\\beta_{0}^{\\mathsf{T}}\\widetilde{R}_{00}\\beta_{0}-\\nu_{2,0}\\Psi_{1}(\\nu_{2,0};A_{0})\\right\\vert\\leq C_{\\ast,D,K}\\cdot\\left\\{\\frac{\\log^{3}(p)}{\\sqrt{p}}+\\mathcal{E}_{3}(p)\\right\\}\\nu_{2,0}\\Psi_{1}(\\nu_{2,0};A_{0}).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Similarly, denoting $\\widetilde{\\mathbfcal{G}}_{0}=(\\ensuremath{\\boldsymbol{F}}_{0}\\ensuremath{\\boldsymbol{F}}_{0}^{\\sf T}+p\\ensuremath{\\boldsymbol{\\nu}}_{1})^{-1}$ , we can rewrite the third term as ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\beta_{+}^{\\top}\\widetilde{R}_{++}\\beta_{+}=\\beta_{+}^{\\top}\\left(F_{+}^{\\top}\\widetilde{G}_{0}F_{+}+1\\right)^{-1}\\beta_{+}=\\|\\beta_{+}\\|_{2}^{2}-\\Theta_{++},\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where with probability at least $1-p^{-D}$ , ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Theta_{++}=\\beta_{+}^{\\uparrow}F_{+}^{\\top}\\widetilde{G}_{0}^{1/2}\\left(\\widetilde{G}_{0}^{1/2}F_{+}F_{+}^{\\top}\\widetilde{G}_{0}^{1/2}+1\\right)^{-1}\\widetilde{G}_{0}^{1/2}F_{+}\\beta_{+}}\\\\ &{\\qquad\\leq\\|(F_{+}F_{+}^{\\top}+\\widetilde{G}_{0}^{-1})^{-1}\\|_{\\mathrm{op}}\\cdot\\|F_{+}\\beta_{+}\\|_{2}^{2}}\\\\ &{\\qquad\\leq\\cfrac{C}{\\gamma_{+}}\\cdot p\\|\\Sigma_{+}^{1/2}\\beta_{+}\\|_{2}^{2}\\leq\\cfrac{C}{p}\\|\\beta_{+}\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where we used the same concentration argument as in Step 3 of the proof of Lemma B.2. Finally, we rewrite ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\beta_{0}^{\\mathsf{T}}\\widetilde{R}_{0+}\\beta_{+}=-\\beta_{0}^{\\mathsf{T}}\\left(F_{0}^{\\mathsf{T}}\\widetilde{G}_{+}F_{0}+1\\right)^{-1}F_{0}^{\\mathsf{T}}(F_{+}F_{+}^{\\mathsf{T}}+p\\nu_{1})^{-1}F_{+}\\beta_{+}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Hence, using Eqs. (154) and (155) as well as conditions (114), we obtain ", "page_idx": 44}, {"type": "equation", "text": "$$\n|\\beta_{0}^{\\mathsf{T}}\\widetilde{R}_{0+}\\beta_{+}|\\leq C_{*,D,K}\\cdot\\frac{1}{\\sqrt{p}}\\left\\{\\nu_{2,0}\\Psi_{1}(\\nu_{2,0};A_{0})+\\|\\beta_{+}\\|_{2}^{2}\\right\\}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Finally, note that by Lemma B.20 stated below, we have ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\Big|\\nu_{2,0}\\langle\\beta_{0},(\\Sigma_{0}+\\nu_{2,0})^{-1}\\beta_{0}\\rangle+\\|\\beta_{+}\\|_{2}^{2}-\\nu_{2}\\Psi_{1}(\\nu_{2};A_{*})\\Big|\\leq\\frac{C}{p}\\nu_{2}\\Psi_{1}(\\nu_{2};A_{*}).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Combining Eqs. (154), (155), (156), and (157), we deduce that with probability at least $1-p^{-D}$ , ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\left|(p\\nu_{1})\\widetilde{\\Phi}_{1}(F;A_{*},p\\nu_{1})-\\nu_{2}\\Psi_{1}(\\nu_{2};A_{*})\\right|\\leq C_{*,D,K}\\cdot\\left\\{\\frac{\\log^{3}(p)}{\\sqrt{p}}+\\mathcal{E}_{3}(p)\\right\\}\\cdot\\nu_{2}\\Psi_{1}(\\nu_{2};A_{*}).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Step 2: Bounding term $\\widetilde{\\Phi}_{4}(F;A_{*},p\\nu_{1})$ . ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "We rewrite this term as ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\langle\\beta_{*},R F^{\\top}F R\\beta_{*}\\rangle=\\langle\\beta_{*},F^{\\top}G_{0}^{2}F\\beta_{*}\\rangle+\\Theta,\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\Theta|=\\left|\\langle\\beta_{*},F^{\\top}(G_{F}^{2}-G_{0}^{2})F\\beta_{*}\\rangle\\right|}\\\\ &{\\quad\\quad=\\left|\\langle\\beta_{*},F^{\\top}G_{0}\\left(-2\\Delta G_{F}+\\Delta G_{F}^{2}\\Delta\\right)G_{0}F\\beta_{*}\\rangle\\right|\\leq C_{*,D,K}\\frac{\\log^{3}(d)}{p}\\langle\\beta_{*},F^{\\top}G_{0}^{2}F\\beta_{*}\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Let us decompose ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\langle\\beta_{*},F^{\\top}G_{0}^{2}F\\beta_{*}\\rangle=\\langle\\beta_{0},F_{0}^{\\top}G_{0}^{2}F_{0}\\beta_{0}\\rangle+2\\langle\\beta_{+},F_{+}^{\\top}G_{0}^{2}F_{0}\\beta_{0}\\rangle+\\langle\\beta_{+},F_{+}^{\\top}G_{0}^{2}F_{+}\\beta_{+}\\rangle.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "For the first term, we have directly from Proposition B.8 that ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\langle\\beta_{0},F_{0}^{\\mathsf{T}}G_{0}^{2}F_{0}\\beta_{0}\\rangle-p\\Psi_{3}(\\nu_{2,0};A_{0})\\right|\\leq C_{*,D,K}\\cdot\\mathcal{E}_{3}(p)\\cdot p\\Psi_{3}(\\nu_{2,0};A_{0}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "For the two other terms, notice that they correspond to the terms (101) and (102) in Proposition B.9 with $\\pmb{v}=\\beta_{0}$ and ${\\pmb u}={\\pmb F}_{+}{\\beta}_{+}$ , where ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[f_{0,j}\\langle f_{+,j},\\beta_{+}\\rangle]=0,\\qquad\\quad\\mathbb{E}[u_{i}^{2}]=\\lVert\\boldsymbol{\\Sigma}_{+}^{1/2}\\boldsymbol{\\beta}_{*}\\rVert_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Hence, by Proposition B.9, we have with probability at least $1-p^{-D}$ that ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\left|\\langle\\beta_{+},F_{+}^{\\top}G_{0}^{2}F_{+}\\beta_{+}\\rangle-\\frac{1}{\\nu_{2,0}^{2}}\\frac{\\|\\Sigma_{+}^{1/2}\\beta_{+}\\|_{2}^{2}}{p-\\,\\mathrm{Tr}\\left(\\Sigma_{0}^{2}(\\Sigma_{0}+\\nu_{2,0})^{-2}\\right)}\\right|\\leq C_{*,D,K}\\cdot\\mathcal{E}_{4}(p)\\cdot\\frac{p\\|\\Sigma_{+}^{1/2}\\beta_{+}\\|_{2}^{2}}{\\gamma_{+}^{2}},}&\\\\ &{\\left|\\langle\\beta_{+},F_{+}^{\\top}G_{0}^{2}F_{0}\\beta_{0}\\rangle\\right|\\leq C_{*,D,K}\\cdot\\mathcal{E}_{4}(p)\\cdot\\|\\Sigma_{+}^{1/2}\\beta_{+}\\|_{2}\\frac{p^{2}\\nu_{2,0}}{\\gamma_{+}^{2}}\\sqrt{\\Psi_{3}(\\nu_{2,0};A_{0})}.}&{\\ \\ \\ \\ \\ \\ \\ }\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Furthermore, by Lemma B.20 stated below ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\left|\\frac{\\langle\\beta_{0},\\Sigma_{0}(\\Sigma_{0}+\\nu_{2,0})^{-2}\\beta_{0}\\rangle+\\langle\\beta_{+},\\Sigma_{+}\\beta_{+}\\rangle/\\nu_{2,0}^{2}}{p-\\mathrm{Tr}(\\Sigma_{0}^{2}(\\Sigma_{0}+\\nu_{2,0})^{-2})}-p\\Psi_{3}(\\nu_{2};A_{*})\\right|\\leq C\\frac{\\rho_{\\gamma_{+}}(p)}{p}\\cdot p\\Psi_{3}(\\nu_{2};A_{*}).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Combining Eqs. (154), (155), (156), and (157), we deduce that with probability at least $1-p^{-D}$ ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|p\\widetilde{\\Phi}_{4}(F;A_{*},p\\nu_{1})-p\\Psi_{3}(\\nu_{2};A_{*})\\right|\\leq C_{*,D,K}\\cdot\\rho_{\\gamma_{+}}(p)^{2}\\mathcal{E}_{4}(p)\\cdot p\\Psi_{3}(\\nu_{2};A_{*}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where we used that ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\frac{p\\nu_{2,0}}{\\gamma_{+}}\\sqrt{\\frac{p\\|\\boldsymbol{\\Sigma}_{+}^{1/2}\\beta_{*}\\|_{2}^{2}}{\\gamma_{+}^{2}}p\\Psi_{3}(\\nu_{2,0};A_{0})}\\leq C\\rho_{\\gamma_{+}}(p)\\left\\{\\frac{\\|\\boldsymbol{\\Sigma}_{+}^{1/2}\\beta_{*}\\|_{2}^{2}/\\nu_{2,0}^{2}}{p-\\mathrm{Tr}(\\boldsymbol{\\Sigma}_{0}^{2}(\\boldsymbol{\\Sigma}_{0}+\\nu_{2,0})^{-2})}+p\\Psi_{3}(\\nu_{2,0};A_{0})\\right\\},\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Step 3: Combining the terms. ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "From Eqs. (158) and (163), we have with probability at least $1-p^{-D}$ that ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left|(p\\nu_{1})^{2}\\langle\\beta_{*},R^{2}\\beta_{*}\\rangle-\\nu_{2}\\Psi_{1}(\\nu_{2};A_{*})+(p\\nu_{1})p\\Psi_{3}(\\nu_{2};A_{*})\\right|}\\\\ &{\\leq C_{*,D,K}\\cdot\\rho_{\\gamma_{+}}(p)^{2}\\mathcal{E}_{4}(p)\\cdot\\left[\\nu_{2}\\Psi_{1}(\\nu_{2};A_{*})+(p\\nu_{1})p\\Psi_{3}(\\nu_{2};A_{*})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "First, it is straightforward to verify that indeed ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\nu_{2}\\Psi_{1}(\\nu_{2};A_{*})-(p\\nu_{1})p\\Psi_{3}(\\nu_{2};A_{*})=\\widetilde{\\mathsf{B}}_{n,p}(\\beta_{*},\\lambda).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Then by Eq. (121) in Lemma B.14, we conclude that with probability at least $1-p^{-D}$ ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|(p\\nu_{1})^{2}\\langle\\beta_{*},R^{2}\\beta_{*}\\rangle-\\widetilde{\\mathsf{B}}_{n,p}(\\beta_{*},\\lambda)\\right|\\leq C_{*,D,K}\\cdot\\rho_{\\gamma_{+}}(p)^{2}\\mathcal{E}_{4}(p)\\cdot\\widetilde{\\mathsf{B}}_{n,p}(\\beta_{*},\\lambda),}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "which concludes the proof of this lemma. ", "page_idx": 45}, {"type": "text", "text": "Lemma B.20. Assuming that $p^{2}\\xi_{\\mathfrak{m}}^{2}\\leq\\gamma_{+}$ , we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\left\\lvert\\nu_{2,0}\\langle\\beta_{0},(\\Sigma_{0}+\\nu_{2,0})^{-1}\\beta_{0}\\rangle+\\lVert\\beta_{+}\\rVert_{2}^{2}-\\nu_{2}\\Psi_{1}(\\nu_{2};A_{*})\\right\\rvert\\leq\\displaystyle\\frac{C}{p}\\nu_{2}\\Psi_{1}(\\nu_{2};A_{*}),}\\\\ &{\\displaystyle\\left\\lvert\\frac{\\langle\\beta_{0},\\Sigma_{0}(\\Sigma_{0}+\\nu_{2,0})^{-2}\\beta_{0}\\rangle+\\langle\\beta_{+},\\Sigma_{+}\\beta_{+}\\rangle/\\nu_{2,0}^{2}}{p-\\mathrm{Tr}(\\Sigma_{0}^{2}(\\Sigma_{0}+\\nu_{2,0})^{-2})}-p\\Psi_{3}(\\nu_{2};A_{*})\\right\\rvert\\leq C\\frac{\\rho_{\\gamma_{+}}(p)}{p}\\cdot p\\Psi_{3}(\\nu_{2};A_{*}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Proof of Lemma B.20. This lemma follows from the same arguments as in the proofs of Lemma B.5 and Lemma B.19. \u53e3 ", "page_idx": 45}, {"type": "text", "text": "C Details of the numerical illustration ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "In this section we provide further examples of comparison between the excess risk computed using the deterministic equivalent in Theorem 3.3 and numerical simulations, together with details about their realization. Results from numerical experiments are obtained averaging over 20-50 seeds. The data dimension is $d=100$ , with the exception of experiments involving real data (see Appendix C.4) and the ones realized considering the Gaussian design described in Appendix C.2. ", "page_idx": 45}, {"type": "image", "img_path": "FBLJIfW64D/tmp/0862ab2406015f8635e1b2b9e53a39e4ad5656bad596bc073a5137799828025e.jpg", "img_caption": ["Figure 4: Relative difference between the excess risk (eq. (6)) of random features ridge regression from numerical simulation and its deterministic equivalent (Theorem 3.3), with regularization strength $\\lambda=0.1$ , and noise variance $\\sigma_{\\varepsilon}^{2}=0.1$ . The relative error is $O((n{\\land}p)^{-1/2})$ , in agreement with eq. (32). The simulations are made following the procedure described in appendix C.2, with $\\xi_{k}=k^{-1.2}$ and $\\beta_{*,k}=k^{-1.46}$ ; (left) $p=3000$ fixed (right) $n=3000$ fixed. "], "img_footnote": [], "page_idx": 46}, {"type": "text", "text": "C.1 Self-consistent equations ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "To solve Equations 18 and 19 numerically, the following approach has been employed. From equation 19, ", "page_idx": 46}, {"type": "equation", "text": "$$\n{\\sqrt{\\left(1-{\\frac{n}{p}}\\right)^{2}+4{\\frac{\\lambda}{p\\nu_{2}}}}}=2{\\frac{\\nu_{1}}{\\nu_{2}}}-1+{\\frac{n}{p}}.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Substituting this expression in equation 18, we obtain ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{2\\left(1-\\displaystyle\\frac{\\nu_{1}}{\\nu2}\\right)=\\displaystyle\\frac{2}{p}\\mathrm{Tr}(\\Sigma(\\Sigma+\\nu_{2})^{-1})}}\\\\ {{\\displaystyle\\xrightarrow{\\nu_{2}>0}\\nu_{2}=\\nu_{1}+\\displaystyle\\frac{\\nu_{2}}{p}\\mathrm{Tr}(\\Sigma(\\Sigma+\\nu_{2})^{-1}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Therefore, the parameters $\\nu_{1}$ and $\\nu_{2}$ have been computed by iterating ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\nu_{1}^{t+1}=\\frac{\\nu_{2}^{t}}{2}\\left[1-\\frac{n}{p}+\\sqrt{\\left(1-\\frac{n}{p}\\right)^{2}+4\\frac{\\lambda}{p\\nu_{2}^{t}}}\\right],}\\\\ {\\displaystyle\\nu_{2}^{t+1}=\\nu_{1}^{t+1}+\\frac{\\nu_{2}^{t}}{p}\\mathrm{Tr}(\\Sigma(\\Sigma+\\nu_{2}^{t})^{-1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "until a chosen tolerance $\\epsilon$ was reached. ", "page_idx": 46}, {"type": "text", "text": "C.2 Gaussian design ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Figures 3, 4 and 9 have been realized considering Gaussian design for the vectors in \u2019feature space\u2019 defined in Appendix B.1. In particular, fixed $\\Sigma$ and $\\beta_{*}$ , we have drawn ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\{g_{i}\\}_{i\\in[n]}\\sim_{\\mathrm{i.i.d.}}{\\mathcal{N}}(\\mathbf{0},I),\\quad\\{f_{j}\\}_{j\\in[p]}\\sim_{\\mathrm{i.i.d.}}{\\mathcal{N}}(\\mathbf{0},\\Sigma),\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "and consequently $\\{y_{i}=\\beta_{*}^{\\top}\\pmb{g}_{i}\\}_{i\\in[n]}$ . Then the random feature estimator can be computed according to eqs. (61) and (62). In the figures produced with this setting, the elements of $\\beta_{*}$ and $\\operatorname{diag}(\\Sigma)$ follow power-laws truncated at the component $10^{4}$ . ", "page_idx": 46}, {"type": "text", "text": "C.3 Empirical diagonalization ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Whenever the data probability distribution $\\mu_{x}$ or the weights distribution $\\mu_{w}$ are unknown (e.g. in all cases involving real data), we estimated the matrix $\\Sigma$ and the vector $\\beta_{*}$ following the procedure described in this section and summarized in Algorithm 1. Consider a data set of $N$ covariates $\\{x_{i}\\}_{i\\in[N]}$ drawn from $\\mu_{x}$ and $N$ noiseless labels $\\{y_{i}\\;=\\;f_{*}(x_{i})\\}_{i\\in[N]}$ , and a set of $P$ weights $\\{w_{j}\\}_{j\\in[P]}$ . In Figures 1, 5, 6, 7, 8, for which this procedure was used, we take both $N,P=10^{4}$ (with the exception of Fig. 7 (right), where $P=80001$ ) and approximate $\\mu_{x}$ and $\\mu_{w}$ respectively with the empirical distributions $\\begin{array}{r}{\\tilde{\\mu}_{x}=\\sum_{i=1}^{N}N^{-1}\\delta({\\pmb x}-{\\pmb x}_{i})}\\end{array}$ and $\\begin{array}{r}{\\tilde{\\mu}_{w}=\\sum_{j=1}^{P}P^{-1}\\delta(\\pmb{w}-\\pmb{w}_{j}).}\\end{array}$ . Then eq. (2) becomes ", "page_idx": 46}, {"type": "text", "text": "", "page_idx": 47}, {"type": "equation", "text": "$$\nK({\\pmb x},{\\pmb x}^{\\prime})=\\mathbb{E}_{{\\pmb w}\\sim\\mu_{w}}\\left[\\varphi({\\pmb x},{\\pmb w})\\varphi({\\pmb x}^{\\prime},{\\pmb w})\\right]=\\sum_{j=1}^{P}P^{-1}\\varphi({\\pmb x},{\\pmb w}_{j})\\varphi({\\pmb x}^{\\prime},{\\pmb w}_{j}),\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "and since eq. (11) implies $\\mathbb{E}_{\\pmb{x}}K(\\pmb{x},\\pmb{x}^{\\prime})\\psi_{k}(\\pmb{x})=\\xi_{k}^{2}\\psi_{k}(\\pmb{x}^{\\prime})$ , defining the Gram matrix $K^{\\mathrm{emp}}\\in\\mathbb{R}^{N\\times N}$ with elements $\\tilde{K}_{i i^{\\prime}}=K(\\pmb{x}_{i},\\pmb{x}_{i^{\\prime}})N^{-1}$ and the vectors $\\tilde{\\boldsymbol{\\psi}}^{k}=(\\psi_{k}(\\mathbf{x}_{1}),\\dots,\\psi_{k}(\\mathbf{x}_{N}))^{\\top}$ , we can write the following eigenvalue problems, for $k\\in[N]$ : ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\tilde{K}\\tilde{\\psi}_{k}=\\tilde{\\xi}_{k}^{2}\\tilde{\\psi}_{k}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "We then constructed the matrix $\\tilde{\\Sigma}=\\mathrm{diag}(\\tilde{\\xi}_{1}^{2},\\ldots,\\tilde{\\xi}_{N}^{2})$ and used it as an approximation of $\\Sigma$ . One should note that in this situation $\\mathbb{E}_{{\\pmb x}}\\psi_{k}({\\pmb x})\\psi_{k^{\\prime}}({\\pmb x})=\\delta_{k k^{\\prime}}$ corresponds to $\\tilde{\\psi}_{k}\\tilde{\\psi}_{k^{\\prime}}=N\\delta_{k k^{\\prime}}$ . Similarly, eq. (13) implies $\\beta_{*,k}=\\mathbb{E}_{\\pmb{x}}\\left[f_{*}(\\pmb{x})\\psi_{k}(\\pmb{x})\\right]$ , which can be approximated by ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{\\beta}_{k}=N^{-1}{\\pmb y}^{\\top}\\tilde{\\psi}_{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Algorithm 1 Empirical diagonalization ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Require: $\\{\\boldsymbol{x}_{i}\\}_{i\\in[N]}$ \u223ci.i.d. $\\mu_{x}$ , $\\{y_{i}=f_{*}(\\pmb{x}_{i})\\}_{i\\in[N]}$ , {wj}j\u2208[P ] \u223ci.i.d. \u00b5w   \nEnsure: $\\Sigma$ , $\\beta_{*}$ , $\\bar{\\mathsf{R}}_{n,p}^{\\mathsf{~\\,~}}(\\beta_{*},\\lambda)$ for $i,i^{\\prime}\\in\\{1,\\dots,N\\}$ do $\\begin{array}{r}{\\tilde{K}_{i i^{\\prime}}\\gets(N P)^{-1}\\sum_{j=1}^{P}\\varphi(\\pmb{x}_{i},\\pmb{w}_{j})\\varphi(\\pmb{x}_{i^{\\prime}},\\pmb{w}_{j})}\\end{array}$ end for $\\{(\\tilde{\\xi}_{k},\\tilde{\\psi}_{k})_{k\\in[N]}\\}\\leftarrow\\mathrm{eig}(\\tilde{\\pmb{K}})$ $\\circ\\tilde{\\psi}_{k}\\tilde{\\psi}_{k^{\\prime}}\\overset{!}{=}N\\delta_{k k^{\\prime}}$ for $k\\in\\{1,\\ldots,N\\}$ do \u03b2\u02dck \u2190N \u22121y\u22a4\u03c8\u02dck end for $\\begin{array}{r l}&{\\Sigma\\leftarrow\\mathrm{diag}(\\Tilde{\\xi}_{1},\\dots,\\Tilde{\\xi}_{N})}\\\\ &{\\beta_{*}\\leftarrow(\\beta_{1},\\dots,\\beta_{N})^{\\top}}\\end{array}$ Iterate eqs. (167-168) up to tolerance $\\epsilon$ Compute the deterministic equivalent for the excess risk (22-24) ", "page_idx": 47}, {"type": "text", "text": "C.4 Real data ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "We performed numerical simulations sampling the training data from the MNIST data set Lecun et al. [1998] and the FashionMNIST data set Xiao et al. [2017], standardizing both covariates and labels, reshaping the images into vectors with $d=748$ . Results are shown in Figures 1 (right) and 7 (left). ", "page_idx": 47}, {"type": "text", "text": "C.5 Trained network ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "In Figure 7 (right), we apply the procedure described in Appendix C.3 to the trained weights of a two layer neural network with hidden layer of size $p$ ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\hat{f}(\\pmb{x};\\pmb{W},\\pmb{a})=\\frac{1}{\\sqrt{p}}\\sum_{j=1}^{p}a_{j}\\varphi(\\langle\\pmb{x},\\pmb{w}_{j}\\rangle).\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "At initialization, the weights are randomly drawn; then, after sampling a training dataset $X_{\\mathrm{tr}}\\in\\mathbb{R}^{n_{\\mathrm{tr}}\\times d}$ , $\\pmb{y}_{\\mathrm{tr}}\\in\\mathbb{R}^{n_{\\mathrm{tr}}}$ , the weights of the first layer are trained using gradient descent, iterating for $t=1,\\dots,T$ the following ", "page_idx": 47}, {"type": "equation", "text": "$$\nW_{t+1}=W_{t}+\\frac{\\eta}{n_{\\mathrm{tr}}}X_{\\mathrm{tr}}^{\\top}\\left[\\left((y_{\\mathrm{tr}}-\\hat{f}(\\langle X_{\\mathrm{tr}};W_{t},a)\\rangle^{\\top}a\\right)\\odot\\varphi^{\\prime}(X_{\\mathrm{tr}}W_{t}^{\\top})\\right],\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where $\\eta$ is the learning rate, $\\odot$ is the Hadamard product, $\\hat{f}$ and $\\varphi$ are applied component-wise; finally, the weights of the second layer are minimized using ridge regression, as in eq. (5). ", "page_idx": 47}, {"type": "image", "img_path": "FBLJIfW64D/tmp/e005135da357aed9c49db81b71fb4241e072f7660b381fb4494c4555de630cc1.jpg", "img_caption": ["Figure 5: Excess risk eq. (6) of random features ridge regression. Solid lines are obtained from the deterministic equivalent in Theorem 3.3, and points are numerical simulations, with the different curves denoting different regularization strengths $\\lambda\\:\\geq\\:0$ . Training data $(x_{i},y_{i})_{i\\in[n]}$ , sampled from a teacher-student model $y_{i}\\;=\\;\\operatorname{tanh}(\\langle\\beta,\\pmb{x}_{i}\\rangle)\\,+\\,\\varepsilon_{i}$ , $\\sigma_{\\varepsilon}^{2}\\;=\\;0.1$ , with random feature map $\\varphi(\\pmb{x},\\pmb{w})=\\operatorname{ReLU}(\\langle\\pmb{w},\\pmb{x}\\rangle)$ . Both covariates $\\{x_{i}\\}$ and weights $\\{w_{i}\\}$ are uniformly sampled from the $d$ -dimensional spheres respectively with radius $\\sqrt{d}$ and 1. (Left) Excess risk as a function of $n$ , with $p=600$ fixed. (Right) Excess risk as a function of $p$ , with $n=500$ fixed. "], "img_footnote": [], "page_idx": 48}, {"type": "image", "img_path": "FBLJIfW64D/tmp/d14f9ee0b4754c03803d20c4fe4891a48b6dc6545f7b0ce6819470f7f12fc6ce.jpg", "img_caption": ["Figure 6: Excess risk eq. (6) of random features ridge regression. Solid lines are obtained from the deterministic equivalent in Theorem 3.3, and points are numerical simulations, with the different curves denoting different regularization strengths $\\lambda\\geq0$ . Training data $(x_{i},y_{i})_{i\\in[n]}$ , sampled from a teacher-student model $y_{i}=\\operatorname{tanh}\\!\\left(\\left\\langle\\beta,\\pmb{x}_{i}\\right\\rangle\\right)+\\varepsilon_{i}$ , $\\sigma_{\\varepsilon}^{2}=0.1$ , $\\mathbf{\\boldsymbol{x}}_{i}\\sim_{\\mathrm{i.i.d.}}$ $\\mathcal{N}(0,\\pmb{I}_{d})$ , with a spiked random feature map $\\varphi({\\pmb x},{\\pmb w})=\\operatorname{erf}(\\langle{\\pmb w}+u{\\pmb v},{\\pmb x}\\rangle)$ where $\\pmb{w}\\sim\\mathcal{N}(0,d^{-1}\\pmb{I}_{d})$ , $\\pmb{v}\\in\\mathbb{R}^{d}\\sim\\mathcal{N}(0,d^{-1}\\pmb{I}_{d})$ , and $u\\sim\\mathcal{N}(0,1)$ . (Left) Excess risk as a function of $n$ , with $p=500$ fixed. (Right) Excess risk as a function of $p$ , with $n=300$ fixed. "], "img_footnote": [], "page_idx": 48}, {"type": "image", "img_path": "FBLJIfW64D/tmp/984c988e1dcffe7357ba5ff095a362c22893070d8a41aea48d3f1f78855f2378.jpg", "img_caption": ["Figure 7: (Left) Excess risk eq. (6) of random features ridge regression. Solid lines are obtained from the deterministic equivalent in Theorem 3.3, and points are numerical simulations, with the different curves denoting different regularization strengths $\\lambda\\ \\geq\\ 0$ . Training data $({\\bf{x}}_{i},y_{i})_{i\\in[n]}$ , $n=300$ , sub-sampled from the MNIST data set Lecun et al. [1998], with feature map given by $\\varphi(\\pmb{x},\\pmb{w})\\stackrel{}{=}\\mathrm{erf}\\left(\\left<\\pmb{w},\\pmb{x}\\right>\\right)$ and $\\mu_{w}\\,=\\mathcal{N}(0,d^{-1}I_{d})$ . (Right) Excess risk eq. (6) of random features ridge regression. Solid lines are obtained from the deterministic equivalent in Theorem 3.3, and points are numerical simulations, with the different curves denoting different number of total iterations of gradient descent on the weight of the first layer with learning rate $\\eta\\,=\\,10^{-2}$ , before training the second layer with regularization strength $\\lambda=10^{-4}$ (details in Appendix C.5). Zero iterations corresponds to random feature regression (RF). Training data $({\\bf x}_{i},y_{i})_{i\\in[n]}$ , sampled from a teacherstudent model $y_{i}\\,=\\,\\langle\\beta,{\\pmb x}_{i}\\rangle$ , with random feature map $\\varphi(\\pmb{x},\\pmb{w})=\\operatorname{ReLU}(\\pmb{\\langle w,x\\rangle})$ and $p=8000$ fixed. Both covariates $\\{x_{i}\\}$ and initializatio\u221an weights $\\{w_{i}\\}$ are uniformly sampled from the $d\\!.$ - dimensional spheres respectively with radius $\\sqrt{d}$ and 1. "], "img_footnote": [], "page_idx": 49}, {"type": "image", "img_path": "FBLJIfW64D/tmp/9ebdbd3a6c22fec27ec746352c30c71172c86ee971630e3caec63c5bf4b1394f.jpg", "img_caption": ["Figure 8: Excess risk eq. (6) of random features ridge regression. Solid lines are obtained from the deterministic equivalent in Theorem 3.3, and points are numerical simulations, with the different curves denoting different regularization strengths $\\lambda\\geq0$ . Training data $(x_{i},y_{i})_{i\\in[n]}$ , sampled from a teacher-student model $y_{i}\\,=\\,\\operatorname{tanh}(\\langle{\\pmb\\beta},{\\pmb x}_{i}\\rangle)+\\varepsilon_{i}$ , $\\sigma_{\\varepsilon}^{2}\\,=\\,0.1$ , with random feature map given by the convolutional features with global average pooling $\\begin{array}{r}{\\varphi(\\pmb{x},\\pmb{w})\\,=\\,1/{d}\\sum_{\\ell=1}^{d}\\operatorname{ReLU}(\\langle\\pmb{w},g_{\\ell}\\cdot\\pmb{x}\\rangle)}\\end{array}$ where $g_{\\ell}\\cdot\\pmb{x}=(x_{\\ell+1},\\ldots,x_{d},x_{1},\\ldots,x_{\\ell})$ is the -shift operator with cyclic boundary conditions. Both covariates $\\{x_{i}\\}$ an\u221ad weights $\\{w_{i}\\}$ are uniformly sampled from the $d$ -dimensional spheres respectively with radius $\\sqrt{d}$ and 1. (Left) Excess risk as a function of $n$ , with $p=500$ fixed. (Right) Excess risk as a function of $p$ , with $n=500$ fixed. The discrepancy between theoretical results and numerical experiments is $\\approx\\mathcal{R}/\\sqrt{n\\wedge p}$ , compatible with the approximation rate in eq. (32). "], "img_footnote": [], "page_idx": 49}, {"type": "text", "text": "D Derivation of the rates ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "In this appendix we present a sketch of the derivation of the decay rates for the excess error given in Section 4. We choose ", "page_idx": 50}, {"type": "equation", "text": "$$\np=n^{q},\\quad\\lambda=n^{-(\\ell-1)},\\quad\\mathrm{with~}q,l\\geq0\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "and we assume ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\eta_{k}=k^{-\\alpha},\\quad\\beta_{*k}=k^{-\\frac{1+2\\alpha r}{2}},\\quad\\mathrm{with}\\;\\alpha>1,\\;r>0,\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "which follows the source and capacity conditions stated in (37). In order to simplify the derivation of the rates, we introduce the following notation: ", "page_idx": 50}, {"type": "equation", "text": "$$\nT_{\\delta\\gamma}^{s}(\\nu):=\\sum_{k=1}^{\\infty}\\frac{k^{-s-\\delta\\alpha}}{(k^{-\\alpha}+\\nu)^{\\gamma}},\\qquad\\qquad\\qquad s\\in{0,1,\\;0\\leq\\delta\\leq\\gamma}.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "For $s+\\alpha(\\delta-\\gamma)<1$ and in the limit $\\nu\\to0$ , this term can be written as a Riemann sum as follows ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{T_{\\delta\\gamma}^{s}(\\nu)=\\nu^{-\\gamma+\\delta+s/\\alpha}\\displaystyle\\sum_{k=1}^{\\infty}\\displaystyle\\frac{(k\\nu^{1/\\alpha})^{-s-\\delta\\alpha}}{((k\\nu^{1/\\alpha})^{-\\alpha}+1)^{\\gamma}}}}\\\\ {{\\overset{\\nu\\to0}{\\approx}\\nu^{-(\\gamma-\\delta)-(1-s)/\\alpha}\\displaystyle\\int_{\\nu^{1/\\alpha}}^{\\infty}\\displaystyle\\frac{x^{-s-\\delta\\alpha}}{(x^{-\\alpha}+1)^{\\gamma}}={\\cal O}\\left(\\nu^{-(\\gamma-\\delta)-(1-s)/\\alpha}\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Otherwise, if $s+\\alpha(\\delta-\\gamma)>1$ , we can write: ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle T_{\\delta\\gamma}^{s}(\\nu)=\\sum_{k=1\\atop k\\to0}^{\\lfloor\\nu^{1/\\alpha}\\rfloor}\\frac{k^{-s-\\delta\\alpha}}{(k^{-\\alpha}+\\nu)^{\\gamma}}+\\displaystyle\\sum_{k=\\lfloor\\nu^{1/\\alpha}\\rfloor+1}^{\\infty}\\frac{k^{-s-\\delta\\alpha}}{(k^{-\\alpha}+\\nu)^{\\gamma}}}}\\\\ {{\\displaystyle\\overset{\\nu\\to0}{\\approx}O(1)+\\nu^{-(\\gamma-\\delta)-(1-s)/\\alpha}\\int_{1+\\nu^{1/\\alpha}}^{\\infty}\\frac{x^{-s-\\delta\\alpha}}{(x^{-\\alpha}+1)^{\\gamma}}=O(1).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Hence, for $\\nu\\to0$ , ", "page_idx": 50}, {"type": "equation", "text": "$$\nT_{\\delta\\gamma}^{s}(\\nu)=O\\left(\\nu^{1/\\alpha\\left[s-1+\\alpha(\\delta-\\gamma)\\right]\\wedge0}\\right).\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Rewriting (18-19) as follows, we study the dependence of the positive parameters $\\nu_{1}$ and $\\nu_{2}$ with $n$ : ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\left\\{\\nu_{2}=\\frac{\\nu_{2}}{p}T_{11}^{0}(\\nu_{2})+\\nu_{1}\\right.}}\\\\ {{\\displaystyle\\left.\\nu_{1}=\\frac{\\nu_{1}}{n}T_{11}^{0}(\\nu_{2})+\\frac{\\lambda}{n}\\right.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "In the limit $n\\to\\infty$ , we can distinguish the following regimes ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\{\\begin{array}{l l}{T_{11}^{0}(\\nu_{2})\\leqslant n,p}\\\\ {\\nu_{1}=n^{-1}\\lambda\\left(1+{\\cal O}\\left(T_{11}^{0}(\\nu_{2})n^{-1}\\right)\\right)}\\\\ {\\nu_{2}=n^{-1}\\lambda\\left(1+{\\cal O}\\left(T_{11}^{0}(\\nu_{2})(n\\wedge p)^{-1}\\right)\\right)}\\\\ {\\left\\{\\begin{array}{l l}{T_{11}^{0}(\\nu_{2})\\leqslant p}\\\\ {\\nu_{1}\\geqslant\\lambda n^{-1}}\\\\ {T_{11}^{1}(\\nu_{2})=n}\\end{array}\\right.\\right.}&{\\stackrel{(a)}{\\Longrightarrow}\\left\\{\\begin{array}{l l}{\\nu_{2}\\frac{-1\\alpha}{\\nu_{2}}\\leqslant n^{-1}\\infty\\quad n^{1/\\alpha}\\Longrightarrow\\ell<\\alpha(1\\wedge q)}\\\\ {\\nu_{2}\\frac{-1\\alpha}{\\nu_{1}}\\ll n^{-1}}\\\\ {\\nu_{1}\\frac{-1\\alpha}{\\nu_{2}}\\=n^{-1}}\\end{array}\\right.}\\\\ {\\left\\{\\begin{array}{l l}{T_{11}^{0}(\\nu_{2})=n-(n\\nu_{1})^{-1}\\lambda}\\\\ {\\nu_{2}=\\nu_{1}(1+\\cal O)\\left(T_{12}^{0})p^{-1}\\right\\}}\\\\ {\\left\\{\\begin{array}{l l}{T_{22}^{0}=\\nu_{1}(1+\\omega(1))}\\\\ {\\nu_{1}\\leqslant\\nu_{2}}\\\\ {\\nu_{1}\\leqslant\\nu_{2}}\\\\ {\\nu_{1}\\leqslant n^{-1}\\lambda\\left(1+{\\cal O}\\left(T_{11}^{0}(\\nu_{2})n^{-1}\\right)\\right)}\\\\ {\\nu_{1}\\left(\\nu_{2}\\right)=p-\\nu_{1}\\nu_{2}^{-1}\\end{array}\\right.}&{\\stackrel{(a)}{\\Longrightarrow}\\left\\{\\begin{array}{l l}{\\nu_{2}\\frac{-1\\alpha}{\\nu_{2}}\\leqslant n^{-1}\\infty\\quad n^{-\\ell}}\\\\ {\\nu_{1}\\leqslant n^{-\\ell}(1+\\sigma(1))}\\\\ {\\nu_{2}\\frac{-1\\alpha}{\\nu_{1}}=n^{-\\ell}(1)}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "In $(a)$ we have used the fact that, for $\\nu_{2}$ constant or diverging with $n$ , $T_{11}^{0}(\\nu_{2})$ is respctively $O(1)$ or infinitesimal, while we have that $T_{11}^{0}(\\nu_{2})$ is diverging in both cases. Hence, $\\nu_{2}$ must be infinitesimal, ", "page_idx": 50}, {"type": "text", "text": "allowing us to use (173). In conclusion, as $n\\to\\infty$ , ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nu_{1}\\approx\\left\\{O\\left(n^{-\\alpha}\\right),\\quad\\mathrm{for}\\,q>1\\mathrm{~and~}\\ell>\\alpha,\\right.}\\\\ &{\\left.\\qquad\\qquad\\quad n^{-\\ell},\\quad\\mathrm{otherwise}\\,\\right.}\\\\ &{\\nu_{2}\\approx O\\left(n^{-\\alpha\\left(1\\wedge q\\wedge\\ell/\\alpha\\right)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "in particular ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\frac{\\nu_{1}}{\\nu_{2}}=\\left\\{1-O\\left(\\nu_{2}^{-1/\\alpha}n^{-q}\\right),\\quad\\mathrm{for}\\,\\,q>1\\wedge\\ell/\\alpha\\quad.\\right.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "D.1 Variance term ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Considering the results (176-178), we can write eq. (20) as ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Upsilon(\\nu_{1},\\nu_{2})=\\frac{p}{n}\\left[\\left(1-\\frac{\\nu_{1}}{\\nu_{2}}\\right)^{2}+\\left(\\frac{\\nu_{1}}{\\nu_{2}}\\right)^{2}\\frac{T_{22}^{0}(\\nu_{2})}{p-T_{22}^{0}(\\nu_{2})}\\right]}\\\\ &{\\qquad\\qquad=\\left\\{n^{-1}O(\\nu_{2}^{-1/\\alpha})=O\\left(n^{-(1-(1\\wedge\\ell/\\alpha))}\\right),\\quad\\mathrm{for}\\,q>1\\wedge\\ell/\\alpha}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "One could notice, using the integral approximation of the Riemann sum $T_{22}^{0}(\\nu_{2})$ given in (173), that $1-\\Upsilon(\\nu_{1},\\nu_{2})=O(1)$ for any choice of $\\ell$ and $q$ . Hence, the variance term given by (23) decays with $n$ with rate ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\gamma_{\\nu}(\\ell,q)=1-\\left(\\frac{\\ell}{\\alpha}\\wedge q\\wedge1\\right).\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "D.2 Bias term ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Using again (176-178), we can compute the rate of $\\chi(\\nu_{2})$ defined in eq. (21), as $n\\to\\infty$ : ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\chi(\\nu_{2})=\\displaystyle\\frac{T_{12}^{0}(\\nu_{2})}{p-T_{22}^{0}(\\nu_{2}))}=n^{-q}{\\cal O}\\left(\\nu_{2}^{-1-1/\\alpha}\\right)\\left(1+n^{-q}{\\cal O}\\left(\\nu_{2}^{-1/\\alpha}\\right)\\right)}}\\\\ {{=n^{-q}{\\cal O}\\left(\\nu_{2}^{-1-1/\\alpha}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Using the integral approximation given in (173), one could verify that $p-T_{22}^{0}(\\nu_{2})=O(p)$ for any choice of $\\ell$ and $q$ . ", "page_idx": 51}, {"type": "text", "text": "The deterministic equivalent for the bias term, given in eq. (22), can be written as ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{B}_{n,p}(\\beta_{*},\\lambda)=\\frac{\\nu_{2}^{2}}{1-\\Upsilon\\left(\\nu_{1},\\,\\nu_{2}\\right)}\\left(T_{2r,2}^{1}(\\nu_{2})+\\chi(\\nu_{2})T_{2r+1,2}^{1}(\\nu_{2})\\right)}\\\\ &{\\qquad\\qquad=O\\left(\\nu_{2}^{2}\\right)O\\left(\\nu_{2}^{2\\left(r-1\\wedge0\\right)}+n^{-q}\\nu_{2}^{-1-1/\\alpha+(2r-1)\\wedge0}\\right)}\\\\ &{\\qquad\\qquad=O\\left(\\nu_{2}^{2\\left(r\\wedge1\\right)}+n^{-q}\\nu_{2}^{-1/\\alpha+2\\left(r\\wedge1/2\\right)}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "where we have used (175) to compute the scalings of the terms $T^{1}\\delta\\gamma$ and the fact that $1\\!-\\!\\Upsilon(\\nu_{1},\\nu_{2})=$ $O(1)$ .   \nFrom eq. (183), and using the result (177), it is straightforward to see that the decay rate of the bias term is given by ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\gamma_{B}=\\left[2\\alpha\\left({\\frac{\\ell}{\\alpha}}\\wedge q\\wedge1\\right)(r\\wedge1)\\right]\\wedge\\left[\\left(2\\alpha\\left(r\\wedge{\\frac{1}{2}}\\right)-1\\right)\\left({\\frac{\\ell}{\\alpha}}\\wedge q\\wedge1\\right)+q\\right].\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Examples of the results of Theorem 4.1 and Corollary 4.2 are shown in Fig. 3 and 9. ", "page_idx": 51}, {"type": "image", "img_path": "FBLJIfW64D/tmp/f98e7cb91fb2bf0236fcf02fbe86e51235e3a22e536ec4d393f735c0b2e339af.jpg", "img_caption": ["Figure 9: Excess risk eq. (6) of random features ridge regression as a function of the number of samples $n$ under source and capacity conditions eq. (37) and power-law assumptions $\\lambda=n^{-(\\ell-1)}$ , $p=n^{q}$ , with noise variance $\\sigma_{\\varepsilon}^{2}=1$ , obtained from the deterministic equivalent Theorem 3.3. Dashed and dotted lines are the analytical rates from Theorem 4.1, stated in the legend. The colour scheme is the following: variance dominated region: orange and brown for the slow decay regime; cyan for the bias dominated region; shades of green for the optimal decay (red lines in Fig. 2 (right). In particular we show: (left) the crossover between the orange and teal regions in Fig. 2 at fixed regularization and $r<1/2$ ; (right) the optimal decay rate along the horizontal red line line in Fig. 2 at $q=q_{*}$ and $r<1/2$ , for any $\\lambda\\le\\lambda_{*}$ , included the non regularized case. "], "img_footnote": [], "page_idx": 52}, {"type": "text", "text": "D.3 Details of Remark 4.1 ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "In order to extend the results of Theorem 4.1 and Corollary 4.2 to the excess risk defined in (6), in this section we compute the intervals for $\\ell$ and $q$ such that the assumptions of Theorem 3.3 hold and the approximation rates $\\mathcal{E}(n,p)$ are vanishing, under source and capacity conditions. ", "page_idx": 52}, {"type": "text", "text": "Given $n$ , $p=n^{q}$ , $\\lambda^{-(\\ell-1)}$ and $\\nu_{2}$ as in (177) assumption 3.2 is verified for $\\mathrm{m}=n^{q+\\ell}$ and $\\mathsf{C}_{*}=$ $O(\\nu_{2}^{-1})$ . In fact ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\sum_{k=\\mathrm{m}+1}^{\\infty}\\xi_{k}^{2}>\\int_{\\mathrm{m}+1}^{\\infty}x^{-\\alpha}={\\frac{(\\mathrm{m}+1)^{1-\\alpha}}{\\alpha-1}}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "and the inequality in (16) holds if ", "page_idx": 52}, {"type": "equation", "text": "$$\nn^{2q}(\\mathrm{m}+1)^{-\\alpha}\\leq n^{q-\\ell}{\\frac{(\\mathrm{m}+1)^{1-\\alpha}}{\\alpha-1}}\\implies\\mathrm{m}\\geq n^{q+\\ell}(\\alpha-1)-1.\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "The inequalities in (17) can be written as ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{C}_{*}\\ge\\frac{T_{11}^{0}\\left(\\nu_{2}\\right)}{T_{22}^{0}\\left(\\nu_{2}\\right)}=O(1),}\\\\ &{\\mathsf{C}_{*}\\ge\\frac{T_{2r,1}^{1}\\left(\\nu_{2}\\right)}{\\nu_{2}T_{2r,2}^{1}\\left(\\nu_{2}\\right)}=O\\left(\\nu_{2}^{-((0\\vee(2r-1))\\wedge1)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Then, introducing $\\eta_{*}\\in(0,1/2)$ , we can compute the following quantities of interest (introduced in eqs. (25) to (27)): ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{r\\_{\\Sigma}(|\\eta_{*}\\cdot k|)=\\frac{\\mathrm{Tr}(\\Sigma_{\\Sigma[\\eta_{*}\\cdot k]})}{||\\Sigma_{\\Sigma[\\eta_{*}\\cdot k]}||_{\\infty}}=O(|\\eta_{*}\\cdot k|)=O(k)}\\\\ &{\\quad\\quad M_{\\Sigma}(k)=1+\\frac{r\\_{\\Sigma}(|\\eta_{*}\\cdot k|)\\vee k}{k}\\log\\left(r_{\\Sigma}(|\\eta_{*}\\cdot k|)\\vee k\\right)=1+O(\\log k),}\\\\ &{\\quad\\quad\\rho_{\\kappa}(p)=1+\\frac{p\\cdot\\xi_{[\\eta_{*}\\cdot p]}^{\\prime}}{\\kappa}M_{\\Sigma}(p)=1+O\\left(\\frac{n^{q(1-\\alpha)}}{\\kappa}\\log n\\right),}\\\\ &{\\quad\\quad\\widetilde{\\rho}_{\\kappa}(n,p)=1+1[n\\leq p/\\eta_{*}]\\cdot\\left\\{\\frac{n\\xi_{[\\eta_{*}\\cdot n]}^{2}}{\\kappa}+\\frac{n}{p}\\cdot\\rho_{\\kappa}(p)\\right\\}M_{\\Sigma}(n)}\\\\ &{\\quad\\quad\\quad\\quad\\quad n\\geq\\eta_{*}^{\\eta_{1}/(1-q)}1+1[q\\geq1]O\\left(\\frac{n^{1-\\alpha}}{\\kappa}\\log n\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Similarly, considering $\\nu_{1}$ scaling as in eq. (176), we have that eq. (31) ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\gamma_{\\lambda}=\\displaystyle\\frac{p\\lambda}{n}+\\sum_{k=m+1}^{\\infty}\\xi_{k}^{2}=O\\left(n^{q-\\ell}+n^{(q+\\ell)(1-\\alpha)}\\right),}&{(194)}\\\\ &{\\gamma_{+}=p\\nu_{1}+\\displaystyle\\sum_{k=m+1}^{\\infty}\\xi_{k}^{2}=O\\left(1[q\\ge1]n^{q-(\\ell\\wedge\\alpha)}+1[q<1]n^{q-\\ell}+n^{(q+\\ell)(1-\\alpha)}\\right)}&{(195)}\\\\ &{\\quad=O\\left(1[q\\ge1]n^{q-(\\ell\\wedge\\alpha)}+1\\left[\\displaystyle\\frac{\\ell}{\\alpha}(2-\\alpha)\\le q<1\\right]n^{q-\\ell}+1\\left[q<\\displaystyle\\frac{\\ell}{\\alpha}(2-\\alpha)\\right]n^{(q+\\ell)(1-\\alpha)}\\right).}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "The last step is a consequence of ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{q-(\\ell\\wedge\\alpha)>(q+\\ell)(1-\\alpha)\\implies q>\\displaystyle\\frac{\\ell}{\\alpha}\\underbrace{(1-\\alpha)}_{<0}+\\left(\\frac{\\ell}{\\alpha}\\wedge1\\right),}}\\\\ {{q-\\ell>(q+\\ell)(1-\\alpha)\\implies q>\\displaystyle\\frac{\\ell}{\\alpha}(2-\\alpha)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Fixing $K>0$ and considering the , we consider condition (28): ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\lambda\\ge n^{-K}\\ \\iff\\ell\\le1+K,}&{}\\\\ {\\gamma_{\\lambda}\\ge p^{-K}\\iff\\left\\{\\ell\\le q(1+K)\\ }&{\\forall\\left\\{\\ell\\le\\frac{q(1+K-\\alpha)}{\\alpha-1}\\ \\ }\\right.}\\\\ {\\left.\\qquad\\qquad\\qquad\\quad\\ \\right.}&{,}\\\\ {\\widetilde{\\rho}_{\\lambda}(n,p)^{5/2}\\cdot\\log^{3/2}(n)\\le\\ K\\sqrt{n}\\iff\\ell>1[q\\ge1]\\left(\\alpha+\\frac{1}{5}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "and, similarly, condition (29) is satisfied if, for $q\\geq1$ ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\left(1+O\\left(n^{\\ell-\\alpha}\\log n\\right)\\right)^{2}\\left(O\\left(1+n^{(\\ell\\wedge\\alpha)-q\\alpha}\\log n\\right)\\right)^{8}q\\log^{4}n\\leq K n^{q/2}}\\\\ &{\\Longrightarrow2(\\ell-\\alpha)\\vee0<\\frac{q}{2}\\implies\\ell<\\frac{q}{4}+\\alpha,}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "while, for $q<1$ , if ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\{1>q\\ge\\frac{\\ell}{\\alpha}(2-\\alpha)\\right.}\\\\ &{\\left.\\left(1+O\\left(n^{\\ell-q\\alpha}\\right)\\right)^{8}q\\log^{4}n\\le K n^{q/2}\\right.\\quad\\forall\\left\\{\\left.\\left(1+O\\left(n^{\\ell(\\alpha-1)}\\right)\\right)^{8}q\\log^{4}n\\le K n^{q/2}\\right.\\right.}\\\\ &{\\implies\\left.\\left\\{1>q\\ge\\frac{\\ell}{\\alpha}(2-\\alpha)\\quad\\forall\\left.\\left\\{q<\\frac{\\ell}{\\alpha}(2-\\alpha)\\right.\\right.}\\\\ &{\\implies\\ell<\\alpha q+\\frac{q}{16}}\\end{array}\\right.\\right.\\quad}\\\\ &{\\implies\\ell<q\\left(\\left(\\alpha+\\frac{1}{16}\\right)\\lor\\frac{1}{16(\\alpha-1)}\\right),^{4}}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "table", "img_path": "FBLJIfW64D/tmp/361e299d01efb2c269a03040147048db8f3480fd03d8679977cb668e0a31daab.jpg", "table_caption": [], "table_footnote": ["Table 1: Dictionary of notation between the source and capacity conditions defined in eq. (38) and the scalings in different neural scaling laws works. Note that since Bahri et al. [2024], Maloney et al. [2022] also employ the greek letter ${}^{\\bullet}\\alpha^{\\bullet}$ , we denote theirs by $\\tilde{\\alpha}$ to avoid confusion. "], "page_idx": 54}, {"type": "text", "text": "where the last step is a consequence of ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle\\frac{\\alpha}{2-\\alpha}\\leq\\alpha+\\frac{1}{16}\\leq\\frac{1}{16(\\alpha-1)},}&{\\qquad\\mathrm{for}\\ \\alpha\\geq\\overline{{\\alpha}}:=\\frac{15+\\sqrt{353}}{32}\\approx1.05588,}\\\\ {\\displaystyle\\frac{\\alpha}{2-\\alpha}>\\alpha+\\frac{1}{16}>\\frac{1}{16(\\alpha-1)},}&{\\qquad\\mathrm{for}\\ \\alpha<\\overline{{\\alpha}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Finally, the approximation rate defined in remark 4.1 is ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{E}(n,p)=\\left(n^{-1/2}+1[q\\geq1]\\tilde{O}\\left(n^{6(\\ell-\\alpha)-1/2}\\right)\\right)+}\\\\ &{\\qquad\\qquad\\qquad\\left(n^{-\\,q/2}+1[q\\geq1]\\tilde{O}\\left(n^{2(\\ell-\\alpha)-q/2}\\right)\\right)\\left(1+\\tilde{O}\\left(\\frac{n^{8q(1-\\alpha)}}{\\gamma_{+}^{8}}\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "where the second term vanishes under the conditions in (203) and (206), and the first term vanishes by further assuming, for $q\\geq1$ ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\ell<\\alpha+\\frac{1}{12}.\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "E Comparison with neural scaling laws ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "In this appendix we discuss the relationship between our results and the recent literature of the theory of neural scaling laws with linear models. We adopt a notation close to ours, with dictionary to their notation given in Table 1 and Table 2. ", "page_idx": 54}, {"type": "text", "text": "Bahri et al. [2024] and Maloney et al. [2022] have considered a model where with Gaussian input data and linear target function: ", "page_idx": 54}, {"type": "equation", "text": "$$\nf_{\\star}(x_{i})=\\langle\\beta_{\\star},x_{i}\\rangle,\\qquad\\qquad\\qquad x_{i}\\sim{\\mathcal N}(0,\\Lambda),\\qquad\\qquad\\qquad i\\in[n]\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "The covariance matrix $\\mathbf{A}=\\mathrm{diag}(\\lambda_{k})_{k\\in[d]}$ is taken to be diagonal, with eigenvalues following a power-law scaling: ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\lambda_{k}\\sim\\left({\\frac{d}{k}}\\right)^{\\alpha},\\qquad\\qquad\\qquad\\qquad k\\in[d]\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "with $\\alpha>1$ and the target weights are assumed to be random Gaussian vectors $\\boldsymbol{\\beta}_{\\star}\\sim\\mathcal{N}(\\boldsymbol{0},\\boldsymbol{1}/d I_{d})$ . In particular, note that $\\mathrm{Tr}\\Lambda\\sim d$ for $d\\rightarrow\\infty$ . Given the training data, they consider least-squares regression in the class of linear random features predictor: ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\hat{f}(\\pmb{x};\\pmb{a})=\\langle\\pmb{a},W\\pmb{x}\\rangle\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "where $W\\in\\mathbb{R}^{p\\times d}$ is a Gaussian random matrix elements in $\\mathcal{N}(0,1/d I_{d})$ .5 ", "page_idx": 54}, {"type": "table", "img_path": "FBLJIfW64D/tmp/b484b2ea363824142f86b02fdc54ec48a5684303ec75a832304c5f8ba8820383.jpg", "table_caption": [], "table_footnote": [], "page_idx": 55}, {"type": "text", "text": "Table 2: Dictionary of notation between the source and capacity conditions defined in eq. (38) and the scalings in different neural scaling laws works. Note that since Paquette et al. [2024] also employs the greek letter \u201c $\\alpha^{\\bullet}$ , we denote theirs by $\\tilde{\\alpha}$ to avoid confusion. ", "page_idx": 55}, {"type": "text", "text": "This setting is a particular case of the one introduced in Section 2. In particular, it satisfies particular source and capacity conditions eq. (38). To see this, note that the feature population covariance is identical to the input data covariance: ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\mathbb{E}[W x x^{\\top}W^{\\top}]=1/d\\Lambda\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Therefore, we can identify $\\pmb{\\Sigma}=\\mathbf{\\Psi}^{1}/d\\pmb{\\Lambda}$ which has $\\mathrm{Tr}{\\bf\\Sigma}\\Sigma<\\infty$ for $\\alpha>1$ in the limit $d\\to\\infty$ . Therefore, the features satisfy a capacity condition with scaling $\\alpha$ . Moreover, the asymptotic kernel is simply the linear kernel: ", "page_idx": 55}, {"type": "equation", "text": "$$\nK(\\pmb{x},\\pmb{x}^{\\prime})=\\mathbb{E}_{\\pmb{w}}[\\langle\\pmb{w},\\pmb{x}\\rangle\\langle\\pmb{w},\\pmb{x}^{\\prime}\\rangle]=\\frac{\\langle\\pmb{x},\\pmb{x}^{\\prime}\\rangle}{d}.\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Since the target variance is constant, this is equivalent to a source condition with: ", "page_idx": 55}, {"type": "equation", "text": "$$\nr=\\frac{1}{2}\\left(1-\\frac{1}{\\alpha}\\right)\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Since $\\alpha\\in(1,\\infty)$ , we are always in the hard regime $r\\in(0,1/2)$ where the target does not belong to the RKHS $\\mathcal{H}=\\mathbb{R}^{d}$ . Indeed, since $\\beta_{\\star}\\sim\\mathcal{N}(0,{^1/d I_{d}})$ , we have: ", "page_idx": 55}, {"type": "equation", "text": "$$\n||f_{\\star}||_{\\mathcal{H}}^{2}=\\sum_{k=1}^{d}\\beta_{\\star,k}^{2}\\lambda_{k}\\sim d^{\\alpha-1}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "which indeed diverges as $d\\rightarrow\\infty$ . Moreover, note that least-squares regression correspond to the case $\\ell=\\infty$ . ", "page_idx": 55}, {"type": "text", "text": "From the discussion above, the bias term scalings from Bahri et al. [2024] (resolution limited regime) and Maloney et al. [2022] (underparametrized regime $n\\gg p$ , i.e. $q\\ll1$ , and overparametrized regime $n\\ll p$ , i.e. $q\\gg1,$ ), correspond to a vertical cross-section on the large $\\ell$ region of Fig. 2 (Right). Indeed, we recover exactly the rate of the label term in eqs. (167)-(168) of Maloney et al. [2022]: ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathcal{B}(f_{\\star},X,W,\\varepsilon,\\lambda)=O\\left(n^{-2\\alpha r q}\\right)=O\\left(p^{-(\\alpha-1)}\\right),\\qquad\\qquad n\\gg p}\\\\ &{}&{\\mathcal{B}(f_{\\star},X,W,\\varepsilon,\\lambda)=O\\left(n^{-2\\alpha r}\\right)=O\\left(n^{-(\\alpha-1)}\\right),\\qquad\\qquad n\\ll p}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Similarly, it is possible to recover the rates for the noise term (first two results in eq. (86) of Maloney et al. [2022]) as the vertical cross-section on the large $\\ell$ region of Figure 2 for the rates of the variance term. In particular: ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\mathcal{V}(f_{\\star},X,W,\\varepsilon,\\lambda)=\\left\\{\\overset{O}{O}\\left(\\sigma_{\\varepsilon}^{2}n^{-(1-q)}\\right)=O\\left(\\sigma_{\\varepsilon\\,\\,n}^{2\\,\\,p}\\right),\\quad n\\gg p\\quad.\\right.\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Comparison with the SGD rates from Paquette et al. [2024], Lin et al. [2024] \u2014 Furthermore, in the linear noiseless target setting, lifting the condition in eq. (217), it is possible to compare our results to the compute-optimal rates for the risk obtained through stochastic gradient descent in Paquette et al. [2024]. In particular, we consider unitary batch-size and the correspondence between the number of iterations of stochastic gradient descent and the number of samples $n$ in ridge regression. Then, defining $\\hat{\\gamma}$ the decay rate of the compute-optimal curves for the risk $\\hat{\\mathcal{R}}\\asymp n^{-\\hat{\\gamma}}$ in Paquette et al. [2024], corresponding to the compute-optimal number of features $p\\asymp\\hat{p}=:n^{\\hat{q}}$ ,6 coincides with $\\gamma_{B}(\\ell=1,\\hat{q})$ in Theorem 4.1, i.e. with fixed regularization parameter $\\lambda=1\\:(\\ell=1)$ . In particular, consider the following regions in the phase diagram provided in their work:7 ", "page_idx": 55}, {"type": "text", "text": "", "page_idx": 56}, {"type": "text", "text": "\u2022 Phase Ia $(r<1/2)$ ): ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{\\hat{q}=\\frac{1}{\\alpha},}}\\\\ {\\displaystyle{\\hat{\\gamma}=2r=2\\alpha\\hat{q}r=\\gamma_{B}(1,\\hat{q});}}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "\u2022 Phase II $(r>{^{1}\\!/}{^{\\prime}\\!2}$ and $r<1-1/2\\alpha<1)$ ): ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\hat{q}=\\frac{1+2\\alpha r-\\alpha}{\\alpha}<1,}\\\\ {\\displaystyle\\hat{\\gamma}=2r=\\frac{\\alpha-1}{\\alpha}+\\hat{q}=\\gamma_{B}(1,\\hat{q});}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "\u2022 Phase III $\\textit{r}\\!>1\\!/2$ and $r>1-{1}/{2\\alpha})$ ): ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\hat{q}=1,}\\\\ {\\hat{\\gamma}=\\displaystyle\\frac{2\\alpha-1}{\\alpha}=\\displaystyle\\frac{\\alpha-1}{\\alpha}+\\hat{q}=\\gamma_{B}(1,\\hat{q}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "We emphasize that, in Phases Ia and II, $\\hat{\\gamma}=\\operatorname*{max}_{q}\\gamma_{B}(1,q)$ , while, in Phase III, $\\hat{\\gamma}\\leq\\operatorname*{max}_{q}\\gamma_{B}(1,q)$ . Hence, the compute-optimal decay rate of the risk for stochastic gradient descent is equal or smaller than the largest rate achievable by RFRR with fixed regularization $\\lambda=1$ and therefore always smaller than the optimal one in Corollary 4.2. ", "page_idx": 56}, {"type": "text", "text": "A similar setting has been investigated by the recent work Lin et al. [2024], providing scaling laws for the excess risk obtained by stochastic gradient descent with stepsize schedule $\\bar{\\eta_{t}}\\bar{=\\eta}/2^{t\\,\\bar{\\log{n}}/n}$ , for $t=1,...,n$ .8 Under the same source and capacity conditions, assuming $r\\in(0,1/2)$ and $\\eta=O(1)$ , the result in their Theorem 4.2 may be rephrased as follows: ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{X,\\varepsilon}\\mathcal{R}(f_{\\star},X,W,\\varepsilon,\\eta)\\asymp n^{-\\gamma_{\\mathrm{SGD}}(\\eta,p)},}\\\\ &{\\gamma_{\\mathrm{SGD}}(\\eta,p)=\\left[2\\alpha r\\left(\\cfrac{1+\\log_{n}\\eta}{\\alpha}\\wedge\\log_{n}p\\right)\\right]\\wedge\\left[1-\\left(\\cfrac{1+\\log_{n}\\eta}{\\alpha}\\wedge\\log_{n}p\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Hence, choosing $p\\asymp n^{q}$ and $\\eta\\asymp n^{\\ell-1}$ , i.e. $\\eta\\asymp\\lambda^{-1}$ , provided $\\eta=O(1)\\implies\\ell<1$ , this result recovers precisely the same rates as in our Theorem 4.1. ", "page_idx": 56}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 57}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 57}, {"type": "text", "text": "Justification: All claims in the abstract and introduction are supported by mathematical proofs or numerical results. ", "page_idx": 57}, {"type": "text", "text": "Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 57}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 57}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Justification: All mathematical proofs include clear assumptions reflecting the scope of applicability. Numerical results are replicated for different data sets and choices of feature maps. ", "page_idx": 57}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 57}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 57}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Justification: Clear assumptions are provided in an \u201cassumption environment\u201d. Guidelines: ", "page_idx": 58}, {"type": "text", "text": "", "page_idx": 58}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 58}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 58}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Justification: We have included a detailed discussion of how all the numerical experiments were conducted in Appendix C. ", "page_idx": 58}, {"type": "text", "text": "Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 58}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 59}, {"type": "text", "text": "Answer: [No] ", "page_idx": 59}, {"type": "text", "text": "Justification: We judge the code is too simple to be released, and that we give enough information for the reproducibility of the numerical plots. All data sets used in the numerical experiments are either synthetic or open source. ", "page_idx": 59}, {"type": "text", "text": "Guidelines: ", "page_idx": 59}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 59}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 59}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Justification: We have added a detailed discussion of the experiments in the captions and in Appendix C. ", "page_idx": 59}, {"type": "text", "text": "Guidelines: ", "page_idx": 59}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 59}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 59}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Justification: The numerical agreement between theory and experiments is so good that error bars are unnecessary. ", "page_idx": 59}, {"type": "text", "text": "Guidelines: ", "page_idx": 59}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 59}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 60}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 60}, {"type": "text", "text": "Answer: [No] ", "page_idx": 60}, {"type": "text", "text": "Justification: The experiments are simple illustrations of the theorems. They are simple enough to be ran on a standard laptop in a a few hours. ", "page_idx": 60}, {"type": "text", "text": "Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 60}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 60}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 60}, {"type": "text", "text": "Justification: This work is of theoretical nature, and therefore has no major ethical implications. ", "page_idx": 60}, {"type": "text", "text": "Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 60}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 60}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 60}, {"type": "text", "text": "Justification: This work is of theoretical nature, and therefore has no relevant societal impacts. ", "page_idx": 60}, {"type": "text", "text": "Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 61}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 61}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 61}, {"type": "text", "text": "Justification: This work is of theoretical nature, and therefore has no risk of misusage. Guidelines: ", "page_idx": 61}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 61}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 61}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 61}, {"type": "text", "text": "Justification: All resources used from other works are properly acknowledged. Guidelines: ", "page_idx": 61}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 61}, {"type": "text", "text": "", "page_idx": 62}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 62}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 62}, {"type": "text", "text": "Justification: No new assets are introduced in this paper. ", "page_idx": 62}, {"type": "text", "text": "Guidelines: ", "page_idx": 62}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 62}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 62}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 62}, {"type": "text", "text": "Justification: This work does not involve crowdsourcing. ", "page_idx": 62}, {"type": "text", "text": "Guidelines: ", "page_idx": 62}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 62}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 62}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 62}, {"type": "text", "text": "Justification: This work is of theoretical nature and does not have potential risks to the people involved. ", "page_idx": 62}, {"type": "text", "text": "Guidelines: ", "page_idx": 62}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 62}, {"type": "text", "text": "", "page_idx": 63}]