[{"figure_path": "Oo7HY9kmK6/figures/figures_8_1.jpg", "caption": "Figure 1: The regularized training loss G\u03bb(\u03bd) (1.1) of a 2NN with the ReLU activation, learning a teacher 2NN with the 4th degree Hermite polynomial as its activation. In both plots, d = 10 and \u03bb = \u03b2\u22121 = 10\u22123. The implementation details are provided in Sec. F.4. Plots are averaged over 5 experiments. G\u2217 is the best value achieved at each experiment. In Fig. (1b), \u201cConic\u201d refers to using the metric (3.2) with qr = 1, qw = 1, while \u201cCanonical\u201d refers to the choice of qr = 2, qw = 0.", "description": "This figure compares the performance of different optimization methods for training a two-layer neural network (2NN) with a ReLU activation function.  It shows the training loss (Gx(\u03bd)) over iterations for different approaches, including using the bilevel and lifting formulations. The results highlight the superior performance of the bilevel approach, specifically when using Mean-Field Langevin Dynamics (MFLD). The figure also illustrates the impact of noise on the training process and the effect of different choices of metric in the optimization process.", "section": "5 Local LSI constant at optimality for learning a single neuron"}, {"figure_path": "Oo7HY9kmK6/figures/figures_8_2.jpg", "caption": "Figure 1: The regularized training loss Gx(v) (1.1) of a 2NN with the ReLU activation, learning a teacher 2NN with the 4th degree Hermite polynomial as its activation. In both plots, d = 10 and \u03bb = \u03b2\u22121 = 10\u22123. The implementation details are provided in Sec. F.4. Plots are averaged over 5 experiments. Gx* is the best value achieved at each experiment. In Fig. (1b), \u201cConic\u201d refers to using the metric (3.2) with qr = 1, qw = 1, while \u201cCanonical\u201d refers to the choice of qr = 2, qw = 0.", "description": "This figure compares the performance of three different methods for training a two-layer neural network (2NN): MFLD-Bilevel, MFLD-Lifting (Conic), and MFLD-Lifting (Canonical).  The x-axis represents the number of iterations, and the y-axis represents the regularized training loss, Gx(v). The results are averaged over 5 experiments. The figure shows that MFLD-Bilevel converges faster and achieves a lower training loss compared to the other two methods.", "section": "5 Local LSI constant at optimality for learning a single neuron"}]