[{"type": "text", "text": "Approximating the Top Eigenvector in Random Order Streams ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Praneeth Kacham\u2217 Google Research pkacham@google.com ", "page_idx": 0}, {"type": "text", "text": "David P. Woodruff Carnegie Mellon University dwoodruf@cs.cmu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "When rows of an $n\\times d$ matrix $A$ are given in a stream, we study algorithms for approximating the top eigenvector of the matrix $A^{\\mathsf{T}}A$ (equivalently, the top right singular vector of $A$ ). We consider worst case inputs $A$ but assume that the rows are presented to the streaming algorithm in a uniformly random order. We show that when the gap parameter $\\bar{R}=\\overline{{\\sigma}}_{1}(A)^{2}/\\sigma_{2}(A)^{2}=\\Omega\\dot{(1)}$ , then there is a randomized algorithm that uses $O(h\\cdot d\\cdot\\mathrm{polylog}(d))$ bits of space and outputs a unit vector $v$ that has a correlation $1-O(1/{\\sqrt{R}})$ with the top eigenvector $v_{1}$ . Here $h$ denotes the number of heavy rows in the matrix, defined as the rows with Euclidean norm at least $||A||_{\\mathsf{F}}/\\sqrt{d\\cdot\\mathrm{polylog}(d)}$ . We also provide a lower bound showing that any algorithm using $O(h d/R)$ bits of space can obtain at most $1-\\Omega(1/R^{2})$ correlation with the top eigenvector. Thus, parameterizing the space complexity in terms of the number of heavy rows is necessary for high accuracy solutions. ", "page_idx": 0}, {"type": "text", "text": "Our results improve upon the $R=\\Omega(\\log n\\cdot\\log d)$ requirement in a recent work of Price and Xun (FOCS 2024). We note that the algorithm of Price and Xun works for arbitrary order streams whereas our algorithm requires a stronger assumption that the rows are presented in a uniformly random order. We additionally show that the gap requirements in their analysis can be brought down to $R=\\Omega(\\log^{2}d)$ for arbitrary order streams and $R\\ =\\ \\Omega(\\log d)$ for random order streams. The requirement of $R\\,=\\,\\Omega(\\log d)$ for random order streams is nearly tight for their analysis as we obtain a simple instance with $R=\\Omega(\\log d/\\log\\log\\bar{d})$ for which their algorithm, with any fixed learning rate, cannot output a vector approximating the top eigenvector $v_{1}$ . ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We consider the problem of approximating the top eigenvector in the streaming setting. In this problem, we are given vectors $\\bar{a_{1}},\\ldots,a_{n}\\in\\bar{\\mathbb{R}}^{d}$ one at a time in a stream. Let $A$ be an $n\\times d$ matrix with rows $a_{1},\\ldots,a_{n}$ . The task is to approximate the top eigenvector of the matrix $A^{\\mathsf{T}}A$ . Throughout the paper, we use $v_{1}\\in\\mathbb{R}^{d}$ to denote the top eigenvector of $A^{\\mathsf{T}}A$ . We focus on obtaining streaming algorithms that use a small amount of space and can output a unit vector $\\hat{v}$ such that $\\langle\\hat{v},\\bar{v}_{1}\\rangle^{2}\\geq1\\bar{-}$ $f(R)$ , where $f(R)$ is a decreasing function in the gap $R=\\lambda_{1}(A^{\\top}A)/\\lambda_{2}(A^{\\top}A)$ . Here $\\lambda_{1}(\\cdot),\\lambda_{2}(\\cdot)$ denote the two largest eigenvalues. As the gap $R$ becomes larger, the eigenvector approximation problem becomes easier and we want more accurate approximations to the eigenvector $v_{1}$ . ", "page_idx": 0}, {"type": "text", "text": "If one is allowed to use $\\tilde{O}(d^{2})^{2}$ bits of space, we can maintain the matrix $\\begin{array}{r}{A^{\\mathsf{T}}A=\\sum_{i}a_{i}a_{i}^{\\mathsf{T}}}\\end{array}$ as we see the rows $a_{i}$ in the stream, and at the end of processing the stream, we can co mpute the exact top eigenvector $v_{1}$ . When the dimension $d$ is large, the requirement of $\\Omega(d^{2})$ bits of memory can be impractical (see e.g., applications that require a large value of $d$ in Mitliagkas et al. (2013).) Hence, an interesting question is to study non-trivial streaming algorithms that use less memory. In this work, we focus on obtaining algorithms that use $\\tilde{O}(d)$ bits of space. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In the offline setting (where the entire matrix $A$ is available to us), fast iterative algorithms such as Gu (2015); Musco and Musco (2015); Musco et al. (2018) can be used to quickly obtain accurate approximations to the top eigenvector when the gap $R=\\Omega(1)$ . In a single pass streaming setting, we cannot run these algorithms as these iterative algorithms need to see the entire matrix multiple times. ", "page_idx": 1}, {"type": "text", "text": "There have been two major lines of work studying the problem of eigenvector approximation and the related Principal Component Analysis (PCA) problem in the streaming setting with near-linear in $d$ memory. In the first line of work, each row encountered in the stream is assumed to be sampled independently from an unknown distribution with mean 0 and covariance $\\Sigma$ and the task is to approximate the top eigenvector of $\\Sigma$ using the samples. In this line of work, the sample complexity required for algorithms using $O(d\\cdot\\mathrm{polylog}(d))$ bits of space to output an approximation to $v_{1}$ , is the main question. The algorithms are usually a variant of Oja\u2019s algorithm (Oja, 1982; Jain et al., 2016; Allen-Zhu and Li, 2017; Huang et al., 2021; Kumar and Sarkar, 2023) or the block power method (Hardt and Price, 2014; Balcan et al., 2016). We note that Kumar and Sarkar (2023) relax the i.i.d. assumption and analyze the sample complexity of Oja\u2019s algorithm for estimating the top eigenvector in the Markovian data setting. ", "page_idx": 1}, {"type": "text", "text": "The other line of work studies algorithms for arbitrary streams appearing in an arbitrary order. In this setting, we want algorithms to work for any input stream given in any order. A problem closely related to the eigenvector estimation problem is the Frobenius-norm Low Rank Approximation (Clarkson and Woodruff, 2017; Boutsidis et al., 2016; Upadhyay, 2016; Ghashami et al., 2016). The deterministic Frequent Directions sketch of Ghashami et al. (2016) can, using $\\tilde{O}(d/\\varepsilon)$ bits of space, output a unit vector $u$ such that ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|A(I-u u^{\\mathsf{T}})\\|_{\\mathsf{F}}^{2}\\leq(1+\\varepsilon)\\|A(I-v_{1}v_{1}^{\\mathsf{T}})\\|_{\\mathsf{F}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Although the vector $u$ is a $1+\\varepsilon$ approximate solution to the Frobenius norm Low Rank Approximation problem, it is possible that the vector $u$ may be (nearly) orthogonal to the top eigenvector $v_{1}$ . Hence the Frequent Directions sketch does not guarantee top eigenvector approximation. Recently, Price and Xun (2024) study the eigenvector approximation problem in arbitrary streams and obtain results in terms of the gap $R$ of the instance. Price and Xun prove that when $R=\\Omega(\\log n\\cdot\\log d)$ , a variant of Oja\u2019s algorithm outputs a unit vector $\\hat{v}$ such that ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\langle\\hat{v},v_{1}\\rangle^{2}\\ge1-\\frac{C\\log d}{R}-\\frac{1}{\\mathrm{poly}(d)}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $C$ is a large enough universal constant. On the lower bound side, Price and Xun showed that any algorithm that outputs a vector $\\hat{v}$ satisfying ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\langle\\hat{v},v_{1}\\rangle^{2}\\geq1-\\frac{1}{C R^{2}},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "must use $\\Omega(d^{2}/R^{3})$ bits of space while processing the stream. This lower bound shows that in the important case of $R=O(1)$ , the correlation3 that can be obtained by an algorithm using $\\tilde{O}(d)$ bits of space is at most a constant less than 1. Thus, the current best algorithms for arbitrary streams work only when $R=\\Omega(\\log n\\cdot\\log d)$ and for the important case of $\\bar{R_{\\L}}=O(1)$ , there are no existing algorithms requiring significantly fewer than $d^{2}$ bits of memory. They also give a lower bound on the size of mergeable summaries for approximating the top eigenvector. ", "page_idx": 1}, {"type": "text", "text": "We identify an instance with $R=\\Theta(\\log d/\\log\\log d)$ where the algorithm of Price and Xun fails to produce a vector with even a constant correlation with the vector $v_{1}$ . This shows that their algorithm or other variants of Oja\u2019s algorithm may fail to extend to the case when $R=O(1)$ . We further show that the algorithm of Price and Xun fails to produce such a vector even when the rows in our hard instance are ordered uniformly at random, showing that even randomly ordered streams can be hard to solve for variants of Oja\u2019s algorithm. ", "page_idx": 1}, {"type": "text", "text": "In this work, we focus on algorithms that work on worst case inputs $A$ while assuming that the rows of $A$ are uniformly randomly ordered. This model is mid-way between the i.i.d. setting and the arbitrary order stream setting in terms of the generality of streams that can be modeled. We note that a number of works (Munro and Paterson, 1980; Guha et al., 2005; Chakrabarti et al., 2008; Guha and McGregor, 2009; Assadi and Sundaresan, 2023) have previously considered streaming algorithms and lower bounds for worst case inputs with random order streams, as it is a natural model often arising in practical settings. Our algorithms are parameterized in terms of the number of heavy rows in the stream. See Gupta and Singla (2021) for a gentle introduction to the random-order model. We define a row $a_{i}$ to be heavy if $\\lVert a_{i}\\rVert_{2}\\geq\\lVert A\\rVert_{\\mathsf{F}}/\\sqrt{d\\cdot\\mathrm{polylog}(d)}$ . Note that in any stream of rows, by definition, there are at most $d\\cdot\\mathrm{polylog}(d)$ heavy rows. We state our theorem informally below: ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Theorem 1.1. Let $a_{1},\\ldots,a_{n}\\;\\in\\;\\mathbb{R}^{d}$ be a randomly ordered stream and let $A$ denote the $n\\,\\times\\,d$ matrix with rows given by $a_{1},\\ldots,a_{n}$ . If $\\mathit{\\Pi}^{\\mathrm{\\Pi}}{\\mathit{R}}=\\lambda_{1}(A^{\\dagger}\\dot{A})/\\lambda_{2}(A^{\\mathsf{T}}A)>C$ for a large enough constant $C$ and the number of heavy rows in the stream is at most $h$ , then there is a streaming algorithm using $O(h\\cdot d\\cdot\\mathrm{polylog}(d))$ bits of space and outputting a unit vector $\\hat{v}$ satisfying ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\langle\\hat{v},v_{1}\\rangle^{2}\\ge1-O(1/\\sqrt{R})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "with a probability $\\geq4/5$ . ", "page_idx": 2}, {"type": "text", "text": "Our algorithm is a variant of the block power method. Along the way, we also improve the gap requirements in the results of Price and Xun (2024). We show that by subsampling a stream of rows, the algorithm of Price and Xun can be made to work even when the gap $R$ is $\\Omega(\\bar{\\log}^{2}\\,d)$ in arbitrary order streams, improving on the $\\Omega(\\log n\\cdot\\log d)$ requirement in their analysis. We also show that in random order streams, a gap of $\\Omega(\\log d)$ is sufficient for their algorithm, though our algorithm improves on this and works for even a constant gap. ", "page_idx": 2}, {"type": "text", "text": "Similar to the lower bound of Price and Xun, we show that any algorithm for random order streams must use $\\Omega(h\\cdot d/R)$ bits of space to output a vector $\\hat{v}$ satisfying $\\tilde{\\langle v,v_{1}\\rangle}^{2}\\geq1-1/C R^{2}$ where $C$ is a constant. We summarize the theorem below. ", "page_idx": 2}, {"type": "text", "text": "Theorem 1.2. Consider an arbitrary random order stream $a_{1},\\ldots,a_{n}$ with the gap parameter $\\frac{\\sigma_{1}(A)^{2}}{\\sigma_{2}(A)^{2}}~=~R.$ . Let $h$ be the number of heavy rows in the stream. Any streaming algorithm that outputs a unit vector $\\hat{v}$ such that ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\langle\\hat{v},v_{1}\\rangle^{2}\\geq1-1/C R^{2}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "for a large enough constant $C$ , with a probability $\\geq1-(1/2)^{R+1}$ over the ordering of the stream and its internal randomness, must use $\\Omega(h\\cdot d/R)$ bits of space. ", "page_idx": 2}, {"type": "text", "text": "Techniques. The randomized power method (Gu, 2015) algorithm to approximate the top eigenvector samples a random Gaussian vector $\\textbf{\\textit{g}}$ and iteratively computes the vector $v=(A^{\\mathsf{T}}{\\dot{A}})^{t}g^{\\bar{4}}$ for $t=\\Theta(\\log d)$ iterations and shows that when the gap $R$ is large, $\\bar{v}/\\|v\\|_{2}$ is a good approximation for $v_{1}$ . Thus, the algorithm needs to see the quadratic form $A^{\\mathsf{T}}A$ multiple times and hence, it cannot be implemented in the single-pass streaming setting of this paper. ", "page_idx": 2}, {"type": "text", "text": "Assume that the stream is randomly ordered and that there are no heavy rows. Our key observation is that if the stream is long enough, then we can see $t$ approximations $\\dot{B}_{j}^{\\top}B_{j}^{\\mathrm{~5~}}$ of the quadratic form $A^{\\mathsf{T}}A$ . Here the matrices $B_{1},\\ldots,B_{t}$ are formed by sampling and rescaling the rows of the matrix $A$ and importantly, the rows of $B_{1},\\ldots,B_{t}$ do not overlap in the stream, that is, they appear one after the other. Thus we can compute $v^{\\prime}=(B_{t}^{\\mathsf{T}}B_{t})\\cdot\\cdot\\cdot(B_{1}^{\\mathsf{T}}B_{1})\\cdot g$ for the starting vector $\\textbf{\\textit{g}}$ in a single pass over the stream. We prove that such matrices $B_{j}$ exist using the row norm sampling result of Magdon-Ismail (2010). Now, the main issue is to show that $v^{\\prime}/\\|\\bar{v^{\\prime}}\\|_{2}$ is a good approximation to the top eigenvector $v_{1}$ . We crucially use a singular value inequality of Wang and Xi (1997) to prove that $\\|B_{j}^{\\top}B_{j}-A^{\\top}A\\|_{2}\\leq\\varepsilon\\|A\\|_{2}^{2}$ for all $j$ suffices for $v^{\\prime}/\\|v^{\\prime}\\|_{2}$ to be a good approximation to $v_{1}$ . ", "page_idx": 2}, {"type": "text", "text": "The above analysis assumes that there are no heavy rows. Indeed, suppose that a matrix $A$ has a row $a$ with a large Euclidean norm which is orthogonal to all the other rows. Also assume that the top eigenvector of the matrix $A$ is in this direction. Since, the matrices $B_{1},\\ldots,B_{t}$ are non-overlapping substreams of the matrix $A$ , at most one of the matrices $B_{j}$ can have the row $a$ and hence the vector $v^{\\prime}/\\Vert v^{\\prime}\\Vert_{2}$ will not be a good approximation to $a/\\Vert a\\Vert_{2}$ , the top eigenvector. Thus, we need to handle the heavy rows separately. We show that, by storing all the rows with a Euclidean norm larger than $||A||_{\\mathsf{F}}/\\sqrt{d\\cdot\\mathrm{polylog}(d)}$ and running the above described algorithm on the remaining set of rows, we can obtain a good approximation to the top eigenvector. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Our lower bound (Theorem 1.2) shows that any single-pass streaming algorithm must use space proportional to the number of heavy rows, and therefore our procedure that handles the heavy rows separately gives near-optimal bounds. ", "page_idx": 3}, {"type": "text", "text": "Finally, the row norm sampling technique of Magdon-Ismail (2010) serves as a general technique to reduce the number of rows in the stream while (approximately) preserving the top eigenvector. We use this observation to improve the $R\\,=\\,\\Omega(\\log n\\cdot\\log d)$ for arbitrary streams in Price and Xun (2024) to $R=\\Omega(\\log^{2}d)$ . We then show that assuming a uniformly random order, the analysis of Price and Xun (2024) can be improved to show that $R\\,=\\,\\Omega(\\log d)$ suffices. Thus, for random order streams, techniques before our work can be used to approximate the top eigenvector when the gap $R=\\Omega(\\log d)$ . Our work improves upon this to give an algorithm that works for streams with ${\\cal R}=\\Omega(1)$ . ", "page_idx": 3}, {"type": "text", "text": "Implications to practice. Often, in practical situations, we can assume that the rows being streamed are sampled independently from a nice-enough distribution, in which case Oja\u2019s algorithm, as discussed, can approximate the top eigenvector accurately given enough samples. However, independence and assumptions on the covariance matrix can be very strong assumptions in some cases and in such cases, our algorithm only requires that the order of the rows in the stream be uniformly random, in which case we output an approximation with provable guarantees. ", "page_idx": 3}, {"type": "text", "text": "Organization. We first introduce the row-norm sampling procedure to obtain approximate quadratic forms. The proof is a slight modification of that of Magdon-Ismail (2010). The only difference is that we instead consider a version that samples each row in the input independently with some appropriate probability and keeps the rows that are sampled after scaling appropriately. We then introduce and analyze our block power iteration algorithm when all rows have roughly the same Euclidean norm, and then extend it to the general case, which is our main result. Finally, we provide a lower bound showing that $\\Omega(t d/R)$ bits of space is necessary to obtain constant correlation with the top eigenvector. Due to space constraints, all of our proofs are placed in the appendix. ", "page_idx": 3}, {"type": "text", "text": "2 Power Method with Approximate Quadratic Forms ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we present and analyze our algorithm for approximating the top eigenvector of $A^{\\mathsf{T}}A$ when the rows of $A$ are presented to the algorithm in a uniformly random order. ", "page_idx": 3}, {"type": "text", "text": "We first show a row sampling technique that reduces the number of rows in the stream. The rownorm sampling technique for approximating the quadratic form $A^{\\mathsf{T}}A$ with spectral norm guarantees was given by Magdon-Ismail (2010). The technique works irrespective of the order of the rows. ", "page_idx": 3}, {"type": "text", "text": "2.1 Sampling for Row Reduction ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Theorem 2.1. Let $A$ be an arbitrary $n\\times d$ matrix. Given $p\\in[0,1]^{n}$ , let $Q$ be an $n\\times n$ diagonal matrix such that for each $i\\;\\in\\;[n]$ , we independently set $Q_{i i}\\,=\\,1/\\sqrt{p_{i}}$ with probability $p_{i}$ and 0 otherwise. If for all $i$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\np_{i}\\geq\\operatorname*{min}\\left(1,C\\frac{\\|a_{i}\\|_{2}^{2}}{\\varepsilon^{2}\\|A\\|_{2}^{2}}\\log d\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "then with probability $1-1/\\mathrm{poly}(d)$ , $\\|A^{\\mathsf{T}}A-A^{\\mathsf{T}}Q^{\\mathsf{T}}Q A\\|_{2}\\leq\\varepsilon\\|A\\|_{2}^{2}$ . With probability at least $1-1/\\operatorname{poly}(d)$ , the matrix $Q$ has at most $O(\\varepsilon^{-2}\\rho\\log d)$ non-zero entries, where $\\rho=\\|\\dot{A}\\|_{\\mathsf{F}}^{2}/\\|A\\|_{2}^{2}$ denotes the stable rank of matrix $A$ . ", "page_idx": 3}, {"type": "text", "text": "Note that given the value of $\\Vert A\\Vert_{2}$ , the sampling procedure in this theorem can be performed in a stream. Additionally, as the original stream is uniformly randomly ordered, the sub-sampled stream is also uniformly randomly ordered assuming that the sampling is independent of the order of the rows. ", "page_idx": 3}, {"type": "text", "text": "Given that all of the non-zero entries of the matrix have absolute value at least $1/\\operatorname{poly}(n d)$ and at most $\\mathrm{poly}(n d)$ , we have that $\\|A\\|_{2}^{2}$ lies in the interval $[1/\\,\\mathrm{poly}(n d),\\mathrm{poly}(n d)]$ . Thus, we can guess the value of $\\|\\dot{A}\\|_{2}^{2}$ as $2^{i}/\\operatorname{poly}(n{\\ddot{d}})$ for $i\\,=\\,0,\\ldots,O(\\log(n d))$ and one of these values must be a 2-approximation to $\\|A\\|_{2}^{2}$ , and thus sub-sampling the rows using that guess satisfies the conditions in the above theorem. We can run the streaming algorithms on all the streams simultaneously to obtain $O(\\log n d)$ vectors $u_{1},\\ldots,u_{O(\\log n d)}$ as the candidates for being an approximation to the top eigenvector. From Theorem 2.1, the candidate vector $u_{j}$ computed on the stream obtained by sampling the rows with the correct probabilities is a good approximation to the top eigenvector, and therefore $\\|A\\cdot u_{j}\\|_{2}$ is large for that value of $j$ . Thus, the vector $u_{j}$ with the largest value $\\lVert A\\cdot u_{j}\\rVert_{2}$ is a good approximation to the top eigenvector $v_{1}$ . If $\\boldsymbol{G}$ is a Gaussian matrix with $O(\\varepsilon^{-2}\\log d)$ rows, then for all $u_{j}$ , we can approximate $\\lVert A\\cdot u_{j}\\rVert_{2}$ up to a $1\\pm\\varepsilon$ factor using $\\|\\pmb{G}\\cdot\\pmb{A}\\cdot\\pmb{u}_{j}\\|_{2}$ by the Johnson-Lindenstrauss lemma. Additionally, the matrix $G\\cdot A$ can be maintained in the stream using $O(\\varepsilon^{-2}\\cdot d\\log d)$ bits (when we see a row $a_{i}$ , we sample an independent Gaussian vector $\\pmb{g}_{i}$ and add $\\pmb{g}_{i}a_{i}^{\\mathsf{T}}$ to an accumulator to maintain $G\\cdot A)$ . Thus, at the end of processing the stream, we can compute a vector $u_{j}$ that has a large value $\\|A\\cdot u_{j}\\|_{2}$ , and hence is a good approximation for $v_{1}$ . ", "page_idx": 4}, {"type": "text", "text": "If we can process each created stream using $s$ bits of space, then the overall space requirement is $O(s\\cdot\\log(n d)+d\\cdot\\mathrm{polylog}(d))$ bits, using $O(s)$ bits for each guess for the value of $\\|A\\|_{2}^{2}$ and $O(d\\cdot\\mathrm{polylog}(d))$ bits for storing a Gaussian sketch of the matrix with $\\varepsilon=1/\\operatorname{polylog}(d)$ . ", "page_idx": 4}, {"type": "text", "text": "2.2 Random-Order Streams with bounds on Norms ", "text_level": 1, "page_idx": 4}, {"type": "image", "img_path": "gITGmIEinf/tmp/164964b6c87fbbcacda9e6377157e0eb3b6efb1793d39361b64bf9b635a27de1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "We now present the analysis of the block power method for random order streams assuming that the Euclidean norms of all the rows in $A$ are close to each other. We later remove this assumption. Suppose there exists a parameter $\\eta$ such that $(\\operatorname*{max}_{i}\\|a_{i}\\|_{2}^{2})/(\\operatorname*{min}_{i}\\|a_{i}\\|_{2}^{2})\\leq\\eta$ . If $\\eta$ is close to 1 then all the rows in the stream have roughly the same norm. ", "page_idx": 4}, {"type": "text", "text": "Let $p=C\\eta\\rho\\log(d)/\\varepsilon^{2}n.$ . We can see that for any row $a_{i}$ in the stream, ", "page_idx": 4}, {"type": "equation", "text": "$$\nC\\frac{\\|a_{i}\\|_{2}^{2}}{\\varepsilon^{2}\\|A\\|_{2}^{2}}\\log d\\leq C\\frac{\\eta\\|A\\|_{\\mathsf{F}}^{2}/n}{\\varepsilon^{2}\\|A\\|_{2}^{2}}\\log d\\leq\\frac{C\\eta\\rho\\log d}{n\\varepsilon^{2}}=p.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Thus, $p$ is greater than the probability with which we need to sample each row in the row-norm sampling result in Theorem 2.1. Now if we perform such a sampling of the rows of $A$ , we sample $\\mathrm{Bin}(n,p)^{6}$ number of rows, which is tightly concentrated around $n\\bar{p}=\\varepsilon^{-2}C\\eta\\rho\\log d$ . Thus, if we first sample $\\pmb{y}\\sim\\mathrm{Bin}(n,p)$ and then consider the first $\\textit{\\textbf{y}}$ number of rows in the random order stream, then we will have sampled from a distribution satisfying the requirements in Theorem 2.1 and can therefore obtain a matrix $_B$ such that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\boldsymbol{B}^{\\top}\\boldsymbol{B}-\\boldsymbol{A}^{\\top}\\boldsymbol{A}\\|_{2}\\leq\\varepsilon\\|\\boldsymbol{A}\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Thus, assuming that the rows appear in a uniformly random order lets us show that the first $\\textit{\\textbf{y}}$ rows of the stream can be used to compute an approximation to the quadratic form $A^{\\mathsf{T}}A$ . We will now show that we can obtain $O(\\log d)$ such quadratic forms in the stream given that the stream is long enough. ", "page_idx": 5}, {"type": "text", "text": "Assume that the number of rows in the stream $n=\\Omega(\\eta\\rho\\log^{2}d/\\varepsilon^{2})$ . We partition the stream into $t=\\Theta(\\log d)$ groups as follows: the first $2n p$ rows are placed in the group 1, the second $2n p$ rows are placed in the group 2, and so on. Note that since $n\\,=\\,\\Omega(\\eta\\rho\\log^{2}d/\\varepsilon^{2})$ , we can form $t$ such groups. Since the rows are uniformly randomly ordered, the joint distribution of the rows appearing in group 1 is the same as that of the joint distribution of the rows appearing in group 2 and so on. Let $\\pmb{y}_{1},\\bar{\\mathbf{\\alpha}}\\cdot\\bar{\\mathbf{\\alpha}},\\pmb{y}_{t}\\sim\\mathrm{Bin}(n,p)$ be drawn independently. With probability $\\ge1-1\\bar{/}\\,\\mathrm{poly}(d)$ , we have $y_{i}\\leq(3/2)n p$ for all $i$ . For $i=1,\\dots,t$ , let $B_{i}$ be the matrix formed by the first $\\pmb{y}_{i}$ rows in group $i$ . Using a union bound, we have that with probability $\\ge1-1/\\mathrm{poly}(d)$ , for all $i=1,\\dots,t$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\|A^{\\mathsf{T}}A-{\\frac{1}{p}}B_{i}^{\\mathsf{T}}B_{i}\\|_{2}\\leq\\varepsilon\\|A\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Conditioned on the above event, we will now show that running the power method on the blocks $B_{1},\\ldots,B_{t}$ lets us approximate the top singular vector of the matrix $A$ . ", "page_idx": 5}, {"type": "text", "text": "Assumption 2.2. We assume that $\\sigma_{1}(A)/\\sigma_{2}(A)\\geq2$ . ", "page_idx": 5}, {"type": "text", "text": "Lemma 2.3. Let $\\varepsilon\\,>\\,1/\\operatorname{poly}(d)$ be an accuracy parameter and $t\\,=\\,\\Omega(\\log d)$ be the number of iterations. Let $\\varepsilon\\le c/t^{2}$ for $a$ small constant $c,$ . Suppose $B_{1},\\ldots,B_{t}$ all satisfy $\\|\\boldsymbol{A}^{\\top}\\boldsymbol{A}-\\boldsymbol{B}_{j}^{\\top}\\boldsymbol{B}_{j}\\|_{2}\\leq$ $\\varepsilon\\|A\\|_{2}^{2}$ for $\\varepsilon<1/5$ . If $\\textbf{\\textit{g}}$ is a random vector sampled from the Gaussian distribution, then the unit vector ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{v}:=\\frac{(B_{t}^{\\top}B_{t})\\cdot\\cdot\\cdot(B_{1}^{\\top}B_{1})g}{\\|(B_{t}^{\\top}B_{t})\\cdot\\cdot\\cdot(B_{1}^{\\top}B_{1})g\\|_{2}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "satisfies ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\langle{\\hat{v}},v_{1}\\rangle^{2}\\geq{\\frac{1}{1+C^{\\prime}t\\sqrt{\\varepsilon}}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with probability $\\geq\\,9/10$ for a large enough constant $C^{\\prime}$ . Here $v_{1}$ denotes the top right singular vector of the matrix $A$ . ", "page_idx": 5}, {"type": "text", "text": "To prove this lemma, our strategy is to show that the matrix product $M:=(B_{t}^{\\top}B_{t})\\cdot\\cdot\\cdot(B_{1}^{\\top}B_{1})$ has a stable rank close to 1 \u2014 meaning it has one very large singular value and the rest of the singular values are small. We can then argue that the vector $\\hat{v}\\bar{=}\\,M\\bar{\\pmb g}/\\|M\\pmb g\\|_{2}$ is in the direction of the top singular vector $M$ . Using the fact that $v_{1}^{\\mathsf{T}}(B_{j}^{\\mathsf{T}}B_{j})v_{1}\\geq(1-\\varepsilon)\\|A\\|_{2}^{2}$ for all $j$ , we show that the top singular vector of $M$ must have a large correlation with $v_{1}$ . Therefore, it follows that the vector $\\hat{v}$ has a large correlation with $v_{1}$ as well. As part of the proof, we crucially use an inequality from Wang and Xi (1997). ", "page_idx": 5}, {"type": "text", "text": "If $t=\\Theta(\\log d)$ and $1/\\operatorname{poly}(d)\\,\\leq\\,\\varepsilon\\,\\leq\\,c/(\\log d)^{2}$ , then the above lemma shows that $\\hat{v}$ has a large correlation with the top singular vector $v_{1}$ . Using this lemma, we show that Algorithm 1 can be used to obtain an approximation for $v_{1}$ in random order streams with bounded norms. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2.4. Let $\\alpha\\,\\geq\\,1/\\operatorname{poly}(d)$ be an accuracy parameter. Let $\\eta$ be a parameter such that $\\begin{array}{r}{\\frac{\\operatorname*{max}_{i}\\|a_{i}\\|_{2}^{2}}{\\operatorname*{min}_{i}\\|a_{i}\\|_{2}^{2}}\\leq\\eta.}\\end{array}$ . If the number of rows in the stream $n=\\Omega(\\alpha^{-4}\\cdot\\rho(A)\\cdot\\eta\\cdot\\log^{6}d)$ , where $\\rho(A)=$ $\\lVert A\\rVert_{\\mathsf{F}}^{2}/\\lVert A\\rVert_{2}^{2}$ and the rows in the stream are ordered uniformly at random, then we can compute a vector $\\hat{v}$ using the block power method that satisfies ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n|\\langle v_{1},\\hat{v}\\rangle|^{2}\\geq1-3\\alpha\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "with probabilit $\\nu\\geq4/5\\,i f\\sigma_{1}(A)/\\sigma_{2}(A)\\geq2.$ . The algorithm uses $O(d\\!\\cdot\\!\\mathrm{polylog}(d)/\\alpha^{4})$ bits of space. ", "page_idx": 6}, {"type": "text", "text": "Proof. Set $\\varepsilon\\;=\\;\\alpha^{2}/C\\log^{2}d$ for a large enough constant $C$ . Assuming $n\\,=\\,\\Omega(\\alpha^{-4}\\rho\\eta\\log^{6}d)$ , we have $n\\;=\\;\\Omega(\\varepsilon^{-2}\\rho\\eta\\log^{2}d)$ . Now consider the execution of Algorithm 1 on matrix $A$ , with parameters $\\eta$ and $\\varepsilon$ . Let $\\rho=2^{j}$ be such that $\\rho(A)/2\\le\\rho\\le\\rho(A)$ , and consider the execution in the algorithm with parameter $\\rho$ . Using Theorem 2.1, with probability $\\ge1-1/\\mathrm{poly}(d)$ , the algorithm computes $t$ matrices $B_{1},\\ldots,B_{t}$ such that for all $j\\in[t]$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\|{\\frac{1}{p}}B_{j}^{\\mathsf{T}}B_{j}-A^{\\mathsf{T}}A\\|_{2}\\leq\\varepsilon\\|A\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Noting that $z_{\\rho}=(B_{t}^{\\top}B_{t})\\cdot\\cdot\\cdot(B_{1}^{\\top}B_{1})g/\\|(B_{t}^{\\top}B_{t})\\cdot\\cdot\\cdot(B_{1}^{\\top}B_{1})g\\|_{2}$ , by Lemma 2.3, we have with probability $\\geq9/10$ that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\langle z_{\\rho},v_{1}\\rangle^{2}\\geq\\frac{1}{1+C^{\\prime}t\\sqrt{\\varepsilon}}\\geq1-\\alpha.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Thus, for $\\rho$ which satisfies $\\rho(A)/2\\le\\rho\\le\\rho(A)$ , the algorithm computes a vector $z_{\\rho}$ that has a large correlation with the vector $v_{1}$ . Since the algorithm does not know the exact value of $\\rho$ , it computes an approximation for $\\|A z\\|_{2}^{2}$ for all $\\boldsymbol{z}\\in\\{\\,z_{1},z_{2},z_{4},\\dots,z_{d}\\,\\}$ . First, we condition on the fact that with probability $\\ge1-1/\\mathrm{poly}(d)$ , for all $\\boldsymbol{z}_{i}$ , $\\|G A z_{i}\\|_{2}^{2}=(1\\pm\\varepsilon)\\|A z_{i}\\|_{2}^{2}$ . Since $\\langle z_{\\rho},v_{1}\\rangle^{2}\\geq(1-\\alpha).$ , we note that $||G A z_{\\rho}||_{2}^{2}\\geq(1-\\varepsilon)(1-\\alpha)\\sigma_{1}(A)^{2}$ . Now, for the vector $_{\\textit{z}}$ returned by the algorithm, we have $\\|A z\\|_{2}^{2}\\geq(1-O(\\varepsilon))(1-\\alpha)\\sigma_{1}(A)^{2}$ which implies that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\langle z,v_{1}\\rangle^{2}\\cdot\\sigma_{1}(A)^{2}+(1-\\langle z,v_{1}\\rangle^{2})\\frac{\\sigma_{1}(A)^{2}}{R}\\geq\\|A z\\|_{2}^{2}\\geq(1-\\alpha-O(\\varepsilon))\\sigma_{1}(A)^{2}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and therefore $\\langle z,v_{1}\\rangle^{2}\\geq1-3\\alpha$ since $R\\geq2$ . ", "page_idx": 6}, {"type": "text", "text": "2.3 Random Order Streams without Norm Bounds ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Assuming that the random order streams are long enough, Theorem 2.4 shows that if all the squared row norms are within an $\\eta$ factor, then the block power method outputs a vector with a large correlation with the top eigenvector of the matrix $A^{\\mathsf{T}}{\\dot{A}}$ . For general streams, the factor $\\eta$ could be quite large and hence the algorithm requires very long streams to output an approximation to $v_{1}$ . ", "page_idx": 6}, {"type": "text", "text": "If there are no heavy rows, i.e., rows with a Euclidean norm larger than $\\|A\\|_{\\mathsf{F}}/\\sqrt{d\\cdot\\mathrm{polylog}(d)}.$ , then the row norm sampling procedure in Theorem 2.1 can be used to convert any randomly ordered stream of rows into a uniformly random stream of rows that all have the same norm. The row norm sampling procedure computes a probability $p_{i}=\\operatorname*{min}(1,C\\varepsilon^{-2}\\|a_{i}\\|_{2}^{2}\\log d/\\|A\\|_{2}^{2})$ and samples the row $a_{i}$ with probability $p_{i}$ . If sampled, then the row $a_{i}$ is scaled by $1/\\sqrt{p_{i}}$ . From Theorem 2.1, we have that the top eigenvector of the quadratic form of the sampled-and-rescaled submatrix is a good approximation to the top eigenvector $A^{\\mathsf{T}}A$ when the gap $R$ is large enough. Suppose $p_{i}<1$ . If the row $a_{i}$ is sampled, we then have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\|a_{i}/\\sqrt{p_{i}}\\|_{2}=\\frac{\\varepsilon\\|A\\|_{2}}{\\sqrt{C\\log d}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Thus, if $p_{i}<1$ for all $i$ , then all the sampled-and-rescaled rows have the same Euclidean norm and therefore, we can run the algorithm from Theorem 2.4 by setting $\\eta=1$ . Note that $p_{i}=1$ only if $\\|a_{i}\\|_{2}^{2}\\,\\geq\\,\\varepsilon^{2}\\|A\\|_{2}^{2}/C\\log(d)$ . Since we assumed that there are no heavy rows, there is no row with $p_{i}=1$ as long as $\\varepsilon\\geq1/\\operatorname{polylog}(d)$ . Thus, using Theorem 2.4 on the row norm sampled substream directly gives us a good approximation to the top eigenvector. However, in general, the streams can have rows with large Euclidean norm. We will now state our theorem and describe how such streams can be handled. ", "page_idx": 6}, {"type": "text", "text": "Theorem 2.5. Let $A$ be an $n\\times d$ matrix with its non-zero entries satisfying $1/\\mathrm{poly}(d)\\leq|A_{i,j}|\\leq$ $\\mathrm{poly}(d)$ , and hence representable using $O(\\log d)$ bits of precision. Let $R~=~\\sigma_{1}(A)^{2}/\\sigma_{2}(A)^{2}$ . Assume $\\begin{array}{r l r}{2}&{{}\\leq}&{R}&{\\leq\\ C_{1}\\log^{2}d.}\\end{array}$ . Let $h$ be the number of rows in $A$ with norm at most $||A||_{\\mathsf{F}}/\\sqrt{d\\cdot\\mathrm{polylog}(d)}$ , where $\\mathrm{\\polylog}(d)~=~\\mathrm{\\log}^{C_{2}}\\,d$ for a large enough universal constant $C_{2}$ . Given the rows of the matrix $A$ in a uniformly random order, there is an algorithm using $O((h{+}1)\\!\\cdot\\!d\\!\\cdot\\!\\mathrm{polylog}(d)\\!\\cdot\\!\\log n)$ bits o\u221af space and which outputs a vector $\\hat{v}$ such that with probability $\\geq4/5,$ , $\\hat{v}$ satisfies $\\langle\\hat{v},v_{1}\\rangle^{2}\\geq1-8/\\sqrt{R},$ , where $v_{1}$ is the top eigenvector of the matrix $A^{\\mathsf{T}}A$ . ", "page_idx": 7}, {"type": "text", "text": "The key idea in proving this theorem is to partition the matrix $A$ into $A_{\\mathrm{heavy}}$ and $A_{\\mathrm{light}}$ , where $A_{\\mathrm{heavv}}$ denotes the matrix with the heavy rows and $A_{\\mathrm{light}}$ denotes the matrix with the rest of the rows of $A$ . Since we assume that there are at most $h$ heavy rows, we can store the matrix $A_{\\mathrm{heavv}}$ using ${\\cal O}(h\\cdot$ $d\\cdot\\mathrm{polylog}(d))$ bits of space. Now consider the following two cases: (i) $\\|A_{\\mathrm{heavy}}\\|_{2}\\geq\\tilde{(1-\\beta)}\\|A\\|_{2}$ or (ii) $\\|A_{\\mathrm{heavy}}\\|_{2}<(1-\\beta)\\|A\\|_{2}$ for some parameter $\\beta$ . In the first case, we can show that the top eigenvector $u$ of $A_{\\mathrm{heavy}}^{\\mathsf{T}}A_{\\mathrm{heavy}}$ is a good approximation for $v_{1}$ . Since, we store the full matrix $A_{\\mathrm{heavy}}$ , we can compute $u$ exactly at the end of the stream. Suppose $\\|A_{\\mathrm{heavy}}\\|_{2}\\,<\\,(1\\,-\\,\\beta)\\|A\\|_{2}$ . By the triangle inequality, we have $\\|A_{\\mathrm{light}}\\|_{2}>\\beta\\|A\\|_{2}.$ . If we set $\\beta$ large enough compared to $1/R$ , then we can show that the top eigenvector $u^{\\prime}$ of $A_{\\mathrm{light}}^{\\mathsf{T}}A_{\\mathrm{light}}$ is a good approximation of $v_{1}$ . From the above discussion, since all the rows of $A_{\\mathrm{light}}$ are light, we can obtain a stream using Theorem 2.1 such that all the rows have the same norm and additionally, the top eigenvector of this stream is a good approximation for $u^{\\prime}$ and therefore $v_{1}$ . We then approximate the top eigenvector of the new stream using Theorem 2.4. Setting $\\beta$ appropriat\u221aely, we show that this procedure can be used to compute a vector $\\hat{v}$ satisfying $\\langle\\hat{v},v_{1}\\rangle^{2}\\ge1-O(1/\\sqrt{R})$ proving the theorem. ", "page_idx": 7}, {"type": "text", "text": "3 Lower Bounds ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Our algorithm uses ${\\tilde{O}}(h\\cdot d)$ space when the number of heavy rows in the stream is $h$ . We want to argue that it is nearly tight. We show the following theorem. ", "page_idx": 7}, {"type": "text", "text": "Theorem 3.1. Given a dimension $d_{:}$ , let h and $R$ be arbitrary with $R\\leq h\\leq d$ and $R^{2}\\cdot h=O(d)$ . Consider an algorithm $\\boldsymbol{\\mathcal{A}}$ with the following property: ", "page_idx": 7}, {"type": "text", "text": "Given any fixed matrix $n\\times d$ matrix $A$ with $O(h)$ heavy rows and gap $\\sigma_{1}(A)^{2}/\\sigma_{2}(A)^{2}\\geq R,$ , in the form of a uniform random order stream, the algorithm $\\boldsymbol{\\mathcal{A}}$ outputs a unit vector $\\hat{v}$ such that, with probability $\\geq1-(1/2)^{4R+4}$ over the randomness of the stream and the internal randomness of the algorithm, $|\\langle\\hat{v},v_{1}\\rangle|^{\\frac{\\gamma}{2}}\\geq1-c/R^{2}$ . ", "page_idx": 7}, {"type": "text", "text": "If c is a small enough constant, then the algorithm $\\boldsymbol{\\mathcal{A}}$ must use $\\Omega(h\\cdot d/R)$ bits of space. ", "page_idx": 7}, {"type": "text", "text": "The theorem shows that a streaming algorithm must use $\\Omega(h d/R)$ bits of space assuming that with high probability, it outputs a vector with a large enough correlation with the top eigenvector of $A^{\\mathsf{T}}A$ when the rows are given in a random order stream. ", "page_idx": 7}, {"type": "text", "text": "Our proof uses the same lower bound instance as that of Price and Xun (2024). The key difference from their proof is that our lower bound must hold against random order streams. ", "page_idx": 7}, {"type": "text", "text": "4 Improving the Gap Requirements in the Algorithm of Price and Xun ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "4.1 Arbitrary Order Streams ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "As discussed in Section 2.1, we can guess an approximation of $\\|A\\|_{2}^{2}$ in powers of 2 and sample at most $O(d\\log d/\\varepsilon^{2})$ rows in the stream to obtain a matrix $_B$ , in the form of a stream, satisfying $\\|B^{\\top}B-A^{\\top}A\\|_{2}\\leq\\varepsilon\\|A\\|_{2}^{2}$ , with a large probability. Using Weyl\u2019s inequalities, we obtain that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\sigma_{2}(B^{\\boldsymbol{\\mathsf{T}}}B)\\leq\\sigma_{2}(A^{\\boldsymbol{\\mathsf{T}}}A)+\\varepsilon\\|A\\|_{2}^{2}\\quad\\mathrm{and}\\quad\\sigma_{1}(B^{\\boldsymbol{\\mathsf{T}}}B)\\geq(1-\\varepsilon)\\sigma_{1}(A^{\\boldsymbol{\\mathsf{T}}}A)\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "implying $R^{\\prime}=\\sigma_{1}(B)^{2}/\\sigma_{2}(B)^{2}\\geq(1-\\varepsilon)/(1/R+\\varepsilon).$ . For $\\varepsilon=1/(2R)\\leq1/2$ , we note $R^{\\prime}\\ge R/3$ . Let $\\bar{n^{\\prime}}\\,\\bar{=}\\,O(R^{2}\\cdot d\\log\\dot{d})$ be the number of rows in the matrix $_B$ and note that $R^{\\prime}=\\Omega(\\log n^{\\prime}\\cdot\\log d)$ assuming $R=\\Omega(\\log^{2}d)$ . Hence, running the algorithm of Price and Xun on the rows of the matrix ", "page_idx": 7}, {"type": "text", "text": "$_B$ , we compute a vector $\\hat{v}$ for which ", "page_idx": 8}, {"type": "equation", "text": "$$\n|\\langle\\hat{v},v_{1}^{\\prime}\\rangle|^{2}\\geq1-\\frac{\\log d}{C R^{\\prime}}-\\frac{1}{\\mathrm{poly}(d)}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "with a large probability, where $v_{1}^{\\prime}$ is the top eigenvector of the matrix $B^{\\top}B$ . We now note that if $v_{1}$ denotes the top eigenvector of the matrix $A^{\\mathsf{T}}A$ , then $|\\langle v_{1},v_{1}^{\\prime}\\rangle|^{2}\\ge1-O(1/R)$ which therefore implies that with a large probability, ", "page_idx": 8}, {"type": "equation", "text": "$$\n|\\langle\\hat{v},v_{1}\\rangle|^{2}\\geq1-\\frac{\\log d}{C R}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Thus, sub-sampling the stream using row norm sampling and then running the algorithm of Price and Xun (2024), we obtain an algorithm for arbitrary order streams with a gap $\\bar{R}=\\Omega(\\log^{2}d)$ . ", "page_idx": 8}, {"type": "text", "text": "4.2 Random Order Streams ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Lemma 3.5 in Price and Xun (2024) can be tightened when the rows of the stream are uniformly randomly ordered. Specifically, we want to bound the following quantity: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\langle a_{i},P\\hat{v}_{i-1}\\rangle^{2}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $P=I-v_{1}v_{1}^{\\mathsf{T}}$ denotes the projection away from the top eigenvector, and $\\hat{v}_{i-1}$ is a function of $v_{1},a_{1},\\ldots,a_{i-1}$ . We have ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbf{E}[\\langle a_{i},P\\hat{v}_{i-1}\\rangle^{2}]=\\mathbf{E}[\\mathbf{E}[\\langle a_{i},P\\hat{v}_{i-1}\\rangle^{2}\\mid a_{1},\\dots,a_{i-1}]].\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Given that the first $i-1$ rows are $a_{1},\\dotsc,a_{i-1}$ , assuming uniform random order, we have ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{E}[\\langle a_{i},P\\hat{v}_{i-1}\\rangle^{2}\\mid a_{1},\\dots,a_{i-1}]=\\frac{1}{n-i+1}\\hat{v}_{i-1}^{\\top}P(A^{\\top}A-a_{1}a_{1}^{\\top}-\\dots-a_{i-1}a_{i-1}^{\\top})P\\hat{v}_{i-1}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\frac{\\sigma_{2}(A)^{2}}{n-i+1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Hence $\\mathbf{E}[\\langle a_{i},P\\hat{v}_{i-1}\\rangle^{2}]\\le\\sigma_{2}(A)^{2}/(n\\!-\\!i\\!+\\!1)$ and $\\begin{array}{r}{\\mathbf{E}[\\sum_{i=1}^{n}\\langle a_{i},P\\hat{v}_{i-1}\\rangle^{2}]\\leq\\sigma_{2}(A)^{2}(1\\!+\\!\\log n).}\\end{array}$ . Price and Xun define $\\eta\\cdot\\sigma_{2}(A)^{2}$ as $\\sigma_{2}$ and in that notation, we obtain $\\begin{array}{r l}{\\eta\\sum_{i=1}^{n}\\langle a_{i},P\\hat{v}_{i-1}\\rangle^{2}\\leq10\\sigma_{2}(1+}\\end{array}$ $\\log n)$ with probability $\\geq9/10$ by Markov\u2019s inequality. In the proo f of Lemma 3.6 in Price and Xun (2024), if $\\sigma_{1}/\\sigma_{2}\\geq20(1+\\log_{2}n)$ , we obtain $\\log\\|v_{n}\\|_{2}\\gtrsim\\sigma_{1}$ . Now, $\\sigma_{1}\\geq O(\\log d)$ ensures that the Proof of Theorem 1.1 in their work goes through. ", "page_idx": 8}, {"type": "text", "text": "Using the row-norm sampling analysis from the previous section, we can assume $n=\\mathrm{poly}(d)$ and therefore a gap of $O(\\log d)$ between the top two eigenvalues of $A^{\\mathsf{T}}A$ is enough for Oja\u2019s algorithm to output a vector with a large correlation with the top eigenvector in random order streams. ", "page_idx": 8}, {"type": "text", "text": "5 Hard Instance for Oja\u2019s Algorithm ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "At a high level, the algorithm of Price and Xun (2024) runs Oja\u2019s algorithm with different learning rates $\\eta$ and in the event that the norm of the output vector with each of the learning rates $\\eta$ is small, then the row with the largest norm is output. The algorithm is simple and can be implemented using an overall space of $O(d\\cdot\\mathrm{polylog}(d))$ bits. ", "page_idx": 8}, {"type": "text", "text": "The algorithm initializes $z_{0}~=~g$ where $\\textbf{\\textit{g}}$ is a random Gaussian vector. The algorithm streams through the rows $a_{1},\\ldots,a_{n}$ and performs the following operation ", "page_idx": 8}, {"type": "equation", "text": "$$\nz_{i}\\leftarrow z_{i-1}+\\eta\\cdot\\langle z_{i-1},a_{i}\\rangle a_{i}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The algorithm computes the smallest learning rate $\\eta$ when $\\|z_{n}\\|_{2}$ is large enough, and then outputs either $z_{n}/\\Vert z_{n}\\Vert_{2}$ or $\\bar{a}/\\|\\bar{a}\\|_{2}$ as an approximation to the eigenvector of the matrix $A^{\\mathsf{T}}A$ . Here $\\bar{a}$ denotes the row in $A$ with the largest Euclidean norm. ", "page_idx": 8}, {"type": "text", "text": "The following theorem shows that at ga $\\ p s\\,\\leq\\,O(\\log d/\\log\\log d)$ , we cannot use Oja\u2019s algorithm with a fixed learning rate $\\eta$ to obtain constant correlation with the top eigenvector. ", "page_idx": 8}, {"type": "text", "text": "Theorem 5.1. Given dimension $d,$ a constant $c>0$ , a parameter $M$ , for all gap parameters $R=$ $O_{c}(\\log d/\\log\\log d)$ there is a stream of vectors $a_{1},\\dots,\\bar{a}_{n}\\in\\mathbb{R}^{d}$ with $n=O(R+M)$ such that: ", "page_idx": 9}, {"type": "text", "text": "2. Oja\u2019s algorithm with any learning rate $\\eta<M$ fails to output a unit vector $\\hat{v}$ that satisfies, with probability $\\geq9/10$ , ", "page_idx": 9}, {"type": "equation", "text": "$$\n|\\langle\\hat{v},v_{1}\\rangle|\\ge c\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "where $v_{1}$ is the top eigenvector of the matrix $A^{\\mathsf{T}}A$ . ", "page_idx": 9}, {"type": "text", "text": "Moreover, the result holds irrespective of the order in which the vectors $a_{1},\\ldots,a_{n}$ are presented to the Oja\u2019s algorithm. We will additionally show that even keeping track of the largest norm vector is insufficient to output a vector that has a large correlation with $v_{1}$ . ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The authors were supported in part by a Simons Investigator Award and NSF CCF-2335412. D.   \nWoodruff was visiting Google Research while performing this work. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Zeyuan Allen-Zhu and Yuanzhi Li. First efficient convergence for streaming k-PCA: a global, gapfree, and near-optimal rate. In 2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS), pages 487\u2013492. IEEE, 2017. 2 ", "page_idx": 9}, {"type": "text", "text": "Sepehr Assadi and Janani Sundaresan. (Noisy) gap cycle counting strikes back: Random order streaming lower bounds for connected components and beyond. In Proceedings of the 55th Annual ACM Symposium on Theory of Computing, pages 183\u2013195, 2023. 3 ", "page_idx": 9}, {"type": "text", "text": "Maria-Florina Balcan, Simon Shaolei Du, Yining Wang, and Adams Wei Yu. An improved gapdependency analysis of the noisy power method. In Conference on Learning Theory, pages 284\u2013 309. PMLR, 2016. 2 ", "page_idx": 9}, {"type": "text", "text": "Christos Boutsidis, David P Woodruff, and Peilin Zhong. Optimal principal component analysis in distributed and streaming models. In Proceedings of the forty-eighth annual ACM symposium on Theory of Computing, pages 236\u2013249, 2016. 2 ", "page_idx": 9}, {"type": "text", "text": "Amit Chakrabarti, Graham Cormode, and Andrew McGregor. Robust lower bounds for communication and stream computation. In Proceedings of the fortieth annual ACM symposium on Theory of computing, pages 641\u2013650, 2008. 3 ", "page_idx": 9}, {"type": "text", "text": "Kenneth L Clarkson and David P Woodruff. Low-rank approximation and regression in input sparsity time. Journal of the ACM (JACM), 63(6):1\u201345, 2017. 2 ", "page_idx": 9}, {"type": "text", "text": "Mina Ghashami, Edo Liberty, Jeff M Phillips, and David P Woodruff. Frequent directions: Simple and deterministic matrix sketching. SIAM Journal on Computing, 45(5):1762\u20131792, 2016. 2 ", "page_idx": 9}, {"type": "text", "text": "Ming Gu. Subspace iteration randomization and singular value problems. SIAM Journal on Scientific Computing, 37(3):A1139\u2013A1173, 2015. 2, 3 ", "page_idx": 9}, {"type": "text", "text": "Sudipto Guha and Andrew McGregor. Stream order and order statistics: Quantile estimation in random-order streams. SIAM Journal on Computing, 38(5):2044\u20132059, 2009. 3 ", "page_idx": 9}, {"type": "text", "text": "Sudipto Guha, Andrew McGregor, and Suresh Venkatasubramanian. Streaming and sublinear approximation of entropy and information distances. arXiv preprint cs/0508122, 2005. 3 ", "page_idx": 9}, {"type": "text", "text": "Anupam Gupta and Sahil Singla. Random-order models. In Tim Roughgarden, editor, Beyond the Worst-Case Analysis of Algorithms, chapter 11. Oxford University Press, 2021. doi: 10.1017/ 9781108637435. URL https://arxiv.org/pdf/2002.12159. 3 ", "page_idx": 9}, {"type": "text", "text": "Moritz Hardt and Eric Price. The noisy power method: A meta algorithm with applications. Advances in neural information processing systems, 27, 2014. 2   \nDe Huang, Jonathan Niles-Weed, and Rachel Ward. Streaming k-PCA: Efficient guarantees for oja\u2019s algorithm, beyond rank-one updates. In Conference on Learning Theory, pages 2463\u20132498. PMLR, 2021. 2   \nPrateek Jain, Chi Jin, Sham M Kakade, Praneeth Netrapalli, and Aaron Sidford. Streaming pca: Matching matrix bernstein and near-optimal finite sample guarantees for Oja\u2019s algorithm. In Conference on learning theory, pages 1147\u20131164. PMLR, 2016. 2   \nSyamantak Kumar and Purnamrita Sarkar. Streaming pca for markovian data. In Advances in Neural Information Processing Systems, volume 36, 2023. 2   \nMalik Magdon-Ismail. Row sampling for matrix algorithms via a non-commutative bernstein bound. arXiv preprint arXiv:1008.0587, 2010. 3, 4   \nIoannis Mitliagkas, Constantine Caramanis, and Prateek Jain. Memory-limited, streaming PCA. In Advances in Neural Information Processing Systems, volume 26, 2013. 2   \nJ Ian Munro and Mike S Paterson. Selection and sorting with limited storage. Theoretical computer science, 12(3):315\u2013323, 1980. 3   \nCameron Musco and Christopher Musco. Randomized block krylov methods for stronger and faster approximate singular value decomposition. Advances in neural information processing systems, 28, 2015. 2   \nCameron Musco, Christopher Musco, and Aaron Sidford. Stability of the lanczos method for matrix function approximation. In Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1605\u20131624. SIAM, 2018. 2   \nErkki Oja. Simplified neuron model as a principal component analyzer. Journal of mathematical biology, 15:267\u2013273, 1982. 2   \nEric Price and Zhiyang Xun. Spectral guarantees for adversarial streaming PCA. In FOCS, 2024. 2, 3, 4, 8, 9, 15   \nJoel A Tropp. An introduction to matrix concentration inequalities. Foundations and Trends\u00ae in Machine Learning, 8(1-2):1\u2013230, 2015. 12   \nJalaj Upadhyay. Fast and space-optimal low-rank factorization in the streaming model with application in differential privacy. arXiv preprint arXiv:1604.01429, 2016. 2   \nBo-Ying Wang and Bo-Yan Xi. Some inequalities for singular values of matrix products. Linear algebra and its applications, 264:109\u2013115, 1997. 3, 6, 13 ", "page_idx": 10}, {"type": "text", "text": "A Omitted Proofs ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "A.1 Proof of Theorem 2.1 ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Proof. Let $X_{i}$ denote an indicator random variable which denotes if $Q_{i i}$ is nonzero. Note $\\mathbf{E}[X_{i}]=$ $p_{i}$ and $X_{1},\\ldots,X_{n}$ are independent. Define a $d\\times d$ random matrix $\\mathbf{Y}_{i}=(\\mathbf{X}_{i}/p_{i}-1)a_{i}a_{i}^{\\mathsf{T}}$ , where $a_{i}$ denotes the $i$ -th row of $A$ . We note that ", "page_idx": 11}, {"type": "equation", "text": "$$\nA^{\\mathsf{T}}A-A^{\\mathsf{T}}Q^{\\mathsf{T}}Q A=\\sum_{i=1}^{n}(X_{i}/p_{i}-1)a_{i}a_{i}^{\\mathsf{T}}=\\sum_{i=1}^{n}Y_{i}.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "We use the Matrix Bernstein inequality (Tropp, 2015) to bound $\\|\\textstyle\\sum_{i}Y_{i}\\|_{2}$ . We first uniformly upper bound $\\lVert\\boldsymbol{Y}_{i}\\rVert_{2}$ . If $p_{i}\\,=\\,1\\$ , by definition $\\|\\pmb{Y}_{i}\\|_{2}\\,=\\,0$ with probability 1. Let $p_{i}~\\neq~0$ . Then, $\\|(X_{i}/p_{i}-1)a_{i}a_{i}^{\\sf T}\\|_{2}\\leq\\|a_{i}a_{i}^{\\sf T}\\|_{2}/p_{i}\\leq\\varepsilon^{2}\\|A\\|_{2}^{2}/C\\log d$ with probability 1. ", "page_idx": 11}, {"type": "text", "text": "We now bound $\\|\\sum_{i}\\mathbf{E}[Y_{i}^{2}]\\|_{2}$ . ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i}\\mathbf{E}[Y_{i}^{2}]=\\sum_{i}\\mathbf{E}[(1/p_{i}-1)^{2}]\\|a_{i}\\|_{2}^{2}a_{i}a_{i}^{\\top}}\\\\ &{\\displaystyle\\qquad\\qquad=\\sum_{i:p_{i}>0}(1/p_{i}-1)\\|a_{i}\\|_{2}^{2}a_{i}a_{i}^{\\top}}\\\\ &{\\displaystyle\\qquad\\qquad\\preceq\\sum_{i:p_{i}>0}\\frac{\\varepsilon^{2}\\|A\\|_{2}^{2}}{C\\|a_{i}\\|_{2}^{2}\\log d}\\|a_{i}\\|_{2}^{2}a_{i}a_{i}^{\\top}}\\\\ &{\\displaystyle\\qquad\\qquad\\preceq\\frac{\\varepsilon^{2}\\|A\\|_{2}^{2}}{C\\log d}A^{\\top}A}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "which implies $\\begin{array}{r}{\\|\\sum_{i}\\mathbf{E}[\\mathbf{Y}_{i}^{2}]\\|_{2}\\leq\\varepsilon^{2}\\|A\\|_{2}^{4}/(C\\log d)}\\end{array}$ . Now, we obtain ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{Pr}[\\|\\displaystyle\\sum_{i}\\mathbf{Y}_{i}\\|_{2}\\ge\\varepsilon\\|A\\|_{2}^{2}]\\le2d\\cdot\\exp\\left(-\\frac{\\varepsilon^{2}\\|A\\|_{2}^{4}/2}{\\varepsilon^{2}\\|A\\|_{2}^{4}/(C\\log d)+\\varepsilon^{3}\\|A\\|_{2}^{4}/(3C\\log d)}\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\le2d\\cdot\\exp\\left(-\\frac{C\\log d}{2(1+\\varepsilon/3)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "If $C\\geq6(1+\\varepsilon/3)$ , then $\\begin{array}{r}{\\mathbf{Pr}[\\|\\sum_{i}\\mathbf{Y}_{i}\\|_{2}\\ge\\varepsilon\\|A\\|_{2}^{2}]\\le1-2/d^{2}}\\end{array}$ which implies that with probability $\\geq1-2/d^{2},\\,\\|A^{\\top}A-A^{\\top}Q^{\\top}Q A\\|_{2}\\leq\\varepsilon\\|A\\|_{2}^{2}$ . ", "page_idx": 11}, {"type": "text", "text": "Now, the number of non-zero entries in the matrix $Q$ is equal to $\\sum_{i}X_{i}$ . We note $\\mathbf{E}[\\sum_{i}X_{i}]\\;\\leq$ $C\\varepsilon^{-2}\\rho\\cdot\\log d$ . By a Chernoff bound, we obtain that $\\textstyle\\sum_{i}X_{i}\\,=\\,O(\\varepsilon^{-2}\\rho\\cdot\\log d)$ with probability $\\ge1-1/\\mathrm{poly}(d)$ . \u53e3 ", "page_idx": 11}, {"type": "text", "text": "A.2 Proof of Lemma 2.3 ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Proof. Define $M:=(B_{t}^{\\top}B_{t})\\cdot\\cdot\\cdot(B_{1}^{\\top}B_{1})$ . Our strategy is to show that if $v_{1}$ is the top singular vector of the matrix $A$ , then $\\|v_{1}^{\\mathsf{T}}M\\|_{2}$ is comparable to $\\lVert M\\rVert_{\\mathsf{F}}$ given that $\\sigma_{1}(A)/\\sigma_{2}(A)\\geq2.$ . We can then prove the lemma using simple properties of the Gaussian vector $\\textbf{\\textit{g}}$ . ", "page_idx": 11}, {"type": "text", "text": "For an arbitrary $j$ , let $(B_{j}^{\\mathsf{T}}B_{j})v_{1}\\,=\\,\\alpha v_{1}+\\Delta$ where $\\Delta\\perp v_{1}$ . We note that $v_{1}^{\\mathsf{T}}(B_{j}^{\\mathsf{T}}B_{j})v_{1}\\,=\\,\\alpha$ . We have $\\alpha\\,=\\,v_{1}^{\\mathsf{T}}B_{j}^{\\mathsf{T}}B_{j}v_{1}\\,\\geq\\,(1-\\varepsilon)\\sigma_{1}(A)^{2}$ using the fact that $\\|B_{j}^{\\mathsf{T}}B_{j}\\,-A^{\\mathsf{T}}A\\|_{2}\\,\\leq\\,\\varepsilon\\|A\\|_{2}^{2}$ and $v_{1}^{\\mathsf{T}}A^{\\mathsf{T}}A v_{1}=\\sigma_{1}(A)^{\\mathsf{\\bar{2}}}=\\|A\\|_{2}^{2}$ . If we show that $\\Delta$ is small, then the vector $(B_{j}^{\\top}B_{j})v_{1}$ is oriented in a direction very close to that of $v_{1}$ . Note that ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\|(B_{j}^{\\mathsf{T}}B_{j})v_{1}\\|_{2}\\leq\\|B_{j}^{\\mathsf{T}}B_{j}\\|_{2}\\leq(1+\\varepsilon)\\sigma_{1}(A)^{2}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "and $\\|(B_{j}^{\\top}B_{j})v_{1}\\|_{2}^{2}=\\alpha^{2}+\\|\\Delta\\|_{2}^{2}$ which implies $\\|\\Delta\\|_{2}^{2}\\le((1+\\varepsilon)^{2}-(1-\\varepsilon)^{2})\\sigma_{1}(A)^{4}=4\\varepsilon\\cdot\\sigma_{1}(A)^{4}$ and thus $\\|\\Delta\\|_{2}\\leq{\\sqrt{4\\varepsilon}}\\sigma_{1}(A)^{2}$ . Now, ", "page_idx": 11}, {"type": "text", "text": "$\\|\\boldsymbol{M}^{\\boldsymbol{\\mathsf{T}}}\\boldsymbol{v}_{1}\\|_{2}$ $\\begin{array}{r l}&{=\\|(B_{1}^{\\boldsymbol{\\mathsf{T}}}B_{1})\\cdots(B_{t-1}^{\\boldsymbol{\\mathsf{T}}}B_{t-1})(\\langle B_{t}^{\\boldsymbol{\\mathsf{T}}}B_{t}v_{1},v_{1}\\rangle v_{1}+\\Delta_{1})\\|_{2}}\\\\ &{\\geq\\langle B_{t}^{\\boldsymbol{\\mathsf{T}}}B_{t}v_{1},v_{1}\\rangle\\|(B_{1}^{\\boldsymbol{\\mathsf{T}}}B_{1})\\cdots(B_{t-1}^{\\boldsymbol{\\mathsf{T}}}B_{t-1})v_{1}\\|_{2}-\\|(B_{1}^{\\boldsymbol{\\mathsf{T}}}B_{1})\\cdots(B_{t-1}^{\\boldsymbol{\\mathsf{T}}}B_{t-1})\\|_{2}\\|\\Delta_{1}\\|_{2}}\\\\ &{\\geq((1-\\varepsilon)\\sigma_{1}(A)^{2})\\|(B_{1}^{\\boldsymbol{\\mathsf{T}}}B_{1})\\cdots(B_{t-1}^{\\boldsymbol{\\mathsf{T}}}B_{t-1})v_{1}\\|_{2}-(\\sqrt{4\\varepsilon}\\sigma_{1}(A)^{2})\\|(B_{1}^{\\boldsymbol{\\mathsf{T}}}B_{1})\\cdots(B_{t-1}^{\\boldsymbol{\\mathsf{T}}}B_{t-1})\\|_{2}.}\\end{array}$ ", "page_idx": 11}, {"type": "text", "text": "Expanding similarly, we obtain ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\|M^{\\top}v_{1}\\|_{2}\\geq(1-\\varepsilon)^{t}\\sigma_{1}(A)^{2t}-t\\sqrt{4\\varepsilon}(1+\\varepsilon)^{t-1}\\sigma_{1}(A)^{2t}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Assuming $\\varepsilon\\leq c/t$ for a small constant $c$ , we note that $(1-\\varepsilon)^{t}\\geq(1-2t\\varepsilon)$ and $(1+\\varepsilon)^{t}\\leq(1+2t\\varepsilon)$ which implies ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\|M^{\\mathsf{T}}v_{1}\\|_{2}=\\|(B_{1}^{\\mathsf{T}}B_{1})\\cdot\\cdot\\cdot(B_{t}^{\\mathsf{T}}B_{t})v_{1}\\|_{2}\\geq(1-2t\\varepsilon-4t\\sqrt{\\varepsilon})\\sigma_{1}(A)^{2t}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "We shall now show a bound on $\\|M\\|_{\\mathsf{F}}=\\|(B_{t}^{\\mathsf{T}}B_{t})\\cdot\\cdot\\cdot(B_{1}^{\\mathsf{T}}B_{1})\\|_{\\mathsf{F}}$ which lets us show that the unit vector $\\hat{v}$ is highly correlated with $v_{1}$ . To bound the quantity $\\lVert M\\rVert_{\\mathsf{F}}$ , we first note the following facts: ", "page_idx": 12}, {"type": "text", "text": "1. $\\|B_{j}^{\\mathsf{T}}B_{j}\\|_{2}\\leq(1+\\varepsilon)\\sigma_{1}(A)^{2}$ , and   \n2. $\\sigma_{2}(B_{j}^{\\top}B_{j})\\leq\\sigma_{2}(A)^{2}+\\varepsilon\\sigma_{1}(A)^{2}\\leq(1/4+\\varepsilon)\\sigma_{1}(A)^{2}$ by our gap assumption. ", "page_idx": 12}, {"type": "text", "text": "Now, we use the following theorem. ", "page_idx": 12}, {"type": "text", "text": "Theorem A.1 ((Wang and Xi, 1997, Theorem 3(ii))). For any $r>0$ and any matrices $A_{1},\\ldots,A_{t},$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\sum_{i}(\\sigma_{i}(A_{1}\\cdot\\cdot\\cdot A_{t}))^{r}\\leq\\sum_{i}\\sigma_{i}(A_{1})^{r}\\cdot\\cdot\\cdot\\sigma_{i}(A_{t})^{r}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Applying the above theorem with $r=2$ , we obtain ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|(B_{t}^{\\top}B_{t})\\cdots(B_{1}^{\\top}B_{1})\\|_{\\mathsf F}^{2}\\leq(1+\\varepsilon)^{2t}\\sigma_{1}(A)^{4t}+(d-1)(1/4+\\varepsilon)^{t}\\sigma_{1}(A)^{4t}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq(1+4t\\varepsilon)\\sigma_{1}(A)^{4t}+\\displaystyle\\frac{d}{3^{t}}\\sigma_{1}(A)^{4t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "When $t\\geq3\\log(d/\\varepsilon)$ , we have $\\|(B_{t}^{\\mathsf{T}}B_{t})\\cdot\\cdot\\cdot(B_{1}^{\\mathsf{T}}B_{1})\\|_{\\mathsf{F}}^{2}\\leq(1+4t\\varepsilon+\\varepsilon)\\sigma_{1}(A)^{4t}$ . We now use the following lemma. ", "page_idx": 12}, {"type": "text", "text": "Lemma A.2. Let g be a Gaussian random vector with each of the components being an independent standard Gaussian random variable. Let $\\hat{v}=M\\mathbf{g}/\\lVert M\\mathbf{g}\\rVert_{2}$ . For any unit vector $v$ , with probability $\\geq4/5$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\n|\\langle{\\hat{v}},v\\rangle|^{2}\\geq{\\frac{1}{1+C{\\frac{\\|M\\|_{\\mathsf{F}}^{2}-\\|M^{\\mathsf{T}}v\\|_{2}^{2}}{\\|M^{\\mathsf{T}}v\\|_{2}^{2}}}}}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "for a large enough universal constant $C$ . ", "page_idx": 12}, {"type": "text", "text": "Proof. Since $v$ is a unit vector, we can write $\\|\\b{M}\\pmb{g}\\|_{2}^{2}=|\\b{v}^{\\mathsf{T}}\\b{M}\\pmb{g}|^{2}+\\|(\\b{I}-\\b{v v}^{\\mathsf{T}})\\b{M}\\pmb{g}\\|_{2}^{2}$ . Hence, we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n|\\langle{\\hat{v}},v\\rangle|^{2}={\\frac{|v^{\\mathsf{T}}M g|^{2}}{\\|M g\\|_{2}^{2}}}={\\frac{1}{1+{\\frac{\\|(I-v v^{\\mathsf{T}})M g\\|_{2}^{2}}{|v^{\\mathsf{T}}M g|^{2}}}}}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "We now note that $v^{\\mathsf{T}}M\\pmb{g}\\sim N(0,\\|M^{\\mathsf{T}}v\\|_{2}^{2})$ and $\\mathbf{E}[\\|(I-v v^{\\mathsf{T}})M\\mathbf{g}\\|_{2}^{2}]=\\operatorname{tr}(M^{\\mathsf{T}}(I-v v^{\\mathsf{T}})M)=$ $\\lVert M\\rVert_{\\mathsf{F}}^{2}-\\lVert M^{\\mathsf{T}}v\\rVert_{2}^{2}$ . By a union bound, with probability $\\geq4/5$ , we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\frac{\\|(I-v v^{\\mathsf{T}})M\\pmb{g}\\|_{2}^{2}}{|v^{\\mathsf{T}}M\\pmb{g}|^{2}}\\leq C\\frac{\\|M\\|_{\\mathsf{F}}^{2}-\\|M^{\\mathsf{T}}v\\|_{2}^{2}}{\\|M^{\\mathsf{T}}v\\|_{2}^{2}}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "for a large enough constant $C$ . Therefore, with probability $\\geq4/5$ , we get that ", "page_idx": 12}, {"type": "equation", "text": "$$\n|\\langle\\hat{v},v\\rangle|^{2}\\ge\\frac{1}{1+C\\frac{\\|M\\|_{\\mathsf{F}}^{2}-\\|M^{\\mathsf{T}}v\\|_{2}^{2}}{\\|M^{\\mathsf{T}}v\\|_{2}^{2}}}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Applying the above lemma for $M=(B_{t}^{\\top}B_{t})\\cdot\\cdot\\cdot(B_{1}^{\\top}B_{1})$ and $v=v_{1}$ , we obtain ", "page_idx": 12}, {"type": "equation", "text": "$$\n|\\langle{\\hat{v}},v_{1}\\rangle|^{2}\\geq{\\frac{1}{1+C^{\\prime}t\\sqrt{\\varepsilon}}}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "with probability $\\geq4/5$ . ", "page_idx": 12}, {"type": "text", "text": "A.3 Proof of Theorem 2.5 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proof. Partition the matrix $A$ into $A_{\\mathrm{light}}$ and $A_{\\mathrm{heavy}}$ , where $A_{\\mathrm{heavy}}$ is the submatrix with rows $a_{i}$ such that $\\|a_{i}\\|_{2}\\,>\\,\\|A\\|_{\\mathsf{F}}/\\sqrt{d\\cdot\\mathrm{polylog}(d)}$ and $A_{\\mathrm{light}}$ is the remaining rows. From our assumption, the number of rows in $A_{\\mathrm{heavy}}$ is at most $h$ . Note that given a uniformly random stream of rows of $A$ , we can obtain a uniformly random stream of rows of $A_{\\mathrm{light}}$ by just filtering out the rows in $A_{\\mathrm{heavy}}$ . ", "page_idx": 13}, {"type": "text", "text": "Suppose, $\\|A_{\\mathrm{heavy}}\\cdot v_{1}\\|_{2}\\,\\geq\\,(1-\\beta)\\|A\\|_{2}$ for a parameter $\\beta$ to be chosen later. Let $v_{1}^{\\prime}$ be the top singular vector of the matrix $A_{\\mathrm{heavy}}$ . Note ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|A\\cdot v_{1}^{\\prime}\\|_{2}^{2}\\geq\\|A_{\\mathrm{heavy}}\\cdot v_{1}^{\\prime}\\|_{2}^{2}\\geq\\|A_{\\mathrm{heavy}}\\cdot v_{1}\\|_{2}^{2}\\geq(1-\\beta)^{2}\\|A\\|_{2}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and therefore we have $\\langle v_{1}^{\\prime},v_{1}\\rangle^{2}\\geq1-4\\beta$ , assuming $R\\geq2$ . Thus, while processing the stream, we can store all the heavy rows and at the end of the stream compute the top right singular vector of $A_{\\mathrm{heavy}}$ , in order to obtain a good approximation for $v_{1}$ . ", "page_idx": 13}, {"type": "text", "text": "Suppose $\\|A_{\\mathrm{heavy}}{\\cdot}v_{1}\\|_{2}\\leq(1\\!-\\!\\beta)\\|A\\|_{2}$ . This implies $\\begin{array}{r}{\\|A_{\\mathrm{light}}\\!\\cdot\\!v_{1}\\|_{2}^{2}\\geq\\|A\\|_{2}^{2}\\!-\\!\\|A_{\\mathrm{heavy}}\\!\\cdot\\!v_{1}\\|_{2}^{2}\\geq\\beta\\!\\cdot\\!\\|A\\|_{2}^{2}.}\\end{array}$ If we set $\\beta\\geq2/R$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{\\sigma_{1}(A_{\\mathrm{light}})^{2}}{\\sigma_{2}(A_{\\mathrm{light}})^{2}}\\geq\\frac{\\beta\\|A\\|_{2}^{2}}{\\sigma_{2}(A)^{2}}\\geq2.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Let $v_{1}^{\\prime}$ be the top singular vector of $A_{\\mathrm{light}}$ . We will describe how to approximate $v_{1}^{\\prime}$ . Consider applying the row norm sampling procedure with parameter $\\varepsilon$ to the matrix $A_{\\mathrm{light}}$ . Given a row $a_{i}\\in A_{\\mathrm{light}}$ the corresponding sampling probability $p_{i}$ is given by ", "page_idx": 13}, {"type": "equation", "text": "$$\np_{i}=\\frac{C\\log d\\cdot\\|a_{i}\\|_{2}^{2}}{\\varepsilon^{2}\\|A_{\\mathrm{light}}\\|_{2}^{2}}\\le\\frac{C\\log d\\cdot\\|A\\|_{\\mathsf{F}}^{2}/(d\\cdot\\mathrm{polylog}(d))}{\\varepsilon^{2}\\beta^{2}\\|A\\|_{2}^{2}}\\le\\frac{C}{\\varepsilon^{2}\\beta^{2}\\mathrm{polylog}(d)}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Assuming that $\\varepsilon^{2}\\beta^{2}\\geq1/\\mathrm{polylog}(d)$ , we obtain that $p_{i}<1$ for all the rows in the matrix $A_{\\mathrm{light}}$ . Let $B_{\\mathrm{light}}$ be the matrix obtained after applying the row norm sampling procedure to the matrix $A_{\\mathrm{light}}$ . Note that $\\rho(B_{\\mathrm{light}})\\,\\approx\\,\\rho(A_{\\mathrm{light}})$ and the number of rows in $B_{\\mathrm{light}}$ is $\\Theta(\\rho(A_{\\mathrm{light}})\\cdot\\log d\\cdot\\varepsilon^{-2})$ , and therefore $\\Theta(\\rho(B_{\\mathrm{light}})\\!\\cdot\\!\\log d\\!\\cdot\\!\\varepsilon^{-2})$ . Setting $\\varepsilon=\\alpha^{2}/\\log^{5/2}d,$ , we obtain that the number of rows in the matrix $B_{\\mathrm{light}}$ is $\\Theta(\\alpha^{-4}\\cdot\\rho(B_{\\mathrm{light}})\\cdot\\log^{6}d)$ and thus assuming $\\varepsilon^{2}\\beta^{2}=\\alpha^{4}\\beta^{2}/\\log^{5}d\\geq1/\\operatorname{polylog}(d)$ , we can use Theorem 2.4 to obtain a vector $\\hat{v}$ satisfying ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\langle\\hat{v},v_{1}^{\\prime}\\rangle^{2}\\geq1-3\\alpha.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We will now show that $v_{1}^{\\prime}$ has a large correlation with $v_{1}$ which then implies $\\hat{v}$ has a large correlation with $v_{1}$ . Since $\\|A_{\\mathrm{light}}\\|_{2}\\dot{\\geq}\\ \\|A\\|_{2}\\dot{-}\\ \\|A_{\\mathrm{heav}}\\|_{2}\\,\\geq\\,\\beta\\|A\\|_{2},\\ \\|A_{\\mathrm{light}}\\|_{2}^{2}\\ =\\ \\|A_{\\mathrm{light}}\\cdot v_{1}^{\\prime}\\|_{2}^{2}\\,\\geq\\,\\beta\\|A\\|_{2}^{2}$ . Consider the following upper bound on $\\lVert A_{\\mathrm{light}}\\cdot v_{1}^{\\prime}\\rVert_{2}^{2}$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|A_{\\mathrm{light}}||_{2}^{2}=\\|A_{\\mathrm{light}}\\cdot v_{1}^{\\prime}\\|_{2}^{2}=\\|A_{\\mathrm{light}}\\cdot(\\langle v_{1}^{\\prime},v_{1}\\rangle\\cdot v_{1}+(I-v_{1}v_{1}^{\\top})v_{1}^{\\prime})\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\|\\langle v_{1},v_{1}^{\\prime}\\rangle A_{\\mathrm{light}}\\cdot v_{1}+A_{\\mathrm{light}}(I-v_{1}v_{1}^{\\top})v_{1}^{\\prime}\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq(1+\\theta)\\cdot\\langle v_{1},v_{1}^{\\prime}\\rangle^{2}\\cdot\\|A_{\\mathrm{light}}\\cdot v_{1}\\|_{2}^{2}+(1+1/\\theta)\\cdot\\|A_{\\mathrm{light}}(I-v_{1}v_{1}^{\\top})v_{1}^{\\prime}\\|_{2}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "for any $\\theta>0$ . Using the fact that the rows of the \u221amatrix $A_{\\mathrm{light}}$ are a subset of the rows of the matrix $A$ and that $\\|A(I-v_{1}v_{1}^{\\mathsf{T}})\\|_{2}=\\sigma_{2}(A)=\\sigma_{1}(A)/\\sqrt{R}$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|A_{\\mathrm{light}}\\|_{2}^{2}\\leq(1+\\theta)\\cdot\\langle v_{1},v_{1}^{\\prime}\\rangle^{2}\\cdot\\|A_{\\mathrm{light}}\\|_{2}^{2}+(1+1/\\theta)\\cdot\\frac{\\sigma_{1}^{2}}{R}\\cdot(1-\\langle v_{1},v_{1}^{\\prime}\\rangle^{2})}\\\\ &{\\qquad\\qquad=\\langle v_{1},v_{1}^{\\prime}\\rangle^{2}((1+\\theta)\\cdot\\|A_{\\mathrm{light}}\\|_{2}^{2}-(1+1/\\theta)\\sigma_{1}^{2}/R)+(1+1/\\theta)\\cdot\\sigma_{1}^{2}/R}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "which implies ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\langle v_{1},v_{1}^{\\prime}\\rangle^{2}\\geq\\frac{\\|A_{\\mathrm{light}}\\|_{2}^{2}-(1+1/\\theta)\\cdot\\sigma_{1}^{2}/R}{(1+\\theta)\\|A_{\\mathrm{light}}\\|_{2}^{2}-(1+1/\\theta)\\sigma_{1}^{2}/R}=1-\\frac{\\theta\\cdot\\|A_{\\mathrm{light}}\\|_{2}^{2}}{(1+\\theta)\\|A_{\\mathrm{light}}\\|_{2}^{2}-(1+1/\\theta)\\sigma_{1}^{2}/R}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "using the fact that $\\mathrm{\\|\\calA_{\\mathrm{light}}\\|_{2}^{2}\\,\\geq\\,\\beta^{2}\\sigma_{1}^{2}}$ . Now assuming $R\\beta\\ge1$ and picking $\\theta\\,=\\,2/(R\\beta\\mathrm{~-~}1)$ , we obtain ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\langle v_{1},v_{1}^{\\prime}\\rangle^{2}\\geq1-\\frac{4R\\beta}{(1+R\\beta)^{2}}\\geq1-\\frac{4}{R\\beta}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We therefore have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\langle\\hat{v},v_{1}\\rangle^{2}\\geq1-\\frac{4}{R\\beta}-4\\alpha.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Setting $\\beta=1/\\sqrt{R}$ and $\\alpha=1/\\sqrt{R}$ , we satisfy all\u221a the requirements assuming that $R\\leq\\mathrm{polylog}(d)$ and obtain a vector $\\hat{v}$ satisfying $\\langle\\hat{v},v_{1}\\rangle^{2}\\geq1-8/\\sqrt{R}$ . When $\\|A_{\\mathrm{heavy}}\\|_{2}\\geq(1-\\beta)\\|A\\|_{2}$ , we already have a vector $v^{\\prime}=$ top eigenvector of $A_{\\mathrm{heavy}}$ that satisfies $\\langle\\hat{v},v_{1}\\rangle^{2}\\geq1-4\\beta\\geq1-4/\\sqrt{R}$ . Thus, in both the cases, we obtain a vector $\\hat{v}$ satisfying $\\langle\\hat{v},v_{1}\\rangle^{2}\\ge1-O(1/\\sqrt{R})$ . ", "page_idx": 14}, {"type": "text", "text": "The procedure described requires knowing the approximate values of $\\Vert A\\Vert_{\\mathsf{F}},\\;\\Vert A_{\\mathrm{light}}\\Vert_{2}$ . Since, we assume that all the non-zero entries of the matrix have an absolute value at least $1/\\operatorname{poly}(d)$ and at most poly $(d)$ , the values $\\Vert A\\Vert_{\\mathsf{F}},\\Vert A_{\\mathrm{light}}\\Vert_{2}$ lie in the interval $[1/\\mathrm{poly}(d),\\mathrm{poly}(n d)]$ . Hence, using $O(\\log n d)$ guesses each for $\\Vert A\\Vert_{\\mathsf{F}}$ and $\\bar{\\|}A_{\\mathrm{light}}\\|_{2}$ and using a Gaussian sketch of $A$ similar to that in Algorithm 1, we can obtain a vector satisfying the guarantees in the theorem. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "A.4 Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof. For each $i\\;\\in\\;[h]$ , let $\\boldsymbol{x}_{1},\\ldots,\\boldsymbol{x}_{h}$ be drawn independently and uniformly at random from $\\{+1,-1\\}^{d}$ . Let $i\\sim[h]$ be drawn uniformly at random, and for an integer $k$ to be chosen later, let $\\pmb{y}_{1},\\dots,\\pmb{y}_{k}\\in\\mathbb{R}^{d}$ be vectors that share the first $(1-\\gamma)d$ coordinates with the vector $\\pmb{x}_{i}$ . Each of the last $\\gamma\\cdot d$ elements of each of $\\pmb{y}_{1},\\dots,\\pmb{y}_{k}$ are sampled uniformly at random from the set $\\{+1,-1\\}$ . Define $z_{1},\\ldots,z_{h+k}$ such that for $j\\leq h$ , $z_{j}=x_{j}$ and for $j>h$ , let $z_{j}=y_{j-h}$ . ", "page_idx": 14}, {"type": "text", "text": "Now consider the stream $z_{1},\\ldots,z_{h+k}$ . Price and $X{\\mathfrak{u n}}$ argue that when $k\\,\\geq\\,4R$ , the gap of this stream is at least $R$ with large probability over the randomness used in the construction of the stream. Let $\\pi:[h+k]\\to[h+\\bar{k}]$ be a uniformly random permutation independent of $\\pmb{i}$ . Consider the following event $\\mathcal{E}$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\pi(i)\\leq h/2\\;\\mathrm{and}\\;\\pi(h+1),\\ldots,\\pi(h+k)>h/2.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We have that the probability of the event $\\mathcal{E}$ is ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\frac{h/2+k}{h+k}}\\cdot{\\frac{h/2+k-1}{h+k-1}}\\cdot\\cdot\\cdot{\\frac{h/2+1}{h+1}}\\cdot{\\frac{h/2}{h}}\\geq(1/2)^{k+1}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Let $S_{i}$ be the set of permutations $\\pi$ that satisfy the above event. Therefore we have $\\mathbf{Pr}_{\\pi}[\\pi\\in S_{i}]\\geq$ $(1/2)^{k+1}$ . If the probability of failure, $\\delta$ , of the algorithm $\\boldsymbol{\\mathcal{A}}$ satisfies $\\delta\\leq(1/2)^{k+4}$ , we have that ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\underset{\\pi,{\\mathrm{~internal~randomness}}}{\\mathbf{Pr}}}[{\\mathcal{A}}{\\mathrm{~succeeds~on~}}z_{\\pi(1)},\\ldots,z_{\\pi(h+k)}\\mid\\pi\\in S_{i}]\\;\\geq{\\frac{3}{4}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Let $s_{\\mathrm{mid}}$ be the state of the algorithm after $h/2$ steps and $s_{\\mathrm{fin}}$ be the final state of the algorithm. The randomness in $s_{\\mathrm{fin}}$ is from the following sources: (i) randomness of the vectors $\\boldsymbol{x}_{1},\\ldots,\\boldsymbol{x}_{h}$ , (ii) the index $i\\in[h]$ , (iii) the vectors $\\pmb{y}_{1},\\dots,\\pmb{y}_{k}$ , (iv) the permutation $\\pi$ , and (v) the internal randomness of the algorithm. From here on, condition on the event $\\mathcal{E}$ , i.e., that the permutation $\\pi\\in S_{i}$ . We will not explicitly mention that all entropy and information terms in the proof are conditioned on $\\mathcal{E}$ . Since $\\pi(i)\\leq\\dot{h}/2$ , we have ", "page_idx": 14}, {"type": "text", "text": "Using the data processing inequality, we obtain that ", "page_idx": 14}, {"type": "equation", "text": "$$\nI\\big(s_{\\mathrm{mid}};x_{i}[(1-\\gamma)\\cdot d+1:d]\\big)\\geq I\\big(s_{\\mathrm{fin}};x_{i}[(1-\\gamma)\\cdot d+1:d]\\big).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "When $h\\leq c d/R^{2},\\,k=4R,\\,\\gamma=1/4$ and $\\varepsilon\\le c/k^{2}$ for a small constant, we have as in the proof of Theorem 1.5 in Price and Xun (2024) that, ", "page_idx": 14}, {"type": "equation", "text": "$$\nI(s_{\\mathrm{fin}};x_{i}[(1-\\gamma)\\cdot d+1:d])\\geq\\Omega(d/R)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which now implies ", "page_idx": 14}, {"type": "equation", "text": "$$\nI(s_{\\mathrm{mid}};x_{i}[(1-\\gamma)\\cdot d+1:d])\\geq\\Omega(d/R).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note that conditioned on the event $\\mathcal{E}$ , the distribution of $\\pmb{i}$ is uniform over $\\{\\,\\pi^{-1}(1),...\\,,\\pi^{-1}(h/2)\\,\\}$ . We now prove the following lemma: ", "page_idx": 14}, {"type": "text", "text": "Lemma A.3. Let $Y_{1},\\mathbf{\\nabla}_{}...,Y_{\\ell}$ be independent random variables. Let $i\\sim[\\ell]$ be a uniform random variable independent of $\\mathbf{\\deltaX}$ . We have ", "page_idx": 15}, {"type": "equation", "text": "$$\nI(X\\,;Y_{1})+\\cdot\\,\\cdot\\,+\\,I(X\\,;\\,Y_{\\ell})\\geq\\ell\\cdot(I(X;Y_{i})-\\log_{2}\\ell).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. By definition, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\nI(X\\,;Y_{i})=H(Y_{i})-H(Y_{i}\\mid X).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now, we note that H(Y i) \u2264H(Y i, i) = H(i)+H(Y i | i) = log2 \u2113+ H(Y 1)+\u00b7\u2113\u00b7\u00b7+H(Y \u2113). We now lower bound $H(Y_{i}\\mid X)$ . Since conditioning always decreases entropy, we obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\nH(Y_{i}\\mid X)\\geq H(Y_{i}\\mid i,X).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "As $\\mathbf{\\deltaX}$ is independent of $\\pmb{i}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\nH(Y_{i}\\mid X)\\geq H(Y_{i}\\mid i,X)={\\frac{H(Y_{1}\\mid X)+\\cdot\\cdot\\cdot+H(Y_{\\ell}\\mid X)}{\\ell}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which then implies ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{I(X\\,;\\,Y_{i})\\leq H(i)+{\\frac{H(Y_{1})+\\,\\cdots+\\,H(Y_{\\ell})}{\\ell}}-{\\frac{H(Y_{1}\\mid X)+\\,\\cdots+\\,H(Y_{\\ell}\\mid X)}{\\ell}}}\\\\ &{\\qquad\\qquad\\leq H(i)+{\\frac{I(X\\,;\\,Y_{1})+\\,\\cdots+\\,I(X\\,;\\,Y_{\\ell})}{\\ell}}.}\\end{array}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $H(i)=\\log_{2}\\ell$ , we have the proof. ", "page_idx": 15}, {"type": "text", "text": "Using this lemma, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I(s_{\\mathrm{mid}};x_{\\pi^{-1}(1)}[(1-\\gamma)\\cdot d+1:d])+\\cdot\\cdot+I(s_{\\mathrm{mid}};x_{\\pi^{-1}(h/2)}[(1-\\gamma)\\cdot d+1:d])}\\\\ &{=(h/2)\\cdot I(s_{\\mathrm{mid}};x_{i}[(1-\\gamma)\\cdot d+1:d]-\\log_{2}(h/2))}\\\\ &{\\geq\\Omega(h d/R)-h\\log_{2}h.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Lemma A.4. If $X,Y$ are independent, then $I(Z\\,;\\,(X,Y))\\ge I(Z\\,;\\,X)+I(Z\\,;\\,Y).$ ", "page_idx": 15}, {"type": "text", "text": "Proof. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I(Z;(X,Y))=H((X,Y))-H((X,Y)\\mid Z)}\\\\ &{\\qquad\\qquad=H(X)+H(Y)-H((X,Y)\\mid Z).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now, we note that for any three random variables $X,Y,Z$ , we have $H((X,Y)\\mid Z)\\leq H(X\\mid$ $Z)+H(Y\\mid Z)$ which proves the lemma. ", "page_idx": 15}, {"type": "text", "text": "Using the independence of $\\boldsymbol{x}_{1},\\ldots,\\boldsymbol{x}_{h}$ conditioned on the event $\\mathcal{E}$ , we obtain ", "page_idx": 15}, {"type": "text", "text": "$I(s_{\\mathrm{mid}};(x_{\\pi^{-1}(1)}[(1-\\gamma)\\cdot d+1:d],\\dots,x_{\\pi^{-1}(h/2)}[(1-\\gamma)\\cdot d+1:d]))\\ge\\Omega(h d/R)-h\\log_{2}h\\,$ which then implies ", "page_idx": 15}, {"type": "equation", "text": "$$\nH(s_{\\mathrm{mid}})\\geq\\Omega(h d/R)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "using the fact that $R^{2}\\cdot h=O(d)$ . Finally, we have max $\\left|\\,s_{\\mathrm{mid}}\\,\\right|\\geq\\Omega(h d/R)$ . Here $|s_{\\mathrm{mid}}|$ is the number of bits used in the representation of the state $s_{\\mathrm{mid}}$ . \u53e3 ", "page_idx": 15}, {"type": "text", "text": "A.5 Proof of Theorem 5.1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. Our instance consists of the following vectors: ", "page_idx": 15}, {"type": "text", "text": "1. $R$ copies of the vector $(1/\\sqrt{R})e_{1}$ ,   \n2. 1 copy of the vector $(1/\\sqrt{R-\\varepsilon})e_{2}$ , and   \n3. $\\alpha$ copies of the vector $(1/{\\sqrt{\\alpha\\cdot R}})e_{3}$ . ", "page_idx": 15}, {"type": "text", "text": "where $\\alpha=2M$ . Let $A$ be a matrix with rows given by the stream of vectors defined above. We note that the matrix $A$ has rank 3 and the non-zero eigenvalues of the matrix $A^{\\mathsf{T}}A$ are $1,1/(R-\\varepsilon),1/R$ and therefore the gap $\\lambda_{1}(A^{\\mathsf{T}}A)/\\lambda_{2}(A^{\\mathsf{T}}A)=R-\\varepsilon.$ The top eigenvector of the matrix $A^{\\mathsf{T}}A$ is $e_{1}$ and the row with the largest norm is $(1/\\sqrt{R-\\varepsilon})e_{2}$ . Thus, the row with the largest norm is not useful to obtain correlation with the true top eigenvector $e_{1}$ . ", "page_idx": 16}, {"type": "text", "text": "Consider an execution of Oja\u2019s algorithm with a learning rate $\\eta$ on the above stream of vectors. The final vector $z_{n}$ can be written as ", "page_idx": 16}, {"type": "equation", "text": "$$\nz_{n}=\\left(I+\\frac{\\eta}{R}e_{1}e_{1}^{\\mathsf{T}}\\right)^{R}\\left(I+\\frac{\\eta}{R\\alpha}e_{3}e_{3}^{\\mathsf{T}}\\right)^{\\alpha}\\left(I+\\frac{1}{R-\\varepsilon}e_{2}e_{2}^{\\mathsf{T}}\\right)v_{0}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For $j\\in[d]$ , let $z_{i j}$ denote the $j$ -th coordinate of the vector $z_{i}$ so that we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle z_{n1}=\\left(1+\\frac{\\eta}{R}\\right)^{R}\\cdot z_{01},}\\\\ {\\displaystyle z_{n2}=\\left(1+\\frac{\\eta}{R-\\varepsilon}\\right)\\cdot z_{02},}&{\\mathrm{and}}\\\\ {\\displaystyle z_{n3}=\\left(1+\\frac{\\eta}{R\\alpha}\\right)^{\\alpha}\\cdot z_{03}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We note that $z_{n j}~=~z_{0j}$ for all $j~>~3$ . Since $\\alpha\\;=\\;2M$ , we have $\\eta/R\\alpha\\;\\leq\\;1/2$ and therefore $(1+\\eta/R\\alpha)\\geq\\mathrm{exp}(\\eta/2\\bar{R}\\alpha)$ and $(1+\\eta/R\\alpha)^{\\alpha}\\ge\\exp(\\eta/2R)$ . ", "page_idx": 16}, {"type": "text", "text": "Recall that we want to show that $|\\langle z_{n},e_{1}\\rangle|\\,<\\,c\\|z_{n}\\|_{2}$ with a large probability. Suppose otherwise and that with probability $\\geq1/10$ , we have $|\\langle z_{n},\\bar{e}_{1}\\rangle|>c\\|z_{n}\\|_{2}>c\\|(0,\\;0,\\;0,\\;z_{04},\\;.\\;.\\;.\\;,\\;z_{0d})\\|_{2}$ . ", "page_idx": 16}, {"type": "text", "text": "Since, $z_{0}$ is initialized to be a random Gaussian, we have $\\|(0,0,0,z_{04},\\ldots,z_{0d})\\|_{2}\\,\\geq\\,\\sqrt{d}/2$ with probability $1-\\exp(-d)$ . Thus, we have with probability $\\geq1/11$ that, ", "page_idx": 16}, {"type": "equation", "text": "$$\n|z_{n1}|\\geq c{\\sqrt{d}}/2\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which implies the learning rate must satisfy ", "page_idx": 16}, {"type": "equation", "text": "$$\n(1+\\eta/R)^{R}\\geq c^{\\prime}\\sqrt{d}/2\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "since $|z_{01}|~\\leq~10$ with probability $\\geq~99/100$ . Hence $\\eta~\\geq~R((c^{\\prime}d^{1/2})^{1/R}\\:-\\:1)$ . Now consider $|\\langle z_{n},e_{3}\\rangle|/|\\langle z_{n},e_{1}\\rangle|$ . We have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{|\\langle z_{n},e_{3}\\rangle|}{|\\langle z_{n},e_{1}\\rangle|}=\\frac{\\exp(\\eta/R)}{(1+\\eta/R)^{R}}\\cdot\\frac{|z_{03}|}{|z_{01}|}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "With probability $\\geq\\,95/100$ , we have $1/C\\,\\leq\\,|z_{03}|/|z_{01}|\\,\\leq\\,C$ for a large enough constant $C$ . We now consider the expression ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\exp(\\eta/R)}{(1+\\eta/R)^{R}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The expression is minimized at $\\eta\\,=\\,R^{2}\\,-\\,R$ and is increasing in the range $\\eta\\,\\in\\,[R^{2}\\,-\\,R,\\infty)$ . When, $R=O(\\log d/\\log\\log d)$ , we have that $R^{2}-R\\leq R((c^{\\prime}d^{1/2})^{1/R}-1)$ and therefore for all $\\eta\\geq R((c^{\\prime}d^{1/2})^{1/R}-1)$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\exp(\\eta/R)}{(1+\\eta/R)^{R}}\\ge\\frac{\\exp((c^{\\prime}d^{1/2})^{1/R})}{e\\cdot c^{\\prime}d^{1/2}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "When $R=O(\\log d/\\log\\log d)$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\exp(\\eta/R)}{(1+\\eta/R)^{R}}\\geq\\mathrm{poly}(d)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which then implies $|\\langle z_{n},e_{3}\\rangle|\\geq|\\langle z_{n},e_{1}\\rangle|\\cdot\\mathrm{poly}(d)/C$ with probability $\\geq95/100$ which contradicts our assumption that $|\\langle z_{n},e_{1}\\rangle|\\geq c\\|z_{n}\\|_{2}$ . \u53e3 ", "page_idx": 16}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: Our paper is purely theoretical studying space-efficient algorithms for approximating the top eigenvector. We prove all the claims made in the abstract and introduction. ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: We do not have a specific limitations section but we do qualify all the statements noting the assumptions that need to be made to prove that our algorithms work. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 17}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We include all the proofs in the main body and the appendix. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: No experimental results are given in this paper. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 19}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 20}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Results in this paper are purely theoretical. While the algorithms proposed in this paper may be used with potentially negative consequences, the authors are unaware of such uses. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 20}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 20}, {"type": "text", "text": "Answer: [No] ", "page_idx": 20}, {"type": "text", "text": "Justification: Our work studies algorithms for the top eigenvector estimation problem. Our work is purely theoretical. While our algorithms may be used to impact society in a negative way, we are unaware of such usecases. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 20}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. ", "page_idx": 21}, {"type": "text", "text": "\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 21}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 21}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 21}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: [NA] Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. ", "page_idx": 21}, {"type": "text", "text": "\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 22}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 22}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 22}]