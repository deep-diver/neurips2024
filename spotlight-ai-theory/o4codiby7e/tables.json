[{"figure_path": "o4coDIby7e/tables/tables_16_1.jpg", "caption": "D.1 Tables of results", "description": "This table presents the results of two experiments conducted to measure known-utility MEG (maximum entropy goal-directedness) with respect to the environment reward function and unknown-utility MEG with respect to a hypothesis class of utility functions. The experiments involved the CliffWorld environment, where the agent aims to reach the top right while avoiding a cliff along the top row.  The first experiment varied the level of randomness in the agent's policy (\u03b5-greedy) and measured MEG for different levels of randomness.  The second experiment kept the agent's policy optimal but changed the difficulty of the task by altering the length of the goal region (k). For each condition, the table reports the average known-utility MEG (single value), and the average unknown-utility MEG (with standard deviation) across multiple runs.", "section": "Experimental Evaluation"}, {"figure_path": "o4coDIby7e/tables/tables_16_2.jpg", "caption": "D.1 Tables of results", "description": "This table presents the results of two experiments conducted to evaluate MEG. The first experiment measured known-utility MEG with respect to the environment reward function for epsilon-greedy policies with varying epsilon values. The second experiment measured unknown-utility MEG for optimal policies with respect to a hypothesis class of utility functions of varying difficulty, controlled by the length of the goal region. The results show that known-utility MEG decreases as the policy gets less optimal and as the goal becomes easier to satisfy.  However, unknown-utility MEG remains high even as the task gets easier, because the optimal policies still perform well on narrower utility functions.", "section": "Experimental Evaluation"}]