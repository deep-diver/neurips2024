[{"figure_path": "qOSFiJdVkZ/tables/tables_15_1.jpg", "caption": "Algorithm 1 Neural Tangent Ensemble posterior update rule with current gradients", "description": "This algorithm describes the process of updating the weights of a neural network using a Bayesian approach. The algorithm interprets the network as an ensemble of classifiers (experts), where each parameter in the network corresponds to an expert.  It updates the weights based on the posterior probability of each expert given the data, which approximates stochastic gradient descent. The algorithm includes steps for calculating the likelihood of each expert given the current example, updating the expert's weight multiplicatively, renormalizing the weights, and optionally clipping the weight changes to prevent excessively large adjustments. The primary goal is to mitigate catastrophic forgetting in continual learning.", "section": "8.3 Pseudocode for the NTE update rule using current gradients"}]