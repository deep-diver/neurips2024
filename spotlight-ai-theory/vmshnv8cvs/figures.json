[{"figure_path": "VMsHnv8cVs/figures/figures_2_1.jpg", "caption": "Figure 1: Overall NeuRes architecture", "description": "The figure shows the overall architecture of the NeuRes model. The input is a CNF formula.  The formula is first processed by a Message-Passing Embedder, which generates embeddings for the clauses and literals. These embeddings are then fed into two parallel tracks: an attention network that selects clause pairs for resolution and an assignment decoder that attempts to find a satisfying assignment. The attention network iteratively selects clause pairs and generates new clauses through resolution. Both tracks operate on the shared formula embedding, and NeuRes outputs either a satisfying assignment or a resolution proof of unsatisfiability.", "section": "4 Models"}, {"figure_path": "VMsHnv8cVs/figures/figures_3_1.jpg", "caption": "Figure 2: Cascaded attention", "description": "This figure illustrates the mechanism of cascaded attention used in the NeuRes model for clause pair selection. It involves two consecutive attention queries on the clause pool. The first query uses the mean of the literal embeddings concatenated with a zero vector to select a clause. The second query, conditioned on the outcome (selected clause) of the first query, uses the mean of the literal embeddings concatenated with the embedding vector of the clause selected in the first query to select a second clause.  The selected pair of clauses is then used for resolution.", "section": "4 Models"}, {"figure_path": "VMsHnv8cVs/figures/figures_4_1.jpg", "caption": "Figure 3: Full self-attention", "description": "This figure illustrates the mechanism of Full Self-Attention. Clause embeddings are input to a self-attention layer, producing an attention matrix. The entry with the maximum attention score determines the clause pair selected for resolution. The figure highlights the selected pair (3, 6) along with the attention matrix.", "section": "4 Models"}, {"figure_path": "VMsHnv8cVs/figures/figures_5_1.jpg", "caption": "Figure 4: Anchored self-attention", "description": "This figure illustrates the Anchored Self-Attention mechanism used in the NeuRes model for clause pair selection.  It shows how an attention mechanism first selects an anchor variable (v) and then uses a self-attention mechanism on clauses containing either the positive (v) or negative (-v) literal of that variable.  The result is a reduced attention grid, improving efficiency by focusing only on relevant clause pairs for resolution.", "section": "4 Models"}, {"figure_path": "VMsHnv8cVs/figures/figures_9_1.jpg", "caption": "Figure 5: SAT success rate over iterations.", "description": "This figure shows the SAT success rate over various iterations for different problem sizes (SR(40), SR(80), SR(120), SR(160), SR(200)).  The x-axis represents the number of iterations, and the y-axis shows the success rate (percentage of problems solved successfully).  It demonstrates the model's scalability and performance on larger problems. The success rate generally increases as the number of iterations increases, indicating the effectiveness of the learning process.", "section": "9 Generalizing to Larger Problems"}, {"figure_path": "VMsHnv8cVs/figures/figures_14_1.jpg", "caption": "Figure 6: Two-phase message-passing round on NeuroSAT formula graph.", "description": "NeuroSAT-style formula graphs have two designated node types: clause nodes connected to the literal nodes corresponding to their constituent literals. Each message-passing round involves two exchange phases: (1) Literal-to-Clause, and (2) Clause-to-Literal. This construction is particularly efficient as it allows the message-passing protocol to cover the entire graph connectivity in at most |V| + 1 rounds where V is the set of variables in the formula.", "section": "4.2 Message-Passing Embedder"}, {"figure_path": "VMsHnv8cVs/figures/figures_16_1.jpg", "caption": "Figure 7: Teacher Proof Reduction Example", "description": "This figure shows an example of how NeuRes, through its bootstrapping mechanism, reduces the length of a resolution proof.  The figure displays two resolution proofs: (a) the original, longer teacher proof generated by a traditional SAT solver; and (b) the shorter proof produced by NeuRes after training with the bootstrapping technique. The reduction in proof length highlights NeuRes's ability to learn and improve upon the teacher algorithm's output, contributing to improved performance and data efficiency.", "section": "6.2 Shortening Teacher Proofs with Bootstrapping"}]