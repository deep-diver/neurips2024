[{"type": "text", "text": "No-Regret Learning in Harmonic Games: Extrapolation in the Face of Conflicting Interests ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Davide Legacci\u2217 Panayotis Mertikopoulos Univ. Grenoble Alpes, CNRS, Inria, Grenoble INP, LIG 38000 Grenoble, France {davide.legacci,panayotis.mertikopoulos}@univ-grenoble-alpes.fr ", "page_idx": 0}, {"type": "text", "text": "Christos Papadimitriou   \nColumbia University, NYC, &   \nArchimedes/Athena RC, Greece christos@columbia.edu ", "page_idx": 0}, {"type": "text", "text": "Georgios Piliouras Google DeepMind London, UK gpil@google.com ", "page_idx": 0}, {"type": "text", "text": "Bary Pradelski CNRS, Maison Fran\u00e7aise d\u2019Oxford 2\u201310 Norham Road, Oxford, OX2 6SE, United Kingdom bary.pradelski@cnrs.fr ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The long-run behavior of multi-agent learning \u2013 and, in particular, no-regret learning \u2013 is relatively well-understood in potential games, where players have common interests. By contrast, in harmonic games \u2013 the strategic counterpart of potential games, where players have conflicting interests \u2013 very little is known outside the narrow subclass of 2-player zero-sum games with a fully-mixed equilibrium. Our paper seeks to partially flil this gap by focusing on the full class of (generalized) harmonic games and examining the convergence properties of follow-the-regularizedleader (FTRL), the most widely studied class of no-regret learning schemes. As a first result, we show that the continuous-time dynamics of FTRL are Poincar\u00e9 recurrent, that is, they return arbitrarily close to their starting point infinitely often, and hence fail to converge. In discrete time, the standard, \u201cvanilla\u201d implementation of FTRL may lead to even worse outcomes, eventually trapping the players in a perpetual cycle of best-responses. However, if FTRL is augmented with a suitable extrapolation step \u2013 which includes as special cases the optimistic and mirror-prox variants of FTRL \u2013 we show that learning converges to a Nash equilibrium from any initial condition, and all players are guaranteed at most ${\\mathcal{O}}(1)$ regret. These results provide an in-depth understanding of no-regret learning in harmonic games, nesting prior work on 2-player zero-sum games, and showing at a high level that harmonic games are the canonical complement of potential games, not only from a strategic, but also from a dynamic viewpoint. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The question of \u201cas if\u201d rationality \u2013 that is, whether selfishly-minded, myopic agents may learn to behave \u201cas if\u201d they were fully rational \u2013 has been one of the cornerstones of non-cooperative game theory, and for good reason. Especially in modern applications of game theory to machine learning and data science \u2013 from online ad auctions to recommender systems and multi-agent reinforcement learning \u2013 the standard postulates of rationality (knowledge of the game, capacity to compute an equilibrium, flawless execution of equilibrium strategies, common knowledge of rationality, etc.) are almost never met in practice; as a result, game-theoretic predictions that rely on these assumptions are likewise put into question. By contrast, given the ease of implementing and deploying cheap, computationally efficient learning algorithms and policies at a large scale, it is often more logical to turn to the policy being deployed as the object of interest. The aim is then to understand its long-run behavior \u2013 and, in particular, whether it ultimately leads to equilibrium. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "A major obstacle in this approach is the complexity of computing a Nash equilibrium, a problem which is known to be complete for PPAD \u2013 and hence intractable \u2013 by the seminal work of Daskalakis et al. [12]. This result implies that it is not plausible to expect any algorithm to converge to Nash equilibrium in all games (at least, not in a reasonable amount of time), so it dovetails naturally with the impossibility results of Hart & Mas-Colell [23, 24] who showed that there are no uncoupled learning dynamics that converge to Nash equilibrium in all games. On that account, it is natural to aim to understand in which classes of games we can expect a learning algorithm to converge, in which classes we cannot, and under what conditions. ", "page_idx": 1}, {"type": "text", "text": "Perhaps the most well-behaved class of games in terms of learning is the class of potential games [44, 54], where players have common interests \u2013 not necessarily driving them to play the same strategy, but in the sense that externalities are symmetric and aligned towards a common objective (the potential of the game). In this class of games, the behavior of learning dynamics \u2013 and, in particular, no-regret learning [8, 10, 13, 19, 20, 28, 29, 37, 40, 43, 58] \u2013 are relatively well understood, and there is a wide range of equilibrium convergence results, from continuous to discrete time, and even with bandit, payoff-based feedback [25, 26, 54]. ", "page_idx": 1}, {"type": "text", "text": "By contrast, in the presence of confilcting interests, the situation can be quite different. In two-player zero-sum games with a fully-mixed equilibrium \u2013 such as Matching Pennies \u2013 the continuous-time dynamics of no-regret, regularized learning are recurrent in the sense of Poincar\u00e9 \u2013 that is, the induced trajectory of play returns arbitrarily close to where it started infinitely many times [41, 48]. In discrete time, the situation becomes more complicated: the vanilla version of follow-the-regularizedleader (FTRL) \u2013 the most widely studied family of no-regret algorithms \u2013 is no longer recurrent, but it diverges away from equilibrium in the same class of games [18, 42]. On the other hand, if players employ an optimistic / extra-gradient variant of FTRL, the induced trajectory of play converges to equilibrium [15, 42] and, under certain conditions, it is even possible to show that it converges at a geometric rate [61]. ", "page_idx": 1}, {"type": "text", "text": "At the same time, zero-sum games may also admit a potential function, so it is not possible to predict the outcome of a learning process based on where it stands along the potential / zero-sum axis. The non-trivial intersection of these classes means that potential and zero-sum games are not complementary, and this, not only from a strategic, but also from a dynamic viewpoint. Instead, the true strategic complement of potential games is the class of harmonic games. This class was first formalized by Candogan et al. [6], who established a remarkable decomposition result: Every game in normal form can be decomposed as the sum of a potential game and a harmonic game, and this decomposition is unique up to affine transformations that do not alter the equilibrium outcomes of the game. In particular, the class of potential and harmonic games intersect trivially (up to strategic equivalence), and all two-player zero-sum games with an interior equilibrium are harmonic, thus lending credence to the fact that it is harmonic games, not zero-sum games, that correctly capture the notion of conflicting interests in this context. ", "page_idx": 1}, {"type": "text", "text": "This raises the question: ", "page_idx": 1}, {"type": "text", "text": "What is the behavior of no-regret algorithms and dynamics in harmonic games? ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Except for a very recent paper by Legacci et al. [36] (which we discuss below), almost nothing is known on this question. With this backdrop, our contributions can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "1. Starting with a continuous-time model of learning, we show that all FTRL dynamics are Poincar\u00e9 recurrent in all harmonic games. This generalizes and extends the recent result of Legacci et al. [36] which proves the same result for the replicator dynamics in uniform harmonic games (a subclass of harmonic games in which the uniform distribution is always a Nash equilibrium). Other than that, our work has no overlap with that of [36].2 ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2. In discrete-time models of learning, the standard implementation of FTRL cannot be expected to converge (since it fails to do so in Matching Pennies, which is a uniform harmonic game). To correct this behavior, we consider a flexible algorithmic template, dubbed extrapolated FTRL (FTRL+), which augments FTRL with a forward-looking, extrapolation step (including as special cases the optimistic and extra-step variants of FTRL, cf. Section 4). We then establish the following results: ", "page_idx": 2}, {"type": "text", "text": "(a) Under extrapolated FTRL, players are guaranteed constant individual regret (so, as a consequence, the players\u2019 empirical frequency of play converges to coarse correlated equilibrium at a rate of $\\mathcal{O}(1/T);$ .3 This should be contrasted with the results of [13, 14] who showed that players can achieve polylogarithmic regret in any game (finite or convex).   \n(b) The induced trajectory of play converges to Nash equilibrium from any initial condition. ", "page_idx": 2}, {"type": "text", "text": "Our results aim to provide an in-depth understanding of no-regret learning in harmonic games, nesting prior work on 2-player zero-sum games \u2013 from Poincar\u00e9 recurrence [41, 48] to constant regret [28] and convergence under optimistic / extra-gradient schemes [11, 15, 18, 42, 61] \u2013 and showing at a high level that harmonic games are the canonical complement of potential games, not only from a strategic, but also from a dynamic, learning viewpoint. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1. Preliminaries on finite games. Throughout our article, we will work with finite games in normal form. Formally, such games consist of $(i)$ a finite set of players $i\\in\\mathcal{N}\\equiv\\{1,\\dots,N\\}$ ; $(i i)$ a finite set of actions \u2013 or pure strategies \u2013 $\\mathcal{A}_{i}$ per player $i\\,\\in\\,{\\mathcal{N}}$ ; and $(i i i)$ an ensemble of payoff functions $u_{i}\\colon\\,\\Pi_{j}\\,\\mathcal{A}_{j}\\,\\rightarrow\\,\\mathbb{R}$ , each determining the reward $u_{i}(\\alpha)$ of player $i\\in\\mathcal{N}$ in a given action profile $\\alpha\\,=\\,(\\alpha_{1},\\ldots,\\alpha_{N})$ . Putting everything together, we will write $\\textstyle A:=\\prod_{i}A_{i}$ for the game\u2019s action space and $\\Gamma\\equiv\\Gamma(\\mathcal{N},\\mathcal{A},u)$ for the game with primitives as above. ", "page_idx": 2}, {"type": "text", "text": "During play, each player selects an action according to some mixed strategy, that is, a probability distribution $x_{i}$ over $\\mathcal{A}_{i}$ which assigns probability $x_{i\\alpha_{i}}$ to $\\alpha_{i}\\in\\mathcal{A}_{i}$ . We will write $\\mathcal{X}_{i}:=\\Delta(\\bar{\\mathcal{A}}_{i})\\subseteq\\mathbb{R}^{\\bar{A}_{i}}$ for the mixed strategy space of player $i,x=(x_{1},\\dots,x_{N})$ for the associated strategy proflie collecting the strategies of all players, and $\\mathcal X:=\\Pi_{i}\\,\\mathcal X_{i}$ for the game\u2019s strategy space. The mixed payoff of player $i$ under $x$ may then be written as ", "page_idx": 2}, {"type": "equation", "text": "$$\nu_{i}(x)=\\mathbb{E}_{\\alpha\\sim x}[u_{i}(\\alpha)]=\\sum_{\\alpha\\in\\mathcal{A}}u_{i}(\\alpha)\\,x_{\\alpha}=\\sum_{\\alpha_{i}\\in\\mathcal{A}_{i}}u_{i}(\\alpha_{i};x_{-i})\\,x_{i\\alpha_{i}}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\begin{array}{r}{x_{\\alpha}:=\\prod_{i}x_{i\\alpha_{i}}}\\end{array}$ denotes the joint probability of $\\alpha=(\\alpha_{1},...,\\alpha_{N})\\in\\mathcal{A}$ under $x\\in\\mathcal{X}$ , and, in standard gam e-theoretic notation, we write $(x_{i};x_{-i})\\,=\\,(x_{1},\\ldots,x_{i},\\ldots,x_{N})$ for the profile where player $i$ plays $x_{i}\\ \\in\\ \\mathcal X_{i}$ against the strategy $x_{-i}\\,\\in\\,\\mathcal{X}_{-i}\\,:=\\,\\prod_{j\\neq i}\\mathcal{X}_{j}$ of all other players. We also respectively define the individual payoff field of player $i$ and the game\u2019s payoff field as ", "page_idx": 2}, {"type": "equation", "text": "$$\nv_{i}(x)=\\left(u_{i}(\\alpha_{i};x_{-i})\\right)_{\\alpha_{i}\\in\\mathcal{A}_{i}}\\quad\\mathrm{and}\\quad v(x)=\\left(v_{1}(x),\\ldots,v_{N}(x)\\right)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "so $\\begin{array}{r}{u_{i}(x)=\\sum_{\\alpha_{i}\\in\\mathcal{A}_{i}}v_{i\\alpha_{i}}(x)x_{i\\alpha_{i}}\\equiv\\langle v_{i}(x),x_{i}\\rangle}\\end{array}$ , where $\\langle\\cdot,\\cdot\\rangle$ is the standard duality pairing on $\\mathbb{R}^{\\mathcal{A}_{i}}$ . By multilinearity, each player\u2019s individual payoff field is Lipschitz continuous on $\\mathcal{X}$ , and we will write $G_{i}$ for its Lipschitz modulus, that is ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\|v_{i}(x^{\\prime})-v_{i}(x)\\|_{*}\\leq G_{i}\\|x^{\\prime}-x\\|\\quad{\\mathrm{for~all}}\\,x,x^{\\prime}\\in{\\mathcal{X}}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Remark. In the above, $\\lVert\\cdot\\rVert$ denotes an ambient norm on $\\mathbb{R}^{\\mathcal{A}_{i}}$ (usually the $L^{1}\\;\\mathrm{norm}\\rangle$ ), and $\\left\\|\\cdot\\right\\|_{*}$ is the corresponding dual norm (usually the $L^{\\infty}$ norm). We discuss the relevant details in Appendix B. ", "page_idx": 2}, {"type": "text", "text": "We will focus almost exclusively on the solution concept of the Nash equilibrium (NE), i.e., a strategy profile $x^{*}\\in\\mathcal{X}$ that is unilaterally stable in the sense that ", "page_idx": 3}, {"type": "equation", "text": "$$\nu_{i}(x^{*})\\geq u_{i}(x_{i};x_{-i}^{*})\\quad\\mathrm{{for\\,all}}\\;x_{i}\\in\\mathcal{X}_{i},i\\in\\mathcal{N}\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Equivalently, (NE) can be expressed in terms of the game\u2019s payoff field as a variational inequality of the form ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left\\langle v(x^{*}),x-x^{*}\\right\\rangle\\leq0\\quad{\\mathrm{for~all}}\\;x\\in{\\mathcal{X}}\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Thus, writing $\\operatorname{supp}(x_{i}^{*})\\,=\\,\\{\\alpha_{i}\\,\\in\\,\\mathcal{A}_{i}\\,:\\,x_{i\\alpha_{i}}\\,>\\,0\\}$ for the support of $x_{i}^{*}$ , it follows that $x^{*}$ is a Nash equilibrium if and only if $u_{i}(\\alpha_{i};x_{-i}^{*})\\geq u_{i}(\\beta_{i};x_{-i}^{*})$ for all $\\alpha_{i}\\in\\mathrm{supp}(\\dot{x_{i}^{*}})$ and all $\\beta_{i}\\in\\mathcal A_{i}$ , $i\\in\\mathcal{N}$ . We will use all this freely in the rest of our paper. ", "page_idx": 3}, {"type": "text", "text": "2.2. Harmonic games. Our main focus in what follows will be the class of harmonic games, first introduced by Candogan et al. [6] as a game-theoretic framework for modeling strategic situations with conflicting, anti-aligned interests. Specifically, as was shown by Candogan et al. [6] \u2013 and, in a more general setting, by Abdou et al. [1] \u2013 every game in normal form can be decomposed as the sum of a potential game and a harmonic game, and this decomposition is unique up to affine transformations that do not alter the equilibrium outcomes of the game.4 In this decomposition, the potential component of a game captures multi-agent strategic interactions with common interests, whereas the harmonic component covers interactions with conflicting interests.5 ", "page_idx": 3}, {"type": "text", "text": "Formally, adapting the more general setup by Abdou et al. [1], we have the following definition: ", "page_idx": 3}, {"type": "text", "text": "Definition 1. A finite game $\\Gamma\\,\\equiv\\,\\Gamma(\\mathcal{N},\\mathcal{A},u)$ is said to be harmonic when it admits a harmonic measure, i.e., a collection of weights $\\mu_{i\\alpha_{i}}\\in(0,\\infty),\\,\\alpha_{i}\\in\\mathcal{A}_{i},\\,i\\in\\mathcal{N}$ , such that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\sum_{i\\in{\\mathcal{N}}}\\sum_{\\beta_{i}\\in A_{i}}\\mu_{i\\beta_{i}}[u_{i}(\\alpha_{i};\\alpha_{-i})-u_{i}(\\beta_{i};\\alpha_{-i})]=0\\quad{\\mathrm{for~all~}}\\alpha\\in{\\mathcal{A}}\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In particular, if $\\Gamma$ is harmonic relative to the uniform measure $\\mu_{i\\alpha_{i}}=1$ , $\\alpha_{i}\\in\\mathcal{A}_{i}$ , $i\\in\\mathcal{N}$ , we will say that $\\Gamma$ is a uniform harmonic game (UHG). ", "page_idx": 3}, {"type": "text", "text": "Remark. With regard to terminology, Candogan et al. [6] actually call \u201charmonic games\u201d what we call \u201cuniform harmonic games\u201d, and Abdou et al. [1] call ${}^{\\bullet\\bullet}\\mu$ -harmonic games\u201d what we call \u201charmonic games\u201d.6 We use this convention because it simultaneously simplifies notation and terminology while capturing all relevant strategic features of the game; for a detailed discussion, see Appendix A. To avoid needless repetition in the sequel, and unless there is danger in confusion, when we say that $\\Gamma$ is harmonic, we will write $\\mu_{i}$ for the corresponding measure, and we will write $\\begin{array}{r}{m_{i}=\\left|\\mu_{i}\\right|=\\sum_{\\beta_{i}\\in\\mathcal{A}_{i}}\\mu_{i\\beta_{i}}}\\end{array}$ for the total mass of $\\mu_{i}$ . $\\diamond$ ", "page_idx": 3}, {"type": "text", "text": "Broadly speaking, in harmonic games, for any player considering a deviation toward a specific pure strategy profile, there exist other players with an incentive to deviate away from said profile. In this regard, harmonic games can be seen as the strategic complement of potential games, where player interests are aligned and sequences of unilateral best responses generate a finite improvement path that terminates at a pure Nash equilibrium [44]. By contrast, except for trivial cases (like the zero game) harmonic games do not admit pure equilibria, and they possess non-terminating best-response paths. For all these reasons, harmonic games can be considered as \u201corthogonal\u201d to potential games, in a sense made precise by the decomposition results of Candogan et al. [6] and Abdou et al. [1]. ", "page_idx": 3}, {"type": "text", "text": "It is of course natural to ask what is the relation between harmonic games and zero-sum games. Games belonging to the latter class \u2013 such as Matching Pennies and Rock-Paper-Scissors \u2013 have long been used as prototypical examples of strategic confilct; at the same time, there are zero-sum games that are also potential (and even possess strict equilibria), so the potential / zero-sum distinction does not capture the whole picture. As a matter of fact, it is not a coincidence that the textbook examples of zero-sum games admit fully-mixed Nash equilibria: as we discuss in Appendix A, two-player zero-sum games with an interior Nash equilibrium are harmonic, so the existing results for such games are, in a sense, more closely attuned to their harmonic character. ", "page_idx": 3}, {"type": "text", "text": "3 Continuous-time analysis: Poincar\u00e9 recurrence ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The most basic rationality postulate in the context of online learning is the minimization of a player\u2019s regret, i.e., the difference between a player\u2019s cumulative payoff and that of the player\u2019s best possible strategy in hindsight. In more detail, assuming for the moment that play evolves in continuous time, the regret of player $i\\in\\mathcal{N}$ relative to a sequence of play $x(t)\\in\\mathcal{X}$ is defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname{Reg}_{i}(T)=\\operatorname*{max}_{p_{i}\\in\\mathcal{X}_{i}}\\int_{0}^{T}[u_{i}(p_{i};x_{-i}(t))-u_{i}(x(t))]\\;d t\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and we say that the player has no regret under $x(t)$ if $\\operatorname{Reg}_{i}(T)=o(T)$ as $T\\to\\infty$ . ", "page_idx": 4}, {"type": "text", "text": "The most widely used scheme for attaining no regret is the family of policies known as follow-theregularized-leader (FTRL) [56, 57]. At a high level, the idea behind FTRL is that, at all times $t\\geq0$ , each player $i\\in\\mathcal{N}$ plays a mixed strategy $x_{i}(t)\\in\\mathcal{X}_{i}$ that maximizes the player\u2019s cumulative payoff up to time $t$ minus a certain regularization penalty. In our continuous-time setting, this gives rise to the FTRL dynamics ", "page_idx": 4}, {"type": "equation", "text": "$$\nx_{i}(t)=\\underset{p_{i}\\in\\mathcal{X}_{i}}{\\arg\\operatorname*{max}}\\left\\{\\int_{0}^{t}u_{i}(p_{i};x_{-i}(\\tau))\\ d\\tau-h_{i}(p_{i})\\right\\}=\\underset{p_{i}\\in\\mathcal{X}_{i}}{\\arg\\operatorname*{max}}\\left\\{\\int_{0}^{t}\\langle v_{i}(x(\\tau)),p_{i}\\rangle\\ d\\tau-h_{i}(p_{i})\\right\\}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "or, more compactly, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\dot{y}_{i}(t)=v_{i}(x(t))\\qquad x_{i}(t)=Q_{i}(y_{i}(t))\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $h_{i}\\colon\\mathcal{X}_{i}\\to\\mathbb{R}$ is a convex penalty function known as the regularizer of the method, $Q_{i}$ denotes the regularized choice map of player $i$ , and $Q=(Q_{1},\\dots,Q_{N})$ denotes the proflie thereof. Formally, writing $\\mathcal{D}_{i}\\,\\equiv\\,\\mathbb{R}^{A_{i}}$ for the payoff space of player $i\\in\\mathcal{N}-$ that is, the space of all possible payoff vectors $v_{i}$ of player $i-$ the regularized choice map $Q_{i}\\colon\\mathcal{Y}_{i}\\rightarrow\\mathcal{X}_{i}$ is defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Q_{i}(y_{i})=\\arg\\operatorname*{max}_{x_{i}\\in\\mathcal{X}_{i}}\\{\\langle y_{i},x_{i}\\rangle-h_{i}(x_{i})\\}\\quad\\mathrm{for~all~}y_{i}\\in\\mathcal{Y}_{i}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In essence, $Q_{i}$ is a \u201csoft\u201d version of the arg max correspondence $y_{i}\\mapsto\\arg\\operatorname*{max}_{x_{i}\\in\\mathcal{X}_{i}}\\langle y_{i},x_{i}\\rangle$ , suitably regularized by a penalty term intended to incentivize exploration. For technical reasons, we will also assume that the regularizers $h_{i}$ are strongly convex, i.e., ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h_{i}(x_{i})\\geq h_{i}(p_{i})+\\partial h_{i}(p_{i};x_{i}-p_{i})+\\frac{1}{2}K_{i}\\|x_{i}-p_{i}\\|^{2}\\quad\\mathrm{for~all~}p_{i},x_{i}\\in\\mathcal{X}_{i},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $(a)$ we write $\\begin{array}{r}{\\partial h_{i}(p_{i};x_{i}-p_{i})=\\operatorname*{lim}_{\\theta\\rightarrow0^{+}}\\big[h_{i}(p_{i}+\\theta(x_{i}-p_{i}))-h_{i}(p_{i})\\big]/\\theta}\\end{array}$ for the one-sided directional derivative of $h_{i}$ at $p_{i}$ along $x_{i}-p_{i};(b)\\parallel\\cdot\\parallel$ is an ambient norm on $\\bar{\\mathbb{R}}^{\\bar{A}_{i}}$ ; and $\\left(c\\right)K_{i}>0$ is a positive constant (commonly referred to as the strong convexity modulus of $h_{i}$ ). ", "page_idx": 4}, {"type": "text", "text": "The go-to example of this setup is the entropic regularizer ", "page_idx": 4}, {"type": "equation", "text": "$$\nh_{i}(x_{i})=\\sum_{\\alpha_{i}\\in\\mathcal{A}_{i}}x_{i\\alpha_{i}}\\log x_{i\\alpha_{i}}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which yields the so-called logit choice map ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Q_{i}(y_{i})\\equiv\\Lambda_{i}(y_{i}):=\\frac{(\\exp(y_{i\\,\\alpha_{i}}))_{\\alpha_{i}\\in\\cal A_{i}}}{\\sum_{\\alpha_{i}\\in\\cal A_{i}}\\exp(y_{i\\,\\alpha_{i}})}\\quad\\mathrm{for\\,all\\,}y_{i}\\in\\mathcal{Y}_{i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "By Pinsker\u2019s inequality, the entropic regularizer is 1-strongly convex relative to the $L^{1}$ -norm on $\\mathcal{X}_{i}$ [56], and by a standard calculation, the induced sytem (FTRL-D) boils down to the replicator dynamics of Taylor $\\&$ Jonker [59]. Some other standard examples of (FTRL-D) include the Euclidean projection dynamics of Friedman [17] when $h_{i}(x_{i})=(1/2)\\lVert\\dot{x}_{i}\\rVert_{2}^{2}$ , the $q$ -replicator dynamics [22, 38], etc. To streamline our presentation, we defer a detailed discussion of these examples to Appendix C, and we proceed below to state the main regret guarantee of (FTRL-D), originally due to [34]: ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. Under (FTRL-D), each player\u2019s regret is bounded as $\\operatorname{Reg}_{i}(T)\\leq H_{i}:=\\operatorname*{max}h_{i}-\\operatorname*{min}h_{i}$ ", "page_idx": 4}, {"type": "text", "text": "Theorem 1 showcases the excellent no-regret properties of (FTRL-D): it is not possible to guarantee less than constant, ${\\mathcal{O}}(1)$ regret, so (FTRL-D) is optimal in this regard. In turn, by standard results [47], Theorem 1 implies further that the players\u2019 (correlated) empirical frequencies $z_{\\alpha_{1},\\dots,\\alpha_{N}}(t):=$ $\\begin{array}{r}{(1/t)\\int_{0}^{t}\\prod_{i}x_{i\\alpha_{i}}(\\tau)\\,d\\tau}\\end{array}$ converge to the game\u2019s set of coarse correlated equilibria (CCE) at a likewise optimal rate of $\\mathcal{O}(1/t)$ . Importantly, this result makes no assumptions about the underlying game, but it does not carry the same predictive power in all games: for one thing, a game\u2019s set of CCE may include highly non-rationalizble outcomes (such as dominated strategies and the like) [60]; for another, the time-averaging that is inherent in the definition of empirical distributions may conceal a wide range of non-convergence phenomena, from limit cycles to chaos [41, 48, 55]. On that account, the day-to-day behavior of (FTRL-D) in harmonic games cannot be understood from Theorem 1 alone, and requires a closer, more specialized look. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Our first result below provides such a lense and shows that (FTRL-D) is almost-periodic in harmonic games, a property known as Poincar\u00e9 recurrence. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2. Suppose $\\Gamma$ is harmonic. Then almost every orbit $x(t)$ of (FTRL-D) returns arbitrarily close to its starting point infinitely often: specifically, for (Lebesgue) almost every initial condition $x(0)=Q(y(0))\\in{\\bar{\\mathcal{X}}}$ , there exists an increasing sequence of times $t_{n}\\uparrow\\infty$ such that $x(t_{n})\\to x(0)$ . ", "page_idx": 5}, {"type": "text", "text": "An immediate consequence of Theorem 2 is that no-regret learning under (FTRL-D) fails to converge in any harmonic game; in particular, since the orbits of (FTRL-D) eventually return to (almost) where they started, it is debatable if the players have learned anything at all, despite the fact that they incur at most constant regret. This cyclic, non-convergent landscape is the polar opposite of the long-run behavior of (FTRL-D) in potential games, where the dynamics are known to converge globally [25]. Thus, in addition to the strategic viewpoint of the previous section, Theorem 2 shows that harmonic games are complementary to potential games also from a dynamic viewpoint. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2 also provides a far-reaching generalization of existing results on Poincar\u00e9 recurrence in (possibly networked) two-player zero-sum games with an interior equilibrium [41] to general-sum, $N$ -player games. Combined with our previous remark, and given that the zero-sum property is not as meaningful for $N$ players as it is for two,7the class of harmonic games can be seen as the more natural $N$ -player generalization of two-player zero-sum games from a learning viewpoint. ", "page_idx": 5}, {"type": "text", "text": "To the best of our knowledge, the only comparable result to Theorem 2 in the literature is the very recent paper of Legacci et al. [36] who showed that the replicator dynamics \u2013 a special case of (FTRL-D) \u2013 are Poincar\u00e9 recurrent in uniform harmonic games, that is, in harmonic games where the uniform distribution is a Nash equilibrium, cf. Eq. (A.1) and the discussion surrounding Definition 1. In this regard, Theorem 2 extends [36] along two axes: $(i)$ it applies to the entire class of FTRL dynamics (not only the replicator dynamics); and $(i i)$ it applies to the entire class of harmonic games (not only uniformly harmonic games). ", "page_idx": 5}, {"type": "text", "text": "In terms of techniques, Legacci et al. [36] obtained their result through a surprising connection between a certain Riemannian metric underlying the replicator dynamics and the defining relation of uniformly harmonic games. This relation no longer holds for different instances of (FTRL-D) or for non-uniform harmonic games, so the techniques of [36] cannot be extended \u2013 and, in fact, Legacci et al. [36] stated this generalization as an open problem. Our techniques instead rely on the fact that the orbits $y(t)$ of (FTRL-D) comprise a volume-preserving flow in the game\u2019s payoff space $\\mathcal{V}\\equiv\\Pi_{i}\\,\\mathcal{V}_{i}$ (though not necessarily on $\\mathcal{X}$ ), and then deriving a suitable constant of motion. In the case of the logit map (9), this constant of motion can be written as ", "page_idx": 5}, {"type": "equation", "text": "$$\nG(x)=\\prod_{i\\in{\\mathcal{N}}}\\prod_{\\alpha_{i}\\in{\\mathcal{A}}_{i}}x_{i\\alpha_{i}}^{\\mu_{i\\alpha_{i}}}\\qquad{\\mathrm{~for~all~}}x\\in{\\mathcal{X}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mu=(\\mu_{i\\alpha_{i}})_{\\alpha_{i}\\in\\mathcal{A}_{i},i\\in\\mathcal{N}}$ is the harmonic measure on $\\mathcal{X}$ defining $\\Gamma$ . In the more general case, the construction of a constant of motion for (FTRL-D) involves a characterization of harmonic games in terms of a \u201cstrategic center\u201d, which we carry out in detail in Appendix C. ", "page_idx": 5}, {"type": "text", "text": "4 Discrete-time analysis: Convergence and constant regret via extrapolation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We proceed to examine the long-run convergence properties of regularized learning algorithms in harmonic games. Starting with the standard, vanilla implementation of FTRL, we reproduce a well-known observation that FTRL spirals out to a non-terminating cycle of best-responses in Matching Pennies (which is a harmonic game). Subsequently, to correct this non-convergent behavior, we examine a flexible, extrapolation-based algorithmic template, which we call extrapolated FTRL $\\mathrm{(FTRL+)}$ ), and which includes as special cases the optimistic and extra-gradient versions of FTRL. ", "page_idx": 5}, {"type": "text", "text": "4.1. Vanilla implementation of FTRL. Building on the discussion of the previous section, the standard implementation of FTRL in discrete time for $n=1,2,\\ldots$ is ", "page_idx": 6}, {"type": "equation", "text": "$$\nx_{i,n+1}=\\underset{p_{i}\\in\\mathcal{X}_{i}}{\\arg\\operatorname*{max}}\\left\\{\\sum_{s=1}^{n}u_{i}(p_{i};x_{-i,n})-\\varrho_{i}h_{i}(p_{i})\\right\\}=\\underset{p_{i}\\in\\mathcal{X}_{i}}{\\arg\\operatorname*{max}}\\left\\{\\sum_{s=1}^{n}\\langle v_{i}(x_{s}),p_{i}\\rangle-\\varrho_{i}h_{i}(p_{i})\\right\\}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "or, in more compact, iterative notation ", "page_idx": 6}, {"type": "equation", "text": "$$\ny_{i,n+1}=y_{i,n}+\\eta_{i}v_{i}(x_{n})\\qquad x_{i,n}=Q_{i}(y_{i,n})\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where, as per (6), the map $Q_{i}\\colon\\mathcal{Y}_{i}\\rightarrow\\mathcal{X}_{i}$ denotes the regularized choice map of player $i\\in\\mathcal{N}$ , $\\varrho_{i}$ is a player-specific regularization weight parameter, and $\\eta_{i}=1/\\varrho_{i}$ represents the learning rate of player $i$ . Apart from their obvious differences \u2013 discrete vs. continuous time \u2013 a salient point that sets (FTRL) apart from (FTRL-D) is the inclusion of the parameter $\\eta_{i}$ ; this parameter is necessary to control the algorithm\u2019s behavior, and we will discuss it in detail in the sequel. ", "page_idx": 6}, {"type": "text", "text": "As mentioned in the introduction, a major shortfall of (FTRL) \u2013 and one of the main reasons for the increased popularity of optimistic / extra-gradient methods \u2013 is that it may spiral away from Nash equilibrium, even in simple $2\\times2$ games with a unique equilibrium. The standard example of this behavior is Matching Pennies, a two-player zero-sum game with a fully-mixed equilibrium which is also uniformly harmonic, so the trajectories of (FTRL-D) are Poincar\u00e9 recurrent (and, in fact, periodic). In more detail, this game can be compactly represented by the payoff field $v(x_{1},x_{2})=(4x_{2}-2,2-4x_{1})$ for $x_{1},x_{2}\\in[0,1]$ , and its unique Nash equilibrium is $x^{*}=(1/2,1/2)$ Thus, if we run (FTRL) with a Euclidean reqularizer \u2013 that is, $h_{i}(x_{i})\\dot{)}\\,=\\,x_{i}^{2}/2$ for $i\\,=\\,1,2\\,-$ and the same learning rate $\\eta$ per player, a straightforward calculation shows that the distance $D_{n}\\ =$ $(x_{1,n}-x_{1}^{*})^{2}/2+\\bar{(x}_{2,n}-\\overset{..}{x_{2}^{*}})^{2}/2$ between $x_{n}$ and $x^{*}$ evolves as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{D_{n+1}=\\frac{1}{2}(x_{1,n}+\\eta v_{1}(x_{n})-x_{1}^{*})^{2}+\\frac{1}{2}(x_{2,n}+\\eta v_{2}(x_{n})-x_{2}^{*})^{2}=(1+16\\eta^{2})D_{n}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "as long as $x_{n}+\\eta v(x_{n})\\in\\mathcal{X}$ . In other words, the distance of the iterates of (FTRL) from the game\u2019s equilibrium grows at a geometric rate until $x_{n}$ reaches the boundary of $\\mathcal{X}$ and is ultimately trapped in a non-terminating cycle of best responses, cf. Fig. 1. In this regard, the rationality properties of (FTRL) are even worse than those of (FTRL-D) because the game\u2019s equilibrium is now repelling. ", "page_idx": 6}, {"type": "text", "text": "4.2. Extrapolated FTRL. To mitigate this undesirable, divergent behavior of (FTRL), a standard approach in the literature is the inclusion of a forward-looking, \u201cextrapolation step\u201d. Instead of updating the algorithm\u2019s \u201cbase state\u201d $x_{n}$ directly, players first move to an interim \u201cleading state\u201d $x_{n+1/2}$ using payoff information from $x_{n}$ (this is the extrapolation step); subsequently, players update $x_{n}$ using payoff information from the leading state $x_{n+1/2}$ , and the process repeats. In this way, players attempt to anticipate their payoff landscape and, in so doing, to take a more informed update step at each iteration. ", "page_idx": 6}, {"type": "text", "text": "The seed of this idea goes back to Korpelevich [33] and Popov [49] in the context of solving monotone variational inequality problems, and it has since percolated to a wide array of \u201cextra-gradient\u201d or \u201coptimistic\u201d methods, such as the mirror-prox algorithm of Nemirovski [45], the dual extrapolation variant of Nesterov [46], the optimistic mirror descent algorithm of Chiang et al. [9] and Rakhlin & Sridharan [50], and many others. Given the different operational envelope of each of these methods, we consider below an integrated algorithmic template which is sufficiently flexible to account for a broad range of these schemes, which we call extrapolated FTRL (FTRL+). ", "page_idx": 6}, {"type": "text", "text": "Formally, the proposed algorithmic blueprint unfolds in two phases as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{y_{i,n+1/2}=y_{i,n}+\\eta_{i}\\hat{v}_{i,n}\\quad\\quad}&{{}x_{i,n+1/2}=Q_{i}(y_{i,n+1/2})}\\\\ {y_{i,n+1}=y_{i,n}+\\eta_{i}\\hat{v}_{i,n+1/2}\\quad\\quad}&{{}x_{i,n+1}=Q_{i}(y_{i,n+1})}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In the above, $\\eta_{i}>0$ is the learning rate of player $i$ , $x_{n}$ and $x_{n+1/2}$ respectively denote the method\u2019s base and leading states at stage $n=1,2,\\ldots$ , and $\\hat{v}_{i,n}$ and $\\hat{v}_{i,n+1/2}$ are sequences of black-box \u201cpayoff signals\u201d that model different update structures. Specifically, we will assume throughout that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\hat{v}_{i,n+1/2}=v_{i}(x_{n+1/2})\\quad\\mathrm{for}\\;\\mathrm{all}\\;i\\in\\mathcal{N}\\;\\mathrm{and}\\;\\mathrm{all}\\;n=1,2,\\dots.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "i.e., players always update their base state $x_{n}$ using payoff information from the leading state $x_{n+1/2}$ By contrast, the leading state $x_{n+1/2}$ can be generated in many different ways, depending on the targeted update structure; for concreteness, building on an idea of Azizian et al. [3], we will employ a linear model of the form ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\hat{v}_{i,n}=p_{i}\\,v_{i}(x_{n})+q_{i}\\,v_{i}(x_{n-1/2})\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where the player-specific coefficients $p_{i},q_{i}\\ge0$ satisfy $p_{i}+q_{i}\\leq1$ and represent a mix of past and present payoff information. In this way, depending on the values of $p_{i}$ and $q_{i}$ , we obtain the following prototypical regularized learning methods as special cases of $\\mathrm{(FTRL+)}$ ): ", "page_idx": 7}, {"type": "text", "text": "a) FTRL: if $q_{i}=p_{i}=0$ for all $i\\in\\mathcal{N}$ , players essentially forego any look-ahead efforts, so we get ", "page_idx": 7}, {"type": "equation", "text": "$$\n{\\hat{v}}_{n}=0\\qquad\\qquad{\\mathrm{for~all~}}n=1,2,\\ldots\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "In turn, this gives $x_{n+1/2}=x_{n}$ , i.e., $\\mathrm{(FTRL+)}$ ) regresses to (FTRL). ", "page_idx": 7}, {"type": "text", "text": "b) Extra-step FTRL: if $p_{i}=1$ and $q_{i}=0$ for all $i\\in\\mathcal{N}$ , we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n{\\hat{v}}_{n}=v(x_{n})\\qquad{\\mathrm{~for~all~}}n=1,2,\\ldots\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "i.e., players use payoff information from their current state to generate the leading state $x_{n+1/2}$ This update structure requires two payoff queries per iteration and, depending on the choice of $h_{i}$ , it is essentially equivalent to the mirror-prox [45] and dual extrapolation [46] algorithms, it contains as a special case the forward-looking algorithm of [15, 42], etc. ", "page_idx": 7}, {"type": "text", "text": "c) Optimistic FTRL: if $p_{i}=0$ and $q_{i}=1$ for all $i\\in\\mathcal{N}$ , we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n{\\hat{v}}_{n}=v(x_{n-1/2})\\quad{\\mathrm{for~all}}\\;n=1,2,\\ldots\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "i.e., players reuse the latest available payoff information instead of making a fresh query at $x_{n}$ (so the algorithm only requires one payoff query per iteration). In this way, $(\\mathrm{FTRL+})$ recovers the optimistic algorithms of [9, 27, 50, 58], the OMW update scheme of [11, 58] when $Q=\\Lambda$ , etc. ", "page_idx": 7}, {"type": "text", "text": "Clearly, the list above is not exhaustive: many other configurations are possible, and different players can use different parameter settings for $p_{i}$ and $q_{i}$ , depending on the information they have at hand and any other individual considerations. To avoid needlessly complicating the analysis, our only standing assumption will be that $p_{i}+q_{i}>0$ for all $i\\in\\mathcal{N}$ (since, otherwise, the benefits of the extrapolation step would vanish). In particular, by rescaling the players\u2019 learning rates if needed, we will normalize $p_{i}$ and $q_{i}$ to $p_{i}+q_{i}=1$ , leading to the convex signal model ", "page_idx": 7}, {"type": "equation", "text": "$$\n{\\hat{v}}_{i,n}=\\lambda_{i}\\,v_{i}(x_{n})+(1-\\lambda_{i})\\,v_{i}(x_{n-1/2})\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "for some arbitrarily chosen ensemble of player-specific extrapolation coefficients $\\lambda_{i}\\in[0,1],i\\in\\mathcal{N}$ Remark. To simplify the presentation of our results, we will assume throughout the rest of our paper that $\\mathrm{(FTRL+)}$ ) is initialized with $y_{1}=y_{1/2}=0$ . ", "page_idx": 7}, {"type": "text", "text": "4.3. Analysis $\\&$ results. With all this in hand, we are finally in a position to state our main results for $\\left(\\mathrm{FTRL+}\\right)$ ) in harmonic games. We begin by showing that $(\\mathrm{FTRL+})$ achieves order-optimal regret: Theorem 3. Suppose that each player in a harmonic game $\\Gamma$ is following (FTRL $^+$ ) with learning rate $\\eta_{i}\\leq m_{i}K_{i}\\lbrack\\bar{2(}N\\!+\\!2)\\operatorname*{max}_{j}\\bar{m_{j}}\\bar{G_{j}}\\rbrack^{-1}$ and payoff signals as per (13) and (16). Then the individual regret of each player $i\\in\\mathcal{N}$ is bounded as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathrm{Reg}_{i}(T):=\\operatorname*{max}_{p_{i}\\in\\mathcal{X}_{i}}\\sum_{n=1}^{T}[u_{i}(p_{i};x_{-i,n})-u_{i}(x_{n})]\\le\\frac{H_{i}}{\\eta_{i}}+\\frac{2G_{i}}{N+2}\\sum_{j\\in\\mathcal{N}}\\frac{H_{j}}{\\eta_{j}G_{j}}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $H_{i}=\\operatorname*{max}h_{i}-\\operatorname*{min}h_{i},$ , and $G_{i}$ is the Lipschitz modulus of $v_{i}$ . ", "page_idx": 7}, {"type": "text", "text": "Even though Theorem 3 invites a natural comparison with the constant regret bound of Theorem 1, the continuous- and discrete-time settings are fundamentally different, so any conclusions drawn from such a comparison would be specious. Indeed, constant regret guarantees in the spirit of (17) are particularly rare in the context of discrete-time algorithms, and as far as we are aware, similar bounds have only been established for optimistic methods in variationally stable and two-player zero-sum games [28]; other than that, and to the best of our knowledge, the tightest regret bounds available for general games (finite or convex) seem to be (poly)logarithmic [13, 14]. In this regard, just like the recurrence result of Theorem 2, the $\\mathcal{O}(1)$ regret bound of Theorem 3 represents a significant extension of existing results on zero-sum games (and polylogarithmic regret in general games), and suggests that, from a learning viewpoint, harmonic games are the most natural generalization of two-player zero-sum games to a general $N$ -player context. We defer the proof of Theorem 3 to Appendix D. ", "page_idx": 7}, {"type": "image", "img_path": "HW9S9vY5gZ/tmp/35a4ebfc2f5bb17e32b8a8bb1da88de63513a8a2bf25d97cd5a35dc86e7837be.jpg", "img_caption": ["Figure 1: The evolution of vanilla vs. extrapolated FTRL schemes in harmonic games. In the left figure, we consider the game of Matching Pennies (blue: $\\mathrm{FTRL+}$ ; green: FTRL; red: continuous time FTRL); in the center and to the right, two different orbits in a $2\\times2\\times2$ harmonic game from two different viewpoints (blue: $\\mathrm{FTRL+}$ ; green/orange:FTRL; payoff profiles on vertices). In all cases, we ran the optimistic variant of $\\mathrm{FTRL+}$ $\\lambda_{i}=0$ for all players), and we see that the trajectories of (FTRL) diverge away from equilibrium and the trajectories of (FTRL-D) are recurrent (actually, periodic), whereas $(\\mathrm{FTRL+})$ converges. We also see the highly non-convex structure of harmonic games as evidence by their equilibrium set (thick red line in center and right subfigures). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "As an immediate corollary of the above, we conclude that, under $\\mathrm{(FTRL+)}$ ), the empirical frequencies of play $z_{\\alpha,n}\\,:=\\,(1/n)\\textstyle\\sum_{s=1}^{n}x_{\\alpha,s},\\,a$ $\\alpha\\,\\in\\,{\\mathcal{A}}$ , converge to the game\u2019s set of CCE at a rate of ${\\mathcal{O}}(1/n)$ This rate is, again, op timal, but as we discussed in Section 3, it offers little information in games where the marginalization of CCE does not lead to Nash equilibrium \u2013 and, in general $N$ -player harmonic games, there is little hope that it would. In addition, even when the marginalization of CCE is Nash, the actual trajectory of play may \u2013 and, in fact, often does \u2013 behave very differently from the time-averaged frequency of play. ", "page_idx": 8}, {"type": "text", "text": "Our last result below shows that, despite these hurdles, $(\\mathrm{FTRL+})$ does converge to Nash equilibrium. To state it, we will focus on the case where each player\u2019s regularizer $h_{i}$ satisfies the following technical requirements: ", "page_idx": 8}, {"type": "text", "text": "1. Smoothness: For all $x_{i}\\,\\in\\,{\\mathrm{ri}}\\,\\mathcal{X}_{i}$ and all $x_{i}^{\\prime}\\,\\in\\,\\mathcal{X}_{i}$ , $i\\in\\mathcal{N}$ , the function $h_{i}(x_{i}+t(x_{i}^{\\prime}-x_{i}))$ is continuously differentiable in a neighborhood of $t=0$ . ", "page_idx": 8}, {"type": "text", "text": "In the above, ri $\\mathcal{X}_{i}=\\{x_{i}\\in\\mathcal{X}_{i}:\\operatorname{supp}(x_{i})=\\mathcal{A}_{i}\\}$ and bd $\\mathcal{X}_{i}=\\{x_{i}\\in\\mathcal{X}_{i}:\\mathrm{supp}(x_{i})\\neq\\mathcal{A}_{i}\\}$ respectively denote the relative interior and boundary of $\\mathcal{X}_{i}$ , that is, the set of fully- and partially-mixed strategies of player $i\\in\\mathcal{N}$ . With this in mind, the smoothness requirement simply posits that $h_{i}$ is smooth along any line segment in the interior of $\\mathcal{X}_{i}$ , while steepness means that $h_{i}$ becomes \u201cinfinitely steep\u201d near the boundary bd $\\mathcal{X}_{i}$ of $\\mathcal{X}_{i}$ (hence the name). Our prototypical example \u2013 the entropic regularizer of Eq. (8) \u2013 satisfies both requirements, as do all regularizers of the form $\\begin{array}{r}{\\boldsymbol{h}_{i}(\\boldsymbol{x}_{i})=\\sum_{\\alpha_{i}\\in\\mathcal{A}_{i}}\\theta_{i}(x_{i\\alpha_{i}})}\\end{array}$ for some continuous convex function $\\theta_{i}\\colon[0,1]\\to\\ensuremath{\\mathbb{R}}$ with $\\begin{array}{r}{\\operatorname*{lim}_{t\\to0^{+}}\\theta^{\\prime}(t)=-\\infty}\\end{array}$ . ", "page_idx": 8}, {"type": "text", "text": "Under these mild requirements, we obtain the following equilibrium convergence result:8 ", "page_idx": 8}, {"type": "text", "text": "Theorem 4. Suppose that each player in a harmonic game \u0393 follows (FTRL $^+$ ) with learning rate $\\eta_{i}\\leq m_{i}K_{i}\\left[2(N+2)\\operatorname*{max}_{j}m_{j}G_{j}\\right]^{-1}$ and payoff signals as per (13) and (16). Then $x_{n}$ converges to $a$ Nash equilibrium of $\\Gamma$ . ", "page_idx": 8}, {"type": "text", "text": "To the best of our knowledge, Theorem 4 is the first result of its kind for harmonic games \u2013 and, in that regard, it is somewhat unexpected. To be sure, two-player zero-sum games with a fully-mixed equilibrium exhibit a comparable pattern: FTRL is Poincar\u00e9 recurrent in continuous time, its vanilla discretization is unstable, and its optimistic / forward-looking implementation is convergent. However, the convex-concave structure of min-max games which enables this analysis is completely absent in harmonic games, so it is less clear what to expect in this case (where even the set of Nash equilibria is non-convex, cf. Fig. 1). By this token, the convergence of $\\left(\\mathrm{FTRL+}\\right)$ in harmonic games is a property that one could optimistically hope for, but not one that can be taken for granted. ", "page_idx": 8}, {"type": "text", "text": "From a technical standpoint, the proof of Theorems 3 and 4 involves tackling two concurrent challenges: $(i)$ to derive a Lyapunov function with a \u201csufficient descent\u201d property for all harmonic games and all regularizers; and $(i i)$ to provide an integrated analysis for the entire gamut of possible update structures in $\\mathrm{(FTRL+)}$ . The precise construction and calculations are too cumbersome to record here, but one of the key steps in the analysis is to derive a \u201ctemplate inequality\u201d of the form ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{E_{n+1}\\leq E_{n}+\\sum_{i\\in\\mathcal{N}}m_{i}\\langle v_{i}(x_{n+1/2}),x_{i,n+1/2}-p_{i}\\rangle}}\\\\ &{\\qquad\\qquad+\\sum_{i\\in\\mathcal{N}}m_{i}\\langle v_{i}(x_{n+1/2})-v_{i}(x_{n}),x_{i,n+1}-x_{i,n+1/2}\\rangle}\\\\ &{\\qquad+\\sum_{i\\in\\mathcal{N}}m_{i}(1-\\lambda_{i})\\langle v_{i}(x_{n})-v_{i}(x_{n-1/2}),x_{i,n+1}-x_{i,n+1/2}\\rangle}\\\\ &{\\qquad-\\sum_{i\\in\\mathcal{N}}\\frac{m_{i}K_{i}}{\\eta_{i}}\\big[\\|x_{i,n+1}-x_{i,n+1/2}\\|^{2}+\\|x_{i,n+1/2}-x_{i,n}\\|^{2}\\big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "where $E_{n}\\equiv E(y_{n})$ is the Lyapunov function in question (presented in detail in Appendix D). ", "page_idx": 9}, {"type": "text", "text": "A first important consequence of the template inequality (18) is that the sequences $a_{n}=\\|x_{n+1}-$ $x_{n+1/2}||^{2}$ and $b_{n}=\\|x_{n+1/2}-x_{n}\\|^{2}$ are both summable: this requires a repeated use of the FenchelYoung inequality, and an instantiation of $p$ to the strategic center of $\\Gamma$ , which we detail in Appendices A and D. Then, by establishing a similar template inequality for each player $i\\in\\mathcal{N}$ , we are able to bound the regret by the same upper bound that we derived for $\\textstyle\\sum_{n}a_{n}$ and $\\textstyle\\sum_{n}b_{n}$ , and which is (up to certain secondary factors) the bound (17). ", "page_idx": 9}, {"type": "text", "text": "For the convergence to Nash equilibrium, the summability argument above also plays a crucial role. As we show in Appendix D, this summability coupled with the template inequality (18) allow us to conclude that any limit point of $(\\mathrm{FTRL+})$ is a Nash equilibrium; subsequently, by a second application of the template inequality (18) to any of these limit points, we are able to extract the algorithm\u2019s pointwise convergence. ", "page_idx": 9}, {"type": "text", "text": "5 Concluding remarks ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our results suggest that the long-run behavior of online learning \u2013 and, in particular, no-regret learning \u2013 algorithms and dynamics in harmonic games is a very rich topic, and one which opens the door to an entirely new class of games where positive convergence results can be obtained. We find this particularly appealing, not only because harmonic games comprise the strategic complement of potential games, but also because they represent highly non-convex problems \u2013 in fact, even their equilibrium set is non-convex. As such, the fact that it is possible to obtain optimal regret guarantees and positive equilibrium convergence results in this setting is very promising for future work. ", "page_idx": 9}, {"type": "text", "text": "Future directions in this topic abound: First and foremost, an open question is the rate of convergence of $\\mathrm{(FTRL+)}$ ) to equilibrium. Even though $\\mathrm{(FTRL+)}$ ) has order-optimal regret bounds, this only helps in establishing a convergence rate to the game\u2019s set of coarse correlated equilibria; for Nash equilibria, building on earlier work by Golowich et al. [19], some recent results by Cai et al. [5], Gorbunov et al. [21] have shed some light on the convergence of constrained Euclidean optimistic methods, but the technology therein does not extend to non-monotone, non-Euclidean problems. Inspired by Wei et al. [61], we conjecture that the convergence rate of (FTRL $^+$ ) in harmonic games is linear: we conjecture this because any harmonic game admits a fully-mixed Nash equilibrium, and the weighted sum in the definition of a harmonic game formally looks similar to the condition needed to establish metric subregularity in [61]; however, a proof would likely require different techniques. ", "page_idx": 9}, {"type": "text", "text": "Another important research direction has to do with the information available to the players. A first open question here concerns the case where players do not have access to full information on their mixed payoff vectors, but can only observe their pure payoffs \u2013 either in a \u201cwhat if\u201d, counterfactual manner, or in the form of bandit, payoff-based feedback. In a similar manner, the algorithms presented here are not adaptive, in the sense that the players\u2019 step-size policy has to satisfy a certain bound that depends on correctly estimating some of the game\u2019s parameters. Obtaining an adaptive version of $(\\mathrm{FTRL+})$ which, in the spirit of Rakhlin & Sridharan [50] and Hsieh et al. [28, 29, 30], remains convergent and attains order-optimal regret in both adversarial and game-theoretic settings without any need for hyperparameter tuning is also an ambitious question for future research. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This research was supported in part by the French National Research Agency (ANR) in the framework of the PEPR IA FOUNDRY project (ANR-23-PEIA-0003), the \u201cInvestissements d\u2019avenir program\u201d (ANR-15-IDEX-02), the LabEx PERSYVAL (ANR-11-LABX-0025-01), MIAI $@$ Grenoble Alpes (ANR-19-P3IA-0003), the project IRGA2024-SPICE-G7H-IRG24E90, NSF grant CCF2212233, and project MIS 5154714 of the National Recovery and Resilience Plan Greece 2.0 funded by the European Union under the NextGenerationEU Program. PM is also with the Archimedes Research Unit \u2013 Athena RC \u2013 Department of Mathematics, National & Kapodistrian University of Athens. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Abdou, J., Pnevmatikos, N., Scarsini, M., and Venel, X. Decomposition of games: Some strategic considerations. Mathematics of Operations Research, 47(1):176\u2013208, February 2022.   \n[2] Arnold, V. I. Mathematical Methods of Classical Mechanics. Springer-Verlag, 1989.   \n[3] Azizian, W., Iutzeler, F., Malick, J., and Mertikopoulos, P. The rate of convergence of Bregman proximal methods: Local geometry vs. regularity vs. sharpness. https://arxiv.org/abs/2211.08043, 2022. [4] Bertsekas, D. P. Convex optimization algorithms. Athena Scientific, 2015. [5] Cai, Y., Oikonomou, A., and Zheng, W. Tight last-iterate convergence of the extragradient and the optimistic gradient descent-ascent algorithm for constrained monotone variational inequalities. 2022.   \n[6] Candogan, O., Menache, I., Ozdaglar, A., and Parrilo, P. A. Flows and decompositions of games: Harmonic and potential games. Mathematics of Operations Research, 36(3):474\u2013503, 2011.   \n[7] Chen, G. and Teboulle, M. Convergence analysis of a proximal-like minimization algorithm using Bregman functions. SIAM Journal on Optimization, 3(3):538\u2013543, August 1993.   \n[8] Chen, X. and Peng, B. Hedging in games: Faster convergence of external and swap regrets. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 18990\u201318999. Curran Associates, Inc., 2020. [9] Chiang, C.-K., Yang, T., Lee, C.-J., Mahdavi, M., Lu, C.-J., Jin, R., and Zhu, S. Online optimization with gradual variations. In COLT \u201912: Proceedings of the 25th Annual Conference on Learning Theory, 2012.   \n[10] Daskalakis, C. and Panageas, I. The limit points of (optimistic) gradient descent in min-max optimization. In NeurIPS \u201918: Proceedings of the 32nd International Conference of Neural Information Processing Systems, 2018.   \n[11] Daskalakis, C. and Panageas, I. Last-iterate convergence: Zero-sum games and constrained min-max optimization. In ITCS \u201919: Proceedings of the 10th Conference on Innovations in Theoretical Computer Science, 2019.   \n[12] Daskalakis, C., Goldberg, P. W., and Papadimitriou, C. H. The complexity of computing a Nash equilibrium. Communications of the ACM, 52(2):89\u201397, 2009.   \n[13] Daskalakis, C. C., Fishelson, M., and Golowich, N. Near-optimal no-regret learning in general games. In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, 2021.   \n[14] Farina, G., Anagnostides, I., Luo, H., Lee, C.-W., Kroer, C., and Sandholm, T. Near-optimal no-regret learning dynamics for general convex games. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022.   \n[15] Fasoulakis, M., Markakis, E., Pantazis, Y., and Varsos, C. Forward looking best-response multiplicative weights update methods for bilinear zero-sum games. In Camps-Valls, G., Ruiz, F. J. R., and Valera, I. (eds.), Proceedings of The 25th International Conference on Artificial Intelligence and Statistics, volume 151 of Proceedings of Machine Learning Research, pp. 11096\u201311117. PMLR, 28\u201330 Mar 2022.   \n[16] Flokas, L., Vlatakis-Gkaragkounis, E. V., Lianeas, T., Mertikopoulos, P., and Piliouras, G. No-regret learning and mixed Nash equilibria: They do not mix. In NeurIPS \u201920: Proceedings of the 34th International Conference on Neural Information Processing Systems, 2020.   \n[17] Friedman, D. Evolutionary games in economics. Econometrica, 59(3):637\u2013666, 1991.   \n[18] Gidel, G., Berard, H., Vignoud, G., Vincent, P., and Lacoste-Julien, S. A variational inequality perspective on generative adversarial networks. In ICLR \u201919: Proceedings of the 2019 International Conference on Learning Representations, 2019.   \n[19] Golowich, N., Pattathil, S., and Daskalakis, C. Tight last-iterate convergence rates for no-regret learning in multi-player games. In NeurIPS \u201920: Proceedings of the 34th International Conference on Neural Information Processing Systems, 2020.   \n[20] Golowich, N., Pattathil, S., Daskalakis, C., and Ozdaglar, A. Last iterate is slower than averaged iterate in smooth convex-concave saddle point problems. In COLT \u201920: Proceedings of the 33rd Annual Conference on Learning Theory, 2020.   \n[21] Gorbunov, E., Taylor, A., and Gidel, G. Last-iterate convergence of optimistic gradient method for monotone variational inequalities. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022.   \n[22] Harper, M. Escort evolutionary game theory. Physica D: Nonlinear Phenomena, 240(18):1411\u20131415, September 2011.   \n[23] Hart, S. and Mas-Colell, A. A simple adaptive procedure leading to correlated equilibrium. Econometrica, 68(5):1127\u20131150, September 2000.   \n[24] Hart, S. and Mas-Colell, A. Stochastic uncoupled dynamics and Nash equilibrium. Games and Economic Behavior, 57:286\u2013303, 2006.   \n[25] H\u00e9liou, A., Cohen, J., and Mertikopoulos, P. Learning with bandit feedback in potential games. In NIPS \u201917: Proceedings of the 31st International Conference on Neural Information Processing Systems, 2017.   \n[26] Hofbauer, J. and Sigmund, K. Evolutionary Games and Population Dynamics. Cambridge University Press, Cambridge, UK, 1998.   \n[27] Hsieh, Y.-G., Iutzeler, F., Malick, J., and Mertikopoulos, P. On the convergence of single-call stochastic extra-gradient methods. In NeurIPS \u201919: Proceedings of the 33rd International Conference on Neural Information Processing Systems, pp. 6936\u20136946, 2019.   \n[28] Hsieh, Y.-G., Antonakopoulos, K., and Mertikopoulos, P. Adaptive learning in continuous games: Optimal regret bounds and convergence to Nash equilibrium. In COLT \u201921: Proceedings of the 34th Annual Conference on Learning Theory, 2021.   \n[29] Hsieh, Y.-G., Antonakopoulos, K., Cevher, V., and Mertikopoulos, P. No-regret learning in games with noisy feedback: Faster rates and adaptivity via learning rate separation. In NeurIPS \u201922: Proceedings of the 36th International Conference on Neural Information Processing Systems, 2022.   \n[30] Hsieh, Y.-G., Iutzeler, F., Malick, J., and Mertikopoulos, P. Multi-agent online optimization with delays: Asynchronicity, adaptivity, and optimism. Journal of Machine Learning Research, 23(78):1\u201349, May 2022.   \n[31] Jiang, X., Lim, L.-H., Yao, Y., and Ye, Y. Statistical ranking and combinatorial Hodge theory. Mathematical Programming, 127(1):203\u2013244, 2011.   \n[32] Juditsky, A., Nemirovski, A. S., and Tauvel, C. Solving variational inequalities with stochastic mirror-prox algorithm. Stochastic Systems, 1(1):17\u201358, 2011.   \n[33] Korpelevich, G. M. The extragradient method for finding saddle points and other problems. \u00c8konom. i Mat. Metody, 12:747\u2013756, 1976.   \n[34] Kwon, J. and Mertikopoulos, P. A continuous-time approach to online optimization. Journal of Dynamics and Games, 4(2):125\u2013148, April 2017.   \n[35] Lee, J. M. Introduction to Smooth Manifolds. Number 218 in Graduate Texts in Mathematics. SpringerVerlag, New York, NY, 2 edition, 2003.   \n[36] Legacci, D., Mertikopoulos, P., and Pradelski, B. S. R. A geometric decomposition of finite games: Convergence vs. recurrence under exponential weights. https://arxiv.org/abs/2405.07224, 2024.   \n[37] Lin, T., Zhou, Z., Mertikopoulos, P., and Jordan, M. I. Finite-time last-iterate convergence for multi-agent learning in games. In ICML \u201920: Proceedings of the 37th International Conference on Machine Learning, 2020.   \n[38] Mertikopoulos, P. and Sandholm, W. H. Learning in games via reinforcement and regularization. Mathematics of Operations Research, 41(4):1297\u20131324, November 2016.   \n[39] Mertikopoulos, P. and Sandholm, W. H. Riemannian game dynamics. Journal of Economic Theory, 177: 315\u2013364, September 2018.   \n[40] Mertikopoulos, P. and Zhou, Z. Learning in games with continuous action sets and unknown payoff functions. Mathematical Programming, 173(1-2):465\u2013507, January 2019.   \n[41] Mertikopoulos, P., Papadimitriou, C. H., and Piliouras, G. Cycles in adversarial regularized learning. In SODA \u201918: Proceedings of the 29th annual ACM-SIAM Symposium on Discrete Algorithms, 2018.   \n[42] Mertikopoulos, P., Lecouat, B., Zenati, H., Foo, C.-S., Chandrasekhar, V., and Piliouras, G. Optimistic mirror descent in saddle-point problems: Going the extra (gradient) mile. In ICLR \u201919: Proceedings of the 2019 International Conference on Learning Representations, 2019.   \n[43] Mertikopoulos, P., Hsieh, Y.-P., and Cevher, V. A unified stochastic approximation framework for learning in games. Mathematical Programming, 203:559\u2013609, January 2024.   \n[44] Monderer, D. and Shapley, L. S. Potential games. Games and Economic Behavior, 14(1):124 \u2013 143, 1996.   \n[45] Nemirovski, A. S. Prox-method with rate of convergence $O(1/t)$ for variational inequalities with Lipschitz continuous monotone operators and smooth convex-concave saddle point problems. SIAM Journal on Optimization, 15(1):229\u2013251, 2004.   \n[46] Nesterov, Y. Dual extrapolation and its applications to solving variational inequalities and related problems. Mathematical Programming, 109(2):319\u2013344, 2007.   \n[47] Nisan, N., Roughgarden, T., Tardos, \u00c9., and Vazirani, V. V. (eds.). Algorithmic Game Theory. Cambridge University Press, 2007.   \n[48] Piliouras, G. and Shamma, J. S. Optimization despite chaos: Convex relaxations to complex limit sets via Poincar\u00e9 recurrence. In SODA \u201914: Proceedings of the 25th annual ACM-SIAM Symposium on Discrete Algorithms, 2014.   \n[49] Popov, L. D. A modification of the Arrow\u2013Hurwicz method for search of saddle points. Mathematical Notes of the Academy of Sciences of the USSR, 28(5):845\u2013848, 1980.   \n[50] Rakhlin, A. and Sridharan, K. Optimization, learning, and games with predictable sequences. In NIPS \u201913: Proceedings of the 27th International Conference on Neural Information Processing Systems, 2013.   \n[51] Robinson, C. Dynamical Systems: Stability, Symbolic Dynamics, and Chaos. CRC Press, November 1998.   \n[52] Rockafellar, R. T. Convex Analysis. Princeton University Press, Princeton, NJ, 1970.   \n[53] Rockafellar, R. T. and Wets, R. J. B. Variational Analysis, volume 317 of A Series of Comprehensive Studies in Mathematics. Springer-Verlag, Berlin, 1998.   \n[54] Sandholm, W. H. Potential games with continuous player sets. Journal of Economic Theory, 97:81\u2013108, 2001.   \n[55] Sato, Y., Akiyama, E., and Farmer, J. D. Chaos in learning a simple two-person game. Proceedings of the National Academy of Sciences of the USA, 99(7):4748\u20134751, April 2002.   \n[56] Shalev-Shwartz, S. Online learning and online convex optimization. Foundations and Trends in Machine Learning, 4(2):107\u2013194, 2011.   \n[57] Shalev-Shwartz, S. and Singer, Y. Convex repeated games and Fenchel duality. In NIPS\u2019 06: Proceedings of the 19th Annual Conference on Neural Information Processing Systems, pp. 1265\u20131272. MIT Press, 2006.   \n[58] Syrgkanis, V., Agarwal, A., Luo, H., and Schapire, R. E. Fast convergence of regularized learning in games. In NIPS \u201915: Proceedings of the 29th International Conference on Neural Information Processing Systems, pp. 2989\u20132997, 2015.   \n[59] Taylor, P. D. and Jonker, L. B. Evolutionary stable strategies and game dynamics. Mathematical Biosciences, 40(1-2):145\u2013156, 1978.   \n[60] Viossat, Y. and Zapechelnyuk, A. No-regret dynamics and fictitious play. Journal of Economic Theory, 148(2):825\u2013842, March 2013.   \n[61] Wei, C.-Y., Lee, C.-W., Zhang, M., and Luo, H. Linear last-iterate convergence in constrained saddle-point optimization. In ICLR \u201921: Proceedings of the 2021 International Conference on Learning Representations, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendices ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Harmonic Games 14 ", "page_idx": 13}, {"type": "text", "text": "A.1 Harmonic games, measures and comeasures 14   \nA.2 Preference equivalence between harmonic games . . 15   \nA.3 Mixed characterization of harmonic games . . . 16   \nA.4 Harmonic and zero-sum games . . 18 ", "page_idx": 13}, {"type": "text", "text": "B Basic properties of regularizers and the induced choice maps 19 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 Preliminary definitions 19   \nB.2 Basic lemmas 20 ", "page_idx": 13}, {"type": "text", "text": "C Continuous-time analysis 22 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "C.1 Dynamical systems notions . . 22   \nC.2 Basic properties of FTRL . . 22   \nC.3 Constant of motion for harmonic games 23   \nC.4 FTRL in the space of payoff differences 23   \nC.5 Recurrence of FTRL in harmonic games 26 ", "page_idx": 13}, {"type": "text", "text": "D Discrete-time analysis 26 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "D.1 Lyapunov functions and template inequalities for (FTRL $^{+}$ ) 26   \nD.2 Proof of Theorem 3 . 29   \nD.3 Proof of Theorem 4 30 ", "page_idx": 13}, {"type": "text", "text": "A Harmonic Games ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The class of uniform harmonic games (UHGs) introduced by Candogan et al. [6] provides a gametheoretic framework for modeling strategic situations with confilcting, anti-aligned interests.9 Broadly speaking, the characterizing property of uniform harmonic games is the following: for any player considering a deviation towards a specific pure strategy profile, there exist other players who are motivated to deviate away from that profile. ", "page_idx": 13}, {"type": "text", "text": "Given a finite game $\\Gamma=\\Gamma(\\mathcal{N},\\mathcal{A},u)$ , this is formalized by the condition that, for all $\\alpha\\in{\\mathcal{A}}$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\sum_{i\\in\\mathcal{N}}\\sum_{\\beta_{i}\\in\\mathcal{A}_{i}}\\left[u_{i}(\\alpha_{i};\\alpha_{-i})-u_{i}(\\beta_{i};\\alpha_{-i})\\right]=0\\,.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "From a strategic viewpoint, uniform harmonic games complement potential games: Candogan et al. [6] showed that any finite game can be uniquely decomposed into the sum of a potential game and a uniform harmonic game, up to linear transformations of the payoff functions that do not change the strategic structure of the game. ", "page_idx": 13}, {"type": "text", "text": "Since their introduction, harmonic games have generated a substantial body of literature; for a brief survey, we refer the reader to [36]. ", "page_idx": 13}, {"type": "text", "text": "A.1. Harmonic games, measures and comeasures. The class of uniform harmonic games exhibits intriguing, yet restrictive, properties. Notably, a UHG always admits the uniformly mixed strategy as a NE, and it generally possesses a continuum of Nash equilibria [6]. Additionally, the framework of UHGs and the decomposition proposed by Candogan et al. [6] are incompatible with common game-theoretical transformations, such as the duplication of strategies or rescaling of payoffs [1]. To address the above limitations, Abdou et al. [1] extended the definition of harmonic games by the introduction of two parameters: a measure, that is a positive weight each player assigns to each of their own strategy; and a comeasure, that is a positive weight each player assigns to each of the other players\u2019 action profiles. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Definition A.1. Let $\\Gamma(\\mathcal{N},\\mathcal{A},u)$ be a finite game. A player measure $\\mu_{i}$ is a function $\\mu_{i}\\colon\\ensuremath{\\mathcal{A}}_{i}\\to\\ensuremath{\\mathbb{R}}_{++}$ ; a player co-measure $\\gamma_{i}$ is a function $\\gamma_{i}\\colon A_{-i}\\rightarrow\\mathbb{R}_{++}$ . Correspondingly, a collection $\\mu=\\{\\mu_{i}\\}_{i\\in\\mathcal{N}}$ (resp. $\\boldsymbol{\\gamma}\\,=\\,\\{\\gamma_{i}\\}_{i\\in\\mathcal{N}})$ of player measures (resp. comeasures) is called game measure (resp. game comeasure). If $\\mu_{i}$ is a player measure, we will write $\\begin{array}{r}{\\left|\\mu_{i}\\right|:=\\sum_{\\alpha_{i}}\\mu_{i\\alpha_{i}}}\\end{array}$ . Finally, a probability measure is a game measure $\\mu$ such that $\\left|\\mu_{i}\\right|=1$ for all $i\\in\\mathcal{N}$ ; a uni form measure is a game measure $\\mu$ such that $\\mu_{i\\alpha_{i}}\\,=\\,1$ for all $i\\in\\mathcal{N},\\alpha_{i}\\in\\mathcal{A}_{i}$ ; and a uniform comeasure is a game comeasure $\\gamma$ such that $\\gamma_{i\\alpha_{-i}}=1$ for all $i\\in\\mathcal{N},\\alpha_{-i}\\in\\mathcal{A}_{-i}$ . ", "page_idx": 14}, {"type": "text", "text": "With these notions in place, Abdou et al. [1] define a finite game $\\Gamma$ to be $(\\mu,\\gamma)$ -harmonic if there exist a game measure $\\mu$ and a game comeasure $\\gamma$ such that, for all $\\alpha\\in{\\mathcal{A}}$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sum_{i}\\sum_{\\beta_{i}}\\mu_{i\\beta_{i}}\\gamma_{i\\alpha_{-i}}\\big[u_{i}(\\alpha_{i};\\alpha_{-i})-u_{i}(\\beta_{i};\\alpha_{-i})\\big]=0\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In this work, we focus solely on harmonic games with uniform comeasure. As discussed after Definition 1 in the main body of the article, this choice comes without loss of generality: the game comeasure in Eq. (A.2) can be absorbed by a payoff rescaling to give a game that is still harmonic, and preference equivalent to the original game \u2013 in a sense that we make precise in the next section. ", "page_idx": 14}, {"type": "text", "text": "A.2. Preference equivalence between harmonic games. The strategic structure of a game is preserved under monotonic transformations of the utility functions, since the set of pure Nash equilibria of a game is an ordinal object \u2013 it depends only on the signs of unilateral payoff differences, and not on their absolute values. For this reason, two games $\\Gamma(\\mathcal{N},\\mathcal{A},u)$ and $\\Gamma^{\\prime}(\\mathcal{N},\\mathcal{A},u^{\\prime})$ are called preference-equivalent (PE) if for all $\\alpha,\\beta\\in A$ and all $i\\in\\mathcal{N}$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname{sgn}\\left[u_{i}^{\\prime}(\\beta_{i};\\alpha_{-i})-u_{i}^{\\prime}(\\alpha_{i};\\alpha_{-i})\\right]=\\operatorname{sgn}\\left[u_{i}(\\beta_{i};\\alpha_{-i})-u_{i}(\\alpha_{i};\\alpha_{-i})\\right].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Two games are strategically equivalent (SE) \u2013 and we write $\\Gamma\\sim\\Gamma^{\\prime}-$ if they have the same unilateral payoff differences, that is if ", "page_idx": 14}, {"type": "equation", "text": "$$\nu_{i}^{\\prime}(\\beta_{i};\\alpha_{-i})-u_{i}^{\\prime}(\\alpha_{i};\\alpha_{-i})=u_{i}(\\beta_{i};\\alpha_{-i})-u_{i}(\\alpha_{i};\\alpha_{-i})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for all $\\alpha,\\beta\\in A$ and all $i\\in\\mathcal{N}$ ; strategically equivalent games are clearly preference-equivalent. ", "page_idx": 14}, {"type": "text", "text": "Lemma A.2. Let $\\Gamma_{\\mu,\\gamma}=\\Gamma_{\\mu,\\gamma}(\\mathcal{N},\\mathcal{A},u)$ be a harmonic game in the sense of Eq. (A.2). Then the game $(\\mathcal{N},\\mathcal{A},u^{\\prime})$ with $u_{i}^{\\prime}(\\alpha_{i};\\alpha_{-i})=\\gamma_{i\\alpha_{-i}}u_{i}(\\alpha_{i};\\alpha_{-i})$ is preference-equivalent to the game $\\Gamma_{\\mu,\\gamma}$ , and $i t$ is harmonic in the sense of Eq. (A.2) with measure $\\mu$ and uniform comeasure. ", "page_idx": 14}, {"type": "text", "text": "Proof. Let $u_{i}^{\\prime\\prime}(\\alpha_{i};\\alpha_{-i})=\\mu_{i\\alpha_{i}}\\gamma_{i\\alpha_{-i}}u_{i}(\\alpha_{i};\\alpha_{-i})$ . Then replacing above, for all $\\alpha\\in{\\mathcal{A}}$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n0=\\sum_{i\\in\\mathcal{N}}\\sum_{\\beta_{i}\\in\\mathcal{A}_{i}}\\mu_{i\\beta_{i}}\\left[\\frac{u_{i}^{\\prime\\prime}(\\alpha_{i};\\boldsymbol{\\alpha}_{-i})}{\\mu_{i\\alpha_{i}}}-\\frac{u_{i}^{\\prime\\prime}(\\beta_{i};\\boldsymbol{\\alpha}_{-i})}{\\mu_{i\\beta_{i}}}\\right]\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "$\\begin{array}{r}{u_{i}^{\\prime}(\\alpha_{i};\\alpha_{-i})=\\frac{u_{i}^{\\prime\\prime}(\\alpha_{i};\\alpha_{-i})}{\\mu_{i}\\alpha_{i}}=\\gamma_{i\\alpha_{-i}}u_{i}(\\alpha_{i};\\alpha_{-i})}\\end{array}$ . The game $u^{\\prime}$ is preference-equivalent to $u$ , and ", "page_idx": 14}, {"type": "equation", "text": "$$\n0=\\sum_{i\\in\\mathcal{N}}\\sum_{\\beta_{i}\\in\\mathcal{A}_{i}}\\mu_{i\\beta_{i}}\\left[u_{i}^{\\prime}(\\alpha_{i};\\alpha_{-i})-u_{i}^{\\prime}(\\beta_{i};\\alpha_{-i})\\right]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for all $\\alpha\\in{\\mathcal{A}}$ , so $u^{\\prime}$ is harmonic in the sense of A.2 with measure $\\mu$ and uniform comeasure. ", "page_idx": 14}, {"type": "text", "text": "In the proof above we perform the intermediate step $u\\rightarrow u^{\\prime\\prime}$ rather than defining directly $u\\rightarrow u^{\\prime}$ to stress the difference between rescaling the payoffs of a game by a game measure $\\mu$ and by a game comeasure $\\gamma$ . The game with payoffs $u^{\\prime}\\,=\\,\\gamma u$ (the meaning of this notation made precise in the proof above) is preference-equivalent to the game with payoffs $u$ , i.e., rescaling the payoffs by a comeasure does not change the strategic structure of the game. On the other hand, the game with payoffs $u^{\\prime\\prime}=\\mu u^{\\prime}-\\mathrm{again}$ , the meaning made precise in the proof \u2013 is not PE to the game with payoffs $u^{\\prime}$ : rescaling the payoffs by a measure can change the preferences of the players, and leads to a game with intrinsically different strategic structure. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Lemma A.2 motivates our choice to focus in this work on harmonic games with arbitrary measures and uniform comeasures, and to adopt (HG) from Definition 1 to characterize harmonic games: a harmonic game (HG) $\\Gamma_{\\mu}=\\Gamma_{\\mu}(\\mathcal{N},\\bar{\\mathcal{A}},u)$ is a finite game $(\\mathcal{N},\\mathcal{A},u)$ with a game measure $\\mu$ such that (HG) holds, i.e., $\\begin{array}{r}{\\sum_{i\\in\\mathcal{N}}\\dot{\\sum}_{\\beta_{i}\\in\\dot{\\mathcal{A}}_{i}}\\,\\mu_{i\\beta_{i}}\\big[u_{i}(\\alpha_{i};\\alpha_{-i})-u_{i}(\\beta_{i};\\alpha_{-i})\\big]=0}\\end{array}$ for all $\\alpha\\in A$ . ", "page_idx": 15}, {"type": "text", "text": "A.3. Mixed characterization of harmonic games. The defining property (HG) allows for an equivalent characterization of harmonic games in terms of their mixed payoffs: ", "page_idx": 15}, {"type": "text", "text": "Lemma A.3. A finite game $\\Gamma=\\Gamma(\\mathcal{N},\\mathcal{A},u)$ is harmonic with measure $\\mu$ if and only if ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{i\\in\\mathcal{N}}\\left|\\mu_{i}\\right|\\left\\langle v_{i}(x),x_{i}-\\frac{\\mu_{i}}{\\left|\\mu_{i}\\right|}\\right\\rangle=0\\quad f o r\\,a l l\\,x\\in\\mathcal{X}\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Given a finite game $\\Gamma=\\Gamma(\\mathcal{N},\\mathcal{A},u)$ and a game measure $\\mu$ , let $F_{i}\\colon A\\to\\mathbb{R}$ be defined by $\\begin{array}{r}{F_{i}(\\alpha)=\\sum_{\\beta_{i}\\in\\mathcal{A}_{i}}\\mu_{i\\beta_{i}}\\big[u_{i}(\\alpha_{i};\\alpha_{-i})-u_{i}(\\beta_{i};\\alpha_{-i})\\big]}\\end{array}$ . By definition, $\\Gamma$ is a $\\mu$ -harmonic game if and only if $\\textstyle F(\\alpha):=\\sum_{i\\in\\mathcal{N}}F_{i}(\\alpha)=0$ for all $\\alpha\\in{\\mathcal{A}}$ . Denote (with slight abuse of notation) by $F\\colon\\mathcal{X}\\rightarrow\\mathbb{R}$ the multilinear extension of $F\\colon A\\rightarrow\\mathbb{R}.$ , i.e., $\\begin{array}{r}{\\begin{array}{r}{F(x)=\\sum_{\\alpha}x_{\\alpha}F(\\alpha)}\\end{array}}\\end{array}$ , with $\\textstyle x_{\\alpha}:=\\prod_{i}x_{i\\alpha_{i}}$ . Now, $F(\\alpha)=0$ for all $\\alpha\\in{\\mathcal{A}}$ if and only if $F(x)=0$ for all $x\\in\\mathcal{X}$ , which is the case if and only if ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{0=F(x)=\\sum_{\\alpha}x_{\\alpha}\\sum_{i}F_{i}(\\alpha)=\\sum_{i}\\sum_{\\alpha_{i}}\\sum_{\\alpha_{i}}x_{i\\alpha_{i}}x_{-i\\alpha_{-i}}\\sum_{\\beta_{i}}\\mu_{i\\beta_{i}}[u_{i}(\\alpha_{i};\\alpha_{-i})-u_{i}(\\beta_{i};\\alpha_{-i})]}\\\\ &{\\;\\;\\;=\\sum_{i}\\sum_{\\beta_{i}}\\mu_{i\\beta_{i}}[u_{i}(x_{i};x_{-i})-u_{i}(\\beta_{i};x_{-i})]=\\sum_{i}\\left[|\\mu_{i}|\\langle v_{i}(x),x_{i}\\rangle-\\langle v_{i}(x),\\mu_{i}\\rangle\\right]\\quad{\\mathrm{for~all~}}x\\in{\\mathcal{X}},}\\end{array}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "from which we conclude by factoring out the $\\left|\\mu_{i}\\right|$ terms. ", "page_idx": 15}, {"type": "text", "text": "Remark. The first equality in the second line holds true for harmonic games with uniform comeasure $\\gamma_{i\\alpha_{-i}}=1$ , since $\\gamma_{i\\alpha_{-i}}\\neq1$ terms would couple with the corresponding $x_{-i\\alpha_{-i}}$ terms in the sum. ", "page_idx": 15}, {"type": "text", "text": "The above result can be reformulated as follows: ", "page_idx": 15}, {"type": "text", "text": "Proposition A.4. A finite game $\\Gamma=\\Gamma(\\mathcal{N},\\mathcal{A},u)$ is harmonic if and only if it admits $a$ strategic center $(m,q)$ , viz. if there exist $(i)$ a vector $m\\in\\mathbb{R}_{++}^{N}$ and (ii) a fully mixed strategy $q\\in\\mathcal{X}$ such that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{i\\in\\mathcal{N}}m_{i}\\left\\langle v_{i}(x),x_{i}-q_{i}\\right\\rangle=0\\quad f o r\\,a l l\\,x\\in\\mathcal{X}\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This expression is intriguing: it suggest that a game is harmonic precisely if there exists a fully mixed strategy $q$ such that, for all $x\\,\\in\\,{\\mathcal{X}}$ , the payoff vector $v(x)$ is perpendicular (with respect to a $m$ -weighted inner product) to $x\\mathrm{~-~}q$ ; cf. Example A.1 and Fig. 2. The striking dynamical consequences of this \u201ccircular\u201d strategic structure \u2013 hinted at in Fig. 2, showing a periodic orbit of FTRL in continuous time \u2013 are captured precisely by Theorem 2 in the main text. ", "page_idx": 15}, {"type": "text", "text": "Proof of Proposition A.4. Let $\\Gamma_{\\mu}=\\Gamma_{\\mu}(\\mathcal{N},\\mathcal{A},u)$ be harmonic; then by Lemma A.3 that there exist a strategic center $(m,q)$ given by $m_{i}\\,:=\\,|\\mu_{i}|$ and $q_{i}:=\\,\\mu_{i}/\\vert\\mu_{i}\\vert$ with $i\\in\\mathcal{N}$ . Conversely let $\\Gamma=$ $\\Gamma(\\mathcal{N},\\mathcal{A},u)$ admit a strategic center $(m,q)$ ; then by the same argument $\\Gamma$ is harmonic with $\\mu_{i}:=m_{i}q_{i}$ for all $i\\in\\mathcal{N}$ . ", "page_idx": 15}, {"type": "text", "text": "An immediate corollary is the following: ", "page_idx": 15}, {"type": "text", "text": "Corollary A.5. If a finite game $\\Gamma$ admits a strategic center $(m,q)$ , then \ud835\udc5eis a Nash equilibrium. ", "page_idx": 15}, {"type": "text", "text": "Proof. By Proposition A.4 if $\\Gamma$ admits a strategic center $(m,q)$ then it is $\\mu$ -harmonic with $\\mu_{i}=m_{i}q_{i}$ for all $i\\in\\mathcal{N}$ ; and $(\\mu_{i}/|\\mu_{i}|)_{i\\in\\mathcal{N}}$ is always a NE for $\\mu$ -harmonic games [1, Theorem 1]. \u25a0 ", "page_idx": 15}, {"type": "text", "text": "Remark. The converse does not hold: a fully mixed Nash equilibrium is not necessarily a strategic center. If it were, a game would be harmonic precisely if it admitted a fully mixed NE, which is not the case \u2013 think for example of coordination or anti-coordination games, that admit a fully mixed Nash equilibrium and are not harmonic. ", "page_idx": 15}, {"type": "image", "img_path": "HW9S9vY5gZ/tmp/c28c01a45d3c34bdf057bee66281aaae3a9d4eaf59e104c3889515c739a4f52f.jpg", "img_caption": ["Figure 2: Representation of the harmonic payoff structure for the game in Example A.1. Each payoff vector $v(x)$ (black arrows) is perpendicular (with respect to a weighted inner product) to the vector $x-q$ (dotted segment) between the evaluation point $x$ of the payoff field and the fully mixed Nash equilibrium $q$ (red point). As a consequence every orbit of FTRL in continuous time (such as the one represented by the black curve) is Poincar\u00e9 recurrent (in this low-dimensional example, even periodic), as detailed in Theorem 2 in the main text. Color shading and dotted lines represents player 1\u2019s utility level sets, with brighter regions indicating higher payoffs. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Example A.1 (A harmonic game: Siege). Consider the following $2\\times2$ game: an army (the row player) must choose between Attacking a fortress (pure strategy $A$ ) and Not attacking (pure strategy $N$ ). Simultaneously, the fortress (the column player) decides whether to activate its Defenses (pure strategy $D$ ) or Not (pure strategy $N$ ). Engaging in either action (the attack or the defense) incurs a preparation cost of $c>0$ . The army gains $a_{s}>c$ if it attacks an undefended fortress, but suffers a loss of $a_{f}>0$ if it attacks and encounters defenses (the subscripts $s$ and $f$ standing respectively for \u201csuccessful\u201d and \u201cfailed\u201d). Conversely, the fortress beneftis by $d_{s}>0$ if it is defended against an attack, while it incurs a loss of $d_{f}>0$ if attacked without defenses; defeating the attacking army is worth the preparation cost for the fortress, namely $d_{s}-c>-d_{f}$ . This scenario is captured by the following payoff matrix, specialized on the right to the case $c=\\overset{.}{1},a_{s}=3,a_{f}=2,d_{s}=2,d_{f}=4$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\textit{A}}{\\textit{A}}\\left|\\begin{array}{c c c}{D}&{N}&{}\\\\ {\\left(-a_{f}-c,\\,d_{s}-c\\right)}&{\\left(a_{s}-c,\\,-d_{f}\\right)}&{\\widehat{A}}\\\\ {\\left(0,\\,-c\\right)}&{\\left(0,\\,0\\right)}&{N}\\end{array}\\right|\\begin{array}{c c c}{D}&{N}\\\\ {-3,1}&{2,-4}\\\\ {0,-1}&{0,0}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "To determine if the game is harmonic, look for a solution of the linear system ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{i\\in\\mathcal{N}}\\sum_{\\beta_{i}\\in\\mathcal{A}_{i}}\\mu_{i\\beta_{i}}[u_{i}(\\alpha_{i};\\alpha_{-i})-u_{i}(\\beta_{i};\\alpha_{-i})]=0\\quad\\mathrm{for~all~}\\alpha\\in\\mathcal{A}\\,,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "subject to the constraints $\\mu_{i\\alpha_{i}}>0$ for all $i\\in\\mathcal{N},\\alpha_{i}\\in\\mathcal{A}_{i}$ . For a fixed payoff function $u$ , this is a system of $\\Pi_{j\\in\\mathcal{N}}\\,A_{j}$ linear equations (one for each $\\alpha\\,\\in\\,{\\mathcal{A}},$ ) in the $\\textstyle\\sum_{j\\in{\\mathcal{N}}}A_{j}$ variables $\\displaystyle\\bigl((\\mu_{i\\alpha_{i}})_{\\alpha_{i}\\in\\mathcal A_{i}}\\bigr)_{i\\in\\mathcal N}$ where $A_{i}$ is the number of pure actions of player $i\\in\\mathcal{N}$ . With $u$ given by (A.6) \u2013 left, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mu=\\lambda\\left[\\left(\\frac{c}{a_{f}+c},\\frac{-c+d_{f}+d_{s}}{a_{f}+c}\\right),\\left(\\frac{a_{s}-c}{a_{f}+c},1\\right)\\right]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "is a feasible solution of (HG) for any $\\lambda>0$ , so the game is harmonic with a 1-dimensional set of measures. The corresponding strategic center $(m,q)$ with $m_{i}=\\textstyle\\sum_{\\alpha_{i}}\\mu_{i\\alpha_{i}},q_{i}=\\mu_{i}/m_{i},i\\in\\{1,2\\}$ is ", "page_idx": 16}, {"type": "equation", "text": "$$\nm=\\lambda\\left(\\frac{d_{f}+d_{s}}{a_{f}+c},\\frac{a_{f}+a_{s}}{a_{f}+c}\\right),\\;\\;\\;\\;q=\\left[\\left(\\frac{c}{d_{f}+d_{s}},\\frac{-c+d_{f}+d_{s}}{d_{f}+d_{s}}\\right),\\left(\\frac{a_{s}-c}{a_{f}+a_{s}},\\frac{a_{f}+c}{a_{f}+a_{s}}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "As a sanity check, compute the payoff field and verify that (HG-center) holds true in the specialized case (A.6) \u2013 right. Denoting the mixed strategies of players 1 and 2 respectively by $x\\in\\Delta(\\{A,N\\})$ and $y\\in\\Delta(\\{D,N\\})$ , the payoff fields are $v_{1}(x,y)\\,=\\,(-3y_{D}+2y_{N},0)$ , $v_{2}(x,y)=$ $(x_{A}-x_{N},-4x_{A})$ . Choosing $\\lambda=3$ the strategic center gives weights $m=(6,5)$ and Nash equilibrium $q=\\left[\\left(1/6,5/6\\right),\\left(2/5,3/5\\right)\\right]$ . Condition (HG-center) boils down to $6\\left<v_{1},x-q_{1}\\right>+5\\left<v_{2},y-q_{2}\\right>=0$ , which one readily verifies to hold true by replacing the expressions above and recalling that $x_{A}+x_{N}=1=y_{D}+y_{N}$ . Fig. 2 illustrates the situation: each payoff vector $v(x)$ (black arrows) is perpendicular (with respect to a weighted inner product) to the vector $x-q$ (dotted segment) between the evaluation point $x$ of the payoff field and the fully mixed Nash equilibrium $q$ (red point). $\\diamond$ ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "A.4. Harmonic and zero-sum games. Candogan et al. [6]\u2019s uniform harmonic games, defined by Eq. (A.1), are precisely the harmonic games with uniform measure, which makes uniform harmonic games a strict subset of the set of HGs. Importantly, HGs include another archetypal class of perfectcompetition games: as we show in this section, two-player zero-sum games (2ZSGs) with an interior NE $x^{*}$ are harmonic with (probability) measure $\\mu=x^{*}$ . ", "page_idx": 17}, {"type": "text", "text": "To show this, we will need the following definition and lemma: ", "page_idx": 17}, {"type": "text", "text": "Definition A.6 (Non-strategic game). A finite normal form game $\\Gamma=\\Gamma(\\mathcal{N},\\mathcal{A},k)$ is called nonstrategic if the payoff of each player does not depend on their own choice, viz. if $k_{i}(\\alpha_{i},\\alpha_{-i})\\,=$ $k_{i}(\\beta_{i},\\alpha_{-i})$ for all $i\\in\\mathcal{N},\\alpha\\in\\bar{\\mathcal{A}},\\beta_{i}\\in\\mathcal{A}_{i}$ . ", "page_idx": 17}, {"type": "text", "text": "Lemma A.7. Two finite games $\\Gamma(\\mathcal{N},\\mathcal{A},u),\\Gamma^{\\prime}(\\mathcal{N},\\mathcal{A},u^{\\prime})$ are strategically equivalent in the sense of Eq. (A.4) if and only if their difference is a non-strategic game. ", "page_idx": 17}, {"type": "text", "text": "Proof. Let $\\Gamma-\\Gamma^{\\prime}$ be non-strategic; then $k:=u^{\\prime}-u$ fulfills the condition of Definition A.6, which shows that $u$ and $u^{\\prime}$ fulflil Eq. (A.4). Conversely let $\\Gamma$ and $\\Gamma^{\\prime}$ be strategically equivalent; set $k:=u^{\\prime}\\!-\\!u$ and rearrange the terms in Eq. (A.4) to immediately conclude that $k$ is a non-strategic game. \u25a0 ", "page_idx": 17}, {"type": "text", "text": "Proposition A.8. Let $\\Gamma_{\\mu}=\\Gamma_{\\mu}(\\mathcal{N},\\mathcal{A},u)$ be a harmonic game. If the measure $\\mu$ fulfills $\\left|\\mu_{i}\\right|=\\left|\\mu_{j}\\right|$ for all $i,j\\in\\mathcal{N}$ then $\\Gamma_{\\mu}$ is strategically equivalent to a zero-sum game. ", "page_idx": 17}, {"type": "text", "text": "Proof. Recall that $\\begin{array}{r}{|\\mu_{i}|\\equiv\\sum_{\\alpha_{i}}\\mu_{i\\alpha_{i}}}\\end{array}$ . Under the assumption $\\left|\\mu_{i}\\right|=\\left|\\mu_{j}\\right|$ for all $i,j\\in\\mathcal{N}$ , let $c:=\\left|\\mu_{i}\\right|$ for any $i\\in\\mathcal{N}$ . By (HG), the payoff $u$ of $\\Gamma_{\\mu}$ in this case fulfills $\\begin{array}{r}{\\sum_{i\\in\\mathcal{N}}[u_{i}(\\alpha)-k_{i}(\\alpha)]=0}\\end{array}$ for all $\\alpha\\in{\\mathcal{A}}$ , with $\\begin{array}{r}{k_{i}(\\alpha_{i};\\alpha_{-i}):=c^{-1}\\sum_{\\beta_{i}}\\mu_{i\\beta_{i}}u_{i}(\\beta_{i},\\alpha_{-i})}\\end{array}$ . Set $u_{i}^{\\prime}:=u_{i}-k_{i}$ . By definition $u^{\\prime}$ is a zero-sum game; furthermore, the difference between $u_{i}$ and $u_{i}^{\\prime}$ is non-strategic, since $k_{i}(\\alpha_{i};\\alpha_{-i})$ does not depend on $\\alpha_{i}$ . Thus $u_{i}$ and $u_{i}^{\\prime}$ are strategically equivalent by Lemma A.7. \u25a0 ", "page_idx": 17}, {"type": "text", "text": "In particular we have the following: ", "page_idx": 17}, {"type": "text", "text": "Corollary A.9. Let $\\Gamma_{\\mu}\\,=\\,\\Gamma_{\\mu}({\\mathcal N},{\\mathcal A},u)$ be a harmonic game. If the measure $\\mu$ is a probability measure, then $\\Gamma_{\\mu}$ is strategically equivalent to a zero-sum game. ", "page_idx": 17}, {"type": "text", "text": "The converse holds true only in the case of two-player games: ", "page_idx": 17}, {"type": "text", "text": "Proposition A.10. Every two-player zero-sum game with an interior Nash equilibrium $x^{*}$ is harmonic, with (probability) measure $\\mu=x^{*}$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. Let $\\Gamma=\\Gamma(\\mathcal{N},\\mathcal{A},u)$ be a two-player zero-sum game with interior Nash equilibrium $x^{*}$ . If we show that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{i\\in\\mathcal{N}}\\lvert x_{i}^{*}\\rvert\\left\\langle v_{i}(x),x_{i}-\\frac{x_{i}^{*}}{\\lvert x_{i}^{*}\\rvert}\\right\\rangle=0\\quad\\mathrm{for~all}\\;x\\in\\mathcal{X}\\,,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "then we can conclude by Lemma A.3 that $\\Gamma$ is harmonic with measure $x^{*}$ . Eq. (A.9) holds indeed true: $|x_{i}^{*}|=1$ for all $i\\in\\mathcal{N}$ , and it is well known [41, 42] that two-player zero-sum games with an interior equilibrium $x^{*}$ fulfill $\\sum_{i\\in\\mathcal{N}}\\langle v_{i}(x),x_{i}-x_{i}^{*}\\rangle=0$ for all $x\\in\\mathcal{X}$ , so we are done. \u25a0 ", "page_idx": 17}, {"type": "text", "text": "Harmonic games thus encompass and substantially generalize two prototypical classes of games with anti-aligned incentives, serving as an ideal complement to the class of potential games. This is made precise in [1]: building on the work of Candogan et al. [6], Abdou et al. [1] showed that, for any choice of game measure $\\mu$ , every finite game can be uniquely decomposed into the sum of a potential and a $\\mu$ -harmonic game, up to strategic equivalence. ", "page_idx": 17}, {"type": "text", "text": "This establishes harmonic games as the natural complement of potential games from a strategic perspective; Theorem 2 in the main text shows that this holds true from a dynamic perspective as well. ", "page_idx": 17}, {"type": "text", "text": "B Basic properties of regularizers and the induced choice maps ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this appendix, we collect a number of properties concerning regularizers and the associated choice maps. To avoid carrying around the player index $i\\in\\mathcal{N}$ , we state all our results for a generic convex subset $\\mathcal{C}$ of some real vector space $\\mathcal{V}$ . The desired properties for FTRL will then be obtained by specializing $\\mathcal{C}$ to $\\mathcal{X}_{i}$ or $\\mathcal{X}$ and $\\mathcal{V}$ to $\\mathbb{R}^{A_{i}}$ or $\\prod_{j}\\mathbb{R}^{A_{j}}$ , depending on the context. ", "page_idx": 18}, {"type": "text", "text": "B.1. Preliminary definitions. To begin, let $\\mathcal{V}$ be a $d$ -dimensional normed space with norm $\\lVert\\cdot\\rVert$ . In what follows, we will write $\\mathcal{V}:=\\mathcal{V}^{*}$ for the dual space of $\\mathcal{V}.$ , $\\langle y,x\\rangle$ for the canonical pairing between $x\\in\\mathcal{V}$ and $y\\in\\mathcal{V}^{*}$ , and $\\|y\\|_{*}=\\operatorname*{max}\\{\\langle y,x\\rangle:\\|x\\|\\leq1\\}$ for the induced dual norm on $\\boldsymbol{\\wp}$ . Following standard conventions in convex analysis, functions will be allowed to take values in the extended real line $\\mathbb{R}\\cup\\{\\infty\\}$ , and if $f\\colon\\mathcal{V}\\rightarrow\\mathbb{R}\\cup\\{\\infty\\}$ is a convex function on $\\mathcal{V}$ , we will denote its effective domain as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname{dom}f:=\\{x\\in\\mathcal{V}:f(x)<\\infty\\}\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In addition, assuming dom $f\\neq\\emptyset$ , the subdifferential of $f$ at $x$ is defined as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\partial f(x):=\\{y\\in\\mathcal{y}:f(x^{\\prime})\\geq f(x)+\\langle y,x^{\\prime}-x\\rangle\\;{\\mathrm{for~all}}\\;x^{\\prime}\\in\\mathcal{V}\\}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and we will denote the domain of subdifferentiability of $f$ as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname{dom}\\partial f=\\left\\{x\\in\\mathcal{V}:\\partial f(x)\\neq\\varnothing\\right\\}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Finally, to ease notation, a convex function $f\\colon{\\mathcal{C}}\\rightarrow\\mathbb{R}$ will be identified with the extended-real-valued function $\\bar{f}\\colon\\mathcal{V}\\rightarrow\\mathbb{R}\\cup\\{\\infty\\}$ that agrees with $f$ on $\\mathcal{C}$ and is identically equal to $\\infty$ on $\\nu\\,\\backslash\\,{\\mathcal{C}}$ . ", "page_idx": 18}, {"type": "text", "text": "With all this in hand, let $\\mathcal{C}$ be a closed convex subset of $\\mathcal{V}$ , and let $h\\colon{\\mathcal{C}}\\to\\mathbb{R}$ be a $K$ -strongly convex regularizer on $\\mathcal{C}$ , that is, ", "page_idx": 18}, {"type": "equation", "text": "$$\nh(p)\\geq h(x)+\\partial h(x;p-x)+\\frac{K}{2}\\|p-x\\|^{2}\\quad\\mathrm{for~all~}p,x\\in\\mathcal{X},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\begin{array}{r}{\\partial h(x;x^{\\prime}\\!-\\!x)=\\operatorname*{lim}_{\\theta\\to0^{+}}\\big[h(x\\!+\\!\\theta(x^{\\prime}\\!-\\!x))\\!-\\!h(x)\\big]/\\theta}\\end{array}$ denotes the one-sided directional derivative of $h$ at $x$ along the direction of $x^{\\prime}-x$ . To proceed, we will need the following basic objects: ", "page_idx": 18}, {"type": "text", "text": "1. The convex conjugate $h^{\\ast}\\colon\\mathcal{V}\\rightarrow\\mathbb{R}$ of $h$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\nh^{*}(y)=\\operatorname*{max}_{x\\in{\\mathcal{X}}}\\{\\langle y,x\\rangle-h(x)\\}\\qquad\\qquad{\\mathrm{~for~all~}}y\\in{\\mathcal{Y}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "2. The regularized choice map \u2013 or mirror map $-\\,Q\\colon\\mathcal{Y}\\rightarrow\\mathcal{X}$ induced by $h$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\nQ(y)=\\operatorname*{arg\\,max}_{x\\in\\mathcal{X}}\\{\\langle y,x\\rangle-h(x)\\}\\qquad{\\mathrm{for~all~}}y\\in\\mathcal{Y}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "3. The associated Fenchel coupling $F\\colon\\mathcal{X}\\times\\mathcal{Y}\\rightarrow\\mathbb{R}$ of $h$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\nF(p,y)=h(p)+h^{*}(y)-\\langle y,p\\rangle\\qquad{\\mathrm{~for~all~}}p\\in{\\mathcal{X}},y\\in{\\mathcal{Y}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Remark. The terminology \u201cFenchel coupling\u201d is due to [38, 40], which we follow closely in terms of notation and conventions. ", "page_idx": 18}, {"type": "text", "text": "The proposition below provides some basic properties concerning the first two objects above: ", "page_idx": 18}, {"type": "text", "text": "Proposition B.1. Let \u210ebe a $K$ -strongly convex regularizer on $\\mathcal{C}$ . Then: ", "page_idx": 18}, {"type": "text", "text": "(a) $Q$ is single-valued on $\\boldsymbol{\\wp}$ ; in particular, for all $x\\in\\operatorname{dom}\\partial h$ and all $y\\in\\mathcal{V}$ , we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\nx=Q(y)\\ \\ \\ i f a n d\\,o n l y\\,i f\\ \\ \\ y\\in\\partial h(x)\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "(b) The image im $Q$ of $Q$ satisfies ri ${\\mathcal{C}}\\subseteq\\operatorname{im}Q=\\operatorname{dom}\\partial h\\subseteq{\\mathcal{C}}$ . ", "page_idx": 18}, {"type": "text", "text": "(c) The convex conjugate $h^{\\ast}\\colon\\mathcal{V}\\to\\mathbb{R}$ of \u210eis differentiable and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Q(y)=\\nabla h^{*}(y)\\quad f o r\\,a l l\\,y\\in\\mathcal{Y}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "(d) $Q$ is $(1/K)$ -Lipschitz continuous, that is, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|Q(y^{\\prime})-Q(y)\\|\\leq(1/K)\\|y^{\\prime}-y\\|_{*}\\quad f o r\\,a l l\\,y,y^{\\prime}\\in\\mathcal{Y}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "(e) For all $p\\in\\mathcal{X}$ and all $y\\in\\mathcal{V}$ , $x=Q(y)$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\partial h(x;p-x)\\geq\\langle y,p-x\\rangle\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. These properties are fairly well known (except possibly the last one), so we only provide a quick proof or a specific pointer to the literature. ", "page_idx": 19}, {"type": "text", "text": "$(a)$ The maximum in (B.6) is attained for all $y\\in\\mathcal{V}^{*}$ and is unique because $h$ is strongly convex. Furthermore, $x$ solves (B.6) if and only if $y-\\partial h(x)\\ni0$ , i.e., if and only if $y\\in\\partial h(x)$ .   \n$(b)$ By (B.8), we readily get $\\operatorname{im}Q=\\operatorname{dom}\\partial h$ . Consequently, the rest of our claim follows from standard results in convex analysis, see e.g., Rockafellar [52, Chap. 26].   \n$(c)$ The equality $\\boldsymbol{Q}=\\boldsymbol{\\nabla}h^{*}$ follows immediately from Danskin\u2019s theorem, see e.g., Bertsekas [4, Proposition 5.4.8, Appendix B]. ", "page_idx": 19}, {"type": "text", "text": "$(d)$ See Rockafellar & Wets [53, Theorem 12.60(b)]. ", "page_idx": 19}, {"type": "text", "text": "$(e)$ Since $y\\in\\partial h(x)$ by (B.8), we readily get that ", "page_idx": 19}, {"type": "equation", "text": "$$\nh(x+\\theta(p-x))\\geq h(x)+\\theta\\langle y,p-x\\rangle\\quad{\\mathrm{for~all~}}\\theta\\in[0,1]\\;.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Hence, by rearranging and taking the limit $\\theta\\to0^{+}$ , we conclude that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\partial h(x;p-x)=\\operatorname*{lim}_{\\theta\\to0^{+}}\\frac{h(x+\\theta(p-x))-h(x)}{\\theta}\\geq\\langle y,p-x\\rangle\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "as claimed. ", "page_idx": 19}, {"type": "text", "text": "Our next proposition collects some basic properties of the Fenchel coupling. ", "page_idx": 19}, {"type": "text", "text": "Proposition B.2. Let $h$ be a $K$ -strongly convex regularizer on $\\mathcal{C}$ . Then, for all $p\\,\\in\\,\\mathcal{X}$ and all $y,y^{\\prime}\\in\\mathcal{Y},$ , we have: ", "page_idx": 19}, {"type": "text", "text": "Proof. These properties are also fairly standard, but we provide a quick proof for completeness. ", "page_idx": 19}, {"type": "text", "text": "$(a)$ By the Fenchel\u2013Young inequality, we have $h(p)+h^{*}(y)\\geq\\langle y,p\\rangle$ for all $p\\in\\mathcal{X},y\\in\\mathcal{Y}$ , with equality if and only if $y\\in\\partial h(p)$ , so our claim is immediate by (B.8). ", "page_idx": 19}, {"type": "text", "text": "$(b)$ Let $x=Q(y)$ so $y\\in\\partial h(x)$ by (B.8). Then, by the definition of $F$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F(p,y)=h(p)+h^{*}(y)-\\langle y,p\\rangle}\\\\ &{\\qquad\\qquad=h(p)+\\langle y,x\\rangle-h(x)-\\langle y,p\\rangle}\\\\ &{\\qquad\\qquad\\geq h(p)-h(x)-\\partial h(x;p-x)}\\\\ &{\\qquad\\qquad\\geq\\frac12K\\Vert x-p\\Vert^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and our proof is complete. ", "page_idx": 19}, {"type": "text", "text": "In view of Proposition B.2, $F(p,y)$ can be seen a \u201cprimal-dual\u201d measure of divergence between $p\\in\\mathcal X$ and $y\\in\\mathcal{V}$ . This observation will play a major role in the sequel. ", "page_idx": 19}, {"type": "text", "text": "B.2. Basic lemmas. Moving forward, we note that the various update steps in (FTRL $+$ ) can be written as ", "page_idx": 19}, {"type": "equation", "text": "$$\ny^{+}=y+w\\quad{\\mathrm{and}}\\quad x^{+}=Q(y^{+})\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for some $y,w\\in\\mathcal{V}$ . With this in mind, we proceed below to state a series of basic lemmas for the Fenchel coupling before and after an update of the form (B.15). These results are not new, see e.g., [32, 40] and references therein; however, the assumptions used to derive them vary significantly in the literature, so we provide detailed proofs for completeness. ", "page_idx": 19}, {"type": "text", "text": "All of the results that follow below are stated for a $K$ -strongly convex regularizer on $\\mathcal{C}$ . The first result is a primal-dual version of the so-called \u201cthree-point identity\u201d for mirror descent [7]: ", "page_idx": 19}, {"type": "text", "text": "Lemma B.1. Fix some $p\\in\\mathcal{X}$ , $y\\in\\mathcal{V}$ , and let $x=Q(y)$ . Then, for all $y^{+}\\in\\mathcal{V}$ , we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\nF(p,y^{+})=F(p,y)+F(x,y^{+})+\\langle y^{+}-y,x-p\\rangle.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. By definition, we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F(p,y^{+})=h(p)+h^{*}(y^{+})-\\langle y^{+},p\\rangle}\\\\ &{\\ F(p,y)=h(p)+h^{*}(y)-\\langle y,p\\rangle}\\\\ &{F(x,y^{+})=h(x)+h^{*}(y^{+})-\\langle y^{+},x\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus, subtracting (B.17b) and (B.17c) from (B.17a), and rearranging, we get ", "page_idx": 20}, {"type": "equation", "text": "$$\nF(p,y^{+})=F(p,y)+F(x,y^{+})-h(x)-h^{*}(y)+\\left<y^{+},x\\right>-\\left<y^{+}-y,p\\right>.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Our assertion then follows by recalling that $x=Q(y)$ , so $h(x)+h^{*}(y)=\\langle y,x\\rangle$ . ", "page_idx": 20}, {"type": "text", "text": "The next result we present concerns the Fenchel coupling before and after a direct update step; similar results exist in the literature, but we again provide a proof for completeness. ", "page_idx": 20}, {"type": "text", "text": "Lemma B.2. Fix some $p\\in\\mathcal X$ and $y,w\\in\\mathcal{V}$ . Then, letting $x=Q(y),\\,y^{+}=y+w,$ , and $x^{+}=Q(y^{+})$ as per (B.15), we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{F(p,y^{+})=F(p,y)+\\langle w,x^{+}-p\\rangle-F(x^{+},y)}\\\\ {\\leq F(p,x)+\\langle w,x-p\\rangle+\\frac{1}{2}K\\|w\\|_{*}^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. By the three-point identity (B.16), we have ", "page_idx": 20}, {"type": "equation", "text": "$$\nF(x,y)=F(x,y^{+})+F(x^{+},x)+\\langle y-y^{+},x^{+}-p\\rangle\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "so our first claim follows by rearranging. For our second claim, simply note that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle F(p,y)+\\langle w,x^{+}-p\\rangle-F(x^{+},y)=F(p,y)+\\langle w,x-p\\rangle+\\langle w,x^{+}-x\\rangle-F(p,y)}\\\\ &{\\quad\\qquad\\qquad\\qquad\\qquad\\leq F(p,y)+\\langle w,x-p\\rangle+\\displaystyle\\frac{1}{2K}\\|w\\|_{*}^{2}+\\displaystyle\\frac{K}{2}\\|x-p\\|^{2}-F(p,y)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "so our claim follows from Proposition B.2. ", "page_idx": 20}, {"type": "text", "text": "The last result we present here is sometimes referred to as a \u201cfour-point lemma\u201d, and concerns the Fenchel coupling before and after an extrapolation step: ", "page_idx": 20}, {"type": "text", "text": "Lemma B.3. Fix some $p\\,\\in\\,{\\mathcal{X}}$ and $y,w_{1},w_{2}\\;\\in\\;\\mathcal{V}$ . Then, letting $x\\;=\\;Q(y),~y_{i}^{+}\\;=\\;y\\:+\\:w_{i},$ , and $x_{i}^{+}=Q(y_{i}^{+}),\\,i=1,2,$ , as per (B.15), we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle F(p,y_{2}^{+})=F(p,y)+\\langle w_{2},x_{1}^{+}-p\\rangle+\\big[\\langle w_{2},x_{2}^{+}-x_{1}^{+}\\rangle-F(x_{2}^{+},y)\\big]}\\\\ {\\displaystyle=F(p,y)+\\langle w_{2},x_{1}^{+}-p\\rangle+\\langle w_{2}-w_{1},x_{2}^{+}-x_{1}^{+}\\rangle-F(x_{2}^{+},y_{1}^{+})-F(x_{1}^{+},y)}\\\\ {\\displaystyle\\leq F(p,y)+\\langle w_{2},x_{1}^{+}-p\\rangle+\\frac{1}{2K}\\|w_{2}-w_{1}\\|_{*}^{2}-\\frac{K}{2}\\|x_{1}^{+}-x\\|^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. By Lemma B.2, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\nF(p,y_{2}^{+})=F(p,y)+\\langle w_{2},x_{2}^{+}-p\\rangle-F(x_{2}^{+},y)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "so (B.22a) follows by writing $\\langle w_{2},x_{2}^{+}-p\\rangle=\\langle w_{2},x_{1}^{+}-p\\rangle+\\langle w_{2},x_{2}^{+}-x_{1}^{+}\\rangle$ , and (B.22b) follows from the three-point identity (B.16) for the Fenchel coupling. Finally, for (B.22c), the Fenchel-Young inequality in Peter-Paul form yields ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\langle w_{2}-w_{1},x_{2}^{+}-x_{1}^{+}\\rangle\\leq\\frac{1}{2K}\\|w_{2}-w_{1}\\|_{*}^{2}+\\frac{K}{2}\\|x_{2}^{+}-x_{1}^{+}\\|^{2}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and our claim follows again by invoking Proposition B.2 to write ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{K}{2}\\|x_{2}^{+}-x_{1}^{+}\\|^{2}-F(x_{2}^{+},y_{1}^{+})-F(x_{1}^{+},y)\\leq-F(x_{1}^{+},y)\\leq-\\frac{K}{2}\\|x_{1}^{+}-x\\|^{2}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and then substituting the result in (B.24) ", "page_idx": 20}, {"type": "text", "text": "Lemmas B.2 and B.3 will be responsible for most of the heavy lifting to derive a Lyapunov function for $\\mathrm{(FTRL+)}$ ). We discuss the relevant details in Appendix D. ", "page_idx": 20}, {"type": "text", "text": "C Continuous-time analysis ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "C.1. Dynamical systems notions. To fix notation, we recall here some basics from the theory of dynamical systems, roughly following [2, 51]. In this section, $\\mathcal{M}$ is an open subset of a Euclidean space of dimension $d$ . ", "page_idx": 21}, {"type": "text", "text": "We consider a system of ordinary differential equations (ODEs) of the form ", "page_idx": 21}, {"type": "equation", "text": "$$\n{\\dot{x}}(t)=X(x(t))\\,,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $x(t)$ is a curve in $\\mathcal{M}$ defined on an open interval $\\mathcal{T}\\subseteq\\mathbb{R}$ (that without loss of generality we assume to contain 0), and $X\\colon\\mathcal{M}\\rightarrow\\mathbb{R}^{d}$ is a smooth function. The function $X$ is called vector field because it assigns a vector $X(x)$ to each point $x$ in $\\mathcal{M}$ , and (DS) is called dynamical system generated by $X$ . ", "page_idx": 21}, {"type": "text", "text": "Given $x_{0}\\in\\mathcal{M}$ , an orbit with initial condition $x_{0}$ is a solution $x(t)$ of (DS) with $x(0)=x_{0}$ . The flow generated by $X$ is the smooth function $\\Theta\\colon\\mathcal{T}\\times\\mathcal{M}\\rightarrow\\mathcal{M}$ such that $\\Theta_{0}{\\big(}x_{0}{\\big)}=x_{0}$ for all $x_{0}\\in\\mathcal{M}$ and $\\begin{array}{r}{\\frac{d}{d t}\\Theta_{t}(x)=\\dot{X}(\\Theta_{t}(x))}\\end{array}$ for all $t\\in\\mathcal{T}$ . In words, $\\Theta_{t}\\left(x_{0}\\right)$ is the orbit $x(t)$ with initial condition $x_{0}$ ; the existence and uniqueness of this function is guaranteed by the existence and uniqueness theorem of solutions of ordinary differential equations. ", "page_idx": 21}, {"type": "text", "text": "A flow $\\Theta$ is called volume-preserving if $\\operatorname{vol}(\\Theta_{t}(\\mathcal{U}))\\ =\\ \\operatorname{vol}(\\mathcal{U})$ for any (Lebesgue) measurable subset $\\mathcal{U}\\subseteq\\mathcal{M}$ and all $t\\,\\in\\,{\\mathcal{T}}$ . Liouville\u2019s theorem gives a sufficient condition for a flow to be volume-preserving based on the divergence of its generating field:10 ", "page_idx": 21}, {"type": "text", "text": "Theorem (Liouville). If div $X\\equiv0$ then the flow generated by $X$ is volume-preserving. ", "page_idx": 21}, {"type": "text", "text": "Volume-preserving flows are closely related to recurrent dynamical patterns. A point $x\\in\\mathcal{M}$ is said to be recurrent under (DS) if, for every neighborhood $\\boldsymbol{\\mathcal{U}}$ of $x\\in\\mathcal{M}$ , there exists an increasing sequence of time $t_{n}\\uparrow\\infty$ such that $\\Theta_{t_{n}}(x)$ is defined and falls in $\\boldsymbol{\\mathcal{U}}$ for all $n$ . Moreover, (DS) is said to be Poincar\u00e9 recurrent if almost every point $x\\in\\mathcal{M}$ is recurrent. The celebrated Poincar\u00e9 recurrence theorem gives a sufficient condition for a dynamical system to be Poincar\u00e9 recurrent: ", "page_idx": 21}, {"type": "text", "text": "Theorem (Poincar\u00e9). Let $X$ be a smooth vector field on $\\mathcal{M}$ . If the flow induced by $X$ is volumepreserving and all the orbits of (DS) are bounded, then (DS) is Poincar\u00e9 recurrent. ", "page_idx": 21}, {"type": "text", "text": "C.2. Basic properties of FTRL. In this section we survey some of the properties of the followthe-regularized-leader learning scheme in a continuous-time, multi-agent setting, in line with the presentations of [16, 38, 41]. For ease of reference we recall here some of the notions introduced in Appendix B and in Sections 2 and 3 from the main body of the paper. ", "page_idx": 21}, {"type": "text", "text": "Let $\\Gamma\\,=\\,\\Gamma(\\mathcal{N},\\mathcal{A},u)$ be a finite normal form game, and let $v$ denote its payoff field. The game\u2019s strategy space is $\\begin{array}{r}{\\mathcal{X}\\stackrel{}{=}\\prod_{j\\in\\mathcal{N}}\\Delta(\\mathcal{A}_{j})\\subseteq\\mathcal{V}:=\\prod_{j}\\mathbb{R}^{\\mathcal{A}_{j}}}\\end{array}$ , and the game\u2019s payoff space is $\\mathcal{V}:=\\mathcal{V}^{*}$ . The payoff field is a map $v\\colon\\tilde{\\mathcal{V}}\\rightarrow\\mathcal{V}$ that evaluat ed at a strategy $x\\in\\mathcal{X}$ acts linearly on any $x^{\\prime}\\in\\mathcal{X}$ by ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\langle v(x),x^{\\prime}\\right\\rangle=\\sum_{i\\in\\mathcal{N}}\\langle v_{i}(x),x_{i}^{\\prime}\\rangle=\\sum_{i\\in\\mathcal{N}}\\sum_{\\alpha_{i}\\in\\mathcal{A}_{i}}v_{i\\,\\alpha_{i}}(x)\\,x_{i\\,\\alpha_{i}}^{\\prime}}\\\\ &{\\qquad\\qquad=\\sum_{i\\in\\mathcal{N}}u_{i}(x_{i}^{\\prime},x_{-i})\\in\\mathbb{R}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Assume now that $\\Gamma$ is played continuously over time. As discussed in Section 3, the main idea behind the follow-the-regularized-leader learning scheme is that, at any given time $t\\geq0$ , each player $i\\in\\mathcal{N}$ tracks their cumulative payoff up to time $t$ and plays a \u201cregularized\u201d best response strategy in light of this information. Concretely, given a cumulative payoff vector $y_{i}(t)\\in\\mathcal{Y}_{i}$ , each player $i\\in\\mathcal{N}$ selects this optimal strategy $x_{i}(t)\\in\\mathcal{X}_{i}$ by means of a regularized best response map $Q_{i}\\colon\\mathcal{Y}_{i}\\rightarrow\\mathcal{X}_{i}$ , a singlevalued analogue of the best-response correspondence $y_{i}\\mapsto\\operatorname{arg\\,max}_{x_{i}\\in\\mathcal{X}_{i}}\\langle y_{i},x_{i}\\rangle$ . A standard way [56] of obtaining such map is to introduce a regularizer function $h_{i}\\colon\\mathcal{X}_{i}\\stackrel{\\cdot}{\\to}\\dot{\\mathbb{R}}$ that is $(i)$ continuous on $\\mathcal{X}_{i},(i i)$ smooth on ri $\\mathcal{X}_{i}$ , the relative interior of $\\mathcal{X}_{i}$ , and $(i i i)$ strongly convex on $\\mathcal{X}_{i}$ (as per Eq. (B.4)); and to consider the induced choice map $Q_{i}\\colon\\mathcal{Y}_{i}\\rightarrow\\mathcal{X}_{i}$ defined by ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Q_{i}(y_{i})=\\arg\\operatorname*{max}_{x_{i}\\in\\mathcal{X}_{i}}\\{\\langle y_{i},x_{i}\\rangle-h_{i}(x_{i})\\}\\quad\\mathrm{for~all~}y_{i}\\in\\mathcal{Y}_{i}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By Proposition B.1, $Q_{i}$ is well-defined and Lipschitz continuous, and it coincides with the differential $\\nabla h_{i}^{*}$ of $h_{i}^{*}:\\mathcal{D}_{i}\\rightarrow\\mathbb{R}$ , the convex conjugate of $h_{i}$ . ", "page_idx": 21}, {"type": "text", "text": "In a continuous time setting, this regularized learning scheme translates into the following implicit equations of motion, which govern the evolution of the cumulative payoff $y(t)\\in\\mathcal{Y}$ and of the mixed strategy profile $x(t)\\in\\mathcal{X}$ as the players attempt to maximize their payoff over time: ", "page_idx": 22}, {"type": "equation", "text": "$$\ny_{i\\,\\alpha_{i}}(t)=y_{i\\,\\alpha_{i}}(0)+\\int_{0}^{t}v_{i\\,\\alpha_{i}}(x(\\tau))\\,d\\tau\\quad\\mathrm{with}\\quad x_{i}(t)=Q_{i}(y_{i}(t))\\,,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for all $i\\in\\mathcal{N},\\alpha_{i}\\in\\mathcal{A}_{i}$ . A straightforward computation shows that this is equivalent to Eq. (5) from Section 3 in the main text, that governs the evolution of the mixed strategy $x(t)\\in\\mathcal{X}$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\nx_{i}(t)=\\underset{p_{i}\\in\\mathcal{X}_{i}}{\\arg\\operatorname*{max}}\\left\\{\\int_{0}^{t}\\!\\!u_{i}(p_{i};x_{-i}(\\tau))\\,d\\tau-h_{i}(p_{i})\\right\\}=\\underset{p_{i}\\in\\mathcal{X}_{i}}{\\arg\\operatorname*{max}}\\left\\{\\int_{0}^{t}\\!\\langle v_{i}(x(\\tau)),p_{i}\\rangle\\,d\\tau-h_{i}(p_{i})\\right\\}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Importantly, Eq. (C.2) can be cast in the form (DS) of a dynamical system in the game\u2019s payoff space. For each $i\\in\\mathcal{N}$ , differentiation with respect to $t$ yields ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\dot{y}_{i}(t)=v_{i}(x(t))\\qquad x_{i}(t)=\\mathcal{Q}_{i}(y_{i}(t))\\,,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and by aggregating the player indices we obtain the system of ODEs ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\dot{y}=Y(y)\\,,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $Y:=v\\circ Q:\\mathcal{Y}\\to\\mathcal{Y}$ is a continuous vector field on $\\boldsymbol{\\wp}$ ; cf. Fig. 3. ", "page_idx": 22}, {"type": "text", "text": "Existence and uniqueness of a global solution $y(t)\\in\\mathcal{Y}$ of Eq. (C.3) for any initial condition $y(0)\\in\\mathcal{Y}$ are guaranteed by standard arguments [38, Prop. 3.1]; in line with the terminology of the previous section we will refer to such a solution as a dual orbit. ", "page_idx": 22}, {"type": "text", "text": "C.3. Constant of motion for harmonic games. The following result shows that FTRL in harmonic games admits a constant of motion. ", "page_idx": 22}, {"type": "text", "text": "Proposition C.1. Let $\\Gamma=\\Gamma(\\mathcal{N},\\mathcal{A},u)$ be a finite game and consider a vector $m\\in\\mathbb{R}_{++}^{N}$ and a fully mixed strategy $q\\in\\mathcal{X}$ . Then the weighted Fenchel coupling $F_{m,q}\\colon\\mathcal{V}\\rightarrow\\mathbb{R}$ defined by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{F_{m,q}(y):=\\sum_{i}m_{i}F_{i}(q_{i},y_{i})=\\sum_{i}m_{i}\\left(h_{i}(q_{i})+h_{i}^{\\ast}(y_{i})-\\langle q_{i},y_{i}\\rangle\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "is a constant of motion under (FTRL-D) if and only if $\\Gamma$ is harmonic with strategic center $(m,q)$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. Let $y(t)$ be a dual orbit. Then by chain rule ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{d}{d t}F_{m,q}(\\boldsymbol{y}(t))=\\sum_{i}m_{i}\\left[\\langle\\nabla h_{i}^{*}(y_{i}),\\dot{y}_{i}\\rangle-\\langle q_{i},\\dot{y}_{i}\\rangle\\right]=\\sum_{i}m_{i}\\left\\langle x_{i}(t)-q_{i},v_{i}(x(t))\\right\\rangle\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the second equality holds by (FTRL-D) and Eq. (B.9). Then, by the characterization of harmonic games in terms of a strategic center (HG-center), the time derivative of the weighted Fenchel coupling vanishes identically along a dual orbit of (FTRL-D) precisely if the underlying game is harmonic. \u25a0 ", "page_idx": 22}, {"type": "text", "text": "The existence of this constant of motion is fundamental for proving Theorem 2, i.e., the Poincar\u00e9 recurrence of continuous-time FTRL in harmonic games. With this key element established, the remainder of this appendix closely follows the proof technique described by [41] for the analogous result in the context of two-player zero-sum games. ", "page_idx": 22}, {"type": "text", "text": "C.4. FTRL in the space of payoff differences. For any initial condition $y(0)\\in\\mathcal{Y}$ , a dual orbit of (FTRL-D) induces a curve $x(t)=Q(y(t))$ in the game\u2019s strategy space $\\mathcal{X}$ which solves Eq. (5) for all $t\\geq0$ ; in the following we will refer to such curve as trajectory of play. Crucially, a trajectory of play is in general not the global solution of a dynamical system ${\\dot{x}}=X(x)$ for some vector field $X\\colon\\mathcal{X}\\rightarrow\\mathcal{X}$ in the game\u2019s strategy space. The reason for this is that the map $Q\\colon\\mathcal{Y}\\rightarrow\\mathcal{X}$ is not necessarily invertible, so there is in general no way to identify a unique a vector field $X$ on $\\mathcal{X}$ that is related to the vector field $Y$ on $\\boldsymbol{\\wp}$ via $Q$ . ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\mathcal{V}=\\prod_{j}\\mathbb{R}^{A_{j}}\\xrightarrow[{\\hphantom{e}\\mathcal{Q}}]{v}3\\mathcal{V}=\\mathcal{V}^{*}\\xrightarrow[{\\hphantom{e}\\mathcal{F}}]{F}\\,\\mathbb{R}}}\\\\ {{\\displaystyle\\sum_{X\\,\\underline{{\\hat{\\sigma}}}}\\sum_{\\hat{Q}}\\Biggl[\\Pi_{\\hat{\\mathbf{Z}}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "image", "img_path": "", "img_caption": ["Figure 3: FTRL diagram. Commutative diagram of the maps discussed in Appendices C.2\u2013C.4; note in particular that $v\\circ Q$ is a vector field on $\\boldsymbol{\\wp}$ . The notation $\\mathcal X\\hookrightarrow\\mathcal V$ is equivalent to $\\mathcal{X}\\subseteq\\mathcal{V}$ . "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Related vector fields and induced dynamical systems The concept of vector fields related by a smooth map is standard in differential geometry (e.g., [35, p. 181]). Let $\\mathcal{M}$ , $\\mathcal{M}^{\\prime}$ be open subsets of Euclidean space: given a vector field $Y$ on $\\mathcal{M}$ and a smooth map $F\\colon\\mathcal{M}\\rightarrow\\mathcal{M}^{\\prime}$ , a vector field $X$ on $\\mathcal{M}^{\\prime}$ is called $F$ -related to $Y$ if, for all $\\prime\\in\\mathcal{M},(\\mathrm{Jac}\\,F)_{y}\\cdot Y(y)=X(x)$ , with $x=F(y)$ . Here Jac $F$ is the Jacobian matrix of $F$ , and $\\cdot$ represents matrix-vector multiplication. If $F$ is invertible then such vector field exists always and is unique; else, it might exist and not be unique, or not exist at all. ", "page_idx": 23}, {"type": "text", "text": "Vector fields that are related via a smooth map are useful inasmuch as they generate \u201ccompatible\u201d dynamical systems: ", "page_idx": 23}, {"type": "text", "text": "Lemma C.2. Let $F\\colon\\mathcal{M}\\rightarrow\\mathcal{M}^{\\prime}$ be a smooth map between open subsets of Euclidean spaces, and let ${\\dot{y}}=Y(y)$ be a dynamical system on $\\mathcal{M}$ . Let $y(t)$ be an orbit with initial condition $y_{0}\\in\\mathcal{M}$ , and consider the curve on $\\mathcal{M}^{\\prime}$ defined by $x(t):=F(y(t))$ . If there exists a vector field $X$ on ${\\mathcal{M}}^{\\prime}$ that is $F$ -related to $Y$ , then the curve $x(t)$ is an orbit of the dynamical system ${\\dot{x}}=X(x)$ with initial condition $x_{0}=F(y_{0})$ . ", "page_idx": 23}, {"type": "text", "text": "Proof. By chain rule, ", "page_idx": 23}, {"type": "equation", "text": "$$\n{\\frac{d}{d t}}x(t)={\\frac{d}{d t}}F(y(t))=(\\operatorname{Jac}F)_{y(t)}\\cdot{\\dot{y}}(t)=(\\operatorname{Jac}F)_{y(t)}\\cdot Y(y(t))=X(x(t))\\,,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the last equality follows by the assumption that $X$ is $F$ -related to $Y$ . ", "page_idx": 23}, {"type": "text", "text": "In the following, if $F\\colon\\mathcal{M}\\rightarrow\\mathcal{M}^{\\prime}$ is a smooth function between open subsets of Euclidean spaces, and $Y,X$ are vector fields fulfliling the assumptions of Lemma C.2, we say that the dynamical system ${\\dot{y}}=Y(y)$ on $\\mathcal{M}$ induces the dynamical system ${\\dot{x}}=X(x)$ on ${\\mathcal{M}}^{\\prime}$ via $F$ . ", "page_idx": 23}, {"type": "text", "text": "FTRL induced in the space of payoff differences The choice map $Q\\colon\\mathcal{Y}\\rightarrow\\mathcal{X}$ is in general not smooth, and neither injective nor surjective [16, Sec.3], so it generally does not allow to induce the dynamical system (C.3) from the game\u2019s payoff space $\\boldsymbol{\\wp}$ to the game\u2019s strategy space $\\mathcal{X}$ . 11 In other words, the learning process (FTRL-D) in a finite game gives rise to a dynamical system in the game\u2019s payoff space $\\boldsymbol{\\wp}$ , to which the theorems presented in Appendix C.1 can in principle be applied; however, it can be shown that the orbits of Eq. (C.3) in $\\boldsymbol{\\wp}$ are not bounded, preventing the application of Poincar\u00e9\u2019s theorem. Furthermore, the dual orbits do not convey direct information on the day-to-day behavior of the players, due to the lack of invertibility of the choice map. ", "page_idx": 23}, {"type": "text", "text": "Conversely, the objects of interest from a dynamical, learning viewpoint \u2013 that is, the trajectories of play in the game\u2019s strategy space $\\mathcal{X}$ \u2013 present technical difficulties and do not easily fti the dynamical systems framework depicted in Appendix C.1. In the following we show how these difficulties can be circumvented by analyzing the dynamics induced by (FTRL-D) in yet a third space $\\mathcal{Z}$ , that arises by taking the differences between payoffs \u2013 rather than their absolute values \u2013 as the objects of study. ", "page_idx": 23}, {"type": "text", "text": "To make this precise, given the game $\\Gamma=\\Gamma(\\mathcal{N},\\mathcal{A},u)$ fix a benchmark strategy $\\hat{\\alpha}_{i}\\in\\mathcal{A}_{i}$ for every player $i\\in\\mathcal{N}$ , and consider the hyperplane $\\mathcal{Z}_{i}:=\\{z_{i}\\in\\mathbb{R}^{A_{i}}:z_{i\\hat{\\alpha}_{i}}=0\\}\\subset\\mathbb{R}^{A_{i}}$ . Clearly, $\\mathcal{Z}_{i}\\cong\\mathbb{R}^{A_{i}-\\tilde{1}}$ . Each player\u2019s strategy space $\\mathcal{V}_{i}=\\mathbb{R}^{A_{i}}$ can be mapped onto $\\mathcal{Z}_{i}$ by the linear operator ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\Pi_{i}:\\mathcal{V}_{i}\\to\\mathcal{Z}_{i}\\quad\\mathrm{with}\\quad z_{i\\alpha_{i}}=y_{i\\alpha_{i}}-y_{i\\hat{\\alpha}_{i}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for all $\\alpha_{i}\\in\\mathcal{A}_{i}$ ", "page_idx": 23}, {"type": "text", "text": "$\\Pi_{i}$ is clearly smooth, and a standard check shows that $\\Pi_{i}$ is surjective and not injective: ker $\\Pi_{i}=$ $\\{y_{i}\\,:\\,y_{i\\alpha_{i}}\\,=\\,y_{i\\beta_{i}}$ for all $\\alpha_{i},\\beta_{i}\\,\\in\\,\\mathcal{A}_{i}\\}$ is the 1-dimensional linear subspace spanned by the vector $\\mathbf{1}_{i}=(1,\\ldots,1)\\in\\mathcal{V}_{i}$ ; and $\\Pi^{-1}(z_{i})=z_{i}+\\ker\\Pi_{i}$ for any $z_{i}\\in\\mathcal{Z}_{i}$ . In particular, for all $y_{i},y_{i}^{\\prime}\\in\\mathcal{Y}_{i}$ , we have that $\\Pi_{i}(y_{i})=\\Pi_{i}(y_{i}^{\\prime})$ if and only if $y_{i}-y_{i}^{\\prime}$ is proportional to $\\mathbf{1}_{i}$ . ", "page_idx": 24}, {"type": "text", "text": "Since every $z_{i}\\in\\mathcal{Z}_{i}$ is the image of some payoff $y_{i}$ via $\\Pi_{i}$ , the space $\\mathcal{Z}:=\\Pi_{j}\\,\\mathcal{Z}_{j}$ is called the game\u2019s payoff-difference space; we will denote by $\\Pi$ the product map $\\Pi\\equiv\\Pi_{j}\\,\\Pi_{j}$ , i.e., (cf. Fig. 3) ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\Pi\\colon\\mathcal{X}\\to\\mathcal{Z},\\quad\\Pi(y):=(\\Pi_{i}(y_{i}))_{i\\in\\mathcal{N}}\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Lemma C.3. The choice map $Q\\colon\\mathcal{Y}\\rightarrow\\mathcal{X}$ is invariant on the level sets of $\\Pi$ . ", "page_idx": 24}, {"type": "text", "text": "Proof. Let $y,y^{\\prime}\\in\\mathcal{Y}$ . By the discussion above, $\\Pi(y)=\\Pi(y^{\\prime})$ iff $y_{i}^{\\prime}-y_{i}=\\lambda\\mathbf{1}_{i}$ for some $\\lambda\\in\\mathbb{R}$ . Then for each $i\\in\\mathcal{N}$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\nQ_{i}(y_{i}^{\\prime})=\\underset{x_{i}\\in\\mathcal{X}_{i}}{\\mathrm{arg\\,max}}\\{\\langle y_{i}^{\\prime},x_{i}\\rangle-h_{i}(x_{i})\\}=\\underset{x_{i}\\in\\mathcal{X}_{i}}{\\mathrm{arg\\,max}}\\{\\langle y_{i},x_{i}\\rangle+\\lambda\\langle\\mathbf{1}_{i},x_{i}\\rangle-h_{i}(x_{i})\\}=Q_{i}(y_{i})\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proposition C.4. The dynamical system (C.3) in the game\u2019s payoff space $\\boldsymbol{\\wp}$ induces a dynamical system ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\dot{z}=Z(z)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "in the game\u2019s payoff-difference space $\\mathcal{Z}$ via the map (C.8). ", "page_idx": 24}, {"type": "text", "text": "Proof. By the discussion in the previous section (and in particular Lemma C.2), if we exhibit a vector field $Z$ on $\\mathcal{Z}$ that is $\\Pi$ -related to $Y$ , then our proof is complete. Thus we look for a vector field $Z$ such that, for all $y\\in\\mathcal{V}$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n(\\mathrm{Jac}\\,\\Pi)_{y}\\cdot Y(y)=Z(z),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "with $z=\\Pi(y)$ . By Eq. (C.7), $(\\mathrm{Jac}\\,\\Pi_{i})_{\\alpha_{i}\\beta_{i}}=\\delta_{\\alpha_{i}\\beta_{i}}-\\delta_{\\hat{\\alpha}_{i}\\beta_{i}}$ . Since $Y=v\\circ Q$ , the sought-after vector field $Z$ must fulfill, for all $y\\in\\mathcal{V}$ and all $\\alpha_{i}\\in\\mathcal{A}_{i}$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left(v_{i\\alpha_{i}}-v_{i\\hat{\\alpha}_{i}}\\right)\\circ Q_{i}(y_{i})=Z_{i\\alpha_{i}}(z_{i})\\,,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "with $z=\\Pi(y)$ . For each $i\\in\\mathcal{N}$ define now (cf. Fig. 3) ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\hat{Q}_{i}\\colon\\mathcal{Z}_{i}\\to\\mathcal{X}_{i},\\quad\\hat{Q}_{i}(z_{i})=Q(y_{i})\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for any $y_{i}\\in\\Pi_{i}^{-1}(z_{i})$ , and denote by $\\hat{Q}\\colon\\mathcal{Z}\\rightarrow\\mathcal{X}$ the induced product map. Such map exists since $\\Pi_{i}$ is surjective, and is well-defined by Lemma C.3. It follows that the vector field on $\\mathcal{Z}$ defined by ", "page_idx": 24}, {"type": "equation", "text": "$$\nZ_{i\\alpha_{i}}(z_{i}):=(v_{i\\alpha_{i}}-v_{i\\hat{\\alpha}_{i}})\\circ\\hat{Q}_{i}(z_{i})\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for all $i\\in\\mathcal{N},z_{i}\\in\\mathcal{Z}_{i},\\alpha_{i}\\in\\mathcal{A}_{i}$ fulfills Eq. (C.11), and is hence $\\Pi$ -related to $Y$ . ", "page_idx": 24}, {"type": "text", "text": "This result shows that, for every dual orbit $y(t)$ of Eq. (C.3) with initial condition $y_{0}\\in\\mathcal{V}$ , the curve $z(t)\\,=\\,\\Pi(y(t))$ is an orbit of the dynamical system (C.9) in $\\mathcal{Z}$ with initial condition $\\Pi(y_{0})$ . To conclude this section we give a result implying that if the weighted Fenchel coupling (C.4) is a constant of motion constant then the solution orbits of (C.9) in $\\mathcal{Z}$ are bounded. ", "page_idx": 24}, {"type": "text", "text": "Lemma C.5. For any $i\\,\\in\\,{\\mathcal{N}}$ , let $y_{i,n}$ be a sequence in $y_{i}$ , and let $q_{i}$ be a point in the relative interior of $\\mathcal{X}_{i}$ . If $\\begin{array}{r}{\\mathrm{\\large~\\left.~sup}_{n}|h^{*}(y_{i,n})-\\langle y_{i,n},q_{i}\\rangle|<\\infty}\\end{array}$ , then also the score differences remain bounded, i.e., $|y_{i\\alpha_{i},n}-y_{i\\beta_{i},n}|<\\infty$ for all $\\alpha_{i},\\beta_{i}\\in\\mathcal{A}_{i}$ and all $n$ . ", "page_idx": 24}, {"type": "text", "text": "Proof. See [41, Appendix D]. ", "page_idx": 24}, {"type": "text", "text": "Lemma C.6. If the weighted Fenchel coupling (C.4) is a constant of motion under (FTRL-D) for some fully mixed $q\\in\\mathcal{X}$ then the orbits of $\\dot{z}=Z(z)$ as in Eq. (C.9) are bounded in $\\mathcal{Z}$ . ", "page_idx": 24}, {"type": "text", "text": "Proof. Assume that $F_{m,q}(y)\\;=\\;\\sum_{i}m_{i}F_{i}(q_{i},y_{i})\\;=\\;\\sum_{i}m_{i}\\,\\left(h_{i}(q_{i})+h_{i}^{*}(y_{i})-\\langle q_{i},y_{i}\\rangle\\right)$ is a constant of motion for (FTRL-D) for some fully mixed $q\\,\\in\\,{\\mathcal{X}}$ and some $m\\,\\in\\,\\mathbb{R}_{++}^{N}$ . Let $y(t)$ be an orbit of (FTRL-D) in $\\boldsymbol{\\wp}$ , and let $y_{i,n}\\;:=\\;y_{i}(t_{n})$ for any sequence of time $t_{n}$ . Let furthermore $F_{i,n}\\;:=\\;$ $h_{i}^{*}(y_{i,n})-\\langle q_{i},y_{i,n}\\rangle$ . Then $\\operatorname*{sup}_{n}\\lvert F_{i,n}\\rvert\\,<\\,\\infty$ . By Lemma C.5, this implies that $|z_{i\\alpha_{i},n}|\\,<\\,\\infty$ for all $\\alpha_{i}\\in\\mathcal{A}_{i}$ , all $i\\in\\mathcal{N}$ , and all $n$ . \u25a0 ", "page_idx": 24}, {"type": "text", "text": "C.5. Recurrence of FTRL in harmonic games. We now have all the ingredients to prove that almost every trajectory of play $x(t)$ of (FTRL-D) in harmonic games returns arbitrarily close to its starting point infinitely often. ", "page_idx": 25}, {"type": "text", "text": "Theorem 2. Suppose $\\Gamma$ is harmonic. Then almost every orbit $x(t)$ of (FTRL-D) returns arbitrarily close to its starting point infinitely often: specifically, for (Lebesgue) almost every initial condition $x(0)=Q(y(0))\\in{\\bar{\\mathcal{X}}}$ , there exists an increasing sequence of times $t_{n}\\uparrow\\infty$ such that $x(t_{n})\\to x(0)$ . ", "page_idx": 25}, {"type": "text", "text": "Proof of Theorem 2. The proof relies on the following steps: ", "page_idx": 25}, {"type": "text", "text": "1. the vector field $Z$ defined in Eq. (C.13) has vanishing divergence, so its induced flow is volumepreserving in $\\mathcal{Z}$ by Liouville\u2019s theorem; 2. the orbits of the dynamical system $\\dot{z}=Z(z)$ of Eq. (C.9) are bounded in $\\mathcal{Z}$ since the weighted Fenchel coupling (C.4) is a constant of motion for FTRL in harmonic games; 3. the dynamical system $\\dot{z}=Z(z)$ is recurrent in $\\mathcal{Z}$ by Poincar\u00e9 theorem; 4. by continuity of Eq. (C.12), almost every trajectory of play $x(t)$ of (FTRL-D) with initial condition in the image of $\\hat{Q}$ returns arbitrarily close to its starting point infinitely often. ", "page_idx": 25}, {"type": "text", "text": "Indeed, div $\\begin{array}{r}{Z(z)=\\sum_{i}\\sum_{\\alpha_{i}}\\frac{\\partial}{\\partial z_{i\\alpha_{i}}}\\big((v_{i\\alpha_{i}}-v_{i\\hat{\\alpha}_{i}})\\circ\\hat{Q}_{i}(z_{i})\\big)}\\end{array}$ . For the first term, by chain rule, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{div}\\,Z(z)=\\sum_{i}\\sum_{\\alpha_{i}}\\frac{\\partial v_{i\\alpha_{i}}}{\\partial z_{i\\alpha_{i}}}(\\hat{Q_{i}}(z_{i}))=\\sum_{i}\\sum_{\\alpha_{i}}\\sum_{j}\\sum_{\\beta_{j}}\\frac{\\partial v_{i\\alpha_{i}}}{\\partial x_{j\\beta_{j}}}(\\hat{Q}(z))\\frac{\\partial\\hat{Q}_{j\\beta_{j}}}{\\partial z_{i\\alpha_{i}}}(z)}\\\\ &{\\quad\\quad\\quad=\\sum_{i}\\sum_{\\alpha_{i}}\\sum_{\\beta_{i}}\\frac{\\partial v_{i\\alpha_{i}}}{\\partial x_{i\\beta_{i}}}(\\hat{Q}(z))\\frac{\\partial\\hat{Q}_{i\\beta_{i}}}{\\partial z_{i\\alpha_{i}}}(z)\\equiv0}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "since \ud835\udf15\ud835\udf15\ud835\udc63\ud835\udc65\ud835\udc56\ud835\udc56\ud835\udefc\ud835\udefd\ud835\udc56\ud835\udc56 \u22610 by multilinearity of the payoff functions. The second term yields identical result with $\\hat{\\alpha}_{i}\\gets\\alpha_{i}$ , so we conclude that div $Z=0$ . By Lemma C.6, the invariance of the weighted Fenchel coupling under (FTRL-D) implies that the payoff differences $z_{i\\alpha_{i}}(t)\\,=\\,y_{i\\alpha_{i}}(t)\\,-\\,z_{i\\hat{\\alpha}_{i}}(t)$ remain bounded for all $t\\,\\in\\,[0,\\infty)$ . So, by Poincar\u00e9 theorem, the dynamical system $\\dot{z}=Z(z)$ is Poincar\u00e9 recurrent, i.e., there exists a sequence of time $t_{n}\\uparrow\\infty$ such that $\\textstyle\\operatorname*{lim}_{n\\to\\infty}z(t_{n})=z_{0}$ for almost every $z_{0}\\in\\mathcal{Z}$ . By continuity of (C.12), almost every trajectory of play $x(t)\\,=\\,Q(y(t))\\,=\\,\\hat{Q}(z(t))$ with $x_{0}\\in\\operatorname{im}\\hat{Q}$ fulflils $\\begin{array}{r}{\\operatorname*{lim}_{n\\to\\infty}x(t_{n})=x_{0}}\\end{array}$ , which concludes our proof by noting that the image of $\\hat{Q}$ is the same as the image of $Q$ . \u25a0 ", "page_idx": 25}, {"type": "text", "text": "D Discrete-time analysis ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this appendix, our aim is to provide the full proofs for the discrete-time guarantees of $\\left(\\mathrm{FTRL+}\\right)$ , as presented in Section 4. Our analysis hinges on a series of energy functions and associated template inequalities, which we introduce in the next section. ", "page_idx": 25}, {"type": "text", "text": "D.1. Lyapunov functions and template inequalities for $(\\mathrm{FTRL+})$ ). The main building block of our analysis is a suitable Lyapunov function for the discrete-time algorithmic template $(\\mathrm{FTRL+})$ . Motivated by the continuous-time analysis of Appendix C, we begin by considering the player-specific Fenchel couplings ", "page_idx": 25}, {"type": "equation", "text": "$$\nF_{i}(p_{i},y_{i})=h_{i}(p_{i})+h_{i}^{\\ast}(y_{i})-\\langle y_{i},p_{i}\\rangle\\quad{\\mathrm{for~}}p_{i}\\in{\\mathcal{X}}_{i},y_{i}\\in{\\mathcal{Y}}_{i}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "induced by the regularizer $h_{i}$ of player $i\\in\\mathcal{N}$ . ", "page_idx": 25}, {"type": "text", "text": "Suppose now that the game is harmonic relative to some measure $\\mu\\;=\\;(\\mu_{1},.\\dots,\\mu_{N})$ , let $m_{i}\\;=\\;$ $\\textstyle\\sum_{\\alpha_{i}\\in{\\mathcal{A}}_{i}}\\mu_{i\\alpha_{i}}$ denote the mass of $\\mu_{i}$ , and assume further that each player is running $\\mathrm{(FTRL+)}$ ) with learning rate $\\eta_{i}>0$ . Our analysis will hinge on the energy function ", "page_idx": 25}, {"type": "equation", "text": "$$\nE_{n}=\\sum_{i\\in\\mathcal{N}}\\frac{m_{i}}{\\eta_{i}}F_{i}(p_{i},y_{i,n})\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which, as we show below, satisfies the following template inequality: ", "page_idx": 25}, {"type": "text", "text": "Proposition D.1. Suppose that each player is running $\\mathrm{(FTRL+)}$ ) with learning rate $\\eta_{i}\\,>\\,0$ in $a$ harmonic game as above. Then, for all $p_{i}\\in\\mathcal{X}_{i}$ , $i\\in\\mathcal{N}$ , and all $n=1,2,\\ldots$ , we have: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E_{n+1}\\leq E_{n}+\\displaystyle\\sum_{i\\in N}m_{i}\\langle v_{i}(x_{n+1/2}),x_{i,n+1/2}-p_{i}\\rangle}\\\\ &{\\qquad\\qquad+\\displaystyle\\sum_{i\\in N}m_{i}\\langle v_{i}(x_{n+1/2})-v_{i}(x_{n}),x_{i,n+1}-x_{i,n+1/2}\\rangle}\\\\ &{\\qquad\\qquad+\\displaystyle\\sum_{i\\in N}m_{i}(1-\\lambda_{i})\\langle v_{i}(x_{n})-v_{i}(x_{n-1/2}),x_{i,n+1}-x_{i,n+1/2}\\rangle}\\\\ &{\\qquad\\qquad-\\displaystyle\\sum_{i\\in N}\\frac{m_{i}}{\\eta_{i}}F_{i}(x_{i,n+1},y_{i,n+1/2})}\\\\ &{\\qquad\\qquad-\\displaystyle\\sum_{i\\in N}\\frac{m_{i}}{\\eta_{i}}F_{i}(x_{i,n+1/2},y_{i,n})}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. We begin by applying the bound (B.22b) of Lemma B.3 with the array of substitutions ", "page_idx": 26}, {"type": "text", "text": "1. \ud835\udc5d\u2190\ud835\udc5d\ud835\udc56   \n2. $w_{1}\\gets\\eta_{i}\\hat{v}_{i,n}=\\eta_{i}\\lambda_{i}\\,v_{i}(x_{n})+\\eta_{i}(1-\\lambda_{i})\\,v_{i}(x_{n-1/2})$   \n3. $w_{2}\\gets\\eta_{i}\\hat{v}_{i,n+1/2}=\\eta_{i}v_{i}(x_{n+1/2})$   \n$\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\$   \n5. $y_{1}^{+}\\gets y_{i,n+1/2}$ so \ud835\udc651+ \u2190\ud835\udc65\ud835\udc56,\ud835\udc5b+1/2   \n6. $y_{2}^{+}\\gets y_{i,n+1}$ so \ud835\udc652+ \u2190\ud835\udc65\ud835\udc56,\ud835\udc5b+1 ", "page_idx": 26}, {"type": "text", "text": "We then get ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle w_{2}-w_{1},x_{2}^{+}-x_{1}^{+}\\rangle=\\eta_{i}\\langle v_{i}(x_{n+1/2})-\\lambda_{i}\\,v_{i}(x_{n})-(1-\\lambda_{i})\\,v_{i}(x_{n-1/2}),x_{i,n+1}-x_{i,n+1/2}\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\eta_{i}\\langle v_{i}(x_{n+1/2})-v_{i}(x_{n}),x_{i,n+1}-x_{i,n+1/2}\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\quad+\\,\\eta_{i}(1-\\lambda_{i})\\langle v_{i}(x_{n})-v_{i}(x_{n-1/2}),x_{i,n+1}-x_{i,n+1/2}\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and hence, by (B.22b): ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F_{i}(p_{i},y_{i,n+1})\\leq F_{i}(p_{i},y_{i,n})+\\eta_{i}\\langle v_{i}(x_{n+1/2}),x_{i,n+1/2}-p_{i}\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\eta_{i}\\langle v_{i}(x_{n+1/2})-v_{i}(x_{n}),x_{i,n+1}-x_{i,n+1/2}\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\eta_{i}(1-\\lambda_{i})\\langle v_{i}(x_{n})-v_{i}(x_{n-1/2}),x_{i,n+1}-x_{i,n+1/2}\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\quad-F_{i}(x_{i,n+1},y_{i,n+1/2})}\\\\ &{\\qquad\\qquad\\qquad\\quad-F_{i}(x_{i,n+1/2},y_{i,n})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Accordingly, by the definition (D.2) of $E_{n}$ , (D.3) follows by multiplying both sides by $m_{i}/\\eta_{i}$ and summing over $i\\in\\mathcal{N}$ . \u25a0 ", "page_idx": 26}, {"type": "text", "text": "Thanks to this template inequality, we are in a position to establish the following summability guarantee for $\\mathrm{(FTRL+)}$ ). ", "page_idx": 26}, {"type": "text", "text": "Proposition D.2. Suppose that each player in a harmonic game $\\Gamma$ with harmonic measure $\\mu$ is following (FTRL+) with learning rate $\\bar{\\eta}_{i}\\leq m_{i}K_{i}\\,[2(N+2)\\,\\mathrm{max}_{j}\\,m_{j}G_{j}]^{-1}$ . Then, for all $T$ , we have: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{n=1}^{T}\\lVert x_{n+1/2}-x_{n}\\rVert^{2}+\\sum_{n=2}^{T}\\lVert x_{n}-x_{n-1/2}\\rVert^{2}\\leq\\frac{2E_{1}}{(N+2)\\operatorname*{max}_{i}m_{i}G_{i}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. By reshuffling the terms of the template inequality (D.3), we get ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{i\\in\\mathcal{N}}m_{i}\\langle v_{i}(x_{n+1/2}),p_{i}-x_{i,n+1/2}\\rangle\\leq E_{n}-E_{n+1}}}\\\\ &{+\\sum_{i\\in\\mathcal{N}}m_{i}\\langle v_{i}(x_{n+1/2})-v_{i}(x_{n}),x_{i,n+1}-x_{i,n+1/2}\\rangle}\\\\ &{+\\sum_{i\\in\\mathcal{N}}m_{i}(1-\\lambda_{i})\\langle v_{i}(x_{n})-v_{i}(x_{n-1/2}),x_{i,n+1}-x_{i,n+1/2}\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n-\\sum_{i\\in\\mathcal{N}}\\frac{m_{i}}{\\eta_{i}}F_{i}(x_{i,n+1},y_{i,n+1/2})-\\sum_{i\\in\\mathcal{N}}\\frac{m_{i}}{\\eta_{i}}F_{i}(x_{i,n+1/2},y_{i,n})\\;.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We will now proceed to bound each term of (D.7) individually, paying no heed to make the resulting bounds as tight as possible. ", "page_idx": 27}, {"type": "text", "text": "Bounding (D.7a). By the Fenchel-Young inequality, we have: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{D.7a)}\\leq\\displaystyle\\sum_{i\\in N}\\frac{m_{i}}{2G_{i}}\\|v_{i}(x_{n+1/2})-v_{i}(x_{n})\\|_{*}^{2}+\\displaystyle\\sum_{i\\in N}\\frac{m_{i}G_{i}}{2}\\|x_{i,n+1}-x_{i,n+1/2}\\|^{2}}&{}\\\\ {\\leq\\displaystyle\\sum_{i\\in N}\\frac{m_{i}G_{i}}{2}\\|x_{n+1/2}-x_{n}\\|^{2}+\\displaystyle\\sum_{i\\in N}\\frac{m_{i}G_{i}}{2}\\|x_{i,n+1}-x_{i,n+1/2}\\|^{2}\\qquad\\mathrm{~\\mathrm{~\\,~\\,~\\,~}~N B:~}v_{i}(x)\\mathrm{~is~}G_{i}\\mathrm{-Lipschitz}}&{}\\\\ {\\leq\\displaystyle\\frac{1}{2}N\\operatorname*{max}_{i}m_{i}G_{i}\\cdot\\|x_{n+1/2}-x_{n}\\|^{2}+\\displaystyle\\frac{1}{2}\\operatorname*{max}_{i}m_{i}G_{i}\\cdot\\|x_{n+1}-x_{n+1/2}\\|^{2}}&{\\qquad{\\mathrm{~\\,~\\,~}}(\\mathrm{D.8})}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Bounding (D.7b). Again, by the Fenchel-Young inequality, we obtain: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{(\\mathrm{D.7b})\\leq\\sum_{i\\in\\mathcal{N}}\\frac{m_{i}(1-\\lambda_{i})}{2G_{i}}\\|v_{i}(x_{n})-v_{i}(x_{n-1/2})\\|_{*}^{2}+\\sum_{i\\in\\mathcal{N}}\\frac{m_{i}(1-\\lambda_{i})G_{i}}{2}\\|x_{i,n+1}-x_{i,n+1/2}\\|^{2}}}\\\\ &{}&{\\leq\\sum_{i\\in\\mathcal{N}}\\frac{m_{i}(1-\\lambda_{i})G_{i}}{2}\\|x_{n}-x_{n-1/2}\\|^{2}+\\sum_{i\\in\\mathcal{N}}\\frac{m_{i}(1-\\lambda_{i})G_{i}}{2}\\|x_{i,n+1}-x_{i,n+1/2}\\|^{2}}\\\\ &{}&{\\succ\\mathrm{NB};v_{i}(x)\\;\\mathrm{is}\\;G_{i}\\cdot\\mathrm{Lip}}\\\\ &{}&{\\leq\\frac{1}{2}N\\operatorname*{max}_{i}m_{i}G_{i}\\cdot\\|x_{n}-x_{n-1/2}\\|^{2}+\\frac{1}{2}\\operatorname*{max}_{i}m_{i}G_{i}\\cdot\\|x_{n+1}-x_{n+1/2}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Bounding (D.7c). Finally, by the lower bound on the Fenchel coupling of Proposition B.2, we get: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{-\\sum_{i\\in\\mathcal{N}}\\frac{m_{i}}{\\eta_{i}}F_{i}(x_{i,n+1},y_{i,n+1/2})-\\sum_{i\\in\\mathcal{N}}\\frac{m_{i}}{\\eta_{i}}F_{i}(x_{i,n+1/2},y_{i,n})}}\\\\ &{}&{\\leq-\\sum_{i\\in\\mathcal{N}}\\frac{m_{i}K_{i}}{2\\eta_{i}}\\|x_{i,n+1}-x_{i,n+1/2}\\|^{2}-\\sum_{i\\in\\mathcal{N}}\\frac{m_{i}K_{i}}{2\\eta_{i}}\\|x_{i,n+1/2}-x_{i,n}\\|^{2}}\\\\ &{}&{\\leq-\\operatorname*{min}_{i}\\frac{m_{i}K_{i}}{2\\eta_{i}}\\cdot[\\|x_{n+1}-x_{n+1/2}\\|^{2}+\\|x_{n+1/2}-x_{n}\\|^{2}]\\quad\\quad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus, by folding Eqs. (D.8)\u2013(D.10) back into (D.7), we obtain the bound ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{i\\in N}m_{i}\\langle v_{i}(x_{n+1/2}),p_{i}-x_{i,n+1/2}\\rangle\\leq E_{n}-E_{n+1}}&{}\\\\ {\\displaystyle}&{\\qquad+\\,\\frac{1}{2}\\Biggl(N\\operatorname*{max}_{i}m_{i}G_{i}-\\operatorname*{min}_{i}\\frac{m_{i}K_{i}}{\\eta_{i}}\\Biggr)\\|x_{n+1/2}-x_{n}\\|^{2}}\\\\ &{+\\,\\displaystyle\\frac{1}{2}\\Biggl(2\\operatorname*{max}_{i}m_{i}G_{i}-\\operatorname*{min}_{i}\\frac{m_{i}K_{i}}{\\eta_{i}}\\Biggr)\\|x_{n+1}-x_{n+1/2}\\|^{2}}\\\\ &{+\\,\\displaystyle\\frac{1}{2}N\\operatorname*{max}_{i}m_{i}G_{i}\\cdot\\|x_{n}-x_{n-1/2}\\|^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Now, if we instantiate (D.11) to $p\\leftarrow q$ where $q$ is the strategic center of $\\Gamma$ , its left-hand side (LHS) will vanish by (HG-center). Hence, summing over all $n=1,2,\\ldots,T$ , (D.11) ultimately yields ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{0\\le E_{1}+\\displaystyle\\frac{1}{2}\\Bigg(N\\operatorname*{max}_{i}m_{i}G_{i}-\\operatorname*{min}_{i}\\frac{m_{i}K_{i}}{\\eta_{i}}\\Bigg)\\displaystyle\\sum_{n=1}^{T}\\lVert x_{n+1/2}-x_{n}\\rVert^{2}}\\\\ {+\\displaystyle\\frac{1}{2}\\Bigg((N+2)\\operatorname*{max}_{i}m_{i}G_{i}-\\operatorname*{min}_{i}\\frac{m_{i}K_{i}}{\\eta_{i}}\\Bigg)\\displaystyle\\sum_{n=2}^{T}\\lVert x_{n}-x_{n-1/2}\\rVert^{2}}\\\\ {+\\displaystyle\\frac{1}{2}\\Bigg(2\\operatorname*{max}_{i}m_{i}G_{i}-\\operatorname*{min}_{i}\\frac{m_{i}K_{i}}{\\eta_{i}}\\Bigg)\\left\\lVert x_{T+1}-x_{T+1/2}\\right\\rVert^{2}}\\\\ {+\\displaystyle\\frac{1}{2}N\\operatorname*{max}_{i}m_{i}G_{i}\\cdot\\left\\lVert x_{1}-x_{1/2}\\right\\rVert^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Now, by our step-size assumption, we readily obtain ", "page_idx": 28}, {"type": "equation", "text": "$$\n(N+2)\\operatorname*{max}_{i}m_{i}G_{i}\\leq\\frac{1}{2}\\operatorname*{min}_{i}\\frac{m_{i}K_{i}}{\\eta_{i}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "so (D.12) becomes ", "page_idx": 28}, {"type": "equation", "text": "$$\n0\\leq E_{1}-\\frac{1}{4}\\operatorname*{min}_{i}\\frac{m_{i}K_{i}}{\\eta_{i}}\\sum_{n=1}^{T}\\lVert x_{n+1/2}-x_{n}\\rVert^{2}-\\frac{1}{4}\\operatorname*{min}_{i}\\frac{m_{i}K_{i}}{\\eta_{i}}\\sum_{n=2}^{T}\\lVert x_{n}-x_{n-1/2}\\rVert^{2}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where we used our initialization convention $x_{1}=x_{1/2}$ and the fact that the third line of (D.12) is negative. We thus get ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\sum_{n=1}^{T}\\lVert x_{n+1/2}-x_{n}\\rVert^{2}+\\sum_{n=2}^{T}\\lVert x_{n}-x_{n-1/2}\\rVert^{2}\\leq{\\frac{4E_{1}}{\\operatorname*{min}_{i}m_{i}K_{i}/\\eta_{i}}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "from which our assertion follows immediately. ", "page_idx": 28}, {"type": "text", "text": "D.2. Proof of Theorem 3. We are now in a position to prove the regret guarantees of $\\mathrm{(FTRL+)}$ ), which we restate below for convenience. ", "page_idx": 28}, {"type": "text", "text": "Theorem 3. Suppose that each player in a harmonic game $\\Gamma$ is following $\\mathrm{(FTRL+)}$ ) with learning rate $\\eta_{i}\\leq m_{i}K_{i}[\\bar{2(}N\\!+\\!2)\\operatorname*{max}_{j}\\bar{m_{j}}\\bar{G_{j}}]^{-1}$ and payoff signals as per (13) and (16). Then the individual regret of each player $i\\in\\mathcal{N}$ is bounded as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathrm{Reg}_{i}(T):=\\operatorname*{max}_{p_{i}\\in\\mathcal{X}_{i}}\\sum_{n=1}^{T}[u_{i}(p_{i};x_{-i,n})-u_{i}(x_{n})]\\le\\frac{H_{i}}{\\eta_{i}}+\\frac{2G_{i}}{N+2}\\sum_{j\\in\\mathcal{N}}\\frac{H_{j}}{\\eta_{j}G_{j}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $H_{i}=\\operatorname*{max}h_{i}-\\operatorname*{min}h_{i},$ , and $G_{i}$ is the Lipschitz modulus of $v_{i}$ . ", "page_idx": 28}, {"type": "text", "text": "Proof. By a minor reshuffling of terms in (D.5), we readily get ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\langle v_{i}(x_{n+1/2}),p_{i}-x_{i,n+1/2}\\rangle\\leq\\frac{1}{\\eta_{i}}[F_{i}(p_{i},y_{i,n})-F_{i}(p_{i},y_{i,n+1})]}}\\\\ &{}&{\\quad+\\left\\langle v_{i}(x_{n+1/2})-v_{i}(x_{n}),x_{i,n+1}-x_{i,n+1/2}\\right\\rangle}\\\\ &{}&{\\quad+\\left(1-\\lambda_{i}\\right)\\langle v_{i}(x_{n})-v_{i}(x_{n-1/2}),x_{i,n+1}-x_{i,n+1/2}\\rangle}\\\\ &{}&{\\quad-\\,\\frac{1}{\\eta_{i}}F_{i}(x_{i,n+1},y_{i,n+1/2})-\\frac{1}{\\eta_{i}}F_{i}(x_{i,n+1/2},y_{i,n})}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and hence, by a repeated application of the Fenchel-Young inequality in Peter-Paul form: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\langle v_{i}(x_{n+1/2}),p_{i}-x_{i,n+1/2}\\rangle\\leq\\frac{1}{\\eta_{i}}[F_{i}(p_{i},y_{i,n})-F_{i}(p_{i},y_{i,n+1})]}}\\\\ &{}&{\\mathrm+\\,\\frac{1}{2G_{i}}\\|v_{i}(x_{n+1/2})-v_{i}(x_{n})\\|_{*}^{2}+\\frac{G_{i}}{2}\\|x_{i,n+1}-x_{i,n+1/2}\\|^{2}}\\\\ &{}&{\\mathrm+\\,\\frac{1-\\lambda_{i}}{2G_{i}}\\|v_{i}(x_{n})-v_{i}(x_{n-1/2})\\|_{*}^{2}+\\frac{(1-\\lambda_{i})G_{i}}{2}\\|x_{i,n+1}-x_{i,n+1/2}\\|^{2}}\\\\ &{}&{\\mathrm-\\,\\frac{K_{i}}{2\\eta_{i}}\\big[\\|x_{i,n+1}-x_{i,n+1/2}\\|^{2}+\\|x_{i,n+1/2}-x_{i,n}\\|^{2}\\big]\\,.\\qquad\\qquad\\qquad(\\mathrm{D})}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Hence, by using the Lipschitz continuity of $v_{i}$ , we finally get ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\langle v_{i}(x_{n+1/2}),p_{i}-x_{i,n+1/2}\\rangle\\leq\\frac{1}{\\eta_{i}}[F_{i}(p_{i},y_{i,n})-F_{i}(p_{i},y_{i,n+1})]}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad\\qquad+\\frac{G_{i}}{2}\\|x_{n+1/2}-x_{n}\\|^{2}+\\frac{G_{i}}{2}\\|x_{i,n+1}-x_{i,n+1/2}\\|^{2}}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad+\\frac{G_{i}}{2}\\|x_{n}-x_{n-1/2}\\|^{2}+\\frac{G_{i}}{2}\\|x_{i,n+1}-x_{i,n+1/2}\\|^{2}}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad\\qquad-\\frac{K_{i}}{2\\eta_{i}}\\big[\\|x_{i,n+1}-x_{i,n+1/2}\\|^{2}+\\|x_{i,n+1/2}-x_{i,n}\\|^{2}\\big]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Thus, summing over $n\\,=\\,1,2,\\ldots,T$ , and keeping in mind that our assumptions for $\\eta_{i}$ also give $G_{i}<K_{i}/(2\\eta_{i})$ , we finally get ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\sum_{n=1}^{T}\\langle v_{i}(x_{n+1/2}),p_{i}-x_{i,n+1/2}\\rangle\\leq\\frac{H_{i}}{\\eta_{i}}+\\frac{G_{i}}{2}\\left[\\sum_{n=1}^{T}\\lVert x_{n+1/2}-x_{n}\\rVert^{2}+\\sum_{n=2}^{T}\\lVert x_{n}-x_{n-1/2}\\rVert^{2}\\right]\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where we used the fact that $F_{i}(p_{i},0)\\,=\\,h(p)\\,-\\,\\mathrm{min}\\,h_{i}\\,\\le\\,\\mathrm{max}\\,h_{i}\\,-\\,\\mathrm{min}\\,h_{i}\\,=:\\,H_{i}$ . Our result then follows by invoking (D.6) and using the fact that $m_{i}G_{i}\\leq\\operatorname*{max}_{j}m_{j}G_{j}$ for all $i\\in\\mathcal{N}$ . \u25a0 ", "page_idx": 29}, {"type": "text", "text": "D.3. Proof of Theorem 4. With all this said and done, we are finally in a position to prove the convergence of $\\mathrm{(FTRL+)}$ ). For convenience, we restate the relevant result below. ", "page_idx": 29}, {"type": "text", "text": "Theorem 4. Suppose that each player in a harmonic game \u0393 follows $\\mathrm{(FTRL+)}$ ) with learning rate $\\eta_{i}\\leq m_{i}K_{i}\\thinspace[2(N+2)\\operatorname*{max}_{j}m_{j}G_{j}]^{-1}$ and payoff signals as per (13) and (16). Then $x_{n}$ converges to a Nash equilibrium of $\\Gamma$ . ", "page_idx": 29}, {"type": "text", "text": "Proof. By telescoping the template inequality (D.3) over $n=1,2,\\ldots$ and invoking the summability property of $\\left(\\mathrm{FTRL+}\\right)$ as stated in Proposition D.2, we readily conclude that the \u201cdeflated\u201d Fenchel energy (D.2) has $\\operatorname*{sup}_{n}E_{n}<\\infty$ for the range of step-sizes identified in the statement of the theorem (which is the same range that guarantees summability under Proposition D.2). ", "page_idx": 29}, {"type": "text", "text": "Now, since $\\mathcal{X}$ is compact, it follows further that $x_{n}$ and $x_{n+1/2}$ both admit convergent subsequences; in particular, by invoking one more time the summability property (D.6) of $\\left(\\mathrm{FTRL+}\\right)$ , we conclude that there exists some $\\hat{x}\\in\\mathcal{X}$ and an increasing sequence of times $t_{n}\\uparrow\\infty$ such that $x_{t_{n}}$ and $x_{t_{n}+1/2}$ both converge to $\\hat{x}$ . Moreover, since $x=Q(y)$ if and only if $y\\in\\partial h(x)$ , and since the subdifferential is upper hemicontinuous as a set-valued correspondence [52, Chap. 26], we conclude that there exists some $\\hat{y}\\in\\mathcal{Y}$ such that $y_{t_{n}}\\to{\\hat{y}}+\\operatorname{PC}({\\hat{x}})$ , where $\\mathrm{PC}(\\hat{x})$ denotes the polar cone to $\\mathcal{X}$ at $\\hat{x}$ , that is ${\\mathrm{PC}}({\\hat{x}})=\\{y\\in\\mathcal{y}:\\langle y,x-{\\hat{x}}\\rangle\\leq0$ for all $x\\in\\mathcal{X}\\}$ , and the notion of set-theoretic convergence above means that any limit point of $y_{t_{n}}$ lies in $\\hat{y}+\\mathrm{PC}(\\hat{x})$ , which in turn means that $\\hat{x}\\in\\mathrm{PC}(x)$ . However, by the variational characterization (VI) of Nash equilibria, we have that $\\hat{x}\\in\\mathrm{PC}(\\hat{x})$ if and only if $\\hat{x}$ is a Nash equilibrium, implying in turn that $\\hat{x}$ is Nash. Since the above holds for every limit point of $x_{n}$ , we conclude that $x_{n}$ converges to the set of Nash equilibria of the game. Finally, for the pointwise convergence of the algorithm, going back to (D.7) and instantiating it to a (Nash) limit point of $x_{n}$ , we conclude that $E_{n}$ is decreasing. Thus, by descending to the subsequence $x_{t_{n}}$ constructed above, we conclude that $E_{n}\\to0$ , and our proof is complete. \u25a0 ", "page_idx": 29}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Our abstract and introduction follows the same logic and mostly also order as the technical results of the article. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 30}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We discuss exact assumptions throughout. Moreover, in the concluding remarks we focus on limitations that are also natural avenues for future research. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We wrote the article with highest attention to rigor and mathematical detail. As such we provide all assumptions for each result and use unambigious notation and language. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Our simulations are merely illustrative and as such are not critical for the main results of the paper. However, we fully disclose how they were performed. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 31}, {"type": "text", "text": "Answer: [No] ", "page_idx": 32}, {"type": "text", "text": "Justification: We do not release the code at this point as we don\u2019t have time to anonymize it. We describe in detail how simulations are performed and will make the code available at a later stage. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips. cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 32}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We provide all details for the simulations. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 32}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean. ", "page_idx": 32}, {"type": "text", "text": "\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified. \u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). \u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 33}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: All simulations were performed on standard laptops and as such do not carry any particular computational burden. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 33}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 33}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We discuss in the introduction the use of learning in games. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 34}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. \u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 34}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. ", "page_idx": 34}, {"type": "text", "text": "\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 34}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA]   \nJustification:   \nGuidelines:   \n\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 35}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 35}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 35}, {"type": "text", "text": "Guidelines:   \n\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 35}]