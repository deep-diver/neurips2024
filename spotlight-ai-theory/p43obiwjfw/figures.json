[{"figure_path": "p43ObIwJFW/figures/figures_3_1.jpg", "caption": "Figure 1: The processes of DRL-based policy models and our QUBO solver. Both PN and GNN-based DRL policy models construct solutions sequentially by capturing environmental embeddings at each step to support decision-making. In contrast, our neural solver VCM outputs solutions directly in a classification way, without any additional decision steps.", "description": "This figure compares the solution processes of traditional DRL-based methods (PN and GNN) and the proposed Value Classification Model (VCM) for solving the Quadratic Unconstrained Binary Optimization (QUBO) problem.  DRL-based methods solve QUBO sequentially, making decisions one variable at a time based on features extracted at each step.  In contrast, VCM directly generates a complete solution by classifying the values of all variables simultaneously.  The figure highlights the difference in computational complexity and efficiency between the approaches.", "section": "3 The Neural Solver VCM"}, {"figure_path": "p43ObIwJFW/figures/figures_4_1.jpg", "caption": "Figure 2: Working logic of the GST.", "description": "The figure illustrates the workflow of the Greedy-guided Self Trainer (GST).  It starts with input problem instances (Q1 to QNdata). These are fed into the Depth Value Network (DVN) and Value Classification Network (VCN) which produce an initial solution. This solution's objective function value (OFV) is calculated.  The Batch Greedy Flip (BGF) algorithm then iteratively refines the solution. The OFV and the refined solution are used to calculate a Binary Cross Entropy (BCE) Loss, and this loss is used to update the DVN and VCN weights in a training loop.  The best solution obtained across all training epochs is stored as a Historical Best Solution and fed back into the next training cycle. The entire process is self-supervised, meaning it learns without needing pre-labeled optimal solutions.", "section": "3 The Neural Solver VCM"}, {"figure_path": "p43ObIwJFW/figures/figures_7_1.jpg", "caption": "Figure 1: The processes of DRL-based policy models and our QUBO solver. Both PN and GNN-based DRL policy models construct solutions sequentially by capturing environmental embeddings at each step to support decision-making. In contrast, our neural solver VCM outputs solutions directly in a classification way, without any additional decision steps.", "description": "This figure compares the solution processes of traditional Deep Reinforcement Learning (DRL) models using Pointer Networks (PN) or Graph Neural Networks (GNN) with the proposed Value Classification Model (VCM).  DRL methods sequentially build solutions by making decisions based on learned embeddings at each step.  In contrast, VCM directly generates complete solutions through a classification approach.", "section": "3 The Neural Solver VCM"}, {"figure_path": "p43ObIwJFW/figures/figures_7_2.jpg", "caption": "Figure 1: The processes of DRL-based policy models and our QUBO solver. Both PN and GNN-based DRL policy models construct solutions sequentially by capturing environmental embeddings at each step to support decision-making. In contrast, our neural solver VCM outputs solutions directly in a classification way, without any additional decision steps.", "description": "This figure compares the solution processes of traditional DRL-based methods (PN and GNN) and the proposed VCM for solving the QUBO problem.  DRL methods build solutions step-by-step, making sequential decisions based on environmental embeddings. In contrast, VCM directly generates a complete solution through a single classification process, significantly improving efficiency.", "section": "3 The Neural Solver VCM"}, {"figure_path": "p43ObIwJFW/figures/figures_8_1.jpg", "caption": "Figure 5: VCM training process under different methods at instance size 50.", "description": "This figure shows the training curves of various training methods for the VCM model at instance size 50.  The methods compared include an unsupervised training method (UnS), supervised learning with optimal labels (LHB), supervised learning with labels generated by the current VCM-BGF (LGF), and the proposed Greedy-guided Self Trainer (GST). The figure demonstrates that the GST outperforms other methods in both efficiency and stability, achieving similar performance to LHB while requiring fewer epochs and maintaining consistent performance compared to the fluctuating results of UnS and LGF.", "section": "4.3 Model Study"}, {"figure_path": "p43ObIwJFW/figures/figures_8_2.jpg", "caption": "Figure 3: The size generalization ability of VCM and VCM-BGF under different datasets.", "description": "This figure shows the generalization ability of the Value Classification Model (VCM) and its enhanced version VCM-BGF across different dataset sizes.  The x-axis represents the different datasets used, and the y-axis shows the average OFV gap (%). The bars represent the performance of VCM and VCM-BGF on these datasets. The figure demonstrates that even when trained on small datasets, VCM and VCM-BGF maintain good performance on larger datasets. The results indicate remarkable generalization ability.", "section": "4 Experiments"}, {"figure_path": "p43ObIwJFW/figures/figures_8_3.jpg", "caption": "Figure 1: The processes of DRL-based policy models and our QUBO solver. Both PN and GNN-based DRL policy models construct solutions sequentially by capturing environmental embeddings at each step to support decision-making. In contrast, our neural solver VCM outputs solutions directly in a classification way, without any additional decision steps.", "description": "This figure compares the solution processes of traditional deep reinforcement learning (DRL) based methods, specifically those using Pointer Networks (PN) or Graph Neural Networks (GNN), with the proposed Value Classification Model (VCM).  DRL methods solve QUBO problems sequentially, making decisions step-by-step and updating the solution iteratively.  In contrast, VCM directly generates the solution in one step via a classification approach.", "section": "3 The Neural Solver VCM"}, {"figure_path": "p43ObIwJFW/figures/figures_13_1.jpg", "caption": "Figure 1: The processes of DRL-based policy models and our QUBO solver. Both PN and GNN-based DRL policy models construct solutions sequentially by capturing environmental embeddings at each step to support decision-making. In contrast, our neural solver VCM outputs solutions directly in a classification way, without any additional decision steps.", "description": "This figure compares the solution processes of traditional Deep Reinforcement Learning (DRL) models using Pointer Networks (PN) or Graph Neural Networks (GNN) with the proposed Value Classification Model (VCM).  DRL methods solve the Quadratic Unconstrained Binary Optimization (QUBO) problem sequentially, making a series of decisions to flip individual binary variables.  Each decision requires evaluating the impact of the flip on the objective function. VCM, in contrast, solves QUBO in a single classification step, directly predicting the optimal values of all variables simultaneously without sequential decision-making.", "section": "3 The Neural Solver VCM"}, {"figure_path": "p43ObIwJFW/figures/figures_13_2.jpg", "caption": "Figure 1: The processes of DRL-based policy models and our QUBO solver. Both PN and GNN-based DRL policy models construct solutions sequentially by capturing environmental embeddings at each step to support decision-making. In contrast, our neural solver VCM outputs solutions directly in a classification way, without any additional decision steps.", "description": "This figure compares the solution processes of traditional Deep Reinforcement Learning (DRL) methods and the proposed Value Classification Model (VCM).  DRL methods, using Pointer Networks (PN) or Graph Neural Networks (GNNs), iteratively build solutions by making sequential decisions based on learned embeddings. In contrast, VCM directly generates the entire solution in one step via classification, significantly improving efficiency.", "section": "3 The Neural Solver VCM"}, {"figure_path": "p43ObIwJFW/figures/figures_14_1.jpg", "caption": "Figure 1: The processes of DRL-based policy models and our QUBO solver. Both PN and GNN-based DRL policy models construct solutions sequentially by capturing environmental embeddings at each step to support decision-making. In contrast, our neural solver VCM outputs solutions directly in a classification way, without any additional decision steps.", "description": "This figure compares the processes of traditional DRL-based methods (PN and GNN) and the proposed Value Classification Model (VCM) for solving the QUBO problem.  DRL methods solve QUBO sequentially, making decisions step-by-step, evaluating the impact of each decision on the objective function. In contrast, the VCM generates a complete solution directly using a classification approach, significantly improving computational efficiency.", "section": "3 The Neural Solver VCM"}, {"figure_path": "p43ObIwJFW/figures/figures_16_1.jpg", "caption": "Figure 1: The processes of DRL-based policy models and our QUBO solver. Both PN and GNN-based DRL policy models construct solutions sequentially by capturing environmental embeddings at each step to support decision-making. In contrast, our neural solver VCM outputs solutions directly in a classification way, without any additional decision steps.", "description": "This figure compares the solution processes of traditional Deep Reinforcement Learning (DRL) methods (using Pointer Networks (PN) or Graph Neural Networks (GNN)) and the proposed Value Classification Model (VCM). DRL methods solve the Quadratic Unconstrained Binary Optimization (QUBO) problem sequentially, making decisions step-by-step based on embeddings of problem data. In contrast, the VCM directly generates a complete solution through a classification process, significantly improving efficiency.", "section": "3 The Neural Solver VCM"}, {"figure_path": "p43ObIwJFW/figures/figures_17_1.jpg", "caption": "Figure 1: The processes of DRL-based policy models and our QUBO solver. Both PN and GNN-based DRL policy models construct solutions sequentially by capturing environmental embeddings at each step to support decision-making. In contrast, our neural solver VCM outputs solutions directly in a classification way, without any additional decision steps.", "description": "This figure compares the solution processes of traditional Deep Reinforcement Learning (DRL) models (PN and GNN-based) with the proposed Value Classification Model (VCM). DRL models solve the QUBO problem sequentially, making decisions step-by-step based on embedding features. In contrast, the VCM solves the problem directly in a classification way, providing all solution variables simultaneously. The figure highlights the fundamental difference in efficiency and approach between these methods.", "section": "3 The Neural Solver VCM"}, {"figure_path": "p43ObIwJFW/figures/figures_19_1.jpg", "caption": "Figure 1: The processes of DRL-based policy models and our QUBO solver. Both PN and GNN-based DRL policy models construct solutions sequentially by capturing environmental embeddings at each step to support decision-making. In contrast, our neural solver VCM outputs solutions directly in a classification way, without any additional decision steps.", "description": "This figure compares the solution processes of traditional deep reinforcement learning (DRL) based models for solving Quadratic Unconstrained Binary Optimization (QUBO) problems with the proposed Value Classification Model (VCM).  DRL models, using either Pointer Networks (PN) or Graph Neural Networks (GNN), build solutions step-by-step, making decisions at each step based on learned embeddings. In contrast, VCM directly generates a complete solution via a classification approach. The figure visually illustrates this difference in the solution process, emphasizing VCM's efficiency and directness.", "section": "3 The Neural Solver VCM"}, {"figure_path": "p43ObIwJFW/figures/figures_19_2.jpg", "caption": "Figure 1: The processes of DRL-based policy models and our QUBO solver. Both PN and GNN-based DRL policy models construct solutions sequentially by capturing environmental embeddings at each step to support decision-making. In contrast, our neural solver VCM outputs solutions directly in a classification way, without any additional decision steps.", "description": "This figure compares the processes of traditional Deep Reinforcement Learning (DRL) based methods (using Pointer Networks or Graph Neural Networks) and the proposed Value Classification Model (VCM) for solving the Quadratic Unconstrained Binary Optimization (QUBO) problem.  DRL methods solve QUBO sequentially, making decisions one variable at a time, while VCM solves it directly through a single classification step, significantly improving efficiency.", "section": "3 The Neural Solver VCM"}, {"figure_path": "p43ObIwJFW/figures/figures_20_1.jpg", "caption": "Figure 1: The processes of DRL-based policy models and our QUBO solver. Both PN and GNN-based DRL policy models construct solutions sequentially by capturing environmental embeddings at each step to support decision-making. In contrast, our neural solver VCM outputs solutions directly in a classification way, without any additional decision steps.", "description": "This figure compares the solution processes of traditional deep reinforcement learning (DRL) models, which use pointer networks (PN) or graph neural networks (GNN), and the proposed Value Classification Model (VCM).  DRL models solve QUBO problems sequentially by making decisions at each step, which are guided by learned embeddings of the problem's structure. This sequential process can be computationally expensive. In contrast, VCM directly generates the full solution in a single classification step, which significantly increases computational efficiency.", "section": "3 The Neural Solver VCM"}, {"figure_path": "p43ObIwJFW/figures/figures_20_2.jpg", "caption": "Figure 1: The processes of DRL-based policy models and our QUBO solver. Both PN and GNN-based DRL policy models construct solutions sequentially by capturing environmental embeddings at each step to support decision-making. In contrast, our neural solver VCM outputs solutions directly in a classification way, without any additional decision steps.", "description": "This figure compares the solution processes of traditional Deep Reinforcement Learning (DRL) based methods (PN and GNN) and the proposed Value Classification Model (VCM).  DRL methods solve QUBO problems sequentially, making decisions step-by-step.  In contrast, VCM solves the problem by directly generating a classification-based solution in a single step, significantly improving efficiency.", "section": "3 The Neural Solver VCM"}, {"figure_path": "p43ObIwJFW/figures/figures_21_1.jpg", "caption": "Figure 1: The processes of DRL-based policy models and our QUBO solver. Both PN and GNN-based DRL policy models construct solutions sequentially by capturing environmental embeddings at each step to support decision-making. In contrast, our neural solver VCM outputs solutions directly in a classification way, without any additional decision steps.", "description": "This figure compares the solution processes of traditional Deep Reinforcement Learning (DRL) models (PN and GNN-based) with the proposed Value Classification Model (VCM).  DRL methods solve the problem sequentially by making decisions at each step, evaluating the impact of each action on the objective function. In contrast, VCM directly generates a complete solution through a classification process, which avoids the repeated evaluations of objective function values that are computationally costly in the DRL approaches. The figure highlights the different components and workflows of the two types of methods, illustrating VCM's efficiency and innovation.", "section": "3 The Neural Solver VCM"}, {"figure_path": "p43ObIwJFW/figures/figures_22_1.jpg", "caption": "Figure 1: The processes of DRL-based policy models and our QUBO solver. Both PN and GNN-based DRL policy models construct solutions sequentially by capturing environmental embeddings at each step to support decision-making. In contrast, our neural solver VCM outputs solutions directly in a classification way, without any additional decision steps.", "description": "This figure compares the solution processes of traditional Deep Reinforcement Learning (DRL) based models and the proposed Value Classification Model (VCM) for solving Quadratic Unconstrained Binary Optimization (QUBO) problems.  DRL methods, using either Pointer Networks (PN) or Graph Neural Networks (GNN), build solutions step-by-step, making sequential decisions at each step based on learned embeddings of the problem's state. In contrast, the VCM directly generates a complete solution through a single classification step, eliminating the iterative decision-making process of DRL approaches.  The visual representation highlights the key difference in approach, showing the sequential steps of DRL models versus the direct solution output of VCM.", "section": "3 The Neural Solver VCM"}, {"figure_path": "p43ObIwJFW/figures/figures_23_1.jpg", "caption": "Figure 2: Working logic of the GST.", "description": "The figure illustrates the working mechanism of the Greedy-guided Self Trainer (GST). It shows how the GST uses a VCM (Value Classification Model), a BGF (Batch Greedy Flip) algorithm, and an HB (Historical Best Solution) set to iteratively improve solutions. The VCM generates an initial solution, which is then refined by the BGF algorithm to find better solutions. These improved solutions are then stored in the HB set, which provides labels for the next training iteration. This iterative process continues until satisfactory performance is achieved.", "section": "3 The Neural Solver VCM"}, {"figure_path": "p43ObIwJFW/figures/figures_24_1.jpg", "caption": "Figure 21: The optimal training OFV gap of GCN and VCM under different training layer L or depth d at instance size 50.", "description": "This figure compares the performance of GCN and VCM in terms of the optimal OFV gap achieved during training.  The x-axis represents the training depth (for VCM) or the number of layers (for GCN), while the y-axis shows the optimal OFV gap (%).  The graph illustrates that VCM demonstrates significantly better performance and stability compared to GCN as the training depth/number of layers increases. The optimal OFV gap for VCM remains consistently low, while it increases significantly for GCN, highlighting VCM's advantage in this aspect.", "section": "4.3 Model Study"}]