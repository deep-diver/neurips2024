[{"figure_path": "61YYSy078Z/tables/tables_4_1.jpg", "caption": "Table 1b: Computation time (seconds)", "description": "This table presents the computation time, in seconds, required for various algorithms (ECLipsE, ECLipsE-Fast, LipSDP-Neuron, LipSDP-Layer, and CPLip) to estimate the Lipschitz constant for randomly generated neural networks with varying numbers of neurons and layers.  The table showcases the significant speed advantage of ECLipsE and especially ECLipsE-Fast, particularly as network size increases.", "section": "4 Experiments"}, {"figure_path": "61YYSy078Z/tables/tables_17_1.jpg", "caption": "Table 1a: Lipschitz constant estimates", "description": "This table presents the Lipschitz constant estimates obtained using different methods (ECLipsE, ECLipsE-Fast, LipSDP-Neuron, LipSDP-Layer, and CPLip) for randomly generated neural networks with varying numbers of neurons (20, 40, 60, 80, 100) and layers (2, 5, 10, 20, 30, 50, 75, 100).  The \"Trivial Bound\" column provides a naive upper bound for comparison.  The table demonstrates the accuracy and scalability of the proposed methods.", "section": "4 Experiments"}, {"figure_path": "61YYSy078Z/tables/tables_18_1.jpg", "caption": "Table 1b: Computation time (seconds)", "description": "This table presents the computation time in seconds for randomly generated neural networks with varying numbers of neurons (20, 40, 60, 80, 100) and layers (2, 5, 10, 20, 30, 50, 75, 100). The algorithms compared include ECLipSE, ECLipSE-Fast, LipSDP-Neuron, LipSDP-Layer, and CPLip.  The table shows how the computation time increases with the number of neurons and layers for each algorithm, highlighting the scalability and efficiency differences among the methods. Note that LipSDP-Neuron and LipSDP-Layer fail to provide results for larger networks within the 15-minute time limit.", "section": "4 Experiments"}, {"figure_path": "61YYSy078Z/tables/tables_19_1.jpg", "caption": "Table 2a: Normalized Lipschitz Estimates for Randomly Generated NN with 80 Neurons", "description": "This table presents the normalized Lipschitz constant estimates obtained using various methods (ECLipsE, ECLipsE-Fast, LipDiff, LipSDP-Neuron, LipSDP-Layer, and CP-Lip) for randomly generated neural networks with 80 neurons per layer and varying depths (number of layers).  The estimates are normalized by the trivial upper bound, allowing for easier comparison of the tightness of the bounds.  The table shows that ECLipsE and ECLipsE-Fast consistently provide estimates close to the state-of-the-art methods for shallower networks, while other methods fail to provide results or produce estimates that are orders of magnitude larger than the trivial bound for deeper networks.", "section": "4 Experiments"}, {"figure_path": "61YYSy078Z/tables/tables_19_2.jpg", "caption": "Table 2b: Time used (sec) for Randomly Generated NN with 80 Neurons", "description": "This table presents the computation time in seconds for different neural network configurations using various algorithms.  The network configurations vary in terms of the number of layers (20, 30, 50, 75, 100) and a consistent number of neurons (80) per layer.  Algorithms compared include ECLipSE, ECLipSE-Fast, LipDiff, LipSDP-Neuron, LipSDP-Layer, and CP-Lip.  The table shows how computation time increases for deeper networks and compares the efficiency of the proposed algorithms (ECLipsE and ECLipSE-Fast) against existing methods. Note that times greater than 15 minutes are indicated as '>15min'.", "section": "4 Experiments"}, {"figure_path": "61YYSy078Z/tables/tables_19_3.jpg", "caption": "Table 3a: Normalized Lipschitz Estimates for Randomly Generated NN with 50 layers", "description": "This table presents the normalized Lipschitz constant estimates obtained using different methods (ECLipsE, ECLipsE-Fast, LipDiff, LipSDP-Neuron, LipSDP-Layer, and CP-Lip) for randomly generated neural networks with 50 layers and varying number of neurons (20, 40, 60, 80, and 100). The results showcase the performance of the proposed methods against state-of-the-art techniques in terms of accuracy of Lipschitz constant estimation.", "section": "4.1 Randomly Generated Neural Networks"}, {"figure_path": "61YYSy078Z/tables/tables_19_4.jpg", "caption": "Table 3b: Time used (sec) for Randomly Generated NN with 50 Layers", "description": "This table presents the computation time (in seconds) required by different algorithms to compute Lipschitz bounds for randomly generated neural networks with 50 layers. The algorithms compared include ECLipsE, ECLipsE-Fast, LipDiff, LipSDP-Neuron, LipSDP-Layer, and CP-Lip. The number of neurons in each layer varies across rows (20, 40, 60, 80, and 100).  The table shows that ECLipsE-Fast is significantly faster than other methods, especially as the number of neurons increases. LipSDP-Neuron and LipSDP-Layer also show increasing computation times with the number of neurons but are slower than ECLipsE.", "section": "4 Experiments"}, {"figure_path": "61YYSy078Z/tables/tables_20_1.jpg", "caption": "Table 4a: Normalized Lipschitz Estimates for Randomly Generated NN with 50 Layers", "description": "This table presents the normalized Lipschitz constant estimates obtained using different methods for randomly generated neural networks with 50 layers.  The methods compared include ECLipSE, ECLipSE-Fast, LipSDP-Neuron (split into 5 sub-networks), and LipSDP-Layer (split into 5 sub-networks). The number of neurons in each network varies, and the results show the scalability of the proposed methods (ECLipsE and ECLipSE-Fast) compared to the baseline methods.", "section": "4.1 Randomly Generated Neural Networks"}, {"figure_path": "61YYSy078Z/tables/tables_20_2.jpg", "caption": "Table 4b: Time Used (sec) for Randomly Generated NN with 50 Layers", "description": "This table shows the computation time in seconds for different neural network configurations using four different algorithms: ECLipsE, ECLipsE-Fast, LipSDP-Neuron (split into 5 sub-networks), and LipSDP-Layer (split into 5 sub-networks). The number of neurons in each layer is varied (150, 200, 300, 400, 500, 1000), and the table demonstrates how the computation time changes for each algorithm under these different conditions.  A cutoff time of 30 minutes is noted for some experiments indicating where the algorithm did not complete in that timeframe.", "section": "4 Experiments"}]