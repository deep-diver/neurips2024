[{"figure_path": "61YYSy078Z/figures/figures_5_1.jpg", "caption": "Figure 1: Geometric Analysis of ECLipsE", "description": "This figure geometrically illustrates the process of finding the largest constant *c<sub>i</sub>* in the optimization problem (12) for algorithm ECLipsE.  It uses ellipsoids to represent positive semidefinite matrices. The green ellipsoid represents *M<sub>i</sub>*, the blue ellipsoid represents *W<sub>i+1</sub>W<sup>T</sup><sub>i+1</sub>*, and the pink ellipsoid represents *M<sub>i</sub>/c<sub>i</sub>*.  Panel (a) shows the case where *c<sub>i</sub> > 1*, illustrating how the ellipsoid of *W<sub>i+1</sub>W<sup>T</sup><sub>i+1</sub>* is contained within the contracted ellipsoid *M<sub>i</sub>/c<sub>i</sub>*. Panel (b) shows the case where *c<sub>i</sub> < 1*, illustrating how the ellipsoid *M<sub>i</sub>* needs to expand to contain *W<sub>i+1</sub>W<sup>T</sup><sub>i+1</sub>*. The grey vector *v* points to the direction of the zero eigenvector of the singular matrix *N*. The figure visually demonstrates the relationship between the matrix inequalities, the geometric interpretation of the solution, and how finding the optimal *c<sub>i</sub>* leads to a tighter Lipschitz estimate.", "section": "3.3 Theory"}, {"figure_path": "61YYSy078Z/figures/figures_6_1.jpg", "caption": "Figure 1: Geometric Analysis of ECLipsE", "description": "This figure provides a geometric interpretation of the optimization problem in Proposition 3. It illustrates how the largest constant \\(c_i\\) can be found by comparing the shapes of matrices \\(M_i\\) and \\(W_{i+1}W_{i+1}^T\\), represented as ellipsoids.  The green ellipsoid represents \\(M_i\\), and the blue ellipsoid represents \\(W_{i+1}W_{i+1}^T\\).  The pink ellipsoid shows the result of the contraction, where \\(c_i\\) represents the ratio between the lengths of the green and pink arrows.  Figure (a) demonstrates the case where \\(c_i > 1\\), while Figure (b) shows \\(c_i < 1\\).", "section": "3.3 Theory"}, {"figure_path": "61YYSy078Z/figures/figures_6_2.jpg", "caption": "Figure 1: Geometric Analysis of ECLipsE", "description": "This figure geometrically illustrates the process of finding the optimal value of \\(c_i\\) in the optimization problem (12).  It uses ellipsoids to represent positive semidefinite matrices. The blue ellipsoid represents \\(W_{i+1}W_{i+1}^T\\), the green ellipsoid depicts \\(M_i\\), and the pink ellipsoid shows \\(M_i/c_i\\). The figure demonstrates how, by adjusting \\(c_i\\), the ellipsoid \\(W_{i+1}W_{i+1}^T\\) can be contained within \\(M_i/c_i\\). The optimal \\(c_i\\) is the largest value that allows this containment while ensuring \\(M_i > 0\\). (a) shows the case where \\(c_i > 1\\), representing a contraction of the ellipsoid, and (b) illustrates the case where \\(c_i < 1\\), indicating an expansion of the ellipsoid.", "section": "3.3 Theory"}, {"figure_path": "61YYSy078Z/figures/figures_7_1.jpg", "caption": "Figure 3: Performance of ECLipsE-Fast and ECLipsE, with respect to baselines for increasing network depth, with 80 neurons per layer. The red x markings indicate that the algorithm fails to provide an estimate within the computational cutoff time beyond this network size.", "description": "This figure compares the performance of ECLipsE and ECLipsE-Fast algorithms against several baseline methods (LipSDP-neuron, LipSDP-layer, and CPLip) for estimating the Lipschitz constant of neural networks with varying depths (number of layers).  The x-axis represents the number of layers, while the y-axis represents the normalized Lipschitz estimates.  The plot shows that both ECLipsE and ECLipsE-Fast maintain efficiency and accuracy with increasing network depth, while baseline methods struggle and fail to provide estimates beyond a certain number of layers (indicated by red 'x' marks).", "section": "4 Experiments"}, {"figure_path": "61YYSy078Z/figures/figures_7_2.jpg", "caption": "Figure 3: Performance of ECLipsE-Fast and ECLipsE, with respect to baselines for increasing network depth, with 80 neurons per layer. The red x markings indicate that the algorithm fails to provide an estimate within the computational cutoff time beyond this network size.", "description": "The figure shows the performance comparison of ECLipsE and ECLipsE-Fast algorithms against baseline methods (LipSDP-neuron, LipSDP-layer, and CPLip) for estimating Lipschitz constants of neural networks with varying depth (number of layers).  The x-axis represents the number of layers, and the y-axis represents the computation time in seconds.  ECLipsE-Fast shows significantly faster computation times compared to all other algorithms, especially as the network depth increases.  ECLipsE also shows a considerable speed advantage over LipSDP algorithms. The red 'x' markers highlight where the baseline algorithms failed to provide estimates within the given time limit.", "section": "4 Experiments"}, {"figure_path": "61YYSy078Z/figures/figures_8_1.jpg", "caption": "Figure 4: Performance of ECLipsE-Fast and ECLipsE, with respect to baselines as network width increases, for a randomly generated network with 20 layers ((a) and (b)) and 50 layers ((c) and (d)). The Red x markings indicate that the algorithms fail to provide an estimate within the computational cutoff time of 15 min beyond this network size.", "description": "This figure compares the performance of ECLipsE and ECLipsE-Fast against various baseline methods (LipSDP-neuron, LipSDP-layer, CPLip) as the width (number of neurons per layer) of randomly generated neural networks increases.  Two network depths are shown: 20 layers and 50 layers. The plots show normalized Lipschitz estimates and computation times.  The red 'x' denotes cases where the baseline methods failed to produce results within the 15-minute time limit.  The results illustrate the scalability and efficiency of the proposed ECLipsE algorithms, particularly ECLipsE-Fast, even with wider networks.", "section": "4 Experiments"}, {"figure_path": "61YYSy078Z/figures/figures_8_2.jpg", "caption": "Figure 4: Performance of ECLipsE-Fast and ECLipsE, with respect to baselines as network width increases, for a randomly generated network with 20 layers ((a) and (b)) and 50 layers ((c) and (d)). The red x markings indicate that the algorithms fail to provide an estimate within the computational cutoff time of 15 min beyond this network size.", "description": "This figure demonstrates the scalability of ECLipsE and ECLipsE-Fast algorithms in comparison with other baseline methods such as LipSDP-Neuron, LipSDP-Layer and CPLip, as the width of the network (number of neurons per layer) increases.  Subfigures (a) and (c) show the normalized Lipschitz estimates while (b) and (d) show the computation time.  The results show that ECLipsE and ECLipsE-Fast maintain good scalability and accuracy even as the network becomes wider, while other methods fail to produce estimates within a 15-minute time limit for wider networks.", "section": "4 Experiments"}, {"figure_path": "61YYSy078Z/figures/figures_8_3.jpg", "caption": "Figure 4: Performance of ECLipsE-Fast and ECLipsE, with respect to baselines as network width increases, for a randomly generated network with 20 layers ((a) and (b)) and 50 layers ((c) and (d)). The Red x markings indicate that the algorithms fail to provide an estimate within the computational cutoff time of 15 min beyond this network size.", "description": "This figure shows the performance comparison of ECLipsE and ECLipsE-Fast algorithms against several baseline methods (LipSDP-neuron, LipSDP-layer, CPLip) for randomly generated neural networks. The comparison is made by varying the number of neurons (network width) while keeping the number of layers fixed at 20 and 50. The plots show both normalized Lipschitz estimates and computation time in seconds. The red 'x' marks indicate cases where baseline algorithms failed to provide estimates within the 15-minute time limit.", "section": "4 Experiments"}, {"figure_path": "61YYSy078Z/figures/figures_8_4.jpg", "caption": "Figure 4: Performance of ECLipsE-Fast and ECLipsE, with respect to baselines as network width increases, for a randomly generated network with 20 layers ((a) and (b)) and 50 layers ((c) and (d)). The Red x markings indicate that the algorithms fail to provide an estimate within the computational cutoff time of 15 min beyond this network size.", "description": "This figure demonstrates the scalability and efficiency of the proposed algorithms (ECLipsE and ECLipsE-Fast) compared to baseline methods (LipSDP-neuron, LipSDP-layer, and CPLip) as the width (number of neurons per layer) of randomly generated neural networks increases.  It shows the normalized Lipschitz estimates and computation times for networks with 20 and 50 layers. The results highlight that ECLipsE and ECLipsE-Fast maintain relatively low computation times even with increasing network width, unlike the baseline methods which struggle to provide estimates within the time limit for wider networks.", "section": "4 Experiments"}, {"figure_path": "61YYSy078Z/figures/figures_8_5.jpg", "caption": "Figure 5: Computation time vs estimation accuracy for ECLipsE, ECLipsE-Fast and LipSDP splitting with different sub-network sizes.", "description": "This figure compares the computation time and estimation accuracy of ECLipsE, ECLipsE-Fast, and LipSDP (with different sub-network splitting sizes) for neural networks with 100 layers and varying neuron counts (80, 100, 120, 140, 160).  The x-axis represents the computation time used, and the y-axis represents the normalized Lipschitz estimates.  The plot shows that ECLipsE-Fast consistently achieves the lowest computation times, while maintaining relatively high accuracy. ECLipsE also demonstrates good efficiency and accuracy, outperforming LipSDP across different splitting sizes.", "section": "4 Experiments"}, {"figure_path": "61YYSy078Z/figures/figures_9_1.jpg", "caption": "Figure 3: Performance of ECLipsE-Fast and ECLipsE, with respect to baselines for increasing network depth, with 80 neurons per layer. The red x markings indicate that the algorithm fails to provide an estimate within the computational cutoff time beyond this network size.", "description": "This figure compares the performance of the proposed algorithms, ECLipsE and ECLipsE-Fast, against several baseline methods (LipSDP-neuron, LipSDP-layer, and CPLip) for estimating the Lipschitz constant of neural networks with varying depth. The x-axis represents the number of layers, while the y-axis shows the normalized Lipschitz estimates and the computation time (in seconds).  The plot highlights that ECLipsE and ECLipsE-Fast are scalable to much deeper networks compared to the baselines, which fail to provide estimates beyond a certain depth (indicated by red 'x' marks). Notably, ECLipsE-Fast is significantly faster than others while maintaining accuracy.", "section": "4 Experiments"}, {"figure_path": "61YYSy078Z/figures/figures_9_2.jpg", "caption": "Figure 4: Performance of ECLipsE-Fast and ECLipsE, with respect to baselines as network width increases, for a randomly generated network with 20 layers ((a) and (b)) and 50 layers ((c) and (d)). The Red x markings indicate that the algorithms fail to provide an estimate within the computational cutoff time of 15 min beyond this network size.", "description": "This figure compares the performance of ECLipsE and ECLipsE-Fast against baseline methods (CPLip, LipSDP-neuron, LipSDP-layer, and LipDiff) as the width (number of neurons per layer) of randomly generated neural networks increases. The networks have 20 and 50 layers. The plots show (a,c) normalized Lipschitz estimates and (b,d) computation times (in seconds).  The red 'x' symbol indicates that a method failed to return an estimate within the 15-minute time limit.", "section": "4 Experiments"}, {"figure_path": "61YYSy078Z/figures/figures_15_1.jpg", "caption": "Figure 1: Geometric Analysis of ECLipsE", "description": "This figure provides a geometric interpretation of the optimization problem in Proposition 3.  Panel (a) illustrates the case where the optimal value \\(c_i > 1\\), showing how the ellipsoid representing \\(M_i\\) contracts to contain the ellipsoid representing \\(W_{i+1}W_{i+1}^T\\).  The largest such contraction is shown to be tangent to \\(W_{i+1}W_{i+1}^T\\). Panel (b) shows the case where \\(c_i < 1\\), illustrating the expansion needed for \\(M_i\\) to contain \\(W_{i+1}W_{i+1}^T\\).  In both cases, the relationship between the ellipsoids demonstrates the geometric meaning of maximizing \\(c_i\\).", "section": "3.3 Theory"}]