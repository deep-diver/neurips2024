[{"figure_path": "bOYVESX7PK/figures/figures_3_1.jpg", "caption": "Figure 1: Schematic of Koopman operator theory-based identification of conjugate dynamical systems. (A) By lifting nonlinear dynamics from a finite dimensional state-space to an infinite dimensional function space, a linear representation can be achieved (from which a finite dimensional approximation can be obtained). (B) The linearity of the Koopman operator enables a mode decomposition, which includes Koopman eigenvalues (orange), eigenfunctions (green), and modes (blue). (C) Dynamical systems with the same Koopman eigenvalues are topologically conjugate.", "description": "This figure schematically illustrates the Koopman operator theory and its application to identifying conjugate dynamical systems. Panel A shows how nonlinear dynamics in a finite-dimensional state space can be lifted to a linear representation in an infinite-dimensional function space using the Koopman operator. Panel B illustrates the Koopman mode decomposition, which breaks down the lifted dynamics into a sum of Koopman modes, each characterized by its eigenvalue, eigenfunction, and mode. Finally, panel C demonstrates that two dynamical systems are topologically conjugate if they have the same Koopman eigenvalues.", "section": "3 Identifying equivalent training dynamics"}, {"figure_path": "bOYVESX7PK/figures/figures_5_1.jpg", "caption": "Figure 2: Conjugacy between online mirror descent and online gradient descent is identifiable from Koopman spectra. (A) Comparing example trajectories of variables optimized via OMD (x1, x2), OGD (u1, u2), and BM (z1, z2), the existence of a conjugacy between OMD and OGD is not obvious. (B) Similarly, the existence of a conjugacy is not apparent when looking at the loss incurred by using OMD and OGD. (C) Comparing the Koopman eigenvalues associated with optimizing using OMD, OGD, and BM correctly identifies the existence of a conjugacy between OMD and OGD, and the lack of a conjugacy between OMD/OGD and BM. The function optimized is in all subfigures is f(x) = tan(x).", "description": "This figure demonstrates that the Koopman spectra can identify the conjugacy between Online Mirror Descent (OMD) and Online Gradient Descent (OGD) which is not obvious from the trajectories or loss functions.  Panel A shows example trajectories of the variables optimized by OMD, OGD, and Bisection Method (BM). Panel B shows loss curves for each method. Panel C shows the Koopman eigenvalues associated with each method; OMD and OGD have very similar eigenvalues while the BM eigenvalues are clearly distinct.", "section": "4.1 Identifying conjugate optimizers"}, {"figure_path": "bOYVESX7PK/figures/figures_6_1.jpg", "caption": "Figure 3: Narrow and wide fully connected neural networks have non-conjugate training dynamics. (A) Training loss curves for FCNs with hidden layer widths h = 5, 10, and 40. Solid line is mean and shaded area is \u00b1 standard deviation across 25 independently trained networks. (B), (C) Example weight trajectories, across training iterations, for narrow, intermediate, and wide FCNs. (D) Koopman eigenvalues associated with training FCNs of varying width. (E) Same as (D), but zoomed out and with the eigenvalues associated with h = 5 and h = 10 compared to those associated with h = 40. Dashed line in (D) and (E) denotes unit circle. (F) Wasserstein distance between Koopman eigenvalues associated with training FCNs of varying width. Error bars are \u00b1 standard deviation across 25 independently trained FCNs. Kolmogorov-Smirnov (KS) tests were performed to assess statistical significance of distance: * denotes p < 0.01 and *** denotes p < 0.0001.", "description": "This figure shows the results of comparing the training dynamics of narrow, intermediate, and wide fully connected neural networks (FCNs).  It demonstrates that narrow and wide networks exhibit non-conjugate training dynamics, meaning their dynamics are not equivalent, while intermediate-width networks show more similarity to the wide networks. This is assessed by analyzing training loss, weight trajectories, Koopman eigenvalues (which indicate the timescales of the system), and the Wasserstein distance between the Koopman eigenvalues.", "section": "4.2 Identifying the effect of width on fully connected neural network training"}, {"figure_path": "bOYVESX7PK/figures/figures_7_1.jpg", "caption": "Figure 4: Koopman-based framework enables identification of transitions in dynamics during the early phase of training for LeNet and ResNet-20. (A) Log10 Wasserstein distance between Koopman eigenvalues associated with LeNet training over windows of 100 training iterations during epoch 1. (B) Same as (A), but for ResNet-20 training. (C) Koopman eigenvalues associated with the dynamics that occur during training iterations intervals 0\u201399, 400\u2013499, and 600\u2013699. Dashed line denotes the unit circle.", "description": "This figure demonstrates the application of the Koopman operator framework to analyze the early training dynamics of two different Convolutional Neural Networks (CNNs): LeNet (trained on MNIST) and ResNet-20 (trained on CIFAR-10).  The figure shows how the Wasserstein distance between Koopman eigenvalues changes over the course of training.  The results suggest that CNN training dynamics undergo transitions, with similar patterns observed for both LeNet and ResNet-20, although the specific dynamics are architecture-specific.", "section": "4.3 Identifying dynamical transitions in convolutional neural network training"}, {"figure_path": "bOYVESX7PK/figures/figures_8_1.jpg", "caption": "Figure 5: Transformers that do, and that do not undergo grokking have early training dynamics that are not conjugate. (A) Train and test loss, as a function of training steps, for a Transformer model that undergoes grokking. (B) Same as (A), but for a Transformer whose training is constrained to have a constant weight norm [62]. In this case, no grokking is observed. (C) In the first 100 training steps, little difference is seen between the test loss of Transformers with and without constrained training. Lines are mean and shaded area is \u00b1 standard deviation across 20 independently trained networks. (D) Koopman eigenvalues associated with the dynamics that occur over the first 100 training iterations for Transformers that do, and that not undergo grokking.", "description": "This figure shows the comparison of training dynamics between transformers with and without grokking. The top panels show the training and testing accuracy curves for both models, where the constrained model (no grokking) shows a steady increase in accuracy, while the unconstrained model (grokking) exhibits a sharp increase in accuracy after a certain number of iterations.  The bottom left panel shows the train and test accuracy for the first 100 training iterations of both models, revealing subtle differences in the learning behavior in early training. The bottom right panel presents a comparison of Koopman eigenvalues, highlighting the distinct spectral properties that indicate non-conjugate training dynamics between the two models.", "section": "4.4 Identifying non-conjugate training dynamics for Transformers that do and do not grok"}, {"figure_path": "bOYVESX7PK/figures/figures_15_1.jpg", "caption": "Figure 2: Conjugacy between online mirror descent and online gradient descent is identifiable from Koopman spectra. (A) Comparing example trajectories of variables optimized via OMD (x1, x2), OGD (u1, u2), and BM (z1, z2), the existence of a conjugacy between OMD and OGD is not obvious. (B) Similarly, the existence of a conjugacy is not apparent when looking at the loss incurred by using OMD and OGD. (C) Comparing the Koopman eigenvalues associated with optimizing using OMD, OGD, and BM correctly identifies the existence of a conjugacy between OMD and OGD, and the lack of a conjugacy between OMD/OGD and BM. The function optimized is in all subfigures is f(x) = tan(x).", "description": "This figure demonstrates that the topological conjugacy between Online Mirror Descent (OMD) and Online Gradient Descent (OGD) can be identified using Koopman spectra, despite not being obvious from comparing trajectories or loss alone. The non-conjugacy between OMD/OGD and the Bisection Method (BM) is also shown.  The figure uses the function f(x) = tan(x) for all subfigures.", "section": "4.1 Identifying conjugate optimizers"}, {"figure_path": "bOYVESX7PK/figures/figures_17_1.jpg", "caption": "Figure 3: Narrow and wide fully connected neural networks have non-conjugate training dynamics. (A) Training loss curves for FCNs with hidden layer widths h = 5, 10, and 40. Solid line is mean and shaded area is \u00b1 standard deviation across 25 independently trained networks. (B), (C) Example weight trajectories, across training iterations, for narrow, intermediate, and wide FCNs. (D) Koopman eigenvalues associated with training FCNs of varying width. (E) Same as (D), but zoomed out and with the eigenvalues associated with h = 5 and h = 10 compared to those associated with h = 40. Dashed line in (D) and (E) denotes unit circle. (F) Wasserstein distance between Koopman eigenvalues associated with training FCNs of varying width. Error bars are \u00b1 standard deviation across 25 independently trained FCNs. Kolmogorov-Smirnov (KS) tests were performed to assess statistical significance of distance: * denotes p < 0.01 and *** denotes p < 0.0001.", "description": "This figure demonstrates that narrow and wide fully connected neural networks exhibit different training dynamics, which are non-conjugate.  The analysis uses Koopman eigenvalues to compare the dynamics across varying network widths (h=5, 10, 40).  The figure shows training loss curves, example weight trajectories, Koopman eigenvalue plots, and Wasserstein distances between eigenvalues, highlighting the non-conjugate nature of the training dynamics for narrow vs. wide networks.", "section": "4.2 Identifying the effect of width on fully connected neural network training"}, {"figure_path": "bOYVESX7PK/figures/figures_18_1.jpg", "caption": "Figure 3: Narrow and wide fully connected neural networks have non-conjugate training dynamics. (A) Training loss curves for FCNs with hidden layer widths h = 5, 10, and 40. Solid line is mean and shaded area is \u00b1 standard deviation across 25 independently trained networks. (B), (C) Example weight trajectories, across training iterations, for narrow, intermediate, and wide FCNs. (D) Koopman eigenvalues associated with training FCNs of varying width. (E) Same as (D), but zoomed out and with the eigenvalues associated with h = 5 and h = 10 compared to those associated with h = 40. Dashed line in (D) and (E) denotes unit circle. (F) Wasserstein distance between Koopman eigenvalues associated with training FCNs of varying width. Error bars are \u00b1 standard deviation across 25 independently trained FCNs. Kolmogorov-Smirnov (KS) tests were performed to assess statistical significance of distance: * denotes p < 0.01 and *** denotes p < 0.0001.", "description": "This figure shows that narrow and wide fully connected neural networks (FCNs) exhibit non-conjugate training dynamics. The training loss, weight trajectories, Koopman eigenvalues, and Wasserstein distances between Koopman eigenvalues are compared across FCNs with different widths (h = 5, 10, and 40).  The results demonstrate that the training dynamics change fundamentally as the width increases, highlighting a non-conjugate relationship between narrow and wide FCNs.", "section": "4.2 Identifying the effect of width on fully connected neural network training"}, {"figure_path": "bOYVESX7PK/figures/figures_18_2.jpg", "caption": "Figure S4: Conjugate training dynamics across random initializations of FCNs. (A) Example Koopman spectra associated with training FCNs, h = 40, from two different random seeds. (B) Distribution of Wasserstein distance between the Koopman spectra of all possible pairs of 25 independently trained FCNs.", "description": "This figure shows the results of an experiment designed to test whether the training dynamics of fully connected neural networks (FCNs) are conjugate across different random initializations.  Panel A displays example Koopman spectra for two different random initializations of an FCN with 40 hidden units.  Panel B shows a histogram of the Wasserstein distances between all pairs of Koopman spectra from 25 independently trained FCNs. The results suggest that the training dynamics are conjugate across different random initializations, at least for sufficiently wide FCNs.", "section": "C.4 Conjugate training dynamics across random initializations of FCNs"}, {"figure_path": "bOYVESX7PK/figures/figures_19_1.jpg", "caption": "Figure S5: Koopman-based framework enables identification of transitions in dynamics across the training of LeNet and ResNet-20. (A)\u2013(B) Same as Fig. 4A-B, but for dynamics computed over individual epochs.", "description": "This figure shows the log10 Wasserstein distance between Koopman eigenvalues associated with training LeNet and ResNet-20 across individual epochs.  It extends Figure 4 by looking at the dynamics over larger time windows (entire epochs) rather than smaller intervals, offering a coarser-grained perspective on dynamical transitions during training.  The heatmaps visualize the distance between the Koopman eigenvalues of different epochs, revealing similarities and differences in the dynamics over time for both architectures.", "section": "4.3 Identifying dynamical transitions in convolutional neural network training"}, {"figure_path": "bOYVESX7PK/figures/figures_20_1.jpg", "caption": "Figure 5: Transformers that do, and that do not undergo grokking have early training dynamics that are not conjugate. (A) Train and test loss, as a function of training steps, for a Transformer model that undergoes grokking. (B) Same as (A), but for a Transformer whose training is constrained to have a constant weight norm [62]. In this case, no grokking is observed. (C) In the first 100 training steps, little difference is seen between the test loss of Transformers with and without constrained training. Lines are mean and shaded area is \u00b1 standard deviation across 20 independently trained networks. (D) Koopman eigenvalues associated with the dynamics that occur over the first 100 training iterations for Transformers that do, and that not undergo grokking.", "description": "This figure demonstrates that the early training dynamics of Transformers that do and do not undergo grokking are non-conjugate.  Panel A shows the training and testing loss curves for a Transformer that exhibits grokking (sudden improvement in test accuracy after a period of seemingly poor generalization), and panel B shows the same for a Transformer with a constrained weight norm (preventing grokking). Panel C compares the test loss curves for both types of Transformers in the first 100 training steps, showing little difference. Finally, Panel D displays the Koopman eigenvalues for both types of Transformers, which show distinct non-overlapping spectra, supporting the conclusion of non-conjugate dynamics.", "section": "4.4 Identifying non-conjugate training dynamics for Transformers that do and do not grok"}]