[{"type": "text", "text": "Online Bayesian Persuasion Without a Clue ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Francesco Bacchiocchi Politecnico di Milano francesco.bacchiocchi@polimi.it ", "page_idx": 0}, {"type": "text", "text": "Matteo Bollini Politecnico di Milano matteo.bollini@polimi.it ", "page_idx": 0}, {"type": "text", "text": "Matteo Castiglioni Politecnico di Milano matteo.castiglioni@polimi.it ", "page_idx": 0}, {"type": "text", "text": "Alberto Marchesi Politecnico di Milano alberto.marchesi@polimi.it ", "page_idx": 0}, {"type": "text", "text": "Nicola Gatti Politecnico di Milano nicola.gatti@polimi.it ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study online Bayesian persuasion problems in which an informed sender repeatedly faces a receiver with the goal of influencing their behavior through the provision of payoff-relevant information. Previous works assume that the sender has knowledge about either the prior distribution over states of nature or receiver\u2019s utilities, or both. We relax such unrealistic assumptions by considering settings in which the sender does not know anything about the prior and the receiver. We design an algorithm that achieves sublinear\u2014in the number of rounds\u2014regret with respect to an optimal signaling scheme, and we also provide a collection of lower bounds showing that the guarantees of such an algorithm are tight. Our algorithm works by searching a suitable space of signaling schemes in order to learn receiver\u2019s best responses. To do this, we leverage a non-standard representation of signaling schemes that allows to cleverly overcome the challenge of not knowing anything about the prior over states of nature and receiver\u2019s utilities. Finally, our results also allow to derive lower/upper bounds on the sample complexity of learning signaling schemes in a related Bayesian persuasion PAC-learning problem. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Bayesian persuasion has been introduced by Kamenica and Gentzkow [2011] to model how strategically disclosing information to decision makers influences their behavior. Over the last years, it has received a terrific attention in several fields of science, since it is particularly useful for understanding strategic interactions involving individuals with different levels of information, which are ubiquitous in the real world. As a consequence, Bayesian persuasion has been applied in several settings, such as online advertising [Emek et al., 2014, Badanidiyuru et al., 2018, Bacchiocchi et al., 2022, Agrawal et al., 2023], voting [Alonso and C\u00e2mara, 2016, Castiglioni et al., 2020a, Castiglioni and Gatti, 2021], traffic routing [Vasserman et al., 2015, Bhaskar et al., 2016, Castiglioni et al., 2021a], recommendation systems [Cohen and Mansour, 2019, Mansour et al., 2022], security [Rabinovich et al., 2015, Xu et al., 2016], e-commerce [Bro Miltersen and Sheffet, 2012, Castiglioni et al., 2022] medical research [Kolotilin, 2015], and financial regulation [Goldstein and Leitner, 2018]. ", "page_idx": 0}, {"type": "text", "text": "In its simplest form, Bayesian persuasion involves a sender observing some information about the world, called state of nature, and a receiver who has to take an action. Agents\u2019 utilities are misaligned, but they both depend on the state of nature and receiver\u2019s action. Thus, sender\u2019s goal is to devise a mechanism to (partially) disclose information to the receiver, so as to induce them to take a favorable action. This is accomplished by committing upfront to a signaling scheme, encoding a randomized policy that defines how to send informative signals to the receiver based on the observed state. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Classical Bayesian persuasion models (see, e.g., [Dughmi and Xu, 2016, 2017, Xu, 2020]) rely on rather stringent assumptions that considerably limit their applicability in practice. Specifically, they assume that the sender perfectly knows the surrounding environment, including receiver\u2019s utilities and the probability distribution from which the state of nature is drawn, called prior. This has motivated a recent shift of attention towards Bayesian persuasion models that incorporate concepts and ideas from online learning, with the goal of relaxing some of such limiting assumptions. However, existing works only partially fulfill this goal, as they still assume some knowledge of either the prior (see, e.g., [Castiglioni et al., 2020b, 2021b, 2023, Babichenko et al., 2022, Bernasconi et al., 2023]) or receiver\u2019s utilities (see, e.g., [Zu et al., 2021, Bernasconi et al., 2022, Wu et al., 2022]). ", "page_idx": 1}, {"type": "text", "text": "1.1 Original contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We address\u2014for the first time to the best of our knowledge\u2014Bayesian persuasion settings where the sender has no clue about the surrounding environment. In particular, we study the online learning problem faced by a sender who repeatedly interacts with a receiver over multiple rounds, without knowing anything about both the prior distribution over states of nature and receiver\u2019s utilities. At each round, the sender commits to a signaling scheme, and, then, they observe a state realization and send a signal to the receiver based on that. After each round, the sender gets partial feedback, namely, they only observe the best-response action played by the receiver in that round. In such a setting, the goal of the sender is to minimize their regret, which measures how much utility they lose with respect to committing to an optimal (i.e., utility-maximizing) signaling scheme in every round. ", "page_idx": 1}, {"type": "text", "text": "We provide a learning algorithm that achieves regret of the order of $\\widetilde{\\mathcal{O}}(\\sqrt{T})$ , where $T$ is the number of rounds. We also provide lower bounds showing that the regret guarantees attained by our algorithm are tight in $T$ and in the parameters characterizing the Bayesian persuasion instance, i.e., the number of states of nature $d$ and that of receiver\u2019s actions $n$ . Our algorithm implements a sophisticated explorethen-commit scheme, with exploration being performed in a suitable space of signaling schemes so as to learn receiver\u2019s best responses exactly. This is crucial to attain tight regret guarantees, and it is made possible by employing a non-standard representation of signaling schemes, which allows to cleverly overcome the challenging lack of knowledge about both the prior and receiver\u2019s utilities. ", "page_idx": 1}, {"type": "text", "text": "Our results also allow us to derive lower/upper bounds on the sample complexity of learning signaling schemes in a related Bayesian persuasion PAC-learning problem, where the goal is to find, with high probability, an approximately-optimal signaling scheme in the minimum possible number of rounds. ", "page_idx": 1}, {"type": "text", "text": "1.2 Related works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Castiglioni et al. [2020b] were the first to introduce online learning problems in Bayesian persuasion scenarios, with the goal of relaxing sender\u2019s knowledge about receiver\u2019s utilities (see also follow-up works [Castiglioni et al., 2021b, 2023, Bernasconi et al., 2023]). In their setting, sender\u2019s uncertainty is modeled by means of an adversary selecting a receiver\u2019s type at each round, with types encoding information about receiver\u2019s utilities. However, in such a setting, the sender still needs knowledge about the finite set of possible receiver\u2019s types and their associated utilities, as well as about the prior. ", "page_idx": 1}, {"type": "text", "text": "A parallel research line has focused on relaxing sender\u2019s knowledge about the prior. Zu et al. [2021] study online learning in a repeated version of Bayesian persuasion. Differently from this paper, they consider the sender\u2019s learning problem of issuing persuasive action recommendations (corresponding to signals in their case), where persuasiveness is about correctly incentivizing the receiver to actually follow such recommendations. They provide an algorithm that attains sublinear regret while being persuasive at every round with high probability, despite having no knowledge of the prior. Wu et al. [2022], Gan et al. [2023], Bacchiocchi et al. [2024c] achieve similar results for Bayesian persuasion in episodic Markov decision processes, while Bernasconi et al. [2022] in non-Markovian environments. All these works crucially differ from ours, since they strongly rely on the assumption that receiver\u2019s utilities are known to the sender, which is needed in order to meet persuasiveness requirements. As a result, the techniques employed in such works are fundamentally different from ours as well. ", "page_idx": 1}, {"type": "text", "text": "Finally, learning receiver\u2019s best responses exactly (a fundamental component of our algorithm) is related to learning in Stackelberg games [Letchford et al., 2009, Peng et al., 2019, Bacchiocchi et al., 2024a]. For more details on these works and other related works, we refer the reader to Appendix A. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In Section 2.1, we introduce all the needed ingredients of the classical Bayesian persuasion model by Kamenica and Gentzkow [2011], while, in the following Section 2.2, we formally define the Bayesian persuasion setting faced in the rest of the paper and its related online learning problem. ", "page_idx": 2}, {"type": "text", "text": "2.1 Bayesian persuasion ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "A Bayesian persuasion instance is characterized by a finite set $\\Theta:=\\{\\theta_{i}\\}_{i=1}^{d}$ of $d$ states of nature and a finite set $\\mathcal{A}:=\\{a_{i}\\}_{i=1}^{n}$ of $n$ receiver\u2019s actions. Agents\u2019 payoffs are encoded by utility functions $u,u^{\\mathrm{s}}:\\Theta\\times A\\rightarrow[0,1]$ , with $u_{\\theta}(a):=u(\\theta,a)$ , respectively $u_{\\theta}^{\\mathrm{s}}(a):=u^{\\mathrm{s}}(\\theta,a)$ , denoting the payoff of the receiver, respectively the sender, when action $a\\in A$ is played in state $\\theta\\in\\Theta$ . The sender observes a state of nature drawn from a commonly-known prior probability distribution $\\mu\\in\\mathrm{int}(\\Delta_{\\Theta})$ ,1 with $\\mu_{\\theta}\\in(0,1]$ denoting the probability of $\\theta\\in\\Theta$ . To disclose information about the realized state, the sender can publicly commit upfront to a signaling scheme $\\phi:\\Theta\\rightarrow\\Delta_{S}$ , which defines a randomized mapping from states of nature to signals being sent to the receiver, for a finite set $\\boldsymbol{S}$ of signals. For ease of notation, we let $\\phi_{\\theta}:=\\phi(\\theta)$ be the probability distribution over signals prescribed by $\\phi$ when the the sate of nature is $\\theta\\in\\Theta$ , with $\\bar{\\phi}_{\\theta}(s)\\bar{\\leftarrow}\\,[0,1]$ denoting the probability of sending signal $s\\in S$ . ", "page_idx": 2}, {"type": "text", "text": "The sender-receiver interaction goes as follows: (1) the sender commits to a signaling scheme $\\phi$ ; (2) the sender observes a state of nature $\\theta\\sim\\mu$ and sends a signal $s\\sim\\phi_{\\theta}$ to the receiver; (3) the receiver updates their belief over states of nature according to Bayes rule; and (4) the receiver plays a best-response action $a\\in A$ , with sender and receiver getting payoffs $u_{\\theta}(a)$ and $u_{\\theta}^{\\mathrm{s}}(a)$ , respectively. Specifically, an action is a best response for the receiver if it maximizes their expected utility given the belief computed in step (3) of the interaction. Formally, given a signaling scheme $\\phi:\\Theta\\rightarrow\\Delta_{S}$ and a signal $s\\in S$ , we let $\\bar{\\mathcal{A}}^{\\phi}(s)\\subseteq\\mathcal{A}$ be the set of receivers\u2019 best-response actions, where: ", "page_idx": 2}, {"type": "equation", "text": "$$\n{\\cal A}^{\\phi}(s):=\\left\\{a_{i}\\in\\cal A\\mid\\sum_{\\theta\\in\\Theta}\\mu_{\\theta}\\phi_{\\theta}(s)u_{\\theta}(a_{i})\\ge\\sum_{\\theta\\in\\Theta}\\mu_{\\theta}\\phi_{\\theta}(s)u_{\\theta}(a_{j})\\quad\\forall a_{j}\\in\\cal A\\right\\}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "As customary in the literature on Bayesian persuasion (see, e.g., [Dughmi and $\\mathrm{Xu}$ , 2016]), we assume that, when the receiver has multiple best responses available, they break ties in favor of the sender. In particular, we let $a^{\\phi}(s)$ be the best response that is actually played by the receiver when observing signal $s\\in S$ under signaling scheme $\\phi$ , with $\\begin{array}{r}{\\begin{array}{r}{a^{\\phi}(s)\\in\\arg\\operatorname*{max}_{a\\in\\mathcal{A}^{\\phi}(s)}\\sum_{\\theta\\in\\Theta}\\mu_{\\theta}\\phi_{\\theta}(s)u_{\\theta}^{\\mathrm{s}}(a)}\\end{array}}\\end{array}$ . ", "page_idx": 2}, {"type": "text", "text": "The goal of the sender is to commit to an optimal signaling scheme, namely, a $\\phi:\\Theta\\rightarrow\\Delta_{S}$ that maximizes sender\u2019s expected utility, defined as $\\begin{array}{r}{u^{\\mathrm{s}}(\\bar{\\phi^{\\big}}):=\\bar{\\sum}_{s\\in\\mathcal{S}}\\sum_{\\theta\\in\\Theta}\\mu_{\\theta}\\dot{\\phi_{\\theta}}(s)u_{\\theta}^{\\mathrm{s}}(a^{\\phi}(s))}\\end{array}$ . In the following, we let $\\mathrm{OPT}:=\\operatorname*{max}_{\\phi}u^{\\mathrm{s}}(\\phi)$ be the optimal value of sender\u2019s expected utility. Moreover, given an additive error $\\gamma\\in(0,1)$ , we say that a signaling scheme $\\phi$ is $\\gamma$ -optimal if $u^{\\mathrm{s}}(\\phi)\\bar{\\geq}\\,\\mathrm{OPT}-\\gamma$ ", "page_idx": 2}, {"type": "text", "text": "2.2 Learning in Bayesian persuasion ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We study settings in which the sender repeatedly interacts with the receiver over multiple rounds, with each round involving a one-shot Bayesian persuasion interaction (as described in Section 2.1). We assume that the sender has no knowledge about both the prior $\\mu$ and receiver\u2019s utility $u$ , and that the only feedback they get after each round is the best-response action played by the receiver. ", "page_idx": 2}, {"type": "text", "text": "At each round $t\\in[T]$ ,2 the sender commits to a signaling scheme $\\phi_{t}:\\Theta\\to\\Delta_{S}$ and observes a state of nature $\\theta^{t}\\sim\\bar{\\mu}$ . Then, they draw a signal $s^{t}\\sim\\phi_{t,\\theta^{t}}}$ and send it to the receiver, who plays a best-response action $a^{t}:=a^{\\phi_{t}}(s^{t})$ . Finally, the sender gets payoff $u_{t}^{\\mathrm{s}}:=u_{\\theta^{t}}^{\\mathrm{s}}(a^{t})$ and observes a feedback consisting in the action $a^{t}$ played by the receiver. The goal of the sender is to learn how to maximize their expected utility while repeatedly interacting with the receiver. When the sender commits to a sequence $\\{\\phi_{t}\\}_{t\\in[T]}$ of signaling schemes, their performance over the $T$ rounds is measured by means of the following notion of cumulative (Stackelberg) regret: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nR_{T}(\\{\\phi_{t}\\}_{t\\in[T]}):=T\\cdot\\mathrm{OPT}-\\mathbb{E}\\left[\\sum_{t=1}^{T}u^{s}(\\phi_{t})\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the expectation is with respect to the randomness of the algorithm. In the following, for ease of notation, we omit the dependency on $\\{\\phi_{t}\\}_{t\\in[T]}$ from the cumulative regret, by simply writing $R_{T}$ . Then, our goal is to design no-regret learning algorithms for the sender, which prescribe a sequence of signaling schemes $\\phi_{t}$ that results in the regret $R_{T}$ growing sublinearly in $T$ , namely $R_{T}=o(T)$ . ", "page_idx": 3}, {"type": "text", "text": "3 Warm-up: A single signal is all you need ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In order to design our learning algorithm in Section 4, we exploit a non-standard representation of signaling schemes, which we introduce in this section. Adopting such a representation is fundamental to be able to learn receiver\u2019s best responses without any knowledge of both the prior $\\mu$ and receiver\u2019s utility function $u$ . The crucial observation that motivates its adoption is that receiver\u2019s best responses $a^{\\phi}(s)$ only depend on the components of $\\phi$ associated with $s\\in S$ , namely $\\phi_{\\theta}(s)$ for $\\theta\\in\\Theta$ . Thus, in order to learn them, it is sufficient to learn how $a^{\\phi}(s)$ varies as a function of such components. ", "page_idx": 3}, {"type": "text", "text": "The signaling scheme representation introduced in this section revolves around the concept of slice. ", "page_idx": 3}, {"type": "text", "text": "Definition 1 (Slice). Given a signaling scheme $\\phi:\\Theta\\rightarrow\\Delta_{S}$ , the slice of $\\phi$ with respect to signal $s\\in S$ is the $d$ -dimensional vector $x\\in[0,1]^{d}$ with components $x_{\\theta}:=\\phi_{\\theta}(s)$ for $\\theta\\in\\Theta$ . ", "page_idx": 3}, {"type": "text", "text": "In the following, we denote by $\\mathcal{X}^{\\Omega}:=[0,1]^{d}$ the set of all the possible slices of signaling schemes. Moreover, we let $\\chi\\triangle$ be the set of normalized slices, which is simply obtained by restricting slices to lie in the $(d-1)$ -dimensional simplex. Thus, it holds that $\\begin{array}{r}{\\chi\\triangle:=\\left\\{x\\in[0,1]^{d}\\;\\right\\}\\sum_{\\theta\\in\\Theta}x_{\\theta}^{\\widecheck{\\mathbf{\\alpha}}}=1\\big\\}}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "Next, we show that receiver\u2019s actions induce particular coverings of the sets $\\mathcal{X}^{\\boxminus}$ and $\\chi\\triangle$ , which also depend on both the prior $\\mu$ and receiver\u2019s utility $u$ . First, we introduce $\\mathcal{H}_{i j}\\subseteq\\mathbb{R}^{d}$ to denote the halfspace of slices under which action $a_{i}\\ \\in\\ A$ is (weakly) better than action $a_{j}\\in A$ for the receiver. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{H}_{i j}:=\\left\\{x\\in\\mathbb{R}^{d}\\mid\\sum_{\\theta\\in\\Theta}x_{\\theta}\\mu_{\\theta}\\big(u_{\\theta}(a_{i})-u_{\\theta}(a_{j})\\big)\\geq0\\right\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "image", "img_path": "XNpVZ8E1tY/tmp/88f2ef9cbc3153c6db9a6bce366a593585efb273d3f03e094576aa80681b968b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Moreover, we denote by $H_{i j}:=\\partial\\mathcal{H}_{i j}$ the hyperplane constituting the boundary of the halfspace $\\mathcal{H}_{i j}$ , which we call the separating hyperplane between actions $a_{i}$ and $a_{j}$ .3 Then, for every $a_{i}\\in A$ , we introduce the polytopes $\\mathcal{X}^{\\Omega}(a_{i})\\subseteq\\mathcal{X}^{\\Omega}$ and $\\mathcal{X}^{\\triangle}(a_{i})\\subseteq\\mathcal{X}^{\\triangle}$ : ", "page_idx": 3}, {"type": "text", "text": "Figure 1: Representation of sets $\\mathcal{X}^{\\boxminus}(a_{i})$ and $\\mathcal{X}^{\\triangle}(a_{i})$ for an instance with $d=2$ states of nature and $n=3$ receivers\u2019 actions. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{X}^{\\Omega}(a_{i}):=\\mathcal{X}^{\\Omega}\\cap\\left(\\bigcap_{a_{j}\\in\\mathcal{A}:a_{i}\\neq a_{j}}\\mathcal{H}_{i j}\\right)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$$\n\\chi^{\\triangle}(a_{i}):=\\chi^{\\triangle}\\cap\\left(\\bigcap_{a_{j}\\in\\mathcal{A}:a_{i}\\neq a_{j}}\\mathcal{H}_{i j}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Clearly, the sets $\\mathcal{X}^{\\boxminus}(a_{i})$ , respectively $\\mathcal{X}^{\\triangle}(a_{i})$ , define a cover of $\\mathcal{X}^{\\boxminus}$ , respectively $\\chi\\triangle$ .4 Intuitively, the set $\\mathcal{X}^{\\boxminus}(a_{i})$ encompasses all the slices under which action $a_{i}$ is a best response for the receiver. The set $\\mathcal{X}^{\\triangle}(a_{i})$ has the same interpretation, but for normalized slices. Specifically, if $x\\in\\mathcal{X}^{\\square}(a_{i})$ is a slice of $\\phi$ with respect to $s\\in S$ , then $a_{i}\\in\\mathcal{A}^{\\phi}(s)$ . Notice that a slice $x\\in\\mathcal{X}^{\\square}$ may belong to more than one polytope $\\mathcal{X}^{\\boxminus}(a_{i})$ , when it is the case that $|{\\mathcal{A}}^{\\phi}(s)|>1$ for signaling schemes $\\phi$ having $x$ as slice with respect to $s\\in S$ .5 In order to denote the best-response action actually played by the receiver under a slice $x\\in\\mathcal{X}^{\\square}$ , we introduce the symbol $a(x)$ , where $a(x):=a^{\\phi}(s)$ for any $\\phi$ having $x$ as slice with respect to $s\\in S$ . Figure 1 depicts an example of the polytopes $\\mathcal{X}^{\\boxminus}(a_{i})$ and $\\mathcal{X}^{\\triangle}(a_{i})$ in order to help the reader to grasp more intuition about them. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "A crucial fact exploited by the learning algorithm developed in Section 4 is that knowing the separating hyperplanes $H_{i j}$ defining the polytopes $\\mathcal{X}^{\\triangle}(a_{i})$ of normalized slices is sufficient to determine an optimal signaling scheme. Indeed, the polytopes $\\mathcal{X}^{\\boxminus}(a_{i})$ of unnormalized slices can be easily reconstructed by simply removing the normalization constraint $\\textstyle\\sum_{\\theta\\in\\Theta}x_{\\theta}=1$ from $\\mathcal{X}^{\\triangle}(a_{i})$ . Furthermore, as we show in Section 4 (see the proof of Lemma 4 in pa rticular), there alway exits an optimal signaling scheme using at most one slice $x^{a}\\in\\mathcal{X}^{\\square}(a)$ for each receiver\u2019s action $a\\in A$ . ", "page_idx": 4}, {"type": "text", "text": "We conclude the section with some remarks that help to better clarify why we need to work with signaling scheme slices in order to design our learning algorithm in Section 4. ", "page_idx": 4}, {"type": "text", "text": "Why we need slices for learning The coefficients of separating hyperplanes $H_{i j}$ are products between prior probabilities $\\mu_{\\theta}$ and receiver\u2019s utility differences $u_{\\theta}(a_{i})-u_{\\theta}(a_{j})$ . In order to design a no-regret learning algorithm, it is fundamental that such coefficients are learned exactly, since even an arbitrarily small approximation error may result in \u201cmissing\u201d some receiver\u2019s best responses, and this may potentially lead to a large loss in sender\u2019s expected utility (and, in its turn, to large regret). As a result, any na\u00efve approach that learns the prior and receiver\u2019s payoffs separately is deemed to fail, as it would inevitably result in approximate separating hyperplanes being learned. Operating in the space of signaling scheme slices crucially allows us to learn separating hyperplanes exactly. As we show in Section 4, it makes it possible to directly learn the coefficients of separating hyperplanes without splitting them into products of prior probabilities and receiver\u2019s utility differences. ", "page_idx": 4}, {"type": "text", "text": "Why we need normalized slices One may wonder why we cannot work with (unnormalized) slices in $\\mathcal{X}^{\\boxminus}$ , rather than with normalized ones in $\\chi\\triangle$ . Indeed, as we show in Section 4, our procedure to learn separating hyperplanes crucially relies on the fact that we can restrict the search space to a suitable subset of the $(d-1)$ -dimensional simplex. This makes it possible to avoid always employing a number of rounds exponential in $d$ , which would lead to non-tight regret guarantees. ", "page_idx": 4}, {"type": "text", "text": "Why not working with posteriors In Bayesian persuasion, it is oftentimes useful to work in the space of posterior distributions induced by sender\u2019s signals (see, e.g., [Castiglioni et al., 2020b]). These are the beliefs computed by the receiver according to Bayes rule at step (3) of the interaction. Notice that posteriors do not only depend on the signaling scheme $\\phi$ and the sent signal $s\\in S$ , but also on the prior distribution $\\mu$ . Indeed, the same signaling scheme may induce different posteriors for different prior distributions. Thus, since in our setting the sender has no knowledge of $\\mu$ , we cannot employ posteriors. Looking at signaling scheme slices crucially allows us to overcome the lack of knowledge of the prior. Indeed, one way of thinking of them is as \u201cprior-free\u201d posterior distributions. ", "page_idx": 4}, {"type": "text", "text": "4 Learning to persuade without a clue ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we design our no-regret algorithm (Algorithm 1). We adopt a sophisticated explorethen-commit approach that exploits the signaling scheme representation based on slices introduced in Section 3. Specifically, our algorithm works by first exploring the space $\\mathcal{X}:=\\mathcal{X}^{\\triangle}$ of normalized slices in order to learn satisfactory \u201capproximations\u201d of the polytopes $\\mathcal{X}(a_{i}):=\\mathcal{X}^{\\triangle}(a_{i})$ .6 Then, it exploits them in order to compute suitable approximately-optimal signaling schemes to be employed in the remaining rounds. Effectively implementing this approach raises considerable challenges. ", "page_idx": 4}, {"type": "text", "text": "The first challenge is that the algorithm cannot directly \u201cquery\u201d a slice $x\\in\\mathscr{X}$ to know action $a(x)$ , as it can only commit to fully-specified signaling schemes. Indeed, even if the algorithm commits to a signaling scheme including the selected slice $x\\in\\mathscr{X}$ , the probability that the signal associated with $x$ is sent depends on the (unknown) prior, as it is equal to $\\sum_{\\theta\\in\\Theta}\\mu_{\\theta}\\dot{x_{\\theta}}$ . This probability can be arbitrarily small. Thus, in order to observe $a(x)$ , the algorithm may need to commit to the signaling scheme for an unreasonably large number of rounds. To circumvent this issue, we show that it is possible to focus on a subset $\\mathcal{X}_{\\epsilon}\\subseteq\\mathcal{X}$ of normalized slices \u201cinducible\u201d with at least a suitably-defined probability $\\epsilon\\in(0,1)$ . Such a set $\\textstyle{\\mathcal{X}}_{\\epsilon}$ is built by the algorithm in its first phase. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "The second challenge that we face is learning the polytopes $\\mathcal{X}_{\\epsilon}(a_{i}):=\\mathcal{X}(a_{i})\\cap\\mathcal{X}_{\\epsilon}$ . This is done by means of a technically-involved procedure that learns the separating hyperplanes $H_{i j}$ needed to identify them. This procedure is an adaptation to Bayesian persuasion settings of an algorithm recently introduced for a similar problem in Stackelberg games [Bacchiocchi et al., 2024a]. ", "page_idx": 5}, {"type": "text", "text": "Finally, the third challenge is how to use the polytopes $\\mathcal{X}_{\\epsilon}(a_{i})$ to compute suitable approximatelyoptimal signaling schemes to commit to after exploration. We show that this can be done by solving an LP, which, provided that the set $\\textstyle{\\mathcal{X}}_{\\epsilon}$ is carefully constructed, gives signaling schemes with sender\u2019s expected utility sufficiently close to that of an optimal signaling scheme. ", "page_idx": 5}, {"type": "text", "text": "The pseudocode of our no-regret learning algorithm is provided in Algorithm 1. In the pseudocode, we assume that all the sub-procedures have access to the current round counter $t$ , all the observed states of nature $\\theta^{t}$ , and all the feedbacks $a^{t}$ , $u_{t}^{\\mathrm{s}}$ received by the sender.7 Moreover, in Algorithm 1 and its sub-procedures, we use $\\widehat{\\mu}_{t}\\,\\in\\,\\Delta_{\\Theta}$ to denote the prior estimate at any round $t\\,>\\,1$ , which is a vector with components defined as $\\widehat{\\mu}_{t,\\theta}:={N}_{t,\\theta}/(t-1)$ for $\\theta\\in\\Theta$ , where $N_{t,\\theta}\\,\\in\\,\\mathbb{N}$ denotes the number of times that state of nature $\\theta$ is observed up to round $t$ (excluded). Algorithm 1 can be conceptually divided into three phases. In phase 1, the algorithm employs the Build-Search-Space procedure (Algorithm 2) to build a suitable subset $\\mathcal{X}_{\\epsilon}\\subseteq\\mathcal{X}$ of \u201cinducible\u201d normalized slices. Then, ", "page_idx": 5}, {"type": "table", "img_path": "XNpVZ8E1tY/tmp/485deddc5299407a4043be8de19fe078f10741d803a5e94fa3a2d26a9f72c5d1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "in phase 2, the algorithm employs the Find-Polytopes procedure (see Algorithm 6 in Appendix E) to find a collection of polytopes $\\mathcal{R}:=\\{\\mathcal{R}_{\\epsilon}(a)\\}_{a\\in\\mathcal{A}}$ , where each $\\mathcal{R}_{\\epsilon}(a)$ is either $\\mathcal{X}_{\\epsilon}(a)$ or a suitable subset of $\\mathcal{X}_{\\epsilon}(a)$ that is sufficient for achieving the desired goals (see Section 4.2). Finally, phase $^3$ uses the remaining rounds to exploit the knowledge acquired in the preceding two phases. Specifically, at each $t$ , this phase employs the Compute-Signaling procedure (Algorithm 3) to compute an approximately-optimal signaling scheme, by using $\\mathcal{R}_{\\epsilon}$ , the set $\\textstyle{\\mathcal{X}}_{\\epsilon}$ , and the current prior estimate ${\\widehat{\\mu}}_{t}$ . ", "page_idx": 5}, {"type": "text", "text": "In the rest of this section, we describe in detail the three phases of Algorithm 1, bounding the regret attained by each of them. This allows us to prove the following main result about Algorithm 1.8 ", "page_idx": 5}, {"type": "text", "text": "Theorem 1. The regret attained by Algorithm 1 is $R_{T}\\leq\\tilde{\\mathcal{O}}\\big(\\binom{d+n}{d}n^{3/2}d^{3}\\sqrt{B T}\\big).$ ", "page_idx": 5}, {"type": "text", "text": "We observe that the regret bound in Theorem 1 has an exponential dependence on the number of states of nature $d$ and the number of receiver\u2019s actions $n$ , due to the binomi\u221aal coefficient. Indeed, w\u221ahen $d$ , respectively $n$ , is constant, the regret bound is of the order of $\\widetilde{\\mathcal{O}}(n^{d}\\sqrt{T})$ , respectively $\\widetilde{\\mathcal{O}}(d^{n}\\sqrt{T})$ . Such a dependence is tight, as shown by the lower bounds that we provide in Section 5. ", "page_idx": 5}, {"type": "text", "text": "4.1 Phase 1: Build-Search-Space ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Given an $\\epsilon\\,\\in\\,(0,{1}/{6d})$ and a number of rounds $T_{1}$ , the Build-Search-Space procedure (Algorithm 2) computes a subset $\\mathcal{X}_{\\epsilon}\\subseteq\\mathcal{X}$ of normalized slices satisfying two crucial properties needed by the learning algorithm to attain the desired guarantees. Specifically, the first property is that any slice $x\\in\\mathcal{X}_{\\epsilon}$ can be \u201cinduced\u201d with sufficiently high probability by a signaling scheme, while the second one is that, if $x\\notin\\mathcal{X}_{\\epsilon}$ , then signaling schemes \u201cinduce\u201d such a slice with sufficiently small probability. Intuitively, the first property ensures that it is possible to associate any $x\\in\\mathcal{X}_{\\epsilon}$ with the action $a(x)$ in a number of rounds of the order of $^1\\!/\\!\\epsilon$ , while the second property is needed to bound the loss in sender\u2019s expected utility due to only considering signaling schemes with slices in $\\textstyle{\\mathcal{X}}_{\\epsilon}$ . ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Algorithm 2 works by simply observing realized states of nature $\\theta^{t}$ for $T_{1}$ rounds, while committing to any signaling scheme meanwhile. This allows the algorithm to build a sufficiently accurate estimate $\\widehat{\\mu}$ of the true prior $\\mu$ . Then, the algorithm uses such an estimate to build the set $\\textstyle{\\mathcal{X}}_{\\epsilon}$ . Specifically, it constructs $\\textstyle{\\mathcal{X}}_{\\epsilon}$ as the set containing all the normalized slices that are \u201cinducible\u201d with probability at least $2\\epsilon$ under the estimated prior $\\widehat{\\mu}$ , after flitering out all the states of nature whose estimated probability is not above the $2\\epsilon$ threshold (see Lines 8\u201310 in Algorithm 2). ", "page_idx": 6}, {"type": "text", "text": "The following lemma formally establishes the two crucial properties that are guaranteed by Algorithm 2, as informally described above. ", "page_idx": 6}, {"type": "table", "img_path": "XNpVZ8E1tY/tmp/7a8c2fcadce3ff74ce9e2fe5c2ce89ef646d8a9b0f967148af94abfc0eb770f0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Lemma 1. Given $\\begin{array}{r}{T_{1}\\;:=\\;\\lceil\\frac{12}{\\epsilon}\\log\\left(2d/\\delta\\right)\\rceil}\\end{array}$ and $\\epsilon\\;\\in\\;(0,{1}/{6d})$ , Algorithm 2 employs $T_{1}$ rounds and terminates with a set $\\mathcal{X}_{\\epsilon}\\subseteq\\mathcal{X}$ such that, with probability at least $1-\\delta$ : ( $\\begin{array}{r}{i)\\sum_{\\theta\\in\\Theta}\\mu_{\\theta}x_{\\theta}\\geq\\epsilon}\\end{array}$ for every slice $x\\in\\mathcal{X}_{\\epsilon}$ and (ii) $\\textstyle\\sum_{\\theta\\in\\Theta}\\mu_{\\theta}x_{\\theta}\\leq10\\epsilon$ for every slice $x\\in\\mathcal{X}\\setminus\\mathcal{X}_{\\epsilon}$ . ", "page_idx": 6}, {"type": "text", "text": "To prove Lemma 1, we employ the multiplicative version of Chernoff bound, so as to show that it is possible to distinguish whether prior probabilities $\\mu_{\\theta}$ are above or below suitable thresholds in a number of rounds of the order of $^1\\!/\\!\\epsilon$ . Specifically, we show that, after $T_{1}$ rounds and with probability at least $1-\\delta$ , the set $\\widetilde{\\Theta}$ in the definition of $\\textstyle{\\mathcal{X}}_{\\epsilon}$ does not contain states $\\theta\\in\\Theta$ with $\\mu_{\\theta}\\leq\\epsilon$ , while it contains all the states with a sufficiently large $\\mu_{\\theta}$ . This immediately proves properties (i) and (ii) in Lemma 1. Notice that using a multiplicative Chernoff bound is a necessary technicality, since standard concentration inequalities would result in a number of needed rounds of the order of $^1\\!/\\!\\epsilon^{2}$ , leading to a suboptimal regret bound in the number of rounds $T$ . ", "page_idx": 6}, {"type": "text", "text": "For ease of presentation, we introduce the following clean event for phase 1 of Algorithm 1. This encompasses all the situations in which Algorithm 2 outputs a set $\\textstyle{\\mathcal{X}}_{\\epsilon}$ with the desired properties. ", "page_idx": 6}, {"type": "text", "text": "Definition 2 (Phase 1 clean event). ${\\mathcal{E}}_{1}$ is the event in which $\\textstyle{\\mathcal{X}}_{\\epsilon}$ meets properties (i)\u2013(ii) in Lemma 1. ", "page_idx": 6}, {"type": "text", "text": "4.2 Phase 2: Find-Polytopes ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Given a set $\\mathcal{X}_{\\epsilon}\\subseteq\\mathcal{X}$ computed by the Build-Search-Space procedure and $\\zeta\\in(0,1)$ as inputs, the Find-Polytopes procedure (Algorithm 6 in Appendix E) computes a collection $\\mathcal{R}_{\\epsilon}:=\\{\\mathcal{R}_{\\epsilon}(a)\\}_{a\\in\\mathcal{A}}$ of polytopes enjoying suitable properties sufficient to achieve the desired goals. ", "page_idx": 6}, {"type": "text", "text": "Ideally, we would like $\\mathcal{R}_{\\epsilon}(a)=\\mathcal{X}_{\\epsilon}(a)$ for every $a\\in A$ . However, it is not possible to completely identify the polytopes $\\mathcal{X}_{\\epsilon}(a)$ with $\\operatorname{vol}(\\dot{\\mathcal X}_{\\epsilon}(a))=\\dot{0}$ . Indeed, if $\\operatorname{vol}(\\mathcal{X}_{\\epsilon}(a_{i}))=0$ , then $\\mathcal{X}_{\\epsilon}(a_{i})\\subseteq\\mathcal{X}_{\\epsilon}(a_{j})$ for some other polytope $\\mathcal{X}_{\\epsilon}(a_{j})$ with positive volume. Thus, due to receiver\u2019s tie breaking, it could be impossible to identify the whole polytope $\\mathcal{X}_{\\epsilon}(a_{i})$ . As a result, the Find-Polytopes procedure can output polytopes $\\mathcal{R}_{\\epsilon}(a)=\\mathcal{X}_{\\epsilon}(a)$ only if $\\operatorname{vol}(\\dot{\\mathcal X}_{\\epsilon}(\\dot{a}))>0$ . However, we show that, if $\\mathrm{vol}(\\mathcal{X}_{\\epsilon}(a))=0$ , it is sufficient to guarantee that the polytope $\\mathcal{R}_{\\epsilon}(a)$ contains a suitable subset $\\mathcal{V}_{\\epsilon}(a)$ of the vertices of $\\mathcal{X}_{\\epsilon}(a)$ ; specifically, those in which the best response actually played by the receiver is $a$ . For every $a\\in A$ , such a set is formally defined as $\\mathcal{V}_{\\epsilon}(a\\bar{)}:=\\{x\\in V(\\bar{\\mathcal{X}}_{\\epsilon}(\\bar{a}))\\mid\\dot{a}(x)=a\\}$ . Thus, we design Find-Polytopes so that it returns a collection $\\mathcal{R}_{\\epsilon}:=\\{\\mathcal{R}_{\\epsilon}(a)\\}_{a\\in\\mathcal{A}}$ of polytopes such that: ", "page_idx": 6}, {"type": "text", "text": "As a result each polytope $\\mathcal{R}_{\\epsilon}(a)$ can be either $\\mathcal{X}_{\\epsilon}(a)$ or a face of $\\mathcal{X}_{\\epsilon}(a)$ , or it can be empty, depending on receiver\u2019s tie breaking. In all these cases, it is always guaranteed that $\\mathcal{V}_{\\epsilon}(a)\\subseteq\\mathcal{R}_{\\epsilon}\\bar{(a)}$ . ", "page_idx": 6}, {"type": "text", "text": "To achieve its goal, the Find-Polytopes procedure works by searching over the space of normalized slices $\\textstyle{\\mathcal{X}}_{\\epsilon}$ , so as to learn exactly all the separating hyperplanes $H_{i j}$ characterizing the needed vertices. ", "page_idx": 6}, {"type": "text", "text": "The algorithm does so by using and extending tools that have been developed for a related learning problem in Stackelberg games (see [Bacchiocchi et al., 2024a]). Notice that our Bayesian persuasion setting has some distinguishing features that do not allow us to use such tools off the shelf. We refer the reader to Appendix E for a complete description of the Find-Polytopes procedure. ", "page_idx": 7}, {"type": "text", "text": "A crucial component of Find-Polytopes is a tool to \u201cquery\u201d a normalized slice $x\\in\\mathcal{X}_{\\epsilon}$ in order to obtain the action $a(x)$ . This is done by using a sub-procedure that we call Action-Oracle (see Algorithm 5 in Appendix E), which works by committing to a signaling scheme including slice $x$ until the signal corresponding to such a slice is actually sent. Under the clean event ${\\mathcal{E}}_{1}$ , the set $\\textstyle{\\mathcal{X}}_{\\epsilon}$ is built in such a way that Action-Oracle returns the desired action $a(x)$ in a number of rounds of the order of $^1\\!/\\!\\epsilon$ , with high probability. This is made formal by the following lemma. ", "page_idx": 7}, {"type": "text", "text": "Lemma 2. Under event ${\\mathcal{E}}^{1}$ , given any $\\rho\\,\\in\\,(0,1)$ and a normalized slice $x\\,\\in\\,{\\mathcal{X}}_{\\epsilon}$ , if the sender commits to a signaling scheme $\\phi:\\Theta\\rightarrow S:=\\{s_{1},s_{2}\\}$ such that $\\phi_{\\theta}(s_{1})=x_{\\theta}$ for all $\\theta\\in\\Theta$ during $\\begin{array}{r}{q:=\\lceil\\frac{1}{\\epsilon}\\log(^{1/\\rho})\\rceil}\\end{array}$ rounds, then, with probability at least $1-\\rho_{i}$ , signal $s_{1}$ is sent at least once. ", "page_idx": 7}, {"type": "text", "text": "Notice that the signaling scheme used to \u201cquery\u201d an $x\\,\\in\\,{\\mathcal{X}}_{\\epsilon}$ only employs two signals: one is associated with slice $x$ , while the other crafted so as to make the signaling scheme well defined. ", "page_idx": 7}, {"type": "text", "text": "The following lemma provides the guarantees of Algorithm 6 in Appendix E. ", "page_idx": 7}, {"type": "text", "text": "Lemma 3. Given inputs $\\mathcal{X}_{\\epsilon}\\subseteq\\mathcal{X}$ and $\\zeta\\in(0,1)$ for Algorithm $^{6}$ , let $L:=B+B_{\\epsilon}+B_{\\hat{\\mu}}$ , where $B$ , $B_{\\epsilon}$ , and $B_{\\widehat{\\mu}}$ denote the bit-complexity of numbers $\\mu_{\\theta}u_{\\theta}(a_{i})$ , \u03f5, and ${\\widehat{\\mu}},$ , respectively. Then, under event ${\\mathcal{E}}_{1}$ and with  at probability at least $1-\\zeta$ , Algorithm 6 outputs a coll e ction $\\mathcal{R}_{\\epsilon}:=\\{\\mathcal{R}_{\\epsilon}(a)\\}_{a\\in\\mathcal{A}}$ , where $\\mathcal{R}_{\\epsilon}(a)$ is a (possibly improper) face of $\\mathcal{X}_{\\epsilon}(a)$ such that $\\bar{\\mathcal{V}}_{\\epsilon}(a)\\subseteq\\mathcal{X}_{\\epsilon}(a),$ , in a number of rounds $T_{2}$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\nT_{2}\\leq\\tilde{\\mathcal{O}}\\left(\\frac{n^{2}}{\\epsilon}\\log^{2}\\left(\\frac{1}{\\zeta}\\right)\\left(d^{7}L+\\binom{d+n}{d}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "For ease of presentation, we introduce the clean event for phase 2 of Algorithm 1, defined as follows: ", "page_idx": 7}, {"type": "text", "text": "Definition 3 (Phase 2 clean event). $\\mathcal{E}_{2}$ is the event in which $\\mathcal{V}_{\\epsilon}(a)\\subseteq\\mathcal{R}_{\\epsilon}(a)$ for every $a\\in{\\mathcal{A}}$ . ", "page_idx": 7}, {"type": "text", "text": "4.3 Phase 3: Compute-Signaling ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Given the collection of polytopes $\\mathcal{R}_{\\epsilon}:=\\{\\mathcal{R}_{\\epsilon}(a)\\}_{a\\in\\mathcal{A}}$ returned by the Find-Polytopes procedure and an estimated prior $\\widehat{\\mu}_{t}\\,\\in\\,\\Delta_{\\Theta}$ , the Compute-Signaling procedure (Algorithm 3) outputs an approximately-optimal signaling scheme by solving an LP (Program (2) in Algorithm 3). ", "page_idx": 7}, {"type": "text", "text": "Program (2) maximizes an approximated version of sender\u2019s expected utility over a suitable space of (partially-specified) signaling schemes. These are defined by tuples of slices $(x^{a})_{a\\in A}$ containing an (unnormalized) slice $x^{a}\\in\\mathcal{R}_{\\epsilon}^{\\Pi}(a)$ for every receiver\u2019s action $a\\in{\\mathcal{A}}$ . The objective function being maximized by Program (2) accounts for the sender\u2019s approximate utility under each of the slices $x^{a}$ , where the approximation comes from the estimated prior ${\\widehat{\\mu}}_{t}$ . The intuitive idea exploited by the LP formulation is that, under slice $x^{a}$ , the receiver always plays the same action $a$ as best response, since $a(x^{a})=a$ holds by the way in which $\\bar{\\mathcal{R}}_{\\epsilon}(a)$ is constructed by Find-Polytopes. In particular, each polytope $\\dot{\\mathcal{R}}_{\\epsilon}^{\\Pi}(a)$ is built so as to include all the unnormalized slices corresponding to the normalized slices in the set $\\mathcal{R}_{\\epsilon}(a)$ . Formally, for every receiver\u2019s action $a\\in A$ , it holds: ", "page_idx": 7}, {"type": "table", "img_path": "XNpVZ8E1tY/tmp/b39bdc533619a06be50afc494c8b9f9c3e7ec22cd2679ae97743795481b9ed29.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\epsilon}^{\\Omega}(a):=\\{x\\in\\mathcal{X}^{\\Omega}\\mid x=\\alpha x^{\\prime}\\wedge x^{\\prime}\\in\\mathcal{R}_{\\epsilon}(a)\\wedge\\alpha\\in[0,1]\\}\\cup\\{\\mathbf{0}\\},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where 0 denotes the vector of all zeros in $\\mathbb{R}^{d}$ . We observe that, since the polytopes $\\mathcal{R}_{\\epsilon}(a)$ are constructed as the intersection of some halfspaces $\\mathcal{H}_{i j}$ and $\\textstyle{\\mathcal{X}}_{\\epsilon}$ , it is possible to easily build polytopes $\\mathcal{R}_{\\epsilon}^{\\square}(a_{i})$ by simply removing the normalization constraint $\\textstyle\\sum_{\\theta\\in\\Theta}x_{\\theta}=1$ . Notice that, if $\\mathcal{R}_{\\epsilon}(a)=\\mathcal{D}$ , then $\\mathcal{R}_{\\epsilon}^{\\perp}(a)=\\{\\mathbf{0}\\}$ , which implies that action $a$ is never induced as a best response, since $x^{a}=\\mathbf{0}$ . ", "page_idx": 7}, {"type": "text", "text": "After solving Program (2) for an optimal solution $x^{\\star}:=(x^{\\star,a})_{a\\in\\mathcal{A}}$ , Algorithm 3 employs such a solution to build a signaling scheme $\\phi$ . This employs a signal $s^{a}$ for every action $a\\in A$ , plus an additional signal $s^{\\star}$ , namely $S:=\\{s^{\\star}\\}\\cup\\{s^{a}\\mid a\\in\\mathcal{A}\\}$ . Specifically, the slice of $\\phi$ with respect to $s^{a}$ is set to be equal to $x^{a}$ , while its slice with respect to $s^{\\star}$ is set so as to render $\\phi$ a valid signaling scheme (i.e., probabilities over signal sum to one for every $\\theta\\in\\Theta$ ). Notice that this is always possible thanks to the additional constraints $\\textstyle\\sum_{a\\in{\\mathcal{A}}}x_{\\theta}^{a}\\leq1$ in Program (2). Moreover, such a slice may belong to $\\mathcal{X}^{\\sqcap}\\setminus\\mathcal{X}_{\\epsilon}^{\\sqcap}$ . Indeed, in instances where there are some $\\mathcal{X}^{\\boxminus}(a)$ falling completely outside $\\mathcal{X}_{\\epsilon}^{\\Pi}$ , this is fundamental to build a valid signaling scheme. Intuitively, one may think of $s^{\\star}$ as incorporating all the \u201cmissing\u201d signals in $\\phi$ , namely those corresponding to actions $a\\in{\\mathcal{A}}$ with $\\mathcal{X}^{\\boxminus}(a)$ outside $\\mathcal{X}_{\\epsilon}^{\\square}$ . ", "page_idx": 8}, {"type": "text", "text": "The following lemma formally states the theoretical guarantees provided by Algorithm 3. ", "page_idx": 8}, {"type": "text", "text": "Lemma 4. Given inputs $\\mathcal{R}_{\\epsilon}:=\\{\\mathcal{R}_{\\epsilon}(a)\\}_{a\\in\\mathcal{A}}$ , $\\textstyle{\\mathcal{X}}_{\\epsilon}$ , and $\\widehat{\\mu}_{t}\\in\\Delta_{\\Theta}$ for Algorithm 3, under events ${\\mathcal{E}}_{1}$ and $\\mathcal{E}_{2}$ , the signaling scheme $\\phi$ output by the algorithm is $O(\\epsilon n d\\!+\\!\\nu)$ -optimal for $\\begin{array}{r}{\\nu\\leq\\left|\\sum_{\\theta\\in\\Theta}\\widehat{\\mu}_{t,\\theta}-\\mu_{\\theta}\\right|}\\end{array}$ . ", "page_idx": 8}, {"type": "text", "text": "In order to provide some intuition on how Lemma 4 is proved, let us assume that each polytope $\\mathcal{X}_{\\epsilon}(a)$ is either empty or has volume larger than zero, implying that $\\mathcal{R}_{\\epsilon}(a)=\\mathcal{X}_{\\epsilon}(a)$ . In Appendix D, we provide the complete formal proof of Lemma 4, working even with zero-measure non-empty polytopes. The first observation the we need is that sender\u2019s expected utility under a signaling scheme $\\phi$ can be decomposed across its slices, with each slice $x$ providing a utility of $\\begin{array}{r}{\\sum_{\\theta\\in\\Theta}\\bar{\\mu}_{\\theta}x_{\\theta}\\bar{u}_{\\theta}^{\\mathrm{s}}(a(x))}\\end{array}$ The second crucial observation is that there always exists an optimal signaling s cheme $\\phi^{\\star}$ that is direct and persuasive, which means that $\\phi^{\\star}$ employs only one slice $x^{a}$ for each action $a\\in{\\mathcal{A}}$ , with $a$ being a best response for the receiver under $x^{a}$ . It is possible to show that the slices $x^{a}$ that also belong to $\\textstyle{\\mathcal{X}}_{\\epsilon}$ can be used to construct a feasible solution to Program 2, since $x^{a}\\in\\mathcal{R}_{\\epsilon}^{\\square}(a)$ by definition. Thus, restricted to those slices, the signaling scheme $\\phi$ computed by Algorithm 3 achieves an approximate sender\u2019s expected utility that is greater than or equal to the one achieved by $\\phi^{\\star}$ . Moreover, the loss due to dropping the slices that are not in $\\textstyle{\\mathcal{X}}_{\\epsilon}$ can be bounded thanks to point (ii) in Lemma 1. Finally, it remains to account for the approximation due to using ${\\widehat{\\mu}}_{t}$ instead of the true prior in the objective of Program 2. All the observations above allow to bound sender\u2019s expected utility loss as in Lemma 4. ", "page_idx": 8}, {"type": "text", "text": "5 Lower bounds for online Bayesian persuasion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we present two lower bounds on the regret attainable in the setting faced by Algorithm 1. The first lower bound shows that an exponential dependence in the number of states of nature $d$ and the number of receiver\u2019s actions $n$ is unavoidable. This shows that one cannot get rid of the binomial coefficient in the regret bound of Algorithm 1 provided in Theorem 1. Formally: ", "page_idx": 8}, {"type": "text", "text": "Theorem 2. For any sender\u2019s algorithm, there exists a Bayesian persuasion instance in which $n=d+2$ and the regret $R_{T}$ suffered by the algorithm is at least $2^{\\Omega(\\mathbf{\\bar{d}})}$ , or, equivalently, $2^{\\Omega(n)}$ . ", "page_idx": 8}, {"type": "text", "text": "Theorem 2 is proved by constructing a collection of Bayesian persuasion instances in which an optimal signaling scheme has to induce the receiver to take an action that is a best response only for a unique posterior belief (among those computable by the receiver at step (3) of the interaction). This posterior belief belongs to a set of possible candidates having size exponential in the number of states of nature $d$ . As a result, in order to learn such a posterior belief, any algorithm has to commit to a number of signaling schemes that is exponential in $d$ (and, given how the instances are built, in $n$ ). ", "page_idx": 8}, {"type": "text", "text": "The second lower bound shows that the regret bound attained by Algorithm 1 is tight in $T$ . ", "page_idx": 8}, {"type": "text", "text": "Theorem 3. For any sender\u2019s algorithm, there ex\u221aists a Bayesian persuasion instance in which the regret $R_{T}$ suffered by the algorithm is at least $\\Omega({\\sqrt{T}})$ . ", "page_idx": 8}, {"type": "text", "text": "To prove Theorem 3, we construct two Bayesian persuasion instances with $\\Theta=\\{\\theta_{1},\\theta_{2}\\}$ such that, in the first instance, $\\mu_{\\theta_{1}}$ is slightly greater than $\\mu_{\\theta_{2}}$ , while the opposite holds in the second instance. Furthermore, the two instances are built so that the sender does not gain any information that helps to distinguish between them by committing to signaling schemes. As a consequence, to make a distinction, the sender can only leverage the information gained by observi\u221ang the states of nature realized at each round, and this clearly results in the regret being at least $\\Omega({\\sqrt{T}})$ . ", "page_idx": 8}, {"type": "text", "text": "6 The sample complexity of Bayesian persuasion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this section, we show how the no-regret learning algorithm developed in Section 4 can be easily adapted to solve a related Bayesian persuasion $P A C$ -learning problem. Specifically, given an (additive) approximation error $\\gamma\\in(0,1)$ and a probability $\\eta\\in(0,1)$ , the goal of such a problem is to learn a $\\gamma$ -optimal signaling scheme with probability at least $1-\\eta$ , by using the minimum possible number of rounds. This can be also referred to as the sample complexity of learning signaling schemes. ", "page_idx": 9}, {"type": "text", "text": "As in the regret-minimization problem addressed in Section 4, we assume that the sender does not know anything about both the prior distribution $\\mu$ and receiver\u2019s utility function $u$ . ", "page_idx": 9}, {"type": "text", "text": "We tackle the Bayesian persuasion PAC-learning problem with a suitable adaptation of Algorithm 1, provided in Algorithm 4. The first two phases of the algorithm follow the line of Algorithm 1, with the Build-Search-Space and Find-Polytopes procedures being called for suitably-defined parameters $\\epsilon,\\delta,T_{1}$ , and $\\zeta$ (taking different values with respect to their counterparts in Algorithm 1). In particular, the value of $\\epsilon$ depends on $\\gamma$ and is carefully computed so as to control the bit-complexity of numbers used in the Find-Polytopes procedure (see Lemma 3), as detailed in Appendix G. Finally, in its third phase, the algorithm calls Compute-Signaling to compute a signaling scheme $\\phi$ that can be proved to $\\gamma$ -optimal with probability at least $1-\\eta$ . ", "page_idx": 9}, {"type": "table", "img_path": "XNpVZ8E1tY/tmp/5ab0cfa1705a945e6034549a5b0eb98ab778c026bf96a38cda6ea359307be750.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "The most relevant difference between Algorithm 4 and Algorithm 1 is the number of rounds used to build the prior estimate defining $\\textstyle{\\mathcal{X}}_{\\epsilon}$ . Specifically, while the latter has to employ $T_{1}$ of the order of $^1\\!/\\!\\epsilon$ and rely on a multiplicative Chernoff bound to get tight regret guarantees, the former has to use $T_{1}$ of the order of $^1\\!/\\!\\epsilon^{2}$ and standard concentration inequalities to get an $O(\\epsilon)$ -optimal solution. Formally: ", "page_idx": 9}, {"type": "text", "text": "Lemma 5. Given $\\begin{array}{r}{T_{1}:=\\left\\lceil\\frac{1}{2\\epsilon^{2}}\\log\\left(2d/\\delta\\right)\\right\\rceil}\\end{array}$ and $\\epsilon\\in(0,1)$ , Algorithm 2 employs $T_{1}$ rounds and outputs $\\mathcal{X}_{\\epsilon}\\subseteq\\mathcal{X}$ such that, with probability at least $1-\\delta$ : (i) $\\textstyle\\sum_{\\theta\\in\\Theta}\\mu_{\\theta}x_{\\theta}\\geq\\epsilon$ for every slice $x\\in\\mathcal{X}_{\\epsilon}$ , (ii) $\\textstyle\\sum_{\\theta\\in\\Theta}\\mu_{\\theta}x_{\\theta}\\leq6\\epsilon$ for every slice $x\\in\\mathcal{X}\\setminus\\mathcal{X}_{\\epsilon}$ , and (iii) $|{\\widehat{\\mu}}_{\\theta}-\\mu_{\\theta}|\\leq\\epsilon$ for every $\\theta\\in\\Theta$ . ", "page_idx": 9}, {"type": "text", "text": "By Lemma 5, it is possible to show that the event ${\\mathcal{E}}^{1}$ holds. Hence, the probability that a signaling scheme including a slice $x\\in\\mathcal{X}_{\\epsilon}$ actually \u201cinduces\u201d such a slice is at least $\\epsilon$ , and, thus, the results concerning the second phase of Algorithm 1 are valid also in this setting. Finally, whenever the events ${\\mathcal{E}}_{1}$ and $\\mathcal{E}_{2}$ hold, we can provide an upper bound on the number of rounds required by Algorithm 4 to compute a $\\gamma.$ -optimal signaling scheme as desired. Formally: ", "page_idx": 9}, {"type": "text", "text": "Theorem 4. Given $\\gamma\\in(0,1)$ and $\\eta\\in(0,1)$ , with probability at least $1-\\eta,$ , Algorithm $^{4}$ outputs a $\\gamma$ -optimal signaling scheme in a number of rounds $T$ such that: ", "page_idx": 9}, {"type": "equation", "text": "$$\nT\\leq\\widetilde{\\mathcal{O}}\\left(\\frac{n^{3}}{\\gamma^{2}}\\log^{2}\\left(\\frac{1}{\\eta}\\right)\\left(d^{8}B+d\\binom{d+n}{d}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "We conclude by providing two negative results showing that the result above is tight. ", "page_idx": 9}, {"type": "text", "text": "Theorem 5. There exist two absolute constants $\\kappa,\\lambda>0$ such that no algorithm is guaranteed to return a $\\kappa$ -optimal signaling scheme with probability of at least $1-\\lambda$ by employing less than $2^{\\Omega(n)}$ and $2^{\\Omega(d)}$ rounds, even when the prior distribution $\\mu$ is known to the sender. ", "page_idx": 9}, {"type": "text", "text": "Theorem 6. Given $\\gamma\\in(0,{^1\\!/8})$ and $\\eta\\in(0,1)$ , no algorithm is guaranteed to return a $\\gamma$ -optimal signaling scheme with probability at least $1-\\eta$ by employing less than $\\begin{array}{r}{\\Omega\\big(\\frac{1}{\\gamma^{2}}\\log({1/\\eta})\\big)}\\end{array}$ rounds. ", "page_idx": 9}, {"type": "text", "text": "In Appendix $\\mathrm{H}$ , we also study the case in which the prior $\\mu$ is known to the sender. In such a case, we show that the sample complexity can be improved by a factor $^1/\\gamma$ , which is tight. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by the Italian MIUR PRIN 2022 Project \u201cTargeted Learning Dynamics: Computing Efficient and Fair Equilibria through No-Regret Algorithms\u201d, by the FAIR (Future ", "page_idx": 9}, {"type": "text", "text": "Artificial Intelligence Research) project, funded by the NextGenerationEU program within the PNRRPE-AI scheme (M4C2, Investment 1.3, Line on Artificial Intelligence), and by the EU Horizon project ELIAS (European Lighthouse of AI for Sustainability, No. 101120237). This work was also partially supported by project SERICS (PE00000014) under the MUR National Recovery and Resilience Plan funded by the European Union - NextGenerationEU. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Shipra Agrawal, Yiding Feng, and Wei Tang. Dynamic pricing and learning with bayesian persuasion. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 59273\u201359285, 2023. ", "page_idx": 10}, {"type": "text", "text": "Ricardo Alonso and Odilon C\u00e2mara. Persuading voters. American Economic Review, 2016.   \nYakov Babichenko, Inbal Talgam-Cohen, Haifeng Xu, and Konstantin Zabarnyi. Regret-minimizing Bayesian persuasion. Games and Economic Behavior, 136:226\u2013248, 2022. ISSN 0899-8256.   \nFrancesco Bacchiocchi, Matteo Castiglioni, Alberto Marchesi, Giulia Romano, and Nicola Gatti. Public signaling in Bayesian ad auctions. In IJCAI, 2022.   \nFrancesco Bacchiocchi, Matteo Bollini, Matteo Castiglioni, Alberto Marchesi, and Nicola Gatti. The sample complexity of Stackelberg games. arXiv preprint arXiv:2405.06977, 2024a.   \nFrancesco Bacchiocchi, Matteo Castiglioni, Alberto Marchesi, and Nicola Gatti. Learning optimal contracts: How to exploit small action spaces. In The Twelfth International Conference on Learning Representations, 2024b.   \nFrancesco Bacchiocchi, Francesco Emanuele Stradi, Matteo Castiglioni, Alberto Marchesi, and Nicola Gatti. Markov persuasion processes: Learning to persuade from scratch. arXiv preprint arXiv:2402.03077, 2024c.   \nAshwinkumar Badanidiyuru, Kshipra Bhawalkar, and Haifeng Xu. Targeting and signaling in ad auctions. In Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms, 2018.   \nYu Bai, Chi Jin, Huan Wang, and Caiming Xiong. Sample-efficient learning of stackelberg equilibria in general-sum games. Advances in Neural Information Processing Systems, 34:25799\u201325811, 2021.   \nMartino Bernasconi, Matteo Castiglioni, Alberto Marchesi, Nicola Gatti, and Francesco Trov\u00f2. Sequential information design: Learning to persuade in the dark. In Advances in Neural Information Processing Systems, volume 35, pages 15917\u201315928, 2022.   \nMartino Bernasconi, Matteo Castiglioni, Andrea Celli, Alberto Marchesi, Francesco Trov\u00f2, and Nicola Gatti. Optimal rates and efficient algorithms for online Bayesian persuasion. In Proceedings of the 40th International Conference on Machine Learning, pages 2164\u20132183, 2023.   \nUmang Bhaskar, Yu Cheng, Young Kun Ko, and Chaitanya Swamy. Hardness results for signaling in bayesian zero-sum and network routing games. In EC, 2016.   \nPeter Bro Miltersen and Or Sheffet. Send mixed signals: earn more, work less. In EC, 2012.   \nModibo K. Camara, Jason D. Hartline, and Aleck Johnsen. Mechanisms for a no-regret agent: Beyond the common prior. In 2020 IEEE 61st Annual Symposium on Foundations of Computer Science (FOCS), pages 259\u2013270, 2020.   \nMatteo Castiglioni and Nicola Gatti. Persuading voters in district-based elections. In AAAI, 2021.   \nMatteo Castiglioni, Andrea Celli, and Nicola Gatti. Persuading voters: It\u2019s easy to whisper, it\u2019s hard to speak loud. In AAAI, 2020a.   \nMatteo Castiglioni, Andrea Celli, Alberto Marchesi, and Nicola Gatti. Online Bayesian persuasion. Advances in Neural Information Processing Systems, 33:16188\u201316198, 2020b.   \nMatteo Castiglioni, Andrea Celli, Alberto Marchesi, and Nicola Gatti. Signaling in Bayesian network congestion games: the subtle power of symmetry. In AAAI, 2021a.   \nMatteo Castiglioni, Alberto Marchesi, Andrea Celli, and Nicola Gatti. Multi-receiver online Bayesian persuasion. In Proceedings of the 38th International Conference on Machine Learning, pages 1314\u20131323, 2021b.   \nMatteo Castiglioni, Giulia Romano, Alberto Marchesi, and Nicola Gatti. Signaling in posted price auctions. In AAAI, 2022.   \nMatteo Castiglioni, Andrea Celli, Alberto Marchesi, and Nicola Gatti. Regret minimization in online Bayesian persuasion: Handling adversarial receiver\u2019s types under full and partial feedback models. Artificial Intelligence, 314:103821, 2023.   \nAlon Cohen, Argyrios Deligkas, and Moran Koren. Learning approximately optimal contracts. In Algorithmic Game Theory: 15th International Symposium, SAGT 2022, page 331\u2013346, 2022.   \nLee Cohen and Yishay Mansour. Optimal algorithm for Bayesian incentive-compatible exploration. In EC, 2019.   \nShaddin Dughmi and Haifeng Xu. Algorithmic Bayesian persuasion. In STOC, 2016.   \nShaddin Dughmi and Haifeng Xu. Algorithmic persuasion with no externalities. In EC, 2017.   \nYuval Emek, Michal Feldman, Iftah Gamzu, Renato PaesLeme, and Moshe Tennenholtz. Signaling schemes for revenue maximization. ACM Transactions on Economics and Computation, 2014.   \nYiding Feng, Wei Tang, and Haifeng Xu. Online bayesian recommendation with no regret. In Proceedings of the 23rd ACM Conference on Economics and Computation, page 818\u2013819, 2022.   \nMichal Fori\u0161ek. Approximating rational numbers by fractions. In Fun with Algorithms, pages 156\u2013165. Springer Berlin Heidelberg, 2007.   \nJiarui Gan, Rupak Majumdar, Debmalya Mandal, and Goran Radanovic. Sequential principalagent problems with communication: Efficient computation and learning. arXiv preprint arXiv:2306.03832, 2023.   \nItay Goldstein and Yaron Leitner. Stress tests and information disclosure. Journal of Economic Theory, 177:34\u201369, 2018.   \nChien-Ju Ho, Aleksandrs Slivkins, and Jennifer Wortman Vaughan. Adaptive contract design for crowdsourcing markets: Bandit algorithms for repeated principal-agent problems. In Proceedings of the Fifteenth ACM Conference on Economics and Computation, page 359\u2013376, 2014.   \nEmir Kamenica and Matthew Gentzkow. Bayesian persuasion. American Economic Review, 101(6): 2590\u20132615, 2011.   \nAnton Kolotilin. Experimental design to persuade. Games and Economic Behavior, 90:215\u2013226, 2015.   \nNiklas Lauffer, Mahsa Ghasemi, Abolfazl Hashemi, Yagiz Savas, and Ufuk Topcu. No-regret learning in dynamic Stackelberg games. arXiv preprint arXiv:2202.04786, 2024.   \nJoshua Letchford, Vincent Conitzer, and Kamesh Munagala. Learning and approximating the optimal strategy to commit to. In Algorithmic Game Theory: Second International Symposium, SAGT 2009, pages 250\u2013262, 2009.   \nYishay Mansour, Alex Slivkins, Vasilis Syrgkanis, and Zhiwei Steven Wu. Bayesian exploration: Incentivizing exploration in Bayesian games. Operations Research, 70(2):1105\u20131127, 2022.   \nBinghui Peng, Weiran Shen, Pingzhong Tang, and Song Zuo. Learning optimal strategies to commit to. In AAAI Conference on Artificial Intelligence, volume 33, pages 2149\u20132156, 2019.   \nZinovi Rabinovich, Albert Xin Jiang, Manish Jain, and Haifeng Xu. Information disclosure as a means to security. In AAMAS, 2015.   \nShoshana Vasserman, Michal Feldman, and Avinatan Hassidim. Implementing the wisdom of waze. In IJCAI, 2015.   \nJibang Wu, Zixuan Zhang, Zhe Feng, Zhaoran Wang, Zhuoran Yang, Michael I. Jordan, and Haifeng Xu. Sequential information design: Markov persuasion process and its efficient reinforcement learning. In EC, 2022.   \nHaifeng Xu. On the tractability of public persuasion with no externalities. In SODA, 2020.   \nHaifeng Xu, Rupert Freeman, Vincent Conitzer, Shaddin Dughmi, and Milind Tambe. Signaling in Bayesian Stackelberg games. In AAMAS, 2016.   \nBanghua Zhu, Stephen Bates, Zhuoran Yang, Yixin Wang, Jiantao Jiao, and Michael I. Jordan. The sample complexity of online contract design. In Proceedings of the 24th ACM Conference on Economics and Computation, page 1188, 2023.   \nYou Zu, Krishnamurthy Iyer, and Haifeng Xu. Learning to persuade on the fly: Robustness against ignorance. In EC, pages 927\u2013928, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The appendixes are organized as follows: ", "page_idx": 13}, {"type": "text", "text": "\u2022 Appendix A provides a discussion of the previous works most related to ours.   \n\u2022 Appendix B presents additional preliminaries.   \n\u2022 Appendix C presents the omitted proofs from Section 4.   \n\u2022 Appendix D presents the proof of Lemma 4 from Section 4.3.   \n\u2022 Appendix E provides a description of the results and the procedures employed in phase 2.   \n\u2022 Appendix F presents the omitted proofs from Section 5.   \n\u2022 Appendix G presents the omitted proofs and some technical details from Section 6.   \n\u2022 Appendix H discusses the PAC-learning problem when the prior is known. ", "page_idx": 13}, {"type": "text", "text": "A Additional Related Works ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Learning in Bayesian persuasion settings In addition to the works presented in Section 1.2, the problem of learning optimal signaling schemes in Bayesian persuasion settings has received growing attention over the last few years. Camara et al. [2020] study an adversarial setting where the receiver does not know the prior, and the receiver\u2019s behavior is aimed at minimizing internal regret. Babichenko et al. [2022] consider an online Bayesian persuasion setting with binary actions when the prior is known, and the receiver\u2019s utility function has some regularities. Feng et al. [2022] study the online Bayesian persuasion problem faced by a platform that observes some relevant information about the state of a product and repeatedly interacts with a population of myopic receivers through a recommendation mechanism. Agrawal et al. [2023] design a regret-minimization algorithm in an advertising setting based on the Bayesian persuasion framework, assuming that the receiver\u2019s utility function satisfies some regularity conditions. ", "page_idx": 13}, {"type": "text", "text": "Online learning in problems with commitment From a technical point of view, our work is related to the problem of learning optimal strategies in Stackelberg games when the leader has no knowledge of the follower\u2019s utility. Letchford et al. [2009] propose the first algorithm to learn optimal strategies in Stackelberg games. Their algorithm is based on an initial random sampling that may require an exponential number of samples, both in the number of leader\u2019s actions $m$ and in the representation precision $L$ . Peng et al. [2019] improve the algorithm of Letchford et al. [2009], while Bacchiocchi et al. [2024a] further improve the approach by Peng et al. [2019] by relaxing some of their assumptions. ", "page_idx": 13}, {"type": "text", "text": "Furthermore, our work is also related to the problem of learning optimal strategies in Stackelberg games where the leader and the follower interaction is modelled by a Markov Decision Process. Lauffer et al. [2024] study Stackelberg games with a state that influences the leader\u2019s utility and available actions. Bai et al. [2021] consider a setting where the leader commits to a pure strategy and observes a noisy measurement of their utility. ", "page_idx": 13}, {"type": "text", "text": "Finally, our work is also related to online hidden-action principal-agent problems, in which a principal commits to a contract at each round to induce an agent to take favorable actions. Ho et al. [2014] initiated the study by proposing an algorithm that adaptively refines a discretization over the space of contracts, framing the model as a multi-armed bandit problem where the discretization provides a finite number of arms to play with. Cohen et al. [2022] similarly work in a discretized space but with milder assumptions. Zhu et al. [2023] provide a more general algorithm that works in hidden-action principal-agent problems with multiple agent types. Finally, Bacchiocchi et al. [2024b] study the same setting and propose an algorithm with smaller regret when the number of agent actions is small. ", "page_idx": 13}, {"type": "text", "text": "B Additional preliminaries ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 Additional preliminaries on Bayesian persuasion ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In step (3) of the sender-receiver interaction presented in Section 2.1, after observing $s\\in S$ , the receiver performs a Bayesian update and infers a posterior belief $\\xi^{s}\\in\\Delta_{\\Theta}$ over the states of nature, ", "page_idx": 13}, {"type": "text", "text": "according to the following equation: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\xi_{\\theta}^{s}=\\frac{\\mu_{\\theta}\\,\\phi_{\\theta}(s)}{\\sum_{\\theta^{\\prime}\\in\\Theta}\\mu_{\\theta^{\\prime}}\\,\\phi_{\\theta^{\\prime}}(s)}\\qquad\\forall\\theta\\in\\Theta.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Consequently, given a signaling scheme $\\phi$ , we can equivalently represent it as a distribution over the set of posteriors it induces. Formally, we say that $\\phi$ induces $\\gamma:\\bar{\\Delta}_{\\Theta}\\rightarrow[0,1]$ if, for each posterior distribution $\\xi\\in\\Delta_{\\Theta}$ , we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\gamma(\\xi)=\\sum_{s\\in S:\\xi^{s}=\\xi}\\sum_{\\theta\\in\\Theta}\\mu_{\\theta}\\phi_{\\theta}(s)\\qquad\\mathrm{and}\\qquad\\sum_{\\xi\\in\\mathrm{supp}(\\gamma)}\\gamma(\\xi)=1.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Furthermore, we say that a distribution over a set of posteriors $\\gamma$ is consistent, $i.e.$ ., there exists a valid signaling scheme $\\phi$ inducing $\\gamma$ if the following holds: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sum_{\\xi\\in\\mathrm{supp}(\\gamma)}\\gamma(\\xi)\\xi_{\\theta}=\\mu_{\\theta}\\;\\;\\;\\forall\\theta\\in\\Theta.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "With an abuse of notation, we will sometimes refer to a consistent distribution over a set of posteriors $\\gamma$ as a signaling scheme. This is justified by the fact that there exists a signaling scheme $\\phi$ inducing such distribution, but we are interested only in the distribution over the set of posteriors that $\\phi$ induces. ", "page_idx": 14}, {"type": "text", "text": "B.2 Additional preliminaries on the representation of numbers ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In the following, we assume that all the numbers manipulated by our algorithms are rational. Furthermore, we assume that rational numbers are represented as fractions, by specifying two integers which encode their numerator and denominator. Given a rational number $q\\in\\mathbb{Q}$ represented as a fraction $b/c$ with $b,c\\in\\mathbb{Z}$ , we denote the number of bits that $q$ occupies in memory, called bit-complexity, as $B_{q}:=B_{b}+B_{c}$ , where $B_{b}$ $\\left(B_{c}\\right)$ is the number of bits required to represent the numerator (denominator). For the sake of the presentation, with an abuse of terminology, given a vector in $\\mathbb{Q}^{D}$ of $D$ rational numbers represented as fractions, we let its bit-complexity be the maximum bit-complexity among its entries. ", "page_idx": 14}, {"type": "text", "text": "Furthermore, we assume that the bit-complexity encoding both the receiver\u2019s utility and the prior distribution is bounded. Formally, we denote by $B_{\\mu}$ the bit-complexity of the prior $\\mu$ , while we assume $B_{u}$ to be an upper bound to the bit-complexity of each $d$ -dimensional vector $u_{\\theta}(a)$ with $\\theta\\in\\Theta$ . Moreover, we let $B:=B_{\\mu}+B_{u}$ . Finally, we also denote with $B_{\\epsilon}$ the bit-complexity of the parameter $\\epsilon$ computed by our algorithms, while we denote with $B_{\\widehat{\\mu}}$ the bit-complexity of the estimator $\\widehat{\\mu}$ computed by Algorithm 2. ", "page_idx": 14}, {"type": "text", "text": "C Omitted proofs from Section 4 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Lemma 1. Given $\\begin{array}{r}{T_{1}\\;:=\\;\\left\\lceil\\frac{12}{\\epsilon}\\log\\left(2d/\\delta\\right)\\right\\rceil}\\end{array}$ and $\\epsilon\\;\\in\\;(0,{1}/{6d})$ , Algorithm 2 employs $T_{1}$ rounds and terminates with a set $\\mathcal{X}_{\\epsilon}\\subseteq\\mathcal{X}$ such that, with probability at least $\\begin{array}{r}{1-\\delta\\colon(i)\\sum_{\\theta\\in\\Theta}\\mu_{\\theta}x_{\\theta}\\geq\\epsilon}\\end{array}$ for every slice $x\\in\\mathcal{X}_{\\epsilon}$ and (ii) $\\textstyle\\sum_{\\theta\\in\\Theta}\\mu_{\\theta}x_{\\theta}\\leq10\\epsilon$ for every slice $x\\in\\mathcal{X}\\setminus\\mathcal{X}_{\\epsilon}$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. For each $\\theta\\in\\Theta$ we consider two possible cases. ", "page_idx": 14}, {"type": "text", "text": "1. If $\\mu_{\\theta}>\\epsilon$ , then we employ the multiplicative Chernoff inequality as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(|\\mu_{\\theta}-\\widehat{\\mu}_{\\theta}|\\ge\\frac{1}{2}\\mu_{\\theta}\\right)\\le2e^{-\\frac{T_{1}\\mu_{\\theta}}{12}},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $T_{1}\\in\\mathbb{N}_{+}$ is the number of rounds employed to estimate ${\\widehat{\\mu}}_{\\theta}$ . As a result, by setting the number of rounds to estimate $\\mu_{\\theta}$ equal to $\\dot{T_{1}\\sp{\\prime}}=\\lceil{^{12}}/{\\epsilon}\\log\\left({^{2d}\\!/}{\\delta}\\right)\\rceil$ we get: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(|\\mu_{\\theta}-\\widehat{\\mu}_{\\theta}|\\geq\\frac{1}{2}\\mu_{\\theta}\\right)\\leq2\\left(\\frac{\\delta}{2d}\\right)^{\\frac{\\mu_{\\theta}}{\\epsilon}}\\leq\\frac{\\delta}{d},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "since $\\mu_{\\theta}>\\epsilon$ . Then, we get: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\frac{\\mu_{\\theta}}{2}\\leq\\widehat{\\mu}_{\\theta}\\leq\\frac{3\\mu_{\\theta}}{2}\\right)\\geq1-\\frac{\\delta}{d}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Consequently, with a probability of at least $1-\\delta/d$ the estimator ${\\widehat{\\mu}}_{\\theta}$ is such that $\\widehat{\\mu}_{\\theta}\\;\\in$ $\\left[\\mu_{\\theta}/2,\\,3\\mu_{\\theta}/2\\right]$ . Thus, if $\\mu_{\\theta}\\geq6\\epsilon$ , then $\\widehat{\\mu}_{\\theta}\\geq3\\epsilon>2\\epsilon$ and \u03b8 \u2208\u0398. We also notice that there always exits a $\\theta\\in\\Theta$ such that $\\mu_{\\theta}\\,\\geq\\,^{1}\\!/d\\,\\geq\\,6\\epsilon$ , since $\\epsilon\\,\\in\\,(0,{^1\\mathord{\\left/{\\vphantom{^16d}}\\right.\\kern-\\nulldelimiterspace}6d})$ . Consequently, with a probability of at least $1-\\delta/d$ , there always exist a $\\theta\\in\\widetilde{\\Theta}$ . ", "page_idx": 15}, {"type": "text", "text": "2. If $\\mu_{\\theta}\\leq\\epsilon$ , then we employ the multiplicative Chernoff inequality as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\widehat{\\mu}_{\\theta}\\geq(1+c)\\mu_{\\theta}\\right)\\leq e^{-\\frac{c^{2}T_{1}\\mu_{\\theta}}{2+c}},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "with $c=\\epsilon/\\mu_{\\theta}$ . Thus, by setting the number of rounds employed to estimate $\\mu_{\\theta}$ equal to $T_{1}=\\lceil12/\\epsilon\\log\\left(2d/\\delta\\right)\\rceil$ , we get: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\widehat{\\mu}_{\\theta}\\geq\\mu_{\\theta}+\\epsilon\\right)\\leq\\exp\\left(-\\frac{\\frac{\\epsilon^{2}}{\\mu_{\\theta}^{2}}\\,\\left(\\frac{12}{\\epsilon}\\log\\left(\\frac{2d}{\\delta}\\right)\\right)\\,\\mu_{\\theta}}{2+\\frac{\\epsilon}{\\mu_{\\theta}}}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\exp\\left(-\\frac{\\frac{12\\epsilon}{\\mu_{\\theta}}\\log\\left(\\frac{2d}{\\delta}\\right)}{2+\\frac{\\epsilon}{\\mu_{\\theta}}}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\left(\\frac{\\delta}{2d}\\right)^{4}\\leq\\frac{\\delta}{d},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "since $x/(x+2)\\geq1/3$ , for each $x\\geq1$ . As a result, we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\widehat{\\mu}_{\\theta}\\leq\\mu_{\\theta}+\\epsilon\\right)\\geq1-\\frac{\\delta}{d}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus, with a probability of at least $1-\\delta/d$ , if $\\mu_{\\theta}\\leq\\epsilon$ , then $\\widehat{\\mu}_{\\theta}\\leq\\mu_{\\theta}+\\epsilon$ , which implies that $\\widehat{\\mu}_{\\theta}\\leq2\\epsilon$ . Furthermore, if $\\mu_{\\theta}\\leq\\epsilon$ , then $\\widehat{\\mu}_{\\theta}\\leq2\\epsilon$ and $\\theta\\not\\in{\\widetilde{\\Theta}}$ . ", "page_idx": 15}, {"type": "text", "text": "Thus, by employing a union bound over the set of natures, we have that if $\\mu_{\\theta}\\leq\\epsilon$ , then its corresponding estimate ${\\widehat{\\mu}}_{\\theta}$ falls within the interval $[0,2\\epsilon]$ and $\\theta\\not\\in{\\widetilde{\\Theta}}$ , while if $\\mu_{\\theta}\\geq6\\epsilon$ , then its corresponding estimate is such that $\\widehat{\\mu}_{\\theta}>2\\epsilon$ and $\\theta\\in\\widetilde{\\Theta}$ , with a probability of at least $1-\\delta$ . We also notice that, with the same probability, the set $\\widetilde{\\Theta}$ is always non empty. ", "page_idx": 15}, {"type": "text", "text": "Consequently, for each slice $x\\in\\mathcal X_{\\epsilon}$ with respect to a signal $s$ , the probability of observing $s$ can be lower bounded as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\epsilon\\leq\\frac{1}{2}\\sum_{\\theta\\in\\tilde{\\Theta}}\\widehat{\\mu}_{\\theta}x_{\\theta}\\leq\\frac{3}{4}\\sum_{\\theta\\in\\tilde{\\Theta}}\\mu_{\\theta}x_{\\theta}\\leq\\sum_{\\theta\\in\\tilde{\\Theta}}\\mu_{\\theta}x_{\\theta}\\leq\\sum_{\\theta\\in\\Theta}\\mu_{\\theta}x_{\\theta},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the inequalities above hold because of the definition of $\\textstyle{\\mathcal{X}}_{\\epsilon}$ and observing that each \u03b8 \u2208\u0398 satisfies Equation 4 with probability at least $1-\\delta$ . ", "page_idx": 15}, {"type": "text", "text": "Furthermore, for each $x\\notin\\mathcal{X}_{\\epsilon}$ , the two following conditions hold: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\sum_{\\theta\\in\\widetilde{\\Theta}}\\mu_{\\theta}x_{\\theta}\\leq\\sum_{\\theta\\in\\widetilde{\\Theta}}\\widehat{\\mu}_{\\theta}x_{\\theta}\\leq2\\epsilon\\quad\\mathrm{and}\\quad\\sum_{\\theta\\notin\\widetilde{\\Theta}}\\mu_{\\theta}x_{\\theta}\\leq6\\epsilon\\sum_{\\theta\\notin\\widetilde{\\Theta}}x_{\\theta}\\leq6\\epsilon,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "with probability at least $1-\\delta$ . Thus, by putting the two inequalities above together, for each $x\\notin\\mathcal{X}_{\\epsilon}$ , we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{\\theta\\in\\Theta}\\mu_{\\theta}x_{\\theta}\\leq10\\epsilon,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "with probability at least $1-\\delta$ , concluding the proof. ", "page_idx": 15}, {"type": "text", "text": "Lemma 2. Under event ${\\mathcal{E}}^{1}$ , given any $\\rho\\,\\in\\,(0,1)$ and a normalized slice $x\\,\\in\\,{\\mathcal{X}}_{\\epsilon}$ , if the sender commits to a signaling scheme $\\phi:\\Theta\\rightarrow S:=\\{s_{1},s_{2}\\}$ such that $\\phi_{\\theta}(s_{1})=x_{\\theta}$ for all $\\theta\\in\\Theta$ during $\\begin{array}{r}{q:=\\lceil\\frac{1}{\\epsilon}\\log(^{1/\\rho})\\rceil}\\end{array}$ rounds, then, with probability at least $1-\\rho,$ , signal $s_{1}$ is sent at least once. ", "page_idx": 15}, {"type": "text", "text": "Proof. In the following, we let $\\tau$ be the first round in which the sender commits to $\\phi$ . The probability of observing the signal $s_{1}$ at a given round $t\\geq\\tau$ is can be lower bounded as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(s^{t}=s_{1}\\right)=\\sum_{\\theta\\in\\Theta}\\mu_{\\theta}x_{\\theta}\\geq\\epsilon,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the inequality holds under the event ${\\mathcal{E}}_{1}$ . Thus, at each round, the probability of sampling the signal $s_{1}\\in\\mathcal{S}$ is greater or equal to $\\epsilon>0$ . Consequently, the probability of never observing the signal $s_{1}\\in\\mathcal{S}$ in $q$ rounds is given by: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\bigcap_{t=\\tau}^{\\tau+q-1}\\{s^{t}\\neq s_{1}\\}\\right)\\leq(1-\\epsilon)^{q}\\leq\\rho,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the last inequality holds by taking $\\begin{array}{r}{q=\\left\\lceil\\frac{\\log(\\rho)}{\\log(1-\\epsilon)}\\right\\rceil\\leq\\left\\lceil\\frac{\\log(1/\\rho)}{\\epsilon}\\right\\rceil}\\end{array}$ , for each $\\epsilon\\in(0,1)$ ", "page_idx": 16}, {"type": "text", "text": "As a result, the probability of observing the signal $s_{1}$ at least once in $q$ rounds is greater or equal to: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\bigcup_{t=\\tau}^{\\tau+q-1}\\{s^{t}\\neq s_{1}\\}\\right)=1-\\mathbb{P}\\left(\\bigcap_{t=\\tau}^{\\tau+q-1}\\{s^{t}\\neq s_{1}\\}\\right)\\geq1-\\rho,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "concluding the proof. ", "page_idx": 16}, {"type": "text", "text": "Theorem 1. The regret attained by Algorithm $^{\\,l}$ is $R_{T}\\leq\\tilde{\\mathcal{O}}\\big(\\binom{d+n}{d}n^{3/2}d^{3}\\sqrt{B T}\\big).$ ", "page_idx": 16}, {"type": "text", "text": "Proof. In the following, we let \u03b4 = \u03b6 = T1 and \u03f5 = \u2308\u2308B\u221anT d\u23094\u2309, as defined in Algorithm 1. To prove the theorem, we decompose the regret suffered in the three phases of Algorithm 1: ", "page_idx": 16}, {"type": "text", "text": "1. Phase 1. We observe that the number of rounds to execute the Build-Search-Space procedure (Algorithm 2) is equal to $\\begin{array}{r}{T_{1}\\,=\\,\\mathcal{O}\\left(^{1}/\\epsilon\\log\\left(^{1}/\\delta\\right)\\log(d)\\right).}\\end{array}$ . Thus, the cumulative regret of Phase 1 can be upper bounded as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\nR_{T}^{1}\\leq\\widetilde{\\mathcal{O}}\\left(\\frac{1}{\\epsilon}\\log(T)\\log(d)\\right)\\leq\\widetilde{\\mathcal{O}}\\left(\\frac{\\sqrt{T}}{d^{4}\\sqrt{n B}}\\log(d)\\right)\\leq\\widetilde{\\mathcal{O}}\\left(\\sqrt{T}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This is because, at each round, the regret suffered during the execution of Algorithm 2 is at most one. ", "page_idx": 16}, {"type": "text", "text": "2. Phase 2. Under the event ${\\mathcal{E}}_{1}$ , which holds with probability $1-\\delta$ , Algorithm 6 correctly terminates with probability $1-\\zeta$ . Thus, with probability at least $1-\\delta-\\zeta$ , the number of rounds employed by such algorithm is of the order: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{2}\\le\\widetilde{\\mathcal{O}}\\left(\\frac{n^{2}}{\\epsilon}\\log^{2}\\left(\\frac{1}{\\zeta}\\right)\\left(d^{7}(B+B_{\\epsilon}+B_{\\widehat{\\mu}})+\\binom{d+n}{d}\\right)\\right)}\\\\ &{\\quad=\\widetilde{\\mathcal{O}}\\left(\\frac{n^{2}}{\\epsilon}\\log^{2}(T)\\left(d^{7}(B+B_{\\epsilon})+\\binom{d+n}{d}\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the last equality holds because $B_{\\widehat{\\mu}}=\\mathcal{O}(\\log(1/\\epsilon)+\\log(d)+\\log(T))$ . As a result, by taking the expectation, the regret suffere d in Phase 2 by Algorithm 1 can be upper bounded as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\nR_{T}^{2}\\leq\\widetilde{\\mathcal{O}}\\left(n^{3/2}d^{3}\\binom{d+n}{d}\\sqrt{B T}\\right),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "since, at each round, the regret suffered during the execution of Algorithm 6 is at most one. ", "page_idx": 16}, {"type": "text", "text": "3. Phase 3. Let $\\tau$ be the number of rounds required by Phase 1 and Phase 2 to terminate. Under the events ${\\mathcal{E}}_{1}$ and $\\mathcal{E}_{2}$ , which hold with probability at least $1-\\delta-\\zeta$ , thanks to Lemma 4, the solution returned by Algorithm 3 at each round $t>\\tau$ is $O(d n\\epsilon+\\nu_{t})$ -optimal, where we define $\\begin{array}{r}{\\nu_{t}=\\left|\\sum_{\\theta\\in\\Theta}\\mu_{\\theta}-\\widehat{\\mu}_{t,\\theta}\\right|}\\end{array}$ . We introduce the following event: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{E_{t}=\\left\\{|\\mu_{\\theta}-\\widehat{\\mu}_{t,\\theta}|\\leq\\epsilon_{t}~~\\forall\\theta\\in\\Theta\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where we let $\\epsilon_{t}>0$ be defined as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\epsilon_{t}=\\sqrt{\\frac{\\log{\\left({2d T/\\iota}\\right)}}{{2(t-\\tau)}}},\\,\\,\\,\\,t>\\tau.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then, by Hoeffding\u2019s inequality and a union bound we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\bigcap_{t>\\tau}E_{t}\\Big)\\geq1-\\iota.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus, by setting $\\iota=1/T$ , the regret suffered in Phase 3 by Algorithm 1 can be upper bounded as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{T}^{3}\\leq\\displaystyle\\sum_{t=\\tau+1}^{T}\\left|\\sum_{\\theta\\in\\Theta}\\mu_{\\theta}-\\widehat\\mu_{t,\\theta}\\right|+\\mathcal{O}\\left(d\\epsilon T\\right)}\\\\ &{\\quad\\quad\\leq d\\displaystyle\\sum_{t=\\tau+1}^{T}\\epsilon_{t}+\\mathcal{O}\\left(d n\\epsilon T\\right)}\\\\ &{\\quad\\quad\\leq\\tilde{\\mathcal{O}}\\left(d\\sqrt{T}+d n\\epsilon T\\right)}\\\\ &{\\quad\\quad=\\tilde{\\mathcal{O}}\\left(d\\sqrt{T}+n^{3/2}d^{5}\\sqrt{B T}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "As a result, the regret of Algorithm 1 is in the order of: ", "page_idx": 17}, {"type": "equation", "text": "$$\nR_{T}\\le\\widetilde{\\mathcal{O}}\\left(n^{3/2}d^{3}\\binom{d+n}{d}\\sqrt{B T}+n^{3/2}d^{5}\\sqrt{B T}\\right)=\\widetilde{\\mathcal{O}}\\left(n^{3/2}d^{3}\\binom{d+n}{d}\\sqrt{B T}\\right),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "concluding the proof. ", "page_idx": 17}, {"type": "text", "text": "D Proof of Lemma 4 from Section 4.3 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In order to prove Lemma 4, we first consider an auxiliary LP (Program 5a) that works on the vertices of the regions $\\mathcal{X}_{\\epsilon}(a)$ . This is useful to take into account the polytopes $\\mathcal{X}_{\\epsilon}(a)$ with null volume. Indeed, for every action $a\\in A$ such that $\\operatorname{vol}(\\mathcal{X}_{\\epsilon}(a))=0$ , Algorithm 3 takes in input only a face $\\mathcal{R}_{\\epsilon}(a)$ of $\\mathcal{X}_{\\epsilon}(a)$ such that $\\mathcal{V}_{\\epsilon}(a)\\subseteq V(\\mathcal{R}_{\\epsilon}(a))$ . By working on the vertices of the regions $\\mathcal{X}_{\\epsilon}(a)$ , we can show that the vertices in $\\mathcal{V}_{\\epsilon}(a)$ are sufficient to compute an approximately optimal signaling scheme. ", "page_idx": 17}, {"type": "text", "text": "The auxiliary LP that works on the vertices is the following: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\operatorname*{max}_{\\alpha\\geq0}}&{\\displaystyle\\sum_{x\\in\\mathcal{V}_{\\epsilon}}\\alpha_{x}\\sum_{\\theta\\in\\Theta}\\mu_{\\theta}x_{\\theta}u_{\\theta}^{s}(a(x))}\\\\ {\\mathrm{s.t.}\\quad}&{\\displaystyle\\sum_{x\\in\\mathcal{V}_{\\epsilon}}\\alpha_{x}x_{\\theta}\\leq1\\quad\\forall\\theta\\in\\Theta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Program 5a takes in input the set of vertices $\\begin{array}{r}{\\mathcal{V}_{\\epsilon}:=\\bigcup_{a\\in\\mathcal{A}}V(\\mathcal{X}_{\\epsilon}(a))}\\end{array}$ , along with the corresponding best-responses $(a(x))_{x\\in\\mathcal{V}_{\\epsilon}}$ and the exact prior $\\mu$ . It  then optimizes over the non-negative variables $\\alpha_{x}\\geq0$ , one for vertex $x\\in\\mathcal{V}_{\\epsilon}$ . These variables $\\alpha_{x}$ act as weights for the corresponding slices $x\\in\\mathcal{V}_{\\epsilon}$ , identifying a non-normalized slice $\\alpha_{x}x\\in\\gamma^{\\sqcup}$ . ", "page_idx": 17}, {"type": "text", "text": "In the following, we show that the value of an optimal solution $v^{\\star}$ to Program 5a is at least $v^{\\star}\\geq$ $\\mathrm{OPT}-{\\mathcal{O}}(\\epsilon n d)$ . Then, we prove that the signaling scheme $\\phi$ computed by Algorithm 3 achieves a principal\u2019s expected utility of at least $v^{\\star}$ minus a quantity related to the difference between the estimated prior $\\widehat{\\mu}_{t}$ and with the actual prior $\\mu$ . Thus, by considering that $v^{\\star}\\geq\\mathrm{OPT}-\\mathcal{O}(\\epsilon n d)$ , we will be able to p r ove Lemma 4. ", "page_idx": 17}, {"type": "text", "text": "As a first step, we show that it is possible to decompose each slice $x\\in\\mathcal{X}_{\\epsilon}$ into a weighted sum of the vertices $x^{\\prime}\\in\\mathcal{V}_{\\epsilon}$ without incurring a loss in the sender\u2019s utility. Thus, a generic slice $x$ of an optimal signaling scheme can be written as a convex combination of slices $x^{\\prime}$ with $x^{\\prime}\\in\\mathcal{V}_{\\epsilon}$ . This property is formalized in the following lemma. ", "page_idx": 17}, {"type": "text", "text": "Lemma 6. For every $x\\in\\mathcal{X}_{\\epsilon}$ , there exists a distribution $\\alpha\\in\\Delta_{\\nu_{\\epsilon}}$ such that: ", "page_idx": 18}, {"type": "equation", "text": "$$\nx_{\\theta}=\\sum_{x^{\\prime}\\in\\mathcal{V}_{\\epsilon}}\\alpha_{x^{\\prime}}x_{\\theta}^{\\prime}\\quad\\forall\\theta\\in\\Theta.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Furthermore, the following holds: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{\\theta\\in\\Theta}\\mu_{\\theta}x_{\\theta}u_{\\theta}^{\\mathrm{s}}(a(x))\\leq\\sum_{x^{\\prime}\\in\\mathcal{V}_{\\epsilon}}\\!\\alpha_{x}\\!\\!\\sum_{\\theta\\in\\Theta}\\mu_{\\theta}x_{\\theta}^{\\prime}u_{\\theta}^{\\mathrm{s}}(a(x^{\\prime})).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. Let $a:=a(x)$ . Since $x\\in\\mathcal{X}_{\\epsilon}(a)$ , by the Carath\u00e9odory theorem, there exists an $\\alpha\\in\\Delta_{V(\\mathcal{X}_{\\epsilon}(a))}$ such that: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{x^{\\prime}\\in V(\\mathcal{X}_{\\epsilon}(a))}\\alpha_{x^{\\prime}}x_{\\theta}^{\\prime}=x_{\\theta}\\;\\;\\forall\\theta\\in\\Theta.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Furthermore: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{\\theta\\in\\Theta}\\mu_{\\theta}x_{\\theta}u_{\\theta}^{s}(a)=\\sum_{\\theta\\in\\Theta}\\mu_{\\theta}\\left(\\sum_{x^{\\prime}\\in V(X_{\\epsilon}(a))}\\alpha_{x^{\\prime}}x_{\\theta}^{\\prime}\\right)u_{\\theta}^{s}(a).}}\\\\ &{}&{=\\sum_{x^{\\prime}\\in V(X_{\\epsilon}(a))}\\alpha_{x^{\\prime}}\\sum_{\\theta\\in\\Theta}\\mu_{\\theta}x_{\\theta}^{\\prime}u_{\\theta}^{s}(a)}\\\\ &{}&{\\leq\\sum_{x^{\\prime}\\in V(X_{\\epsilon}(a))}\\alpha_{x^{\\prime}}\\sum_{\\theta\\in\\Theta}\\mu_{\\theta}x_{\\theta}^{\\prime}u_{\\theta}^{s}(a(x^{\\prime}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the inequality holds because the receiver breaks ties in favor of the sender. Finally, we observe that for each distribution over the set $V(\\mathcal X_{\\epsilon}(a))$ for a given $\\mathcal{X}_{\\epsilon}(a)$ , we can always recover a probability distribution supported in $\\mathcal{V}_{\\epsilon}$ , since $V(\\mathcal X_{\\epsilon}(a))\\subseteq\\mathcal V_{\\epsilon}$ by construction. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "Thanks to the the result above, in the next lemma (Lemma 7) we prove that an optimal solution of Program 5a has value at least $v^{\\star}\\geq\\mathrm{OPT}-10\\epsilon n d$ . To show this, we begin by observing that there exists a set $\\mathcal{I}$ of slices of the optimal signaling scheme that belong to the search space $\\textstyle{\\mathcal{X}}_{\\epsilon}$ . By applying Lemma 6 to each of these slices, we obtain a feasible solution for Program 5a. The value of this solution is at least the sender\u2019s expected utility given by the slices in $\\mathcal{I}$ . Finally, thanks to the properties of the search space $\\textstyle{\\mathcal{X}}_{\\epsilon}$ , we can bound the expected sender\u2019s utility provided by the slices that lie outside the search space. ", "page_idx": 18}, {"type": "text", "text": "Lemma 7. Under the event ${\\mathcal{E}}_{1}$ , the optimal solution of Program 5a has value at least $v^{\\star}\\geq\\mathrm{OPT}-$ 10\u03f5nd. ", "page_idx": 18}, {"type": "text", "text": "Proof. In the following we let $\\phi_{\\theta}\\in\\Delta_{\\mathcal{A}}$ for each $\\theta\\in\\Theta$ be an optimal signaling scheme, where we assume, without loss of generality, such a signaling scheme to be direct, meaning that ${\\mathcal{S}}={\\mathcal{A}}$ and $a\\in A^{\\phi}(a)$ for every action $a\\in A$ . Furthermore, for each action $a\\in\\operatorname{supp}(\\phi)$ , we define: ", "page_idx": 18}, {"type": "equation", "text": "$$\nx_{\\theta}^{a}=\\frac{\\phi_{\\theta}(a)}{\\sum_{\\theta\\in\\Theta}\\phi_{\\theta}(a)}\\ \\forall\\theta\\in\\Theta\\ \\ \\mathrm{and}\\ \\ \\alpha_{x^{a}}=\\sum_{\\theta\\in\\Theta}\\phi_{\\theta}(a).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We observe that each $x^{a}\\in\\mathcal{X}(a)$ , indeed we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{\\theta\\in\\Theta}\\mu_{\\theta}x_{\\theta}u_{\\theta}(a)=\\frac{1}{\\alpha_{x^{a}}}\\sum_{\\theta\\in\\Theta}\\mu_{\\theta}\\phi_{\\theta}(a)u_{\\theta}(a)\\geq\\frac{1}{\\alpha_{x^{a}}}\\sum_{\\theta\\in\\Theta}\\mu_{\\theta}\\phi_{\\theta}(a)u_{\\theta}(a^{\\prime})=\\sum_{\\theta\\in\\Theta}\\mu_{\\theta}x_{\\theta}u_{\\theta}(a^{\\prime}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for every action $a^{\\prime}\\in A$ . We define the subset of actions ${\\mathcal{A}}^{\\prime}\\subseteq A$ in a way that if $a\\in A^{\\prime}$ , then $x^{a}\\in\\mathcal{X}_{\\epsilon}$ , i.e., $\\mathcal{A}^{\\prime}:=\\{a\\in\\mathcal{A}\\mid x^{a}\\in\\mathcal{X}_{\\epsilon}\\}$ . Then, we let $A^{\\prime\\prime}:=\\operatorname*{supp}(\\phi)\\setminus A^{\\prime}$ . Furthermore, for each $a\\in A^{\\prime}$ , thanks to Lemma 6, there exists a distribution $\\alpha^{a}\\in\\Delta_{\\upnu_{\\epsilon}}$ such that: ", "page_idx": 18}, {"type": "equation", "text": "$$\nx_{\\theta}^{a}=\\sum_{x^{\\prime}\\in\\mathcal{V}_{\\epsilon}}\\alpha_{x^{\\prime}}^{a}x_{\\theta}^{\\prime},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and the following holds: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{\\theta\\in\\Theta}\\mu_{\\theta}x_{\\theta}^{a}u_{\\theta}(a)=\\sum_{\\theta\\in\\Theta}\\mu_{\\theta}\\left(\\sum_{x^{\\prime}\\in\\mathcal{V}_{\\epsilon}}\\alpha_{x^{\\prime}}^{a}x_{\\theta}^{\\prime}\\right)u_{\\theta}^{s}(a).}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\displaystyle\\sum_{\\theta\\in\\Theta}\\sum_{x^{\\prime}\\in\\mathcal{V}_{\\epsilon}}\\mu_{\\theta}\\alpha_{x^{\\prime}}^{a}x_{\\theta}^{\\prime}u_{\\theta}^{s}(a)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\leq\\displaystyle\\sum_{\\theta\\in\\Theta}\\sum_{x^{\\prime}\\in\\mathcal{V}_{\\epsilon}}\\mu_{\\theta}\\alpha_{x^{\\prime}}^{a}x_{\\theta}^{\\prime}u_{\\theta}^{s}(a(x^{\\prime})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We also define $\\alpha^{\\star}:\\mathcal{V}_{\\epsilon}\\to\\mathbb{R}_{+}$ as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\alpha_{x^{\\prime}}^{\\star}=\\sum_{a\\in\\mathcal{A}^{\\prime}}\\alpha_{x^{\\prime}}^{a}\\alpha_{x^{a}},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for each $x^{\\prime}\\in\\mathcal{V}_{\\epsilon}$ . First, we show that $\\alpha^{\\star}:\\mathcal{V}_{\\epsilon}\\rightarrow\\mathbb{R}_{+}$ is a feasible solution to LP 5a. Indeed, for each $\\theta\\in\\Theta$ , it holds: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{x^{\\prime}\\in\\mathcal{V}_{\\epsilon}}\\alpha_{x^{\\prime}}^{\\star}x_{\\theta}^{\\prime}=\\displaystyle\\sum_{x^{\\prime}\\in\\mathcal{V}_{\\epsilon}}\\sum_{\\alpha\\in A^{\\prime}}\\alpha_{x^{\\prime}}^{a}\\alpha_{x^{\\alpha}}x_{\\theta}^{\\prime}}&{}\\\\ {\\displaystyle=\\sum_{a\\in A^{\\prime}}\\alpha_{x^{a}}\\sum_{x^{\\prime}\\in\\mathcal{V}_{\\epsilon}}\\alpha_{x^{\\prime}}^{a}x_{\\theta}^{\\prime}}&{}\\\\ {\\displaystyle=\\sum_{a\\in A^{\\prime}}\\alpha_{x^{a}}x_{\\theta}^{a}}&{}\\\\ {\\displaystyle=\\sum_{a\\in A^{\\prime}}\\phi_{\\theta}(a)\\le1}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Where the equalities above holds thanks to Equation 6 and the definition of $\\alpha^{\\star}$ . Then, we show that the utility achieved by $\\alpha^{\\star}:\\mathcal{V}_{\\epsilon}\\to\\mathbb{R}_{+}^{m}$ is greater or equal to $\\mathrm{OPT}-10\\epsilon d$ . Formally, we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{x^{\\prime}\\in\\mathbb{R}_{+}}\\alpha_{x^{\\prime}}\\sum_{i\\in\\mathbb{R}_{+}}\\mu_{\\alpha^{\\prime}}v_{i i}^{\\alpha}(\\alpha^{\\prime})-\\sum_{x^{\\prime}\\in\\mathbb{R}_{+}}\\sum_{i\\in\\mathbb{N}}\\alpha_{x^{\\prime}}\\alpha_{x^{\\prime}}\\sum_{\\theta\\in\\Theta}\\mu_{\\alpha^{\\prime}}^{*}\\mu_{\\theta}^{*}(\\alpha^{\\prime})}\\\\ {=}&{\\displaystyle\\sum_{x^{\\prime}\\in\\mathbb{R}_{+}}\\sum_{i\\in\\mathbb{N}}\\sum_{\\ell\\in\\Theta}\\mu_{\\alpha^{\\prime}}\\alpha_{x^{\\prime}}^{*}\\mu_{\\theta}^{*}(\\alpha^{\\prime})}\\\\ {=}&{\\displaystyle\\sum_{x^{\\prime}\\in\\mathbb{R}_{+}}\\sum_{i\\in\\mathbb{N}}\\sum_{\\ell\\in\\Theta}\\mu_{\\alpha^{\\prime}}\\alpha_{x^{\\prime}}^{*}\\mu_{\\theta}^{*}(\\alpha^{\\prime})}\\\\ {\\ge}&{\\displaystyle\\sum_{\\ell\\in\\mathbb{R}_{+}}\\sum_{\\ell\\in\\mathbb{R}_{+}}\\sum_{\\ell\\in\\mathbb{R}_{+}}\\mu_{\\ell}^{*}\\mu_{\\theta}^{*}(\\alpha)}\\\\ {=}&{\\displaystyle\\sum_{\\alpha\\in\\mathbb{R}_{+}}\\sum_{i\\in\\mathbb{N}}\\mu_{\\alpha\\in\\mathbb{R}_{+}}(\\alpha_{1})s_{\\theta}^{*}(\\alpha)}\\\\ {=}&{\\displaystyle=\\alpha_{\\ell}\\lambda^{*}\\alpha_{\\ell}\\exp\\sum_{\\ell=\\lfloor\\alpha^{\\prime}\\rfloor\\leq\\ell\\rfloor}\\mu_{\\alpha\\in\\mathbb{R}_{+}}(\\alpha)s_{\\theta}^{*}(\\alpha)}\\\\ {=}&{\\displaystyle=\\exp{-\\sum_{x^{\\prime}\\in\\mathbb{R}_{+}}\\sum_{\\ell\\in\\mathbb{R}_{+}}\\sum_{\\ell\\in\\mathbb{R}_{+}}\\mu_{\\ell}(\\alpha_{1})s_{\\theta}^{*}(\\alpha)}}\\\\ {=}&{\\displaystyle\\sum_{\\alpha\\in\\mathbb{R}_{+}}\\sum_{\\ell\\in\\mathbb{R}_{+}}\\sum_{\\ell\\in\\mathbb{R}_{+}}\\mu_{\\alpha\\in\\mathbb{R}_{+}^{*}(\\alpha)}}\\\\ {{}}&{{}\\ge\\mathrm{O r r-d-\\sum_{x\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Where the first inequality holds thanks to Inequality (6), the second inequality holds since $\\alpha_{x^{a}}\\leq d$ and the last inequality holds since, for each $x\\notin\\mathcal{X}_{\\epsilon}$ , it holds $\\begin{array}{r}{\\sum_{\\theta\\in\\Theta}\\mu_{\\theta}x_{\\theta}^{a}(a)\\leq10\\epsilon}\\end{array}$ , under the event ${\\mathcal{E}}_{1}$ and $x^{a}\\notin\\mathcal{X}_{\\epsilon}$ for every $a\\in A^{\\prime\\prime}$ . Consequently, $\\alpha^{\\star}$ is a feasible solution to LP 5a and provides, under the event ${\\mathcal{E}}_{1}$ , a value of at least $\\mathrm{OPT}\\mathrm{~-~}10\\epsilon n d$ . As a result, under the event ${\\mathcal{E}}_{1}$ the optimal solution of LP 5a has value $v^{\\star}\\geq\\mathrm{OPT}-10\\epsilon n d$ , concluding the proof. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "Lemma 4. Given inputs $\\mathcal{R}_{\\epsilon}:=\\{\\mathcal{R}_{\\epsilon}(a)\\}_{a\\in\\mathcal{A}},\\,\\mathcal{X}_{\\epsilon},$ , and $\\widehat{\\mu}_{t}\\in\\Delta_{\\Theta}$ for Algorithm 3, under events ${\\mathcal{E}}_{1}$ and $\\mathcal{E}_{2}$ , the signaling scheme $\\phi$ output by the algorithm is $O(\\epsilon n d\\!+\\!\\nu)$ -optimal for $\\begin{array}{r}{\\nu\\leq\\left|\\sum_{\\theta\\in\\Theta}\\widehat{\\mu}_{t,\\theta}-\\mu_{\\theta}\\right|}\\end{array}$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. We observe that under the events ${\\mathcal{E}}_{1}$ and $\\mathcal{E}_{2}$ , the collection $\\mathcal{R}_{\\epsilon}=\\{\\mathcal{R}_{\\epsilon}(a)\\}_{a\\in\\mathcal{A}}$ is composed of faces $\\mathcal{R}_{\\epsilon}(a)$ of $\\mathcal{X}_{\\epsilon}(a)$ (possibly the improper face $\\mathcal{X}\\epsilon(\\boldsymbol{a})$ itself) such that every vertex $x\\in\\bar{V}(\\mathcal{X}_{\\epsilon}(a))$ that satisfies $a(x)=a$ belongs to $\\mathcal{R}_{\\epsilon}(a)$ . The following statements hold under these two events. ", "page_idx": 20}, {"type": "text", "text": "As a first step, we prove that given a feasible solution $\\alpha\\,=\\,(\\alpha_{x})_{x\\in\\mathcal{V}_{\\epsilon}}$ to Program 5a, one can construct a feasible solution $\\varphi$ to Program 2 with the same value. In particular, we consider a solution $\\varphi=(\\widetilde{x}^{a})_{a\\in\\mathcal{A}}$ defined as: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\widetilde{x}_{\\theta}^{a}:=\\sum_{x\\in\\mathcal{V}_{\\epsilon}(a)}\\alpha_{x}x_{\\theta}\\quad\\forall a\\in\\mathcal{A},\\theta\\in\\Theta.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We observe that for every $a\\in A$ and $\\theta\\in\\Theta$ we can bound ${\\widetilde{x}}_{\\theta}^{a}$ as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n0\\leq\\widetilde{x}_{\\theta}^{a}=\\sum_{x\\in\\mathcal{V}_{\\epsilon}(a)}\\alpha_{x}x_{\\theta}\\leq\\sum_{x\\in\\mathcal{V}_{\\epsilon}}\\alpha_{x}x_{\\theta}\\leq1,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the last inequality holds due to the constraints of Program 5a. Consequently, the vectors $\\widetilde{x}^{a}$ belong to $\\mathcal{X}^{\\boxed{}}$ . ", "page_idx": 20}, {"type": "text", "text": "Now we show that $\\widetilde{x}^{a}$ belongs to $\\mathcal{R}_{\\epsilon}^{\\square}(a)$ for every $a\\in{\\mathcal{A}}$ . This holds trivially for every action $a\\in{\\mathcal{A}}$ such that $\\textstyle\\sum_{x^{\\prime}\\in\\mathcal{V}_{\\epsilon}(a)}\\alpha_{x^{\\prime}}=0$ , as $\\widetilde{x}^{a}=\\mathbf{0}\\in\\mathcal{R}_{\\epsilon}^{\\Omega}(a)$ . Consider instead an action $a\\in{\\mathcal{A}}$ such that $\\begin{array}{r}{\\sum_{x^{\\prime}\\in\\mathcal{V}_{\\epsilon}(a)}\\alpha_{x^{\\prime}}>\\dot{0}}\\end{array}$ , and let us define the coefficient: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\beta_{x}^{a}:=\\frac{\\alpha_{x}}{\\sum_{x^{\\prime}\\in\\mathcal{V}_{\\epsilon}(a)}\\alpha_{x^{\\prime}}},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for every vertex $x\\in\\mathcal{V}_{\\epsilon}(a)$ . One can easily verify that $\\beta^{a}\\in\\Delta_{\\nu_{\\epsilon}(a)}$ . Now consider the normalized slice: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\widetilde{x}^{\\mathrm{N},a}:=\\sum_{x\\in\\mathcal{V}_{\\epsilon}(a)}\\beta_{x}^{a}x.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This slice belongs to $\\mathcal{R}_{\\epsilon}^{\\triangle}(a):=\\mathcal{R}_{\\epsilon}(a)$ , as it is the weighted sum of the vertices $\\mathcal{V}_{\\epsilon}(a)\\subseteq V(\\mathcal{R}_{\\epsilon}^{\\triangle}(a))$ with weights $\\beta^{a}\\in\\Delta_{\\nu_{\\epsilon}(a)}$ . Furthermore, we can rewrite the component $\\widetilde{x}^{a}$ of the solution $\\phi$ as: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\widetilde x^{a}=\\widetilde x^{\\mathrm{N},a}\\sum_{x\\in\\mathcal V_{\\epsilon}(a)}\\alpha_{x}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus, by considering that $\\widetilde{x}^{\\mathrm{N},a}\\in\\mathcal{R}_{\\epsilon}^{\\triangle}(a)$ and $\\widetilde{x}^{a}\\in\\mathcal{X}^{\\sqcup}$ , we have that $\\widetilde{x}^{a}\\in\\mathcal{R}_{\\epsilon}^{\\Pi}(a)$ . ", "page_idx": 20}, {"type": "text", "text": "Finally, since $\\alpha$ is a feasible solution for Program 5a, we can observe that: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{a\\in{\\mathcal{A}}}{\\widetilde{x}}_{\\theta}^{a}=\\sum_{a\\in{\\mathcal{A}}}\\sum_{x\\in\\mathcal{V}_{\\epsilon}(a)}\\alpha_{x}x=\\sum_{x\\in\\mathcal{V}_{\\epsilon}}\\alpha_{x}x\\leq1.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "As a result, $\\varphi=(\\widetilde{x}^{a})_{a\\in\\mathcal{A}}$ is a feasible solution to Program 2. ", "page_idx": 20}, {"type": "text", "text": "If the estimator $\\widehat{\\mu}_{t}$ coincides with the exact prior $\\mu$ , then direct calculations show that the solution $\\varphi$ to Program 2 ac h ieves the same value of the solution $\\alpha$ to Program 5a. ", "page_idx": 20}, {"type": "text", "text": "It follows that, when $\\widehat{\\mu}_{t}=\\mu$ , the optimal solution of Program 2 has at least the same value of the optimal solution of Program 5a. ", "page_idx": 20}, {"type": "text", "text": "In order to conclude the proof, we provide a lower bound on the utility of the signaling scheme computed by Algorithm 3. Let $\\phi^{\\mathrm{LP}}$ be the signaling scheme computed by Algorithm 3, while let $\\Psi=\\mathring{(}x^{\\mathrm{LP},a}\\big)_{a\\in\\mathcal{A}}$ be the optimal solution to Program 2. Furthermore, we let $\\boldsymbol{\\psi}=(\\boldsymbol{x}^{\\mathrm{E},a})_{a\\in\\mathcal{A}}$ be the optimal solution of Program 2 and $\\phi^{\\mathrm{E}}$ the signaling scheme computed by Algorithm 3 when the prior estimator coincides exactly with the prior itself, i.e., $\\widehat{\\mu}:=\\widehat{\\mu}_{t}=\\widehat{\\mu}$ . ", "page_idx": 20}, {"type": "text", "text": "Since $x^{\\mathrm{LP},a}\\in\\mathcal{R}_{\\epsilon}^{\\Omega}(a)$ for every $a\\in A$ , we have that $a\\in\\mathcal{A}^{\\phi^{\\mathrm{LP}}}(a)$ .9 Breaking ties in favor of the sender, the action $a^{\\phi^{\\mathrm{LP}}}(s^{a})$ is such that: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{\\theta\\in\\Theta}\\mu_{\\theta}x_{\\theta}^{\\mathrm{{LP}},a}u_{\\theta}^{\\mathrm{s}}(a^{\\phi^{\\mathrm{LP}}}(s^{a}))\\geq\\sum_{\\theta\\in\\Theta}\\mu_{\\theta}x_{\\theta}^{\\mathrm{{LP}},a}u_{\\theta}^{\\mathrm{s}}(a).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{u(\\phi^{D})=\\displaystyle\\sum_{s\\in A}\\displaystyle\\sum_{i=0}^{n}\\mu_{i}u_{j}^{\\alpha_{D}}v_{i}^{\\alpha_{D}}(a^{\\alpha^{D}}(s))}\\\\ &{\\geq\\displaystyle\\sum_{s\\in A}\\displaystyle\\sum_{i=1}^{n}\\mu_{i}u_{j}^{\\alpha_{D}}v_{i}^{\\alpha_{D}}(a^{\\alpha^{D}}(s))}\\\\ &{=\\displaystyle\\sum_{s\\in A}\\displaystyle\\sum_{s\\in A}\\mu_{i}u_{j}^{\\alpha_{D}}v_{i}^{\\alpha_{D}}(a^{\\alpha^{D}}(s))}\\\\ &{=\\displaystyle\\sum_{s\\in A}\\displaystyle\\sum_{s\\in A}\\mu_{i}u_{j}^{\\alpha_{D}}v_{i}^{\\alpha_{D}}u_{i}^{\\alpha_{D}}(a^{\\alpha^{D}}(s))}\\\\ &{\\geq\\displaystyle\\sum_{s\\in A}\\displaystyle\\sum_{s\\in A}\\mu_{i}u_{j}^{\\alpha_{D}}v_{i}^{\\alpha_{D}}u_{i}^{\\alpha_{D}}(a)}\\\\ &{\\geq\\displaystyle\\sum_{s\\in A}\\displaystyle\\sum_{s\\in A}\\mu_{i}u_{j}^{\\alpha_{D}}v_{i}^{\\alpha_{D}}u_{i}^{\\alpha_{D}}(a)-\\displaystyle\\left\\lvert\\sum_{s\\in A}\\hat{\\mu}_{i}u_{j}^{\\alpha_{D}}\\right\\rvert}\\\\ &{\\geq\\displaystyle\\sum_{s\\in A}\\displaystyle\\sum_{s\\in A}\\hat{\\mu}_{i}u_{j}^{\\alpha_{D}}u_{i}^{\\alpha_{D}}(a)-\\displaystyle\\left\\lvert\\sum_{s\\in A}\\hat{\\mu}_{i}u_{j}^{\\alpha_{D}}\\right\\rvert}\\\\ &{\\geq\\displaystyle\\sum_{s\\in A}\\displaystyle\\sum_{s\\in B}\\mu_{i}u_{j}^{\\alpha_{D}}v_{i}^{\\alpha_{D}}u_{i}^{\\alpha_{D}}(a)-2\\displaystyle\\sum_{s\\in A}\\mu_{i}u_{j}^{\\alpha_{D}}}\\\\ &{\\geq\\displaystyle\\sum_{s\\in A}\\displaystyle\\sum_{s\\in B}\\mu_{i}u_{j}^{\\alpha_{D}}u_{i}^{\\alpha_{D}}(a)-2\\displaystyle\\sum_{s\\in B}\\hat{\\mu}_{i}u_{j}^{\\alpha_{D}}\\displaystyle\\sum_{s\\in B}\\hat{\\mu}_{i}u_{j}^{\\alpha_{D}}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We recall that the value of $\\psi=(x^{\\mathrm{E},a})_{a\\in\\mathcal{A}}$ is at least the optimal value of Program 5a when $\\widehat{\\mu}=\\mu$ , and such a value is at least $v^{\\star}\\geq\\mathrm{OPT}-10\\epsilon n\\partial$ according to Lemma 7, Thus, we have that: ", "page_idx": 21}, {"type": "equation", "text": "$$\nu(\\phi^{\\mathrm{LP}})\\geq\\sum_{a\\in A}\\sum_{\\theta\\in\\Theta}\\mu_{\\theta}x_{\\theta}^{\\mathrm{E},a}u_{\\theta}^{\\mathrm{s}}(a)-2\\left|\\sum_{\\theta\\in\\Theta}\\widehat{\\mu}_{\\theta}-\\mu_{\\theta}\\right|\\geq\\mathrm{OPT}-10\\epsilon n d-2\\left|\\sum_{\\theta\\in\\Theta}\\widehat{\\mu}_{\\theta}-\\mu_{\\theta}\\right|,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "concluding the proof. ", "page_idx": 21}, {"type": "text", "text": "E Omitted proofs and sub-procedures of Phase 2 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "E.1 Action-Oracle ", "page_idx": 21}, {"type": "text", "text": "The goal of the Action-Oracle procedure (Algorithm 5) is to assign the corresponding best-response to a slice $x\\in\\mathcal X_{\\epsilon}$ received as input. In order to do so, it repeatedly commits to a signaling scheme $\\phi$ such that $x$ is the slice of $\\phi$ with respect to the signal $s_{1}$ . When the signal $s_{1}$ is sampled, the procedure returns the best-response $a(x)$ . ", "page_idx": 21}, {"type": "text", "text": "Algorithm 5 Action-Oracle ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Require: $x\\in\\mathcal{X}_{\\epsilon}$   \n1: $\\phi_{\\theta}(s_{1})\\gets x_{\\theta}$ and $\\phi_{\\theta}(s_{2})\\leftarrow1-x_{\\theta}\\:\\forall\\theta\\in\\Theta$   \n2: do   \n3: Commit to $\\phi^{t}=\\phi$ , observe $\\theta^{t}$ , and send $s^{t}$   \n4: Observe feedback $a^{t}$   \n5: $a\\gets a^{t}$   \n6: while $s^{t}\\neq s_{1}$   \n7: return $a$ ", "page_idx": 21}, {"type": "text", "text": "In the following, for the sake of analysis, we introduce the definition of a clean event under which the Action-Oracle procedure always returns the action $a(x)\\in A$ , as formally stated below. ", "page_idx": 21}, {"type": "text", "text": "Definition 4 (Clean event of Action-Oracle). We denote $\\mathcal{E}^{\\mathrm{a}}$ as the event in which Algorithm 5 correctly returns the follower\u2019s best response $a(x)$ whenever executed. ", "page_idx": 21}, {"type": "text", "text": "In the proof of Lemma 3 we show that, thanks to Lemma 2 and the definition of $\\textstyle{\\mathcal{X}}_{\\epsilon}$ , it is possible to bound the number of rounds required to Algorithm 5 to ensure that it always returns the best response $a(x)\\in A$ with high probability. ", "page_idx": 21}, {"type": "table", "img_path": "XNpVZ8E1tY/tmp/776040daa923d6bcbfd7ca9bc4294de85ccf99597a3732d60c204f5c016259fa.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Algorithm 6 can be divided in two parts. First, by means of Algorithm 7, it computes the polytopes $\\bar{\\mathcal{R}_{\\epsilon}}(a)\\,=\\,\\mathcal{X}_{\\epsilon}(a)$ with volume strictly larger than zero. This procedure is based on the algorithm developed by Bacchiocchi et al. [2024a] for Stackelberg games. The main differences are the usage of Action-Oracle to query a generic normalized slice, and some technical details to account for the shape of the search space $\\textstyle{\\mathcal{X}}_{\\epsilon}$ . ", "page_idx": 22}, {"type": "text", "text": "In the second part (loop at Line 2 Algorithm 6), it finds, for every polytope $\\mathcal{X}_{\\epsilon}(a)$ with null volume, a face $\\mathcal{R}_{\\epsilon}(a)$ such that $\\mathcal{V}_{\\epsilon}(a)\\subseteq\\mathcal{R}_{\\epsilon}(a)$ . Let us remark that $\\mathcal{R}_{\\epsilon}(a)$ could be the improper face $\\mathcal{X}_{\\epsilon}(a)$ itself, and it is empty if $\\mathcal{V}_{\\epsilon}(a)=\\emptyset$ . ", "page_idx": 22}, {"type": "text", "text": "Lemma 3. Given inputs $\\mathcal{X}_{\\epsilon}\\subseteq\\mathcal{X}$ and $\\zeta\\in(0,1)$ for Algorithm $\\delta,$ , let $L:=B+B_{\\epsilon}+B_{\\hat{\\mu}}$ , where $B$ , $B_{\\epsilon}$ , and $B_{\\widehat{\\mu}}$ denote the bit-complexity of numbers $\\mu_{\\theta}u_{\\theta}(a_{i})$ , $\\epsilon,$ and ${\\widehat{\\mu}},$ , respectively. Then, under event ${\\mathcal{E}}_{1}$ and with  at probability at least $1-\\zeta$ , Algorithm $6$ outputs a collection $\\mathcal{R}_{\\epsilon}:=\\{\\mathcal{R}_{\\epsilon}(a)\\}_{a\\in\\mathcal{A}}$ , where $\\mathcal{R}_{\\epsilon}(a)$ is a (possibly improper) face of $\\mathcal{X}_{\\epsilon}(a)$ such that $\\bar{\\mathcal{V}}_{\\epsilon}(a)\\subseteq\\mathcal{X}_{\\epsilon}(a)$ , in a number of rounds $T_{2}$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\nT_{2}\\leq\\tilde{\\mathcal{O}}\\left(\\frac{n^{2}}{\\epsilon}\\log^{2}\\left(\\frac{1}{\\zeta}\\right)\\left(d^{7}L+\\binom{d+n}{d}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. In the following we let $L=B+B_{\\epsilon}+B_{\\widehat{\\mu}}$ . ", "page_idx": 22}, {"type": "text", "text": "As a first step, Algorithm 6 invokes the procedure Find-Fully-Dimensional-Regions. Thus, according to Lemma 8, under the event $\\mathcal{E}^{\\mathrm{a}}$ , with probability at least $1-\\zeta/2$ , Algorithm 6 computes every polytope $\\mathcal{X}_{\\epsilon}(a)$ with volume larger than zero by performing at most: ", "page_idx": 22}, {"type": "equation", "text": "$$\nC_{1}=\\widetilde{\\mathcal{O}}\\left(n^{2}\\left(d^{7}L\\log(1/\\varsigma)+\\binom{d+n}{d}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "calls to the Action-Oracle procedure. Together with these polytopes, it computes the set ${\\mathcal{C}}\\subseteq A$ containing the actions $a$ such that $\\operatorname{vol}(\\mathcal{X}_{\\epsilon}(a))>0$ . ", "page_idx": 22}, {"type": "text", "text": "Subsequently, Algorithm 6 employs the procedure Find-Face at most $n$ times and, according to Lemma 12, it computes the polytopes $\\mathcal{R}_{\\epsilon}(a_{j})$ for every $a_{j}\\notin{\\mathcal{C}}$ . Overall, this computation requires: ", "page_idx": 22}, {"type": "equation", "text": "$$\nC_{2}=n^{2}{\\binom{d+n}{d}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "calls to the Action-Oracle procedure. Thus, under the event $\\mathcal{E}^{\\mathrm{a}}$ and with probability at least $1-\\zeta/2$ , Algorithm 6 correctly computes the polytopes $\\mathcal{R}_{\\epsilon}(a_{j})$ for every action $a_{j}\\in A$ . Furthermore, the number of calls $C\\geq0$ to the Action-Oracle procedure can be upper bounded as: ", "page_idx": 22}, {"type": "equation", "text": "$$\nC:=C_{1}+C_{2}\\leq\\widetilde{\\mathcal{O}}\\left(n^{2}\\left(d^{7}L\\log(1/\\zeta)+\\binom{d+n}{d}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Moreover, by setting $\\rho=\\zeta/2C$ , and thanks to Lemma 2, with a probability of at least $1-\\rho$ , every execution of Action-Oracle requires at most $N\\geq0$ rounds, where $N$ can be bounded as follows: ", "page_idx": 22}, {"type": "equation", "text": "$$\nN\\leq\\mathcal{O}\\left(\\frac{\\log(1/\\rho)}{\\epsilon}\\right)=\\mathcal{O}\\left(\\frac{1}{\\epsilon}\\log\\left(\\frac{2C}{\\zeta}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Consequently, since the number of calls to the Action-Oracle procedure is equal to $C$ , by employing a union bound, the probability that each one of these calls requires $N$ rounds to terminate is greater than or equal to: ", "page_idx": 22}, {"type": "equation", "text": "$$\n1-C\\rho=1-\\frac{\\zeta}{2C}C=1-\\frac{\\zeta}{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "To conclude the proof, we employ an union bound over the probability that every execution of Action-Oracle terminates in at most $N$ rounds, and the probability that Algorithm 6 performs $C$ calls to the Action-Oracle procedure. Since the probability that each one of these two events hold is at least $1-\\zeta/2$ , with probability at least $1-\\zeta$ , Algorithm 6 correctly terminates by using a number of samples of the order: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\widetilde{\\mathcal{O}}\\left(\\frac{C}{\\epsilon}\\log\\left(\\frac{2C}{\\zeta}\\right)\\right)\\leq\\widetilde{\\mathcal{O}}\\left(\\frac{n^{2}}{\\epsilon}\\log^{2}\\left(\\frac{1}{\\zeta}\\right)\\left(d^{7}L+\\binom{d+n}{d}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "concluding the proof. ", "page_idx": 23}, {"type": "text", "text": "E.3 Find-Fully-Dimensional-Regions ", "page_idx": 23}, {"type": "text", "text": "Algorithm 7 Find-Fully-Dimensional-Regions   \nRequire: Search space $\\mathcal{X}_{\\epsilon}\\subseteq\\mathcal{X}$ , parameter $\\delta\\in(0,1)$   \n1: $\\delta\\leftarrow\\zeta/2n^{2}(2(d+n)+n)$   \n2: ${\\mathcal{C}}\\gets{\\mathcal{D}}$   \n3: while 4: $\\begin{array}{r l}&{\\mathrm{uet~}\\cup_{a\\ne c}c\\cup\\left(a\\right)\\nearrow\\infty.}\\\\ &{\\boldsymbol{x}^{\\mathrm{im}}\\leftarrow\\mathrm{Sample~apoint~from~int}\\left(\\mathcal{X}_{\\epsilon}\\setminus\\bigcup_{a_{k}\\in C}{\\mathcal{U}}(a_{k})\\right)}\\\\ &{\\ a_{j}\\leftarrow\\mathrm{Action-oracle~}(x^{\\mathrm{im}})}\\\\ &{\\mathcal{U}(a_{j})\\leftarrow\\mathcal{X}_{\\epsilon}}\\\\ &{\\ B_{x}\\leftarrow\\mathrm{Bircomplexity~of~}x^{\\mathrm{im}}}\\\\ &{\\lambda\\leftarrow d^{2-4/{(B_{x}+4(B+B_{k}+B_{l}))}-1}}\\\\ &{\\mathrm{for~}\\mathrm{all~}v\\in V\\left({\\mathcal{U}}(a_{j})\\right)\\equiv0}\\\\ &{\\quad\\ x\\leftarrow\\lambda x^{\\mathrm{im}}+(1-\\lambda)v}\\\\ &{\\ a\\leftarrow\\mathrm{Action-oracle}(x)}\\\\ &{\\mathrm{~if~}a\\ne\\ a_{j}\\mathrm{~fleq~inen~}}\\\\ &{\\ H_{j k}\\leftarrow\\mathrm{Find-prerplane}(a_{j},{\\mathcal{U}}(a_{j}),x^{\\mathrm{im}},v,\\delta}\\\\ &{\\quad\\,\\,\\,\\mu(a_{j})\\leftarrow{\\mathcal{U}}(a_{j})\\cap{\\mathcal{U}}_{j k}}\\end{array}$ $\\begin{array}{r}{\\bigcup_{a_{j}\\in{\\mathcal{C}}}\\mathcal{U}(a_{j})\\neq\\mathcal{X}_{\\epsilon}}\\end{array}$ do   \n5:   \n6:   \n7:   \n8:   \n9:   \n10:   \n11:   \n12:   \n13: )   \n14:   \n15: else   \n16: restart the for-loop at Line 9   \n17: $\\begin{array}{r l}&{\\mathcal{C}\\leftarrow\\mathcal{C}\\cup\\{a_{j}\\}}\\\\ &{\\mathcal{X}_{\\epsilon}(a_{j})\\leftarrow\\mathcal{U}(a_{j})}\\\\ &{\\mathbf{urn}\\,\\mathcal{C},\\{\\mathcal{X}_{\\epsilon}(a_{j})\\}_{a_{j}\\in\\mathcal{C}}}\\end{array}$   \n18:   \n19: re ", "page_idx": 23}, {"type": "text", "text": "At a high level, Algorithm 7 works by keeping track of a set ${\\mathcal{C}}\\subseteq A$ of closed actions, meaning that the corresponding polytope $\\mathcal{X}_{\\epsilon}(a)$ has been completely identified. First, at Line 4, Algorithm 7 samples at random a normalized slice $x^{\\mathrm{int}}$ from the interior of one of the polytopes $\\mathcal{X}_{\\epsilon}(a_{j})$ that have not yet been closed and queries it, observing the best-response $a_{j}\\,\\in\\,A$ . Then, it initializes the entire $\\textstyle{\\mathcal{X}}_{\\epsilon}$ as the upper bound $\\mathcal{U}(a_{j})$ of the region $\\mathcal{X}_{\\epsilon}(a_{j})$ . As a further step, to verify whether the upper bound $\\mathcal{U}(a_{j})$ coincides with $\\mathscr{X}_{\\epsilon}(a_{j})$ , Algorithm 7 queries at Line 11 one of the vertices of the upper bound $\\mathcal{U}(\\bar{a_{j}})$ . Since the same vertex may belong to the intersection of multiple regions, the Action-Oracle procedure is called upon an opportune convex combination of $x^{\\mathrm{int}}$ and the vertex $v$ itself (Line 10). If the vertex does not belong to $\\mathcal{X}_{\\epsilon}(a_{j})$ , then a new separating hyperplane can be computed (Line 13) and the upper bound $\\mathcal{U}(a_{j})$ is updated accordingly. In this way, the upper bound $\\bar{\\mathcal{U}}(a_{j}\\bar{\\mathbf{\\rho}})$ is refined by finding new separating hyperplanes until it coincides with the polytope $\\mathcal{X}_{\\epsilon}(a_{j})$ . Finally, such a procedure is iterated for all the receiver\u2019s actions $a_{j}\\in A$ such that $\\mathrm{vol}(\\mathcal{X}_{\\epsilon}(a_{j}))>\\bar{0}$ , ensuring that all actions corresponding to polytopes with volume larger than zero are closed. ", "page_idx": 23}, {"type": "text", "text": "We observe that the estimator ${\\widehat{\\mu}}_{t}$ is updated during the execution of Algorithm 7 according to the observed states. However, let  u s remark that the search space $\\begin{array}{r}{\\mathcal{X}_{\\epsilon}=\\left\\{\\bar{x^{\\mathrm{~\\}}}\\in\\mathcal{X}\\;\\vert\\;\\sum_{\\theta\\in\\widetilde{\\Theta}}\\widehat{\\mu}_{\\theta}x_{\\theta}^{\\mathsf{~\\biggr~}}\\geq2\\epsilon\\right\\}}\\end{array}$ does not change during the execution of this procedure. ", "page_idx": 23}, {"type": "text", "text": "Lemma 8. Given in input $\\mathcal{X}_{\\epsilon}\\subseteq\\mathcal{X}$ and $\\zeta\\in(0,1)$ , then under the event $\\mathcal{E}^{\\mathrm{a}}$ with probability at least $1-\\zeta/2$ Algorithm 7 computes the collection of polytopes $\\{\\mathcal{X}_{\\epsilon}(a_{j})\\}_{a_{j}\\in\\mathcal{C}}$ with volume larger than zero, and the corresponding set of actions $\\mathcal{C}$ . Furthermore, it employs at most: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\widetilde{\\mathcal{O}}\\left(n^{2}\\left(d^{7}L\\log(1/\\varsigma)+\\binom{d+n}{d}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. Thanks to Lemma 9 and Lemma 10, with an approach similar to the one proposed in Theorem 4.3 by Bacchiocchi et al. [2024a], we can prove that, under the event $\\mathcal{E}^{\\mathrm{a}}$ , with probability at least $1-\\delta n^{2}(\\dot{2}(d+n)^{2}+n)$ , Algorithm 7 computes every polytope $\\mathcal{X}_{\\epsilon}(a)$ with volume larger than zero by performing at most: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{O}\\left(n^{2}\\left(d^{7}L\\log(1/\\delta)+\\binom{d+n}{d}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "calls to the Action-Oracle procedure. Together with these polytopes, it computes the set ${\\mathcal{C}}\\subseteq A$ containing the actions $a$ such that $\\mathrm{vol}(\\mathcal{X}_{\\epsilon}(a)\\bar{)}>0$ . ", "page_idx": 24}, {"type": "text", "text": "Furthermore, we observe that $\\zeta=2\\delta n^{2}(2(d+n)+n)$ , as defined at Line 1 in Algorithm 7. As a result, under the event $\\mathcal{E}^{\\mathrm{a}}$ , with probability at least $1-\\zeta/2$ , the number of calls $C_{1}\\geq0$ performed by Algorithm 7 to the Action-Oracle procedure can be bounded as follows: ", "page_idx": 24}, {"type": "equation", "text": "$$\nC_{1}\\leq\\tilde{\\mathcal{O}}\\left(n^{2}\\left(d^{7}L\\log(1/\\varsigma)+\\binom{d+n}{d}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "concluding the proof. ", "page_idx": 24}, {"type": "text", "text": "E.4 Find-Hyperplane ", "page_idx": 24}, {"type": "table", "img_path": "XNpVZ8E1tY/tmp/3b61bf8bf8223cac02e71e7da84ab8c068820794b0208c21b01cfd2fb2b0cbea.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "The goal of the Find-Hyperplane procedure is to compute a new separating hyperplane $H_{j k}$ between a given region $\\mathcal{X}_{\\epsilon}(a_{j})$ and some other polytope $\\mathcal{X}_{\\epsilon}(a_{k})$ , with $a_{j},a_{k}\\,\\in{\\mathcal{A}}$ . To do so, it receives as input an upper bound $\\mathcal{U}(a_{j})$ of some polytope $\\mathcal{X}_{\\epsilon}(a_{j})$ , an interior point $x^{\\mathrm{int}}\\in\\operatorname{int}(\\mathcal{U}(a_{j}))$ , a vertex $v\\in\\mathbf{V}(\\mathcal{U}(a_{j}))$ that does not belong to $\\mathcal{X}_{\\epsilon}(a_{j})$ , and a parameter $\\delta>0$ as required by the Sample-Int procedure. As a first step, Algorithm 8 samples at random a slice $x$ from the interior of the upper bound $\\mathcal{U}(a_{j})$ . Subsequently, it performs a binary search on the segment between $x$ and either $v$ or $x^{\\mathrm{int}}$ , depending on the best response $a(x)$ in $x$ . This binary search returns a point $x^{\\circ}$ on some new separating hyperplane $H_{j k}$ (Line 7). As a further step, the algorithm computes two sets of normalized slices, $S_{j}\\,\\subseteq\\,\\mathcal{X}_{\\epsilon}(a_{j})$ and $S_{k}\\,\\subseteq\\,\\mathcal{X}_{\\epsilon}(a_{k})$ . Finally, it performs $d-1$ binary searches between different couples of points, one in ${\\mathbf{}}S_{j}$ and the other in $\\scriptstyle S_{k}$ , in order to completely identify the separating hyperplane. ", "page_idx": 24}, {"type": "text", "text": "Lemma 9. With probability at least $1\\!-\\!(d\\!+\\!n)^{2}\\delta,$ , under the event $\\mathcal{E}^{\\mathrm{a}}$ Algorithm 8 returns a separating hyperplane $H_{j k}$ by using $\\mathcal{O}(d^{7}(B+B_{\\epsilon}+B_{\\widehat{\\mu}})+d^{4}\\log(^{1/\\delta}))$ calls to Algorithm 5. ", "page_idx": 24}, {"type": "text", "text": "Proof. We observe that, with the same analysis provided in Lemma 4.7 by Bacchiocchi et al. [2024a], we can prove that, under the event $\\mathcal{E}^{\\mathrm{a}}$ , the binary-search procedure described in Algorithm 9 correctly computes a point on a separating hyperplane by calling the Action-Oracle procedure at most $\\mathcal{O}(d(B_{x}+B))$ times. ", "page_idx": 25}, {"type": "text", "text": "With the same reasoning applied in Lemma 4.4 and 4.5 by Bacchiocchi et al. [2024a], we can prove that with probability at least $\\dot{1}-(d\\!+\\!n)^{2}\\delta$ , under the event $\\mathcal{E}^{\\mathrm{a}}$ , the points $x^{+i}$ are linearly independent and do not belong to $H_{j k}$ . To conclude the proof, we have to show that every $x^{+i}$ belongs to either $\\mathcal{X}_{\\epsilon}(a_{j})$ or $\\mathcal{X}_{\\epsilon}(a_{k})$ . This is because, if the previous condition holds, under the event $\\mathcal{E}^{\\mathrm{a}}$ , Algorithm 8 correctly computes a new separating hyperplane with probability at least $1-(d+n)^{2}\\delta$ . ", "page_idx": 25}, {"type": "text", "text": "To do that, we show that the constant $\\alpha$ defined at Line 8 in Algorithm 8 is such that all the points $x^{+i}$ and $x^{-i}$ either belong to $\\mathcal{X}_{\\epsilon}(a_{j})$ or $\\mathcal{X}_{\\epsilon}(a_{k})$ , given that $x^{\\circ}$ belongs to the hyperplane between these polytopes. With an argument similar to the one proposed in Bacchiocchi et al. [2024a], the distance between $x^{i}$ and any separating hyperplane can be lower bounded by $2^{-d(B_{x}+4B)}$ , where $B_{x}$ is the bit-complexity of $x^{\\circ}$ . ", "page_idx": 25}, {"type": "text", "text": "Similarly, the distance between $x^{\\circ}$ and the hyperplane $\\widehat{H}=\\{x\\in\\mathbb{R}^{d}\\ |\\ \\sum_{\\theta\\in\\widetilde{\\Theta}}\\widehat{\\mu}_{\\theta}x_{\\theta}\\geq2\\epsilon\\}$ can be lower bounded as follows: ", "page_idx": 25}, {"type": "equation", "text": "$$\nd(x^{\\circ},\\widehat{H})=\\frac{\\left|\\sum_{\\theta\\in\\widetilde{\\Theta}}x_{\\theta}\\widehat{\\mu}_{\\theta}+2\\epsilon\\right|}{\\sqrt{\\sum_{\\theta\\in\\widetilde{\\Theta}}\\widehat{\\mu}_{\\theta}^{2}}}\\geq\\frac{1}{d2^{3d(B_{x}+B_{\\widehat{\\mu}}+B_{\\epsilon})}},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $B_{\\widehat{\\mu}}$ is the bit-complexity of $\\widehat{\\mu}$ . The inequality follows by observing that the denominator of the fraction a bove is at most $d$ , while  t o lower bound the numerator we define the following quantities: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sum_{\\theta\\in\\widetilde{\\Theta}}x_{\\theta}\\widehat{\\mu}_{\\theta}=\\frac{\\alpha}{\\beta}\\quad\\mathrm{and}\\quad\\epsilon=\\frac{\\gamma}{\\nu},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\alpha$ and $\\beta$ are integers numbers, while $\\gamma$ and $\\nu$ are natural numbers. Thus, the numerator $\\begin{array}{r}{\\left|\\sum_{\\theta\\in\\widetilde{\\Theta}}x_{\\theta}\\widehat{\\mu}_{\\theta}+2\\epsilon\\right|}\\end{array}$ of the fraction defined in Equation 7 can be lower bounded as follows: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left|\\sum_{\\theta\\in\\tilde{\\Theta}}x_{\\theta}\\widehat{\\mu}_{\\theta}+2\\epsilon\\right|=\\left|\\frac{\\alpha\\nu+2\\beta\\gamma}{\\beta\\nu}\\right|\\geq\\left|\\frac{1}{\\beta\\nu}\\right|\\geq2^{-3d(B_{x}+B_{\\widehat{\\mu}}+B_{\\epsilon})},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the last inequality follows from the fact that the bit-complexity of $\\nu$ is at most $B_{\\epsilon}$ , while the bit-complexity of $\\beta$ cannot exceed $3d(B_{x}+B_{\\widehat{\\mu}})$ as stated by Lemma D.1 of Bacchiocchi et al. [2024a]. ", "page_idx": 25}, {"type": "text", "text": "Overall, the distance between $x^{\\circ}$ and the boundary of the polytope $\\mathcal{X}_{\\epsilon}(a_{j})\\cap\\mathcal{X}_{\\epsilon}(a_{k})$ is strictly larger than $\\alpha:=2^{-4d(B_{x}+B+B_{\\widehat{\\mu}}+B_{\\epsilon})-\\log_{2}(d)}$ . Thus, every signaling scheme $x^{+i}$ and $x^{-i}$ belongs to $\\textstyle{\\mathcal{X}}_{\\epsilon}$ and either $\\mathcal{X}(a_{j})$ or $\\mathcal{X}(a_{k})$ . ", "page_idx": 25}, {"type": "text", "text": "Finally, we observe that the bit-complexity of $x$ is bounded by $B_{x}=\\mathcal{O}(d^{3}(B+B_{\\epsilon}+B_{\\widehat{\\mu}})+\\log(1/\\delta))$ , as stated by Lemma 10. Thus, the first binary-search requires $\\mathcal{O}(d(B_{x}+B))=\\mathcal{O}(d^{4}L+d\\log(1/\\delta))$ calls to Action-Oracle, where $L=B+B_{\\epsilon}+B_{\\widehat{\\mu}}$ . ", "page_idx": 25}, {"type": "text", "text": "Furthermore, the bit-complexity of $x^{\\circ}$ is bounded by $\\mathcal{O}(d^{4}L+d\\log(1/\\delta))$ . As a result, the bitcomplexity of the slices $x^{+i}$ and ${\\dot{x}}^{-i}$ is bounded by $\\mathcal{O}(d^{5}L\\!+\\!d\\log(^{1}\\!/\\delta))$ , given that the bit-complexity of $\\alpha$ is $\\dot{\\cal O}(d L)$ . It follows that each binary search between two points in ${\\mathcal{S}}_{j}$ and $\\ensuremath{\\boldsymbol{S}}_{k}$ requires $\\mathcal{O}(d^{6}L+d^{2}\\log(1/\\delta))$ calls to Action-Oracle. ", "page_idx": 25}, {"type": "text", "text": "Overall, Algorithm 8 invokes the Action-Oracle procedure at most $\\mathcal{O}(d^{7}L+d^{4}\\log(1/\\delta))$ times, accounting for the $d-1$ binary searches, concluding the proof. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "E.5 Binary-Search ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The Binary-Search procedure performs a binary search on the segment connecting two points, $x^{1},x^{2}\\in\\bar{\\mathcal{X}}_{\\epsilon}$ such that $\\dot{x}^{1}\\in\\mathcal{X}_{\\epsilon}(\\dot{a_{j}})$ and $x^{2}\\notin\\mathcal{X}_{\\epsilon}(\\dot{a}_{j})$ for some $a_{j}\\in A$ , in order to find a point $x^{\\circ}$ on some separating hyperplane $H_{j k}$ . At each iteration, the binary search queries the middle point of the segment. Depending on the receiver\u2019s best-response in such a point, it keeps one of the two halves of the segment for the subsequent iteration. The binary search ends when the segment is sufficiently small, so that it contains a single point with a bit-complexity appropriate for a point that lies on both the hyperplane $H_{j k}$ and the segment connecting ${\\mathrm{\\Omega}}_{x}{^{1}}$ and $x^{\\frac{\\imath}{2}}$ . Such a point can be found traversing the Stern-Brocot-Tree. For an efficient implementation, see Fori\u0161ek [2007]. Overall, Algorithm 9 performs $\\mathcal{O}(d(B_{x}+B))$ calls to the Action-Oracle procedure, and returns a normalized slice $x^{\\circ}$ with bit-complexity bounded by $\\mathcal{O}(d(B_{x}+B))$ , where $B_{x}$ is the bit-complexity of the points $x^{1}$ and $x^{2}$ . ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "table", "img_path": "XNpVZ8E1tY/tmp/72b1c539d44ada87e6902653fa667394bca3aeb4415f791da8cd3ccfea622b04.jpg", "table_caption": [], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "E.6 Sample-Int ", "page_idx": 26}, {"type": "table", "img_path": "XNpVZ8E1tY/tmp/78a23ab1d074fede8d82031b36ca4b4fd247d15103cacb937749849592bc314f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "The Sample-Int procedure (Algorithm 10) samples at random a normalized slice from the interior of a given polytope $\\mathcal{P}$ . We observe that each polytope Algorithm 7 is required to sample from is defined as the intersection of $\\textstyle{\\mathcal{X}}_{\\epsilon}$ with some separating half-spaces as the ones defined in Section 3. This procedure provides theoretical guarantees both on the bit-complexity of the point $x$ being sampled and on the probability that such a point belongs to a given hyperplane. Furthermore, it can be easily modified to sample a point from a facet of the simplex $\\mathcal{X}=\\Delta_{d}$ (intuitively, this is equivalent to sample a point from $\\Delta_{d-1}$ ). As a first step, Algorithm 10 computes a normalized slice $x^{\\diamond}$ in the interior of $\\mathcal{P}$ (Line 2). Subsequently, it samples randomly a vector $y$ from a suitable grid belonging to the $(d-1)$ -dimensional hypercube with edges of length 2. As a further step, it sums each component $x_{i}^{\\diamond}$ of the normalized slice $x^{\\diamond}$ with the corresponding component $y_{i}$ of $y$ , scaled by an opportune factor $\\rho$ , where the constant $\\rho$ is defined to ensure that $x$ belongs to the interior of $\\mathcal{P}$ . ", "page_idx": 26}, {"type": "text", "text": "The theoretical guarantees provided by Algorithm 10 are formalized in the following lemma: ", "page_idx": 26}, {"type": "text", "text": "Lemma 10. Given a polytope $\\mathcal P\\ \\subseteq\\ \\mathcal X_{\\epsilon}\\ :\\ \\mathrm{vol}_{d-1}(\\mathcal P)\\ >\\ \\ 0$ defined by separating or boundary hyperplanes, Algorithm $I O$ computes $x\\in\\operatorname{int}(\\mathcal{P})$ such that, for every linear space $H\\subset\\mathbb{R}^{d}:{\\mathcal{P}}\\not\\subseteq H$ of dimension at most $d-1$ , the probability that $x\\in H$ is at most $\\delta$ . Furthermore, the bit-complexity of $x$ is $\\mathcal{O}(d^{3}(B+B_{\\epsilon}+B_{\\widehat{\\mu}})+\\mathrm{\\bar{log}}(1/\\delta))$ . ", "page_idx": 26}, {"type": "text", "text": "Proof. In the following, we prove that $x$ belongs to the interior of the polytope $\\mathcal{P}$ . To do that, we observe that the point $x^{\\diamond}$ belongs to the interior of $\\mathcal{P}$ , while, with the same analysis proposed in Lemma 4.8 by Bacchiocchi et al. [2024a], the distance between $x^{\\diamond}$ and $x$ can be upper bounded by $\\rho n$ . To ensure that $x$ belongs to $\\operatorname{int}(\\mathcal{P})$ , we have to show that $d(x^{\\diamond},x)$ is smaller than the distance between $x^{\\diamond}$ and any hyperplane defining the boundary of $\\mathcal{P}$ . ", "page_idx": 26}, {"type": "text", "text": "Let us denote with $v^{h}$ the $h$ -vertex of the set $\\nu$ , so that $\\mathcal{V}\\,=\\,\\{v^{1},v^{2},\\ldots,v^{d}\\}$ . Furthermore, we define: ", "page_idx": 27}, {"type": "equation", "text": "$$\nv_{\\theta}^{h}:=\\frac{\\gamma_{\\theta}^{h}}{\\nu_{h}}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "for each $h\\,\\in\\,[d]$ and $\\theta\\,\\in\\,\\Theta$ . Since $x^{\\diamond}$ belongs to $\\operatorname{int}(\\mathcal{P})$ , then the distance between $x^{\\diamond}$ and any separating hyperplane $H_{j k}$ can be lower bounded as follows: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d(x^{\\diamond},H_{j k})=\\left|\\frac{\\sum_{\\theta\\in\\Theta}x_{\\theta}^{\\diamond}\\mu_{\\theta}\\left(u_{\\theta}\\left(a_{j}\\right)-u_{\\theta}\\left(a_{k}\\right)\\right)}{\\sqrt{\\sum_{\\theta\\in\\Theta}\\mu_{\\theta}^{2}\\left(u_{\\theta}\\left(a_{j}\\right)-u_{\\theta}\\left(a_{k}\\right)\\right)^{2}}}\\right|}\\\\ &{\\qquad\\qquad=\\left|\\frac{\\sum_{\\theta\\in\\Theta}\\sum_{h=1}^{d}v_{\\theta}^{h}\\mu_{\\theta}\\left(u_{\\theta}\\left(a_{j}\\right)-u_{\\theta}\\left(a_{k}\\right)\\right)}{d\\sqrt{\\sum_{\\theta\\in\\Theta}\\mu_{\\theta}^{2}\\left(u_{\\theta}\\left(a_{j}\\right)-u_{\\theta}\\left(a_{k}\\right)\\right)^{2}}}\\right|}\\\\ &{\\qquad\\qquad\\geq\\frac{1}{d^{2}}2^{-4d B-9d^{3}\\left(B+B_{\\epsilon}+B_{\\tilde{\\mu}}\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "To prove the last inequality, we observe that the denominator of the fraction above can be upper bounded by $d^{2}$ . To lower bound the nominator, we define: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mu_{\\theta}(u_{\\theta}(a_{j})-u_{\\theta}(a_{k})):=\\frac{\\alpha_{\\theta}}{\\beta_{\\theta}}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "for each $\\theta\\in\\Theta$ . As a result, we have: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\left\\lvert\\sum_{\\theta\\in\\Theta}\\displaystyle\\sum_{h=1}^{d}v_{\\theta}^{h}\\mu_{\\theta}(u_{\\theta}(a_{j})-u_{\\theta}(a_{k}))\\right\\rvert=\\displaystyle\\left\\lvert\\sum_{\\theta\\in\\Theta}\\displaystyle\\sum_{h=1}^{d}\\frac{\\alpha_{\\theta}\\gamma_{\\theta}^{h}}{\\beta_{\\theta}\\nu_{h}}\\right\\rvert}\\\\ {\\displaystyle}&{=\\left\\lvert\\frac{\\sum_{\\theta\\in\\Theta}\\sum_{h=1}^{d}\\alpha_{\\theta}\\gamma_{\\theta}^{h}\\left(\\prod_{\\theta^{\\prime}\\ne\\theta}\\beta_{\\theta^{\\prime}}\\prod_{h^{\\prime}\\ne h}\\nu_{h^{\\prime}}\\right)}{\\prod_{\\theta\\in\\Theta}\\beta_{\\theta}\\prod_{h=1}^{d}\\nu_{h}}\\right\\rvert}\\\\ {\\displaystyle}&{\\geq\\left(\\displaystyle\\prod_{\\theta\\in\\Theta}\\beta_{\\theta}\\displaystyle\\prod_{h=1}^{d}\\nu_{h}\\right)^{-1}}\\\\ {\\displaystyle}&{\\geq2\\overline{{\\mathrm{\\Omega}^{-4d B-9\\underline{{i}}}({\\boldsymbol B}+{\\boldsymbol B}_{\\varepsilon}+{\\boldsymbol B}_{\\hat{\\boldsymbol{\\mu}}})}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The first inequality holds because the numerator of the fraction above can be lower bounded by one. The denominator can be instead upper bounded observing that the bit-complexity of each $\\beta_{\\theta}$ is at most $4B$ while the bit-complexity of each $\\nu_{h}$ is at most $9d^{\\textstyle\\frac{5}{2}}(B+B_{\\epsilon}+B_{\\widehat{\\mu}})$ , as stated by Lemma 11. ", "page_idx": 27}, {"type": "text", "text": "In a similar way, we can lower bound the distance between $x^{\\diamond}$ and $\\widehat{H}:=\\{x\\in\\mathbb{R}^{d}\\,|\\sum_{\\theta\\in\\widetilde{\\Theta}}\\widehat{\\mu}_{\\theta}x_{\\theta}\\geq2\\epsilon\\}$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d(x^{\\diamond},\\widehat{H})=\\left|\\frac{\\sum_{\\theta\\in\\widetilde{\\Theta}}\\widehat{\\mu}_{\\theta}x_{\\theta}^{\\diamond}+2\\epsilon}{\\sqrt{\\sum_{\\theta\\in\\widetilde{\\Theta}}\\widehat{\\mu}^{2}}}\\right|}\\\\ &{\\qquad\\qquad=\\left|\\frac{\\sum_{\\theta\\in\\widetilde{\\Theta}}\\sum_{h=1}^{d}\\widehat{\\mu}_{\\theta}v_{\\theta}^{h}+2\\epsilon}{d\\sqrt{\\sum_{\\theta\\in\\widetilde{\\Theta}}\\widehat{\\mu}^{2}}}\\right|}\\\\ &{\\qquad\\qquad\\geq\\frac{1}{d^{2}}2^{-B_{\\widehat{\\mu}}-B_{\\epsilon}-9d^{3}(B+B_{\\epsilon}+B_{\\widehat{\\mu}})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "To prove the last inequality, we observe that the denominator of the fraction above can be upper bounded by $d^{2}$ , while to lower bound the numerator, we define: ", "page_idx": 27}, {"type": "equation", "text": "$$\n{\\hat{\\mu}}_{\\theta}:={\\frac{N_{\\theta}}{p}}{\\mathrm{~and~}}\\epsilon={\\frac{\\alpha}{\\beta}}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "for each $\\theta\\in\\widetilde{\\Theta}$ . As a result, we have: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left|\\sum_{\\theta\\in\\widetilde{\\Theta}}\\sum_{h=1}^{d}\\widehat{\\mu}_{\\theta}v_{\\theta}^{h}+2\\epsilon\\right|=\\left|\\sum_{\\theta\\in\\widetilde{\\Theta}}\\sum_{h=1}^{d}\\frac{N_{\\theta}\\gamma_{\\theta}^{h}}{p\\nu_{h}}+2\\epsilon\\right|\n$$", "text_format": "latex", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~=\\left|\\frac{\\sum_{\\theta\\in\\widetilde{\\Theta}}\\sum_{h=1}^{d}\\beta N_{\\theta}\\gamma_{\\theta}^{h}\\prod_{h^{\\prime}\\neq h}\\nu_{h^{\\prime}}+2\\alpha p\\prod_{h=1}^{d}\\nu_{h}}{p\\prod_{h=1}^{d}\\nu_{h}\\beta}\\right|}\\\\ &{~~\\geq2^{-B_{\\widehat{\\mu}}-B_{\\epsilon}-9d^{3}(B+B_{\\epsilon}+B_{\\widehat{\\mu}})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Thus, the distance between $x^{\\diamond}$ and any hyperplane $H$ defining the boundary of $\\mathcal{P}$ can be lower bounded as follows: ", "page_idx": 28}, {"type": "equation", "text": "$$\nd(x^{\\diamond},H)\\geq{\\frac{1}{d^{2}}}2^{-9d^{3}(B+B_{\\epsilon}+B_{\\widehat{\\mu}})-4d B-B_{\\widehat{\\mu}}-B_{\\epsilon}}\\geq{\\frac{1}{d^{2}}}2^{-10d^{3}(B+B_{\\epsilon}+B_{\\widehat{\\mu}})}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We observe that, given the definition of $\\rho$ at Line 3, the distance $d(x^{\\diamond},x)$ is strictly smaller than $d(x^{\\diamond},H)$ , showing that $x\\in\\operatorname*{int}(\\mathcal{P})$ . ", "page_idx": 28}, {"type": "text", "text": "Furthermore, we can prove that the bit-complexity of $x$ is bounded by ${\\mathcal{O}}(d^{3}(B\\!+\\!B_{\\epsilon}\\!+\\!B_{\\widehat{\\mu}})\\!+\\!\\log(1/\\delta))$ . To do so, we obser\u221ave that the denominator of $x^{\\diamond}$ is equal to $d\\prod_{h=1}^{d}\\nu_{h}$ , while the denominator of $y_{i}$ is equal to $M=\\lceil\\sqrt{d}/\\delta\\rceil$ . As a result, the denominator of every $x_{i}=x_{i}^{\\circ}+\\rho y_{i}$ , with $i\\in[d-1]$ , can be written as follows: ", "page_idx": 28}, {"type": "equation", "text": "$$\nD=d\\prod_{h=1}^{d}\\nu_{h}D_{\\rho}M,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $D_{\\rho}$ is the denominator of the rational number $\\rho$ . Similarly, the last component $x_{d}$ can be written with the same denominator. As a result, the bit complexity of $x\\,\\in\\,[0,1]^{d}$ can be upper bounded as follows: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B_{x}\\leq2\\left\\lceil\\log(D)\\right\\rceil}\\\\ &{\\quad=\\mathcal{O}(\\log(d\\prod_{h=1}^{d}\\nu_{h}D_{\\rho}M))}\\\\ &{\\quad=\\mathcal{O}\\left(\\log\\left(\\prod_{h=1}^{d}2^{9d^{2}(B+B_{\\varepsilon}+B_{\\hat{\\mu}})}\\right)+\\log(d2^{10d^{3}(B+B_{\\varepsilon}+B_{\\hat{\\mu}})})+\\log(\\sqrt{d}/\\delta)\\right)}\\\\ &{\\quad=\\mathcal{O}\\left(d^{3}(B+B_{\\varepsilon}+B_{\\hat{\\mu}})+\\log\\left(\\frac1\\delta\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Finally, with the same analysis performed in Lemma 4.8 by Bacchiocchi et al. [2024a], we can show that the probability that $x$ belongs to a given hyperplane $H$ is at most $\\delta$ . \u53e3 ", "page_idx": 28}, {"type": "text", "text": "Lemma 11. Each vertex v of a polytope ${\\mathcal P}\\ \\subseteq\\ {\\mathcal X}_{\\epsilon}\\ :\\ \\operatorname{vol}_{d-1}({\\mathcal P})\\ >\\ \\ 0,$ , defined by separating or boundary hyperplanes, has bit-complexity at most $9d^{2}(B+\\dot{B_{\\epsilon}}+B_{\\widehat{\\mu}})$ . Furthermore, with a bitcomplexity of $9d^{2}(B+B_{\\epsilon}+B_{\\widehat{\\mu}})$ , all the components of the vector $v$ identifying a vertex can be written as fractions with the same denominator. ", "page_idx": 28}, {"type": "text", "text": "Proof. We follow a line of reasoning similar to the proof of Lemma D.2 in Bacchiocchi et al. [2024a]. Let $v$ be a vertex of the polytope $\\mathcal{P}$ . Then such a vertex lies on the hyperplane $H^{\\prime}$ ensuring that the sum of its components is equal to one. Furthermore, it also belongs to a subset of $d-1$ linearly independent hyperplanes. These can be separating hyperplanes: ", "page_idx": 28}, {"type": "equation", "text": "$$\nH_{i j}=\\left\\{x\\in\\mathbb{R}^{d}\\mid\\sum_{\\theta\\in\\Theta}\\mu_{\\theta}x_{\\theta}{\\big(}u_{\\theta}(a_{i})-u_{\\theta}(a_{j}){\\big)}=0\\right\\}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "with $a_{i},a_{j}\\in A$ , boundary hyperplanes of the form $H_{i}=\\{x\\in\\mathbb{R}^{d}\\mid x_{i}>0\\}$ , or the hyperplane $\\begin{array}{r}{\\widehat{H}:=\\left\\{x\\in\\mathbb{R}^{d}\\mid\\sum_{\\theta\\in\\widetilde{\\Theta}}\\widehat{\\mu}_{\\theta}x_{\\theta}\\geq2\\epsilon\\right\\}}\\end{array}$ . Consequently, there exists a matrix $A\\in\\mathbb{Q}^{d\\times d}$ and a vector $b\\in\\mathbb{Q}^{d}$ such that $A v=b$ . ", "page_idx": 28}, {"type": "text", "text": "Suppose that $v$ is not defined by the hyperplane $\\begin{array}{r}{\\widehat{H}:=\\left\\{x\\in\\mathbb{R}^{d}\\mid\\sum_{\\theta\\in\\Theta}\\widehat{\\mu}_{\\theta}x_{\\theta}\\geq2\\epsilon\\right\\}}\\end{array}$ . Then, each entry of the matrix $A$ is either equal to one or the quantity $\\mu_{\\theta}(u_{\\theta}(a)-u_{\\theta}(a^{\\prime})$ for some $\\theta\\in\\Theta$ and $a,a^{\\dot{\\prime}}\\in A$ . Thus, its bit-complexity is bounded by $B$ . Similarly, each entry of the vector $b$ is either ", "page_idx": 28}, {"type": "text", "text": "equal to one or zero. With a reasoning similar to the one applied in Bacchiocchi et al. [2024a], the bit-complexity of $v$ is at most $9d^{2}(B_{\\mu}+B_{u})$ . ", "page_idx": 29}, {"type": "text", "text": "Suppose instead that $v$ is defined also by the hyperplane $\\widehat{H}$ , corresponding to the last row of the matrix $A$ and the last component of the vector $b$ . This hype rplane can be rewritten as: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\widehat{H}=\\left\\{x\\in\\mathbb{R}^{d}\\mid\\sum_{\\theta\\in\\widetilde{\\Theta}}\\frac{\\widehat{\\mu}_{\\theta}}{2\\epsilon}x_{\\theta}\\geq1\\right\\}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Thus, each element of the last row of $A$ is either zero or the quantity $\\underset{-}{\\widehat{\\mu}}_{\\theta}\\big/2\\epsilon$ for some $\\theta\\in\\widetilde{\\Theta}$ . We observe that $\\widehat{\\mu}_{\\theta}/_{2\\epsilon}$ is a rational number with numerator bounded by $2^{B_{\\widehat{\\mu}}+B_{\\epsilon}}$ and denominator bo unded by $2^{B_{\\widehat{\\mu}}+B_{\\epsilon}+1}$ . Thus, we multiply the last row of $A$ and the last component of $b$ by a constant bounded by $2^{d(B_{\\widehat{\\mu}}+B_{\\epsilon}+1)}$ . The other rows of $A$ and the corresponding components of $b$ are multiplied instead by some constants bounded by $2^{4d B}$ . This way, we obtain an equivalent system $A^{\\prime}v=b^{\\prime}$ with integer coefficients. ", "page_idx": 29}, {"type": "text", "text": "We define $A^{\\prime}(j)$ as the matrix obtained by substituting the $j$ -th column of $A^{\\prime}$ with $b^{\\prime}$ . Then, by Cramer\u2019s rule, the value of the $j$ -th component of $v_{j}$ can be computed as follows: ", "page_idx": 29}, {"type": "equation", "text": "$$\nv_{j}={\\frac{\\operatorname*{det}(A^{\\prime}(j))}{\\operatorname*{det}(A^{\\prime})}}\\ \\ \\forall j\\in[d].\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We observe that both determinants are integer numbers as the entries of both $A^{\\prime}$ and $b^{\\prime}$ are all integers, thus by Hadamard\u2019s inequality we have: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\operatorname*{det}(A^{\\prime})!\\leq\\prod_{i\\in[d]}\\sqrt{\\sum_{\\ell\\in[d]}a_{\\ell i}^{\\prime}}^{2}}}\\\\ &{\\leq\\left(\\prod_{i\\in[d]}\\sqrt{\\sum_{\\ell\\in[d]}(\\sum_{l}^{2(d B_{\\ell}+B_{\\ell}+1)})}\\right)\\sqrt{\\sum_{\\ell\\in[d]}(2^{(B_{\\ell}+B_{\\ell}+1)})^{2}}}\\\\ &{=\\left(\\prod_{i\\in[d]-1}\\sqrt{d(2^{(d A)})}\\right)\\sqrt{d^{2(d B_{\\ell}+B_{\\ell}+1)}}}\\\\ &{=\\left(\\prod_{i\\in[d]-1}d^{\\frac{1}{2}}(2^{d B})\\right)d^{\\frac{1}{2}}2^{(B_{\\ell}+B_{\\ell}+1)}}\\\\ &{=d^{\\frac{1}{2}}(2^{(d A)(d-1)})^{2}}\\\\ &{\\leq d^{\\frac{1}{2}}(2^{(d B)})2^{d(B_{\\ell}+B_{\\ell}+1)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "With a reasoning similar to Bacchiocchi et al. [2024a], we can show that that the bit-complexity $D_{v}$ of the vertex $v$ is bounded by: ", "page_idx": 29}, {"type": "equation", "text": "$$\nD_{v}\\le9B d^{2}+2(d(B_{\\widehat{\\mu}}+B_{\\epsilon}+1))\\le9d^{2}(B+B_{\\epsilon}+B_{\\widehat{\\mu}})\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Furthermore, this result holds when the denominator of every component $v_{j}$ of the vertex $v$ is written with the same denominator $\\operatorname*{det}(A^{\\prime})$ , concluding the proof. \u53e3 ", "page_idx": 29}, {"type": "table", "img_path": "XNpVZ8E1tY/tmp/ff6805a1554c2dd985a6cbdef6cda0e93944f1cf484951df8fda71a012f6a025.jpg", "table_caption": [], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "Algorithm 11 takes in input the collection of polytopes $\\{\\mathcal{X}_{\\epsilon}(a_{i})\\}_{a_{i}\\in\\mathcal{C}}$ and another action $a_{j}\\ \\notin\\mathcal{C}$ such that $\\mathrm{vol}(\\mathcal{X}_{\\epsilon}(a_{j}))=\\bar{0}$ , and outputs the $\\mathrm{H}$ -representation of a (possibly improper) face of $\\bar{\\mathcal{X}}_{\\epsilon}(a_{j})$ that contains all those vertices $x\\in V(\\mathcal{X}_{\\epsilon}(a_{j}))$ where $a(x)=a_{j}$ . As we will show by means of a pair of technical lemmas, the polytope $\\bar{\\mathcal{X}}_{\\epsilon}(a_{j})$ is a face of some other polytope $\\mathcal{X}_{\\epsilon}(a_{k})$ , with $a_{k}\\in\\mathcal{C}$ . Consequently, Algorithm 11 looks for a face of some polytope $\\mathcal{X}_{\\epsilon}(a_{k})$ containing the set of vertices $\\mathscr{V}_{\\epsilon}(a_{j})$ . ", "page_idx": 30}, {"type": "text", "text": "As a first step, Algorithm 11 computes, for every action $a_{i}\\in\\mathcal{C}$ , the set of hyperplanes $\\mathcal{M}(\\mathcal{X}_{\\epsilon}(a_{i}))$ corresponding to the minimal $\\mathrm{H}$ -representation of $\\mathcal{X}_{\\epsilon}(a_{i})$ . This set includes every separating hyperplane $H_{i k}$ found by Algorithm 7, together with the non-redundant boundary hyperplanes that delimit $\\textstyle{\\mathcal{X}}_{\\epsilon}$ . Subsequently, Algorithm 11 iterates over the vertices of the regions with volume larger than zero, which we prove to include all the vertices of the region $\\mathcal{X}_{\\epsilon}(a_{j})$ . While doing so, it builds a set of hyperplanes $\\mathcal{H}(a_{i})\\subseteq\\mathcal{M}(\\mathcal{X}_{\\epsilon}(a_{i}))$ for every action $a_{i}\\in\\mathcal{C}$ . Such a (possibly empty) set includes all and only the hyperplanes in $\\mathcal{M}(\\mathcal{X}_{\\epsilon}(a_{i}))$ that contain all the vertices where the action $a_{j}$ has been observed, i.e, $a(x)=a_{j}$ . ", "page_idx": 30}, {"type": "text", "text": "Finally, at Line 14 Algorithm 11 intersects every region $\\mathcal{X}_{\\epsilon}(a_{i})$ with the corresponding hyperplanes in $\\mathcal{H}(a_{i})$ , obtaining a (possibly empty) face for every polytope $\\mathcal{X}_{\\epsilon}(a_{i})$ , $a_{i}\\in\\mathcal{C}$ . At least one of these faces is the face the algorithm is looking for, and corresponds to the output of Algorithm 11. ", "page_idx": 30}, {"type": "text", "text": "The main result concerning Algorithm 11 is the following: ", "page_idx": 30}, {"type": "text", "text": "Lemma 12. Given the collection of polytopes $\\{\\mathcal{X}_{\\epsilon}(a_{i})\\}_{a_{i}\\in\\mathcal{C}}$ with volume larger than zero and an another action $a_{j}$ , then, under the event $\\mathcal{E}^{\\mathrm{a}}$ , Algorithm $_{l l}$ returns a (possibly improper) face $\\mathcal{F}_{\\epsilon}(a_{j})$ of $\\mathcal{X}_{\\epsilon}(a_{j})$ such that $\\mathcal{V}_{\\epsilon}(a_{j})\\,\\subseteq\\,\\mathcal{F}_{\\epsilon}(a_{j})$ . Furthermore, the Algorithm requires $O(n{\\binom{d+n}{d}})$ calls to Algorithm 5. ", "page_idx": 30}, {"type": "text", "text": "In order to prove it, we first need to introduce two technical lemmas to characterize the relationship between regions with null volume and those with volume larger than zero. ", "page_idx": 30}, {"type": "text", "text": "Lemma 13. Let $a_{i}$ , $a_{j}\\in A$ such that vo $l(\\mathcal{X}_{\\epsilon}(a_{i}))>0$ and $\\nu o l(\\mathcal{X}_{\\epsilon}(a_{j}))=0$ . Then $\\mathcal{X}_{\\epsilon}(a_{i})\\cap\\mathcal{X}_{\\epsilon}(a_{j})$ is $a$ (possibly improper) face of $\\mathcal{X}_{\\epsilon}(a_{i})$ and $\\mathcal{X}_{\\epsilon}(a_{j})$ . ", "page_idx": 30}, {"type": "text", "text": "Proof. In the following we assume that $\\mathcal{X}_{\\epsilon}(a_{i})\\cap\\mathcal{X}_{\\epsilon}(a_{j})$ is non-empty, as the empty set is an improper face of every polytope. ", "page_idx": 30}, {"type": "text", "text": "We prove that $\\mathcal{X}_{\\epsilon}(a_{i})\\cap\\mathcal{X}_{\\epsilon}(a_{j})=\\mathcal{X}_{\\epsilon}(a_{i})\\cap H_{i j}=\\mathcal{X}_{\\epsilon}(a_{j})\\cap H_{i j}$ . In order to do that, we first show that $\\mathcal{X}_{\\epsilon}(a_{i})\\cap H_{i j}\\,\\subseteq\\,\\mathcal{X}_{\\epsilon}(a_{i})\\cap\\mathcal{X}_{\\epsilon}(a_{j})$ . Consider a normalized slice $x\\in\\mathcal{X}_{\\epsilon}(a_{i})\\cap H_{i j}$ . Then we have thatx $\\in\\mathcal{X}_{\\epsilon}(a_{j})$ . As this holds for every $x\\in\\mathcal{X}_{\\epsilon}(a_{i})\\cap H_{i j}$ , it follows that $\\mathcal{X}_{\\epsilon}(\\bar{a_{i}})\\cap H_{i j}\\subseteq$ $\\mathcal{X}_{\\epsilon}(a_{i})\\cap\\mathcal{X}_{\\epsilon}(a_{j})$ . ", "page_idx": 30}, {"type": "text", "text": "Similarly, we show that $\\mathcal{X}_{\\epsilon}(a_{i})\\cap\\mathcal{X}_{\\epsilon}(a_{j})\\subseteq\\mathcal{X}_{\\epsilon}(a_{i})\\cap H_{i j}$ . Take any normalized slice $x\\in\\mathcal{X}_{\\epsilon}(a_{i})\\cap$ $\\mathcal{X}_{\\epsilon}(a_{j})$ . Then $x\\in H_{i j}$ as it belongs to both $\\mathcal{X}_{\\epsilon}(a_{i})$ and $\\breve{\\mathscr X}_{\\epsilon}(a_{j})$ , thus $x\\in\\mathcal{X}_{\\epsilon}(a_{i})\\cap H_{i j}$ . This implies that $\\mathring{\\mathcal{X}}_{\\epsilon}(a_{i})\\cap\\mathcal{X}_{\\epsilon}(a_{j})\\subseteq\\mathcal{X}_{\\epsilon}(a_{i})\\cap H_{i j}$ . ", "page_idx": 31}, {"type": "text", "text": "Consequently, we have that $\\mathcal{X}_{\\epsilon}(a_{i})\\cap\\mathcal{X}_{\\epsilon}(a_{j})=\\mathcal{X}_{\\epsilon}(a_{i})\\cap H_{i j}$ . With a similar argument, we can prove that $\\mathcal{X}_{\\epsilon}(a_{i})\\cap\\mathcal{X}_{\\epsilon}(a_{j})=\\mathcal{X}_{\\epsilon}(a_{j})\\cap H_{i j}$ . As a result, we have that $\\mathcal{X}_{\\epsilon}(a_{i})\\cap\\mathcal{X}_{\\epsilon}(a_{j}^{-})=\\mathcal{X}_{\\epsilon}(a_{i})\\cap\\bar{H}_{i j}=$ $\\mathcal{X}_{\\epsilon}(a_{j})\\cap H_{i j}$ . ", "page_idx": 31}, {"type": "text", "text": "In order to conclude the proof, we show that $\\mathcal{X}_{\\epsilon}(a_{i})\\cap\\mathcal{X}_{\\epsilon}(a_{j})\\,=\\,\\mathcal{X}_{\\epsilon}(a_{i})\\cap H_{i j}\\,=\\,\\mathcal{X}_{\\epsilon}(a_{j})\\cap H_{i j}$ is a face of both $\\mathcal{X}_{\\epsilon}(a_{i})$ and $\\mathcal{X}_{\\epsilon}(a_{j})$ . We observe that $\\mathcal{X}_{\\epsilon}(a_{i})\\subseteq\\mathcal{H}_{i j}$ , thus the non-empty region $\\mathcal{X}_{\\epsilon}(a_{i})\\cap H_{i j}$ is by definition a face of $\\mathcal{X}_{\\epsilon}(a_{i})$ . Similarly, $\\mathcal{X}_{\\epsilon}(a_{j})\\subseteq\\mathcal{H}_{j i}$ , thus the non-empty region $\\mathcal{X}_{\\epsilon}(a_{j})\\cap H_{i j}^{\\bar{\\ }}$ is a face of $\\mathcal{X}_{\\epsilon}(a_{j})$ (possibly the improper face $\\bar{\\mathcal{X}}_{\\epsilon}(a_{j})$ itself). \u53e3 ", "page_idx": 31}, {"type": "text", "text": "Lemma 14. Let $\\mathcal{X}_{\\epsilon}(a_{j})$ be a polytope such that $\\nu o l(\\mathcal{X}_{\\epsilon}(a_{j}))=0$ . Then $\\mathcal{X}_{\\epsilon}(a_{j})$ is a face of some polytope $\\mathcal{X}_{\\epsilon}(a_{i})$ with $\\bar{\\nu}o l(\\mathcal{X}_{\\epsilon}(a_{i}))>0$ . ", "page_idx": 31}, {"type": "text", "text": "Proof. First, we observe that if $\\mathcal{X}_{\\epsilon}(a_{j})$ is empty, then it is the improper face of any region $\\mathcal{X}_{\\epsilon}(a_{i})$ with $\\operatorname{vol}(\\mathcal{X}_{\\epsilon}(a_{i}))>0$ . Thus, in the following, we consider $\\mathcal{X}_{\\epsilon}(a_{j})$ to be non-empty. ", "page_idx": 31}, {"type": "text", "text": "As a first step, we observe that any normalized slice $x\\,\\in\\,{\\mathcal X}_{\\epsilon}(a_{j})$ belongs also to some region $\\mathcal{X}_{\\epsilon}(a_{k})$ , where $a_{k}\\in\\mathcal A$ depends on $x$ , such that $\\mathrm{vol}(\\mathcal{X}_{\\epsilon}(a_{k}))>0$ . Suppose, by contradiction, that $x\\in\\mathrm{int}(\\mathcal{X}_{\\epsilon}(a_{i}))$ . Then $a_{i}$ is a best-response in $\\mathcal{X}_{\\epsilon}(a_{j})\\cap\\mathcal{H}_{i j}$ , i.e., $\\bar{\\mathcal{X}_{\\epsilon}}(a_{j})\\cap\\bar{\\mathcal{H}}_{i j}\\subseteq\\mathcal{X}_{\\epsilon}(a_{i})$ . One can easily observe that such a region has positive volume, thus contradicting the hypothesis that $\\mathrm{vol}(\\lambda_{\\epsilon}(\\dot{a_{j}}))=0$ . ", "page_idx": 31}, {"type": "text", "text": "Now we prove that there exists some $\\mathcal{X}_{\\epsilon}(a_{i})$ with $\\mathrm{vol}(\\mathcal{X}_{\\epsilon}(a_{i}))>0$ such that $\\mathcal{X}_{\\epsilon}(a_{j})\\subseteq\\mathcal{X}_{\\epsilon}(a_{i})$ . If $\\mathcal{X}_{\\epsilon}(a_{j})$ is a single normalized slice $x$ , then this trivially holds. ", "page_idx": 31}, {"type": "text", "text": "Suppose instead that $\\mathcal{X}_{\\epsilon}(a_{j})$ has dimension at least one. Consider a fixed normalized slice $\\bar{x}\\in$ $\\operatorname*{int}(\\bar{\\boldsymbol{\\mathcal{X}}}_{\\epsilon}(a_{j}))$ , where the interior is taken relative to the subspace that contains $\\mathcal{X}_{\\epsilon}(a_{j})$ and has minimum dimension. There exists a region $\\mathcal{X}_{\\epsilon}(a_{i})$ with $\\operatorname{vol}(\\mathcal{X}_{\\epsilon}(a_{i})\\bar{)}>0$ such that $\\bar{x}\\in\\mathcal{X}_{\\epsilon}(a_{i})$ . ", "page_idx": 31}, {"type": "text", "text": "We prove that $\\mathcal{X}_{\\epsilon}(a_{j})\\,\\subseteq\\,\\mathcal{X}(a_{i})$ . Suppose, by contradiction, that there exists a normalized slice $x\\in\\mathcal{X}_{\\epsilon}(a_{j})$ such that $x\\not\\in\\mathcal{X}_{\\epsilon}(a_{i})$ . It follows that the line segment $\\cos(\\bar{x},x)$ intersect the separating hyperplane $H_{i j}$ in some normalized slice $\\widetilde{x}\\;\\in\\;\\mathrm{co}(\\bar{x},x)\\cap H_{i j}$ . Furthermore, since $\\widetilde{x}\\ne\\,x$ and $\\bar{x}\\in\\mathrm{int}(\\mathcal{X}_{\\epsilon}(a_{j})\\bar{)}$ , then $\\widetilde{x}\\in\\mathrm{int}(\\mathcal{X}_{\\epsilon}(a_{j}))$ . How e ver, if the internal point $\\widetilde{x}$ belongs to the hy p erplane $H_{i j}$ and $\\mathcal{X}_{\\epsilon}(a_{j})\\subseteq\\mathcal{H}_{j i}$ , th e n it must be the case that $\\mathcal{X}_{\\epsilon}(a_{j})\\subseteq H_{i j}$ . Thi s  implies that $\\mathcal{X}_{\\epsilon}(a_{j})\\subseteq\\mathcal{X}_{\\epsilon}(a_{i})$ and thus $\\bar{x}\\in\\mathcal X_{\\epsilon}(a_{i})$ , which contradicts the hypothesis.. ", "page_idx": 31}, {"type": "text", "text": "Given that there exists some $\\mathcal{X}_{\\epsilon}(a_{i})$ with $\\mathrm{vol}(\\mathcal{X}_{\\epsilon}(a_{i}))>0$ such that $\\mathcal{X}_{\\epsilon}(a_{j})\\subseteq\\mathcal{X}_{\\epsilon}(a_{i})$ , then $\\mathcal{X}_{\\epsilon}(a_{j})=$ $\\mathcal{X}_{\\epsilon}(a_{i})\\cap\\mathcal{X}_{\\epsilon}(a_{j})$ is a face of $\\mathcal{X}_{\\epsilon}(a_{i})$ by Lemma 13. ", "page_idx": 31}, {"type": "text", "text": "Lemma 12. Given the collection of polytopes $\\{\\mathcal{X}_{\\epsilon}(a_{i})\\}_{a_{i}\\in\\mathcal{C}}$ with volume larger than zero and an another action $a_{j}$ , then, under the event $\\mathcal{E}^{\\mathrm{a}}$ , Algorithm $_{l l}$ returns a (possibly improper) face $\\mathcal{F}_{\\epsilon}(a_{j})$ of $\\mathcal{X}_{\\epsilon}(a_{j})$ such that $\\mathcal{V}_{\\epsilon}(a_{j})\\,\\subseteq\\,\\mathcal{F}_{\\epsilon}(a_{j})$ . Furthermore, the Algorithm requires $O(n{\\binom{d+n}{d}})$ calls to Algorithm 5. ", "page_idx": 31}, {"type": "text", "text": "Proof. In the following, for the sake of notation, given a polytope $\\mathcal{X}_{\\epsilon}(a)$ and the a set of hyperplanes $\\mathcal{H}(a)$ , with an abuse of notation we denote with $\\mathcal{X}_{\\epsilon}(a)\\cap\\mathcal{H}(a)$ the intersection of $\\mathcal{X}_{\\epsilon}(a)$ with every hyperplane in $\\mathcal{H}(a)$ . Formally: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathcal{X}_{\\epsilon}(a)\\cap\\,\\mathcal{H}(a):=\\mathcal{X}_{\\epsilon}(a)\\cap\\bigcap_{H\\in\\mathcal{H}(a)}H.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Suppose that $\\mathcal{X}_{\\epsilon}(a_{j})\\,=\\,\\emptyset$ . Then, one can easily verify that Algorithm 11 returns $\\varnothing$ . Thus, in the following we assume $\\mathcal{X}_{\\epsilon}(a_{j})\\neq\\emptyset$ . ", "page_idx": 31}, {"type": "text", "text": "Let $a_{i}$ be action selected at Line 14 Algorithm 11. We denote with $\\mathcal{F}_{\\epsilon}(a_{i})$ the face returned by Algorithm 11: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathcal{F}_{\\epsilon}(a_{i}):=\\mathcal{X}_{\\epsilon}(a_{i})\\cap\\mathcal{H}(a_{i}).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We observe that by Lemma 14, there exists an action $a_{k}\\in\\mathcal{C}$ such that $\\mathcal{X}_{\\epsilon}(a_{j})$ is a face of $\\mathcal{X}_{\\epsilon}(a_{k})$ .   \nConsequently, Algorithm 11 queries every vertex $x\\in\\mathcal{V}_{\\epsilon}(a_{j})$ . ", "page_idx": 31}, {"type": "text", "text": "As a first step we show that $\\mathcal{F}_{\\epsilon}(a_{i})$ actually is a face of $\\mathcal{X}_{\\epsilon}(a_{i})$ and contains every vertex of $\\mathscr{V}_{\\epsilon}(a_{j})$ . Being the non-empty intersection of $\\mathcal{X}_{\\epsilon}(a_{i})$ with some hyperplanes in $\\mathcal{M}(\\mathcal{X}_{\\epsilon}(a_{i}))$ , $\\mathcal{F}_{\\epsilon}(a_{i})$ is a face of $\\bar{\\mathcal{X}_{\\epsilon}}(a_{i})$ . One can easily prove by induction that $\\mathcal{H}(a_{i})$ includes all and only the hyperplanes within $\\mathcal{M}(\\mathcal{X}_{\\epsilon}(a_{i}))$ containing every vertex in $\\mathscr{V}_{\\epsilon}(a_{j})$ . Thus, $\\dot{\\mathcal{V}}_{\\epsilon}(a_{j})\\subseteq\\mathcal{F}_{\\epsilon}(a_{i})$ . ", "page_idx": 32}, {"type": "text", "text": "Now we show that $\\mathcal{F}_{\\epsilon}(a_{i})$ is not only a (proper) face of $\\mathcal{X}_{\\epsilon}(a_{i})$ containing the set $\\mathscr{V}_{\\epsilon}(a_{j})$ , but also a face of $\\mathcal{X}_{\\epsilon}(a_{j})$ (possibly the improper face $\\mathcal{X}_{\\epsilon}(a_{j})$ itself). We consider the set $\\mathcal{X}_{\\epsilon}(a_{i})\\cap\\bar{\\mathcal{X}}_{\\epsilon}(a_{j})$ , which is a face of both $\\mathcal{X}_{\\epsilon}(a_{i})$ and $\\mathcal{X}_{\\epsilon}(a_{j})$ thanks to Lemma 13. Thus, there exists some set of hyperplanes $\\mathcal{H}^{\\prime}(a_{i})\\subset\\mathcal{M}(\\mathcal{X}_{\\epsilon}(a_{i}))$ such that: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathcal{X}_{\\epsilon}(a_{i})\\cap\\mathcal{H}^{\\prime}(a_{i})=\\mathcal{X}_{\\epsilon}(a_{i})\\cap\\mathcal{X}_{\\epsilon}(a_{j}).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Furthermore, we observe that $\\mathcal{V}_{\\epsilon}(a_{j})\\,\\subseteq\\,\\mathcal{X}_{\\epsilon}(a_{i})\\cap\\mathcal{X}_{\\epsilon}(a_{j})$ . Indeed, we have that $\\mathcal{V}_{\\epsilon}(a_{j})\\subseteq\\mathcal{X}_{\\epsilon}(a_{i})$ since $\\mathcal{V}_{\\epsilon}(a_{j})\\subseteq\\mathcal{F}_{\\epsilon}(a_{i})$ and $\\mathcal{F}_{\\epsilon}(a_{i})$ is a face of $\\mathcal{X}_{\\epsilon}(a_{i})$ , and $\\mathcal{V}_{\\epsilon}(a_{j})\\subseteq\\mathcal{X}_{\\epsilon}(a_{j})$ by definition. ", "page_idx": 32}, {"type": "text", "text": "We want to prove that $\\mathcal{H}^{\\prime}(a_{i})\\subseteq\\mathcal{H}(a_{i})$ , where the set $\\mathcal{H}(a_{i})$ contains all and only the hyperplanes within $\\mathcal{M}(\\mathcal{X}_{\\epsilon}(a_{i}))$ that include the whole set $\\mathscr{V}_{\\epsilon}(a_{j})$ . In order to do that, suppose, by contradiction, that there exists a vertex $x\\in\\mathcal{V}_{\\epsilon}(a_{j})$ such that $x\\notin H$ for some $H\\in{\\mathcal{H}}^{\\prime}(a_{i})$ . Then, $x\\not\\in\\mathcal{X}_{\\epsilon}(a_{i})\\cap$ $\\mathcal{H}^{\\prime}(a_{i})\\subseteq H$ . However, we proved that $\\mathcal{V}_{\\epsilon}(a_{j})\\subseteq\\mathcal{X}_{\\epsilon}(a_{i})\\cap\\mathcal{X}_{\\epsilon}(a_{j})$ and $\\mathcal{X}_{\\epsilon}(a_{i})\\cap\\mathcal{X}_{\\epsilon}(a_{j})=\\mathcal{X}_{\\epsilon}(a_{i})\\cap$ $\\mathcal{H}^{\\prime}(a_{i})$ by definition of $\\mathcal{H}^{\\prime}(a_{i})$ , reaching a contradiction. ", "page_idx": 32}, {"type": "text", "text": "Consequently: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{F}_{\\epsilon}(a_{i}):=\\mathcal{X}_{\\epsilon}(a_{i})\\cap\\mathcal{H}(a_{i})=\\mathcal{X}_{\\epsilon}(a_{i})\\cap\\bigcap\\bigcap_{H\\in\\mathcal{H}(a_{i})}H}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\subseteq\\mathcal{X}_{\\epsilon}(a_{i})\\cap\\bigcap\\bigcap_{H\\in\\mathcal{H}^{\\prime}(a_{i})}H}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\mathcal{X}_{\\epsilon}(a_{i})\\cap\\mathcal{H}^{\\prime}(a_{i})}\\\\ &{\\qquad=\\mathcal{X}_{\\epsilon}(a_{i})\\cap\\mathcal{H}^{\\prime}(a_{i})}\\\\ &{\\qquad=\\mathcal{X}_{\\epsilon}(a_{i})\\cap\\mathcal{X}_{\\epsilon}(a_{j}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where we applied Equation 8, the fact that $\\mathcal{H}^{\\prime}(a_{i})\\subseteq\\mathcal{H}(a_{i})$ , and Equation 9. ", "page_idx": 32}, {"type": "text", "text": "Finally, we can show that $\\mathcal{F}_{\\epsilon}(a_{i})$ is a face of $\\mathcal{X}_{\\epsilon}(a_{j})$ . We have that $\\mathcal{F}_{\\epsilon}(a_{i})$ is a face of $\\mathcal{X}_{\\epsilon}(a_{i})$ and $\\mathcal{F}_{\\epsilon}(a_{i})\\subseteq\\mathcal{X}_{\\epsilon}(a_{i})\\cap\\mathcal{X}_{\\epsilon}(a_{j})$ , which is itself a face of $\\mathcal{X}_{\\epsilon}(a_{i})$ by Lemma 13. Thus, $\\mathcal{F}_{\\epsilon}(a_{i})$ is a face of $\\mathcal{X}_{\\epsilon}(a_{i})\\cap\\mathcal{X}_{\\epsilon}(a_{j})$ . Furthermore, Lemma 13 states that $\\dot{\\mathcal{X}_{\\epsilon}}(\\dot{a_{i}})\\cap\\mathcal{X}_{\\epsilon}(a_{j})$ is also a face of $\\mathcal{X}_{\\epsilon}(a_{j})$ . This implies that $\\bar{\\mathcal{F}_{\\epsilon}}(a_{i})$ is a face of a face of $\\mathcal{X}_{\\epsilon}(a_{j})$ , and thus a face of $\\dot{\\mathcal{X}}_{\\epsilon}(a_{j})$ itself. ", "page_idx": 32}, {"type": "text", "text": "In order to conclude the proof, we have to prove that at Line 14 Algorithm 11 can actually find an action $a_{i}$ such that $\\bar{\\mathcal{F}}_{\\epsilon}(a_{i})=\\bar{\\mathcal{X}}_{\\epsilon}(a_{i})\\cap\\bar{\\mathcal{H}}(a_{i})$ is non-empty. Let $a_{k}\\in\\mathcal{C}$ be such that $\\mathcal{X}_{\\epsilon}(a_{j})$ is a face of $\\mathcal{X}_{\\epsilon}(a_{k})$ , which exists thanks to Lemma 14. Let $x$ be any vertex in the set $\\mathscr{V}_{\\epsilon}(a_{j})$ and define $\\mathcal{H}^{\\prime\\prime}(a_{j})$ as: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathcal{H}^{\\prime\\prime}(a_{k}):=\\{H\\in\\mathcal{R}^{H}(\\mathcal{X}_{\\epsilon}(a_{k}))\\mid x\\in H\\}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Then $\\mathcal{X}_{\\epsilon}(a_{k})\\cap\\mathcal{H}^{\\prime\\prime}(a_{k})=\\{x\\}$ . Consequently, $\\mathcal{H}(a_{k})\\subseteq\\mathcal{H}^{\\prime\\prime}(a_{k})$ , and thus: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\chi_{\\epsilon}(a_{k})\\cap\\mathcal H(a_{k})=\\chi_{\\epsilon}(a_{k})\\cap\\bigcap_{H\\in\\mathcal H(a_{k})}H}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\\\ &{\\qquad\\qquad\\qquad\\subseteq\\mathcal X_{\\epsilon}(a_{k})\\cap\\bigcap_{H\\in\\mathcal H^{\\prime\\prime}(a_{k})}H}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\mathcal X_{\\epsilon}(a_{k})\\cap\\mathcal H^{\\prime\\prime}(a_{k})}\\\\ &{\\qquad\\qquad\\qquad\\quad=\\{x\\}\\neq\\emptyset.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "As a result, there is always an action $a_{k}\\in\\mathcal{C}$ such that $\\mathcal{X}_{\\epsilon}(a_{k})\\cap\\mathcal{H}(a_{k})\\neq\\emptyset$ . ", "page_idx": 32}, {"type": "text", "text": "Finally, we observe that Algorithm 11 executes Algorithm 5 once for every vertex in the set $\\textstyle\\bigcup_{a_{i}\\in c}V({\\mathcal{X}}_{\\epsilon}(a_{i}))$ , which has size $O(n{\\binom{d+n}{d}})$ . \u53e3 ", "page_idx": 32}, {"type": "text", "text": "F Omitted proofs from Section 5 ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Theorem 2. For any sender\u2019s algorithm, there exists a Bayesian persuasion instance in which $n=d+2$ and the regret $R_{T}$ suffered by the algorithm is at least $2^{\\Omega(d)}$ , or, equivalently, $2^{\\Omega(n)}$ . ", "page_idx": 32}, {"type": "text", "text": "Proof. In the following, for the sake of the presentation, we consider a set of instances characterized by an even number $d\\in\\mathbb{N}_{+}$ of states of nature and $n=d+2$ receiver\u2019s actions. All the instances share the same uniform prior distribution and the same sender\u2019s utility, given by $u_{\\theta}^{\\mathrm{s}}(a_{d+1})=1$ for all $\\theta\\in\\Theta$ , and $u_{\\theta}^{\\mathrm{s}}(a)=0$ for all $\\theta\\in\\Theta$ and $\\forall a\\in A\\setminus\\{a_{d+1}\\}$ . Each instance is parametrized by a vector $p$ belonging to a set $\\mathcal{P}$ defined as follows: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathcal{P}:=\\left\\{p\\in\\{0,1\\}^{d}\\,|\\,\\sum_{i=1}^{d}p_{i}=\\frac{d}{2}\\right\\}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Furthermore, we assume that the receiver\u2019s utility in each instance $I_{p}$ is given by: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\langle\\varGamma_{p}\\rangle\\left\\{u_{\\theta_{i}}(a_{j})=\\delta_{i j}\\;\\;\\forall i,j\\in[d],\\right.}\\\\ {\\left.\\varPsi_{\\theta_{i}}(a_{d+1})=\\frac{2}{d}p_{i}\\;\\;\\forall i\\in[d],\\right.}\\\\ {\\left.u_{\\theta_{i}}(a_{d+2})=\\frac{2}{d}\\;\\;\\forall i\\in[d].\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We show that $\\begin{array}{r}{\\xi_{\\theta_{i}}^{\\prime}:=\\frac{2}{d}p_{i}}\\end{array}$ for each $i\\in[d]$ is the only posterior inducing the receiver\u2019s action $a_{d+1}\\in A$ in the instance ${\\dot{I}}_{p}$ , since the receiver breaks ties in favor of the sender. To prove that, we observe that the action $a_{d+1}$ is preferred to the action $a_{d+2}$ only in those posteriors that satisfy the condition $\\xi_{\\theta_{i}}\\,=\\,0$ for each $i\\in[d]$ with $p_{i}=0$ . Furthermore, to incentivize the action $a_{d+1}$ over the set of actions $a_{i}$ with $i\\in[d]$ , the following condition must hold: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\sum_{i\\in[d]:p_{i}>0}\\xi_{\\theta_{i}}u_{\\theta_{i}}(a_{n+1})=\\frac2d\\sum_{i\\in[d]:p_{i}>0}\\xi_{\\theta_{i}}=\\frac2d\\ge\\operatorname*{max}_{i\\in[d]:p_{i}>0}\\xi_{\\theta_{i}}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We notice that the last step holds only if $\\xi_{\\theta_{i}}\\leq2/d$ for each $i\\in[d]$ such that $p_{i}>0$ . Consequently, since the number of $\\xi_{\\theta_{i}}>0$ is equal to $d/2$ , it holds $\\xi_{\\theta_{i}}=2/\\bar{d}$ for each $i\\in[d]$ such that $p_{i}>0$ . Thus, the only posterior inducing action $a_{d+1}$ is equal to $\\begin{array}{r}{\\xi_{\\theta_{i}}^{\\prime}:=\\frac{2}{d}p_{i}}\\end{array}$ . ", "page_idx": 33}, {"type": "text", "text": "We also notice that, given $p\\,\\in\\,\\mathcal{P}$ , the optimal signaling scheme $\\gamma$ is defined as $\\gamma(\\xi^{\\prime})\\,=\\,{^1\\!/}2$ and $\\gamma(\\xi^{\\prime\\prime})={}^{1}\\!/2$ , with $\\begin{array}{r}{\\xi_{\\theta}^{\\prime\\prime}=\\mu_{\\theta}-\\frac{1}{2}\\xi_{\\theta}^{\\prime}}\\end{array}$ for each $\\theta\\in\\Theta$ . With a simple calculation, we can show that the expected sender\u2019s utility in $\\gamma$ is equal to $^1\\!/\\!2$ . ", "page_idx": 33}, {"type": "text", "text": "We set the time horizon $T\\,=\\,\\lfloor\\,|\\mathcal{P}|/4\\rfloor$ to show that any algorithm suffers regret of at least $2^{\\Omega(d)}$ . This is sufficient to prove the statement. We start by making the following simplifications about the behavior of the algorithm. First, we observe that if the algorithm can choose any posterior (instead of a signaling scheme), then this will only increase the performance of the algorithm. Consequently, we assume that the algorithm chooses a posterior $\\xi_{t}$ at each round $t\\in[T]$ . ", "page_idx": 33}, {"type": "text", "text": "Thus, we can apply Yao\u2019s minimax principle to show that any deterministic algorithm fails against a distribution over instances. In the following, we consider a uniform distribution over instances $I_{p}$ with $p\\in\\mathcal{P}$ . Furthermore, we observe that the feedback of any algorithm is actually binary. Thus, it is easy to see that an optimal algorithm works as follows: (i) it ignores the feedback whenever the action is not $a_{d+1}$ , and (ii) it starts to play the optimal posterior when the action is $a_{d+1}$ since it found an optimal posterior. ", "page_idx": 33}, {"type": "text", "text": "This observation is useful for showing that any deterministic algorithm does not find a posterior that induces action $a_{d+1}$ with a probability of at least $1-|\\mathcal{P}|/(4|\\mathcal{P}|)=3/4$ (since it can choose only $\\lfloor\\left\\lvert\\mathcal{P}\\right\\rvert\\!/\\!4\\rfloor$ posteriors among the $|\\mathcal P|$ possible optimal posteriors). Hence, by Yao\u2019s minimax principle, for any (randomized) algorithm there exists an instance such that the regret suffered in the $T$ rounds is at least: ", "page_idx": 33}, {"type": "equation", "text": "$$\nR_{T}\\geq\\frac{3}{4}\\frac{T}{2}\\geq\\frac{1}{4}\\left\\lfloor\\frac{|\\mathcal{P}|}{4}\\right\\rfloor\\geq\\frac{|\\mathcal{P}|}{32},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "since $[x]\\geq x-1\\geq x/2$ , for each $x\\geq2$ . Finally, we notice that $|\\mathcal{P}|\\,=\\,\\binom{d}{d/2}\\,=\\,2^{\\Omega(d)}$ , which concludes the proof. \u53e3 ", "page_idx": 33}, {"type": "text", "text": "Theorem 3. For any sender\u2019s algorithm, there ex\u221aists a Bayesian persuasion instance in which the regret $R_{T}$ suffered by the algorithm is at least $\\Omega({\\sqrt{T}})$ . ", "page_idx": 33}, {"type": "text", "text": "Proof. To prove the theorem, we introduce two instances characterized by two states of nature and four receiver\u2019s actions. In both the instances the sender\u2019s utility is given by $\\begin{array}{r}{\\mathcal{u}_{\\theta}^{\\mathrm{s}}(a_{1})=u_{\\theta}^{\\mathrm{s}}(a_{2})=0}\\end{array}$ for ", "page_idx": 33}, {"type": "text", "text": "all $\\theta\\in\\Theta$ , while $u_{\\theta_{1}}^{\\mathrm{s}}(a_{3})=1$ , $u_{\\theta_{2}}^{\\mathrm{s}}(a_{3})=0$ and $u_{\\theta_{2}}^{\\mathrm{s}}(a_{4})=0\\;u_{\\theta_{2}}^{\\mathrm{s}}(a_{4})=1$ . The receiver\u2019s utilities and the prior distributions in the two instances are: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\odot\\left\\{\\begin{array}{l l}{\\mu_{\\theta_{1}}^{1}=\\frac{1}{2}+\\epsilon,\\,\\mu_{\\theta_{2}}^{1}=\\frac{1}{2}-\\epsilon}\\\\ {u_{\\theta_{1}}^{1}(a_{1})=\\frac{1}{(2+4\\epsilon)},\\,u_{\\theta_{2}}^{1}(a_{1})=\\frac{1}{(10-20\\epsilon)}}\\\\ {u_{\\theta_{1}}^{1}(a_{2})=\\frac{1}{(10+20\\epsilon)},\\,u_{\\theta_{2}}^{1}(a_{2})=\\frac{1}{(2-4\\epsilon)}}\\\\ {u_{\\theta_{1}}^{1}(a_{3})=\\frac{3}{10},\\,u_{\\theta_{2}}^{1}(a_{3})=\\frac{3}{10}}\\\\ {u_{\\theta_{1}}^{1}(a_{4})=0,\\,u_{\\theta_{2}}^{1}(a_{4})=\\frac{1}{(2-4\\epsilon)}}\\end{array}\\right.\\quad\\textcircled{2}\\left\\{\\begin{array}{l l}{\\mu_{\\theta_{1}}^{2}=\\frac{1}{2}-\\epsilon,\\,\\mu_{\\theta_{2}}^{2}=\\frac{1}{2}+\\epsilon}\\\\ {u_{\\theta_{1}}^{2}(a_{1})=\\frac{1}{(2-4\\epsilon)},\\,u_{\\theta_{2}}^{2}(a_{1})=\\frac{1}{(10+20\\epsilon)}}\\\\ {u_{\\theta_{1}}^{2}(a_{2})=\\frac{1}{(10-20\\epsilon)},\\,\\,u_{\\theta_{2}}^{2}(a_{2})=\\frac{1}{(2+4\\epsilon)}}\\\\ {u_{\\theta_{1}}^{2}(a_{3})=\\frac{3}{10},\\,u_{\\theta_{2}}^{2}(a_{3})=\\frac{3}{10}}\\\\ {u_{\\theta_{1}}^{2}(a_{4})=0,\\,u_{\\theta_{2}}^{2}(a_{4})=\\frac{1}{(2+4\\epsilon)}}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "with $\\epsilon\\,\\in\\,(0,{^1\\mathord{/}{\\vphantom{^14}}4})$ . With a simple calculation, we can show that, in both the two instances, for any signaling scheme $\\phi$ employing a generic set of signals $\\boldsymbol{S}$ , the sender receives the following feedback: ", "page_idx": 34}, {"type": "text", "text": "1. $\\forall s\\in S$ such that $\\phi_{\\theta_{1}}(s)>\\phi_{\\theta_{2}}(s)$ , then $a^{\\phi}(s)=a_{1}$ .   \n2. $\\forall s\\in S$ such that $0<\\phi_{\\theta_{1}}(s)<\\phi_{\\theta_{2}}(s)$ , then $a^{\\phi}(s)=a_{2}$ .   \n3. $\\forall s\\in S$ such that $\\phi_{\\theta_{1}}(s)=\\phi_{\\theta_{2}}(s)$ , then $a^{\\phi}(s)=a_{3}$ .   \n4. $\\forall s\\in S$ such that $0=\\phi_{\\theta_{1}}(s)$ , then $a^{\\phi}(s)=a_{4}$ . ", "page_idx": 34}, {"type": "text", "text": "As a result, for any signaling scheme the sender may commit to, the resulting feedback in each signal of the signaling scheme is the same. Thus, we assume without loss of generality, that the sender only commits to signaling schemes that maximizes the probability of inducing actions $a_{3}$ or $a_{4}$ . This is because, the sender does not gain any information by committing to one signaling scheme over another, while the signaling schemes that induce these two actions are the only ones that provide the sender with strictly positive expected utility. ", "page_idx": 34}, {"type": "text", "text": "Furthermore, thanks to what we have observed before, we can restrict our attention to direct signaling schemes, i.e., those in which the set of signals coincides with the set of actions. Thus, at each round, we assume that the sender commits to a signaling scheme $\\phi^{t}$ of the form: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\phi^{t}:=\\left\\{\\phi_{\\theta_{1}}^{t}(a_{3})=\\phi_{\\theta_{2}}^{t}(a_{3}):=\\phi_{1}^{t},\\right.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "with $\\phi_{1}^{t}\\in[0,1]$ . We also notice that, in each round, the optimal signaling scheme in the first instance is the one that induces action $a_{3}$ , meaning $\\phi_{1}^{t}=1$ for each $t\\in[T]$ . While the optimal signaling scheme in the second instance at each round is the one that reveals the state of nature to the receiver, meaning $\\phi_{1}^{t}=0$ for each $t\\in[T]$ . In such a way, the learning task undertaken by the sender reduces to select a value of $\\phi_{1}^{t}\\in[0,1]$ for each $t\\in[T]$ controlling the probability of inducing action $a_{3}$ over action $a_{4}$ . ", "page_idx": 34}, {"type": "text", "text": "In the following, we define $\\mathbb{P}^{1}\\;(\\mathbb{P}^{2})$ as the probability distribution generated by the execution of a given algorithm in the first (second) instance and we let $\\mathbb{E}^{1}$ $\\left(\\mathbb{E}^{2}\\right)$ be the expectation induced by such a distribution. ", "page_idx": 34}, {"type": "text", "text": "The cumulative regret in the first instance can be written as follows: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle R_{T}^{1}=\\mathbb{E}^{1}\\left[\\sum_{t=1}^{T}\\left(\\frac{1}{2}+\\epsilon-\\phi_{1}^{t}\\left(\\frac{1}{2}+\\epsilon\\right)-\\left(1-\\phi_{1}^{t}\\right)\\left(\\frac{1}{2}-\\epsilon\\right)\\right)\\right]}}\\\\ {{\\displaystyle=2\\epsilon\\mathbb{E}\\left[\\sum_{t=1}^{T}\\left(1-\\phi_{1}^{t}\\right)\\right].}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Similarly, in the second instance, the cumulative regret is given by: ", "page_idx": 34}, {"type": "equation", "text": "$$\nR_{T}^{2}=2\\epsilon\\mathbb{E}\\left[\\sum_{t=1}^{T}\\phi_{1}^{t}\\right].\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Furthermore, it is easy to check that: ", "page_idx": 34}, {"type": "equation", "text": "$$\nR_{T}^{1}\\geq\\mathbb{P}^{1}\\left(\\sum_{t=1}^{T}\\phi_{t}^{1}\\leq T/2\\right)\\epsilon T\\quad\\mathrm{and}\\quad R_{T}^{2}\\geq\\mathbb{P}^{2}\\left(\\sum_{t=1}^{T}\\phi_{t}^{1}\\geq T/2\\right)\\epsilon T.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "By employing the relative entropy identities divergence decomposition we also have that: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{K L}\\left(\\mathbb{P}^{1},\\mathbb{P}^{2}\\right)=T\\cdot\\mathcal{K L}\\left(\\mu^{1},\\mu^{2}\\right)}\\\\ &{\\qquad\\qquad\\leq\\frac{64}{3}T\\epsilon^{2}\\leq22T\\epsilon^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where we employed the fact that for two Bernoulli distribution it holds ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathcal{K L}(\\mathrm{Be}(p),\\mathrm{Be}(q))\\leq\\frac{(p-q)^{2}}{q(1-q)}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Then, by employing the Bretagnolle\u2013Huber inequality we have that, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{T}^{1}+R_{T}^{2}\\geq\\epsilon T\\left(\\mathbb{P}^{1}\\left(\\displaystyle\\sum_{t=1}^{T}\\phi_{t}^{1}\\leq T/2\\right)+\\mathbb{P}^{2}\\left(\\sum_{t=1}^{T}\\phi_{t}^{1}\\geq T/2\\right)\\right)}\\\\ &{\\qquad\\qquad\\geq\\displaystyle\\frac{1}{2}\\epsilon T\\exp\\left(-K\\mathcal{L}\\left(\\mathbb{P}^{1},\\mathbb{P}^{2}\\right)\\right)}\\\\ &{\\qquad\\qquad\\geq\\displaystyle\\frac{1}{2}\\epsilon T\\exp\\left(-22T\\epsilon^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "By taking \u03f5 = $\\begin{array}{r}{\\epsilon=\\sqrt{\\frac{1}{22T}}}\\end{array}$ we get: ", "page_idx": 35}, {"type": "equation", "text": "$$\nR_{T}^{1}+R_{T}^{2}\\ge C_{1}\\sqrt{T}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "with $C_{1}=e^{-1}/(2\\sqrt{22})$ Thus, we have: ", "page_idx": 35}, {"type": "equation", "text": "$$\nR_{T}^{1}\\geq\\frac{C_{1}}{2}\\sqrt{T}\\quad\\vee\\quad R_{T}^{2}\\geq\\frac{C_{1}}{2}\\sqrt{T},\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "concluding the proof. ", "page_idx": 35}, {"type": "text", "text": "G Details and omitted proofs from Section 6 ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "G.1 Compute-Threshold procedure ", "page_idx": 35}, {"type": "text", "text": "The Compute-Threshold procedure takes as input a real parameter $\\epsilon_{1}\\,>\\,0$ . Then, it iteratively halves the value of a different parameter $\\epsilon$ , initially set to one, until it is smaller than or equal to $\\epsilon_{1}$ . In this way, Algorithm 12 computes a parameter $\\epsilon\\,\\in\\,[\\epsilon_{1}/2,\\epsilon_{1}]$ in ${\\mathcal{O}}(\\log(1/\\epsilon_{1}))$ rounds with bit complexity $B_{\\epsilon}\\,\\bar{=}\\,\\mathcal{O}(\\log(^{1/\\epsilon_{1}}))$ . This technical component is necessary to ensure that the bitcomplexity of the parameter $\\epsilon$ is not too large while guaranteeing that the solution returned by Algorithm 4 is still $\\gamma.$ -optimal with probability at least $1-\\eta$ . ", "page_idx": 35}, {"type": "table", "img_path": "XNpVZ8E1tY/tmp/6cba5507f1bb498b1b8ffce9785eb9552884dcd7da3c9f95dc29b7b657a911d3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 35}, {"type": "text", "text": "G.2 Omitted proofs from Section 6 ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Lemma 5. Given $\\begin{array}{r}{T_{1}:=\\left\\lceil\\frac{1}{2\\epsilon^{2}}\\log\\left(2d/\\delta\\right)\\right\\rceil}\\end{array}$ and $\\epsilon\\in(0,1)$ , Algorithm 2 employs $T_{1}$ rounds and outputs $\\mathcal{X}_{\\epsilon}\\subseteq\\mathcal{X}$ such that, with probability at least $1-\\delta$ : (i) $\\textstyle\\sum_{\\theta\\in\\Theta}\\mu_{\\theta}x_{\\theta}\\geq\\epsilon$ for every slice $x\\in\\mathcal{X}_{\\epsilon}$ , (ii) $\\textstyle\\sum_{\\theta\\in\\Theta}\\mu_{\\theta}x_{\\theta}\\leq6\\epsilon$ for every slice $x\\in\\mathcal{X}\\setminus\\mathcal{X}_{\\epsilon}$ , and (iii) $|{\\widehat{\\mu}}_{\\theta}-\\mu_{\\theta}|\\leq\\epsilon$ for every $\\theta\\in\\Theta$ . ", "page_idx": 35}, {"type": "text", "text": "Proof. Thanks to the definition of $T_{1}\\;:=\\;\\lceil1/2\\epsilon^{2}\\log\\left(2d/\\delta\\right)\\rceil$ in Algorithm 4 and employing both a union bound and the Hoeffding bound we have: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(\\left|\\widehat{\\mu}_{\\theta}-\\mu_{\\theta}\\right|\\leq\\epsilon\\right)\\geq1-\\delta,\\;\\;\\forall\\theta\\in\\Theta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Consequently, if $|{\\widehat{\\mu}}_{\\theta}-\\mu_{\\theta}|\\leq\\epsilon$ for each $\\theta\\in\\Theta$ , then for each $x\\in\\mathcal{X}_{\\epsilon}$ , the probability of inducing the slice $x$ can be lower bounded as follows: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\epsilon\\leq\\sum_{\\theta\\in\\Theta^{\\prime}}\\widehat{\\mu}_{\\theta}x_{\\theta}-\\epsilon\\leq\\sum_{\\theta\\in\\Theta^{\\prime}}(\\widehat{\\mu}_{\\theta}-\\epsilon)x_{\\theta}\\leq\\sum_{\\theta\\in\\Theta^{\\prime}}\\mu_{\\theta}x_{\\theta}\\leq\\sum_{\\theta\\in\\Theta}\\mu_{\\theta}x_{\\theta}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where the above inequalities hold because each $x\\in\\mathcal{X}_{\\epsilon}$ satisfies the constraint $\\textstyle\\sum_{\\theta\\in\\Theta^{\\prime}}\\widehat{\\mu}_{\\theta}x_{\\theta}\\geq2\\epsilon$ . Furthermore, if $|{\\widehat{\\mu}}_{\\theta}-\\mu_{\\theta}|\\leq\\epsilon$ for each $\\theta\\in\\Theta$ , then for each $x\\notin\\mathcal{X}_{\\epsilon}$ the following holds : ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\sum_{\\theta\\in\\Theta^{\\prime}}\\mu_{\\theta}x_{\\theta}\\le\\epsilon+\\sum_{\\theta\\in\\Theta^{\\prime}}(\\mu_{\\theta}-\\epsilon)x_{\\theta}\\le\\epsilon+\\sum_{\\theta\\in\\Theta^{\\prime}}\\widehat{\\mu}_{\\theta}x_{\\theta}\\le3\\epsilon,\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "since, if $x\\notin\\mathcal{X}_{\\epsilon}$ , it holds $\\begin{array}{r}{\\sum_{\\theta\\in\\Theta^{\\prime}}\\widehat{\\mu}_{\\theta}x_{\\theta}\\leq2\\epsilon}\\end{array}$ , and, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\sum_{\\theta\\notin\\Theta^{\\prime}}\\mu_{\\theta}x_{\\theta}\\le\\sum_{\\theta\\notin\\Theta^{\\prime}}(\\mu_{\\theta}-\\epsilon)x_{\\theta}+\\epsilon\\leq\\sum_{\\theta\\notin\\Theta^{\\prime}}\\widehat{\\mu}_{\\theta}x_{\\theta}+\\epsilon\\leq3\\epsilon.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Thus, by combining Inequality (11) and Inequality (12), we have: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\sum_{\\theta\\in\\Theta}\\mu_{\\theta}x_{\\theta}\\leq6\\epsilon,\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "when $x\\notin\\mathcal{X}_{\\epsilon}$ , concluding the proof. ", "page_idx": 36}, {"type": "text", "text": "Theorem 4. Given $\\gamma\\in(0,1)$ and $\\eta\\in(0,1)$ , with probability at least $1-\\eta,$ , Algorithm $^{4}$ outputs $a$ $\\gamma$ -optimal signaling scheme in a number of rounds $T$ such that: ", "page_idx": 36}, {"type": "equation", "text": "$$\nT\\leq\\widetilde{\\mathcal{O}}\\left(\\frac{n^{3}}{\\gamma^{2}}\\log^{2}\\left(\\frac{1}{\\eta}\\right)\\left(d^{8}B+d\\binom{d+n}{d}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Proof. Thanks to Lemma 5, with probability at least $1-\\delta=1-\\eta/2$ Algorithm 4 correctly completes Phase 1 in $T_{1}\\,=\\,\\mathcal{O}\\bigl(^{1}\\vert\\epsilon^{2}\\log(1/\\eta)\\mathrm{\\bar{\\log}}(d)\\bigr)$ rounds. Thus, with probability at least $1-\\eta/2$ , both the event ${\\mathcal{E}}_{1}$ and the inequalities $|{\\widehat{\\mu}}_{\\theta}-\\mu_{\\theta}|\\leq\\epsilon$ for each $\\theta\\in\\Theta$ hold. ", "page_idx": 36}, {"type": "text", "text": "Consequently, under the event ${\\mathcal{E}}_{1}$ , with probability at least $1-\\zeta=1-\\eta/2$ , Algorithm 4 correctly partitions the search space $\\textstyle{\\mathcal{X}}_{\\epsilon}$ in at most: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\widetilde{\\mathcal{O}}\\left(\\frac{n^{2}}{\\epsilon}\\log^{2}\\left(\\frac{1}{\\eta}\\right)\\left(d^{7}L+\\binom{d+n}{d}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "rounds, as stated by Lemma 3. Furthermore, we notice that $L=B+B_{\\epsilon}+B_{\\widehat{\\mu}}$ , with: ", "page_idx": 36}, {"type": "equation", "text": "$$\nB_{\\widehat{\\mu}}=\\mathcal{O}(\\log(T_{1}))=\\mathcal{O}\\left(\\log(1/\\epsilon)+\\log(1/\\eta)+\\log(d)\\right).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "As a result, with probability at least $1-\\eta$ , Algorithm 4 correctly terminates in a number of rounds $N$ which can be upper bounded as follows: ", "page_idx": 36}, {"type": "equation", "text": "$$\nN\\leq\\widetilde{\\mathcal{O}}\\left(\\frac{1}{\\epsilon^{2}}\\log\\left(\\frac{1}{\\eta}\\right)\\log(d)+\\frac{n^{2}}{\\epsilon}\\log^{2}\\left(\\frac{1}{\\eta}\\right)\\left(d^{7}(B+B_{\\epsilon})+\\binom{d+n}{d}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Furthermore, we observe that if $|{\\widehat{\\mu}}_{\\theta}-\\mu_{\\theta}|\\leq\\epsilon$ for each $\\theta\\in\\Theta$ , then the following holds: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\left|\\sum_{\\theta\\in\\Theta}\\widehat{\\mu}_{\\theta}-\\mu_{\\theta}\\right|\\leq\\sum_{\\theta\\in\\Theta}\\left|\\widehat{\\mu}_{\\theta}-\\mu_{\\theta}\\right|\\leq\\sum_{\\theta\\in\\Theta}\\epsilon=\\epsilon d.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Consequently, thanks to the result provided by Lemma 4, with probability at least $1-\\eta$ , Algorithm 4 computes a $12n\\epsilon d$ -optimal solution. Thus, by setting $\\epsilon_{1}:=\\gamma/12n d$ and $\\epsilon\\leq\\epsilon_{1}$ , with probability at least $1-\\eta$ Algorithm 4 computes a $\\gamma$ -optimal solution in a number of rounds $N$ bounded by: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{N\\leq\\widetilde{\\mathcal{O}}\\left(\\frac{n^{2}d^{2}}{\\gamma^{2}}\\log\\left(\\frac{1}{\\eta}\\right)+\\frac{n^{3}}{\\gamma}\\log^{2}\\left(\\frac{1}{\\eta}\\right)\\left(d^{8}(B+B_{\\epsilon})+d\\binom{d+n}{d}\\right)\\right)}\\\\ &{\\quad=\\widetilde{\\mathcal{O}}\\left(\\frac{n^{3}}{\\gamma^{2}}\\log^{2}\\left(\\frac{1}{\\eta}\\right)\\left(d^{8}(B+B_{\\epsilon})+d\\binom{d+n}{d}\\right)\\right)}\\\\ &{\\quad=\\widetilde{\\mathcal{O}}\\left(\\frac{n^{3}}{\\gamma^{2}}\\log^{2}\\left(\\frac{1}{\\eta}\\right)\\left(d^{8}B+d\\binom{d+n}{d}\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where the last equality holds because the bit-complexity of $\\epsilon$ is $B_{\\epsilon}=\\mathcal{O}(\\log({n d/\\gamma}))$ , concluding the proof. \u53e3 ", "page_idx": 36}, {"type": "text", "text": "Theorem 5. There exist two absolute constants $\\kappa,\\lambda>0$ such that no algorithm is guaranteed to return a \u03ba-optimal signaling scheme with probability of at least $1-\\lambda$ by employing less than $2^{\\Omega(n)}$ and $2^{\\Omega(d)}$ rounds, even when the prior distribution $\\mu$ is known to the sender. ", "page_idx": 37}, {"type": "text", "text": "Proof. In the proof of Theorem 2 we showed that, with probability $^{3}\\!/\\!4$ , in $N\\,=\\,\\lfloor|\\mathcal{P}|/4\\rfloor$ rounds any algorithm does not correctly identify the posterior inducing action $a_{d+1}$ . This is because, any deterministic algorithm can identify the optimal posterior only in $\\left[|\\mathcal{P}|/4\\right]$ instances, as observed in the proof of Theorem 2. ", "page_idx": 37}, {"type": "text", "text": "As a result, in the remaining $|\\mathcal P|-N$ instances, any deterministic algorithm will receive the same feedback and thus will always output the same posterior after $N$ rounds, which will result in the optimal one in only a single instance. ", "page_idx": 37}, {"type": "text", "text": "Thus, thanks to the Yao\u2019s minimax principle, there is no algorithm that is guaranteed to return an optimal solution with probability at least: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\frac{3}{4}\\left(\\frac{|\\mathcal{P}|-N-1}{|\\mathcal{P}|-N}\\right)\\geq\\frac{3}{8}\\left(\\frac{|\\mathcal{P}|-N}{|\\mathcal{P}|-N}\\right)=\\frac{3}{8}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Finally, we observe that $\\mathrm{OPT}={}^{1/2}$ , while any algorithm that does not induce the posterior $\\xi^{\\prime}$ provides an expected utility equal to zero. As a result, for each $\\kappa<1/2$ there is no algorithm that is guaranteed to return a solution which is $\\kappa$ -optimal in $\\lfloor|\\mathcal{P}|/4\\rfloor=2^{\\Omega(d)}$ rounds with probability at least $3/8$ . ", "page_idx": 37}, {"type": "text", "text": "Theorem 6. Given $\\gamma\\in(0,{^1\\!/8})$ and $\\eta\\in(0,1)$ , no algorithm is guaranteed to return a $\\gamma$ -optimal signaling scheme with probability at least $1-\\eta$ by employing less than $\\begin{array}{r}{\\Omega\\big(\\frac{1}{\\gamma^{2}}\\log({1/\\eta})\\big)}\\end{array}$ rounds. ", "page_idx": 37}, {"type": "text", "text": "Proof. To prove the theorem, we consider the same instance and the same definitions introduced in the proof of Theorem 3. In this case, we let $\\mathbb{P}^{1}$ $\\textstyle\\left(\\mathbb{P}^{2}\\right)$ be the probability distribution generated by the execution of a given algorithm in the first (second) instance for $\\dot{N}=\\lceil\\log(1/4\\eta)\\bar{/}2\\epsilon^{2}\\rceil$ rounds. Furthermore, we introduce the event $\\mathcal{E}$ , under which the signaling scheme returned at the round $N$ , according to the definition presented in Equation 10, is such that $\\phi^{N}\\leq1/2$ . We notice that, if such signaling scheme is such that $\\phi^{N}<1/2$ , then the sender\u2019s expected utility in the first instance is smaller or equal to $1/2$ , thus being $\\epsilon/2$ -optimal. At the same time, if $\\phi^{N}\\overset{*}{\\underset{=}{\\geq}}1/2$ in the second instance, then the solution returned by the algorithm is not $\\epsilon/2$ -optimal. ", "page_idx": 37}, {"type": "text", "text": "Thus, by employing the Bretagnolle\u2013Huber inequality we have: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{P}^{1}\\left({\\mathcal{E}}\\right)+\\mathbb{P}^{2}\\left({\\mathcal{E}}^{C}\\right)\\geq{\\frac{1}{2}}\\exp\\left(-K{\\mathcal{L}}\\left(\\mathbb{P}^{1},\\mathbb{P}^{2}\\right)\\right)\\geq{\\frac{1}{2}}\\exp\\left(-22N\\epsilon^{2}\\right),\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "since $\\kappa\\mathcal{L}\\left(\\mathbb{P}^{1},\\mathbb{P}^{2}\\right)\\,\\leq\\,22N\\epsilon^{2}$ , as observed in the proof of Theorem 6. Finally, by employing the definition of $N$ , we have: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{P}^{1}\\left(\\mathcal{E}\\right)\\geq\\eta~~\\vee~~\\mathbb{P}^{2}\\left(\\mathcal{E}^{C}\\right)\\geq\\eta.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "As a result, by setting $2\\gamma=\\epsilon$ , the statement of the lemma holds. ", "page_idx": 37}, {"type": "text", "text": "H Sample complexity with known prior ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "In this section, we discuss the Bayesian persuasion PAC-learning problem when the prior distribution $\\mu$ is known to the sender. To tackle the problem, we propose Algorithm 13. The main difference with respect to Algorithm 4 is that, in this case, we do not need to employ the Build-Search-Space procedure, as the prior is already known to the sender. This allows us to compute a $\\gamma$ -optimal signaling scheme in only $\\mathcal{O}(^{1}\\!/\\gamma)$ rounds, instead of ${\\mathcal{O}}({^{1}\\!/}{\\gamma^{2}})$ rounds as in the case with an unknown prior. ", "page_idx": 37}, {"type": "text", "text": "Require: $\\eta\\in(0,1),\\gamma\\in(0,1),\\mu\\in\\Delta_{\\Theta}$   \n1: $\\epsilon_{1}\\leftarrow\\gamma/10n d$   \n2: $\\epsilon\\gets$ Compute-Epsilon(\u03f51), ${\\widehat{\\mu}}\\gets\\mu$   \n3: $\\widetilde{\\Theta}\\gets\\{\\theta\\in\\Theta\\;|\\;\\widehat{\\mu}_{\\theta}>2\\epsilon\\}$   \n4: $\\mathcal{X}_{\\epsilon}\\gets\\{x\\in\\mathcal{X}\\;|\\;\\sum_{\\theta\\in\\widetilde{\\Theta}}\\widehat{\\mu}_{\\theta}x_{\\theta}\\geq2\\epsilon\\}$   \n5: $\\mathcal{R}_{\\epsilon}\\leftarrow$ Find-Polytope s $(\\lambda_{\\epsilon},\\eta)$   \n6: $\\phi\\gets\\mathsf{C o m p u t e-S i g n a l i n g}(\\mathcal{R}_{\\epsilon},\\mathcal{X}_{\\epsilon},\\mu)$   \n7: return $\\phi$ ", "page_idx": 38}, {"type": "text", "text": "In this case, the following theorem holds. ", "page_idx": 38}, {"type": "text", "text": "Theorem 7. With probability at least $1-\\eta$ and in Algorithm 2 computes a $\\gamma$ -optimal signaling scheme in $\\widetilde{\\mathcal{O}}\\left({}^{n^{3}}/\\gamma\\log^{2}\\left(1/\\eta\\right)\\left(d^{8}B+d\\binom{d+n}{d}\\right)\\right)$ rounds. ", "page_idx": 38}, {"type": "text", "text": "Proof. Since $\\widehat{\\mu}=\\mu$ , the clean event ${\\mathcal{E}}_{1}$ holds with probability one. Consequently, thanks to Lemma 3, with probabil i ty at least $1-\\eta$ , Algorithm 13 correctly partitions the search space $\\textstyle{\\mathcal{X}}_{\\epsilon}$ in at most: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\widetilde{\\mathcal{O}}\\left(\\frac{n^{2}}{\\epsilon}\\log^{2}\\left(1/\\eta\\right)\\left(d^{7}L+\\binom{d+n}{d}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "rounds, with $L:=B+B_{\\epsilon}+B_{\\widehat{\\mu}}$ . According to Algorithm 13, we have $\\epsilon\\le\\epsilon_{1}:=\\gamma/(10n d)$ . As a result, $L=\\mathcal{O}\\left(B+\\log(n d)+\\log(1/\\gamma)\\right)$ , since $B_{\\widehat{\\mu}}\\leq B$ and $B_{\\epsilon}=\\mathcal{O}(\\log(1/\\epsilon_{1}))=\\mathcal{O}(\\log(n d)+\\log(1/\\gamma))$ . Furthermore, under the event $\\mathcal{E}_{2}$ , Algorithm 13 computes a 10\u03f5nd-optimal solution, as guaranteed by Lemma 4, with $\\widehat{\\mu}=\\mu$ . ", "page_idx": 38}, {"type": "text", "text": "Thus, with probability at least $1-\\eta$ , Algorithm 13 computes a $\\gamma.$ -optimal solution in: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\widetilde{\\mathcal{O}}\\left(\\frac{n^{3}}{\\gamma}\\log^{2}\\left(1/\\eta\\right)\\bigg(d^{8}B+d\\bigg(d+n\\bigg)\\bigg)\\right)\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "rounds, concluding the proof. ", "page_idx": 38}, {"type": "text", "text": "We notice that, differently from the case with an unknown prior, it is possible to achieve a $\\mathcal{O}\\left(\\log(1/\\eta)/\\gamma\\right)$ upper bound with respect to the input parameters $\\gamma,\\eta>0$ . Finally, we show that such a dependence is tight, as shown in the following theorem. ", "page_idx": 38}, {"type": "text", "text": "Theorem 8. Given $\\gamma,\\eta>0$ no algorithm is guaranteed to return an $\\gamma$ -optimal signaling scheme with probability of at least $1-\\eta$ employing $\\Omega\\left(\\log(1/\\eta)/\\gamma\\right)$ rounds, even when the prior distribution is known to the sender. ", "page_idx": 38}, {"type": "text", "text": "Proof. We consider two instances characterized by two states of nature and three receiver\u2019s actions. The two instances share the same prior distribution, defined as $\\mu_{\\theta_{1}}=4\\gamma$ and $\\mu_{\\theta_{2}}=1-4\\gamma$ . In both the instances the sender\u2019s utility is given by $u_{\\theta}^{\\mathrm{s}}(a_{1})=0$ , $u_{\\theta}^{\\mathrm{s}}(a_{2})=1/2$ and $u_{\\theta}^{\\mathrm{s}}(a_{3})=1$ for all $\\theta\\in\\Theta$ Furthermore, we assume that the receiver\u2019s utility in the two instances are given by: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{\\odot}\\left\\{\\begin{array}{l l}{u_{\\theta_{1}}(a_{1})=1,\\,u_{\\theta_{2}}(a_{1})=1/2}\\\\ {u_{\\theta_{1}}(a_{2})=1/2,\\,u_{\\theta_{2}}(a_{2})=1}\\\\ {u_{\\theta_{1}}(a_{3})=1,\\,u_{\\theta_{2}}(a_{3})=0}\\end{array}\\right.\\qquad\\qquad}&{\\mathcal{D}\\left\\{\\begin{array}{l l}{u_{\\theta_{1}}(a_{1})=1,\\,u_{\\theta_{2}}(a_{1})=1/2}\\\\ {u_{\\theta_{1}}(a_{2})=1/2,\\,u_{\\theta_{2}}(a_{2})=1}\\\\ {u_{\\theta_{1}}(a_{3})=1/2,\\,u_{\\theta_{2}}(a_{3})=0}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "We observe that the only case in which the sender receives different feedback in the two instances is when they induce the posterior distribution $\\xi_{1}:=(1,0)$ . This is because, when the sender induces $\\xi_{1}$ in the first instance, the receiver plays the action $a_{3}\\in A$ , breaking ties in favor of the sender, while in the second instance, the receiver plays the action $a_{1}\\in\\mathcal{A}$ . Such a posterior can be induced, in both the two instances, with a probability of at most $\\gamma$ to be consistent with the prior. ", "page_idx": 38}, {"type": "text", "text": "We also observe that in the first instance the optimal sender\u2019s signaling scheme $\\gamma$ is such that $\\gamma(\\xi_{1})=4\\gamma$ and $\\gamma(\\xi_{2})=1-4\\gamma$ , where we let $\\xi_{2}:=(0,1)$ . Furthermore, the sender\u2019s expected utility in $\\gamma$ is equal to $(1+4\\gamma)/2$ . In the second instance, the optimal sender\u2019s signaling scheme $\\gamma$ is such that $\\gamma(\\xi_{2})=1-8\\gamma$ and $\\gamma(\\xi_{3})=8\\gamma$ , with $\\xi_{3}:=(1/2,1/2)$ . It is easy to verify that such a signaling scheme achieves an expected utility of $1/2$ . ", "page_idx": 38}, {"type": "text", "text": "", "page_idx": 39}, {"type": "text", "text": "In the following, we let $\\mathbb{P}^{1}$ and $\\mathbb{P}^{2}$ be the probability measures induced by the interconnection of a given algorithm executed in the first and in the second instances, respectively. Furthermore, we introduce the event $E_{N}$ , under which, during the first $N$ rounds, the sender never observes the action $a_{3}\\in A$ . It is easy to verify that such an event holds with a probability of at least $\\mathbb{P}^{1}(E_{N})\\ge(1\\!-\\!4\\gamma)^{N}$ in the first instance. This is because, at each round, the action $a_{3}$ can be observed with a probability of at most $4\\gamma$ . In contrast, since in the second instance the receiver never plays the action $a_{3}$ , it holds $\\mathbb{P}^{2}(E_{N})=\\mathrm{i}\\geq(1-4\\gamma)^{N}$ . ", "page_idx": 39}, {"type": "text", "text": "Then, by letting $\\gamma_{1}^{N}~(\\gamma_{2}^{N})$ be the signaling scheme returned after $N$ rounds in the first (second) instance, we have: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathbb{P}^{1}\\left(\\gamma_{1}^{N}(\\xi_{1})\\geq2\\gamma\\right)+\\mathbb{P}^{2}\\left(\\gamma_{2}^{N}(\\xi_{1})\\leq2\\gamma\\right)\\geq\\mathbb{P}^{1}\\left(\\gamma_{1}^{N}(\\xi_{1})\\geq2\\gamma,E_{N}\\right)+\\mathbb{P}^{2}\\left(\\gamma_{2}^{N}(\\xi_{1})\\leq2\\gamma,E_{N}\\right)}&{}\\\\ {\\geq\\mathbb{P}^{1}\\left(\\gamma_{1}^{N}(\\xi_{1})\\geq2\\gamma\\left|E_{N}\\right)\\mathbb{P}^{1}\\left(E_{N}\\right)+\\mathbb{P}^{2}\\left(\\gamma_{2}^{N}(\\xi_{1})\\leq2\\gamma\\left|E_{N}\\right)\\mathbb{P}^{2}\\left(E_{N}\\right)}&{}\\\\ {\\geq\\left(\\mathbb{P}^{1}\\left(\\gamma_{1}^{N}(\\xi_{1})\\geq2\\gamma\\left|E_{N}\\right)+\\mathbb{P}^{2}\\left(\\gamma_{2}^{N}(\\xi_{1})\\leq2\\gamma\\left|E_{N}\\right)\\right)\\mathbb{P}^{1}\\left(E_{N}\\right)}&{}\\\\ {=\\mathbb{P}^{1}\\left(E_{N}\\right)}&{}\\\\ {\\geq(1-4\\gamma)^{N}\\geq2\\eta.}&{}&{(1-4\\gamma)^{N}\\geq2\\eta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "The above result holds observing that, under the event $E_{N}$ , the behaviour of any algorithm working in the first instance coincides with the behaviour of the same algorithm working in the second one, as they receive the same feedback. Furthermore, Inequality 13 holds when $N$ is such that: ", "page_idx": 39}, {"type": "equation", "text": "$$\nN\\leq\\frac{\\log(1/2\\eta)}{10\\gamma}\\leq\\frac{\\log(2\\eta)}{\\log(1-4\\gamma)},\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "if $\\gamma\\leq1/5$ . ", "page_idx": 39}, {"type": "text", "text": "Finally, we observe that if $\\gamma_{1}^{N}(\\xi_{1})\\leq2\\gamma$ , then the sender\u2019s expected utility in the first instance is of most $1/2+\\gamma$ . This is because, for any signaling scheme $\\gamma_{1}^{N}$ , we have: ", "page_idx": 39}, {"type": "equation", "text": "$$\nu^{s}(\\gamma_{1}^{N})\\leq\\gamma_{1}^{N}(\\xi_{1})u^{s}(\\xi_{1})+\\frac{1}{2}\\left(1-\\gamma_{1}^{N}(\\xi_{1})\\right)=\\frac{1}{2}+\\gamma.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Thus, if $\\gamma_{1}^{N}(\\xi_{1})\\leq2\\gamma$ the the signaling scheme $\\gamma_{1}^{N}$ is at most $\\gamma$ -optimal. ", "page_idx": 39}, {"type": "text", "text": "Equivalently, if the sender\u2019s final signaling scheme $\\gamma_{2}^{N}$ in the second instance is such that $\\gamma_{2}^{N}(\\xi_{1})\\geq$ $2\\gamma$ , then the sender\u2019s utility in the second instance is of at most $1/2-\\gamma$ . Thus, if $\\gamma_{2}^{N}(\\xi_{1})\\bar{\\geq2}\\gamma$ the the signaling scheme $\\gamma_{2}^{N}$ is at most $\\gamma$ -optimal. Then, we either have: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathbb{P}^{1}\\left(\\gamma_{1}^{N}(\\xi_{1})\\leq2\\gamma\\right)\\geq\\eta\\;\\;\\mathrm{or}\\;\\;\\mathbb{P}^{2}\\left(\\gamma_{2}^{N}(\\xi_{1})\\geq2\\gamma\\right)\\geq\\eta,\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "showing that if the number of rounds $N\\leq\\log(1/2\\eta)/(10\\gamma)$ , there exists an instance such that no algorithm is guaranteed to return a $\\gamma$ -optimal signaling scheme with probability greater than or equal to $1-\\eta$ . \u53e3 ", "page_idx": 39}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: The abstract and the introduction state all the main contributions of this work. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 40}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Justification: All the assumptions are stated in the paper. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 40}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: The assumptions needed are reported in the statements of the theorems and lemmas, while all the proofs are reported in the appendices. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 41}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 4.1 If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 4.2 If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 4.3 If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4.4 We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 41}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 41}, {"type": "text", "text": "Answer:[NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: The paper does not include experiments. Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 42}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: The paper does not include experiments. Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 42}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 42}, {"type": "text", "text": "", "page_idx": 43}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 43}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: The paper conforms with the NeurIPS Code of Ethics. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 43}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: There is no societal impact of the work performed, since the work is mainly theoretical. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 43}, {"type": "text", "text": "", "page_idx": 44}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 44}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 44}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 45}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 45}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 45}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 45}]