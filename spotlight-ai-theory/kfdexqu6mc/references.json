{"references": [{"fullname_first_author": "Arthur Jacot", "paper_title": "Neural tangent kernel: Convergence and generalization in neural networks", "publication_date": "2018-12-01", "reason": "This paper introduced the Neural Tangent Kernel (NTK) which is central to the current work's theoretical analysis of neural network training."}, {"fullname_first_author": "Jaehoon Lee", "paper_title": "Wide neural networks of any depth evolve as linear models under gradient descent", "publication_date": "2019-12-01", "reason": "This paper significantly advanced the understanding of the NTK by providing theorems on its convergence during training, which are key to analyzing the surrogate gradient learning method."}, {"fullname_first_author": "Sanjeev Arora", "paper_title": "On exact computation with an infinitely wide neural net", "publication_date": "2019-12-01", "reason": "This paper contributed to the theoretical foundation of the NTK by showing that infinitely wide neural networks can perform exact computations under certain conditions."}, {"fullname_first_author": "Alexander G. de G. Matthews", "paper_title": "Gaussian process behaviour in wide deep neural networks", "publication_date": "2018-04-11", "reason": "This foundational paper established the connection between wide neural networks and Gaussian processes, providing the theoretical basis for understanding NTK behavior in the infinite-width limit."}, {"fullname_first_author": "Adityanarayanan Radhakrishnan", "paper_title": "Wide and deep neural networks achieve consistency for classification", "publication_date": "2023-04-04", "reason": "This paper connected the NTK theory to the problem of classification, offering insight into the behavior of networks with activation functions that lack a well-defined derivative, such as the sign activation function."}]}