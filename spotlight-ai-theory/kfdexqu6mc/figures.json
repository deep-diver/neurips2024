[{"figure_path": "kfdEXQu6MC/figures/figures_7_1.jpg", "caption": "Figure 1: We plot empirical and analytic NTKs of 10 networks for different hidden layer widths n and activation functions erfm. The kernels are plotted at initialization and after gradient descent training with t = 1e4 time steps, learning rate \u03b7 = 0.1, and MSE error. The y-axis is asinh-scaled.", "description": "This figure shows the convergence of empirical NTKs to their analytic counterparts for various network widths (n) and activation functions (erfm with different m values). The plots are shown for both network initialization (t=0) and after training for 10000 time steps. The y-axis uses the inverse hyperbolic sine transformation to better visualize the convergence.", "section": "3 Numerical experiments"}, {"figure_path": "kfdEXQu6MC/figures/figures_8_1.jpg", "caption": "Figure 2: We plot empirical and analytic SG-NTKs of ten networks for different hidden layer widths n and activation functions erfm. The kernels are plotted at initialization and after surrogate gradient learning with t = 1e4 time steps, learning rate \u03b7 = 0.1, MSE error, and surrogate derivative \u1ee1 = erf.", "description": "This figure shows the convergence of empirical and analytical surrogate gradient Neural Tangent Kernels (SG-NTKs) in the infinite-width limit.  The plots compare empirical SG-NTKs from ten networks with different hidden layer widths (n = 10, 100, 500, 1000) and activation functions erfm (m = 2, 5, 20)  to the analytical SG-NTKs at initialization and after gradient-descent training.  The y-axis is scaled using the inverse hyperbolic sine function (asinh) for better visualization of the convergence behavior. The results demonstrate that the empirical SG-NTKs converge to the analytical ones as the network width increases, supporting the theoretical findings about the SG-NTK's convergence.", "section": "Numerical experiments"}, {"figure_path": "kfdEXQu6MC/figures/figures_8_2.jpg", "caption": "Figure 3: Comparison of SGL learning in networks with different hidden layer widths with SG-NTK predictions. (a) 500 networks (blue) with sign activation function and hidden layer widths n = 500 trained with SGL using the surrogate derivative \u1ee1 = erf for t = 3e4 time steps plotted together with their mean (cyan), the SG-NTK-GP's mean (black) and confidence band (grey), and the Esign kernel regression (dashed). Training points are indicated with crosses. (b) The mean of ensembles of 500 networks is plotted as in (a) for different hidden layer widths.", "description": "This figure compares the results of surrogate gradient learning (SGL) with the surrogate gradient neural tangent kernel (SG-NTK).  Panel (a) shows the results for networks with hidden layer width of 500, comparing the mean network prediction (blue) with the mean and confidence bounds of the Gaussian process prediction (black line and grey area). The mean network predictions using SGL agrees well with the SG-NTK prediction. Panel (b) demonstrates that the agreement between the SG-NTK and the SGL mean prediction is even better with increasing network width (100 and 20).", "section": "3 Numerical experiments"}, {"figure_path": "kfdEXQu6MC/figures/figures_14_1.jpg", "caption": "Figure B.1: Target function and training points for the numerical experiments, f(x,y) = 4xy\u00b2 \u2212 0.8x\u00b3 + 1.2y\u00b2 \u2212 0.8x\u00b2y.", "description": "This figure shows the target function used in the numerical experiments of the paper, along with the training data points.  The function is a 2D surface plotted against angle \u03b1, showing its oscillating, non-linear nature. The training points are superimposed on the curve, illustrating the data points used to train and validate the models.", "section": "B Additional figures"}, {"figure_path": "kfdEXQu6MC/figures/figures_14_2.jpg", "caption": "Figure 3: Comparison of SGL learning in networks with different hidden layer widths with SG-NTK predictions. (a) 500 networks (blue) with sign activation function and hidden layer widths n = 500 trained with SGL using the surrogate derivative \u1ee1 = erf for t = 3e4 time steps plotted together with their mean (cyan), the SG-NTK-GP's mean (black) and confidence band (grey), and the Esign kernel regression (dashed). Training points are indicated with crosses. (b) The mean of ensembles of 500 networks is plotted as in (a) for different hidden layer widths.", "description": "This figure compares the results of surrogate gradient learning (SGL) with the predictions of the surrogate gradient neural tangent kernel (SG-NTK).  Panel (a) shows the distribution of 500 networks trained using SGL, their mean, the SG-NTK Gaussian process (GP) mean and confidence intervals, and the Esign kernel regression. Panel (b) shows the same but for different network widths. The agreement between SGL and SG-NTK indicates that SG-NTK provides a good characterization of SGL.", "section": "3 Numerical experiments"}, {"figure_path": "kfdEXQu6MC/figures/figures_15_1.jpg", "caption": "Figure B.3: Mean squared errors between empirical NTKs and analytic NTKs in Figure 1 for all 10 networks (thin lines) and averaged over the 10 networks (thick lines).", "description": "This figure shows the mean squared errors between the empirical and analytical NTKs from Figure 1.  The thin lines represent the MSE for each of the 10 individual networks, while the thicker lines show the average MSE across all 10 networks.  The plot illustrates the convergence of the empirical NTKs to their analytical counterparts as the hidden layer width (n) increases, for different values of the parameter m (which controls the sharpness of the activation function approximation).", "section": "B Additional figures"}, {"figure_path": "kfdEXQu6MC/figures/figures_15_2.jpg", "caption": "Figure B.3: Mean squared errors between empirical NTKs and analytic NTKs in Figure 1 for all 10 networks (thin lines) and averaged over the 10 networks (thick lines).", "description": "This figure shows the mean squared error between the empirical and analytical NTKs plotted in Figure 1.  The thin lines represent the MSE for each of the ten individual networks, while the thick line shows the average MSE across all ten networks. The plot is broken down by the parameter 'm' (which determines the steepness of the approximation to the sign function) and the hidden layer width 'n'. This visualization helps to demonstrate the convergence of empirical NTKs to their analytical counterparts as the network width increases.", "section": "B Additional figures"}]