[{"figure_path": "LEzx6QRkRH/figures/figures_1_1.jpg", "caption": "Figure 1: An overview of RL-GPT. After the optimization in an environment, LLMs agents obtain optimized coded actions, RL achieves an optimized neural network, and our RL-GPT gets both optimized coded actions and neural networks. Our framework integrates the coding parts and the learning parts.", "description": "This figure provides a comparison of three different approaches for solving tasks in an environment: using only GPT (Large Language Models), using only Reinforcement Learning (RL), and using a combined approach called RL-GPT.  The GPT approach uses LLMs to generate optimized coded actions. The RL approach uses a neural network to learn optimized actions. RL-GPT combines both approaches, leveraging the strengths of LLMs for high-level planning and RL for low-level control to achieve both optimized coded actions and an optimized neural network.  The line graph shows the successful rate over epochs, demonstrating RL-GPT's improved performance over the other two methods. ", "section": "1 Introduction"}, {"figure_path": "LEzx6QRkRH/figures/figures_1_2.jpg", "caption": "Figure 2: To learn a subtask, the LLM can generate environment configurations (task, observation, reward, and action space) to instantiate RL. In particular, by reasoning about the agent behavior to solve the subtask, the LLM generates code to provide higher-level actions in addition to the original environment actions, improving the sample efficiency for RL.", "description": "This figure illustrates how LLMs can be used to improve the efficiency of reinforcement learning (RL) by generating environment configurations and code that provide higher-level actions.  The LLM reasons about the agent's behavior to solve a subtask, then generates code for higher-level actions, supplementing the original environment actions.  This integration of LLMs and RL increases sample efficiency in the RL process. The diagram shows the LLM generating code that becomes part of a policy network, which takes observations from the environment and produces actions.", "section": "1 Introduction"}, {"figure_path": "LEzx6QRkRH/figures/figures_3_1.jpg", "caption": "Figure 3: Overview of RL-GPT. The overall framework consists of a slow agent (orange) and a fast agent (green). The slow agent decomposes the task and determines \u201cwhich actions\u201d to learn. The slow agent will improve the decision based on the high-level action feedbacks. The fast agent writes code and RL configurations. The fast agent debugs the written code based on the environment feedback (\u201cDirect Code Implementation\u201d). Correct codes will be inserted into the action space as high-level actions (\u201cRL Implementation\u201d).", "description": "This figure illustrates the overall architecture of RL-GPT, a two-agent hierarchical framework. The slow agent (orange) focuses on high-level task decomposition, deciding which actions should be implemented using code and which should be learned via reinforcement learning (RL).  The fast agent (green) is responsible for generating and refining the code for coded actions and setting up the RL pipeline for the remaining actions. There is an iterative process involving feedback from the environment to optimize both agents. The figure also showcases the specific tasks and interactions within each agent.", "section": "3 Methods"}, {"figure_path": "LEzx6QRkRH/figures/figures_3_2.jpg", "caption": "Figure 4: The two-loop iteration. We design a method to optimize both slow agent and fast agent with a critic agent.", "description": "This figure illustrates the iterative optimization process used in RL-GPT. It shows two loops: one for the slow agent which decomposes the tasks and decides which actions to code or learn using RL, and the other for the fast agent that generates code and configures the RL training. A critic agent provides feedback to both the slow and fast agents, allowing them to iteratively refine their decisions and improve performance.", "section": "3 Methods"}, {"figure_path": "LEzx6QRkRH/figures/figures_6_1.jpg", "caption": "Figure 5: Demonstrations of how different agents learn to harvest a log. While both RL agent and LLM agent learn a single type of solution (RL or code-as-policy), our RL-GPT can reasonably decompose the task and correct how to learn each sub-action through the slow iteration process. RL-GPT decomposes the task into \u201cfind a tree\u201d and \u201ccut a log\u201d, solving the former with code generation and the latter with RL. After a few iterations, it learns to provide RL with a necessary high-level action (attack 20 times) and completes the task with a high success rate. Best viewed by zooming in.", "description": "This figure demonstrates how different approaches to solving the task of harvesting a log in Minecraft differ in their success rates.  MineAgent, relying solely on reinforcement learning (RL), achieves only a 10% success rate.  RL-GPT, in its initial iterations (iter-0 and iter-1), also performs poorly. However, through the iterative process of task decomposition and integrating code-as-policy, RL-GPT gradually improves its performance, reaching a 58% success rate by iter-3. This showcases the effectiveness of RL-GPT's two-level hierarchical framework, which combines the strengths of LLMs and RL for efficient task completion. The figure shows a sequence of images illustrating the agent's actions at each stage of the process.", "section": "4.3 Main Results"}, {"figure_path": "LEzx6QRkRH/figures/figures_21_1.jpg", "caption": "Figure 3: Overview of RL-GPT. The overall framework consists of a slow agent (orange) and a fast agent (green). The slow agent decomposes the task and determines \u201cwhich actions\u201d to learn. The slow agent will improve the decision based on the high-level action feedbacks. The fast agent writes code and RL configurations. The fast agent debugs the written code based on the environment feedback (\"Direct Code Implementation\"). Correct codes will be inserted into the action space as high-level actions (\"RL Implementation\").", "description": "This figure provides a high-level overview of the RL-GPT framework. It shows two main agents: a slow agent (orange) responsible for task decomposition and determining which actions are best suited for coding versus reinforcement learning, and a fast agent (green) responsible for writing code and configuring reinforcement learning, and debugging generated code based on the environment's feedback. The framework iteratively refines both agents' decisions.  The image illustrates how the framework combines code and reinforcement learning to solve tasks efficiently.", "section": "3 Methods"}, {"figure_path": "LEzx6QRkRH/figures/figures_22_1.jpg", "caption": "Figure 6: Qualitative demonstrations in the Furniture environment show that our motion planning action effectively aids in furniture assembly, whereas baselines struggle to find the correct location.", "description": "This figure compares the performance of the proposed RL-GPT model against baseline methods on a furniture assembly task in a simulated environment.  The top row shows a baseline agent's attempts to assemble the furniture, which result in the robot arm repeatedly inserting parts into incorrect positions. The bottom row demonstrates the RL-GPT agent successfully assembling the furniture. This is achieved by utilizing motion planning as an action in the agent's action space; this allows the agent to efficiently locate the correct positions for inserting parts.", "section": "4.3 Main Results"}, {"figure_path": "LEzx6QRkRH/figures/figures_23_1.jpg", "caption": "Figure 7: The training curves in the Kitchen environment. Integrating coded motion planning and RL accelerates learning. RL-GPT learns faster compared to the RL baseline.", "description": "This figure shows the success rate of RL and RL-GPT on four different tasks in the Kitchen environment over a certain number of timesteps.  RL-GPT consistently outperforms the baseline RL approach, demonstrating faster learning and improved performance by integrating coded motion planning into the RL process. The results indicate that incorporating high-level coded actions with RL enhances learning efficiency for complex tasks.", "section": "4 Experiments"}, {"figure_path": "LEzx6QRkRH/figures/figures_23_2.jpg", "caption": "Figure 6: Qualitative demonstrations in the Furniture environment show that our motion planning action effectively aids in furniture assembly, whereas baselines struggle to find the correct location.", "description": "This figure showcases a qualitative comparison of the proposed RL-GPT method against baseline methods for furniture assembly tasks within the Furniture environment.  The top row demonstrates a baseline approach, showing the robot arm's struggles to accurately locate and assemble parts of a piece of furniture. In contrast, the bottom row illustrates the RL-GPT approach, demonstrating significantly improved accuracy in part location and successful assembly. The improved performance highlights the effectiveness of incorporating coded motion planning actions into the RL framework for enhanced task completion. The images provide a visual representation of the steps involved in the furniture assembly task for both the baseline and the RL-GPT methods.", "section": "4.3 Main Results"}, {"figure_path": "LEzx6QRkRH/figures/figures_23_3.jpg", "caption": "Figure 9: VLMs can offer more precise critiques based on vision feedback.", "description": "This figure demonstrates how Vision-Language Models (VLMs) provide more detailed and precise feedback compared to LLMs.  It shows examples from two different environments: Minecraft (Harvest Milk task) and a driving simulation. In both cases, the VLM not only identifies whether the agent succeeded or failed but also explains the reasons for success or failure. For instance, in the Minecraft example, the VLM points out that the agent was incorrectly attacking the ground instead of the cow; while in the driving simulation example, the VLM correctly observes that the vehicle was gradually drifting off the road. This more nuanced feedback allows for faster improvement in both the slow agent's task planning and the fast agent's code generation.", "section": "4.3 Main Results"}, {"figure_path": "LEzx6QRkRH/figures/figures_23_4.jpg", "caption": "Figure 10: MuJoCo example. GPT-4 can code an action to reverse the car and then move it forward.", "description": "This figure shows a comparison of a baseline approach and the RL-GPT approach on a MuJoCo task. The task involves navigating a car along a winding road. The baseline approach is less efficient, while the RL-GPT approach is able to successfully navigate the car by using GPT-4 to generate code for reversing the car and then moving it forward. This demonstrates the capability of the RL-GPT framework to generate code for complex actions that can improve the efficiency of RL agents. ", "section": "G Broader Impact"}]