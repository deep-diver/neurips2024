[{"figure_path": "kr7eN85mIT/figures/figures_1_1.jpg", "caption": "Figure 1: VATT is a flexible audio generative model capable of generating audio in two modes: i) When a silent video is the sole input, the model generates the audio along with a caption describing the possible audio that could match the video. ii) When in addition to the video, a text prompt is provided, the model generates audio aligned with both the video and the given text prompt.", "description": "This figure illustrates the architecture of the proposed Video-to-Audio Through Text (VATT) model, highlighting its two main operational modes.  The first mode takes only a silent video as input and generates corresponding audio along with an automatically generated caption describing the audio. The second mode adds a user-provided text prompt as an additional input, enabling more refined control over the generated audio by aligning it with both the visual content and the textual description.", "section": "1 Introduction"}, {"figure_path": "kr7eN85mIT/figures/figures_4_1.jpg", "caption": "Figure 2: Two stages of VATT system training pipeline: (1) Video-to-Caption stage that maps video features into an audio caption through LLM. (2) Video + Text to Audio stage that learns to generate audio tokens through masked tokens prediction conditioned on Stage (1) features.", "description": "This figure illustrates the two-stage training pipeline of the VATT model. Stage 1 (Video-to-Caption) uses an LLM with a projection layer to map video features into an audio caption. The LLM is fine-tuned using LoRA on V2A instruction data to enable audio caption generation. Stage 2 (Video + Text to Audio) uses a bi-directional transformer to generate audio tokens conditioned on video and textual features (or generated captions in Stage 1).  Masked parallel decoding is employed in this stage.  The generated audio tokens are then converted into a waveform using a neural audio codec. The figure highlights the flow of information, including video frames, textual prompts, features from the visual encoder and LLM, masked audio tokens, and final audio waveforms.", "section": "3 Methods"}, {"figure_path": "kr7eN85mIT/figures/figures_5_1.jpg", "caption": "Figure 3: Audio Tokens Decoder: VATT Audio is a bi-directional transformer that models the audio tokens and the conditioning inputs (LLM hidden states) jointly. We extract the part that corresponds to the audio features and apply L Linear layers in parallel to perform classification on masked tokens at each codebook layer.", "description": "The figure shows the architecture of the VATT Audio, which is a bi-directional transformer decoder. It takes as input masked audio tokens and the hidden states from the last layer of the VATT Converter (which processes video and text features).  The decoder uses multi-head self-attention to model the relationship between the audio tokens and the conditioning inputs.  A feed-forward network (FFN) further processes the output of the self-attention layer. Finally, L linear layers are used in parallel to classify the masked audio tokens at each codebook layer of the Encodec neural audio codec. This process is used to generate audio waveforms.", "section": "3.2.1 Audio Token Decoder"}, {"figure_path": "kr7eN85mIT/figures/figures_8_1.jpg", "caption": "Figure 2: Two stages of VATT system training pipeline: (1) Video-to-Caption stage that maps video features into an audio caption through LLM. (2) Video + Text to Audio stage that learns to generate audio tokens through masked tokens prediction conditioned on Stage (1) features.", "description": "This figure illustrates the two-stage training pipeline of the proposed Video-to-Audio Through Text (VATT) model. Stage 1, Video-to-Caption, uses an LLM to generate an audio caption from video features.  Stage 2, Video + Text to Audio, takes the caption (or an optional user-provided text prompt) and the video features as input to a bi-directional transformer decoder. This decoder uses masked token modeling to generate audio tokens, which are subsequently converted to an audio waveform.", "section": "3 Methods"}, {"figure_path": "kr7eN85mIT/figures/figures_16_1.jpg", "caption": "Figure 5: Qualitative samples that showcase text controllability: For same video inputs, VATT is able to generate different sounds that align with the additional text prompts, showcasing its capability of performing controllable generation.", "description": "This figure demonstrates VATT's ability to generate different audio outputs from the same video input by using different text prompts. Three video clips are presented, each paired with three different text prompts.  The resulting spectrograms of the generated audio for each prompt-video pair are displayed. This illustrates how VATT leverages text to refine audio generation, providing control over the final output's characteristics.", "section": "B Qualitative Examples and Analysis"}, {"figure_path": "kr7eN85mIT/figures/figures_17_1.jpg", "caption": "Figure 6: Steering generation towards ground truth audio: For same video inputs, we compare our generation results without text prompt v.s feeding ground truth audio caption as additional prompt.", "description": "This figure compares the audio generated by the VATT model with and without ground truth audio captions as input. The left column shows the video input. The middle column shows the spectrogram of audio generated without ground truth captions (no prompt). The right column shows the spectrogram of audio generated with ground truth captions (with prompt), along with a textual description of the generated audio. This illustrates how the model's output can be steered towards more accurate and relevant audio when ground truth captions are provided.", "section": "3.2 Video + Text to Audio Stage"}, {"figure_path": "kr7eN85mIT/figures/figures_18_1.jpg", "caption": "Figure 7: Video-to-Audio Captioning samples by VATT.", "description": "This figure shows nine example images from the VGGSound dataset used to evaluate the VATT model. Each image is accompanied by a caption describing the sounds that could be heard in the video.  These examples highlight the model's ability to generate audio captions that accurately reflect the acoustic events occurring in the videos.", "section": "Details and Examples of V2A Instruction Dataset"}, {"figure_path": "kr7eN85mIT/figures/figures_22_1.jpg", "caption": "Figure 2: Two stages of VATT system training pipeline: (1) Video-to-Caption stage that maps video features into an audio caption through LLM. (2) Video + Text to Audio stage that learns to generate audio tokens through masked tokens prediction conditioned on Stage (1) features.", "description": "This figure illustrates the two-stage training pipeline of the VATT model. Stage 1 (Video-to-Caption) uses a large language model (LLM) with a projection layer to convert video features into audio captions. This stage is used to generate text descriptions of the audio for a given video. Stage 2 (Video + Text to Audio) uses an encoder-decoder architecture where the encoder is the finetuned LLM from Stage 1, and the decoder is a bi-directional transformer that generates audio tokens. The decoder is trained using masked token modeling to predict masked audio tokens given the context from the video and text prompts.  This stage generates the actual audio waveform conditioned on the video and optional text.", "section": "3 Methods"}]