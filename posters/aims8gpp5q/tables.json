[{"figure_path": "AiMs8GPP5q/tables/tables_7_1.jpg", "caption": "Table 1: Absolute values of Pearson correlation coefficients (R2) between various representational measurements and the neural predictivity across each of the four IT datasets. The correlation was measured over each value of \u03bb and base objective function for a total of 21 networks. Each column corresponds to a panel in Fig. 2, except for Hue, which is the regression score obtained for the random hue modulation parameter in isolation.", "description": "This table presents the Pearson correlation coefficients (R-squared values) between different representational measurements and the neural predictivity across four IT datasets.  The representational measures include augmentation-augmentation distance, augmentation-centroid distance, spatial-photometric distance, class-class distance, and parameter regression scores. The neural predictivity is assessed using the BrainScore evaluation pipeline.  A total of 21 networks were evaluated, varying both the base objective function and the hyperparameter \u03bb (lambda), which controls the strength of the equivariance loss.  The table helps to understand which representational characteristics contribute most to the model's ability to predict neural activity.", "section": "3.2 Representational Analyses"}, {"figure_path": "AiMs8GPP5q/tables/tables_8_1.jpg", "caption": "Table 2: Frozen-Linear Evaluation for invariant and equivariant trained networks on 6 different downstream datasets: Cifar-10/100 [Krizhevsky et al., 2009], Oxford-Pets Parkhi et al. [2012], Describable Textures Database [Cimpoi et al., 2014], Flowers-102 [Nilsback and Zisserman, 2008], and Food-101 [Bossard et al., 2014]. We closely follow the evaluation procedure from [Lee et al., 2021] (see Appendix A.4 for details) and report top1 accuracy for each objective/dataset. In all cases we report the mean over 5 runs of the evaluation procedure, we observed very little variability (maximum of .2%, over all evaluations, we report the standard deviation over runs in Appendix A.4. The equivariant networks are denoted by prepending a \u201cCE\u201d before the objective and were trained using \u03bb = 0.1, which enabled a substantial amount of structure variability without significantly impacting frozen-linear classification on the SSL training dataset (see Appendix A.2). For ImageNet-1k trained networks out of distribution performance decreased for most evaluations, while for ImageNet-100 trained networks performance was improved in 15 of 18 cases.", "description": "This table presents the results of a frozen linear evaluation comparing the performance of invariant and equivariant self-supervised learning models on six different downstream datasets.  The table shows the top-1 accuracy for each model (invariant and equivariant versions of MMCR, Barlow Twins, and SimCLR) on each dataset, highlighting the impact of incorporating equivariance into the self-supervised learning framework.  The results are averaged across five independent runs, with standard deviations reported in the appendix.", "section": "3.4 Transfer Learning"}, {"figure_path": "AiMs8GPP5q/tables/tables_8_2.jpg", "caption": "Table 2: Frozen-Linear Evaluation for invariant and equivariant trained networks on 6 different downstream datasets: Cifar-10/100 [Krizhevsky et al., 2009], Oxford-Pets Parkhi et al. [2012], Describable Textures Database [Cimpoi et al., 2014], Flowers-102 [Nilsback and Zisserman, 2008], and Food-101 [Bossard et al., 2014]. We closely follow the evaluation procedure from [Lee et al., 2021] (see Appendix A.4 for details) and report top1 accuracy for each objective/dataset. In all cases we report the mean over 5 runs of the evaluation procedure, we observed very little variability (maximum of .2%, over all evaluations, we report the standard deviation over runs in Appendix A.4. The equivariant networks are denoted by prepending a \u201cCE\u201d before the objective and were trained using \u03bb = 0.1, which enabled a substantial amount of structure variability without significantly impacting frozen-linear classification on the SSL training dataset (see Appendix A.2). For ImageNet-1k trained networks out of distribution performance decreased for most evaluations, while for ImageNet-100 trained networks performance was improved in 15 of 18 cases.", "description": "This table presents the top-1 accuracy results of frozen linear evaluation on six different downstream datasets (Cifar-10, Cifar-100, Oxford-Pets, Describable Textures Database, Flowers-102, Food-101) for both invariant and equivariant trained networks.  The equivariant networks were trained with a hyperparameter \u03bb = 0.1, balancing invariance and equivariance. The table compares performance across different self-supervised learning objectives (MMCR, Barlow Twins, SimCLR) and shows the impact of incorporating equivariance on transfer learning capabilities for ImageNet-100 and ImageNet-1k pretrained models.", "section": "3.4 Transfer Learning"}, {"figure_path": "AiMs8GPP5q/tables/tables_14_1.jpg", "caption": "Table 3: In distribution accuracy on the validation set of ImageNet-100 evaluated by online-linear classification.", "description": "This table shows the results of online linear classification on the ImageNet-100 validation set.  It compares the performance of three different self-supervised learning methods (MMCR, Barlow Twins, and SimCLR) with their contrastive-equivariant counterparts (CE-MMCR, CE-Barlow, CE-SimCLR). The accuracy of each model is presented as a percentage.", "section": "A.3 Online-Linear Evaluation for the Pretraining Dataset"}, {"figure_path": "AiMs8GPP5q/tables/tables_15_1.jpg", "caption": "Table 2: Frozen-Linear Evaluation for invariant and equivariant trained networks on 6 different downstream datasets: Cifar-10/100 [Krizhevsky et al., 2009], Oxford-Pets Parkhi et al. [2012], Describable Textures Database [Cimpoi et al., 2014], Flowers-102 [Nilsback and Zisserman, 2008], and Food-101 [Bossard et al., 2014]. We closely follow the evaluation procedure from [Lee et al., 2021] (see Appendix A.4 for details) and report top1 accuracy for each objective/dataset. In all cases we report the mean over 5 runs of the evaluation procedure, we observed very little variability (maximum of .2%, over all evaluations, we report the standard deviation over runs in Appendix A.4. The equivariant networks are denoted by prepending a \u201cCE\u201d before the objective and were trained using \u03bb = 0.1, which enabled a substantial amount of structure variability without significantly impacting frozen-linear classification on the SSL training dataset (see Appendix A.2). For ImageNet-1k trained networks out of distribution performance decreased for most evaluations, while for ImageNet-100 trained networks performance was improved in 15 of 18 cases.", "description": "This table presents the results of a transfer learning experiment, comparing the performance of invariant and equivariant trained networks on six different downstream datasets.  It shows the top-1 accuracy for each objective function (MMCR, Barlow Twins, SimCLR) and training dataset (ImageNet-100, ImageNet-1k), with and without the addition of an equivariance loss. The results highlight the impact of incorporating equivariance on the generalization capability of the models.", "section": "3.4 Transfer Learning"}, {"figure_path": "AiMs8GPP5q/tables/tables_17_1.jpg", "caption": "Table 5: Table defining the sources of variability producing covariance matrices and the random variables that the expected Bures distance is computed over for the experiments in Fig. 2.", "description": "This table details the sources of variability used to calculate the covariance matrices (C1 and C2) and the random variables used to compute the expected Bures distance for the representational analyses in Figure 2 of the paper.  It clarifies how the covariance matrices are derived based on augmentations of images and how the expected Bures distance is calculated for each type of comparison (augmentation-augmentation, augmentation-centroid, spatial-photometric, and class-class).", "section": "3.2 Representational Analyses"}]