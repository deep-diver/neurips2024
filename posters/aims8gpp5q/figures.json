[{"figure_path": "AiMs8GPP5q/figures/figures_2_1.jpg", "caption": "Figure 1: Diagram of the proposed training method. At the beginning of each training epoch the dataset is randomly split into two non-overlapping halves. Left Gray Panel: corresponding images in each subset are augmented using the same set of two random transformations (so the total number of random transformations is halved relative to a standard invariant SSL training scheme). Every view is passed through a representation network f (ResNet-50 in this work) and the outputs are projected into two embedding spaces by different projector networks, ginv and gequi. In the invariant embedding space a standard iSSL loss is applied, while in the equivariant embedding space the same iSSL loss is applied to the difference vectors between transformation-positive pairs (visualized to the right).", "description": "This figure illustrates the contrastive-equivariant self-supervised learning (CE-SSL) training method.  The dataset is split into two halves. Each image in each half is augmented with the same transformation.  The augmented images pass through a ResNet-50 network and then are projected into two embedding spaces, one for invariant and one for equivariant losses.  The invariant loss is applied to the pairs of images. The equivariant loss is applied to the difference vector between transformation pairs. ", "section": "2 Method"}, {"figure_path": "AiMs8GPP5q/figures/figures_4_1.jpg", "caption": "Figure 2: Representational analyses. In all cases increasing \u03bb indicates increased importance of the equivariance loss term. A: Description of augmentation-augmentation distance, where each gray ellipsoid represents a single augmentation manifold and the expected distance is lower when they are aligned. B: Description of augmentation-centroid distance, where gray ellipsoids represent augmentation manifolds and the blue disk represents the centroid manifold. Expected distance is lower when augmentation variability is factorized (orthogonal to) the centroid manifold. C: Description of spatial-photometric distance where gray ellipsoids represent single augmentation manifolds, and blue/yellow points indicate the mean over the outputs from many spatial/photometric transformations of a single view obtained via a photometric/spatial transformation respectively. Expected distance is larger when the two sources of variability are factorized. D: Same as A., but for class manifolds. E: A schematic of the parameter regression experiment. In each panel, the bottom row depicts the results of each analysis described in the text of Sections 3.2 and 3.2. All shaded regions indicate 95% confidence intervals (estimated over the same comparisons the expected distance is estimated over for A-D and over 5 independent runs of the regression experiments for E).", "description": "This figure presents a representational analysis of the model's learned features. It shows how the model's representation of images changes with the strength of the equivariance loss.  The analysis uses several metrics (augmentation-augmentation distance, augmentation-centroid distance, spatial-photometric distance, and class-class distance) to quantify the structure and organization of the learned representations. It also shows the results of a parameter regression experiment, which assesses the model's ability to predict neural responses based on the learned representations.", "section": "3.2 Representational Analyses"}, {"figure_path": "AiMs8GPP5q/figures/figures_6_1.jpg", "caption": "Figure 3: Brain-Score (noise-ceiled predictivity evaluated via ridge regression) for each value of \u03bb (different colored bars) for each IT dataset (groups of columns) and base objective functions (different figure panels). For all datasets and base objectives the invariant network (\u03bb = 0, lightest bars) is outperformed by at least one equivariant network, and the spread in predictivity over values of \u03bb is significantly larger than the spread in predictivity over base objective functions for invariant networks.", "description": "This figure displays the neural predictivity results for different values of lambda (\u03bb), representing the strength of the equivariance loss.  The x-axis represents four different IT datasets, and each group of bars represents different base objective functions (MMCR, Barlow Twins, SimCLR). The height of each bar indicates the neural predictivity score for a given dataset, objective function, and \u03bb value. The results show that invariant networks (\u03bb = 0) are generally outperformed by at least one of the equivariant networks for each dataset and objective function. Moreover, the improvement in predictivity varies significantly depending on the \u03bb value, indicating the importance of the hyperparameter in balancing invariance and equivariance.", "section": "3 Results"}, {"figure_path": "AiMs8GPP5q/figures/figures_15_1.jpg", "caption": "Figure 2: Representational analyses. In all cases increasing \u03bb indicates increased importance of the equivariance loss term. A: Description of augmentation-augmentation distance, where each gray ellipsoid represents a single augmentation manifold and the expected distance is lower when they are aligned. B: Description of augmentation-centroid distance, where gray ellipsoids represent augmentation manifolds and the blue disk represents the centroid manifold. Expected distance is lower when augmentation variability is factorized (orthogonal to) the centroid manifold. C: Description of spatial-photometric distance where gray ellipsoids represent single augmentation manifolds, and blue/yellow points indicate the mean over the outputs from many spatial/photometric transformations of a single view obtained via a photometric/spatial transformation respectively. Expected distance is larger when the two sources of variability are factorized. D: Same as A., but for class manifolds. E: A schematic of the parameter regression experiment. In each panel, the bottom row depicts the results of each analysis described in the text of Sections 3.2 and 3.2. All shaded regions indicate 95% confidence intervals (estimated over the same comparisons the expected distance is estimated over for A-D and over 5 independent runs of the regression experiments for E).", "description": "This figure presents representational analyses to determine the impact of the equivariance loss on the learned representations.  It uses the Bures metric to quantify the relationships between different sources of variability (augmentation-augmentation, augmentation-centroid, spatial-photometric, class-class).  The results show that incorporating the equivariance loss leads to a more structured and factorized representation of variability, aligning better with known features of visual perception.", "section": "3.2 Representational Analyses"}, {"figure_path": "AiMs8GPP5q/figures/figures_16_1.jpg", "caption": "Figure 2: Representational analyses. In all cases increasing \u03bb indicates increased importance of the equivariance loss term. A: Description of augmentation-augmentation distance, where each gray ellipsoid represents a single augmentation manifold and the expected distance is lower when they are aligned. B: Description of augmentation-centroid distance, where gray ellipsoids represent augmentation manifolds and the blue disk represents the centroid manifold. Expected distance is lower when augmentation variability is factorized (orthogonal to) the centroid manifold. C: Description of spatial-photometric distance where gray ellipsoids represent single augmentation manifolds, and blue/yellow points indicate the mean over the outputs from many spatial/photometric transformations of a single view obtained via a photometric/spatial transformation respectively. Expected distance is larger when the two sources of variability are factorized. D: Same as A., but for class manifolds. E: A schematic of the parameter regression experiment. In each panel, the bottom row depicts the results of each analysis described in the text of Sections 3.2 and 3.2. All shaded regions indicate 95% confidence intervals (estimated over the same comparisons the expected distance is estimated over for A-D and over 5 independent runs of the regression experiments for E).", "description": "This figure presents the results of representational analyses performed to evaluate the impact of the contrastive-equivariant self-supervised learning (CE-SSL) method on the structure of learned representations.  It shows how different measures of variability (augmentation-augmentation distance, augmentation-centroid distance, spatial-photometric distance, and class-class distance) change with increasing emphasis on the equivariance loss (\u03bb).  The results suggest that CE-SSL leads to more structured variability, where different sources of variability are better factorized (orthogonal) from each other.  A parameter regression experiment is also schematically shown.", "section": "3.2 Representational Analyses"}, {"figure_path": "AiMs8GPP5q/figures/figures_17_1.jpg", "caption": "Figure 2: Representational analyses. In all cases increasing \u03bb indicates increased importance of the equivariance loss term. A: Description of augmentation-augmentation distance, where each gray ellipsoid represents a single augmentation manifold and the expected distance is lower when they are aligned. B: Description of augmentation-centroid distance, where gray ellipsoids represent augmentation manifolds and the blue disk represents the centroid manifold. Expected distance is lower when augmentation variability is factorized (orthogonal to) the centroid manifold. C: Description of spatial-photometric distance where gray ellipsoids represent single augmentation manifolds, and blue/yellow points indicate the mean over the outputs from many spatial/photometric transformations of a single view obtained via a photometric/spatial transformation respectively. Expected distance is larger when the two sources of variability are factorized. D: Same as A., but for class manifolds. E: A schematic of the parameter regression experiment. In each panel, the bottom row depicts the results of each analysis described in the text of Sections 3.2 and 3.2. All shaded regions indicate 95% confidence intervals (estimated over the same comparisons the expected distance is estimated over for A-D and over 5 independent runs of the regression experiments for E).", "description": "This figure presents a series of representational analyses that investigate the impact of the proposed contrastive-equivariant self-supervised learning (CE-SSL) method on learned representations.  It shows how different measures of variability (augmentation-augmentation distance, augmentation-centroid distance, spatial-photometric distance, and class-class distance) change as the importance of the equivariance loss (\u03bb) is increased.  Additionally, a schematic of the parameter regression experiment used to evaluate the linearly decodable information regarding augmentations is shown. The results are presented as line plots for each comparison, illustrating the relationship between the Bures metric distance and \u03bb across multiple base objectives (MMCR, Barlow Twins, and SimCLR).", "section": "3.2 Representational Analyses"}, {"figure_path": "AiMs8GPP5q/figures/figures_18_1.jpg", "caption": "Figure 2: Representational analyses. In all cases increasing \u03bb indicates increased importance of the equivariance loss term. A: Description of augmentation-augmentation distance, where each gray ellipsoid represents a single augmentation manifold and the expected distance is lower when they are aligned. B: Description of augmentation-centroid distance, where gray ellipsoids represent augmentation manifolds and the blue disk represents the centroid manifold. Expected distance is lower when augmentation variability is factorized (orthogonal to) the centroid manifold. C: Description of spatial-photometric distance where gray ellipsoids represent single augmentation manifolds, and blue/yellow points indicate the mean over the outputs from many spatial/photometric transformations of a single view obtained via a photometric/spatial transformation respectively. Expected distance is larger when the two sources of variability are factorized. D: Same as A., but for class manifolds. E: A schematic of the parameter regression experiment. In each panel, the bottom row depicts the results of each analysis described in the text of Sections 3.2 and 3.2. All shaded regions indicate 95% confidence intervals (estimated over the same comparisons the expected distance is estimated over for A-D and over 5 independent runs of the regression experiments for E).", "description": "This figure presents a series of representational analyses to determine how variability in the dataset is organized in both invariant and equivariant networks.  It shows the Bures distance (a measure of the distance between Gaussian distributions) between different sources of variability, such as augmentation-augmentation, augmentation-centroid, spatial-photometric, and class-class.  The results demonstrate that incorporating an equivariance loss leads to factorized variability (i.e., different sources of variability are more orthogonal), which is further supported by a parameter regression experiment showing that augmentation parameters can be linearly decoded more effectively from equivariant networks.", "section": "3.2 Representational Analyses"}]