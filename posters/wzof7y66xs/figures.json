[{"figure_path": "wzof7Y66xs/figures/figures_1_1.jpg", "caption": "Figure 1: A detailed example of HSC for the output of ViT-L/16-384 on a specific sample. The base classifier outputs leaf softmax scores, with internal node scores being the sum of their descendant leaves' scores, displayed in parentheses next to each node. The base classifier incorrectly classifies the image as a 'Golden Retriever' with low confidence. A selective classifier can either make the same incorrect leaf prediction if the confidence threshold is below 0.29, or reject the sample. A hierarchical selective classifier with the Climbing inference rule (see Section 3) climbs the path from the predicted leaf to the root until the confidence threshold \u03b8 is met. Setting \u03b8 above 0.29 yields a hierarchically correct prediction, with smaller \u03b8 values increasing the coverage. An Algorithm for determining the optimal threshold is introduced in Section 4.", "description": "This figure illustrates how Hierarchical Selective Classification (HSC) works using the example of a Labrador Retriever image classified by a ViT-L/16-384 model.  The base classifier initially misclassifies the image as a 'Golden Retriever' with low confidence.  HSC, however, leverages a hierarchical class structure (shown as a tree) to provide a more accurate, higher-level classification ('Dog') when uncertainty exists at the leaf node level. The Climbing inference rule is used to ascend the hierarchy until a sufficient confidence threshold is reached, improving both accuracy and coverage.", "section": "1 Introduction"}, {"figure_path": "wzof7Y66xs/figures/figures_4_1.jpg", "caption": "Figure 2: (a) hierarchical RC curve of a ViT-L/16-384 model trained on ImageNet1k, evaluated with the 0/1 loss as the risk and softmax response as its confidence function \u03ba. The purple shaded area represents the area under the RC curve (hAURC). Full coverage occurs when the model accepts all leaf predictions, for which the risk is 0.13. Increasing the confidence threshold leads to the rejection of more samples. For example, when the threshold is 0.77 the the risk is 0.04, with coverage 0.8. (b) hierarchical RC curves of different inference rules with EVA-L/14-196 [14] as the base classifier. When the coverage is 1.0, all inference rules predict leaves. Each inference rule achieves a different trade-off, resulting in distinct curves. This example represents the prevalent case, where the \u201chierarchically-ignorant\u201d selective inference rule performs the worst and Climbing outperforms MC.", "description": "This figure shows two hierarchical risk-coverage (RC) curves.  (a) demonstrates a single model's performance, highlighting the relationship between risk and coverage as the confidence threshold changes. The shaded area represents the hierarchical area under the RC curve (hAURC). (b) compares three different inference rules for hierarchical selective classification using a different base model.  It shows how each rule achieves a distinct trade-off between risk and coverage, illustrating the advantages of the \"Climbing\" rule.", "section": "3 Hierarchical Selective Inference Rules"}, {"figure_path": "wzof7Y66xs/figures/figures_7_1.jpg", "caption": "Figure 3: Individual model examples comparing the hierarchical selective threshold algorithm against DARTS, with each algorithm repeated 1000 times. The mean and median results are shown in dark green. The light green area shows the  interval around the target accuracy, and the remaining area is marked in red (i.e., each repetition has a 1 \u2013  probability of being in the green area and a  probability of being in the red area). The target accuracy is 95% and 1 \u2013  = 0.9. In both examples, the target accuracy error of DARTS is high, and the entirety of its accuracy distribution lies outside of the confidence interval. Left: EVA-Giant/14 [14]. DARTS fails to meet the constraint, whereas our algorithm's mean accuracy is very close to the target. Right: ResNet-152 [50]. While our algorithm has a near-perfect mean accuracy, DARTS rejects all samples, resulting in zero coverage.", "description": "This figure compares the performance of the proposed hierarchical selective threshold algorithm against DARTS for two different models (EVA-Giant/14 and ResNet-152). It visualizes the distribution of accuracy results obtained from 1000 runs of each algorithm for a target accuracy of 95% and a confidence level of 90%.  The plots show that the proposed algorithm consistently achieves higher accuracy closer to the target, whereas DARTS either fails to meet the accuracy constraint or leads to zero coverage.", "section": "4 Optimal Selective Threshold Algorithm"}, {"figure_path": "wzof7Y66xs/figures/figures_7_2.jpg", "caption": "Figure 4: Comparison of different methods by their improvement in hAURC, relative to the same model's performance without the method. The number of models evaluated for each method: knowledge distillation: 42, pretraining: 61, CLIP: 16, semi-supervised learning: 11, adversarial training: 8.", "description": "This figure shows a box plot comparing the improvement in hierarchical area under the risk-coverage curve (hAURC) achieved by different training methods (knowledge distillation, pretraining on ImageNet21k or ImageNet12k, contrastive language-image pretraining (CLIP), semi-supervised learning, and adversarial training) relative to a baseline model trained without these methods.  The x-axis represents the training method, and the y-axis represents the percentage improvement in hAURC. Each data point represents a single model, and the box plot shows the median, quartiles, and range of improvement for each method.", "section": "Empirical Study of Training Regimes"}, {"figure_path": "wzof7Y66xs/figures/figures_8_1.jpg", "caption": "Figure 5: Aggregated (mean and SEM) CC curves of 1,115 ImageNet models.", "description": "This figure shows the aggregated Calibration-Coverage (CC) curves for 1115 ImageNet models.  The CC curves plot the Expected Calibration Error (ECE) against coverage.  Two inference rules, Selective and Climbing, are compared; Climbing generally shows better calibration than Selective, indicating that hierarchical selective classification improves calibration.", "section": "Hierarchical Calibration"}, {"figure_path": "wzof7Y66xs/figures/figures_18_1.jpg", "caption": "Figure 3: Individual model examples comparing the hierarchical selective threshold algorithm against DARTS, with each algorithm repeated 1000 times. The mean and median results are shown in dark green. The light green area shows the  interval around the target accuracy, and the remaining area is marked in red (i.e., each repetition has a 1 \u2013  probability of being in the green area and a  probability of being in the red area). The target accuracy is 95% and 1 \u2013  = 0.9. In both examples, the target accuracy error of DARTS is high, and the entirety of its accuracy distribution lies outside of the confidence interval. Left: EVA-Giant/14 [14]. DARTS fails to meet the constraint, whereas our algorithm's mean accuracy is very close to the target. Right: ResNet-152 [50]. While our algorithm has a near-perfect mean accuracy, DARTS rejects all samples, resulting in zero coverage.", "description": "This figure compares the performance of the proposed hierarchical selective threshold algorithm against DARTS using individual model examples. Each algorithm was run 1000 times with a target accuracy of 95% and a confidence level of 90%. The plot shows the mean and median accuracy errors, along with the confidence intervals for each algorithm.  The results demonstrate that the proposed algorithm consistently achieves lower accuracy errors and higher coverage compared to DARTS.", "section": "Optimal Selective Threshold Algorithm"}, {"figure_path": "wzof7Y66xs/figures/figures_19_1.jpg", "caption": "Figure 7: Comparison between zero-shot and linear-probe CLIP models sharing the same backbone. The round markers represent the zero-shot models, and the star markers represent their respective linear-probes.", "description": "This figure compares the hierarchical selective performance of zero-shot CLIP models and their linear-probe counterparts across different backbones. Each point represents a model, with the x-coordinate representing AUROC and the y-coordinate representing -log(hAURC). The round markers show results for zero-shot CLIP models, while the star markers show results for linear-probe CLIP models.  The plot shows that linear-probe CLIP models consistently outperform their zero-shot counterparts in terms of both AUROC and hAURC (lower hAURC indicates better performance).", "section": "Empirical Study of Training Regimes"}]