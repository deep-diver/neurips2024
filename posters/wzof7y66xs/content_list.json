[{"type": "text", "text": "Hierarchical Selective Classification ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Shani Goren\\* Ido Galil\\* Technion Technion, NVIDIA shani.goren@gmail.com idogalil.ig@gmail.com, igalil@nvidia.com ", "page_idx": 0}, {"type": "text", "text": "Ran El-Yaniv Technion, NVIDIA rani@cs.technion.ac.il, relyaniv@nvidia.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deploying deep neural networks for risk-sensitive tasks necessitates an uncertainty estimation mechanism. This paper introduces hierarchical selective classification (HSC), extending selective classification to a hierarchical setting. Our approach leverages the inherent structure of class relationships, enabling models to reduce the specificity of their predictions when faced with uncertainty. In this paper, we first formalize hierarchical risk and coverage, and introduce hierarchical riskcoverage curves. Next, we develop algorithms for performing HSC (which we refer to as \u201cinference rules\u201d), and propose an efficient algorithm that guarantees a target accuracy constraint with high probability. We also introduce CalibrationCoverage curves, which analyze the effect of hierarchical selective classification on calibration. Our extensive empirical studies on over a thousand ImageNet classifiers reveal that training regimes such as CLIP, pretraining on ImageNet21k, and knowledge distillation boost hierarchical selective performance. Lastly, we show that HSC improves both selective performance and confidence calibration. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deep neural networks (DNNs) have achieved incredible success across various domains, including computer vision and natural language processing. To ensure the reliability of models intended for real-world applications we must incorporate an uncertainty estimation mechanism, such as selective classification [19], which allows a model to abstain from classifying samples when it is uncertain about their predictions. Standard selective classification, however, has an inherent shortcoming: for any given sample, it is limited to either making a prediction or rejecting it, thereby ignoring potentially useful information about the sample, in case of rejection. ", "page_idx": 0}, {"type": "text", "text": "To illustrate the potential consequences of this limitation, consider a model trained for classifying different types of brain tumors from MRI scans, including both benign and malignant tumors. If a malignant tumor is identified with high confidence, immediate action is needed. For a particular hypothetical scan, suppose the model struggles to distinguish between 3 subtypes of malignant tumors, assigning a confidence score of 0.33 to each of them (assuming those estimates are well-calibrated and sum to 1). In the traditional selective classification framework, if the confidence threshold is higher than 0.33, the model will reject the sample, failing to provide valuable information to alert healthcare professionals about the patient\u2019s condition, even though the model has enough information to conclude that the tumor is malignant with high certainty. This could potentially result in delayed diagnosis and treatment, posing a significant risk to the patient\u2019s life. However, a hierarchically-aware selective model with an identical confidence threshold would classify the tumor as malignant with $99\\%$ confidence. Although this prediction is less specific, it remains highly valuable as it can promptly notify healthcare professionals, leading to early diagnosis and life-saving treatment for the patient. This motivates us to propose hierarchical selective classification (HSC), an extension of selective classification to a setting where the classes are organized in a hierarchical structure. Such hierarchies are typically represented by a tree-like structure, where each node represents a class, and the edges reflect a semantic relationship between the classes, most commonly an \u2019is-a\u2019 relationship. Datasets with an existing hierarchy are fairly common, as there are many well-established predefined hierarchies available, such as WordNet [33] and taxonomic data for plants and animals. Consequently, popular datasets like ImageNet [11], which is widely used in computer vision tasks, and iNaturalist [24], have been built upon these hierarchical structures, providing ready-to-use trees. The ImageNet dataset, for example, is organized according to the WordNet hierarchy. A visualization of a small portion of the ImageNet hierarchy is shown in Figure 1. In cases where a hierarchical tree structure is not provided with the dataset, it can be automatically generated using methods such as leveraging LLMs [7, 17, 28, 57]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "image", "img_path": "wzof7Y66xs/tmp/83eae0cc4d0041770c1293987b0f0e969dbf9b3ca2b846abd755d4f2933dbec8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: A detailed example of HSC for the output of ViT-L/16-384 on a specific sample. The base classifier outputs leaf softmax scores, with internal node scores being the sum of their descendant leaves\u2019 scores, displayed in parentheses next to each node. The base classifier incorrectly classifies the image as a \u2019Golden Retriever\u2019 with low confidence. A selective classifier can either make the same incorrect leaf prediction if the confidence threshold is below 0.29, or reject the sample. A hierarchical selective classifier with the Climbing inference rule (see Section 3) climbs the path from the predicted leaf to the root until the confidence threshold $\\theta$ is met. Setting $\\theta$ above 0.29 yields a hierarchically correct prediction, with smaller $\\theta$ values increasing the coverage. An Algorithm for determining the optimal threshold is introduced in Section 4. ", "page_idx": 1}, {"type": "text", "text": "The key contributions of this paper are as follows: ", "page_idx": 1}, {"type": "text", "text": "(1) We extend selective classification to a hierarchical setting. We define hierarchical selective risk and hierarchical coverage, leading us to introduce hierarchical risk-coverage curves. ", "page_idx": 1}, {"type": "text", "text": "(2) We introduce hierarchical selective inference rules, i.e., algorithms used to hierarchically reduce the information in predictions based on their uncertainty estimates, improving hierarchical selective performance compared to existing baselines. We also identify and define useful properties of inference rules. ", "page_idx": 1}, {"type": "text", "text": "(3) We propose a novel algorithm to find the optimal confidence threshold compatible with any base classifier without requiring any fine-tuning, that achieves a user-defined target accuracy with high probability, which can also be set by the user, greatly improving over the existing baseline. ", "page_idx": 1}, {"type": "text", "text": "(4) We conduct a comprehensive empirical study evaluating HSC on more than 1,000 ImageNet classifiers. We present numerous previously unknown observations, most notably that training approaches such as pretraining on larger datasets, contrastive language-image pretraining (CLIP) [35], and knowledge distillation significantly boost selective hierarchical performance. Furthermore, we define hierarchical calibration and discover that HSC consistently improves confidence calibration. ", "page_idx": 1}, {"type": "text", "text": "2 Problem Setup ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Selective Classification: Let $\\mathcal{X}$ be the input space, and $\\boldsymbol{\\wp}$ be the label space. Samples are drawn from an unknown joint distribution $P(\\mathcal X\\times\\mathcal Y)$ over $\\mathcal X\\times\\mathcal X$ . A classifier $f$ is a prediction function $f:\\mathcal{X}\\to\\mathcal{Y}$ , and ${\\hat{y}}=f(x)$ is the model\u2019s prediction for $x$ . The true risk of a classifier $f$ w.r.t. $P$ is defined as: $R(f|P)=\\mathbb{E}_{P}[\\ell(f(X),Y)]$ , where $\\ell:\\mathcal{V}\\times\\mathcal{V}\\to\\mathbb{R}^{+}$ is a given loss function, for instance, the 0/1 loss. Given a set of labeled samples ${\\cal{S}}_{m}\\,=\\,\\{(x_{i},y_{i})\\}_{i=1}^{m}$ , the empirical risk of $f$ is: $\\begin{array}{r}{\\hat{R}(f|S_{m})=\\frac{1}{m}\\sum_{i=1}^{m}\\ell(f(x_{i}),y_{i})}\\end{array}$ . Following the notation of [21], we use a confidence score function $\\kappa(x,\\hat{y}|f)$ to quantify prediction confidence. We require $\\kappa$ to induce a partial order over instances in $\\mathcal{X}$ . In this work, we focus on the most common and well-known $\\kappa$ function, softmax response [10, 40]. For a classifier $f$ with softmax as its last layer: $\\kappa(x,\\hat{y}|f)\\,=\\,f_{\\hat{y}}(x)$ . Softmax response has been shown to be a reliable confidence score in the context of selective classification [18, 19] as well as in hierarchical classification [46], consistently achieving solid performance. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "A selective model [9, 13] is a pair $(f,g)$ where $f$ is a classifier and $g:\\mathcal{X}\\to\\{0,1\\}$ is a selection function, which serves as a binary selector for $f$ . The selective model abstains from predicting instance $x$ if and only if $g(x)=0$ . The selection function $g$ can be defined by a confidence threshold $\\theta$ $\\!\\!\\mid\\!\\!:g_{\\theta}(x|\\kappa,f)\\!\\!=\\!\\!\\!1[\\kappa(x,\\hat{y}|f)>\\theta]$ . The performance of a selective model is measured using selective risk and coverage. Coverage is defined as the probability mass of the non-rejected instances in $\\mathcal{X}$ : \u03d5(f, g) = EP [g(X)]. The selective risk of (f, g) is: R(f, g) = EP [\u2113(f(\u03d5(Xf),,gY) )g(X)]. Risk and coverage can be evaluated over a labeled set $S_{m}$ , with the empirical coverage defined as: $\\begin{array}{c c l}{{\\hat{\\phi}(f,g|S_{m})}}&{{=}}&{{\\frac{1}{m}\\sum_{i=1}^{m}g(x_{i})}}\\end{array}$ , and the empirical selective risk: $\\begin{array}{r l}{\\hat{R}(f,g|S_{m})}&{{}=}\\end{array}$ m1 im=1\u03d5\u02c6 (\u2113f(,fg(|xSi),)yi)g(xi). The performance profile of a selective classifier can be visualized by a risk-coverage curve (RC curve) [13], a curve showing the risk as a function of coverage, measured on a set of samples. The area under the RC curve, namely AURC, was defined by [21] for quantifying selective performance via a single scalar. See Figure 2a for an example of an RC Curve. ", "page_idx": 2}, {"type": "text", "text": "Hierarchical Classification: Following the notations of [12] and [46], a hierarchy $H=(V,E)$ is defined by a tree, with nodes $V$ and edges $E$ , the root of the tree is denoted $r\\in V$ . Each node $v\\in V$ represents a semantic class: the leaf nodes ${\\mathcal{L}}\\subseteq V$ are mutually exclusive ground-truth classes, and the internal nodes are unions of leaf nodes determined by the hierarchy. The root represents the semantic class containing all other objects. A sample $x$ that belongs to class $y$ also belongs to the ancestors of $y$ . Each node has exactly one parent and one unique path to it from the root node. The set of leaf descendants of node $v$ is denoted by $\\mathcal{L}(v)$ , and the set of ancestors of node $v$ , including $v$ itself, is denoted by $\\boldsymbol{A}(\\boldsymbol{v})$ . A hierarchical classifier $f:\\mathcal{X}\\rightarrow V$ labels a sample $x\\in\\mathscr{X}$ as a node $v\\in V$ , at any level of the hierarchy, as opposed to a flat classifier, that only predicts leaves. ", "page_idx": 2}, {"type": "text", "text": "Given the hierarchy, it is correct to label an image as either its ground truth leaf node or any of its ancestors. For instance, a Labrador is also a dog, a canine, a mammal, and an animal. While any of these labels is technically correct for classifying a Labrador, the most specific label is clearly preferable as it contains the most information. Thus, it is crucial to observe that the definition of hierarchical correctness is incomplete without considering the amount of information held by the predictions. The correctness of a hierarchical classifier $f$ on a set of samples $S_{m}$ is defined by: $\\begin{array}{r}{\\frac{1}{m}\\sum_{i=1}^{m}\\mathbb{1}[f(x_{i})\\in\\mathcal{A}(y_{i})]}\\end{array}$ . Note that when $H$ is flat and does not contain internal nodes, the hierarchical accuracy reduces to the accuracy of a standard leaf classifier. In the context of classification tasks, the goal is usually to maximize accuracy. However, a trivial way to achieve $100\\%$ hierarchical accuracy would be to simply classify all samples as the root node. For this reason, a mechanism that penalizes the model for predicting less specific nodes must be present as well. In section 3 we define coverage, which quantifies prediction specificity. We aim for a trade-off between the accuracy and information gained by the prediction, which can be controlled according to a user\u2019s requirements. A review of related work involving hierarchical classification can be found in section 6. ", "page_idx": 2}, {"type": "text", "text": "Hierarchical Selective Classification: Selective classification offers a binary choice: either to predict or completely reject a sample. We propose hierarchical selective classification (HSC), a hierarchical extension of selective classification, which allows the model to retreat to less specific nodes in the hierarchy in case of uncertainty. Instead of rejecting the whole prediction, the model can now partially reject a sample, and the degree of the rejection is determined by the model\u2019s confidence. For example, if the model is uncertain about the specific dog breed of an image of a Labrador but is confident enough to determine that it is a dog, the safest choice by a classic selective framework would be to reject it. In our setting, however, the model can still provide useful information that the object in the image is a dog (see Figure 1). ", "page_idx": 2}, {"type": "text", "text": "A hierarchical selective classifier $f^{H}\\triangleq(f,g_{\\theta}^{H})$ consists of $f$ , a base classifier, and $g_{\\theta}^{H}:\\mathcal{X}\\rightarrow V$ , a hierarchical selection function with a confidence threshold $\\theta$ . $g_{\\theta}^{H}$ determines the degree of partial rejection by selecting a node in the hierarchy with confidence higher than $\\theta$ . Defining $g_{\\theta}^{H}$ now becomes non-trivial. For instance, $g_{\\theta}^{H}$ can traverse the hierarchy tree, directly choose a single node, or follow any other algorithm, as long as the predicted node has sufficient confidence. For this reason, we refer to $g_{\\theta}^{H}$ as a hierarchical inference rule. In section 3 we introduce several inference rules and discuss their properties. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Hierarchical selective classifiers differ from the previously discussed hierarchical classifiers by requiring a hierarchical selection function. In contrast to non-selective hierarchical classifiers, which may produce predictions at internal nodes without a selection function, a hierarchical selective classifier can handle uncertainty by gradually trading off risk and coverage, controlled by $g_{\\theta}^{H}$ . This distinction is crucial because hierarchical selective classifiers provide control over the full trade-off between risk and coverage, which is not always attainable with non-selective hierarchical classifiers. ", "page_idx": 3}, {"type": "text", "text": "To our knowledge, controlling the trade-off between accuracy and specificity was only previously explored by [12], who proposed the Dual Accuracy Reward Trade-off Search (DARTS) algorithm, which aims to obtain the most specific classifier for a user-specified accuracy constraint. Our approach differs from theirs in that we guarantee control over the full trade-off, while some coverages cannot be achieved by DARTS. ", "page_idx": 3}, {"type": "text", "text": "As mentioned in section 2, it is considered correct to label an image as either its ground truth leaf node or any of its ancestors. Thus, we employ a natural extension of the 0/1 loss to define the true risk of a hierarchical classifier $f^{H}$ with regard to $P$ $^{D:}\\,R^{H}(f^{H}|P)=\\mathbb{E}_{P}[f^{H}(X)\\notin{A}(Y)]$ , and the empirical risk over a labeled set of samples $\\begin{array}{r}{S_{m}{:}\\;\\hat{R^{H}}(f_{\\cdot}^{H}|S_{m})=\\frac{1}{m}\\sum_{i=1}^{m}\\mathbb{1}[f_{\\cdot}^{H}(x_{i})\\notin{\\mathcal{A}}(y_{i})]}\\end{array}$ . An additional risk penalizing hierarchical mistake severity is discussed in Appendix A. ", "page_idx": 3}, {"type": "text", "text": "To ensure that specific labels are preferred, it is necessary to consider the specificity of predictions. Hierarchical selective coverage, which we propose as a hierarchical extension of selective coverage, measures the amount of information present in the model\u2019s predictions. A natural quantity for that is entropy, which measures the uncertainty associated with the classes beneath a given node. Assuming a uniform prior on the leaf nodes, the entropy of a node $v~\\in~V$ is $\\begin{array}{r}{\\bar{H(v)}=-\\sum_{v^{\\prime}\\in\\mathcal{L}(v)}\\frac{1}{|\\mathcal{L}(v)|}\\log(\\frac{1}{|\\mathcal{L}(v)|})\\overleftarrow{=}\\log(|\\mathcal{L}(v)|)}\\end{array}$ . At the root, the entropy reaches its maximum value, $H(r)=\\log(|\\mathcal{L}|)$ , while at the leaves the entropy is minimized, with $H(y)=0$ for any leaf node $y\\in{\\mathcal{L}}$ . This allows us to define coverage for a single node $v$ , regardless of $g_{\\theta}^{H}$ . We define hierarchical coverage (from now on referred to as coverage) as the entropy of $v$ relative to the entropy of the root node: $\\begin{array}{r}{\\phi^{H}(v)=1-\\frac{H(v)}{H(r)}}\\end{array}$ HH((vr)). The root node has zero coverage, as it does not contain any information. The coverage gradually increases until it reaches 1 at the leaves. We can also define true coverage for a hierarchical selective model: $\\phi^{H}(f^{H})=\\mathbb{E}_{P}[\\phi(f^{H}(X)]$ . The empirical coverage of a classifier over a labeled set of samples $S_{m}$ is defined as the mean coverage over its predictions: $\\begin{array}{r}{\\hat{\\phi^{H}}(f^{H}|S_{m})=\\frac{1}{m}\\sum_{i=1}^{m}\\phi(f^{H}(x_{i}))}\\end{array}$ . For a hierarchy comprised of leaves and a root node, hierarchical coverage reduces to selective coverage, where classifying a sample as the root corresponds with rejection. ", "page_idx": 3}, {"type": "text", "text": "The hierarchical selective performance of a model can be visualized with a hierarchical RC curve. The area under the hierarchical RC curve, which we term hAURC, extends the non-hierarchical AURC, by using the hierarchical extensions of selective risk and coverage. For examples of hierarchical RC curves see Figure 2a and Figure 2b. ", "page_idx": 3}, {"type": "text", "text": "Hierarchical Calibration: Confidence calibration refers to the task of predicting probability estimates that accurately reflect the true likelihood of correctness. As reducing hierarchical coverage may improve accuracy, it may also improve calibration over the samples the model has not abstained from. To capture this relationship, we introduce the concept of Calibration-Coverage Curves (CC curves), which, analogous to RC curves, plot the Expected Calibration Error (ECE) [34] as a function of coverage. see Figure 5 for an example. ", "page_idx": 3}, {"type": "text", "text": "3 Hierarchical Selective Inference Rules ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To define a hierarchical selective model $f^{H}=(f,g_{\\theta}^{H})$ given a base classifier $f$ , an explicit definition of $g_{\\theta}^{H}$ is required. $g_{\\theta}^{H}$ determines the internal node in the hierarchy $f^{H}$ retreats to when the leaf prediction of $f$ is uncertain. $g_{\\theta}^{H}$ requires obtaining confidence for internal nodes in the hierarchy. Since most modern classification models only assign probabilities to leaf nodes, we follow [12] by setting the probability of an internal node to be the sum of its leaf descendant probabilities: $\\begin{array}{r}{\\dot{f_{v}^{H}}(x)=\\bar{\\sum_{y\\in\\mathcal{L}(v)}f_{y}(x)}}\\end{array}$ . Unlike other works that assign the sum of leaf descendant probabilities to internal nodes, our algorithm first calibrates leaf probabilities through temperature scaling. Since the probabilities of internal nodes heavily rely on the leaf probabilities supplied by $f$ , calibration is beneficial for obtaining more reliable leaf probabilities, and since the internal nodes probabilities are ", "page_idx": 3}, {"type": "text", "text": "Input: Classifier $f$ , class hierarchy $H=(V,E)$ , sample $x_{i}\\in\\mathcal{X}$ , confidence threshold $\\theta$ .   \nOutput: Predicted node $\\hat{v}$ . Perform temperature scaling [23] Obtain $f_{v}(\\bar{x_{i})}\\,\\forall v\\in V$ $\\hat{v}\\leftarrow\\arg\\operatorname*{max}f_{y}(x_{i})$ y\u2208L while $f_{\\hat{v}}(x_{i})<\\theta$ do v\u02c6 \u2190parent(v\u02c6) end while ", "page_idx": 4}, {"type": "text", "text": "sums of leaf probabilities, this cumulative effect is even more pronounced. Further, [18] found that applying temperature scaling also improves ranking and selective performance, which is beneficial to our objective. In this section, we introduce several inference rules, along with useful theoretical properties. We propose the Climbing inference rule (Algorithm 1), which starts at the most likely leaf and climbs the path to the root, until reaching an ancestor with confidence above the threshold. A visualization of the Climbing inference rule is shown in Figure 1. ", "page_idx": 4}, {"type": "text", "text": "We compare our proposed inference rules to the following baselines: The first is the selective inference rule, which predicts \u2019root\u2019 for every uncertain sample (this approach is identical to standard \u201chierarchically-ignorant\u201d selective classification). The second hierarchical baseline, proposed by [46], selects the node with the highest coverage among those with confidence above the threshold. We refer to this rule as \u201cMax-Coverage\u201d (MC), and its algorithm is detailed in Appendix B. ", "page_idx": 4}, {"type": "text", "text": "Certain tasks might require guarantees on inference rules. For example, it can be useful to ensure that an inference rule does not turn a correct prediction into an incorrect one. Therefore, for an inference rule $g_{\\theta}^{H}$ we define: ", "page_idx": 4}, {"type": "text", "text": "1. Monotonicity in Correctness: For any $\\theta_{1}\\leq\\theta_{2}$ , base classifier $f$ and labeled sample $(x,y)$ : $(f,g_{\\theta_{1}}^{H})(x)\\,\\in\\,\\dot{\\mathcal{A}}(y)\\,\\Rightarrow\\,(f,g_{\\theta_{2}}^{H})(x)\\,\\in\\,\\mathcal{A}(\\dot{y})$ . Increasing the threshold will never cause a correct prediction to become incorrect. ", "page_idx": 4}, {"type": "text", "text": "2. Monotonicity in Coverage: For any $\\theta_{1}\\ \\leq\\ \\theta_{2}$ , base classifier $f$ and labeled sample $(x,y)$ : $\\phi^{H}[(f,g_{\\theta_{1}}^{H})(x)]\\stackrel{*}{\\geq}\\phi^{H}[(f,g_{\\theta_{2}}^{H})\\stackrel{\\smile}{(x)}]$ . Increasing the threshold will never increase the coverage. ", "page_idx": 4}, {"type": "text", "text": "The Climbing inference rule outlined in Algorithm 1 satisfies both monotonicity properties. MC satisfies monotonicity in coverage, but not in correctness. An additional algorithm that does not satisfy any of the properties is discussed in Appendix C. ", "page_idx": 4}, {"type": "image", "img_path": "wzof7Y66xs/tmp/2aa937707e623494473461a30ade3becb03c32fda83e0d2f68a2054ec14c53b1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "image", "img_path": "", "img_caption": ["Figure 2: (a) hierarchical RC curve of a ViT-L/16-384 model trained on ImageNet1k, evaluated with the 0/1 loss as the risk and softmax response as its confidence function $\\kappa$ . The purple shaded area represents the area under the RC curve (hAURC). Full coverage occurs when the model accepts all leaf predictions, for which the risk is 0.13. Increasing the confidence threshold leads to the rejection of more samples. For example, when the threshold is 0.77 the the risk is 0.04, with coverage 0.8. (b) hierarchical RC curves of different inference rules with EVA-L/14-196 [14] as the base classifier. When the coverage is 1.0, all inference rules predict leaves. Each inference rule achieves a different trade-off, resulting in distinct curves. This example represents the prevalent case, where the \u201chierarchically-ignorant\u201d selective inference rule performs the worst and Climbing outperforms MC. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "When comparing hierarchical selective models, we find it useful to measure the performance improvement gained by using hierarchical selective inference. We propose a new metric: hierarchical gain, ", "page_idx": 4}, {"type": "text", "text": "Input: Hierarchical selective classifier $f^{H}\\;=\\;(f,g^{H})$ , hierarchy $H\\;=\\;(V,E)$ , calibration set $S_{n}=\\{(x_{i},y_{i})\\}_{i=1}^{n}$ , target accuracy $1-\\alpha\\in(0,1)$ , confidence level $1-\\delta$ .   \nOutput: Optimal threshold $\\hat{\\theta}$ , and $\\epsilon$ s.t. $P(|C(n,\\alpha)-(1-\\alpha)|\\leq\\epsilon)\\geq1-\\delta$ , where $C(n,\\alpha)=$ $P(\\theta_{n+1}\\leq\\hat{\\theta}|S_{n})$   \n1: Based on $\\alpha,n,\\delta$ , calculate $\\epsilon$ \u25b7See Appendix E 2: for $(x_{i},y_{i})$ in $S_{n}$ do   \n3: Obtain $f_{v}^{H}({\\boldsymbol{x}}_{i}),\\forall v\\in V$   \n4: $\\theta_{i}\\gets(g_{\\theta}^{H})^{-1}(x_{i}|\\kappa,f)$ \u25b7See Algorithm 5 in Appendix D 5: end for   \n6: Sort $\\{\\theta_{i}\\}_{i=1}^{n}$ in ascending order   \n7: \u03b8\u02c6 \u2190\u03b8\u2308(n+1)(1\u2212\u03b1)\u2309 ", "page_idx": 5}, {"type": "text", "text": "defined as the improvement in hAURC between $(f,g_{\\theta})$ , a \u201chierarchically-ignorant\u201d selective model, and $(f,g_{\\theta}^{\\mathbf{H}})$ , the same base classifier with a hierarchical inference rule. This metric might also point to which models have a better hierarchical understanding, as it directly measures the improvement gained by allowing models to leverage hierarchical knowledge. Note that if the selective inference rule is better than the hierarchical inference rule being assessed, the hierarchical gain will be negative. An illustrative individual example of RC curves comparison for one model is shown in Figure 2b. ", "page_idx": 5}, {"type": "text", "text": "4 Optimal Selective Threshold Algorithm ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In Section 3, we defined several inference rules that, given a confidence threshold, return a predicted node in the hierarchy. In this section, we propose an algorithm that efficiently finds the optimal threshold for a user-defined target accuracy and confidence level. The algorithm, outlined in Algorithm 2, receives as input a hierarchical selective classifier $f^{H}$ , an accuracy target $1-\\alpha$ , a confidence level $1-\\delta$ (which refers to the interval around $1-\\alpha$ , not to be confused with the model\u2019s confidence), and a calibration set. It outputs the optimal threshold $\\hat{\\theta}$ that ensures the classifier\u2019s accuracy on an unseen test set falls within a $1-\\delta$ confidence interval around $1-\\alpha$ , with a resulting error margin of $\\epsilon$ . The threshold is calculated once on the calibration set and then used statically during inference. The algorithm does not require any retraining or fine-tuning of the model\u2019s weights. For each sample $(x_{i},y_{i})$ in the calibration set, the algorithm first calculates $\\theta_{i}$ , the minimal threshold that would have made the prediction $f^{H}(x_{i})$ hierarchically correct. Then $\\hat{\\theta}$ , the optimal threshold, is calculated in a method inspired by split conformal prediction [48]. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1 Suppose the calibration set $S_{n}=\\{(x_{i},y_{i})\\}_{i=1}^{n}$ and a given test sample $(x_{n+1},y_{n+1})$ are exchangeable. For any target accuracy $\\alpha\\in(0,1)$ and $\\delta$ , define $\\theta_{n+1}$ , $\\hat{\\theta}$ and $\\epsilon$ as in Algorithm 2, and $C(n,\\alpha)=P(\\theta_{n+1}\\leq\\hat{\\theta}|S_{n})$ . Then: $\\cdot P(|C(n,\\alpha)-(1-\\alpha)|\\leq\\epsilon)\\geq1-\\delta.$ ", "page_idx": 5}, {"type": "text", "text": "Proof: See Appendix E. ", "page_idx": 5}, {"type": "text", "text": "Remark on Theorem 1: our algorithm can provide an even greater degree of flexibility: the user may choose to set the values of any three parameters out of $\\alpha,n,\\epsilon,\\delta$ . With the three parameters fixed, we can compute the remaining parameter. The size of the calibration set is of particular interest. Although increasing $n$ yields more stable results, our guarantees hold for calibration sets of any size. Even for a small calibration set, our algorithm provides users with the flexibility to set the other parameters according to their requirements and constraints. For example: a user with a low budget for a calibration set who may be, perhaps, more interested in controlling $\\delta$ can still achieve a reasonable constraint by relaxing $\\epsilon$ instead of increasing the calibration set size. See Appendix E for details. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section we evaluate the methods introduced in Section 3 and Section 4. The evaluation was performed on 1,115 vision models pretrained on ImageNet1k [11], and 6 models pretrained on iNat-21 [24] (available in $\\tan\\mathrm{m}\\,0.9.16$ [49] and torchvision 0.15.1 [32]). The reported results were obtained on the corresponding validation sets (ImageNet1k and iNat-21). The complete results and source code necessary for reproducing the experiments are provided in the Supplementary Material. ", "page_idx": 5}, {"type": "text", "text": "Table 1: Comparison of mean hAURC and hierarchical gain results for the selective, Max-Coverage (MC) and Climbing inference rules applied to 1,115 models trained on ImageNet1k, and 6 models trained on iNat21. ", "page_idx": 6}, {"type": "table", "img_path": "wzof7Y66xs/tmp/59de03bec75fcfa83e1c5f5e28dad109b05212dca281bd5db440d8a6cce034c0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Inference Rules: Table 1 compares the mean results of the Climbing inference rule to both hierarchical and non-hierarchical baselines. The evaluation is based on RC curves generated for 1,115 models pretrained on ImageNet1k and 6 models pretrained on iNat21 with each inference rule applied to their output. These aggregated results show the effect of different hierarchical selective inference rules, independent of the properties or output of a specific model. Compared to the non-hierarchical selective baseline, hierarchical inference has a clear benefit. Allowing models to handle uncertainty by partially rejecting a prediction instead of rejecting it as a whole, proves to be advantageous; the average model is capable of leveraging the hierarchy to predict internal nodes that reduce risk while preserving coverage. Nonetheless, the differences remain stark when comparing the hierarchical inference rules. Climbing, achieving almost $15\\%$ hierarchical gain, outperforms MC, with more than double the gain of the latter. These results highlight the fact that the approach taken by inference rules to leverage the hierarchy can have a significant impact. A possible explanation for Climbing\u2019s superior performance could stem from the fact that most models are trained to optimize leaf classification. By starting from the most likely leaf, Climbing utilizes the prior knowledge embedded in models for leaf classification, while MC ignores it. ", "page_idx": 6}, {"type": "text", "text": "Optimal Threshold Algorithm: We compare our algorithm to DARTS [12]. We evaluate the performance of both algorithms on 1,115 vision models trained on ImageNet1k, for a set of target accuracies. For each model and target accuracy the algorithm was run 1000 times, each with a randomly drawn calibration set. We also evaluate both algorithms on 6 models trained on the iNat21 dataset. We compare the algorithms based on two metrics: (1) target accuracy error, i.e., the mean distance between the target accuracy and the accuracy measured on the test set; (2) coverage achieved by using the algorithms\u2019 output on the test set. The results presented in Table 2, and Table 5 in Appendix G, show that our algorithm consistently achieves substantially lower target accuracy errors, indicating that our algorithm succeeds in nearing the target accuracy more precisely. This property allows the model to provide a better balance between risk and coverage. Our algorithm is more inclined towards this trade-off, as it almost always achieves higher coverage than DARTS. This is particularly noteworthy when the target accuracy is high: while DARTS loses coverage quickly, our algorithm maintains coverage that is up to twice as high. Importantly, DARTS does not capture the whole risk-coverage trade-off. Specifically, at the extreme point in the trade-off when it aims to maximize specificity, it still falls short of providing full coverage, and its predictions do not reduce to a flat classifier\u2019s predictions, i.e. it may still predict internal nodes. ", "page_idx": 6}, {"type": "text", "text": "Our algorithm offers additional flexibility to the user by allowing the tuning of the confidence interval $(1-\\delta)$ , while DARTS does not offer such control. Figure 3 illustrates the superiority of our technique over DARTS in detail. Appendix H compares the results of an additional baseline, conformal risk control [3]. The results show that the conformal risk control algorithm exhibits a significantly higher mean accuracy error than both our algorithm and DARTS. ", "page_idx": 6}, {"type": "text", "text": "Table 2: Results (mean scores) comparing the hierarchical selective threshold algorithm (Algorithm 2) with DARTS, repeated 1000 times for each model and target accuracy with a randomly drawn calibration set of 5,000 samples, applied to 1,115 models trained on ImageNet1k (meaning we evaluated each algorithm 1,115,000 times). $1-\\delta$ is set at 0.9. ", "page_idx": 6}, {"type": "table", "img_path": "wzof7Y66xs/tmp/dbe5542c23efa10a1cb4995e56fbf820632f8ee0e0b2205610945f5a08558168.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "wzof7Y66xs/tmp/3cf61cdd9ae621e00a3878a686fdf0d4f0fa77513b0e4111dbea013d7b65cab8.jpg", "img_caption": ["Figure 3: Individual model examples comparing the hierarchical selective threshold algorithm against DARTS, with each algorithm repeated 1000 times. The mean and median results are shown in dark green. The light green area shows the $\\epsilon$ interval around the target accuracy, and the remaining area is marked in red (i.e., each repetition has a $1-\\delta$ probability of being in the green area and a $\\delta$ probability of being in the red area). The target accuracy is $95\\%$ and $1-\\delta=0.9$ . In both examples, the target accuracy error of DARTS is high, and the entirety of its accuracy distribution lies outside of the confidence interval. Left: EVA-Giant/14 [14]. DARTS fails to meet the constraint, whereas our algorithm\u2019s mean accuracy is very close to the target. Right: ResNet-152 [50]. While our algorithm has a near-perfect mean accuracy, DARTS rejects all samples, resulting in zero coverage. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "wzof7Y66xs/tmp/eca35b49ed5788acea35ed769a406d4999a3e70f952c12fbe7979a678cfc9577.jpg", "img_caption": ["Figure 4: Comparison of different methods by their improvement in hAURC, relative to the same model\u2019s performance without the method. The number of models evaluated for each method: knowledge distillation: 42, pretraining: 61, CLIP: 16, semi-supervised learning: 11, adversarial training: 8. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Empirical Study of Training Regimes: Inspired by [18], which demonstrated that training methods such as knowledge distillation significantly impact selective performance, we aimed to investigate whether training regimes could also contribute to hierarchical selective performance. The goal of this experimental section is to provide practitioners with valuable insights for selecting effective training regimes or pretrained models for HSC. ", "page_idx": 7}, {"type": "text", "text": "We evaluated the hAURC of models trained with several training regimes: (a) Knowledge Distillation (KD) [42, 1, 37]; (b) Pretraining: models pretrained either on ImageNet21k [41, 43, 56, 29, 37] or on ImageNet12k, a 11,821 class subset of the full ImageNet21k [49]; (c) Contrastive Language-Image pretraining (CLIP) [35]: CLIP models equipped with linear-probes, pretrained on WIT-400M imagetext pairs by OpenAI, as well as models pretrained with OpenCLIP on LAION-2B [39, 8], fine-tuned either on ImageNet1k or on ImageNet12k and then ImageNet1k. Note that zero-shot CLIP models (i.e., CLIP models without linear-probes) were not included in this evaluation, and are discussed later in this section. (d) Adversarial Training [53, 44]; (e) various forms of Weakly-Supervised [30] or Semi-Supervised Learning [55, 54]. To ensure a fair comparison, we only compare pairs of models that share identical architectures, except for the method being assessed (e.g., a model trained with KD is compared to its vanilla counterpart without KD). Sample sizes vary according to the number of available models for each method. The hAURC results of all models were obtained by using the Climbing inference rule. ", "page_idx": 7}, {"type": "text", "text": "(1) CLIP exceptionally improves hierarchical selective performance, compared to other training regimes. Out of the methods mentioned above, CLIP (orange box), when equipped with a \u201clinear-probe\u201d, improves hierarchical performance the most by a large margin. As seen in Figure 4, the improvement is measured by the relative improvement in hAURC between the vanilla version of the model and the model itself. CLIP achieves an exceptional improvement surpassing $40\\%$ . Further, its median improvement is almost double the next best methods, pretraining (purple box) and semi-supervised learning (light blue box). One possible explanation for this improvement is that the rich representations learned by CLIP lead to improved hierarchical understanding. The image-text alignment of CLIP can express semantic concepts that may not be present when learning exclusively from images. Alternatively, it could be the result of the vast amount of pertaining data. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "(2) Pretraining on larger ImageNet datasets benefits hierarchical selective performance, with certain models achieving up to a $40\\%$ improvement. However, the improvement rates vary significantly: not all models experience the same benefit from pretraining. Semi-supervised learning also shows a noticeable improvement in hierarchical selective performance. ", "page_idx": 8}, {"type": "text", "text": "(3) Knowledge Distillation achieves a notable improvement, with a median improvement of around $10\\%$ . Although not as substantial as the dramatic increase seen with CLIP, it still offers a solid improvement. This observation aligns with [18], who found that knowledge distillation improves selective prediction performance as well as ranking and calibration. ", "page_idx": 8}, {"type": "text", "text": "(4) Linear-probe CLIP significantly outperforms zero-shot CLIP in HSC: We compared pairs of models with identical backbones, where one is the original zero-shot model, and the other was equipped with a linear-probe, that is, it uses the same frozen feature extractor but has an added head trained to classify ImageNet1k. The zero-shot models evaluated are the publicly available models released by OpenAI. The mean relative improvement from zero-shot CLIP to linear-probe CLIP is $45\\%$ , with improvement rates ranging from $32\\%$ to $53\\%$ . We hypothesize that the hierarchical selective performance boost may be related to better calibration or ranking abilities. Specifically, all CLIP models with linear-probes showed significantly higher AUROC than their zero-shot counterparts, indicating superior ranking. Figure 7 in Appendix I shows these results in more detail. ", "page_idx": 8}, {"type": "text", "text": "Since hAURC is closely related to accuracy, we sought to determine whether its improvement could be attributed solely to gains in predictive performance from the training regimes. Our analysis, detailed in Appendix J, suggests this is not the case. Additionally, we explored a training regime specifically aimed at enhancing HSC performance. Our approach is described in Appendix K. ", "page_idx": 8}, {"type": "text", "text": "HSC Improves Confidence Calibration: We plotted the Calibration-Coverage (CC) Curves (defined in Section 2 for 1,115 ImageNet models and compared the curves produced by the Selective inference rule to those from Climbing. Figure 5 presents the aggregated CC curve for all 1,115 models across the two inference rules. The results indicate that HSC not only improves risk, but also improves calibration, with the Climbing inference rule nearly always outperforming Selective. ", "page_idx": 8}, {"type": "text", "text": "6 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In selective classification, several alternative methods for confidence thresholding have been developed [5, 31, 38]. However, these methods generally necessitate some form of training. This work is focused on post-hoc methods that are compatible with any pretrained classifier, which is particularly advantageous for practitioners. Furthermore, despite the availability of other options, threshold-based rejection even with the popular Softmax confidence score continues to be widely used [18, 21], and can be readily enhanced with temperature scaling or other techniques [6, 15, 18]. ", "page_idx": 8}, {"type": "image", "img_path": "wzof7Y66xs/tmp/272aee8f5fb6ebca671404a0b6547ecc5bdfa7ed49f120f93d97e3e5d672ef88.jpg", "img_caption": ["Figure 5: Aggregated (mean and SEM) CC curves of 1,115 ImageNet models. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Various works leverage a hierarchy of classes to improve leaf predictions of flat classifiers [51, 4, 26] but comparatively fewer works explore hierarchical classifiers. [51] optimized a \u201cwin\u201d metric comprising a weighted combination of likelihoods on the path from the root to the leaf. [4] focused on reducing leaf mistake severity, measured by graph distance. They introduced a hierarchical cross-entropy loss, as well as a soft label loss that generalizes label smoothing. [26] claimed that both methods in [4] result in poorly calibrated models, and proposed an alternative inference method. [12] proposed the Dual Accuracy Reward Trade-off Search (DARTS) algorithm, which attempted to obtain the most specific classifier for a user-specified accuracy constraint. They used information gain and hierarchical accuracy as two competing factors, which are integrated into a generalized Lagrange function to effectively obtain multi-granularity decisions. Additional approaches include the Level Selector network, trained by self-supervision to predict the appropriate level in the hierarchy [25]. [46] proposed a loss based on [52] and performed inference using a threshold-based inference rule. Other works allow non-mandatory leaf node prediction, although not directly addressing the accuracy-specificity trade-off [36, 52]. The evaluation of hierarchical classifiers has received relatively little attention previous to our research. The performance profile of a classifier could be inferred from either the average value of a metric across the operating range or by observing operating curves that compare correctness and exactness or information precision and recall [46], or measuring information gain [12, 52]. Set-valued prediction [22] and hierarchical multi-label classification [45] tackle a similar problem to hierarchical classification by allowing a classifier to predict a set of classes. Although both approaches handle uncertainty by predicting a set of classes, the HSC framework hard-codes the sets as nodes in the hierarchy. This way, HSC injects additional world knowledge contained in the hierarchical relations between the classes, yielding a more interpretable prediction. Conformal risk control [3] is of particular interest because it constrains the prediction sets to be hierarchical. Appendix H shows a comparison of this baseline to our method. ", "page_idx": 9}, {"type": "text", "text": "Calibration is another important aspect of uncertainty estimation [27, 58, 18, 16]. Fisch et al. [16] demonstrate that selectively abstaining from uncertain predictions may improve calibration. ", "page_idx": 9}, {"type": "text", "text": "7 Concluding Remarks ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper presents HSC, an extension of selective classification to a hierarchical setting, allowing models to reduce the information in predictions by retreating to internal nodes in the hierarchy when faced with uncertainty. The key contributions of this work include the formalization of hierarchical risk and coverage, hierarchical calibration, the introduction of hierarchical risk-coverage curves and hierarchical calibration-coverage curves, the development of hierarchical selective inference rules, and an efficient algorithm to find the optimal confidence threshold for a given target accuracy. Extensive empirical studies on over a thousand ImageNet classifiers demonstrate the advantages of the proposed algorithms over existing baselines and reveal new findings on training methods that boost hierarchical selective performance. However, there are a few aspects of this work that present opportunities for further investigation and improvement: (1) Our approach utilizes softmaxbased confidence scores. exploring alternative confidence functions and their impact on hierarchical selective classification could provide further insights; (2) While we have identified certain training methods that boost hierarchical selective performance, the training regimes were not optimized for hierarchical selective classification. Future research could focus on optimizing selective hierarchical performance. (3) Although our threshold algorithm is effective, it could be beneficial to train models to guarantee specific risk or coverage constraints supplied by users, in essence constructing a hierarchical \u201cSelectiveNet\u201d [20]. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The research was partially supported by Israel Science Foundation, grant No 765/23. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Yonathan Aflalo, Asaf Noy, Ming Lin, Itamar Friedman, and Lihi Zelnik-Manor. Knapsack pruning with inner distillation. CoRR, abs/2002.08258, 2020. URL https://arxiv.org/ abs/2002.08258.   \n[2] Anastasios N. Angelopoulos and Stephen Bates. A gentle introduction to conformal prediction and distribution-free uncertainty quantification. CoRR, abs/2107.07511, 2021. URL https: //arxiv.org/abs/2107.07511.   \n[3] Anastasios N. Angelopoulos, Stephen Bates, Adam Fisch, Lihua Lei, and Tal Schuster. Conformal risk control. CoRR, abs/2208.02814, 2022.   \n[4] Luca Bertinetto, Romain M\u00fcller, Konstantinos Tertikas, Sina Samangooei, and Nicholas A. Lord. Making better mistakes: Leveraging class hierarchies with deep networks. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 12503\u201312512. Computer Vision Foundation / IEEE, 2020. doi: 10.1109/CVPR42600.2020.01252. URL https://openaccess.thecvf.com/ content_CVPR_2020/html/Bertinetto_Making_Better_Mistakes_Leveraging_ Class_Hierarchies_With_Deep_Networks_CVPR_2020_paper.html.   \n[5] Yuzhou Cao, Tianchi Cai, Lei Feng, Lihong Gu, Jinjie Gu, Bo An, Gang Niu, and Masashi Sugiyama. Generalizing consistent multi-class classification with rejection to be compatible with arbitrary losses. In NeurIPS, 2022.   \n[6] Lu\u00eds Felipe P. Cattelan and Danilo Silva. How to fix a broken confidence estimator: Evaluating post-hoc methods for selective classification with deep neural networks, 2024.   \n[7] Catherine Chen, Kevin Lin, and Dan Klein. Constructing taxonomies from pretrained language models. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-T\u00fcr, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, pages 4687\u20134700. Association for Computational Linguistics, 2021. doi: 10.18653/ V1/2021.NAACL-MAIN.373. URL https://doi.org/10.18653/v1/2021.naacl-main. 373.   \n[8] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pages 2818\u20132829. IEEE, 2023. doi: 10.1109/CVPR52729.2023.00276. URL https://doi.org/10.1109/ CVPR52729.2023.00276.   \n[9] C. K. Chow. An optimum character recognition system using decision functions. IRE Trans. Electron. Comput., 6(4):247\u2013254, 1957. doi: 10.1109/TEC.1957.5222035. URL https: //doi.org/10.1109/TEC.1957.5222035.   \n[10] Luigi P. Cordella, Claudio De Stefano, Francesco Tortorella, and Mario Vento. A method for improving classification reliability of multilayer perceptrons. IEEE Trans. Neural Networks, 6(5):1140\u20131147, 1995. doi: 10.1109/72.410358. URL https://doi.org/10.1109/72. 410358.   \n[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A largescale hierarchical image database. In 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2009), 20-25 June 2009, Miami, Florida, USA, pages 248\u2013255. IEEE Computer Society, 2009. doi: 10.1109/CVPR.2009.5206848. URL https: //doi.org/10.1109/CVPR.2009.5206848.   \n[12] Jia Deng, Jonathan Krause, Alexander C. Berg, and Li Fei-Fei. Hedging your bets: Optimizing accuracy-specificity trade-offs in large scale visual recognition. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, Providence, RI, USA, June 16-21, 2012, pages 3450\u20133457. IEEE Computer Society, 2012. doi: 10.1109/CVPR.2012.6248086. URL https: //doi.org/10.1109/CVPR.2012.6248086.   \n[13] Ran El-Yaniv and Yair Wiener. On the foundations of noise-free selective classification. J. Mach. Learn. Res., 11:1605\u20131641, 2010. doi: 10.5555/1756006.1859904. URL https: //dl.acm.org/doi/10.5555/1756006.1859904.   \n[14] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. EVA: exploring the limits of masked visual representation learning at scale. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pages 19358\u201319369. IEEE, 2023. doi: 10.1109/ CVPR52729.2023.01855. URL https://doi.org/10.1109/CVPR52729.2023.01855.   \n[15] Leo Feng, Mohamed Osama Ahmed, Hossein Hajimirsadeghi, and Amir H. Abdi. Towards better selective classification. In ICLR. OpenReview.net, 2023.   \n[16] Adam Fisch, Tommi S. Jaakkola, and Regina Barzilay. Calibrated selective classification. Trans. Mach. Learn. Res., 2022, 2022.   \n[17] Maurice Funk, Simon Hosemann, Jean Christoph Jung, and Carsten Lutz. Towards ontology construction with language models. In Simon Razniewski, Jan-Christoph Kalo, Sneha Singhania, and Jeff Z. Pan, editors, Joint proceedings of the 1st workshop on Knowledge Base Construction from Pre-Trained Language Models (KBC-LM) and the 2nd challenge on Language Models for Knowledge Base Construction (LM-KBC) co-located with the 22nd International Semantic Web Conference (ISWC 2023), Athens, Greece, November 6, 2023, volume 3577 of CEUR Workshop Proceedings. CEUR-WS.org, 2023. URL https://ceur-ws.org/Vol-3577/paper16.pdf.   \n[18] Ido Galil, Mohammed Dabbah, and Ran El-Yaniv. What can we learn from the selective prediction and uncertainty estimation performance of 523 imagenet classifiers? In ICLR, 2023.   \n[19] Yonatan Geifman and Ran El-Yaniv. Selective classification for deep neural networks. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 4878\u20134887, 2017. URL https://proceedings.neurips.cc/ paper/2017/hash/4a8423d5e91fda00bb7e46540e2b0cf1-Abstract.html.   \n[20] Yonatan Geifman and Ran El-Yaniv. Selectivenet: A deep neural network with an integrated reject option. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 2151\u20132159. PMLR, 2019. URL http://proceedings.mlr.press/v97/geifman19a.html.   \n[21] Yonatan Geifman, Guy Uziel, and Ran El-Yaniv. Bias-reduced uncertainty estimation for deep neural classifiers. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview. net/forum?id $=$ SJfb5jCqKm.   \n[22] Eugen Grycko. Classification with set-valued decision functions. In Otto Opitz, Berthold Lausen, and R\u00fcdiger Klar, editors, Information and Classification, pages 218\u2013224, Berlin, Heidelberg, 1993. Springer Berlin Heidelberg. ISBN 978-3-642-50974-2.   \n[23] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural networks. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pages 1321\u20131330. PMLR, 2017. URL http://proceedings.mlr.press/v70/guo17a.html.   \n[24] Grant Van Horn, Elijah Cole, Sara Beery, Kimberly Wilber, Serge J. Belongie, and Oisin Mac Aodha. Benchmarking representation learning for natural world image collections. In CVPR, pages 12884\u201312893. Computer Vision Foundation / IEEE, 2021.   \n[25] Ahsan Iqbal and Juergen Gall. Level selector network for optimizing accuracy-specificity trade-offs. In 2019 IEEE/CVF International Conference on Computer Vision Workshops, ICCV Workshops 2019, Seoul, Korea (South), October 27-28, 2019, pages 1466\u20131473. IEEE, 2019. doi: 10.1109/ICCVW.2019.00184. URL https://doi.org/10.1109/ICCVW.2019.00184.   \n[26] Shyamgopal Karthik, Ameya Prabhu, Puneet K. Dokania, and Vineet Gandhi. No cost likelihood manipulation at test time for making better mistakes in deep networks. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id $\\cdot$ 193sEnKY1ij.   \n[27] Joey Kuang and Alexander Wong. On calibration of modern quantized efficient neural networks. CoRR, abs/2309.13866, 2023.   \n[28] Grace Li, Tao Long, and Lydia B. Chilton. Eliciting topic hierarchies from large language models. CoRR, abs/2310.19275, 2023. doi: 10.48550/ARXIV.2310.19275. URL https: //doi.org/10.48550/arXiv.2310.19275.   \n[29] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021, pages 9992\u201310002. IEEE, 2021. doi: 10.1109/ICCV48922.2021.00986. URL https://doi.org/10.1109/ICCV48922.2021.00986.   \n[30] Dhruv Mahajan, Ross B. Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining. In Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss, editors, Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part II, volume 11206 of Lecture Notes in Computer Science, pages 185\u2013201. Springer, 2018. doi: 10.1007/978-3-030-01216-8\\_12. URL https://doi.org/10. 1007/978-3-030-01216-8_12.   \n[31] Anqi Mao, Mehryar Mohri, and Yutao Zhong. Theoretically grounded loss functions and algorithms for score-based multi-class abstention. CoRR, abs/2310.14770, 2023.   \n[32] S\u00e9bastien Marcel and Yann Rodriguez. Torchvision the machine-vision package of torch. In ACM Multimedia, pages 1485\u20131488. ACM, 2010.   \n[33] George A. Miller. Wordnet: A lexical database for english. Commun. ACM, 38(11):39\u201341, 1995. doi: 10.1145/219717.219748. URL https://doi.org/10.1145/219717.219748.   \n[34] Mahdi Pakdaman Naeini, Gregory F. Cooper, and Milos Hauskrecht. Obtaining well calibrated probabilities using bayesian binning. In AAAI, pages 2901\u20132907. AAAI Press, 2015.   \n[35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 8748\u20138763. PMLR, 2021. URL http://proceedings.mlr.press/v139/radford21a.html.   \n[36] Joseph Redmon and Ali Farhadi. YOLO9000: better, faster, stronger. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 6517\u20136525. IEEE Computer Society, 2017. doi: 10.1109/CVPR.2017.690. URL https://doi.org/10.1109/CVPR.2017.690.   \n[37] Tal Ridnik, Emanuel Ben Baruch, Asaf Noy, and Lihi Zelnik. Imagenet-21k pretraining for the masses. In Joaquin Vanschoren and Sai-Kit Yeung, editors, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/ 98f13708210194c475687be6106a3b84-Abstract-round1.html.   \n[38] Mahmoud Salem, Mohamed Osama Ahmed, Frederick Tung, and Gabriel L. Oliveira. Gumbelsoftmax selective networks. CoRR, abs/2211.10564, 2022. doi: 10.48550/ARXIV.2211.10564. URL https://doi.org/10.48550/arXiv.2211.10564. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "[39] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5B: an open large-scale dataset for training next generation image-text models. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ a1859debfb3b59d094f3504d5ebb6c25-Abstract-Datasets_and_Benchmarks.html. ", "page_idx": 13}, {"type": "text", "text": "[40] Claudio De Stefano, Carlo Sansone, and Mario Vento. To reject or not to reject: that is the question-an answer in case of neural classifiers. IEEE Trans. Syst. Man Cybern. Part C, 30(1): 84\u201394, 2000. doi: 10.1109/5326.827457. URL https://doi.org/10.1109/5326.827457.   \n[41] Mingxing Tan and Quoc V. Le. Efficientnetv2: Smaller models and faster training. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 10096\u201310106. PMLR, 2021. URL http://proceedings.mlr. press/v139/tan21a.html.   \n[42] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 10347\u201310357. PMLR, 2021. URL http://proceedings. mlr.press/v139/touvron21a.html.   \n[43] Hugo Touvron, Matthieu Cord, and Herv\u00e9 J\u00e9gou. Deit III: revenge of the vit. In Shai Avidan, Gabriel J. Brostow, Moustapha Ciss\u00e9, Giovanni Maria Farinella, and Tal Hassner, editors, Computer Vision - ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXIV, volume 13684 of Lecture Notes in Computer Science, pages 516\u2013533. Springer, 2022. doi: 10.1007/978-3-031-20053-3\\_30. URL https://doi.org/10. 1007/978-3-031-20053-3_30.   \n[44] Florian Tram\u00e8r, Alexey Kurakin, Nicolas Papernot, Ian J. Goodfellow, Dan Boneh, and Patrick D. McDaniel. Ensemble adversarial training: Attacks and defenses. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/ forum?id $\\equiv$ rkZvSe-RZ.   \n[45] Chhavi Tyagi and Wenge Guo. Multi-label classification under uncertainty: A tree-based conformal prediction approach. In Harris Papadopoulos, Khuong An Nguyen, Henrik Bostr\u00f6m, and Lars Carlsson, editors, Conformal and Probabilistic Prediction with Applications, 13-15 September 2023, Limassol, Cyprus, volume 204 of Proceedings of Machine Learning Research, pages 488\u2013512. PMLR, 2023. URL https://proceedings.mlr.press/v204/tyagi23a. html.   \n[46] Jack Valmadre. Hierarchical classification at multiple operating points. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ 727855c31df8821fd18d41c23daebf10-Abstract-Conference.html.   \n[47] Vladimir Vovk. Conditional validity of inductive conformal predictors. CoRR, abs/1209.2673, 2012.   \n[48] Volodya Vovk, Alexander Gammerman, and Craig Saunders. Machine-learning applications of algorithmic randomness. In Ivan Bratko and Saso Dzeroski, editors, Proceedings of the Sixteenth International Conference on Machine Learning (ICML 1999), Bled, Slovenia, June 27 - 30, 1999, pages 444\u2013453. Morgan Kaufmann, 1999.   \n[49] Ross Wightman. Pytorch image models. https://github.com/huggingface/ pytorch-image-models, 2019.   \n[50] Ross Wightman, Hugo Touvron, and Herv\u00e9 J\u00e9gou. Resnet strikes back: An improved training procedure in timm. CoRR, abs/2110.00476, 2021. URL https://arxiv.org/abs/2110. 00476.   \n[51] Cinna Wu, Mark Tygert, and Yann LeCun. Hierarchical loss for classification. CoRR, abs/1709.01062, 2017. URL http://arxiv.org/abs/1709.01062.   \n[52] Tz-Ying Wu, Pedro Morgado, Pei Wang, Chih-Hui Ho, and Nuno Vasconcelos. Solving longtailed recognition with deep realistic taxonomic classifier. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm, editors, Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part VIII, volume 12353 of Lecture Notes in Computer Science, pages 171\u2013189. Springer, 2020. doi: 10.1007/978-3-030-58598-3\\ _11. URL https://doi.org/10.1007/978-3-030-58598-3_11.   \n[53] Cihang Xie, Mingxing Tan, Boqing Gong, Jiang Wang, Alan L. Yuille, and Quoc V. Le. Adversarial examples improve image recognition. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 816\u2013825. Computer Vision Foundation / IEEE, 2020. doi: 10.1109/CVPR42600. 2020.00090. URL https://openaccess.thecvf.com/content_CVPR_2020/html/Xie_ Adversarial_Examples_Improve_Image_Recognition_CVPR_2020_paper.html.   \n[54] Qizhe Xie, Minh-Thang Luong, Eduard H. Hovy, and Quoc V. Le. Self-training with noisy student improves imagenet classification. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 10684\u201310695. Computer Vision Foundation / IEEE, 2020. doi: 10.1109/CVPR42600.2020.01070. URL https://openaccess.thecvf.com/content_CVPR_2020/html/Xie_Self-Training_ With_Noisy_Student_Improves_ImageNet_Classification_CVPR_2020_paper. html.   \n[55] I. Zeki Yalniz, Herv\u00e9 J\u00e9gou, Kan Chen, Manohar Paluri, and Dhruv Mahajan. Billion-scale semi-supervised learning for image classification. CoRR, abs/1905.00546, 2019. URL http: //arxiv.org/abs/1905.00546.   \n[56] Weihao Yu, Chenyang Si, Pan Zhou, Mi Luo, Yichen Zhou, Jiashi Feng, Shuicheng Yan, and Xinchao Wang. Metaformer baselines for vision. IEEE Trans. Pattern Anal. Mach. Intell., 46 (2):896\u2013912, 2024. doi: 10.1109/TPAMI.2023.3329173. URL https://doi.org/10.1109/ TPAMI.2023.3329173.   \n[57] Qingkai Zeng, Yuyang Bai, Zhaoxuan Tan, Shangbin Feng, Zhenwen Liang, Zhihan Zhang, and Meng Jiang. Chain-of-layer: Iteratively prompting large language models for taxonomy induction from limited examples. CoRR, abs/2402.07386, 2024. doi: 10.48550/ARXIV.2402. 07386. URL https://doi.org/10.48550/arXiv.2402.07386.   \n[58] Shengjia Zhao, Tengyu Ma, and Stefano Ermon. Individual calibration with randomized forecasting. In ICML, volume 119 of Proceedings of Machine Learning Research, pages 11387\u201311397. PMLR, 2020. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Hierarchical Severity Risk ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The presence of a hierarchy allows us to measure the severity of incorrect predictions, instead of treating all mistakes as equal. For instance, classifying a Labrador as a Golden Retriever is a much milder mistake compared to classifying it as a snake. We use the coverage of the Lowest Common Ancestor (LCA) of the predicted node $\\hat{v}$ and the label v to define a novel loss function that accounts for mistake severity: lH(v\u02c6, y) = 1 \u2212\u03d5(LC\u03d5(Av\u02c6()v\u02c6,v)). ", "page_idx": 15}, {"type": "text", "text": "Building upon the above example, the LCA of Labrador Retriever and Golden Retriever is the node \u2019Retriever\u2019, which has significantly higher coverage relative to the prediction \u2019Golden Retriever\u2019, in contrast to \u2019Vertebrate\u2019, the LCA of Labrador Retriever and Snake, resulting in the former misclassification having lower risk compared to the latter. ", "page_idx": 15}, {"type": "text", "text": "We provide below the results of our experiments on ImageNet, complementing Table 1: ", "page_idx": 15}, {"type": "text", "text": "Table 3: Comparison of mean hAURC and hierarchical gain results for the selective, Max-Coverage (MC) and Climbing inference rules applied to 1,115 models trained on ImageNet1k, using the hierarchical severity risk. ", "page_idx": 15}, {"type": "table", "img_path": "wzof7Y66xs/tmp/c42d35a15ba0b1c9864c93a493b489c890afd88ffd6586e6f8b53a610952801a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "The results align with those presented in the main body of the paper, where the risk is the the 0/1 loss.   \nClimbing remains the inference rule with the best (lowest) hAURC and the highest hierarchical gain.   \nIn practice, the results of both losses are highly correlated. ", "page_idx": 15}, {"type": "text", "text": "B Max-Coverage Inference Rule Algorithm ", "text_level": 1, "page_idx": 15}, {"type": "table", "img_path": "wzof7Y66xs/tmp/281c66033301baabf7eef23b339902057bcf5344fa84ececbd51f4f6cb01a8ae.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "C Jumping Inference Rule ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The Jumping inference rule (Algorithm 4) traverses the most likely nodes at each level of the hierarchy, until reaching $\\theta$ . Unlike Climbing, jumping is not confined to a single path from the root to a leaf. Although it performed better than a \u201chierarchically-ignorant\u201d selective inference, it still underperformed by the other algorithms. ", "page_idx": 15}, {"type": "text", "text": "Input: Classifier $f$ , class hierarchy $H=(V,E)$ , sample $x_{i}\\in\\mathcal{X}$ , confidence threshold $\\theta$ .   \nOutput: Predicted node $\\hat{v}$ . Perform temperature scaling Obtain $f_{v}(\\bar{x_{i})}\\,\\forall v\\in V$ $\\hat{v}\\leftarrow\\arg\\operatorname*{max}f_{y}(x_{i})$ $y\\!\\in\\!L$ while $f_{\\hat{v}}(x_{i})<\\theta$ do $\\begin{array}{r l}&{\\hat{v}\\leftarrow\\underset{v}{\\operatorname{arg\\,max}}\\{f_{v}(x_{i})|v\\in V,\\ h e i g h t(v)=h\\}}\\\\ &{h\\leftarrow h+1}\\end{array}$ end while ", "page_idx": 16}, {"type": "text", "text": "D Hierarchy Traversal for Threshold Finding ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We demonstrate an example algorithm for the Climbing inference rule that obtains $\\theta_{i}$ for a single instance $(x_{i},y_{i})$ . ", "page_idx": 16}, {"type": "text", "text": "Algorithm 5 Hierarchy Traversal - Climbing ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Input: Calibration sample $\\left({x_{i},y_{i}}\\right)$ , classifier $f$ , class hierarchy $H=(V,E)$ , tightness coefficient $\\epsilon$ Output: $\\theta_{i}$ - the minimal threshold that would have made the prediction correct.   \n1: $\\theta_{i}\\gets0$   \n2: $v_{i}\\leftarrow\\underset{y\\in L}{\\arg\\operatorname*{max}}f_{y}(x_{i})$   \n3: while $v_{i}\\not\\in{\\mathcal{A}}(y_{i})\\;{\\bf d o}$   \n4: $\\theta_{i}\\leftarrow f_{v_{i}}(x_{i})+\\epsilon\\cdot f_{\\mathrm{parent}(v_{i})}(x_{i})$   \n5: vi \u2190parent(vi)   \n6: end while ", "page_idx": 16}, {"type": "text", "text": "E Proof of Theorem 1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Recall that for each sample $\\left({x_{i},y_{i}}\\right)$ , $\\theta_{i}$ is the minimal threshold that would have made the prediction $f^{H}(x_{i})$ hierarchically correct. ", "page_idx": 16}, {"type": "text", "text": "The following is based on [2]:   \nTo avoid handling ties, we assume $\\{\\theta_{i}\\}_{i}^{n}$ are distinct, and without loss of generality that the calibration thresholds are sorted: $0\\leq\\theta_{1}<...<\\theta_{n}\\leq1$ . ", "page_idx": 16}, {"type": "text", "text": "To keep indexing inside the array limits: \u03b8\u02c6=\u2206 $\\hat{\\theta}\\triangleq\\left\\{\\!\\!\\begin{array}{l l}{\\theta_{\\lceil(n+1)(1-\\alpha)},}&{\\alpha\\geq\\frac{1}{n+1}}\\\\ {1,}&{o t h e r w i s e}\\end{array}\\!\\!.\\right.$ ", "page_idx": 16}, {"type": "text", "text": "We also assume the samples $(x_{1},y_{1}),...,(x_{n+1},y_{n+1})$ are exchangeable. Therefore, for any integer $k\\in[1,n]$ , we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\nP(\\theta_{n+1}\\leq\\theta_{k})=\\frac{k}{n+1}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "That is, $\\theta_{n+1}$ is equally likely to fall in anywhere between the calibration points. By setting $k=\\lceil(n+\\mathrm{i})(1-\\alpha)\\rceil$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\nP(\\theta_{n+1}\\leq\\theta_{\\lceil(n+1)(1-\\alpha)\\rceil})=\\frac{\\lceil(n+1)(1-\\alpha)\\rceil}{n+1}\\geq1-\\alpha.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In conformal prediction, the probability $P(\\theta_{n+1}\\leq\\hat{\\theta})$ is called \u201cmarginal coverage\u201d (not to be confused with the hierarchical selective definition of coverage in section 2). The analytic form of marginal coverage given a fixed calibration set, for a sufficiently large test set, is shown by [47] to be ", "page_idx": 16}, {"type": "equation", "text": "$$\nC(n,\\alpha)=P(\\theta_{n+1}\\leq\\hat{\\theta}|\\{(X_{i},Y_{i})\\}_{i=1}^{n})\\sim B e t a(n+1-l,l),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $l=\\lfloor(n+1)\\alpha\\rfloor$ ", "page_idx": 17}, {"type": "text", "text": "From here, the CDF of the Beta distribution paves the way to calculating the size $n$ of the calibration set needed in order to achieve coverage of $1-\\alpha\\pm\\epsilon$ with probability $1-\\delta$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\nP(|C(n,\\alpha)-(1-\\alpha)|\\leq\\epsilon)\\geq1-\\delta.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Remarks on theorem 1: ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "1. To simplify, we made an implicit assumption that users can usually allocate a limited number of samples to the calibration set, and thus would prefer the threshold\u2019s optimality guarantee to hold without imposing additional requirements on the data. However, our algorithm can provide an even greater degree of flexibility: the user may choose to set the values of any three parameters out of: $\\alpha,n,\\epsilon,\\delta$ . With the three parameters fixed, the CDF of the Beta distribution can be used to compute the remaining parameter. For instance, a certain user may have an unlimited budget of calibration samples, while they require a specific error margin. In that case, the using the Beta distribution, the algorithm computes the calibration set size required for Theorem 1 to hold. ", "page_idx": 17}, {"type": "text", "text": "2. The output of the algorithm holds for a calibration set of any size $n$ . Following the previous point, increasing $n$ does yield more stable results.   \n3. In the context of conformal prediction, the property stated in Theorem 1 is referred to as marginal coverage, not to be confused with the previous definition of coverage in section 2. This property is called marginal coverage, since the probability is marginal (averaged) over the randomness in the calibration and test points. ", "page_idx": 17}, {"type": "text", "text": "F Comparison of Inference Rules Without Temperature Scaling ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Table 4: hAURC and gain results for each inference rule. Mean results of 1,115 pretrained ImageNet1k models. ", "page_idx": 17}, {"type": "table", "img_path": "wzof7Y66xs/tmp/fbb221f56a1080dbdad785286210446eb6ff8f76c835b77d5c7777d9c9e3e9b1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "G Threshold Algorithm Results on iNat21 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Table 5: Comparison of hierarchical selective threshold algorithm (Algorithm 2) with DARTS, repeated 100 times for each model and target accuracy with a randomly drawn calibration set of 10,000 samples, applied to 6 models trained on iNat21. $1-\\delta$ is set at 0.9. ", "page_idx": 18}, {"type": "table", "img_path": "wzof7Y66xs/tmp/64feb96ef6f460b0413abe98a8ad1c7b58b9cb266a68dd74b13f3f5602dcd695.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "H Conformal Risk Control Results ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Figure 6 shows the mean accuracy error of the threshold algorithms introduced in section 4 compared to the additional conformal risk control (CRC) algorithm [3]. The mean accuracy error of CRC is strikingly higher than the other algorithms, reaching a climax when the target accuracy is set to $95\\%$ . ", "page_idx": 18}, {"type": "image", "img_path": "wzof7Y66xs/tmp/36965dedb92dc75604dfa887cee920c44c5064a239e0df9563c3b7eeb68ad4f1.jpg", "img_caption": ["Figure 6: Mean accuracy error comparison between the hierarchical selective threshold algorithm (Algorithm 2), conformal risk control (CRC) and DARTS, repeated 1000 times for each model and target accuracy with a randomly drawn calibration set of 5,000 samples, applied to 1,115 models trained on ImageNet1k (meaning we evaluated each algorithm 1,115,000 times). $1-\\delta$ is set at 0.9. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "I CLIP Zero Shot VS Linear Probe ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Figure 7 compares the improvement in hAURC and AUROC between zero-shot CLIP models and their linear-probe counterparts. It can be easily observed that for all the tested backbones the linearprobe models have significantly better hAURC and AUROC. We encourage follow-up research to explore further the possible connection between ranking and HSC. ", "page_idx": 18}, {"type": "image", "img_path": "wzof7Y66xs/tmp/d329973005cf53f49f9cc141f392f2331eafe26594a0cf9ddc41ed4253c4b63b.jpg", "img_caption": ["Figure 7: Comparison between zero-shot and linear-probe CLIP models sharing the same backbone. The round markers represent the zero-shot models, and the star markers represent their respective linear-probes. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "J Training Regimes Improvement in hAURC VS Accuracy ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In Table 6 we compare the relative improvement in hAURC to the relative improvement in accuracy. It can be seen that although the training regimes with the highest and lowest improvement in hAURC have the highest and lowest improvement in accuracy, the improvement in hAURC does not determine the rate of improvement in accuracy. ", "page_idx": 19}, {"type": "table", "img_path": "wzof7Y66xs/tmp/8f9c9f6245c8c8a9ddfe5501192ee8cd6f3c902a7b2289a2767e0c439db395d3.jpg", "table_caption": ["Table 6: Relative improvement in Accuracy compared to relative improvement in hAURC "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "K Training Regime to Optimize HSC Performance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Since hAURC cannot be optimized directly, we developed an alternative that could be optimized. Our best-performing method entailed training models to predict the lowest common ancestor (LCA) of pairs of samples, including identical pairs (intended for testing). To achieve this, we fine-tuned models using the hierarchical loss we proposed in Appendix A. ", "page_idx": 19}, {"type": "text", "text": "All models were fine-tuned on the ImageNet validation set for 20 epochs using the SGD optimizer with a warmup scheduler and a batch size of 2048 on a single NVIDIA A40 GPU. After fine-tuning models of various architectures (including ResNet50, ViTs, EfficientNet, and others) with this loss function and utilizing various other configurations, such as leveraging in-batch negatives for a richer training signal and multiple classification heads to prevent deterioration in the model\u2019s accuracy, the ", "page_idx": 19}, {"type": "text", "text": "improvement in hAURC we observed ranged from $3\\%$ to $5\\%$ compared to the pretrained baseline.   \nWe encourage future work to explore this direction further. ", "page_idx": 20}, {"type": "text", "text": "L Technical Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Most experiments consider image classification on the ImageNet1k [11] validation set, containing 50 examples each for 1,000 classes. Additional evaluations were performed on the iNat21-Mini dataset [24], containing 50 examples each for 10,000 biological species in a seven-level taxonomy. For experiments requiring calibration set, it was sampled randomly from the validation set. ", "page_idx": 20}, {"type": "text", "text": "The evaluation was performed on 1,115 vision models pretrained on ImageNet1k [11], and 6 models pretrained on iNat-21 [24] (available in timm 0.9.16 [49] and torchvision 0.15.1 [32]), both available under the MIT license. ", "page_idx": 20}, {"type": "text", "text": "All experiments were conducted on a single machine with one Nvidia A4000 GPU. The evaluation of all experiments on a single GPU took approximately two weeks. ", "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The abstract and introduction clearly state the claims made, including the contributions of formalizing hierarchical risk and coverage, developing algorithms for hierarchical selective classification, and conducting extensive empirical studies on ImageNet classifiers. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: The potential limitations of this work are discussed in Section 7. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper provides a detailed description of the assumptions and the complete proofs for the theoretical results presented. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The supplementary material includes the source code necessary to replicate the findings ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper provides open access to the source code, along with sufficient instructions to faithfully reproduce the experimental results. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: the details are provided in Section 5. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper reports error bars and confidence intervals for the experimental results, as shown in the relevant figures and tables. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Section 5 and Appendix L detail all necessary compute information. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The research adheres to ethical guidelines, including the responsible use of datasets and consideration of potential societal impacts of the work. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: An example of societal impact was provided in the introduction, where an example of tumor classification demonstrates the works\u2019 impact, both positive and negative where hierarchical granularity is not used. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 24}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 25}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: The necessary details are included in Appendix L. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The source code provided in the supplementary material is properly documented to facilitate faithful reproduction. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}]