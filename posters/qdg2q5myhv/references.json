{"references": [{"fullname_first_author": "Vladimir Karpukhin", "paper_title": "Dense Passage Retrieval for Open-Domain Question Answering", "publication_date": "2020-11-16", "reason": "This paper introduces the Dense Passage Retrieval (DPR) framework, a foundational technique for dense retrieval which the current paper builds upon and improves."}, {"fullname_first_author": "Jacob Devlin", "paper_title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "publication_date": "2019-06-01", "reason": "BERT is the foundational language model used in the current paper's experiments, providing the core embeddings for both query and passage representations."}, {"fullname_first_author": "Luyu Gao", "paper_title": "Condenser: a Pre-training Architecture for Dense Retrieval", "publication_date": "2021-11-16", "reason": "This paper introduces the Condenser architecture, a significant improvement over previous DPR architectures that is a key comparative baseline in the current paper's experiments."}, {"fullname_first_author": "Gautier Izacard", "paper_title": "Unsupervised Dense Information Retrieval with Contrastive Learning", "publication_date": "2022-01-01", "reason": "This paper proposes an unsupervised approach to dense retrieval which directly addresses the high resource requirements of previous methods, allowing for comparison to the current paper's approach."}, {"fullname_first_author": "A\u00e4ron van den Oord", "paper_title": "Representation Learning with Contrastive Predictive Coding", "publication_date": "2018-07-18", "reason": "This paper introduces Contrastive Predictive Coding (CPC), the theoretical foundation for the InfoNCE loss function used in the current paper's training, making it a key theoretical reference."}]}