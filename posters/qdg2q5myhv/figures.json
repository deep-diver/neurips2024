[{"figure_path": "QDG2q5MYHV/figures/figures_1_1.jpg", "caption": "Figure 1: Illustrations of CONTACCUM and Comparative Methods. The illustrations show a total batch size (Ntotal) of 4, a local batch size (Nlocal) of 2, and a memory bank size (Nmemory) of 4. (a) GradCache uses Ntotal - 1 negative passages. (b) GradAccum uses Nlocal - 1 negative passages. (c) CONTACCUM leverages Nlocal + Nmemory - 1 negative samples, more than Ntotal.", "description": "This figure compares three methods for training dense retrievers under memory constraints: Gradient Cache (GradCache), Gradient Accumulation (GradAccum), and Contrastive Accumulation (CONTACCUM).  Each panel illustrates how these methods handle a total batch of 4 data points split into local batches of size 2.  The key difference lies in how they use negative samples for contrastive learning. GradCache decomposes the backpropagation and uses all available negative samples. GradAccum only uses negative samples within each local batch.  CONTACCUM leverages a memory bank to include previously seen negative samples, resulting in a higher number of negative samples than even the full batch size.", "section": "2 Related works"}, {"figure_path": "QDG2q5MYHV/figures/figures_3_1.jpg", "caption": "Figure 2: Training process of CONTACCUM at each accumulation step. The illustration shows a total batch size (Ntotal) of 4, an accumulation step (K) of 2, and a memory bank size (Nmemory) of 4. The dual memory bank caches both query and passage representations. New representations are enqueued, and the oldest are dequeued at each step, maintaining the similarity matrix (Sk) size at (Nlocal + Nmemory, Nlocal + Nmemory).", "description": "This figure illustrates the training process of the CONTACCUM method at each accumulation step.  It uses a total batch size of 4, broken down into 2 accumulation steps (K=2), and a memory bank size of 4.  The key feature is the dual memory bank which stores query and passage representations.  As new representations are generated in each step, older ones are removed (dequeued), maintaining a constant size for the similarity matrix used in calculating the loss.  The diagram visually shows how the memory bank allows the method to effectively leverage past representations.", "section": "3.2 CONTACCUM"}, {"figure_path": "QDG2q5MYHV/figures/figures_7_1.jpg", "caption": "Figure 3: Analysis of accumulation step and memory bank size. DPR performance in low-resource (BSZ=8) and high-resource (BSZ=128) settings is shown as baselines, along with the performance of gradient accumulation for each total batch size (Ntotal).", "description": "This figure displays the impact of the memory bank size and accumulation steps on the performance of CONTACCUM, compared to DPR and GradAccum baselines.  The x-axis shows different total batch sizes, while the y-axis represents the Top 20 accuracy.  Different shades of green bars depict varying memory bank sizes (N_memory) relative to the total batch size.  The figure demonstrates that CONTACCUM consistently outperforms GradAccum, and its performance improves as N_memory increases, regardless of the accumulation step.", "section": "5.3 Memory bank size analysis"}, {"figure_path": "QDG2q5MYHV/figures/figures_7_2.jpg", "caption": "Figure 4: Comparison of the speed of one weight update for different methods as the total batch size (Ntotal) changes.", "description": "This figure compares the training speed of CONTACCUM with several baseline methods under various memory constraints.  The x-axis represents the total batch size (Ntotal), and the y-axis represents the time (in seconds) required for a single training iteration (one weight update).  Different methods and memory bank configurations are compared, demonstrating how CONTACCUM maintains relatively fast iteration times even when using large memory banks, unlike GradCache, which shows significantly slower speeds as the total batch size grows.", "section": "5.4 Train speed"}, {"figure_path": "QDG2q5MYHV/figures/figures_8_1.jpg", "caption": "Figure 5: Analysis of GradNormRatio throughout the training process on the NQ dataset.", "description": "The figure shows the gradient norm ratio (||\u2207A||2/||\u2207\u03b8||2) during the training of the NQ dataset.  The left panel displays the ratio over epochs, comparing the performance of DPR (with batch sizes of 8 and 128), ContAcum (without the query memory bank Mq), and ContAcum (with the dual memory banks Mq and Mp). The right panel provides a zoomed-in view of the ratio, highlighting that CONTACCUM maintains a ratio close to 1 (indicating balanced gradient norms between the two encoders), while omitting the query memory bank leads to a significant imbalance, especially in later training epochs.", "section": "3.3 Gradient analysis with dual memory bank"}, {"figure_path": "QDG2q5MYHV/figures/figures_15_1.jpg", "caption": "Figure 6: Experiments on similarity probability mass.", "description": "This figure visualizes the similarity probability mass over training epochs for both CONTACCUM and DPR (with a batch size of 128).  The similarity mass represents the sum of similarities between current-epoch queries and passage representations generated in past epochs (t-1 to t-6). It shows the relative importance of negative passages from previous steps for both methods, illustrating that CONTACCUM maintains similar behavior to a full-batch DPR, indicating that leveraging previous representations is beneficial, contrary to some previous work. The in-batch negative samples are also included for comparison.", "section": "5.2 Influence of each components in CONTACCUM"}, {"figure_path": "QDG2q5MYHV/figures/figures_15_2.jpg", "caption": "Figure 5: Analysis of GradNormRatio throughout the training process on the NQ dataset.", "description": "This figure presents the gradient norm ratio (||\u2207A||2/||\u2207\u0398||2) during the training of the Natural Questions (NQ) dataset.  The gradient norm ratio is a measure of the balance between the gradients of the query and passage encoders. A ratio close to 1 indicates balanced gradients, while deviations suggest an imbalance. The figure shows the gradient norm ratio over epochs for different training scenarios: standard DPR (with small and large batch sizes), Contrastive Accumulation (CONTACCUM) and CONTACCUM variants where the query memory bank (Mq) is removed at different epochs (10, 20, 30).  The plot helps illustrate the effect of the query memory bank on maintaining balanced gradients and the impact on training stability.", "section": "3.3 Gradient analysis with dual memory bank"}]