[{"type": "text", "text": "Exact, Tractable Gauss-Newton Optimization in Deep Reversible Architectures Reveal Poor Generalization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Davide Buffelli\u2217 Jamie McGowan\u2217 MediaTek Research MediaTek Research Wangkun $\\mathbf{X}\\mathbf{u}^{\\dagger}$ Alexandru Cioba Da-shan Shiu Imperial College London MediaTek Research MediaTek Research ", "page_idx": 0}, {"type": "text", "text": "Guillaume Hennequin MediaTek Research & University of Cambridge ", "page_idx": 0}, {"type": "text", "text": "Alberto Bernacchia MediaTek Research ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Second-order optimization has been shown to accelerate the training of deep neural networks in many applications, often yielding faster progress per iteration on the training loss compared to first-order optimizers. However, the generalization properties of second-order methods are still being debated. Theoretical investigations have proved difficult to carry out outside the tractable settings of heavily simplified model classes \u2013 thus, the relevance of existing theories to practical deep learning applications remains unclear. Similarly, empirical studies in large-scale models and real datasets are significantly confounded by the necessity to approximate secondorder updates in practice. It is often unclear whether the observed generalization behaviour arises specifically from the second-order nature of the parameter updates, or instead reflects the specific structured (e.g. Kronecker) approximations used or any damping-based interpolation towards first-order updates. ", "page_idx": 0}, {"type": "text", "text": "Here, we show for the first time that exact Gauss-Newton (GN) updates take on a tractable form in a class of deep reversible architectures that are sufficiently expressive to be meaningfully applied to common benchmark datasets. We exploit this novel setting to study the training and generalization properties of the GN optimizer. We find that exact GN generalizes poorly. In the mini-batch training setting, this manifests as rapidly saturating progress even on the training loss, with parameter updates found to overfit each mini-batchatch without producing the features that would support generalization to other mini-batches. We show that our experiments run in the \u201clazy\u201d regime, in which the neural tangent kernel (NTK) changes very little during the course of training. This behaviour is associated with having no significant changes in neural representations, explaining the lack of generalization. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Efficient optimization of overparameterized neural networks is a major challenge for deep learning. For large models, training remains one of the main computational and time bottlenecks. Much work has therefore been devoted to the development of neural network optimizers that could accelerate training, enabling researchers and engineers to iterate faster and at lower cost in their search for better performing models. Second-order optimizers, in particular, have been shown to deliver substantially faster per-iteration progress on the training loss [Martens and Grosse, 2015, Botev et al., 2017, George et al., 2018, Goldfarb et al., 2020, Bae et al., 2022, Petersen et al., 2023, Garcia et al., 2023], and much work has been done to scale them to large models via suitable approximations [Ba et al., 2017, Anil et al., 2021]. However, the generalization properties of second-order optimizers remain poorly understood. Here, we focus on the training and generalization properties of the Gauss-Newton (GN) method, which \u2013 in many cases of interest \u2013 also encompasses natural gradient descent (NGD) [Martens, 2020]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Theoretical studies of generalization in GN/NGD have been limited to simplified models, such as linear models [Amari et al., 2021] or nonlinear models taken to their NTK limit [Zhang et al., 2019]. When applied to real-world networks and large datasets, GN/NGD has so far required approximations, such as truncated conjugate gradient iterations in matrix-free approaches [Martens et al., 2010], or block-diagonal and Kronecker-factored estimation of the Gauss-Newton / Fisher matrix [Martens and Grosse, 2015, Botev et al., 2017, George et al., 2018, Goldfarb et al., 2020, Bae et al., 2022, Petersen et al., 2023, Garcia et al., 2023]. Those approximations are exact only in the limit of constant NTK [Karakida and Osawa, 2020], in which models cannot learn any features [Yang and Hu, 2021, Aitchison, 2020]. To our knowledge, the only case in which exact and tractable GN updates have been obtained is that of deep linear networks [Bernacchia et al., 2018, Huh, 2020], which \u2013 despite exhibiting non-trivial learning dynamics [Saxe et al., 2013] \u2013 cannot learn interesting datasets nor yield additional insights into generalization beyond the linear regression setting. Critically, the use of necessary approximations makes it difficult to understand how much of the observed generalization (or lack thereof) can be attributed to the GN method itself, or to the various ways in which it has been simplified. ", "page_idx": 1}, {"type": "text", "text": "Here, we derive an exact, computationally tractable expression for Gauss-Newton updates in deep reversible networks [Dinh et al., 2015, Mangalam et al., 2022]. In reversible architectures made of stacked, volume-preserving MLP-based coupling layers (which we call RevMLPs), we show that it is possible to analytically derive a specific form of a generalized inverse for the network\u2019s Jacobian. This generalized inverse enables fast, exact GN updates in the overparameterized regime. We highlight that, in contrast to the work of Zhang et al. [2019], Cai et al. [2019], Rudner et al. [2019], Karakida and Osawa [2020], we do not assume constant NTK, instead we only require the NTK to remain non-singular during training [Nguyen et al., 2021, Liu et al., 2022, Charles and Papailiopoulos, 2018] as, for example, in the mean-field limit [Arbel et al., 2023]. Equipped with this new model, we study for the first time the generalization behaviour of GN in realistic settings. In the stochastic regime, we find that GN trains too well, overftiting single mini-batch at the expense of impaired performance not only on the test set, but also on the training set. To understand this severe lack of generalization, we conduct a careful examination of the model\u2019s neural tangent kernel and show that the NTK remains almost unchanged during training, and that the neural representations that arise from after training are not different from those set by the network\u2019s initialization. Thus, GN tends to remain in the \u201clazy\u201d regime [Jacot et al., 2018, Chizat et al., 2019], in which representations remain close to those at initialization, lacking generalization. ", "page_idx": 1}, {"type": "text", "text": "In summary: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We show that GN updates computed with any generalized inverse of the model Jacobian results in the same dynamics of the loss, provided that the NTK does not become singular during training (Theorem 4.3).   \n\u2022 We derive an exact and tractable generalized inverse of the Jacobian in the case of deep reversible neural networks (Proposition 4.4). The corresponding GN updates have the same complexity as gradient descent.   \n\u2022 We study the generalization properties of GN in models up to 147 million parameters on MNIST and CIFAR-10, and we show that neural representations do not change during training, as the model remains in the \u201clazy\u201d regime. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Exact vs approximate Gauss-Newton in deep learning. Previous work on second-order optimization of deep learning models focused on either Natural Gradient Descent (NGD), or Gauss-Newton (GN). Since the two are equivalent in many important cases [Martens, 2020], here we do not distinguish them and we refer simply to GN. The most popular methods for computing Gauss-Newton updates assume block-diagonal and Kronecker-factored pre-conditioning matrices [Martens and Grosse, 2015, Botev et al., 2017, George et al., 2018, Goldfarb et al., 2020, Bae et al., 2022, Petersen et al., 2023, Garcia et al., 2023]. Such approximations are known to be exact only in deep linear networks [Bernacchia et al., 2018, Huh, 2020] and in the Neural Tangent Kernel (NTK) limit [Karakida and Osawa, 2020], both of which cannot learn features [Yang and Hu, 2021, Aitchison, 2020]. Recent work focused on exact Gauss-Newton in the feature learning (mean-field) regime [Arbel et al., 2023] but they studied only small models applied to synthetic data. The work of Cai et al. [2019] studies exact Gauss-Newton on real data but only models with one-dimensional outputs. Our work is the first to investigate exact Gauss-Newton in the feature learning regime on real data and sizeable neural networks. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Reversible neural networks. Reversible neural networks [Dinh et al., 2015] allow saving memory during training of large models, because they do not require storing activations [Gomez et al., 2017, MacKay et al., 2018], and achieve near state-of-the-art performance [Mangalam et al., 2022]. Reversible neural networks also feature in normalizing likelihood-based generative models, or normalizing flows [Dinh et al., 2016]. In different reversible models, the inverse is either computed analytically with coupling layers [Kingma and Dhariwal, 2018, Chang et al., 2018, Jacobsen et al., 2018] and similar algebraic tricks [Papamakarios et al., 2017, Hoogeboom et al., 2019, Finzi et al., 2019, Xiao and Liu, 2020, Lu and Huang, 2020], is computed numerically [Behrmann et al., 2019, Song et al., 2019, Huang et al., 2020], or is learned [Keller et al., 2021, Teng and Choromanska, 2019]. In this work, we use analytical inversion with coupling layers, because of the efficiency of automatic differentiation through the inverse function. Our work is the first to use reversible neural networks to compute Gauss-Newton updates. A previous work made a connection between reversible models and Gauss-Newton [Meulemans et al., 2020], but they studied Target Propagation, a very different optimizer. ", "page_idx": 2}, {"type": "text", "text": "Generalization of Gauss-Newton in overparameterized models. The generalization properties of Gauss-Newton are currently debated. While Wilson et al. [2017] shows worst-case scenarios for adaptive methods, Zhang et al. [2019] suggests that GN has similar generalization properties as gradient descent (GD) in the NTK limit. In overparameterized linear models, GN and GD find the same optimum [Amari et al., 2021], however GD transiently achieves better test loss before convergence [Wadia et al., 2021]. The loss dynamics of Gauss-Newton is approximately re-parameterization invariant, and it remains unclear whether a specific parameterizations allows GD to generalize better [Kerekes et al., 2021]. Previous work also suggests a trade-off between training speed and generalization of GN: a good generalization is obtained only when slowing down training, either by damping [Wadia et al., 2021] or by small learning rates [Arbel et al., 2023]. Here we study for the first time generalization for exact GN in sizeable neural networks and real data, and we show that GN achieves poor generalization with respect to gradient descent and similar first order optimizers. ", "page_idx": 2}, {"type": "text", "text": "3 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We provide a brief introduction to Gauss-Newton and Generalized Gauss-Newton. Given input and target data pairs $(x,y)\\in\\mathbb{R}^{d_{x}}\\times\\mathbb{R}^{d_{y}}$ and parameters $\\pmb\\theta\\in\\mathbb{R}^{p}$ , the loss is a sum over a batch $B=\\{(x_{i},y_{i})_{i=1}^{n}\\}$ of $n$ data points ", "page_idx": 2}, {"type": "equation", "text": "$$\n{\\mathcal{L}}(\\pmb{\\theta})=\\sum_{i=1}^{n}\\ell\\left(y_{i},f(x_{i},\\pmb{\\theta})\\right)={\\tilde{\\mathcal{L}}}(\\mathbf{f}(\\pmb{\\theta}))\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "with a twice differentiable and convex function $\\ell$ (e.g. square loss or cross entropy) and a parameterized model $f(x_{i},\\pmb\\theta)$ (e.g. a deep neural network). In the second equality of (1), we concatenate the model outputs $f(x_{i},\\pmb{\\theta})\\in\\mathbb{R}^{d_{y}}$ for all $n$ data points in a single (column) vector $\\mathbf{f}(\\pmb{\\theta})\\in\\mathbb{R}^{n d_{y}}$ with entries $\\mathbf{f}_{i+n\\cdot(j-1)}=f(x_{i},\\pmb\\theta)_{j}$ , and define concisely the loss in function space as $\\tilde{\\mathcal{L}}(\\mathbf{f}(\\pmb{\\theta}))$ . The loss $\\tilde{\\mathcal{L}}(\\mathbf{f})$ is a convex and twice differentiable function of the model $\\mathbf{f}$ , but ${\\mathcal{L}}(\\pmb{\\theta})$ is usually a non-convex function of the parameters $\\pmb{\\theta}$ , due to the non-linearity of the model $\\mathbf f(\\pmb\\theta)$ . Gradient descent optimizes parameters according to: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pmb{\\theta}_{t+1}=\\pmb{\\theta}_{t}-\\alpha\\,\\nabla_{\\pmb{\\theta}}\\mathcal{L}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\alpha$ is the learning rate and $\\nabla_{\\theta}\\mathcal{L}$ is the gradient of the loss with respect to the parameters. In the full-batch setting, $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ is the full training dataset. In the mini-batch setting (stochastic gradient descent, SGD), a batch of data $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ is drawn at random from the dataset at each iteration, without replacement, until all data is covered (one epoch), after which random batches are re-drawn. ", "page_idx": 3}, {"type": "text", "text": "Gauss-Newton. We review two alternative but equivalent views on Gauss-Newton: the Hessian view and the the functional view, which provide different intuitions into the method. The Hessian view understands Gauss-Newton as a second-order optimization method, from the point of view of the curvature of the loss. The functional view understands Gauss-Newton as model inversion, and is more appropriate in the context of our work. ", "page_idx": 3}, {"type": "text", "text": "In the functional view, Gauss-Newton corresponds to gradient descent in function space [Zhang et al., 2019, Cai et al., 2019, Bae et al., 2022, Amari, 1998, Martens, 2020]. By assumption, the loss $\\tilde{\\mathcal{L}}$ is a convex function of the model outputs $\\mathbf{f}$ , thus it would be convenient to optimize the model outputs directly. Gradient flow in function space is given by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{d\\mathbf{f}}{d t}=-\\nabla_{\\mathbf{f}}\\tilde{\\mathcal{L}}\\big|_{\\mathbf{f}(t)}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "However, we need to optimize parameters $\\pmb{\\theta}$ to have a model that can be applied to new data. If ${\\bf f}(t)\\,=\\,{\\bf f}(\\pmb{\\theta}(t))$ , we use the chain rule to find the update in parameter space that corresponds to gradient flow in function space, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{\\partial\\mathbf{f}}{\\partial\\pmb{\\theta}}\\frac{d\\pmb{\\theta}}{d t}=-\\nabla_{\\mathbf{f}}\\mathcal{\\tilde{L}}\\big|_{\\mathbf{f}(\\pmb{\\theta}(t))}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Given the gradient $\\nabla_{\\mathbf{f}}\\tilde{\\mathcal{L}}$ and the Jacobian $\\begin{array}{r}{J=\\frac{\\partial\\mathbf{f}}{\\partial\\theta}}\\end{array}$ defined as $\\begin{array}{r}{J_{a b}={\\frac{\\partial\\mathbf{f}_{a}}{\\partial\\theta_{b}}}}\\end{array}$ (of shape $n d_{y}\\times p)$ , this is a linear system of equations that can be solved for the update $\\frac{d\\theta}{d t}$ , by pseudo-inverting the Jacobian. In discrete time, with learning rate $\\alpha$ , the update is equal to [Bj\u00f6rck, 1996, Ben-Israel, 1965] ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pmb{\\theta}_{t+1}=\\pmb{\\theta}_{t}-\\alpha\\ J^{+}\\ \\nabla_{\\mathbf{f}}\\tilde{\\mathcal{L}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the superscript $^+$ denotes matrix pseudo-inversion. We use this update in our work, in either the full-batch or mini-batch setting. We note that equation (5) implies equation (3), in the continuous time limit, only if the Jacobian has linearly independent rows $'J J^{+}=\\mathrm{I}_{n d_{y}})$ , which also guarantees convergence to a global minimum (full-batch). This requires overparameterization $p\\geq n d_{y}$ , however, even if the model is underparameterized and does not converge to a global minimum, equation (5) is still equivalent to Gauss-Newton in the Hessian view, as shown below. ", "page_idx": 3}, {"type": "text", "text": "In the Hessian view, Gauss-Newton corresponds to Newton\u2019s method with a positive-definite approximation of the Hessian, in the case of square loss [Dennis Jr and Schnabel, 1996, Nocedal and Wright, 1999, Bottou et al., 2018]. The approximation is accurate near a global minimum of the loss, therefore Gauss-Newton inherits the accelerated convergence of Newton\u2019s method near global minima [Dennis Jr and Schnabel, 1996]. The Gauss-Newton update, with learning rate $\\alpha$ , is equal to ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pmb{\\theta}_{t+1}=\\pmb{\\theta}_{t}-\\alpha\\ \\left(\\boldsymbol{J}^{T}\\boldsymbol{J}\\right)^{+}\\nabla_{\\pmb{\\theta}}\\mathcal{L}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Matrix pseudo-inverse is used instead of inverse when $J^{T}J$ is singular (damping is also a popular choice, see Nocedal and Wright [1999]). It is straightforward to prove that equations (6) and (5) are identical, by noting that, since $\\mathcal{L}(\\pmb{\\theta})\\;=\\;\\tilde{\\mathcal{L}}(\\mathbf{f}(\\pmb{\\dot{\\theta}}))$ , then $\\bar{\\nabla}_{\\theta}\\mathcal{L}\\dot{\\mathbf{\\Psi}}=\\ J^{T}\\nabla_{\\mathbf{f}}\\tilde{\\mathcal{L}}$ by chain rule, and $\\left(J^{T}J\\right)^{+}J^{T}=J^{+}$ by the properties of matrix pseudo-inverse. The Gram-Gauss-Newton update of Cai et al. [2019] is also equivalent to equation (5), it just requires the formula for the Jacobian pseudo-inverse in the case of linearly independent rows. ", "page_idx": 3}, {"type": "text", "text": "Generalized Gauss Newton. Following the Hessian view, Generalized Gauss-Newton (GGN) was introduced for convex losses that are different from square loss [Ortega and Rheinboldt, 2000, Schraudolph, 2002]. The Hessian is approximated by the positive semi-definite matrix $J^{T}H J$ , where $H=\\nabla_{\\mathbf{f}}^{2}\\dot{\\tilde{\\mathcal{L}}}$ . As in the case of square loss, the approximation is accurate near a global minimum. That leads to the following update: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\boldsymbol{\\theta}_{t+1}=\\boldsymbol{\\theta}_{t}-\\alpha\\:\\left(\\boldsymbol{J}^{T}\\boldsymbol{H}\\boldsymbol{J}\\right)^{+}\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Note that GGN reduces to GN for $H=\\mathrm{I}_{n d_{y}}$ (square loss). In the functional view, Appendix A shows that Generalized Gauss-Newton corresponds to Newton\u2019s method in function space, provided that the Jacobian has linearly independent rows and $\\tilde{\\mathcal{L}}$ is strongly convex. Furthermore, Appendix B provides some intuition into the convergence of GGN flow. ", "page_idx": 3}, {"type": "text", "text": "4 Exact and tractable Gauss-Newton ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The main hurdle in the GN update of equation (5) is the computation of the Jacobian pseudo-inverse. For a batch size $n$ , number of parameters $p$ and output dimension $d$ , that requires $\\mathcal{O}(n\\bar{d}p\\operatorname*{min}(n d,p))$ compute and $O(n d p)$ memory. In this Section, we show that the GN update can be computed efficiently for reversible models. For a dense neural network of $L$ layers and dimension $d$ , implying $p=\\mathcal{O}(\\dot{L}d^{2})$ parameters, our GN update requires the same memory as gradient descent and $O(L n d^{2}+$ $L n^{2}d$ ) compute, compared to $O(\\bar{L}n d^{2})$ compute of gradient descent. ", "page_idx": 4}, {"type": "text", "text": "Our method consists of two steps: first, we replace the Jacobian pseudoinverse with a generalized inverse, and show that it has identical convergence properties (Theorem 4.3). Second, we show that a specific generalized inverse can be computed efficiently in reversible neural networks (Proposition 4.4). We present both results in the case of square loss (GN). Results for other convex loss functions (GGN) can be derived following steps similar to Appendix B. ", "page_idx": 4}, {"type": "text", "text": "Replacing the pseudoinverse with a generalized inverse. We show that the Jacobian pseudoinverse in equation (5) can be replaced by a generalized inverse that has the same convergence properties. A similar approach was proposed by Bernacchia et al. [2018], Karakida and Osawa [2020], but it was only valid in the case of, respectively, deep linear networks or constant Neural Tangent Kernel (NTK) limit. Here we provide a more general formulation that holds under less restrictive assumptions, e.g. it holds in the mean field regime [Arbel et al., 2023]. We need the following assumption ", "page_idx": 4}, {"type": "text", "text": "Assumption 4.1. Assume $J(\\theta)$ has linearly independent rows (is surjective) for all $\\pmb{\\theta}$ in the domain where GN dynamics takes place. ", "page_idx": 4}, {"type": "text", "text": "Note that this implies that the network is overparametrized, i.e. $p\\geq n d_{y}$ . While, in practice, this assumption may seem strong, it is only slightly stronger than the following version, employed in Arbel et al. [2023]: ", "page_idx": 4}, {"type": "text", "text": "Assumption 4.2. $J(\\pmb\\theta_{0})$ is surjective at initialization $\\theta_{0}$ . ", "page_idx": 4}, {"type": "text", "text": "In Arbel et al. [2023], the authors argue that since surjectivity of $J$ is an open condition, it holds for a neighbourhood of $\\theta_{0}$ , and moreover continue to prove that the dynamics of GN is well defined up to some exit time $T$ from this neighbourhood. They then continue to give assumptions guaranteeing that this dynamics extends to $\\infty$ . We directly assume we are in this latter setting. ", "page_idx": 4}, {"type": "text", "text": "Theorem 4.3. Under Assumption 4.1 so that there is a right inverse $J^{+}$ satisfying $J J^{-1}=I,$ consider the update in parameter space with respect to the flow induced by an arbitrary right inverse $J^{+}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pmb{\\theta}_{t+1}=\\pmb{\\theta}_{t}-\\alpha J^{-1}\\nabla_{\\mathbf{f}}\\tilde{\\mathcal{L}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Then the loss along these trajectories is the same up to ${\\mathcal{O}}(\\alpha)$ , i.e. for any two choices $J_{1}^{-}$ and $J_{2}^{-1}$ , the corresponding iterates $\\pmb{\\theta}_{t}^{(1)}$ and $\\pmb{\\theta}_{t}^{(2)}$ satisfy: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\|\\nabla_{\\mathbf{f}}\\tilde{\\mathcal{L}}(\\mathbf{f}(\\pmb{\\theta}_{t}^{(1)}))-\\nabla_{\\mathbf{f}}\\tilde{\\mathcal{L}}(\\mathbf{f}(\\pmb{\\theta}_{t}^{(2)}))\\|\\leq\\mathcal{O}(\\alpha).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Moreover, as the Moore-Penrose pseudo-inverse is a right inverse under the assumptions, the result applies to $J^{+}$ , and consequently to the dynamics of (5). ", "page_idx": 4}, {"type": "text", "text": "The proof is in Appendix C. The intuition behind this result becomes clearer once we examine the differential of the loss w.r.t. the function outputs, $\\nabla_{\\mathbf{f}}\\tilde{\\mathcal{L}}$ . Notice that, as $\\tilde{\\mathcal{L}}$ is a convex function, it has a unique stationary point, and hence it is natural to interpret $\\nabla_{\\mathbf{f}}\\tilde{\\mathcal{L}}(\\pmb{\\theta})$ as the error at $\\pmb{\\theta}$ , especially close to the global minimum. We will therefore adopt the notation ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\epsilon(\\theta):=\\nabla_{\\mathbf{f}}\\tilde{\\mathcal{L}}(\\theta)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "here and throughout the proofs to refer to the deviation from the global minimum at the current parameter value. A key ingredient of the proof of Theorem 4.3 will be to establish that, for trajectories induced by GGN or the update in equation (8), $\\pmb{\\epsilon}(t):=\\pmb{\\epsilon}(\\pmb{\\theta}(t))$ satisfies: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{d\\pmb{\\epsilon}}{d t}=-\\pmb{\\epsilon}(t)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This trivially implies that $\\epsilon\\to0$ from any initial condition $\\epsilon_{\\mathrm{0}}$ , so that the evolution of the weights approaches a stationary point for the loss, and hence its global minimum. ", "page_idx": 4}, {"type": "text", "text": "The right inverse of the Jacobian, $J^{+}$ is non-unique, and, in general, it is not feasible to compute for large models. However, it turns out that in the case of reversible models, we have an analytic expression for $J^{+}$ , which allows computing exact GN at nearly the same cost as SGD. ", "page_idx": 4}, {"type": "text", "text": "Computing GN of a reversible deep network. Throughout this Section we employ the following notation: for an arbitrary matrix $X$ of shape $(d,n)$ we write the lowercase boldfont corresponding symbol, e.g. x for the row-wise vectorization of the matrix, i.e. $\\mathbf{x}_{i+d\\cdot(j-1)}=X_{i,j}$ . ", "page_idx": 5}, {"type": "text", "text": "We consider networks composed of $L$ reversible layers, and we denote by $X_{\\ell}$ (with the associated vectorization $\\mathbf{x}_{\\ell}$ ) and by $W_{\\ell}$ (and ${\\bf w}_{\\ell}$ ), respectively, the output and the parameters of layer $\\ell$ in matrix and vector forms. The output of the model is the output of the last layer, $\\mathbf{f}=\\mathbf{x}_{L}$ . ", "page_idx": 5}, {"type": "text", "text": "The Jacobian of the full neural network can be expressed as a block matrix consisting of the Jacobians of different layers. Letting $\\pmb{\\theta}=(\\mathbf{w}_{1},\\mathbf{w}_{2},\\dots,\\mathbf{w}_{L})$ the concatenated vector with parameters of all layers ", "page_idx": 5}, {"type": "equation", "text": "$$\nJ=\\frac{\\partial\\mathbf{x}_{L}}{\\partial(\\mathbf{w}_{1},\\mathbf{\\Omega}_{\\cdot\\cdot\\cdot},\\mathbf{w}_{L})}=(J_{1},\\cdot\\cdot\\cdot,J_{L})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with J\u2113= \u2202\u2202xwL\u2113 . Since the only way ${\\bf w}_{\\ell}$ affects $\\mathbf{x}_{L}$ is through the way it affects $\\mathbf{x}_{\\ell}$ , by the chain rule, the layer-wise Jacobian can be written as ", "page_idx": 5}, {"type": "equation", "text": "$$\nJ_{\\ell}=\\frac{\\partial\\mathbf{x}_{L}}{\\partial\\mathbf{x}_{\\ell}}\\frac{\\partial\\mathbf{x}_{\\ell}}{\\partial\\mathbf{w}_{\\ell}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "First, we note that a right inverse of the full Jacobian in equation (12) can be computed by finding right inverses of the individual, layer-wise Jacobians of equation (13). Then we show that, given that the neural network is reversible, a right inverse of equation (13) can be computed easily. In particular, the product of the inverse of the first factor $\\partial\\mathbf{x}_{L}^{-}/\\partial\\mathbf{x}_{\\ell}$ with any vector can be computed exactly with a single forward differentiation pass on the neural network\u2019s inverse. The inverse of the second factor $\\partial\\mathbf{x}_{\\ell}/\\partial\\mathbf{w}_{\\ell}$ can be also computed at low complexity when individual layers are linear in the parameters, even if nonlinear in the input. These observations hold for any reversible neural network, but here we use dense coupling layers as a specific realization (see Section 2), which we call RevMLP. The activation $\\mathbf{x}_{\\ell}\\in\\mathbb{R}^{d n}$ for layer $\\ell$ is written in matrix form $X_{\\ell}\\in\\mathbb{R}^{d\\times n}$ and is split along the first dimension into two components $X_{\\ell}=(X_{\\ell}^{(1)},X_{\\ell}^{(2)})$ , where $X_{0}$ is the input. Here $d$ is an even integer and both $X_{\\ell}^{(1)}$ and $X_{\\ell}^{(2)}$ have shape $\\textstyle\\left({\\frac{d}{2}}\\times n\\right)$ . The equations for a single coupling layer are ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{X_{\\ell}^{(1)}=X_{\\ell-1}^{(1)}+W_{\\ell}^{(1)}\\sigma(V_{\\ell-1}^{(2)}X_{\\ell-1}^{(2)})}}\\\\ {{X_{\\ell}^{(2)}=X_{\\ell-1}^{(2)}+W_{\\ell}^{(2)}\\sigma(V_{\\ell}^{(1)}X_{\\ell}^{(1)})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $W_{\\ell}\\,=\\,(W_{\\ell}^{(1)},W_{\\ell}^{(2)})$ are trainable parameters, while $V_{\\ell}\\,=\\,(V_{\\ell}^{(1)},V_{\\ell}^{(2)})$ are non-trainable parameters (also known as inverted bottleneck, see Bachmann et al. [2024]), and $\\sigma(\\cdot)$ is any differentiable non-linear function. In the rest of this paper, we use the term layer and block interchangeably to refer to a full coupling layer (i.e., where the output is the concatenation of $X_{\\ell}^{(1)}$ and $\\bar{X_{\\ell}^{(2)}}$ as defined in Equation (14) and Equation (15)). Whereas we explicitly refer to \u201chalf\"-coupled layers to mean Equation (14) or Equation (15). We also define the reshaping operator: for $\\mathbf{x}$ , a vector of $d n$ components we write $\\mathcal{R}^{(d,n)}\\{\\mathbf{x}\\}$ for the matrix $A$ of size $(d\\times n)$ satisfying: $A_{i,j}={\\bf x}_{i+d(j-1)}$ ", "page_idx": 5}, {"type": "text", "text": "Proposition 4.4. Assuming $\\sigma(V_{\\ell-1}^{(2)}X_{\\ell-1}^{(2)})$ , $\\sigma(V_{\\ell}^{(1)}X_{\\ell}^{(1)})$ have linearly independent columns, the GN update for the weights of each layer is given by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{\\ell}^{(1)}(t+1)=W_{\\ell}^{(1)}(t)-\\displaystyle\\frac{\\alpha}{L}\\underbrace{\\mathcal{R}^{(\\frac{d}{2},n)}\\left\\{\\frac{\\partial\\mathbf{x}_{\\ell}^{(1)}}{\\partial\\mathbf{x}_{L}}\\epsilon\\right\\}\\sigma\\left(V_{\\ell-1}^{(2)}X_{\\ell-1}^{(2)}\\right)^{+}}_{\\Delta_{\\ell}^{(1)}}\\quad}\\\\ &{W_{\\ell}^{(2)}(t+1)=W_{\\ell}^{(2)}(t)-\\displaystyle\\frac{\\alpha}{L}\\mathcal{R}^{(\\frac{d}{2},n)}\\left\\{\\frac{\\partial\\mathbf{x}_{\\ell}^{(2)}}{\\partial\\mathbf{x}_{L}}\\epsilon-\\left(\\frac{\\partial\\mathbf{x}_{\\ell}^{(2)}}{\\partial\\mathbf{w}_{\\ell}^{(1)}}\\right)\\mathcal{R}^{(\\frac{d}{2},d^{\\prime})^{-1}}\\left\\{\\Delta_{\\ell}^{(1)}\\right\\}\\right\\}\\sigma\\left(V_{\\ell}^{(1)}X_{\\ell}^{(1)}\\right)^{+}}\\\\ &{\\cdots\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The proof is in Appendix D. ", "page_idx": 5}, {"type": "text", "text": "Computational and Memory Complexity ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The terms in the braces in equations (16), (17), are Jacobian-vector products and can be easily computed using automatic mode differentiation at the cost of one (reverse) inference pass, i.e., $O(n d^{2})$ . ", "page_idx": 5}, {"type": "image", "img_path": "p37NlKi9vl/tmp/7cf9533a602dfa61d7186825a16a78526d4e37a241fc9d5862be95e042f924ff.jpg", "img_caption": ["Figure 1: Training loss and accuracy on (a) MNIST and (b) CIFAR-10 in a full-batch scenario where each dataset is trimmed to a fixed subset of $n=1024$ images. GN converges much faster than Adam and SGD. "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "p37NlKi9vl/tmp/a8504f764f1b4932488fb004aea5fccde097b0fec88ad04ac1d9187c21e4b291.jpg", "img_caption": ["Figure 2: Training loss, test loss, and test accuracy on (a) MNIST and (b) CIFAR-10 in a mini-batch scenario. GN does not exhibit the same properties observed in the full-batch setting. In fact, Adam reaches lower training and test loss. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "The last factor requires pseudo-inverting a matrix of size $n\\times d/2$ , which requires $O(n d\\,\\operatorname*{min}(n,d))$ . Since these operations are required in each layer, the overall cost of the update for the full network is $O(L n d^{2}+L n^{2}d)$ , compared to $O(L n d^{2})$ of SGD, while the memory complexity is the same. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "For our experiments we train RevMLPs (equations (14) and (15)) with 2 (6) blocks for MNIST [LeCun et al., 2010] (CIFAR-10; Krizhevsky, 2009), ReLU non-linearities at all half-coupled layers, and an inverted bottleneck of size 8000 resulting in models with 12M (MNIST) and 147M (CIFAR-10) parameters. We train these models to classify images flattened to 1D vectors, using a cross-entropy objective. Note that the chosen size of the inverted bottleneck ensures that the assumptions of Proposition 4.4 hold. ", "page_idx": 6}, {"type": "image", "img_path": "p37NlKi9vl/tmp/95177af4c548b08a75b556766ac7b1450bbac79b5dd8a824efcd140ad156b420.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "At each training iteration, we compute the pseudoinverses in equations (16), (17) using an SVD. For numerical stability we truncate the SVD to a $1\\%$ tolerance relative to the largest singular value and an absolute tolerance of $10^{-5}$ , whichever gives the smallest rank \u2013 our main findings are qualitatively robust to these tolerance levels. Full hyperparameters and additional details are reported in Appendix L, and code is provided with the submission. We ", "page_idx": 6}, {"type": "text", "text": "Figure 3: Percentage change in training loss after each update. GN decreases the loss for the current mini-batch more than SGD and Adam early in training. ", "page_idx": 6}, {"type": "text", "text": "report results averaged over 3 random seeds. All experiments are performed on a single NVIDIA RTXA6000 GPU. ", "page_idx": 6}, {"type": "text", "text": "5.1 Full-Batch Setting ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We first examine the full-batch setting in which the dataset is a random subset of size 1024 of MNIST or CIFAR-10. We tuned the learning rate for each optimizer by selecting the largest one that did not cause the loss to diverge. Figure 1 shows that GN is significantly faster than Adam and SGD in both datasets, in line with theoretical predictions (Equation 11). ", "page_idx": 7}, {"type": "text", "text": "5.2 Mini-Batch Setting ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Next, we consider the full MNIST and CIFAR-10 datasets in the mini-batch setting. We follow standard train and test splits for the datasets, with a mini-batch size of $n=1024$ . The learning rate for all methods is tuned towards the largest progress after 1 epoch that does not exhibit any training instabilities. In both datasets, GN makes good initial progress on the training and test losses, but then struggles to sustain the continued progress which Adam exhibits (Fig. 2). This surprising early saturation of the GN training and test losses is most pronounced for the CIFAR dataset, where even SGD eventually overtakes GN (see Fig. 6 in Appendix E for a longer training run). In the rest of this Section, we use the CIFAR-10 setup to study the possible origins of such weak generalization. ", "page_idx": 7}, {"type": "text", "text": "Overfitting each mini-batch. Based on the full-batch results of Figure 1 in which GN was seen to converge very fast, we postulate that the poor generalization behaviour observed in the mini-batch case may be caused by overfitting to each mini-batch. To test this hypothesis, at each iteration, we compute the loss on a single mini-batch before and after applying the update computed on that same mini-batch. The resulting percentage change in mini-batch loss is shown in Figure 3. Compared to SGD and Adam, GN leads to a much stronger immediate decrease in loss after each update, especially early in training. Whilst this difference gradually weakens during the course of training, it subsists for 80 epochs, i.e. until well after GN\u2019s overall training and test losses have saturated (c.f. Fig.2). These results suggest that GN might require some form of regularization to prevent aggressive incorporation of each mini-batch into the model\u2019s parameters. However, we find that neither smaller learning rates (Appendix I), nor weight decay (Appendix J), nor any of the usual techniques for regularizing the pseudoinverse in Equation (16) (Appendix K) appear to help in this respect (Figures 12, 13 and 14). ", "page_idx": 7}, {"type": "text", "text": "Evolution of the Neural Tangent Kernel. We further hypothesize that GN\u2019s poor generalization may be due to a lack of feature learning. In a similar fashion to Fort et al. [2020], we study the evolution of the neural tangent kernel (NTK) when training with GN compared to SGD and Adam. A changing NTK would suggest that the model learns features different from those present at initialization. Figure 4a and Figure 4b show the rate of change of the NTK between epochs, and the evolution of the NTK similarity with initialization, respectively. ", "page_idx": 7}, {"type": "image", "img_path": "p37NlKi9vl/tmp/7ab7921c39c5f74268721fcda1209afa9f5ff5965b63316e6b8b27d3c56679ae.jpg", "img_caption": ["Figure 4: NTK and CKA similarity evolution across training for GN, Adam and SGD. Top two panels include (a) the rate of change of the NTK and (b) the NTK similarity during training with respect to initialization. Bottom three panels, along the same axis, include the CKA similarities for the (c) first, (d) middle and (e) last block with respect to their initial values. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Overall, the NTK changes very little for SGD and GN, suggesting that SGD and GN operate close to the \u201clazy\u201d training regime [Jacot et al., 2018, Chizat et al., 2019]. On the other side, Adam causes the NTK to change significantly, i.e. Adam does learn features different from the initial ones. ", "page_idx": 7}, {"type": "text", "text": "Feature Learning with Gauss-Newton. Even if features (i.e., the NTK) change during GN training, it remains unclear whether they are associated with changes in neural representations. We examine the change in neural representations during training by computing the Centered Kernel Alignment (CKA; Kornblith et al., 2019 ) measure of similarity between the representations at initialization and those learned at each epoch, across all layers of the model. ", "page_idx": 8}, {"type": "text", "text": "Figures 4c, 4d and 4e illustrate the evolution of CKA similarities for the last, middle and first block (i.e., a \u201cfull\u201d coupling layer as described by equations 14, 15) of a 12 layer RevMLP trained on CIFAR-10. Plots for all blocks are provided in Appendix H along with pairwise similarities across optimizers (Figures 10 and 11). Similar to the NTK, neural representations do not change during training with GN. SGD behaves similarly, with little change in CKA. Adam, on the contrary, has changes in neural representations that coincide with changes in NTK. Appendix G shows that the lack of changes in neural representations for GN cannot be explained by a smaller change in parameters, in fact both GN and Adam show changes in weight space, while weights of SGD change little (Figure 9). ", "page_idx": 8}, {"type": "text", "text": "Contrary to the findings of Arbel et al. [2023], it is evident that the CKA similarities in Figures 4c, 4d and 4e remain higher for longer in earlier blocks for GN, implying that GN is slower than Adam at adapting its deeper internal representations. Furthermore, in Appendix F and I, we address two suggestions from Arbel et al. [2023] and find that the generalization improvements when using smaller learning rates and/or different initializations (close-to-optimal in Figure 7 and low variance in Figure 8) do not carry over to deeper networks. In particular, Figure 7 shows that continuing training with GN after initially training with Adam exhibits the same phenomena as training with GN throughout \u2013 albeit at a slightly lower loss than can be achieved using only GN. ", "page_idx": 8}, {"type": "text", "text": "5.3 Experiments without Inverted Bottleneck ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The previous experiments used inverted bottlenecks to ensure \u201clinear independence\u201d, i.e., to ensure that the model is adequately overparametrized such that the proposed efficient generalized inverse is valid. In other words, inverted bottlenecks ensure that the scalable GN weight updates (equations 16) do implement gradient flow in function space (equation 3), such that our results are not potentially confounded by broken theoretical assumptions. Nevertheless, the proposed GN update can still be applied in the absence of inverted bottlenecks. In Figure 5 we report results on the CIFAR10 dataset, following the same experimental procedure of the previous experiments, but removing all inverted bottlenecks. In the full-batch setting, GN is still performing much better than Adam and SGD. In the mini-batch setting we observe a very similar trend to what is observed in the previous experiments: GN leads to an early saturation of the loss, which instead does not appear in Adam and SGD. ", "page_idx": 8}, {"type": "image", "img_path": "p37NlKi9vl/tmp/c0d791fc933087ede41a52988c32af32064b1227ac06a6051b658dae7171af45.jpg", "img_caption": ["Figure 5: Experiments on CIFAR using a model without Inverted Bottleneck (Full-batch on the left, mini-batch on center and right). While the theoretical guarantees do not hold in this setting, the results follow the same trend observed in Figure 2. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.4 Regression Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We further performed some experiments on regression tasks from the UCI regression dataset\u2020. In more detail, we used the Superconductivity Hamidieh [2018] and Wine Aeberhard and Forina [1992] datasets, and followed the same experimental procedure used for the classification datasets (i.e., we use the same RevMLP architecture with an inverted bottleneck for all optimizers and select the highest learning rate that does not cause the loss to diverge). Results are shown in Appendix M and follow the same trend observed in the classification experiments: in the full-batch case GN is significantly faster than SGD and Adam, while in the mini-batch case there is an apparent stagnation of the test and train losses under GN. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6 Summary and limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper we have introduced a new, tractable way of computing exact Gauss-Newton updates in models with millions of parameters. We have used this theory to study the generalization behaviour of GN in realistic task settings. We found that, although GN yields fast convergence in the full batch regime as predicted, it does not perform as well in the stochastic setting where it tends to overfit each mini-batch. We observed that the NTK does not change when training with GN, suggesting that it operates in the \u201clazy\u201d regime. In line with the above, using the CKA metric, we performed an analysis of the neural representations at the start and end of training showing that they remain very close to each other. This can explain the observed lack of generalization. ", "page_idx": 9}, {"type": "text", "text": "Our investigations have relied on a specific formulation of GN based on a tractable generalized inverse of the Jacobian in reversible networks. While we proved that this inverse leads to the same training loss dynamics, in the limit of small learning rate, as the standard GN formulation is based on the Moore-Penrose pseudoinverse (MPP), one cannot exclude the possibility that those two update rules have different learning and generalization properties for finite learning rates. Indeed, the functional view of GN (Section 3) makes it clear that the standard MPP-based GN update corresponds to the minimum-norm weight update that guarantees (infinitesimal) steepest descent in function space. Whilst also achieving steepest descent, our generalized inverse does not have the same least-squares interpretation \u2013 although it could imply another form of regularization which future work could uncover. In any case, these differences are difficult to assess precisely because the full Jacobian of the network (let alone its MPP) simply cannot be computed for large models. ", "page_idx": 9}, {"type": "text", "text": "Previous applications of approximate GN to deep models found that damping, or truncating, the pseudoinverse of the GN matrix (or, equivalently, of the Jacobian) is key not only for good generalization but also for successful training [Martens et al., 2010, Wadia et al., 2021]. Our generalized inverse is based on a layer-wise factorization of the Jacobian, teasing apart (i) the sensitivity of the network\u2019s output to small changes in layer activations and (ii) the sensitivity of those activations to small changes in parameters. The use of exactly reversible networks allows us to invert the former very efficiently, but does not easily accommodate damping or truncation, making it difficult to study their effect on generalization in large scale settings. We speculate that a variation on coupling layer-based reversible networks could be developed that allows for damping or truncation, potentially improving the generalization behaviour of GN. If this can be done, our framework would then enable very efficient training of large models, effectively achieving the training acceleration of second-order methods at the cost of first-order optimizers, all in a memory efficient architecture. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The authors would like to thank Emmeran Johnson for proving that our layer-wise right inverse of the Jacobian (Equation (38)) is actually the Moore-Penrose pseudoinverse. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "S. Aeberhard and M. Forina. Wine. UCI Machine Learning Repository, 1992. DOI: https://doi.org/10.24432/C5PC7J.   \nL. Aitchison. Why bigger is not always better: on finite and infinite neural networks. In International Conference on Machine Learning, pages 156\u2013164. PMLR, 2020.   \nS.-i. Amari. Natural Gradient Works Efficiently in Learning. Neural Computation, 10(2):251\u2013276, 1998.   \nS.-i. Amari, J. Ba, R. Grosse, X. Li, A. Nitanda, T. Suzuki, D. Wu, and J. Xu. When does preconditioning help or hurt generalization? ICLR, 2021.   \nR. Anil, V. Gupta, T. Koren, K. Regan, and Y. Singer. Scalable second order optimization for deep learning. 2021.   \nM. Arbel, R. Menegaux, and P. Wolinski. Rethinking Gauss-Newton for learning over-parameterized models. Advances in Neural Information Processing Systems, 37, 2023.   \nJ. Ba, R. Grosse, and J. Martens. Distributed second-order optimization using kronecker-factored approximations. In International Conference on Learning Representations, 2017.   \nG. Bachmann, S. Anagnostidis, and T. Hofmann. Scaling MLPs: A tale of inductive bias. Advances in Neural Information Processing Systems, 36, 2024.   \nJ. Bae, P. Vicol, J. Z. HaoChen, and R. B. Grosse. Amortized proximal optimization. Advances in Neural Information Processing Systems, 35:8982\u20138997, 2022.   \nJ. Behrmann, W. Grathwohl, R. T. Chen, D. Duvenaud, and J.-H. Jacobsen. Invertible residual networks. In International conference on machine learning, pages 573\u2013582. PMLR, 2019.   \nA. Ben-Israel. A modified Newton-Raphson method for the solution of systems of equations. Israel journal of mathematics, 3:94\u201398, 1965.   \nA. Bernacchia, M. Lengyel, and G. Hennequin. Exact natural gradient in deep linear networks and its application to the nonlinear case. Advances in Neural Information Processing Systems, 31, 2018.   \n\u00c5. Bj\u00f6rck. Numerical methods for least squares problems. SIAM, 1996.   \nA. Botev, H. Ritter, and D. Barber. Practical Gauss-Newton optimisation for deep learning. In International Conference on Machine Learning, pages 557\u2013565, 2017.   \nL. Bottou, F. E. Curtis, and J. Nocedal. Optimization methods for large-scale machine learning. SIAM Review, 60(2):223\u2013311, 2018.   \nT. Cai, R. Gao, J. Hou, S. Chen, D. Wang, D. He, Z. Zhang, and L. Wang. Gram-gauss-newton method: Learning overparameterized neural networks for regression problems. 2019.   \nB. Chang, L. Meng, E. Haber, L. Ruthotto, D. Begert, and E. Holtham. Reversible architectures for arbitrarily deep residual neural networks. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018.   \nZ. Charles and D. Papailiopoulos. Stability and generalization of learning algorithms that converge to global optima. In International conference on machine learning, pages 745\u2013754. PMLR, 2018.   \nL. Chizat, E. Oyallon, and F. Bach. On lazy training in differentiable programming. Advances in Neural Information Processing Systems, 32, 2019.   \nJ. E. Dennis Jr and R. B. Schnabel. Numerical methods for unconstrained optimization and nonlinear equations. SIAM, 1996.   \nL. Dinh, D. Krueger, and Y. Bengio. Nice: Non-linear independent components estimation. ICLR 2015 Workshop Track, 2015.   \nL. Dinh, J. Sohl-Dickstein, and S. Bengio. Density estimation using Real NVP. International Conference on Learning Representations, 2016.   \nM. Finzi, P. Izmailov, W. Maddox, P. Kirichenko, and A. G. Wilson. Invertible convolutional networks. In Workshop on Invertible Neural Nets and Normalizing Flows, International Conference on Machine Learning, volume 2, 2019.   \nS. Fort, G. K. Dziugaite, M. Paul, S. Kharaghani, D. M. Roy, and S. Ganguli. Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the neural tangent kernel. Advances in Neural Information Processing Systems, 33:5850\u20135861, 2020.   \nJ. R. Garcia, F. Freddi, S. Fotiadis, M. Li, S. Vakili, A. Bernacchia, and G. Hennequin. FisherLegendre (FishLeg) optimization of deep neural networks. In The Eleventh International Conference on Learning Representations, 2023.   \nT. George, C. Laurent, X. Bouthillier, N. Ballas, and P. Vincent. Fast approximate natural gradient descent in a Kronecker factored eigenbasis. Advances in Neural Information Processing Systems, 31, 2018.   \nX. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. AISTATS, 2010.   \nD. Goldfarb, Y. Ren, and A. Bahamou. Practical quasi-newton methods for training deep neural networks. Advances in Neural Information Processing Systems, 33:2386\u20132396, 2020.   \nA. N. Gomez, M. Ren, R. Urtasun, and R. B. Grosse. The reversible residual network: Backpropagation without storing activations. Advances in neural information processing systems, 30, 2017.   \nK. Hamidieh. Superconductivty Data. UCI Machine Learning Repository, 2018. DOI: https://doi.org/10.24432/C53P47.   \nE. Hoogeboom, R. Van Den Berg, and M. Welling. Emerging convolutions for generative normalizing flows. In International conference on machine learning, pages 2771\u20132780. PMLR, 2019.   \nC.-W. Huang, R. T. Chen, C. Tsirigotis, and A. Courville. Convex potential flows: Universal probability distributions with optimal transport and convex optimization. International Conference on Learning Representations, 2020.   \nD. Huh. Curvature-corrected learning dynamics in deep neural networks. ICML, page 9, 2020.   \nA. Iserles. A first course in the numerical analysis of differential equations. Cambridge University Press.   \nJ.-H. Jacobsen, A. W. Smeulders, and E. Oyallon. i-revnet: Deep invertible networks. International Conference on Learning Representations, 2018.   \nA. Jacot, F. Gabriel, and C. Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Advances in neural information processing systems, 31, 2018.   \nR. Karakida and K. Osawa. Understanding approximate Fisher information for fast convergence of natural gradient descent in wide neural networks. Advances in neural information processing systems, 33:10891\u201310901, 2020.   \nT. A. Keller, J. W. Peters, P. Jaini, E. Hoogeboom, P. Forr\u00e9, and M. Welling. Self normalizing flows. In International Conference on Machine Learning, pages 5378\u20135387. PMLR, 2021.   \nA. Kerekes, A. M\u00e9sz\u00e1ros, and F. Husz\u00e1r. Depth Without the Magic: Inductive Bias of Natural Gradient Descent. arXiv:2111.11542, 2021.   \nD. P. Kingma and P. Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. Advances in neural information processing systems, 31, 2018.   \nS. Kornblith, M. Norouzi, H. Lee, and G. Hinton. Similarity of neural network representations revisited. In International conference on machine learning, pages 3519\u20133529. PMLR, 2019.   \nA. Krizhevsky. Learning multiple layers of features from tiny images. pages 32\u201333, 2009.   \nY. LeCun, C. Cortes, and C. Burges. Mnist handwritten digit database. ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist, 2, 2010.   \nC. Liu, L. Zhu, and M. Belkin. Loss landscapes and optimization in over-parameterized non-linear systems and neural networks. Applied and Computational Harmonic Analysis, 59:85\u2013116, 2022.   \nY. Lu and B. Huang. Woodbury transformations for deep generative flows. Advances in Neural Information Processing Systems, 33:5801\u20135811, 2020.   \nM. MacKay, P. Vicol, J. Ba, and R. B. Grosse. Reversible recurrent neural networks. Advances in Neural Information Processing Systems, 31, 2018.   \nK. Mangalam, H. Fan, Y. Li, C.-Y. Wu, B. Xiong, C. Feichtenhofer, and J. Malik. Reversible vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10830\u201310840, 2022.   \nJ. Martens. New insights and perspectives on the natural gradient method. The Journal of Machine Learning Research, 21(1):5776\u20135851, 2020.   \nJ. Martens and R. Grosse. Optimizing neural networks with Kronecker-factored approximate curvature. In International conference on machine learning, pages 2408\u20132417. PMLR, 2015.   \nJ. Martens et al. Deep learning via Hessian-free optimization. In ICML, volume 27, pages 735\u2013742, 2010.   \nA. Meulemans, F. Carzaniga, J. Suykens, J. Sacramento, and B. F. Grewe. A theoretical framework for target propagation. Advances in Neural Information Processing Systems, 33:20024\u201320036, 2020.   \nQ. Nguyen, M. Mondelli, and G. F. Montufar. Tight bounds on the smallest eigenvalue of the neural tangent kernel for deep relu networks. In International Conference on Machine Learning, pages 8119\u20138129. PMLR, 2021.   \nJ. Nocedal and S. J. Wright. Numerical optimization. Springer, 1999.   \nJ. M. Ortega and W. C. Rheinboldt. Iterative solution of nonlinear equations in several variables. SIAM, 2000.   \nG. Papamakarios, T. Pavlakou, and I. Murray. Masked autoregressive flow for density estimation. Advances in neural information processing systems, 30, 2017.   \nA. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. K\u00f6pf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. PyTorch: An imperative style, high-performance deep learning library, 2019.   \nF. Petersen, T. Sutter, C. Borgelt, D. Huh, H. Kuehne, Y. Sun, and O. Deussen. ISAAC Newton: Input-based approximate curvature for Newton\u2019s method. In The Eleventh International Conference on Learning Representations, 2023.   \nT. G. Rudner, F. Wenzel, Y. W. Teh, and Y. Gal. The natural neural tangent kernel: Neural network training dynamics under natural gradient descent. In 4th workshop on Bayesian Deep Learning (NeurIPS 2019), 2019.   \nA. M. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.   \nN. N. Schraudolph. Fast curvature matrix-vector products for second-order gradient descent. Neural computation, 14(7):1723\u20131738, 2002.   \nY. Song, C. Meng, and S. Ermon. Mintnet: Building invertible neural networks with masked convolutions. Advances in Neural Information Processing Systems, 32, 2019.   \nY. Teng and A. Choromanska. Invertible autoencoder for domain adaptation. Computation, 7(2):20, 2019.   \nN. Wadia, D. Duckworth, S. S. Schoenholz, E. Dyer, and J. Sohl-Dickstein. Whitening and second order optimization both make information in the dataset unusable during training, and can reduce or prevent generalization. In International Conference on Machine Learning, pages 10617\u201310629. PMLR, 2021.   \nA. C. Wilson, R. Roelofs, M. Stern, N. Srebro, and B. Recht. The marginal value of adaptive gradient methods in machine learning. Advances in neural information processing systems, 30, 2017.   \nC. Xiao and L. Liu. Generative flows with matrix exponential. In International Conference on Machine Learning, pages 10452\u201310461. PMLR, 2020.   \nG. Yang and E. J. Hu. Feature learning in infinite-width neural networks. In International Conference on Machine Learning, pages 11727\u201311737. PMLR, 2021.   \nG. Zhang, J. Martens, and R. B. Grosse. Fast convergence of natural gradient descent for overparameterized neural networks. Advances in Neural Information Processing Systems, 32, 2019. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Functional view of Generalized Gauss-Newton ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Assuming that $\\tilde{\\mathcal{L}}$ is strongly convex, Newton\u2019s flow in function space is equal to ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{d\\mathbf{f}}{d t}=-H^{-1}\\,\\nabla_{\\mathbf{f}}\\tilde{\\mathcal{L}}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "with $H\\,=\\,\\nabla_{\\mathbf{f}}^{2}\\tilde{\\mathcal{L}}$ . Following steps similar to Section 3, we use $\\begin{array}{r}{\\frac{d\\mathbf{f}}{d t}\\,=\\,J\\frac{d\\theta}{d t}}\\end{array}$ and pseudo-invert the Jacobian. In discrete time, with learning rate , the functional view of Generalized Gauss Newton is given by ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\pmb{\\theta}_{t+1}=\\pmb{\\theta}_{t}-\\alpha~J^{+}H^{-1}\\nabla_{\\mathbf{f}}\\tilde{\\mathcal{L}}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "It is straightforward to show that equations (19) and (7) are identical when the Jacobian has linearly independent rows. Under these assumptions, $\\left(J^{T}H J\\right)^{+}\\,=\\,J^{+}H^{-1}J^{T^{+}}$ and $J^{T^{+}}J^{T}\\,=\\,I$ . Furthermore, as in Section 3, $\\nabla_{\\theta}\\mathcal{L}=J^{T}\\nabla_{\\mathbf{f}}\\tilde{\\mathcal{L}}$ . ", "page_idx": 13}, {"type": "text", "text": "B Convergence of GGN flow ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this Section, we give an informal derivation of the continuous-time dynamics of GGN. See Ortega and Rheinboldt [2000], Bottou et al. [2018] for convergence of GGN in discrete time. We consider the optimization of the error under GGN flow in continuous time. Using the definition of the error $\\epsilon=\\bar{\\nabla}_{\\mathbf{f}}\\tilde{\\mathcal{L}}$ (equation (10)), the optimization flow of the error can be derived using the chain rule ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{d\\pmb{\\epsilon}}{d t}=H J\\frac{d\\pmb{\\theta}}{d t}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The definition of GGN flow is ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{d\\pmb{\\theta}}{d t}=-\\alpha~(J^{T}H J)^{+}\\nabla_{\\pmb{\\theta}}\\mathcal{L}=-\\alpha~(J^{T}H J)^{+}J^{T}\\pmb{\\epsilon}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Therefore, optimization of the error under GGN flow is equal to ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{d\\pmb{\\epsilon}}{d t}=-\\alpha\\,H J(J^{T}H J)^{+}J^{T}\\pmb{\\epsilon}=-\\alpha\\,A\\pmb{\\epsilon}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where we defined the matrix $A=H J(J^{T}H J)^{+}J^{T}$ . Using the properties of the matrix pseudoinverse, we note that $A$ is a projection operator, namely $A^{n}\\,=\\,A$ for any integer power $n$ . Therefore, eigenvalues of $A$ are either zero or one, implying that the error decreases exponentially in the range of $A$ , while it remains constant in the null space of $A$ . In general, the range and null space of $A$ change during training. We note that the projection is orthogonal with respect to the inner product $\\epsilon^{T}H\\epsilon$ . Using the same steps as in Appendix A, if $J$ has linearly independent rows and $\\tilde{\\mathcal{L}}$ is strongly convex, then it is straightforward to show that $A=\\mathrm{I}_{n d_{y}}$ . ", "page_idx": 13}, {"type": "text", "text": "C Proof of Theorem 4.3 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "During the proof we will compare the dynamics of the flow curves of two vector fields, namely $-J^{+}(\\theta)\\epsilon(\\theta)$ and the corresponding $-J^{-1}(\\theta)\\epsilon(\\theta)$ . The dependence on $\\pmb{\\theta}$ is assumed throughout and we will write $\\left(-J^{+}\\epsilon\\right)\\rvert_{\\theta}$ or just $-J^{+}\\epsilon$ . The flowlines of these vector fields are given by: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{d\\pmb\\theta}{d t}=(-J^{+}\\pmb\\epsilon)\\big|_{\\pmb\\theta(t)}}\\\\ {\\displaystyle\\frac{d\\pmb\\theta}{d t}=(-J^{-}\\pmb\\epsilon)\\big|_{\\pmb\\theta(t)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and ", "page_idx": 13}, {"type": "text", "text": "respectively. We will further write plain $\\pmb\\theta(t)$ for the solutions of equation (23) and $\\tilde{{\\boldsymbol{\\theta}}}(t)$ for the solutions of equation (24). Moreover, we\u2019ll write $\\tilde{\\mathbf{f}}$ to mean $\\mathbf f(\\tilde{\\pmb\\theta}(t))$ for arbitrary (possibly tensor valued) functions $\\mathbf{f}$ , to distinguish from f $:\\!(\\pmb\\theta(t))$ . ", "page_idx": 13}, {"type": "text", "text": "First notice that the right inverses $J^{+}$ are not canonical and hence equation (24) represents a family of equations and associated flowlines. However, the dynamics of the associated error \u03f5\u02dc is the same regardless of the choice, and moreover, the same as that of $\\epsilon$ itself. Note that $\\begin{array}{r}{\\frac{d}{d t}\\epsilon(\\pmb{\\theta}(t))=J\\vert_{\\pmb{\\theta}(t)}\\cdot\\frac{d\\pmb{\\theta}}{d t}}\\end{array}$ which gives: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{d\\epsilon}{d t}=-J J^{+}\\epsilon\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "If $J$ has linearly independent rows we have $J J^{+}=\\mathrm{I},$ , therefore ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{d\\pmb{\\epsilon}}{d t}=-\\pmb{\\epsilon}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "If we consider the flow of $\\tilde{\\pmb{\\epsilon}}:=\\pmb{\\epsilon}(\\tilde{\\pmb{\\theta}}(t))$ , replacing $J^{+}$ with a right inverse $J^{+}$ satisfies $J J^{-1}=\\operatorname{I}$ , and again we obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{d\\tilde{\\epsilon}}{d t}=-\\tilde{\\epsilon}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Integrating these equations from the same initial condition $\\tilde{{\\pmb\\theta}}(t_{0})={\\pmb\\theta}(t_{0})$ indeed gives the same error flowlines. ", "page_idx": 14}, {"type": "text", "text": "So despite the different dynamics of the $\\pmb{\\theta}$ and $\\tilde{\\pmb{\\theta}}$ , errors propagate identically. We can use this to derive a bound between the errors incurred by the $k$ -th forward Euler iterates of equations (23) and (24), which define the gradient descent equations. Denote by $\\theta_{i}$ the $i$ -th Euler iterate of $\\pmb\\theta(t)$ and by $\\tilde{\\pmb{\\theta}}_{i}$ , the corresponding iterate of $\\tilde{{\\boldsymbol{\\theta}}}(t)$ . Then, we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle||\\pmb{\\theta}_{i}-\\pmb{\\theta}(t_{i})||\\leq\\frac{\\alpha}{K}\\left(e^{L(t_{i}-t_{0})}-1\\right)}\\\\ {\\displaystyle||\\pmb{\\tilde{\\theta}}_{i}-\\pmb{\\tilde{\\theta}}(t_{i})||\\leq\\frac{\\alpha}{\\tilde{K}}\\left(e^{\\tilde{L}(t_{i}-t_{0})}-1\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\alpha$ is the step-size, $i$ , the number of steps can be computed as $\\begin{array}{r}{i=\\left\\lfloor\\frac{t_{i}-t_{0}}{\\alpha}\\right\\rfloor}\\end{array}$ , $L$ and $\\tilde{L}$ are the Lipschitz constants of $(-J^{+}\\epsilon)$ and $\\left(-J^{-1}\\epsilon\\right)$ respectively, and $K$ and $\\tilde{K}$ are constants which depend on the maximum norm of $\\scriptstyle{\\frac{d^{2}}{d t^{2}}}\\pmb{\\theta}(t)$ and $\\begin{array}{r}{\\frac{d^{2}}{d t^{2}}\\pmb{\\theta}(t)}\\end{array}$ across our domain, see e.g. Iserles. Since $\\epsilon$ is at least $\\mathcal{C}^{2}$ and the dynamics of $\\pmb{\\theta}$ and $\\tilde{\\pmb{\\theta}}$ take place over a bounded domain, $\\epsilon$ is Lipschitz with constant $L_{\\epsilon}$ . Then we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|{\\epsilon}(\\theta_{i})-{\\epsilon}(\\tilde{\\theta}_{i})\\|\\leq\\|{\\epsilon}(\\theta_{i})-{\\epsilon}(\\theta(t_{i}))\\|+\\|{\\epsilon}(\\theta(t_{i}))-{\\epsilon}(\\tilde{\\theta}(t_{i}))\\|+\\|{\\epsilon}(\\tilde{\\theta}(t_{i}))-{\\epsilon}(\\tilde{\\theta}_{i})\\|}\\\\ &{\\qquad\\qquad\\qquad\\leq L_{\\epsilon}\\|\\theta_{i}-\\theta(t_{i})\\|+0+L_{\\epsilon}\\|\\tilde{\\theta}_{i}-\\tilde{\\theta}(t_{i})\\|}\\\\ &{\\qquad\\qquad\\qquad\\leq L_{\\epsilon}\\frac{\\underline{{\\alpha}}}{\\bar{K}}\\left(e^{\\bar{L}(t_{i}-t_{0})}-1\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\bar{K}=\\operatorname*{min}(K,\\tilde{K})$ and $\\bar{L}=\\operatorname*{max}(L,\\tilde{L})$ . This shows that the dynamics of the gradient descent iterates coincides up to first order in $\\alpha$ . ", "page_idx": 14}, {"type": "text", "text": "D Proof of Proposition 4.4 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof. We rewrite the layer-wise Jacobian as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J_{\\ell}=\\frac{\\partial\\mathbf{x}_{L}}{\\partial\\mathbf{x}_{\\ell}}\\frac{\\partial\\mathbf{x}_{\\ell}}{\\partial\\mathbf{w}_{\\ell}}=\\frac{\\partial\\mathbf{x}_{L}}{\\partial(\\mathbf{x}_{\\ell}^{(1)},\\mathbf{x}_{\\ell}^{(2)})}\\frac{\\partial(\\mathbf{x}_{\\ell}^{(1)},\\mathbf{x}_{\\ell}^{(2)})}{\\partial(\\mathbf{w}_{\\ell}^{(1)},\\mathbf{w}_{\\ell}^{(2)})}}\\\\ &{=\\left(\\frac{\\partial\\mathbf{x}_{L}}{\\partial\\mathbf{x}_{\\ell}^{(1)}},\\frac{\\partial\\mathbf{x}_{L}}{\\partial\\mathbf{x}_{\\ell}^{(2)}}\\right)\\left(\\begin{array}{c c}{\\sigma(V_{\\ell-1}^{(2)}X_{\\ell-1}^{(2)})^{T}\\otimes\\mathbf{I}_{d/2}}&{0}\\\\ {\\frac{\\partial\\mathbf{x}_{\\ell}^{(2)}}{\\partial\\mathbf{w}_{\\ell}^{(1)}}}&{\\sigma(V_{\\ell}^{(1)}X_{\\ell}^{(1)})^{T}\\otimes\\mathbf{I}_{d/2}}\\end{array}\\right)}\\\\ &{=\\left(\\frac{\\partial\\mathbf{x}_{L}}{\\partial\\mathbf{x}_{\\ell}^{(1)}},\\frac{\\partial\\mathbf{x}_{L}}{\\partial\\mathbf{x}_{\\ell}^{(2)}}\\right)\\left(\\begin{array}{c c}{\\sigma_{\\ell-1}^{(2)}{^M}_{\\ell}}&{0}\\\\ {\\frac{\\partial\\mathbf{x}_{\\ell}^{(2)}}{\\partial\\mathbf{w}_{\\ell}^{(1)}}}&{\\sigma_{\\ell}^{(1)^{T}}\\otimes\\mathbf{I}_{d/2}}\\end{array}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where \u03c3\u2113\u22121 $\\sigma_{\\ell-1}^{(2)}=\\sigma(V_{\\ell-1}^{(2)}X_{\\ell-1}^{(2)})$ and $\\sigma_{\\ell}^{(1)}=\\sigma(V_{\\ell}^{(1)}X_{\\ell}^{(1)})$ for brevity. Given a lower-triangular block matrix ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left(\\begin{array}{c c}{{A}}&{{0}}\\\\ {{B}}&{{C}}\\end{array}\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "it is possible to define a right inverse as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left(\\begin{array}{c c}{{A^{+}}}&{{0}}\\\\ {{-C^{+}B A^{+}}}&{{C^{+}}}\\end{array}\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Using the above, we prove that a right inverse $J_{\\ell}^{-1}$ of Equation (34) is equal to ", "page_idx": 15}, {"type": "equation", "text": "$$\nJ_{\\ell}^{-1}=\\left(\\begin{array}{c c}{\\left[\\boldsymbol{\\sigma}_{\\ell-1}^{(2)\\mathrm{~}T+}\\otimes\\mathbf{I}_{d/2}\\right]}&{0}\\\\ {-\\left[\\boldsymbol{\\sigma}_{\\ell}^{(1)^{T+}}\\otimes\\mathbf{I}_{d/2}\\right]\\frac{\\partial\\mathbf{x}_{\\ell}^{(2)}}{\\partial\\mathbf{w}_{\\ell}^{(1)}}\\left[\\boldsymbol{\\sigma}_{\\ell-1}^{(2)\\mathrm{~}T+}\\otimes\\mathbf{I}_{d/2}\\right]}&{\\left[\\boldsymbol{\\sigma}_{\\ell}^{(1)^{T+}}\\otimes\\mathbf{I}_{d/2}\\right]}\\end{array}\\right)\\left(\\begin{array}{c}{\\frac{\\partial\\mathbf{x}_{\\ell}^{(1)}}{\\partial\\mathbf{x}_{L}}}\\\\ {\\frac{\\partial\\mathbf{x}_{\\ell}^{(2)}}{\\partial\\mathbf{x}_{L}}}\\end{array}\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "It is possible to prove that $J_{\\ell}^{-1}$ defined above in Equation 38 actually corresponds to the Moore-Penrose pseudoinverse of $J_{\\ell}$ (see Appendix N). From Equation (34) and Equation (38), we have that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J_{\\ell}J_{\\ell}^{-1}=\\left(\\frac{\\partial\\mathbf{x}_{L}}{\\partial\\mathbf{x}_{\\ell}^{(1)}},\\frac{\\partial\\mathbf{x}_{L}}{\\partial\\mathbf{x}_{\\ell}^{(2)}}\\right)\\left(\\begin{array}{l}{\\left[\\sigma_{\\ell-1}^{(2)}{\\sigma_{\\ell-1}^{(2)}}^{+}\\right]^{T}\\otimes\\mathbf{I}_{d/2}}\\\\ {\\Lambda}\\end{array}\\right.}&{\\left[\\sigma_{\\ell}^{(1)}{\\sigma_{\\ell}^{(1)}}^{+}\\right]^{T}\\otimes\\mathbf{I}_{d/2}\\;\\right)\\left(\\begin{array}{l}{\\frac{\\partial\\mathbf{x}_{\\ell}^{(1)}}{\\partial\\mathbf{x}_{\\ell}^{(1)}}}\\\\ {\\frac{\\partial\\mathbf{x}_{\\ell}^{(2)}}{\\partial\\mathbf{x}_{L}}}\\end{array}\\right)}\\\\ &{\\quad\\quad=\\left(\\frac{\\partial\\mathbf{x}_{L}}{\\partial\\mathbf{x}_{\\ell}^{(1)}},\\frac{\\partial\\mathbf{x}_{L}}{\\partial\\mathbf{x}_{\\ell}^{(2)}}\\right)\\left(\\begin{array}{l l}{I}&{0}\\\\ {0}&{I}\\end{array}\\right)\\left(\\begin{array}{l}{\\frac{\\partial\\mathbf{x}_{\\ell}^{(1)}}{\\partial\\mathbf{x}_{\\ell}^{(1)}}}\\\\ {\\frac{\\partial\\mathbf{x}_{\\ell}^{(2)}}{\\partial\\mathbf{x}_{L}}}\\end{array}\\right)}\\\\ &{\\quad\\quad=\\frac{\\partial\\mathbf{x}_{L}}{\\partial\\mathbf{x}_{\\ell}^{(1)}}\\frac{\\partial\\mathbf{x}_{\\ell}^{(1)}}{\\partial\\mathbf{x}_{L}}+\\frac{\\partial\\mathbf{x}_{L}}{\\partial\\mathbf{x}_{\\ell}^{(2)}}\\frac{\\partial\\mathbf{x}_{\\ell}^{(2)}}{\\partial\\mathbf{x}_{L}}=\\frac{\\partial\\mathbf{x}_{L}}{\\partial\\left(\\mathbf{x}_{\\ell}^{(1)},\\mathbf{x}_{\\ell}^{(2)}\\right)}\\frac{\\partial\\left(\\mathbf{x}_{\\ell}^{(1)},\\mathbf{x}_{\\ell}^{(2)}\\right)}{\\partial\\mathbf{x}_{L}}=\\frac{\\partial\\mathbf{x}_{L}}{\\partial\\mathbf{x}_{\\ell}}\\frac{\\partial\\mathbf{x}_{\\ell}}{\\partial\\mathbf{x}_ \n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where Equation (40) follows from the assumption that $\\sigma_{\\ell-1}^{(2)}$ and \u03c3\u2113 $\\sigma_{\\ell}^{(1)}$ have linearly independent columns and, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\Lambda=\\frac{\\partial\\mathbf{x}_{\\ell}^{(2)}}{\\partial\\mathbf{w}_{\\ell}^{(1)}}\\left[\\boldsymbol{\\sigma}_{\\ell-1}^{(2)}\\right.^{T+}\\otimes\\mathbf{I}_{d/2}\\right]-\\left(\\left[\\boldsymbol{\\sigma}_{\\ell}^{(1)}\\boldsymbol{\\sigma}_{\\ell}^{(1)}\\right]^{T}\\otimes\\mathbf{I}_{d/2}\\right)\\frac{\\partial\\mathbf{x}_{\\ell}^{(2)}}{\\partial\\mathbf{w}_{\\ell}^{(1)}}\\left[\\boldsymbol{\\sigma}_{\\ell-1}^{(2)}\\right.^{T+}\\otimes\\mathbf{I}_{d/2}\\right]=0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Additionally, the last equality in Equation (41) holds since $\\mathbf{x}_{L}$ is a bijective function of $\\mathbf{x}_{\\ell}$ , due to the reversibility of the RevMLP. Finally, following from Equation (12), we note that, ", "page_idx": 15}, {"type": "equation", "text": "$$\nJ^{\\dashv}=\\frac{1}{L}\\left(\\begin{array}{c}{J_{1}^{\\to}}\\\\ {\\vdots}\\\\ {J_{L}^{\\to}}\\end{array}\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which when substituted into Equation (8), along with Equation (38), results in the GN update for the weights of each layer, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{\\ell}^{(1)}(t+1)=W_{\\ell}^{(1)}(t)-\\displaystyle\\frac{\\alpha}{L}\\underbrace{\\mathcal{R}^{(\\frac{d}{2},n)}\\left\\{\\frac{\\partial\\mathbf{x}_{\\ell}^{(1)}}{\\partial\\mathbf{x}_{L}}\\epsilon\\right\\}\\sigma\\left(V_{\\ell-1}^{(2)}X_{\\ell-1}^{(2)}\\right)^{+}}_{\\Delta_{\\ell}^{(1)}}\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad(44)}\\\\ &{W_{\\ell}^{(2)}(t+1)=W_{\\ell}^{(2)}(t)-\\displaystyle\\frac{\\alpha}{L}\\mathcal{R}^{(\\frac{d}{2},n)}\\left\\{\\frac{\\partial\\mathbf{x}_{\\ell}^{(2)}}{\\partial\\mathbf{x}_{L}}\\epsilon-\\left(\\frac{\\partial\\mathbf{x}_{\\ell}^{(2)}}{\\partial\\mathbf{w}_{\\ell}^{(1)}}\\right)\\mathcal{R}^{(\\frac{d}{2},d^{\\prime})^{-1}}\\left\\{\\Delta_{\\ell}^{(1)}\\right\\}\\right\\}\\sigma\\left(V_{\\ell}^{(1)}X_{\\ell}^{(1)}\\right)^{+}}\\\\ &{\\cdots\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "image", "img_path": "p37NlKi9vl/tmp/617af596463c320bf74b4a1299e687cae3b92873642a8c5f9d1c6a68ef64dd98.jpg", "img_caption": ["Figure 6: Training loss, test loss, and test accuracy when training for a longer amount of epochs on CIFAR-10 shows that Gauss-Newton is unable to further decrease the training loss, while even plain SGD can reach lower values. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "E Longer training curves ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Figure 6 displays the result of training the same RevMLP as in Section 5.2 for 1000 epochs on the CIFAR-10 dataset in a mini-batch setting $n=1024)$ ). We observe that by continuing the training for longer on CIFAR-10, SGD is able to reach lower values of the training loss when compared to Gauss-Newton. In particular, we highlight that even in 1000 epochs, Gauss-Newton appears unable to increase its training performance further than the value it reaches after just 50 epochs. In fact, the results in Figure 6 show that Gauss-Newton tends to increase its training loss after 150 epochs of training. ", "page_idx": 16}, {"type": "text", "text": "F Initialization dependencies for Gauss-Newton ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "To examine if the poor performance of GaussNewton depends on a poor initialization point, we first train a model with Adam for 50 epochs, before continuing the training with Gauss-Newton. For comparison, we also train a model with Gauss-Newton for 50 epochs and subsequently continue training with Adam \u2013 to observe if Gauss-Newton reaches reaches a \u201cbad\u201d local minimum that is hard to escape from. These results are provided in Figure 7 and compared with their single optimizer counterparts. We choose a \u201cgood\u201d initialization point as an Adam trained model at 50 epochs (indicated by the dashed line in Figure 7), which has a lower training loss than GN can achieve in the same (or larger) number of iterations. One can observe that, even when starting from this \u201cgood\u201d initialization, GN eventually saturates at a higher value of the loss when compared with the values ", "page_idx": 16}, {"type": "image", "img_path": "p37NlKi9vl/tmp/2278c6568ed094f8e0b573e54e6a4ccc152953de9e6f31eb86a845ebf9280819.jpg", "img_caption": ["Figure 7: Training loss when first using Adam (or GN), and then continuing with GN (or Adam) \u2013 the purple dashed line indicates the 50 epochs mark at which the optimizers are switched. GN shows early saturation of the loss even when starting from a better intialization point. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "achievable by continuing training with Adam. We also find that Adam can start from a point found by GN and continue training without issues, reaching a value of the loss that is lower than the saturation point of GN. ", "page_idx": 16}, {"type": "text", "text": "In reference to Arbel et al. [2023], we also provide additional results in Figure 8 to show the dependency of Gauss-Newton on the initial weight variance chosen. Interestingly, our results are different from those in Arbel et al. [2023] and suggest that choosing a higher variance is preferable. However, all curves exhibit the same phenomena as discussed in Section 5 and under-perform with respect to Adam. The default choice for all experiments we report is $\\sigma=10^{-3}$ . ", "page_idx": 16}, {"type": "text", "text": "G Change in weights during training ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We analyze the change in norm and cosine similarity between the weights at initialization and at the end of training when a model is trained with different optimizers. Results are shown in Figure 9. We ", "page_idx": 16}, {"type": "image", "img_path": "p37NlKi9vl/tmp/319890abd654a38ebdefd8e8f2e0e7326aa131750624990a9cf5cb6fe66c8bb0.jpg", "img_caption": ["Figure 8: GN Train and test loss with weights initialized accounding to different variances at initialization $\\sigma$ . "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "p37NlKi9vl/tmp/e590d038946af8ad88f5c929919bb4ef9665bde24934b01d7b81c3342919514c.jpg", "img_caption": ["Figure 9: Cosine similarity with the initial weight initialization across training. ADAM and GN move similarly in weight space indicating a consistent behaviour in weight space between the two optimizers. Note that, in this Figure, we use the term \u201clayer\u201d to refer to half-coupled layer in the reversible blocks. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "observe that Gauss-Newton changes the weights to an extent similar to Adam, while SGD show much smaller weight changes, suggesting a lazy training regime. ", "page_idx": 18}, {"type": "text", "text": "H Extended Centered Kernel Alignment (CKA) analysis ", "text_level": 1, "page_idx": 18}, {"type": "image", "img_path": "p37NlKi9vl/tmp/ee119a06ce796b5dd6fb7087f2229ec4386d41267d43828005f2b3ded0da047e.jpg", "img_caption": ["Figure 10: CKA similarity evolution across training for GN, Adam and SGD. GN maintains a high CKA similarity with its initial feature space, very similarly to SGD. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "In Figure 10 we present the full spectrum of CKA similarities from initialization across blocks (i.e., full coupling layer) when training on CIFAR-10. We observe that GN behaves very much SGD: the representqations remain very similar to those at initialization. Adam instead tends to change the representations during training, more quickly for later blocks, and more slowly for earlier blocks. ", "page_idx": 18}, {"type": "image", "img_path": "p37NlKi9vl/tmp/ffcd11b1fb8937bb557dca5bf0310c6b216a3cfd6ef8b75d191ad65050a19a65.jpg", "img_caption": ["Figure 11: Pairwise CKA similarity evolution across training between GN and models trained with Adam and SGD. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "In addition to the CKA evolution results for single optimizers, Figure 11 presents the pairwise similarities between models at each epoch trained with different optimization strategies. These results demonstrate explicitly the close correspondence between SGD and GN learned features for each block. ", "page_idx": 18}, {"type": "text", "text": "I Learning rate variations of Gauss-Newton ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Figure 12 provides additional training runs of Gauss-Newton with different learning rates. These results indicate that forcing GN to learn slower is not sufficient to reduce the effect of the observed saturation of performance. ", "page_idx": 18}, {"type": "image", "img_path": "p37NlKi9vl/tmp/4903dd569119b124ed6d0390711b35abd5e5b021fc37b3029bdf4a84d85e2727.jpg", "img_caption": ["Figure 12: Train loss, test loss and test accuracy (left to right) for a RevMLP trained on CIFAR-10 with Gauss-Newton using different learning rates. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "p37NlKi9vl/tmp/2cfc4384c0428a138a24858c423ffa8d1d6658fb64149759591ee77c00b1a59a.jpg", "img_caption": ["Figure 13: Train loss, test loss, and test accuracy (left to right) for a RevMLP trained on CIFAR-10 with Gauss-Newton with weight-decay. Adam is also added for comparison. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "J Adding Regularization to Gauss-Newton ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this Section we explore wether weight-decay can be used to improve the performance of GaussNewton. We use a RevMLP on the full CIFAR-10 dataset with the same setting presented in Section 5 and we add weight decay to the loss during training. We tune the strength of the weight decay using the validation set. Results are shown in Figure 13. We notice that weight decay has a minimal effect of the performance of Gauss-Newton. ", "page_idx": 19}, {"type": "text", "text": "K Pseudo-Inverse Regularization ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this Section we explore the effects of different strategies for regularizing the pseudoinverse in the proposed Gauss-Newton update (see equations (16), (17)). We note that regularization is necessary, as the presence of very small singular values causes numerical instabilities. We compute the pseudoinverse using a singular value decomposition, and we try three different strategies: ", "page_idx": 19}, {"type": "text", "text": "\u2022 Damping: we add a constant to all the singular values. In particular we add a quantity equal to $1\\%$ of the maximum singular value (this quantity of damping was tuned by selecting the best performing one over the values $1\\%$ , $10\\%,0.1\\%)$ .   \n\u2022 Truncation: we set to zero all the singular values smaller than a certain threshold. In particular, we use relative tolerance of $1\\%$ with respect to the largest singular value and an absolute tolerance of $10^{-5}$ (we tune this values in a similar fashion to the previous method). This is the strategy used for the results in Section 5.   \n\u2022 Noise: we add noise to the matrix to be pseudoinverted; we then compute the SVD and use all singular values. The noise is sampled from a zero-mean Gaussian with a standard deviation equal to $10\\%$ (this value was selected though a tuning procedure as above) of the standard deviation of the matrix to be pseudoinverted. ", "page_idx": 19}, {"type": "text", "text": "Results are shown in Figure 14, where Adam is also added for comparison. We notice that there is a small difference between damping and truncating (with the former performing slightly better), while adding noise does not seem as effective. Nevertheless, Gauss-Newton is always under-performing when compared to Adam. ", "page_idx": 19}, {"type": "text", "text": "L Full Hyperparameters & Experimental Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this Section we provide additional details on the hyperparameters and experimental details used for our experiments. Full code to reproduce our results is also provided with the submission. ", "page_idx": 19}, {"type": "image", "img_path": "p37NlKi9vl/tmp/f4e00c0f6280b3f85ee99b27102fbb13c91e42e439bb1d66444a6ea8d7bf001b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 14: Train loss, test loss, and test accuracy (left to right) for a RevMLP trained on CIFAR-10 with Gauss-Newton using different regularization strategies for the pseudoinverse. Adam is also added for comparison. ", "page_idx": 20}, {"type": "image", "img_path": "p37NlKi9vl/tmp/834762fea19192ba0c7b28c3807a7dfe025e1790f0264e5521e4d6bfb8f4da2d.jpg", "img_caption": ["Figure 15: Train loss and test loss on UCI (a) wine and (b) superconductivity regression datasets. (Full-batch on top, mini-batch on bottom). "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Implementation details. Our code is based on the PyTorch framework Paszke et al. [2019]. In more detail we use version 2.0 for Linux with CUDA 12.1. ", "page_idx": 20}, {"type": "text", "text": "Weight initialization. We use standard Xavier Glorot and Bengio [2010] initialization for the weights, while we initialize the biases to zero. ", "page_idx": 20}, {"type": "text", "text": "Sampling the inverted bottleneck. We sample the entries of each inverted bottleneck from a zero-centered gaussian with a variance oflayer di1mension. ", "page_idx": 20}, {"type": "text", "text": "Data augmentations. For the MNIST dataset we do not use any data augmentations. For the CIFAR-10 dataset we follow the standard practice of applying random crops and resizes. We do not use data augmentations for the regression datasets. ", "page_idx": 20}, {"type": "text", "text": "Additional hyperparameters for Adam. We tune the learning rate for each experiment and method, as explained in Section 5, and we use the PyTorch default values for the betas parameters in Adam. ", "page_idx": 20}, {"type": "text", "text": "M Regression Results ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We report the results for the regression experiments in Figure 15. ", "page_idx": 20}, {"type": "text", "text": "N Proof Layer-wise Right-inverse is the Moore-Penrose Pseudo-inverse ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Consider the Jacobian for layer $\\ell$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{J_{\\ell}=\\left(\\frac{\\partial x_{L}}{\\partial x_{\\ell}^{(1)}}\\quad\\frac{\\partial x_{L}}{\\partial x_{\\ell}^{(2)}}\\right)\\left(\\begin{array}{c c}{A}&{0}\\\\ {B}&{C}\\end{array}\\right)=}\\\\ {\\left(\\frac{\\partial x_{L}}{\\partial x_{\\ell}^{(1)}}\\quad\\frac{\\partial x_{L}}{\\partial x_{\\ell}^{(2)}}\\right)\\left(\\sigma_{\\ell}\\left(V_{\\ell-1}^{(2)}X_{\\ell-1}^{(2)}\\right)^{T}\\otimes\\operatorname{I}_{d/2}\\right.}&{{}\\left.0}\\\\ {\\left.\\frac{\\partial x_{\\ell}^{(2)}}{\\partial w_{\\ell}^{(1)}}\\right.}&{{}\\left.\\sigma_{\\ell}\\left(V_{\\ell}^{(1)}X_{\\ell}^{(1)}\\right)^{T}\\otimes\\operatorname{I}_{d/2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Denote J\u2113,1 $J_{\\ell,1}={\\binom{A}{B}}_{C}^{\\phantom{\\ell}0}\\qquad$ For our method, we used ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J_{\\ell,1}^{-}=\\left(\\!\\!\\begin{array}{c c}{A^{+}}&{0}\\\\ {-C^{+}B A^{+}}&{C^{+}}\\end{array}\\!\\!\\right)\\mathrm{~with~}}\\\\ &{A^{+}=\\sigma_{\\ell}\\left(V_{\\ell-1}^{(2)}X_{\\ell-1}^{(2)}\\right)^{T+}\\otimes\\mathbf{I}_{d/2},}\\\\ &{B=\\frac{\\partial x_{\\ell}^{(2)}}{\\partial w_{\\ell}^{(1)}},}\\\\ &{C^{+}=\\sigma_{\\ell}\\left(V_{\\ell}^{(1)}X_{\\ell}^{(1)}\\right)^{T+}\\otimes\\mathbf{I}_{d/2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then: ", "page_idx": 21}, {"type": "equation", "text": "$$\nJ_{\\ell,1}^{-1}J_{\\ell,1}=\\binom{A^{+}A}{C^{+}B-C^{+}B A^{+}A}\\quad{\\cal C}^{+}{\\cal C}\\bigg)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We show below that $B=B A^{+}A$ , and hence that $J_{\\ell,1}^{-1}=J_{\\ell,1}^{+}$ (i.e., our right-inverse corresponds to the Moore-Penrose Pseudo-inverse). ", "page_idx": 21}, {"type": "text", "text": "We can show that (see Section N.1): ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle B_{i+\\frac{d}{2}(j-1),a+\\frac{d}{2}(b-1)}=\\left(\\frac{\\partial x_{\\ell}^{(2)}}{\\partial w_{\\ell}^{(1)}}\\right)_{i+\\frac{d}{2}(j-1),a+\\frac{d}{2}(b-1)}}}\\\\ {{\\displaystyle=\\sigma_{\\ell}\\left(V_{\\ell-1}^{(2)}X_{\\ell-1}^{(2)}\\right)_{j,b}^{T}\\sum_{k}\\left(W_{\\ell,G}\\right)_{i,k}\\left(V_{\\ell}^{(1)}\\right)_{k,a}\\sigma_{\\ell}^{\\prime}\\left(V_{\\ell}^{(1)}X_{\\ell}^{(1)}\\right)_{k,j}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "With the above, we first compute $(B A^{+})$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(B A^{\\bot})_{i+\\frac12(\\beta(l-1),i+\\frac12(\\beta(l-1))}}\\\\ &{\\quad=\\sum_{k=\\frac{1}{\\beta}}(B_{1i+\\frac12(\\beta(l-1),i+\\frac12(\\beta(l-1))}(A^{+})_{i+\\frac12(\\beta(l-1),i+\\frac12(\\beta(l-1))}}\\\\ &{\\quad=\\sum_{j=2}^{\\infty}(B)_{i+\\frac12(\\beta(l-1),i+\\frac12(\\beta(l-1))^{2}}\\sigma\\left(V_{i-1}^{(2)}X_{i-1}^{(2)}\\right)_{j,k,j}^{-+}\\right.}\\\\ &{\\quad\\left.=\\sum_{k=\\frac{1}{\\beta}}(\\gamma_{--1}^{(2)}X_{i-1}^{(2)})_{j,k,j}^{-}\\left(\\mathfrak{N}_{l}C_{i,l_{1},k}\\left(V_{i}^{(1)}\\right)_{k,j_{1},k}\\sigma_{i}^{\\prime}\\left(V_{i}^{(1)}X_{i}^{(1)}\\right)_{k,j_{1},k}\\sigma_{i}\\left(V_{i-1}^{(2)}X_{i-1}^{(2)}\\right)_{j,k,j_{1},j_{1}}^{-+}}\\\\ &{\\quad=\\sum_{k}^{\\infty}(\\mathfrak{N}_{\\ell,G})_{i,k}\\left(V_{i}^{(1)}\\right)_{k,i,k}\\sigma_{i}^{\\prime}\\left(V_{i}^{(1)}X_{i}^{(1)}\\right)_{k,j_{1},k}\\cdot}\\\\ &{\\qquad\\qquad\\qquad\\left.\\sum_{k}^{\\left\\{\\sigma\\left(V_{i}^{(2)}\\right)\\right\\}}_{j=1}^{N}\\sigma_{j}\\left(V_{i}^{(2)}X_{i-1}^{(2)}\\right)_{k,j_{1},k}^{-}\\left(V_{i}^{(2)}X_{i-1}^{(2)}\\right)_{j,k,j}^{-+}}\\\\ &{\\qquad\\qquad\\qquad\\left.=\\sum_{k}^{\\infty}(\\mathfrak{N}_{\\ell,G})_{i,k}\\left(V_{i}^{(1)}\\right)_{k,i,k}\\sigma_{i}^{\\prime}\\left(V_{i}^{(1)}X_{i-1}^{(2)}\\right)_{k,j_{1},j_{\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and finally ", "text_level": 1, "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{|B(\\mathcal{A}^{\\lambda}\\lambda_{1}+\\{\\xi(x,\\cdot)\\}_{{1\\leq i}+\\frac{\\delta}{i}})(z-\\nu)-z|}}\\\\ &{=\\sum_{i=1}^{D}((\\delta^{\\lambda}\\lambda_{i})_{i+\\frac{\\delta}{i}+\\frac{\\delta}{i}})\\epsilon_{\\alpha}([\\delta\\,\\sigma_{i}^{(\\lambda)},\\delta])_{\\mathrm{i,f}}\\;[\\delta\\,\\sigma_{i}^{(\\lambda)},\\delta]}\\\\ &{=-\\sum_{i}((\\delta^{\\lambda}\\lambda_{i})_{i+\\frac{\\delta}{i}+\\frac{\\delta}{i}}(z-\\nu)_{i+\\frac{\\delta}{i}})\\epsilon_{\\alpha}([\\nu_{\\alpha}^{(\\lambda)},\\delta])_{\\mathrm{i,f}}\\;[\\delta\\,\\sigma_{i}^{(\\lambda)}\\,\\delta]}\\\\ &{=-\\sum_{i=1}^{D}(\\mathrm{Re})_{\\partial_{\\lambda}\\phi_{i}}\\Big([\\nu_{\\alpha}^{(\\lambda)},\\delta]\\Big)_{\\mathrm{i,f}}\\;\\sigma_{i}^{\\alpha}\\Big([\\nu_{\\beta}^{(\\lambda)},\\delta]\\Big)_{\\mathrm{i,f}}\\;,}\\\\ &{\\leq\\sum_{j=1}^{D}\\Big(\\epsilon_{j}\\,\\Big([\\sigma_{j}^{(\\lambda)}\\lambda_{j}]\\Big)_{\\mathrm{or}}\\Big([\\nu_{\\alpha}^{(\\lambda)},\\delta\\Big(\\nu_{\\beta}^{(\\lambda)}\\Big)^{T}\\Big)\\Big)_{\\mathrm{i,f}}\\;\\sigma_{j}\\Big([\\nu_{\\beta}^{(\\lambda)},\\delta\\,\\Big(\\nu_{\\alpha}^{(\\lambda)}\\Big)^{T}\\Big)\\Big)_{\\mathrm{i,f}}}\\\\ &{=-\\sum_{i}^{D}(\\mathrm{Re})_{\\partial_{\\lambda}\\phi_{i}}\\Big([\\nu_{\\alpha}^{(\\lambda)},\\delta]\\Big)_{\\mathrm{i,f}}\\;\\sigma_{i}^{\\alpha}\\Big([\\nu_{\\beta}^{(\\lambda)},\\delta]\\Big)_{\\mathrm{i,f}}\\;}\\\\ &{\\qquad\\sum_{k=1}^{D}\\Big(\\epsilon_{j}\\,\\Big([\\nu_{\\alpha}^{(\\lambda)},\\delta\\,]\\Big)^{T}\\Big)_{\\rho}\\;\\sigma_{k}\\Big([\\nu_{\\beta}^{(\\lambda)},\\delta\\,]\\Big \n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "(68) ", "page_idx": 22}, {"type": "text", "text": "N.1 Expression for $B$ ", "text_level": 1, "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{B_{i+\\frac12(i-1)\\dots i+\\frac12}=}&{\\left(\\frac{\\partial E_{i}^{(1)}}{\\partial x_{i}^{(1)}}\\right)_{i+\\frac12(i-1)\\dots i}}\\\\ &{=}&{\\left(\\frac{\\partial U_{i}^{(2)}}{\\partial x_{i}^{(1)}\\partial x_{i}+(i-1)}\\right)}\\\\ &{=}&{\\left(\\frac{\\partial U_{i}^{(2)}}{\\partial x_{i}^{(1)}\\partial x_{i}}\\right)_{i+\\frac12(i-1)\\dots i}}\\\\ &{=}&{\\left(\\frac{\\partial U_{i}^{(2)}}{\\partial x_{i}\\partial x_{i}}\\right)_{i+\\frac12(i-1)\\dots i}}\\\\ &{=}&{\\frac{\\partial(\\mathrm{WL}_{\\sigma}\\sigma(\\mathrm{F}_{i}^{(1)}X_{i}^{(1)}))_{i,j}}{\\partial x_{i}\\partial x_{i}}}\\\\ &{=}&{-\\sum_{k}(W_{i}\\omega_{i,k}\\frac{\\partial u_{k}^{(2)}(X_{i}^{(1)}X_{i}^{(1)})}{\\partial x_{i}\\partial x_{j}})_{k,j}}\\\\ &{=}&{\\sum_{k}(W_{i}\\omega_{i,k})_{\\sigma}\\psi\\left(V_{i}^{(1)}X_{i}^{(1)}X_{i}^{(1)}\\right)_{i,j}}\\\\ &{=}&{\\sum_{k}(W_{i}\\omega_{i,k})_{\\sigma}\\psi\\left(V_{i}^{(1)}X_{i}^{(1)}X_{j}^{(1)}\\right)_{i,j}\\frac{\\partial\\left(V_{i}^{(1)}X_{i}^{(1)}\\right)_{i,j}}{\\partial x_{i}\\partial x_{j}}}\\\\ &{=}&{\\sum_{k}(W_{i}\\omega_{i,k})_{\\sigma}\\psi\\left(V_{i}^{(1)}X_{i}^{(1)}\\right)_{i,j}\\psi_{i}^{(1)},\\quad\\hat{\\psi}_{i}^{(1)}(W_{i}^{(2)}\\right)_{i,j}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\displaystyle\\sum_{k,t}\\left(W_{\\ell,G}\\right)_{i,k}\\sigma_{\\ell}^{\\prime}\\left(V_{\\ell}^{(1)}X_{\\ell}^{(1)}\\right)_{k,j}\\left(V_{\\ell}^{(1)}\\right)_{k,t}\\frac{\\partial\\left(W_{\\ell}^{(1)}\\sigma_{\\ell}\\left(V_{\\ell-1}^{(2)}X_{\\ell-1}^{(2)}\\right)\\right)_{t,j}}{\\partial(W_{\\ell}^{(1)})_{a,b}}}\\\\ &{=\\displaystyle\\sum_{k,t,s}\\left(W_{\\ell,G}\\right)_{i,k}\\sigma_{\\ell}^{\\prime}\\left(V_{\\ell}^{(1)}X_{\\ell}^{(1)}\\right)_{k,j}\\left(V_{\\ell}^{(1)}\\right)_{k,t}\\sigma_{\\ell}\\left(V_{\\ell-1}^{(2)}X_{\\ell-1}^{(2)}\\right)_{s,j}\\frac{\\partial\\left(W_{\\ell}^{(1)}\\right)_{t,s}}{\\partial(W_{\\ell}^{(1)})_{a,b}}}\\\\ &{=\\displaystyle\\sum_{k}\\left(W_{\\ell,G}\\right)_{i,k}\\sigma_{\\ell}^{\\prime}\\left(V_{\\ell}^{(1)}X_{\\ell}^{(1)}\\right)_{k,j}\\left(V_{\\ell}^{(1)}\\right)_{k,a}\\sigma_{\\ell}\\left(V_{\\ell-1}^{(2)}X_{\\ell-1}^{(2)}\\right)_{b,j}}\\\\ &{=\\sigma_{\\ell}\\left(V_{\\ell-1}^{(2)}X_{\\ell-1}^{(2)}\\right)_{j,b}^{T}\\displaystyle\\sum_{k}\\left(W_{\\ell,G}\\right)_{i,k}\\left(V_{\\ell}^{(1)}\\right)_{k,a}\\sigma_{\\ell}^{\\prime}\\left(V_{\\ell}^{(1)}X_{\\ell}^{(1)}\\right)_{k,j}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: All claims made in the abstract and introduction are reflected in the paper. We show that GN updates computed with any generalized inverse of the model Jacobian results in the same dynamics of the loss, and we introduce a tractable form of the Gauss-Newton update for reversible neural networks in Section 4. We then study its behaviour showing poor generalization in Section 5. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We discuss limitations in Section 6. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We provide full assumptions and proofs for all the theoretical results introduced in the paper. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We provide all details about architecture, datasets, and hyperparameters in the main text and appendix. Code is also provided with the submission. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We provide code with instructions to replicate our results. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Yes we provide all information regarding the training details and test details. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Yes our results are averaged over multiple seeds, and we include error bars. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: In the experimental Section we provide a description of our computational setting (single NVIDIA RTXA6000 GPU). ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper adheres to the NeurIPS Code of Ethics. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: the paper has mainly theoretical motivations and outcomes, so there is no direct societal impact of the work performed. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 27}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: the paper poses no such risks. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: we provide a reference for the datasets used in this paper. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: the paper does not release new assets. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]