[{"figure_path": "p37NlKi9vl/figures/figures_6_1.jpg", "caption": "Figure 1: Training loss and accuracy on (a) MNIST and (b) CIFAR-10 in a full-batch scenario where each dataset is trimmed to a fixed subset of n = 1024 images. GN converges much faster than Adam and SGD.", "description": "This figure compares the training loss and accuracy of three different optimizers (GN, Adam, and SGD) on MNIST and CIFAR-10 datasets.  The key observation is that the Gauss-Newton (GN) optimizer converges significantly faster than both Adam and SGD in a full-batch setting where datasets were reduced to 1024 images.  The figure highlights the superior convergence speed of GN in this specific setting.", "section": "5 Experiments"}, {"figure_path": "p37NlKi9vl/figures/figures_6_2.jpg", "caption": "Figure 2: Training loss, test loss, and test accuracy on (a) MNIST and (b) CIFAR-10 in a mini-batch scenario. GN does not exhibit the same properties observed in the full-batch setting. In fact, Adam reaches lower training and test loss.", "description": "This figure compares the performance of Gauss-Newton (GN), Adam, and SGD optimizers on MNIST and CIFAR-10 datasets using mini-batch training.  It shows that unlike the full-batch scenario (Figure 1), GN does not outperform Adam and SGD in this setting.  GN exhibits rapid saturation of training and test loss, while Adam achieves lower loss values. This demonstrates that the superior performance of GN observed in the full-batch setting does not translate to the mini-batch setting.", "section": "5 Experiments"}, {"figure_path": "p37NlKi9vl/figures/figures_6_3.jpg", "caption": "Figure 3: Percentage change in training loss after each update. GN decreases the loss for the current mini-batch more than SGD and Adam early in training.", "description": "This figure shows the percentage change in the training loss after each update for three different optimizers: Gauss-Newton (GN), Adam, and SGD.  The results demonstrate that GN initially reduces the loss more significantly on each mini-batch than Adam and SGD, especially in the early stages of training.  This difference gradually diminishes over time, but persists even after the overall training and test losses have saturated.", "section": "5 Experiments"}, {"figure_path": "p37NlKi9vl/figures/figures_7_1.jpg", "caption": "Figure 4: NTK and CKA similarity evolution across training for GN, Adam and SGD. Top two panels include (a) the rate of change of the NTK and (b) NTK similarity during training with respect to initialization. Bottom three panels, along the same axis, include the CKA similarities for the (c) first, (d) middle and (e) last block with respect to their initial values.", "description": "This figure compares the evolution of the Neural Tangent Kernel (NTK) and the Centered Kernel Alignment (CKA) across training for three different optimizers: Gauss-Newton (GN), Adam, and SGD.  The top panel (a) shows the rate of change of the NTK over epochs, indicating how much the NTK changes during training. Panel (b) displays the NTK similarity to the initial NTK across epochs, showing how similar the learned NTK is to the initial one. The bottom three panels (c-e) show the CKA similarity for three different blocks (first, middle, and last) of the network to their initial CKA values over epochs. These plots illustrate the evolution of neural representations during training and how these representations change with different optimizers.", "section": "Evolution of the Neural Tangent Kernel"}, {"figure_path": "p37NlKi9vl/figures/figures_8_1.jpg", "caption": "Figure 5: Experiments on CIFAR using a model without Inverted Bottleneck (Full-batch on the left, mini-batch on center and right). While the theoretical guarantees do not hold in this setting, the results follow the same trend observed in Figure 2.", "description": "This figure shows the results of experiments conducted on the CIFAR-10 dataset using RevMLPs without inverted bottlenecks.  The left panel displays results from a full-batch training setting, while the center and right panels show results from a mini-batch setting.  Despite the theoretical guarantees not applying without inverted bottlenecks, the results show a consistent trend with Figure 2, demonstrating that the poor generalization performance of Gauss-Newton in mini-batch settings persists even without this specific architectural feature.", "section": "5.3 Experiments without Inverted Bottleneck"}, {"figure_path": "p37NlKi9vl/figures/figures_16_1.jpg", "caption": "Figure 6: Training loss, test loss, and test accuracy when training for a longer amount of epochs on CIFAR-10 shows that Gauss-Newton is unable to further decrease the training loss, while even plain SGD can reach lower values.", "description": "This figure shows the training and testing results for three different optimizers (GN, Adam, and SGD) on the CIFAR-10 dataset.  The key takeaway is that while Gauss-Newton initially performs well and converges much faster than the others, it fails to further reduce the loss even after prolonged training, unlike Adam and even SGD, which continue to improve after many epochs. This highlights a significant limitation of GN in the mini-batch setting, specifically its inability to continue learning useful features after initially overfitting to the mini-batches.", "section": "E Longer training curves"}, {"figure_path": "p37NlKi9vl/figures/figures_16_2.jpg", "caption": "Figure 7: Training loss when first using Adam (or GN), and then continuing with GN (or Adam) \u2013 the purple dashed line indicates the 50 epochs mark at which the optimizers are switched. GN shows early saturation of the loss even when starting from a better intialization point.", "description": "This figure displays the training loss curves when using two different optimizers sequentially.  First, a model is trained for 50 epochs using either Adam or Gauss-Newton. Then, training continues for an additional 1000 epochs, switching to the other optimizer. The purple dashed vertical line marks the 50-epoch transition point.  The plot shows that Gauss-Newton exhibits early saturation of the training loss, even when initialized with weights from a well-trained Adam model, indicating its limited ability to escape from suboptimal solutions or improve generalization despite potentially good initial progress.", "section": "F Initialization dependencies for Gauss-Newton"}, {"figure_path": "p37NlKi9vl/figures/figures_17_1.jpg", "caption": "Figure 8: GN Train and test loss with weights initialized accounding to different variances at initialization \u03c3.", "description": "This figure shows the training and test loss curves for Gauss-Newton (GN) optimization with different weight initialization variances (\u03c3 = 10\u207b\u00b2, 10\u207b\u00b3, 10\u207b\u2076).  The results illustrate the impact of weight initialization on the performance of GN, especially in the context of generalization. It demonstrates how different variances affect the training and test loss, highlighting the sensitivity of GN's performance to initialization parameters.", "section": "F Initialization dependencies for Gauss-Newton"}, {"figure_path": "p37NlKi9vl/figures/figures_17_2.jpg", "caption": "Figure 9: Cosine similarity with the initial weight initialization across training. ADAM and GN move similarly in weight space indicating a consistent behaviour in weight space between the two optimizers. Note that, in this Figure, we use the term \"layer\" to refer to half-coupled layer in the reversible blocks.", "description": "This figure shows the cosine similarity between the weights at initialization and at the end of training for different optimizers.  The results indicate that Adam and Gauss-Newton (GN) exhibit similar behavior in weight space, while SGD shows considerably less change. The \"layer\" designation refers to a half-coupled layer within the reversible blocks of the network architecture.", "section": "G Change in weights during training"}, {"figure_path": "p37NlKi9vl/figures/figures_18_1.jpg", "caption": "Figure 10: CKA similarity evolution across training for GN, Adam and SGD. GN maintains a high CKA similarity with its initial feature space, very similarly to SGD.", "description": "This figure shows the evolution of the Centered Kernel Alignment (CKA) similarity between the neural network representations at initialization and at each epoch during training, for three different optimizers: Gauss-Newton (GN), Adam, and SGD. The results are presented for six blocks of a neural network trained on the CIFAR-10 dataset.  The figure demonstrates that GN and SGD maintain a high CKA similarity throughout training, indicating that their learned representations remain very close to their initial representations. In contrast, Adam shows a significant decrease in CKA similarity over time, suggesting that it learns features that are different from those present at initialization.", "section": "5.2 Mini-Batch Setting"}, {"figure_path": "p37NlKi9vl/figures/figures_18_2.jpg", "caption": "Figure 10: CKA similarity evolution across training for GN, Adam and SGD. GN maintains a high CKA similarity with its initial feature space, very similarly to SGD.", "description": "This figure shows the evolution of the Centered Kernel Alignment (CKA) similarity between the neural network representations at initialization and at different epochs during training, for three different optimizers: Gauss-Newton (GN), Adam, and SGD.  The results indicate that GN and SGD maintain a high similarity to their initial feature representations throughout training, suggesting that they operate in a \"lazy\" training regime where they do not significantly change their feature representations. In contrast, Adam shows a much greater change in CKA similarity over time, indicating a substantial change in feature representations.", "section": "Evolution of the Neural Tangent Kernel"}, {"figure_path": "p37NlKi9vl/figures/figures_19_1.jpg", "caption": "Figure 12: Train loss, test loss, and test accuracy (left to right) for a RevMLP trained on CIFAR-10 with Gauss-Newton using different learning rates.", "description": "This figure shows the training and test performance of a RevMLP model trained on CIFAR-10 using Gauss-Newton optimization with different learning rates.  The three subplots display the training loss, test loss, and test accuracy over 100 epochs.  Each line represents a different learning rate (10\u207b\u00b9, 10\u207b\u00b3, 10\u207b\u2074). The figure illustrates the effect of the learning rate on the convergence and generalization performance of Gauss-Newton in this specific experimental setting.", "section": "I Learning rate variations of Gauss-Newton"}, {"figure_path": "p37NlKi9vl/figures/figures_19_2.jpg", "caption": "Figure 13: Train loss, test loss, and test accuracy (left to right) for a RevMLP trained on CIFAR-10 with Gauss-Newton with weight-decay. Adam is also added for comparison.", "description": "This figure compares the training and testing performance of three optimizers on the CIFAR-10 dataset: Gauss-Newton (GN), GN with weight decay, and Adam.  The plots show that adding weight decay to the GN optimizer does not significantly improve its performance, and it still underperforms Adam, particularly in terms of generalization as measured by test accuracy.", "section": "5.3 Experiments without Inverted Bottleneck"}, {"figure_path": "p37NlKi9vl/figures/figures_20_1.jpg", "caption": "Figure 14: Train loss, test loss, and test accuracy (left to right) for a RevMLP trained on CIFAR-10 with Gauss-Newton using different regularization strategies for the pseudoinverse. Adam is also added for comparison.", "description": "This figure compares the performance of Gauss-Newton with different regularization techniques for the pseudoinverse on the CIFAR-10 dataset.  It shows the training loss, test loss, and test accuracy for Gauss-Newton with truncation, damping, and noise added to the pseudoinverse, and compares the results to the Adam optimizer. The figure illustrates how different regularization approaches affect the training and generalization of the Gauss-Newton method.", "section": "K Pseudo-Inverse Regularization"}, {"figure_path": "p37NlKi9vl/figures/figures_20_2.jpg", "caption": "Figure 2: Training loss, test loss, and test accuracy on (a) MNIST and (b) CIFAR-10 in a mini-batch scenario. GN does not exhibit the same properties observed in the full-batch setting. In fact, Adam reaches lower training and test loss.", "description": "This figure compares the performance of Gauss-Newton (GN), Adam, and SGD optimizers on MNIST and CIFAR-10 datasets using mini-batch training.  It shows training loss, test loss, and test accuracy over epochs. The key observation is that GN, unlike its full-batch behavior (shown in Figure 1), fails to maintain its superior performance and is even outperformed by Adam, which achieves lower training and testing losses.", "section": "Experiments"}]