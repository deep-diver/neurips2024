[{"type": "text", "text": "Symmetric Linear Bandits with Hidden Symmetry ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Nam Phuong Tran   \nDepartment of Computer Science   \nUniversity of Warwick   \nCoventry, United Kingdom   \nnam.p.tran@warwick.ac.uk   \nThe Anh Ta   \nCSIRO\u2019s Data61   \nMarsfield, NSW, Australia   \ntheanh.ta@csiro.au ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "Debmalya Mandal ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Department of Computer Science University of Warwick Coventry, United Kingdom debmalya.mandal@warwick.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Long Tran-Thanh Department of Computer Science University of Warwick Coventry, United Kingdom long.tran-thanh@warwick.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "High-dimensional linear bandits with low-dimensional structure have received considerable attention in recent studies due to their practical significance. The most common structure in the literature is sparsity. However, it may not be available in practice. Symmetry, where the reward is invariant under certain groups of transformations on the set of arms, is another important inductive bias in the highdimensional case that covers many standard structures, including sparsity. In this work, we study high-dimensional symmetric linear bandits where the symmetry is hidden from the learner, and the correct symmetry needs to be learned in an online setting. We examine the structure of a collection of hidden symmetry and provide a method based on model selection within the collection of low-dimensional subspaces. Our algorithm achieves a regret bound of $O(d_{0}^{2/3}T^{2/3}\\log(d))$ , where $d$ is the ambient dimension which is potentially very large, and $d_{0}$ is the dimension of the true low-dimensional subspace such that $d_{0}\\ll d$ . With an extra assumption on well-separated models, we can further improve the regret to $O(d_{0}{\\sqrt{T\\log(d)}})$ . ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Stochastic bandit is a sequential decision-making problem in which a player, who aims to maximize her reward, selects an action at each step and receives a stochastic reward, drawn from an initially unknown distribution of the selected arm, in response. Linear stochastic bandit (LSB) [1] is an important variant in which the expected value of the reward is a linear function of the action. It is one of the most studied bandit variants and has many practical applications [27]. ", "page_idx": 0}, {"type": "text", "text": "Actions in LSB are specified as feature vectors in $\\mathbb{R}^{d}$ for very large feature dimension $d$ , with performance i.e. the resulting regret scaling with $d$ . Many works have addressed this curse of dimensionality by leveraging different low-dimensional structures as inductive biases for the learner. For example, sparsity, which assumes that the reward is a sparse linear function, has been used extensively in LSB to design bandit algorithms with better performance [2, 36, 23]. However, when the reward function lacks the structure for sparsity (which may occur in many real-world situations), a question arises: Are there different structures in the features of LSB that we can exploit to overcome the curse of dimensionality and design bandit algorithms with better performance? ", "page_idx": 0}, {"type": "text", "text": "In this paper, we study the inductive bias induced by symmetry structures in LSB, which is a more general model inductive bias than sparsity, and can facilitate efficient and effective learning [9]. Symmetry describes how, under certain transformations of the input of the problem, the outcome should either remain unchanged (invariance) or shift predictably (equivariance). In supervised learning, it has been empirically observed [17, 12] and theoretically proven [16, 6] that explicitly integrating symmetry into models leads to improved generalization error. However, in the literature on sequential decision-making, unlike sparsity, symmetry is rarely considered to date. This leads us to the following research question: Can one leverage symmetry in sequential decision-making tasks to enable effective exploration, and eventually break the curse of dimensionality? ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In the machine learning literature, especially in supervised learning, most studies on symmetry assume prior knowledge of symmetry structures of the tasks under consideration [31, 16]. However, in numerous practical scenarios, the learner can only access to partial knowledge of the symmetry, necessitating the incorporation of symmetry learning mechanisms into the algorithms to achieve better performance. Examples of hidden symmetry can be found in multi-agent learning with cooperative behavior. As a motivating example, consider a company undertaking a large project that consists of several subtasks. The company must hire subcontractors with the goal of maximizing project quality while staying within budget constraints. Symmetry may arise in this situation when coalitions form among subcontractors, where members of a coalition work together to complete their allocated tasks using shared resources. In particular, the allocation of tasks within a coalition can be swapped without affecting overall team performance, inducing symmetry (i.e., performance remains invariant under permutation) in the task assignments. Coalitions among subcontractors often arise since sharing labor and resources reduces operational costs, making their work more efficient and cost-effective. However, these coalitions are typically hidden from the hiring company. One reason is that if the hiring company were aware of these collaborations, they could use this information to negotiate lower prices, knowing that the subcontractors are benefiting from shared resources. Another reason is that coalitions may raise concerns about collusion. In particular, in a competitive market, such as when subcontractors are hired through a bidding platform, coalition members can collaborate to manipulate the bidding process, which is considered unfair and could undermine the integrity of the bidding. For more practical examples of hidden symmetry in multi-agent reinforcement learning [34] and robotics [3], we refer the reader to Appendix D.2. Motivated by these examples, we believe that hidden symmetry is much more relevant in the context of sequential decision-making because the environment and its symmetry structure may not be readily available to the learner, as opposed to supervised learning and offline settings where data are provided during the training phase. As the learner has the power to freely collect data, it is expected that they will learn the hidden symmetry structures as they explore the environment. ", "page_idx": 1}, {"type": "text", "text": "Against this background, we ask the question of whether learner can leverage symmetry to enable effective exploration, and break the curse of dimensionality without a prior knowledge of the symmetry structure? Moreover, in the presence of symmetry, when can we design learning algorithms with optimal regret bounds? Towards answering this question, we investigate the setting of symmetric linear stochastic bandit in $\\mathbb{R}^{d}$ , where $d$ is potentially very large, and the expected reward function is invariant with respect to the actions of a hidden group $\\mathcal{G}$ of coordinate permutations. Our contributions are summarised as follows: ", "page_idx": 1}, {"type": "text", "text": "1. We first give an impossibility result that no algorithm can get any benefti by solely knowing that $\\mathcal{G}$ is a subgroup of permutation matrices. We achieve this by formally establishing a relation between the class of subgroup to the partition over the set $\\{1,...,d\\}$ . A direct implication of this impossibility result is that it is necessary to have further information about the structure of the hidden subgroup in order to achieve improved regret bounds. ", "page_idx": 1}, {"type": "text", "text": "2. Given this, we establish a cardinality condition on the class of symmetric linear bandits with hidden $\\mathcal{G}$ , in which the learner can learn the true symmetry structure and overcome the curse of dimensionality. Notably, this class includes a sparsity as a special case, and therefore inherits all the computational and statistical complexities of sparsity. Apart from sparsity, this class includes many other practically relevant classes, such as partitions that respect underlying hierarchies, non-crossing partitions, and non-nesting partitions (see Subsection 4.2.1 and Appendix D.2). ", "page_idx": 1}, {"type": "text", "text": "3. We cast our problem of learning with hidden subgroup $\\mathcal{G}$ into model selection with collection of low-dimensional subspaces [25, 35]. To address the polynomial scaling of regret bounds with respect to the number of models and arms in previous works, we depart from model aggregation, which is typically used in LSB model selection, and introduce a new framework inspired by Gaussian model selection [7] 1and compressed sensing [8]. Based on this framework, we introduce a new algorithm, called EMC (for Explore Models then Commit). Under the assumption that the set of arm is explorat\u221aory, we prove that the regret bound of the EMC algorithm is $O(d_{0}^{2/3}T^{2/3}\\log(d))$ , and $O(d_{0}{\\sqrt{T}}\\log(d))$ with an additional assumption on well-separated partitions, where $d_{0}\\ \\ll\\ d$ is the dimension of the low-dimensional subspace associated with group $\\mathcal{G}$ . ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "To the best of our knowledge, our work is the first in the linear stochastic bandits literature that leverages symmetry in designing provably efficient algorithms. To save space, all proofs in this paper are deferred to the Appendix. ", "page_idx": 2}, {"type": "text", "text": "1.1 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We now briefly outline related work and compare them with our results. We refer the reader to Appendix F for a more in-depth literature review. ", "page_idx": 2}, {"type": "text", "text": "Sparse linear bandits. As we will explain in Section 4.2, sparsity is equivalent to a subset symmetry structures, and thus, can be seen as a special case of our setting. As such, we first review the literature of spar\u221asity. Sparse linear bandits were first investigated in [2], where the authors achieve a regret of $\\bar{\\tilde{O}}(\\sqrt{d s T})$ , with $\\tilde{O}$ disregarding the logarithmic factor, and $s$ representing the sparsity leve\u221al, and $T$ is the time horizon. This matches the regret lower bound for sparse bandits, which is $\\Omega({\\sqrt{d s T}})$ [27]. More recently, the contextual version of linear bandits has gained popularity, where additional assumptions are made regarding the context distribution and set of arms [26, 36, 28, 11, 23] to avoid polynomial dependence on $d$ . Notably, with the assumption on exploratory set of arms, [23] propose an Explore then Commit style strategy that achieves $\\tilde{O}(s^{\\frac{2}{3}}T^{\\frac{2}{3}})$ , nearly matching the regret lower bound $\\Omega(s^{\\frac{2}{3}}T^{\\frac{2}{3}})$ [24] in the data-poor regime. As sparsity is equivalent to a subclass of hidden symmetry, all the lower bounds for sparse problems apply to our setting of learning with hidden symmetry. ", "page_idx": 2}, {"type": "text", "text": "Model selection. Our problem is also closely related to the problem of model selection in linear bandits, as the learner can collect potential candidates for the hidden symmetry model. Particularly, in model selection, there is a collection of $M$ features, and different linear bandits running with each of these features serve as base algorithms. By exploiting the fact that the data can be shared across all the base algorithms, the dependence of regret in terms of the number of features can be reduced to $\\log(M)$ . In particular, [25] propose a method that concatenates all $M$ features of dimension $d$ into one feature of dimension $M d$ , and uses the Lasso estimation as a aggregation of models. Their algorithm achieves a regret bound of $O(T^{\\frac{3}{4}}{\\sqrt{\\log(M)}})$ under the assumption that the Euclidean norm of the concatenated feature is bounded by a con\u221astant. However, in our \u221acase, the Euclidean norm of the concatenated feature vector can be as large as $\\sqrt{M}$ , which leads to a $\\sqrt{M}$ multiplicative factor in the regret bound. Besides, [35] uses the online aggregation oracle approach, and is able to obtain regret of $O({\\sqrt{K d T\\log(M)}})$ , where $K$ is the number of arms. In contrast, we use algorithmic mechanisms that are different from aggregation of models. In particular, we explicitly exploit the structure of the model class as a collection of subspaces and invoke results from Gaussian model selection [21, 7] and dimension reduction on the union of subspaces [8]. With this technique, we are able to achieve $O(T^{\\frac{2}{3}}\\log(M))$ , which is rate-optimal in the data-poor regime, has logarithmic dependence on $M$ without strong assumptions on the norm of concatenated features, and is independent of the number of arms $K$ . We refer the reader to Section 4.2 for a more detailed explanation. ", "page_idx": 2}, {"type": "text", "text": "Symmetry in online learning. The notion of symmetry in Markov decision process dates back to works such as [22, 40]. Generally, the reward function and probability transition are preserved under an action of a group on the state-action space. Exploiting known symmetry has been shown to help achieve better performance empirically [43, 42] or tighter regret bounds theoretically [41]. However, all these works requires knowledge of symmetry group, while our setting consider hidden symmetry group which may be considerably harder. Hidden symmetry on the context or state space has been studied by few authors, with the term context-lumpable bandits [29], meaning that the set of contexts can be partitioned into classes of similar contexts. It is important to note that the symmetry group acts differently on the context space and the action space. As we shall explain in detail in Section 3, while one can achieve a reduction in terms of regret in the case of hidden symmetry acting on context spaces [29], this is not the case when the symmetry group acts on the action space. The work closest to ours is [39], where the authors consider the setting of a $K$ -armed bandit, where the set of arms can be partitioned into groups with similar mean rewards, such that each group has at least $q>2$ arms. With the constrained partition, the instance-dependent regret bounds are shown asymptotically to be of order $\\textstyle O\\left({\\frac{K}{q}}\\log{\\bar{T}}\\right)$ . Comparing to [39], we study the setting of stochastic linear bandits with similar arms, in which the (hidden) symmetry and linearity structure may intertwine, making the problem more sophisticated. We also impose different constraints on the way one partitions the set of arms, which is more natural in the setting of linear bandits with infinite arms. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "2 Problem Setting ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "For any $k\\in\\mathbb{N}^{+}$ , denote $[k]=\\{1,\\ldots,k\\}$ . For $\\mathcal{X}\\subseteq\\mathbb{R}^{d}$ , let $\\Delta(\\mathcal X)$ denote the set of all probability measures supported on $\\mathcal{X}$ . Given a set $S\\subset\\mathbb{R}^{k}$ , for some $k>1$ , denote $\\Pi_{S}(x)$ as the Euclidean projection of $\\boldsymbol{x}\\in\\mathbb{R}^{k}$ on $S$ , and $\\mathrm{conv}(S)$ as the convex hull of $S$ . ", "page_idx": 3}, {"type": "text", "text": "We denote by $T$ the number of rounds, which is assumed to be known in advance. Each round $t\\in[T]$ , the agent chooses an arm $x_{t}\\in\\mathcal{X}\\subset\\mathbb{R}^{d}$ , and nature returns a stochastic reward $y_{t}=\\left<x_{t},\\theta_{\\star}\\right>+\\eta_{t}$ , where $\\eta_{t}$ is an i.i.d. $\\sigma$ -Gaussian random variable. Now, denote $f(x_{t})=\\mathbb{E}[y_{t}\\mid x_{t}]$ . A bandit strategy is a decision rule for choosing an arm $x_{t}$ in round $t\\in[T]$ , given past observations up to round $t-1$ . Formally, a bandit strategy is a mapping $A:(\\mathcal{X}\\times\\mathbb{R})^{T}\\overset{\\mathcal{-}}{\\rightarrow}\\Delta(\\mathcal{X})$ . ", "page_idx": 3}, {"type": "text", "text": "Let $x_{\\star}=\\arg\\operatorname*{max}_{x\\in\\mathcal{X}}f(x)$ , and let $\\begin{array}{r}{\\mathbf{R}_{T}=\\mathbb{E}\\left[\\sum_{t=1}^{T}\\left\\langle x_{\\star}-x_{t},\\theta_{\\star}\\right\\rangle\\right]}\\end{array}$ denote the expected cumulative regret. In this paper, we investigate the question whether one can get any reduction in term of regret, if the reward function is invariant under the action of a hidden group of transformations on the set of arms. We define the notion of group of symmetry as follows: ", "page_idx": 3}, {"type": "text", "text": "Group and group action. Given $d\\,\\in\\,\\mathbb{N}^{+}$ , let $\\textstyle S_{d}$ denote the symmetry group of $[d]$ , that is, $S_{d}:=\\bar{\\{h:[d]\\rightarrow[d]\\ |\\ h\\}}$ is bijective} the collection of all bijective mappings from $[d]$ to itself. We also define the group action $\\hat{\\phi}$ of $\\textstyle{\\mathcal{S}}_{d}$ on the vector space $\\mathbb{R}^{d}$ as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\phi:S_{d}\\times\\mathbb{R}^{d}}&{{}\\to\\mathbb{R}^{d}}\\\\ {\\left(g,(x_{i})_{i\\in[d]}\\right)}&{{}\\mapsto(x_{g(i)})_{i\\in[d]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In other words, a group element $g$ acts on an arm $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ by permuting the coordinates of $x$ . In the setting of linear bandit, the permutation group action also acts on the set of parameters via coordinate permutation. For brevity, we simply denote $g\\cdot\\theta$ and $g\\cdot x$ as $\\phi(g,\\theta)$ and $\\phi(g,x)$ , respectively. Denote by $A_{g}$ the permutation matrix corresponding to $g$ . We write $\\mathcal{G}\\leq S_{d}$ to denote that $\\mathcal{G}$ is a subgroup of $\\textstyle S_{d}$ . Given any point $\\theta\\in\\ensuremath{\\mathbb{R}}^{d}$ , we write $\\mathcal{G}\\cdot\\theta=\\{g\\cdot\\theta\\ |\\ g\\in\\mathcal{G}\\}$ to denote the orbit of $\\theta$ under $\\mathcal{G}$ . It is well known that the orbit induced by the induced action of a subgroup $\\mathcal{G}\\leq S_{d}$ corresponds to a set partition of $[d]$ . We denote this partition as $\\pi_{\\mathcal{G}}$ . ", "page_idx": 3}, {"type": "text", "text": "Let $\\mathcal{G}$ be a subgroup of $\\textstyle{\\mathcal{S}}_{d}$ that acts on $\\mathbb{R}^{d}$ via the action $\\phi$ . In a symmetric linear bandit, the expected reward is invariant under the group action of $\\mathcal{G}$ on $\\mathcal{X}$ , that is, $f(g\\cdot x)=f(x)$ . Due to the linear structure of $f$ , this is equivalent to $g\\cdot\\theta_{\\star}=\\theta_{\\star}$ for all $g\\in{\\mathcal{G}}$ . We assume that, while the group action $\\phi$ is known to the learner, the specific subgroup $\\mathcal{G}$ is hidden and must be learned in an online manner. ", "page_idx": 3}, {"type": "text", "text": "3 Impossibility Result of Learning with General Hidden Subgroups ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We now show how to frame the learning problem with hidden symmetry group as the problem of model selection. We further analyse the structure of the collection of models, and show that no algorithm can benefit by solely knowing that $\\mathcal{G}\\leq S_{d}$ , which implies that further assumptions are required to achieve significant improvement in term of regret. ", "page_idx": 3}, {"type": "text", "text": "3.1 Fixed Point Subspace and Partition ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The analysis of learning with hidden subgroup requires a group-theoretic notion which is referred to as fixed-point subspaces [10]. As we shall explain promptly, there is a tight connection between the collection of fixed-point subspaces and set partitions. ", "page_idx": 3}, {"type": "text", "text": "Fixed-point subspaces. For a subset $\\mathcal{X}\\subseteq\\mathbb{R}^{d}$ , denote $\\operatorname{Fix}_{\\mathcal{G}}(\\mathcal{X}):=\\{x\\in\\mathcal{X}\\mid g\\cdot x=x,\\,\\forall g\\in\\mathcal{G}\\}$ as the fixed-point subspace of $\\mathcal{G}$ ; and ${\\mathcal F}_{S_{d}}({\\boldsymbol{\\chi}}):=\\{\\mathrm{Fix}_{\\mathcal{G}}({\\boldsymbol{\\chi}})\\mid\\mathcal{G}\\leq S_{d}\\}$ as the collection of all fixedpoint subspaces of all subgroups of $\\textstyle S_{d}$ . We simply write $\\mathcal{F}_{S_{d}}=\\mathcal{F}_{S_{d}}(\\mathbb{R}^{d})$ and $\\operatorname{Fix}_{\\mathcal{G}}=\\operatorname{Fix}_{\\mathcal{G}}(\\mathbb{R}^{d})$ for brevity. ", "page_idx": 4}, {"type": "text", "text": "Set partition. Given $d\\in\\mathbb{N}^{+}$ , we denote $\\mathcal{P}_{d}$ as the set of all partitions of $[d]$ . Let $\\mathcal{P}_{d,k}$ as the set of all partitions of $[d]$ with exactly $k$ classes, and $\\mathcal{P}_{d,\\leq k}$ be the set of all partitions of $[d]$ with at most $k$ classes. The number of set partitions with $k$ classes $|\\mathcal{P}_{d,k}|$ is known as the Stirling number of the second kind, and $|\\mathcal{P}_{d}|$ is known as Bell number. ", "page_idx": 4}, {"type": "text", "text": "3.2 Impossibility Result ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Problem with known symmetry. Before discussing the problem of hidden symmetry, let us explain why the learner with an exact knowledge of $\\mathcal{G}$ can trivially achieve smaller regret. The reason is that $\\theta_{\\star}\\in\\mathrm{Fix}_{\\mathcal{G}}$ by the assumption that $\\theta_{\\star}$ is invariant w.r.t the action of group $\\mathcal{G}$ . If $\\mathcal{G}$ is known in advance, the learner can restrict the support of $\\theta_{\\star}$ in $\\operatorname{Fix}_{\\mathcal{G}}$ , and immediately obtains that the regret scales with $\\mathrm{dim}(\\mathrm{Fix}_{\\mathcal{G}})$ instead of $d$ , which can be significantly smaller (e.g., if $\\mathcal{G}=S_{d}$ , then $\\dim(\\operatorname{Fix}_{\\mathcal{G}})=1)$ ). ", "page_idx": 4}, {"type": "text", "text": "For any subgroup, there exists a fixed point subspace, and some subgroups may share the same fixed point subspace. Therefore, instead of constructing a collection of subgroups, one can create a smaller collection of models using the collection of fixed point subspaces. As $\\mathcal{G}$ is hidden, one must learn $\\operatorname{Fix}_{\\mathcal{G}}$ within the set of candidates ${\\mathcal{F}}_{S_{d}}$ , leading to the formulation of the model selection. ", "page_idx": 4}, {"type": "text", "text": "From the setting with hidden subgroup to the setting with hidden set partition. Now, we discuss the structure of the collection of models ${\\mathcal{F}}_{S_{d}}$ . First, we show the equivalent structure between the collection of fixed point subspaces and the set partitions as follows. ", "page_idx": 4}, {"type": "text", "text": "Proposition 1. There is a bijection $\\mathbf{H}$ between $\\mathcal{P}_{d}$ and ${\\mathcal{F}}_{S_{d}}$ ", "page_idx": 4}, {"type": "text", "text": "As there is a bijection between $\\mathcal{P}_{d}$ and ${\\mathcal{F}}_{S_{d}}$ , we can count the number of subspaces of each dimension $k$ explicitly using the following. ", "page_idx": 4}, {"type": "text", "text": "Proposition 2 ([10]\u2019s Theorem 14). Given a subgroup $\\Gamma\\le S_{d}$ and its fixed-point subspace Fix\u0393, suppose that $\\pi_{\\Gamma}$ partitions $[d]$ into $k$ classes, then $\\dim(\\operatorname{Fix}_{\\Gamma})=k$ . ", "page_idx": 4}, {"type": "text", "text": "By Proposition 2, we have that the number of subspaces of dimension $k$ in ${\\mathcal{F}}_{S_{d}}$ is exactly the number of set partitions with $k$ -classes. Suppose that the learner knows that the orbit under action of $\\mathcal{G}$ partitions the index of $\\theta_{\\star}$ into 2 equivalent classes that is, $\\mathrm{dim}(\\mathrm{Fix}_{\\mathcal{G}})=2$ . The learner cannot get any reduction in terms of regret. ", "page_idx": 4}, {"type": "text", "text": "Proposition 3. Assume that the action set is the unit cube $\\mathcal{X}=\\{\\boldsymbol{x}\\in\\mathbb{R}^{d}\\mathrm{~}|\\mathrm{~}\\|\\boldsymbol{x}\\|_{\\infty}\\leq1\\}$ , and $f$ is invariant w.r.t. action of subgroup $\\mathcal{G}\\leq S_{d},$ \u221asuch that $\\dim(\\operatorname{Fix}_{\\mathcal{G}})=2$ . Then, the regret of any bandit algorithm is lower bounded by ${\\bf R}_{T}=\\Omega(d\\sqrt{T})$ . ", "page_idx": 4}, {"type": "text", "text": "The implication of Proposition 3 is that even if the learner knows $\\theta_{\\star}$ lies in an extremely lowdimensional subspace within the finite pools of candidates, they still suffer a regret that scales linearly with the ambient dimension $d$ . This suggests that further information about the group $\\mathcal{G}$ must assumed to be known in order to break this polynomial dependence on $d$ in the regret bound. ", "page_idx": 4}, {"type": "text", "text": "4 The Case of Hidden Subgroups with Subexponential Size ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "As indicated by Proposition 3, there is no improvement in terms of regret, despite the learner having access to a collection of extremely low-dimensional fixed point subspaces. Therefore, we assume that the learner can access only a reasonably small subset of the collection of low-dimensional fixed point subspaces. Let $d_{0}$ be the upper bound for the dimension of fixed point subspaces; that is, we know that the orbit of $\\mathcal{G}$ partitions $\\bar{[}d\\!]$ into at most $d_{0}$ classes. Now, let us assume that the learner knows that $\\mathcal{G}$ does not partition $[d]$ freely, but must satisfy certain constraints, that is, $\\pi g\\in\\mathcal{Q}_{d,\\leq d_{0}}\\subset\\mathcal{P}_{d,\\leq d_{0}}$ Here, $\\mathcal{Q}_{d,\\leq d_{0}}$ is a small collection of partitions with at most $d_{0}$ classes, which encodes the constraints on the way $\\mathcal{G}$ partitions $[d]$ . We introduce an assumption regarding the cardinality of $\\mathcal{Q}_{d,\\leq d_{0}}$ , which is formally stated in Section 4.2. Using the Proposition 1, we can define the collection of fixed point subspaces associated with the collection of partition $\\mathcal{Q}_{d,\\leq d_{0}}$ via the bijection $\\mathbf{H}$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{M}:=\\mathbf{H}\\left(\\mathcal{Q}_{d,\\leq d_{0}}\\right)\\quad\\mathrm{and}\\quad\\boldsymbol{M}:=|\\mathcal{M}|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In addition, let us define the extension of the collection $\\mathcal{M}$ as $\\begin{array}{r l r}{\\overline{{\\mathcal{M}}}}&{{}}&{:=}\\end{array}$ $\\{{\\mathrm{conv}}\\left(m\\cup m^{\\prime}\\right)\\mid\\ m,\\ m^{\\prime}\\in{\\mathcal{M}}\\}$ , where $\\mathrm{conv}(S)$ is the convex hull of the set $\\textit{S}\\subset\\textsuperscript{\\mathbb{R}}^{n}$ . ", "page_idx": 4}, {"type": "text", "text": "We have that $\\overline{{\\mathcal{M}}}$ is a collection of subspaces, that is, conv $(m\\cup m^{\\prime})$ is indeed a subspace [8]. Denote $\\overline{{M}}:=|\\overline{{M}}|$ , then we have $\\overline{{M}}=(M^{2}-M)/2$ . Moreover, if dimension of subspace in $\\mathcal{M}$ is at most $d_{0}$ , then the dimension of subspace in $\\overline{{\\mathcal{M}}}$ is at most $2d_{0}$ . ", "page_idx": 5}, {"type": "text", "text": "4.1 The Explore-Models-then-Commit Algorithm ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Given some $n\\in[T]$ , we define ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\boldsymbol{Y}=\\boldsymbol{X}\\boldsymbol{\\theta}_{\\star}+\\boldsymbol{\\eta},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $Y\\,\\in\\,\\mathbb{R}^{n}$ , $X\\,=\\,\\left[x_{1},\\ldots,x_{n}\\right]^{\\top}\\;\\in\\;\\mathbb{R}^{n\\times d}$ is the design matrix, $\\theta_{\\star}\\,\\in\\,\\mathbb{R}^{d}$ is the true model; $\\pmb{\\eta}=[\\eta_{1},...,\\eta_{n}]$ . We have the information that $\\theta_{\\star}$ must be contained in some (not necessarily unique) subspace $m\\in\\mathcal{M}$ . Denote by $d_{m}$ the dimension of $m$ , we have $d_{m}\\leq d_{0}$ for any $m\\in\\mathcal{M}$ . Let $\\bar{X_{m}}\\ =\\ [\\Pi_{m}(x_{t})]_{t\\in[n]}^{\\top}$ , and $S_{m}$ be the column space of $X_{m}$ , one has $\\mathrm{dim}(S_{m})\\,\\leq\\,d_{m}$ . For any $m\\in\\mathcal{M}$ , and given $Y$ , let $\\Pi_{S_{m}}(\\cdot)$ be the projection onto $S_{m}$ . Define ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widehat{f}_{m}:=\\Pi_{S_{m}}(Y);\\quad\\widehat{\\theta}_{m}:=\\underset{\\theta\\in m}{\\operatorname{arg\\,min}}\\left\\|Y-X\\theta\\right\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Now, given $n$ data points, we can choose the model $\\widehat{m}\\in\\mathcal{M}$ that minimises the least square error ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widehat{m}\\in\\arg\\operatorname*{min}_{m\\in\\mathcal{M}}\\|Y-\\widehat{\\pmb{f}}_{m}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Based on the framework of model selection, we now introduce our Algorithm 1, Explore-Models-then-Commit (EMC). Our algorithm falls into the class explore-then-commit bandit algorithms. The exploration phase consists of $t_{1}$ rounds. During this phase, one samples data independently and identically distributed (i.i.d.) from an exploratory distribution $\\nu$ . After the exploration phase, one computes the solution to the model selection problem and then commits to the best arm corresponding to the chosen model. ", "page_idx": 5}, {"type": "text", "text": "Remark 4. The key step of Algorithm 1 that may incur significant costs is solving equation (4) (line 6). Without additional information about $\\mathcal{M}$ , one might need to enumerate all models in $\\mathcal{M}$ and optimize among them, which would induce a time complexity of $O(n d^{c d_{0}})$ . However, if we have more information about the partitions, e.g., if they are non-crossing or non-nesting partitions, their lattice structures can be exploited to speed up the optimization process of solving equation (4). Due to space limitations, we refer readers to Appendix D.3 for a detailed explanation of a subroutine that leverages these lattice structures for more efficient computation. Additionally, Section 6 demonstrates that our Algorithm 1, when using the lattice search algorithm for non-crossing partitions and nonnesting partitions as a subroutine, achieves polynomial computational complexity of $O(n d^{5})$ and guarantees low regret. ", "page_idx": 5}, {"type": "text", "text": "4.2 Regret Analysis ", "text_level": 1, "page_idx": 5}, {"type": "table", "img_path": "aLzA7MSc6Y/tmp/e8ed66bf8fe42fd91ab4edfa28c60ac9ccfb96ffa81afff37c30a1dafdddf90f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "The regret analysis of Algorithm 1 uses results from the Gaussian model selection literature [7, 21] as a basis. As such, we first state the assumptions that are common in the Gaussian model selection literature on the collection of models $\\mathcal{M}$ and the set of arms $\\mathcal{X}$ (Section 4.2.1). We then provide our main analysis in Section 4.2.2, highlighting the key technical novelties of our approach. ", "page_idx": 5}, {"type": "text", "text": "4.2.1 Assumptions ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Recall that due to our lower bound in Proposition 3, further assumptions are required on the collection of fixed-point subspaces to achieve a reduction in terms of regret. As suggested by the model selection literature [25, 35], one can achieve regret in terms of $\\log(M)$ for a collection of $M$ models. Adopting this ", "page_idx": 5}, {"type": "text", "text": "1: Input: $T,~\\nu,~t_{1}$   \n2: for $t=1,\\ldots,t_{1}$ do   \n3: Independently pull arm $x_{t}$ according   \nto $\\nu$ and receive a reward $y_{t}$ .   \n4: end for   \n5: $X\\gets[x_{1},...,x_{t_{1}}]^{\\intercal}$ , $Y\\gets[y_{t}]_{t\\in[t_{1}]}$ .   \n6: Compute $\\widehat{\\vphantom{\\left(m\\right)}}\\widehat{m}$ as (4).   \n7: Compute $\\widehat{\\theta}_{t_{1}}$ as (3) corresponding to $\\widehat{\\vphantom{\\left(m\\right)}}\\widehat{m}$ .   \n8: for $t=t_{1}+1$ to $T$ do   \n9: Take greedy actions:   \n$x_{t}=\\operatorname*{arg\\,min}_{x\\in\\mathcal{X}}\\left\\langle\\widehat{\\theta}_{t_{1}},x\\right\\rangle.$ ", "page_idx": 5}, {"type": "text", "text": "10: end for ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "idea, we make the following assumption regarding the number of potential fixed-point subspaces and the set of arms. ", "page_idx": 5}, {"type": "text", "text": "Assumption 5 (Sub-exponential number of partitions). The partition corresponding to $\\mathcal{G}$ belongs to a small subclass of partitions $\\mathcal{Q}_{d,\\leq d_{0}}\\subset\\mathcal{P}_{d,\\leq d_{0}}$ . In particular, $\\pi_{\\mathcal{G}}\\in\\mathcal{Q}_{d,d_{\\star}}$ , for some $d_{\\star}\\leq d_{0}$ , and for each $k\\in[d_{0}]$ , there exists a constant $c>0$ , such that $|\\mathcal{Q}_{d,k}|\\leq O(d^{c k})$ . ", "page_idx": 6}, {"type": "text", "text": "Assumption 6 (Bounded set of arms). There are positive numbers $K_{x},R_{\\mathrm{max}}$ , such that, for all $x\\in\\mathscr{X}$ and $m\\in\\overline{{\\mathcal{M}}}$ , $\\left\\|\\Pi_{m}(x)\\right\\|_{2}^{2}\\leq K_{x}$ , and $\\mid\\left\\langle\\boldsymbol{x},\\boldsymbol{\\theta}_{\\star}\\right\\rangle\\mid\\leq R_{\\mathrm{max}}$ . ", "page_idx": 6}, {"type": "text", "text": "As a consequence of Assumption 5, the cardinality of the collection of fixed point subspaces is not too large, particularly, $M=O(d^{c d_{0}})$ . First, we note that this class includes interval partitions, a structure equivalent to sparsity as a strict subset, as explained below. ", "page_idx": 6}, {"type": "text", "text": "Remark 7 (Equivalence between sparsity and interval partition). A set partition of $[d]$ is an interval partition or partition of interval if its parts are interval. We denote $\\mathcal{T}_{d}$ as the collection of all interval partition of $d$ . $\\mathcal{T}_{d}$ admits a Boolean lattice of order $2^{d-1}$ , making it equivalent to the sparsity structure in $d-1$ dimensions. Specifically, consider the set of entries of parameters $\\varphi\\in\\mathbb{R}^{d}$ with a linear order, that is, $\\varphi_{1}\\,\\geq\\,\\varphi_{2}\\,\\geq\\,\\cdots\\,\\geq\\,\\varphi_{d}$ . Then define the variable $\\boldsymbol{\\theta}^{\\ast}\\in\\mathbb{R}^{d-1}$ such that $\\theta_{i}=(\\varphi_{i}-\\varphi_{i+1})$ . Each interval partition on the entries of $\\varphi$ will determine a unique sparse pattern of $\\theta$ . Therefore, it is clear that the cardinality of the set of interval partition with $d_{0}$ classes is bounded as $\\left|Z_{d,\\le d_{0}}\\right|=O(d^{d_{0}})$ . Moreover, as a result, symmetric linear bandit is strictly harder than sparse bandit and inherits all the computational complexity challenges of sparse linear bandit, including the NP-hardness of computational complexity. ", "page_idx": 6}, {"type": "text", "text": "Apart from sparsity, class of partitions with subexponential size also naturally appears when there is a hierarchical structure on the set $\\bar{[d]}$ , and the partitioning needs to respect this hierarchical structure. A partition that respects an ordered tree groups the children of the same node into a single equivalence class, for example, see Figure 1. It is shown in [15] and the cardinality of the set of partitions that respect ordered trees is sub-exponential. Furthermore, as shown in [15] there is a bijection between partitions that respect ordered trees and the set of noncrossing partitions. ", "page_idx": 6}, {"type": "image", "img_path": "aLzA7MSc6Y/tmp/c7e631f5f89e9b047427cbd74e033c7961bfc174f737b72c9d92d4fbc5366aa2.jpg", "img_caption": ["Figure 1: Partition that respects the underlying ordered tree. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "A real-life example that meets these assumptions is the subcontractor example in the introduction: A hierarchical structure may exist, where a hired subcontractor can further subcontract parts of the work to others. A tree represents the hierarchical order among subcontractors, where subcontractors hired by another contractor can be grouped into one class. Further real-life examples of non-crossing partitions and other structured partitions that satisfy sub-exponential cardinality, such as non-nesting partitions and pattern-avoidance partitions [33], can be found in Appendix D.2. ", "page_idx": 6}, {"type": "text", "text": "Next, similar as [23], we define the exploratory distribution as follows. ", "page_idx": 6}, {"type": "text", "text": "Definition 8 (Exploratory distribution). The exploratory distribution $\\nu\\in\\Delta(\\mathcal{X})$ is solution of the following optimisation problem ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\nu=\\underset{\\omega\\in\\Delta(\\mathcal{X})}{\\arg\\operatorname*{max}}\\,\\lambda_{\\operatorname*{min}}\\left(\\mathbb{E}_{x\\sim\\omega}[x x^{\\top}]\\right),\\quad V:=\\mathbb{E}_{x\\sim\\nu}[x x^{\\top}],\\quad C_{\\operatorname*{min}}(\\mathcal{X}):=\\lambda_{\\operatorname*{min}}(V).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Since our setting includes sparsity as a special case, the regret lower bound in [24] applies to our setting as well. In particular, we have: ", "page_idx": 6}, {"type": "text", "text": "Proposition 9 (Regret lower bound). There exist symmetric linear bandit instances in which Assumption 5, 6 hold with $K_{x}\\,=\\,8d_{0}$ , such that, any bandit algorithm must suffer regret ${\\bf R}_{T}=$ $\\Omega\\left(\\operatorname*{min}{\\left(C_{\\operatorname*{min}}(\\mathcal{X})^{-\\frac{1}{3}}d_{0}^{\\frac{2}{3}}T^{\\frac{2}{3}},\\sqrt{d T}\\right)}\\right)$ . ", "page_idx": 6}, {"type": "text", "text": "We note that the lower bound can be relaxed if we have a stronger assumption on the group $\\mathcal{G}$ , which allows algorithm to go beyond the lower bound in Proposition 9 of the sparsity class. For example, if we consider a collection of fixed-point subsp\u221aaces with a nested structure, similar to those discussed in [20], the algorithm may achieve a $O(d_{0}{\\sqrt{T}})$ rate. The key takeaway is that symmetry exhibits significant flexibility in structure. Depending on the specific class of symmetry, one may achieve either no reduction at all or significant reduction in terms of regret bound. ", "page_idx": 6}, {"type": "text", "text": "4.2.2 Main Regret Upper Bound Result ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We now state the main result of the regret upper bound for Algorithm 1. ", "page_idx": 7}, {"type": "text", "text": "Theorem 10 (Regret upper bound). Suppose the Assumptions 5, 6 hold. With the choice of $t_{1}=R_{\\operatorname*{max}}^{-\\frac{2}{3}}\\sigma^{\\frac{2}{3}}C_{\\operatorname*{min}}^{-\\frac{1}{3}}(\\chi)\\bar{K}_{x}^{\\frac{7}{3}}d_{0}^{\\frac{1}{3}}T^{\\frac{2}{3}}(\\log(d T))^{\\frac{1}{3}}$ , then the regret of Algorithm $^{\\,l}$ is upper bounded as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbf{R}_{T}=O\\left(R_{\\operatorname*{max}}^{\\frac{1}{3}}\\sigma^{\\frac{2}{3}}C_{\\operatorname*{min}}^{-\\frac{1}{3}}(\\chi)K_{x}^{\\frac{1}{3}}d_{0}^{\\frac{1}{3}}T^{\\frac{2}{3}}(\\log(d T))^{\\frac{1}{3}}\\right)\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Remark 11. We note that when $K_{x}=O(d_{0})$ , as in the lower bound instances, our upper bound is $\\tilde{O}\\left(C_{\\mathrm{min}}^{-\\frac{1}{3}}(X)d_{0}^{\\frac{2}{3}}T^{\\frac{2}{3}}\\right)$ , which matches the lower bound in Proposition 9. ", "page_idx": 7}, {"type": "text", "text": "The main idea is to bound the risk error after exploration rounds, as stated in the following lemma which implies the regret bound after standard manipulations. ", "page_idx": 7}, {"type": "text", "text": "Lemma 12. Suppose the Assumptions $^{5}$ , 6 hold. For $t_{1}\\,=\\,\\Omega(K_{x}^{2}d_{0}C_{\\mathrm{min}}^{-2}(\\mathcal{X})\\log(d/\\delta))$ , with probability at least $1-\\delta$ , one has the estimate ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\left\\|\\theta_{\\star}-\\widehat{\\theta}_{t_{1}}\\right\\|_{2}=O\\left(\\sqrt{\\frac{\\sigma^{2}d_{0}\\log(d/\\delta)}{C_{\\operatorname*{min}}(\\chi)t_{1}}}\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Remark 13 (Non-triviality of Lemma 12). At the first glance, it seems that we can cast the problem of learning with a collection of $M$ subspaces into a model selection problem in linear bandit with $M$ features. This leads to a question: Can we apply the model selection framework based on model aggregation in [35, 25] to our case? ", "page_idx": 7}, {"type": "text", "text": "First, let us explain how to cast our problem into a model selection problem in linear bandit. For each subspace $m$ , let $\\Phi_{m}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d_{0}}$ be the feature map that computes the image of the projection $\\Pi_{m}$ with respect to the orthogonal basis of subspace $m$ . Thus, we then have a collection of $M$ features $\\{\\Phi_{m}\\}_{m\\in\\mathcal{M}}$ . Consider the algorithm introduced in [25], which concatenates the feature maps into $\\dot{\\Phi}(x)=[\\Phi_{1}(x),\\ldots,\\Phi_{M}(x)]\\in\\mathbb{R}^{M d_{0}}$ , and the regret bound depends on $\\|\\Phi(x)\\|_{2}$ . ", "page_idx": 7}, {"type": "text", "text": "Ho\u221awever, in our case where $\\|\\Phi_{m}(x)\\|_{2}<1$ , we can only bound $\\|\\Phi(x)\\|_{2}\\leq{\\sqrt{M}}$ , which leads to a $\\sqrt{M}$ dependence on regret, if we use their algorithm. Regarding [35], their algorithm aggregates the predictions among models for each arm, and based on that, they compute the distribution for choosing each arm. This leads to the regret scaling with the number of arms $K$ , which is not feasible in our case when $K=\\infty$ . ", "page_idx": 7}, {"type": "text", "text": "We note that the similarity with the model selection technique in [25, 35] is that they use model aggregation among $\\mathcal{M}$ to bound the prediction error $\\begin{array}{r}{\\sum_{t=1}^{T}\\left\\langle x_{t},\\widehat{\\theta}-\\theta_{\\star}\\right\\rangle^{2}}\\end{array}$ , but this does not necessarily guarantee the risk error $\\lVert\\widehat{\\theta}-\\theta_{\\star}\\rVert_{2}$ . The reason is that, although model aggregation can guarantee a small prediction error, it imposes no restriction on the estimator\u03b8 , which limits its ability to leverage the further benign property of designed matrix $X$ . Instead of model aggregation, our algorithm explicitly picks the best model from the pool $\\mathcal{M}$ , ensuring the following two properties: (1) The prediction error is small, similar to model aggregation; and (2) We can guarantee that\u03b8 lies in one of the subspaces of $\\mathcal{M}$ . The second property gives us control over $(\\widehat{\\theta}-\\theta_{\\star})$ by ensuring it lies in at most $M^{2}$ subspaces. Then, exploiting the restricted isometry property (see Definition 15) of designed matrix $X$ , we can guarantee that with ${\\cal O}(\\log(M))$ exploratory samples, we can bound the risk error $\\lVert\\widehat{\\theta}-\\theta_{\\star}\\rVert_{2}$ . This is crucial for eliminating polynomial dependence on $M$ and the number of arms $K$ . ", "page_idx": 7}, {"type": "text", "text": "Proof sketch of Lemma 12. We provide a proof sketch here and defer their full proof to Appendix B.1. Our proof borrows techniques from Gaussian model selection [21] and the compressed sensing literature [8]. There are two steps to bound the risk error as in Lemma 12: ", "page_idx": 7}, {"type": "text", "text": "Step 1 - Bounding the prediction error. We can bound the prediction error $\\left\\|X\\theta_{\\star}-X{\\widehat{\\theta}}_{t_{1}}\\right\\|_{2}$ using the Gaussian model selection technique [21] as follows. ", "page_idx": 7}, {"type": "text", "text": "Proposition 14. Let $\\pmb{f}_{\\star}=X\\theta_{\\star}$ . For the choice of $\\widehat{\\pmb f}_{\\widehat{m}}$ as in Eqn. (3) & Eqn. (4), with probability at least $1-\\delta_{i}$ , there exists a constant $C>1$ such that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\lVert\\widehat{f}_{\\widehat{m}}-f_{\\star}\\right\\rVert_{2}^{2}\\leq C\\sigma^{2}\\log\\left(M\\delta^{-1}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Step 2 - Bounding the risk error from prediction error. To bound the risk error from the prediction error, we invoke the restricted isometry property on the union of subspaces of a subGaussian random matrix as in [8]. Note that $\\widehat{\\theta}$ and $\\theta_{\\star}$ can belong to two different subspaces of $\\mathcal{M}$ , and $\\widehat{\\theta}-\\theta_{\\star}$ may not lie in any subspace of $\\mathcal{M}$ , but in $\\overline{{\\mathcal{M}}}$ . An important property of the design matrix $X$ ,  which allows one to recover $\\theta_{\\star}$ with the knowledge that $\\theta_{\\star}$ is in a subspace $m\\in\\mathcal{M}$ , can be captured by the following notion of restricted isometry property (RIP): ", "page_idx": 8}, {"type": "text", "text": "Definition 15 (Restricted isometry property). For any matrix $X$ , any collection of subspaces $\\overline{{\\mathcal{M}}}$ and any $\\theta\\in m\\in\\overline{{\\mathcal{M}}}$ , we define $\\overline{{\\mathcal{M}}}$ -restricted isometry constant $\\delta_{\\overline{{{\\cal M}}}}(X)$ to be the smallest quantity such that ", "page_idx": 8}, {"type": "equation", "text": "$$\n(1-\\delta_{\\overline{{\\mathcal{M}}}}(X))\\|\\theta\\|_{2}^{2}\\leq\\|X\\theta\\|_{2}^{2}\\leq(1+\\delta_{\\overline{{\\mathcal{M}}}}(X))\\|\\theta\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "We have the minimum number of samples required so that a random matrix $X$ satisfies RIP for a given constant with high probability as Proposition 16 below. Then, Lemma 12 is followed by combining Proposition 16 and Proposition 14. ", "page_idx": 8}, {"type": "text", "text": "Proposition 16. Let $X~=~[x_{t}]_{t\\in[n]}$ , where $x_{t}$ \u2019s are is i.i.d. drawn from $\\nu$ , and let $n\\ =$ $\\Omega\\left(C_{\\mathrm{min}}^{-2}(\\mathcal{X})K_{x}^{2}\\left(\\log(2\\overline{{M}}d_{0}\\delta^{-1})\\right)\\right)$ . Then, with probability at least $1-\\delta$ , and for any $\\theta_{1},\\theta_{2}$ in subspaces of $\\mathcal{M}$ , one has that ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\theta_{1}-\\theta_{2}\\right\\|_{2}^{2}\\leq2C_{\\operatorname*{min}}^{-1}(\\chi)n^{-1}\\left\\|X(\\theta_{1}-\\theta_{2})\\right\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "5 Improved Regret Upper Bound with Well-Separated Partitions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We now show that by adding more struct\u221aure (i.e., well-separatedness) to the setting, we can further improve the regret upper bound to $O({\\sqrt{T}})$ . In particular, we will introduce the noti\u221aon of wellseparatedness in this section, and show that this notion can lead to improved (i.e., $O(\\sqrt{T}))$ regret bounds. ", "page_idx": 8}, {"type": "text", "text": "For each partition $p\\in\\mathcal{P}_{d}$ , there is a unique equivalence relation on $[d]$ corresponding to $p$ . Denote by $\\stackrel{p}{\\sim}$ the equivalence relation corresponding to $p$ . Next, we define well-separatedness. ", "page_idx": 8}, {"type": "text", "text": "Assumption 17 (Well-separated partitioning). Given the true subgroup $\\mathcal{G}$ , and the corresponding partition $\\pi_{\\mathcal{G}}$ . For all $(i,j)$ such that $i\\stackrel{\\pi_{\\mathcal{G}}}{\\nsim}j$ , it holds that $|\\theta_{\\star,i}-\\theta_{\\star,j}|\\geq\\varepsilon_{0}$ , for some $\\varepsilon_{0}>0$ . ", "page_idx": 8}, {"type": "text", "text": "Algorithm 2 Exploring Model then Commit with well-separated partition   \n1: Input: $T,~\\nu,~t_{2}$   \n2: for $t=1,\\cdot\\cdot\\cdot\\,,t_{2}$ do   \n3: Independently pull arm $x_{t}$ according to $\\nu$ and receive a reward $y_{t}$ .   \n4: end for   \n5: $X\\gets[x_{1},...,x_{t_{1}}]^{\\intercal}$ , $Y\\gets[y_{t}]_{t\\in[t_{1}]}$ . 6: Compute $\\widehat{\\vphantom{\\left(m\\right)}}\\widehat{m}$ as (4).   \n7: for $t=t_{2}+1$ to $T$ do   \n8: Playing OFUL algorithm [1] on $\\widehat{\\vphantom{\\left(m\\right)}}\\widehat{m}$ . 9: end for ", "page_idx": 8}, {"type": "text", "text": "The implication of Assumption 17 is that the projection of $\\theta_{\\star}$ to any subspace $m\\in\\mathcal{M}$ not containing $\\theta_{\\star}$ will cause some bias in the estimation error. In particular, one can show that for any $m\\in\\mathcal{M}$ such that $\\theta_{\\star}\\not\\in m$ , it holds that ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\theta_{\\star}-\\Pi_{m}(\\theta_{\\star})\\|_{2}^{2}\\geq\\varepsilon_{0}^{2}/2.}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "We now show that under the Assumption 17, after the exploring phase, the algorithm returns a true fixed-point subspace $\\widehat{m}\\ni\\theta_{\\star}$ with high probability. ", "page_idx": 8}, {"type": "text", "text": "Theorem 18. Suppose the Assumptions 5, 6, 17 hold. Let $\\begin{array}{r}{t_{2}=\\Omega\\left(\\frac{\\sigma^{2}K_{x}^{2}d_{0}\\log\\left(d T\\right)}{C_{\\operatorname*{min}}^{2}\\left(\\mathcal{X}\\right)\\varepsilon_{0}^{2}}\\right)}\\end{array}$ . Then, Algorithm 2 returns $\\widehat{m}\\ni\\theta_{\\star}$ with probability at least $1-1/T$ , and its regret is upper bounded as ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbf{R}_{T}=O\\left(\\frac{R_{\\mathrm{max}}\\sigma^{2}K_{x}^{2}d_{0}\\log(d T)}{C_{\\mathrm{min}}^{2}(\\mathcal{X})\\varepsilon_{0}^{2}}+\\sigma d_{0}\\sqrt{T\\log(K_{x}T)}\\right).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "That i\u221as, if the separating constant $\\varepsilon_{0}$ is known in advance and $\\varepsilon_{0}\\geq T^{-1/4}$ , then we can achieve $O(d_{0}{\\sqrt{T}}\\log(K_{x}T))$ regret upper bound. ", "page_idx": 8}, {"type": "text", "text": "Remark 19. A weakness of Algorithm 2 is that without knowing that $\\varepsilon_{0}\\geq T^{-1/4}$ is true a priori, there may be possible mis-specification error, which leads to linear regret if one applies the algorithm naively. On the other hand, Algorithm 1 can always achieve regret $O(T^{2/3})$ in the worst case. As such, the following qu\u221aestion arises: Does there exist an algorithm that, without the knowledge of $\\varepsilon_{0}$ , can achieve regret $\\bar{O(\\sqrt{T})}$ whenever $\\varepsilon_{0}\\geq T^{-1/4}$ , but guarantees the worst-cas\u221ae regret of $O(T^{2/3})$ ? Toward answering this question, we propose a simple method which has $O(\\sqrt{T})$ regret whenever the separating constant is large, and enjoys a worst-case regret guarantee of $O(T^{3/4})$ (slightly worse than $O(T^{2/3}))$ . We refer the reader to Appendix C.2 for a detailed description of the algorithm, its regret bound and further discussion. ", "page_idx": 9}, {"type": "text", "text": "6 Experiment ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "To illustrate the performance of our algorithm, we conduct simulations where the entries of $\\theta_{\\star}$ satisfy three cases: sparsity, non-crossing partitions and non-nesting partitions. We refer readers to Appendix D.1 for a more formal description of non-crossing partitions, non-nesting partitions, and why the interval partition (i.e., the partition structure equivalent to sparsity) is a strict subset of both non-crossing and non-nesting partitions. Since sparsity is equivalent to a strict subset of non-crossing and non-nesting partitions, we compare our Algorithm 1 with the sparse-ba\u221andit ESTC algorithm proposed in [23] as a benchmark in all environments. The set of arms $\\mathcal{X}$ is $\\sqrt{d}\\mathbb{S}^{d-1}$ , $\\sigma=0.1$ , and $d=100$ , $d_{0}=15$ . The ground-truth sparse patterns, partitions and $\\theta_{\\star}$ are randomized before each simulation. ", "page_idx": 9}, {"type": "text", "text": "The regret of both algorithms is shown in Figure 2, which indicates that our algorithm performs competitively in the sparsity case and significantly outperforms the sparse-bandit algorithm in cases of non-crossing and non-nesting partitions. Due to space limitations, we refer the reader to Appendix E for a detailed description of the experiments, including how we applied the sparse-bandit algorithm in the cases of non-crossing and non-nesting partitions, and how we ran Algorithm 1 in the case of sparsity. Additionally, we explain how we exploited the particular structure of non-crossing and non-nesting partitions to enable efficient computation in Appendix E. ", "page_idx": 9}, {"type": "image", "img_path": "aLzA7MSc6Y/tmp/2711291bbbebd536d814529e378a601699d9b071697428afefd480b2fdc508e9.jpg", "img_caption": ["Figure 2: Regret of EMC (Algorithm 1) and of ESTC proposed in [23], in cases of sparsity, noncrossing partitions, and non-nesting partitions. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "7 Conclusion and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we study symmetric linear stochastic bandits in high dimensions, where the linear reward function is invariant with respect to some hidden subgroup $\\mathcal{G}\\leq S_{d}$ . We first prove that no algorithm can gain any advantage solely by knowing $g\\leq S_{d}$ . Given this, we introduce a cardinality condition on the hidden subgroup $\\mathcal{G}$ , allowing the learner to overcome the curse of dimensionality. Under this condition, we propose novel model selection algorithms that achieve regrets of $\\tilde{O}(\\bar{d}_{0}^{2/3}T^{2/3})$ and $\\tilde{O}(d_{0}\\sqrt{T})$ with an additional assumption on the well-separated partition. For future work, we will explore convex relaxation techniques for efficient computation, leveraging specific structures of symmetries. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Yasin Abbasi-yadkori, D\u00e1vid P\u00e1l, and Csaba Szepesv\u00e1ri. Improved algorithms for linear stochastic bandits. In Advances in Neural Information Processing Systems, 2011. [2] Yasin Abbasi-Yadkori, D\u00e1vid P\u00e1l, and Csaba Szepesv\u00e1ri. Online-to-confidence-set conversions and application to sparse stochastic bandits. In Journal of Machine Learning Research, 2012. [3] Miguel Abreu, Luis Paulo Reis, and Nuno Lau. Addressing imperfect symmetry: a novel symmetry-learning actor-critic extension. arXiv preprint arXiv:2309.02711, 2023. [4] Alekh Agarwal, Haipeng Luo, Behnam Neyshabur, and Robert E. Schapire. Corralling a band of bandit algorithms. In Proceedings of the 2017 Conference on Learning Theory, 2016. [5] Richard Baraniuk, Mark Davenport, Ronald DeVore, and Michael Wakin. A simple proof of the restricted isometry property for random matrices. Constructive Approximation, 2008. [6] Arash Behboodi, Gabriele Cesa, and Taco Cohen. A pac-bayesian generalization bound for equivariant networks. In Advances in Neural Information Processing Systems, 2022. [7] Lucien Birg\u00e9 and Pascal Massart. Minimal penalties for gaussian model selection. Probability Theory and Related Fields, 2007. [8] Thomas Blumensath and Mike E. Davies. Sampling theorems for signals from the union of finite-dimensional linear subspaces. IEEE Transactions on Information Theory, 55, 2009. [9] Michael M. Bronstein, Joan Bruna, Taco Cohen, and Petar Velic\u02c7kovic\u00b4. Geometric deep learning: Grids, groups, graphs, geodesics, and gauges, 2021. [10] R. B\u00f6di and K. Herr. Symmetries in linear and integer programs. arXiv preprint arXiv:0908.3329, 2009. [11] Alexandra Carpentier and Remi Munos. Bandit theory meets compressed sensing for highdimensional stochastic linear bandit. In Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics, 2012. [12] Gabriele Cesa, Leon Lang, and Maurice Weiler. A program to build e(n)-equivariant steerable cnns. In International Conference on Learning Representations, 2022. [13] Minshuo Chen, Yan Li, Ethan Wang, Zhuoran Yang, Zhaoran Wang, and Tuo Zhao. Pessimism meets invariance: Provably efficient offline mean-field multi-agent rl. In Advances in Neural Information Processing Systems, 2021. [14] William Chen, Eva Deng, Rosena Du, Richard Stanley, and Catherine Yan. Crossings and nestings of matchings and partitions. Transactions of the American Mathematical Society, 2006. [15] Nachum Dershowitz and Shmuel Zaks. Ordered trees and non-crossing partitions. Discrete Mathematics, 1986. [16] Bryn Elesedy and Sheheryar Zaidi. Provably strict generalisation benefti for equivariant models. In International Conference on Machine Learning, 2021. [17] Marc Finzi, Samuel Stanton, Pavel Izmailov, and Andrew Gordon Wilson. Generalizing convolutional neural networks for equivariance to lie groups on arbitrary continuous data. In International Conference on Machine Learning, 2020. [18] Dylan J. Foster, Akshay Krishnamurthy, and Haipeng Luo. Model selection for contextual bandits. In Advances in Neural Information Processing Systems, 2019. [19] Dylan J Foster, Claudio Gentile, Mehryar Mohri, and Julian Zimmert. Adapting to misspecification in contextual bandits. In Advances in Neural Information Processing Systems, 2020. [20] Avishek Ghosh, Abishek Sankararaman, and Kannan Ramchandran. Problem-complexity adaptive model selection for stochastic linear bandits. arXiv preprint arXiv:2006.02612, 2020. [21] Christophe Giraud. Introduction to High-Dimensional Statistics. 2021. ", "page_idx": 10}, {"type": "text", "text": "[22] Robert Givan, Thomas Dean, and Matthew Greig. Equivalence notions and model minimization in markov decision processes. Artificial Intelligence, 7 2003.   \n[23] Botao Hao, Tor Lattimore, and Mengdi Wang. High-dimensional sparse linear bandits. In Advances in Neural Information Processing Systems, 2020.   \n[24] Kyoungseok Jang, Chicheng Zhang, and Kwang-Sung Jun. Popart: Efficient sparse regression and experimental design for optimal sparse linear bandits. In Advances in Neural Information Processing Systems, 2022.   \n[25] Parnian Kassraie, Aldo Pacchiano, Nicolas Emmenegger, and Andreas Krause. Anytime model selection in linear bandits. In Advances in Neural Information Processing Systems, 2023.   \n[26] Gi-Soo Kim and Myunghee Cho Paik. Doubly-robust lasso bandit. In Advances in Neural Information Processing Systems, 2019.   \n[27] Tor Lattimore and Csaba Szepesv\u00e1ri. Bandit Algorithms. Cambridge University Press, 7 2020.   \n[28] Tor Lattimore, Koby Crammer, and Csaba Szepesv\u00e1ri. Linear multi-resource allocation with semi-bandit feedback. In Advances in Neural Information Processing Systems, 2015.   \n[29] Chung-Wei Lee, Qinghua Liu, Yasin Abbasi-Yadkori, Chi Jin, Tor Lattimore, and Csaba Szepesv\u00e1ri. Context-lumpable stochastic bandits. 6 2023.   \n[30] Chi Kwong Li and Roy Mathias. The lidskii-mirsky-wielandt theorem-additive and multiplicative versions. Numerische Mathematik, 1999.   \n[31] Zhiyuan Li, Ruosong Wang, Dingli Yu, Simon S. Du, Wei Hu, Ruslan Salakhutdinov, and Sanjeev Arora. Enhanced convolutional neural tangent kernels. 11 2019.   \n[32] Anuj Mahajan and Theja Tulabandhula. Symmetry learning for function approximation in reinforcement learning. arXiv preprint arXiv:1706.02999, 2017.   \n[33] Toufik Mansour. Combinatorics of Set Partitions. 2012.   \n[34] Washim Uddin Mondal, Mridul Agarwal, Vaneet Aggarwal, and Satish V. Ukkusuri. On the approximation of cooperative heterogeneous multi-agent reinforcement learning (marl) using mean field control (mfc). Journal of Machine Learning Research, 2022.   \n[35] Ahmadreza Moradipari, Berkay Turan, Yasin Abbasi-Yadkori, Mahnoosh Alizadeh, and Mohammad Ghavamzadeh. Feature and parameter selection in stochastic linear bandits. In International Conference on Machine Learning, 2022.   \n[36] Min-hwan Oh, Garud Iyengar, and Assaf Zeevi. Sparsity-agnostic lasso bandit. In International Conference on Machine Learning, 2021.   \n[37] Aldo Pacchiano, Christoph Dann, Claudio Gentile, and Peter Bartlett. Regret bound balancing and elimination for model selection in bandits and rl. arXiv preprint arXiv:2012.13045, 2020.   \n[38] Aldo Pacchiano, Christoph Dann, and Claudio Gentile. Data-driven online model selection with regret guarantees. In International Conference on Artificial Intelligence and Statistics, 2024.   \n[39] Fabien Pesquerel, Hassan Saber, and Odalric Ambrym Maillard. Stochastic bandits with groups of similar arms. In Advances in Neural Information Processing Systems, 2021.   \n[40] B Ravindran and A G Barto. Approximate homomorphisms: A framework for non-exact minimization in markov decision processes. Proceedings of the Fifth International Conference on Knowledge Based Computer Systems (KBCS 04), 2004.   \n[41] Nam Phuong Tran and Long Tran-Thanh. Invariant lipschitz bandits: A side observation approach. In Machine Learning and Knowledge Discovery in Databases: Research Track, 12 2022.   \n[42] Elise van der Pol, Thomas Kipf, Frans A. Oliehoek, and Max Welling. Plannable approximations to mdp homomorphisms: Equivariance under actions. In International Joint Conference on Autonomous Agents and Multiagent Systems, 2020. ", "page_idx": 11}, {"type": "text", "text": "[43] Elise van der Pol, Daniel E. Worrall, Herke van Hoof, Frans A. Oliehoek, and Max Welling. Mdp homomorphic networks: Group symmetries in reinforcement learning. In Advances in Neural Information Processing Systems, 2020. ", "page_idx": 12}, {"type": "text", "text": "Contents of Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Impossibility Result of Learning with General Hidden Subgroups 14 ", "page_idx": 13}, {"type": "text", "text": "B The Case of Hidden Subgroups with Subexponential Size 15 ", "page_idx": 13}, {"type": "text", "text": "B.1 Lower Bound for the Case of Hidden Subgroups with Subexponential Size . 15   \nB.2 High Probability Prediction Error of Model Selection . 17   \nB.3 Regret Upper bound . 20 ", "page_idx": 13}, {"type": "text", "text": "C Improved Regret Bound 21 ", "page_idx": 13}, {"type": "text", "text": "C.1 Improve Regret Bound with Well-Separated Partitions . 21   \nC.2 Adapting to Separating Constant $\\varepsilon_{0}$ . 23 ", "page_idx": 13}, {"type": "text", "text": "D On Collections of Partitions with Subexponential-Size 24 ", "page_idx": 13}, {"type": "text", "text": "D.1 Important Classes of Partitions with Subexponential-Size 24   \nD.2 Practical Examples of Partitions with Subexponential-Size 25   \nD.3 Efficient Greedy Algorithm for Specific Classes of Partitions 25 ", "page_idx": 13}, {"type": "text", "text": "E Experiment details 26 ", "page_idx": 13}, {"type": "text", "text": "F Extended Related Work ", "page_idx": 13}, {"type": "text", "text": "Additional notations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "For any subset $S\\subseteq\\mathbb{R}^{d}$ , and matrix $W\\in\\mathbb{R}^{d\\times d}$ , we write $W(S):=\\{W x\\mid x\\in S\\}$ . ", "page_idx": 13}, {"type": "text", "text": "A Impossibility Result of Learning with General Hidden Subgroups ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "To prove Proposition 1, we need to prove Proposition 20 and use Proposition 21. ", "page_idx": 13}, {"type": "text", "text": "Proposition 20. For each $\\Gamma\\le S_{d}$ acting naturally on $[d]$ , the orbit of $\\Gamma$ on $[d]$ forms a unique partition of $[d]$ . Moreover, for each partition $\\rho\\in\\mathcal P_{d}$ , there exists at least one $\\Gamma\\leq S_{d}$ such that its orbit on $[d]$ under natural action is exactly $\\rho$ . ", "page_idx": 13}, {"type": "text", "text": "Proof. The first claim is obvious by the property of orbit, that is, orbit consists of non-empty and disjoint subsets of $[d]$ , whose union is $[d]$ . ", "page_idx": 13}, {"type": "text", "text": "We prove the second claim. Let $\\rho\\,=\\,\\{\\rho_{i}\\}_{i\\in I}$ , where $I$ is the index of partition; note that $\\rho_{i}$ is nonempty and mutually disjoint. Define a group as follows ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\Gamma_{i}=\\{f:\\rho_{i}\\to\\rho_{i}\\mid f{\\mathrm{~is~bijective}}\\}\\,;\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "It is clear that $\\Gamma_{i}$ is a group under function composition. Now, define the product group ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\Gamma:=\\prod_{i\\in I}\\Gamma_{i},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and the action $\\psi:\\,\\Gamma\\times[d]\\to[d]$ such that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\psi\\left((f_{i})_{i\\in I},x\\right):=f_{j}(x),\\quad\\mathrm{for}\\;\\rho_{j}\\ni x.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Therefore, it is clear that $(f_{i})_{i\\in I}$ is a bijection from $[d]$ onto itself, hence, $\\Gamma$ is a subgroup of $\\textstyle S_{d}$ . ", "page_idx": 13}, {"type": "text", "text": "Let $E=(e_{i})_{i\\in[d]}$ be the standard basis. Given that group $\\mathcal{G}$ partition $[d]$ into $k$ disjoint orbits, $\\mathcal{G}$ also partition $E$ into $k$ disjoint orbits corresponding to its action $\\phi$ on $\\mathbb{R}^{d}$ , that is, ", "page_idx": 14}, {"type": "equation", "text": "$$\nE=\\bigcup_{i=1}^{k}E_{i}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Let $V_{i}=\\operatorname{Span}(E_{i})$ , then, one has the following. ", "page_idx": 14}, {"type": "text", "text": "Proposition 21 ([10]\u2019s Theorem 14). ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname{Fix}_{\\mathcal{G}}(\\mathbb{R}^{d})=\\bigoplus_{i=1}^{k}\\operatorname{Fix}_{\\mathcal{G}}(V_{i}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We now state the proof of Proposition 1 as a corollary of Proposition 20 and Proposition 21. ", "page_idx": 14}, {"type": "text", "text": "Proposition 1. There is a bijection $\\mathbf{H}$ between $\\mathcal{P}_{d}$ and ${\\mathcal{F}}_{S_{d}}$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. First, we show that for each set partition of $[d]$ , there exists a unique $H\\in\\mathcal{F}_{S_{d}}(\\mathbb{R}^{d})$ . This is straightforward due to the fact that, given a set partition $P$ , the partition of basis $(E_{i})_{i\\in I}$ is unique by definition, so as $V_{i}=\\operatorname{span}(E_{i})$ . As a result, let $\\textstyle H=\\bigoplus_{i=1}^{k}{\\mathrm{Fix}}_{\\Gamma}(V_{i})$ , by Proposition 21, $H\\in\\mathcal{F}_{S_{d}}(\\mathbb{R}^{d})$ . As $(V_{i})_{i\\in I}$ is unique, $H$ is unique. ", "page_idx": 14}, {"type": "text", "text": "Second, we show that for each $H\\in\\mathcal{F}_{S_{d}}(\\mathbb{R}^{d})$ , there is a unique set partition/equivalent relation $P$ . DenoteP\u223cas an equivalent relation under the set partition $P$ . Suppose there are two different set partitions $P,\\ Q$ , there must be two set element $p\\textit{q}$ such that $p\\overset{\\mathrm{~P~}}{\\sim}q$ under $P$ , and $\\textit{p}\\stackrel{\\mathrm{Q}}{\\nsimiters}q$ under $Q$ . Denote $H_{P},\\ H_{Q}$ as the subspaces defined by $P$ and $Q$ respectively. As $\\boldsymbol{p}\\stackrel{\\mathrm{Q}}{\\nsim}\\boldsymbol{q}$ , there must exist a point $x\\in H_{Q}$ such that $x_{p}\\neq x_{q}$ . However, $x_{p}=x_{q}$ for all $x\\in H_{P}$ . Hence, $H_{P}$ cannot be the same as $H_{Q}$ . \u53e3 ", "page_idx": 14}, {"type": "text", "text": "Proposition 3. Assume that the action set is the unit cube $\\mathcal{X}=\\{\\boldsymbol{x}\\in\\mathbb{R}^{d}\\mathrm{~}|\\mathrm{~}\\|\\boldsymbol{x}\\|_{\\infty}\\leq1\\}$ , and $f$ is invariant w.r.t. action of subgroup $\\mathcal{G}\\leq S_{d},$ , \u221asuch that $\\dim(\\operatorname{Fix}_{\\mathcal{G}})=2$ . Then, the regret of any bandit algorithm is lower bounded by ${\\bf R}_{T}=\\Omega(d\\sqrt{T})$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. Suppose there is a collection of model parameter $\\Theta=\\{-\\varepsilon,\\,\\varepsilon\\}^{d}$ , for some $\\varepsilon>0$ . For each $\\theta\\in\\Theta$ , it is straightforward that since $\\theta$ has two classes of indices, by Proposition 1 and 2, there must be a subgroup $\\mathcal{G}\\leq S_{d}$ such that $\\theta\\in{\\mathrm{Fix}}g$ and $\\dim\\left(\\operatorname{Fix}_{\\mathcal{G}}\\right)=2$ . ", "page_idx": 14}, {"type": "text", "text": "Now, $\\Theta$ can be used as a family of problem instances for minimax lower bound of linear bandit. By Theorem 24.1 in [27], we have that with the choice of $\\varepsilon=T^{-1/2}$ , one has that ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\bf R}_{T}\\geq\\frac{\\exp-2}{\\sqrt{8}}d\\sqrt{T}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "B The Case of Hidden Subgroups with Subexponential Size ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Lower Bound for the Case of Hidden Subgroups with Subexponential Size ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proposition 9 (Regret lower bound). There exist symmetric linear bandit instances in which Assumption 5, 6 hold with $K_{x}\\,=\\,8d_{0}$ , such that, any bandit algorithm must suffer regret ${\\bf R}_{T}=$ $\\Omega\\left(\\operatorname*{min}{\\left(C_{\\operatorname*{min}}(\\mathcal{X})^{-\\frac{1}{3}}d_{0}^{\\frac{2}{3}}T^{\\frac{2}{3}},\\sqrt{d T}\\right)}\\right)$ . ", "page_idx": 14}, {"type": "text", "text": "Proof sketch. First, we show a change of variable between the sparsity and interval partition cases, including the parameter space and set of arms. Next, we demonstrate that mapping the problem instances in the lower bound of sparse bandits in [23, 24] satisfies the constraints in Assumption 6. After that, the proof follows immediately by carrying out step-by-step calculations similar to those in [23, 24]. ", "page_idx": 14}, {"type": "text", "text": "First, let us define the sparse bandit instances. Let $\\mathcal{Z}$ be the set of arm of sparse bandit problem, such that $\\forall z\\in{\\mathcal{Z}}$ , $\\|z\\|_{\\infty}\\leq\\bar{1}$ . Let $\\varphi\\in\\mathbb{R}^{d}$ be a $d_{0}$ -sparse parameters. Now, let us define the mapping that corresponds to change of variable between $\\theta$ and $\\varphi$ as follows: (1) For $i\\in[d-1]$ , $\\varphi_{i}=\\theta_{i}-\\theta_{i+1}$ , (2) $\\varphi_{d}=\\theta_{d}$ . Therefore, one can write ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\varphi=\\left[\\begin{array}{l l l l l l l}{1}&{-1}&{0}&{0}&{\\cdots}&{0}&{0}\\\\ {0}&{1}&{-1}&{0}&{\\cdots}&{0}&{0}\\\\ {0}&{0}&{1}&{-1}&{\\cdots}&{0}&{0}\\\\ {\\vdots}&{\\vdots}&{\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}&{\\vdots}\\\\ {\\vdots}&{\\vdots}&{\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}&{\\vdots}\\\\ {0}&{0}&{0}&{0}&{\\cdots}&{1}&{-1}\\\\ {0}&{0}&{0}&{0}&{\\cdots}&{0}&{1}\\end{array}\\right]\\theta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "It is easy to verify that entries of $\\theta$ has $k$ equivalent classes if and only if $\\varphi$ is $k$ sparse. Now, suppose expected reward for a arm $x$ in interval-partition instance is equal to expected reward for an arm $z$ in sparsity instance, that is, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\langle x,\\theta\\rangle=\\left\\langle(W^{\\top})^{-1}x,W\\theta\\right\\rangle=\\left\\langle z,\\varphi\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, to guarantee the bijection between sparsity and interval partition instance, we define the set of arm $\\check{\\chi}\\stackrel{\\smile}{=}W^{\\top}(\\mathcal{Z}):=\\left\\{W^{\\top}z\\mid z\\in\\mathcal{Z}\\right\\}$ . ", "page_idx": 15}, {"type": "text", "text": "We need to prove that $\\mathcal{X}$ satisfies Assumption 6. Therefore, $x=W^{\\top}z$ , and $\\mathcal{X}=W^{\\top}(\\mathcal{Z})$ . Our job is to verify that, for all $z\\in[-1,\\ 1]^{d}$ , the projection $\\|\\Pi_{m}(V z)\\|_{2}\\leq\\sqrt{8d_{0}}$ , for any $m$ such that $\\mathrm{dim}(m)\\leq2\\dot{d}_{0}$ . Let $S_{1},...,S_{\\mathrm{dim}(m)}$ be the classes according to the interval partition corresponding to $m$ , such that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left|\\left|\\Pi_{m}(x)\\right|\\right|_{2}^{2}=}&{\\displaystyle\\sum_{k=1}^{\\dim(m)}\\left(\\underbrace{\\frac{\\sum_{j\\leq k}|S_{k}|}{\\left|\\underbrace{S_{k}}_{\\operatorname{i}=(\\sum_{j<k}|S_{j}|)+1}}x_{i}}_{\\operatorname{average\\,of\\,entries\\,with}\\,\\mathrm{clas}\\,S_{k}}\\right)^{2}}\\\\ {=}&{\\displaystyle\\sum_{k=1}^{\\dim(m)}\\,\\frac{1}{|S_{k}|}\\left(\\underbrace{\\sum_{j\\leq k}|S_{k}|}_{\\sum_{i=(\\sum_{j<k}|S_{j}|+1)}x_{i}}x_{i}\\right)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $x=W^{\\top}z$ , we have that $x_{i}=-z_{i-1}+z_{i}$ , $\\forall i\\in[d]$ , with $z_{0}=0$ for convenience. Note that $z_{i}\\in[-1,1]\\forall i\\in[d]$ . We have that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\Pi_{m}(W^{\\top}z)\\|_{2}^{2}=\\displaystyle\\sum_{k=1}^{\\dim(m)}\\frac{1}{|S_{k}|}\\left(\\underbrace{\\sum_{j\\leq k}|S_{j}|}_{\\mathrm{Telex\\,\\mathrm{coping\\,sum}}}-z_{i-1}+z_{i}\\right)^{2}}\\\\ &{\\leq\\displaystyle\\sum_{k=1}^{\\dim(m)}\\frac{4}{|S_{k}|}\\leq8d_{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, we have that, for all $x\\in\\mathcal{X}=W^{\\top}(\\mathcal{Z}),\\|\\Pi_{m}(x)\\|_{2}\\leq2\\sqrt{2d_{0}}.$ ", "page_idx": 15}, {"type": "text", "text": "Moreover, we also have that $C_{\\mathrm{min}}(\\mathcal X)\\,\\geq\\,C_{\\mathrm{min}}(\\mathcal Z)$ . Particularly, let us denote the exploratory distribution on $\\mathcal{Z}$ as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\rho=\\underset{\\omega\\in\\Delta(\\mathcal{Z})}{\\arg\\operatorname*{max}}\\,\\lambda_{\\operatorname*{min}}\\left(\\mathbb{E}_{z\\sim\\omega}[z z^{\\top}]\\right),\\quad P:=\\mathbb{E}_{z\\sim\\rho}[z z^{\\top}],\\quad C_{\\operatorname*{min}}(\\mathcal{Z}):=\\lambda_{\\operatorname*{min}}(P).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Define $\\mu(x)\\,:=\\,\\rho((W^{\\top})^{-1}z)$ , and let $U:=\\mathbb{E}_{x\\sim\\mu}[x^{\\top}x]$ , we have that $U\\,=\\,W^{\\top}P W$ . Since all eigenvalue of $W$ is $1$ , according to [30], we have that $\\lambda_{i}(\\mathbf{\\dot{\\boldsymbol{P}}})=\\lambda_{i}(U)$ for all $i\\in[d]$ . Therefore, we have that $C_{\\mathrm{min}}(\\mathcal X)\\ge C_{\\mathrm{min}}(\\mathcal Z)\\$ . ", "page_idx": 16}, {"type": "text", "text": "Then, using change of variable argument, the calculation of the lower bound is identical to that of [23, 24]. Therefore, we can conclude that all policies must suffer regret ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\bf R}_{T}=\\Omega(C_{\\mathrm{min}}(\\mathcal{X})^{-\\frac{1}{3}}d_{0}^{\\frac{2}{3}}T^{\\frac{2}{3}},\\sqrt{d T}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "B.2 High Probability Prediction Error of Model Selection ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The proof of Proposition 14 uses the following concentration. ", "page_idx": 16}, {"type": "text", "text": "Proposition 22 ([21]\u2019s Theorem B.7). For some subspace $S\\subset\\mathbb{R}^{n}$ of dimension $d_{S}$ , with probability at least $1-\\delta$ , one has that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left|\\|\\Pi_{S}(\\eta)\\|_{2}-\\sigma\\sqrt{d_{S}}\\right|\\leq\\sigma\\sqrt{2\\log\\left(\\frac{1}{\\delta}\\right)}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proposition 14. Let $\\ensuremath{f_{\\star}}=X\\theta_{\\star}$ . For the choice of $\\widehat{\\pmb{f}}_{\\widehat{m}}$ as in Eqn. (3) & Eqn. (4), with probability at least $1-\\delta_{i}$ , there exists a constant $C>1$ such tha t ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\lVert\\widehat{f}_{\\widehat{m}}-f_{\\star}\\right\\rVert_{2}^{2}\\leq C\\sigma^{2}\\log\\left(M\\delta^{-1}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. Define ${\\mathcal{E}}_{1}$ as the event such that for all $m\\in\\mathcal{M}$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|\\Pi_{S_{m}}(\\eta)\\|_{2}\\leq\\sigma\\sqrt{d_{m}}+\\sigma\\sqrt{2\\log\\left(\\frac{1}{\\delta_{0}}\\right)}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By Proposition 22 and union bound, ${\\mathcal{E}}_{1}$ occurs with probability $1-M\\delta_{0}$ . For the rest of the proof, we assume ${\\mathcal{E}}_{1}$ occurs . ", "page_idx": 16}, {"type": "text", "text": "Let us denote ${\\widehat{\\pmb f}}:={\\widehat{\\pmb f}}_{\\widehat{m}}$ . By the model selection procedure (4), one has that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|Y-\\widehat{\\pmb f}\\|^{2}\\leq\\|Y-\\widehat{\\pmb f}_{m_{\\star}}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Also, as $Y=f_{\\star}+\\eta$ , one has that, for some $K>1$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\widehat{f}-f_{\\star}\\right\\|_{2}^{2}\\leq\\left\\|f_{\\star}-\\widehat{f}_{m_{\\star}}\\right\\|_{2}^{2}+2\\left\\langle\\eta,f_{\\star}-\\widehat{f}_{m_{\\star}}\\right\\rangle+K\\sigma^{2}d_{\\widehat{m}}-2\\left\\langle\\eta,f_{\\star}-\\widehat{f}\\right\\rangle-K\\sigma^{2}d_{\\widehat{m}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "First, consider the term $\\left\\|f_{\\star}-\\widehat{f}_{m_{\\star}}\\right\\|_{2}^{2}$ . Note that, $\\Pi_{S_{m_{\\star}}}(f_{\\star})=f_{\\star}$ , as $\\pmb{f}_{\\star}\\in S_{m_{\\star}}$ . We have that, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|f_{\\star}-\\widehat f_{m_{\\star}}\\right\\|_{2}^{2}=\\left\\|f_{\\star}-\\Pi_{S_{m_{\\star}}}(f_{\\star}+\\eta)\\right\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad=\\left\\|f_{\\star}-f_{\\star}\\right\\|_{2}^{2}-2\\left\\langle\\Pi_{S_{m_{\\star}}}(\\eta),f_{\\star}-f_{\\star}\\right\\rangle+\\left\\|\\Pi_{S_{m_{\\star}}}(\\eta)\\right\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\left(\\sigma\\sqrt{d_{m_{\\star}}}+\\sigma\\sqrt{2\\log{\\left(\\frac{1}{\\delta_{0}}\\right)}}\\right)^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq2\\sigma^{2}d_{0}+4\\sigma^{2}\\log{\\left(\\frac{1}{\\delta_{0}}\\right)}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Second, consider the term $\\left<\\eta,f_{\\star}-\\widehat{f}_{m_{\\star}}\\right>$ . We have that, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\phantom{\\left\\langle\\eta,\\,f_{\\star}-\\widehat{f}_{m_{\\star}}\\right\\rangle}}&{=\\left\\langle\\eta,f_{\\star}-\\Pi_{S_{m_{\\star}}}(f_{\\star}+\\eta)\\right\\rangle}\\\\ &{\\phantom{=\\left\\langle\\eta,f_{\\star}-\\widehat{f}_{\\star}\\right\\rangle}=\\left\\langle\\eta,f_{\\star}-f_{\\star}\\right\\rangle-\\left\\Vert\\Pi_{S_{m_{\\star}}}(\\eta)\\right\\Vert_{2}^{2}\\le0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Third, we need to control the magnitude of the term $2\\left\\langle\\eta,\\widehat{\\pmb f}-\\pmb f_{\\star}\\right\\rangle-K\\sigma^{2}d_{\\widehat{m}}$ . We show that this term can be bounded as ", "page_idx": 17}, {"type": "equation", "text": "$$\n2\\left<\\eta,\\widehat{f}-f_{\\star}\\right>-K\\sigma^{2}d_{\\widehat{m}}\\leq a^{-1}\\left\\lVert\\widehat{f}-f_{\\star}\\right\\rVert_{2}^{2}+O(\\log(1/\\delta_{0})),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with probability at least $1-M\\delta_{0}$ , for any constant $a>0$ . Denote $\\langle f_{\\star}\\rangle$ be the line that is spanned by $f_{\\star}$ . For each $m\\in\\mathcal{M}$ , let us define the subspace $\\bar{S}_{m}=S_{m}+\\langle\\pmb{f}_{\\star}\\rangle$ . Define $\\tilde{S}_{m}\\subset\\bar{S}_{m}$ as the subspace that is orthogonal to $\\langle f_{\\star}\\rangle$ , that is, one can write $\\bar{S}_{m}=\\tilde{S}_{m}\\bigoplus\\left\\langle f_{\\star}\\right\\rangle$ . By AM-GM inequality, for some $a>0$ , we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\left\\langle\\eta,\\widehat{f}-f_{\\star}\\right\\rangle=2\\left\\langle\\Pi_{\\bar{S}_{\\widehat{m}}}(\\eta),\\widehat{f}-f_{\\star}\\right\\rangle=2\\left\\langle\\sqrt{a}\\cdot\\Pi_{\\bar{S}_{\\widehat{m}}}(\\eta),\\frac{1}{\\sqrt{a}}\\left(\\widehat{f}-f_{\\star}\\right)\\right\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq a\\left\\lVert\\Pi_{\\bar{S}_{\\widehat{m}}}(\\eta)\\right\\rVert_{2}^{2}+a^{-1}\\left\\lVert\\widehat{f}-f_{\\star}\\right\\rVert_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=a\\sigma^{2}V+a\\sigma^{2}U_{\\widehat{m}}+a^{-1}\\left\\lVert\\widehat{f}-f_{\\star}\\right\\rVert_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $V\\,=\\,\\sigma^{-2}\\left|\\left|\\Pi_{\\langle\\pmb{f}_{\\star}\\rangle}(\\pmb{\\eta})\\right|\\right|_{2}^{2}$ and $\\begin{array}{r}{U_{\\hat{m}}\\,=\\,\\sigma^{-2}\\left\\lVert\\Pi_{\\tilde{S}_{\\hat{m}}}(\\pmb{\\eta})\\right\\rVert_{2}^{2}}\\end{array}$ . Note that, as $\\mathrm{dim}(\\langle f_{\\star}\\rangle)\\,=\\,1$ , with probability at least $1-\\delta_{0}$ , one has that ", "page_idx": 17}, {"type": "equation", "text": "$$\nV\\leq2+4\\log(\\delta_{0}^{-1}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Define the event $\\mathcal{E}_{2}$ as the event where the above inequality holds. ", "page_idx": 17}, {"type": "text", "text": "Therefore, our final task is to control the quantity $a U_{\\hat{m}}-K d_{\\hat{m}}$ . Choose $a=(K+1)/2>1$ , one has that ", "page_idx": 17}, {"type": "equation", "text": "$$\na U_{\\hat{m}}-K d_{\\hat{m}}=\\frac{K+1}{2}\\left(U_{\\hat{m}}-\\frac{2K}{K+1}d_{\\hat{m}}\\right)\\le\\frac{K+1}{2}\\operatorname*{max}_{m\\in\\mathcal{M}}\\left(U_{m}-\\frac{2K}{K+1}d_{m}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "dNiostwri, bduitrieocnt lym icgohntt rboel  cthoem pmlaicgantietdu.d eI nosft $\\begin{array}{r}{U_{\\hat{m}}\\,-\\,\\frac{2K}{K+1}d_{\\hat{m}}}\\end{array}$ inst rdoilf tfhiceu ltm, aaxsi $\\widehat{\\vphantom{\\left(m\\right)}}\\widehat{m}$ dme poef ntdhse  oanb $\\eta$ ,e  aqnuda tnhtietiyr $\\begin{array}{r}{U_{m}-\\frac{2K}{K+1}d_{m}}\\end{array}$ over all $m\\in\\mathcal{M}$ . Since the dimension of ${\\tilde{S}}_{m}$ is at most $d_{m}$ , similar as the event ${\\mathcal{E}}_{1}$ , we define the event ${\\mathcal{E}}_{3}$ as for all $m\\in\\mathcal{M}$ , we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left\\Vert\\Pi_{\\tilde{S}_{m}}(\\eta)\\right\\Vert_{2}\\leq\\sigma\\sqrt{d_{m}}+\\sigma\\sqrt{2\\log\\left(\\frac{1}{\\delta_{0}}\\right)}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By Proposition 22 and the union bound, $\\mathcal{E}_{3}$ occurs with probability at least $1-M\\delta_{0}$ . We assume that ${\\mathcal{E}}_{3}$ occurs, then for all $m\\in\\mathcal{M}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle U_{m}\\leq\\left(\\sqrt{d_{m}}+\\sqrt{2\\log\\left(\\frac{1}{\\delta_{0}}\\right)}\\right)^{2}}\\\\ {\\displaystyle\\leq\\frac{2K}{K+1}d_{m}+\\frac{4K}{K-1}\\log\\left(\\frac{1}{\\delta_{0}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, one has that for all $m\\in\\mathcal{M}$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{K+1}{2}\\left(U_{\\hat{m}}-\\frac{2K}{K+1}d_{\\hat{m}}\\right)\\leq\\frac{K+1}{2}\\operatorname*{max}_{m\\in\\mathcal{M}}\\left(U_{m}-\\frac{2K}{K+1}d_{m}\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\frac{2K(K+1)}{K-1}\\log\\left(\\frac{1}{\\delta_{0}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Putting things together, let $\\delta=(2M\\!+\\!1)\\delta_{0}$ , we assure that event $\\mathcal{E}_{1}\\cap\\mathcal{E}_{2}\\cap\\mathcal{E}_{3}$ occurs with probability at least $1-\\delta$ . Combining (26), (27), (29), (32), with (25), one has that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{K-1}{K+1}\\left\\|\\hat{\\boldsymbol f}-\\boldsymbol f_{*}\\right\\|_{2}^{2}\\le2\\sigma^{2}d_{0}+4\\sigma^{2}\\log\\left(\\displaystyle\\frac{2M+1}{\\delta}\\right)+K\\sigma^{2}d_{0}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\sigma^{2}(K+1)+2\\sigma^{2}(K+1)\\log\\left(\\displaystyle\\frac{2M+1}{\\delta}\\right)+\\sigma^{2}\\displaystyle\\frac{2K(K+1)}{K-1}\\log\\left(\\displaystyle\\frac{2M+1}{\\delta}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that, the dominating term in the above equation is $\\log\\left(\\frac{M}{\\delta}\\right)$ , we have that, there is some constant $C>0$ . ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\|{\\widehat{f}}-f_{\\star}\\right\\|_{2}^{2}\\leq C\\sigma^{2}\\log\\left({\\frac{M}{\\delta}}\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proposition 16. Let $X~=~[x_{t}]_{t\\in[n]}$ , where $x_{t}\\,\\mathrm{\\Delta}^{\\prime}s$ are is i.i.d. drawn from $\\nu$ , and let $n\\ =$ $\\Omega\\left(C_{\\mathrm{min}}^{-2}(\\mathcal{X})K_{x}^{2}\\left(\\log(2\\overline{{M}}d_{0}\\delta^{-1})\\right)\\right)$ . Then, with probability at least $1-\\delta$ , and for any $\\theta_{1},\\theta_{2}$ in subspaces of $\\mathcal{M}$ , one has that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\theta_{1}-\\theta_{2}\\|_{2}^{2}\\leq2C_{\\operatorname*{min}}^{-1}(\\chi)n^{-1}\\left\\|X(\\theta_{1}-\\theta_{2})\\right\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. Our proof strategy is inspired by [8]. ", "page_idx": 18}, {"type": "text", "text": "First, we compute the subgaussian norm for the normalised distribution along fews directions $\\nu$ . For any $x\\in\\mathscr{X}$ , we have that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{||v||_{2}=1,}{\\operatorname*{max}}\\quad}&{\\Big\\langle V^{-1/2}x,v\\Big\\rangle^{2}=\\underset{||V^{1/2}\\omega||_{2}=1,}{\\operatorname*{max}}\\Big\\langle V^{-1/2}x,V^{1/2}\\omega\\Big\\rangle^{2}}\\\\ {v\\in V^{1/2}(\\overline{{m}}),\\,\\overline{{m}}\\in\\overline{{\\mathcal{M}}}}&{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad}\\\\ &{=\\underset{||V^{1/2}\\omega||_{2}=1,}{\\operatorname*{max}}\\langle x,\\omega\\rangle^{2}}\\\\ &{\\quad\\omega\\in\\overline{{\\mathcal{m}}},\\,\\overline{{m}}\\in\\overline{{\\mathcal{M}}}}\\\\ &{\\leq\\underset{|\\overline{{m}}\\in\\overline{{\\mathcal{M}}}}{\\operatorname*{max}}\\,||\\Pi_{\\overline{{m}}}(x)||_{2}^{2}C_{\\operatorname*{min}}^{-1}(\\mathcal{X})\\leq\\frac{K_{x}}{C_{\\operatorname*{min}}(\\mathcal{X})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This means that, the distribution $\\left\\langle V^{-1/2}x,v\\right\\rangle^{2}$ has bounded support [0, $\\frac{K_{x}}{C_{\\operatorname*{min}}}]$ for any above direction of $v$ . ", "page_idx": 18}, {"type": "text", "text": "Next, for any $\\theta\\in{\\overline{{m}}}$ , denote $\\theta_{V}\\mathrel{\\mathop:}=V^{1/2}\\theta$ , note that the direction $v$ corresponding to $\\theta_{V}$ satisfies (35). Using Hoeffding\u2019s inequality for bounded random variable, there is an absolute constant $c_{1}>0$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\quad\\mathrm{Pr}\\left\\{\\left|\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\left<V^{-1/2}x_{i},\\theta_{V}\\right>^{2}-\\|\\theta_{V}\\|_{2}^{2}\\right|\\ge\\epsilon\\left\\|\\theta_{V}\\right\\|_{2}^{2}\\right\\}\\le2\\exp\\left(-\\displaystyle\\frac{n c_{1}C_{\\operatorname*{min}}^{2}(\\mathcal{X})\\epsilon^{2}}{K_{x}^{2}}\\right)}\\\\ &{\\Longleftrightarrow\\mathrm{Pr}\\left\\{\\left|\\displaystyle\\frac{1}{n}\\left\\|X V^{-1/2}\\theta_{V}\\right\\|_{2}^{2}-\\|\\theta_{V}\\|_{2}^{2}\\right\\rvert\\ge\\epsilon\\left\\|\\theta_{V}\\right\\|_{2}^{2}\\right\\}\\le2\\exp\\left(-\\displaystyle\\frac{n c_{1}C_{\\operatorname*{min}}^{2}(\\mathcal{X})\\epsilon^{2}}{K_{x}^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Let $\\begin{array}{r}{Z:=\\frac{1}{\\sqrt{n}}X V^{-1/2}}\\end{array}$ . From Lemma 5.1 in [5], we know that if the above inequality holds, then ", "page_idx": 18}, {"type": "equation", "text": "$$\n(1-\\delta_{\\overline{{M}}}\\left(Z\\right)\\Vert\\theta_{V}\\Vert_{2})\\leq\\Vert Z\\theta_{V}\\Vert_{2}\\leq\\left(1+\\delta_{\\overline{{M}}}\\left(Z\\right)\\Vert\\theta_{V}\\Vert_{2}\\right),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "holds with probability more than ", "page_idx": 19}, {"type": "equation", "text": "$$\n1-2\\left(\\frac{12}{\\delta_{\\overline{{{M}}}}\\left(Z\\right)}\\right)^{2d_{0}}\\exp\\left(-\\frac{c_{1}C_{\\mathrm{min}}^{2}(\\chi)n\\delta_{\\overline{{{M}}}}^{2}\\left(Z\\right)}{K_{x}^{2}}\\right),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for any $\\theta_{V}\\in V^{1/2}(\\overline{{m}})$ in a subspace ${\\overline{{m}}}\\in{\\overline{{M}}}$ . Take union bound for $\\overline{{M}}$ subspaces, and let ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\delta=2\\overline{{M}}\\left(\\frac{2}{\\delta_{\\overline{{M}}}\\,(Z)}\\right)^{2d_{0}}\\exp\\left(-\\frac{c_{1}C_{\\mathrm{min}}^{2}(\\mathcal{X})n\\delta_{\\overline{{M}}}^{2}(Z)}{K_{x}^{2}}\\right),\\quad\\quad\\quad}\\\\ &{}&{\\iff n=O\\left(\\frac{K_{x}^{2}}{\\delta_{\\overline{{M}}}^{2}\\,(Z)\\,C_{\\mathrm{min}}^{2}(\\mathcal{X})}\\left(\\log(2\\overline{{M}})+d_{0}\\log\\left(\\frac{1}{\\delta_{\\overline{{M}}}\\,(Z)}\\right)+\\log(\\delta^{-1})\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Let $\\delta_{\\overline{{M}}}^{2}\\left(Z\\right)=1/2$ . Then, Given C2miKnx2(X) log(2M) + d0 + log(\u03b4\u22121)  , for probability at least $1-\\delta$ , we have that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\left.\\frac{1}{2}\\left\\|V^{1/2}\\theta\\right\\|_{2}^{2}\\right.}&{\\leq\\frac{1}{n}\\left\\|X V^{-1/2}V^{1/2}\\theta\\right\\|_{2}^{2},}\\\\ {\\displaystyle\\implies\\frac{1}{2}C_{\\operatorname*{min}}(\\mathcal{X})\\left\\|\\theta\\right\\|_{2}^{2}}&{\\leq\\frac{1}{n}\\left\\|X\\theta\\right\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Let $\\theta:=\\theta_{1}-\\theta_{2}$ , for any $\\theta_{1}\\in m,\\theta_{2}\\in m^{\\prime}$ , for any $m,m^{\\prime}\\in\\mathcal{M}$ . We conclude the proof. ", "page_idx": 19}, {"type": "text", "text": "B.3 Regret Upper bound ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Lemma 12. Suppose the Assumptions 5, 6 hold. For $t_{1}\\,=\\,\\Omega(K_{x}^{2}d_{0}C_{\\mathrm{min}}^{-2}(\\mathcal{X})\\log(d/\\delta))$ , with probability at least $1-\\delta$ , one has the estimate ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\|\\theta_{\\star}-\\widehat{\\theta}_{t_{1}}\\right\\|_{2}=O\\left(\\sqrt{\\frac{\\sigma^{2}d_{0}\\log(d/\\delta)}{C_{\\operatorname*{min}}(\\chi)t_{1}}}\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. First, let consider Proposition 16, and recall that $\\log(M)$ and $\\log(\\overline{{M}})$ are both $O(d_{0}\\log(d))$ . Therefore, for the choice of $t_{1}=\\Omega(K_{x}^{2}d_{0}C_{\\mathrm{min}}^{-2}(\\mathcal{X})\\log(d/\\delta))$ , with probability at least $1-\\delta/2$ , one has that ", "page_idx": 19}, {"type": "equation", "text": "$$\nC_{\\mathrm{min}}(\\boldsymbol{\\mathcal{X}})\\left\\|\\theta_{\\star}-\\widehat{\\theta}_{t_{1}}\\right\\|_{2}^{2}\\leq\\frac{2}{t_{1}}\\left\\|\\boldsymbol{X}\\big(\\theta_{\\star}-\\widehat{\\theta}_{t_{1}}\\big)\\right\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Second, by Proposition 14, one has that with probability at least $1-\\delta/2$ . ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\|X(\\theta_{\\star}-\\widehat\\theta_{t_{1}})\\right\\|_{2}^{2}\\leq c_{1}\\sigma^{2}d_{0}\\log\\left(\\frac{d}{\\delta}\\right),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for some constant $c_{1}>0$ . ", "page_idx": 19}, {"type": "text", "text": "Therefore, putting thing together, we have that with probability at least $1-\\delta$ , one has that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\|\\theta_{\\star}-\\widehat{\\theta}_{t_{1}}\\right\\|_{2}\\leq c_{2}\\sqrt{\\frac{\\sigma^{2}d_{0}\\log\\left(\\frac{d}{\\delta}\\right)}{C_{\\operatorname*{min}}(\\mathcal{X})t_{1}}},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for some constant $c_{2}>0$ . ", "page_idx": 19}, {"type": "text", "text": "Theorem 10 (Regret upper bound). Suppose the Assumptions 5, 6 hold. With the choice of $t_{1}=R_{\\operatorname*{max}}^{-\\frac{2}{3}}\\sigma^{\\frac{2}{3}}C_{\\operatorname*{min}}^{-\\frac{1}{3}}(\\chi)\\bar{K}_{x}^{\\frac{7}{3}}d_{0}^{\\frac{1}{3}}T^{\\frac{2}{3}}(\\log(d T))^{\\frac{1}{3}}$ , then the regret of Algorithm $^{\\,l}$ is upper bounded as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{R}_{T}=O\\left(R_{\\operatorname*{max}}^{\\frac{1}{3}}\\sigma^{\\frac{2}{3}}C_{\\operatorname*{min}}^{-\\frac{1}{3}}(\\chi)K_{x}^{\\frac{1}{3}}d_{0}^{\\frac{1}{3}}T^{\\frac{2}{3}}(\\log(d T))^{\\frac{1}{3}}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. Let define the pesudo regret as $\\begin{array}{r}{\\widehat{\\mathbf{R}}_{T}=\\sum_{t=1}^{T}\\left\\langle x_{\\star}-x_{t},\\theta_{\\star}\\right\\rangle}\\end{array}$ . Denote ${\\overline{{m}}}\\in{\\overline{{M}}}$ be the subspace contains $\\theta_{\\star}-\\widehat{\\theta}_{t_{1}}$ . We start by simple regret decomposition as follows. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\mathbf{R}}_{T}=\\displaystyle\\sum_{t=1}^{T}\\left(\\theta_{s},\\mathbf{x}_{s}-\\boldsymbol{x}_{t}\\right)=\\frac{1}{t-\\omega}\\left(\\theta_{s},\\boldsymbol{x}_{s}-\\boldsymbol{x}_{t}\\right)+\\displaystyle\\sum_{t=1+1}^{T}\\left(\\theta_{s},\\alpha_{s}-\\boldsymbol{x}_{t}\\right)}\\\\ &{\\qquad\\le R_{\\operatorname*{max}\\{t_{1}+\\}}+\\displaystyle\\sum_{t=1+1}^{T}\\left(\\theta_{s}-\\widehat{\\theta}_{t_{1}},\\alpha_{s}-\\boldsymbol{x}_{t}\\right)+\\displaystyle\\sum_{t=1+1}^{T}\\left(\\widehat{\\theta}_{t_{1}},\\alpha_{s}-\\boldsymbol{x}_{t}\\right)}\\\\ &{\\qquad\\le R_{\\operatorname*{max}\\{t_{1}+\\}}+\\displaystyle\\sum_{t=1+1}^{T}\\left(\\theta_{s}-\\widehat{\\theta}_{t_{1}},\\alpha_{s}-\\boldsymbol{x}_{t}\\right)}\\\\ &{\\qquad=R_{\\operatorname*{max}\\{t_{1}+\\}}+\\displaystyle\\sum_{t=1+1}^{T}\\left(\\theta_{s}-\\widehat{\\theta}_{t_{1}},\\Pi_{m}(\\alpha_{s}-\\boldsymbol{x}_{t})\\right)}\\\\ &{\\qquad=R_{\\operatorname*{max}\\{t_{1}+\\}}+\\displaystyle\\sum_{t=1+1}^{T}\\left(\\theta_{s}-\\widehat{\\theta}_{t_{1}},\\Pi_{m}(\\alpha_{s}-\\boldsymbol{x}_{t})\\right)}\\\\ &{\\qquad\\le R_{\\operatorname*{max}\\{t_{1}+\\}}+\\displaystyle\\sum_{t=1+1}^{T}\\left\\|\\theta_{s}-\\widehat{\\theta}_{t_{1}}\\right\\|_{2}\\left\\|\\Pi_{m}(\\alpha_{s}-\\boldsymbol{x}_{t})\\right\\|_{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\left[5\\operatorname*{suc}\\|\\Pi_{m}(\\alpha_{s}-\\boldsymbol{x}_{t})\\right\\|_{2}\\right]}\\\\ &{\\qquad\\le R_{\\operatorname*{max}\\{t_{1}+\\}}+\\displaystyle\\sum_{t=1+1}^{T}2\\sqrt{K_{m}}\\left\\|\\theta_{s}-\\widehat{\\theta}_{t_{1}}\\right\\|_{2};}\\end{array}\n$$$$\n\\begin{array}{r}{\\left[\\mathrm{Since~}||\\Pi_{\\overline{{m}}}(x_{\\star}-x_{t})||_{2}\\leq2\\sqrt{K_{x}}\\,\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now, we invoke Lemma 12. Let $\\mathcal{E}$ is the event in that the exploration error is bounded as in Lemma 12, then there is an absolute constant $c_{1}>0$ such that, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbf{R}_{T}\\leq R_{\\operatorname*{max}}t_{1}+\\mathbb{E}\\left[\\sum_{t=t_{1}+1}^{T}2\\sqrt{K_{x}}\\left\\|\\theta_{\\star}-\\widehat{\\theta}_{t_{1}}\\right\\|_{2}\\bigg|\\mathcal{E}\\right]+T\\mathrm{Pr}(\\mathcal{E})R_{\\operatorname*{max}}}\\\\ {\\leq R_{\\operatorname*{max}}t_{1}+c_{1}\\sqrt{K_{x}}\\sqrt{\\frac{\\sigma^{2}d_{0}\\log(d/\\delta)}{C_{\\operatorname*{min}}(\\mathcal{X})t_{1}}}T+T\\delta R_{\\operatorname*{max}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Let $\\delta=1/T$ , and $t_{1}=R_{\\operatorname*{max}}^{-\\frac{2}{3}}\\sigma^{\\frac{2}{3}}C_{\\operatorname*{min}}^{-\\frac{1}{3}}(\\chi)K_{x}^{\\frac{1}{3}}d_{0}^{\\frac{1}{3}}T^{\\frac{2}{3}}(\\log(d T))^{\\frac{1}{3}}$ , one has that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbf{R}_{T}=O\\left(R_{\\operatorname*{max}}^{\\frac{1}{3}}\\sigma^{\\frac{2}{3}}C_{\\operatorname*{min}}^{-\\frac{1}{3}}(\\chi)K_{x}^{\\frac{1}{3}}d_{0}^{\\frac{1}{3}}T^{\\frac{2}{3}}(\\log(d T))^{\\frac{1}{3}}\\right)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "C Improved Regret Bound ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "C.1 Improve Regret Bound with Well-Separated Partitions ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Theorem 18. Suppose the Assumptions $5$ , 6, 17 hold. Let $\\begin{array}{r}{t_{2}=\\Omega\\left(\\frac{\\sigma^{2}K_{x}^{2}d_{0}\\log\\left(d T\\right)}{C_{\\operatorname*{min}}^{2}\\left(\\mathcal{X}\\right)\\varepsilon_{0}^{2}}\\right)}\\end{array}$ Then, Algorithm 2 returns $\\widehat{m}\\ni\\theta_{\\star}$ with probability at least $1-1/T$ , and its regret is upper bounded as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbf{R}_{T}=O\\left(\\frac{R_{\\mathrm{max}}\\sigma^{2}K_{x}^{2}d_{0}\\log(d T)}{C_{\\mathrm{min}}^{2}(\\mathcal{X})\\varepsilon_{0}^{2}}+\\sigma d_{0}\\sqrt{T\\log(K_{x}T)}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. Define the event $\\mathcal{E}$ as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{E}=\\left\\{\\|Y-\\Pi_{S_{\\widehat{m}}}(Y)\\|_{2}^{2}\\leq\\|Y-\\Pi_{S_{m}}(Y)\\|_{2}^{2}\\;\\vert\\;\\theta_{\\star}\\in\\widehat{m},\\;\\theta_{\\star}\\notin m\\right\\},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The event $\\mathcal{E}$ is equivalent as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|Y-\\Pi_{S_{\\widehat{m}}}(Y)\\right\\|_{2}^{2}\\leq\\left\\|Y-\\Pi_{S_{m}}(Y)\\right\\|_{2}^{2}}\\\\ {\\iff}&{\\left\\|f_{\\star}+\\eta-\\widehat{f}_{\\widehat{m}}\\right\\|_{2}^{2}\\leq\\left\\|f_{\\star}+\\eta-\\widehat{f}_{m}\\right\\|_{2}^{2}}\\\\ {\\iff}&{\\left\\|f_{\\star}-\\widehat{f}_{\\widehat{m}}\\right\\|_{2}^{2}+2\\left\\langle\\eta,f_{\\star}-\\widehat{f}_{\\widehat{m}}\\right\\rangle\\leq\\left\\|f_{\\star}-\\widehat{f}_{m}\\right\\|_{2}^{2}+2\\left\\langle\\eta,f_{\\star}-\\widehat{f}_{m}\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "First, we upper the LHS of (45) with high probability. ", "page_idx": 21}, {"type": "text", "text": "$\\|f_{\\star}-f_{\\star}-\\Pi_{S_{\\widehat{m}}}(\\eta)\\|_{2}^{2}+2\\,\\langle\\eta,-\\Pi_{S_{\\widehat{m}}}(\\eta)\\rangle=\\|\\Pi_{S_{\\widehat{m}}}(\\eta)\\|_{2}^{2}-2\\,\\|\\Pi_{S_{\\widehat{m}}}(\\eta)\\|_{2}^{2}\\le0\\quad[\\mathrm{as~}\\theta_{\\star}\\in\\widehat{m}]_{,}$ (46) with probability at least $1-\\delta$ . The above inequality uses the union bound for all $m\\in\\mathcal{M}$ . ", "page_idx": 21}, {"type": "text", "text": "Second, we lower bound the RHS of (45), $\\left\\|f_{\\star}-\\widehat{f}_{m}\\right\\|_{2}^{2}+2\\left\\langle\\eta,f_{\\star}-\\widehat{f}_{m}\\right\\rangle$ , with high probability. Let $\\overline{{{S}}}_{m}=S_{m}\\oplus\\langle f_{\\star}\\rangle,$ , then $\\left\\langle\\eta,f_{\\star}-\\widehat{f}_{m}\\right\\rangle=\\left\\langle\\Pi_{\\overline{{S}}_{m}}(\\eta),f_{\\star}-\\widehat{f}_{m}\\right\\rangle$ . Therefore, we have that, with probability at least $1-\\delta/3$ . ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\Vert f_{\\star}-\\widehat{f_{m}}\\right\\Vert_{2}^{2}+2\\left\\langle\\Pi_{\\bar{S}_{m}}(\\eta),f_{\\star}-\\widehat{f_{m}}\\right\\rangle\\geq\\left\\Vert f_{\\star}-\\widehat{f_{m}}\\right\\Vert_{2}^{2}-2\\left\\Vert\\Pi_{\\bar{S}_{m}}(\\eta)\\right\\Vert_{2}^{2}-\\frac{1}{2}\\left\\Vert f_{\\star}-\\widehat{f_{m}}\\right\\Vert_{2}^{2}}&{}\\\\ {\\geq\\frac{1}{2}\\left\\Vert f_{\\star}-\\widehat{f_{m}}\\right\\Vert_{2}^{2}-4\\left(\\sigma^{2}(d_{0}+1)+\\sigma^{2}(\\log(3M\\delta^{-1}))\\right)}&{}\\\\ {\\geq\\frac{1}{2}\\left\\Vert f_{\\star}-\\widehat{f_{m}}\\right\\Vert_{2}^{2}-5\\left(\\sigma^{2}d_{0}+\\sigma^{2}(\\log(3M\\delta^{-1}))\\right),}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "as $d_{m}\\leq d_{0}$ . ", "page_idx": 21}, {"type": "text", "text": "Also, we have that, with probability at least $1-\\delta/3$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\Big\\|f_{\\star}-\\widehat f_{m}\\Big\\|_{2}^{2}=\\|f_{\\star}-\\Pi_{S_{m}}(f_{\\star})-\\Pi_{S_{m}}(\\pmb\\eta)\\|_{2}^{2}}}\\\\ &{\\geq\\|f_{\\star}-\\Pi_{S_{m}}(f_{\\star})\\|_{2}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Where the inequalities holds because $\\Pi_{S_{m}}(f_{\\star})$ is the projection of $f_{\\star}$ to $S_{m}$ . Therefore, we have that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left\\|f_{\\star}-\\widehat f_{m}\\right\\|_{2}^{2}+2\\left\\langle\\eta,f_{\\star}-\\widehat f_{m}\\right\\rangle\\geq\\frac12\\left\\|f_{\\star}-\\Pi_{S_{m}}(f_{\\star})\\right\\|_{2}^{2}-5\\left(\\sigma^{2}d_{0}+\\sigma^{2}(\\log(M\\delta^{-1}))\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Now, by choosing $t_{2}=\\Omega\\left(C_{\\mathrm{min}}^{-2}(\\mathcal{X})K_{x}^{2}\\log(M\\delta^{-1})\\right)$ , by Proposition 16, we have that with probability at least $1-\\delta/3$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|f_{\\star}-\\Pi_{S_{m}}(f_{\\star})\\|_{2}^{2}=\\|X(\\theta_{\\star}-\\Pi_{S_{m}}(\\theta_{\\star}))\\|_{2}^{2}\\geq\\frac{t_{2}}{2C_{\\operatorname*{min}}(\\chi)}\\,\\|\\theta_{\\star}-\\Pi_{S_{m}}(\\theta_{\\star})\\|_{2}^{2}\\geq\\frac{t_{2}\\varepsilon_{0}^{2}}{4C_{\\operatorname*{min}}(\\chi)}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, the RHS of (45) is lower bounded as follows ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left\\|f_{\\star}-\\widehat f_{m}\\right\\|_{2}^{2}+2\\left\\langle\\eta,f_{\\star}-\\widehat f_{m}\\right\\rangle\\geq\\frac{t_{2}\\varepsilon_{0}^{2}}{8C_{\\operatorname*{min}}(\\mathscr X)}-5\\left(\\sigma^{2}d_{0}+\\sigma^{2}(\\log(3M\\delta^{-1}))\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, the sufficient condition for event $\\mathcal{E}$ holds with probability at least $1-\\delta$ is that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{t_{2}\\varepsilon_{0}^{2}}{8C_{\\mathrm{min}}(\\mathcal{X})}-5\\left(\\sigma^{2}d_{0}+\\sigma^{2}(\\log(M\\delta^{-1}))\\right)\\geq0;\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Note that as $M=O(d_{0}\\log(d))$ , for $\\mathcal{E}$ holds with probability at least $1-\\delta$ , it suffice to choose ", "page_idx": 21}, {"type": "equation", "text": "$$\nt_{2}=\\Omega\\left(\\frac{\\sigma^{2}K_{x}^{2}d_{0}\\log(d\\delta^{-1})}{C_{\\operatorname*{min}}^{2}(\\mathcal{X})\\varepsilon_{0}^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The regret upper bound is an immediate consequence of the fact that Algorithm 2 return the true subspace $\\hat{m}\\,\\ni\\,\\theta_{\\star}$ , combined with the regret bound of OFUL in the exploitation phase and $\\delta\\,=$ $1/T$ . \u53e3 ", "page_idx": 22}, {"type": "text", "text": "C.2 Adapting to Separating Constant $\\varepsilon_{0}$ ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "As stated in Theorem 18, if\u221a one knows in advance that the separating constant $\\varepsilon_{0}\\geq T^{-1/4}$ , then using \u221aAlgorithm 2 leads to $\\sqrt{T}$ regret. The reason is that the learner can learn the true subspace $m_{\\star}$ after $\\sqrt{T}$ steps with no mis-specification errors. However, without knowing $\\varepsilon_{0}\\geq T^{-1/4}$ a priori, one cannot guarantee to recover the true subspace. Hence, naively using Algorithm 2, which is not aware of potential mis-specification errors, leads to linear regret. On the other hand, using Algorithm 1 can achieve regret $T^{2/3}$ in the worst case (without the knowledge of $\\varepsilon_{0}\\geq T^{-1/4}$ ). A qu\u221aestion arises: does there exist an algorithm that, without the knowledge of $\\varepsilon_{0}$ , can achieve regret $\\sqrt{T}$ whenever $\\varepsilon_{0}\\geq T^{-1/4}$ , but guarantee the worst-case regret as $T^{2/\\breve{3}}$ ? ", "page_idx": 22}, {"type": "text", "text": "We note that the role of $\\varepsilon_{0}$ is similar to the minimum signal in sparsity, and it is somewhat surprising that the question of adapting to unknown minimum signal has not been resolved in the literature of sparse linear bandits. Towards answering the question, \u221awe propose a simple method using adaptation to misspecified error in linear bandit [19], which has a $\\sqrt{T}$ regret whenever the separating constant is large, and enjoys a worst-case regret guarantee of slightly worse $T^{3/4}$ regret. ", "page_idx": 22}, {"type": "text", "text": "The algorithm described in 4 is a direct application of the algorithm proposed in [19], designed for adapting to misspecification errors in linear bandits. Particularly, the alg\u221aorithm in [19] can adapt to unknown misspecification errors and achieve a regret bound of $\\tilde{O}(d_{0}\\sqrt{T}+\\epsilon_{\\mathrm{mis}}T)$ , where $\\epsilon_{\\mathrm{mis}}$ is the misspecification error. At a high level, our algorithm exploring $\\sqrt{T}$ rounds using exploratory distribution, which ensures that the misspecification error $\\epsilon_{\\mathrm{mis}}$ of the chosen subspace $\\hat{m}$ is at most $T^{-1/4}$ . Therefore, we can run multiple linear bandit algorithms using different levels of misspecification error. Particularly, we use a collection of $K=\\lfloor\\log(T)\\rfloor$ base algorithms, where a base algorithm $k\\in[K]$ is a linear bandit algorithm\u221a with misspecified level $\\varepsilon_{k}=2^{-k}$ . Note that the base algorithm $K$ has the same order of regret $\\sqrt{T}$ as a well-specified model. Therefore, in exploitation phase, one can guarantee that in\u221a the case of well-separated partitions where $\\epsilon_{\\mathrm{mis}}~=$ 0, the algorithm can achieve a regret\u221a of $d_{0}\\sqrt{T}$ , while in the general case, the regret caused by misspecification error is at most $T^{3/4}\\sqrt{d_{0}}$ . ", "page_idx": 22}, {"type": "text", "text": "Algorithm 3 Adaptive algorithm   \n1: Input $T,~\\nu,~t_{3}$ .   \n2: for $t=1,\\cdot\\cdot\\cdot\\,,t_{3}$ do   \n3: Independently pull arm $x_{t}$ according to $\\nu$ and receive a reward $y_{t}$ .   \n4: end for   \n5: $X\\gets[x_{1},...,x_{t_{1}}]^{\\intercal}$ , $Y\\gets[y_{t}]_{t\\in[t_{1}]}$ .   \n6: Compute $\\widehat{\\vphantom{\\left(m\\right)}}\\widehat{m}$ as (4).   \n7: Let $\\dot{K}=\\lfloor\\log(T^{1/4})\\rfloor,\\mathcal{E}=\\big\\{\\varepsilon_{k}:=2^{-k},\\ k\\in[K]\\big\\}.$   \n8: for $t=t_{2}+1$ to $T$ do   \n9: Corralling $K$ base misspecified linear bandit algorithms SquareCB. $\\mathtt{L i n+}(\\varepsilon_{k})$ [19] on $\\widehat{\\vphantom{\\left(m\\right)}}\\widehat{m}$ .   \n10: end for ", "page_idx": 22}, {"type": "text", "text": "Corollary 23. Suppose the Assumptions 5, $^{6}$ hold. Then, there exists an algorithm which achieves regret bound as follows: ", "page_idx": 22}, {"type": "text", "text": "(i) [Well-separated partitions] If $\\ '_{\\varepsilon_{0}}\\geq T^{-1/4}$ , then ${\\bf R}_{T}=\\tilde{O}(d_{0}\\sqrt{T})$ .   \n(ii) [Non-well-separated partitions] If $\\varepsilon_{0}<T^{-1/4}$ , then ${\\bf R}_{T}=\\tilde{O}(d_{0}\\sqrt{T}+T^{\\frac{3}{4}}\\sqrt{d_{0}})$ . ", "page_idx": 22}, {"type": "text", "text": "Proof sketch. Denote $\\epsilon_{\\mathrm{mis}}=\\|\\theta_{\\star}-\\Pi_{\\widehat{m}}(\\theta_{\\star})\\|_{2}$ as the misspecification error. With ", "page_idx": 22}, {"type": "equation", "text": "$$\nt_{3}=\\Omega\\left(\\sigma^{2}d_{0}\\log(d\\delta^{-1})\\sqrt{T}\\right),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We can guarantee that, with probability at least $1-\\delta$ , if: ", "page_idx": 22}, {"type": "text", "text": "The regret of adaptive algorithm in [19] is of the fo\u221arm $\\tilde{O}(d_{0}\\sqrt{T}{+}\\epsilon_{\\mathrm{mis}}T\\sqrt{d_{0}})$ . Let $\\delta=1/T$ . Consider case (i) where $\\epsilon_{\\mathrm{mis}}=0$ , we h\u221aave ${\\bf R}_{T}=\\tilde{O}(d_{0}\\sqrt{T})$ . Consider case (ii), where $\\epsilon_{\\mathrm{mis}}=T^{-1/4}$ , we have ${\\bf R}_{T}=\\tilde{O}(d_{0}\\sqrt{T}+T^{3/4}\\sqrt{d_{0}})$ . \u53e3 ", "page_idx": 23}, {"type": "text", "text": "The result in Corollary 23 is still sub-optimal in the worst case, as it can only achieve $O(T^{3/4})$ regret bound instead of ${\\cal O}(T^{2/3})$ . We conjecture that new techniques are required to achieve order-optimal regret in both cases, and will continue to investigate this question in future works. ", "page_idx": 23}, {"type": "text", "text": "D On Collections of Partitions with Subexponential-Size ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "D.1 Important Classes of Partitions with Subexponential-Size ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section, we discuss several important classes of partitions which satisfy Assumption 5. ", "page_idx": 23}, {"type": "text", "text": "Pattern-avoidance partitions is arguable the most important class of studied partition [33], in which, non-crossing partition is one the most studied. ", "page_idx": 23}, {"type": "text", "text": "Definition 24 (Non-Crossing Partition). Let $[d]$ admits a cylic order as $1<2<...<d$ , and $d<1$ . A non-crossing partition of $[d]$ is a partition such that for if $i,\\;j$ in one block and $p,\\ q$ in one block, then they are not arranged in the order $i<p<j<q$ . ", "page_idx": 23}, {"type": "text", "text": "Similarly, we denote $\\mathcal{N C}_{d},\\mathcal{N C}_{d,k},\\mathcal{N C}_{d,\\leq k}$ as the set of all non-crossing partition of $[d]$ , the set of all partition of $[d]$ with $k$ classes, and the set of all partition of $[d]$ with at most $k$ classes. We have the following fact ([33]\u2019 section 3.2). ", "page_idx": 23}, {"type": "equation", "text": "$$\n|{\\mathcal{N C}}_{d}|={\\frac{1}{d+1}}{\\binom{2d}{d}},\\quad|{\\mathcal{N C}}_{d,k}|={\\frac{1}{d}}{\\binom{d}{k}}{\\binom{d}{k-1}},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which is the Catalan number and the Narayana number. Note that, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{1}{d}{\\binom{d}{k}}{\\binom{d}{k-1}}\\leq\\frac{1}{d}\\left(\\frac{e d}{k}\\right)^{k}\\left(\\frac{e d}{k-1}\\right)^{k-1}\\leq\\frac{1}{d}\\left(\\frac{e d}{k}\\right)^{2k}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore, non-crossing partition satisfied the cardinality restriction as Assumption 5, that is, $\\mathcal{N C}_{d,\\leq d_{0}}\\subset\\mathcal{Q}_{d,\\leq d_{0}}$ . ", "page_idx": 23}, {"type": "text", "text": "Another important class of pattern-avoidence partitions is nonnesting partition [14]. ", "page_idx": 23}, {"type": "text", "text": "Definition 25 (Non-Nesting Partition). Let $[d]$ admits a cylic order as $1<2<...<d$ , and $d<1$ . A non-crossing partition of $[d]$ is a partition such that for if $i,\\;j$ in one block and $p,\\ q$ in one block, then they are not arranged in the order $i<p<q<j$ . ", "page_idx": 23}, {"type": "text", "text": "Similarly, we denote $\\mathcal{N N}_{d},\\mathcal{N N}_{d,k},\\mathcal{N N}_{d,\\le k}$ as the set of all non-crossing partition of $[d]$ , the set of all partition of $[d]$ with $k$ classes, and the set of all partition of $[d]$ with at most $k$ classes. There is bijections between class of $\\mathcal{N N}_{d}$ and $\\mathcal{N C}_{d}$ , non-nesting partitions also satisfy the sub-exponential constraint (Assumption 5). ", "page_idx": 23}, {"type": "text", "text": "One special case of both non-crossing partitions and non-nesting partitions is interval partition, which has identical structure as sparsity. ", "page_idx": 23}, {"type": "text", "text": "Definition 26 (Interval Partition). A set partition of $[d]$ is an interval partition or partition of interval if its parts are interval. ", "page_idx": 23}, {"type": "text", "text": "We denote $\\mathcal{T}_{d}$ as the collection of all interval partition of $d$ , we have that $\\mathcal{T}_{d}\\subset\\mathcal{N C}_{d}\\subset\\mathcal{P}_{d}$ , and $\\mathcal{T}_{d}\\subset\\mathcal{N N}_{d}\\subset\\mathcal{P}_{d}$ . ", "page_idx": 23}, {"type": "text", "text": "Remark 27. $\\mathcal{T}_{d}$ admits a Boolean lattice of order $2^{d-1}$ , making it equivalent to the sparsity structure in $d{-}1$ dimensions. Specifically, consider the set of entries of parameters $\\varphi\\in\\mathbb{R}^{d}$ with a linear order, that is, $\\varphi_{1}<\\varphi_{2}<\\cdot\\cdot<\\varphi_{d}$ . Then define the variable $\\theta\\in\\mathbb{R}^{d-\\hat{1}}$ such that $\\theta_{i}=(\\varphi_{i+1}\\!-\\!\\varphi_{i})$ . Each interval partition on the entries of $\\varphi$ will determine a unique sparse pattern of $\\theta$ . In other words, symmetric linear bandit is strictly harder than sparse bandit and inherits all the computational complexity challenges of sparse linear bandit, including the $N P$ -hardness of computational complexity. ", "page_idx": 23}, {"type": "text", "text": "Inspired by the literature on sparse linear regression, where one can relax solving exact sparse linear regression by using norm-1 minimization, also known as LASSO methods, we ask whether there is a convex relaxation for the case of non-crossing partitions or pattern-avoidance partitions in general. ", "page_idx": 24}, {"type": "text", "text": "D.2 Practical Examples of Partitions with Subexponential-Size ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "General hidden symmetries. Examples of hidden symmetry in reinforcement learning tasks can be found in robotic control [32, 3], where robot is initially designed symmetrical, but part of symmetry is destroyed by mechanical imperfection. Further examples of hidden symmetry can be also found in the literature on multi-agent reinforcement learning with a large number of agents. To avoid the curse of dimensionality, researchers often rely on the assumption of the existence of homogeneous agents [13, 34]. In the extreme case where all agents are homogeneous, such as in mean-field games, sample complexity becomes independent of the number of agents [13]. However, in practice, agents can be clustered into different types [34], and this information may not be known in advance to the learner (here symmetry occurs between different agents from the same type). ", "page_idx": 24}, {"type": "text", "text": "Non-crossing partitions. Sub-exponential size naturally appears when there is a hierarchical structure on the set $[d]$ , and the partitioning needs to respect this hierarchical structure. Particularly, let $T(d,d_{0})$ be the set of ordered trees with $(d+1)$ nodes and $d_{0}$ internal nodes (i.e., nodes that are not the leaves). A partition that respects an ordered tree groups the children of the same node into a single equivalence class (for example, see Figure 1). It is shown in [15] that the cardinality of the set of partitions that respect ordered trees in $T(d,d_{0})$ is sub-exponential. More precisely, it is $O(d^{2d_{0}})$ . Furthermore, there is a natural bijection between partitions that respect ordered trees in $T(d,d_{0})$ and the set of non-crossing partitions $\\mathcal{N C}_{d,d_{0}}$ [15]. ", "page_idx": 24}, {"type": "text", "text": "Recall the subcontractor example in the introduction. Here, after the company hires subcontractors $\\{1,4,6\\}$ to do the job, these subcontractors further break down the tasks into smaller subtasks and hire additional subcontractors $\\{2,3\\}$ , $\\lbrace5\\rbrace$ , and $\\{7,8,9\\}$ , respectively, to execute the subtasks. ", "page_idx": 24}, {"type": "text", "text": "Non-nesting partitions. Besides non-crossing partitions, another sub-exponential-size class of partitions with practical relevance is non-nesting partitions. Consider the resource allocation task where there are $d$ upcoming tasks and $d_{0}$ machines. The job of the designer is to allocate these tasks to each machine. ", "page_idx": 24}, {"type": "text", "text": "Now, assume each task will appear in time $t_{1}<t_{2}<\\cdot\\cdot<t_{d}$ , but the exact time (the value of $t_{i}$ ) is unknown to the designer. Moreover, the cost of machine $k\\in[d_{0}]$ , given a subset of tasks $A_{k}$ (ordered according to execution time), is $c_{k}=t_{\\operatorname*{max}(A_{k})}-t_{\\operatorname*{min}(A_{k})}$ . ", "page_idx": 24}, {"type": "text", "text": "The goal of the designer is to minimize the maximum cost of all machines: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{k\\in[d_{0}]}c_{k}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "To achieve this, the designer should avoid nesting allocations (i.e., searching among non-nesting partitions). In particular, assuming that if tasks at times $t_{i}$ and $t_{j}$ are assigned to machine $k$ , (where $t_{i}<t_{j},$ ), and tasks at times $t_{p}$ and $t_{q}$ are assigned to machine $k^{\\check{\\prime}}$ (where $t_{p}<t_{q},$ ), then it should not be the case that $t_{i}<t_{p}<t_{q}<t_{j}$ . This is because the cost of machine $k$ would be significantly higher than that of machine $k^{\\prime}$ , and the cost could be reduced by swapping task $t_{q}$ for machine $k$ with task $t_{j}$ for machine $k^{\\prime}$ . ", "page_idx": 24}, {"type": "text", "text": "D.3 Efficient Greedy Algorithm for Specific Classes of Partitions ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The model selection procedure in the exploration phase of Algorithms 1 and 2 requires finding the best subspace in the pool $m\\in\\mathcal{M}$ with respect to least square errors. In the worst case, the algorithm needs to solve $M$ linear regression approaches. While the exact computation of the best subspace $\\hat{m}$ as in (4) is an NP-hard problem in general (since it contains interval partition as a subclass), we argue that an greedy algorithm can find the ground-truth subspace $m_{\\star}$ in $O(n d^{5})$ time complexity, given sufficient large number of samples. ", "page_idx": 24}, {"type": "text", "text": "The pseudo code of the greedy algorithm is given in Figure 4. Given that the set of partitions $\\mathcal{Q}_{d}$ is equipped with a lattice structure, in which the finest partition is $(1|2|\\ldots|d)$ and the coarsest partition is $(1,2,\\ldots,d)$ . ", "page_idx": 24}, {"type": "text", "text": "The algorithm starts with the finest partition ${\\hat{\\pi}}=(1|2|\\cdot..\\cdot|d)$ . In each iteration, the algorithm finds the finest coarsening of the current partition $\\hat{\\pi}$ . In graph-theoretic terms, it finds all neighbors of $\\hat{\\pi}$ in the lattice that are coarsenings of $\\hat{\\pi}$ . The operator for finding the finest coarsening of $\\hat{\\pi}$ is denoted as $\\mathtt{C o a r s e n}(\\hat{\\pi})$ , and it returns a collection of the finest coarsened partitions. Next, the algorithm finds the partition that minimizes the prediction error $\\lVert Y-\\Pi_{S_{m}}(Y)\\rVert_{2}^{2}$ among the current coarsening collection. At the end of each iteration, as the number of classes in $\\hat{\\pi}$ is reduced by one, the dimension variable is also reduced by 1. The while loop stops when the dimension equals $d_{0}$ . ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "Since the algorithm only optimizes locally within the current coarsening collection, it exhibits a behavior similar to a greedy algorithm. We note that for non-crossing and non-nesting partitions, the cardinality of the coarsening collection at any level of the lattice is at most $d^{2}$ . Therefore, assuming that creating a finest coarsening partition take $O(d)$ operator, and solving least square takes $O(n d^{2})$ , the algorithms time complexity is $O(n d^{5})$ . ", "page_idx": 25}, {"type": "text", "text": "Algorithm 4 Greedily Search within Lattice ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1: Input: Compact representation of $\\mathcal{M}$ , design matrix $X$ , reward vector $Y$ .   \n2: Initialise $\\hat{\\pi}=(1|2|3|4...|d)$ , dimension $=d$ .   \n3: while dimension $>d_{0}$ do   \n4: Collection = Coarsen(\u03c0\u02c6).   \n5: $\\begin{array}{r}{\\hat{m}=\\arg\\operatorname*{min}_{m\\in{\\bf H}({\\bf\\Psi})}}\\end{array}$ Collection) $\\|Y-\\Pi_{S_{m}}(Y)\\|_{2}^{2}$ .   \n6: $\\hat{\\pi}=\\mathbf{H}^{-1}(\\hat{m})$ .   \n7: dimension $=$ dimension \u22121.   \n8: end while   \n9: $\\begin{array}{r}{\\hat{\\theta}=\\arg\\operatorname*{min}_{\\theta\\in\\hat{m}}\\|Y-X\\theta\\|_{2}^{2}}\\end{array}$ .   \n10: Return m\u02c6, $\\hat{\\theta}$ . ", "page_idx": 25}, {"type": "text", "text": "E Experiment details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We conduct si\u221amulations where the entries of $\\theta_{\\star}$ satisfy non-crossing partition constraints. The set of arms $\\mathcal{X}$ is $\\sqrt{d}\\mathbb{S}^{d-1}$ , $\\sigma\\,=\\,0.1$ , and $(d,d_{0})\\,\\in\\,\\{(40,4),(80,10),(100,15)\\}$ . We let exploratory distribution $\\nu$ be the uniform distribution on the unit sphere. The ground-truth partition $\\pi_{\\mathcal{G}}$ and $\\theta_{\\star}$ are randomized before each simulation. ", "page_idx": 25}, {"type": "text", "text": "To run ESTC-Lasso algorithm [23], we introduce an auxiliary sparse vector $\\varphi$ corresponding to $\\theta_{\\star}$ , whose entries are defined as $\\varphi_{i}=\\theta_{i+1}-\\theta_{i}$ , and $\\varphi_{d}=\\theta_{d}$ . We apply Lasso regression for $\\varphi_{\\star}$ , get the estimate $\\hat{\\varphi}$ , then convert back to $\\hat{\\theta}$ using the map that transforms sparse vector to interval-partition vector (inversion of the map we defined above). ", "page_idx": 25}, {"type": "text", "text": "Regarding implementing our algorithm, we use greedy Algorithm 4 as introduced in Appendix D.3, to solve the optimisation in equation (3), (4), and its complexity is $O(t_{1}d^{5})$ . It is shown in the simulation result (Figure 2, 3, 4), the greedy algorithm achieves small risk error and consequently leads to small regret. Code is available at: ", "page_idx": 25}, {"type": "text", "text": "https://github.com/NamTranKekL/Symmetric-Linear-Bandit-with-Hidden-Symmetry.   \ngit. ", "page_idx": 25}, {"type": "image", "img_path": "aLzA7MSc6Y/tmp/f249684808e589208543d0da2dac856538e22f8999d487adbace1b7d6cfbd525.jpg", "img_caption": ["Figure 3: Regret of EMC (Algorithm 1) and of ESTC proposed in [23], in cases of sparsity, noncrossing partitions, and non-nesting partitions, with $d=40,\\ d_{0}=4$ . "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "aLzA7MSc6Y/tmp/892b1673e310e8a68fc6275520d3b32f3ee1f1bf7da034985087e01a384d2d33.jpg", "img_caption": ["Figure 4: Regret of EMC (Algorithm 1) and of ESTC proposed in [23], in cases of sparsity, noncrossing partitions, and non-nesting partitions, with $d=80$ , $d_{0}=10$ . "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "F Extended Related Work ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Sparse linear bandits. As we will explain in Section 4.2, sparsity is equivalent to a subset symmetry structures, and thus, can be seen as a special case of our setting. As such, we first review the literature of spar\u221asity. Sparse linear bandits were first investigated in [2], where the authors achieve a regret of $\\bar{\\tilde{O}}(\\sqrt{d s T})$ , with $\\tilde{O}$ disregarding the logarithmic factor, and $s$ representing the sparsity level, and $T$ is the time horizon. Without additional assumptions on the arm set and feature distribution, the lower bound for regret in the sparsity case is $\\Omega({\\sqrt{d s T}})$ [27]. Consequently, the contextual setting has recently gained popularity in the sparsity literature, where additional assumptions are made regarding the context distribution an\u221ad set of arms. With this assumption, it can be shown that one can achieve regret of the form $\\tilde{O}(\\tau s\\sqrt{T})$ , where $\\tau$ is a problem-dependent constant that may have a complex form and varies from paper to paper [26, 36]. Apart from the contextual assumption, to avoid polynomial dependence on $T$ in the regret bound, assumptions are required for the set of arms [28, 11, 23]. Recently, [23] offers a unified perspective on the assumption regarding the set of arms by assuming the existence of an exploratory distribution on the set of arms. With this assumption, the authors propose an Explore then Commit style strategy that achieves $\\tilde{O}(s^{{\\frac{2}{3}}}T^{{\\frac{2}{3}}})$ , nearly matching the lower bound $\\Omega(s^{\\frac{2}{3}}T^{\\frac{2}{3}})$ in the poor data regime [24]. As the sparsity structure can be reduced to a subset of the symmetry structure, all the lower bounds for sparse problems apply to (unknown) symmetric problems. ", "page_idx": 26}, {"type": "text", "text": "Model selection. Our problem is also closely related to model selection in linear bandits, as the learner can collect potential candidates for the unknown symmetry. Bandit model selection involves the problem where there is a collection of $M$ base algorithms (with unknown performance guarantees) and a master algorithm, aiming to perform as well as the best base algorithm. The majority of the literature assumes the black-box collection of models $M$ base algorithms and employs a variant of online mirror descent to select the recommendations of the base agent [4, 37, 38]. Due to the black-box nature, the regret guarantee bound depends on $\\ensuremath{\\operatorname{poly}}(M)$ . There is a growing literature on model selection in stochastic linear bandits, where there is a collection of $M$ features, and linear bandits running with these features serve as base algorithms. By exploiting the fact that the data can be shared across all the base algorithms, the dependence of regret in terms of the number of models can be reduced to $\\log(M)$ . In particular, [25] propose a method that concatenates all $M$ features of dimension $d$ into one feature of dimension $M d$ , and then runs a group-Lasso bandit algorithm on top of this concatenated feature space, using the Lasso estimation as a aggregation of models. Their algorithm achieves a regret bound of $O(T^{\\frac{3}{4}}{\\sqrt{\\log(M)}})$ under the assumption that the Euclidean norm of the concatenated feature is bounded by\u221a a constant. Howeve\u221ar, in our case, the Euclidean norm of concatenated feature can be as large as $\\sqrt{M}$ , which leads to $\\sqrt{M}$ multiplicative factor in regret. Besides, [35] uses the online aggregation oracle approach, and able to obtain regret as $O({\\sqrt{K d T\\log(M)}})$ , where $K$ is the number of arms. In contrast, we use different algorithmic mechanism than aggregation of models. In particular, we explicitly exploiting the structure of the model class as a collection of subspaces and invoking results from Gaussian model selection [21] and dimension reduction on the union of subspaces [8]. With this technique, we are able to achieve $O(T^{\\frac{2}{3}}\\log(M))$ , which is rate-optimal in the data-poor regime, has logarithmic dependence on $M$ without strong assumptions on the norm of concatenated features, and is independent of the number of arms $K$ . A special case of feature selection where one can achieve a very tight regret compared to the best model is the nested feature class [18, 20]. In particular, in the nested feature class where dimensions range from $\\{1,\\ldots,d\\}$ , and $d_{m_{\\star}}<d$ represents the realizable feature of the smallest dimension, the regret bound can be $\\tilde{O}(\\sqrt{T d_{m_{\\star}}})$ as shown in [20]. While the regret bound nearly matches the regret of the best model in the nested feature class, the assumption on nested features cannot be applied in our setting. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "Symmetry in online learning. The notion of symmetry in Markov Decision Making dates back to works such as [22, 40]. Generally, the reward function and probability transition are preserved under an action of a group on the state-action space. Exploiting known symmetry has been shown to help achieve better performance empirically [43, 42] or tighter regret bounds theoretically [41]. However, all these works requires knowledge of symmetry group, while setting consider unknown subgroup which may be considerably harder. Unknown symmetry on the context or state space has been studied by few authors, with the term context-lumpable bandit [29], meaning that the set of contexts can be partitioned into classes of similar contexts. It is important to note that the symmetry group acts differently on the context space and the action space. As we shall explain in detail in Section 3, while one can achieve a reduction in terms of regret in the case of unknown symmetry acting on context spaces [29], this is not the case when the symmetry group acts on the action space. Particularly, we show that without any extra information on the partition structure of similar classes of arms, no algorithm can achieve any reduction in terms of regret. ", "page_idx": 27}, {"type": "text", "text": "The work closest to ours is [39], where the authors consider the setting of a $K$ -armed bandit, where the set of arms can be partitioned into groups with similar mean rewards, such that each group has at least $q>2$ arms. With the constrained partition, the instance-dependent regret bounds are shown asymptotically to be of order $O\\,((K/q)\\,\\bar{\\log}\\,T)$ . Comparing to [39], we study the setting of stochastic linear bandits with similar arms, in which the (unknown) symmetry and linearity structure may intertwine, making the problem more sophisticated. We also impose different constraints on the way one partitions the set of arms, which is more natural in the setting of linear bandits with infinite arms. As a result, we argue that the technique used in [39] cannot be applied in our setting. To clarify this, let us first review the algorithmic technique from [39]: The algorithm in [39] assumes there is an equivalence among the parameters $\\theta$ , and that the set of arms $\\mathcal{X}$ is a simplex. At each round $t$ , given an estimation $\\widehat{\\theta}_{t}$ , the algorithm maintains a sorted list of indices in $[d]$ that follows the ascending order of the magnitude of $\\widehat{\\theta}_{i}$ . The algorithm then uses the sorted list $({\\hat{\\theta}}_{i})_{i\\in[d]}$ to choose arm $x$ accordingly. The key assumption here is that, since the set of arms $\\mathcal{X}$ is a simplex, we can estimate each $\\theta_{i}$ independently. This implies that the list $({\\hat{\\theta}}_{i})_{i\\in[d]}$ should respect the true order of the list $(\\theta_{i})_{i\\in[d]}$ when there are a sufficiently large number of samples. Unfortunately, this is typically not the case in linear bandits where $\\mathcal{X}$ has a more general shape. In linear bandits, there can be correlations between the estimates $\\widehat{\\theta}_{i}$ and ${\\hat{\\theta}}_{j}$ for any $i,j\\in[d]$ . Hence, one should not expect that $({\\hat{\\theta}}_{i})_{i\\in[d]}$ will maintain the same order as $(\\theta_{i})_{i\\in[d]}$ . In other words, the correlations among the estimates $\\{\\hat{\\theta}_{1},\\dotsc,\\hat{\\theta}_{d}\\}$ may destroy the original order in $\\{\\theta_{1},\\ldots,\\theta_{d}\\}$ . In fact, we can only guarantee the risk error of estimation $\\theta$ , i.e., $\\lVert\\hat{\\theta}-\\theta_{\\star}\\rVert$ is small, but not necessarily the order of the indices in $\\theta$ . Therefore, the technique used in [39] cannot be directly applied to linear bandits in its current form. ", "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Our claims made in the abstract and introduction reflect the paper\u2019s contributions and scope. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We discussed the limitations of the work in the main paper and the conclusion. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We stated the assumptions in the main paper and provide the proof in the appendix. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We described our simulations and provided a link to the code available online. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We provided a link to the code available online. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We described our simulations and provided a link to the code available online. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 30}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We described our simulations and statistical significance of the experiments. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We discussed the time complexity of our algorithm. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 31}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS Code of Ethics. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: This paper is primarily theoretical and may have limited direct impact or implications. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: This paper poses no such risks. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: We does not use existing assets. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: This paper does not release new assets ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 33}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}]