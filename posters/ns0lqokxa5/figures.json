[{"figure_path": "Ns0LQokxa5/figures/figures_0_1.jpg", "caption": "Figure 1: Our method, GaussianCut, enables interactive object(s) selection. Given an optimized 3D Gaussian Splatting model for a scene with user inputs (clicks, scribbles, or text) on any viewpoint, GaussianCut partitions the set of Gaussians as foreground and background.", "description": "This figure shows the overall pipeline of the GaussianCut method.  Starting with multiview images of a scene, a pretrained 3D Gaussian Splatting (3DGS) model is used to represent the scene as a set of 3D Gaussians. The user provides input in the form of sparse point clicks, scribbles, or a text prompt on a single viewpoint. GaussianCut then uses this input to partition the set of Gaussians into foreground and background, effectively segmenting the object(s) of interest.  The figure visually depicts this process, showing how the user input leads to the separation of foreground and background Gaussians in the 3D model.", "section": "Abstract"}, {"figure_path": "Ns0LQokxa5/figures/figures_3_1.jpg", "caption": "Figure 2: Overall pipeline of GaussianCut. User input from any viewpoint is passed to a video segmentation model to produce multi-view masks. We rasterize every view and track the contribution of each Gaussian to masked and unmasked pixels. Then, Gaussians are formulated as nodes in an undirected graph and we adapt graph cut to partition the graph. The red edges in the graph highlight the set of edges graph cut removes for partitioning the graph.", "description": "This figure illustrates the overall pipeline of the GaussianCut method.  It starts with user input (clicks, scribbles, or text) on a single viewpoint image. This input is fed into a video segmentation model to generate dense segmentation masks across multiple views. Then, these masks are used with a rasterization step to determine the contribution of each 3D Gaussian in the optimized 3D Gaussian Splatting (3DGS) scene representation to the foreground (masked pixels) and background (unmasked pixels).  This results in a weighted graph where each node is a Gaussian and weights represent the foreground/background likelihoods.  Finally, a graph cut algorithm partitions the graph into foreground and background Gaussian sets, enabling separation of objects of interest from the scene.", "section": "3 Method"}, {"figure_path": "Ns0LQokxa5/figures/figures_5_1.jpg", "caption": "Figure 2: Overall pipeline of GaussianCut. User input from any viewpoint is passed to a video segmentation model to produce multi-view masks. We rasterize every view and track the contribution of each Gaussian to masked and unmasked pixels. Then, Gaussians are formulated as nodes in an undirected graph and we adapt graph cut to partition the graph. The red edges in the graph highlight the set of edges graph cut removes for partitioning the graph.", "description": "This figure illustrates the overall pipeline of the GaussianCut method.  It shows how user input (from any viewpoint) is processed through a video segmentation model to generate multi-view masks. These masks are then used to track each Gaussian's contribution to masked and unmasked pixels in the scene.  The Gaussians are then represented as nodes in a graph, and a graph cut algorithm is applied to partition the graph into foreground and background Gaussians based on user input and scene properties.", "section": "3.2 Overview"}, {"figure_path": "Ns0LQokxa5/figures/figures_8_1.jpg", "caption": "Figure 4: Qualitative comparison: 3D segmentation results of GaussianCut using text on 360-garden [1] scene. Compared to ISRF [17], SA3D [7], SAGD [21], GaussianCut segment contain finer details. The graph cut component of GaussianCut also retrieves fine details (like decorations on the plant) that are missed in coarse splatting.", "description": "This figure compares the 3D segmentation results obtained from GaussianCut and three other state-of-the-art methods (ISRF, SA3D, SAGD) when using a textual prompt.  The image shows that GaussianCut produces a segmentation with finer details and a more accurate representation of the plant's features compared to the other methods, especially when considering the details captured in the graph cut component, such as the plant decorations.  Coarse splatting, a simpler technique, is also included for further comparison and shows a less accurate result.", "section": "5.2 Qualitative results"}, {"figure_path": "Ns0LQokxa5/figures/figures_13_1.jpg", "caption": "Figure 5: Limitation of LangSplat on Trex and Leaves scenes from NVOS benchmark. Parts of the trex can not be extracted in the top row. In the bottom row, background leaves are also selected along with front leaf.", "description": "This figure shows limitations of LangSplat method in segmenting objects from two scenes in the NVOS benchmark dataset.  The top row demonstrates the failure to extract parts of a T-Rex model, while the bottom row highlights the inclusion of background leaves along with the intended foreground leaves. This showcases the challenges faced by LangSplat in achieving precise object segmentation, particularly when dealing with complex scenes containing intricate objects or overlapping elements.", "section": "A.1 Baseline implementation details"}, {"figure_path": "Ns0LQokxa5/figures/figures_14_1.jpg", "caption": "Figure 7: SAM-Track fails to capture major sections of the bicycle when its orientation significantly deviates from the initial position. Even in the reference image, the segmentation mask omits finer details such as the bicycle wheel rims, pedals, and bottle holder. GaussianCut improves segmentation by eliminating substantial portions of the bench to isolate the bicycle, and it partially restores the visibility of the wheel rims. Despite these improvements, the segmentation remains imprecise.", "description": "This figure shows a comparison of segmentation results between SAM-Track and GaussianCut on a bicycle scene.  SAM-Track struggles to accurately capture the bicycle when its orientation changes, missing details such as the wheels and pedals.  GaussianCut, while improving upon SAM-Track's output, still struggles with some aspects of accurate segmentation.  It highlights how the accuracy of the method relies heavily on the quality of the input 2D video segmentation.", "section": "Limitations"}, {"figure_path": "Ns0LQokxa5/figures/figures_14_2.jpg", "caption": "Figure 7: SAM-Track fails to capture major sections of the bicycle when its orientation significantly deviates from the initial position. Even in the reference image, the segmentation mask omits finer details such as the bicycle wheel rims, pedals, and bottle holder. GaussianCut improves segmentation by eliminating substantial portions of the bench to isolate the bicycle, and it partially restores the visibility of the wheel rims. Despite these improvements, the segmentation remains imprecise.", "description": "This figure demonstrates a limitation of the SAM-Track model in handling significant changes in object orientation. The video segmentation model fails to accurately capture the entire bicycle in several views, missing key details like wheel rims. GaussianCut enhances the segmentation by removing parts of the background, thus isolating the bicycle more effectively, though some imprecision remains. This highlights the challenges in using video segmentation for accurate 3D object extraction.", "section": "Limitations"}, {"figure_path": "Ns0LQokxa5/figures/figures_15_1.jpg", "caption": "Figure 8: GaussianCut precisely retrieves fine details, such as the mirrors on the front of the truck, even in instances where video-segmentation model struggles to maintain consistency across different views in the scene.", "description": "This figure shows how GaussianCut handles inconsistencies in video segmentation masks.  Even when the video segmentation model produces inaccurate or incomplete masks across multiple views (due to challenges like object pose changes), GaussianCut can still accurately extract the target object with fine details preserved, demonstrating its robustness to noisy inputs.", "section": "5.2 Qualitative results"}, {"figure_path": "Ns0LQokxa5/figures/figures_15_2.jpg", "caption": "Figure 9: Visualization of selected objects on the Mip-NeRF and LERF dataset. Initial object selection, based on point clicks, and the reference image is shown on the left.", "description": "This figure shows examples of object selection using GaussianCut on different scenes from the Mip-NeRF and LERF datasets.  For each scene, the leftmost image shows the reference image with the initial selection (point clicks) overlaid. The remaining images in each row display the segmented object rendered from various viewpoints, showcasing the accuracy and consistency of GaussianCut's 3D segmentation across different perspectives. This demonstrates the effectiveness of the method in isolating objects from complex scenes.", "section": "5.2 Qualitative results"}, {"figure_path": "Ns0LQokxa5/figures/figures_16_1.jpg", "caption": "Figure 10: Qualitative results on the Shiny dataset, compared against SA3D [7]. The points used as user inputs are highlighted in the reference image.", "description": "This figure compares the qualitative results of object segmentation on the Shiny dataset between the proposed GaussianCut method and the SA3D method.  It shows four scenes from the Shiny dataset with the reference image showing the user input points used for object selection.  The results demonstrate that GaussianCut achieves higher accuracy and better rendering of the segmented objects than SA3D, particularly in capturing finer details and handling complex object shapes in cluttered environments.", "section": "D.1 Shiny dataset"}, {"figure_path": "Ns0LQokxa5/figures/figures_17_1.jpg", "caption": "Figure 11: Qualitative comparison of segmentation masks obtained from GaussianCut and the ground-truth used in SPIn-NeRF dataset.", "description": "This figure presents a qualitative comparison of segmentation masks generated by the proposed GaussianCut method and the ground truth masks from the SPIn-NeRF dataset.  It visually demonstrates the performance of GaussianCut on several scenes, including those with trucks, pinecones, orchids, and Lego constructions. Each row showcases an example scene: (a) shows the original image; (b) displays the ground-truth segmentation mask; (c) presents the segmentation mask generated by GaussianCut; and (d) shows a rendering of the segmented object from the GaussianCut output against a black background. This visual comparison allows for an assessment of the accuracy and quality of the segmentation achieved by the GaussianCut method compared to the ground truth.", "section": "D.2 SPIn-NeRF"}, {"figure_path": "Ns0LQokxa5/figures/figures_19_1.jpg", "caption": "Figure 12: Visualization of overall segmentation masks from SAM and GaussianCut.", "description": "This figure shows a comparison of the segmentation masks produced by SAM (Segment Anything) and GaussianCut for three different scenes: Garden, Bonsai, and Truck.  For each scene, the RGB image, the SAM mask, and the GaussianCut mask are displayed.  The comparison highlights how GaussianCut refines the segmentation provided by SAM, particularly in terms of precision and detail.", "section": "5.2 Qualitative results"}, {"figure_path": "Ns0LQokxa5/figures/figures_21_1.jpg", "caption": "Figure 13: We compare coarse splatting (w/o graph cut) and GaussianCut. Scribbles refer to using direct input, single mask refers to taking the mask from one viewpoint, and multi-view masks refer to using video segmentation. The effectiveness of GaussianCut becomes more prominent when the inputs are sparse.", "description": "This figure compares the results of three different segmentation methods: coarse splatting, GaussianCut with a single mask, and GaussianCut with multiple masks.  The top row shows the input image, the coarse splatting result (without graph cut), and GaussianCut results using different input types (scribbles, single mask, and multi-view masks). The bottom row shows the ground truth segmentation for comparison. The figure highlights how GaussianCut's performance improves significantly with the increased input information from multiple views, especially when compared to using only scribbles or a single mask.", "section": "5.2 Qualitative results"}]