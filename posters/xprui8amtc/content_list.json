[{"type": "text", "text": "Scene Graph Generation with Role-Playing Large Language Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Guikun Chen1\u2217, Jin $\\mathbf{Li}^{3*}$ , Wenguan Wang1,2\u2020 ", "page_idx": 0}, {"type": "text", "text": "1Zhejiang University 2National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, Xi\u2019an Jiaotong University 3Changsha University of Science & Technology ", "page_idx": 0}, {"type": "text", "text": "https://github.com/guikunchen/SDSGG ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Current approaches for open-vocabulary scene graph generation (OVSGG) use vision-language models such as CLIP and follow a standard zero-shot pipeline \u2013 computing similarity between the query image and the text embeddings for each category (i.e., text classifiers). In this work, we argue that the text classifiers adopted by existing OVSGG methods, i.e., category-/part-level prompts, are sceneagnostic as they remain unchanged across contexts. Using such fixed text classifiers not only struggles to model visual relations with high variance, but also falls short in adapting to distinct contexts. To plug these intrinsic shortcomings, we devise SDSGG, a scene-specific description based OVSGG framework where the weights of text classifiers are adaptively adjusted according to the visual content. In particular, to generate comprehensive and diverse descriptions oriented to the scene, an LLM is asked to play different roles (e.g., biologist and engineer) to analyze and discuss the descriptive features of a given scene from different views. Unlike previous efforts simply treating the generated descriptions as mutually equivalent text classifiers, SDSGG is equipped with an advanced renormalization mechanism to adjust the influence of each text classifier based on its relevance to the presented scene (this is what the term \u201cspecific\u201d means). Furthermore, to capture the complicated interplay between subjects and objects, we propose a new lightweight module called mutual visual adapter. It refines CLIP\u2019s ability to recognize relations by learning an interaction-aware semantic space. Extensive experiments on prevalent benchmarks show that SDSGG outperforms top-leading methods by a clear margin. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "SGG [1] aims to create a structured representation of an image by identifying objects as nodes and their relations as edges within a graph. The emerging field of OVSGG [2, 3], which broadens the scope of SGG to identify and associate objects beyond a predefined set of categories, has become a research hotspot for its prospective to amplify the practicality of SGG in diverse real-world applications. ", "page_idx": 0}, {"type": "text", "text": "OVSGG has achieved notable progress due to the success of vision-language models (VLMs) [4, 5] and prompt learning [6, 7]. Existing OVSGG methods adopt a standard zero-shot pipeline [4], which computes similarity between the visual embedding from query image and the text embedding from pre-defined text classifiers $\\chi_{\\cdot}f$ . Fig. 1a). One straightforward direction for OVSGG is to use only the category name (e.g., \u201criding\u201d) [2, 8, 9, 3, 10] as the text classifier and perform vision-language alignment as in prompt learning to learn the underlying patterns. On the other hand, [11, 12] argue that such methods fail to utilize the rich context of additional information that language can provide. To address this, [12] decomposes relation detection into several separate components, computing similarities by checking how well the visual features of the object match the part-level descriptions1. As shown in Fig. 1b, the object of relation \u201criding\u201d should have four legs, a saddle, etc. ", "page_idx": 0}, {"type": "image", "img_path": "xpRUi8amtC/tmp/11637f49d2ea6ffdb69b9d317fea57809d6fe1ce64a4aded0438f60ce3f0ce7c.jpg", "img_caption": ["Figure 1: Illustration of the used text classifiers in OVSGG. (a) CLIP performs zero-shot classification by computing similarity between the query image and the text embeddings for each category, then choosing the highest. (b) To further utilize the learned semantic space of CLIP, one can compute similarities of multiple part-level prompts (e.g., the object of \u27e8man, riding, horse\u27e9may be described with \u201cwith four legs\u201d and \u201cwith a saddle\u201d). (c) Instead of using these scene-agnostic text classifiers, SDSGG adopts comprehensive, scene-specific descriptions generated by LLMs, which can adapt to specific contexts by using the proposed renormalization. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Despite these technological advances, we still observe current OVSGG systems lack in-depth inspection of the expressive range of the used text classifiers, which puts a performance cap on them. Concretely, OVSGG models that rely on scene2-agnostic text classifiers have the following flaws. First, methods based on category names [2, 3] struggle to model the large variance in visual relations. Using only category names as classifiers [4] does hold water when applied to object recognition [13]. For instance, CLIP shares common visual features across diverse image-text pairs of tigers which encompasses a variety of tiger appearances and corresponding descriptions. Nonetheless, the scenario becomes far more complex when it comes to relation detection. The visual features that define the relation \u201con\u201d can vary dramatically across scenes, e.g., \u201cdog on bed\u201d vs. \u201cpeople on road\u201d. RECODE [12] proposes to decompose relation detection into recognizing part-level descriptions for both subject and object, hence partially easing the aforementioned difficulty. Yet, it computes similarities for the subject and object separately and does not model the interplay between subjects and objects. Second, part-level prompt based methods uniformly process all descriptions as affirmative classifiers [11, 12], overlooking the possibility that some text classifiers might be contrary to specific contexts. When querying LLMs for distinctive visual features of subjects and objects to distinguish the predicate \u201criding\u201d, with the subject as \u201chuman\u201d and the object as \u201chorse\u201d, LLMs provide part-level descriptions of the expected appearance of both entities. All generated descriptions are treated equally as definitive text classifiers. However, these descriptions could potentially be misleading, as LLMs produce them without considering the specific context, even resulting in some descriptions that are wholly irrelevant to the presented image. For example, LLMs typically associate the predicate \u201criding\u201d with the animal \u201cwith four legs\u201d. Nonetheless, such associations are indeed irrelevant, as an animal\u2019s legs are not always visible in the presented image, or the animal may have only two legs. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Filling the gaps identified above calls for a fundamental paradigm shift: moving from the fixed, scene-agnostic (i.e., category-/part-level) text classifiers towards flexible, scene-specific ones. In light of this, we develop SDSGG, a scene-specific description based OVSGG framework that utilizes text classifiers generated from LLMs, complemented by a renormalization technique, to understand scenes from different perspectives. For the textual part, given a scene with specified content, an LLM is assigned distinct roles, akin to experts specializing in biology, physics, and engineering, to analyze descriptive scene features comprehensively $(.f)$ . Fig. 1c). Such a multi-persona scheme is designed to improve the diversity of the generated scene descriptions as LLMs tend to generate repetitive content. LLM can be queried multiple times to obtain a large number of scene descriptions. Moreover, since not all descriptions are relevant to the presented image (e.g., some parts of the object may not appear), SDSGG is equipped with an advanced mechanism that renormalizes each scene description via opposite descriptions corresponding to the original descriptions. This involves evaluating two vision-language similarities: one for the original scene description and the other for its opposite. The difference between the two similarities is viewed as the self-normalized similarity of the scene description, allowing for flexible control over its influence. For instance, an irrelevant description would yield a self-normalized similarity close to zero, as the two similarity scores of it and its opposite would be very close. By doing so, the generated scene-level descriptions become flexible, scene-specific descriptions (SSDs). For the visual part, we propose a new adapter for relation detection, called mutual visual adapter, which consists of several lightweight learnable modules. The proposed adapter projects CLIP\u2019s semantic embeddings into another interaction-aware space, modeling the complicated interplay between the subject and object through cross-attention. ", "page_idx": 2}, {"type": "text", "text": "With the proposed adaptive SSDs, our SDSGG is capable of: i) adapting to the given context via evaluating the self-normalized similarity of each SSD; ii) alleviating the overfitting problem in OVSGG models [2, 3] that use only one classifier; and iii) naturally generalizing to novel relations by associating them with SSDs. We validate SDSGG on two widely-used benchmarks, i.e. Visual Genome (VG) [14] and GQA [15]. Experimental results show that SDSGG outperforms existing OVSGG methods [3] by a large margin. The strong generalization and promising performance of SDSGG evidence the great potential of scene-specific description based relation detection. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Scene Graph Generation. Since [1] introduces iterative message passing for SGG, research studies in structured visual scene understanding have witnessed phenomenal growth. Tremendous progress has been achieved and can be categorized into: i) Two-stage SGG [16, 17, 18, 19, 20], which first detects all objects in the images and then recognizes the pairwise relations between them; ii) Debiased SGG [21, 22, 23, 24, 25, 26, 27], which focuses on the problem of long-tailed predicate distribution in the current dataset; iii) Weakly-supervised SGG [28, 29, 30, 31, 32], which investigates how to generate scene graph with only image-level supervision; iv) One-stage SGG [33, 34, 35, 36, 37, 38], which implements SGG within an end-to-end framework (also exemplified in other relation detection tasks [39, 40, 41]), discarding several hand-crafted procedures; v) Open-vocabulary SGG, which learns to recognize unseen categories during training by using category-level [2, 8, 9, 3, 10] or part-level prompts [12]. ", "page_idx": 2}, {"type": "text", "text": "Existing OVSGG frameworks adopt a standard open-vocabulary learning paradigm, i.e., perform vision-language alignment in the pre-trained or random initialized semantic space with supervision of only the category names. One except [12] reformulates OVSGG from recognizing category-level prompts into recognizing part-level prompts, by decomposing SGG into several separate components and computing their similarities independently, in a training-free manner. SDSGG represents the best of both worlds. On the one hand, we point out the drawbacks of the commonly used scene-agnostic text classifiers and introduce scene-specific alternates to understand scenes from different perspectives. On the other hand, SDSGG incorporates a learnable mutual visual adapter to capture the underlying patterns in the dataset and proposes to renormalize text classifiers for adapting to specific contexts. ", "page_idx": 2}, {"type": "text", "text": "Open-vocabulary Learning. Most deep neural networks operate on the close-set assumption, which can only identify pre-defined categories that are present in the training set. Early zero-shot learning approaches [42, 43, 44] adopt word embedding projection to constitute the classifiers for unseen class classification. With the rapid progress of vision language pre-training [4, 5, 45], open vocabulary learning [46] has been proposed and demonstrates impressive capabilities by, for example, distilling knowledge from VLMs [47, 48, 49, 50, 51, 52], exploiting caption data [53, 54], generating pseudo labels [55, 56, 57, 58, 59], training without dense labels [60, 61, 62], joint learning of several tasks [63, 64, 65], and training with more balanced data [66, 67]. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "While sharing a very high-level idea of vision-language alignment in open-vocabulary methods, our SDSGG i) explicitly models the context-dependent scenarios and introduces scene-specific text classifiers as the flexible learning targets, and ii) incoroperates a new mechanism for computing self-normalized similarities, thereby renormalizing text classifiers according to the presented image. ", "page_idx": 3}, {"type": "text", "text": "VLMs Meet LLMs [68]. The big win for VLMs has been all about getting the model to match up pictures and their descriptions closely while keeping the mismatched ones apart [4, 5, 45]. This trick, inspired by contrastive learning from self-supervised learning [69, 70, 71, 72, 73], helps VLMs get really good at figuring out what text goes with what image. Moreover, prompt learning acts as a flexible way to communicate with VLMs, giving them a nudge or context to apply their knowledge of images and text in specific ways [6, 74, 75, 7, 76]. In addition to hand-crafted or learnable prompts, [11] offers a fresh perspective, i.e., using LLMs to generate detailed, comprehensive prompts as the inputs of VLMs\u2019 text encoder. Many follow-up works [77, 78, 79, 80, 81, 82] across various domains and tasks demonstrate the effectiveness of integrating VLMs and LLMs. ", "page_idx": 3}, {"type": "text", "text": "Category-/part-level prompts are scene-agnostic and cannot adapt to specific contexts. To this end, SDSGG adopts scene-specific descriptions, generated by LLMs in a multi-persona collaboration fashion, as the inputs of CLIP\u2019s text encoder. Different from part-level prompt based approaches [11, 12] which processes all part-level prompts as affirmative classifiers, SDSGG provides a flexible alternative via the association between classifiers (i.e., SSDs) and categories, and the renormalizing strategy w.r.t. each SSD. Since the learned semantic space of VLMs may not be sensitive to relations [12], we design a lightweight mutual visual adapter to project them into interaction-aware space for capturing the complicated interplay between the subject and object. ", "page_idx": 3}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Task Setup and Notations. Given an image $\\boldsymbol{\\mathit{I}}$ , SGG transforms it into a structured representation, i.e., a directed graph $\\mathcal{G}=\\{\\mathcal{O},\\mathcal{R}\\}$ , where $\\scriptscriptstyle\\mathcal{O}$ represents localized (i.e., bounding box) objects with object category information and $\\mathcal{R}$ represents pairwise relations between objects. For a fair comparison, this work focuses on predicting $\\mathcal{R}$ given $\\scriptscriptstyle\\mathcal{O}$ , i.e., the predicate classification task which avoids the noise from object detection, as suggested by [1, 3, 12]. Our study delves into the intricacies of transitioning SGG from a traditional closed-set setting to an open vocabulary paradigm. This transition enables the system to recognize previously unseen predicate categories (i.e., novel split) by learning from observed predicate categories (i.e., base split) during training. ", "page_idx": 3}, {"type": "text", "text": "SDSGG follows the standard zero-shot pipeline with VLMs [4], which computes similarity between the visual embedding $\\pmb{v}$ and the text embedding $\\pmb{t}$ for each category, and the category with highest similarity is viewed as the final classification result ( ${\\boldsymbol{c}}.{\\boldsymbol{f}}$ . Fig. 2a). For each subject-object pair, $\\pmb{v}$ can be derived by feeding cropped patches from the input image $\\boldsymbol{\\mathit{I}}$ into the visual encoder. The text embedding $\\pmb{t}$ used in existing OVSGG frameworks falls into two main settings: i) Each category consists of only one text classifier, i.e., the category name itself. ii) Each category consists of multiple text classifiers w.r.t. subject and object, i.e., part-level descriptions. SDSGG reformulates the text classifiers as scene-specific descriptions which will be detailed in $\\S3.1$ . ", "page_idx": 3}, {"type": "text", "text": "Algorithmic Overview. SDSGG is a SSD based framework for OVSGG, supported by the cooperation of VLMs and LLMs. For the textual part $({}_{c.f})$ . Fig. 2b), SDSSG enjoys the expressive range of the comprehensive SSDs generated by LLMs\u2019 multi-persona collaboration. This is complemented by a renormalizing mechanism to adjust the influence of each text classifier. For the visual part $(c.f$ . Fig. 2c), SDSSG is equipped with a mutual visual adapter to aggregate visual features $\\pmb{v}$ from $\\boldsymbol{\\mathit{I}}$ for a given subject-object pair. After introducing how we generate and use SSDs for the text part (\u00a73.1) and the mutual visual adapter for interplay modeling of the subject and object (\u00a73.2), we will elaborate on SDSGG\u2019s training objective (\u00a73.3). ", "page_idx": 3}, {"type": "image", "img_path": "xpRUi8amtC/tmp/432464745b0378e85eb941720b14d36e41d8c65113ede205afb3d9d9a528ff3a.jpg", "img_caption": ["Figure 2: (a) Overview of SDSGG. (b) Each text classifier of SDSGG contains a raw description $\\pmb{d}_{a}^{n}$ and an opposite description $\\pmb{d}_{p}^{n}$ . As such, the self-normalized similarities can be computed with the association $(C_{r}^{n})$ between predicate categories and SSDs. (c) Given the visual features (i $.e.,f_{s}^{i m g},f_{o}^{c l s}$ , and $\\pmb{f}_{o}^{i m g}$ ) of both the subject and object extracted from CLIP\u2019s visual encoder, our mutual visual adapter (MVA) projects them into interaction-aware space and models their complicated interplay with cross-attention. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "3.1 Scene-specific Text Classifiers ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Scene-level Description Generation via Multi-persona Collaboration. Using standard prompts to query LLMs is a direct way to generate scene descriptions. For instance, one could straightforwardly prompt LLM with a question like \u201cImagine there is an animal that is eating, what should the scene look like?\u201d. LLM\u2019s response would typically sketch out an envisioned scene based on its statistical training on large corpus. However, these responses may not fully capture the scene\u2019s complexity, often overlooking aspects such as the spatial arrangement of elements and the background environment. ", "page_idx": 4}, {"type": "text", "text": "To alleviate this, we draw inspiration from recent advances in LLMs\u2019 multi-persona capabilities [83, 84, 85, 86]. Specifically, LLM adopts three distinct roles, mirroring the expertise found in experts specializing in biology, physics, and engineering. This approach allows for a comprehensive discussion of what a given scene entails. Because each query to LLM usually only yields 3-5 sentences of description, we query LLM several times, each time giving LLM a different scene content to be discussed, thus obtaining a large number of scene descriptions. Since these initial descriptions may suffer from noise and semantic overlap, we ask LLM to streamline and combine these descriptions, ensuring more cohesive and distinct scene-level descriptions $\\mathcal{D}_{l}=\\{d^{1},d^{2},\\cdot\\cdot\\cdot,d^{N}\\}$ and corresponding text embeddings $\\mathcal{T}=\\{t^{1},t^{2},\\cdot\\cdot\\cdot,t^{N}\\}$ , where $N$ denotes the number of SSDs and text embeddings $\\tau$ are extracted by the text encoder of CLIP. Due to the limited space, we provide more details and prompts for generating scene descriptions in the appendix $\\langle\\S D)$ . ", "page_idx": 4}, {"type": "text", "text": "Association between Scene-level Descriptions and Relation Categories. So far, we have obtained various scene-level descriptions that have the ability to represent diverse scenes. A critical inquiry arises regarding their utility for relation detection, given their lack of explicit association with specific relation categories. To address this, we delineate three distinct scenarios characterizing the interplay between relation categories and scene descriptions: i) certain coexistence ( $C_{r}^{n}=1)$ ), where a direct correlation exists; ii) possible coexistence $C_{r}^{n}=0)$ ), indicating a potential but not guaranteed association; and iii) contradiction $(C_{r}^{n}\\!=\\!-1)$ ), denoting an incompatibility between the scene description and relation category. Here $C_{r}^{n}$ denotes the correlation between relation $r\\in\\mathcal{R}$ and $n_{t h}$ scene description, and is generated by LLMs (prompts are shown in $\\S D$ ). Such a categorization enables us to calculate the similarity for each relation category: ", "page_idx": 4}, {"type": "equation", "text": "$$\ns i m(\\boldsymbol{v},\\boldsymbol{r})=\\sum_{n=1}^{N}C_{r}^{n}*\\langle\\boldsymbol{v},t^{n}\\rangle,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\langle\\cdot,\\cdot\\rangle$ denotes the cosine similarity with temperature [4]. ", "page_idx": 4}, {"type": "text", "text": "Scene-specific Descriptions $=$ Scene-level Descriptions $^{+}$ Reweighing. Upon examining text classifiers in depth, we noticed that certain classifiers are contextually bound, e.g., \u201ctwo or more objects partially overlap each other\u201d may not exist in all scenes. This observation underscores the necessity for a mechanism to evaluate the significance of each text classifier, rather than applying a uniform weight across the board. This is exactly what the term \u201cspecific\u201d means. Recall that the similarity measurement between an image and the text \u201ca photo of a cat\u201d alone yields limited insight. However, when juxtaposed with multiple texts, such as \u201ca photo of a cat/dog/tiger\u201d, the comparison of similarity scores across these categories reveals which category (cat, dog, or tiger) the image most closely resembles. Inspired by this, we propose the incorporation of an opposite description $d_{p}^{n}$ $\\mathrm{\\bf\\left\\{\\hat{S}D\\right\\}}$ for each raw scene description $d_{a}^{n}$ as a reference point (e.g., \u201ctwo or more objects partially overlap each other\u201d vs. \u201ceach object is completely separate with clear space between them\u201d), resulting in SSDs $\\mathcal{D}_{s}=\\{(d_{a}^{1},d_{p}^{1}),(d_{a}^{2},\\stackrel{\\triangledown}{d}_{p}^{2}),\\cdot\\cdot\\cdot\\mathbf{\\Pi},(\\stackrel{\\triangledown}{d}_{a}^{N},\\stackrel{\\triangledown}{d}_{p}^{N})\\}$ and updated text embeddings $\\mathcal{T}=\\{(t_{a}^{1},t_{p}^{1}),(t_{a}^{2},t_{p}^{2}),\\dots,(t_{a}^{N},t_{p}^{N})\\}$ . Subsequently, the self-normalized similarity is defined as: ", "page_idx": 5}, {"type": "equation", "text": "$$\ns i m(\\boldsymbol{v},\\boldsymbol{r})=\\sum_{n=1}^{N}C_{r}^{n}*\\big(\\langle\\boldsymbol{v},\\boldsymbol{t}_{a}^{n}\\rangle-\\langle\\boldsymbol{v},\\boldsymbol{t}_{p}^{n}\\rangle\\big).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The difference in similarity scores, i.e., $\\langle\\pmb{v},\\pmb{t}_{a}^{n}\\rangle-\\langle\\pmb{v},\\pmb{t}_{p}^{n}\\rangle$ , quantifies the relative contribution of that SSD. By such means, a SSD irrelevant to the presented context will have a minimal effect, as the similarity scores of it $(\\langle{\\pmb v},{\\pmb t}_{a}^{n}\\rangle)$ and its opposite $(\\langle{\\pmb v},{\\pmb t}_{p}^{n}\\rangle)$ would be nearly identical. ", "page_idx": 5}, {"type": "text", "text": "3.2 Mutual Visual Adapter ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "After introducing how to obtain the text embeddings and how to compute vision-language similarity, one question remains at this point: how to obtain visual embeddings? When given a subject-object pair with bounding boxes from $\\scriptscriptstyle\\mathcal{O}$ , there exist various strategies for aggregating visual features for the subject and object within $\\boldsymbol{\\mathit{I}}$ . For example, traditional closed-set SGG frameworks [16, 17] employ RoI pooling to extract visual features for specified bounding boxes, subsequently fusing these features for further classification. In contrast, the more recent OVSGG framework [12] uses the visual encoder of CLIP to extract visual embeddings of both subject and object. Then, it processes two visual embeddings independently through part-level descriptions. Such an independent approach, however, overlooks the informative interplay between the subject and object. ", "page_idx": 5}, {"type": "text", "text": "To address this oversight and capture the complicated interactions between subject and object, we introduce a new component: the mutual visual adapter (MVA). MVA is composed of several lightweight, learnable modules designed to fine-tune CLIP\u2019s visual encoder specifically for pairwise relation detection. This approach aims to enhance the model\u2019s ability to recognize the nuanced interactions that define relationships between subject and object in an image. ", "page_idx": 5}, {"type": "text", "text": "Regional Encoder. Given an image $\\boldsymbol{\\mathit{I}}$ and a subject-object pair with bounding boxes ( $b_{s}$ and $b_{o}$ ) from $\\scriptscriptstyle\\mathcal{O}$ , the initial visual embeddings can be obtained from the visual encoder of CLIP: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{\\Delta}f_{s/o}\\!=\\![f_{s/o}^{c l s}|f_{s/o}^{i m g}]\\!=\\![f_{s/o}^{c l s}|f_{s/o}^{1},f_{s/o}^{2},\\allowbreak\\cdot\\cdot\\cdot\\cdot\\cdot,f_{s/o}^{M}]\\!=\\!\\mathtt{E n c o d e r}_{v}\\!\\left(\\mathtt{C r o p}(I,b_{s/o})\\right)\\!,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $M$ denotes the number of patches, $\\mathtt{E n c o d e r}_{v}$ is CLIP\u2019s visual encoder that is kept frozen during training, and Crop represents image cropping. ", "page_idx": 5}, {"type": "text", "text": "Visual Aggregator. Next, MVA is adopted to aggregate $f_{s}$ and $\\scriptstyle f_{o}$ by cross-attention and two lightweight projection modules. Let the subject part be the query, and the object part be the key and value. The patch embeddings of object $\\pmb{f}_{o}^{i m j}$ are first projected into low-dimensional, semantic space: ", "page_idx": 5}, {"type": "equation", "text": "$$\nl_{o}^{i m g}=\\mathrm{Linear}_{d o w n}(\\pmb{f}_{o}^{i m g}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where Linear denotes a standard fully connected layer. Afterwards, cross-attention is adopted to capture the complicated interplay between subject and object, resulting in an aggregated visual embedding for the given subject-object pair: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\boldsymbol{v}_{s o}=\\mathsf{L i n e a r}_{u p}\\big(\\mathsf{A v g P o o1}\\big(\\mathsf{L N}(f_{o}^{c l s}+\\mathsf{C r o s s A t t n}(f_{s}^{i m g},l_{o}^{i m g}))\\big)\\big),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where AvgPool is the average pooling. LN is the standard layer normarlization. $\\mathtt{C r o s s A t t n}(Q,K V)$ denotes the standard cross-attention operation. $\\pmb{v_{o s}}$ can be computed in a similar way by exchanging the query and key of cross-attention. Combining them together leads to the final visual embedding $\\pmb{v}=(\\pmb{v}_{s o}+\\pmb{v}_{o s})/2$ for final similarity measurement. As such, MVA captures the interplay of subject and object in the projected, interaction-aware space. ", "page_idx": 5}, {"type": "text", "text": "Directional Marker. One may notice that the structure of MVA is symmetric and has no information about which input branch is the subject/object. This has a relatively small effect on semantic relations, but a significant effect on geometric relations. For instance, after exchanging the location of the subject and object (image flipping), the relation \u201ceating\u201d remains unchanged, while the relation \u201con the left\u201d would become \u201con the right\u201d. Here we simply incorporate two text embeddings $\\pmb{t}_{s}$ and $\\scriptstyle t_{o}$ ) of \u201ca photo of subject/object\u201d into MVA and thus update the visual embedding as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\pmb{v}_{s o}=\\mathtt{L i n e a r}\\big(\\mathtt{C o n c a t e}(\\pmb{v}_{s o},\\pmb{t}_{s})\\big),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where Concate denotes concatenation. $\\pmb{v}_{o s}$ and $\\pmb{v}$ can be updated accordingly. Further exploration of directional marker, e.g., incorporating more complex feature fusion modules, is left for future work. ", "page_idx": 6}, {"type": "text", "text": "3.3 Training Objective ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "A typical training objective in open-vocabulary learning aims to bring representations of positive pairs closer and push negative pairs apart in the embedding space. In SDSGG, the term \u201cpositive/negative pairs\u201d is not defined at the category level but at the description level, requiring losses tailored for different relation-description association types. Given a labeled relation triplet, one simplest contrastive loss can be defined as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}=\\frac{1}{|\\mathcal{T}|}\\sum_{t^{n}\\in\\mathcal{T}}\\big(\\underbrace{\\langle v,t_{a}^{n}\\rangle\\!-\\!\\langle v,t_{p}^{n}\\rangle}_{s i m i l a r i t y}-\\underbrace{\\alpha*C_{r}^{n}}_{t a r g e t}\\big)^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\alpha$ is a scaling factor. However, as for scene descriptions marked by possible coexistence (i.e., $C_{r}^{n}=0$ ), there is no direct target that can be used for training. Inspired by the identical mapping in residual learning [87], we make the prediction results of MVA close to those of CLIP. As such, MVA can learn the implicit knowledge embedded in CLIP\u2019s semantic space. In addition, this regularization term prevents MVA from overfitting to relations in the base split, which is a common problem in open-vocabulary learning. Hence, the loss is further reformulated as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}=\\frac{1}{|T|}\\sum_{t^{n}\\in\\mathcal{T}}\\big(\\underbrace{\\langle v,t_{a}^{n}\\rangle\\!-\\!\\langle v,t_{p}^{n}\\rangle}_{s i m i l a r i t y}-\\underbrace{\\alpha*C_{r}^{n}}_{t a r g e t}-\\underbrace{\\big(\\beta*s i m_{C L I P}(I,r e l)-\\lambda\\big)}_{m a r g i n}\\big)^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\beta$ is another scaling factor, $s i m_{C L I P}(I,r e l)$ denotes the vision-language similarity derived from the original CLIP, and $\\lambda$ is a constant scalar and is empirically set to 3e-2. ", "page_idx": 6}, {"type": "text", "text": "4 Experiment ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Dataset. We evaluate our method on GQA [15] and VG [14] following [3, 12]. ", "page_idx": 6}, {"type": "text", "text": "Split. Following previous work [3], VG is divided into two splits: base and novel split. The base split comprises $70\\%$ of the relation categories for training, while the novel split contains the remaining $30\\%$ categories invisible during training. For a more comprehensive comparison, we also conduct testing on the semantic set, encompassing 24 predicate categories [16, 12] with richer semantics. base and novel split of GQA [15] are obtained in a similar manner $(\\S B)$ . ", "page_idx": 6}, {"type": "text", "text": "Evaluation Metrics. We report Recall $@\\,\\mathrm{K}$ $(\\mathbf{R}@\\mathbf{K})$ and Recall $\\mathcal{\\varpi}\\mathrm{K}(\\mathrm{m}\\mathrm{R}\\mathcal{\\varpi}\\mathrm{K})$ following [3, 22]. ", "page_idx": 6}, {"type": "text", "text": "Base Models and Competitors. As for the base and novel split, we compare SDSGG with two baselines: i) CLS [4], which uses only the category-level prompts to compute the similarity between the image and text; and ii) Epic [3], a latest OVSGG method, which introduces an entangled crossmodal prompt approach and learns the cross-modal embeddings using contrastive learning. In terms of the semantic split, we compare our SDSGG with three baselines: i) CLS [4], which uses only the category-level prompts; ii) CLSDE [12], which uses prompts of relation class description; and iii) RECODE [12], which uses visual cues of several separate components. Since [10] has neither released the detailed split nor the code, it is not included in the comparisons for fairness. ", "page_idx": 6}, {"type": "text", "text": "Implementation Details. Due to limited space, implementation details are left in the appendix (\u00a7B). ", "page_idx": 6}, {"type": "text", "text": "4.2 Quantitative Comparison Result ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We conduct quantitative experiments on VG [14] and GQA [15]. To ensure the performance gain is reliable, each experiment is repeated three times. The average and standard deviation are reported. ", "page_idx": 6}, {"type": "table", "img_path": "xpRUi8amtC/tmp/da3be44718c783bfb493fea9f48cb4ecd417db1668fdf095210e8fda8fbbfef0.jpg", "table_caption": ["Table 1: Quantitative results (\u00a74.2) on VG [14] base and novel "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "xpRUi8amtC/tmp/5fd23c737846c0719462980590fcf1a93197b76e0c6fa52880474a014627c0a1.jpg", "table_caption": ["Table 2: Quantitative results (\u00a74.2) on VG [14] semantic "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "VG [14] base and novel. Table 1 illustrates our compelling results over the latest OVSGG model, Epic [3]. Since Epic did not release the full code, we report values under the same setting as in the original paper. CLS [4] leverages the original CLIP\u2019s checkpoint ${w/o}$ fine-tuning) and only the category-level prompts. The performance of CLS on the novel split is better than that on the base split because i) the base split contains more relation candidates to be classified ( $35\\;\\nu s$ . 15); and ii) the base split contains head, uninformative relations (e.g., \u201con\u201d and \u201chas\u201d) which are hard to be distinguished by CLIP [12]. By perform contrastive learning on the base split to learn the entangled cross-modal prompt, Epic [3] achieves significant performance gains on the base split (i.e., $22.6\\%$ vs. $3.2\\%\\,\\mathrm{R}@50$ and $27.2\\%$ vs. $3.9\\%$ $\\mathbf{R}@100;$ . However, Epic demonstrates worse performance on the novel split (i.e., $7.4\\%$ vs. $18.1\\%$ ${\\bf R}\\otimes50$ and $9.7\\%$ vs. $22.2\\%$ $\\mathrm{R}@100\\$ ), which indicates its overftiting on the training data. By incorporating SSDs, SDSGG outperforms Epic by a large margin on both the base split and novel split. For instance, SDSGG exceeds Epic by $3.9\\%/4.4\\%$ $\\mathrm{R}@50/100$ on the base split and ${\\pmb18.0\\%}/{\\pmb19.9\\%}\\,{\\mathrm R}@50/100$ on the novel split, respectively. The significant performance improvements on the novel set demonstrate the strong zero-shot capability of our approach. In addition, our margins over the CLS are $16.6\\%{\\sim}27.7\\%$ $\\mathbf{R}\\ @\\mathbf{K}$ and $2.2\\%{\\sim}3.9\\%\\mathrm{~mR}@\\mathrm{K}$ on the base split, and $5.2\\%{\\sim}7.3\\%$ $\\mathbf{R}\\ @\\mathbf{K}$ and ${\\bf5.6\\%\\sim}7.4\\%\\mathrm{~mR}{\\odot}\\mathrm{K}$ on the novel split, respectively. ", "page_idx": 7}, {"type": "text", "text": "VG [14] semantic. In Table 2, we present the numerical results of SDSGG against the latest OVSGG work [12] on the semantic split. By leveraging part-level prompts, RECODE demonstrates superior performance compared to CLS [4] and CLSDE [12]. The introduction of filtering strategies, i.e., $\\mathrm{RECODE^{\\star}}$ , shows substantial improvements. As seen, SDSGG surpasses all counterparts with remarkable gains on all metrics. In particular, SDSGG exceeds $\\mathrm{RECODE^{\\star}}$ by $\\mathbf{10.9\\%/11.0\\%/9.9\\%}$ on $\\mathrm{R}\\ @20/50/100$ , and $\\mathbf{6.1\\%}/\\mathbf{4.0\\%}/\\mathbf{0.6\\%}$ on $\\mathrm{mR}@20/50/100$ . Notably, SDSGG achieves impressive performance without relying on additional augmentation or data [10]. Since an increase on $\\operatorname{mR}({\\boldsymbol{\\omega}}\\mathbf{K}$ implies the average of the improvements for all categories, yields on $\\operatorname{mR}(\\!\\!\\alpha\\operatorname{K}$ may be relatively lower than that on $\\mathbf{R}\\ @\\mathbf{K}$ . Without bells and whistles, SDSGG establishes a new state-of-the-art. ", "page_idx": 7}, {"type": "text", "text": "GQA [15] base and novel. Since the codes of Epic [3] and RECODE [12] are insufficient to support replication, we only compare SDSGG with CLS on GQA [15]. As shown in Table 3, SDSGG outperforms CLS by a large margin across all splits and metrics. Since CLS is not trained on the base split, its performance on $\\mathbf{R}(\\!\\!\\mathscr{a}\\!\\!\\,\\mathbf{K}$ is rel", "page_idx": 7}, {"type": "table", "img_path": "xpRUi8amtC/tmp/e685c74e399e2c55f6db8acddb1d5e2847bb577d3abb8718cd4484ccecbda41f.jpg", "table_caption": ["Table 3: Quantitative results on GQA [14] base & novel "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "atively low due to the massive annotations of geometric relations that are not semantically rich [12]. ", "page_idx": 7}, {"type": "text", "text": "Taking together, our extensive results provide solid evidence that SDSGG successfully unlocks the power of LLMs in OVSGG, and SSD is a promising alternative to category-/part-level prompts. ", "page_idx": 7}, {"type": "image", "img_path": "xpRUi8amtC/tmp/7c1e60eaca57837f084c087ba3b6c450edba85fbed36890e69637e4066c4e1db.jpg", "img_caption": ["Figure 3: Visual results (\u00a74.3) on VG [14]. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.3 Qualitative Comparison Result ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Fig. 3 visualizes qualitative comparisons of SDSGG against CLIP [4] on VG [14]. As seen, with the proposed SSDs, SDSGG can generate higher-quality relation predictions even in challenging scenarios. We respectfully refer the reviewer to the appendix $(\\S\\mathrm{E})$ for more qualitative comparisons. ", "page_idx": 8}, {"type": "text", "text": "4.4 Diagnostic Experiment ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "For thorough evaluation, we conduct a series of ablative studies on VG [14]. ", "page_idx": 8}, {"type": "text", "text": "Textual Part. We first study the effectiveness of our multi-persona collaboration (MPC) for scene-specific description generation (\u00a73.1) in Table 4. Here we use the standard prompts to query LLMs for generating descriptions. As seen, without MPC, the performance drops drastically, ", "page_idx": 8}, {"type": "table", "img_path": "xpRUi8amtC/tmp/7634928a16b018f25bfdafe218c6835882db9bc295877826e952f3e31bff6e1e.jpg", "table_caption": ["Table 4: Ablation studies (\u00a74.4) on MPC. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "e.g., $11.8\\%/11.8\\%$ vs. $31.6\\%/30.0\\%$ $\\mathbf{R}\\mathcal{@100}$ on the base/novel split, respectively. This indicates the importance of the text classifiers used as they have impact on both training and testing. ", "page_idx": 8}, {"type": "text", "text": "While the effectiveness of MPC has been validated, one may wonder: i) Why use these three roles? ii) How to ensure the completeness and quality of generated classifiers? We want to highlight that: i) Different roles are used to increase the variety of descriptions. There is no word on exactly which roles should be used. ii) These open problems are beyond the scope of this work. We leave them for future work. iii) This work makes the first attempt to enhance classifier generation for OVSGG via MPC. iv) The experimental results suggest that the current generated SSDs are good enough for commonly used relation categories. To provide more empirical results, we investigate the impact of the proposed self-normalized similarities and the number of used SSDs in the appendix (\u00a7A). ", "page_idx": 8}, {"type": "text", "text": "Visual Part. Then, we examine the impact of mutual visual adapter (MVA, $\\S3.2)$ and directional marker (DM, $\\S3.2)$ in Table 5. The $1^{\\mathrm{st}}$ row denotes a strong baseline, i.e., a multi-layer perceptron with comparable parameters to aggregate the visual features. Upon projecting the visual features into interaction-aware space and applying cross", "page_idx": 8}, {"type": "table", "img_path": "xpRUi8amtC/tmp/53859caa0fc88e009e4c038a6c09f3134ceac465205c6ffa41c16ba84335ca42.jpg", "table_caption": ["Table 5: Ablation studies (\u00a74.4) on the visual part. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "attention, we observe consistent and substantial improvements for both $\\mathbf{R}\\ @\\mathbf{K}$ and $\\operatorname{mR}({\\widehat{\\alpha}}\\operatorname{K}$ on both the base and novel split. These results demonstrate the efficacy of our adapter and the necessity of incorporating cross-attention for capturing the complicated interplay between subjects and objects. Since DM is designed for geometric relations with massive annotations [21, 24], the improvements on $\\mathbf{R}\\ @\\mathbf{K}$ are considerable, while those on $\\operatorname{mR}({\\widehat{\\alpha}}\\operatorname K$ are relatively small. See $\\S\\mathrm{A}$ for more results. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This work presents SDSGG, a scene-specific description based framework for OVSGG. Despite the previous works based upon category-/part-level prompts, we argue that these text classifiers are scene-agnostic, which cannot adapt to specific contexts and may even be misleading. To address this, we carefully design a multi-persona collaboration strategy for generating flexible, context-aware SSDs, a self-normalized similarity computation module for renormalizing the influence of each SSD, and a mutual visual adapter that consists of several trainable lightweight modules for learning interaction-aware space. Our approach distinguishes itself by using SSDs derived from LLMs, which are tailored to the content of the presented image. This is further enhanced by MVA, which captures the underlying interaction patterns based on the semantic space of VLMs. We expect the introduction of SDSGG will not only set a new benchmark for OVSGG, but also encourage the community to explore the potential of integrating LLMs with VLMs for deeper, contextual understanding of images. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Acknowledgement. This work was supported by the National Science and Technology Major Project (No. 2023ZD0121300), the National Natural Science Foundation of China (No. 62372405), the Fundamental Research Funds for the Central Universities 226-2024-00058, National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, Xi\u2019an Jiaotong University (No. HMHAI202403), and Bytedance Doubao Fund. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Danfei Xu, Yuke Zhu, Christopher B Choy, and Li Fei-Fei. Scene graph generation by iterative message passing. In CVPR, pages 5410\u20135419, 2017.   \n[2] Tao He, Lianli Gao, Jingkuan Song, and Yuan-Fang Li. Towards open-vocabulary scene graph generation with prompt-based finetuning. In ECCV, pages 56\u201373, 2022.   \n[3] Qifan Yu, Juncheng Li, Yu Wu, Siliang Tang, Wei Ji, and Yueting Zhuang. Visually-prompted language model for fine-grained scene graph generation in an open world. In ICCV, 2023.   \n[4] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, pages 8748\u20138763, 2021.   \n[5] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In ICML, pages 12888\u201312900, 2022.   \n[6] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):1\u201335, 2023.   \n[7] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. IJCV, 130(9):2337\u20132348, 2022.   \n[8] Kaifeng Gao, Long Chen, Hanwang Zhang, Jun Xiao, and Qianru Sun. Compositional prompt tuning with motion cues for open-vocabulary video relation detection. In ICLR, 2023.   \n[9] Yong Zhang, Yingwei Pan, Ting Yao, Rui Huang, Tao Mei, and Chang-Wen Chen. Learning to generate language-supervised and open-vocabulary scene graph using pre-trained visual-semantic space. In CVPR, pages 2915\u20132924, 2023.   \n[10] Zuyao Chen, Jinlin Wu, Zhen Lei, Zhaoxiang Zhang, and Changwen Chen. Expanding scene graph boundaries: Fully open-vocabulary scene graph generation via visual-concept alignment and retention. arXiv preprint arXiv:2311.10988, 2023.   \n[11] Sachit Menon and Carl Vondrick. Visual classification via description from large language models. In ICLR, 2023.   \n[12] Lin Li, Jun Xiao, Guikun Chen, Jian Shao, Yueting Zhuang, and Long Chen. Zero-shot visual relation detection via composite visual cues from large language models. In NeurIPS, 2023.   \n[13] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Fei-Fei Li. Imagenet large scale visual recognition challenge. IJCV, 115(3):211\u2013252, 2015.   \n[14] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. IJCV, 123:32\u201373, 2017.   \n[15] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In CVPR, pages 6700\u20136709, 2019.   \n[16] Rowan Zellers, Mark Yatskar, Sam Thomson, and Yejin Choi. Neural motifs: Scene graph parsing with global context. In CVPR, pages 5831\u20135840, 2018.   \n[17] Kaihua Tang, Hanwang Zhang, Baoyuan Wu, Wenhan Luo, and Wei Liu. Learning to compose dynamic tree structures for visual contexts. In CVPR, pages 6619\u20136628, 2019.   \n[18] Xin Lin, Changxing Ding, Jinquan Zeng, and Dacheng Tao. Gps-net: Graph property sensing network for scene graph generation. In CVPR, pages 3746\u20133753, 2020.   \n[19] Xin Lin, Changxing Ding, Jing Zhang, Yibing Zhan, and Dacheng Tao. Ru-net: Regularized unrolling network for scene graph generation. In CVPR, pages 19457\u201319466, 2022.   \n[20] Chaofan Zheng, Xinyu Lyu, Lianli Gao, Bo Dai, and Jingkuan Song. Prototype-based embedding network for scene graph generation. In CVPR, pages 22783\u201322792, 2023.   \n[21] Kaihua Tang, Yulei Niu, Jianqiang Huang, Jiaxin Shi, and Hanwang Zhang. Unbiased scene graph generation from biased training. In CVPR, pages 3716\u20133725, 2020.   \n[22] Lin Li, Guikun Chen, Jun Xiao, Yi Yang, Chunping Wang, and Long Chen. Compositional feature augmentation for unbiased scene graph generation. In ICCV, pages 21685\u201321695, 2023.   \n[23] Lin Li, Jun Xiao, Hanrong Shi, Wenxiao Wang, Jian Shao, An-An Liu, Yi Yang, and Long Chen. Label semantic knowledge distillation for unbiased scene graph generation. IEEE TCSVT, 2023.   \n[24] Lin Li, Long Chen, Yifeng Huang, Zhimeng Zhang, Songyang Zhang, and Jun Xiao. The devil is in the labels: Noisy label correction for robust scene graph generation. In CVPR, pages 18869\u201318878, 2022.   \n[25] Xingchen Li, Long Chen, Jian Shao, Shaoning Xiao, Songyang Zhang, and Jun Xiao. Rethinking the evaluation of unbiased scene graph generation. In BMVC, 2023.   \n[26] Guikun Chen, Lin Li, Yawei Luo, and Jun Xiao. Addressing predicate overlap in scene graph generation with semantic granularity controller. In ICME, 2023.   \n[27] Lin Li, Jun Xiao, Hanrong Shi, Hanwang Zhang, Yi Yang, Wei Liu, and Long Chen. Nicest: Noisy label correction and training for robust scene graph generation. IEEE TPAMI, 2024.   \n[28] Jing Shi, Yiwu Zhong, Ning Xu, Yin Li, and Chenliang Xu. A simple baseline for weakly-supervised scene graph generation. In ICCV, pages 16393\u201316402, 2021.   \n[29] Xingchen Li, Long Chen, Wenbo Ma, Yi Yang, and Jun Xiao. Integrating object-aware and interactionaware knowledge for weakly supervised scene graph generation. In ACM MM, pages 4204\u20134213, 2022.   \n[30] Alireza Zareian, Svebor Karaman, and Shih-Fu Chang. Weakly supervised visual semantic parsing. In CVPR, pages 3736\u20133745, 2020.   \n[31] Keren Ye and Adriana Kovashka. Linguistic structures as weak supervision for visual scene graph generation. In CVPR, pages 8289\u20138299, 2021.   \n[32] Yuan Yao, Ao Zhang, Xu Han, Mengdi Li, Cornelius Weber, Zhiyuan Liu, Stefan Wermter, and Maosong Sun. Visual distant supervision for scene graph generation. In ICCV, pages 15816\u201315826, 2021.   \n[33] Tao He, Lianli Gao, Jingkuan Song, and Yuan-Fang Li. Toward a unified transformer-based framework for scene graph generation and human-object interaction detection. IEEE TIP, 32:6274\u20136288, 2023.   \n[34] Yao Teng and Limin Wang. Structured sparse r-cnn for direct scene graph generation. In CVPR, pages 19437\u201319446, 2022.   \n[35] Yuren Cong, Michael Ying Yang, and Bodo Rosenhahn. Reltr: Relation transformer for scene graph generation. IEEE TPAMI, 2023.   \n[36] Hengyue Liu, Ning Yan, Masood Mortazavi, and Bir Bhanu. Fully convolutional scene graph generation. In CVPR, pages 11546\u201311556, 2021.   \n[37] Rongjie Li, Songyang Zhang, and Xuming He. Sgtr: End-to-end scene graph generation with transformer. In CVPR, pages 19486\u201319496, 2022.   \n[38] Minghan Chen, Guikun Chen, Wenguan Wang, and Yi Yang. Hydra-sgg: Hybrid relation assignment for one-stage scene graph generation. arXiv preprint arXiv:2409.10262, 2024.   \n[39] Liulei Li, Jianan Wei, Wenguan Wang, and Yi Yang. Neural-logic human-object interaction detection. In NeurIPS, 2023.   \n[40] Liulei Li, Wenguan Wang, and Yi Yang. Human-object interaction detection collaborated with large relation-driven diffusion models. In NeurIPS, 2024.   \n[41] Jianan Wei, Tianfei Zhou, Yi Yang, and Wenguan Wang. Nonverbal interaction detection. In ECCV, 2024.   \n[42] Yongqin Xian, Zeynep Akata, Gaurav Sharma, Quynh Nguyen, Matthias Hein, and Bernt Schiele. Latent embeddings for zero-shot classification. In CVPR, pages 69\u201377, 2016.   \n[43] Ankan Bansal, Karan Sikka, Gaurav Sharma, Rama Chellappa, and Ajay Divakaran. Zero-shot object detection. In ECCV, pages 384\u2013400, 2018.   \n[44] Bernardino Romera-Paredes and Philip Torr. An embarrassingly simple approach to zero-shot learning. In ICML, pages 2152\u20132161, 2015.   \n[45] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.   \n[46] Jianzong Wu, Xiangtai Li, Shilin Xu, Haobo Yuan, Henghui Ding, Yibo Yang, Xia Li, Jiangning Zhang, Yunhai Tong, Xudong Jiang, et al. Towards open vocabulary learning: A survey. IEEE TPAMI, 2024.   \n[47] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui. Open-vocabulary object detection via vision and language knowledge distillation. In ICLR, 2022.   \n[48] Jian Ding, Nan Xue, Gui-Song Xia, and Dengxin Dai. Decoupling zero-shot semantic segmentation. In CVPR, pages 11583\u201311592, 2022.   \n[49] Size Wu, Wenwei Zhang, Sheng Jin, Wentao Liu, and Chen Change Loy. Aligning bag of regions for open-vocabulary object detection. In CVPR, pages 15254\u201315264, 2023.   \n[50] Le Xue, Mingfei Gao, Chen Xing, Roberto Mart\u00edn-Mart\u00edn, Jiajun Wu, Caiming Xiong, Ran Xu, Juan Carlos Niebles, and Silvio Savarese. Ulip: Learning a unified representation of language, images, and point clouds for 3d understanding. In CVPR, pages 1179\u20131189, 2023.   \n[51] Songyou Peng, Kyle Genova, Chiyu Jiang, Andrea Tagliasacchi, Marc Pollefeys, Thomas Funkhouser, et al. Openscene: 3d scene understanding with open vocabularies. In CVPR, pages 815\u2013824, 2023.   \n[52] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen Koltun, and Rene Ranftl. Language-driven semantic segmentation. In ICLR, 2022.   \n[53] Alireza Zareian, Kevin Dela Rosa, Derek Hao Hu, and Shih-Fu Chang. Open-vocabulary object detection using captions. In CVPR, pages 14393\u201314402, 2021.   \n[54] Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin. Scaling open-vocabulary image segmentation with image-level labels. In ECCV, pages 540\u2013557, 2022.   \n[55] Runyu Ding, Jihan Yang, Chuhui Xue, Wenqing Zhang, Song Bai, and Xiaojuan Qi. Pla: Language-driven open-vocabulary 3d scene understanding. In CVPR, pages 7010\u20137019, 2023.   \n[56] Dat Huynh, Jason Kuen, Zhe Lin, Jiuxiang Gu, and Ehsan Elhamifar. Open-vocabulary instance segmentation via robust cross-modal pseudo-labeling. In CVPR, pages 7020\u20137031, 2022.   \n[57] Chong Zhou, Chen Change Loy, and Bo Dai. Extract free dense labels from clip. In ECCV, pages 696\u2013712, 2022.   \n[58] Feng Liang, Bichen Wu, Xiaoliang Dai, Kunpeng Li, Yinan Zhao, Hang Zhang, Peizhao Zhang, Peter Vajda, and Diana Marculescu. Open-vocabulary semantic segmentation with mask-adapted clip. In CVPR, pages 7061\u20137070, 2023.   \n[59] Mingfei Gao, Chen Xing, Juan Carlos Niebles, Junnan Li, Ran Xu, Wenhao Liu, and Caiming Xiong. Open vocabulary object detection with pseudo bounding-box labels. In ECCV, pages 266\u2013282, 2022.   \n[60] Vibashan VS, Ning Yu, Chen Xing, Can Qin, Mingfei Gao, Juan Carlos Niebles, Vishal M Patel, and Ran Xu. Mask-free ovis: Open-vocabulary instance segmentation without manual mask annotations. In CVPR, pages 23539\u201323549, 2023.   \n[61] Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, and Xiaolong Wang. Groupvit: Semantic segmentation emerges from text supervision. In CVPR, pages 18134\u201318144, 2022.   \n[62] Huaishao Luo, Junwei Bao, Youzheng Wu, Xiaodong He, and Tianrui Li. Segclip: Patch aggregation with learnable centers for open-vocabulary semantic segmentation. In ICML, pages 23033\u201323044, 2023.   \n[63] Hao Zhang, Feng Li, Xueyan Zou, Shilong Liu, Chunyuan Li, Jianwei Yang, and Lei Zhang. A simple framework for open-vocabulary segmentation and detection. In ICCV, pages 1020\u20131031, 2023.   \n[64] Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Harkirat Behl, Jianfeng Wang, Lu Yuan, et al. Generalized decoding for pixel, image, and language. In CVPR, pages 15116\u201315127, 2023.   \n[65] Jie Qin, Jie Wu, Pengxiang Yan, Ming Li, Ren Yuxi, Xuefeng Xiao, Yitong Wang, Rui Wang, Shilei Wen, Xin Pan, et al. Freeseg: Unified, universal and open-vocabulary image segmentation. In CVPR, pages 19446\u201319455, 2023.   \n[66] Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp Kr\u00e4henb\u00fchl, and Ishan Misra. Detecting twentythousand classes using image-level supervision. In ECCV, pages 350\u2013368, 2022.   \n[67] Matthias Minderer, Alexey Gritsenko, and Neil Houlsby. Scaling open-vocabulary object detection. In NeurIPS, 2024.   \n[68] Wenguan Wang, Yi Yang, and Yunhe Pan. Visual knowledge in the big model era: Retrospect and prospect. FITEE, 2024.   \n[69] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In ICML, pages 1597\u20131607, 2020.   \n[70] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In CVPR, pages 9729\u20139738, 2020.   \n[71] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey E Hinton. Big selfsupervised models are strong semi-supervised learners. In NeurIPS, pages 22243\u201322255, 2020.   \n[72] Xinlei Chen\\*, Saining Xie\\*, and Kaiming He. An empirical study of training self-supervised vision transformers. In ICCV, pages 9620\u20139629, 2021.   \n[73] Longlong Jing and Yingli Tian. Self-supervised visual feature learning with deep neural networks: A survey. IEEE TPAMI, 43(11):4037\u20134058, 2020.   \n[74] Dongxu Li, Junnan Li, Hongdong Li, Juan Carlos Niebles, and Steven CH Hoi. Align and prompt: Video-and-language pre-training with entity prompts. In CVPR, pages 4953\u20134963, 2022.   \n[75] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In ECCV, pages 709\u2013727, 2022.   \n[76] Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, and Fahad Shahbaz Khan. Maple: Multi-modal prompt learning. In CVPR, pages 19113\u201319122, 2023.   \n[77] Mayug Maniparambil, Chris Vorster, Derek Molloy, Noel Murphy, Kevin McGuinness, and Noel E O\u2019Connor. Enhancing clip with gpt-4: Harnessing visual descriptions as prompts. In ICCV, pages 262\u2013271, 2023.   \n[78] Sarah Pratt, Ian Covert, Rosanne Liu, and Ali Farhadi. What does a platypus look like? generating customized prompts for zero-shot image classification. In ICCV, pages 15691\u201315701, 2023.   \n[79] Leonard Salewski, Stephan Alaniz, Isabel Rio-Torto, Eric Schulz, and Zeynep Akata. In-context impersonation reveals large language models\u2019 strengths and biases. In NeurIPS, volume 36, 2024.   \n[80] Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Hanqiu Deng, Yu Qiao, Peng Gao, and Hongsheng Li. Prompt, generate, then cache: Cascade of foundation models makes strong few-shot learners. In CVPR, pages 15211\u201315222, 2023.   \n[81] Yue Yang, Artemis Panagopoulou, Shenghao Zhou, Daniel Jin, Chris Callison-Burch, and Mark Yatskar. Language in a bottle: Language model guided concept bottlenecks for interpretable image classification. In CVPR, pages 19187\u201319197, 2023.   \n[82] An Yan, Yu Wang, Yiwu Zhong, Chengyu Dong, Zexue He, Yujie Lu, William Yang Wang, Jingbo Shang, and Julian McAuley. Learning concise and descriptive attributes for visual recognition. In ICCV, pages 3090\u20133100, 2023.   \n[83] Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. Unleashing the emergent cognitive synergy in large language models: A task-solving agent through multi-persona selfcollaboration. arXiv preprint arXiv:2307.05300, 2023.   \n[84] Haoyang Liu, Yijiang Li, Jinglin Jian, Yuxuan Cheng, Jianrong Lu, Shuyi Guo, Jinglei Zhu, Mianchen Zhang, Miantong Zhang, and Haohan Wang. Toward a team of ai-made scientists for scientific discovery from gene expression data. arXiv preprint arXiv:2402.12391, 2024.   \n[85] Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, and Jun Zhao. Large language models are better reasoners with self-verification. In Findings of EMNLP, pages 2550\u20132575, 2023.   \n[86] Yu Shang, Yu Li, Fengli Xu, and Yong Li. Defint: A default-interventionist framework for efficient reasoning with hybrid large language models. arXiv preprint arXiv:2402.02563, 2024.   \n[87] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pages 770\u2013778, 2016.   \n[88] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020.   \n[89] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. Transformer quality in linear time. In ICML, pages 9099\u20139117, 2022.   \n[90] Chun-Fu Richard Chen, Quanfu Fan, and Rameswar Panda. Crossvit: Cross-attention multi-scale vision transformer for image classification. In ICCV, pages 357\u2013366, 2021.   \n[91] Wenxiao Wang, Wei Chen, Qibo Qiu, Long Chen, Boxi Wu, Binbin Lin, Xiaofei He, and Wei Liu. Crossformer $^{++}$ : A versatile vision transformer hinging on cross-scale attention. IEEE TPAMI, 2023.   \n[92] Guangyi Chen, Weiran Yao, Xiangchen Song, Xinyue Li, Yongming Rao, and Kun Zhang. Plot: Prompt learning with optimal transport for vision-language models. In ICLR, 2023.   \n[93] Yaohua Zha, Jinpeng Wang, Tao Dai, Bin Chen, Zhi Wang, and Shu-Tao Xia. Instance-aware dynamic prompt tuning for pre-trained point cloud models. In ICCV, pages 14161\u201314170, 2023.   \n[94] Lifeng Fan, Wenguan Wang, Siyuan Huang, Xinyu Tang, and Song-Chun Zhu. Understanding human gaze communication by spatio-temporal graph reasoning. In ICCV, pages 5724\u20135733, 2019.   \n[95] Jiaming Lei, Lin Li, Chunping Wang, Jun Xiao, and Long Chen. Seeing beyond classes: Zero-shot grounded situation recognition via language explainer. In ACM MM, 2024. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Summary of the Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "For a better understanding of the main paper, we provide additional details in this supplementary material, which is organized as follows: ", "page_idx": 13}, {"type": "text", "text": "\u2022 $\\S\\mathrm{A}$ introduces more ablative experiments.   \n\u2022 $\\S B$ details the implementation details.   \n\u2022 $\\S C$ provides the pseudo code of SDSGG.   \n\u2022 $\\S D$ shows the generated scene-specific descriptions and the corresponding prompts.   \n\u2022 $\\S\\mathrm{E}$ offers more qualitative results.   \n\u2022 $\\S\\mathrm{F}$ discusses our limitations, societal impact, and directions of future work.   \n\u2022 $\\S\\mathrm{G}$ presents more experimental results. ", "page_idx": 13}, {"type": "text", "text": "A More Ablative Experiment ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Self-normalized Similarity. Furthermore, we study the effectiveness of self-normalized similarity for evaluating the influence of each scene-specific description (\u00a73.1) in Table S1. Here we remove the opposite description and use only the original scene description as the baseline. As seen, without the reference of opposite description, the performance drops drastically, e.g., $28.3\\%/25.5\\%/28.4\\%$ vs. $\\mathbf{31.6\\%}/\\mathbf{30.0\\%}/\\mathbf{\\bar{35}.1\\%}$ $\\mathbf{R}@100$ & $12.1\\bar{\\%}/29.8\\%/24.7\\%$ vs. $\\mathbf{14.7\\%/31.5\\%/28.6\\%\\mR}\\ @100$ on the base/novel/semantic split, respectively. This indicates the importance of renormalizing similarity scores, and also validates our hypothesis \u2013 the absolute value of one single similarity yields only limited insight, while the difference of two similarities provides more information. ", "page_idx": 13}, {"type": "table", "img_path": "xpRUi8amtC/tmp/e60ac34c2c9a541c97bab9a8ff11a1b6350cfbc408965bf320d2afb173c2daed.jpg", "table_caption": ["Table S1: Ablation studies $(\\S\\mathrm{A})$ on self-normalized similarity (SNS, $\\S3.1)$ . "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "Number of SSDs. We first study the impact of the number of used classifiers (i.e., SSDs). The results, presented in Table S2, reveal a considerable performance enhancement when the number of pairs increased from 11 to 21, affirming the efficiency of SSDs. However, we also witnessed a dip in performance when too many SSDs were employed. This could be due to the unnecessary pairing of a relation category with an excessive number of descriptions (52 descriptions for 26 pairs). ", "page_idx": 13}, {"type": "table", "img_path": "xpRUi8amtC/tmp/61574fa9f1ad41a603f129006c2fd6af396736a5b3635014e5846a2dbff1adfc.jpg", "table_caption": ["Table S2: Ablation studies (\u00a7A) on the number of used classifiers (Eq. 2). The adopted hyperparameters are marked in red. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "Scaling Factors. We then study the effectiveness of the scaling factors used in our training objectives (\u00a73.3) in Table S3. As seen, the performance remains consistent for those compared hyperparameters. This indicates the robustness of SDSGG to changes in the scale of the training targets. Considering the performance on all metrics together, $\\alpha$ and $\\beta$ are set to be 2 and 1e-1, respectively. ", "page_idx": 13}, {"type": "text", "text": "Number of Attention Heads. We further investigate the impact of the number of attention heads $H$ used in the mutual visual adapter (\u00a73.2, Eq. 5). As shown in Table S4, the performance improves from $32.7\\%$ to $35.1\\%$ $\\mathrm{R}@100$ on the semantic split when increasing the number of heads from 4 to 8, but the number of parameters steadily increase as the number of heads grows. When increasing the number of heads from 8 to 16, we observe some improvements (e.g., $31.6\\%$ to $32.8\\%$ $\\mathbf{R}@100$ on the base split), but also some performance drops (e.g., $30.0\\%$ to $29.0\\%$ $\\mathbf{R}@100$ on the novel split). Consequently, we set $H=8$ as the default to strike an optimal balance between accuracy and computation cost. ", "page_idx": 13}, {"type": "table", "img_path": "xpRUi8amtC/tmp/05c50b1beae39afd31ff7c36a3674202cf65d2b546eeee97a9600c1a999c9a07.jpg", "table_caption": ["Table S3: Ablation studies (\u00a7A) on the scaling factors used in the training objectives, i.e., $\\alpha$ and $\\beta$ (\u00a73.3, Eq. 8). The adopted hyperparameters are marked in red. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "table", "img_path": "xpRUi8amtC/tmp/dad90a48a1af4030ba54f3e862de08b9ed25dc931245cc7f5f6dd481ee55839e.jpg", "table_caption": ["Table S4: Ablation studies $(\\S\\mathrm{A})$ on the number of attention heads $H$ used in the mutual visual adapter (\u00a73.2, Eq. 5). The adopted hyperparameters are marked in red. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Margin. Last, we examine the impact of the margin $\\lambda$ in our training objectives (\u00a73.3, Eq. 8). As shown in Table S5, the performance remains consistent for those compared values, which indicates the robustness of our learning procedure. $\\lambda$ is set to be 3e-2 by default. ", "page_idx": 14}, {"type": "table", "img_path": "xpRUi8amtC/tmp/956e08036652f53a95b088e82cbc1493a67db29b87f63d1dcf0b8df9cf8043a6.jpg", "table_caption": ["Table S5: Ablation studies (\u00a7A) on the margin $\\lambda$ (Eq. 8). The adopted hyperparameters are marked in red. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "B Implementation Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Training. Our model is trained with a batch size of 4. One RTX 3090 is used for training. During the training process, images are resized to dimensions within the range of [600, 1,000]. For each relation, up to 50K samples are included. Random filpping is adopted for data augmentation. SGD is adopted for optimization. The initial learning rate, momentum, and weight decay are set to be 2e-2, 9e-1, 1e-4, respectively. We utilize the pre-trained weights of CLIP to initialize our model. To avoid data leakage, we remove annotations in the training set which contains categories in the novel split. ", "page_idx": 14}, {"type": "text", "text": "Testing. Testing is conducted on the same machine as in training. No data augmentation is used during testing. In terms of the semantic split, we directly applied the same weights trained on the base split for testing. A similar filtering strategy [12] is adopted. ", "page_idx": 15}, {"type": "text", "text": "Codebase and Architecture. We use the same codebase as in [21]. We adopt GPT-3.5 from OpenAI as our LLM. In terms of CLIP, we employ a widely used architecture, i.e., ViT-B/32, for initializing our mutual visual adapter. ", "page_idx": 15}, {"type": "text", "text": "Split. We visualize all relation categories in Table S6. To guarantee fairness, we select GQA\u2019s [15] relation categories that also exist in VG [14]. As such, we can reuse SSDs obtained in our experiments on VG, thus verifying the generalizability of the proposed SSDs and self-normalizing similarity. ", "page_idx": 15}, {"type": "table", "img_path": "xpRUi8amtC/tmp/2b9d5b1c9f9f1df0f49ea5e9d193075af1d6150651f95581a85ac2d35be08cac.jpg", "table_caption": ["Table S6: Relation categories in each split. "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "xpRUi8amtC/tmp/a49a5a5a628f7545239a2527c5aa9519732235a8d0b78740902fba1b18cd4641.jpg", "table_caption": ["(a) VG [14] ", "(b) GQA [15] "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "C Pseudo Code ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The pseudo code of SDSGG is given in Algorithm S1 and Algorithm S2. We respectfully refer the reviewer to the supplementary Python files for the PyTorch implementation of SDSGG\u2019s key modules. Moreover, to guarantee reproducibility, our full code and pre-trained models will be publicly available. ", "page_idx": 15}, {"type": "text", "text": "Algorithm S1 Pseudo-code for MVA of SDSGG in a PyTorch-like style. ", "text_level": 1, "page_idx": 15}, {"type": "table", "img_path": "xpRUi8amtC/tmp/c0b4ec724e82199b9bcce65335ea8ef666ef2599f5d1fba4e446e9b28c999f06.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Algorithm S2 Pseudo-code for the forward process of SDSGG in a PyTorch-like style. ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "bboxes: bounding boxes of all objects.   \nimg: input image.   \ntext_raw_des: raw description.   \ntext_op_des: opposite description.   \nasso: the association presented in \"Association between Scene-level Descriptions and Relation Categories\".   \ntargets: training targets. ", "page_idx": 16}, {"type": "text", "text": "def forward(bboxes, img, text_raw_des, text_op_des, asso, targets): # Batch crop and encode images. crop_img $=$ Crop(img, bboxes) img_feats $=$ EncodeImage(crop_img) # CLIP\u2019s visual encoder, Eq. 3 # Tokenize and encode descriptive text. text_raw_feats $=$ EncodeText(Tokenize(text_raw_des)) # CLIP\u2019s text encoder text_op_feats $=$ EncodeText(Tokenize(text_op_des)) # Define directional marker mark_sub $=$ EncodeText(Tokenize(\"a photo of subject\")) mark_obj $=$ EncodeText(Tokenize(\"a photo of object\")) scores, losses $\\mathbf{\\mu}=\\mathbf{\\mu}\\left[\\right]$ # Calculate self-normalized similarity for each subject-object pair for sub, obj in sub_obj_pairs: mva_out_s2o $=$ MVA(img_feats[sub], img_feats[obj], mark_sub) mva_out_o2s $=$ MVA(img_feats[obj], img_feats[sub], mark_obj) mva_out $=$ (mva_out_s2o $^+$ mva_out_o2s) / 2.0 sim_raw $=$ CosSim(mva_out, text_raw_feats) sim_op $=$ CosSim(mva_out, text_op_feats) score $=$ ((sim_raw - sim_op) $^*$ asso).sum() # Eq. 2 scores.append(score) # If is training if training: loss $=$ compute_loss(sim_raw, sim_op, targets) # Eq. 8 losses.append(loss) return scores, losses ", "page_idx": 16}, {"type": "text", "text": "D Scene-specific Description ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The detailed, scene-specific descriptions generated by LLM\u2019s multi-persona collaboration are provided in Fig. S1. As seen, our scene-specific descriptions (21 pairs, 42 descriptions in total) cover different scenes from a comprehensive view, e.g., spatial descriptions (\u201ctwo or more objects partially overlap each other\u201d, \u201cinteraction between objects\u201d, etc.), environment (background) descriptions (\u201con a road\u201d, \u201con a flat plane, it should appear balanced with no visible tilting\u201d, etc.), semantic descriptions (\u201cbelong to animal or human behavior\u201d, \u201cmay have contact behavior\u201d, etc.), appearance descriptions (\u201cmight have flat teeth or sharp teeth\u201d, \u201chave a curvy body\u201d, etc.). ", "page_idx": 17}, {"type": "image", "img_path": "xpRUi8amtC/tmp/47e905d2e5c539734ce2912ddec000838e335e0560568a0ef183e55ac131df9a.jpg", "img_caption": ["Figure S1: Illustration of the generated scene-specific descriptions. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Next, we detail each step to generate scene-specific descriptions. ", "page_idx": 17}, {"type": "text", "text": "$\\pmb{\\mathrm{\\Sigma}}$ Initial Description Generation. This step can be repeated many times to generate a large number (72 after manual selection) of scene descriptions. The prompt is shown in Fig. S2. ", "page_idx": 17}, {"type": "text", "text": "$\\pmb{\\varphi}$ Summarizing Descriptions. Since these initial descriptions may suffer from noise and semantic overlap, we ask LLM to streamline and combine these descriptions, ensuring more cohesive and distinct scene-level descriptions. The prompt is shown in Fig. S3. ", "page_idx": 17}, {"type": "text", "text": "$\\pmb{\\otimes}$ Description-Relation Association. After obtaining various scene descriptions, a critical inquiry arises regarding their utility for relation detection, given their lack of explicit association with specific relation categories. To address this, we delineate three distinct scenarios characterizing the interplay between relation categories and scene descriptions: i) certain coexistence ( $C_{r}^{n}=1$ ), where a direct correlation exists; ii) possible coexistence $C_{r}^{n}=0)$ ), indicating a potential but not guaranteed association; and iii) contradiction $(C_{r}^{n}\\!=\\!-1)$ ), denoting an incompatibility between the scene description and relation category. Here $C_{r}^{n}$ denotes the correlation between relation $r\\in\\mathcal{R}$ and $n_{t h}$ scene description, and is generated by LLMs. The prompt is shown in Fig. S4. ", "page_idx": 17}, {"type": "text", "text": "$\\pmb{\\mathbb{\\otimes}}$ Opposite Description Generation. Since classifiers are contextually bound, we generate opposite descriptions to compute self-normalized similarities. The prompt is shown in Fig. S5. ", "page_idx": 17}, {"type": "text", "text": "# Input: {scene content to be discussed}   \n# Output: $3^{\\sim}5$ descriptions   \nBegin by embodying three distinct personas: a biology expert, a physics expert, and an engineering expert. Each expert will articulate a step-by-step approach along with their thought process, considering various hypothetical scenarios relevant to their field of study, and then share their insights with the group. If any expert does not know the answer, he will exit the discussion. Once all experts have provided their analyses, summarize the final generic scene descriptions.   \nThe generic scene description involves the specific appearance and the visible action/motion/interaction may appear in the scene (use your imagination here). Here are some examples of generic scene descriptions:   \n{some in-context examples}   \nHere is an example of discussion:   \nDiscussion started!   \nQuestion: Suppose there is ... (this involves a detailed discussion of three roles)   \nDiscussion started!   \nQuestion: Suppose there is {scene content to be discussed}, please give concise, generic scene descriptions of this scene.   \n\"\"\" ", "page_idx": 18}, {"type": "text", "text": "Figure S2: Prompts for initial description generation. ", "page_idx": 18}, {"type": "text", "text": "# Input: initial {scene descriptions}, all {relation categories}, {number of final scene descriptions}   \n# Output: final scene-level descriptions   \n\"\"\"   \nHere is a text pool that includes a series of descriptive texts:   \n{scene descriptions}   \nHere are all the relation categories:   \n{relation categories}   \nYou are asked to pick {number of final scene descriptions} descriptive statements from the text pool that can describe at least two predicates. Think step by step.   \n\"\"\" ", "page_idx": 18}, {"type": "text", "text": "Figure S3: Prompts for summarizing descriptions. ", "page_idx": 18}, {"type": "text", "text": "# Input: {scene descriptions}, all {relation categories}   \n# Output: description-relation associations   \n\"\"\"   \nFor {scene descriptions}, decide whether it is likely to appear in one of the following flat photos where {relation categories} appears.   \nThe judgment result is to choose between [certain coexistence, possible coexistence, and contradiction]. When the photo shows that the scene must have a certain description, it is judged as \"contradiction\". When the photo has little relationship with the description, it is judged as \"possible coexistence\". When the photo shows that the scene must not appear with a certain description, it is judged as \"contradiction\".   \n\"\"\" ", "page_idx": 18}, {"type": "text", "text": "Figure S5: Prompts for opposite description generation. ", "page_idx": 19}, {"type": "text", "text": "E More Qualitative Comparison Result ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We provide more visual results that compare SDSGG to CLIP [4] in Fig. S6. It can be observed that SDSGG performs robust in hard cases and can consistently deliver more satisfying results, based upon the scene-specific descriptions and self-normalized similarities. ", "page_idx": 19}, {"type": "image", "img_path": "xpRUi8amtC/tmp/da154def1d2980dfd6738b7c890913207f9d40b288eb29489ddd10025c99d897.jpg", "img_caption": ["Figure S6: Visual results $(\\S\\mathrm{E})$ on VG [14] test. As seen, SDSGG makes the right predictions based upon the scene-specific descriptions and self-normalized similarities. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "F Discussion ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Limitation Analysis. Following [12], currently our algorithm is specifically designed for the predicate classification task [1, 16] to make fair comparisons with relation detection models. Such a pairwise classification task gives the ground-truth annotations of objects in the scene for easier representation learning of relations. However, these annotations are unavailable in real-world applications. ", "page_idx": 20}, {"type": "text", "text": "Societal Impact. This work points out the drawbacks of existing scene-agnostic text classifiers and accordingly introduces several new modules for both the visual and textual components, leading to a scene-specific description based OVSGG framework that combines the strengths of VLMs and LLMs. Like every coin has two sides, using our framework will have both positive and negative impacts. On the positive side, our work contributes to research on intelligent scene understanding, and thus is expected to eventually benefit industries such as autonomous driving. For potential negative social impact, the reliance on VLMs and LLMs could lead to the perpetuation of biases and inequalities present in the data used in these models\u2019 large-scale pre-training stage. ", "page_idx": 20}, {"type": "text", "text": "Future Work. As mentioned before, the focus of this work is not on object detection. It is interesting to extend our algorithm to handle the object detection task simultaneously by, for example, incorporating set-prediction architectures [88]. Moreover, the design of our multi-persona collaboration stands for an early attempt and deserves to be further explored. In addition, the architectural designs of directional marker and mutual visual adapter certainly worth further explorations, e.g., efficiency [89], architecture [90, 91], and adaptive prompting [92, 93]. Furthermore, extending our algorithm to other relation detection tasks [94, 39, 41, 95] may lead to an uniformed relation detection algorithm. ", "page_idx": 20}, {"type": "text", "text": "G More Experiment ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Training on the Full Set of Relation. We trained our model with frequency bias [25] on the full set of relations. The results are shown in Table S7. ", "page_idx": 20}, {"type": "table", "img_path": "xpRUi8amtC/tmp/d02abdf5a92570949d82cff85527e5fa49bfc86ece4b456ce5c3859bf7edd080.jpg", "table_caption": ["Table S7: Results on the full set of relation. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Different Base/Novel Splits. We trained our model on different base/novel splits to investigate the robustness further. Specifically, we i) change the proportion of the base and novel split and ii) change the categories within the base and novel split (i.e., different No. for the same ratio). The results are shown in Table S8. ", "page_idx": 20}, {"type": "table", "img_path": "xpRUi8amtC/tmp/800c0feacb03779de19d0cf24d0a20c389fa6e96c640067c61794ef3a8761f25.jpg", "table_caption": ["Table S8: Results on different base/novel splits "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Inference Time. Since the renormalization and similarity measurement involve only a few matrix operations that can be omitted from the complexity analysis, we will focus on the inference time of three main modules. The results are shown in Table S9. ", "page_idx": 20}, {"type": "table", "img_path": "xpRUi8amtC/tmp/3e8a46cc4578f0708fbdd3c2b304ca47fdbbf34009b498873a77ce7825ec8fbe.jpg", "table_caption": ["Table S9: Inference time for different modules. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Different Personas. The involvement of multiple personas in the discussion process enhances the diversity of generated descriptions. The key point of our multi-persona collaboration is about the \u201ccollaboration\u201d rather than a specific persona. Actually, using only one persona can even decrease the diversity of generated descriptions and hurt the performance, as it can only generate descriptions from its own viewpoint without discussion with others. In addition, we changed the system prompt of LLM from the default like \u201cyou are a helpful AI assistant\u201d into a persona-specific one like \u201cyou are a biologist\u201d. We then evaluate the performance of our model with these generated descriptions. The results are shown in Table S10. ", "page_idx": 21}, {"type": "table", "img_path": "xpRUi8amtC/tmp/d47509aa4685334e3fb24fb99069a59f430eb573a4f46fc4137375654e524be8.jpg", "table_caption": ["Table S10: Comparison of different personas. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We carefully described our contributions in the abstract and introduction. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: In the appendix, we discussed our limitations, societal impact, and directions for future work. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: This paper is not about theory. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We provided details about the methodology and implementation in the main paper and appendix. The code will be publicly available. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The code will be publicly available at https://github.com/guikunchen/SDSGG. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We present the experimental setup and details in the main paper and appendix. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We run each experiment three times and report the average and standard deviation. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We introduce the used computer resources in the appendix. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We carefully reviewed the NeurIPS Code of Ethics ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: In the appendix, we discussed our limitations, societal impact, and directions for future work. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We cited related papers. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 26}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]