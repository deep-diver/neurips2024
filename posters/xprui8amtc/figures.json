[{"figure_path": "xpRUi8amtC/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of the used text classifiers in OVSGG. (a) CLIP performs zero-shot classification by computing similarity between the query image and the text embeddings for each category, then choosing the highest. (b) To further utilize the learned semantic space of CLIP, one can compute similarities of multiple part-level prompts (e.g., the object of (man, riding, horse) may be described with \"with four legs\" and \"with a saddle\"). (c) Instead of using these scene-agnostic text classifiers, SDSGG adopts comprehensive, scene-specific descriptions generated by LLMs, which can adapt to specific contexts by using the proposed renormalization.", "description": "This figure illustrates three different approaches to text classifiers used in open-vocabulary scene graph generation (OVSGG). (a) shows the standard zero-shot approach using CLIP, where the similarity between image and text embeddings is computed for each category, and the highest similarity is selected. (b) expands on (a) by using multiple part-level prompts to better utilize CLIP's semantic space. (c) presents the proposed SDSGG approach, which uses comprehensive scene-specific descriptions generated by large language models (LLMs), adaptively adjusted based on the image content using a renormalization mechanism.", "section": "1 Introduction"}, {"figure_path": "xpRUi8amtC/figures/figures_4_1.jpg", "caption": "Figure 2: (a) Overview of SDSGG. (b) Each text classifier of SDSGG contains a raw description da and an opposite description dr. As such, the self-normalized similarities can be computed with the association (Cr) between predicate categories and SSDs. (c) Given the visual features (i.e., fimg, fols, and fimg) of both the subject and object extracted from CLIP's visual encoder, our mutual visual adapter (MVA) projects them into interaction-aware space and models their complicated interplay with cross-attention.", "description": "This figure shows the architecture of the proposed SDSGG model. (a) illustrates the overall framework, highlighting the interaction between the mutual visual adapter, scene-specific descriptions, and the similarity calculation. (b) details the self-normalized similarity measurement process, showcasing how raw and opposite descriptions are used to adjust the importance of text classifiers based on context. (c) zooms in on the mutual visual adapter, explaining how it leverages visual features from CLIP's visual encoder to capture the interplay between subject and object.", "section": "3 Methodology"}, {"figure_path": "xpRUi8amtC/figures/figures_8_1.jpg", "caption": "Figure 3: Visual results (\u00a74.3) on VG [14].", "description": "This figure shows qualitative comparison results of SDSGG against CLIP on the Visual Genome dataset.  The left panel shows an image of an elephant and a tire, where SDSGG more accurately predicts the relationship between them as \"hanging from\" compared to CLIP. The right panel shows an image of a man and a child playing tennis, where SDSGG correctly predicts the relationship as \"holding\" while CLIP assigns a lower confidence to this.", "section": "4.3 Qualitative Comparison Result"}, {"figure_path": "xpRUi8amtC/figures/figures_17_1.jpg", "caption": "Figure 1: Illustration of the used text classifiers in OVSGG. (a) CLIP performs zero-shot classification by computing similarity between the query image and the text embeddings for each category, then choosing the highest. (b) To further utilize the learned semantic space of CLIP, one can compute similarities of multiple part-level prompts (e.g., the object of (man, riding, horse) may be described with \u201cwith four legs\u201d and \u201cwith a saddle\u201d). (c) Instead of using these scene-agnostic text classifiers, SDSGG adopts comprehensive, scene-specific descriptions generated by LLMs, which can adapt to specific contexts by using the proposed renormalization.", "description": "This figure illustrates three different approaches to text classifiers used in open-vocabulary scene graph generation (OVSGG). (a) shows the standard zero-shot pipeline using CLIP, where similarity between image and text embeddings determines the classification. (b) enhances this by using multiple part-level prompts to utilize CLIP's semantic space. (c) introduces SDSGG's approach: using scene-specific descriptions generated by LLMs, whose weights are adjusted adaptively based on visual content.", "section": "1 Introduction"}, {"figure_path": "xpRUi8amtC/figures/figures_19_1.jpg", "caption": "Figure 2: (a) Overview of SDSGG. (b) Each text classifier of SDSGG contains a raw description da and an opposite description dr. As such, the self-normalized similarities can be computed with the association (Cr) between predicate categories and SSDs. (c) Given the visual features (i.e., fimg, fols, and fimg) of both the subject and object extracted from CLIP's visual encoder, our mutual visual adapter (MVA) projects them into interaction-aware space and models their complicated interplay with cross-attention.", "description": "This figure provides a detailed overview of the SDSGG architecture. Panel (a) shows the overall framework, highlighting the interaction between visual and textual components. Panel (b) illustrates the method for calculating self-normalized similarities using raw and opposite descriptions. Panel (c) details the mutual visual adapter (MVA) module, showing how it uses visual features to model interactions between subjects and objects.", "section": "3 Methodology"}]