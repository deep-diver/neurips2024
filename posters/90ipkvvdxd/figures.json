[{"figure_path": "90IpKvVdXd/figures/figures_16_1.jpg", "caption": "Figure 1: BanditRandSOA is an optimal randomized learner for online learning with bandit feedback of pattern classes, where the adversary is allowed to be adaptive. It is inspired by the RandSOA algorithm of Filmus, Hanneke, Mehalel, and Moran [2023], which is a randomized variant of Littlestnoe\u2019s [Littlestone, 1988] well-known SOA algorithm.", "description": "BanditRandSOA is an algorithm for online learning with bandit feedback.  It's optimal for pattern classes when the adversary is adaptive. The algorithm is inspired by the RandSOA algorithm and Littlestone's SOA algorithm.", "section": "B.1 An optimal learner for the primal game"}, {"figure_path": "90IpKvVdXd/figures/figures_27_1.jpg", "caption": "Figure 2: The \u201cdoubling trick\u201d algorithm DT.", "description": "The figure shows Algorithm DT, which is a modified algorithm that does not require prior knowledge of the inconsistency budget r*.  It works by iteratively doubling its guess of r* until it finds a value that is sufficient. This addresses the limitation of algorithms that require advance knowledge of r*. The algorithm uses a base learning algorithm A that works well under the r-realizability assumption. DT is designed to be used when A's mistake bound is of the form d\u2081(r + d\u2082), where d\u2081 and d\u2082 are constants independent of r.  In each phase, DT runs A with a certain assumption about the inconsistency budget. If that assumption is found to be incorrect, the algorithm restarts, doubling its budget estimate for the next phase.", "section": "G Prediction without prior knowledge"}]