{"references": [{"fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "publication_date": "2023-07-09", "reason": "This paper introduces Llama 2, a foundational large language model (LLM) that AmoebaLLM adapts for efficient deployment."}, {"fullname_first_author": "Meta AI", "paper_title": "Introducing Meta Llama 3: The most capable openly available LLM to date", "publication_date": "2024", "reason": "Llama 3 is another significant LLM that demonstrates the growing need for efficient deployment strategies addressed by AmoebaLLM."}, {"fullname_first_author": "Edward J Hu", "paper_title": "Lora: Low-rank adaptation of large language models", "publication_date": "2021", "reason": "The LoRA technique is a key component of AmoebaLLM's architecture, enabling efficient fine-tuning of LLMs."}, {"fullname_first_author": "Jiahui Yu", "paper_title": "BigNAS: Scaling up neural architecture search with big single-stage models", "publication_date": "2020", "reason": "BigNAS is a relevant work on one-for-all training techniques, which AmoebaLLM builds upon to derive subnets of arbitrary shapes."}, {"fullname_first_author": "Xinyin Ma", "paper_title": "LLM-Pruner: On the structural pruning of large language models", "publication_date": "2023", "reason": "LLM-Pruner is a state-of-the-art LLM compression method that AmoebaLLM compares against, demonstrating its performance improvements."}]}