[{"type": "text", "text": "Spatiotemporal Predictive Pre-training for Robotic Motor Control ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Robotic motor control necessitates the ability to predict the dynamics of envi  \n2 ronments and interaction objects. However, advanced self-supervised pre-trained   \n3 visual representations (PVRs) in robotic motor control, leveraging large-scale   \n4 egocentric videos, often focus solely on learning the static content features of   \n5 sampled image frames. This neglects the crucial temporal motion clues in human   \n6 video data, which implicitly contain key knowledge about sequential interacting   \n7 and manipulating with the environments and objects. In this paper, we present   \n8 a simple yet effective robotic motor control visual pre-training framework that   \n9 jointly performs spatiotemporal prediction with dual decoders, utilizing large-scale   \n10 video data, termed as STP. STP adheres to two key designs in a multi-task learning   \n11 manner. First, we perform spatial prediction on the masked current frame for   \n12 learning content features. Second, we utilize the future frame with an extremely   \n13 high masking ratio as a condition, based on the masked current frame, to conduct   \n14 temporal prediction of future frame for capturing motion features. This asymmet  \n15 ric masking and decoder architecture design is very efficient, ensuring that our   \n16 representation focusing on motion information while capturing spatial details. We   \n17 carry out the largest-scale BC evaluation of PVRs for robotic motor control to date,   \n18 which encompasses 21 tasks within a real-world Franka robot arm and 5 simulated   \n19 environments. Extensive experiments demonstrate the effectiveness of STP as well   \n20 as unleash its generality and data efficiency by further post-pre-training and hybrid   \n21 pre-training. Our code and weights will be released for further applications. ", "page_idx": 0}, {"type": "text", "text": "22 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "23 In NLP and CV, adapting pre-trained foundation models from large-scale data to various downstream   \n24 tasks has seen great success. For example, pre-trained visual representations using self-supervised [38,   \n25 15, 67, 2, 93] or weakly-supervised [71, 25, 55] methods exhibit strong generalization ability for   \n26 visual understanding. However, in robot learning, due to data scarcity and homogeneity, some   \n27 groundbreaking methods [53, 1] resort to training from scratch only using domain-specific data.   \n28 Recently, inspired by the success of transfer learning in CV, many works [69, 73, 65, 58, 59, 19] have   \n29 explored developing a pre-trained visual representation (PVR) using large-scale out-of-domain data   \n30 for various robotic motor control tasks. Currently, one successful paradigm [73, 99, 59, 19] is to use   \n3 large-scale egocentric video datasets [29] and train vanilla vision transformers (ViT) [22] based on   \n32 MAE [38], which exhibits excellent learning efficiency and generalization ability for learning policy   \n33 from raw pixel. Among them, the Ego4D [29] dataset offers numerous first-person human-object   \n34 interaction scenes and good motion clues. We argue that although learning static spatial structure   \n35 priors from task-relevant pre-training data sources is crucial, designing a more relevant self-supervised   \n36 proxy task for motor control should not be overlooked. Therefore, in this paper, we aim to develop a   \n37 more relevant self-supervised proxy task for robotic motor control representation learning.   \n38 Robotic motor control typically requires fine-grained spatial localization and relatively dense se  \n39 mantics. With its ability to effectively capture low-level geometry and space structure, MAE [38]   \n40 pre-training excels at this task. However, is dense spatial content sufficient for robotic motor control?   \n41 Some neuroscientific studies [50, 21, 88] suggest the brain\u2019s different areas or cells show special  \n42 ization. Some are dedicated to processing the information of temporal object motion, while others   \n43 focus on static spatial details. Their combination results in subjective pattern perception. Inspired by   \n44 this finding, we hypothesize that an effective robotic motor control pre-training proxy task should   \n45 require joint learning of spatial content features and temporal motion features. However, current   \n46 methods [73, 59, 19] use MAE pre-training with image frames from human videos, capturing only   \n47 static content features. They overlook the temporal motion clues in human videos, which implicitly   \n48 contain key knowledge about sequential interaction with environment and manipulation of objects.   \n49 Therefore, we aim to bridge this gap by incorporating these motion clues into our proxy task.   \n50 Based on the analysis above, the most critical challenge is the absence of action annotations in human   \n51 video data for modeling object motion. To model interaction and manipulation actions from actionless   \n52 video data, we propose to implicitly capture them by predicting future frame pixels based on current   \n53 frame. However, predicting the future frame without any conditions could contain high uncertainty   \n54 and be extremely difficult. Therefore, we propose to use the future frame with an extremely high   \n55 masking ratio as a prompt condition, specifically $95\\%$ , which serves to reveal some behavior and   \n56 dynamic priors, i.e. what to do and how to do it. In the experiments section, we will further explore   \n57 different condition alternatives, including language narration and their combination. Additionally,   \n58 directly and simply executing temporal prediction could lead the model to overlook static spatial   \n59 details, and it is also not efficient enough. Therefore, another technical contribution of STP is to jointly   \n60 perform spatial prediction by masking the current frame with $75\\%$ masking ratio. In summary, we   \n61 present STP, a multi-task self-supervised pre-training framework through spatiotemporal predictive   \n62 learning. Our STP asymmetrically mask the current frame and future frame from a video clip, using   \n63 a spatial decoder to conduct spatial prediction for content learning and a temporal decoder to conduct   \n64 temporal prediction for motion learning. This asymmetric masking and decoder architecture design   \n65 ensures that our pre-trained encoder focusing on motion information while capturing spatial details.   \n66 Subsequently, we establish our evaluation scheme. Currently, how to adapt pre-trained visual   \n67 representations for robotic motor control still remain an open question. Considering the expensive   \n68 cost of robot data collection or exploration, we employ a data-efficient paradigm of few-shot behavior   \n69 cloning by learning from demonstrations (Lfd). To demonstrate the generalization ability of visual   \n70 representation, our primary evaluation scheme involves freezing the visual encoder during policy   \n71 training. Additionally, considering that fine-tuning ViT with few demonstrations might lead to   \n72 overfitting and masked modeling exhibits excellent data efficiency [86, 102, 52] in domain-in data,   \n73 we further follow the post-pre-training [7, 93, 59] paradigm to perform STP pre-training with task  \n74 specific data to achieve better results. It is noteworthy that different tasks do not share representation   \n75 in this setting. Finally, we conduct the largest-scale BC evaluation of PVRs for robotic motor control   \n76 to date to demonstrate the effectiveness of STP, which encompasses 21 tasks ( 2 real-world tasks and   \n77 19 simulation tasks across 5 environments). These simulation tasks are derived from the union of   \n78 manipulation and locomotion tasks from prior works [65, 59].   \n79 We make the following four contributions: (1) We present STP, a self-supervised visual pre  \n80 training framework for robotic motor control, which jointly conducts spatiotemporal prediction with   \n81 asymmetric masking and decoder architecture design for content and motion features learning. (2)   \n82 We further expand STP by performing hybrid pre-training with ImageNet-MAE and post-pre-training   \n83 with task-specific data, unleashing its generality and data efficiency. (3) To our best knowledge, we   \n84 conduct the largest-scale BC evaluation of PVRs for robotic motor control to date to demonstrate the   \n85 effectiveness of STP. (4) Our experiments yield some insightful observations. In temporal prediction,   \n86 language does not significantly enhance performance. Instead, single-modality self-supervised   \n87 paradigm achieves the best results. This finding is highly encouraging for self-supervised robotic   \n88 motor control representation learning. Moreover, in the few-shot BC setting, naively scaling up model   \n89 size does not necessarily lead to improved outcomes. Finally, incorporating more diverse data and   \n90 domain-in data into the pre-training can further enhance performance. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "91 2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "92 Pre-trained Visual Representation Learning. Large-scale visual representation pre-training are   \n93 continually empowering computer vision. The primary supervised learning methods include learning   \n94 image recognition [40, 87] from ImageNet [20] and learning multi-modal alignment [71] from image  \n95 text pairs. Currently, self-supervised learning methods are enjoying significant popularity, primarily   \n96 falling into two main categories. The first category utilizes contrastive learning [39, 15, 14] technique   \n97 or joint-embedding architecture [13] to learn view-invariance. The second category performs masked   \n98 modeling [7, 38, 100, 95, 4, 2] and predict the pixel or representation of invisible parts in space. In   \n99 addition, some methods [106, 67, 8] have also proposed to combine different optimization objectives   \n100 in a multi-task learning manner. Recently pre-trained visual representation learning for robotic motor   \n101 control have bee rapidly developing [69, 65, 73, 99, 58, 57, 46, 59, 19]. These methods cover different   \n102 backbones (ResNet [40], ViT [22]), different policy learning methods (reinforcement learning [99],   \n103 behavior cloning [69, 65, 59], reward function [58] and task specification [42]), different adaptation   \n104 schemes (linear probing [69, 65, 46, 59], fine-tuning [19] and designing adapters [78, 56]), and   \n105 different evaluation environments (diverse simulation benchmarks). At present, it is still unclear how   \n106 these factors collectively influence the performance. In this paper, we choose scalable vanilla vision   \n107 transformer [22] as our backbone and data-efficient few-shot behavior cloning paradigm to conduct   \n108 policy learning, while ensuring the backbone is frozen during policy training.   \n109 Temporal Predictive Learning. Early works once explored representation learning through future   \n110 prediction, encompassing image [61], video [35, 80] and audio [66]. VideoMAE [86, 93] extend   \n111 MAE [38] to 3D video architecture. Recently TrackMAE [17] and SiamMAE [33] predict the   \n112 masked future frame based on unmasked current frame, leading to a better capture of temporal   \n113 correspondence and achieving outstanding performance in object tracking and segmentation tasks. In   \n114 robot learning, predicting future visual states primarily serves as a transition dynamic model such as   \n115 World Models [62, 77] and Dreamer [76]. [85, 9] predict the future visual states using goal image in   \n116 robot data. GR-1 [97] conducts language-conditioned video prediction for policy model pre-training   \n117 in a frozen visual representation space. [96] proposed dynamics-aware representation learning,   \n118 and [82, 72] employed forward dynamics for self-supervised pre-training. Some works explored   \n119 to train video prediction models and utilize visual foresight [32], inverse dynamics models [18],   \n120 goal-conditioned policy learning [23], and geometry estimation [51] methods for motor control,   \n121 respectively. [92] fine-tuned pre-trained representations into dynamic and functional distance   \n122 modules for manipulation tasks. Unlike these works, we utilize the public large-scale egocentric   \n123 video data and employ masked spatiotemporal predictive learning as a self-supervised proxy task   \n124 (without any language or action annotations) for robotic motor control representation learning,   \n125 instead of designing elaborate architectures or methods for specific predictive tasks [28, 37].   \n126 Vision-based Robot Learning. Vision-based robot learning plays a crucial role in robotics com  \n127 munity. Recently some related works focus on studying model architectures [44, 12, 47], observa  \n128 tion spaces [107], downstream policy learning methods [41], sim-to-real transfer [79], designing   \n129 adapters [78, 56], learning-from-scratch baseline [36], and affordance model [6, 105, 45, 60], in   \n130 visuo-motor representation learning. Other related works [70, 5, 91, 101, 48] attempt to learn ma  \n131 nipulation skills from small-scale and in-domain human videos. In addition, language-conditioned   \n132 vision robot learning has received significant attention. Some works scale multimodal robotic   \n133 data [42, 11, 34, 90, 24, 68, 84] or introduce Internet data and knowledge [81, 103, 10, 54, 43, 94, 64]   \n134 for end-to-end robot learning. In our study, we pre-train a off-the-shelf visual representation from   \n135 large-scale egocentric video datasets for robotic motor control tasks. Our method is more simple and   \n136 general for different downstream tasks of motor control. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "137 3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "138 In this section, we describe our method in details. First, we give an overview of our spatiotemporal   \n139 predictive pretraining (STP) framework. Then, we give a technical description on our core components   \n140 during pre-training: the masked image encoder and dual decoders scheme. Finally, we describe how   \n141 to adapt our pre-trained encoder to downstream robotic motor control tasks. ", "page_idx": 2}, {"type": "text", "text": "142 3.1 Overiew of STP ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "143 As illustrated in Figure 1, our STP aims to pre-train an image encoder for robotic motor control from   \n144 video datasets. This pre-trained image encoder is subsequently frozen and directly transferred to solve   \n145 motor control tasks. Specifically, given a video dataset $\\mathcal{D}$ , our goal is to learn an image encoder $\\Phi_{e n c}$ ,   \n146 that maps images to the visual representations. During pre-training and post-pre-training, $\\mathcal{D}$ represents   \n147 large-scale out-of-domain videos and task-specific demonstration videos, respectively. After pre  \n148 training, we reuse $\\Phi_{e n c}$ for downstream motor control policy learning. Specifically, the downstream   \n149 task will require an agent to make sequential action decisions based on visual observations $\\scriptscriptstyle\\mathcal{O}$ . Instead   \n150 of using the raw observation images as direct input like end-to-end policy learning from pixel, the   \n151 agent will employ the pre-trained $\\Phi_{e n c}$ to extract its visual state representation $\\Phi_{e n c}(O)$ for the   \n152 subsequent policy learning module. ", "page_idx": 2}, {"type": "image", "img_path": "3Rtn1OMTC4/tmp/e15e0eaabe84ed31d469369100b8cbbe12d2544060cf9bf2efe788b1b75516fb.jpg", "img_caption": ["Figure 1: STP framework. Left: During pre-training, we sample the current frame and the future frame from the video clip, and carry out spatiotemporal predictive pre-training. Right: During motor control tasks evaluation, we freeze the pre-trained encoder to extract visual state representations and discard the decoders. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "153 3.2 Masked Image Encoder ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "154 We first introduce the pipeline of our image encoder. Our image encoder processes image frames   \n155 using a vanilla vision transformer [22]. Given a image $\\mathbf{I}\\in\\mathbb{R}^{C\\times H\\times W}$ , we initially process it by the   \n156 patch embedding layer to obtain its token sequences $\\mathbf{T}$ , where $\\mathbf{T}=\\{P_{i}\\}_{i=1}^{N}$ and $N$ is the the total   \n157 token number, (e.g., $\\Nu=196$ for a $224\\times224$ image with a patch size of $16\\times16)$ ). Then we add the   \n158 fixed 2D sine-cosine positional embeddings for all tokens. Following this, we mask and remove a   \n159 part of tokens, according to a randomly generated masking map $\\mathbb{M}(\\rho)$ , where $\\rho$ is the masking ratio.   \n160 The encoder applies several transformer blocks (consisting of a global self-attention layer and a FFN   \n161 layer) on all unmasked tokens: $\\mathbf{Z}=\\Phi_{e n c}(\\mathbf{T}^{u})$ , where $\\mathbf{T}^{\\bar{u}}=\\{\\bar{T}_{i}\\}_{i\\in(1-\\mathbb{M}(\\rho))}$ . During this process, a   \n162 [CLS] token is added at the beginning.   \n163 Then we describe our encoding process during pre-training. We randomly sample two frames from a   \n164 video clip based on an interval: the current frame $\\mathbf{I_{c}}$ and the future frame ${\\bf I}_{f}$ . Following the above   \n165 pipeline, we randomly generate two asymmetric masking maps for the current frame and the future   \n166 frame, denoted as $\\bar{\\mathbb{M}_{c}}\\,=\\,\\mathcal{M}_{c}(\\rho^{c})$ and $\\mathbb{M}_{f}\\,=\\,\\mathcal{M}_{f}(\\rho^{f})$ , respectively. Each of these maps has a   \n167 different masking ratio. We then use these maps to separately process the two frames and obtain their   \n168 features, $\\mathbf{Z}_{c}$ and $\\mathbf{Z}_{f}$ . As analyzed above, our STP aims to jointly learn content and motion features   \n169 by spatiotemporal predictive learning. For content feature learning, we follow MAE [38], masking a   \n170 portion of the current frame based on $\\mathbb{M}_{c}$ , with $\\rho^{c}=75\\%$ , and predict the masked parts during the   \n71 decoding process. This encourages the model to learn spatial and geometric structure priors from the   \n172 current frame data through spatial reasoning. For motion feature learning, we establish an objective   \n173 to predict the future frame based on the masked current frame. However, predicting the future frame   \n174 without any conditions could be meaningless and extremely challenging. Therefore, we use the future   \n175 frame with an extremely high masking ratio as a condition, specifically $\\rho^{f}=95\\%$ , which reveals   \n176 some behavior and dynamic priors. In the experiments section, we will further discuss different   \n177 condition schemes, including language narration and the combination between them. In summary,   \n178 our encoding process during pre-training can be formally described as follows: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\mathbf{Z}_{c}=\\Phi_{e n c}(\\mathbf{I}_{c},\\mathbb{M}_{c}),}\\\\ {\\mathbf{Z}_{f}=\\Phi_{e n c}(\\mathbf{I}_{f},\\mathbb{M}_{f}).}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "image", "img_path": "3Rtn1OMTC4/tmp/0c68a3f317f777442b0973e2a8b8e9deaed96d981601cb4717b2d187aaefaa9e.jpg", "img_caption": ["Figure 2: Temporal decoder design. (a) Standard joint-self architecture. (b) Our self-cross architecture. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "179 3.3 Dual Decoders ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "180 To jointly capture static content and object motion features for better spatiotemporal understanding,   \n181 our STP present a dual decoders scheme to predict both the pixel of current and future frame   \n182 simultaneously in a multi-task learning manner. As shown in Figure 1, our dual decoder scheme   \n183 includes a spatial decoder $\\Phi_{d e c\\_s}$ for spatial prediction and a temporal decoder $\\Phi_{d e c\\_t}$ for temporal   \n184 prediction. We firstly give a technical description on them, respectively. Then we describe how we   \n185 combine them into our final method.   \n186 Spatial Decoder. To capture static content features, our spatial decoder is solely utilized for pro  \n187 cessing the current frame visual feature. Specifically, after obtaining the masked current frame   \n188 visual feature $\\mathbf{Z}_{c}$ , we concatenate it with some learnable masking tokens, leading to the formation   \n189 of $\\mathbf{Z}_{c}^{d}=\\mathbf{Z}_{c}\\cup\\{\\mathbf{M}_{i}\\}_{i\\in\\mathbb{M}_{c}}$ , where $\\mathbb{M}_{c}$ is the current frame masking map. Then, each of these tokens   \n190 further adds a corresponding positional embedding. Subsequently, $\\mathbf{Z}_{c}^{d}$ undergoes decoding in the   \n191 decoder and is continuously updated. The architecture of the spatial decoder block aligns with the   \n192 standard transformer encoder block, comprised of a global self-attention layer and a FFN layer.   \n193 Finally, with the deocoded token sequence $\\dot{\\mathbf{Z}}_{c}^{d}$ , our spatial decoder predicts the invisible tokens of the   \n194 current frame $\\hat{\\mathbf{I}_{c}^{d}}$ , operating under the current frame masking map $\\mathbb{M}_{c}$ .   \n195 Temporal Decoder. To capture motion features, our temporal decoder jointly processes the current   \n196 frame and the future frame which serves as the temporal prediction condition. To elaborate, we   \n197 firstly obtain the masked current frame feature $\\mathbf{Z}_{c}$ and the masked future frame feature $\\mathbf{Z}_{f}$ . We then   \n198 concatenate $\\mathbf{Z}_{f}$ with the masking tokens that have the positional embedding added, resulting in $\\mathbf{Z}_{f}^{d}$ .   \n199 Following this, $\\mathbf{Z}_{f}^{d}$ and $\\mathbf{Z}_{c}$ interact within the temporal decoder for decoding. The architecture of our   \n200 temporal decoder block is in alignment with the standard transformer decoder block [89], consisting   \n201 of a self-attention layer, a cross-attention layer, and a FFN layer, as shown in Figure 2 (b). During   \n202 decoding, the self-attention layer and FFN are solely used to process $\\mathbf{Z}_{f}^{d}$ . For the cross-attention   \n203 layer, $\\mathbf{Z}_{f}^{d}$ is continuously updated as the query, while $\\mathbf{Z}_{c}$ , acting as the key and value, is kept constant.   \n204 Compared to standard architecture, it ensures that the past frame representation space will not be   \n205 updated in the temporal decoder and are specifically used for temporal correlation and prediction.   \n206 This asymmetric interact architecture not only achieves more efficient training but also produces better   \n207 results. Finally, with the decoded token sequence $\\mathbf{Z}_{f}^{d}$ , our temporal decoder predicts the invisible ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "208 tokens of the future frame $\\hat{\\mathbf{I}_{f}^{d}}$ , operating under the future frame masking map $\\mathbb{M}_{f}$ . ", "page_idx": 4}, {"type": "text", "text": "209 Multi-task Predictive Learning. As mentioned above, our STP jointly conducts spatiotemporal   \n210 prediction by asymmetric masking ratio and dual decoders scheme, the whole decoding pipeline can   \n211 be formally described as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\underset{c}{\\hat{\\mathbf{I}}_{c}^{d}}=\\Phi_{d e c_{-}s}(\\mathbf{Z}_{c}^{d}),\\right.}\\\\ {\\left.\\frac{\\hat{\\mathbf{I}}_{f}^{d}}{\\mathbf{f}}=\\Phi_{d e c_{-}t}(\\mathbf{Z}_{c},\\mathbf{Z}_{f}^{d}).\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "212 Our loss function is the mean squared error (MSE) loss between the normalized masked pixels and   \n213 the predicted pixels. So our loss function $\\ell$ is as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\ell=\\mathrm{MSE}(\\hat{\\mathbf{I}}_{c},\\mathbf{I}_{c})+\\mathrm{MSE}(\\hat{\\mathbf{I}}_{f},\\mathbf{I}_{f}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "214 3.4 Downstream Policy Learning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "215 To enable data and computation efficiency during the policy learning process, we adopt the paradigm   \n216 of few-shot behavior cloning by learning from demonstrations (Lfd), and we keep the image encoder   \n217 frozen. Concretely, for each task, we are given offilne expert demonstrations $S=\\{\\tau_{1},...,\\tau_{n}\\}$ , where   \n218 each $\\tau_{i}$ is a trajectory of robot observations and actions, denoted as $\\tau_{i}=[(o_{0},a_{0}),\\dots,(o_{T},a_{T})]$ .   \n219 Based on the $\\boldsymbol{S}$ , we train a policy mdoel, $\\pi_{\\theta}(a|\\mathcal{C}(\\Phi_{e n c}(o)))$ , parameterized by $\\theta$ , which maps from   \n220 robot\u2019s state representations to actions. Here, $\\mathcal{C}$ represents an optional concatenation operation that   \n221 effectively fuses multi-view and multi-frame visual features, along with the robot\u2019s proprioceptive   \n222 state in the channel dimension. We optimize the $\\pi_{\\theta}$ through a standard behavior cloning MSE loss: ", "page_idx": 4}, {"type": "image", "img_path": "3Rtn1OMTC4/tmp/fe11f06a31fa4e2abfa6ab2478df1fc27090a94cce903f592ce555a937d1255f.jpg", "img_caption": ["Figure 3: The evaluation demonstrations of our real-world tasks. For picking, the robot arm needs to pick up the bowl on the desktop. For pouring, the robot arm needs to pour the ingredients from the bowl into the pot. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{\\theta}{\\operatorname*{min}}\\sum_{(o,a)\\sim\\mathcal{S}}\\mathrm{MSE}(a,\\pi_{\\theta}(\\mathcal{C}(\\Phi_{e n c}(o)))).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "223 4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "224 4.1 Implementation on Pre-training ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "225 We execute pre-training with data from EgoVLP [55] for comprehensive ablation and fair comparison.   \n226 It processes untrimmed videos of Ego4D and filters out that miss language narrations and belong   \n227 to validation or test sets, resulting in a total of 3.8 million clips, called as Egoclip. In pre-training,   \n228 we sample a frame pair from each clip for training. As for all experiments, we employ ViT [22] as   \n229 backbone. Additionally, we maintain consistency with prior works [73, 59], directly using the [CLS]   \n230 token as the global representation. The pre-training hyperparameters can be found in section A.3. ", "page_idx": 5}, {"type": "text", "text": "231 4.2 Implementation on Downstream Policy ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "232 Evaluation Scheme. Following popular settings on PVRs for robotic motor control [65, 46, 59], for   \n233 each task, we learn a single policy $\\pi$ which is structured as a MLPs network. The policy models   \n234 utilize both the history of visual observation embeddings and optional robot proprioceptive as inputs,   \n235 subsequently generating executable actions as outputs.   \n236 Simulation Tasks. We select the union of manipulation and locomotion tasks from prior   \n237 works [65, 59] for evaluation, encompassing 19 tasks across 5 simulated environments. These inclue   \n238 Meta-World [104] (Assembly, Bin-Picking, Button-Press, Drawer-Open, and Hammer), Franka  \n239 Kitchen [31] (Sliding Door, Turning Light On, Opening Door, Turning Knob, and Opening Mi  \n240 crowave), Adroit [74] (Relocate and Reorient-Pen), DMControl [83] (Finger-Spin, Reacher-Hard,   \n241 Cheetah-Run, Walker-Stand, and Walker-Walk), and Trifinger [98] (Reach-Cube and Push-Cube).   \n242 More detailed simulation evaluation details can be found in section A.4.   \n243 Real-World Tasks. In our real-world experiments, we evaluate contact-rich picking and pouring   \n244 tasks using a Franka Emika Research 3 robot arm in a tabletop environment, ensuring no duplication   \n245 with simulation Franka-Kitchen [31]. For each task, we collect 100 noise demonstrations for training,   \n246 and we conduct 20 trials per task during evaluation phase. The robotic arm and objects have different   \n247 initial pose between training and testing. The evaluation demonstrations of our real-world tasks is   \n248 shown in Figure 3. Please see section A.5 for more real-world setup details. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "image", "img_path": "3Rtn1OMTC4/tmp/37ca24a289106f7252164683d41d7172c56e94427d0c9bf96823e3f91a3c1d28.jpg", "img_caption": ["Figure 4: Attention Visualization. We use the [CLS] token as query, average the attention of all heads at the last layer of the frozen ViT encoder, and perform min-max normalization. We then upsample the attention map and overlay it on the original image, where the size of the attention value is directly proportional to the intensity of the yellow light. Top: MAE pre-training. Bottom: STP pre-training. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "249 4.3 Performance on Downstream Simulation Tasks ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "250 In this section, we mainly analyze the performance of some pre-trained image representations on   \n251 reproducible simulation tasks. Specifically, we first evaluate the following models: (1) public   \n252 DINOv2 [67] that combines masked image modeling with self-distillation on large-scale image   \n253 datasets; (2) public CLIP [71] that conducts contrastive learning on large-scale image-text pairs;   \n254 (3) R3M trained based on Egoclip [55]; (4) public VC-1 [59]; (5) MAE trained based on Egoclip;   \n255 (6) STP trained based on Egoclip. (7) STP that conducts hybrid pre-training with initialization   \n256 using ImageNet-MAE [59]. Among them, (1) and (2) achieve excellent performance on core visual   \n257 understanding tasks using zero-shot or linear probing evaluation settings. (3) and (4) utilize egocentric   \n258 videos for robotic motor control. (5), (6) and (7) are used for fair comparison and exploring the   \n259 potential benefits of STP from more diverse image data, respectively. The experimental results are   \n260 presented in Table 1. Consistent with prior findings [41, 59], there is not a universal foundation model   \n261 that performs optimally across all benchmarks. However, on the whole, the MAE method is superior   \n262 due to its effective modeling of low-level geometry and spatial structure, especially for the MetaWorld   \n263 tasks that demand fine-grained control. Another intriguing observation is that MAE underperforms   \n264 in the Franka-Kitchen and Adroit tasks. We believe that this could be due to its relatively weaker   \n265 semantic representation. Under a fair comparison, our STP outperforms MAE by 4.1 $(59.6\\rightarrow63.7)$ ),   \n266 and additionally benefits from a more diverse image data, improving by 0.5 $(63.7\\rightarrow64.2)$ ). This is   \n267 attributed to that our STP not only captures static content features but also effectively models motion   \n268 information by extracting temporal clues from videos of interactions and manipulations with the   \n269 environment and objects. Additionally, we provide the visualization of the attention maps (model (5)   \n270 and (6)) of several specific tasks in Figure 4. The results indicate that, on top of effectively capturing   \n271 spatial information, our method further encourages the model to focus on motion areas or objects,   \n272 thereby providing a more sparse and compact representation for downstream low-data BC paradigm.   \n273 Next, we also evaluate and compare the adaptation results of our representations to downstream motor   \n274 control tasks. Specifically, we evaluate following settings: (a) The MAE pre-trained representation   \n275 undergoes further MAE post-pre-training with task-specific data, and is frozen during policy training;   \n276 (b) The STP pre-trained representation undergoes further STP post-pre-training with task-specific   \n277 data, and is frozen during policy training; (c) The STP pre-trained representation undergoes end-to  \n278 end fine-tuning with task-specific data; (d) STP pre-training is performed directly using task-specific   \n279 data and the resulting representation is frozen during policy training. The results show that end-to-end   \n280 fine-tuning fails to yield the best results, suggesting that naively fine-tuning VIT-base could still lead   \n281 to overfitting under few-shot behavior cloning scheme. Conversely, (a) and (b) achieve competitive   \n282 results, with our STP achieving a 3.9 $72.5\\rightarrow76.4\\$ ) improvement on the weight average success rate   \n283 than MAE, further demonstrating the effectiveness and data efficiency of our STP for in-domain data.   \n284 In addition, the comparison between (a) and (d) also proves the effectiveness of pre-training with   \n285 out-of-domain data. Finally, we also scale up both MAE and our STP to ViT-L/16, and the results   \n286 still demonstrate the superiority of STP. Among them, compared to ViT-B/16, ViT-L/16 brings a   \n287 smaller performance improvement, which may be due to the task\u2019s performance saturation. However,   \n288 the ViT-L/16 of STP does not show improvement in Meta-World and Trifinger, indicating that simply ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Table 1: Performance comparations of visual representations on simulation benchmarks. We report the average score across all tasks for each simulation environment. DINOv2 uses ViT-B/14, CLIP uses ViT-B/32, and unless otherwise specified, others use ViT-B/16. Mt-Wd, Fr-Ki, DMC, Adro, Tr-f,i and WA respectively represent MetaWorld, Franka-Kitchen, DMControl, Adroit, Trifinger, and weight average. \\* denotes that public VC-1 samples image frmaes form full Ego4D dataset. ", "page_idx": 7}, {"type": "table", "img_path": "3Rtn1OMTC4/tmp/c1fb51d78020ba389702b2ef15ae28ed3972d2cba78a07515065fd9f2805be43.jpg", "table_caption": [], "table_footnote": ["Table 2: The ablation experiment results. Me, Fra, DMC, Adr, Tri, and WA respectively represent MetaWorld, Franka-Kitchen, DMControl, Adroit, Trifinger, and weight average. All models use ViT-B/16. "], "page_idx": 7}, {"type": "table", "img_path": "", "table_caption": ["(b) Temporal Prediction Condition Design. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "3Rtn1OMTC4/tmp/0eedc4990658e03b5a34ab1cd50023e60b0d88949b8ae356fb2ae8fd4a4fe307.jpg", "table_caption": ["(a) Current Frame Masking and Spatial Prediction. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "3Rtn1OMTC4/tmp/e48c447bda16f793e38685716c512900717b9e0b32a329bf983f340d95e2da25.jpg", "table_caption": ["(c) Temporal Decoder Architecture Design. ", "(d) Frame Sampling Strategy. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "289 scaling up model capacity does not necessarily lead to performance gains. In the few-shot BC setting,   \n290 there is a risk of overfitting in both policy and backbone training. ", "page_idx": 7}, {"type": "text", "text": "291 4.4 Ablation on Downstream Simulation Tasks ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "292 In this section, we perform extensive ablation studies to further demonstrate the effectiveness of our   \n293 joint spatial and temporal prediction, as well as temporal prediction condition design. In addition, we   \n294 also study the influence of temporal decoder architecture design and future frame sampling strategy.   \n295 Current frame masking. The design of the current frame masking is crucial. On one hand, similar   \n296 to MAE [38], masking some patches and predicting the missing parts can effectively promote the   \n297 learning of image content features. On the other hand, the visible patches of the current frame need   \n298 to interact with the condition to predict the future frame. Specifically, we mask the current frame at   \n299 masking rates of $75\\%$ , $50\\%$ , and $0\\%$ , respectively, and optionally predict the missing parts through   \n300 the spatial decoder. The results are shown in Table 2 (a). From results, we see that the masking ratio   \n301 of $75\\%$ and performing spatial prediction still lead to the best performance. This demonstrates the   \n302 importance of retaining MAE [38] for content features learning, especially for low-level manipulation   \n303 in Meta-World, while a current frame with a high masking ratio $(75\\%)$ is sufficient to interact with   \n304 other conditions to predict the future frame.   \n305 Temporal prediction condition design. Subsequently, we discuss the influence of temporal predic  \n306 tion condition design. We implicitly model motion in actionless video data by predicting the pixels of   \n307 the future frame. A direct and simple idea is to use language narration as a condition. The text tokens   \n308 can be flexibly utilized as inputs to ViT [22], forming a multimodal encoder. Language narration   \n309 provides a high-level behavior description, but lacks low-level visual dynamic priors for pixel-level   \n310 prediction. However, leaking part of the future frame can effectively provide these priors. In order   \n311 to explore how to construct a more meaningful temporal prediction proxy task, we compare the   \n312 following schemes: (1) only language narration, (2) masking $95\\%$ of the future frame, (3) masking   \n313 $90\\%$ of the future frame, (4) masking $95\\%$ of the future frame and language narration, and (5) masking   \n314 $95\\%$ of the future frame and language narration, but the language is added in the temporal decoder,   \n315 instead of being fused with the visible image patches in the multimodal encoder. We tokenize all   \n316 language narration by pre-trained DistilBERT [75]. The results are shown in Table 2 (b). From   \n317 results, we see that using only language as a prediction condition leads to a significant decline in   \n318 performance, while leaking a small amount of future frame (masking $95\\%$ ) in the temporal decoder   \n319 can achieve competitive results. As for joint conditions of language and future frame with $95\\%$   \n320 masking ratio, adding language in the encoder is better than in the decoder. Additionally, adding   \n321 language performs better on DMControl (64.1 vs. 62.1) and Trifinger (70.8 vs. 69.3), while not   \n322 adding language performs better on Meta-World (92.0 vs. 91.0), Franka-Kitchen (40.9 vs. 37.7) and   \n323 Adroit (48.0 vs. 46.7). We speculate the reasons for language hurts performance are as follows: (i)   \n324 The input gap (multi-modal and single-modal) between upstream and downstream; (ii) Extra language   \n325 in ViT may result in the loss of some fine-grained information capture. Furthermore, the latter does   \n326 not require language supervision, and can provide a more scalable self-supervised solution.   \n327 Temporal decoder design. We also investigate the impact of the temporal decoder design. Specifi  \n328 cally, we consider two types of decoder blocks. One is the joint-self architecture, as shown in Figure 2   \n329 (a), and similar joint architecture are adopted in [26, 102]. The other is the self-cross architecture, as   \n330 shown in Figure 2 (b), and similar cross architecture are adopted in [3, 33]. We consider the following   \n331 settings: (1) 8 joint-self decoder blocks, (2) 12 joint-self decoder blocks, (3) 8 self-cross decoder   \n332 blocks. Among them, setting (2) and (3) have similar amounts of parameters for a fairer comparison.   \n333 The results are shown in Table 2 (c). The results demonstrate the importance of maintaining a fixed   \n334 representation space of the past frame during temporal prediction.   \n335 Frame sampling strategy. Finally, we investigate the impact of the sampling strategy between the   \n336 current frame and future frame. The difficulty of temporal prediction is directly proportional to the   \n337 frame interval values. We establish four settings where we fix the sampling intervals at 8, 16, and 24   \n338 respectively, and for the fourth setting, we randomly select an interval within the range of [8, 24].   \n339 The results are shown in Table 2 (d). The results show that an interval of 16 achieves the best balance   \n340 for building temporal prediction proxy task. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "341 4.5 Performance on Downstream Real-world Tasks ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "342 In this section, we report our experiment results on   \n343 real-world picking and pouring tasks. We report the   \n344 average success rate for each task. Specifically, we   \n345 compare STP with the baseline MAE, both of which   \n346 are trained on out-of-domain videos and kept frozen   \n347 during policy training. The results are shown in Ta  \n348 ble 3. From the results, it can be seen that STP has   \n349 achieved significant advantages in the pouring task.   \n350 It can more accurately align with the moving bowl   \n351 and the pot. In addition, although MAE and STP have a same success rate in picking tasks, STP tends   \n352 to execute grasping in a better position. This indicates that the trend and conclusion of our STP are   \n353 consistent in both simulation and the real-world, which also aligns with the findings of [79]. ", "page_idx": 8}, {"type": "table", "img_path": "3Rtn1OMTC4/tmp/d7633193bd0a34922cdbacf2f8785b6820cafbd3ab140db465d9afdd963ca80d.jpg", "table_caption": ["Table 3: Performance comparations on real-world tasks. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "354 5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "355 In this work, we have proposed the STP, a simple, efficient and effective self-supervised visual repre  \n356 sentation pre-training framework for robotic motor control. Our STP jointly performs spatiotemporal   \n357 predictive learning on large-scale videos within a multi-task learning manner. Our STP captures   \n358 content features by predicting the invisible areas within the masked current frame, and simultaneously   \n359 captures motion features by using a future frame with an extremely high masking ratio as a condition   \n360 to predict the invisible areas within that future frame. We carry out the largest-scale BC evaluation of   \n361 PVRs for robotic motor control to date to demonstrate the effectiveness of STP. Furthermore, as for   \n362 pre-training data, we also prove that extending STP to hybrid pre-training and post-pre-training could   \n363 further unleash its generality and data efficiency. ", "page_idx": 8}, {"type": "text", "text": "364 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "365 [1] OpenAI: Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob   \n366 McGrew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al.   \n367 Learning dexterous in-hand manipulation. IJRR, 39(1):3\u201320, 2020.   \n368 [2] Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael   \n369 Rabbat, Yann LeCun, and Nicolas Ballas. Self-supervised learning from images with a   \n370 joint-embedding predictive architecture. In CVPR, pages 15619\u201315629, 2023.   \n371 [3] Roman Bachmann, David Mizrahi, Andrei Atanov, and Amir Zamir. Multimae: Multi-modal   \n372 multi-task masked autoencoders. In ECCV, pages 348\u2013367. Springer, 2022.   \n373 [4] Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and Michael Auli.   \n374 Data2vec: A general framework for self-supervised learning in speech, vision and language.   \n375 In ICML, pages 1298\u20131312. PMLR, 2022.   \n376 [5] Shikhar Bahl, Abhinav Gupta, and Deepak Pathak. Human-to-robot imitation in the wild. In   \n377 Kris Hauser, Dylan A. Shell, and Shoudong Huang, editors, RSS, 2022.   \n378 [6] Shikhar Bahl, Russell Mendonca, Lili Chen, Unnat Jain, and Deepak Pathak. Affordances   \n379 from human videos as a versatile representation for robotics. In CVPR, pages 13778\u201313790,   \n380 2023.   \n381 [7] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image   \n382 transformers. In ICLR, 2021.   \n383 [8] Adrien Bardes, Jean Ponce, and Yann LeCun. Mc-jepa: A joint-embedding predictive   \n384 architecture for self-supervised learning of motion and content features. arXiv preprint   \n385 arXiv:2307.12698, 2023.   \n386 [9] Konstantinos Bousmalis, Giulia Vezzani, Dushyant Rao, Coline Devin, Alex X Lee, Maria   \n387 Bauza, Todor Davchev, Yuxiang Zhou, Agrim Gupta, Akhil Raju, et al. Robocat: A self  \n388 improving foundation agent for robotic manipulation. arXiv preprint arXiv:2306.11706,   \n389 2023.   \n390 [10] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof   \n391 Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision  \n392 language-action models transfer web knowledge to robotic control. In CoRL, 2023.   \n393 [11] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea   \n394 Finn, Keerthana Gopalakrishnan, Karol Hausman, Alexander Herzog, Jasmine Hsu, Julian   \n395 Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil J. Joshi, Ryan Julian,   \n396 Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu,   \n397 Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn   \n398 Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael S. Ryoo, Grecia   \n399 Salazar, Pannag R. Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone,   \n400 Clayton Tan, Huong T. Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted   \n401 Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. RT-1: robotics transformer for   \n402 real-world control at scale. In Kostas E. Bekris, Kris Hauser, Sylvia L. Herbert, and Jingjin   \n403 Yu, editors, RSS, 2023.   \n404 [12] Shaofei Cai, Zihao Wang, Xiaojian Ma, Anji Liu, and Yitao Liang. Open-world multi-task   \n405 control through goal-aware representation learning and adaptive horizon prediction. In CVPR,   \n406 pages 13734\u201313744, 2023.   \n407 [13] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski,   \n408 and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV,   \n409 pages 9650\u20139660, 2021.   \n410 [14] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework   \n411 for contrastive learning of visual representations. In ICML, pages 1597\u20131607. PMLR, 2020.   \n412 [15] X Chen, S Xie, and K He. An empirical study of training self-supervised vision transformers.   \n413 in 2021 ieee. In ICCV, pages 9620\u20139629.   \n414 [16] Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, and   \n415 Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. In Kostas E.   \n416 Bekris, Kris Hauser, Sylvia L. Herbert, and Jingjin Yu, editors, RSS, 2023.   \n417 [17] Yutao Cui, Cheng Jiang, Gangshan Wu, and Limin Wang. Mixformer: End-to-end tracking   \n418 with iterative mixed attention. arXiv preprint arXiv:2302.02814, 2023.   \n419 [18] Yilun Dai, Mengjiao Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Josh Tenenbaum, Dale Schuur  \n420 mans, and Pieter Abbeel. Learning universal policies via text-guided video generation. arXiv   \n421 preprint arXiv:2302.00111, 2023.   \n422 [19] Sudeep Dasari, Mohan Kumar Srirama, Unnat Jain, and Abhinav Gupta. An unbiased look at   \n423 datasets for visuo-motor pre-training. In CoRL, 2023.   \n424 [20] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale   \n425 hierarchical image database. In CVPR, pages 248\u2013255. Ieee, 2009.   \n426 [21] AM Derrington and P Lennie. Spatial and temporal contrast sensitivities of neurones in lateral   \n427 geniculate nucleus of macaque. The Journal of physiology, 357(1):219\u2013240, 1984.   \n428 [22] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,   \n429 Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,   \n430 Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image   \n431 recognition at scale. In ICLR.   \n432 [23] Yilun Du, Mengjiao Yang, Pete Florence, Fei Xia, Ayzaan Wahid, Brian Ichter, Pierre Sermanet,   \n433 Tianhe Yu, Pieter Abbeel, Joshua B Tenenbaum, et al. Video language planning. arXiv preprint   \n434 arXiv:2310.10625, 2023.   \n435 [24] Hao-Shu Fang, Hongjie Fang, Zhenyu Tang, Jirong Liu, Chenxi Wang, Junbo Wang, Haoyi   \n436 Zhu, and Cewu Lu. Rh20t: A comprehensive robotic dataset for learning diverse skills in   \n437 one-shot. In Towards Generalist Robots: Learning Paradigms for Scalable Skill Acquisition $@$   \n438 CoRL2023, 2023.   \n439 [25] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang,   \n440 Xinlong Wang, and Yue Cao. Eva: Exploring the limits of masked visual representation   \n441 learning at scale. In CVPR, pages 19358\u201319369, 2023.   \n442 [26] Xinyang Geng, Hao Liu, Lisa Lee, Dale Schuurams, Sergey Levine, and Pieter Abbeel.   \n443 Multimodal masked autoencoders learn transferable representations. arXiv preprint   \n444 arXiv:2205.14204, 2022.   \n445 [27] Abraham George, Alison Bartsch, and Amir Barati Farimani. Openvr: Teleoperation for   \n446 manipulation. arXiv preprint arXiv:2305.09765, 2023.   \n447 [28] Rohit Girdhar and Kristen Grauman. Anticipative video transformer. In ICCV, pages 13505\u2013   \n448 13515, 2021.   \n449 [29] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit   \n450 Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the   \n451 world in 3,000 hours of egocentric video. In CVPR, pages 18995\u201319012, 2022.   \n452 [30] Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, Tri  \n453 antafyllos Afouras, Kumar Ashutosh, Vijay Baiyya, Siddhant Bansal, Bikram Boote, et al.   \n454 Ego-exo4d: Understanding skilled human activity from first-and third-person perspectives.   \n455 arXiv preprint arXiv:2311.18259, 2023.   \n456 [31] Abhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine, and Karol Hausman. Relay   \n457 policy learning: Solving long-horizon tasks via imitation and reinforcement learning. In CoRL,   \n458 pages 1025\u20131037. PMLR, 2020.   \n459 [32] Agrim Gupta, Stephen Tian, Yunzhi Zhang, Jiajun Wu, Roberto Mart\u00edn-Mart\u00edn, and Li Fei-Fei.   \n460 Maskvit: Masked visual pre-training for video prediction. In ICLR, 2022.   \n461 [33] Agrim Gupta, Jiajun Wu, Jia Deng, and Li Fei-Fei. Siamese masked autoencoders. 2023.   \n462 [34] Huy Ha, Pete Florence, and Shuran Song. Scaling up and distilling down: Language-guided   \n463 robot skill acquisition. In Conference on Robot Learning, pages 3766\u20133777. PMLR, 2023.   \n464 [35] Tengda Han, Weidi Xie, and Andrew Zisserman. Video representation learning by dense   \n465 predictive coding. In ICCV Workshops, pages 0\u20130, 2019.   \n466 [36] Nicklas Hansen, Zhecheng Yuan, Yanjie Ze, Tongzhou Mu, Aravind Rajeswaran, Hao Su,   \n467 Huazhe Xu, and Xiaolong Wang. On pre-training for visuo-motor control: Revisiting a   \n468 learning-from-scratch baseline. arXiv preprint arXiv:2212.05749, 2022.   \n469 [37] William Harvey, Saeid Naderiparizi, Vaden Masrani, Christian Weilbach, and Frank Wood.   \n470 Flexible diffusion modeling of long videos. NeurIPS, 35:27953\u201327965, 2022.   \n471 [38] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked   \n472 autoencoders are scalable vision learners. In CVPR, pages 16000\u201316009, 2022.   \n473 [39] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for   \n474 unsupervised visual representation learning. In CVPR, pages 9729\u20139738, 2020.   \n475 [40] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image   \n476 recognition. In CVPR, pages 770\u2013778, 2016.   \n477 [41] Yingdong Hu, Renhao Wang, Li Erran Li, and Yang Gao. For pre-trained vision models in mo  \n478 tor control, not all policy learning methods are created equal. arXiv preprint arXiv:2304.04591,   \n479 2023.   \n480 [42] Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, Frederik Ebert, Corey Lynch, Sergey   \n481 Levine, and Chelsea Finn. BC-Z: zero-shot task generalization with robotic imitation learning.   \n482 In Aleksandra Faust, David Hsu, and Gerhard Neumann, editors, CoRL, volume 164 of   \n483 Proceedings of Machine Learning Research, pages 991\u20131002. PMLR, 2021.   \n484 [43] Chuhao Jin, Wenhui Tan, Jiange Yang, Bei Liu, Ruihua Song, Limin Wang, and Jianlong Fu.   \n485 Alphablock: Embodied finetuning for vision-language reasoning in robot manipulation. arXiv   \n486 preprint arXiv:2305.18898, 2023.   \n487 [44] Ya Jing, Xuelin Zhu, Xingbin Liu, Qie Sima, Taozheng Yang, Yunhai Feng, and Tao Kong.   \n488 Exploring visual pre-training for robot manipulation: Datasets, models and methods. arXiv   \n489 preprint arXiv:2308.03620, 2023.   \n490 [45] Yuanchen Ju, Kaizhe Hu, Guowei Zhang, Gu Zhang, Mingrun Jiang, and Huazhe Xu. Robo  \n491 abc: Affordance generalization beyond categories via semantic correspondence for robot   \n492 manipulation. arXiv preprint arXiv:2401.07487, 2024.   \n493 [46] Siddharth Karamcheti, Suraj Nair, Annie S. Chen, Thomas Kollar, Chelsea Finn, Dorsa Sadigh,   \n494 and Percy Liang. Language-driven representation learning for robotics. In Kostas E. Bekris,   \n495 Kris Hauser, Sylvia L. Herbert, and Jingjin Yu, editors, RSS, 2023.   \n496 [47] Heecheol Kim, Yoshiyuki Ohmura, and Yasuo Kuniyoshi. Multi-task robot data for dual-arm   \n497 fine manipulation. arXiv preprint arXiv:2401.07603, 2024.   \n498 [48] Moo Jin Kim, Jiajun Wu, and Chelsea Finn. Giving robots a hand: Learning generalizable   \n499 manipulation with eye-in-hand human video demonstrations. arXiv preprint arXiv:2307.05959,   \n500 2023.   \n501 [49] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv   \n502 preprint arXiv:1412.6980, 2014.   \n503 [50] Andreas Kleinschmidt, Kai V Thilo, Christian B\u00fcchel, Michael A Gresty, Adolfo M Bronstein,   \n504 and Richard SJ Frackowiak. Neural correlates of visual-motion perception as object-or self  \n505 motion. Neuroimage, 16(4):873\u2013882, 2002.   \n506 [51] Po-Chen Ko, Jiayuan Mao, Yilun Du, Shao-Hua Sun, and Joshua B Tenenbaum. Learning to   \n507 act from actionless videos through dense correspondences. arXiv preprint arXiv:2310.08576,   \n508 2023.   \n509 [52] Xiangwen Kong and Xiangyu Zhang. Understanding masked image modeling via learning   \n510 occlusion invariant feature. In CVPR, pages 6241\u20136251, 2023.   \n511 [53] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep   \n512 visuomotor policies. JMLR, 17(1):1334\u20131373, 2016.   \n513 [54] Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, Jie Xu, Hongtao Wu, Chilam Cheang,   \n514 Ya Jing, Weinan Zhang, Huaping Liu, et al. Vision-language foundation models as effective   \n515 robot imitators. arXiv preprint arXiv:2311.01378, 2023.   \n516 [55] Kevin Qinghong Lin, Jinpeng Wang, Mattia Soldan, Michael Wray, Rui Yan, Eric Z XU,   \n517 Difei Gao, Rong-Cheng Tu, Wenzhe Zhao, Weijie Kong, et al. Egocentric video-language   \n518 pretraining. NeurIPS, 35:7575\u20137586, 2022.   \n519 [56] Xingyu Lin, John So, Sashwat Mahalingam, Fangchen Liu, and Pieter Abbeel. Spawn  \n520 net: Learning generalizable visuomotor skills from pre-trained networks. arXiv preprint   \n521 arXiv:2307.03567, 2023.   \n522 [57] Yecheng Jason Ma, William Liang, Vaidehi Som, Vikash Kumar, Amy Zhang, Osbert Bastani,   \n523 and Dinesh Jayaraman. Liv: Language-image representations and rewards for robotic control.   \n524 arXiv preprint arXiv:2306.00958, 2023.   \n525 [58] Yecheng Jason Ma, Shagun Sodhani, Dinesh Jayaraman, Osbert Bastani, Vikash Kumar, and   \n526 Amy Zhang. Vip: Towards universal visual reward and representation via value-implicit   \n527 pre-training. In ICLR, 2023.   \n528 [59] Arjun Majumdar, Karmesh Yadav, Sergio Arnaud, Yecheng Jason Ma, Claire Chen, Sneha   \n529 Silwal, Aryan Jain, Vincent-Pierre Berges, Pieter Abbeel, Jitendra Malik, et al. Where are we   \n530 in the search for an artificial visual cortex for embodied intelligence? 2023.   \n531 [60] Pierre Marza, Laetitia Matignon, Olivier Simonin, and Christian Wolf. Task-conditioned   \n532 adaptation of visual features in multi-task policy learning. arXiv preprint arXiv:2402.07739,   \n533 2024.   \n534 [61] Michael Mathieu, Camille Couprie, and Yann LeCun. Deep multi-scale video prediction   \n535 beyond mean square error. arXiv preprint arXiv:1511.05440, 2015.   \n536 [62] Russell Mendonca, Shikhar Bahl, and Deepak Pathak. Structured world models from human   \n537 videos. In Kostas E. Bekris, Kris Hauser, Sylvia L. Herbert, and Jingjin Yu, editors, RSS, 2023.   \n538 [63] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and   \n539 Josef Sivic. Howto100m: Learning a text-video embedding by watching hundred million   \n540 narrated video clips. In CVPR, pages 2630\u20132640, 2019.   \n541 [64] Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang,   \n542 Jifeng Dai, Yu Qiao, and Ping Luo. EmbodiedGPT: Vision-language pre-training via embodied   \n543 chain of thought. In NeurIPS, 2023.   \n544 [65] Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, and Abhinav Gupta. R3m: A   \n545 universal visual representation for robot manipulation. In CoRL, 2022.   \n546 [66] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive   \n547 predictive coding. arXiv preprint arXiv:1807.03748, 2018.   \n548 [67] Maxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov,   \n549 Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2:   \n550 Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023.   \n551 [68] Abhishek Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, Alex Irpan,   \n552 Alexander Khazatsky, Anant Rai, Anikait Singh, Anthony Brohan, et al. Open x-embodiment:   \n553 Robotic learning datasets and rt-x models. arXiv preprint arXiv:2310.08864, 2023.   \n554 [69] Simone Parisi, Aravind Rajeswaran, Senthil Purushwalkam, and Abhinav Gupta. The unsur  \n555 prising effectiveness of pre-trained vision models for control. In ICML, pages 17359\u201317371.   \n556 PMLR, 2022.   \n557 [70] Yuzhe Qin, Yueh-Hua Wu, Shaowei Liu, Hanwen Jiang, Ruihan Yang, Yang Fu, and Xiaolong   \n558 Wang. Dexmv: Imitation learning for dexterous manipulation from human videos. In ECCV,   \n559 pages 570\u2013587. Springer, 2022.   \n560 [71] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,   \n561 Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual   \n562 models from natural language supervision. In ICLR, pages 8748\u20138763. PMLR, 2021.   \n563 [72] Ilija Radosavovic, Baifeng Shi, Letian Fu, Ken Goldberg, Trevor Darrell, and Jitendra Malik.   \n564 Robot learning with sensorimotor pre-training. 2023.   \n565 [73] Ilija Radosavovic, Tete Xiao, Stephen James, Pieter Abbeel, Jitendra Malik, and Trevor Darrell.   \n566 Real-world robot learning with masked visual pre-training. In CoRL, pages 416\u2013426. PMLR,   \n567 2023.   \n568 [74] Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman,   \n569 Emanuel Todorov, and Sergey Levine. Learning complex dexterous manipulation with deep   \n570 reinforcement learning and demonstrations. RSS, 2018.   \n571 [75] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled   \n572 version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.   \n573 [76] Younggyo Seo, Danijar Hafner, Hao Liu, Fangchen Liu, Stephen James, Kimin Lee, and Pieter   \n574 Abbeel. Masked world models for visual control. In CoRL, pages 1332\u20131344. PMLR, 2023.   \n575 [77] Younggyo Seo, Kimin Lee, Stephen L James, and Pieter Abbeel. Reinforcement learning with   \n576 action-free pre-training from videos. In ICML, pages 19561\u201319579. PMLR, 2022.   \n577 [78] Mohit Sharma, Claudio Fantacci, Yuxiang Zhou, Skanda Koppula, Nicolas Heess, Jon Scholz,   \n578 and Yusuf Aytar. Lossless adaptation of pretrained vision models for robotic manipulation. In   \n579 ICLR, 2023.   \n580 [79] Sneha Silwal, Karmesh Yadav, Tingfan Wu, Jay Vakil, Arjun Majumdar, Sergio Arnaud, Claire   \n581 Chen, Vincent-Pierre Berges, Dhruv Batra, Aravind Rajeswaran, et al. What do we learn from   \n582 a large-scale study of pre-trained visual representations in sim and real environments? arXiv   \n583 preprint arXiv:2310.02219, 2023.   \n584 [80] Nitish Srivastava, Elman Mansimov, and Ruslan Salakhudinov. Unsupervised learning of   \n585 video representations using lstms. In ICML, pages 843\u2013852. PMLR, 2015.   \n586 [81] Austin Stone, Ted Xiao, Yao Lu, Keerthana Gopalakrishnan, Kuang-Huei Lee, Quan Vuong,   \n587 Paul Wohlhart, Brianna Zitkovich, Fei Xia, Chelsea Finn, et al. Open-world object manipula  \n588 tion using pre-trained vision-language models. In CoRL, 2023.   \n589 [82] Yanchao Sun, Shuang Ma, Ratnesh Madaan, Rogerio Bonatti, Furong Huang, and Ashish   \n590 Kapoor. Smart: Self-supervised multi-task pretraining with control transformers. In ICLR,   \n591 2023.   \n592 [83] Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David   \n593 Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite.   \n594 arXiv preprint arXiv:1801.00690, 2018.   \n595 [84] Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep   \n596 Dasari, Joey Hejna, Charles Xu, Jianlan Luo, et al. Octo: An open-source generalist robot   \n597 policy, 2023.   \n598 [85] Garrett Thomas, Ching-An Cheng, Ricky Loynd, Felipe Vieira Frujeri, Vibhav Vineet, Mihai   \n599 Jalobeanu, and Andrey Kolobov. Plex: Making the most of the available data for robotic   \n600 manipulation pretraining. In CoRL, pages 2624\u20132641. PMLR, 2023.   \n601 [86] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are   \n602 data-efficient learners for self-supervised video pre-training. NeurIPS, 35:10078\u201310093, 2022.   \n603 [87] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and   \n604 Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. In   \n605 ICLR, pages 10347\u201310357. PMLR, 2021.   \n606 [88] David C Van Essen and Jack L Gallant. Neural mechanisms of form and motion processing in   \n607 the primate visual system. Neuron, 13(1):1\u201310, 1994.   \n608 [89] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,   \n609 \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 30, 2017.   \n610 [90] Homer Rich Walke, Kevin Black, Tony Z Zhao, Quan Vuong, Chongyi Zheng, Philippe   \n611 Hansen-Estruch, Andre Wang He, Vivek Myers, Moo Jin Kim, Max Du, et al. Bridgedata v2:   \n612 A dataset for robot learning at scale. In Conference on Robot Learning, pages 1723\u20131736.   \n613 PMLR, 2023.   \n614 [91] Chen Wang, Linxi Fan, Jiankai Sun, Ruohan Zhang, Li Fei-Fei, Danfei Xu, Yuke Zhu, and   \n615 Anima Anandkumar. Mimicplay: Long-horizon imitation learning by watching human play.   \n616 arXiv preprint arXiv:2302.12422, 2023.   \n617 [92] Jianren Wang, Sudeep Dasari, Mohan Kumar Srirama, Shubham Tulsiani, and Abhinav Gupta.   \n618 Manipulate by seeing: Creating manipulation controllers from pre-trained representations. In   \n619 ICCV, pages 3859\u20133868, 2023.   \n620 [93] Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang, and   \n621 Yu Qiao. Videomae v2: Scaling video masked autoencoders with dual masking. In CVPR,   \n622 pages 14549\u201314560, 2023.   \n623 [94] Lirui Wang, Jialiang Zhao, Yilun Du, Edward H Adelson, and Russ Tedrake. Poco: Policy   \n624 composition from and for heterogeneous robot learning. arXiv preprint arXiv:2402.02511,   \n625 2024.   \n626 [95] Chen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan Yuille, and Christoph Feichtenhofer.   \n627 Masked feature prediction for self-supervised visual pre-training. In CVPR, pages 14668\u2013   \n628 14678, 2022.   \n629 [96] William F Whitney, Rajat Agarwal, Kyunghyun Cho, and Abhinav Gupta. Dynamics-aware   \n630 embeddings. In ICLR, 2020.   \n631 [97] Hongtao Wu, Ya Jing, Chilam Cheang, Guangzeng Chen, Jiafeng Xu, Xinghang Li, Minghuan   \n632 Liu, Hang Li, and Tao Kong. Unleashing large-scale video generative pre-training for visual   \n633 robot manipulation. arXiv preprint arXiv:2312.13139, 2023.   \n634 [98] Manuel Wuthrich, Felix Widmaier, Felix Grimminger, Shruti Joshi, Vaibhav Agrawal, Bi  \n635 lal Hammoud, Majid Khadiv, Miroslav Bogdanovic, Vincent Berenz, Julian Viereck, et al.   \n636 Trifinger: An open-source robot for learning dexterity. In CoRL, pages 1871\u20131882. PMLR,   \n637 2021.   \n638 [99] Tete Xiao, Ilija Radosavovic, Trevor Darrell, and Jitendra Malik. Masked visual pre-training   \n639 for motor control. arXiv preprint arXiv:2203.06173, 2022.   \n640 [100] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han   \n641 Hu. Simmim: A simple framework for masked image modeling. In CVPR, pages 9653\u20139663,   \n642 2022.   \n643 [101] Mengda Xu, Zhenjia Xu, Cheng Chi, Manuela Veloso, and Shuran Song. Xskill: Cross   \n644 embodiment skill discovery. In CoRL, 2023.   \n645 [102] Jiange Yang, Sheng Guo, Gangshan Wu, and Limin Wang. Comae: Single model hybrid   \n646 pre-training on small-scale rgb-d datasets. arXiv preprint arXiv:2302.06148, 2023.   \n647 [103] Jiange Yang, Wenhui Tan, Chuhao Jin, Bei Liu, Jianlong Fu, Ruihua Song, and Limin Wang.   \n648 Pave the way to grasp anything: Transferring foundation models for universal pick-place   \n649 robots. arXiv preprint arXiv:2306.05716, 2023.   \n650 [104] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn,   \n651 and Sergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta   \n652 reinforcement learning. In CoRL, pages 1094\u20131100. PMLR, 2020.   \n653 [105] Chengbo Yuan, Chuan Wen, Tong Zhang, and Yang Gao. General flow as foundation affordance   \n654 for scalable robot learning. arXiv preprint arXiv:2401.11439, 2024.   \n655 [106] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong.   \n656 ibot: Image bert pre-training with online tokenizer. arXiv preprint arXiv:2111.07832, 2021.   \n657 [107] Haoyi Zhu, Yating Wang, Di Huang, Weicai Ye, Wanli Ouyang, and Tong He. Point cloud   \n658 matters: Rethinking the impact of different observation spaces on robot learning. arXiv   \n659 preprint arXiv:2402.02500, 2024. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "660 A Appendix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "661 A.1 Limitations and Discussion ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "662 Although STP has demonstrated superior performance in extensive experiments, there remain some   \n663 challenges and future works. From the perspective of pre-training data, Ego4D provides numerous   \n664 human-object interaction scenes and good motion clues. Building larger-scale and more diverse   \n665 potential datasets such as [63, 30] to scale up STP is worth exploring. Regarding pre-training methods,   \n666 exploring predictive targets outside of pixel space and more effective sampling and masking strategies   \n667 present intriguing research directions. From an evaluation standpoint, we utilize a frozen ViT to   \n668 extract agent state representations and adopt the paradigm of few-shot behavior cloning, other policy   \n669 learning methods (reinforcement learning, visual reward function, visual task specification), have not   \n670 been explored. In conclusion, as the first method of performing temporal prediction on large-scale   \n671 videos for self-supervised visual representation learning intended for robotic motor control tasks, we   \n672 hope STP can be taken as a strong baseline and facilitate further research along this direction. ", "page_idx": 15}, {"type": "text", "text": "673 A.2 The influence of the loss weight ratio between temporal prediction and spatial prediction ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "674 In this section, we further explore the influence of the loss weight ratio between temporal prediction   \n675 and spatial prediction. Specifically, taking five tasks from Franka-Kitchen as examples, we load the   \n676 pre-trained STP and perform post-pre-training with three different loss weight ratios (temporal to   \n677 spatial). The results, as shown in Figure 5, are 54.7, 55.2, and 57.4 for the average results of the ratios   \n678 3:1, 1:3, and 1:1, respectively. The results indicate that due to the different attributes of the tasks, the   \n679 trends are not consistent. However, overall, the 1:1 ratio achieves the best balance and results. We   \n680 chose it as a universal setting. ", "page_idx": 15}, {"type": "text", "text": "681 A.3 Pre-training Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "682 In this section, we describe the details of our STP pre-training. Specifically, we list some key training   \n683 and architectural hyperparameters of STP in Table 4. In addition, as for our MAE [38] baseline, we   \n684 mainly follow the publicly available code of $\\mathbf{M}\\mathbf{A}\\mathbf{E}^{1}$ . Additionally, we train MAE and STP using the   \n685 same data and number of epochs to ensure that the comparison between them is completely fair.   \n686 Finally, we also provide some STP prediction results in Figure 6. ", "page_idx": 15}, {"type": "text", "text": "687 A.4 Simulation Environments Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "688 In this section, we first present further details of the STP post-pre-training on downstream simula  \n689 tion environments. Subsequently, we delineate the specific hyperparameters used in the behavior   \n690 cloning policy training within these simulation environments. Finally, we provide the comprehensive   \n691 evaluation scheme for each simulation environment.   \n692 In regards to the STP post-pre-training, we utilize data that aligns with the policy training, and the   \n693 specific architecture hyperparameters correspond to those listed in Table 4. Depending on the specific   \n694 demonstration data, we adjust the values of total epochs, warmup epochs, effective batch size, and   \n695 the frame interval, as shown in Table 5.   \n696 As for policy training and evaluation schemes, we primarily refer to the publicly available code2   \n697 and training data of VC-1 [59] for Metaworld [104], DMControl [83], Adroit [74] and Trifinger [98].   \n698 Similarly, for Franka-Kitchen [31], we follow the public code3and training data of R3M [65].   \n699 Specifically, the policy training hyperparameters and evaluation schemes are shown in Table 6 and   \n700 Table 7, respectively. About policy training, we completely follow the setting of prior works [65, 59]   \n701 when freezing the encoder; when performing end-to-end fine-tuning, we make appropriate adjustments   \n702 to the batch size and learning rate. About evaluation details, similar to prior works[65, 59], we   \n703 establish all evaluation details such as the number of expert demonstrations and test trajectories,   \n704 environmental viewpoints, optimization hyperparameters, base seeds, history windows size, and   \n705 the use of robot proprioceptive. In Table 7, the term \u2018prop.\u2019 stands for whether proprioceptive   \n706 information is used or not, and \u2018history window size\u2019 signifies the number of frames received by   \n707 the policy model at each step, with features between frames being fused through concatenation.   \n708 \u2018Number of trajectories\u2019 represents the quantity of trajectories evaluated. For tasks in Meta-World,   \n709 Franka-Kitchen, Adroit, and Trifinger, we report the maximum success rate, whereas for tasks in   \n710 DMControl, we report the maximum reward score, rescaling to be in the range of [0, 100] by dividing   \n711 by 10. We report the average metric across tasks for each environment. In addition, it is worth noting   \n712 that the metrics we report are the average value across all base seeds and camera viewpoints.   \n713 Finally, we also report the results of our post-pre-training STP (ViT-B/16) on each task in Table 8.   \n714 In addition, we emphasize that different random seeds primarily affect the rendering of the initial   \n715 frame in the sampled trajectories, as shown in Figure 7. During evaluation, the seed value we provide   \n716 serves as the base seed, and the trajectory sampling process is depicted in Algorithm 1. Therefore,   \n717 the actual number of trajectories we evaluate is the number of trajectories multiplied by the   \n718 number of base seeds. For instance, for MetaWorld, we evaluate $25\\times3=75$ trajectories, with   \n719 random seeds for rendering being 100-124, 200-224, and 300-324.   \n720 Finally, for Franka-Kitchen, we utilize MuJoCo210, while all other simulation environments are   \n721 based on MuJoCo200. Our policy training and evaluation environments are conducted on Cuda 11.3,   \n722 NVIDIA TITAN Xp GPUs, and OpenGL 3.1. ", "page_idx": 15}, {"type": "image", "img_path": "3Rtn1OMTC4/tmp/21a7edd5d7f266694014b91dcaeba32a3eef2ffb0adb117a032615757d2c1f0d.jpg", "img_caption": ["Figure 5: The results of different loss weight ratios between temporal prediction and spatial prediction. "], "img_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "3Rtn1OMTC4/tmp/6ef5feafa055441d7472a56cb66b80df9f5d3c68f19e0ea509a057f8f58ac2df.jpg", "table_caption": ["Table 4: Training and architectural hyperparameters for STP pre-training. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "image", "img_path": "3Rtn1OMTC4/tmp/bd9035369b70d0b889e8c741c8507cb3555efccb16df852bdcb0c88724c04aca.jpg", "img_caption": ["Figure 6: Some examples of our STP prediction result on Ego4D videos. For each six tuple, we show the ground-truth (left), masked frames (middle), STP prediciton results (right), current frames (top), and future frames (bottom). We simply overlay the output with the visible patches to improve visual quality. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "3Rtn1OMTC4/tmp/e5c3e41d0d004eb2a35761594b53432d950e88e111bc9778706aaf68943d6524.jpg", "img_caption": ["Figure 7: The visualization of initial frame rendering under different random seeds. Algorithm 1 Trajectories Sampling Pseudocode "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "# num_traj: the number of evaluation trajectories # base_seed: base seed for rollouts ", "page_idx": 18}, {"type": "text", "text": "# rollout to sample trajectories for ep in range(num_traj ): seed $=$ base_seed + ep env.set_seed(seed) o = env.reset () ", "page_idx": 18}, {"type": "text", "text": "723 A.5 Real-World Environments Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "724 In this section, we outline the details of our real-world setup and evaluation scheme. As depicted   \n725 in Figure 8, our real-world scenario includes four camera viewpoints: top, left, right, and wrist. It   \n726 includes two Kinect DK and two RealSense cameras. An example of four views is shown in Figure 9.   \n727 Specifically, we utilize four different camera views and resize their resolution uniformly to $224\\times224$ .   \n728 To effectively model the complex and multimodal action distribution in our real-world tasks, we   \n729 select diffusion policy [16] as our policy model. In accordance with this approach, we concatenate   \n730 the visual embeddings of all views from two sequential frames. Following the approach in [27], we   \n731 collect robot data using a VR tele-operation setup. In this way, we collect 100 continuous trajectories   \n732 for each task. It is worth noting that the quality of these demonstrations leaves room for improvement   \n733 and contains a lot of noise. During the evaluation phase, we primarily evaluate two contact-rich   \n734 tasks that have not appeared in Franka-Kitchen [31] benchmark: (1) Picking. It requires the robot   \n735 arm to pick up the transparent bowel off the table; (2) Pouring. It requires the robot arm to pour   \n736 at least three-quarter of the ingredients from the transparent bowl into the black pot. For each task,   \n737 we change the initial pose of the robot arm and objects within a certain range as well as conduct 20   \n738 trials. In addition, there are different distractors on the desktop during training and testing, which   \n739 also evaluates the robustness of the model to distractors. Throughout the process, we use ROS and   \n740 MoveIt for hardware communication and motion planning.   \n742 The checklist is designed to encourage best practices for responsible machine learning research,   \n743 addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove   \n744 the checklist: The papers not including the checklist will be desk rejected. The checklist should   \n745 follow the references and follow the (optional) supplemental material. The checklist does NOT count   \n746 towards the page limit.   \n747 Please read the checklist guidelines carefully for information on how to answer these questions. For   \n748 each question in the checklist:   \n749 \u2022 You should answer [Yes] , [No] , or [NA] .   \n750 \u2022 [NA] means either that the question is Not Applicable for that particular paper or the   \n751 relevant information is Not Available.   \n752 \u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA).   \n753 The checklist answers are an integral part of your paper submission. They are visible to the   \n754 reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it   \n755 (after eventual revisions) with the final version of your paper, and its final version will be published   \n756 with the paper.   \n757 The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.   \n758 While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a   \n759 proper justification is given (e.g., \"error bars are not reported because it would be too computationally   \n760 expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering   \n761 \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we   \n762 acknowledge that the true answer is often more nuanced, so please just use your best judgment and   \n763 write a justification to elaborate. All supporting evidence can appear either in the main paper or the   \n764 supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification   \n765 please point to the section(s) where related material for the question can be found. ", "page_idx": 18}, {"type": "image", "img_path": "3Rtn1OMTC4/tmp/0d3dd2a59e3e9268a21bd1c2e6aa1d431eeb747fef56369eb024cc17f920da24.jpg", "img_caption": ["Figure 8: Our real-world scene with four cameras and a Franka Emika robot arm. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "3Rtn1OMTC4/tmp/7e9242289d71fa55804371b5139489bcab715af1f02944126d9db9967f3967d8.jpg", "img_caption": ["Figure 9: An example of four views. "], "img_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "3Rtn1OMTC4/tmp/672f9aef9fd162f34b742a5693d2e4fde236a79c88635b97308099237bde1a14.jpg", "table_caption": ["Table 5: STP post-pre-training hyperparameters on simulation environments. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "3Rtn1OMTC4/tmp/d99a54105094039e525d72b1bbdc2fe7da11b3b2b6b49a1fa1e8b41a12cdc1c7.jpg", "table_caption": ["Table 6: Policy training hyperparameters on simulation environments. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "3Rtn1OMTC4/tmp/63e5adb4d6edfa797a1c0c8c49137fe08b63515086125b95892853dfdf4d07ce.jpg", "table_caption": ["Table 7: Evaluation schemes on simulation environments. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "3Rtn1OMTC4/tmp/dd02215ca4a129bfbdacecb8fc3959a71413fc137d43fd13c3ba01d8ba946de8.jpg", "table_caption": ["Table 8: The success rate for each task on simulation bechmarks. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "766 IMPORTANT, please: ", "page_idx": 21}, {"type": "text", "text": "767 \u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\",   \n768 \u2022 Keep the checklist subsection headings, questions/answers and guidelines below.   \n769 \u2022 Do not modify the questions and only use the provided macros for your answers.   \n770 1. Claims   \n771 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n772 paper\u2019s contributions and scope?   \n773 Answer: [Yes]   \n774 Justification: Yes, the main claims made in the abstract and introduction accurately reflect   \n775 the paper\u2019s contributions and scope.   \n776 Guidelines:   \n777 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n778 made in the paper.   \n779 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n780 contributions made in the paper and important assumptions and limitations. A No or   \n781 NA answer to this question will not be perceived well by the reviewers.   \n782 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n783 much the results can be expected to generalize to other settings.   \n784 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n785 are not attained by the paper.   \n786 2. Limitations   \n787 Question: Does the paper discuss the limitations of the work performed by the authors?   \n788 Answer: [Yes]   \n789 Justification: Yes, the paper discusses the limitations of the work performed by the authors.   \n790 Guidelines:   \n791 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n792 the paper has limitations, but those are not discussed in the paper.   \n793 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n794 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n795 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n796 model well-specification, asymptotic approximations only holding locally). The authors   \n797 should reflect on how these assumptions might be violated in practice and what the   \n798 implications would be.   \n799 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n800 only tested on a few datasets or with a few runs. In general, empirical results often   \n801 depend on implicit assumptions, which should be articulated.   \n802 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n803 For example, a facial recognition algorithm may perform poorly when image resolution   \n804 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n805 used reliably to provide closed captions for online lectures because it fails to handle   \n806 technical jargon.   \n807 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n808 and how they scale with dataset size.   \n809 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n810 address problems of privacy and fairness.   \n811 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n812 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n813 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n814 judgment and recognize that individual actions in favor of transparency play an impor  \n815 tant role in developing norms that preserve the integrity of the community. Reviewers   \n816 will be specifically instructed to not penalize honesty concerning limitations.   \n817 3. Theory Assumptions and Proofs   \n818 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n819 a complete (and correct) proof?   \n820 Answer: [Yes]   \n821 Justification: The paper does not include theoretical results.   \n822 Guidelines:   \n823 \u2022 The answer NA means that the paper does not include theoretical results.   \n824 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n825 referenced.   \n826 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n827 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n828 they appear in the supplemental material, the authors are encouraged to provide a short   \n829 proof sketch to provide intuition.   \n830 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n831 by formal proofs provided in appendix or supplemental material.   \n832 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.   \n833 4. Experimental Result Reproducibility   \n834 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n835 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n836 of the paper (regardless of whether the code and data are provided or not)?   \n837 Answer: [Yes]   \n838 Justification: The paper fully disclose all the information needed to reproduce results.   \n839 Guidelines:   \n840 \u2022 The answer NA means that the paper does not include experiments.   \n841 \u2022 If the paper includes experiments, a No answer to this question will not be perceived   \n842 well by the reviewers: Making the paper reproducible is important, regardless of   \n843 whether the code and data are provided or not.   \n844 \u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken   \n845 to make their results reproducible or verifiable.   \n846 \u2022 Depending on the contribution, reproducibility can be accomplished in various ways.   \n847 For example, if the contribution is a novel architecture, describing the architecture fully   \n848 might suffice, or if the contribution is a specific model and empirical evaluation, it may   \n849 be necessary to either make it possible for others to replicate the model with the same   \n850 dataset, or provide access to the model. In general. releasing code and data is often   \n851 one good way to accomplish this, but reproducibility can also be provided via detailed   \n852 instructions for how to replicate the results, access to a hosted model (e.g., in the case   \n853 of a large language model), releasing of a model checkpoint, or other means that are   \n854 appropriate to the research performed.   \n855 \u2022 While NeurIPS does not require releasing code, the conference does require all submis  \n856 sions to provide some reasonable avenue for reproducibility, which may depend on the   \n857 nature of the contribution. For example   \n858 (a) If the contribution is primarily a new algorithm, the paper should make it clear how   \n859 to reproduce that algorithm.   \n860 (b) If the contribution is primarily a new model architecture, the paper should describe   \n861 the architecture clearly and fully.   \n862 (c) If the contribution is a new model (e.g., a large language model), then there should   \n863 either be a way to access this model for reproducing the results or a way to reproduce   \n864 the model (e.g., with an open-source dataset or instructions for how to construct   \n865 the dataset).   \n866 (d) We recognize that reproducibility may be tricky in some cases, in which case   \n867 authors are welcome to describe the particular way they provide for reproducibility.   \n868 In the case of closed-source models, it may be that access to the model is limited in   \n869 some way (e.g., to registered users), but it should be possible for other researchers   \n870 to have some path to reproducing or verifying the results.   \n871 5. Open access to data and code   \n872 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n873 tions to faithfully reproduce the main experimental results, as described in supplemental   \n874 material?   \n875 Answer: [Yes]   \n876 Justification: We will release all codes and model weights on github.   \n877 Guidelines:   \n878 \u2022 The answer NA means that paper does not include experiments requiring code.   \n879 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n880 public/guides/CodeSubmissionPolicy) for more details.   \n881 \u2022 While we encourage the release of code and data, we understand that this might not be   \n882 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n883 including code, unless this is central to the contribution (e.g., for a new open-source   \n884 benchmark).   \n885 \u2022 The instructions should contain the exact command and environment needed to run to   \n886 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n887 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n888 \u2022 The authors should provide instructions on data access and preparation, including how   \n889 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n890 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n891 proposed method and baselines. If only a subset of experiments are reproducible, they   \n892 should state which ones are omitted from the script and why.   \n893 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n894 versions (if applicable).   \n895 \u2022 Providing as much information as possible in supplemental material (appended to the   \n896 paper) is recommended, but including URLs to data and code is permitted.   \n897 6. Experimental Setting/Details   \n898 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n899 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n900 results?   \n901 Answer: [Yes]   \n902 Justification: The paper specify all the training and test details.   \n903 Guidelines:   \n904 \u2022 The answer NA means that the paper does not include experiments.   \n905 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n906 that is necessary to appreciate the results and make sense of them.   \n907 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n908 material.   \n909 7. Experiment Statistical Significance   \n910 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n911 information about the statistical significance of the experiments?   \n912 Answer: [No]   \n913 Justification: In our experiments, different seeds are primarily used for rendering different   \n914 initial frames. Therefore, our evaluatation is comprehensive and sufficient, while our   \n915 comparisons are absolutely fair.   \n916 Guidelines:   \n917 \u2022 The answer NA means that the paper does not include experiments.   \n918 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n919 dence intervals, or statistical significance tests, at least for the experiments that support   \n920 the main claims of the paper.   \n921 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n922 example, train/test split, initialization, random drawing of some parameter, or overall   \n923 run with given experimental conditions).   \n924 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n925 call to a library function, bootstrap, etc.)   \n926 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n927 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n928 of the mean.   \n929 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n930 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n931 of Normality of errors is not verified.   \n932 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n933 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n934 error rates).   \n935 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n936 they were calculated and reference the corresponding figures or tables in the text.   \n937 8. Experiments Compute Resources   \n938 Question: For each experiment, does the paper provide sufficient information on the com  \n939 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n940 the experiments?   \n941 Answer: [Yes]   \n942 Justification: We provide sufficient information on the computer resources.   \n943 Guidelines:   \n944 \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "947   \n948   \n949   \n950   \n951   \n952 9. Code Of Ethics   \n953 Question: Does the research conducted in the paper conform, in every respect, with the   \n954 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n955 Answer:[Yes]   \n956 Justification: Our paper aligns with these.   \n957 Guidelines:   \n958 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n959 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n960 deviation from the Code of Ethics.   \n961 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n962 eration due to laws or regulations in their jurisdiction).   \n963 10. Broader Impacts   \n964 Question: Does the paper discuss both potential positive societal impacts and negative   \n965 societal impacts of the work performed?   \n966 Answer: [NA]   \n967 Justification: There is no societal impact of the work performed.   \n968 Guidelines:   \n969 \u2022 The answer NA means that there is no societal impact of the work performed.   \n970 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n971 impact or why the paper does not address societal impact.   \n972 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n973 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n974 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n975 groups), privacy considerations, and security considerations.   \n976 \u2022 The conference expects that many papers will be foundational research and not tied   \n977 to particular applications, let alone deployments. However, if there is a direct path to   \n978 any negative applications, the authors should point it out. For example, it is legitimate   \n979 to point out that an improvement in the quality of generative models could be used to   \n980 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n981 that a generic algorithm for optimizing neural networks could enable people to train   \n982 models that generate Deepfakes faster.   \n983 \u2022 The authors should consider possible harms that could arise when the technology is   \n984 being used as intended and functioning correctly, harms that could arise when the   \n985 technology is being used as intended but gives incorrect results, and harms following   \n986 from (intentional or unintentional) misuse of the technology.   \n987 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n988 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n989 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n990 feedback over time, improving the efficiency and accessibility of ML).   \n991 11. Safeguards   \n992 Question: Does the paper describe safeguards that have been put in place for responsible   \n993 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n994 image generators, or scraped datasets)?   \n995 Answer: [NA]   \n996 Justification: The paper poses no such risks.   \n997 Guidelines:   \n998 \u2022 The answer NA means that the paper poses no such risks.   \n999 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n1000 necessary safeguards to allow for controlled use of the model, for example by requiring   \n1001 that users adhere to usage guidelines or restrictions to access the model or implementing   \n1002 safety filters.   \n1003 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n1004 should describe how they avoided releasing unsafe images.   \n1005 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n1006 not require this, but we encourage authors to take this into account and make a best   \n1007 faith effort. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "008 12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1009 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n1010 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n1011 properly respected? ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "1033 Answer: [NA]   \n1034 Justification: The paper does not release new assets.   \n1035 Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "044 14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "45 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n46 include the full text of instructions given to participants and screenshots, if applicable, as   \n47 well as details about compensation (if any)?   \n1048   \n1049   \n1050   \n1051   \n1052   \n1053   \n1054   \n1055   \n1056   \n1057   \n1058   \n1059   \n1060   \n1061   \n1062   \n1063   \n1064   \n1065   \n1066   \n1067   \n1068   \n1069   \n1070   \n1071   \n1072   \n1073   \n1074   \n1075   \n1076   \n1077   \n1078 ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. \u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. \u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]