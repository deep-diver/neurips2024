[{"figure_path": "3Rtn1OMTC4/figures/figures_3_1.jpg", "caption": "Figure 1: STP framework. Left: During pre-training, we sample the current frame and the future frame from the video clip, and carry out spatiotemporal predictive pre-training. Right: During motor control tasks evaluation, we freeze the pre-trained encoder to extract visual state representations and discard the decoders.", "description": "This figure illustrates the STP (Spatiotemporal Predictive) framework. The left side shows the pre-training process where current and future frames are sampled from a video clip.  The current frame undergoes spatial prediction (75% masking) and the future frame is used as a condition for temporal prediction (95% masking) to learn both static and dynamic features.  The right side displays the evaluation phase of the framework applied to motor control tasks. Here, the pre-trained encoder (from the left) is frozen, extracting visual state representations for the policy model, while the decoders are discarded.  The evaluation encompasses 21 robotic motor control tasks across various simulated and real-world environments.", "section": "3 Method"}, {"figure_path": "3Rtn1OMTC4/figures/figures_4_1.jpg", "caption": "Figure 2: Temporal decoder design. (a) Standard joint-self architecture. (b) Our self-cross architecture.", "description": "This figure illustrates the architecture of the temporal decoder used in the STP framework.  Two variations are shown: (a) a standard joint-self-attention architecture where both the current and future frame features are processed together using self-attention, and (b) a self-cross-attention architecture that separately processes the current frame features using self-attention and then uses a cross-attention mechanism to incorporate the future frame features.  The cross-attention mechanism in (b) is designed to focus on the motion information while still retaining spatial context from the current frame, thus improving efficiency and effectiveness.", "section": "3.3 Dual Decoders"}, {"figure_path": "3Rtn1OMTC4/figures/figures_5_1.jpg", "caption": "Figure 1: STP framework. Left: During pre-training, we sample the current frame and the future frame from the video clip, and carry out spatiotemporal predictive pre-training. Right: During motor control tasks evaluation, we freeze the pre-trained encoder to extract visual state representations and discard the decoders.", "description": "This figure illustrates the STP (Spatiotemporal Predictive Pre-training) framework. The left side shows the pre-training stage where current and future frames are sampled from a video clip for spatiotemporal prediction.  The right side shows the evaluation on motor control tasks. Here, the pre-trained encoder is frozen to extract visual state representations, while the decoders are discarded.  The framework is designed to improve robotic motor control by leveraging large-scale video data and focusing on both spatial and temporal aspects of visual information.", "section": "3 Method"}, {"figure_path": "3Rtn1OMTC4/figures/figures_6_1.jpg", "caption": "Figure 4: Attention Visualization. We use the [CLS] token as query, average the attention of all heads at the last layer of the frozen ViT encoder, and perform min-max normalization. We then upsample the attention map and overlay it on the original image, where the size of the attention value is directly proportional to the intensity of the yellow light. Top: MAE pre-training. Bottom: STP pre-training.", "description": "This figure visualizes the attention mechanism of both MAE and STP pre-training methods.  The [CLS] token's attention weights are averaged across all attention heads in the final layer of the frozen ViT encoder, normalized, upsampled, and overlaid onto the input images. The intensity of yellow in the overlay directly correlates to the attention weight. The top row shows the results for MAE pre-training, while the bottom row shows the results for STP pre-training. The visualization highlights which parts of the image each method focuses on during learning, allowing for a comparison of their attention patterns.", "section": "4.3 Performance on Downstream Simulation Tasks"}, {"figure_path": "3Rtn1OMTC4/figures/figures_16_1.jpg", "caption": "Figure 1: STP framework. Left: During pre-training, we sample the current frame and the future frame from the video clip, and carry out spatiotemporal predictive pre-training. Right: During motor control tasks evaluation, we freeze the pre-trained encoder to extract visual state representations and discard the decoders.", "description": "This figure illustrates the STP (Spatiotemporal Predictive Pre-training) framework.  The left side shows the pre-training process:  a current frame and a future frame are sampled from a video clip.  These frames undergo masking, and then the masked information is used for spatial and temporal prediction via dual decoders. The right side depicts the evaluation phase for robotic motor control: the pre-trained image encoder is frozen and used to extract visual features from the input frames, while the decoders are discarded. The output features are then fed into a downstream policy learning module for action decision-making.", "section": "3 Method"}, {"figure_path": "3Rtn1OMTC4/figures/figures_17_1.jpg", "caption": "Figure 1: STP framework. Left: During pre-training, we sample the current frame and the future frame from the video clip, and carry out spatiotemporal predictive pre-training. Right: During motor control tasks evaluation, we freeze the pre-trained encoder to extract visual state representations and discard the decoders.", "description": "This figure illustrates the STP (Spatiotemporal Predictive Pre-training) framework. The left side shows the pre-training process where current and future frames are sampled from a video clip, and a spatiotemporal predictive model is trained.  The right side shows how the pre-trained encoder (which extracts visual features) is used for downstream motor control tasks, with the decoders discarded.", "section": "3 Method"}, {"figure_path": "3Rtn1OMTC4/figures/figures_18_1.jpg", "caption": "Figure 1: STP framework. Left: During pre-training, we sample the current frame and the future frame from the video clip, and carry out spatiotemporal predictive pre-training. Right: During motor control tasks evaluation, we freeze the pre-trained encoder to extract visual state representations and discard the decoders.", "description": "This figure illustrates the STP (Spatiotemporal Predictive Pre-training) framework used in the paper. The left side shows the pre-training process where current and future frames are sampled from a video clip.  The dual decoders (spatial and temporal) and encoder process these frames for both spatial and temporal predictive learning. The right side shows the evaluation on motor control tasks. Here, the pre-trained encoder is frozen, used to extract visual state representations, and the dual decoders are discarded.  The visual state representations are fed to a policy model which outputs actions to control a robot arm.", "section": "3 Method"}, {"figure_path": "3Rtn1OMTC4/figures/figures_19_1.jpg", "caption": "Figure 1: STP framework. Left: During pre-training, we sample the current frame and the future frame from the video clip, and carry out spatiotemporal predictive pre-training. Right: During motor control tasks evaluation, we freeze the pre-trained encoder to extract visual state representations and discard the decoders.", "description": "The figure illustrates the STP (Spatiotemporal Predictive) framework.  The left side shows the pre-training process using dual decoders for spatial and temporal prediction on masked current and future frames from video clips. The right side illustrates the evaluation on motor control tasks.  Here, the pre-trained encoder is frozen, extracting visual features, and the decoders are discarded, enabling the use of pre-trained representations for downstream policy learning.", "section": "3 Method"}, {"figure_path": "3Rtn1OMTC4/figures/figures_19_2.jpg", "caption": "Figure 1: STP framework. Left: During pre-training, we sample the current frame and the future frame from the video clip, and carry out spatiotemporal predictive pre-training. Right: During motor control tasks evaluation, we freeze the pre-trained encoder to extract visual state representations and discard the decoders.", "description": "This figure illustrates the STP (Spatiotemporal Predictive) framework. The left side shows the pre-training process where current and future frames from a video clip are sampled.  A spatiotemporal predictive pre-training is performed. The right side depicts the motor control task evaluation. The pre-trained encoder is frozen, visual state representations are extracted, and the decoders are discarded. ", "section": "3 Method"}]