{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "This paper introduces CLIP, a vision-language model that is used as the vision encoder in the proposed Slot-VLM, playing a crucial role in the framework."}, {"fullname_first_author": "Junnan Li", "paper_title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models", "publication_date": "2023-01-01", "reason": "BLIP-2 is a significant VLM that uses Q-Former to align visual features with LLMs; it is compared with the proposed Slot-VLM in the paper."}, {"fullname_first_author": "Francesco Locatello", "paper_title": "Object-centric learning with slot attention", "publication_date": "2020-12-01", "reason": "This paper introduces slot attention, which is the core component of the Object-Event Slots module in Slot-VLM."}, {"fullname_first_author": "Muhammad Maaz", "paper_title": "Video-ChatGPT: Towards detailed video understanding via large vision and language models", "publication_date": "2023-06-01", "reason": "Video-ChatGPT is a state-of-the-art VLM that uses pooling to reduce the number of video tokens; it is compared against the proposed Slot-VLM."}, {"fullname_first_author": "Christoph Feichtenhofer", "paper_title": "Slowfast networks for video recognition", "publication_date": "2019-10-01", "reason": "SlowFast is a video recognition model that uses dual-branch design to capture both spatial and temporal information; its design inspires the dual-branch Object-Event Slots module in Slot-VLM."}]}