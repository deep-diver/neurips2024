[{"figure_path": "tacb2bFZcm/figures/figures_1_1.jpg", "caption": "Figure 1: (a) We observe that the SwinIR-light (termed as Base) models exhibit significant similarities (CKA [10]) in projection layers. (b) Comparison between our proposed UPS and SOTA lightweight SISR models on BSD100 [11] for \u00d72 setting. A bigger circle size means a larger number of parameters. While being the most computationally and parameter-efficient, UPS-S (a more lightweight version of our method) demonstrates highly competitive results compared to SOTA methods.", "description": "This figure shows two subfigures. The first one (a) displays the similarities of projection layers across different scales in the SwinIR-light model. These similarities are computed using the Centered Kernel Alignment (CKA) metric. The second one (b) compares the proposed UPS method with other state-of-the-art (SOTA) lightweight single image super-resolution (SISR) models. The models are compared in terms of PSNR on the BSD100 dataset and the number of parameters and FLOPs. The results show that the UPS method achieves state-of-the-art performance while having the least number of parameters and FLOPs.", "section": "1 Introduction"}, {"figure_path": "tacb2bFZcm/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of Transformer-based architecture for lightweight SISR. There are three main components: (i) a head shallow feature extraction module, (ii) a deep feature extraction and aggregation (FEA) module consisting of N Swin Transformer layers (STL\u2081,\u2026\u2026, STLN), and (iii) a tail reconstruction module. Previous transformers (i.e., SwinIR [7], NGSwin [28]) synchronously perform multiple layer-specific deep image feature extraction (FE) and projection space (PS) optimization within a Swin Transformer Layer (STL). In contrast, we develop a decoupled Swin Transformer Layer (D-STL) in UPS to optimize per-layer feature extraction and a unified projection space (\u201cPSu\u201d defined by a learnable projection matrix UQ).", "description": "This figure illustrates the architecture of Transformer-based lightweight single image super-resolution (SISR) methods.  It compares the traditional approach (layer-specific feature extraction and projection space optimization) with the proposed UPS method (decoupled layer-specific feature extraction and a unified projection space). The UPS method aims to improve efficiency and performance by decoupling these two optimization steps. The figure clearly shows the difference in the optimization strategy between traditional and UPS methods.", "section": "3 Understanding Swin Transformer"}, {"figure_path": "tacb2bFZcm/figures/figures_4_1.jpg", "caption": "Figure 3: Comparison between SOTA SISR models and ours. We show the SR results overlaid with the local attribution map (LAM [33]) of each model. The LAM visually illustrates the activation of local and non-local pixels involved in super-resolving the highlighted patch within the red box. The numbers beneath are the DI (\u2191) [33] and PSNR (\u2191) values. Zoom in for better visual comparison.", "description": "This figure compares the super-resolution (SR) results of different state-of-the-art (SOTA) lightweight single image super-resolution (SISR) models with the proposed UPS model.  It highlights two example patches from different images.  For each patch, it displays the ground truth, results from SwinIR-light, NGSwin, and UPS.  Below each set of results is a quantitative comparison showing the distortion index (DI) and peak signal-to-noise ratio (PSNR).  The local attribution maps (LAMs) visually represent which pixels were most heavily used in the SR process by each model.", "section": "4 Unified Projection Sharing for Lightweight SISR"}, {"figure_path": "tacb2bFZcm/figures/figures_5_1.jpg", "caption": "Figure 2: Overview of Transformer-based architecture for lightweight SISR. There are three main components: (i) a head shallow feature extraction module, (ii) a deep feature extraction and aggregation (FEA) module consisting of N Swin Transformer layers (STL\u2081,\u2026\u2026, STLN), and (iii) a tail reconstruction module. Previous transformers (i.e., SwinIR [7], NGSwin [28]) synchronously perform multiple layer-specific deep image feature extraction (FE) and projection space (PS) optimization within a Swin Transformer Layer (STL). In contrast, we develop a decoupled Swin Transformer Layer (D-STL) in UPS to optimize per-layer feature extraction and a unified projection space (\u201cPSu\u201d defined by a learnable projection matrix UQ).", "description": "This figure compares the architectures of general transformer-based SISR methods, previous Swin Transformer-based methods, and the proposed UPS method.  It highlights the key difference: previous methods perform layer-specific optimization for both feature extraction and projection space, while UPS decouples these, using a unified projection space for all layers.  This decoupling is intended to improve efficiency and performance in lightweight settings.", "section": "4 Unified Projection Sharing for Lightweight SISR"}, {"figure_path": "tacb2bFZcm/figures/figures_7_1.jpg", "caption": "Figure 4: Qualitative comparison between LAPAR-A [18], SwinIR-light [7], NGSwin [28] and UPS (ours) on four popular benchmarks (BSD100 [11], Urban100 [40], Manga109 [41] and DIV2K [37].) under x4 setting. Our predictions present more detailed textures and fewer artifacts.", "description": "This figure compares the visual results of four different super-resolution models (LAPAR-A, SwinIR-light, NGSwin, and UPS) on four benchmark datasets (BSD100, Urban100, Manga109, and DIV2K) at a scaling factor of 4.  It highlights that the proposed UPS model produces images with more detailed textures and fewer artifacts compared to other models.", "section": "5.2 Comparison with SOTA Methods"}, {"figure_path": "tacb2bFZcm/figures/figures_8_1.jpg", "caption": "Figure 6: Impact of different similarity calculation methods. The left Table shows the quantitative results of employing different similarity calculation methods on the Urban100 [39] (\u00d72). The right figure gives a visual example to illustrate the SR results overlaid the LAM [33] maps of each model. The numbers beneath are the DI (\u2191) [33] and PSNR (\u2191) values.", "description": "This figure shows an ablation study on the impact of different similarity calculation methods in the UPS model. The left table presents quantitative results comparing four different methods: A (Matrix dot product + Softmax), B (Cosine + Softmax), C (Matrix dot product + ReLU), and D (Cosine + ReLU). The results are measured by PSNR and SSIM on the Urban100 dataset. The right side of the figure shows a visual comparison of the four methods on a sample image, displaying the super-resolution results overlaid with local attribution maps (LAM).  Method D (Cosine + ReLU) achieves the best performance, indicating the effectiveness of using the cosine similarity and ReLU activation for similarity calculation in the UPS model.", "section": "5.3 Ablation Studies"}, {"figure_path": "tacb2bFZcm/figures/figures_8_2.jpg", "caption": "Figure 7: Visual comparison of layer-specific projection optimization and our proposed UPS scheme. UPS achieves better similarity calculation and yields better image structural restoration.", "description": "This figure compares the layer-specific projection optimization method used in traditional Swin Transformers with the unified projection sharing (UPS) method proposed in the paper.  The top row shows the low-resolution (LR) input image and its ground truth (GT) high-resolution counterpart. The two middle sections visually represent the intermediate feature maps (X), similarity maps (S), and aggregated feature maps (Y) at two different layers (Layer 12 and Layer 23) of both the SwinIR-light baseline model and the UPS model. The comparison highlights how the UPS method, by employing a unified projection space, achieves better similarity calculation and, consequently, better reconstruction of the high-resolution image, particularly in terms of structural details.", "section": "3.1 Decomposing Swin Transformer Layer"}, {"figure_path": "tacb2bFZcm/figures/figures_14_1.jpg", "caption": "Figure 10: Comparison between several lightweight SISR models on Set14 [39] (\u00d74). (a) Evaluation of using different numbers of training samples. (b) Validation performance of different models.", "description": "This figure compares the performance of several lightweight Single Image Super-Resolution (SISR) models, namely SwinIR-S, LAPAR-A, LatticeNet, and UPS-S, under two different experimental conditions.  Subfigure (a) shows how the PSNR (Peak Signal-to-Noise Ratio) varies as the percentage of training images used changes.  Subfigure (b) plots the PSNR against training iterations (in steps of 5000), again demonstrating performance differences. The results show UPS-S outperforms other models in both data efficiency and training speed.", "section": "A.2 Data Efficiency"}, {"figure_path": "tacb2bFZcm/figures/figures_15_1.jpg", "caption": "Figure 11: Comparison between SwinIR and our proposed UPS under noise optimization setup. We also report visual examples of the \u2018input feature F1\u2019, \u2018input feature with noise (the noise std is set to 0.3) Fr\u2019, and \u2018output feature FN\u2019 enhanced by methods and corresponding final predictions.", "description": "This figure compares the performance of SwinIR and UPS under noisy input conditions.  Gaussian noise with a standard deviation of 0.3 is added to the input features. The left panel shows a graph comparing the PSNR values achieved by both models at different noise levels. The right panel displays visual examples showcasing the input features (with and without noise), the output features processed by each model, and the final super-resolution results.  It demonstrates that UPS is more robust to noisy inputs, maintaining better image quality compared to SwinIR.", "section": "A.3 Robustness Optimization of UPS"}, {"figure_path": "tacb2bFZcm/figures/figures_16_1.jpg", "caption": "Figure 12: Qualitative robustness comparison of different lightweight SISR models on out-of-domain degraded inputs under x4 setting. The first sample is from Set14 and the last two samples are from the Urban100 benchmark.", "description": "This figure compares the performance of SwinIR-light, NGswin, and UPS on three different types of degraded images (compression, blur, and noise) at a scaling factor of x4.  The results show that UPS consistently outperforms the other two models in terms of PSNR and SSIM, indicating its superior robustness to out-of-domain data.", "section": "5.2 Comparison with SOTA Methods"}]