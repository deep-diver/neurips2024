[{"figure_path": "VaJ4XOW7Ey/figures/figures_5_1.jpg", "caption": "Figure 1: Illustrative Toy Examples. The above figure details the three relevant variables x, y, and z that we will consider for our toy examples. Note the action a0 is interpreted as a1 or vice versa with a 10% failure probability, which is the same as the rate at which z switches regardless of the agent\u2019s actions. Irrelevant Variables Example: In this case, we consider only the variables x and y (z is not needed in this example) where the reward is +1 if x = x0 and 0 otherwise. The result is that variable x is relevant to the task whereas variable y is irrelevant. Independent Subtasks Example: In this case, we consider that the variable x evolves (according to the diagram) with y remaining constant when z = z0 and the variables y evolves with x remaining constant when z = z1. The reward is +1 if z = z0 and x = x0 or if z = z1 and y = y0, or 0 otherwise.", "description": "This figure shows two toy Markov Decision Processes (MDPs) used to illustrate how a policy's input impacts mixing time.  The first MDP demonstrates irrelevant variables, where only one variable (x) affects the reward, while the second demonstrates independent subtasks, where the reward depends on different subsets of variables (x and y) depending on the value of a third variable (z).  Each MDP is visualized as a graph where nodes are states and edges are transitions between states based on the action. The different scenarios are used to highlight the effect of context length and the mixing time.", "section": "Building an Intuition with Examples"}, {"figure_path": "VaJ4XOW7Ey/figures/figures_7_1.jpg", "caption": "Figure 1: Illustrative Toy Examples. The above figure details the three relevant variables x, y, and z that we will consider for our toy examples. Note the action a0 is interpreted as a1 or vice versa with a 10% failure probability, which is the same as the rate at which z switches regardless of the agent's actions. Irrelevant Variables Example: In this case, we consider only the variables x and y (z is not needed in this example) where the reward is +1 if x = x0 and 0 otherwise. The result is that variable x is relevant to the task whereas variable y is irrelevant. Independent Subtasks Example: In this case, we consider that the variable x evolves (according to the diagram) with y remaining constant when z = z0 and the variables y evolves with x remaining constant when z = z1. The reward is +1 if z = z0 and x = x0 or if z = z1 and y = y0, or 0 otherwise.", "description": "This figure illustrates two toy examples used in the paper to demonstrate the effect of context length on mixing time.  The first example shows how irrelevant state variables increase mixing time when included in the policy's input. The second demonstrates how independent subtasks can be handled more efficiently by conditioning on relevant variables only.", "section": "Building an Intuition with Examples"}, {"figure_path": "VaJ4XOW7Ey/figures/figures_7_2.jpg", "caption": "Figure 3: Mixing Times Encountered vs. Context Length. We plot the average mixing time and 95% confidence intervals encountered during 1 billion steps of learning over 100 seeds at each context length k. We bin the average mixing time computation by the nearest 0.1 increment of the reward rate of the policy. k = 1 is not visible because the reward rate is always 0.", "description": "The figure shows the relationship between the average mixing time and the context length (k) during online reinforcement learning.  The average mixing time is calculated for different context lengths (k=1, 2, 4, 10) and across various reward rates. The shaded areas represent 95% confidence intervals based on 100 independent runs. The plot demonstrates that longer context lengths generally lead to higher average mixing times, particularly at higher reward rates.  The absence of the k=1 line at higher reward rates is attributed to the fact that the reward rate never reaches higher values for this setting.", "section": "3.2 Empirical Verification During Online RL"}, {"figure_path": "VaJ4XOW7Ey/figures/figures_8_1.jpg", "caption": "Figure 7: Policy Architecture vs. Encountered Mixing Times. We plot the average mixing time for each choice of policy architecture between tabular, MLP, LSTM, and Transformer models with averages binned by the reward rate of the policy.", "description": "This figure shows the average mixing time for different reinforcement learning architectures (tabular, MLP, LSTM, and Transformer) across various reward rates.  The x-axis represents the average reward rate achieved by the agent, and the y-axis represents the average mixing time. Each line represents a different architecture, and the shaded area around each line represents the 95% confidence interval.  The figure is broken into subplots, each corresponding to a different context length (k). The results illustrate the trade-off between context length and mixing time, showing how longer context lengths can lead to increased mixing times, especially for neural network architectures. ", "section": "3.2 Empirical Verification During Online RL"}, {"figure_path": "VaJ4XOW7Ey/figures/figures_8_2.jpg", "caption": "Figure 5: Context Length vs. Training Accuracy. We plot the achieved training accuracy and 95% confidence intervals across 5 random seeds as a function of the Decision Transformer context length in the crossing environment with 1,000 episodes of data generated by either random behavior policies with a context length k = 1 or a REINFORCE based learning agent using a context length k = 1.", "description": "This figure shows the training accuracy of Decision Transformer models with varying context lengths.  The data used for training was generated by two types of policies: random policies with a context length of 1 and learning policies (REINFORCE) also with a context length of 1. The plot demonstrates the relationship between context length used by the model during training and the resulting training accuracy. It shows that longer context lengths are needed to achieve optimal training accuracy, especially when the training data comes from learning policies.", "section": "Understanding Growing Context Lengths in Foundation Models for RL"}, {"figure_path": "VaJ4XOW7Ey/figures/figures_21_1.jpg", "caption": "Figure 1: Illustrative Toy Examples. The above figure details the three relevant variables x, y, and z that we will consider for our toy examples. Note the action a0 is interpreted as a1 or vice versa with a 10% failure probability, which is the same as the rate at which z switches regardless of the agent's actions. Irrelevant Variables Example: In this case, we consider only the variables x and y (z is not needed in this example) where the reward is +1 if x = x0 and 0 otherwise. The result is that variable x is relevant to the task whereas variable y is irrelevant. Independent Subtasks Example: In this case, we consider that the variable x evolves (according to the diagram) with y remaining constant when z = z0 and the variables y evolves with x remaining constant when z = z1. The reward is +1 if z = z0 and x = x0 or if z = z1 and y = y0, or 0 otherwise.", "description": "This figure presents two simple MDP examples used to illustrate the impact of irrelevant variables and independent subtasks on mixing times.  The first example shows an MDP with two state variables, x and y, where only x affects the reward. The second example has three state variables (x, y, and z), where the reward depends on which subtask is active (determined by z), and each subtask involves only one of the other variables (x or y). The diagrams show the state transitions and rewards.", "section": "Building an Intuition with Examples"}, {"figure_path": "VaJ4XOW7Ey/figures/figures_23_1.jpg", "caption": "Figure 7: Policy Architecture vs. Encountered Mixing Times. We plot the average mixing time for each choice of policy architecture between tabular, MLP, LSTM, and Transformer models with averages binned by the reward rate of the policy.", "description": "This figure shows the average mixing times encountered during online reinforcement learning experiments using different policy architectures (tabular, MLP, LSTM, Transformer) for varying context lengths (2, 3, 4, 5, 7, 10).  The average mixing time is calculated and binned by reward rate. The results illustrate the tradeoff between context length and mixing time, particularly for Transformer-based models.", "section": "3.2 Empirical Verification During Online RL"}, {"figure_path": "VaJ4XOW7Ey/figures/figures_24_1.jpg", "caption": "Figure 8: Transformers: Average Encountered Mixing Time vs. Number of Parameters. We plot the average encountered mixing time during learning for Transformer models trained at a number of different model sizes regulated by the number of hidden units. Averages are taken over 100 random seeds and we also provide 95% confidence intervals.", "description": "This figure shows how the average mixing time changes as a function of the number of parameters in the Transformer model for different context lengths (k=2 and k=10).  The shaded area represents the 95% confidence interval, showing variability across 100 different random seeds. It illustrates the relationship between model capacity and mixing time, suggesting that larger models, while potentially achieving higher performance,  may experience longer mixing times.", "section": "3.2 Empirical Verification During Online RL"}, {"figure_path": "VaJ4XOW7Ey/figures/figures_26_1.jpg", "caption": "Figure 9: Decision Transformers Context Length vs. Reward During Evaluation. We highlight learning results for Decision Transformers in the crossing environment with data generated from random behavior policies with a context length k of 1. This figure details the evaluation performance when prompting each policy with 100 return to go values stepping from 0 to 1.0. This is also compared to the actual distribution of performance for the behavior policies that generated the data. Each line color represents a different context length k for the Decision Transformer model with the training accuracy rounded to the nearest percent included in parenthesis.", "description": "This figure compares the performance of Decision Transformers with different context lengths (k) on a crossing environment. The models were trained on data generated from random behavior policies (k=1).  The evaluation involved prompting the models with various return-to-go values (0 to 1.0) and measuring their reward rate. The figure shows how increasing the context length improves the model's ability to match the performance of the behavior policies, although longer context lengths may also lead to overfitting.", "section": "Understanding Growing Context Lengths in Foundation Models for RL"}]