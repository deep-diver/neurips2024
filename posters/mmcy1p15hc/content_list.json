[{"type": "text", "text": "Intrinsic Robustness of Prophet Inequality to Strategic Reward Signaling ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Wei Tang\u2217 Haifeng Xu\u2020 Ruimin Zhang\u2021 Derek Zhu\u00a7 ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Prophet inequality concerns a basic optimal stopping problem and states that simple threshold stopping policies \u2014 i.e., accepting the first reward larger than a certain threshold \u2014 can achieve tight $^1/2$ -approximation to the optimal prophet value. Motivated by its economic applications, this paper studies the robustness of this approximation to natural strategic manipulations in which each random reward is associated with a self-interested player who may selectively reveal his realized reward to the searcher in order to maximize his probability of being selected. ", "page_idx": 0}, {"type": "text", "text": "We say a threshold policy is $\\alpha$ (-strategically)-robust if it (a) achieves the $\\alpha$ - approximation to the prophet value for strategic players; and (b) meanwhile remains a $^1/2$ -approximation in the standard non-strategic setting. Starting with a characterization of each player\u2019s optimal information revealing strategy, we demonstrate the intrinsic robustness of prophet inequalities to strategic reward signaling through the following results: (1) for arbitrary reward distributions, there is a threshold policy that is $\\frac{1\\!-\\!1/e}{2}$ -robust, and this ratio is tight; (2) for i.i.d. reward distributions, there is a threshold policy that is $^1/2$ -robust, which is tight for the setting; and (3) for log-concave (but non-identical) reward distributions, the $^1/2$ -robustness can also be achieved under certain regularity assumptions.5 ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The prophet inequality of Krengel and Sucheston [37] is a foundational framework for the theory of optimal stopping problems and sequential decision-making. In the classic prophet inequality, a searcher faces a finite sequence of non-negative, real-valued and independent random variables $X_{1},\\ldots,X_{N}$ with known distributions $H_{i}$ from which a reward of value $X_{i}$ is drawn sequentially for $i=1,\\ldots,N$ . Once a random reward is realized, the searcher decides whether to accept the realized reward and stop searching, or reject the reward and proceed to the next reward. The searcher\u2019s objective is to maximize the value of the accepted reward. The performance of the searcher\u2019s stopping policy is evaluated against a prophet value which equals to the ex-post maximum realized reward. The classic and elegant result of Samuel-Cahn [42] showed that a simple static threshold policy achieves at least half of the prophet value, and surprisingly, this bound is the best possible even among dynamic policies. Samuel-Cahn\u2019s policy uses the threshold that is the median of the distribution of the highest prizes, and then accepts the first realized reward that exceeds this threshold. The existence of this $^1/2$ -approximation is now known as the prophet inequality. ", "page_idx": 0}, {"type": "text", "text": "Recently, there is a regained interest of the prophet inequality due to its beautiful connection to online mechanism design (see, e.g., [31, 16]), and many different settings of the prophet inequality have been studied (see the survey by [40, 19]). For example, Kleinberg and Weinberg [36] extend the prophet inequality to all matroid constraints and similar to [42], they also show that a threshold stopping policy with the threshold equal to half of the expected maximum reward can also lead to the optimal $^1/2$ -approximation. Indeed, it is now well-known that there exists a range of thresholds for the classic prophet inequality that can achieve the optimal $^1/2$ -approximation (see Definition 2.2). ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "An important assumption in the classic prophet inequality is that the distribution of each random variable is an inanimate object and, once the searcher reaches it, it will fully disclose its realized reward to the searcher. Yet this may not be the case in many real-world applications where each distribution may often be associated with a strategic player6 who may have incentives to selectively disclose information to maximize his own probability of being chosen by the searcher. This is usually the case when information is not controlled by nature but by humans or algorithms. Such examples are ample in economic activities. For instance, when a recruiter searches for the best candidate for a position by sequentially interviewing a set of job applicants, each job applicant is naturally a strategic player and would want to control how much information they disclose about their characteristics (including strengths, weaknesses, personality, and experience) to the recruiter (the searcher) so as to maximize the probability of being hired. Similarly, when a venture capitalist searches for the most promising startup to invest by sequentially visiting each startup, the startups are strategic players. They have more accurate information about their own potential and can control how much of this information they disclose to attract the investment from the venture capitalist (the searcher). Finally, in the real estate market, when a buyer searches for the most attractive property to purchase, the property sellers are strategic players who can control the amount of information they disclose about the property\u2019s condition and potential returns to the buyer (the searcher). ", "page_idx": 1}, {"type": "text", "text": "Motivated by real-world applications like above, we introduce and study a natural variant of the prophet inequality where each reward distribution is associated with a strategic player who can decide what information about the realized reward will be disclosed to the searcher. To capture such partial information revelation, we adopt the standard framework of information design (also known as Bayesian persuasion [35, 10]) and assume that each player can selectively disclose reward information by implementing an information strategy \u2013 often referred to as an experiment or signaling scheme [35, 10] \u2013 which stochastically maps the realized reward (unobservable to the searcher) to a random signal (observable to the searcher). For the motivating applications above, such revelation strategies encode answers to standard interview questions (e.g., experiences, behavioral questions) prepared in advance by job candidates, the presentations prepared by startups for the VC, or the property\u2019s brochures prepared by property sellers. Each player aims to maximize his probability of being chosen by the searcher, leading to a (constant-sum) competition among them. ", "page_idx": 1}, {"type": "text", "text": "Our Results. In this work, we stand in the searcher\u2019s shoes and look to understand how robust the classic prophet inequalities are to the above strategic player behaviors. Like most previous work in this space, we restrict our attention to threshold stopping policies. We start by characterizing players\u2019 optimal information revealing strategies. Given any threshold stopping policy with threshold $T$ , the optimal information strategy of player $i$ with prior reward distribution $H_{i}$ has the following clean threshold structure (albeit using a threshold different from $T$ ): there exists a reward cutoff $t_{i}$ such that player $i$ simply reveals whether $X_{i}\\geq t_{i}$ or $X_{i}<t_{i}$ . Moreover, $t_{i}$ satisfies $\\mathbb{E}[X_{i}|X_{i}\\geq t_{i}]=T$ . In words, player $i$ simply pools all the \u201cgood rewards\u201d together to make their expectation barely pass the threshold $T$ .7 This characterization allows us to reduce players\u2019 strategic behaviors to a related prophet inequality problem with binary reward supports. Our later analysis hence only needs to further investigate how well the thresholds for classic prophet inequalities perform in this (related) additional problem. ", "page_idx": 1}, {"type": "text", "text": "Armed with the above characterization, next we turn to analyze the intrinsic robustness of the classic prophet inequality. We say a threshold stopping policy with threshold $T$ is $\\alpha($ (-strategically)- robust if it retains the $^1/2$ -approximation in the classic setting and meanwhile also can achieve $\\alpha$ - approximation in the strategic scenario when all the players optimally reveal information. Our first main result is that, for arbitrary reward distributions, the threshold policy with $T$ equaling the half expected max threshold of Kleinberg and Weinberg [36] is 1\u221221/ -robust (Theorem 4.1). Moreover, this competitive ratio of $\\frac{1-^{1}/e}{2}\\approx0.316$ is the best among all known threshold policies that can secure the $^1/2$ -approximation in the classic non-strategic setting (Proposition 4.2). This suggests that this well-studied threshold policy can also perform robustly well even when players are all strategic, illustrating the intrinsic robustness of the classic prophet inequality. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "When the reward distributions are identical \u2013 referred to as IID distributions which have been extensively studied in literature [41, 18, 6, 30, 1, 33] \u2013 we show that there exists a threshold in the range of known thresholds (Definition 2.2) that is $^1/2$ -robust (Theorem 5.1). Moreover, this $^1/2$ approximation ratio is optimal among all possible threshold stopping policies for any IID distributions (Proposition 5.2). Finally, when the reward distributions are not identical but log-concave, under certain regularity conditions we show that any threshold between the expected max and the median of the highest reward [42] is $^1/2$ -robust (Theorem 5.4). Note that log-concave reward distributions have been considered in previous works on prophet inequality (see, e.g., [6, 18]); moreover, it is satisfied by a wide range of distributions (e.g., normal, uniform, Gamma, Beta, Laplace, etc., [8]) and is a widely adopted assumption in algorithmic game theory (see, e.g., [15]). ", "page_idx": 2}, {"type": "text", "text": "Additional Related Work. Prophet inequality is a fundamental problem in optimal stopping theory which was introduced in the 70s [38, 37]. Recently there has been a growing interest in prophet inequalities, generalizing the problem to different settings [2, 7, 18, 21, 22, 36, 17, 4, 6, 20]. Our discussion here cannot do justice to its rich literature; hence, we refer interested readers to the recent survey by Lucier [40], Correa et al. [19] for comprehensive overviews of recent developments and its connections to economic problems. This work takes an informational perspective and associate each distribution in the prophet inequality with a strategic player that strategically signals reward information to the searcher. We follow the information design literature [35, 10] and allow players to design signaling schemes to influence the searcher\u2019s beliefs about the realized rewards. In our considered game, players are competing with each other for the selection of the searcher. The players\u2019 game thus also shares similarity with competitive information design [26, 28, 5, 23, 34]. ", "page_idx": 2}, {"type": "text", "text": "Conceptually, our work also relates to the rapidly growing literature on strategic machine learning (see, e.g., [45, 32, 29, 24, 43, 46, 9]), which studies learning from strategic data providers. We also study similar strategic reward providers, albeit in a different algorithmic problem, i.e., optimal stopping. More generally, our work subscribes to the literature on information design in sequential decision-making. In particular, our paper relates to the recently increased interests on using online learning approaches to study the regret minimization when an information-advantaged player repeatedly interacts with an information-disadvantaged player [11, 14, 25, 44, 47] without knowing their preferences. Instead of focusing on regret minimization, we use the competitive ratio (as conventionally used in prophet inequalities) to measure the searcher\u2019s policy. It is worth mentioning that [30] also studies strategic information revealing in prophet inequality problems. In their setting, a centralized player strategically discloses reward information to the searcher, while players in our setting form a decentralized game and each player acts on his own to maximize his payoff. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminary ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we first revisit the formulation of the classic prophet inequality problem, and then formally introduce our setting as its natural variant where each distribution is associated with a strategic player who will be strategically signaling their reward information. ", "page_idx": 2}, {"type": "text", "text": "Classic prophet inequality. In standard settings, a searcher faces a finite sequence of known distributions $H_{1:N}\\triangleq(H_{i})_{i\\in[N]}$ of $N$ non-negative independent random variables. The outcomes (i.e., the rewards) $X_{i}\\,\\sim\\,H_{i}^{\\mathrm{~8~}}$ for $i\\,\\in\\,[N]$ are revealed sequentially to the searcher for $i\\,=\\,1,\\ldots,N$ . Let $\\lambda_{i}\\triangleq\\mathbb{E}_{H_{i}}[X_{i}]$ denote the mean reward for distribution $H_{i}$ . Upon seeing a reward, the searcher decides whether to accept the observed reward and stop the process, or irrevocably move on to the next reward. The searcher\u2019s goal is to maximize the expected accepted reward. The searcher\u2019s expected payoff cannot be more than that of a prophet who knows in advance the realizations of the rewards, namely, $X_{1},\\allowbreak\\cdot\\cdot,X_{N}$ . We denote by $\\mathsf{O P T}\\triangleq\\mathbb{E}_{H_{1},\\ldots,H_{N}}\\left[\\operatorname*{max}_{i\\in[N]}X_{i}\\right]$ the prophet value. Given a stopping policy $q$ , let $X^{(q)}$ be the accepted reward. The stopping policy $q$ is said to be $\\alpha$ -approximate (of the prophet value) if the following holds for the searcher\u2019s expected payoff for ", "page_idx": 2}, {"type": "text", "text": "any input distributions $H_{1:N}$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Big[X^{(q)}\\Big]\\geq\\alpha\\cdot0\\mathsf{P T}\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The above statement is often referred to as the prophet inequality. For any stopping policy $q$ , the largest constant $\\alpha\\in(0,1]$ that satisfies the above inequality is named the competitive ratio. Classic results of [38, 37] elegantly show that there exist simple threshold stopping policies that achieve the competitive ratio $^1/2$ , and moreover the ratio of $^1/2$ is tight.9 ", "page_idx": 3}, {"type": "text", "text": "Definition 2.1 (Threshold Stopping Policy). A threshold stopping policy is an online algorithm which pre-computes a threshold value $T$ as a function of the distributions $H_{1:N}$ , and then accepts the first reward $X_{i}$ whose value is no smaller than the threshold, i.e., $X_{i}\\geq T$ . ", "page_idx": 3}, {"type": "text", "text": "It is well-known that multiple thresholds can achieve the optimal $^1/2$ -approximation ratio. ", "page_idx": 3}, {"type": "text", "text": "Definition 2.2 $^{1/2}$ -approximation Threshold Spectrum). Let $T_{\\mathsf{K W}}$ \u225c $\\mathbb{E}[\\operatorname*{max}_{i}\\,X_{i}]/_{2}$ (Kleinberg and Weinberg $[36],$ ), let $T^{*}$ satisfy $\\begin{array}{r l r}{T^{*}\\!}&{{}=}&{\\!\\sum_{i\\in[N]}\\mathbb{E}[(X_{i}-T^{*})^{+}],}\\end{array}$ , and let $T_{\\mathsf{S C}}\\;\\;\\triangleq\\;\\;\\operatorname{sup}\\{t\\;\\;:$ $P r[\\operatorname*{max}_{i}X_{i}\\geq t]\\implies1/2\\right\\}$ (Samuel-Cahn $[42],$ . Then any threshold stopping policy with threshold $T\\in[T_{\\mathsf{K W}},\\operatorname*{max}\\{T_{\\mathsf{S C}},T^{*}\\}]$ ] guarantees $^1/2$ -approximation to the prophet value.10 ", "page_idx": 3}, {"type": "text", "text": "Prophet inequality with strategic reward signaling. In the strategic setting, each distribution $H_{i}$ may be associated with a strategic player that governs how much information about the realized reward he would like to reveal to the searcher once the searcher reaches his distribution. Formally, upon arriving at the reward distribution $H_{i}$ , the searcher does not directly observe the realized reward $X_{i}\\sim\\;H_{i}$ . Instead, she observes an information signal, designed by the player $i$ with distribution $H_{i}$ , that is correlated with the reward $X_{i}$ . We follow the literature in information design [35] to model this strategic reward signaling: Each player $i$ chooses a signaling scheme $\\phi_{i}(\\cdot\\mid x)\\,\\overline{{\\in}}\\,\\Delta(\\Sigma_{i})$ , where $\\Sigma_{i}$ is a measurable signal space and $\\bar{\\phi_{i}}(\\bar{\\sigma}\\mid x)\\in[0,1]$ specifies the conditional probability of a signal $\\sigma\\in\\Sigma_{i}$ that will be sent to the searcher when the reward $X_{i}=x\\sim H_{i}$ is realized. Notice that, upon seeing a signal $\\sigma\\,\\sim\\,\\phi_{i}(\\cdot\\,\\mid\\,x)$ , together with the prior information $H_{i}$ from which the reward is realized, the searcher can update her Bayesian belief about the underlying realized value $X_{i}$ , and then decides whether to stop and choose player $i$ , or reject $i$ to continue her search. ", "page_idx": 3}, {"type": "text", "text": "Each player $i$ is competing with each other for the final selection from the searcher. Namely, a player obtains payoff 1 if his reward is accepted and payoff 0 if his reward is not accepted.11 Each player\u2019s goal is to design an information revealing strategy that maximizes his probability of being chosen by the searcher. We below give two simple examples of information revealing strategies. ", "page_idx": 3}, {"type": "text", "text": "Example 2.3 (Examples of Information Strategies). $(I)$ No information strategy: the signal is completely uninformative (e.g., the distribution $s\\phi_{i}(\\cdot\\mid x)$ is a Dirac delta function on a single signal, i.e., $|\\Sigma_{i}|=1_{.}$ ), hence the searcher infers an expected reward of $\\lambda_{i}=\\mathbb{E}[X_{i}]$ as her perceived reward from player $i$ ; (2) Full information revealing strategy: the signal perfectly reveals player i\u2019s value to the searcher (i.e., $\\phi_{i}(\\sigma\\equiv x\\mid x)=1$ for every realized $X_{i}=x$ , and $\\Sigma_{i}=\\mathsf{s u p p}(H_{i}).$ ) ", "page_idx": 3}, {"type": "text", "text": "In this work, standing in the searcher\u2019s shoes, we are interested in how threshold stopping policies perform under players\u2019 optimal strategic reward signaling. ", "page_idx": 3}, {"type": "text", "text": "Game Timeline. The timeline of our prophet inequality with strategic players problem, where the searcher employs a threshold stopping policy, can be detailed as follows: (1) Knowing $H_{1:N}$ , the searcher first announces a threshold stopping policy with threshold $T$ that is a function of $H_{1:N}$ ; (2) Knowing threshold $T$ , each player then picks a signaling scheme (also known as an experiment in economics literature [35, 10]) to reveal partial information about the underlying reward; and (3) The searcher learns all players\u2019 information strategies, and then conducts a search based on her threshold stopping policy with threshold $T$ (i.e., accepting the first player whose posterior mean of his reward distribution, given his revealed information signal, exceeds the threshold $T$ ). The assumption that the searcher knows information strategies as well as realized signals is commonly adopted in the information design literature [35, 10], and is also well motivated for the domains of our interest. For instance, when startups persuade VCs or property sellers persuade buyers, the signaling scheme could correspond to startups\u2019 product exhibitions or sellers\u2019 promotion brochures which determine what information the searcher could see. Misreporting realized signals corresponds to revealing untrue information, which not only violates regulation policies and but also causes the players to lose credibility in the long term. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Slightly abusing the notation, we also use $X^{(q)}$ to denote the accepted reward given the searcher\u2019s stopping policy $q$ under the strategic reward signaling, and let $u^{\\mathsf{s}}(q)\\triangleq\\mathbb{E}\\!\\left[X^{(q)}\\right]$ be the searcher\u2019s expected payoff. Notice that here the expectation is not only over the randomness of the distributions $H_{1:N}$ , but also the information strategies $\\{\\phi_{i}(\\cdot\\mid x)\\}$ . Anticipating the players\u2019 strategic behavior, the searcher wants a stopping policy that can still guarantee a good performance competing against the prophet value OPT. ", "page_idx": 4}, {"type": "text", "text": "As mentioned earlier, we are particularly interested in how previously studied threshold stopping policies described in Definition 2.2 perform under the players\u2019 strategic reward signaling. To formalize our goal, we introduce the following notion of strategic robustness. ", "page_idx": 4}, {"type": "text", "text": "Definition 2.4 $\\mathcal{\\ N}($ (-strategically)-robust Stopping Policies). A stopping policy $q$ is $\\alpha$ -robust if it achieves $\\alpha$ -approximation to the OPT when players are strategically signaling their rewards, and it remains a $^1/2$ -approximation in the standard non-strategic setting. ", "page_idx": 4}, {"type": "text", "text": "3 Warm-up: Characterizing Optimal Information Revealing Strategy ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We start our analysis by showing that when the searcher adopts a threshold stopping policy (Definition 2.1), each player\u2019s optimal information revealing strategy admits clean characterizations. ", "page_idx": 4}, {"type": "text", "text": "Proposition 3.1 (Optimal Information Revealing Strategy). Given a threshold stopping policy as in Definition 2.1 with threshold $T$ , for each player $i$ : ", "page_idx": 4}, {"type": "text", "text": "\u2022 if $T\\leq\\lambda_{i}$ , then player i\u2019s optimal information revealing strategy is the no information strategy; \u2022 if $T>\\lambda_{i}$ , then player i\u2019s optimal information revealing strategy is threshold signaling and determined by a cutoff $t_{i}$ that satisfies $\\begin{array}{r}{\\bar{T}=\\mathbb{E}[X_{i}|X_{i}\\geq t_{i}]=\\int_{t_{i}}^{\\infty}\\overset{\\cdot\\cdot}{x}\\,\\mathrm{d}H_{i}(x)/(1-\\bar{H}_{i}(t_{i}))}\\end{array}$ . That is, player i\u2019s optimal signaling scheme sends one of two signals: $X_{i}\\geq t_{i}$ or $X_{i}<t_{i}$ .12 ", "page_idx": 4}, {"type": "text", "text": "We highlight the intuition behind Proposition 3.1 below. Under a threshold stopping policy, every player maximizes his utility by maximizing the probability that the signal\u2019s posterior expected reward is at least $T$ (hence is selected). When the stopping threshold $T\\leq\\lambda_{i}$ , this probability is 1 and hence maximized when player $i$ simply reveals no information. When $T>\\lambda_{i}$ , this probability is maximized when player $i$ blends the highest rewards together to form a posterior mean just equal to $T$ , which is exactly the scheme described in Proposition 3.1. ", "page_idx": 4}, {"type": "text", "text": "Remark 3.1 (Addressing Point Masses). For ease of presentation, Proposition 3.1 assumes the distribution $H_{i}$ is continuous. When $H_{i}$ has point masses, Proposition 3.1 still holds, but with a more subtle description of the pooling cutoff $t_{i}$ . We provide more detailed discussions in the full version of the paper. ", "page_idx": 4}, {"type": "text", "text": "Remark 3.2. In game-theoretic terminology, Proposition 3.1 characterizes the subgame Perfect Nash equilibrium (SPNE) for the multi-player sequential game induced by any threshold stopping policy that the searcher commits to. This SPNE happens to enjoy simple structures; indeed, each player\u2019s equilibrium strategy is only a function of the threshold $T$ and not on other players\u2019 strategies or their order. This clean characterization is a consequence of the simple structure of (static) threshold policies. If the searcher\u2019s stopping policy is instead dynamic (i.e., allowing the decision of player i to depend on previously realized rewards), SPNE is also well-defined but will adopt significantly more complex structures. While analyzing the SPNE under these dynamic policies may also be interesting, it is beyond the scope of this work since our focus is to study the power of static threshold policies, which is also a central theme in the study of prophet inequalities. Finally, we note that the optimal strategies characterized in Proposition 3.1 are not unique but they all result in the same utility for every player. This is because player $i$ is actually indifferent on how to disclose the reward when $X_{i}\\leq t_{i}$ , leading to many different but utility-equivalent information strategies. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Equivalent Representation of Optimal Information Revealing Strategy. When $T>\\lambda_{i}$ , player $i$ \u2019s optimal information revealing strategy described in Proposition 3.1 pools all the rewards $X_{i}\\sim H_{i}$ above $t_{i}$ together to forge a conditional mean value of $T$ , and pools the remaining smaller rewards into another signal. We hence also refer to $t_{i}$ as the pooling cutoff. This threshold signaling scheme can be equivalently represented as a binary-support distribution $G_{i}$ supported on two realizations corresponding to the two signals respectively: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{x\\sim G_{i}}[x=T]=1-H_{i}(t_{i}),\\quad\\operatorname*{Pr}_{x\\sim G_{i}}[x=a_{i}]=H_{i}(t_{i}){\\mathrm{~where~}}a_{i}\\triangleq{\\frac{\\lambda_{i}-T(1-H_{i}(t_{i}))}{H_{i}(t_{i})}}\\ .\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In the literature of information design, this distribution $G_{i}$ is also known as a mean-preserving contraction of prior reward distribution $H_{i}$ [27, 12, 13]. ", "page_idx": 5}, {"type": "text", "text": "Viewing the player $i$ \u2019s optimal information strategy as the distribution $G_{i}$ , one can simplify the interaction between the searcher and player $i$ as the following when the searcher visits player $i$ : a random reward $X_{i}\\sim G_{i}$ is realized, and the searcher stops if and only if $X_{i}\\,\\geq\\,T$ . With this observation, one can also reduce the original interaction to the following simplified protocol: (1) the searcher first decides a stopping threshold $T$ ; (2) each player $i$ chooses the distribution $G_{i}$ as in Equation (1) according to his prior $H_{i}$ and the threshold $T$ ;13 and (3) the searcher visits each $G_{i}$ sequentially and stops when the first realized reward $X_{i}\\geq T$ where $X_{i}\\sim G_{i}$ . ", "page_idx": 5}, {"type": "text", "text": "We emphasize that in the above reduction, given a threshold stopping policy with threshold $T$ , the searcher\u2019s expected payoff under the strategic reward signaling can be computed as $u^{\\mathsf{s}}(T)=$ $\\mathbb{E}\\big[X^{(q)}\\big]$ where the expectation is over distributions $\\boldsymbol{G}_{1:N}$ and $X^{(q)}$ is the first realized $X_{i}\\sim G_{i}$ such that $X^{(q)}\\geq T$ . ", "page_idx": 5}, {"type": "text", "text": "4 Achieving $\\frac{1{-}1/e}{2}$ -robustness for Arbitrary Distributions ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we show that for any distributions $H_{1:N}$ , there exists a 1\u221221/e(-strategically)-robust threshold stopping policy using a threshold within the spectrum in Definition 2.2. The main result in this section is stated below. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.1. For any distributions $H_{1:N}$ , a threshold stopping policy with threshold $T=T\\mathsf{k w}$ is 1\u221221/e-robust.t ", "page_idx": 5}, {"type": "text", "text": "From Definition 2.2, we know that using the threshold stopping policy with threshold $T_{\\mathsf{K W}}$ can achieve the optimal $^1/2$ -approximation in the classic prophet inequality for any distributions $H_{1:N}$ . Theorem 4.1 above shows another desired property of the threshold $T_{\\mathsf{K W}}$ : it can achieve $\\frac{1\\!-\\!1/\\!e}{2}$ - approximation even when distributions $H_{1:N}$ are strategically signaling their rewards, thus establishing its $\\frac{1\\!-\\!1/\\!e}{2}$ -robustness. Given the optimality of the threshold $T_{\\mathsf{K W}}$ in the non-strategic setting, it would be intriguing to ask whether this threshold $T_{\\mathsf{K W}}$ can also achieve $^1/2$ -approximation under the strategic setting, or if there exists a threshold within the spectrum in Definition 2.2 that can achieve $^1/2$ -approximation under the strategic setting. Below we show that the answer is No. In fact, any threshold stopping policy using a threshold from the spectrum in Definition 2.2 cannot achieve ( 1\u221221/e+ \u03b5)-approximation for any \u03b5 > 0 under strategic reward signaling. ", "page_idx": 5}, {"type": "text", "text": "Proposition 4.2 (Tightness of Theorem 4.1). There exist distributions $H_{1:N}$ such that no threshold from the spectrum in Definition 2.2 can achieve \u03b1-robustness where \u03b1 > 1\u2212(1\u22121/(2N\u22121))N\u22121. Notice that limN\u2192\u221e 1\u2212(1\u22121/(2N\u22121))N\u22121 = 1\u221221/e. Thus, the above Proposition 4.2 establishes the tightness of the results in Theorem 4.1. ", "page_idx": 5}, {"type": "text", "text": "Remark 4.1. We point out a subtlety in the above lower bound, which leads to an intriguing open problem. Proposition 4.2 shows that any threshold within the spectrum in Definition 2.2 cannot achieve $\\left({\\frac{1-1/e}{2}}+\\varepsilon\\right)$ -approximation under strategic reward signaling. However, this does not rule out the possibility of having a threshold outside that spectrum that achieves $^1/2$ -robustness. This is an interesting open question to resolve, though necessarily challenging since it is even already quite non-trivial to prove $^1/2$ -approximation in the non-strategic setting for thresholds outside the spectrum of Definition 2.2, let alone achieving $^1/2$ -approximation simultaneously in both worlds. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Theorem 4.1 holds for all distributions, regardless of being discrete or continuous. In the remainder of this subsection, we present the proof of Theorem 4.1 only for the continuous distribution case. The proof of this case carries our core ideas but is cleaner to present. ", "page_idx": 6}, {"type": "text", "text": "4.1 (Partial) Proof of Theorem 4.1: The Case of Continuous Distributions ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Our proof starts by upper bounding the prophet value OPT.14 ", "page_idx": 6}, {"type": "text", "text": "Lemma 4.3 (Upper Bounding OPT via Pooling Cutoffs). Given a threshold stopping policy with threshold $T$ , let $t_{i}$ be the pooling cutoff for each player $i$ defined as in Proposition 3.1. Let $I$ \u225c arg $\\operatorname*{max}_{i\\in[N]}t_{i}$ , then we have $\\begin{array}{r}{\\mathsf{O P T}\\leq H_{I}(t_{I})t_{I}+\\sum_{i}T(1-H_{i}(t_{i}))}\\end{array}$ . ", "page_idx": 6}, {"type": "text", "text": "Proof of Lemma 4.3. Let us fix a threshold $T$ , and let $t_{i}$ be the pooling cutoff for player $i$ defined in Proposition 3.1. Define $b_{i}\\triangleq(X_{i}-t_{i})^{+}$ . By definition, for each $i$ , we have $X_{i}\\leq t_{i}+b_{i}$ . Thus, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathsf{O P T}=\\mathbb{E}_{H_{1:N}}\\Bigl[\\underset{i}{\\operatorname*{max}}\\,X_{i}\\Bigr]\\,\\le\\underset{i}{\\operatorname*{max}}\\,t_{i}+\\displaystyle\\sum_{i}\\mathbb{E}_{H_{i}}[b_{i}]}&\\\\ {\\overset{(a)}{\\le}\\underset{i}{\\operatorname*{max}}\\,t_{i}+\\displaystyle\\sum_{i}(T-t_{i})\\cdot\\bigl(1-H_{i}(t_{i})\\bigr)}&\\\\ {\\overset{(b)}{\\le}t_{I}H_{I}(t_{I})+\\displaystyle\\sum_{i}T(1-H_{i}(t_{i}))\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where inequality (a) follows from the definition of pooling cutoff $t_{i}$ in Proposition 3.1 and inequality (b) follows from the definition of $I$ , $t_{i}\\geq0$ , and rearranging the terms. \u53e3 ", "page_idx": 6}, {"type": "text", "text": "With the above upper bound of prophet value, we have the following results: ", "page_idx": 6}, {"type": "text", "text": "Lemma 4.4. Let $T^{\\dagger}$ satisfy $\\begin{array}{r}{\\prod_{i=1}^{N}H_{i}(t_{i}^{\\dagger})\\,=\\,(\\frac{N-1}{N})^{N}}\\end{array}$ where $t_{i}^{\\dagger}$ is defined in Proposition 3.1 with threshold $T^{\\dagger}$ , then searcher\u2019s expected payoff $\\begin{array}{r}{u^{\\mathrm{s}}(T^{\\dagger})\\geq T^{\\dagger}\\left(1-\\prod_{i}H_{i}(t_{i}^{\\dagger})\\right)\\geq\\frac{1-(\\frac{N-1}{N})^{N}}{2}}\\end{array}$ \u00b7OPT.15 Proof of Lemma 4.4. Given a stopping threshold $T$ , by Proposition 3.1, we can lower bound the searcher\u2019s expected payoff as follows: $\\begin{array}{r}{u^{s}(T)\\;\\geq\\;T\\,\\cdot\\,\\Big(1-\\prod_{i=1}^{N}H_{i}(t_{i})\\Big)}\\end{array}$ . Thus, together with ", "page_idx": 6}, {"type": "text", "text": "Lemma 4.3, we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{u^{s}(T)}{\\mathrm{OPT}}\\geq\\frac{T\\cdot\\big(1-\\prod_{i=1}^{N}H_{i}(t_{i})\\big)}{\\mathrm{OPT}}}\\\\ &{\\qquad\\stackrel{(a)}{\\geq}\\frac{T\\cdot\\big(1-\\prod_{i=1}^{N}H_{i}(t_{i})\\big)}{t_{i}H_{i}(t_{i})+\\sum_{i}T(1-H_{i}(t_{i}))}}\\\\ &{\\qquad\\stackrel{(b)}{\\geq}\\frac{1-\\prod_{i=1}^{N}H_{i}(t_{i})}{H_{i}(t_{i})+\\sum_{i}(1-H_{i}(t_{i}))}}\\\\ &{\\qquad\\stackrel{(c)}{\\geq}\\frac{1-\\prod_{i=1}^{N}H_{i}(t_{i})}{N+1-\\sum_{i}H_{i}(t_{i})}}\\\\ &{\\qquad\\stackrel{(d)}{\\geq}\\frac{1-\\prod_{i=1}^{N}H_{i}(t_{i})}{N+1-(\\prod_{i=1}^{N}H_{i}(t_{i}))^{\\frac{1}{N}}}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where inequality (a) is by Lemma 4.3; inequality (b) is by the fact that $t_{i}\\,\\leq\\,T$ for all $i\\,\\in\\,[N]$ ; inequality (c) is due to $\\dot{H_{I}}(t_{I})\\leq1$ ; and inequality (d) is by the AM-GM inequality. Now consider the function $\\begin{array}{r}{f(x)\\triangleq\\frac{1-x}{N+1-N x^{\\frac{1}{N}}}}\\end{array}$ N+11\u2212\u2212NxxN1 over x \u2208[0, 1]. By choosing x\u2020 = ( NN\u2212 1)N, we have f(x\u2020) = $\\frac{1\\!-\\!\\big(\\frac{N-1}{N}\\big)^{N}}{2}$ . This implies that we have $\\begin{array}{r}{T^{\\dagger}\\left(1-\\prod_{i}H_{i}(t_{i}^{\\dagger})\\right)\\geq\\frac{1-(\\frac{N-1}{N})^{N}}{2}\\cdot{\\mathsf{O P T}}.}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "With the above Lemma 4.4, we are now ready to prove Theorem 4.1: ", "page_idx": 7}, {"type": "text", "text": "Proof of Theorem 4.1. From Lemma 4.4, we showed ", "page_idx": 7}, {"type": "equation", "text": "$$\nT^{\\dagger}\\left(1-\\prod_{i}H_{i}(t_{i}^{\\dagger})\\right)\\geq\\mathsf{O P T}\\cdot\\frac{1-(\\frac{N-1}{N})^{N}}{2}\\;,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $t_{i}^{\\dagger}$ is the pooling cutoff defined in Proposition 3.1 with the threshold $T^{\\dagger}$ . By definition of $T^{\\dagger}$ , we have $\\begin{array}{r}{\\prod_{i}H_{i}(t_{i}^{\\dagger})=(\\frac{N-1}{N})^{N}\\leq1/e}\\end{array}$ . Thus we can deduce that $T^{\\dagger}\\,\\geq\\,^{\\mathsf{O P T}}/2=T_{\\mathsf{K W}}.$ . Now let $t_{i}^{\\ddag}$ be the pooling cutoff when the searcher uses the stopping threshold $T_{\\mathsf{K W}}$ . Then we have ", "page_idx": 7}, {"type": "equation", "text": "$$\nu^{s}(T_{\\mathsf{K W}})\\geq T_{\\mathsf{K W}}\\cdot\\left(1-\\prod_{i}H_{i}(t_{i}^{\\dagger})\\right)\\stackrel{(a)}\\geq T_{\\mathsf{K W}}\\cdot\\left(1-\\prod_{i}H_{i}(t_{i}^{\\dagger})\\right)\\geq0\\mathsf{P T}\\cdot\\frac{1-1/e}{2}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where inequality (a) is due to $T^{\\dagger}\\geq T_{\\mathsf{K W}}$ , and thus we have $t_{i}^{\\dagger}\\geq t_{i}^{\\ddag}$ ", "page_idx": 7}, {"type": "text", "text": "5 Achieving $\\frac{1}{2}$ -robustness for Special Distributions ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The preceding section showed the $(1\\mathrm{~-~}^{1}/e)/2$ -robustness of the $T_{\\mathsf{K W}}$ -threshold stopping policy for arbitrary reward distributions. In this section, we show that this ratio can be improved to $^1/2$ -robustness when the distributions $H_{1:N}$ satisfy certain conditions: (1) IID distributions \u2013 all reward distributions are identical, namely, $H\\equiv H_{i}$ for all $i\\in[N]$ (see Section 5.1); and (2) Log-concave distributions \u2013 reward distribution $H_{i}$ has log-concave density (see Section 5.2). We also show that $^1/2$ -robustness is tight under IID distributions. ", "page_idx": 7}, {"type": "text", "text": "5.1 IID Distributions ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Our main findings for IID distributions are stated below: ", "page_idx": 7}, {"type": "text", "text": "Theorem 5.1. For any distributions $H_{1:N}$ where $H\\equiv H_{i},\\forall i\\in[N]$ , a threshold stopping policy with threshold $T=T^{*}$ is $\\frac{1}{2}$ -robust where $T^{*}$ is defined in Definition 2.2. ", "page_idx": 7}, {"type": "text", "text": "For IID distributions, we show that the searcher is able to achieve a better robustness approximation ratio compared to arbitrary distributions. Below we argue that this $^1/2$ -robustness is tight in the sense that there exists no threshold policy that can achieve better robustness approximation ratios. ", "page_idx": 7}, {"type": "text", "text": "Proposition 5.2 (Tightness of Theorem 5.1). There exist IID distributions such that there exists no threshold stopping policy that can achieve $\\alpha$ -robustness where $\\alpha>{\\textstyle{\\frac{1}{2}}}+\\varepsilon$ for any $\\varepsilon>0$ . ", "page_idx": 7}, {"type": "text", "text": "We note that Proposition 5.2 is a slightly stronger lower bound than Proposition 4.2 as it rules out the possibility for all possible threshold stopping polices, being within the spectrum in Definition 2.2 or beyond. We prove Proposition 5.2 by constructing a hard instance. In particular, we construct IID distributions with binary support. With this instance, we show that any threshold policy that achieves competitive ratio at least $^1/2$ in non-strategic setting will have competitive ratio approaching to 0 in the strategic setting. Please see ", "page_idx": 8}, {"type": "text", "text": "A crucial requirement in our robustness study so far is that we insist that the threshold policy should, first of all, remains an $^1/2$ -approximation in the non-strategic setting16, conditioned on which we look for additional guarantee for in the strategic setting. We conclude this section by pointing that if one was willing to give up the $^1/2$ -approximation in the non-strategic setting, then it is indeed possible to have a threshold policy that achieves better approximation (specifically, an $(1-^{1}/e)$ -approximation) in the strategic setting. While this does not satisfy our robustness requirement, it is useful to note. ", "page_idx": 8}, {"type": "text", "text": "Corollary 5.3. For any IID distributions, there exists a threshold stopping policy that is $(1-{^1}/{e})$ - approximation under strategic reward signaling. Moreover, there exist IID distributions such that no threshold stopping policy can achieve $\\left(1-{1\\mathord{\\left/{\\vphantom{1\\varepsilon}}\\right.\\kern-\\nulldelimiterspace}e}+\\varepsilon\\right)$ -approximation for any $\\varepsilon>0$ . ", "page_idx": 8}, {"type": "text", "text": "5.2 Log-concave Heterogeneous Distributions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this subsection, we show that when the distributions $H_{1:N}$ satisfy certain regularity assumptions, there exist threshold stopping policies with thresholds from Definition 2.2 that can also achieve $^1/2$ -robustness. The main result in this section is stated as follows: ", "page_idx": 8}, {"type": "text", "text": "Theorem 5.4. For $\\alpha,\\beta\\,>\\,0,$ , let $F_{\\alpha,\\beta}$ be the family of distributions with log-concave probability density functions $f$ on support $[0,1]$ such that $f(1)\\;\\geq\\;\\alpha$ and $f^{\\prime}(1)\\,\\geq\\,-\\bar{\\beta}$ . If the distributions $H_{1},\\dots,H_{N}$ are all from $F_{\\alpha,\\beta}$ and $\\begin{array}{r}{N\\,\\geq\\,1+\\,\\frac{\\beta}{\\alpha^{2}}}\\end{array}$ , then we always have $2\\cdot T_{\\mathsf{K W}}\\,\\leq\\,T_{\\mathsf{S C}}$ and any threshold $T$ satisfying $2\\cdot T_{\\mathsf{K W}}\\leq T\\leq T_{\\mathsf{S C}}\\ i s\\ ^{1}/2$ -robust. ", "page_idx": 8}, {"type": "text", "text": "A few remarks on the assumptions in Theorem 5.4 are worth mentioning. First, log-concavity of probability density functions17 is a commonly used assumption; they include but are not limited to: normal, beta, gamma, and exponential distributions. The restriction to support on $[0,1]$ is for normalization reasons, hence without loss of generality. The main non-trivial restriction is that this result holds when the number of players $N$ is large enough, formally $\\begin{array}{r}{N\\ge1+\\frac{\\beta}{\\alpha^{2}}}\\end{array}$ . This condition becomes less restrictive when $\\alpha$ (lower bounding $f(1))$ becomes larger and/or $\\vec{\\beta}$ (upper bounding $-f^{\\prime}(1))$ becomes smaller. These together, intuitively, imply that $f$ decreases slowly within $[0,1]$ . ", "page_idx": 8}, {"type": "text", "text": "Define $\\begin{array}{r}{\\bar{H}(x)\\,\\triangleq\\,\\prod_{i=1}^{N}H_{i}(x)}\\end{array}$ . Theorem 5.4 follows directly from the following Lemma 5.5 and Lemma 5.6. ", "page_idx": 8}, {"type": "text", "text": "Lemma 5.5. If $\\bar{H}$ is convex, then $2\\cdot T_{\\mathsf{K W}}\\leq T_{\\mathsf{S C}}$ and any threshold stopping policy with the threshold $T$ satisfying $2\\cdot T_{\\mathsf{K W}}\\leq T\\leq T_{\\mathsf{S C}}$ , where $T_{\\mathsf{K W}},T_{\\mathsf{S C}}$ are defined as in Definition 2.1, is $^1/2$ -robust. ", "page_idx": 8}, {"type": "text", "text": "Lemma 5.6. If the distributions $H_{1},\\dots,H_{N}$ are all from $F_{\\alpha,\\beta}$ and $\\begin{array}{r}{N\\geq1+\\frac{\\beta}{\\alpha^{2}}}\\end{array}$ , then $\\bar{H}$ is convex. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion and Future Directions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this paper, we study a variant of the prophet inequality problem where each random variable is associated with a strategic player who can strategically signal their reward to the searcher. We first fully characterize the optimal information strategy of each player, then we show the threshold stopping policies that can perform robustly well under both the strategic and non-strategic settings. ", "page_idx": 8}, {"type": "text", "text": "Our novel consideration of natural strategic manipulations in prophet inequalities open the door for many interesting future directions. First, it is interesting to see if we can improve the 1\u221221/erobustness guarantee for arbitrary reward distributions by using not commonly used thresholds outside the spectrum of Definition 2.2. This may be technically challenging since finding a threshold outside this spectrum with optimal $^1/2$ -approximation for the classic non-strategic prophet inequality is already non-trivial. Another interesting direction is to go beyond threshold policies. That leads to a different research theme, not about robustness of the classic prophet inequality, but rather in the search of potentially much more complex best-of-both-world policies. Our preliminary result reveals that a dynamic threshold stopping policy (using different thresholds for different players) has the potential to help the searcher to achieve more than $\\frac{1\\!-\\!1/e}{2}$ -approximation under the strategic setting. However, it is still unclear whether $^1/2$ -robustness is achievable under threshold stopping policies, no matter whether it is static threshold or dynamic threshold. Finally, even if one only cares about the performance under the strategic reward signaling (and ignore the non-strategic world), it is still unclear which static threshold stopping policy achieves the highest competitive ratio. The authors in [20] study a model with a very similar mathematical structure and proved that this best upper bound is strictly less than $^1/2$ in Section 4.1, but an interesting direction is improving this bound further. We note that all our constructed examples in the hardness results (e.g., Proposition 4.2 and Proposition 5.2) do not rule out the existence of such $^1/2$ -approximation threshold stopping policies. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Acknowledgment. Haifeng $\\mathrm{Xu}$ is supported in part by the NSF Award CCF-2303372, Army Research Office Award W911NF-23-1-0030 and ONR Award N00014-23-1-2802. We thank the anonymous reviewers for constructive and helpful comments. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Melika Abolhassani, Soheil Ehsani, Hossein Esfandiari, MohammadTaghi Hajiaghayi, Robert Kleinberg, and Brendan Lucier. Beating 1-1/e for ordered prophets. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 61\u201371, 2017.   \n[2] Saeed Alaei. Bayesian combinatorial auctions: Expanding single buyer mechanisms to many buyers. SIAM Journal on Computing, 43(2):930\u2013972, 2014.   \n[3] Nick Arnosti and Will Ma. Tight guarantees for static threshold policies in the prophet secretary problem. In Proceedings of the 23rd ACM Conference on Economics and Computation, EC \u201922, page 242, 2022.   \n[4] Makis Arsenis and Robert Kleinberg. Individual fairness in prophet inequalities. In Proceedings of the 23rd ACM Conference on Economics and Computation, pages 245\u2013245, 2022.   \n[5] Pak Hung Au and Keiichi Kawai. Competitive information disclosure by multiple senders. Games and Economic Behavior, 119:56\u201378, 2020. [6] Pablo D Azar, Robert Kleinberg, and S Matthew Weinberg. Prophet inequalities with limited information. In Proceedings of the twenty-fifth annual ACM-SIAM symposium on Discrete algorithms, pages 1358\u20131377. SIAM, 2014. [7] Moshe Babaioff, Nicole Immorlica, and Robert Kleinberg. Matroids, secretary problems, and online mechanisms. In Symposium on Discrete Algorithms (SODA\u201907), pages 434\u2013443, 2007.   \n[8] Mark Bagnoli and Ted Bergstrom. Log-concave probability and its applications. In Rationality and Equilibrium: A Symposium in Honor of Marcel K. Richter, pages 217\u2013241. Springer, 2006. [9] Yahav Bechavod, Katrina Ligett, Steven Wu, and Juba Ziani. Gaming helps! learning from strategic interactions in natural dynamics. In International Conference on Artificial Intelligence and Statistics, pages 1234\u20131242. PMLR, 2021.   \n[10] Dirk Bergemann and Stephen Morris. Bayes correlated equilibrium and the comparison of information structures in games. Theoretical Economics, 11(2):487\u2013522, 2016.   \n[11] Martino Bernasconi, Matteo Castiglioni, Andrea Celli, Alberto Marchesi, Francesco Trovo\\`, and Nicola Gatti. Optimal rates and efficient algorithms for online bayesian persuasion. In International Conference on Machine Learning, pages 2164\u20132183. PMLR, 2023.   \n[12] David Blackwell. Comparison of experiments. In Proceedings of the second Berkeley symposium on mathematical statistics and probability, volume 2, pages 93\u2013103. University of California Press, 1951.   \n[13] David A Blackwell and Meyer A Girshick. Theory of games and statistical decisions. Courier Corporation, 1979.   \n[14] Matteo Castiglioni, Andrea Celli, Alberto Marchesi, and Nicola Gatti. Online bayesian persuasion. Advances in Neural Information Processing Systems, 33, 2020.   \n[15] Shuchi Chawla, Jason D Hartline, and Robert Kleinberg. Algorithmic pricing via virtual valuations. In Proceedings of the 8th ACM Conference on Electronic Commerce, pages 243\u2013251, 2007.   \n[16] Shuchi Chawla, Jason D Hartline, David L Malec, and Balasubramanian Sivan. Multiparameter mechanism design and sequential posted pricing. In Proceedings of the forty-second ACM symposium on Theory of computing, pages 311\u2013320, 2010.   \n[17] Shuchi Chawla, Nikhil Devanur, and Thodoris Lykouris. Static pricing for multi-unit prophet inequalities. Operations Research, 2023.   \n[18] Jose\u00b4 Correa, Patricio Foncea, Ruben Hoeksma, Tim Oosterwijk, and Tjark Vredeveld. Posted price mechanisms for a random stream of customers. In Proceedings of the 2017 ACM Conference on Economics and Computation, pages 169\u2013186, 2017.   \n[19] Jose Correa, Patricio Foncea, Ruben Hoeksma, Tim Oosterwijk, and Tjark Vredeveld. Recent developments in prophet inequalities. ACM SIGecom Exchanges, 17(1):61\u201370, 2019.   \n[20] Yuan Deng, Vahab Mirrokni, and Hanrui Zhang. Posted pricing and dynamic prior-independent mechanisms with value maximizers. Advances in Neural Information Processing Systems, 35: 24158\u201324169, 2022.   \n[21] Nikhil R Devanur, Kamal Jain, Balasubramanian Sivan, and Christopher A Wilkens. Near optimal online algorithms and fast approximation algorithms for resource allocation problems. In Proceedings of the 12th ACM conference on Electronic commerce, pages 29\u201338, 2011.   \n[22] Nikhil R Devanur, Balasubramanian Sivan, and Yossi Azar. Asymptotically optimal algorithm for stochastic adwords. In Proceedings of the 13th ACM Conference on Electronic Commerce, pages 388\u2013404, 2012.   \n[23] Bolin Ding, Yiding Feng, Chien-Ju Ho, Wei Tang, and Haifeng Xu. Competitive information design for pandora\u2019s box. In Proceedings of the 2023 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 353\u2013381. SIAM, 2023.   \n[24] Jinshuo Dong, Aaron Roth, Zachary Schutzman, Bo Waggoner, and Zhiwei Steven Wu. Strategic classification from revealed preferences. In Proceedings of the 2018 ACM Conference on Economics and Computation, pages 55\u201370, 2018.   \n[25] Yiding Feng, Wei Tang, and Haifeng Xu. Online bayesian recommendation with no regret. In Proceedings of the 23rd ACM Conference on Economics and Computation, pages 818\u2013819, 2022.   \n[26] Matthew Gentzkow and Emir Kamenica. Competition in persuasion. The Review of Economic Studies, 84(1):300\u2013322, 2016.   \n[27] Matthew Gentzkow and Emir Kamenica. A rothschild-stiglitz approach to bayesian persuasion. American Economic Review, 106(5):597\u2013601, 2016.   \n[28] Matthew Gentzkow and Emir Kamenica. Bayesian persuasion with multiple senders and rich signal spaces. Games and Economic Behavior, 104:411\u2013429, 2017.   \n[29] Ganesh Ghalme, Vineet Nair, Itay Eilat, Inbal Talgam-Cohen, and Nir Rosenfeld. Strategic classification in the dark. In International Conference on Machine Learning, pages 3672\u2013 3681. PMLR, 2021.   \n[30] Niklas Hahn, Martin Hoefer, and Rann Smorodinsky. Prophet inequalities for bayesian persuasion. In IJCAI, pages 175\u2013181, 2020.   \n[31] Mohammad Taghi Hajiaghayi, Robert Kleinberg, and Tuomas Sandholm. Automated online mechanism design and prophet inequalities. In AAAI, volume 7, pages 58\u201365, 2007.   \n[32] Moritz Hardt, Nimrod Megiddo, Christos Papadimitriou, and Mary Wootters. Strategic classification. In Proceedings of the 2016 ACM conference on innovations in theoretical computer science, pages 111\u2013122, 2016.   \n[33] Theodore P Hill and Robert P Kertz. Comparisons of stop rule and supremum expectations of iid random variables. The Annals of Probability, pages 336\u2013345, 1982.   \n[34] Safwan Hossain, Tonghan Wang, Tao Lin, Yiling Chen, David C Parkes, and Haifeng Xu. Multi-sender persuasion\u2013a computational perspective. arXiv preprint arXiv:2402.04971, 2024.   \n[35] Emir Kamenica and Matthew Gentzkow. Bayesian persuasion. American Economic Review, 101(6):2590\u20132615, 2011.   \n[36] Robert Kleinberg and Seth Matthew Weinberg. Matroid prophet inequalities. In Proceedings of the forty-fourth annual ACM symposium on Theory of computing, pages 123\u2013136, 2012.   \n[37] Ulrich Krengel and Louis Sucheston. Semiamarts and finite values. 1977.   \n[38] Ulrich Krengel and Louis Sucheston. On semiamarts, amarts, and processes with finite value. Probability on Banach spaces, 4:197\u2013266, 1978.   \n[39] Allen Liu, Renato Paes Leme, Martin Pal, Jon Schneider, and Sivan Balasubramanian. Variable decomposition for prophet inequalities and optimal ordering. EC \u201921: Proceedings of the 22nd ACM Conference on Economics and Computation, page 692, 2021.   \n[40] Brendan Lucier. An economic view of prophet inequalities. ACM SIGecom Exchanges, 16(1): 24\u201347, 2017.   \n[41] Sebastian Perez-Salazar, Mohit Singh, and Alejandro Toriello. The iid prophet inequality with limited flexibility. arXiv preprint arXiv:2210.05634, 2022.   \n[42] Ester Samuel-Cahn. Comparison of threshold stop rules and maximum for independent nonnegative random variables. the Annals of Probability, pages 1213\u20131216, 1984.   \n[43] Ravi Sundaram, Anil Vullikanti, Haifeng Xu, and Fan Yao. Pac-learning for strategic classification. Journal of Machine Learning Research, 24(192):1\u201338, 2023.   \n[44] Jibang Wu, Zixuan Zhang, Zhe Feng, Zhaoran Wang, Zhuoran Yang, Michael I Jordan, and Haifeng Xu. Sequential information design: Markov persuasion process and its efficient reinforcement learning. In Proceedings of the 23th ACM Conference on Economics and Computation, 2022.   \n[45] Hanrui Zhang and Vincent Conitzer. Incentive-aware pac learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 5797\u20135804, 2021.   \n[46] Tijana Zrnic, Eric Mazumdar, Shankar Sastry, and Michael Jordan. Who leads and who follows in strategic classification? Advances in Neural Information Processing Systems, 34:15257\u2013 15269, 2021.   \n[47] You Zu, Krishnamurthy Iyer, and Haifeng Xu. Learning to persuade on the fly: Robustness against ignorance. In Proceedings of the 22nd ACM Conference on Economics and Computation, pages 927\u2013928, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: The abstract and introduction accurately summarize our results. In the main text, we provide all rigorous statements and proofs for these results. ", "page_idx": 13}, {"type": "text", "text": "Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 13}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Justification: They are discussed in conclusion section. ", "page_idx": 13}, {"type": "text", "text": "Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 13}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: All assumptions are clearly stated, and proofs are rigorously written. Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 14}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 14}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 14}, {"type": "text", "text": "Justification: This is a theory work, no experiment is provided. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 14}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 14}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 15}, {"type": "text", "text": "Justification: No data and code is used. Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 15}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 15}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 15}, {"type": "text", "text": "Justification: No experiments. Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 15}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 15}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 15}, {"type": "text", "text": "Justification: No experiments. Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 16}, {"type": "text", "text": "Justification: No experiments. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 16}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: No violation of NeurIPS Code of Ethics. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 16}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 16}, {"type": "text", "text": "Justification: The paper studies a theoretical problem, to the best of our knowledge, there is no societal impact of the work that we need to address. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. \u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. ", "page_idx": 16}, {"type": "text", "text": "\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 17}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: Not applicable to our paper. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 17}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: No assets used. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 17}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 18}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: No new assets as this is a theory work. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 18}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: This project is not a crowd sourcing project and has no human subjects. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 18}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: No IRB needed. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 18}, {"type": "text", "text": "The full version of this paper, including detailed proofs, can be found at https://arxiv.org/ pdf/2409.18269 ", "page_idx": 19}]