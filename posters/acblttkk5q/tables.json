[{"figure_path": "AcBLtTKK5q/tables/tables_6_1.jpg", "caption": "Table 1: Input-level average harmful scores on existing benchmarks and JAMBench", "description": "This table compares the average harmful scores obtained from four different question benchmarks (In-the-Wild, HarmBench, JailbreakBench, and JAMBench) across four categories (Hate and Fairness, Sexual, Violence, and Self-Harm) at medium and high severity levels.  The scores reflect how often questions from each benchmark trigger OpenAI's moderation guardrails. JAMBench shows a significantly higher average score across all categories, indicating its improved effectiveness in triggering the guardrails.", "section": "5 Experiments"}, {"figure_path": "AcBLtTKK5q/tables/tables_7_1.jpg", "caption": "Table 2: Jailbreak success rate and filtered-out rate on JAMBench.", "description": "This table presents the results of evaluating the effectiveness of the JAM jailbreaking method against four different LLMs (GPT-3.5, GPT-4, Gemini, and Llama-3).  It shows the jailbreak success rate (the percentage of attempts that successfully bypassed moderation guardrails) and the filtered-out rate (the percentage of attempts that were blocked by the guardrails) for each LLM, broken down by four categories of malicious content (Hate and Fairness, Sexual, Violence, and Self-Harm) and two severity levels (Medium and High).  The purpose of the table is to compare JAM's performance to several baseline jailbreaking techniques.", "section": "5.2 Effectiveness on Jailbreaking LLMs"}, {"figure_path": "AcBLtTKK5q/tables/tables_8_1.jpg", "caption": "Table 2: Jailbreak success rate and filtered-out rate on JAMBench.", "description": "This table presents the results of evaluating different jailbreaking methods (GCG, ICA, PAIR, CipherChat, GUARD, and JAM) on the JAMBench benchmark across four LLMs (GPT-3.5, GPT-4, Gemini, and Llama-3).  It shows the success rate of each method in bypassing moderation guardrails and the rate at which responses were filtered out by these guardrails for different content categories (Hate and Fairness, Sexual, Violence, Self-Harm) at medium and high severity levels.", "section": "5.2 Effectiveness on Jailbreaking LLMs"}, {"figure_path": "AcBLtTKK5q/tables/tables_8_2.jpg", "caption": "Table 4: The impact of jailbreak prefixes", "description": "This table presents the results of an ablation study conducted to evaluate the impact of different jailbreak prefixes on the effectiveness of the JAM method in bypassing moderation guardrails across various LLMs and different categories of malicious questions (Hate and Fairness, Sexual, Violence, and Self-Harm). It shows that using jailbreak prefixes significantly improves the jailbreak success rate and reduces the filtered-out rate.  The table compares three scenarios: using no prefixes, using a predefined DAN 12.0 prompt, and using prefixes generated by the GUARD method.  The results demonstrate the importance of effective jailbreak prefixes for successful jailbreaking.", "section": "Ablation and Sensitivity Studies"}, {"figure_path": "AcBLtTKK5q/tables/tables_8_3.jpg", "caption": "Table 5: The impact of fine-tuning the shadow model", "description": "This table presents the results of an ablation study evaluating the impact of fine-tuning the shadow model on the effectiveness of JAM. It compares the jailbreak success rate and filtered-out rate for different models (GPT-3.5, GPT-4, Gemini, and Llama-3) under medium and high severity settings.  The results show that fine-tuning the shadow model significantly improves the jailbreak success rate and reduces the filtered-out rate across all categories.", "section": "5.3 Ablation and Sensitivity Studies"}, {"figure_path": "AcBLtTKK5q/tables/tables_9_1.jpg", "caption": "Table 6: The impact on the length of cipher characters", "description": "This table presents the results of an ablation study conducted to evaluate the impact of different cipher character lengths (10, 20, and 40 tokens) on the jailbreak success rate and filtered-out rate across various categories (Hate and Fairness, Sexual, Violence, and Self-Harm) and severity levels (Medium and High). The results show that the default setting of 20 tokens generally provides the best balance between high jailbreak success rates and low filtered-out rates across all categories. Increasing the length beyond 20 tokens does not significantly enhance performance, while shorter lengths (10 tokens) lead to lower success rates and higher filtered-out rates.", "section": "5.3 Ablation and Sensitivity Studies"}, {"figure_path": "AcBLtTKK5q/tables/tables_13_1.jpg", "caption": "Table 2: Jailbreak success rate and filtered-out rate on JAMBench.", "description": "This table presents the results of the jailbreaking experiments conducted on four different LLMs (GPT-3.5, GPT-4, Gemini, and Llama-3) using the JAMBench dataset.  The results are categorized by the type of harmful content (Hate and Fairness, Sexual, Violence, Self-Harm) and severity level (Medium, High). For each LLM and category, the table shows the jailbreak success rate (the percentage of attempts that successfully bypassed the moderation guardrails) and the filtered-out rate (the percentage of attempts that were blocked by the guardrails).  Various baseline methods (GCG, ICA, PAIR, CipherChat, GUARD) are also included for comparison, demonstrating the effectiveness of the proposed JAM method.", "section": "5.2 Effectiveness on Jailbreaking LLMs"}, {"figure_path": "AcBLtTKK5q/tables/tables_18_1.jpg", "caption": "Table 2: Jailbreak success rate and filtered-out rate on JAMBench.", "description": "This table presents the results of the jailbreaking experiments conducted on the JAMBench benchmark. It shows the success rates and filtered-out rates for different LLMs and methods, categorized by the severity level (medium or high) and content categories (Hate and Fairness, Sexual, Violence, Self-Harm).  The table allows for a comparison of the effectiveness of various jailbreaking techniques (GCG, ICA, PAIR, CipherChat, GUARD) against the proposed JAM method.  Higher success rates and lower filtered-out rates indicate more effective jailbreaking.", "section": "5.2 Effectiveness on Jailbreaking LLMs"}, {"figure_path": "AcBLtTKK5q/tables/tables_18_2.jpg", "caption": "Table 2: Jailbreak success rate and filtered-out rate on JAMBench.", "description": "This table presents a comparison of different jailbreaking methods on the JAMBench benchmark, evaluating their success rate and the rate at which they are filtered out by the moderation guardrails.  The results are broken down by LLM model (GPT-3.5, GPT-4, Gemini, Llama-3), jailbreaking method (GCG, ICA, PAIR, CipherChat, GUARD, JAM), and severity level (medium, high) across four categories of malicious prompts (Hate and Fairness, Sexual, Violence, Self-Harm).  It demonstrates JAM's superior performance in achieving high jailbreak success rates while minimizing filtered-out responses.", "section": "5.2 Effectiveness on Jailbreaking LLMs"}, {"figure_path": "AcBLtTKK5q/tables/tables_19_1.jpg", "caption": "Table 2: Jailbreak success rate and filtered-out rate on JAMBench.", "description": "This table presents the results of the jailbreaking experiments conducted on the JAMBench benchmark.  It compares the performance of the proposed JAM method against several baseline methods across different categories (Hate and Fairness, Sexual, Violence, Self-Harm) and severity levels (Medium, High). The table shows the percentage of successful jailbreaks (Jailbreak Success Rate) and the percentage of attempts that were filtered out by the model's moderation system (Filtered-out Rate).  Higher Jailbreak Success Rates and lower Filtered-out Rates indicate better performance.", "section": "5.2 Effectiveness on Jailbreaking LLMs"}, {"figure_path": "AcBLtTKK5q/tables/tables_19_2.jpg", "caption": "Table 2: Jailbreak success rate and filtered-out rate on JAMBench.", "description": "This table presents the performance of JAM against various baseline methods on the JAMBench benchmark.  It shows the jailbreak success rate and filtered-out rate for each method across different categories (Hate and Fairness, Sexual, Violence, Self-Harm) and severity levels (Medium, High).  The results highlight JAM's superior performance in achieving high jailbreak success rates while keeping filtered-out rates low.", "section": "5.2 Effectiveness on Jailbreaking LLMs"}, {"figure_path": "AcBLtTKK5q/tables/tables_20_1.jpg", "caption": "Table 5: Perplexity score on baselines and JAM", "description": "This table presents the perplexity scores of the prompts used in the experiments for different methods (GCG, ICA, PAIR, CipherChat, and JAM) across four different LLMs (GPT-3.5, GPT-4, Gemini, and Llama-3).  Perplexity is a measure of how well a language model predicts a sequence of words; lower scores indicate higher fluency and coherence.  The results show that JAM's prompts generally have higher perplexity scores compared to baselines, which is explained by the inclusion of cipher characters in JAM's prompts.", "section": "F.1 Effectiveness Analysis"}, {"figure_path": "AcBLtTKK5q/tables/tables_20_2.jpg", "caption": "Table 2: Jailbreak success rate and filtered-out rate on JAMBench.", "description": "This table presents the results of the experiment evaluating the effectiveness of the JAM jailbreaking method against four different LLMs (GPT-3.5, GPT-4, Gemini, and Llama-3).  It shows the jailbreak success rate and filtered-out rate for each LLM across four categories of malicious content (Hate and Fairness, Sexual, Violence, and Self-Harm) at medium and high severity levels. The table also includes results for several baseline jailbreaking methods for comparison, highlighting JAM's superior performance in bypassing moderation guardrails.", "section": "5.2 Effectiveness on Jailbreaking LLMs"}]