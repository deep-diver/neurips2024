[{"figure_path": "AcBLtTKK5q/figures/figures_1_1.jpg", "caption": "Figure 1: Examples of jailbreaks. (a) A malicious question that receives a refusal response from the LLM. (b) An affirmative response with detailed steps to implement the malicious question by adding a jailbreak prompt as the prefix. (c) A filtered-out error is triggered by the moderation guardrail, even when a successful jailbreak prompt is added. (d) An affirmative response using JAM, which combines a jailbreak prefix, the malicious question, and the cipher characters to bypass the guardrail.", "description": "This figure shows four examples of interactions with a large language model (LLM) to illustrate the concept of jailbreaking.  (a) shows a normal refusal response to a malicious prompt. (b) demonstrates a successful jailbreak where the malicious prompt is disguised using a prefix, leading to an affirmative, harmful response. (c) illustrates how a moderation guardrail can filter out even a successful jailbreak, returning an error. Finally, (d) shows how the proposed method, JAM, successfully bypasses the guardrail by using a combination of a jailbreak prefix and cipher characters, leading to an affirmative response.", "section": "1 Introduction"}, {"figure_path": "AcBLtTKK5q/figures/figures_1_2.jpg", "caption": "Figure 2: Three types of structural built-in safeguardrails.", "description": "This figure illustrates three different types of safety mechanisms used in LLMs to prevent harmful outputs: Input-only guardrails filter malicious prompts before processing; output-only guardrails filter unsafe responses after generation; input-output guardrails combine both methods for enhanced safety.", "section": "1 Introduction"}, {"figure_path": "AcBLtTKK5q/figures/figures_3_1.jpg", "caption": "Figure 3: Overview workflow of JAM for generating a jailbreak prompt, details in Section 3.2.", "description": "This figure illustrates the four main steps involved in generating a jailbreak prompt using the JAM method.  Step 1 focuses on constructing a filtered corpus by pairing harmful texts with their corresponding filtered responses from an LLM's moderation guardrail.  In Step 2, a shadow model is trained using this filtered corpus to mimic the behavior of the guardrail. Step 3 involves optimizing cipher characters to reduce the harmfulness score of the texts. Finally, in Step 4, these components\u2014jailbreak prefix, malicious question, and cipher characters\u2014are combined to create a complete jailbreak prompt designed to evade the moderation guardrail and elicit a harmful response from the LLM.", "section": "3 Methodology"}, {"figure_path": "AcBLtTKK5q/figures/figures_6_1.jpg", "caption": "Figure 4: Filtered-out rates of existing question benchmarks and JAMBench", "description": "This figure compares the filtered-out rates of four different question benchmarks (In-the-Wild, HarmBench, JailbreakBench, and JAMBench) when used to evaluate the effectiveness of jailbreaking LLMs.  Each radar chart represents a benchmark and shows the percentage of questions in each category (Hate and Fairness, Sexual, Violence, Self-Harm) that triggered the filtered-out error, indicating the benchmark's ability to test moderation guardrails.  JAMBench is shown to significantly improve the coverage and effectiveness in triggering these filters compared to existing methods.", "section": "Existing Question Benchmarks vs JAMBench"}]