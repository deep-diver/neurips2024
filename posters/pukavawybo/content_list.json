[{"type": "text", "text": "Learning and Transferring Sparse Contextual Bigrams with Linear Transformers ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yunwei Ren\\* ZixuanWang\\* JasonD.Lee Princeton University {yunwei.ren, wangzx, jasonlee}@princeton.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Transformers have excelled in natural language modeling and one reason behind this success is their exceptional ability to combine contextual informal and global knowledge. However, the theoretical basis remains unclear. In this paper, first we introduce the Sparse Contextual Bigram (SCB), a natural extension of the classical bigram model, where the next token's generation depends on a sparse set of earlier positions determined by the last token. We then analyze the training dynamics and sample complexity of learning SCB using a one-layer linear transformer with a gradient-based algorithm. We show that when trained from scratch, the training process can be split into an initial sample-intensive stage where the correlation is boosted from zero to a nontrivial value, followed by a more sample-efficient stage of further improvement. Additionally, we prove that, provided a nontrivial correlation between the downstream and pretraining tasks, finetuning from a pretrained model allows us to bypass the initial sample-intensive stage. We also empirically demonstrate that our algorithm can outperform SGD in this setting and discuss its relationship with the usual softmax-based transformers. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Transformers have played a central role in modern deep learning, achieving significant success across various fields, including language modeling (OpenA1, 2023), computer vision (Dosovitskiy et al., 2020), and natural sciences (Jumper et al., 2021). The core of transformers is the self-attention layer (Vaswani et al., 2017), which can attend to any subset of the input sequence to output a weighted linear combination of the (transformed) tokens. ", "page_idx": 0}, {"type": "text", "text": "Several capabilities of the transformers contribute to their success in language modeling. First, they can extract contextual information from the input token sequences, which is essential in some arithmetic tasks (Edelman et al., 2022; Liu et al., 2022; Nanda et al., 2023; Ya0 et al., 2021). In addition, transformers can memorize global in-domain knowledge (Petroni et al., 2019; Zhang et al., 2023a; Haviv et al., 2022; Carlini et al., 2021). These two abilities combined enable transformers to predict the next token based on the in-context information as well as global knowledge (OpenAI, 2023) acquired during training. ", "page_idx": 0}, {"type": "text", "text": "To theoretically understand how transformers learn both capabilities, we propose a minimalist datagenerating model, the Sparse Contextual Bigram (SCB). This model builds on the classical bigram model and requires learning both contextual information and the (global) transition probabilities. Here, the next token depends on the transition matrix $P$ and a sparse set of prior tokens that is determined by the last token. In particular, SCB can be represented by a one-layer linear transformer \u2014- a simplified architecture that can serve as an abstraction for studying transformer optimization (Ahn et al., 2023), which makes it suitable for theoretical analysis. ", "page_idx": 0}, {"type": "text", "text": "In this paper, we investigate the training dynamics and sample complexity of training a linear transformer to learn the SCB task using a stochastic gradient-based algorithm. Our contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u00b7 Data model: We introduce the Sparse Contextual Bigram (SCB) model, a simple task that requires the model to learn both in-context and global information.   \n\u00b7 Convergence: We prove convergence guarantees for a one-layer linear transformer trained on with the nonconvex $\\ell_{1}$ -regularized MSE loss using preconditioned projected proximal descent, given a dataset sampled from the SCB model.   \n\u00b7 Sample Complexity: Under mild conditions on the data distribution, initialization, and hyperparameters, we prove that our algorithm can recover the ground-truth with polynomial dependence on the sequence length $T$ , number of states $N$ , and the sparsity parameter $Q\\ll T$ We show that the training first goes through an initial sample-intensive stage which boosts the signal with poly $(T)$ samples, followed by a more sample-efficient stage to achieve final convergence with poly $(N,Q)$ samples. We empirically verify that our gradient-based methods converge to the ground truth with a small batch size, while unregularized stochastic gradient descent fails due to the large variance.   \n\u00b7 Transfer Learning: We prove that, when there is a nontrivial correlation between the pretraining and downstream tasks, we can transfer a pre-trained model to bypass the first sample intensive stage, so that our algorithm converges to the ground truth of the downstream task with only poly $(N,Q)$ samples. ", "page_idx": 1}, {"type": "text", "text": "1.1 Related works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Training dynamics of transformers. Several works have studied the learnability aspects of specific transformer architectures. Jelassi et al. (2022) demonstrated that a Vision Transformer (ViT) (Dosovitskiy et al., 2020) trained through GD, augmented with positional-embedding attention matrices, can effectively capture spatial structures. Li et al. (2023) investigated the sample complexity necessary to achieve good generalization performance on a similar ViT model. Tarzanagh et al. (2023) established a connection between the optimization landscape of self-attention and the formulation of a hard-margin Support Vector Machine (SVM) problem that separates and selects specific optimal tokens and established global convergence under strong assumptions. Tian et al. (2023a,b) provided insights into the training dynamics of the self-attention and MLP layers, respectively, although they did not establish convergence guarantees. ", "page_idx": 1}, {"type": "text", "text": "Another line of work focuses on the training dynamics of in-context learning. Mahankali et al. (2023) was among the first to introduce linear regression as an in-context learning task, while Zhang et al. (2023b) proved global convergence of gradient fow for a single-layer linear self-attention layer on this task. Huang et al. (2023) provided a convergence guarantee for a one-layer transformer with softmax attention on a similar task where the in-context tokens are drawn from a specific data distribution. Chen et al. (2024) generalized the single-task linear regression task to a multi-task setting and proved the global convergence of multi-head attention architecture using gradient fow on the population loss with specific initialization. In contrast, our work focuses on the language modeling ability of transformers instead of their in-context learning ability. ", "page_idx": 1}, {"type": "text", "text": "Several recent works analyzed transformers from a Markov chain perspective. Bietti et al. (2024) studied the in-context bigram (phrased as induction head) from an associative memory viewpoint. Nichani et al. (2024) proved that a simplified two-layer transformer can learn the induction head and generalize it to certain latent causal graphs. Edelman et al. (2024) further investigated training process on bigram and general $n$ -gram tasks, and observed multi-phase dynamics. Makkuva et al. (2024) studied the loss landscape of transformers trained on sequences sampled from a single Markov Chain. Our SCB model extends the classical bigram models to allow context-dependent sparse attention on previous tokens. ", "page_idx": 1}, {"type": "text", "text": "Several works, including Tian et al. (2023a); Zhang et al. (2023b); Huang et al. (2023); Tarzanagh et al. (2023); Nichani et al. (2024); Kim and Suzuki (2024), and ours, use a similar reparameterization, consolidating the key and query matrices into a single matrix W to simplify the dynamics of the training process. Most previous studies (Tian et al., 2023a; Zhang et al., 2023b; Huang et al., 2023; Tarzanagh et al., 2023; Nichani et al., 2024; Kim and Suzuki, 2024; Wang et al., 2024; Chen et al., 2024) uses population loss to simplify the analysis. In contrast, our work goes beyond the population loss to analyze the sample complexity of the stochastic gradient descent dynamics. Although Li et al. (2023) also investigated the sample complexity on a different task, their model requires a pre-trained initialization, while our model is trained from scratch. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Transfer Learning. Transfer learning (Devlin et al., 2018) has gained significant attention in this deep learning era. From a theoretical perspective, several works have investigated the statistical guarantees of transfer learning from the representation learning perspective (Tripuraneni et al., 2020; Du et al., 2020; Arora et al., 2019; Hanneke et al., 2023). Recent studies on transfer learning mostly focus on linear models (Li et al., 2022; Tian and Feng, 2023; Fei and Li, 2021; Zhang et al., 2022; Ju et al., 2023; Dar and Baraniuk, 2022). For dynamics of transfer learning, Lampinen and Ganguli (2018) studied the behaviors of multi-layer linear networks in a teacher-student setting, while Dhifallah and Lu (2021) analyzed single-layer perceptrons. Damian et al. (2022) showed that a two-layer neural network can efficiently learn polynomials dependent on a few directions, enabling transfer learning. ", "page_idx": 2}, {"type": "text", "text": "To the best of our knowledge, this is the first work studying transfer learning for transformers Moreover, unlike previous works that assume a shared structure between the pretraining and downstream tasks, we only require them to have a non-trivial correlation, which is a much weaker assumption. ", "page_idx": 2}, {"type": "text", "text": "1.2  Outline of this paper ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In Section 2 we formalize the problem setup, including the SCB task, the transformer architecture, and the training algorithm. Section 3 consists of our main results, and we analyze the population dynamics to provide intuitions. Section 4 contains our transfer learning results. Experimental results can be found in Section 5. ", "page_idx": 2}, {"type": "text", "text": "2 Setup", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we describe our data-generating model, the one-layer linear transformer architecture, and the training algorithm. ", "page_idx": 2}, {"type": "text", "text": "Notations. We use $[T]$ to denote the set $\\{1,2,...,T\\}$ . Matrices and vectors are denoted in upper-case bold letters $(A,V,\\Delta$ , etc.) and lower-case bold letters $\\mathbf{\\Gamma}(a,q$ , etc.), respectively. For norm, $\\lVert\\cdot\\rVert$ denotes $\\ell_{2}$ norm and ${\\|\\cdot\\|_{F}}$ denotes the Frobenius norm. Additionally, for $\\pmb{\\mu}\\in\\mathbb{R}^{N}$ \uff0c $\\|\\dot{\\boldsymbol{A}}\\|_{\\mu}$ denotes $\\mu$ -norm for matrix $A\\in\\mathbb{R}^{d\\times N}$ for arbitrary $d$ , which is defined as $\\|A\\|_{\\mu}^{2}:=\\operatorname{Tr}\\left(A\\mathrm{diag}(\\mu)A^{\\top}\\right)$ . We use $\\mathbb{1}\\{\\cdot\\}$ to denote the indicator function. We use $\\tilde{O}(\\cdot)$ to hide logarithmic factors in the asymptotic notations. ", "page_idx": 2}, {"type": "text", "text": "2.1  Data-generating model: Sparse Contextual Bigram ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The bigram model, where the next token depends only on the current one, is arguably one of the simplest language models. To learn this model, it suffices to learn the transition probabilities $P\\in\\mathbb{R}^{N\\times N}$ Where $P_{n,m}=\\mathbb{P}[X_{t+1}=n\\mid X_{t}=m]$ ,whichis achievable through alinear model O-layer transformer). ", "page_idx": 2}, {"type": "text", "text": "A natural way to extend the classical bigram model is to allow the next token to depend on a context-dependent set of previous tokens. This extension can model situations such as generating the words after the phrase \u201cby Theorem $3.2^{\\bullet}$ , which requires us to retrieve the statement of \u201cTheorem $3.2^{\\circ}$ . Here, we propose a simple extension of this type, which we call the Sparse Contextual Bigram (SCB). The contextual information is encoded by a sparse probability vector $\\pmb q$ determined by the last token. To generate the next token, the model retrieves the tokens referenced by $\\pmb q$ and applies the transition matrix $P$ (global knowledge) to one of them according to the distribution $\\pmb q$ ", "page_idx": 2}, {"type": "text", "text": "Formally, our data-generating model SCB can be described as follows. Let $T$ be the sequence length and $[N]$ the vocabulary. Let $\\mathbf{\\bar{\\boldsymbol{P}}}\\in\\mathbb{R}^{N\\times N}$ be a transition matrix, with column $\\boldsymbol{P}_{k}$ being the transition probability vector of token $k$ . Suppose that $\\pmb{\\mu}\\in\\mathbb{R}_{\\ge0}^{N}$ is the stationary distribution of $P$ ", "page_idx": 2}, {"type": "text", "text": "Each input sequence consists of $T+1$ tokens $(x_{1},\\dots,x_{T+1})$ , i.i.d. sampled from distribution $\\pmb{\\mu}$ .The output token (label) is generated as follows. For each $k\\in[N]$ , there is a probability vector $\\pmb q^{(k)}\\in R_{\\geq0}^{T}$ that represents the tokens the model needs to attend to when the last token $x_{T+1}$ is $k$ . For notational simplicity, we will write $\\pmb{x}=(x_{1},\\dots,x_{T})$ \uff0c $\\pmb{X}=(e_{x_{1}},\\dots,e_{x_{T}})$ and $\\boldsymbol{Q}=(\\pmb{q}^{(1)},\\dots,\\pmb{q}^{(N)})\\in\\mathbb{R}^{T\\times N}$ When $x_{T+1}=k$ , the output token $x_{o}$ is sampled from the distribution ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{P}(x_{o}=n\\mid x_{T+1}=k,\\pmb{x})=\\sum_{t=1}^{T}q_{t}^{(k)}P_{n,x_{t}},\\quad\\forall n\\in[N].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In words, we first sample a position $s\\in[T]$ according to $\\pmb q^{(k)}$ and then run one step of the Markov Chain $P$ from $x_{s}$ to generate $x_{o}$ . Note that this model can be represented by a one-layer linear transformer (see the next subsection for details). We make the following assumptions on SCB task. ", "page_idx": 3}, {"type": "text", "text": "Assumption 2.1 (Sparse Contextual Bigram, SCB). In the SCB task, we assume the following: ", "page_idx": 3}, {"type": "text", "text": "(a) ( $\\boldsymbol{Q}$ -sparse) For some $Q\\ll T$ and each of $\\pmb q^{(k)}$ , at most $Q$ entries are nonzero.   \n$(b)$ (Well-conditioned) There exists some constant $C\\ge1$ such that for every $k\\,\\in\\,[N]$ and $t\\in[T],\\,q_{t}^{(k)}\\in[1/(C Q),C/Q]$ if it is nonzero, and $\\mu_{k}\\in[1/(C N),C/N]$   \n(c) (Nontrivial transition) $\\|P\\|_{\\mu}^{2}-\\|\\mu\\|^{2}\\geq\\|\\mu\\|^{2}$   \n(d) (Long sequence) $T\\geq(N Q)^{10}$ ", "page_idx": 3}, {"type": "text", "text": "Remark on condition (c). We say the transition $P$ is trivial if the transition probability vectors are all the same, i.e., $P\\,=\\,\\mu1^{\\top}$ . In this case, we have $\\|P\\|_{\\mu}^{2}\\,=\\,\\langle\\pmb{\\mu}\\mathbf{1}^{\\top},\\pmb{\\mu}\\pmb{\\mu}^{\\top}\\rangle\\,=\\,\\|\\pmb{\\mu}\\|^{2}$ .Requiring $\\|\\pmb{P}\\|_{\\mu}^{2}-\\|\\pmb{\\mu}\\|^{2}\\geq\\|\\pmb{\\mu}\\|^{2}$ rules out situations where $P$ is too close to the trivial one. Also, note that for any well-conditioned $\\pmb{\\mu}$ , we have $\\|\\pmb{\\mu}\\|^{2}\\geq\\Omega(1/N)$ ", "page_idx": 3}, {"type": "text", "text": "In this work, we focus on the case where $(\\pmb{x},x_{T+1},x_{o})$ are given as (one data point of) the training data with $(x_{1},\\dots,x_{T+1})$ i.i.d. sampled from $\\mu$ . The SCB task can be extended to a sequence-to-sequence model: we drop $x_{1}$ and append $x_{o}$ to get a new input sequence $(x_{2},\\ldots,x_{T+1},x_{o})$ , and then repeat theameanrcduratnhiratqe $(x_{t})_{t=1}^{\\infty}$ Where $(x_{T+2},x_{T+1},\\dots)$ are not independent, and this makes our model a true language model. We leave the study of the more complicated learning-from- $(x_{t})_{t=1}^{\\infty}$ task to future works. ", "page_idx": 3}, {"type": "text", "text": "2.2  Transformer architecture ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our learner model is a one-layer single-head linear transformer (Akyirek et al., 2022; Zhang et al. 2023b; Ahn et al., 2023). A general linear transformer can be expressed as: $F(x,x_{T+1};V,A)=$ $V E\\left(E^{\\top}A E\\right)$ ,where ${\\pmb E}$ is the embedding of the input tokens and positions, and $\\boldsymbol{A}$ $V$ are the parameters of the attention and output layers, respectively. In our setting, we only need a simpler model: ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\cal F}(x,x_{T+1};V,A):=V X\\left(I_{T}A e_{x_{T+1}}\\right)=:V X a^{\\left(x_{T+1}\\right)},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $V\\in\\mathbb{R}^{N\\times N}$ and $A\\in\\mathbb{R}^{T\\times N}$ are the trainable parameters, and ${\\pmb a}^{(k)}$ denotes the $k$ -th column of $\\boldsymbol{A}$ . This model uses cross-attention (replacing the last ${\\pmb E}$ with $e_{x_{T+1}}.$ ), uses only the positional embeddings together with the last token to compute the attention weights (replacing the second ${\\pmb E}$ with $I_{T}$ ), and discards the positional embeddings in the output layer (replacing the first ${\\pmb E}$ with $X$ This is equivalent to manually set certain blocks in the weight matrices to O, which is a common practice in the theoretical literature to simplify the analysis (Nichani et al., 2024; Huang et al., 2023; Zhang et al., 2023b). ", "page_idx": 3}, {"type": "text", "text": "Note that our data-generating model (1) can be represented using (2) by setting $A=Q$ and $V=P$ We will show that a modified version of SGD can approximately recover this ground-truth model. ", "page_idx": 3}, {"type": "text", "text": "2.3  Training algorithm ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We assume that the stationary distribution $\\pmb{\\mu}$ and certain norms of the ground-truth $P$ and $\\b{Q}$ are known when choosing the initialization and learning rate. The goal here is to recover $P$ and $Q$ . Our loss function is the $\\ell_{1}$ -regularized MSE loss. The standard way to optimize an $\\ell_{1}$ -regularized loss is to use the proximal gradient descent. We adopt this algorithm with several additional pre-conditioning and a projection step to ensure some basic properties. ", "page_idx": 3}, {"type": "text", "text": "Formally, let the per-sample loss be defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\nl(\\pmb{x},x_{T+1},x_{o};\\pmb{V},\\pmb{A}):=\\frac{1}{2}\\left\\|\\pmb{e}_{x_{o}}-V\\pmb{X}\\pmb{A}\\pmb{e}_{x_{T+1}}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We initialize $A=1_{T}\\mathbf{1}_{N}^{\\top}/T$ to have uniform attention and $V=\\mu\\mathbf{1}^{\\top}$ to be the trivial transition. At each step\u22650,we samleBfreshsamles , \\* x t form amini-batch. The -regularized mini-batch loss is defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\nl_{\\mathrm{reg}}^{(B_{\\tau},\\lambda)}\\left(\\{\\pmb{x}^{(i)},x_{T+1}^{(i)},x_{o}^{(i)}\\}_{i=1}^{B_{\\tau}};\\pmb{V},\\pmb{A}\\right):=\\frac{1}{B_{\\tau}}\\sum_{i=1}^{B_{\\tau}}l(\\pmb{x}^{(i)},x_{T+1}^{(i)},x_{o}^{(i)};\\pmb{V},\\pmb{A})+\\lambda\\sum_{k=1}^{N}\\left\\|\\pmb{a}^{(k)}\\right\\|_{1},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Where $\\lambda\\,>\\,0$ is a parameter that controls the strength of regularization. Let $\\nabla_{V}^{(B_{\\tau})}l$ and $\\nabla_{A}^{(B_{\\tau})}l$ denote the mini-batch gradients of the original $l$ w.r.t. $V$ and $\\boldsymbol{A}$ , respectively. We then define the preconditioned gradients as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\hat{\\nabla}_{V}^{(B_{\\tau})}l:=\\left({\\cal I}_{N}-\\frac{{\\bf1}_{N}{\\bf1}_{N}^{\\top}}{N}\\right)\\left(\\nabla_{V}^{(B_{\\tau})}l\\right)\\mathrm{diag}(1/\\mu)\\left({\\cal I}_{N}-\\frac{\\mu\\mu^{\\top}}{\\left\\|\\mu\\right\\|^{2}}\\right),}\\\\ {\\hat{\\nabla}_{a^{(k)}}^{(B_{\\tau})}l:=\\displaystyle\\frac{1}{\\mu_{k}}\\left({\\cal I}_{T}-\\frac{{\\bf1}_{T}{\\bf1}_{T}^{\\top}}{T}\\right)\\left(\\nabla_{a^{(k)}}^{(B_{\\tau})}l\\right),\\quad\\forall k\\in[N].}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, the $1/\\mu$ rescaling plays a role similar to importance sampling. We multiply $\\nabla_{V}^{(B_{\\tau})}l$ with $I-11^{\\top}/N$ and $I-\\mu\\mu^{\\top}/\\|\\mu\\|^{2}$ to ensure at least $\\mathbf{1}_{N}^{\\top}V=\\mathbf{1}_{N}^{\\top},\\mathbf{1}_{T}^{\\top}A=\\mathbf{1}_{N}^{\\top}$ , and $V\\pmb{\\mu}=\\pmb{\\mu}$ always hold throughout training. Note that we project each column of $V$ to the affine space $\\left\\{\\pmb{\\nu}\\in\\mathbb{R}^{T}:\\mathbf{1}^{\\top}\\pmb{\\nu}=1\\right\\}$ instead of the probability simplex. This is suficient for our analysis and is much easier to compute than the latter. We update the output layer using ", "page_idx": 4}, {"type": "equation", "text": "$$\nV_{\\tau+1}=V_{\\tau}-\\eta_{V}\\hat{\\nabla}_{V}^{(B_{\\tau})}l,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\eta_{V}\\,>\\,0$ is the step size. Now, consider the attention layer. Due to the existence of the $\\ell_{1}$ -regularization, the update rule becomes a simple variant of the standard proximal gradient descent. Formally, for step size $\\eta_{A}>0$ ,each $k\\in[N]$ and $t\\in[T]$ ,wehave ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{a_{\\tau+1}^{(k,\\tau)}=a_{\\tau}^{(k)}-\\frac{\\eta_{A}}{\\mu_{k}}\\nabla_{a^{(k)}}^{(B_{\\tau})}l,}&{\\mathrm{(preconditioned~}\\mathbf{G}),}\\\\ &{a_{t,\\tau+1}^{(k,\\tau)}=\\left\\{a_{t,\\tau+1}^{(k,\\tau)}-\\lambda,\\quad\\mathrm{if~}a_{t,\\tau+1}^{(k,\\tau)}\\ge\\lambda,\\right.}&{\\mathrm{(proximal~step),}}\\\\ &{\\left.a_{\\tau+1}^{(k)}=\\displaystyle\\int_{0}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For the proximal step, we will later show that no $a_{t}^{(k)}$ can ever become smaller than $-\\lambda$ , so it suffices to consider those two cases. During the proximal sep, all small a(k) are set to O, and $\\lambda$ is subtracted from allarge coordinate. For notational simlicity, we defne g := -(a $\\begin{array}{r}{\\pmb{g}_{\\lambda,\\tau}^{(k)}:=-\\eta_{A}^{-1}(\\pmb{a}_{\\tau+1}^{(k)}-\\pmb{a}_{\\tau}^{(k)})}\\end{array}$ so that we can write the update as $\\pmb{a}_{\\tau+1}^{(k)}=\\pmb{a}_{\\tau}^{(k)}-\\eta_{A}\\pmb{g}_{\\tau}^{(k)}$ g(k). We will choose  = 0 in certan stages of training. In this case, (6) becomes the usual projected preconditioned gradient descent and we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pmb{a}_{\\tau+1}^{(k)}=\\pmb{a}_{\\tau}^{(k)}-\\eta_{A}\\hat{\\nabla}_{\\pmb{a}^{(k)}}^{(B_{\\tau})}l\\qquad(\\mathrm{when}\\;\\lambda=0).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Our algorithm consists of three stages with different hyperparameters being used in different stages and certain rounding required between stages. The pseudocode is given in Algorithm 1 and more details on the projection/normalization steps are provided in Appendix $\\mathrm{E}$ and F. When we train the model from scratch, all three stages are used and the initialization is $V_{0}=\\mu\\mathbf{1}^{\\top}$ and $A=1_{T}\\mathbf{1}_{N}^{\\top}/T$ ", "page_idx": 4}, {"type": "text", "text": "Transfer learning. When doing transfer learning, the initialization will be obtained from the weights of the pre-trained model and one step of gradient update. Then, we will run Algorithm 1 from Stage 2. ", "page_idx": 4}, {"type": "text", "text": "$\\pmb{\\mu}$ m $V_{0},A_{0}$ $\\eta_{A}^{(i)},\\eta_{V}^{(i)}$ $i\\in[3]$ $\\lambda_{0}$ $\\hat{\\lambda}$ $\\mathcal{T}_{1},\\mathcal{T}_{2},\\mathcal{T}_{3}$ Stage 1: Run (5) and (6) with $\\eta_{A}=\\eta_{A}^{(1)},\\eta_{V}=\\eta_{V}^{(1)},\\lambda$ $\\lambda=0$ $\\mathcal{T}_{1}$ steps: Thresholding-projection: $\\forall k\\in[n]$ $\\hat{{\\boldsymbol a}}^{(k)}=[a_{t}^{(k)}\\mathbb{1}\\{a_{t}^{(k)}\\ge\\lambda_{0}\\}]_{t},{\\boldsymbol a}^{(k)}\\leftarrow(I_{T}-\\mathbf{1}_{T}\\mathbf{1}_{T}^{\\top}/T)\\hat{{\\boldsymbol a}}^{(k)}$ Stage2: Run (5) and (6) with $\\eta_{A}=\\eta_{A}^{(2)},\\eta_{V}=\\eta_{V}^{(2)},.$ $\\lambda=\\hat{\\lambda}$ 0 $\\mathcal{T}_{2}-\\mathcal{T}_{1}$ steps;: Thresholding-normalization: $\\forall k\\ \\in\\ [n]$ \uff0c $\\hat{\\pmb{a}}^{(k)}\\;=\\;[a_{t}^{(k)}\\mathbb{1}\\{a_{t}^{(k)}\\;\\ge\\;\\Omega(1/Q)\\}]_{t}.\\quad\\pmb{a}^{(k)}\\;\\in\\;\\$ $\\hat{\\pmb q}^{(k)}/\\mathbf1^{\\top}\\hat{\\pmb q}^{(k)}$ Stage3: Run 5) and (6) with $\\eta_{A}=0$ $\\eta_{V}=\\eta_{V}^{(3)}$ \uff0c $\\lambda=0$ for $\\mathcal{T}_{3}-\\mathcal{T}_{2}$ steps ", "page_idx": 5}, {"type": "text", "text": "Output: $A_{\\mathcal{T}_{3}},V_{\\mathcal{T}_{3}}$ ", "page_idx": 5}, {"type": "text", "text": "3   Results for training from scratch ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we consider the situations where we train the model from scratch, i.e., the initialization is $V_{0}=\\mu\\mathbf{1}^{\\top}$ and $A_{0}=\\mathbf{1}\\mathbf{1}^{\\top}/T$ and discuss the ideas of the proof of the following theorem. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.1 (Theorem G.1). Let $\\varepsilon\\ \\succ\\ 0$ be our target accuracy and $\\mathcal{T}_{1}\\ =\\ \\operatorname*{min}\\{\\tau\\ \\geq\\ 0\\ :$ $\\operatorname*{max}\\{\\alpha_{V,\\tau},\\alpha_{A,\\tau}\\}~\\geq~\\Theta(1/(Q N))\\}$ \uff1aWe can choose thehyperparametersin Algorithm $^{\\,l}$ such that within poly $(N,Q,1/\\varepsilon,\\log T)$ steps, we have $\\|A-Q\\|_{\\mu}^{2}\\;\\leq\\;\\varepsilon$ and $\\|V-P\\|_{\\mu}^{2}\\;\\leq\\;\\varepsilon$ with probability at least $1-\\delta$ and thenumbersof samplesusedbeforeand after $\\mathcal{T}_{1}$ are poly $(T,\\delta)$ and poly $(N,Q,1/\\varepsilon,\\log T,\\delta)$ ,respectively. ", "page_idx": 5}, {"type": "text", "text": "The overall strategy is analyzing the population process and then controlling the distance between the mini-batch trajectory and the population process2. In Section 3.1, we discuss the key properties of the population process that simplify the analysis. After that, we describe the dynamics of the algorithm and the signal-noise-ratio (SNR) in each of the three stages of Algorithm 1 in Section $3.2{\\sim}3.4$ ", "page_idx": 5}, {"type": "text", "text": "3.1  The population process ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this subsection, we analyze the behavior of the population process and the evolution of the signal-noise ratio. More details can be found in Appendix C, where the so-called population projected process are defined and rigorously analyzed. ", "page_idx": 5}, {"type": "text", "text": "For ease of presentation, we assume $\\lambda=0$ and access to the population loss $\\mathcal{L}:=\\mathbb{E}\\,l$ In other words, we consider the projected preconditioned gradient descent. By Lemma B.8, the dynamics of the population process is controlled by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{V_{\\tau+1}=V_{\\tau}-\\eta_{V}\\left(\\|A\\|_{\\mu}^{2}\\left(V-\\mu{\\bf1}^{\\top}\\right)-\\langle Q,A\\rangle_{\\mu}\\left(P-\\mu{\\bf1}^{\\top}\\right)\\right),}}}\\\\ {{\\displaystyle{A_{\\tau+1}=A_{\\tau}-\\eta_{A}\\left(\\left(\\|V\\|_{\\mu}^{2}-\\|\\mu\\|^{2}\\right)\\left(a^{(k)}-\\frac{1}{T}\\right)-\\left(\\langle V,P\\rangle_{\\mu}-\\|\\mu\\|^{2}\\right)\\left(q^{(k)}-\\frac{1}{T}\\right)\\right).}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "One can prove via induction on $\\tau$ that $V$ (resp. $\\boldsymbol{A}$ ) always stays on the straight line crossing $\\mu1^{\\top}$ and $P$ (resp. $\\mathbf{11^{\\top}}/T$ and $\\b{Q}$ ). In other words, there exists some time-dependent real numbers $\\alpha_{V,\\tau},\\alpha_{A,\\tau}$ $\\beta_{V,\\tau}:=1-\\alpha_{V,\\tau},\\beta_{A,\\tau}:=1-\\alpha_{A,\\tau}$ such that $V_{\\tau}=\\alpha_{V,\\tau}P+\\beta_{V,\\tau}\\mu\\bar{\\mathbf{l}}^{\\top}$ and $A_{\\tau}=\\alpha_{A,\\tau}\\mathbf{Q}+\\beta_{A,\\tau}\\mathbf{1}\\mathbf{1}^{\\top}/T$ The same calculation yields the following equations that govern the dynamics of $\\alpha_{V}$ and $\\alpha_{A}$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\alpha_{V,\\tau+1}=\\alpha_{V,\\tau}+\\eta_{V}K_{Q}\\left(1-\\alpha_{A}\\alpha_{V}\\right)\\alpha_{A}+\\eta_{V}\\frac{1-\\alpha_{V}}{T},}\\\\ {\\displaystyle\\alpha_{A,\\tau+1}=\\alpha_{A,\\tau}+\\eta_{A}K_{P}\\left(1-\\alpha_{V}\\alpha_{A}\\right)\\alpha_{V},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\alpha_{V,0}\\,=\\,\\alpha_{A,0}\\,=\\,0,$ $K_{P}:=\\,\\|P\\|_{\\mu}^{2}\\,-\\,\\|\\mu\\|^{2}\\,\\gtrsim\\,1/N$ and $K_{Q}\\,:=\\,\\|Q\\|_{\\mu}^{2}\\,-\\,1/T\\,\\gtrsim\\,1/Q$ .Choose $\\eta_{V}=\\eta/K_{Q}$ and $\\eta_{A}=\\eta/K_{P}$ , and we can write the above in matrix form as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\left[\\alpha_{V,\\tau+1}\\right]=\\eta(1-\\alpha_{A}\\alpha_{V})\\left[\\!\\!\\begin{array}{c c}{0}&{1}\\\\ {1}&{0}\\end{array}\\!\\!\\right]\\left[\\!\\!\\begin{array}{c}{\\alpha_{V,\\tau}}\\\\ {\\alpha_{A,\\tau}}\\end{array}\\!\\!\\right]+\\frac{\\eta}{K_{Q}}\\left[\\!\\!\\begin{array}{c}{(1-\\alpha_{V})/T}\\\\ {0}\\end{array}\\!\\!\\right].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "2 Strictly speaking, what we actually control is the distance of the mini-batch trajectory to the subspace the population process lies. This allows us to prevent the potential exponential growth of the error caused by error compounding in the analysis. For details on this technique, see Appendix C. ", "page_idx": 5}, {"type": "text", "text": "Hence, in order to analyze the population process, it suffices to analyze the above 2-dimensional ODE. In what follows, when we say the signal, we usually refer to these $\\alpha$ 's or some quantities whose size is proportional to them. In particular, as one can see from (8), the size of the expected gradients is proportionalto $\\alpha_{V}$ and/or $\\Bar{\\alpha_{A}}^{3}$ ", "page_idx": 6}, {"type": "text", "text": "Note that when both $\\alpha_{V},\\alpha_{A}$ are still small, the population dynamics of $\\alpha_{V},\\alpha_{A}$ are a linear system with coeficient matrix $\\eta\\big[\\mathop{0}_{1}^{\\mathrm{~0~l~}}\\big]$ and drift $\\left[\\begin{array}{c}{{\\eta T/K_{Q}}}\\\\ {{0}}\\end{array}\\right]$ The driterm ill provide a sallinitial signal that guides the process toward the correct direction and then the linear term will amplify this signal. Since the linear term is close to O at initial and the initial signal provided by the drift term has order $1/T$ , we should expect that poly $T$ samples are necessary to distinguish it from noises (Stage 1). After the signal becomes reasonably large, the first term will have order $1/\\mathfrak{p o l y}(N,Q)$ , and we can then rely on it (combined with the $l_{1}$ -regularization) instead of the drift term to learn the model (Stage 2). ", "page_idx": 6}, {"type": "text", "text": "3.2 Stage 1: boosting the signal ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "At initialization, we have $\\alpha_{V}=\\alpha_{A}=0$ . We define Stage 1 to be the phase until at least one of them has grown from O to some small $\\sigma_{1}=1/\\mathrm{poly}(N,Q)$ . Note that in this stage, the mini-batch version of (8) is approximately equivalent to ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\left[\\!\\!\\begin{array}{c c}{\\alpha_{V,\\tau+1}}\\\\ {\\alpha_{A,\\tau+1}}\\end{array}\\!\\!\\right]\\approx\\eta\\left[\\!\\!\\begin{array}{c c}{0}&{1}\\\\ {1}&{0}\\end{array}\\!\\!\\right]\\,\\left[\\!\\!\\begin{array}{c c}{\\alpha_{V,\\tau}}\\\\ {\\alpha_{A,\\tau}}\\end{array}\\!\\!\\right]+\\frac{\\eta}{K_{Q}}\\left[\\!\\!\\begin{array}{c c}{1/T}\\\\ {0}\\end{array}\\!\\!\\right]+\\pmb{\\varepsilon}_{\\mathrm{noi\\,se}}+\\pmb{\\varepsilon}_{\\mathrm{approx}},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\pmb{\\varepsilon}_{\\mathrm{noise}}$ and $\\pmb{\\varepsilon}_{\\mathrm{approx}}$ represent the errors introduced by the difference between the mini-batch and population gradients, and the fact that we are not exactly on the population trajectory. If we had infinite amount of samples so that both $\\pmb{\\varepsilon}_{\\mathrm{noise}}$ and $\\pmb{\\varepsilon}_{\\mathrm{approx}}$ were O,then the second term on the RHS of (9) could provide a small positive signal to $\\alpha$ and the first term would quickly boost it to $\\sigma_{1}$ within $\\mathcal{T}_{1}=\\log(T)/\\eta$ iterations. In order for the above analysis to work, we need both $\\pmb{\\varepsilon}_{\\mathrm{noise}}$ and $\\pmb{\\varepsilon}_{\\mathrm{approx}}$ to be at least ${\\cal O}(1)/T$ small. Since, unfortunately, $\\pmb{\\varepsilon}_{\\mathrm{noise}}$ does not scale with $1/T$ , we need poly $(T)$ samples to ensure these conditions. ", "page_idx": 6}, {"type": "text", "text": "We conjecture that this poly $(T)$ dependence is unavoidable (when only a polynomial amount of computing time is available). That is because around the initialization, the only signal comes from the second term and the first term amplifies whatever the second term provides, even if it has been corrupted by the errors. It either takes poly $(T)$ fresh samples each step to reveal the signal or poly $(T)$ steps (whence also poly $(T)$ samples) for the random noises to (hopefully) cancel with each other. ", "page_idx": 6}, {"type": "text", "text": "3.3 Stage 2: learning the model ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We know that at the end of Stage 1, at least one of $\\alpha_{V}$ and $\\alpha_{A}$ is $\\sigma_{1}={1}/{\\mathrm{poly}}(\\boldsymbol{Q},N)$ large. Hence, one may expect that the signal is $1/\\mathfrak{p o l y}(Q,N)$ large now so that we no longer need to make the noises $1/T$ small and therefore, only poly $(N,Q)$ samples are needed. Unfortunately, this argument will not work directly, since the variance of the mini-batch gradients scales with $T$ 4 Therefore, we still need poly $(T)$ samples to reduce the squared $\\ell_{2}$ -norm of $\\pmb{\\varepsilon}_{\\mathrm{noise}}$ from $\\Omega(T)$ to $1/\\ensuremath{\\mathrm{poly}}(Q,N)$ . To address this issue, we introduce the $\\ell_{1}$ -regularizer and use a variant of proximal gradient descent. ", "page_idx": 6}, {"type": "text", "text": "The idea is, while the concentration in the $\\ell_{2}$ sense is difficult, controlling the $\\ell_{\\infty}$ -error is easy as every entry of $\\nabla_{\\pmb{a}^{(k)}}l$ is bounded whence subgaussian. As a result, we can make the coordinatewise difference between the population and mini-batch gradients $1/\\mathfrak{p o l y}(N,Q)$ small using only $\\mathsf{o l y}(N,Q)\\log T$ samples by a standard concentration argument. Moreover, we have (cf. the proof of Lemma E.3) ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{-\\mathbb{B}\\,\\partial_{a_{t}^{(k)}}l=\\mu_{k}\\alpha_{V}K_{P}\\left(q_{t}^{(k)}-\\alpha_{V}a_{t}^{(k)}\\right)=\\mu_{k}\\alpha_{V}K_{P}\\times\\left\\{\\Omega(1/Q),\\qquad\\mathrm{if~}q_{t}^{(k)}\\neq0,\\qquad\\mathrm{if~}q_{t}^{(k)}\\neq0,}\\\\ {O(\\alpha_{V}a_{t}^{(k)}),\\quad\\mathrm{if~}q_{t}^{(k)}=0.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Thus, as long as $\\alpha_{V}\\geq1/\\mathrm{poly}(N,Q)$ , the $\\ell_{\\infty}$ -norm of the gradient noise being small is enough to Ccreateaseparaion bewen thoseusefulenties $(q_{t}^{(k)}\\neq0)$ anduseles enties (k) = 0) and ensure the $\\ell_{2}$ -error of those $Q$ useful entries is small. ", "page_idx": 6}, {"type": "text", "text": "The above analysis suggests removing all small entries from the gradient will work. Now, we claim that $\\ell_{1}$ -regularization and proximal gradient descent naturally implement this strategy, at least approximately. We believe softmax-based attention layers also automatically implement this strategy. See Section 5 for more discussion on the relationship between our model and softmax transformers. ", "page_idx": 7}, {"type": "text", "text": "Note that, at the end of Stage 1 and after the thresholding-projection step \u2014 which is approximately equivalent to running one proximal step first - we know that ll useful $\\bar{a}_{t}^{(k)}$ are at least $\\Omega(\\alpha_{V}/Q)=$ $1/\\mathfrak{p o l y}(N,Q)$ , while all useless entries are of size ${\\cal O}(1)/T$ . By our previous discussion, we know that if $\\lambda$ is chosen appropriately, with poly $(N,Q,\\log T)$ samples, the gradients w.r.t. those useful entries can be made approximately correct, while the gradients w.r.t. those useless entries are much smallrthan $\\lambda$ $a_{t}^{(k)}$ much smaller than $\\lambda$ . As a result, they will be set to O in the proximal step (and to $O(1/T)$ after the projection step), which is equivalent to filtering out all those entries, up to a small bias. Therefore, the proximal gradient updates stay close to the population trajectory, and the growth of the signals $\\alpha_{A},\\alpha_{V}$ can be analyzed using the population dynamics. ", "page_idx": 7}, {"type": "text", "text": "WeendStage 2when $(\\alpha_{V}+\\alpha_{A})/2\\approx1$ . Similar to Stage 1, this also only takes $\\mathcal{T}_{2}=\\tilde{O}(1/\\eta)$ steps. We also show that the difference between the mini-batch trajectory and the \u201cpopulation trajectory' can decrease to a small value (cf. Lemma E.10). This allows us to decouple the error introduced by Stage 1 and the target accuracy. We defer the proof details to Appendix E. ", "page_idx": 7}, {"type": "text", "text": "3.4  Stage 3: final rounding and convergence ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The purpose of Stage 3 is to fix a minor issue regarding $\\left|\\alpha_{V}\\mathrm{~-~}\\alpha_{A}\\right|$ .Taylor expand (8) around $(\\alpha_{A},\\alpha_{V})=(1,1)$ and one will notice that although $(\\alpha_{V}+\\alpha_{A})/2$ can converge to 1 at a linear rate (and the approximation error also decreases exponentially fast), the convergence rate of $\\alpha_{A}-\\alpha_{V}$ is much slower, and the process will get stuck around $(1+\\delta,1-\\delta)$ for some small nonzero $\\delta$ , instead of converging to $(1,1)$ . To accelerate this process, we directly round $\\boldsymbol{A}$ via normalization, which is possible only after the approximation error becomes small in Stage 2. Then we freeze $\\boldsymbol{A}$ and train $V$ to the desired accuracy. More details about this stage can be found in Appendix F. ", "page_idx": 7}, {"type": "text", "text": "4  Results for transfer learning ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The transferability of neural networks and transformers and their benefits have been widely observed and studied in both practice and theory. It is often assumed that the downstream and pretraining tasks share a common structure or representations/features, and these models can learn these common structures during training, and then leverage them in fine-tuning. ", "page_idx": 7}, {"type": "text", "text": "In this section, we offer a different perspective: as long as there is a (potentially small) nontrivial correlation between the pretraining and downstream tasks, the pretrained model can be used to provide a nonzero initial signal, allowing us to bypass the initial sample-intensive signal-boosting stage. ", "page_idx": 7}, {"type": "text", "text": "Formally, we consider the following setting. Let $\\hat{P}$ be the transition matrix of the pretraining task and $(P,Q)$ the transition matrix and $\\b{Q}$ -matrix of the downstream task. We still assume Assumption 2.1. In addition, we assume $\\hat{P}$ and $P$ share the same stationary distribution $\\pmb{\\mu}$ $\\left\\|\\hat{\\pmb{P}}\\right\\|_{\\mu}^{2}=\\Theta(1)\\left\\|\\hat{\\pmb{P}}\\right\\|_{\\mu}^{2}$ and $\\left\\langle\\hat{P},P\\right\\rangle_{\\mu}-\\|\\pmb{\\mu}\\|^{2}\\geq\\|\\pmb{\\mu}\\|^{2}$ . The last condition can be viewed as the transfer learning version of condition (c) of Assumption 2.1. Note that we allow the correlation between $\\hat{P}$ and $P$ to be as small as $o(1)$ Theorem 4.1 (informal version of Theorem H.3). Consider the above setting. Initialize $A=11^{\\top}/T$ $\\pmb{V}\\,=\\,\\theta\\hat{\\pmb{P}}+(1\\,-\\,\\theta)\\pmb{\\mu}\\mathbf{1}^{\\top}$ for some small $\\theta\\:>\\:0_{:}$ . and run one step of gradient update on $\\boldsymbol{A}$ :Then, running Algorithm 1 fromStage 2 allows us to recover $(P,Q)$ to $\\varepsilon$ -accuracywith high probability with poly $(N,Q,1/\\varepsilon,\\log T)$ samples. ", "page_idx": 7}, {"type": "text", "text": "To intuitively see why using poly $(N,Q,1/\\varepsilon,\\log T)$ samples is possible, recall from (9) that the reason we need poly $(T)$ samples in Stage 1 is the signal is additive and has order ${\\cal O}(1/T)$ , so we need the size of the noise to be at most ${\\cal O}(1/T)$ . On the other hand, when we initialize $\\pmb{V}=\\theta\\hat{\\pmb{P}}+(1-\\theta)\\pmb{\\mu}\\mathbf{1}^{\\top}$ ,we have $\\alpha_{V}\\geq\\Theta(\\theta/(N K_{P}))\\gg1/T$ . Then we can rely on $\\alpha_{V}$ , instead of the $1/T$ -sized additive signal, to boost the signal of $\\alpha_{A}$ to $\\omega(1/T)$ in one step, which leads to a sample complexity that depends on $\\alpha_{V}$ instead of the $1/T$ -sized additive signal. Then, we can reuse the analysis of Stage 2 and 3 to show that the downstream $(P,Q)$ can be approximately recovered using poly $(N,Q,1/\\varepsilon,\\log T)$ ", "page_idx": 7}, {"type": "text", "text": "Note that unlike the case of training from scratch, when performing transfer learning, the initial approximation error $\\|\\Delta_{V}\\|_{\\mu}$ , i.e., the distance between $V$ and its population projection, can be much larger than the signal $\\alpha_{V}$ , and it might seem unreasonable to expect that we can leverage the small signal in the presence of a large approximation error. To handle this issue, we show that the infuence of $\\|\\Delta_{V}\\|_{\\mu}^{2}$ on the dynamics scales with $\\alpha_{A}(\\approx\\alpha_{V})$ , which is small. In addition, we also show that as long as $\\alpha_{A}$ is bounded away from O and the batch size is large, the approximation error will not grow. This allows us to ignore the approximation errors in the signal-boosting stage until we enter the regime of the Stage 2 analysis. ", "page_idx": 8}, {"type": "text", "text": "5  Experiments and relationship with softmax transformers ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This section contains our experimental results. We also discuss the relationship between our linear model and the usual softmax transformers. ", "page_idx": 8}, {"type": "text", "text": "Experiment setup We use the same shallow transformer model (2) to train on the synthetic data. The data distribution follows the SCB model (1) with a randomly sampled transition matrix $P$ together with its stationary $\\mu$ , and the ground truth attention pattern $\\b{Q}$ .We choose the number of states $N=3$ sparsity $Q=2$ , and the sequence length $T=5000\\gg N,Q$ . We use a batch size $B=64$ to run the online projected proximal gradient descent with $\\lambda=1\\mathrm{e}{-5}$ and the vanilla SGD for $\\mathcal{T}=1000$ iterations. Through the signal boosting stage $\\tau\\in[0,400]$ , we use $\\eta_{1}=0.01$ to accelerate the process. After $\\tau>400$ ,we use $\\eta_{2}=0.005$ for further improvement. For SGD, we add another set of experiments with $\\eta_{2}^{\\prime}=0.001$ to prevent potential instability. For more details, see Appendix I. ", "page_idx": 8}, {"type": "text", "text": "5.1 Convergence ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Our experiments (cf. Fig. 1) show that after switching to proximal gradient descent after Stage 1 (the signal-boosting stage), both $\\|P-V\\|_{\\mu}$ and $\\|A-Q\\|_{\\mu}$ decrease faster than SGD. The final distance to the ground-truth after normalization gets close to'O, and the similarity between the ground truth and parameters quickly converges close to 1. In comparison, SGD struggles to converge with the same small batch size and large learning rate, while the convergence rate is too slow when a smaller learning rate is used. This phenomenon verifies our theory that the variance of the original stochastic gradient will be too large for SGD to converge when $T\\gg Q,N$ , while proximal gradient descent with an $\\ell_{1}$ regularizer can resolve this issue. ", "page_idx": 8}, {"type": "image", "img_path": "PukaVAwYBo/tmp/9ca7b9f6c637df5798af498f6caca4cb1ebae31aca3be2e39a57f02d22020741.jpg", "img_caption": ["Figure 1: Convergence analysis: We plot the distance to the ground truth $\\|V-P\\|_{\\mu},\\|A-Q\\|_{\\mu}$ in different settings. After stage 1 ends at $\\tau=400$ (when $\\alpha_{A},\\alpha_{V}\\approx0.1)$ 0, we use vanillia SGD and our proximal gradient method to train the transformer. Compared with SGD, the $\\ell_{1}$ regularized proximal gradient descent quickly converges, and the final solution (the star) recovers the ground truth. SGD either suffers from the large gradient variance (when $\\eta_{2}$ is large) or a slow convergence rate (small $\\eta_{2}^{\\prime}$ "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.2  Relationship between our model and softmax transformers ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We claim that they have our linear model and softmax transformers have qualitatively similar behaviors: there will be a sample-intensive initial stage, and after the model and the target have a nontrivial correlation, proximal gradient descent/SGD will become much more sample efficient. ", "page_idx": 8}, {"type": "text", "text": "For ease of presentation, in the following, we will assume $N=1$ , write $\\pmb{a}:=\\pmb{a}^{(1)}$ , and assume the ground-truth $\\pmb q:=\\pmb q^{(1)}$ .s $\\pmb{e}_{1}=(1,0,\\ldots,0)$ . Most of our argument below can be generalized to the general setting at least at a heuristic level. Recall that our linear model is $f(X;V,\\pmb{a})=V X\\pmb{a}$ . By a softmax transformer, we mean the model $f_{\\sigma}(X;V,w)=V X\\sigma(w)=:V X\\pmb{a}_{\\sigma}$ where $\\sigma$ is the softmax function and $\\pmb{w}\\in\\mathbb{R}^{T}$ is the trainable first-layer weights. ", "page_idx": 8}, {"type": "image", "img_path": "PukaVAwYBo/tmp/746c0be1fe9c820a25a68eb23f0f0c94471ad6cc7f88b8ad696cebe14fd8c0ef.jpg", "img_caption": ["Figure 2: Similarity between the softmax and linear attention. We train two transformers with (1) (Left) softmax attention and (2) (Middle) linear attention layer on the SCB tasks with the same ground-truth $(T=50,N=10,Q=2)$ . The attention pattern and the value matrix (learned transition matrix) are very similar (left two plots) and they converge to approximately the same loss (right plot). "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Let $l$ denote the (per-sample) loss. We have $\\nabla_{w}l(f_{\\sigma}(X))=\\left(\\mathrm{diag}(a_{\\sigma})-a_{\\sigma}a_{\\sigma}^{\\top}\\right)(V X)^{\\top}\\nabla l(f_{\\sigma}(X)).$ As a result, the dynamics of the attention weights are controlled by ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{a(\\tau+1)\\approx a(\\tau)-\\eta\\left(I-\\frac{\\displaystyle\\mathbf{1}\\mathbf{1}^{\\top}}{T}\\right)(V X)^{\\top}\\nabla l(f(X)),}&{\\mathrm{in~our~linear~model},}\\\\ &{a_{\\sigma}(\\tau+1)\\approx a_{\\sigma}(\\tau)-\\eta\\left(\\mathrm{diag}(a_{\\sigma})-a_{\\sigma}a_{\\sigma}^{\\top}\\right)^{2}(V X)^{\\top}\\nabla l(f_{\\sigma}(X)),}&{\\mathrm{in~softmax~transformers}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "In other words, the main difference is that there will be a preconditioning matrix $(\\mathrm{diag}(\\pmb{a})-\\pmb{a}\\pmb{a}^{\\top})^{2}$ in the dynamics of softmax transformers. ", "page_idx": 9}, {"type": "text", "text": "Near initialization, i.e., when the attention pattern is still close to the uniform attention, we have $\\begin{array}{r}{\\big(\\mathrm{diag}(\\pmb{a}_{\\sigma})-\\pmb{a}_{\\sigma}\\pmb{a}_{\\sigma}^{\\top}\\big)^{2}\\approx\\frac{1}{T^{2}}\\left(\\pmb{I}-\\frac{\\mathbf{1}\\mathbf{1}^{\\top}}{T}\\right)}\\end{array}$ In other words, our linear model and softmax transformers are approximately equivalent up to a change in learning rates. ", "page_idx": 9}, {"type": "text", "text": "Now, suppose that there is a nontrivial correlation between $\\pmb{a}_{\\sigma}$ and $\\pmb q=e_{1}$ , say, $a_{\\sigma,1}$ is a small constant while all other entries are ${\\cal O}(1/T)$ . In this case, we have $\\bigl(\\mathrm{diag}\\bigl(a_{\\sigma}\\bigr)-a_{\\sigma}a_{\\sigma}^{\\top}\\bigr)^{2}\\approx a_{\\sigma,1}\\bigl(1\\!-\\!a_{\\sigma,1}\\bigr)e_{1}e_{1}^{\\top}+$ ${\\cal O}(1/T)$ . Effectively, softmax transformers automatically adjust the learning rate according to $a_{\\sigma,t}$ and roughly ignore those positions with a small attention weight to stabilize the gradients. Note that this is also what $\\ell_{1}$ -regularization does in our algorithm. In fact, mimicking this behavior is one of the motivations of using $\\ell_{1}$ -regularization in our linear setting. We run further experiments to highlight the resemblance between softmax attention and our linear attention model (Figure 2). ", "page_idx": 9}, {"type": "text", "text": "6   Conclusion and discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we propose the Sparse Contextual Bigram (SCB) model, which is a natural extension of the bigram model, that requires both contextual and global information. Then, we analyze the problem of learning a SCB model using a one-layer linear transformer and a gradient-based algorithm. We prove quantitative bounds on the convergence rate and the sample complexity. In particular, we show when trained from scratch, the training process can be split into two stages, where the first stage uses a lot of samples to boost the signal from zero to a nontrivial value, while the second stage is much more sample-efficient. Then, we consider the problem in a transfer learning setting and prove that when there is a nontrivial correlation between the pretraining and downstream tasks, the first sample intensive stage can be bypassed. ", "page_idx": 9}, {"type": "text", "text": "Our data-generating model and results also lead to some interesting future directions. For example, can we improve the sample complexity of the first stage? What can we gain if the datapoints are sequences generated by repeatedly applying the SCB model? ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "JDL acknowledges support of the NSF CCF 2002272, NSF IIS 2107304, and NSF CAREER Award 2144994. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "OpenA1. Gpt-4 technical report, 2023. ", "page_idx": 10}, {"type": "text", "text": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. ", "page_idx": 10}, {"type": "text", "text": "John M. Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Zidek, Anna Potapenko, Alex Bridgland, Clemens Meyer, Simon A A Kohl, Andy Ballard, Andrew Cowie, Bernardino Romera-Paredes, Stanislav Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen, David Reiman, Ellen Clancy, Michal Zielinski, Martin Steinegger, Michalina Pacholska, Tamas Berghammer, Sebastian Bodenstein, David Silver, Oriol Vinyals, Andrew W. Senior, Koray Kavukcuoglu, Pushmeet Kohli, and Demis Hassabis. Highly accurate protein structure prediction with alphafold. Nature, 596:583 - 589, 2021. URL https://api.semanticscholar.org/CorpusID:235959867. ", "page_idx": 10}, {"type": "text", "text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Ilia Polosukhin. Attention is all you need. Advances in neural information processing systems,30,2017. ", "page_idx": 10}, {"type": "text", "text": "Benjamin L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang. Inductive biases and variable creation in self-attention mechanisms. In International Conference on Machine Learning, pages 5793-5831.PMLR,2022. ", "page_idx": 10}, {"type": "text", "text": "Bingbin Liu, Jordan T Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Transformers learn shortcuts to automata. arXiv preprint arXiv:2210.10749, 2022. ", "page_idx": 10}, {"type": "text", "text": "Neel Nanda, Lawrence Chan, Tom Liberum, Jess Smith, and Jacob Steinhardt. Progress measures for grokking via mechanistic interpretability. arXiv preprint arXiv:2301.05217, 2023. ", "page_idx": 10}, {"type": "text", "text": "Shunyu Yao, Binghui Peng, Christos Papadimitriou, and Karthik Narasimhan. Self-attention networks can process bounded hierarchical languages. arXiv preprint arXiv:2105.11115, 2021. ", "page_idx": 10}, {"type": "text", "text": "Fabio Petroni, Tim Rocktaschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel. Language models as knowledge bases? arXiv preprint arXiv: 1909.01066, 2019. ", "page_idx": 10}, {"type": "text", "text": "Chiyuan Zhang, Daphne Ippolito, Katherine Lee, Matthew Jagielski, Florian Tramer, and Nicholas Carlini. Counterfactual memorization in neural language models. Advances in Neural Information Processing Systems,36:39321-39362,2023a. ", "page_idx": 10}, {"type": "text", "text": "Adi Haviv, Ido Cohen, Jacob Gidron, Roei Schuster, Yoav Goldberg, and Mor Geva. Understanding transformer memorization recall through idioms. arXiv preprint arXiv:2210.03588, 2022. ", "page_idx": 10}, {"type": "text", "text": "Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea, and Colin Raffel. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21), pages 2633-2650. USENIX Association, August 2021. ISBN 978-1- 939133-24-3. URL https://www.usenix.org/conference/usenixsecurity21/ presentation/carlini-extracting. ", "page_idx": 10}, {"type": "text", "text": "Kwangjun Ahn, Xiang Cheng, Minhak Song, Chulhee Yun, Ali Jadbabaie, and Suvrit Sra. Linear attention is (maybe) all you need (to understand transformer optimization). arXiv preprint arXiv:2310.01082,2023. ", "page_idx": 10}, {"type": "text", "text": "Samy Jelassi, Michael Sander, and Yuanzhi Li. Vision transformers provably learn spatial structure. Advances in Neural Information Processing Systems, 35:37822-37836, 2022. ", "page_idx": 10}, {"type": "text", "text": "Hongkang Li, Meng Wang, Sijia Liu, and Pin-Yu Chen. A theoretical understanding of shallow vision transformers: Learning, generalization, and sample complexity. arXiv preprint arXiv:2302.06015, 2023. ", "page_idx": 10}, {"type": "text", "text": "Davoud Ataee Tarzanagh, Yingcong Li, Christos Thrampoulidis, and Samet Oymak. Transformers as support vector machines. arXiv preprint arXiv:2308.16898, 2023.   \nYuandong Tian, Yiping Wang, Beidi Chen, and Simon Du. Scan and snap: Understanding training dynamics and token composition in 1-layer transformer. arXiv preprint arXiv:2305.16380, 2023a.   \nYuandong Tian, Yiping Wang, Zhenyu Zhang, Beidi Chen, and Simon Du. Joma: Demystifying multilayer transformers via joint dynamics of mlp and attention. arXiv preprint arXiv:2310.00535, 2023b.   \nArvind Mahankali, Tatsunori B Hashimoto, and Tengyu Ma. One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention. arXiv preprint arXiv:2307.03576, 2023.   \nRuiqi Zhang, Spencer Frei, and Peter L Bartlett. Trained transformers learn linear models in-context. arXiv preprint arXiv:2306.09927, 2023b.   \nYu Huang, Yuan Cheng, and Yingbin Liang. In-context convergence of transformers. arXiv preprint arXiv:2310.05249, 2023.   \nSiyu Chen, Heejune Sheen, Tianhao Wang, and Zhuoran Yang. Training dynamics of multi-head softmax attention for in-context learning: Emergence, convergence, and optimality. arXiv preprint arXiv:2402.19442, 2024.   \nAlberto Bietti, Vivien Cabannes, Diane Bouchacourt, Herve Jegou, and Leon Bottou. Birth of a transformer: A memory viewpoint. Advances in Neural Information Processing Systems, 36, 2024.   \nEshaan Nichani, Alex Damian, and Jason D Lee. How transformers learn causal structure with gradient descent. arXiv preprint arXiv:2402.14735, 2024.   \nBenjamin L Edelman, Ezra Edelman, Surbhi Goel, Eran Malach, and Nikolaos Tsilivis. The evolution of statistical induction heads: In-context learning markov chains. arXiv preprint arXiv:2402.11004, 2024.   \nAshok Vardhan Makkuva, Marco Bondaschi, Adway Girish, Alliot Nagle, Martin Jaggi, Hyeji Kim, and Michael Gastpar. Attention with markov: A framework for principled analysis of transformers via markov chains. arXiv preprint arXiv:2402.04161, 2024.   \nJuno Kim and Taiji Suzuki. Transformers learn nonlinear features in context: Nonconvex mean-field dynamics on the attention landscape. arXiv preprint arXiv:2402.01258, 2024.   \nZixuan Wang, Stanley Wei, Daniel Hsu, and Jason D Lee. Transformers provably learn sparse token selection while fully-connected nets cannot. arXiv preprint arXiv:2406.06893, 2024.   \nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of dep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.   \nNilesh Tripuraneni MichaelJordan, and Chi Jin. On the thry of transfelarning: The mportance of task diversity. Advances in neural information processing systems, 33:7852-7862, 2020.   \nSimon S Du, Wei Hu, Sham M Kakade, Jason D Lee, and Qi Lei. Few-shot learning via learning the representation, provably. arXiv preprint arXiv:2002.09434, 2020.   \nSanjeev Arora, Hrishikesh Khandeparkar, Mikhail Khodak, Orestis Plevrakis, and Nikunj Saunshi. A theoretical analysis of contrastive unsupervised representation learning. arXiv preprint arXiv: 1902.09229, 2019.   \nSteve Hanneke, Samory Kpotufe, and Yasaman Mahdaviyeh. Limits of model selection under transfer learning. In The Thirty Sixth Annual Conference on Learning Theory, pages 5781-5812. PMLR, 2023.   \nSai Li, T Tony Cai, and Hongzhe Li. Transfer learning for high-dimensional linear regression: Prediction, estimation and minimax optimality. Journal of the Royal Statistical Society Series B: Statistical Methodology, 84(1):149-173, 2022.   \nYe Tian and Yang Feng. Transfer learning under high-dimensional generalized linear models. Journal of the American Statistical Association, 118(544):2684-2697, 2023.   \nZhe Fei and Yi Li. Estimation and inference for high dimensional generalized linear models: A splitting and smoothing approach. Journal of Machine Learning Research, 22(58):1-32, 2021.   \nXuhui Zhang, Jose Blanchet, Soumyadip Ghosh, and Mark S Squillante. A class of geometric structures in transfer learning: Minimax bounds and optimality. In International Conference on Artificial Intelligence and Statistics, pages 3794-3820. PMLR, 2022.   \nPeizhong Ju, Sen Lin, Mark S Squillante, Yingbin Liang, and Ness B Shroff. Generalization performance of transfer learning: Overparameterized and underparameterized regimes. arXiv preprint arXiv:2306.04901, 2023.   \nYehuda Dar and Richard G Baraniuk. Double double descent: On generalization errors in transfer learning between linear regression tasks. SIAM Journal on Mathematics of Data Science, 4(4): 1447-1472,2022.   \nAndrew K Lampinen and Surya Ganguli. An analytic theory of generalization dynamics and transfer learning in deep linear networks. arXiv preprint arXiv: 1809.10374, 2018.   \nOussama Dhifallah and Yue M Lu. Phase transitions in transfer learning for high-dimensional perceptrons. Entropy, 23(4):400, 2021.   \nAlexandru Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn representations with gradient descent. In Conference on Learning Theory, pages 5413-5452. PMLR, 2022.   \nEkin Akyurek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. arXiv preprint arXiv:2211.15661, 2022. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Limitation ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we briefly discuss the limitation of this work. ", "page_idx": 13}, {"type": "text", "text": "First, we consider one-layer single-head linear transformers with certain (blocks of the) weights merged or fixed. Though this simplification are widely used in theoretical works and linear and nonlinear transformers share some training behaviors (Ahn et al., 2023), this architecture is still very far away from the transformers used in practice. ", "page_idx": 13}, {"type": "text", "text": "We also use a non-standard training algorithm that has several manually separated stages. Some parts of the modification are made to address certain issues of linear transformers, while the other are made to simplify the analysis. It would be interesting (and more challenging) to consider more natural/practical training algorithms. ", "page_idx": 13}, {"type": "text", "text": "Finally, for our data-generating model, we only use it to generate one next token, instead of repeatedly apply SCB on the previous generated results to obtain a long sequence. In our setting, the contextual tokens are independent. While this simplifies the analysis, it deviates from how natural language works. ", "page_idx": 13}, {"type": "text", "text": "B  Probabilities, expectations, and variances ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We collect in this section closed-form formulas for the probabilities of certain events, and the expectations and variances of some random vectors of interest. All proofs are deferred to the end of this section. ", "page_idx": 13}, {"type": "text", "text": "B.1 Probabilities ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Lemma B.1. For any $t\\in[T]$ and $k,n,m\\in[N]$ we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}(x_{o}=n,x_{t}=m\\mid x_{T+1}=k)=q_{t}^{(k)}P_{n,m}\\mu_{m}+\\left(1-q_{t}^{(k)}\\right)\\mu_{n}\\mu_{m}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Lemma B.2. For any $s\\neq t\\in[T],\\,k,n,m,l\\in[N]$ we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{P}(x_{o}=n\\ |\\ x_{T+1}=k,x_{s}=m,x_{t}=l)=q_{s}^{(k)}P_{n,m}+q_{t}^{(k)}P_{n,l}+\\left(1-q_{s}^{(k)}-q_{t}^{(k)}\\right)\\mu_{n}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "B.2  Gradient and Expectations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Lemma B.3. Suppose the last input token $x_{T+1}$ and $\\pmb{a}:=A\\pmb{x}_{T+1}$ . The gradients of the objective are ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\nabla_{V}l=\\left(V X\\pmb{a}-\\pmb{e}_{x_{o}}\\right)(X\\pmb{a})^{\\top},}\\\\ &{\\nabla_{\\pmb{a}^{(k)}}l=\\mathbb{1}\\{x_{T+1}=k\\}(V X)^{\\top}\\left(V X\\pmb{a}-\\pmb{e}_{x_{o}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Lemma B.4. For any $k\\in[N]$ and $s,t\\in[T]$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}^{(k)}[e_{x_{s}}e_{x_{t}}^{\\top}]=\\mathbb{E}[e_{x_{s}}e_{x_{t}}^{\\top}]=\\left\\{\\!\\!\\operatorname{diag}(\\mu)\\!,\\!\\quad s=t,\\!\\quad\\!\\right.}\\\\ {\\left.\\quad\\mathbb{E}^{(k)}[e_{x_{s}}e_{x_{t}}^{\\top}]=\\mathbb{E}[e_{x_{s}}e_{x_{t}}^{\\top}]=\\!\\!\\operatorname{diag}(\\mu)^{\\top}\\!,\\!\\quad\\quad\\!\\!s\\neq t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Lemma B.5. For any $V\\in\\mathbb{R}^{N\\times N}$ with $V\\pmb{\\mu}=\\pmb{\\mu}$ and $s,t\\in[T]$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\sum_{n=1}^{N}\\mathbb{E}[V_{n,x_{s}}V_{n,x_{t}}]=\\left\\{\\|V\\|_{\\mu}^{2}\\,,\\quad s=t,\\right.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Lemma B.6. For any $V\\in\\mathbb{R}^{N\\times N}$ with $V\\pmb{\\mu}=\\pmb{\\mu}$ and $t\\in[T]$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}^{(k)}V_{x_{o},x_{t}}=q_{t}^{(k)}\\,\\langle V,P\\rangle_{\\mu}+\\left(1-q_{t}^{(k)}\\right)\\|\\pmb{\\mu}\\|^{2}\\,.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Lemma B.7 (Expected gradients). ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}\\,\\nabla_{V}l=\\|A\\|_{\\mu}^{2}\\,V\\mathrm{diag}(\\mu)+\\left(1-\\|A\\|_{\\mu}^{2}\\right)\\mu\\mu^{\\top}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\qquad-\\left\\langle Q,A\\right\\rangle_{\\mu}P\\mathrm{diag}(\\mu)-\\left(1-\\left\\langle Q,A\\right\\rangle_{\\mu}\\right)\\mu\\mu^{\\top},}\\\\ &{\\qquad\\qquad\\qquad\\quad\\quad\\mathbb{Z}\\nabla_{a^{(k)}}l=\\mu_{k}\\left(\\left\\|V\\right\\|_{\\mu}^{2}-\\left\\|\\mu\\right\\|^{2}\\right)a^{(k)}+\\mu_{k}\\mathbf{1}\\left\\|\\mu\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad-\\mu_{k}q^{(k)}\\left\\langle V,P\\right\\rangle_{\\mu}-\\mu_{k}\\left(\\mathbf{1}-q^{(k)}\\right)\\left\\|\\mu\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Lemma B.8 (Expected preconditioned gradients). ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\hat{\\nabla}_{V}l=\\left\\|A\\right\\|_{\\mu}^{2}\\left(V-\\mu\\mathbf{1}^{\\top}\\right)-\\left\\langle Q,A\\right\\rangle_{\\mu}\\left(P-\\mu\\mathbf{1}^{\\top}\\right),}\\\\ &{\\mathbb{E}\\hat{\\nabla}_{a^{(k)}}l=\\left(\\left\\|V\\right\\|_{\\mu}^{2}-\\left\\|\\mu\\right\\|^{2}\\right)\\left(a^{(k)}-\\displaystyle\\frac{1}{T}\\right)-\\left(\\left\\langle V,P\\right\\rangle_{\\mu}-\\left\\|\\mu\\right\\|^{2}\\right)\\left(q^{(k)}-\\displaystyle\\frac{1}{T}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "B.3  Deferred proofs of this section ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.3.1 Probabilities ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof of Lemma B.1. For notational simplicity, define $\\pmb{x}_{-t}=(x_{1},\\dots,x_{t-1},x_{t+1},\\dots,x_{T})$ .Wecompute ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~~~\\mathbb{P}(x_{o}=n,x_{t}=m\\ |\\ x_{T+1}=k)}\\\\ &{=\\displaystyle\\sum_{\\hat{m}\\in[N]^{T-1}}\\mathbb{P}(x_{o}=n,x_{t}=m,\\pmb{x}_{-t}=\\hat{m}\\ |\\ x_{T+1}=k)}\\\\ &{=\\displaystyle\\sum_{\\hat{m}\\in[N]^{T-1}}\\mathbb{P}(x_{o}=n\\ |\\ x_{t}=m,\\pmb{x}_{-t}=\\hat{m},x_{T+1}=k)}\\\\ &{~~~~~~~~~~~~~~~\\times\\mathbb{P}(x_{t}=m,\\pmb{x}_{-t}=\\hat{m}\\ |\\ x_{T+1}=k).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By the independence assumption, we have $\\mathbb{P}(x_{t}=m,\\pmb{x}_{-t}=\\pmb{\\hat{m}}\\mid x_{T+1}=k)=\\mathbb{P}(x_{t}=m,\\pmb{x}_{-t}=\\pmb{\\hat{m}})=$ $\\mathbb{P}\\dot{(}x_{t}\\,=\\,m\\big)\\,\\bar{\\mathbb{P}}(\\pmb{x}_{-t}\\,=\\,\\hat{\\pmb{m}})$ . For the first factor, we have $\\mathbb{P}(x_{o}=n\\ |\\ x_{t}\\,=\\,m,\\pmb{x}_{-t}\\,=\\,\\pmb{\\hat{m}},x_{T+1}\\,=\\,k)\\,=$ $\\begin{array}{r}{Q_{t}^{(k)}P_{n,m}+\\sum_{s\\neq t}Q_{s}^{(k)}P_{n,\\hat{m}_{s}}}\\end{array}$ .Therefore, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{P}(x_{o}=n,x_{t}=m\\ |\\ x_{T+1}=k)}\\\\ &{=\\displaystyle\\sum_{\\hat{m}\\in[T]^{N-1}}\\left(Q_{t}^{(k)}P_{n,m}+\\sum_{s\\neq t}Q_{s}^{(k)}P_{n,\\hat{m}_{s}}\\right)\\mathbb{P}(x_{t}=m)\\,\\mathbb{P}(x_{-t}=\\hat{m})}\\\\ &{=\\displaystyle\\left(Q_{t}^{(k)}P_{n,m}+\\sum_{s\\neq t}Q_{s}^{(k)}\\sum_{\\hat{m}\\in[T]^{N-1}}P_{n,\\hat{m}_{s}}\\,\\mathbb{P}(x_{-t}=\\hat{m})\\right)\\mu_{m}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note that for any $s\\neq t$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sum_{\\hat{m}\\in[T]^{N-1}}P_{n,\\hat{m}_{s}}\\operatorname{\\mathbb{P}}(x_{-t}=\\hat{m})=\\sum_{\\hat{m}_{-s}\\in[T]^{N-2}}\\left(\\sum_{\\hat{m}_{s}=1}^{N}P_{n,\\hat{m}_{s}}\\mu_{\\hat{m}_{s}}\\right)\\operatorname{\\mathbb{P}}(x_{-s,-t}=\\hat{m}_{-s})=\\mu_{n}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(x_{o}=n,x_{t}=m\\mid x_{T+1}=k)=\\left(q_{t}^{(k)}P_{n,m}+\\displaystyle\\sum_{s\\neq t}q_{s}^{(k)}\\mu_{n}\\right)\\mu_{m}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=q_{t}^{(k)}P_{n,m}\\mu_{m}+\\left(1-q_{t}^{(k)}\\right)\\mu_{n}\\mu_{m}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof of Lemma B.2. For notational simplicity, let $x_{-s,-t}$ denote the vector obtained by removing the $s,t$ coordinates from $\\boldsymbol{x}$ . Then, we compute ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{P}(x_{o}=n\\mid x_{T+1}=k,x_{s}=m,x_{t}=l)}\\\\ &{=\\displaystyle\\sum_{\\hat{m}\\in[N]^{T-2}}\\mathbb{P}(x_{o}=n,\\pmb{x}_{-s,-t}=\\hat{m}\\mid x_{T+1}=k,x_{s}=m,x_{t}=l)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{=\\sum_{\\hat{m}\\in[N]^{T-2}}\\mathbb{P}(x_{o}=n\\mid x_{T+1}=k,x_{s}=m,x_{t}=l,\\pmb{x}_{-s,-t}=\\hat{m})\\,\\mathbb{P}(\\pmb{x}_{-s,-t}=\\hat{m})}}\\\\ &{=\\displaystyle\\sum_{\\hat{m}\\in[N]^{T-2}}\\left(q_{s}^{(k)}P_{n,m}+q_{t}^{(k)}P_{n,l}+\\sum_{i\\notin\\{s,t\\}}q_{i}^{(k)}P_{n,\\hat{m}_{i}}\\right)\\mathbb{P}(\\pmb{x}_{-s,-t}=\\hat{m})}\\\\ &{=q_{s}^{(k)}P_{n,m}+q_{t}^{(k)}P_{n,l}+\\left(1-q_{s}^{(k)}-q_{t}^{(k)}\\right)\\mu_{n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "B.3.2 Expectations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof of Lemma B.3. For each sample $X$ ,wehave ", "page_idx": 15}, {"type": "equation", "text": "$$\nl(\\pmb{x},x_{T+1},x_{o};\\pmb{V},\\pmb{A}):=\\frac{1}{2}\\left\\|\\pmb{e}_{x_{o}}-V\\pmb{X}\\pmb{A}\\pmb{e}_{x_{T+1}}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and $\\pmb{a}$ Then we have the matrix differential: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{d}l=\\left(V X a-e_{x_{o}}\\right)^{\\top}\\!\\mathrm{d}V X a+\\left(V X a-e_{x_{o}}\\right)^{\\top}\\!V X\\mathrm{d}a\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\nabla_{V}l=\\left(V X\\pmb{a}-\\pmb{e}_{x_{o}}\\right)(X\\pmb{a})^{\\top},}\\\\ &{\\nabla_{\\pmb{a}^{(k)}}l=\\mathbb{1}\\{x_{T+1}=k\\}(V X)^{\\top}\\left(V X\\pmb{a}-\\pmb{e}_{x_{o}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof of Lemma B.4. When $s\\neq t$ , we have $\\mathbb{E}^{(k)}[e_{x_{s}}e_{x_{t}}^{\\top}]=\\mathbb{E}^{(k)}[e_{x_{s}}]\\mathbb{E}^{(k)}[e_{x_{t}}^{\\top}]=\\mu\\pmb{\\mu}^{\\top}$ When $s=t$we have $\\begin{array}{r}{\\mathbb{E}^{(k)}[e_{x_{t}}e_{x_{t}}^{\\top}]=\\sum_{k=1}^{N}\\mu_{k}e_{k}e_{k}^{\\top}=\\mathrm{diag}(\\mu)}\\end{array}$ \u53e3", "page_idx": 15}, {"type": "text", "text": "Proof of Lemma B.5. When $s\\neq t$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{n=1}^{N}\\mathbb{E}[V_{n,x_{s}}V_{n,x_{t}}]=\\sum_{n=1}^{N}\\left(\\mathbb{E}\\,V_{n,x_{s}}\\right)^{2}=\\sum_{n=1}^{N}\\left(\\sum_{k=1}^{N}\\mu_{k}V_{n,k}\\right)^{2}}}\\\\ &{}&{\\qquad=\\displaystyle\\sum_{k,l=1}^{N}\\mu_{k}\\mu_{l}\\sum_{n=1}^{N}V_{n,k}V_{n,l}=\\mu^{\\top}V^{\\top}V\\mu=\\|\\mu\\|^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "When $s=t$ we have $\\begin{array}{r}{\\sum_{n=1}^{N}\\mathbb{E}\\,V_{n,x_{t}}^{2}=\\sum_{n=1}^{N}\\sum_{k=1}^{N}\\mu_{k}V_{n,k}^{2}=\\|V\\|_{\\mu}^{2}\\,.}\\end{array}$ ", "page_idx": 15}, {"type": "text", "text": "Proof of Lemma B.6. Recall Lemma B.1. Then, we compute ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}^{(k)}V_{x_{o},x_{t}}=\\sum_{n,m=1}^{N}V_{n,m}\\mathbb{P}(x_{o}=n,x_{t}=m\\ |\\ x_{T+1}=k)}\\\\ {\\displaystyle=q_{t}^{(k)}\\sum_{n,m=1}^{N}V_{n,m}P_{n,m}\\mu_{m}+\\left(1-q_{t}^{(k)}\\right)\\sum_{n,m=1}^{N}V_{n,m}\\mu_{n}\\mu_{m}}\\\\ {\\displaystyle=q_{t}^{(k)}\\left\\langle V,P\\right\\rangle_{\\mu}+\\left(1-q_{t}^{(k)}\\right)\\mu^{\\top}V\\mu.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof of Lemma B.7. First, we consider $\\nabla_{V}l\\,=\\,\\left(V X a-e_{x_{o}}\\right)(X a)^{\\top}$ , and compute $\\mathbb{E}(X\\pmb{a})(X\\pmb{a})^{\\top}$ and $\\mathbb{E}\\,\\pmb{e}_{x_{o}}(\\pmb{X}\\pmb{a})^{\\top}$ Write $\\begin{array}{r}{X\\pmb{a}=\\sum_{t=1}^{T}a_{t}\\pmb{e}_{x_{t}}}\\end{array}$ . Then, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}^{(k)}\\left[(X\\pmb{a})(X\\pmb{a})^{\\top}\\right]=\\sum_{s,t=1}^{T}a_{s}^{(k)}a_{t}^{(k)}\\,\\mathbb{E}[e_{x_{s}}\\pmb{e}_{x_{t}}^{\\top}]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\displaystyle\\sum_{t=1}^{T}(a_{t}^{(k)})^{2}\\,\\mathbb{E}\\big[e_{x_{t}}e_{x_{t}}^{\\top}\\big]+\\displaystyle\\sum_{s\\neq t}a_{s}^{(k)}a_{t}^{(k)}\\,\\mathbb{E}\\big[e_{x_{s}}e_{x_{t}}^{\\top}\\big]}\\\\ &{=\\displaystyle\\left\\|{\\pmb{a}}^{(k)}\\right\\|^{2}\\mathrm{diag}(\\pmb{\\mu})+\\left(1-\\left\\|{\\pmb{a}}^{(k)}\\right\\|^{2}\\right)\\mu\\pmb{\\mu}^{\\top},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the last line comes from Lemma B.4. Then, we compute ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}^{(k)}[e_{x_{o}}(X a)^{\\top}]=\\sum_{t=1}^{T}a_{t}^{(k)}\\mathbb{E}^{(k)}[e_{x_{o}}e_{x_{t}}^{\\top}]=\\displaystyle\\sum_{t=1}^{T}a_{t}^{(k)}\\left[\\mathbb{P}(x_{o}=n,x_{t}=m\\mid x_{T+1}=k)\\right]_{n,m\\in[N]}}}\\\\ &{}&{=\\displaystyle\\sum_{t=1}^{T}a_{t}^{(k)}\\left(q_{t}^{(k)}P{\\mathrm{diag}}(\\mu)+(1-q_{t}^{(k)})\\mu\\mu^{\\top}\\right)}\\\\ &{}&{=\\displaystyle\\left\\langle q^{(k)},a^{(k)}\\right\\rangle P{\\mathrm{diag}}(\\mu)+\\left(1-\\left\\langle q^{(k)},a^{(k)}\\right\\rangle\\right)\\mu\\mu^{\\top},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the second line comes from Lemma B.1. Thus, for $\\nabla_{V}l$ ,wehave ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{B}\\nabla_{V}l=V\\displaystyle\\sum_{k=1}^{N}\\mu_{k}\\mathbb{B}^{(k)}\\left[(X a)(X a)^{\\top}\\right]-\\displaystyle\\sum_{k=1}^{N}\\mu_{k}\\mathbb{B}^{(k)}\\left[e_{x_{o}}(X a)^{\\top}\\right]}\\\\ &{\\qquad=\\|A\\|_{\\mu}^{2}\\,V\\mathrm{diag}(\\mu)+\\displaystyle\\left(1-\\|A\\|_{\\mu}^{2}\\right)V\\mu\\mu^{\\top}-\\langle Q,A\\rangle_{\\mu}\\,P\\mathrm{diag}(\\mu)-\\left(1-\\langle Q,A\\rangle_{\\mu}\\right)\\mu\\mu^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now,consider $\\nabla_{\\pmb{a}^{(k)}}l~=~\\mathbb{1}\\{x_{T+1}~=~k\\}(V X)^{\\top}\\left(V X\\pmb{a}-\\pmb{e}_{x_{o}}\\right)$ andcompute $\\mathbb{E}^{(k)}(V X)^{\\top}(V X)$ and $\\mathbb{E}^{(k)}(V X)^{\\top}\\pmb{e}_{x_{i}}$ .By LemmaB.5,for each $s,t\\in[T]$ ,wehave ", "page_idx": 16}, {"type": "equation", "text": "$$\ne_{s}^{\\mathsf{T}}\\mathbb{E}^{(k)}[(V X)^{\\top}(V X)]e_{t}=\\mathbb{E}^{(k)}[(V e_{x_{s}})^{\\top}V e_{x_{t}}]=\\sum_{n=1}^{N}\\mathbb{E}\\,V_{n,x_{s}}V_{n,x_{t}}=\\left\\{\\|V\\|_{\\mu}^{2}\\,,\\quad s=t,\\right.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In matrix form, this is ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}^{(k)}(V X)^{\\top}V X=\\left(\\left\\|V\\right\\|_{\\mu}^{2}-\\left\\|\\mu\\right\\|^{2}\\right)I+\\mathbf{1}\\mathbf{1}^{\\top}\\left\\|\\mu\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, by Lemma B.6, for each $t\\in[T]$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{e_{t}^{\\top}\\mathbb{E}[(V X)^{\\top}e_{x_{o}}]=\\mathbb{E}^{(k)}[(V e_{x_{t}})^{\\top}e_{x_{o}}]=\\mathbb{E}^{(k)}V_{x_{o},x_{t}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=q_{t}^{(k)}\\left\\langle V,P\\right\\rangle_{\\mu}+\\left(1-q_{t}^{(k)}\\right)\\left\\|\\mu\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In matrix for, this is ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[(V X)^{\\top}e_{x_{o}}]=\\pmb{q}^{(k)}\\,\\langle V,P\\rangle_{\\mu}+\\left(\\mathbf{1}-\\pmb{q}^{(k)}\\right)\\left\\|\\pmb{\\mu}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Combine these together, and we obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{k}^{-1}\\operatorname{\\mathbb{E}}\\nabla_{a^{(k)}}l=\\operatorname{\\mathbb{E}}^{(k)}\\left[(V X)^{\\top}V X\\right]a^{(k)}-\\operatorname{\\mathbb{E}}^{(k)}\\left[(V X)^{\\top}e_{x_{o}}\\right]}\\\\ &{\\qquad\\qquad\\qquad=\\left(\\|V\\|_{\\mu}^{2}-\\|\\mu\\|^{2}\\right)a^{(k)}+1\\|\\mu\\|^{2}-q^{(k)}\\left\\langle V,P\\right\\rangle_{\\mu}-\\left(\\mathbf{1}-q^{(k)}\\right)\\|\\mu\\|^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof of Lemma B.8. Recall from Lemma B.7 that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\nabla_{V}l=\\left\\|A\\right\\|_{\\mu}^{2}V\\mathrm{diag}(\\mu)+\\left(1-\\left\\|A\\right\\|_{\\mu}^{2}\\right)V\\mu\\mu^{\\top}}\\\\ &{\\qquad\\qquad\\qquad-\\left\\langle Q,A\\right\\rangle_{\\mu}P\\mathrm{diag}(\\mu)-\\left(1-\\left\\langle Q,A\\right\\rangle_{\\mu}\\right)\\mu\\mu^{\\top},}\\\\ &{\\mathbb{E}\\nabla_{a^{(k)}}l=\\mu_{k}\\left(\\left\\|V\\right\\|_{\\mu}^{2}-\\left\\|\\mu\\right\\|^{2}\\right)a^{(k)}+\\mu_{k}\\mathbf{1}\\left\\|\\mu\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad-\\mu_{k}q^{(k)}\\left\\langle V,P\\right\\rangle_{\\mu}-\\mu_{k}\\left(\\mathbf{1}-q^{(k)}\\right)\\left\\|\\mu\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For $\\nabla_{V}$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{V\\mathrm{diag}(\\mu)\\mathrm{diag}(1/\\mu)\\left(I-\\frac{\\mu\\mu^{\\top}}{\\|\\mu\\|^{2}}\\right)=V-\\frac{\\mu\\mu^{\\top}}{\\|\\mu\\|^{2}},}\\\\ {P\\mathrm{diag}(\\mu)\\mathrm{diag}(1/\\mu)\\left(I-\\frac{\\mu\\mu^{\\top}}{\\|\\mu\\|^{2}}\\right)=P-\\frac{\\mu\\mu^{\\top}}{\\|\\mu\\|^{2}},}\\\\ {\\mu\\mu^{\\top}\\mathrm{diag}(1/\\mu)\\left(I-\\frac{\\mu\\mu^{\\top}}{\\|\\mu\\|^{2}}\\right)=\\mu\\mathbf{1}^{\\top}-\\frac{\\mu\\mu^{\\top}}{\\|\\mu\\|^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In particular, note that the $\\mu\\mu^{\\top}/\\|\\mu\\|^{2}$ terms will cancel with each other. Thus, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\hat{\\nabla}_{V}l=\\|A\\|_{\\mu}^{2}\\,V+\\left(1-\\|A\\|_{\\mu}^{2}\\right)\\mu\\mathbf{1}^{\\top}-\\left\\langle Q,A\\right\\rangle_{\\mu}P-\\left(1-\\left\\langle Q,A\\right\\rangle_{\\mu}\\right)\\mu\\mathbf{1}^{\\top}}\\\\ &{\\qquad\\quad=\\|A\\|_{\\mu}^{2}\\left(V-\\mu\\mathbf{1}^{\\top}\\right)-\\left\\langle Q,A\\right\\rangle_{\\mu}\\left(P-\\mu\\mathbf{1}^{\\top}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "For $\\hat{\\nabla}_{\\pmb{a}^{(k)}}l$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\,\\hat{\\nabla}_{a^{(k)}}l=\\left(\\|V\\|_{\\mu}^{2}-\\|\\mu\\|^{2}\\right)\\left(I-\\frac{\\mathbf{1}\\mathbf{1}^{\\top}}{T}\\right)a^{(k)}-\\left(I-\\frac{\\mathbf{1}\\mathbf{1}^{\\top}}{T}\\right)q^{(k)}\\left\\langle V,P\\right\\rangle_{\\mu}}\\\\ {\\displaystyle\\qquad\\quad+\\left(I-\\frac{\\mathbf{1}^{\\top}}{T}\\right)q^{(k)}\\left\\|\\mu\\right\\|^{2}}\\\\ {\\displaystyle=\\left(\\|V\\|_{\\mu}^{2}-\\|\\mu\\|^{2}\\right)\\left(a^{(k)}-\\frac{\\mathbf{1}}{T}\\right)-\\left(q^{(k)}-\\frac{\\mathbf{1}}{T}\\right)\\left\\langle V,P\\right\\rangle_{\\mu}+\\left(q^{(k)}-\\frac{\\mathbf{1}}{T}\\right)\\|\\mu\\|^{2}}\\\\ {\\displaystyle=\\left(\\|V\\|_{\\mu}^{2}-\\|\\mu\\|^{2}\\right)\\left(a^{(k)}-\\frac{\\mathbf{1}}{T}\\right)-\\left(\\left\\langle V,P\\right\\rangle_{\\mu}-\\|\\mu\\|^{2}\\right)\\left(q^{(k)}-\\frac{\\mathbf{1}}{T}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "B.4 Concentration ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we provide concentration inequalities for the gradients of the loss function. The concentration is applied on the gradient noise term ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{h}_{V,\\tau}:=\\left(\\hat{\\nabla}_{V}^{(B)}l-\\mathbb{E}\\hat{\\nabla}_{V}l\\right)}\\\\ &{\\pmb{h}_{A,\\tau}:=\\left(\\hat{\\nabla}_{A}^{(B)}l-\\mathbb{E}\\hat{\\nabla}_{A}l\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Where $\\hat{\\nabla}_{V}^{(B)}l$ and $\\hat{\\nabla}_{A_{\\cdot}}^{(B)}l$ are the preconditioned empirical gradients computed from a batch of size $B$ Here, we first consider the concentration of the original gradients: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\nabla_{V}l=\\left(V X\\pmb{a}-\\pmb{e}_{x_{o}}\\right)(X\\pmb{a})^{\\top},}\\\\ &{\\nabla_{\\pmb{a}^{(k)}}l=\\mathbb{1}\\{x_{T+1}=k\\}(V X)^{\\top}\\left(V X\\pmb{a}-\\pmb{e}_{x_{o}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and then consider the concentration of the preconditioned gradients. In this paper, we focus on $\\left\\|\\cdot\\right\\|_{\\mu}$ as the mostly used metric for the gradient matrices. ", "page_idx": 17}, {"type": "text", "text": "First we prove a naive concentration w.r.t. any random vector $\\pmb{y}$ with bounded second moment with any $\\lVert\\cdot\\rVert$ ", "page_idx": 17}, {"type": "text", "text": "LemmaB.9.Fix $\\delta,\\varepsilon\\,>\\,0$ Let ${\\boldsymbol y}$ be a $D$ -dimensionalrandomvectorwithE $\\|y\\|^{2}\\leq G$ .Define $\\begin{array}{r}{\\mathbf{y}^{(B)}:=B^{-1}\\sum_{i=1}^{B}\\mathbf{y}_{i}}\\end{array}$ where $({\\bf{y}}_{i})_{i}$ are i.d versions of $\\bf\\delta y$ If ", "page_idx": 17}, {"type": "equation", "text": "$$\nB\\geq\\frac{G}{\\delta\\varepsilon^{2}},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "then with probability at least $1-\\delta$ we have $\\|\\mathbf{y}-\\mathbb{E}\\,\\mathbf{y}\\|\\leq\\varepsilon$ ", "page_idx": 17}, {"type": "text", "text": "Proof of Lemma B.9. Assume w.1.o.g. that $\\mathbb{E}\\,\\mathbf{y}=0$ . First, note that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left\\|y^{(B)}\\right\\|^{2}=\\frac{1}{B^{2}}\\sum_{i,j=1}^{N}\\mathbb{E}\\left\\langle y_{i},y_{j}\\right\\rangle=\\frac{1}{B^{2}}\\sum_{i=1}^{N}\\mathbb{E}\\left\\|y_{i}\\right\\|^{2}\\leq\\frac{G}{B}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Hence, by the Markov inequality, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left\\|y^{(B)}\\right\\|\\geq\\varepsilon\\right)=\\mathbb{P}\\left(\\left\\|y^{(B)}\\right\\|^{2}\\geq\\varepsilon^{2}\\right)\\leq\\frac{\\mathbb{E}\\left\\|y^{(B)}\\right\\|^{2}}{\\varepsilon^{2}}\\leq\\frac{G}{B\\varepsilon^{2}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus, for fixed $\\varepsilon,\\delta\\in(0,1)$ , if we choose $B=G/(\\delta\\varepsilon^{2})$ , then we have with probability at least $1-\\delta$ $\\left\\|y^{(B)}\\right\\|\\leq\\varepsilon$ \u53e3 ", "page_idx": 18}, {"type": "text", "text": "Now we upper bound the infinity norm of the preconditioned gradients to apply concentration. ", "page_idx": 18}, {"type": "text", "text": "LemmaB.10.Suppose that $\\|\\mathrm{Vec}\\,V\\|_{\\infty}=O(1),V=\\tilde{V}+\\Delta_{V},A=\\tilde{A}+\\Delta_{A}$ where $\\tilde{V}\\in\\mathbb{R}^{N\\times N}$ is $a$ transition probability matrix, and in the attentionmatrix $\\tilde{A}$ each column $\\tilde{\\pmb{a}}^{(k)}$ is a probability vector. Moreover, $\\|\\mathbf{A}_{A}\\|_{F}^{2},\\|\\mathbf{\\dot{A}}_{V}\\|_{F}^{2}\\leq O(1/T)$ . Then, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\lVert\\hat{\\nabla}_{a^{(k)}}l\\right\\rVert_{\\infty}\\leq O(N),\\ \\ \\left\\lVert\\mathrm{Vec}\\hat{\\nabla}_{V}l\\right\\rVert_{\\infty}\\leq O(N)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. We first consider the infinity norm of the original gradient. Recall that the gradient for $V$ and $\\boldsymbol{A}$ are ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\pmb{a}^{(k)}}l=\\mathbb{1}\\{x_{T+1}=k\\}(V X)^{\\top}(V X\\pmb{a}^{(k)}-\\pmb{e}_{x_{o}})}\\\\ &{\\quad\\nabla_{\\pmb{V}}l=\\left(V X\\pmb{a}-\\pmb{e}_{x_{o}}\\right)(X\\pmb{a})^{\\top}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and the preconditioned gradient is: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\hat{\\nabla}_{V}^{(B_{\\tau})}l:=\\left({\\cal I}_{N}-\\frac{{\\bf1}_{N}{\\bf1}_{N}^{\\top}}{N}\\right)\\left(\\nabla_{V}^{(B_{\\tau})}l\\right)\\mathrm{diag}(1/\\mu)\\left({\\cal I}_{N}-\\frac{\\mu\\mu^{\\top}}{\\left\\|\\mu\\right\\|^{2}}\\right),}\\\\ {\\hat{\\nabla}_{a^{(k)}}^{(B_{\\tau})}l:=\\displaystyle\\frac{1}{\\mu_{k}}\\left({\\cal I}_{T}-\\frac{{\\bf1}_{T}{\\bf1}_{T}^{\\top}}{T}\\right)\\left(\\nabla_{a^{(k)}}^{(B_{\\tau})}l\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We first consider the maximum absolute value in the original gradients. For $\\nabla_{\\pmb{a}^{(k)}}l$ wehave ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~~\\left\\|\\mathbb{1}\\{x_{T+1}=k\\}(V X)^{\\top}(V X a^{(k)}-e_{x_{o}})\\right\\|_{\\infty}}\\\\ &{\\leq\\left\\|\\mathbb{1}\\{x_{T+1}=k\\}(V X)^{\\top}(V X a^{(k)})\\right\\|_{\\infty}+\\left\\|\\mathbb{1}\\{x_{T+1}=k\\}(V X)^{\\top}(e_{x_{o}})\\right\\|_{\\infty}}\\\\ &{\\leq\\underset{s\\in[T]}{\\operatorname*{max}}\\left|\\sum_{t=1}^{T}a_{t}(V e_{x_{s}})^{\\top}V e_{x_{t}}\\right|+\\underset{s\\in[T]}{\\operatorname*{max}}\\left|(V e_{x_{s}})^{\\top}e_{x_{o}}\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The first term can be upper-bounded in the following way: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\operatorname*{max}_{s\\in[T]}\\left\\vert\\sum_{t=1}^{T}a_{t}(V e_{x_{s}})^{\\top}V e_{x_{t}}\\right\\vert=\\displaystyle\\operatorname*{max}_{s\\in[T]}\\left\\vert\\sum_{t=1}^{T}\\tilde{a_{t}}(V e_{x_{s}})^{\\top}V e_{x_{t}}+\\sum_{t=1}^{T}\\Delta_{a,t}(V e_{x_{s}})^{\\top}V e_{x_{t}}\\right\\vert}\\\\ {\\displaystyle\\leq\\left(1+\\displaystyle\\sum_{t=1}^{T}\\|\\Delta_{a,t}\\|_{1}\\right)\\operatorname*{max}_{s,t}\\left\\vert(V e_{x_{s}})^{\\top}V e_{x_{t}}\\right\\vert}\\\\ {\\displaystyle\\leq\\left(1+\\sqrt{T}\\|\\Delta_{a,t}\\|_{2}\\right)\\operatorname*{max}_{s,t}\\left\\vert(V e_{x_{s}})^{\\top}V e_{x_{t}}\\right\\vert}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since $V=\\tilde{V}+\\Delta_{V}$ and $\\|\\mathbf{A}_{V}\\|_{F}^{2},\\|\\mathbf{A}_{A}\\|_{F}^{2}\\le O(1/T)$ , we have $\\operatorname*{max}_{s,t}\\left|(V e_{x_{s}})^{\\top}V e_{x_{t}}\\right|$ upper bounded by $O(1)$ . Therefore ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{s\\in[T]}\\left|\\sum_{t=1}^{T}a_{t}(V e_{x_{s}})^{\\top}V e_{x_{t}}\\right|\\leq O(1)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "And similarly, the second term $\\mathrm{max}_{s\\in[T]}\\left|(V e_{x_{s}})^{\\top}e_{x_{o}}\\right|$ can be bounded by $O(1)$ because the infinity norm of $V$ is also upper bounded by $O(1)$ . Therefore, we know $\\|\\nabla_{\\mathbf{\\Phi}_{\\mathbf{q}^{(k)}}}l\\|_{\\infty}\\le O(1)$ ", "page_idx": 18}, {"type": "text", "text": "Now we consider the preconditioned gradient $\\hat{\\nabla}_{\\pmb{a}^{(k)}}^{(B_{\\tau})}l$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|\\hat{\\nabla}_{\\pmb{a}^{(k)}}^{(B_{\\tau})}l\\|_{\\infty}=\\left\\|\\frac{1}{\\mu_{k}}\\left(I_{T}-\\frac{\\mathbf{1}_{T}\\mathbf{1}_{T}^{\\top}}{T}\\right)\\left(\\nabla_{\\pmb{a}^{(k)}}^{(B_{\\tau})}l\\right)\\right\\|_{\\infty}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\leq\\left\\|\\frac{1}{\\mu_{k}}I_{T}\\left(\\nabla_{a^{(k)}}^{(B_{\\tau})}l\\right)\\right\\|_{\\infty}+\\left\\|\\frac{1}{\\mu_{k}}\\left(\\frac{\\mathbf{1}_{T}\\mathbf{1}_{T}^{\\top}}{T}\\right)\\left(\\nabla_{a^{(k)}}^{(B_{\\tau})}l\\right)\\right\\|_{\\infty}\\leq O(N)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "since $\\begin{array}{r}{\\mu_{k}\\geq\\frac{c}{N}}\\end{array}$ for all $k\\in[N]$ ", "page_idx": 19}, {"type": "text", "text": "We use similar echnique on $\\hat{\\nabla}_{V}^{(B_{\\tau})}l$ First, we prove the infnity norm upperbound onthe original gradient. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\lVert\\mathrm{Vec}\\nabla_{V}l\\right\\rVert_{\\infty}=\\left\\lVert\\mathrm{Vec}\\left(V X a-\\epsilon_{x,\\epsilon_{x}}\\right)\\left(X a\\right)^{\\top}\\right\\rVert_{\\infty}}&{}\\\\ {=\\left\\lVert\\mathrm{Vec}\\left(V X a\\right)\\left(X a\\right)^{\\top}\\right\\rVert_{\\infty}+\\left\\lVert\\mathrm{Vec}\\left(\\epsilon_{x,\\epsilon}\\left(X a\\right)^{\\top}\\right\\rVert_{\\infty}\\right)}\\\\ {=\\left.\\frac{\\mathrm{max}}{s_{x}\\varepsilon\\left(V I\\right)}\\left|\\left(V X a\\right)_{s}\\left(X a\\right)^{\\top}\\right\\rVert_{\\infty}+\\left\\lVert\\displaystyle\\sum_{l=1}^{T}a_{l}\\mathrm{Vec}\\left(\\epsilon_{x,\\epsilon}\\frac{\\epsilon_{x}^{\\top}}{s_{x}\\varepsilon_{l}}\\right)\\right\\rVert_{\\infty}}\\\\ {\\leq\\left\\lVert\\left(V X a\\right)\\right\\rVert_{\\infty}\\left\\lVert\\left(X a\\right)_{l}^{\\top}\\right\\rVert_{\\infty}+\\left\\lVert\\displaystyle\\sum_{l=1}^{T}a_{l}\\mathrm{Vec}\\left(\\epsilon_{x,\\epsilon}\\frac{\\epsilon_{x}^{\\top}}{s_{x}\\varepsilon_{l}}\\right\\rVert_{\\infty}\\right.}\\\\ {=\\displaystyle\\sum_{s_{x}\\varepsilon=1}^{T}\\left(\\hat{a}_{s}+\\left\\lVert{\\Delta_{A,s}}\\right\\rVert_{1}\\right)\\left(\\hat{a}_{l}+\\left\\lVert{\\Delta_{A,l}}\\right\\rVert_{1}\\right)\\left\\lVert V\\varepsilon_{x,\\varepsilon_{s}}\\right\\rVert_{\\infty}\\right\\rVert_{\\infty}}\\\\ {\\left.+\\left.\\displaystyle\\sum_{k_{x}\\varepsilon=1}^{T}\\left(\\hat{a}_{t}+\\left\\lVert{\\Delta_{A,s}}\\right\\rVert_{1}\\right)\\left\\lVert\\mathrm{Vec}\\left(\\epsilon_{x,\\epsilon}\\frac{\\epsilon_{x}^{\\top}}{s_{x}\\varepsilon_{l}}\\right\\rVert_{\\infty}\\leq\\Theta\\left(1\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "And therefore, the preconditioned gradient can also be bounded. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|\\hat{\\nabla}_{V}^{(B_{\\tau})}l\\|_{\\infty}=\\left\\|\\left(I_{N}-\\frac{\\mathbf{1}_{N}\\mathbf{1}_{N}^{\\top}}{N}\\right)\\left(\\nabla_{V}^{(B_{\\tau})}l\\right)\\mathrm{diag}(1/\\mu)\\left(I_{N}-\\frac{\\mu\\mu^{\\top}}{\\|\\mu\\|^{2}}\\right)\\right\\|_{\\infty}}\\\\ {\\displaystyle\\leq\\left\\|\\left(\\nabla_{V}^{(B_{\\tau})}l\\right)\\mathrm{diag}(1/\\mu)\\left(I_{N}-\\frac{\\mu\\mu^{\\top}}{\\|\\mu\\|^{2}}\\right)\\right\\|_{\\infty}}\\\\ {\\displaystyle+\\left\\|\\frac{\\mathbf{1}_{N}\\mathbf{1}_{N}^{\\top}}{N}\\left(\\nabla_{V}^{(B_{\\tau})}l\\right)\\mathrm{diag}(1/\\mu)\\left(I_{N}-\\frac{\\mu\\mu^{\\top}}{\\|\\mu\\|^{2}}\\right)\\right\\|_{\\infty}}\\\\ {\\displaystyle\\leq O(N)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "since $\\begin{array}{r}{\\mu_{k}\\geq\\frac{c}{N}}\\end{array}$ for all $k\\in[N]$ . Now we finished the proof. ", "page_idx": 19}, {"type": "text", "text": "With the upper bound of the infinity norm, we have the following upper bound on the second order moments of the preconditioned gradients of $\\boldsymbol{A}$ and $V$ ", "page_idx": 19}, {"type": "text", "text": "Corollary B.11. With the same setting in Lemma B.10 and $\\begin{array}{r}{\\left\\|\\hat{\\nabla}_{a^{(k)}}l\\right\\|_{\\infty}\\leq O(N),\\left\\|\\mathrm{Vec}\\hat{\\nabla}_{V}l\\right\\|_{\\infty}\\leq O(N).}\\end{array}$ Moreover, $\\|\\mathbf{A}\\mathbf{}_{A}\\|_{F}^{2},\\|\\mathbf{A}_{V}\\|_{F}^{2}\\leq O(1/T)$ .Then, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left\\lVert\\hat{\\nabla}_{A}l\\right\\rVert_{\\mu}^{2}\\leq O(T N^{2}),\\quad\\mathbb{E}\\left\\lVert\\hat{\\nabla}_{V}l\\right\\rVert_{\\mu}^{2}\\leq O(N^{3})\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. We directly upper bound $\\big\\|\\hat{\\nabla}_{A}l\\big\\|_{\\mu}^{2}$ and $\\left\\|\\hat{\\nabla}_{V}l\\right\\|_{\\mu}^{2}$ using the upper bound on infinity norm. Since $\\|\\mathbf{A}\\mathbf{}_{A}\\|_{F}^{2},\\|\\mathbf{A}_{V}\\|_{F}^{2}\\leq O(1/T)$ , we have the infinity norm be upper bounded by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\lVert\\hat{\\nabla}_{a^{(k)}}l\\right\\rVert_{\\infty}\\leq O(N),\\ \\ \\left\\lVert\\mathrm{Vec}\\hat{\\nabla}_{V}l\\right\\rVert_{\\infty}\\leq O(N)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, we can first bound the Frobenius norm $\\|\\hat{\\nabla}_{A}l\\|_{F}^{2},\\|\\hat{\\nabla}_{V}l\\|_{F}^{2}$ . We have $V\\in\\mathbb{R}^{N\\times N},A\\in\\mathbb{R}^{T\\times N}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|\\hat{\\nabla}_{V}l\\|_{F}^{2}\\le N^{2}\\left\\|\\hat{\\nabla}_{V}l\\right\\|_{\\infty}^{2}=O(N^{4}),\\quad\\|\\hat{\\nabla}_{A}l\\|_{F}^{2}\\le N T\\left\\|\\mathrm{Vec}\\hat{\\nabla}_{A}l\\right\\|_{\\infty}^{2}=O(N^{3}T).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "That leads to: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\lVert\\hat{\\nabla}_{V}l\\right\\rVert_{\\mu}^{2}=\\langle\\hat{\\nabla}_{V}l,\\hat{\\nabla}_{V}l\\mathrm{\\,diag}(\\mu)\\rangle\\leq O\\left(\\frac1N\\right)\\lVert\\hat{\\nabla}_{V}l\\rVert_{F}^{2}\\leq O(N^{3})}\\\\ &{\\left\\lVert\\hat{\\nabla}_{A}l\\right\\rVert_{\\mu}^{2}=\\langle\\hat{\\nabla}_{A}l,\\hat{\\nabla}_{A}l\\mathrm{\\,diag}(\\mu)\\rangle\\leq O\\left(\\frac1N\\right)\\lVert\\hat{\\nabla}_{A}l\\rVert_{F}^{2}\\leq O(N^{2}T)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the second inequality comes from the assumption that $\\mu\\sim\\Theta(1/N)$ ", "page_idx": 19}, {"type": "text", "text": "Now with the upper bound of the second moments of the gradients, we begin to prove the concentration of the gradients. We first consider the first-order terms that need to be bounded in the signal dynamics: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\langle h_{V},P\\rangle_{\\mu}}{K_{P}K_{Q}},\\ \\ \\frac{\\langle h_{A},Q\\rangle_{\\mu}}{K_{P}K_{Q}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Lemma B.12. Fix $\\varepsilon,\\delta>0$ .Under Assumption 2.1, suppose $\\begin{array}{r}{\\left\\|\\hat{\\nabla}_{\\pmb{a}^{(k)}}l\\right\\|_{\\infty}\\leq\\Theta(N),\\left\\|\\hat{\\nabla}_{V}l\\right\\|_{\\infty}\\leq\\Theta(N).}\\end{array}$ If B\u2265 9(1)max(N4,Q2) g , then with probability at least $1-\\delta$ we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left|\\frac{\\langle h_{V},P\\rangle_{\\mu}}{K_{P}K_{Q}}\\right|\\leq\\varepsilon,\\ \\left|\\frac{\\langle h_{A},Q\\rangle_{\\mu}}{K_{P}K_{Q}}\\right|\\leq\\varepsilon.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. Note $\\begin{array}{r}{\\pmb{h}_{A}=\\frac{1}{B}\\sum_{i}\\hat{\\nabla}_{\\pmb{a}^{(k_{i})}}\\pmb{l}(i)-\\mathbb{E}\\hat{\\nabla}_{\\pmb{a}^{(k)}}\\pmb{l}}\\end{array}$ thus we have the upper bound for each coordinate of the gradient error bounded by $\\Theta(N)$ . Similarly, we have the upper bound for each coordinate of the gradient error of $\\pmb{h}_{V}$ bounded by $\\Theta(N)$ ", "page_idx": 20}, {"type": "text", "text": "Then, we can bound the infinity norm of $\\langle\\hat{\\nabla}_{V}l(i),P\\rangle_{\\mu}$ and $\\langle\\hat{\\nabla}_{A}l(i),P\\rangle_{\\mu}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left|\\frac{\\langle\\hat{\\nabla}_{\\mathbf{V}}I(i),\\,D_{p}\\rangle_{\\mu}}{K_{P}K_{Q}}\\right|=\\left|\\frac{\\sum_{n=1}^{K_{Q}}\\hat{\\nabla}_{\\mathbf{V}}I(i)_{n,m}P_{\\mu,m}}{K_{P}K_{Q}}\\right|}&{}\\\\ &{\\leq\\frac{X^{2}\\left|\\hat{\\nabla}_{\\mathbf{V}}I(i)\\right|_{\\mathbf{V}}\\left|\\hat{\\mathbf{P}}\\right|_{\\infty}}{K_{P}K_{Q}}}\\\\ &{\\leq\\frac{\\Theta(1)N^{3}}{K_{P}K_{Q}}.}\\\\ {\\left|\\frac{\\langle\\hat{\\nabla}_{\\mathbf{A}}I(i),\\,D_{p}\\rangle_{\\mu}}{K_{P}K_{Q}}\\right|=\\left|\\frac{\\sum_{n=1}^{T_{Q}}\\sum_{n=1}^{K_{Q}}\\hat{\\nabla}_{\\mathbf{A}}I(i)_{n,m}Q_{n,m}}{K_{P}K_{Q}}\\right|}\\\\ &{\\leq\\frac{C D N^{3}\\left|\\hat{\\nabla}_{\\mathbf{A}}I(i)\\right|_{\\mathbf{V}}\\left|\\hat{\\mathbf{Q}}\\right|_{\\infty}}{K_{P}K_{Q}}}\\\\ &{\\leq\\frac{\\Theta(1)Q N^{2}K_{Q}}{K_{P}K_{Q}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n(\\|Q\\|_{\\infty}\\leq1.)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that $\\begin{array}{r}{\\mathbb{E}\\left[\\nabla_{a^{(k_{i})}}l(i)-\\mathbb{E}\\,\\nabla_{a^{(k)}}l\\right]=0,\\mathbb{E}\\left[\\nabla_{V}l(i)-\\mathbb{E}\\,\\nabla_{V}l\\right]=0}\\end{array}$ , which means the two terms above have expectation O. Since $h_{A},\\bar{h}_{V}$ are both averages of $B$ gradients of a single sample, we use Hoeffding Inequality: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbb{P}\\left(\\left|\\frac{\\langle h_{V},P\\rangle_{\\mu}}{K_{P}K_{Q}}\\right|\\geq\\varepsilon\\right)\\leq2\\exp\\left(\\frac{-B\\varepsilon^{2}K_{P}^{2}K_{Q}^{2}}{N^{6}}\\right)}\\\\ &{}&{\\mathbb{P}\\left(\\left|\\frac{\\langle h_{A},Q\\rangle_{\\mu}}{K_{P}K_{Q}}\\right|\\geq\\varepsilon\\right)\\leq2\\exp\\left(\\frac{-B\\varepsilon^{2}K_{P}^{2}K_{Q}^{2}}{N^{4}Q^{2}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By union bound, if $\\begin{array}{r}{B\\geq\\operatorname*{max}\\left(N^{2},Q^{2}\\right)\\frac{N^{4}\\log\\frac{4}{\\delta}}{\\epsilon^{2}K_{P}^{2}K_{Q}^{2}}}\\end{array}$ it has at least $1-\\delta$ probability, s.t. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left|\\frac{\\langle h_{V},P\\rangle_{\\mu}}{K_{P}K_{Q}}\\right|\\leq\\varepsilon,\\ \\left|\\frac{\\langle h_{A},Q\\rangle_{\\mu}}{K_{P}K_{Q}}\\right|\\leq\\varepsilon.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then we finish this section with the concentration of the second order terms $\\|\\pmb{h}_{A}\\|^{2}$ and $\\|\\pmb{h}_{V}\\|^{2}$ ,which need to be bounded in the error evolution. ", "page_idx": 20}, {"type": "text", "text": "Lemma B.13. Fi $x\\;\\varepsilon,\\delta\\;>\\;0$ Under Assumption $2.I,\\,i f\\,\\mathbb{E}\\,\\|\\hat{\\nabla}_{V}l\\|_{\\mu}^{2}=\\mathrm{Tm}{\\mathsf{p}}_{3}=O(N^{3}),\\mathbb{E}\\,\\|\\hat{\\nabla}_{A}l\\|_{\\mu}^{2}=$ Tmp4 = O(TN2), B\u2265 TN), , then with probability at least $1-\\delta$ we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|h_{A}\\|_{\\mu}\\leq\\varepsilon,\\ \\ \\|h_{V}\\|_{\\mu}\\leq\\varepsilon.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. Simlart LemmaB.12,Ehy =0.EhA =0. By Lemma B.9, when we pickB\u2265TN we have $\\|h_{A}\\|_{\\mu}\\leq\\varepsilon,\\|h_{V}\\|_{\\mu}\\leq\\varepsilon$ with probability at least $1-\\delta$ \u53e3 ", "page_idx": 21}, {"type": "text", "text": "C  The population projected process ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we define the projection of the true SGD process onto the \u201cspace of population trajectories\". Then, we derive formulas for the dynamics of projected process and the distance of the true SGD process to the space of population trajectories. All proofs \u2014- except for those short ones \u2014 are deferred to the end of this section. ", "page_idx": 21}, {"type": "text", "text": "C.1Definition of the population projection ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The main reason we analyze the population process first is that on the population trajectory, both layers possess special structures. Recall that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\hat{\\nabla}_{V}l=\\left\\|A\\right\\|_{\\mu}^{2}\\left(V-\\mu\\mathbf{1}^{\\top}\\right)-\\langle Q,A\\rangle_{\\mu}\\left(P-\\mu\\mathbf{1}^{\\top}\\right),}\\\\ &{\\mathbb{E}\\hat{\\nabla}_{A}l=\\left(\\left\\|V\\right\\|_{\\mu}^{2}-\\left\\|\\mu\\right\\|^{2}\\right)\\left(A-\\displaystyle\\frac{\\mathbf{1}_{T}\\mathbf{1}_{N}^{\\top}}{T}\\right)-\\left(\\langle V,P\\rangle_{\\mu}-\\left\\|\\mu\\right\\|^{2}\\right)\\left(Q-\\displaystyle\\frac{\\mathbf{1}_{T}\\mathbf{1}_{N}^{\\top}}{T}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and we initialize $V_{0}\\!-\\!\\mu\\mathbf{1}^{\\top}=0$ and $A-11^{\\top}/T=0$ . Note that for any $(z_{\\tau})_{\\tau}$ , if $z_{0}=0$ and $\\dot{z}=A z\\!+\\!B z_{*}$ \uff0c then $z_{\\tau}\\propto z_{*}$ for all $\\tau\\geq0$ . In other words, $z_{\\tau}$ moves only along the direction $z_{\\ast}$ and therefore, can be characterized by a single real number. This is exactly the same case of $V-\\mu\\mathbf{1}^{\\top}$ and $A-11^{\\top}/T$ in the population case. Hence, in the population case, $V$ stays on the line crossing $\\mu1^{\\top}$ and $P$ , and $\\boldsymbol{A}$ stays on the line crossing $\\mathbf{11^{\\top}}/T$ and $\\b{Q}$ ", "page_idx": 21}, {"type": "text", "text": "Unfortunately, mini-batch SGD does not stay exactly on the population trajectory. We can still, however, look at the projection of SGD onto the \u201cpopulation trajectories\". Formally, for any $V,A$ satisfying $\\mathbf{1}_{N}^{\\top}V=\\mathbf{1}_{N}^{\\top}$ $\\mathbf{1}_{T}^{\\mathcal{\\bar{\\tau}}}A=\\mathbf{1}_{N}^{\\top}$ ,and $V\\pmb{\\mu}=\\pmb{\\mu}$ wedefine ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\alpha_{V}:=\\operatorname*{argmin}_{\\alpha\\in\\mathbb{R}}\\left\\|\\alpha P+(1-\\alpha)\\pmb{\\mu}\\mathbf{1}_{N}^{\\top}-V\\right\\|_{\\mu}^{2},}\\\\ {\\displaystyle\\alpha_{A}:=\\operatorname*{argmin}_{\\alpha\\in\\mathbb{R}}\\left\\|\\alpha\\pmb{Q}+(1-\\alpha)\\frac{\\mathbf{1}_{T}\\mathbf{1}_{N}^{\\top}}{T}-A\\right\\|_{\\mu}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By setting the derivative to be O, we can obtain the following closed-form formulas for $\\alpha_{V}$ and $\\alpha_{A}$ ", "page_idx": 21}, {"type": "text", "text": "Note: Without specification, we drop the the time subscript $\\tau$ and consider $\\alpha_{V}:=\\alpha_{V,\\tau}$ for similicity. Lemma C.1. For any $V,A$ satisfying $\\mathbf{1}_{N}^{\\top}V=\\mathbf{1}_{N}^{\\top},\\mathbf{1}_{T}^{\\top}A=\\mathbf{1}_{N}^{\\top}$ , and $V\\pmb{\\mu}=\\pmb{\\mu}$ we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\alpha_{V}=K_{V P}/K_{P}\\quad a n d\\quad\\alpha_{A}=K_{A Q}/K_{Q},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\nK_{P}=\\|P\\|_{\\mu}^{2}-\\|\\mu\\|^{2},\\,K_{V P}=\\langle V,P\\rangle_{\\mu}-\\|\\mu\\|^{2},\\,K_{Q}=\\|Q\\|_{\\mu}^{2}-1/T,\\,K_{A Q}=\\langle A,Q\\rangle_{\\mu}-1/T.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For notational simplicity, we define $\\beta_{V}=1-\\alpha_{V},\\beta_{A}=1-\\alpha_{A}.$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\tilde{V}=\\alpha_{V}P+\\beta_{V}\\mu\\mathbf{1}^{\\top}\\quad\\mathrm{and}\\quad\\tilde{A}=\\alpha_{A}Q+\\beta_{A}\\frac{\\mathbf{1}\\mathbf{1}^{\\top}}{T}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then, define $\\Delta_{V}=V-\\tilde{V}$ and $\\Delta_{A}=A-\\tilde{A}$ so that we can decompose $\\pmb{V}=\\tilde{\\pmb{V}}+\\Delta_{V}$ and $A=\\tilde{A}+\\Delta_{A}$ By our construction, we have $\\tilde{V}-\\mu\\mathbf{1}^{\\top}\\perp\\Delta_{V}$ and similarly for $\\Delta_{A}$ . We will now show that we can in fact drop $\\mu1^{\\top}$ ", "page_idx": 21}, {"type": "text", "text": "Lemma C.2. For any $V^{\\prime}\\;=\\;\\theta P+(1\\,-\\,\\theta)\\mu\\mathbf{1}^{\\top}$ and $A^{\\prime}\\,=\\,\\theta Q\\,+\\,(1\\,-\\,\\theta){\\bf11}^{\\top}/T$ with $\\theta\\:\\in\\:\\mathbb{R},$ we have $\\langle\\mathbf{A}_{V},V^{\\prime}\\rangle_{\\mu}\\,=\\,0$ and $\\langle\\mathbf{\\DeltaA}_{A},A^{\\prime}\\rangle_{\\mu}\\,=\\,0$ .In particular, we have $\\langle\\Delta_{V},\\dot{P^{\\prime}}\\rangle_{\\mu}\\,=\\,\\Bigl\\langle\\Delta_{V},\\tilde{V}\\Bigr\\rangle_{\\mu}\\,=\\,0$ and $\\langle\\Delta_{A},Q\\rangle_{\\mu}=\\left\\langle\\Delta_{A},\\tilde{A}\\right\\rangle_{\\mu}=0.$ ", "page_idx": 21}, {"type": "text", "text": "Proof. Note that $\\langle\\mu\\mathbf{1}^{\\top},\\Delta_{V}\\rangle_{\\mu}=\\left\\langle\\mu,(V-\\tilde{V})\\mu\\right\\rangle=0$ . Hence, $\\langle\\Delta_{V},V^{\\prime}\\rangle=\\langle\\Delta_{V},V^{\\prime}-\\mu\\mathbf{1}^{\\top}\\rangle=0$ . For $\\langle\\mathbf{A}_{A},A^{\\prime}\\rangle$ , it suffices to note that $\\langle\\Delta_{A},\\mathbf{11}^{\\top}/T\\rangle_{\\mu}=\\left\\langle\\mathbf{1}^{\\top}(A-\\tilde{A}),\\mathbf{1}^{\\top}/T\\right\\rangle_{\\mu}=0$ \u53e3 ", "page_idx": 21}, {"type": "text", "text": "The following lemma the basic definitions and results about the population projection. ", "page_idx": 21}, {"type": "text", "text": "Lemma C.3 (Definitions and basic results on the population projection). Suppose that $V,A$ satisfy $\\mathbf{1}_{N}^{\\top}V=\\mathbf{1}_{N}^{\\top},\\mathbf{1}_{T}^{\\top}A=\\mathbf{1}_{N}^{\\top}$ and $V\\pmb{\\mu}=\\pmb{\\mu}$ Wedefinethefollowing: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{K_{P}=\\left\\|P\\right\\|_{\\mu}^{2}-\\left\\|\\mu\\right\\|^{2},}&{K_{V P}=\\langle V,P\\rangle_{\\mu}-\\left\\|\\mu\\right\\|^{2},}&{K_{V}=\\left\\|V\\right\\|_{\\mu}^{2}-\\left\\|\\mu\\right\\|^{2},}\\\\ &{}&{\\alpha_{V}=K_{V P}/K_{P},\\quad\\beta_{V}=1-\\alpha_{V},\\quad\\tilde{V}=\\alpha_{V}P+\\beta_{V}\\mu\\mathbf{1}^{\\top},\\quad\\quad}\\\\ &{}&{\\Delta_{V}=V-\\tilde{V},\\quad}\\\\ &{}&{K_{Q}=\\left\\|Q\\right\\|_{\\mu}^{2}-1/T,\\quad K_{A Q}=\\langle A,Q\\rangle_{\\mu}-1/T,\\quad K_{A}=\\left\\|A\\right\\|_{\\mu}^{2}-1/T,\\quad}\\\\ &{}&{\\alpha_{A}=K_{A Q}/K_{Q},\\quad\\beta_{A}=1-\\beta_{A},\\quad\\tilde{A}=\\alpha_{A}A+\\beta_{A}\\mathbf{1}_{T}\\mathbf{1}_{N}^{\\top}/T,\\quad}\\\\ &{}&{\\Delta_{A}=A-\\tilde{A}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Moreover, by Lemma C.2, the following hold. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{K_{V P}=\\left\\langle\\tilde{V},P\\right\\rangle_{\\mu}-\\|\\mu\\|^{2}=\\alpha_{V}K_{P},}\\\\ &{\\quad K_{V}=\\left\\|\\tilde{V}\\right\\|_{\\mu}^{2}+\\left\\|\\mathbf{A}_{V}\\right\\|_{\\mu}^{2}-\\|\\mu\\|^{2}=\\alpha_{V}^{2}K_{P}+\\left\\|\\mathbf{A}_{V}\\right\\|_{\\mu}^{2},}\\\\ &{K_{A Q}=\\left\\langle\\tilde{A},Q\\right\\rangle_{\\mu}-1/T=\\alpha_{A}K_{Q},}\\\\ &{\\quad K_{A}=\\left\\|\\tilde{A}\\right\\|_{\\mu}^{2}+\\left\\|\\mathbf{A}_{A}\\right\\|_{\\mu}^{2}-1/T=\\alpha_{A}^{2}K_{Q}+\\left\\|\\mathbf{A}_{A}\\right\\|_{\\mu}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "C.2  Dynamics of the population projected process and the approximation error ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We write ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{\\tau+1}=V_{\\tau}-\\eta_{V}\\,\\mathbb{E}\\,\\hat{\\nabla}_{V}l-\\eta_{V}\\left(\\hat{\\nabla}_{V}^{(B)}l-\\mathbb{E}\\,\\hat{\\nabla}_{V}l\\right)=:V_{\\tau}-\\eta_{V}\\,\\mathbb{E}\\,\\hat{\\nabla}_{V}l-\\eta_{V}h_{V,\\tau},}\\\\ &{A_{\\tau+1}=A_{\\tau}-\\eta_{A}\\,\\mathbb{E}\\,\\hat{\\nabla}_{A}l-\\eta_{A}\\left(\\hat{\\nabla}_{A}^{(B)}l-\\mathbb{E}\\,\\hat{\\nabla}_{A}l\\right)=:A_{\\tau}-\\eta_{A}\\,\\mathbb{E}\\,\\hat{\\nabla}_{A}l-\\eta_{A}h_{A,\\tau},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the expectations are taken over the fresh samples at step $\\tau$ ", "page_idx": 22}, {"type": "text", "text": "First, we expand the expected preconditioned gradients around the population projection.   \nLemma C.4 (Expanding the gradients). ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathbb{E}\\hat{\\nabla}_{A}l=K_{P}\\alpha_{V}\\left(\\alpha_{V}\\alpha_{A}-1\\right)\\left(Q-\\frac{\\mathbf{1}_{T}\\mathbf{1}_{N}^{\\top}}{T}\\right)+K_{P}\\alpha_{V}^{2}\\Delta_{A}+\\|\\Delta_{V}\\|_{\\mu}^{2}\\left(A-\\frac{\\mathbf{1}_{T}\\mathbf{1}_{N}^{\\top}}{T}\\right),}\\\\ &{\\displaystyle\\mathbb{E}\\hat{\\nabla}_{V}l=\\alpha_{A}K_{Q}\\left(\\alpha_{A}\\alpha_{V}-1\\right)\\left(P-\\mu\\mathbf{1}^{\\top}\\right)+\\frac{\\alpha_{V}-1}{T}\\left(P-\\mu\\mathbf{1}^{\\top}\\right)}\\\\ &{\\quad\\quad\\quad\\quad+\\left(\\alpha_{A}^{2}K_{Q}+\\displaystyle\\frac{1}{T}\\right)\\Delta_{V}+\\|\\Delta_{A}\\|^{2}\\left(V-\\mu\\mathbf{1}^{\\top}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then, we compute the dynamics of the projected process. ", "page_idx": 22}, {"type": "text", "text": "Lemma C.5 (Dynamics of the population projection). ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha_{V,\\tau+1}=\\alpha_{V,\\tau}+\\eta_{V}K_{Q}\\left(1-\\alpha_{A}\\alpha_{V}\\right)\\alpha_{A}+\\eta_{V}\\frac{1-\\alpha_{V}}{T}-\\eta_{V}\\alpha_{V}\\left\\|\\mathbf{A}_{A}\\right\\|^{2}-\\frac{\\eta_{V}}{K_{P}}\\left<h_{V,\\tau},P\\right>_{\\mu},}\\\\ &{\\alpha_{A,\\tau+1}=\\alpha_{A,\\tau}+\\eta_{A}K_{P}\\left(1-\\alpha_{V}\\alpha_{A}\\right)\\alpha_{V}-\\eta_{A}\\alpha_{A}\\left\\|\\mathbf{A}_{V}\\right\\|_{\\mu}^{2}-\\frac{\\eta_{A}}{K_{Q}}\\left<h_{A,\\tau},Q\\right>_{\\mu}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Note that $\\tilde{V}=\\mu\\mathbf{1}^{\\top}+\\alpha_{V}(P-\\mu\\mathbf{1}^{\\top})$ and $\\tilde{A}=\\mathbf{1}_{T}\\mathbf{1}_{N}^{\\top}/T+\\alpha_{A}(\\pmb{Q}-\\mathbf{1}_{T}\\mathbf{1}_{N}^{\\top}/T)$ Hence, this also gives formulas for $\\tilde{V}_{\\tau+1}$ and $\\tilde{A}_{\\tau+1}$ ", "page_idx": 22}, {"type": "text", "text": "Now, we consider the dynamics of the errors. ", "page_idx": 22}, {"type": "text", "text": "Lemma C.6 (Dynamics of the errors). ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\Delta_{A,\\tau+1}\\right\\|_{\\mu}^{2}=\\left(1-\\eta_{A}K_{P}\\alpha_{V}^{2}-\\eta_{A}\\left\\|\\Delta_{V}\\right\\|_{\\mu}^{2}\\right)^{2}\\left\\|\\Delta_{A}\\right\\|_{\\mu}^{2}}\\\\ &{\\qquad\\qquad\\qquad-\\left2\\eta_{A}\\left(1-\\eta_{A}K_{P}\\alpha_{V}^{2}-\\eta_{A}\\left\\|\\Delta_{V}\\right\\|_{\\mu}^{2}\\right)\\langle\\Delta_{A},h_{A}\\rangle_{\\mu}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\qquad-\\frac{\\eta_{A}^{2}}{K_{Q}}\\left<h_{A,\\tau},Q\\right>_{\\mu}^{2}+\\eta_{A}^{2}\\left|\\left|h_{A}\\right|\\right|_{\\mu}^{2},}\\\\ &{\\left|\\Delta_{V,\\tau+1}\\right|\\right|_{\\mu}^{2}=\\left(1-\\eta_{V}\\left(\\alpha_{A}^{2}K_{Q}+\\frac{1}{T}\\right)-\\eta_{V}\\left\\|\\Delta_{A}\\right\\|^{2}\\right)^{2}\\left\\|\\Delta_{V}\\right\\|_{\\mu}^{2}}\\\\ &{\\qquad\\qquad\\qquad+2\\eta_{V}\\left(1-\\eta_{V}\\left(\\alpha_{A}^{2}K_{Q}+\\frac{1}{T}\\right)-\\eta_{V}\\left\\|\\Delta_{A}\\right\\|^{2}\\right)\\left<\\Delta_{V},h_{V}\\right>_{\\mu}}\\\\ &{\\qquad\\qquad\\qquad-\\frac{\\eta_{V}^{2}}{K_{P}}\\left<P,h_{V}\\right>_{\\mu}^{2}+\\eta_{V}^{2}\\left\\|h_{V}\\right\\|_{\\mu}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "C.3 Omitted proofs in this section ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Proof of Lemma C.1. We compute ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{1}{2}\\partial_{\\alpha}\\left\\|\\alpha P+(1-\\alpha)\\mu\\mathbf{1}^{\\top}-V\\right\\|_{\\mu}^{2}=\\left\\langle\\alpha P+(1-\\alpha)\\mu\\mathbf{1}^{\\top}-V,P-\\mu\\mathbf{1}^{\\top}\\right\\rangle_{\\mu}}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\qquad=\\left\\langle\\alpha P+(1-\\alpha)\\mu\\mathbf{1}^{\\top}-V,P\\right\\rangle_{\\mu}=\\alpha K_{P}+\\left\\|\\mu\\right\\|^{2}-\\left\\langle V,P\\right\\rangle_{\\mu}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Set the derivative to be O, and we get $\\alpha_{V}=(\\langle V,P\\rangle_{\\mu}-\\|\\mu\\|^{2})/K_{P}$ . Similarly, we compute ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{1}{2}\\partial_{\\alpha}\\left\\|\\alpha Q+(1-\\alpha)\\frac{\\mathbf{11}^{\\top}}{T}-A\\right\\|_{\\mu}^{2}=\\left\\langle\\alpha Q+(1-\\alpha)\\frac{\\mathbf{11}^{\\top}}{T}-A,Q-\\frac{\\mathbf{11}^{\\top}}{T}\\right\\rangle_{\\mu}}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\qquad=\\left\\langle\\alpha Q+(1-\\alpha)\\frac{\\mathbf{11}^{\\top}}{T}-A,Q\\right\\rangle_{\\mu}}\\\\ {\\displaystyle\\qquad\\qquad=\\alpha\\left(\\|Q\\|_{\\mu}^{2}-1/T\\right)-\\left(\\langle A,Q\\rangle_{\\mu}-1/T\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Again, set the derivative tobe O, and we get $\\alpha_{A}=\\left(\\langle A,Q\\rangle_{\\mu}-1/T\\right)/(\\|Q\\|_{\\mu}^{2}-1/T).$ Proof of Lemma C.4. Recall from Lemma B.8 that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\,\\hat{\\nabla}_{V}l=\\left(K_{A}+\\frac{1}{T}\\right)\\left(V-\\mu\\mathbf{1}^{\\top}\\right)-\\left(K_{A Q}+\\frac{1}{T}\\right)\\left(P-\\mu\\mathbf{1}^{\\top}\\right),}\\\\ {\\displaystyle\\mathbb{E}\\,\\hat{\\nabla}_{A}l=K_{V}\\left(A-\\frac{\\mathbf{1}_{T}\\mathbf{1}_{N}^{\\top}}{T}\\right)-K_{V P}\\left(Q-\\frac{\\mathbf{1}_{T}\\mathbf{1}^{\\top}}{T}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "First, consider the dynamics of $\\boldsymbol{A}$ . By Lemma C.3, we can further decompose it as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\hat{\\mathbf{V}}_{A}l=\\left(\\alpha_{V}^{2}K_{P}+\\|\\Delta_{V}\\|_{\\mu}^{2}\\right)\\Bigg(A-\\displaystyle\\frac{\\mathbf{1}_{T}\\mathbf{1}_{N}^{\\top}}{T}\\Bigg)-\\alpha_{V}K_{P}\\left(Q-\\displaystyle\\frac{\\mathbf{1}_{T}\\mathbf{1}^{\\top}}{T}\\right)}\\\\ &{\\qquad=K_{P}\\alpha_{V}\\left(\\alpha_{V}\\left(A-\\displaystyle\\frac{\\mathbf{1}_{T}\\mathbf{1}_{N}^{\\top}}{T}\\right)-\\left(Q-\\displaystyle\\frac{\\mathbf{1}_{T}\\mathbf{1}^{\\top}}{T}\\right)\\right)+\\left\\|\\Delta_{V}\\right\\|_{\\mu}^{2}\\left(A-\\displaystyle\\frac{\\mathbf{1}_{T}\\mathbf{1}_{N}^{\\top}}{T}\\right)}\\\\ &{\\qquad=K_{P}\\alpha_{V}\\left(\\alpha_{V}\\left(\\tilde{A}-\\displaystyle\\frac{\\mathbf{1}_{T}\\mathbf{1}_{N}^{\\top}}{T}\\right)-\\left(Q-\\displaystyle\\frac{\\mathbf{1}_{T}\\mathbf{1}^{\\top}}{T}\\right)\\right)+K_{P}\\alpha_{V}^{2}\\Delta_{A}+\\left\\|\\Delta_{V}\\right\\|_{\\mu}^{2}\\left(A-\\displaystyle\\frac{\\mathbf{1}_{T}\\mathbf{1}_{N}^{\\top}}{T}\\right)}\\\\ &{\\qquad=K_{P}\\alpha_{V}\\left(\\alpha_{V}\\alpha_{A}-1\\right)\\left(Q-\\displaystyle\\frac{\\mathbf{1}_{T}\\mathbf{1}_{N}^{\\top}}{T}\\right)+K_{P}\\alpha_{V}^{2}\\Delta_{A}+\\left\\|\\Delta_{V}\\right\\|_{\\mu}^{2}\\left(A-\\displaystyle\\frac{\\mathbf{1}_{T}\\mathbf{1}_{N}^{\\top}}{T}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Similarly, we can rewrite the expected preconditioned gradient of $V$ as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\hat{\\nabla}_{V}l=\\left(\\alpha_{A}^{2}K_{Q}+\\frac{1}{T}\\right)\\left(V-\\mu\\mathbf{1}^{\\top}\\right)-\\left(\\alpha_{A}K_{Q}+\\frac{1}{T}\\right)\\left(P-\\mu\\mathbf{1}^{\\top}\\right)+\\left\\Vert\\Delta_{A}\\right\\Vert^{2}\\left(V-\\mu\\mathbf{1}^{\\top}\\right)}\\\\ &{\\quad\\quad\\quad=\\left(\\alpha_{A}^{2}K_{Q}+\\frac{1}{T}\\right)\\left(\\Tilde{V}-\\mu\\mathbf{1}^{\\top}\\right)-\\left(\\alpha_{A}K_{Q}+\\frac{1}{T}\\right)\\left(P-\\mu\\mathbf{1}^{\\top}\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad+\\left(\\alpha_{A}^{2}K_{Q}+\\frac{1}{T}\\right)\\Delta_{V}+\\left\\Vert\\Delta_{A}\\right\\Vert^{2}\\left(V-\\mu\\mathbf{1}^{\\top}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\alpha_{A}K_{Q}\\left(\\alpha_{A}\\alpha_{V}-1\\right)\\left(P-\\mu\\mathbf{1}^{\\top}\\right)+\\frac{\\alpha_{V}-1}{T}\\left(P-\\mu\\mathbf{1}^{\\top}\\right)}\\\\ &{\\qquad+\\left(\\alpha_{A}^{2}K_{Q}+\\frac{1}{T}\\right)\\Delta_{V}+\\left\\Vert\\Delta_{A}\\right\\Vert^{2}\\left(V-\\mu\\mathbf{1}^{\\top}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof of Lemma C.5. Recall that $\\alpha_{V}={\\langle V,P\\rangle_{\\mu}}\\,/K_{P_{\\!\\perp}}$ and $\\alpha_{A}=\\left<\\boldsymbol{A},\\boldsymbol{Q}\\right>_{\\mu}/K_{Q}$ . First, consider the dynamics of $V$ . By Lemma C.4 and Lemma C.3, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha_{V,\\tau+1}=\\alpha_{V,\\tau}-\\eta_{V}\\frac{\\left<\\hat{\\nabla}_{V}\\mathcal{L}+h_{V,\\tau},P\\right>_{\\mu}}{K_{P}}}\\\\ &{\\qquad=\\alpha_{V,\\tau}-\\frac{\\eta_{V}}{K_{P}}\\alpha_{A}K_{Q}\\left(\\alpha_{A}\\alpha_{V}-1\\right)\\left<P-\\mu\\mathbf{1}^{\\top},P\\right>_{\\mu}-\\frac{\\eta_{V}}{K_{P}}\\frac{\\alpha_{V}-1}{T}\\left<P-\\mu\\mathbf{1}^{\\top},P\\right>_{\\mu}}\\\\ &{\\qquad\\qquad-\\frac{\\eta_{V}}{K_{P}}\\left\\|\\mathbf{\\DeltaA}\\right\\|^{2}\\left<V-\\mu\\mathbf{1}^{\\top},P\\right>_{\\mu}-\\eta_{V}\\frac{\\left<h_{V,\\tau},P\\right>_{\\mu}}{K_{P}}}\\\\ &{\\qquad=\\alpha_{V,\\tau}+\\eta_{V}K_{Q}\\left(1-\\alpha_{A}\\alpha_{V}\\right)\\alpha_{A}+\\eta_{V}\\frac{1-\\alpha_{V}}{T}-\\eta_{V}\\alpha_{V}\\left\\|\\mathbf{\\DeltaA}\\right\\|^{2}-\\frac{\\eta_{V}}{K_{P}}\\left<h_{V,\\tau},P\\right>_{\\mu}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Similarly, for $V$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha_{A,\\tau+1}=\\alpha_{A,\\tau}-\\eta_{A}\\displaystyle\\frac{\\left<\\hat{\\nabla}_{A}\\mathcal{L}+h_{A,\\tau},Q\\right>_{\\mu}}{K_{Q}}}\\\\ &{\\phantom{x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x}}\\\\ &{\\phantom{x x x x x x x x x x x x x x x x x x x x x x x x x x x x}\\displaystyle\\alpha-\\frac{1_{T}\\mathbf{1}_{N}^{\\top}}{K_{Q}}K_{P}\\alpha_{V}\\left(\\alpha_{V}\\alpha_{A}-1\\right)\\left<Q-\\frac{\\mathbf{1}_{T}\\mathbf{1}_{N}^{\\top}}{T},Q\\right>_{\\mu}}\\\\ &{\\phantom{x x x x x x}-\\displaystyle\\frac{\\eta_{A}}{K_{Q}}\\left||\\mathbf{1}_{V}|\\right|_{\\mu}^{2}\\left<A-\\frac{\\mathbf{1}_{T}\\mathbf{1}_{N}^{\\top}}{T},Q\\right>_{\\mu}-\\eta_{A}\\displaystyle\\frac{\\left<h_{A,\\tau},Q\\right>_{\\mu}}{K_{Q}}}\\\\ &{\\phantom{x x x x}=\\alpha_{A,\\tau}+\\eta_{A}K_{P}\\left(1-\\alpha_{V}\\alpha_{A}\\right)\\alpha_{V}-\\eta_{A}\\alpha_{A}\\left|\\mathbf{1}_{V}\\right|_{\\mu}^{2}-\\frac{\\eta_{A}}{K_{Q}}\\left<h_{A,\\tau},Q\\right>_{\\mu}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof of Lemma C.6. First, consider the dynamics of $\\Delta_{A}$ , which is given by ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta_{A,\\tau+1}=\\Delta_{A}-\\eta_{A}K_{P}\\alpha_{V}^{2}\\Delta_{A}-\\eta_{A}\\left\\lVert\\Delta_{V}\\right\\rVert_{\\mu}^{2}\\left(A-\\frac{\\mathbf{1}_{T}\\mathbf{1}_{N}^{\\top}}{T}\\right)-\\eta_{A}h_{A}\\mathrm{~}}\\\\ {+\\left(\\eta_{A}\\alpha_{A}\\left\\lVert\\Delta_{V}\\right\\rVert_{\\mu}^{2}+\\frac{\\eta_{A}}{K_{Q}}\\left\\langle h_{A,\\tau},Q\\right\\rangle_{\\mu}\\right)\\left(Q-\\frac{\\mathbf{1}_{T}\\mathbf{1}_{N}^{\\top}}{T}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Decompose $\\boldsymbol{A}$ into $\\tilde{A}+\\Delta_{A}$ , rearrange terms, and we obtain ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta_{A,\\tau+1}=\\Delta_{A}-\\eta_{A}K_{P}\\alpha_{V}^{2}\\Delta_{A}-\\eta_{A}\\left\\lVert\\Delta_{V}\\right\\rVert_{\\mu}^{2}\\Delta_{A}}\\\\ &{\\phantom{\\Delta_{A,\\tau+1}}-\\eta_{A}\\left\\lVert\\Delta_{V}\\right\\rVert_{\\mu}^{2}\\left(\\tilde{A}-\\displaystyle\\frac{\\mathbf{1}_{T}\\mathbf{1}_{N}^{\\top}}{T}\\right)+\\eta_{A}\\alpha_{A}\\left\\lVert\\Delta_{V}\\right\\rVert_{\\mu}^{2}\\left(Q-\\displaystyle\\frac{\\mathbf{1}_{T}\\mathbf{1}_{N}^{\\top}}{T}\\right)}\\\\ &{\\phantom{\\Delta_{A,\\tau+1}}+\\displaystyle\\frac{\\eta_{A}}{K_{Q}}\\left\\langle h_{A,\\tau},Q\\right\\rangle_{\\mu}\\left(Q-\\displaystyle\\frac{\\mathbf{1}_{T}\\mathbf{1}_{N}^{\\top}}{T}\\right)-\\eta_{A}h_{A}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Note that $\\tilde{A}-\\mathbf{1}\\mathbf{1}^{\\top}/T=\\alpha_{A}Q+(1-\\alpha_{A})\\mathbf{1}\\mathbf{1}^{\\top}/T-\\mathbf{1}\\mathbf{1}^{\\top}/T=\\alpha_{A}(Q-\\mathbf{1}\\mathbf{1}^{\\top}/T).$ Hence, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta_{A,\\tau+1}=\\Delta_{A}-\\eta_{A}K_{P}\\alpha_{V}^{2}\\Delta_{A}-\\eta_{A}\\left\\lVert\\mathbf{A}_{V}\\right\\rVert_{\\mu}^{2}\\Delta_{A}}\\\\ &{\\qquad\\qquad-\\underbrace{\\eta_{A}\\alpha_{A}\\left\\lVert\\mathbf{A}_{V}\\right\\rVert_{\\mu}^{2}\\!\\!\\left(\\!Q\\!-\\!\\frac{\\mathbf{1}_{T}\\mathbf{1}_{N}^{\\top}}{T}\\!\\right)}_{\\displaystyle+\\left.\\frac{\\eta_{A}}{K_{Q}}\\left\\langle\\mathbf{1}_{A,\\tau},Q\\right\\rangle_{\\mu}\\left(Q-\\frac{\\mathbf{1}_{T}\\mathbf{1}_{N}^{\\top}}{T}\\right)-\\eta_{A}h_{A}}\\!\\right\\rVert\\Delta_{V}\\!\\right\\lVert_{\\mu}^{2}\\!\\left(\\!Q\\!-\\!\\frac{\\mathbf{1}_{T}\\mathbf{1}_{N}^{\\top}}{T}\\!\\right)}\\\\ &{\\qquad\\qquad+\\left.\\frac{\\eta_{A}}{K_{Q}}\\left\\langle h_{A,\\tau},Q\\right\\rangle_{\\mu}\\left(Q-\\frac{\\mathbf{1}_{T}\\mathbf{1}_{N}^{\\top}}{T}\\right)-\\eta_{A}h_{A}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "$=\\left(1-\\eta_{A}K_{P}\\alpha_{V}^{2}-\\eta_{A}\\left\\|\\Delta_{V}\\right\\|_{\\mu}^{2}\\Delta_{A}\\right)\\Delta_{A}+\\frac{\\eta_{A}}{K_{Q}}\\left<h_{A,\\tau},\\underline{{{Q}}}\\right>_{\\mu}\\left(\\underline{{{Q}}}-\\frac{\\mathbf{1}_{T}\\mathbf{1}_{N}^{\\top}}{T}\\right)-\\eta_{A}h_{A}.$ Recall that $\\langle\\Delta_{A},Q-{\\bf11^{\\top}}/T\\rangle_{\\mu}=0$ $\\|\\boldsymbol{Q}-\\mathbf{1}\\mathbf{1}^{\\top}/T\\|_{\\mu}^{2}=K_{Q}$ , and $\\left\\langle{\\bf1}_{T}{\\bf1}_{N}^{\\top}/T,h_{A}\\right\\rangle_{\\mu}=0.$ Hence, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\|\\mathbf{A}_{A,\\tau+1}\\right\\|_{\\mu}^{2}=\\left\\|\\left(1-\\eta_{A}K p\\alpha_{V}^{2}-\\eta_{A}\\left\\|\\mathbf{A}\\gamma\\right\\|_{\\mu}^{2}\\right)\\Delta_{A}+\\frac{\\eta_{A}}{K_{Q}}\\left\\langle h_{A,\\tau},Q\\right\\rangle_{\\mu}\\left(Q-\\frac{\\mathbf{1}T\\mathbf{1}_{N}^{2}}{T}\\right)-\\eta_{A}h_{A}\\right\\|_{\\mu}^{2}}&{}\\\\ {=\\left(1-\\eta_{A}K p\\alpha_{V}^{2}-\\eta_{A}\\left\\|\\mathbf{1}\\Delta\\gamma\\right\\|_{\\mu}^{2}\\right)^{2}\\left\\|\\mathbf{A}_{\\mathbf{A}}\\right\\|_{\\mu}^{2}+\\left(\\frac{\\eta_{A}}{K_{Q}}\\left\\langle h_{A,\\tau},Q\\right\\rangle_{\\mu}\\right)^{2}K_{Q}+\\eta_{A}^{2}\\left\\|h_{A}\\right\\|_{\\mu}^{2}}&{}\\\\ {-2\\eta_{A}\\left(1-\\eta_{A}K p\\alpha_{V}^{2}-\\eta_{A}\\left\\|\\mathbf{1}\\Delta\\gamma\\right\\|_{\\mu}^{2}\\right)\\left(\\Delta_{A},h_{A}\\right)_{\\mu}}&{}\\\\ {-2\\frac{\\eta_{A}^{2}}{K_{Q}}\\left\\langle h_{A,\\tau},Q\\right\\rangle_{\\mu}\\left\\langle Q,h_{A}\\right\\rangle_{\\mu}}&{}\\\\ {=\\left(1-\\eta_{A}K p\\alpha_{V}^{2}-\\eta_{A}\\left\\|\\mathbf{1}\\Delta\\gamma\\right\\|_{\\mu}^{2}\\right)^{2}\\left\\|\\mathbf{A}_{\\mathbf{A}}\\right\\|_{\\mu}^{2}-2\\eta_{A}\\left(1-\\eta_{A}K p\\alpha_{V}^{2}-\\eta_{A}\\left\\|\\mathbf{1}\\Delta\\gamma\\right\\|_{\\mu}^{2}\\right)\\left\\langle\\mathbf{A}_{A},h_{A}\\right\\rangle_{\\mu}}&{}\\\\ {+\\eta_{A}^{2}\\left\\|h_{A}\\right\\|_{\\mu}^{2}-\\frac{\\eta_{A}^{2}}{K_{Q}}\\left\\langle h_{A,\\tau},Q\\right\\rangle_{\\mu}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Now, consider $\\Delta_{V}$ . Similar to the previous calculation, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta_{V,\\tau+1}=\\Delta_{V}-\\eta_{V}\\left(\\alpha_{A}^{2}K_{Q}+\\displaystyle\\frac{1}{T}\\right)\\Delta_{V}-\\eta_{V}\\left\\|\\Delta_{A}\\right\\|^{2}\\Delta_{V}-\\eta_{V}h_{V}}\\\\ &{\\qquad\\qquad\\quad-\\eta_{V}\\frac{\\alpha_{V}-1}{T}\\left(P-\\mu^{\\mathrm{I}\\top}\\right)-\\eta_{V}\\left\\|\\Delta_{A}\\right\\|^{2}\\left(\\tilde{V}-\\mu^{\\mathrm{I}\\top}\\right)}\\\\ &{\\qquad\\qquad\\quad-\\left(\\eta_{V}\\displaystyle\\frac{1-\\alpha_{V}}{T}-\\eta_{V}\\alpha_{V}\\,\\|\\Delta_{A}\\|^{2}-\\displaystyle\\frac{\\eta_{V}}{K_{P}}\\left<h_{V,\\tau},P\\right>_{\\mu}\\right)\\left(P-\\mu^{\\mathrm{I}\\top}\\right)}\\\\ &{\\qquad=\\left(1-\\eta_{V}\\left(\\alpha_{A}^{2}K_{Q}+\\displaystyle\\frac{1}{T}\\right)-\\eta_{V}\\left\\|\\Delta_{A}\\right\\|^{2}\\right)\\Delta_{V}}\\\\ &{\\qquad\\qquad\\quad+\\displaystyle\\frac{\\eta_{V}}{K_{P}}\\left<h_{V,\\tau},P\\right>_{\\mu}\\left(P-\\mu^{\\mathrm{I}\\top}\\right)-\\eta_{V}h_{V}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Again, note that $\\Delta_{V}\\ \\bot_{\\mu}\\,P-\\mu\\mathbf{1}^{\\top},\\,\\|P-\\mu\\mathbf{1}^{\\top}\\|_{\\mu}^{2}=K_{P}$ , and $\\langle\\pmb{\\mu}\\mathbf{1}^{\\top},\\pmb{h}_{V}\\rangle_{\\mu}=0$ . Hence, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\left\\|\\Delta_{V,\\tau+1}\\right\\|_{\\mu}^{2}=\\left(1-\\eta_{V}\\left(\\alpha_{A}^{2}K_{Q}+\\frac1T\\right)-\\eta_{V}\\left\\|\\Delta_{A}\\right\\|^{2}\\right)^{2}\\left\\|\\Delta_{V}\\right\\|_{\\mu}^{2}}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad+\\,2\\eta_{V}\\left(1-\\eta_{V}\\left(\\alpha_{A}^{2}K_{Q}+\\frac1T\\right)-\\eta_{V}\\left\\|\\Delta_{A}\\right\\|^{2}\\right)\\left\\langle\\Delta_{V},h_{V}\\right\\rangle_{\\mu}}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad-\\,\\frac{\\eta_{V}^{2}}{K_{P}}\\left\\langle P,h_{V}\\right\\rangle_{\\mu}^{2}+\\eta_{V}^{2}\\left\\|h_{V}\\right\\|_{\\mu}^{2}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "D Stage 1: signal boosting ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section, we assume both $\\alpha_{V}$ and $\\alpha_{A}$ are close to O. In this case, we can approximate Lemma C.5 With ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha_{V,\\tau+1}\\approx\\alpha_{V,\\tau}+\\eta\\alpha_{A}+\\eta\\frac{1}{T K_{Q}}-\\eta\\alpha_{V}\\frac{\\|\\mathbf{\\DeltaA}_{A}\\|_{\\mu}^{2}}{K_{Q}}-\\eta\\frac{\\langle h_{V},P\\rangle_{\\mu}}{K_{Q}K_{P}},}\\\\ &{\\alpha_{A,\\tau+1}\\approx\\alpha_{A,\\tau}+\\eta\\alpha_{V}-\\eta\\alpha_{A}\\frac{\\|\\mathbf{\\DeltaA}_{V}\\|_{\\mu}^{2}}{K_{P}}-\\eta\\frac{\\langle h_{A},Q\\rangle_{\\mu}}{K_{P}K_{Q}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We can also write this matrix form as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left[\\!\\!\\begin{array}{c c}{\\alpha_{V,\\tau+1}}\\\\ {\\alpha_{A,\\tau+1}}\\end{array}\\!\\!\\right]\\approx\\left[\\!\\!\\begin{array}{c c}{\\alpha_{V,\\tau}}\\\\ {\\alpha_{A,\\tau}}\\end{array}\\!\\!\\right]+\\eta\\left[\\!\\!\\begin{array}{c c}{-\\left\\Vert\\Delta_{A}\\right\\Vert_{\\mu}^{2}/K_{Q}}&{1}\\\\ {1}&{-\\left\\Vert\\Delta_{V}\\right\\Vert_{\\mu}^{2}/K_{P}\\right]\\left[\\!\\!\\begin{array}{c c}{\\alpha_{V,\\tau}}\\\\ {\\alpha_{A,\\tau}}\\end{array}\\!\\!\\right]\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbf{\\Sigma}+\\eta\\left[\\begin{array}{c}{1/(T K_{Q})}\\\\ {0}\\end{array}\\right]-\\eta\\left[\\alpha_{A}\\left\\|\\mathbf{A}_{V}\\right\\|_{\\mu}^{2}/(K_{P}K_{Q})\\right]\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Suppose that $\\|\\mathbf{A}_{A}\\|_{\\mu}^{2}\\,/K_{Q}$ and $\\|\\Delta_{V}\\|_{\\mu}^{2}\\,/K_{P}$ are both bounded by $\\delta^{2}$ . Then, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha_{V,\\tau+1}+\\alpha_{A,\\tau+1}\\gtrsim\\left(1+\\eta-2\\delta^{2}\\eta\\right)\\left(\\alpha_{V,\\tau}+\\alpha_{A,\\tau}\\right)+\\eta\\frac{1}{T K_{Q}}}\\\\ &{\\qquad\\qquad\\qquad-\\eta\\frac{\\langle h_{V},P\\rangle_{\\mu}}{K_{Q}K_{P}}-\\eta\\frac{\\langle h_{A},Q\\rangle_{\\mu}}{K_{P}K_{Q}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Aslongas $\\delta\\ll1$ and we choose a sufficiently large batch size so that the second line is bounded by $\\eta/(2T K_{Q})$ $\\alpha_{V}+\\alpha_{A}$ grows exponentially fast. Similarly, one can also bound the difference between $\\alpha_{V}$ and $\\alpha_{A}$ . Formally, we have the following lemma. ", "page_idx": 26}, {"type": "text", "text": "Lemma D.1 (Main result of Stage 1). Define the end of Stage 1 as ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathcal{T}_{1}:=\\operatorname*{min}\\left\\{\\tau\\geq0:\\operatorname*{max}\\{\\alpha_{V,\\tau},\\alpha_{A,\\tau}\\}\\geq\\operatorname*{min}\\left\\{\\frac{1}{2},\\frac{\\Theta(1)}{Q N}\\right\\}\\right\\}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Suppose $\\eta\\leq1/10,\\eta_{V,\\tau}=\\eta/K_{Q}$ and $\\eta_{A,\\tau}=\\eta/K_{P}$ for some $\\eta\\leq\\operatorname*{min}\\{K_{P},K_{Q}\\}$ . Let $B_{\\tau}>0$ be the number fresh samples we use at step $\\tau$ .Suppose that $\\begin{array}{r}{B_{\\tau}\\,\\geq\\,\\tilde{O}\\left(\\frac{T^{2}Q^{4}N^{5}}{\\operatorname*{min}\\{K_{P}^{3},K_{Q}^{3}\\}}\\right)}\\end{array}$ are chosen s.t.with probability $1-\\delta_{\\tau}$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{max}\\left\\{\\left|\\frac{\\left\\langle h_{V},P\\right\\rangle_{\\mu}}{K_{Q}K_{P}}\\right|,\\left|\\frac{\\left\\langle h_{A},Q\\right\\rangle_{\\mu}}{K_{P}K_{Q}}\\right|\\right\\}\\leq\\frac{1}{4T K_{Q}},}\\\\ {\\operatorname*{max}\\left\\{\\left\\|h_{V,\\tau}\\right\\|_{\\mu},\\left\\|h_{A,\\tau}\\right\\|_{\\mu}\\right\\}\\leq\\frac{\\Theta(1)\\operatorname*{min}\\left\\{K_{Q}^{1/2},K_{P}^{1/2},\\frac{1}{\\sqrt{Q N^{1/2}}}\\right\\}\\operatorname*{min}\\{K_{Q},K_{P}\\}}{T\\log T K_{Q}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Then, the following hold with probability at least $1-\\delta_{P}$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\Delta:=\\operatorname*{min}\\left\\{\\frac{1}{4}K_{Q},\\frac{1}{4}K_{P},\\frac{\\Theta(1)}{Q^{4}N^{3}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The proof of this lemma is a large induction argument. We will first assume the bounds on $\\|\\Delta_{A}\\|_{\\mu}$ and $\\|\\Delta_{V}\\|_{\\mu}$ are true, so that the approximation (11) is valid. This will give us an upper bound on the length of Stage 1. Then, we show that within this many steps, the errors cannot exceed the given maximum values. Thus, the induction hypotheses are true and Lemma D.1 can be established. ", "page_idx": 26}, {"type": "text", "text": "Part I of the proof of Lemma D.1: Signal growth rate ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Proof.Since $\\begin{array}{r}{\\alpha_{V}\\alpha_{A}\\leq\\frac{1}{4}}\\end{array}$ by definition of Stage I, we can rewrite Lemma C.5 as ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha_{V,\\tau+1}+\\alpha_{A,\\tau+1}\\geq\\left(1+3\\eta/4\\right)\\left(\\alpha_{V,\\tau}+\\alpha_{A,\\tau}\\right)+\\eta\\frac{3}{4T K_{Q}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad-\\eta\\alpha_{V}\\frac{\\left\\Vert\\Delta_{A}\\right\\Vert_{\\mu}^{2}}{K_{Q}}-\\eta\\alpha_{A}\\frac{\\left\\Vert\\Delta_{V}\\right\\Vert_{\\mu}^{2}}{K_{P}}-\\eta\\frac{\\left\\langle h_{V},P\\right\\rangle_{\\mu}}{K_{Q}K_{P}}-\\eta\\frac{\\left\\langle h_{A},Q\\right\\rangle_{\\mu}}{K_{P}K_{Q}}}\\\\ &{\\qquad\\qquad\\qquad\\geq\\left(1+3\\eta/4\\right)\\left(\\alpha_{V,\\tau}+\\alpha_{A,\\tau}\\right)+\\eta\\frac{3}{4T K_{Q}}-\\frac{1}{4}\\eta\\alpha_{V}-\\frac{1}{4}\\eta\\alpha_{A}-\\frac{1}{4T K_{Q}}\\times2\\frac{\\left\\Vert\\Delta_{V}\\right\\Vert_{\\mu}}{2T K_{Q}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\geq\\left(1+\\frac{\\eta}{2}\\right)\\left(\\alpha_{V,\\tau}+\\alpha_{A,\\tau}\\right)+\\eta\\frac{1}{4T K_{Q}},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the second line comes from induction hypothesis (b) and (12). Recursively expand the RHS, andweobtain ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\alpha_{V,\\tau}+\\alpha_{A,\\tau}\\geq\\left(1+\\frac{\\eta}{2}\\right)^{\\tau}\\left(\\alpha_{V,0}+\\alpha_{A,0}+\\frac{1}{2T K_{Q}}\\right)-\\frac{1}{2T K_{Q}}=\\left(\\left(1+\\frac{\\eta}{2}\\right)^{\\tau}-1\\right)\\frac{1}{2T K_{Q}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Since the RHS is upper bounded by 1 by definition of stage I, and we obtain ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal{T}_{1}\\leq\\Theta(1)\\frac{\\log\\left(T K_{Q}\\right)}{\\eta}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Part II of the proof of Lemma D.1: Upper bounds on $\\left\\|\\mathbf{A}\\right\\|_{\\mu}$ ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Proof. Recall from Lemma C.6 that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\Delta_{V,\\tau+1}\\right\\|_{\\mu}^{2}\\leq\\left\\|\\Delta_{V,\\tau}\\right\\|_{\\mu}^{2}+2\\eta_{V}\\left\\|\\Delta_{V,\\tau}\\right\\|_{\\mu}\\left\\|h_{V,\\tau}\\right\\|_{\\mu}+\\eta_{V}^{2}\\left\\|h_{V,\\tau}\\right\\|_{\\mu}^{2},}\\\\ &{\\left\\|\\Delta_{A,\\tau+1}\\right\\|_{\\mu}^{2}\\leq\\left\\|\\Delta_{A,\\tau}\\right\\|_{\\mu}^{2}+2\\eta_{A}\\left\\|\\Delta_{A,\\tau}\\right\\|_{\\mu}\\left\\|h_{A,\\tau}\\right\\|_{\\mu}+\\eta_{A}^{2}\\left\\|h_{A,\\tau}\\right\\|_{\\mu}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "By part I of the proof, Stage 1 takes at most $\\Theta(1)\\log\\bigl(T K_{Q}\\bigr)/\\eta$ steps. Hence, it suffices to bound theinerease o these $\\|\\mathbf{A}\\|_{\\mu}^{2}$ $\\begin{array}{r}{\\Delta=\\operatorname*{min}\\left\\{\\frac{1}{4}K_{Q},\\frac{1}{4}K_{P},\\frac{\\Theta(1)}{Q N^{1/2}}\\right\\}}\\end{array}$ By induction hypothesis (b), we have for all $\\tau$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\Delta\\boldsymbol{V}_{\\tau:\\tau+}\\right\\|_{\\mu}^{2}\\leq\\left\\|\\Delta\\boldsymbol{V}_{\\tau}\\right\\|_{\\mu}^{2}+2\\eta_{V}\\left\\|\\Delta\\boldsymbol{V}_{\\tau}\\right\\|_{\\mu}\\left\\|\\boldsymbol{h}_{V,\\tau}\\right\\|_{\\mu}+\\eta_{V}^{2}\\left\\|\\boldsymbol{h}_{V,\\tau}\\right\\|_{\\mu}^{2}}\\\\ &{\\leq\\left\\|\\Delta\\boldsymbol{V}_{\\tau}\\right\\|_{\\mu}^{2}+2\\eta_{V}\\sqrt{\\Delta/T}\\left\\|\\boldsymbol{h}_{V,\\tau}\\right\\|_{\\mu}+\\eta_{V}^{2}\\left\\|\\boldsymbol{h}_{V,\\tau}\\right\\|_{\\mu}^{2}}\\\\ &{\\qquad\\qquad\\leq\\left\\|\\Delta\\boldsymbol{V}_{\\tau}\\right\\|_{\\mu}^{2}+\\frac{\\Theta(1)\\eta\\sqrt{\\Delta/T}}{K_{\\rho}}\\cdot\\frac{\\sqrt{\\Delta/T}\\operatorname*{min}\\left\\{\\boldsymbol{K}_{P},K_{Q}\\right\\}}{\\log\\left(T K_{Q}\\right)}+\\eta^{2}\\frac{\\Theta(1)\\Delta\\cdot\\operatorname*{min}\\left\\{\\boldsymbol{K}_{P}^{2},K_{Q}^{2}\\right\\}}{T K_{Q}^{2}\\log\\left(T K_{Q}\\right)^{2}}}\\\\ &{\\left\\|\\Delta_{A,\\tau+}\\right\\|_{\\mu}^{2}\\leq\\left\\|\\Delta_{A,\\tau}\\right\\|_{\\mu}^{2}+2\\eta_{A}\\left\\|\\Delta_{A,\\tau}\\right\\|_{\\mu}\\left\\|\\boldsymbol{h}_{A,\\tau}\\right\\|_{\\mu}+\\eta_{A}^{2}\\left\\|\\boldsymbol{h}_{A,\\tau}\\right\\|_{\\mu}^{2}}\\\\ &{\\qquad\\qquad\\leq\\left\\|\\Delta_{A,\\tau}\\right\\|_{\\mu}^{2}+2\\eta_{A}\\sqrt{\\Delta/T}\\left\\|\\boldsymbol{h}_{A,\\tau}\\right\\|_{\\mu}+\\eta_{A}^{2}\\left\\|\\boldsymbol{h}_{A,\\tau}\\right\\|_{\\mu}^{2}}\\\\ &{\\qquad\\qquad\\leq\\left\\|\\Delta_{A,\\tau}\\right\\|_{\\mu}^{2}+\\frac{\\Theta(1)\\eta\\sqrt{\\Delta/T}}{K_{\\rho}}\\cdot\\frac{\\sqrt{\\Delta/T}\\operatorname*{min}\\left\\{\\boldsymbol{K}_{P},K_{Q}\\right\\}}{\\log\\left(T K_{Q}\\right)}+\\eta^{2}\\frac{\\Theta(1)\\Delta\\cdot\\operatorname*{min}\\left\\{\\boldsymbol{K}_{P}^{2 \n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Since Stage I at most takes $\\mathcal{T}_{1}=\\Theta(1)\\log\\bigl(T K_{Q}\\bigr)/\\eta$ steps, the increase of $\\big\\|\\Delta_{A,\\tau}\\big\\|_{\\mu}^{2}$ and $\\big\\|\\Delta_{A,\\tau}\\big\\|_{\\mu}^{2}$ in $\\tau\\leq\\mathcal{T}_{1}$ are at most (since $\\Delta_{A,0}=0,\\Delta_{V,0}=0)$ \uff1a ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\Delta_{V,\\,r}\\right\\|_{\\mu}^{2}\\leq\\left(\\frac{\\Theta(1)\\eta\\sqrt{\\Delta/T}}{K_{Q}}\\cdot\\frac{\\sqrt{\\Delta/T}\\operatorname*{min}\\left\\{K_{P},K_{Q}\\right\\}}{\\log(T K_{Q})}+\\eta^{2}\\frac{\\Theta(1)\\Delta\\cdot\\operatorname*{min}\\left\\{K_{P}^{2},K_{Q}^{2}\\right\\}}{T K_{Q}^{2}\\log(T K_{Q})}\\right)t}\\\\ &{\\qquad\\qquad\\leq\\left(\\frac{\\Theta(1)\\eta\\sqrt{\\Delta/T}}{K_{Q}}\\cdot\\frac{\\sqrt{\\Delta/T}\\operatorname*{min}\\left\\{K_{P},K_{Q}\\right\\}}{\\log(T K_{Q})^{2}}+\\eta^{2}\\frac{\\Theta(1)\\Delta\\cdot\\operatorname*{min}\\left\\{K_{P}^{2},K_{Q}^{2}\\right\\}}{T K_{Q}^{2}\\log(T K_{Q})^{2}}\\right)\\mathcal{T}_{1}\\leq\\Delta/T,}\\\\ &{\\left\\|\\Delta_{A,\\,r}\\right\\|_{\\mu}^{2}\\leq\\left(\\frac{\\Theta(1)\\eta\\sqrt{\\Delta/T}}{K_{P}}\\cdot\\frac{\\sqrt{\\Delta/T}\\operatorname*{min}\\left\\{K_{P},K_{Q}\\right\\}}{\\log(T K_{Q})}+\\eta^{2}\\frac{\\Theta(1)\\Delta\\cdot\\operatorname*{min}\\left\\{K_{P}^{2},K_{Q}^{2}\\right\\}}{T K_{P}^{2}\\log(T K_{Q})^{2}}\\right)t}\\\\ &{\\qquad\\qquad\\leq\\left(\\frac{\\Theta(1)\\eta\\sqrt{\\Delta/T}}{K_{P}}\\cdot\\frac{\\sqrt{\\Delta/T}\\operatorname*{min}\\left\\{K_{P},K_{Q}\\right\\}}{\\log(T K_{Q})}+\\eta^{2}\\frac{\\Theta(1)\\Delta\\cdot\\operatorname*{min}\\left\\{K_{P}^{2},K_{Q}^{2}\\right\\}}{T K_{P}^{2}\\log(T K_{Q})^{2}}\\right)\\mathcal{T}_{1}\\leq\\Delta/T.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Therefore, we completed the induction for the error terms. ", "page_idx": 27}, {"type": "text", "text": "Part IlI of the proof of Lemma D.1: Ending state ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Proof. First, consider the distance between $\\alpha_{V}$ and $\\alpha_{A}$ . Similar to the part I of the proof, we rewrite Lemma C.5 as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha_{V,\\tau+1}-\\alpha_{A,\\tau+1}=-\\left(1-\\eta\\left(1-\\alpha_{V}\\alpha_{A}\\right)\\right)\\left(\\alpha_{V,\\tau}-\\alpha_{A,\\tau}\\right)+\\eta\\frac{1-\\alpha_{V}}{T K_{Q}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad-\\left.\\eta\\alpha_{V}\\frac{\\left\\Vert\\mathbf{A}_{A}\\right\\Vert_{\\mu}^{2}}{K_{Q}}+\\eta\\alpha_{A}\\frac{\\left\\Vert\\mathbf{A}_{V}\\right\\Vert_{\\mu}^{2}}{K_{P}}-\\eta\\frac{\\left\\langle h_{V},P\\right\\rangle_{\\mu}}{K_{Q}K_{P}}+\\eta\\frac{\\left\\langle h_{A},Q\\right\\rangle_{\\mu}}{K_{P}K_{Q}}}\\\\ &{\\qquad=-\\left(1-\\eta/2\\right)\\left(\\alpha_{V,\\tau}-\\alpha_{A,\\tau}\\right)\\pm O\\left(\\eta\\frac{1}{T K_{Q}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Thus, whenever $\\alpha_{V,\\tau}-\\alpha_{A,\\tau}\\geq\\Omega(1/(T K_{Q}))$ , it will start to decrease. Since the amount of increase at each step is also upper bounded by $O(1/(T K_{Q}))$ , this implies $|\\alpha_{V,\\tau}-\\alpha_{A,\\tau}|\\leq O(1/(T K_{Q}))$ .The other direction can be proved in the same way. ", "page_idx": 28}, {"type": "text", "text": "Finally, we bound the possible amount of overshot. By the part I of the proof, we can also upper bound the signal term growth ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\alpha_{V,\\tau+1}+\\alpha_{V,\\tau+1}\\leq\\left(1+2\\eta\\right)\\left(\\alpha_{V,\\tau}+\\alpha_{A,\\tau}\\right)+O\\left(\\frac{\\eta}{T K_{Q}}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Since $\\alpha_{V,\\tau}+\\alpha_{A,\\tau}\\leq1$ for all $\\tau\\leq\\mathcal{T}_{1}$ wehave $\\begin{array}{r}{\\alpha_{V}+\\alpha_{A}=\\Theta(\\frac{1}{\\mathcal{Q}N})}\\end{array}$ at time $\\mathcal{T}_{1}$ ", "page_idx": 28}, {"type": "text", "text": "Part IV of the proof of Lemma D.1: Upper bound on Infinity norm of $V$ and $\\boldsymbol{A}$ ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Here we consider the upper bound of the weights $V$ and $\\boldsymbol{A}$ , which can be used in the concentration section below. ", "page_idx": 28}, {"type": "text", "text": "Proof. First, we upper bound the infinity norm of $\\textstyle V_{\\tau}$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left\\lVert V_{\\tau}\\right\\rVert_{\\infty}=\\left\\lVert\\tilde{V}_{\\tau}+\\Delta_{V}\\right\\rVert_{\\infty}\\leq\\left\\lVert\\tilde{V}_{\\tau}\\right\\rVert_{\\infty}+\\left\\lVert\\Delta_{V}\\right\\rVert_{\\infty}\\leq\\left\\lVert\\tilde{V}_{\\tau}\\right\\rVert_{\\infty}+\\left\\lVert\\Delta_{V}\\right\\rVert_{F}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and we can upper bound $\\|\\Delta_{V}\\|_{F}$ by its $\\mu$ -norm: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|\\Delta_{V}\\|_{\\mu}^{2}=\\langle\\Delta_{V},\\Delta_{V}\\mathrm{diag}(\\mu)\\rangle\\geq{\\frac{c}{N}}\\,\\|\\Delta_{V}\\|_{F}^{2}\\,.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Thus we have $\\begin{array}{r}{\\|V_{\\tau}\\|_{\\infty}\\leq\\left\\|\\tilde{V}_{\\tau}\\right\\|_{\\infty}+\\Theta(1)\\sqrt{N}\\left\\|\\Delta_{V}\\right\\|_{\\mu}\\leq\\left\\|\\tilde{V}_{\\tau}\\right\\|_{\\infty}+\\Theta(\\frac{1}{Q^{2}N})}\\end{array}$ by Induction hypothesis (b). And we can further bound $\\left\\|\\tilde{V}_{\\tau}\\right\\|_{\\infty}$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left\\|\\tilde{V}_{\\tau}\\right\\|_{\\infty}=\\left\\|\\alpha_{V,\\tau}\\pmb{P}+(1-\\alpha_{V,\\tau})\\mu\\mathbf{1}^{\\top}\\right\\|_{\\infty}\\leq\\|\\pmb{P}\\|_{\\infty}+\\|\\mu\\mathbf{1}^{\\top}\\|_{\\infty}\\leq\\Theta(1).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Therefore, we have $\\|V_{\\tau}\\|_{\\infty}\\leq\\Theta(1)$ ", "page_idx": 28}, {"type": "text", "text": "Similarly, for $\\boldsymbol{A}$ we have (since $\\left\\|\\tilde{A}_{\\tau}\\right\\|_{\\infty}=\\left\\|\\mathbf{1}_{T}\\mathbf{1}_{N}^{\\top}/T+\\alpha_{A,\\tau}(Q-\\mathbf{1}_{T}\\mathbf{1}_{N}^{\\top}/T)\\right\\|_{\\infty}\\leq C/Q.)$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|A_{\\tau}\\|_{\\infty}=\\|\\tilde{A}_{\\tau}+\\Delta_{A}\\|_{\\infty}\\leq\\left\\|\\tilde{A}_{\\tau}\\right\\|_{\\infty}+\\left\\|\\Delta_{A}\\right\\|_{\\infty}\\leq\\left\\|\\tilde{A}_{\\tau}\\right\\|_{\\infty}+\\left\\|\\Delta_{A}\\right\\|_{F}\\leq\\Theta(1/Q).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Part $\\mathbf{V}$ of the proof of Lemma D.1: Concentration ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Finally, we need to ensure that with high probability, all the error terms $\\pmb{h}$ cannot exceed the given bounds throughout $t\\leq\\mathcal{T}_{1}$ . We use Lemma B.12 and Lemma B.13 to bound the concentration of the errorterms. ", "page_idx": 28}, {"type": "text", "text": "By Lemma B.13 and union bound, we have that if $\\begin{array}{r}{B_{\\tau}\\ \\geq\\ \\frac{\\Theta(1)\\mathcal{T}_{\\mathrm{l}}\\cdot T^{2}Q^{4}N^{5}\\mathcal{T}_{\\mathrm{l}}\\log^{2}(T K_{Q})}{\\delta_{\\tau}\\operatorname*{min}\\{K_{P}^{3},K_{Q}^{3},1\\}}}\\end{array}$ @(1)T:T2g\\*NsT og(TKo), then with probability at least $1-\\delta_{\\tau}/2$ , the following holds for all $t\\leq\\mathcal{T}_{1}$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\left\\{\\left\\|h_{V,\\tau}\\right\\|_{\\mu},\\left\\|h_{A,\\tau}\\right\\|_{\\mu}\\right\\}\\leq\\frac{\\Theta(1)\\operatorname*{min}\\left\\{K_{Q}^{1/2},K_{P}^{1/2},\\frac{1}{Q^{2}N^{3/2}}\\right\\}\\operatorname*{min}\\{K_{Q},K_{P}\\}}{\\sqrt{T}\\log T K_{Q}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By Lemma B.12 and union bound, we have that if $\\begin{array}{r}{B_{\\tau}\\geq\\frac{\\Theta(1)\\mathcal{T}_{1}\\cdot T^{2}\\operatorname*{max}\\{N^{2},Q^{2}\\}N^{4}\\log\\left(\\frac{16\\log\\left(T K_{Q}\\right)}{\\delta\\tau\\eta}\\right)}{K_{P}^{2}}}\\end{array}$ then with probability at least $1-\\delta_{\\tau}/2$ , the following holds for all $t\\leq\\mathcal{T}_{1}$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left|\\frac{\\langle h_{V},P\\rangle_{\\mu}}{K_{P}K_{Q}}\\right|\\leq\\frac{1}{4T K_{Q}},\\ \\left|\\frac{\\langle h_{A},Q\\rangle_{\\mu}}{K_{P}K_{Q}}\\right|\\leq\\frac{1}{4T K_{Q}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "And by union bound, we have that with probability at least $1-\\delta_{\\tau}$ , all the bounds above hold for all $t\\leq\\mathcal{T}_{1}$ . Therefore, we conclude the proof of Lemma D.1. ", "page_idx": 29}, {"type": "text", "text": "E  Stage 2: learning the model ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "In this Stage 2, we use a positive $\\lambda$ for the $\\ell_{1}$ -regularization. In this case, we can write the update rule of each $\\pmb{a}^{(\\bar{k})}$ as ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{\\displaystyle a_{\\tau+1}^{(k,\\prime)}=a_{\\tau}^{(k)}-\\frac{\\eta_{A}}{\\mu_{k}}\\nabla_{a^{(k)}}^{(B_{\\tau})}l,}}&{{\\mathrm{(gradient\\;descent\\;step),}}}\\\\ {{\\displaystyle a_{\\tau+1,t}^{(k,\\prime)}=\\left\\{a_{\\tau+1,t}^{(k,\\prime)}-\\lambda,\\quad\\mathrm{if}\\;a_{\\tau+1,t}^{(k,\\prime)}\\geq\\lambda,\\right.}}\\\\ {{\\displaystyle0,}}&{{\\mathrm{if}\\;\\left|a_{\\tau+1,t}^{(k,\\prime)}\\right|\\leq\\lambda,}}\\\\ {{\\displaystyle a_{\\tau+1}^{(k)}=a_{\\tau+1}^{(k,\\prime)}+\\left(1-1^{\\top}a_{\\tau+1}^{(k,\\prime^{\\prime})}\\right)\\frac{1}{T},}}&{{\\mathrm{(projection\\;step).}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "For notational simplicity, we define ", "page_idx": 30}, {"type": "equation", "text": "$$\ng_{\\tau}^{(k)}:=-\\eta_{\\tau}^{-1}(\\pmb{a}_{\\tau+1}^{(k)}-\\pmb{a}_{\\tau}^{(k)}).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We further define $\\pmb{G}_{\\lambda,\\tau}=-\\eta_{A}^{-1}(\\pmb{A}_{\\tau+1}-\\pmb{A}_{\\tau})$ as the full gradient of the matrix $\\boldsymbol{A}$ ", "page_idx": 30}, {"type": "text", "text": "First, we will show that with appropriate rounding at the beginning of Stage 2, we can ensure $\\pmb{g}_{\\tau}^{(k)}\\approx\\eta\\mathbb{E}\\hat{\\nabla}_{\\pmb{a}^{(k)}}l$ usingoly $B_{\\tau}\\propto\\log(T)$ fresh sample at eachstep Setion E.I). ", "page_idx": 30}, {"type": "text", "text": "E.1 Rounding and gradient denoising ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Recall that the first step of the Stage 2 is rounding each ${\\pmb a}^{(k)}$ by setting all small coordinates to O and then projecting it back to the affine space the probability simplex lies. Since after Stage 1, there wil be a separaion between $a_{t}^{(k)}$ with $t\\in{\\pmb q}^{(k)}$ and $t\\not\\in{\\pmb q}^{(k)}$ thisrounding step make all $a_{t}^{(k)}$ with $t\\not\\in{\\pmb q}^{(k)}$ have the same small value. ", "page_idx": 30}, {"type": "text", "text": "Also recall that we use $\\ell_{1}$ -regularization and proximal gradients in the Stage 2. Effectively, the $\\ell_{1}$ -regularization ensures those useless $a_{t}^{(k)}$ are always 0 (before projection). Then, similar to the first rounding step, projection will again make then have the same small value. ", "page_idx": 30}, {"type": "text", "text": "In this subsection, we formalize the above argument. We show that the rounding step can recover the support of $\\pmb q^{(k)}$ , analyze its inffuence on $\\alpha_{A}$ and the distance to the population subspace. Then, we analyze the effect of use of the proximal gradients and show that with po $\\mathsf{I y}(N,Q)\\log(T)$ fresh samples at each step, we can make sure the difference between update and the population update is small with high probability. ", "page_idx": 30}, {"type": "text", "text": "Lemma E.1 (Separation between noise and signal). Assume that at the beginning of Stage 2, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\alpha_{V}\\geq\\Theta\\left(\\frac{Q}{T}+Q\\sqrt{N}\\left\\|\\Delta_{A}\\right\\|_{\\mu}\\right).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Then, we can choose a threshold $\\theta_{\\mathrm{Tmp}}=\\Theta(\\alpha_{V}/Q)$ s. $a_{t}^{(k)}\\leq\\theta_{\\mathrm{Tmp}}$ $q_{t}^{(k)}=0.$ ", "page_idx": 30}, {"type": "text", "text": "Proof. Note that there exists some universal constant $c>0$ such that $\\pmb{q}_{t}^{(k)}\\geq c/Q$ for all nonzero $q_{t}^{(k)}$ $\\mu_{k}\\geq c/N$ foral $k\\in[N]$ $\\tilde{A}=\\alpha_{A}Q+(1-\\alpha_{A})\\mathbf{1}_{T}\\mathbf{1}_{N}^{\\top}/T$ Hence, for all $k\\in[N]$ and $t\\in[T]$ \uff0c ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{a}_{t}^{(k)}\\leq1/T,\\;\\;\\mathrm{if}\\;t\\notin\\pmb{q}^{(k)},}\\\\ {\\tilde{a}_{t}^{(k)}\\geq c\\alpha_{V}/\\pmb{Q},\\;\\;\\mathrm{if}\\;t\\in\\pmb{q}^{(k)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Then, note that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\|\\Delta_{A}\\|_{\\mu}^{2}=\\sum_{t=1}^{T}\\sum_{k=1}^{N}\\Big(a_{t}^{(k)}-\\tilde{a}_{t}^{(k)}\\Big)^{2}\\,\\mu_{k}}}\\\\ &{}&{\\geq\\frac{C}{N}\\sum_{t=1}^{T}\\sum_{k=1}^{N}\\Big(a_{t}^{(k)}-\\tilde{a}_{t}^{(k)}\\Big)^{2}\\geq\\frac{C}{N}\\left\\|\\mathrm{Vec}(\\tilde{A}-A)\\right\\|_{2}^{2}\\geq\\frac{C}{N}\\left\\|\\mathrm{Vec}(\\tilde{A}-A)\\right\\|_{\\infty}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Hence, for any $k\\in[N]$ and $t\\in[T]$ , we have $|a_{t}^{(k)}-\\tilde{a}_{t}^{(k)}|\\leq\\|\\mathbf{\\DeltaA}\\|_{\\mu}\\,\\sqrt{N/c}$ Combine these together, and we obtain ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{a_{t}^{(k)}\\leq1/T+\\|\\Delta_{A}\\|_{\\mu}\\,\\sqrt{N/c},\\quad\\mathrm{if~}t\\not\\in\\pmb q^{(k)},}\\\\ {a_{t}^{(k)}\\geq c\\alpha_{V}/Q-\\|\\Delta_{A}\\|_{\\mu}\\,\\sqrt{N/c},\\quad\\mathrm{if~}t\\in\\pmb q^{(k)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Hence, in order to get a separation, ", "page_idx": 31}, {"type": "equation", "text": "$$\n1/T+\\|\\mathbf{A}_{A}\\|_{\\mu}\\,\\sqrt{N/c}\\leq\\frac{1}{2}\\left(c\\alpha_{V}/Q-\\|\\mathbf{A}_{A}\\|_{\\mu}\\,\\sqrt{N/c}\\right)\\quad\\Leftarrow\\quad\\frac{2Q}{c T}+\\frac{3Q\\sqrt{N}}{c^{1.5}}\\left\\|\\mathbf{A}_{A}\\right\\|_{\\mu}\\leq\\alpha_{V}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Lemma E.2 (Effect of rounding). Under the conditions of Lemma E.1, choose A as in Lemma E.1, andset ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{a}^{(k)}\\leftarrow\\left(\\pmb{I}-\\frac{\\lVert\\mathbf{1}^{\\top}\\rVert}{T}\\right)\\pmb{a}^{(k)}\\odot\\left(\\mathbb{1}\\{a_{t}^{(k)}\\geq\\lambda\\}\\right)_{t=1}^{T}+\\frac{1}{T}}\\\\ &{\\qquad=\\pmb{a}^{(k)}\\odot\\left(\\mathbb{1}\\{a_{t}^{(k)}\\geq\\lambda\\}\\right)_{t=1}^{T}+\\left(1-\\displaystyle\\sum_{t\\in\\pmb{q}^{(k)}}a_{t}^{(k)}\\right)\\frac{1}{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We have $\\alpha_{A}\\leftarrow\\alpha_{A}+O(1)/T$ and $\\|\\mathbf{A}_{A}\\|_{\\mu}^{2}\\leftarrow\\|\\mathbf{A}_{A}\\|_{\\mu}^{2}+O(1)/T.$ ", "page_idx": 31}, {"type": "text", "text": "Proof. For notational simplity, put $\\pmb{b}^{(k)}=\\pmb{a}^{(k)}\\odot(\\mathbb{1}\\{a_{t}^{(k)}\\geq\\lambda\\})_{t=1}^{T}$ By Lemma E.1, we know $\\pmb{b}^{(k)}$ is supported within q(k) Set b(k) =Z= bb). Then, we can write ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbf{\\sigma}^{\\left(k\\right)}\\leftarrow\\mathbf{\\}^{\\left(k\\right)}+\\left(1-b^{\\left(k\\right)}\\right)\\frac{\\mathbf{1}}{T}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Recall that $\\alpha_{A}=K_{A Q}/K_{Q}$ where $\\begin{array}{r}{K_{A Q}=\\langle\\pmb{A},\\pmb{Q}\\rangle_{\\mu}=\\sum_{k=1}^{N}\\mu_{k}\\left\\langle\\pmb{a}^{(k)},\\pmb{q}^{(k)}\\right\\rangle}\\end{array}$ . Hence, fo each $k\\in[N]$ \uff0c wehave ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\left<\\pmb{a}^{(k)},\\pmb{q}^{(k)}\\right>\\leftarrow\\left<\\pmb{b}^{(k)},\\pmb{q}^{(k)}\\right>+\\left(1-b^{(k)}\\right)\\left<\\frac{1}{T},\\pmb{q}^{(k)}\\right>}}\\\\ {{\\displaystyle\\leftarrow\\left<\\pmb{a}^{(k)},\\pmb{q}^{(k)}\\right>+\\left(1-b^{(k)}\\right)\\frac{1}{T}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "As a result, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\alpha_{A}\\gets\\alpha_{A}+\\frac{1}{T}\\sum_{k=1}^{N}\\mu_{k}\\left(1-b^{(k)}\\right)=\\alpha_{A}+\\frac{O(1)}{T}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Now, consider the effect of projection on the distance to the population subspace. We will use subscript new to indicate values after rounding and use notations such as ${\\pmb a}^{(k)}$ todenotethevalues before rounding. Recall that $\\begin{array}{r}{\\|\\Delta\\mathbf{\\boldsymbol{A}}\\|_{\\mu}^{2}=\\sum_{k=1}^{N}\\mu_{k}\\left\\|\\pmb{\\boldsymbol{a}}^{(k)}-\\tilde{\\pmb{\\boldsymbol{a}}}^{(k)}\\right\\|^{2}}\\end{array}$ Wehave ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\|{\\boldsymbol a}_{\\mathrm{new}}^{(k)}-{\\boldsymbol{\\tilde{a}}}_{\\mathrm{new}}^{(k)}\\right\\|^{2}=\\left\\|{\\boldsymbol b}^{(k)}+(1-{\\boldsymbol b}^{(k)})\\frac{1}{T}-\\frac{1}{T}-\\alpha_{A,\\mathrm{new}}\\left({\\boldsymbol q}^{(k)}-\\frac{1}{T}\\right)\\right\\|^{2}}\\\\ &{=\\left\\|{\\boldsymbol b}^{(k)}-\\alpha_{A,\\mathrm{new}}{\\boldsymbol q}^{(k)}-\\left({\\boldsymbol b}^{(k)}-\\alpha_{A,\\mathrm{new}}\\right)\\frac{1}{T}\\right\\|^{2}}\\\\ &{=\\left\\|{\\boldsymbol b}^{(k)}-\\alpha_{A,\\mathrm{new}}{\\boldsymbol q}^{(k)}\\right\\|^{2}+\\left({\\boldsymbol b}^{(k)}-\\alpha_{A,\\mathrm{new}}\\right)^{2}\\frac{1}{T}}\\\\ &{\\qquad-2\\left({\\boldsymbol b}^{(k)}-\\alpha_{A,\\mathrm{new}}\\right)\\left\\langle{\\boldsymbol b}^{(k)}-\\alpha_{A,\\mathrm{new}}{\\boldsymbol q}^{(k)},\\frac{1}{T}\\right\\rangle}\\\\ &{=\\left\\|{\\boldsymbol b}^{(k)}-(\\alpha_{A}+O(1)/T){\\boldsymbol q}^{(k)}\\right\\|^{2}-\\left({\\boldsymbol b}^{(k)}-\\alpha_{A,\\mathrm{new}}\\right)^{2}\\frac{1}{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Note that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\left\\|a^{(k)}-\\tilde{a}^{(k)}\\right\\|^{2}=\\left\\|a^{(k)}-\\alpha_{A}q^{(k)}-(1-\\alpha_{A})\\frac{1}{T}\\right\\|^{2}=\\left\\|a^{(k)}-\\alpha_{A}q^{(k)}\\right\\|^{2}-(1-\\alpha_{A})^{2}\\frac{1}{T}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "equation", "text": "$$\n\\geq\\left\\|\\pmb{b}^{(k)}-\\alpha_{A}\\pmb{q}^{(k)}\\right\\|^{2}-(1-\\alpha_{A})^{2}\\frac{1}{T}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Hence, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|a_{\\mathrm{new}}^{(k)}-\\tilde{a}_{\\mathrm{new}}^{(k)}\\right\\|^{2}-\\left\\|a^{(k)}-\\tilde{a}^{(k)}\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\left\\|b^{(k)}-\\alpha_{A}q^{(k)}-\\frac{O(1)}{T}q^{(k)}\\right\\|^{2}-\\left\\|b^{(k)}-\\alpha_{A}q^{(k)}\\right\\|^{2}+\\frac{O(1)}{T}\\leq\\frac{O(1)}{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Thus, $\\|\\mathbf{A}_{A}\\|_{\\mu}^{2}\\leftarrow\\|\\mathbf{A}_{A}\\|_{\\mu}^{2}+O(1)/T$ ", "page_idx": 32}, {"type": "text", "text": "As we have seen in Stage 1, the norm of $\\nabla_{\\pmb{a}^{(k)}}l$ can scale linearly with $T$ . Hence, in order to make $\\pmb{h}_{\\pmb{a}^{(k)},\\tau}$ has size $O(1)$ in terms of $\\left\\|\\cdot\\right\\|_{2}$ , it is necessary to use poly $(T)$ samples, which is undesirable. However, note that all entries of $\\nabla_{\\pmb{a}^{(k)}}l$ are bounded, and therefore are subgaussian. Hence, for each entry of $\\nabla_{\\pmb{a}^{(k)}}l$ , with ${\\cal O}(\\log{T})$ samples, we can make sure the relative error is small with probability at least $1-1/\\mathrm{poly}(T)$ . By union bound, this means the $\\|\\cdot\\|_{\\infty}$ error can be made small using only $\\log(T)$ samples. Note that there is a separation between the signal and noise parts of $\\mathbb{E}\\,\\nabla_{\\pmb{a}^{(k)}}\\,l$ . This implies that we can distinguish them using $\\log(T)$ samples and directly remove the noise part. Formally, we have the following lemma. ", "page_idx": 32}, {"type": "text", "text": "Lemma E.3 (Gradient denoising). Given $\\varepsilon\\in\\bigg(\\Theta\\bigg(\\frac{Q^{2}N^{3}}{\\sqrt{T}}\\bigg),0.1\\bigg)$ Supose that fo any $k,m,n\\in[N]$ $|\\pmb{a}_{t}^{(k)}|\\leq1/T\\,i f t\\notin\\pmb{q}^{(k)}$ $|V_{n,m}|\\leq O(1)$ and $\\lVert\\mathbf{A}_{A}\\rVert_{\\mu}\\leq c(1\\!-\\!\\alpha_{V})/(\\sqrt{N}Q)$ and $\\|\\Delta_{V}\\|_{\\mu}^{2}\\leq c(1\\!-\\!\\alpha_{V})K_{P}$ for somesmall constant $c>0$ For the targe accuracy $\\begin{array}{r}{\\pmb{\\varepsilon}_{\\mathrm{Tmp}}=\\Theta(\\frac{\\varepsilon}{Q N^{2}})}\\end{array}$ wehavewithprobability $1-\\delta_{\\tau}$ . If we choose ", "page_idx": 32}, {"type": "equation", "text": "$$\nB_{\\tau}\\geq C\\frac{N^{8}Q^{4}}{\\varepsilon^{2}\\alpha_{V}^{2}K_{P}^{2}}\\log\\left(\\frac{C N T}{\\delta_{\\tau}}\\right),\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "for some large constant $C>0$ Then, for each $k\\in[N]$ , with probability at least $1-\\delta_{\\mathrm{Tmp}}$ , we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\partial_{a_{t}^{(k)}}^{(B_{\\tau})}l=\\left(1\\pm\\varepsilon_{\\mathrm{Tmp}}\\right)\\mathbb{E}\\,\\partial_{a_{t}^{(k)}}l\\ge\\Theta\\left(\\frac{\\alpha_{V}K_{P}}{N Q}\\right)}\\\\ {\\displaystyle\\partial_{a_{t}^{(k)}}^{(B_{\\tau})}l={\\cal O}\\left(\\frac{1}{T}+\\varepsilon_{\\mathrm{Tmp}}\\frac{\\alpha_{V}K_{P}}{N Q}\\right)={\\cal O}\\left(\\frac{\\varepsilon K_{P}}{N^{3}Q^{2}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\forall t\\in\\pmb q^{(k)},}\\\\ {\\forall t\\notin\\pmb q^{(k)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof. For $s\\in[T]$ , recall that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\partial_{a_{s}^{(k)}}l=\\mathbb{1}\\{x_{T+1}=k\\}(V e_{x_{s}})^{\\top}\\left(V X a-e_{x_{o}}\\right),}\\\\ &{\\mathbb{E}\\,\\partial_{a_{s}^{(k)}}l=\\mu_{k}K_{V}a_{s}-\\mu_{k}K_{V P}q_{s}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "First, consider the xpectations. If $s\\not\\in{\\pmb q}^{(k)}$ , then by our assumption, we have $|\\,\\mathbb{E}\\,\\partial_{a_{s}^{(k)}}l|=\\mu_{k}K_{V}a_{s}=$ $O(K_{V}/(N T))$ . Meanwhile, for $s\\in{\\pmb q}^{(k)}$ , we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\mathbb{E}\\partial_{a_{s}^{(k)}}l\\geq\\mu_{k}K_{V P}(q_{s}-a_{s})+\\mu_{k}(K_{V}-K_{V P})a_{s}}\\\\ &{\\qquad\\qquad\\geq\\mu_{k}\\left(\\alpha_{V}(1-\\alpha_{V})K_{P}q_{s}-\\alpha_{V}K_{P}\\left\\|\\mathbf{\\DeltaA}_{A}\\right\\|_{\\mu}\\sqrt{N}-\\left\\|\\mathbf{\\DeltaA}_{V}\\right\\|_{\\mu}^{2}a_{s}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "As a result, we have for any $k\\in[N]$ and $s\\in{\\pmb q}^{(k)}$ \uff0c ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{-\\mathbb{E}\\,\\partial_{a_{s}^{(k)}}l\\geq c\\frac{\\alpha_{V}K_{P}}{N Q}\\gg O\\left(\\frac{K_{V}}{N T}\\right)}&{\\Leftarrow}&{\\left\\{\\begin{array}{l l}{\\displaystyle\\|\\Delta_{A}\\|_{\\mu}\\leq c\\frac{1-\\alpha_{V}}{Q\\sqrt{N}},}\\\\ {\\displaystyle\\|\\Delta_{V}\\|_{\\mu}^{2}\\leq c(1-\\alpha_{V})K_{P},}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "for some small constant $c>0$ . Then, for the size of each entry, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\left|\\partial_{a_{s}^{(k)}}l\\right|\\leq\\left|(V e_{x_{s}})^{\\top}V X a\\right|+\\left|(V e_{x_{s}})^{\\top}e_{x_{o}}\\right|\\leq\\sum_{t\\in q^{(k)}}a_{t}\\sum_{n=1}^{N}|V_{n,x_{s}}V_{n,x_{t}}|+|V_{x_{o},x_{s}}|\\leq N.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Therefore, $\\partial_{a_{s}^{(k)}}l$ .s $N^{2}K_{\\mathrm{Tmp}}^{4}$ -subgaussian whence $\\partial_{a_{s}^{(k)}}^{(B_{\\tau})}l$ .is $N^{2}K_{\\mathrm{Tmp}}^{4}/B_{\\tau}$ -subgaussan As aresult, for each $\\xi>0$ ,we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\left|\\partial_{a_{s}^{(k)}}^{(B_{\\tau})}l-\\mathbb{E}\\,\\partial_{a_{s}^{(k)}}l\\right|\\geq\\xi\\right]\\leq2\\exp\\left(\\frac{-\\xi^{2}B_{\\tau}}{N^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Apply union bound and we get ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\Vert\\nabla_{A}l-\\mathbb{E}\\nabla_{A}l\\Vert_{\\infty}\\geq\\xi\\right]\\leq2T N\\exp\\left(\\frac{-\\xi^{2}B_{\\tau}}{N^{2}}\\right)=\\exp\\left(\\log(2N T)-\\frac{\\xi^{2}B_{\\tau}}{N^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Recall that the separation between the expectations is $c\\alpha_{V}K_{P}/(N Q)$ .Hence, it suffices to choose $\\xi=c\\alpha_{V}K_{P}/(2N Q)$ . Then, to make the failure probability at most $\\delta_{\\mathrm{Tmp}}$ wecanchoose $B_{\\tau}$ asfollows: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\exp\\left(\\log(2N T)-\\frac{\\xi^{2}B_{\\tau}}{N^{2}}\\right)\\leq\\delta_{\\tau}\\quad\\Leftarrow\\quad\\Leftarrow\\quad B_{\\tau}\\geq C\\frac{N^{4}Q^{2}}{\\alpha_{V}^{2}K_{P}^{2}}\\log\\left(\\frac{C N T}{\\delta_{\\tau}}\\right),\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "for some large constant $C>0$ . Then, to boost the accuracy from $1/2$ to $\\varepsilon_{\\mathrm{Tmp}}$ , it suffices to increase the batch size to $\\begin{array}{r}{C\\frac{N^{4}Q^{2}}{\\varepsilon_{\\mathrm{Tmp}}^{2}\\alpha_{V}^{2}K_{P}^{2}}\\log\\left(\\frac{C N T}{\\delta_{\\tau}}\\right)}\\end{array}$ ", "page_idx": 33}, {"type": "text", "text": "Using this lemma, we can pick $\\begin{array}{r}{\\lambda=\\Theta(\\frac{\\varepsilon K_{P}}{Q^{2}N^{3}})}\\end{array}$ to sparsify or proximal gradient Notie that our proximal gradient is a biased estimate of the true preconditioned gradient, but the separation guarantees that it is possible to make the error controllable. The following lemma calculates the error each proximal step introduces. Here we define $\\hat{\\pmb{h}}_{{\\pmb{A}},\\tau}=\\pmb{G}_{\\lambda,\\tau}^{(k)}-\\mathbb{E}\\hat{\\nabla}_{\\pmb{A}}\\bar{l}$ instead of $\\pmb{h}$ because of the bias introduced by the proximal gradient. ", "page_idx": 33}, {"type": "text", "text": "Lemma E.4. Under the same setting of Lemma E.3, if the batch size ", "page_idx": 33}, {"type": "equation", "text": "$$\nB_{\\tau}\\geq\\operatorname*{max}\\Bigg\\{C\\frac{N^{8}Q^{4}}{\\varepsilon^{2}\\alpha_{V}^{2}K_{P}^{2}}\\log\\left(\\frac{2C N T}{\\delta_{\\tau}}\\right),\\frac{\\Theta(N^{6}Q^{3})}{\\delta\\varepsilon^{2}}\\Bigg\\},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "the noise atention score $|a_{t}^{(k)}|\\leq O(1/T)$ for all $t\\not\\in{\\pmb q}^{(k)}$ and all $k\\in[N]$ and $\\lambda=\\Theta\\bigg(\\frac{\\varepsilon K_{P}}{Q^{2}N^{3}}\\bigg),$ then with probability $1-\\delta_{\\tau}$ , the gradient error at iteration $\\tau$ ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\|\\hat{h}_{A,\\tau}\\|_{\\mu}:=\\|G_{\\lambda,\\tau}-\\mathbb{E}\\,\\hat{\\nabla}_{A}l\\|_{\\mu}\\leq O\\bigg(\\frac{\\varepsilon}{Q N^{2}}\\bigg).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof. For notational simplicity, we drop the superscript $k$ . The goal is to estimate the difference between $\\mathbb{E}\\,\\hat{\\nabla}_{\\pmb{a}}l$ and ${\\pmb g}_{\\lambda,\\tau}$ by calculating the magnitude of the bias of the proximal gradient together with the concentration error. ", "page_idx": 33}, {"type": "text", "text": "We consider the population gradient first. Since we have $\\left\\{a_{t,\\tau}\\right\\}_{t\\notin{\\pmb q}}$ are all the same in Stage 2, we can write ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\pmb{a}_{\\tau}=[\\pmb{a}_{\\tau}]_{\\pmb{q}}+\\frac{(1-b_{\\tau})\\mathbf{1}_{\\pmb{q}^{c}}}{T-Q}\\quad\\mathrm{where}\\quad b_{\\tau}=\\sum_{t\\in\\pmb{q}}a_{t},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and recall the projected preconditioned gradient for $\\boldsymbol{A}$ ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{E}\\hat{\\nabla}_{a}l=\\left(\\|V\\|_{\\mu}^{2}-\\|\\mu\\|^{2}\\right)\\left({a}^{(k)}-\\frac{1}{T}\\right)-\\left(\\langle V,P\\rangle_{\\mu}-\\|\\mu\\|^{2}\\right)\\left({\\pmb q}^{(k)}-\\frac{1}{T}\\right).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Therefore, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{E}\\hat{\\nabla}_{a}l=\\left[\\mathbb{E}\\hat{\\nabla}_{a}l\\right]_{q}+K_{V}\\frac{(1-b_{\\tau})\\mathbf{1}_{q^{c}}}{T-Q}+(\\|V\\|_{\\mu}^{2}-\\langle V,P\\rangle_{\\mu})\\frac{\\mathbf{1}_{q^{c}}}{T}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Now, we consider $\\pmb{g}$ by calculating the expression of $\\pmb{a}_{\\tau+1}$ . First, note that by Lemma E.3, we have (\\*,+1) It E g) This mlies that (a,+1tg areal the same and the valu is at most $1/T$ .Moreover, it also implies that it suffices to focus on ${[{\\pmb a}_{\\tau+1}^{\\prime\\prime}]}_{\\pmb q}$ , for which we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\big[\\pmb{\\mathscr{a}}_{\\tau+1}^{\\prime\\prime}\\big]_{q}=\\big[\\pmb{\\mathscr{a}}_{\\tau+1}^{\\prime}\\big]_{q}-\\lambda\\mathbf{1}_{q}=\\big[\\pmb{\\mathscr{a}}_{\\tau}\\big]_{q}-\\frac{\\eta}{\\mu_{k}}\\left[\\nabla_{\\pmb{a}}^{(B_{\\tau})}l\\right]_{\\pmb{q}}-\\lambda\\mathbf{1}_{q}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Therefore, $\\begin{array}{r}{\\sum_{t\\in q}a_{t,\\tau+1}^{\\prime}=b_{\\tau}-\\frac{\\eta}{\\mu_{k}}\\sum_{t\\in q}\\partial_{a_{t}}^{(B_{\\tau})}l-Q\\lambda}\\end{array}$ and ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle a_{\\tau+1}=[a_{\\tau}]_{q}-\\frac{\\eta}{\\mu_{k}}\\left[\\nabla_{a}^{(B_{\\tau})}l\\right]_{q}-\\lambda\\mathbf{1}_{q}+\\left(1-b_{\\tau}+\\frac{\\eta}{\\mu_{k}}\\sum_{t\\in q}\\partial_{a_{t}}^{(B_{\\tau})}l+Q\\lambda\\right)\\frac{\\mathbf{1}}{T}}}\\\\ {{\\displaystyle\\qquad=a_{\\tau}-\\frac{(1-b_{\\tau})\\mathbf{1}_{q^{c}}}{T-Q}-\\frac{\\eta}{\\mu_{k}}\\left[\\nabla_{a}^{(B_{\\tau})}l\\right]_{q}-\\lambda\\mathbf{1}_{q}+\\left(1-b_{\\tau}+\\frac{\\eta}{\\mu_{k}}\\sum_{t\\in q}\\partial_{a_{t}}^{(B_{\\tau})}l+Q\\lambda\\right)\\frac{\\mathbf{1}}{T}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Thus, we can write an explicit update ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{g_{\\lambda,\\tau}=\\frac{1}{\\mu_{k}}\\left[\\nabla_{a}^{(B_{\\tau})}l\\right]_{q}+\\frac{\\lambda}{\\eta}\\mathbf{1}_{q}-\\left(1-b_{\\tau}+\\frac{\\eta}{\\mu_{k}}\\sum_{t\\in q}\\partial_{a_{t}}^{(B_{\\tau})}l+Q\\lambda\\right)\\frac{\\mathbf{1}}{\\eta T}+\\frac{(1-b_{\\tau})\\mathbf{1}_{q^{c}}}{\\eta(T-Q)}\\nabla_{a_{t}}\\mathcal{A}}}\\\\ {{\\displaystyle\\qquad=\\left[\\hat{\\nabla}_{a}^{(B_{\\tau})}l\\right]_{q}+\\frac{\\mathbf{1}_{q}\\mathbf{1}^{\\top}\\nabla_{a}^{(B_{\\tau})}l}{\\mu_{k}T}-\\left(1-b_{\\tau}+\\frac{\\eta}{\\mu_{k}}\\sum_{t\\in q}\\partial_{a_{t}}^{(B_{\\tau})}l+Q\\lambda\\right)\\frac{\\mathbf{1}}{\\eta T}}}\\\\ {{\\displaystyle\\qquad+\\left.\\frac{(1-b_{\\tau})\\mathbf{1}_{q^{c}}}{\\eta(T-Q)}+\\frac{\\lambda}{\\eta}\\mathbf{1}_{q}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Then the gradient error at step $\\tau$ can be decomposed into: ", "page_idx": 34}, {"type": "text", "text": "(Concentration error) ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\pmb{g}_{\\lambda,\\tau}-\\mathbb{E}\\hat{\\nabla}_{\\pmb{a}}l=\\left[\\hat{\\nabla}_{\\pmb{a}}^{(B_{\\tau})}l-\\mathbb{E}\\hat{\\nabla}_{\\pmb{a}}l\\right]_{\\pmb{q}}}&{}\\\\ {+\\;\\frac{\\mathbf{1}_{\\pmb{q}}\\mathbf{1}^{\\top}\\nabla_{\\pmb{a}}^{(B_{\\tau})}l}{\\mu_{k}T}-\\left(1-b_{\\tau}+\\frac{\\eta}{\\mu_{k}}\\sum_{t\\in q}\\partial_{a_{t}}^{(B_{\\tau})}l+Q\\lambda\\right)\\frac{\\mathbf{1}}{\\eta T}+\\frac{\\lambda}{\\eta}\\mathbf{1}_{q}}&{}\\\\ {+\\;\\frac{\\left(1-b_{\\tau}\\right)\\mathbf{1}_{q^{c}}}{\\eta(T-Q)}-K_{V}\\frac{\\left(1-b_{\\tau}\\right)\\mathbf{1}_{q^{c}}}{T-Q}-(\\|V\\|_{\\mu}^{2}-\\langle V,P\\rangle_{\\mu})\\frac{\\mathbf{1}_{q^{c}}}{T}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Here the gradient bias error can be further simplified to ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left[\\displaystyle\\frac{1}{\\mu_{k}T}\\sum_{t\\notin q}\\partial_{a_{t}}^{(B_{\\tau})}l-\\displaystyle\\frac{1-b_{\\tau}+Q\\lambda}{\\eta T}+\\displaystyle\\frac{\\lambda}{\\eta}\\right]\\mathbf{1}_{q}}\\\\ &{+\\left[\\displaystyle\\frac{Q(1-b_{\\tau}-Q\\lambda)-Q\\lambda T-\\eta T K_{V}}{\\eta T(T-Q)}-\\displaystyle\\frac{\\sum_{t\\in q}\\partial_{a_{t}}^{(B_{\\tau})}l}{\\mu_{k}T}-\\displaystyle\\frac{\\|V\\|_{\\mu}^{2}-\\langle V,P\\rangle_{\\mu}}{T}\\right]\\mathbf{1}_{q^{c}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "First, we estimate the concentration error. Similar to Lemma B.10, we first upper bound the infinity norm of the gradient. Consider the maximum absolute value in the original gradients. For $\\nabla_{\\pmb{a}^{(k)}}l$ ,we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~~\\left\\|\\mathbb{1}\\{x_{T+1}=k\\}(V X)^{\\top}(V X a^{(k)}-e_{x_{o}})\\right\\|_{\\infty}}\\\\ &{\\leq\\left\\|\\mathbb{1}\\{x_{T+1}=k\\}(V X)^{\\top}(V X a^{(k)})\\right\\|_{\\infty}+\\left\\|\\mathbb{1}\\{x_{T+1}=k\\}(V X)^{\\top}(e_{x_{o}})\\right\\|_{\\infty}}\\\\ &{\\leq\\underset{s\\in[T]}{\\operatorname*{max}}\\left|\\sum_{t=1}^{T}a_{t}(V e_{x_{s}})^{\\top}V e_{x_{t}}\\right|+\\underset{s\\in[T]}{\\operatorname*{max}}\\left|(V e_{x_{s}})^{\\top}e_{x_{o}}\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "The first term can be upper-bounded in the following way: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{s\\in[T]}{\\operatorname*{max}}\\left|\\displaystyle\\sum_{t=1}^{T}a_{t}(V e_{x_{s}})^{\\top}V e_{x_{t}}\\right|\\leq\\left(\\displaystyle\\sum_{t\\in q}|a_{t}|+\\displaystyle\\sum_{t\\notin q}|a_{t}|\\right)\\underset{s,t}{\\operatorname*{max}}\\left|(V e_{x_{s}})^{\\top}V e_{x_{t}}\\right|}&{}\\\\ {\\quad\\leq\\Theta(1)\\operatorname*{max}_{s,t}\\left|(V e_{x_{s}})^{\\top}V e_{x_{t}}\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "The second inequality is due to $|a_{t}^{(k)}|\\;\\leq\\;O(1/T)$ for $\\textbf{\\textit{t}}\\not\\in{\\textbf{\\textit{q}}}$ Since $V_{n,m}~\\leq~O(1)$ , we have $\\operatorname*{max}_{s,t}\\left|(V e_{x_{s}})^{\\top}\\bar{V}e_{x_{t}}\\right|$ upper bounded by $O(1)$ . Therefore ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{s\\in[T]}\\left|\\sum_{t=1}^{T}a_{t}(V e_{x_{s}})^{\\top}V e_{x_{t}}\\right|\\leq O(1)\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "And similarly, the second term $\\mathrm{max}_{s\\in[T]}\\left|(V e_{x_{s}})^{\\top}e_{x_{o}}\\right|$ can be bounded by $O(1)$ because the infinity norm of $V$ is also upper bounded by $O(1)$ . Therefore, we know $\\|\\nabla_{\\mathbf{\\Phi}_{\\mathbf{q}^{(k)}}}l\\|_{\\infty}\\le O(1)$ Now we consider the preconditioned gradient $\\hat{\\nabla}_{\\pmb{a}}^{(B_{\\tau})}l$ ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|\\hat{\\nabla}_{a}^{(B_{\\tau})}l\\|_{\\infty}=\\left\\|\\frac{1}{\\mu_{k}}\\left(I_{T}-\\frac{\\mathbf{1}_{T}\\mathbf{1}_{T}^{\\top}}{T}\\right)\\left(\\nabla_{a^{(k)}}^{(B_{\\tau})}l\\right)\\right\\|_{\\infty}}\\\\ {\\displaystyle\\leq\\left\\|\\frac{1}{\\mu_{k}}I_{T}\\left(\\nabla_{a^{(k)}}^{(B_{\\tau})}l\\right)\\right\\|_{\\infty}+\\left\\|\\frac{1}{\\mu_{k}}\\left(\\frac{\\mathbf{1}_{T}\\mathbf{1}_{T}^{\\top}}{T}\\right)\\left(\\nabla_{a^{(k)}}^{(B_{\\tau})}l\\right)\\right\\|_{\\infty}\\leq{\\cal O}(N)}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "since $\\begin{array}{r}{\\mu_{k}\\geq\\frac{c}{N}}\\end{array}$ for all $k\\in[N]$ . Now since $\\left[\\hat{\\nabla}_{a}^{(B_{\\tau})}l\\right]_{q}$ .is $Q$ sparse, we have $\\Xi\\left\\|\\left[\\hat{\\nabla}_{A}l\\right]_{q}\\right\\|_{\\mu}^{2}\\le{\\cal O}(Q N^{2})$ By Lemma B.9, when $\\begin{array}{r}{B_{\\tau}\\geq\\frac{\\Theta(N^{6}Q^{3})}{\\delta\\varepsilon^{2}}}\\end{array}$ , with probability $\\begin{array}{r}{1-\\frac{\\delta_{\\tau}}{2}}\\end{array}$ \uff0c ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left\\|\\left[\\hat{\\nabla}_{a}^{(B_{\\tau})}l-\\mathbb{E}\\,\\hat{\\nabla}_{a}l\\right]_{q}\\right\\|\\leq O\\left(\\frac{\\varepsilon}{Q N^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Then, considerthe gradien bia term.Wit the seletedt $\\lambda=\\Theta\\bigg(\\frac{\\varepsilon K_{P}}{Q^{2}N^{3}}\\bigg)$ and $\\begin{array}{r}{\\left\\|\\partial_{a_{t}}^{(B_{\\tau})}l\\right\\|\\leq O\\Big(\\frac{\\varepsilon K_{P}}{N^{3}Q^{2}}\\Big)}\\end{array}$ for $t\\not\\in{\\pmb q}$ , with probability $1-\\delta_{\\tau}/2$ we have the $\\mu$ -norm of first term (since $K_{P}\\leq O(N))$ ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\|(1^{*})\\|\\le\\left(\\left\\|\\frac{1}{\\mu_{k}T}\\sum_{t\\notin q}\\partial_{a_{t}}^{(B_{\\tau})}l\\right\\|+\\frac{Q\\lambda}{T}+\\lambda\\right)\\cdot\\sqrt{Q}}}\\\\ &{}&{\\le{\\cal O}\\left(\\frac{\\varepsilon}{Q N^{2}}\\right)+{\\cal O}\\left(\\frac{\\varepsilon}{T N^{2}\\sqrt{Q}}\\right)+{\\cal O}\\left(\\frac{\\varepsilon}{Q^{3/2}N^{2}}\\right)\\le{\\cal O}\\left(\\frac{\\varepsilon}{Q N^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "and the second term can be upper-bounded by ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\|(2^{*})\\|\\leq\\left(O\\left(\\frac{Q N}{T}\\right)+O\\left(\\frac{Q N}{T}\\right)+O\\left(\\frac{N}{T}\\right)\\right)\\cdot\\sqrt{T}\\leq O\\left(\\frac{\\varepsilon}{Q N^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "since $\\begin{array}{r}{\\frac{1}{\\sqrt{T}}\\le O\\Big(\\frac{\\varepsilon}{Q^{2}N^{3}}\\Big),\\partial_{a_{t}}^{(B_{\\tau})}l=O(1)}\\end{array}$ and $\\|V\\|_{\\mu}^{2}\\leq N$ ", "page_idx": 35}, {"type": "text", "text": "Combine all three terms and by union bound, we have with probability $1-\\delta_{\\tau}$ ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\hat{h}_{A,\\tau}\\right\\|=\\left\\|G_{\\lambda,\\tau}^{(k)}-\\mathbb{E}\\,\\hat{\\nabla}_{A}l\\right\\|_{\\mu}}\\\\ &{\\qquad\\quad=\\sqrt{\\displaystyle\\sum_{k=1}^{N}\\mu_{k}\\|{g}_{\\lambda,\\tau}^{(k)}-\\mathbb{E}\\,\\hat{\\nabla}_{a^{(k)}}l\\|^{2}}\\leq O\\bigg(\\frac{\\varepsilon}{Q N^{2}}\\bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "since $\\mu_{k}=\\Theta(1/N)$ ", "page_idx": 35}, {"type": "text", "text": "E.2  Model aligning and the decrease of the errors ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "First, we show that the signal will continue to grow and approximation error will decrease. This decouples the error at the end of Stage 1 and the final error. In particular, we show that eventually we will have $\\alpha_{V}+\\alpha_{A}\\approx2$ $\\|\\mathbf{A}_{A}\\|_{\\mu}^{2}\\approx0$ and $\\|\\Delta_{V}\\|_{\\mu}^{2}\\approx0^{5}$ ", "page_idx": 35}, {"type": "text", "text": "For notational simplicity, define $\\delta_{A}^{2}=\\left|\\left|\\mathbf{\\DeltaA}_{A}\\right|\\right|_{\\mu}^{2}/K_{Q}$ and $\\delta_{V}^{2}=\\left\\|\\Delta_{V}\\right\\|_{\\mu}^{2}/K_{P}$ . Recall Lemma C.5 and Lemma C.6 and that we choose $\\eta_{V}=\\eta/K_{Q}$ \uff0c $\\dot{\\eta}_{A}=\\eta/K_{P}$ . The dynamics of the signals and the errors can be described using6 ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\alpha_{V,\\tau+1}=\\alpha_{V,\\tau}+\\eta\\left(1-\\alpha_{A}\\alpha_{V}\\right)\\alpha_{A}+\\frac{\\eta}{K_{Q}}\\frac{1-\\alpha_{V}}{T}-\\eta\\alpha_{V}\\delta_{A}^{2}-\\eta\\frac{\\left<h_{V,\\tau},P\\right>_{\\mu}}{K_{P}K_{Q}},\n$$", "text_format": "latex", "page_idx": 35}, {"type": "equation", "text": "$$\n\\alpha_{A,\\tau+1}=\\alpha_{A,\\tau}+\\eta\\left(1-\\alpha_{V}\\alpha_{A}\\right)\\alpha_{V}-\\eta\\alpha_{A}\\delta_{V}^{2}-\\eta\\frac{\\left<\\hat{h}_{A,\\tau},Q\\right>_{\\mu}}{K_{P}K_{Q}},\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "and ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta_{A,\\tau+1}^{2}=\\left(1-\\eta\\alpha_{V}^{2}-\\eta\\delta_{A}^{2}\\right)^{2}\\delta_{A}^{2}-2\\eta\\left(1-\\eta\\alpha_{V}^{2}-\\eta\\delta_{V}^{2}\\right)\\frac{\\left<\\Delta_{A},\\hat{h}_{A,\\tau}\\right>_{\\mu}}{K_{P}K_{Q}}}\\\\ &{\\phantom{\\delta_{A,\\tau+1}^{2}=}-\\eta^{2}\\frac{\\left<\\hat{h}_{A,\\tau},Q\\right>_{\\mu}^{2}}{K_{P}^{2}K_{Q}^{2}}+\\eta^{2}\\frac{\\left|\\displaystyle\\hat{h}_{A,\\tau}\\right|\\left|_{\\mu}^{2}\\right.}{K_{P}^{2}K_{Q}},}\\\\ &{\\delta_{V,\\tau+1}^{2}=\\left(1-\\eta\\left(\\alpha_{A}^{2}+\\frac{1}{K_{Q}T}+\\delta_{A}^{2}\\right)\\right)^{2}\\delta_{V}^{2}+2\\eta\\left(1-\\eta\\left(\\alpha_{A}^{2}+\\frac{1}{K_{Q}T}+\\delta_{A}^{2}\\right)\\right)\\frac{\\left<\\Delta_{V},h_{V}\\right>_{\\mu}}{K_{P}K_{Q}}}\\\\ &{\\phantom{\\delta_{A,\\tau+1}^{2}=}-\\eta^{2}\\frac{\\left<P,h_{V}\\right>_{\\mu}^{2}}{K_{Q}^{2}K_{P}^{2}}+\\eta^{2}\\frac{\\left|\\displaystyle\\|h_{V}\\right|\\left|_{\\mu}^{2}\\right.}{K_{P}K_{Q}^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "E.2.1  Lemmas for the dynamics ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Before we come to the final convergence analysis, we first simplify the dynamics with some basic lemmas. ", "page_idx": 36}, {"type": "text", "text": "Lemma E.5 (Dynamics of the errors). For the errors, we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta_{A,\\tau+1}^{2}\\leq\\exp\\left(-2\\eta\\alpha_{V}^{2}\\right)\\delta_{A,\\tau}^{2}+3\\eta\\left(\\delta_{A}+\\eta\\frac{\\left\\|\\hat{h}_{A}\\right\\|_{\\mu}}{K_{P}\\sqrt{K_{Q}}}\\right)\\frac{\\left\\|\\hat{h}_{A}\\right\\|_{\\mu}}{K_{P}\\sqrt{K_{Q}}},}\\\\ &{\\delta_{V,\\tau+1}^{2}\\leq\\exp\\left(-2\\eta\\alpha_{A}^{2}\\right)\\delta_{V,\\tau}^{2}+3\\eta\\left(\\delta_{V}+\\eta\\frac{\\left\\|h_{V}\\right\\|_{\\mu}}{\\sqrt{K_{P}}K_{Q}}\\right)\\frac{\\left\\|h_{V}\\right\\|_{\\mu}}{\\sqrt{K_{P}}K_{Q}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Proof. First, we write ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta_{A,\\tau+1}^{2}=\\left(1-\\eta\\alpha_{V}^{2}-\\eta\\delta_{A}^{2}\\right)^{2}\\delta_{A}^{2}-2\\eta\\left(1-\\eta\\alpha_{V}^{2}-\\eta\\delta_{V}^{2}\\right)\\frac{\\left<\\hat{A}_{A},\\hat{h}_{A}\\right>_{\\mu}}{K_{P}K_{Q}}}\\\\ &{\\qquad\\qquad-\\eta^{2}\\frac{\\left<\\hat{h}_{A},\\tau,Q\\right>_{\\mu}^{2}}{K_{P}^{2}K_{Q}^{2}}+\\eta^{2}\\frac{\\left|\\hat{h}_{A}\\right|_{\\mu}^{2}}{K_{P}^{2}K_{Q}}}\\\\ &{\\qquad\\qquad\\leq\\left(1-\\eta\\alpha_{V}^{2}-\\eta\\delta_{A}^{2}\\right)^{2}\\delta_{A}^{2}+2\\eta\\frac{\\delta_{A}\\left|\\hat{h}_{A}\\right|_{\\mu}}{K_{P}\\sqrt{K_{Q}}}+3\\eta^{2}\\frac{\\left|\\hat{h}_{A,\\tau}\\right|_{\\mu}^{2}}{K_{P}^{2}K_{Q}}}\\\\ &{\\qquad\\leq\\left(1-\\eta\\alpha_{V}^{2}-\\eta\\delta_{A}^{2}\\right)^{2}\\delta_{A}^{2}+3\\eta\\left(\\delta_{A}+\\frac{\\eta}{K_{P}}\\frac{\\left|\\hat{h}_{A}\\right|_{\\mu}}{\\sqrt{K_{Q}}}\\right)\\frac{\\left|\\hat{h}_{A}\\right|_{\\mu}}{K_{P}\\sqrt{K_{Q}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "For the first term, we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\left(1-\\eta\\alpha_{V}^{2}-\\eta\\delta_{A}^{2}\\right)^{2}\\leq\\exp\\left(-\\eta\\alpha_{V}^{2}-\\eta\\delta_{A}^{2}\\right)^{2}\\leq\\exp\\left(-2\\eta\\alpha_{V}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Thus, for $\\delta_{A}$ , we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\delta_{A,\\tau+1}^{2}\\leq\\exp\\left(-2\\eta\\alpha_{V}^{2}\\right)\\delta_{A,\\tau}^{2}+3\\eta\\left(\\delta_{A}+\\eta\\frac{\\left\\|\\hat{h}_{A}\\right\\|_{\\mu}}{K_{P}\\sqrt{K_{Q}}}\\right)\\frac{\\left\\|\\hat{h}_{A}\\right\\|_{\\mu}}{K_{P}\\sqrt{K_{Q}}}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Similarly, for $\\delta_{V}$ , we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\delta_{V,\\tau+1}^{2}\\leq\\left(1-\\eta\\left(\\alpha_{A}^{2}+\\frac{1}{K_{Q}T}+\\delta_{A}^{2}\\right)\\right)^{2}\\delta_{V}^{2}+3\\eta\\frac{\\delta_{V}\\left\\|h_{V}\\right\\|_{\\mu}}{\\sqrt{K_{P}}K_{Q}}+3\\eta^{2}\\frac{\\left\\|h_{V}\\right\\|_{\\mu}^{2}}{K_{P}K_{Q}^{2}}}\\\\ {\\leq\\exp\\left(-2\\eta\\alpha_{A}^{2}\\right)\\delta_{V}^{2}+3\\eta\\left(\\delta_{V}+\\eta\\frac{\\left\\|h_{V}\\right\\|_{\\mu}}{\\sqrt{K_{P}}K_{Q}}\\right)\\frac{\\left\\|h_{V}\\right\\|_{\\mu}}{\\sqrt{K_{P}}K_{Q}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Lemma E.6 (Dynamics of $\\alpha_{A}-\\alpha_{V}.$ 0. The difference between the signals evolves as follows ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\alpha_{V,\\tau+1}-\\alpha_{A,\\tau+1}\\right)^{2}\\leq\\exp\\left(-2\\eta\\left(1-\\alpha_{A}\\alpha_{V}\\right)\\right)\\left(\\alpha_{V}-\\alpha_{A}\\right)^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\thinspace8\\eta\\left(\\displaystyle\\frac{1}{K_{Q}T}+\\delta_{A}^{2}+\\delta_{V}^{2}+\\displaystyle\\frac{\\left\\|h_{V,\\tau}\\right\\|_{\\mu}}{\\sqrt{K_{P}}K_{Q}}+\\displaystyle\\frac{\\left\\|\\hat{h}_{A,\\tau}\\right\\|_{\\mu}}{K_{P}\\sqrt{K_{Q}}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof. First, we write ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\alpha_{V,\\tau+1}-\\alpha_{A,\\tau+1}=\\alpha_{V,\\tau}-\\alpha_{A,\\tau}-\\eta\\left(1-\\alpha_{A}\\alpha_{V}\\right)\\left(\\alpha_{V}-\\alpha_{A}\\right)}&{{}}\\\\ {+\\displaystyle\\frac{\\eta}{K_{Q}}\\frac{1-\\alpha_{V}}{T}-\\eta\\alpha_{V}\\delta_{A}^{2}-\\eta\\displaystyle\\frac{\\left<h_{V,\\tau},P\\right>_{\\mu}}{K_{P}K_{Q}}+\\eta\\alpha_{A}\\delta_{V}^{2}+\\eta\\displaystyle\\frac{\\left<\\hat{h}_{A,\\tau},Q\\right>_{\\mu}}{K_{P}K_{Q}}}&{{}}\\\\ {=:\\left(1-\\eta\\left(1-\\alpha_{A}\\alpha_{V}\\right)\\right)\\left(\\alpha_{V}-\\alpha_{A}\\right)+\\Upsilon\\mathrm{mp}.}&{{}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Therefore, we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\alpha_{V,\\tau+1}-\\alpha_{A,\\tau+1}\\right)^{2}=\\left(\\left(1-\\eta\\left(1-\\alpha_{A}\\alpha_{V}\\right)\\right)\\left(\\alpha_{V}-\\alpha_{A}\\right)+\\mathrm{Tmp}\\right)^{2}}\\\\ &{\\qquad\\qquad=\\left(1-\\eta\\left(1-\\alpha_{A}\\alpha_{V}\\right)\\right)^{2}\\left(\\alpha_{V}-\\alpha_{A}\\right)^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\left.2\\left(1-\\eta\\left(1-\\alpha_{A}\\alpha_{V}\\right)\\right)\\left(\\alpha_{V}-\\alpha_{A}\\right)\\mathrm{Tmp}+\\mathrm{Tmp}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\exp\\left(-2\\eta\\left(1-\\alpha_{A}\\alpha_{V}\\right)\\right)\\left(\\alpha_{V}-\\alpha_{A}\\right)^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\left.3\\middle|\\alpha_{V}-\\alpha_{A}\\middle|\\right|\\mathrm{Tmp}\\right|+\\mathrm{Tmp}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Then, for Tmp, we compute ", "page_idx": 37}, {"type": "equation", "text": "$$\n0.5\\eta^{-1}|\\mathrm{Tmp}|\\leq\\frac{1}{K_{Q}T}+\\delta_{A}^{2}+\\delta_{V}^{2}+\\frac{\\left\\|h_{V,\\tau}\\right\\|_{\\mu}}{\\sqrt{K_{P}}K_{Q}}+\\frac{\\left\\|\\hat{h}_{A,\\tau}\\right\\|_{\\mu}}{K_{P}\\sqrt{K_{Q}}}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "In particular, this implies $|\\mathrm{Tmp}|\\leq1$ . Thus, we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\alpha_{V,\\tau+1}-\\alpha_{A,\\tau+1}\\right)^{2}\\leq\\exp\\left(-2\\eta\\left(1-\\alpha_{A}\\alpha_{V}\\right)\\right)\\left(\\alpha_{V}-\\alpha_{A}\\right)^{2}+4\\left\\vert\\mathrm{Tmp}\\right\\vert}\\\\ &{\\qquad\\qquad\\qquad\\leq\\exp\\left(-2\\eta\\left(1-\\alpha_{A}\\alpha_{V}\\right)\\right)\\left(\\alpha_{V}-\\alpha_{A}\\right)^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,8\\eta\\left(\\cfrac{1}{K_{Q}T}+\\delta_{A}^{2}+\\delta_{V}^{2}+\\cfrac{\\left\\Vert h_{V,\\tau}\\right\\Vert_{\\mu}}{\\sqrt{K_{P}}K_{Q}}+\\cfrac{\\left\\Vert\\hat{h}_{A,\\tau}\\right\\Vert_{\\mu}}{K_{P}\\sqrt{K_{Q}}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Lemma E.7. Suppose that both $\\alpha_{V},\\alpha_{A}$ are at most 1. Then, we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{1-\\frac{\\alpha_{V,\\tau+1}+\\alpha_{A,\\tau+1}}{2}\\leq\\left(1-\\eta\\frac{\\alpha_{V}+\\alpha_{A}}{2}\\right)\\left(1-\\frac{\\alpha_{V}+\\alpha_{A}}{2}\\right)+\\eta\\frac{\\alpha_{V}+\\alpha_{A}}{2}\\left(\\delta_{A}^{2}+\\delta_{V}^{2}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\left.\\frac{\\eta}{2}\\frac{\\left\\Vert h_{V,\\tau}\\right\\Vert_{\\mu}}{\\sqrt{K_{P}}K_{Q}}+\\frac{\\eta}{2}\\frac{\\left\\Vert\\hat{h}_{A,\\tau}\\right\\Vert_{\\mu}}{K_{P}\\sqrt{K_{Q}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof. First, we write ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\alpha_{V,\\tau+1}+\\alpha_{A,\\tau+1}=\\alpha_{V,\\tau}+\\alpha_{A,\\tau}+\\eta\\left(1-\\alpha_{A}\\alpha_{V}\\right)\\left(\\alpha_{V}+\\alpha_{A}\\right)}&{{}}\\\\ {+\\:\\frac{\\eta}{K_{Q}}\\frac{1-\\alpha_{V}}{T}-\\eta\\alpha_{V}\\delta_{A}^{2}-\\eta\\alpha_{A}\\delta_{V}^{2}-\\eta\\frac{\\left<h_{V,\\tau},P\\right>_{\\mu}}{K_{P}K_{Q}}-\\eta\\frac{\\left<\\hat{h}_{A,\\tau},Q\\right>_{\\mu}}{K_{P}K_{Q}}}&{{}}\\\\ {=:\\alpha_{V,\\tau}+\\alpha_{A,\\tau}+\\eta\\mathrm{Tmp}_{\\mathrm{sig}}+\\eta\\mathrm{Tmp}_{\\mathrm{err}}.}&{{}}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "For the signal growth, note that $\\alpha_{A}\\alpha_{V}\\leq(\\alpha_{V}^{2}+\\alpha_{A}^{2})/2\\leq(\\alpha_{V}+\\alpha_{A})/2$ .Hence, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\left(1-\\alpha_{A}\\alpha_{V}\\right)\\left(\\alpha_{V}+\\alpha_{A}\\right)\\geq\\frac12\\left(\\alpha_{V}+\\alpha_{A}\\right)\\left(2-\\alpha_{V}-\\alpha_{A}\\right).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "For the error terms, we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\left|\\frac{\\left\\langle h_{V,\\tau},P\\right\\rangle_{\\mu}}{K_{P}K_{Q}}+\\frac{\\left\\langle\\hat{h}_{A,\\tau},Q\\right\\rangle_{\\mu}}{K_{P}K_{Q}}\\right|\\leq\\frac{\\left\\|h_{V,\\tau}\\right\\|_{\\mu}}{\\sqrt{K_{P}}K_{Q}}+\\frac{\\left\\|\\hat{h}_{A,\\tau}\\right\\|_{\\mu}}{K_{P}\\sqrt{K_{Q}}}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Thus, we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{c}{\\alpha_{V,\\tau+1}+\\alpha_{A,\\tau+1}\\geq\\alpha_{V,\\tau}+\\alpha_{A,\\tau}+\\eta\\left(\\alpha_{V}+\\alpha_{A}\\right)\\left(1-\\displaystyle\\frac{\\alpha_{V}+\\alpha_{A}}{2}-\\delta_{A}^{2}-\\delta_{V}^{2}\\right)}\\\\ {-\\displaystyle\\eta\\frac{\\left\\|h_{V,\\tau}\\right\\|_{\\mu}}{\\sqrt{K_{P}}K_{Q}}-\\eta\\displaystyle\\frac{\\left\\|\\hat{h}_{A,\\tau}\\right\\|_{\\mu}}{K_{P}\\sqrt{K_{Q}}},}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "and, therefore, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{1-\\frac{\\alpha_{V,\\tau+1}+\\alpha_{A,\\tau+1}}{2}\\leq\\left(1-\\eta\\frac{\\alpha_{V}+\\alpha_{A}}{2}\\right)\\left(1-\\frac{\\alpha_{V}+\\alpha_{A}}{2}\\right)+\\eta\\frac{\\alpha_{V}+\\alpha_{A}}{2}\\left(\\delta_{A}^{2}+\\delta_{V}^{2}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\left.\\frac{\\eta}{2}\\frac{\\left\\Vert h_{V,\\tau}\\right\\Vert_{\\mu}}{\\sqrt{K_{P}}K_{Q}}+\\frac{\\eta}{2}\\frac{\\left\\Vert\\hat{h}_{A,\\tau}\\right\\Vert_{\\mu}}{K_{P}\\sqrt{K_{Q}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Lemma E.8. Put $\\varepsilon_{A}=1-\\alpha_{A}$ and $\\varepsilon_{V}=1-\\alpha_{V}$ .When $|\\varepsilon_{A}|,|\\varepsilon_{V}|\\leq1/2;$ we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(\\varepsilon_{A,\\tau+1}+\\varepsilon_{V,\\tau+1}\\right)^{2}\\leq\\exp\\left(-2\\eta\\right)\\left(\\varepsilon_{A}+\\varepsilon_{V}\\right)^{2}+8\\eta|\\varepsilon_{A}+\\varepsilon_{V}|\\varepsilon_{A}\\varepsilon_{V}+16\\eta^{2}\\varepsilon_{A}^{2}\\varepsilon_{V}^{2}\\,}\\\\ {+\\left.8\\eta|\\varepsilon_{A}+\\varepsilon_{V}\\right|\\left(\\cfrac{1}{K_{Q}T}+\\delta_{A}^{2}+\\delta_{V}^{2}+\\cfrac{\\left\\|h_{V,\\tau}\\right\\|_{\\mu}}{\\sqrt{K_{P}}K_{Q}}+\\cfrac{\\left\\|\\hat{h}_{A,\\tau}\\right\\|_{\\mu}}{K_{P}\\sqrt{K_{Q}}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Proof. Similar to the proof of the previous lemma, we write ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\alpha_{V,\\tau+1}+\\alpha_{A,\\tau+1}=\\alpha_{V,\\tau}+\\alpha_{A,\\tau}+\\eta\\left(1-\\alpha_{A}\\alpha_{V}\\right)\\left(\\alpha_{V}+\\alpha_{A}\\right)}&{{}}\\\\ {+\\:\\frac{\\eta}{K_{Q}}\\frac{1-\\alpha_{V}}{T}-\\eta\\alpha_{V}\\delta_{A}^{2}-\\eta\\alpha_{A}\\delta_{V}^{2}-\\eta\\frac{\\left<h_{V,\\tau},P\\right>_{\\mu}}{K_{P}K_{Q}}-\\eta\\frac{\\left<\\hat{h}_{A,\\tau},Q\\right>_{\\mu}}{K_{P}K_{Q}}}&{{}}\\\\ {=:\\alpha_{V,\\tau}+\\alpha_{A,\\tau}+\\eta\\mathrm{Tmp}_{\\mathrm{sig}}+\\eta\\mathrm{Tmp}_{\\mathrm{err}}.}&{{}}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "For the signal term, we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Tmp}_{\\mathrm{sig}}=\\left(1-\\left(1-\\varepsilon_{A}\\right)\\left(1-\\varepsilon_{V}\\right)\\right)\\left(2-\\varepsilon_{A}-\\varepsilon_{V}\\right)}\\\\ &{\\qquad\\quad=\\left(2-\\varepsilon_{A}-\\varepsilon_{V}\\right)\\left(\\varepsilon_{A}+\\varepsilon_{V}\\right)-\\varepsilon_{A}\\varepsilon_{V}\\left(2-\\varepsilon_{A}-\\varepsilon_{V}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "For the error term, we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\left|\\mathrm{Tmp}_{\\mathrm{err}}\\right|\\leq\\frac{1}{K_{Q}T}+2\\delta_{A}^{2}+2\\delta_{V}^{2}+\\frac{2\\left\\|h_{V,\\tau}\\right\\|_{\\mu}}{\\sqrt{K_{P}}K_{Q}}+\\frac{2\\left\\|\\hat{h}_{A,\\tau}\\right\\|_{\\mu}}{K_{P}\\sqrt{K_{Q}}}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Combine these together, and we obtain ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\varepsilon_{A,\\tau+1}+\\varepsilon_{V,\\tau+1}=\\varepsilon_{A,\\tau}+\\varepsilon_{V,\\tau}-\\eta\\left(2-\\varepsilon_{A}-\\varepsilon_{V}\\right)\\left(\\varepsilon_{A}+\\varepsilon_{V}\\right)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ +\\eta\\varepsilon_{A}\\varepsilon_{V}\\left(2-\\varepsilon_{A}-\\varepsilon_{V}\\right)\\pm2\\eta\\left(\\cfrac{1}{K_{Q}T}+\\delta_{A}^{2}+\\delta_{V}^{2}+\\cfrac{\\left\\|h_{V,\\tau}\\right\\|_{\\mu}}{\\sqrt{K_{P}}K_{Q}}+\\cfrac{\\left\\|\\hat{h}_{A,\\tau}\\right\\|_{\\mu}}{K_{P}\\sqrt{K_{Q}}}\\right)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ }}\\\\ &{=:\\left(1-\\eta\\left(2-\\varepsilon_{A}-\\varepsilon_{V}\\right)\\right)\\left(\\varepsilon_{A}+\\varepsilon_{V}\\right)+\\mathrm{Tmp}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Thus, ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\varepsilon_{A,\\tau+1}+\\varepsilon_{V,\\tau+1}\\right)^{2}=\\left(\\left(1-\\eta\\left(2-\\varepsilon_{A}-\\varepsilon_{V}\\right)\\right)\\left(\\varepsilon_{A}+\\varepsilon_{V}\\right)+\\mathrm{Tmp}\\right)^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\exp\\left(-2\\eta\\left(2-\\varepsilon_{A}-\\varepsilon_{V}\\right)\\right)\\left(\\varepsilon_{A}+\\varepsilon_{V}\\right)^{2}+2|\\varepsilon_{A}+\\varepsilon_{V}||\\mathrm{Tmp}|+\\mathrm{Tmp}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Note that ", "page_idx": 39}, {"type": "equation", "text": "$$\n|\\mathrm{Tmp}|\\leq4\\eta\\left(\\varepsilon_{A}\\varepsilon_{V}+\\frac{1}{K_{Q}T}+\\delta_{A}^{2}+\\delta_{V}^{2}+\\frac{\\left\\|h_{V,\\tau}\\right\\|_{\\mu}}{\\sqrt{K_{P}}K_{Q}}+\\frac{\\left\\|\\hat{h}_{A,\\tau}\\right\\|_{\\mu}}{K_{P}\\sqrt{K_{Q}}}\\right).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Recall $|\\varepsilon_{A}|,|\\varepsilon_{V}|\\leq1/2$ . Thus, ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\left(\\mathcal{E}_{A,\\tau+1}+\\mathcal{E}_{V,\\tau+1}\\right)^{2}\\leq\\exp\\left(-2\\eta\\right)\\left(\\varepsilon_{A}+\\varepsilon_{V}\\right)^{2}}\\quad}&{}\\\\ &{\\qquad+\\left.8\\eta\\lvert\\varepsilon_{A}+\\varepsilon_{V}\\rvert\\left(\\varepsilon_{A}\\varepsilon_{V}+\\frac{1}{K_{Q}T}+\\delta_{A}^{2}+\\delta_{V}^{2}+\\frac{\\left\\lVert h_{V,\\tau}\\right\\rVert_{\\mu}}{\\sqrt{K_{P}}K_{Q}}+\\frac{\\left\\lVert h_{A,\\tau}\\right\\rVert_{\\mu}}{K_{P}\\sqrt{K_{Q}}}\\right)\\right.}\\\\ &{\\qquad\\quad+\\left.16\\eta^{2}\\left(\\varepsilon_{A}^{2}\\varepsilon_{V}^{2}+\\left(\\frac{1}{K_{Q}T}+\\delta_{A}^{2}+\\delta_{V}^{2}+\\frac{\\left\\lVert h_{V,\\tau}\\right\\rVert_{\\mu}}{\\sqrt{K_{P}}K_{Q}}+\\frac{\\left\\lVert h_{A,\\tau}\\right\\rVert_{\\mu}}{K_{P}\\sqrt{K_{Q}}}\\right)^{2}\\right)\\right.}\\\\ &{\\leq\\exp\\left(-2\\eta\\right)\\left(\\varepsilon_{A}+\\varepsilon_{V}\\right)^{2}+8\\eta\\lvert\\varepsilon_{A}+\\varepsilon_{V}\\rvert\\left\\lvert\\varepsilon_{A}\\varepsilon_{V}+16\\eta^{2}\\varepsilon_{A}^{2}\\varepsilon_{V}^{2}}\\\\ &{\\qquad\\quad+\\left.8\\eta\\lvert\\varepsilon_{A}+\\varepsilon_{V}\\rvert\\left(\\frac{1}{K_{Q}T}+\\delta_{A}^{2}+\\delta_{V}^{2}+\\frac{\\left\\lVert h_{V,\\tau}\\right\\rVert_{\\mu}}{\\sqrt{K_{P}}K_{Q}}+\\frac{\\left\\lVert h_{A,\\tau}\\right\\rVert_{\\mu}}{K_{P}\\sqrt{K_{Q}}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Lemma E.9. Suppose that $\\left(X_{\\tau}\\right)_{\\tau}$ satisfies $X_{\\tau+1}\\,\\leq\\,e^{-A}X_{\\tau}+B$ for some $A\\,\\in\\,(0,1]$ \uff0c $B\\,\\geq\\,0$ If $X_{0}\\leq2B/A$ , then we have $X_{\\tau}\\leq3B/A$ for all $\\tau\\geq0$ If $X_{0}\\geq2B/A$ , then we have $X_{\\tau}\\leq3B/A$ for all $\\begin{array}{r}{\\tau\\geq\\frac{6}{A}\\log\\left(\\frac{X_{0}A}{3B}\\right)}\\end{array}$ ", "page_idx": 39}, {"type": "text", "text": "Proof. Since $e^{-A}\\leq1-A/2$ for $A\\in(0,1]$ , we have $X_{\\tau+1}\\leq X_{\\tau}-A X_{\\tau}/2+B$ . Hence, whenever $X_{\\tau}\\geq2B/A$ , we will have $X_{\\tau+1}\\leq X_{\\tau}$ . Moreover, if $X_{\\tau}<2B/A$ , we have $X_{\\tau+1}\\leq2B/A+B\\leq3B/A$ This proves the first part of the lemma. ", "page_idx": 39}, {"type": "text", "text": "Now, suppose that $X_{0}\\geq2B/A$ . When $X_{\\tau}\\geq3B/A$ , we have ", "page_idx": 39}, {"type": "equation", "text": "$$\nX_{\\tau+1}\\leq X_{\\tau}-A X_{\\tau}/2+B\\leq X_{\\tau}-A X_{\\tau}/6\\leq e^{-A/6}X_{\\tau}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Thus it takes at most $\\textstyle{\\frac{6}{A}}\\log\\left({\\frac{X_{0}A}{3B}}\\right)$ steps to reduce $X_{\\tau}$ from $X_{0}$ 0 $3B/A$ .After that, the previous analysis applies. \u53e3 ", "page_idx": 39}, {"type": "text", "text": "E.2.2 Main lemma of Stage 2 ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "We split the analysis of Stage 2 into two substage. Let $c_{\\alpha}\\in(0,0.05)$ be a small constant. Define ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathcal{T}_{2.1}:=\\operatorname*{inf}\\left\\lbrace\\tau\\geq\\mathcal{T}_{1}\\,:\\,(\\alpha_{V,\\tau}+\\alpha_{A,\\tau})/2\\geq1-c_{\\alpha}\\right\\rbrace.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "We call $\\{\\tau\\,:\\,\\tau\\leq\\mathcal{T}_{2,1}\\}$ stage 2.1 and $\\{\\tau\\,:\\,\\tau\\geq\\mathcal{T}_{2,1}\\}$ stage 2.2. For notational simplicity, we define ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathrm{Err}_{\\mathrm{small}}=\\operatorname*{max}_{\\tau}\\left\\{\\frac{1}{K_{Q}T}+\\frac{\\left\\|h_{V,\\tau}\\right\\|_{\\mu}}{\\sqrt{K_{P}}K_{Q}}+\\frac{\\left\\|\\hat{h}_{A,\\tau}\\right\\|_{\\mu}}{K_{P}\\sqrt{K_{Q}}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Note that this can be made essentially arbitrarily small by choosing a large enough batch size. ", "page_idx": 39}, {"type": "text", "text": "Lemma E.10. Suppose that the following hold at the beginning of Stage 2 (after thresholding and projection): ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{(a)~\\alpha_{V},\\alpha_{A}\\geq\\alpha^{(2)}}}\\\\ {{(b)~\\delta_{A}^{2}+\\delta_{V}^{2}\\leq(\\delta^{(2)})^{2}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Let $\\delta_{*}$ be our target value for $\\delta_{A}$ and $\\delta_{V}$ .Choose $\\lambda$ as in Lemma E.4. Suppose that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{max}\\left\\{\\delta^{(2)},\\operatorname{Err}_{\\mathrm{small}}\\right\\}\\operatorname{Err}_{\\mathrm{small}}\\leq\\delta_{*}^{2},\\quad\\operatorname{Err}_{\\mathrm{small}}\\leq O(\\alpha^{(2)}),}\\\\ &{\\operatorname*{max}\\left\\{(\\alpha_{V,\\mathcal{T}_{1}}-\\alpha_{A,\\mathcal{T}_{1}})^{2},\\delta^{(2)},\\operatorname{Err}_{\\mathrm{small}}\\right\\}\\leq O\\left(\\frac{1}{\\log(1/\\delta_{*})}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Then, within $O(1/(\\eta\\alpha^{(2)})+\\log(1/\\delta_{*})/\\eta)$ steps, we will have $\\delta_{A}^{2},\\delta_{V}^{2}\\leq\\delta_{*}^{2}$ and $\\alpha_{V},\\alpha_{A}\\in(0.9,1.1)$ ", "page_idx": 40}, {"type": "text", "text": "Remark. Note that our conditions on $\\alpha_{V},\\alpha_{A}$ and $\\delta^{(2)}$ are much weaker that what one can obtain from Stage 1. This allows us to apply the analysis here to transfer learning. ", "page_idx": 40}, {"type": "text", "text": "The following proof should be treated as a large induction argument though we do not explicitly write down the induction as in the proof of Stage 1. In particular, we will show (by induction) that the approximation errors $\\delta_{A}$ and $\\delta_{V}$ are small, so that most of the naive bounds on the entries of A and V can be transferred to $\\boldsymbol{A}$ and $V$ . In particular, $|V_{n,m}|=O(1)$ for all $n,m\\,\\in\\,[N]$ so that our bounds in Section E.1 are valid, and $|a_{t}^{(k)}|=O(1)$ for all $k\\,\\in\\,[N]$ \uff0c $t\\,\\in\\,[N]$ , which implies that after the projection step, $a_{t}^{(k)}=O(1/T)$ for all $t\\not\\in{\\pmb q}^{(k)}$ ", "page_idx": 40}, {"type": "text", "text": "Proof of the Lemma E.10 ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Common results for Stage 2.1 and 2.2  First, we prove some basic results that hold for both Stage 2.1 and 2.2. First, we show (by induction) that $\\bar{\\alpha_{V}},\\bar{\\alpha_{A}}\\,\\geq\\,\\alpha^{(2)}$ and $\\delta_{V}^{2}+\\delta_{A}^{2}\\,\\leq\\,\\delta^{(2)}$ hold throughout Stage 2. Recall from Lemma E.5 that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta_{A,\\tau+1}^{2}\\leq\\exp\\left(-2\\eta\\alpha_{V}^{2}\\right)\\delta_{A,\\tau}^{2}+3\\eta\\left(\\delta_{A}+\\eta\\frac{\\left\\|\\hat{h}_{A}\\right\\|_{\\mu}}{K_{P}\\sqrt{K_{Q}}}\\right)\\frac{\\left\\|\\hat{h}_{A}\\right\\|_{\\mu}}{K_{P}\\sqrt{K_{Q}}}}\\\\ &{\\qquad\\leq\\exp\\left(-2\\eta(\\alpha^{(2)})^{2}\\right)\\delta_{A,\\tau}^{2}+\\eta\\mathrm{Err}_{\\mathrm{small}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Hence, as long as $\\mathrm{Err}_{\\mathrm{small}}\\le O(1)\\delta^{(2)}/(\\alpha^{(2)})^{2}$ , we can ensure $\\delta_{A,\\tau+1}^{2}+\\delta_{V,\\tau+1}^{2}\\leq(\\delta^{(2)})^{2}$ always hold. ", "page_idx": 40}, {"type": "text", "text": "Stage 2.1: signal growth  By Lemma E.7, we have ", "text_level": 1, "page_idx": 40}, {"type": "equation", "text": "$$\n1-\\frac{\\alpha_{V,\\tau+1}+\\alpha_{A,\\tau+1}}{2}\\leq\\left(1-\\eta\\frac{\\alpha_{V}+\\alpha_{A}}{2}\\right)\\left(1-\\frac{\\alpha_{V}+\\alpha_{A}}{2}\\right)+\\eta\\frac{\\alpha_{V}+\\alpha_{A}}{2}\\left(\\delta_{A}^{2}+\\delta_{V}^{2}\\right)+\\eta\\mathrm{Err}_{\\mathrm{small}}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "When $(\\alpha_{V}+\\alpha_{A})/2\\leq1-c_{\\alpha}$ and $\\alpha_{V},\\alpha_{A}\\geq\\alpha^{(2)}$ , we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\eta\\frac{\\alpha_{V}+\\alpha_{A}}{2}\\left(1-\\frac{\\alpha_{V}+\\alpha_{A}}{2}\\right)\\geq\\eta\\frac{\\alpha_{V}+\\alpha_{A}}{2}c_{\\alpha}\\geq\\eta\\alpha^{(2)}c_{\\alpha}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Hence, as long as $\\mathrm{Err}_{\\mathrm{small}}\\leq\\alpha^{(2)}c_{\\alpha}/2$ and $\\delta^{(2)}\\leq c_{\\alpha}/2$ , we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n1-\\frac{\\alpha_{V,\\tau+1}+\\alpha_{A,\\tau+1}}{2}\\leq\\left(1-\\frac{\\eta}{2}\\frac{\\alpha_{V}+\\alpha_{A}}{2}\\right)\\left(1-\\frac{\\alpha_{V}+\\alpha_{A}}{2}\\right)\\leq\\exp\\left(-\\frac{\\eta\\alpha^{(2)}}{2}\\right)\\left(1-\\frac{\\alpha_{V}+\\alpha_{A}}{2}\\right).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Thus, stage 2.1 takes at most $O(1/(\\eta\\alpha^{(2)}))$ steps. ", "page_idx": 40}, {"type": "text", "text": "Stage 2.1: difference between the $\\alpha$ 's By Lemma E.6, we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\left(\\alpha_{V,\\tau+1}-\\alpha_{A,\\tau+1}\\right)^{2}\\leq\\exp\\left(-2\\eta\\left(1-\\alpha_{A}\\alpha_{V}\\right)\\right)\\left(\\alpha_{V}-\\alpha_{A}\\right)^{2}+8\\eta\\left(\\left(\\delta^{(2)}\\right)^{2}+\\mathrm{Err}_{\\mathrm{small}}\\right).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "BytheAM-GMinequality,we have $\\alpha_{A}\\alpha_{V}\\le((\\alpha_{A}\\!+\\!\\alpha_{V})/2)^{2}\\le(1\\!-\\!c_{\\alpha})^{2}$ .Therefore, $1\\!-\\!\\alpha_{A}\\alpha_{V}\\geq c_{\\alpha}$ and the above inequality can be further rewritten as ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\left(\\alpha_{V,\\tau+1}-\\alpha_{A,\\tau+1}\\right)^{2}\\leq\\exp\\left(-2c_{\\alpha}\\eta\\right)\\left(\\alpha_{V}-\\alpha_{A}\\right)^{2}+8\\eta\\left((\\delta^{(2)})^{2}+\\mathrm{Err}_{\\mathrm{small}}\\right).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Thus, by (the proof of) Lemma E.9, we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n(\\alpha_{V,\\tau}-\\alpha_{A,\\tau})^{2}\\leq\\operatorname*{max}\\left\\{2(\\alpha_{V,\\mathcal{T}_{1}}-\\alpha_{A,\\mathcal{T}_{2}})^{2},\\frac{4\\left((\\delta^{(2)})^{2}+\\mathrm{Err}_{\\mathrm{small}}\\right)}{c_{\\alpha}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Stage 2.2: error decrease By Lemma E.5, we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta_{A,\\tau+1}^{2}\\leq\\exp\\left(-2\\eta\\alpha_{V}^{2}\\right)\\delta_{A,\\tau}^{2}+3\\eta\\operatorname*{max}\\left\\{\\delta^{(2)},\\mathrm{Err}_{\\mathrm{small}}\\right\\}\\mathrm{Err}_{\\mathrm{small}}}\\\\ &{\\qquad\\qquad\\leq\\exp\\left(-\\eta\\right)\\delta_{A,\\tau}^{2}+3\\eta\\operatorname*{max}\\left\\{\\delta^{(2)},\\mathrm{Err}_{\\mathrm{small}}\\right\\}\\mathrm{Err}_{\\mathrm{small}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Thus, by Lemma E.9, we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\delta_{A,\\tau}^{2}\\leq O(1)\\operatorname*{max}\\left\\{\\delta^{(2)},\\mathrm{Err}_{\\mathrm{small}}\\right\\}\\mathrm{Err}_{\\mathrm{small}}\\leq\\delta_{*}^{2},\\qquad\\forall\\tau\\geq O\\left(\\frac{\\log(1/\\delta_{*})}{\\eta}\\right).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "In other words, Stage 2.2 takes at most $O\\left(\\frac{\\log(1/\\delta_{*})}{\\eta}\\right)$ steps. ", "page_idx": 41}, {"type": "text", "text": "Stage 2.2: stability of $\\alpha_{V}\\pm\\alpha_{A}$ Recall from Lemma E.8 and Lemma E.6 that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\varepsilon_{A,\\tau+1}+\\varepsilon_{V,\\tau+1}\\right)^{2}\\leq\\exp\\left(-2\\eta\\right)\\left(\\varepsilon_{A}+\\varepsilon_{V}\\right)^{2}+8\\eta|\\varepsilon_{A}+\\varepsilon_{V}|\\varepsilon_{A}\\varepsilon_{V}+16\\eta^{2}\\varepsilon_{A}^{2}\\varepsilon_{V}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\left.8\\eta\\right|\\varepsilon_{A}+\\varepsilon_{V}|\\left(\\delta^{(2)}+\\mathrm{Err}_{\\mathrm{small}}\\right),}\\\\ &{\\left(\\alpha_{V,\\tau+1}-\\alpha_{A,\\tau+1}\\right)^{2}\\leq\\exp\\left(-2\\eta\\left(1-\\alpha_{A}\\alpha_{V}\\right)\\right)\\left(\\alpha_{V}-\\alpha_{A}\\right)^{2}+8\\eta\\left(\\delta^{(2)}+\\mathrm{Err}_{\\mathrm{small}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "We wish to maintain the induction hypotheses $\\left|\\alpha_{V}-\\alpha_{A}\\right|=\\left|\\varepsilon_{V}-\\varepsilon_{A}\\right|=\\theta_{-}$ for some $\\theta_{-}=o(1)$ and $(\\alpha_{A}+\\alpha_{V})/2\\leq1+c/\\log(1/\\delta_{*})=:1+\\theta_{+}$ ", "page_idx": 41}, {"type": "text", "text": "First, assume these conditions are true. Then, we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\left(\\alpha_{V,\\tau+1}-\\alpha_{A,\\tau+1}\\right)^{2}\\leq\\exp\\left(O(1)\\eta/\\log(1/\\delta_{*})\\right)\\left(\\alpha_{V}-\\alpha_{A}\\right)^{2}+8\\eta\\left(\\delta^{(2)}+\\mathrm{Err}_{\\mathrm{small}}\\right).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Since Stage 2.2 takes at most $O(\\log(1/\\delta_{*})/\\eta)$ steps, when the constant $c$ is small, we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\alpha_{V,\\tau}-\\alpha_{A,\\tau}\\right)^{2}\\leq2\\left((\\alpha_{V,\\mathcal{T}_{2,1}}-\\alpha_{A,\\mathcal{T}_{2,1}})^{2},+8\\left(\\delta^{(2)}+\\mathrm{Err}_{\\mathrm{small}}\\right)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\leq O(1)\\operatorname*{max}\\left\\{(\\alpha_{V,\\mathcal{T}_{2,1}}-\\alpha_{A,\\mathcal{T}_{2,1}})^{2},\\delta^{(2)},\\mathrm{Err}_{\\mathrm{small}}\\right\\}=:\\theta_{-}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "We can choose the parameters appropriately so that $\\theta_{-}<o(\\theta_{+})$ . Then, when $\\varepsilon_{A}+\\varepsilon_{V}\\,\\in\\,(-2\\theta_{+},-\\theta_{+})$ we have, for some large universal constant $C>0$ ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\varepsilon_{A,\\tau+1}+\\varepsilon_{V,\\tau+1}\\right)^{2}\\leq\\exp\\left(-2\\eta\\right)\\theta_{+}^{2}+C\\eta\\theta_{+}^{3}+C\\eta^{2}\\theta_{+}^{4}+C\\eta\\theta_{+}\\left(\\delta^{(2)}+\\mathrm{Err}_{\\mathrm{small}}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\exp\\left(-2\\eta\\right)\\theta_{+}^{2}+C\\eta\\theta_{+}^{3}+C\\eta\\theta_{+}\\theta_{-}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\theta_{+}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "This establishes the induction hypotheses on $\\alpha_{V}\\pm\\alpha_{A}$ ", "page_idx": 41}, {"type": "text", "text": "F  Stage 3: Final Convergence ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "As mentioned last section, we cannot ensure $\\alpha_{A}\\approx\\alpha_{V}$ since $\\alpha_{A}-\\alpha_{V}$ is not contractive toward the end of training. However, we can add a final rounding step and then continue train the model to recover the ground-truth with $\\varepsilon$ -eror. ", "page_idx": 42}, {"type": "text", "text": "First, we formally define our rounding procedure. Let $c>0$ be a small constant (cf. the proof of Lemma F.1). For each $k\\in[N]$ ,define ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\hat{a}^{(k)}:=\\left[a_{t}^{(k)}\\mathbb{1}\\left\\{a_{t}^{(k)}\\geq c/Q\\right\\}\\right]_{t\\in[T]}\\quad\\mathrm{~and~}\\quad a^{(*,k)}:=\\frac{\\hat{a}^{(k)}}{\\mathbf{1}^{\\top}\\hat{a}^{(k)}}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "This $A^{*}:=[\\pmb{a}^{(*,k)}]_{k\\in[N]}$ is our rounded version of $\\boldsymbol{A}$ . For the error between $A^{*}$ and $Q$ , we have the following lemma. ", "page_idx": 42}, {"type": "text", "text": "Lemma F.1 (Rounding $\\boldsymbol{A}$ 0.Lei $:\\varepsilon\\in\\left(\\Theta(Q^{2}/T^{2}),0.1\\right).$ beourtargetaccuracy.Supposethat $\\alpha_{A}=1\\!-\\!\\varepsilon_{A}$ for some $|\\varepsilon_{A}|\\leq0.1$ and $\\|\\mathbf{A}_{A}\\|_{\\mu}^{2}\\leq\\delta_{A}^{2}$ for some $0<\\delta_{A}\\ll1/(Q\\sqrt{N})$ . Then, after rounding, we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\|A_{*}-Q\\|_{\\mu}^{2}\\leq O\\left(\\frac{Q^{2}}{T^{2}}+\\delta_{A}^{2}N Q\\right).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "In particular, to achieve & accuracy in terms of $\\|\\cdot\\|_{\\mu}.$ we only need $|\\varepsilon_{A}|\\leq0.1$ and $\\delta_{A}^{2}\\leq O(\\varepsilon/\\sqrt{N Q})$ ", "page_idx": 42}, {"type": "text", "text": "Remark. In particular, this lemma implies that as long as $\\varepsilon_{A}$ is not too large, after rounding, the error depends solely on $\\|\\mathbf{A}_{A}\\|_{\\mu}^{2}$ ", "page_idx": 42}, {"type": "text", "text": "Proof. For notational simplicity, we omit the superscript $k$ for now. Write ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\pmb{a}=\\tilde{\\pmb{a}}+\\Delta_{\\pmb{a}}=\\alpha_{V}\\pmb{q}+(1-\\alpha_{V})\\frac{\\mathbf{1}}{T}+\\Delta_{\\pmb{a}}=(1-\\varepsilon_{A})\\pmb{q}+\\varepsilon_{A}\\frac{\\mathbf{1}}{T}+\\Delta_{\\pmb{a}}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Note that $\\|\\pmb{\\Delta}_{a}\\|_{\\infty}\\leq\\|\\pmb{\\Delta}_{a}\\|_{2}\\leq O(1)\\sqrt{N}\\,\\|\\pmb{\\Delta}_{A}\\|_{\\mu}\\leq O(\\sqrt{N}\\delta_{A})$ . Since all nonzero $q_{s}$ are lower bounded by $\\Omega(1/Q)$ , we have, for any $s\\in\\mathbf q$ and $t\\not\\in{\\pmb q}$ \uff0c ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle a_{s}\\geq\\frac{\\Omega(1)}{Q}-\\frac{|\\varepsilon_{A}|}{T}-O\\left(\\sqrt{N}\\delta_{A}\\right)=\\frac{\\Omega(1)}{Q},}\\\\ {\\displaystyle\\left|a_{t}\\right|\\leq\\frac{|\\varepsilon_{A}|}{T}+O\\left(\\sqrt{N}\\delta_{A}\\right)\\ll\\frac{1}{Q}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Hence, we can choose a small constant $c>0$ ,so that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\hat{a}:=[a_{t}\\mathbb{1}\\{|a_{t}|\\geq c/Q\\}]_{t\\in[T]}=(1-\\varepsilon_{A})q+\\varepsilon_{A}\\frac{\\mathbf{1}_{q}}{T}+[\\Delta_{a}]_{q},\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where for $\\pmb{\\nu}\\,\\in\\,\\mathbb{R}^{T},\\,\\pmb{\\nu}_{\\pmb{q}}\\,\\in\\,\\mathbb{R}^{T}$ is defined as $[\\nu_{t}\\mathbb{1}\\{t\\,\\in\\,\\pmb{q}\\}]_{t\\in[T]}$ here. Now, consider the difference between $\\pmb q$ and $\\hat{a}/1^{\\top}\\hat{a}$ . We have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbf{1}^{\\top}\\hat{a}=1-\\varepsilon_{A}+\\frac{\\varepsilon_{A}Q}{T}\\pm O\\left(Q\\sqrt{N}\\delta_{A}\\right),\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "and therefore, ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{a_{*}:=\\displaystyle\\frac{\\hat{a}}{\\mathbf{1}^{\\top}\\hat{a}}=\\displaystyle\\frac{(1-\\varepsilon_{A})q+\\varepsilon_{A}\\mathbf{1}_{q}/T+[\\Delta_{a}]_{q}}{(1-\\varepsilon_{A})+\\varepsilon_{A}Q/T\\pm O\\left(Q\\sqrt{N}\\delta_{A}\\right)}}\\\\ &{\\qquad\\qquad=\\displaystyle\\frac{(1-\\varepsilon_{A})q+\\varepsilon_{A}\\mathbf{1}_{q}/T+[\\Delta_{a}]_{q}}{(1-\\varepsilon_{A})}\\left(1\\pm O\\left(\\varepsilon_{A}Q/T+Q\\sqrt{N}\\delta_{A}\\right)\\right)}\\\\ &{\\qquad=\\left(q\\pm O\\left(\\varepsilon_{A}\\mathbf{1}_{q}/T+[\\Delta_{a}]_{q}\\right)\\right)\\left(1\\pm O\\left(\\varepsilon_{A}Q/T+Q\\sqrt{N}\\delta_{A}\\right)\\right)}\\\\ &{\\qquad=q\\pm O_{2}\\left(\\varepsilon_{A}Q/T+\\sqrt{Q N}\\delta_{A}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Thus, ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\|A_{*}-Q\\|_{\\mu}^{2}=\\sum_{k=1}^{N}\\mu_{k}\\left\\|a_{*}^{(k)}-\\pmb{q}^{(k)}\\right\\|^{2}=O\\left(\\frac{\\varepsilon_{A}^{2}Q^{2}}{T^{2}}+\\delta_{A}^{2}N Q\\right).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Lemma F.2. Let $\\varepsilon\\in\\big(\\Theta(1)/(K_{Q}T),0.1\\big)$ be our target accuracy. Suppose that $\\left\\Vert A-Q\\right\\Vert_{\\mu}\\leq\\delta_{A,*}\\leq$ $0.01\\varepsilon$ and $\\Vert\\Delta_{V}\\Vert_{\\mu}\\leq\\delta_{V}\\leq0.01$ at the beginning of Stage 3, and $\\|h_{V}\\|_{\\mu}\\leq c\\varepsilon\\sqrt{K_{P}}K_{Q}$ for all $\\tau$ and $a$ suffciently small constant $c$ . Then, we have $\\|V-P\\|_{\\mu}^{2}\\leq\\varepsilon$ for all $\\tau\\geq\\mathcal{T}_{2}+\\Theta(\\log(1/\\varepsilon)/\\eta)$ ", "page_idx": 43}, {"type": "text", "text": "Proof. Under the condition $\\|A-Q\\|_{\\mu}\\leq\\delta_{A,*}$ we have $\\|\\mathbf{\\Delta}\\mathbf{A}\\|_{\\mu}\\leq\\|\\mathbf{A}-\\mathcal{Q}\\|_{\\mu}\\leq\\delta_{A,*}$ and ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\alpha_{A}=\\frac{1}{K_{Q}}\\left(\\langle{\\cal A},Q\\rangle_{\\mu}-\\frac{1}{T}\\right)=\\frac{1}{K_{Q}}\\left(K_{Q}+\\langle\\Delta_{A},Q\\rangle_{\\mu}\\right)=1\\pm O\\left(\\sqrt{Q}\\delta_{A,*}\\right).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Recall that we only train $V$ in Stage 3. Note that by Lemma C.3. ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|V-P\\|_{\\mu}^{2}=\\left\\|\\tilde{V}-P\\right\\|_{\\mu}^{2}+\\|\\Delta_{V}\\|_{\\mu}^{2}=(1-\\alpha_{V})^{2}K_{P}+\\|\\Delta_{V}\\|_{\\mu}^{2}\\leq(1-\\alpha_{V})^{2}+\\|\\Delta_{V}\\|_{\\mu}^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Hence, to get $\\varepsilon$ accuracy, it suffices to have $(1-\\alpha_{V})^{2}\\leq\\varepsilon/2$ and $\\|\\mathbf{A}_{V}\\|_{\\mu}^{2}\\leq\\varepsilon/2$ ", "page_idx": 43}, {"type": "text", "text": "First, for $\\alpha_{V}$ , by Lemma C.5 and $\\eta_{V}=\\eta/K_{Q}$ , we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha_{V,\\tau+1}=\\alpha_{V,\\tau}+\\eta_{V}K_{Q}\\left(1-\\alpha_{V}\\alpha_{A}\\right)\\alpha_{V}+\\eta_{V}\\frac{1-\\alpha_{V}}{T}-\\eta_{V}\\alpha_{V}\\left\\|\\mathbf{A}_{A}\\right\\|_{\\mu}^{2}-\\frac{\\eta_{V}}{K_{P}}\\left\\langle h_{V,\\tau},P\\right\\rangle_{\\mu}}\\\\ &{\\qquad=\\alpha_{V,\\tau}+\\eta\\left(1-\\alpha_{V}\\right)\\alpha_{V}\\pm\\eta\\mathcal{O}\\left(\\frac{1}{K_{Q}T}+\\delta_{A,*}\\alpha_{V}\\right)\\pm\\eta\\mathcal{O}\\left(\\frac{\\left\\langle h_{V,\\tau},P\\right\\rangle_{\\mu}}{K_{P}K_{Q}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Hence, ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(1-\\alpha_{V,\\tau+1}\\right)^{2}=\\left(\\left(1-\\eta\\alpha_{V}\\right)\\left(1-\\alpha_{V,\\tau}\\right)\\pm\\eta O\\left(\\displaystyle\\frac{1}{K_{Q}T}+\\delta_{A,*}\\alpha_{V}\\right)\\pm\\eta O\\left(\\displaystyle\\frac{\\left\\langle h_{V,\\tau},P\\right\\rangle_{\\mu}}{K_{P}K_{Q}}\\right)\\right)^{2}}\\\\ &{\\qquad\\qquad\\leq\\exp\\left(-2\\eta\\alpha_{V}\\right)\\left(1-\\alpha_{V,\\tau}\\right)^{2}+\\eta O\\left(\\displaystyle\\frac{1}{K_{Q}T}+\\delta_{A,*}\\alpha_{V}\\right)+\\eta O\\left(\\displaystyle\\frac{\\left\\langle h_{V,\\tau},P\\right\\rangle_{\\mu}}{K_{P}K_{Q}}\\right)}\\\\ &{\\qquad\\qquad\\leq\\exp\\left(-\\eta\\right)\\left(1-\\alpha_{V,\\tau}\\right)^{2}+0.1\\eta\\varepsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "For $\\|\\Delta_{A}\\|_{\\mu}^{2}$ , by (the proof of) Lemma E.5, we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\Delta_{V,\\tau+1}\\right\\|_{\\mu}^{2}\\leq\\exp\\left(-2\\eta\\alpha_{A}^{2}\\right)\\left\\|\\Delta_{V,\\tau}\\right\\|_{\\mu}^{2}+3\\eta\\left(\\left\\|\\Delta_{V,\\tau}\\right\\|_{\\mu}+\\eta\\frac{\\left\\|h_{V}\\right\\|_{\\mu}}{\\sqrt{K_{P}}K_{Q}}\\right)\\frac{\\left\\|h_{V}\\right\\|_{\\mu}}{\\sqrt{K_{P}}K_{Q}}}\\\\ &{\\qquad\\qquad\\leq\\exp\\left(-\\eta\\right)\\left\\|\\Delta_{V,\\tau}\\right\\|_{\\mu}^{2}+0.1\\eta\\varepsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Thus, by Lemma E.9, we have $\\left(1-\\alpha_{V}\\right)^{2}\\leq\\varepsilon/2$ and $\\|\\mathbf{A}_{V}\\|_{\\mu}^{2}\\leq\\varepsilon/2$ for all $\\tau\\geq\\mathcal{T}_{2}{+}\\Theta\\left(\\log(1/\\varepsilon)/\\eta\\right)$ \uff1a\u53e3 ", "page_idx": 43}, {"type": "text", "text": "Corollary F.3. Let $\\varepsilon\\in(\\Theta(1)/(K_{Q}T),0.1)$ be our target accuracy. Suppose that $\\alpha_{A}\\in(0.9,1.1)$ \uff0c $\\|\\mathbf{A}_{A}\\|_{\\mu}^{2}\\leq O(\\varepsilon/(Q{\\sqrt{N}}))$ , and $\\|\\mathbf{A}_{V}\\|_{\\mu}^{2}\\leq0.01$ . Then with poly $(N,Q,1/\\varepsilon)$ samples, we have with high probability that $\\|A-Q\\|_{\\mu}^{2}\\leq\\varepsilon$ and $\\|V-P\\|_{\\mu}^{2}\\leq\\varepsilon$ after Stage $^3$ which takes ${\\cal O}(\\log(1/\\varepsilon)/\\eta)$ steps. ", "page_idx": 43}, {"type": "text", "text": "Proof. It suffices to combine the previous two lemmas, the concentration results in Section B, and apply union bound. \u53e3 ", "page_idx": 43}, {"type": "text", "text": "G Proof of the main theorem ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "In this section, we combine the results from the last three sections and prove the following formal version of Theorem 3.1. ", "page_idx": 43}, {"type": "text", "text": "Theorem G.1. Let $\\varepsilon\\;>\\;0$ be our target accuracy and $\\mathcal{T}_{1}\\;=\\;\\operatorname*{min}\\{\\tau\\;\\geq\\;0\\;:\\;\\operatorname*{max}\\{\\alpha_{V,\\tau},\\alpha_{A,\\tau}\\}\\;\\geq\\;$ $\\Theta(1/(Q N))\\}$ :We can choose the hyperparameters in Algorithm $^{\\,l}$ suchthat within $O\\left(\\log(T)/\\eta_{1}+1/(\\eta\\alpha^{(2)})+\\log(1/\\varepsilon)/\\eta\\right)$ steps, we have $\\|A-Q\\|_{\\mu}^{2}\\,\\leq\\,\\varepsilon$ and $\\|V-P\\|_{\\mu}^{2}\\;\\leq\\;\\varepsilon$ with probability at least $1-\\delta$ and the number of samples used before and after $\\mathcal{T}_{1}$ are poly $(T,\\delta)$ and poly $(N,Q,1/\\varepsilon,\\log T,\\delta)$ ,respectively. ", "page_idx": 43}, {"type": "text", "text": "Proof. The results for Stage 1 follow directly from Lemma D.1. Now, consider the results for Stage 2 and 3. First, by Corollary F.3, it suffices to make sure at time $\\mathcal{T}_{2}$ , we have $\\alpha_{A}\\in(0.9,1.1)$ $\\|\\Delta_{A}\\|_{\\mu}^{2}\\leq O(\\varepsilon/(Q\\sqrt{N}))$ and $\\|\\Delta_{V}\\|_{\\mu}^{2}\\leq0.01$ (with high probability). By Lemma D.1, we know the following hold at time $\\mathcal{T}_{1}$ w.h.p: ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\alpha_{A},\\alpha_{V}=\\Theta(1/(Q N))\\quad\\mathrm{and}\\quad\\|\\Delta_{A}\\|_{\\mu}^{2}\\,,\\|\\Delta_{V}\\|_{\\mu}^{2}\\leq O(1/T).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Therefore, by Lemma E.1, if we choose the threshold to be $\\Theta(1/(Q^{2}N))$ , then after thresholding andprjetion, we have $a_{t}^{(k)}=O(1/T)$ for all $t\\not\\in{\\pmb q}^{(k)}$ ThusbyLmaandLmmai $\\mathrm{poly}(Q,N,1/\\varepsilon,\\log(T))$ samples, we have with high probability that $a_{t}^{(k)}=O(1/T)$ for all $t\\not\\in{\\pmb q}^{(k)}$ holds and $\\left\\|\\hat{\\pmb{h}}_{A}\\right\\|_{\\mu}$ satisfes the requirements in Lemma E.10 throughout Stage 2. Thus, by Lemma E.10 with $\\delta_{*}^{2}=O(\\varepsilon/Q\\sqrt{N})$ , at the end of Stage 2, we have with high probability that $\\alpha_{A}\\,\\in\\,(0.9,1.1)$ $\\|\\Delta_{A}\\|_{\\mu}^{2}\\leq O(\\varepsilon/(Q{\\sqrt{N}}))$ and $\\|\\Delta_{V}\\|_{\\mu}^{2}\\leq0.01$ . When combined with Corollary F.3, this completes the proof. \u53e3 ", "page_idx": 44}, {"type": "text", "text": "H  Transfer learning ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Lemma H.1 (Initialization). Suppose that we have learned $\\hat{P}$ and $\\left\\langle{\\hat{P}},P\\right\\rangle_{\\mu}\\;\\geq\\;2\\left\\|\\mu\\right\\|^{2},\\;\\left\\|{\\hat{P}}\\right\\|_{\\mu}^{2}\\;=$ $\\Theta(1)\\left\\|P\\right\\|_{\\mu}^{2}$ .Let $\\pmb{V}=\\theta\\hat{\\pmb{P}}+(1-\\theta)\\pmb{\\mu}\\mathbf{1}^{\\top}$ forsome $\\theta\\in(0,1)$ .Wehave ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\alpha_{V}\\in\\left[\\Theta\\left(\\frac{\\theta}{N K_{P}}\\right),\\Theta\\left(\\theta\\right)\\right]\\quad a n d\\quad\\left\\|\\mathbf{A}_{V}\\right\\|_{\\mu}^{2}\\leq\\Theta(1)\\theta^{2}K_{P}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Proof. First, consider $\\Delta_{V}$ .Since $\\|\\Delta_{V}\\|_{\\mu}$ is the distance to a projection, we have ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\Delta_{V}\\right\\|_{\\mu}^{2}\\leq\\left\\|V-\\left(\\theta P+(1-\\theta)\\mu\\mathbf{1}^{\\top}\\right)\\right\\|_{\\mu}^{2}=\\theta^{2}\\left\\|\\hat{P}-P\\right\\|_{\\mu}^{2}\\leq\\Theta(1)\\theta^{2}\\left\\|P\\right\\|_{\\mu}^{2}=\\Theta(1)\\theta^{2}K_{P}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "For $\\alpha_{V}$ , we compute ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\langle V,P\\rangle_{\\mu}-\\|\\mu\\|^{2}=\\theta\\left(\\left\\langle\\hat{P},P\\right\\rangle_{\\mu}-\\|\\mu\\|^{2}\\right)\\geq\\theta\\left\\|\\mu\\right\\|^{2}\\geq\\Theta(1)\\theta/N.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "In particular, this implies $\\begin{array}{r}{\\alpha_{V}\\geq\\Theta\\left(\\frac{\\theta}{N K_{P}}\\right)}\\end{array}$ ", "page_idx": 44}, {"type": "text", "text": "Lemma H.2 (First gradient step). Let $A\\ =\\ 11^{\\top}/T$ and $V$ given by Lemma H.1. Let $\\varepsilon_{\\mathrm{Tmp}}~\\leq$ $O(1/(N K_{P}))$ . Run one gradient step with po $\\mathrm{ly}(N,Q,\\log T,1/\\varepsilon_{\\mathrm{Tmp}})$ samples and $\\eta=1$ , remove all entries of ${\\pmb a}^{(k)}$ with $\\lvert a_{t}^{(k)}\\rvert\\leq\\Theta(\\theta/(N K_{P}))$ and the replace ${\\pmb a}^{(k)}$ with the projection $(I-1\\mathbf{1}^{\\top}/T)\\mathbf{{a}}^{(k)}$ With high probability, we have $\\alpha_{A}=(1\\pm O(\\varepsilon_{\\mathrm{Tmp}}))\\alpha_{V}$ and $\\begin{array}{r}{\\|\\Delta_{A}\\|_{\\mu}^{2}\\leq O\\,\\bigg(\\frac{\\varepsilon_{\\mathrm{Tmp}}^{2}\\alpha_{V}^{2}}{Q}+\\frac{1}{T}\\bigg).}\\end{array}$ ", "page_idx": 44}, {"type": "text", "text": "Proof. The proof idea is essentially the same as Lemma E.3, though the reinitialization of the first layer allows better estimations in several places. Recall that for each $s\\in[T]$ ,wehave ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\partial_{a_{s}^{(k)}}l=\\mathbb{1}\\{x_{T+1}=k\\}(V e_{x_{s}})^{\\top}\\left(V X\\mathbf{1}/T-e_{x_{o}}\\right),}\\\\ &{\\mathbb{E}\\,\\partial_{a_{s}^{(k)}}l=\\mu_{k}K_{V}/T-\\mu_{k}K_{V P}q_{s}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Also recall from Lemma C.3 that $K_{V P}=\\alpha_{V}K_{P}$ and $K_{V}=\\alpha_{V}^{2}K_{P}+\\|\\mathbf{A}_{V}\\|_{\\mu}^{2}$ . For any $s\\in{\\pmb q}^{(k)}$ ,we have ", "page_idx": 44}, {"type": "equation", "text": "$$\n-\\mathbb{E}\\partial_{a_{s}^{(k)}}l=\\mu_{k}\\left(\\alpha_{V}K_{P}q_{s}-\\frac{\\alpha_{V}^{2}K_{P}+\\|\\Delta_{V}\\|_{\\mu}^{2}}{T}\\right)\\geq\\frac{\\mu_{k}\\alpha_{V}K_{P}q_{s}}{2}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where the inequality comes from $\\mu_{k}\\alpha_{V}K_{P}q_{s}\\ge\\Omega(\\alpha_{V}/N^{2}/Q)\\gg1/T$ . Meanwhile, for any $s\\notin[T]$ \uff0c we have $\\mathbb{E}\\,\\partial_{a_{s}^{(k)}}l=O(1/T)$ ", "page_idx": 44}, {"type": "text", "text": "Meanwhile, for any $s\\in[T]$ we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\left|\\partial_{a_{s}^{(k)}}l\\right|\\leq\\left|(V e_{x_{s}})^{\\top}V X\\mathbf{1}/T\\right|+\\left|(V e_{x_{s}})^{\\top}e_{x_{o}}\\right|\\leq\\frac{1}{T}\\sum_{t=1}^{T}\\left|\\left\\langle V_{\\vdots,x_{s}},V_{\\vdots,x_{t}}\\right\\rangle\\right|+\\left|V_{x_{o},x_{s}}\\right|\\leq O(N).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Thus, by some standard concentration argument similar to the one in Lemma E.3, we can show that withpoly $(N,Q,1/\\alpha_{V},1/\\varepsilon_{\\mathrm{Tmp}},\\log T)$ samples, we can make sure with high probability, ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{\\partial_{a_{t}^{(k)}}^{(B_{\\tau})}l=\\left(1\\pm\\varepsilon_{\\mathrm{Tmp}}\\right)\\mathbb{E}\\,\\partial_{a_{t}^{(k)}}l=\\left(1\\pm\\varepsilon_{\\mathrm{Tmp}}\\right)\\mu_{k}\\alpha_{V}K_{P}q_{s}\\qquad}&&{\\forall t\\in q^{(k)},}\\\\ &{\\partial_{a_{t}^{(k)}}^{(B_{\\tau})}l=O\\left(\\displaystyle\\frac{1}{T}+\\varepsilon_{\\mathrm{Tmp}}\\frac{\\alpha_{V}K_{P}}{N Q}\\right)=O\\left(\\varepsilon_{\\mathrm{Tmp}}\\frac{\\alpha_{V}K_{P}}{N Q}\\right)\\qquad}&&{\\forall t\\notin q^{(k)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Thus, after one gradient step with $\\eta=1$ , we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\boldsymbol{a}_{t}^{(k)}=(1\\pm\\varepsilon_{\\mathrm{Tmp}})\\alpha_{V}\\boldsymbol{q}_{t}^{(k)}\\qquad\\qquad\\qquad}&{\\forall t\\in\\boldsymbol{q}^{(k)},}\\\\ &{\\boldsymbol{a}_{t}^{(k)}={O}\\left(\\varepsilon_{\\mathrm{Tmp}}\\frac{\\alpha_{V}}{Q}\\right)\\qquad\\qquad}&{\\forall t\\notin\\boldsymbol{q}^{(k)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Recall from Lemma H.1 that $\\alpha_{V}\\in[\\Theta(\\theta/(N K_{P})),\\Theta(\\theta)]$ . Hence, we can choose the threshold $\\lambda_{0}$ to be $\\Theta(\\theta/(N K_{P}))$ and $\\varepsilon_{\\mathrm{Tmp}}\\leq1/(N K_{P})$ so that ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\hat{a}^{(k)}=\\left[a_{t}^{(k)}\\mathbb{1}\\{a_{t}^{(k)}\\geq\\lambda_{0}\\}\\right]_{t\\in[T]}=\\left[(1\\pm\\varepsilon_{\\mathrm{Tmp}})\\alpha_{V}q_{t}^{(k)}\\mathbb{1}\\{t\\in q^{(k)}\\}\\right]_{t\\in[T]}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Now, set ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\pmb{a}^{(k)}\\gets(\\pmb{I}-\\pmb{1}\\mathbf{1}^{\\top}/T)\\hat{a}^{(k)}=(1\\pmb{\\Sigma}_{\\infty}(\\varepsilon_{\\mathrm{Tmp}}))\\alpha_{V}\\pmb{q}^{(k)}+O_{\\infty}(1)\\frac{\\mathbf{1}}{T}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Note that this implies that after the first step, we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\alpha_{A}=\\frac{\\sum_{k}\\mu_{k}\\left\\langle a^{(k)},q^{(k)}\\right\\rangle-1/T}{K_{Q}}=\\frac{\\alpha_{V}(1\\pm O(\\varepsilon_{\\mathrm{Tmp}}))\\left\\|Q\\right\\|_{\\mu}^{2}+O(Q/T)}{K_{Q}}=(1\\pm O(\\varepsilon_{\\mathrm{Tmp}}))\\alpha_{V},\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "and ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\|\\mathbf{A}_{A}\\|_{\\mu}^{2}\\leq\\sum_{k=1}^{N}\\mu_{k}\\left\\|a^{(k)}-\\left(\\alpha_{V}q^{(k)}+(1-\\alpha_{V})\\mathbf{1}/T\\right)\\right\\|_{\\mu}^{2}\\leq O\\left(\\frac{\\varepsilon_{\\mathtt{T m p}}^{2}\\alpha_{V}^{2}}{Q}+\\frac{1}{T}\\right).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Theorem H.3 (Main theorem for transfer learning). Let $\\varepsilon>0$ be our target accuracy. Consider the same setting of Lemma $H.l$ and Lemma H.2. Choose $\\theta=\\sqrt{O(1/\\log(1/\\varepsilon))}$ and $\\begin{array}{r}{\\varepsilon_{\\mathrm{Tmp}}=O\\,\\bigg(\\frac{1}{\\log(1/\\varepsilon)}\\bigg).}\\end{array}$ Then, after one step of update on $\\boldsymbol{A}$ as in Lemma $H.2$ $\\boldsymbol{A}$ and $V$ satisfies the conditions of Lemma $E.I O$ and thereforewe can learn $(P,Q)$ to $\\varepsilon$ -accuracy using poly $(N,Q,1/\\varepsilon,1/\\delta)$ sampleswith probability at least $1-\\delta$ within poly $(N,Q,1/\\varepsilon,1/\\delta)$ steps. ", "page_idx": 45}, {"type": "text", "text": "1  Additional Experiment ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "For all our experiments, we use Numpy and run on a normal laptop which takes about 20 minutes. ", "page_idx": 45}, {"type": "text", "text": "Setup. In all our experiments, we choose $T=5000,Q=2,N=3$ . The architecture is ", "page_idx": 45}, {"type": "equation", "text": "$$\n{\\pmb F}({\\pmb x},x_{T+1};{\\pmb V},{\\pmb A}):={\\pmb V}{\\pmb X}\\left({\\pmb I}_{T}{\\pmb A}{\\pmb e}_{x_{T+1}}\\right)=:{\\pmb V}X{\\pmb A}^{(x_{T+1})},\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "and the data model is the SCB (1) data-generating model. The batch size is $B\\,=\\,64$ and the regularization hyperparameter is $\\lambda=1\\mathrm{e}{-5}$ . The total time is $\\mathcal{T}=1000$ iterations where stage 1 takes $\\tau\\in[0,400]$ with learning rate $\\eta_{1}=0.01$ . After $\\tau>400$ , we use $\\eta_{2}=0.005$ for further improvement (stages 2 and 3). ", "page_idx": 45}, {"type": "text", "text": "Hyperparameter selection. Due to the limitation of computational resources, we do experiments With $N\\,\\in\\,[3,20]$ for real-world batched gradient experiments, and $N\\,\\in\\,\\{100,500\\},T\\,=\\,100000$ experiments by using Gaussian noise SGD simulations based on the dynamics of Lemma C.5 and C.6.As $T$ needs to scalewith $N$ polynomially, it would be beyond our computation capability to experiment with larger $N$ . As for other hyperparameters, $\\lambda$ is chosen based on our theoretical results (Theorem 3.1 and G.1): $\\lambda\\sim\\Theta(\\epsilon K_{P}/\\dot{Q^{2}}N^{3})$ in Lemma E.4. The batch size can be chosen from standard $\\{64,128,256\\}$ , while smaller batch size will lead to divergence for both SGD and regularized GD. $\\eta$ is chosen as the largest learning rate without divergence. ", "page_idx": 45}, {"type": "text", "text": "", "page_idx": 46}, {"type": "text", "text": "Besides the original parameters, we consider the approximation error after the normalization step (stage 3) in real-time. That is thresholding and normalizing the attention block ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\forall k\\in[n],\\hat{a}^{(k)}=[a_{t}^{(k)}\\mathbb{1}\\{a_{t}^{(k)}\\geq\\Omega(1/Q)\\}]_{t}.\\;\\;a^{(k)}\\leftarrow\\hat{a}^{(k)}/\\mathbf{1}^{\\top}\\hat{a}^{(k)}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "and we do further gradient descent to recover $V$ . In this case, we will directly use the linear regression solution on population loss for $V$ ", "page_idx": 46}, {"type": "text", "text": "Here we report in addition: (1) original signal projection on the population process trajectory $\\alpha_{A},\\alpha_{V}$ and the distance to the trajectory $\\Delta_{A},\\Delta_{V}$ . (2) The approximation error/similarity before and after normalization for both SGD and proximal gradient descent. We conclude that in all metrics proximal gradient descent performs better than the vanilla gradient descent with a small batch size (when the noise is large). ", "page_idx": 46}, {"type": "image", "img_path": "PukaVAwYBo/tmp/e9393ed9ce6752436ca62a55fb4b0aa8778ccd65c67ca1add4c74c53bd49f9dc.jpg", "img_caption": ["Figure 3: Signals $\\alpha_{A},\\alpha_{V}$ and the distance to population process $\\Delta_{A},\\Delta_{V}$ . For the SGD, the distance to the population process of the attention matrix $\\boldsymbol{A}$ keeps growing and dominates the signal term. That explains the failure to learn the correct attention pattern, which leads to saturation of the signal. In comparison, our proximal methods dramatically help reduce the gradient noise and keep close to the population process. Though $\\|\\mathbf{A}_{A}\\|$ eventually grows up due to the bias of the gradient estimate (the original signal growth is also slowed down), after normalization it can still approximately learn the correct pattern. Both $\\|\\Delta_{V}\\|$ stay small empirically. "], "img_footnote": [], "page_idx": 46}, {"type": "image", "img_path": "PukaVAwYBo/tmp/3c086b19a007a3118018c123c5ef7fdb8a1c5ad8ca481c6ba90c991c784f0404.jpg", "img_caption": ["Figure 4: Similarity with the ground-truth. The figure shows after Stage 1, normalization helps further improve the solution of the proximal method. Meanwhile, with or without normalization, our proximal method always outperforms the vanilla SGD, which fails to recover the ground-truth. "], "img_footnote": [], "page_idx": 46}, {"type": "text", "text": "We tried different orders of state number $N$ and show that $\\ell_{1}$ regularization is necessary and outperforms SGD when batch size is small (gradient noise is large). Due to computation limitation, we experiment with real batched gradient on $N\\leq20$ $T\\le5000$ , and do SGD simulation by combining our population gradient $^+$ Gaussian noise (to mimic the batch gradient noise) for $N\\leq500,T\\leq100000.$ ", "page_idx": 46}, {"type": "text", "text": "We also corroborate our previous experiments with the new test loss plot to show the convergence of training. Note that since there are multiple global minima for the linear attention, $\\ell_{1}$ regularized dynamics eventually will make $A$ and $Q$ deviate from the ground-truth while representing the same function. That is why the loss converges but the distance to the ground-truth increases after some point, making the final normalization step essential to recover the ground-truth. Another point is that according to our theory, the regularization will eventually distort the learned pattern when trained for too many iterations. Empirically, the loss also increases a little after it converges. Therefore, we must stop early and normalize before the distortion happens. ", "page_idx": 46}, {"type": "text", "text": "", "page_idx": 47}, {"type": "image", "img_path": "PukaVAwYBo/tmp/7929a6fa6995a1a83e66d00e5b12a5aa17daf7c8522a97b59d63ff89c54f7967.jpg", "img_caption": ["Figure 5: Convergence analysis. We plot the distance to the ground-truth and the test loss for $N=3$ , 10, 20 (from top to bottom). It shows that when gradient noise is large, $\\ell_{1}$ regularized algorithm with normalization and early-stopping can almost perfectly recover the ground-truth (the star), while SGD struggles to learn the target function. Figure 3 (Appendix I) also shows when the gradient noise is large, SGD never learns ground-truth $Q$ even with normalization. "], "img_footnote": [], "page_idx": 47}, {"type": "image", "img_path": "PukaVAwYBo/tmp/ebb26b0d1bd28c99c6eb5707cce5c935a54970f61ae7585a6b63b1193ca6baae.jpg", "img_caption": ["Figure 6: Simulation with larger $N$ and $T$ .We simulate the ${\\mathrm{SGD}}/\\ell_{1}$ regularized dynamics by replacing the batched noise with Gaussian noise in the dynamics formula in Lemma C.5 and C.6. The gaussian noise variance scales with the inverse of batch size. The experiments show that the conclusions drawn from the small $N$ cases still hold in those simulations: when $T=100000$ \uff0c $N=100/500$ our $\\ell_{1}$ regularized algorithm can recover the ground-truth since the distance to the population trajectory $(\\Delta_{A},\\Delta_{V})$ stays very small, while the error along SGD trajectories quickly increases with the same batch size. "], "img_footnote": [], "page_idx": 47}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: We accurately summarized our claims and contributions in the abstract as well as the introduction. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 48}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Justification: It is included in Appendix A. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should refect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should refect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 48}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: The assumptions and conditions are included in the Setup section 2. Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 49}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: All information is included in Section 5 and Appendix I. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. () If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 49}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Justification: We will upload the codes in the supplementary materials. Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https : / /nips . CC / public/guides /CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https:/ /nips.cc/public/guides/CodeSubmissionPolicy)formore details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 50}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: All information is included in Section 5 and Appendix I Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 50}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 50}, {"type": "text", "text": "Answer: [No] ", "page_idx": 50}, {"type": "text", "text": "Justification: Our experiments are for dynamics simulations. We do not include performance/accuracy results. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \" Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 50}, {"type": "text", "text": "", "page_idx": 51}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 51}, {"type": "text", "text": "Justification: All information is included in Section 5 and Appendix I. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 51}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPs Code of Ethics https: / /neurips.cc/public/EthicsGuidelines? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 51}, {"type": "text", "text": "Justification: This work is a pure theory paper without potential harmful effect. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 51}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 51}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 51}, {"type": "text", "text": "Justification: This work is a theoretical paper without direct societal impact. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 51}, {"type": "text", "text": "", "page_idx": 52}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 52}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 52}, {"type": "text", "text": "Justification: This paper is a theoretical work without such risks. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 52}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 52}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 52}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paper s withcode . com/ dataset s has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 52}, {"type": "text", "text": "\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset'screators. ", "page_idx": 53}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 53}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 53}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 53}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 53}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 53}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 53}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 53}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 53}]