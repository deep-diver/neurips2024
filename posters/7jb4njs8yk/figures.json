[{"figure_path": "7Jb4NJS8Yk/figures/figures_1_1.jpg", "caption": "Figure 1: A new paradigm for building AI Diplomacy agent.", "description": "This figure illustrates the evolution of AI Diplomacy agents from 2019 to 2024.  In 2019, DipNet used reinforcement learning (RL) and human data for no-press Diplomacy (no negotiation).  In 2022, Cicero incorporated negotiation using RL and still relied on human data.  In 2024, Richelieu represents a new paradigm, leveraging LLMs (Large Language Models), self-play for evolution, and eliminating the need for human data. This signifies a significant advancement in AI's ability to handle complex social interactions and strategic planning in the game of Diplomacy.", "section": "1 Introduction"}, {"figure_path": "7Jb4NJS8Yk/figures/figures_3_1.jpg", "caption": "Figure 2: The framework of the proposed LLM-based-agent, Richelieu. It can explicitly reason social beliefs, propose sub-goals with reflection, negotiate with others, and take actions to master diplomacy. It augments memories by self-play games for self-evolving without any human annotation.", "description": "This figure illustrates the architecture of the Richelieu agent.  The agent uses a memory module to store past experiences, which are used to inform its social reasoning, strategic planning (with reflection), negotiation, and actions. The agent's memory is augmented via self-play, allowing for self-evolution without the need for human-provided data.  The system components interact to generate actions in the Diplomacy game environment.", "section": "4 Self-Evolving LLM-based Diplomat"}, {"figure_path": "7Jb4NJS8Yk/figures/figures_5_1.jpg", "caption": "Figure 3: The social reasoning flow for negotiation. With the received words and memory, the agent will reason by answering the following questions: \u201cIs the opponent lying?", "description": "This figure illustrates the social reasoning process of the Richelieu agent during the negotiation phase.  It shows a flowchart outlining how the agent processes received words from other players, combines this information with its memory of past interactions, and uses this knowledge to determine the true intentions of the other players and respond appropriately.  The flowchart begins with the agent receiving words from an opponent. Based on these words, the agent determines if the opponent is an enemy and whether or not deception is involved. This leads to one of four possible actions:  words for cooperation, words for fake cooperation, words to change the other's intentions, or confrontation. These four actions are further refined based on whether or not the agent deems the other player an enemy, ultimately leading to a choice of cooperation or a change in relationship with that player.  The entire process is informed by the agent's memory, represented in the figure by a symbol of a memory storage unit.  This memory allows the agent to leverage past experiences to influence its current decision making and refine its strategies. The flowchart thus represents the agent's capacity for complex social reasoning, adapting its communication style based on its perception of the other player's intentions, and leveraging its historical knowledge for strategic decision-making.", "section": "4.3 Negotiator and Actor"}, {"figure_path": "7Jb4NJS8Yk/figures/figures_7_1.jpg", "caption": "Figure 4: The relative scores among 7 different agents when massively playing on the no-press setting. Each point shows the ratio of the model's score on the vertical axis to the score gained by the model on the horizontal axis.", "description": "This figure presents a heatmap showing the relative performance of seven different AI agents playing a simplified version of the Diplomacy game (no-press diplomacy, meaning no negotiation).  The color intensity represents the ratio of each agent's score compared to other agents.  Darker colors indicate that the agent significantly outperformed others, while lighter colors suggest a closer performance.", "section": "5.2 Results"}, {"figure_path": "7Jb4NJS8Yk/figures/figures_8_1.jpg", "caption": "Figure 5: Richelieu modules benefit different LLMs. The solid line represents the experimental results for Richelieu, while the dashed line corresponds to Cicero. Different colors are used for different LLMs. The horizontal axis represents the logarithm of the number of training sessions, and the vertical axis denotes the rate.", "description": "This figure shows the performance of Richelieu (solid lines) compared to Cicero (dashed lines) across different LLMs (GPT-4, ERNIE Bot, Spark Desk, and Llama 3).  Each LLM is represented by a different color.  The x-axis shows the number of training sessions (log scale), and the y-axis displays the win rate, win & most SC rate, and defeated rate for each model/LLM combination. The graph illustrates how Richelieu's performance improves with more training sessions, and how this improvement varies depending on the underlying LLM used.", "section": "5.2 Results"}, {"figure_path": "7Jb4NJS8Yk/figures/figures_18_1.jpg", "caption": "Figure 6: Case of self-playing before and after comparison.", "description": "This figure shows two scenarios of the game of Diplomacy, one before and one after the model is trained using self-play.  In the first scenario (Case 1), Richelieu (France) misses the opportunity to ally with Austria against the stronger Russia, eventually leading to France's defeat. In the second scenario (Case 2), after self-play, Richelieu recognizes the long-term threat of Russia and forms an alliance with Austria, achieving a more favorable outcome.  The change highlights the model's improved strategic thinking and ability to learn from past experiences via self-play.", "section": "4.4 Memory Management and Evolution in Self-Play Games"}, {"figure_path": "7Jb4NJS8Yk/figures/figures_18_2.jpg", "caption": "Figure 6: Case of self-playing before and after comparison.", "description": "This figure shows two scenarios of a game. In the first scenario (Case 1), the agent without self-play memory makes a decision that results in Russia winning the game by ignoring long-term gains. In the second scenario (Case 2), after self-play, the agent considers long-term gains, forming an alliance to counter Russia, leading to a better outcome where France has most supply centers. This illustrates the impact of self-play on improving long-term strategic thinking in the AI agent.", "section": "4.4 Memory Management and Evolution in Self-Play Games"}, {"figure_path": "7Jb4NJS8Yk/figures/figures_19_1.jpg", "caption": "Figure 7: An example case of avoiding being deceived by other countries during negotiations.", "description": "This figure shows a scenario in the Diplomacy game where England deceives Germany into an alliance to attack France.  Richelieu, controlling Germany, suspects this deception and strategically seeks an alliance with France to counter England's plan. The map illustrates the troop movements and the text boxes display the negotiations, highlighting Richelieu's ability to detect and counter deceptive strategies.", "section": "4.3 Negotiator and Actor"}]