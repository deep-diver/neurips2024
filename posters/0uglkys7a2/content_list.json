[{"type": "text", "text": "Maximizing utility in multi-agent environments by anticipating the behavior of other learners ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Angelos Assos Yuval Dagan MIT CSAIL UC Berkeley Cambridge, MA Berkeley, CA assos@mit.edu yuvaldag@berkeley.edu ", "page_idx": 0}, {"type": "text", "text": "Constantinos Daskalakis   \nMIT CSAIL   \nCambridge, MA   \ncostis@mit.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Learning algorithms are often used to make decisions in sequential decision-making environments. In multi-agent settings, the decisions of each agent can affect the utilities/losses of the other agents. Therefore, if an agent is good at anticipating the behavior of the other agents, in particular how they will make decisions in each round as a function of their experience that far, it could try to judiciously make its own decisions over the rounds of the interaction so as to influence the other agents to behave in a way that ultimately beneftis its own utility. In this paper, we study repeated two-player games involving two types of agents: a learner, which employs an online learning algorithm to choose its strategy in each round; and an optimizer, which knows the learner\u2019s utility function and the learner\u2019s online learning algorithm. The optimizer wants to plan ahead to maximize its own utility, while taking into account the learner\u2019s behavior. We provide two results: a positive result for repeated zero-sum games and a negative result for repeated generalsum games. Our positive result is an algorithm for the optimizer, which exactly maximizes its utility against a learner that plays the Replicator Dynamics \u2014 the continuous-time analogue of Multiplicative Weights Update (MWU). Additionally, we use this result to provide an algorithm for the optimizer against MWU, i.e. for the discrete-time setting, which guarantees an average utility for the optimizer that is higher than the value of the one-shot game. Our negative result shows that, unless $\\scriptstyle\\mathrm{P=NP},$ there is no Fully Polynomial Time Approximation Scheme (FPTAS) for maximizing the utility of an optimizer against a learner that best-responds to the history in each round. Yet, this still leaves open the question of whether there exists a polynomial-time algorithm that optimizes the utility up to $o(T)$ . ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "With the increased use of learning algorithms as a means to optimize agents\u2019 objectives in unknown or complex environments, it is inevitable that they will be used in multi-agent settings as well. This encompasses scenarios where multiple agents are repeatedly taking actions in the same environment, and their actions influence the payoffs of the other players. For example, participants in repeated online auctions use learning algorithms to bid, and the outcome, who gets to win and how much they pay, depends on all the bids (see e.g. [39, 40]). Other examples arise in contract design (see e.g. [30]), ", "page_idx": 0}, {"type": "text", "text": "Bayesian persuasion (see e.g. [19]), competing retailers that use learning algorithms to set their prices (see e.g. [25, 14]), and more. ", "page_idx": 1}, {"type": "text", "text": "It is thus natural to contemplate the following: in a setting where multiple agents take actions repeatedly, using their past observations of the other players\u2019 actions to decide on their future actions, what is the optimal strategy for an agent? This question is rather difficult, hence players often resort to simple online learning algorithms such as mean-based learners: those algorithms select actions that approximately maximize the performance on the past, and include methods such as MWU, Best Response Dynamics, FTRL, FTPL, etc; see e.g. [16]. Yet, when an agent knows that the other agents are using mean-based learners, it can it can adjust its strategy to take advantage of this knowledge. This was shown in specific games [9, 24]; however, in general settings it is not known how to best play against mean-based learners, even if the agent knows everything about the learners, and can predict with certainty what actions they would take as a function of the history of play. This raises the following question: ", "page_idx": 1}, {"type": "text", "text": "Meta Question. In a multi-agent environment where agents repeatedly take actions, what is the best strategy for an agent, if it knows that the other agents are using mean-based learners to select their actions? Can it plan ahead if it can predict how the other agents will react? Can such an optimal strategy be computed efficiently? ", "page_idx": 1}, {"type": "text", "text": "Setting. We study environments with two agents: a learner who uses a learning algorithm to decide on its actions, and an optimizer, that plans its actions, by trying to optimize its own reward, taking into account the learner\u2019s behavior. The setting is modeled as a repeated game: in each iteration $t=1,\\ldots,T$ , the optimizer selects a strategy $x(t)$ , which is a distribution over a finite set of pure actions $\\mathcal{A}=\\{a_{1},\\ldots,a_{n}\\}$ , i.e. $x(t)\\in\\Delta(\\bar{\\mathcal{A}})$ . At the same time, the learner selects a distribution $y(t)$ over $B=\\{b_{1},\\ldots,b_{m}\\}$ , i.e. $y(t)\\in\\Delta(B)$ . The strategies $x(t)$ and $y(t)$ are viewed as elements of $\\mathbb{R}^{n}$ and $\\mathbb{R}^{m}$ , respectively, and the elements of $\\boldsymbol{\\mathcal{A}}$ and $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ are identified with unit vectors. In each round, each player gains a utility, which is a function of the strategies played by both agents. The utilities for the optimizer and the learner equal $x(t)^{\\top}A y(t)$ and $\\bar{x(t)^{\\top}}B\\bar{y(t)}$ , respectively, where $A,B\\in\\mathbb{R}^{n\\times m}$ . The goal of each player is to maximize their own utility, which is summed over all rounds $t=1,\\dots,T$ . We split our following discussion according to the study of zero-sum and general-sum games, as defined below. ", "page_idx": 1}, {"type": "text", "text": "Zero-sum games. Zero-sum games are those where $A=-B$ . Namely, the learner and the optimizer have opposing goals. Such games have a value, which determines the best utility that the players can hope for, if they are both playing optimally. It is defined as: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname{Val}(A)=\\operatorname*{max}_{x\\in\\Delta(\\mathcal{A})}\\operatorname*{min}_{y\\in\\Delta(\\mathcal{B})}x^{\\top}A y=\\operatorname*{min}_{y\\in\\Delta(\\mathcal{B})}\\operatorname*{max}_{x\\in\\Delta(\\mathcal{A})}x^{\\top}A y\\;.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "The two quantities in the definition above are equal, as follows from von Neumann\u2019s minmax theorem. The first of these quantities implies that the optimizer has a strategy that guarantees them a utility of $\\operatorname{Val}(A)$ against any learner, and the second quantity implies that the learner has a strategy which guarantees that the optimizer\u2019s utility is at most $\\operatorname{Val}(A)$ . Such strategies, namely, those that guarantee minmax value, are termed minmax strategies. ", "page_idx": 1}, {"type": "text", "text": "If the learner would have used a minmax strategy in each round $t$ , then the optimizer could have only received a utility of $T\\operatorname{Val}(A)$ . Yet, when the learner uses a learning algorithm, the optimizer can receive higher utility. A standard theoretical guarantee in online learning is no-regret. When a learner uses a no-regret algorithm to play a repeated game, it is guaranteed that the optimizer\u2019s reward after $T$ rounds is at most $T\\operatorname{Val}(A)+o(T)$ . Yet, $T$ is finite, and the $o(T)$ -term can be significant. Consequently, we pose the following question, which we study in Section 2. ", "page_idx": 1}, {"type": "text", "text": "Question 1. In repeated zero-sum games between an optimizer and a learner, when can the optimizer capitalize on the sub-optimality of the learner\u2019s strategy, to obtain significantly higher utility than the value of the game? Can the optimizer\u2019s algorithm be computationally efficient? ", "page_idx": 1}, {"type": "text", "text": "General-sum games. Games where $B\\neq-A$ are termed general-sum. These games are significantly more intricate: they do not posses a value or minmax strategies. In order to study such games, various notions of game equilibria have been proposed, such as the celebrated Nash Equilibrium. In the context related to the topic of our paper, if the optimizer could commit on a strategy, and the other player would best respond to it, rather than using a learning algorithm, then the optimizer could get the Stackelberg value, by playing according to the Stackelberg Equilibrium. Further, there is a polynomial-time algorithm to compute the optimizer\u2019s strategy in this case [21, 41, 20]. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Against a learner that uses a no-regret learning algorithm, the optimizer could still guarantee the Stackelberg value of the game, by playing according to the Stackelberg Equilibrium. Yet, it was shown that in certain games, the optimizer can gain up to $\\Omega(T)$ more utility compared to the Stackelberg value when they are playing against mean-based learners [9, 24, 37, 30, 18]. However, this often requires playing a carefully-planned strategy which changes across time. While such a strategy could be computed in polynomial-time for specific games [9, 13, 23], no efficient algorithm was known for general games. Deng et al. [24] devised a control problem whose solution is approximately the optimal strategy for the optimizer, against mean-based learners. Yet, they do not provide a computationally-efficient algorithm for this control problem. Finding an optimal strategy for the learner was posed as an open problem [24, 30, 11], that can be summarized as follows: ", "page_idx": 2}, {"type": "text", "text": "Question 2. In repeated general-sum games between an optimizer and a mean-based learner, is there a polynomial-time algorithm to find an optimal strategy for the optimizer? ", "page_idx": 2}, {"type": "text", "text": "We study this question in Section 3. ", "page_idx": 2}, {"type": "text", "text": "1.1 Our results ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our results are two-fold; In zero sum games, we provide positive results, and show what the optimizer\u2019s optimal rewards and strategies for a given game should be, in both discrete and continuous time games. Our results in that realm are the following: ", "page_idx": 2}, {"type": "text", "text": "\u2022 For continuous time games, we provide an exact, closed form solution of the rewards of the optimizer against a learner that uses the replicator dynamics, i.e. continuous MWU (Section 2, Theorem 1). In the same theorem we prove that the optimizer can achieve optimal rewards by playing a constant strategy throughout the game. i.e. $x(t)=x^{*},x\\in\\Delta(A)\\forall t\\in[0,T]$ .   \n\u2022 We also prove a range where the optimal rewards might be and provide a lower bound for when $\\eta T\\to\\infty$ (Section 2, Proposition 1).   \n\u2022 For discrete time games, when the optimizer is up against a learner that uses MWU, we first prove that the optimal rewards of the optimizer in this setting, will be lower bounded by the rewards of the optimizer in the continuous time game, if they use the same strategy $x^{*}$ (Section 2 Proposition 2).   \n\u2022 We then prove that the reward \u2019gap\u2019 between continuous and discrete time games cannot be greater than $\\eta T/2$ (Section B, Proposition 10), and there are games that achieve a gap of at least $\\Omega(\\eta T)$ (Section 2, Proposition 3). In fact we give a class of games that can achieve that gap in rewards (Section 2,Proposition 4). ", "page_idx": 2}, {"type": "text", "text": "In general-sum games, we provide the first known computational lower bound for calculating the optimal strategy against a mean based learner. We formalize the problem of optimizing rewards as a decision problem, where the answer for an instance is \u2019YES\u2019 if the optimizer can achieve rewards more than $T$ and \u2019NO\u2019 if the optimizer can receive rewards at most $T-1$ . We prove that there is no polynomial time algorithm, assuming ${\\mathrm{P}}\\neq{\\mathrm{NP}}$ that distinguishes between the two cases by using a reduction from Hamiltonian Cycle. The formal theorem and a sketch of the reduction construction can be found in Section 2 Theorem 4. ", "page_idx": 2}, {"type": "text", "text": "1.2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The game-theoretic modeling of multi-agent environments has long history [16, 45, 27]. Such studies often revolve around proving that if multiple agents use learning algorithms to repeatedly play against each other, the avarege history of play converges to various notions of game equilibria [43, 16]. Recent works have shown that some learning algorithms yield fast convergence to game equilibria and fast-decaying regret, if all players are using the same algorithm [44, 22, 2, 4, 26, 42, 46]. ", "page_idx": 2}, {"type": "text", "text": "Optimizing against no regret learners. Braverman et al. [9] initiated a recent thread of works, which studies how a player can take advantage of the learning algorithms run by other agents in order to gain high utility. They showed that in various repeated auctions where a seller sells an item to a single buyer in each iteration $t=1,\\dots,T$ , the seller can gain nearly the maximal utility that they could possibly hope for, if the learner runs an EXP3 learning algorithm (i.e., the seller\u2019s utility can be arbitrarily close to the total welfare). This can be $\\Omega(T)$ larger than what they can get if the buyer is strategic. Deng et al. [24] generalized the study to arbitrary normal-form games. They showed an example for a game where an optimizer that plays against any mean-based learner gets $\\Omega(T)$ more than the Stackelberg value, which is what they would get against strategic agents. The same paper further showed that against no-swap-regret learners, the optimizer cannot gain $\\Omega(T)$ more than the Stackelberg value and Mansour et al. [37] showed that no-swap-regret learners are the only algorithms with this property. Brown et al. [11] showed a polynomial time algorithm to compute the optimal strategy for the optimizer against the no-regret learner that is most favorable to the optimizer \u2014 yet, such no-regret learner may be unnatural. Additional work obtained deep insights into the optimizer/learner interactions in general games [11, 5], in Bayesian games [37] and in specific games such as auctions [23, 13, 35, 34], contract design [30] and Bayesian persuasion [18]. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Optimizing against MWU in $2\\times2$ -games. Guo and Mu [29] obtain a computationally-efficient algorithm for an optimizer that is playing a zero-sum game against a learner that uses MWU, in games where each player holds 2 actions, and they analyze that optimal strategy. ", "page_idx": 3}, {"type": "text", "text": "Regret lower bounds for online learners. Regret lower bounds for learning algorithms are tightly related to upper bounds for an optimizer in zero-sum games. The optimizer can be viewed as an adversary that seeks to maximize the regret of the algorithm. The main difference is that these lower bounds construct worst-case instances, yet, we seek to maximize the utility of the optimizer in any game. Regret lower bounds for MWU and generalizations of it were obtained by [17, 32, 15, 28]. Minmax regret lower bounds against any online learner can be found, for example, in [16]. ", "page_idx": 3}, {"type": "text", "text": "Approximating the Stackelberg value. Lastly, the problem of computing the Stackelberg equilibrium is well studied [21, 41, 20]. Recent works also study repeated games between an optimizer and a learner, where the optimizer does not know the utilities of the learner and its goal is to learn to be nearly as good as the Stackalberg strategy [6, 38, 36, 31]. ", "page_idx": 3}, {"type": "text", "text": "2 Optimizing utility in zero-sum games ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Continuous-time games. We begin our discussion with continuous-time dynamics, where $x(t)$ and $y(t)$ are functions of the continuous-time parameter $t\\in[0,T]$ . The total reward for the optimizer is $\\begin{array}{r}{\\int_{0}^{T}x(t)^{\\top}A y(t)d t}\\end{array}$ , whereas the learner\u2019s utility equals the optimizer\u2019s utility multiplied by $-1$ . We assume that the learner is playing the Replicator Dynamics with parameter $\\eta>0$ , which is the continuous-time analogue of the Multiplicative Weights Update algorithm, which plays at every time $t$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\ny_{i}(t)=\\frac{\\exp\\left(\\eta\\int_{0}^{t}x(s)^{\\top}B e_{i}d s\\right)}{\\sum_{j=1}^{m}\\exp\\left(\\eta\\int_{0}^{t}x(s)^{\\top}B e_{j}d s\\right)}\\;,\\quad i=1,\\ldots,m\\;.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This formula gives a higher weight to actions that would have obtained higher utility in the past. When $\\eta$ is larger, the learner is more likely to play the actions that maximize the past utility. The value of $\\eta$ usually becomes smaller as $T$ increases, and typically $\\eta\\rightarrow0$ as $T\\rightarrow\\infty$ . ", "page_idx": 3}, {"type": "text", "text": "The following definition will be helpful for the analysis: ", "page_idx": 3}, {"type": "text", "text": "Definition 1 (Historical Rewards for the learner). The historical rewards for the learner in continuous games at time $t$ , denoted by $h(t)\\,\\in\\,\\mathbb{R}^{m}$ , is an m dimensional vector where $h_{i}(t),i\\,=1,\\ldots,m$ corresponds to the sum of rewards achieved by action $a_{i}$ of the learner against the history of play in the game so far. We assume a general setting where the learner at $t=0$ might have non-zero historical rewards, i.e. $h(0)\\in\\mathbb{R}^{m}$ . If the strategy of the optimizer is $x:[0,t]\\rightarrow\\Delta(A)$ we get: ", "page_idx": 3}, {"type": "equation", "text": "$$\nh(t)=h(0)+\\int_{0}^{t}B^{\\top}x(t)d t\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Suppose the game has been played for some time $t$ and the learner has collected historical rewards $h(\\bar{t})$ . The total reward that the optimizer gains from the remainder of this game can be written as a ", "page_idx": 3}, {"type": "text", "text": "function of the time left for the game $T-t$ , the historical rewards of the learner at time $t,h(t)$ , and the strategy $x:[0,T-t]\\rightarrow\\Delta(\\bar{A})$ of the optimizer for the remainder of the game. ", "page_idx": 4}, {"type": "equation", "text": "$$\nR_{c o n t}(x,h(t),T-t,A,B)=\\int_{0}^{T-t}\\frac{\\sum_{i=1}^{m}e^{\\eta\\left(h_{i}(u)+e_{i}^{\\top}B^{\\top}\\int_{0}^{u}x(s)d s\\right)}\\cdot e_{i}^{\\top}A^{\\top}x(u)}{\\sum_{i=1}^{m}e^{\\eta\\left(h_{i}(u)+e_{i}^{\\top}B^{\\top}\\int_{0}^{u}x(s)d s\\right)}}d u\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For simplicity, we can just transfer the time to 0 and assume that the historical rewards of the learner are just $h(0)$ . That way we can rewrite the above definition as: ", "page_idx": 4}, {"type": "equation", "text": "$$\nR_{c o n t}(x,h(0),\\tau,A,B)=\\int_{0}^{\\tau}\\frac{\\sum_{i=1}^{m}e^{\\eta\\left(h_{i}(t)+e_{i}^{\\top}B^{\\top}\\int_{0}^{t}x(s)d s\\right)}\\cdot e_{i}^{\\top}A^{\\top}x(t)}{\\sum_{i=1}^{m}e^{\\eta\\left(h_{i}(t)+e_{i}^{\\top}B^{\\top}\\int_{0}^{t}x(s)d s\\right)}}d t\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In general we are interested in finding the maximum value that the optimizer can achieve at any moment of the game: ", "page_idx": 4}, {"type": "equation", "text": "$$\nR_{c o n t}^{*}(h(0),\\tau,A,B)=\\operatorname*{max}_{x}R_{c o n t}(x,h(0),\\tau,A,B)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For finding the optimal reward for the optimizer from the beginning of the game we would have to find $R_{c o n t}^{*}(\\mathbf{0},T,A,B)$ , where $\\mathbf{0}=(0,0,\\bar{\\dots},0)^{\\top}$ . ", "page_idx": 4}, {"type": "text", "text": "The next theorem characterizes the exact optimal strategy for the optimizer, against a learner that uses the Replicator Dynamics in zero sum games: ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. In a zero-sum continuous game, when the learner is using the Replicator Dynamics, the optimal rewards for the optimizer can be achieved with a constant strategy throughout the game, i.e. $\\bar{x^{}}(t)=x^{*}\\in\\Delta(\\bar{A})$ . The optimal reward is obtained by the following formula: ", "page_idx": 4}, {"type": "equation", "text": "$$\nR_{c o n t}^{*}(h(0),T,A,-A)=\\operatorname*{max}_{x\\in\\Delta(\\mathcal{A})}\\left\\{\\frac{\\ln\\left(\\sum_{i=1}^{m}e^{\\eta h_{i}(0)}\\right)-\\ln\\left(\\sum_{i=1}^{m}e^{\\eta\\left(h_{i}(0)-T e_{i}^{\\top}A^{\\top}x\\right)}\\right)}{\\eta}\\right\\}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Further, $x^{*}$ is the maximizer in the formula above. ", "page_idx": 4}, {"type": "text", "text": "Proof. For zero sum games in continuous time, we have that equation 3 becomes: ", "page_idx": 4}, {"type": "equation", "text": "$$\nR_{c o n t}(x,h(0),T,A,-A)=\\int_{0}^{T}\\frac{\\sum_{i=1}^{m}e^{\\eta\\left(h_{i}(0)-e_{i}^{\\top}A^{\\top}\\int_{0}^{t}x(s)d s\\right)}\\cdot e_{i}^{\\top}A^{\\top}x(t)}{\\sum_{i=1}^{m}e^{\\eta\\left(h_{i}(0)-e_{i}^{\\top}A^{\\top}\\int_{0}^{t}x(s)d s\\right)}}d t\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Notice that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{d}{d t}\\left(e^{\\eta\\left(h_{i}\\left(0\\right)-e_{i}^{\\top}A^{\\top}\\int_{0}^{t}x\\left(s\\right)d s\\right)}\\right)=e^{\\eta\\left(h_{i}\\left(0\\right)-e_{i}^{\\top}A^{\\top}\\int_{0}^{t}x\\left(s\\right)d s\\right)}\\cdot\\frac{d}{d t}\\left(\\eta h_{i}(0)-\\eta\\cdot e_{i}^{\\top}A^{\\top}\\int_{0}^{t}x(s)d s\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "From Leibniz rule we have that $\\begin{array}{r}{\\frac{d}{d t}\\int_{0}^{t}x(s)d s=x(t)}\\end{array}$ , thus we get: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{d}{d t}\\left(e^{\\eta\\left(h_{i}(0)-e_{i}^{\\top}A^{\\top}\\int_{0}^{t}x(s)d s\\right)}\\right)=-e^{\\eta\\left(h_{i}(0)-e_{i}^{\\top}A^{\\top}\\int_{0}^{t}x(s)d s\\right)}\\cdot\\eta\\cdot e_{i}^{\\top}A^{\\top}x(t)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Given that, note that we can find a closed form solution for $R_{c o n t}(x,h(0),T,A,-A)$ as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{c o n t}(x,h(0),T,A,-A)=\\left[-\\displaystyle\\frac{1}{\\eta}\\ln\\left(\\sum_{i=1}^{m}e^{\\eta\\left(h_{i}(0)-e_{i}^{\\top}A^{\\top}\\int_{0}^{t}x(s)d s\\right)}\\right)\\right]_{0}^{T}}\\\\ &{\\phantom{\\sum_{\\alpha_{c o n t}(x,h(0),T,A,-A)}}=\\frac{\\ln\\left(\\sum_{i=1}^{m}e^{\\eta h_{i}(0)}\\right)-\\ln\\left(\\sum_{i=1}^{m}e^{\\eta\\left(h_{i}(0)-e_{i}^{\\top}A^{\\top}\\int_{0}^{T}x(s)d s\\right)}\\right)}{\\eta}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Suppose the optimal rewards $R_{c o n t}^{*}(h(0),T,A,-A)$ are achieved with $x^{o p t}(t)$ . Note that the final reward only depends on $\\boldsymbol{\\int_{0}^{T}x(s)d s}$ , thus the same reward that is achieved by $x^{o p t}(t)$ , can be achieved by $\\begin{array}{r}{x^{*}=\\frac{1}{T}\\int_{0}^{T}x^{o p t}(s)d s}\\end{array}$ . Thus there exists a $x^{*}$ such that: ", "page_idx": 4}, {"type": "equation", "text": "$$\nR_{c o n t}^{*}(h(0),T,A,-A)=\\frac{\\ln\\left(\\sum_{i=1}^{m}e^{\\eta h_{i}(0)}\\right)-\\ln\\left(\\sum_{i=1}^{m}e^{\\eta\\left(h_{i}(0)-e_{i}^{\\top}A^{\\top}x^{*}T\\right)}\\right)}{\\eta}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which is what we wanted. ", "page_idx": 4}, {"type": "text", "text": "The optimal strategy $x^{*}$ for the optimizer in continuous zero-sum games can be therefore obtained by finding the minimum of the convex function $\\begin{array}{r}{f(x)=\\ln\\left(\\sum_{i=1}^{m}e^{\\eta\\bar{\\left(h_{i}(0)-e_{i}^{\\top}A^{\\top}x T\\right)}}\\right)}\\end{array}$ . We can compute the optimal strategy of the optimizer in an efficient way using techniques from convex optimization. More details can be found in the appendix at Proposition 5. ", "page_idx": 5}, {"type": "text", "text": "We further analyze how larger the optimal achievable reward is, compared to the naive bound of $T\\operatorname{Val}(A)$ . We specifically show that always the optimal rewards of the optimizer are in the range of $[T\\operatorname{Val}(A),T\\operatorname{Val}(A)+\\log(m)/\\eta]$ , where $m$ is the number of actions of the learner, showing the optimizer can always get more utility than just the value of the game. ", "page_idx": 5}, {"type": "text", "text": "We also analyze what happens to the rewards of the optimizer as $\\eta T\\to\\infty$ . First, let us define for any optimizer\u2019s strategy $x$ , the set of all best-responses, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname{BR}(x)=\\arg\\operatorname*{max}_{b\\in B}x^{\\top}B b.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This defines a set, and if there are multiple maximizers, $|\\operatorname{BR}(x)|>1$ . We then have the following proposition: ", "page_idx": 5}, {"type": "text", "text": "Proposition 1 (informal). The optimal reward for an optimizer playing against a learner that uses the replicator dynamics with parameter $\\eta$ in an $n\\times m$ game, is in the range $[T\\operatorname{Val}(A),T\\operatorname{Val}(A)+$ $\\log(m)/\\eta$ . Further, as $\\eta T\\to\\infty,$ , this optimal utility is at least ", "page_idx": 5}, {"type": "equation", "text": "$$\nT\\operatorname{Val}(A)+\\frac{\\log(m/k)}{\\eta}\\,,\\qquad w h e r e\\,k=\\operatorname*{min}_{\\substack{x\\in\\Delta(A)\\,m i n m a x\\,s t r a t e g y}}|\\operatorname{BR}(x)|\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The proof of Proposition 1 can be found in Appendix B, Propositions 6 and 7. We note that the limiting utility is obtained by playing constantly any minmax strategy $x$ that attains the minimum in the definition of $k$ above. ", "page_idx": 5}, {"type": "text", "text": "Connection to optimal control and the Hamilton-Jacobi-Bellman equation. We will present another way of achieving Theorem 1, using literature from control theory. One can view the problem of maximizing the optimizer\u2019s utilities as an optimal control problem; what control (or strategy) should the optimizer use in order to maximize their utility given that the learner has some specific dynamics that depend on the control of the optimizer? The Hamilton-Jacobi-Bellman equation [8] gives us a partial differential equation (PDE) of $R_{c o n t}^{*}(h,t,A,B)$ that if we solve, we can find a closed form solution of the optimal utility of the optimizer. The equation (for general sum games): ", "page_idx": 5}, {"type": "equation", "text": "$$\n-\\frac{d R_{c o n t}^{*}(h,t,A,B)}{d t}=\\operatorname*{max}_{x\\in\\Delta(\\mathcal{A})}\\left(\\frac{\\sum_{i=1}^{m}e^{\\eta h_{i}}\\cdot e_{i}^{\\top}A^{\\top}x}{\\sum_{i=1}^{m}e^{\\eta h_{i}}}+\\left(\\nabla_{h}R_{c o n t}^{*}(h,t,A,B)\\right)^{\\top}\\cdot B^{\\top}x\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The intuition of the PDE is as follows; the current state of the learner can be defined given only the history $h$ of the sum of what the optimizer played so far, and the time left in the game $t$ . Given we are at a state $h,t$ , the optimal rewards for the optimizer are going to be equal to the rewards of playing action $x\\in\\Delta(\\bar{\\mathcal{A}})$ for time $\\Delta t$ added together with the optimal reward in the new state, namely $R_{c o n t}^{*}(h+{B^{\\top}}x\\Delta t,t+\\Delta t,A,B)$ . Taking the limit as $\\Delta t\\rightarrow0$ , gives us the above partial differential equation. ", "page_idx": 5}, {"type": "text", "text": "Plugging in $B=-A$ , for the case of zero-sum games, we get: ", "page_idx": 5}, {"type": "equation", "text": "$$\n-\\frac{d R_{c o n t}^{*}(h,t,A,-A)}{d t}=\\operatorname*{max}_{x\\in\\Delta(A)}\\left(\\frac{\\sum_{i=1}^{m}e^{\\eta h_{i}}\\cdot e_{i}^{\\top}A^{\\top}x}{\\sum_{i=1}^{m}e^{\\eta h_{i}}}-\\left(\\nabla_{h}R_{c o n t}^{*}(h,t,A,-A)\\right)^{\\top}\\cdot A^{\\top}x\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "If one plugs in the formula we calculated in Theorem 1, they would find that indeed it is a solution to the above PDE. ", "page_idx": 5}, {"type": "text", "text": "Discrete-time games. We now move to the discrete-time setting. The learner is assumed to be playing the Multiplicative-Weights update algorithm, defined by: ", "page_idx": 5}, {"type": "equation", "text": "$$\ny_{i}(t)=\\frac{\\exp\\left(\\eta\\sum_{s=0}^{t-1}x(s)^{\\top}B e_{i}\\right)}{\\sum_{j=1}^{m}\\exp\\left(\\eta\\sum_{s=1}^{t-1}x(s)^{\\top}B e_{j}\\right)},\\quad i=1,2,\\ldots,m\\;.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We show that if the learner constantly plays the strategy $x^{*}$ that is optimal for continuous-time, the obtained utility against MWU can only be higher, compared to playing against the Replicator Dynamics in continuous-time, with the same step size $\\eta$ : ", "page_idx": 5}, {"type": "text", "text": "Proposition 2 (informal). Let A be a zero-sum game matrix and $\\eta>0$ and $T\\in\\mathbb{N}$ . Let $x^{*}\\in\\Delta(A)$ be the optimal strategy against the replicator dynamics, from Theorem $^{\\,l}$ . Then, the utility in the discrete time, achieved by an optimizer which plays ${\\boldsymbol{x}}(t)={\\boldsymbol{x}}^{*}$ for all $t\\in\\{1,\\ldots,T\\}$ against MWU with parameter \u03b7, is at least the utility achieved in the continuous-time by playing $x^{*}$ against the replicator dynamics with the same parameters $\\eta,T$ . ", "page_idx": 6}, {"type": "text", "text": "The proof can be found in Appendix B (Proposition 8). ", "page_idx": 6}, {"type": "text", "text": "Proposition 2 implies that Proposition 1 provides lower bounds on the achievable utility of this constant discrete-time strategy. In order to further analyze its performance, we would like to ask how much larger the discrete-time utility of the optimizer can be from the optimal continuous-time utility. We include the following statement, which follows from a standard approach: ", "page_idx": 6}, {"type": "text", "text": "Proposition 3. Let $T\\in\\mathbb N$ and $\\eta>0$ . For any zero-sum game whose utilities are bounded in $[-1,1]$ , the best discrete-time optimizer obtains a utility of at most $\\eta T/2$ more than the best continuous-time optimizer. Further, there exists a game where the best discrete-time optimizer achieves a utility of $\\mathrm{tanh}(\\eta)T/2=(\\eta-O(\\eta^{2}))T/2$ more than the optimal continuous-time optimizer, and $\\operatorname{tanh}(\\eta)T/2$ more than the discrete-time optimizer from Proposition 2. ", "page_idx": 6}, {"type": "text", "text": "The proof can be found in Appendix B (Propositions 9 and 10). ", "page_idx": 6}, {"type": "text", "text": "Proposition 3 implies that in some cases, the best discrete-time optimizer can gain $T\\operatorname{Val}(A)+\\Omega(\\eta T)$ . We would like to understand in which games this is possible for any choice of $\\eta\\in(0,1)$ . We provide a definition that guarantees that this would be possible: ", "page_idx": 6}, {"type": "text", "text": "Condition 1. There exists a minmax strategy $x$ for the optimizer such that there exist two best responses for the learner, $b_{i_{1}},b_{i_{2}}\\in\\mathrm{BR}(x)$ , which do not coincide on support $(x)$ . Namely, there exists an action $a_{k}\\in\\mathrm{support}(x)$ such that $a_{k}^{\\top}A b_{i_{1}}\\neq a_{k}^{\\top}A b_{i_{2}}$ . ", "page_idx": 6}, {"type": "text", "text": "To motivate Condition 1, notice that in order to achieve a gain of $\\Omega(\\eta T)$ for any $\\eta$ , the discrete optimizer has to be significantly better than the continuous-time optimizer. For that to happen the learner would have to change actions frequently. Indeed, the difference between the discrete and continuous learners is that the discrete learner is slower to change actions: they only change actions at integer times, whereas the continuous learner could change actions at each real-valued time. In order to change actions, they need to have at least two good actions, and this is what Assumption 1 guarantees. We derive the following statement: ", "page_idx": 6}, {"type": "text", "text": "Proposition 4. For any zero-sum game $A\\in\\mathbb{R}^{n\\times m}$ that satisfies Assumption $^{\\,l}$ , $\\eta\\,\\in\\,(0,1)$ and $T\\in\\mathbb{N}$ , there exists an optimizer, such that against a learner that uses MWU with step size $\\eta,$ achieves a utility of at least $T\\operatorname{Val}(A)+\\Omega(\\eta T)$ , where the constant possibly depends on $A$ . ", "page_idx": 6}, {"type": "text", "text": "The proof can be found in Appendix B (Proposition 11). ", "page_idx": 6}, {"type": "text", "text": "Proposition 4 considers a strategy for the optimizer which takes the minmax strategy $x$ from Assumption 1, and splits into two strategies: $\\overline{{x^{\\prime}}},x^{\\prime\\prime}\\in\\Delta(A)$ , such that $(x^{\\prime}+x^{\\prime\\prime})/2\\stackrel{\\Sigma}{=}x$ . Further, $x^{\\prime\\top}\\bar{A}b_{i_{1}}\\neq x^{\\prime\\prime\\top}A\\bar{b}_{i_{2}}$ , where $b_{i_{1}},b_{i_{2}}$ are defined in Assumption 1. It plays $x^{\\prime}$ in each odd round $t$ , and $x^{\\prime\\prime}$ in each even round. It is possible to show that in each two consecutive iterations, the reward for the optimizer is $2\\operatorname{Val}(A)+\\Omega(\\eta)$ . Summing over $T/2$ consecutive pairs yields the final bound. ", "page_idx": 6}, {"type": "text", "text": "3 A computational lower bound for optimization in general-sum games ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we present the first limitation on optimizing against a mean-based learner. Specifically, we study the algorithm which is termed Best-Response or Fictitious Play [10, 43]. At each time step $t=1,\\dots,T$ , this algorithm selects an action $y(t)$ that maximizes the cumulative utility for rounds $1,\\ldots,t-1$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\ny(t)=\\arg\\operatorname*{max}_{y\\in\\Delta(\\mathcal{B})}\\sum_{s=1}^{t-1}x(s)^{\\top}B y\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "There is always a maximizer which corresponds to a pure action $b_{i}\\in\\boldsymbol{B}$ of the learner and we assume that if there are ties, they are broken lexicographically. In this section, we constrain the optimizer to also play pure actions. ", "page_idx": 6}, {"type": "text", "text": "To put this algorithm in context, we note the following connections to general mean-based learners: (1) mean-based learners are any algorithms which select an action $y(t)$ that approximately maximizes Eq. (9); (2) Best-Response is equivalent to MWU with $\\eta\\rightarrow\\infty$ . ", "page_idx": 7}, {"type": "text", "text": "We prove that there is no algorithm that approximates the optimal utility of the optimizer up to an approximation factor of $1-\\epsilon$ , whose runtime is polynomial in $n,m,T$ and $1/\\epsilon$ , as exemplified by the following (informal) Theorem: ", "page_idx": 7}, {"type": "text", "text": "Theorem 2 (informal). Let Alg be an algorithm that receives parameters $\\epsilon\\,>\\,0$ , $m,n,T\\,\\in\\,\\mathbb{N}$ and utility matrices $A,B$ of dimension $m\\times n$ and entries in $[0,1]$ , and outputs a control sequence $x(1),\\ldots,x(T)\\in{\\mathcal{A}}$ . Let $U$ denote the utility attained by the learner and let $U^{*}$ denote the optimal possible utility. If $U\\geq(1-\\epsilon)U^{*}$ for any instance of the problem, and $i f\\mathrm{P\\neqNP}$ , then Alg is not polynomial-time in $m,n,T$ and $1/\\epsilon$ . ", "page_idx": 7}, {"type": "text", "text": "A sketch of the proof can be found below, and a full proof can be found in the Appendix C. The proof is obtained via a reduction from the Hamiltonian cycle problem. We note two limitations of this result: (1) The lower bound is for $T=n/2+1$ , and it shows hardness in distinguishing between the case that the optimal reward is $T$ and the case that the optimal reward is at most $T-1$ . It is still open whether one could efficiently find a sequence that optimizes the reward up to $o(T)$ ; (2) fictitious-play is a fundamental online learning algorithm. Yet, it does not possess the no-regret guarantee. It is still open whether one could obtain maximal utility against no-regret learners such as MWU with a small step size. ", "page_idx": 7}, {"type": "text", "text": "We continue by framing the problem of maximizing rewards against a Best-Response learner as a decision problem, called OCDP: ", "page_idx": 7}, {"type": "text", "text": "Problem 1 (Optimal Control Discrete Pure (OCDP)). An OCDP instance is defined by $(A,B,n,m,k,T),$ , where $n,m,k,T\\,\\in\\,\\mathbb{N}_{*}$ , $A\\,\\in\\,\\{0,1\\}^{n\\times m}$ and $B\\,\\in\\,[0,1]^{n\\times m}$ . The numbers $n$ and m correspond to the actions of the optimizer and learner in a game, where $A$ is the utility matrix of the optimizer and $B$ is the utility matrix of the learner. This instance is a \u2019YES\u2019 instance if the optimizer can achieve utility at least k after playing the game for $T$ rounds with a learner that uses the Best Response Algorithm (Eq. (9)), and \u2019NO\u2019 if otherwise. ", "page_idx": 7}, {"type": "text", "text": "We will prove that OCDP is NP-hard, using a reduction from the Hamiltonian cycle problem. ", "page_idx": 7}, {"type": "text", "text": "Problem 2 (Hamiltonian cycle). Given a directed graph $G(V,E)$ , find whether there exists a Hamiltonian cycle, i.e. a cycle that starts from any vertex, visits every vertex exactly once and closes at the same vertex where it started. ", "page_idx": 7}, {"type": "text", "text": "It is a known theorem that the Hamiltonian Cycle is an NP-complete problem, as it is one of Karp\u2019s 21 initial NP-Complete problems. ", "page_idx": 7}, {"type": "text", "text": "Theorem 3 ([33]). Hamiltonian Cycle is NP-complete. ", "page_idx": 7}, {"type": "text", "text": "We conclude with the main result of this section, followed by a proof sketch. The full proof appears in Section C. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4. OCDP is NP-hard. That is, $i f\\mathrm{P\\neqNP}$ , there exists no algorithm that runs in polynomial time in $n,m$ and $k$ which distinguishes between the case that a reward of $k$ is achievable and the case that it is impossible to obtain reward more than $k-1$ . ", "page_idx": 7}, {"type": "text", "text": "Proof sketch. Consider an instance of the Hamiltonian cycle problem: $\\pi_{H}=(V,E)$ , where $V=$ $\\{v_{1},\\ldots,v_{n}\\},E=\\{e_{1},\\ldots,e_{m}\\}$ . We create an instance of OCDP as follows. First, set ${\\cal T}=k=$ $n+1$ . We will construct the instance such that the optimizer can receive reward $n+1$ if and only if there is a Hamiltonian cycle in the graph. Define the optimizer\u2019s actions to be $\\{a_{1},\\ldots,a_{m}\\}$ , and the learner\u2019s actions to be $\\{b_{1},\\ldots,b_{n},b_{1}^{\\prime},\\ldots,b_{n}^{\\prime}\\}$ . Namely, for each node $v_{i}$ of the graph, the learner has two associated actions, $b_{i}$ and $b_{i}^{\\prime}$ . The details in the sketch differ slightly from the proof for clarity. The reduction is constructed to satisfy the following properties: ", "page_idx": 7}, {"type": "text", "text": "\u2022 The only way for the optimizer to receive maximal utility, is to play a strategy as follows: the first $n$ actions should correspond to edges $e_{i_{1}}-e_{i_{2}}-\\cdot\\cdot\\cdot-e_{i_{n}}$ that form a Hamiltonian cycle which starts and ends at the vertex $v_{1}$ ; and the last action corresponds to $e_{i_{n+1}}$ which is an outgoing edge from $v_{1}$ . ", "page_idx": 7}, {"type": "text", "text": "\u2022 We define the utility matrix for the learner such that, if the optimizer is playing according to this strategy, then the learner\u2019s actions will correspond to the vertices of the same cycle, denoted as $v_{j_{1}}-\\cdot\\cdot\\cdot-v_{j_{n}}-v_{j_{1}}$ . This is achieved by defining the learner\u2019s tie-breaking rule to play $a_{1}$ in the first round; and defining the learner\u2019s utilities such that, if the optimizer has played in rounds $1,\\ldots,t-1$ the edges along a path $v_{j_{1}}\\textrm{--}\\cdot\\cdot\\textrm{--}v_{j_{t}}$ , then the best response for the learner at time $t$ would be to play $a_{j_{t}}$ . To achieve this, we define for any edge $e_{k}=(v_{p},v_{q})\\colon B[a_{k},b_{p}]=-1,B[a_{k},b_{q}]=$ and $B[a_{k},b]=0$ for any other action $b$ Consequently, after observing the edges along $b_{j_{1}}-\\cdot\\cdot-b_{j_{t}}$ , the cumulative reward for the learner would be 1 for $b_{j_{t}}$ , $-1$ for $b_{j_{1}}$ and 0 for any other $b_{j}$ . Consequently, the learner will best respond with $b_{j_{t}}$ \u2014 we ignore the actions $b_{1}^{\\prime},\\ldots,b_{n}^{\\prime}$ of the learner at the moment. ", "page_idx": 8}, {"type": "text", "text": "\u2022 In order to guarantee that the above optimizer\u2019s strategy yields a utility of $n\\!+\\!1$ , we define the optimizer\u2019s utility such that $A[e_{k},b_{p}]=1$ if $e_{k}=(v_{p},v_{q})$ and for all $q\\not=p\\;\\cal A[e_{k},b_{q}]=0$ . This will also force the optimizer to play edges that form a path. Indeed, if the optimizer has previously played the edges along a path $v_{j_{1}}-\\cdot\\cdot\\cdot-v_{j_{t}}$ , then, as we discussed, the learner will play $b_{i_{t}}$ in the next round. For the optimizer to gain a reward at the next round, they must play an edge outgoing from $a_{j_{t}}$ . This preserves the property that the optimizer\u2019s actions form a path. ", "page_idx": 8}, {"type": "text", "text": "\u2022 Next, we would like to guarantee that the optimizer\u2019s actions align with a Hamiltonian cycle. Therefore, we need to ensure that the path played by the optimizer does not close too early. Namely, if the learner has played the edges along the path $v_{j_{1}}-\\cdot\\cdot\\cdot-v_{j_{t}}$ and if $t<n$ , then $j_{1},\\ldots,j_{t}$ are all distinct. For this purpose, the actions $v_{1}^{\\prime},\\bar{\\ldots},v_{n}^{\\prime}$ are defined. We define for any edge $e_{k}=(v_{i},v_{j})$ : $B[a_{k},b_{i}^{\\prime}]\\,{\\stackrel{\\rightharpoonup}{=}}\\,\\bar{0}.85$ . This will prevent the optimizer from playing the same vertex twice, for any round $t=1,\\dots,n$ (recall that there are $T=n+1$ rounds), as argued below. ", "page_idx": 8}, {"type": "text", "text": "Assume for the sake of contradiction that the first time the same vertex is visited is at $t$ , where $j_{t}=j_{r}$ for some $r<t\\leq n$ . Then, the cumulative utility of action $b_{j_{r}}^{\\prime}$ for rounds $1,\\ldots,t$ is 1.7, which is larger than any other action. This implies that at the next round, the learner will play action $b_{j_{r}}^{\\prime}$ . We define the utility for the optimizer to be zero against any action $b_{j}^{\\prime}$ . Hence, any scenario where the learner plays an action $b_{j}^{\\prime}$ , prevents the optimizer from receiving a utility of $n+1$ . This happens whenever $i_{t}=\\dot{\\i}_{j}$ for some $j\\,<\\,t\\,\\leq\\,n$ . Hence, an optimizer that receives a reward of 1 at any round must play a path that does not visit the same vertex twice, in rounds $t=1,\\dots,n$ . ", "page_idx": 8}, {"type": "text", "text": "\u2022 Lastly, notice that we do want the optimizer to play the same vertex $a_{1}$ twice, at rounds 1 and $n+1$ . This does not prevent the optimizer from receiving optimal utility. Indeed, if the optimizer would play the action $a_{1}$ twice, then the learner would play $b_{1}^{\\prime}$ at the next round. Yet, since there are only $n+1$ rounds, there is no \u201cnext round\u201d and no reward is lost. The details that force the learner to play $b_{1}$ in round $n+1$ appear in the full version of the proof. ", "page_idx": 8}, {"type": "text", "text": "The above explanation sketches why it is possible to get $n+1$ reward if and only if there is a Hamiltonian cycle and this concludes the reduction. \u53e3 ", "page_idx": 8}, {"type": "text", "text": "4 Conclusion and future directions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this paper we studied how an optimizer should play in a two-player repeated game knowing that the other agent is a learner that is using a known mean-based algorithm. In zero-sum games we showed how they can gain optimal utility against the Replicator Dynamics and we further analyzed the utility that they could gain against MWU. In general sum games, we showed the first computational hardness result on optimizing against a mean-based learner, by reduction from Hamiltonian Cycle. ", "page_idx": 8}, {"type": "text", "text": "One interesting problem that remains is the open is analyzing the optimal reward in general sum games against the Replicator Dynamics (or the MWU), which was denoted as $R_{c o n t}^{*}(\\mathbf{0},T,A,B)$ . In the fully general case with no restriction on the utility matrices $A$ and $B$ , we believe there is no closed form solution for the optimal utility, differently from the zero-sum case. However, it would be interesting to understand how $R_{c o n t}^{*}(\\mathbf{0},T,A,B)$ behaves as a function of $A$ and $B$ and how the best strategy for the optimizer looks like. A conjecture in this direction was given by Deng et al. [24]. Perhaps it would be easier to study simpler scenarios, such as the one where r $\\operatorname{ank}(A+B)=1$ , which has been explored in the context of computing equilibria and the convergence of learning algorithms to them ([1], [3]). Another direction is improving on the lower bound for general sum games. Currently, we prove that it is hard to distinguish between the case where the optimizer can achieve reward $\\alpha=T$ and the case where the optimizer cannot achieve more than $\\beta=T-1$ . Is it also hard to distinguish between a reward of at least $\\alpha$ or at most $\\beta$ in cases where $\\alpha-\\beta=\\Omega(T)!$ Are there lower bounds when the learner uses different learning algorithms, such as MWU? Other relevant open directions are extensions to multi-agent settings [13], analyzing how the learner\u2019s utility is impacted by interaction with the optimizer in general-sum games [30], which learning algorithms yield higher utilities against an optimizer, and what algorithms should be used to both learn and optimize at the same time? ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Societal impact. The work studies multi-agent environments and how agents can benefit by anticipating the behavior of other agents. We believe that increasing the academic knowledge in this topic can help learning agents assess their risk of being utilized by other agents and can help to build algorithms that are more resilient to manipulation. As always with new technologies, there is a risk that malicious players will utilize ideas from this paper, which could cause a harmful effect on other agents. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Bharat Adsul, Jugal Garg, Ruta Mehta, Milind Sohoni, and Bernhard Von Stengel. Fast algorithms for rank-1 bimatrix games. Operations Research, 69(2):613\u2013631, 2021. [2] Ioannis Anagnostides, Constantinos Daskalakis, Gabriele Farina, Maxwell Fishelson, Noah Golowich, and Tuomas Sandholm. Near-optimal no-regret learning for correlated equilibria in multi-player general-sum games. In ACM Symposium on Theory of Computing, 2022. [3] Ioannis Anagnostides, Gabriele Farina, Ioannis Panageas, and Tuomas Sandholm. Optimistic mirror descent either converges to nash or to strong coarse correlated equilibria in bimatrix games. Advances in Neural Information Processing Systems, 35:16439\u201316454, 2022. [4] Ioannis Anagnostides, Ioannis Panageas, Gabriele Farina, and Tuomas Sandholm. On last-iterate convergence beyond zero-sum games. In International Conference on Machine Learning, 2022. [5] Eshwar Ram Arunachaleswaran, Natalie Collina, and Jon Schneider. Pareto-optimal algorithms for learning in games. arXiv preprint arXiv:2402.09549, 2024. [6] Maria-Florina Balcan, Avrim Blum, Nika Haghtalab, and Ariel D. Procaccia. Commitment without regrets: Online learning in stackelberg security games. In Proceedings of the Sixteenth ACM Conference on Economics and Computation, pages 61\u201378, 2015. [7] Amir Beck. First-order methods in optimization. SIAM, 2017. [8] Richard Bellman. Dynamic programming and a new formalism in the calculus of variations. Proceedings of the national academy of sciences, 40(4):231\u2013235, 1954. [9] Mark Braverman, Jieming Mao, Jon Schneider, and Matt Weinberg. Selling to a no-regret buyer. In Proceedings of the 2018 ACM Conference on Economics and Computation, pages 523\u2013538, 2018.   \n[10] George W Brown. Iterative solution of games by fictitious play. Act. Anal. Prod Allocation, 13 (1):374, 1951.   \n[11] William Brown, Jon Schneider, and Kiran Vodrahalli. Is learning in games good for the learners? Advances in Neural Information Processing Systems, 36, 2024.   \n[12] S\u00e9bastien Bubeck et al. Convex optimization: Algorithms and complexity. Foundations and Trends\u00ae in Machine Learning, 8(3-4):231\u2013357, 2015.   \n[13] Linda Cai, S Matthew Weinberg, Evan Wildenhain, and Shirley Zhang. Selling to multiple no-regret buyers. In International Conference on Web and Internet Economics, pages 113\u2013129. Springer, 2023.   \n[14] Emilio Calvano, Giacomo Calzolari, Vincenzo Denicolo, and Sergio Pastorello. Artificial intelligence, algorithmic pricing, and collusion. American Economic Review, 110(10):3267\u2013 3297, 2020.   \n[15] Nicolo Cesa-Bianchi. Analysis of two gradient-based algorithms for on-line regression. In Proceedings of the tenth annual conference on Computational learning theory, pages 163\u2013170, 1997.   \n[16] Nicolo Cesa-Bianchi and G\u00e1bor Lugosi. Prediction, learning, and games. Cambridge university press, 2006.   \n[17] Nicolo Cesa-Bianchi, Yoav Freund, David Haussler, David P Helmbold, Robert E Schapire, and Manfred K Warmuth. How to use expert advice. Journal of the ACM (JACM), 44(3):427\u2013485, 1997.   \n[18] Yiling Chen and Tao Lin. Persuading a behavioral agent: Approximately best responding and learning. arXiv preprint arXiv:2302.03719, 2023.   \n[19] Yiling Chen and Tao Lin. Persuading a behavioral agent: Approximately best responding and learning. arXiv preprint arXiv:2302.03719, 2023.   \n[20] Natalie Collina, Eshwar Ram Arunachaleswaran, and Michael Kearns. Efficient stackelberg strategies for finitely repeated games. In Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems, pages 643\u2013651, 2023.   \n[21] Vincent Conitzer and Tuomas Sandholm. Computing the optimal strategy to commit to. In Proceedings of the 7th ACM Conference on Electronic Commerce, pages 82\u201390, 2006.   \n[22] Constantinos Daskalakis, Maxwell Fishelson, and Noah Golowich. Near-optimal no-regret learning in general games. Advances in Neural Information Processing Systems, 34:27604\u2013 27616, 2021.   \n[23] Yuan Deng, Jon Schneider, and Balasubramanian Sivan. Prior-free dynamic auctions with low regret buyers. Advances in Neural Information Processing Systems, 32, 2019.   \n[24] Yuan Deng, Jon Schneider, and Balasubramanian Sivan. Strategizing against no-regret learners. Advances in neural information processing systems, 32, 2019.   \n[25] Heng Du and Tiaojun Xiao. Pricing strategies for competing adaptive retailers facing complex consumer behavior: Agent-based model. International Journal of Information Technology & Decision Making, 18(06):1909\u20131939, 2019.   \n[26] Gabriele Farina, Ioannis Anagnostides, Haipeng Luo, Chung-Wei Lee, Christian Kroer, and Tuomas Sandholm. Near-optimal no-regret learning dynamics for general convex games. In Neural Information Processing Systems (NeurIPS), 2022.   \n[27] S. Freund and L. Gittins. Strategic learning and teaching in games. Econometrica, 66(3): 597\u2013625, 1998.   \n[28] Nick Gravin, Yuval Peres, and Balasubramanian Sivan. Tight lower bounds for multiplicative weights algorithmic families. In 44th International Colloquium on Automata, Languages, and Programming (ICALP 2017). Schloss-Dagstuhl-Leibniz Zentrum f\u00fcr Informatik, 2017.   \n[29] Xinxiang Guo and Yifen Mu. The optimal strategy against hedge algorithm in repeated games. arXiv preprint arXiv:2312.09472, 2023.   \n[30] Guru Guruganesh, Yoav Kolumbus, Jon Schneider, Inbal Talgam-Cohen, Emmanouil-Vasileios Vlatakis-Gkaragkounis, Joshua R Wang, and S Matthew Weinberg. Contracting with a learning agent. arXiv preprint arXiv:2401.16198, 2024.   \n[31] Nika Haghtalab, Thodoris Lykouris, Sloan Nietert, and Alexander Wei. Learning in stackelberg games with non-myopic agents. In Proceedings of the 23rd ACM Conference on Economics and Computation, pages 917\u2013918, 2022.   \n[32] David Haussler, Jyrki Kivinen, and Manfred K Warmuth. Tight worst-case loss bounds for predicting with expert advice. In European Conference on Computational Learning Theory, pages 69\u201383. Springer, 1995.   \n[33] Richard M Karp. Reducibility among combinatorial problems. Springer, 2010.   \n[34] Yoav Kolumbus and Noam Nisan. Auctions between regret-minimizing agents. In Proceedings of the ACM Web Conference 2022, pages 100\u2013111, 2022.   \n[35] Yoav Kolumbus and Noam Nisan. How and why to manipulate your own agent: On the incentives of users of learning agents. In Advances in Neural Information Processing Systems, volume 35, pages 28080\u201328094, 2022.   \n[36] Niklas Lauffer, Mahsa Ghasemi, Abolfazl Hashemi, Yagiz Savas, and Ufuk Topcu. No-regret learning in dynamic stackelberg games. In arXiv preprint arXiv:2203.17184, 2022.   \n[37] Yishay Mansour, Mehryar Mohri, Jon Schneider, and Balasubramanian Sivan. Strategizing against learners in bayesian games. In Conference on Learning Theory, pages 5221\u20135252. PMLR, 2022.   \n[38] Janusz Marecki, Gerry Tesauro, and Richard Segal. Playing repeated stackelberg games with unknown opponents. In Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems - Volume 2, pages 821\u2013828, 2012.   \n[39] Denis Nekipelov, Vasilis Syrgkanis, and Eva Tardos. Econometrics for learning agents. In Proceedings of the sixteenth acm conference on economics and computation, pages 1\u201318, 2015.   \n[40] Gali Noti and Vasilis Syrgkanis. Bid prediction in repeated auctions with learning. In Proceedings of the Web Conference 2021, pages 3953\u20133964, 2021.   \n[41] Binghui Peng, Weiran Shen, Pingzhong Tang, and Song Zuo. Learning optimal strategies to commit to. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 2149\u20132156, 2019.   \n[42] Georgios Piliouras, Ryann Sim, and Stratis Skoulakis. Beyond time-average convergence: Nearoptimal uncoupled online learning via clairvoyant multiplicative weights update. Advances in Neural Information Processing Systems, 35:22258\u201322269, 2022.   \n[43] Julia Robinson. An iterative method of solving a game. Annals of mathematics, pages 296\u2013301, 1951.   \n[44] Vasilis Syrgkanis, Alekh Agarwal, Haipeng Luo, and Robert E. Schapire. Fast convergence of regularized learning in games. In Advances in Neural Information Processing Systems, volume 28, 2015.   \n[45] J. Youn. Learning algorithms in games. Journal of Game Theory, 56(2):123\u2013134, 2004.   \n[46] Brian Hu Zhang, Gabriele Farina, Ioannis Anagnostides, Federico Cacciamani, Stephen Marcus McAleer, Andreas Alexander Haupt, Andrea Celli, Nicola Gatti, Vincent Conitzer, and Tuomas Sandholm. Computing optimal equilibria and mechanisms via learning in zero-sum extensiveform games. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Additional Definitions ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We introduce the following definitions that will help us in the future proofs. ", "page_idx": 13}, {"type": "text", "text": "Definition 2 (Historical Rewards for the learner). The historical rewards for the learner in continuous games at time $t_{\\perp}$ , denoted by $h(t)\\in\\mathbb{R}^{m}$ , is an $m$ dimensional vector that corresponds to the sum of rewards achieved by each action of the learner against the history of play in the game so far. We assume a general setting where the learner at $t=0$ might have non-zero historical rewards, i.e. $h(0)\\in\\mathbb{R}^{m}$ . If the strategy of the optimizer is $x:[0,t]\\rightarrow\\Delta(A)$ we get: ", "page_idx": 13}, {"type": "equation", "text": "$$\nh(t)=h(0)+\\int_{0}^{t}B^{\\top}x(t)d t\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "In the discrete setting, given that the optimizer has played $x_{0},x_{1},\\ldots,x_{t-1}\\in\\Delta(A)$ , we have for $t\\geq1$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\nh(t)=h(0)+\\sum_{i=0}^{t-1}B^{\\top}x_{i}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Definition 3 (Value of the game). In a zero sum case, where $B=-A$ we define the value of the game for the optimizer as $\\operatorname{Val}(A)$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname{Val}(A)=\\operatorname*{min}_{y\\in\\Delta(B)}\\operatorname*{max}_{x\\in\\Delta(A)}x^{\\top}A y\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Definition 4 (Best Response Set). For a given strategy of the optimizer $x\\in\\Delta(A)$ , denote by $\\mathrm{BR}(x)$ the set of best responses by the learner: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathrm{{BR}}(x)=\\left\\{e_{i}|i\\in\\mathrm{{argmin}}_{i\\in[m]}\\left[x^{\\top}A e_{i}\\right]\\right\\}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Definition 5 (Min-max strategies). Denote with MinMaxStrats $_{o p t}(A)$ the set of strategies of the optimizer that achieve the value of the game. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathrm{MinMaxStrats}_{o p t}(A)=\\left\\{x\\in\\Delta(A)\\;\\middle|\\;\\mathrm{Val}(A)=\\operatorname*{min}_{i\\in[m]}x^{\\top}A e_{i}\\right\\}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "A.1 Algorithms for the learner ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "For completeness, we provide pseudocode for the algorithms. ", "page_idx": 13}, {"type": "text", "text": "Definition 6 (MWU Algorithm). Fix the step size $0\\le\\eta\\le\\frac{1}{2}$ . In a game where the utility matrix of the learner isB, the MWU algorithm is as follows: ", "page_idx": 13}, {"type": "table", "img_path": "0uGlKYS7a2/tmp/290938a360035672a07623f2f8aad6951b2fc143ca6e7caf74ad199e27e5929e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "Definition 7 (Replicator Dynamics). Suppose $x:[0,T]\\rightarrow\\Delta(A)$ is a strategy for the optimizer. If the learner is using the replicator dynamics, the strategy for the learner at time $t$ is given by: ", "page_idx": 13}, {"type": "equation", "text": "$$\ny_{i}(t)=\\frac{\\exp\\left(\\eta\\int_{0}^{t}x(s)^{\\top}B e_{i}\\right)}{\\sum_{j=1}^{n}\\exp\\left(\\eta\\int_{0}^{t}x(s)^{\\top}B e_{j}\\right)},i=1,2,\\ldots,m\\;.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Definition 8 (Best Response Algorithm). In a game where the utility matrix of the learner is $B$ , the best response algorithm is as follows, assuming we break ties lexicographically (i.e. if $h_{i}(t)=h_{j}(t)$ , then $b_{i}$ beats $b_{j}$ if $i<j.$ ): ", "page_idx": 14}, {"type": "table", "img_path": "0uGlKYS7a2/tmp/c3d87b556551036b8794d9ac43b4e8a078cc4e7ee5adc3cbadb2be17154aac64.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "B Optimization in zero-sum games: missing proofs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we present some ommited proofs of lemmas mentioned in the main section. ", "page_idx": 14}, {"type": "text", "text": "B.1 Continuous games ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "First, we prove that the optimal strategy for the optimizer in continuous games can be computed efficiently. From theorem 1 we know that the optimal strategy for the optimizer throughout the game, is just $x(t)=x^{*}$ , where $x^{*}$ minimizes $f(x)$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\nf(x)=\\ln\\left(\\sum_{i=1}^{m}e^{\\eta\\left(h_{i}(0)-T e_{i}^{\\top}A^{\\top}x\\right)}\\right)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The above is also known as the log-sum-exp function. This is a convex function, which allows us to compute its minima efficiently. We use the following two lemmas. ", "page_idx": 14}, {"type": "text", "text": "Lemma 1 (Example 5.15 [7]). The log-sum-exp function $f\\ :\\ \\mathbb{R}^{n}\\ \\rightarrow\\ \\mathbb{R},$ for which $f(\\mathbf{x})\\;=$ $\\ln\\left(\\sum_{i=1}^{m}e^{x_{i}}\\right)$ , is 1-smooth with respect to the $\\ell_{2},\\ell_{\\infty}$ norms and convex. ", "page_idx": 14}, {"type": "text", "text": "Lemma 2 (Theorem 3.8, [12]). For a function $f:\\mathcal{X}\\to\\mathbb{R}$ that is convex and $\\beta$ -smooth with respect to a norm $\\lVert\\cdot\\rVert$ and where $R=\\operatorname*{sup}_{x,y}\\|x-y\\|$ , the Frank-Wolfe algorithm can find $x_{t}$ on $O(t)$ time for which: ", "page_idx": 14}, {"type": "equation", "text": "$$\nf(x_{t})-f(x^{*})\\leq\\frac{2\\beta R^{2}}{t+1}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We can therefore compute the approximate optimal strategy for the optimizer efficiently: ", "page_idx": 14}, {"type": "text", "text": "Proposition 5. For zero sum continuous games where the learner is using replicator dynamics, we can find an \u03f5 approximate optimal strategy for the optimizer in $\\begin{array}{r}{O(\\frac{1}{\\epsilon\\eta})}\\end{array}$ time. That is we can find $x$ for which: ", "page_idx": 14}, {"type": "equation", "text": "$$\nR_{c o n t}^{*}(\\mathbf{0},T,A,-A)-R_{c o n t}(x,\\mathbf{0},T,A,-A)\\leq\\epsilon\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. Consider the function $\\begin{array}{r}{g(x)=\\ln\\left(\\sum_{i=1}^{m}e^{\\eta\\left(h_{i}(0)-e_{i}^{\\top}A^{\\top}x T\\right)}\\right)}\\end{array}$ . From 1, we also know that $g$ is 1\u2212smooth with respect to the $\\ell_{\\infty}$ norm. Note that since the strategies are on the simplex, we have that $R\\,=\\,\\operatorname*{sup}_{x,y}\\left\\|x\\,-\\,y\\right\\|\\,=\\,1$ . Using lemma 2, with $\\begin{array}{r}{t=\\frac{2}{\\epsilon\\eta}-\\bar{1}}\\end{array}$ we can find $x$ for which $g(x)-g(x^{*})\\leq\\epsilon\\eta$ . Using theorem 1, we get: ", "page_idx": 14}, {"type": "equation", "text": "$$\nR_{c o n t}^{*}(\\mathbf{0},T,A,-A)-R_{c o n t}(x,\\mathbf{0},T,A,-A)=\\frac{g(x)-g(x^{*})}{\\eta}\\leq\\epsilon\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "as required. ", "page_idx": 14}, {"type": "text", "text": "Next, we bound the range of the possibilities for the utility gained by the best optimizer: ", "page_idx": 14}, {"type": "text", "text": "Proposition 6. In the zero-sum continuous setting, the optimizer\u2019s optimal utility is bounded as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{Val}(A)\\cdot T\\leq R_{c o n t}^{*}(\\mathbf{0},T,A,-A)\\leq\\mathrm{Val}(A)\\cdot T+\\frac{\\ln m}{\\eta}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\operatorname{Val}(A)$ is as defined in 3. ", "page_idx": 15}, {"type": "text", "text": "Proof. We have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\mathbb{t}}_{c o n t}^{*}(\\mathbf{0},T,A,-A)=\\displaystyle\\frac{1}{\\eta}\\left(\\ln\\left(m\\right)-\\operatorname*{min}_{x}\\left(\\sum_{i=1}^{m}e^{-\\eta\\cdot\\frac{\\mathbf{r}^{\\top}}{A_{i}^{\\top}}x T}\\right)\\right)=}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\frac{1}{\\eta}\\left(\\ln\\left(m\\right)-\\operatorname*{min}_{x\\in\\Delta(A)}\\operatorname*{max}_{j\\in\\{1,2,\\ldots,m\\}}\\ln\\left(e^{-\\eta\\cdot\\frac{\\mathbf{r}^{\\top}}{A}\\mathrm{T}_{x}}\\underset{i=1}^{m}e^{\\eta(e_{j}^{\\top}-e_{i}^{\\top})A^{\\top}x T}\\right)\\right)=}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\frac{1}{\\eta}\\left(\\ln\\left(m\\right)-\\operatorname*{min}_{x\\in\\Delta(A)}j\\in\\{1,2,\\ldots,m\\}\\left(-\\eta e_{j}^{\\top}A^{\\top}x T+\\ln\\left(\\sum_{i=1}^{m}e^{\\eta(e_{j}^{\\top}-e_{i}^{\\top})A^{\\top}x T}\\right)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\frac{1}{\\eta}\\left(\\ln\\left(m\\right)+\\operatorname*{max}_{x\\in\\Delta(A)}j\\in\\{1,2,\\ldots,m\\}\\left(\\eta e_{j}^{\\top}A^{\\top}x T-\\ln\\left(\\sum_{i=1}^{m}e^{\\eta(e_{j}^{\\top}-e_{i}^{\\top})A^{\\top}x T}\\right)\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that picking out the $j$ for which $e_{j}^{\\top}A^{\\top}x\\leq e_{i}^{\\top}A^{\\top}x$ for all $i\\in[m]$ , makes $e^{\\eta(e_{j}^{\\top}-e_{i}^{\\top})A^{\\top}x T}\\leq$ 1 for all $i\\;=\\;1,2,\\ldots,m$ . On the other hand $e^{\\eta(e_{j}^{\\top}-e_{j}^{\\top})A^{\\top}x T}\\;=\\;1$ . That gives $\\ln1\\;=\\;0\\;\\leq$ $\\begin{array}{r}{\\ln\\left(\\sum_{i=1}^{m}e^{\\eta(e_{j}^{\\top}-e_{i}^{\\top})A^{\\top}x T}\\right)\\le\\ln m}\\end{array}$ . We finally have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{x\\in\\Delta(A)}\\operatorname*{min}_{j\\in\\{1,2,\\ldots,m\\}}e_{j}^{\\top}A^{\\top}x T\\leq R_{c o n t}^{*}(\\mathbf{0},T,A,-A)\\leq\\operatorname*{max}_{x\\in\\Delta(A)}\\operatorname*{min}_{j\\in\\{1,2,\\ldots,m\\}}e_{j}^{\\top}A^{\\top}x T+\\frac{\\ln m}{\\eta}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which is exactly what we want. ", "page_idx": 15}, {"type": "text", "text": "If we take the limit of the game as $\\eta T$ goes to infinity, we can lower bound the optimal rewards of the optimizer by considering a strategy from the set MinMaxStrats $(A)$ as defined in 5, that has the least number of best responses. ", "page_idx": 15}, {"type": "text", "text": "Proposition 7. As $\\eta T\\to\\infty,$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{c o n t}^{*}(\\mathbf{0},T,A,-A)}\\\\ &{\\quad\\ge\\mathrm{Val}(A)\\cdot T+\\frac{1}{\\eta}\\left(\\ln(m)-\\ln\\left(\\operatorname*{min}_{x^{*}\\in\\mathrm{MinMaxStrats}_{o p t}(A)}|\\mathrm{\\mathrm{BR}}(x^{*})|\\right)(1+o_{\\eta T}(1))\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $o_{\\eta T}(1)$ denotes a terms that decays to 0 as $\\eta T\\to\\infty,$ , and $\\operatorname{BR}(x)$ is the set of best responses of a strategy $x$ of the optmizer as defined in $^{4}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. Fix some $x\\in\\Delta(A)$ . Note that from Theorem 6 we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\nR_{c o n t}^{*}(\\mathbf{0},T,A,-A)\\geq\\frac{\\ln{(m)}}{\\eta}+\\left(e_{j}^{\\top}A^{\\top}x T-\\frac{1}{\\eta}\\ln\\left(\\sum_{i=1}^{m}e^{\\eta(e_{j}^{\\top}-e_{i}^{\\top})A^{\\top}x T}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "for any $j\\,\\in\\,[m]$ . Choose $j\\,\\in\\,\\mathrm{BR}(x)$ (thus it minimizes $e_{j}^{\\top}A^{\\top}x)$ . Denote with $d_{i}(j,x)=(e_{j}\\,-$ $e_{i})^{\\top}A^{\\top}x$ . Note that for all $i\\,\\in\\,[m]/\\,\\mathrm{BR}(x)$ we have $d_{i}(j,x)\\,=\\,0$ and for $i\\not\\in\\mathrm{BR}(x)$ we have ", "page_idx": 15}, {"type": "text", "text": "$d_{i}(j,x)<0$ . We can rewrite this as: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{c o n}^{\\mathrm{'}}(\\mathbf{0},T,A,-A)}\\\\ &{\\geq\\displaystyle\\frac{\\ln(m)}{\\eta}+\\left(e_{f}^{\\top}A^{\\top}x T-\\frac{1}{\\eta}\\ln\\left(|\\mathrm{BR}(x)|+\\sum_{i\\in[m]/\\mathrm{BR}(x)}e^{\\eta T_{i}\\hat{T}_{i}(x,\\cdot)}\\right)\\right)}\\\\ &{=\\displaystyle\\frac{\\ln(m)}{\\eta}+}\\\\ &{+\\left(e_{f}^{\\top}A^{\\top}x T-\\frac{1}{\\eta}\\ln\\left(|\\mathrm{BR}(x)|\\right)+\\frac{1}{\\eta}\\ln\\left(1+\\frac{\\sum_{i\\in[m]/\\mathrm{BR}(x)}e^{\\eta T_{i}A(\\cdot,\\cdot,x)}}{|\\mathrm{BR}(x)|}\\right)\\right)=}\\\\ &{=\\displaystyle\\frac{\\ln(m)}{\\eta}+}\\\\ &{+\\left(e_{f}^{\\top}A^{\\top}x T-\\frac{1}{\\eta}\\ln\\left(|\\mathrm{BR}(x)|\\right)\\left(1+\\frac{\\ln\\left(1+\\frac{\\sum_{i\\in[m]/\\mathrm{BR}(x)}e^{\\eta T_{i}A(\\cdot,\\cdot,\\theta)}}{|\\mathrm{BR}(x)|}\\right)}{\\ln\\left(|\\mathrm{BR}(x)|\\right)}\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Note that as $\\eta T\\to\\infty$ , the term $\\begin{array}{r}{\\frac{\\ln\\left(1+\\frac{\\sum_{i\\in[m]/\\mathrm{\\tiny~BR}(x)}e^{\\eta T\\cdot d_{i}(j,x)}}{|\\mathrm{\\tiny~BR}(x)|}\\right)}{\\ln\\left(|\\mathrm{\\tiny~BR}(x)|\\right)}}\\end{array}$ goes to 0, as $d_{i}(j,x)<0$ . Therefore this gives : ", "page_idx": 16}, {"type": "equation", "text": "$$\nR_{c o n t}^{*}(\\mathbf{0},T,A,-A)\\ge\\frac{\\ln{(m)}}{\\eta}+\\left(e_{j}^{\\top}A^{\\top}x T-\\frac{1}{\\eta}\\ln(|\\operatorname{BR}(x)|)(1+o_{\\eta T}(1))\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Choosing $x\\in\\mathrm{MinMaxStrats}_{o p t}(A)$ that minimizes $|\\operatorname{BR}(x)|$ yields the desired result. ", "page_idx": 16}, {"type": "text", "text": "Below we provide an example of a game with multiple min-max strategies for the optimizer, however only one of them gives optimal rewards when used against MWU. ", "page_idx": 16}, {"type": "text", "text": "Example 1. Consider the zero sum game where: ", "page_idx": 16}, {"type": "table", "img_path": "0uGlKYS7a2/tmp/74b9c2233e3248c3781a244379fa2953e84f37d9e6719551f23b61bdcd6ff42d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Notice that the $\\operatorname{Val}(A)=1$ , and there are multiple strategies that achieve this result, that might have a different number of best responses. For example $\\begin{array}{r}{x_{1}=(\\bar{\\frac{1}{n}},\\frac{1}{n},\\dots,\\frac{1}{n},0,0)}\\end{array}$ has $n+1$ best responses; $\\operatorname{BR}(x_{1})\\,=\\,\\{b_{1},b_{2},\\dots,b_{n+3}\\}$ . One of the strategies that achieves the value of the game with the least number of best responses is x\u2217 = (0, 0, . . . , 12, 12, 0 that has only 1 best response, namely $\\mathrm{BR}(x^{*})=\\{b_{n+3}\\}$ . For this game playing $x^{*}$ as $\\eta T\\to\\infty$ yields $\\begin{array}{r}{R_{c o n t}^{*}({\\bf0},0,A,-A)=T+\\frac{\\ln(n+3)}{\\eta}}\\end{array}$ ", "page_idx": 16}, {"type": "text", "text": "B.2 Discrete Case ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We can have the analogous definitions of optimal rewards for the optimizer for the discrete case. Given the sequence of actions of the optimizer given by $x:\\{1,\\ldots,T\\}\\rightarrow\\Delta(A)$ , we can define: ", "page_idx": 16}, {"type": "equation", "text": "$$\nR_{d i s c}(x,h(0),T,A,B)=\\sum_{t=1}^{T}\\frac{\\sum_{i=1}^{m}e^{\\eta\\left(h_{i}(0)+e_{i}^{\\top}B^{\\top}\\sum_{s=1}^{t}x(s)\\right)}\\cdot e_{i}^{\\top}A^{\\top}x(t)}{\\sum_{i=1}^{m}e^{\\eta\\left(h_{i}(0)+e_{i}^{\\top}B^{\\top}\\sum_{s=1}^{t}x(s)\\right)}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Again, we are interested in finding the sequence of actions that maximize the rewards: ", "page_idx": 17}, {"type": "equation", "text": "$$\nR_{d i s c}^{*}(h(0),T,A,B)=\\operatorname*{max}_{0\\leq t\\leq T}\\sum_{t=1}^{T}\\frac{\\sum_{i=1}^{m}e^{\\eta\\left(h_{i}(0)+e_{i}^{\\top}B^{\\top}\\sum_{s=1}^{t}x(s)\\right)}\\cdot e_{i}^{\\top}A^{\\top}x(t)}{\\sum_{i=1}^{m}e^{\\eta\\left(h_{i}(0)+e_{i}^{\\top}B^{\\top}\\sum_{s=1}^{t}x(s)\\right)}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We begin by proving that the rewards for the discrete game will always be more than the rewards for the analogous continuous game. ", "page_idx": 17}, {"type": "text", "text": "Proposition 8. In the zero-sum discrete game, the optimizer can receive more utility compared to the continuous game. This can be done by playing discretely the same optimal strategy $x^{*}$ for the continuous game (as defined in Theorem $^{\\,l}$ ). Consequently, ", "page_idx": 17}, {"type": "equation", "text": "$$\nR_{d i s c}^{*}(\\mathbf{0},T,A,-A)\\geq R_{c o n t}^{*}(\\mathbf{0},T,A,-A)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. From Theorem 1 we know that $R_{c o n t}^{*}(\\mathbf{0},0,A,-A)$ is achieved by a constant strategy $x^{*}$ for the optimizer throughout the game. We will prove that using that same strategy $x^{*}$ will yield more reward for the discrete case than the continuous case, i.e.: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{d i s c}^{*}(\\mathbf{0},\\boldsymbol{0},\\boldsymbol{A},-\\boldsymbol{A})\\geq\\displaystyle\\sum_{t=0}^{T-1}\\frac{\\sum_{i=1}^{m}e^{-\\eta e_{i}^{\\top}A^{\\top}x^{*}t}\\cdot e_{i}^{\\top}A^{\\top}x^{*}}{\\sum_{i=1}^{m}e^{-\\eta e_{i}^{\\top}A^{\\top}x^{*}t}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geq\\displaystyle\\int_{t=0}^{T}\\frac{\\sum_{i=1}^{m}e^{-\\eta e_{i}^{\\top}A^{\\top}x^{*}t}\\cdot e_{i}^{\\top}A^{\\top}x^{*}}{\\sum_{i=1}^{m}e^{-\\eta e_{i}^{\\top}A^{\\top}x^{*}t}}d t=R_{c o n t}^{*}(\\mathbf{0},\\boldsymbol{0},\\boldsymbol{A},-\\boldsymbol{A})}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "All we need for the above to be true is that the function $\\begin{array}{r l r}{f(t)\\!\\!\\!}&{{}=}&{\\!\\!\\!\\frac{\\sum_{i=1}^{m}e^{-\\eta e_{i}^{\\top}A^{\\top}x^{*}t}\\cdot e_{i}^{\\top}A^{\\top}x^{*}}{\\sum_{i=1}^{m}e^{-\\eta e_{i}^{\\top}A^{\\top}x^{*}t}}\\!\\!\\!}&{=}\\end{array}$ \u03b71  im=im1= 1e e\u2212c\u2212icti\u00b7tci is non-increasing. This is because Rdisc(0, 0, A, \u2212A) \u2212Rcont(0, 0, A, \u2212A) = $\\begin{array}{r}{\\sum_{t=0}^{T-1}f(t)-\\int_{0}^{T}f(t)d t=\\sum_{t=0}^{T-1}\\left(f(t)-\\int_{t}^{t+1}f(s)d s\\right)}\\end{array}$ , so by proving $f$ is non-increasing we can prove $\\begin{array}{r}{f(t)\\geq\\int_{t}^{t+1}f(s)d s}\\end{array}$ . Therefore, all we are left with is showing that $\\frac{d f(t)}{d t}\\,<\\,0$ for all $t\\geq0$ . Notice that: ", "page_idx": 17}, {"type": "equation", "text": "$$\nf^{\\prime}(t)=-\\frac{1}{\\eta}\\left(\\frac{\\sum_{i=1}^{m}e^{-c_{i}t}\\cdot c_{i}^{2}}{\\sum_{i=1}^{m}e^{-c_{i}t}}+\\left(\\frac{\\sum_{i=1}^{m}e^{-c_{i}t}\\cdot c_{i}}{\\sum_{i=1}^{m}e^{-c_{i}t}}\\right)^{2}\\right)<0\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which concludes the proof. ", "page_idx": 17}, {"type": "text", "text": "We continue by proving that there are instances of discrete games, that achieve reward for the optimizer approximately $\\textstyle{\\frac{\\eta T}{2}}$ more than the analogous continuous game in Theorem 9. We also prove that this difference between rewards in continuous and discrete games cannot be greater than $\\textstyle{\\frac{\\eta T}{2}}$ in Theorem 10. ", "page_idx": 17}, {"type": "text", "text": "Proposition 9. There is a zero sum game instance for which we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\nR_{d i s c}^{*}(\\mathbf{0},T,A,-A)\\geq R_{c o n t}^{*}(\\mathbf{0},T,A,-A)+\\frac{\\operatorname{tanh}{(\\eta)}\\cdot T}{2}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The above is achieved by taking the optimal strategy $x^{*}$ for the continuous game (as defined in $^{\\,l}$ ) and playing it over discrete time. ", "page_idx": 17}, {"type": "text", "text": "Proof. Consider the matching pennies problem, where the utility matrices are: ", "page_idx": 17}, {"type": "table", "img_path": "0uGlKYS7a2/tmp/10a2f2f2e331995bfc448a1496df3cd80155ed665b5584842a686ed8873dc4bc.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Note that $\\operatorname{Val}(A)=0$ , and $\\mathrm{MinMaxStrats}_{o p t}(A)=\\{({\\frac{1}{2}},{\\frac{1}{2}})\\}$ , and $\\textstyle|\\operatorname{BR}(({\\frac{1}{2}},{\\frac{1}{2}}))|=2$ . This gives $R_{c o n t}^{*}({\\bf0},T,A,-A)=0$ . On the other hand, consider the sequence of actions of the optimizer for the discrete game to be as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\nx(t)={\\binom{a_{2},\\quad{\\mathrm{if~}}t{\\mathrm{~is~odd}}}{a_{1},\\quad{\\mathrm{otherwise}}}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We claim that at round $t$ where $t$ is odd, the learner will play $\\left({\\frac{1}{2}},{\\frac{1}{2}}\\right)$ , while when $t$ is even, the learner will play (e\u03b7+e\u03b7e\u2212\u03b7 ,e\u03b7e+\u2212e\u03b7\u2212\u03b7 ) . Indeed, at t = 1, 3, 5, . . . , the optimizer will have played equal times to play the action $\\left({\\frac{1}{2}},{\\frac{1}{2}}\\right)$ $a_{1}$ and . When $a_{2}$ , thus the rewards for actions $t$ is even, action $a_{2}$ has been played one more time than $b_{1}$ and $b_{2}$ so far will be equal, leading the learner $a_{1}$ is played. That means that at time $t$ for $b_{1}$ the historical reward is 1 , while the historical reward for $b_{2}$ is $-1$ . Thus the strategy for the learner for even $t$ will indeed be $\\Big(\\frac{e^{\\eta}}{e^{\\eta}\\!+\\!e^{-\\eta}},\\,\\frac{e^{-\\eta}}{e^{\\eta}\\!+\\!e^{-\\eta}}\\Big)$ . Therefore the optimizer\u2019s reward will be: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l l l}{{\\displaystyle\\mathbb{7}_{d i s c}(x(t),\\mathbf{0},T,A,-A)=\\frac{T}{2}\\cdot[0}}&{{1]\\left[\\frac{1}{-1}}}&{{-1\\right]\\left[\\frac{1}{2}\\right]+\\frac{T}{2}\\cdot\\left[\\frac{e^{\\eta}}{e^{\\eta}+e^{-\\eta}}}}&{{\\frac{e^{-\\eta}}{e^{\\eta}+e^{-\\eta}}}}\\end{array}\\right]\\left[\\begin{array}{c c}{{1}}&{{-1}}\\\\ {{-1}}&{{1}}\\end{array}\\right]\\left[\\begin{array}{c c}{{1}}&{{-1}}\\\\ {{0}}\\\\ {{17}}&{{-2}}\\end{array}\\right]\\frac{\\sqrt{2}}{2}\\cdot\\left[\\begin{array}{c c}{{\\frac{e^{-\\eta}}{e^{\\eta}+e^{-\\eta}}}}&{{\\frac{e^{-\\eta}}{e^{\\eta}+e^{-\\eta}}}}\\end{array}\\right]\\frac{\\displaystyle[1}{-1}}&{{\\frac{e^{-\\eta}}{1}}}\\end{array}\\right]\\left[\\begin{array}{c c}{{1}}&{{-1}}\\\\ {{0}}\\\\ {{-1}}&{{\\frac{e^{-\\eta}}{2}}}\\end{array}\\right]\\frac{\\sqrt{2}}{\\displaystyle\\mathbb{7}_{\\rho}}\\left[\\begin{array}{c c}{{1}}&{{-1}}\\\\ {{0}}&{{-2}}\\end{array}\\right]\\frac{\\sqrt{2}}{\\displaystyle\\mathbb{7}_{\\rho}}\\left[\\begin{array}{c c}{{\\frac{e^{\\eta}}{e^{\\eta}+e^{-\\eta}}}}&{{-1}}\\\\ {{0}}&{{\\frac{e^{-\\eta}}{2}}}\\end{array}\\right]\\frac{\\sqrt{2}}{\\displaystyle\\mathbb{7}_{\\rho}}\\left[\\begin{array}{c c}{{\\frac{e^{\\eta}}{e^{\\eta}+e^{-\\eta}}}}&{{-1}}\\\\ {{0}}&{{\\frac{e^{-\\eta}}{2}}}\\end{array}\\right]\\frac{\\sqrt{2}}{\\displaystyle\\mathbb{7}_{\\rho}}\\left[\\begin{array}{c c}{{\\frac{e^{\\eta}}{e^{\\eta}+e^{-\\eta}}}}&{{-1}}\\\\ {{0}}&{{\\frac{e^{-\\eta}}{2}}}\\end{array}\\right].}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Combining the two gives the desired result. ", "page_idx": 18}, {"type": "text", "text": "Proposition 10. In a zero-sum game where the entries of $A$ satisfy $A_{i j}\\in[-1,1]$ , we have that the optimal utility the optimizer can achieve in the discrete game is not more than $\\eta T/2$ compared to the continuous game: ", "page_idx": 18}, {"type": "equation", "text": "$$\nR_{d i s c}^{*}({\\bf0},T,A,-A)-R_{c o n t}^{*}({\\bf0},T,A,-A)\\leq\\eta T/2\\;.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. Suppose the optimal strategy for the discrete game is $x_{d i s c}$ . We will construct a strategy $x_{c o n t}$ for the continuous game, that approximates the utility of the discrete game pretty well. The strategy is as follows; if $x_{d i s c}(t)=x_{t}\\in\\Delta(A)$ , then $x_{c o n t}(s)=x_{t},\\forall a\\in[t,t+1)$ . Note that the historical rewards of the learner for $t=0,1,2,\\ldots,T-1$ are the same for both the continuous and the discrete game for these specific strategies. Suppose the history at time $t$ is $h(t)$ , the strategy being played at time $t$ be $x$ , and $u_{i}=e_{i}^{\\top}A^{\\top}x$ . We have: ", "page_idx": 18}, {"type": "equation", "text": "$$\nR_{d i s c}^{*}(h(t),1,A,-A)=\\frac{\\sum_{i=1}^{m}e^{\\eta h_{i}(t)}u_{i}}{\\sum_{i=1}^{m}e^{\\eta h_{i}(t)}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We also have: ", "page_idx": 18}, {"type": "equation", "text": "$$\nR_{c o n t}^{*}(h(t),1,A,-A)=\\int_{0}^{1}\\frac{\\sum_{i=1}^{m}e^{\\eta h_{i}(t)-\\eta u_{i}t}\\cdot u_{i}}{\\sum_{i=1}^{m}e^{\\eta h_{i}(t)-\\eta u_{i}t}}d t\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Denote by $\\begin{array}{r}{f(t)=\\frac{\\sum_{i=1}^{m}e^{\\eta h_{i}(t)-\\eta u_{i}t}\\cdot u_{i}}{\\sum_{i=1}^{m}e^{\\eta h_{i}(t)-\\eta u_{i}t}}}\\end{array}$ 1 e\u03b7hi(t)\u2212\u03b7uit\u00b7ui. Then, ", "page_idx": 18}, {"type": "equation", "text": "$$\nR_{d i s c}^{*}(h(t),1,A,-A)=f(0),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "whereas ", "page_idx": 18}, {"type": "equation", "text": "$$\nR_{c o n t}^{*}(h(t),1,A,-A)=\\int_{0}^{1}f(t)d t\\;.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that: ", "page_idx": 18}, {"type": "equation", "text": "$$\nf^{\\prime}(t)={\\frac{-\\eta\\sum_{i=1}^{m}e^{\\eta h_{i}(t)-\\eta u_{i}t}\\cdot u_{i}^{2}}{\\sum_{i=1}^{m}e^{\\eta h_{i}(t)-\\eta u_{i}t}}}+{\\frac{\\eta\\left(\\sum_{i=1}^{m}e^{\\eta h_{i}(t)-\\eta u_{i}t}\\cdot u_{i}\\right)^{2}}{\\left(\\sum_{i=1}^{m}e^{\\eta h_{i}(t)-\\eta u_{i}t}\\right)^{2}}}\\;.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since $u_{i}$ \u2019s are bounded by in $[-1,1]$ , we get that $f^{\\prime}(t)\\geq-\\eta$ . Hence $f(t)\\geq f(0)-\\eta t$ , consequently, ", "page_idx": 18}, {"type": "equation", "text": "$$\nR_{c o n t}^{*}(h(t),1,A,-A)=\\int_{0}^{1}f(t)d t\\geq\\int_{0}^{1}(f(0)-\\eta t)d t=f(0)-\\frac{\\eta}{2}\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore summing up for all $t=0,1,\\ldots,T-1$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\nR_{d i s c}^{*}(\\mathbf{0},T,A,-A)-R_{c o n t}^{*}(\\mathbf{0},T,A,-A)\\leq\\eta T/2\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We conclude with the last proposition of this section which states that in games that satisfy the following Condition, we get that the optimizer can achieve reward at least $T\\operatorname{Val}(A)+\\Omega(\\eta T)$ against a MWU learner. ", "page_idx": 18}, {"type": "text", "text": "Condition 2. There exists a minmax strategy $x$ for the optimizer such that there exists two best responses for the learner, $b_{i_{1}},b_{i_{2}}\\in\\mathrm{BR}(x)$ , which do not identify on support $(x)$ . Namely, there exists an action $a_{k}\\in\\mathrm{support}(x)$ such that $a_{k}^{\\top}A b_{i_{1}}\\neq a_{k}^{\\top}A b_{i_{2}}$ . ", "page_idx": 19}, {"type": "text", "text": "Proposition 11. For any zero-sum game $A\\in\\mathbb{R}^{n\\times n}$ that satisfies Condition $^{\\,l}$ , $\\eta\\in(0,1)$ and $T\\in\\mathbb{N}$ there exists an optimizer, such that against a learner that uses MWU with step size $\\eta,$ , achieves $a$ utility of at least ", "page_idx": 19}, {"type": "equation", "text": "$$\nT\\operatorname{Val}(A)+C_{A}\\eta T,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $C_{A}$ is a constant that depends only on the game matrix $A$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. Let $x,b_{i_{1}},b_{i_{2}},a_{k}$ be the strategies and actions guaranteed from Condition 1. Since $b_{i_{1}}$ and $b_{i_{2}}$ are both best responses for $x$ , $x^{\\top}A e_{i_{1}}=x^{\\top}A\\bar{e}_{i_{2}}$ . By the Condition that there exists an action $a_{k}\\,\\in\\,\\mathrm{support}(x)$ such that $e_{k}^{\\top}A e_{i_{1}}\\neq e_{k}^{\\top}A e_{i_{2}}$ , it follows that there exists two strategies $x^{\\prime},x^{\\prime\\prime}\\in\\Delta(A)$ such that $(x^{\\prime}+x^{\\prime\\prime})/2\\,=\\,x.$ , $x^{\\prime\\top}A e_{i_{1}}\\,>\\,x^{\\prime\\top}A e_{i_{2}}$ and $x^{\\prime\\prime\\top}A e_{i_{1}}<x^{\\prime\\prime\\top}A e_{i_{2}}$ . We propose the following strategy for the learner: at any odd time $t$ , play $x^{\\prime}$ and at any even time $t$ , play $x^{\\prime\\prime}$ . We will prove that in iterations $t$ and $t+1$ the reward of the optimizer is at least $2\\operatorname{Val}(A)+C_{A}\\eta$ by playing as described. Therefore, if we just sum up over all $t$ , we will get the desired result. Now, fix some odd $t$ , and we will lower bound the sum of utilities of the optimizer in iterations $t$ and $t+1$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{x(t)^{\\top}A y(t)+x(t+1)^{\\top}A y(t+1)=x^{\\prime^{\\top}}A y(t)+x^{\\prime\\prime^{\\top}}A y(t+1)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Denote $u_{i}=x^{\\prime\\top}A e_{i}$ and $v_{i}=x^{\\prime\\prime\\top}A e_{i}$ , then ", "page_idx": 19}, {"type": "equation", "text": "$$\nx^{\\prime^{\\top}}A y(t)+x^{\\prime\\prime^{\\top}}A y(t+1)=\\sum_{i=1}^{n}u_{i}y_{i}(t)+\\sum_{i=1}^{n}v_{i}y_{i}(t+1)\\;.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Notice that ", "page_idx": 19}, {"type": "equation", "text": "$$\ny_{i}(t+1)=\\frac{y_{i}(t)e^{-\\eta x^{\\prime}^{\\top}A b_{i}}}{\\sum_{j=1}^{n}y_{j}(t)e^{-\\eta x^{\\prime}^{\\top}A b_{j}}}=\\frac{y_{i}(t)e^{-\\eta u_{i}}}{\\sum_{j=1}^{n}y_{j}(t)e^{-\\eta u_{j}}}\\;.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}u_{i}y_{i}(t)+\\sum_{i=1}^{n}v_{i}y_{i}(t+1)=\\sum_{i=1}^{n}u_{i}y_{i}(t)+\\sum_{i=1}^{n}{\\frac{v_{i}(t)y_{i}(t)e^{-\\eta u_{i}}}{\\sum_{j=1}^{n}y_{j}(t)e^{-\\eta u_{j}}}}\\;.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Notice that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{u_{i}+v_{i}}2=\\frac{(x^{\\prime}+x^{\\prime\\prime})^{\\top}}2A e_{i}=x^{\\top}A e_{i}\\geq\\mathrm{Val}(A)\\;.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Hence, $v_{i}(t)\\geq2\\operatorname{Val}(A)-u_{i}(t)$ , hence the right hand side of Eq. (21) is at least ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}y_{i}(t)u_{i}+2\\operatorname{Val}(A)-\\sum_{i=1}^{n}{\\frac{u_{i}y_{i}(t)e^{-\\eta u_{i}}}{\\sum_{j=1}^{n}y_{i}(t)e^{-\\eta u_{i}}}}=2\\operatorname{Val}(A)+\\sum_{i=1}^{n}y_{i}(t)u_{i}\\left(1-{\\frac{e^{-\\eta u_{i}}}{\\sum_{j=1}^{n}y_{j}(t)e^{-\\eta u_{j}}}}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The right hand side equals $2\\operatorname{Val}(A)$ if $\\eta=0$ . We will differentiate it wrt $\\eta$ to get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{d}{d\\eta}\\left(\\frac{-\\sum_{i=1}^{n}y_{i}(t)u_{i}e^{-\\eta u_{i}}}{\\sum_{j=1}^{n}y_{j}(t)e^{-\\eta u_{j}}}\\right)=\\frac{\\sum_{i=1}^{n}y_{i}(t)u_{i}^{2}e^{-\\eta u_{i}}}{\\sum_{j=1}^{n}y_{j}(t)e^{-\\eta u_{j}}}-\\left(\\frac{\\sum_{i=1}^{n}y_{i}(t)u_{i}e^{-\\eta u_{i}}}{\\sum_{j=1}^{n}y_{j}(t)e^{-\\eta u_{j}}}\\right)^{2}\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The right hand side equals the variance of a random variable, which we denote by $Z_{\\eta}$ , such that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[Z_{\\eta}=u]=\\sum_{i:\\ u_{i}=u}{\\frac{y_{i}e^{-\\eta y_{i}}}{\\sum_{j=1}^{n}u_{j}e^{-\\eta u_{j}}}}\\ .\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We obtain that the sum of utilities for the optimzer in steps $t$ and $t+1$ is at least $2\\operatorname{Val}(A)+$ $\\begin{array}{r}{\\int_{0}^{\\eta}\\mathrm{Var}(Z_{r})d r}\\end{array}$ . Recall that we assumed that $\\eta\\leq1$ . Hence, if we prove that there exists some constant $C_{A}$ such that $\\mathrm{Var}(Z_{r})\\geq C_{A}$ for all $r\\in[0,1]$ , we will obtain that the sum of utilities in iterations $t$ and $t+1$ is at least $2\\operatorname{Val}(A)+\\eta C_{A}$ . This will imply that the sum of utilities in iterations 1 through $T$ is at least $T\\operatorname{Val}(A)+\\eta T C_{A}/2$ , assuming that $T$ is even (if $T$ is odd then we can play a minmax strategy in the last round and get a similar bound). It remains to bound $\\operatorname{Var}(Z_{r})$ . Recall that $b_{i_{1}}$ and $b_{i_{2}}$ are two best responses to $x$ , and since $\\begin{array}{r}{\\sum_{i=1}^{t-1}x(i)=\\frac{t}{2}x.}\\end{array}$ , it holds from the definition of MWU that $y_{i_{1}}(t)$ and $y_{i_{2}}(\\bar{t})$ are among the largest entries of $y(t)$ . Consequently, $y_{i_{1}}(t),y_{i_{2}}(t)\\geq1/n$ . This implies that there exists some $C>0$ (that possibly depends on $A$ and $n$ ), such that for all $r\\in(0,1)$ , $\\mathrm{Pr}[Z_{r}\\,=\\,u_{i_{1}}],\\mathrm{Pr}[Z_{r}\\,=\\,u_{i_{2}}]\\,\\ge\\,C$ . Further, by Condition, $u_{i_{1}}\\,=\\,x^{\\prime\\top}A b_{i_{1}}\\,\\neq\\,x^{\\prime\\top}A b_{i_{2}}\\,=\\,u_{i_{2}}$ Consequently, there exists some $C_{A}>0$ such that $\\mathrm{Var}(Z_{r})\\geq C_{A}$ for all $r\\in[0,1]$ . This is what we wanted to prove. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "C Computational lower bound ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we present the full omitted proof of Theorem 4. ", "page_idx": 20}, {"type": "text", "text": "Proof of theorem 4. We\u2019ll show a polynomial time reduction from OCDP to the hamiltonian cycle problem. Let $\\textit{n}=\\lvert V\\rvert$ and $m\\ =\\ |E|$ . Given an instance $\\pi_{H a m}~=~G(V,E)$ , where $V\\,{\\bar{=}}\\,\\left\\{v_{1},v_{2},\\dots,v_{n}\\right\\}$ and $\\dot{\\boldsymbol{E}}=\\{e_{1},\\dots,e_{m}\\}$ of the Hamiltonian cycle problem, we create an instance of the OCDP problem as follows: the optimizer has $m$ actions, denoted by $\\mathcal{A}=\\{a_{1},\\ldots,a_{m}\\}$ and the learner has $2n$ actions, denoted by $\\begin{array}{r}{\\mathcal{B}=\\big\\{b_{1},\\dots,b_{|V|},b_{\\mathrm{in}_{1}},\\dots,b_{\\mathrm{in}_{|V|}}\\big\\}.}\\end{array}$ . The matrices $A$ and $B$ , which correspond to the optimizer and learner\u2019s utility matrices respectively are defined as follows: ", "page_idx": 20}, {"type": "text", "text": "A[ai, bj] = 1, if there exists $u\\in V$ such that $e_{i}=(v_{j},u)$ , i.e. $e_{i}$ is outgoing edge of node $v_{j}$ 0, otherwise ", "page_idx": 20}, {"type": "equation", "text": "$$\nA[a_{i},b_{\\mathrm{in}_{j}}]=0\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "$B[a_{i},b_{j}]=\\left\\{\\begin{array}{l l}{-0.1}&{\\mathrm{if~}j=1\\mathrm{~and~}\\exists u\\in V\\mathrm{~s.t.~}e_{i}=(v_{1},u),\\mathrm{i.e.~}e_{i}\\mathrm{~is~an~outgoing~edge}}\\\\ {-4,}&{\\mathrm{if~}j\\neq1\\mathrm{~and~}\\exists u\\in V\\mathrm{~s.t.~}e_{i}=(v_{j},u),\\mathrm{i.e.~}e_{i}\\mathrm{~is~an~outgoing~edge}}\\\\ {1,}&{\\mathrm{if~}\\exists u\\in V\\mathrm{~such~that~}e_{i}=(u,v_{j}),\\mathrm{i.e.~}e_{i}\\mathrm{~is~an~incoming~edge~of~n~}}\\\\ {0,}&{\\mathrm{o.w.~}}\\end{array}\\right.$ of node $v_{1}$ oodf en e $v_{j}$ $v_{j}$   \n$B[a_{i},b_{\\mathrm{in}_{j}}]=\\binom{0.85}{0,}$ , if \u2203u \u2208V such that $e_{i}=(v_{j},u)$ , i.e. $e_{i}$ is an outgoing edge of node $v_{j}$ o.w. ", "page_idx": 20}, {"type": "text", "text": "Finally, we set $k\\,=\\,T\\,=\\,n+1$ , constructing the instance $\\pi_{O C D P}\\,=\\,(A,B,m,2n,k,T)$ . We conclude the reduction by proving that $\\pi_{H a m}$ is a \u2019Yes\u2019 instance if and only if $\\pi_{O C D P}$ is a \u2019Yes\u2019 instance. ", "page_idx": 20}, {"type": "text", "text": "1. $\\pi_{\\mathbf{Ham}}\\implies\\pi_{\\mathbf{OCDP}}:\\mathbf{S}$ uppose we have a Hamiltonian cycle that visits vertices $v_{1}=v_{u_{1}}\\to$ $v_{u_{2}}\\,\\rightarrow\\,\\cdot\\,\\cdot\\,\\rightarrow\\,v_{u_{n}}\\,\\rightarrow\\,v_{u_{n+1}}\\,=\\,v_{u_{1}}\\,=\\,v_{1}$ . Suppose that edge $e_{p_{i}}$ connects $v_{u_{i}}$ to $v_{u_{i+1}}$ . We will prove that the sequence of actions $a_{p_{1}},a_{p_{2}},\\ldots,a_{p_{n-1}},a_{p_{n}},a_{p_{n+1}}=a_{p_{1}}$ achieve reward exactly $n{+}1$ for the optimizer. To prove that, it is enough to argue that if the optimizer plays as we described above, the learner will respond with $b_{u_{1}},b_{u_{2}},\\ldots,b_{u_{n}},b_{u_{n+1}}=b_{u_{1}}$ . Indeed, notice that $A[a_{p_{i}},b_{u_{i}}]\\,=\\,1,\\forall i\\,\\in\\,[n+\\overline{{1}}]$ , since $e_{p_{i}}$ is an outgoing edge of $v_{u_{i}}$ , therefore the optimizer will be getting reward 1 every round. Now, to prove that the learner best responds as described, let us look at the historical rewards of the learner. Suppose the rewards for each of the $m=2\\cdot|V|$ actions of the learner for rounds $1,2,\\ldots,t\\!-\\!1$ is denoted by $h(t)=(h_{1}(t),\\hdots,h_{n}(t),h_{\\mathrm{in}_{1}}(t),\\hdots,h_{\\mathrm{in}_{n}}(t))\\in\\mathbb{R}^{2n}$ . We will prove inductively that after $t$ rounds we will have $h(t)=H(t)$ , where $H(t)$ is defined as: ", "page_idx": 20}, {"type": "equation", "text": "$$\nH_{1}(t)={\\left\\{\\begin{array}{l l}{0,}&{{\\mathrm{if~}}t=1}\\\\ {-\\ 0.1,}&{{\\mathrm{if~}}2\\leq t\\leq n}\\\\ {0.9,}&{{\\mathrm{if~}}t=n+1}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\nH_{u_{i}}(t)={\\left\\{\\begin{array}{l l}{0,}&{{\\mathrm{if~}}t<i}\\\\ {1,}&{{\\mathrm{if~}}t=i{\\mathrm{~,~}}i>1}\\\\ {-\\,3,}&{{\\mathrm{if~}}t>i}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\nH_{\\mathrm{in}_{u_{i}}}(t)={\\binom{0,}{0.85,}}\\quad{\\mathrm{if~}}t\\leq i\\quad,i=1,2,...\\,,n\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Notice that the cases $t=1,2$ are trivial; at $t=1$ everything is equal to 0 since the learner has accumulated no reward yet. At time $t=2$ we only update $h_{\\mathrm{in_{1}}}\\rightarrow0.85,h_{1}\\rightarrow-0.1,h_{u_{2}}\\rightarrow$ 1, thus giving us $h(2)\\ =\\ H(2)$ . Suppose that after some $t$ rounds, where $2\\,\\leq\\,t\\,\\leq\\,n$ rounds we have $h(t)=H(t)$ . After $a_{p_{t}}$ is played by the optimizer, the learner has to update $h_{\\mathrm{in}_{u_{t}}}\\rightarrow0.85,h_{u_{t}}\\rightarrow1-4=-3$ and if $t<n$ we update $h_{u_{t+1}}\\to1$ otherwise if $t=n$ we update $h_{1}\\rightarrow-0.1+1=0.9$ . Either way, we get $h(t+1)=H(t+1)$ . To complete the proof, note that for each $t$ , given that the of the learner history is $H(t)$ , best response for the learner is $b_{u_{t}}$ , thus the sequence of actions $a_{p_{1}},a_{p_{2}},\\ldots,a_{p_{n}},a_{p_{n+1}}$ achieves reward $n+1$ for the optimizer. ", "page_idx": 20}, {"type": "text", "text": "2. $\\pi_{\\bf O C D P}\\implies\\pi_{\\bf H a m}:\\mathrm{Supp}$ pose there is a sequence of actions played by the optimizer that receive reward $n+1$ in $n+1$ rounds, with actions $a_{p_{1}},a_{p_{2}},\\dotsc,a_{p_{n+1}}$ . We will prove that $a_{p_{1}},\\dotsc.\\dotsc a_{p_{n}}$ correspond to the edges of a Hamiltonian Cycle starting from node $v_{1}\\,=\\,1$ , i.e. edges $e_{p_{1}},...,e_{p_{n}}$ make a Hamiltonian Cycle, by connecting nodes $v_{1}=$ $v_{u_{1}},v_{u_{2}},\\ldots,v_{u_{n}}$ . We again denote the rewards of the learner for rounds $1,2,\\ldots,t-1$ with $h(\\bar{t})=\\bar{(}h_{1}(\\bar{t}),\\cdots,h_{n}(\\bar{t}),h_{\\mathrm{in}_{1}}(t),\\cdots,h_{\\mathrm{in}_{n}}(t))\\in\\mathbb{R}^{2n},$ . ", "page_idx": 21}, {"type": "text", "text": "\u2022 Action at $t=1$ : Start by observing that the learner will play $b_{1}$ at $t=1$ as it is the tie breaking rule, so the optimizer needs to play an action that achieves reward 1 against that. Note that the way we constructed the utility matrix of the optimizer, the only actions that achieve reward against $v_{i}$ correspond to outgoing edges from vertex $v_{i}$ . So at $t=1$ , the optimizer is forced to play an action that corresponds to an outgoing edge of $v_{1}=1$ . Suppose the first action of the optimizer is $a_{p_{1}}$ , corresponding to the edge $e_{p_{1}}=(v_{1},v_{u_{2}})$ . ", "page_idx": 21}, {"type": "text", "text": "\u2022 Action at $t=2:$ : The rewards of the optimizer will be updated as follows, after the optimizer plays $a_{p_{1}}$ and the learner plays $b_{1}$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\nH_{i}(t)={\\binom{-\\;0.1,\\quad{\\mathrm{if~}}i=1}{1,\\quad}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\nH_{\\mathrm{in}_{i}}(t)={\\binom{0.85,}{0,}}_{0.\\mathrm{W}.}^{\\mathrm{~\\,~if~}i=1}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We note that now new best response of the learner will be $b_{u_{2}}$ . Thus, the only way for the optimizer to obtain reward 1 for the second round is to play an action corresponding to an outgoing edge of node $v_{u_{2}}$ . Suppose the action the optimizer plays is $b_{p_{2}}$ , corresponding to an edge $e_{p_{2}}=(v_{u_{2}},v_{u_{3}})$ . ", "page_idx": 21}, {"type": "text", "text": "\u2022 Actions at $\\mathrm{~\\textit~{~2~}~}<\\mathrm{~\\textit~{~t~}~}<\\mathrm{~\\textit~{~n~}~}$ : We will prove inductively that the first $t~\\le~n\\,-\\,1$ actions $a_{p_{1}},\\ldots,a_{p_{t}}$ correspond to edges of the form $e_{p_{1}}\\;\\;=\\;\\;(v_{u_{1}},v_{u_{2}}),e_{p_{2}}\\;\\;=$ $(v_{u_{2}},v_{u_{3}}),\\ldots,e_{p_{t}}\\sp{\\,\\bullet}=\\,(v_{u_{t}},v_{u_{t+1}})$ where $u_{i}\\neq u_{j}$ for $1\\,\\leq\\,i\\,<\\,j\\,\\leq\\,n$ , i.e. edges that define a simple path in the graph, starting from node $v_{1}$ . By playing actions in such a manner, the historical rewards of the learner will be of the form: ", "page_idx": 21}, {"type": "equation", "text": "$$\nH_{1}(t)={\\binom{0}{-\\,0.1,}}\\quad{\\mathrm{if~}}t=1\\qquad\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\nH_{u_{i}}(t)={\\left\\{\\begin{array}{l l}{0,}&{{\\mathrm{if~}}t<i}\\\\ {1,}&{{\\mathrm{if~}}t=i{\\mathrm{~,~}}i>1}\\\\ {-\\,3,}&{{\\mathrm{if~}}t>i}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\nH_{\\mathrm{in}_{u_{i}}}(t)={\\binom{0,}{0.85,}}\\quad{\\mathrm{if~}}t\\leq i\\,,i=1,2,\\dots,n\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We proved that the first $t$ actions correspond to edges of a simple path for $t\\le2$ . We assume for the inductive step that it is true for the first $t<n-1$ actions $a_{p_{1}},\\ldots,a_{p_{t}}$ of the optimizer. We will prove that the next action $a_{p_{t+1}}$ by the optimizer corresponds to an edge $e_{p_{t+1}}$ that extends this simple path. Since the first $t$ actions are corresponding to a simple path, at round $t+1$ the learner\u2019s historical rewards are going to be equal to $H(t+\\bar{1})$ . Note that the best response for the learner is going to be $b_{u_{t+1}}$ . In order for the optimizer to gain reward 1 the $t\\!+\\!1$ \u2019th round, they have to be playing an action $a_{p_{t+1}}$ for which $e_{p_{t+1}}$ is an outgoing edge of the node $\\boldsymbol{v}_{u_{t+1}}$ . Suppose the optimizer chooses to play $a_{p_{t+1}}$ , where $e_{p_{t+1}}=(v_{u_{t+1}},v_{u_{t+2}})$ , and $u_{t+2}=u_{l}$ for some $1\\leq l\\leq t+1$ At round $t+2$ the optimizer will still have to play an action that corresponds to an outgoing edge of $v_{u_{l}}$ . However, after that, we will have $h_{\\mathrm{in}_{u_{l}}}(t+3)=1.70$ , which will make $b_{u_{l}}$ the best response. However, there is no action the optimizer can play against $b_{u_{l}}$ in which they receive reward 1, making it impossible for the optimizer to obtain $n+1$ reward in $n+1$ rounds. Thus the optimizer needs to play $e_{p_{t+1}}=(v_{u_{t+1}},v_{u_{t+2}})$ , and $u_{t+2}\\neq u_{l},l\\in[t+1]$ , completing the inductive step. Since we also proved that the first 2 actions are forming a simple path starting from $v_{1}$ , using the inductive step, we conclude that the first $n-1$ actions have to correspond to a simple path in the graph starting from $v_{1}$ . ", "page_idx": 21}, {"type": "text", "text": "\u2022 Actions at t = n, n + 1: Suppose the last two actions of the optimizer are apn, apn+1. We want to prove that $e_{p_{n}}\\ ^{-}=\\ (v_{u_{n}},v_{1}),e_{p_{n+1}}\\;=\\;(v_{1},u)$ , where $u$ is some node connected to $v_{1}$ . We know that the history of the learner at time $t=n$ , as proven in the previous bullet point, is as follows: ", "page_idx": 22}, {"type": "equation", "text": "$$\nh_{u_{i}}(n)={\\left\\{\\begin{array}{l l}{-\\ 0.1,}&{{\\mathrm{if~}}i=1}\\\\ {1,}&{{\\mathrm{if~}}i=n\\ ,i>1}\\\\ {-\\ 3,}&{{\\mathrm{o.w.}}}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\nh_{\\mathrm{in}_{u_{i}}}(n)=\\left\\{0,\\begin{array}{l l}{\\mathrm{if}\\;i=n}\\\\ {0.85,}&{\\mathrm{o.w.}}\\end{array}\\right.,i=1,2,...\\,,n\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Note that the best response for the learner at time $t\\,=\\,n$ will be $b_{u_{n}}$ . Thus the optimizer will have to play an action $a_{p_{n}}$ corresponding to an outgoing edge of $v_{u_{n}}$ . Suppose $e_{p_{n}}\\:=\\:(v_{u_{n}},\\bar{x})$ . Assume $x\\ne1$ . Then the history is updated to $h_{u_{n}}\\;\\to$ $-3,h_{x}\\to-2,h_{\\mathrm{in}_{u_{n}}}\\to0.85$ . Note that the best response is any action $b_{u_{\\mathrm{in}_{i}}}$ for which the optimizer cannot get reward 1 from. Thus if $x\\ne1$ it is impossible to get reward 1 every round. On the other hand, if $x=1$ , then the best response at time $n+1$ will be $b_{1}$ since the $h_{1}(n+1)\\,=\\,-0.1+1\\,=\\,0.9\\,>\\,0.85$ , and thus playing any action corresponding to an outgoing edge of $v_{1}$ will obtain reward 1 for the learner at that round. ", "page_idx": 22}, {"type": "text", "text": "Lastly, notice that the problem statement requests the entries of the weight matrix $B$ to be in $[0,1]$ . While our construction have weights in $[-4,4]$ , scaling and shifting all the entries by the same amount does not affect the identity of the best response. \u53e3 ", "page_idx": 22}, {"type": "image", "img_path": "0uGlKYS7a2/tmp/345fe0c506ea013e78abe888473f9c5676e42f7c024ca24da6e8c4a7bc49170d.jpg", "img_caption": ["Figure 1: Graph $G$ "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "We will show an example of how we can make the reduction from Hamiltonian Cycle to OCDP. Imagine we have an instance of Hamiltonian Cycle that corresponds to the graph shown in 1, and we want to reduce to OCDP. Since we have $|V|=5$ vertices and $|E|=7$ edges, we instantiate the following instance of OCDP: $(A,B,n=|E|,m=2|V|,k=|V|+1,T=|\\mathbb{1}$ V | + 1), where the actions sets are ${\\cal A}\\,=\\,\\{e_{1}\\,=\\,(1,5),e_{2}\\,=\\,(5,2),e_{3}\\,=\\,(1,2),e_{4}$ $),e_{3}\\,=\\,(1,2),e_{4}\\,=\\,(2,4),e_{5}\\,=\\,(4,1),e_{6}\\,=$ $\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\$ , and the utility matrices $A$ and $B$ as shown in the tables below. ", "page_idx": 22}, {"type": "table", "img_path": "0uGlKYS7a2/tmp/681b853aa93e2093e49012f32f1863511eef9ef2f830b47677147b132db9b534.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "0uGlKYS7a2/tmp/6d9eec4872511436152bb541be22122e6a73af9fc1899e401f756c878582cfde.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Notice that the only way for the optimizer to achieve reward $|V|+6=5+1=6$ is if he plays the following actions: ", "page_idx": 23}, {"type": "equation", "text": "$$\ne_{1},e_{2},e_{4},e_{6},e_{7},e_{1}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and the learner\u2019s actions are the following: ", "page_idx": 23}, {"type": "equation", "text": "$$\nv_{1},v_{5},v_{2},v_{4},v_{3},v_{1}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The following table shows the rewards history of the learner during this game: ", "page_idx": 23}, {"type": "table", "img_path": "0uGlKYS7a2/tmp/2a3c4da7ee64521f0352ab20192c4aad0ddd8807988404755325f25f9ff8bbb3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? NEUR ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: The introduction and abstract accurately reflect the main scope of the paper.   \nWe have proofs for the propositions and theorems of the paper in the appendix. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: The assumptions made in each claim are clearly described in the statement of the claim. We discuss possible future directions in section 4. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The assumptions are stated clearly in each theorem, lemma or proposition.   \nFull proofs of what was omitted in the main section, are included in the appendix. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: This paper does not include any experiments. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: This paper does not include any experiments requiring code. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: This paper does not include any experiments. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: This paper does not include any experiments. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: This paper does not include any experiments. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: The paper is compliant with the code of NeurIPS Code of Ethics. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Discussed in the end of page 9. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: This paper does not contain data models and therefore does not pose such risks. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: This paper does not use any existing assets. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 28}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This paper does not use any existing assets. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This paper does not include crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: This paper does not include crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]