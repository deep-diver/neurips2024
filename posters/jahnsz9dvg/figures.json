[{"figure_path": "JAhNsZ9dvG/figures/figures_1_1.jpg", "caption": "Figure 1: Acceptance counts vs draft size (left), forward pass GPU time vs input size (right). Llama 2-7B draft model, offloaded Llama 2-70B target model, MTBench dataset, t=0.6 and top-p=0.9.", "description": "This figure shows two graphs. The left graph displays the relationship between the number of tokens accepted by the target model (Llama 2-70B) and the size of the output from the draft model (Llama 2-7B) during speculative decoding.  The right graph illustrates the GPU forward pass time for processing different input sizes on an A100 GPU, comparing Llama 2-7B and an offloaded Llama 2-70B.  The experiments used the MTBench dataset with temperature (t) of 0.6 and top-p of 0.9.", "section": "3 Preliminary analysis"}, {"figure_path": "JAhNsZ9dvG/figures/figures_3_1.jpg", "caption": "Figure 2: Llama-2 70B Chat model cumulative probability of most likely tokens compared to the draft model choice (all Llama draft models are chat versions), OASST1 dataset.", "description": "This figure compares the cumulative probability of the most likely tokens generated by the Llama-2 70B Chat model to the cumulative probability of tokens selected by various draft models (Llama-2 7B, Llama-2 13B, TinyLlama 1B, and JackFram 160M).  The x-axis represents the number of tokens considered (Top-k), while the y-axis shows the cumulative probability. The results are based on the OASST1 dataset.  The figure demonstrates how well different draft models can predict the high-probability tokens of the target Llama-2 70B Chat model.", "section": "5.1 Probability Coverage"}, {"figure_path": "JAhNsZ9dvG/figures/figures_7_1.jpg", "caption": "Figure 3: Generation rate depending on the draft budget size for Llama 2-7B Chat as the draft model and Llama 2-70B Chat as the target model, MTBench Zheng et al. [2023] dataset. Results are obtained with an A100 GPU.", "description": "This figure compares the performance of SpecExec and SpecInfer in terms of the number of generated tokens per step, varying the size of the draft model's output.  The x-axis represents the draft model output size (in tokens), and the y-axis shows the number of generated tokens per step. Two subfigures are presented, one for a setting with temperature 0.6 and top_p 0.9, and another with temperature 0. The results demonstrate that SpecExec consistently outperforms SpecInfer, particularly as the draft budget increases, showcasing its efficiency in speculative decoding with larger draft sizes.", "section": "5.2 Draft Acceptance Rates"}, {"figure_path": "JAhNsZ9dvG/figures/figures_7_2.jpg", "caption": "Figure 3: Generation rate depending on the draft budget size for Llama 2-7B Chat as the draft model and Llama 2-70B Chat as the target model, MTBench Zheng et al. [2023] dataset. Results are obtained with an A100 GPU.", "description": "This figure shows the relationship between the number of tokens generated per step and the size of the draft model's output.  It compares the performance of SpecExec and SpecInfer, two speculative decoding algorithms, using Llama 2-7B Chat as the draft model and Llama 2-70B Chat as the target model. The experiment uses the MTBench dataset, and the results are obtained using an A100 GPU.  The graph illustrates that SpecExec consistently outperforms SpecInfer, particularly when the draft budget size is large.", "section": "5.2 Draft Acceptance Rates"}, {"figure_path": "JAhNsZ9dvG/figures/figures_9_1.jpg", "caption": "Figure 3: Generation rate depending on the draft budget size for Llama 2-7B Chat as the draft model and Llama 2-70B Chat as the target model, MTBench Zheng et al. [2023] dataset. Results are obtained with an A100 GPU.", "description": "This figure shows the performance comparison of SpecExec and SpecInfer in terms of generated tokens per step and inference speed (tokens per second) with varying draft model output sizes.  The experiment uses Llama 2-7B Chat as the draft model and Llama 2-70B Chat as the target model on the MTBench dataset, performed on an A100 GPU.  The results demonstrate the effect of the draft budget size on the efficiency of speculative decoding algorithms.", "section": "5.2 Draft Acceptance Rates"}, {"figure_path": "JAhNsZ9dvG/figures/figures_16_1.jpg", "caption": "Figure 6: A high-level overview of the SpecExec algorithm.", "description": "This flowchart shows the steps of the Speculative Execution algorithm.  It starts by generating text and building a tree using a draft model and pre-filling a cache of probabilities from the target model. It then attempts to sample the next token from the cache. If successful, the token is added to the output sequence; otherwise, the algorithm recalculates the cache. The process continues until a stop condition is met, at which point the output sequence is returned. This approach aims to improve the speed and efficiency of large language model inference by predicting and caching likely next tokens. ", "section": "4 Method"}, {"figure_path": "JAhNsZ9dvG/figures/figures_18_1.jpg", "caption": "Figure 7: Number of accepted tokens as a function of the draft size B for the Llama 2-70B Chat target model and different draft models.", "description": "This figure shows the relationship between the number of accepted tokens and the size of the draft model output for four different draft models: Llama 2 13b chat, Llama 2 7b chat, TinyLlama 1.1B Chat, and JackFram/llama-160m.  The x-axis represents the draft model output size in tokens, and the y-axis shows the number of accepted tokens.  The figure demonstrates that larger draft models generally lead to a higher number of accepted tokens.  The results highlight the effectiveness of larger, more capable draft models in SpecExec for improving the acceptance rate during speculative decoding.", "section": "5.2 Draft Acceptance Rates"}, {"figure_path": "JAhNsZ9dvG/figures/figures_18_2.jpg", "caption": "Figure 3: Generation rate depending on the draft budget size for Llama 2-7B Chat as the draft model and Llama 2-70B Chat as the target model, MTBench Zheng et al. [2023] dataset. Results are obtained with an A100 GPU.", "description": "This figure shows the relationship between the draft budget size and the generation rate (number of tokens generated per step) for two different speculative decoding methods, SpecExec and SpecInfer.  The experiment uses Llama 2-7B Chat as the draft model and Llama 2-70B Chat as the target model with the MTBench dataset, using an A100 GPU. It demonstrates how the number of generated tokens per step changes as the size of the draft model output increases for each method. The results show how SpecExec outperforms SpecInfer, especially with larger draft sizes. This comparison highlights the performance advantage of SpecExec in handling large draft budgets.", "section": "5.2 Draft Acceptance Rates"}, {"figure_path": "JAhNsZ9dvG/figures/figures_19_1.jpg", "caption": "Figure 9: Acceptance rate in generation with token penalty: \u201cdon\u2019t start words with \u201cR\u201d\u201d\u201d (left) and \u201cdon\u2019t use the letter \u201cR\u201d (right); Llama 2 7B Chat (+ TinyLlama-1.1B Chat draft) model (t=0.6, p=0.9), MT-Bench dataset. The rest of the experimental configuration is the same as in Figure 1.", "description": "This figure shows the impact of token penalties on the performance of SpecExec and SpecInfer.  Two different penalty schemes were tested: one where tokens starting with the letter \"r\" were penalized, and another where all tokens containing the letter \"r\" were penalized. The results demonstrate that SpecExec is more robust to token penalties than SpecInfer, maintaining a higher acceptance rate even with heavier penalties.", "section": "H Drafting penalty effects"}]