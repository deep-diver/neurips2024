{"references": [{"fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "publication_date": "2023-02-23", "reason": "This paper introduces Llama, a foundational large language model used extensively in the experiments for comparison and analysis."}, {"fullname_first_author": "Yaniv Leviathan", "paper_title": "Fast inference from transformers via speculative decoding", "publication_date": "2023-00-00", "reason": "This paper introduces speculative decoding, a core concept that the current paper builds upon and improves, significantly impacting the efficiency of LLM inference."}, {"fullname_first_author": "Zhuoming Chen", "paper_title": "Accelerating large language model decoding with speculative sampling", "publication_date": "2023-02-01", "reason": "This work provides a foundation for the speculative decoding algorithms analyzed in the current paper, significantly influencing the approach and performance comparison."}, {"fullname_first_author": "Xupeng Miao", "paper_title": "Specinfer: Accelerating generative LLM serving with speculative inference and token tree verification", "publication_date": "2023-05-09", "reason": "This paper introduces SpecInfer, a key speculative decoding algorithm used for comparison and benchmarking in the paper, highlighting its performance characteristics."}, {"fullname_first_author": "Keivan Alizadeh", "paper_title": "LLM in a flash: Efficient large language model inference with limited memory", "publication_date": "2023-12-23", "reason": "This work directly addresses the problem of parameter offloading, which is a critical component of the current paper's approach to efficient inference on consumer devices."}]}