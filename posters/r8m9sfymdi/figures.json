[{"figure_path": "r8M9SfYMDi/figures/figures_1_1.jpg", "caption": "Figure 1: Scaling the training of the OpenLM-410M model to 1.1 trillion tokens on the RefinedWeb dataset. Note that each point on the figure represents a separate training from scratch (not different checkpoints of a single run). For the largest run, some tokens are seen more than once (the dataset has approximately 525 billion tokens). For dataset decomposition, we use the Grow-P2 curriculum with 8 cycles. Maximum context length is 8192 and all hyper-parameters are the same for DD and the baseline. (a) Regular metrics average when training with the baseline method and the proposed method. We observe more than 4\u00d7 data efficiency compared to the baseline. Also, even at 1.1 trillion tokens, DD has a +2.4 accuracy improvement compared to the baseline (which has a plateauing accuracy curve even on a logarithmic x-axis). (b) Comparison of model average accuracy versus training cost (GPU-hours). DD reaches the best accuracy of the baseline more than 6\u00d7 faster. This is the combined effect of DD accuracy and speed gains.", "description": "This figure shows the results of scaling the training of the OpenLM-410M language model up to 1.1 trillion tokens using the RefinedWeb dataset.  It compares the performance of the baseline concat-and-chunk approach against the proposed dataset decomposition method.  Two subfigures are presented: (a) demonstrates the data efficiency of each approach by plotting regular evaluation metrics against the number of seen tokens. Dataset decomposition shows significantly better data efficiency, achieving a 4x improvement, and maintaining a +2.4 accuracy boost over the baseline even with far more training data. (b) shows the computational efficiency by comparing model accuracy against training cost in GPU hours. Dataset Decomposition reaches the same accuracy as the baseline model over 6 times faster.", "section": "Experiments and analysis"}, {"figure_path": "r8M9SfYMDi/figures/figures_1_2.jpg", "caption": "Figure 1: Scaling the training of the OpenLM-410M model to 1.1 trillion tokens on the RefinedWeb dataset. Note that each point on the figure represents a separate training from scratch (not different checkpoints of a single run). For the largest run, some tokens are seen more than once (the dataset has approximately 525 billion tokens). For dataset decomposition, we use the Grow-P2 curriculum with 8 cycles. Maximum context length is 8192 and all hyper-parameters are the same for DD and the baseline. (a) Regular metrics average when training with the baseline method and the proposed method. We observe more than 4\u00d7 data efficiency compared to the baseline. Also, even at 1.1 trillion tokens, DD has a +2.4 accuracy improvement compared to the baseline (which has a plateauing accuracy curve even on a logarithmic x-axis). (b) Comparison of model average accuracy versus training cost (GPU-hours). DD reaches the best accuracy of the baseline more than 6\u00d7 faster. This is the combined effect of DD accuracy and speed gains.", "description": "This figure shows the results of scaling the training of the OpenLM-410M language model on the RefinedWeb dataset up to 1.1 trillion tokens using two different methods: the baseline concat-and-chunk method and the proposed dataset decomposition (DD) method.  Subfigure (a) demonstrates the superior data efficiency of DD, achieving a more than 4x improvement and a +2.4 accuracy increase compared to the baseline at 1.1 trillion tokens. Subfigure (b) showcases the significant computational efficiency gains offered by DD, reaching the same accuracy as the baseline more than 6 times faster, highlighting the combined benefits of data and computational speedup.", "section": "Experiments and analysis"}, {"figure_path": "r8M9SfYMDi/figures/figures_2_1.jpg", "caption": "Figure 2: Each cell in the figure represents a token. Left: Original documents with variable lengths. Middle: Concat-and-chunk baseline to form sequences with a fixed target length (here = 4). Right: Dataset decomposition method with D1, D2, and D3 buckets.", "description": "This figure illustrates the difference between the concat-and-chunk method and the proposed dataset decomposition method. The concat-and-chunk method concatenates documents of varying lengths and then chunks them into fixed-length sequences. This can lead to sequences that span multiple documents, resulting in cross-document attention. The dataset decomposition method, on the other hand, decomposes the dataset into buckets of sequences with the same length, each extracted from a single document. This eliminates cross-document attention and allows for more efficient training.", "section": "2 Method"}, {"figure_path": "r8M9SfYMDi/figures/figures_3_1.jpg", "caption": "Figure 3: For the RefinedWeb dataset [46]: (a) Distribution of chunk lengths using different dataset preparation methods. Peaks show the percentage of chunks for each method with the same length as the target sequence length. (b) Distribution of tokens over Di's in DD. Color/pattern shows the [log2 l], where l is the length of the original document each token is extracted from. (c) Probability distribution of context length (number of tokens from the same document a token can attend to) observed during training for the concat-and-chunk baseline with target sequence length 8192 and DD with > 256 mixture defined in Table 1.", "description": "This figure compares the distribution of chunk lengths for different dataset preparation methods (concat-and-chunk with different target sequence lengths, best-fit packing, and dataset decomposition). It shows that dataset decomposition preserves the original document length better, leading to a more natural distribution of context lengths during training.  Subplot (a) shows the chunk length distributions. Subplot (b) illustrates the distribution of tokens across different buckets in dataset decomposition, highlighting the alignment with the original document lengths. Subplot (c) contrasts the context length distribution (how far the model attends within a single document during training) across the methods.", "section": "2 Method"}, {"figure_path": "r8M9SfYMDi/figures/figures_4_1.jpg", "caption": "Figure 1: Scaling the training of the OpenLM-410M model to 1.1 trillion tokens on the RefinedWeb dataset. Note that each point on the figure represents a separate training from scratch (not different checkpoints of a single run). For the largest run, some tokens are seen more than once (the dataset has approximately 525 billion tokens). For dataset decomposition, we use the Grow-P2 curriculum with 8 cycles. Maximum context length is 8192 and all hyper-parameters are the same for DD and the baseline. (a) Regular metrics average when training with the baseline method and the proposed method. We observe more than 4\u00d7 data efficiency compared to the baseline. Also, even at 1.1 trillion tokens, DD has a +2.4 accuracy improvement compared to the baseline (which has a plateauing accuracy curve even on a logarithmic x-axis). (b) Comparison of model average accuracy versus training cost (GPU-hours). DD reaches the best accuracy of the baseline more than 6\u00d7 faster. This is the combined effect of DD accuracy and speed gains.", "description": "This figure shows the results of scaling the training of the OpenLM-410M language model up to 1.1 trillion tokens using the RefinedWeb dataset.  It compares the performance of the proposed Dataset Decomposition (DD) method against a baseline concat-and-chunk approach.  Subfigure (a) demonstrates the significant improvement in data efficiency (more than 4x) and accuracy (+2.4) achieved by DD compared to the baseline. Subfigure (b) highlights the substantial speedup in training time (over 6x faster) using DD, while reaching the same accuracy as the baseline. The improvements are attributed to the combined effects of increased accuracy and faster training speed provided by DD.", "section": "Experiments and analysis"}, {"figure_path": "r8M9SfYMDi/figures/figures_5_1.jpg", "caption": "Figure 5: (a) Performance of OpenLM-1B model trained on 234 tokens from buckets with different sequence lengths. (b) distribution of lengths of documents for different benchmarks. (c) Effect of chunking (D13\u219210) and concatenating (D7\u219213) sequences during pretraining on model performance.", "description": "This figure analyzes the impact of pretraining sequence length on model performance.  Panel (a) shows how accuracy on various benchmarks (Commonsense Reasoning, Language Understanding, Reading Comprehension, World Knowledge, and Multi-Document Question Answering) changes as the pretraining sequence length increases.  Panel (b) illustrates the distribution of document lengths for those same benchmarks.  Finally, panel (c) demonstrates the effects of modifying pretraining data by either chunking long sequences into smaller ones (D13\u219210) or concatenating short ones into longer ones (D7\u219210) on model performance, highlighting that maintaining the original document length distribution is crucial for optimal results.", "section": "3.2 Sequence length bias"}, {"figure_path": "r8M9SfYMDi/figures/figures_16_1.jpg", "caption": "Figure 6: Comparison of length-based curriculum schedule with learning rate schedule. Sequence length varies between 256 and 8192 based on the Grow-P2 curriculum with 8 cycles. Note that the choice of bucket (and hence the sequence length) is random, with sampling probabilities determined by the curriculum. In the figure, we show the length of the sampled sequence at every 9 optimization steps. For the learning rate, we use a cosine learning rate with a warm-up for 4k steps. The job corresponds to training for a total of 236 tokens, with 220 tokens seen per optimization step.", "description": "This figure visualizes the curriculum used in the dataset decomposition method.  The x-axis represents the optimization steps during training. The blue bars show the length of the sequence sampled at each step (determined randomly, but biased by the curriculum), ranging from 256 to 8192 tokens.  The orange line represents the learning rate schedule which follows a cosine curve with a warm-up period.  The Grow-P2 curriculum with 8 cycles ensures a gradual increase in the proportion of longer sequences over the course of training.", "section": "B.2 Length based sampling and curriculum algorithm"}, {"figure_path": "r8M9SfYMDi/figures/figures_17_1.jpg", "caption": "Figure 1: Scaling the training of the OpenLM-410M model to 1.1 trillion tokens on the RefinedWeb dataset. Note that each point on the figure represents a separate training from scratch (not different checkpoints of a single run). For the largest run, some tokens are seen more than once (the dataset has approximately 525 billion tokens). For dataset decomposition, we use the Grow-P2 curriculum with 8 cycles. Maximum context length is 8192 and all hyper-parameters are the same for DD and the baseline. (a) Regular metrics average when training with the baseline method and the proposed method. We observe more than 4\u00d7 data efficiency compared to the baseline. Also, even at 1.1 trillion tokens, DD has a +2.4 accuracy improvement compared to the baseline (which has a plateauing accuracy curve even on a logarithmic x-axis). (b) Comparison of model average accuracy versus training cost (GPU-hours). DD reaches the best accuracy of the baseline more than 6\u00d7 faster. This is the combined effect of DD accuracy and speed gains.", "description": "This figure shows the results of scaling the training of the OpenLM-410M language model using two different methods: the baseline concat-and-chunk approach and the proposed dataset decomposition (DD) method. The training was performed on the RefinedWeb dataset with a total of approximately 1.1 trillion tokens.  The figure highlights the superior data and computational efficiency of DD. Subfigure (a) demonstrates that DD achieves over 4x data efficiency and a 2.4 point accuracy improvement over the baseline. Subfigure (b) shows that DD achieves more than 6x faster training speed compared to the baseline.", "section": "Experiments and analysis"}, {"figure_path": "r8M9SfYMDi/figures/figures_19_1.jpg", "caption": "Figure 8: We compare the training loss when training with Baseline-8k versus DD with the \"Grow-P100\" curriculum. Both models are trained with identical hyperparameters, a high learning rate (= 10<sup>-2</sup>), and no gradient clipping. It is evident that DD results in greater stability.", "description": "This figure compares the training loss curves for two different training methods: the Baseline-8k method and the Dataset Decomposition (DD) method with the \"Grow-P100\" curriculum.  Both methods used identical hyperparameters, a high learning rate (10^-2), and no gradient clipping. The key observation is that the DD method exhibits greater stability during training, indicating a more robust training process.", "section": "3.4 Length-based curriculum"}]