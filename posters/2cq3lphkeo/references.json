{"references": [{"fullname_first_author": "Paul F Christiano", "paper_title": "Deep reinforcement learning from human preferences", "publication_date": "2017-12-01", "reason": "This paper is foundational to the field of reinforcement learning from human feedback (RLHF), introducing key concepts and techniques that are still widely used today."}, {"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-12-01", "reason": "This paper details the RLHF training methodology used to align large language models with human preferences, a critical step in the development of modern LLMs."}, {"fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2023-12-01", "reason": "This paper introduces Direct Preference Optimization (DPO), a more efficient alternative to traditional RLHF methods which avoids explicit reward modeling."}, {"fullname_first_author": "Ted Moskovitz", "paper_title": "Confronting reward model overoptimization with constrained RLHF", "publication_date": "2023-10-26", "reason": "This paper directly addresses the problem of reward overoptimization in RLHF, proposing a novel technique to mitigate this issue."}, {"fullname_first_author": "Jacob Eisenstein", "paper_title": "Helping or herding? reward model ensembles mitigate but do not eliminate reward hacking", "publication_date": "2023-12-01", "reason": "This paper investigates the limitations of reward model ensembles for mitigating reward hacking, demonstrating that these techniques can reduce but not eliminate overoptimization."}]}