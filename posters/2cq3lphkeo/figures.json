[{"figure_path": "2cQ3lPhkeO/figures/figures_1_1.jpg", "caption": "Figure 1: Left: Reward overoptimization due to the distributional shift and uncertainty in reward. Right: Overoptimization causes the probability of outputting preferred responses in the preference data to decrease substantially using original DPO proposed by [46]. Our algorithm (RPO) significantly alleviates this decrease. See more discussions in Section 6.", "description": "The figure illustrates the problem of reward overoptimization in reinforcement learning from human feedback (RLHF).  The left panel shows how distributional shift and uncertainty in the reward model can lead to the model prioritizing undesired responses (those with high uncertainty but spuriously high estimated reward). The right panel shows how the proposed algorithm (RPO) improves upon DPO (Direct Preference Optimization) by maintaining a higher probability of outputting the desired responses over the training epochs.  This demonstrates the effectiveness of RPO in mitigating reward overoptimization.", "section": "Introduction"}, {"figure_path": "2cQ3lPhkeO/figures/figures_5_1.jpg", "caption": "Figure 1: Left: Reward overoptimization due to the distributional shift and uncertainty in reward. Right: Overoptimization causes the probability of outputting preferred responses in the preference data to decrease substantially using original DPO proposed by [46]. Our algorithm (RPO) significantly alleviates this decrease. See more discussions in Section 6.", "description": "The left panel shows how overoptimization happens due to distributional shift and uncertainty in the reward model.  The right panel compares the performance of the original DPO method and the proposed RPO method in terms of the probability of generating preferred responses during training. The RPO method is shown to significantly mitigate the decrease in probability caused by overoptimization.", "section": "1 Introduction"}, {"figure_path": "2cQ3lPhkeO/figures/figures_8_1.jpg", "caption": "Figure 1: Left: Reward overoptimization due to the distributional shift and uncertainty in reward. Right: Overoptimization causes the probability of outputting preferred responses in the preference data to decrease substantially using original DPO proposed by [46]. Our algorithm (RPO) significantly alleviates this decrease. See more discussions in Section 6.", "description": "The left panel shows how reward overoptimization happens due to distributional shift and uncertainty in reward estimation. The right panel compares the performance of the original DPO algorithm and the proposed RPO algorithm in terms of the probability of generating preferred responses during training. RPO significantly improves upon DPO by mitigating overoptimization.", "section": "1 Introduction"}]