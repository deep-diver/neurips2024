[{"figure_path": "2cQ3lPhkeO/tables/tables_3_1.jpg", "caption": "Table 1: Pairwise win rate (left vs. right) among RPO-trained model, DPO-trained model, and the reference model. Annotated by GPT-4, evaluations of beta and gemma series are made on the 150 examples of the test split of the Ultrafeedback and the Argilla-DPO-Mix-7K dataset, respectively.", "description": "This table presents the pairwise win rates between the RPO model, DPO model, and the reference model in terms of human preference, evaluated by GPT-4. The evaluations are performed on 150 examples (test splits) from the Ultrafeedback dataset (for the beta series) and the Argilla-DPO-Mix-7K dataset (for the gemma series).  It demonstrates the improved performance of RPO over DPO in terms of human preference.", "section": "Experiments"}, {"figure_path": "2cQ3lPhkeO/tables/tables_8_1.jpg", "caption": "Table 1: Pairwise win rate (left vs. right) among RPO-trained model, DPO-trained model, and the reference model. Annotated by GPT-4, evaluations of beta and gemma series are made on the 150 examples of the test split of the Ultrafeedback and the Argilla-DPO-Mix-7K dataset, respectively.", "description": "This table presents the results of pairwise comparisons between the RPO, DPO, and reference models using human evaluation (GPT-4).  It shows the win rate for each model against the others on 150 examples from the test datasets of two model series (beta and gemma).  Higher win rates indicate better performance.", "section": "Experiments"}, {"figure_path": "2cQ3lPhkeO/tables/tables_9_1.jpg", "caption": "Table 2: Results on MT-Bench scores and AlpacaEval 2.0. zephyr-beta-7b and zephyr-gemma-7b are the officially released models. win rates and Length-Control (LC) win rates in AlpacaEval 2.0 are evaluated by GPT-4 compared with GPT-4.", "description": "This table presents the results of evaluating the RPO, DPO, and reference models on the MT-Bench and AlpacaEval 2.0 benchmarks.  It shows the MT-Bench scores, AlpacaEval 2.0 win rates, and AlpacaEval 2.0 length-control win rates.  The results demonstrate the improved performance of RPO compared to DPO and the reference model.", "section": "Experiments"}, {"figure_path": "2cQ3lPhkeO/tables/tables_25_1.jpg", "caption": "Table 3: Training configurations for beta series and gemma series models in this paper.", "description": "This table lists the hyperparameters used for training the language models in the beta and gemma series.  It shows the learning rate, learning scheduler type, warmup ratio, batch size, gradient accumulation, batch size per device, training epoch, beta, optimizer, seed, and precision used for each series.", "section": "F.1 Training Details"}, {"figure_path": "2cQ3lPhkeO/tables/tables_26_1.jpg", "caption": "Table 1: Pairwise win rate (left vs. right) among RPO-trained model, DPO-trained model, and the reference model. Annotated by GPT-4, evaluations of beta and gemma series are made on the 150 examples of the test split of the Ultrafeedback and the Argilla-DPO-Mix-7K dataset, respectively.", "description": "This table shows the pairwise win rates between different models (RPO, DPO, and reference models) in both beta and gemma series.  The win rates are determined by GPT-4 annotations on 150 test examples from the Ultrafeedback dataset (for the beta series) and the Argilla-DPO-Mix-7K dataset (for the gemma series).  A higher win rate indicates superior performance.", "section": "Experiments"}, {"figure_path": "2cQ3lPhkeO/tables/tables_26_2.jpg", "caption": "Table 4: Pairwise win rates (left vs. right) for beta series models on MT-Benchmark.", "description": "This table shows the pairwise win rates between three models: RPO (beta), Ref. (beta), and DPO (beta), on the MT-Benchmark dataset.  The win rate is calculated for each model compared against the others.  For example, the value 83.75 in the first row and second column indicates that RPO (beta) wins against Ref. (beta) 83.75% of the time.", "section": "6 Experiments"}, {"figure_path": "2cQ3lPhkeO/tables/tables_26_3.jpg", "caption": "Table 5: Pairwise win rate (left vs. right) for gemma series models on AlpacaEval 2.0.", "description": "This table presents the pairwise win rates of the Regularized Preference Optimization (RPO) model, the reference model, and the Direct Preference Optimization (DPO) model on the AlpacaEval 2.0 benchmark for the gemma series.  The win rate is calculated based on pairwise comparisons, with a higher win rate indicating better performance.  For example, RPO(gemma) has a 50% win rate against itself, an 80.13% win rate against the reference model, and a 52.02% win rate against the DPO(gemma) model.", "section": "Experiments"}, {"figure_path": "2cQ3lPhkeO/tables/tables_26_4.jpg", "caption": "Table 6: Pairwise Length-Control (LC) win rates (left vs. right) for gemma series models on AlpacaEval 2.0.", "description": "This table shows the pairwise win rates in the AlpacaEval 2.0 benchmark for the gemma series models. Length Control (LC) win rate is used to mitigate length bias.  It compares the performance of RPO (gemma), Ref. (gemma), and DPO (gemma) models against each other, showing the percentage of times each model wins against the others.  The diagonal shows 50.00% because a model is always compared with itself.", "section": "G.2 Experimental Results"}, {"figure_path": "2cQ3lPhkeO/tables/tables_27_1.jpg", "caption": "Table 8: Results on GSM8K, ARC, and MBPP. Here, zephyr-gemma-7b is the officially released models trained by DPO and Ref. denotes the reference model zephyr-7b-gemma-sft used for our training. RPO and DPO are trained with the OpenRLHF codebase [27] and we average the SFT loss regularizer in RPO by the number of tokens of the chosen response. We do not use chain-of-thought or few shots in all the benchmarks. We compare the greedy decoding result (pass @1) for MBPP.", "description": "This table presents the results of GSM8K, ARC, and MBPP benchmarks for four different models: RPO, DPO, the reference model, and the officially released zephyr-gemma-7b model.  The RPO model shows competitive performance, especially in the GSM8K and MBPP benchmarks.  The table notes the use of the OpenRLHF codebase and the averaging of the SFT loss regularizer by token count in the RPO model.", "section": "G Experiments on Math, Reasoning, and Coding Tasks"}, {"figure_path": "2cQ3lPhkeO/tables/tables_27_2.jpg", "caption": "Table 1: Pairwise win rate (left vs. right) among RPO-trained model, DPO-trained model, and the reference model. Annotated by GPT-4, evaluations of beta and gemma series are made on the 150 examples of the test split of the Ultrafeedback and the Argilla-DPO-Mix-7K dataset, respectively.", "description": "This table presents the results of pairwise comparisons between the RPO, DPO, and reference models, evaluated using GPT-4 annotations on the test splits of the Ultrafeedback and Argilla-DPO-Mix-7K datasets.  The win rate indicates the percentage of times each model's response was preferred over the other model's response, revealing performance differences between models in terms of human preference alignment.", "section": "6 Experiments"}]