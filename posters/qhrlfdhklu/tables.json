[{"figure_path": "QHRLFdhkLu/tables/tables_5_1.jpg", "caption": "Table 1: Comparison of RTD and MH-RTD on Open Book QA.", "description": "This table compares the performance of the Reference Trustable Decoding (RTD) method and its multi-head variant (MH-RTD) on the Open Book Question Answering (OBQA) benchmark.  It shows the scores achieved by each method using three different large language models (LLMs): MPT-7B, LLaMA2-7B, and LLaMA2-70B.  The results demonstrate the improvement in performance offered by the multi-head approach over the single-head RTD.", "section": "3.3 Multi-head Reference Trustable Decoding"}, {"figure_path": "QHRLFdhkLu/tables/tables_7_1.jpg", "caption": "Table 2: RTD on language understanding benches. Baseline refers to zero-shot performance. ICL exceeds MPT-7B's 2048 context window, with a 0 score result, recorded as failed in the table.", "description": "This table presents the results of the Reference Trustable Decoding (RTD) method on several language understanding benchmarks.  It compares the performance of RTD against a baseline (zero-shot), 5-shot In-Context Learning (ICL), and 5-shot RTD.  The benchmarks include various reasoning and comprehension tasks. The table shows that RTD consistently improves performance over the baseline, and often matches or surpasses the performance of 5-shot ICL, demonstrating the effectiveness of RTD in adapting LLMs to downstream tasks without the need for additional training.", "section": "4.1 Language Understanding"}, {"figure_path": "QHRLFdhkLu/tables/tables_7_2.jpg", "caption": "Table 2: RTD on language understanding benches. Baseline refers to zero-shot performance. ICL exceeds MPT-7B's 2048 context window, with a 0 score result, recorded as failed in the table.", "description": "This table presents the results of applying the Reference Trustable Decoding (RTD) method to various language understanding benchmarks.  It compares the performance of RTD against a baseline (zero-shot performance) and In-Context Learning (ICL) with 5-shot and 5-shot RTD. The table shows improvements gained by RTD on various Language Models across multiple benchmarks.  Note that some ICL results are marked as \"failed\" due to exceeding the model's context window limit. ", "section": "4.1 Language Understanding"}, {"figure_path": "QHRLFdhkLu/tables/tables_8_1.jpg", "caption": "Table 1: Comparison of RTD and MH-RTD on Open Book QA.", "description": "This table presents a performance comparison between the Reference Trustable Decoding (RTD) method and its multi-head variant (MH-RTD) on the Open Book Question Answering (OBQA) benchmark.  It shows the scores achieved by RTD and MH-RTD using three different large language models: MPT-7B, LLaMA2-7B, and LLaMA2-70B. The results demonstrate the improvement in performance offered by the multi-head approach.", "section": "3.3 Multi-head Reference Trustable Decoding"}, {"figure_path": "QHRLFdhkLu/tables/tables_8_2.jpg", "caption": "Table 6: Different \u03bb in Language Understanding", "description": "This table presents the results of the OBQA benchmark using different values for the hyperparameter \u03bb in the RTD method.  It shows how the performance of the model on the OBQA task changes with different values of \u03bb, demonstrating the impact of this hyperparameter on the overall performance of the RTD method in language understanding tasks.", "section": "4.1 Language Understanding"}, {"figure_path": "QHRLFdhkLu/tables/tables_8_3.jpg", "caption": "Table 7: Efficiency of RTD.", "description": "This table compares the inference speed (tokens per second) and extra memory usage (in MB) for different methods: baseline (default LLM), RTD, ICL, and ICL+RTD.  It shows that RTD has a comparable speed to the baseline while having significantly less memory consumption. Combining ICL and RTD further improves speed but increases memory usage.", "section": "4.1 Language Understanding"}, {"figure_path": "QHRLFdhkLu/tables/tables_9_1.jpg", "caption": "Table 1: Comparison of RTD and MH-RTD on Open Book QA.", "description": "This table presents a comparison of the performance of Reference Trustable Decoding (RTD) and Multi-head Reference Trustable Decoding (MH-RTD) on the Open Book Question Answering (OBQA) benchmark.  It shows the accuracy achieved by each method using three different large language models: MPT-7B, LLaMA2-7B, and LLaMA2-70B. The results demonstrate the effectiveness of MH-RTD compared to RTD in improving the accuracy on OBQA.", "section": "3.3 Multi-head Reference Trustable Decoding"}, {"figure_path": "QHRLFdhkLu/tables/tables_14_1.jpg", "caption": "Table 10: Average length by token in OBQA question answering process, split by sections.", "description": "This table shows the average token length of different sections in the Open Book Question Answering (OBQA) dataset.  The sections include the Wikipedia context, the question itself, and the response. The data helps to understand the relative lengths of different parts of the OBQA task, which is important in the context of the paper's discussion of the effect of input length on language model performance.", "section": "4.2 Language Generation"}, {"figure_path": "QHRLFdhkLu/tables/tables_15_1.jpg", "caption": "Table 1: Comparison of RTD and MH-RTD on Open Book QA.", "description": "This table compares the performance of Reference Trustable Decoding (RTD) and Multi-head Reference Trustable Decoding (MH-RTD) on the Open Book Question Answering (OBQA) benchmark.  It shows the scores achieved by these methods using different language models (MPT-7B, LLaMA2-7B, and LLaMA2-70B). The results highlight the performance improvement offered by MH-RTD over RTD.", "section": "3.3 Multi-head Reference Trustable Decoding"}]