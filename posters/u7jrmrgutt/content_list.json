[{"type": "text", "text": "Graph Edit Distance with General Costs Using Neural Set Divergence ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Eeshaan Jain\u2217\u2020 Indradyumna Roy\u2217\u2021 Saswat Meher\u2021 Soumen Chakrabarti\u2021 Abir De\u2021 ", "page_idx": 0}, {"type": "text", "text": "\u2020EPFL \u2021IIT Bombay eeshaan.jain@epfl.ch {saswatmeher,soumen,indraroy15,abir}@cse.iitb.ac.in ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Graph Edit Distance (GED) measures the (dis-)similarity between two given graphs, in terms of the minimum-cost edit sequence that transforms one graph to the other. However, the exact computation of GED is NP-Hard, which has recently motivated the design of neural methods for GED estimation. However, they do not explicitly account for edit operations with different costs. In response, we propose GRAPHEDX, a neural GED estimator that can work with general costs specified for the four edit operations, viz., edge deletion, edge addition, node deletion and node addition. We first present GED as a quadratic assignment problem (QAP) that incorporates these four costs. Then, we represent each graph as a set of node and edge embeddings and use them to design a family of neural set divergence surrogates. We replace the QAP terms corresponding to each operation with their surrogates. Computing such neural set divergence require aligning nodes and edges of the two graphs. We learn these alignments using a Gumbel-Sinkhorn permutation generator, additionally ensuring that the node and edge alignments are consistent with each other. Moreover, these alignments are cognizant of both the presence and absence of edges between node-pairs. Experiments on several datasets, under a variety of edit cost settings, show that GRAPHEDX consistently outperforms state-of-the-art methods and heuristics in terms of prediction error. The code is available at https://github.com/structlearning/GraphEdX. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The Graph Edit Distance (GED) between a source graph, $G$ , and a target graph, $G^{\\prime}$ , quantifies the minimum cost required to transform $G$ into a graph isomorphic to $G^{\\prime}$ . This transformation involves a sequence of edit operations, which can include node and edge insertions, deletions and substitutions. Each type of edit operation may incur a different and distinctive cost, allowing the GED framework to incorporate domain-specific knowledge. Its flexibility has led to the widespread use of GED for comparing graphs across diverse applications including graph retrieval [5, 6], pattern recognition [47], image and video indexing [51, 49] and chemoinformatics [21]. Because costs for addition and deletion may differ, GED is not necessarily symmetric, i.e., $\\operatorname{GED}(G,G^{\\prime})\\neq\\operatorname{GED}(G^{\\prime},G)$ . This flexibility allows GED to model a variety of graph comparison scenarios, such as finding the Maximum Common Subgraph [43] and checking for Subgraph Isomorphism [13]. In general, it is hard to even approximate GED [32]. Recent work [5, 6, 19, 56, 39] has leveraged graph neural networks (GNNs) to build neural models for GED computation, but many of these approaches cannot account for edit operations with different costs. Moreover, several approaches [40, 31, 56, 6] cast GED as the Euclidean distance between graph embeddings, leading to models that are overly attuned to cost-invariant edit sequences. ", "page_idx": 0}, {"type": "text", "text": "1.1 Present work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We propose a novel neural model for computing GED, designed to explicitly incorporate the various costs of edit operations. Our contributions are detailed as follows. ", "page_idx": 1}, {"type": "text", "text": "Neural set divergence surrogates for GED We formulate GED under general (non-uniform) cost as a quadratic assignment problem (QAP) with four asymmetric distance terms representing edge deletion, edge addition, node deletion and node addition. The edge-edit operations involve quadratic dependencies on a node alignment plan \u2014 a proposed mapping of nodes from the source graph to the target graph. To avoid the the complexity of QAP [45], we design a family of differentiable set divergence surrogates, which can replace the QAP objective with a more benign one. In this approach, each graph is represented as a set of embeddings of nodes and node-pairs (edges or non-edges). We replace the original QAP distance terms with their corresponding set divergences, and obtain the node alignment from a differentiable alignment generator modeled using a Gumbel-Sinkhorn network. This network produces a soft node permutation matrix based on contextual node embeddings from the graph pairs, enabling the computation of the overall set divergence in a differentiable manner, which facilitates end-to-end training. Our proposed model relies on late interaction, where the interactions between the graph pairs occur only at the final layer, rather than during the embedding computation in the GNN. This supports the indexing of embedding vectors, thereby facilitating efficient retrieval through LSH [25, 24, 12], inverted index [20], graph based ANN [34, 37] etc. ", "page_idx": 1}, {"type": "text", "text": "Learning all node-pair representations The optimal sequence of edits in GED is heavily influenced by the global structure of the graphs. A perturbation in one part of the graph can have cascading effects, necessitating edits in distant areas. To capture this sensitivity to structural changes, we associate both edges as well as non-edges with suitable expressive embeddings that capture the essence of subgraphs surrounding them. Note that the embeddings for non-edges are never explicitly computed during GNN message-passing operations. They are computed only once, after the GNN has completed its usual message-passing through existing edges, thereby minimizing additional computational overhead. ", "page_idx": 1}, {"type": "text", "text": "Node-edge consistent alignment To ensure edge-consistency in the learned node alignment map, we explicitly compute the node-pair alignment map from the node alignment map and then utilize this derived map to compute collective edge deletion and addition costs. More precisely, if $(u,v)\\in G$ and $(u^{\\prime},v^{\\prime})\\bar{\\in}\\;G^{\\prime}$ are matched, then the nodes $u$ and $v$ are constrained to match with $u^{\\prime}$ and $v^{\\prime}$ (or, $v^{\\prime}$ and $u^{\\prime}$ ) respectively. We call our neural framework as GRAPHEDX. ", "page_idx": 1}, {"type": "text", "text": "Our experiments across several real datasets show that (1) GRAPHEDX outperforms several state-ofthe-art methods including those that use early interaction; (2) the performance of current state-ofthe-art methods improves significantly when their proposed distance measures are adjusted to reflect GED-specific distances, as in our approach. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Heuristics for Graph Edit Distance GED was first introduced in [46]. Bunke and Allermann [14] used it as a tool for non exact graph matching. Later on, [13] connected GED with maximum common subgraph estimation. Blumenthal [7] provide an excellent survey. As they suggest, combinatorial heuristics to solve GED predominantly follows three approaches: (1) Linear sum assignment problem with error-correction, which include [27, 41, 53, 55] (2) Linear programming, which predominantly uses standard tools like Gurobi, (3) Local search [42]. However, they can be extremely time consuming, especially for a large number of graph pairs. Among them Zheng et al. [55] operate in our problem setting, where the cost of edits are different across the edit operations, but for the same edit operation, the cost is same across node or node pairs. ", "page_idx": 1}, {"type": "text", "text": "Optimal Transport In our work, we utilize Graph Neural Networks (GNNs) to represent each graph as a set of node embeddings. This transforms the inherent Quadratic Assignment Problem (QAP) of graph matching into a Linear Sum Assignment Problem (LSAP) on the sets of node embeddings. Essentially, this requires solving an optimal transport problem in the node embedding space. The use of neural surrogates for optimal transport was first proposed by Cuturi [16], who introduced entropy regularization to make the optimal transport objective strictly convex and utilized Sinkhorn iterations [50] to obtain the transport plan. Subsequently, Mena et al. [35] proposed the neural Gumbel Sinkhorn network as a continuous and differentiable surrogate of a permutation matrix, which we incorporate into our model. ", "page_idx": 1}, {"type": "text", "text": "In various generative modeling applications, optimal transport costs are used as loss functions, such as in Wasserstein GANs [1, 3]. Computing the optimal transport plan is a significant challenge, with approaches leveraging the primal formulation [52, 33], the dual formulation with entropy regularization [17, 48, 22], or Input Convex Neural Networks (ICNNs) [2]. ", "page_idx": 2}, {"type": "text", "text": "Neural graph similarity computation Most earlier works on neural graph similarity computation have focused on training with GED values as ground truth [5, 6, 19, 40, 56, 39, 54, 31], while some have used MCS as the similarity measure [6, 5]. Current neural models for GED approximation primarily follow two approaches. The first approach uses a trainable nonlinear function applied to graph embeddings to compute GED [5, 39, 6, 56, 54, 19]. The second approach calculates GED based on the Euclidean distance in the embedding space [31, 40]. ", "page_idx": 2}, {"type": "text", "text": "Among these models, GOTSIM [19] focuses solely on node insertion and deletion, and computes node alignment using a combinatorial routine that is decoupled from end-to-end training. However, their network struggles with training efficiency due to the operations on discrete values, which are not amenable to backpropagation. With the exception of GREED [40] and Graph Embedding Network (GEN) [31], most methods use early interaction or nonlinear scoring functions, limiting their adaptability to efficient indexing and retrieval pipelines. ", "page_idx": 2}, {"type": "text", "text": "3 Problem setup ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notation The source graph is denoted by $G=(V,E)$ and the target graph by $G^{\\prime}=(V^{\\prime},E^{\\prime})$ . Both graphs are undirected and are padded with isolated nodes to equalize the number of nodes to $N$ . The adjacency matrices for $G$ and $G^{\\prime}$ after padding are $\\mathbf{4},A^{\\prime}\\in\\dot{\\{}0,}1\\big\\}^{N\\times N}$ . (Note that we will use $M^{\\top}$ , not $M^{\\prime}$ , for the transpose of matrix $M$ .) The sets of padded nodes in $G$ and $G^{\\prime}$ are denoted by PaddedNodesG and PaddedNodes $G^{\\prime}$ respectively. We construct $\\pmb{\\eta}\\in\\{0,1\\}^{N}$ , where $\\pmb{\\eta}[u]=0$ if $u\\in\\mathrm{PaddedNodes}_{G}$ and 1 otherwise (same for $G^{\\prime}$ ). The embedding of a node $u\\in V$ computed at propagation layer $k$ by the GNN, is represented as $\\mathbf{\\boldsymbol{x}}_{k}(u)$ . Edit operations, denoted by edit, belong to one of four types, viz., (i) node deletion, (ii) node addition, (iii) edge deletion, (iv) edge addition. Each operation edit is assigned a cost cost(edit). The node and node-pair alignment maps are described using (hard) permutation matrices $P\\,\\in\\,\\{0,1\\}^{N\\times N}$ and $\\pmb{S}\\in\\{0,1\\}^{\\binom{N}{2}\\times\\binom{N}{2}}$ respectively. Given that the graphs are undirected, node-pair alignment need only be specified across at most $\\binom{N}{2}$ pairs. When a hard permutation matrix is relaxed to a doubly-stochastic matrix, we call it a soft permutation matrix. We use $_{P}$ and $\\boldsymbol{S}$ to refer to both hard and soft permutations, depending on the context. We denote $\\mathbb{P}_{N}$ as the set of all hard permutation matrices of dimension $N$ ; $[N]$ as $\\{1,\\ldots,N\\}$ and $\\left\\Vert A\\right\\Vert_{1,1}$ to describe $\\textstyle\\sum_{u,v}|A[u,v]|$ . For two binary variables $c_{1},c_{2}\\in\\{0,1\\}$ , we denote $J(c_{1},c_{2})$ as $(c_{1}\\,\\mathrm{XOR}$ $c_{2}$ ), i.e., $J(c_{1},\\dot{c}_{2})=c_{1}c_{2}+(1-c_{1})(1-c_{2})$ . ", "page_idx": 2}, {"type": "text", "text": "Graph edit distance with general cost We define an edit path as a sequence of edit operations $\\pmb{o}=\\{\\mathrm{edit}_{1},\\mathrm{edit}_{2},...\\}$ ; and ${\\mathcal{O}}(G,G^{\\prime})$ as the set of all possible edit paths that transform the source graph $G$ into a graph isomorphic to the target graph $G^{\\prime}$ . Given $O(G,{\\bar{G}}^{\\prime})$ and the cost associated with each operation edit, the GED between $G$ and $G^{\\prime}$ is the minimum collective cost across all edit paths in $O(G,G^{\\prime})$ . Formally, we write [14, 7]: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{GED}(G,G^{\\prime})=\\underset{o=\\{\\mathrm{edit}_{1},\\mathrm{edit}_{2},\\ldots\\}\\in\\mathcal{O}(G,G^{\\prime})}{\\operatorname*{min}}\\,\\sum_{i\\in[|o|]}\\mathrm{cost}(\\mathrm{edit}_{i}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "In this work, we assume a fixed cost for each of the four types of edit operations. Specifically, we use $a^{\\ominus}$ , $a^{\\oplus}$ , $b^{\\ominus}$ and $b^{\\oplus}$ to represent the costs for edge deletion, edge addition, node deletion, and node addition, respectively. These costs are not necessarily uniform, in contrast to the assumptions made in previous works [5, 31, 56, 39]. Additional discussion on GED with node substitution in presence of labels can be found in Appendix B. ", "page_idx": 2}, {"type": "text", "text": "Problem statement Our objective is to design a neural architecture for predicting GED under a general cost framework, where the edit costs $a^{\\ominus}$ , $a^{\\oplus}$ , $b^{\\ominus}$ and $b^{\\oplus}$ are not necessarily the same. During the learning stage, these four costs are specified, and remain fixed across all training instances $\\mathcal{D}=\\bar{\\{}(G_{i},G_{i}^{\\prime},\\mathrm{G}\\bar{\\mathrm{E}}\\mathrm{D}(\\bar{G}_{i},G_{i}^{\\prime}))\\}_{i\\in[n]}$ . Note that the edit paths are not supervised. Later, given a test instance $G,G^{\\prime}$ , assuming the same four costs, the trained system has to predict $\\mathrm{GED}(G,G^{\\prime})$ . ", "page_idx": 2}, {"type": "text", "text": "4 Proposed approach ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we first present an alternative formulation of GED as described in Eq. (1), where the edit paths are induced by node alignment maps. Then, we adapt this formulation to develop ", "page_idx": 2}, {"type": "text", "text": "GRAPHEDX, a neural set distance surrogate, amenable to end-to-end training. Finally, we present the network architecture of GRAPHEDX. ", "page_idx": 3}, {"type": "text", "text": "4.1 GED computation using node alignment map ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Given the padded graph pair $G$ and $G^{\\prime}$ , deleting a node $u\\in V$ can be viewed as aligning node $u$ with some padded node $u^{\\prime}\\in\\mathrm{PaddedNodes}_{G^{\\prime}}$ . Similarly, adding a new node $u^{\\prime}$ to $G$ can be seen as aligning some padded node $u\\in$ PaddedNodes $G$ with node $u^{\\prime}\\in V^{\\prime}$ . Likewise, adding an edge to $G$ corresponds to aligning a non-edge $(u,v)\\not\\in E$ with an edge $(u^{\\prime},v^{\\prime})\\in G^{\\prime}$ . Conversely, deleting an edge in $G$ corresponds to aligning an edge $(u,v)\\in G$ with a non-edge $(u^{\\prime},v^{\\prime})\\notin G^{\\prime}$ . ", "page_idx": 3}, {"type": "text", "text": "Therefore, $\\mathrm{GED}(G,G^{\\prime})$ can be defined in terms of a node alignment map. Let $\\Pi_{N}$ represent the set of all node alignment maps $\\pi:[N]\\to[N]$ from $V$ to $V^{\\prime}$ . Recall that $\\eta_{G}[u]=0$ if $u\\in$ PaddedNodes $G$ and 1 otherwise. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\operatorname*{min}_{\\tau\\in\\Pi_{N}}\\frac{1}{2}\\sum_{u,v}\\left(a^{\\Theta}\\cdot\\mathbb{I}\\left[(u,v)\\in E\\wedge(\\pi(u),\\pi(v))\\notin E^{\\prime}\\right]+a^{\\oplus}\\cdot\\mathbb{I}\\left[(u,v)\\notin E\\wedge(\\pi(u),\\pi(v))\\in E^{\\prime}\\right]\\right)}}\\\\ &{}&{+\\sum_{u}\\left(b^{\\Theta}\\cdot\\eta_{G}[u]\\left(1-\\eta_{G^{\\prime}}[\\pi(u)]\\right)+b^{\\oplus}\\cdot\\left(1-\\eta_{G}[u]\\right)\\eta_{G^{\\prime}}[\\pi(u)]\\right).}&{{\\mathrm{(2)}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In the above expression, the first sum iterates over all pairs of $(u,v)\\in[N]\\times[N]$ and the second sum iterates over $u\\in[N]$ . Because both graphs are undirected, the fraction $1/2$ accounts for double counting of the edges. The first and second terms quantify the cost of deleting and adding the edge $(u,v)$ from and to $G$ , respectively. The third and the fourth terms evaluate the cost of deleting and adding node $u$ from and to $G$ , respectively. ", "page_idx": 3}, {"type": "text", "text": "GED as a Quadratic Assignment Problem In its current form, Eq. (2) cannot be immediately adapted to a differentiable surrogate. To circumvent this problem, we provide an equivalent matricized form of Eq. (2), using a hard node permutation matrix $_{P}$ instead of the alignment map $\\pi$ . We compute the asymmetric distances between $\\pmb{A}$ and $P A^{\\prime}P^{\\intercal}$ and combine them with weights $a^{\\ominus}$ and $a^{\\oplus}$ . Notably, ReLU $\\left(A-P A^{\\prime}P^{\\top}\\right)[u,v]$ is non-zero if the edge $(u,v)\\in E$ is mapped to a non-edge $(u^{\\prime},v^{\\prime})\\in E^{\\prime}$ with $P[u,u^{\\prime}]=P[v,v^{\\prime}]=1$ , indicating deletion of the edge $(u,v)$ from $G$ . Similarly, ReLU $\\left(P A^{\\prime}P^{\\intercal}-\\Bar{A}\\right)\\Bar{[u,v]}$ becomes non-zero if an edge $(u,v)$ is added to $G$ . Therefore, for the edit operations involving edges, we have: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{I}\\left[(u,v)\\in E\\wedge(\\pi(u),\\pi(v))\\notin E^{\\prime}\\right]=\\mathrm{ReLU}\\left(A-P A^{\\prime}P^{\\top}\\right)[u,v],}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{I}\\left[(u,v)\\notin E\\land(\\pi(u),\\pi(v))\\in E^{\\prime}\\right]=\\mathrm{ReLU}\\left(P A^{\\prime}P^{\\top}-A\\right)[u,v].}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Similarly, we note that ReLU $(\\eta_{G}[u]-\\eta_{G^{\\prime}}[\\pi(u)])~>~0$ if $u\\ \\notin\\ {\\mathrm{PaddedNodes}}_{G}$ and $\\pi(u)~\\in$ PaddedNodes $G^{\\prime}$ , which allows us to compute the cost of deleting the node $u$ from $G$ . Similarly, we use ReLU $(\\eta_{G^{\\prime}}[\\pi(u)]-\\eta_{G}[u])$ to account for the addition of the node $u$ to $G$ . Formally, we write: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\eta_{G}[u]\\,\\dot{(1-\\eta_{G^{\\prime}}[\\pi(u)])}=\\mathrm{ReLU}\\left(\\eta_{G}[u]-\\eta_{G^{\\prime}}[\\pi(u)]\\right),}\\\\ {\\left(1-\\eta_{G}[u]\\right)\\eta_{G^{\\prime}}[\\pi(u)]=\\mathrm{ReLU}\\left(\\eta_{G^{\\prime}}[\\pi(u)]-\\eta_{G}[u]\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Using Eqs. (3)\u2013(6), we rewrite Eq. (2) as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{GED}(G,G^{\\prime})=\\operatorname*{min}_{P\\in\\mathbb{P}_{N}}\\,\\frac{a^{\\ominus}}{2}\\left\\|\\mathrm{ReLU}\\left(A-P A^{\\prime}P^{\\top}\\right)\\right\\|_{1,1}+\\frac{a^{\\oplus}}{2}\\left\\|\\mathrm{ReLU}\\left(P A^{\\prime}P^{\\top}-A\\right)\\right\\|_{1,1}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$$\n+\\left.b^{\\ominus}\\left\\|\\mathrm{ReLU}\\left(\\eta_{G}-P\\eta_{G^{\\prime}}\\right)\\right\\|_{1}+b^{\\oplus}\\left\\|\\mathrm{ReLU}\\left(P\\eta_{G^{\\prime}}-\\eta_{G}\\right)\\right\\|_{1}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The first and the second term denote the collective costs of deletion and addition of edges, respectively. The third and the fourth terms present a matricized representation of Eqs. (5)- (6). The above problem can be viewed as a quadratic assignment problem (QAP) on graphs, given that the hard node permutation matrix $_{P}$ has a quadratic involvement in the first two terms. Note that, in general, $\\operatorname{GED}(G,G^{\\prime})\\neq\\operatorname{GED}(G^{\\prime},G)$ . However, the optimal edit paths for these two GED values, encoded by the respective node permutation matrices, are inverses of each other, as formally stated in the following proposition (proven in Appendix B). ", "page_idx": 3}, {"type": "text", "text": "Proposition 1 Given a fixed set of values of $b^{\\ominus},b^{\\oplus},a^{\\ominus},a^{\\oplus}$ , let $_{P}$ be an optimal node permutation matrix corresponding to $\\mathrm{GED}(G,G^{\\prime})$ , computed using Eq. (7). Then, $P^{\\prime}\\stackrel{\\cdot}{=}P^{\\top}$ is an optimal node permutation corresponding to GED $(G^{\\prime},G)$ . ", "page_idx": 3}, {"type": "text", "text": "Connection to different notions of graph matching The above expression of GED can be used to represent various notions of graph matching and similarity measures by modifying the edit costs. These include graph isomorphism, subgraph isomorphism, and maximum common edge subgraph detection. For example, by setting all costs to one, $\\begin{array}{r}{\\mathrm{GED}(G,G^{\\prime})=\\operatorname*{min}_{P}\\frac{1}{2}||A-P A^{\\prime}P^{\\top}||_{1}+}\\end{array}$ $||\\eta_{G}-P\\eta_{G^{\\prime}}||_{1}$ , which equals zero only when $G$ and $G^{\\prime}$ are isomorphic. Further discussion on this topic is provided in Appendix B. ", "page_idx": 3}, {"type": "image", "img_path": "u7JRmrGutT/tmp/17031014df400871d72c524b0899af17f19d4f8fb95661abda955fca34c59c79.jpg", "img_caption": ["Figure 1: Top: Example graphs $G$ and $G^{\\prime}$ are shown with color-coded nodes to indicate alignment corresponding to the optimal edit path transforming $G$ to $G^{\\prime}$ . Bottom: GRAPHEDX\u2019s GED prediction pipeline. $G$ and $G^{\\prime}$ are independently encoded using $\\mathrm{MPNN}_{\\theta}$ , and then padded with zero vectors to equalize sizes, resulting in contextual node representations $\\bar{\\b X^{\\prime}}\\b X^{\\prime}\\in\\mathbb{R}^{N^{\\star}\\times d}$ . For each node-pair, the corresponding embeddings and edge presence information are gathered and fed into ${\\bf M L P}_{\\theta}$ to obtain $R,R^{\\prime}\\in\\mathbb{R}^{N(N-1)/2\\times D}$ . Simultaneously, $X,X^{\\prime}$ are fed into $\\mathbf{PERMNET}_{\\phi}$ to obtain the soft node alignment $_{P}$ (Eq.(16)) which constructs the node-pair alignment matrix $S\\in\\mathbb{R}^{N(N-1)/2\\times N(N-1)/2}$ as $\\bar{\\pmb{S}}[(u,v),(u^{\\prime},\\bar{v}^{\\prime})]=\\pmb{P}[u,u^{\\prime}]\\pmb{P}[v,v^{\\prime}]+\\pmb{P}[u,v^{\\prime}]\\pmb{P}[v,\\bar{u}^{\\prime}]$ . Finally, $X,X^{\\prime},P$ are used to approximate node insertion and deletion costs, while $R,R^{\\prime},S$ are used to approximate edge insertion and deletion costs. The four costs are summed to give the final prediction $\\bar{\\mathrm{GED}}_{\\theta,\\phi}(G,\\bar{G^{\\prime}})$ (Eq.(8)). "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "4.2 GRAPHEDX model ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Minimizing the objective in Eq. (7) is a challenging problem. In similar problems, recent methods have approximated the hard node permutation matrix $_{P}$ with a soft permutation matrix obtained using Sinkhorn iterations on a neural cost matrix. However, the binary nature of the adjacency matrix and the pad indicator $\\eta$ still impede the flow of gradients during training. To tackle this problem, we make relaxations in two key places within each term in Eq. (7), leading to our proposed GRAPHEDX model. ", "page_idx": 4}, {"type": "text", "text": "(1) We replace the binary values in $\\eta_{G},\\eta_{G^{\\prime}}$ , $\\pmb{A}$ and $A^{\\prime}$ with real values from node and node-pair embeddings: $\\pmb{X}\\in\\mathbb{R}^{N\\times d}$ and $\\pmb{R}\\in\\mathbb{R}^{\\binom{N}{2}\\times D}$ . These embeddings are computed using a GNN guided neural module $\\operatorname{EMBED}\\theta$ with parameter $\\theta$ . Since the graphs are undirected, $\\boldsymbol{R}$ gathers the embeddings of the unique node-pairs, resulting in $\\binom{N}{2}$ rows instead of $N^{2}$ .   \n(2) We substitute the hard node permutation matrix $_{P}$ with a soft alignment matrix, generated using a differentiable alignment planner $\\mathbf{PERMNET}_{\\phi}$ with parameter $\\phi$ . Here, $_{P}$ is a doubly stochastic matrix, with $P[u,u^{\\prime}]$ indicating the \"score\" or \"probability\" of aligning $u\\mapsto u^{\\prime}$ . Additionally, we also compute the corresponding node-pair alignment matrix $\\boldsymbol{S}$ . Using these relaxations, we approximate the four edit costs in Eq. (7) with four continuous set distance surrogate functions.   \n$\\big|\\mathrm{ReLU}\\left(A-P A^{\\prime}P^{\\top}\\right)\\big|_{1,1}\\to\\Delta^{\\odot}(R,R^{\\prime}\\,|\\,S),\\quad\\big|\\big|\\mathrm{ReLU}\\left(P A^{\\prime}P^{\\top}-A\\right)\\big|_{1,1}\\to\\Delta^{\\oplus}(R,R^{\\prime}\\,|\\,S),$ $\\begin{array}{r}{\\|\\mathrm{ReLU}\\left(\\eta_{G}-P\\eta_{G^{\\prime}}\\right)\\|_{1}\\to\\Delta^{\\Theta}(X,X^{\\prime}\\mid P),\\quad\\|\\mathrm{ReLU}\\left(P\\eta_{G^{\\prime}}-\\eta_{G}\\right)\\|_{1}\\to\\Delta^{\\Theta}(X,X^{\\prime}\\mid P).}\\end{array}$ This gives us an approximated GED parameterized by $\\theta$ and $\\phi$ . ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{GED}_{\\theta,\\phi}(G,G^{\\prime})\\stackrel{!}{=}a^{\\ominus}\\Delta^{\\ominus}({\\pmb R},{\\pmb R}^{\\prime}\\,|\\,{\\pmb S})+a^{\\oplus}\\Delta^{\\oplus}({\\pmb R},{\\pmb R}^{\\prime}\\,|\\,{\\pmb S})}\\\\ &{\\qquad\\qquad\\qquad+b^{\\ominus}\\Delta^{\\ominus}({\\pmb X},{\\pmb X}^{\\prime}\\,|\\,{\\pmb P})+b^{\\oplus}\\Delta^{\\oplus}({\\pmb X},{\\pmb X}^{\\prime}\\,|\\,{\\pmb P})}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Note that since $\\boldsymbol{R}$ and $R^{\\prime}$ contain the embeddings of each node-pair only once, there is no need to multiply $1/2$ in the first two terms, unlike Eq. (7). Next, we propose three types of neural surrogates to approximate each of the four operations. ", "page_idx": 4}, {"type": "text", "text": "(1) ALIGNDIFF Given the node-pair embeddings $\\boldsymbol{R}$ and $R^{\\prime}$ for the graph pairs $G$ and $G^{\\prime}$ , we apply the soft node-pair alignment $\\boldsymbol{S}$ to $\\scriptstyle{R^{\\prime}}$ . We then define the edge edits in terms of asymmetric ", "page_idx": 4}, {"type": "text", "text": "differences between $\\boldsymbol{R}$ and $S R^{\\prime}$ , which serves as a replacement for the corresponding terms in Eq. (7). We write $\\Delta^{\\ominus}(R,R^{\\prime}\\,|\\,S)$ and $\\Delta^{\\oplus}(R,R^{\\prime}\\,|\\,S)$ as: ", "page_idx": 5}, {"type": "equation", "text": "$\\Delta^{\\odot}(R,R^{\\prime}\\,|\\,S)=\\lVert\\mathrm{ReLU}\\left(R-S R^{\\prime}\\right)\\rVert_{1,1}\\,,\\quad\\Delta^{\\oplus}(R,R^{\\prime}\\,|\\,S)=\\lVert\\mathrm{ReLU}\\left(S R^{\\prime}-R\\right)\\rVert_{1,1}\\,.$ ", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Similarly, for the node edits, we can compute $\\Delta^{\\ominus}(X,X^{\\prime}\\,|\\,P)$ and $\\Delta^{\\oplus}(X,X^{\\prime}\\,|\\,P)$ as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Delta^{\\ominus}(X,X^{\\prime}\\,|\\,P)=\\|\\mathrm{ReLU}\\,(X-P X^{\\prime})\\|_{1,1}\\,,\\quad\\Delta^{\\oplus}(X,X^{\\prime}\\,|\\,P)=\\|\\mathrm{ReLU}\\,(P X^{\\prime}-X)\\|_{1,1}\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "(2) DIFFALIGN In Eq. (9), we first aligned $\\scriptstyle{R^{\\prime}}$ using $\\boldsymbol{S}$ and then computed the difference from $\\boldsymbol{R}$ . Instead, here we first computed the pairwise differences between $\\scriptstyle{R^{\\prime}}$ and $\\boldsymbol{R}$ for all pairs of node-pairs $(e,e^{\\prime})$ , and then combine these differences with the corresponding alignment scores $S[e,e^{\\prime}]$ . We compute the edge edit surrogates $\\Delta^{\\ominus}(R,R^{\\prime}\\,|\\,S)$ and $\\Delta^{\\oplus}(\\bar{\\cal R},{\\cal R}^{\\prime}\\,|\\,\\bar{\\cal S})$ as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta^{\\odot}(R,R^{\\prime}\\,|\\,S)=\\displaystyle\\sum_{e,e^{\\prime}}\\|\\mathrm{ReLU}\\left(R[e,:]-R^{\\prime}[e^{\\prime},:]\\right)\\|_{1}\\,S[e,e^{\\prime}],}\\\\ &{\\Delta^{\\oplus}(R,R^{\\prime}\\,|\\,S)=\\displaystyle\\sum_{e,e^{\\prime}}\\|\\mathrm{ReLU}\\left(R^{\\prime}[e^{\\prime},:]-R[e,:]\\right)\\|_{1}\\,S[e,e^{\\prime}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here, $e$ and $e^{\\prime}$ represent node-pairs, which are not necessarily edges. When the node-pair alignment matrix $\\boldsymbol{S}$ is a hard permutation, $\\Delta^{\\oplus}$ and $\\Delta^{\\ominus}$ remain the same across ALIGNDIFF and DIFFALIGN (as shown in Appendix B). Similar to Eqs. (10)\u2014(11), we can compute $\\begin{array}{r}{\\Delta^{\\ominus}(X,X^{\\prime}|\\,P)\\;=\\;\\sum_{u,u^{\\prime}}\\|\\mathrm{Re}\\mathrm{\\hat{L}U}\\left(X[u,:]-X^{\\prime}[u^{\\prime},:]\\right)\\|_{1}\\,P[u,u^{\\prime}]}\\end{array}$ and $\\Delta^{\\oplus}(X,X^{\\prime}\\,|\\,P)\\;=$ $\\begin{array}{r}{\\sum_{u,u^{\\prime}}\\|\\mathrm{ReLU}\\left(X^{\\prime}[u^{\\prime},:]-X[u,:]\\right)\\|_{1}\\,P[u,u^{\\prime}].}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "(3) XOR-DIFFALIGN As indicated by the combinatorial formulation of GED in Eq. (7), the edit cost of a particular node-pair is non-zero only when an edge is mapped to a non-edge or vice-versa. However, the surrogates for the edge edits in ALIGNDIFF or DIFFALIGN fail to capture this condition because they can assign non-zero costs to the pairs $(e=(u,v),e^{\\prime}=(u^{\\prime},v^{\\prime}))$ even when both $e$ and $e^{\\prime}$ are either edges or non-edges. To address this, we explicitly discard such pairs from the surrogates defined in Eqs. (10)\u2013(11). This is ensured by applying a XOR operator $\\bar{J}(\\cdot,\\cdot)$ between the corresponding entries in the adjacency matrices, i.e., $A[u,v]$ and $A^{\\prime}[u^{\\prime},v^{\\prime}]$ , and then multiplying this result with the underlying term. Hence, we write: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta^{\\odot}(R,R^{\\prime}\\,|\\,S)=\\displaystyle\\sum_{\\stackrel{e=(u,v)}{e^{\\prime}=(u^{\\prime},v^{\\prime})}}J\\big(A[u,v],A^{\\prime}[u^{\\prime},v^{\\prime}]\\big)\\,\\lVert\\mathrm{ReLU}\\left(R[e,:]-R^{\\prime}[e^{\\prime},:]\\right)\\rVert_{1}\\,S[e,e^{\\prime}],}\\\\ &{\\Delta^{\\oplus}(R,R^{\\prime}\\,|\\,S)=\\displaystyle\\sum_{\\stackrel{e=(u,v)}{e^{\\prime}=(u^{\\prime},v^{\\prime})}}J\\big(A[u,v],A^{\\prime}[u^{\\prime},v^{\\prime}]\\big)\\,\\lVert\\mathrm{ReLU}\\left(R^{\\prime}[e^{\\prime},:]-R[e,:]\\right)\\rVert_{1}\\,S[e,e^{\\prime}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Similarly, the cost contribution for node operations arises from mapping a padded node to a non-padded node or vice versa. We account for this by multiplying $\\bar{J}(\\eta\\bar{\\cal G}[u],\\eta_{\\cal G^{\\prime}}[u^{\\prime}])$ with each term of $\\Delta^{\\ominus}(X,X^{\\prime}\\,|\\,P)$ and $\\Delta^{\\oplus}(X,X^{\\prime}\\,|\\,P)$ computed using DIFFALIGN. Hence, we compute $\\begin{array}{r}{\\Delta^{\\ominus}(X,X^{\\prime}|\\,P)=\\sum_{u,u^{\\prime}}J(\\eta_{G}[u],\\eta_{G^{\\prime}}[u^{\\prime}])\\,|}\\end{array}$ $\\|\\mathrm{ReLU}\\left(\\boldsymbol{X}[u,:]-\\boldsymbol{X}^{\\prime}[u^{\\prime},:]\\right)\\|_{1}\\,\\boldsymbol{P}[u,u^{\\prime}]$ and $\\begin{array}{r}{\\Delta^{\\oplus}(X,X^{\\prime}\\,|\\,P)=\\sum_{u,u^{\\prime}}J(\\eta_{G}[u],\\eta_{G^{\\prime}}[u^{\\prime}])\\,||\\mathrm{ReLU}\\left(X^{\\prime}[u^{\\prime},:]-X[u,:]\\right)||_{1}\\,P[u,u^{\\prime}]}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "Comparison between ALIGNDIFF, DIFFALIGN and XOR-DIFFALIGN ALIGNDIFF and DIFFALIGN become equivalent when $\\boldsymbol{S}$ is a hard permutation. However, when $\\boldsymbol{S}$ is doubly stochastic, the above three surrogates, ALIGNDIFF, DIFFALIGN and XOR-DIFFALIGN, are not equivalent. As we move from ALIGNDIFF to DIFFALIGN to XOR-DIFFALIGN, we increasingly align the design to the inherent inductive biases of GED, thereby achieving a better representation of its cost structure. ", "page_idx": 5}, {"type": "text", "text": "Suppose we are computing the GED between two isomorphic graphs, $G$ and $G^{\\prime}$ , with uniform costs for all edit operations. In this scenario, we ideally expect a neural network to consistently output a zero cost. Now consider a proposed soft alignment $\\boldsymbol{S}$ which is close to the optimal alignment. Under the ALIGNDIFF design, the aggregated value $\\begin{array}{r}{\\sum_{e^{\\prime}}S[e,e^{\\prime}]R^{\\prime}[e^{\\prime},:]}\\end{array}$ \u2014 where $e$ and $e^{\\prime}$ represent two edges matched in the optimal alignment \u2014 can accumulate over the large number of $\\bar{N}(N\\bar{-}1)/2$ node-pairs. This aggregation leads to high values of $||\\boldsymbol{R}[e,:]-\\boldsymbol{S}\\boldsymbol{R}^{\\prime}[e^{\\prime},:]||_{1}^{-}$ , implying that ALIGNDIFF captures an aggregate measure of the cost incurred by spurious alignments, but cannot disentangle the effect of individual misalignments, making it difficult for ALIGNDIFF to learn the optimal alignment. ", "page_idx": 5}, {"type": "text", "text": "In contrast, the DIFFALIGN approach, which relies on pairwise differences between embeddings to explicitly guide $\\boldsymbol{S}$ towards the optimal alignment, significantly ameliorates this issue. For example, in the aforementioned setting of GED with uniform costs, the cost associated with each pairing $(e,e^{\\prime})$ is explicitly encoded using $||R[e,:]-R^{\\prime}[e^{\\prime},:]||_{1}$ , and is explicitly set to zero for pairs that are correctly aligned. Moreover, this representation allows DIFFALIGN to isolate the cost incurred by each misalignment, making it easier to train the model to reduce the cost of these spurious matches to zero. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "However, DIFFALIGN does not explicitly set edge-to-edge and non-edge-to-non-edge mapping costs to zero, potentially leading to inaccurate GED estimates. XOR-DIFFALIGN addresses these concerns by applying a XOR of the adjacency matrices to the cost matrix, ensuring that non-zero cost is computed only when mapping an edge to a non-edge or vice versa. This resolves the issues in both ALIGNDIFF and DIFFALIGN by focusing on mismatches between edges and non-edges, while disregarding redundant alignments that do not contribute to the GED. ", "page_idx": 6}, {"type": "text", "text": "Amenability to indexing and approximate nearest neighbor (ANN) search. All of the aforementioned distance surrogates are based on a late interaction paradigm, where the embeddings of $G$ and $G^{\\prime}$ are computed independently of each other before computing the distances $\\Delta$ . This is particularly useful in the context of graph retrieval, as it allows for the corpus graph embeddings to be indexed a-priori, thereby enabling efficient retrieval of relevant graphs for new queries. ", "page_idx": 6}, {"type": "text", "text": "When the edit costs are uniform, our predicted GED (8) becomes symmetric with respect to $G$ and $G^{\\prime}$ . In such cases, DIFFALIGN and ALIGNDIFF yield a structure similar to the Wasserstein distance induced by $L_{1}$ norm. This allows us to leverage ANN techniques like Quadtree or Flowtree [4]. However, while the presence of the XOR operator $J$ within each term in Eq. (12) \u2013 (13) of XORDIFFALIGN enhances the interaction between $G$ and $G^{\\prime}$ , this same feature prevents XOR-DIFFALIGN from being cast to an ANN-amenable setup, unlike DIFFALIGN and ALIGNDIFF. ", "page_idx": 6}, {"type": "text", "text": "4.3 Network architecture of $\\mathbf{EMBED}\\theta$ and PERMNET\u03d5 ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we present the network architectures of the two components of GRAPHEDX, viz., $\\operatorname{EMBED}\\theta$ and $\\mathbf{PERMNET}_{\\phi}$ , as introduced in items (1) and (2) in Section 4.2. Notably, in our proposed graph representation, non-edges and edges alike are embedded as non-zero vectors. In other words, all node-pairs are endowed with non-trivial embeddings. We then explain the design approach for edge-consistent node alignment. ", "page_idx": 6}, {"type": "text", "text": "Neural architecture of $\\mathbf{EMBED}\\theta$ $\\operatorname{EMBED}\\theta$ consists of a message passing neural network $\\mathrm{MPNN}_{\\theta}$ and a decoupled neural module ${\\bf M L P}_{\\theta}$ . Given the graphs $G,G^{\\prime}$ , $\\mathrm{MPNN}_{\\theta}$ with $K$ propagation layers is used to iteratively compute the node embeddings $\\left\\{x_{K}(u)\\in\\mathbb{R}^{d}\\,|\\,u\\in V\\right\\}$ and $\\left\\{{\\pmb{x}}_{K}^{\\prime}(u)\\in\\mathbb{R}^{d}\\,|\\,u\\in V^{\\prime}\\right\\}$ , then collect them into $\\mathbf{\\deltaX}$ and $X^{\\prime}$ after padding, i.e., ", "page_idx": 6}, {"type": "text", "text": "The optimal alignment $\\boldsymbol{S}$ is highly sensitive to the global structure of the graph pairs, i.e., $S[e,e^{\\prime}]$ can significantly change when we perturb $G$ or $G^{\\prime}$ in regimes distant from $e$ or $e^{\\prime}$ . Conventional representations mitigate this sensitivity while training models, by setting non-edges to zero, rendering them invariant to structural changes. To address this limitation, we utilize more expressive graph representations, where non-edges are also embedded using trainable non-zero vectors. This approach allows information to be captured from the structure around the nodes through both edges and non-edges, thereby enhancing the representational capacity of the embedding network. For each node-pair $e=(u,\\dot{v})\\in G$ (and equivalently $(v,u))$ , and $e^{\\prime}\\,\\stackrel{}{=}(u^{\\prime},v^{\\prime})\\in G^{\\prime}$ , the embeddings of the corresponding nodes and their connectivity status are concatenated, and then passed through an MLP to obtain the embedding vectors $\\pmb{r}(e),\\pmb{r}^{\\prime}(e^{\\prime})\\in\\mathbb{R}^{D}$ . For $e=(u,v)\\in G$ , we compute $\\pmb{r}(e)$ as: ", "page_idx": 6}, {"type": "text", "text": "We can compute ${\\pmb r}^{\\prime}(e)$ in similar manner. The property $\\pmb{r}((u,v))=\\pmb{r}((v,u))$ reflects the undirected property of graph. Finally, the vectors $\\pmb{r}(e)$ and $\\pmb{r}^{\\prime}(e^{\\prime})$ are stacked into matrices $\\boldsymbol{R}$ and $R^{\\prime}$ , both with dimensions $\\mathbb{R}^{\\left(N\\right)\\times D}$ . We would like to highlight that $\\pmb{r}((u,v))$ or ${\\pmb r}^{\\prime}((u^{\\prime},v^{\\prime}))$ are computed only once for all node-pairs, after the MPNN completes its final $K$ th layer of execution. The message passing in the MPNN occurs only over edges. Therefore, this approach does not significantly increase the time complexity. ", "page_idx": 6}, {"type": "text", "text": "Neural architecture of $\\mathbf{PERMNET}_{\\phi}$ The network $\\mathrm{PERMNET}_{\\phi}$ provides $_{P}$ as a soft node alignment matrix by taking the node embeddings as input, i.e., $P\\,=\\,\\dot{\\mathrm{PERMNET}}_{\\phi}(X,X^{\\prime})$ . $\\mathbf{PERMNET}_{\\phi}$ is implemented in two steps. In the first step, we apply a neural network $c_{\\phi}$ on both $\\pmb{x}_{K}$ and $\\mathbf{\\Delta}x_{K}^{\\prime}$ , and then compute the normed difference between their outputs to construct the matrix $_{C}$ , where $C[u,u^{\\prime}]=\\|\\dot{c_{\\phi}}\\left({\\pmb x}_{K}(u)\\right)-c_{\\phi}\\left({\\pmb x}_{K}^{\\prime}(u^{\\prime})\\right)\\|_{1}$ . Next, we apply iterative Sinkhorn normalizations [16, ", "page_idx": 6}, {"type": "text", "text": "35] on $\\exp(-C/\\tau)$ , to obtain a soft node alignment $_{P}$ . Therefore, ", "page_idx": 7}, {"type": "equation", "text": "$$\nP=\\mathrm{Sinkhorn}\\left(\\left[\\exp\\left(-\\left\\|c_{\\phi}\\left(\\pmb{x}_{K}(u)\\right)-c_{\\phi}\\left(\\pmb{x}_{K}^{\\prime}(u^{\\prime})\\right)\\right\\|_{1}/\\tau\\right)\\right]_{(u,u^{\\prime})\\in[N]\\times[N]}\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Here, $\\tau$ is a temperature hyperparameter. In a general cost setting, GED is typically asymmetric, so it may be desirable for $C[u,u^{\\prime}]$ to be asymmetric with respect to $\\textbf{\\em x}$ and $\\mathbf{\\nabla}x^{\\prime}$ . However, as noted in Proposition 1, when we compute $\\mathrm{GED}(G^{\\prime},G)$ , the alignment matrix $P^{\\prime}=\\mathrm{PERMNET}_{\\phi}(X^{\\prime},X)$ should satisfy the condition that $P^{\\prime}=P^{\\top}$ , where $_{P}$ is computed from Eq. (16). The current form of $_{C}$ supports this condition, whereas an asymmetric form might not, as shown in Appendix B. ", "page_idx": 7}, {"type": "text", "text": "We construct $\\pmb{S}\\in\\mathbb{R}^{(\\frac{N}{2})}\\times\\mathbb{R}^{(\\frac{N}{2})}$ as follows. Each pair of nodes $(u,v)$ in $G$ and $(u^{\\prime},v^{\\prime})$ in $G^{\\prime}$ can be mapped in two ways, regardless of whether they are edges or non-edges: (1) node $u\\mapsto u^{\\prime}$ and $v\\mapsto v^{\\prime}$ which is denoted by $P[u,u^{\\prime}]P[v,v^{\\prime}]$ ; (2) node $u\\mapsto v^{\\prime}$ and $v\\mapsto u^{\\prime}$ , which is denoted by $P[u,v^{\\prime}]P[v,u^{\\prime}]$ Combining these two scenarios, we compute the node-pair alignment matrix $\\boldsymbol{S}$ as: $S[(u,v),(\\bar{u}^{\\prime},v^{\\prime})]\\,=\\,P[u,u^{\\prime}]P[v,v^{\\prime}]+P[u,v^{\\prime}]P[v,u^{\\prime}]$ . This explicit formulation of $\\boldsymbol{S}$ from $_{P}$ ensures mutually consistent permutation across nodes and node-pairs. ", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We conduct extensive experiments on GRAPHEDX to showcase the effectiveness of our method across several real-world datasets, under both uniform and non-uniform cost settings for GED. Additiional experimental results can be found in Appendix D. ", "page_idx": 7}, {"type": "text", "text": "5.1 Setup", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Datasets We experiment with seven real-world datasets: Mutagenicity (Mutag) [18], Ogbg-Code2 (Code2) [23], Ogbg-Molhiv (Molhiv) [23], Ogbg-Molpcba (Molpcba) [23], AIDS [36], Linux [5] and Yeast [36]. For each dataset\u2019s training, test and validation sets $\\mathcal{D}_{\\mathrm{split}}$ , we generate |Dsplit| + |Dsplit| graph pairs, considering combinations between every two graphs, including self-pairing. We calculate the exact ground truth GED using the F2 solver [29], implemented within GEDLIB [10]. For GED with uniform cost setting, we set the cost values to $b^{\\ominus^{\\ast}}\\!=b^{\\oplus}\\,=\\,a^{\\ominus}\\,=\\,a^{\\oplus}\\,=\\,1$ . For GED with non-uniform cost setting, we use $b^{\\ominus}\\,=\\,3,b^{\\oplus}\\,=\\,1,a^{\\ominus}\\,=\\,2,a^{\\oplus}\\,=\\,1$ . Further details on dataset generation and statistics are presented in Appendix C. In the main paper, we present results for the first five datasets under both uniform and non-uniform cost settings for GED. Additional experiments for Linux and Yeast, as well as GED with node label substitutions, are presented in Appendix D. ", "page_idx": 7}, {"type": "text", "text": "Baselines We compare our approach with nine state-of-the-art methods. These include two variants of GMN [31]: (1) GMN-Match and (2) GMN-Embed; (3) ISONET [44], (4) GREED [40], (5) ERIC [56], (6) SimGNN [5], (7) H2MN [54], (8) GraphSim [6] and (9) EGSC [39]. To compute the GED, GMN-Match, GMN-Embed, and GREED use the Euclidean distance between the vector representation of two graphs. ISONET uses an asymmetric distance specifically tailored to subgraph isomorphism. H2MN is an early interaction network that utilizes higher-order node similarity through hypergraphs. ERIC, SimGNN, and EGSC leverage neural networks to calculate the distance between two graphs. Furthermore, the last three methods predict a score based on the normalized GED in the form of e $\\cdot\\mathrm{xp}\\left(-2\\mathrm{GED}(G,G^{\\prime})/(|V|+|V^{\\prime}|)\\right)$ . Notably, none of these baseline approaches have been designed to incorporate non-uniform edit costs into their models. To address this limitation, when working with GED under non-uniform cost setting, we include the edit costs as initial features in the graphs for all baseline models. In Appendix D.3, we compare the performance of baselines without cost features. ", "page_idx": 7}, {"type": "text", "text": "Evaluation Given a dataset ${\\mathcal{D}}=\\{(G_{i},G_{i}^{\\prime},\\mathrm{GED}(G_{i},G_{i}^{\\prime}))\\}_{i\\in[n]}$ , we divide it into training, validation and test folds with a split ratio of 60:20:20. We train the models using the Mean Squared Error (MSE) between the predicted GED and the ground truth GED as the loss. For model evaluation, we calculate the Mean Squared Error (MSE) between the actual and predicted GED on the test set. For ERIC, SimGNN and EGSC, we rescale the predicted score to obtain the true (unscaled) GED as $\\mathrm{GED}(G,G^{\\prime})=-(|V|+|V|^{\\prime})\\log(s)/2$ . In Appendix D, we also report Kendall\u2019s Tau (KTau) to evaluate the rank correlation across different experiments. ", "page_idx": 7}, {"type": "text", "text": "5.2 Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Selection of $\\Delta^{\\bullet}(X,X^{\\prime}\\,|\\,P)$ and $\\Delta^{\\bullet}(R,R^{\\prime}\\,|\\,S)$ We start by comparing the performance of the nine different combinations (three for edge edits, and three for node edits) of our neural distance surrogates from the cartesian space of Edge-{ALIGNDIFF, DIFFALIGN, XOR-DIFFALIGN} $\\downarrow\\times$ NodeALIGNDIFF, DIFFALIGN, XOR-DIFFALIGN . Table 2 summarizes the results. We make the following observations. (1) The best combinations share the XOR-DIFFALIGN on the edge edit formulation, because, XOR-DIFFALIGN offers more inductive bias, by zeroing the edit cost of aligning an edge to edge and a non-edge to non-edge, as we discussed in Section 4.2. Consequently, one can limit the cartesian space to only three surrogates for node edits, while using XOR-DIFFALIGN as the fixed surrogate for edge edits. (2) There is no clear winner between DIFFALIGN and ALIGNDIFF. GRAPHEDX is chosen from the model which has the lowest validation error, and the numbers in Table 2 are on the test set. Hence, in datasets such as AIDS under uniform cost, or Molhiv under non-uniform cost, the model chosen for GRAPHEDX doesn\u2019t have the best test set performance. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "table", "img_path": "u7JRmrGutT/tmp/20675045e87e578b346622e0ffd9518aa9e21fc25aced39f5967e0b7c5d53739.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 2: Prediction error measured in terms of MSE of the nine combinations of our neural set distance surrogate across five datasets on test set, for GED with uniform costs and non-uniform costs. For GED with uniform (non-uniform) costs we have $b^{\\ominus}=b^{\\oplus}=a^{\\ominus}=a^{\\oplus}=1$ $(b^{\\ominus}\\,=\\,3,b^{\\oplus}\\,=$ $1,a^{\\ominus}=2,a^{\\oplus}=1.$ ) The GRAPHEDX model was selected based on the lowest MSE on the validation set, and we report the results of the MSE on the test set. Green ( yellow) numbers report the best (second best) performers. ", "page_idx": 8}, {"type": "text", "text": "Comparison with baselines We compare the performance of GRAPHEDX against all state-of-the-art baselines for GED with both uniform and non-uniform costs. Table 3 summarizes the results. We make the following observations. (1) GRAPHEDX outperforms all the baselines by a significant margin. For GED with uniform costs, this margin often goes as high as $15\\%$ . This advantage becomes even more pronounced for GED with non-uniform costs, where our method outperforms the baselines by a margin as high as $30\\%$ , as seen in Code2. (2) There is no clear second-best method. Among the baselines, EGSC and ERIC each outperforms the others in two out of five datasets for both uniform and non-uniform cost settings. Also, EGSC demonstrates competitive performance in AIDS. ", "page_idx": 8}, {"type": "table", "img_path": "u7JRmrGutT/tmp/d2dda2f296fe807dcac1d2fae1b312f8c0ff7dabe5819fa360f0cec83468c069.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 3: Prediction error measured in terms of MSE of GRAPHEDX and all the state-of-theart baselines across five datasets on test set, for GED with uniform costs and non-uniform costs. For GED with uniform (non-unfiform) costs we have $b^{\\ominus}\\ =\\ b^{\\oplus}\\ =\\ a^{\\ominus}\\ =\\ a^{\\oplus}\\ =\\ 1$ $(b^{\\ominus}\\,=\\,3,b^{\\oplus}\\,=\\,1,a^{\\ominus}\\,=\\,2,a^{\\oplus}\\,=\\,1.$ ) GRAPHEDX represents the best model based on the validation set from the cartesian space of Edge-{ALIGNDIFF, DIFFALIGN, XOR-DIFFALIG $\\mathrm{N}\\}\\times\\mathrm{Node}.$ {ALIGNDIFF, DIFFALIGN, XOR-DIFFALIGN}. Green ( yellow) numbers report the best (second best) performers. ", "page_idx": 8}, {"type": "text", "text": "Impact of cost-guided GED Among the baselines, GMN-Match, GMN-Embed and GREED compute GED using the euclidean distance between the graph embeddings, i.e., $\\mathrm{GED}(G,G^{\\prime})=$ $\\|\\pmb{x}_{G}-\\pmb{x}_{G^{\\prime}}\\|_{2}$ , whereas we compute it by summing the set distance surrogates between the node and edge embedding sets. To understand the impact of our cost guided distance, we adapt it to the graph-level embeddings used by the above three baselines as follows: $\\mathrm{GED}(G,G^{\\prime})\\;=$ $\\begin{array}{r}{\\frac{b^{\\ominus}+a^{\\ominus}}{2}\\left\\|\\mathrm{ReLU}\\left(\\pmb{x}_{G}-\\pmb{x}_{G^{\\prime}}\\right)\\right\\|_{1}+\\frac{b^{\\oplus}+a^{\\oplus}}{2}\\left\\|\\mathrm{ReLU}\\left(\\pmb{x}_{G^{\\prime}}-\\pmb{x}_{G}\\right)\\right\\|_{1}}\\end{array}$ . Table 4 summarizes the results in ", "page_idx": 8}, {"type": "table", "img_path": "u7JRmrGutT/tmp/ef2d285a6f66adb576f5300bc62b12c06da189b14e4adc8cdfaf1db34f97dec8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Table 4: Impact of cost guided distance in terms of MSE; \\* represents the variant of the baseline with cost-guided distance. Green (bold) shows the best among all methods (only baselines). ", "page_idx": 9}, {"type": "table", "img_path": "u7JRmrGutT/tmp/5443cc565af9da4d9f0590db17c9f400cccf345da05f7525d2de29dbe5aaf03f.jpg", "table_caption": [], "table_footnote": ["Table 5: MSE for different methods with unit node substitution cost in uniform cost setting. Green (yellow) show (second) best method. "], "page_idx": 9}, {"type": "text", "text": "terms of MSE, which shows that (1) our set-divergence-based cost guided distance reduces the MSE by a significant margin in most cases (2) the margin of improvement is more prominent with GED involving non-uniform costs, where the modeling of specific cost values is crucial (3) GRAPHEDX outperforms the baselines even after changing their default distance to our cost guided distance. ", "page_idx": 9}, {"type": "text", "text": "Performance for GED under node substitution cost The scoring function in Eq. 8 can also be extended to incorporate node label substitution cost, which has been described in Appendix B. Here, we compare the performance of our model with the baselines in terms of MSE where we include node substitution cost $b^{\\sim}$ , with cost setting as $b^{\\ominus}=b^{\\oplus}=b^{\\sim}=a^{\\ominus}=a^{\\oplus}=1$ . In Table 5, we report the results across 5 datasets equipped with node labels, passed as one-hot encoded node features. We observe that (1) our model outperforms all other baselines across all datasets by significant margin; (2) there is no clear second winner but ERIC, EGSC and ISONET performs better than the others. ", "page_idx": 9}, {"type": "text", "text": "Benefits of using all node-pairs representati (i) Edge-only (edge $\\rightarrow$ edge): where we o resulting in $\\boldsymbol{S}$ being an edge-alignment matrix, and $R,R^{\\prime}\\in\\ \\mathbb{R}^{\\operatorname*{max}(|E|,|E^{\\prime}|)\\times D}$ (ii) Edge-only (pair $\\rightarrow$ pair): In this variant, the embeddings of the nonedges in $R,R^{\\prime}\\in\\mathbb{R}^{N(N-1)/2\\times D}$ are explicitly set to zero. in terms of MSE, which show that (1) both these sparse representations perform significantly worse compared to our method using non-trivial representa", "page_idx": 9}, {"type": "table", "img_path": "u7JRmrGutT/tmp/5bc0d129739f263cb8142418678374e0b89aa9a23df65f4141ff09b290908690.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Table 6: Comparison of variants of edge representation under uniform cost setting. Green (yellow) numbers report the best (second best) performers. ", "page_idx": 9}, {"type": "text", "text": "tions for both edges and non-edges, and (2) Edge-only (edge $\\rightarrow$ edge) performs better than Edge-only $\\mathrm{\\pair}\\to\\mathrm{pair})$ ). This underscores the importance of explicitly modeling trainable non-edge embeddings to capture the sensitivity of GED to global graph structure. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our work introduces a novel neural model for computing GED that explicitly incorporates general costs of edit operations. By leveraging graph representations that recognize both edges and non-edges, together with the design of suitable set distance surrogates, we achieve a more robust neural surrogate for GED. Our experiments demonstrate that this approach outperforms state-of-the-art methods, especially in settings with general edit costs, providing a flexible and effective solution for a range of applications. Future work could focus on extending the GED formulation to richly-attributed graphs by modeling the structure of edit operations and the similarity of all node-pair features. ", "page_idx": 9}, {"type": "text", "text": "Limitations Our neural model for GED showcases significant improvements in accuracy and flexibility for modeling edit costs. However, there are some limitations to consider. (1) While computing graph representations over $\\binom{N}{2}\\times\\binom{N}{2}$ node-pairs does not require additional parameters due to parameter-sharing, it does demand significant memory resources. This could pose challenges, especially with larger-sized graphs. (2) The assumption of fixed edit costs across all graph pairs within a dataset might not reflect real-world scenarios where costs vary based on domain-specific factors and subjective human relevance judgements. This calls for more specialized approaches to accurately model the impact of each edit operation, which may differ across node pairs. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements Indradyumna acknowledges Qualcomm Innovation Fellowship, Abir and Soumen acknowledge grants from Amazon, Google, IBM and SERB. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] J. Adler and S. Lunz. Banach wasserstein gan. Advances in neural information processing systems, 31, 2018.   \n[2] B. Amos, L. Xu, and J. Z. Kolter. Input convex neural networks. In International Conference on Machine Learning, pages 146\u2013155. PMLR, 2017.   \n[3] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein generative adversarial networks. In International conference on machine learning, pages 214\u2013223. PMLR, 2017.   \n[4] A. Backurs, Y. Dong, P. Indyk, I. Razenshteyn, and T. Wagner. Scalable nearest neighbor search for optimal transport. In International Conference on machine learning, pages 497\u2013506. PMLR, 2020.   \n[5] Y. Bai, H. Ding, S. Bian, T. Chen, Y. Sun, and W. Wang. Simgnn: A neural network approach to fast graph similarity computation. In Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining, pages 384\u2013392, 2019.   \n[6] Y. Bai, H. Ding, K. Gu, Y. Sun, and W. Wang. Learning-based efficient graph similarity computation via multi-scale convolutional set matching. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 3219\u20133226, 2020.   \n[7] D. B. Blumenthal. New techniques for graph edit distance computation. arXiv preprint arXiv:1908.00265, 2019.   \n[8] D. B. Blumenthal and J. Gamper. Improved lower bounds for graph edit distance. IEEE Transactions on Knowledge and Data Engineering, 30:503\u2013516, 2018. URL https://api. semanticscholar.org/CorpusID:3438059.   \n[9] D. B. Blumenthal, E. Daller, S. Bougleux, L. Brun, and J. Gamper. Quasimetric graph edit distance as a compact quadratic assignment problem. In 2018 24th International Conference on Pattern Recognition (ICPR), pages 934\u2013939, 2018. doi: 10.1109/ICPR.2018.8546055.   \n[10] D. B. Blumenthal, S. Bougleux, J. Gamper, and L. Brun. Gedlib: A $\\mathrm{c}++$ library for graph edit distance computation. In D. Conte, J.-Y. Ramel, and P. Foggia, editors, Graph-Based Representations in Pattern Recognition, pages 14\u201324, Cham, 2019. Springer International Publishing. ISBN 978-3-030-20081-7.   \n[11] S. Bougleux, L. Brun, V. Carletti, P. Foggia, B. Ga\u00fcz\u00e8re, and M. Vento. Graph edit distance as a quadratic assignment problem. Pattern Recognition Letters, 87:38\u201346, 2017. ISSN 0167-8655. doi: https://doi.org/10.1016/j.patrec.2016.10.001. URL https://www.sciencedirect.com/ science/article/pii/S0167865516302665. Advances in Graph-based Pattern Recognition.   \n[12] A. Z. Broder, M. Charikar, A. M. Frieze, and M. Mitzenmacher. Min-wise independent permutations. In Proceedings of the thirtieth annual ACM symposium on Theory of computing, pages 327\u2013336, 1998.   \n[13] H. Bunke. On a relation between graph edit distance and maximum common subgraph. Pattern Recognition Letters, 18(8):689\u2013694, 1997.   \n[14] H. Bunke and G. Allermann. Inexact graph matching for structural pattern recognition. Pattern Recognition Letters, 1(4):245\u2013253, 1983.   \n[15] L. Chang, X. Feng, X. Lin, L. Qin, and W. Zhang. Efficient graph edit distance computation and verification via anchor-aware lower bound estimation. arXiv preprint arXiv:1709.06810, 2017.   \n[16] M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in neural information processing systems, 26:2292\u20132300, 2013.   \n[17] M. Daniels, T. Maunu, and P. Hand. Score-based generative neural networks for large-scale optimal transport. Advances in neural information processing systems, 34:12955\u201312965, 2021.   \n[18] A. K. Debnath, R. L. Lopez de Compadre, G. Debnath, A. J. Shusterman, and C. Hansch. Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity. Journal of Medicinal Chemistry, 34(2):786\u2013797, 1991. doi: 10.1021/jm00106a046. URL https://doi.org/10.1021/ jm00106a046.   \n[19] K. D. Doan, S. Manchanda, S. Mahapatra, and C. K. Reddy. Interpretable graph similarity computation via differentiable optimal alignment of node embeddings. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 665\u2013674, 2021.   \n[20] M. Douze, A. Guzhva, C. Deng, J. Johnson, G. Szilvasy, P.-E. Mazar\u00e9, M. Lomeli, L. Hosseini, and H. J\u00e9gou. The faiss library. arXiv preprint arXiv:2401.08281, 2024.   \n[21] C. Garcia-Hernandez, A. Fernandez, and F. Serratosa. Ligand-based virtual screening using graph edit distance as molecular similarity measure. Journal of chemical information and modeling, 59(4):1410\u20131421, 2019.   \n[22] A. Genevay, M. Cuturi, G. Peyr\u00e9, and F. Bach. Stochastic optimization for large-scale optimal transport. Advances in neural information processing systems, 29, 2016.   \n[23] W. Hu, M. Fey, M. Zitnik, Y. Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec. Open graph benchmark: Datasets for machine learning on graphs. Advances in neural information processing systems, 33:22118\u201322133, 2020.   \n[24] P. Indyk and R. Motwani. Approximate nearest neighbors: towards removing the curse of dimensionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing, pages 604\u2013613, 1998.   \n[25] P. Indyk, R. Motwani, P. Raghavan, and S. Vempala. Locality-preserving hashing in multidimensional spaces. In Proceedings of the twenty-ninth annual ACM symposium on Theory of computing, pages 618\u2013625, 1997.   \n[26] S. Ivanov, S. Sviridov, and E. Burnaev. Understanding isomorphism bias in graph data sets. arXiv 1910.12091, 2019. URL https://arxiv.org/abs/1910.12091.   \n[27] D. Justice and A. Hero. A binary linear programming formulation of the graph edit distance. IEEE Transactions on Pattern Analysis and Machine Intelligence, 28(8):1200\u20131214, 2006.   \n[28] M. G. Kendall. A new measure of rank correlation. Biometrika, 30(1/2):81\u201393, 1938.   \n[29] J. Lerouge, Z. Abu-Aisheh, R. Raveaux, P. H\u00e9roux, and S. Adam. New binary linear programming formulation to compute the graph edit distance. Pattern Recognition, 72:254\u2013 265, 2017. ISSN 0031-3203. doi: https://doi.org/10.1016/j.patcog.2017.07.029. URL https://www.sciencedirect.com/science/article/pii/S003132031730300X.   \n[30] Y. Li, D. Tarlow, M. Brockschmidt, and R. Zemel. Gated graph sequence neural networks. arXiv preprint arXiv:1511.05493, 2015.   \n[31] Y. Li, C. Gu, T. Dullien, O. Vinyals, and P. Kohli. Graph matching networks for learning the similarity of graph structured objects. In International conference on machine learning, pages 3835\u20133845. PMLR, 2019.   \n[32] C.-L. Lin. Hardness of approximating graph transformation problem. In D.-Z. Du and X.-S. Zhang, editors, Algorithms and Computation, pages 74\u201382, Berlin, Heidelberg, 1994. Springer Berlin Heidelberg. ISBN 978-3-540-48653-4.   \n[33] Z. Lou, J. You, C. Wen, A. Canedo, J. Leskovec, et al. Neural subgraph matching. arXiv preprint arXiv:2007.03092, 2020.   \n[34] Y. A. Malkov and D. A. Yashunin. Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE transactions on pattern analysis and machine intelligence, 42(4):824\u2013836, 2018.   \n[35] G. Mena, D. Belanger, S. Linderman, and J. Snoek. Learning latent permutations with gumbelsinkhorn networks. arXiv preprint arXiv:1802.08665, 2018. URL https://arxiv.org/pdf/ 1802.08665.pdf.   \n[36] C. Morris, N. M. Kriege, F. Bause, K. Kersting, P. Mutzel, and M. Neumann. Tudataset: A collection of benchmark datasets for learning with graphs, 2020.   \n[37] B. Naidan, L. Boytsov, Y. Malkov, and D. Novak. Non-metric space library manual. arXiv preprint arXiv:1508.05470, 2015.   \n[38] E. Ozdemir and C. Gunduz-Demir. A hybrid classification model for digital pathology using structural and statistical pattern recognition. IEEE Transactions on Medical Imaging, 32(2): 474\u2013483, 2013. doi: 10.1109/TMI.2012.2230186.   \n[39] C. Qin, H. Zhao, L. Wang, H. Wang, Y. Zhang, and Y. Fu. Slow learning and fast inference: Efficient graph similarity computation via knowledge distillation. In Thirty-Fifth Conference on Neural Information Processing Systems, 2021.   \n[40] R. Ranjan, S. Grover, S. Medya, V. Chakaravarthy, Y. Sabharwal, and S. Ranu. Greed: A neural framework for learning graph distance functions. Advances in Neural Information Processing Systems, 35:22518\u201322530, 2022.   \n[41] K. Riesen and H. Bunke. Approximate graph edit distance computation by means of bipartite graph matching. Image and Vision Computing, 27(7):950\u2013959, 2009. ISSN 0262-8856. doi: https://doi.org/10.1016/j.imavis.2008.04.004. URL https://www.sciencedirect.com/ science/article/pii/S026288560800084X. 7th IAPR-TC15 Workshop on Graph-based Representations (GbR 2007).   \n[42] K. Riesen, A. Fischer, and H. Bunke. Combining bipartite graph matching and beam search for graph edit distance approximation. In Artificial Neural Networks in Pattern Recognition: 6th IAPR TC 3 International Workshop, ANNPR 2014, Montreal, QC, Canada, October 6-8, 2014. Proceedings 6, pages 117\u2013128. Springer, 2014.   \n[43] I. Roy, S. Chakrabarti, and A. De. Maximum common subgraph guided graph retrieval: late and early interaction networks. Advances in Neural Information Processing Systems, 35: 32112\u201332126, 2022.   \n[44] I. Roy, V. S. Velugoti, S. Chakrabarti, and A. De. Interpretable Neural Subgraph Matching for Graph Retrieval. AAAI, 2022.   \n[45] S. Sahni and T. Gonzalez. P-complete approximation problems. J. ACM, 23(3):555\u2013565, jul 1976. ISSN 0004-5411. doi: 10.1145/321958.321975. URL https://doi.org/10.1145/ 321958.321975.   \n[46] A. Sanfeliu and K.-S. Fu. A distance measure between attributed relational graphs for pattern recognition. IEEE transactions on systems, man, and cybernetics, pages 353\u2013362, 1983.   \n[47] A. Sanfeliu, R. Alqu\u00e9zar, J. Andrade, J. Climent, F. Serratosa, and J. Verg\u00e9s. Graph-based representations and techniques for image processing and image analysis. Pattern recognition, 35(3):639\u2013650, 2002.   \n[48] V. Seguy, B. B. Damodaran, R. Flamary, N. Courty, A. Rolet, and M. Blondel. Large-scale optimal transport and mapping estimation. arXiv preprint arXiv:1711.02283, 2017.   \n[49] K. Shearer, H. Bunke, and S. Venkatesh. Video indexing and similarity retrieval by largest common subgraph detection using decision trees. Pattern Recognition, 34(5):1075\u20131091, 2001.   \n[50] R. Sinkhorn and P. Knopp. Concerning nonnegative matrices and doubly stochastic matrices. Pacific Journal of Mathematics, 21(2):343\u2013348, 1967.   \n[51] S. Tirthapura, D. Sharvit, P. Klein, and B. B. Kimia. Indexing based on edit-distance matching of shape graphs. In Multimedia storage and archiving systems III, volume 3527, pages 25\u201336. SPIE, 1998.   \n[52] Y. Xie, M. Chen, H. Jiang, T. Zhao, and H. Zha. On scalable and efficient computation of large scale optimal transport. In International Conference on Machine Learning, pages 6882\u20136892. PMLR, 2019.   \n[53] Z. Zeng, A. K. Tung, J. Wang, J. Feng, and L. Zhou. Comparing stars: On approximating graph edit distance. Proceedings of the VLDB Endowment, 2(1):25\u201336, 2009.   \n[54] Z. Zhang, J. Bu, M. Ester, Z. Li, C. Yao, Z. Yu, and C. Wang. H2mn: Graph similarity learning with hierarchical hypergraph matching networks. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, KDD \u201921, page 2274\u20132284, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450383325. doi: 10.1145/3447548.3467328. URL https://doi.org/10.1145/3447548.3467328.   \n[55] W. Zheng, L. Zou, X. Lian, D. Wang, and D. Zhao. Efficient graph similarity search over large graph databases. IEEE Transactions on Knowledge and Data Engineering, 27(4):964\u2013978, 2014.   \n[56] W. Zhuo and G. Tan. Efficient graph similarity computation with alignment regularization. Advances in Neural Information Processing Systems, 35:30181\u201330193, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Graph Edit Distance with General Costs Using Neural Set Divergence (Appendix) ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Broader impact 16 ", "page_idx": 14}, {"type": "text", "text": "B Discussion on our proposed formulation of GED 17 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Modification of scoring function from label substitution 17   \nB.2 Proof of Proposition 1 . . 17   \nB.3 Connections with other notions of graph matching 18   \nB.4 Relation between ALIGNDIFF and DIFFALIGN 18   \nB.5 Proof that our design ensures conditions of Proposition 1 19   \nB.6 Alternative surrogate for GED 19 ", "page_idx": 14}, {"type": "text", "text": "C Details about experimental setup 21 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "C.1 Generation of datasets 21   \nC.2 Details about state-of-the-art baselines 21   \nC.3 Details about GRAPHEDX 23   \nC.4 Evaluation metrics 23   \nC.5 Hardware and license 24   \nAdditional experiments 25   \nD.1 Comparison of performance of GRAPHEDX on non-uniform cost Edge-GED 25   \nD.2 Comparison of GRAPHEDX with baselines on uniform and non-uniform cost setting 26   \nD.3 Comparison of GRAPHEDX with baselines with and without cost features 27   \nD.4 Comparison of GRAPHEDX with baselines with node substitution cost 28   \nD.5 Performance evaluation for edge-only vs. all-node-pair representations 28   \nD.6 Effect of using cost-guided scoring function on baselines 29   \nD.7 Results on performance of the alternate surrogates for GED 29   \nD.8 Comparison of zero-shot performance on other datasets 29   \nD.9 Importance of node-edge consistency 30   \nD.10 Comparison of nine possible combinations our proposed set distances 31   \nD.11 Comparison of performance of our model with baselines using scatter plot . 32   \nD.12 Comparison of performance of our model with baselines using error distribution . . 32   \nD.13 Comparison of combinatorial optimisation gadgets for GED prediction 33   \nD.14 Prediction timing analysis . . . 34   \nD.15 Visualization (optimal edit path) $^+$ Pseudocode 34 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Broader impact ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Graphs serve as powerful representations across diverse domains, capturing complex relationships and structural notions inherent in various systems. From biological networks to social networks, transportation networks, and supply chains, graphs provide a versatile framework for modeling interactions between interconnected entities. In domains where structure-similarity based applications are prevalent, GED emerges as a valuable and versatile tool. ", "page_idx": 15}, {"type": "text", "text": "For example, in bio-informatics, molecular structures can naturally be represented as graphs. GED computation expedites tasks such as drug discovery, protein-protein interaction modeling, and molecular similarity analysis by identifying structurally similar molecular compounds. Similarly, in social network analysis, GED can measure similarities between user interactions, aiding in friend recommendation systems or community detection tasks. In transportation networks, GEDbased tools assess similarity between road networks for route planning or traffic optimizations. Further applications include learning to edit scene graphs, analyzing gene regulatory pathways, fraud detection, and more ", "page_idx": 15}, {"type": "text", "text": "Moreover, our proposed variations of GED, particularly those amenable to hashing, find utility in retrieval based setups. In various information retrieval systems, hashed graph representations can be used to efficiently index and retrieve relevant items using our GED based scores. Such applications include image retrieval from image databases where images are represented as scene graphs, retrieval of relevant molecules from molecular databases, etc. ", "page_idx": 15}, {"type": "text", "text": "Furthermore, our ability to effectively model different edit costs in GED opens up new possibilities in various applications. In recommendation systems, it can model user preferences of varying importance, tailoring recommendations based on user-specific requirements or constraints. Similarly, in image or video processing, different types of distortions may have varying impacts on perceptual quality, and GED with adaptive costs can better assess similarity. In NLP tasks such as text similarity understanding and document clustering, assigning variable costs to textual edits corresponding to word insertion, deletions or substitutions, provides a more powerful framework for measuring textual similarity, improving performance in downstream tasks such as plagiarism detection, summarization, etc. ", "page_idx": 15}, {"type": "text", "text": "Lastly, and most importantly, the design of our model encourages interpretable alignment-driven justifications, thereby promoting transparency and reliability while minimizing potential risks and negative impacts, in high stake applications like drug discovery. ", "page_idx": 15}, {"type": "text", "text": "B Discussion on our proposed formulation of GED ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1 Modification of scoring function from label substitution ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "To incorporate the effect of node substitution into account when formulating the GED, we first observe that the effect of node substitution cost $b^{\\sim}$ only comes into account when a non-padded node maps to a non-padded node. In all other cases, when a node is deleted or inserted, we do not additionally incur any substitution costs. Note that, we consider the case when node substitution cannot be replaced by node addition and deletion, i.e., $b^{\\sim}\\leq b^{\\ominus}+b^{\\oplus}$ . Such a constraint on costs has uses in multiple applications [9, 38]. Let $\\mathcal{L}$ denote the set of node labels, and $\\ell(u)$ , $\\mathcal{E}^{\\prime}(u^{\\prime})\\in\\mathcal{L}$ denote the node label corresponding to nodes $u$ and $u^{\\prime}$ in $G$ and $G^{\\prime}$ respectively. We construct the node label matrix $\\textbf{\\emph{L}}$ for $G$ as follows: $L\\in\\{0,1\\}^{N\\times|\\mathcal{L}|}$ , such that $L[i,:]=\\mathsf{o n e\\_h o t}(\\ell(i))$ , i.e., $\\textbf{\\emph{L}}$ is the one-hot indicator matrix for the node labels, which each row corresponding to the one-hot vector of the label. Similarly, we can construct $L^{\\prime}$ for $G^{\\prime}$ . Then, the distance between labels of two nodes $u\\in V$ and $u^{\\prime}\\in V^{\\prime}$ can be given as $\\|L[u,:]-L^{\\prime}[u^{\\prime},:]\\|_{1}$ . To ensure that only valid node to node mappings contribute to the cost, we multiply the above with $\\Lambda(u,u^{\\prime})=\\mathrm{AND}(\\dot{\\eta}_{G}[u],\\eta_{G^{\\prime}}[u^{\\prime}])$ . This allows us to write the expression for GED with node label substitution cost as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{GED}(G,G^{\\prime})=\\underset{P\\in\\mathbb{P}_{N}}{\\mathrm{min}}\\ \\frac{a^{\\ominus}}{2}\\left\\|\\mathrm{ReLU}\\left(A-P A^{\\prime}P^{\\top}\\right)\\right\\|_{1,1}+\\frac{a^{\\oplus}}{2}\\left\\|\\mathrm{ReLU}\\left(P A^{\\prime}P^{\\top}-A\\right)\\right\\|_{1,1}}&{}\\\\ &{\\quad\\qquad\\qquad\\quad+\\,b^{\\ominus}\\left\\|\\mathrm{ReLU}\\left(\\eta_{G}-P\\eta_{G^{\\prime}}\\right)\\right\\|_{1}+b^{\\oplus}\\left\\|\\mathrm{ReLU}\\left(P\\eta_{G^{\\prime}}-\\eta_{G}\\right)\\right\\|_{1}}&{}\\\\ &{\\quad\\qquad\\qquad\\quad+\\,b^{\\sim}\\underset{\\underset{u,u^{\\prime}}{\\sideset{}{u,u^{\\prime}}}}{\\sum}\\Lambda(u,u^{\\prime})\\left\\|L[u,:]-L[u^{\\prime},:]\\right\\|_{1}P[u,u^{\\prime}]}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We can design a neural surrogate for above in the same way as done in Section 4.2, and write ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{GED}_{\\theta,\\phi}(G,G^{\\prime})=a^{\\ominus}\\Delta^{\\ominus}(R,R^{\\prime}\\!\\mid\\!S)+a^{\\oplus}\\Delta^{\\oplus}(R,R^{\\prime}\\!\\mid\\!S)}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,b^{\\ominus}\\Delta^{\\ominus}(X,X^{\\prime}\\!\\mid\\!P)+b^{\\oplus}\\Delta^{\\oplus}(X,X^{\\prime}\\!\\mid\\!P)}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,b^{\\sim}\\Delta^{\\sim}(L,L^{\\prime}\\vert P)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In this case, to account for node substitutions in the proposed permutation, we use $L[u,:]$ and $L^{\\prime}[u^{\\prime},:]$ as the features for node $u$ in $G$ and node $u^{\\prime}$ in $G^{\\prime}$ , respectively. We present the comparison of our method including subsitution cost with state-of-the-art baselines in Appendix D. ", "page_idx": 16}, {"type": "text", "text": "B.2 Proof of Proposition 1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proposition Given a fixed set of values of $b^{\\ominus},b^{\\oplus},a^{\\ominus},a^{\\oplus}$ , let $_{P}$ be an optimal node permutation matrix corresponding to $\\mathrm{GED}(G,G^{\\prime})$ , computed using Eq. (7). Then, $P^{\\prime}\\stackrel{\\bullet}{=}P^{\\top}$ is an optimal node permutation corresponding to GED $(G^{\\prime},G)$ . ", "page_idx": 16}, {"type": "text", "text": "Proof: Noticing that ReLU $(c-d)=\\operatorname*{max}(c,d)-d$ , we can write ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\|\\mathrm{ReLU}\\left(A-P A^{\\prime}P^{\\top}\\right)\\right\\|_{1,1}=\\left\\|\\operatorname*{max}(A,P A^{\\prime}P^{\\top})-P A^{\\prime}P^{\\top}\\right\\|_{1,1}}&{}\\\\ {=\\left\\|\\operatorname*{max}(A,P A^{\\prime}P^{\\top})\\right\\|_{1,1}-2|E^{\\prime}|}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The last equality follows since $\\operatorname*{max}(A,P A^{\\prime}P^{\\top})\\geq P A^{\\prime}P^{\\top}$ element-wise, and $\\left\\|P A^{\\prime}P^{\\top}\\right\\|_{1,1}=$ $\\|A^{\\prime}\\|_{1,1}=2|E^{\\prime}|$ . Similarly, we can rewrite $\\left\\|\\mathrm{ReLU}\\left(P A^{\\prime}P^{\\top}-A\\right)\\right\\|_{1,1},\\|\\mathrm{ReLU}\\left(\\eta_{G}-P\\eta_{G^{\\prime}}\\right)\\|_{1}$ , and $\\left\\|\\mathrm{ReLU}\\left(P\\eta_{G^{\\prime}}-\\eta_{G}\\right)\\right\\|_{1}$ , and finally rewrite Eq. (7) as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{GED}(G,G^{\\prime})=\\underset{P\\in\\mathbb{P}_{N}}{\\mathrm{min}}\\,\\frac{a^{\\oplus}+a^{\\ominus}}{2}\\left\\|\\operatorname*{max}(A,P A^{\\prime}P^{\\top})\\right\\|_{1,1}-a^{\\ominus}|E^{\\prime}|-a^{\\oplus}|E|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\frac{b^{\\oplus}+b^{\\ominus}}{2}\\left\\|\\operatorname*{max}(\\eta_{G},P\\eta_{G^{\\prime}})\\right\\|_{1}-b^{\\ominus}|V^{\\prime}|-b^{\\oplus}|V|}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{GED}(G^{\\prime},G)=\\underset{P\\in\\mathbb{P}_{N}}{\\mathrm{min}}\\,\\frac{a^{\\oplus}+a^{\\ominus}}{2}\\left\\|\\operatorname*{max}(A^{\\prime},P A P^{\\top})\\right\\|_{1,1}-a^{\\ominus}|E|-a^{\\oplus}|E^{\\prime}|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\frac{b^{\\oplus}+b^{\\ominus}}{2}\\left\\|\\operatorname*{max}(\\eta_{G^{\\prime}},P\\eta_{G}\\right)\\right\\|_{1}-b^{\\ominus}|V|-b^{\\oplus}|V^{\\prime}|}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We can rewrite the max term as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left|\\operatorname*{max}(A,P A^{\\prime}P^{\\top})\\right|\\right|_{1,1}=\\displaystyle\\sum_{u,v}\\operatorname*{max}(A,P A^{\\prime}P^{\\top})[u,v]}&{}\\\\ &{\\qquad\\qquad\\qquad=\\displaystyle\\sum_{u,v}\\operatorname*{max}(P P^{\\top}A P P^{\\top},P A^{\\prime}P^{\\top})[u,v]}\\\\ &{\\qquad\\qquad=\\displaystyle\\sum_{u,v}P\\operatorname*{max}(P^{\\top}A P,A^{\\prime})P^{\\top}[u,v]}\\\\ &{\\qquad\\qquad=\\displaystyle\\sum_{u,v}\\operatorname*{max}(P^{\\top}A P,A^{\\prime})[u,v]}\\\\ &{\\qquad=\\left\\|\\operatorname*{max}(P^{\\top}A P,A^{\\prime})\\right\\|_{1,1}=\\left\\|\\operatorname*{max}(A^{\\prime},P^{\\top}A P)\\right\\|_{1,1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Similarly we can re write $\\left\\Vert\\operatorname*{max}(\\eta_{G},P\\eta_{G^{\\prime}})\\right\\Vert_{1}$ as $\\left\\|\\operatorname*{max}(\\eta_{G^{\\prime}},P^{\\top}\\eta_{G})\\right\\|_{1}$ . Given a fixed set of cost function $b^{\\ominus},b^{\\oplus},a^{\\ominus},a^{\\oplus}$ , the terms containing $|E^{\\prime}|,|E|,|V^{\\prime}|,|V|$ are constant and do not affect choosing an optimal $_{P}$ . Let $C=-a^{\\Theta}|E^{\\prime}|-a^{\\oplus}|E|-b^{\\Theta}|V|-b^{\\oplus}|V^{\\prime}|$ , Using the above equations, we can write: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{a^{\\oplus}+a^{\\ominus}}{2}\\left\\|\\operatorname*{max}(A,P A^{\\prime}P^{\\top})\\right\\|_{1,1}+\\frac{b^{\\oplus}+b^{\\ominus}}{2}\\left\\|\\operatorname*{max}(\\eta_{G},P\\eta_{G^{\\prime}})\\right\\|_{1}}\\\\ &{\\displaystyle=\\frac{a^{\\oplus}+a^{\\ominus}}{2}\\left\\|\\operatorname*{max}(A^{\\prime},P^{\\top}A P)\\right\\|_{1,1}+\\frac{b^{\\oplus}+b^{\\ominus}}{2}\\left\\|\\operatorname*{max}(\\eta_{G^{\\prime}},P^{\\top}\\eta_{G})\\right\\|_{1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Let the first term be $\\rho(G,G^{\\prime}\\,|\\,P)$ . Then second term can be expressed as $\\rho(G^{\\prime},G\\,|\\,P^{\\intercal})$ and $\\rho(G,G^{\\prime}\\,|\\,P)=\\rho(G^{\\prime},G\\,|\\,P^{\\top})$ for all $P\\in\\mathbb{P}_{N}$ . If $_{P}$ is the optimal solution of $\\mathrm{min}_{P\\in\\mathbb{P}_{N}}\\,\\rho(G,G^{\\prime}\\,|\\,P)$ then, $\\rho(G^{\\prime},G\\,|\\,\\underline{{P}}^{\\top})\\;=\\;\\rho(G,G^{\\prime}\\,|\\,\\pmb{P})\\;\\leq\\;\\rho(G,G^{\\prime}\\,|\\,\\widetilde{\\pmb{P}}^{\\top})\\;=\\;\\rho(G^{\\prime},G\\,|\\,\\widetilde{\\pmb{P}})$ for any permutation $\\tilde{P}$ . Hence, $\\pmb{P}^{\\prime}=\\pmb{P}^{\\top}\\in\\mathbb{P}_{N}$ is one optimal permutation for $\\mathrm{GED}(G^{\\prime},G)$ . ", "page_idx": 17}, {"type": "text", "text": "B.3 Connections with other notions of graph matching ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Graph isomorphism: When we set all costs to zero, we can write that ${\\mathrm{GED}}(G,G^{\\prime})\\;\\;=$ $\\begin{array}{r}{\\operatorname*{min}_{P}0.5\\|A-P A^{\\prime}P^{\\top}\\|_{1,1}+\\|\\eta_{G}-P\\eta_{G^{\\prime}}\\|_{1}.}\\end{array}$ . In such a scenario, $\\mathrm{GED}(G,G^{\\prime})$ is symmetric, i.e., $\\operatorname{GED}(G^{\\prime},G)=\\operatorname{GED}(G,G^{\\prime})$ and it becomes zero only when $G$ and $G^{\\prime}$ are isomorphic. ", "page_idx": 17}, {"type": "text", "text": "Subgraph isomorphism: Assume $b^{\\ominus}\\;=\\;b^{\\oplus}\\;=\\;0$ . Then, if we set the cost of edge addition to be arbitrarily small as compared to the cost of edge deletion, i.e., $a^{\\oplus}\\ \\ll\\ a^{\\ominus}$ . This yields $\\begin{array}{r}{\\mathrm{GED}(G,G^{\\prime})\\stackrel{}{=}\\operatorname*{min}_{P}(b^{\\ominus}\\sum_{u,v}\\mathrm{ReLU}\\left(A-P A^{\\prime}P^{\\top}\\right\\}[u,v])}\\end{array}$ , which can be reduced to zero for some permutation $_{P}$ , $G\\subseteq G^{\\prime}$ . ", "page_idx": 17}, {"type": "text", "text": "Maximum common edge subgraph: From Appendix B.2, we can write that $\\mathrm{GED}(G,G^{\\prime})\\;=$ 1 $\\operatorname*{min}_{P}0.5(a^{\\oplus}+a^{\\ominus})\\left\\|\\operatorname*{max}(A,\\bar{P}A^{\\prime}P^{\\top})\\right\\|_{1,1}\\stackrel{\\cdot\\cdot}{+}0.5(b^{\\oplus}+b^{\\ominus})\\left\\|\\operatorname*{max}\\{\\eta_{G},P\\eta_{G^{\\prime}}\\}\\right\\|_{1}-a^{\\ominus}|\\bar{E^{\\prime}}|\\ -1$ $a^{\\oplus}|E|\\,-\\,b^{\\ominus}|V^{\\prime}|\\,-\\,b^{\\oplus}|V|$ . When $a^{\\circleddash}\\ =\\ a^{\\oplus}\\ =\\ 1$ and $b^{\\oplus}\\;=\\;b^{\\ominus}\\;=\\;0$ , then ${\\mathrm{GED}}(G,G^{\\prime})\\;=$ $\\begin{array}{r}{\\left\\|\\operatorname*{max}(A,P A^{\\prime}P^{\\top})\\right\\|_{1,1}=\\left|E\\right|+\\left|E^{\\prime}\\right|-\\left\\|\\operatorname*{min}(A,P A^{\\prime}P^{\\top})\\right\\|_{1,1}}\\end{array}$ . Here, $\\operatorname*{min}(A,P A^{\\prime}P^{\\top})$ characterizes maximum common edge subgraph and $\\left\\|\\operatorname*{min}(A,P A^{\\prime}P^{\\top})\\right\\|_{1,1}$ provides the number of edges of it. ", "page_idx": 17}, {"type": "text", "text": "B.4 Relation between ALIGNDIFF and DIFFALIGN ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Lemma 2 Let $Z,Z^{\\prime}\\in\\mathbb{R}^{N\\times M}$ , and $\\boldsymbol{S}\\in\\mathbb{R}_{\\ge0}^{N\\times N}$ be double stochastic. Then, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\Vert\\mathrm{ReLU}\\left(\\boldsymbol{Z}-\\boldsymbol{S}\\boldsymbol{Z}^{\\prime}\\right)\\Vert_{1,1}\\leq\\sum_{i,j}\\Vert\\mathrm{ReLU}\\left(\\boldsymbol{Z}[i,:]-\\boldsymbol{Z}^{\\prime}[j,:]\\right)\\Vert_{1}\\,\\boldsymbol{S}[i,j]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof: We can write, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\|\\mathrm{ReLU}\\left(Z-S Z^{\\prime}\\right)\\right\\|_{1,1}=\\displaystyle\\sum_{i,j}\\left|\\mathrm{ReLU}\\left(Z[i,j]-\\sum_{k}S[i,k]Z^{\\prime}[k,j]\\right)\\right|}&{}\\\\ {\\overset{(*)}{=}\\displaystyle\\sum_{i,j}\\mathrm{ReLU}\\left(\\sum_{k}S[i,k]Z[i,j]-S[i,k]Z^{\\prime}[k,j]\\right)}&{}\\\\ {\\overset{(**)}{\\leq}\\displaystyle\\sum_{i,j}\\sum_{k}S[i,k]\\mathrm{ReLU}\\left(Z[i,j]-Z^{\\prime}[k,j]\\right)}&{}\\\\ {=\\displaystyle\\sum_{i,k}\\|\\mathrm{ReLU}\\left(Z[i,:]-Z^{\\prime}[k,:]\\right)\\|_{1}S[i,k]}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $(*)$ follows since $\\begin{array}{r}{\\sum_{k}S[i,k]=1\\,\\forall i\\in[N]}\\end{array}$ , and $(**)$ follows due to convexity of ReLU (). Now, notice that when $S\\in\\mathbb{P}_{N}$ , then $S[i,:]$ is $^{1}$ at one element while 0 at the rest. In that case, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{i,j}^{\\infty}\\mathrm{ReLU}\\left(\\sum_{k}S[i,k]Z[i,j]-S[i,k]Z^{\\prime}[k,j]\\right)=\\sum_{i,j}\\mathrm{ReLU}\\left(Z[i,j]-Z^{\\prime}[k_{i}^{*},j]\\right)}}\\\\ &{}&{=\\sum_{i,j}\\sum_{k}S[i,k]\\mathrm{ReLU}\\left(Z[i,j]-Z^{\\prime}[k,j]\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $k_{i}^{*}$ is the index where $S[i,:]$ is 1. Hence, we have an equality when $\\boldsymbol{S}$ is a hard permutation. Replacing $(Z,Z^{\\prime})$ with $(R,R^{\\prime})$ and $(X,X^{\\prime})$ , we get that ALIGNDIFF and DIFFALIGN are equivalent when $\\boldsymbol{S}$ is a hard permutation matrix, and moreover DIFFALIGN is an upper bound on ALIGNDIFF when $\\boldsymbol{S}$ is a soft permutation matrix. ", "page_idx": 18}, {"type": "text", "text": "B.5 Proof that our design ensures conditions of Proposition 1 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Here we show why it is necessary to have a symmetric form for $C[u,u^{\\prime}]$ in $\\mathrm{PERMNET}_{\\phi}$ . For $\\mathrm{GED}(G,G^{\\prime})$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\nC[u,v]=\\|c_{\\phi}\\left({\\pmb x}_{K}(u)\\right)-c_{\\phi}\\left({\\pmb x}_{K}^{\\prime}(v)\\right)\\|_{1}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\nC^{\\prime}[v,u]=\\|c_{\\phi}\\left(\\pmb{x}_{K}^{\\prime}(v)\\right)-c_{\\phi}\\left(\\pmb{x}_{K}(u)\\right)\\|_{1}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Because the Sinkhorn cost $C[u,v]$ is symmetric, using the above equations we can infer, ", "page_idx": 18}, {"type": "text", "text": "This leads to $\\begin{array}{r l r}{P^{\\prime}}&{{}=}&{P^{\\intercal}}\\end{array}$ . If we use an asymmetric Sinkhorn cost $\\begin{array}{r l}{(C[u,v]}&{{}=}\\end{array}$ $\\left\\|\\mathrm{ReLU}\\left(c_{\\phi}\\left(\\pmb{x}_{K}(u)\\right)-c_{\\phi}\\left(\\pmb{x}_{K}^{\\prime}(v)\\right)\\right)\\right\\|_{1})$ , we cannot ensure $C[u,v]\\,=\\,C^{\\prime}[v,u]$ , which fails to satisfy $P=P^{\\top}$ . ", "page_idx": 18}, {"type": "text", "text": "B.6 Alternative surrogate for GED ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "From Appendix B.2, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{GED}(G,G^{\\prime})=\\underset{P\\in\\mathbb{P}_{N}}{\\mathrm{min}}\\,\\frac{a^{\\oplus}+a^{\\ominus}}{2}\\left\\|\\operatorname*{max}(A,P A^{\\prime}P^{\\top})\\right\\|_{1,1}-a^{\\ominus}|E^{\\prime}|-a^{\\oplus}|E|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\frac{b^{\\oplus}+b^{\\ominus}}{2}\\left\\|\\operatorname*{max}(\\eta_{G},P\\eta_{G^{\\prime}})\\right\\|_{1}-b^{\\ominus}|V^{\\prime}|-b^{\\oplus}|V|}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Following the relaxations done in Section 4.2, we propose an alternative neural surrogate by replacing $\\left\\|\\operatorname*{max}(A,\\^{\\bullet}\\!P A^{\\prime}P^{\\top})\\right\\|_{1,1}$ by $\\|\\mathrm{max}(R,S R^{\\prime})\\|_{1,1}$ and $\\|\\mathrm{max}(\\eta_{G},P\\eta_{G^{\\prime}})\\|_{1}$ by $\\|\\operatorname*{max}(X,P X^{\\prime})\\|_{1,1}$ , which gives us the approximated GED parameterized by $\\theta$ and $\\phi$ as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{GED}_{\\theta,\\phi}(G,G^{\\prime})=\\frac{a^{\\oplus}+a^{\\ominus}}{2}\\|\\mathrm{max}(R,S R^{\\prime})\\|_{1,1}-a^{\\ominus}|E^{\\prime}|-a^{\\oplus}|E|}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\frac{b^{\\oplus}+b^{\\ominus}}{2}\\|\\mathrm{max}(X,P X^{\\prime})\\|_{1,1}-b^{\\ominus}|V^{\\prime}|-b^{\\oplus}|V|}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We call this neural surrogate as MAX. We note that element-wise maximum over $\\pmb{A}$ and $P A^{\\prime}P^{\\intercal}$ , only allows non-edge to non-edge mapping attribute a value of zero. However, the neural surrogate described in Equation 20 fails to capture this, due to the presence of the soft alignment matrix $\\boldsymbol{S}$ . To address this, we explicitly discard such pairs from MAX by applying an OR operator over the edge presence between concerned node pairs, derived from the adjacency matrices $\\pmb{A}$ and $A^{\\prime}$ and populated in $\\mathrm{OR}(A,A^{\\prime})\\in\\mathbb{R}^{\\binom{N}{2}\\times\\binom{N}{2}}$ given by $\\operatorname{OR}(A[u,v],A^{\\prime}[u^{\\prime},v^{\\prime}])$ . Similarly, the indication of node presence can be given be given as $\\mathrm{OR}(\\eta_{G},\\eta_{G^{\\prime}})[\\Bar{u_{}}u^{\\prime}]^{\\prime}\\mathrm{=OR}(\\eta_{G}\\Bar{[u]},\\eta_{G^{\\prime}}[u^{\\prime}])$ . Hence, we write $\\begin{array}{r l}&{\\mathrm{GED}_{\\theta,\\phi}(G,G^{\\prime})=\\frac{a^{\\oplus}+a^{\\ominus}}{2}\\left\\|\\mathrm{OR}(A,A^{\\prime})\\odot\\operatorname*{max}(R,S R^{\\prime})\\right\\|_{1,1}-a^{\\ominus}|E^{\\prime}|-a^{\\oplus}|E|}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\frac{b^{\\oplus}+b^{\\ominus}}{2}\\left\\|\\mathrm{OR}(\\eta_{G},\\eta_{G^{\\prime}})\\odot\\operatorname*{max}(X,P X^{\\prime})\\right\\|_{1,1}-b^{\\ominus}|V^{\\prime}|-b^{\\oplus}|V|}\\end{array}$ ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "We call this formulation as MAX-OR. We provide the comparison between MAX, MAX-OR, and our models in Appendix D. ", "page_idx": 19}, {"type": "text", "text": "C Details about experimental setup ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "C.1 Generation of datasets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We have evaluated the performance of our methods and baselines on seven real-world datasets: Mutagenicity (Mutag), Ogbg-Code2 (Code2), Ogbg-Molhiv (Molhiv), Ogbg-Molpcba (Molpcba), AIDS, Linux and Yeast. We split each dataset into training, validation, and test splits in ratio of 60:20:20. For each split $\\mathcal{D}$ , we construct $(|\\mathcal{D}|(|\\mathcal{D}|+1))/2$ source and target graph instance pairs as follows: ${\\cal{S}}=\\{(G_{i},G_{j}):G_{i},G_{j}\\in{\\mathcal{D}}\\wedge i\\leq j\\}$ . We perform experiment in four GED regimes: ", "page_idx": 20}, {"type": "text", "text": "1. GED under uniform cost functions, where $b^{\\ominus}=b^{\\oplus}=a^{\\ominus}=a^{\\oplus}=1$ and substitution costs are 0   \n2. GED under non-uniform cost functions, where $b^{\\ominus}=3,b^{\\oplus}=1,a^{\\ominus}=2,a^{\\oplus}=1$ and substitution costs are 0   \n3. edge GED under non-uniform cost functions, where $b^{\\ominus}\\,=\\,b^{\\oplus}\\,=\\,0$ , $a^{\\circleddash}\\,=\\,2,a^{\\oplus}\\,=\\,1$ , and substitution costs are 0   \n4. GED with node substitution under uniform cost functions, where $b^{\\ominus}=b^{\\oplus}=a^{\\ominus}=a^{\\oplus}=1$ , as well as the node substitution cost $b^{\\sim}=1$ . ", "page_idx": 20}, {"type": "text", "text": "We emphasize that we generated clean datasets by filtering out isomorphic graphs from the original datasets before performing the training, validation, and test splits. This step is crucial to prevent isomorphism bias in the models, which can occur due to leakage between the training and testing splits, as highlighted by [26]. ", "page_idx": 20}, {"type": "text", "text": "For each graph, we have limited the maximum number of nodes to twenty, except for Linux, where the limit is ten. Information about the datasets is summarized in Table 7. Mutag contains nitroaromatic compounds, with each node having labels representing atom types. Molhiv and Molpcba contain molecules with node features representing atomic number, chirality, and other atomic properties. Code2 contains abstract syntax trees generated from Python codes. AIDS contains graphs of chemical compounds, with node types representing different atoms. For Molhiv, Molpcba and Linux, we have randomly sampled 1,000 graphs from each original dataset. ", "page_idx": 20}, {"type": "table", "img_path": "u7JRmrGutT/tmp/2a10668ca5a6c8898cb7b733a3114387575ad454f74cbaf2155cf1e79fd99ee5.jpg", "table_caption": [], "table_footnote": ["Table 7: Salient characteristics of data sets. "], "page_idx": 20}, {"type": "text", "text": "C.2 Details about state-of-the-art baselines ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We compared our model against nine state-of-the-art neural baselines and three combinatorial GED baselines. Below, we provide details of the methodology and hyperparameter settings used for each baseline. We ensured that the number of model parameters were in a comparable range. Specifically, we set the number of GNN layers to 5, each with a node embedding dimension of 10, to ensure consistency and comparability with our model. The following hyperparameters are used for training: Adam optimiser with a learning rate of 0.001 and weight decay of 0.0005, batch size of 256, early stopping with patience of 100 epochs, and Sinkhorn temperature set to 0.01. Neural Baselines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 GMN-Match and GMN-Embed Graph Matching Networks (GMN) use Euclidean distance to assess the similarity between graph-level embeddings of each graph. GMN is available in two variants: GMN-Embed, a late interaction model, and GMN-Match, an early interaction model. For this study, we used the official implementation of GMN to compute Graph Edit Distance (GED).1 \u2022 ISONET ISONET utilizes the Gumbel-Sinkhorn operator to learn asymmetric edge alignments between two graphs for subgraph matching. In our study, we extend ISONET\u2019s approach to predict the Graph Edit Distance (GED) score. We utilized the official PyTorch implementation provided by the authors for our experiments.2 ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "\u2022 GREED GREED utilizes a siamese network architecture to compute graph-level embeddings in parallel for two graphs. It calculates the Graph Edit Distance (GED) score by computing the norm of the difference between these embeddings. The official implementation provided by the authors was used for our experiments.3 ", "page_idx": 21}, {"type": "text", "text": "\u2022 ERIC ERIC utilizes a regularizer to learn node alignment, eliminating the need for an explicit node alignment module. The similarity score is computed using a Neural Tensor Network (NTN) and a Multi-Layer Perceptron (MLP) applied to the final graph-level embeddings of both graphs. These embeddings are derived by concatenating graph-level embeddings from each layer of a Graph Isomorphism Network (GIN). The model is trained using a combined loss from the regularizer and the predicted similarity score. For our experiments, we used the official PyTorch implementation to compute the Graph Edit Distance (GED). The GED scores were inverse normalized from the model output to predict the absolute GED.4 ", "page_idx": 21}, {"type": "text", "text": "\u2022 SimGNN SimGNN leverages both graph-level and node-level embeddings at each layer of the GNN. The graph-level embeddings are processed through a Neural Tensor Network to obtain a pair-level embedding. Concurrently, the node-level embeddings are used to compute a pairwise similarity matrix between nodes, which is then converted into a histogram feature vector. A similarity score is calculated by passing the concatenation of these embeddings through a MultiLayer Perceptron (MLP). We used the official PyTorch implementation of SimGNN and inverse normalization of the predicted Graph Edit Distance (GED) score to obtain the absolute GED value.5 \u2022 H2MN H2MN presents an early interaction model for graph similarity tasks. Instead of learning pairwise node relations, this method attempts to find higher-order node similarity using hypergraphs. At each time step of the hypergraph convolution, a subgraph matching module is employed to learn cross-graph similarity. After the convolution layers, a readout function is utilized to obtain graphlevel embeddings. These embeddings are then concatenated and passed through a Multi-Layer Perceptron (MLP) to compute the similarity score. We used the official PyTorch implementation of H2MN.6 ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "\u2022 GraphSim GraphSim uses GNN, where at each layer, a node-to-node similarity matrix is computed using the node embeddings. These similarity matrices are then processed using Convolutional Neural Networks (CNNs) and Multi-Layer Perceptrons (MLPs) to calculate a similarity score. We utilized the official PyTorch implementation.7 ", "page_idx": 21}, {"type": "text", "text": "\u2022 EGSC We used the Teacher model proposed by Efficient Graph Similarity Computation (EGSC), which leverages an Embedding Fusion Network (EFN) at each layer of the Graph Isomorphism Network (GIN). The EFN generates a single embedding from a pair of graph embeddings. The embeddings of the graph pair from each layer are concatenated and subsequently passed through an additional EFN layer and a Multi-Layer Perceptron (MLP) to obtain the similarity score. To predict the absolute Graph Edit Distance (GED), we inversely normalized the GED score obtained from the output of EGSC. We utilized the official PyTorch implementation provided by the authors for our experiments. 8 ", "page_idx": 21}, {"type": "text", "text": "Combinatorial Baselines: We use the GEDLIB9 library for implementation of all combinatorial baselines. ", "page_idx": 21}, {"type": "text", "text": "\u2022 Bipartite [41] Bipartite is an approximate algorithm that considers nodes and surrounding edges of nodes into account try to make a bipartite matching between two graphs. They use linear assignment algorithms to match nodes and their surroundings in two graphs. ", "page_idx": 21}, {"type": "text", "text": "\u2022 Branch [8], Branch Tight [8] improve upon [41] by decomposing graphs into branches. Branch Tight algorithm is another version of Branch that calculates a tighter lower bound but has a higher time complexity than Branch. ", "page_idx": 21}, {"type": "text", "text": "\u2022 Anchor Aware GED Chang et al. [15] provides an approximation algorithm that calculates a tighter lower bound using the anchor aware technique. ", "page_idx": 21}, {"type": "text", "text": "\u2022 IPFP [11] is an approximation algorithm which handles node and edge mapping simultaneously unlike previously discussed methods. This solves a quadratic assignment problem on edges and nodes. \u2022 F2 [29] uses a binary linear programming approach to find a higher lower bound on GED calculation. This method was used with a very high time limit to generate Ground truth for our experiments. ", "page_idx": 22}, {"type": "text", "text": "C.3 Details about GRAPHEDX ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "At the high level, GRAPHEDX consists of two components $\\operatorname{EMBED}\\theta$ and $\\mathbf{PERMNET}_{\\phi}$ . ", "page_idx": 22}, {"type": "text", "text": "Neural Parameterization of $\\mathbf{EMBED}\\theta$ : $\\operatorname{EMBED}\\theta$ consists of two modules: a GNN denoted as $\\mathrm{MPNN}_{\\theta}$ and a ${\\bf M L P}_{\\theta}$ . The $\\mathrm{MPNN}_{\\theta}$ consists of $K=5$ propagation layers used to compute node embeddings of dimension $d=10$ . At each layer $k$ , we compute the updated the node embedding as follows: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\pmb{x}_{k+1}(u)=\\mathrm{UPDATE}_{\\theta}\\left(\\pmb{x}_{k}(u),\\sum_{v\\in\\mathrm{nbr}(u)}\\mathrm{LRL}_{\\theta}(\\pmb{x}_{k}(u),\\pmb{x}_{k}(v))\\right)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\operatorname{LRL}\\theta$ is a Linear-ReLU-Linear network, with $d=10$ features, and the $\\mathrm{UPDATE}_{\\theta}$ network consists of a Gated Recurrent Unit [30]. In case of GED setting under uniform cost and GED setting under non-uniform cost, we set the initial node features $\\begin{array}{r}{x_{0}(\\bar{u})=1}\\end{array}$ , following [30]. However, in case of computation of GED with node substitution costs, we explicitly provide the one-hot labels as node features. Given the node embeddings and edge-presence indicator obtained from the adjacency matrices, after 5 layer propogations, we compute the edge embeddings $\\pmb{r}(e)$ using ${\\bf M L P}_{\\theta}$ , which is decoupled from $\\mathrm{MPNN}_{\\theta}$ . ${\\bf M L P}_{\\theta}$ consists of a Linear-ReLU-Linear network that maps the $2d+1=21$ dimensional input consisting of forward $({\\pmb x}_{K}(u)\\,||\\,{\\pmb x}_{K}(v)\\,||\\,{\\pmb A}[u,v])$ and backward $({\\pmb x}_{K}(v)\\,||\\,{\\pmb x}_{K}(u)\\,||\\,{\\pmb A}[v,u])$ signals to $D=20$ dimensions. ", "page_idx": 22}, {"type": "text", "text": "Neural Parameterization of $\\mathbf{PERMNET}_{\\phi}$ : Given the node embeddings $\\mathbf{\\nabla}x_{K}(\\cdot)$ and $\\pmb{x}_{K}^{\\prime}(\\cdot)$ , we first pass them through a neural network $c_{\\phi}$ which consists of a Linear-ReLU-Linear network transforming the features from $d=10$ to $N$ dimensions, which is the number of nodes after padding. Except for Linux where $N\\,=\\,10$ , all other datasets have $N\\,=\\,20$ . We obtain the matrix $_{C}$ such that $C[u,u^{\\prime}]\\;=\\;\\|c_{\\phi}({\\pmb x}_{K}(u))-c_{\\phi}({\\pmb x}_{K}^{\\prime}(u^{\\prime}))\\|_{1}$ . Using temperature $\\tau\\:=\\:0.01$ , we perform Sinkhorn iterations on $\\exp(-C/\\tau)$ as follows for $T=20$ iterations to get $_{P}$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\nP_{k}=\\mathrm{NORMCOL}\\left(\\mathrm{NORMROW}\\left(P_{k-1}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $P_{0}\\,=\\,\\exp(-C/\\tau)$ . Here NORMROW $(M)[i,j]\\,=\\,M[i,j]/\\sum_{\\ell}M[\\ell,j]$ denotes the row normalization function and $\\begin{array}{r}{\\mathrm{NoRMCoL}(M)[i,j]=M[i,j]/\\sum_{\\ell}M[i,\\tilde{\\ell}]}\\end{array}$ denotes the column normalization function. We note that the soft alignment $_{P}$ obtained does not depend on the GED cost values, as discussed in Appendix B. The soft alignment $_{P}$ for nodes is used to construct soft alignment $\\boldsymbol{S}$ for as follows: $\\pmb{S}[\\bar{(u,v)},(u^{\\prime},v^{\\prime})]=\\pmb{P}[u,\\bar{u^{\\prime}}]\\pmb{P}[v,v^{\\prime}]+\\pmb{P}[u,v^{\\prime}]\\pmb{P}[v,u^{\\prime}]$ . ", "page_idx": 22}, {"type": "text", "text": "C.4 Evaluation metrics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Given the dataset $\\boldsymbol{S}$ consisting of input pairs of graphs $(G,G^{\\prime})$ along with the ground truth $\\mathrm{GED}(G,G^{\\prime})$ and model prediction $\\widehat{\\mathrm{GED}}(G,G^{\\prime})$ , we evaluate the performance of the model using the Root Mean Square Error (RMSE) and Kendall-Tau (KTau) [28] between the predicted GED scores and actual GED values. ", "page_idx": 22}, {"type": "text", "text": "\u2022 MSE: It evaluates how far the predicted GED values are from the ground truth. A better performing model is indicated by a lower MSE value. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{MSE}=\\frac{1}{|S|}\\sum_{(G,G^{\\prime})\\in\\mathcal{S}}\\left(\\mathrm{GED}(G,G^{\\prime})-\\widehat{\\mathrm{GED}}(G,G^{\\prime})\\right)^{2}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "\u2022 KTau: Selection of relevant corpus graphs via graph similarity scoring is crucial to graph retrieval setups. In this context, we would like the number of concordant pairs $N_{+}$ (where the ranking of ground truth GED and model prediction agree) to be high, and the discordant pairs $N_{-}$ (where the two disagree) to be low. Formally, we write ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{KTau}=\\frac{N_{+}-N_{-}}{\\binom{|S|}{2}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For the methods which compute a similarity score between the pair of graphs through the notion of normalized GED, we map the similarity score $s$ back to the GED as $\\begin{array}{r}{\\widehat{\\mathrm{GED}}(\\dot{G_{\\star}}G^{\\prime})=-\\frac{\\bar{|V|}+|V|^{\\prime}}{2}\\log(s+}\\end{array}$ $\\epsilon$ ) where $\\epsilon=10^{-7}$ is added for stability of the logarithm. ", "page_idx": 23}, {"type": "text", "text": "C.5 Hardware and license ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We implement our models using Python 3.11.2 and PyTorch 2.0.0. The training of our models and the baselines was performed across servers containing Intel Xeon Silver 4216 2.10GHz CPUs, and Nvidia RTX A6000 GPUs. Running times of all methods are compared on the same GPU. ", "page_idx": 23}, {"type": "text", "text": "D Additional experiments ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section, we present results from various additional experiments performed to measure the performance of our model under different cost settings. ", "page_idx": 24}, {"type": "text", "text": "D.1 Comparison of performance of GRAPHEDX on non-uniform cost Edge-GED ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We consider another cost setting \u2013 where the node costs are explicitly set to 0, and $a^{\\oplus}=1,a^{\\ominus}=2$ . In such a case, GRAPHEDX only consists of $\\Delta^{\\ominus}(R,R^{\\prime}\\,|\\,S)$ and $\\Delta^{\\oplus}\\!\\left(R,R^{\\prime}\\,|\\,S\\right)$ terms. To showcase the importance of aligning edges through edge alignment, we generate an alternate model, where the alignment happens through the terms $\\bar{\\Delta}^{\\ominus}(\\mathbf{\\bar{{X}}},\\bar{{X^{\\prime}}}\\,|\\,P)$ and $\\bar{\\Delta}^{\\oplus}(X,X^{\\prime}\\,|\\,P)$ , where we set $b^{\\oplus}=1$ and $b^{\\ominus}\\;=\\;2$ , and set the edge costs to 0. We call this model NodeSwap (w/o XOR), and the corresponding XOR variant as NodeSwap $+\\,\\mathrm{XOR}$ . In Table 8, we compare the performance variants of GRAPHEDX with NodeSwap (w/o XOR) and the rest of the baselines to predict the Edge GED score in an non-uniform cost setting. From the results, we can infer that the performance of edgealignment based model to predict Edge-GED outperforms the corresponding node-alignment version. ", "page_idx": 24}, {"type": "table", "img_path": "u7JRmrGutT/tmp/6982690515f2c897f65e8758699810f85afc2c6ff1767c80cbd9e9e4936a6cf8.jpg", "table_caption": [], "table_footnote": ["Table 8: Comparison of edge-alignment based GED scoring function with node-alignment based GED scoring function and state-of-the-art baselines under the cost setting: $a^{\\ominus}=2,a^{\\oplus}\\,\\,{\\overline{{{=}}}}\\,\\,1,b^{\\ominus}=b^{\\oplus}=0$ In case of NodeSwap (w/o XOR), we swap the edge costs and node costs, and expect the model to learn the alignments in Edge GED through node alignment only. Green (yellow) numbers report the best (second best) performers. "], "page_idx": 24}, {"type": "text", "text": "D.2 Comparison of GRAPHEDX with baselines on uniform and non-uniform cost setting ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Tables 9 and 10 report performance in terms of MSE under uniform and non-uniform cost settings, respectively. Table 11 reports performance in terms of KTau under both uniform and non-uniform cost settings. The results are similar to those in Table 3, where our model is the clear winner across all datasets, outperforming the second-best performer by a significant margin. There is no consistent second-best model, but ERIC, EGSC, and ISONET perform comparably and better than the others. ", "page_idx": 25}, {"type": "table", "img_path": "u7JRmrGutT/tmp/94646c9699d81acaf67588e4de5a48db1ca56bcc49c6e71406167ccff4a044c4.jpg", "table_caption": [], "table_footnote": ["Table 9: Comparison with baselines in terms of MSE including standard error for uniform cost setting ( $b^{\\ominus}=b^{\\oplus}=a^{\\ominus}=a^{\\oplus}=1;$ . Green (yellow) numbers report the best (second best) performers. "], "page_idx": 25}, {"type": "table", "img_path": "u7JRmrGutT/tmp/6aa35dcbc502e8b85aec0072c9ad21cbefc911e52cc7b8547aaafb84068bfd69.jpg", "table_caption": [], "table_footnote": ["Table 10: Comparison with baselines in terms of MSE including standard error for non-uniform cost setting $(b^{\\ominus}=\\bar{3},b^{\\oplus}=1,a^{\\ominus}=2,a^{\\oplus}=1)$ . Green (yellow) numbers report the best (second best) performers. "], "page_idx": 25}, {"type": "table", "img_path": "u7JRmrGutT/tmp/76c5ea7297e19869afb1401bb9e724714fa26dea4ffe9dc9e23d3f0574fe0008.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "Table 11: Comparison with baselines in terms of KTau for both uniform and non-uniform cost settings, where for uniform cost settings costs are $b^{\\ominus}=b^{\\oplus}=a^{\\ominus}=a^{\\oplus}=1$ and for non-uniform cost settings costs are $b^{\\ominus}=3,b^{\\oplus}={\\bar{1,}}a^{\\ominus}=2,a^{\\oplus}=1$ . Green (yellow) numbers report the best (second best) performers. ", "page_idx": 25}, {"type": "text", "text": "D.3 Comparison of GRAPHEDX with baselines with and without cost features ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Table 12 reports performance in terms of MSE under non-uniform cost setting, with and without costs used as features to the baselines. We notice that that in some cases, providing cost features boost the performance of baselines significantly, and in a few cases, withholding the costs gives a slight improvement in performance. However, GRAPHEDX, which uses costs in the distance formulation rather than features, outperforms all baselines by a significant margin. ", "page_idx": 26}, {"type": "table", "img_path": "u7JRmrGutT/tmp/13c7b656d2e3484cb589117b04a69309c33a334ae2b0220bf46161c14f18ef13.jpg", "table_caption": [], "table_footnote": ["Table 12: Comparison of performance (MSE) of methods for the non-uniform cost setting when nodes are initialized with costs as features versus without. For each method, the better performance between with and without cost-feature initialization is highlighted in bold for both uniform and non-uniform cost settings. In each column, Green (yellow) numbers report the best (second best) performers. "], "page_idx": 26}, {"type": "text", "text": "D.4 Comparison of GRAPHEDX with baselines with node substitution cost ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In Tables 13 and 14, we compare the performance of GRAPHEDX with baselines under a node substitution cost $b^{\\sim}$ . The cost setting is $\\mathbf{\\dot{\\mathfrak{b}}}^{\\ominus}={\\mathfrak{b}}^{\\oplus}={\\mathfrak{b}}^{\\sim}=a^{\\ominus}=a^{\\oplus}=1$ . This experiment includes only five datasets where node labels are present. We observe that GRAPHEDX outperforms all other baselines. There is no clear second-best model, but ERIC, EGSC, and ISONET perform better than the others. ", "page_idx": 27}, {"type": "table", "img_path": "u7JRmrGutT/tmp/69a7d91cd8d7b92c8e7f54824828a487ea4dd4ef42dd9509786c19b4b3c36cfb.jpg", "table_caption": [], "table_footnote": ["Table 13: Comparison with baselines in terms of MSE including standard error, in presence of the node substitution cost, which set to one in uniform cost setting: $b^{\\ominus}=b^{\\oplus}=b^{\\sim}={\\bar{a}}^{\\ominus}=a^{\\oplus}=1$ . Green (yellow) numbers report the best (second best) performers. "], "page_idx": 27}, {"type": "table", "img_path": "u7JRmrGutT/tmp/446343f282909c8c36629464c0182abdeab548604a3ed6c80f7f987cbaeec32b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "D.5 Performance evaluation for edge-only vs. all-node-pair representations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this section, we compare the performance of using graph representation with two variants of our method. (i) Edge-only $({\\mathrm{edge}}\\to{\\mathrm{edge}})$ : Here, $R,R^{\\prime}\\in\\mathbb{R}^{\\operatorname*{max}(|E|,|E^{\\prime}|)\\times D}$ are computed using only the embeddings of node-pairs that are edges, and excluding non-edges. This means that $\\boldsymbol{S}$ becomes an edge-to-edge alignment matrix instead of a full node-pair alignment matrix. (ii) Edge-only $({\\mathrm{pair}}\\to{\\mathrm{pair}})$ ): In this variant, $\\boldsymbol{S}$ remains a node-pair alignment matrix, but the embeddings of the non-edges in $\\boldsymbol{R}$ , $\\pmb{R}^{\\prime}\\in\\mathbb{R}^{N(N-1)/2\\times D}$ are explicitly set to zero. Tables 15 and 16 contain extended results from Table 6 across seven datasets. The results are similar to those discussed in the main paper: (1) both these sparse representations perform significantly worse compared to our method using non-trivial representations for both edges and non-edges, and (2) Edge-only (edge $\\rightarrow$ edge) performs better than Edge-only $({\\mathrm{pair}}\\to{\\mathrm{pair}})$ . This underscores the importance of explicitly modeling trainable non-edge embeddings to capture the sensitivity of GED to global graph structure. ", "page_idx": 27}, {"type": "table", "img_path": "u7JRmrGutT/tmp/7de069889890fec1f9090aa40357fd731a038f230fc87f4b14be4cb2628d5baf.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "Table 16: Comparison of using all-node-pairs against edge-only representations using MSE for non-uniform cost setting. Green (yellow) numbers report the best (second best) performers. ", "page_idx": 28}, {"type": "text", "text": "D.6 Effect of using cost-guided scoring function on baselines ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In Tables 17 and 18, we report the impact of replacing the baselines\u2019 scoring function with our proposed cost-guided scoring function on three baselines across seven datasets for uniform and non-uniform cost settings, respectively. We notice that similar to the results reported in Section 5.2, the cost-guided scoring function helps the baselines perform significantly better in both the cost settings. ", "page_idx": 28}, {"type": "table", "img_path": "u7JRmrGutT/tmp/de8c010db62a78080869754232e66767026db758982908a9ca022c1efb84e554.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "table", "img_path": "u7JRmrGutT/tmp/c25c3ad61e5a9110df298b27d6a2a3a16e8b4e86419e66b04bdab3fdcedf02c2.jpg", "table_caption": ["Table 17: Impact of cost-guided distance on MSE in uniform cost setting (b\u2296= b\u2295= a\u2296= a\u2295= 1). \\* represents the variant of the baseline with cost-guided distance. Green shows the best performing model. Bold font indicates the best variant of the baseline. "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "Table 18: Impact of cost-guided distance on MSE in non-uniform cost setting $(b^{\\ominus}=3,b^{\\oplus}=1,a^{\\ominus}=$ 2, $a^{\\oplus}=1$ ). \\* represents the variant of the baseline with cost-guided distance. Green shows the best performing model. Bold font indicates the best variant of the baseline. ", "page_idx": 28}, {"type": "text", "text": "D.7 Results on performance of the alternate surrogates for GED ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In Table 19, we present the performance of the alternate surrogates scoring function for GED discussed in B under non-uniform cost settings $(b^{\\ominus}=3,b^{\\oplus}=1,a^{\\breve{\\ominus}}=2,a^{\\oplus}=^{\\bullet}\\!1)$ . From the results, we can infer that the alternate surrogates have comparable performance to GRAPHEDX however GRAPHEDX outperforms it by a small margin on six out of the seven datasets. ", "page_idx": 28}, {"type": "table", "img_path": "u7JRmrGutT/tmp/e39ab710a3419315e02462a23514b2d7df5dde0ea074d10056f9603d89d4ef88.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "Table 19: Comparison of MSE between GRAPHEDX MAX-OR, and MAX. Green (yellow) numbers report the best (second best) performers. ", "page_idx": 28}, {"type": "text", "text": "D.8 Comparison of zero-shot performance on other datasets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In Table 20, we compare all baselines with GRAPHEDX on zero-shot GED prediction on a new dataset. For each method, we select the best-performing models for {AIDS, Yeast, Mutag, Molhiv }, and test each one on the AIDS dataset under non-uniform cost setting. ", "page_idx": 28}, {"type": "table", "img_path": "u7JRmrGutT/tmp/66e796a46940b5b2c8a50e02898d37f38ffe45f590bc5ede205147bb497e2d7c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "Table 20: Comparison of MSE between GRAPHEDX and baselines on zero-shot GED prediction on the AIDS test dataset under non-uniform cost setting. Green (yellow) numbers report the best (second best) performers. ", "page_idx": 28}, {"type": "text", "text": "D.9 Importance of node-edge consistency ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "GRAPHEDX enforces consistency between node and edge alignments by design. However, one might choose to enforce node-edge consistency through alignment regularization between independently learnt soft node and edge alignment. However, as shown in Figure 21, we notice that such nonconstrained learning might lead to under-prediction or incorrect alignments. We demonstrate the importance of constraining the node-pair alignment $\\boldsymbol{S}$ with the node alignment $_{P}$ by showing the mapping of nodes and edges between two graphs. The required edit operations for subfigure a) with the constrained $\\boldsymbol{S}$ are two node additions $\\bar{\\{\\boldsymbol{e}},\\bar{\\boldsymbol{f}}\\}$ , one edge deletion $(\\bar{d},a)$ , and three edge additions $\\{(a,f),(e,d),(e,f)\\}$ . Assuming that each edit costs one, the true GED is 6. However, in subplot b), $\\boldsymbol{S}$ is not constrained, and the edit operations with the lowest cost are two node additions $\\{e,f\\}$ and two edge additions $\\{(a,f),(e,f)\\}$ . This erroneously results in a GED of 4. ", "page_idx": 29}, {"type": "image", "img_path": "u7JRmrGutT/tmp/90dfc0a80751ac9564ed3aa22e3118f62822ec05160cf515b41c0c15a78c7c30.jpg", "img_caption": ["Figure 21: Node and edge alignment with constrained and unconstrained alignment $\\boldsymbol{S}$ . A dashed edge represents the deleted edge. Grey edges represent added edges. ", "Table 22: Comparison of alignment regularizer usage versus no alignment regularizer usage on uniform cost GED, Measured by MSE. Green (yellow) numbers report the best (second best) performers. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Further, in Table 22, we compare the performance of enforcing node-edge consistency through design (GRAPHEDX), and through alignment regularization (REG). Following the discussion in Section 4.2, such a model also exhibits a variant with XOR, called REG-xor. We notice that GRAPHEDX even outperforms such the described model in 4 out of 6 cases. We also notice that REG-xor outperforms GRAPHEDX in the other two cases. However, the above example shows a tendency to learn wrong alignments which in turn gives wrong optimal edit paths. ", "page_idx": 29}, {"type": "table", "img_path": "u7JRmrGutT/tmp/1c63ea0e59e6e9f46c95ab62f69ce8bc188cb8049de5b5948a9ef3c9c1dbedc2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "D.10 Comparison of nine possible combinations our proposed set distances ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "In Tables 23 and 24, we compare the performance of nine possible combinations our proposed set distances for uniform and non-uniform cost settings respectively. Results follow the observations in Table 2, where the variant with XOR-DIFFALIGN outperforms those without it. ", "page_idx": 30}, {"type": "table", "img_path": "u7JRmrGutT/tmp/ef803d6774ad814211c7fed4ea8d85ace2fc9a1348bbfd3260e1348b65f117b6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 30}, {"type": "table", "img_path": "u7JRmrGutT/tmp/f31379f933bf6cc67f305a6b4aeb1a6a34ce0c078a4def85e3a90c187b858b3e.jpg", "table_caption": ["Table 23: Comparison of MSE for nine combinations of our neural set distance surrogates under uniform cost settings. The GRAPHEDX model was selected based on the best MSE on the validation set, while the reported results represent MSE on the test set. Green (yellow) numbers report the best (second best) performers. "], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "Table 24: Comparison of MSE for nine combinations under non-uniform cost settings. The GRAPHEDX model was selected based on the best MSE on the validation set, while the reported results represent MSE on the test set. Green (yellow) numbers report the best (second best) performers. ", "page_idx": 30}, {"type": "text", "text": "D.11 Comparison of performance of our model with baselines using scatter plot ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In Figure 25, we illustrate the performance of our model compared to the second-best performing model, under both uniform and non-uniform cost settings, by visualizing the distribution of outputs of the predicted GEDs by both models. We observe that predictions from our model consistently align closer to the $y=x$ line across various datasets showcasing lower output variance as compared to the next best-performing model. ", "page_idx": 31}, {"type": "image", "img_path": "u7JRmrGutT/tmp/cc343ec8cb1f7038c105006eca802d554534085bfd722145edb495b7a97ddaf3.jpg", "img_caption": ["Figure 25: Scatter plot comparing the distribution of the predicted GED of our model with the next best-performing model across various datasets under both uniform and non-uniform cost settings. "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "D.12 Comparison of performance of our model with baselines using error distribution ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In Figure 26, we plot the distribution of error (MSE) of our model against the second-best performing model, under both uniform and non-uniform cost settings. We observe that our model performs better, exhibiting a higher probability density for lower MSE values and a lower probability density for higher MSE values. ", "page_idx": 31}, {"type": "image", "img_path": "u7JRmrGutT/tmp/71cee48dc27de487090c916517cb0611a50589a17aaf782181e7a6abe1049f5e.jpg", "img_caption": ["Figure 26: Error distribution of our model compared to the next best-performing model across various datasets under both uniform and non-uniform cost settings. "], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "u7JRmrGutT/tmp/0020e2dc109dc289576e2cf618cb6ef2b94a08fb95c613a698da44a380fa9e74.jpg", "img_caption": ["Figure 27: Performance of combinatorial optimization algorithms on various datasets under both uniform and non-uniform cost settings is evaluated. We plot MSE against the time limit allocated to the combinatorial algorithms. Additionally, we include the amortized time of our model and its MSE. "], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "We compare the runtime performance of six combinatorial optimization algorithms described in Appendix C (ipfp [11], anchor-aware GED [15], branch tight [8], F2 [29], bipartite [41] and branch [8]). We note that combinatorial algorithms are slow to approximate the GED between two graphs. Specifically, GRAPHEDX often predicts the GED in $\\sim10^{-4}$ seconds per graph, however, the performance of the combinatorial baselines are extremely poor under such a time constraint. Hence, we execute the combinatorial algorithms with four different time limits per graph: ranging from $10^{-2}$ seconds ( $100\\mathrm{x}$ our method) to 10 seconds ( $\\mathrm{10^{5}x}$ our method). ", "page_idx": 32}, {"type": "text", "text": "In Figure 27, we depict the MSE versus time limit for the aforementioned combinatorial algorithms under both uniform and non-uniform cost settings. We also showcase the inference time per graph of our method in the figure. It is evident that even with a time limit scaled by $10^{5}\\mathrm{x}$ , most combinatorial algorithms struggle to achieve a satisfactory approximation for the GED. ", "page_idx": 32}, {"type": "text", "text": "D.14 Prediction timing analysis ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "In Figure 28 illustrates the inference time per graph of our model versus under uniform cost settings, averaged over ten runs. From the figure, we observe the following (1) GRAPHEDX outperforms four of the baselines in terms of inference time, and is comparable to ISONET\u2019s inference time (2) GMN-Embed, GREED, ERIC, and EGSC run faster compared to all other methods due to lack of interaction between graphs, which results in poorer performance at predicting the GED. ", "page_idx": 33}, {"type": "image", "img_path": "u7JRmrGutT/tmp/17c4763a1f4211f4ce8c35113cfd9992244c57fc82a7b8ef660a6d1d404370f1.jpg", "img_caption": ["Figure 28: GED inference time comparison between our model and baselines. We notice that GRAPHEDX is consistently the third-fastest amongst all baselines. Although GMN-Embed and GREED have the lowest inference time, GRAPHEDX has much lower MSE consistently. "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "D.15 Visualization (optimal edit path) $^+$ Pseudocode ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "In Algorithm 1, we present the pseudocode to generate the optimal edit path given the learnt node and edge alignments from GRAPHEDX. Figure 29 demonstrates how the operations in the edit path can be utilized to convert $G$ to $G^{\\prime}$ . ", "page_idx": 33}, {"type": "image", "img_path": "u7JRmrGutT/tmp/c4e196fe4733083625c8aefd5cb40e8b980e803b0d011db088e21e00ca90f07e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "Figure 29: An example of the sequence of edit operations performed to convert one graph into another. ", "page_idx": 33}, {"type": "table", "img_path": "u7JRmrGutT/tmp/c815463bd698c2b3c1cec98c950698b9070ed21e42f51a2036293d4cd66c7812.jpg", "table_caption": [], "table_footnote": [], "page_idx": 33}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: In Section 5, we present experiments and results to support the claims made in the abstract and introduction. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 34}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: In Conclusions, we discuss the limitations of our work. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 34}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 34}, {"type": "text", "text": "Justification: In Appendix B, we provide proof for the theoretical results mentioned in the   \npaper. jideli ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results. \u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. ", "page_idx": 34}, {"type": "text", "text": "\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems. \u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. \u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 35}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We provide code and dataset in the supplementary material with instructions to reproduce the results.   \nGuidelines:   \n\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: In the supplementary material, we provide code for our model, baselines, and experimental datasets, as well as instructions for reproducing the results. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code. \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. \u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not ", "page_idx": 35}, {"type": "text", "text": "including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 36}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: In Appendix C, we provide training details, such as hyperparameters and optimizer used.   \nGuidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 36}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate   \ninformation about the statistical significance of the experiments?   \nAnswer: [Yes] Justification: Along with Mean Squared Error we also provide Standard deviation to report the statistical significance of our results.   \nGuidelines:   \n\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: In Appendix C we provide information on hardware used for running experiments.   \nGuidelines:   \n\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 37}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: Yes, the research conducted in the paper conforms, in every aspect, with Neurips Code of Ethics. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 37}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 37}, {"type": "text", "text": "Justification: In Appendix A we have discuss broader impact of our work. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 37}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA]   \nJustification: We do not use any such dataset/method.   \nGuidelines: \u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 38}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 38}, {"type": "text", "text": "Justification: We cite and provide URLs for datasets and codes that we use for the experiments. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 38}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation   \nprovided alongside the assets?   \nAnswer: [Yes] Justification: We provide our code and dataset with README file having instructions on how to run the experiments.   \nGuidelines:   \n\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 38}, {"type": "text", "text": "", "page_idx": 38}, {"type": "text", "text": "", "page_idx": 38}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?   \nAnswer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: This work does not involve such research Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. ", "page_idx": 38}, {"type": "text", "text": "\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 39}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 39}, {"type": "text", "text": "uidelines:   \n\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 39}]