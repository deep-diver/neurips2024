[{"figure_path": "zIr2QjU4hl/tables/tables_21_1.jpg", "caption": "Table 1: Basic architecture of networks for diffusion models", "description": "This table shows the architecture of the neural networks used for diffusion models in the paper. It details the input and output dimensions for each layer, along with the activation functions used.  This architecture is specifically designed for modeling biological sequences.", "section": "E.1.1 Architecture of Neural Networks"}, {"figure_path": "zIr2QjU4hl/tables/tables_21_2.jpg", "caption": "Table 2: Important hyperparameters for fine-tuning. For all methods, we use Adam as an optimizer.", "description": "This table lists the hyperparameters used in the fine-tuning process for all methods mentioned in the paper.  It includes parameters such as batch size, KL parameter alpha, LCB parameter (bonus) c, the number of bootstrap heads, the sampling method used (Euler Maruyama), the step size used during fine-tuning, and the guidance level and target.  These parameters are crucial to the performance and reproducibility of the experiments.", "section": "E.1.2 Hyperparameters"}, {"figure_path": "zIr2QjU4hl/tables/tables_22_1.jpg", "caption": "Table 1: Basic architecture of networks for diffusion models", "description": "This table details the architecture of the neural networks used for the diffusion models in the paper. It shows the input and output dimensions for each layer, along with the activation function used.  The architecture is designed for processing biological sequences, and is a key component in the proposed BRAID method for generating high-quality designs.", "section": "E.1.1 Architecture of Neural Networks"}, {"figure_path": "zIr2QjU4hl/tables/tables_23_1.jpg", "caption": "Table 2: Important hyperparameters for fine-tuning. For all methods, we use Adam as an optimizer.", "description": "This table lists the hyperparameters used for fine-tuning the diffusion models.  It includes parameters for BRAID (including the bonus parameter c), STRL, and offline guidance.  The optimization parameters (optimizer, learning rate, weight decay, gradient clipping, and truncated backpropagation steps) are also listed and are consistent across all methods.", "section": "E.1.2 Hyperparameters"}, {"figure_path": "zIr2QjU4hl/tables/tables_23_2.jpg", "caption": "Table 5: Statistics of LLaVA-adjusted scores.", "description": "This table presents the results of evaluating the generated images using LLaVA, a large multi-modal model.  It shows the mean, minimum, and maximum LLaVA scores for 400 generated samples, comparing the pre-trained model and several checkpoints of the STRL algorithm. The crucial aspect is the \"invalid/total samples\" column, which indicates the number of images deemed invalid (i.e., not correctly aligned with the prompt) out of the total 400 samples.  This metric helps to quantify the level of over-optimization, which is a significant issue addressed in the paper. ", "section": "E.2.5 Effectiveness of LLaVA-aided evaluation"}]