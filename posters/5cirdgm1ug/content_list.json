[{"type": "text", "text": "Position Coupling: Improving Length Generalization of Arithmetic Transformers Using Task Structure ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Hanseul Cho\u2217 Jaeyoung Cha\u2217 Pranjal Awasthi Graduate School of AI, KAIST Google Research {jhs4015,chajaeyoung}@kaist.ac.kr pranjalawasthi@google.com ", "page_idx": 0}, {"type": "text", "text": "Srinadh Bhojanapalli Anupam Gupta Chulhee Yun Google Research NYU & Google Research Graduate School of AI, KAIST bsrinadh@google.com anupam.g@nyu.edu chulhee.yun@kaist.ac.kr ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Even for simple arithmetic tasks like integer addition, it is challenging for Transformers to generalize to longer sequences than those encountered during training. To tackle this problem, we propose position coupling, a simple yet effective method that directly embeds the structure of the tasks into the positional encoding of a (decoder-only) Transformer. Taking a departure from the vanilla absolute position mechanism assigning unique position IDs to each of the tokens, we assign the same position IDs to two or more \u201crelevant\u201d tokens; for integer addition tasks, we regard digits of the same significance as in the same position. On the empirical side, we show that with the proposed position coupling, a small (1-layer) Transformer trained on 1 to 30-digit additions can generalize up to 200-digit additions $_{(6.67\\times}$ of the trained length). On the theoretical side, we prove that a 1-layer Transformer with coupled positions can solve the addition task involving exponentially many digits, whereas any 1-layer Transformer without positional information cannot entirely solve it. We also demonstrate that position coupling can be applied to other algorithmic tasks such as $N\\times2$ multiplication and a two-dimensional task. Our codebase is available at github.com/HanseulJo/position-coupling. ", "page_idx": 0}, {"type": "image", "img_path": "5cIRdGM1uG/tmp/65f13be27fd3a80dc8d0348b2999fdac733f1339124547a83ca633999a116cad.jpg", "img_caption": ["Figure 1: Methods for Length Generalization in the Integer Addition Task. We report exactmatch (EM) accuracies (markers: medians over experiments; light area: $95\\%$ confidence intervals). We employ the reversed format and zero-paddings (Lee et al., 2024) into the input sequence. With our proposed position coupling, we achieve more than $95\\%$ exact-match accuracy for up to 200-digit additions with decoder-only Transformers trained on up to 30-digit additions. For index hinting (Zhou et al., 2024a), we separately test absolute positional embedding (APE) with a random starting position ID (mimicking the original implementation by Zhou et al. (2024a)) and without positional encoding (NoPE) (Kazemnejad et al., 2023) (as tested by Zhou et al. (2024b)). "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Since the appearance of a sequence-to-sequence deep neural architecture called Transformer (Vaswani et al., 2017), it has brought tremendous success in various fields including natural language process (NLP) (Chowdhery et al., 2023; Gemini et al., 2023; OpenAI, 2023; Thoppilan et al., 2022) and many applications such as mathematical reasoning and theorem proving (Lewkowycz et al., 2022; Trinh et al., 2024; Wu et al., 2022). Despite its triumph, it has recently been illuminated that Transformers often lack the ability of length generalization (Anil et al., 2022; Deletang et al., 2023; Press et al., 2022; Zhang et al., 2023). It refers to a special kind of out-of-distribution generalization capability to extrapolate the model\u2019s performance to longer sequences than those encountered during training. Understanding length generalization is of great importance because a lack of it provides evidence that language models do not genuinely understand the structure of a given task. Improving Transformer\u2019s length generalization has received much attention, particularly because the time/memory complexities for training Transformers grow up to quadratically in the sequence length. ", "page_idx": 1}, {"type": "text", "text": "Even for simple arithmetic tasks such as integer addition, length generalization is still difficult for Transformers (Kazemnejad et al., 2023; Kim et al., 2021; Lee et al., 2024; Nogueira et al., 2021; Zhou et al., 2024a,b). Humans can length-generalize in integer addition because they understand the essential principle of the task. Nevertheless, it is observed that Transformers typically learn to solve addition only up to the training sequence length (Lee et al., 2024), which is different from the true arithmetic algorithm that humans \u201cimplement\u201d. This raises an important question: can we make a Transformer truly understand the structure of a task so that it can generalize to the longer sequences without training on them? In other words, can we inject the known structure of a task into a Transformer so that it can automatically length-generalize? ", "page_idx": 1}, {"type": "text", "text": "In this paper, we propose position coupling, a simple yet effective method for length generalization that directly embeds the structure of the tasks into a Transformer. In contrast to the vanilla absolute position mechanism assigning unique and consecutive position IDs to each token, we assign the same position IDs to certain input tokens that are semantically relevant. Coupling such tokens together helps the model learn to solve the task regardless of the length of the given input sequence. For example, in the addition task, it is important to consider the significance of digits, so we couple the positions at the same significance (unique in each operand and the answer). ", "page_idx": 1}, {"type": "text", "text": "1.1 Summary of Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "\u2022 We propose position coupling to tackle the length generalization problem of decoder-only Transformers. Our approach injects the structure of the task into the absolute position encoding by assigning the same position IDs to relevant tokens (Section 3). ", "page_idx": 1}, {"type": "text", "text": "\u2022 With position coupling, we achieve a robust and near-perfect generalization up to 200-digit additions by training Transformers on up to 30-digit additions, which is a $6.67\\times$ extrapolation of the operand lengths (Figure 1, Section 4). It is promising since it was unclear whether the length generalization on the addition task can be solved reliably with Transformers (Zhou et al., 2024b). \u2022 We theoretically prove by concrete construction that a small (1-layer, 2-head) Transformer equipped with coupled position IDs can add two decimal integers whose lengths are exponential in the embedding dimension (Theorem 5.1). Interestingly, we observe a striking similarity between the attention patterns from our theoretical construction and those extracted from a Transformer trained with a standard optimizer (Section 5.1.1). As a complementary result, we also prove that any 1-layer Transformer without positional information cannot fully solve any permutation-sensitive tasks such as addition (Section 5.2). \u2022 We empirically demonstrate that position coupling can effectively address various tasks beyond addition, including multiplication between $N$ -digit and 2-digit integers (Section 6.1, in which we also provide a theoretical construction of a 2-layer Transformer that solves this task for exponentially large $N$ ). We also verify that position coupling can aid Transformers in learning tasks with multi-dimensional structures (Section 6.2). Moreover, we evaluate position coupling on some other tasks (addition with multiple operands, copy/reverse) in Appendix B. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We focus on decoder-only Transformers that solve the tasks using next-token prediction (See Appendix A for a brief background on it). Since we study deterministic tasks with a unique answer, we consider greedy decoding throughout the paper. ", "page_idx": 1}, {"type": "text", "text": "2.1 Data Formats ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Each task in this work is represented as a collection of sequences of the form \u2018(query) $:=$ (response)\u2019: given a query, our task is to infer the response correctly. Thus, we only care about the result of the next-token prediction for the $\\mathcal{\\bullet}_{=},$ token and the tokens in the response (except its last token). That is, we only compute the losses and accuracies for those output tokens. ", "page_idx": 2}, {"type": "text", "text": "Previous works commonly observe that data formats play an important role in solving downstream tasks with Transformers because a proper data format enables the model to learn a simple function to solve a task. Here we overview some well-known methods we apply, focusing on the addition task. ", "page_idx": 2}, {"type": "text", "text": "Reversed Format. Lee et al. (2024) observe that reversing the response leads to improvement in both performance and sample efficiency. For example, $\\mathbf{^{653}}+49=702^{\\circ}$ becomes $653+49=207^{\\circ}$ \u2019 in a reversed format. This enables a decoder-only Transformer to infer the response from the least significant digit to the most significant digit, similar to how humans add two numbers. ", "page_idx": 2}, {"type": "text", "text": "Zero-padding. Zero-paddings ensure that the length of both operands in a query is the same and the length of a response is fixed when the length of the operand is given. That is, by padding the query and the response of an $M$ -digit $+\\ N$ -digit addition with $\\mathrm{0\\,\\mathrm{\\Omega}}$ , the input sequence becomes a n $\\operatorname{nax}\\{M,N\\}$ -digit addition with $(\\operatorname*{max}\\{M,N\\}+1)$ -digit response. For example, $653+49=702^{\\circ}$ \u2019 becomes $^{\\bullet}653+049=0702^{\\circ}$ . ", "page_idx": 2}, {"type": "text", "text": "Wrapping with BOS/EOS token(s). It is conventional in NLP to put BOS/EOS (beginning-/end-ofsequence) tokens at the beginning/end of the sequence. Lee et al. (2024) use the same token $^{\\bullet}\\S^{\\bullet}$ for BOS and EOS tokens and observe that it is beneficial to wrap each sequence with the $\\mathbb{S}$ token when solving the addition task. We do not observe any significant difference in the performance between sequences with the same and different BOS and EOS tokens. ", "page_idx": 2}, {"type": "text", "text": "2.2 Positional Embeddings/Encodings (PE) ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Vaswani et al. (2017) introduce the absolute positional embedding (APE) to Transformers to inject the positional information into the model. The usual APE works as follows: given an input sequence of tokens, we assign a sequence of consecutive position IDs (integers). Each position ID is mapped to a unique PE vector, and the vector is either added or concatenated to the corresponding token embedding vector. We focus on the learned APE initially proposed by Gehring et al. (2017). ", "page_idx": 2}, {"type": "text", "text": "Length Generalization and PE. It is actively studied whether PE is a crucial factor in solving the length generalization problem of Transformers. Kazemnejad et al. (2023) argue that decoder-only Transformers with no positional encoding (NoPE) can achieve length generalization of downstream tasks since a Transformer decoder can implicitly capture the generalizable positional information due to its causal nature. However, there is a line of works proposing new PE methods to improve the length generalization performance of Transformers (Li et al., 2024; Ruoss et al., 2023). ", "page_idx": 2}, {"type": "text", "text": "3 Position Coupling: A Method for Length Generalization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We propose position coupling, which assigns position IDs that directly encode the structure of given tasks. Here, we explain the general position ID assignment rule of position coupling in two steps. ", "page_idx": 2}, {"type": "text", "text": "First, we partition the tokens of the input sequence. The detailed principles for grouping the tokens differ by task, but the common desiderata are the following: there are two or more groups of consecutive tokens, and each token in a group must have a unique semantic meaning so that a one-to-one correspondence between tokens in different groups can be made. ", "page_idx": 2}, {"type": "text", "text": "Next, for each group of tokens, we assign a sequence of consecutive numbers (usually, positive integers) as position IDs, starting from a random number (at training time) or a fixed predetermined number (at evaluation time). We use random position IDs at training time for inducing length generalization by enabling all position embedding vectors to be trained, up to a pre-defined hyperparameter of maximum position ID (max_pos).1 Very importantly, we assign the same position IDs to the tokens in all groups that are relevant to each other for solving the given task: we refer to this as \u201ccoupling the positions\u201d. Lastly, we set 0 as the position IDs of special tokens like BOS/EOS tokens and the PAD token (padding for minibatch training and evaluation). ", "page_idx": 2}, {"type": "image", "img_path": "5cIRdGM1uG/tmp/b740f63c83ae19451fe7907ef1f4e2c00387add753e866ba8c6c2771e6cbe9c3.jpg", "img_caption": ["Figure 2: Position coupling for decimal integer addition task, displaying $653+49\\,=\\,702$ with appropriate input formats. The starting position ID $\\surd6\\ '$ is an arbitrarily chosen number. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "We illustrate position coupling for the decimal integer addition task (or addition task for short). To study the length generalization of the addition task, we regard each digit (0\u20139) as a single token. We will use an explicit example of the addition $\\mathbf{^{653}}+49=702^{\\circ}$ for illustration. ", "page_idx": 3}, {"type": "text", "text": "Before applying the position coupling, we adopt an input format similar to Lee et al. (2024) so that we reverse the response, but we use zero-padding and wrapping with BOS/EOS token $\\mathbf{\\cdot}\\mathbb{S}^{\\,,}$ at the same time. For example, $\\mathbf{^{653}}+49=702^{\\circ}$ becomes $\\begin{array}{r}{\\mathbf{\\nabla}^{*}\\mathbb{S}653+049=2070\\mathbb{4}^{*}}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "We partition the tokens in the sequence into three groups: (1) first operand $\\&\\ ^{\\star}+^{\\star}$ , (2) second operand, and $(3)\\mathrel{\\mathop:}=\\mathrel{\\mathop:}\\&$ response (which we call \u2018sum\u2019). Then each number token is \u201cunique\u201d in the corresponding group in terms of significance, which naturally induces a one-to-one correspondence between (most of) the tokens across different groups. We group $\\bullet_{=},$ and the sum together because these tokens are where we perform next-token prediction. ", "page_idx": 3}, {"type": "text", "text": "Now we assign the coupled position IDs to the tokens. Most importantly, we assign the same position ID to the digits of the same significance. Let us say that the random starting number is 6. In our example, we assign 6, 7, and 8 to the tokens in the operands, and assign 5, 6, 7, and 8 to the tokens in the sum in a reversed order: see Figure 2. We remark that, first, we assign 9 as position IDs of $^\\star+$ \u2019 and $\\bullet=^{,}$ tokens because they are adjacent to the number token with position ID 8, even if there are no \u2018significances\u2019 for those tokens. Second, we assign 5 as a position ID of the most significant digit of the sum (which may be $\\surd0\\ '$ due to the zero-padding) just because it is next to the number token with position $\\mathrm{ID}\\ 6$ , even though there are no other corresponding tokens in the other groups (operands). We also note that the $\"+\"$ token is not grouped with the second operand and is not given the ID 5; this is to prevent unnecessary coupling between $\"+\"$ and the most significant digit of the sum. ", "page_idx": 3}, {"type": "text", "text": "Remark. A concurrent work by McLeish et al. (2024) proposes an analogous approach for solving arithmetic tasks, while they employ a different input format. We provide a detailed comparison with our work in Appendix A. ", "page_idx": 3}, {"type": "text", "text": "Comparison with Index Hinting. Even though the idea of implanting the structure of a task into the positional encoding is novel, there is an existing approach named index hinting (Zhou et al., 2024a) that applies a similar idea but to the input sequence. Index hinting is an input augmentation technique that places position markers in front of the tokens to couple the semantically relevant tokens. For example, Zhou et al. (2024a) transform $653+49=702^{\\circ}$ \u2019 into $\\mathrm{^{\\bullet}a0b6\\mathrm{c}5d3+a0b0c4d9=a0b7\\mathrm{c}0d2}^{\\circ}$ with some zero-paddings, where a, b, c, and d are consecutive index hints. Here, the starting hint character a is randomly selected during training, similar to our method of choosing the starting position ID. The reversed format and BOS/EOS tokens can be applied as well. ", "page_idx": 3}, {"type": "text", "text": "One way in which index hinting differs from position coupling is that it doubles the input sequence length. This is because the position information and the token information do not merge: the index hints and the normal tokens are mapped to separate token embedding vectors which are alternately placed in the input embedding matrix. As a result, a Transformer must figure out the correspondence between each adjacent pair of an index hint and a normal token. Moreover, the doubled input length requires up to $4\\times$ the training time and memory consumption. In contrast, position coupling explicitly combines token and position information: every token embedding and corresponding position embedding are mixed into a single vector. Hence, a Transformer can effortlessly utilize the positional structure of the task, without hurting the training time. We highlight that, as will be mentioned in Section 4.1, position coupling exhibits better length generalization than index hinting. ", "page_idx": 3}, {"type": "text", "text": "Another difference is that the index hints should be inferred by Transformers in addition to the normal tokens in the response, which might be an additional burden. Our position coupling circumvents this difficulty, eliminating the need to estimate anything other than the tokens in the original response. ", "page_idx": 3}, {"type": "text", "text": "4 Experiments on the Addition Task ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we empirically demonstrate that position coupling allows extensive length generalization of Transformers on the addition task. We delve into the impact of training length and architecture on the length generalization performance and provide comparisons with NoPE, APE with a random starting position ID (we call random-start APE), and index hinting (Zhou et al., 2024a). ", "page_idx": 4}, {"type": "text", "text": "Data Sampling. We opt for the balanced sampling in terms of the number of digits (Nogueira et al., 2021). Given the maximum number of digits $D_{\\mathrm{max}}$ , we do balanced sampling for each operand in two steps. First, we sample the number of digits $D\\in[1,D_{\\operatorname*{max}}]$ uniformly at random. Next, we sample an operand from $[\\dot{1}0^{D-1},10^{D}-1]$ uniformly at random, except for $D=1$ where we sample from [0, 9]. This procedure addresses the imbalance problem in the number of digits of operands. ", "page_idx": 4}, {"type": "text", "text": "Model and Training. We train decoder-only Transformer models from scratch. We properly choose max_pos so that the maximum testable length of summands is 200. We do not use packing or shifting for simplicity of implementation. Since we manually put coupled position IDs with a random starting index during training, we can train all the positions without packing and shifting. We run each experiment 8 times with 2 different random seeds for data generation and 4 different random seeds for model initialization & stochastic optimization unless mentioned otherwise. We summarize all hyperparameters in Appendix C. ", "page_idx": 4}, {"type": "text", "text": "4.1 Results ", "text_level": 1, "page_idx": 4}, {"type": "image", "img_path": "5cIRdGM1uG/tmp/53fb3b2746ff15943e1f8a34d5ff39a60e168403cc47c27398f40c192ccff7ea.jpg", "img_caption": ["Figure 3: Ablation on the trained operand lengths (1-layer 4-head models). "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Longer Trained Sequences Lead to Longer Generalizable Lengths. We train 1-layer 4-head models with $D_{\\mathrm{max}}\\in\\{10,20,30,40\\}$ and evaluate them on up to 200-digit additions.For each run of training, we choose and evaluate the best model in terms of the validation loss for 200-digit additions. The result is showcased in Figure 3. We decide that a model successfully generalizes to a certain length of operands (referred to as \u201cgeneralizable length\u201d) if the median EM accuracy exceeds $95\\%$ . ", "page_idx": 4}, {"type": "text", "text": "We observe that the generalizable length becomes longer as we train on longer training sequences. The generalizable length is 70 for the models trained on additions involving 1\u201310 digits, 135 for models trained on 1\u201320, and 200 for $_{1-30}$ and 1\u201340. We believe that we could achieve even longer generalizable length for the models trained on 1\u201340 if we use a larger max_pos. We note that we could scale up the generalizable length to 500 by training with lengths 1\u2013160: refer to Appendix B.1. Although each test sample contains the operands of the same length, we also provide an extended evaluation on test samples with operands of different lengths: see Appendix B.2. ", "page_idx": 4}, {"type": "image", "img_path": "5cIRdGM1uG/tmp/8d13f09eb4b7fb375c5a4b1dc6f62eb3095a160e5a213fe6c1feab65a0e96fa7.jpg", "img_caption": ["Figure 4: Ablation on the number of layers (trained with position coupling). "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Ablation on Number of Layers. Since Zhou et al. (2024a) and Zhou et al. (2024b) choose 6-layer 8-head models as their base models, we also test our method to deeper models. The results evaluated with models trained on 1\u201330 digits are displayed in Figure 4, whose experimental details are listed in Table 2. Overall, the performances are well extrapolated to test lengths (longer than trained lengths) ", "page_idx": 4}, {"type": "text", "text": "and are similar for the models with 1\u20135 layers. For the 6-layer model, however, the performance slightly deteriorates. We hypothesize that the performance degradation is due to the bad implicit bias of deeper models (learning shortcuts only to achieve in-distribution generalization) when learning a simple algorithm to solve the task. Since the theoretical construction of a 1-layer addition Transformer (that will appear in Section 5.1) naturally extends to larger architectures, deeper models have at least as much generalization capability as shallower models. We believe exploring a theoretical explanation for the bad implicit bias of large models on low-complexity tasks is a promising research direction. We also highlight that we present median accuracies over multiple runs, while Zhou et al. (2024b) report maximum accuracies. To better the comparison, we also report the maximum accuracies (for the experiments in Figures 3 and 4) in Appendix B.3, showing that our 6-layer models can achieve near-perfect generalization for up to 200-digit additions as well. ", "page_idx": 5}, {"type": "image", "img_path": "5cIRdGM1uG/tmp/3fad55aae798a559a429fcd4e48ab614f1625d1ef8633c63c239e42ceb549594.jpg", "img_caption": ["Figure 5: Ablation on the data formats (1-layer 4-head models trained with position coupling). "], "img_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "5cIRdGM1uG/tmp/88a7f63d139fce7f8fd3703dac9b727ca5687bfcfdb1fb330f3d1d20472f354b.jpg", "img_caption": ["Figure 6: Ablation on the data formats (6-layer 8-head models trained with position coupling). "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Ablation on Data Formats. Our input format is primarily selected to simplify the algorithm of solving the addition task, not through extensive ablation studies. Thus, we are not arguing that our choice of input format is empirically optimal for training Transformers. However, since the data format is one of the crucial factors in general machine learning, we test various formats for 1-layer 4-head (Figure 5) and 6-layer 8-head models (Figure 6). The results clearly show that the performance varies with input formats, as they affect the complexity of the algorithm that the model should learn. ", "page_idx": 5}, {"type": "text", "text": "Small models (1-layer 4-head) achieve near-perfect generalization when the numbers are zero-padded and the response or all the numbers are reversed. We believe this is because the combination of zeropadding and reversing enabled a small Transformer to learn a simple length-generalizing algorithm. Zero-padding seems crucial since it aids length generalization to some extent even without reversing. Without reversing any numbers, however, even the in-distribution performance slightly decays. ", "page_idx": 5}, {"type": "text", "text": "Larger models (6-layer 8-head) perform better than small models when the numbers are no longer zero-padded or reversed. We believe this is because the task-solving algorithm without reversing or zero-padding that the model should learn is more sophisticated, which larger models can learn more easily. Contrarily, we observe a slight degradation in performance when we add zero-padding and reversing in the larger model, which suggests that the model may have learned a \u201cshortcut\u201d due to its (overly) strong expressive power relative to the problem\u2019s complexity. ", "page_idx": 5}, {"type": "text", "text": "Comparison with NoPE and APE (with random starting position ID). Our experiments demonstrate that simple PE methods like NoPE and random-start APE cannot length-generalize well on the addition task. In particular, we implement random-start APE to mimic the training process with the usual APE combined with packing and shifting. The results showcased in Figure 1 imply that naively training all position embeddings does not necessarily help produce a strictly better model in terms of length generalization than that does not use position embeddings at all. We also remark that even training itself is difficult for shallower models (e.g., 1-layer) with NoPE and random-start APE. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Comparison with Index Hinting. We test index hinting by running the code we implemented ourselves since the original code is unavailable. From Figure 1, we observe that index hinting indeed helps the model to length-generalize more than the baselines (NoPE & random-start APE). However, the generalizable lengths of the models trained with index hinting do not extend further than 50; the models completely fail starting from the length 70. We also observe that Transformers with index hinting require enough depth to achieve high enough training and in-distribution validation accuracies. Particularly, the training accuracies of 1-layer models do not deviate from near zero. Thus, we only present the results for the 6-layer 8-head model as done by Zhou et al. (2024a). ", "page_idx": 6}, {"type": "text", "text": "Comparison & Combination with RoPE. We also examine the possibility of combining position coupling and RoPE (Su et al., 2024): See Appendix B.4 for the experimental results and details. ", "page_idx": 6}, {"type": "text", "text": "5 Theoretical Analyses on 1-layer Transformers ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In the previous section, we provided empirical results exhibiting the outstanding performance of position coupling. One might ask why and how position coupling works so effectively. In Section 5.1, we provide a theoretical explanation by carefully constructing a 1-layer Transformer model that is capable of solving the addition task involving exponentially long operands when the input is encoded with position coupling. We also present the necessity of proper positional information for a 1-layer Transformer to solve the addition task in Section 5.2. ", "page_idx": 6}, {"type": "text", "text": "5.1 1-layer Transformer with Coupled Positions can Perform Long Additions ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "For the sake of simplicity of presentation, we consider a Transformer without any normalization layers, as conventionally done in theoretical constructions by previous works (Awasthi and Gupta, 2023; Yun et al., 2020a,b). For the sake of completeness, readers can find a mathematical formulation of the decoder-only Transformer architecture in Appendix D. ", "page_idx": 6}, {"type": "text", "text": "Theorem 5.1. With the input format described in Section 3.1, there exists a depth-1 two-head decoderonly Transformer with coupled positions that solves the addition task with next-token prediction. Here, the operand length is at most $2^{\\lfloor(d-17)/2\\rfloor}-2$ , where the embedding dimension is $d\\geq21$ . ", "page_idx": 6}, {"type": "text", "text": "We provide our proof in Appendix E. We highlight that our proof is constructive and does not rely on any universal approximation result of neural networks. ", "page_idx": 6}, {"type": "text", "text": "Theorem 5.1 shows that a 1-layer 2-head Transformer is sufficient for implementing addition between two exponentially long integers. We emphasize that this result can be naturally extended to larger architectures with more layers/heads, with the help of residual connections. ", "page_idx": 6}, {"type": "text", "text": "5.1.1 Probing the Attention Patterns in Trained Transformers with Position Coupling ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We discover a striking similarity between the attention patterns in our theoretical construction (Theorem 5.1) and those extracted from a Transformer trained with position coupling and a standard optimizer. In particular, the manually constructed attention patterns described in Tables 11 and 17 in Appendix E closely resemble the actual attention patterns in Figure 7.2 Drawn from this discovery, we claim that a Transformer trained with position coupling spontaneously learns two separate components of the addition task: (1) adding two numbers without carries, and (2) predicting the carries. ", "page_idx": 6}, {"type": "text", "text": "Let us revisit the example in Figure 2 and consider predicting $\\surd7\\$ (position ID 6) as the next token of $\\surd0\\ '$ (position ID 7). Note that the token $\\surd7\\ r$ is the result of combining the digit-wise sum $_{6+0=6}$ and a propagated carry 1. To find out the sum without carry, it is enough for the model to attend to the two previous positions with ID 6: tokens $\\surd6\\ '$ and $\\surd0\\ '$ . On the other hand, to predict the carry, the model may attend to the three positions with ID 7: tokens $\\surd5\\textrangle$ , $\\cdot_{4},$ , and $\\surd0^{\\circ}$ . The reason why we should care about $\\surd0\\ '$ is that considering the sum $5{+}4$ $(=\\!9)$ of the two digits in the operands is not sufficient to determine the existence of the carry. By looking at the token $\\surd0\\ '$ in the response (with position ID 7), we can detect that the actual sum in this position is 10 $\\scriptstyle(=5+4+1$ , where 1 is another carry propagated from the previous position) and hence we need to propagate a carry $\\underline{{1}}$ to the next position (with ID 6). ", "page_idx": 6}, {"type": "image", "img_path": "5cIRdGM1uG/tmp/7670b49236d86387d421d4d3c8927052b0d39d3a1b9b67be71b9623138cce05b.jpg", "img_caption": ["Figure 7: Probing attention matrices of a 1-layer 2-head Transformer with position coupling, trained on up to 5-digit additions. (Left) There are two heatmaps (clipped to zero below 0.01) corresponding to the (transposed) attention matrices observed from the attention heads. Averaged over 10K sequences of 6-digit additions. (Right) We magnify parts of the attention matrices that are involved in inferring the response (sum). The arrows explain the process of inferring the next token $\\surd0^{\\circ}$ from $\\varsigma_{3},$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Now we inspect the aforementioned claim by examining the attention matrices of an actual trained Transformer. In the model, we discover two different patterns of attention matrices,3 playing distinct roles. The first attention pattern (top of the figure) seems to correspond to the addition without carries: each token in the response (including $\\bullet_{=}\\bullet$ ) attends to two positions needed to find out the sum without carry. Conversely, the second attention pattern (bottom of the figure) seems to correspond to the carry prediction: again, each token in the response attends to three positions required to find out the carry. ", "page_idx": 7}, {"type": "text", "text": "Remark. Similarly to our analysis, Quirke and Barez (2024) study the attention patterns of a 1-layer 3-head decoder-only Transformer model trained solely on 5-digit addition. They also observe that each head handles different subtasks of addition, such as digit-wise summation and carry detection. ", "page_idx": 7}, {"type": "text", "text": "5.2 1-layer Transformers Require Positional Information ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In Section 4.1, we observed that 1-layer Transformers fail to perform the addition task without position coupling. Here, we provide a partial result that theoretically explains why this happens inevitably, particularly in the case of NoPE. We start with a general proposition: a 1-layer Transformer without positional encoding cannot distinguish queries that are identical up to permutation when inferring the first token of the response using greedy next-token prediction. ", "page_idx": 7}, {"type": "text", "text": "Proposition 5.2. Consider any depth-1 finite-head decoder-only Transformer model $\\tau$ without positional encoding (NoPE). Given an input sequence $\\mathcal{T}$ and its arbitrary permutation ${\\mathcal{Z}}^{\\prime}$ , if the last tokens of $\\mathcal{Z}$ and ${\\mathcal{Z}}^{\\prime}$ are identical, then the next tokens predicted by $\\tau$ will also be identical for both sequences when applying a greedy decoding scheme. ", "page_idx": 7}, {"type": "text", "text": "The proof is deferred to Appendix F. According to the proposition above, the 1-layer Transformer without positional encoding will always output the same values starting from the $\\mathcal{\\bullet}_{=},$ token, provided that the combination of query tokens is identical, even if their order varies. However, the addition task is permutation-sensitive, meaning that the permuted queries may result in different responses. Therefore, the 1-layer Transformer cannot completely solve the task without positional encoding. It is important to note that this result remains unchanged regardless of the input format: neither reversed format nor index hinting provides any benefti. We also highlight that this impossibility result can be extended to any other permutation-sensitive tasks, such as arithmetic tasks and copy/reverse tasks. ", "page_idx": 7}, {"type": "text", "text": "Based on this, we write code to directly calculate the maximum EM accuracy on the $m$ -digit addition task that a 1-layer decoder-only Transformer can achieve (see Appendix F for the code). The accuracies rapidly decrease to zero: $6.2\\%$ for 3-digit addition, $1\\%$ for 4-digit integers, and $0.13\\%$ for 5-digit integers. We leave it for future work to investigate the necessary conditions of the architecture for implementing addition when other positional encoding schemes are employed. ", "page_idx": 7}, {"type": "text", "text": "6 Applying Position Coupling Beyond Addition Task ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To demonstrate the versatility of position coupling, we consider two other tasks in this section: $N\\times2$ multiplication and a two-dimensional (2D) task. Other example tasks (e.g., addition with multiple summands, copy/reverse allowing duplicates) can be found in Appendix B. ", "page_idx": 8}, {"type": "text", "text": "6.1 Position Coupling for $N\\times2$ Multiplication Tasks ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Here, we study length generalization on the $N$ -digit $\\times\\ 2$ -digit multiplication task in terms of the length $N$ of the first operand, while fixing the length of the second operand by 2. Similar tasks have been studied before (Duan and Shi, 2023; Jelassi et al., 2023); we discuss further in Appendix A. ", "page_idx": 8}, {"type": "text", "text": "We reverse and zero-pad the response, setting the length of it as $N+2$ . We couple the position starting from the least significant digits of both operands and response, decrementing the ID as we move to their most significant digits: see Figure 17 in Appendix B.5. The experimental results showcased in Figure 8 verify the efficacy of position coupling compared to NoPE and random-start APE. We observe that a 1-layer model fails even with position coupling, even for training. However, as the depth increases to 2 or more, it immediately becomes capable of length generalization. ", "page_idx": 8}, {"type": "image", "img_path": "5cIRdGM1uG/tmp/a146c7eefc003bf44ae60abe2b72a6c3a38393e3fc3e0a690224d6ea579155f7.jpg", "img_caption": ["Figure 8: $N\\times2$ multiplication task, trained on sequences of length 1\u201340. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Unlike addition, position coupling for $N\\times2$ multiplication is less intuitive, as predicting the token in the middle of the response requires multiple digits from both operands while each token in the response is linked with at most 2 tokens in the query. Perhaps surprisingly, we can still construct a Transformer that provably solves this task for exponentially long sequences. ", "page_idx": 8}, {"type": "text", "text": "Theorem 6.1. Given an appropriate format of the input sequence, there exists a depth-2 decoder-only Transformer model with coupled positions that can perform the $N\\times2$ multiplication task with next-token prediction. Here, the number of the total heads is $I O$ and the length of the first operand is at most $2^{\\lfloor(\\stackrel{\\cdot}{d}-34)/6\\rfloor}-3$ , where we denote the token embedding dimension by $d\\geq46$ . ", "page_idx": 8}, {"type": "text", "text": "We defer the proof to Appendix G. This result suggests that the proposed position coupling scheme for the $N\\times2$ multiplication task sufficiently captures the inherent structure of the task, and thus provides the potential for the trained model to generalize across unseen lengths. Also, we believe that Theorem 6.1 is optimal in terms of the number of attention layers, as the depth-1 model exhibits total failure even for in-distribution samples in our experiment. ", "page_idx": 8}, {"type": "text", "text": "6.2 Two-dimensional Position Coupling for Minesweeper Generator Task ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Now, we investigate the extension of position coupling for handling a 2D task, where the query and the response are originally 2D objects. In particular, we define and investigate a task we call minesweeper generator. Given a rectangular board where each cell is fliled with either $\\mathbf{\\ddot{M}}$ (mine) or $*\\!\\;,$ (an empty cell), the task is to generate a new board of the same size, having each cell filled with: ", "page_idx": 8}, {"type": "text", "text": "\u2022 $\\mathbf{\\omega}^{\\bullet}\\mathbf{M}^{\\bullet}$ , if the corresponding cell in the original board contains $\\mathbf{\\omega}^{\\bullet}\\mathbf{M}^{\\bullet}$ ;   \n\u2022 The count of mines in 8 adjacent cells, if the corresponding cell in the original board contains $\\mathbf{\\omega}^{\\star}\\ast\\mathbf{\\omega}^{\\star}$ . ", "page_idx": 8}, {"type": "text", "text": "Data Format & Position Coupling. We introduce two position coupling modules: one for the row direction and another for the column direction. Following this, we flatten the board to feed it into a Transformer: see Figure 9. Within the model, an embedding vector for each token (cell) is generated by adding the token embedding vector and corresponding two PE vectors. ", "page_idx": 8}, {"type": "image", "img_path": "5cIRdGM1uG/tmp/0e30d6fa4550d8332907768e520a8dc058583a6679ba3e089b676f893939e816.jpg", "img_caption": ["Figure 9: Position coupling for the two-dimensional \u2018minesweeper generator\u2019 task. (Left) The idea of assigning coupled position IDs. (Right) The model receives a flattened sequence of input tokens and two-dimensional position IDs. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Experiments. To assess the efficacy of position coupling, we contrast its performance with NoPE. The training samples are designed with the width and height of the board between 5 and 9 inclusively. We allow the width and height to be different for training samples. We evaluate the test performance on a square board with a width between 5 and 14 inclusively. We also employ a 4-layer 8-head model for position coupling and a 6-layer 8-head model for NoPE. In particular, for position coupling, we use the same embedding layer for both position coupling modules, as this approach empirically performs better than using distinct embedding layers for each module (see Appendix B.8). ", "page_idx": 9}, {"type": "text", "text": "The experimental results are described in Figure 10. Position coupling maintains over $98\\%$ accuracy until a width of 12 and near $90\\%$ accuracy even at a width of 14. In contrast, NoPE fails even for in-distribution samples. One might be concerned that the generalizable length of 12 seems only slightly higher than the trained length of 9. However, we stress that our query is a 2D board, therefore the actual length generalization is from 81 to 144. ", "page_idx": 9}, {"type": "image", "img_path": "5cIRdGM1uG/tmp/e9ba3a123c6fde2dc9dfdf7d2e1bc1e69ebebe415cdcd55305e2a4caa40db1c6.jpg", "img_caption": ["Figure 10: Minesweeper generator task, trained on sequences of length $(5{-}9){\\times}(5{-}9)$ . "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Achieving length generalization of Transformers even in the simple case of the addition task has been a challenge that received a lot of attention. We propose position coupling, a variant of learned APE, which enables capturing task structure to improve the length generalization performance of Transformers for addition. We show that a Transformer trained on 1\u201330 digit addition can generalize up to 200-digit addition. We also provide the construction of a 1-layer Transformer model capable of adding two exponentially long integers when position coupling is applied. Furthermore, we verify the efficacy of position coupling for length generalization in other arithmetic and algorithmic tasks. ", "page_idx": 9}, {"type": "text", "text": "Limitations & Future Directions. We intentionally limited ourselves to the tasks with an explicit structure between the tokens in each sequence. This is because we are proposing a method to instill the known structure of the task into a Transformer by training on short sequences. Designing the coupling of positions for tasks whose structure is implicit or black-box (e.g., for general NLP tasks) remains a fascinating next step: we leave the methodology for uncovering hidden structures and autonomously creating appropriate couplings (without manually designing them) for future work. ", "page_idx": 9}, {"type": "text", "text": "We also leave two challenging arithmetic tasks to length-generalize for future work. One is the addition with a varying number of summands, i.e., determining if the model can generalize to summing multiple integers when trained on samples with fewer summands. The second task is multiplication, where the lengths of both operands can vary. Note that our method is further extended to solve these two challenging length generalization problems in a recent work (Cho et al., 2024). ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was partly supported by a National Research Foundation of Korea (NRF) grant (No. RS-2024-00421203) funded by the Korean government (MSIT), and an Institute for Information & communications Technology Planning & Evaluation (IITP) grant (No.RS-2019-II190075, Artificial Intelligence Graduate School Program (KAIST)) funded by the Korean government (MSIT). HC, JC, and CY acknowledge support from a Google Gift on the research related to Long Context Transformers. The experiments contained in this work were supported in part through a Google Cloud Platform Credit Award. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Emmanuel Abbe, Samy Bengio, Aryo Lotf,i and Kevin Rizk. Generalization on the unseen, logic reasoning and degree curriculum. In International Conference on Machine Learning, pages 31\u201360. PMLR, 2023. A.2 ", "page_idx": 10}, {"type": "text", "text": "Kartik Ahuja and Amin Mansouri. On provable length and compositional generalization. arXiv preprint arXiv:2402.04875, 2024. A.2   \nCem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. Exploring length generalization in large language models. Advances in Neural Information Processing Systems, 35:38546\u201338556, 2022. 1   \nPranjal Awasthi and Anupam Gupta. Improving length-generalization in transformers via task hinting. arXiv preprint arXiv:2310.00726, 2023. 5.1   \nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. 3, F   \nHanseul Cho, Jaeyoung Cha, Srinadh Bhojanapalli, and Chulhee Yun. Arithmetic transformers can length-generalize in both operand length and count. arXiv preprint arXiv:2410.15787, 2024. 7   \nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1\u2013113, 2023. 1   \nYann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks. In International conference on machine learning, pages 933\u2013941. PMLR, 2017. D   \nGregoire Deletang, Anian Ruoss, Jordi Grau-Moya, Tim Genewein, Li Kevin Wenliang, Elliot Catt, Chris Cundy, Marcus Hutter, Shane Legg, Joel Veness, and Pedro A Ortega. Neural networks and the chomsky hierarchy. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id $\\fallingdotseq$ WbxHAzkeQcn. 1   \nShaoxiong Duan and Yining Shi. From interpolation to extrapolation: Complete length generalization for arithmetic transformers. arXiv preprint arXiv:2310.11984, 2023. 6.1, A.2   \nDan Friedman, Alexander Wettig, and Danqi Chen. Learning transformer programs. Advances in Neural Information Processing Systems, 36, 2023. 10   \nJonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional sequence to sequence learning. In International conference on machine learning, pages 1243\u20131252. PMLR, 2017. 2.2   \nGemini, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 1   \nDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. D ", "page_idx": 10}, {"type": "text", "text": "Kevin Jarrett, Koray Kavukcuoglu, Marc\u2019Aurelio Ranzato, and Yann LeCun. What is the best multi-stage architecture for object recognition? In 2009 IEEE 12th international conference on computer vision, pages 2146\u20132153. IEEE, 2009. D ", "page_idx": 11}, {"type": "text", "text": "Samy Jelassi, St\u00e9phane d\u2019Ascoli, Carles Domingo-Enrich, Yuhuai Wu, Yuanzhi Li, and Fran\u00e7ois Charton. Length generalization in arithmetic transformers. arXiv preprint arXiv:2306.15400, 2023. 6.1, A.2   \nAmirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva Reddy. The impact of positional encoding on length generalization in transformers. Advances in Neural Information Processing Systems, 36, 2023. 1, 1, 2.2, C   \nJeonghwan Kim, Giwon Hong, Kyung-min Kim, Junmo Kang, and Sung-Hyon Myaeng. Have you seen that number? investigating extrapolation in question answering models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7031\u20137037, 2021. 1   \nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015. 1   \nNayoung Lee, Kartik Sreenivasan, Jason D. Lee, Kangwook Lee, and Dimitris Papailiopoulos. Teaching arithmetic to small transformers. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id $=$ dsUB4bst9S. 1, 1, 2.1, 3.1, A.2   \nAitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:3843\u20133857, 2022. 1   \nShanda Li, Chong You, Guru Guruganesh, Joshua Ainslie, Santiago Ontanon, Manzil Zaheer, Sumit Sanghai, Yiming Yang, Sanjiv Kumar, and Srinadh Bhojanapalli. Functional interpolation for relative positions improves long context transformers. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id $\\equiv$ rR03qFesqk. 2.2   \nDavid Lindner, J\u00e1nos Kram\u00e1r, Sebastian Farquhar, Matthew Rahtz, Tom McGrath, and Vladimir Mikulik. Tracr: Compiled transformers as a laboratory for interpretability. Advances in Neural Information Processing Systems, 36, 2023. 10   \nSean McLeish, Arpit Bansal, Alex Stein, Neel Jain, John Kirchenbauer, Brian R. Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, Jonas Geiping, Avi Schwarzschild, and Tom Goldstein. Transformers can do arithmetic with the right embeddings. arXiv preprint arXiv:2405.17399, 2024. 3.1, A.2   \nVinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th international conference on machine learning (ICML-10), pages 807\u2013814, 2010. D   \nRodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. Investigating the limitations of transformers with simple arithmetic tasks. arXiv preprint arXiv:2102.13019, 2021. 1, 4   \nOpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1   \nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019. C   \nOfir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=R8sQPpGCv0. 1   \nPhilip Quirke and Fazl Barez. Understanding addition in transformers. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id $=$ rIx1YXVWZb. 5.1.1   \nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1\u201367, 2020. C   \nAnian Ruoss, Gr\u00e9goire Del\u00e9tang, Tim Genewein, Jordi Grau-Moya, R\u00f3bert Csord\u00e1s, Mehdi Bennani, Shane Legg, and Joel Veness. Randomized positional encodings boost length generalization of transformers. arXiv preprint arXiv:2305.16843, 2023. 2.2, 1   \nNoam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. 1, D   \nRuoqi Shen, S\u00e9bastien Bubeck, Ronen Eldan, Yin Tat Lee, Yuanzhi Li, and Yi Zhang. Positional description matters for transformers arithmetic. arXiv preprint arXiv:2311.14737, 2023. A.2   \nJianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 4.1, B.4   \nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022. 1   \nTrieu H. Trinh, Yuhuai Wu, Quoc V. Le, He He, and Thang Luong. Solving olympiad geometry without human demonstrations. Nature, 627(8004):E8\u2013E8, 2024. doi: 10.1038/s41586-024-07115-7. URL https://doi.org/10.1038/s41586-024-07115-7. 1   \nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 1, 2.2, C, D   \nGail Weiss, Yoav Goldberg, and Eran Yahav. Thinking like transformers. In International Conference on Machine Learning, pages 11080\u201311090. PMLR, 2021. 10   \nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, et al. Huggingface\u2019s transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019. C   \nYuhuai Wu, Albert Qiaochu Jiang, Wenda Li, Markus Rabe, Charles Staats, Mateja Jamnik, and Christian Szegedy. Autoformalization with large language models. Advances in Neural Information Processing Systems, 35:32353\u201332368, 2022. 1   \nChangnan Xiao and Bing Liu. Conditions for length generalization in learning reasoning skills. arXiv preprint arXiv:2311.16173, 2023. A.2   \nChangnan Xiao and Bing Liu. A theory for length generalization in learning to reason. arXiv preprint arXiv:2404.00560, 2024. A.2   \nRuibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture. In International Conference on Machine Learning, pages 10524\u201310533. PMLR, 2020. C   \nChulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. Are transformers universal approximators of sequence-to-sequence functions? In International Conference on Learning Representations, 2020a. URL https://openreview.net/forum?id= ByxRM0Ntvr. 5.1, D, D   \nChulhee Yun, Yin-Wen Chang, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. O(n) connections are expressive enough: Universal approximability of sparse transformers. Advances in Neural Information Processing Systems, 33:13783\u201313794, 2020b. 5.1   \nBiao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019. C, 1, F   \nChiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107\u2013115, 2021. G.8, G.8   \nYi Zhang, Arturs Backurs, Sebastien Bubeck, Ronen Eldan, Suriya Gunasekar, and Tal Wagner. Unveiling transformers with LEGO: A synthetic reasoning task, 2023. URL https: //openreview.net/forum?id $^{=1}$ 1jDN-RfQfrb. 1   \nHattie Zhou, Arwen Bradley, Etai Littwin, Noam Razin, Omid Saremi, Joshua M. Susskind, Samy Bengio, and Preetum Nakkiran. What algorithms can transformers learn? a study in length generalization. In The Twelfth International Conference on Learning Representations, 2024a. URL https://openreview.net/forum?id $\\equiv$ AssIuHnmHX. 1, 1, 3.1, 4, 4.1, 4.1, A.2, B.7, 10   \nYongchao Zhou, Uri Alon, Xinyun Chen, Xuezhi Wang, Rishabh Agarwal, and Denny Zhou. Transformers can achieve length generalization but not robustly. arXiv preprint arXiv:2402.09371, 2024b. 1, 1, 1.1, 4.1, A.2, B.3, C ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Introduction 2   \n1.1 Summary of Contributions 2 ", "page_idx": 14}, {"type": "text", "text": "2 Preliminaries 2 ", "page_idx": 14}, {"type": "text", "text": "2.1 Data Formats 3   \n2.2 Positional Embeddings/Encodings (PE) 3 ", "page_idx": 14}, {"type": "text", "text": "3 Position Coupling: A Method for Length Generalization 3 ", "page_idx": 14}, {"type": "text", "text": "3.1 Position Coupling for Decimal Integer Addition Task 4 ", "page_idx": 14}, {"type": "text", "text": "4 Experiments on the Addition Task 5   \n4.1 Results . 5 ", "page_idx": 14}, {"type": "text", "text": "5 Theoretical Analyses on 1-layer Transformers 7 ", "page_idx": 14}, {"type": "text", "text": "5.1 1-layer Transformer with Coupled Positions can Perform Long Additions 7   \n5.1.1 Probing the Attention Patterns in Trained Transformers with Position Coupling 7   \n5.2 1-layer Transformers Require Positional Information 8 ", "page_idx": 14}, {"type": "text", "text": "6 Applying Position Coupling Beyond Addition Task 9 ", "page_idx": 14}, {"type": "text", "text": "6.1 Position Coupling for $N\\times2$ Multiplication Tasks 9   \n6.2 Two-dimensional Position Coupling for Minesweeper Generator Task 9 ", "page_idx": 14}, {"type": "text", "text": "7 Conclusion 10 ", "page_idx": 14}, {"type": "text", "text": "A Omitted Backgrounds 17 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Next-token Prediction with Decoder-only Transformers 17   \nA.2 Related Works . . 17 ", "page_idx": 14}, {"type": "text", "text": "B More Applications & Experiments of Position Couping 19 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Decimal Integer Addition Task: Scale-up to Length of 500 19   \nB.2 Decimal Integer Addition Task: Operands of Different Lengths 19   \nB.3 Decimal Integer Addition Task: Maximum Exact-Match Accuracies 20   \nB.4 Decimal Integer Addition Task: Comparison & Combination with RoPE 20   \nB.5 Position Coupling for $N\\times2$ Multiplication Tasks . . 21   \nB.6 Addition Task with Multiple Summands 21   \nB.7 Position Coupling for Copy/Reverse Tasks . . 22   \nB.8 Position Coupling for Minesweeper Generator Tasks 22 ", "page_idx": 14}, {"type": "text", "text": "C Experiment Details and Hyperparameters 24 ", "page_idx": 14}, {"type": "text", "text": "D Decoder-only Transformer Architecture 29 ", "page_idx": 14}, {"type": "text", "text": "E Formal Construction of Addition Transformer with Position Coupling 30 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "E.1 Notation 30   \nE.2 Input Sequence 30   \nE.3 Encoding Function 32   \nE.3.1 Token Embedding 32   \nE.3.2 Coupled Position IDs and Position Embedding 32   \nE.4 Transformer Block \u2014 Causal Attention Layer . 33   \nE.4.1 Attention Head 1: Digit-wise Addition without Carries 33   \nE.4.2 Attention Head 2: Carry & EOS Detection 36   \nE.4.3 Residual Connection 40   \nE.5 Transformer Block \u2014 Token-wise Feed-forward Layer 41   \nE.5.1 Subnetwork 1: Construction for SUM (dimension 7\u201316). 42   \nE.5.2 Subnetwork 2: Construction for IS_EOS (dimension 17). 43   \nE.5.3 Residual Connection 44   \nE.6 Decoding Function . 44 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "F Impossibility of Addition with No Positional Encoding 46 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "G (Formal) Construction of $N\\times2$ Multiplication Transformer with Position Coupling 48 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "G.1 Notation . . 48   \nG.2 Input Sequence 48   \nG.3 Encoding Function 49   \nG.3.1 Token Embedding 49   \nG.3.2 Coupled Position IDs and Position Embedding 50   \nG.4 Construction Idea . . 51   \n5 Transformer Block 1 \u2014 Causal Attention Layer 52   \nG.5.1 Attention Head 1: Detecting the Ones Digit of the Second Operand 53   \nG.5.2 Attention Head 2: Detecting the Tens Digit of the Second Operand 54   \nG.5.3 Attention Head 3: Position Masking . . 56   \nG.5.4 Residual Connection 58   \nG.6 Transformer Block 1 \u2014 Token-wise Feed-forward Layer 58   \nG.6.1 Residual Connection 59   \nG.7 Transformer Block 2 \u2014 Causal Attention Layer . . 59   \nG.7.1 Attention Head 1: Copying the Ones Digit of the Second Operand 60   \nG.7.2 Attention Head 2: Copying the Tens Digit of the Second Operand 61   \nG.7.3 Attention Head 3: Copying the Appropriate Digit from the First Operand I 62   \nG.7.4 Attention Head 4: Copying the Appropriate Digit from the First Operand II 64   \nG.7.5 Attention Head 5: Copying the Appropriate Digit from the First Operand III 66   \nG.7.6 Attention Head 6: Copying the Appropriate Digit from the First Operand IV 68   \nG.7.7 Attention Head 7: Copying the Appropriate Digit from the First Operand V 69   \nG.7.8 Residual Connection 71   \nG.8 Transformer Block 2 \u2014 Token-wise Feed-forward Layer 72   \nG.8.1 Residual Connection 74   \nG.9 Decoding Function 74 ", "page_idx": 15}, {"type": "text", "text": "A Omitted Backgrounds ", "text_level": 1, "page_idx": 16}, {"type": "image", "img_path": "5cIRdGM1uG/tmp/0c0d5c807fa37fb54d91f5b1d4bcaddcd69b225243b3114adfbca06745cb1679.jpg", "img_caption": ["A.1 Next-token Prediction with Decoder-only Transformers "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 11: Schematic of solving an integer addition task instance using next-token prediction with a decoder-only Transformers. BOS/EOS mean beginning-/end-of-sequence tokens, respectively. PAD means a padding token, used for matching the sequence lengths in a single minibatch of sequences. Here we assume a basic input format (plain, no zero-padding), which is different from that we used in our experiment. ", "page_idx": 16}, {"type": "text", "text": "A decoder-only Transformer returns an output sequence of the same length as the input sequence. One difference from a Transformer encoder is that the attention mechanism in a Transformer decoder occurs only in a single forward direction due to the causal attention mask. Due to this causal nature, the Transformer decoder is mostly used for inferring the next token of each token, just based on the information of the current and the previous tokens. ", "page_idx": 16}, {"type": "text", "text": "A.2 Related Works ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Length Generalization in the Addition Tasks. Lee et al. (2024) observe that reversing the output in the addition task enables the model to learn a simple function. Shen et al. (2023) propose \u201cRandom Spacing\u201d and \u201cRecursive Scratchpad\u201d, achieving near-perfect generalization from 10-digits to 12- digits addition. Zhou et al. (2024a) introduce \u201cindex hints\u201d, position markers placed in front of each token, in both the input and output of addition tasks. Most recently, Zhou et al. (2024b) demonstrate a possibility of extrapolation to the length 100 with training length 1\u201340 in the addition task by combining appropriate input format and advanced PE, yet they also observe that the performances are not robust and highly depend on the random seeds. ", "page_idx": 16}, {"type": "text", "text": "Length Generalization in the $N\\times M$ Multiplication Task $M$ is fixed). Jelassi et al. (2023) investigate $N\\times3$ using an encoder-only model and Duan and Shi (2023) study $N\\times1$ with an encoder-decoder Transformer architecture. Besides the architectural difference, Jelassi et al. (2023) fail to observe length generalization with RPE and only achieve it by supplementing a small number of long samples to the training set. Furthermore, although Duan and Shi (2023) provide perfect length generalization results even for test samples $10\\times$ longer than those observed during training, their approach requires a retraining step with hand-crafted bias correction on attention score matrices. ", "page_idx": 16}, {"type": "text", "text": "Analyzing Length Generalization in Theoretical Perspectives. An emerging line of research seeks to theoretically address why length generalization is difficult and under what conditions it can be achieved. In Abbe et al. (2023), the authors demonstrate that various neural network models have an implicit bias towards min-degree interpolators, which may not be ideal for various reasoning tasks. Xiao and Liu (2023, 2024) investigate problems whose reasoning processes can be formulated as directed acyclic graph (DAG) structures, introducing the concept of maximal input element distance to identify a sufficient condition for length generalization. Recently, Ahuja and Mansouri (2024) formulate the conditions of function classes required to guarantee the length generalization of the empirical risk minimizer function. ", "page_idx": 16}, {"type": "text", "text": "Comparison with McLeish et al. (2024) A very recent concurrent work by McLeish et al. (2024) proposes a new position embedding method called \u201cAbacus\u201d. From a methodological perspective, Abacus is almost identical to our position coupling except for two main differences: Abacus reverses both the query and the response and does not use padding. From now on, we outline the differences between their work and ours beyond the methodology. ", "page_idx": 16}, {"type": "text", "text": "In terms of the model architecture, they use a depth-16 decoder-only Transformer model. They combine their method with looped Transformers and input injection and report an improved performance. In contrast, our main results are obtained with shallower models (up to 6 layers) with standard Transformer architecture of stacked decoder layers. ", "page_idx": 17}, {"type": "text", "text": "Besides the addition task, they study multiplication, sorting, and Bitwise OR. On the other hand, we study multiplication, triple addition, copy/reverse, and a 2D task. Specifically, for the multiplication task, their study mainly considers the case where the length of both operands could vary up to 15. In contrast, we focus solely on the $N\\times2$ task, fixing the length of the second operand by 2. While we achieve length generalization up to 90-digit multiplication by training the model on up to 40-digit multiplication, they report near-perfect in-distribution performance but poor length generalization. ", "page_idx": 17}, {"type": "text", "text": "Finally and notably, we provide novel theoretical analyses, including (1) the constructive proof that a depth-1 Transformer equipped with position coupling can completely solve the addition task for exponentially long digits and (2) the impossibility of the same model being capable of the addition task. We also present theoretical results for the $N\\times2$ multiplication task. ", "page_idx": 17}, {"type": "text", "text": "B More Applications & Experiments of Position Couping ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "B.1 Decimal Integer Addition Task: Scale-up to Length of 500 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Here, we demonstrate the scalability of our proposed position coupling approach for large lengths of up to 500. Specifically, we again train a depth-1 decoder-only model for the addition task and evaluate the performance for instances with up to 500 digits. The results are shown in Figure 12. We notice that at a train length of 160, we achieve excellent length generalization for 500-digit addition. On the other hand, training on sequences of length up to 40 or 80 is insufficient for extreme length generalization. The results demonstrate that position coupling, as an approach, is highly scalable. ", "page_idx": 18}, {"type": "image", "img_path": "5cIRdGM1uG/tmp/34a96da3857547f8b82f1ba3a1ac242ced1d1d31c7c24166a0ae00b41c5e7830.jpg", "img_caption": ["Figure 12: The exact-match accuracies obtained by training a depth-1 transformer on the addition task. We see that while training with sequences of length up to 40 and 80 is insufficient for generalization to large lengths, at training length 160 we achieve strong performance for lengths up to 500. The experimental details can be found in Table 3. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "B.2 Decimal Integer Addition Task: Operands of Different Lengths ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In the main text, we mainly focus on the test examples of additions where the lengths of both operands are the same. For the sake of completeness, we also report the evaluation results for the cases where the operand lengths can be different (although the zero-padding is applied to ensure the consistency of the data format). See Figure 13. ", "page_idx": 18}, {"type": "image", "img_path": "5cIRdGM1uG/tmp/4dc42130b8c6c44f6f3d8b924e23d6d6bac69034c1974e84d9034096288378ed.jpg", "img_caption": ["Figure 13: Exact-match accuracies $(\\%)$ on additions with operands of different lengths. Different heatmap corresponds to different trained length of operands (1\u201310, 1\u201320, 1\u201330, and 1\u201340, expressed with a red box for each). For each heatmap, the $x$ -axis and the $y$ -axis are for the length of the first and the second operand, respectively. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "B.3 Decimal Integer Addition Task: Maximum Exact-Match Accuracies ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "A prior work by Zhou et al. (2024b) provides a similar analysis on the addition tasks as ours. Combining appropriate input format and advanced PE, they achieve ${\\ge}98\\%$ EM accuracy for 100-digit additions with a 6-layer 8-head model trained on 1\u201340. Moreover, they achieve a generalizable length of 45 for a model trained on 1\u201330, 25 for 1\u201320, and 10 for 1\u201310 (no length generalization). One big difference between their analysis and ours is they report the maximum accuracy for each testing length over trials, while we report the medians. Thus, we choose a bit lower threshold $(95\\%)$ for generalizability than theirs. For a better comparison with Zhou et al. (2024b), we report the maximum exact-match (EM) accuracies. See Figures 14 and 15. ", "page_idx": 19}, {"type": "image", "img_path": "5cIRdGM1uG/tmp/e3aede21440e1aa2ca87a9bf884ad50bd77be11a9060cc31eef7515e332de835.jpg", "img_caption": ["Figure 14: Ablation on the trained lengths (1-layer 4-head model trained with position coupling). Here, we report maximum EM accuracies over 8 runs for each tested length. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "5cIRdGM1uG/tmp/cc861c76686efa47b772fb5092befaba75b93ca6420c7232626b7391bcb94bad.jpg", "img_caption": ["Figure 15: Ablation on the number of layers (trained with position coupling). Here, we report maximum EM accuracies over 8 runs for each tested length. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "B.4 Decimal Integer Addition Task: Comparison & Combination with RoPE ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We examine further the possibility of combining our position coupling and RoPE (Su et al., 2024). ", "page_idx": 19}, {"type": "text", "text": "RoPE incorporates the positional information into the key and the query vectors by rotating them. Suppose a position ID $m$ is assigned to a key vector $\\pmb{k}$ , for every two consecutive entries of $^k$ (i.e., $(k_{2i-1},k_{2i}))$ , we rotate by a predefined angle $\\theta_{i}$ multiplied by $m$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\binom{k_{2i-1}}{k_{2i}}\\mapsto\\binom{k_{2i-1}\\cos m\\theta_{i}-k_{2i}\\sin m\\theta_{i}}{k_{2i-1}\\sin m\\theta_{i}+k_{2i}\\cos m\\theta_{i}}\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We also apply a similar rotation to the query vectors (say, $\\pmb q$ with position $\\mathbf{ID}\\;n_{\\rightmoon}^{\\rightmoon}$ ). As a result, the attention score for this key-query pair becomes a function of $k,q,$ , and the relative distance $n-m$ . ", "page_idx": 19}, {"type": "text", "text": "Unlike the original implementation of RoPE, we apply rotations based on the re-assigned position IDs according to our position coupling method. We incorporate this RoPE variant into every attention head of every layer. One more difference in implementation is that, during training, we randomly sampled an integer scaler $\\ell\\in\\{1,2,3,4,5\\}$ and multiplied it by the rotation angle. By such random re-scaling of rotation angles, we expect the model could handle unseen large rotation angles at test time. ", "page_idx": 19}, {"type": "text", "text": "The result on 12-layer models is showcased in Figure 16 (the orange line). Unlike vanilla RoPE (the blue line), which fails immediately outside the trained lengths (1\u201340), our combination of RoPE and position coupling achieves a much better generalization up to operand lengths 100. ", "page_idx": 20}, {"type": "image", "img_path": "5cIRdGM1uG/tmp/65c0250a83dc30b1de141026945d04bf7bf65d11436c9fc27d4bbc6e546ae6d2.jpg", "img_caption": ["Figure 16: RoPE-based position coupling, 12-layer model. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "B.5 Position Coupling for $N\\times2$ Multiplication Tasks ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We present an example of the position coupling method for $N\\times2$ ( $N$ -digit by 2-digit) multiplication, which is omitted from the main text. See Section 6.1 for the experimental results. ", "page_idx": 20}, {"type": "image", "img_path": "5cIRdGM1uG/tmp/04697e1acbd677e58bb7613b7088bf26d87403031c93e5e019e466aa77ace900.jpg", "img_caption": ["Figure 17: Illustration of position coupling for $N\\times2$ multiplication task. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "B.6 Addition Task with Multiple Summands ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The position coupling scheme for the vanilla addition task (with two operands) can naturally extend to the addition task with multiple summands: assign position IDs in ascending order from most significant digits to least significant digits for every operand and the response. Here, we focus on the addition of three summands. ", "page_idx": 20}, {"type": "image", "img_path": "5cIRdGM1uG/tmp/658b04bdfa20d009bd9f8fd20fdee6b2840672bc8399a2407c8b3d55fbc51934.jpg", "img_caption": ["Figure 18: Exact-match accuracy (median over 4 runs) for triple addition task, trained on sequences of length 1-40 with position coupling, NoPE, and random-start APE. For further experiment details, refer to Table 6. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Experiments. We train on sequences with operands of 1\u201340 digits. Our choice of max_pos is 102, so we test the operands of up to 100 digits. We investigate the performance of 3 different architectures, each with a different depth. The experimental results are described in Figure 18. 1-layer models keep their generalization capability until 100 digits, whereas the 3-layer models exhibit great stability across random seeds and achieve the highest generalizable length of 90. ", "page_idx": 20}, {"type": "text", "text": "Lastly, we note that the result of Theorem 5.1 can be extended to addition tasks with multiple summands with slight adjustments to the feed-forward layer in the construction. ", "page_idx": 20}, {"type": "text", "text": "B.7 Position Coupling for Copy/Reverse Tasks ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Data Format & Position Coupling. Each token of the query sequence is a digit (10 distinct characters). We couple the positions in the query and the response by their correspondence. Note that the position ID assigned to the equal token is different for the two tasks because as per our design principle (Sec 3.1), the equal token is grouped to the response tokens and position IDs have to be consecutive numbers within each group. ", "page_idx": 21}, {"type": "image", "img_path": "5cIRdGM1uG/tmp/67db8f0c7836b88592bd2e8abf9a3f811d16f2c66f503643ae3f0371ca01bd68.jpg", "img_caption": ["Figure 19: Illustration of position coupling for copy/reverse tasks. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Experiments. We compare position coupling with NoPE and random-start APE. We train a model on lengths 1\u201340 and evaluate its performance on lengths from 5 to 300, at intervals of 5. While a 1-layer 4-head model is used for the position coupling, we observe that the same architecture fails to memorize training samples for both NoPE and random-start APE. Therefore, we use a 6-layer 8-head model for the latter cases as it is commonly used in the literature (Zhou et al., 2024a). ", "page_idx": 21}, {"type": "image", "img_path": "5cIRdGM1uG/tmp/8f3b932290fd2c6849c46251b1209991fe7d75aca943412e32039229da977599.jpg", "img_caption": ["Figure 20: Exact-match accuracy (median over 4 runs) for (a) copying task and (b) reversing task, trained on sequences of length 1\u201340 with position coupling, NoPE, and random-start APE. For further experiment details, refer to the Table 7. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "The experimental results are described in Figure 20. For both copy and reverse tasks, position coupling exhibits near-perfect accuracy across the entire test length $7.5\\times$ for the trained length). In contrast, NoPE and random-start APE immediately fail to length-generalize. ", "page_idx": 21}, {"type": "text", "text": "B.8 Position Coupling for Minesweeper Generator Tasks ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Here, we present the extra experimental results for training the minesweeper generator task with position coupling. Specifically, we compare the performance of two configurations: one where the model shares the same positional embedding layer for both position coupling modules, and another where the model uses separate positional embedding layers for each position coupling module. ", "page_idx": 21}, {"type": "text", "text": "The results are described in Figure 21. When sharing the same positional embedding layer, position coupling achieves over $98\\%$ accuracy on a $12\\!\\times\\!12$ board, and maintains near $90\\%$ accuracy on a ", "page_idx": 21}, {"type": "text", "text": "$14\\!\\times\\!14$ board. However, with distinct positional embedding layers, position coupling only successfully generalizes to a $10\\!\\times\\!10$ board. We currently do not have a clear explanation for why the former method exhibits significantly better performance than the latter one. We leave the investigation and explanation of this phenomenon for future work. ", "page_idx": 22}, {"type": "image", "img_path": "5cIRdGM1uG/tmp/6fdbe9b78acc5c3e7da7428646f5b144f3951e0e44e35e467faab7786057fb93.jpg", "img_caption": ["Figure 21: Exact-match accuracy (median over 4 runs) for minesweeper generator task, trained on sequences of length $(5\\mathrm{-}9)\\!\\times\\!(5\\mathrm{-}9)$ with position coupling. For further experiment details, see Table 8. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "C Experiment Details and Hyperparameters ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Position coupling can be easily implemented on top of usual libraries of training transformer models like HuggingFace (Wolf et al., 2019) and Flaxformer4 since these libraries support an arbitrary array of position IDs (in the case of using APE). All we need is to build up a short routine implementing the assigning rule of position IDs when establishing the dataset and data loaders. To compare with NoPE, we use the code base provided by Kazemnejad et al. (2023) for most of the experiments.5 It contains a custom implementation of decoder-only T5 (Raffel et al., 2020) established on top of PyTorch (Paszke et al., 2019) and Huggingface, including several PE methods. We additionally implement a custom RMSNorm module (Zhang and Sennrich, 2019) and various positioning schemes of normalization layers (e.g., PreNorm (Xiong et al., 2020), PostNorm (Vaswani et al., 2017), and their combination), to follow the implementation details of Zhou et al. (2024b). ", "page_idx": 23}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/2fb17fc07235edac13b5b8aa1bf1d3cff8e717643ae47fe870c9abcc972e2499.jpg", "table_caption": ["Table 1: Hyperparameter summary for decimal integer addition task: comparison between trained lengths (Figures 3 and 14). "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Table 2: Hyperparameter summary for decimal integer addition task: comparison between the number of layers (Figures 4 and 15). ", "page_idx": 24}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/726f88eb228101c5f581527809f34c08c4de32140de18d7e43d3e571b14032ae.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/cac57aba28caa8f05f5ed3ac762ad42af951f3bc7b8e1e6a110c8693b365917d.jpg", "table_caption": ["Table 3: Hyperparameter summary for decimal integer addition task: generalization up to length 500 (Figure 12). "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "Table 4: Hyperparameter summary for decimal integer addition task: extracting attention patterns (Figure 7). ", "page_idx": 25}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/cf58ef7feae319e720b96e0b40300f494d2c121e28ac0bf497eefd15f5d410a8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/4831b5d213b4eb29abcc9f2c61aa4802f8551d6e197e90db9cfbe38954a63dfd.jpg", "table_caption": ["Table 5: Hyperparameter summary for $N\\times2$ multiplication task (Figure 8). "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "Table 6: Hyperparameter summary for addition task with three summands (Figure 18). ", "page_idx": 26}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/d92feb365e0b4f9a5142d55da23c8c51b678afc27440e75e176467a267329ae9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/3bdbd87fa63931d89f20bbdec9261ff86e19d8bf218de87e3bbcb867e04d8d26.jpg", "table_caption": ["Table 7: Hyperparameter summary for copy/reverse task (Figure 20) "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/b24df4e240079729781f45c5371e5c37c2bbe4e70cee0a3ae30c418d438bc3a3.jpg", "table_caption": ["Table 8: Hyperparameter summary for minesweeper generator task (Figures 10 and 21). "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "D Decoder-only Transformer Architecture ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Here we detail the architecture of a depth- $L$ , $H$ -head decoder-only Transformer (Vaswani et al., 2017). For a simple presentation, we ignore the normalization layers, as in Yun et al. (2020a). ", "page_idx": 28}, {"type": "text", "text": "Let $\\nu$ be the (ordered) vocabulary, a set of all tokens. Given an input sequence $\\mathcal{Z}\\in\\mathcal{V}^{N}$ and its length $N$ , the encoding function Enc : $\\dot{\\upgamma}^{N}\\rightarrow\\mathbb{R}^{d\\times N}$ maps it to ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{X^{(0)}:=\\mathtt{E n c}({\\mathcal T}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "It is a sum of the token embedding and the position embedding. ", "page_idx": 28}, {"type": "text", "text": "Next, there are $L$ Transformer blocks that sequentially transform this input. We denote by $\\mathbb{T}\\mathbf{f}_{l}:$ $\\mathbb{R}^{d\\times N}\\rightarrow\\mathbb{R}^{d\\times N}$ the operation of the $l$ -th block $(l\\in[L])$ , so that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\pmb{X}^{(l)}:=\\mathbb{T f}_{l}\\left(\\pmb{X}^{(l-1)}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The block $\\up T\\mathbf{f}_{l}$ consists of a (causal) attention layer $\\mathtt{A t t}_{l}\\ :\\ \\mathbb{R}^{d\\times N}\\ \\to\\ \\mathbb{R}^{d\\times N}$ and a (token-wise) feed-forward layer $\\mathsf{F}\\mathsf{F}_{l}:\\mathbb{R}^{d\\times N}\\to\\mathbb{R}^{d\\times N}$ , each of which contains a residual connection: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathtt{T f}_{l}:=\\big(\\mathtt{i d}+\\mathtt{F F}_{l}\\big)\\circ\\big(\\mathtt{i d}+\\mathtt{A t t}_{l}\\big),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where we denote by $\\mathrm{id}:\\mathbb{R}^{d\\times N}\\rightarrow\\mathbb{R}^{d\\times N}$ an identity map. ", "page_idx": 28}, {"type": "text", "text": "Each attention layer $\\mathtt{A t t}_{l}$ consists of $H$ attention heads. Its $h$ -th head $(h\\,\\in\\,[H])$ has matrices $Q_{h}^{(l)},K_{h}^{(l)}\\;\\in\\;\\mathbb{R}^{d_{Q K,h}^{(l)}\\times d}$ $V_{h}^{(l)}\\,\\in\\,\\mathbb{R}^{d_{V,h}^{(l)}\\times d}$ and U h(l) $\\pmb{U}_{h}^{(l)}\\;\\in\\;\\mathbb{R}^{d\\times d_{V,h}^{(l)}}$ as its parameters.6 With these matrices, borrowing the notation from Yun et al. (2020a), the attention layer with an input $\\boldsymbol{X}\\in\\mathbb{R}^{d\\times N}$ can be written as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathsf{A t t}_{l}(X):=\\sum_{h=1}^{H}U_{h}^{(l)}V_{h}^{(l)}X\\cdot\\mathsf{s o f t m a x}\\left((K_{h}^{(l)}X)^{\\top}Q_{h}^{(l)}X\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Here the softmax operator takes a square matrix $M\\in\\mathbb{R}^{N\\times N}$ and outputs an $N\\!\\times\\!N$ upper-triangular column-stochastic7 matrix ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\lbrack\\mathrm{sof}\\mathrm{tmax}(M)\\rbrack_{i j}=\\frac{e^{M_{i j}}}{\\sum_{1\\leq i^{\\prime}\\leq j}e^{M_{i^{\\prime}j}}}\\rbrack\\Uparrow_{\\{i\\leq j\\}},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\mathbb{1}_{\\{\\mathcal{E}\\}}$ is an indicator function for a predicate $\\mathcal{E}$ : it equals 1 if $\\mathcal{E}$ is true and 0 otherwise. Note that the upper triangularity captures the auto-regressive behavior of the causal attention. For the sake of convenience, we denote by $\\pmb{Y}^{(l)}:=\\pmb{X}^{(l-1)}\\overset{\\circ}{+}\\mathtt{A t t}_{l}(\\pmb{X}^{(l-1)})\\in\\mathbb{R}^{d\\times N}$ which is a consequence of residual connection right after the attention layer. ", "page_idx": 28}, {"type": "text", "text": "Each feed-forward layer $\\mathsf{F F}_{l}$ is a two-layer perceptron having $W_{1}^{(l)}\\in\\mathbb{R}^{d_{F}\\times d}$ , $b_{1}^{(l)}\\in\\mathbb{R}^{d_{F}}$ , $W_{2}^{(l)}\\in$ $\\mathbb{R}^{d\\times d_{F}}$ , $b_{2}^{(l)}\\in\\mathbb{R}^{d}$ as its parameters. It applies the following map to each column $\\textit{\\textbf{y}}$ of an input $\\mathbf{Y}$ : ", "page_idx": 28}, {"type": "equation", "text": "$$\ny\\mapsto W_{2}^{(l)}\\phi(W_{1}^{(l)}{\\pmb y}+{\\pmb b}_{1}^{(l)})+{\\pmb b}_{2}^{(l)},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\phi$ is a component-wise activation function. That is, the feed-forward layer is defined as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathsf{F F}_{l}({\\boldsymbol{Y}}):=W_{2}^{(l)}\\phi(W_{1}^{(l)}{\\boldsymbol{Y}}+b_{1}^{(l)}\\mathbf{1}_{d_{F}}^{\\top})+b_{2}^{(l)}\\mathbf{1}_{d}^{\\top},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\mathbf{1}_{d}$ is the $d$ -dimensional vectors filled with 1\u2019s. Here we mainly use the ReLU operation $\\phi(\\cdot)=\\operatorname*{max}\\left\\{\\cdot,0\\right\\}$ (Jarrett et al., 2009; Nair and Hinton, 2010), but there are many other popular choices such as GeLU (Hendrycks and Gimpel, 2016), GLU (Dauphin et al., 2017), ReGLU, and GEGLU (Shazeer, 2020). ", "page_idx": 28}, {"type": "text", "text": "The final component of the Transformer model is the decoding function $\\mathsf{D e c}:\\mathbb{R}^{d\\times N}\\to\\mathcal{V}^{N}$ , which is composed of a linear readout and a (token-wise) arg-max operation. Here, the linear readout is simply a linear layer having $W_{\\mathrm{out}}\\in\\mathbb{R}^{|\\mathcal{V}|\\times d}$ as its parameter. The decoding function produces the output sequence ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathcal{O}:=\\mathtt{D e c}(X^{(L)})\\in\\mathcal{V}^{N}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "E Formal Construction of Addition Transformer with Position Coupling ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Here we show how to implement the addition by employing a single-layer two-head decoder-only Transformer equipped with position coupling. We restate the theorem for the sake of readability. ", "page_idx": 29}, {"type": "text", "text": "Theorem 5.1. With the input format described in Section 3.1, there exists a depth-1 two-head decoderonly Transformer with coupled positions that solves the addition task with next-token prediction. Here, the operand length is at most $2^{\\lfloor(d-17)/2\\rfloor}-2$ , where the embedding dimension is $d\\geq21$ . ", "page_idx": 29}, {"type": "text", "text": "Organization of the Proof. A whole section is dedicated to prove Theorem 5.1. ", "page_idx": 29}, {"type": "text", "text": "\u2022 We start with the notation (Appendix E.1).   \n\u2022 We review and formalize the format of the input sequence (zero-padding, reversed format, and wrapping with BOS/EOS) (Appendix E.2).   \n\u2022 We define the encoding function Enc with a table of a concrete example (Appendix E.3), where Enc maps an input sequence of length $N$ to a $d\\times N$ encoding matrix $X^{(0)}$ .   \n\u2022 We devote a lot of pages to the detailed construction of the parameters of a causal attention layer $\\tt A t t_{1}$ to generate desired attention patterns (Appendix E.4). The attention layer has two attention heads playing distinct roles: (1) preparing for a sum without considering carries; and (2) preparing for the carry prediction & EOS detection.   \n\u2022 We provide a construction of a token-wise feed-forward neural network $\\mathsf{F F_{1}}$ which is a two-layer ReLU network (Appendix E.5). It consists of two subnetworks playing different roles: (1) producing one-hot vectors, each of which indicates a digit of the sum (response); and (2) binary values indicating whether the position is the end of the sequence.   \n\u2022 We conclude the proof by defining the decoding function Dec which performs the linear readout and the arg-max operation to generate the output sequence (Appendix E.6). ", "page_idx": 29}, {"type": "text", "text": "We illustrate the roadmap of the proof in Figure 22. ", "page_idx": 29}, {"type": "text", "text": "E.1 Notation ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "For the architecture of the decoder-only Transformer, we follow the notation introduced in Appendix D. ", "page_idx": 29}, {"type": "text", "text": "Let $e_{i}^{d}$ denote the $i$ -th standard basis vector of $\\mathbb{R}^{d}$ . For example, $\\pmb{e}_{1}^{3}=\\left[1\\quad0\\quad0\\right]^{\\top}$ . Let $\\pmb{I_{m}}$ be the $m\\times m$ identity matrix. Let ${\\bf0}_{p}$ and ${\\bf1}_{p}$ denote the $p$ -dimensional vectors filled with 0\u2019s and $1\\,\\mathrm{\\dot{s}}$ , respectively. Similarly, let ${\\mathbf{0}}_{m\\times n}$ denote the $m\\times n$ zero matrix. For a positive integer $n$ , we frequently use the set $[n]:=\\{1,...,n\\}$ . For any matrix $\\pmb{A}$ , denote the $i$ -th row and $j$ -th column of $\\pmb{A}$ by $A_{i}$ \u2022 and $A_{\\bullet j}$ , respectively. Given two non-negative integers $a$ and $b$ , let $\\ell(a,b)$ be the length of a longer one between $a$ and $b$ . For example, $\\ell(12,3456)=4$ . ", "page_idx": 29}, {"type": "text", "text": "Consider an ordered vocabulary $\\mathcal{V}=(0,1,2,3,4,5,6,7,8,9,+,=,\\Phi)$ . We include a special token $\\mathbb{\\cdot}\\mathbb{\\Phi}^{\\bullet}$ that plays the role of both the beginning-of-sequence (BOS) token and the end-of-sequence (EOS) token.8 We denote $\\lvert\\lambda_{k}$ as $k$ -th element of $\\mathcal{V}$ . For instance, $\\protect\\nu_{4}=3$ and $\\nu_{13}=\\mathfrak{S}$ . Lastly, since we and the size of dimensions $d_{Q K,h}^{(l)}$ and $d_{V,h}^{(l)}$ $(l)$ ", "page_idx": 29}, {"type": "text", "text": "E.2 Input Sequence ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We seek to perform an addition $a+b=c$ using next-token prediction. To this end, we want to transform it into an input sequence ${\\mathcal{I}}={\\overline{{\\mathfrak{G}A+B=C}}}$ of an appropriate format. Note that the EOS token is the last token that needs to be predicted, so we exclude EOS in the input sequence. Let $\\ell:=\\ell(a,b)$ . ", "page_idx": 29}, {"type": "text", "text": "We first zero-pad the shorter one between $a$ and $b$ to match the length of the part $A$ and part $B$ as $\\ell$ Sometimes, the sum $c$ might be longer than $a$ or $b$ due to a carry. To make the length of the part $C$ consistent, we also put a zero-pad in front of $c$ to set its length as $\\ell+1$ . Also, to ease calculating the addition with next-token prediction, we reverse the sum $c$ to make the part $C$ . For example, if we have a sum $3812+98=3910$ , we use $\\overline{{\\mathbb{S}3812+0098=01930}}$ as an input sequence; if a sum $98+9907=10005$ is given, we use $\\overline{{\\mathbb{S}0098+9907=50001}}$ as an input sequence. The red digits are zero-paddings, and the blue digits are the reversed sum. ", "page_idx": 29}, {"type": "image", "img_path": "5cIRdGM1uG/tmp/1978f44656db00d8129e4788702928301bcf84decf96526fc0c79489f72d1f5b.jpg", "img_caption": ["Figure 22: Roadmap to the formal construction of addition Transformer with position coupling. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "To recap, the input sequence $\\mathcal{T}=\\overline{{\\sigma_{1}\\sigma_{2}\\ldots\\sigma_{N}}}\\in\\mathcal{V}^{N}$ of length $N=3\\ell+4$ consists of six parts: ", "page_idx": 30}, {"type": "text", "text": "1. the BOS token $\\sigma_{1}={}^{\\bullet}\\mathfrak{F}$ \u2019   \n2. the first operand $A=\\overline{{\\sigma_{2}\\ldots\\sigma_{\\ell+1}}}$ where $\\sigma_{i}\\in\\{0,\\ldots,9\\}$ ;   \n3. the addition symbol $\\sigma_{\\ell+2}=\\mathrm{{}^{\\bullet}+}^{\\bullet}$ ;   \n4. the second operand $B=\\overline{{\\sigma_{\\ell+3}\\ldots\\sigma_{2\\ell+2}}}$ where $\\sigma_{i}\\in\\{0,\\ldots,9\\}$ ;   \n5. the equality symbol $\\sigma_{2\\ell+3}=^{\\bullet}=^{\\bullet}$ ;   \n6. the (reversed) sum $C=\\overline{{\\sigma_{2\\ell+4}\\ldots\\sigma_{3\\ell+4}}}$ where $\\sigma_{i}\\in\\{0,\\ldots,9\\}$ . ", "page_idx": 30}, {"type": "text", "text": "Note that the part $C$ might be incomplete (i.e., $N<3\\ell+4)$ at the inference time; we infer the digits of the part $C$ one by one using next-token prediction. Throughout this section on a formal construction, however, we only consider the train time setup in which we infer all the digits of the part $C$ at once using simultaneous next-token prediction in a single forward pass. Precisely, we want to use an input sequence $\\mathcal{T}=\\overline{{\\sigma_{1\\,\\cdot\\,\\cdot\\,\\sigma_{N}}}}$ to produce an output sequence $\\mathcal{O}=\\overline{{\\sigma_{1}^{\\prime}\\ldots\\sigma_{N}^{\\prime}}}$ where $\\overline{{\\sigma_{2\\ell+3}^{\\prime}\\cdot\\cdot\\cdot\\sigma_{N-1}^{\\prime}}}=C=\\overline{{\\sigma_{2\\ell+4}\\cdot\\cdot\\cdot\\cdot\\sigma_{N}}}$ and $\\sigma_{N}^{\\prime}=\\mathbf{\\cdot}\\mathfrak{F}^{\\bullet}$ (EOS). ", "page_idx": 30}, {"type": "text", "text": "E.3 Encoding Function ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "We plan to produce an input encoding, given an input sequence $\\mathcal{T}$ designed as above. The encoding matrix $\\pmb{X}^{(0)}$ is of size $d\\times N$ : each column represents an embedding vector for a token, while each row represents a particular named dimension. What we mean by named dimension is that we give a name to each dimension for a clear description of our formal construction. ", "page_idx": 31}, {"type": "text", "text": "We construct an input encoding by concatenating the token embedding and the position embedding, which can be viewed as a sum of two different embedding matrices of the same size. ", "page_idx": 31}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/25909fdec911b4d64d0df436b86bab2aa53b82d4e87138d84e7bbf973c1303ff.jpg", "table_caption": ["Table 9: Example initial encoding. Here we consider the input sequence $\\overline{{8653+049=2070}}$ and the starting position ID is chosen as $s=2$ . The vectors ${\\pmb v}_{\\boxminus}^{P}$ are defined in Equation (11). The gray rows will be filled in later. "], "table_footnote": [], "page_idx": 31}, {"type": "text", "text": "E.3.1 Token Embedding ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "The token embedding consists of 17 dimensions: we call them ", "page_idx": 31}, {"type": "text", "text": "Initially, we let the last 14 dimensions be empty (i.e., all zeros). Thus, we explain the first three dimensions, NUM, IS_BOS, and FULL_ONES. ", "page_idx": 31}, {"type": "text", "text": "Dimension 1 (NUM). For a number token $(0,\\ldots,9)$ , we put itself in the dimension NUM. For the other tokens $(+,=,\\Phi)$ , we put 0. ", "page_idx": 31}, {"type": "text", "text": "Dimension 2 (IS_BOS). For a special token $\\mathbf{\\cdot}\\Phi^{\\bullet}$ , we put 1 in the dimension IS_BOS. Otherwise, we put 0. ", "page_idx": 31}, {"type": "text", "text": "Dimension 3 (FULL_ONES). We put 1 everywhere in this dimension. ", "page_idx": 31}, {"type": "text", "text": "E.3.2 Coupled Position IDs and Position Embedding ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Before constructing a position embedding, we specify the coupled position IDs for the addition task. Let max_pos be a hyperparameter of the maximum position IDs, where position IDs are non-negative integers. Basically, we match the significance of the digits: e.g., a least significant digit is always coupled to the other least significant digits. To this end, we first randomly choose a starting position $I D\\;\\bar{s}\\in[\\tt m a x\\_p o s-\\ell-1]$ . (For that, $\\mathtt{m a x\\_p o s}\\geq\\ell+2$ must hold.) Then we allocate the position IDs of token $\\sigma_{i}$ in the input sequence $\\mathcal{T}=\\overline{{\\sigma_{1\\,\\cdot\\,\\cdot\\,\\sigma_{N}}}}$ as ", "page_idx": 31}, {"type": "equation", "text": "$$\np(i)=\\left\\{\\begin{array}{l l}{0,}&{i=1,}\\\\ {s+i-1,}&{i=2,\\ldots,\\ell+2,}\\\\ {s+i-(\\ell+2),}&{i=\\ell+3,\\ldots,2\\ell+3,}\\\\ {s+(3\\ell+4)-i}&{i=2\\ell+4,\\ldots,3\\ell+4.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Recall that $N=3\\ell+4$ . Also, observe that for $i\\in\\{2,\\ldots,\\ell+1\\}$ , ", "page_idx": 32}, {"type": "equation", "text": "$$\np(i)=p(i+\\ell+1)=p(3\\ell+5-i)=s+i,\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "which couples the position of $(\\ell-i+2)$ -th significant digit in the first operand $(A)$ , the second operand $(B)$ , and the sum $(C)$ . Also, the position of tokens $\\mathbf{\\omega}^{\\star}+\\mathbf{\\omega}^{\\star}$ and $\\mathcal{\\bullet}=\\mathcal{\\ '}$ are coupled. Lastly, the only token that has the position $\\operatorname{ID}\\,0$ is the special token $\\mathbf{\\Delta}^{\\bullet}\\mathbf{\\Phi}^{\\bullet}$ . ", "page_idx": 32}, {"type": "text", "text": "Before moving on to the positional embedding, we define $\\pmb{v}_{k}^{D}$ $\\mathit{\\Delta}(k\\in[2^{D}])$ as ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\pmb{v}_{k}^{D}=\\left[(-1)^{b_{i}^{(D,k)}}\\right]_{i=1}^{D}\\in\\mathbb{R}^{D}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $b_{i}^{(D,k)}$ is defined as the $i$ -th (from left) digit of $D$ -digit binary representation of $k-1$ . For example, if $D=2$ , ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{v_{1}^{2}=\\left[1\\quad1\\right]^{\\top},\\;v_{2}^{2}=\\left[-1\\quad1\\right]^{\\top},\\;v_{3}^{2}=\\left[1\\quad-1\\right]^{\\top},\\;v_{4}^{2}=\\left[-1\\quad-1\\right]^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We remark that the points $\\pmb{v}_{k}^{D}$ are the vertices of $D$ -dimensional hypercube with side length 2, centered at the origin.9 Note that for $k\\neq l$ , ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\left\\|v_{k}^{D}\\right\\|^{2}=D,\\quad\\left\\langle v_{k}^{D},v_{l}^{D}\\right\\rangle\\leq D-2.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Now we explain the position embedding. It consists of $2P$ dimensions, which eventually become from 18-th to $(2P+17)$ -th dimension after concatenation. If $p(i)=0$ , we let ${\\bf0}_{2P}$ as a position embedding vector. For the positive position IDs $p(i)\\geq1$ , we let a concatenation ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\left[{v}_{p(i)}^{P}\\right]\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "as a position embedding vector of a token $\\sigma_{i}$ . (In case of $p(i)=2^{P}$ , we use $v_{1}^{P}$ instead of ${\\pmb v}_{p(i)+1}^{P}.$ .) We call the former dimensions for the position embedding as POS_1 and the latter dimensions as POS_2. ", "page_idx": 32}, {"type": "text", "text": "Concatenating the token embedding and the position embedding, we get the input embedding $\\pmb{X}^{(0)}$ . See Table 9 for an example. As a result, the total embedding dimension is $d=2P+17.$ . Note the maximum possible position ID that can be represented with $\\pmb{v}_{k}^{P}$ \u2019s is max $\\mathtt{p o s}=2^{P}=2^{\\lfloor(d-17)/2\\rfloor}$ . Therefore, the length of an operand must be $\\ell\\le\\operatorname*{max}_{-\\mathrm{pos}{}\\ -{}2}=2^{\\lfloor(d-17)/2\\rfloor}-2$ . ", "page_idx": 32}, {"type": "text", "text": "E.4 Transformer Block \u2014 Causal Attention Layer ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "The goal of the causal attention layer is to flil in the zero-blanks10 of the encoding matrix at dimensions PRE_SUM, PRE_CARRY, and PRE_EOS. We divide the roles into two different heads. ", "page_idx": 32}, {"type": "text", "text": "E.4.1 Attention Head 1: Digit-wise Addition without Carries ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "The goal of the first head is to perform a digit-wise addition and to fill in the blanks of the encoding matrix at dimension PRE_SUM. Later, using this dimension, combined with the dimension PRE_CARRY, we will be able to perform the next-token prediction for addition. For now, we do not care about the carries, which will be dealt with in a later section. Formally, we aim to perform $\\sigma_{i}+\\sigma_{i+\\ell+1}$ for each $i\\in\\{2,\\cdots,\\ell+1\\}$ and put its result at the $(3\\ell+4-i)$ -th position (column) of the dimension PRE_SUM (row). To this end, we utilize our position embedding. ", "page_idx": 32}, {"type": "text", "text": "Recall that $d=2P+17$ and let $d_{Q K,1}=P+1$ . Let $M>0$ be a number determined later. Let ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{\\boldsymbol{Q}}_{1}=\\left(\\underset{\\sqrt{M P}(e_{\\mathrm{FUL}_{-}0\\mathrm{NE}}^{\\mathbf{\\boldsymbol{Q}}_{P\\times17}})^{\\top}}{\\mathbf{\\boldsymbol{Q}}_{P\\times1}}\\right)^{\\top}\\quad\\sqrt{M}I_{P}\\quad\\mathbf{\\boldsymbol{0}}_{P\\times P}\\bigg)\\in\\mathbb{R}^{d_{Q K,1}\\times d},}\\\\ &{\\mathbf{\\boldsymbol{K}}_{1}=\\left(\\underset{\\sqrt{M P}(e_{\\mathrm{FS,8O}}^{\\mathbf{\\boldsymbol{B}}_{P\\times17}})^{\\top}}{\\mathbf{\\boldsymbol{0}}_{P\\times P}}\\quad\\mathbf{\\boldsymbol{0}}_{1\\times P}\\quad\\mathbf{\\boldsymbol{\\Omega}}_{1\\times P}\\right)\\in\\mathbb{R}^{d_{Q K,1}\\times d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "The linear transformations with matrices $Q_{1}$ and $\\kappa_{1}$ do two different jobs at once. (1) $Q_{1}$ ( $\\mathbf{\\nabla}K_{1}$ , resp.) t\u221aakes the dimensions POS_1 (POS_2, resp.) from the input encoding matrix and scale them u\u221ap by $\\sqrt{M}$ ; (2) $Q_{1}$ ( $\\kappa_{1}$ , resp.) takes the dimension FULL_ONES (IS_BOS, resp.) and scale it up by $\\sqrt{M P}$ . For concrete examples, please refer to Tables 12 and 13. By these, the attention score matrix $C_{1}:=(K_{1}X^{(0)})^{\\top}Q_{1}X^{(\\bar{0})}$ becomes as in Table 10. The blanks in Table 10 are the numbers smaller than $M(P-2)$ ; the asterisks $(^{\\ast}\\ast^{\\ast})$ are the entries (or lower triangular submatrices) ignored by the causal softmax operator; the dots represents the hidden $M P$ \u2019s. ", "page_idx": 33}, {"type": "image", "img_path": "5cIRdGM1uG/tmp/365a86a0919eadff8878526a98525c19b1251e6aee23735b9d3a5eb12112115b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "Now consider the attention matrix $\\pmb{A}_{1}:=\\tt s o f t m a x(C_{1})\\in\\mathbb{R}^{N\\times N}$ . Its exact form is a bit messy due to the softmax operation of finite numbers. However, one can observe that, if the number $M$ is large enough, it gets close to the column-stochastic matrix $\\pmb{T}_{1}\\in\\mathbb{R}^{N\\times N}$ described in Table 11. The blanks in Table 11 are zeros; the dots represent the omitted nonzero entries. ", "page_idx": 33}, {"type": "image", "img_path": "5cIRdGM1uG/tmp/4df979f3bb1cb2b7ef5ebc03212ce0359037f47c4ade679827b40467ee6c4c6f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "Let ${\\pmb R}_{1}={\\pmb A}_{1}-{\\pmb T}_{1}\\in\\mathbb R^{N\\times N}$ be the error matrix, which is upper triangular. Its exact form is messy as well, but we can obtain the bounds of their entries. Consider a pair of indices $(i,j)\\in[N]^{2}$ such that $i\\leq j$ . Let $x_{j}=1/[T_{1}]_{1j}\\in\\{1,2,3\\}$ . If $\\begin{array}{r}{[T_{1}]_{i j}=\\frac{1}{x_{j}}}\\end{array}$ , $[R_{1}]_{i j}<0$ and ", "page_idx": 33}, {"type": "equation", "text": "$$\n-[{\\bf R}_{1}]_{i j}\\le\\frac{1}{x_{j}}-\\frac{e^{M P}}{x_{j}e^{M P}+(j-x_{j})e^{M(P-2)}}=\\frac{j-x_{j}}{x_{j}(x_{j}e^{2M}+(j-x_{j}))}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "On the other hand, if $[T_{1}]_{i j}=0,[R_{1}]_{i j}>0$ and ", "page_idx": 33}, {"type": "equation", "text": "$$\n[R_{1}]_{i j}\\leq{\\frac{e^{M(P-2)}}{x_{j}e^{M P}+(j-x_{j})e^{M(P-2)}}}={\\frac{1}{x_{j}e^{2M}+(j-x_{j})}}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Now let $d_{V,1}=1$ and ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{V}_{1}=3(e_{\\mathrm{NUM}}^{d})^{\\top}\\in\\mathbb{R}^{d_{V,1}\\times d},}\\\\ &{\\boldsymbol{U}_{1}=e_{\\mathrm{PRE_{\\_SUM}}}^{d}\\in\\mathbb{R}^{d\\times d_{V,1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "The linear transformation with matrix $U_{1}V_{1}$ takes the dimension NUM from the input encoding matrix, scales it up by 3, and puts it to the dimension PRE_SUM. A concrete example is provided in Table 14. ", "page_idx": 34}, {"type": "text", "text": "Obtaining $U_{1}V_{1}X^{(0)}A_{1}$ , its every entry is zero except at the dimension PRE_SUM. Observe that $[U_{1}V_{1}{\\pmb X}^{(0)}]_{(\\mathrm{PRE\\mathrm{-}\\mathrm{SUM}})1}=0.$ , because in the input encoding matrix, the dimension NUM starts with 0. Also, note that it is enough to focus on the columns $j\\in\\{2\\ell+3,\\ldots,3\\ell+4\\}$ since we only care about the next-token prediction of the tokens after $\\sigma_{2\\ell+3}=^{\\prime}=^{\\prime}$ . Specifying the dimension (i.e., the particular row) for these columns, we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{[U_{1}V_{1}X^{(0)}T_{1}]_{\\scriptstyle\\left(\\mathrm{pRE\\toSUM}\\right)j}=\\left\\{X_{\\mathrm{(NUM)}(3\\ell+4-j)}^{(0)}+X_{\\mathrm{(NUM)}(4\\ell+5-j)}^{(0)}}&{\\mathrm{if~}j\\in\\{2\\ell+3,\\ldots,3\\ell+2\\},}\\\\ {0}&{\\mathrm{if~}j\\in\\{3\\ell+3,3\\ell+4\\},}\\\\ &{\\quad=\\left\\{\\begin{array}{l l}{\\sigma_{(3\\ell+4)-j}+\\sigma_{(4\\ell+5)-j}}&{\\mathrm{if~}j\\in\\{2\\ell+3,\\ldots,3\\ell+2\\},}\\\\ {0}&{\\mathrm{if~}j\\in\\{3\\ell+3,3\\ell+4\\}.}\\end{array}\\right.}&{(21)}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Refer to Table 15 for a concrete example of computing $U_{1}V_{1}X^{(0)}T_{1}$ . Also, for the softmax errors, ", "page_idx": 34}, {"type": "equation", "text": "$$\n[U_{1}V_{1}X^{(0)}R_{1}]_{(\\mathrm{PRE\\mathrm{}_{-}S U M})j}=\\sum_{2\\leq i\\leq j}3X_{(\\mathrm{NUM})i}^{(0)}[R_{1}]_{i j}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Specifically, if $j\\in\\{2\\ell+3,\\ldots,3\\ell+2\\}$ (thus $x_{j}=3$ ), ", "page_idx": 34}, {"type": "equation", "text": "$$\n[U_{1}V_{1}X^{(0)}R_{1}]_{(\\mathrm{PR}_{-}\\mathrm{sUM})j}=\\sum_{\\substack{i\\in\\{(3\\ell+4)-j,(4\\ell+5)-j\\}}}3X_{(\\mathrm{NUM})i}^{(0)}[R_{1}]_{i j}+\\sum_{\\substack{2\\leq i\\leq j\\,\\,\\,\\,\\,\\,}}3X_{(\\mathrm{NUM})i}^{(0)}[R_{1}]_{i j},\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where ", "page_idx": 34}, {"type": "equation", "text": "$$\n0\\leq-\\sum_{i\\in\\{(3\\ell+4)-j,(4\\ell+5)-j\\}}3X_{(\\mathrm{NUM})i}^{(0)}[R_{1}]_{i j}\\leq\\frac{2\\cdot9(j-3)}{3e^{2M}+(j-3)}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "holds by Equation (17), and ", "page_idx": 34}, {"type": "equation", "text": "$$\n0\\leq\\sum_{\\stackrel{2\\leq i\\leq j}{i\\neq(3\\ell+4)-j}}3X_{\\mathrm{(NUM)}i}^{\\mathrm{(0)}}[R_{1}]_{i j}\\leq\\frac{27(j-3)}{3e^{2M}+(j-3)}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "holds by Equation (18). On the other hand, if $j\\in\\{3\\ell+3,3\\ell+4\\}$ , ", "page_idx": 34}, {"type": "equation", "text": "$$\n0\\leq[U_{1}V_{1}X^{(0)}R_{1}]_{(\\mathrm{PR}_{-}\\mathrm{SUM})j}=\\sum_{2\\leq i\\leq j}3X_{(\\mathrm{NUM})i}^{(0)}[R_{1}]_{i j}\\leq\\frac{27(j-1)}{e^{2M}+(j-1)}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "One can easily prove these inequalities by using the bounds of $[R_{1}]_{i j}$ \u2019s and the fact that the entries in $X_{(\\mathrm{NUM})\\bullet}^{(0)}$ lie in the interval $[0,9]$ . ", "page_idx": 34}, {"type": "text", "text": "If we let $\\begin{array}{r}{M\\geq\\frac{1}{2}\\log(N-1)+3}\\end{array}$ , we can ensure that $\\big\\vert[U_{1}V_{1}X^{(0)}R_{1}]_{\\left(\\mathrm{PRE\\mathrm{}_{-}S U M}\\right)j}\\big\\vert$ smaller than 0.1 for each $j\\in\\{2\\ell+3,\\ldots,3\\ell+4\\}$ . The proof is simple: it is enough to check ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{27(N-3)}{3e^{2M}+(N-3)}<\\frac{1}{10},\\quad\\frac{27(N-1)}{e^{2M}+(N-1)}<\\frac{1}{10}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/b19162dfe16de5a228f6d00c4a1a498fcde97d8ace2226b8a26c27e74d1102c1.jpg", "table_caption": ["Table 12: Example of $Q_{1}X^{(0)}$ , continuing from Table 9 "], "table_footnote": [], "page_idx": 35}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/240db9d14c6fd90d38c12ebd0ad7ac16af12c728c5f9641c476a7140a6f7802f.jpg", "table_caption": ["Table 13: Example of $K_{1}X^{(0)}$ , continuing from Table 9. "], "table_footnote": [], "page_idx": 35}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/93f237fd0f8f1101f6043cd7b03b735b0b1a3ee1c133740a99b81f5e9db251b9.jpg", "table_caption": ["Table 14: Example of $U_{1}V_{1}X^{(0)}$ , continuing from Table 9. "], "table_footnote": [], "page_idx": 35}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/6fc4d9308dceb598ff603738d9c9f8be5250bb3b656053ea4510ae6c27025497.jpg", "table_caption": ["Table 15: Example of $U_{1}V_{1}X^{(0)}T_{1}$ , continuing from Table 14. See Table 11 for the definition of ${\\cal T}_{1}$ "], "table_footnote": [], "page_idx": 35}, {"type": "text", "text": "E.4.2 Attention Head 2: Carry & EOS Detection ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "The goal of the second head is to flil in the blanks of the encoding matrix at dimensions PRE_CARRY and PRE_EOS. At dimension PRE_EOS, we will put (approximately) 1 if the next token would be the EOS token $(\\sqrt[6]{9})$ , otherwise, we will put strictly smaller numbers like (approximately) 2/3 and 1/2. ", "page_idx": 35}, {"type": "text", "text": "What we will put at dimension PRE_CARRY is the evidence of the presence of an additional carry, which is not quite straightforward to understand. Let us take a look at some examples. Consider an addition $3+9=12$ . Since it is greater than or equal to 10, the least significant digits in the operands generate a carry 1. But in some cases, a pair of digits with a sum less than 10 can make a carry. Next, consider an addition $53+49=102$ . In the second least significant digits, An addition of 5 and 4 occurs. However, a carry is already produced in the least significant digits $3+9=12)$ ), so the total sum including the carry is 10, not 9. Thus, it also produces a carry. But how can we know the presence of a carry while only looking at the second least significant digits? The answer is to observe the second least significant digit in the sum, 0 of 102. Somehow, the consequence of adding 5 and 4 is 0, (or 10, implicitly) so it makes a carry. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "To generalize this explanation, let $a$ and $b$ be digits of the operands in the same significance, and $c$ be a digit of the sum in the same significance as $a$ and $b$ . We find that the rule of recognizing that the addition of $a$ and $b$ generates a carry is that ", "page_idx": 36}, {"type": "equation", "text": "$$\n{\\begin{array}{r l r}&{\\int\\!\\!\\mathrm{If}\\ a+b-c\\in\\{9,10\\},}&{\\mathrm{then\\a\\carry\\is\\generated,}}\\\\ &{\\int\\!\\!\\mathrm{Otherwise},}&{\\mathrm{then\\the\\carry\\is\\not\\generated.}}\\end{array}}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Thus, it is crucial to store the information of $a+b-c$ or any related one somewhere. In fact, we can store $a+b+c$ at dimension PRE_CARRY of the encoding matrix, and it can be transformed into $a+b-c$ and used later in the feed-forward layer. Formally, we aim to perform $\\sigma_{i}+\\sigma_{i+\\ell+1}+\\sigma_{3\\ell+5-i}$ for each $i\\in\\{2,...,\\ell+1\\}$ and put its result at the $(3\\ell+\\dot{5}-i)$ -th position (column) of the dimension PRE_CARRY (row). To this end, we again utilize our position embedding. ", "page_idx": 36}, {"type": "text", "text": "Recall that $d=2P+17$ and let $d_{Q K,2}=P+1$ . Let ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{\\calQ}_{2}=\\left(\\sqrt{\\cal M P}(e_{\\mathrm{FUL_{-}0N E s}}^{{\\bf0}_{P\\times17}})^{\\top}\\thinspace\\thinspace\\thinspace\\thinspace\\thinspace\\mathbf{\\calW}\\thinspace{\\cal I}_{P}\\thinspace\\thinspace\\thinspace\\thinspace\\mathbf{0}_{P\\times P}\\right)\\in\\mathbb{R}^{d_{Q K,2}\\times d},}\\\\ &{\\mathbf{\\calK}_{2}=\\left(\\sqrt{\\cal M P}(e_{\\mathrm{FUL_{-}0N E s}}^{{\\bf17}})^{\\top}\\thinspace\\thinspace\\thinspace\\thinspace\\thinspace\\mathbf{0}_{1\\times P}\\thinspace\\thinspace\\thinspace\\thinspace\\mathbf{0}_{1\\times P}\\right)\\in\\mathbb{R}^{d_{Q K,2}\\times d},}\\\\ &{\\mathbf{\\calK}_{2}=\\left(\\sqrt{\\cal M P}(e_{\\mathrm{FS,80S}}^{{\\bf17}})^{\\top}\\thinspace\\thinspace\\thinspace\\thinspace\\thinspace\\mathbf{0}_{1\\times P}\\thinspace\\thinspace\\thinspace\\thinspace\\mathbf{0}_{P\\times P}\\right)\\in\\mathbb{R}^{d_{Q K,2}\\times d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "The linear transformations with matrices $Q_{2}$ and $K_{2}$ do two different jobs a\u221at once. (1) they take the dimensions POS_1 from the input encoding matrix and scale them u\u221ap by $\\sqrt{M}$ ; (2) $Q_{2}$ ( $\\scriptstyle{K_{2}}$ , resp.) takes the dimension FULL_ONES (IS_BOS, resp.) and scale it up by $\\sqrt{M P}$ . For concrete examples, refer to Tables 18 and 19. By these, the attention score matrix $C_{2}:=(K_{2}X^{(0)})^{\\top}Q_{2}X^{(0)}$ becomes as in Table 16. The blanks in Table 16 are the numbers less than equal to $M(P-2)$ ; the asterisks $(^{\\ast}\\ast^{\\ast})$ are the entries (or lower triangular submatrices) ignored by the causal softmax operator; the dots represent the hidden $M P$ \u2019s. ", "page_idx": 36}, {"type": "image", "img_path": "5cIRdGM1uG/tmp/0412e44bf870a1cb6c9fb62b11b1ee6b08fa0e492497b6703c23f278304b5501.jpg", "img_caption": ["Table 16: Exact attention score matrix $C_{2}$ (with explicit row/column indices) of Head 2. row \\ col $j=1$ 2 \u00b7 \u00b7 \u00b7 $\\ell+1$ $\\ell+2$ $\\ell+3$ \u00b7 \u00b7 \u00b7 $2\\ell+2$ $2\\ell+3$ $2\\ell+4$ \u00b7 \u00b7 \u00b7 $3\\ell+3$ $3\\ell+4$ "], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "Now consider the attention matrix $\\pmb{A}_{2}:=\\mathsf{s o f}\\tan\\mathsf{a x}(C_{2})\\in\\mathbb{R}^{N\\times N}$ . Similarly to the previous head, if the number $M$ is large enough, it gets close to the column-stochastic matrix $\\pmb{T}_{2}\\in\\mathbb{R}^{N\\times N}$ described in Table 17. The blanks in Table 17 are zeros; the dots represent the omitted nonzero entries. ", "page_idx": 36}, {"type": "text", "text": "Let $\\pmb{R}_{2}=\\pmb{A}_{2}-\\pmb{T}_{2}\\in\\mathbb{R}^{N\\times N}$ be the error matrix, which is upper triangular as well. Its exact form is messy as well, but we can obtain the bounds of their entries. Consider a pair of indices $(i,j)\\in[N]^{2}$ such that $i\\leq j$ . Let $x_{j}=1/[T_{2}]_{1j}\\in\\{1,2,3,4\\}$ . If $\\begin{array}{r}{[T_{2}]_{i j}=\\frac{1}{x_{j}}}\\end{array}$ , $[R_{2}]_{i j}\\dot{<}0$ and ", "page_idx": 36}, {"type": "equation", "text": "$$\n-[{\\pmb R}_{2}]_{i j}\\le\\frac{1}{x_{j}}-\\frac{e^{M P}}{x_{j}e^{M P}+(j-x_{j})e^{M(P-2)}}=\\frac{j-x_{j}}{x_{j}(x_{j}e^{2M}+(j-x_{j}))}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Table 17: Limiting attention matrix $\\mathbf{{T}}_{2}$ (with explicit row/column indices) of Head 2, as $M$ gets large. row \\ col j = 1 2 \u00b7 \u00b7 \u00b7 \u2113+ 1 \u2113+ 2 \u2113+ 3 \u00b7 \u00b7 \u00b7 2\u2113+ 2 2\u2113+ 3 $2\\ell+4$ \u00b7 \u00b7 \u00b7 3\u2113+ 3 $3\\ell+4$ ", "page_idx": 37}, {"type": "image", "img_path": "5cIRdGM1uG/tmp/f9c1b87eae6936b4f67b88e221af9d15f713458211c004b6c320637f1db0d32c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 37}, {"type": "text", "text": "On the other hand, if $[\\pmb{T}_{2}]_{i j}=0$ , $[R_{2}]_{i j}>0$ and ", "page_idx": 37}, {"type": "equation", "text": "$$\n[{\\cal R}_{2}]_{i j}\\le\\frac{e^{M(P-2)}}{x_{j}e^{M P}+(j-x_{j})e^{M(P-2)}}=\\frac{1}{x_{j}e^{2M}+(j-x_{j})}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Now let $d_{V,2}=2$ and ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{2}=\\left(\\O_{2(e_{\\mathrm{IS}_{-}\\mathrm{BOS}}^{d})}^{4(e_{\\mathrm{NUM}}^{d})^{\\top}}\\right)\\in\\mathbb{R}^{d_{V,2}\\times d},}\\\\ &{U_{2}=\\left(e_{\\mathrm{PRE}_{-}\\mathrm{CARRY}}^{d}\\quad e_{\\mathrm{PRE}_{-}\\mathrm{EOS}}^{d}\\right)\\in\\mathbb{R}^{d\\times d_{V,2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "The linear combination with matrix $U_{2}V_{2}$ does two jobs at once. First, it takes the dimension NUM from the encoding matrix, scales it up by 4, and puts it to the dimension PRE_CARRY. Second, it takes the dimension IS_BOS from the encoding matrix, scales it up by 2, and puts it to the dimension PRE_EOS. A concrete example is provided in Table 20. ", "page_idx": 37}, {"type": "text", "text": "Obtaining $U_{2}V_{2}X^{(0)}A_{2}$ , its every entry is zero except at the dimensions PRE_CARRY and PRE_EOS. Observe that $[U_{2}V_{2}{\\pmb X}^{(0)}]_{(\\mathrm{PRE\\_CARRY})1}=0$ , because in the input encoding matrix, the dimension NUM starts with 0. Also, note again that it is enough to focus on the columns $j\\in\\{2\\ell+3,\\ldots,3\\ell+4\\}$ , since we only care about the next-token prediction of the tokens after $\\sigma_{2\\ell+3}=^{\\circ}=$ . Specifying the dimensions (i.e., the particular rows) for these columns, we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{U_{2}V_{2}X^{(0)}T_{2}\\big]_{\\{\\mathrm{ret,c}\\leq\\mathrm{ARF}\\}}j=\\left\\{\\begin{array}{l l}{\\frac{4}{3}\\left(X_{(\\mathrm{wud})(\\ell+2)}^{(0)}+X_{(\\mathrm{wud})j}^{(0)}\\right)}&{\\mathrm{if~}(2\\ell+3)=2\\ell+3,}\\\\ {X_{(\\mathrm{wud})(3\\ell+5-j)}^{(0)}+X_{(\\mathrm{wud})(4\\ell+6-j)}^{(0)}+X_{(\\mathrm{sud})j}^{(0)}}&{\\mathrm{if~}j\\in\\{2\\ell+4,\\ldots,3\\ell+3\\},}\\\\ {0}&{\\mathrm{if~}j=3\\ell+4,}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{if~}j\\in\\{2\\ell+3,3\\ell+4\\},}\\\\ &{=\\binom{0}{\\sigma_{(3\\ell+5)-j}+\\sigma_{(4\\ell+6)-j}+\\sigma_{j}}\\quad\\mathrm{if~}j\\in\\{2\\ell+4,\\ldots,3\\ell+3\\},}\\\\ {\\left[U_{2}V_{2}X^{(0)}T_{2}\\right]_{\\{\\mathrm{RE-E0s}\\}}j=\\left\\{\\begin{array}{l l}{2/3}&{\\mathrm{if~}j=2\\ell+3,}\\\\ {1/2}&{\\mathrm{if~}j\\in\\{2\\ell+4,\\ldots,3\\ell+3\\},}\\\\ {1}&{\\mathrm{if~}j=3\\ell+4.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Refer to Table 21 for a concrete example of computing $U_{2}V_{2}X^{(0)}{\\pmb T}_{2}$ . Also, for the softmax errors, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{[U_{2}V_{2}X^{(0)}R_{2}]_{(\\mathrm{PRE}_{-}\\mathrm{CARRY})j}=\\displaystyle\\sum_{2\\leq i\\leq j}4X_{(\\mathrm{NUM})i}^{(0)}[{\\pmb R}_{1}]_{i j},}\\\\ &{\\displaystyle[U_{2}V_{2}X^{(0)}R_{2}]_{(\\mathrm{PRE}_{-}\\mathrm{EOS})j}=\\displaystyle\\sum_{1\\leq i\\leq j}2X_{(\\mathrm{IS}_{-}\\mathrm{BOS})i}^{(0)}[{\\pmb R}_{1}]_{i j}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Let us first obtain a bound of the softmax error term at dimension PRE_CARRY. If $j=2\\ell+3$ , since $\\begin{array}{r}{X_{(\\mathrm{NUM})(\\ell+2)}^{(0)}=X_{(\\mathrm{NUM})(2\\ell+3)}^{(0)}=0,}\\end{array}$ ", "page_idx": 38}, {"type": "equation", "text": "$$\n[U_{2}V_{2}X^{(0)}R_{2}]_{(\\mathrm{PRE}_{-}\\mathrm{CARF})(2\\ell+3)}=\\sum_{2\\le i\\le2\\ell+2}4X_{(\\mathrm{NUM})i}^{(0)}[R_{1}]_{i j}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "and ", "page_idx": 38}, {"type": "equation", "text": "$$\n0\\leq\\sum_{2\\leq i\\leq2\\ell+2}4X_{\\mathrm{(NUM)}i}^{\\mathrm{(0)}}[R_{1}]_{i j}\\leq\\frac{36(2\\ell)}{3e^{2M}+2\\ell}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "If $j\\in\\{2\\ell+4,\\ldots,3\\ell+3\\}$ , ", "page_idx": 38}, {"type": "equation", "text": "$$\nU_{2}V_{2}X^{(0)}R_{2}|_{(\\mathrm{PRE}_{-}\\mathrm{CARRY})j}=\\sum_{\\substack{i\\in\\{(3\\ell+5)-j,(4\\ell+6)-j,j\\}}}4X_{(\\mathrm{NUM})i}^{(0)}[R_{1}]_{i j}+\\sum_{\\substack{i\\neq(3\\ell+5)-j}}4X_{(\\mathrm{NUM})i}^{(0)}[R_{1}]_{i j},\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where ", "page_idx": 38}, {"type": "equation", "text": "$$\n0\\leq-\\sum_{i\\in\\{(3\\ell+5)-j,(4\\ell+6)-j,j\\}}4X_{(\\mathrm{NUM})i}^{(0)}[R_{1}]_{i j}\\leq\\frac{3\\cdot9(j-4)}{4e^{2M}+(j-4)}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "and ", "page_idx": 38}, {"type": "equation", "text": "$$\n0\\leq\\sum_{\\stackrel{2\\leq i\\leq j-1}{i\\neq(3\\ell+5)-j}}4X_{(\\mathrm{NUM})i}^{(0)}[R_{1}]_{i j}\\leq\\frac{36(j-4)}{4e^{2M}+(j-4)}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "And if $j=3\\ell+4=N$ , ", "page_idx": 38}, {"type": "equation", "text": "$$\n[U_{2}V_{2}X^{(0)}R_{2}]_{(\\mathrm{PRE}_{-}\\mathrm{CARF})N}=\\underbrace{4X_{\\mathrm{(NUM)}N}^{(0)}[R_{1}]_{N N}}_{\\mathrm{negative}}+\\sum_{2\\leq i\\leq N-1}4X_{\\mathrm{(NUM)}i}^{(0)}[R_{1}]_{i N},\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where ", "page_idx": 38}, {"type": "equation", "text": "$$\n0\\leq-4X_{\\mathrm{(NUM)}N}^{\\mathrm{(0)}}[R_{1}]_{N N}\\leq\\frac{18(N-2)}{2e^{2M}+N-2}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "and ", "page_idx": 38}, {"type": "equation", "text": "$$\n0\\leq\\sum_{2\\leq i\\leq N-1}4X_{(\\mathrm{NUM})i}^{(0)}[R_{1}]_{i N}\\leq\\frac{36(N-2)}{2e^{2M}+N-2}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Next, we obtain a bound of the softmax error term at dimension PRE_EOS. Since ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\sum_{1\\leq i\\leq j}2X_{(\\mathrm{IS}_{-}\\mathrm{BOS})i}^{(0)}[R_{1}]_{i j}=2X_{(\\mathrm{IS}_{-}\\mathrm{BOS})1}^{(0)}[R_{1}]_{1j},\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "the error term can be bounded as ", "page_idx": 38}, {"type": "equation", "text": "$$\n0\\leq-[U_{2}V_{2}X^{(0)}R_{2}]_{\\mathrm{(pR_{-},E o s)}j}\\leq\\left\\{\\begin{array}{l l}{\\displaystyle\\frac{2(j-3)}{3(3e^{2M}+j-3)}}&{\\mathrm{if~}j=2\\ell+3}\\\\ {\\displaystyle\\frac{2(j-4)}{4(4e^{2M}+j-4)}}&{\\mathrm{if~}j\\in\\{2\\ell+4,\\ldots,3\\ell+3\\},}\\\\ {\\displaystyle\\frac{(j-2)}{2e^{2M}+j-2}}&{\\mathrm{if~}j=3\\ell+4.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "We then can ensure that both $\\left|[U_{2}V_{2}X^{(0)}R_{2}]_{(\\mathrm{PRE}_{-}\\mathrm{SUM})j}\\right|$ and $\\left|[U_{2}V_{2}{\\pmb X}^{(0)}{\\pmb R}_{2}]_{(\\mathrm{PRE}_{-}\\mathrm{EOS})j}\\right|$ smaller than 0.1 for each $j\\in\\{2\\ell+3,\\ldots,3\\ell+4\\}$ , by letting $\\begin{array}{r}{M\\geq\\frac{1}{2}\\log(N)+3}\\end{array}$ . The proof is similar to the one that is presented for head 1. ", "page_idx": 39}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/a33199f3e4a81cef5a3d40532c341f860ad03d927ad08927191770c7416a6b0b.jpg", "table_caption": ["Table 18: Example of $Q_{2}X^{(0)}$ , continuing from Table 9. "], "table_footnote": [], "page_idx": 39}, {"type": "text", "text": "Table 19: Example of $K_{2}X^{(0)}$ , continuing from Table 9. ", "text_level": 1, "page_idx": 39}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/3d63e619b8feee51ec265d1877351c2d1f7e1777cdb64af6c611dd15c8cce041.jpg", "table_caption": [], "table_footnote": [], "page_idx": 39}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/6fef37694b67dff1c892ab41184f6113f921d9ec196337ed1fec514b7df9cc06.jpg", "table_caption": ["Table 20: Example of $U_{2}V_{2}X^{(0)}$ , continuing from Table 9. "], "table_footnote": [], "page_idx": 39}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/111dda7f66a318bc19483a2ccb39f9dd0fb2f030016b1fe9f502463ed1ef55ed.jpg", "table_caption": ["Table 21: Example of $U_{2}V_{2}X^{(0)}{\\pmb T}_{2}$ , continuing from Table 20. See Table 17 for definition of $\\mathbf{{T}}_{2}$ "], "table_footnote": [], "page_idx": 39}, {"type": "text", "text": "E.4.3 Residual Connection ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "So far we have computed the output of $\\tt A t t_{1}$ operation. Passing through the residual connection, the output of the attention layer is the sum of the original input encoding matrix and the output of Att operation: ", "page_idx": 39}, {"type": "equation", "text": "$$\n{\\cal Y}^{(1)}={\\bf X}^{(0)}+\\sum_{h\\in\\{1,2\\}}U_{h}V_{h}X^{(0)}T_{h}+\\sum_{h\\in\\{1,2\\}}U_{h}V_{h}X^{(0)}{\\cal R}_{h}\\,.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Since the term $\\begin{array}{r}{\\sum_{h\\in\\{1,2\\}}U_{h}V_{h}X^{(0)}T_{h}}\\end{array}$ has nonzero entries only at dimensions PRE_SUM, PRE_CARRY, and PRE_EOS, the residual connection plays a role of \u201cfilling in some blanks\u201d in the input encoding matrix. A concrete example of the output of residual connection is presented in Table 22, ignoring the softmax error term, whose entries have an absolute value smaller than 0.1. ", "page_idx": 40}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/f451f1d2c8c19ef742655250db6e63b739f929779486fc214d192f8a16ac6ef6.jpg", "table_caption": ["Table 22: Example output of residual connection, continuing from Tables 9, 15 and 21. Here we ignore the softmax error terms in the orange rows. The gray rows will be filled in later. "], "table_footnote": [], "page_idx": 40}, {"type": "text", "text": "E.5 Transformer Block \u2014 Token-wise Feed-forward Layer ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "The goal of the feed-forward layer is to fill in the blanks of the encoding matrix at dimensions SUM and IS_EOS. Be careful that the feed-forward layer can only implement token-wise mappings; a tokenwise mapping takes inputs only from the entries in the same column of the encoding matrix. Besides, the architecture of our feed-forward layer (except for the residual connection) is a one-hidden-layer ReLU network. ", "page_idx": 40}, {"type": "text", "text": "For a token $\\sigma_{i}$ for $i\\in\\{2\\ell+3,\\ldots,3\\ell+3\\}$ $\\operatorname{from}\\,^{\\ast}\\!=\\!^{\\ast}$ token to the second ), we will put a standard unit vector e1k0+1 to dimensions SUM if the next token is $k\\in\\{0,\\ldots,9\\}$ . ", "page_idx": 40}, {"type": "text", "text": "Recall from the discussion in Appendix E.4.2 that we can judge whether a carry 1 is generated at a certain position by exploiting only the digits (of the operands and the sum) in the same significance. Bringing the notation, let $a$ and $b$ be digits of the operands in the same significance, and $c$ be a digit of the sum in the same significance as $a$ and $b$ . Then the rule of recognizing that the addition of $a$ and $b$ generates a carry is that ", "page_idx": 40}, {"type": "equation", "text": "$$\n{\\left\\{\\begin{array}{l l}{{\\mathrm{If~}}a+b-c\\in\\{9,10\\},}\\\\ {{\\mathrm{Otherwise:if~}}a+b-c\\in\\{-1,0\\}}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "A simple case analysis shows that the value of $a+b-c$ must be one of $\\textstyle-1,0,9$ , and 10. Let us briefly check this claim in our example: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{6+0-7=-1;}&{\\qquad\\qquad}&{\\mathrm{no~carry~from~}6+0}\\\\ &{5+4-0=9;}&{\\qquad\\qquad}&{\\mathrm{there~is~a~carry~from~}5+4}\\\\ &{3+9-2=10.}&{\\qquad\\qquad}&{\\mathrm{there~is~a~carry~from~}3+9}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Recall that a noisy version of $a+b+c$ is already stored at dimension PRE_CARRY of ${\\pmb Y}^{(1)}$ , and $c$ is exactly at dimension NUM. Thus, we can (approximately) implement $a+b-c$ for a token $\\sigma_{j}$ by ", "page_idx": 40}, {"type": "equation", "text": "$$\nY_{(\\mathrm{PRE}_{-}\\mathrm{CARRY})j}^{(0)}-2Y_{(\\mathrm{NUM})j}^{(0)}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "This is a kind of token-wise linear transform, so we do not need to consume any hidden layer (with ReLU activation $\\phi$ ) to implement it. ", "page_idx": 40}, {"type": "text", "text": "Combining with Y ((P0R)E _SUM)j, a noisy version of addition without carry, we can indeed implement the addition. Note that a digit-wise addition should be done as ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathrm{addition}=\\mathrm{(addition~without~carry+\\mathbb{1}_{\\{\\mathrm{carry\\,propagates}\\}})}\\quad\\mathrm{mod}\\ 10.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "We first describe the formal construction of feed-forward network $\\mathsf{F F_{1}}$ for dimensions SUM and IS_EOS and then explain the intuition behind the construction. For the example result of applying the feed-forward network is presented in Table 23. ", "page_idx": 40}, {"type": "text", "text": "E.5.1 Subnetwork 1: Construction for SUM (dimension 7\u201316). ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Given a vector $\\pmb{y}=[\\pmb{y}_{j}]_{j=1}^{d}\\in\\mathbb{R}^{d}$ , define a linear function $g:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ as ", "page_idx": 41}, {"type": "equation", "text": "$$\ng(y):=y_{\\mathrm{{PRE}_{-}\\mathrm{{SUM}}}}+\\frac{y_{\\mathrm{{PRE}_{-}\\mathrm{{CARRY}}}}-2y_{\\mathrm{{NUM}}}}{10}+0.21=y_{3}+\\frac{y_{4}-2y_{1}}{10}+0.21\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "and consider a one-hidden-layer ReLU network $f_{k}:\\mathbb{R}\\to\\mathbb{R}\\,(k=0,1,\\dots,9)$ defined as ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{k}(x)=2\\Big[\\phi(x-(k-0.5))-\\phi(x-k)-\\phi(x-(k+0.5))+\\phi(x-(k+1))}\\\\ &{\\qquad\\qquad+\\;\\phi(x-(k+9.5))-\\phi(x-(k+10))-\\phi(x-(k+10.5))+\\phi(x-(k+11))\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Then we construct a subnetwork of our feed-forward network for a token $\\sigma_{j}$ by ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\left[\\operatorname{FF}_{1}\\left(\\pmb{Y}^{(1)}\\right)\\right]_{(\\operatorname{suM})j}=\\left[f_{0}\\left(g\\left(\\pmb{Y}_{\\bullet j}^{(1)}\\right)\\right)\\quad\\cdot\\cdot\\cdot\\quad f_{9}\\left(g\\left(\\pmb{Y}_{\\bullet j}^{(1)}\\right)\\right)\\right]^{\\top}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "image", "img_path": "5cIRdGM1uG/tmp/649362b5a866fbe2a79babdad14f90d34935711ab887f143bb41dfceeee1529a.jpg", "img_caption": ["Figure 23: Example plots of $f_{k}(x)$ defined in Equation (59). $(k=0,1,2,3)$ ) "], "img_footnote": [], "page_idx": 41}, {"type": "text", "text": "Explanation. The purpose of the first subnetwork is to generate a 10-dimensional one-hot vector whose position of 1 indicates the next digit: $e_{k}^{10}$ for the answer of next-token prediction $\\mathbf{\\nabla}^{\\bullet}k\\mathbf{\\cdot}$ . There are two cases where we need to predict the next token as $\\mathbf{\\nabla}k\\mathbf{\\cdot}$ : ", "page_idx": 41}, {"type": "text", "text": "\u2022 Case 1: (Addition without carry) $=k$ mod 10 and no carry propagates.   \n\u2022 Case 2: (Addition without carry $)=k-1$ mod 10 and there is a propagating carry 1. ", "page_idx": 41}, {"type": "text", "text": "In the first case, due to the softmax error (with magnitude at most 0.1), ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{Y_{(\\textsc{p r}_{-}\\mathrm{SUM})j}^{(0)}\\in[k-0.1,k+0.1]\\cap[k+9.9,k+10.1],}\\\\ &{}&{Y_{(\\textsc{p r}_{-}\\mathrm{CARRY})j}^{(0)}-2Y_{(\\textsc{n u m})j}^{(0)}\\in[-1.1,-0.9]\\cap[-0.1,0.1]\\subset[-1.1,0.1].}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "In the second case, again due to the softmax error (with magnitude at most 0.1), ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Y_{(\\textsc{p r}_{-}\\mathrm{sun})j}^{(0)}+1\\in[k-0.1,k+0.1]\\cap[k+9.9,k+10.1],}\\\\ {Y_{(\\textsc{p r}_{-}\\mathrm{carr})j}^{(0)}-2Y_{(\\textsc{p r})j}^{(0)}-10\\in[-1.1,-0.9]\\cap[-0.1,0.1]\\subset[-1.1,0.1].}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "In both cases, ", "page_idx": 42}, {"type": "equation", "text": "$$\nY_{\\mathrm{(PRE_{-}S U M)}j}^{\\mathrm{(0)}}+\\frac{Y_{\\mathrm{(PRE_{-}C A R R Y)}j}^{\\mathrm{(0)}}-2Y_{\\mathrm{(NUM)}j}^{\\mathrm{(0)}}}{10}+0.21\\in\\mathrm{[}k,k+0.32]\\cap\\mathrm{[}k+10,k+10.32\\mathrm{]}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "We can map the column Y \u2022(j0) to the set $[k,k\\!+\\!0.5]\\cap[k\\!+\\!10,k\\!+\\!10.5]$ if the next token is $\\sigma_{j+1}={}^{*}k^{*}$ . This job is done by the function $g$ . Note that the resulting sets $[k,k+0.5]\\cap[k+10,k+10.5]$ are disjoint for different $k$ \u2019s. ", "page_idx": 42}, {"type": "text", "text": "Recall that our objective is to output 1 to the dimension $k+6$ (among the dimensions 7, 8, ..., 16 in SUM) and to output 0 to the other dimensions in SUM if we need to predict $\\mathbf{\\nabla}k\\mathbf{\\cdot}$ as the next token. To this end, it is enough to map the set $[k,k+0.5]\\cap[k+10,k+10.5]$ to 1 and to map the other sets (for different $k$ \u2019s) to 0. This can be done by a ReLU network $f_{k}(x)$ is a ReLU network having two bumps at intervals $[k-0.5,k+1]$ and $[\\dot{k}+9.5,k+11]$ . In particular, $f_{k}(x)=1$ if $x\\in[\\bar{k},k+0.5]\\,\\bar{\\cup}\\,[k+10,k+\\bar{1}0.5]$ : see Figure 23 for an illustration. ", "page_idx": 42}, {"type": "text", "text": "Lastly, we have a desired one-hot vector output for each $j$ by taking a composition between $g$ and $[f_{0}(\\cdot),\\ldots,f_{9}(\\cdot)]^{\\top}$ as written in Equation (60). ", "page_idx": 42}, {"type": "text", "text": "E.5.2 Subnetwork 2: Construction for IS_EOS (dimension 17). ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "We move on to the dimension IS_EOS. For a token $\\sigma_{j}$ for $j\\in\\{2\\ell+3,\\ldots,3\\ell+4\\}$ , if $k$ is the next token, we will put $\\mathbb{1}_{\\{k=\\mathbb{8}\\}}$ to dimension IS_EOS: 1 if $k$ is the special token $\\mathbf{\\cdot}\\mathbb{S}^{\\bullet}$ and 0 otherwise. To this end, we define a ReLU network $h:\\mathbb{R}\\to\\mathbb{R}$ as ", "page_idx": 42}, {"type": "equation", "text": "$$\nh(x)=10\\phi\\left(x-0.8\\right)-10\\phi\\left(x-0.9\\right).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Then, we can construct a subnetwork of our feed-forward network for a token $\\sigma_{j}$ by ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\left[\\mathrm{FF}_{1}\\left(\\pmb{Y}^{(1)}\\right)\\right]_{(\\mathrm{IS_{-}E O S})j}=h\\left(\\pmb{Y}_{(\\mathrm{PRE_{-}E O S})j}^{(1)}\\right).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Explanation. Note that for columns $j~\\in~\\{2\\ell+3,\\ldots,3\\ell+4\\}$ , if we consider the presence of softmax errors with magnitude at most 0.1, the values that Y ((P1R)E_ EOS)j can have lie in the set $[0.4,0.6]\\cap[2/3-0.1,2/3+0.1]\\cap[0.9,1.1]\\,\\subset\\,(-\\infty,0.8)\\cap[0.9,\\stackrel{\\cdot}{\\infty})$ $Y_{(\\mathrm{PRE\\_EOS})j}^{(1)}\\geq0.9$ $h$ with two neurons. ", "page_idx": 42}, {"type": "text", "text": "Remarks: ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "\u2022 In total, we consume $8\\times10+2=82$ ReLU neurons in our feed-forward network $\\mathsf{F F}_{1}$ . However, it is possible to construct the addition Transformer with a smaller number of neurons, with a slight modification in the linear readout of the decoding function (Appendix E.6). \u2022 Unlike in the attention layer, now we do not have to worry about softmax errors in the output since the feed-forward ReLU network plays the role of denoiser. ", "page_idx": 42}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/eef5512b8a1f5576f994d6a18b3da13b2a306f140d03951a0c45987dce7c1466.jpg", "table_caption": ["Table 23: Example output after applying the feed-forward network. "], "table_footnote": [], "page_idx": 42}, {"type": "text", "text": "E.5.3 Residual Connection ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "The last task of the feed-forward layer is to pass $\\mathbf{F}\\mathbf{F}_{1}\\left(\\mathbf{Y}^{(1)}\\right)$ through the residual connection. As a result, we have ", "page_idx": 43}, {"type": "equation", "text": "$$\nX^{(1)}=Y^{(1)}+\\operatorname{FF}_{1}\\left(Y^{(1)}\\right).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "A concrete example of the output of the second residual connection is showcased in Table 24. ", "page_idx": 43}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/90a199095942669db94104f076085a2e3d508cea4a7ff7f02a294f5bb0d7732d.jpg", "table_caption": ["Table 24: Example output of residual connection, continuing from Table 23. Here we ignore the softmax error terms in the orange rows. "], "table_footnote": [], "page_idx": 43}, {"type": "text", "text": "E.6 Decoding Function ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "As mentioned in Appendix D, the decoding function performs a linear readout (with a weight matrix $W_{\\mathrm{out}}\\in\\mathbb{R}^{|\\mathcal{V}|\\times d})$ and a (token-wise) arg-max operation. That is, ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathsf{D e c}\\left(\\mathbf{{X}}^{(1)}\\right):=\\left(\\mathcal{V}_{k_{i}}\\right)_{i=1,\\ldots,N}\\in\\mathcal{V}^{N},\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where $\\mathcal{V}_{k}$ is the $k$ -th element of $\\mathcal{V}$ and ", "page_idx": 43}, {"type": "equation", "text": "$$\nk_{i}:=\\underset{k\\in[|\\mathcal{V}|]}{\\arg\\operatorname*{max}}\\left\\{o_{k}:W_{\\mathrm{out}}\\pmb{X}_{\\bullet i}^{(1)}=\\left[o_{1}\\quad\\cdot\\,\\cdot\\,\\cdot\\quad o_{|\\mathcal{V}|}\\right]^{\\top}\\right\\}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "The objective of the decoding function is to perform a proper next-token prediction for addition, especially utilizing the dimensions SUM and IS_EOS of $X^{(1)}$ . ", "page_idx": 43}, {"type": "text", "text": "We now construct the weight matrix $W_{\\mathrm{{out}}}$ . For a token $\\sigma_{i}$ , if the value of dimension IS_EOS of $X^{(1)}$ is 0, then the linear readout output the dimensions SUM as it is to return one of a number token (0-9). On the other hand, if the value of dimension IS_EOS is 1, then the linear readout outputs a large number (like 100 for example) for the token $\\mathbf{\\cdot}\\mathbb{S}^{\\,,}$ to return EOS $(\\mathfrak{G})$ . This can be implemented by the weight matrix $W_{\\mathrm{{out}}}$ described in Table 25. Also, an example of applying the linear transform is showcased in Table 26. ", "page_idx": 43}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/8eb4d226982b8858b61ad46f6f15f8c86b19937078a2e6616a2c1ecb3936b984.jpg", "table_caption": ["Table 25: The transposed weight matrix $\\underline{W_{o u t}^{\\top}}$ of the linear readout in decoding function. "], "table_footnote": [], "page_idx": 44}, {"type": "text", "text": "Table 26: Example output of linear readout $(W_{\\mathrm{out}}X^{(1)})$ , continuing from Tables 24 and 25. The yellow cells represent the maximum value of each column, from the $\\bullet_{=},$ token\u2019s column to the rightmost column (used for next-token prediction). ", "page_idx": 44}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/1b74ddfc1df4d27a440fdf934786a969063d8f47c38e4308706544715f5ff5e4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 44}, {"type": "text", "text": "Table 27: Example output sequence ${\\mathcal{O}}=\\mathsf{D e c}\\left(X^{(1)}\\right)$ , continuing from Table 26. The yellow cells in the bottom row exactly predict the next tokens. ", "page_idx": 44}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/cfd1fced92aa14ae3ac7dbfe8f276fb4197fb6bc016432430bc1692ed120afa1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 44}, {"type": "text", "text": "F Impossibility of Addition with No Positional Encoding ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "For the sake of readability, we restate the proposition below. ", "page_idx": 45}, {"type": "text", "text": "Proposition 5.2. Consider any depth-1 finite-head decoder-only Transformer model $\\tau$ without positional encoding (NoPE). Given an input sequence $\\mathcal{T}$ and its arbitrary permutation ${\\mathcal{Z}}^{\\prime}$ , if the last tokens of $\\mathcal{Z}$ and ${\\mathcal{Z}}^{\\prime}$ are identical, then the next tokens predicted by $\\tau$ will also be identical for both sequences when applying a greedy decoding scheme. ", "page_idx": 45}, {"type": "text", "text": "Remark. We assume the 1-layer $\\mathcal{L}=1$ ) $H$ -head Transformer achitecture specified in Appendix D. Although it omits normalization layers, we remark that Proposition 5.2 remains valid even for the architecture with a standard layer normalization (Ba et al., 2016) or its variants (e.g., Zhang and Sennrich, 2019). ", "page_idx": 45}, {"type": "text", "text": "Proof. We keep following the notation about matrices introduced in Appendix E.1. Throughout the proof, we denote the value/vector/matrix related to ${\\mathcal{Z}}^{\\prime}$ by appending \u2018\u2032\u2019 to it. ", "page_idx": 45}, {"type": "text", "text": "Let encoding matrices generated from the input sequences $\\mathcal{Z},\\mathcal{Z}^{\\prime}\\in\\mathcal{V}^{N}$ as ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r}{X:=\\mathtt{E n c}(\\mathbb{Z})\\in\\mathbb{R}^{d\\times N}\\quad\\mathrm{and}\\quad X^{\\prime}:=\\mathtt{E n c}(\\mathbb{Z}^{\\prime})\\in\\mathbb{R}^{d\\times N}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Since there is no positional encoding, the encoding function $\\mathtt{E n c}(\\cdot)$ maps the same tokens to the same columns. In particular, $\\mathcal{T}_{i}=\\mathcal{T}_{j}^{\\prime}$ implies $X_{\\bullet i}=X_{\\bullet j}^{\\prime}$ . Since we assume that ${\\mathcal{Z}}^{\\prime}$ is a permutation of $\\mathcal{T}$ such that $\\mathcal{Z}_{N}=\\mathcal{Z}_{N}^{\\prime}$ , there exists a bijection $\\pi:[N]\\stackrel{}{\\rightarrow}[N]$ such that $\\mathcal{T}_{i}^{\\prime}=\\mathcal{T}_{\\pi(i)}$ for each $i\\in[N]$ and $\\pi(N)=N$ . Then, it follows that $X_{\\bullet i}^{\\prime}=X_{\\bullet(\\pi(i))}$ for each $i$ and, specifically, $X_{\\bullet N}^{\\prime}=X_{\\bullet N}$ . ", "page_idx": 45}, {"type": "text", "text": "Recall that the single $H$ -head attention layer Att : $\\mathbb{R}^{d\\times N}~\\rightarrow~\\mathbb{R}^{d\\times N}$ operates as $\\tt{A t t}(X)\\ {=}\\,$ $\\begin{array}{r}{\\sum_{h=1}^{H}{\\tt H e a d}_{h}(X)}\\end{array}$ where the attention head $h$ is defined as ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\mathtt{H e a d}_{h}(X):=U_{h}V_{h}X\\cdot\\mathsf{s o f}\\tan\\mathtt{a x}\\left((K_{h}X)^{\\top}Q_{h}X\\right)\\in\\mathbb{R}^{d\\times N},\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where $Q_{h},K_{h}\\in\\mathbb{R}^{d_{Q K}\\times d}$ , $V_{h}\\in\\mathbb{R}^{d_{V}\\times d}$ and $U_{h}\\in\\mathbb{R}^{d\\times d_{V}}$ . ", "page_idx": 45}, {"type": "text", "text": "Claim: $\\left[\\operatorname{Head}_{h}(X)\\right]_{\\bullet N}=[\\operatorname{Head}_{h}(X^{\\prime})]_{\\bullet N}$ for all $h\\in[H]$ . ", "page_idx": 45}, {"type": "text", "text": "The claim suffices to prove the proposition because of the following: first, the claim implies that the last ( $N$ -th) columns of the attention layer outputs are the same, i.e., $\\lceil\\mathsf{A t t}(X)\\rceil_{\\bullet N}=\\lceil\\mathsf{A t t}(X^{\\prime})\\rceil_{\\bullet N}$ Note that the operations after the attention layer\u2014residual connections, FF, and Dec\u2014all operate in a token-wise (column-by-column) manner: the $j$ -th column of the output of a token-wise operation is a function of $j$ -th column of the input for the operation. Therefore, the last column of the attention layer output totally determines the next-token prediction at $N$ -th input token. As a result, the predicted next-tokens are the same for $\\mathcal{T}$ and ${\\mathcal{Z}}^{\\prime}$ . ", "page_idx": 45}, {"type": "text", "text": "The rest of the proof is devoted to proving the aforementioned claim. Fix any $h\\in[H]$ . Let ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left[\\mathrm{softmax}\\left((K_{h}X)^{\\top}Q_{h}X\\right)\\right]_{\\bullet N}=\\left[s_{1}\\quad\\ldots\\quad s_{N}\\right]^{\\top},}\\\\ {\\left[\\mathrm{softmax}\\left((K_{h}X^{\\prime})^{\\top}Q_{h}X^{\\prime}\\right)\\right]_{\\bullet N}=\\left[s_{1}^{\\prime}\\quad\\ldots\\quad s_{N}^{\\prime}\\right]^{\\top},}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "which are both stochastic (sum to 1) column vectors. Considering that we are taking the last column of the softmax output, it follows that $s_{i}^{\\prime}=s_{\\pi(i)}$ for each $i\\in[N]$ : this can be proved by applying the definition of the softmax operation and the fact ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\left[(K_{h}X^{\\prime})^{\\top}Q_{h}X^{\\prime}\\right]_{i N}=X_{\\bullet i}^{\\prime\\top}K_{h}^{\\top}Q_{h}X_{\\bullet N}^{\\prime}=X_{\\bullet\\pi(i)}^{\\top}K_{h}^{\\top}Q_{h}X_{\\bullet N}=\\left[(K_{h}X)^{\\top}Q_{h}X\\right]_{(\\pi(i))N}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Consequently, since ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{N}s_{i}^{\\prime}\\mathbf{X_{\\bulleti}^{\\prime}}=\\sum_{i=1}^{N}s_{\\pi(i)}X_{\\bullet(\\pi(i))}=\\sum_{i=1}^{N}s_{i}X_{\\bullet i},\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "we have ", "page_idx": 45}, {"type": "equation", "text": "$$\nX^{\\prime}\\cdot\\left[\\operatorname{softnax}\\left((K_{h}X^{\\prime})^{\\top}Q_{h}X^{\\prime}\\right)\\right]_{\\bullet N}=X\\cdot\\left[\\operatorname{softnax}\\left((K_{h}X)^{\\top}Q_{h}X\\right)\\right]_{\\bullet N}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Therefore, the claim holds. This concludes the proof. ", "page_idx": 45}, {"type": "text", "text": "Here, we provide the Python code that calculates the maximum possible exact-match accuracy that a 1-layer Transformer with NoPE can achieve for the $m$ -digit addition problem. ", "page_idx": 46}, {"type": "text", "text": "from itertools import product from collections import defaultdict m = 4 # Change here total $\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\quad0$ counter_dict $=$ defaultdict(dict) for a, b in product(product(range (10) , repeat $=\\mathtt{m}$ ), product(range (10) , repeat $=\\mathtt{m}$ )): if $a\\left[0\\right]\\;\\;\\;{\\bf=}\\;\\;0$ or b $[0]\\quad=\\quad0$ : continue total $+=~~1$ $\\texttt{c}=$ tuple(sorted $(a+b)$ ) a_num $=$ int(\u2019\u2019.join(map(str , a))) b_num $=$ int(\u2019\u2019.join(map(str , b))) ab_sum $=$ a_num $^+$ b_num if ab_sum in counter_dict[c]: counter_dict[c][ ab_sum] $+=~~1$ else: counter_dict[c][ ab_sum] = 1 count $=$ sum(max(d.values ()) for _, d in counter_dict .items ()) print( $\"\\mathtt{m}\\ =\"$ m) print(\"Permutation Invariant Additions Count:\" count) print(\" Total m-digit Additions Count:\" total) print(\" Ratio:\", count / total) 11 11 11 [Example Outputs] $\\mathrm{~\\textit~{~m~}~}=\\mathrm{~\\textit~{~1~}~}$ Permutation Invariant Additions Count: 81 Total m-digit Additions Count: 81 Ratio: 1.0 $34~\\mathrm{~m~}~=~2$ Permutation Invariant Additions Count: 2668 Total m-digit Additions Count: 8100 Ratio: 0.32938271604938274 $\\mathrm{~\\textbf~{~m~}~}=\\mathrm{~\\textbf~{~3~}~}$ Permutation Invariant Additions Count: 50150 Total m-digit Additions Count: 810000 Ratio: 0.06191358024691358 $\\mathrm{~\\textit~{~m~}~}=\\mathrm{~\\textit~{~4~}~}$ Permutation Invariant Additions Count: 765139 Total m-digit Additions Count: 81000000 Ratio: 0.00944616049382716 $\\mathrm{~\\textit~{~m~}~}=\\mathrm{~\\textit~{~5~}~}$ Permutation Invariant Additions Count: 10033314 Total m-digit Additions Count: 8100000000 Ratio: 0.0012386807407407407 ", "page_idx": 46}, {"type": "text", "text": "G (Formal) Construction of $N\\times2$ Multiplication Transformer with Position Coupling ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Here we show how to implement the $N\\times2$ multiplication using a depth-2 decoder-only Transformer equipped with position coupling. Our construction involves 3 heads in the first Transformer block and 7 heads in the second Transformer block, requiring a total of 10 heads. ", "page_idx": 47}, {"type": "text", "text": "Theorem 6.1. Given an appropriate format of the input sequence, there exists a depth-2 decoder-only Transformer model with coupled positions that can perform the $N\\times2$ multiplication task with next-token prediction. Here, the number of the total heads is $I O$ and the length of the first operand is at most $2^{\\lfloor\\left(\\stackrel{\\cdot}{d}-34\\right)/6\\rfloor}-3$ , where we denote the token embedding dimension by $d\\geq46$ . ", "page_idx": 47}, {"type": "text", "text": "We note that our construction for the $N\\times2$ multiplication task permits the use of multiple FFN layers at the second decoder block. However, we believe that there exists a potential improvement in our construction, wherein a single FFN layer could suffice for each decoder block, leveraging the expressivity of the neural network. Additionally, we do not provide a detailed error analysis but assume that the softmax operation with sufficiently large attention weights can reduce small attention scores to zero values, thereby clearly revealing the desired attention patterns. ", "page_idx": 47}, {"type": "text", "text": "G.1 Notation ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Consider an ordered vocabulary $\\mathcal{V}=(0,1,2,3,4,5,6,7,8,9,\\times,=,\\ast)$ . We include a special token $\\cdot\\Phi^{\\bullet}$ that plays the role of both the beginning-of-sequence (BOS) token and the end-of-sequence (EOS) token. We denote $\\mathcal{V}_{k}$ as $k$ -th element of $\\nu$ . For instance, $\\nu_{4}=3$ and $\\rangle\\!\\!\\nu_{13}=\\mathbb{5}$ . Unlike the addition task, our construction for the multiplication involves multiple layers and hence we do not omit the superscripts $(l)$ in the parameter matrices/vectors and the size of dimensions. ", "page_idx": 47}, {"type": "text", "text": "G.2 Input Sequence ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Our objective is to use next-token prediction for implementing $a\\times b=c$ . To this end, we want to transform it into an input sequence ${\\mathcal{T}}={\\overline{{\\Phi A\\times B=C}}}$ of an appropriate format. Let $\\ell_{a}$ and $\\ell_{b}$ represent the lengths of $a$ and $b$ , respectively, and we denote their sum as $\\ell=\\ell_{a}+\\ell_{b}$ . While our immediate focus is on the case where $\\ell_{b}=2$ , it is worth noting that our approach can be extended to the case where $\\ell_{b}>2$ , as the key insight for the construction does not rely on $\\ell_{b}$ . Thus, we present the input sequence and encoding function in a more general form applicable to $\\ell_{b}\\geq2$ . ", "page_idx": 47}, {"type": "text", "text": "Unlike the addition case, we do not zero-pad both $a$ and $b$ . Instead, we only zero-pad the response, as the length of $c$ may either equal the sum of the lengths of $a$ and $b$ , or be less than the sum of their lengths by 1. Hence, we zero-pad in front of $c$ for the latter case to fix the length of $c$ by $\\ell$ . We also reverse the response $c$ to make the part $C$ . For instance, if we have $312\\times24=7488$ , the input sequence transforms to $\\overline{{{\\mathbb{S}}}}312\\times24=88470,$ . If we have $589\\times62=36518$ , then the input sequence would be $\\textstyle{\\overline{{\\mathbb{5589}\\times62=81563}}}$ . The red digit is a zero-padding, and the blue digits are the reversed product. ", "page_idx": 47}, {"type": "text", "text": "To recap, the input sequence $\\mathcal{T}=\\overline{{\\sigma_{1}\\sigma_{2}\\ldots\\sigma_{N}}}\\in\\mathcal{V}^{N}$ of length $N=2\\ell+3$ consists of six parts: ", "page_idx": 47}, {"type": "text", "text": "1. the BOS token $\\sigma_{1}={}^{\\bullet}\\mathfrak{F}$ \u2019   \n2. the first operand $A=\\overline{{\\sigma_{2}\\ldots\\sigma\\ell_{a}\\!+\\!1}}$ where $\\sigma_{i}\\in\\{0,\\ldots,9\\}$ ;   \n3. the multiplication symbol $\\sigma_{\\ell_{a}+2}={^{\\ast}\\times}^{\\ast}$ ;   \n4. the second operand $B=\\overline{{\\sigma\\ell_{a}\\!+\\!3\\cdot\\!.\\cdot\\sigma\\ell\\!+\\!2}}$ (note that $\\ell=\\ell_{a}+\\ell_{b})$ where $\\sigma_{i}\\in\\{0,\\ldots,9\\}$ ;   \n5. the equality symbol $\\sigma_{\\ell+3}=\\mathrm{`=}^{\\prime}$ ;   \n6. the (reversed) product $C=\\overline{{\\sigma_{\\ell+4}\\ldots\\sigma_{2\\ell+3}}}$ where $\\sigma_{i}\\in\\{0,\\ldots,9\\}$ . ", "page_idx": 47}, {"type": "text", "text": "Note that the part $C$ might be incomplete (i.e., $N\\,<\\,2\\ell+3)$ at the inference time; we infer the digits of the part $C$ one by one using next-token prediction. Throughout this section on a formal construction, however, we only consider the train time setup in which we infer all the digits of the part $C$ at once using simultaneous next-token prediction in a single forward pass. Precisely, we want to use an input sequence $\\mathcal{T}=\\overline{{\\sigma_{1\\,\\cdot\\,\\cdot\\,\\sigma_{N}}}}$ to produce an output sequence $\\mathcal{O}=\\overline{{\\sigma_{1}^{\\prime}\\ldots\\sigma_{N}^{\\prime}}}$ where $\\overline{{\\sigma_{\\ell+3}^{\\prime}\\cdot\\cdot\\cdot\\sigma_{N-1}^{\\prime}}}=C=\\overline{{\\sigma_{\\ell+4}\\cdot\\cdot\\cdot\\cdot\\sigma_{N}}}$ and $\\sigma_{N}^{\\prime}={}^{\\bullet}\\mathfrak{F}^{\\bullet}$ (EOS). ", "page_idx": 47}, {"type": "text", "text": "G.3 Encoding Function ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "We now explain the input embedding for given an input sequence $\\mathcal{T}$ designed as above. The embedding matrix $X^{(0)}$ is of size $d\\times N$ : each column represents an embedding vector for a token, while each row represents a particular named dimension. We concatenate the token embedding and the position embedding, which can be viewed as a sum of two different embedding matrices of the same size. ", "page_idx": 48}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/09d4080ca8037adb8b89ce995ef8c97791deba697ec53a2bc0d38650388e3111.jpg", "table_caption": ["Table 28: Example initial encoding. Here we consider the input sequence $\\overline{{\\mathbb{S}7595\\times79=500006}}$ and the starting position ID is chosen as $s=1$ . The vectors $v_{\\boxminus}^{\\vec{P}}$ are defined in Equation (79). The gray rows will be filled in later. "], "table_footnote": [], "page_idx": 48}, {"type": "text", "text": "G.3.1 Token Embedding ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "The token embedding consists of $(34+P)$ dimensions, where $P$ represents the dimension for the position embedding which will be described in the very next section. While the token embedding dimension for the addition task was independent of $P$ , our construction strategy for the multiplication task involves copying the position embedding into the token embedding. This is why we have the $P$ term in our token embedding dimension. For the first 34 dimensions, we label them as: ", "page_idx": 48}, {"type": "text", "text": "$1\\mathrm{=}$ NUM, $2\\!=$ FULL_ONES, 3=IS_BOS, $4\\mathrm{=}$ IS_MUL, $5\\mathrm{=}$ IS_EQUAL, 6=IS_OP2_ONE, 7=IS_OP2_TEN, $8{=}$ OP2_ONE, $\\}=$ OP2_TEN, 10=OP1_SHIFT0, 11=OP1_SHIFT1, 12=OP1_SHIFT2, 13=OP1_SHIFT3, 14=OP1_SHIFT4, $15\\mathrm{=}$ RESULT1, 16=RESULT2, 17=RESULT3, 18=RESULT4, 19=PRE_PROD, 20 $\\leftrightharpoons$ PRE_CARRY, 21 $=$ PRE_EOS1, 22 $=$ PRE_EOS2 {23,...,32}=PROD, 33=IS_EOS, 34=MASK, ", "page_idx": 48}, {"type": "text", "text": "and for the last $P$ dimensions $(\\{35,...,34+P\\})$ ), we named them as POS_2_MASK. ", "page_idx": 49}, {"type": "text", "text": "The initial token embedding flils only NUM, FULL_ONES, IS_BOS, IS_MUL, and IS_EQUAL, leaving the other $(29+P)$ dimensions empty (i.e., all zeros). These $(29+P)$ dimensions will be filled by passing through the layers. Here we describe how we fill the first 5 dimensions. ", "page_idx": 49}, {"type": "text", "text": "Dimension 1 (NUM). For a number token $(0,\\ldots,9)$ , we put itself into the dimension NUM. For the other tokens $(\\times,=,\\Phi)$ , we put 0. ", "page_idx": 49}, {"type": "text", "text": "Dimension 2 (FULL_ONES). We put 1 everywhere in this dimension. ", "page_idx": 49}, {"type": "text", "text": "Dimension 3 (IS_BOS). For a special token $\\mathbf{\\cdot}\\Phi^{\\bullet}$ , we put 1 into the dimension IS_BOS. Otherwise, we put 0. ", "page_idx": 49}, {"type": "text", "text": "Dimension 4 (IS_MUL). For a special token $\\times^{\\,,}$ , we put 1 into the dimension IS_MUL. Otherwise, we put 0. ", "page_idx": 49}, {"type": "text", "text": "Dimension 5 (IS_EQUAL). For a special token $\\\"=\"$ , we put 1 into the dimension IS_EQUAL.   \nOtherwise, we put 0. ", "page_idx": 49}, {"type": "text", "text": "G.3.2 Coupled Position IDs and Position Embedding ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "We now specify the allocation of coupled position IDs for the $N\\times M$ multiplication task as the following: given an input sequence $\\mathcal{T}=\\overline{{\\sigma_{1\\,\\cdot\\,\\cdot\\,\\sigma_{N}}}}$ , ", "page_idx": 49}, {"type": "equation", "text": "$$\np(i)=\\left\\{\\!\\!\\begin{array}{l l}{0,}&{i=1,}\\\\ {s+i-2+\\ell_{b},}&{i=2,\\ldots,\\ell_{a}+2,}\\\\ {s+i-3,}&{i=\\ell_{a}+3,\\ldots,\\ell+3,}\\\\ {s-i+3+2\\ell}&{i=\\ell+4,\\ldots,2\\ell+3.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Compared to the addition case, the position allocating function $p$ becomes more complicated since the length of two operands can be different, but the core remains simple: coupling the position IDs for the least significant digit in the first operand $(A)$ , the second operand $(B)$ , and the result $(C)$ , and then decreasing the IDs as the digit position increases for each $A,B.$ , and $C$ . ", "page_idx": 49}, {"type": "text", "text": "Now we explain the position embedding. We utilize the same $\\pmb{v}_{k}^{D}$ $(k\\in[2^{D}])$ defined for the addition task, specifically ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\pmb{v}_{k}^{D}=\\left[(-1)^{b_{i}^{(D,k)}}\\right]_{i=1}^{D}\\in\\mathbb{R}^{D}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where $b_{i}^{(D,k)}$ is defined as the $i^{\\th}$ -th (from left) digit of $D$ -digit binary representation of $k-1$ . Using $\\pmb{v}_{k}^{D}$ , we design the position embedding for each position ID $p(i)$ by ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left[\\begin{array}{l}{\\pmb{v}_{p(i)}^{P}}\\\\ {\\pmb{v}_{p(i)+1}^{P}}\\\\ {\\pmb{v}_{p(i)+2}^{P}}\\\\ {\\pmb{v}_{p(i)+3}^{P}}\\\\ {\\pmb{v}_{p(i)+4}^{P}}\\end{array}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "The first $P$ dimensions of the position embedding are named as $\\mathrm{POS}_{-}1$ , and subsequent sets of $P$ dimensions are named as POS_2, POS_3, POS_4, and POS_5, respectively. Thus, the position embedding is a 5P-dimensional vector. In case of p(i)+j (j \u2208[4]) exceeding 2P , we use vpP(i)+j\u22122P instead of ${\\pmb v}_{p(i)+j}^{P}$ . If $p(i)=0$ , we let ${\\bf0}_{5P}$ as a position embedding vector. ", "page_idx": 49}, {"type": "text", "text": "By concatenating the token embedding and the position embedding, we get the input embedding $X^{(0)}$ . Specifically, the position embedding is placed under the token embedding $(P+35)$ -th to $(6P+34)$ -th dimension). See Table 9 for an example. As a result, the total embedding dimension is $d\\,=\\,6P+34$ . Note the maximum possible position ID that can be represented with $\\pmb{v}_{k}^{P}$ \u2019s is m $\\mathfrak{x}\\_{\\mathsf{p o s}}=2^{P}=2^{\\lfloor(d-34)/6\\rfloor}$ . Therefore, the length of the first operand must be $\\ell_{a}\\le\\mathtt{m a x\\_p o s}-$ $\\ell_{b}-\\bar{1}=2^{\\lfloor(d-34)/6\\rfloor}-\\ell_{b}\\!-\\!1.$ . For the case when $\\ell_{b}=2$ , this inequality becomes $\\ell_{a}\\leq2^{\\lfloor(d-34)/6\\rfloor}-3$ ", "page_idx": 49}, {"type": "text", "text": "G.4 Construction Idea ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Here, we provide an example that demonstrates how we construct the $N\\times2$ multiplication. Consider the calculation $7595\\times79=600005$ . While a typical method for computing such a multiplication is illustrated in Table 29, we consider an alternative approach, as shown in Table 30. In this method, we pair the digits from the first and second operands at each step where the sum of their digit positions is the same, and then calculate the sum of the pairwise products. For example, the number 116 in Table 30 is generated by $9\\times9+5\\times7$ , and the number 108 is generated by $5\\times9+9\\times7$ , where blue indicates numbers from the first operand and red indicates numbers from the second operand. The main reason for considering such a method is to provide a clearer intuition for determining which numbers from each operand we should attend to when predicting the next token. ", "page_idx": 50}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/c1d91a633cf68b5b2d012fff654d9ee549288d350650fbcc1887a5aeaeb27564.jpg", "table_caption": ["Table 29: Multiplication I "], "table_footnote": [], "page_idx": 50}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/2a76e10c5f2c1fba812c57ad64f834845466bb5ba764e89f2d109e36bf6dd020.jpg", "table_caption": ["Table 30: Multiplication II "], "table_footnote": [], "page_idx": 50}, {"type": "text", "text": "Suppose the current input sequence is $\\overline{{\\mathbb{S}7595\\times79=5000}}$ . During this step, the model is tasked with predicting 0 (the 0 just before 6) for the next token. As illustrated in Table 30, this 0 is computed from the sum of 9, 9, 1, and an additional 1, representing the carry from the previous step. Similar to the explanation in E.4.2, we highlight that the carry 1 can be detected by computing 8 (ones digit of $98)+0$ (tens digit of $108)+1$ (hundreds digit of $116)-0$ (current token): yielding a result of 9, indicating the occurrence of a carry 1. ", "page_idx": 50}, {"type": "text", "text": "In summary, the correct prediction of the next token 0 (the 0 just before 6) can be achieved by summing the main summation part and the carry part, where the main summation part is computed using 49, 98, 108, and the carry part is calculated using 98, 108, and 116. Additionally, it\u2019s noteworthy to detail the breakdowns: ", "page_idx": 50}, {"type": "text", "text": "\u2022 $49=0\\times9+7\\times7,$ \u2022 $98=7\\times9+5+7$ , \u2022 $108=5\\times9+9+7,$ \u2022 $116=9\\times9+5+7.$ ", "page_idx": 50}, {"type": "text", "text": "Thus, for predicting the next token, we need 0, 7, 5, 9, 5, 9, 7. Here, we highlight that this structure, requiring 5 consecutive tokens from the first operand and every token from the second operand for the next-token prediction, remains unchanged for any prediction time and any query length. ", "page_idx": 50}, {"type": "text", "text": "As we will see in the later subsection, a depth-2 decoder-only Transformer model can be constructed to flil OP2_ONE by 9, OP2_TEN by 7, and OP1_SHIFT0 to OP1_SHIFT4 by 0, 7, 5, 9, and 5, respectively. One may be concerned that 0 is not given in the first operand at the input sequence. This requirement of 0 beyond the most significant digit arises in the later stage of the prediction, i.e., predicting the token that is near the most significant digit of the response. Although 0 is not explicitly given in the first operand, our construction can automatically manage as if the 0 were originally at the start of the first operand. A similar situation occurs in the early stage of the prediction that 0 is required before the least significant digit of the first operand, and our construction is also capable of handling this issue. ", "page_idx": 50}, {"type": "text", "text": "Consequently, the embedding vector of the current token 0 (the 0 preceding 60) will be structured as the left-most table in Table 31, with some irrelevant dimensions omitted for readability. We then utilize a feed-forward layer to fill ", "page_idx": 50}, {"type": "text", "text": "\u2022 RESULT1 with OP1_SHIFT $0\\times$ OP2_ONE + OP1_SHIFT1 \u00d7 OP2_TEN, \u2022 RESULT2 with OP1_SHIFT1 \u00d7 OP2_ONE + OP1_SHIFT2 \u00d7 OP2_TEN, \u2022 RESULT3 with $\\mathrm{OP1}_{\\bf-}\\mathrm{SHIFT2}\\times\\mathrm{OP2}_{\\bf-}\\mathrm{ONE}+\\mathrm{OP1}_{\\bf-}\\mathrm{SHIFT3}\\times\\mathrm{OP2}_{\\bf-}\\mathrm{TEN},$ \u2022 RESULT4 w $\\mathrm{ith~OP1\\!_{-}S H I F T3\\times O P2\\_O N E}+\\mathrm{OP1\\!_{-}S H I F T4\\times O P2\\_T E N}.$ . ", "page_idx": 50}, {"type": "text", "text": "", "page_idx": 51}, {"type": "text", "text": "The result is illustrated in the center table of Table 31. Next, we employ an additional feed-forward layer to fill ", "page_idx": 51}, {"type": "text", "text": "\u2022 PRE_PROD with ones digit of RESULT1 $\\cdot+$ tens digit of RESULT $^{2+}$ hundreds digit of RESULT3, \u2022 PRE_CARRY with ones digit of RESULT $^{2+}$ tens digit of RESULT $3+$ hundreds digit of RESULT4. ", "page_idx": 51}, {"type": "text", "text": "These computations yield the result illustrated in the right-most table of Table 31. Once this process is done, we can finally predict the next token by the following two steps: ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\bullet\\,\\,\\,\\mathrm{CARRY}=\\left\\{\\!\\!\\begin{array}{l l}{0,}&{\\mathrm{if~}\\mathrm{PRE\\_CARRY}-\\mathrm{NUM\\in\\{-2,\\,-1,\\,0\\}},}\\\\ {1,}&{\\mathrm{if~}\\mathrm{PRE\\_CARRY}-\\mathrm{NUM\\in\\{8,\\,9,\\,10\\}},}\\\\ {2,}&{\\mathrm{if~}\\mathrm{PRE\\_CARRY-NUM\\in\\{18,\\,19,\\,20\\}},}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "\u2022 NEXT_TOKEN $=$ PRE_PROD $^+$ CARRY (mod 10). ", "page_idx": 51}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/3d0be15028c5ebfe3f6bd4f7ce680f58a0ffec5e2d8de7f344193175b76e34d6.jpg", "table_caption": ["Table 31: Illustration of the construction idea. "], "table_footnote": [], "page_idx": 51}, {"type": "text", "text": "G.5 Transformer Block 1 \u2014 Causal Attention Layer ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "To implement the concept introduced in Appendix G.4, it is essential to design a Transformer block capable of generating an embedding matrix depicted in the left-most table of Table 31. The goal of the first Transformer block is to flil IS_OP2_ONE (6-th dimension) and IS_OP2_TEN (7-th dimension) by 1 if the token corresponds to the ones or tens digit of the second operand, respectively, and 0 otherwise. These two dimensions enable the filling of OP2_ONE (8-th dimension) and OP2_TEN (9-th dimension) at the second Transformer block. Furthermore, we will flil MASK (34-th dimension) in the first block, which will serve as a base for filling OP1_SHIFT0 to OP1_SHIFT4 in the second block. Thus, we currently have 3 objectives(IS_OP2_ONE, IS_OP2_TEN, MASK), each of which will be addressed by an individual head. ", "page_idx": 51}, {"type": "text", "text": "G.5.1 Attention Head 1: Detecting the Ones Digit of the Second Operand ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "The goal of the first head is to make the dimension IS_OP2_ONE as a one-hot row vector, where 1 is placed only at the token corresponding to the ones digit of the second operand. ", "page_idx": 52}, {"type": "text", "text": "Recall that $d=6P+34$ and let $d_{Q K,11}=P+1$ . Let $M>0$ be a sufficiently large positive real number. Let ", "page_idx": 52}, {"type": "equation", "text": "$$\n{Q_{1}^{(1)}}=\\left(\\begin{array}{l l l l l}{\\mathbf{0}_{P\\times(P+34)}}&{\\mathbf{0}_{P\\times P}}&{\\sqrt{M}I_{P}}&{\\mathbf{0}_{P\\times P}}&{\\mathbf{0}_{P\\times P}}&{\\mathbf{0}_{P\\times P}}\\\\ {\\sqrt{M P}\\left(e_{\\mathrm{FUL-ONES}}^{P+34}\\right)^{\\top}}&{\\mathbf{0}_{1\\times P}}&{\\mathbf{0}_{1\\times P}}&{\\mathbf{0}_{1\\times P}}&{\\mathbf{0}_{1\\times P}}&{\\mathbf{0}_{1\\times P}}\\end{array}\\right)\\in\\mathbb{R}^{d_{Q K,11}\\times d},\n$$", "text_format": "latex", "page_idx": 52}, {"type": "equation", "text": "$$\n\\mathbf{K}_{1}^{(1)}=\\left(\\begin{array}{c c c c c}{\\mathbf{0}_{P\\times(P+34)}}&{\\sqrt{M}I_{P}}&{\\mathbf{0}_{P\\times P}}&{\\mathbf{0}_{P\\times P}}&{\\mathbf{0}_{P\\times P}}&{\\mathbf{0}_{P\\times P}}\\\\ {\\sqrt{M P}\\left(e_{\\mathrm{IS-80S}}^{P+34}\\right)^{\\top}}&{\\mathbf{0}_{1\\times P}}&{\\mathbf{0}_{1\\times P}}&{\\mathbf{0}_{1\\times P}}&{\\mathbf{0}_{1\\times P}}&{\\mathbf{0}_{1\\times P}}\\end{array}\\right)\\in\\mathbb{R}^{d_{Q K,11}\\times d}.\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Unlike the construction for the addition task, we do not provide the table for the exact matrix and detailed error analysis due to their complex characterization. Instead, we provide an illustrative example for each step. We will also simply regard $M$ as a sufficiently large real scalar and thus the attention values can be clearly separated after going through the softmax operation. ", "page_idx": 52}, {"type": "text", "text": "The matrix ${Q}_{1}^{(1)}$ maps the embedding matrix $X^{(0)}$ into a query matrix $Q_{1}^{(1)}X^{(0)}\\,\\in\\,\\mathbb{R}^{(P+1)\\times N}$ , where the first $P$ rows are obtained by copying from the dimensions P\u221aOS_2 and scaling by $\\sqrt{M}$ , while the last row is the copy of the dimension FULL_ONES scaled by $\\sqrt{M P}$ . Similarly, the matrix $K_{1}^{(1)}$ maps the embedding matrix to a key matrix $\\bar{K_{1}^{(1)}}X^{(0)}\\in\\mathbb{R}^{(P\\dot{+}1)\\times N}$ . In this case, the first $P$ rows are obtained by copying from \u221athe dimensions $\\mathrm{POS}_{-}1$ and scaled by $\\sqrt{M}$ , with the last row being the dimension IS_BOS, scaled by $\\sqrt{M P}$ . For concrete examples, refer to Tables 32 and 33. ", "page_idx": 52}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/a849f7b518d745e6b07f197b532d399effc2204f421159f759bf746e9abc33fc.jpg", "table_caption": ["Table 32: Example of $Q_{1}^{(1)}X^{(0)}$ , continuing from Table 28. "], "table_footnote": [], "page_idx": 52}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/2180f31ce6d4fb464a331efdc32a46cf8676518aa2094ada4b39ba918c55f9a3.jpg", "table_caption": ["Table 33: Example of $K_{1}^{(1)}X^{(0)}$ , continuing from Table 28. "], "table_footnote": [], "page_idx": 52}, {"type": "text", "text": "By these, the attention score matrix $C_{1}^{(1)}\\;:=\\;({\\pmb K}_{1}^{(1)}{\\pmb X}^{(0)})^{\\top}{\\pmb Q}_{1}^{(1)}{\\pmb X}^{(0)}$ and the attention matrix $\\pmb{A}_{1}^{(1)}:=\\mathsf{s o f}\\tan\\mathsf{a x}(\\pmb{C}_{1}^{(1)})\\in\\mathbb{R}^{N\\times N}$ can be obtained. We provide the example of ${A}_{1}^{(1)}$ in Table 34. Specifically, an entry in ${A}_{1}^{(1)}$ is non-zero if and only if the inner product between the query and key vectors equals $M P$ . ", "page_idx": 52}, {"type": "text", "text": "Now let $d_{V,11}=1$ and define ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\!\\!\\!\\!\\!V_{1}^{(1)}=2(e_{\\scriptscriptstyle{\\mathrm{IS}_{\\mathrm{-}}\\mathrm{MUL}}}^{d}-e_{\\scriptscriptstyle{\\mathrm{IS}_{\\mathrm{-}}\\mathrm{EQUAL}}}^{d})^{\\top}\\in\\mathbb{R}^{d_{V,11}\\times d},}\\\\ &{\\!\\!\\!\\!\\!\\!U_{1}^{(1)}=e_{\\scriptscriptstyle{\\mathrm{IS}_{\\mathrm{-}}\\mathrm{OP}2_{-}\\mathrm{ONE}}}^{d}\\in\\mathbb{R}^{d\\times d_{V,11}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "The matrix $U_{1}^{(1)}V_{1}^{(1)}X^{(0)}$ takes the dimension IS_MUL and IS_EQUAL from the embedding matrix $X^{(0)}$ , subtracts one from the other, scales the result by 2, and puts it to the dimension IS_OP2_SUM. Consequently, the matrix $U_{1}^{(1)}V_{1}^{(1)}X^{(0)}A_{1}^{(1)}$ is a matrix that matches the size of the input embedding matrix $\\pmb{X}^{(0)}$ and is filled with zeroes, except for a unique 1 located at the ones place of the second operand in the input sequence, in the dimension IS_OP2_ONE (6-th). A concrete example is provided in Tables 35 and 36. ", "page_idx": 52}, {"type": "text", "text": "Table 34: Example of ${A}_{1}^{(1)}$ (with explicit row/column indices and sufficiently large $M$ ), continuing from Tables 32 and 33. ", "page_idx": 53}, {"type": "equation", "text": "${\\left[\\begin{array}{l l l l l l l l l l l l l l l l l l l}{{\\frac{\\ C O A}{1}}}&{2}&{1}&{2}&{3}&{4}&{5}&{6}&{7}&{8}&{9}&{10}&{11}&{12}&{13}&{14}&{15}\\\\ {2}&{1}&{1}&{1}&{1}&{1}&{1}&{1}&{12}&{17}&{12}&{1}&{13}&{1/4}&{1/4}&{1/3}&{1/3}&{1/3}\\\\ {2}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{10}&{13}&{0}\\\\ {3}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{1/4}&{0}&{0}&{0}\\\\ {4}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{1/4}&{0}&{0}&{0}\\\\ {5}&{0}&{0}&{0}&{0}&{0}&{0}&{1/2}&{0}&{0}&{0}&{1/4}&{0}&{0}&{0}&{0}&{0}\\\\ {7}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{1/2}&{0}&{0}&{0}&{0}&{1/4}&{0}&{0}&{0}&{0}\\\\ {8}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{1/4}&{0}&{0}&{0}&{0}\\\\ {9}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}\\\\ {10}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{1/3}&{0}&{0}&{0}&{$ ", "text_format": "latex", "page_idx": 53}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/fd42269e6615641f46ea3ea7787dfe9937fbb767d4f1fa7647969446a5b6ffa9.jpg", "table_caption": ["Table 35: Example of $U_{1}^{(1)}V_{1}^{(1)}X^{(0)}$ , continuing from Table 28. (Irrelevant dimensions are omitted for readability) "], "table_footnote": [], "page_idx": 53}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/1c310396cefb3c6bcf6a83a5cc1e0b5498fc0fbcb31cf41f73dd1dffa08ab2ce.jpg", "table_caption": ["Table 36: Example of $U_{1}^{(1)}V_{1}^{(1)}X^{(0)}A_{1}^{(1)}$ , continuing from Tables 34 and 35. (Irrelevant dimensions are omitted for readability) "], "table_footnote": [], "page_idx": 53}, {"type": "text", "text": "G.5.2 Attention Head 2: Detecting the Tens Digit of the Second Operand ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "In the previous head, we set the dimension IS_OP2_ONE (6-th dimension) to a one-hot row vector, where 1 is placed only in the token corresponding to the ones digit of the second operand. The objective of Attention head 2 is to fill the dimension IS_OP2_TEN (7-th dimension) similarly to IS_OP2_ONE, but with 1 placed only in the tens digit of the second operand. ", "page_idx": 53}, {"type": "text", "text": "The design of the query, key, and value weight is not significa\u221antly different from the previous head. Compared to the construction of attention head 1, we only push $\\sqrt{M}I_{P}$ to the next block for designing ", "page_idx": 53}, {"type": "text", "text": "${Q}_{2}^{(1)}$ . Specifically, ${Q}_{2}^{(1)}$ and $K_{2}^{(1)}$ are defined as ", "page_idx": 54}, {"type": "equation", "text": "$$\n{Q}_{2}^{(1)}=\\left(\\begin{array}{l l l l l}{\\mathbf{0}_{P\\times(P+34)}}&{\\mathbf{0}_{P\\times P}}&{\\mathbf{0}_{P\\times P}}&{\\sqrt{M}I_{P}}&{\\mathbf{0}_{P\\times P}}&{\\mathbf{0}_{P\\times P}}\\\\ {\\sqrt{M P}\\left(e_{\\mathrm{FUL-ONES}}^{P+34}\\right)^{\\top}}&{\\mathbf{0}_{1\\times P}}&{\\mathbf{0}_{1\\times P}}&{\\mathbf{0}_{1\\times P}}&{\\mathbf{0}_{1\\times P}}&{\\mathbf{0}_{1\\times P}}\\end{array}\\right)\\in\\mathbb{R}^{d_{Q K,12}\\times d},\n$$", "text_format": "latex", "page_idx": 54}, {"type": "equation", "text": "$$\n\\mathbf{K}_{2}^{(1)}=\\left(\\begin{array}{c c c c c}{\\mathbf{0}_{P\\times(P+34)}}&{\\sqrt{M}I_{P}}&{\\mathbf{0}_{P\\times P}}&{\\mathbf{0}_{P\\times P}}&{\\mathbf{0}_{P\\times P}}&{\\mathbf{0}_{P\\times P}}\\\\ {\\sqrt{M P}\\left(e_{\\mathrm{ls-80s}}^{P+34}\\right)^{\\top}}&{\\mathbf{0}_{1\\times P}}&{\\mathbf{0}_{1\\times P}}&{\\mathbf{0}_{1\\times P}}&{\\mathbf{0}_{1\\times P}}&{\\mathbf{0}_{1\\times P}}\\end{array}\\right)\\in\\mathbb{R}^{d_{Q K,12}\\times d},\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "where $d_{Q K,12}$ is set to $P+1$ . We refer to Tables 37 and 38 for specific examples. ", "page_idx": 54}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/86519dbe7bd16313882f16b687240438d810f8be537447a631c678ab81db74a0.jpg", "table_caption": ["Table 37: Example of $Q_{2}^{(1)}X^{(0)}$ , continuing from Table 28. "], "table_footnote": [], "page_idx": 54}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/d6f6163403a8cc75feff11b7ca81d7a7261fa98abbf54ac7824f2a4909763c9d.jpg", "table_caption": ["Table 38: Example of $K_{2}^{(1)}X^{(0)}$ , continuing from Table 28. "], "table_footnote": [], "page_idx": 54}, {"type": "text", "text": "By these, the attention score matrix $C_{2}^{(1)}\\;:=\\;(K_{2}^{(1)}X^{(0)})^{\\top}Q_{2}^{(1)}X^{(0)}$ and the attention matrix $\\pmb{A}_{2}^{(1)}:=\\mathsf{s o f}\\mathsf{t m a x}(\\pmb{C}_{2}^{(1)})\\in\\mathbb{R}^{N\\times N}$ can be obtained, and the example of ${A}_{2}^{(1)}$ is provided in Table 39. ", "page_idx": 54}, {"type": "text", "text": "Table 39: Example of ${A}_{2}^{(1)}$ (with explicit row/column indices and sufficiently large $M$ ), continuing from Tables 37 and 38. ", "page_idx": 54}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/f4d36ba9d5175dd17f369f5325b23c25bfb17e6f80ce41d3d654978e5bff68fa.jpg", "table_caption": [], "table_footnote": [], "page_idx": 54}, {"type": "text", "text": "Finally, we set $V_{2}^{(1)}$ and $U_{2}^{(1)}$ the same to that of the previous head. That is, with $d_{V,12}=1$ ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\!\\!\\!\\!\\!\\!V_{2}^{(1)}=2(e_{\\scriptscriptstyle{\\mathrm{IS}_{\\mathrm{-}}\\mathrm{MUL}}}^{d}-e_{\\scriptscriptstyle{\\mathrm{IS}_{\\mathrm{-}}\\mathrm{EQUAL}}}^{d})^{\\top}\\in\\mathbb{R}^{d_{V,12}\\times d},}\\\\ &{\\!\\!\\!\\!\\!\\!\\!U_{2}^{(1)}=e_{\\scriptscriptstyle{\\mathrm{IS}_{\\mathrm{-}}\\mathrm{OP}2_{-}\\mathrm{TEN}}}^{d}\\in\\mathbb{R}^{d\\times d_{V,12}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "and the example of $U_{2}^{(1)}V_{2}^{(1)}X^{(0)}$ and $U_{2}^{(1)}V_{2}^{(1)}X^{(0)}A_{2}^{(1)}$ is provided in Tables 40 and 41. Consequently, the matrix $U_{2}^{(1)}V_{2}^{(1)}X^{(0)}A_{2}^{(1)}$ is a matrix that matches the size of the input embedding ", "page_idx": 54}, {"type": "text", "text": "matrix and is fliled with zeroes, except for a unique 1 located at the tens place of the second operand in the input sequence, with the dimension IS_OP2_TEN (7-th dimension). ", "page_idx": 55}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/c38b373457cce159769c501fbb72c19cb50adb120dd0f5d89c26cd2703b6827a.jpg", "table_caption": ["Table 40: Example of $U_{2}^{(1)}V_{2}^{(1)}X^{(0)}$ , continuing from Table 28. (Irrelevant dimensions are omitted for readability) "], "table_footnote": [], "page_idx": 55}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/ee98edff09d443a0c76cf1336bac87dacc4db8321214bd2a09a873456929a855.jpg", "table_caption": ["Table 41: Example of $U_{2}^{(1)}V_{2}^{(1)}X^{(0)}A_{2}^{(1)}$ , continuing from Tables 39 and 40. (Irrelevant dimensions are omitted for readability) "], "table_footnote": [], "page_idx": 55}, {"type": "text", "text": "G.5.3 Attention Head 3: Position Masking ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "The goal of Attention head 3 is to generate a binary mask at the dimension MASK (34-th dimension), with $\\surd0\\ '$ placed before the multiplication symbol $(\\times)$ and $\"1\"$ placed starting from the multiplication symbol to the end. ", "page_idx": 55}, {"type": "text", "text": "To this end, we set $d_{Q K,13}=1$ and design query and key weights by ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{Q}_{3}^{(1)}=\\left(\\pmb{e}_{\\mathrm{FULL\\mathrm{-}0N E S}}^{d}\\right)^{\\top}\\in\\mathbb{R}^{d_{Q K,13}\\times d},}\\\\ &{\\pmb{K}_{3}^{(1)}=\\left(\\pmb{e}_{\\mathrm{IS\\mathrm{-}M U L}}^{d}\\right)^{\\top}\\in\\mathbb{R}^{d_{Q K,13}\\times d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "The matrices $Q_{3}^{(1)}X^{(0)}$ and $K_{3}^{(1)}X^{(0)}$ take the dimension FULL_ONES and IS_MUL, respectively, from the input embedding matrix. For concrete examples, please refer to Tables 42 and 43. ", "page_idx": 55}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/a69068ea46f5faa07614dad5cd6f08e82ff50847dde804c0f2c4a5adc052927f.jpg", "table_caption": ["Table 42: Example of $Q_{3}^{(1)}X^{(0)}$ , continuing from Table 28. "], "table_footnote": [], "page_idx": 55}, {"type": "table", "img_path": "", "table_caption": ["Table 43: Example of $K_{3}^{(1)}X^{(0)}$ , continuing from Table 28. "], "table_footnote": [], "page_idx": 55}, {"type": "equation", "text": "$$\n\\frac{\\textit{Z}\\mid\\mathbb{S}\\quad7\\quad5\\quad9\\quad5\\quad\\times\\quad7\\quad9\\quad=\\quad5\\quad0\\quad0\\quad0\\quad0\\quad6\\quad}{}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "By these, the attention score matrix $C_{3}^{(1)}\\;:=\\;(K_{3}^{(1)}X^{(0)})^{\\top}Q_{3}^{(1)}X^{(0)}$ and the attention matrix $\\pmb{A}_{3}^{(1)}:=\\mathsf{s o f}\\mathsf{t m a x}(\\pmb{C}_{3}^{(1)})\\in\\mathbb{R}^{N\\times N}$ can be obtained and the example of ${A}_{3}^{(1)}$ is provided in Table 44. ", "page_idx": 55}, {"type": "text", "text": "Table 44: Example of ${A}_{3}^{(1)}$ (with explicit row/column indices), continuing from Tables 42 and 43. ", "page_idx": 56}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/42fc9080f6787aa7dfcb991be63ca0a8dcff377286a17fce19267d77630b255b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 56}, {"type": "text", "text": "Finally, we set $V_{3}^{(1)}$ and $U_{3}^{(1)}$ by $d_{V,13}=1$ and ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{3}^{(1)}=(e_{\\scriptscriptstyle{\\mathrm{IS}_{-}\\mathrm{MUL}}}^{d})^{\\top}\\in\\mathbb{R}^{d_{V,13}\\times d},}\\\\ &{U_{3}^{(1)}=e_{\\scriptscriptstyle{\\mathrm{MASK}}}^{d}\\in\\mathbb{R}^{d\\times d_{V,13}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "qTuhee netlxya, mthpel e moaft $U_{3}^{(1)}V_{3}^{(1)}X^{(0)}$ $U_{3}^{(1)}V_{3}^{(1)}X^{(0)}A_{3}^{(1)}$ misa tpcrhoevsi dtehde  isni zTe aobfl etsh e4 i5n apnudt  e46m.b eCdodnisneg$U_{3}^{(1)}V_{3}^{(1)}X^{(0)}A_{3}^{(1)}$   \nmatrix and is fliled with 1 only at the dimension MASK (34-th dimension) starting from the token   \nto the end of sequence, and 0 otherwise. ", "page_idx": 56}, {"type": "text", "text": "At this point, the objective of attention head 3 may seem somewhat unclear. We note that the output of Attention head 3 will be utilized to flil the dimensions POS_2_MASK in the subsequent FFN layer, and this POS_2_MASK plays a crucial role in designing the key matrices in the Attention heads 3 to 7 at the second Transformer block. ", "page_idx": 56}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/9cf803b505ea5b72210751a0d1caba28aed445cfbeb559eebe354ccb647f48d9.jpg", "table_caption": ["Table 45: Example of $U_{3}^{(1)}V_{3}^{(1)}X^{(0)}$ , continuing from Table 28. (Irrelevant dimensions are omitted for readability) "], "table_footnote": ["34: MASK "], "page_idx": 56}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/8d2bd1abea358ab069552da1a2345c556d268b397334b32de313b80b6c0a6cd0.jpg", "table_caption": ["Table 46: Example of $U_{3}^{(1)}V_{3}^{(1)}X^{(0)}A_{3}^{(1)}$ , continuing from Tables 44 and 45. (Irrelevant dimensions are omitted for readability) "], "table_footnote": [], "page_idx": 56}, {"type": "text", "text": "G.5.4 Residual Connection ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "So far we have computed the output of $\\tt A t t_{1}$ operation. Passing through the residual connection, the output of the attention layer becomes the sum of the original input embedding matrix and the output of $\\tt A t t_{1}$ operation: ", "page_idx": 57}, {"type": "equation", "text": "$$\n{\\pmb Y}^{(1)}={\\pmb X}^{(0)}+\\sum_{h\\in\\{1,2,3\\}}{\\pmb U}_{h}^{(1)}{\\pmb V}_{h}^{(1)}{\\pmb X}^{(0)}{\\pmb A}_{h}^{(1)}.\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "An example of the output of residual connection is presented in Table 47. ", "page_idx": 57}, {"type": "text", "text": "Table 47: Example output of residual connection, continuing from Tables 28, 36, 41 and 46. Uncolored rows represent the initial embedding. Yellow rows indicate the rows filled by the attention heads in the first Transformer block. A pink row indicates the row that will be filled by the subsequent FFN layer. ", "page_idx": 57}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/56a215ebb69905dc1d87725cf9bb4d191421112c8617ca7ac423c9ee8b15562d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 57}, {"type": "text", "text": "G.6 Transformer Block 1 \u2014 Token-wise Feed-forward Layer ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "The goal of the feed-forward layer involves filling the dimensions POS_2_MASK. Specifically, for each token \u03c3i, if the dimension MASK is 1 (i.e., Y ((M1A)S K)i = 1), we want to fill the dimensions POS_2_MASK by copying the the corresponding token\u2019s POS_2; otherwise, we want to fill with ${\\bf0}_{P}$ . Be careful that the feed-forward operation is restricted to a token-wise mapping, meaning it only takes inputs from entries within the same column of the encoding matrix. ", "page_idx": 57}, {"type": "text", "text": "Construction for POS_2_MASK. Given a vector $\\pmb{y}\\,=\\,[\\pmb{y}_{j}]_{j=1}^{d}\\,\\in\\,\\mathbb{R}^{d}$ , define functions $g_{l},h_{l}:$ $\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ for every $j\\in[P]$ as ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g_{l}(\\pmb{y}):=\\pmb{y}_{\\mathrm{POS}_{-}2,l}-2\\pmb{y}_{\\mathrm{MASK}}}\\\\ &{h_{l}(\\pmb{y}):=-\\pmb{y}_{\\mathrm{POS}_{-}2,l}-2\\pmb{y}_{\\mathrm{MASK}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "where $y_{\\mathrm{POS}_{-}2,l}\\in\\mathbb{R}$ is the $l$ -th dimension of $\\pmb{\\mathscr{y}}_{\\mathrm{pos\\_2}}\\in\\mathbb{R}^{P}\\,(l\\in{1,2,...,P}).$ . ", "page_idx": 57}, {"type": "text", "text": "Consider a simple one-hidden-layer ReLU networks $f_{l}:\\ensuremath{\\mathbb{R}^{d}}\\to\\ensuremath{\\mathbb{R}}$ defined as ", "page_idx": 57}, {"type": "equation", "text": "$$\nf_{l}({\\pmb y})=\\phi(g_{l}({\\pmb y}))-\\phi(h_{l}({\\pmb y})).\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Using the fact that $\\scriptstyle y_{\\mathrm{POS}_{-}2,l}$ is either $-1$ or 1, we can easily check that $f_{l}({\\pmb y})=y_{\\mathrm{{POS}}_{-}2,l}$ if $\\mathbf{\\mathcal{y}}_{\\mathrm{MASK}}$ is $0$ , and $f_{l}(\\pmb{y})=0$ if $\\mathbf{\\mathcal{y}}_{\\mathrm{MASK}}$ is 1. ", "page_idx": 57}, {"type": "text", "text": "Now, we can construct the width- $2P$ feed-forward network that outputs the desired value at the dimension POS_2_MASK by ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\left[\\operatorname{FF}_{1}\\left(\\mathbf{Y}^{(1)}\\right)\\right]_{\\left(\\operatorname{pos}_{=2,\\,\\dots}\\!\\operatorname{MaSK}\\right)i}=\\left[f_{1}\\left(\\mathbf{Y}_{\\bullet i}^{(1)}\\right)\\quad\\cdots\\quad f_{P}\\left(\\mathbf{Y}_{\\bullet i}^{(1)}\\right)\\right]^{\\top}\\in\\mathbb{R}^{P\\times1},\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "and 0 for any other dimensions. The example output for this layer is presented in Table 48. ", "page_idx": 58}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/e45aef24804fd1dd6b6a9ff4ddf804e5f91c80d933e8f932fc6ecd5c21b5d014.jpg", "table_caption": ["Table 48: Example output of FFN layer at the first Transformer block, continuing from Table 47. "], "table_footnote": [], "page_idx": 58}, {"type": "text", "text": "G.6.1 Residual Connection ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "The last task of the feed-forward layer is to pass $\\mathbf{F}\\mathbf{F}_{1}\\left(\\mathbf{Y}^{(1)}\\right)$ through the residual connection. As a result, we have ", "page_idx": 58}, {"type": "equation", "text": "$$\nX^{(1)}=Y^{(1)}+\\operatorname{FF}_{1}\\left(Y^{(1)}\\right).\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "This is the end of the first Transformer block, and a concrete example of $X^{(1)}$ is illustrated in Table 49. ", "page_idx": 58}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/0c9c17b0c698afd209267bdc6f0b05baf7ffa7ecf70293f33e5882a46cba8965.jpg", "table_caption": ["Table 49: Example embedding matrix after the first Transformer block. The yellow rows represent the results introduced during the first block, while the gray rows will be filled in the second block. "], "table_footnote": [], "page_idx": 58}, {"type": "text", "text": "G.7 Transformer Block 2 \u2014 Causal Attention Layer ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Consider a scenario where the model is at the step of predicting the $i$ -th least significant digit of the multiplication result. There are two goals for the causal attention layer at the second Transformer block. The first goal is to generate the embedding matrix as the left-most figure in Table 31, that is, flil OP2_ONE, OP2_TEN, and OP1_SHIFT0 to OP1_SHIFT4 with the ones digit of the second operand, the tens digit of the second operand, and the $i$ , $(i-1)$ , $(i-2)$ , $(i-3)$ , $(i-4)$ -th least significant digit of the first operand, respectively. Our construction assigns each head to each dimension. The second goal is to fill PRE_EOS1 and PRE_EOS2 with appropriate values. These 2 dimensions will be utilized in the subsequent FFN layer to predict whether we should predict the next token as EOS or not. Also, we note that filling these 2 dimensions can be implemented within the same head for OP1_SHIFT0 and OP1_SHIFT2 respectively, thus requiring a total of seven heads. ", "page_idx": 58}, {"type": "text", "text": "", "page_idx": 59}, {"type": "text", "text": "G.7.1 Attention Head 1: Copying the Ones Digit of the Second Operand ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "The objective of Attention head 1 is to flil the dimension OP2_ONE with the ones digit of the second operand. To do so, we design the weights by defining $d_{Q K,21}=1$ and ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{Q}_{1}^{(2)}=\\left(e_{\\mathrm{FULL\\mathrm{-}0N E S}}^{d}\\right)^{\\top}\\in\\mathbb{R}^{d_{Q K,21}\\times d},}\\\\ {\\pmb{K}_{1}^{(2)}=\\left(e_{\\mathrm{IS\\mathrm{-}0P2\\mathrm{-}0N E}}^{d}\\right)^{\\top}\\in\\mathbb{R}^{d_{Q K,21}\\times d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "We also define $d_{V,21}=1$ and ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r}{V_{1}^{(2)}=(e_{\\mathrm{{NUM}}}^{d})^{\\top}\\in\\mathbb{R}^{d_{V,21}\\times d},}\\\\ {U_{1}^{(2)}=e_{\\mathrm{{op}}2_{\\mathrm{-0NE}}}^{d}\\in\\mathbb{R}^{d\\times d_{V,21}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "A concrete example of $Q_{1}^{(2)}X^{(1)}$ , $K_{1}^{(2)}X^{(1)}$ , $A_{1}^{2}$ , $U_{1}^{(2)}V_{1}^{(2)}X^{(1)}$ , and $U_{1}^{(2)}V_{1}^{(2)}X^{(1)}A_{1}^{(2)}$ is provided in Tables 50 to 54. One might be concerned that in Table 54, the dimension OP2_ONE is not completely filled with $\\cdot_{9},$ , but only the latter part. However, we note that given our focus on next-token prediction, it suffices to accurately fill values starting from the $=$ token, and filling the preceding tokens with placeholder values does not cause any issues. ", "page_idx": 59}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/54906efa546680b8dab1b79ce6a1f03821fd567576940ca7bd201c8febca4659.jpg", "table_caption": ["Table 50: Example of $Q_{1}^{(2)}X^{(1)}$ , continuing from Table 49. "], "table_footnote": [], "page_idx": 59}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/6e50af8629df129bf95f8eb74a01be34aac3c55f8b3ed65e9e5081c7344db5af.jpg", "table_caption": ["Table 51: Example of $K_{1}^{(2)}X^{(1)}$ , continuing from Table 49. "], "table_footnote": [], "page_idx": 59}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/bad782d69933302da8229136254901e4fde462e2be211cb3e8ea83d475e3772f.jpg", "table_caption": ["Table 53: Example of $U_{1}^{(2)}V_{1}^{(2)}X^{(1)}$ , continuing from Table 49. (Irrelevant dimensions are omitted for readability) "], "table_footnote": [], "page_idx": 59}, {"type": "text", "text": "Table 52: Example of ${A}_{1}^{(2)}$ (with explicit row/column indices), continuing from Tables 50 and 51. row \\ col j = 1 2 13 4 5 6 7 8 9 10 11 12 13 14 15 ", "page_idx": 60}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/737ddaecef2141a44fe78be7d81c649dea29d6d55daef6c94575ae314bea9884.jpg", "table_caption": [], "table_footnote": [], "page_idx": 60}, {"type": "text", "text": "Table 54: Example of $U_{1}^{(2)}V_{1}^{(2)}X^{(1)}A_{1}^{(2)}$ , continuing from Tables 52 and 53. (Irrelevant dimensions are omitted for readability) ", "page_idx": 60}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/d3f49da969c07c91aa92af1b579b823da3696214a1597aafb445cc5e559dd211.jpg", "table_caption": [], "table_footnote": [], "page_idx": 60}, {"type": "text", "text": "G.7.2 Attention Head 2: Copying the Tens Digit of the Second Operand ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "The objective of Attention head 2 is to flil the dimension OP2_TEN with the tens digit of the second operand. We take a similar approach to Attention head 1, but the main difference is that we utilize the dimension IS_OP2_TEN instead of IS_OP2_ONE for generating the key weight. We design the weights by defining $d_{Q K,22}=1$ and ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{Q}_{2}^{(2)}=\\left(e_{\\mathrm{FULL\\mathrm{-}0N E S}}^{d}\\right)^{\\top}\\in\\mathbb{R}^{d_{Q K,22}\\times d},}\\\\ {\\pmb{K}_{2}^{(2)}=\\left(e_{\\mathrm{IS\\mathrm{-}0P2\\mathrm{-}T E N}}^{d}\\right)^{\\top}\\in\\mathbb{R}^{d_{Q K,22}\\times d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "We also define $d_{V,22}=1$ and ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\begin{array}{r}{V_{2}^{(2)}=(e_{\\mathrm{{NUM}}}^{d})^{\\top}\\in\\mathbb{R}^{d_{V,22}\\times d},}\\\\ {U_{2}^{(2)}=e_{\\mathrm{{op}}2_{\\mathrm{-TEN}}}^{d}\\in\\mathbb{R}^{d\\times d_{V,22}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "A concrete example of $Q_{2}^{(2)}X^{(1)}$ , $K_{2}^{(2)}X^{(1)}$ , ${A}_{2}^{2}$ , $U_{2}^{(2)}V_{2}^{(2)}X^{(1)}$ , and $U_{2}^{(2)}V_{2}^{(2)}X^{(1)}A_{2}^{(2)}$ is provided in Tables 55 to 59. Once again, the dimension OP2_TEN is not entirely filled with in Table 59. As mentioned in the previous head, this does not cause any issues because the front part (before $=$ ) does not affect the final prediction unless additional attention blocks are introduced after the second Transformer block. ", "page_idx": 60}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/c82b28a6ef947eee316bd20237457bfe307327645253eb4bfc7a8f73533dc460.jpg", "table_caption": ["Table 55: Example of $Q_{2}^{(2)}X^{(1)}$ , continuing from Table 49. "], "table_footnote": [], "page_idx": 60}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/c39771f1711c69ed9a523281a4099b2d39b6404320e2f8ffecc0519058a1f5d8.jpg", "table_caption": ["Table 56: Example of $\\underline{{\\boldsymbol{K}}}_{2}^{(2)}\\boldsymbol{X}^{(1)}$ , continuing from Table 49. "], "table_footnote": [], "page_idx": 61}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/ad9a8391658faa45fe94cd7fc1dcb5a5e31b8837f9613b350285fe1c3c73b706.jpg", "table_caption": ["Table 57: Example of ${A}_{2}^{(2)}$ (with explicit row/column indices), continuing from Tables 55 and 56. "], "table_footnote": [], "page_idx": 61}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/d0ef4b6d1a2f910e5bd96885a31dcd1db5f9d441ff9d35a61ba33f3cea4318fc.jpg", "table_caption": ["Table 58: Example of $U_{2}^{(2)}V_{2}^{(2)}X^{(1)}$ , continuing from Table 49. (Irrelevant dimensions are omitted for readability) "], "table_footnote": [], "page_idx": 61}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/f354cb77599ec2530153bbeed0bcaece19d73f65b95b453c73c8fed9a387fb70.jpg", "table_caption": ["aTraeb loe m5i9tt: eEd xfaorm rpelea doaf $U_{2}^{(2)}V_{2}^{(2)}X^{(1)}A_{2}^{(2)}$ , continuing from Tables 57 and 58. (Irrelevant dimensions "], "table_footnote": [], "page_idx": 61}, {"type": "text", "text": "G.7.3 Attention Head 3: Copying the Appropriate Digit from the First Operand I ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "The objectives of the first and the second Attention heads were to extract the ones and tens digits of the second operand and display them in the dimensions OP2_ONE and OP2_TEN, respectively. For Attention head 3 to 7, we mainly focus on the first operand. Specifically, in Attention head 3, the goal is to flil the dimension OP1_SHIFT0 at the $i$ -th least significant digit of the response (when predicting the $(i+1)$ -th least significant digit of the response) with the $(i+1)$ -th least significant digit of the first operand. For our example, we want to flil OP1_SHIFT0 of the token $=$ by 5. Here, $i$ ranges from 0 to $\\ell_{a}+2$ , where the 0-th least significant digit of the response denotes the equal token. In cases where $i\\geq\\ell_{a}$ , we fill by 0. ", "page_idx": 61}, {"type": "text", "text": "", "page_idx": 62}, {"type": "text", "text": "Additionally, the third head has an extra objective: fliling the dimension PRE_EOS1. This dimension is utilized for EOS prediction in the subsequent FFN layer along with PRE_EOS2, which is filled by the fifth head of the same layer. We observed that both objectives can be achieved by utilizing the same attention map. Thus, instead of implementing these objectives in separate heads, we can achieve them by utilizing the matrices $V_{3}^{(2)}$ and $U_{3}^{(2)}$ described below. Unlike previous heads, $V_{3}^{(2)}$ and $U_{3}^{(2)}$ each have two elements, with each element contributing to one of the objectives. ", "page_idx": 62}, {"type": "text", "text": "Our specific construction is as follows. With $d_{Q K,23}=P+1$ , ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\mathbf{Q}_{3}^{(2)}=\\left(\\frac{\\mathbf{0}_{P\\times34}}{\\sqrt{M P}\\left(e_{\\mathrm{{FULL-ONE}}}^{34}\\right)^{\\top}}\\right.\\quad\\mathbf{0}_{P\\times P}\\quad\\sqrt{M}I_{P}\\quad\\mathbf{0}_{P\\times P}\\quad\\mathbf{0}_{P\\times P}\\quad\\mathbf{0}_{P\\times P}\\quad\\mathbf{0}_{P\\times P}\\right)\\in\\mathbb{R}^{d_{Q K,23}\\times d_{P\\times P}}.\n$$", "text_format": "latex", "page_idx": 62}, {"type": "equation", "text": "$$\n\\mathbf{{K}}_{3}^{(2)}=\\left(\\begin{array}{l l l l l l}{\\mathbf{0}_{P\\times34}}&{\\sqrt{M}I_{P}}&{\\mathbf{0}_{P\\times P}}&{\\mathbf{0}_{P\\times P}}&{\\mathbf{0}_{P\\times P}}&{\\mathbf{0}_{P\\times P}}&{\\mathbf{0}_{P\\times P}}\\\\ {\\sqrt{M P}\\left(e_{1\\times-80\\times}^{34}\\right)^{\\top}}&{\\mathbf{0}_{1\\times P}}&{\\mathbf{0}_{1\\times P}}&{\\mathbf{0}_{1\\times P}}&{\\mathbf{0}_{1\\times P}}&{\\mathbf{0}_{1\\times P}}&{\\mathbf{0}_{1\\times P}}\\end{array}\\right)\\in\\mathbb{R}^{d_{Q K,23}\\times d},\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "and with $d_{V,23}=2$ , ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{3}^{(2)}=\\bigg(\\frac{2\\left(e_{\\scriptscriptstyle{\\mathrm{NUM}}}^{d}\\right)^{\\top}}{\\left(e_{\\scriptscriptstyle{\\mathrm{IS}_{-}\\mathrm{BOS}}}^{d}\\right)^{\\top}}\\bigg)\\in\\mathbb{R}^{d_{V,23}\\times d},}\\\\ &{U_{3}^{(2)}=\\left(e_{\\scriptscriptstyle{\\mathrm{OP1}_{-}\\mathrm{SHIFT0}}}^{d}\\quad e_{\\scriptscriptstyle{\\mathrm{PRE}_{-}\\mathrm{EOS1}}}^{d}\\right)\\in\\mathbb{R}^{d\\times d_{V,23}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "We provide the examples in Tables 60 to 64. We note that within the dimension PRE_EOS1 of the matrix $U_{3}^{(2)}V_{3}^{(2)}X^{(1)}A_{3}^{(2)}$ , if we restrict our view to the equal symbol $=$ and the response sequence, 1 is only assigned to the first, second, and third most significant digits of the response (regardless of the query length). ", "page_idx": 62}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/2861c5eb7a31ddfe3b8d32892487a4ff26df992a3f33c87d12d36d46f8cec365.jpg", "table_caption": ["Table 60: Example of $Q_{3}^{(2)}X^{(1)}$ , continuing from Table 49. "], "table_footnote": [], "page_idx": 62}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/8889e977e02ad63b53edb9362e9e61638ac5d9de19ae095952cf880e235c10f7.jpg", "table_caption": ["Table 61: Example of $\\underline{{\\boldsymbol{K}}}_{3}^{(2)}\\boldsymbol{X}^{(1)}$ , continuing from Table 49. "], "table_footnote": [], "page_idx": 62}, {"type": "text", "text": "Table 62: Example of ${A}_{3}^{(2)}$ (with explicit row/column indices and sufficiently large $M$ ), continuing from Tables 60 and 61. ", "page_idx": 63}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/033d0fd8941260e8da6455dfcbc0d39488348069b51494963ea4abf7758ebe7a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 63}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/00616cd16066a03cc5a89d4c911aabf26c14db8f3fab41094c85ee0513dae638.jpg", "table_caption": ["Table 63: Example of $U_{3}^{(2)}V_{3}^{(2)}X^{(1)}$ , continuing from Table 49. (Irrelevant dimensions are omitted for readability) "], "table_footnote": [], "page_idx": 63}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/1224a12f4b1b1cbd9c7db90efbbc5072428c8110eede3d4c609260e38aa84f36.jpg", "table_caption": ["Table 64: Example of $U_{3}^{(2)}V_{3}^{(2)}X^{(1)}A_{3}^{(2)}$ , continuing from Tables 62 and 63. (Irrelevant dimensions are omitted for readability) "], "table_footnote": [], "page_idx": 63}, {"type": "text", "text": "G.7.4 Attention Head 4: Copying the Appropriate Digit from the First Operand II ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "The objective of Attention head 4 is to fill the dimension OP1_SHIFT1 at the $i^{\\th}$ -th least significant digit of the response (when predicting the $(i+1)$ -th least significant digit of the response) with the $i$ -th least significant digit of the first operand. Similarly to the previous head, $i$ ranges from 0 to $\\ell_{a}+2$ . In cases where the $i$ -th least significant digit of the first operand is not well-defined (i.e., $i\\in\\{0,\\ell_{a}+1,\\ell_{a}+2\\})$ , we assign 0. ", "page_idx": 63}, {"type": "text", "text": "The design of Attention head 4 is as follows. With $d_{Q K,24}=P+1$ , ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Q_{4}^{(2)}=\\left(\\frac{\\mathbf{0}_{P\\times34}}{\\sqrt{M P}\\left(e_{\\mathrm{{FULL-ONE}}}^{34}\\right)^{\\top}}\\right.\\quad\\mathbf{0}_{P\\times P}\\quad\\mathbf{0}_{P\\times P}\\quad\\sqrt{M}I_{P}\\quad\\mathbf{0}_{P\\times P}\\quad\\mathbf{0}_{P\\times P}\\quad\\mathbf{0}_{P\\times P}\\right)\\in\\mathbb{R}^{d_{Q K,24}\\times d_{P}},}\\\\ {\\left.\\sqrt{M P}\\left(e_{\\mathrm{FUL-ONE}}^{34}\\right)^{\\top}\\quad\\mathbf{0}_{1\\times P}\\quad\\mathbf{0}_{1\\times P}\\quad\\mathbf{0}_{1\\times P}\\quad\\mathbf{0}_{1\\times P}\\quad\\mathbf{0}_{1\\times P}\\quad\\mathbf{0}_{1\\times P}\\quad\\mathbf{0}_{1\\times P}\\right)\\in\\mathbb{R}^{d_{Q K,24}\\times d_{P}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 64}, {"type": "equation", "text": "$$\n{\\bf K}_{4}^{(2)}=\\left(\\!\\!\\begin{array}{l l l l l l}{\\mathbf{0}_{P\\times34}}&{\\sqrt{M}I_{P}}&{\\mathbf{0}_{P\\times P}}&{\\mathbf{0}_{P\\times P}}&{\\mathbf{0}_{P\\times P}}&{\\mathbf{0}_{P\\times P}}&{\\mathbf{0}_{P\\times P}}\\\\ {\\sqrt{M P}\\left(e_{1\\times-80\\times}^{34}\\right)^{\\top}}&{\\mathbf{0}_{1\\times P}}&{\\mathbf{0}_{1\\times P}}&{\\mathbf{0}_{1\\times P}}&{\\mathbf{0}_{1\\times P}}&{\\mathbf{0}_{1\\times P}}&{\\mathbf{0}_{1\\times P}}\\end{array}\\!\\!\\right)\\in\\mathbb{R}^{d_{Q\\times,24}\\times d_{P\\times P}},\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "and with $d_{V,24}=1$ , ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{4}^{(2)}=2(e_{\\mathrm{NUM}}^{d})^{\\top}\\in\\mathbb{R}^{d_{V,24}\\times d},}\\\\ &{U_{4}^{(2)}=e_{\\mathrm{op1\\l_{-}s H I F T1}}^{d}\\in\\mathbb{R}^{d\\times d_{V,24}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "We provide the examples in Tables 65 to 69. ", "page_idx": 64}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/e923ac5c92550841acc6bff1dfbf71a2f899e3331436d371aa3fdc40bb0cec13.jpg", "table_caption": ["Table 65: Example of ${\\cal Q}_{4}^{(2)}X^{(1)}$ , continuing from Table 49. "], "table_footnote": [], "page_idx": 64}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/e57a0febfbe1a351945b2c639d87d910aa8ad1ea2022b4658e2fd7f265b1b02d.jpg", "table_caption": ["Table 66: Example of $K_{4}^{(2)}X^{(1)}$ , continuing from Table 49. "], "table_footnote": [], "page_idx": 64}, {"type": "text", "text": "Table 67: Example of ${A}_{4}^{(2)}$ (with explicit row/column indices and sufficiently large $M$ ), continuing from Tables 65 and 66. ", "page_idx": 64}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/61e0f9bd97c00e4d8d4456b55f71f60fcd5ec7abcf46dd8dee9de6e30545bd36.jpg", "table_caption": [], "table_footnote": [], "page_idx": 64}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/7b50258aa40e344892106040a95bfe6791ecf6a77890840eedac879ea67f8094.jpg", "table_caption": ["Table 68: Example of $U_{4}^{(2)}V_{4}^{(2)}X^{(1)}$ , continuing from Table 49. (Irrelevant dimensions are omitted for readability) "], "table_footnote": [], "page_idx": 65}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/1ed748693431b640a17bca0a8d4c62fa496954b0c710f78887274951293a2be3.jpg", "table_caption": ["Table 69: Example of $U_{4}^{(2)}V_{4}^{(2)}X^{(1)}A_{4}^{(2)}$ , continuing from Tables 67 and 68. (Irrelevant dimensions are omitted for readability) "], "table_footnote": [], "page_idx": 65}, {"type": "text", "text": "G.7.5 Attention Head 5: Copying the Appropriate Digit from the First Operand III ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "The main objective of Attention head 5 is to flil the dimension OP1_SHIFT2 at the $i$ -th least significant digit of the response (when predicting the $(i+1)$ -th least significant digit of the response) with the $(i-1)$ -th least significant digit of the first operand. Similarly to the previous head, $i$ ranges from 0 to $\\ell_{a}+2$ , and in cases where the $i$ -th least significant digit of the first operand is not well-defined (i.e., $i\\in\\{0,1,\\ell_{a}+2\\})$ ), we assign 0. ", "page_idx": 65}, {"type": "text", "text": "As mentioned in Attention head 3, we assign an extra goal to Attention head 5, which is to fill the dimension PRE_EOS2. ", "page_idx": 65}, {"type": "text", "text": "The design of the fifth head is as follows. With $d_{Q K,25}=P+1$ , ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\mathbf{Q}_{5}^{(2)}=\\left(\\begin{array}{l l l l l l}{\\mathbf{0}_{P\\times34}}&{\\mathbf{0}_{P\\times P}}&{\\mathbf{0}_{P\\times P}}&{\\mathbf{0}_{P\\times P}}&{\\sqrt{M}I_{P}}&{\\mathbf{0}_{P\\times P}}&{\\mathbf{0}_{P\\times P}}\\\\ {\\sqrt{M P}\\left(e_{\\mathrm{FUL-OWE}}^{34}\\right)^{\\top}}&{\\mathbf{0}_{1\\times P}}&{\\mathbf{0}_{1\\times P}}&{\\mathbf{0}_{1\\times P}}&{\\mathbf{0}_{1\\times P}}&{\\mathbf{0}_{1\\times P}}&{\\mathbf{0}_{1\\times P}}\\end{array}\\right)\\in\\mathbb{R}^{d_{Q K,2\\times M}},\n$$", "text_format": "latex", "page_idx": 65}, {"type": "equation", "text": "$$\n{\\bf K}_{5}^{(2)}=\\left(\\!\\!\\begin{array}{l l l l l l}{\\mathbf{0}_{P\\times34}}&{\\sqrt{M}I_{P}}&{\\mathbf{0}_{P\\times P}}&{\\mathbf{0}_{P\\times P}}&{\\mathbf{0}_{P\\times P}}&{\\mathbf{0}_{P\\times P}}&{\\mathbf{0}_{P\\times P}}\\\\ {\\sqrt{M P}\\left(e_{1\\times-80\\times}^{34}\\right)^{\\top}}&{\\mathbf{0}_{1\\times P}}&{\\mathbf{0}_{1\\times P}}&{\\mathbf{0}_{1\\times P}}&{\\mathbf{0}_{1\\times P}}&{\\mathbf{0}_{1\\times P}}&{\\mathbf{0}_{1\\times P}}\\end{array}\\!\\!\\right)\\in\\mathbb{R}^{d_{Q K,2\\times}\\times d};\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "and with $d_{V,25}=2$ , ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{V}_{5}^{(2)}=\\left(\\vphantom{\\sum_{\\substack{\\ell_{\\mathrm{D}_{-}}\\leq0}}}2(\\pmb{e}_{\\mathrm{N}_{-}\\mathrm{B}\\mathrm{U}\\mathrm{V}}^{d})^{\\top}\\right)\\in\\mathbb{R}^{d_{V,25}\\times d},}\\\\ &{\\pmb{U}_{5}^{(2)}=\\left(\\pmb{e}_{\\mathrm{OP}1_{-}\\mathrm{SHIFT2}}^{d}\\right)\\in\\pmb{e}_{\\mathrm{PRE}_{-}\\mathrm{EOS}2}^{d}\\right)\\in\\mathbb{R}^{d\\times d_{V,25}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "We provide the examples in Tables 70 to 74. Note that within the dimension PRE_EOS2 of the matrix $U_{5}^{(2)}V_{5}^{(2)}X^{(1)}A_{5}^{(2)}$ , if we restrict our view to the equal symbol $=$ and the response sequence, 1 is only assigned to the most and the least significant digit of the response, and the equal token. An important observation is that upon comparing PRE_EOS1 and PRE_EOS2, the most significant digit of the response is the only token that has a value of 1 in both dimensions. This observation plays a crucial role in predicting EOS for the next token, and we will elaborate further in the later section discussing the FFN layer. ", "page_idx": 65}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/4c6a8490630251a387817f3b6ca23562f191959b0f27e27ac424d99650132a07.jpg", "table_caption": [], "table_footnote": [], "page_idx": 66}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/2ed3beddc55661f6eea249e88bdcbcd04bb40b6cdd90802405307678eff68504.jpg", "table_caption": ["Table 71: Example of $K_{5}^{(2)}X^{(1)}$ , continuing from Table 49. "], "table_footnote": [], "page_idx": 66}, {"type": "text", "text": "Table 72: Example of ${A}_{5}^{(2)}$ (with explicit row/column indices and sufficiently large $M$ ), continuing from Tables 70 and 71. ", "page_idx": 66}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/17b7c1a23769c6a6e3a131c06065d8eaca77b3a41fe00b5a1e1a5d1590c437cb.jpg", "table_caption": [], "table_footnote": [], "page_idx": 66}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/a94ecefd56d73320b87ffc7f04762f5c751cf03d4792030d48201dbf19af229e.jpg", "table_caption": ["Table 73: Example of $U_{5}^{(2)}V_{5}^{(2)}X^{(1)}$ , continuing from Table 49. (Irrelevant dimensions are omitted for readability) "], "table_footnote": [], "page_idx": 66}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/c7946e2fbcdce8f7d64a5c99c7dedcb3aca99164f17fe201396b6c689c218d77.jpg", "table_caption": ["Table 74: Example of $U_{5}^{(2)}V_{5}^{(2)}X^{(1)}A_{5}^{(2)}$ , continuing from Tables 72 and 73. (Irrelevant dimensions are omitted for readability) "], "table_footnote": [], "page_idx": 67}, {"type": "text", "text": "G.7.6 Attention Head 6: Copying the Appropriate Digit from the First Operand IV ", "text_level": 1, "page_idx": 67}, {"type": "text", "text": "The objective of Attention head 6 is to fill the dimension OP1_SHIFT3 at the $i^{\\th}$ -th least significant digit of the response (when predicting the $(i+1)$ -th least significant digit of the response) with the $(i-2)$ -th least significant digit of the first operand. Similarly to the previous head, $i$ ranges from 0 to $\\ell_{a}+2$ . In cases where the $i$ -th least significant digit of the first operand is not well-defined (i.e., $i\\in\\{0,1,2\\}$ ), we assign 0. ", "page_idx": 67}, {"type": "text", "text": "The design of Attention head 6 is as follows. With $d_{Q K,26}=P+1$ , ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\mathbf{Q}_{6}^{(2)}=\\left(\\begin{array}{l l l l l l}{\\mathbf{0}_{P\\times34}}&{\\mathbf{0}_{P\\times P}}&{\\mathbf{0}_{P\\times P}}&{\\mathbf{0}_{P\\times P}}&{\\sqrt{M}I_{P}}&{\\mathbf{0}_{P\\times P}}\\\\ {\\sqrt{M P}\\left(e_{\\mathrm{FUL-OWS}}^{34}\\right)^{\\top}}&{\\mathbf{0}_{P\\times P}}&{\\mathbf{0}_{1\\times P}}&{\\mathbf{0}_{1\\times P}}&{\\mathbf{0}_{1\\times P}}&{\\mathbf{0}_{1\\times P}}\\end{array}\\right)\\in\\mathbb{R}^{d_{Q K,26}\\times d_{P\\times P}},\n$$", "text_format": "latex", "page_idx": 67}, {"type": "equation", "text": "$$\n{\\bf K}_{6}^{(2)}=\\left(\\!\\!\\begin{array}{l l l l l l}{\\mathbf{0}_{P\\times34}}&{\\sqrt{M}I_{P}}&{\\mathbf{0}_{P\\times P}}&{\\mathbf{0}_{P\\times P}}&{\\mathbf{0}_{P\\times P}}&{\\mathbf{0}_{P\\times P}}&{\\mathbf{0}_{P\\times P}}\\\\ {\\sqrt{M P}\\left(e_{1\\times-80\\times}^{34}\\right)^{\\top}}&{\\mathbf{0}_{1\\times P}}&{\\mathbf{0}_{1\\times P}}&{\\mathbf{0}_{1\\times P}}&{\\mathbf{0}_{1\\times P}}&{\\mathbf{0}_{1\\times P}}&{\\mathbf{0}_{1\\times P}}\\end{array}\\!\\!\\right)\\in\\mathbb{R}^{d_{Q K,26}\\times d_{P}},\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "With $d_{V,26}=1$ , ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{6}^{(2)}=2(e_{\\mathrm{NUM}}^{d})^{\\top}\\in\\mathbb{R}^{d_{V,26}\\times d},}\\\\ &{U_{6}^{(2)}=e_{\\mathrm{op1\\L_{3}H I F T3}}^{d}\\in\\mathbb{R}^{d\\times d_{V,26}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "We provide the examples in Tables 75 to 79. ", "page_idx": 67}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/815ee7f31028608993914915b1426474599303737086b4ef0e56566c88cb0636.jpg", "table_caption": ["Table 75: Example of ${Q_{6}^{(2)}}X^{(1)}$ , continuing from Table 49. "], "table_footnote": [], "page_idx": 67}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/1c88e1ff102d9423bf8d83048a0fa41bf5f668b738e15d1e29f308c85356e04a.jpg", "table_caption": ["Table 76: Example of $\\underline{{\\boldsymbol{K}_{6}^{(2)}\\boldsymbol{X}^{(1)}}}$ , continuing from Table 49. "], "table_footnote": [], "page_idx": 67}, {"type": "text", "text": "Table 77: Example of ${A}_{6}^{(2)}$ (with explicit row/column indices and sufficiently large $M$ ), continuing from Tables 75 and 76. ", "page_idx": 68}, {"type": "equation", "text": "${\\left[\\begin{array}{l l l l l l l l l l l l l l l l l l l}{\\log1}&{2}&{3}&{4}&{5}&{6}&{7}&{8}&{9}&{10}&{11}&{12}&{13}&{14}&{15}\\\\ {1}&{1}&{1}&{1}&{1}&{1}&{1}&{1}&{1}&{1}&{1}&{1}&{1}&{1}&{12}&{1/2}&{1/2}\\\\ {2}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{12}&{1/2}\\\\ {3}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{1/2}&{0}\\\\ {4}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{1/2}&{0}\\\\ {5}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{1/2}&{0}&{0}&{0}\\\\ {0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}\\\\ {7}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}\\\\ {8}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}\\\\ {9}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}\\\\ {1}&{0}&{0}&{0}&{0}&{0}&{0}& $ ", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "Table 78: Example of $U_{6}^{(2)}V_{6}^{(2)}X^{(1)}$ , continuing from Table 49. (Irrelevant dimensions are omitted for readability) ", "page_idx": 68}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/c08ce4181cdea70e0c822264693a0c2ea91fce929c8ae54ff0605a069f503a50.jpg", "table_caption": [], "table_footnote": [], "page_idx": 68}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/ac661de5628a634a765a1a941b72c1e0a38c112784449148b71fdd9e84170d3f.jpg", "table_caption": ["Table 79: Example of $U_{6}^{(2)}V_{6}^{(2)}X^{(1)}A_{6}^{(2)}$ , continuing from Tables 77 and 78. (Irrelevant dimensions are omitted for readability) "], "table_footnote": [], "page_idx": 68}, {"type": "text", "text": "G.7.7 Attention Head 7: Copying the Appropriate Digit from the First Operand V ", "text_level": 1, "page_idx": 68}, {"type": "text", "text": "The objective of Attention head 7 is to fill the dimension OP1_SHIFT4 at the $i^{\\th}$ -th least significant digit of the response (when predicting the $(i+1)$ -th least significant digit of the response) with the $(i-3)$ -th least significant digit of the first operand. Similarly to the previous head, $i$ ranges from 0 to $\\ell_{a}+2$ . In cases where the $i$ -th least significant digit of the first operand is not well-defined (i.e., $i\\in\\{0,1,2,3\\}$ ), we assign 0. ", "page_idx": 68}, {"type": "text", "text": "The design of Attention head 7 is as follows. With $d_{Q K,27}=P+1$ , ", "page_idx": 69}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathsf Q_{7}^{(2)}=\\left(\\frac{\\mathbf0_{P\\times34}}{\\sqrt{M P}\\left(e_{\\mathrm{{FULL-ONE}}}^{34}\\right)^{\\top}}\\right.\\quad\\mathbf0_{P\\times P}\\quad\\mathbf0_{P\\times P}\\quad\\mathbf0_{P\\times P}\\quad\\mathbf0_{P\\times P}\\quad\\mathbf0_{P\\times P}\\quad\\sqrt{M}I_{P}\\right)\\in\\mathbb{R}^{d_{Q K,27}\\times d},}\\end{array}\n$$", "text_format": "latex", "page_idx": 69}, {"type": "equation", "text": "$$\n\\mathbf{{K}}_{7}^{(2)}=\\left(\\begin{array}{l l l l l l}{\\mathbf{0}_{P\\times34}}&{\\sqrt{M}I_{P}}&{\\mathbf{0}_{P\\times P}}&{\\mathbf{0}_{P\\times P}}&{\\mathbf{0}_{P\\times P}}&{\\mathbf{0}_{P\\times P}}&{\\mathbf{0}_{P\\times P}}\\\\ {\\sqrt{M P}\\left(e_{1\\times-80\\times}^{34}\\right)^{\\top}}&{\\mathbf{0}_{1\\times P}}&{\\mathbf{0}_{1\\times P}}&{\\mathbf{0}_{1\\times P}}&{\\mathbf{0}_{1\\times P}}&{\\mathbf{0}_{1\\times P}}&{\\mathbf{0}_{1\\times P}}\\end{array}\\right)\\in\\mathbb{R}^{d_{Q K,2\\times d}},\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "With $d_{V,27}=1$ , ", "page_idx": 69}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{7}^{(2)}=2(e_{\\mathrm{NUM}}^{d})^{\\top}\\in\\mathbb{R}^{d_{V,27}\\times d},}\\\\ &{U_{7}^{(2)}=e_{\\mathrm{op1\\l_{-}s H I F T4}}^{d}\\in\\mathbb{R}^{d\\times d_{V,27}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "We provide the examples in Tables 80 to 84. ", "page_idx": 69}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/7d3abb59f2bb2489084ae4382e764e9d9c8c59b93e2cd109568a6326454c530a.jpg", "table_caption": ["Table 80: Example of $Q_{7}^{(2)}X^{(1)}$ , continuing from Table 49. "], "table_footnote": [], "page_idx": 69}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/0e4e37877d7e305dadb10d680de5c02ea0433ed8002312798c082018d6fcf7d6.jpg", "table_caption": ["Table 81: Example of $\\underline{{\\boldsymbol{K}}}_{7}^{(2)}\\boldsymbol{X}^{(1)}$ , continuing from Table 49. "], "table_footnote": [], "page_idx": 69}, {"type": "text", "text": "Table 82: Example of ${A}_{7}^{(2)}$ (with explicit row/column indices and sufficiently large $M$ ), continuing from Tables 80 and 81. ", "page_idx": 69}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/a9acb51d3978e3008cf1fb0a4cc0e24d2a2d015d682ab21024e421833d98da08.jpg", "table_caption": [], "table_footnote": [], "page_idx": 69}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/887191821366c6c30efce0c0732b32167d42847844f5d747f125e0fb043e3836.jpg", "table_caption": ["Table 83: Example of $U_{7}^{(2)}V_{7}^{(2)}X^{(1)}$ , continuing from Table 49. (Irrelevant dimensions are omitted for readability) "], "table_footnote": [], "page_idx": 70}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/35601004753ba71193eae257904d50cef97c3b8f3b4018ef993f730f92435f88.jpg", "table_caption": ["Table 84: Example of $U_{7}^{(2)}V_{7}^{(2)}X^{(1)}A_{7}^{(2)}$ , continuing from Tables 82 and 83. (Irrelevant dimensions are omitted for readability) "], "table_footnote": [], "page_idx": 70}, {"type": "text", "text": "G.7.8 Residual Connection ", "text_level": 1, "page_idx": 70}, {"type": "text", "text": "So far we have computed the output of $\\tt A t t_{2}$ operation. Passing through the residual connection, the output of the attention layer becomes the sum of $X^{(1)}$ (the input to the second Transformer block) and the output of $\\tt A t t_{2}$ operation: ", "page_idx": 70}, {"type": "equation", "text": "$$\n\\pmb{Y}^{(2)}=\\pmb{X}^{(1)}+\\sum_{h\\in[7]}\\pmb{U}_{h}^{(2)}\\pmb{V}_{h}^{(2)}\\pmb{X}^{(1)}\\pmb{A}_{h}^{(2)}.\n$$", "text_format": "latex", "page_idx": 70}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/c276714f2fc1542bd27576d3dfc565b9ca1a26a1adc4ef4f36280154ceaa2a54.jpg", "table_caption": ["Table 85: Example output of residual connection, continuing from Tables 49, 54, 59, 64, 69, 74, 79 and 84. Uncolored rows represent the initial embedding. Gray rows indicate the rows filled by the first Transformer block. Yellow rows indicate the rows filled by the attention layers at the second Transformer block. Pink rows indicate the rows that will be filled by the subsequent FFN layer. "], "table_footnote": [], "page_idx": 71}, {"type": "text", "text": "G.8 Transformer Block 2 \u2014 Token-wise Feed-forward Layer ", "text_level": 1, "page_idx": 71}, {"type": "text", "text": "Our ultimate goal is to flil the dimensions PROD and IS_EOS with appropriate values. The dimensions RESULT1 to RESULT4, PRE_PROD, and PRE_CARRY serve as temporary memories for storing intermediate values, which will help us achieve our ultimate goal. Our construction involves sequentially stacking the MLP networks step-by-step to generate each of these temporary values. (As mentioned in the theorem statement below, we allow $\\mathrm{FF_{2}}$ to be a multi-layer MLP.) ", "page_idx": 71}, {"type": "text", "text": "While our current construction for $\\mathrm{FF_{2}}$ involves multiple hidden layers, we believe that our construction can be improved to employ a single hidden layer. If employing multiple hidden layers in the FFN is not feasible, this issue can be addressed by introducing additional Transformer blocks. Specifically, we can bypass the attention layer in these extra blocks by residual connection and only utilize their FFNs. ", "page_idx": 71}, {"type": "text", "text": "Step 1. Filling RESULT_1 to RESULT_4 Here, we first assume the existence of a single-hiddenlayer MLP network, denoted as $f:\\mathbb{R}^{2}\\rightarrow\\mathbb{R}$ , such that given any integers $a,b\\in\\{0,1,\\dots,9\\}$ , $f(a,b)$ equals to their multiplication, i.e., $a b$ . Such a network can be implemented with 100 hidden nodes (Zhang et al., 2021). ", "page_idx": 71}, {"type": "text", "text": "Recalling Appendix G.4, we construct the first MLP network by utilizing eight instances of the function $f$ in parallel as follows: ", "page_idx": 71}, {"type": "equation", "text": "$$\n\\begin{array}{r}{1.\\ \\mathrm{RESULT}1=f(\\mathrm{op}1_{-}\\mathrm{sHIFT0},\\mathrm{op}2_{-}\\mathrm{oNE})+f(\\mathrm{op}1_{-}\\mathrm{sHIFT1},\\mathrm{op}2_{-}\\mathrm{TEN})\\in\\{0,1,\\ldots,162\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 71}, {"type": "text", "text": "2. $\\begin{array}{r}{\\mathtt{R E S U L T2}=f(\\mathrm{op}1_{-}\\mathrm{SHIFT1},\\mathrm{op}2_{-}\\mathrm{ONE})+f(\\mathrm{op}1_{-}\\mathrm{SHIFT2},\\mathrm{op}2_{-}\\mathrm{TEN})\\in\\{0,1,\\dots,162\\},}\\end{array}$ $\\mathrm{3.~\\RESULT3}=f(\\mathrm{op1}_{-}\\mathrm{{SHIFT2},o p2}_{-}\\mathrm{{ONE})+f(\\mathrm{op1}_{-}\\mathrm{{SHIFT3},o p2}_{-}\\mathrm{{TEN})\\in\\{0,1,...,162\\}}}$ , $4.\\ \\mathrm{RESULT4}=f(\\mathrm{op1}_{-}\\mathrm{SHIFT3},\\mathrm{op2}_{-}\\mathrm{ONE})+f(\\mathrm{op1}_{-}\\mathrm{SHIFT4},\\mathrm{op2}_{-}\\mathrm{TEN})\\in\\{0,1,\\ldots,162\\}$ . ", "page_idx": 72}, {"type": "text", "text": "Step 2. Filling PRE_PROD and PRE_CARRY ", "text_level": 1, "page_idx": 72}, {"type": "text", "text": "Here, we assume the existence of the following three single-hidden-layer MLP networks, denoted as $g_{1},g_{2},g_{3}:\\mathbb{R}\\rightarrow\\mathbb{R}$ , such that given any at most 3-digit integer $a\\in\\{0,1,\\ldots,162\\}$ , $g_{1}(a)$ , $g_{2}(a)$ and $g_{3}(a)$ output the ones, tens, and hundreds digit of $a$ , respectively. Similarly to the previous step, each network can be implemented with 163 hidden nodes (Zhang et al., 2021). ", "page_idx": 72}, {"type": "text", "text": "Recalling Appendix G.4, we construct the second MLP network on top of the first MLP network, by utilizing 2 instances of each of the function $g_{1},g_{2}$ , and $g_{3}$ in parallel as follows: ", "page_idx": 72}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\,\\,\\mathrm{PRE\\!\\!}_{-}\\mathrm{PROD}=g_{1}\\big(\\mathrm{RESULT1}\\big)+g_{2}\\big(\\mathrm{RESULT2}\\big)+g_{3}\\big(\\mathrm{RESULT3}\\big)\\in\\{0,1,\\dots,27\\},}\\\\ &{\\bullet\\,\\,\\mathrm{PRE\\!}_{-}\\mathrm{CARRY}=g_{1}\\big(\\mathrm{RESULT2}\\big)+g_{2}\\big(\\mathrm{RESULT3}\\big)+g_{3}\\big(\\mathrm{RESULT4}\\big)\\in\\{0,1,\\dots,27\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 72}, {"type": "text", "text": "Step 3. Filling PROD Here, we assume the existence of a single-hidden-layer MLP network, denoted as $h:\\breve{\\mathbb{R}}^{2}\\to\\mathbb{R}$ , such that given any integers $a\\in\\{0,1,\\ldots,27\\}$ , $b\\in\\{0,\\ensuremath{\\mathrm{i}},\\dots,9\\}$ satisfying $a-b\\in\\{-2,-1,0,8,9,10,18,1\\bar{9},20\\}$ , $h$ satisfies ", "page_idx": 72}, {"type": "equation", "text": "$$\nh(a,b)=\\left\\{\\begin{array}{l l}{0,}&{\\mathrm{if~}a-b\\in\\{-2,\\,-1,\\,0\\},}\\\\ {1,}&{\\mathrm{if~}a-b\\in\\{8,\\,9,\\,10\\},}\\\\ {2,}&{\\mathrm{if~}a-b\\in\\{18,\\,19,\\,20\\}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 72}, {"type": "text", "text": "We also assume the existence of a single-hidden-layer MLP network, denoted as $h^{\\prime}:\\mathbb{R}\\to\\mathbb{R}$ , such that given any integer $a\\in\\{0,1,\\ldots,\\bar{19}\\}$ , $h^{\\prime}(a)$ equals to $a$ (mod 10). ", "page_idx": 72}, {"type": "text", "text": "We finally assume the existence of a single-hidden-layer MLP network $q_{i}~:~\\mathbb{R}~\\rightarrow~\\mathbb{R}$ for each $i\\in\\{0,1,\\dotsc,9\\}$ , such that given any integers $a\\in\\{0,1,\\dotsc,9\\}$ , $q_{i}$ satisfies ", "page_idx": 72}, {"type": "equation", "text": "$$\nq_{i}(a)=\\mathbb{1}(i=a).\n$$", "text_format": "latex", "page_idx": 72}, {"type": "text", "text": "Similarly to the previous step, each network can be implemented with 280, 20, and 10 hidden nodes. Recalling Appendix G.4, we construct the third MLP network, on top of the second MLP network, by ", "page_idx": 72}, {"type": "equation", "text": "$$\n\\bullet\\,\\,{\\mathrm{PROD}}=\\left({\\begin{array}{c}{q_{0}(h^{\\prime}({\\mathrm{PRE}}_{-}{\\mathrm{PROD}}+h({\\mathrm{PRE}}_{-}{\\mathrm{CARRY}},{\\mathrm{NUM}})))}\\\\ {q_{1}(h^{\\prime}({\\mathrm{PRE}}_{-}{\\mathrm{PROD}}+h({\\mathrm{PRE}}_{-}{\\mathrm{CARRY}},{\\mathrm{NUM}})))}\\\\ {\\vdots}\\\\ {q_{9}(h^{\\prime}({\\mathrm{PRE}}_{-}{\\mathrm{PROD}}+h({\\mathrm{PRE}}_{-}{\\mathrm{CARRY}},{\\mathrm{NUM}})))}\\end{array}}\\right)\\in\\mathbb{R}^{10}.\n$$", "text_format": "latex", "page_idx": 72}, {"type": "text", "text": "One can easily check that $h^{\\prime}(\\mathrm{PRE\\mathrm{~\\ttPROD}}+h(\\mathrm{PRE\\mathrm{~\\ttCARRY,NUM}}))$ ) yields an element of $0,1,\\ldots,9$ , and thus PROD is an one-hot column vector. Specifically, if $h^{\\prime}(\\mathrm{PRE\\_PROD}+h(\\mathrm{PRE\\_CARRY,NUM}))=$ i, then PROD becomes ei1+01. ", "page_idx": 72}, {"type": "text", "text": "Step 4. Filling IS_EOS We construct a single-hidden-layer MLP network $r:\\mathbb{R}^{2}\\to\\mathbb{R}$ by ", "page_idx": 72}, {"type": "equation", "text": "$$\nr(a,b)=2\\phi(a+b-1.5).\n$$", "text_format": "latex", "page_idx": 72}, {"type": "text", "text": "We then can fill the dimension IS_EOS by ", "page_idx": 72}, {"type": "equation", "text": "$$\n\\bullet\\ \\mathrm{IS\\mathrm{\\_EOS}}=r\\big(\\mathrm{PRE\\mathrm{\\_EOS1,PRE\\mathrm{\\_EOS2}}}\\big).\n$$", "text_format": "latex", "page_idx": 72}, {"type": "text", "text": "Since PRE_EOS1 and PRE_EOS2 can have either $1/2$ or 1, IS_EOS equals 1 only when both PRE_EOS1 and PRE_EOS2 are 1. Additionally, we note that PRE_EOS1 and PRE_EOS2 are the direct outputs from the attention layer. Therefore, the network $r$ can be deployed in parallel with the first MLP network and does not require an additional FFN layer. ", "page_idx": 72}, {"type": "text", "text": "The example output resulting from passing through all these steps is presented in Table 86. ", "page_idx": 72}, {"type": "text", "text": "Table 86: Example output of FFN layer in the second Transformer block, continuing from Table 85. Here, we mark \u2212for the entries before the equal token, as these entries do not affect the next-token prediction in our construction and are thus not important. ", "page_idx": 73}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/dd3584a919a3079585170632578c3c9381bcbbb2c0e35038daed3f595d3b3799.jpg", "table_caption": [], "table_footnote": [], "page_idx": 73}, {"type": "text", "text": "G.8.1 Residual Connection ", "text_level": 1, "page_idx": 73}, {"type": "text", "text": "The last task of the feed-forward layer is to pass $\\mathrm{FF_{2}}\\left(\\pmb{Y}^{(2)}\\right)$ through the residual connection. As a result, we have ", "page_idx": 73}, {"type": "equation", "text": "$$\n\\pmb{X}^{(2)}=\\pmb{Y}^{(2)}+\\mathbb{F}\\mathbb{F}_{2}\\left(\\pmb{Y}^{(2)}\\right).\n$$", "text_format": "latex", "page_idx": 73}, {"type": "text", "text": "This is the end of the second Transformer block, and an example of $X^{(2)}$ is illustrated in Table 87. ", "page_idx": 73}, {"type": "text", "text": "G.9 Decoding Function ", "text_level": 1, "page_idx": 73}, {"type": "text", "text": "As mentioned in Appendix D, the decoding function performs a linear readout (with a weight matrix $W_{\\mathrm{out}}\\in\\mathbb{R}^{|\\mathcal{V}|\\times d})$ and a (token-wise) arg-max operation. That is, ", "page_idx": 73}, {"type": "equation", "text": "$$\n\\mathsf{D e c}\\left(\\mathbf{{X}}^{(1)}\\right):=(\\mathcal{V}_{k_{i}})_{i=1,\\ldots,N}\\in\\mathcal{V}^{N},\n$$", "text_format": "latex", "page_idx": 73}, {"type": "text", "text": "where $\\mathcal{V}_{k}$ is the $k$ -th element of $\\mathcal{V}$ and ", "page_idx": 73}, {"type": "equation", "text": "$$\nk_{i}:=\\underset{k\\in[|\\mathcal{V}|]}{\\arg\\operatorname*{max}}\\left\\{o_{k}:W_{\\mathrm{out}}\\pmb{X}_{\\bullet i}^{(1)}=\\left[o_{1}\\quad\\cdot\\,\\cdot\\,\\cdot\\quad o_{|\\mathcal{V}|}\\right]^{\\top}\\right\\}.\n$$", "text_format": "latex", "page_idx": 73}, {"type": "text", "text": "The objective of the decoding function is to perform a proper next-token prediction for $N\\times2$ multiplication, especially utilizing the dimensions PROD and IS_EOS of $X^{(2)}$ . ", "page_idx": 73}, {"type": "text", "text": "We now construct the weight matrix $W_{\\mathrm{{out}}}$ . For a token $\\sigma_{i}$ , if the value of dimension IS_EOS of $X^{(2)}$ is 0, then the linear readout output the dimensions PROD as it is to return one of a number token (0-9). On the other hand, if the value of dimension IS_EOS is 1, then the linear readout outputs a large number (like 9 for example) for the token $\\mathbf{\\cdot}\\mathbb{S}^{\\,,}$ to return EOS (\\$). This can be implemented by the weight matrix $W_{\\mathrm{{out}}}$ described in Table 88. Also, an example of applying the linear transform is showcased in Tables 89 and 90. ", "page_idx": 73}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/f7ecd2b1ae1700aaa73fc78625401392dc8df1b13fa30eb7dad336de598c8689.jpg", "table_caption": ["Table 87: Example embedding matrix after the second Transformer block. The yellow rows represent the results introduced during the second block, while the gray rows indicate the results from the first block. Similarly to Table 85, we mark \u2212for the entries before the equal token, as these entries do not affect the next-token prediction in our construction and are thus not important. "], "table_footnote": [], "page_idx": 74}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/510f6114493562e91cabba7a4c9e7490217e6691809e03fe5766b90645e669fe.jpg", "table_caption": ["Table 88: The transposed weight matrix $W_{o u t}^{\\top}$ of the linear readout in decoding function. $P^{\\prime}$ represents $6P+1$ . "], "table_footnote": [], "page_idx": 74}, {"type": "text", "text": "Table 89: Example output of linear readout $(W_{\\mathrm{out}}X^{(2)})$ , continuing from Tables 87 and 88. The yellow cells represent the maximum value of each column, from the $\\bullet_{=},$ token\u2019s column to the rightmost column (which are used for next-token prediction). ", "page_idx": 75}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/ad760c247245c76a5426f10ba2e092efa29e4f22b15ebfb77e4df6f7038e3f55.jpg", "table_caption": [], "table_footnote": [], "page_idx": 75}, {"type": "text", "text": "Table 90: Example output sequence ${\\mathcal{O}}=\\mathsf{D e c}\\left(X^{(2)}\\right)$ , continuing from Table 89. The yellow cells in the bottom row exactly predict the next tokens. ", "page_idx": 75}, {"type": "table", "img_path": "5cIRdGM1uG/tmp/7902a3aa04901e7e3a5e03e97945e577d532dd1b6ecd43174fee97d027a1431f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 75}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 76}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 76}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 76}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 76}, {"type": "text", "text": "Justification: We summarize our contributions in Section 1.1. ", "page_idx": 76}, {"type": "text", "text": "Guidelines: ", "page_idx": 76}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 76}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 76}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 76}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 76}, {"type": "text", "text": "Justification: We discuss the limitation of our work in conclusion (Section 7). ", "page_idx": 76}, {"type": "text", "text": "Guidelines: ", "page_idx": 76}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 76}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 76}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 76}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 76}, {"type": "text", "text": "Justification: We provide a detailed construction to prove Theorem 5.1 in Appendix E. We prove Proposition 5.2 in Appendix F. Lastly, we provide a constructive proof of Theorem 6.1 in Appendix G. ", "page_idx": 77}, {"type": "text", "text": "Guidelines: ", "page_idx": 77}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 77}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 77}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 77}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 77}, {"type": "text", "text": "Justification: We provide the training method in Sections 3 and 4 and comprehensive details on the hyperparameters used for our experiments in Appendix C. ", "page_idx": 77}, {"type": "text", "text": "Guidelines: ", "page_idx": 77}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 77}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 77}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 78}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 78}, {"type": "text", "text": "Justification: We open our codebase: github.com/HanseulJo/position-coupling. It contains fully reproducible Python codes for data generation, model training, and model evaluation. ", "page_idx": 78}, {"type": "text", "text": "Guidelines: ", "page_idx": 78}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 78}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 78}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 78}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 78}, {"type": "text", "text": "Justification: We organize the experimental details in Appendix C. ", "page_idx": 78}, {"type": "text", "text": "Guidelines: ", "page_idx": 78}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 78}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 78}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 78}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 78}, {"type": "text", "text": "Justification: We provide $95\\%$ confidence intervals in our result plots. ", "page_idx": 78}, {"type": "text", "text": "Guidelines: ", "page_idx": 78}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 78}, {"type": "text", "text": "", "page_idx": 79}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 79}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 79}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 79}, {"type": "text", "text": "Justification: We provide our computation resources and execution time in Appendix C. Guidelines: ", "page_idx": 79}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 79}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 79}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 79}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 79}, {"type": "text", "text": "Justification: We are sufficiently aware of the NeurIPS Code of Ethics and have ensured that our research complies with all its principles. ", "page_idx": 79}, {"type": "text", "text": "Guidelines: ", "page_idx": 79}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 79}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 79}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 79}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 79}, {"type": "text", "text": "Justification: Our paper focuses exclusively on arithmetic/algorithmic tasks, which do not have direct societal impacts. ", "page_idx": 79}, {"type": "text", "text": "Guidelines: ", "page_idx": 79}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 79}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 80}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 80}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 80}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 80}, {"type": "text", "text": "Justification: Our paper focuses exclusively on arithmetic/algorithmic tasks, which inherently do not pose high risks for misuse. ", "page_idx": 80}, {"type": "text", "text": "Guidelines: ", "page_idx": 80}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 80}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 80}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 80}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 80}, {"type": "text", "text": "Justification: We clarify the reference of our code base in Appendix C. ", "page_idx": 80}, {"type": "text", "text": "Guidelines: ", "page_idx": 80}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 80}, {"type": "text", "text": "", "page_idx": 81}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 81}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 81}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 81}, {"type": "text", "text": "Justification: Our paper does not propose any new datasets or models and does not release any code. ", "page_idx": 81}, {"type": "text", "text": "Guidelines: ", "page_idx": 81}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 81}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 81}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 81}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 81}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 81}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 81}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 81}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 81}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 81}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 81}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 81}, {"type": "text", "text": "", "page_idx": 82}]