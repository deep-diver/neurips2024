{"references": [{"fullname_first_author": "Emmanuel Abbe", "paper_title": "Generalization on the unseen, logic reasoning and degree curriculum", "publication_date": "2023", "reason": "This paper provides a theoretical perspective on the challenges of length generalization in neural networks, which is directly relevant to the central theme of the target paper."}, {"fullname_first_author": "Hanseul Cho", "paper_title": "Arithmetic transformers can length-generalize in both operand length and count", "publication_date": "2024", "reason": "This paper is a concurrent work that explores similar ideas to the target paper and offers a comparative analysis of different approaches to length generalization in arithmetic transformers."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017", "reason": "This paper introduced the Transformer architecture, which forms the basis of the models used in the target paper and is thus foundational to the work."}, {"fullname_first_author": "Nayoung Lee", "paper_title": "Teaching arithmetic to small transformers", "publication_date": "2024", "reason": "This paper investigates length generalization in arithmetic transformers, proposing and evaluating methods that directly address the problem explored in the target paper."}, {"fullname_first_author": "Hattie Zhou", "paper_title": "What algorithms can transformers learn? a study in length generalization", "publication_date": "2024", "reason": "This paper focuses on the algorithmic capabilities of transformers, examining their ability to generalize to different sequence lengths, and provides a direct comparison to the methods proposed in the target paper."}]}