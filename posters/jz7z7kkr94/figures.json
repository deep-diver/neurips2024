[{"figure_path": "Jz7Z7KkR94/figures/figures_1_1.jpg", "caption": "Figure 1: REDUCR starts by initializing weights of classes. At each timestep t, the model receives a batch of datapoints Bt. REDUCR computes the selection scores for each datapoint based on its usefulness to the model and the class weights, and selects new datapoints bt C Bt that achieve the highest selection scores. After the model takes gradient steps on the selected datapoints, REDUCR adjusts the weights to reflect increased priorities on underperforming classes.", "description": "This figure illustrates the REDUCR algorithm's workflow. It starts with initializing class weights.  At each timestep, a batch of datapoints (Bt) is received.  The algorithm then calculates selection scores for each datapoint, considering its usefulness to the model and the current class weights. Datapoints (bt) with the highest scores are selected for model training. After model updates based on the selected datapoints, REDUCR adjusts class weights to prioritize underperforming classes, ensuring better generalization performance across all classes.", "section": "4 REDUCR for Robust Online Batch Selection"}, {"figure_path": "Jz7Z7KkR94/figures/figures_1_2.jpg", "caption": "Figure 2: REDUCR significantly improves worst-class test accuracy on Clothing1M outperforming Uniform and other recent works.", "description": "The figure is a bar chart comparing the worst-class test accuracy of different online batch selection methods on the Clothing1M dataset.  REDUCR significantly outperforms other methods, including Uniform sampling and the state-of-the-art method from Loshchilov and Hutter (2015), demonstrating its robustness and effectiveness in improving the performance of the worst-performing class.", "section": "Main contributions"}, {"figure_path": "Jz7Z7KkR94/figures/figures_6_1.jpg", "caption": "Figure 3: REDUCR improves the worst-class test accuracy and data efficiency when compared with the RHO-LOSS, TRAIN LOSS and UNIFORM baselines on the a) Clothing1M dataset, b) the CINIC10 dataset, and c) the CIFAR100 dataset.", "description": "The figure shows the performance of REDUCR compared to three baseline methods (RHO-Loss, Train Loss, Uniform) across three image classification datasets (Clothing1M, CINIC10, CIFAR100).  Each subplot displays the worst-class test accuracy over training steps.  REDUCR consistently outperforms the baselines, demonstrating improved data efficiency and robustness to class imbalance.", "section": "5 Experiments"}, {"figure_path": "Jz7Z7KkR94/figures/figures_8_1.jpg", "caption": "Figure 4: a) The worst-class test accuracy decreases when the model loss, class irreducible loss, and class-holdout loss terms are removed from REDUCR on CINIC10. Comparing REDUCR with clipping for excess losses (Algorithm 1) and REDUCR (no clip) which removes the clipping, we observe that REDUCR achieves more stable performance. We show the class weights wt at each training step for b) REDUCR and c) REDUCR with the class-holdout loss term ablated. The ablation model fails to consistently prioritise the underperforming classes across multiple runs.", "description": "The figure shows the ablation study of the REDUCR algorithm, demonstrating the impact of removing each of its three loss terms (model loss, class-irreducible loss, and class-holdout loss) on the worst-class test accuracy and class weights.  The results highlight that all three terms are crucial for optimal performance and stability, as removing any one significantly degrades the results. The comparison between REDUCR with and without clipping of the excess loss showcases the improved stability of the clipping mechanism.", "section": "4 REDUCR for Robust Online Batch Selection"}, {"figure_path": "Jz7Z7KkR94/figures/figures_8_2.jpg", "caption": "Figure 3: REDUCR improves the worst-class test accuracy and data efficiency when compared with the RHO-LOSS, TRAIN LOSS and UNIFORM baselines on the a) Clothing1M dataset, b) the CINIC10 dataset, and c) the CIFAR100 dataset.", "description": "This figure shows the training curves for worst-class test accuracy for three different datasets: Clothing1M, CINIC10 and CIFAR100. The results show that REDUCR consistently outperforms other methods in terms of worst-class accuracy, achieving a significant boost in performance compared to the baselines (RHO-Loss, Train Loss, and Uniform).  The plots demonstrate REDUCR's improved data efficiency by achieving comparable or better accuracy with fewer training steps.", "section": "5 Experiments"}, {"figure_path": "Jz7Z7KkR94/figures/figures_9_1.jpg", "caption": "Figure 3: REDUCR improves the worst-class test accuracy and data efficiency when compared with the RHO-LOSS, TRAIN LOSS and UNIFORM baselines on the a) Clothing1M dataset, b) the CINIC10 dataset, and c) the CIFAR100 dataset.", "description": "This figure displays the results of comparing REDUCR against three baseline methods (RHO-Loss, Train Loss, and Uniform) using three different datasets (Clothing1M, CINIC10, and CIFAR100).  For each dataset, the worst-class test accuracy is plotted against the number of training steps, illustrating REDUCR's superior performance and efficiency in improving worst-class accuracy.", "section": "5 Experiments"}, {"figure_path": "Jz7Z7KkR94/figures/figures_14_1.jpg", "caption": "Figure 3: REDUCR improves the worst-class test accuracy and data efficiency when compared with the RHO-LOSS, TRAIN LOSS and UNIFORM baselines on the a) Clothing1M dataset, b) the CINIC10 dataset, and c) the CIFAR100 dataset.", "description": "The figure shows three plots visualizing the performance of REDUCR compared to three baseline methods (RHO-Loss, Train Loss, and Uniform) across three different datasets (Clothing1M, CINIC10, and CIFAR100). Each plot shows the worst-class test accuracy over training steps.  REDUCR consistently outperforms the baselines, demonstrating its effectiveness in improving the worst-performing class's accuracy and data efficiency.", "section": "5 Experiments"}, {"figure_path": "Jz7Z7KkR94/figures/figures_15_1.jpg", "caption": "Figure 8: Clothing1M train-test class distribution shift. The number of points in classes 4 and 7 change dramatically between the train and test sets.", "description": "This figure shows the class distribution shift between the training and testing sets of the Clothing1M dataset.  It highlights a significant difference in the proportions of certain classes (particularly classes 4 and 7) between the training and testing data, indicating a potential distributional shift that could impact model performance.", "section": "5.1 Key results"}, {"figure_path": "Jz7Z7KkR94/figures/figures_16_1.jpg", "caption": "Figure 3: REDUCR improves the worst-class test accuracy and data efficiency when compared with the RHO-LOSS, TRAIN LOSS and UNIFORM baselines on the a) Clothing1M dataset, b) the CINIC10 dataset, and c) the CIFAR100 dataset.", "description": "This figure compares the performance of REDUCR against three baseline methods (RHO-Loss, Train Loss, and Uniform) across three different datasets (Clothing1M, CINIC10, and CIFAR100).  Each subfigure shows the worst-class test accuracy over training steps. REDUCR consistently outperforms the baselines, demonstrating improved worst-class accuracy and efficiency.", "section": "5 Experiments"}, {"figure_path": "Jz7Z7KkR94/figures/figures_17_1.jpg", "caption": "Figure 10: The quantiles of the excess loss of points selected at each training step with (a) and without clipping (b) of the excess loss term", "description": "This figure shows the effect of clipping on the excess loss in the REDUCR algorithm.  The excess loss is the difference between the model loss and the class-irreducible loss. Clipping helps to stabilize the selection scores during training, making the algorithm more robust. The figure shows the quantiles (2.5th, 25th, 50th, 75th, 97.5th) of the weighted sum of excess losses for each selected point. The left panel shows the effect of clipping the excess loss while the right panel shows the results without clipping. The non-clipped excess loss shows greater volatility towards the end of training while the clipped excess loss remains much more stable.", "section": "4.2 Computing selection scores"}, {"figure_path": "Jz7Z7KkR94/figures/figures_17_2.jpg", "caption": "Figure 3: REDUCR improves the worst-class test accuracy and data efficiency when compared with the RHO-LOSS, TRAIN LOSS and UNIFORM baselines on the a) Clothing1M dataset, b) the CINIC10 dataset, and c) the CIFAR100 dataset.", "description": "This figure shows the comparison of REDUCR with three baseline methods (RHO-LOSS, TRAIN LOSS, and UNIFORM) in terms of worst-class test accuracy and training efficiency on three image datasets: Clothing1M, CINIC10, and CIFAR100.  Each subfigure (a, b, c) corresponds to one dataset, plotting the worst-class test accuracy over training steps.  The results demonstrate that REDUCR consistently outperforms the baselines across all three datasets, achieving higher worst-class accuracy with fewer training steps.", "section": "5 Experiments"}, {"figure_path": "Jz7Z7KkR94/figures/figures_18_1.jpg", "caption": "Figure 3: REDUCR improves the worst-class test accuracy and data efficiency when compared with the RHO-LOSS, TRAIN LOSS and UNIFORM baselines on the a) Clothing1M dataset, b) the CINIC10 dataset, and c) the CIFAR100 dataset.", "description": "This figure shows the performance of REDUCR compared to three baseline methods (RHO-Loss, Train Loss, Uniform) across three image datasets (Clothing1M, CINIC10, CIFAR100). For each dataset, two subplots are shown: one for worst-class test accuracy and another for training loss.  The results demonstrate that REDUCR consistently outperforms the baselines in terms of worst-class test accuracy and achieves this with improved data efficiency (fewer training steps).", "section": "5 Experiments"}, {"figure_path": "Jz7Z7KkR94/figures/figures_18_2.jpg", "caption": "Figure 13: Class-irreducible loss model test accuracies on the expert class and non-expert classes. Class-irreducible loss models are trained using gradient weights \u03b3\u2208 {0.25, 0.5, 1.0, 4.0, 9.0, 19.0, 49.0, 99.0}.", "description": "This figure shows the test accuracy of the class-irreducible loss models on CIFAR10.  The x-axis represents the gradient weight (\u03b3) used during training, and the y-axis represents the test accuracy. Two sets of bars are shown for each gradient weight: one for the expert class (the class the model is trained to be an expert on) and one for the non-expert classes (all other classes). The figure demonstrates the trade-off between expert class accuracy and non-expert class accuracy as the gradient weight changes.  A higher gradient weight leads to better expert class accuracy, but lower non-expert class accuracy.", "section": "A.9 Amortised Class Irreducible Loss Models"}, {"figure_path": "Jz7Z7KkR94/figures/figures_19_1.jpg", "caption": "Figure 3: REDUCR improves the worst-class test accuracy and data efficiency when compared with the RHO-LOSS, TRAIN LOSS and UNIFORM baselines on the a) Clothing1M dataset, b) the CINIC10 dataset, and c) the CIFAR100 dataset.", "description": "The figure shows the performance of REDUCR compared to three baseline methods (RHO-Loss, Train Loss, and Uniform) on three different datasets (Clothing1M, CINIC10, and CIFAR100).  For each dataset, it plots the worst-class test accuracy over training steps. REDUCR consistently outperforms the baselines, demonstrating improved worst-class accuracy and data efficiency.", "section": "5 Experiments"}, {"figure_path": "Jz7Z7KkR94/figures/figures_20_1.jpg", "caption": "Figure 3: REDUCR improves the worst-class test accuracy and data efficiency when compared with the RHO-LOSS, TRAIN LOSS and UNIFORM baselines on the a) Clothing1M dataset, b) the CINIC10 dataset, and c) the CIFAR100 dataset.", "description": "The figure shows the performance of REDUCR in comparison to three baseline methods (RHO-LOSS, TRAIN LOSS, and UNIFORM) on three image datasets (Clothing1M, CINIC10, and CIFAR100).  For each dataset, three subplots display the worst-class test accuracy over training steps. REDUCR consistently outperforms the baselines, particularly in terms of worst-class accuracy and data efficiency, highlighting its robustness to class imbalance and distributional shifts.", "section": "5 Experiments"}, {"figure_path": "Jz7Z7KkR94/figures/figures_21_1.jpg", "caption": "Figure 3: REDUCR improves the worst-class test accuracy and data efficiency when compared with the RHO-LOSS, TRAIN LOSS and UNIFORM baselines on the a) Clothing1M dataset, b) the CINIC10 dataset, and c) the CIFAR100 dataset.", "description": "This figure compares the performance of REDUCR against three baseline methods (RHO-LOSS, TRAIN LOSS, and UNIFORM) in terms of worst-class test accuracy and data efficiency across three different datasets: Clothing1M, CINIC10, and CIFAR100.  Each subfigure shows the training curves for each method, highlighting REDUCR's superior performance and faster convergence.", "section": "5 Experiments"}, {"figure_path": "Jz7Z7KkR94/figures/figures_21_2.jpg", "caption": "Figure 3: REDUCR improves the worst-class test accuracy and data efficiency when compared with the RHO-LOSS, TRAIN LOSS and UNIFORM baselines on the a) Clothing1M dataset, b) the CINIC10 dataset, and c) the CIFAR100 dataset.", "description": "The figure shows the performance of REDUCR against three baseline methods (RHO-LOSS, TRAIN LOSS, and UNIFORM) on three datasets (Clothing1M, CINIC10, and CIFAR100).  For each dataset, two graphs are shown: one illustrating worst-class test accuracy and another showing average test accuracy.  The x-axis represents the training steps, while the y-axis represents the test accuracy. The graphs illustrate how REDUCR consistently improves both worst-class and average test accuracy across different datasets and training progression, showcasing its superior data efficiency compared to the baseline methods. ", "section": "5 Experiments"}, {"figure_path": "Jz7Z7KkR94/figures/figures_22_1.jpg", "caption": "Figure 3: REDUCR improves the worst-class test accuracy and data efficiency when compared with the RHO-LOSS, TRAIN LOSS and UNIFORM baselines on the a) Clothing1M dataset, b) the CINIC10 dataset, and c) the CIFAR100 dataset.", "description": "This figure shows the performance of REDUCR against three baseline methods (RHO-LOSS, TRAIN LOSS, and UNIFORM) on three different datasets (Clothing1M, CINIC10, and CIFAR100).  The x-axis represents the number of training steps, and the y-axis represents the worst-class test accuracy.  The plots demonstrate that REDUCR consistently outperforms the baselines in terms of worst-class accuracy and achieves this with greater data efficiency.", "section": "5 Experiments"}, {"figure_path": "Jz7Z7KkR94/figures/figures_22_2.jpg", "caption": "Figure 3: REDUCR improves the worst-class test accuracy and data efficiency when compared with the RHO-LOSS, TRAIN LOSS and UNIFORM baselines on the a) Clothing1M dataset, b) the CINIC10 dataset, and c) the CIFAR100 dataset.", "description": "This figure compares the performance of REDUCR against three baseline methods (RHO-Loss, Train Loss, and Uniform) across three different datasets (Clothing1M, CINIC10, and CIFAR100).  The plots show the worst-class test accuracy over training steps.  It demonstrates that REDUCR achieves higher worst-class accuracy and reaches its peak performance more quickly than the baselines.", "section": "5 Experiments"}, {"figure_path": "Jz7Z7KkR94/figures/figures_22_3.jpg", "caption": "Figure 1: REDUCR starts by initializing weights of classes. At each timestep t, the model receives a batch of datapoints Bt. REDUCR computes the selection scores for each datapoint based on its usefulness to the model and the class weights, and selects new datapoints bt C Bt that achieve the highest selection scores. After the model takes gradient steps on the selected datapoints, REDUCR adjusts the weights to reflect increased priorities on underperforming classes.", "description": "This figure illustrates the REDUCR algorithm's workflow.  It starts with initializing class weights. At each timestep, a batch of data points is received, and selection scores are computed based on datapoint usefulness and class weights.  The algorithm selects the highest-scoring data points, updates the model parameters using them, and then adjusts the class weights to prioritize underperforming classes. This iterative process aims to efficiently downsample data while maintaining good performance even for underrepresented classes.", "section": "4 REDUCR for Robust Online Batch Selection"}, {"figure_path": "Jz7Z7KkR94/figures/figures_22_4.jpg", "caption": "Figure 3: REDUCR improves the worst-class test accuracy and data efficiency when compared with the RHO-LOSS, TRAIN LOSS and UNIFORM baselines on the a) Clothing1M dataset, b) the CINIC10 dataset, and c) the CIFAR100 dataset.", "description": "The figure shows three plots that compare REDUCR against three other methods: RHO-Loss, Train Loss, and Uniform. Each plot displays the worst-class test accuracy over training steps for a different dataset: Clothing1M, CINIC10, and CIFAR100. The plots demonstrate that REDUCR consistently achieves higher worst-class test accuracy and often surpasses the other three methods in terms of data efficiency.", "section": "5 Experiments"}]