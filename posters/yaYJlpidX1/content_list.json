[{"type": "text", "text": "Metalearning to Continually Learn In Context ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 General-purpose learning systems should improve themselves in open-ended fash  \n2 ion in ever-changing environments. Conventional learning algorithms for neural   \n3 networks, however, suffer from catastrophic forgetting (CF)\u2014previously acquired   \n4 skills are forgotten when a new task is learned. Instead of hand-crafting new   \n5 algorithms for avoiding CF, we propose Automated Continual Learning (ACL) to   \n6 train self-referential neural networks to meta-learn their own in-context continual   \n7 (meta-)learning algorithms. ACL encodes continual learning desiderata\u2014good   \n8 performance on both old and new tasks\u2014into its meta-learning objectives. Our   \n9 experiments demonstrate that, in general, in-context learning algorithms also suffer   \n10 from CF but ACL effectively solves such \u201cin-context catastrophic forgetting\u201d. Our   \n11 ACL-learned algorithms outperform hand-crafted ones and popular meta-continual   \n12 learning methods on the Split-MNIST benchmark in the replay-free setting, and   \n13 enables continual learning of diverse tasks consisting of multiple few-shot and stan  \n14 dard image classification datasets. Going beyond, we also highlight the limitations   \n15 of in-context continual learning, by investigating the possibilities to extend ACL to   \n16 the realm of state-of-the-art CL methods which leverage pre-trained models.1 ", "page_idx": 0}, {"type": "text", "text": "17 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "18 Enemies of memories are other memories [1]. Continually-learning artificial neural networks (NNs)   \n19 are memory systems in which their weights store memories of task-solving skills or programs, and   \n20 their learning algorithm is responsible for memory read/write operations. Conventional learning   \n21 algorithms\u2014used to train NNs in the standard scenarios where all training data is available at once\u2014   \n22 are known to be inadequate for continual learning (CL) of multiple tasks where data for each task   \n23 is available sequentially and exclusively, one at a time. They suffer from \u201ccatastrophic forgetting\u201d   \n24 (CF; [2\u20135]); the NNs forget, or rather, the learning algorithm erases, previously acquired skills, in   \n25 exchange of learning to solve a new task. Naturally, a certain degree of forgetting is unavoidable   \n26 when the memory capacity is limited, and the amount of things to remember exceeds such an upper   \n27 bound. In general, however, capacity is not the fundamental cause of CF; typically, the same NNs,   \n28 suffering from CF when trained on two tasks sequentially, can perform well on both tasks when they   \n29 are jointly trained on the two tasks at once instead (see, e.g., [6]).   \n30 The real root of CF lies in the learning algorithm as a memory mechanism. A \u201cgood\u201d CL algorithm   \n31 should preserve previously acquired knowledge while also leveraging previous learning experiences   \n32 to improve future learning, by maximally exploiting the limited memory space of model parameters.   \n33 All of this is the decision-making problem of learning algorithms. In fact, we can not blame the   \n34 conventional learning algorithms for causing CF, since they are not aware of such a problem. They   \n35 are designed to train NNs for a given task at hand; they treat each learning experience independently   \n36 (they are stationary up to certain momentum parameters in certain optimizers), and ignore any   \n37 potential influence of current learning on past or future learning experiences. Effectively, more   \n38 sophisticated algorithms previously proposed against CF [7, 8], such as elastic weight consolidation   \n39 [9, 10] or synaptic intelligence [11], often introduce manually-designed constraints as regularization   \n40 terms to explicitly penalize current learning for deteriorating knowledge acquired in past learning.   \n41 Here, instead of hand-crafting learning algorithms for continual learning, we train self-referential   \n42 neural networks [12, 13] to meta-learn their own \u201cin-context\u201d continual learning algorithms. We   \n43 train them through gradient descent on learning objectives that reflect desiderata for continual learn  \n44 ing algorithms\u2014good performance on both old and new tasks, including forward and backward   \n45 transfer. In fact, by extending the standard settings of few-shot or meta-learning based on sequence  \n46 processing NNs [14\u201318], the continual learning problem can also be formulated as a long-span   \n47 sequence processing task [19]. Corresponding CL sequences can be obtained by concatenating multi  \n48 ple few-shot/meta-learning sub-sequences, where each sub-sequence consists of input/target examples   \n49 corresponding to the task to be learned in-context. As we\u2019ll see in Sec. 3, this setting also allows us   \n50 to seamlessly express classic desiderata for CL as part of objective functions of the meta-learner.   \n51 Once formulated as such a sequence-learning task, we let gradient descent search for CL algorithms   \n52 achieving the desired CL behaviors in the program space of NN weights. In principle, all typical   \n53 challenges of CL\u2014such as the stability-plasticity dilemma [20]\u2014are automatically discovered and   \n54 handled by the gradient-based program search process. Once trained, CL is automated through   \n55 recursive self-modification dynamics of the trained NN, without requiring any human intervention   \n56 such as adding extra regularization or setting hyper-parameters for CL. Therefore, we call our   \n57 method, Automated Continual Learning (ACL).   \n58 Our experiments focus on supervised image classification, making use of standard few-shot learning   \n59 datasets for meta-training, namely, Mini-ImageNet [21, 22], Omniglot [23], and FC100 [24], while   \n60 we also meta-test on other datasets including MNIST [25], FashionMNIST [26] and CIFAR-10 [27].   \n61 Our core contribution is a set of focused experiments showing various facets of in-context CL: (1)   \n62 We first reveal the \u201cin-context catastrophic forgetting\u201d problem using two-task settings (Sec. 4.1) and   \n63 analyse its emergence (Sec. 4.2). We are not aware of any prior work discussing this problem. (2)   \n64 We show very promising results of our ACL-trained learning algorithm on the classic Split-MNIST   \n65 [6, 28] benchmark, outperforming hand-crafted learning algorithms and prior meta-continual learning   \n66 methods [29\u201331]. (3) We experimentally illustrate the limitations of ACL on 5-datasets [32] and   \n67 Split-CIFAR100 by comparing to more recent prompt-based state-of-the-art CL methods [33, 34]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "68 2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "69 2.1 Continual Learning ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "70 The main focus of this work is on continual learning [35, 36] in supervised learning settings even   \n71 though high-level principles we discuss here also transfer to reinforcement learning settings [37].   \n72 In addition, we focus on the realm of CL methods that keep model sizes constant (unlike certain   \n73 CL methods that incrementally add more parameters as more tasks are presented; see, e.g., [38]),   \n74 and do not make use of any external replay memory (used in other CL methods; see, e.g., [39\u201343]).   \n75 Classic desiderata for a CL system (see, e.g., [44, 45]) are typically summarized as good performance   \n76 on three metrics: classification accuracies on each dataset (their average), backward transfer (i.e., im  \n77 pact of learning a new task on the model\u2019s performance on previous tasks; e.g., catastrophic forgetting   \n78 is a negative backward transfer), and forward transfer (impact of learning a task for the model\u2019s perfor  \n79 mance on a future task). From a broader perspective of meta-learning systems, we may also measure   \n80 other effects such as learning acceleration (i.e., whether the system leverages previous learning ex  \n81 periences to accelerate future learning); here our primary focus remains the classic CL metrics above. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "82 2.2 Few-shot/meta-learning via Sequence Learning ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "83 In Sec. 3, we\u2019ll formulate continual learning as a long-span sequence processing task. This is a direct   \n84 extension of the classic few-shot/meta learning formulated as a sequence learning problem. In fact,   \n85 since the seminal works [14\u201317] (see also [46]), many sequence processing neural networks (see,   \n86 e.g., [47\u201358] including Transformers [59, 18]) have been trained as a meta-learner [13, 12] that learn   \n87 by observing sequences of training examples (i.e., pairs of inputs and their labels) in-context.   \n88 Here we briefly review such a formulation. Let $d,\\,N,\\,K,$ $P$ be positive integers. In sequential   \n89 $N$ -way $K$ -shot classification settings, a sequence processing NN with a parameter vector $\\boldsymbol{\\dot{\\theta}}\\in\\mathbb{R}^{P}$   \n90 observes a pair $(\\pmb{x}_{t},\\,y_{t})$ where $\\pmb{x}_{t}\\:\\in\\:\\mathbb{R}^{d}$ is the input and $y_{t}\\,\\in\\,\\{1,...,N\\}$ is its label at each step   \n91 $t\\in\\{1,...,N\\cdot K\\}$ , corresponding to $K$ examples for each one of $N$ classes. After the presentation of   \n92 these $N\\cdot K$ examples (often called the support set), one extra input $\\pmb{x}\\in\\mathbb{R}^{d}$ (often called the query)   \n93 is fed to the model without its true label but with an \u201cunknown label\u201d token $\\mathcal{Q}$ (number of input labels   \n94 accepted by the model is thus $N+1$ ). The model is trained to predict its true label, i.e., the parameters   \n95 of the model $\\theta$ are optimized to maximize the probability $p(\\bar{y}|(\\pmb{x}_{1},y_{1}),...,(\\pmb{x}_{N\\cdot K},y_{N\\cdot K})\\bar{,}(\\pmb{x},\\emptyset);\\theta)$   \n96 of the correct label $y\\;\\in\\;\\{1,...,N\\}$ of the input query $\\textbf{\\em x}$ . Since class-to-label associations are   \n97 randomized and unique to each sequence $((\\pmb{x}_{1},y_{1}),...,(\\pmb{x}_{N\\cdot K},y_{N\\cdot K}),(\\pmb{x},\\emptyset))$ , each such a sequence   \n98 represents a new (few-shot or meta) learning example to train the model. To be more specific, this   \n99 is the synchronous label setting of Mishra et al. [18] where the learning phase (observing examples,   \n00 $(\\pmb{x}_{1},y_{1})$ etc.) is separated from the prediction phase (predicting label $y$ given $(\\pmb{x},\\mathcal{O}))$ . We opt for   \n101 this variant in our experiments as we empirically find this (at least in our specific settings) more   \n102 stable than the delayed label setting [14] where the model has to make a prediction for every input,   \n103 and the label is fed to the model with a delay of one time step. ", "page_idx": 1}, {"type": "image", "img_path": "yaYJlpidX1/tmp/94a5b2443f7730d6a26b1439246c0d7326bf57d4296985024b763ab21ebfc581.jpg", "img_caption": ["Figure 1: An illustration of meta-training in Automated Continual Learning (ACL) for a selfreferential/modifying weight matrix $W_{0}$ . Weights $W_{A}$ obtained by observing examples for Task A (blue) are used to predict a test example for Task A. Weights $W_{\\mathcal{A},\\mathcal{B}}$ obtained by observing examples for Task A then those for Task B (yellow) are used to predict a test example for Task A (backward transfer) as well as a test example for Task B (forward transfer). "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "104 2.3 Self-Referential Weight Matrices ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "105 Our method (Sec. 3) can be applied to any sequence-processing NN architectures in principle.   \n106 Nevertheless, certain architectures naturally fit better to parameterize a self-improving continual   \n107 learner. Here we use the modern self-referential weight matrix (SRWM; [19, 60]) to build a generic   \n108 self-modifying NN. An SRWM is a weight matrix that sequentially modifies itself as a response   \n109 to a stream of input observations [12, 61]. The modern SRWM belongs to the family of linear   \n110 Transformers (LTs) a.k.a. Fast Weight Programmers (FWPs; [62\u201368]). Linear Transformers and   \n111 FWPs are an important class of the now popular Transformers [59]: unlike the standard ones whose   \n112 computational requirements grow quadratically and whose state size grows linearly with the context   \n113 length, LTs/FWPs\u2019 complexity is linear and the state size is constant w.r.t. sequence length (like   \n114 in the standard RNNs). This is an important property for in-context CL, since, conceptually, we   \n115 want such a CL system to continue to learn for an arbitrarily long, lifelong time span. Moreover,   \n116 the duality between linear attention and FWPs [67]\u2014and likewise, between linear attention and   \n117 gradient descent-trained linear layers [69, 70]\u2014have played a key role in certain theoretical analyses   \n118 of in-context learning capabilities of Transformers [71, 72].   \n119 The dynamics of an SRWM [19] are described as follows. Let $d_{\\mathrm{in}}$ , $d_{\\mathrm{out}}$ , $t$ be positive integers, and $\\otimes$   \n120 denote outer product. At each time step $t$ , an SRWM $W_{t-1}\\in\\mathbb{R}^{(d_{\\mathrm{out}}+2*d_{\\mathrm{in}}+1)\\times d_{\\mathrm{in}}}$ observes an input ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "121 $\\mathbf{\\boldsymbol{x}}_{t}\\in\\mathbb{R}^{d_{\\mathrm{in}}}$ , and outputs $\\pmb{y}_{t}\\in\\mathbb{R}^{d_{\\mathrm{out}}}$ , while also updating itself to $W_{t}$ as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{\\left[y_{t},k_{t},q_{t},\\beta_{t}\\right]=W_{t-1}\\pmb{x}_{t}}\\\\ &{\\pmb{v}_{t}=W_{t-1}\\phi(\\pmb{q}_{t});\\;\\bar{\\pmb{v}}_{t}=\\pmb{W}_{t-1}\\phi(\\pmb{k}_{t})}\\\\ &{\\pmb{W}_{t}=\\pmb{W}_{t-1}+\\sigma(\\beta_{t})(\\pmb{v}_{t}-\\bar{\\pmb{v}}_{t})\\otimes\\phi(\\pmb{k}_{t})}\\end{array}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "122 where $\\pmb{v}_{t}$ $\\boldsymbol{\\mathbf{\\rho}}_{t},\\bar{\\boldsymbol{v}}_{t}\\in\\mathbb{R}^{(d_{\\mathrm{out}}+2*d_{\\mathrm{in}}+1)}$ are value vectors, $\\pmb{q}_{t}\\in\\mathbb{R}^{d_{\\mathrm{in}}}$ and $\\pmb{k}_{t}\\in\\mathbb{R}^{d_{\\mathrm{in}}}$ are query and key vectors,   \n123 and $\\sigma(\\beta_{t})\\in\\mathbb{R}$ is the learning rate. $\\sigma$ and $\\phi$ denote sigmoid and softmax functions respectively. $\\phi$   \n124 is typically also applied to $\\pmb{x}_{t}$ in Eq. 1; here we follow Irie et al. [19]\u2019s few-shot image classification   \n125 setting, and use the variant without it. Eq. 3 corresponds to a rank-one update of the SRWM, from   \n126 $W_{t-1}$ to $W_{t}$ , through the delta learning rule [73, 67] where the self-generated patterns, $\\pmb{v}_{t}$ , $\\phi(\\pmb{k}_{t})$ ,   \n127 and $\\sigma(\\beta_{t})$ , play the role of target, input, and learning rate of the learning rule respectively. The   \n128 delta rule is crucial for the performance of LTs [67, 68, 74, 75].   \n129 The initial weight matrix $W_{0}$ is the only trainable parameters of this layer, that encodes the initial   \n130 self-modification algorithm. We use the layer above as a direct replacement to the self-attention layer   \n131 in the Transformer architecture [59]; and use the multi-head version of the computation above [19]. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "132 3 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "133 Task Formulation. We formulate continual learning as a long-span sequence learning task. Let   \n134 D, N, $K$ , $L$ denote positive integers. Consider two $N$ -way classification tasks A and $\\mathbf{B}$ to be   \n135 learned sequentially (as we\u2019ll see, this can be straightforwardly extended to more tasks). The   \n136 formulation here applies to both \u201cmeta-training\u201d and \u201cmeta-test\u201d phases (see Appendix A.1 for more   \n137 on this terminology). We denote the respective training datasets as $\\boldsymbol{\\mathcal{A}}$ and $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ , and test sets as $\\mathcal{A^{\\prime}}$   \n138 and $\\boldsymbol{{\\beta^{\\prime}}}$ . We assume that each datapoint in these datasets consists of one input feature $\\pmb{x}\\in\\mathbb{R}^{D}$ of   \n139 dimension $D$ (generically denoted as vector $\\textbf{\\em x}$ , but it is an image in all our experiments) and one label   \n140 $y\\in\\{1,...,N\\}$ . We consider two sequences of $L$ training examples $\\big((\\pmb{x}_{1}^{A},y_{1}^{A}),...,(\\pmb{x}_{L}^{A},y_{L}^{A})\\big)$ and   \n141 $\\left((\\pmb{x}_{1}^{B},y_{1}^{B}),...,(\\pmb{x}_{L}^{B},y_{L}^{B})\\right)$ sampled from the respective training sets $\\boldsymbol{\\mathcal{A}}$ and $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ . In practice, $L=N K$   \n142 where $K$ is the number of training examples for each class. By concatenating these two sequences,   \n143 we obtain one long sequence representing CL examples to be presented as an input sequence to   \n144 a (left-to-right) auto-regressive model. At the end of the sequence, the model is tasked to make   \n145 predictions on test examples sampled from both $\\mathcal{A}^{\\prime}$ and $\\boldsymbol{{\\beta^{\\prime}}}$ ; we assume a single test example for   \n146 each task (hence, without index): $(x^{A^{\\prime}},y^{A^{\\prime}})$ and $({\\pmb x}^{B^{\\prime}},{\\pmb y}^{B^{\\prime}})$ respectively; which we simply denote as   \n147 $(x_{\\mathrm{test}}^{A},y_{\\mathrm{test}}^{A})$ and $(\\pmb{x}_{\\mathrm{test}}^{B},y_{\\mathrm{test}}^{B})$ instead.   \n148 Our model is a self-referential NN that modifies its own weight matrices as a function of input   \n149 observations. To simplify the notation, we denote the state of our self-referential NN as a   \n150 single SRWM $W_{*}$ (even though it may have many of them in practice) where we\u2019ll replace $^*$   \n115512 $(\\stackrel{\\cdot}{(}x_{1}^{\\mathcal{A}},y_{1}^{\\mathcal{A}}),\\stackrel{\\cdot}{...},(x_{L}^{\\mathcal{A}},y_{L}^{\\mathcal{A}}\\stackrel{\\cdot}{)},(x_{1}^{\\mathcal{B}},y_{1}^{\\mathcal{B}}),...,(x_{L}^{\\mathcal{B}},y_{L}^{\\mathcal{B}}))$ ,p uotusr  itm ohdase l oabusteor-vreegdr.e sGsiivveelny  ac otnrasiunminegs  osenqe uiennpcuet   \n153 at a time, from left to right, in the auto-regressive fashion. Let $W_{A}$ denote the state of the SRWM that   \n154 has consumed the first part of the sequence, i.e., the examples from Task A, $(\\pmb{x}_{1}^{A},y_{1}^{A}),...,(\\pmb{x}_{L}^{A},y_{L}^{A})$ ,   \n155 and let $W_{A,B}$ denote the state of our SRWM having observed the entire sequence.   \n156 ACL Meta-Training Objectives. The ACL meta-training objective function tasks the model to   \n157 correctly predict the test examples of all tasks learned so far at each task boundaries. That is, in the   \n158 case of two-task scenario described above (learning Task A then Task $\\mathbf{B}$ ), we use the weight matrix   \n159 $W_{A}$ to predict the label $y_{\\mathrm{test}}^{A}$ from input $(x_{\\mathrm{test}}^{A},\\mathcal{O})$ , and we use the weight matrix $W_{A,B}$ to predict the   \n160 label $y_{\\mathrm{test}}^{\\mathcal{B}}$ from input $(\\pmb{x}_{\\mathrm{test}}^{B},\\emptyset)$ as well as the label $y_{\\mathrm{test}}^{A}$ from input $(x_{\\mathrm{test}}^{A},\\mathcal{O})$ . By letting $p(y|\\pmb{x};\\pmb{W}_{*})$   \n161 denote the model\u2019s output probability for label $y\\in\\{1,..,N\\}$ given input $\\textbf{\\em x}$ and model weights/state   \n162 $W_{*}$ , the ACL objective can be expressed as: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{\\theta}{\\mathrm{minimize-}}\\left(\\log(p(y_{\\mathrm{test}}^{A}|x_{\\mathrm{test}}^{A};W_{A}))+\\log(p(y_{\\mathrm{test}}^{B}|x_{\\mathrm{test}}^{B};W_{A,B}))+\\log(p(y_{\\mathrm{test}}^{A}|x_{\\mathrm{test}}^{A};W_{A,B}))\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "163 for an arbitrary input meta-training sequence $\\big((\\pmb{x}_{1}^{A},y_{1}^{A}),...,(\\pmb{x}_{L}^{A},y_{L}^{A}),(\\pmb{x}_{1}^{B},y_{1}^{B}),...,(\\pmb{x}_{L}^{B},y_{L}^{B})\\big)$   \n164 (which is extensible to mini-batches with multiple such sequences), where $\\theta$ denotes the model   \n165 parameters (for the SRWM layer, it is the initial weights $W_{0}$ ). Figure 1 illustrates the overall   \n166 meta-training process of ACL.   \n167 The ACL objective function above (Eq. 4) is simple but encapsulates desiderata for continual learning   \n168 (Sec. 2.1). The last term of Eq. 4 with $p(y_{\\mathrm{test}}^{A}|x_{\\mathrm{test}}^{\\dot{A}};W_{A,B})$ or schematically $p({\\mathcal{A}}^{\\prime}|{\\mathcal{A}},{\\mathcal{B}})$ , optimizes   \n169 for backward transfer: (1) remembering the first task $\\mathbf{A}$ after learning (combatting catastrophic   \n117701 fsoercgoenttdi tnegr),m  aonf dE (q2. )4 l, p(ytBest|xtBest; oofr $\\mathbf{B}$ hteo miamtipcraolvlye $p(\\mathcal{B}^{\\prime}|\\mathcal{A},\\mathcal{B})$ c, eo potni mthizee sp afsotr twaasrkd  tAr.a nsTfheer   \n172 leveraging the past learning experience of to improve predictions in the second task B, in addition   \n173 to simply learning to solve Task $\\mathbf{B}$ from the corresponding training examples. To complete, the first   \n174 term of Eq. 4 is the single-task meta-learning objective for Task A.   \n175 Overall Model Architecture. As we mention in Sec. 2, in our NN architecture, the core sequential   \n176 dynamics of CL are learned by the self-referential layers. However, as an image-processing NN, our   \n177 model makes use of a vision backend. We use the \u201cConv- $.4^{\\circ}$ architecture [21] (typically used in the   \n178 context of few-shot learning) in all our experiments, except in the last one where we use a pre-trained   \n179 vision Transformer [76]. Overall, the model takes an image as input, process it through a feedforward   \n180 vision NN, whose output is fed to the SRWM-layer block. Note that this is one of the limitations of   \n181 this work: more general ACL should also learn to modify the vision components.2   \n182 Another crucial architectural choice that is specific to continual/multi-task image processing is   \n183 normalization layers (see also Bronskill et al. [78]). Typical NNs used in few-shot learning (e.g.,   \n184 Vinyals et al. [21]) contain batch normalization (BN; [79]) layers. All our models use instance   \n185 normalization (IN; [80]) instead of BN because in our preliminary experiments, we expectably found   \n186 IN to generalize much better than BN layers in the CL setting. ", "page_idx": 3}, {"type": "table", "img_path": "yaYJlpidX1/tmp/fcf44af90b68f8cc2cb2c272333ad46ace9f42e3bb543e33aaf3076b0b67ce1a.jpg", "table_caption": ["Table 1: 5-way classification accuracies using 15 (meta-test training) examples for each class in the context. Each row is a single model. Bold numbers highlight cases where in-context catastrophic forgetting is avoided through ACL. "], "table_footnote": [], "page_idx": 4}, {"type": "table", "img_path": "yaYJlpidX1/tmp/5e9a67789a68eaf93097abda9ff64064c0bf5ca127f744ec49a9400b766db659.jpg", "table_caption": ["Table 2: Similar to Table 1 above but using MNIST and CIFAR-10 (unseen domains) for meta-testing. "], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "187 4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "188 4.1 Two-Task Setting: Comprehensible Study ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "189 We first reveal the problem of \u201cin-context catastrophic forgetting\u201d and show how our ACL method   \n190 (Sec. 3) can overcome it. As a minimum setting for this, we focus on the two-task \u201cdomain", "page_idx": 4}, {"type": "image", "img_path": "yaYJlpidX1/tmp/2356e09919ca3e31b254340c81ece6095d3b0f9f905fc1ff385a8776d0e04b6e.jpg", "img_caption": ["(a) Case: Two tasks are learned simultaneously. (b) Case: One task is learned first (here Task A). "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 2: ACL/No-case meta-training curves displaying 6 individual meta-training loss terms, when the last term of the ACL objective (the backward tranfer loss; \u201cTask A ACL bwd\u201d and \u201cTask B ACL bwd\u201d in the legend) is not minimized (ACL/No case in Tables 1 and 2). Here Task A is Omniglot and Task B is Mini-ImageNet. We observe that, in both cases, without explicit minimization, backward transfer capability (purple and brown curves) of the learned learning algorithm gradually degrades as it learns to learn a new task (all other colors), causing in-context catastrophic forgetting. Note that blue/orange and green/red curve pairs almost overlap; indicating that when a task is learned, the model can learn it whether it is in the first or second segment of the continual learning sequence. ", "page_idx": 5}, {"type": "text", "text": "191 incremental\u201d CL setting (see Appendix A.1). We consider two meta-training task combinations:   \n192 Omniglot [23] and Mini-ImageNet [21, 22] or FC100 [24] (which is based on CIFAR100 [27]) and   \n193 Mini-ImageNet. The order of appearance of two tasks within meta-training sequences is alternated   \n194 for every batch. Appendix A.2 provides further details. We compare systems trained with or without   \n195 the backward transfer term in the ACL loss (the last term in Eq. 4).   \n196 Unless otherwise indicated (e.g, later for classic Split-MNIST; Sec. 4.3), all tasks are configured   \n197 to be a 5-way classification task. This is one of the classic configurations for few-shot learning tasks,   \n198 and also allows us to evaluate the principle of ACL with reasonable computational costs (like any   \n199 sequence learning-based meta-learning methods, scaling this to many more classes is challenging; we   \n200 also discuss this in Sec. 5). For standard datasets such as MNIST, we split the dataset into sub-datasets   \n201 of disjoint classes [81]: for example for MNIST which is originally a 10-way classification task, we   \n202 split it into two 5-way tasks, one consisting of images of class $\\surd0\\ '$ to $\\cdot_{4},$ (\u2018MNIST-04\u2019), and another   \n203 one made of class $\\varsigma_{5},$ to $\\cdot_{9},$ images (\u2018MNIST-59\u2019). When we refer to a dataset without specifying   \n204 the class range, we refer to the first sub-set. Unless stated otherwise, we concatenate 15 examples   \n205 from each class for each task in the context for both meta-training and meta-testing (resulting in   \n206 sequences of length 75 for each task). All images are resized to $32\\times32$ -size 3-channel images, and   \n207 normalized according to the original dataset statistics. We refer to Appendix A for further details.   \n208 Table 1 shows the results when the models are meta-tested on the test sets of the corresponding   \n209 few-shot learning datasets used for meta-training. We observe that for both pairs of meta-training   \n210 tasks, the models without the ACL loss catastrophically forget the first task after learning the second   \n211 one: the accuracy on the first task is at the chance level of about $20\\%$ for 5-way classification after   \n212 learning the second task in-context (see rows with \u201cACL No\u201d). The ACL loss clearly addresses this   \n213 problem: the ACL-learned CL algorithms preserve the performance of the first task. This effect is   \n214 particularly pronounced in the Omniglot/Mini-ImageNet case (involving two very different domains).   \n215 Table 2 shows evaluations of the same models but using two standard datasets, 5-way MNIST and   \n216 CIFAR-10, for meta-testing. Again, ACL-trained models better preserve the memory of the first   \n217 task after learning the second one. In the Omniglot/Mini-ImageNet case, we even observe certain   \n218 positive backward tranfer effects: in particular, in the \u201cMNIST-then-CIFAR10\u201d continual learning   \n219 case, the performance on MNIST noticeably improves after learning CIFAR10 (possibly leveraging   \n220 \u2018more data\u2019 provided in-context). ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "221 4.2 Analysis: Emergence of In-Context Catastrophic Forgetting ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "222 Now we closely look at the emergence of \u201cin-context catastrophic forgetting\u201d during meta-training   \n223 for the baseline models trained without the backward transfer term (the last/third term in Eq. 4) in   \n24 the ACL objective loss (corresponding to the ACL/No cases in Tables 1 and 2). We focus on the   \n25 Omniglot/Mini-ImageNet case, but similar trends can also be observed in the FC100/Mini-ImageNet   \n26 case. Figures 2a and 2b show two representative cases we typically observe. These figures show an   \n27 evolution of six individual meta-training loss terms (the lower the better), reported separately for   \n28 the cases where Task A (here Omniglot) or Task B (here Mini-ImageNet) appears at the first (1) or   \n29 second (2) position in the 2-task CL meta-training training sequences. 4 out of 6 curves correspond to   \n30 the learning progress, showing whether the model becomes capable of in-context learning the given   \n231 task (A or B) at the given position (1 or 2). The 2 remaining curves are the ACL backward tranfer   \n232 losses, also measured for Task A and B separately here.   \n233 Figure 2a shows the case where two tasks are learned about at the same time. We observe that when   \n234 the learning curves go down, the ACL losses go up, indicating that more the model learns, more it   \n235 tends to forget the task in-context learned previously. We also find this same trend when one task   \n236 is learned before the other one as is the case in Figure 2b. Here Task A alone is learned first; while   \n237 Task B is not learned, both learning and ACL curves go down for Task A (essentially, as the model   \n238 does not learn the second task, there is no force that encourages forgetting). After around 3000 steps,   \n239 the model also starts learning Task B. From this point, the ACL loss for Task A also starts to go   \n240 up, indicating again an opposing force effect between learning a new task and remembering a past   \n241 task. These observations clearly indicate that, without explicitly taking into account the backward   \n242 transfer loss as part of learning objectives, our gradient descent search tends to find solutions/CL   \n43 algorithms that prefer to erase previously learned knowledge (this is rather intuitive; it seems easier to   \n44 find such algorithms that ignore any influence of the current learning to past learning than those that   \n245 also preserve prior knowledge). In all cases, we find our ACL objective to be crucial for the learned   \n46 CL algorithms to be capable of remembering the old task while also learning the new one. ", "page_idx": 5}, {"type": "table", "img_path": "yaYJlpidX1/tmp/47884c911d45d17691fcab2aece74c068690e9d8a4217084eab68ba2e17064fc.jpg", "table_caption": ["Table 3: Classification accuracies $(\\%)$ on the Split-MNIST domain-incremental (DIL) and classincremental learning (CIL) settings [6]. Both tasks are 5-task CL problems. For the CIL case, we also report the 2-task case for which we can directly evaluate our out-of-the-box ACL meta-learner of Sec. 4.1 (trained with a 5-way output and the 2-task ACL loss) which, however, is not applicable (N.A.) to the 5-task CIL requiring a 10-way output. Mean/std over 10 training/meta-testing runs. No method here requires replay memory. See Appendix A.7 & B for further details and discussions. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "247 4.3 General Evaluation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "248 Evaluation on Standard Split-MNIST. Here we evaluate ACL on the standard Split-MNIST task in   \n249 domain-incremental and class-incremental settings [6, 28], and compare its performance to existing   \n250 CL and meta-CL algorithms (see Appendix A.7 for full references of these methods). Our comparison   \n251 focuses on methods that do not require replay memory. Table 3 shows the results. Since our   \n252 ACL-trained models are general-purpose learners, they can be directly evaluated (meta-tested) on   \n253 a new task, here Split-MNIST. The second-to-last row of Table 3, \u201cACL (Out-of-the-box model)\u201d,   \n254 corresponds to our model from Sec. 4.1 meta-trained on Omniglot and Mini-ImageNet using the   \n255 2-task ACL objective. It performs very competitively with the best existing methods in the domain  \n256 incremental setting, while it largely outperforms them (all but another meta-CL method, GeMCL) in   \n257 the 2-task class-incremental setting. The same model can be further meta-finetuned using the 5-task   \n258 version of the ACL loss (here we only used Omniglot as the meta-training data). The resulting model   \n259 (the last row of Table 3) outperforms all other methods in all settings studied here. Note that on   \n260 the \u2018in-domain\u2019 Omniglot test set, ACL and GeMCL perform similarly (see Appendix B.2/Table 9).   \n261 We are not aware of any existing hand-crafted CL algorithms that can achieve ACL\u2019s performance   \n262 without any replay memory. We refer to Appendix A.7/B for further discussions and ablation studies.   \n263 Evaluation on diverse task domains. Using the setting of Sec. 4.1, we also evaluate our ACL-trained   \n264 models for CL involving more tasks/domains; using meta-test sequences made of MNIST, CIFAR-10,   \n265 and Fashion MNIST. We also evaluate the impact of the number of tasks in the ACL objective: in   \n266 addition to the model meta-trained on Omniglot/Mini-ImageNet (Sec. 4.1), we also meta-train a model   \n267 (with the same architecture and hyper-parameters) using 3 tasks, Omniglot, Mini-ImageNet, and   \n268 FC100, using the 3-task ACL objective (see Appendix A.5); which is meta-trained not only on longer   \n269 CL sequences but also on more data. The full results of this experiment can be found in Appendix   \n270 B.4. We find that the two ACL-trained models are indeed capable of retaining the knowledge without   \n271 catastrophic forgetting for multiple tasks during meta-testing, while the performance on prior tasks   \n272 gradually degrades as the model learns new tasks, and performance on new tasks becomes moderate   \n273 (see also Sec. 5 on limitations). The 3-task version outperforms the 2-task one overall, encouragingly   \n274 indicating a potential for further improvements even with a fixed parameter count.   \n275 Going beyond: limitations and outlook. The experiments presented above effectively demonstrate   \n276 the possibility to encode a continual learning algorithm into self-referential weight matrices, that   \n277 outperforms handcrafted learning algorithms and existing metalearning approaches for CL. While   \n278 we consider this as an important result for metalearning and in-context learning in general, we note   \n279 that current state-of-the-art CL methods use neither regularization-based CL algorithms nor meta  \n280 continual learning methods we mention above, but the so-called learning to prompt (L2P)-family   \n281 of methods [33, 34] that leverage pre-trained models, namely a vision Transformer (ViT) pre-trained   \n282 on ImageNet [76]. A natural question we should ask is whether we could foresee ACL beyond the   \n283 scope considered so far, and evaluate it in such a setting. To study this, we take a pre-trained (frozen)   \n284 vision model, and add self-referential layers (to be meta-trained from scratch) on top of it to build a   \n285 continual learner. This allows us to highlight an important challenge of in-context CL in what follows.   \n286 We use two tasks from the L2P works above [33, 34]: 5-datasets [32] and Split-CIFAR-100, in the   \n287 class-incremental setting, but we focus on a \u201cmini\u201d versions thereof: we only use the two first classes   \n288 within each task (i.e., 2-way version) and for Split-CIFAR100, we only use the 5 first tasks; as we\u2019ll   \n289 see, this setting is enough to illustrate an important limitation of in-context CL. Again following   \n290 L2P [33, 34], we use ViT-B/16 [76] (available via PyTorch) as the pre-trained vision model, which   \n291 we keep frozen. We use the same configuration for the self-referential component from the Split  \n292 MNIST experiment. We meta-train the resulting model using Mini-ImageNet and Omniglot with the   \n293 5-task ACL loss. Table 4 shows the results. Even in this simple \u201cmini\u201d version of the tasks, ACL\u2019s   \n294 performance is far behind that of L2P methods. Notably, the frozen ImageNet-pre-trained features   \n295 with the meta-learner trained on Mini-ImageNet and Omniglot are not enough to perform well on the   \n296 5-th task of Split-CIFAR100, and SVHN and notMNIST of 5-datasets. This shows the necessity to   \n297 meta-train on more diverse tasks for in-context CL to be possibly successful in more general settings. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "yaYJlpidX1/tmp/873b68443585e745d99d64de5cb72c6898e6164fdcaaf808e06f89192ba7d960.jpg", "table_caption": ["Table 4: Experiments with \u201cmini\u201d Split-CIFAR100 and 5-datasets tasks. Meta-training is done using Mini-ImageNet and Omniglot. All meta-evaluation images are therefore from unseen domains. Numbers marked with \\* are reference numbers (evaluated in the more challenging, original version of these tasks) which can not be directly compared to ours. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "298 5 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "299 Other Limitations. In addition to the limitations already mentioned above, here we discuss others.   \n300 First of all, as an in-context/learned learning algorithm, there are challenges in terms of both domain   \n301 and length generalization (we qualitatively observe these to some extent in Sec. 4; further discussion   \n302 and experimental results are presented in Appendix B.3 & B.5). Regarding the length generalization,   \n303 we note that unlike the standard \u201cquadratic\" Transformers, linear Transformers/FWPs-like SRWMs   \n304 can be trained by carrying over states across two consecutive batches for arbitrarily long sequences.   \n305 Such an approach has been successfully applied to language modeling with FWPs [67]. This   \n306 possibility, however, has not been investigated here, and is left for future work. Also, directly scaling   \n307 ACL for real-world tasks requiring many more classes does not seem straightforward: it would   \n308 require very long training sequences. That said, it may be possible that ACL could be achieved   \n309 without exactly following the process we propose; as we discuss below for the case of LLMs, certain   \n310 real-world data may naturally give rise to an ACL-like objective. This work is also limited to the   \n311 task of image classification, which can be solved by feedforward NNs. Future work may investigate   \n312 the possibility to extend ACL to continual learning of sequence learning tasks, such as continually   \n313 learning new languages. Finally, ACL learns CL algorithms that are specific to the pre-specified   \n314 model architecture; more general meta-learning algorithms may aim at achieving learning algorithms   \n315 that are applicable to any model, as is the case for many classic learning algorithms.   \n316 Related work. There are several recent works that are catagorized as \u2018meta-continual learning\u2019 or   \n317 \u2018continual meta-learning\u2019 (see, e.g., [29, 30, 82\u201384, 51]). For example, Javed and White [29], Beaulieu   \n318 et al. [30] use \u201cmodel-agnostic meta-learning\u201d (MAML; [85, 86]) to meta-learn representations for   \n319 CL while still making use of classic learning algorithms for CL; this requires tuning of the learning   \n320 rate and number of iterations for optimal performance during CL at meta-test time (see, e.g., Appendix   \n321 A.7). In contrast, our approach learn learning algorithms in the spirit of Hochreiter et al. [14], Younger   \n322 et al. [15]; this may be categorized as \u2018in-context continual learning.\u2019 Several recent works (see, e.g.,   \n323 [87, 88]) mention the possibility of such in-context CL but existing works [19, 89, 90] that learn mul  \n324 tiple tasks sequentially in-context do not focus on catastrophic forgetting which is one of the central   \n325 challenges of CL. Here we show that in-context learning also suffers from catastrophic forgetting in   \n326 general (Sec. 4.1-4.2) and propose ACL to address this problem. We also note that the use of SRWM is   \n327 relevant to \u2018continual meta-learning\u2019 since with a regular sequence processor with slow weights, there   \n328 remains the question of how to continually learn the slow weights (meta-parameters). In principle, re  \n329 cursive self-modification as in SRWM is an answer to this question as it collapses such meta-levels into   \n330 single self-reference [12]. We also refer to [91\u201393] for other prior work on meta-continual learning.   \n331 Artificial v. Natural ACL in Large Language Models? Recently, \u201con-the-fly\u201d few-shot/meta   \n332 learning capability of sequence processing NNs has attracted broader interests in the context of large   \n333 language models (LLMs; [94]). In fact, the task of language modeling itself has a form of sequence   \n334 processing with error feedback (essential for meta-learning [95]): the correct label to be predicted is   \n335 fed to the model with a delay of one time step in an auto-regressive manner. Trained on a large amount   \n336 of text covering a wide variety of credit assignment paths, LLMs exhibit certain sequential few-shot   \n337 learning capabilities in practice [96]. This was rebranded as in-context learning, and has been the   \n338 subject of numerous recent studies (e.g., [97\u2013103, 71, 72]). Here we explicitly/artificially construct   \n339 ACL meta-training sequences and objectives, but in modern LLMs trained on a large amount of data   \n340 mixing a large diversity of dependencies using a large backpropagation span, it is conceivable that   \n341 some ACL-like objectives may naturally appear in the data. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "342 6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "343 Our Automated Continual Learning (ACL) trains sequence-processing self-referential neural networks   \n344 (SRNNs) to learn their own in-context continual (meta-)learning algorithms. ACL encodes classic   \n345 desiderata for continual learning (e.g., forward and backward transfer) into the objective function of   \n346 the meta-learner. ACL uses gradient descent to deal with classic challenges of CL, to automatically   \n347 discover CL algorithms with good behavior. Once trained, our SRNNs autonomously run their   \n348 own CL algorithms without requiring any human intervention. Our experiments reveal the original   \n349 problem of in-context catastrophic forgetting, and demonstrate the effectiveness of the proposed   \n350 approach to combat it. We demonstrate very promising results on the classic Split-MNIST benchmark   \n351 where existing hand-crafted algorithms fail, while also discussing its limitations in more general   \n352 scenarios. We believe this comprehensive study to be an important step for in-context CL research. ", "page_idx": 8}, {"type": "text", "text": "353 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "354 [1] David Eagleman. Livewired: The inside story of the ever-changing brain. 2020.   \n355 [2] Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks:   \n356 The sequential learning problem. In Psychology of learning and motivation, volume 24, pages   \n357 109\u2013165. 1989.   \n358 [3] Roger Ratcliff. Connectionist models of recognition memory: constraints imposed by learning   \n359 and forgetting functions. Psychological review, 97(2):285, 1990.   \n360 [4] Robert M French. Catastrophic forgetting in connectionist networks. Trends in cognitive   \n361 sciences, 3(4):128\u2013135, 1999.   \n362 [5] James L McClelland, Bruce L McNaughton, and Randall C O\u2019Reilly. Why there are comple  \n363 mentary learning systems in the hippocampus and neocortex: insights from the successes and   \n364 failures of connectionist models of learning and memory. Psychological review, 102(3):419,   \n365 1995.   \n366 [6] Yen-Chang Hsu, Yen-Cheng Liu, Anita Ramasamy, and Zsolt Kira. Re-evaluating continual   \n367 learning scenarios: A categorization and case for strong baselines. In NeurIPS Workshop on   \n368 Continual Learning, Montr\u00e9al, Canada, December 2018.   \n369 [7] Chris A Kortge. Episodic memory in connectionist networks. In 12th Annual Conference. CSS   \n370 Pod, pages 764\u2013771, 1990.   \n371 [8] Robert M French. Using semi-distributed representations to overcome catastrophic forgetting   \n372 in connectionist networks. In Proc. Cognitive science society conference, volume 1, pages   \n373 173\u2013178, 1991.   \n374 [9] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins,   \n375 Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska,   \n376 et al. Overcoming catastrophic forgetting in neural networks. Proc. National academy of   \n377 sciences, 114(13):3521\u20133526, 2017.   \n378 [10] Jonathan Schwarz, Wojciech Czarnecki, Jelena Luketina, Agnieszka Grabska-Barwinska,   \n379 Yee Whye Teh, Razvan Pascanu, and Raia Hadsell. Progress & compress: A scalable frame  \n380 work for continual learning. In Proc. Int. Conf. on Machine Learning (ICML), pages 4535\u2013   \n381 4544, Stockholm, Sweden, July 2018.   \n382 [11] Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic   \n383 intelligence. In Proc. Int. Conf. on Machine Learning (ICML), pages 3987\u20133995, Sydney,   \n384 Australia, August 2017.   \n385 [12] J\u00fcrgen Schmidhuber. Steps towards \u201cself-referential\u201d learning. Technical Report CU-CS-627-   \n386 92, Dept. of Comp. Sci., University of Colorado at Boulder, November 1992.   \n387 [13] J\u00fcrgen Schmidhuber. Evolutionary principles in self-referential learning, or on learning how   \n388 to learn: the meta-meta-... hook. PhD thesis, Technische Universit\u00e4t M\u00fcnchen, 1987.   \n389 [14] Sepp Hochreiter, A. Steven Younger, and Peter R. Conwell. Learning to learn using gradient   \n390 descent. In Proc. Int. Conf. on Artificial Neural Networks (ICANN), volume 2130, pages   \n391 87\u201394, Vienna, Austria, August 2001.   \n392 [15] A Steven Younger, Peter R Conwell, and Neil E Cotter. Fixed-weight on-line learning. IEEE   \n393 Transactions on Neural Networks, 10(2):272\u2013283, 1999.   \n394 [16] Neil E Cotter and Peter R Conwell. Learning algorithms and fixed dynamics. In Proc. Int.   \n395 Joint Conf. on Neural Networks (IJCNN), pages 799\u2013801, Seattle, WA, USA, July 1991.   \n396 [17] Neil E Cotter and Peter R Conwell. Fixed-weight networks can learn. In Proc. Int. Joint Conf.   \n397 on Neural Networks (IJCNN), pages 553\u2013559, San Diego, CA, USA, June 1990.   \n398 [18] Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive   \n399 meta-learner. In Int. Conf. on Learning Representations (ICLR), Vancouver, Cananda, 2018.   \n400 [19] Kazuki Irie, Imanol Schlag, R\u00f3bert Csord\u00e1s, and J\u00fcrgen Schmidhuber. A modern self  \n401 referential weight matrix that learns to modify itself. In Proc. Int. Conf. on Machine Learning   \n402 (ICML), pages 9660\u20139677, Baltimore, MA, USA, July 2022.   \n403 [20] Stephen T Grossberg. Studies of mind and brain: Neural principles of learning, perception,   \n404 development, cognition, and motor control. Springer, 1982.   \n405 [21] Oriol Vinyals, Charles Blundell, Tim Lillicrap, Koray Kavukcuoglu, and Daan Wierstra.   \n406 Matching networks for one shot learning. In Proc. Advances in Neural Information Processing   \n407 Systems (NIPS), pages 3630\u20133638, Barcelona, Spain, December 2016.   \n408 [22] Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In Int. Conf.   \n409 on Learning Representations (ICLR), Toulon, France, April 2017.   \n410 [23] Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept   \n411 learning through probabilistic program induction. Science, 350(6266):1332\u20131338, 2015.   \n412 [24] Boris N. Oreshkin, Pau Rodr\u00edguez L\u00f3pez, and Alexandre Lacoste. TADAM: task dependent   \n413 adaptive metric for improved few-shot learning. In Proc. Advances in Neural Information   \n414 Processing Systems (NeurIPS), pages 719\u2013729, Montr\u00e9al, Canada, December 2018.   \n415 [25] Yann LeCun, Corinna Cortes, and Christopher JC Burges. The MNIST database of handwritten   \n416 digits. URL http://yann. lecun. com/exdb/mnist, 1998.   \n417 [26] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: a novel image dataset for   \n418 benchmarking machine learning algorithms. Preprint arXiv:1708.07747, 2017.   \n419 [27] Alex Krizhevsky. Learning multiple layers of features from tiny images. Master\u2019s thesis,   \n420 Computer Science Department, University of Toronto, 2009.   \n421 [28] Gido M Van de Ven and Andreas S Tolias. Three scenarios for continual learning. In NeurIPS   \n422 Workshop on Continual Learning, Montr\u00e9al, Canada, December 2018.   \n423 [29] Khurram Javed and Martha White. Meta-learning representations for continual learning.   \n424 In Proc. Advances in Neural Information Processing Systems (NeurIPS), pages 1818\u20131828,   \n425 Vancouver, BC, Canada, December 2019.   \n426 [30] Shawn Beaulieu, Lapo Frati, Thomas Miconi, Joel Lehman, Kenneth O. Stanley, Jeff Clune,   \n427 and Nick Cheney. Learning to continually learn. In Proc. European Conf. on Artificial   \n428 Intelligence (ECAI), pages 992\u20131001, August 2020.   \n429 [31] Mohammadamin Banayeeanzade, Rasoul Mirzaiezadeh, Hosein Hasani, and Mahdieh So  \n430 leymani. Generative vs. discriminative: Rethinking the meta-continual learning. In Proc.   \n431 Advances in Neural Information Processing Systems (NeurIPS), pages 21592\u201321604, Virtual   \n432 only, December 2021.   \n433 [32] Sayna Ebrahimi, Franziska Meier, Roberto Calandra, Trevor Darrell, and Marcus Rohrbach.   \n434 Adversarial continual learning. In Proc. European Conf. on Computer Vision (ECCV), pages   \n435 386\u2013402, Glasgow, UK, August 2020.   \n436 [33] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su,   \n437 Vincent Perot, Jennifer G. Dy, and Tomas Pfister. Learning to prompt for continual learning.   \n438 In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pages 139\u2013149,   \n439 New Orleans, LA, USA, June 2022.   \n440 [34] Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun, Han Zhang, Chen-Yu Lee, Xiaoqi   \n441 Ren, Guolong Su, Vincent Perot, Jennifer G. Dy, and Tomas Pfister. Dualprompt: Complemen  \n442 tary prompting for rehearsal-free continual learning. In Proc. European Conf. on Computer   \n443 Vision (ECCV), pages 631\u2013648, Tel Aviv, Israel, October 2022.   \n444 [35] Sebastian Thrun. Lifelong learning algorithms. In Learning to learn, pages 181\u2013209. 1998.   \n445 [36] Rich Caruana. Multitask learning. Machine learning, 28:41\u201375, 1997.   \n446 [37] Mark B. Ring. Continual Learning in Reinforcement Environments. PhD thesis, University of   \n447 Texas at Austin, Austin, TX, USA, 1994.   \n448 [38] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick,   \n449 Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. Preprint   \n450 arXiv:1606.04671, 2016.   \n451 [39] Anthony Robins. Catastrophic forgetting, rehearsal and pseudorehearsal. Connection Science,   \n452 7(2):123\u2013146, 1995.   \n453 [40] Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep   \n454 generative replay. In Proc. Advances in Neural Information Processing Systems (NIPS), pages   \n455 2990\u20132999, Long Beach, CA, USA, December 2017.   \n456 [41] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy P. Lillicrap, and Gregory Wayne.   \n457 Experience replay for continual learning. In Proc. Advances in Neural Information Processing   \n458 Systems (NeurIPS), pages 348\u2013358, Vancouver, Canada, December 2019.   \n459 [42] Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, and   \n460 Gerald Tesauro. Learning to learn without forgetting by maximizing transfer and minimizing   \n461 interference. In Int. Conf. on Learning Representations (ICLR), New Orleans, LA, USA, May   \n462 2019.   \n463 [43] Yaqian Zhang, Bernhard Pfahringer, Eibe Frank, Albert Bifet, Nick Jin Sean Lim, and Yunzhe   \n464 Jia. A simple but strong baseline for online continual learning: Repeated augmented rehearsal.   \n465 In Proc. Advances in Neural Information Processing Systems (NeurIPS), New Orleans, LA,   \n466 USA, December 2022.   \n467 [44] David Lopez-Paz and Marc\u2019Aurelio Ranzato. Gradient episodic memory for continual learning.   \n468 In Proc. Advances in Neural Information Processing Systems (NIPS), pages 6467\u20136476, Long   \n469 Beach, CA, USA, December 2017.   \n470 [45] Tom Veniat, Ludovic Denoyer, and Marc\u2019Aurelio Ranzato. Efficient continual learning with   \n471 modular networks and task-driven priors. In Int. Conf. on Learning Representations (ICLR),   \n472 Virtual only, May 2021.   \n473 [46] Devang K Naik and Richard J Mammone. Meta-neural networks that learn by learning. In   \n474 Proc. International Joint Conference on Neural Networks (IJCNN), volume 1, pages 437\u2013442,   \n475 Baltimore, MD, USA, June 1992.   \n476 [47] Tom Bosc. Learning to learn neural networks. In NIPS Workshop on Reasoning, Attention,   \n477 Memory, Montreal, Canada, December 2015.   \n478 [48] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy P. Lillicrap.   \n479 Meta-learning with memory-augmented neural networks. In Proc. Int. Conf. on Machine   \n480 Learning (ICML), pages 1842\u20131850, New York City, NY, USA, June 2016.   \n481 [49] Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. RL2:   \n482 Fast reinforcement learning via slow reinforcement learning. Preprint arXiv:1611.02779,   \n483 2016.   \n484 [50] Jane Wang, Zeb Kurth-Nelson, Hubert Soyer, Joel Z. Leibo, Dhruva Tirumala, R\u00e9mi Munos,   \n485 Charles Blundell, Dharshan Kumaran, and Matt M. Botvinick. Learning to reinforcement   \n486 learn. In Proc. Annual Meeting of the Cognitive Science Society (CogSci), London, UK, July   \n487 2017.   \n488 [51] Tsendsuren Munkhdalai and Hong Yu. Meta networks. In Proc. Int. Conf. on Machine   \n489 Learning (ICML), pages 2554\u20132563, Sydney, Australia, August 2017.   \n490 [52] Tsendsuren Munkhdalai and Adam Trischler. Metalearning with Hebbian fast weights. Preprint   \n491 arXiv:1807.05076, 2018.   \n492 [53] Thomas Miconi, Kenneth Stanley, and Jeff Clune. Differentiable plasticity: training plastic   \n493 neural networks with backpropagation. In Proc. Int. Conf. on Machine Learning (ICML),   \n494 pages 3559\u20133568, Stockholm, Sweden, July 2018.   \n495 [54] Thomas Miconi, Aditya Rawal, Jeff Clune, and Kenneth O. Stanley. Backpropamine: training   \n496 self-modifying neural networks with differentiable neuromodulated plasticity. In Int. Conf. on   \n497 Learning Representations (ICLR), New Orleans, LA, USA, May 2019.   \n498 [55] Tsendsuren Munkhdalai, Alessandro Sordoni, Tong Wang, and Adam Trischler. Metalearned   \n499 neural memory. In Proc. Advances in Neural Information Processing Systems (NeurIPS),   \n500 pages 13310\u201313321, Vancouver, Canada, December 2019.   \n501 [56] Louis Kirsch and J\u00fcrgen Schmidhuber. Meta learning backpropagation and improving it. In   \n502 Proc. Advances in Neural Information Processing Systems (NeurIPS), pages 14122\u201314134,   \n503 Virtual only, December 2021.   \n504 [57] Mark Sandler, Max Vladymyrov, Andrey Zhmoginov, Nolan Miller, Tom Madams, Andrew   \n505 Jackson, and Blaise Ag\u00fcera y Arcas. Meta-learning bidirectional update rules. In Proc. Int.   \n506 Conf. on Machine Learning (ICML), pages 9288\u20139300, Virtual only, July 2021.   \n507 [58] Mike Huisman, Thomas M Moerland, Aske Plaat, and Jan N van Rijn. Are LSTMs good   \n508 few-shot learners? Machine Learning, pages 1\u201328, 2023.   \n509 [59] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,   \n510 \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proc. Advances in Neural   \n511 Information Processing Systems (NIPS), pages 5998\u20136008, Long Beach, CA, USA, December   \n512 2017.   \n513 [60] Kazuki Irie, R\u00f3bert Csord\u00e1s, and J\u00fcrgen Schmidhuber. Practical computational power of linear   \n514 transformers and their recurrent and self-referential extensions. In Proc. Conf. on Empirical   \n515 Methods in Natural Language Processing (EMNLP), Sentosa, Singapore, 2023.   \n516 [61] J\u00fcrgen Schmidhuber. A self-referential weight matrix. In Proc. Int. Conf. on Artificial Neural   \n517 Networks (ICANN), pages 446\u2013451, Amsterdam, Netherlands, September 1993.   \n518 [62] J\u00fcrgen Schmidhuber. Learning to control fast-weight memories: An alternative to recurrent   \n519 nets. Technical Report FKI-147-91, Institut f\u00fcr Informatik, Technische Universit\u00e4t M\u00fcnchen,   \n520 March 1991.   \n521 [63] J\u00fcrgen Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic   \n522 recurrent networks. Neural Computation, 4(1):131\u2013139, 1992.   \n523 [64] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers   \n524 are RNNs: Fast autoregressive transformers with linear attention. In Proc. Int. Conf. on   \n525 Machine Learning (ICML), Virtual only, July 2020.   \n526 [65] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane,   \n527 Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking   \n528 attention with performers. In Int. Conf. on Learning Representations (ICLR), Virtual only,   \n529 2021.   \n530 [66] Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A Smith, and Lingpeng   \n531 Kong. Random feature attention. In Int. Conf. on Learning Representations (ICLR), Virtual   \n532 only, 2021.   \n533 [67] Imanol Schlag, Kazuki Irie, and J\u00fcrgen Schmidhuber. Linear Transformers are secretly fast   \n534 weight programmers. In Proc. Int. Conf. on Machine Learning (ICML), Virtual only, July   \n535 2021.   \n536 [68] Kazuki Irie, Imanol Schlag, R\u00f3bert Csord\u00e1s, and J\u00fcrgen Schmidhuber. Going beyond linear   \n537 transformers with recurrent fast weight programmers. In Proc. Advances in Neural Information   \n538 Processing Systems (NeurIPS), Virtual only, December 2021.   \n539 [69] Kazuki Irie, R\u00f3bert Csord\u00e1s, and J\u00fcrgen Schmidhuber. The dual form of neural networks   \n540 revisited: Connecting test time predictions to training patterns via spotlights of attention. In   \n541 Proc. Int. Conf. on Machine Learning (ICML), Baltimore, MD, USA, July 2022.   \n542 [70] Mark A. Aizerman, Emmanuil M. Braverman, and Lev I. Rozonoer. Theoretical foundations   \n543 of potential function method in pattern recognition. Automation and Remote Control, 25(6):   \n544 917\u2013936, 1964.   \n545 [71] Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo\u00e3o Sacramento, Alexander   \n546 Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by   \n547 gradient descent. In Proc. Int. Conf. on Machine Learning (ICML), Honolulu, HI, USA, July   \n548 2023.   \n549 [72] Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei. Why can   \n550 GPT learn in-context? language models secretly perform gradient descent as meta-optimizers.   \n551 In Proc. Findings Association for Computational Linguistics (ACL), pages 4005\u20134019, Toronto,   \n552 Canada, July 2023.   \n553 [73] Bernard Widrow and Marcian E Hoff. Adaptive switching circuits. In Proc. IRE WESCON   \n554 Convention Record, pages 96\u2013104, Los Angeles, CA, USA, August 1960.   \n555 [74] Kazuki Irie, Francesco Faccio, and J\u00fcrgen Schmidhuber. Neural differential equations for   \n556 learning to program neural nets through continuous learning rules. In Proc. Advances in Neural   \n557 Information Processing Systems (NeurIPS), New Orleans, LA, USA, December 2022.   \n558 [75] Kazuki Irie and J\u00fcrgen Schmidhuber. Images as weight matrices: Sequential image generation   \n559 through synaptic learning rules. In Int. Conf. on Learning Representations (ICLR), Kigali,   \n560 Rwanda, May 2023.   \n561 [76] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,   \n562 Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,   \n563 Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image   \n564 recognition at scale. In Int. Conf. on Learning Representations (ICLR), Virtual only, May   \n565 2021.   \n566 [77] Ilya O. Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas   \n567 Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic,   \n568 and Alexey Dosovitskiy. MLP-Mixer: An all-MLP architecture for vision. In Proc. Advances   \n569 in Neural Information Processing Systems (NeurIPS), pages 24261\u201324272, Virtual only,   \n570 December 2021.   \n571 [78] John Bronskill, Jonathan Gordon, James Requeima, Sebastian Nowozin, and Richard E. Turner.   \n572 TaskNorm: Rethinking batch normalization for meta-learning. In Proc. Int. Conf. on Machine   \n573 Learning (ICML), pages 1153\u20131164, Virtual only, 2020.   \n574 [79] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training   \n575 by reducing internal covariate shift. In Proc. Int. Conf. on Machine Learning (ICML), pages   \n576 448\u2013456, Lille, France, July 2015.   \n577 [80] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing   \n578 ingredient for fast stylization. Preprint arXiv:1607.08022, 2016.   \n579 [81] Rupesh Kumar Srivastava, Jonathan Masci, Sohrob Kazerounian, Faustino J. Gomez, and J\u00fcr  \n580 gen Schmidhuber. Compete to compute. In Proc. Advances in Neural Information Processing   \n581 Systems (NIPS), pages 2310\u20132318, Lake Tahoe, NV, USA, December 2013.   \n582 [82] Massimo Caccia, Pau Rodr\u00edguez, Oleksiy Ostapenko, Fabrice Normandin, Min Lin, Lucas   \n583 Page-Caccia, Issam Hadj Laradji, Irina Rish, Alexandre Lacoste, David V\u00e1zquez, and Laurent   \n584 Charlin. Online fast adaptation and knowledge accumulation (OSAKA): a new approach to   \n585 continual learning. In Proc. Advances in Neural Information Processing Systems (NeurIPS),   \n586 Virtual only, December 2020.   \n587 [83] Xu He, Jakub Sygnowski, Alexandre Galashov, Andrei A Rusu, Yee Whye Teh, and Razvan   \n588 Pascanu. Task agnostic continual learning via meta learning. Preprint arXiv:1906.05201,   \n589 2019.   \n590 [84] Pau Ching Yap, Hippolyt Ritter, and David Barber. Addressing catastrophic forgetting in   \n591 few-shot problems. In Proc. Int. Conf. on Machine Learning (ICML), pages 11909\u201311919,   \n592 Virtual only, July 2021.   \n593 [85] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast   \n594 adaptation of deep networks. In Proc. Int. Conf. on Machine Learning (ICML), pages 1126\u2013   \n595 1135, Sydney, Australia, August 2017.   \n596 [86] Chelsea Finn and Sergey Levine. Meta-learning and universality: Deep representations   \n597 and gradient descent can approximate any learning algorithm. In Int. Conf. on Learning   \n598 Representations (ICLR), Vancouver, Canada, April 2018.   \n599 [87] Kazuki Irie and J\u00fcrgen Schmidhuber. Accelerating neural self-improvement via bootstrapping.   \n600 In ICLR Workshop on Mathematical and Empirical Understanding of Foundation Models,   \n601 Kigali, Rwanda, May 2023.   \n602 [88] Johannes von Oswald, Eyvind Niklasson, Maximilian Schlegel, Seijin Kobayashi, Nicolas   \n603 Zucchet, Nino Scherrer, Nolan Miller, Mark Sandler, Max Vladymyrov, Razvan Pascanu, et al.   \n604 Uncovering mesa-optimization algorithms in Transformers. Preprint arXiv:2309.05858, 2023.   \n605 [89] Julian Coda-Forno, Marcel Binz, Zeynep Akata, Matthew Botvinick, Jane X Wang, and Eric   \n606 Schulz. Meta-in-context learning in large language models. Preprint arXiv:2305.12907, 2023.   \n607 [90] Soochan Lee, Jaehyeon Son, and Gunhee Kim. Recasting continual learning as sequence   \n608 modeling. In Proc. Advances in Neural Information Processing Systems (NeurIPS), New   \n609 Orleans, LA, USA, December 2023.   \n610 [91] J\u00fcrgen Schmidhuber. On learning how to learn learning strategies. Technical Report FKI-198-   \n611 94, Institut f\u00fcr Informatik, Technische Universit\u00e4t M\u00fcnchen, November 1994.   \n612 [92] J\u00fcrgen Schmidhuber. Beyond \u201cgenetic programming\": Incremental self-improvement. In Proc.   \n613 Workshop on Genetic Programming at ML95, pages 42\u201349, 1995.   \n614 [93] J\u00fcrgen Schmidhuber, Jieyu Zhao, and Marco Wiering. Shifting inductive bias with success  \n615 story algorithm, adaptive Levin search, and incremental self-improvement. Machine Learning,   \n616 28(1):105\u2013130, 1997.   \n617 [94] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Lan  \n618 guage models are unsupervised multitask learners. [Online]. : https://blog.openai.com/better  \n619 language-models/, 2019.   \n620 [95] J\u00fcrgen Schmidhuber. Making the world differentiable: On using fully recurrent self-supervised   \n621 neural networks for dynamic reinforcement learning and planning in non-stationary environ  \n622 ments. Institut f\u00fcr Informatik, Technische Universit\u00e4t M\u00fcnchen. Technical Report FKI-126,   \n623 90, 1990.   \n624 [96] Tom B Brown et al. Language models are few-shot learners. In Proc. Advances in Neural   \n625 Information Processing Systems (NeurIPS), Virtual only, December 2020.   \n626 [97] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of   \n627 in-context learning as implicit bayesian inference. In Int. Conf. on Learning Representations   \n628 (ICLR), Virtual only, April 2022.   \n629 [98] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and   \n630 Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning   \n631 work? In Proc. Conf. on Empirical Methods in Natural Language Processing (EMNLP), pages   \n632 11048\u201311064, Abu Dhabi, UAE, December 2022.   \n633 [99] Kang Min Yoo, Junyeob Kim, Hyuhng Joon Kim, Hyunsoo Cho, Hwiyeol Jo, Sang-Woo   \n634 Lee, Sang-goo Lee, and Taeuk Kim. Ground-truth labels matter: A deeper look into input  \n635 label demonstrations. In Proc. Conf. on Empirical Methods in Natural Language Processing   \n636 (EMNLP), pages 2422\u20132437, Abu Dhabi, UAE, December 2022.   \n637 [100] Stephanie CY Chan, Adam Santoro, Andrew Kyle Lampinen, Jane X Wang, Aaditya K Singh,   \n638 Pierre Harvey Richemond, James McClelland, and Felix Hill. Data distributional properties   \n639 drive emergent in-context learning in transformers. In Proc. Advances in Neural Information   \n640 Processing Systems (NeurIPS), New Orleans, LA, USA, November 2022.   \n641 [101] Stephanie CY Chan, Ishita Dasgupta, Junkyung Kim, Dharshan Kumaran, Andrew K   \n642 Lampinen, and Felix Hill. Transformers generalize differently from information stored in   \n643 context vs in weights. In NeurIPS Workshop on Memory in Artificial and Real Intelligence   \n644 (MemARI), New Orleans, LA, USA, November 2022.   \n645 [102] Louis Kirsch, James Harrison, Jascha Sohl-Dickstein, and Luke Metz. General-purpose in  \n646 context learning by meta-learning transformers. In NeurIPS Workshop on Memory in Artificial   \n647 and Real Intelligence (MemARI), New Orleans, LA, USA, November 2022.   \n648 [103] Ekin Aky\u00fcrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning   \n649 algorithm is in-context learning? investigations with linear models. In Int. Conf. on Learning   \n650 Representations (ICLR), Kigali, Rwanda, May 2023.   \n651 [104] Gido M Van de Ven and Andreas S Tolias. Generative replay with feedback connections as a   \n652 general strategy for continual learning. Preprint arXiv:1809.10635, 2018.   \n653 [105] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Baolin Wu, Andrew Y Ng, et al.   \n654 Reading digits in natural images with unsupervised feature learning. In NIPS workshop on   \n655 deep learning and unsupervised feature learning, Granada, Spain, December 2011.   \n656 [106] Yaroslav Bulatov. Notmnist dataset. Google (Books/OCR), Tech. Rep.[Online]. Available:   \n657 http://yaroslavvb. blogspot. it/2011/09/notmnist-dataset. html, 2011.   \n658 [107] Tristan Deleu, Tobias W\u00fcrf,l Mandana Samiei, Joseph Paul Cohen, and Yoshua Bengio.   \n659 Torchmeta: A meta-learning library for PyTorch. Preprint arXiv:1909.06576, 2019.   \n660 [108] Jerry A Fodor and Zenon W Pylyshyn. Connectionism and cognitive architecture: A critical   \n661 analysis. Cognition, 28(1-2):3\u201371, 1988.   \n662 [109] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuyte  \n663 laars. Memory aware synapses: Learning what (not) to forget. In Proc. European Conf. on   \n664 Computer Vision (ECCV), pages 144\u2013161, Munich, Germany, September 2018.   \n665 [110] Zhizhong Li and Derek Hoiem. Learning without forgetting. In Proc. European Conf. on   \n666 Computer Vision (ECCV), pages 614\u2013629, Amsterdam, Netherlands, October 2016.   \n667 [111] Adam Paszke et al. Pytorch: An imperative style, high-performance deep learning library.   \n668 In Proc. Advances in Neural Information Processing Systems (NeurIPS), pages 8026\u20138037,   \n669 Vancouver, Canada, December 2019.   \n670 [112] R\u00f3bert Csord\u00e1s, Kazuki Irie, and J\u00fcrgen Schmidhuber. The devil is in the detail: Simple tricks   \n671 improve systematic generalization of transformers. In Proc. Conf. on Empirical Methods in   \n672 Natural Language Processing (EMNLP), Punta Cana, Dominican Republic, November 2021.   \n673 [113] Kazuki Irie, Imanol Schlag, R\u00f3bert Csord\u00e1s, and J\u00fcrgen Schmidhuber. Improving baselines in   \n674 the wild. In Workshop on Distribution Shifts, NeurIPS, Virtual only, 2021.   \n675 [114] James Requeima, Jonathan Gordon, John Bronskill, Sebastian Nowozin, and Richard E. Turner.   \n676 Fast and flexible multi-task classification using conditional neural adaptive processes. In Proc.   \n677 Advances in Neural Information Processing Systems (NeurIPS), pages 7957\u20137968, Vancouver,   \n678 Canada, December 2019.   \n679 [115] Eleni Triantaflilou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Utku Evci, Kelvin Xu, Ross   \n680 Goroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine Manzagol, and Hugo Larochelle.   \n681 Meta-dataset: A dataset of datasets for learning to learn from few examples. In Int. Conf. on   \n682 Learning Representations (ICLR), Addis Ababa, Ethiopia, April 2020.   \n683 [116] J\u00fcrgen Schmidhuber. One big net for everything. Preprint arXiv:1802.08864, 2018.   \n684 [117] Alex Graves, Marc G. Bellemare, Jacob Menick, R\u00e9mi Munos, and Koray Kavukcuoglu.   \n685 Automated curriculum learning for neural networks. In Proc. Int. Conf. on Machine Learning   \n686 (ICML), pages 1311\u20131320, Sydney, Australia, August 2017. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "687 A Experimental Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "688 A.1 Continual and Meta-learning Terminologies ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "689 We review the following classic terminologies of continual learning and meta-learning used through  \n690 out this paper.   \n691 Continual learning. \u201cDomain-incremental learning (DIL)\u201d and \u201cclass-incremental learning (CIL)\u201d   \n692 are two classic settings in continual learning [104, 28, 6]. They differ as follows. Let $M$ and $N$   \n693 denote positive integers. Consider continual learning of $M$ tasks where each task is an $N$ -way   \n694 classification. In the DIL case, a model has an $N$ -way output classification layer, i.e., the class $\\surd0\\ '$ of   \n695 the first task shares the same weights as the class $\\surd0^{\\circ}$ of the second task, and so on. In the CIL case, a   \n696 model\u2019s output dimension is $N*M$ ; the class indices of different tasks are not shared, neither are the   \n697 corresponding weights in the output layer. In our experiments, all CIL models have the $(N*M)$ -way   \n698 output from the first task (instead of progressively increasing the output size). In this work, we skip   \n699 the third variant called \u201ctask-incremental learning\u201d which assumes that we have access to the task   \n700 identity as an extra input, as it makes the CL problem almost trivial. CIL is typically reported to be   \n701 the hardest setting among them.   \n702 Meta-learning. We need to introduce \u201cmeta-training\u201d and \u201cmeta-test\u201d terminologie since each of   \n703 these phases involve \u201ctraining/test\u201d processes within itself. Each of them requires the corresponding   \n704 training and test examples. We refer to these as \u201cmeta-training training/test examples\u201d, and \u201cmeta-test   \n705 training/test examples\u201d following the terminology of Beaulieu et al. [30]. While these are rather   \n706 \u201cheavy\u201d terminologies, they are unambiguous and help avoid potential confusions. In both phases,   \n707 our sequence-processing neural net observes a sequence of (meta-training or meta-test) training   \n708 examples\u2014each consisting of input features and a correct label\u2014, and the resulting states of the   \n709 sequence processor (i.e., weights in the case of SRWM) are used to make predictions on (meta  \n710 training or meta-test) test examples\u2014input features presented to the model without its label. During   \n711 the meta-training phase, we modify the trainable parameters of the meta-learner through gradient   \n712 descent minimizing the meta-learning loss function (using backpropagation through time). During   \n713 meta-testing, no human-designed optimization for weight modification is used anymore; the SRWMs   \n714 modify their own weights following their own learning rules defined as their forward pass (Eqs. 1-3).   \n715 In connection with the now-popular in-context learning [96], we also refer to a (meta-training or   \n716 meta-test) training-example sequence as context. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "717 A.2 Datasets ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "718 For classic image classification datasets such as MNIST [25], CIFAR10 [27], and FashionMNIST   \n719 (FMNIST; Xiao et al. [26]) we refer to the original references for details.   \n720 For Omniglot [23], we use Vinyals et al. [21]\u2019s 1028/172/432-split for the train/validation/test set, as   \n721 well as their data augmentation methods using rotation of 90, 180, and 270 degrees. Original images   \n722 are grayscale hand-written characters from 50 different alphabets. There are 1632 different classes   \n723 with 20 examples for each class.   \n724 Mini-ImageNet contains color images from 100 classes with 600 examples for each class. We use the   \n725 standard train/valid/test class splits of 64/16/20 following [22].   \n726 FC100 is based on CIFAR100 [27]. 100 color image classes (600 images per class, each of size   \n727 $32\\times32)$ ) are split into train/valid/test classes of 60/20/20 [24].   \n728 The \u201c5-datasets\u201d dataset [32] consists of 5 datasets: CIFAR10, MNIST, FashionMNST, SVNH [105],   \n729 and notMNIST [106].   \n730 Split-CIFAR100 is also based on CIFAR100. The standard setting splits CIFAR100 into 10 10-way   \n731 classification tasks.   \n732 Meta-train/test sequence construction procedure. We use torchmeta [107] which provides   \n733 common few-shot/meta learning settings for these datasets to sample and construct their meta  \n734 train/test datasets. The construction of \u201cmeta-training training\u201d sequences for an $N$ -way classification,   \n735 using a dataset containing $C$ classes works as follows; for each sequence, we sample $N$ random   \n736 but distinct classes out of $C$ $\\langle N<C\\rangle$ ). The resulting classes are re-labelled such that each class is   \n737 assigned to one out of $N$ distinct random label index which is unique to the sequence. For each of   \n738 these $N$ classes, we sample $K$ examples. We randomly order these $N*K$ examples to obtain a   \n739 sequence. Each such a sequence \u201csimulates\u201d an unknown task the model has to learn. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "740 A.3 Training Details & Hyper-Parameters ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "741 We use the same model and training hyper-parameters in all our experiments. All hyper-parameters   \n742 are summarized in Table 5. We use the Adam optimizer with the standard Transformer learning rate   \n743 warmup scheduling [59]. The vision backend is the classic 4-layer convolutional NN of Vinyals   \n744 et al. [21]. Most configurations follow those of Irie et al. [19]; except that we initialize the \u2018query\u2019   \n745 sub-matrix in the self-referential\u221a weight matrix using a normal distribution with a\u221a mean value of 0   \n746 and standard deviation of $0.01/\\sqrt{d_{\\mathrm{head}}}$ while other sub-matrices use an std of $1/\\sqrt{d_{\\mathrm{head}}}$ (motivated   \n747 by the fact that a generated query vector is immediately multiplied with the same SRWM to produce   \n748 a value vector). For any further details, we\u2019ll refer the readers to our public code we\u2019ll release upon   \n749 acceptance. We conduct our experiments using a single V100-32GB, 2080-12GB or P100-16GB   \n750 GPUs, and the longest single training run takes about one day. ", "page_idx": 17}, {"type": "table", "img_path": "yaYJlpidX1/tmp/832b05e326943a33fd5997a022921a7140b7230d1d5d1f97ff9e817acb299d5d.jpg", "table_caption": ["Table 5: Hyper-parameters. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "751 A.4 Evaluation Procedure ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "752 For evaluation on few-shot learning datasets (i.e., Omniglot, Mini-Imagenet and FC100), we use 5   \n753 different sets consisting of $32\\,\\mathrm{K}$ random test episodes each, and report mean and standard deviation.   \n754 For evaluation on standard datasets, we use 5 different random support sets for in-context learning,   \n755 and evaluate on the entire test set. We report the corresponding mean and standard deviation across   \n756 these 5 evaluation runs.   \n757 For the Split-MNIST experiment, we do 10 meta-testing runs to compute the mean and standard   \n758 deviation as the baseline models are also trained for 10 runs in Hsu et al. [6] (see other details in   \n759 Appendix A.7). ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "760 A.5 ACL Objectives with More Tasks ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "761 We can straightforwardly extend the 2-task version of ACL presented in Sec. 3 to more tasks. In   \n762 the 3-task case (we denote the three tasks as A, B, and C) used in Sec. 4.3, the objective function   \n763 contains six terms. Following three terms are added to Eq. 4: ", "page_idx": 17}, {"type": "equation", "text": "$$\n-\\left(\\log(p(y_{\\mathrm{test}}^{C}|x_{\\mathrm{test}}^{C};W_{A,B,C}))+\\log(p(y_{\\mathrm{test}}^{B}|x_{\\mathrm{test}}^{B};W_{A,B,C}))+\\log(p(y_{\\mathrm{test}}^{A}|x_{\\mathrm{test}}^{A};W_{A,B,C}))\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "764 This also naturally extends to the 5-task loss used in the Split-MNIST experiment (Table 3). As   \n765 one can observe, the number of terms rapidly/quadratically increases with the number of tasks.   \n766 Nevertheless, computing these loss terms isn\u2019t immediately impractical because they essentially just   \n767 require forwarding the network for one step, for many independent inputs/images. This can be heavily   \n768 parallelized as a batch operation. While this can be a concern when scaling up more, a natural open   \n769 research question is whether we really need all these terms in the case we have many more tasks.   \n770 Ideally, we want these models to \u2018systematically generalize\u2019 to more tasks even when they are trained   \n771 with only a handful of them [108]. This is an interesting research question on generalization to be   \n772 studied in a future work. ", "page_idx": 17}, {"type": "table", "img_path": "yaYJlpidX1/tmp/74f69aa8f1008bbe7664224e1b53bf25ced49b28e8f3f34dcdbed40010c6b3d9.jpg", "table_caption": ["Table 6: Impact of the choice of meta-validation datasets. Classification accuracies $(\\%)$ on three datasets: Split-CIFAR-10, Split-Fashion MNIST (Split-FMNIST), and Split-MNIST in the domainincremental setting (we omit \u201cSplit-\u201d in the second column). \u201cOOB\u201d denotes \u201cout-of-the-box\u201d. \u201cmImageNet\u201d here refers to mini-ImageNet. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "773 A.6 Auxiliary 1-shot Learning Objective ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "774 In practice, instead of training the models only for \u201c15-shot learning,\u201d we also add an auxiliary loss   \n775 for 1-shot learning. This naturally encourages the models to learn in-context from the first examples. ", "page_idx": 18}, {"type": "text", "text": "776 A.7 Details of the Split-MNIST experiment ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "777 Here we provide details of the Split-MNIST experiments presented in Sec. 4 and Table 3. ", "page_idx": 18}, {"type": "text", "text": "778 Split-MNIST is obtained by transforming the classic 10-class single-task MNIST dataset into a   \n779 sequence of 5 tasks by partitioning the 10 classes into 5 groups/pairs of two classes each, in a fixed   \n780 order from 0 to 9 (i.e., grouping 0/1, 2/3, 4/5, 6/7, and 8/9). Regarding the difference between   \n781 domain/class-incremental settings, we refer to Appendix A.1.   \n782 The baseline methods presented in Table 3 include: standard SGD and Adam optimizers, Adam   \n783 with the L2 regularization, elastic weight consolidation [9] and its online variant [10], synaptic   \n784 intelligence [11], memory aware synapses [109], learning without forgetting (LwF; Li and Hoiem   \n785 [110]). For these methods, we directly take the numbers reported in Hsu et al. [6] for the 5-task   \n786 domain/class-incremental settings.   \n787 For the 2-task class incremental setting, we use Hsu et al. [6]\u2019s code to train the correspond models   \n788 (the number for LwF is currently missing as it is not implemented in their code base; we plan to add   \n789 the corresponding/missing entry in Table 3 for the final version of this paper).   \n790 Finally we also evaluate two meta-CL baselines: Online-aware Meta-Learning (OML; Javed and   \n791 White [29]) and Generative Meta-Continual Learning (GeMCL; Banayeeanzade et al. [31]). OML is   \n92 a MAML-based meta-learning approach. We note that as reported by Javed and White [29] in their   \n793 public code repository; after some critical bug fix, the performance of their OML matches that of   \n94 Beaulieu et al. [30] (which is a direct application of OML to another model architecture). Therefore,   \n95 we focus on OML as our main MAML-based baseline. We take the out-of-the-box model (meta  \n796 trained for Omniglot, with a 1000-way output) made publicly available by Javed and White [29]. We   \n97 evaluate the corresponding model in two ways. In the first, \u2018out-of-the-box\u2019 case, we take the meta  \n798 pre-trained model and only tune its meta-testing learning rate (which is done by Javed and White [29]   \n799 even for meta-testing in Omniglot). We find that this setting does not perform very well; in the other   \n800 case (\u2018optimized # meta-testing iterations\u2019), we additionally tune the number of meta-test training   \n801 iterations. We\u2019ve done a grid search of the meta-test learning rate in $3*\\{1e^{-2},1e^{-3},1e^{-4},1e^{-5}\\}$   \n802 and the number of meta-test training steps in $\\{1,2,5,8,10\\}$ using a meta-validation set based on an   \n803 MNIST validation set (5 K held-out images from the training set); we found the learning rate of $3e^{-4}$   \n804 and 8 steps to consistently perform the best in all our settings. We\u2019ve also tried it \u2018with\u2019 and \u2018without\u2019   \n805 the standard mean/std normalization of the MNIST dataset; better performance was achieved without   \n806 such normalization (which is in fact consistent as they do not normalize the Omniglot dataset for   \n807 their meta-training/testing). Their performance on the 5-task class-incremental setting is somewhat   \n808 surprising/disappointing (since genenralization from Omniglot to MNIST is typically straightforward,   \n809 at least, in common non-continual few-shot learning settings; see, e.g., Munkhdalai and Yu [51]). At   \n810 the same time, to the best of our knowledge, OML-trained models have not been tested in such a   \n811 condition in prior work; from what we observe, the publicly available out-of-the-box model might   \n812 be overtuned for Omniglot/Mini-ImageNet or the frozen \u2018representation network\u2019 is not ideal for   \n813 genenralization. We note that the sensitivity of these MAML-based methods [29, 30] w.r.t. meta-test   \n814 hyper-parameters has been also noted by Banayeeanzade et al. [31]; these are characteristics of   \n815 hand-crafted learning algorithms that we want to avoid with learned learning algorithms.   \n816 We use code and a pre-trained model (trained on Omniglot) made public by Banayeeanzade et al.   \n817 [31] for the GeMCL baseline (see also Table 7); like our method, GeMCL also do not require any   \n818 special tuning at test-time.   \n819 Our out-of-the-box ACL models (trained on Omniglot and Mini-ImageNet) do not require any   \n820 tuning at meta-test time. Nevertheless, we\u2019ve checked the effect of the number of meta-test training   \n821 examples (5 vs. 15; 15 is the number used in meta-training); we found the consistent number, i.e., 15,   \n822 to work better than 5. For the version that is meta-finetuned using the 5-task ACL objective (using   \n823 only the Omniglot dataset), we use 5 or 15 examples for both meta-train and meta-test training (see an   \n824 ablation study in Table 7). To obtain a sequence of 5 tasks, we simply sample 5 tasks from Omniglot   \n825 (in principle, we should make sure that different tasks in the same sequence have no class overlap;   \n826 in practice, our current implementation simply randomly draws 5 independent tasks from Omniglot). ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "table", "img_path": "yaYJlpidX1/tmp/19f2229e9b39132c056c2f6c7f2a80e5ba96607d00d86896ca761e12bfed9e28.jpg", "table_caption": ["Table 7: Impact of the number of in-context examples. Classification accuracies $(\\%)$ on Split-MNIST in the 2-task and 5-task class-incremental learning (CIL) settings and the 5-task domain-incremental learning (DIL) setting. For ACL models, we use the same number of examples for meta-validation as for meta-training. According to Banayeeanzade et al. [31], GeMCL is meta-trained with the 5-shot setting but meta-validated in the 15-shot setting. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "827 A.8 Details of the Split-CIFAR100 and 5-datasets experiment using ViT ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "828 As we described in Sec. 4, for the experiments on Split-CIFAR100 and 5-datasets, following   \n829 Wang et al. [33, 34], we use ViT-B/16 pre-trained on ImageNet [76] which is available through   \n830 torchvision [111]. In this experiments, we resize all images to $3\\mathrm{x}224\\mathrm{x}224$ and feed them to the   \n831 ViT. We remove the output layer of the ViT, and use its 768-dimensional feature from the penultimate   \n832 layer as the image encoding. The self-referential component which is added to this encoder has the   \n833 same architecture (2 layers, 16 heads) as the rest of the paper (see all hyper-parameters in Table 5)   \n834 All ViT parameters are frozen during meta-training. ", "page_idx": 19}, {"type": "text", "text": "835 B Extra Experimental Results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "836 B.1 Ablation Studies on the Meta-validation Dataset ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "837 Here we conduct ablation studies on the choice of meta-validation sets to select model checkpoints. In   \n838 general, when dealing with out-of-domain generalization, the choice of validation procedures to select   \n839 final model checkpoints plays a crucial role in the evaluation of the corresponding method [112, 113].   \n840 The out-of-the-box models are chosen based on the average meta-validation performance on the   \n841 validation set corresponding to the few-shot learning datasets used in meta-training: Omniglot and   \n842 mini-ImageNet (or Omniglot, mini-ImageNet, and FC100 in the case of 3-task ACL), independently of   \n843 any potential meta-test datasets. In contrast, in the meta-finetuning process of Table 3, we selected our   \n844 model checkpoint by meta-validation on the MNIST validation dataset (we held out $5\\,\\mathrm{K}$ images from   \n845 the training set). Here we evaluate ACL models meta-finetuned for the \u201c5-task domain-incremental   \n846 binary classification\u201d on three Split-\u2018X\u2019 tasks where \u2018X\u2019 is MNIST, FashionMNIST (FMNIST) or   \n847 CIFAR-10 for various choices of meta-validation sets (in each case we hold out 5 K images from   \n848 the corresponding training set). In addition, we also evaluate the effect of meta-finetuning datasets   \n849 (Omniglot only v. Omniglot and mini-ImageNet). Table 6 shows the results (we use 15 meta-training   \n850 and meta-testing examples except for the Omniglot-finedtuned/MNIST-validated model from Table 3   \n851 which happens to be configured with 5 examples; this will be fixed in the final version). Effectively,   \n852 meta-validation on the matching validation set is useful. Also, meta-finetuning only on Omniglot is   \n853 beneficial for the performance on MNIST when meta-validated on MNIST or FMNIST. However,   \n854 importantly, we emphasize that our ultimate goal is not to obtain a model that is specifically tuned for   \n855 certain datasets; we aim at building models that generally work well across a wide range of tasks   \n856 (ideally on any tasks); in fact, several existing works in the few-shot learning literature evaluate   \n857 their methods in such settings (see, e.g., Requeima et al. [114], Bronskill et al. [78], Triantafillou   \n858 et al. [115]). This also goes hand-in-hand with scaling up ACL (our current model is tiny; see   \n859 hyper-parameters in Table 5; the vision component is also a shallow \u2018Conv-4\u2019 net) and various other   \n860 considerations on self-improving continual learners (see, e.g., Schmidhuber [116]), such as automated   \n861 curriculum learning [117]. ", "page_idx": 19}, {"type": "table", "img_path": "yaYJlpidX1/tmp/323471ec9de3bc69b902de4ccdae1fa371ffcdaf169430843404765ce239b0a0.jpg", "table_caption": ["Table 8: Meta-testing on sequences that are longer than those from meta-training. Classification accuracies $(\\%)$ on 5-task Split-FMNIST and 5-task Split-MNIST in the domain-incremental settings. The model is the one finetuned with 5-task ACL loss using Omniglot as the meta-finetuning set and FMNIST as the meta-validation set (i.e., the numbers in the top part of the table are taken from Table 6). In the first column, \u201cSplit-FMNIST, Split-MNIST\u201d indicates continual learning of 5 Split-FMNIST tasks followed by 5 tasks of Split-MNIST (and \u201cSplit-MNIST, Split-FMNIST\u201d is the opposite order). Performance is measured at the end of the entire sequence. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "yaYJlpidX1/tmp/1ad562261efa84167aac03be8e66042bc377ab8b54efdb72ff17f2a67ef7b0af.jpg", "table_caption": ["Table 9: Classification acuracies $(\\%)$ on 5-task 2-way Split-Omniglot. Mean/std is computed over 10 meta-test runs. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "862 B.2 Performance on Split-Omniglot ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "863 Here we report the performance of the models used in the Split-MNIST experiment (Sec. 4.3) on   \n864 \u201cin-domain\u201d 5-task 2-way Split-Omniglot. Table 9 shows the result. Performance is very similar   \n865 between our ACL and the baseline GeMCL on this task in the class incremental setting, unlike on   \n866 Split-MNIST (Table 3) where we observe a larger performance gap between these same models. Here   \n867 we also include the \u201cdomain incremental\u201d setting for the sake of completeness but note that GeMCL   \n868 is not originally trained for this setting. ", "page_idx": 20}, {"type": "text", "text": "Table 10: 5-way classification accuracies using 15 examples for each class for each task in the context. 2-task models are meta-trained on Omniglot and Mini-ImageNet, while 3-task models are in addition meta-trained on FC100. \u2018A, B\u2019 in \u2018Context/Train\u2019 column indicates that models sequentially observe meta-test training examples of Task A then B; evaluation is only done at the end of the sequence. \u201cno ACL\u201d is the baseline 2-task models trained without the ACL loss. ", "page_idx": 21}, {"type": "table", "img_path": "yaYJlpidX1/tmp/4203966e1ef3f3d2eca22e3341341f0183c7b5104736d47d8c925ec28ffe9353.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "869 B.3 Effect of Number of In-Context Examples ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "870 Table 7 shows an ablation study on the number of examples used for meta-training and meta-testing   \n871 on the Split-MNIST task. We observe that for an ACL model trained only with 5 examples during   \n872 meta-training, more examples (15 examples) provided during meta-testing is not beneficial. In fact,   \n873 they even largely hurt in certain cases (see the last column); this is one form of \u201clength generalization\u201d   \n874 problem. When the number of meta-training examples is consistent with the one used during   \n875 meta-testing, the 15-example case consistently outperforms the 5-example one. ", "page_idx": 21}, {"type": "text", "text": "876 B.4 Effect of Number of Tasks in the ACL Loss ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "877 Table 10 provides the complete results discussed in Sec. 4.3 under \u201cEvaluation on diverse task   \n878 domains\u201d. ", "page_idx": 21}, {"type": "text", "text": "879 B.5 Further Discussion on Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "880 Here we provide further discussion and experimental results on the limitations of our approach as a   \n881 learned algorithm.   \n882 Domain generalization. As a data-driven learned algorithm, the domain generalization capability   \n883 is a typical limitation as it depends on the meta-trained data. Certain results we presented above   \n884 are representative of this limitation. In particular, in Table 6, the model meta-trained/finetuned on   \n885 Omniglot using Split-MNIST as meta-validation set do not perform well on Split-CIFAR10. While   \n886 meta-training and meta-validating on a larger/diverse set of datasets may be an immediate remedy   \n887 to obtain more robust ACL models, we note that since ACL is also a \u201ccontinual meta-learning\u201d   \n888 algorithm (Sec. 5), an ideal ACL model should also continually incorporate and learn from more data   \n889 during potentially lifelong meta-testing; we leave such an investigation for future work.   \n890 Length generalization. We already qualitatively observed the limited length generalization capabil  \n891 ity in Table 10 (meta-trained with up to 3 tasks and meta-tested with up to 4 tasks). Here we provide   \n892 one more experiment evaluating ACL models meta-trained for 5 tasks on a concatenation of two   \n893 5-task Split-MNIST and Split-FMNIST tasks (resulting in 10 tasks). Table 8 shows the results. Again,   \n894 while the model does not completely break, increasing the number of tasks to 10 rapidly degrades the   \n895 performance compared to the 5-task setting the model is meta-trained for. Similarly, its performance   \n896 on the Split-Omniglot domain incremental setting (Sec. B.2) degrades with increased numbers of   \n897 tasks: accuracies for 5, 10 and 20 tasks are $92.3\\%\\pm0.4$ , $82.0\\%\\pm0.4$ and $67.6\\%\\pm1.1$ respectively.   \n898 As noted in Sec. 5, this is a general limitation of sequence processing neural networks, and there is a   \n899 potential remedy for this limitation (meta-training on more tasks and \u201ccontext carry-over\u201d) which we   \n900 leave for future work. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "901 B.6 A Comment on Meta-Generalization ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "902 We also note that in general, \u201cunseen\u201d datasets do not necessarily imply that they are harder tasks than   \n903 \u201cin-domain\u201d test sets; when meta-trained on Omniglot and mini-ImageNet, meta-generalization on   \n904 \u201cunseen\u201d MNIST is easier (the accuracy is higher) than on the \u201cin-domain\u201d test set of mini-ImageNet   \n905 with heldout/unseen classes (compare Tables 1 and 2). ", "page_idx": 22}, {"type": "text", "text": "906 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "909 paper\u2019s contributions and scope?   \n910 Answer: [Yes]   \n911 Justification: We accurately state contributions and scope of the work in the abstract and   \n912 introduction.   \n913 Guidelines:   \n914 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n915 made in the paper.   \n916 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n917 contributions made in the paper and important assumptions and limitations. A No or   \n918 NA answer to this question will not be perceived well by the reviewers.   \n919 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n920 much the results can be expected to generalize to other settings.   \n921 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n922 are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "23 2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We discuss limitations of our method in Sec. 4 and 5. ", "page_idx": 23}, {"type": "text", "text": "27 Guidelines:   \n28 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n29 the paper has limitations, but those are not discussed in the paper.   \n30 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n31 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n32 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n3 model well-specification, asymptotic approximations only holding locally). The authors   \n34 should reflect on how these assumptions might be violated in practice and what the   \n5 implications would be.   \n36 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n7 only tested on a few datasets or with a few runs. In general, empirical results often   \n38 depend on implicit assumptions, which should be articulated.   \n39 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n0 For example, a facial recognition algorithm may perform poorly when image resolution   \nis low or images are taken in low lighting. Or a speech-to-text system might not be   \n42 used reliably to provide closed captions for online lectures because it fails to handle   \n43 technical jargon.   \n44 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n45 and how they scale with dataset size.   \n46 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n47 address problems of privacy and fairness.   \n48 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n49 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n50 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n51 judgment and recognize that individual actions in favor of transparency play an impor  \n2 tant role in developing norms that preserve the integrity of the community. Reviewers   \n53 will be specifically instructed to not penalize honesty concerning limitations.   \n3. Theory Assumptions and Proofs ", "page_idx": 23}, {"type": "text", "text": "954 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "955 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n956 a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "958 Justification: This is not a theoretical paper.   \n959 Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "970 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We provide experimental details in the main text and details in Appendix A.   \nWe also provide our code in the supplemental material. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes]   \nJustification: We provide experimental details in the main text and details in Appendix A. We also provide our code in the supplemental material. The data we use are classic datasets which are publicly available.   \n\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1013   \n1014   \n1015   \n1016   \n1017   \n1018   \n1019   \n1020   \n1021   \n1022   \n1023   \n1024   \n1025   \n1026   \n1027   \n1028   \n1029   \n1030   \n1031   \n1032   \n1033   \n1034   \n1035   \n1036   \n1037   \n1038   \n1039   \n1040   \n1041   \n1042   \n1043   \n1044   \n1045   \n1046   \n1047   \n1048   \n1049   \n1050   \n1051   \n1052   \n1053   \n1054   \n1055   \n1056   \n1057   \n1058   \n1059   \n1060   \n1061   \n1062   \n1063   \n1064 ", "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We provide experimental details in the main text and details in Appendix A.   \nWe also provide our code in the supplemental material. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: All our results are mean/std computed using 10 evaluation seeds. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) ", "page_idx": 25}, {"type": "text", "text": "1065 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n1066 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n1067 of the mean.   \n1068 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n1069 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n1070 of Normality of errors is not verified.   \n1071 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n1072 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n1073 error rates).   \n1074 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n1075 they were calculated and reference the corresponding figures or tables in the text.   \n1076 8. Experiments Compute Resources   \n1077 Question: For each experiment, does the paper provide sufficient information on the com  \n1078 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n1079 the experiments?   \n1080 Answer: [Yes]   \n1081 Justification: We provide compute resource related information in Appendix A.   \n1082 Guidelines:   \n1083 \u2022 The answer NA means that the paper does not include experiments.   \n1084 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n1085 or cloud provider, including relevant memory and storage.   \n1086 \u2022 The paper should provide the amount of compute required for each of the individual   \n1087 experimental runs as well as estimate the total compute.   \n1088 \u2022 The paper should disclose whether the full research project required more compute   \n1089 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n1090 didn\u2019t make it into the paper).   \n1091 9. Code Of Ethics   \n1092 Question: Does the research conducted in the paper conform, in every respect, with the   \n1093 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n1094 Answer: [NA]   \n1095 Justification: We do not have anything to report.   \n1096 Guidelines:   \n1097 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n1098 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n1099 deviation from the Code of Ethics.   \n1100 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n1101 eration due to laws or regulations in their jurisdiction).   \n1102 10. Broader Impacts   \n1103 Question: Does the paper discuss both potential positive societal impacts and negative   \n1104 societal impacts of the work performed?   \n1105 Answer: [NA]   \n1106 Justification: Our work does not have any such impacts.   \n1107 Guidelines:   \n1108 \u2022 The answer NA means that there is no societal impact of the work performed.   \n1109 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n1110 impact or why the paper does not address societal impact.   \n1111 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n1112 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n1113 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n1114 groups), privacy considerations, and security considerations.   \n1115 \u2022 The conference expects that many papers will be foundational research and not tied   \n1116 to particular applications, let alone deployments. However, if there is a direct path to   \n1117 any negative applications, the authors should point it out. For example, it is legitimate   \n1118 to point out that an improvement in the quality of generative models could be used to   \n1119 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n1120 that a generic algorithm for optimizing neural networks could enable people to train   \n1121 models that generate Deepfakes faster.   \n1122 \u2022 The authors should consider possible harms that could arise when the technology is   \n1123 being used as intended and functioning correctly, harms that could arise when the   \n1124 technology is being used as intended but gives incorrect results, and harms following   \n1125 from (intentional or unintentional) misuse of the technology.   \n1126 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n1127 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n1128 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n1129 feedback over time, improving the efficiency and accessibility of ML).   \n1130 11. Safeguards   \n1131 Question: Does the paper describe safeguards that have been put in place for responsible   \n1132 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n1133 image generators, or scraped datasets)?   \n1134 Answer: [NA]   \n1135 Justification: Our work does not imply any such risks.   \n1136 Guidelines:   \n1137 \u2022 The answer NA means that the paper poses no such risks.   \n1138 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n1139 necessary safeguards to allow for controlled use of the model, for example by requiring   \n1140 that users adhere to usage guidelines or restrictions to access the model or implementing   \n1141 safety filters.   \n1142 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n1143 should describe how they avoided releasing unsafe images.   \n1144 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n1145 not require this, but we encourage authors to take this into account and make a best   \n1146 faith effort.   \n1147 12. Licenses for existing assets   \n1148 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n1149 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n1150 properly respected?   \n1151 Answer: [Yes]   \n1152 Justification: Our codebase includes certain publicly available code. The corresponding   \n1153 license files are included in the supplemental material.   \n1154 Guidelines:   \n1155 \u2022 The answer NA means that the paper does not use existing assets.   \n1156 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n1157 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n1158 URL.   \n1159 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n1160 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n1161 service of that source should be provided.   \n1162 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n1163 package should be provided. For popular datasets, paperswithcode.com/datasets   \n1164 has curated licenses for some datasets. Their licensing guide can help determine the   \n1165 license of a dataset.   \n1166 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n1167 the derived asset (if it has changed) should be provided.   \n1168 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n1169 the asset\u2019s creators.   \n1170 13. New Assets   \n1171 Question: Are new assets introduced in the paper well documented and is the documentation   \n1172 provided alongside the assets?   \n1173 Answer: [Yes]   \n1174 Justification: The documentations of our code are included in the readme flie in the supple  \n1175 mental material.   \n1176 Guidelines:   \n1177 \u2022 The answer NA means that the paper does not release new assets.   \n1178 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n1179 submissions via structured templates. This includes details about training, license,   \n1180 limitations, etc.   \n1181 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n1182 asset is used.   \n1183 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n1184 create an anonymized URL or include an anonymized zip file.   \n1185 14. Crowdsourcing and Research with Human Subjects   \n1186 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n1187 include the full text of instructions given to participants and screenshots, if applicable, as   \n1188 well as details about compensation (if any)?   \n1189 Answer: [NA]   \n1190 Justification: We do not have such experiments.   \n1191 Guidelines:   \n1192 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n1193 human subjects.   \n1194 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n1195 tion of the paper involves human subjects, then as much detail as possible should be   \n1196 included in the main paper.   \n1197 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n1198 or other labor should be paid at least the minimum wage in the country of the data   \n1199 collector.   \n1200 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n1201 Subjects   \n1202 Question: Does the paper describe potential risks incurred by study participants, whether   \n1203 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n1204 approvals (or an equivalent approval/review based on the requirements of your country or   \n1205 institution) were obtained?   \n1206 Answer: [NA]   \n1207 Justification: We do not have such experiments.   \n1208 Guidelines:   \n1209 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n1210 human subjects.   \n1211 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n1212 may be required for any human subjects research. If you obtained IRB approval, you   \n1213 should clearly state this in the paper.   \n1214 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n1215 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n1216 guidelines for their institution.   \n1217 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n1218 applicable), such as the institution conducting the review. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}]