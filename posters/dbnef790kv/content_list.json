[{"type": "text", "text": "FUSE: Fast Unified Simulation and Estimation for PDEs ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Levi E. Lingsch Seminar for Applied Mathematics & AI Center ETH Zurich levi.lingsch@ai.ethz.ch ", "page_idx": 0}, {"type": "text", "text": "Dana Grund Institute for Atmospheric and Climate Science, ETH Zurich dana.grund@ethz.ch ", "page_idx": 0}, {"type": "text", "text": "Siddhartha Mishra Seminar for Applied Mathematics & AI Center ETH Zurich siddhartha.mishra@ethz.ch ", "page_idx": 0}, {"type": "text", "text": "Georgios Kissas   \nAI Center   \nETH Zurich   \ngkissas@ai.ethz.ch ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The joint prediction of continuous fields and statistical estimation of the underlying discrete parameters is a common problem for many physical systems, governed by PDEs. Hitherto, it has been separately addressed by employing operator learning surrogates for field prediction while using simulation-based inference (and its variants) for statistical parameter determination. Here, we argue that solving both problems within the same framework can lead to consistent gains in accuracy and robustness. To this end, We propose a novel and flexible formulation of the operator learning problem that allows jointly predicting continuous quantities and inferring distributions of discrete parameters, and thus amortizing the cost of both the inverse and the surrogate models to a joint pre-training step. We present the capabilities of the proposed methodology for predicting continuous and discrete biomarkers in full-body haemodynamics simulations under different levels of missing information. We also consider a test case for atmospheric large-eddy simulation of a two-dimensional dry cold bubble, where we infer both continuous time-series and information about the system conditions. We present comparisons against different baselines to showcase significantly increased accuracy in both the inverse and the surrogate tasks. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Partial Differential Equations (PDEs) describe the propagation of system conditions for a very wide range of physical systems. Parametric PDEs consider different system conditions as well as an underlying solution operator characterized by a set of finite-dimensional parameters. Traditional numerical methods based on different discretization schemes such as Finite Differences, Finite Volumes, and Finite Elements have been developed along with fast and parallelizable implementations to tackle complex problems, such as atmospheric modeling and cardiovascular biomechanics. For parametric PDEs, these methods define maps from the underlying set of discrete parameters, which describe the dynamics and the boundary/initial conditions, to physical quantities such as velocity or pressure that are continuous in the spatio-temporal domain. Despite their successful application, there still exist well-known drawbacks of traditional solvers. To describe a particular physical phenomenon, PDE parameters and solvers need to be calibrated on precise conditions that are not known a priori and cannot easily be measured in realistic applications. Therefore, iterative and thus expensive calibration procedures are considered in the cases where the parameters and conditions are inferred from data [2]. Even after the solvers are calibrated, an ensemble of solutions needs to be generated to account for uncertainties in the model parameters or assess the sensitivity of the solution to different parameters which are computationally prohibitive downstream tasks [43]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Related work A variety of deep learning algorithms have recently been proposed for scientific applications, broadly categorized into surrogate and inverse modeling algorithms, to either reduce the computational time of complex forward simulations or infer missing finite-dimensional information from data to calibrate a simulator to precise conditions. ", "page_idx": 1}, {"type": "text", "text": "Surrogate learning is a paradigm for accelerating computations. Hence, the cost of evaluating continuous quantities is amortized to an offline training stage. For functional data, so-called Neural Operators [28, 3], generalize across different conditions and discretizations of a complex system (resolution invariance). A multitude of operator learning approaches has been designed [37, 27, 50, 21, 44, 33, 32, 17], such as the Fourier Neural Operator (FNO) [31]. However, a priori, neural operators are not designed to process maps between finite-dimensional and continuous quantities. A different surrogate modeling strategy is constructed by learning a map between parameters of PDEs and solutions, as in traditional Reduced Order Model (ROM) approaches [16, 8, 19], and their deep learning counterparts [23, 10], including Generative Adversarial ROM (GAROM) [10]. In addition, Gaussian Processes and kriging have been re-visited in the context of PDE emulation [15, 61]. ", "page_idx": 1}, {"type": "text", "text": "Amortizing the cost of parameter inference has also been widely explored in the literature of generative modeling and Simulation Based Inference (SBI). In SBI, Invertible Neural Networks are combined with discrete Normalizing Flows to develop Neural Posterior Estimation [11, 12, 24, 45] or continuous Normalizing Flows in Flow Matching Posterior Estimation (FMPE) [60, 35]. However, they commonly rely on an expensive physical solver to sample continuous predictions, and are hindered by the use of approximate physical models, Gaussian distributions, or a limited sample size. ", "page_idx": 1}, {"type": "text", "text": "Approaches that attempt to jointly infer parameters and learn a surrogate are much rarer in the literature. Many are built on Variational Autoencoders (VAEs) [57], in finite [25] or infinite [51] dimensions. They inherently assume an underlying bijective relation between the continuous and discrete quantities or are restricted to Gaussian latent representations, which limits their applicability to nonlinear problems. Among them, InVAErt [57] extends a deterministic encoder-decoder pair with a variational latent encoder. However, InVAErt is primarily constructed for investigating identifiability of parameters in systems of PDEs, and may not be suitable for learning uncertainty propagation from parameters to continuous fields. Making use of the resolution invariance of neural operators, the conditional Variational Neural Operator (cVANO) [26], based on [51]. cVANO relies on a VAE to learn a low-dimensional manifold of the system constrained by finite-dimensional parameters, but is restricted to Gaussian approximations which are not suitable for problems which lack bijectivity. As alternative approaches, Simformer [14] estimates both finite-dimensional and function-valued parameters using transformers within the SBI framework, and PAGP [62] leverages the expressive power of Gaussian Processes. However, both are not suited for functional input due to quadratic growth with the input size. Finally, OpFlow [53] brings together operator learning and normalizing flows for quantifying uncertainty in output functions, but it lacks interpretability and inference on the latent space. ", "page_idx": 1}, {"type": "text", "text": "Our contribution Based on the experience and limitations of the aforementioned approaches, we formulate the following requirements for a unified forward-inverse framework relating functional quantities to a known space of finite-dimensional parameters: First, the interpretability given by the given parameterization should be retained in both the forward and inverse tasks as well as their combination (A). Then, both the forward and inverse model should be discretization invariant with respect to all functional variables (B). Finally, a general probabilistic representation should be chosen for the latent parameter space, allowing for arbitrary parameter distributions (C). Following these guidelines, the main contributions of our paper are the following. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose Fast Unified Simulation and Estimation for PDEs (FUSE), a rigorous framework for unified inverse and forward problems for parametric PDEs. The FUSE framework, illustrated in Figure 1, allows to combine different forward and inverse models, and we choose to instantiate it with the FNO and FMPE for the experiments in this work. \u2022 We formulate a mathematical framework with a unified objective for the forward and inverse problems, which allows us to assess how uncertainty in the inverse problem propagates through the forward problem (propagated uncertainty) and evaluate both models together to avoid nonlinear error amplification at model concatenation. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We showcase how to adapt forward surrogate and inverse estimation models to fti the FUSE framework, mapping between finite and infinite-dimensional spaces. In particular, we extend Fourier Neural Operators (FNO) with a custom lifting to take finite-dimensional inputs, and we equip Flow-Matching Posterior Estimation (FMPE) with an FNO-based encoding to allow for functional conditional information.   \n\u2022 We show that our implementation of FUSE overcomes struggles of baselines (cVANO, GAROM, InVAErt) and ablations (U-Net) on two complex and realistic PDE examples, pulse wave propagation (PWP) in the human arterial network and an atmospheric cold bubble (ACB). The experiments exemplify its ability for accurate and fast surrogate modeling, parameter inference, out-of-distribution generalization, and the flexibility to handle different levels of input information. ", "page_idx": 2}, {"type": "text", "text": "To our knowledge, the extensions we present to FNO and FMPE have not been explored in the literature yet. We would like to emphasize that we chose to base ourselves on these methods to illustrate the FUSE framework, but other forward and inverse methods may be more suitable for particular test cases. ", "page_idx": 2}, {"type": "image", "img_path": "dbnEf790Kv/tmp/d2e4733dc29c26b544376df552c1537306c2428bc5b93e2219df674ab0904bce.jpg", "img_caption": ["Figure 1: FUSE models a posterior distribution over finite-dimensional parameters $\\xi$ given infinitedimensional functions $u$ with $d_{u}$ components (channels). It learns other continuous functions $s$ with $d_{s}$ channels from parameters $\\xi$ . Band-limited Fourier transforms and a lifting operator act as a bridge between finite and infinite dimensions for the forward problem. Likewise, as inference models such as FMPE or NPE require fixed-size inputs, the operator layers are conjoined with a band-limited Fourier transform to learn a fixed-size representation of the input function. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "2 Methods ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notation and Assumptions Let $\\mathcal{U}\\,\\subset\\,\\mathcal{C}(X,\\mathbb{R}^{d_{u}})$ and $\\mathcal{S}\\,\\subset\\,\\mathcal{C}(Y,\\mathbb{R}^{d_{s}})$ be spaces of continuous functions on compact domains $X\\subseteq\\mathbb{R}^{d}$ and $Y\\subseteq\\mathbb{R}^{d^{\\prime}}$ , respectively, and let $\\Xi\\subseteq\\mathbb{R}^{m}$ be a space of finite-dimensional parameters. For a map $F:A\\rightarrow B$ , where we take $\\boldsymbol{\\mathcal{A}}$ and $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ to be among the spaces $\\mathcal{U},\\mathcal{S}$ , or $\\Xi$ , and a probability measure $\\pi\\in\\operatorname{Prob}({\\mathcal{A}})$ , we denote by $F_{\\#\\pi}(B)\\in{\\mathrm{Prob}}(B)$ the push-forward measure that expresses uncertainty on a set $B\\subseteq B$ by the propagation of $\\pi$ through $F$ , defined as $F_{\\#\\pi}(B)=\\pi(\\overdot{F^{-1}}(B))$ . Further, we assume all metrics on measures in this paper to be the total variation metric, and denote them by $d$ both on $\\mathrm{Prob}(S)$ and $\\operatorname{Prob}(\\Xi)$ . In general, we assume all maps to be Lipschitz continuous, and we omit Lipschitz constants in inequalities (details are provided in the Appendix). ", "page_idx": 2}, {"type": "text", "text": "Problem Formulation Consider the setting of a parametric PDE with forward solution mapping $\\mathcal{G}:\\xi\\mapsto s$ , where $\\xi\\in\\Xi$ is a vector of finite-dimensional parameters and $s\\in S$ is a function-valued output. Since it is the goal to calibrate the input parameters, we omit any functional inputs to $\\mathcal{G}$ , such as initial or boundary conditions, and assume they are kept constant or encoded in the parameters $\\xi$ . Given the forward operator $\\mathcal{G}$ , the forward model uncertainty is quantified by propagating a given distribution of parameters $\\rho\\in{\\mathrm{Prob}}(\\Xi)$ through the model, i.e. evaluating the push-forward measure $\\mathcal{G}_{\\#\\rho}$ . In practice, the parameters $\\xi$ are not available and need to be inferred from indirect measurements $u\\in\\mathcal{U}$ , which we assume to be functions in space or time (e.g., time series), and, in general, $u~\\ne~s$ . The extended solution operator $\\tilde{\\mathcal{G}}\\,:\\,u\\,\\stackrel{\\,-}{\\mapsto}\\,s$ then maps between functional measurements and functional model output, omitting the intermediate parameter space. Since the measurements $u$ are impacted by the uncertain parameters $\\xi$ , the function inputs $u$ are as well equipped with uncertainty and represented by the measure $\\mu\\in\\mathrm{Prob}(\\mathcal{U})$ . Finally, the inverse problem consists in estimating the distribution $\\rho(\\xi|u)$ of parameters given measurements $u$ . The corresponding uncertainty on the predicted outcomes $s$ is then given by the propagated uncertainty $\\mathscr{G}_{\\#\\rho(\\cdot|u)}$ . ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Unified Objective Given an approximate forward operator $\\tilde{\\mathcal{G}}^{\\theta}\\approx\\tilde{\\mathcal{G}}$ between function spaces, and an estimated distribution $\\mu^{\\phi}\\approx\\bar{\\mu}$ , parameterized by $\\theta$ and $\\phi$ , respectively, we use triangle inequality to observe that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{d(\\tilde{\\mathcal{G}}_{\\#\\mu^{\\phi}}^{\\theta},\\tilde{\\mathcal{G}}_{\\#\\mu})\\leq\\underbrace{d(\\tilde{\\mathcal{G}}_{\\#\\mu^{\\phi}}^{\\theta},\\tilde{\\mathcal{G}}_{\\#\\mu}^{\\theta})}_{\\mathrm{Measure\\;matching}}+\\underbrace{d(\\tilde{\\mathcal{G}}_{\\#\\mu}^{\\theta},\\tilde{\\mathcal{G}}_{\\#\\mu})}_{\\mathrm{Operator\\;learning}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Thus, we found a unified objective consisting of two steps corresponding to the two terms in the right hand side (rhs) of Equation (1). In the measure matching step, the objective amounts to learn an approximation $\\mu^{\\phi}$ of the measure $\\mu$ , whereas in the operator learning step, the objective is to learn a Neural Operator $\\mathcal{G}^{\\theta}$ that approximates the underlying ground truth operator $\\mathcal{G}$ . ", "page_idx": 3}, {"type": "text", "text": "Our reformulation of operator learning naturally ftis into the aim of this paper to propose a method that can act as an operator surrogate (forward problem, operator learning objective) as well as performing parameter inference by minimizing distances on measures (inverse problem, measure matching objective), unifying the apparently unrelated problems of surrogate modeling and inference. ", "page_idx": 3}, {"type": "text", "text": "Forward Problem: Operator Learning The operator $\\tilde{\\mathcal{G}}$ resembles the solution operator to a PDE that maps function inputs such as initial and boundary conditions, coefficients, sources etc to (observables of) the solution of the PDE. It is the goal of supervised operator learning [29] to learn this type of operator as a parameterized Neural Operator (NO) $\\tilde{\\mathcal{G}}^{\\theta}$ on a (training) distribution $\\mu\\in\\mathrm{Prob}(\\mathcal{U})$ by minimizing the operator learning objective in (1). This objective is bound by the supervised learning objective on the parameter space $\\Xi$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\nd(\\tilde{\\mathcal{G}}_{\\#\\mu}^{\\theta},\\tilde{\\mathcal{G}}_{\\#\\mu})\\leq d(\\mathcal{G}_{\\#\\rho}^{\\theta},\\mathcal{G}_{\\#\\rho}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and further by the corresponding objective (Appendix A.6), ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{1}(\\theta)=\\int_{\\Xi}\\|\\mathcal{G}^{\\theta}(\\xi)-\\mathcal{G}(\\xi)\\|_{L^{1}}d\\rho(\\xi).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The loss is approximated by training samples of the form $\\{\\xi^{i},\\mathcal{G}(\\xi^{i})\\}_{i=1}^{N}$ , sampled from an underlying data distribution $\\rho\\in\\mathrm{Prob}(\\Xi)$ . In order to use a NO on finite-dimensional inputs, we define the band-limited lifting $h^{\\theta_{3}}(\\xi)$ , composed of a lifting increasing the dimension of the parameters and an inverse Fourier transform. We then choose to instantiate the $\\mathcal{G}^{\\theta}$ based on Fourier Neural Operator (FNO) [31] layers $K^{\\theta_{2}}$ , implemented with discrete spectral evaluations as in [34] to handle irregularly sampled measurements for the output function, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{G}^{\\theta}(\\xi)=\\mathcal{Q}^{\\theta_{1}}\\circ K^{\\theta_{2}}\\circ h^{\\theta_{3}}(\\xi),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathcal{Q}^{\\theta_{1}}$ is a learnable map that projects the channels to the dimensions of the output function and $\\theta=[\\theta_{1},\\theta_{2},\\theta_{3}]$ are the trainable parameters. ", "page_idx": 3}, {"type": "text", "text": "Inverse Problem: Measure Matching We would like to approximate the true posterior measure $\\rho(\\xi|u)$ given measurements $u$ by $\\rho^{\\phi}(\\xi|\\breve{u})$ . This task is equivalent to minimizing the measure matching objective in (1) since (Appendix A.7) ", "page_idx": 3}, {"type": "equation", "text": "$$\nd(\\tilde{\\mathcal{G}}_{\\#\\mu^{\\phi}}^{\\theta},\\tilde{\\mathcal{G}}_{\\#\\mu}^{\\theta})\\leq d(\\rho^{\\phi}(\\xi|u),\\rho(\\xi|u)).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "To minimize the latter distance, we adapt Flow-Matching Posterior Estimation (FMPE) to handle function-valued conditional inputs $u$ . FMPE trains a flow function that maps samples from a standard normal distribution to those from the target distribution $\\rho(\\xi|u)$ . Unlike diffusion models [54, 20, 58], which require multiple steps, FMPE achieves this transformation in a single step, enabling faster training and evaluation. Both approaches are similar, but differ in their probability paths: diffusion models use a diffusion-based path, while FMPE employs an optimal-transport path, leading to greater regularity and improved performance in low-data settings. We provide an ablation using diffusion in Table 5, and examine FMPE\u2019s performance with varying data sizes in Table 6. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "The function input $u$ is handed to the FMPE flow parameterized by $v_{t,\\hat{u}}^{\\phi_{0}}(\\xi)$ as finite-dimensional conditional information $\\hat{u}$ . We encode the infinite-dimensional $u$ into $\\hat{u}=T^{\\phi_{1},\\phi_{2}}(u)$ as an adapted FNO, mimicking the forward surrogate. We project the functions to the parameters, using a bandlimited forward Fourier transform $\\tilde{h}$ to create a fixed-size latent representation of a function regardless of the input dimensions, subsequently projecting the large latent dimension to the smaller dimension $m$ of $\\xi$ by FMPE, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{u}=T^{\\phi_{1},\\phi_{2}}(u)=\\tilde{h}\\circ K^{\\phi_{1}}\\circ\\mathcal{P}^{\\phi_{2}}(u),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathcal{P}^{\\phi_{2}}$ is lifting layer, and $K^{\\phi_{1}}$ a concatenation of FNO layers. For the detailed formulation of the FMPE loss, we refer to the original reference [60] and equation (32) in Appendix A.8, and formulate it simply as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{2}(\\phi)=\\mathcal{L}^{F M P E}(\\phi),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\phi\\,=\\,[\\phi_{0},\\phi_{1},\\phi_{2}]$ are the joint trainable parameters of the flow itself and the FNO-based conditional information. The flow is then trained on data of the form $\\{(u_{i},\\xi_{i})\\}_{i=1}^{n}$ . ", "page_idx": 4}, {"type": "text", "text": "Unified Training and Evaluation The proposed methodology FUSE obtains the optimally trained parameters $\\theta$ and $\\phi$ by minimizing the respective objectives of Equations (3) and (7), ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{FUSE}}(\\theta,\\phi)=\\mathcal{L}_{\\mathrm{FUSE}}^{\\mathrm{forward}}(\\theta)+\\mathcal{L}_{\\mathrm{FUSE}}^{\\mathrm{inverse}}(\\phi)=\\mathcal{L}_{1}(\\theta)+\\mathcal{L}_{2}(\\phi),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "thus minimizing the joint objective (1). For all experiments presented here, the two FUSE objectives are decoupled during training, implying that for each training batch, ${\\mathcal{L}}_{1}$ and $\\mathcal{L}_{2}$ are calculated and backpropagation is performed separately over each set of network parameters. This strategy aids the training, allowing us to avoid hyperparameter tuning of the loss function. It is possible to backpropagate losses through the entire network as discussed in Appendix A.1; however, this does not result in improvements at evaluation time, further supporting our claim that decoupled objectives may be unified at evaluation time to draw connections between forward and inverse problems. ", "page_idx": 4}, {"type": "text", "text": "When employing the trained FUSE model, we assume that measurements $u$ are available and it is the goal to infer both the distribution of parameters $\\xi$ that best fti the data and the distribution of function outputs $s$ that would result from those parameters. We obtain $M$ parameter samples from the trained FMPE model as $\\xi_{i}\\sim\\rho^{\\phi}(\\xi|u)$ , $i=1,...,M$ , and pass these to the FNO forward surrogate to obtain $M$ samples of the simulated outputs $s_{i}=\\mathcal{G}^{\\theta}(\\xi_{i}),\\;i=1,...,M$ , where we assume $M$ sufficiently large to adequately sample the parameter distribution. We hence obtain an ensemble of estimates with mean prediction $\\begin{array}{r}{\\bar{s}={\\frac{1}{M}}\\sum_{i=1}^{M}s_{i}}\\end{array}$ and standard deviation $\\begin{array}{r}{\\sigma_{s}=(\\frac{1}{M-1}\\sum_{i=1}^{M}(s_{i}-\\bar{s})^{2})^{1/2}}\\end{array}$ . While both model parts can also be evaluated separately, it is this propagated uncertainty on $s$ that offers most insight in the effect of parametric uncertainty within the emulated models. ", "page_idx": 4}, {"type": "text", "text": "3 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "For evaluation, we are given testing data of the form $(u,\\xi^{*},s)$ obtained by numerical simulation, where we will refer to $\\xi^{*}$ as the data-generating parameters. Based on measurements $s^{*}$ , the inverse FMPE model provides samples of the distribution of discrete parameters $\\xi_{i}$ , that are compared to the true data-generating parameters $\\xi^{*}$ using the Continuous Ranked Probability Score (CRPS). The performance of the deterministic forward prediction is measured with the relative $L^{1}$ and $L^{2}$ errors over the continuous data and predictions. We compare FUSE against cVANO [26], inVAErt [57], and GAROM [10], and we consider an ablation where we substitute the FNO layers with UNet layers [47] in the FUSE model and leave out the Fourier transforms. For details on the training and evaluation metrics, please refer to A.2. ", "page_idx": 4}, {"type": "text", "text": "PDE Test Cases As test cases, we consider a simulation of the pulse wave propagation (PWP) in the human arterial network and one of an atmospheric cold bubble (ACB). Both experiments are described by a set of PDEs whose solutions are fully parameterized by finite-dimensional parameters encoding model properties, as well as boundary and initial conditions. In both of these experiments, to replicate a patient or downburst observed in the real world, the solver needs to be calibrated for precise conditions, e.g. a specific patient or atmospheric conditions, and forward uncertainty quantification of the calibrated predictions are of interest. Details on both cases are provided in Appendices A.3 and A.4. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Atmospheric Cold Bubble (ACB) In atmospheric modeling, the two-dimensional dry cold bubble case [56] is a well-known test case for numerical simulations resembling downbursts, which cause extreme surface winds in thunderstorms. Within a domain of neutral atmospheric stability without background wind, an elliptic cold air anomaly is prescribed and initiates a turbulent flow as it sinks to the ground, see Figure A.12 for an example. Time series of horizontal and vertical velocity, $u$ and $w$ , are recorded at eight sensors placed inside the domain. Similar data is obtained by weather stations $(z\\,\\approx\\,2\\textrm{m})$ or turbulence flux towers $(z\\approx10-50\\;\\mathrm{m})$ , experienced by wind turbines or high-rise buildings $(z\\,\\approx\\,100\\mathrm{~m})$ ), or with unmanned aerial vehicles. The PDE model PyCLES [42] used for simulation is parameterized by the turbulent eddy viscosity $\\nu_{t}$ and diffusivity $D_{t}$ as model parameters, as well as four parameters describing the amplitude and shape of the initial cold perturbation $(a,\\ x_{r},\\ z_{r},\\ z_{c})$ . The target task for FUSE here is to calibrate these parameters $\\xi\\in\\mathbb{R}^{6}$ to time series measurements $u:[0,\\bar{T}]\\rightarrow\\mathbb{R}^{20}$ , at ten locations for horizontal and vertical velocity each, where the input measurements used for calibration and the model output used for prediction and uncertainty quantification share the same space $\\mathcal{U}=S$ of time series. ", "page_idx": 5}, {"type": "text", "text": "Pulse Wave Propagation (PWP) in the Human Body The pulse wave propagation in the cardiovascular system contains a great deal of information regarding the health of an individual. For this reason, there are efforts to measure and leverage PWP in both wearable devices, e.g. smart-watches, and clinical medicine. Different systemic parameters such as stroke volume (SV), heart rate (HR), and patient age have been shown to affect the morphology of pulse waves [7]. These are used to parameterize pulse wave time series, measuring pressure, velocity, and photoplethysmography (PPG), at 13 locations of systemic arteries of the human cardiovascular system through a reduced-order PDE model in the data set published by [7]. See Tables 7 and 8 in Section A.3 for the full set of parameters and locations. In clinical applications, only measurements at easily accessible locations (such as the wrist) are available, while it is the goal to predict the pulse signal at other locations of interest from the inferred parameters $\\boldsymbol{\\xi}\\in\\mathbb{R}^{32}$ . The space of input functions $\\boldsymbol{\\mathcal{U}}$ hence differs from the space of output functions $\\boldsymbol{S}$ in this test case. In order to account for different clinical scenarios, we train the FUSE model using random masking of locations and evaluate it on different levels of available input information, ", "page_idx": 5}, {"type": "text", "text": "Level 1. Perfect information: Pressure, velocity, and PPG at all locations, Level 2. Intensive care unit information: Pressure, velocity, and PPG at the wrist, Level 3. Minimal information: PPG at the fingertip. ", "page_idx": 5}, {"type": "text", "text": "4 Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Collected results for all experiments are summarized in Table 1. While the main tasks of the FUSE model are evaluated for both test cases, we focus on ACB to exemplify its generalization properties and on PWP for different levels of input information. We would like to emphasize that the goal of uncertainty representation here is to capture parametric uncertainty inherent to the data-generating numerical model and its parameterizations. Our model does not provide a notion of uncertainty in the predictions due to imperfect training of the neural networks. Given that these errors are generally low (Table 1), we are confident to interpret all spread on $\\xi$ and $s$ given by FUSE as parametric model uncertainty. In particular, we expect the true posterior distributions to have positive spread, hence a prediction with small ensemble spread is not necessarily accurate. The CRPS is designed to judge model performance in this probabilistic setting. ", "page_idx": 5}, {"type": "text", "text": "Inverse Problem Given time series measurements $u$ , the inverse model samples from the distribution of parameters, $\\xi\\sim\\rho^{\\phi}(\\xi|u)$ , with more samples in areas of the parameter space that are more likely to match the data-generating parameters $\\xi^{*}$ . We find that FUSE outperforms the other methods when sufficient input information is given. We also found that inVAErt experiences posterior collapse for PWP in this setting, which is represented in high CRPS (Figure A.9). Histograms reveal that FUSE captures the expected dependencies of the data on the parameters (Figures A.1, A.2), such as wider distributions for scarcer information in PWP. In the ACB case, sharp Dirac-like estimates are obtained for most of the samples (Figure A.14). However, a good fit for the numerical model parameters seems to rely on an accurate estimate of the initial condition (Figure A.16). ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Forward Problem The surrogate $\\mathcal G^{\\theta}(\\xi)$ is tasked to assess the ability of FUSE to predict the continuous $s$ from the data-generating parameters $\\xi^{*}$ . FUSE consistantly outperforms all other methods in both test cases. In particular, FUSE is able to reproduce very nonlinear dynamics and sharp velocity gradients in the ACB case (Figure A.15). Potential errors mostly lie in the bulk velocity of the anomaly, resulting in a stretching or squeezing of the time series at later times. As expected, predictions are slightly worse further away from the perturbation location, where the flow has become more turbulent, especially for the vertical velocity which exhibits strong oscillations (Figure A.21a). For the PWP, continuous predictions based on the true parameters have an L1 error of well below $0.3\\%$ at all locations (Figures A.6, A.7). Smaller arteries generally exhibit a slightly higher error variance, which ftis the physical problem interpretation, as smaller diameters show more variations to pressure than larger system arteries. ", "page_idx": 6}, {"type": "text", "text": "Unified Uncertainty Propagation Given an input $u$ , the concatenated evaluation of the inverse and forward parts of FUSE provides a distribution over the output functions $s$ , by first sampling parameters $\\xi_{i}$ and then evaluating the forward emulator on this ensemble, yielding continuous samples $s_{i}$ . This can be interpreted as the uncertainty introduced by simulating the data with the parameterized model FUSE was trained on: Uncertainty is introduced as the parameters are not fully identifiable in the inverse step, and is propagated and expanded onto the continuous output through the ensemble predictions on the parameter samples. Again, FUSE outperforms all other models significantly in all experiments in the $L^{1}$ and $L^{2}$ error over the mean predictions s\u00af. Figures 2 and 3 show how FUSE captures both smooth and rough structures, even on the sample it performs worst on. It only exhibits slight difficulties in predicting a constant velocity close to zero for ACB. At each location of the ACB, the $L^{1}$ error obtained with the propagated ensemble mean $\\bar{s}$ is comparable to the error by evaluating only the forward model, $s=\\mathcal{G}^{\\sharp}(\\breve{\\xi}^{*})$ (Figure A.21b). An inspection of the predictions for all locations for the PWP case shows that the true time series lie within one standard deviation of the FUSE ensemble mean for the median sample, and within the min-max uncertainty bounds for the worst sample (Figures A.3, A.4). We observe that the accurate prediction of model parameters correlates to accurate predictions of pressures (Figure A.11). ", "page_idx": 6}, {"type": "image", "img_path": "dbnEf790Kv/tmp/8f71e7407ed9f5be1a6c5c2a65fc555fce277c384c7a52411a3da3c934f3c792.jpg", "img_caption": ["(a) FUSE (b) UNet (c) InVAErt (d) cVANO (e) GAROM\\* "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 2: ACB, propagated uncertainty: Comparison of the benchmark models on the horizontal velocities at location 1, showing the unified ensemble predictions for the sample where FUSE has the greatest $L^{1}$ error, i.e. a worst-case scenario. \\*GAROM predicts the velocity from the true parameters, while all other models predict pressure from the posterior distribution over parameters given continuous inputs. ", "page_idx": 6}, {"type": "image", "img_path": "dbnEf790Kv/tmp/48c152853d9670adc7b6b63e02132a8f6f590d28005075ba40666ee636dfba3a.jpg", "img_caption": ["(a) FUSE (b) UNet (c) InVAErt (d) cVANO (e) GAROM\\* "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 3: PWP, Comparison of models ability to predict the Sup. Middle Cerebral pressure and its uncertainty for the sample where FUSE has the greatest $L^{1}$ error, i.e. a worst-case scenario. \\*see Figure 2. ", "page_idx": 6}, {"type": "text", "text": "Sensitivity Analysis Based on the good approximative properties showcased above, the forward operator $\\dot{\\mathcal{G}^{\\theta}}$ of FUSE can be used as a fast surrogate model to assess the sensitivity of the underlying PDE to varying the parameters $\\xi$ . Such analysis is usually constrained to a small sample size due to the large computational costs, and the dense sampling enabled by the surrogate helps to explore the parameter space for parameters that optimally fit patient data in PWP, or exhibit extreme winds in ACB. Varying one parameter at a time, while keeping all others fixed at their default value, exhibits the single effect on the data (fingerprint). For ACB, an increased amplitude of the cold anomaly mainly results in a speed-up and hence a squeezing of the time series, while other parameters have more nonlinear effects (Figure A.17). As the FUSE model provides no access to the full velocity fields, only numerical simulations reveal that these sharp sensitivities are mainly caused by certain features (eddies) of the flow reaching or missing a sensor depending on the parameter value. A validation of this sensitivity analysis for an estimate of the maximal horizontal velocity over the time series is provided in Figure 4 for a pairwise fingerprint on $100\\times100$ samples, with consistently low errors. Only for one sample with a weak and small perturbation, the signal is not captured accurately. An additional evaluation of 60 samples at the margins of the parameter space shows that the peak velocities are consistently represented well (Figures A.20, A.19). For PWP, the sensitivity results (Figure A.10) are consistent with observations reported in the literature [7]. ", "page_idx": 7}, {"type": "image", "img_path": "dbnEf790Kv/tmp/524c0eada1dd6bc9e8cf14131be9f8b3005cc7b1be7c169bdc8d1eddd9fa03ae.jpg", "img_caption": ["Figure 4: ACB, sensitivity analysis and generalization: Validation of the FUSE model against numeric simulations on peak horizontal velocities $u$ at location 1 $x=15\\:\\mathrm{km}$ , $z=50\\;\\mathrm{m}$ ). Samples above the dashed line correspond to amplitudes larger than seen during training. The parameter resolution is $100\\times100$ for FUSE, and $(10\\bar{\\phantom{+}}+10)\\times\\bar{10}$ for the numerical samples. Figure continued in the appendix, Figure A.18. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Out of Distribution Generalization for ACB We test the generalization capabilities of the FUSE forward model on test samples with amplitudes larger than seen in training in the pairwise sensitivity setup (Figure 4). The FUSE model is able to capture the shape of the pairwise parameter dependencies, but struggles to locate the sharp dependence of the local peak velocity on the eddy diffusivity. Averaged errors in the out of distribution range are shown in Table 2, with FUSE performing clearly superior to all other models. ", "page_idx": 7}, {"type": "text", "text": "Levels of Available Information for PWP The case of missing input information is modeled by masking certain input components when evaluating the models. While FUSE shows regular performance comparable with the baselines on levels 2 and 3 of missing information, cVANO seems to be better suited for this setting (Table 1), probably due to the assumption that the latent dimensions are uncorrelated. In a case without conditional information, FUSE provides the prior distribution of the data (Figure A.8). When the available input measurement locations of the PWP experiment are known a priori, it is possible to get more accurate results when training FUSE without masking. For example, training to predict pressures and parameters from only the PPG data at the fingertip only, without masking, results in a CRPS of $4.28^{'}\\!\\times10^{-2}$ and a relative $L^{1}$ error of $3.6\\%$ , roughly half the error of the masked model. If the measurement data is fixed, predictions without masking are much more suitable for analysis. ", "page_idx": 7}, {"type": "text", "text": "5 Discussion ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Summary We propose FUSE, a framework for unifying surrogate modeling and parameter identification for parametric PDEs by unifying, as an example, operator learning with flow-matching posterior estimation. Both are represented in the FUSE objective through a deterministic forward loss and a probabilistic inverse loss, respectively. The joint architecture allows for inverse estimation of finite-dimensional parameters $\\xi$ given continuous measurements $u$ , as well as forward predictions of (other) continuous measurements $s$ given $\\xi$ . Inheriting from the properties of its constitutive components, FUSE is discretization invariant with respect to the continuous inputs and outputs, and able to represent arbitrary distributions in the fully interpretable parameter space. As this latent space is defined through a known parameterization of the underlying PDE used for numerical modeling, FUSE allows for accurate calibration and evaluation of a numerical solver, as well as performing downstream tasks such as parameter uncertainty quantification. On two test cases of parametric PDEs, namely the pulse wave propagation (PWP) in the human cardiovascular system and an atmospheric cold bubble (ACB) with time series measurements, FUSE consistently outperforms four baseline methods designed to perform similar tasks. It also shows good out-of-distribution generalization and great flexibility when trained and evaluated on different levels of available input information. ", "page_idx": 7}, {"type": "table", "img_path": "dbnEf790Kv/tmp/0ff22d151883216cca4a0d727d6466b48af5cd7e22a58f86039bd65d7c0390ef.jpg", "table_caption": ["Table 1: Performance of FUSE and the baseline models in estimating parameters $\\xi$ from continuous inputs $u$ , quantified by CRPS, and predicting time series data $s$ , quantified by a relative $L_{1}$ and $L_{2}$ error. Here, \"True parameters\" evaluates the forward model part only, and \"Estimated parameters\" and levels one to three evaluates the sample mean $\\bar{s}$ predicted by the unified model. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "dbnEf790Kv/tmp/0ba61df0d350c288d37f266212759bbd3a8f5b091cecacd6737ef88e4bcf3885.jpg", "table_caption": ["Table 2: Performance of FUSE on out-of-distribution samples for ACB, as described in Figure 4, averaged over the samples, the velocity components $u$ and $w$ , and the measurement locations. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Connection to Existing Methods The proposed methodology and the adapted implementation of the FNO and FMPE offer the following advantages over the existing methods for joint parameter estimation and forward emulation mentioned in the introduction. By incorporating Neural Operators in both the forward and inverse parts, we ensure discretization invariance throughout the model [3]. In contrast to Variational Autoencoders, we present a fully probabilistic formulation and require no restriction on the latent data distribution. This allows us to model arbitrary posterior distributions, whereas cVANO is restricted to Gaussian distributions. Likewise, FUSE allows one to leverage powerful, existing neural operators which improves predictions on the output functions. The experiments have shown that FUSE excels over all baselines in both test cases. In particular, cVANO and GAROM fail to capture even rough characteristics of the flow in the forward emulation for the ACB case with nonlinear measurements, and InVAErt shows a consistent bias towards the mean in the PWP task. Only in the case of missing information in the smoother PWP case, cVANO outperforms FUSE on the inverse problem. In terms of the choice of FNO in both the forward and inverse part of FUSE, the ablation with UNet layers exhibits larger uncertainties and less performance of the predicted mean. An additional ablation, presented in the Appendix 5, shows that optimal transport probability paths perform better than the diffusion-based paths in a conditional DDPM model. Finally, Gaussian processes are often chosen for their ability to handle unstructured data. We incorporate this property by using the grid-independent FNO implementation by [34]. ", "page_idx": 9}, {"type": "text", "text": "Applicability FUSE is explicitly formulated for finite-dimensional parameters and function measurements and outputs. This setting naturally arises when calibrating a numerical solver, with training data generated synthetically by the solver itself, and was demonstrated with out two test cases. However, it is very common for real measured datasets e.g. in bioengineering [22], and more specifically when involving PWP, to contain both time-series data and vectors of parameters available for different patients. FUSE naturally extends to these datasets, where an underlying PDE is assumed but not formulated or simulated explicitly, including parameter inference using real data [59], precision medicine or solver calibration [46], and fingerprinting to discover parameter-disease correlations [55]. In the case that the parameters are non-physical, and hence not measurable, such as the numerical model parameters in the ACB case, this does of course not apply, and the main aim of solving the forward and inverse problem is to explore the solver itself. Moreover, FUSE may help to refine existing parameterizations either through disentanglement [18] or by identifying parameters with little influence on the simulated data. ", "page_idx": 9}, {"type": "text", "text": "Limitations and Future Work The FUSE framework aims at emulating a given parameterization of a PDE. If such a parameterization (or a complete dataset) is not available, VAE-based pre-processing [4] or manifold discovery [5] may be used. When applying FUSE to real measurements, the parameterization turns to be only an approximation of the dynamical system, whereas it is considered a perfect model in the experiments presented here. This structural model uncertainty has to be added to the interpretation of the unified uncertainties given by the FUSE model, as it has to be for any other inverse algorithm. In practice, measurements are always associated with a measurement error, which is considerable in particular for extreme measurements. The current implementation of FUSE does not allow for errors in the continuous inputs to be taken into account, and it is left for a future extension of the framework to include these into a fully Bayesian formulation of the inverse model. In two or three-dimensional applications, such as atmospheric modeling, the parameters of interest might be themselves space/time-dependent functions, such as surface properties or initial conditions [48]. Based on the scaling properties of the FMPE model for high-dimensional data such as images [35], FUSE is expected to scale well in size for high-dimensional parameter spaces, but is yet to be extended to a resolution-invariant function representation in latent space. All these limitations will be addressed in future research. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "G.K. would like to acknowledge support from Asuera Stiftung via the ETH Zurich Foundation. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] J. Alastruey, K. H. Parker, S. J. Sherwin, et al. Arterial pulse wave haemodynamics. In 11th international conference on pressure surges, volume 30, pages 401\u2013443. Virtual PiE Led t/a BHR Group Lisbon, Portugal, 2012. [2] A. Arzani, J.-X. Wang, M. S. Sacks, and S. C. Shadden. Machine learning for cardiovascular biomechanics modeling: challenges and beyond. Annals of Biomedical Engineering, 50(6): 615\u2013627, 2022. doi:10.1007/s10439-022-02967-4.   \n[3] F. Bartolucci, E. de Bezenac, B. Raonic, R. Molinaro, S. Mishra, and R. Alaifari. Representation equivalent neural operators: a framework for alias-free operator learning. Advances in Neural Information Processing Systems, 36, 2024. URL https://papers.nips.cc/paper_files/ paper/2023/hash/dc35c593e61f6df62db541b976d09dcf-Abstract-Conference. html.   \n[4] P. Bojanowski, A. Joulin, D. Lopez-Pas, and A. Szlam. Optimizing the latent space of generative networks. In Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 600\u2013609. PMLR, 10\u201315 Jul 2018. URL https://proceedings.mlr.press/v80/bojanowski18a.html.   \n[5] J. Brehmer and K. Cranmer. Flows for simultaneous manifold learning and density estimation. In Advances in Neural Information Processing Systems (NeurIPS), 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 051928341be67dcba03f0e04104d9047-Abstract.html.   \n[6] S. C\u02c7anic\u00b4, J. Tambac\u02c7a, G. Guidoboni, A. Mikelic\u00b4, C. J. Hartley, and D. Rosenstrauch. Modeling viscoelastic behavior of arterial walls and their interaction with pulsatile blood flow. SIAM Journal on Applied Mathematics, 67(1):164\u2013193, 2006. doi:10.1137/060651562.   \n[7] P. H. Charlton, J. Mariscal Harana, S. Vennin, Y. Li, P. Chowienczyk, and J. Alastruey. Modeling arterial pulse waves in healthy aging: a database for in silico evaluation of hemodynamics and pulse wave indexes. American Journal of Physiology-Heart and Circulatory Physiology, 317 (5):H1062\u2013H1085, 2019. doi:10.1152/ajpheart.00218.2019.   \n[8] P. Y. Chen, J. Xiang, D. H. Cho, Y. Chang, G. A. Pershing, H. T. Maia, M. M. Chiaramonte, K. T. Carlberg, and E. Grinspun. CROM: Continuous reduced-order modeling of PDEs using implicit neural representations. In The Eleventh International Conference on Learning Representations, 2023. URL https://iclr.cc/virtual/2023/poster/12094.   \n[9] E. Cleary, A. Garbuno-Inigo, S. Lan, T. Schneider, and A. M. Stuart. Calibrate, emulate, sample. Journal of Computational Physics, 424:109716, 2021. ISSN 0021- 9991. doi:10.1016/j.jcp.2020.109716. URL https://www.sciencedirect.com/science/ article/pii/S0021999120304903.   \n[10] D. Coscia, N. Demo, and G. Rozza. Generative adversarial reduced order modelling. Sci Rep, 14(3826), 2024. doi:10.1038/s41598-024-54067-z.   \n[11] L. Dinh, D. Krueger, and Y. Bengio. Nice: Non-linear independent components estimation. In International Conference on Learning Representations, 2015. URL https://arxiv.org/ abs/1410.8516.   \n[12] L. Dinh, J. Sohl-Dickstein, and S. Bengio. Density estimation using real NVP. In The Fifth International Conference on Learning Representations, 2017. URL https://openreview. net/forum?id $=$ HkpbnH9lx.   \n[13] L. Formaggia, A. Quarteroni, and A. Veneziani. Cardiovascular Mathematics: Modeling and simulation of the circulatory system, volume 1. Springer Science & Business Media, 2010.   \n[14] M. Gloeckler, M. Deistler, C. D. Weilbach, F. Wood, and J. H. Macke. All-in-one simulationbased inference. In Proceedings of the 41st International Conference on Machine Learning, volume 235, pages 15735\u201315766. PMLR, 21\u201327 Jul 2024. URL https://proceedings.mlr. press/v235/gloeckler24a.html.   \n[15] M. Gulian, A. Frankel, and L. Swiler. Gaussian process regression constrained by boundary value problems. Computer Methods in Applied Mechanics and Engineering, 388:114117, 2022. doi:https://doi.org/10.1016/j.cma.2021.114117.   \n[16] R. Halder, M. Damodaran, and B. C. Khoo. Deep learning based reduced order model for airfoil-gust and aeroelastic interaction. AIAA Journal, 58(4):1595\u20131606, 8 2020. doi:10.2514/1.J059027.   \n[17] Z. Hao, Z. Wang, H. Su, C. Ying, Y. Dong, S. Liu, Z. Cheng, J. Song, and J. Zhu. GNOT: A general neural operator transformer for operator learning. In Proceedings of the 40th International Conference on Machine Learning, volume 202, pages 12556\u201312569. PMLR, 23\u201329 Jul 2023. URL https://proceedings.mlr.press/v202/hao23c.html.   \n[18] I. Higgins, L. Matthey, A. Pal, C. P. Burgess, X. Glorot, M. M. Botvinick, S. Mohamed, and A. Lerchner. beta-VAE: Learning basic visual concepts with a constrained variational framework. In The Fifth International Conference on Learning Representations, volume 3, 2017. URL https://openreview.net/forum?id $=$ Sy2fzU9gl.   \n[19] S. Hijazi, M. Freitag, and N. Landwehr. POD-galerkin reduced order models and physicsinformed neural networks for solving inverse problems for the navier\u2013stokes equations. Adv. Model. and Simul. in Eng. Sci., 10(5):5, 03 2023. doi:10.1186/s40323-023-00242-2.   \n[20] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, volume 33, pages 6840\u20136851. Curran Associates, Inc., 2020. URL https://papers.nips.cc/paper_files/paper/2020/hash/ 4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html.   \n[21] P. Jin, S. Meng, and L. Lu. MIONet: Learning multiple-input operators via tensor product. SIAM Journal on Scientific Computing, 44(6):A3490\u2013A3514, 2022. doi:10.1137/22M1477751.   \n[22] A. E. W. Johnson, T. J. Pollard, L. Shen, H. L. Li-wei, M. Feng, M. Ghassemi, B. Moody, P. Szolovits, L. A. Celi, and R. G. Mark. MIMIC-III, a freely accessible critical care database. Scientific data, 3:160035, 2016. doi:10.1038/sdata.2016.35.   \n[23] B. Kim, V. C. Azevedo, N. Thuerey, T. Kim, M. Gross, and B. Solenthaler. Deep fluids: A generative network for parameterized fluid simulations. Computer graphics forum, 38(2):59\u201370, 2019. doi:10.1111/cgf.13619.   \n[24] D. P. Kingma and P. Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URL https://papers.nips.cc/paper_files/paper/2018/hash/ d139db6a236200b21cc7f752979132d0-Abstract.html.   \n[25] D. P. Kingma and M. Welling. Auto-encoding variational bayes. In International Conference on Learning Representations, 2013. URL https://arxiv.org/abs/1312.6114.   \n[26] G. Kissas. Towards Digital Twins for Cardiovascular Flows: A Hybrid Machine Learning and Computational Fluid Dynamics Approach. PhD thesis, University of Pennsylvania, 2023.   \n[27] G. Kissas, J. H. Seidman, L. F. Guilhoto, V. M. Preciado, G. J. Pappas, and P. Perdikaris. Learning operators with coupled attention. J. Mach. Learn. Res., 23(1), Jan. 2022. URL https://dl.acm.org/doi/10.5555/3586589.3586804.   \n[28] N. Kovachki, Z. Li, B. Liu, K. Azizzadenesheli, K. Bhattacharya, A. Stuart, and A. Anandkumar. Neural operator: Learning maps between function spaces with applications to pdes. Journal of Machine Learning Research, 24(89):1\u201397, 2023. URL http://jmlr.org/papers/v24/ 21-1524.html.   \n[29] S. Lanthaler, S. Mishra, and G. E. Karniadakis. Error estimates for DeepONets: A deep learning framework in infinite dimensions. Transactions of Mathematics and Its Applications, 6(1): tnac001, 2022. doi:10.1093/imatrm/tnac001.   \n[30] M. Letafati, S. Ali, and M. Latva-aho. Conditional denoising diffusion probabilistic models for data reconstruction enhancement in wireless communications, 2024. URL https://arxiv. org/abs/2310.19460.   \n[31] Z. Li, N. Kovachki, K. Azizzadenesheli, B. Liu, K. Bhattacharya, A. Stuart, and A. Anandkumar. Fourier neural operator for parametric partial differential equations. In International Conference on Learning Representations, volume 2010.08895, 2021. URL https://iclr.cc/virtual/ 2021/poster/3281.   \n[32] Z. Li, K. Meidani, and A. B. Farimani. Transformer for partial differential equations\u2019 operator learning. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https: //openreview.net/forum?id $\\cdot=$ EPPqt3uERT.   \n[33] Z. Li, D. Shu, and A. Barati Farimani. Scalable transformer for pde surrogate modeling. In Advances in Neural Information Processing Systems, volume 36, pages 28010\u201328039. Curran Associates, Inc., 2023. URL https://papers.nips.cc/paper_files/paper/2023/ hash/590daf74f99ee85df3d8c007df9c8187-Abstract-Conference.html.   \n[34] L. E. Lingsch, M. Y. Michelis, E. De Bezenac, S. M. Perera, R. K. Katzschmann, and S. Mishra. Beyond regular grids: Fourier-based neural operators on arbitrary domains. In Proceedings of the 41st International Conference on Machine Learning, volume 235, pages 30610\u201330629. PMLR, 21\u201327 Jul 2024. URL https://proceedings.mlr.press/v235/lingsch24a.html.   \n[35] Y. Lipman, R. T. Q. Chen, H. Ben-Hamu, M. Nickel, and M. Le. Flow matching for generative modeling. In International Conference on Learning Representations, 2023. URL https: //iclr.cc/virtual/2023/poster/11309.   \n[36] S. Liu, C. Zeman, S. L. S\u00f8rland, and C. Sch\u00e4r. Systematic Calibration of a Convection-Resolving Model: Application Over Tropical Atlantic. Journal of Geophysical Research: Atmospheres, 127(23), 2022. doi:10.1029/2022JD037303.   \n[37] L. Lu, P. Jin, G. Pang, Z. Zhang, and G. E. Karniadakis. Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators. Nature Machine Intelligence, 3(3):218\u2013229, Mar. 2021. doi:10.1038/s42256-021-00302-5.   \n[38] A. Moore. Investigating the Near-Surface Wind Fields of Downbursts using a Series of HighResolution Idealized Simulations. Weather and Forecasting, 2024. doi:10.1175/WAF-D-23- 0164.1.   \n[39] L. Orf, E. Kantor, and E. Savory. Simulation of a downburst-producing thunderstorm using a very high-resolution three-dimensional cloud model. Journal of Wind Engineering and Industrial Aerodynamics, 2012. doi:10.1016/j.jweia.2012.02.020.   \n[40] A. Parodi, M. Lagasio, M. Maugeri, B. Turato, and W. Gallus. Observational and Modelling Study of a Major Downburst Event in Liguria: The 14 October 2016 Case. Atmosphere, 10(12): 788, 2019. doi:10.3390/atmos10120788.   \n[41] O. Pauluis. Thermodynamic Consistency of the Anelastic Approximation for a Moist Atmosphere. Journal of the Atmospheric Sciences, 65(8), 2008. doi:10.1175/2007JAS2475.1.   \n[42] K. G. Pressel, C. M. Kaul, T. Schneider, Z. Tan, and S. Mishra. Large-eddy simulation in an anelastic framework with closed water and entropy balances. Journal of Advances in Modeling Earth Systems, 7(3):1425\u20131456, 2015. ISSN 1942-2466. doi:10.1002/2015MS000496. URL https://onlinelibrary.wiley.com/doi/abs/10.1002/2015MS000496.   \n[43] A. Quarteroni, A. Manzoni, and F. Negri. Reduced basis methods for partial differential equations: an introduction, volume 92. Springer, 2015.   \n[44] B. Raonic, R. Molinaro, T. De Ryck, T. Rohner, F. Bartolucci, R. Alaifari, S. Mishra, and E. de B\u00e9zenac. Convolutional neural operators for robust and accurate learning of pdes. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36. Curran Associates, Inc., 2023. URL https://papers.nips.cc/paper_files/paper/2023/hash/ f3c1951b34f7f55ffaecada7fde6bd5a-Abstract-Conference.html.   \n[45] D. Rezende and S. Mohamed. Variational inference with normalizing flows. In F. Bach and D. Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37, Lille, France, 07\u201309 Jul 2015. PMLR. URL https://proceedings.mlr.press/ v37/rezende15.html.   \n[46] J. Richter, J. Nitzler, L. Pegolotti, K. Menon, J. Biehler, W. A. Wall, D. E. Schiavazzi, A. L. Marsden, and M. R. Pfaller. Bayesian windkessel calibration using optimized 0d surrogate models, 2024. URL https://arxiv.org/abs/2404.14187.   \n[47] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention \u2013 MICCAI 2015, pages 234\u2013241, Cham, 2015. Springer International Publishing. doi:10.1007/978-3-319- 24574-4_28.   \n[48] Y. Ruckstuhl and T. Janji\u00b4c. Combined State-Parameter Estimation with the LETKF for Convective-Scale Weather Forecasting. Monthly Weather Review, 148(4):1607\u20131628, 2020. doi:10.1175/MWR-D-19-0233.1.   \n[49] P. Segers, E. Rietzschel, M. De Buyzere, N. Stergiopulos, N. Westerhof, L. Van Bortel, T. Gillebert, and P. Verdonck. Three-and four-element windkessel models: assessment of their fitting performance in a large cohort of healthy middle-aged individuals. Proceedings of the Institution of Mechanical Engineers, Part H: Journal of Engineering in Medicine, 222(4):417\u2013428, 2008. doi:10.1243/09544119JEIM287.   \n[50] J. Seidman, G. Kissas, P. Perdikaris, and G. J. Pappas. NOMAD: Nonlinear manifold decoders for operator learning. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35. Curran Associates, Inc., 2022. URL https://papers.nips.cc/paper_files/paper/2022/hash/ 24f49b2ad9fbe65eefbfd99d6f6c3fd2-Abstract-Conference.html.   \n[51] J. H. Seidman, G. Kissas, G. J. Pappas, and P. Perdikaris. Variational autoencoding neural operators. In Proceedings of the 40th International Conference on Machine Learning, volume 202. PMLR, 23\u201329 Jul 2023. URL https://proceedings.mlr.press/v202/seidman23a. html.   \n[52] S. Sherwin, L. Formaggia, J. Peiro, and V. Franke. Computational modelling of 1D blood flow with variable mechanical properties and its application to the simulation of wave propagation in the human arterial system. International Journal for Numerical Methods in Fluids, 43(6-7): 673\u2013700, 2003. doi:10.1002/fld.543.   \n[53] Y. Shi, A. F. Gao, Z. E. Ross, and K. Azizzadenesheli. Universal functional regression with neural operator flows. In Advances in Neural Information Processing Systems:Workshop on Bayesian Decision-making and Uncertainty, 2024. URL https://openreview.net/forum? id=ZCtnWuaZiI.   \n[54] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, Lille, France, 07\u2013 09 Jul 2015. PMLR. URL https://proceedings.mlr.press/v37/sohl-dickstein15. html.   \n[55] R. Sokolow, G. Kissas, C. Beeche, S. Swago, E. W. Thompson, M. Viswanadha, J. Chirinos, S. Damrauer, P. Perdakaris, D. J. Rader, and W. R. Witschey. An aortic hemodynamic fingerprint reduced order modeling analysis reveals traits associated with vascular disease in a medical biobank. bioRxiv, 2024. doi:10.1101/2024.04.19.590260.   \n[56] J. M. Straka, R. B. Wilhelmson, L. J. Wicker, J. R. Anderson, and K. K. Droegemeier. Numerical solutions of a non-linear density current: A benchmark solution and comparisons. International Journal for Numerical Methods in Fluids, 17(1):1\u201322, 1993. doi:10.1002/fld.1650170103.   \n[57] G. G. Tong, C. A. Sing Long, and D. E. Schiavazzi. InVAErt networks: A data-driven framework for model synthesis and identifiability analysis. Computer Methods in Applied Mechanics and Engineering, 423:116846, 2024. doi:https://doi.org/10.1016/j.cma.2024.116846.   \n[58] R. Wang, Z. Chen, Q. Luo, and F. Wang. A conditional denoising diffusion probabilistic model for radio interferometric image reconstruction. In Proceedings of the 11th International Conference on Artificial Intelligence and Applications (FAIA), 2023. doi:10.3233/FAIA230554.   \n[59] A. Wehenkel, J. Behrmann, A. C. Miller, G. Sapiro, O. Sener, M. C. Cameto, and J.-H. Jacobsen. Simulation-based inference for cardiovascular models. In NeurIPS Workshop, 2024. URL https://arxiv.org/abs/2307.13918.   \n[60] J. Wildberger, M. Dax, S. Buchholz, S. Green, J. H. Macke, and B. Sch\u00f6lkopf. Flow matching for scalable simulation-based inference. In Advances in Neural Information Processing Systems, volume 36. Curran Associates, Inc., 2023. URL https://papers.nips.cc/paper_files/ paper/2023/hash/3663ae53ec078860bb0b9c6606e092a0-Abstract-Conference. html.   \n[61] J. Xiong, X. Cai, and J. Li. Clustered active-subspace based local gaussian process emulator for high-dimensional and complex computer models. Journal of Computational Physics, 450, 2022. doi:10.1016/j.jcp.2021.110840.   \n[62] J. Zhang, S. Zhang, and G. Lin. PAGP: A physics-assisted gaussian process framework with active learning for forward and inverse problems of partial differential equations, 2022. URL http://arxiv.org/abs/2204.02583. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Supplementary Material ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Training and Architecture Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "All experiments were performed on a Nvidia GeForse RTX 3090 GPU with 24GB of memory. The number of training, validation, and test samples are provided in Table 3, along with the data dimensions. Further details regarding the experimental data are provided in Sections A.3 and A.4. For each experiment, we performed a hyperparameter sweep when possible to select the learning rate, scheduler rate, weight decay, and network size. Due to the large number of hyperparameters, we selected 64 random combinations of hyperparameters and trained each model for 500 epochs. Following the sweep, we selected the model with the best performance and trained it for 2000 epochs, saving it at the epoch with the best performance on the validation set. Following this, we evaluated its performance on the test samples. A special exception must be made for the VAE-based models, InVAErt and cVANO. InVAErt was particularly susceptible to posterior collapse. Following the advice of the authors in Section 2.3.2 of [57], we hand-tuned the model to prevent over-parameterization and used early stopping. Similarly, cVANO was susceptible to training instability and exploding gradients, which also necessitated careful tuning and training in order to get the best results. The final size of each model, in number of network parameters, is provided in Table 4. The results from the model selection revealed all models are fairly consistent in size, typically all on the same order of magnitude. ", "page_idx": 15}, {"type": "text", "text": "We observed some sensitivity to the normalization scheme used on the data. In the PWP experiment, we normalize each input channel to a range between zero and one by dividing by the maximum value across all samples at that channel. In the ACB experiment, we normalize each channel by taking the minimum and maximum value across all samples in that channel and rescaling these to a range between zero and one. ", "page_idx": 15}, {"type": "text", "text": "The training time per epoch of all experiments is consistent, ranging from 1.5 to 2 seconds. During the hyperparameter sweep, each model is trained for 500 epochs, taking approximately 12.5 minutes. The best model from the sweep is trained for a total of 2000 epochs, taking approximately 1 hour. Throughout this entire project, we estimate the total number of GPU-hours for all model hyperparameter sweeps, hand-tuning, and training of the final model to be approximately 180 GPU-hours. ", "page_idx": 15}, {"type": "text", "text": "Additional Training Approaches Although the two objectives of FUSE are decoupled during training time, it is also possible to backpropagate losses on the outputs through both the forward and inverse model components. FMPE supports differentiable sampling, therefore the loss on the outputs $s$ could be formulated as a function of continuous inputs $u$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{L}_{3}(\\theta,\\phi)=\\mathcal{L}_{1}(\\theta;\\;\\mathcal{G}^{\\theta}\\circ\\rho^{\\phi}(.|.))=\\int_{\\mathcal{U}}\\|\\mathcal{G}^{\\theta}(\\rho^{\\phi}(\\xi|u))-\\mathcal{G}(\\rho(\\xi^{*}|u))\\|_{L^{1}}\\;d u.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In the case where the inputs and outputs, respectively $u$ and $s$ , are identical, a reconstruction loss may be applied to further train the inverse model component. This entails predicting outputs $s$ from the data-generating parameters $\\xi^{*}$ and predicting the inverse problem from the predictions $s$ , as opposed to the original inputs $u$ . This is formulated as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{L}_{r}(\\phi,\\theta)=\\mathcal{L}_{2}(\\phi;\\;\\hat{u}=T\\circ\\mathcal{G}^{\\theta}(\\xi^{*}))=\\mathcal{L}^{F M P E}(\\phi;\\;\\hat{u}=T\\circ\\mathcal{G}^{\\theta}(\\xi^{*})).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Although each approach has the potential to aid training it is still required to train each objective separately. Likewise, we did not observe improvements in our experiments by implementing these additional losses. This further supports our claim in Equation 1, that model components trained by decoupled objectives for forward and inverse problems may be unified at evaluation to propagate uncertainties in the parameters to outputs and provide interpretable results for complex and dynamic systems of PDEs. ", "page_idx": 15}, {"type": "text", "text": "A.2 Evaluation Metrics ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Continuous Ranked Probability Score We define the Continuous Ranked Probability Score (CRPS) and present how it compares a single ground truth value $y$ to a Cumulative Distribution Function (CDF) $F$ , quantifying how well a distribution of samples fits a single target. The CRPS is defined as ", "page_idx": 15}, {"type": "equation", "text": "$$\nC R P S(F,y)=\\mathbb{E}\\left[|X-y|\\right]-\\frac{1}{2}\\cdot\\mathbb{E}\\left[X-X^{\\prime}\\right],\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Table 3: Data details. $N_{t r a i n}$ , $N_{v a l}$ , and $N_{t r a i n}$ provide the number of training, validation, and test samples. The number of scalar parameters of the PDE system is given as $m$ . \"Channels\" describe the number of continuous input components (locations/variables), and the number of points in the time discretization of the continuous inputs and outputs is given by \"Sample Points\". The \"Batch Size\" is the number of training samples per mini-batch during training. ", "page_idx": 16}, {"type": "table", "img_path": "dbnEf790Kv/tmp/6aed4b1d2b2d878b054a18ffdfd8b1391d5ef2d10581bf91e4c56f8dd1b8338f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Table 4: Number of network parameters (millions) per model for the experiments. ", "page_idx": 16}, {"type": "table", "img_path": "dbnEf790Kv/tmp/936e283e6e80342e367d0587464c9dff1fedd02fdfb93545b8a2c540027a1257.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "dbnEf790Kv/tmp/d62988551d778558b02f84c49ad32ca33507c4d26cf1cdc6175e80e8c67978c7.jpg", "table_caption": ["Table 5: Evaluating inference time, CRPS, and relative L1 errors over continuous outputs for a trained model as the number of FMPE and Conditional DDPM samples increase. Conditional DDPM is implemented as described [30] for a 1D problem using 32 diffusion steps. We observe that the FMPE method results to higher accuracy in both metrics and slight increases in accuracy up until 512 samples, while DDPM is fairly consistent. Sampling times are approximately equivalent between the two models. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Table 6: Evaluating training time, CRPS, and relative L1 errors for models as the number of training data increases. As more training samples are used, CRPS and relative L1 error improve; however, the model converges after several thousand training samples resulting in smaller performance gains per new training sample. ", "page_idx": 16}, {"type": "table", "img_path": "dbnEf790Kv/tmp/21873f14f639b1a64a8823df40bfccf41f7e66b6cf64cd100ef4e9de77d084d2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "with independent and identically distributed random variables $X$ and $X^{\\prime}$ following the distribution dictated by $F$ . The first expectation computes the Mean Absolute Error of the samples compared to the target value, and the second expectation penalizes tight distributions which are far from the true value. A CRPS of zero indicates that the distribution perfectly fits the target sample by returning a Dirac measure. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "$L^{p}$ Error We employ the relative $L^{1}$ and $L^{2}$ error for evaluating predictions of time series data. This metric is calculated by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{||\\mathcal{G}(\\xi)-\\mathcal{G}^{\\theta}(\\xi)||_{p}}{||\\mathcal{G}(\\xi)||_{p}},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\mathcal G(\\xi)$ is the true function described by the system of PDEs, and $\\mathcal G^{\\theta}(\\xi)$ is a prediction produced by the network. ", "page_idx": 17}, {"type": "text", "text": "A.3 Pulse Wave Propagation: Reduced Order Model for Blood Flow ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The reduced order Navier-Stokes model considers a system of discrete compliant tubes and arterial segments, connected at points [1]. The length of the vessels is considered much larger than the local curvature and the wave propagation happens on the axial direction. Therefore, their properties can be described using a Cartesian coordinate $x$ [52]. The luminal cross-sectional area is defined as $\\textstyle A(x,t)=\\int_{A}d\\sigma$ , the average velocity is defined as $\\begin{array}{r}{U(x,t)=\\frac{1}{A}\\int_{S}\\mathbf{u}(\\mathbf{x},t)\\cdot\\mathbf{n}d\\sigma}\\end{array}$ and the volume flux at a given cross-section as $Q(x,t)=U A$ [1]. The arteries are allowed to deform in the radial direction due to the internal pressure $P(x,t)$ which is considered constant over a cross-section and the wall is considered impermeable. The blood is considered as an incompressible Newtonian fluid, with viscosity $\\mu=2.5m P a\\;s$ and density $\\rho=1060k g\\ m^{-3}$ [13]. All the gravitational effects are ignored assuming a patient in a supine position [1]. Using the above assumptions, the pulse wave propagation is formulated as the following system of hyperbolic constitutive laws [1, 13]: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{\\frac{\\partial A}{\\partial t}+\\frac{\\partial A U}{\\partial x}=0,}}\\\\ {\\displaystyle{\\frac{\\partial U}{\\partial t}+(2\\alpha-1)U\\frac{\\partial U}{\\partial x}+(\\alpha-1)\\frac{U^{2}}{A}\\frac{\\partial A}{\\partial x}+\\frac{1}{\\rho}\\frac{\\partial P}{\\partial x}=\\frac{f}{\\rho A},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\begin{array}{r}{a(x,t)\\,=\\,\\frac{1}{A U^{2}}\\int_{A}\\mathbf{u}^{2}d\\sigma}\\end{array}$ the Coriolis coefficient that accounts for the non-linearity of the integration over the cross-section and describes the shape of the velocity proflie. The velocity proflie is commonly considered as axisymmetric, constant, and satisfying the non-slip boundary conditions on the arterial walls. An example of such profile is [1]: ", "page_idx": 18}, {"type": "equation", "text": "$$\nu(x,r,t)=U\\frac{\\zeta+2}{\\zeta}\\Big[1-(\\frac{r}{R})^{\\zeta}\\Big]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $r$ is the radial coordinate, $R(x,t)$ the lumen radius and $\\begin{array}{r}{\\zeta=\\frac{2-\\alpha}{\\alpha-1}}\\end{array}$ a constant. A flat profile is defined by $\\alpha\\,=\\,1$ [7]. Integrating the three-dimensional Navier-Stokes equations for the axisymmetric flow provides the friction force per unit length $f(x,t)=-2(\\zeta+2)\\mu\\pi U$ . To account for the fluid-structure interaction of the problem, a pressure area relation is derived by considering the tube law. For a thin isotropic homogenous and incompressible vessel that deforms axisymmetrically at each circular cross-section the following Voigt-type visco-elastic law [6] is typically used: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{P=P_{e}(A;x)+\\displaystyle\\frac{\\Gamma(x)}{A_{0}(x)\\sqrt{A}}\\frac{\\partial A}{\\partial t},}}\\\\ {{P_{e}(A,x)=P_{\\mathrm{ext}}+\\displaystyle\\frac{\\beta(x)}{A_{0}(x)}\\Big(\\sqrt{A}-\\sqrt{A_{0}(x)},}}\\\\ {{\\beta(x)=\\displaystyle\\frac{4}{3}\\sqrt{\\pi}E(x)h(x),\\quad\\Gamma(x)=\\frac{2}{3}\\sqrt{\\pi}\\phi(x)h(x),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $P_{e}$ the elastic pressure component, $h(x)$ the wall thickness, $E(x)$ the Young\u2019s modulus, $\\phi(x)$ the wall viscosity, $\\beta(x)$ a wall stiffness coefficient, and $\\Gamma(x)$ the wall viscosity. The scalars $P_{e x t}$ and $A_{0}$ correspond to the external pressure and equilibrium cross-sectional area, respectively. The parameters $\\beta(x)$ and $\\Gamma(x)$ are computed using empirical relations found in the literature [7]. For this problem, a pulse wave from the heart is considered as a boundary condition for the inlet of the artery. For merging and splitting of arteries, the conservation of mass and the continuity of dynamic and kinematic pressure are considered, as well as, no energy losses. For the outlet of the arterial system, boundary conditions that model the downstream circulation are considered. In [7], the three-element Windkessel model [49] is used: ", "page_idx": 18}, {"type": "equation", "text": "$$\nQ(1+{\\frac{R_{1}}{R_{2}}})+C R_{1}{\\frac{\\partial Q}{\\partial t}}={\\frac{P_{e}-P_{\\mathrm{out}}}{R_{2}}}+C{\\frac{P_{e}}{t}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In [7], the authors consider algebraic relations from the literature that allows them to include different parameters, e.g. the volume flux, in the pulse wave propagation problem. For example, the digital PPG measurements can be related to the volume flux at the extremities as: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{PPG}(t)=\\int_{0}^{T}Q_{W}(t)-Q_{\\mathrm{out}}(t)d t,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $T$ is the duration of a cardiac cycle, $Q_{W}$ the flow entering the Windkessel model, and $Q_{\\mathrm{out}}$ the flow in the outlet. ", "page_idx": 18}, {"type": "table", "img_path": "dbnEf790Kv/tmp/9cc985a6c9e99c91ae3192f9001df873eaf357c9714b37a732e4e1e398f51cf9.jpg", "table_caption": ["Table 7: Cardiovascular parameters used in the reduced order model for the blood flow pulse wave propagation. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "dbnEf790Kv/tmp/5223f29926532ee617c28b387df101250b3f6e9f79faa1378479efeb43aee2b3.jpg", "table_caption": ["Table 8: Artery locations used in the reduced order model for the blood flow pulse wave propagation. "], "table_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "dbnEf790Kv/tmp/d5c70f041e7cbe0e4378cb8c2ad9e588cc28c1d800219c6efa7623c714ab18c6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "dbnEf790Kv/tmp/d5c467227260be7891e23ff843c09f5e5cabbcb51d2e0f7582e4fc3b22c3cdb4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "dbnEf790Kv/tmp/f1b7bdadd54d838044b43baec0d8e2bdcc52bf19ad96dd10fa2e20dcc16a8e23.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "dbnEf790Kv/tmp/69e25a9a72f29877216da5f6f8f75e054a6edeaa23d9a8eaa1403418cd377f2a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "dbnEf790Kv/tmp/d47ec8e70647d7b54914737b4f9b1a6cfa21d8734605cc3fe17a3064273fd544.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "dbnEf790Kv/tmp/d0a4f934029302d2baef1d0309e4213734fae8578cfbfb1393902f6acb7f15c1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure A.2: PWP, inverse problem: Histograms for the sample with the greatest CRPS (worst case) for each level of available input information. ", "page_idx": 21}, {"type": "image", "img_path": "dbnEf790Kv/tmp/4a58ac49d75bc9ddf53fb8056b85d50b193687c33e6e0169669478c17bd7b8e6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "dbnEf790Kv/tmp/3890387a8a3d739479d5d96accc657403fe8be1314eb4a9d70881ee9fd216cce.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure A.3: PWP, propagated uncertainty: Pressure time series for the sample with the median rel. L1 Error for each level of available input information. ", "page_idx": 22}, {"type": "image", "img_path": "dbnEf790Kv/tmp/aeb520512f619005391958fc4a7fcd9e2feb64ed6b0e2d486a3572946915a7a3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "dbnEf790Kv/tmp/a5a101eef03a34eb370815d5e08e1f23e1d5e04c6950e1246089b09de5eff55c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "dbnEf790Kv/tmp/5d7b8a1ce66fc0206d9aa694127f2c338f44ffac60882b98cc52e50c42984b07.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "dbnEf790Kv/tmp/73b30573a7d984430c19a67c5428e31de5182d58965180cb46391aa01cd0e10e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "dbnEf790Kv/tmp/8246dbebbe5d8f3fda7d5310511420a3d1171c101122b0d8c319c1ca20b3caab.jpg", "img_caption": ["(a) CRPS on the inverse problem. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "dbnEf790Kv/tmp/53cd9e85040fb5171cf33bd5af7cf41564ccad65cfb639a2791be39b7eb2452e.jpg", "img_caption": ["(b) $L^{1}$ error on the forward and unified problem. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure A.5: PWP: Box plots of the errors of the forward and inverse components of FUSE, as reported in Table 1, for different levels of available input information (\"test cases\"). For the inverse problem, parameter samples $\\xi_{i}$ are sampled from $\\rho^{\\phi}(\\dot{\\xi}^{i}|u)$ . For the forward problem, the output function $s$ is predicted based on the true parameter values $\\xi^{*}\\sim\\rho(\\xi|u)$ . For the unified problem evaluating both the inverse and forward model parts, the mean $\\bar{s}$ of the ensemble prediction $s_{i}$ from inferred parameters $\\xi_{i}\\sim\\rho^{\\phi}(\\xi^{i}|u)$ is compared to the true output time series $s$ . As information is removed from the input in the different cases, it becomes more difficult to estimate $s$ . ", "page_idx": 24}, {"type": "image", "img_path": "dbnEf790Kv/tmp/e2eaf41654b88fe27bc5fee60565ec86e0f488c1a7ce3348bd1a93ffc59dd9ca.jpg", "img_caption": ["Figure A.6: PWP, propagated uncertainty: Relative $L^{1}$ Error per vessel predicting pressure time series from the true parameters. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "dbnEf790Kv/tmp/0c046ffae82d66970ef8aa8a34d084aea7c637853c1ba6b36ac14fed5edf353a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "image", "img_path": "dbnEf790Kv/tmp/a5c95720bf3277e5ec3f106c9c952882b2d090c07299e84440d7c4fb3b090f96.jpg", "img_caption": [], "img_footnote": [""], "page_idx": 25}, {"type": "image", "img_path": "dbnEf790Kv/tmp/38c8aec3bb4dee45c723aead60e40dcb4e78eb8b870cc865934b4e8350af8652.jpg", "img_caption": [], "img_footnote": ["Figure A.9: PWP, inverse problem: InVAErt predictions overparameters, the median sample with ful input information(level 1). InVAErt is particularly susceptible to posteriorcollapse, i.e., the predictions over the parameters becomeoverconfident and the uncertainty bounds fail to capture thetrue parameter. "], "page_idx": 26}, {"type": "image", "img_path": "dbnEf790Kv/tmp/fd06e0fdc161c5fab42c0e2a3c79b605f448053b3878442dafb7ffcfbc3570c7.jpg", "img_caption": ["Figure A.10: PWP, sensitivity analysis: Pressure in the Aortic Root. We study how the pressure changes due to parameters of the system. Blue represents the lower bound of possible values, and red represents the upper bound, with colors of varying hue at intermediate values. ", ""], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "dbnEf790Kv/tmp/47209eedcf8737e40a4bb956e6f2c868f12eb486d594cda7e5fad5b6241a5404.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure A.11: PWP: Correlation of the errors between continuous $(L^{1})$ and discrete (CRPS) parameters for different levels of available input information (\"cases\"). ", "page_idx": 27}, {"type": "text", "text": "A.4 Atmospheric Cold Bubble ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In atmospheric modelling, large-eddy simulations (LES) help to understand the complicated dynamics within convective storms [39] that are usually observed only by scalar local measurements of wind (anemometers), other scalar measurements obtained at surface-level weather stations (e.g., pressure, humidity), and, if available, three-dimensional wind lidar records. A particularly destructive phenomenon associated with storms are downbursts, sudden local downdrafts that cause extreme winds when hitting the surface. Manual tuning of a numerical model is usually required to fit a simulation to match an observed downburst (e.g., [40]), due to the model complexity involving several parameterizations such as cloud microphysics. The model can then be configured to explore the sensitivity of the extreme winds both to the numerical model parameters and to the physical environmental conditions [38]. In addition to improving the scientific understanding and forecasts for more precise warnings, simulations of downbursts help to design robust infrastructure, such as wind turbines. However, the amount of simulations performed for model tuning (inverse problem) and sensitivity analysis (forward problem) is limited by the high computational cost to typically tens to hundreds of simulations. The affordable number of numerical simulations is either used for strategic expert tuning or to train an emulator for the parameter-to-data map, which is cheaper to evaluate and can then be used for in-depth sensitivity analysis (e.g., Gaussian Processes [9]; quadratic regression [36]). We would like to highlight that the presented test case is conceptually similar to downbursts, but far from simulating real-world scenarios due to its idealized nature and reduced complexity. However, we believe that test cases like this one will help to pave the way for handling realistic cases with methodologies such as FUSE. ", "page_idx": 28}, {"type": "text", "text": "Model Description The numerical data for the cold bubble simulation is obtained with the PyCLES model [42]. It solves the anelastic Navier-Stokes equations as formulated by [41], evolving around a hydrostatic reference state, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\alpha_{0}\\frac{\\partial p_{0}}{\\partial x_{3}}}&{{}=}&{-g,}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "with reference proflies $\\alpha_{0}(z)$ for specific volume and $p_{0}(z)$ for pressure, and $(x_{1},x_{2},x_{3})=(x,y,z)$ and $(u_{1},u_{2},u_{3})=\\bar{(u,v,w)}$ for simplicity. The anelastic equations of motion are given by ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l}{\\displaystyle\\frac{\\partial u_{i}}{\\partial t}+\\frac{1}{\\rho_{0}}\\frac{\\partial(\\rho_{0}u_{i}u_{j})}{\\partial x_{i}}=-\\frac{\\partial\\alpha_{0}p^{\\prime}}{\\partial x_{i}}+b\\delta_{13}-\\frac{1}{\\rho_{0}}\\frac{\\partial(\\rho_{0}\\tau_{i j})}{\\partial x_{j}}+\\Sigma_{i},}\\\\ {\\displaystyle\\ \\ \\frac{\\partial s}{\\partial t}+\\frac{1}{\\rho_{0}}\\frac{\\partial(\\rho_{0}u_{i}s)}{\\partial x_{i}}=\\frac{Q}{T}-\\frac{1}{\\rho_{0}}\\frac{\\partial(\\rho_{0}\\gamma_{s,i})}{\\partial x_{i}}+\\dot{S},}\\\\ {\\displaystyle\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\frac{\\partial\\rho_{0}u_{i}}{\\partial x_{i}}=0,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "encompassing the momentum equations (a), the entropy equation (b), and the continuity equation (c). We use standard conventions for summing and the Kronecker delta $\\delta_{i j}$ , and left out the terms and further equations referring to the Coriolis force and moisture, as they do not apply to the small-scale and dry cold bubble simulation. The buoyancy term $b$ depends on the specific volume, moisture, gravity, and the reference pressure proflie, $p^{\\prime}$ is the dynamic pressure perturbation over the hydrostatic reference state, and $\\Sigma_{i}$ is an additional momentum source term, which is expected to be small for the cold bubble experiment. In the entropy equation, $Q$ is the diabatic heating rate, $T$ is the temperature, and $\\dot{S}$ is a source term of irreversible entropy sources. Finally, $\\tau_{i j}$ and $\\gamma_{s,i}$ are the sub-grid scale (SGS) stresses acting on velocity and entropy, which model the effect of turbulence smaller than the grid scale in a diffusive way as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\tau_{i j}=-2\\nu_{t}S_{i j},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "relating the sub-grid stress to the strain rate $S_{i j}=1/2(\\partial_{i}u_{j}+\\partial_{j}u_{i})$ of the resolved flow. The eddy viscosity $\\nu_{t}$ may be related to the diffusivity $D_{t}$ for heat and other scalars $\\phi$ , and ultimately to the corresponding SGS stresses $\\gamma_{\\phi,i}$ by the turbulent Prandtl number $\\mathrm{Pr}_{t}$ by ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{D_{t}}&{=}&{\\nu_{t}/\\mathrm{Pr}_{t},}\\\\ {\\gamma_{\\phi,i}}&{=}&{-D_{t}\\frac{\\partial\\phi}{\\partial x_{i}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "For three-dimensional simulations, one may choose a sub-grid scale parameterization for $\\nu_{t}$ such as a Smagorinsky-type first-order closure or a higher-order closure based on a prognostic equation for turbulent kinetic energy. For the two-dimensional cold bubble case, however, we stick to uniform values for both $\\nu_{t}$ and $D_{t}$ , independent of the turbulent Prandtl number. ", "page_idx": 28}, {"type": "text", "text": "Table 9: ACB: Discrete parameters with their ranges used for uniform sampling of the training data, as well as default values used by [56]. The parameters either encode the initial condition (IC) or are part of the sub-grid scale (SGS) model (M). The horizontal location $x_{c}$ is kept fixed to ensure the horizontal symmetry of the domain. ", "page_idx": 29}, {"type": "table", "img_path": "dbnEf790Kv/tmp/f78b8889236e9d65804cbca691bb0ad6e39bc098514c3f405687af2915ffade1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "The domain of size $(L_{x},\\ L_{z})=(51.6\\,\\mathrm{km}$ , $6.4\\;\\mathrm{km})$ exhibits a neutrally stratified background proflie $\\partial_{z}\\theta=0$ , where $\\theta$ denotes potential temperature), and no background winds. An elliptic cold air anomaly is prescribed by ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\Delta T}&{=}&{-a\\left(\\cos(\\pi L)+1\\right)/2,\\quad}\\\\ {L}&{=}&{\\left|\\left|\\big((x-x_{c})x_{r}^{-1},\\;(z-z_{c})z_{r}^{-1}\\big)\\right|\\right|_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where default values and ranges for all parameters can be found in Table 9, with the resulting flow fields shown in Figure A.12. ", "page_idx": 29}, {"type": "text", "text": "Numerical Setup The PyCLES model by [42] relies on a WENO finite volumes scheme that exhibits exceptional stability, implemented in Cython. It is used here in the setup described by the authors, at resolution $\\Delta x=\\Delta z=50\\mathrm{\\;m}$ and an adaptive time stepping bound to $\\Delta t\\leq5$ s in order to record time series measurements at a temporal resolution of five seconds. ", "page_idx": 29}, {"type": "text", "text": "The data is recorded at the eight locations marked in Figure A.12, at horizontal locations $x=$ 15, $20~\\mathrm{km}$ (about 10 and $5~\\mathrm{km}$ from the center of the anomaly), and vertical heights of $z=$ 50, 100, 250, 500, and $2000\\;\\mathrm{m}$ , each being part of the numerical grid. ", "page_idx": 29}, {"type": "text", "text": "In terms of computational complexity, a PyCLES simulation in the given setup takes about half an hour on eight cores. ", "page_idx": 29}, {"type": "image", "img_path": "dbnEf790Kv/tmp/5c2b6412b66d12477771f95ae8c1f99ec05a91d382856199be760c5b1df12df6.jpg", "img_caption": ["Figure A.12: ACB: Time evolution of horizontal velocity (left), temperature anomaly (middle), and vertical velocity (right), for the default values of the parameters given in Table 9, showing three main vortices that develop until $T=900~\\mathrm{s}$ . Measurement locations for the time series data are marked with triangles, with their location indices indicated in the first row. "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "dbnEf790Kv/tmp/18f0ea7fe140f86ec1eb7e57bec14ec6536451db04c5883e25cfec0cb8ffa56f.jpg", "img_caption": ["Figure A.13: ACB: As Figure A.12, for a sample with a stronger anomaly, reaching the state of three main vortices around $t=600$ s instead of $t=900\\;\\mathrm{s}$ . "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "dbnEf790Kv/tmp/0782e4aaa173a007257fe5f47d0dd6683862b69805b9b565d463ff0362f4b317.jpg", "img_caption": [], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "dbnEf790Kv/tmp/c51579037b074ba166f3e61166512961a589ffe32ce5033095c4c8c4692c3f63.jpg", "img_caption": [], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "dbnEf790Kv/tmp/766a26d00dee131781f16e7584760f04ae90a4fbabcfe58ec017cb0128e93a23.jpg", "img_caption": [], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "dbnEf790Kv/tmp/6292298c4a00ae2ae1241fe706cc313013f842e3895ba43b1aaf8ef9b9d7270c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "dbnEf790Kv/tmp/4e277d0332219889e6408acfe6764dbb09eb6ef7457ce968f67d2d9f2c53fbed.jpg", "img_caption": [], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "dbnEf790Kv/tmp/c48170221c22a5c08d119f9d47b78f14a686e8187ab437aea6b74bf758d9defd.jpg", "img_caption": ["(a) Sample on which FUSE has the median CRPS. "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "dbnEf790Kv/tmp/f9d4218748e948913a5b1292c41b869cdcdb1f592c31dfb704246945fdf6a2ac.jpg", "img_caption": ["(b) Sample on which FUSE has worst CRPS. "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "Figure A.16: ACB, inverse problem: Histograms of the parameters inferred from continuous time series measurements $u$ , for different test samples. Given the very small amplitude and horizontal extend of the worst-case sample, this perturbation hardly reaches the sensor and the parameters are hence difficult to infer from the weak velocities measured. ", "page_idx": 33}, {"type": "image", "img_path": "dbnEf790Kv/tmp/36b5b8c4ceadee9abfc86ad3e4c47c84473f65ada2b316c28c3d19038ef925b8.jpg", "img_caption": ["Figure A.17: ACB, sensitivity analysis: Fingerprints showing the model sensitivity to one parameter at a time at location 1, while keeping all others at their default value. "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "dbnEf790Kv/tmp/bca84d47d9759596080cb9dc91311835b35570e503a46f3796ac2c53edf53324.jpg", "img_caption": ["(a) Location 1 $x=15\\,{\\mathrm{km}}$ , $z=50\\;\\mathrm{m})$ ) "], "img_footnote": [], "page_idx": 34}, {"type": "image", "img_path": "dbnEf790Kv/tmp/ebf8f26d2618b1a6161a203db895213f169b18b14cea4e7ae5628a70fe689b39.jpg", "img_caption": ["(b) Location 5 $x=20\\,{\\mathrm{km}}$ , $z=50\\;\\mathrm{m}$ ) "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "Figure A.18: ACB, sensitivity analysis, continuation of Fig. 4: Validation of the FUSE model against numeric simulations on peak horizontal velocities $u$ at location 1 (left, further away from the perturbation center) and 5 (right, closer). From top to bottom: relative error between FUSE and the numerical model, difference between FUSE and the numerical model, maximum velocity calculated by the numerical model, maximum velocity calculated by FUSE. ", "page_idx": 34}, {"type": "image", "img_path": "dbnEf790Kv/tmp/696b992b05812f8c0e9f005fef96e0fca09002da6b9224fe0c751833a39fcd3e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "Figure A.19: ACB, sensitivity analysis: Peak horizontal velocities $u$ at location 1 $x=15\\,{\\mathrm{km}}$ , $z=50$ m), sampled for pairwise combinations of the parameters, while keeping all others at their default value. For the neural model (a), 100 samples are drawn for each parameter, corresponding to 150,000 evaluations. For the numerical model (b), four simulations were run per pair of parameters, with each taking values corresponding to $1/6$ and $5/6$ of the parameter range, corresponding to 60 model evaluations. ", "page_idx": 35}, {"type": "image", "img_path": "dbnEf790Kv/tmp/5b931daf201232c0b1359434c50ba1cd8036923d042185666ce5928c5c74a519.jpg", "img_caption": ["Figure A.20: ACB, forward problem: Same as Figure A.19, at location 5 $x=20\\,{\\mathrm{km}}$ , $z=2000\\;\\mathrm{m}$ . "], "img_footnote": [], "page_idx": 35}, {"type": "image", "img_path": "dbnEf790Kv/tmp/88ff9bf09876f900095c8fa76ba0839393a6bfc0c3c8af499aebf4423b9493e6.jpg", "img_caption": ["(a) Forward problem: Error between the prediction of $s$ based on the true parameters $\\xi^{*}$ and the true output time series $s$ . "], "img_footnote": [], "page_idx": 36}, {"type": "image", "img_path": "dbnEf790Kv/tmp/fd6ffc85c261227bef5d49aa8adc207162e44a994acb70533d153e44c927df54.jpg", "img_caption": ["(b) Unified prediction: Error between the predicted ensemble mean s\u00af based on the input time series $u$ , and the true output time series s. "], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "Figure A.21: ACB, propagated uncertainty: Box plots of relative errors in the time series predictions at each location. ", "page_idx": 36}, {"type": "text", "text": "A.5 Properties of the Total Variation Metric ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Consider the total variation distance $d(\\mu,\\mu^{*})$ , where $\\mu$ and $\\mu^{*}$ are measures over $u\\in\\mathcal{U}$ , which is defined as ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d(\\mu,\\mu^{*})=\\displaystyle\\operatorname*{sup}_{A\\subseteq\\mathcal{U}}|\\mu(A)-\\mu^{*}(A)|,}\\\\ &{\\qquad\\qquad=\\displaystyle\\operatorname*{sup}_{A\\subseteq\\mathcal{U}}|\\int_{\\mathcal{U}}\\mu(u)d u-\\int_{\\mathcal{U}}\\mu^{*}(u)d u|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Further, consider a random vector $\\xi\\,\\in\\,\\Xi\\,\\subseteq\\,\\mathbb{R}^{m}$ , the function $h:\\Xi\\to\\mathcal{U}$ , and the probability measures $\\rho$ and $\\rho^{*}$ over $\\Xi$ . Since $u\\,=\\,h(\\xi)$ , the measures $\\mu$ and $\\mu^{*}$ can be considered as the pushforward measures induced by $\\rho$ through the function $h$ , $\\mu=h_{\\#\\rho}$ and $\\mu^{*}=h_{\\#\\rho^{*}}$ . We assume all measures admit densities and use the same notation for measures ${\\dot{\\mu}}(A),\\,A\\subseteq\\mathcal{U}$ , and their densities $\\mu(u),\\,u\\in\\mathcal{U}$ , since the distinction is clear from the arguments. ", "page_idx": 37}, {"type": "text", "text": "We consider the conditional probability measures $\\rho(\\xi|u),\\rho^{*}(\\xi|u)$ . Then, $\\mu$ and $\\mu^{*}$ are defined as: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mu(u)=\\int_{\\Xi}\\rho(\\boldsymbol{\\xi})\\rho(\\boldsymbol{\\xi}|u)d\\boldsymbol{\\xi},\\quad\\mu^{*}(u)=\\int_{\\Xi}\\rho^{*}(\\boldsymbol{\\xi})\\rho^{*}(\\boldsymbol{\\xi}|u)d\\boldsymbol{\\xi},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "and the measure of set $A$ as: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mu(A)=\\int_{A}\\mu(u)d u=\\int_{A}\\int_{\\Xi}\\rho(\\xi)\\rho(\\xi|u)d\\xi d u}\\\\ {\\displaystyle\\mu^{*}(A)=\\int_{A}\\mu^{*}(u)d u=\\int_{A}\\int_{\\Xi}\\rho^{*}(\\xi)\\rho^{*}(\\xi|u)d\\xi d u}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Assume $u$ is uniquely determined by $\\xi$ through the map $u=h(\\xi)$ and that $\\rho(\\xi|u)$ is hence highly concentrated at $\\xi^{\\doteq}\\bar{h}^{-1}(u)$ and therefore: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\rho(\\xi|u)=\\delta(\\xi-h^{-1}(u))}\\\\ {\\rho^{\\ast}(\\xi|u)=\\delta(\\xi-h^{-1}(u))}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $\\delta$ is the Dirac delta function and ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\mu(u)=\\displaystyle\\int_{\\Xi}\\rho(\\xi)\\delta(\\xi-h^{-1}(u))d\\xi=\\rho(h^{-1}(u))}}\\\\ {{\\mu^{*}(u)=\\displaystyle\\int_{\\Xi}\\rho^{*}(\\xi)\\delta(\\xi-h^{-1}(u))d\\xi=\\rho^{*}(h^{-1}(u)).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "The definition of the total variation is then written as: ", "page_idx": 37}, {"type": "equation", "text": "$$\nd(\\mu,\\mu^{*})=\\operatorname*{sup}_{A\\subseteq\\mathcal{U}}\\left|\\int_{A}\\rho(h^{-1}(u))d u-\\int_{A}\\rho^{*}(h^{-1}(u))d u\\right|.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "The terms $\\rho(h^{-1}(u))$ and $\\rho^{*}(h^{-1}(u))$ represent the pullback operators and therefore we can write: ", "page_idx": 37}, {"type": "equation", "text": "$$\nd(\\mu,\\mu^{*})=\\operatorname*{sup}_{A\\subseteq\\mathcal{U}}\\left|\\rho(h^{-1}(A))-\\rho^{*}(h^{-1}(A))\\right|=d(\\rho,\\rho^{*})\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Therefore due to the unique definition of $u$ from $\\xi$ via $h$ , the two definitions of the total variation are equivalent. The relation between the total variation of the original and the conditional measures is then given by: ", "page_idx": 37}, {"type": "equation", "text": "$$\nd(\\mu,\\mu^{*})=d(\\rho,\\rho^{*})\\leq\\int_{\\mathcal{U}}d(\\rho(\\xi|u),\\rho^{*}(\\xi|u))d\\mu^{*}=d(\\rho(\\xi|u),\\rho^{*}(\\xi|u)).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "A.6 Detailed Derivation of the Forward Objective ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "We start with $d(\\tilde{\\mathcal{G}}_{\\#\\mu}^{\\theta},\\tilde{\\mathcal{G}}_{\\#\\mu})$ and our goal is to bound this metric by the supervised operator learning objective. For a function $u$ , its corresponding images $\\tilde{\\mathcal{G}}(u)$ and $\\tilde{\\mathcal{G}}^{\\theta}(u)$ , and for any set $A\\in B(S)$ , where $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ is a Borel $\\sigma$ -algebra over $\\boldsymbol{S}$ : ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{G}}_{\\#\\mu}(A)=\\int_{S}w_{A}(s)d\\tilde{\\mathcal{G}}_{\\#\\mu}(s),\\quad\\tilde{\\mathcal{G}}_{\\#\\mu}^{\\theta}(A)=\\int_{S}w_{A}(s)d\\tilde{\\mathcal{G}}_{\\#\\mu}^{\\theta}(s),\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $w_{A}(s)$ is the indicator function that is one if $s\\ \\in\\ A$ and zero otherwise. Let $B^{\\prime}({\\mathcal{U}})\\;:=\\;$ $\\tilde{\\mathcal{G}}^{-1}(B(S))$ be the $\\sigma$ -algebra on $\\boldsymbol{\\mathcal{U}}$ generated by the pre-images of sets in $\\boldsymbol{B}(\\boldsymbol{S})$ under $\\tilde{\\mathcal{G}}$ . Then, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d(\\tilde{\\mathcal{G}}_{\\#\\mu},\\tilde{\\mathcal{G}}_{\\#\\mu}^{\\theta})=\\displaystyle\\operatorname*{sup}_{A\\in\\mathcal{B}(S)}\\left\\vert\\int_{S}w_{A}(s)d\\tilde{\\mathcal{G}}_{\\#\\mu}^{\\theta}(s)-\\int_{S}w_{A}(s)d\\tilde{\\mathcal{G}}_{\\#\\mu}^{\\theta}(s)\\right\\vert}\\\\ &{\\qquad\\qquad\\qquad=\\displaystyle\\operatorname*{sup}_{\\tilde{A}\\in\\mathcal{B}^{\\prime}(\\mathcal{U})}\\left\\vert\\int_{\\mathcal{U}}w_{\\tilde{A}}(\\tilde{\\mathcal{G}}(u))d\\mu(u)-\\int_{\\mathcal{U}}w_{\\tilde{A}}(\\tilde{\\mathcal{G}}^{\\theta}(u))d\\mu(u)\\right\\vert.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Further, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{d(\\tilde{\\mathcal{G}}_{\\#\\mu},\\mathcal{G}_{\\#\\mu}^{\\theta})=\\operatorname*{sup}_{\\tilde{A}\\in B^{\\prime}(\\mathcal{U})}\\left|\\int_{\\mathcal{U}}(w_{\\tilde{A}}(\\tilde{\\mathcal{G}}(u))-w_{\\tilde{A}}(\\tilde{\\mathcal{G}}^{\\theta}(u))d\\mu(u)\\right|}}\\\\ &{\\leq\\left|\\int_{\\mathcal{U}}(\\tilde{\\mathcal{G}}(u)-\\tilde{\\mathcal{G}}^{\\theta}(u))d\\mu(u)\\right|}\\\\ &{\\leq\\displaystyle\\int_{\\mathcal{U}}\\|\\tilde{\\mathcal{G}}(u)-\\tilde{\\mathcal{G}}^{\\theta}(u)\\|_{L^{1}(Y)}d\\mu(u).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Here, $\\|\\cdot\\|_{L^{1}(Y)}$ is the $L^{1}$ norm over the space of function $\\boldsymbol{S}$ . We are interested in parametric PDEs where we have access to parameters $\\xi\\in\\Xi\\subset\\mathbb{R}^{m}$ , the true PDE parameters, sampled from $\\rho$ , a probability measure on $\\mathbb{R}^{m}$ . Operator learning involves a map between continuous functions, so an intermediate step needs to be considered to first lift the parameters $\\xi$ to a continuous space before performing the action of the operator. For this purpose, we consider a function $h$ composed using two maps: $l:\\mathbb{R}^{m}\\rightarrow B_{\\Omega}$ and $T^{-1}:B_{\\Omega}\\to\\mathcal{U}$ , the inverse Fourier transform. In this formulation, $l$ maps the parameters to $k$ to the Paley-Wiener space of band-limited functions in Fourier space, $B_{\\Omega}\\,\\boldsymbol{\\bar{\\mathbf{\\rho}}}=\\{l\\in\\mathbf{\\hat{\\r}}L^{2}(\\mathbb{R}):\\operatorname{supp}\\,l\\subset[-\\Omega,\\Omega]\\}$ , thus truncating the frequencies that can be used to represent the data. For this reason, we consider $\\mu=h_{\\#\\rho(\\xi|u)}$ , the pushforward of $\\rho(\\xi|u)$ under $h$ , and use a change of variables: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\int_{\\mathcal{U}}f(u)d\\mu(u)=\\int_{\\Xi}f(h(\\xi))d\\rho(\\xi|u),\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "for any integrable function $f$ . We can apply this property to: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d(\\tilde{\\mathcal{G}}_{\\#\\mu},\\tilde{\\mathcal{G}}_{\\#\\mu}^{\\theta})=\\underset{\\tilde{A}\\in B^{\\prime}(M)}{\\operatorname*{sup}}\\left|\\int_{\\mathcal{U}}(w_{\\tilde{A}}(\\tilde{\\mathcal{G}}(u))-w_{\\tilde{A}}(\\tilde{\\mathcal{G}}^{\\theta}(u))d\\mu(u)\\right|}\\\\ &{\\qquad\\qquad\\qquad=\\underset{\\tilde{A}\\in B^{\\prime}(M)}{\\operatorname*{sup}}\\left|\\int_{\\Xi}(w_{\\tilde{A}}(\\tilde{\\mathcal{G}}(h(\\xi)))-w_{\\tilde{A}}(\\tilde{\\mathcal{G}}^{\\theta}(h(\\xi)))d\\rho(\\xi)\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "We can define the operators $\\mathcal{G}:\\Xi\\rightarrow\\mathcal{U}$ and $\\mathcal{G}^{\\theta}:\\Xi\\rightarrow\\mathcal{U}$ and write: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d(\\tilde{\\mathcal{G}}_{\\#\\mu},\\tilde{\\mathcal{G}}_{\\#\\mu}^{\\theta})=\\underset{\\tilde{A}\\in\\mathcal{B}^{\\prime}(U)}{\\operatorname*{sup}}\\left|\\int_{\\Xi}(w_{\\tilde{A}}(\\mathcal{G}(\\xi))-w_{\\tilde{A}}(\\mathcal{G}^{\\theta}(\\xi)))d\\rho(\\xi)\\right|}\\\\ &{\\qquad\\qquad\\qquad\\leq\\int_{\\Xi}\\|\\mathcal{G}(\\xi)-\\mathcal{G}^{\\theta}(\\xi))\\|_{L^{1}(Y)}d\\rho(\\xi)}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Finally, we arrive at ", "page_idx": 38}, {"type": "equation", "text": "$$\nd(\\tilde{\\mathcal{G}}_{\\#\\mu},\\tilde{\\mathcal{G}}_{\\#\\mu}^{\\theta})\\leq\\int_{\\Xi}\\|\\mathcal{G}(\\xi)-\\mathcal{G}^{\\theta}(\\xi))\\|_{L^{1}(Y)}d\\rho(\\xi).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "A.7 Detailed Derivation of the Inverse Objective ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "In the supervised operator learning problem, we have access to samples $u$ of functions but we do not have access to the distribution that generates $u$ . Assume that ${\\tilde{\\mathcal{G}}}^{\\theta}$ is Lipschitz continuous: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\Vert\\tilde{\\mathcal{G}}^{\\theta}(u)-\\tilde{\\mathcal{G}}^{\\theta}(v)\\Vert_{L^{1}(Y)}\\leq C\\Vert u-v\\Vert_{L^{1}(X)}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Assuming that $\\mu^{\\phi}$ is an approximation of $\\mu$ , we can relate these two measures by the total variation of their respective pushforward measures under the forward surrogate $\\mathcal{G}^{\\theta}$ using the continuity properties of the operator (see Appendix A.5), ", "page_idx": 39}, {"type": "equation", "text": "$$\nd(\\tilde{\\mathcal{G}}_{\\#\\mu^{\\phi}}^{\\theta},\\tilde{\\mathcal{G}}_{\\#\\mu}^{\\theta})\\leq C\\;d(\\mu^{\\phi},\\mu),\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "for some constant $C\\,\\in\\,\\mathbb{R}$ , which we generally assume to be equal to one. This property holds because of the continuity of ${\\tilde{\\mathcal{G}}}^{\\theta}$ and because the total variation is preserved under continuous transformations. We assume that the functions $u$ are generated by solving a parametric PDE, where the parametric uncertainty over $\\xi$ is propagated onto $u$ through the likelihood $\\mu(u|\\xi)$ . Performing parameter estimation corresponds to sampling from a conditional distribution $\\rho^{\\phi}(\\xi|u)$ approximating the posterior $\\rho(\\xi|u)$ . To do so, we first bound the previous distance by the total variation between $\\rho^{\\phi}(\\xi|u)$ and $\\rho(\\xi|u)$ over $\\Xi$ (see Appendix A.5): ", "page_idx": 39}, {"type": "equation", "text": "$$\nd(\\mu^{\\phi},\\mu)\\leq\\tilde{d}(\\rho^{\\phi}(\\xi|u),\\rho(\\xi|u)).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Combining the two inequalities, we get ", "page_idx": 39}, {"type": "equation", "text": "$$\nd(\\tilde{\\mathcal{G}}_{\\#\\mu^{\\phi}}^{\\theta},\\tilde{\\mathcal{G}}_{\\#\\mu}^{\\theta})\\leq d(\\rho^{\\phi}(\\xi|u),\\rho(\\xi|u)),\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "which means that we can minimize the right hand side of the inequality (29). ", "page_idx": 39}, {"type": "text", "text": "A.8 Formulation of the FMPE loss ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "For our implementation of Flow-Matching Posterior Estimation (FMPE), we closely follow [60]. In the main text (Eqn. 5), we formulated the FMPE objective as ", "page_idx": 39}, {"type": "equation", "text": "$$\nd(\\rho^{\\phi}(\\xi|u),\\rho(\\xi|u)),\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "for the total variation distance $d$ on measures over the parameter space $\\Xi\\subseteq\\mathbb{R}^{m}$ . We will minimize this distance in order to approximate the underlying distribution, $\\left|\\rho^{\\phi}\\approx\\rho\\right|$ , and sample $M$ samples from $\\rho^{\\phi}(\\xi|u)$ with the help of FMPE. We also defined the transform (6), ", "page_idx": 39}, {"type": "equation", "text": "$$\nT^{\\phi_{1},\\phi_{2}}(u)=\\hat{u}=\\tilde{h}\\circ K^{\\phi_{1}}\\circ\\mathcal{P}^{\\phi_{2}}(u),\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "which transforms an infinite-dimensional input $u\\in\\mathcal{U}$ to a finite-dimensional conditional information $\\boldsymbol{\\hat{u}}\\in\\mathbb{R}^{k}$ in Fourier space. ", "page_idx": 39}, {"type": "text", "text": "The FMPE method learns an invertible transform between the target distribution of parameters, conditional on the additional information, and a standard normal distribution. This transform is modelled as an ordinary differential equation with artificial time dimension $t\\in[0,1]$ and solution $\\psi_{t,\\hat{u}}:\\mathbb{R}^{m}\\rightarrow\\mathbb{R}^{m}$ as ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\frac{d}{d t}\\psi_{t,\\hat{u}}(\\xi)=v_{t,\\hat{u}}(\\psi_{t,\\hat{u}}(\\xi)),\\quad\\psi_{0,\\hat{u}}(\\xi)=\\xi_{0},\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where $\\xi_{0}$ follows a standard normal distribution. Trajectories in the artificial time $t$ can be obtained as $\\xi_{t}=\\psi_{t,h a t u}(\\xi)$ . This way, the target probability $\\rho(\\xi|\\hat{u})$ is approximated as ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\rho^{\\phi}(\\xi|\\hat{u})=(\\psi_{1,\\hat{u}})_{\\#\\rho_{0}}(\\xi)=\\rho_{0}(\\xi)\\exp\\Big(-\\int_{0}^{1}\\mathrm{div}\\;v_{t,\\hat{u}}(\\xi_{t})d t\\Big),\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "by solving the transport equation $\\begin{array}{r}{\\partial_{t}\\rho_{t}+\\mathrm{div}(\\rho_{t}v_{t,\\hat{u}})=0}\\end{array}$ . ", "page_idx": 39}, {"type": "text", "text": "During training, $\\psi_{0,\\hat{u}_{i}}(\\xi_{i})$ is fit to resemble a standard normal on the training samples $\\{\\xi_{i},\\hat{u}_{i}\\}_{i=1}^{n}$ . During evaluation, a standard normal sample $\\xi_{0}$ is drawn, and transformed to a sample that approximately follows $\\rho^{*}(\\xi|u)$ as ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\xi^{\\prime}=\\psi_{\\hat{u}_{i},0}(\\xi_{0})\\sim\\rho(\\xi|u).}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "In order to fully describe $\\psi_{t,\\hat{u}}$ , it hence suffices to learn the trajectory velocity function $v_{t,\\hat{u}}$ . Let $p_{t}(\\xi|\\xi_{1})$ be a sample-conditional Gaussian probability path with vector field $l_{t}(\\xi_{t}|\\xi_{1})$ , and let time be distributed as $t\\sim p(t)$ . Then, [60] formulate the FMPE loss as ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}^{F M P E}(\\phi)=\\mathbb{E}_{t\\sim p(t),\\xi\\sim\\rho^{*}(\\xi|u),\\hat{u}\\sim p(\\hat{u}|\\xi_{1}),\\xi_{t}\\sim p_{t}(\\xi_{t}|\\xi_{1})}||v_{t,T^{\\phi_{1},\\phi_{2}}(u)}^{\\phi_{0}}(\\xi_{t})-l_{t}(\\xi_{t}|\\xi)||_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: The claims made in this paper are clearly highlighted in the abstract and introduction. The main contributions have been listed clearly in the introduction through bullet points. The results section is structured such that it justifies these claims. ", "page_idx": 40}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: Critical assumptions and practical limitations are presented in the Discussion section. ", "page_idx": 40}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: All assumptions and formulas are clearly stated, numbered, and crossreferenced in the paper with theoretical justification and detailed derivation provided in the appendix. ", "page_idx": 40}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: The experimental setup is described extensively, with model architectures and training details outlined in the appendix. Furthermore, we provide all code, data, and models (including baselines) required to reproduce the experiments outlined in this paper. ", "page_idx": 40}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: The data and code is provided in the supplementary material, uploaded with the paper submission. ", "page_idx": 40}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: Training details are described for the all experiments, with more extensive training details specified in the appendix and Supplementary Material. Additionally, all the selected model configurations are provided in the code. ", "page_idx": 40}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: Errors or scores are reported by the mean $+/.$ -standard deviation across all test samples. Furthermore, figures displaying box plots of these errors and scores are presented in the appendix. The quantities that these numbers and figures represent are clearly stated in the respective text and captions. ", "page_idx": 41}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: The amount of compute required for each of the experiments, as well as the total amount of compute hours for all experiments, is outlined in the training details reported in the appendix. ", "page_idx": 41}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: This work complies with the NeuIPS code of ethics, with special attention to the data and model documentation, access to research artifacts, and disclosure of essential elements for reproducibility. ", "page_idx": 41}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: This work is primarily focused on a broader strategy to solve problems in scientific computing, i.e. solve systems of differential equations. Direct paths to negative applications, such as deepfakes or generation of misinformation are not present. Additionally, while this work presents experiments on synthetic cardiovascular data, it does not meet the NeurIPS guidelines to immediately cause negative societal impact without a prior evaluation protocol. For health data specifically, detailed evaluation protocols issued by public health and safety organizations must be considered to meet in-silico, in-vitro, and in-vivo evaluation standards. ", "page_idx": 41}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: The data used in these experiments has a very low risk for misuse. The dataset which is released with this work is based on fluid dynamics for turbulent systems, while the other dataset used in experiments is a publicly available, synthetic dataset of cardiovascular models. ", "page_idx": 41}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: The dataset for full-body haemodynamics is open source and appropriate cited and credited. The atmospheric turbulence dataset is produced by the authors, but based on an openly available code cited in the text. Baseline models used for comparison are properly cited in the text. ", "page_idx": 41}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: The code that reproduces the results is documented and easy to read. The documentation is contained in the actual code. ", "page_idx": 42}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "page_idx": 42}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 42}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 42}]