[{"figure_path": "V4tzn87DtN/figures/figures_9_1.jpg", "caption": "Figure 1: Iteration complexity comparison for minimizing log-sum-exp on a synthetic dataset.", "description": "The figure shows the iteration complexity comparison for minimizing the regularized log-sum-exp objective function on a synthetic dataset.  Different optimization methods are compared: Stochastic Newton with uniform averaging, Stochastic Newton with weighted averaging, SNPE (Stochastic Newton Proximal Extragradient) with uniform averaging, SNPE with weighted averaging, Accelerated Gradient Descent (AGD), Damped Newton, and Newton Proximal Extragradient (NPE). The x-axis represents the number of iterations, and the y-axis represents the difference between the objective function value and its optimal value (f(x) - f*). The results are shown for three different dataset sizes (n = 50,000, 100,000, and 150,000, with d = 500 in all cases). The figure demonstrates the convergence behavior of various methods, highlighting the relative performance of SNPE compared to other methods in terms of convergence speed.", "section": "7 Numerical experiments"}, {"figure_path": "V4tzn87DtN/figures/figures_9_2.jpg", "caption": "Figure 1: Iteration complexity comparison for minimizing log-sum-exp on a synthetic dataset.", "description": "The figure shows the iteration complexity comparison for minimizing the regularized log-sum-exp function on three synthetic datasets with varying sample sizes (n = 50,000, 100,000, and 150,000) and dimension (d = 500). It compares the performance of several algorithms, including Stochastic Newton with uniform and weighted Hessian averaging (SN-UnifAvg, SN-WeightAvg), Stochastic Newton Proximal Extragradient (SNPE) with uniform and weighted Hessian averaging (SNPE-UnifAvg, SNPE-WeightAvg), Accelerated Gradient Descent (AGD), Damped Newton, and Newton Proximal Extragradient (NPE).  The y-axis represents the value of f(x) - f*, where f* is the optimal value, showing the convergence progress of the algorithms over iterations (x-axis). The plot demonstrates that SNPE methods converge faster than other methods, especially in later iterations.", "section": "7 Numerical experiments"}, {"figure_path": "V4tzn87DtN/figures/figures_27_1.jpg", "caption": "Figure 3: The effect of the extragradient step in stochastic NPE.", "description": "This figure compares the performance of the Stochastic Newton Proximal Extragradient (SNPE) method with and without the extragradient step.  The results show that removing the extragradient step generally leads to faster convergence for all three datasets (n=50,000, 100,000, and 150,000 with d=500).  The SNPE method with the extragradient step still outperforms the Stochastic Newton method from [1].", "section": "7 Numerical experiments"}]