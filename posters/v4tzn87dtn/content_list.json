[{"type": "text", "text": "Stochastic Newton Proximal Extragradient Method ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ruichen Jiang Micha\u0142 Derezin\u00b4ski Aryan Mokhtari ECE Department EECS Department ECE Department UT Austin University of Michigan UT Austin rjiang@utexas.edu derezin@umich.edu mokhtari@austin.utexas.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Stochastic second-order methods achieve fast local convergence in strongly convex optimization by using noisy Hessian estimates to precondition the gradient. However, these methods typically reach superlinear convergence only when the stochastic Hessian noise diminishes, increasing per-iteration costs over time. Recent work in [1] addressed this with a Hessian averaging scheme that achieves superlinear convergence without higher per-iteration costs. Nonetheless, the method has slow global convergence, requiring up to $\\tilde{\\mathcal{O}}(\\kappa^{2})$ iterations to reach the superlinear rate of $\\tilde{\\mathcal{O}}((1/t)^{t/2})$ , where $\\kappa$ is the problem\u2019s condition number. In this paper, we propose a novel stochastic Newton proximal extragradient method that improves these bounds, achieving a faster global linear rate and reaching the same fast superlinear rate in $\\tilde{\\mathcal{O}}(\\kappa)$ iterations. We accomplish this by extending the Hybrid Proximal Extragradient (HPE) framework, achieving fast global and local convergence rates for strongly convex functions with access to a noisy Hessian oracle. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this paper, we focus on the use of second-order methods for solving the optimization problem ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{x}\\in\\mathbb{R}^{d}}\\;f(\\mathbf{x}),\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $f:\\mathbb{R}^{d}\\,\\rightarrow\\,\\mathbb{R}$ is strongly convex and twice differentiable. There is an extensive literature on second-order methods and their fast local convergence properties; e.g., [2\u20135]. However, these results necessitate access to the exact Hessian, which can pose computational challenges. To address this issue, several studies have explored scenarios where only the exact gradient can be queried, while a stochastic estimate of the Hessian is available\u2014similar to the setting we investigate in this paper. This oracle model is commonly encountered in large-scale machine learning problems, as computing the gradient is often much less expensive than computing the Hessian, and approximating the Hessian is a more affordable approach. Specifically, consider a finite-sum minimization problem $\\begin{array}{r}{\\operatorname*{min}_{x\\in\\mathbb{R}^{d}}\\sum_{i=1}^{n}f_{i}(x)}\\end{array}$ , where $n$ denotes the number of data points and $d$ denotes the dimension of the probl em. To achieve a fast convergence rate, standard first-order methods need to compute one full gradient in each iteration, resulting in a per-iteration computational cost of $O(n d)$ . In contrast, implementing a second-order method such as damped Newton\u2019s method involves computing the full Hessian, which costs $O(n d^{2})$ . An inexact Hessian estimate can be constructed efficiently at a cost of $O(s d^{2})$ , where $s$ is the sketch size or subsampling size [1, 6]. Hence, when the number of samples $n$ significantly exceeds $d$ , the per-iteration cost of stochastic second-order methods becomes comparable to that of first-order methods. Moreover, using second-order information often reduces the number of iterations needed to converge, thereby lowering overall computational complexity. ", "page_idx": 0}, {"type": "text", "text": "A common template among stochastic second-order methods is to combine a deterministic secondorder method, such as Newton\u2019s method or cubic regularized Newton method, with techniques such as Hessian subsampling [7\u201312] or Hessian sketching [4, 6, 13] that only require a noisy estimate of the Hessian. We refer the reader to [14, 15] for recent surveys and empirical comparisons. In terms of convergence guarantees, the majority of these works, including [4, 6, 8\u201311, 13], have shown that stochastic second-order methods exhibit a global linear convergence and a local linear-quadratic convergence, either with high probability or in expectation. The linear-quadratic behavior holds when ", "page_idx": 0}, {"type": "table", "img_path": "V4tzn87DtN/tmp/c0af49ce18f7120f1497b23ebfe7977499aceb9b425b1a8a06efcd3d07fa32b8.jpg", "table_caption": ["Table 1: Comparison between Algorithm 1 and the stochastic Newton method in [1], in terms of how many iterations it takes to transition to each phase, and the convergence rates achieved. We drop constant factors as well as logarithmic dependence and $1/\\delta$ , and assume $1/\\mathrm{poly}(\\kappa)\\le\\Upsilon\\le{\\mathcal O}(\\kappa)$ . "], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\n\\|\\mathbf{x}_{t+1}-\\mathbf{x}^{*}\\|\\leq c_{1}\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|+c_{2}\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|^{2},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\mathbf{x}^{*}$ denotes the optimal solution of Problem (1) and $c_{1},c_{2}$ are constants depending on the sample/sketch size at each step. In particular, the presence of the linear term in (2) implies that the algorithm can only achieve linear convergence when the iterate is sufficiently close to the optimal solution $\\mathbf{x}^{*}$ . Consequently, as discussed in [9, 10], to achieve superlinear convergence, the coefficient $c_{1}=c_{1,t}$ needs to gradually decrease to zero as $t$ increases. However, since $c_{1}$ is determined by the magnitude of the stochastic noise in the Hessian estimate, this in turn demands the sample/sketch size to increase across the iterations, leading to a blow-up of the per-iteration computational cost. ", "page_idx": 1}, {"type": "text", "text": "The only prior work addressing this limitation and achieving a superlinear rate for a stochastic second-order method without requiring the stochastic Hessian noise to converge to zero is by [1]. It uses a weighted average of all past Hessian approximations as the current Hessian estimate. This approach reduces stochastic noise variance in the Hessian estimate, though it introduces bias to the Hessian approximation matrix. When combined with Newton\u2019s method, it was shown that the proposed method achieves local superlinear convergence with a non-asymptotic rate of $(\\Upsilon{\\sqrt{\\log(t)/t}})^{t}$ with high probability, where $\\Upsilon$ characterizes the noise level of the stochastic Hessian oracle (see Assumption 4). However, the method may require many iterations to achieve superlinear convergence. Specifically, with the uniform averaging scheme, it takes $\\tilde{\\mathcal{O}}(\\kappa^{3})$ iterations before the method starts converging superlinearly and $\\tilde{\\mathcal{O}}(\\kappa^{6}/\\Upsilon^{2})$ iterations before it reaches the final superlinear rate. Here, $\\kappa=L_{1}^{\\bar{\\prime}}/\\mu$ denotes the condition number of the function $f$ , where $L_{1}$ is the Lipschitz constant of the gradient and $\\mu$ is the strong convexity parameter. To address this, [1] proposed a weighted averaging scheme that assigns more weight to recent Hessian estimates, improving both\u221a transition points to $\\tilde{\\mathcal{O}}(\\Upsilon^{2}+\\kappa^{2})$ while achieving a slightly slower superlinear rate of ${\\mathcal{O}}(\\Upsilon\\log(t)/\\sqrt{t})$ . ", "page_idx": 1}, {"type": "text", "text": "Our contributions. In this paper, we improve the complexity of Stochastic Newton in [1] with a method that attains a superlinear rate in significantly fewer iterations. As shown in Table 1, our method requires fewer iterations for linear convergence, denoted as $\\mathcal{T}_{1}$ , by a factor of $\\kappa^{2}$ compared to [1]. Additionally, our method achieves a linear convergence rate of $(1-{\\mathcal{O}}(1/\\kappa))^{t}$ , outperforming the $(1-{\\cal O}(1/\\dot{\\kappa^{2}}))^{t}$ rate in [1]. Thus, our method reaches the local neighborhood of the optimal solution $\\mathbf{x}^{*}$ and transitions from linear to superlinear convergence faster. Specifically, the second transition point,\u221a $\\mathcal{T}_{2}$ , is smaller by a factor of $\\kappa$ in both uniform and non-uniform averaging schemes when $\\Upsilon=\\mathcal{O}(\\sqrt{\\kappa})$ . Similarly, our method\u2019s initial superlinear rate has a better dependence on $\\kappa$ , leading to fewer iterations, $\\mathcal{T}_{3}$ , to enter the final superlinear phase. To achieve this result, we use the hybrid proximal extragradient (HPE) framework [16, 17] instead of Newton\u2019s method as the base algorithm. The HPE framework provides a principled approach for designing second-order methods with superior global convergence guarantees [3, 17\u201320]. However, [16] and subsequent works focus on cases where $f$ is merely convex, not leveraging strong convexity. Thus, we modify the HPE framework to suit our setting. Specifically, we relax the error condition for computing the proximal step in HPE, enabling a larger step size when the iterate is close to the optimal solution, crucial for achieving the final superlinear convergence rate. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we formally present our assumptions. ", "page_idx": 2}, {"type": "text", "text": "Assumption 1. The function $f$ is twice differentiable and $\\mu$ -strongly convex. ", "page_idx": 2}, {"type": "text", "text": "Assumption 2. The Hessian $\\nabla^{2}f$ satisfies $\\|\\nabla^{2}f(\\mathbf{x})-\\nabla^{2}f(\\mathbf{y})\\|\\le M_{1}$ . ", "page_idx": 2}, {"type": "text", "text": "Assumption 3. The Hessian $\\nabla^{2}f$ is $L_{2}$ -Lipschitz, i.e., $\\|\\nabla^{2}f({\\bf x})-\\nabla^{2}f({\\bf y})\\|\\le L_{2}\\|{\\bf x}-{\\bf y}\\|_{2}.$ ", "page_idx": 2}, {"type": "text", "text": "Assumption 2 is more general than the assumption that $\\nabla f$ is $L_{1}$ -Lipschitz. In particular, if the latter assumption holds, then $M_{1}\\leq L_{1}$ . Moreover, we define $\\kappa\\triangleq M_{1}/\\mu$ as the condition number. ", "page_idx": 2}, {"type": "text", "text": "To simplify our notation, we denote the exact gradient $\\nabla f(\\mathbf{x})$ and the exact Hessian $\\nabla^{2}f(\\mathbf{x})$ of the objective function by $\\mathbf{g}(\\mathbf{x})$ and $\\mathbf{H}(\\mathbf{x})$ , respectively. As mentioned earlier, we assume that we have access to the exact gradient, but we only have access to a noisy estimate of the Hessian denoted by $\\hat{\\bf H}({\\bf x})$ . In fact, we require a mild assumption on the Hessian noise. We define the stochastic Hessian noise as $\\mathbf{E}(\\mathbf{x})\\triangleq\\hat{\\mathbf{H}}(\\mathbf{x})-\\mathbf{H}(\\mathbf{x})$ , where it is assumed to be mean zero and sub-exponential. ", "page_idx": 2}, {"type": "text", "text": "Assumption 4. If we define $\\mathbf{E}(\\mathbf{x})\\triangleq\\hat{\\mathbf{H}}(\\mathbf{x})-\\mathbf{H}(\\mathbf{x}),$ , then $\\mathbb{E}[\\mathbf{E}(\\mathbf{x})]=0$ and $\\mathbb{E}[\\|\\mathbf{E}(\\mathbf{x})\\|^{p}]\\leq p!\\Upsilon_{E}^{p}/2$ for all integers $p\\geq2$ . Also, define $\\Upsilon\\triangleq\\Upsilon_{E}/\\mu$ to be the relative noise level. ", "page_idx": 2}, {"type": "text", "text": "Assumption 5. The Hessian approximation matrix is positive semi-definite, i.e., $\\hat{\\mathbf{H}}(\\mathbf{x})\\succeq0,\\forall\\mathbf{x}\\in\\mathbb{R}^{d}$ . ", "page_idx": 2}, {"type": "text", "text": "Stochastic Hessian construction. The two most popular approaches to construct stochastic Hessian approximations are \u201csubsampling\u201d and \u201csketching\u201d. Hessian subsampling is designed for a finite-sum objective of the form $\\begin{array}{r}{f(\\mathbf{x})\\stackrel{\\cdot}{=}\\frac{1}{n}\\sum_{i=1}^{n}f_{i}(\\mathbf{x})}\\end{array}$ , where $n$ is the number of samples. In each iteration, a subset $S\\,\\subset\\,\\{1,2,\\ldots,n\\}$ is drawn uniformly at random, and then the subsampled Hessian at $\\mathbf{x}$ is constructed as $\\begin{array}{r}{\\hat{\\mathbf{H}}(\\mathbf{x})=\\frac{1}{|S|}\\sum_{i\\in S}\\nabla^{2}f_{i}(\\mathbf{x})}\\end{array}$ . In this case, if each $f_{i}$ is convex, then the condition in Assumption 5 is satisfied. Moreover, if we further assume that $\\|\\nabla^{2}f_{i}(\\mathbf{x})\\|\\,\\leq\\,c M_{1}$ for some $c>0$ and for all $i$ , then Assumption 4 is satisfied with $\\Upsilon=\\mathcal{O}(\\sqrt{c\\kappa\\log(d)/|S|}+c\\kappa\\log(d)/|S|)$ (see [1, Example 1]). The other approach is Hessian sketching, applicable when the Hessian H can be easily factorized as $\\mathbf{H}=\\mathbf{M}^{\\top}\\mathbf{M}$ , where $\\mathbf{M}\\in\\mathbb{R}^{n\\times d}$ is the square-root Hessian matrix, and $n$ is the number of samples. This is the case for generalized linear models; see [1]. To form the sketched Hessian, we draw a random sketch matrix $\\mathbf{S}\\in\\mathbb{R}^{s\\times n}$ with sketch size $s$ from a distribution $\\mathcal{D}$ that satisfies $\\mathbb{E}_{\\mathcal{D}}[\\mathbf{S}^{\\top}\\mathbf{S}]\\,=\\,\\mathbf{I}$ . The sketched Hessian is then $\\hat{\\mathbf{H}}\\,=\\,\\mathbf{M}^{\\top}\\mathbf{S}^{\\top}\\mathbf{S}\\mathbf{M}$ . In this case, Assumption 5 is automatically satisfied. Moreover, for Gaussian sketch, Assumption 4 is satisfied with $\\Upsilon=\\mathcal{O}(\\kappa(\\sqrt{d/s}+d/s))$ (see [1, Example 2]). ", "page_idx": 2}, {"type": "text", "text": "Remark 1. The above assumptions are common in the study of stochastic second-order methods, appearing in works on Subsampled Newton [7\u201310, 12], Newton Sketch [4, 6, 13], and notably, [1]. The strong convexity requirement is crucial as stochastic second-order methods have a clear advantage over first-order methods like gradient descent when the function is strongly convex. Specifically, stochastic second-order methods attain a superlinear convergence rate, as shown in this paper, which is superior to the linear rate of first-order methods. ", "page_idx": 2}, {"type": "text", "text": "3 Stochastic Newton Proximal Extragradient ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our approach involves developing a stochastic Newton-type method grounded in the Hybrid Proximal Extragradient (HPE) framework and its second-order variant. Therefore, before introducing our proposed algorithm, we will provide a brief overview of the core principles of the HPE framework. Following this, we will present our method as it applies to the specific setting addressed in this paper. ", "page_idx": 2}, {"type": "text", "text": "Hybrid Proximal Extragradient. Next, we first present the Hybrid Proximal Extragradient (HPE) framework for strongly convex functions. To solve problem (1), the HPE algorithm consists of two steps. In the first step, given $\\mathbf{x}_{t}$ , we find a mid-point $\\hat{\\mathbf{x}}_{t}$ by applying an inexact proximal point update $\\hat{\\mathbf{x}}_{t}\\approx\\mathbf{x}_{t}-\\eta_{t}\\nabla f(\\hat{\\mathbf{x}}_{t})$ , where $\\eta_{t}$ is the step size. More precisely, we require ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\|\\hat{{\\mathbf x}}_{t}-{\\mathbf x}_{t}+\\eta_{t}\\nabla f(\\hat{{\\mathbf x}}_{t})\\|\\le\\alpha\\sqrt{\\gamma_{t}}\\|\\hat{{\\mathbf x}}_{t}-{\\mathbf x}_{t}\\|,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\gamma_{t}=1\\!+\\!2\\eta_{t}\\mu,\\mu$ is the strong convexity parameter, and $\\alpha\\in(0,1)$ is a user-specified parameter. Then, in the second step, we perform the extra-gradient update and compute $\\mathbf{x}_{t+1}$ based on ", "page_idx": 2}, {"type": "equation", "text": "$$\n{\\bf x}_{t+1}=\\frac{1}{\\gamma_{t}}({\\bf x}_{t}-\\eta_{t}\\nabla f(\\hat{\\bf x}_{t}))+\\Big(1-\\frac{1}{\\gamma_{t}}\\Big)\\hat{\\bf x}_{t},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The weights $\\textstyle{\\frac{1}{\\gamma_{t}}}$ in the above convex combination are chosen to optimize the convergence rate. ", "page_idx": 3}, {"type": "text", "text": "Remark 2. When $\\mu=0$ , the algorithm outline above reduces to the original HPE framework studied in [16, 18]. Our modification in (3) is inspired by [21] and allows a larger error when performing the inexact proximal point update, which turns out to be crucial for achieving a fast superlinear convergence rate. Moreover, the modification in (4) has been adopted in [22]. ", "page_idx": 3}, {"type": "text", "text": "Stochastic Newton Proximal Extragradient (SNPE). The HPE method described above provides a useful algorithmic framework, instead of a directly implementable method. The main challenge comes from implementing the first step in (3), which involves an inexact proximal point update. Specifically, the naive approach is to solve the implicit nonlinear equation $\\mathbf{x}-\\mathbf{x}_{t}+\\eta_{t}\\nabla f(\\mathbf{x})=0$ , which can be as costly as solving the original problem in (1). To address this issue, [18] proposed to approximate the gradient operator $\\nabla f(\\mathbf{x})$ by its local linearization $\\nabla f({\\bf x}_{t})+\\nabla^{2}f(\\bar{\\bf x_{t}})(\\bar{\\bf x_{-}}-\\bar{\\bf x_{t}})$ , and then compute $\\hat{\\mathbf{x}}_{t}$ by solving the linear system of equations $\\hat{\\mathbf{x}}_{t}-\\mathbf{x}_{t}+\\eta_{t}(\\nabla f(\\mathbf{x}_{t})+\\nabla^{2}f(\\mathbf{x}_{t})(\\hat{\\mathbf{x}}_{t}-\\mathbf{x}_{t}))=0$ . This leads to the Newton proximal extragradient method that was proposed and analyzed in [18]. ", "page_idx": 3}, {"type": "text", "text": "However, in our setting, the exact Hessian $\\nabla^{2}f(\\mathbf{x}_{t})$ is not available. Thus, we construct a stochastic Hessian approximation $\\tilde{\\mathbf{H}}_{t}$ from our noisy Hessian oracle as a surrogate of $\\nabla^{2}f(\\mathbf{x}_{t})$ . We will elaborate on the construction of $\\tilde{\\mathbf{H}}_{t}$ later, but for the present discussion assume that this stochastic Hessian approximation $\\tilde{\\mathbf{H}}_{t}$ is already provided. Then in the first step, we will compute $\\hat{\\mathbf{x}}_{t}$ by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{x}}_{t}=\\mathbf{x}_{t}-\\eta_{t}\\bigl(\\nabla f\\bigl(\\mathbf{x}_{t}\\bigr)+\\tilde{\\mathbf{H}}_{t}\\bigl(\\hat{\\mathbf{x}}_{t}-\\mathbf{x}_{t}\\bigr)\\bigr),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where we replace $\\nabla f(\\hat{{\\bf x}}_{t})$ by its local linear approximation $\\nabla f(\\mathbf{x}_{t})+\\tilde{\\mathbf{H}}_{t}\\big(\\hat{\\mathbf{x}}_{t}-\\mathbf{x}_{t}\\big)$ . Moreover, (5) is equivalent to solving the following linear system of equations $(\\mathbf{I}+\\eta_{t}\\tilde{\\mathbf{H}}_{t})(\\mathbf{x}-\\mathbf{x}_{t})=-\\eta_{t}\\nabla f(\\mathbf{x}_{t})$ For ease of presentation, we set $\\hat{\\mathbf{x}}_{t}$ as the exact solution of this system, leading to ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{x}}_{t}=\\mathbf{x}_{t}-\\eta_{t}(\\mathbf{I}+\\eta_{t}\\tilde{\\mathbf{H}}_{t})^{-1}\\nabla f(\\mathbf{x}_{t}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "However, we note that an inexact solution to this linear system is also sufficient for our convergence guarantees so long as $\\begin{array}{r}{\\|(\\mathbf{I}+\\eta_{t}\\tilde{\\mathbf{H}}_{t})(\\hat{\\mathbf{x}}_{t}-\\mathbf{x}_{t})+\\eta_{t}\\nabla f(\\dot{\\mathbf{x}_{t}})\\|\\,\\le\\,\\frac{\\alpha}{2}\\|\\hat{\\mathbf{x}}_{t}-\\mathbf{x}_{t}\\|}\\end{array}$ ; We refer the reader to Appendix A.3 for details. Additionally, since we employed a linear approximation to determine the mid-point $\\hat{\\mathbf{x}}_{t}$ , the condition in (3) may no longer be satisfied. Consequently, it is crucial to verify the accuracy of our approximation after selecting $\\hat{\\mathbf{x}}_{t}$ . To achieve this, we implement a line-search scheme to ensure that the step size is not large and the linear approximation error is small. ", "page_idx": 3}, {"type": "text", "text": "Next, we discuss constructing the stochastic Hessian approximation ${\\tilde{\\bf H}}_{t}$ . A simple strategy is using $\\hat{\\mathbf{H}}(\\mathbf{x}_{t})$ instead of $\\nabla^{2}f(\\mathbf{x}_{t})$ , but the Hessian noise would lead to a highly inaccurate approximation of the prox operator, ruining the superlinear convergence rate. To reduce Hessian noise, we follow [1] and use an averaged Hessian estimate $\\tilde{\\mathbf{H}}(\\mathbf{x}_{t})$ . We consider two schemes: (i) uniform averaging; (ii) nonuniform averaging with general weights. In the first case, $\\begin{array}{r}{\\tilde{\\mathbf{H}}_{t}=\\frac{1}{t+1}\\sum_{i=0}^{t}\\hat{\\mathbf{H}}(\\mathbf{x}_{t})}\\end{array}$ uniformly averages past stochastic Hessian approximations. Motivated by the central limit theorem for martingale differences, we expect $\\tilde{\\mathbf{H}}_{t}$ to have smaller variance than $\\hat{\\mathbf{H}}(\\mathbf{x}_{t})$ . It can be implemented online as $\\begin{array}{r}{\\tilde{\\mathbf{H}}_{t}=\\frac{t}{t+1}\\tilde{\\mathbf{H}}_{t-1}+\\frac{1}{t+1}\\hat{\\mathbf{H}}(\\mathbf{x}_{t})}\\end{array}$ , without storing past Hessian estimates. However, $\\tilde{\\mathbf{H}}(\\mathbf{x}_{t})$ is a biased estimator of $\\nabla^{2}f(\\mathbf{x}_{t})$ , since it incorporates stale Hessian information. To address the bias-variance trade-off, the second case uses non-uniform averaging to weight recent Hessian estimates more. Given an increasing non-negative weight sequence $\\{w_{t}\\}_{t=-1}^{\\infty}$ with $w_{-1}\\!=\\!0$ , the running average is: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\tilde{\\mathbf{H}}_{t}=\\frac{w_{t-1}}{w_{t}}\\tilde{\\mathbf{H}}_{t-1}+\\Big(1-\\frac{w_{t-1}}{w_{t}}\\Big)\\hat{\\mathbf{H}}(\\mathbf{x}_{t}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Equivalently, with $\\begin{array}{r}{z_{i,t}\\,=\\,\\frac{w_{i}-w_{i-1}}{w_{t}}}\\end{array}$ , $\\tilde{\\mathbf{H}}_{t}$ can be written as $\\begin{array}{r}{\\sum_{i=0}^{t}z_{i,t}\\hat{\\bf H}({\\bf x}_{i})}\\end{array}$ . We discuss uniform averaging in Section 4 and non-uniform averaging in Section 5. ", "page_idx": 3}, {"type": "text", "text": "Building on the discussion thus far, we are ready to integrate all the components and present our Stochastic Newton Proximal Extragradient (SNPE) method. The steps of SNPE are summarized in Algorithm 1. Each iteration of our SNPE method includes two stages. In the first stage, starting with the current point $\\mathbf{x}_{t}$ , we first query the noisy Hessian oracle and compute the averaged stochastic Hessian $\\tilde{\\mathbf{H}}_{t}$ from (7), as stated in Step 4. Then given the gradient $\\nabla f(\\mathbf{x}_{t})$ , the Hessian approximation ${\\tilde{\\bf H}}_{t}$ , and an initial trial step size $\\sigma_{t}$ , we employ a backtracking line search to obtain $\\eta_{t}$ and $\\hat{\\mathbf{x}}_{t}$ , as stated in Step 6 of Algorithm 1. Specifically, in this step, we set $\\eta_{t}\\gets\\sigma_{t}$ and compute $\\hat{\\mathbf{x}}_{t}$ as suggested in (6). If $\\hat{\\mathbf{x}}_{t}$ and its corresponding step size $\\eta_{t}$ satisfy (3), meaning the linear approximation error is small, then the step size $\\eta_{t}$ and the mid-point $\\hat{\\mathbf{x}}_{t}$ are accepted and we proceed to the second stage of SNPE. If not, we backtrack the step size $\\eta_{t}$ and try a smaller step size $\\beta\\eta_{t}$ , where $\\beta\\in(0,1)$ is a user-specified parameter. We repeat the process until the condition in (3) is satisfied. The details of the backtracking line search scheme are summarized in Subroutine 1. After completing the first stage and obtaining the pair $(\\eta_{t},\\hat{{\\mathbf x}}_{t})$ , we proceed to the extragradient step and follow the update in (4), as in Step 7 of Algorithm 1. Finally, before moving to the next time index, we follow a warm-start strategy and set the next initial trial step size $\\sigma_{t+1}$ as $\\eta_{t}/\\beta$ , as shown in Step 8 of Algorithm 1. ", "page_idx": 3}, {"type": "table", "img_path": "V4tzn87DtN/tmp/a7a6b0728180e856798235c18fd6f594a75b6c48cf95b900a705b02451fd7a69.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Remark 3. Similar to the analysis in [22], we can show that the total number of line search steps after $t$ iterations can be bounded by $\\textstyle2t-1+\\log\\!\\left(\\frac{\\sigma_{0}}{\\eta_{t-1}}\\right)$ . Moreover, when $t$ is large enough, on average the line search requires 2 steps per iteration. We defer the details to Appendix A.4. ", "page_idx": 4}, {"type": "text", "text": "Remark 4. Our motivation behind the choice $\\sigma_{t+1}=\\eta_{t}/\\beta$ is to allow the step size to grow, which is necessary for achieving a superlinear convergence rate. Specifically, as shown in Proposition 1 below, we require the step size $\\eta_{t}$ to go to infinity to ensure that $\\begin{array}{r}{\\operatorname*{lim}_{t\\rightarrow\\infty}\\frac{\\|\\mathbf x_{t+1}-\\mathbf x^{*}\\|}{\\|\\mathbf x_{t}-\\mathbf x^{*}\\|}\\,=\\,0}\\end{array}$ . Note that this would not be possible if we simply set $\\sigma_{t+1}=\\eta_{t}$ , since it would automatically result in $\\eta_{t+1}\\leq\\sigma_{t+1}\\leq\\eta_{t}$ . Moreover, this condition $\\sigma_{t+1}=\\eta_{t}/\\beta$ is explicitly utilized in Lemmas 8 and 16 in the Appendix, where we demonstrate that $\\eta_{t}$ can be lower bounded by the minimum of $\\sigma_{0}/\\beta^{t}$ and another term. We should also note that this more aggressive choice of the initial step size at each round could potentially increase the number of backtracking steps. However, as mentioned above, this does not cause a significant issue, since the average number of backtracking steps per iteration can be bounded by a constant close to 2. ", "page_idx": 4}, {"type": "text", "text": "3.1 Key properties of SNPE ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "This section outlines key properties of SNPE, applied in Sections 4 and 5 to determine its convergence rates. The first result reveals the connection between SNPE\u2019s convergence rate and the step size $\\eta_{t}$ . ", "page_idx": 4}, {"type": "text", "text": "Proposition 1. Let $\\{\\mathbf{x}_{t}\\}_{t\\ge0}$ and $\\{\\hat{\\mathbf{x}}_{t}\\}_{t\\ge0}$ be the iterates generated by Algorithm 1. Then for any $t\\geq0$ , we have $\\|\\mathbf{x}_{t+1}-\\mathbf{x}^{\\overline{{*}}}\\|^{2}\\leq\\|\\mathbf{x}_{t}-\\overline{{\\mathbf{x}}}^{*}\\|^{2}(1+2\\eta_{t}\\mu)^{-1}$ . ", "page_idx": 4}, {"type": "text", "text": "Proposition 1 guarantees that the distance to the optimal solution is monotonically decreasing, and it shows a larger step size implies faster convergence. Hence, we need to provide an explicit lower bound on the step size. This task is accomplished in the next lemma. For ease of notation, we let $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ be the set of iteration indices where the line search scheme backtracks, i.e., $B\\triangleq\\{t:\\eta_{t}<\\sigma_{t}\\}$ . Moreover, we use $\\mathbf{g}(\\mathbf{x})$ and $\\mathbf{H}(\\mathbf{x})$ to denote the gradient $\\nabla f(\\mathbf{x})$ and the Hessian $\\nabla^{2}f(\\mathbf{x})$ , respectively. ", "page_idx": 4}, {"type": "text", "text": "Lemma 1. For $t\\,\\notin\\,B$ , we have $\\eta_{t}~=~\\sigma_{t}$ . For $t\\ \\in\\ B$ , let $\\tilde{\\eta}_{t}\\,=\\,\\eta_{t}/\\beta$ and $\\tilde{\\mathbf{x}}_{t}\\;=\\;\\mathbf{x}_{t}\\;-\\;\\tilde{\\eta}_{t}(\\mathbf{I}+$ $\\tilde{\\eta}_{t}\\tilde{\\mathbf{H}}_{t})^{-1}\\nabla f(\\mathbf{x}_{t})$ . Then, $\\begin{array}{r}{\\|\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t}\\|\\leq\\frac{1}{\\beta}\\|\\hat{\\mathbf{x}}_{t}-\\mathbf{x}_{t}\\|}\\end{array}$ . Moreover, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\eta_{t}\\geq\\operatorname*{max}\\Biggl\\{\\frac{\\alpha\\beta\\|\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t}\\|}{\\|\\mathbf{g}(\\tilde{\\mathbf{x}}_{t})-\\mathbf{g}(\\mathbf{x}_{t})-\\tilde{\\mathbf{H}}_{t}(\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t})\\|},\\frac{2\\alpha^{2}\\beta\\mu\\|\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t}\\|^{2}}{\\|\\mathbf{g}(\\tilde{\\mathbf{x}}_{t})-\\mathbf{g}(\\mathbf{x}_{t})-\\tilde{\\mathbf{H}}_{t}(\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t})\\|^{2}}\\Biggr\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "As Lemma 1 demonstrates, in the first case where $t\\not\\in\\mathcal{B}$ , we have $\\eta_{t}=\\sigma_{t}$ . Moreover, since we set $\\sigma_{t}=\\eta_{t-1}/\\beta$ for $t\\geq1$ , in this case the step size will increase by a factor of $1/\\beta$ . In the second case that $t\\in{\\boldsymbol{B}}$ , our lower bound on the step size $\\eta_{t}$ depends inversely on the normalized approximation error $\\begin{array}{r}{\\mathcal{E}_{t}=\\frac{\\Vert\\mathbf{g}(\\tilde{\\mathbf{x}}_{t})-\\mathbf{g}(\\mathbf{x}_{t})-\\tilde{\\mathbf{H}}_{t}\\left(\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t}\\right)\\Vert}{\\Vert\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t}\\Vert}}\\end{array}$ . Also, note that $\\mathcal{E}_{t}$ involves an auxiliary iterate $\\tilde{{\\bf x}}_{t}$ instead of the actual iterate $\\hat{\\mathbf{x}}_{t}$ accepted by our line search. We use the first result to relate $\\|\\tilde{{\\bf x}}_{t}-{\\bf x}_{t}\\|$ to $\\left\\|\\hat{\\mathbf x}-\\mathbf x_{t}\\right\\|$ . To shed light on our analysis, we use the triangle inequality and decompose this error into two terms: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{E}_{t}\\leq\\frac{\\|\\mathbf{g}(\\tilde{\\mathbf{x}}_{t})-\\mathbf{g}(\\mathbf{x}_{t})-\\mathbf{H}_{t}(\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t})\\|}{\\|\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t}\\|}+\\|\\mathbf{H}_{t}-\\tilde{\\mathbf{H}}_{t}\\|.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The first term in (8) represents the intrinsic error from the linear approximation in the inexact proximal update, while the second term arises from the Hessian approximation error. Using the smoothness properties of $f$ , we can upper bound the first term, as shown in the following lemma. ", "page_idx": 5}, {"type": "text", "text": "Lemma 2. Under Assumptions 2 and 3, we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{\\|\\mathbf{g}(\\widetilde{\\mathbf{x}}_{t})-\\mathbf{g}(\\mathbf{x}_{t})-\\mathbf{H}_{t}(\\widetilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t})\\|}{\\|\\widetilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t}\\|}\\leq\\operatorname*{min}\\left\\{M_{1},\\frac{L_{2}\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|}{2\\beta\\sqrt{1-\\alpha^{2}}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Lemma 2 shows that the linear approximation error is upper bounded by $M_{1}$ . Moreover, the second upper bound is $O(\\|\\mathbf x_{t}-\\mathbf x^{*}\\|)$ . Thus, as Algorithm 1 converges to the optimal solution $\\mathbf{x}^{*}$ , the second bound in (9) will become tighter than the first one, and the right hand side approaches zero. ", "page_idx": 5}, {"type": "text", "text": "To analyze the second term in (8), we isolate the noise component in our averaged Hessian estimate. Specifically, recall $\\begin{array}{r}{\\tilde{\\mathbf{H}}_{t}=\\sum_{i=0}^{t}z_{i,t}\\hat{\\mathbf{H}}_{i}}\\end{array}$ and $\\hat{\\bf H}_{i}={\\bf H}_{i}+{\\bf E}_{i}$ . Thus, we have $\\tilde{\\mathbf{H}}_{t}=\\bar{\\mathbf{H}}_{t}+\\bar{\\mathbf{E}}_{t}$ , where $\\begin{array}{r}{\\bar{\\mathbf{H}}_{t}=\\sum_{i=0}^{t}z_{i,t}\\mathbf{H}_{i}}\\end{array}$ is the aggregated Hessian and $\\begin{array}{r}{\\bar{\\mathbf{E}}_{t}=\\sum_{i=0}^{t}z_{i,t}\\mathbf{E}_{i}}\\end{array}$ is the aggregated Hessian noise, and it follows from the triangle inequality that $\\|\\mathbf{H}_{t}-\\tilde{\\mathbf{H}}_{t}\\|\\leq\\|\\mathbf{H}_{t}-\\bar{\\mathbf{H}}_{t}\\|+\\|\\bar{\\mathbf{E}}_{t}\\|$ . We refer to the first part, $\\left\\|\\mathbf{H}_{t}-\\bar{\\mathbf{H}}_{t}\\right\\|$ , as the bias of our Hessian estimate, and the second part, $\\left\\|\\mathbf{E}_{t}\\right\\|$ , as the averaged stochastic error. There is an intrinsic trade-off between the two error terms. For the fastest error concentration, we assign equal weights to all past stochastic Hessian noises, i.e., $z_{i,t}=1/(t+1)$ for all $0\\leq i\\leq t$ , corresponding to the uniform averaging scheme discussed in Section 4. To eliminate bias, we assign all weights to the most recent Hessian matrix $\\mathbf{H}_{t}$ , i.e., $z_{t,t}=1$ and $z_{i,t}=0$ for all $i<t$ , but this incurs a large stochastic error. To balance these, we present a weighted averaging scheme in Section 5, gradually assigning more weight to recent stochastic Hessian approximations. ", "page_idx": 5}, {"type": "text", "text": "4 Analysis of uniform Hessian averaging ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we present the convergence analysis of the uniform Hessian averaging scheme, where $w_{t}\\,=\\,t+1$ . In this case, we have $\\begin{array}{r}{\\mathbf{\\bar{H}}_{t}\\,=\\,\\frac{1}{t+1}\\sum_{i=0}^{t}\\mathbf{\\hat{H}}_{i}}\\end{array}$ . As discussed in Section 3.1, our main task is to lower bound the step size $\\eta_{t}$ , which requires us to control the approximation error $\\mathcal{E}_{t}$ by analyzing the two error terms in (8). The first term is bounded by Lemma 2, and the second term can be bounded as $\\|\\mathbf{H}_{t}-\\tilde{\\mathbf{H}}_{t}\\|\\leq\\|\\mathbf{H}_{t}-\\bar{\\mathbf{H}}_{t}\\|+\\|\\bar{\\mathbf{E}}_{t}\\|$ . Next, we establish a bound on $\\|\\bar{\\mathbf E}_{t}\\|$ , referred to as the Averaged Stochastic Error, and a bound on $\\lVert\\mathbf{H}_{t}-\\tilde{\\mathbf{H}}_{t}\\rVert$ , referred to as the Bias Term. ", "page_idx": 5}, {"type": "text", "text": "Averaged stochastic error. To control the averaged Hessian noise $\\left\\|\\bar{\\mathbf{E}}_{t}\\right\\|$ , we rely on the concentration of sub-exponential martingale difference, as shown in [1]. ", "page_idx": 5}, {"type": "text", "text": "Lemma 3 ([1, Lemma 2]). Let $\\delta\\in(0,1)$ with $d/\\delta\\geq e$ . Then with probability $1-\\delta\\pi^{2}/6,$ , we have $\\begin{array}{r}{\\|\\bar{\\mathbf{E}}_{t}\\|\\leq8\\Upsilon_{E}\\sqrt{\\frac{\\log\\left(d(t+1)/\\delta\\right)}{t+1}}}\\end{array}$ for any $t\\geq4\\log(d/\\delta)$ . ", "page_idx": 5}, {"type": "text", "text": "Lemma 3 shows that, w\u221aith high probability, the norm of averaged Hessian noise $\\|\\bar{\\mathbf{E}}_{t}\\|$ approaches zero at the rate of $\\tilde{\\mathcal{O}}(\\Upsilon_{E}/\\sqrt{t})$ . As discussed in Section 4.1, this error eventually becomes the dominant factor in the approximation error $\\mathcal{E}_{t}$ and determines the final superlinear rate of our algorithm. Remark 5. Our subsequent results are conditioned on the event that the bound on $\\|\\bar{\\mathbf{E}}_{t}\\|$ stated in Lemma 3 is satisfied for all $t\\geq4\\log(d/\\delta)$ . Thus, to avoid redundancy, we will omit the \u201cwith high probability\u201d qualification in the following discussion. ", "page_idx": 5}, {"type": "text", "text": "Bias. We proceed to establish an upper bound on $\\left\\|\\mathbf{H}_{t}-\\bar{\\mathbf{H}}_{t}\\right\\|$ . The proof can be found in Appendix B.1. ", "page_idx": 5}, {"type": "text", "text": "Lemma 4. If $\\begin{array}{r}{\\tilde{\\mathbf{H}}_{t}=\\frac{1}{t+1}\\sum_{i=0}^{t}\\hat{\\mathbf{H}}_{i}}\\end{array}$ , then $\\begin{array}{r}{\\|\\mathbf{H}_{t}-\\bar{\\mathbf{H}}_{t}\\|\\leq\\frac{1}{t+1}\\sum_{i=0}^{t}\\|\\mathbf{H}_{t}-\\mathbf{H}_{i}\\|}\\end{array}$ . Moreover, for any $i\\geq0$ , we have $\\|\\mathbf{H}_{t}-\\mathbf{H}_{i}\\|\\leq\\operatorname*{max}\\{M_{1},2L_{2}\\|\\mathbf{x}_{i}-\\mathbf{x}^{*}\\|\\}$ . ", "page_idx": 5}, {"type": "text", "text": "The analysis of the bias term is more complicated. Specifically, to obtain the best result, we break the sum in Lemma 4 into two parts, $\\begin{array}{r l}{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\frac{1}{t}\\sum_{i=0}^{\\hat{\\mathcal{I}}-1}\\big\\|\\mathbf{H}_{t}-\\mathbf{\\dot{H}}_{i}\\big\\|}&{{}}\\end{array}$ and $\\begin{array}{r}{\\frac{1}{t}\\sum_{i=\\mathcal{T}}^{t}\\|\\mathbf{H}_{t}-\\mathbf{H}_{i}\\|}\\end{array}$ , where $\\mathcal{T}$ is an integer to be specified later. The first part corresponds to the bias from stale Hessian information and converges to zero at $\\mathcal{O}(M_{1}\\mathcal{Z}/t)$ , as shown by the first bound in Lemma 4. The second part is the bias from recent Hessian information when the iterates are near the optimal solution $\\mathbf{x}^{*}$ . Using the second bound in Lemma 4, we show this part contributes less to the total bias and is dominated by the first part. Thus, we can conclude that $\\begin{array}{r}{\\|\\mathbf{H}_{t}-\\bar{\\mathbf{H}}_{t}\\|=\\mathcal{O}(\\frac{M_{1}\\mathcal{T}}{t+1})}\\end{array}$ . ", "page_idx": 6}, {"type": "text", "text": "Based on the previous discussions, it is evident that the terms contributing to the upper bound of $\\mathcal{E}_{t}$ all converge to zero, albeit at different rates. Furthermore, the linear approximation error and bias term display distinct global and local convergence patterns, depending on the distance $\\lVert\\mathbf x_{t}-\\mathbf x^{*}\\rVert$ . Hence, this necessitates a multi-phase convergence analysis, which we undertake in the following section. ", "page_idx": 6}, {"type": "text", "text": "4.1 Convergence analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Similar to [1], we consider four convergence phases with three transitions points $\\tau_{1},\\tau_{2}$ , and $\\tau_{3}$ , whose expressions will be specified later. Due to space limitations, in the following we provide an overview of the four phases and relegate the details to Appendix B. ", "page_idx": 6}, {"type": "text", "text": "Warm-up phase $0\\leq t<\\tau_{1}$ . At the beginning of the algorithm, the averaged Hessian estimate is dominated by stochastic noise and provides little useful information for convergence. Thus, there are generally no guarantees on the convergence rate for $0\\leq t<\\tau_{1}$ . However, due to the line search scheme, Proposition 1 ensures that the distance to $\\mathbf{x}^{*}$ is non-increasing, i.e., $\\|\\mathbf{x}_{t+1}-\\mathbf{x}^{*}\\|\\leq\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|$ for all $t\\geq0$ . During the warm-up phase, the averaged Hessian noise $\\left\\Vert\\bar{\\mathbf{E}}_{t}\\right\\Vert$ , which contributes most to the approximation error $\\mathcal{E}_{t}$ , is gradually suppressed. Once the averaged Hessian noise is sufficiently concentrated, Algorithm 1 transitions to the linear convergence phase, denoted by $\\mathcal{T}_{1}$ . ", "page_idx": 6}, {"type": "text", "text": "Linear convergence phase $\\tau_{1}\\leq t<\\tau_{2}$ . After $\\mathcal{T}_{1}$ iterations, Algorithm 1 starts converging linearly to the optimal solution $\\mathbf{x}^{*}$ . Moreover, during this phase, all the three errors discussed in Section 3.1 continue to decrease. Specifically, Lemma 2 shows the linear approximation error is bounded by $\\mathcal{O}(\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|)$ , which converges to zero at a linear rate. Furthermore, Lemma 3 implies that the averaged Hessian error $\\left\\|\\bar{\\mathbf{E}}_{t}\\right\\|$ diminishes at a rate of $\\tilde{\\mathcal{O}}(\\frac{\\Upsilon_{E}}{\\sqrt{t}})$ . Finally, regarding the bias term, it can be shown $\\begin{array}{r}{\\|{\\mathbf{H}}_{t}-\\bar{{\\mathbf{H}}}_{t}\\|=\\mathcal{O}(\\frac{M_{1}\\mathcal{T}}{t})}\\end{array}$ following the discussions after Lemma 4. Thus, once all the three errors are sufficiently small, Algorithm 1 moves to the superlinear phase, denoted by $\\mathcal{T}_{2}$ . ", "page_idx": 6}, {"type": "text", "text": "Superlinear phases $\\tau_{2}\\leq t<\\tau_{3}$ and $\\tau_{3}\\leq t<\\tau_{4}$ . After $\\mathcal{T}_{2}$ iterations, Algorithm 1 converges at a superlinear rate. Moreover, the superlinear rate is determined by the averaged noise $\\|\\bar{\\mathbf E}_{t}\\|$ , which decays at the rate of $\\tilde{\\mathcal{O}}(\\frac{\\Upsilon_{E}}{\\sqrt{t}})$ , and the bias of our averaged Hessian estimate $\\bar{\\tilde{\\mathbf{H}}_{t}}$ , which decays at the rate of $\\ O\\big(\\frac{M_{1}\\mathbb{Z}}{t}\\big)$ . Hence, as the number of iterations $t$ increases, the averaged noise will dominate and the algorithm transitions from the initial superlinear rate to the final superlinear rate. ", "page_idx": 6}, {"type": "text", "text": "We summarize our convergence guarantees in the following theorem and the proofs are in Appendix B. ", "page_idx": 6}, {"type": "text", "text": "Theorem 1. Suppose Assumptions 1-5 hold and the weights for Hessian averaging in SNPE are   \nuniform, and define 2\u03b2\u221a11\u2212\u03b12 + 5. Then, the followings hold: (a) Warm-up phase: If ${}^{\\mathrm{c}}0\\leq t<\\tau_{1}$ , then $\\|\\mathbf{x}_{t+1}-\\mathbf{x}^{*}\\|\\leq\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|$ , where $\\begin{array}{r}{\\mathcal{T}_{1}=\\tilde{\\mathcal{O}}(\\frac{\\Upsilon^{2}}{\\kappa^{2}})}\\end{array}$ . (b) Linear convergence phase: If $\\begin{array}{r}{\\tau_{1}\\leq t<\\tau_{2}}\\end{array}$ , then $\\begin{array}{r}{\\|\\mathbf{x}_{t+1}-\\mathbf{x}^{*}\\|^{2}\\leq\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|^{2}(1+\\frac{2\\alpha\\beta}{3\\kappa})^{-1}}\\end{array}$ , where $\\begin{array}{r}{\\mathcal{T}_{2}=\\tilde{\\mathcal{O}}(\\operatorname*{max}\\{\\frac{\\Upsilon^{2}}{\\kappa}+\\kappa^{2},\\Upsilon^{2}\\})=\\tilde{\\mathcal{O}}(\\Upsilon^{2}+\\kappa^{2})}\\end{array}$ . (c) Initial superlinear phase: For $\\tau_{2}\\leq t<\\tau_{3}$ , we have $\\|\\mathbf{x}_{t+1}-\\mathbf{x}^{*}\\|\\leq C\\rho_{t}^{(1)}\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|$ , where $\\begin{array}{r}{\\rho_{t}^{(1)}=\\frac{6\\kappa\\mathcal{I}}{\\alpha\\sqrt{2\\beta}(t+1)}=\\tilde{\\mathcal{O}}\\left(\\frac{\\Upsilon^{2}/\\kappa+\\kappa^{2}}{t}\\right)}\\end{array}$ with I defined in (28) and T3 = O\u02dc( (\u03a52/\u03ba\u03a52+\u03ba2)2). (d) Final superlinear phase: Finally, for $t\\ge\\tau_{3}$ , we have $\\|\\mathbf{x}_{t+1}-\\mathbf{x}^{*}\\|\\leq C\\rho_{t}^{(2)}\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|,$ , where \u03c1t(2) $\\begin{array}{r}{\\rho_{t}^{(2)}=\\frac{8\\sqrt{2}\\Upsilon}{\\alpha\\sqrt{\\beta}}\\sqrt{\\frac{\\log\\left(d(t+1)/\\delta\\right)}{t+1}}=\\mathcal{O}\\Big(\\Upsilon\\sqrt{\\frac{\\log(t)}{t}}\\Big)}\\end{array}$ . ", "page_idx": 6}, {"type": "text", "text": "Comparison with [1]. As shown in Table 1, our method in Algorithm 1 with uniform averaging achieves the same final superlinear convergence rate as the stochastic Newton method in [1]. However, it transitions to the linear and superlinear phases much earlier. Specifically, the initial transition point $\\mathcal{T}_{1}$ is improved by a factor of $\\kappa^{2}$ , and our linear rate in Lemma 6 is faster. This reduces the iterations needed to reach the local neighborhood, cutting the time to reach $\\mathcal{T}_{2}$ and $\\tau_{3}$ by factors of $\\kappa$ and $\\kappa^{2}$ . ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "5 Analysis of weighted Hessian averaging ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Previously, we showed Algorithm 1 with uniform averaging eventually achieves superlinear convergence. However, as per Theorem 1, it requires $\\tilde{\\mathcal{O}}\\big(\\frac{\\kappa^{4}}{\\Upsilon^{2}}\\big)$ iterations to reach this rate. To achieve a faster transition, we follow [1] and use Hessian averaging with a general weight sequence $\\{w_{t}\\}$ . We show this method also outperforms the stochastic Newton method in [1]. Specifically, we set $\\overline{{w_{t}}}=w(t)$ for all integer $t\\geq0$ , where $w(\\cdot):\\mathbb{R}\\rightarrow\\mathbb{R}$ satisfies certain regularity conditions as in [1, Assumption 3]. ", "page_idx": 7}, {"type": "text", "text": "Assumption 6. $(i)\\,w(\\cdot)$ is twice differentiable; (ii) $w(-1)=0,$ , $w(t)>0$ , $\\forall t\\ge0$ ; (iii) $w^{\\prime}(-1)\\geq0.$ ;   \n(iv) w\u2032\u2032(t) \u22650, \u2200t \u2265\u22121; (v) max w(wt(+t)1 ), w\u2032w(\u2032t(+t)1) \u2264\u03a8, \u2200t \u22650 for some \u03a8 \u22651. ", "page_idx": 7}, {"type": "text", "text": "Choosing $w(t)=t^{p}$ for any $p\\geq1$ satisfies Assumption 6. Additionally, as discussed in [1], a suitable choice is $w(t)\\,=\\,(t+1)^{\\log(t+4)}$ , allowing us to achieve the optimal transition to the superlinear rate. Since the analysis in this section closely resembles that in Section 4 on uniform averaging, we will only present the final result here for brevity. The four stages of convergence are detailed in the following theorem, with intermediate lemmas and proofs in the appendix. To simplify our bounds, we report results for non-uniform averaging with $w\\bar{(t)}=(t+1)^{\\bar{\\log(t+4)}}$ . ", "page_idx": 7}, {"type": "text", "text": "Theorem 2. Suppose Assumptions 1-5 hold and the weights for Hessian averaging in SNPE are   \ndefined as $w(t)\\stackrel{*}{=}(t+1)^{\\log({t+4})}$ , and define (10\u03b2\u221a21(1\u2212\u03b12) +\u221a12). Then, the following hold: (a) Warm-up phase: I ${}^{\\epsilon}0\\leq t<\\mathcal{U}_{1}$ , then $\\|\\mathbf{x}_{t+1}-\\mathbf{x}^{*}\\|\\leq\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|$ , where $\\begin{array}{r}{\\mathcal{U}_{1}=\\tilde{\\mathcal{O}}\\big(\\frac{\\Upsilon^{2}}{\\kappa^{2}}\\big)}\\end{array}$ . (b) Linear convergence phase: If $\\mathcal{U}_{1}\\leq t<\\mathcal{U}_{2}$ , then $\\begin{array}{r}{\\|\\mathbf{x}_{t+1}-\\mathbf{x}^{*}\\|^{2}\\leq\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|^{2}(1+\\frac{2\\alpha\\beta}{3\\kappa})^{-1}}\\end{array}$ , where $\\begin{array}{r}{\\mathcal{U}_{2}=\\tilde{\\mathcal{O}}(\\operatorname*{max}\\{\\frac{\\Upsilon^{2}}{\\kappa^{2}}+\\kappa,\\Upsilon^{2}\\})=\\tilde{\\mathcal{O}}(\\Upsilon^{2}+\\kappa).}\\end{array}$ . (c) Initial superlinear phase: If $\\mathcal{U}_{2}\\leq t<\\mathcal{U}_{3}$ , then $\\|\\mathbf{x}_{t+1}-\\mathbf{x}^{*}\\|\\leq C^{\\prime}\\theta_{t}^{(1)}\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|$ , where $\\begin{array}{r}{\\theta_{t}^{(1)}\\!=\\!\\frac{5\\kappa w(\\mathcal{I})}{\\alpha\\sqrt{2\\beta}w(t)}=\\tilde{\\mathcal{O}}\\!\\left(\\kappa(\\Upsilon^{2}\\!+\\!\\kappa)^{\\log(\\Upsilon^{2}\\!+\\!\\kappa)}\\middle/t^{\\log t}\\right)}\\end{array}$ with $\\mathcal{I}$ defined in (49) and $\\mathcal{U}_{3}\\!=\\!\\tilde{O}(\\Upsilon^{2}\\!+\\!\\kappa)$ . (d) Final superlinear phase: Finally, if $t\\geq\\mathcal{U}_{3}$ , then $\\|\\mathbf{x}_{t+1}-\\mathbf{x}^{*}\\|\\leq C^{\\prime}\\theta_{t}^{(2)}\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|,$ , where $\\begin{array}{r}{{\\theta}_{t}^{(2)}\\!=\\!\\frac{8\\sqrt{2}\\Upsilon}{\\alpha\\sqrt{\\beta}}\\sqrt{\\frac{w^{\\prime}(t)\\log(d\\frac{t+1}{\\delta})}{w(t)}}\\!=\\!\\mathcal{O}\\left(\\frac{\\Upsilon\\log(t)}{\\sqrt{t}}\\right)\\!.}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "In the weighted averaging case, similar to the uniform averaging scenario, we observe four distinct phases of convergence. The warm-up phase for SNPE, during which the distance to the optimal solution does not increase, has the same duration as in the uniform averaging case but is shorter than the warm-up phase for the stochastic Newton method in [1] by a factor of $1/\\kappa^{2}$ . The linear convergence rates of both uniform and weighted Hessian averaging methods are $1-\\kappa^{-1}$ , improving over the $1-\\kappa^{-2}$ rate achieved by the stochastic Newton method in [1]. The number of iterations to reach the initial superlinear phase is $\\tilde{\\mathcal{O}}(\\Upsilon^{2}\\!+\\!\\kappa)$ , smalle\u221ar than the $\\tilde{\\mathcal{O}}(\\kappa^{2})$ needed for uniform averaging in SNPE when we focus on the regime where $\\dot{\\Upsilon}=\\mathcal{O}(\\sqrt{\\kappa})$ . The non-uniform averaging method in [1] requires $\\kappa^{2}$ iterations to achieve the initial superlinear phase, whereas the non-uniform SNPE achieves an initial superlinear rate of $\\mathcal{O}(\\kappa^{\\log(\\kappa)+1}/t)^{t}$ , improving over the rate of $\\mathcal{O}(\\kappa^{4\\log(\\kappa)+1}/t^{\\log(t)})^{t}$ i\u221an [1]. Finally, while the ultimate superlinear rates in all cases are comparable at approximately $\\tilde{\\mathcal{O}}((1/\\sqrt{t})^{t})$ , the non-uniform version of SNPE requires $\\tilde{\\mathcal{O}}(\\Upsilon^{2}+\\kappa)$ iterations to attain this fast rate, whereas [1]\u2019s non-uniform version requires $\\tilde{\\mathcal{O}}(\\kappa^{2})$ iterations, which is less favorable. ", "page_idx": 7}, {"type": "text", "text": "Remark 6. As discussed in [1, Example 1], with a subsampling size of $s$ , we have $\\Upsilon=\\tilde{\\mathcal{O}}(\\kappa/s)$ for subsampled Newton. This implies that when $s=\\tilde{\\Omega}(\\sqrt{\\kappa})$ , we achieve $\\Upsilon={\\mathcal{O}}({\\sqrt{\\kappa}})$ . ", "page_idx": 7}, {"type": "text", "text": "6 Discussion and complexity comparison ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we compare the complexity of our method with accelerated gradient descent (AGD), damped Newton, and stochastic Newton [1] methods. The iteration complexities of these methods are summarized in Table 2, which we use to establish their overall complexities. Note that since both stochastic Newton and our proposed SNPE method achieve superlinear convergence, their complexities depend on the target accuracy $\\epsilon$ in the form $\\mathcal{O}\\big(\\frac{\\log(1/\\epsilon)}{\\log\\log(1/\\epsilon)}\\big)$ , which is provably better than the complexity of AGD by at least a factor of $\\log\\log(\\epsilon^{-1})$ . Further details are provided in Appendix D.1. To better compare them, we focus on the finite-sum problem with $n$ functions: $\\begin{array}{r}{\\operatorname*{min}_{x\\in\\mathbb{R}^{d}}\\sum_{i=1}^{n}f_{i}(x)}\\end{array}$ . Let $\\epsilon$ be the target accuracy, $\\kappa$ the condition number, and $\\Upsilon$ the noise level in Assumption 4. In this case, computing the exact gradient and Hessian costs $\\mathcal{O}(n d)$ and $O(n d^{2})$ , respectively. Thus, the per-iteration cost for AGD is $O(n d)$ . Each iteration of damped Newton\u2019s method requires computing the full Hessian and solving a linear system, resulting in a total periteration cost of $O({\\dot{n}}d^{2}+{\\bar{d}}^{3})$ . For both stochastic Newton in [1] and our SNPE method, the per-iteration cost depends on how the stochastic Hessian is constructed. For example, Subsampled Newton constructs the Hessian estimate with a cost of $O(s d^{2})$ , where $s$ denotes the sample size. Newton Sketch has a similar computation cost (see [6]). Additionally, it takes $O(n d)$ to compute the full gradient and $O(d^{3})$ to solve the linear system. Since the sample/sketch size $s$ is typically chosen as $s=\\mathcal{O}(d)$ , the total per-iteration cost is $\\dot{\\cal O}(n d+d^{3})$ . ", "page_idx": 7}, {"type": "table", "img_path": "V4tzn87DtN/tmp/ae095aabfce650bfc4409b32331c942fc41a2c8a61ec8f607c445926fc0e8dcb.jpg", "table_caption": ["Table 2: Comparisons in terms of overall iteration complexity to find an $\\epsilon$ -accurate solution. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "\u2022 Compared to AGD, SNPE achieves better iteration co\u221amplexity. Specificall\u221ay, when the noise level $\\Upsilon$ and target accuracy $\\epsilon$ are relatively small $\\Upsilon={\\mathcal O}(\\sqrt{\\kappa})$ and $\\log{\\frac{1}{\\epsilon}}=\\Omega(\\dot{\\sqrt{\\kappa}}))$ , SNPE converges in fewer iterations. Additionally, when $n\\geq d^{2}$ (indicating many samples), the per-iteration costs of both methods are $O(n d)$ , giving our method a better overall complexity. \u2022 Compared to damped Newton\u2019s method, our method\u2019s iteration complexity depends better on the condition number $\\kappa$ , while damped Newton\u2019s depends better on $\\epsilon$ . However, when $n\\geq d^{2}$ , the per-iteration cost of damped Newton is $O(n d^{2})$ , significantly more than our method\u2019s $O(n d)$ . \u2022 Compared to the stochastic Newton method in [1], the per-iteration costs of both methods are similar. However, Table 2 shows that our iteration complexity is strictly better. Spec\u221aifically, when the noise level is relatively small compared to the condition number, i.e., $\\Upsilon=\\bar{\\mathcal{O}}(\\sqrt{\\kappa})$ , the complexity of SNPE improves by an additional factor of $\\kappa$ over the stochastic Newton method. ", "page_idx": 8}, {"type": "text", "text": "7 Numerical experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "While our focus is on improving theoretical guarantees, we also provide simple experiments to showcase our method\u2019s improvements. All simulations are implemented on a Windows PC with an AMD processor and 16GB Memory. We consider minimizing the regularized log-sum-exp objective $\\begin{array}{r}{\\bar{f}(x)=\\rho\\log(\\sum_{i=1}^{n}\\exp(\\frac{\\mathbf{a}_{i}^{\\top}\\mathbf{\\bar{x}}-b_{i}}{\\rho}))+\\frac{\\lambda}{2}\\|\\mathbf{x}\\|^{2}}\\end{array}$ , mon test function for second methods [23, 24] due to its high ill-conditioning. Here, $\\lambda>0$ $\\rho>0$ is a smoothing parameter, and the entries of the vectors $\\mathbf{a}_{1},\\ldots,\\mathbf{a}_{n}\\in\\mathbf{\\bar{R}}^{d}$ and $\\mathbf{b}\\in\\dot{\\mathbb{R}}^{d}$ are randomly generated from the standard normal distribution and the uniform distribution over $[0,1]$ , respectively. In our experiments, the regularization parameter $\\lambda$ is $10^{-3}$ , the dimension $d$ is 500, and the number of samples $n$ is chosen from 50,000, 10,000, and 150,000, respectively. ", "page_idx": 8}, {"type": "text", "text": "We compare our SNPE method with the stochastic Newton method in [1], using both uniform Hessian averaging (Section 4) and weighted averaging (Section 5). In addition, we evaluate it against accelerated gradient descent (AGD), damped Newton\u2019s method, and Newton Proximal Extragradient (NPE), which corresponds to our SNPE method with the exact Hessian. For the stochastic Hessian estimate, we use a subsampling strategy with a subsampling size of $s=500$ . Empirically, we found that the extragradient step (Line 7 in Algorithm 1) tends to slow down convergence. To address this, we consider a variant of our method without the extragradient step, modifying Line 7 to $\\mathbf{x}_{k+1}=\\hat{\\mathbf{x}}_{k}$ . In Figure 3 of the Appendix, we further explore the effect of the extragradient step. We also note that similar observations were made in [19], where the simple iteration $\\mathbf{\\bar{x_{t+1}}}=\\mathbf{x}_{t}-\\bar{\\eta_{t}}(\\mathbf{I}+\\eta_{t}\\mathbf{H}_{t})^{-1}\\mathbf{g}_{t}$ outperformed \u201caccelerated\u201d second-order methods. From Figures 1 and 2, we have the following observations: ", "page_idx": 8}, {"type": "image", "img_path": "V4tzn87DtN/tmp/f96ca6f2ad1a9825e2e1c6d6c4aa6b03b6f11b405d19ecaa249e4f8c4fb37d86.jpg", "img_caption": ["Figure 1: Iteration complexity comparison for minimizing log-sum-exp on a synthetic dataset. "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "V4tzn87DtN/tmp/27993a7ef7446f92889eeab6d85540261b87ba14161007d073027097e6c3586f.jpg", "img_caption": ["Figure 2: Runtime comparison for minimizing log-sum-exp on a synthetic dataset. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Comparison with stochastic Newton. In all cases, our SNPE method outperforms stochastic Newton in both the number of iterations and runtime, due to the problem\u2019s highly ill-conditioned nature and our method\u2019s better dependence on the condition number. ", "page_idx": 9}, {"type": "text", "text": "Comparison with AGD. From Figure 1, we observe that our SNPE method, with either uniform or weighted averaging, requires far fewer iterations to converge than AGD due to the use of second-order information. Consequently, while SNPE has a higher per-iteration cost than AGD, it converges faster overall in terms of runtime, as demonstrated in Figure 2. ", "page_idx": 9}, {"type": "text", "text": "Comparison with the damped Newton\u2019s method and NPE. As expected, since both damped Newton and NPE use exact Hessian, Figure 1 shows that they exhibit superlinear convergence and converge in fewer iterations than the other algorithms. However, since the exact Hessian matrix is expensive to compute, they incur a high per-iteration computational cost and overall take more time than our proposed SNPE method to converge (see Figure 2). Moreover, the gap between these two methods and SNPE widens as the number of samples $n$ increases, demonstrating the advantage of our method in the large data regime. ", "page_idx": 9}, {"type": "text", "text": "8 Conclusions and limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We introduced a stochastic variant of the Newton Proximal Extragradient method (SNPE) for minimizing a strongly convex and smooth function with access to a noisy Hessian. Our contributions include establishing convergence guarantees under two Hessian averaging schemes: uniform and non-uniform. We characterized the computational complexity in both cases and demonstrated that SNPE outperforms the best-known results for the considered problem. A limitation of our theory is the assumption of strong convexity. Extending the theory to the convex setting would make it more general. This extension is left for future work due to space limitations. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The research of R. Jiang and A. Mokhtari is supported in part by the NSF Grant 2007668, the NSF AI Institute for Foundations of Machine Learning (IFML), and the Wireless Networking and Communications Group (WNCG) Industrial Affiliates Program at UT Austin. The research of M. Derezin\u00b4ski was partially supported by NSF grant CCF-2338655. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Sen Na, Micha\u0142 Derezin\u00b4ski, and Michael W Mahoney. Hessian averaging in stochastic Newton methods achieves superlinear convergence. Mathematical Programming, pages 1\u201348, 2022. [2] Yu Nesterov. Accelerating the cubic regularization of Newton\u2019s method on convex problems. Mathematical Programming, 112(1):159\u2013181, 2008.   \n[3] Renato DC Monteiro and Benar Fux Svaiter. An accelerated hybrid proximal extragradient method for convex optimization and its implications to second-order methods. SIAM Journal on Optimization, 23(2):1092\u20131125, 2013. [4] Naman Agarwal, Brian Bullins, and Elad Hazan. Second-order stochastic optimization for machine learning in linear time. The Journal of Machine Learning Research, 18(1):4148\u20134187, 2017.   \n[5] Guy Kornowski and Ohad Shamir. High-order oracle complexity of smooth and strongly convex optimization. arXiv preprint arXiv:2010.06642, 2020. [6] Mert Pilanci and Martin J Wainwright. Newton sketch: A near linear-time optimization algorithm with linear-quadratic convergence. SIAM Journal on Optimization, 27(1):205\u2013245, 2017.   \n[7] Richard H Byrd, Gillian M Chin, Will Neveitt, and Jorge Nocedal. On the use of stochastic Hessian information in optimization methods for machine learning. SIAM Journal on Optimization, 21(3):977\u2013995, 2011.   \n[8] Murat A Erdogdu and Andrea Montanari. Convergence rates of sub-sampled Newton methods. Advances in Neural Information Processing Systems, 28, 2015. [9] Raghu Bollapragada, Richard H Byrd, and Jorge Nocedal. Exact and inexact subsampled Newton methods for optimization. IMA Journal of Numerical Analysis, 39(2):545\u2013578, 2019.   \n[10] Farbod Roosta-Khorasani and Michael W Mahoney. Sub-sampled Newton methods. Mathematical Programming, 174:293\u2013326, 2019.   \n[11] Michal Derezinski and Michael W Mahoney. Distributed estimation of the inverse hessian by determinantal averaging. Advances in Neural Information Processing Systems, 32, 2019.   \n[12] Haishan Ye, Luo Luo, and Zhihua Zhang. Approximate Newton methods. Journal of Machine Learning Research, 22(66):1\u201341, 2021.   \n[13] Michal Derezinski, Jonathan Lacotte, Mert Pilanci, and Michael W Mahoney. Newton-LESS: Sparsification without trade-offs for the sketched Newton update. Advances in Neural Information Processing Systems, 34:2835\u20132847, 2021.   \n[14] Albert S Berahas, Raghu Bollapragada, and Jorge Nocedal. An investigation of Newton-sketch and subsampled Newton methods. Optimization Methods and Software, 35(4):661\u2013680, 2020.   \n[15] Micha\u0142 Derezin\u00b4ski and Michael W Mahoney. Recent and upcoming developments in randomized numerical linear algebra for machine learning. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 6470\u20136479, 2024.   \n[16] Mikhail V Solodov and Benar F Svaiter. A hybrid approximate extragradient\u2013proximal point algorithm using the enlargement of a maximal monotone operator. Set-Valued Analysis, 7(4):323\u2013 345, 1999.   \n[17] Renato DC Monteiro and Benar F Svaiter. Iteration-complexity of a Newton proximal extragradient method for monotone variational inequalities and inclusion problems. SIAM Journal on Optimization, 22(3):914\u2013935, 2012.   \n[18] Renato D. C. Monteiro and B. F. Svaiter. On the complexity of the hybrid proximal extragradient method for the iterates and the ergodic mean. SIAM Journal on Optimization, 20(6):2755\u20132787, 2010.   \n[19] Yair Carmon, Danielle Hausler, Arun Jambulapati, Yujia Jin, and Aaron Sidford. Optimal and adaptive Monteiro-Svaiter acceleration. Advances in Neural Information Processing Systems, 35:20338\u201320350, 2022.   \n[20] Dmitry Kovalev and Alexander Gasnikov. The first optimal acceleration of high-order methods in smooth convex optimization. Advances in Neural Information Processing Systems, 35:35339\u2013 35351, 2022.   \n[21] Mathieu Barr\u00e9, Adrien Taylor, and Francis Bach. A note on approximate accelerated forwardbackward methods with absolute and relative errors, and possibly strongly convex objectives. Open Journal of Mathematical Optimization, 3:1\u201315, 2022.   \n[22] Ruichen Jiang, Qiujiang Jin, and Aryan Mokhtari. Online learning guided curvature approximation: A quasi-Newton method with global non-asymptotic superlinear convergence. arXiv preprint arXiv:2302.08580, 2023.   \n[23] Konstantin Mishchenko. Regularized Newton method with global convergence. SIAM Journal on Optimization, 33(3):1440\u20131462, 2023.   \n[24] Nikita Doikov, Konstantin Mishchenko, and Yurii Nesterov. Super-universal regularized Newton method. SIAM Journal on Optimization, 34(1):27\u201356, 2024.   \n[25] Yurii Nesterov. Lectures on Convex Optimization. Springer International Publishing, 2018. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A Missing proofs in Section 3 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Proof of Proposition 1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In this section, we prove Proposition 1. In fact, using the same proof, we can also show an additional result that upper bounds $\\lVert\\mathbf{x}_{t}-\\hat{\\mathbf{x}}_{t}\\rVert$ , which will useful in the proof of Lemma 2. Therefore, we present the full version below for completeness. ", "page_idx": 12}, {"type": "text", "text": "Proposition 2 (Full version of Proposition 1). Let $\\{\\mathbf{x}_{t}\\}_{t\\ge0}$ and $\\{\\hat{\\mathbf{x}}_{t}\\}_{t\\ge0}$ be the iterates generated by Algorithm $^{\\,l}$ . Then for any $t\\,\\geq\\,0$ , we have $\\|\\mathbf{x}_{k+1}-\\mathbf{\\bar{x}}^{*}\\|^{2}\\,\\leq\\,\\|\\mathbf{\\bar{x}}_{k}^{-}-\\mathbf{x}^{*}\\|^{2}(1+2\\eta_{k}\\mu)^{-1}$ and $\\begin{array}{r}{\\|\\dot{\\mathbf x}_{t}-\\hat{\\mathbf x}_{t}\\|\\leq\\frac{1}{\\sqrt{1-\\alpha^{2}}}\\|\\dot{\\mathbf x}_{t}-\\mathbf x^{*}\\|}\\end{array}$ . ", "page_idx": 12}, {"type": "text", "text": "Proof. Our proof is inspired by the approach in [22, Proposition 1]. For any $\\mathbf{x}\\in\\mathbb{R}^{d}$ , we first write ", "page_idx": 12}, {"type": "text", "text": "$\\begin{array}{r}{\\eta_{t}\\langle\\nabla f(\\hat{\\mathbf{x}}_{t}),\\hat{\\mathbf{x}}_{t}-\\mathbf{x}\\rangle=\\langle\\hat{\\mathbf{x}}_{t}-\\mathbf{x}_{t}+\\eta_{t}\\nabla f(\\hat{\\mathbf{x}}_{t}),\\hat{\\mathbf{x}}_{t}-\\mathbf{x}\\rangle+\\langle\\mathbf{x}_{t}-\\hat{\\mathbf{x}}_{t},\\hat{\\mathbf{x}}_{t}-\\mathbf{x}\\rangle.}\\end{array}$ (10) To begin with, we bound the first term in (10) by ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\hat{\\mathbf{x}}_{t}-\\mathbf{x}_{t}+\\eta_{t}\\nabla f(\\hat{\\mathbf{x}}_{t}),\\hat{\\mathbf{x}}_{t}-\\mathbf{x}\\rangle\\leq\\|\\hat{\\mathbf{x}}_{t}-\\mathbf{x}_{t}+\\eta_{t}\\nabla f(\\hat{\\mathbf{x}}_{t})\\|\\|\\hat{\\mathbf{x}}_{t}-\\mathbf{x}\\|}\\\\ &{\\qquad\\qquad\\qquad\\leq\\alpha\\sqrt{1+2\\eta_{t}\\mu}\\|\\hat{\\mathbf{x}}_{t}-\\mathbf{x}_{t}\\|\\|\\hat{\\mathbf{x}}_{t}-\\mathbf{x}\\|}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{\\alpha^{2}}{2}\\|\\hat{\\mathbf{x}}_{t}-\\mathbf{x}_{t}\\|^{2}+\\frac{1+2\\eta_{t}\\mu}{2}\\|\\hat{\\mathbf{x}}_{t}-\\mathbf{x}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where the first inequality is due to Cauchy-Schwarz inequality, the second inequality is due to the condition in (3), and the last inequality is due to Young\u2019s inequality. Moreover, for the second term in (10), we use the three-point equality to get ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\langle\\mathbf{x}_{t}-\\hat{\\mathbf{x}}_{t},\\hat{\\mathbf{x}}_{t}-\\mathbf{x}\\rangle=\\frac{1}{2}\\|\\mathbf{x}_{t}-\\mathbf{x}\\|^{2}-\\frac{1}{2}\\|\\mathbf{x}_{t}-\\hat{\\mathbf{x}}_{t}\\|^{2}-\\frac{1}{2}\\|\\hat{\\mathbf{x}}_{t}-\\mathbf{x}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "By combining (10), (11) and (12), we obtain that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\eta_{t}\\langle\\nabla f({\\hat{\\mathbf{x}}}_{t}),{\\hat{\\mathbf{x}}}_{t}-\\mathbf{x}\\rangle\\leq{\\frac{1}{2}}{\\|\\mathbf{x}_{t}-\\mathbf{x}\\|^{2}}-{\\frac{1-\\alpha^{2}}{2}}{\\|\\mathbf{x}_{t}-{\\hat{\\mathbf{x}}}_{t}\\|^{2}}+\\eta_{t}\\mu\\|{\\hat{\\mathbf{x}}}_{t}-\\mathbf{x}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Moreover, it follows from the update rule in (4) that $\\eta_{t}\\nabla f({\\hat{\\mathbf{x}}}_{t})=\\mathbf{x}_{t}-\\mathbf{x}_{t+1}+2\\eta_{t}\\mu({\\hat{\\mathbf{x}}}_{t}-\\mathbf{x}_{t+1})$ . This implies that, for any $\\mathbf{x}\\in\\mathbb{R}^{d}$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\ }&{\\eta_{t}\\langle\\nabla f(\\hat{\\mathbf{x}}_{t}),\\mathbf{x}_{t+1}-\\mathbf{x}\\rangle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(144\\eta)\\eta_{t}\\langle\\hat{\\mathbf{x}}_{t}-\\mathbf{x}_{t+1},\\mathbf{x}_{t+1}-\\mathbf{x}\\rangle}\\\\ &{=\\langle\\mathbf{x}_{t}-\\mathbf{x}_{t+1},\\mathbf{x}_{t+1}-\\mathbf{x}\\rangle+2\\eta_{t}\\mu\\langle\\hat{\\mathbf{x}}_{t}-\\mathbf{x}_{t+1},\\mathbf{x}_{t+1}-\\mathbf{x}\\rangle}\\\\ &{=\\frac{\\|\\mathbf{x}_{t}-\\mathbf{x}\\|^{2}}{2}-\\frac{\\|\\mathbf{x}_{t}-\\mathbf{x}_{t+1}\\|^{2}}{2}-\\frac{1+2\\eta_{t}\\mu}{2}\\|\\mathbf{x}_{t+1}-\\mathbf{x}\\|^{2}+\\eta_{t}\\mu\\|\\hat{\\mathbf{x}}_{t}-\\mathbf{x}\\|^{2}-\\eta_{t}\\mu\\|\\hat{\\mathbf{x}}_{t}-\\mathbf{x}_{t+1}\\|^{2},}&\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where we applied the three-point equality twice in the last equality. Thus, by combining (13) with $\\mathbf{x}=\\mathbf{x}_{t+1}$ and (15) with $\\mathbf{x}=\\mathbf{x}^{*}$ , we get ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\eta_{t}\\langle\\nabla f(\\hat{\\mathbf{x}}_{t}),\\hat{\\mathbf{x}}_{t}-\\mathbf{x}^{*}\\rangle}\\\\ &{=\\eta_{t}\\langle\\nabla f(\\hat{\\mathbf{x}}_{t}),\\mathbf{x}_{t+1}-\\mathbf{x}^{*}\\rangle+\\eta_{t}\\langle\\nabla f(\\hat{\\mathbf{x}}_{t}),\\hat{\\mathbf{x}}_{t}-\\mathbf{x}_{t+1}\\rangle}\\\\ &{\\leq\\frac{\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|^{2}}{2}-\\frac{\\|\\mathbf{x}_{t}-\\mathbf{x}_{t+1}\\|^{2}}{2}-\\frac{1+2\\eta_{t}\\mu}{2}\\|\\mathbf{x}_{t+1}-\\mathbf{x}^{*}\\|^{2}+\\eta_{t}\\mu\\|\\hat{\\mathbf{x}}_{t}-\\mathbf{x}^{*}\\|^{2}-\\widetilde{\\eta_{t}\\mu}\\|\\hat{\\mathbf{x}}_{t}-\\mathbf{x}_{t+1}\\|^{2}}\\\\ &{\\quad+\\frac{\\|\\mathbf{x}_{t}-\\mathbf{x}_{t+1}\\|^{2}}{2}-\\frac{1-\\alpha^{2}}{2}\\|\\mathbf{x}_{t}-\\hat{\\mathbf{x}}_{t}\\|^{2}+\\overline{{\\eta_{t}\\mu}}\\|\\hat{\\mathbf{x}}_{t}\\longrightarrow\\mathbf{x}_{t+1}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Since $\\nabla f(\\mathbf{x}^{*})=0$ and $f$ is $\\mu$ -strongly convex, we further have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\langle\\nabla f({\\hat{\\mathbf{x}}}_{t}),{\\hat{\\mathbf{x}}}_{t}-\\mathbf{x}^{*}\\rangle=\\langle\\nabla f({\\hat{\\mathbf{x}}}_{t})-\\nabla f(\\mathbf{x}^{*}),{\\hat{\\mathbf{x}}}_{t}-\\mathbf{x}^{*}\\rangle\\geq\\mu\\|{\\hat{\\mathbf{x}}}_{t}-\\mathbf{x}^{*}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Combining (16) and (17) and rearranging the terms, we obtain that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\frac{1+2\\eta_{t}\\mu}{2}\\|\\mathbf{x}_{t+1}-\\mathbf{x}^{*}\\|^{2}\\leq\\frac{1}{2}\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|^{2}-\\frac{1-\\alpha^{2}}{2}\\|\\mathbf{x}_{t}-\\hat{\\mathbf{x}}_{t}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Since $\\alpha<1$ , the last term in (18) is negative and we immediately obtain that $\\|\\mathbf{x}_{t+1}-\\mathbf{x}^{*}\\|^{2}\\leq\\|\\mathbf{x}_{t}-$ $\\mathbf{x}^{*}\\Vert^{2}(1+2\\eta_{t}\\mu)^{-1}$ c .h  lMeaordes otvo $\\begin{array}{r}{\\frac{1+2\\eta_{t}\\mu}{2}\\|\\mathbf{x}_{t+1}-\\mathbf{x}^{*}\\|^{2}\\geq0}\\end{array}$ ,h iet  pfrololoof wiss  tchoamt $\\begin{array}{r}{\\frac{1-\\alpha^{2}}{2}\\|\\mathbf{x}_{t}-\\hat{\\mathbf{x}}_{t}\\|^{2}\\leq}\\end{array}$ $\\frac{1}{2}\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|^{2}$ $\\begin{array}{r}{\\|\\mathbf{x}_{t}-\\hat{\\mathbf{x}}_{t}\\|\\leq\\frac{1}{\\sqrt{1-\\alpha^{2}}}\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|}\\end{array}$ ", "page_idx": 12}, {"type": "text", "text": "A.2 Proof of Lemma 1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Recall that in our backtracking line search scheme in Algorithm 1, the step size $\\eta_{t}$ starts from $\\sigma_{t}$ and keeps backtracking until the condition in (3) is satisfied. Hence, by the definition of $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ , it immediately follows that $\\eta_{t}=\\sigma_{t}$ if $t\\not\\in\\mathcal{B}$ . Moreover, if $t\\in{\\boldsymbol{B}}$ , then the step size $\\tilde{\\eta}_{t}=\\eta_{t}/\\beta$ and the corresponding iterate $\\tilde{\\mathbf{x}}_{t}$ must have failed the condition in (3) (Otherwise, our line search scheme would have accepted the step size $\\tilde{\\eta}_{t}$ instead). This implies that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\|\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t}+\\tilde{\\eta}_{t}\\nabla f(\\tilde{\\mathbf{x}}_{t})\\|>\\alpha\\sqrt{1+2\\tilde{\\eta}_{t}\\mu}\\|\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t}\\|.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Since $\\tilde{\\mathbf{x}}_{t}=\\mathbf{x}_{t}-\\tilde{\\eta}_{t}(\\mathbf{I}+\\tilde{\\eta}_{t}\\tilde{\\mathbf{H}}_{t})^{-1}\\nabla f(\\mathbf{x}_{t})$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t}+\\tilde{\\eta}_{t}\\tilde{\\mathbf{H}}_{t}\\big(\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t}\\big)=-\\tilde{\\eta}_{t}\\nabla f(\\mathbf{x}_{t})\\quad\\Leftrightarrow\\quad\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t}=-\\tilde{\\eta}_{t}\\big(\\nabla f(\\mathbf{x}_{t})+\\tilde{\\mathbf{H}}_{t}\\big(\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t}\\big)\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and hence the left-hand side in (19) equals to $\\tilde{\\eta}_{t}\\|\\nabla f(\\tilde{\\mathbf{x}}_{t})-\\nabla f(\\mathbf{x}_{t})-\\tilde{\\mathbf{H}}_{t}(\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t})\\|$ . Thus, we obtain from (19) that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\tilde{\\eta}_{t}>\\frac{\\alpha\\sqrt{1+2\\tilde{\\eta}_{t}\\mu}\\lVert\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t}\\rVert}{\\lVert\\nabla f(\\tilde{\\mathbf{x}}_{t})-\\nabla f(\\mathbf{x}_{t})-\\tilde{\\mathbf{H}}_{t}(\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t})\\rVert},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "By substituting $\\eta_{t}=\\beta\\tilde{\\eta}_{t}$ , we further have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\eta_{t}>\\frac{\\alpha\\beta\\sqrt{1+2\\eta_{t}\\mu/\\beta}\\lVert\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t}\\rVert}{\\lVert\\nabla f(\\tilde{\\mathbf{x}}_{t})-\\nabla f(\\mathbf{x}_{t})-\\tilde{\\mathbf{H}}_{t}(\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t})\\rVert}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Using the fact that $1+2\\eta_{t}\\mu/\\beta\\geq1$ , we get ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\eta_{t}>\\frac{\\alpha\\beta\\lVert\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t}\\rVert}{\\lVert\\nabla f(\\tilde{\\mathbf{x}}_{t})-\\nabla f(\\mathbf{x}_{t})-\\tilde{\\mathbf{H}}_{t}(\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t})\\rVert}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Moreover, using the fact that $1+2\\eta_{t}\\mu/\\beta\\geq2\\eta_{t}\\mu/\\beta$ , we can also conclude that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\eta_{t}>\\frac{\\alpha\\beta\\sqrt{2\\eta_{t}\\mu/\\beta}\\lVert\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t}\\rVert}{\\lVert\\nabla f(\\tilde{\\mathbf{x}}_{t})-\\nabla f(\\mathbf{x}_{t})-\\tilde{\\mathbf{H}}_{t}(\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t})\\rVert}\\quad\\Rightarrow\\quad\\eta_{t}>\\frac{2\\alpha^{2}\\beta\\mu\\lVert\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t}\\rVert^{2}}{\\lVert\\nabla f(\\tilde{\\mathbf{x}}_{t})-\\nabla f(\\mathbf{x}_{t})-\\tilde{\\mathbf{H}}_{t}(\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t})\\rVert_{\\infty}^{2}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "By combining (21) and (22), we obtain the lower bound in Lemma 1. ", "page_idx": 13}, {"type": "text", "text": "Finally, when $\\tilde{\\mathbf{H}}_{t}\\ \\succeq\\ 0$ , it holds that $\\mathbf{I}+\\tilde{\\eta}_{t}\\tilde{\\mathbf{H}}_{t}\\;\\succeq\\;\\mathbf{I}+\\eta_{t}\\tilde{\\mathbf{H}}_{t}\\;\\succeq\\;0$ and thus $({\\bf I}+\\eta_{t}\\tilde{\\bf H}_{t})^{-1}\\,\\,\\succeq$ $({\\mathbf I}+\\tilde{\\eta}_{t}\\tilde{\\mathbf H}_{t})^{-1}\\succeq0$ . This further implies that $\\|(\\mathbf{I}+\\eta_{t}\\tilde{\\mathbf{H}}_{t})^{-1}\\nabla f(\\mathbf{x}_{t})\\|\\ge\\|(\\mathbf{I}+\\tilde{\\eta}_{t}\\tilde{\\mathbf{H}}_{t})^{-1}\\nabla f(\\mathbf{x}_{t})\\|$ . Hence, we can conclude that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\|\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t}\\|=\\tilde{\\eta}_{t}\\|(\\mathbf{I}+\\tilde{\\eta}_{t}\\tilde{\\mathbf{H}}_{t})^{-1}\\nabla f(\\mathbf{x}_{t})\\|\\le\\frac{\\eta_{t}}{\\beta}\\|(\\mathbf{I}+\\eta_{t}\\tilde{\\mathbf{H}}_{t})^{-1}\\nabla f(\\mathbf{x}_{t})\\|\\le\\frac{1}{\\beta}\\|\\hat{\\mathbf{x}}_{t}-\\mathbf{x}_{t}\\|.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "This completes the proof. ", "page_idx": 13}, {"type": "text", "text": "A.3 Extension to inexact linear solving ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we extend our convergence results to the case where the linear system in (6) is solved inexactly, i.e., we find $\\hat{\\mathbf{x}}_{t}$ such that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\|(\\mathbf{I}+\\eta_{t}\\tilde{\\mathbf{H}}_{t})(\\hat{\\mathbf{x}}_{t}-\\mathbf{x}_{t})+\\eta_{t}\\nabla f(\\mathbf{x}_{t})\\|\\leq\\frac{\\alpha}{2}\\|\\hat{\\mathbf{x}}_{t}-\\mathbf{x}_{t}\\|.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "In this case, since the proof of Proposition 2 does not rely on the update rule in (6), Proposition 2 continues to hold. However, we need to modify the proof of Lemma 1 and replace it by the following lemma. We note that the two results differ only by an absolute constant. ", "page_idx": 13}, {"type": "text", "text": "Lemma 5 (Extension to Lemma 1). For $t\\notin B$ , we have $\\eta_{t}=\\sigma_{t}$ . For $t\\in\\mathcal{B}$ , let $\\tilde{\\eta}_{t}=\\eta_{t}/\\beta$ and $\\tilde{{\\bf x}}_{t}$ be the corresponding iterate rejected by our line search scheme. Then, $\\begin{array}{r}{\\|\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t}\\|\\leq\\frac{3}{\\beta}\\|\\hat{\\mathbf{x}}_{t}-\\mathbf{x}_{t}\\|}\\end{array}$ . Moreover, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\eta_{t}\\geq\\operatorname*{max}\\left\\{\\frac{\\alpha\\beta\\lVert\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t}\\rVert}{2\\lVert\\mathbf{g}(\\tilde{\\mathbf{x}}_{t})-\\mathbf{g}(\\mathbf{x}_{t})-\\tilde{\\mathbf{H}}_{t}(\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t})\\rVert},\\frac{\\alpha^{2}\\beta\\mu\\lVert\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t}\\rVert^{2}}{2\\lVert\\mathbf{g}(\\tilde{\\mathbf{x}}_{t})-\\mathbf{g}(\\mathbf{x}_{t})-\\tilde{\\mathbf{H}}_{t}(\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t})\\rVert^{2}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. We follow a similar argument as in the proof of Lemma 1. If $t\\not\\in\\mathcal{B}$ , it immediately follows that $\\eta_{t}\\,=\\,\\sigma_{t}$ . Further, if $t\\,\\in\\,\\mathcal{B}$ , then $\\tilde{\\eta}_{t}$ and the corresponding iterate $\\tilde{x}_{t}$ must fail to satisfy the condition in (3). This implies that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t}+\\tilde{\\eta}_{t}\\nabla f(\\tilde{\\mathbf{x}}_{t})\\|>\\alpha\\sqrt{1+2\\tilde{\\eta}_{t}\\mu}\\|\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t}\\|.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Moreover, by our inexactness condition in (23), $\\tilde{{\\bf x}}_{t}$ satisfies ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|(\\mathbf{I}+\\tilde{\\eta}_{t}\\tilde{\\mathbf{H}}_{t})(\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t})+\\tilde{\\eta}_{t}\\nabla f(\\mathbf{x}_{t})\\|\\leq\\frac{\\alpha}{2}\\|\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t}\\|.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Hence, by using triangle inequality, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\tilde{\\eta}_{t}\\|\\nabla f(\\tilde{\\mathbf{x}}_{t})-\\nabla f(\\mathbf{x}_{t})-\\tilde{\\mathbf{H}}_{t}(\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t})\\|}\\\\ &{=\\Big\\|(\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t}+\\tilde{\\eta}_{t}\\nabla f(\\tilde{\\mathbf{x}}_{t}))-\\Big((\\mathbf{I}+\\tilde{\\eta}_{t}\\tilde{\\mathbf{H}}_{t})(\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t})+\\tilde{\\eta}_{t}\\nabla f(\\mathbf{x}_{t})\\Big)\\Big\\|}\\\\ &{\\geq\\|\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t}+\\tilde{\\eta}_{t}\\nabla f(\\tilde{\\mathbf{x}}_{t})\\|-\\|(\\mathbf{I}+\\tilde{\\eta}_{t}\\tilde{\\mathbf{H}}_{t})(\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t})+\\tilde{\\eta}_{t}\\nabla f(\\mathbf{x}_{t})\\|}\\\\ &{\\geq\\frac{\\alpha}{2}\\sqrt{1+2\\tilde{\\eta}_{t}\\mu}\\|\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus, we obtain that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\tilde{\\eta}_{t}>\\frac{\\alpha\\sqrt{1+2\\tilde{\\eta}_{t}\\mu}\\lVert\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t}\\rVert}{2\\lVert\\nabla f(\\tilde{\\mathbf{x}}_{t})-\\nabla f(\\mathbf{x}_{t})-\\tilde{\\mathbf{H}}_{t}(\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t})\\rVert},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Combining this with (20), we observe that these two bounds differ only by a constant factor of 2.   \nHence, the rest of the proof for the lower bound follows similarly as in Lemma 1. ", "page_idx": 14}, {"type": "text", "text": "Next, we prove the inequality $\\begin{array}{r}{\\|\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t}\\|\\leq\\frac{1}{\\beta}\\|\\hat{\\mathbf{x}}_{t}-\\mathbf{x}_{t}\\|}\\end{array}$ . Define $\\hat{\\mathbf{x}}_{t}^{*}=\\mathbf{x}_{t}-\\eta_{t}(\\mathbf{I}+\\eta_{t}\\tilde{\\mathbf{H}}_{t})^{-1}\\nabla f(\\mathbf{x}_{t})$ and $\\tilde{\\mathbf{x}}_{t}^{*}=\\mathbf{x}_{t}-\\tilde{\\eta}_{t}(\\mathbf{I}+\\tilde{\\eta}_{t}\\tilde{\\mathbf{H}}_{t})^{-1}\\nabla f(\\mathbf{x}_{t})$ , i.e, they are the exact solutions to the corresponding linear systems. By the argument in the proof of Lemma 1, we have $\\begin{array}{r}{\\|\\tilde{\\mathbf{x}}_{t}^{*}-\\mathbf{x}_{t}\\|\\leq\\frac{1}{\\beta}\\|\\hat{\\mathbf{x}}_{t}^{*}-\\mathbf{x}_{t}\\|}\\end{array}$ . In the following, we first prove that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\|\\hat{{\\mathbf x}}_{t}-{\\mathbf x}_{t}\\|\\leq\\|\\hat{{\\mathbf x}}_{t}^{*}-{\\mathbf x}_{t}\\|\\leq\\frac{3}{2}\\|\\hat{{\\mathbf x}}_{t}-{\\mathbf x}_{t}\\|\\quad\\mathrm{and}\\quad\\frac{1}{2}\\|\\tilde{{\\mathbf x}}_{t}-{\\mathbf x}_{t}\\|\\leq\\|\\tilde{{\\mathbf x}}_{t}^{*}-{\\mathbf x}_{t}\\|\\leq\\frac{3}{2}\\|\\tilde{{\\mathbf x}}_{t}-{\\mathbf x}_{t}\\|.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "It suffices to prove the first set of inequalities, since the second one follows similarly. To see this, note that the condition in (23) can be rewritten as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|(\\mathbf{I}+\\eta_{t}\\tilde{\\mathbf{H}}_{t})(\\hat{\\mathbf{x}}_{t}-\\hat{\\mathbf{x}}_{t}^{*})\\|\\leq\\frac{\\alpha}{2}\\|\\hat{\\mathbf{x}}_{t}-\\mathbf{x}_{t}\\|.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Since $\\tilde{\\mathbf{H}}_{t}\\succeq0$ , this further implies that $\\begin{array}{r}{\\|\\hat{\\mathbf{x}}_{t}-\\hat{\\mathbf{x}}_{t}^{*}\\|\\leq\\|\\big(\\mathbf{I}+\\eta_{t}\\tilde{\\mathbf{H}}_{t}\\big)\\big(\\hat{\\mathbf{x}}_{t}-\\hat{\\mathbf{x}}_{t}^{*}\\big)\\|\\leq\\frac{\\alpha}{2}\\|\\hat{\\mathbf{x}}_{t}-\\mathbf{x}_{t}\\|}\\end{array}$ . Hence, by the triangle inequality, we obtain that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|{\\hat{\\mathbf{x}}}_{t}^{*}-\\mathbf{x}_{t}\\|\\leq\\|{\\hat{\\mathbf{x}}}_{t}^{*}-{\\hat{\\mathbf{x}}}_{t}\\|+\\|{\\hat{\\mathbf{x}}}_{t}-\\mathbf{x}_{t}\\|\\leq\\left(1+\\displaystyle\\frac{\\alpha}{2}\\right)\\|{\\hat{\\mathbf{x}}}_{t}-\\mathbf{x}_{t}\\|\\leq\\displaystyle\\frac{3}{2}\\|{\\hat{\\mathbf{x}}}_{t}-\\mathbf{x}_{t}\\|,}\\\\ {\\displaystyle\\|{\\hat{\\mathbf{x}}}_{t}^{*}-\\mathbf{x}_{t}\\|\\geq\\|{\\hat{\\mathbf{x}}}_{t}^{*}-{\\hat{\\mathbf{x}}}_{t}\\|-\\|{\\hat{\\mathbf{x}}}_{t}-\\mathbf{x}_{t}\\|\\geq\\left(1-\\displaystyle\\frac{\\alpha}{2}\\right)\\|{\\hat{\\mathbf{x}}}_{t}-\\mathbf{x}_{t}\\|\\geq\\displaystyle\\frac{1}{2}\\|{\\hat{\\mathbf{x}}}_{t}-\\mathbf{x}_{t}\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which lead to (25). Finally, we conclude that $\\begin{array}{r}{\\|\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t}\\|\\leq2\\|\\tilde{\\mathbf{x}}_{t}^{*}-\\mathbf{x}_{t}\\|\\leq\\frac{2}{\\beta}\\|\\hat{\\mathbf{x}}_{t}^{*}-\\mathbf{x}_{t}\\|\\leq\\frac{3}{\\beta}\\|\\hat{\\mathbf{x}}_{t}-\\mathbf{x}_{t}\\|.}\\end{array}$ . This completes the proof. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "A.4 The total complexity of line search ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Let $l_{t}$ denote the number of line search steps in iteration $t$ . We first note that $\\eta_{t}=\\sigma_{t}\\beta^{l_{t}-1}$ by our line search subroutine, which implies $l_{t}=\\mathrm{{log}}_{1/\\beta}(\\sigma_{t}/\\eta_{t})+1$ . Moreover, recall that $\\sigma_{t}=\\eta_{t-1}/\\beta$ for $t\\geq1$ . Hence, the total number of line search steps after $t$ iterations can be bounded by (cf. [22, Lemma 22]): ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sum_{i=0}^{t-1}l_{i}=\\sum_{i=0}^{t-1}\\left[\\log_{1/\\beta}\\left({\\frac{\\sigma_{i}}{\\eta_{i}}}\\right)+1\\right]=2t-1+\\log\\left({\\frac{\\sigma_{0}}{\\eta_{t-1}}}\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Moreover, it can be shown that $\\eta_{t-1}\\,\\geq\\,\\alpha\\beta/(3M_{1})$ when $t\\,=\\,\\tilde{\\Omega}(\\Upsilon^{2}/\\kappa^{2})$ in both the uniform averaging and the non-uniform averaging cases (cf. Corollaries 2 and 3). This implies that the total number of line search steps can be bounded by $2t-1+\\log(3M_{1}\\sigma_{0}/\\alpha\\beta)$ . ", "page_idx": 14}, {"type": "text", "text": "A.5 Proof of Lemma 2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Recall that we use $\\mathbf{g}(\\mathbf{x})$ and $\\mathbf{H}(\\mathbf{x})$ to denote the gradient $\\nabla f(\\mathbf{x})$ and the Hessian $\\nabla^{2}f(\\mathbf{x})$ , respectively. We first consider the inequality in (9). By the fundamental theorem of calculus, we can write ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\nabla f(\\tilde{\\mathbf{x}}_{t})-\\nabla f(\\mathbf{x}_{t})=\\int_{0}^{1}\\nabla^{2}f(\\mathbf{x}_{t}+\\tau(\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t}))(\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t})~d\\tau.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, we can further use the triangle inequality to get ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|\\nabla f(\\widetilde\\mathbf{x}_{t})-\\nabla f(\\mathbf{x}_{t})-\\nabla^{2}f(\\mathbf{x}_{t})(\\widetilde\\mathbf{x}_{t}-\\mathbf{x}_{t})\\right\\|}\\\\ &{=\\left\\|\\int_{0}^{1}\\left(\\nabla^{2}f(\\mathbf{x}_{t}+\\tau(\\widetilde\\mathbf{x}_{t}-\\mathbf{x}_{t}))-\\nabla^{2}f(\\mathbf{x}_{t})\\right)(\\widetilde\\mathbf{x}_{t}-\\mathbf{x}_{t})\\;d\\tau\\right\\|}\\\\ &{\\le\\int_{0}^{1}\\left\\|\\left(\\nabla^{2}f(\\mathbf{x}_{t}+\\tau(\\widetilde\\mathbf{x}_{t}-\\mathbf{x}_{t}))-\\nabla^{2}f(\\mathbf{x}_{t})\\right)(\\widetilde\\mathbf{x}_{t}-\\mathbf{x}_{t})\\right\\|\\;d\\tau}\\\\ &{\\le\\int_{0}^{1}\\left\\|\\nabla^{2}f(\\mathbf{x}_{t}+\\tau(\\widetilde\\mathbf{x}_{t}-\\mathbf{x}_{t}))-\\nabla^{2}f(\\mathbf{x}_{t})\\right\\|\\left\\|\\widetilde\\mathbf{x}_{t}-\\mathbf{x}_{t}\\right\\|\\;d\\tau.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Moreover, we have $\\left\\|\\nabla^{2}f(\\mathbf{x}_{t}+\\tau({\\tilde{\\mathbf{x}}}_{t}-\\mathbf{x}_{t}))-\\nabla^{2}f(\\mathbf{x}_{t})\\right\\|\\leq M_{1}$ for any $\\tau\\in[0,1]$ by Assumption 2. Together with (26), this further implies that $\\begin{array}{r}{\\frac{\\|\\nabla f(\\widetilde\\mathbf{x}_{t})-\\nabla f(\\mathbf{x}_{t})-\\nabla^{2}f(\\mathbf{x}_{t})(\\widetilde\\mathbf{x}_{t}-\\mathbf{x}_{t})\\|}{\\|\\widetilde\\mathbf{x}_{t}-\\mathbf{x}_{t}\\|}\\leq M_{1}.}\\end{array}$ , which proves the first bound in (9). ", "page_idx": 15}, {"type": "text", "text": "Next, we consider the second bound in (9). By Assumption 3, it follows from standard arguments that $\\begin{array}{r}{\\|\\nabla f(\\widetilde\\mathbf{x}_{t})-\\nabla f(\\mathbf{x}_{t})-\\nabla^{2}f(\\mathbf{x}_{t})(\\widetilde\\mathbf{x}_{t}-\\mathbf{x}_{t})\\|\\leq\\frac{L_{2}}{2}\\|\\widetilde\\mathbf{x}_{t}-\\mathbf{x}_{t}\\|^{2}}\\end{array}$ (e.g., see [25, Lemma 1.2.4]). This leads to ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\|\\nabla f(\\widetilde{\\mathbf{x}}_{t})-\\nabla f(\\mathbf{x}_{t})-\\nabla^{2}f(\\mathbf{x}_{t})(\\widetilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t})\\|}{\\|\\widetilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t}\\|}\\leq\\frac{L_{2}\\|\\widetilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t}\\|}{2}\\leq\\frac{L_{2}\\|\\widetilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t}\\|}{2\\beta}\\leq\\frac{L_{2}\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|}{2\\beta\\sqrt{1-\\alpha^{2}}},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where we used $\\begin{array}{r}{\\|\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t}\\|\\leq\\frac{1}{\\beta}\\|\\hat{\\mathbf{x}}_{t}-\\mathbf{x}_{t}\\|}\\end{array}$ from Lemma 1 and $\\begin{array}{r}{\\|\\hat{\\mathbf{x}}_{t}-\\mathbf{x}_{t}\\|\\leq\\frac{1}{\\sqrt{1-\\alpha^{2}}}\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|}\\end{array}$ from Proposition 2. This completes the proof. ", "page_idx": 15}, {"type": "text", "text": "B Missing Proofs in Section 4 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Proof of Lemma 4 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Recall that in the case of uniform averaging, we have $\\begin{array}{r}{\\bar{\\mathbf{H}}_{t}=\\frac{1}{t+1}\\sum_{i=0}^{t}\\mathbf{H}_{i}}\\end{array}$ . Hence, it follows from Jensen\u2019s inequality that $\\begin{array}{r}{\\|\\bar{\\mathbf{H}}_{t}-\\mathbf{H}_{t}\\|\\leq\\frac{1}{t+1}\\sum_{i=0}^{t}\\|\\mathbf{H}_{i}-\\mathbf{H}_{t}\\|}\\end{array}$ . To prove the second claim, note that Assumption 2 directly implies $\\|\\mathbf{H}_{i}-\\mathbf{H}_{t}\\|\\,\\le\\,M_{1}$ . Moreover, we can use Assumption 3 and the triangle inequality to bound $\\|\\mathbf{H}_{t}-\\mathbf{H}_{i}\\|\\leq{\\dot{L}}_{2}\\|\\mathbf{x}_{t}-\\mathbf{x}_{i}\\|\\leq L_{2}(\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|+\\|\\mathbf{x}_{i}-\\mathbf{x}^{*}\\|)$ . Since $i\\leq t$ and $\\lVert\\mathbf x_{t}-\\mathbf x^{*}\\rVert$ is non-increasing in $t$ by Proposition 1, we further have $\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|\\leq\\|\\mathbf{x}_{i}-\\mathbf{x}^{*}\\|$ , which proves $\\|\\mathbf{H}_{t}-\\mathbf{H}_{i}\\|\\leq2L_{2}^{\\mathsf{\\bar{\\alpha}}}\\|\\mathbf{x}_{i}-\\mathbf{\\bar{x}}^{*}\\|$ . ", "page_idx": 15}, {"type": "text", "text": "B.2 Proof of Theorem 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Warm-up phase. To determine the transition point $\\mathcal{T}_{1}$ , recall that both the linear approximation error and the bias term can be bounded by $M_{1}$ according to Lemmas 2 and 4. Since Lemma 3 shows that $\\|\\bar{\\mathbf{E}}_{t}\\|=\\tilde{\\mathcal{O}}(\\Upsilon_{E}/\\sqrt{t})$ , we will have $\\|\\bar{\\mathbf{E}}_{t}\\|\\leq M_{1}$ when $t=\\tilde{\\Omega}(\\Upsilon_{E}^{2}/M_{1}^{2})=\\tilde{\\Omega}(\\Upsilon^{2}/\\kappa^{2})$ . More specifically, the transition point is given by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{T}_{1}=\\operatorname*{max}\\Bigl\\{\\frac{256\\Upsilon^{2}}{\\kappa^{2}}\\log\\frac{8d\\Upsilon}{\\kappa\\delta},4\\log\\frac{d}{\\delta},\\log_{\\frac{1}{\\beta}}\\frac{\\alpha\\beta}{3M_{1}\\sigma_{0}}\\Bigr\\},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\delta\\in(0,1)$ satisfies $d/\\delta\\geq e,\\alpha,\\beta\\in(0,1)$ are line-search parameters, and $\\sigma_{0}$ is the initial step size. ", "page_idx": 15}, {"type": "text", "text": "Linear convergence phase. In the following lemma, we prove the linear convergence of Algorithm 1 with uniform averaging. ", "page_idx": 15}, {"type": "text", "text": "Lemma 6. Assume that $\\beta\\leq1/2$ and recall the definition of $\\mathcal{T}_{1}$ in (27). For any $t\\geq\\tau_{1}$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|\\mathbf{x}_{t+1}-\\mathbf{x}^{*}\\|^{2}\\leq\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|^{2}\\left(1+\\frac{2\\alpha\\beta}{3\\kappa}\\right)^{-1},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\kappa\\triangleq M_{1}/\\mu$ is the condition number. ", "page_idx": 16}, {"type": "text", "text": "Proof. See Appendix B.3. ", "page_idx": 16}, {"type": "text", "text": "Now we discuss the transition point $\\mathcal{T}_{2}$ . At a high level, the algorithm transitions to the superlinear phase if all three errors discussed in Section 3.1 are reduced from $\\mathcal{O}(M)$ to $\\mathcal{O}(\\mu)$ . For this to happen, first the iterate $\\mathbf{x}_{t}$ needs to reach a local neighborhood satisfying $\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|=\\mathcal{O}(\\mu/L_{2})$ . As a corollary of Lemma 6, this holds at most after an additional $\\tilde{\\mathcal{O}}(\\kappa)$ iterations. Specifically, let $\\nu\\in(0,1)$ be a parameter and define ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{Z}=\\mathcal{T}_{1}+2\\Big(1+\\frac{3\\kappa}{2\\alpha\\beta}\\Big)\\log\\frac{L_{2}D}{\\nu\\mu},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $D=\\|\\mathbf{x}_{0}-\\mathbf{x}^{*}\\|$ is the initial distance to the optimal solution. Then Lemma 6 implies that we have $\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|\\leq\\nu\\mu/L_{2}$ for all $t\\geq T$ . Moreover, Lemma 3 implies that the averaged Hessian noise satisfies $\\|\\bar{\\mathbf{E}}_{t}\\|\\,=\\,\\mathcal{O}(\\mu)$ when $t\\,=\\,\\tilde{\\Omega}(\\Upsilon_{E}^{2}/\\mu^{2})\\,=\\,\\tilde{\\Omega}(\\Upsilon^{2})$ . Finally, regarding the bias term, following the discussions after Lemma 4, it can be shown that $\\begin{array}{r}{\\|\\mathbf{H}_{t}-\\bar{\\mathbf{H}}_{t}\\|=\\mathcal{O}\\left(\\frac{M_{1}\\mathcal{Z}}{t+1}\\right)}\\end{array}$ . Thus, $\\|\\mathbf{H}_{t}-\\bar{\\mathbf{H}}_{t}\\|=\\mathcal{O}(\\mu)$ when $t=\\Omega(\\kappa T)$ . Combining all pieces together, we formally define the second transition point by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{T}_{2}=\\operatorname*{max}\\bigg\\{\\frac{256\\Upsilon^{2}}{\\nu^{2}}\\log\\frac{8d\\Upsilon}{\\delta\\nu},\\frac{\\kappa\\mathcal{T}}{\\nu}-1\\bigg\\}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Superlinear phase. In the following theorem, we show that after $\\mathcal{T}_{2}$ iterations, Algorithm 1 with uniform averaging converges at a superlinear rate. See Appendix B.4 for proof. ", "page_idx": 16}, {"type": "text", "text": "Theorem 3. Let $\\nu\\in(0,1)$ be a parameter satisfying $\\begin{array}{r}{\\left(\\frac{5}{2\\alpha\\beta\\sqrt{(1-\\alpha^{2})\\beta}}+\\frac{25}{\\alpha\\sqrt{2\\beta}}\\right)\\nu\\leq1.}\\end{array}$ . and recall the definition of $\\mathcal{T}_{2}$ in (29). Then for any $t\\geq\\tau_{2}$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|\\mathbf{x}_{t+1}-\\mathbf{x}^{*}\\|\\leq\\left(\\frac{1}{2\\beta\\sqrt{1-\\alpha^{2}}}+5\\right)\\rho_{t}\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\rho_{t}=\\frac{4\\Upsilon}{\\alpha\\sqrt{\\beta}}\\sqrt{\\frac{\\log(d(t+1)/\\delta)}{t+1}}+\\frac{3\\kappa\\mathcal{T}}{2\\alpha\\sqrt{\\beta}(t+1)}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In Theorem 3, we observe that the rate $\\rho_{t}$ in (30) goes to zero as the number of iterations $t$ increases, and thus it implies that the iterates converge to $\\mathbf{x}^{*}$ superlinearly. Moreover, the rate $\\rho_{t}$ consists of t\u221awo terms. The first term comes from the averaged noise $\\|\\bar{\\mathbf{E}}_{t}\\|$ , which decays at the rate of $\\tilde{\\mathcal{O}}(\\Upsilon/\\sqrt{t})$ . In addition, the second term is due to the bias of our averaged Hessian estimate $\\tilde{\\mathbf{H}}_{t}$ , which decays at the rate of $\\mathcal{O}(\\kappa\\mathcal{Z}/t)$ . Hence, when $t$ is sufficiently large, the averaged noise will dominate and the superlinear rate settles for the slower rate of $\\tilde{\\mathcal{O}}(\\Upsilon/\\sqrt{t})$ . Specifically, the algorithm transitions from the initial superlinear rate to the final superlinear rate when the two terms in (30) are balanced. Hence, we define the third transition point $\\mathcal{T}_{3}$ as the root of ", "page_idx": 16}, {"type": "equation", "text": "$$\n64(\\mathcal{T}_{3}+1)\\log(d(\\mathcal{T}_{3}+1)/\\delta)=\\frac{9\\kappa^{2}\\mathcal{T}^{2}}{\\Upsilon^{2}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since $\\mathcal{Z}=\\tilde{\\mathcal{O}}(\\Upsilon^{2}/\\kappa^{2}+\\kappa)$ , $T_{3}=\\tilde{\\mathcal{O}}((\\Upsilon^{2}/\\kappa+\\kappa^{2})^{2}/\\Upsilon^{2})$ . We summarize our discussions in the following corollary. ", "page_idx": 16}, {"type": "text", "text": "Corollary 1. For $\\begin{array}{r}{\\mathcal{T}_{2}\\leq t\\leq\\mathcal{T}_{3}-1}\\end{array}$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|\\mathbf{x}_{t+1}-\\mathbf{x}^{*}\\|\\leq\\left(\\frac{1}{2\\beta\\sqrt{1-\\alpha^{2}}}+5\\right)\\rho_{t}^{(1)}\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where \u03c1t $\\begin{array}{r}{\\rho_{t}^{(1)}=\\frac{6\\kappa\\mathcal{T}}{\\alpha\\sqrt{2\\beta}(t+1)}}\\end{array}$ \u03b1\u221a26\u03b2\u03ba(It+1). Moreover, for t \u2265T3, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|\\mathbf{x}_{t+1}-\\mathbf{x}^{*}\\|\\leq\\left(\\frac{1}{2\\beta\\sqrt{1-\\alpha^{2}}}+5\\right)\\rho_{t}^{(2)}\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "B.3 Proof of Lemma 6 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We divide the proof of Lemma 6 into the following three steps. First, in Lemma 7, we provide a lower bound on the the step size $\\eta_{t}$ in those iterations where our line search scheme backtracks the step size, i.e., $t\\in{\\boldsymbol{B}}$ . Building on Lemma 7, we use induction in Lemma 8 to prove a lower bound for all $t\\geq0$ . This allows us to establish $\\eta_{t}=\\Omega(1/M_{1})$ for all $t\\geq\\tau_{1}$ in Corollary 2, from which Lemma 6 immediately follows. ", "page_idx": 17}, {"type": "text", "text": "To simplify the notation, we define the function ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\phi(t)=8\\Upsilon_{E}\\sqrt{\\frac{\\log(d(t+1)/\\delta)}{t+1}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then Lemma 3 can be equivalently written as $\\|\\bar{\\mathbf{E}}_{t}\\|\\leq\\phi(t)$ for all $t\\geq4\\log(d/\\delta)$ . We are now ready to state our first lemma. ", "page_idx": 17}, {"type": "text", "text": "Lemma 7. If $t\\in\\mathcal{B}$ , then we have $\\eta_{t}\\geq\\alpha\\beta/(2M_{1}+\\phi(t))$ ", "page_idx": 17}, {"type": "text", "text": "Proof. If $t\\in{\\boldsymbol{B}}$ , by Lemma 1 we can lower bound the step size $\\eta_{t}$ by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\eta_{t}\\geq\\frac{\\alpha\\beta\\|\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t}\\|}{\\|\\mathbf{g}(\\tilde{\\mathbf{x}}_{t})-\\mathbf{g}(\\mathbf{x}_{t})-\\tilde{\\mathbf{H}}_{t}(\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t})\\|}=\\frac{\\alpha\\beta}{\\mathcal{E}_{t}},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathcal{E}_{t}\\triangleq\\frac{\\Vert\\mathbf{g}(\\tilde{\\mathbf{x}}_{t})-\\mathbf{g}(\\mathbf{x}_{t})-\\tilde{\\mathbf{H}}_{t}\\left(\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t}\\right)\\Vert}{\\Vert\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t}\\Vert}}\\end{array}$ is the normalized approximation error. Moreover, as outlined in Section 3.1, we can apply the triangle inequality to upper bound $\\mathcal{E}_{t}$ . Specifically, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{E}_{t}\\leq\\frac{\\|\\mathbf{g}(\\widetilde{\\mathbf{x}}_{t})-\\mathbf{g}(\\mathbf{x}_{t})-\\mathbf{H}_{t}(\\widetilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t})\\|}{\\|\\widetilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t}\\|}+\\|\\mathbf{H}_{t}-\\bar{\\mathbf{H}}_{t}\\|+\\|\\bar{\\mathbf{E}}_{t}\\|.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By (9) in Lemma 2, the first term in (33) is upper bounded by $M_{1}$ . Moreover, it also follows from Lemma 4 that $\\begin{array}{r}{\\|\\mathbf{H}_{t}-\\bar{\\mathbf{H}}_{t}\\|\\,\\leq\\,\\frac{1}{t+1}\\sum_{i=0}^{t}\\|\\mathbf{H}_{t}-\\mathbf{H}_{i}\\|\\,\\leq\\,M_{1}}\\end{array}$ . Hence, we further obtain $\\mathcal{E}_{t}\\,\\leq$ $2M_{1}+\\|\\bar{\\mathbf{E}}_{t}\\|\\leq2M_{1}+\\phi(t)$ . Combining this with (32), we obtain the desired result. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Lemma 7 provides a lower bound on the step size $\\eta_{t}$ , but only for the case where $t\\in{\\boldsymbol{B}}$ . In the next lemma, we further use induction to show a lower bound for the step sizes in all iterations. ", "page_idx": 17}, {"type": "text", "text": "Lemma 8. Assume that $\\beta\\leq\\frac{1}{2}$ . For any $t\\geq0$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\eta_{t}\\geq\\operatorname*{min}\\left\\{\\frac{\\alpha\\beta}{2M_{1}+\\phi(t)},\\frac{\\sigma_{0}}{\\beta^{t}}\\right\\}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. We prove this lemma by induction. For the base case $t=0$ , we consider two subcases. If 0 \u2208B, then by Lemma 7 we obtain that \u03b70 \u22652M1\u03b1+\u03b2\u03d5(0). Otherwise, if $0\\notin\\mathcal{B}$ , we have $\\eta_{0}=\\sigma_{0}$ . In both cases, we observe that (34) is satisfied for the base case $t=0$ . ", "page_idx": 17}, {"type": "text", "text": "Now assume that (34) is satisfied for $t=s$ where $s\\geq0$ . For $t=s+1$ , we again distinguish two subcases. If $s+1\\in B$ , then by Lemma 7 we obtain that $\\begin{array}{r}{\\eta_{s+1}\\geq\\frac{\\alpha\\beta}{2M_{1}+\\phi(s+1)}}\\end{array}$ , which implies that (34) is satisfied. Otherwise, if $s+1\\notin B$ , then we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\eta_{s+1}=\\sigma_{s+1}=\\frac{\\eta_{s}}{\\beta}\\geq\\frac{1}{\\beta}\\operatorname*{min}\\left\\{\\frac{\\alpha\\beta}{2M_{1}+\\phi(s)},\\frac{\\sigma_{0}}{\\beta^{s}}\\right\\}=\\operatorname*{min}\\left\\{\\frac{\\alpha}{2M_{1}+\\phi(s)},\\frac{\\sigma_{0}}{\\beta^{s+1}}\\right\\},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we used the induction hypothesis in the last inequality. Furthermore, note that $\\phi(s)/\\phi(s\\!+\\!1)\\leq$ $\\begin{array}{r}{\\sqrt{\\frac{s+2}{s+1}}\\le2\\le\\frac1\\beta}\\end{array}$ , which implies that $\\phi(s)\\leq\\phi(s+1)/\\beta$ . Hence, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{\\alpha}{2M_{1}+\\phi(s)}\\geq\\frac{\\alpha\\beta}{2\\beta M_{1}+\\phi(s+1)}\\geq\\frac{\\alpha\\beta}{2M_{1}+\\phi(s+1)}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, (35) implies that \u03b7s+1 \u2265 min 2M1+\u03b1\u03d5\u03b2(s+1),\u03b2s\u03c30+1 and thus (34) also holds in this subcase. This completes the induction and we conclude that (34) holds for all $t\\geq0$ . \u53e3 ", "page_idx": 18}, {"type": "text", "text": "As a corollary of Lemma 8, we obtain the following lower bound on $\\eta_{t}$ for $t\\geq\\tau_{1}$ . ", "page_idx": 18}, {"type": "text", "text": "Corollary 2. Recall the definition of $\\mathcal{T}_{1}$ in (27). For any $t\\geq\\tau_{1}$ , we have $\\eta_{t}\\geq\\alpha\\beta/(3M_{1})$ . ", "page_idx": 18}, {"type": "text", "text": "Proof. As shown in [1, Lemma 2], we have $\\phi(t)\\leq M_{1}$ when $\\begin{array}{r}{t\\geq\\operatorname*{max}\\left\\lbrace256\\frac{\\Upsilon^{2}}{\\kappa^{2}}\\log\\frac{8d\\Upsilon}{\\kappa\\delta},4\\log\\frac{d}{\\delta}\\right\\rbrace}\\end{array}$ . Moreover, we have \u03c3\u03b20t \u2265 3 when t \u2265 log \u03b21 3M\u03b11\u03b2\u03c30 . Hence, by Lemma 8 we conclude that $\\begin{array}{r}{\\eta_{t}\\ge\\frac{\\alpha\\beta}{3M_{1}}}\\end{array}$ when $t\\geq\\tau_{1}$ . \u53e3 ", "page_idx": 18}, {"type": "text", "text": "Now we are ready to prove Lemma 6. ", "page_idx": 18}, {"type": "text", "text": "Proof of Lemma 6. By Proposition 1, we have $\\|\\mathbf{x}_{t+1}-\\mathbf{x}^{*}\\|^{2}\\leq\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|^{2}(1+2\\eta_{t}\\mu)^{-1}$ . By using Corollary 2, we obtain that $\\|\\mathbf{x}_{t+1}-\\mathbf{x}^{*}\\|^{2}\\leq\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|^{2}(1+2\\alpha\\beta\\mu/(3M_{1}))^{-1}=\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|^{2}(1+{\\frac{\\alpha}{2}})^{2}.$ $2\\alpha\\beta/(3\\dot{\\kappa}))^{-1}$ . \u53e3 ", "page_idx": 18}, {"type": "text", "text": "B.4 Proof of Theorem 3 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we present the proof of Theorem 3. The proof consists of four steps. To begin with, in Lemma 9 we show that the iterate $\\mathbf{x}_{t}$ stays in a local neighborhood of $\\mathbf{x}^{*}$ when $t\\geq T$ , where $\\mathcal{T}$ is defined in (28). Next, we use this result in Lemma 10 to upper bound $\\frac{1}{\\sqrt{\\mu\\eta_{t}}}$ in those iterations where our line search scheme backtracks the step size, i.e., $t\\in{\\boldsymbol{B}}$ . Then we use induction in Lemma 11 to prove an upper bound for all $t\\geq0$ . Furthermore, we again use induction in Lemma 12 to establish an improved upper bound on $\\frac{1}{\\sqrt{\\mu\\eta_{t}}}$ when $t\\geq\\tau_{2}$ , where $\\mathcal{T}_{2}$ is defined in (29). After proving Lemma 12, Theorem 3 then follows from Proposition 1. ", "page_idx": 18}, {"type": "text", "text": "Lemma 9. We have $\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|\\leq\\nu\\mu/L_{2}$ for all $t\\geq T$ , where $\\mathcal{T}$ is given in (28). ", "page_idx": 18}, {"type": "text", "text": "Proof. By applying Lemma 6, we have $\\begin{array}{r}{\\|\\mathbf{x}_{\\mathcal{T}_{1}+u}-\\mathbf{x}^{*}\\|^{2}\\leq\\|\\mathbf{x}_{\\mathcal{T}_{1}}-\\mathbf{x}^{*}\\|^{2}\\left(1+\\frac{2\\alpha\\beta}{3\\kappa}\\right)^{-u}}\\end{array}$ . Moreover, since $\\lVert\\mathbf x_{t}-\\mathbf x^{*}\\rVert$ is non-increasing in $t$ , we have $\\|\\mathbf{x}_{\\mathcal{T}_{1}}-\\mathbf{x}^{*}\\|\\le\\|\\mathbf{x}_{0}-\\mathbf{x}^{*}\\|^{\\prime}=D$ . Thus, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|\\mathbf{x}_{{\\mathcal{T}}_{1}+u}-\\mathbf{x}^{*}\\|\\le\\frac{\\nu\\mu}{L_{2}}\\,\\Leftarrow\\,D^{2}\\left(1+\\frac{2\\alpha\\beta}{3\\kappa}\\right)^{-u}\\le\\frac{\\nu^{2}\\mu^{2}}{L_{2}^{2}}\\,\\Leftarrow\\,u\\ge2\\Big(1+\\frac{3\\kappa}{2\\alpha\\beta}\\Big)\\log\\left(\\frac{L_{2}D}{\\nu\\mu}\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This completes the proof. ", "page_idx": 18}, {"type": "text", "text": "Note that by Proposition 1, we have $\\|\\mathbf{x}_{t+1}-\\mathbf{x}^{*}\\|^{2}\\leq\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|^{2}(1+2\\eta_{t}\\mu)^{-1}\\leq\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|^{2}/(2\\eta_{t}\\mu),$ which further implies that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|\\mathbf{x}_{t+1}-\\mathbf{x}^{*}\\|\\leq\\frac{\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|}{\\sqrt{2\\eta_{t}\\mu}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "He\u221ance, to characterize the convergence rate of our method, it is sufficient to upper bound the quantity $1/\\sqrt{2\\eta_{t}\\mu}$ . We achieve this goal in the subsequent lemmas. ", "page_idx": 18}, {"type": "text", "text": "Lemma 10. If $t\\in{\\boldsymbol{B}}$ and $t\\geq T$ , then ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{1}{\\sqrt{2\\eta_{t}\\mu}}\\leq\\frac{L_{2}\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|}{4\\alpha\\beta\\sqrt{(1-\\alpha^{2})\\beta}\\mu}+\\frac{\\|\\bar{\\mathbf{E}}_{t}\\|}{2\\alpha\\sqrt{\\beta}\\mu}+\\frac{3\\kappa Z}{2\\alpha\\sqrt{\\beta}(t+1)}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Moreover, it also holds that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{1}{\\sqrt{2\\eta_{t}\\mu}}\\leq\\frac{\\nu}{4\\alpha\\beta\\sqrt{(1-\\alpha^{2})\\beta}}+\\frac{\\phi(t)}{2\\alpha\\sqrt{\\beta}\\mu}+\\frac{3\\kappa\\mathcal{Z}}{2\\alpha\\sqrt{\\beta}(t+1)}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. By using the second bound in Lemma 1, we obtain that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{1}{\\sqrt{2\\eta_{t}\\mu}}\\leq\\frac{\\|\\mathbf{g}(\\tilde{\\mathbf{x}}_{t})-\\mathbf{g}(\\mathbf{x}_{t})-\\tilde{\\mathbf{H}}_{t}(\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t})\\|}{2\\alpha\\sqrt{\\beta}\\mu\\|\\tilde{\\mathbf{x}}_{t}-\\mathbf{x}_{t}\\|}=\\frac{\\mathcal{E}_{t}}{2\\alpha\\sqrt{\\beta}\\mu}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Furthermore, by combining (33) and (9) in Lemma 2, we further have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{E}_{t}\\leq\\frac{L_{2}\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|}{2\\beta\\sqrt{1-\\alpha^{2}}}+\\|\\mathbf{H}_{t}-\\bar{\\mathbf{H}}_{t}\\|+\\|\\bar{\\mathbf{E}}_{t}\\|.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "It remains to bound the bias term $\\left\\|\\mathbf{H}_{t}-\\bar{\\mathbf{H}}_{t}\\right\\|$ . By Lemma 4, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|\\mathbf{H}_{t}-\\bar{\\mathbf{H}}_{t}\\|\\leq\\frac{1}{t+1}\\sum_{i=0}^{t}\\|\\mathbf{H}_{t}-\\mathbf{H}_{i}\\|=\\frac{1}{t+1}\\sum_{i=0}^{T-1}\\|\\mathbf{H}_{t}-\\mathbf{H}_{i}\\|+\\frac{1}{t+1}\\sum_{i=T}^{t}\\|\\mathbf{H}_{t}-\\mathbf{H}_{i}\\|.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For $i=0,1\\dots,{\\mathcal{I}}{-}1$ , we use the first upper bound on $\\left\\|\\mathbf{H}_{t}\\!-\\mathbf{H}_{i}\\right\\|$ in Lemma 4 to bound $\\|\\mathbf{H}_{t}-\\mathbf{H}_{i}\\|\\leq$ $M_{1}$ , and thus $\\begin{array}{r}{\\sum_{i=0}^{{\\mathcal T}-1}\\|{\\bf H}_{t}-{\\bf H}_{i}\\|\\le M_{1}{\\mathcal L}}\\end{array}$ . Moreover, for $i=\\mathcal{I},\\mathcal{I}+1,\\dots,t$ , we use the second upper bound i n Lemma 4 to get $\\|\\mathbf{H}_{t}-\\mathbf{H}_{i}\\|\\leq2L_{2}\\|\\mathbf{x}_{i}-\\mathbf{x}^{*}\\|$ . Moreover, note that converges linearly to $\\mathbf{x}^{*}$ when $i\\geq\\mathcal{T}$ by Lemma 6. Hence, we further have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{t+1}\\sum_{i=\\tau}^{t}\\|\\mathbf{H}_{t}-\\mathbf{H}_{i}\\|\\leq\\frac{2L_{2}}{t+1}\\sum_{i=\\tau}^{t}\\|\\mathbf{x}_{i}-\\mathbf{x}^{*}\\|\\leq\\frac{2L_{2}\\|\\mathbf{x}_{\\mathcal{I}}-\\mathbf{x}^{*}\\|}{t+1}\\sum_{i=0}^{\\infty}\\left(1+\\frac{2\\alpha\\beta}{3\\kappa}\\right)^{-i/2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\frac{4\\nu\\mu}{t+1}\\left(1+\\frac{3\\kappa}{2\\alpha\\beta}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In the last inequality, we used the fact that $\\|\\mathbf{x}_{\\mathcal{T}}-\\mathbf{x}^{*}\\|\\leq\\nu\\mu/L_{2}$ and $\\textstyle\\sum_{i=0}^{\\infty}(1+\\phi)^{-i/2}=1/(1-$ ( $1+\\phi)^{-1/2})=(1+\\phi)^{1/2}/((1+\\phi)^{1/2}-1)=(1+\\phi)^{1/2}((1+\\phi)^{1/2}+1)/\\phi\\leq2(1+1/\\phi),$ where $\\phi=2\\alpha\\beta/(3\\kappa)$ . Moreover, since $\\begin{array}{r}{{\\cal Z}\\ge2\\left(1+\\frac{3\\kappa}{2\\alpha\\beta}\\right)}\\end{array}$ and $\\nu\\leq1$ , from (41) we further have $\\begin{array}{r}{\\frac{1}{t+1}\\sum_{i=\\mathcal{Z}}^{t}\\|\\mathbf{H}_{t}-\\mathbf{H}_{i}\\|\\leq\\frac{2\\mu\\mathcal{T}}{t+1}}\\end{array}$ . Combining the above inequalities, we arrive at ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{H}_{t}-\\bar{\\mathbf{H}}_{t}\\right\\|\\leq\\frac{M_{1}\\mathcal{T}}{t+1}+\\frac{2\\mu\\mathcal{T}}{t+1}\\leq\\frac{3M_{1}\\mathcal{T}}{t+1}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Combining (39), (40), and (42) leads to the first result in (37). Finally, (38) follows from the fact that $\\lVert\\bar{\\mathbf{E}}_{t}\\rVert\\leq\\bar{\\phi(t)}$ and $\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|\\leq\\nu\\mu/L_{2}$ for all $t\\geq T$ . \u53e3 ", "page_idx": 19}, {"type": "text", "text": "Lemma 11. Assume that $\\beta\\leq1/2$ . For any $t\\geq T$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\frac{1}{\\sqrt{2\\eta_{t}\\mu}}}\\leq{\\frac{\\nu}{4\\alpha\\beta{\\sqrt{(1-\\alpha^{2})\\beta}}}}+{\\frac{\\phi(t)}{2\\alpha{\\sqrt{\\beta}}\\mu}}+{\\frac{3\\kappa{\\mathcal{Z}}}{2\\alpha{\\sqrt{\\beta}}(t+1)}}={\\frac{\\nu}{4\\alpha\\beta{\\sqrt{(1-\\alpha^{2})\\beta}}}}+\\rho_{t},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\rho_{t}$ is defined in (30). ", "page_idx": 19}, {"type": "text", "text": "Proof. We shall use induction to prove Lemma 11. For $t=\\mathcal{T}$ , note that by Corollary 2, we have $\\eta_{\\mathcal{T}}\\stackrel{.}{\\geq}\\alpha\\beta/(3M_{1})$ . Thus, this implies that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{1}{\\sqrt{2\\eta\\tau\\mu}}\\leq\\frac{\\sqrt{3\\kappa}}{\\sqrt{2\\alpha\\beta}}\\leq\\frac{3\\kappa\\mathcal{T}}{2\\alpha\\sqrt{\\beta}(\\mathcal{T}+1)},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where we used the fact that $\\kappa\\geq1$ , $\\alpha<1$ , $\\beta\\leq1/2$ and $\\mathcal{Z}\\geq4$ . This proves the base case where $t=\\mathcal{T}$ . ", "page_idx": 19}, {"type": "text", "text": "Now assume that (43) holds for $t=s$ , where $s\\geq\\mathcal{T}$ . For $t=s+1$ , we distinguish two subcases. If $s+1\\in B$ , then by Lemma 10 we obtain that (43) is satisfied for $t=s+1$ . Otherwise, if $s+1\\notin B$ , then we have $\\eta_{s+1}=\\sigma_{s+1}=\\eta_{s}/\\beta$ . Hence, by using the induction hypothesis, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{1}{\\sqrt{2\\eta_{s+1}\\mu}}=\\frac{\\sqrt{\\beta}}{\\sqrt{2\\eta_{s}\\mu}}\\leq\\frac{\\nu}{4\\alpha\\beta\\sqrt{(1-\\alpha^{2})\\beta}}+\\frac{\\sqrt{\\beta}\\phi(s)}{2\\alpha\\sqrt{\\beta}\\mu}+\\frac{3\\sqrt{\\beta}\\kappa\\mathcal{I}}{2\\alpha\\sqrt{\\beta}(s+1)}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since $\\beta\\,\\leq\\,1/2$ an\u221ad $\\mathcal{T}\\geq2$ , we have $(s+2)/(s+1)\\,\\leq\\,(\\mathbb{Z}+2)/(\\mathbb{Z}+1)\\,\\leq\\,1.4\\,\\leq\\,1/\\sqrt{\\beta}$ and $\\phi(s)\\leq\\phi(s+1)/\\sqrt{\\beta}$ . Thus, we further have ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\frac{1}{\\sqrt{2\\eta_{s+1}\\mu}}}\\leq{\\frac{\\nu}{4\\alpha\\beta{\\sqrt{(1-\\alpha^{2})\\beta}}}}+{\\frac{\\phi(s+1)}{2\\alpha{\\sqrt{\\beta}}\\mu}}+{\\frac{3\\kappa{\\mathcal{Z}}}{2\\alpha{\\sqrt{\\beta}}(s+2)}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This shows that (43) also holds in this subcase. This completes the induction and we conclude that (43) holds for all $t\\geq T$ . \u53e3 ", "page_idx": 19}, {"type": "text", "text": "Before proving Lemma 12, recall the definition of $\\phi$ in (31) and first define ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{Z}^{\\prime}=\\operatorname*{sup}_{t}\\left\\{t:\\phi(t)\\geq\\nu\\mu\\right\\}\\quad\\mathrm{and}\\quad\\mathcal{T}_{2}^{\\prime}=\\operatorname*{max}\\left\\{\\mathcal{Z}^{\\prime},\\frac{\\kappa\\mathcal{Z}}{\\nu}-1\\right\\}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that we have $\\phi(t)\\leq\\nu\\mu$ when $\\begin{array}{r}{t\\geq\\frac{256\\Upsilon^{2}}{\\nu^{2}}\\log\\frac{8d\\Upsilon}{\\delta\\nu}}\\end{array}$ . Hence, by the definition of (29), it holds that $\\tau_{2}\\geq T_{2}^{\\prime}$ . ", "page_idx": 20}, {"type": "text", "text": "Lemma 12. Recall the definition of $\\rho_{t}$ in (30). For any $t\\geq T_{2}^{\\prime}$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{L_{2}\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|}{2\\alpha\\sqrt{\\beta}\\mu}\\leq\\rho_{t}\\quad a n d\\quad\\frac{1}{\\sqrt{2\\mu\\eta_{t}}}\\leq\\left(\\frac{1}{2\\beta\\sqrt{1-\\alpha^{2}}}+5\\right)\\rho_{t}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. By Lemma 10, if $t\\in{\\boldsymbol{B}}$ , then ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{1}{\\sqrt{\\mu\\eta_{t}}}\\leq\\frac{L_{2}\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|}{2\\alpha\\beta\\sqrt{2(1-\\alpha^{2})\\beta}\\mu}+\\rho_{t}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We shall prove (45) by induction. First consider the base case where $t=\\tau_{2}^{\\prime}$ , where $\\ensuremath{\\mathcal{T}}_{2}^{\\prime}$ is defined in (44). To begin with, we will show that $\\begin{array}{r}{\\frac{\\nu}{2\\alpha\\sqrt{\\beta}}\\,\\leq\\,\\rho_{T_{2}^{\\prime}}\\,\\leq\\,\\frac{5\\nu}{2\\alpha\\sqrt{\\beta}}}\\end{array}$ . Since $\\mathcal{T}_{2}^{\\prime}$ is the maximum of ${\\mathcal{Z}}^{\\prime}$ $\\frac{\\kappa\\mathcal{I}}{\\nu}$ $\\mathcal{T}_{2}^{\\prime}\\,=\\,\\mathcal{Z}^{\\prime}$ laotrt $\\begin{array}{r}{\\mathcal{T}_{2}^{\\prime}\\,=\\,\\frac{\\kappa\\mathcal{I}}{\\nu}\\mathrm{~-~}1}\\end{array}$ n.  lIonw tehr eb foournmd . $\\begin{array}{r}{\\rho_{{T_{2}^{\\prime}}}\\geq\\frac{1}{2\\alpha\\sqrt{\\beta}\\mu}\\phi({Z^{\\prime}})\\geq\\frac{\\nu}{2\\alpha\\sqrt{\\beta}}}\\end{array}$ $\\begin{array}{r}{\\rho_{\\mathcal{T}_{2}^{\\prime}}\\geq\\frac{3\\kappa\\mathcal{Z}}{2\\alpha\\sqrt{\\beta}(\\mathcal{T}_{2}^{\\prime}+1)}=\\frac{3\\nu}{2\\alpha\\sqrt{\\beta}}}\\end{array}$ Combining both cases leads to the lower bound on $\\rho_{\\mathcal{T}_{2}^{\\prime}}$ . Furthermore, note that both two terms in $\\rho_{t}$ are a decreasing function in terms of $t$ , and hence we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\rho_{T_{2}^{\\prime}}\\leq\\frac{1}{2\\alpha\\sqrt{\\beta}\\mu}\\phi(\\mathbb{Z}^{\\prime})+\\frac{3\\kappa\\mathbb{Z}}{2\\alpha\\sqrt{\\beta}(\\kappa\\mathbb{Z}/\\nu)}\\leq\\frac{2}{2\\alpha\\sqrt{\\beta}\\mu}\\phi(\\mathbb{Z}^{\\prime}+1)+\\frac{3\\nu}{2\\alpha\\sqrt{\\beta}}\\leq\\frac{5\\nu}{2\\alpha\\sqrt{\\beta}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This proves the upper bound on $\\rho_{\\mathcal{T}_{2}^{\\prime}}$ . ", "page_idx": 20}, {"type": "text", "text": "Now we return to the proof in the base case where $t=\\tau_{2}^{\\prime}$ . since $\\|\\mathbf{x}_{T_{2}^{\\prime}}-\\mathbf{x}^{*}\\|\\le\\nu\\mu/L_{2}$ by Lemma 9, we obtain that 22\u03b1T\u221a 2 \u03b2\u00b5 \u22642\u03b1\u03bd\u221a\u03b2 \u2264\u03c1T 2\u2032 . Moreover, by Lemma 11, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n{\\frac{1}{\\sqrt{2\\eta_{T_{2}^{\\prime}}\\mu}}}\\leq{\\frac{\\nu}{4\\alpha\\beta{\\sqrt{(1-\\alpha^{2})\\beta}}}}+\\rho(T_{2}^{\\prime})\\leq{\\frac{\\nu}{4\\alpha\\beta{\\sqrt{(1-\\alpha^{2})\\beta}}}}+{\\frac{5\\nu}{2\\alpha{\\sqrt{\\beta}}}}\\leq\\left({\\frac{1}{2\\beta{\\sqrt{1-\\alpha^{2}}}}}+5\\right)\\rho_{T_{2}^{\\prime}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This shows that (45) holds for $t=\\tau_{2}^{\\prime}$ . ", "page_idx": 20}, {"type": "text", "text": "Now assume that (45) holds for $t=s\\geq T_{2}^{\\prime}$ . For $t=s+1$ , by using the induction hypothesis and (36), we obtain that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{L_{2}\\|\\mathbf{x}_{s+1}-\\mathbf{x}^{*}\\|}{2\\alpha\\sqrt{\\beta}\\mu}\\leq\\frac{L_{2}\\|\\mathbf{x}_{s}-\\mathbf{x}^{*}\\|}{2\\alpha\\sqrt{\\beta}\\mu\\sqrt{2\\eta_{s}\\mu}}\\leq\\frac{1}{\\sqrt{2}}\\left(\\frac{1}{2\\beta\\sqrt{1-\\alpha^{2}}}+5\\right)\\rho_{s}^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Moreover, since $\\rho_{s}/2\\,\\leq\\,\\rho_{s+1}$ , it suffices to show that $\\begin{array}{r}{\\frac{1}{\\sqrt{2}}\\left(\\frac{1}{2\\beta\\sqrt{1-\\alpha^{2}}}+5\\right)\\rho_{s}^{2}\\,\\leq\\,\\rho_{s}/2}\\end{array}$ , which is equivalent to $\\begin{array}{r}{\\sqrt{2}\\left(\\frac{1}{2\\beta\\sqrt{1-\\alpha^{2}}}+5\\right)\\rho_{s}\\leq1}\\end{array}$ . Furthermore, since $\\rho_{s}$ is non-increasing, we further have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sqrt{2}\\left(\\frac{1}{2\\beta\\sqrt{1-\\alpha^{2}}}+5\\right)\\rho_{s}\\le\\sqrt{2}\\left(\\frac{1}{2\\beta\\sqrt{1-\\alpha^{2}}}+5\\right)\\rho_{T_{2}^{\\prime}}\\le\\sqrt{2}\\left(\\frac{1}{2\\beta\\sqrt{1-\\alpha^{2}}}+5\\right)\\frac{5\\nu}{2\\alpha\\sqrt{\\beta}}\\le1,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we used the condition on $\\nu$ stated in Theorem 3 in the last inequality. This proves the first inequality in (45). ", "page_idx": 20}, {"type": "text", "text": "To prove the second inequality in (45) for $t=s+1$ , we distinguish two subcases. If $s+1\\in B$ , then by Lemma 10, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\frac{1}{\\sqrt{2\\eta_{s+1}\\mu}}\\leq\\frac{L_{2}\\|\\mathbf{x}_{s+1}-\\mathbf{x}^{*}\\|}{4\\alpha\\beta\\sqrt{(1-\\alpha^{2})\\beta}\\mu}+\\frac{\\|\\bar{\\mathbf{E}}_{s+1}\\|}{2\\alpha\\sqrt{\\beta}\\mu}+\\frac{3\\kappa\\mathcal{Z}}{2\\alpha\\sqrt{\\beta}(s+1)}\\leq\\frac{1}{2\\beta\\sqrt{1-\\alpha^{2}}}\\rho_{s+1}+\\rho_{s+1}}\\\\ &{}&{\\leq\\left(\\frac{1}{2\\beta\\sqrt{1-\\alpha^{2}}}+5\\right)\\rho_{s+1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Otherwise, if $s+1\\notin B$ , then we have $\\eta_{s+1}=\\eta_{s}/\\beta$ and hence ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{1}{\\sqrt{2\\eta_{s+1}\\mu}}=\\frac{\\sqrt{\\beta}}{\\sqrt{2\\eta_{s}\\mu}}\\leq\\left(\\frac{1}{2\\beta\\sqrt{1-\\alpha^{2}}}+5\\right)\\sqrt{\\beta}\\rho_{s}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since $T_{2}^{\\prime}\\geq\\mathbb{Z}\\geq2$ and $\\beta\\leq1/2$ , we have $\\rho_{s}/\\rho_{s+1}\\leq(\\mathbb{Z}+2)/(\\mathbb{Z}+1)\\leq1.4\\leq1/\\sqrt{\\beta}$ . Thus, we also proved that \u221a\u00b5\u03b7s+1 \u2264 2\u03b2\u221a11\u2212\u03b12 + 5 \u03c1s+1. This completes the induction. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "Proof of Theorem 3. It immediately follows from (36) and Lemma 12. ", "page_idx": 21}, {"type": "text", "text": "C Missing Proofs in Section 5 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we will present the formal version of Theorem 2. Our proof largely mirrors the developments in Section 4. ", "page_idx": 21}, {"type": "text", "text": "C.1 Approximation error analysis ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Averaged stochastic error. Similar to Lemma 3, we can use tools from concentration inequalities to prove the following upper bound on the averaged stochastic error. ", "page_idx": 21}, {"type": "text", "text": "Lemma 13 ([1, Lemma 6]). Let $\\delta\\in(0,1)$ with $d/\\delta\\geq e$ . Then with probability $1-\\delta\\pi^{2}/6$ , we have, for any $t\\geq0$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Vert\\bar{\\mathbf{E}}_{t}\\Vert\\leq8\\Psi\\Upsilon_{E}\\operatorname*{max}\\left\\{\\sqrt{\\log\\Bigl(\\frac{d(t+1)}{\\delta}\\Bigr)\\frac{w^{\\prime}(t)}{w(t)}},\\log\\Bigl(\\frac{d(t+1)}{\\delta}\\Bigr)\\frac{w^{\\prime}(t)}{w(t)}\\right\\}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "C.2 Convergence analysis ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Warm-up phase. Similar to the case of uniform averaging, we can only ensure that the distance to $\\mathbf{x}^{*}$ is monotonically non-increasing by Proposition 1 during this phase. Moreover, Algorithm 1 transitions to the linear phase when $\\|\\bar{\\mathbf{E}}_{t}\\|\\leq M_{1}$ . Specifically, the transition point $\\mathcal{U}_{1}$ is given by ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{U}_{1}=\\operatorname*{sup}_{t}\\bigg\\{t\\ge\\log_{\\frac{1}{\\beta}}\\frac{\\alpha\\beta}{3M_{1}\\sigma_{0}}:\\log\\Big(\\frac{d(t+1)}{\\delta}\\Big)\\frac{w^{\\prime}(t)}{w(t)}\\ge\\Big(1\\land\\frac{\\kappa}{8\\Upsilon}\\Big)^{2}\\bigg\\}+1.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "When $w(t)=(t+1)^{\\log(t+4)}$ , we have $w^{\\prime}(t)/w(t)=\\mathcal{O}\\left(\\log(t)/t\\right)$ . Thus, we conclude that $\\mathcal{U}_{1}=$ $\\tilde{\\mathcal{O}}(\\Upsilon^{2}/\\kappa^{2})$ . ", "page_idx": 21}, {"type": "text", "text": "Linear convergence phase. In the following lemma, we prove the linear convergence of Algorithm 1 with weighted averaging. ", "page_idx": 21}, {"type": "text", "text": "Lemma 14. Assume that $\\beta\\leq1/\\Psi^{2}$ and recall the definition of $\\mathcal{U}_{1}$ in (47). For any $t\\geq\\mathcal{U}_{1}$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|\\mathbf{x}_{t+1}-\\mathbf{x}^{*}\\|^{2}\\leq\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|^{2}\\left(1+\\frac{2\\alpha\\beta}{3\\kappa}\\right)^{-1},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\kappa\\triangleq M_{1}/\\mu$ is the condition number. ", "page_idx": 21}, {"type": "text", "text": "Proof. See Appendix C.3. ", "page_idx": 21}, {"type": "text", "text": "Superlinear convergence phase. Define ", "text_level": 1, "page_idx": 21}, {"type": "equation", "text": "$$\n{\\mathcal{I}}^{\\prime}\\!=\\!\\operatorname*{sup}_{t}\\left\\{t:\\log\\!\\Big(\\frac{d(t+1)}{\\delta}\\Big)\\frac{w^{\\prime}(t)}{w(t)}\\geq\\Big(1\\land\\frac{1}{8\\Upsilon}\\Big)^{2}\\right\\}+1.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Moreover, let $\\nu\\in(0,1)$ be a parameter and define ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{I}=\\operatorname*{max}\\left\\{\\mathcal{U}_{1}+2\\Big(1+\\frac{2\\kappa}{\\alpha\\beta}\\Big)\\log\\frac{L_{2}D}{\\nu\\mu},\\mathcal{I}_{2}^{\\prime}\\right\\}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Finally, let ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{U}_{2}=\\operatorname*{sup}_{t}\\left\\{t:w(t)\\leq w(\\mathcal{I})\\frac{\\kappa}{\\nu}\\right\\}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "When $w(t)=(t+1)^{\\log(t+4)}$ , we remark that $\\mathcal{I}^{\\prime}=\\tilde{\\mathcal{O}}(\\Upsilon^{2})$ and thus $\\mathcal{I}=\\tilde{\\mathcal{O}}(\\kappa+\\Upsilon^{2})$ . Moreover, similar to the derivation in [1], it can be shown that $\\mathcal{U}_{2}=\\mathcal{O}(\\mathcal{I})=\\tilde{\\mathcal{O}}(\\kappa+\\Upsilon^{2})$ . ", "page_idx": 22}, {"type": "text", "text": "Theorem 4. Let $\\nu\\in(0,1)$ be a parameter satisfying ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left(\\frac{1}{2\\alpha\\beta\\sqrt{(1-\\alpha^{2})\\beta}}+\\frac{5}{\\alpha\\sqrt{\\beta}}\\right)\\nu\\leq\\frac{1}{\\Psi},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and recall the definition of $\\mathcal{U}_{2}$ in (50). For any $t\\geq\\mathcal{U}_{2}$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|\\mathbf{x}_{t+1}-\\mathbf{x}^{*}\\|\\leq\\left(\\frac{1}{10\\beta\\sqrt{2(1-\\alpha^{2})}}+\\frac{1}{\\sqrt{2}}\\right)\\theta_{t}\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\theta_{t}=\\frac{8\\Psi\\Upsilon_{E}}{\\alpha\\sqrt{2\\beta}\\mu}\\sqrt{\\log\\biggl(\\frac{d(t+1)}{\\delta}\\biggr)\\frac{w^{\\prime}(t)}{w(t)}}+\\frac{5\\kappa w(\\mathcal{I})}{\\alpha\\sqrt{2\\beta}w(t)}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. See Appendix C.4. ", "page_idx": 22}, {"type": "text", "text": "C.3 Proof of Lemma 14 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "To simplify the notation, define the function ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\phi(t)=8\\Psi\\Upsilon_{E}\\operatorname*{max}\\left\\{\\sqrt{\\log\\Bigl(\\frac{d(t+1)}{\\delta}\\Bigr)\\frac{w^{\\prime}(t)}{w(t)}},\\log\\Bigl(\\frac{d(t+1)}{\\delta}\\Bigr)\\frac{w^{\\prime}(t)}{w(t)}\\right\\}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then we can rewrite (46) as $\\|\\bar{\\mathbf{E}}_{t}\\|\\leq\\phi(t)$ for all $t\\geq0$ . Similar to Lemma 7, we have the following result. ", "page_idx": 22}, {"type": "text", "text": "Lemma 15. If $t\\in\\mathcal{B}$ , then we have $\\eta_{t}\\geq\\alpha\\beta/(2M_{1}+\\|\\bar{\\mathbf{E}}_{t}\\|)\\geq\\alpha\\beta/(2M_{1}+\\phi(t)).$ ", "page_idx": 22}, {"type": "text", "text": "Lemma 16. Assume that $\\beta\\leq1/\\Psi$ . For any $t\\geq0$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\eta_{t}\\geq\\operatorname*{min}\\left\\{\\frac{\\alpha\\beta}{2M_{1}+\\phi(t)},\\frac{\\sigma_{0}}{\\beta^{t}}\\right\\}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. We prove this lemma by induction. For $t=0$ , we distinguish two subcases. If $0\\in\\mathcal{B}$ , then by Lemma 7 we obtain that \u03b70 \u22652M1\u03b1+\u03b2\u03d5(0). Otherwise, if $0\\notin\\mathcal{B}$ , we have $\\eta_{0}=\\sigma_{0}$ . In both cases, we observe that (53) is satisfied for the base case $t=0$ . ", "page_idx": 22}, {"type": "text", "text": "Now assume that (53) is satisfied for $t=s$ . For $t=s+1$ , we again distinguish two subcases. If $s+1\\in B$ , then by Lemma 15 we obtain that $\\begin{array}{r}{\\eta_{s+1}\\geq\\frac{\\alpha\\beta}{2M_{1}+\\phi(s+1)}}\\end{array}$ , which implies that (53) is satisfied. Otherwise, if $s+1\\notin B$ , then we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\eta_{s+1}=\\sigma_{s+1}=\\frac{\\eta_{s}}{\\beta}\\geq\\frac{1}{\\beta}\\operatorname*{min}\\left\\{\\frac{\\alpha\\beta}{M_{1}+\\phi(s)},\\frac{\\sigma_{0}}{\\beta^{s}}\\right\\}=\\operatorname*{min}\\left\\{\\frac{\\alpha}{M_{1}+\\phi(s)},\\frac{\\sigma_{0}}{\\beta^{s+1}}\\right\\},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where we used the induction hypothesis in the last inequality. Furthermore, note that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{\\phi(s)}{\\phi(s+1)}\\leq\\frac{w^{\\prime}(s)w(s+1)}{w^{\\prime}(s+1)w(s)}\\leq\\Psi\\leq\\frac{1}{\\beta},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and hence ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{\\alpha}{M_{1}+\\phi(s)}\\geq\\frac{\\alpha\\beta}{\\beta M_{1}+\\phi(s+1)}\\geq\\frac{\\alpha\\beta}{M_{1}+\\phi(s+1)}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, (54) implies that $\\begin{array}{r}{\\eta_{s+1}\\geq\\operatorname*{min}\\left\\{\\frac{\\alpha\\beta}{M_{1}+\\phi(s+1)},\\frac{\\sigma_{0}}{\\beta^{s+1}}\\right\\}}\\end{array}$ and thus (53) also holds in this case. This completes the induction and we conclude that (53) holds for all $t\\geq0$ . \u53e3 ", "page_idx": 22}, {"type": "text", "text": "Corollary 3. Recall the definition of $\\mathcal{U}_{1}$ in (47). For any $t\\geq\\mathcal{U}_{1}$ , we have $\\eta_{t}\\geq\\alpha\\beta/(3M_{1})$ . ", "page_idx": 23}, {"type": "text", "text": "Proof. By definition, we have U1 \u2265log \u03b21 3M\u03b11\u03b2\u03c30 , and thus $\\begin{array}{r}{\\frac{\\sigma_{0}}{\\beta^{\\mathcal{U}_{1}}}\\,\\geq\\,\\frac{\\alpha\\beta}{3M_{1}}}\\end{array}$ . Moreover, we also have \u03d5(t) \u2264M1. Hence, by Lemma 8 we conclude that \u03b7t \u22653\u03b1M\u03b21 when $t\\geq\\mathcal{U}_{1}$ . \u53e3 Now we are ready to prove Lemma 14. ", "page_idx": 23}, {"type": "text", "text": "Proof of Lemma 14. It follows from Proposition 1 and Corollary 3. ", "page_idx": 23}, {"type": "text", "text": "C.4 Proof of Theorem 4 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Lemma 17. We have $\\|\\bar{\\mathbf{E}}_{t}\\|\\leq\\nu\\mu$ and $\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|\\leq\\nu\\mu/L_{2}$ for all $t\\geq\\mathcal{I}.$ . ", "page_idx": 23}, {"type": "text", "text": "Proof. This follows from Lemmas 13 and 14. ", "page_idx": 23}, {"type": "text", "text": "Lemma 18. If $t\\in{\\boldsymbol{B}}$ and $t\\ge\\mathcal{I}$ , then ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{1}{\\sqrt{\\mu\\eta_{t}}}\\leq\\frac{L_{2}\\lVert\\mathbf{x}_{t}-\\mathbf{x}^{*}\\rVert}{2\\alpha\\beta\\sqrt{2(1-\\alpha^{2})\\beta}\\mu}+\\frac{\\lVert\\bar{\\mathbf{E}}_{t}\\rVert}{\\alpha\\sqrt{2\\beta}\\mu}+\\frac{\\kappa w(\\mathcal{I}-1)}{\\alpha\\sqrt{2\\beta}w(t)}+\\frac{2\\nu}{\\alpha\\sqrt{2\\beta}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and also ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{1}{\\sqrt{\\mu\\eta_{t}}}\\leq\\frac{\\nu}{2\\alpha\\beta\\sqrt{2(1-\\alpha^{2})\\beta}}+\\frac{3\\nu}{\\alpha\\sqrt{2\\beta}}+\\frac{\\kappa w(\\mathcal{I}-1)}{\\alpha\\sqrt{2\\beta}w(t)}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. Similar to the proof in Lemma 10, note that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{1}{\\sqrt{\\mu\\eta_{t}}}\\leq\\frac{L_{2}\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|}{2\\alpha\\beta\\sqrt{2(1-\\alpha^{2})\\beta}\\mu}+\\frac{\\|\\mathbf{H}_{t}-\\bar{\\mathbf{H}}_{t}\\|}{\\alpha\\sqrt{2\\beta}\\mu}+\\frac{\\|\\bar{\\mathbf{E}}_{t}\\|}{\\alpha\\sqrt{2\\beta}\\mu}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For the second term, note that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\bar{\\mathbf{H}}_{t}=\\sum_{i=0}^{t}z_{i,t}\\mathbf{H}_{i},\\quad\\mathrm{where~}z_{i,t}=\\frac{w(i)-w(i-1)}{w(t)}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Hence, by Jensen\u2019s inequality, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|\\mathbf{H}_{t}-\\bar{\\mathbf{H}}_{t}\\|\\leq\\sum_{i=0}^{t}z_{i,t}\\|\\mathbf{H}_{t}-\\mathbf{H}_{i}\\|=\\sum_{i=0}^{\\mathcal{I}-1}z_{i,t}\\|\\mathbf{H}_{t}-\\mathbf{H}_{i}\\|+\\sum_{i=\\mathcal{I}}^{t}z_{i,t}\\|\\mathbf{H}_{t}-\\mathbf{H}_{i}\\|.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "When $0\\leq i\\leq\\mathcal{I}-1$ , we use Assumption 2 to bound $\\|\\mathbf{H}_{t}-\\mathbf{H}_{i}\\|\\leq M_{1}$ and thus ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{i=0}^{\\mathcal{I}-1}z_{i,t}\\Vert\\mathbf{H}_{t}-\\mathbf{H}_{i}\\Vert\\leq M_{1}\\sum_{i=0}^{\\mathcal{I}-1}\\frac{w(i)-w(i-1)}{w(t)}=M_{1}\\frac{w(\\mathcal{I}-1)}{w(t)}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Moreover, for $\\mathcal{I}\\leq i\\leq t$ , we use Assumption 3 to get ", "page_idx": 23}, {"type": "text", "text": "Thus, $\\begin{array}{r l}&{\\|\\mathbf{H}_{t}-\\mathbf{H}_{i}\\|\\leq L_{2}\\|\\mathbf{x}_{t}-\\mathbf{x}_{i}\\|\\leq L_{2}\\left(\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|+\\|\\mathbf{x}_{i}-\\mathbf{x}^{*}\\|\\right)\\leq2L_{2}\\|\\mathbf{x}_{i}-\\mathbf{x}^{*}\\|\\leq2\\nu\\mu.}\\\\ &{\\sum_{i=\\mathcal{I}}^{t}z_{i,t}\\|\\mathbf{H}_{t}-\\mathbf{H}_{i}\\|\\leq2\\nu\\mu\\sum_{i=\\mathcal{I}}^{t}z_{i,t}\\leq2\\nu\\mu.}\\end{array}$ Combining the above inequalities, we arrive at ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|\\mathbf{H}_{t}-\\bar{\\mathbf{H}}_{t}\\|\\leq M_{1}\\frac{w(\\mathcal{J}-1)}{w(t)}+2\\nu\\mu.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This leads to the first result in (55). To show (56), we note that $\\|\\bar{\\mathbf{E}}_{t}\\|\\leq\\phi(t)$ and $\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|\\leq\\nu\\mu/L_{2}$ for all $t\\ge\\mathcal{I}$ . \u5382 ", "page_idx": 23}, {"type": "text", "text": "Lemma 19. Assume that $\\beta\\leq1/\\Psi^{2}$ . For any $t\\ge\\mathcal{I}$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{1}{\\sqrt{\\mu\\eta_{t}}}\\leq\\frac{\\nu}{2\\alpha\\beta\\sqrt{2(1-\\alpha^{2})\\beta}}+\\frac{3\\nu}{\\alpha\\sqrt{2\\beta}}+\\frac{\\sqrt{2}\\kappa w(\\mathcal{I})}{\\alpha\\sqrt{\\beta}w(t)}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. We shall use induction to prove Lemma 19. For $t=\\mathcal{T}$ , note that by Corollary 3, we have $\\eta_{\\mathcal{I}}\\dot{\\geq}\\alpha\\beta/(3M_{1})$ . Thus, this implies that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{1}{\\sqrt{\\mu\\eta_{\\mathcal{I}}}}\\leq\\frac{\\sqrt{2\\kappa}}{\\sqrt{\\alpha\\beta}}\\leq\\frac{\\sqrt{2}\\kappa}{\\alpha\\sqrt{\\beta}},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where we used $\\kappa\\geq1$ , $\\alpha,\\beta<1$ and $\\mathcal T\\geq2$ . This proves the base case where $t=\\mathcal{I}$ . ", "page_idx": 24}, {"type": "text", "text": "Now assume that (57) holds for $t=s$ , where $s\\geq\\mathcal{T}$ . For $t=s+1$ , we distinguish two cases. If $s+1\\in B$ , then by Lemma 18 we obtain that (57) is satisfied for $t=s+1$ . Otherwise, if $s+1\\notin B$ , then we have $\\eta_{s+1}=\\sigma_{s+1}=\\eta_{s}/\\beta$ . Hence, by using the induction hypothesis, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{1}{\\sqrt{\\mu\\eta_{s+1}}}=\\frac{\\sqrt{\\beta}}{\\sqrt{\\mu\\eta_{s}}}\\leq\\frac{\\nu}{2\\alpha\\beta\\sqrt{2(1-\\alpha^{2})\\beta}}+\\frac{3\\nu}{\\alpha\\sqrt{2\\beta}}+\\frac{\\sqrt{2\\beta}\\kappa w(\\mathcal{I})}{\\alpha\\sqrt{\\beta}w(s)}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Since $\\beta\\leq1/\\Psi^{2}$ , we have $w(s+1)/w(s)\\leq\\Psi\\leq1/\\sqrt{\\beta}$ . Thus, we further have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{1}{\\sqrt{\\mu\\eta_{s+1}}}\\leq\\frac{\\nu}{2\\alpha\\beta\\sqrt{2(1-\\alpha^{2})\\beta}}+\\frac{3\\nu}{\\alpha\\sqrt{2\\beta}}+\\frac{\\sqrt{2}\\kappa w(\\mathcal{I})}{\\alpha\\sqrt{\\beta}w(s+1)}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "This shows that (57) also holds in this case. ", "page_idx": 24}, {"type": "text", "text": "Recall that $\\boldsymbol{w}(\\mathcal{U}_{2})=\\boldsymbol{w}(\\mathcal{I})\\frac{\\kappa}{\\nu}$ . Then by Lemma 19, for $t\\geq\\mathcal{U}_{2}$ we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{\\sqrt{\\mu\\eta_{t}}}\\le\\frac{\\nu}{2\\alpha\\beta\\sqrt{2(1-\\alpha^{2})\\beta}}+\\frac{3\\nu}{\\alpha\\sqrt{2\\beta}}+\\frac{\\sqrt{2}\\kappa w(\\mathcal{I})}{\\alpha\\sqrt{\\beta}w(\\mathcal{U}_{2})}\\le\\frac{\\nu}{2\\alpha\\beta\\sqrt{2(1-\\alpha^{2})\\beta}}+\\frac{3\\nu}{\\alpha\\sqrt{2\\beta}}+\\frac{\\sqrt{2}\\nu}{\\alpha\\sqrt{\\beta}}}\\\\ {=\\frac{\\nu}{2\\alpha\\beta\\sqrt{2(1-\\alpha^{2})\\beta}}+\\frac{5\\nu}{\\alpha\\sqrt{2\\beta}}.\\qquad\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We will choose $\\nu$ such that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left(\\frac{1}{2\\alpha\\beta\\sqrt{(1-\\alpha^{2})\\beta}}+\\frac{5}{\\alpha\\sqrt{\\beta}}\\right)\\nu\\leq\\frac{1}{\\Psi}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, this further implies that $\\|\\mathbf{x}_{t+1}-\\mathbf{x}^{*}\\|\\leq\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|/(2\\Psi)$ for $t\\geq\\mathcal{U}_{2}$ . ", "page_idx": 24}, {"type": "text", "text": "Lemma 20. If $t\\in{\\boldsymbol{B}}$ and $t\\geq\\mathcal{U}_{2}$ , then ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{1}{\\sqrt{\\mu\\eta_{t}}}\\leq\\frac{L_{2}\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|}{2\\alpha\\beta\\sqrt{2(1-\\alpha^{2})\\beta}\\mu}+\\frac{\\phi(t)}{\\alpha\\sqrt{2\\beta}\\mu}+\\frac{5\\kappa w(\\mathcal{I})}{\\alpha\\sqrt{2\\beta}w(t)}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. Recall that we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{1}{\\sqrt{\\mu\\eta_{t}}}\\leq\\frac{L_{2}\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|}{2\\alpha\\beta\\sqrt{2(1-\\alpha^{2})\\beta}\\mu}+\\frac{\\|\\mathbf{H}_{t}-\\bar{\\mathbf{H}}_{t}\\|}{\\alpha\\sqrt{2\\beta}\\mu}+\\frac{\\|\\bar{\\mathbf{E}}_{t}\\|}{\\alpha\\sqrt{2\\beta}\\mu}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For the second term, we can write ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbf{H}_{t}-\\bar{\\mathbf{H}}_{t}\\vert\\vert\\leq\\sum_{i=0}^{t}z_{i,t}\\vert\\vert\\mathbf{H}_{t}-\\mathbf{H}_{i}\\vert\\vert=\\sum_{i=0}^{\\mathcal{I}-1}z_{i,t}\\vert\\vert\\mathbf{H}_{t}-\\mathbf{H}_{i}\\vert\\vert+\\sum_{i=\\mathcal{I}}^{\\mathcal{I}_{2}}z_{i,t}\\vert\\vert\\mathbf{H}_{t}-\\mathbf{H}_{i}\\vert\\vert+\\sum_{i=\\mathcal{U}_{2}+1}^{t}z_{i,t}\\vert\\vert\\mathbf{H}_{t}-\\mathbf{H}_{i}\\vert\\vert.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For the first part, $\\begin{array}{r}{\\sum_{i=0}^{\\mathcal{I}-1}z_{i,t}\\|\\mathbf{H}_{t}-\\mathbf{H}_{i}\\|\\leq M_{1}\\frac{w\\left(\\mathcal{I}-1\\right)}{w\\left(t\\right)}\\leq M_{1}\\frac{w\\left(\\mathcal{I}\\right)}{w\\left(t\\right)}}\\end{array}$ . For the second part, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{i=\\mathcal{I}}^{\\mathcal{U}_{2}}z_{i,t}\\|\\mathbf{H}_{t}-\\mathbf{H}_{i}\\|\\leq2\\nu\\mu\\sum_{i=\\mathcal{I}}^{\\mathcal{U}_{2}}z_{i,t}\\leq2\\nu\\mu\\frac{w(\\mathcal{U}_{2})}{w(t)}=2M_{1}\\frac{w(\\mathcal{I})}{w(t)},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where we used the fact that $\\boldsymbol{w}(\\mathcal{U}_{2})=\\boldsymbol{w}(\\mathcal{I})\\frac{\\boldsymbol{\\kappa}}{\\nu}$ . For the third part, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{i=k_{2}+1}^{k_{1}}\\mathbb{E}_{\\mathbf{t}}[\\mathbf{H}_{i}-\\mathbf{H}_{i}]\\leq\\sum_{i=k_{1}+1}^{k_{2}}2L_{\\alpha}\\frac{\\mathbf{w}(i+j)-\\mathbf{w}(i-1)}{w(i)}\\|\\mathbf{x}_{i}-\\mathbf{x}^{*}\\|}\\\\ &{\\leq\\displaystyle\\sum_{j=1}^{k_{2}}2L_{\\alpha}\\frac{\\mathbf{w}(i,j)-\\mathbf{w}(j,i)+j-\\mathbf{w}(j,i)+j-1}{w(i)}\\|\\mathbf{x}_{i}-\\mathbf{x}^{*}\\|}\\\\ &{\\le\\displaystyle\\sum_{j=1}^{k_{2}}2L_{\\alpha}\\frac{\\mathbf{w}(i,j)}{w(i)}\\|\\mathbf{x}_{i}-\\mathbf{x}^{*}\\|}\\\\ &{\\leq\\displaystyle\\sum_{j=1}^{k_{2}}2L_{\\alpha}\\frac{\\mathbf{w}(j,i)+j}{w(i)}\\|\\mathbf{x}_{i}-\\mathbf{x}^{*}\\|}\\\\ &{\\leq\\displaystyle\\sum_{j=1}^{k_{2}}2L_{\\alpha}\\frac{\\mathbf{w}(i,j)}{w(i)}\\frac{\\mathbf{w}(j,i)}{w(j,i)+j}}\\\\ &{\\le2\\nu\\frac{w(i,j)}{w(i)}\\frac{\\mathbf{w}^{*}}{w}\\frac{w(j,i)+j}{w(i,j)+j}}\\\\ &{\\le2\\nu\\frac{w(i,j)}{w(i)}\\frac{\\mathbf{w}^{*}}{w}\\frac{w(j,i)+j}{w(i,j)+j}}\\\\ &{\\le2\\nu\\frac{w(i,j)+j}{w(i)+j}\\frac{1}{w(i)+j}}\\\\ &{\\le2\\nu\\frac{w(i,j)}{w(i)+j}\\frac{2M_{\\alpha}(i,j)}{w(i)+j}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Therefore, we conclude that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|\\mathbf{H}_{t}-\\bar{\\mathbf{H}}_{t}\\|\\leq\\frac{3M_{1}w(\\mathcal{I})}{w(t)}+\\frac{2M_{1}w(\\mathcal{I})}{w(t)}=\\frac{5M_{1}w(\\mathcal{I})}{w(t)}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This completes the proof. ", "page_idx": 25}, {"type": "text", "text": "Lemma 21. Recall the definition of $\\theta_{t}$ in (52). For any $t\\geq0$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{5L_{2}\\|\\mathbf{x}_{\\mathcal{U}_{2}+t}-\\mathbf{x}^{*}\\|}{\\alpha\\sqrt{2\\beta}\\mu}\\leq\\theta_{t}\\quad a n d\\quad\\frac{1}{\\sqrt{\\mu\\eta_{\\mathcal{U}_{2}+t}}}\\leq\\left(\\frac{1}{10\\beta\\sqrt{1-\\alpha^{2}}}+1\\right)\\theta_{t},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. Note that by Proposition 1, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|\\mathbf{x}_{\\mathcal{U}_{2}+t+1}-\\mathbf{x}^{*}\\|\\leq\\|\\mathbf{x}_{\\mathcal{U}_{2}+t}-\\mathbf{x}^{*}\\|(1+2\\eta_{\\mathcal{U}_{2}+t}\\mu)^{-1/2}\\leq\\frac{\\|\\mathbf{x}_{\\mathcal{U}_{2}+t}-\\mathbf{x}^{*}\\|}{\\sqrt{2\\eta_{\\mathcal{U}_{2}+t}\\mu}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By Lemma 18, if $\\mathcal{U}_{2}+t\\in B$ , then ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{1}{\\sqrt{\\mu\\eta_{\\mathcal{U}_{2}+t}}}\\le\\frac{L_{2}\\|\\mathbf{x}_{\\mathcal{U}_{2}+t}-\\mathbf{x}^{*}\\|}{2\\alpha\\beta\\sqrt{2(1-\\alpha^{2})\\beta}\\mu}+\\theta_{t}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We will prove (60) by induction. First consider the base case t = 0. We note that \u03b80 \u2265\u03b1\u221a5\u03ba2\u03b2w(wJ( U)2) \u03b15\u221a\u03bd2\u03b2 . On the other hand, since \u2225xU2 \u2212x\u2217\u2225\u2264\u03bd\u00b5/L2, we obtain tha t5L2\u03b1\u2225x\u221aU22\u03b2\u2212\u00b5x\u2217\u2225\u2264\u03b15\u221a\u03bd2\u03b2 \u2264\u03b80. Moreover, by Lemma 19, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{\\sqrt{\\mu\\eta_{\\mathcal{U}_{2}}}}\\le\\frac{\\nu}{2\\alpha\\beta\\sqrt{2(1-\\alpha^{2})\\beta}}+\\frac{3\\nu}{\\alpha\\sqrt{2\\beta}}+\\frac{\\sqrt{2}\\kappa w(\\mathcal{I})}{\\alpha\\sqrt{\\beta}w(\\mathcal{U}_{2})}\\le\\frac{\\nu}{2\\alpha\\beta\\sqrt{2(1-\\alpha^{2})\\beta}}+\\frac{5\\nu}{\\alpha\\sqrt{2\\beta}}}\\\\ {\\le\\left(\\frac{1}{10\\beta\\sqrt{1-\\alpha^{2}}}+1\\right)\\theta_{0}.\\qquad\\qquad\\qquad\\qquad\\qquad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This shows that (60) holds for $t=0$ . ", "page_idx": 25}, {"type": "text", "text": "Now assume that (60) holds for $t=s\\geq0$ . For $t=s+1$ , by using the induction hypothesis, we obtain ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{5L_{2}\\|\\mathbf{x}_{\\mathcal{U}_{2}+s+1}-\\mathbf{x}^{*}\\|}{\\alpha\\sqrt{2\\beta}\\mu}\\leq\\frac{5L_{2}\\|\\mathbf{x}_{\\mathcal{U}_{2}+s}-\\mathbf{x}^{*}\\|}{\\alpha\\sqrt{2\\beta}\\mu\\sqrt{2\\eta_{\\mathcal{U}_{2}+s}\\mu}}\\leq\\frac{1}{\\sqrt{2}}\\left(\\frac{1}{10\\beta\\sqrt{1-\\alpha^{2}}}+1\\right)\\theta_{s}^{2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Note that $\\theta_{s}/\\Psi\\,\\leq\\,\\theta_{s+1}$ . Thus, it suffices to show that $\\begin{array}{r}{\\frac{1}{\\sqrt{2}}\\left(\\frac{1}{10\\beta\\sqrt{1-\\alpha^{2}}}+1\\right)\\theta_{s}^{2}\\,\\leq\\,\\theta_{s}/\\Psi}\\end{array}$ , which is equivalent to \u221a\u03a82 10\u03b2\u221a11\u2212\u03b12 + 1 \u03b8s \u2264 1. Furthermore, note that \u03b8s is non-increasing and $\\begin{array}{r}{\\theta_{0}\\leq\\frac{\\nu}{\\alpha\\sqrt{2\\beta}}+\\frac{5\\nu}{\\alpha\\sqrt{2\\beta}}=\\frac{6\\nu}{\\alpha\\sqrt{2\\beta}}}\\end{array}$ . Thus, we only need to require ", "page_idx": 26}, {"type": "equation", "text": "$$\n{\\frac{\\Psi}{\\sqrt{2}}}\\left({\\frac{1}{10\\beta{\\sqrt{1-\\alpha^{2}}}}}+1\\right){\\frac{6\\nu}{\\alpha{\\sqrt{2\\beta}}}}\\leq1\\quad\\Leftrightarrow\\quad\\left({\\frac{3}{10\\alpha\\beta{\\sqrt{(1-\\alpha^{2})\\beta}}}}+{\\frac{3}{\\alpha{\\sqrt{\\beta}}}}\\right)\\nu\\leq{\\frac{1}{\\Psi}},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "which is satisfied due to (51). This proves the first inequality in (60). ", "page_idx": 26}, {"type": "text", "text": "To prove the second inequality in (60) for $t=s+1$ , we distinguish two cases. If $s+1\\in B$ , then by Lemma 20, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{\\sqrt{\\mu\\eta_{l2}+s+1}}\\leq\\frac{L_{2}\\|\\mathbf{x}_{\\mathcal{U}_{2}+s+1}-\\mathbf{x}^{*}\\|}{2\\alpha\\beta\\sqrt{2(1-\\alpha^{2})\\beta}\\mu}+\\frac{\\|\\bar{\\mathbf{E}}_{\\mathcal{U}_{2}+s+1}\\|}{\\alpha\\sqrt{2\\beta}\\mu}+\\frac{5\\kappa w(\\mathcal{I})}{\\alpha\\sqrt{2\\beta}w(\\mathcal{U}_{2}+s+1)}}\\\\ &{\\qquad\\qquad\\leq\\frac{1}{10\\beta\\sqrt{1-\\alpha^{2}}}\\theta_{s+1}+\\theta_{s+1}\\leq\\left(\\cfrac{1}{10\\beta\\sqrt{1-\\alpha^{2}}}+1\\right)\\theta_{s+1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Otherwise, if $s+1\\notin B$ , then we have $\\eta\\varkappa_{2+s+1}=\\eta\\varkappa_{2+s}/\\beta$ and hence ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\frac{1}{\\sqrt{\\mu\\eta_{M_{2}+s+1}}}=\\frac{\\sqrt{\\beta}}{\\sqrt{\\mu\\eta_{M_{2}+s}}}\\leq\\left(\\frac{1}{10\\beta\\sqrt{1-\\alpha^{2}}}+1\\right)\\sqrt{\\beta}\\theta_{s}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Since $\\theta_{s}/\\Psi\\leq\\theta_{s+1}$ and $\\sqrt{\\beta}\\leq1/\\Psi$ , this implies that $\\begin{array}{r}{\\frac{1}{\\sqrt{\\mu\\eta\\tau_{2}+s+1}}\\,\\leq\\,\\left(\\frac{1}{10\\beta\\sqrt{1-\\alpha^{2}}}+1\\right)\\theta_{s+1}}\\end{array}$ . This completes the induction. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "Proof of Theorem 4. By Proposition 1, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\|\\mathbf{x}_{\\mathcal{T}_{2}+t+1}-\\mathbf{x}^{*}\\|\\leq\\|\\mathbf{x}_{\\mathcal{T}_{2}+t}-\\mathbf{x}^{*}\\|(1+2\\eta\\tau_{2}+t\\mu)^{-1/2}\\leq\\frac{\\|\\mathbf{x}_{\\mathcal{T}_{2}+t}-\\mathbf{x}^{*}\\|}{\\sqrt{2\\eta_{\\mathcal{T}_{2}+t}\\mu}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The rest follows from Lemma 21. ", "page_idx": 26}, {"type": "text", "text": "D Additional discussions ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "D.1 Iteration complexity of SNPE ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Note that for $t\\geq\\mathcal{U}_{3}=\\tilde{\\mathcal{O}}(\\Upsilon^{2}+\\kappa)$ , we have $\\begin{array}{r}{\\|\\mathbf{x}_{t+1}-\\mathbf{x}^{*}\\|=\\tilde{\\mathcal{O}}(\\frac{\\Upsilon}{\\sqrt{t}})\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|}\\end{array}$ by Theorem 2. By unrolling the inequality, this implies that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\|\\mathbf{x}_{t+1}-\\mathbf{x}^{*}\\|\\leq\\|\\mathbf{x}_{\\mathcal{U}_{3}}-\\mathbf{x}^{*}\\|\\tilde{\\mathcal{O}}\\left(\\prod_{s=\\mathcal{U}_{3}}^{t}\\frac{\\Upsilon}{\\sqrt{s}}\\right)\\leq\\|\\mathbf{x}_{0}-\\mathbf{x}^{*}\\|\\tilde{\\mathcal{O}}\\left(\\prod_{s=\\mathcal{U}_{3}}^{t}\\frac{\\Upsilon}{\\sqrt{s}}\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Further, for $t\\geq2\\mathcal{U}_{3}$ , we can upper bound $\\frac{\\Upsilon}{\\sqrt{s}}$ as follows: (i) $\\begin{array}{r}{\\frac{\\Upsilon}{\\sqrt{s}}\\leq\\frac{\\Upsilon}{\\sqrt{U_{3}}}\\leq1}\\end{array}$ for any $s\\in[\\mathcal{U}_{3},t/2]$ (ii) $\\begin{array}{r}{\\frac{\\Upsilon}{\\sqrt{s}}\\leq\\frac{\\Upsilon}{\\sqrt{t/2}}}\\end{array}$ for any $s\\in[t/2,t]$ . Thus, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\prod_{s=\\mathcal{U}_{3}}^{t}\\frac{\\Upsilon}{\\sqrt{s}}\\le\\prod_{s=t/2}^{t}\\frac{\\Upsilon}{\\sqrt{s}}\\le\\left(\\frac{\\Upsilon}{\\sqrt{t/2}}\\right)^{\\frac{t}{2}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "To derive a complexity bound, we upper bound the required number of iterations $t$ such that $\\left({\\frac{\\Upsilon}{\\sqrt{t/2}}}\\right)^{\\frac{t}{2}}\\,=\\,\\epsilon$ . Taking the logarithm of both sides and with some algebraic manipulation, we obtain ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\frac{t}{2\\Upsilon^{2}}\\log\\frac{t}{2\\Upsilon^{2}}=\\frac{2}{\\Upsilon^{2}}\\log\\frac{1}{\\epsilon}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "image", "img_path": "V4tzn87DtN/tmp/aae202b858a294d9c236ffa7e37a57a54958bf435ec5f9a81ae67bcd8857f51d.jpg", "img_caption": ["Figure 3: The effect of the extragradient step in stochastic NPE. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Using the Lambert W function1, the solution can be expressed as $\\begin{array}{r}{\\log\\frac{t}{2\\Upsilon^{2}}=W(\\frac{2}{\\Upsilon^{2}}\\log\\frac{1}{\\epsilon})\\Rightarrow t=}\\end{array}$ $2\\Upsilon^{2}e^{W(\\frac{2}{\\Upsilon^{2}}\\log\\frac{1}{\\epsilon})}$ . Finally, by applying the bound $\\begin{array}{r}{e^{W(x)}\\leq\\frac{2x+1}{1+\\log(x+1)}}\\end{array}$ for any $x\\geq0$ , we conclude that t = O log(\u03a5lo\u2212g2( l\u03f5o\u2212g1()\u03f5\u22121)) . Note that in the above derivation, we ignore the additional logarithmic factor $\\log(t)$ in our superlinear convergence rate. However, a more careful analysis will show that it does not affect the final complexity bound. We also refer the reader to a similar derivation in [22, Appendix D\u221a.2], where the authors provide the same complexity bound for a similar convergence rate of $(1+\\mathcal{O}(\\sqrt{t}))^{-t}$ . ", "page_idx": 27}, {"type": "text", "text": "D.2 The effect of the extragradient step ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In Figure 3, we test the effect of the extragradient step in our proposed SNPE method. We observe that in all cases, the variant without an extragradient step outperforms the original version, suggesting that the extragradient step may not be beneficial for minimization problems. Nevertheless, the SNPE method with the extragradient step, which is the one analyzed in our paper, still outperforms the stochastic Newton method in [1]. ", "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 28}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 28}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 28}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 28}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 28}, {"type": "text", "text": "Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: In the abstract and introduction of the paper, we claim that we can improve the complexity of the stochastic Newton method presented in [1], and this is exactly what we demonstrate. This improvement is achieved by introducing a novel stochastic variant of the Newton HPE framework, as promised in the abstract and introduction. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: In Section 8, we explicitly mentioned the limitation of our paper. Specifically, we noted that our theory assumes strong convexity. Extending the theory to the convex setting would make it more general. This extension is left for future work due to space limitations. We also highlighted in the introduction of our paper that we focus solely on the setting where the Hessian oracle is noisy, and the gradient can be queried without error. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: All the theorems, formulas, and proofs in the paper are numbered and crossreferenced. All assumptions for each presented result are clearly stated or referenced in the statements of the lemmas, propositions, or theorems. The proofs of all results are presented in the supplemental material. High-level ideas of the proofs are included in the main text whenever possible. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The experimental results and the implementation details are included in Section 7. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: The code and data are attached in the supplementary material. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. ", "page_idx": 30}, {"type": "text", "text": "\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). \u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We explained how we performed the experiments and included the implementation details in Section 7. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [No] ", "page_idx": 31}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The compute resources are reported in Section 7. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 31}, {"type": "text", "text": "\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 33}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}]