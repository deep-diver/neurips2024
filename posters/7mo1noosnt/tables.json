[{"figure_path": "7Mo1NOosNT/tables/tables_5_1.jpg", "caption": "Table 1: The table shows examples of causal query triplets created using the causal graphs (Ge) and observational graphs (Go) in Algorithm 1. The top row is taken from the COPA dataset [Gordon et al., 2012] for the purpose of comparison. Note the examples in the table show the samples taken from the instance version.", "description": "This table presents examples of causal query triplets generated using the COLD framework. Each triplet consists of a premise (an event), two choices (potential causes or effects), a question (identifying the cause or effect), and the correct answer.  The first row shows a comparison example from the COPA dataset, highlighting the similar structure used in COLD for generating causal triplets. The table also indicates that examples provided are from the 'instance version' of the dataset, suggesting that multiple text descriptions might exist for the same event.", "section": "3 COLD (Causal reasOning in cLosed Daily activities)"}, {"figure_path": "7Mo1NOosNT/tables/tables_5_2.jpg", "caption": "Table 2: The table provides details of the observational graph (Go) for 5 activities. The Causal Query Triplets represent the total number of triplets generated via Algorithm 1. The instance version shows the number of samples present in the instance version (including different text instances describing the same event) of the created dataset. Table 1 shows a small sample taken for 2 activities. Overall, the huge number of samples highlights the exhaustive nature of evaluation that can be done for LLMs.", "description": "This table presents a quantitative summary of the data generated using the COLD framework for five different daily activities.  It shows the number of nodes (events) in the observational graph for each activity, the number of compact trajectories (sequences of events) and the total number of trajectories possible from those compact trajectories.  It also shows the number of causal query triplets generated for each activity (this is used in the evaluation) and the number of samples in the instance version of the data. The instance version includes different textual descriptions of the same event, increasing the dataset size. The last row provides the dataset total.", "section": "3 COLD (Causal reasOning in cLosed Daily activities)"}, {"figure_path": "7Mo1NOosNT/tables/tables_6_1.jpg", "caption": "Table 3: The table provides evaluation results of language models over the created causal triplets.", "description": "This table presents the performance of various large language models (LLMs) on a causal reasoning task.  The models were evaluated on a dataset of causal triplets, which are sets of three events (premise, choice 1, choice 2) designed to test causal understanding. The table shows the accuracy of each model on five different activities: baking a cake, grocery shopping, riding a train, planting a tree, and riding a bus.  The results reveal the relative strengths and weaknesses of different LLMs in performing causal reasoning tasks.", "section": "Experiments and Results"}, {"figure_path": "7Mo1NOosNT/tables/tables_8_1.jpg", "caption": "Table 3: The table provides evaluation results of language models over the created causal triplets.", "description": "This table presents the performance of various language models on a causal reasoning task.  The models were evaluated using a dataset of causal triplets generated from the COLD framework.  The table shows the accuracy of each model across different daily activities (e.g., Baking a cake, Going grocery shopping).  The results highlight the challenges involved in causal reasoning, even for tasks that are trivial for humans. ", "section": "Experiments and Results"}, {"figure_path": "7Mo1NOosNT/tables/tables_23_1.jpg", "caption": "Table 5: Comparison of causal experimental settings used in prior LLM evaluation benchmarks. The real-world grounding plays a crucial role in evaluating LLMs, which is not present in the symbolic benchmarks.", "description": "This table compares various existing causal reasoning datasets and benchmarks used to evaluate Large Language Models (LLMs).  It highlights key differences in their design, specifically focusing on whether they incorporate real-world events, utilize causal graphs, employ symbolic representations, and whether they provide an exhaustive set of causal queries (meaning a very large number of queries).  The table aids in understanding the strengths and weaknesses of various methods used to assess an LLM's causal reasoning capabilities.  The last column shows the number of samples available in each dataset.", "section": "A.2 Comparison with previous Causal Reasoning Datasets/Benchmarks"}, {"figure_path": "7Mo1NOosNT/tables/tables_24_1.jpg", "caption": "Table 6: Human validation done for a small sample of 100 causal query triplets. Overall we find that humans do perform well in causal reasoning about these daily activities.", "description": "This table presents the results of a human validation study conducted to assess the performance of humans on the causal reasoning task using a subset of 100 causal query triplets (20 triplets per activity). The table shows the individual performance of five human subjects across five activities: baking a cake, going grocery shopping, going on a train, planting a tree, and riding on a bus. The average human performance across all activities was 92.20%, indicating a high level of accuracy in causal reasoning for these everyday activities.", "section": "Experiments and Results"}, {"figure_path": "7Mo1NOosNT/tables/tables_29_1.jpg", "caption": "Table 3: The table provides evaluation results of language models over the created causal triplets.", "description": "This table presents the performance of several large language models on a causal reasoning task.  The models were evaluated on a dataset of causal query triplets, which were created using the COLD framework. The table shows the accuracy of each model on five different activities: baking a cake, grocery shopping, riding on a train, planting a tree, and riding on a bus.  The results indicate the relative difficulty of these tasks for language models and highlight the strengths and weaknesses of different model architectures in performing causal reasoning.", "section": "Experiments and Results"}, {"figure_path": "7Mo1NOosNT/tables/tables_30_1.jpg", "caption": "Table 3: The table provides evaluation results of language models over the created causal triplets.", "description": "This table presents the performance of various language models on the causal reasoning task using the COLD dataset.  It shows the accuracy of each model in predicting the correct cause or effect in a causal query triplet for five different daily activities: Baking a Cake, Going Grocery Shopping, Going on a Train, Planting a Tree, and Riding on a Bus.  The results highlight the challenges posed by the causal reasoning task, even for activities that are trivial for humans.", "section": "Experiments and Results"}, {"figure_path": "7Mo1NOosNT/tables/tables_31_1.jpg", "caption": "Table 7: The table provides evaluation results of Language models over the causal and causal temporal triplets.", "description": "This table presents the performance of several large language models on two different sets of causal query triplets: causal triplets and causally hard triplets.  Causally hard triplets are designed to be more challenging, introducing temporally close but not causally related options. The table shows the accuracy of each model on each activity (cake, shopping, train, tree, bus) for both triplet types. This allows for a comparison of model performance under varying levels of difficulty in causal reasoning.", "section": "F Additional Results"}]