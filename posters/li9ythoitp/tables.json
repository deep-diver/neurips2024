[{"figure_path": "Li9YTHoItP/tables/tables_6_1.jpg", "caption": "Table 1: The performance of different auxiliary models with various strategies in discovering more ambiguous answers.", "description": "This table presents the performance of different methods for discovering ambiguous answers using various auxiliary language models. The methods compared include using simple prompts, a masking technique to reduce the probability of high-probability answers, and the proposed method. The evaluation metrics used are Exact Match (EM), F1 score, Answer Overlap Rate (AOR), and BLEU scores at different levels (1-4).  The results show that the proposed method outperforms the baselines in generating diverse and accurate ambiguous answers.", "section": "4.2 Overall Performance"}, {"figure_path": "Li9YTHoItP/tables/tables_7_1.jpg", "caption": "Table 2: Ablation study on the key components in our method. We use the same metrics as in Sec. 4.2, apart from those that require manual annotation. AOR and Bleu measure the entity- and word-level overlap respectively between answers from GPT-4 and different model variants. Auxiliary Model prompt GPT-4 with existing answers as examples for more ambiguous answers. Inverse Matrix keeps the near-duplicate tokens during generation. Besides, we adjust the probability influence scaler (\u03bb) to verify its impact on the generation results.", "description": "This table presents the results of an ablation study conducted to evaluate the impact of key components within the proposed method.  It compares different model variants against a baseline, measuring their effectiveness in generating distinctive ambiguous answers using metrics like AOR (Answer Overlap Rate) and BLEU scores. The variants tested include removing the auxiliary model, modifying the inverse matrix, and adjusting the probability influence scaler (\u03bb).", "section": "4.3 Ablation Study"}, {"figure_path": "Li9YTHoItP/tables/tables_7_2.jpg", "caption": "Table 3: Percentages of different categories of answers comparing the LLM self-evaluation and ground truth labels. Following the categorization in Sec. 3.5, we calculate that the percentage of unqualified answers is 40.15% by adding up the underlined results that are incorrect or unverifiable according to the ground truth. We also obtain the percentage of inaccurate evaluations as 28.47%, by adding up the results highlighted in red where self-evaluation is inconsistent with the ground truth.", "description": "This table presents the results of comparing the LLM's self-evaluation of ambiguous answers with the ground truth obtained through RAG-based evaluation.  It categorizes ambiguous answers into four types based on their self-evaluation and ground truth labels: Unqualified answers, Inaccurate evaluations, Hidden correct answers, and Unexpected wrong evaluations. The table quantifies the percentage of each answer type.", "section": "4.4 Results of Perceiving the Knowledge Boundary for GPT-4"}, {"figure_path": "Li9YTHoItP/tables/tables_8_1.jpg", "caption": "Table 4: Percentages of different categories of ambiguous answers comparing the GPT-4 self-evaluation results and their ground truth. Following the categorization in Sec.3.5, we calculate that the percentage of hidden correct answers is 75.12% by adding up the starred (*) results that are correct according to the ground truth. We also obtain the percentage of unexpected wrong evaluations as 62.43%, by adding up the results highlighted in orange where GPT-4-evaluation is inconsistent with the ground truth.", "description": "This table presents the results of comparing GPT-4's self-evaluation of ambiguous answers with the ground truth.  It categorizes ambiguous answers into four types: unqualified, inaccurate evaluations, hidden correct, and unexpected wrong evaluations. The table shows the percentages of each category based on whether GPT-4's self-evaluation matched the ground truth.", "section": "4.5 Results of the Auxiliary Model on Perceiving the Knowledge Boundary for GPT4"}, {"figure_path": "Li9YTHoItP/tables/tables_14_1.jpg", "caption": "Table 1: The performance of different auxiliary models with various strategies in discovering more ambiguous answers.", "description": "This table presents the results of an ablation study comparing different auxiliary models and strategies for discovering ambiguous answers.  The metrics used are Exact Match (EM), F1 score, Answer Overlap Rate (AOR), and BLEU scores (BLEU1-BLEU4).  The results show that the proposed method, which combines an open-sourced auxiliary model with probability reduction, outperforms other methods in discovering diverse and relevant ambiguous answers.", "section": "4.2 Overall Performance"}, {"figure_path": "Li9YTHoItP/tables/tables_16_1.jpg", "caption": "Table 1: The performance of different auxiliary models with various strategies in discovering more ambiguous answers.", "description": "This table presents the results of an experiment evaluating different methods for discovering ambiguous answers using various auxiliary models. The metrics used include Exact Match (EM), F1-score, Answer Overlap Rate (AOR), and BLEU scores (BLEU1-BLEU4).  The methods compared are using a prompt-based approach, a MASK-based approach, and the proposed method from the paper. The table shows that the proposed method achieves the best performance across all metrics, indicating its effectiveness in generating diverse and insightful ambiguous answers.", "section": "4.2 Overall Performance"}, {"figure_path": "Li9YTHoItP/tables/tables_16_2.jpg", "caption": "Table 2: Ablation study on the key components in our method. We use the same metrics as in Sec. 4.2, apart from those that require manual annotation. AOR and Bleu measure the entity- and word-level overlap respectively between answers from GPT-4 and different model variants. Auxiliary Model prompt GPT-4 with existing answers as examples for more ambiguous answers. Inverse Matrix keeps the near-duplicate tokens during generation. Besides, we adjust the probability influence scaler (\u03bb) to verify its impact on the generation results.", "description": "This table presents the results of an ablation study conducted to evaluate the impact of different components of the proposed method on the generation of distinctive ambiguous answers.  It compares the performance using different variations of the method: the full method, a version without the auxiliary model, one without the inverse matrix, and one with a modified probability influence scaler.  The metrics used (AOR and BLEU) quantify the overlap between answers generated by GPT-4 and the different model variations.", "section": "4.3 Ablation Study"}, {"figure_path": "Li9YTHoItP/tables/tables_17_1.jpg", "caption": "Table 1: The performance of different auxiliary models with various strategies in discovering more ambiguous answers.", "description": "This table presents the performance comparison of different methods used to discover ambiguous answers.  The methods include using prompt engineering, a masking technique to reduce the generation probability of high-probability answers, and the proposed method of the paper. The performance is measured across various metrics including Exact Match (EM), F1 score, Answer Overlap Rate (AOR), and BLEU scores (BLEU1-BLEU4).  Different auxiliary models (LLaMA-2-13B and LLaMA-2-7B) were also tested.  The results illustrate the effectiveness of the proposed method in generating diverse and informative ambiguous answers.", "section": "4.2 Overall Performance"}, {"figure_path": "Li9YTHoItP/tables/tables_17_2.jpg", "caption": "Table 1: The performance of different auxiliary models with various strategies in discovering more ambiguous answers.", "description": "This table presents the results of an experiment comparing different methods for discovering ambiguous answers using various auxiliary models. The metrics used for evaluation include Exact Match (EM), F1 score, Answer Overlap Rate (AOR), and BLEU scores (BLEU1-BLEU4).  The results show the effectiveness of the proposed method in generating diverse ambiguous answers compared to baseline approaches like prompt engineering and a masking technique.", "section": "4.2 Overall Performance"}, {"figure_path": "Li9YTHoItP/tables/tables_17_3.jpg", "caption": "Table 1: The performance of different auxiliary models with various strategies in discovering more ambiguous answers.", "description": "This table presents the performance of different auxiliary models (LLaMA-2-13B and LLaMA-2-7B) using various strategies (Prompt, MASK, and Ours) in discovering ambiguous answers.  The evaluation metrics include Exact Match (EM), F1 score, Answer Overlap Rate (AOR), and BLEU scores (BLEU1-BLEU4).  The results show that the proposed method ('Ours') outperforms the baselines in discovering more diverse and accurate ambiguous answers, indicating its effectiveness in exploring the knowledge boundary of the target LLM (GPT-4).", "section": "4.2 Overall Performance"}, {"figure_path": "Li9YTHoItP/tables/tables_18_1.jpg", "caption": "Table 1: The performance of different auxiliary models with various strategies in discovering more ambiguous answers.", "description": "This table presents the results of an experiment comparing different methods for discovering ambiguous answers using various auxiliary models.  The methods include using prompt engineering, a masking technique, and the authors' proposed method. The performance is evaluated across several metrics: Exact Match (EM), F1 score, Answer Overlap Rate (AOR), and BLEU scores (at different n-gram levels).  Higher EM and F1 scores indicate better performance, while a lower AOR and BLEU score is desirable.  The results show that the authors' proposed method outperforms the baseline methods in discovering more ambiguous answers, indicating its effectiveness in exploring the knowledge boundary of the target Large Language Model (LLM).", "section": "4.2 Overall Performance"}]