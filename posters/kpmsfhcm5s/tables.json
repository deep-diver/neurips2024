[{"figure_path": "kPmSfhCM5s/tables/tables_3_1.jpg", "caption": "Table 1: Comparisons of existing (partially, imperfect coverage) representative vision MLLM.", "description": "This table compares various vision LLMs based on their capabilities in understanding, generating, segmenting, and editing both images and videos.  It highlights the differences in pixel/regional understanding, grounding abilities, and support for cross-task synergy. The table helps to position VITRON within the existing landscape of vision LLMs by showing its more comprehensive capabilities compared to others.", "section": "Related Work"}, {"figure_path": "kPmSfhCM5s/tables/tables_6_1.jpg", "caption": "Table 2: Results (cIoU) of referring image segmentation. \u2018w/o syng.\u2019: without synergy learning.", "description": "This table presents the results of the referring image segmentation task, comparing VITRON's performance against several other methods on three benchmark datasets: RefCOCO, RefCOCO+, and RefCOCOg.  The cIoU (Intersection over Union) metric is used to evaluate the accuracy of the models in segmenting the target objects specified by referring expressions. The table also includes a comparison row showing VITRON's performance without the synergy learning module, highlighting its contribution to improved results.", "section": "5.1 Results on Vision Segmentation"}, {"figure_path": "kPmSfhCM5s/tables/tables_6_2.jpg", "caption": "Table 3: Results (mIoU) of video spatial grounding on two datasets.", "description": "This table compares the performance of VITRON against other state-of-the-art models on two video spatial grounding datasets: VidSTG [134] and HC-STVG [98].  The mIoU (mean Intersection over Union) metric is used to evaluate the accuracy of grounding.  The table highlights that VITRON achieves superior performance compared to other models, and also shows the impact of the synergy module by showing results with and without it (w/o syng.).", "section": "5.1 Results on Vision Segmentation"}, {"figure_path": "kPmSfhCM5s/tables/tables_6_3.jpg", "caption": "Table 4: Results of video object segmentation on DAVIS 17 [80] Test-Dev set.", "description": "This table presents the results of video object segmentation on the DAVIS 17 Test-Dev dataset.  It compares the performance of VITRON against several state-of-the-art methods, including  VidSTG [134], HC-STVG [98], RDE [51], XMem [14], DeAOT [122], ISVOS [102] and PG-Video-LLaVA [74]. The metrics used are J&F, J, and F.  The \"w/o syng.\" row shows the performance of VITRON without the cross-task synergy module, highlighting the impact of this module on performance.", "section": "5.1 Results on Vision Segmentation"}, {"figure_path": "kPmSfhCM5s/tables/tables_6_4.jpg", "caption": "Table 5: Performance of image regional captioning on RefCOCOg [68].", "description": "This table presents the results of image regional captioning on the RefCOCOg dataset.  It compares the performance of VITRON against several other models, including GRIT, Kosmos-2, NEXT-Chat, MiniGPT-v2, GLaMM, and Osprey. The metrics used for comparison are METEOR and CIREr. The results show that VITRON achieves the highest scores on both metrics, indicating its superior performance in accurately generating captions for image regions.", "section": "5.2 Results on Fine-grained Vision Understanding"}, {"figure_path": "kPmSfhCM5s/tables/tables_7_1.jpg", "caption": "Table 6: Results (accuracy) on image-based VQA.", "description": "This table presents the results of image-based Visual Question Answering (VQA) experiments, comparing the performance of various models.  The \"Ground?\" column indicates whether the model incorporates pixel-wise vision grounding.  The results are shown as accuracy scores on six different datasets: OKVQA [88], GQA [37], VSR [62], IconVQA [66], VizWiz [32], and HM [41]. VITRON, with its pixel-level grounding, achieves superior performance compared to other models.", "section": "5.2 Results on Fine-grained Vision Understanding"}, {"figure_path": "kPmSfhCM5s/tables/tables_7_2.jpg", "caption": "Table 7: Results (accuracy and confidence Score) on video QA.", "description": "This table compares the performance of various vision LLMs on the ActivityNet-QA dataset, a benchmark for video question answering.  The \"Ground?\" column indicates whether the model incorporates pixel-level grounding.  The table shows accuracy and confidence scores for each model. VITRON achieves the highest accuracy (51.0) and score (3.7), demonstrating superior performance, particularly when compared to models without grounding.", "section": "5.2 Results on Fine-grained Vision Understanding"}, {"figure_path": "kPmSfhCM5s/tables/tables_7_3.jpg", "caption": "Table 1: Comparisons of existing (partially, imperfect coverage) representative vision MLLM.", "description": "This table compares several existing vision-language models (MLLMs) based on their capabilities in understanding, generating, segmenting, and editing both images and videos.  It highlights the lack of unification in current models, showing that many only support either images or videos, and often lack a comprehensive range of functionalities.  The table serves to illustrate the need for a unified, pixel-level vision LLM like VITRON, which the paper introduces.", "section": "Related Work"}, {"figure_path": "kPmSfhCM5s/tables/tables_8_1.jpg", "caption": "Table 11: Image editing results on MagicBrush [129].", "description": "This table presents the quantitative results of image editing experiments conducted on the MagicBrush dataset.  It compares the performance of VITRON against several state-of-the-art image editing methods. The metrics used to evaluate the performance include CLIP similarity scores (CLIP<sub>dir</sub>, CLIP<sub>img</sub>, and CLIP<sub>out</sub>), and the L1 distance between the edited image and the target image (L1).  The results demonstrate VITRON's superior performance in image editing tasks.", "section": "5 Experiments"}, {"figure_path": "kPmSfhCM5s/tables/tables_18_1.jpg", "caption": "Table 1: Comparisons of existing (partially, imperfect coverage) representative vision MLLM.", "description": "This table compares several existing vision-language models (MLLMs) based on their capabilities.  It shows which models support image or video understanding, pixel-level or regional grounding, segmentation, generation, or editing tasks, and whether they include cross-task synergy.  The table highlights the lack of a unified model that supports all these tasks comprehensively.  VITRON, the model proposed in the paper, is shown as having support across all these capabilities.", "section": "2 Related Work"}, {"figure_path": "kPmSfhCM5s/tables/tables_19_1.jpg", "caption": "Table 1: Comparisons of existing (partially, imperfect coverage) representative vision MLLM.", "description": "This table compares several existing vision-language models (MLLMs) across various criteria, including their ability to process images and videos, perform pixel-level understanding, and handle various tasks such as segmentation, grounding, generation, and editing.  The table highlights the limitations of existing models in terms of unified support for both images and videos, insufficient coverage across different vision tasks, and the lack of pixel-level understanding. It sets the stage for introducing VITRON, which aims to address these limitations.", "section": "Related Work"}, {"figure_path": "kPmSfhCM5s/tables/tables_23_1.jpg", "caption": "Table 15: Results of video object segmentation.", "description": "This table compares the performance of VITRON against other state-of-the-art models on video object segmentation tasks using two benchmark datasets: DAVIS 17 and Youtube-VOS 2019.  The results are broken down by different metrics (J&F, J, F, Js, Fs, Ju, Fu), which likely represent variations in the evaluation criteria.", "section": "5.1 Results on Vision Segmentation"}, {"figure_path": "kPmSfhCM5s/tables/tables_23_2.jpg", "caption": "Table 2: Results (cIoU) of referring image segmentation. \u2018w/o syng.\u2019: without synergy learning.", "description": "This table presents the results of the referring image segmentation task, comparing the VITRON model's performance against several other models on three datasets: RefCOCO, RefCOCO+, and RefCOCOg.  The cIoU (Intersection over Union) metric is used to evaluate the performance.  A comparison is also made showing the effect of removing the synergy learning component of the VITRON model.", "section": "5.1 Results on Vision Segmentation"}, {"figure_path": "kPmSfhCM5s/tables/tables_23_3.jpg", "caption": "Table 6: Results (accuracy) on image-based VQA.", "description": "This table presents the accuracy scores of different vision language models on six image-based Visual Question Answering (VQA) datasets.  It compares models with and without pixel-wise vision grounding capabilities, demonstrating the impact of fine-grained grounding on VQA performance.  VITRON achieves the highest accuracy across all datasets.", "section": "5.2 Results on Fine-grained Vision Understanding"}, {"figure_path": "kPmSfhCM5s/tables/tables_23_4.jpg", "caption": "Table 1: Comparisons of existing (partially, imperfect coverage) representative vision MLLM.", "description": "This table compares various vision-language large models (MLLMs) based on their capabilities in understanding, generating, segmenting, and editing both images and videos. It highlights the limitations of existing models, such as the lack of unified support for both image and video modalities, insufficient coverage across various vision tasks, and insufficient pixel-level understanding. The table also shows that VITRON, proposed by the authors, is a pioneering model in achieving a unified and comprehensive vision MLLM framework.", "section": "Related Work"}]