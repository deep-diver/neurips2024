[{"figure_path": "6gMnj9oc6d/figures/figures_0_1.jpg", "caption": "Figure 3: AUC (left) and bounds on \u03c3\u03b2 values (middle) for \u03b5 = 5, \u03b4 = 2.7 \u00b7 10\u22128 and using 1 epoch (top) and 5 epochs (bottom) of training on a linear-log scale; AUC (right) is with non-private training.", "description": "This figure compares the Area Under the Curve (AUC) and noise scale (\u03c3) values for different batch sampling methods in differentially private stochastic gradient descent (DP-SGD) training.  It shows results for both 1 and 5 epochs of training, across varying batch sizes. The left panel displays AUC, a measure of model performance. The middle panel shows the calculated noise scales (\u03c3) for Deterministic, Truncated Poisson, Persistent Shuffling and Dynamic Shuffling batch samplers, using both lower bounds and (where available) optimistic estimates based on privacy analyses in the paper.  The right panel shows AUC achieved with non-private training (for comparison). The linear-log scale emphasizes differences in behavior across different parameter settings.", "section": "4 Experiments"}, {"figure_path": "6gMnj9oc6d/figures/figures_3_1.jpg", "caption": "Figure 3: AUC (left) and bounds on \u03c3\u00df values (middle) for \u03b5 = 5, \u03b4 = 2.7 \u00b7 10\u22128 and using 1 epoch (top) and 5 epochs (bottom) of training on a linear-log scale; AUC (right) is with non-private training.", "description": "This figure compares the Area Under the Curve (AUC) and the noise scale (\u03c3) values for different batch sampling methods in differentially private stochastic gradient descent (DP-SGD).  The left panel shows AUC values for different batch sampling methods (Deterministic, Truncated Poisson, Persistent Shuffling, and Dynamic Shuffling) for 1 and 5 epochs of training. The middle panel displays the lower and upper bounds on the required noise scale (\u03c3) for each method to achieve a specific privacy guarantee (\u03b5 = 5, \u03b4 = 2.7 \u00b7 10\u22128).  The right panel presents the AUC values obtained with non-private training (no noise added) for comparison. The results show that Poisson subsampling and dynamic shuffling perform similarly for the same noise scale, but the upper bounds for shuffling are much higher in high-privacy regimes. ", "section": "Experiments"}, {"figure_path": "6gMnj9oc6d/figures/figures_7_1.jpg", "caption": "Figure 2: Visualization of the massively parallel computation approach for Poisson subsampling at scale. Consider 6 records x1,..., x6 sub-sampled into 4 batches with a maximum batch size of B = 2. The Map operation adds a \"weight\" parameter of 1 to all examples, and samples indices of batches to which each example will belong. The Reduce operation groups by the batch indices. The final Map operation truncates batches with more than B examples (e.g., batches 1 and 3 above), and pads dummy examples with weight 0 in batches with fewer than B examples (e.g., batch 4 above).", "description": "This figure illustrates the massively parallel computation approach used for implementing truncated Poisson subsampling.  The process is broken down into three MapReduce steps. The first Map step adds a weight (1) to each example and samples batch indices for each example. The Reduce step groups examples by their assigned batch index.  The final Map step truncates batches exceeding the maximum size (B) by randomly selecting a subset, and pads batches smaller than B with dummy examples having a weight of 0. This approach enables efficient handling of truncated Poisson subsampling at scale.", "section": "4 Experiments"}, {"figure_path": "6gMnj9oc6d/figures/figures_8_1.jpg", "caption": "Figure 3: AUC (left) and bounds on \u03c3\u00df values (middle) for \u03b5 = 5, \u03b4 = 2.7 \u00b7 10\u207b\u2078 and using 1 epoch (top) and 5 epochs (bottom) of training on a linear-log scale; AUC (right) is with non-private training.", "description": "This figure compares the performance of different batch sampling methods for differentially private stochastic gradient descent (DP-SGD) training.  The left panel shows the area under the curve (AUC) for each method.  The middle panel displays the noise scale (\u03c3) bounds for different privacy parameters. The right panel presents the AUC achieved without differential privacy for comparison.  Results are shown for both 1 and 5 epochs of training, illustrating the effect of the number of epochs on performance and privacy.", "section": "4 Experiments"}, {"figure_path": "6gMnj9oc6d/figures/figures_9_1.jpg", "caption": "Figure 3: AUC (left) and bounds on \u03c3\u00df values (middle) for \u03b5 = 5, \u03b4 = 2.7 \u00b7 10\u22128 and using 1 epoch (top) and 5 epochs (bottom) of training on a linear-log scale; AUC (right) is with non-private training.", "description": "This figure compares the performance of different batch sampling methods in differentially private stochastic gradient descent (DP-SGD).  The left panel shows the area under the curve (AUC) for different batch sampling methods, including truncated Poisson subsampling, persistent shuffling, dynamic shuffling, and deterministic batches. The middle panel displays the bounds on the noise scale (\u03c3) required to achieve the target privacy level (\u03b5, \u03b4) for each method. The right panel shows the AUC for non-private training for comparison.  The results are shown for both 1 and 5 epochs of training.", "section": "4 Experiments"}, {"figure_path": "6gMnj9oc6d/figures/figures_14_1.jpg", "caption": "Figure 3: AUC (left) and bounds on \u03c3\u03b2 values (middle) for \u03b5 = 5, \u03b4 = 2.7 \u00b7 10\u22128 and using 1 epoch (top) and 5 epochs (bottom) of training on a linear-log scale; AUC (right) is with non-private training.", "description": "This figure compares the Area Under the Curve (AUC) and noise scales (\u03c3) for different batch sampling methods in differentially private stochastic gradient descent (DP-SGD) with varying batch sizes and epochs.  The left panel shows AUC performance, the middle panel shows the required noise scale (\u03c3) for different methods, and the right panel shows AUC performance without differential privacy. The figure reveals that the truncated Poisson subsampling method yields better AUC performance and lower noise scales in the high privacy regime, compared to shuffled sampling approaches with even optimistic privacy accounting.", "section": "4 Experiments"}, {"figure_path": "6gMnj9oc6d/figures/figures_15_1.jpg", "caption": "Figure 6: \u03c3\u03b5 values with varying numbers of epochs, fixing \u03b5 = 5, \u03b4 = 2.7 \u00b7 10\u207b\u2078, and batch size 65536.", "description": "This figure shows how the noise scale (\u03c3) required to achieve a given level of privacy (\u03b5 = 5, \u03b4 = 2.7e-8) changes with different batch sampling methods (Poisson, Dynamic Shuffling, Persistent Shuffling, Deterministic) and varying numbers of training epochs.  It demonstrates that the required noise scale increases with the number of epochs for all methods, but the rate of increase differs among methods. It highlights that Poisson subsampling generally requires a lower noise scale compared to shuffling methods for achieving the same level of privacy, even when using optimistic estimates of privacy for shuffling methods.", "section": "4 Experiments"}]