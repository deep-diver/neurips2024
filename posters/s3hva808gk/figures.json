[{"figure_path": "S3HvA808gk/figures/figures_2_1.jpg", "caption": "Figure 1: a) Consider a model f yielding continuous output scores for a binary classification task applied to a dataset consisting of two distinct subpopulations, A \u2208 {0, 1}. If we order samples in ascending order of output score, each misordered pair of samples (e.g., mistake 1-4) represents an opportunity for model improvement. Theorem 2 shows that a model's AUROC will improve by the same amount no matter which mistake you fix, while the model's AUPRC will improve by an amount correlated with the score of the sample. b) When comparing models absent a specific deployment scenario, we have no reason to value improving one mistake over another, and model evaluation metrics should therefore improve equally regardless of which mistake is corrected. c) When false negatives have a high cost relative to false positives, evaluation metrics should favor mistakes that have lower scores, regardless of any class imbalance. d) When limited resources will be distributed among a population according to model score, in a manner that requires certain subpopulations to all be offered commensurate possible benefit from the intervention for ethical reasons, evaluation metrics should prioritize the importance of within-group, high-score mistakes such that the highest risk members of all subgroups receive interventions. e) When false positives are expensive relative to false negatives and there are no fairness concerns, evaluation metrics should favor model improvements in decreasing order with score.", "description": "This figure illustrates the relationship between AUROC and AUPRC when correcting model mistakes in binary classification tasks. It highlights how AUROC treats all mistakes equally, whereas AUPRC prioritizes high-score mistakes which can lead to fairness concerns in subpopulations. The figure uses diagrams and examples to illustrate different scenarios where AUROC or AUPRC may be more suitable metrics.", "section": "2 Theoretical Analyses"}, {"figure_path": "S3HvA808gk/figures/figures_5_1.jpg", "caption": "Figure 2: Synthetic experiment per-group AUROC, showing a confidence interval spanning the 5th to 95th percentile of results observed across all seeds, after successively either fixing individual mistakes, as defined in Definition 2.1, (a) and b)) or successively choosing the optimal score permutation (c) and d)) in order to optimize either AUROC (a) and c)) or AUPRC (b) and d)). It is clear across both forms of optimization that AUPRC definitively favors the higher prevalence subpopulation, whereas AUROC treats subgroups approximately equally. Similar patterns were observed when comparing per-group AUPRCs over the same experimental procedures, as shown in Appendix Figure 4.", "description": "This figure compares the performance of AUROC and AUPRC in a synthetic experiment with two subpopulations.  Two optimization methods are shown.  The results demonstrate that AUPRC optimization disproportionately benefits the higher prevalence subpopulation while AUROC treatment is fairer to both.", "section": "3.1 Synthetic optimization experiments demonstrate AUPRC-induced disparities"}, {"figure_path": "S3HvA808gk/figures/figures_5_2.jpg", "caption": "Figure 2: Synthetic experiment per-group AUROC, showing a confidence interval spanning the 5th to 95th percentile of results observed across all seeds, after successively either fixing individual mistakes, as defined in Definition 2.1, (a) and b)) or successively choosing the optimal score permutation (c) and d)) in order to optimize either AUROC (a) and c)) or AUPRC (b) and d)). It is clear across both forms of optimization that AUPRC definitively favors the higher prevalence subpopulation, whereas AUROC treats subgroups approximately equally. Similar patterns were observed when comparing per-group AUPRCs over the same experimental procedures, as shown in Appendix Figure 4.", "description": "The figure compares the impact of optimizing for AUROC vs. AUPRC in a synthetic experiment with two subpopulations.  Two optimization methods are shown: one that fixes individual mistakes and one that performs random permutations of scores. The results demonstrate that AUPRC disproportionately improves the high-prevalence group, while AUROC treats both groups equally.", "section": "3.1 Synthetic optimization experiments demonstrate AUPRC-induced disparities"}, {"figure_path": "S3HvA808gk/figures/figures_5_3.jpg", "caption": "Figure 2: Synthetic experiment per-group AUROC, showing a confidence interval spanning the 5th to 95th percentile of results observed across all seeds, after successively either fixing individual mistakes, as defined in Definition 2.1, (a) and b)) or successively choosing the optimal score permutation (c) and d)) in order to optimize either AUROC (a) and c)) or AUPRC (b) and d)). It is clear across both forms of optimization that AUPRC definitively favors the higher prevalence subpopulation, whereas AUROC treats subgroups approximately equally. Similar patterns were observed when comparing per-group AUPRCs over the same experimental procedures, as shown in Appendix Figure 4.", "description": "This figure compares the results of optimizing for AUROC vs. AUPRC using two different optimization methods on a synthetic dataset with two subpopulations, one high-prevalence and one low-prevalence. It demonstrates that AUPRC disproportionately favors the high-prevalence group, while AUROC treats both groups more equally.", "section": "3.1 Synthetic optimization experiments demonstrate AUPRC-induced disparities"}, {"figure_path": "S3HvA808gk/figures/figures_5_4.jpg", "caption": "Figure 2: Synthetic experiment per-group AUROC, showing a confidence interval spanning the 5th to 95th percentile of results observed across all seeds, after successively either fixing individual mistakes, as defined in Definition 2.1, (a) and b)) or successively choosing the optimal score permutation (c) and d)) in order to optimize either AUROC (a) and c)) or AUPRC (b) and d)). It is clear across both forms of optimization that AUPRC definitively favors the higher prevalence subpopulation, whereas AUROC treats subgroups approximately equally. Similar patterns were observed when comparing per-group AUPRCs over the same experimental procedures, as shown in Appendix Figure 4.", "description": "This figure shows the results of synthetic experiments designed to compare the effects of optimizing for AUROC vs. AUPRC on subpopulation disparity in binary classification tasks.  The results demonstrate that optimizing for AUPRC favors high-prevalence subpopulations, while optimizing for AUROC treats subpopulations more equally.  Two optimization procedures were tested, one fixing individual mistakes and another using random score permutations.", "section": "3 Experimental Validation"}, {"figure_path": "S3HvA808gk/figures/figures_6_1.jpg", "caption": "Figure 1: a) Consider a model f yielding continuous output scores for a binary classification task applied to a dataset consisting of two distinct subpopulations, A \u2208 {0, 1}. If we order samples in ascending order of output score, each misordered pair of samples (e.g., mistake 1-4) represents an opportunity for model improvement. Theorem 2 shows that a model\u2019s AUROC will improve by the same amount no matter which mistake you fix, while the model\u2019s AUPRC will improve by an amount correlated with the score of the sample. b) When comparing models absent a specific deployment scenario, we have no reason to value improving one mistake over another, and model evaluation metrics should therefore improve equally regardless of which mistake is corrected. c) When false negatives have a high cost relative to false positives, evaluation metrics should favor mistakes that have lower scores, regardless of any class imbalance. d) When limited resources will be distributed among a population according to model score, in a manner that requires certain subpopulations to all be offered commensurate possible benefit from the intervention for ethical reasons, evaluation metrics should prioritize the importance of within-group, high-score mistakes such that the highest risk members of all subgroups receive interventions. e) When false positives are expensive relative to false negatives and there are no fairness concerns, evaluation metrics should favor model improvements in decreasing order with score.", "description": "This figure illustrates how AUROC and AUPRC handle different types of model mistakes in binary classification, particularly highlighting the impact of class imbalance and its relation to fairness and ethical considerations.  Panel (a) sets the stage, showing how AUROC treats all mistakes equally, while AUPRC favors correcting mistakes at higher scores. Panels (b) to (e) illustrate different real-world scenarios where either AUROC or AUPRC would be more appropriate as the evaluation metric.", "section": "2 Theoretical Analyses"}, {"figure_path": "S3HvA808gk/figures/figures_47_1.jpg", "caption": "Figure 1: a) Consider a model f yielding continuous output scores for a binary classification task applied to a dataset consisting of two distinct subpopulations, A \u2208 {0, 1}. If we order samples in ascending order of output score, each misordered pair of samples (e.g., mistake 1-4) represents an opportunity for model improvement. Theorem 2 shows that a model's AUROC will improve by the same amount no matter which mistake you fix, while the model's AUPRC will improve by an amount correlated with the score of the sample. b) When comparing models absent a specific deployment scenario, we have no reason to value improving one mistake over another, and model evaluation metrics should therefore improve equally regardless of which mistake is corrected. c) When false negatives have a high cost relative to false positives, evaluation metrics should favor mistakes that have lower scores, regardless of any class imbalance. d) When limited resources will be distributed among a population according to model score, in a manner that requires certain subpopulations to all be offered commensurate possible benefit from the intervention for ethical reasons, evaluation metrics should prioritize the importance of within-group, high-score mistakes such that the highest risk members of all subgroups receive interventions. e) When false positives are expensive relative to false negatives and there are no fairness concerns, evaluation metrics should favor model improvements in decreasing order with score.", "description": "This figure illustrates the difference between AUROC and AUPRC in terms of how they prioritize correcting model mistakes in binary classification tasks.  AUROC treats all mistakes equally, while AUPRC prioritizes high-score mistakes, which can lead to algorithmic bias and fairness concerns, particularly when dealing with multiple subpopulations with different positive label prevalence.", "section": "2 Theoretical Analyses"}, {"figure_path": "S3HvA808gk/figures/figures_47_2.jpg", "caption": "Figure 1: a) Consider a model f yielding continuous output scores for a binary classification task applied to a dataset consisting of two distinct subpopulations, A \u2208 {0, 1}. If we order samples in ascending order of output score, each misordered pair of samples (e.g., mistake 1-4) represents an opportunity for model improvement. Theorem 2 shows that a model\u2019s AUROC will improve by the same amount no matter which mistake you fix, while the model\u2019s AUPRC will improve by an amount correlated with the score of the sample. b) When comparing models absent a specific deployment scenario, we have no reason to value improving one mistake over another, and model evaluation metrics should therefore improve equally regardless of which mistake is corrected. c) When false negatives have a high cost relative to false positives, evaluation metrics should favor mistakes that have lower scores, regardless of any class imbalance. d) When limited resources will be distributed among a population according to model score, in a manner that requires certain subpopulations to all be offered commensurate possible benefit from the intervention for ethical reasons, evaluation metrics should prioritize the importance of within-group, high-score mistakes such that the highest risk members of all subgroups receive interventions. e) When false positives are expensive relative to false negatives and there are no fairness concerns, evaluation metrics should favor model improvements in decreasing order with score.", "description": "This figure illustrates how AUROC and AUPRC differ in prioritizing model improvements (mistakes) in various scenarios with two subpopulations (A=0 and A=1).  (a) shows how AUROC and AUPRC handle mistakes differently.  Subsequent sections (b-e) illustrate different real-world use cases and how AUROC and AUPRC would prioritize improvements.  These different prioritizations highlight the importance of the choice of metric given the context.", "section": "2 Theoretical Analyses"}, {"figure_path": "S3HvA808gk/figures/figures_47_3.jpg", "caption": "Figure 1: a) Consider a model f yielding continuous output scores for a binary classification task applied to a dataset consisting of two distinct subpopulations, A \u2208 {0, 1}. If we order samples in ascending order of output score, each misordered pair of samples (e.g., mistake 1-4) represents an opportunity for model improvement. Theorem 2 shows that a model's AUROC will improve by the same amount no matter which mistake you fix, while the model's AUPRC will improve by an amount correlated with the score of the sample. b) When comparing models absent a specific deployment scenario, we have no reason to value improving one mistake over another, and model evaluation metrics should therefore improve equally regardless of which mistake is corrected. c) When false negatives have a high cost relative to false positives, evaluation metrics should favor mistakes that have lower scores, regardless of any class imbalance. d) When limited resources will be distributed among a population according to model score, in a manner that requires certain subpopulations to all be offered commensurate possible benefit from the intervention for ethical reasons, evaluation metrics should prioritize the importance of within-group, high-score mistakes such that the highest risk members of all subgroups receive interventions. e) When false positives are expensive relative to false negatives and there are no fairness concerns, evaluation metrics should favor model improvements in decreasing order with score.", "description": "This figure illustrates the key differences between AUROC and AUPRC in prioritizing model improvements based on the score assigned to samples and various use cases.  It highlights how AUROC treats all mistakes equally, while AUPRC prioritizes high-scoring mistakes, potentially leading to fairness concerns.", "section": "2 Theoretical Analyses"}, {"figure_path": "S3HvA808gk/figures/figures_47_4.jpg", "caption": "Figure 1: a) Consider a model f yielding continuous output scores for a binary classification task applied to a dataset consisting of two distinct subpopulations, A \u2208 {0, 1}. If we order samples in ascending order of output score, each misordered pair of samples (e.g., mistake 1-4) represents an opportunity for model improvement. Theorem 2 shows that a model's AUROC will improve by the same amount no matter which mistake you fix, while the model's AUPRC will improve by an amount correlated with the score of the sample. b) When comparing models absent a specific deployment scenario, we have no reason to value improving one mistake over another, and model evaluation metrics should therefore improve equally regardless of which mistake is corrected. c) When false negatives have a high cost relative to false positives, evaluation metrics should favor mistakes that have lower scores, regardless of any class imbalance. d) When limited resources will be distributed among a population according to model score, in a manner that requires certain subpopulations to all be offered commensurate possible benefit from the intervention for ethical reasons, evaluation metrics should prioritize the importance of within-group, high-score mistakes such that the highest risk members of all subgroups receive interventions. e) When false positives are expensive relative to false negatives and there are no fairness concerns, evaluation metrics should favor model improvements in decreasing order with score.", "description": "This figure illustrates how AUROC and AUPRC prioritize correcting different types of model mistakes in binary classification.  It highlights the key difference between the two metrics: AUROC treats all mistakes equally, while AUPRC favors high-scoring mistakes.  The figure provides several use cases, such as cancer screening, public health intervention, and small molecule screening, to showcase how the choice between AUROC and AUPRC impacts the performance and fairness of model predictions.", "section": "2 Theoretical Analyses"}, {"figure_path": "S3HvA808gk/figures/figures_48_1.jpg", "caption": "Figure 1: a) Consider a model f yielding continuous output scores for a binary classification task applied to a dataset consisting of two distinct subpopulations, A \u2208 {0, 1}. If we order samples in ascending order of output score, each misordered pair of samples (e.g., mistake 1-4) represents an opportunity for model improvement. Theorem 2 shows that a model's AUROC will improve by the same amount no matter which mistake you fix, while the model's AUPRC will improve by an amount correlated with the score of the sample. b) When comparing models absent a specific deployment scenario, we have no reason to value improving one mistake over another, and model evaluation metrics should therefore improve equally regardless of which mistake is corrected. c) When false negatives have a high cost relative to false positives, evaluation metrics should favor mistakes that have lower scores, regardless of any class imbalance. d) When limited resources will be distributed among a population according to model score, in a manner that requires certain subpopulations to all be offered commensurate possible benefit from the intervention for ethical reasons, evaluation metrics should prioritize the importance of within-group, high-score mistakes such that the highest risk members of all subgroups receive interventions. e) When false positives are expensive relative to false negatives and there are no fairness concerns, evaluation metrics should favor model improvements in decreasing order with score.", "description": "This figure illustrates how AUROC and AUPRC prioritize correcting different types of model mistakes, which has implications for model evaluation and selection in various scenarios, particularly when considering fairness and cost of errors.  In different use cases (cancer screening, public health intervention, and small molecule screening), the optimal prioritization of mistakes differs, revealing the limitations of AUPRC compared to AUROC in certain contexts.", "section": "2 Theoretical Analyses"}, {"figure_path": "S3HvA808gk/figures/figures_48_2.jpg", "caption": "Figure 1: a) Consider a model f yielding continuous output scores for a binary classification task applied to a dataset consisting of two distinct subpopulations, A \u2208 {0, 1}. If we order samples in ascending order of output score, each misordered pair of samples (e.g., mistake 1-4) represents an opportunity for model improvement. Theorem 2 shows that a model's AUROC will improve by the same amount no matter which mistake you fix, while the model's AUPRC will improve by an amount correlated with the score of the sample. b) When comparing models absent a specific deployment scenario, we have no reason to value improving one mistake over another, and model evaluation metrics should therefore improve equally regardless of which mistake is corrected. c) When false negatives have a high cost relative to false positives, evaluation metrics should favor mistakes that have lower scores, regardless of any class imbalance. d) When limited resources will be distributed among a population according to model score, in a manner that requires certain subpopulations to all be offered commensurate possible benefit from the intervention for ethical reasons, evaluation metrics should prioritize the importance of within-group, high-score mistakes such that the highest risk members of all subgroups receive interventions. e) When false positives are expensive relative to false negatives and there are no fairness concerns, evaluation metrics should favor model improvements in decreasing order with score.", "description": "This figure illustrates how AUROC and AUPRC prioritize correcting different types of model mistakes in binary classification tasks, especially considering class imbalance and subpopulations.  It uses several example scenarios to highlight how the choice of metric influences the focus on improving model performance in different regions of the output score distribution, which in turn can have implications for fairness and ethical considerations.", "section": "2 Theoretical Analyses"}, {"figure_path": "S3HvA808gk/figures/figures_48_3.jpg", "caption": "Figure 1: a) Consider a model f yielding continuous output scores for a binary classification task applied to a dataset consisting of two distinct subpopulations, A \u2208 {0, 1}. If we order samples in ascending order of output score, each misordered pair of samples (e.g., mistake 1-4) represents an opportunity for model improvement. Theorem 2 shows that a model\u2019s AUROC will improve by the same amount no matter which mistake you fix, while the model\u2019s AUPRC will improve by an amount correlated with the score of the sample. b) When comparing models absent a specific deployment scenario, we have no reason to value improving one mistake over another, and model evaluation metrics should therefore improve equally regardless of which mistake is corrected. c) When false negatives have a high cost relative to false positives, evaluation metrics should favor mistakes that have lower scores, regardless of any class imbalance. d) When limited resources will be distributed among a population according to model score, in a manner that requires certain subpopulations to all be offered commensurate possible benefit from the intervention for ethical reasons, evaluation metrics should prioritize the importance of within-group, high-score mistakes such that the highest risk members of all subgroups receive interventions. e) When false positives are expensive relative to false negatives and there are no fairness concerns, evaluation metrics should favor model improvements in decreasing order with score.", "description": "This figure illustrates the differences between AUROC and AUPRC in terms of how they prioritize model improvements (mistakes) across different scenarios, with varying cost ratios and fairness considerations. It demonstrates that AUROC treats all mistakes equally, while AUPRC prioritizes high-score mistakes, which can lead to unfairness in multi-population use cases.", "section": "2 Theoretical Analyses"}, {"figure_path": "S3HvA808gk/figures/figures_48_4.jpg", "caption": "Figure 1: a) Consider a model f yielding continuous output scores for a binary classification task applied to a dataset consisting of two distinct subpopulations, A \u2208 {0, 1}. If we order samples in ascending order of output score, each misordered pair of samples (e.g., mistake 1-4) represents an opportunity for model improvement. Theorem 2 shows that a model\u2019s AUROC will improve by the same amount no matter which mistake you fix, while the model\u2019s AUPRC will improve by an amount correlated with the score of the sample. b) When comparing models absent a specific deployment scenario, we have no reason to value improving one mistake over another, and model evaluation metrics should therefore improve equally regardless of which mistake is corrected. c) When false negatives have a high cost relative to false positives, evaluation metrics should favor mistakes that have lower scores, regardless of any class imbalance. d) When limited resources will be distributed among a population according to model score, in a manner that requires certain subpopulations to all be offered commensurate possible benefit from the intervention for ethical reasons, evaluation metrics should prioritize the importance of within-group, high-score mistakes such that the highest risk members of all subgroups receive interventions. e) When false positives are expensive relative to false negatives and there are no fairness concerns, evaluation metrics should favor model improvements in decreasing order with score.", "description": "This figure illustrates the differences between AUROC and AUPRC in terms of how they prioritize correcting model mistakes.  AUROC treats all mistakes equally, while AUPRC prioritizes high-scoring mistakes.  The figure also shows how this difference affects model selection in various scenarios such as cancer screening and public health intervention, highlighting the importance of selecting the appropriate metric based on the specific application and ethical considerations.", "section": "2 Theoretical Analyses"}, {"figure_path": "S3HvA808gk/figures/figures_49_1.jpg", "caption": "Figure 2: Synthetic experiment per-group AUROC, showing a confidence interval spanning the 5th to 95th percentile of results observed across all seeds, after successively either fixing individual mistakes, as defined in Definition 2.1, (a) and b)) or successively choosing the optimal score permutation (c) and d)) in order to optimize either AUROC (a) and c)) or AUPRC (b) and d)). It is clear across both forms of optimization that AUPRC definitively favors the higher prevalence subpopulation, whereas AUROC treats subgroups approximately equally. Similar patterns were observed when comparing per-group AUPRCs over the same experimental procedures, as shown in Appendix Figure 4.", "description": "The figure compares the performance of AUROC and AUPRC on synthetic data with two subpopulations under two optimization strategies. The results show that AUPRC favors the higher-prevalence subpopulation, while AUROC treats both groups equally.", "section": "3 Experimental Validation"}, {"figure_path": "S3HvA808gk/figures/figures_49_2.jpg", "caption": "Figure 2: Synthetic experiment per-group AUROC, showing a confidence interval spanning the 5th to 95th percentile of results observed across all seeds, after successively either fixing individual mistakes, as defined in Definition 2.1, (a) and b)) or successively choosing the optimal score permutation (c) and d)) in order to optimize either AUROC (a) and c)) or AUPRC (b) and d)). It is clear across both forms of optimization that AUPRC definitively favors the higher prevalence subpopulation, whereas AUROC treats subgroups approximately equally. Similar patterns were observed when comparing per-group AUPRCs over the same experimental procedures, as shown in Appendix Figure 4.", "description": "This figure shows the results of synthetic experiments comparing AUROC and AUPRC optimization methods. Two methods were used: fixing individual mistakes and permuting scores. The results demonstrate that AUPRC disproportionately favors the high-prevalence subpopulation during optimization, unlike AUROC, which treats both groups more equally.", "section": "3.1 Synthetic optimization experiments demonstrate AUPRC-induced disparities"}, {"figure_path": "S3HvA808gk/figures/figures_51_1.jpg", "caption": "Figure 2: Synthetic experiment per-group AUROC, showing a confidence interval spanning the 5th to 95th percentile of results observed across all seeds, after successively either fixing individual mistakes, as defined in Definition 2.1, (a) and b)) or successively choosing the optimal score permutation (c) and d)) in order to optimize either AUROC (a) and c)) or AUPRC (b) and d)). It is clear across both forms of optimization that AUPRC definitively favors the higher prevalence subpopulation, whereas AUROC treats subgroups approximately equally. Similar patterns were observed when comparing per-group AUPRCs over the same experimental procedures, as shown in Appendix Figure 4.", "description": "This figure compares the effects of optimizing for AUROC and AUPRC in a synthetic experiment with two subpopulations.  It shows that AUPRC optimization disproportionately favors the higher-prevalence subpopulation, while AUROC treatment of the groups is more equitable. This is demonstrated using two different optimization procedures: successively correcting individual mistakes (a and b) and successively selecting an optimal score permutation (c and d). The results highlight the bias AUPRC can introduce.", "section": "3.1 Synthetic optimization experiments demonstrate AUPRC-induced disparities"}, {"figure_path": "S3HvA808gk/figures/figures_51_2.jpg", "caption": "Figure 3: Difference in the Spearman\u2019s \u03c1 between the test-set signed AUROC gap versus the validation set overall AUPRC, and the AUROC gap versus the overall AUROC. Numbers in parentheses are the prevalence ratios between the two groups for the particular attribute, and datasets are sorted by this quantity. Error bars are 95% confidence intervals from 20 different random data splits.", "description": "This figure displays the results of real-world experiments to validate the theoretical findings of the paper. It shows the difference in Spearman's correlation between the AUROC gap (the difference in AUROC between two subpopulations) and overall AUPRC, and the AUROC gap and overall AUROC. The x-axis represents the prevalence ratio between the two groups in each dataset, and the y-axis represents the difference in Spearman's correlation.  Datasets are sorted by prevalence ratio. Error bars show 95% confidence intervals based on 20 different random data splits.  The results show a positive correlation between prevalence ratio and the difference in Spearman's correlation, indicating that AUPRC favors high-prevalence subpopulations more than AUROC.", "section": "3 Experimental Validation"}]