[{"type": "text", "text": "A Closer Look at AUROC and AUPRC under Class Imbalance ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Matthew B. McDermott Harvard Medical School matthew_mcdermott@hms.harvard.edu ", "page_idx": 0}, {"type": "text", "text": "Haoran Zhang Massachusetts Institute of Technology haoranz@mit.edu ", "page_idx": 0}, {"type": "text", "text": "Lasse Hyldig Hansen Aarhus University 201908623@post.au.dk ", "page_idx": 0}, {"type": "text", "text": "Giovanni Angelotti IRCCS Humanitas Research Hospital giovanni.angelotti@humanitas.it ", "page_idx": 0}, {"type": "text", "text": "Jack Gallifant Massachusetts Institute of Technology jgally@mit.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In machine learning (ML), a widespread claim is that the area under the precisionrecall curve (AUPRC) is a superior metric for model comparison to the area under the receiver operating characteristic (AUROC) for tasks with class imbalance. This paper refutes this notion on two fronts. First, we theoretically characterize the behavior of AUROC and AUPRC in the presence of model mistakes, establishing clearly that AUPRC is not generally superior in cases of class imbalance. We further show that AUPRC can be a harmful metric as it can unduly favor model improvements in subpopulations with more frequent positive labels, heightening algorithmic disparities. Next, we empirically support our theory using experiments on both semi-synthetic and real-world fairness datasets. Prompted by these insights, we reviewed over 1.5 million scientific papers to understand the origin of this invalid claim\u2013finding is often made without citation, misattributed to papers that do not argue this point, and aggressively overgeneralized from source arguments. Our findings represent a dual contribution: a significant technical advancement in understanding the relationship between AUROC and AUPRC and a stark warning about unchecked assumptions in the ML community. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Machine learning (ML), especially in critical domains like healthcare, necessitates careful selection and application of evaluation metrics to guide appropriate model choices and understand performance nuances [150]. Model evaluation can happen in one of two settings: (1) a methodological/model comparison setting, which occurs outside of a specific deployment setting and target model usage workflows, optimal decision thresholds, or specific false-positive (FP) and false-negative (FN) costs are typically not known, or (2) an application/deployment setting, where reasonably specific estimates of model usage workflows and FP/FN costs can be made. In both settings, appropriate metric choice is critical\u2014 and inappropriate selection can hinder innovation when used for model comparison and lead to significant real-world costs (e.g., misdiagnosis in a medical setting) in deployment settings. ", "page_idx": 0}, {"type": "text", "text": "This study focuses on two widely used metrics for binary classification tasks across both evaluation contexts: Area Under the Precision-Recall Curve (AUPRC) and Area Under the Receiver Operating Characteristic (AUROC). Central to this paper is the following key claim: ", "page_idx": 0}, {"type": "text", "text": "Claim 1. Let $f$ be a model which outputs continuous probabilistic predictions trained to solve a binary classification task for which the prevalence of negative labels is significantly higher than the prevalence of positive labels. For this problem, the AUPRC will yield a \u201cbetter\u201d or \u201cmore accurate\u201d or \u201cfairer\u201d evaluation of $f$ than the AUROC. ", "page_idx": 1}, {"type": "text", "text": "Claim 1 is made widely in both the scientific literature [399, 71, 159, 124], in ML educational content like notable textbooks [119, 141], and in popular press sources [80, 254]. It is so widespread that even basic search results for queries relating to AUROC and AUPRC1 and large language model assistants like ChatGPT or Github Co-pilot will profess its veracity.2Throughout these sources, it has been justified on numerous, often imprecise grounds (see Section 5), but despite this extensive attention, we show in this work that this claim is, in fact, wrong, and may be dangerous from a model fairness perspective; further, many of its justifications are invalid or misapplied in common ML settings. More specifically, we show the following: ", "page_idx": 1}, {"type": "text", "text": "1) AUROC and AUPRC only differ with respect to model-dependent parameters in that AUROC weighs all false positives equally, whereas AUPRC weighs false positives at a threshold $\\tau$ with the inverse of the model\u2019s likelihood of outputting any scores greater than $\\tau$ (Theorem 1). This result shows that we can reason about the suitability of optimizing or selecting AUROC vs. AUPRC based on whether we care more about reducing false positives above low thresholds or high thresholds. In particular, ", "page_idx": 1}, {"type": "text", "text": "2) AUROC favors model improvements uniformly over all positive samples, whereas AUPRC favors improvements for samples assigned higher scores over those assigned lower scores (Theorem 2). This indicates that the key factor differentiating the utility of AUROC or AUPRC as an evaluation metric is not class imbalance at all, but it is rather based on the target use case of the model in question. See Figure 1 for a visual explanation. It also reveals that AUPRC can amplify algorithmic biases. In particular, ", "page_idx": 1}, {"type": "text", "text": "3) AUPRC can unduly prioritize improvements to higher-prevalence subpopulations at the expense of lower-prevalence subpopulations, raising serious fairness concerns in any multi-population use cases (Theorem 3). ", "page_idx": 1}, {"type": "text", "text": "In this work, we establish these three claims theoretically via synthetic experiments and real-world validation on popular public fairness datasets. In addition, we demonstrate through an extensive, large-language model aided, literature review of over 1.5 million scientific papers that Claim 1 has been used to motivate numerous improper uses of AUPRC relative to AUROC across high-stakes domains like healthcare and in several established venues, including AAAI, NeurIPS, ICML, ICLR, Cancer Cell, Nature Journals, PNAS, and more. Through this paper, we hope to shed light on the nuances of appropriate evaluation and provide key guidance to limit future misuse of evaluation metrics in the scientific and machine learning communities. ", "page_idx": 1}, {"type": "text", "text": "2 Theoretical Analyses ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Please note that all notation used is defined in Appendix Section C. ", "page_idx": 1}, {"type": "text", "text": "2.1 Relationship between AUROC and AUPRC ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this section, we introduce Theorem 1, which is as follows: ", "page_idx": 1}, {"type": "text", "text": "Theorem 1. Let $\\mathcal{X},\\mathcal{Y}=0,1$ represent a paired feature and binary classification label space from which i.i.d. samples $(x,y)\\in\\mathcal{X}\\times\\mathcal{Y}$ are drawn via the joint distribution over the random variables $\\times,\\ y$ . Let $f:\\mathcal{X}\\to(0,1)$ be a binary classification model outputting continuous probability scores over this space. Then, ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{AUROC}(f)=1-\\mathbb{E}_{t\\sim f(\\cdot)|\\cdot=1}\\left[\\mathrm{FPR}(f,t)\\right]}\\\\ &{\\mathrm{AUPRC}(f)=1-P_{\\mathrm{y}}(y=0)\\mathbb{E}_{t\\sim f(\\cdot)|\\cdot=1}\\left[\\frac{\\mathrm{FPR}(f,t)}{P_{\\mathrm{x}}(f(x)>t)}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "image", "img_path": "S3HvA808gk/tmp/38b7c5f9d5a769406fc007149cb44bb4c0b437cf02471db1d8976086f1c3c983.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 1: a) Consider a model $f$ yielding continuous output scores for a binary classification task applied to a dataset consisting of two distinct subpopulations, ${\\mathcal{A}}\\in\\{0,1\\}$ . If we order samples in ascending order of output score, each misordered pair of samples (e.g., mistake 1-4) represents an opportunity for model improvement. Theorem 2 shows that a model\u2019s AUROC will improve by the same amount no matter which mistake you fix, while the model\u2019s AUPRC will improve by an amount correlated with the score of the sample. b) When comparing models absent a specific deployment scenario, we have no reason to value improving one mistake over another, and model evaluation metrics should therefore improve equally regardless of which mistake is corrected. c) When false negatives have a high cost relative to false positives, evaluation metrics should favor mistakes that have lower scores, regardless of any class imbalance. d) When limited resources will be distributed among a population according to model score, in a manner that requires certain subpopulations to all be offered commensurate possible benefti from the intervention for ethical reasons, evaluation metrics should prioritize the importance of within-group, high-score mistakes such that the highest risk members of all subgroups receive interventions. e) When false positives are expensive relative to false negatives and there are no fairness concerns, evaluation metrics should favor model improvements in decreasing order with score. ", "page_idx": 2}, {"type": "text", "text": "We provide the proof in Appendix Section D. The two key intuitions are that integrating over the TPR is equivalent to taking the expectation over the induced distribution of positive sample scores, and that via Bayes rule, Prec(f, \u03c4) = 1 \u2212Py(y = 0) PFx(PfR(x(f),>\u03c4\u03c4)). ", "page_idx": 3}, {"type": "text", "text": "Despite its simplicity, Theorem 1 has far-reaching implications. Namely, it reveals that the only difference between AUROC and AUPRC with respect to model dependent parameters (i.e., omitting the dependence of AUPRC on the fixed prevalence of the dataset, which is not model varying) is that optimizing AUROC equates to minimizing the expected false positive rate over all positive samples in an unweighted manner (equivalently, in expectation over the distribution of positive sample scores) whereas optimizing AUPRC equates to minimizing the expected false positive rate over all positive samples weighted by the inverse of the model\u2019s \u201cfiring rate\u201d $(P_{\\times}(f(x)>\\tau))$ at the given positive sample score. This preference can be crystallized when we examine how AUROC vs. AUPRC would prioritize correcting indivisible units of model improvements, termed \u201cmistakes\u201d which we will discuss next. ", "page_idx": 3}, {"type": "text", "text": "2.2 AUPRC prioritizes high-score mistakes, AUROC treats all mistakes equally ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Understanding how a given evaluation metric prioritizes correcting various kinds of model mistakes or errors offers significant insight into when that metric should be used for optimization or model selection. To examine this topic for AUROC and AUPRC, consider the following definition of an \u201cincorrectly ranked adjacent pair\u201d, which we will colloquially refer to as a \u201cmodel mistake\u201d: ", "page_idx": 3}, {"type": "text", "text": "Definition 2.1. Let $f,\\mathcal{X},\\mathcal{Y},\\times,\\mathsf{y}$ be defined as in Theorem 1. Further, let us suppose we have sampled a static dataset from $\\times$ , y for evaluation which will be denoted $X,y=\\{(x_{1},\\overbar{y_{1}}),\\ldots,(x_{N},y_{N})\\}$ , for $x_{i}\\in\\mathcal{X},y_{i}\\in\\{0,1\\}$ , and $N\\in\\mathbb{N}$ . We assume for convenience that $f$ is an injective map and all $x_{i}$ are distinct (i.e., $\\forall(i,j):x_{i}\\neq x_{j}$ which, by injectivity of $f$ , implies that $f(x_{i})\\neq f(x_{j}))$ . ", "page_idx": 3}, {"type": "text", "text": "We say that $(x_{i},x_{j})$ are an incorrectly ranked adjacent pair and thus that the model makes a \u201cmistake\u201d at samples $(x_{i},x_{j})$ if: ", "page_idx": 3}, {"type": "text", "text": "Essentially, Definition 2.1 states that a mistake occurs when a model assigns adjacent probability scores to a pair of samples with discordant labels, as shown in Figure 1. With this in mind, we can then introduce Theorem 2 which states that AUROC improves by a constant amount regardless of which mistake is corrected for a given model and dataset whereas AUPRC improves more when the mistake corrected occurs at a higher score than when it occurs at a lower score: ", "page_idx": 3}, {"type": "text", "text": "Theorem 2. Define $f,\\mathcal{X},X,y$ and $N$ as in Definition 2.1. Further, suppose without loss of generality that the dataset $\\mathbf{\\deltaX}$ is ordered such that $f(x_{i})~<~f(x_{i+1})$ for all $i$ . Then, let us define $M\\,=$ $\\{i|(x_{i},x_{i+1})$ is an incorrectly ranked adjacent pair for model $f\\}$ . Define $f_{i}^{\\prime}$ to be a model that is identical to $f$ except that the probabilities assigned to $x_{i}$ and $x_{i+1}$ are swapped: ", "page_idx": 3}, {"type": "equation", "text": "$$\nf_{i}^{\\prime}:{\\left\\{\\begin{array}{l l}{f(x)}&{{\\mathrm{if~}}x\\not\\in\\{x_{i},x_{i+1}\\}}\\\\ {f(x_{i+1})}&{{\\mathrm{if~}}x=x_{i}}\\\\ {f(x_{i})}&{{\\mathrm{if~}}x=x_{i+1}.}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Then, $\\mathrm{AUROC}(f_{i}^{\\prime})\\,=\\,\\mathrm{AUROC}(f_{j}^{\\prime})$ for all $i,j\\;\\in\\;M$ , and $\\mathrm{AUPRC}(f_{i}^{\\prime})\\,<\\,\\mathrm{AUPRC}(f_{j}^{\\prime})$ for all $i,j\\in M$ such that $i<j$ . ", "page_idx": 3}, {"type": "text", "text": "The proof for Theorem 2 can be found in Appendix E. This proof simply stems from the fact that correcting a single mistake $(x_{i},x_{j})$ (as defined in Definition 2.1) always changes the false positive rate by the same amount, and only changes it at the threshold $f(x_{i})$ . This, combined with the formalization of AUROC and AUPRC in Theorem 1, establishes the proof. Note that this Theorem can be trivially extended to include a case where ties are possible simply by noting that \u201cswapping\u201d two samples $x_{i}$ and $x_{j}$ in the manner of the theorem results in no change to either AUROC or AUPRC, and similarly by the same reasoning separating any tie in the appropriate direction will improve AUROC uniformly over samples and will improve AUPRC in a manner monotonic with model score. ", "page_idx": 3}, {"type": "text", "text": "2.3 AUPRC is explicitly discriminatory in favor of high-scoring subpopulations ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The reliance on a model\u2019s firing rate revealed in Theorem 1 and the optimization behavior in Theorem 2 reveals significant issues with the fairness of AUPRC. In particular, in this section we introduce Theorem 3: ", "page_idx": 4}, {"type": "text", "text": "Theorem 3. Let $f,\\boldsymbol{\\mathcal{X}},\\boldsymbol{X},\\boldsymbol{\\boldsymbol{y}},N,M$ , and $f_{j}^{\\prime}$ all be defined as in Theorem 2. Further, suppose that in this setting the domain $\\mathcal{X}$ now contains an attribute defining two subgroups, $A=\\{0,1\\}$ , such that for any sample $\\left({x_{i},y_{i}}\\right)$ , $a_{i}$ denotes the subgroup to which that sample belongs. Let $f$ be perfectly calibrated for samples in subgroup $a=0$ , such that $P_{\\mathrm{y|a,x}}(y=1|a\\stackrel{\\cdot}{=}0,f(x)\\stackrel{\\cdot}{=}t)=\\dot{t}$ . Then, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{P_{\\gamma|\\mathfrak{a}}(y=1|a=0)\\to0}P\\left(a_{i}=a_{i+1}=1\\left|i=\\arg\\operatorname*{max}_{j\\in M}\\left(\\mathrm{AUPRC}(f_{j}^{\\prime})\\right)\\right)=1.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Essentially, Theorem 3 (proof provided in Appendix F) shows the following. Suppose we are training a model $f$ over a dataset with two subpopulations: Population $a=0$ and $a=1$ . If the model $f$ is calibrated and the rate at which $y=1$ for population $a=0$ is sufficiently low relative to the rate at which $y=1$ for population $a=1$ , then the mistake that, were it fixed, would maximally improve the AUPRC of $f$ will be a mistake purely in population $a\\,=\\,1$ . This demonstrates that AUPRC provably favors higher prevalence subpopulations (those with a higher base rate at which $y=1$ ) under sufficiently severe prevalence imbalance between subpopulations. ", "page_idx": 4}, {"type": "text", "text": "Note that this property is, generally speaking, not desirable. In particular, this property establishes that in settings where model fairness among a set of subpopulations in the data is important, AUPRC should not be used as an evaluation metric due to the risk that it will introduce biases in favor of the highest prevalence subpopulations. We validate this result empirically over both synthetic and real-world data in Section 3, demonstrating that the import of Theorem 3 is not merely limited to an analytical curiosity but can have real-world impact on algorithmic disparities in practice. Furthermore, note that this theorem does not indicate that AUPRC will be superior to AUROC for differentiating a low prevalence (or low risk) subpopulation relative to a high-risk subpopulation, a property that is sometimes attributed to AUPRC in the literature. Rather, Theorem 3 shows that maximizing AUPRC will be more likely to optimize solely within the high-risk subgroup, rather than optimizing to differentiate across subgroups, as low-risk subgroup samples will only occur in lower-score regions under severe class imbalance. ", "page_idx": 4}, {"type": "text", "text": "3 Experimental Validation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we establish via synthetic and real-world experiments that Theorem 3 is not merely an analytical effect but has real world consequences on the implications of optimizing or performing model selection via AUPRC. ", "page_idx": 4}, {"type": "text", "text": "3.1 Synthetic optimization experiments demonstrate AUPRC-induced disparities ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we use a carefully constructed synthetic optimization procedure to demonstrate that, when all other factors are equal, optimizing by or performing model selection on the basis of AUPRC vs. AUROC risks excacerbating algorithmic disparities in the manner predicted by Theorem 3. For analyses under more realistic conditions with more standard models, see our real-world experiments in Section 3.2. ", "page_idx": 4}, {"type": "text", "text": "Experimental Setup. Let $y\\in\\{0,1\\}$ be the binary label, $s\\in[0,1]$ be the predicted score, and $a\\in\\{1,2\\}$ be the subpopulation. We fix $P_{\\mathrm{y|a}}(y=1|a=1)=0.05$ and $P_{\\mathsf{y}|\\mathsf{a}}(\\bar{y}=1|a=2)=0.01$ . We sample a dataset for each group $\\mathcal{D}_{a}\\,=\\,\\bigl\\{\\bigl(s_{1},y_{1}\\bigr),...,\\bigl(s_{n_{a}},y_{n_{a}}\\bigr)\\bigr\\}$ , such that $\\mathrm{AUROC}(\\mathcal{D}_{1})\\approx$ $\\mathrm{AUROC}(\\bar{D}_{2})\\approx\\mathrm{AUROC}(\\bar{D}_{1}\\cup\\bar{\\mathcal{D}}_{2})\\stackrel{\\cdot\\cdot}{=}0.85$ (See Appendix G.1; A target AUROC of 0.65 was also profiled in Appendix Figure 5). ", "page_idx": 4}, {"type": "text", "text": "Our main experimental challenge is to determine how to simulate \u201coptimizing\u201d or \u201cselecting\u201d a model by AUROC or AUPRC. Simulating optimizing by these metrics allows us to explicitly assess how the use of either AUPRC or AUROC as an evaluation metric in model selection processes such as hyperparameter tuning or architecture search, can translate into model-induced inequities in dangerous ways. We explore two approaches here. First, we can simply correct the atomic mistake that maximally improves AUROC or AUPRC in each optimization iteration. In our experiments, we use $n_{1}=n_{2}=200$ and optimize for 50 steps for this experiment. This is the most straightforward optimization procedure to analyze, but it is unrealistic. In real optimization scenarios, larger model changes will be made at once, and a model will have an opportunity to degrade performance in some regions in order to improve it in others. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Next, we proflie an optimization procedure that randomly permutes all the (sorted) model scores up to 3 positions (See Appendix G.3 for details). This has the effect of randomly adjusting all model scores, and can worsen model performance under some random permutations, but offers precisely the same \u201coptimization capacity\u201d to the low and high prevalence subgroups. To ensure the model is under some optimization constraint (and therefore does not always find the \u201cperfect\u201d permutation to maximize both metrics identically), we allow the model to sample only 15 possible permutations before choosing the best option. This means the system will be forced to navigate optimization trade-offs between which permutations improve the right regions of the score most effectively among its limited set. We use $n_{1}=n_{2}=100$ for these experiments and optimize for 25 total steps. ", "page_idx": 5}, {"type": "image", "img_path": "S3HvA808gk/tmp/56145e463f039b5263654f18cf1a79ddfcefadd02002d83447a0b4580a5d4a18.jpg", "img_caption": ["a) Mistakes Fixed by AUROC ", "c) Permutation optimization by AUROC "], "img_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "S3HvA808gk/tmp/f6d170b644338896408572e1e69a5337d466e5f68ef585d6f753117fe646f02f.jpg", "img_caption": ["d) Permutation optimization by AUPRC "], "img_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "S3HvA808gk/tmp/e82cfbbf7722b3bdf7e3bab2c3e78fbd1ffcd72a855dffd897bfb4b248e09035.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "S3HvA808gk/tmp/1e1f02edef6864b4ef46b7be2ff5e9d67ff4e29db87b57e564b3261132deb7ef.jpg", "img_caption": [], "img_footnote": ["Low-prevalence Group High-prevalence Group "], "page_idx": 5}, {"type": "text", "text": "Figure 2: Synthetic experiment per-group AUROC, showing a confidence interval spanning the 5th to 95th percentile of results observed across all seeds, after successively either fixing individual mistakes, as defined in Definition 2.1, (a) and ${\\bf b}.$ )) or successively choosing the optimal score permutation (c) and d)) in order to optimize either AUROC (a) and c)) or AUPRC (b) and d)). It is clear across both forms of optimization that AUPRC definitively favors the higher prevalence subpopulation, whereas AUROC treats subgroups approximately equally. Similar patterns were observed when comparing per-group AUPRCs over the same experimental procedures, as shown in Appendix Figure 4. ", "page_idx": 5}, {"type": "text", "text": "Across both settings, we run these experiments across 20 randomly sampled datasets and show the mean and an empirical $90\\%$ confidence interval around the mean in Figure 2. We present a formal mathematical formulation of these perturbations, as well as proflie a third random perturbation method, in Appendix G.3. ", "page_idx": 5}, {"type": "text", "text": "Results. Our results demonstrate the impact of the optimization metric on subpopulation disparity. In particular, in Figure 2, we observe a notable disparity introduced when optimizing under the AUPRC metric regardless of the optimization procedure. This is evident in the performance metrics across the high and low prevalence subpopulations, which exhibit significant divergence as the optimization process favors the group with higher prevalence. In the more realistic, randompermutation optimization procedure (Figure 2d), this even results in a decrease in the AUROC for the low prevalence subgroup. In comparison, when optimizing for overall AUROC, the AUROC of both groups increase together. Note that we show the effect of this optimization on the AUPRC metric, which shows very similar trends, in Appendix Figure 4. These results demonstrate explicitly that not only does optimizing for AUPRC differ greatly than for AUROC, as has been noted historically by researchers developing explicit AUPRC optimization schemes [409], but it in fact does in an explicitly discriminatory way in very realistic scenarios. ", "page_idx": 5}, {"type": "image", "img_path": "S3HvA808gk/tmp/051562d8b0d6d3107637721d05bf7dd3346f82dfd6fa48d85f9ed78dc3953179.jpg", "img_caption": ["Figure 3: Difference in the Spearman\u2019s $\\rho$ between the test-set signed AUROC gap versus the validation set overall AUPRC, and the AUROC gap versus the overall AUROC. Numbers in parentheses are the prevalence ratios between the two groups for the particular attribute, and datasets are sorted by this quantity. Error bars are $95\\%$ confidence intervals from 20 different random data splits. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "3.2 Real-world experimental validation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To demonstrate the generalizability of our finding to the real world, we evaluate fairness gaps induced by AUROC and AUPRC selection on four common datasets in the fairness literature [441, 99, 205]. ", "page_idx": 6}, {"type": "text", "text": "Datasets. We use the following four tabular binary classification datasets: adult [17], compas [14], lsac [413], and mimic [178]. In each dataset, we consider both sex and race as sensitive attributes. To mimic the setting of our theorems, we balance each dataset by the sensitive attribute during both training and test, by randomly subsampling the majority group. Further details about each dataset, as well as preprocessing steps, can be found in Appendix H. ", "page_idx": 6}, {"type": "text", "text": "Experimental setup. We train XGBoost models [65] on each dataset. For each task, we iterate over a grid of per-group weights in order to create a diverse set of models that favor different groups. For each setting of task and per-group weight, we conduct a random hyperparameter search [37] with 50 runs. Though more complex hyperparameter search methods such as BOHB [100] or TPE [36] might lead to better performance, random searches are far more popular and practical, and have been used in popular benchmarking libraries [131, 337]. ", "page_idx": 6}, {"type": "text", "text": "We evaluate the validation set overall AUROC and AUPRC. We also evaluate the test set AUROC gap and AUPRC gap between groups, where gaps are defined as the value of the metric for the higher prevalence group minus the value for the lower prevalence group. Based on our theorems, our hypothesis is that overall AUPRC should be more positively correlated with the signed AUROC gap than overall AUROC, indicating that it better favors the higher prevalence group, especially when the prevalence ratio between groups is high. To test this hypothesis, we evaluate the Spearman correlation coefficient between these quantities. We repeat this experiment 20 times, with different random data splits, to obtain a $95\\%$ confidence interval. ", "page_idx": 6}, {"type": "text", "text": "Results. In Figure 3, we plot the difference in the Spearman correlation coefficient of the AUROC gap versus the overall AUPRC, and AUROC gap versus overall AUROC. We observe mixed results in datasets with low prevalence ratio. In dataset with higher prevalence ratio, we find that overall AUPRC is more positively correlated with the AUROC gap than overall AUROC, indicating that AUPRC more aggressively favors the higher prevalence group. We emphasize that the prevalence ratios observed in these real-world datasets is much lower than the ratio of 5 used in our synthetic experiments, which may account for the mild effect observed. To see raw results from these experiments, see Appendix Figure 7. Similar results for neural network classifiers can be found in Appendix Section H.3. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Next, in Appendix Figure 8, we plot the difference in the Spearman\u2019s $\\rho$ from Figure 3, versus the prevalence gap. We find that there is a statistically significant correlation between the two (Spearman\u2019s $\\rho=0.905$ , ${\\bf p}=0.002)$ ). Thus, while our power to detect a prevalence mediated AUPRC bias amplification effect is limited due to the limited prevalence disparities in these datasets, we nonetheless observe a strong positive correlation between the extent of the prevalence mismatch between the low and high prevalence group and the amount that AUPRC favors the high prevalence group over AUROC. In other words, our results show that across these fairness datasets and attributes, as the prevalence disparity grows more extreme, we observe a statistically significant corresponding increase in the extent to which AUPRC introduces algorithmic bias, exactly in accordance with what Theorem 3 suggests. ", "page_idx": 7}, {"type": "text", "text": "4 When Should One Use AUPRC vs. AUROC? ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In Sections 2 and 3, we have shown that AUPRC is not universally superior in cases of class imbalance (and that instead, it merely preferentially optimizes high-score regions over low-score regions) and that it also poses serious risks to the model fairness in settings where subgroup prevalences differ. In light of this, how should we revise Claim 1 to reflect when we actually should use AUPRC instead of AUROC or vice versa? Below, we explore this question and provide practical guidance on metric selection for binary classification, building on our theoretical results and highlight specific contexts in which one may be favorable (Figure 1). Note that while we provide guidance below on situations in which AUROC vs. AUPRC is more or less favorable, this is not to suggest that authors should not report both metrics, or even larger sets of metrics or more nuanced analyses such as ROC or PR curves; rather this section is intended to offer guidance on what metrics should be seen as more or less appropriate for use in things like model selection, hyperparameter tuning, or being highlighted as the \u2019critical\u2019 metric in a given problem scenario. ", "page_idx": 7}, {"type": "text", "text": "For context-independent model evaluation, use AUROC: For model evaluations conducted outside of specific deployment contexts, where the differential costs of errors are undefined, the necessity for a metric that impartially values improvements across the entire model output space becomes paramount. As shown in Figure 1a. As it is not known in advance where samples of interest will live in the output space, nor are particular cost ratios known, there should be no preference for mistake correction. Therefore, in this setting AUROC is favorable as it uniformly accounts for every correction, offering a fair assessment irrespective of decision thresholds. ", "page_idx": 7}, {"type": "text", "text": "For deployment scenarios with elevated false negative costs, use AUROC: In applications where the consequences of false negatives are especially high, such as in the early screening for critical illnesses like cancer (Figure 1c), a primary goal of the model will be to achieve the fewest missed cancer cases; equating to prioritizing model recall. In such a scenario, the most important mistakes to correct occur at lower score thresholds, as high-score mistakes will not change which positive samples are missed in deployment settings (as chosen thresholds are likely to be low). This behavior is the inverse of what AUPRC prioritizes, demonstrating that in such situations, AUROC should be preferred over AUPRC. ", "page_idx": 7}, {"type": "text", "text": "For ethical resource distribution among diverse populations, use AUROC: When faced with the challenge of ethically allocating scarce resources across a broad population, necessitating equitable benefit distribution among subgroups (Figure 1d), one must avoid prioritizing model improvements that selectively favor one subpopulation. As AUPRC will target high-score regions selectively, it risks unduely favoring high-prevalence subpopulations, as shown in Theorem 3 and Figures 2 and 3. Even though in this resource distribution problem, high-score regions are selectively important compared to low-score regions, the fact that in this problem, we must prioritize across all subpopulations equally means that AUPRC\u2019s global preference is untenable as it could induce bias. ", "page_idx": 7}, {"type": "text", "text": "For reducing false positives in high-cost, single-group intervention prioritization or information retrieval settings, use AUPRC: In scenarios where the cost associated with false positives significantly outweighs that of false negatives, absent of equity concerns\u2014such as in selecting candidate molecules from a fragment library for drug development trials, where only the most promising molecules will proceed to costly experimental validation (Figure 1e)\u2014the metric of choice should facilitate a reduction in high-score false positives. This necessitates a focus on correcting high-score errors, for which AUROC might not be ideal due to its uniform treatment of errors across the score spectrum, potentially obscuring improvements in critical high-stake decisions. ", "page_idx": 8}, {"type": "text", "text": "5 Literature Review: Examining how Claim 1 Became so Widespread ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Claim 1 states that \u201cAUPRC is better than AUROC in cases of class imbalance\u201d and is widespread in the literature. Via both a manual literature search and an automated search of over 1.5M arXiv papers (see Appendix I for methodology), we observed 424 publications making this claim.3 This widespread disemmination of Claim 1 has a significant impact on the validity of scientific discourse. We observed examples of high-profile papers operating in medically critical settings where high-recall is a key priority evaluating ML systems via AUPRC due to their task\u2019s underlying class imbalance [399]; papers focusing on fairness critical applications relying on AUPRC due to this claim, even while our results demonstrate AUPRC has major problems in the fairness regime [366, 306], and numerous other papers perpetuating this source of scientific misinformation. ", "page_idx": 8}, {"type": "text", "text": "Among the 424 papers we discovered referencing this claim, 167 did so with no associated citation. These papers were published in a wide range of venues, including high proflie venues such as NeurIPS, ICML, and ICLR. This reflects not only the widespread belief in this claim, but also that we may be too comfortable making seemingly \u201ccorrect\u201d assertions without appropriate attribution in ML today. Further, Among the 257 that reference this claim and cite a source for this assertion, 135 do not cite any papers that actually make this claim in the first place. Most often, papers erroneously attribute this claim to [83], which was cited as a source for this claim 144 times. While [83] makes many interesting, meaningful claims about the ROC and PR curves, and does argue that the precision-recall curve may be more informative than the ROC in cases of class imbalance it never asserts that the area under the PR curve should be preferred over the area under the ROC in cases of class imbalance. This distinction is important, because while curves can be used to simultaneously communicate many different performance statistics to their viewers across different FPR/TPR or Precision/Recall criteria, and therefore should be assessed primarily as communication tools to deployment experts, areas under these curves are single-point summarizations which are primarily used for deployment-agnostic model comparison, which is a very different use case. ", "page_idx": 8}, {"type": "text", "text": "Even when appropriate papers are cited, the valid settings in which AUPRC should be preferred (see Section 4 for examples) are often over-shadowed by significant over-generalizations to preferring AUPRC in all settings featuring class imbalance. For example, claims such as that \u201cprecision-recall curves are more informative of deployment metrics\u201d are often used to justify why AUPRC should be used in all cases of class imbalance, rather than just in cases where the relevant deployment metrics are most directly associated with the PR curve. Another class of arguments made in favor of Claim 1 is rooted in claims that AUROC is poor in cases of class imbalance because its scores are misleadingly high. While this argument can reflect a meaningful limitation of the communication value of the ROC or the AUROC, comments about singleton metric results (rather than model comparison through metric values) are inherently orthogonal to the goal of model evaluation. In other words, what matters for model evaluation is not how high a given metric is, but rather the extent to which the metric meaningfully captures the right improvements in the model in the right ways. The widespread nature of Claim 1 has also led researchers astray when exploring new optimization procedures for AUPRC, by advocating for the importance of AUPRC when processing skewed data, even in domains such as medical diagnoses that often have high false negative costs relative to false positive costs [409]. For a more extensive breakdown of the arguments we observed in the literature and the sources making them, see Appendix Tables 2 and 3. ", "page_idx": 8}, {"type": "text", "text": "6 Limitations and Future Works ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "There are still a number of areas for further improvement and future work. Firstly, our theoretical findings can be refined and generalized to, e.g., take into account the difficulty of the target task (which may differ between subgroups), not require models to be calibrated (in the case of Theorem 3), or specifically take into account more than 2 subpopulations for more nuanced comparisons beyond what can be inferred through pairwise comparisons between subpopulations, where our results would naturally apply. Further, extending our real-world experiments to more fairness datasets and identifying more nuanced ways to probe the impact of metric choice on disparity measures would significantly strengthen this work. These analyses can also be extended to consider other metrics, such as the area under the precision-recall-gain curve [104], the area under the net benefti curve [384, 307], and single-threshold, deployment centric metrics as well. In addition, one of the largest limitations of Theorem 3 is its restrictive assumptions, in particular the requirement of perfect calibration. A ripe area of future work is thus to investigate how we can soften our analyses for models with imperfect calibration or to determine whether or not our results imply anything about the viability or safety of post-hoc calibration of models optimized either through AUPRC or AUROC. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This study interrogates the pervasive assumption within the ML community that AUPRC is a better evaluation metric than AUROC in class-imbalanced settings. Our empirical analyses and literature review reveal several concrete findings that challenge this notion. In particular, we show that while optimizing for AUROC equates to minimizing the model\u2019s FPR in an unbiased manner over positive sample scores, optimizing for AUPRC equates to minimizing the FPR specifically for regions where the model outputs higher scores relative to lower scores. Further, we show both theoretically and empirically over synthetic and real-world fairness datasets that AUPRC can be an explicitly discriminatory metric through favoring higher-prevalence subgroups. ", "page_idx": 9}, {"type": "text", "text": "In summary, our research advocates for a more thoughtful and context-aware approach to selecting evaluation metrics in machine learning. This paradigm shift, favoring a balanced and conscientious approach to metric selection, is essential in advancing the field towards developing not only technically sound, but also equitable and just models. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "MBAM gratefully acknowledges support from a Berkowitz Postdoctoral Fellowship. JG is funded by the National Institute of Health through DS-I Africa U54 TW012043-01 and Bridge2AI OT2OD032701. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Nasim Abdollahi, Seyed Ali Madani Tonekaboni, Jay Huang, Bo Wang, and Stephen MacKinnon. Nodecoder: a graph-based machine learning platform to predict active sites of modeled protein structures, 2023.   \n[2] Avraham Adler. Using machine learning techniques to identify key risk factors for diabetes and undiagnosed diabetes, 2021.   \n[3] B Thomas Adler, Luca De Alfaro, Santiago M Mola-Velasco, Paolo Rosso, and Andrew G West. Wikipedia vandalism detection: Combining natural language, metadata, and reputation features. In Computational Linguistics and Intelligent Text Processing: 12th International Conference, CICLing 2011, Tokyo, Japan, February 20-26, 2011. Proceedings, Part II 12, pages 277\u2013288. Springer, 2011.   \n[4] Sergey Afanasiev, Anastasiya Smirnova, and Diana Kotereva. Itsy bitsy spidernet: Fully connected residual network for fraud detection, 2021.   \n[5] Karan Aggarwal, Onur Atan, Ahmed K Farahat, Chi Zhang, Kosta Ristovski, and Chetan Gupta. Two birds with one network: Unifying failure event prediction and time-to-failure modeling. In 2018 IEEE international conference on big data (Big Data), pages 1308\u20131317. IEEE, 2018.   \n[6] Faruk Ahmed and Aaron Courville. Detecting semantic anomalies. Proceedings of the AAAI Conference on Artificial Intelligence, 34(04):3154\u20133162, Apr. 2020.   \n[7] Sajid Ahmed, Farshid Rayhan, Asif Mahbub, Md Rafsan Jani, Swakkhar Shatabda, and Dewan Md Farid. Liuboost: locality informed under-boosting for imbalanced data classification. In Emerging Technologies in Data Mining and Information Security: Proceedings of IEMIS 2018, Volume 2, pages 133\u2013144. Springer, 2019.   \n[8] Giambattista Albora and Andrea Zaccaria. Machine learning to assess relatedness: the advantage of using firm-level data. Complexity, 2022, 2022.   \n[9] Jr. Allam, Tarek and Jason D McEwen. Paying attention to astronomical transients: introducing the time-series transformer for photometric classification. RAS Techniques and Instruments, 3(1):209\u2013223, 10 2023.   \n[10] Maxime Alvarez, Jean-Charles Verdier, D\u2019Jeff K. Nkashama, Marc Frappier, Pierre-Martin Tardif, and Froduald Kabanza. A revealing large-scale evaluation of unsupervised anomaly detection algorithms, 2022.   \n[11] Ilya Amburg, Jon Kleinberg, and Austin R Benson. Planted hitting set recovery in hypergraphs. Journal of Physics: Complexity, 2(3):035004, 2021.   \n[12] Shin Ando and Chun Yuan Huang. Deep over-sampling framework for classifying imbalanced data. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2017, Skopje, Macedonia, September 18\u201322, 2017, Proceedings, Part I 10, pages 770\u2013785. Springer, 2017.   \n[13] Renan Andrades and Mariana Recamonde-Mendoza. Machine learning methods for prediction of cancer driver genes: a survey paper. Briefings in Bioinformatics, 23(3):bbac062, 2022.   \n[14] Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. Machine bias. In Ethics of data and analytics, pages 254\u2013264. Auerbach Publications, 2022.   \n[15] Nino Antulov-Fantulin, Dijana Tolic, Matija Piskorec, Zhang Ce, and Irena Vodenska. Inferring short-term volatility indicators from the bitcoin blockchain. In Luca Maria Aiello, Chantal Cherif,i Hocine Cherif,i Renaud Lambiotte, Pietro Li\u00f3, and Luis M. Rocha, editors, Complex Networks and Their Applications VII, pages 508\u2013520, Cham, 2019. Springer International Publishing.   \n[16] Bruno Arcanjo, Bruno Ferrarini, Michael Milford, Klaus D McDonald-Maier, and Shoaib Ehsan. An efficient and scalable collection of fly-inspired voting units for visual place recognition in changing environments. IEEE Robotics and Automation Letters, 7(2):2527\u2013 2534, 2022.   \n[17] Arthur Asuncion and David Newman. Uci machine learning repository, 2007.   \n[18] Sezin Kircali Ata, Yuan Fang, Min Wu, Jiaqi Shi, Chee Keong Kwoh, and Xiaoli Li. Multiview collaborative network embedding. ACM Transactions on Knowledge Discovery from Data, 15(3):1\u201318, April 2021.   \n[19] Anand Avati, Kenneth Jung, Stephanie Harman, Lance Downing, Andrew Ng, and Nigam H Shah. Improving palliative care with deep learning. BMC medical informatics and decision making, 18(4):55\u201364, 2018.   \n[20] Kleanthis Avramidis, Shanti Stewart, and Shrikanth Narayanan. On the role of visual context in enriching music representations. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1\u20135. IEEE, 2023.   \n[21] Simon Axelrod and Rafael Gomez-Bombarelli. Molecular machine learning with conformer ensembles. Machine Learning: Science and Technology, 4(3):035025, 2023.   \n[22] Kasra Babaei, Zhi Yuan Chen, and Tomas Maul. Aegr: a simple approach to gradient reversal in autoencoders for network anomaly detection. Soft Computing, 25(24):15269\u201315280, 2021.   \n[23] Kasra Babaei, ZhiYuan Chen, and Tomas Maul. Data augmentation by autoencoders for unsupervised anomaly detection, 2019.   \n[24] Van Bach Nguyen, Kanishka Ghosh Dastidar, Michael Granitzer, and Wissam Siblini. The importance of future information in credit card fraud detection. In Gustau Camps-Valls, Francisco J. R. Ruiz, and Isabel Valera, editors, Proceedings of The 25th International Conference on Artificial Intelligence and Statistics, volume 151 of Proceedings of Machine Learning Research, pages 10067\u201310077. PMLR, 28\u201330 Mar 2022.   \n[25] Bart Baesens, Sebastiaan H\u00a8\"oppner, Irene Ortner, and Tim Verdonck. robrose: A robust approach for dealing with imbalanced data in fraud detection. Statistical Methods & Applications, 30(3):841\u2013861, 2021.   \n[26] Leen De Baets, Joeri Ruyssinck, Thomas Peiffer, Johan Decruyenaere, Filip De Turck, Femke Ongenae, and Tom Dhaene. Positive blood culture detection in time series data using a bilstm network, 2016.   \n[27] Vassileios Balntas, Karel Lenc, Andrea Vedaldi, and Krystian Mikolajczyk. Hpatches: A benchmark and evaluation of handcrafted and learned local descriptors. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5173\u20135182, 2017.   \n[28] Amitava Banerjee, Joseph D Hart, Rajarshi Roy, and Edward Ott. Machine learning link inference of noisy delay-coupled networks with optoelectronic experimental tests. Physical Review X, 11(3):031014, 2021.   \n[29] Batuhan Bardak and Mehmet Tan. Using clinical drug representations for improving mortality and length of stay predictions. In 2021 IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB), pages 1\u20138. IEEE, 2021.   \n[30] Ioannis Bargiotas, Argyris Kalogeratos, Myrto Limnios, Pierre-Paul Vidal, Damien Ricard, and Nicolas Vayatis. Revealing posturographic proflie of patients with parkinsonian syndromes through a novel hypothesis testing framework based on machine learning. PLOS ONE, 16(2):1\u2013 22, 02 2021.   \n[31] Jens Bayer, David M\u00fcnch, and Michael Arens. Image-based out-of-distribution-detector principles on graph-based input data in human action recognition. In Pattern Recognition. ICPR International Workshops and Challenges: Virtual Event, January 10\u201315, 2021, Proceedings, Part I, page 26\u201340, Berlin, Heidelberg, 2021. Springer-Verlag.   \n[32] Neslihan Bayramoglu, Miika T Nieminen, and Simo Saarakkala. Automated detection of patellofemoral osteoarthritis from knee lateral view radiographs using deep learning: data from the multicenter osteoarthritis study (most). Osteoarthritis and Cartilage, 29(10):1432\u20131447, 2021.   \n[33] Andreas Beger. Precision-recall curves, April 2016.   \n[34] Hafida Benhidour, Lama Almeshkhas, and Said Kerrache. An approach for link prediction in directed complex networks based on asymmetric similarity-popularity, 2022.   \n[35] Austin R Benson and Jon Kleinberg. Found graph data and planted vertex covers. Advances in Neural Information Processing Systems, 31, 2018.   \n[36] James Bergstra, R\u00e9mi Bardenet, Yoshua Bengio, and Bal\u00e1zs K\u00e9gl. Algorithms for hyperparameter optimization. Advances in neural information processing systems, 24, 2011.   \n[37] James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal of machine learning research, 13(2), 2012.   \n[38] Victor Besnier, Andrei Bursuc, David Picard, and Alexandre Briot. Triggering failures: Outof-distribution detection by learning from local adversarial attacks in semantic segmentation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 15701\u2013 15710, 2021.   \n[39] Debayan Bhattacharya, Benjamin Tobias Becker, Finn Behrendt, Marcel Bengs, Dirk Beyersdorff, Dennis Eggert, Elina Petersen, Florian Jansen, Marvin Petersen, Bastian Cheng, et al. Supervised contrastive learning to classify paranasal anomalies in the maxillary sinus. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 429\u2013438. Springer, 2022.   \n[40] Kevin Bleakley, G\u00e9rard Biau, and Jean-Philippe Vert. Supervised reconstruction of biological networks with local models. Bioinformatics, 23(13):i57\u2013i65, 2007.   \n[41] D Blevins, P Moriano, R Bridges, M Verma, M Iannacone, and S Hollifield. Time-based can intrusion detection benchmark. In Workshop on Automotive and Autonomous Vehicle Security (AutoSec), 2021.   \n[42] Nathaniel J Bloomfield, Susan Wei, Bartholomew A. Woodham, Peter Wilkinson, and Andrew P Robinson. Automating the assessment of biofouling in images using expert agreement as a gold standard. Scientific reports, 11(1):2739, 2021.   \n[43] Hermann Blum, Paul-Edouard Sarlin, Juan Nieto, Roland Siegwart, and Cesar Cadena. The fishyscapes benchmark: Measuring blind spots in semantic segmentation. International Journal of Computer Vision, 129(11):3119\u20133135, 2021.   \n[44] Ali Borji, Ming-Ming Cheng, Huaizu Jiang, and Jia Li. Salient object detection: A benchmark. IEEE transactions on image processing, 24(12):5706\u20135722, 2015.   \n[45] Kendrick Boyd, Vitor Santos Costa, Jesse Davis, and C David Page. Unachievable region in precision-recall space and its effect on empirical evaluation. In Proceedings of the... International Conference on Machine Learning. International Conference on Machine Learning, volume 2012, page 349. NIH Public Access, 2012.   \n[46] Kendrick Boyd, Kevin H Eng, and C David Page. Area under the precision-recall curve: point estimates and confidence intervals. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2013, Prague, Czech Republic, September 23-27, 2013, Proceedings, Part III 13, pages 451\u2013466. Springer, 2013.   \n[47] Jakob Bo\u017eic\u02c7, Domen Tabernik, and Danijel Skoc\u02c7aj. End-to-end training of a two-stage neural network for defect detection. In 2020 25th International conference on pattern recognition (ICPR), pages 5619\u20135626. IEEE, 2021.   \n[48] Jan Brabec, Tom\u00e1\u0161 Kom\u00e1rek, Vojte\u02c7ch Franc, and Luk\u00e1\u0161 Machlica. On model evaluation under non-constant class imbalance. In Computational Science\u2013ICCS 2020: 20th International Conference, Amsterdam, The Netherlands, June 3\u20135, 2020, Proceedings, Part IV 20, pages 74\u201387. Springer, 2020.   \n[49] Jonathan Brophy and Daniel Lowd. Eggs: A flexible approach to relational modeling of social network spam, 2020.   \n[50] Nicolas Brosse, Carlos Riquelme, Alice Martin, Sylvain Gelly, and \u00c9ric Moulines. On lastlayer algorithms for classification: Decoupling representation from uncertainty estimation, 2020.   \n[51] Jason Brownlee. Tour of evaluation metrics for imbalanced classification, January 2020.   \n[52] Jonathan Bryan and Pablo Moriano. Graph-based machine learning improves just-in-time defect prediction. Plos one, 18(4):e0284077, 2023.   \n[53] C\u00e9line Budding, Fabian Eitel, Kerstin Ritter, and Stefan Haufe. Evaluating saliency methods on artificial data with different background types, 2021.   \n[54] Marcin Budka, Akanda Wahid Ul Ashraf, Matthew Bennett, Scott Neville, and Alun Mackrill. Deep multilabel cnn for forensic footwear impression descriptor identification. Applied Soft Computing, 109:107496, 2021.   \n[55] Tian Cai, Li Xie, Muge Chen, Yang Liu, Di He, Shuo Zhang, Cameron Mura, Philip E. Bourne, and Lei Xie. Exploration of dark chemical genomics space via portal learning: Applied to targeting the undruggable genome and covid-19 anti-infective polypharmacology, 2021.   \n[56] Chen Chai, Juanwu Lu, Xuan Jiang, Xiupeng Shi, and Zeng Zeng. An automated machine learning (automl) method for driving distraction detection based on lane-keeping performance. arXiv preprint arXiv:2103.08311, 2021.   \n[57] Satrajit Chakrabarty, Pamela LaMontagne, Joshua Shimony, Daniel S Marcus, and Aristeidis Sotiras. Mri-based classification of idh mutation and $1{\\mathrm{p}}/19{\\mathrm{q}}$ codeletion status of gliomas using a 2.5 d hybrid multi-task convolutional neural network. Neuro-Oncology Advances, 5(1):vdad023, 2023.   \n[58] Neeloy Chakraborty, Aamir Hasan, Shuijing Liu, Tianchen Ji, Weihang Liang, D. Livingston McPherson, and Katherine Driggs-Campbell. Structural attention-based recurrent variational autoencoder for highway vehicle anomaly detection. In Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems, AAMAS \u201923, page 1125\u20131134, Richland, SC, 2023. International Foundation for Autonomous Agents and Multiagent Systems.   \n[59] Saptarshi Chakraborty, Colin B Begg, and Ronglai Shen. Using the \u201chidden\u201d genome to improve classification of cancer types. Biometrics, 77(4):1445\u20131455, 2021.   \n[60] Saptarshi Chakraborty, Zoe Guan, Colin B Begg, and Ronglai Shen. Topical hidden genome: Discovering latent cancer mutational topics using a bayesian multilevel context-learning approach. arXiv preprint arXiv:2212.14567, 2022.   \n[61] Raghavendra Chalapathy, Aditya Krishna Menon, and Sanjay Chawla. Robust, deep and inductive anomaly detection. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2017, Skopje, Macedonia, September 18\u201322, 2017, Proceedings, Part I 10, pages 36\u201351. Springer, 2017.   \n[62] Robin Chan, Matthias Rottmann, and Hanno Gottschalk. Entropy maximization and meta classification for out-of-distribution detection in semantic segmentation. In Proceedings of the ieee/cvf international conference on computer vision, pages 5128\u20135137, 2021.   \n[63] Hugh Chen, Scott Lundberg, and Su-In Lee. Checkpoint ensembles: Ensemble methods from a single training process, 2017.   \n[64] Hugh Chen, Scott Lundberg, and Su-In Lee. Hybrid gradient boosting trees and neural networks for forecasting operating room data, 2018.   \n[65] Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining, pages 785\u2013794, 2016.   \n[66] Yu Chen, Jiaqi Jin, Hui Zhao, Pengjie Wang, Guojun Liu, Jian Xu, and Bo Zheng. Asymptotically unbiased estimation for delayed feedback modeling via label correction. In Proceedings of the ACM Web Conference 2022, pages 369\u2013379, 2022.   \n[67] Zhuohao Chen, James Gibson, Ming-Chang Chiu, Qiaohong Hu, Tara K Knight, Daniella Meeker, James A Tulsky, Kathryn I Pollak, and Shrikanth Narayanan. Automated empathy detection for oncology encounters. In 2020 IEEE International Conference on Healthcare Informatics (ICHI), pages 1\u20138. IEEE, 2020.   \n[68] Davide Chicco. Ten quick tips for machine learning in computational biology. BioData mining, 10(1):35, 2017.   \n[69] Julien Chiquet, Stephane Robin, and Mahendra Mariadassou. Variational inference for sparse network reconstruction from count data. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 1162\u20131171. PMLR, 09\u201315 Jun 2019.   \n[70] Brian Y Cho, Tucker Hermans, and Alan Kuntz. Planning sensing sequences for subsurface 3d tumor mapping. In 2021 International Symposium on Medical Robotics (ISMR), pages 1\u20137. IEEE, 2021.   \n[71] Edward Choi, Cao Xiao, Walter F. Stewart, and Jimeng Sun. Mime: Multilevel medical embedding of electronic health records for predictive healthcare. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS\u201918, page 4552\u20134562, Red Hook, NY, USA, 2018. Curran Associates Inc.   \n[72] Eunji Chong, Katha Chanda, Zhefan Ye, Audrey Southerland, Nataniel Ruiz, Rebecca M. Jones, Agata Rozga, and James M. Rehg. Detecting gaze towards eyes in natural social interactions and its use in child assessment. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol., 1(3), sep 2017.   \n[73] Peter Christen. Application of advanced record linkage techniques for complex population reconstruction. arXiv preprint arXiv:1612.04286, 2016.   \n[74] Xu Chu, Yang Lin, Jingyue Gao, Jiangtao Wang, Yasha Wang, and Leye Wang. Multi-label robust factorization autoencoder and its application in predicting drug-drug interactions, 2018.   \n[75] Marc Claesen, Frank De Smet, Johan A.K. Suykens, and Bart De Moor. A robust ensemble approach to learn from positive and unlabeled data using svm base models. Neurocomputing, 160:73\u201384, July 2015.   \n[76] Johanna C Clauser, Judith Maas, Jutta Arens, Thomas Schmitz-Rode, Ulrich Steinseifer, and Benjamin Berkels. Automation of hemocompatibility analysis using image segmentation and a random forest. arXiv preprint arXiv:2010.06245, 2020.   \n[77] Felipe Colombelli, Thayne Woycinck Kowalski, and Mariana Recamonde-Mendoza. A hybrid ensemble feature selection design for candidate biomarkers discovery from transcriptome profiles. Knowledge-Based Systems, 254:109655, 2022.   \n[78] Jonathan Cook and Vikram Ramadas. When to consult precision-recall curves. The Stata Journal, 20(1):131\u2013148, 2020.   \n[79] Skyler J. Cranmer and Bruce A. Desmarais. What can we learn from predictive modeling?, 2016.   \n[80] Jakub Czakon. F1 score vs roc auc vs accuracy vs pr auc: Which evaluation metric should you choose?, July 2022.   \n[81] Anusha Damodaran, Fabio Di Troia, Corrado Aaron Visaggio, Thomas H Austin, and Mark Stamp. A comparison of static, dynamic, and hybrid analysis for malware detection. Journal of Computer Virology and Hacking Techniques, 13:1\u201312, 2017.   \n[82] Armin Danesh Pazho, Ghazal Alinezhad Noghre, Babak Rahimi Ardabili, Christopher Neff, and Hamed Tabkhi. CHAD: Charlotte Anomaly Dataset, page 50\u201366. Springer Nature Switzerland, 2023.   \n[83] Jesse Davis and Mark Goadrich. The relationship between precision-recall and roc curves. In Proceedings of the 23rd International Conference on Machine Learning, ICML \u201906, page 233\u2013240, New York, NY, USA, 2006. Association for Computing Machinery.   \n[84] Alice Del Vecchio, Andreea Deac, Pietro Li\u00f2, and Petar Velic\u02c7kovic\u00b4. Neural message passing for joint paratope-epitope prediction. arXiv preprint arXiv:2106.00757, 2021.   \n[85] Jianyuan Deng, Zhibo Yang, Hehe Wang, Iwao Ojima, Dimitris Samaras, and Fusheng Wang. Unraveling key elements underlying molecular property prediction: A systematic study, 2023.   \n[86] Amit G. Deshwar, Shankar Vembu, Christina K. Yung, Gun Ho Jang, Lincoln Stein, and Quaid Morris. Reconstructing subclonal composition and evolution from whole genome sequencing of tumors, 2015.   \n[87] Giancarlo Di Biase, Hermann Blum, Roland Siegwart, and Cesar Cadena. Pixel-wise anomaly detection in complex driving scenes. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16918\u201316927, 2021. [88] Daisy Yi Ding, Chlo\u00e9 Simpson, Stephen Pfohl, Dave C Kale, Kenneth Jung, and Nigam H Shah. The effectiveness of multitask learning for phenotyping with electronic health records data. In BIOCOMPUTING 2019: Proceedings of the Pacific Symposium, pages 18\u201329. World Scientific, 2018. [89] Jonas C. Ditz, Bernhard Reuter, and Nico Pfeifer. Convolutional motif kernel networks, 2023. [90] R\u00e9mi Domingues, Pietro Michiardi, J\u00e9r\u00e9mie Barlet, and Maurizio Filippone. A comparative evaluation of novelty detection algorithms for discrete sequences. Artificial Intelligence Review, 53:3787\u20133812, 2020. [91] Shangjia Dong, Tianbo Yu, Hamed Farahmand, and Ali Mostafavi. A hybrid deep learning model for predictive flood warning and situation awareness using channel network sensors data. Computer-Aided Civil and Infrastructure Engineering, 36(4):402\u2013420, 2021. [92] Wen Dong, Tong Guan, Bruno Lepri, and Chunming Qiao. Pocketcare: Tracking the flu with mobile phones using partial observations of proximity and symptoms. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 3(2):1\u201323, 2019.   \n[93] Julia Dressel and Hany Farid. The accuracy, fairness, and limits of predicting recidivism. Science advances, 4(1):eaao5580, 2018. [94] Guodong Du, Liang Yuan, Kong Joo Shin, and Shunsuke Managi. Enhancement of land-use change modeling using convolutional neural networks and convolutional denoising autoencoders, 2018. [95] Marco Due\u00f1as, V\u00edctor Ortiz, Massimo Riccaboni, and Francesco Serti. Assessing the Impact of COVID-19 on Trade: a Machine Learning Counterfactual Analysis. Working papers 79, Red Investigadores de Econom\u00eda, April 2021. [96] Witold Dyrka, Mateusz Pyzik, Fran\u00e7ois Coste, and Hugo Talibart. Estimating probabilistic context-free grammars for proteins using contact map constraints. PeerJ, 7:e6559, March 2019. [97] Gabriel Erion, Hugh Chen, Scott M. Lundberg, and Su-In Lee. Anesthesiologist-level forecasting of hypoxemia with only spo2 data using deep learning, 2017. [98] \u00b8Sirag Erkol, Satyaki Sikdar, Filippo Radicchi, and Santo Fortunato. Consistency pays off in science. Quantitative Science Studies, 4(2):491\u2013500, 2023. [99] Alessandro Fabris, Stefano Messina, Gianmaria Silvello, and Gian Antonio Susto. Algorithmic fairness datasets: the story so far. Data Mining and Knowledge Discovery, 36(6):2074\u20132152, 2022.   \n[100] Stefan Falkner, Aaron Klein, and Frank Hutter. Bohb: Robust and efficient hyperparameter optimization at scale. In International conference on machine learning, pages 1437\u20131446. PMLR, 2018.   \n[101] Yang Feng, Min Zhou, and Xin Tong. Imbalanced classification: A paradigm-based review. Statistical Analysis and Data Mining: The ASA Data Science Journal, 14(5):383\u2013406, 2021.   \n[102] Alberto Fern\u00e1ndez, Salvador Garc\u00eda, Mikel Galar, Ronaldo C Prati, Bartosz Krawczyk, and Francisco Herrera. Learning from imbalanced data sets, volume 10. Springer, 2018.   \n[103] Andres Ferraro, Dmitry Bogdanov, Xavier Serra Jay, Ho Jeon, and Jason Yoon. How low can you go? reducing frequency and time resolution in current cnn architectures for music autotagging. In 2020 28th European signal processing conference (EUSIPCO), pages 131\u2013135. IEEE, 2021.   \n[104] Peter Flach and Meelis Kull. Precision-recall-gain curves: Pr analysis done right. Advances in neural information processing systems, 28, 2015.   \n[105] Carlos Floyd, Herbert Levine, Christopher Jarzynski, and Garegin A Papoian. Understanding cytoskeletal avalanches using mechanical stability analysis. Proceedings of the National Academy of Sciences, 118(41):e2110239118, 2021.   \n[106] Marina Fomicheva, Piyawat Lertvittayakumjorn, Wei Zhao, Steffen Eger, and Yang Gao. The eval4nlp shared task on explainable quality estimation: Overview and results. In Yang Gao, Steffen Eger, Wei Zhao, Piyawat Lertvittayakumjorn, and Marina Fomicheva, editors, Proceedings of the 2nd Workshop on Evaluation and Comparison of NLP Systems, pages 165\u2013178, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.   \n[107] Eduardo Fonseca, Xavier Favory, Jordi Pons, Frederic Font, and Xavier Serra. Fsd50k: An open dataset of human-labeled sound events. IEEE/ACM Trans. Audio, Speech and Lang. Proc., 30:829\u2013852, dec 2021.   \n[108] Fabrizio Frasca, Diego Galeano, Guadalupe Gonzalez, Ivan Laponogov, Kirill Veselkov, Alberto Paccanaro, and Michael M. Bronstein. Learning interpretable disease self-representations for drug repositioning, 2019.   \n[109] Marco Frasca and Nicolo Cesa Bianchi. Multitask protein function prediction through task dissimilarity. IEEE/ACM transactions on computational biology and bioinformatics, 16(5):1550\u2013 1560, 2017.   \n[110] Marco Frasca and Nicol\u00f2 Cesa-Bianchi. Positive and unlabeled learning through negative selection and imbalance-aware classification, 2019.   \n[111] Yuming Fu, Xue-Bing Wu, Qian Yang, Anthony GA Brown, Xiaotong Feng, Qinchun Ma, and Shuyan Li. Finding quasars behind the galactic plane. i. candidate selections with transfer learning. The Astrophysical Journal Supplement Series, 254(1):6, 2021.   \n[112] Silvio Galesso, Maria Alejandra Bravo, Mehdi Naouar, and Thomas Brox. Probing contextual diversity for dense out-of-distribution detection, 2022.   \n[113] Varun Gangal, Abhinav Arora, Arash Einolghozati, and Sonal Gupta. Likelihood ratios and generative classifiers for unsupervised out-of-domain detection in task oriented dialog. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):7764\u20137771, Apr. 2020.   \n[114] Dario Garcia-Gasulla, Eduard Ayguad\u00e9, Jes\u00fas Labarta, Ulises Cort\u00e9s, and Toyotaro Suzumura. Hierarchical hyperlink prediction for the www, 2016.   \n[115] Matthieu Garcin and Samuel St\u00e9phan. Credit scoring using neural networks and sure posterior probability calibration, 2021.   \n[116] Jean-Gabriel Gaudreault, Paula Branco, and Jo\u00e3o Gama. An analysis of performance metrics for imbalanced classification. In International Conference on Discovery Science, pages 67\u201377. Springer, 2021.   \n[117] Sachin Gavali, Chuming Chen, Julie Cowart, Xi Peng, Shanshan Ding, Cathy Wu, and Tammy Anderson. Understanding the factors related to the opioid epidemic using machine learning. In 2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), pages 1309\u20131314. IEEE, 2021.   \n[118] Isaac D Gerg and Vishal Monga. Structural prior driven regularized deep learning for sonar image classification. IEEE Transactions on Geoscience and Remote Sensing, 60:1\u201316, 2021.   \n[119] Aur\u00e9lien G\u00e9ron. Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow. \" O\u2019Reilly Media, Inc.\", 2022.   \n[120] Djordje Gligorijevic, Jelena Gligorijevic, and Aaron Flores. Prospective modeling of users for online display advertising via deep time-aware model. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management, CIKM \u201920, page 2461\u20132468, New York, NY, USA, 2020. Association for Computing Machinery.   \n[121] Mark Goadrich, Louis Oliphant, and Jude Shavlik. Gleaner: Creating ensembles of first-order clauses to improve recall-precision curves. Machine Learning, 64:231\u2013261, 2006.   \n[122] Izhak Golan and Ran El-Yaniv. Deep anomaly detection using geometric transformations. Advances in neural information processing systems, 31, 2018.   \n[123] Samuel Goldman, Ria Das, Kevin K. Yang, and Connor W. Coley. Machine learning modeling of family wide enzyme-substrate specificity screens. PLOS Computational Biology, 18(2):e1009853, February 2022.   \n[124] Hongyu Gong, Alberto Valido, Katherine M Ingram, Giulia Fanti, Suma Bhat, and Dorothy L Espelage. Abusive language detection in heterogeneous contexts: Dataset collection and the role of supervised attention. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 14804\u201314812, 2021.   \n[125] Yuan Gong, Yu-An Chung, and James Glass. Psla: Improving audio tagging with pretraining, sampling, labeling, and aggregation. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29:3292\u20133306, 2021.   \n[126] Eric Goodman, Chase Zimmerman, and Corey Hudson. Packet2vec: Utilizing word2vec for feature extraction in packet data. Technical report, Sandia National Lab.(SNL-NM), Albuquerque, NM (United States); Sandia ..., 2019.   \n[127] Julia Sophia Gottfriedsen, Max Berrendorf, Pierre Gentine, Birgit Hassler, Markus Reichstein, Katja Weigel, and Veronika Eyring. On the generalization of agricultural drought classification from climate data. thirty-fifth conference on neural information processing systems (neurips) workshop 2021\"\" tackling climate change with machine learning\"\". 2021.   \n[128] Anil Goyal and Jihed Khiari. Diversity-aware weighted majority vote classifier for imbalanced data. In 2020 International Joint Conference on Neural Networks (IJCNN), pages 1\u20138, 2020.   \n[129] Eric Granger, Madhu Kiran, Louis-Antoine Blais-Morin, et al. A comparison of cnn-based face and head detectors for real-time video surveillance applications. In 2017 seventh international conference on image processing theory, tools and applications (IPTA), pages 1\u20137. IEEE, 2017.   \n[130] Zoe Guan, Giovanni Parmigiani, Danielle Braun, and Lorenzo Trippa. Prediction of hereditary cancers using neural networks. The annals of applied statistics, 16(1):495, 2022.   \n[131] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. arXiv preprint arXiv:2007.01434, 2020.   \n[132] Ekaterina Gurina, Nikita Klyuchnikov, Alexey Zaytsev, Evgenya Romanenkova, Ksenia Antipova, Igor Simon, Victor Makarov, and Dmitry Koroteev. Application of machine learning to accidents detection at directional drilling. Journal of Petroleum Science and Engineering, 184:106519, 2020.   \n[133] Guillaume Haben, Sarra Habchi, Mike Papadakis, Maxime Cordy, and Yves Le Traon. Discerning legitimate failures from false alerts: A study of chromium\u2019s continuous integration, 2021.   \n[134] Tom Rolandus Hagedoorn and Gerasimos Spanakis. Massive open online courses temporal profiling for dropout prediction. In 2017 IEEE 29th International Conference on Tools with Artificial Intelligence (ICTAI), pages 231\u2013238. IEEE, 2017.   \n[135] Alexander H\"\u00a8agele, Jonas Rothfuss, Lars Lorch, Vignesh Ram Somnath, Bernhard Sch\"\u00a8olkopf, and Andreas Krause. Bacadi: Bayesian causal discovery with unknown interventions. In International Conference on Artificial Intelligence and Statistics, pages 1411\u20131436. PMLR, 2023.   \n[136] Melissa Hall, Bobbie Chern, Laura Gustafson, Denisse Ventura, Harshad Kulkarni, Candace Ross, and Nicolas Usunier. Towards reliable assessments of demographic disparities in multi-label image classifiers, 2023.   \n[137] Jacob A. Harer, Louis Y. Kim, Rebecca L. Russell, Onur Ozdemir, Leonard R. Kosta, Akshay Rangamani, Lei H. Hamilton, Gabriel I. Centeno, Jonathan R. Key, Paul M. Ellingwood, Erik Antelman, Alan Mackay, Marc W. McConley, Jeffrey M. Opper, Peter Chin, and Tomo Lazovich. Automated software vulnerability detection with machine learning, 2018.   \n[138] Hrayr Harutyunyan, Hrant Khachatrian, David C Kale, Greg Ver Steeg, and Aram Galstyan. Multitask learning and benchmarking with clinical time series data. Scientific data, 6(1):96, 2019.   \n[139] Hrayr Harutyunyan, Hrant Khachatrian, David C. Kale, Greg Ver Steeg, and Aram Galstyan. Multitask learning and benchmarking with clinical time series data. Scientific Data, 6(1), June 2019.   \n[140] Seyed Raein Hashemi, Seyed Sadegh Mohseni Salehi, Deniz Erdogmus, Sanjay P Prabhu, Simon K Warfield, and Ali Gholipour. Asymmetric loss functions and deep densely-connected networks for highly-imbalanced medical image segmentation: Application to multiple sclerosis lesion detection. IEEE Access, 7:1721\u20131735, 2018.   \n[141] Haibo He and Yunqian Ma. Imbalanced learning: foundations, algorithms, and applications. 2013.   \n[142] Jieyue He, Xinxing Yang, Zhuo Gong, and lbrahim Zamit. Hybrid attentional memory network for computational drug repositioning. BMC bioinformatics, 21:1\u201317, 2020.   \n[143] J\u00f6rn Hees, Dayananda Herurkar, and Mario Meier. Recol: Reconstruction error columns for outlier detection, 2021.   \n[144] Dan Hendrycks, Steven Basart, Mantas Mazeika, Andy Zou, Joseph Kwon, Mohammadreza Mostajabi, Jacob Steinhardt, and Dawn Song. Scaling out-of-distribution detection for realworld settings. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 8759\u20138773. PMLR, 17\u201323 Jul 2022.   \n[145] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-ofdistribution examples in neural networks. In International Conference on Learning Representations, 2017.   \n[146] Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich. Deep anomaly detection with outlier exposure. In International Conference on Learning Representations, 2019.   \n[147] Jens Henriksson, Christian Berger, Markus Borg, Lars Tornberg, Cristofer Englund, Sankar Raman Sathyamoorthy, and Stig Ursing. Towards structured evaluation of deep neural network supervisors. In 2019 IEEE International Conference On Artificial Intelligence Testing (AITest), pages 27\u201334. IEEE, 2019.   \n[148] Ulysse Herbach. Gene regulatory network inference from single-cell data using a selfconsistent proteomic field, 2021.   \n[149] Justus I. Hibshman and Tim Weninger. Inherent limits on topology-based link prediction, 2023.   \n[150] Steven A Hicks, Inga Str\u00fcmke, Vajira Thambawita, Malek Hammou, Michael A Riegler, P\u00e5l Halvorsen, and Sravanthi Parasa. On evaluation metrics for medical applications of artificial intelligence. Scientific reports, 12(1):5979, 2022.   \n[151] Anna Himmelhuber, Mitchell Joblin, Martin Ringsquandl, and Thomas Runkler. Demystifying graph neural network explanations. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 67\u201375. Springer, 2021.   \n[152] David Hin, Andrey Kan, Huaming Chen, and M Ali Babar. Linevd: Statement-level vulnerability detection using graph neural networks. In Proceedings of the 19th international conference on mining software repositories, pages 596\u2013607, 2022.   \n[153] Kaoutar Daoud Hiri, Matja\u017e Hren, and Toma\u017e Curk. Nlp-based classification of software tools for metagenomics sequencing data analysis into edam semantic annotation, 2022.   \n[154] Ryohei Hisano, Didier Sornette, and Takayuki Mizuno. Prediction of esg compliance using a heterogeneous information network. Journal of Big Data, 7(1), March 2020.   \n[155] Charmgil Hong and Milos Hauskrecht. Mcode: Multivariate conditional outlier detection. arXiv preprint arXiv:1505.04097, 2015.   \n[156] Shenda Hong, Cao Xiao, Trong Nghia Hoang, Tengfei Ma, Hongyan Li, and Jimeng Sun. Rdpd: Rich data helps poor data via imitation. In 28th International Joint Conference on Artificial Intelligence, IJCAI 2019, pages 5895\u20135901. International Joint Conferences on Artificial Intelligence, 2019.   \n[157] Shenda Hong, Cao Xiao, Tengfei Ma, Hongyan Li, and Jimeng Sun. Mina: Multilevel knowledge-guided attention for modeling electrocardiography signals. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19, pages 5888\u20135894. International Joint Conferences on Artificial Intelligence Organization, 7 2019.   \n[158] Julia Hornauer and Vasileios Belagiannis. Heatmap-based out-of-distribution detection. In 2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). IEEE, January 2023.   \n[159] Chao-Chun Hsu, Shantanu Karnwal, Sendhil Mullainathan, Ziad Obermeyer, and Chenhao Tan. Characterizing the value of information in medical notes. In Trevor Cohn, Yulan He, and Yang Liu, editors, Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2062\u20132072, Online, November 2020. Association for Computational Linguistics.   \n[160] Stanley Bryan Z. Hua, Mandy Rickard, John Weaver, Alice Xiang, Daniel Alvarez, Kyla N. Velear, Kunj Sheth, Gregory E. Tasian, Armando J. Lorenzo, Anna Goldenberg, and Lauren Erdman. From single-visit to multi-visit image-based models: single-visit models are enough to predict obstructive hydronephrosis. In Jorge Brieva, Pamela Guevara, Natasha Lepore, Marius G. Linguraru, Let\u00edcia Rittner, and Eduardo Romero Castro M.D., editors, 18th International Symposium on Medical Information Processing and Analysis, volume 12567, page 1256710. International Society for Optics and Photonics, SPIE, 2023.   \n[161] Yiqing Hua, Armin Namavari, Kaishuo Cheng, Mor Naaman, and Thomas Ristenpart. Increasing adversarial uncertainty to scale private similarity testing. In 31st USENIX Security Symposium (USENIX Security 22), pages 1777\u20131794, 2022.   \n[162] Benjamin Hughes and Tilo Burghardt. Automated visual fin identification of individual great white sharks. International Journal of Computer Vision, 122:542\u2013557, 2017.   \n[163] Zepeng Huo, Xiaoning Qian, Shuai Huang, Zhangyang Wang, and Bobak J Mortazavi. Densityaware personalized training for risk prediction in imbalanced medical data. In Machine Learning for Healthcare Conference, pages 101\u2013122. PMLR, 2022.   \n[164] Fantine Huot, R. Lily Hu, Nita Goyal, Tharun Sankar, Matthias Ihme, and Yi-Fan Chen. Next day wildfire spread: A machine learning dataset to predict wildfire spreading from remote-sensing data. IEEE Transactions on Geoscience and Remote Sensing, 60:1\u201313, 2022.   \n[165] V\u00e2n Anh Huynh-Thu and Guido Sanguinetti. Gene regulatory network inference: an introductory survey. Gene regulatory networks: Methods and protocols, pages 1\u201323, 2019.   \n[166] Zina M Ibrahim, Daniel Bean, Thomas Searle, Linglong Qian, Honghan Wu, Anthony Shek, Zeljko Kraljevic, James Galloway, Sam Norton, James TH Teo, et al. A knowledge distillation ensemble framework for predicting short-and long-term hospitalization outcomes from electronic health records data. IEEE Journal of Biomedical and Health Informatics, 26(1):423\u2013435, 2021.   \n[167] Yotam Intrator, Gilad Katz, and Asaf Shabtai. Mdgan: Boosting anomaly detection using multi-discriminator generative adversarial networks, 2018.   \n[168] Mary Isangediok and Kelum Gajamannage. Fraud detection using optimized machine learning tools under imbalance classes. In 2022 IEEE International Conference on Big Data (Big Data), pages 4275\u20134284. IEEE, 2022.   \n[169] Olga Isupova, Danil Kuzin, and Lyudmila Mihaylova. Learning methods for dynamic topic modeling in automated behavior analysis. IEEE transactions on neural networks and learning systems, 29(9):3980\u20133993, 2017.   \n[170] Jay Jacobs, Sasha Romanosky, Octavian Suciu, Benjamin Edwards, and Armin Sarabi. Enhancing vulnerability prioritization: Data-driven exploit predictions with community-driven insights, 2023.   \n[171] Lucas Jaffe, Michael Zelinski, and Wesam Sakla. Remote sensor design for visual recognition with convolutional neural networks. IEEE Transactions on Geoscience and Remote Sensing, 57(11):9090\u20139108, November 2019.   \n[172] Shantanu Jain, Martha White, and Predrag Radivojac. Recovering true classifier performance in positive-unlabeled learning. Proceedings of the AAAI Conference on Artificial Intelligence, 31(1), Feb. 2017.   \n[173] Dong-ju Jeong, Insung Hwang, and Nam Ik Cho. Co-salient object detection based on deep saliency networks and seed propagation over an integrated graph. IEEE Transactions on Image Processing, 27(12):5866\u20135879, 2018.   \n[174] Biaobin Jiang, Kyle Kloster, David F Gleich, and Michael Gribskov. Aptrank: an adaptive pagerank model for protein function prediction on bi-relational graphs. Bioinformatics, 33(12):1829\u20131836, 2017.   \n[175] Wei Jiang, Jing-Hao Xue, and Weichuan Yu. Estimating reproducibility in genome-wide association studies, 2015.   \n[176] Yunjiang Jiang, Yue Shang, Rui Li, Wen-Yun Yang, Guoyu Tang, Chaoyi Ma, Yun Xiao, and Eric Zhao. A unified neural network approach to e-commerce relevance learning. In Proceedings of the 1st International Workshop on Deep Learning Practice for High-Dimensional Sparse Data, pages 1\u20137, 2019.   \n[177] Di Jin, Shuyang Gao, Seokhwan Kim, Yang Liu, and Dilek Hakkani-T\u00fcr. Towards textual out-of-domain detection without in-domain labels. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 30:1386\u20131395, 2022.   \n[178] Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. Mimic-iii, a freely accessible critical care database. Scientific data, 3(1):1\u20139, 2016.   \n[179] Eric Jonas, Monica Bobra, Vaishaal Shankar, J Todd Hoeksema, and Benjamin Recht. Flare prediction using photospheric and coronal image data. Solar Physics, 293(3):48, 2018.   \n[180] Cheng Ju, James Li, Bram Wasti, and Shengbo Guo. Semisupervised learning on heterogeneous graphs and its applications to facebook news feed, 2018.   \n[181] Sanghun Jung, Jungsoo Lee, Daehoon Gwak, Sungha Choi, and Jaegul Choo. Standardized max logits: A simple yet effective approach for identifying unexpected road obstacles in urban-scene segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15425\u201315434, 2021.   \n[182] Ruthwik R Junuthula, Kevin S Xu, and Vijay K Devabhaktuni. Evaluating link prediction accuracy in dynamic networks with added and removed edges. In 2016 IEEE international conferences on big data and cloud computing (BDCloud), social computing and networking (SocialCom), sustainable computing and communications (SustainCom)(BDCloud-SocialComSustainCom), pages 377\u2013384. IEEE, 2016.   \n[183] Georgi Karadzhov, Tom Stafford, and Andreas Vlachos. What makes you change your mind? an empirical investigation in online group decision-making conversations, 2022.   \n[184] Md Rezaul Karim, Michael Cochez, Joao Bosco Jares, Mamtaz Uddin, Oya Beyan, and Stefan Decker. Drug-drug interaction prediction based on knowledge graph embeddings and convolutional-lstm network. In Proceedings of the 10th ACM international conference on bioinformatics, computational biology and health informatics, pages 113\u2013123, 2019.   \n[185] Mostafa Karimi, Di Wu, Zhangyang Wang, and Yang Shen. Explainable deep relational networks for predicting compound\u2013protein affinities and contacts. Journal of chemical information and modeling, 61(1):46\u201366, 2020.   \n[186] Kleomenis Katevas, Katrin H\u00a8\"ansel, Richard Clegg, Ilias Leontiadis, Hamed Haddadi, and Laurissa Tokarchuk. Finding dory in the crowd: Detecting social interactions using multimodal mobile sensing. In Proceedings of the 1st Workshop on Machine Learning on Edge in Sensor Systems, pages 37\u201342, 2019.   \n[187] Michal Kazmierski, Mattea Welch, Sejin Kim, Chris McIntosh, Princess Margaret Head, Neck Cancer Group, Katrina Rey-McIntyre, Shao Hui Huang, Tirth Patel, Tony Tadic, Michael Milosevic, Fei-Fei Liu, Andrew Hope, Scott Bratman, and Benjamin Haibe-Kains. A machine learning challenge for prognostic modelling in head and neck cancer using multi-modal data, 2021.   \n[188] Said Kerrache and Hafida Benhidour. A complex network based graph embedding method for link prediction, 2022.   \n[189] Kathleen R Kerwin and Nathaniel D Bastian. Stacked generalizations in imbalanced fraud data sets using resampling methods. The Journal of Defense Modeling and Simulation, 18(3):175\u2013192, 2021.   \n[190] Refilwe Kgoadi, Chris Engelbrecht, Ian Whittingham, and Andrew Tkachenko. General classification of light curves using extreme boosting, 2019.   \n[191] Jang-Hyun Kim, Sangdoo Yun, and Hyun Oh Song. Neural relation graph: A unified framework for identifying label noise and outlier data. Advances in Neural Information Processing Systems, 36, 2024.   \n[192] Minkyung Kim, Junsik Kim, Jongmin Yu, and Jun Kyun Choi. v. In 2022 IEEE International Conference on Data Mining Workshops (ICDMW), pages 39\u201346, 2022.   \n[193] B. Ravi Kiran, Dilip Mathew Thomas, and Ranjith Parakkal. An overview of deep learning based methods for unsupervised and semi-supervised anomaly detection in videos. Journal of Imaging, 4(2), 2018.   \n[194] Ivan Kiskin, Marianne Sinka, Adam D. Cobb, Waqas Rafique, Lawrence Wang, Davide Zilli, Benjamin Gutteridge, Rinita Dam, Theodoros Marinos, Yunpeng Li, Dickson Msaky, Emmanuel Kaindoa, Gerard Killeen, Eva Herreros-Moya, Kathy J. Willis, and Stephen J. Roberts. Humbugdb: A large-scale acoustic mosquito dataset, 2021.   \n[195] Ross S. Kleiman, Paul S. Bennett, Peggy L. Peissig, Richard L. Berg, Zhaobin Kuang, Scott J. Hebbring, Michael D. Caldwell, and David Page. High-throughput machine learning from electronic health records, 2019.   \n[196] Nikita Klyuchnikov, Alexey Zaytsev, Arseniy Gruzdev, Georgiy Ovchinnikov, Ksenia Antipova, Leyla Ismailova, Ekaterina Muravleva, Evgeny Burnaev, Artyom Semenikhin, Alexey Cherepanov, et al. Data-driven model for the identification of the rock type at a drilling bit. Journal of Petroleum science and Engineering, 178:506\u2013516, 2019.   \n[197] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne David, Ian Stavness, Wei Guo, Berton Earnshaw, Imran Haque, Sara M Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. Wilds: A benchmark of in-the-wild distribution shifts. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 5637\u20135664. PMLR, 18\u201324 Jul 2021.   \n[198] Yunchuan Kong and Tianwei Yu. forgenet: a graph deep neural network model using treebased ensemble classifiers for feature graph construction. Bioinformatics, 36(11):3507\u20133515, 03 2020.   \n[199] Anna-Kathrin Kopetzki, Bertrand Charpentier, Daniel $Z^{\\mathrm{ii}}$ ugner, Sandhya Giri, and Stephan G\"\u00a8unnemann. Evaluating robustness of predictive uncertainty estimation: Are dirichlet-based models reliable? In International Conference on Machine Learning, pages 5707\u20135718. PMLR, 2021.   \n[200] Denis Krompa\u00df, Stephan Baier, and Volker Tresp. Type-constrained representation learning in knowledge graphs. In The Semantic Web-ISWC 2015: 14th International Semantic Web Conference, Bethlehem, PA, USA, October 11-15, 2015, Proceedings, Part I 14, pages 640\u2013655. Springer, 2015.   \n[201] Sofia Ira Ktena, Alykhan Tejani, Lucas Theis, Pranay Kumar Myana, Deepak Dilipkumar, Ferenc Husz\u00e1r, Steven Yoo, and Wenzhe Shi. Addressing delayed feedback for continuous training with neural networks in ctr prediction. In Proceedings of the 13th ACM Conference on Recommender Systems, RecSys \u201919, page 187\u2013195, New York, NY, USA, 2019. Association for Computing Machinery.   \n[202] Viraj Kulkarni, Manish Gawali, and Amit Kharat. Key technology considerations in developing and deploying machine learning models in clinical radiology practice. JMIR Med Inform, 9(9):e28776, Sep 2021.   \n[203] Aditya Kunar. Effective and privacy preserving tabular data synthesizing, 2021.   \n[204] Trent Kyono, Fiona J. Gilbert, and Mihaela van der Schaar. Mammo: A deep learning solution for facilitating radiologist-machine collaboration in breast cancer diagnosis, 2018.   \n[205] Preethi Lahoti, Alex Beutel, Jilin Chen, Kang Lee, Flavien Prost, Nithum Thain, Xuezhi Wang, and Ed Chi. Fairness without demographics through adversarially reweighted learning. Advances in neural information processing systems, 33:728\u2013740, 2020.   \n[206] Conrad Lee, Bobo Nick, Ulrik Brandes, and P\u00e1draig Cunningham. Link prediction with social vector clocks. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 784\u2013792, 2013.   \n[207] Hao-Chih Lee, Matteo Danieletto, Riccardo Miotto, Sarah T Cherng, and Joel T Dudley. Scaling structural learning with no-bears to infer causal transcriptome networks. In Pacific Symposium on Biocomputing 2020, pages 391\u2013402. World Scientific, 2019.   \n[208] I-Ta Lee, Manish Marwah, and Martin Arlitt. Attention-based self-supervised feature learning for security data, 2020.   \n[209] Joongoo Lee and Min-Su Shin. Estimation of photometric redshifts. ii. identification of out-of-distribution data with neural networks. The Astronomical Journal, 163(2):98, 2022.   \n[210] Jimmy Lei Ba, Kevin Swersky, Sanja Fidler, et al. Predicting deep zero-shot convolutional neural networks using textual descriptions. In Proceedings of the IEEE international conference on computer vision, pages 4247\u20134255, 2015.   \n[211] Jussi Leinonen, Ulrich Hamann, and Urs Germann. Seamless lightning nowcasting with recurrent-convolutional deep learning. Artificial Intelligence for the Earth Systems, 1(4):e220043, 2022.   \n[212] Daniel E Leisman. Rare events in the icu: an emerging challenge in classification and prediction. Critical care medicine, 46(3):418\u2013424, 2018.   \n[213] Jake Lever. Classification evaluation: It is important to understand both what a classification metric expresses and what it hides. Nature methods, 13(8):603\u2013605, 2016.   \n[214] Jingyi Jessica Li and Xin Tong. Statistical hypothesis testing versus machine learning binary classification: Distinctions and guidelines. Patterns, 1(7):100115, October 2020.   \n[215] Longyuan Li, Junchi Yan, Haiyang Wang, and Yaohui Jin. Anomaly detection of time series with smoothness-inducing sequential variational auto-encoder. IEEE transactions on neural networks and learning systems, 32(3):1177\u20131191, 2020.   \n[216] Qiujia Li, PM Ness, Anton Ragni, and Mark JF Gales. Bi-directional lattice recurrent neural networks for confidence estimation. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 6755\u20136759. IEEE, 2019.   \n[217] Qiujia Li, Yu Zhang, David Qiu, Yanzhang He, Liangliang Cao, and Philip C Woodland. Improving confidence estimation on out-of-domain data for end-to-end speech recognition. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 6537\u20136541. IEEE, 2022.   \n[218] Wenyuan Li, Yunlong Wang, Yong Cai, Corey Arnold, Emily Zhao, and Yilian Yuan. Semisupervised rare disease detection using generative adversarial network, 2018.   \n[219] Xiaoxiao Li, Rabah Al-Zaidy, Amy Zhang, Stefan Baral, Le Bao, and C. Lee Giles. Automating document classification with distant supervision to increase the efficiency of systematic reviews, 2020.   \n[220] Xiaoya Li, Jiwei Li, Xiaofei Sun, Chun Fan, Tianwei Zhang, Fei Wu, Yuxian Meng, and Jun Zhang. kfolden: $k$ -fold ensemble for out-of-distribution detection. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3102\u20133115, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.   \n[221] Yufei Li, Simin Chen, and Wei Yang. Estimating predictive uncertainty under program data distribution shift. arXiv preprint arXiv:2107.10989, 2021.   \n[222] Ryan Lichtnwalter and Nitesh V Chawla. Link prediction: fair and effective evaluation. In 2012 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, pages 376\u2013383. IEEE, 2012.   \n[223] Bryan Lim and Mihaela van der Schaar. Disease-atlas: Navigating disease trajectories using deep learning. In Machine Learning for Healthcare Conference, pages 137\u2013160. PMLR, 2018.   \n[224] Krzysztof Lis, Sina Honari, Pascal Fua, and Mathieu Salzmann. Detecting road obstacles by erasing them. IEEE transactions on pattern analysis and machine intelligence, 2023.   \n[225] Bin Liu, Konstantinos Blekas, and Grigorios Tsoumakas. Multi-label sampling based on local label imbalance. Pattern Recognition, 122:108294, 2022.   \n[226] Bin Liu, Dimitrios Papadopoulos, Fragkiskos D Malliaros, Grigorios Tsoumakas, and Apostolos N Papadopoulos. Multiple similarity drug\u2013target interaction prediction with random walks and matrix factorization. Briefings in Bioinformatics, 23(5):bbac353, 2022.   \n[227] Bin Liu, Jin Wang, Kaiwei Sun, and Grigorios Tsoumakas. Fine-grained selective similarity integration for drug\u2013target interaction prediction. Briefings in Bioinformatics, 24(2):bbad085, 2023.   \n[228] Hanyang Liu, Michael Montana, Dingwen Li, Chase Renfroe, Thomas Kannampallil, and Chenyang Lu. Predicting intraoperative hypoxemia with hybrid inference sequence autoencoder networks. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management, pages 1269\u20131278, 2022.   \n[229] Yang Liu, Dingkang Yang, Yan Wang, Jing Liu, Jun Liu, Azzedine Boukerche, Peng Sun, and Liang Song. Generalized video anomaly event detection: Systematic taxonomy and comparison of deep models, 2023.   \n[230] Zhining Liu, Pengfei Wei, Jing Jiang, Wei Cao, Jiang Bian, and Yi Chang. Mesa: boost ensemble imbalanced learning with meta-sampler. Advances in neural information processing systems, 33:14463\u201314474, 2020.   \n[231] Artyom Lobanov, Egor Bogomolov, Yaroslav Golubev, Mikhail Mirzayanov, and Timofey Bryksin. Predicting tags for programming tasks by combining textual and source code data, 2023.   \n[232] Daniel Lopez-Martinez, Christina Chen, and Ming-Jun Chen. Machine learning for dynamically predicting the onset of renal replacement therapy in chronic kidney disease patients using claims data. In International Workshop on Applications of Medical AI, pages 18\u201328. Springer, 2022.   \n[233] Daniel Lopez-Martinez, Alex Yakubovich, Martin Seneviratne, Adam D. Lelkes, Akshit Tyagi, Jonas Kemp, Ethan Steinberg, N. Lance Downing, Ron C. Li, Keith E. Morse, Nigam H. Shah, and Ming-Jun Chen. Instability in clinical risk stratification models using deep learning. In Antonio Parziale, Monica Agrawal, Shalmali Joshi, Irene Y. Chen, Shengpu Tang, Luis Oala, and Adarsh Subbaswamy, editors, Proceedings of the 2nd Machine Learning for Health symposium, volume 193 of Proceedings of Machine Learning Research, pages 552\u2013565. PMLR, 28 Nov 2022.   \n[234] Guan-Rong Lu, Yueh-Cheng Liu, Tung-I Chen, Hung-Ting Su, Tsung-Han Wu, and Winston H Hsu. Anomaly-aware semantic segmentation by leveraging synthetic-unknown data. arXiv preprint arXiv:2111.14343, 2021.   \n[235] Yvan Lucas, Pierre-Edouard Portier, L\u00e9a Laporte, Sylvie Calabretto, Olivier Caelen, Liyun He-Guelton, and Michael Granitzer. Multiple perspectives hmm-based feature engineering for credit card fraud detection. In Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing, SAC \u201919, page 1359\u20131361, New York, NY, USA, 2019. Association for Computing Machinery.   \n[236] Jeffrey Lund, Piper Armstrong, Wilson Fearn, Stephen Cowley, Emily Hales, and Kevin Seppi. Cross-referencing using fine-grained topic modeling. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3978\u20133987, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.   \n[237] Julia Lust and Alexandru Paul Condurache. A survey on assessing the generalization envelope of deep neural networks: Predictive uncertainty, out-of-distribution and adversarial samples, 2021.   \n[238] Yingzhe Lyu, Gopi Krishnan Rajbahadur, Dayi Lin, Boyuan Chen, and Zhen Ming (Jack) Jiang. Towards a consistent interpretation of aiops models. ACM Trans. Softw. Eng. Methodol., 31(1), nov 2021.   \n[239] Victoria L\u00f3pez, Alberto Fern\u00e1ndez, Salvador Garc\u00eda, Vasile Palade, and Francisco Herrera. An insight into classification with imbalanced data: Empirical results and current trends on using data intrinsic characteristics. Information Sciences, 250:113\u2013141, 2013.   \n[240] Liantao Ma, Junyi Gao, Yasha Wang, Chaohe Zhang, Jiangtao Wang, Wenjie Ruan, Wen Tang, Xin Gao, and Xinyu Ma. Adacare: Explainable clinical health status representation learning via scale-adaptive feature extraction and recalibration. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 825\u2013832, 2020.   \n[241] Liantao Ma, Chaohe Zhang, Junyi Gao, Xianfeng Jiao, Zhihao Yu, Yinghao Zhu, Tianlong Wang, Xinyu Ma, Yasha Wang, Wen Tang, et al. Mortality prediction with adaptive feature importance recalibration for peritoneal dialysis patients. Patterns, 4(12), 2023.   \n[242] Liantao Ma, Chaohe Zhang, Yasha Wang, Wenjie Ruan, Jiangtao Wang, Wen Tang, Xinyu Ma, Xin Gao, and Junyi Gao. Concare: Personalized clinical feature embedding via capturing the healthcare context. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 833\u2013840, 2020.   \n[243] Xinyu Ma, Xu Chu, Yasha Wang, Hailong Yu, Liantao Ma, Wen Tang, and Junfeng Zhao. Medfact: Modeling medical feature correlations in patient health representation learning via feature clustering, 2022.   \n[244] Yiming Ma, Victor Sanchez, Soodeh Nikan, Devesh Upadhyay, Bhushan Atote, and Tanaya Guha. Real-time driver monitoring systems through modality and view analysis. arXiv preprint arXiv:2210.09441, 2022.   \n[245] Brielen Madureira and David Schlangen. Instruction clarification requests in multimodal collaborative dialogue games: Tasks, and an analysis of the CoDraw dataset. In Andreas Vlachos and Isabelle Augenstein, editors, Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 2303\u20132319, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics.   \n[246] Lena Maier-Hein, Annika Reinke, Patrick Godau, Minu D Tizabi, Florian Buettner, Evangelia Christodoulou, Ben Glocker, Fabian Isensee, Jens Kleesiek, Michal Kozubek, et al. Metrics reloaded: recommendations for image analysis validation. Nature methods, pages 1\u201318, 2024.   \n[247] Andrey Malinin and Mark Gales. Predictive uncertainty estimation via prior networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.   \n[248] Andrey Malinin, Bruno Mlodozeniec, and Mark Gales. Ensemble distribution distillation. In International Conference on Learning Representations, 2020.   \n[249] Sunil Mallya, Marc Overhage, Sravan Bodapati, Navneet Srivastava, and Sahika Genc. Savehr: self attention vector representations for ehr based personalized chronic disease onset prediction and interpretability. arXiv preprint arXiv:1911.05370, 2019.   \n[250] Saurav Manchanda, Pranjul Yadav, Khoa Doan, and S Sathiya Keerthi. Targeted display advertising: the case of preferential attachment. In 2019 IEEE International Conference on Big Data (Big Data), pages 1868\u20131877. IEEE, 2019.   \n[251] Rafael B Mangolin, Rodolfo M Pereira, Alceu S Britto Jr, Carlos N Silla Jr, Val\u00e9ria D Feltrim, Diego Bertolini, and Yandre MG Costa. A multimodal approach for multi-label movie genre classification. Multimedia Tools and Applications, 81(14):19071\u201319096, 2022.   \n[252] Johan Markdahl, Nicolo Colombo, Johan Thunberg, and Jorge Gon\u00e7alves. Experimental design trade-offs for gene regulatory network inference: An in silico study of the yeast saccharomyces cerevisiae cell cycle. In 2017 IEEE 56th Annual Conference on Decision and Control (CDC), pages 423\u2013428. IEEE, 2017.   \n[253] Mansour Zoubeirou A Mayaki and Michel Riveill. Multiple inputs neural networks for fraud detection. In 2022 International Conference on Machine Learning, Control, and Robotics (MLCR), pages 8\u201313, 2022.   \n[254] Samuele Mazzanti. Why you should stop using the roc curve, September 2023.   \n[255] John McKay, Isaac Gerg, and Vishal Monga. Bridging the gap: Simultaneous fine tuning for data re-balancing. In IGARSS 2018-2018 IEEE International Geoscience and Remote Sensing Symposium, pages 7062\u20137065. IEEE, 2018.   \n[256] Mat\u00fa\u0161 Medo, Daniel M Aebersold, and Michaela Medov\u00e1. Protrank: bypassing the imputation of missing values in differential expression analysis of proteomic data. BMC bioinformatics, 20:1\u201312, 2019.   \n[257] Aryan Mehboudi, Shrawan Singhal, and S. V. Sreenivasan. Squeeze flow of micro-droplets: convolutional neural network with trainable and tunable refinement, 2022.   \n[258] Julia A Meister, Khuong An Nguyen, and Zhiyuan Luo. Audio feature ranking for sound-based covid-19 patient detection. In EPIA Conference on Artificial Intelligence, pages 146\u2013158. Springer, 2022.   \n[259] David Merrell and Anthony Gitter. Inferring signaling pathways with probabilistic programming. Bioinformatics, 36(Supplement 2):i822\u2013i830, December 2020.   \n[260] Mika A Merrill and Tim Althoff. Self-supervised pretraining and transfer learning enable flu and covid-19 predictions in small mobile sensing datasets. In Bobak J. Mortazavi, Tasmie Sarker, Andrew Beam, and Joyce C. Ho, editors, Proceedings of the Conference on Health, Inference, and Learning, volume 209 of Proceedings of Machine Learning Research, pages 191\u2013206. PMLR, 22 Jun\u201324 Jun 2023.   \n[261] Jiaju Miao and Wei Zhu. Precision\u2013recall curve (prc) classification trees. Evolutionary intelligence, 15(3):1545\u20131569, 2022.   \n[262] Kai Middlebrook, Shyam Sudhakaran, and David Guy Brizan. Muslcat: Multi-scale multi-level convolutional attention transformer for discriminative music modeling on raw waveforms, 2021.   \n[263] Dimity Miller, Feras Dayoub, Michael Milford, and Niko S\u00a8\"underhauf. Evaluating merging strategies for sampling-based uncertainty techniques in object detection. In 2019 international conference on robotics and automation (icra), pages 2348\u20132354. IEEE, 2019.   \n[264] Shahryar Minhas, Peter D Hoff, and Michael D Ward. Inferential approaches for network analysis: Amen for latent factor models. Political Analysis, 27(2):208\u2013222, 2019.   \n[265] Amanda J Minnich, Kevin McLoughlin, Margaret Tse, Jason Deng, Andrew Weber, Neha Murad, Benjamin D Madej, Bharath Ramsundar, Tom Rush, Stacie Calad-Thomson, et al. Ampl: a data-driven modeling pipeline for drug discovery. Journal of chemical information and modeling, 60(4):1955\u20131968, 2020.   \n[266] Yisroel Mirsky, Tomer Golomb, and Yuval Elovici. Lightweight collaborative anomaly detection for the iot using blockchain. Journal of Parallel and Distributed Computing, 145:75\u2013 97, 2020.   \n[267] Pratik K Mishra, Alex Mihailidis, and Shehroz S Khan. Skeletal video anomaly detection using deep learning: Survey, challenges, and future directions. IEEE Transactions on Emerging Topics in Computational Intelligence, 2024.   \n[268] Ali Mollahosseini, Behzad Hasani, and Mohammad H. Mahoor. Affectnet: A database for facial expression, valence, and arousal computing in the wild. IEEE Transactions on Affective Computing, 10(1):18\u201331, January 2019.   \n[269] Lena Mondrejevski, Ioanna Miliou, Annaclaudia Montanino, David Pitts, Jaakko Hollm\u00e9n, and Panagiotis Papapetrou. Flicu: A federated learning workflow for intensive care unit mortality prediction. In 2022 IEEE 35th International Symposium on Computer-Based Medical Systems (CBMS), pages 32\u201337. IEEE, 2022.   \n[270] Aanchal Mongia, Sanjay Kr Saha, Emilie Chouzenoux, and Angshul Majumdar. A computational approach to aid clinicians in selecting anti-viral drugs for covid-19 trials. Scientific reports, 11(1):9047, 2021.   \n[271] Michael Moor, Max Horn, Bastian Rieck, Damian Roqueiro, and Karsten Borgwardt. Early recognition of sepsis with gaussian process temporal convolutional networks and dynamic time warping. In Finale Doshi-Velez, Jim Fackler, Ken Jung, David Kale, Rajesh Ranganath, Byron Wallace, and Jenna Wiens, editors, Proceedings of the 4th Machine Learning for Healthcare Conference, volume 106 of Proceedings of Machine Learning Research, pages 2\u201326. PMLR, 09\u201310 Aug 2019.   \n[272] Jos\u00e9 Morano, \u00c1lvaro S Hervella, N Barreira, Jorge Novo, and Jos\u00e9 Rouco. Multimodal transfer learning-based approaches for retinal vascular segmentation. In ECAI 2020, pages 1866\u20131873. IOS Press, 2020.   \n[273] Jos\u00e9 Morano, \u00c1lvaro S. Hervella, Jorge Novo, and Jos\u00e9 Rouco. Simultaneous segmentation and classification of the retinal arteries and veins from color fundus images. Artificial Intelligence in Medicine, 118:102116, August 2021.   \n[274] Candelaria Mosquera, Luciana Ferrer, Diego Milone, Daniel Luna, and Enzo Ferrante. Impact of class imbalance on chest x-ray classifiers: towards better evaluation practices for discrimination and calibration performance, 2022.   \n[275] Pablo Mosteiro, Emil Rijcken, Kalliopi Zervanou, Uzay Kaymak, Floortje Scheepers, and Marco Spruit. Machine learning for violence risk assessment using dutch clinical notes. Journal of Artificial Intelligence for Medical Sciences, 2(1-2):44\u201354, 2021.   \n[276] Zaynab Mousavian, Sahand Khakabimamaghani, Kaveh Kavousi, and Ali Masoudi-Nejad. Drug\u2013target interaction prediction from pssm based evolutionary information. Journal of pharmacological and toxicological methods, 78:42\u201351, 2016.   \n[277] Faezeh Movahedi, Rema Padman, and James F Antaki. Limitations of receiver operating characteristic curve on imbalanced data: assist device mortality risk scores. The Journal of Thoracic and Cardiovascular Surgery, 165(4):1433\u20131442, 2023.   \n[278] Christopher W Murphy. Class imbalance techniques for high energy physics. SciPost Physics, 7(6):076, 2019.   \n[279] Daniel Muthukrishna, Gautham Narayan, Kaisey S Mandel, Rahul Biswas, and Ren\u00e9e Hlo\u017eek. Rapid: early classification of explosive transients using deep learning. Publications of the Astronomical Society of the Pacific, 131(1005):118002, 2019.   \n[280] Anubhav Reddy Nallabasannagari, Madhu Reddiboina, Ryan Seltzer, Trevor Zeffiro, Ajay Sharma, and Mahendra Bhandari. All data inclusive, deep learning models to predict critical events in the medical information mart for intensive care iii database (mimic iii), 2020.   \n[281] Santhosh Narayanan, Carsten Maple, and Mark Hooper. A point process model for rare event detection, 2022.   \n[282] Jose M Navarro, Alexis Huet, and Dario Rossi. Human readable network troubleshooting based on anomaly detection and feature scoring. Computer Networks, 219:109447, 2022.   \n[283] Bijay Neupane, Torben Bach Pedersen, and Bo Thiesson. Utilizing device-level demand forecasting for flexibility markets-full version. arXiv preprint arXiv:1805.00702, 2018.   \n[284] Eli Newby, Jorge G\u00f3mez Tejeda Za\u00f1udo, and R\u00e9ka Albert. Structure-based approach to identifying small sets of driver nodes in biological networks. Chaos: An Interdisciplinary Journal of Nonlinear Science, 32(6):063102, 06 2022.   \n[285] Minh-Nghia Nguyen and Ngo Anh Vien. Scalable and interpretable one-class svms with deep learning and random fourier features. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2018, Dublin, Ireland, September 10\u201314, 2018, Proceedings, Part I 18, pages 157\u2013172. Springer, 2019.   \n[286] Maximilian Nickel, Kevin Murphy, Volker Tresp, and Evgeniy Gabrilovich. A review of relational machine learning for knowledge graphs. Proceedings of the IEEE, 104(1):11\u201333, January 2016.   \n[287] Zhale Nowroozilarki, Arash Pakbin, James Royalty, Donald KK Lee, and Bobak J Mortazavi. Real-time mortality prediction using mimic-iv icu data via boosted nonparametric hazards. In 2021 IEEE EMBS international conference on biomedical and health informatics (BHI), pages 1\u20134. IEEE, 2021.   \n[288] Antonios Ntroumpogiannis, Michail Giannoulis, Nikolaos Myrtakis, Vassilis Christophides, Eric Simon, and Ioannis Tsamardinos. A meta-level analysis of online anomaly detectors. The VLDB Journal, pages 1\u201342, 2023.   \n[289] Chris J Oates, Richard Amos, and Simon EF Spencer. Quantifying the multi-scale performance of network inference algorithms. Statistical Applications in Genetics and Molecular Biology, 13(5):611\u2013631, 2014.   \n[290] Philipp Oberdiek, Matthias Rottmann, and Hanno Gottschalk. Classification uncertainty of deep neural networks based on gradient information. In Artificial Neural Networks in Pattern Recognition: 8th IAPR TC3 Workshop, ANNPR 2018, Siena, Italy, September 19\u201321, 2018, Proceedings 8, pages 113\u2013125. Springer, 2018.   \n[291] Mar\u00eda \u00d3skarsd\u00f3ttir, Waqas Ahmed, Katrien Antonio, Bart Baesens, R\u00e9mi Dendievel, Tom Donas, and Tom Reynkens. Social network analytics for supervised fraud detection in insurance. Risk Analysis, 42(8):1872\u20131890, 2022.   \n[292] Brice Ozenne, Fabien Subtil, and Delphine Maucort-Boulch. The precision\u2013recall curve overcame the optimism of the receiver operating characteristic curve in rare diseases. Journal of clinical epidemiology, 68(8):855\u2013859, 2015.   \n[293] Ozan Ozyegen, Devika Kabe, and Mucahit Cevik. Word-level text highlighting of medical texts for telehealth services. Artificial Intelligence in Medicine, 127:102284, 2022.   \n[294] Jos\u00e9 A. Padr\u00f3n-Hidalgo, Valero Laparra, and Gustau Camps-Valls. Unsupervised anomaly and change detection with multivariate gaussianization. IEEE Transactions on Geoscience and Remote Sensing, 60:1\u201310, 2022.   \n[295] Aniello Panariello, Angelo Porrello, Simone Calderara, and Rita Cucchiara. Consistencybased self-supervised learning for temporal anomaly localization. In European Conference on Computer Vision, pages 338\u2013349. Springer, 2022.   \n[296] Guansong Pang, Chunhua Shen, Huidong Jin, and Anton van den Hengel. Deep weaklysupervised anomaly detection. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD \u201923, page 1795\u20131807, New York, NY, USA, 2023. Association for Computing Machinery.   \n[297] Guansong Pang, Chunhua Shen, and Anton Van Den Hengel. Deep anomaly detection with deviation networks. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pages 353\u2013362, 2019.   \n[298] Guansong Pang, Anton van den Hengel, Chunhua Shen, and Longbing Cao. Toward deep supervised anomaly detection: Reinforcement learning from partially labeled anomaly data. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, KDD \u201921. ACM, August 2021.   \n[299] Fragkiskos Papadopoulos and Kaj-Kolja Kleineberg. Link persistence and conditional distances in multiplex networks. Physical Review E, 99(1):012322, 2019.   \n[300] Min Sue Park, Hyeontae Jo, Haeun Lee, Se Young Jung, and Hyung Ju Hwang. Machine learning-based covid-19 patients triage algorithm using patient-generated health data from nationwide multicenter database. Infectious Diseases and Therapy, 11(2):787\u2013805, February 2022.   \n[301] Ilya N Pashchenko, Kirill V Sokolovsky, and Panagiotis Gavras. Machine learning search for variable stars. Monthly Notices of the Royal Astronomical Society, 475(2):2326\u20132343, 2018.   \n[302] Ali Payani and Faramarz Fekri. Inductive logic programming via differentiable deep neural logic networks. arXiv preprint arXiv:1906.03523, 2019.   \n[303] Xueping Peng, Guodong Long, Tao Shen, Sen Wang, and Jing Jiang. Self-attention enhanced patient journey understanding in healthcare system. In Frank Hutter, Kristian Kersting, Jefrey Lijffjit, and Isabel Valera, editors, Machine Learning and Knowledge Discovery in Databases, pages 719\u2013735, Cham, 2021. Springer International Publishing.   \n[304] Xueping Peng, Guodong Long, Tao Shen, Sen Wang, Jing Jiang, and Michael Blumenstein. Temporal self-attention network for medical concept embedding. In 2019 IEEE international conference on data mining (ICDM), pages 498\u2013507. IEEE, 2019.   \n[305] Xueping Peng, Guodong Long, Tao Shen, Sen Wang, Jing Jiang, and Chengqi Zhang. Bitenet: bidirectional temporal encoder network to predict medical outcomes. In 2020 IEEE International Conference on Data Mining (ICDM), pages 412\u2013421. IEEE, 2020.   \n[306] Eike Petersen, Melanie Ganz, Sune Holm, and Aasa Feragen. On (assessing) the fairness of risk score models. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency, FAccT \u201923, page 817\u2013829, New York, NY, USA, 2023. Association for Computing Machinery.   \n[307] Stephen Pfohl, Yizhe Xu, Agata Foryciarz, Nikolaos Ignatiadis, Julian Genkins, and Nigam Shah. Net benefit, calibration, threshold selection, and training objectives for algorithmic fairness in healthcare. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, pages 1039\u20131052, 2022.   \n[308] Dario Piermarini, Antonio M. Sudoso, and Veronica Piccialli. Predicting municipalities in financial distress: a machine learning approach enhanced by domain expertise, 2023.   \n[309] Walter Hugo Lopez Pinaya, Petru-Daniel Tudosiu, Robert Gray, Geraint Rees, Parashkev Nachev, S\u00e9bastien Ourselin, and M. Jorge Cardoso. Unsupervised brain anomaly detection and segmentation with transformers. In Mattias Heinrich, Qi Dou, Marleen de Bruijne, Jan Lellmann, Alexander Schl\u00e4fer, and Floris Ernst, editors, Proceedings of the Fourth Conference on Medical Imaging with Deep Learning, volume 143 of Proceedings of Machine Learning Research, pages 596\u2013617. PMLR, 07\u201309 Jul 2021.   \n[310] Adrian Alan Pol, Thea Aarrestad, Katya Govorkova, Roi Halily, Tal Kopetz, Anat Klempner, Vladimir Loncar, Jennifer Ngadiuba, Maurizio Pierini, Olya Sirkin, et al. Jet single shot detection. In EPJ Web of Conferences, volume 251, page 04027. EDP Sciences, 2021.   \n[311] Jordi Pons, Oriol Nieto, Matthew Prockup, Erik Schmidt, Andreas Ehmann, and Xavier Serra. End-to-end learning for music audio tagging at scale. In Proceedings of the 19th International Society for Music Information Retrieval Conference, 2018.   \n[312] Ioannis Prapas, Akanksha Ahuja, Spyros Kondylatos, Ilektra Karasante, Eleanna Panagiotou, Lazaro Alonso, Charalampos Davalas, Dimitrios Michail, Nuno Carvalhais, and Ioannis Papoutsis. Deep learning for global wildfire forecasting, 2023.   \n[313] Sam Preston, Mu Wei, Rajesh Rao, Robert Tinn, Naoto Usuyama, Michael Lucas, Roshanthi Weerasinghe, Soohee Lee, Brian Piening, Paul Tittel, Naveen Valluri, Tristan Naumann, Carlo Bifulco, and Hoifung Poon. Towards structuring real-world data at scale: Deep learning for extracting key oncology information from clinical text with patient-level supervision, 2022.   \n[314] Qi Qi, Youzhi Luo, Zhao Xu, Shuiwang Ji, and Tianbao Yang. Stochastic optimization of areas under precision-recall curves with provable convergence. Advances in neural information processing systems, 34:1752\u20131765, 2021.   \n[315] Gwenol\u00e9 Quellec, Hassan Al Hajj, Mathieu Lamard, Pierre-Henri Conze, Pascale Massin, and B\u00e9atrice Cochener. Explain: Explanatory artificial intelligence for diabetic retinopathy diagnosis. Medical Image Analysis, 72:102118, August 2021.   \n[316] Mahmudur Rahman, Tanay Kumar Saha, Mohammad Al Hasan, Kevin S. Xu, and Chandan K. Reddy. Dylink2vec: Effective feature representation for link prediction in dynamic networks, 2018.   \n[317] Farzaneh Rajabi and Jack Siyuan He. Click-through rate prediction using graph neural networks and online learning, 2021.   \n[318] Korbinian Randl, N\u00faria Llad\u00f3s Armengol, Lena Mondrejevski, and Ioanna Miliou. Early prediction of the risk of icu mortality with deep federated learning. In 2023 IEEE 36th International Symposium on Computer-Based Medical Systems (CBMS), pages 706\u2013711. IEEE, 2023.   \n[319] Susie Xi Rao, Cl\u00e9mence Lanfranchi, Shuai Zhang, Zhichao Han, Zitao Zhang, Wei Min, Mo Cheng, Yinan Shan, Yang Zhao, and Ce Zhang. Modelling graph dynamics in fraud detection with Attention, 2022.   \n[320] Gopikrishna Rathinavel, Nikhil Muralidhar, Timothy O\u2019Shea, and Naren Ramakrishnan. Detecting irregular network activity with adversarial learning and expert feedback. In 2022 IEEE International Conference on Data Mining (ICDM), pages 1161\u20131166. IEEE, 2022.   \n[321] Farshid Rayhan, Sajid Ahmed, Zaynab Mousavian, Dewan Md Farid, and Swakkhar Shatabda. Frnet-dti: Deep convolutional neural network for drug-target interaction prediction. Heliyon, 6(3), 2020.   \n[322] Farshid Rayhan, Sajid Ahmed, Swakkhar Shatabda, Dewan Md Farid, Zaynab Mousavian, Abdollah Dehzangi, and M Sohel Rahman. idti-esboost: identification of drug target interaction using evolutionary and structural features with boosting. Scientific reports, 7(1):17731, 2017.   \n[323] Haroldas Razvadauskas, Evaldas Vaiciukynas, Kazimieras Buskus, Lukas Drukteinis, Lukas Arlauskas, Saulius Sadauskas, and Albinas Naudziunas. Exploring traditional machine learning for identification of pathological auscultations, 2022.   \n[324] Ismat Ara Reshma, Sylvain Cussat-Blanc, Radu Tudor Ionescu, Herv\u00e9 Luga, and Josiane Mothe. Natural vs balanced distribution in deep learning on whole slide images for cancer detection. In Proceedings of the 36th Annual ACM Symposium on Applied Computing, pages 18\u201325, 2021.   \n[325] Roonak Rezvani, Samaneh Kouchaki, Ramin Nilforooshan, David J. Sharp, and Payam Barnaghi. Semi-supervised learning for identifying the likelihood of agitation in people with dementia, 2021.   \n[326] Georgios Rizos, Jenna Lawson, Simon Mitchell, Pranay Shah, Xin Wen, Cristina BanksLeite, Robert Ewers, and Bj\u00f6rn W. Schuller. Propagating variational model uncertainty for bioacoustic call label smoothing. Patterns, 5(3):100932, 2024.   \n[327] Narjes Rohani and Changiz Eslahchi. Drug-drug interaction predicting by neural network using integrated similarity. Scientific reports, 9(1):13645, 2019.   \n[328] Micha\u0142 Romaszewski, Przemys\u0142aw G\u0142omb, Arkadiusz Sochan, and Micha\u0142 Cholewa. A dataset for evaluating blood detection in hyperspectral images. Forensic science international, 320:110701, 2021.   \n[329] Miguel Romero, Oscar Ram\u00edrez, Jorge Finke, and Camilo Rocha. Feature extraction with spectral clustering for gene function prediction using hierarchical multi-label classification. Applied Network Science, 7(1):28, 2022.   \n[330] Caitlin Rose, Jeyhan S Kartaltepe, Gregory F Snyder, Vicente Rodriguez-Gomez, LY Aaron Yung, Pablo Arrabal Haro, Micaela B Bagley, Antonello Calabr\u00f3, Nikko J Cleri, MC Cooper, et al. Identifying galaxy mergers in simulated ceers nircam images using random forests. The Astrophysical Journal, 942(1):54, 2023.   \n[331] Daniel Rosenberg. Imbalanced data? stop using roc-auc and use auprc instead, June 2022.   \n[332] Sankardas Roy, Jordan DeLoach, Yuping Li, Nic Herndon, Doina Caragea, Xinming Ou, Venkatesh Prasad Ranganath, Hongmin Li, and Nicolais Guevara. Experimental study with real-world data for android app security analysis using machine learning. In Proceedings of the 31st Annual Computer Security Applications Conference, pages 81\u201390, 2015.   \n[333] Timothy N Rubin, America Chambers, Padhraic Smyth, and Mark Steyvers. Statistical topic models for multi-label document classification. Machine learning, 88:157\u2013208, 2012.   \n[334] Clemente Rubio-Manzano, Tom\u00e1s Lermanda, Claudia Mart\u00ednez-Araneda, Christian Vidal, and Alejandra Segura. Teach me to play, gamer! imitative learning in computer games via linguistic description of complex phenomena and decision trees. Soft Computing, 27(6):3023\u20133035, 2023.   \n[335] Lukas Ruff, Jacob R. Kauffmann, Robert A. Vandermeulen, Gr\u00e9goire Montavon, Wojciech Samek, Marius Kloft, Thomas G. Dietterich, and Klaus-Robert M\u00fcller. A unifying review of deep and shallow anomaly detection. Proceedings of the IEEE, 109(5):756\u2013795, 2021.   \n[336] Victor Saase, Holger Wenz, Thomas Ganslandt, Christoph Groden, and M\u00e1t\u00e9 E. Maros. Simple statistical methods for unsupervised brain anomaly detection on mri are competitive to deep learning methods, 2020.   \n[337] Shiori Sagawa, Pang Wei Koh, Tony Lee, Irena Gao, Sang Michael Xie, Kendrick Shen, Ananya Kumar, Weihua Hu, Michihiro Yasunaga, Henrik Marklund, et al. Extending the wilds benchmark for unsupervised adaptation. arXiv preprint arXiv:2112.05090, 2021.   \n[338] Berkman Sahiner, Weijie Chen, Aria Pezeshk, and Nicholas Petrick. Comparison of two classifiers when the data sets are imbalanced: the power of the area under the precision-recall curve as the figure of merit versus the area under the ROC curve. In Matthew A. Kupinski and Robert M. Nishikawa, editors, Medical Imaging 2017: Image Perception, Observer Performance, and Technology Assessment, volume 10136, page 101360G. International Society for Optics and Photonics, SPIE, 2017.   \n[339] Takaya Saito and Marc Rehmsmeier. The precision-recall plot is more informative than the roc plot when evaluating binary classifiers on imbalanced datasets. PloS one, 10(3):e0118432, 2015.   \n[340] Takaya Saito and Marc Rehmsmeier. Precrec: fast and accurate precision\u2013recall and ROC curve calculations in R. Bioinformatics, 33(1):145\u2013147, 09 2016.   \n[341] Amir Sarabadani, Aaron Halfaker, and Dario Taraborelli. Building automated vandalism detection tools for wikidata. In Proceedings of the 26th International Conference on World Wide Web Companion, pages 1647\u20131654, 2017.   \n[342] Mehrzad Saremi and Maryam Amirmazlaghani. Reconstruction of gene regulatory networks using multiple datasets. IEEE/ACM transactions on computational biology and bioinformatics, 19(3):1827\u20131839, 2021.   \n[343] Hamed Sarvari, Carlotta Domeniconi, Bardh Prenkaj, and Giovanni Stilo. Unsupervised boosting-based autoencoder ensembles for outlier detection. In Pacific-Asia Conference on Knowledge Discovery and Data Mining, pages 91\u2013103. Springer, 2021.   \n[344] Hamed Sarvari, Carlotta Domeniconi, and Giovanni Stilo. Graph-based selective outlier ensembles. In Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing, pages 518\u2013525, 2019.   \n[345] Thomas Schlegl, Hrvoje Bogunovic, Sophie Klimscha, Philipp Seeb\u00f6ck, Amir Sadeghipour, Bianca Gerendas, Sebastian M. Waldstein, Georg Langs, and Ursula Schmidt-Erfurth. Fully automated segmentation of hyperreflective foci in optical coherence tomography images, 2018.   \n[346] Thomas Schlegl, Heiko Stino, Michael Niederleithner, Andreas Pollreisz, Ursula SchmidtErfurth, Wolfgang Drexler, Rainer A. Leitgeb, and Tilman Schmoll. Data-centric ai approach to improve optic nerve head segmentation and localization in oct en face images, 2022.   \n[347] Oliver Schulte, Zhensong Qian, Arthur E Kirkpatrick, Xiaoqian Yin, and Yan Sun. Fast learning of relational dependency networks. Machine Learning, 103:377\u2013406, 2016.   \n[348] Kyriakos Schwarz, Ahmed Allam, Nicolas Andres Perez Gonzalez, and Michael Krauthammer. Attentionddi: Siamese attention-based deep learning method for drug\u2013drug interaction predictions. BMC bioinformatics, 22(1):1\u201319, 2021.   \n[349] Kyriakos Schwarz, Alicia Pliego-Mendieta, Amina Mollaysa, Lara Planas-Paz, Chantal Pauli, Ahmed Allam, and Michael Krauthammer. Ddos: A graph neural network based drug synergy prediction algorithm, 2024.   \n[350] Eugene Seo, Rebecca A Hutchinson, Xiao Fu, Chelsea Li, Tyler A Hallman, John Kilbride, and W Douglas Robinson. Stateconet: Statistical ecology neural networks for species distribution modeling. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 513\u2013521, 2021.   \n[351] Hrishikesh Sharma, Prakhar Pradhan, and Balamuralidhar P. Scnet: a generalized attentionbased model for crack fault segmentation. In Proceedings of the twelfth indian conference on computer vision, graphics and image processing, pages 1\u20139, 2021.   \n[352] Mradul Sharma, Jitadeepa Nayak, Maharaj Krishna Koul, Smarajit Bose, and Abhas Mitra. Gamma/hadron segregation for a ground based imaging atmospheric cherenkov telescope using machine learning methods: Random forest leads. Research in Astronomy and Astrophysics, 14(11):1491, 2014.   \n[353] Hongda Shen and Eren Kursun. Label augmentation via time-based knowledge distillation for financial anomaly detection, 2021.   \n[354] Xiupeng Shi, Yiik Diew Wong, Michael Zhi-Feng Li, Chandrasekar Palanisamy, and Chen Chai. A feature learning approach based on xgboost for driving assessment and risk prediction. Accident Analysis & Prevention, 129:170\u2013179, 2019.   \n[355] Yishai Shimoni, Ehud Karavani, Sivan Ravid, Peter Bak, Tan Hung Ng, Sharon Hensley Alford, Denise Meade, and Yaara Goldschmidt. An evaluation toolkit to guide model selection and cohort definition in causal inference, 2019.   \n[356] Samuel Showalter and Zhixin Wu. Minimizing the societal cost of credit card fraud with limited and imbalanced data, 2019.   \n[357] Hai Shu and Hongtu Zhu. Sensitivity analysis of deep neural networks. Proceedings of the AAAI Conference on Artificial Intelligence, 33(01):4943\u20134950, July 2019.   \n[358] Satya Narayan Shukla and Benjamin Marlin. Interpolation-prediction networks for irregularly sampled time series. In International Conference on Learning Representations, 2019.   \n[359] Satya Narayan Shukla and Benjamin M. Marlin. Modeling irregularly sampled clinical time series, 2018.   \n[360] Yaniv Shulman. Unsupervised contextual anomaly detection using joint deep variational generative models, 2019.   \n[361] Yuqi Si and Kirk Roberts. Three-level hierarchical transformer networks for long-sequence and multiple clinical documents classification, 2021.   \n[362] Amit Kumar Sikder, Hidayet Aksu, and A Selcuk Uluagac. A context-aware framework for detecting sensor-based threats on smart devices. IEEE Transactions on Mobile Computing, 19(2):245\u2013261, 2019.   \n[363] Mar\u00edlia Costa Rosendo Silva, Felipe Alves Siqueira, Jo\u00e3o Pedro Mantovani Tarrega, Jo\u00e3o Vitor Pataca Beinotti, Augusto Sousa Nunes, Miguel de Mattos Gardini, Vin\u00edcius Adolfo Pereira da Silva, N\u00e1dia F\u00e9lix Felipe da Silva, and Andr\u00e9 Carlos Ponce de Leon Ferreira de Carvalho. No pattern, no recognition: a survey about reproducibility and distortion issues of text clustering and topic modeling, 2022.   \n[364] Edgar Simo-Serra, Eduard Trulls, Luis Ferraz, Iasonas Kokkinos, and Francesc MorenoNoguer. Fracking deep convolutional image descriptors, 2015.   \n[365] Aditya Kumar Singh and B. Uma Shankar. Multi-label classification on remote-sensing images, 2022.   \n[366] Harvineet Singh, Rina Singh, Vishwali Mhasawade, and Rumi Chunara. Fairness violations and mitigation under covariate shift. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pages 3\u201313, 2021.   \n[367] Joakim Skarding, Bogdan Gabrys, and Katarzyna Musial. Foundations and modeling of dynamic networks using dynamic graph neural networks: A survey. IEEE Access, 9:79143\u2013 79168, 2021.   \n[368] Johannes De Smedt and Jochen De Weerdt. Predictive process model monitoring using recurrent neural networks, 2023.   \n[369] Anna L Smith, Tian Zheng, and Andrew Gelman. Prediction scoring of data-driven discoveries for reproducible research. Statistics and Computing, 33(1):11, 2023.   \n[370] Artur Sokolovsky, Luca Arnaboldi, Jaume Bacardit, and Thomas Gross. Volume-centred range bars: Novel interpretable representation of financial markets designed for machine learning applications, 2022.   \n[371] Roghayeh Soleymani, Eric Granger, and Giorgio Fumera. Progressive boosting for class imbalance and its application to face re-identification. Expert Systems with Applications, 101:271\u2013291, 2018.   \n[372] Janne Spijkervet and John Ashley Burgoyne. Contrastive learning of musical representations. In Proceedings of the 22nd International Society for Music Information Retrieval Conference, pages 673\u2013681. ISMIR, October 2021.   \n[373] Saurabh Srivastava, Vinay P. Namboodiri, and T. V. Prabhakar. Putworkbench: Analysing privacy in ai-intensive systems, 2019.   \n[374] Benjamin Stadnick, Jan Witowski, Vishwaesh Rajiv, Jakub Ch\u0142\u02dbedowski, Farah E. Shamout, Kyunghyun Cho, and Krzysztof J. Geras. Meta-repository of screening mammography classifiers, 2022.   \n[375] Christoph Stanik, Marlo Haering, and Walid Maalej. Classifying multilingual user feedback using traditional machine learning and deep learning. In 2019 IEEE 27th international requirements engineering conference workshops (REW), pages 220\u2013226. IEEE, 2019.   \n[376] Georg Steinbuss and Klemens B\u00f6hm. Benchmarking unsupervised outlier detection with realistic synthetic data. ACM Trans. Knowl. Discov. Data, 15(4), apr 2021.   \n[377] Oliver L. Stephenson, Tobias K\u00f6hne, Eric Zhan, Brent E. Cahill, Sang-Ho Yun, Zachary E. Ross, and Mark Simons. Deep learning-based damage mapping with insar coherence time series. IEEE Transactions on Geoscience and Remote Sensing, 60:1\u201317, 2022.   \n[378] Callum L. Stewart, Amos Folarin, and Richard Dobson. Personalized acute stress classification from physiological signals with neural processes, 2020.   \n[379] Andrew Stolman, Caleb Levy, C Seshadhri, and Aneesh Sharma. Classic graph structural features outperform factorization-based graph embedding methods on community labeling. In Proceedings of the 2022 SIAM International Conference on Data Mining (SDM), pages 388\u2013396. SIAM, 2022.   \n[380] Nils Strodthoff, Baris Goktepe, Thomas Schierl, Cornelius Hellge, and Wojciech Samek. Enhanced machine learning techniques for early harq feedback prediction in 5g. IEEE Journal on Selected Areas in Communications, 37(11):2573\u20132587, November 2019.   \n[381] Olly Styles, Tanaya Guha, and Victor Sanchez. Multi-camera trajectory forecasting with trajectory tensors. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(11):8482\u2013 8491, 2021.   \n[382] Sujanya Suresh, Savitha Ramasamy, P. N. Suganthan, and Cheryl Sze Yin Wong. Incremental knowledge tracing from multiple schools, 2022.   \n[383] Lionel Tabourier, Daniel F Bernardes, Anne-Sophie Libert, and Renaud Lambiotte. Rankmerging: a supervised learning-to-rank framework to predict links in large social networks. Machine Learning, 108:1729\u20131756, 2019.   \n[384] Rajesh Talluri and Sanjay Shete. Using the weighted area under the net benefit curve for decision curve analysis. BMC medical informatics and decision making, 16:1\u20139, 2016.   \n[385] Fei Tan, Zhi Wei, Jun He, Xiang Wu, Bo Peng, Haoran Liu, and Zhenyu Yan. A blended deep learning approach for predicting user intended actions. In 2018 IEEE international conference on data mining (ICDM), pages 487\u2013496. IEEE, 2018.   \n[386] Federico Tavella, Alberto Giaretta, Mauro Conti, and Sasitharan Balasubramaniam. A machine learning-based approach to detect threats in bio-cyber dna storage systems. Computer Communications, 187:59\u201370, 2022.   \n[387] Vajira Thambawita, Debesh Jha, Hugo Lewi Hammer, H\u00e5vard D Johansen, Dag Johansen, P\u00e5l Halvorsen, and Michael A Riegler. An extensive study on cross-dataset bias and evaluation metrics interpretation for machine learning applied to gastrointestinal tract abnormality classification. ACM Transactions on Computing for Healthcare, 1(3):1\u201329, 2020.   \n[388] Santosh Thoduka, Juergen Gall, and Paul G. Ploger. Using visual anomaly detection for task execution monitoring. In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, September 2021.   \n[389] Maud Thomas and Holger Rootz\u00e9n. Real-time prediction of severe influenza epidemics using extreme value statistics. Journal of the Royal Statistical Society Series C: Applied Statistics, 71(2):376\u2013394, 2022.   \n[390] Aleksei Tiulpin, Stefan Klein, Sita MA Bierma-Zeinstra, J\u00e9r\u00f4me Thevenot, Esa Rahtu, Joyce van Meurs, Edwin HG Oei, and Simo Saarakkala. Multimodal machine learningbased knee osteoarthritis progression prediction from plain radiographs and clinical data. Scientific reports, 9(1):20038, 2019.   \n[391] Amirsina Torf,i Edward A Fox, and Chandan K Reddy. Differentially private synthetic medical data generation using convolutional gans. Information Sciences, 586:485\u2013500, 2022.   \n[392] Meredith V. Trotter, Cuong Q. Nguyen, Stephen Young, Rob T. Woodruff, and Kim M. Branson. Epigenomic language models powered by cerebras, 2021.   \n[393] Hasan Md Tusfiqur, Duy M. H. Nguyen, Mai T. N. Truong, Triet A. Nguyen, Binh T. Nguyen, Michael Barz, Hans-Juergen Profitlich, Ngoc T. T. Than, Ngan Le, Pengtao Xie, and Daniel Sonntag. Drg-net: Interactive joint learning of multi-lesion segmentation and classification for diabetic retinopathy grading, 2022.   \n[394] Sagar Vaze, Kai Han, Andrea Vedaldi, and Andrew Zisserman. Open-set recognition: A good closed-set classifier is all you need. In International Conference on Learning Representations, 2022.   \n[395] Celine Vens, Jan Struyf, Leander Schietgat, Sa\u0161o D\u017eeroski, and Hendrik Blockeel. Decision trees for hierarchical multi-label classification. Machine learning, 73:185\u2013214, 2008.   \n[396] Pim Verschuuren, Serena Palazzo, Tom Powell, Steve Sutton, Alfred Pilgrim, and Michele Faucci Giannelli. Supervised machine learning techniques for data matching based on similarity metrics. arXiv preprint arXiv:2007.04001, 2020.   \n[397] V Vijayan, D Critchlow, and T Milenkovic\u00b4. Alignment of dynamic networks. Bioinformatics, 33(14):i180\u2013i189, 07 2017.   \n[398] Gaurav Vishwakarma, Aditya Sonpal, and Johannes Hachmann. Metrics for benchmarking and uncertainty quantification: Quality, applicability, and best practices for machine learning in chemistry. Trends in Chemistry, 3(2):146\u2013156, 2021.   \n[399] Sophia J. Wagner, Daniel Reisenb\u00fcchler, Nicholas P. West, Jan Moritz Niehues, Jiefu Zhu, Sebastian Foersch, Gregory Patrick Veldhuizen, Philip Quirke, Heike I. Grabsch, Piet A. van den Brandt, Gordon G. A. Hutchins, Susan D. Richman, Tanwei Yuan, Rupert Langer, Josien C. A. Jenniskens, Kelly Offermans, Wolfram Mueller, Richard Gray, Stephen B. Gruber, Joel K. Greenson, Gad Rennert, Joseph D. Bonner, Daniel Schmolze, Jitendra Jonnagaddala, Nicholas J. Hawkins, Robyn L. Ward, Dion Morton, Matthew Seymour, Laura Magill, Marta Nowak, Jennifer Hay, Viktor H. Koelzer, David N. Church, David Church, Enric Domingo, Joanne Edwards, Bengt Glimelius, Ismail Gogenur, Andrea Harkin, Jen Hay, Timothy Iveson, Emma Jaeger, Caroline Kelly, Rachel Kerr, Noori Maka, Hannah Morgan, Karin Oien, Clare Orange, Claire Palles, Campbell Roxburgh, Owen Sansom, Mark Saunders, Ian Tomlinson, Christian Matek, Carol Geppert, Chaolong Peng, Cheng Zhi, Xiaoming Ouyang, Jacqueline A. James, Maurice B. Loughrey, Manuel Salto-Tellez, Hermann Brenner, Michael Hoffmeister, Daniel Truhn, Julia A. Schnabel, Melanie Boxberg, Tingying Peng, and Jakob Nikolas Kather. Transformer-based biomarker prediction from colorectal cancer histology: A large-scale multicentric study, September 2023. Publisher: Elsevier.   \n[400] Guanghui Wang, Ming Yang, Lijun Zhang, and Tianbao Yang. Momentum accelerates the convergence of stochastic auprc maximization. In International Conference on Artificial Intelligence and Statistics, pages 3753\u20133771. PMLR, 2022.   \n[401] Ruoyu Wang, Syed Ali Khurram, Amina Asif, Lawrence Young, and Nasir Rajpoot. Rank the triplets: A ranking-based multiple instance learning framework for detecting hpv infection in head and neck cancers using routine h&e images, 2022.   \n[402] Shirly Wang, Matthew BA McDermott, Geeticka Chauhan, Marzyeh Ghassemi, Michael C Hughes, and Tristan Naumann. Mimic-extract: A data extraction, preprocessing, and representation pipeline for mimic-iii. In Proceedings of the ACM conference on health, inference, and learning, pages 222\u2013235, 2020.   \n[403] Siyue Wang, Giles R. S. Atkinson, and Wayne B. Hayes. Sana: cross-species prediction of gene ontology go annotations via topological network alignment. npj Systems Biology and Applications, 8(1), July 2022.   \n[404] Wei-Chen Wang and Jonas Mueller. Detecting label errors in token classification data. arXiv preprint arXiv:2210.03920, 2022.   \n[405] Xuhong Wang, Ying Du, Shijie Lin, Ping Cui, Yuntian Shen, and Yupu Yang. advae: A self-adversarial variational autoencoder with gaussian anomaly prior knowledge for anomaly detection. Knowledge-Based Systems, 190:105187, 2020.   \n[406] Yaqing Wang, Yifan Ethan Xu, Xian Li, Xin Luna Dong, and Jing Gao. Automatic validation of textual attribute values in e-commerce catalog by learning with limited labeled data. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 2533\u20132541, 2020.   \n[407] Michael Weiss and Paolo Tonella. Fail-safe execution of deep learning based systems through uncertainty monitoring. In 2021 14th IEEE conference on software testing, verification and validation (ICST), pages 24\u201335. IEEE, 2021.   \n[408] Michael Weiss and Paolo Tonella. Uncertainty quantification for deep neural networks: An empirical comparison and usage guidelines. Software Testing, Verification and Reliability, 33(6):e1840, 2023.   \n[409] Peisong Wen, Qianqian Xu, Zhiyong Yang, Yuan He, and Qingming Huang. Exploring the algorithm-dependent generalization of auprc optimization with list stability. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 28335\u201328349. Curran Associates, Inc., 2022.   \n[410] Fiorella Wever, T Anderson Keller, Laura Symul, and Victor Garcia. As easy as apc: overcoming missing data and class imbalance in time series with self-supervised learning. 2nd Workshop on Self-Supervised Learning: Theory and Practice of the 35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia., 2021.   \n[411] Julian Wiederer, Julian Schmidt, Ulrich Kressel, Klaus Dietmayer, and Vasileios Belagiannis. A benchmark for unsupervised anomaly detection in multi-agent trajectories. In 2022 IEEE 25th International Conference on Intelligent Transportation Systems (ITSC), pages 130\u2013137. IEEE, 2022.   \n[412] Sarah Wiegreffe, Edward Choi, Sherry Yan, Jimeng Sun, and Jacob Eisenstein. Clinical concept extraction for document-level coding. In Dina Demner-Fushman, Kevin Bretonnel Cohen, Sophia Ananiadou, and Junichi Tsujii, editors, Proceedings of the 18th BioNLP Workshop and Shared Task, pages 261\u2013272, Florence, Italy, August 2019. Association for Computational Linguistics.   \n[413] Linda F Wightman. Lsac national longitudinal bar passage study. lsac research report series. 1998.   \n[414] Minz Won, Keunwoo Choi, and Xavier Serra. Semi-supervised music tagging transformer, 2021.   \n[415] Minz Won, Sanghyuk Chun, and Xavier Serra. Toward interpretable music tagging with self-attention, 2019.   \n[416] Minz Won, Andres Ferraro, Dmitry Bogdanov, and Xavier Serra. Evaluation of cnn-based automatic music tagging models. arXiv preprint arXiv:2006.00751, 2020.   \n[417] Peng Wu, Jing Liu, Yujia Shi, Yujia Sun, Fangtao Shao, Zhaoyang Wu, and Zhiwei Yang. Not only look, but also listen: Learning multimodal violence detection under weak supervision. In Proceedings of the European Conference on Computer Vision, pages 322\u2013339. Springer, 2020.   \n[418] Yifan Wu, Min Zeng, Ying Yu, Yaohang Li, and Min Li. A pseudo label-wise attention network for automatic icd coding. IEEE Journal of Biomedical and Health Informatics, 26(10):5201\u20135212, 2022.   \n[419] Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learning. Chemical science, 9(2):513\u2013530, 2018.   \n[420] Chengyuan Xu, Curtis McCully, Boning Dong, D Andrew Howell, and Pradeep Sen. Cosmicconn: A cosmic-ray detection deep-learning framework, data set, and toolkit. The Astrophysical Journal, 942(2):73, 2023.   \n[421] Nuo Xu, Pinghui Wang, Long Chen, Jing Tao, and Junzhou Zhao. Mr-gnn: multi-resolution and dual graph neural network for predicting structured entity interactions. In Proceedings of the 28th International Joint Conference on Artificial Intelligence, IJCAI\u201919, page 3968\u20133974. AAAI Press, 2019.   \n[422] Weihuang Xu, Guohao Yu, Alina Zare, Brendan Zurweller, Diane L. Rowland, Joel ReyesCabrera, Felix B. Fritschi, Roser Matamala, and Thomas E. Juenger. Overcoming small minirhizotron datasets using transfer learning. Computers and Electronics in Agriculture, 175:105466, August 2020.   \n[423] Yanhua Xu and Dominik Wojtczak. Multi-channel neural networks for predicting influenza a virus hosts and antigenic types. arXiv preprint arXiv:2206.03823, 2022.   \n[424] Tianbao Yang. Deep auc maximization for medical image classification: Challenges and opportunities, 2021.   \n[425] Xinxing Yang, Genke Yang, and Jian Chu. The computational drug repositioning without negative sampling. IEEE/ACM Transactions on Computational Biology and Bioinformatics, 20(2):1506\u20131517, 2022.   \n[426] Xinxing Yang, Genke Yang, and Jian Chu. The neural metric factorization for computational drug repositioning. IEEE/ACM Transactions on Computational Biology and Bioinformatics, 20(1):731\u2013741, 2022.   \n[427] Xinxing Yang, Genke Yang, and Jian Chu. Self-supervised learning for label sparsity in computational drug repositioning. IEEE/ACM Transactions on Computational Biology and Bioinformatics, 2023.   \n[428] Yang Yang, Ryan N Lichtenwalter, and Nitesh V Chawla. Evaluating link prediction methods. Knowledge and Information Systems, 45:751\u2013782, 2015.   \n[429] Zi-Yi Yang, Zhao-Feng Ye, Yi-Jia Xiao, Chang-Yu Hsieh, and Sheng-Yu Zhang. Spldextratrees: robust machine learning approach for predicting kinase inhibitor resistance. Briefings in Bioinformatics, 23(3):bbac050, 2022.   \n[430] Shota Yasui, Gota Morishita, Fujita Komei, and Masashi Shibata. A feedback shift correction in predicting conversion rates under delayed feedback. In Proceedings of The Web Conference 2020, pages 2740\u20132746, 2020.   \n[431] Chun-Kit Yeung and Dit-Yan Yeung. Incorporating features learned by an enhanced deep knowledge tracing model for stem/non-stem job prediction. International Journal of Artificial Intelligence in Education, 29(3):317\u2013341, 2019.   \n[432] Xin Yi, Scott J Adams, Robert DE Henderson, and Paul Babyn. Computer-aided assessment of catheters and tubes on radiographs: How good is artificial intelligence for assessment? Radiology: Artificial Intelligence, 2(1):e190082, 2020.   \n[433] Yilin Yin and Chun-An Chou. Early icu mortality prediction and survival analysis for respiratory failure, 2021.   \n[434] Shuhan Yuan and Xintao Wu. Deep learning for insider threat detection: Review, challenges and opportunities. Computers & Security, 104:102221, 2021.   \n[435] Yan Yuan, Wanhua Su, and Mu Zhu. Threshold-free measures for assessing the performance of medical screening tests. Frontiers in public health, 3:57, 2015.   \n[436] Mubariz Zaffar, Sourav Garg, Michael Milford, Julian Kooij, David Flynn, Klaus McDonaldMaier, and Shoaib Ehsan. Vpr-bench: An open-source visual place recognition evaluation framework with quantifiable viewpoint and appearance change. International Journal of Computer Vision, 129(7):2136\u20132174, May 2021.   \n[437] Roee Zamir, Shai Bagon, David Samocha, Yael Yagil, Ronen Basri, Miri Sklair-Levy, and Meirav Galun. Segmenting microcalcifications in mammograms and its applications. In Medical Imaging 2021: Image Processing, volume 11596, pages 788\u2013795. SPIE, 2021.   \n[438] Vitjan Zavrtanik, Matej Kristan, and Danijel Sko\u02c7caj. Draem-a discriminatively trained reconstruction embedding for surface anomaly detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8330\u20138339, 2021.   \n[439] Vitjan Zavrtanik, Matej Kristan, and Danijel Sko\u02c7caj. Dsr\u2013a dual subspace re-projection network for surface anomaly detection. In European conference on computer vision, pages 539\u2013554. Springer, 2022.   \n[440] Ido Zehori, Nevo Itzhak, Yuval Shahar, and Mia Dor Schiller. Towards a user privacy-aware mobile gaming app installation prediction model, 2023.   \n[441] Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. Mitigating unwanted biases with adversarial learning. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, pages 335\u2013340, 2018.   \n[442] Chaohe Zhang, Xu Chu, Liantao Ma, Yinghao Zhu, Yasha Wang, Jiangtao Wang, and Junfeng Zhao. M3care: Learning with missing modalities in multimodal healthcare data. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD \u201922. ACM, August 2022.   \n[443] Chaohe Zhang, Xin Gao, Liantao Ma, Yasha Wang, Jiangtao Wang, and Wen Tang. Grasp: Generic framework for health status representation learning based on incorporating knowledge from similar patients. Proceedings of the AAAI Conference on Artificial Intelligence, 35(1):715\u2013 723, May 2021.   \n[444] Dingwen Zhang, Huazhu Fu, Junwei Han, Ali Borji, and Xuelong Li. A review of co-saliency detection technique: Fundamentals, applications, and challenges, 2017.   \n[445] Weijia Zhang. Non-i.i.d. multi-instance learning for predicting instance and bag labels using variational auto-encoder, 2021.   \n[446] Weijia Zhang, Xuanhui Zhang, hanwen deng, and Min-Ling Zhang. Multi-instance causal representation learning for instance label prediction and out-of-distribution generalization. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 34940\u201334953. Curran Associates, Inc., 2022.   \n[447] Wencan Zhang, Mariella Dimiccoli, and Brian Y Lim. Debiased-cam to mitigate image perturbations with faithful visual explanations of machine learning. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems, pages 1\u201332, 2022.   \n[448] Wenning Zhang, Ryohei Hisano, Takaaki Ohnishi, and Takayuki Mizuno. Nondiagonal mixture of dirichlet network distributions for analyzing a stock ownership network. In Complex Networks & Their Applications IX: Volume 1, Proceedings of the Ninth International Conference on Complex Networks and Their Applications COMPLEX NETWORKS 2020, pages 75\u201386. Springer, 2021.   \n[449] Xuan Zhang, Shiyu Li, Xi Li, Ping Huang, Jiulong Shan, and Ting Chen. Destseg: Segmentation guided denoising student-teacher for anomaly detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3914\u20133923, 2023.   \n[450] Yan Zhang and Howard D. Bondell. Variable Selection via Penalized Credible Regions with Dirichlet\u2013Laplace Global-Local Shrinkage Priors. Bayesian Analysis, 13(3):823 \u2013 844, 2018.   \n[451] Qian M. Zhou, Zhe Lu, Russell J. Brooke, Melissa M Hudson, and Yan Yuan. Is the new model better? one metric says yes, but the other says no. which metric do i use?, 2020.   \n[452] Yi Zhou, Boyang Wang, Lei Huang, Shanshan Cui, and Ling Shao. A benchmark for studying diabetic retinopathy: Segmentation, grading, and transferability. IEEE Transactions on Medical Imaging, 40(3):818\u2013828, March 2021.   \n[453] Qianqian Zou, Claus Brenner, and Monika Sester. Gaussian process mapping of uncertain building models with gmm as prior. IEEE Robotics and Automation Letters, 8(10):6579\u20136586, 2023.   \n[454] Yang Zou, Jongheon Jeong, Latha Pemula, Dongqing Zhang, and Onkar Dabeer. Spot-thedifference self-supervised pre-training for anomaly detection and segmentation. In European Conference on Computer Vision, pages 392\u2013408. Springer, 2022.   \n[455] Ali Burak \u00dcnal, Nico Pfeifer, and Mete Akg\u00fcn. ppaurora: Privacy preserving area under receiver operating characteristic and precision-recall curves, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 38}, {"type": "text", "text": "A Broader Impact and Ethical Considerations ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "This research paper challenges the conventional wisdom regarding the superiority of the AUPRC over AUROC in binary classification tasks with class imbalance and has several ethical implications and impacts. ", "page_idx": 39}, {"type": "text", "text": "Our analysis reveals that the preference for AUPRC in certain ML applications may not be empirically justified and could inadvertently amplify algorithmic biases. This calls for a re-examination of prevalent metrics within ML, especially in high-stakes domains like healthcare, finance, and criminal justice where biased models can have profound societal repercussions. The tendency of AUPRC to disproportionately favor models with higher prevalence of positive labels could exacerbate existing disparities, underscoring the ethical need for rigorous validation and scrutiny of evaluation metrics. ", "page_idx": 39}, {"type": "text", "text": "Additionally, our use of large language models for literature analysis demonstrates a novel approach in scrutinizing and re-evaluating long-standing assumptions in ML. This method could set a precedent for more comprehensive and robust scientific investigations in the field, fostering a culture of empirical rigor and ethical awareness. ", "page_idx": 39}, {"type": "text", "text": "The ethical dimension of our work lies in the spotlight it casts on metric selection in ML model evaluation. The potential of metrics like AUPRC to skew model performance favoring certain groups raises pressing concerns about fairness in algorithmic decision-making. This is particularly critical when algorithms influence key decisions affecting individuals and communities. ", "page_idx": 39}, {"type": "text", "text": "While we use the COMPAS dataset for recividism prediction in this work, we recognize the many societal issues with automated predictions of recidivism [93]. We utilize this dataset as it is a commonly used dataset in the fairness literature, but do not advocate for deployment of these models in any way. ", "page_idx": 39}, {"type": "text", "text": "Our study contributes to the technical discourse on metric behaviors in ML and serves as a cautionary tale against uncritically embracing established norms. It underscores the imperative for careful metric selection aligned with ethical principles and fairness objectives in ML, highlighting the far-reaching consequences of these choices in shaping societal outcomes and advancing the field of ML. ", "page_idx": 39}, {"type": "text", "text": "B Code Availability ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "All code is available at https://github.com/hzhang0/auc_bias and https://github.com/ Lassehhansen/ArxivMLClaimSearch. ", "page_idx": 39}, {"type": "text", "text": "C Notation ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Let $\\mathcal{X},\\mathcal{Y}=0,1$ represent a paired feature and binary classification label space from which i.i.d. samples $(x,y)\\in\\mathcal{X}\\times\\mathcal{Y}$ are drawn via the joint distribution over the random variables $\\times$ , y. Let $f_{\\theta}:\\mathcal{X}\\to(0,1)$ be a binary classification model parametrized by $\\pmb{\\theta}\\in\\mathbb{R}^{d}$ for some $d\\in\\mathbb N$ outputting continuous probability scores over this space. ", "page_idx": 39}, {"type": "text", "text": "We define random variable $\\mathsf{s}=f_{\\theta}(\\mathsf{x})$ to be the distribution of scores output by the model over input samples. Throughout the paper, $\\pmb{\\theta}$ may be omitted if it is clear from context. We will occasionally also use the notation $\\mathsf{S}_{+}$ and $s_{-}$ to reflect the conditional distributions of model outputs conditioned on the label being 1 or 0, respectively: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\mathfrak{s}_{+}=f(\\mathsf{x})|\\mathsf{y}=1}}\\\\ {{\\mathfrak{s}_{-}=f(\\mathsf{x})|\\mathsf{y}=0.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Let $\\mathrm{N_{P}}$ be the number of data points with a positive label and $\\mathrm{N}_{\\mathrm{N}}$ the number with a negative label. Further, given a threshold $\\tau$ , define ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Tr}_{0}(r)=\\left[\\langle n,c\\rangle\\langle n^{0}\\,z>\\gamma_{0}\\times1\\right]}\\\\ &{\\mathrm{PSG}(t)=\\left[\\langle n,c\\rangle\\langle n^{0}\\,c>n_{0}-1\\right]}\\\\ &{\\mathrm{TNG}(t)=\\left[\\langle n,c\\rangle\\langle n^{0}\\,c>n_{0}-0\\right]}\\\\ &{\\mathrm{TNG}(t)=\\left[\\langle n,c\\rangle\\langle n^{0}\\,c>\\gamma_{0}\\times-n_{0}\\right]}\\\\ &{\\mathrm{PSG}(t)=\\left[\\langle n,c\\rangle\\langle n^{0}\\,c>n_{0}-0\\right]}\\\\ &{\\mathrm{PSG}(t)=\\gamma_{0}\\times\\gamma_{1}\\langle n^{0}\\,c>n_{0}\\rangle}\\\\ &{\\mathrm{TRG}(t)=\\frac{-1}{\\gamma_{0}\\times1}\\langle n^{0}\\,c\\rangle\\langle n^{0}\\,c>n_{0}}\\\\ &{\\mathrm{TRG}(t)=\\frac{-1}{\\gamma_{0}\\times1}\\langle n^{0}\\,c>n^{0}\\rangle}\\\\ &{=-\\frac{\\gamma_{0}\\times1}{\\gamma_{0}\\times1}\\langle n^{0}\\,c>n^{0}\\rangle}\\\\ &{\\mathrm{TRG}(t)=\\frac{-1}{\\gamma_{0}\\times1}\\frac{\\gamma_{1}\\langle n^{0}\\,c\\rangle\\langle n^{0}\\,c\\rangle}{\\gamma_{0}\\times1}}\\\\ &{=E_{\\mathrm{BG}}(t)=\\frac{\\gamma_{1}}{\\gamma_{0}\\times1}\\langle n^{0}\\,c\\rangle}\\\\ &{=E_{\\mathrm{BG}}(t)=\\frac{\\gamma_{1}}{\\gamma_{0}\\times1}\\langle n^{0}\\,c\\rangle}\\\\ &{=E_{\\mathrm{BG}}(t)=\\frac{\\gamma_{1}}{\\gamma_{0}\\times1}\\langle n^{0}\\,c\\rangle}\\\\ &{=E_{\\mathrm{BG}}(t)=\\frac{\\gamma_{1}}{\\gamma_{0}\\times1}\\langle n^{1}\\,c\\rangle}\\\\ &{=E_{\\mathrm{BG}}(t)=\\frac{\\gamma_{1}}{\\gamma_{0}\\times1}\\langle n^{1}\\,c\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Lastly, recall ", "page_idx": 40}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{{\\mathrm{AUROC}}_{\\theta}=\\int_{0}^{1}{\\mathrm{TPR}}_{\\theta}{\\frac{d{\\mathrm{FPR}}_{\\theta}}{d\\tau}}d\\tau}\\\\ &{=\\int_{0}^{1}{\\mathrm{TPR}}_{\\theta}d{\\mathrm{FPR}}_{\\theta}}\\\\ &{=1-\\int_{0}^{1}{\\mathrm{FPR}}_{\\theta}d\\Gamma{\\mathrm{PR}}_{\\theta}}\\\\ &{{\\mathrm{AUPRC}}_{\\theta}=\\int_{0}^{1}{\\mathrm{Free}}_{\\theta}{\\frac{d\\Gamma{\\mathrm{PR}}_{\\theta}}{d\\tau}}d\\tau}\\\\ &{=\\int_{0}^{1}{\\mathrm{Free}}_{\\theta}d\\Gamma{\\mathrm{PR}}_{\\theta}}\\end{array}}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "D Proof of Theorem 1 ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Recall that all notation is defined formally in Appendix C. ", "page_idx": 40}, {"type": "text", "text": "Here, we prove Theorem 1, which states ", "page_idx": 40}, {"type": "text", "text": "Theorem 1. Let $\\mathcal{X},\\mathcal{Y}=0,1$ represent a paired feature and binary classification label space from which i.i.d. samples $(x,y)\\in\\mathcal{X}\\times\\mathcal{Y}$ are drawn via the joint distribution over the random variables $\\times,\\ y$ . Let $f:\\mathcal{X}\\to(0,1)$ be a binary classification model outputting continuous probability scores ", "page_idx": 40}, {"type": "text", "text": "over this space. Then, ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{AUROC}(f)=1-\\mathbb{E}_{t\\sim f(\\mathbf{x})\\mid\\mathbf{y}=1}\\left[\\mathrm{FPR}(f,t)\\right]}\\\\ &{\\mathrm{AUPRC}(f)=1-P_{\\mathrm{y}}(y=0)\\mathbb{E}_{t\\sim f(\\mathbf{x})\\mid\\mathbf{y}=1}\\left[\\frac{\\mathrm{FPR}(f,t)}{P_{\\mathbf{x}}(f(\\mathbf{x})>t)}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Proof. Recall that AUROC and AUPRC are as follows: ", "page_idx": 41}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{{\\mathrm{AUROC}}=\\int_{0}^{1}{\\mathrm{TPR}}\\;d{\\mathrm{FPR}}=1-\\int_{0}^{1}{\\mathrm{FPR}}\\;d{\\mathrm{TPR}}}\\\\ &{{\\mathrm{AUPRC}}=\\int_{0}^{1}{\\mathrm{Prec}}\\;d{\\mathrm{TPR}}}\\end{array}}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "However, we can further clarify these by leveraging the fact that $\\mathrm{TPR}(\\tau)\\,=\\,P_{\\mathsf{s}_{+}}(s_{+}\\,>\\,\\tau)\\,=$ $\\textstyle\\int_{\\tau}^{1}s_{+}(t)d t$ , as below: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\int_{0}^{1}g(\\tau)d(\\mathrm{TPR}(\\tau))=\\int_{1}^{0}g(\\tau)\\frac{d\\mathrm{TPR}(\\tau)}{d\\tau}d\\tau}}\\\\ &{}&{=\\int_{1}^{0}g(\\tau)\\frac{d}{d\\tau}(P_{\\mathrm{s+}}(s_{+}>\\tau))d\\tau}\\\\ &{}&{=\\int_{1}^{0}g(\\tau)\\frac{d}{d\\tau}\\left(\\int_{\\tau}^{1}s_{+}(t)d t\\right)d\\tau}\\\\ &{}&{=\\int_{1}^{0}g(\\tau)(-s_{+}(\\tau))d\\tau}\\\\ &{}&{=\\mathbb{E}_{\\mathrm{s_{+}}}|g|}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "So, $,\\mathrm{AUROC}=1-\\mathbb{E}_{\\mathsf{s}_{+}}$ [FPR] & $\\mathrm{AUPRC}=\\mathbb{E}_{\\mathsf{s}_{+}}$ [Prec]. To further simplify, we expand Prec via Bayes rule: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Prec}=1-P_{\\mathrm{y}|\\mathsf{s}>\\tau}(y=0)}\\\\ &{\\qquad=1-\\underbrace{P_{\\mathsf{s}|\\mathsf{y}=0}(s>\\tau)}_{\\mathrm{FPR}(\\tau)}\\frac{P_{\\mathrm{y}}(y=0)}{P_{\\mathsf{s}}(s>\\tau)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Thus, ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{AUROC}(f)=1-\\mathbb{E}_{t\\sim\\mathsf{s}_{+}}\\left[\\mathrm{FPR}(f,t)\\right]}\\\\ &{\\phantom{\\mathrm{AUROC}(f)=1-\\mathbb{E}_{t\\sim f(\\times)\\vert\\forall=1}}}\\\\ &{\\mathrm{AUPRC}(f)=\\mathbb{E}_{t\\sim\\mathsf{s}_{+}}\\left[\\mathrm{Prec}(f,t)\\right]}\\\\ &{\\phantom{\\mathrm{AUPRC}(f)=1-\\mathbb{P}_{\\eta}}=1-P_{\\mathrm{y}}(y=0)\\mathbb{E}_{t\\sim s_{+}}\\left[\\frac{\\mathrm{FPR}(f,t)}{P_{s\\sim s}(s>t)}\\right]}\\\\ &{\\phantom{\\mathrm{AUPRC}(f)=1-\\mathbb{P}_{\\eta}}=1-P_{\\mathrm{y}}(y=0)\\mathbb{E}_{t\\sim f(\\times)\\vert\\forall=1}\\left[\\frac{\\mathrm{FPR}(f,t)}{P_{\\mathrm{x}}(f(x)>t)}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "as desired. ", "page_idx": 41}, {"type": "text", "text": "Synthetic validation of Theorem 1 can also be found in our public code. Note that this formulation of AUPRC reflects earlier, different formulations of AUPRC, such as those found in the AUPRC optimization literature [409]. ", "page_idx": 41}, {"type": "text", "text": "E Proof of Theorem 2 ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Here, we prove Theorem 2, which states ", "page_idx": 42}, {"type": "text", "text": "Theorem 2. Define $f,\\mathcal{X},X,y$ and $N$ as in Definition 2.1. Further, suppose without loss of generality that the dataset $\\mathbf{\\deltaX}$ is ordered such that $f(x_{i})~<~f(x_{i+1})$ for all $i$ . Then, let us define $M\\,=$ $\\{i|(x_{i},x_{i+1})$ is an incorrectly ranked adjacent pair for model $f\\}$ . Define $f_{i}^{\\prime}$ to be a model that is identical to $f$ except that the probabilities assigned to $x_{i}$ and $x_{i+1}$ are swapped: ", "page_idx": 42}, {"type": "equation", "text": "$$\nf_{i}^{\\prime}:{\\left\\{\\begin{array}{l l}{f(x)}&{{\\mathrm{if~}}x\\not\\in\\{x_{i},x_{i+1}\\}}\\\\ {f(x_{i+1})}&{{\\mathrm{if~}}x=x_{i}}\\\\ {f(x_{i})}&{{\\mathrm{if~}}x=x_{i+1}.}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Then, $\\mathrm{AUROC}(f_{i}^{\\prime})\\,=\\,\\mathrm{AUROC}(f_{j}^{\\prime})$ for all $i,j\\;\\in\\;M$ , and $\\mathrm{AUPRC}(f_{i}^{\\prime})\\,<\\,\\mathrm{AUPRC}(f_{j}^{\\prime})$ for all $i,j\\in M$ such that $i<j$ . ", "page_idx": 42}, {"type": "text", "text": "Proof. Suppose $f$ has a given, non-empty set $M$ of atomic mistakes, such that, without loss of generality, $(x_{i},x_{i+1})\\in M$ . Suppose we construct a new model $f^{\\prime}$ with empirical distributions $p_{+}^{\\prime}$ and $p_{-}^{\\prime}$ by replicating the scores assigned by the model $f$ with $x_{i}$ and $x_{i+1}$ swapped (i.e., we correct the mistake $(x_{i},x_{i+1})$ , so $x_{i}^{\\prime}=x_{i+1}$ and $x_{i+1}^{\\bar{\\prime}}=x_{i}\\,$ ). ", "page_idx": 42}, {"type": "text", "text": "For which thresholds drawn from the original distribution $\\mathsf{p}_{+}$ will the number of false positives of $f^{\\prime}$ differ from the number of false positives of $f$ at that same threshold? For any threshold $\\tau<x_{i}$ , fixing the mistake $(x_{i},x_{i+1})$ will not change the number of false positives with threshold $\\tau$ , because both $x_{i}$ and $x_{i+1}$ are above $\\tau$ . For any threshold $\\tau>x_{i+1}$ , the number will likewise not change as both $x_{i}$ and $x_{i+1}$ are below $\\tau$ . The only $\\tau$ that will have an impact is $\\tau=x_{i}$ (recall that this is for an empirical distribution $p_{+}$ which contains $x_{i}$ and by the definition of atomic mistakes, there are no samples in $f$ with scores between $x_{i}$ and $x_{i+1}$ ). In $f$ , the fact that $x_{i+1}>x_{i}$ yet has a negative label means that there will be one false positive corresponding to sample $i+1$ greater than $x_{i}$ in addition to all those that exist with scores greater than $x_{i+1}$ . For $f^{\\prime}$ , however, the samples have swapped, so $x_{i}^{\\prime}>x_{i+1}^{\\prime}$ and thus there is no false positive corresponding to sample $i+1$ at the positive score threshold corresponding to $\\ensuremath{\\boldsymbol{{x}}}_{i}^{\\prime}$ . Therefore, the number of false positives will only change to decrease by one for the threshold $x_{i}$ when the mistake $(x_{i},x_{i+1})$ is corrected. ", "page_idx": 42}, {"type": "text", "text": "As AUROC weights the false positive rate at all positive samples equally and the false positive rate is proportional to the number of false positives, this shows that AUROC will improve by a constant amount no matter which atomic mistake is fixed. In contrast, as AUPRC weights false positives inversely by the model\u2019s firing rate, it will improve by an amount that is directly linearly correlated with the inverse of the model\u2019s firing rate, implying that it favors mistakes with higher scores and disfavors mistakes with lower scores. ", "page_idx": 42}, {"type": "text", "text": "Note that as we use strict inequalities in our definition of the decision rule underlying the FPR here, a pair of scores that are tied but have different labels will not induce a false positive at the corresponding positively labeled sample\u2019s threshold, so separating such ties will have no impact on AUROC whatsoever. It would similarly not impact AUPRC as neither the FPR nor the model firing rate will decrease when the negative sample within the tie is perturbed to be strictly below the positive sample. \u53e3 ", "page_idx": 42}, {"type": "text", "text": "Synthetic empirical validation of Theorem 2 can also be found in our public code. ", "page_idx": 42}, {"type": "text", "text": "F Proof of Theorem 3 ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "In this section, we formally prove Theorem 2. We begin by establishing Lemma 1 and 2. ", "page_idx": 42}, {"type": "text", "text": "Lemma 1. Let a model $f$ be perfectly calibrated and yield score distributions for positive and negative samples from probability density functions $p_{+}$ and $p_{-}$ . Then $\\begin{array}{r}{p_{+}(t)=\\frac{t}{1-t}\\frac{P_{y}(y=0)}{P_{y}(y=1)}p_{-}(t)}\\end{array}$ ", "page_idx": 42}, {"type": "text", "text": "Proof. As this model is calibrated perfectly, we have that ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p_{+}(t)=P_{s|y=1}(s=t)}\\\\ &{\\quad\\quad=\\cfrac{P_{y|s=t}(y=1)p_{s}(t)}{P_{y}(y=1)}}\\\\ &{\\quad\\quad=t\\cfrac{P_{y}(y=1)p_{+}(t)+P_{y}(y=0)p_{-}(t)}{P_{y}(y=1)}}\\\\ &{\\quad\\quad=t p_{+}(t)+t\\cfrac{P_{y}(y=0)}{P_{y}(y=1)}p_{-}(t).}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Thus, $\\begin{array}{r}{p_{+}(t)=\\frac{t}{1-t}\\frac{P_{\\mathrm{y}}(y=0)}{P_{\\mathrm{y}}(y=1)}p_{-}(t)}\\end{array}$ as desired. ", "page_idx": 43}, {"type": "text", "text": "Lemma 2. Let a model $f$ be perfectly calibrated and yield score distributions for positive and negative samples from probability density functions $p_{+}$ and $p_{-}$ , with overall distribution given by $p(t)=P_{\\mathrm{y}}(y=1)p_{+}(t)+P_{\\mathrm{y}}(y=0)p_{-}(t)$ . Then for all $\\tau\\in(0,1)$ , $\\begin{array}{r}{\\mathrm{FR}(f,\\tau)\\leq\\frac{P_{\\mathrm{y}}(y=1)}{\\tau}}\\end{array}$ . ", "page_idx": 43}, {"type": "text", "text": "Proof. By definition, we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathrm{FR}(f,\\tau)=\\int_{\\tau}^{1}P_{\\mathrm{y}}(y=1)p_{+}(t)+P_{\\mathrm{y}}(y=0)p_{-}(t)d t}\\\\ {\\displaystyle=\\int_{\\tau}^{1}P_{\\mathrm{y}}(y=1)p_{+}(t)+P_{\\mathrm{y}}(y=1)\\frac{1-t}{t}p_{+}(t)d t}\\\\ {\\displaystyle=P_{\\mathrm{y}}(y=1)\\int_{\\tau}^{1}\\frac{1}{t}p_{+}(t)d t,}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where step two leverages the fact that $f$ is perfectly calibrated and the result in Lemma 1. ", "page_idx": 43}, {"type": "text", "text": "A $\\begin{array}{r}{\\mathrm{~s~}t\\geq\\tau,\\,\\frac{1}{t}\\,\\leq\\,\\frac{1}{\\tau}}\\end{array}$ . Then, as $p_{+}(t)\\geq0$ $\\begin{array}{r}{\\geq0,\\int_{\\tau}^{1}\\frac{1}{t}p_{+}(t)d t\\leq\\frac{1}{\\tau}\\int_{\\tau}^{1}p_{+}(t)d t}\\end{array}$ . Finally, as $\\begin{array}{r}{\\int_{0}^{1}p_{+}(t)d t=1}\\end{array}$ , we see that $\\begin{array}{r}{\\int_{\\tau}^{1}p_{+}(t)d t\\leq1}\\end{array}$ . Therefore, ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathrm{FR}(f,\\tau)=P_{\\mathrm{y}}(y=1)\\displaystyle\\int_{\\tau}^{1}\\frac{1}{t}p_{+}(t)d t}\\\\ {\\displaystyle\\leq P_{\\mathrm{y}}(y=1)\\cdot\\frac{1}{\\tau}\\cdot1}\\\\ {\\displaystyle=\\frac{P_{\\mathrm{y}}(y=1)}{\\tau}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Theorem 3. Let $f,\\boldsymbol{\\mathcal{X}},\\boldsymbol{X},\\boldsymbol{\\boldsymbol{y}},N,M$ , and $f_{j}^{\\prime}$ all be defined as in Theorem 2. Further, suppose that in this setting the domain $\\mathcal{X}$ now contains an attribute defining two subgroups, $A=\\{0,1\\}$ , such that for any sample $\\left({x_{i},y_{i}}\\right)$ , $a_{i}$ denotes the subgroup to which that sample belongs. Let $f$ be perfectly calibrated for samples in subgroup $a=0$ , such that $P_{\\mathrm{y|a,x}}(y=1|a\\stackrel{\\cdot}{=}0,f(x)\\stackrel{\\cdot}{=}t)=\\dot{t}.$ . Then, ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{P_{\\gamma|\\mathfrak{a}}(y=1|a=0)\\to0}P\\left(a_{i}=a_{i+1}=1\\left|i=\\underset{j\\in M}{\\arg\\operatorname*{max}}\\left(\\mathrm{AUPRC}(f_{j}^{\\prime})\\right)\\right)=1.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Proof. Given Theorem 2, the atomic mistake that would, upon correction, result in the largest improvement to AUPRC is the mistake which occurs at maximal score (as this minimizes the firing rate, which is the denominator in the weighting term for AUPRC). Suppose that at threshold $\\tau$ , the probability that a mistake will occur above score $\\tau$ in subgroup 1 with $N$ samples drawn is at least $\\bar{\\boldsymbol\\delta}\\in(0,1]$ . As the parameters for subgroup 1 are fixed as we vary the prevalence for subgroup 2, $\\tau$ can be seen as a constant with respect to the limit we are taking. ", "page_idx": 43}, {"type": "text", "text": "But, by Lemma 2 and by the fact that $f$ is perfectly calibrated for subgroup 2, we know that the probability that $f$ will output a score for sample 2 regardless of its label that exceeds $\\tau$ is upper bounded by $\\frac{p_{\\mathrm{y}}^{(2)}}{\\tau}$ . In the limit as $p_{\\mathtt{y}}^{(2)}$ tends to zero, the probability that any probabilities will be observed at our greater than $\\tau$ from subgroup 2 likewise tends to zero. ", "page_idx": 43}, {"type": "text", "text": "This means that while the probability that we observe a mistake from subgroup 1 stays fixed at at least $\\delta>0$ , the probability that we could observe any mistake that involves any sample from subgroup 2 (either a cross-group mistake or a purely subgroup 2 mistake) tends to zero, establishing the claim. \u53e3 ", "page_idx": 44}, {"type": "text", "text": "G Details for Synthetic Experiments ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "G.1 Sampling a random model with a given AUROC ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "A key component of our synthetic experiments is the ability to sample a set of model scores and labels randomly that will have a target AUROC. To do this, we use the following procedure (which may or may not be previously known; we derived it from scratch for this work, but make no claim about its novelty). Let $N$ be the number of points we are sampling overall, and $N_{+}$ be the number of positive points being sampled (which is dictated by the user given prevalence). ", "page_idx": 45}, {"type": "text", "text": "1. Uniformly sample a random collection of positive-label sample scores between zero and one.   \n2. Between each (ascending) model positive score indexed from $1\\ p_{+}^{(i)}$ and $p_{+}^{(i+1)}$ , we can count the number of positive samples that have scores less than any value in this window $(i)$ and the number that have scores greater than any value in this window (which will be $N_{+}-i)$ .   \n3. As the target AUROC is the probability that a randomly sampled negative will be ranked more highly than a randomly sampled positive, we can leverage the number of less-than positive scores $i$ and greater than positive scores $N_{+}-i$ to compute the probability that a randomly sampled negative score will live in the window $(p_{+}^{(i)},p_{+}^{(i+1)})$ via the binomial distribution.   \n4. wNiothw ,t htoe  sparombpalbei lai tireasn dasosmig nneedg aatibvoev, e,w teh esin mupnliyf ofrirmstl ys asammplpel ea  ar avnalduoe iwnidtohiwn $(p_{+}^{(i)},p_{+}^{(i+1)})$ $p_{-}$ We can repeat this process to the target number of negative samples $N-N_{+}$ to form our final set of scores.   \n5. If desired, the output scores can further be scaled to have expectation given by the dataset\u2019s prevalence or can be adjusted via a calibration method to be calibrated given the assigned labels. Both procedures can be done without affecting the AUROC. Note that as any calibrated model will have expected probability given by the label\u2019s prevalence (See Appendix G.2), the former condition is strictly weaker than the latter. ", "page_idx": 45}, {"type": "text", "text": "The procedure outlined above guarantees that, in expectation, the AUROC of the generated set of scores and labels will be precisely the target AUROC. However, if you apply this procedure indpendently across different sample subpopulations, this guarantee can only be applied on each subpopulation individually, and not necessarily on the overall population due to the unspecified xAUC term. However, in practice, for the experiments we ran here, that impact neither meaningfully impacts our experiments nor were the joint AUROCs sufficiently different from the target AUROC to warrant a more complex methodology. ", "page_idx": 45}, {"type": "text", "text": "G.2 Calibration includes prevalence matching ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Let $\\mathsf{p}$ be a random variable describing the probabilities (not scores) output by the model over the input distribution defined by the data generative function. If a model is calibrated, this means that $P_{\\mathsf{y l p}}(y=1|p=q)=q$ \u2014 that the probability that the label for a given point is 1 is given precisely by the models output probability for that sample. With that in mind, we have: ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{E}_{\\mathfrak{p}}\\left[q\\right]=\\mathbb{E}_{\\mathfrak{p}}\\left[P_{\\mathfrak{p}|\\mathfrak{p}}(y=1|p=q)\\right]}\\\\ {\\ \\ \\ \\ \\ \\ \\ =\\int_{0}^{1}P_{\\mathfrak{p}|\\mathfrak{p}}(y=1|p=q)p_{\\mathfrak{p}}(q)d q}\\\\ {\\ \\ \\ \\ \\ \\ \\ =\\int_{0}^{1}P_{\\mathfrak{p},\\mathfrak{p}}(y=1,p=q)d q}\\\\ {\\ \\ \\ \\ \\ \\ =P_{\\mathfrak{p}}(y=1)}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "G.3 Details on optimization procedures ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "M1. Adding Random Noise. We sample a vector $\\epsilon\\,\\in\\,\\mathbb{R}^{n}$ , where each element is uniformly drawn from $[-\\delta,\\delta]$ . We compute the selection metric for $S^{\\prime}=\\,S+\\epsilon$ . We repeat this procedure 100 times, and return the $S^{\\prime}$ that achieves the maximum value for the selection metric. We vary the maximum magnitude of the perturbation $\\delta\\in[0,0.1]$ in a grid. Results for this setting are shown in Figure 6. ", "page_idx": 45}, {"type": "text", "text": "", "page_idx": 46}, {"type": "text", "text": "We note that this approach is subtly biased in favor of the lower-prevalence group. In particular, because scores for the low-prevalence group tend to be \u201csquished\u201d into a smaller region of the probability space, a random perturbation of fixed magnitude will proportionally induce more score permutations in the low-prevalence group than the high-prevalence group, which affords the system greater capacity to improve the model for the low-prevalence group independent of the choice of AUROC or AUPRC. ", "page_idx": 46}, {"type": "text", "text": "M2. Sequentially Fixing Atomic Mistakes. We sequentially correct atomic mistakes, as defined in Figure 1. At each step, we first discover the set of all atomic mistakes $M$ . To maximize AUROC, we randomly select a pair $(S_{i},S_{j})\\,\\in\\,M$ , and swap their scores in $S$ , i.e. $S_{i}^{\\prime}\\ =\\ S_{j},S_{j}^{\\prime}\\ =\\ S_{i}$ . To maximize AUPRC, we swap the scores for the pair $(S_{i},S_{j})=\\arg\\operatorname*{max}_{(s_{i},s_{j})\\in M}s_{j}$ . We repeat this process for 50 steps, with each one sequentially fixing another atomic mistake in $S$ . Results for this setting are shown in Figures 4b and 4a. ", "page_idx": 46}, {"type": "text", "text": "M3. Sequentially Permuting Nearby Scores. We first sort $S$ and $Y$ such that $S$ is in ascending order. We apply a random permutation to $S$ by re-indexing it using a random ordering, but such that scores are not shuffled too far from their original index. Let $\\sigma$ be the ordered sequence $(1,2,...,n)$ . Define $\\Omega$ to be the set of all permutations of $\\sigma$ , such that for all $\\omega\\,\\in\\,\\Omega$ , $|\\omega_{i}\\,-\\,\\sigma_{i}|\\,\\leq\\,\\gamma$ for $i\\,\\in\\,\\{1,...,n\\}$ . At each step, we sample $\\omega\\,\\in\\,\\Omega$ with $\\gamma\\,=\\,3$ twenty times, where each $\\omega$ corresponds to a new candidate ordering of $S$ . We compute the selection metric for each of the twenty orderings, and return $S^{\\prime}$ to be the score permutation that achieves the maximum value for the selection metric. We repeat this procedure for 25 steps, setting $S$ at each step to be the $S^{\\prime}$ output from the previous step. Results for this setting are shown in Figures 4d and $4c$ . ", "page_idx": 46}, {"type": "image", "img_path": "S3HvA808gk/tmp/5b39ef50c40c79d286a2b93abc379e18326463d83e8ee2fe09391cb06c2a2daf.jpg", "img_caption": ["(a) Fixing individual mistakes to optimize overall AUROC "], "img_footnote": [], "page_idx": 47}, {"type": "image", "img_path": "S3HvA808gk/tmp/dd5951956d2da7b89424e101c6d06d46794a7fc38cfe13114b7b5934ef754033.jpg", "img_caption": ["(c) Randomly permuting scores to optimize overall AUROC "], "img_footnote": [], "page_idx": 47}, {"type": "image", "img_path": "S3HvA808gk/tmp/788996cd4aad202751ace563fd5a75cf138fe799cd2f67d6788131aab3624f7d.jpg", "img_caption": ["(b) Fixing individual mistakes to optimize overall AUPRC "], "img_footnote": [], "page_idx": 47}, {"type": "image", "img_path": "S3HvA808gk/tmp/56fcb5ff02f0b0a37fd320a39618e63223b6990af01d6cff47446c318006d7b5.jpg", "img_caption": ["(d) Randomly permuting scores to optimize overall AUPRC "], "img_footnote": [], "page_idx": 47}, {"type": "text", "text": "Figure 4: Comparison of the impact of optimizing for overall AUROC and overall AUPRC on the per-group AUROC and AUPRCs of two groups in a synthetic setting, using both the sequentially fixing individual mistakes optimization procedure (M2; top) and the sequentially permuting nearby scores optimization procedure (M3; bottom) described in Section 3.1. Note that the prevalence of $Y$ in the high-prevalence group and the low-prevalence group are 0.05 and 0.01 respectively. ", "page_idx": 47}, {"type": "image", "img_path": "S3HvA808gk/tmp/952dbf056701810056b6b2488057ec2c5ce81d0472fb04091e947e8a782ae5a8.jpg", "img_caption": ["(a) Fixing individual mistakes to optimize overall AUROC "], "img_footnote": [], "page_idx": 48}, {"type": "image", "img_path": "S3HvA808gk/tmp/70ffd8577fa6e4d1c3b81c80453e3372344397f915f398eee07812b232fcc3d9.jpg", "img_caption": ["(c) Randomly permuting scores to optimize overall AUROC "], "img_footnote": [], "page_idx": 48}, {"type": "image", "img_path": "S3HvA808gk/tmp/ff4584346c2a4323c59617529d41a1d03c50567888da02543def4a6df9a7e49c.jpg", "img_caption": ["(b) Fixing individual mistakes to optimize overall AUPRC "], "img_footnote": [], "page_idx": 48}, {"type": "image", "img_path": "S3HvA808gk/tmp/7b02335f1605b0f61b0ce36679866e9c50b2eae32bfa5c64fad4dd2cde0015ec.jpg", "img_caption": ["(d) Randomly permuting scores to optimize overall AUPRC "], "img_footnote": [], "page_idx": 48}, {"type": "text", "text": "Figure 5: Comparison of the impact of optimizing for overall AUROC and overall AUPRC on the per-group AUROC and AUPRCs of two groups in a synthetic setting where the initial AUROC was set to 0.65 rather than 0.85, using both the sequentially fixing individual mistakes optimization procedure (M2; top) and the sequentially permuting nearby scores optimization procedure (M3; bottom) described in Section 3.1. Note that the prevalence of $Y$ in the high-prevalence group and the low-prevalence group are 0.05 and 0.01 respectively. ", "page_idx": 48}, {"type": "image", "img_path": "S3HvA808gk/tmp/5e84da346b74aff992a131654fd335c96dee749b2a25524b5c8873603256b066.jpg", "img_caption": ["(a) Optimizing for Overall AUROC "], "img_footnote": [], "page_idx": 49}, {"type": "image", "img_path": "S3HvA808gk/tmp/5f33751b75c9c0320c0389c3b8de09d5673960cd265bceb0cfaefee48cdcce03.jpg", "img_caption": ["(b) Optimizing for Overall AUPRC "], "img_footnote": [], "page_idx": 49}, {"type": "text", "text": "Figure 6: Comparison of the impact of optimizing for overall AUROC and overall AUPRC on the per-group AUROC and AUPRCs of two groups in a synthetic setting, using the adding random noise optimization procedure (M1) described in Section 3.1. Note that the prevalence of $Y$ in $G_{1}$ and $G_{2}$ are 0.05 and 0.01 respectively. ", "page_idx": 49}, {"type": "text", "text": "H Additional Details on Real World Experiments ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "H.1 Dataset Details ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "We use the following four datasets. In all datasets, we use sex and race as protected attributes. ", "page_idx": 50}, {"type": "text", "text": "\u2022 adult [17]: The UCI Adult dataset, where the goal is to predict whether an individual\u2019s income is $>{\\mathfrak{G}}{\\mathfrak{H}}$ .   \n\u2022 compas [14]: The task to predict two-year recidivism. We only select samples belonging to \u201cAfrican-American\u201d and \u201cCaucasian\u201d, leading to a binary race variable.   \n\u2022 lsac [413]: The task is to predict whether a law school applicant will pass the bar. We only select samples belonging to White and Black applicants.   \n\u2022 mimic [178]: We use the in-hospital mortality task proposed by [138], where the goal is to predict whether a patient will die in the ICU given labs and vitals from the first 48 hours of their hospital stay. We only select samples belonging to White and Black patients. ", "page_idx": 50}, {"type": "text", "text": "In each dataset, we balance the groups by subsampling the majority group. We then split each dataset into $50\\%$ training, $25\\%$ validation, $25\\%$ test sets, stratified by the group. Dataset statistics can be found in Table 1. ", "page_idx": 50}, {"type": "text", "text": "Table 1: Dataset statistics for the four binary classification datasets used in this study. Note that $n$ refers to the number of samples after balancing by the corresponding attribute. Here, \u201cPrevalence (Higher)\u201d refers to the rate at which the prediction label $y=1$ for the subpopulation with a higher such rate, and \u201cPrevalence (Lower)\u201d refers to the same rate but over the subpopulation of the dataset with a lower rate of $y=1$ . ", "page_idx": 50}, {"type": "table", "img_path": "S3HvA808gk/tmp/3d99d5c039b1acb2cbb6b4423f1cf25eb45133b66a6337d5bcfda90c6789ee15.jpg", "table_caption": [], "table_footnote": [], "page_idx": 50}, {"type": "text", "text": "H.2 Hyperparameter Grid ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "We use the following hyperparameter grid for our experiments: ", "page_idx": 50}, {"type": "text", "text": "\u2022 max depth: $\\{1,2,...,9\\}$   \n\u2022 learning rate: [0,01, 0.3]   \n\u2022 number of estimators: [50, 1000]   \n\u2022 min child weight: $\\{1,2,...,9\\}$   \n\u2022 use protected attribute as input feature: {yes, no} group weight of higher prevalence group: {1, 2, 3, 4, 5, 10, 15, 20, 25, 50} ", "page_idx": 50}, {"type": "text", "text": "H.3 Additional Results ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "All raw AUC results can be found in the code repository for these experiments, at https://github. com/hzhang0/auc_bias. Additionally, summarized results in different views can be found in Figures 7 and 8. ", "page_idx": 50}, {"type": "image", "img_path": "S3HvA808gk/tmp/9c4650da9bf65176cb16365785a17e932477ea560ae0aa0e1d9d6011b28472e9.jpg", "img_caption": ["Figure 7: Spearman\u2019s $\\rho$ between the test-set signed AUROC gap versus the validation set overall AUPRC, and the AUROC gap versus the overall AUROC. Numbers in parentheses are the prevalence ratios between the two groups for the particular attribute, and datasets are sorted by this quantity. Error bars are $95\\%$ confidence intervals from 20 different random data splits. "], "img_footnote": [], "page_idx": 51}, {"type": "image", "img_path": "S3HvA808gk/tmp/d9d7166b102c9f04662ffaf93b270dad5637ccece43bc570e6e6491d471dd697.jpg", "img_caption": ["Figure 8: Correlation between the prevalence ratio, and the difference between the Spearman\u2019s $\\rho$ of the AUROC gap versus AUROC and the AUROC gap versus AUPRC. Each point represents a dataset and attribute combination. This correlation itself has a Spearman\u2019s $\\rho$ of 0.905 $\\left(p=0.002\\right)$ . "], "img_footnote": [], "page_idx": 51}, {"type": "text", "text": "I Literature Review Methodology ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "I.1 Paper Acquisition ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "The initial phase of our comprehensive literature search involved the acquisition of datasets from both the ArXiv preprint server (through the RedPajama dataset on Hugging Face), as well as from a subset of years of NeurIPS, ICML, ICLR, ACL, and CVPR conference proceedings (all scraped manually). The ArXiv dataset, approximately 93.8 GB in size, encompassed over 1.5 million texts in JSONL format. For NeurIPS, we developed a script to scrape conference papers from 1987 to 2019 (9680 texts), aiming to enrich our search. Other venues contributed fewer papers to our assessment process. ", "page_idx": 51}, {"type": "text", "text": "I.2 Keyword-Driven Filtering Process ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "1. Keyword List Development: We developed two distinct keyword lists to systematically identify papers relevant to our research on AUROC (Area Under the Receiver Operating Characteristic) and AUPRC (Area Under the Precision-Recall Curve) in our initial screening phase. The keyword lists can be accessed here for AUPRC and here for AUROC.   \n2. Automated Script-Based Search: Python scripts were employed to traverse the Arxiv and NeurIPS datasets. These scripts detected occurrences of our predefined keywords, allowing efficient parsing of a vast number of texts from both sources.   \n3. Dual Mention Selection Criterion: We focused on papers discussing both AUROC and AUPRC. This criterion ensured the relevance of the papers to our research question. Through ", "page_idx": 51}, {"type": "text", "text": "this process, we narrowed the pool from 16,022 texts (containing either set of keywords) to 8,244 texts mentioning both in the Arxiv dataset. In the NeurIPS dataset, out of 9,680 texts reviewed, 78 were found to contain keywords from AUPRC and AUROC. ", "page_idx": 52}, {"type": "text", "text": "I.3 AI-Assisted Screening and Refinement ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Preliminary Analysis with GPT-3.5: We utilized OpenAI\u2019s GPT-3.5 model for an initial round of AI-assisted analysis for the arXiv dataset. This model identified and extracted papers making explicit claims regarding the comparative effectiveness of AUPRC over AUROC in scenarios of class imbalance, reducing our dataset from Arxiv to 2,728 papers. ", "page_idx": 52}, {"type": "text", "text": "2. Further Refinement Using GPT-4.0 Turbo: To refine our dataset further, we employed the GPT-4.0 Turbo model. Out of the 2,728 papers scrutinized from Arxiv using this model, 201 were found to be relevant. For NeurIPS, our focused search with GPT-4 resulted in identifying 2 papers of particular relevance to our thesis from the initial set that contained keywords related to both AUPRC and AUROC. ", "page_idx": 52}, {"type": "text", "text": "I.4 Manual Review ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "\u2022 Shared Document for Collaborative Analysis: We compiled all pertinent papers, along with their respective Arxiv IDs and the claims identified by GPT-4.0 Turbo, into a shared Google document for team review. Claims made in papers were found manually, and the specific quote of the claim they made was highlighted along with whether or not they had a citation for this claim. ", "page_idx": 52}, {"type": "text", "text": "I.4.1 Final Papers ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "After manual review, we identified 424 papers that make or reference some version of the claim that   \n\u201cAUPRC is better than AUROC in cases of class imbalance.\u201d [78, 212, 428, 116, 8, 223, 229, 318, 393, 308, 450, 407, 4, 217,   \n391, 417, 261, 282, 70, 399, 169, 343, 153, 148, 361, 281, 219, 206, 204, 2, 350, 156, 134, 425, 22, 115, 257, 429, 353, 279, 85, 424, 137, 258, 367, 10, 454, 251, 275,   \n140, 208, 438, 356, 79, 52, 444, 90, 252, 111, 296, 325, 293, 312, 321, 387, 358, 41, 397, 54, 159, 369, 71, 180, 301, 74, 363, 24, 86, 49, 253, 270, 390, 329, 333, 348,   \n238, 233, 6, 124, 448, 359, 236, 243, 335, 210, 58, 317, 284, 21, 192, 12, 379, 274, 202, 415, 377, 373, 271, 82, 193, 376, 242, 183, 88, 276, 322, 395, 327, 239, 338,   \n319, 149, 288, 408, 136, 128, 46, 163, 132, 28, 207, 411, 302, 412, 227, 101, 221, 145, 328, 277, 234, 73, 60, 418, 92, 20, 198, 103, 27, 410, 151, 174, 29, 287, 11,   \n304, 347, 161, 285, 437, 341, 366, 5, 225, 334, 423, 297, 47, 324, 147, 371, 38, 323, 53, 184, 344, 378, 162, 394, 414, 106, 110, 426, 346, 130, 264, 432, 179, 440,   \n191, 94, 189, 362, 316, 220, 175, 421, 39, 188, 19, 401, 389, 419, 309, 224, 109, 95, 197, 97, 31, 244, 135, 351, 133, 352, 402, 63, 405, 23, 81, 305, 396, 330, 266,   \n303, 294, 43, 62, 364, 295, 168, 181, 165, 114, 89, 66, 250, 248, 278, 13, 430, 25, 416, 34, 398, 199, 84, 56, 381, 76, 35, 269, 291, 404, 228, 185, 241, 453, 211, 160,   \n15, 209, 173, 420, 326, 286, 64, 273, 9, 299, 382, 139, 442, 96, 372, 201, 30, 446, 386, 403, 127, 196, 69, 155, 75, 360, 231, 235, 310, 267, 255, 345, 349, 171, 126,   \n190, 452, 342, 57, 182, 59, 455, 439, 123, 383, 112, 59, 357, 55, 142, 214, 203, 374, 314, 170, 87, 187, 300, 355, 262, 7, 260, 268, 280, 1, 44, 298, 265, 120, 249, 91,   \n422, 436, 18, 108, 113, 176, 283, 245, 433, 158, 218, 122, 313, 172, 380, 365, 154, 385, 290, 388, 247, 216, 67, 98, 368, 177, 164, 289, 320, 32, 186, 400, 16, 237, 42,   \n431, 157, 375, 272, 406, 259, 315, 117, 77, 152, 306, 434, 72, 166, 392, 449, 230, 246, 26, 445, 232, 146, 118, 107, 61, 129, 50, 143, 427, 144, 370, 311, 336, 447,   \n105, 263, 256, 215, 125, 167, 240, 226, 195, 48, 194, 200, 3, 213, 354, 340, 45, 141, 102, 332, 51, 443, 33, 68].   \nThose papers that reference this claim without citation include [78, 212, 428, 116, 8, 223, 407, 4, 217, 261, 70, 169, 343, 153,   \n219, 206, 204, 2, 350, 156, 134, 22, 115, 257, 353, 279, 85, 424, 258, 367, 10, 454, 251, 275, 140, 356, 79, 52, 444, 90, 252, 111, 296, 293, 321, 358, 41, 54, 159, 369,   \n71, 180, 301, 74, 363, 24, 86, 253, 270, 390, 329, 333, 348, 233, 6, 124, 448, 359, 236, 243, 335, 210, 284, 21, 12, 379, 274, 202, 415, 377, 373, 271, 82, 376, 242,   \n183, 88, 322, 395, 239, 338, 149, 288, 408, 136, 128, 46, 163, 28, 412, 101, 328, 73, 60, 20, 103, 151, 29, 287, 11, 304, 161, 285, 341, 5, 423, 324, 371, 53, 184, 344,   \n162, 414, 110, 346, 264, 179, 191, 94, 189, 362, 316, 175, 188, 19, 389, 419, 109, 95, 351, 133, 23, 396, 303, 43, 364, 168, 165, 114, 250, 248, 430, 25, 416, 34, 398,   \n199, 56, 76, 35, 269, 291, 404, 228, 241, 453, 211, 160, 15, 420, 286, 9, 299, 139, 442, 96, 372, 30, 196, 69, 75, 360, 231, 235, 267, 171, 190, 342, 57, 439, 383, 112,   \n59, 357, 214, 314, 87, 187, 300, 262, 7, 260, 280, 44, 120, 249, 91, 18, 108, 113, 176, 283, 245, 122, 172, 380, 154, 385, 290, 216, 67, 164, 289, 186, 400, 16, 42, 431,   \n157, 375, 77, 306, 166, 449, 230, 232, 146, 118, 107, 61, 370, 311, 336, 447, 105, 263, 256, 125, 240, 3, 354, 340, 45, 141, 102, 332, 443] ", "page_idx": 52}, {"type": "text", "text": "", "page_idx": 52}, {"type": "text", "text": "Those that do so while citing only other papers that themselves never reference or argue Claim 1 include [428, 219, 204, 350, 156, 134, 22, 454, 251, 275, 356, 79, 52, 444, 90, 358, 41, 159, 369, 74, 86, 270, 333, 6, 124, 359, 243, 210, 284, 12, 379, 415, 377, 373, 395, 239, 136, 128, 328, 73, 20, 103, 29, 11, 304, 285, 5, 162, 414, 264, 191, 94, 189, 175, 419, 133, 303, 364, 165, 250, 25, 416, 398, 35, 228, 241, 453, 211, 160, 15, 286, 139, 96, 372, 30, 69, 75, 360, 231, 235, 171, 112, 59, 357, 314, 262, 7, 44, 120, 18, 113, 176, 283, 122, 172, 154, 385, 290, 216, 289, 431, 157, 375, 230, 146, 118, 107, 61, 311, 336, 3, 45, 141, 332] ", "page_idx": 52}, {"type": "text", "text": "All papers identified, manual screening results, and extracted quotes will be made available upon publication. ", "page_idx": 52}, {"type": "table", "img_path": "S3HvA808gk/tmp/b28a4b61083bef6be264e64eac1fe72992b669d515efff55516a61380530adb1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 53}, {"type": "text", "text": "I.5 Code Availability ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "All code pertaining to the literature review search can be found in the following GitHub repository: https://github.com/Lassehhansen/ArxivMLClaimSearch ", "page_idx": 53}, {"type": "table", "img_path": "S3HvA808gk/tmp/45f797098e68673cf00435d49b8327e94e32352d61b502ee85f84b751f58661b.jpg", "table_caption": ["Table 3: Various arguments and our responses to them present on a subset of papers for this claim in the literature. "], "table_footnote": [], "page_idx": 54}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 55}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 55}, {"type": "text", "text": "Justification: The claims made in the abstract reflect this paper\u2019s contributions and scope. Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 55}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 55}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Justification: We discuss limitations and future work in Section 6 ", "page_idx": 55}, {"type": "text", "text": "Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 55}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 55}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 55}, {"type": "text", "text": "Justification: The requisite assumptions and associated proofs for all theorems are provided in full technical detail in the appendix. ", "page_idx": 56}, {"type": "text", "text": "Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 56}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 56}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 56}, {"type": "text", "text": "Justification: We describe the experiments completely and fully release our code. All datasets used are either synthetic and reproducible in the code itself or publicly available. ", "page_idx": 56}, {"type": "text", "text": "Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 56}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 57}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 57}, {"type": "text", "text": "Justification: As stated above, all data used is either synthetic and fully reproducible or publicly available. ", "page_idx": 57}, {"type": "text", "text": "Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 57}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 57}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 57}, {"type": "text", "text": "Justification: While we do have some model training results to assess metrics (and for such results all training and test details are fully described in this paper and the full set of parameters and code is publicly released), this study is not actually a modelling study, but rather a study of machine learning metrics, so our main contribution is not a modeling result that is directly dependent on released test/split/hyperparameter details. ", "page_idx": 57}, {"type": "text", "text": "Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 57}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 57}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Justification: We perform appropriate statistical significance tests and report them in this work. ", "page_idx": 57}, {"type": "text", "text": "Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 57}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 58}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 58}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 58}, {"type": "text", "text": "Justification: Our synthetic experiments can be replicated in a colab notebook with the provided Jupyter notebook flie, and the real data experiments are adequately described with released code. ", "page_idx": 58}, {"type": "text", "text": "Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 58}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 58}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 58}, {"type": "text", "text": "Justification: We conform with the code of ethics. ", "page_idx": 58}, {"type": "text", "text": "Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 58}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 58}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 58}, {"type": "text", "text": "Justification: Our work is about correcting a major misunderstanding in the ML community regarding a widely used evaluation metric, and the potential fairness implications of this misunderstanding. In that sense, our entire work is clearly about the potential societal impacts of this misunderstanding, and how it should be corrected. We also clearly discuss the limitations of our work in Section 6. ", "page_idx": 58}, {"type": "text", "text": "Guidelines: ", "page_idx": 59}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 59}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 59}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 59}, {"type": "text", "text": "Justification: We do not release new data or models in this work. Guidelines: ", "page_idx": 59}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 59}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 59}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 59}, {"type": "text", "text": "Justification: Only public datasets, appropriately cited, are used in this work. Guidelines: ", "page_idx": 59}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 59}, {"type": "text", "text": "", "page_idx": 60}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 60}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 60}, {"type": "text", "text": "Justification: No new assets are released in this work. ", "page_idx": 60}, {"type": "text", "text": "Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 60}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 60}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 60}, {"type": "text", "text": "Justification: No crowdsourcing experiments were done in this work, nor was any research with human subjects done. The manual annotation of reviewed papers in this work was solely performed by authors of this work. ", "page_idx": 60}, {"type": "text", "text": "Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 60}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 60}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 60}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 60}, {"type": "text", "text": "Justification: No human subjects research was performed in this work. ", "page_idx": 60}, {"type": "text", "text": "Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 60}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 61}]