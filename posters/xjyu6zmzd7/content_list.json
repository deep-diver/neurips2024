[{"type": "text", "text": "Take A Shortcut Back: Mitigating the Gradient Vanishing for Training Spiking Neural Networks ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yufei Guo,\u2217 Yuanpei Chen\u2217, Zecheng Hao, Weihang Peng, Zhou Jie, Yuhan Zhang, Xiaode Liu, Zhe Ma\u2020 ", "page_idx": 0}, {"type": "text", "text": "Intelligent Science & Technology Academy of CASIC, China School of Computer Science, Peking University, China yfguo@pku.edu.cn, rop477@163.com, haozecheng@pku.edu.cn, mazhe_thu@163.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The Spiking Neural Network (SNN) is a biologically inspired neural network infrastructure that has recently garnered significant attention. It utilizes binary spike activations to transmit information, thereby replacing multiplications with additions and resulting in high energy efficiency. However, training an SNN directly poses a challenge due to the undefined gradient of the firing spike process. Although prior works have employed various surrogate gradient training methods that use an alternative function to replace the firing process during back-propagation, these approaches ignore an intrinsic problem: gradient vanishing. To address this issue, we propose a shortcut back-propagation method in the paper, which advocates for transmitting the gradient directly from the loss to the shallow layers. This enables us to present the gradient to the shallow layers directly, thereby significantly mitigating the gradient vanishing problem. Additionally, this method does not introduce any burden during the inference phase. To strike a balance between final accuracy and ease of training, we also propose an evolutionary training framework and implement it by inducing a balance coefficient that dynamically changes with the training epoch, which further improves the network\u2019s performance. Extensive experiments conducted over popular datasets using several popular network structures reveal that our method consistently outperforms state-of-the-art methods. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The Spiking Neural Network (SNN) has become a popular neural network due to its efficiency and has been widely used in various fields such as object recognition Li et al. (2021a); Xiao et al. (2021), object detection Kim et al. (2019); Qu et al. (2023), and pose tracking Zou et al. (2023). The SNN operates by using binary spike signals to transmit information. When the membrane potential exceeds the threshold, the spiking neuron fires a spike represented by 1; otherwise, there is no spike represented by 0. This unique information processing paradigm is energy-efficient since it replaces multiplications of weights and activations with simple additions. Additionally, this information processing paradigm can be implemented in an efficient event-driven-based computation manner on neuromorphic hardware Ma et al. (2017); Akopyan et al. (2015); Davies et al. (2018); Pei et al. (2019); Guo et al. (2023a), where the computational unit activates only when a spike occurs. This feature saves energy since the computational unit remains silent when there is no spike. Studies have shown that an SNN can save orders of magnitude more energy than its Artificial Neural Network (ANN) counterpart Akopyan et al. (2015); Davies et al. (2018). ", "page_idx": 0}, {"type": "image", "img_path": "xjyU6zmZD7/tmp/b2940c6c1ac9a2a55f5a2ba4ca5eae65ef4462e678686cff679b7027726c93f6.jpg", "img_caption": ["Figure 1: The overall workflow of the proposed method. We add multiple shortcut branches from the intermediate layers to the output thus the gradient from the output could be present to the shallow layers directly. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Although the SNN is energy-efficient, it is challenging to train it directly because the gradient of the firing spike process is not well-defined. This means that it is impossible to use gradient-based optimization methods to train an SNN directly. To overcome this problem, researchers have proposed various surrogate gradient training (SG) methods (Courbariaux et al., 2016; Esser et al., 2016; Bellec et al., 2018; Rathi & Roy, 2020; Wu et al., 2019; Neftci et al., 2019). These methods use an alternative function to replace the firing process during back-propagation. For example, in (Bohte, 2011), (Zenke & Ganguli, 2018), (Guo et al., 2022a), and (Cheng et al., 2020; Guo et al., 2024; Zhang et al., 2024), researchers used the truncated quadratic function, the sigmoid function, the tanh-like function, and the rectangular function as surrogates, respectively. However, SG methods have an intrinsic problem: gradient vanishing. All surrogate functions are bounded, and their gradients would be close to 0 in most intervals. As a result, the gradient of the SNN would quickly decrease from output to input, causing the weights of the shallow layers of the SNN to freeze during optimization. In subsection 4.1, we will theoretically and experimentally demonstrate the gradient vanishing problem. ", "page_idx": 1}, {"type": "text", "text": "To address this problem, we propose a shortcut back-propagation method in this paper, which involves transmitting the gradient from the loss to the shallow layers directly. To achieve this, we add multiple shortcut branches from intermediate layers to the output in the network. This allows information from the shallow layers to reach the output and final loss directly. Consequently, the gradient from the output can be present in the shallow layers, and their weights can be updated adequately, resulting in improved accuracy. Importantly, these shortcut branches can be removed without introducing any burden during the inference phase. Our proposed training framework is essentially a joint optimization problem on the weighted sum of the loss functions associated with these shortcut branches. However, if we give more weight to the main branch net, the earlier layer weights may not be updated sufficiently. Conversely, if we give more attention to the side shortcut branches, the accuracy cannot reach a high level since it directly relates to the main branch outputs rather than the side branch outputs. To balance this conflict, we introduce an evolutionary training framework. During early training, we pay more attention to the side branch net, allowing sufficient weight updates of the shallow layers. Towards the end of training, we increase the weight given to the main branch net, which further improves final accuracy. We accomplish this by inducing a dynamically changing balanced coefficient that adjusts with each training epoch. To illustrate our method\u2019s workflow, please refer to Figure 1. Our paper provides several key contributions, which can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Firstly, we have identified that the gradient vanishing problem is a significant issue for SNNs. We have supported this conclusion with theoretical justifications and in-depth experimental analysis. To mitigate this problem, we have proposed the shortcut backpropagation approach, which is a simple yet effective method. Importantly, it will not introduce any additional burden during the inference phase. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Secondly, we have proposed an evolutionary training framework that balances the weights of these branches with a gradual strategy. This approach ensures that earlier layer weights can be adequately updated while also improving overall accuracy. \u2022 Lastly, we have evaluated the effectiveness and efficiency of our proposed methods on both static (CIFAR-10, CIFAR-100, ImageNet) and spiking (CIFAR10-DVS) datasets with widely used backbones. Our results demonstrate that the SNN trained with our proposed method is highly effective, achieving a top-1 accuracy of $77.79\\%$ on CIFAR-100 using ResNet19 with only 2 timesteps. This represents a significant improvement of about $3.3\\%$ compared with the current state-of-the-art SNN models even with more timesteps. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Learning of Spiking Neural Networks ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Unsupervised learning Diehl & Cook (2015); Hao et al. (2020), converting ANN to SNN (ANN2SNN) Sengupta et al. (2019); Hao et al. (2023a,b), and supervised learning Li et al. (2021b); Guo et al. (2022c) are three commonly used learning paradigms for SNNs. In unsupervised learning, the spike-timing-dependent plasticity (STDP) approach Lobov et al. (2020) is utilized to update the SNN model, which is considered more biologically plausible. However, due to the lack of explicit task guidance, this method is typically limited to small-scale networks and datasets. The ANN-SNN conversion method Han & Roy (2020); Li et al. (2021a); Bu et al. (2021); Ho & Chang (2021); Bu et al. (2022); Hao et al. (2023a); Lan et al. (2023) involves training an ANN first and then converting it into a homogeneous SNN by transferring the trained weights and replacing the ReLU neuron with a temporal spiking neuron. However, this method is not suitable for neuromorphic datasets as the ReLU neuron in the ANN cannot capture the rich temporal dynamics required for sequential information. Supervised learning Fang et al. (2021a); Wu et al. (2018), on the other hand, adopts an alternative function during back-propagation to replace the firing process, enabling direct training of the SNN as an ANN. This approach leverages the success of gradient-based optimization and can achieve good performance with only a few time steps, even on large-scale datasets. Moreover, supervised learning can handle temporal data effectively, making it an increasingly popular choice in SNN research. Our work also falls within this domain. ", "page_idx": 2}, {"type": "text", "text": "2.2 Relieving Training Difficulties for Supervised Learning of SNNs ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "As mentioned earlier, the surrogate gradient (SG) approach is commonly employed to address the non-differentiability of SNNs. Various SG functions have been utilized, including the truncated quadratic function (Bohte, 2011), the sigmoid function (Zenke & Ganguli, 2018), the tanh-like function (Guo et al., 2022a), and the rectangular function (Cheng et al., 2020). While the SG method is generally effective, it can also introduce certain issues. Firstly, there is a gradient mismatch between the true gradient and the surrogate gradient, resulting in slow convergence and reduced accuracy. To tackle this problem, IM-Loss (Guo et al., 2022a) proposed a dynamic manual SG method that adapts with each epoch, ensuring sufficient weight updates and accurate gradients simultaneously. In contrast to this manual design, the Differentiable Spike method (Li et al., 2021b) and the differentiable SG search method determine the optimal gradient estimation using finite difference and NAS techniques, respectively. Secondly, due to the firing function being bounded, all these SG functions are also bounded. As a result, the gradient approaches or reaches close to zero in most intervals, exacerbating the vanishing gradient problem in SNNs. To mitigate this issue, SEW-ResNet (Fang et al., 2021a) suggested using the ResNet with activation before addition form, while MS-ResNet (Hu et al., 2021) advocated for the ResNet with pre-activation form. Additionally, normalization techniques have been employed to address the vanishing/explosion gradient problems. For example, Threshold-dependent batch normalization (tdBN) (Zheng et al., 2021) normalized the data along both the channel and temporal dimensions. Other techniques such as Temporal Batch Normalization Through Time (BNTT) (Kim & Panda, 2021), postsynaptic potential normalization (PSP-BN) (Ikegawa et al., 2022), and temporal effective batch normalization (TEBN) (Duan et al., 2022) recognized the significant variation in spike distributions across different timesteps, and thus regulated spike flows by applying separate timestep batch normalization. MPBN (Guo et al., 2023b) introduced an additional batch normalization step after the membrane potential updating function to reestablish data flow. Similarly, regularization loss has been utilized to alleviate gradient explosion/vanishing problems. In RecDis", "page_idx": 2}, {"type": "text", "text": "SNN (Guo et al., 2022c), a membrane potential regularization loss was proposed to control spike flow within an appropriate range. In Spiking PointNet (Ren et al., 2023), a trained-less but learning-more paradigm was proposed. This method can be seen as using a small network in the training to mitigate the training difficulty problem. ", "page_idx": 3}, {"type": "text", "text": "However, all these methods still need to present the gradient from the output layer to the first layer step by step, thus the gradient vanishing problem cannot be solved completely. In this paper, we propose a shortcut back-propagation method. Different from the above methods, we present the gradient from the output layer to these shallow layers directly, thus the gradient vanishing problem can be solved totally. ", "page_idx": 3}, {"type": "text", "text": "3 Preliminary ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The spiking neuron serves as the fundamental and specialized computing unit in SNNs, drawing inspiration from the human brain. In our paper, we employ the widely used spiking Leaky-Integrateand-Fire (LIF) neuron model. This model accurately captures the behavior of biological neurons by considering the interaction between the membrane potential and input current. To show the spiking neuron in detail, we introduce the notation first. Throughout the paper, we denote the vectors in bold italic letters. For instance, we use the $\\textbf{\\em x}$ and $^o$ to represent the input and target output variables. We denote the matrices or tensors by bold capital letters (e.g., W is for weights). We denote the constants by small upright or downright letters. For example, $u_{i}^{(t)}$ means the $i$ -th membrane potential at time step $t$ . Then, the LIF neuron can be described as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pmb{u}^{(t+1),\\mathrm{pre}}=\\tau\\pmb{u}^{(t)}+\\pmb{c}^{(t+1)},\\mathrm{where}\\;\\pmb{c}^{(t+1)}=\\mathbf{W}\\pmb{x}^{(t+1)},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\tau$ is a constant within $(0,1)$ , which controls the leakage of membrane potential. When $\\tau$ is 1, the neuron will degenerate to the Integrate-and-Fire (IF) neuron model. In the paper, we set $\\tau$ as 0.5. $\\pmb{u}^{(t+1),\\mathrm{pre}}$ is the pre-synaptic input at time step $t+1$ , which is charged by the input current $c^{(t+1)}$ . Note that we omit the layer index for simplicity. The input current is computed by the dot-product between the weights, W of the current layer and the spike output, $\\pmb{x}^{(t+1)}$ from the previous layer. Once the membrane potential, $\\pmb{u}^{(t+1),\\mathrm{pre}}$ exceeds the firing threshold $V_{\\mathrm{th}}$ , a spike will be fired from the LIF neuron, given by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{o}^{(t+1)}=\\left\\{\\begin{array}{l l}{1}&{\\mathrm{if}\\;\\pmb{u}^{(t+1),\\mathrm{pre}}>V_{\\mathrm{th}}}\\\\ {0}&{\\mathrm{otherwise}}\\end{array}\\right.,}\\\\ {\\pmb{u}^{(t+1)}=\\pmb{u}^{(t+1),\\mathrm{pre}}\\cdot(1-\\pmb{o}^{(t+1)}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "After firing, the spike output $\\pmb{o}^{(t+1)}$ will propagate to the next layer and become the input $\\pmb{x}^{(t+1)}$ of the next layer. In the paper, we set $V_{\\mathrm{th}}$ as 1. ", "page_idx": 3}, {"type": "text", "text": "There is a notorious problem in SNN training the firing function is undifferentiable. To demonstrate this problem, we formulate the gradient by the chain rule, given as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{\\partial L}{\\partial\\mathbf{W}}=\\sum_{t}(\\frac{\\partial L}{\\partial\\boldsymbol{o}^{(t)}}\\frac{\\partial\\boldsymbol{o}^{(t)}}{\\partial\\boldsymbol{u}^{(t),\\mathrm{pre}}}+\\frac{\\partial L}{\\partial\\boldsymbol{u}^{(t+1),\\mathrm{pre}}}\\frac{\\partial\\boldsymbol{u}^{(t+1),\\mathrm{pre}}}{\\partial\\boldsymbol{u}^{(t),\\mathrm{pre}}})\\frac{\\partial\\boldsymbol{u}^{(t),\\mathrm{pre}}}{\\partial\\mathbf{W}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Since the firing function Equation 2 is similar to the sign function. The $\\frac{\\partial\\pmb{o}^{(t)}}{\\partial\\pmb{u}^{(t)},\\mathrm{pre}}$ is 0 almost everywhere except for the threshold. Therefore, the updates for weights would either be 0 or infinity if we use the actual gradient of the firing function. ", "page_idx": 3}, {"type": "text", "text": "4 Methodology ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "4.1 The Gradient Vanishing Problem for SNNs ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As aforementioned, the non-differentiability of SNNs poses challenges when training them directly. To address this issue, researchers have proposed the use of surrogate gradients. In this approach, the firing function remains unchanged during the forward pass, but a surrogate function is employed during the backward pass. The surrogate gradient is then computed based on this surrogate function. ", "page_idx": 3}, {"type": "image", "img_path": "xjyU6zmZD7/tmp/5c35505d92fd033c2984233ee61ca239d0bfab1285977f63e26629cdb206f39e.jpg", "img_caption": ["Figure 2: The gradient distributions of the first layer for Spiking ResNet34 on CIFAR-100. (a) and (b) show the distributions for the vanilla model and the one with the shortcut back-propagation method. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "There are three commonly used surrogate gradients: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{r l}{\\frac{\\partial o^{(t)}}{\\partial{u^{(t)}},\\mathrm{pre}}}&{=\\gamma\\operatorname*{max}\\left(0,1-\\left|\\frac{{u^{(t),\\mathrm{pre}}}}{V_{\\mathrm{th}}}-1\\right|\\right),}\\\\ {\\frac{\\partial o^{(t)}}{\\partial{u^{(t)}},\\mathrm{pre}}}&{=\\frac{1}{a}\\mathrm{sign}\\left(\\left|{u^{(t),\\mathrm{pre}}}-V_{\\mathrm{th}}\\right|<\\frac{a}{2}\\right),}\\\\ {\\frac{\\partial o^{(t)}}{\\partial{u^{(t)}},\\mathrm{pre}}}&{=k(1-\\operatorname{tanh}({{u^{(t),\\mathrm{pre}}}-V_{\\mathrm{th}}}))^{2}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Each of these surrogate gradients includes a hyperparameter that controls the sharpness and width of the gradient. However, it is evident that these gradients, or their approximations, often become close to zero over a significant portion of their intervals. Consequently, this poses a considerable challenge in terms of severe gradient vanishing. While residual blocks have proven effective in mitigating gradient vanishing problems in traditional neural networks, their performance is not optimal when applied to SNNs. To demonstrate this, we express the skip connection using the following formulation: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pmb{o}=g(f(\\pmb{x})+\\pmb{x}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $f(\\cdot)$ is convolutional layers and $g(\\cdot)$ is the activation function. The standard ResNet network is composed of multiple skip connection blocks cascaded together. In ANN, ReLU is used for $g(\\cdot)$ , since ReLU is unbounded for the positive part, the gradient can be passed to the input of the block directly. However, in the case of LIF neurons in SNNs, the gradient will be reduced through the surrogate gradient. Thus, the standard skip connections still suffer the gradient vanishing problem in SNNs. To visually illustrate this problem, we show the gradient distributions of the first layer for Spiking ResNet34 with 4 timesteps on the CIFAR-100 in the Figure 2(a). It can be seen that these gradients are close to 0 in most intervals, meaning the gradient vanishing problem is very significant for these shallow layers. ", "page_idx": 4}, {"type": "text", "text": "4.2 The Shortcut Back-propagation Method ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Both theoretical analysis and experiments reveal the severity of the gradient vanishing problem in shallow layers of SNNs. In this paper, we propose a shortcut back-propagation method to address this issue. Specifically, the network is divided into several blocks, and we add multiple shortcut branches directly from these blocks to the output, as shown in Figure 1. These blocks are then followed by a global average pooling layer and a fully connected layer, resulting in the final output: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{\\pmb{o}_{\\mathrm{final}}=\\sum_{l}b_{l}(\\pmb{x}),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the $b_{l}({\\pmb x})$ represents the output of the $l$ -th block. While the original final output can be expressed as ", "page_idx": 4}, {"type": "equation", "text": "$$\no_{\\mathrm{final}}=b_{n}(\\pmb x),\\,\\,\\,\\mathrm{where}\\,\\,b_{l}(\\pmb x)=f_{l}(b_{l-1}(\\pmb x)).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In the above equation, the $n$ is the total number of the blocks and $f_{l}(\\cdot)$ is the network of the $l$ -th block. To demonstrate how our method alleviates the gradient vanishing problem, let\u2019s consider the gradient of the weight in the first layer as an example. For the original case, it can be expressed as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{\\partial L}{\\partial\\mathbf{W}_{1}}=\\frac{\\partial L}{\\partial b_{n}(\\pmb{x})}\\frac{\\partial b_{n}(\\pmb{x})}{\\partial b_{n-1}(\\pmb{x})}\\cdot\\cdot\\cdot\\frac{\\partial b_{l+1}(\\pmb{x})}{\\partial b_{l}(\\pmb{x})}\\frac{\\partial b_{l}(\\pmb{x})}{\\partial\\mathbf{W}_{1}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 Training and inference procedure of SNN with our method. ", "page_idx": 5}, {"type": "text", "text": "Training ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Input: An SNN to be trained; Initial balance coefficient $\\lambda$ ; training dataset; total training iteration: Itrain. ", "page_idx": 5}, {"type": "text", "text": "Output: The well-trained SNN. ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "1: for all $i=1,2,\\ldots,I_{\\mathrm{train}}$ iteration do   \n2: Get mini-batch training data, $x_{\\mathrm{in}}(i)$ and class label, $\\pmb{y}(i)$ ;   \n3: Feed the $x_{\\mathrm{in}}(i)$ into the SNN and calculate every block output $b_{l}(\\pmb{x}_{\\mathrm{in}}(i))$ and the final main   \nnet output $b_{n}(\\mathbf{\\boldsymbol{x}}_{\\mathrm{in}}(i))$ ;   \n4: Update $\\lambda$ ;   \n5: Calculate the final output by Equation 10;   \n6: Compute classification loss $L_{\\mathrm{CE}}=\\mathcal{L}_{\\mathrm{CE}}(o_{\\mathrm{final}}(i),\\pmb{y}(i))$ ;   \n7: Calculate the gradient w.r.t. W by Equation 9;   \n8: Update W: $\\begin{array}{r}{\\mathbf{\\nabla}\\mathbf{W}\\leftarrow\\mathbf{W}-\\eta\\frac{\\partial L}{\\partial\\mathbf{W}})}\\end{array}$ where $\\eta$ is learning rate.   \n9: end for ", "page_idx": 5}, {"type": "text", "text": "Inference ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Input: The trained SNN; test dataset; total test iteration: $I_{\\mathrm{test}}$ Output: The result. ", "page_idx": 5}, {"type": "text", "text": "1: for all $i=1,2,\\dots,I_{\\mathrm{test}}$ iteration do   \n2: Get mini-batch test data, $x_{\\mathrm{in}}(i)$ and class label, $\\pmb{y}(i)$ in test dataset;   \n3: Feed the $x_{\\mathrm{in}}(i)$ into the trained SNN;   \n4: Calculate the final main net output $b_{n}(\\mathbf{x}_{\\mathrm{in}}(i))$ ;   \n5: Calculate the final output, $o_{\\mathrm{final}}(i)=b_{n}\\big({\\pmb x}_{\\mathrm{in}}(i)\\big)$ ;   \n6: Compare the final output $o_{\\mathrm{final}}(i)$ and $\\pmb{y}(i)$ to compute the classification result.   \n7: end for ", "page_idx": 5}, {"type": "text", "text": "Since the \u2202\u2202blb+l(1x()x) is always reduced through the surrogate gradient, the\u2202\u2202WL1 becomes very small, resulting in insufficient weight updates. However, for our method, it can be expressed as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{\\partial L}{\\partial{\\bf W}_{1}}=\\sum_{l}\\frac{\\partial L}{\\partial b_{l}({\\pmb x})}\\frac{\\partial b_{l}({\\pmb x})}{\\partial{\\bf W}_{1}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In our method, the gradient is directly fed into the first block and subsequently to $\\mathbf{W}_{1}$ . This completely solves the gradient vanishing problem. To further illustrate this advantage, we visualize the gradient distribution of the first layer for Spiking ResNet34 on CIFAR-100 in Figure 2(b). It can be observed that the distribution is relatively flat, indicating that the gradient vanishing problem has been effectively addressed in these shallow layers. Moreover, these shortcut branches can be removed during inference, incurring no additional cost. ", "page_idx": 5}, {"type": "text", "text": "4.3 The Evolutionary Training Framework ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "While the shortcut back-propagation method effectively addresses the gradient vanishing problem, it introduces a potential conflict. Each shortcut branch contributes to the final output, but if we focus too much on the shallow layer outputs, the overall accuracy may suffer. This is because the shortcut branches are removed during the inference phase, and the final accuracy is primarily influenced by the main branch. On the other hand, if we prioritize the main branch output, the final loss may not capture enough information from the shallow layers. Consequently, the weights of these shallow layers may not be updated adequately. ", "page_idx": 5}, {"type": "text", "text": "To address this issue, we propose an evolutionary training framework. During the early stages of training, we prioritize the former side branch net, allowing for sufficient weight updates in the shallow layers. As training progresses, we gradually shift our focus to the main branch net until all attention is on the main net at the end of training. To achieve this, we introduce a balance coefficient, denoted by $\\lambda(i)$ and adopt a strategy of decreasing to adjust it as follows, ", "page_idx": 5}, {"type": "equation", "text": "$$\no_{\\mathrm{final}}=b_{n}({\\pmb x})+\\lambda(i)\\sum_{l=1}b_{l}({\\pmb x}),\\;\\;\\mathrm{where}\\;\\lambda(i)=\\lambda(1-\\frac{i}{I}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "table", "img_path": "xjyU6zmZD7/tmp/79a6ff9b6291649c97754c4a7e1e667b712e60d178b199304d2699f9ffc36b77.jpg", "table_caption": ["Table 1: Ablation study for the shortcut back-propagation method. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "In the equation mentioned above, $I$ represents the total number of training iterations, $i$ denotes the current training iteration, and $\\lambda$ is a constant. In our work, we set $\\lambda$ to a value of 0.25. The training and inference of our SNN are detailed in Algo. 1. ", "page_idx": 6}, {"type": "text", "text": "5 Experiment ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We conduct extensive experiments on CIFAR-10(100) Krizhevsky et al. (2010), ImageNet Deng et al. (2009), and CIFAR10-DVS Li et al. (2017) to demonstrate the superior performance of our method. The CIFAR-10(100) dataset comprises 50k training images and $10\\mathbf{k}$ test images, divided into 10(100) classes, each with $32\\times32$ pixels. CIFAR10-DVS is a converted dataset derived from CIFAR-10. It consists of $10\\mathbf{k}$ images, with 1k images per class, in 10 classes. ImageNet is a significantly larger dataset, containing over 1,250k training images and 50k test images. For these static datasets (CIFAR10, CIFAR-100, and ImageNet), we applied data normalization to ensure that they have 0 mean and 1 variance. Additionally, to prevent overftiting, we performed random horizontal filpping and cropping on all these datasets. For a fair comparison, AutoAugment Cubuk et al. (2018) was also used for data augmentation following these work Guo et al. (2022b); Li et al. (2021b) on CIFAR-10(100). Regarding the CIFAR10-DVS dataset, we partitioned it into $9\\mathrm{k}$ training images and 1k test images, as described in Wu et al. (2019). The training image frames were resized to $48\\times48$ as in Zheng et al. (2021). Random horizontal flipping and random roll within a range of 5 pixels were also applied during training. For the test images, we simply resized them to $48\\times48$ without any additional processing, following the approach of Li et al. Li et al. (2021b). We run the model with 8 V100. ", "page_idx": 6}, {"type": "text", "text": "5.1 Ablation Study ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To validate the effectiveness of the proposed shortcut back-propagation method, we initially conducted several ablation experiments on the CIFAR-100 dataset using ResNet18 and ResNet34 with different timesteps. The results are detailed in Table 1. For ResNet18, the accuracy achieved through vanilla training is $71.42\\%$ and $72.22\\%$ under 2 and 4 timesteps, respectively, which aligns with existing works. Upon applying our shortcut back-propagation method, the accuracy improved to $73.68\\%$ and $74.78\\%$ , marking a notable $2.5\\%$ enhancement. Furthermore, with the evolutionary training method, the performance of ResNet18 saw an additional improvement, reaching $74.02\\%$ and $74.83\\%$ , respectively. Under vanilla training, ResNet34 achieved accuracies of $69.82\\%$ and $69.98\\%$ with 2 and 4 timesteps, respectively. These results are actually worse than those obtained with ResNet18. This suggests that the deeper model does not exhibit better performance due to the significant gradient vanishing problem in SNNs. However, by utilizing our shortcut back-propagation method, the accuracy significantly improves to $74.06\\%$ and $75.67\\%$ , representing a remarkable $5.0\\%$ enhancement. Notably, these results surpass the performance of ResNet18 as well. This clearly demonstrates the effectiveness of our proposed method. Furthermore, when incorporating the evolutionary training method, we observe further improvements in performance. ", "page_idx": 6}, {"type": "table", "img_path": "xjyU6zmZD7/tmp/4f79b7a350104636a1307ce4a3f33e6870a32c784f76db80ea519aed1f2b3143.jpg", "table_caption": ["Table 2: Comparison with SoTA methods on CIFAR-10(100). "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.2 Comparison with SoTA Methods ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we conducted a comparative experiment for the shortcut back-propagation method and the evolutionary training framework, taking into consideration several state-of-the-art works. To ensure a fair comparison, we present the top-1 accuracy results based on 3 independent trials. We first evaluated our method on CIFAR-10 and CIFAR-100 datasets. The AdamW optimizer was employed with a learning rate of 0.01 which is cosine decay to 0 and a weight decay of 0.02. Throughout the training process, all models were trained using a batch size of 128 for a total of 300 epochs. The results are summarized in Table 2. For the CIFAR-10 dataset, we chose SpikeNorm Sengupta et al. (2019), Hybrid-Train Rathi et al. (2020), TSSL-BP Zhang & Li (2020), TL Wu et al. (2021a), PTL Wu et al. (2021b), PLIF Fang et al. (2021b), DSR Meng et al. (2022), KDSNN Xu et al. (2023), Diet-SNN Rathi & Roy (2020), Dspike Li et al. (2021b), STBP-tdBN Zheng et al. (2021), TET Deng et al. (2022), RecDis-SNN Guo et al. (2022c), and Real Spike Guo et al. (2022d) as our comparison. Previous works utilizing ResNet18, ResNet19, and ResNet20 as backbones achieved the highest accuracies of $95.40\\%$ , $95.31\\%$ , and $93.66\\%$ with 20, 2, and 4 timesteps respectively. While our method based on ResNet18 and ResNet19 could reach $93.92\\%$ and $95.36\\%$ with 4 and 2 timesteps, respectively. Note that, ResNet18 is smaller than ResNet20. On the CIFAR-100 dataset, our method can also achieve better accuracy than other prior state-of-the-art works with fewer timesteps. For instance, our ResNet19 model with only 2 timesteps outperforms the current best method, TET and ", "page_idx": 7}, {"type": "table", "img_path": "xjyU6zmZD7/tmp/195b3f32aaf4776d768ee7132cbe7cb9944efff6674811fad4935975ed88179d.jpg", "table_caption": ["Table 3: Comparison with SoTA methods on ImageNet. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "xjyU6zmZD7/tmp/75ec933fa7f87fa4cb1b35f0a20fb7ab2bcc1c1cf4d24e8ed797a60b8534e4f6.jpg", "table_caption": ["Table 4: Comparison with SoTA methods on CIFAR10-DVS. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "RecDis-SNN even with 4 timesteps by about $3.3\\%$ . These experimental results clearly demonstrate the efficiency and effectiveness of our method. ", "page_idx": 8}, {"type": "text", "text": "We proceeded to conduct experiments on the ImageNet dataset, which is a more complex dataset than CIFAR. The learning rate is adjusted to $4e^{-3}$ here. The comparative results are presented in Table 3. Notably, there have been several state-of-the-art (SoTA) baselines proposed for this dataset recently, such as RecDis-SNN Guo et al. (2022c), GLIF Yao et al. (2022), DSR Meng et al. (2022), MPBN Guo et al. (2023b), MS-ResNet Hu et al. (2023), Real Spike Guo et al. (2022d), and SEW ResNet Fang et al. (2021a). It is important to note that Real Spike and SEW ResNet deviate from the typical ResNet backbone as they generate integer outputs in the intermediate layers, making them more energy-intensive compared to methods with standard backbones. In contrast, our approach adopts the standard ResNet18 and ResNet34 architectures, yet it still outperforms Real Spike and SEW ResNet. Specifically, our method achieves an accuracy of $65.12\\%$ and $68.14\\%$ using ResNet18 and ResNet34, respectively, surpassing Real Spike by $1.44\\%$ and $0.45\\%$ , respectively. This improvement is noteworthy and demonstrates the effectiveness of our method in handling large-scale datasets. ", "page_idx": 8}, {"type": "text", "text": "In addition to the aforementioned experiments, we conducted tests on the highly popular neuromorphic dataset, CIFAR10-DVS. We use the same hyper-parameter setting as CIFAR. Employing ResNet18 as the foundational architecture, which is notably smaller compared to ResNet19, our approach achieved remarkable accuracies of $82.00\\%$ and $83.30\\%$ , respectively. These results demonstrate a substantial improvement over previous methodologies. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In the paper, we proved that the Spiking Neural Network suffers severe gradient vanishing with theoretical justifications and in-depth experimental analysis. To mitigate the problem, we proposed a shortcut back-propagation method. This enables us to present the gradient to the shallow layers directly, thereby significantly mitigating the gradient vanishing problem. Additionally, this method does not introduce any burden during the inference phase. we also presented an evolutionary training framework by inducing a balance coefficient that dynamically changes with the training epoch, which could further improve the accuracy. We conducted various experiments to verify the effectiveness of our method. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgment ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is supported by grants from the National Natural Science Foundation of China under contracts No.12202412 and No.12202413. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Akopyan, F., Sawada, J., Cassidy, A., Alvarez-Icaza, R., Arthur, J., Merolla, P., Imam, N., Nakamura, Y., Datta, P., Nam, G.-J., et al. Truenorth: Design and tool flow of a 65 mw 1 million neuron programmable neurosynaptic chip. IEEE transactions on computer-aided design of integrated circuits and systems, 34(10):1537\u20131557, 2015.   \nBellec, G., Salaj, D., Subramoney, A., Legenstein, R., and Maass, W. Long short-term memory and learning-to-learn in networks of spiking neurons. Advances in neural information processing systems, 31, 2018.   \nBohte, S. M. Error-backpropagation in networks of fractionally predictive spiking neurons. In International Conference on Artificial Neural Networks, pp. 60\u201368. Springer, 2011.   \nBu, T., Fang, W., Ding, J., Dai, P., Yu, Z., and Huang, T. Optimal ann-snn conversion for highaccuracy and ultra-low-latency spiking neural networks. In International Conference on Learning Representations, 2021.   \nBu, T., Ding, J., Yu, Z., and Huang, T. Optimized potential initialization for low-latency spiking neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 11\u201320, 2022.   \nCheng, X., Hao, Y., Xu, J., and Xu, B. Lisnn: Improving spiking neural networks with lateral interactions for robust object recognition. In IJCAI, pp. 1519\u20131525, 2020.   \nCourbariaux, M., Hubara, I., Soudry, D., El-Yaniv, R., and Bengio, Y. Binarized neural networks: Training deep neural networks with weights and activations constrained $\\mathrm{to}+1$ or-1. arXiv preprint arXiv:1602.02830, 2016.   \nCubuk, E. D., Zoph, B., Mane, D., Vasudevan, V., and Le, Q. V. Autoaugment: Learning augmentation policies from data. arXiv preprint arXiv:1805.09501, 2018.   \nDavies, M., Srinivasa, N., Lin, T.-H., Chinya, G., Cao, Y., Choday, S. H., Dimou, G., Joshi, P., Imam, N., Jain, S., et al. Loihi: A neuromorphic manycore processor with on-chip learning. Ieee Micro, 38(1):82\u201399, 2018.   \nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248\u2013255. Ieee, 2009.   \nDeng, S., Li, Y., Zhang, S., and Gu, S. Temporal efficient training of spiking neural network via gradient re-weighting. arXiv preprint arXiv:2202.11946, 2022.   \nDiehl, P. U. and Cook, M. Unsupervised learning of digit recognition using spike-timing-dependent plasticity. Frontiers in computational neuroscience, 9:99, 2015.   \nDuan, C., Ding, J., Chen, S., Yu, Z., and Huang, T. Temporal effective batch normalization in spiking neural networks. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id= fLIgyyQiJqz.   \nEsser, S. K., Merolla, P. A., Arthur, J. V., Cassidy, A. S., Appuswamy, R., Andreopoulos, A., Berg, D. J., McKinstry, J. L., Melano, T., Barch, D. R., et al. From the cover: Convolutional networks for fast, energy-efficient neuromorphic computing. Proceedings of the National Academy of Sciences of the United States of America, 113(41):11441, 2016.   \nFang, W., Yu, Z., Chen, Y., Huang, T., Masquelier, T., and Tian, Y. Deep residual learning in spiking neural networks. Advances in Neural Information Processing Systems, 34:21056\u201321069, 2021a.   \nFang, W., Yu, Z., Chen, Y., Masquelier, T., Huang, T., and Tian, Y. Incorporating learnable membrane time constant to enhance learning of spiking neural networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2661\u20132671, 2021b.   \nGuo, Y., Chen, Y., Zhang, L., Liu, X., Wang, Y., Huang, X., and Ma, Z. IM-loss: Information maximization loss for spiking neural networks. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022a. URL https:// openreview.net/forum?id $\\equiv$ Jw34v_84m2b.   \nGuo, Y., Chen, Y., Zhang, L., Wang, Y., Liu, X., Tong, X., Ou, Y., Huang, X., and Ma, Z. Reducing information loss for spiking neural networks. In Avidan, S., Brostow, G., Ciss\u00e9, M., Farinella, G. M., and Hassner, T. (eds.), Computer Vision \u2013 ECCV 2022, pp. 36\u201352, Cham, 2022b. Springer Nature Switzerland. ISBN 978-3-031-20083-0.   \nGuo, Y., Tong, X., Chen, Y., Zhang, L., Liu, X., Ma, Z., and Huang, X. Recdis-snn: Rectifying membrane potential distribution for directly training spiking neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 326\u2013335, June 2022c.   \nGuo, Y., Zhang, L., Chen, Y., Tong, X., Liu, X., Wang, Y., Huang, X., and Ma, Z. Real spike: Learning real-valued spikes for spiking neural networks. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XII, pp. 52\u201368. Springer, 2022d.   \nGuo, Y., Huang, X., and Ma, Z. Direct learning-based deep spiking neural networks: a review. Frontiers in Neuroscience, 17:1209795, 2023a.   \nGuo, Y., Zhang, Y., Chen, Y., Peng, W., Liu, X., Zhang, L., Huang, X., and Ma, Z. Membrane potential batch normalization for spiking neural networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 19420\u201319430, October 2023b.   \nGuo, Y., Chen, Y., Liu, X., Peng, W., Zhang, Y., Huang, X., and Ma, Z. Ternary spike: Learning ternary spikes for spiking neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 12244\u201312252, 2024.   \nHan, B. and Roy, K. Deep spiking neural network: Energy efficiency through time based coding. In European Conference on Computer Vision, pp. 388\u2013404. Springer, 2020.   \nHao, Y., Huang, X., Dong, M., and Xu, B. A biologically plausible supervised learning method for spiking neural networks using the symmetric stdp rule. Neural Networks, 121:387\u2013395, 2020.   \nHao, Z., Bu, T., Ding, J., Huang, T., and Yu, Z. Reducing ann-snn conversion error through residual membrane potential. arXiv preprint arXiv:2302.02091, 2023a.   \nHao, Z., Ding, J., Bu, T., Huang, T., and Yu, Z. Bridging the gap between anns and snns by calibrating offset spikes. arXiv preprint arXiv:2302.10685, 2023b.   \nHo, N.-D. and Chang, I.-J. Tcl: an ann-to-snn conversion with trainable clipping layers. In 2021 58th ACM/IEEE Design Automation Conference (DAC), pp. 793\u2013798. IEEE, 2021.   \nHu, Y., Wu, Y., Deng, L., and Li, G. Advancing residual learning towards powerful deep spiking neural networks. arXiv preprint arXiv:2112.08954, 2021.   \nHu, Y., Deng, L., Wu, Y., Yao, M., and Li, G. Advancing spiking neural networks towards deep residual learning, 2023.   \nIkegawa, S.-i., Saiin, R., Sawada, Y., and Natori, N. Rethinking the role of normalization and residual blocks for spiking neural networks. Sensors, 22(8), 2022. ISSN 1424-8220. doi: 10.3390/s22082876. URL https://www.mdpi.com/1424-8220/22/8/2876.   \nKim, S., Park, S., Na, B., and Yoon, S. Spiking-yolo: Spiking neural network for energy-efficient object detection, 2019.   \nKim, Y. and Panda, P. Revisiting batch normalization for training low-latency deep spiking neural networks from scratch. Frontiers in neuroscience, pp. 1638, 2021.   \nKrizhevsky, A., Nair, V., and Hinton, G. Cifar-10 (canadian institute for advanced research). URL http://www. cs. toronto. edu/kriz/cifar. html, 5(4):1, 2010.   \nLan, Y., Zhang, Y., Ma, X., Qu, Y., and Fu, Y. Efficient converted spiking neural network for 3d and 2d classification. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9211\u20139220, 2023.   \nLi, H., Liu, H., Ji, X., Li, G., and Shi, L. Cifar10-dvs: an event-stream dataset for object classification. Frontiers in neuroscience, 11:309, 2017.   \nLi, Y., Deng, S., Dong, X., Gong, R., and Gu, S. A free lunch from ann: Towards efficient, accurate spiking neural networks calibration. In International Conference on Machine Learning, pp. 6316\u20136325. PMLR, 2021a.   \nLi, Y., Guo, Y., Zhang, S., Deng, S., Hai, Y., and Gu, S. Differentiable spike: Rethinking gradientdescent for training spiking neural networks. Advances in Neural Information Processing Systems, 34:23426\u201323439, 2021b.   \nLobov, S. A., Mikhaylov, A. N., and Kazantsev, V. B. Spatial properties of stdp in a self-learning spiking neural network enable controlling a mobile robot. Frontiers in Neuroscience, 14:\u2013, 2020.   \nMa, D., Shen, J., Gu, Z., Zhang, M., Zhu, X., Xu, X., Xu, Q., Shen, Y., and Pan, G. Darwin: A neuromorphic hardware co-processor based on spiking neural networks. Journal of Systems Architecture, 77:43\u201351, 2017.   \nMeng, Q., Xiao, M., Yan, S., Wang, Y., Lin, Z., and Luo, Z.-Q. Training high-performance lowlatency spiking neural networks by differentiation on spike representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12444\u201312453, 2022.   \nNeftci, E. O., Mostafa, H., and Zenke, F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to spiking neural networks. IEEE Signal Processing Magazine, 36(6):51\u201363, 2019.   \nPark, S., Kim, S., Na, B., and Yoon, S. T2fsnn: Deep spiking neural networks with time-to-first-spike coding. In 2020 57th ACM/IEEE Design Automation Conference (DAC), pp. 1\u20136. IEEE, 2020.   \nPei, J., Deng, L., Song, S., Zhao, M., Zhang, Y., Wu, S., Wang, G., Zou, Z., Wu, Z., He, W., et al. Towards artificial general intelligence with hybrid tianjic chip architecture. Nature, 572(7767): 106\u2013111, 2019.   \nQu, J., Gao, Z., Zhang, T., Lu, Y., Tang, H., and Qiao, H. Spiking neural network for ultra-low-latency and high-accurate object detection, 2023.   \nRathi, N. and Roy, K. Diet-snn: Direct input encoding with leakage and threshold optimization in deep spiking neural networks. arXiv preprint arXiv:2008.03658, 2020.   \nRathi, N., Srinivasan, G., Panda, P., and Roy, K. Enabling deep spiking neural networks with hybrid conversion and spike timing dependent backpropagation. arXiv preprint arXiv:2005.01807, 2020.   \nRen, D., Ma, Z., Chen, Y., Peng, W., Liu, X., Zhang, Y., and Guo, Y. Spiking pointnet: Spiking neural networks for point clouds. arXiv preprint arXiv:2310.06232, 2023.   \nSengupta, A., Ye, Y., Wang, R., Liu, C., and Roy, K. Going deeper in spiking neural networks: Vgg and residual architectures. Frontiers in neuroscience, 13:95, 2019.   \nWu, J., Chua, Y., Zhang, M., Li, G., Li, H., and Tan, K. C. A tandem learning rule for effective training and rapid inference of deep spiking neural networks. IEEE Transactions on Neural Networks and Learning Systems, 2021a.   \nWu, J., Xu, C., Han, X., Zhou, D., Zhang, M., Li, H., and Tan, K. C. Progressive tandem learning for pattern recognition with deep spiking neural networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(11):7824\u20137840, 2021b.   \nWu, Y., Deng, L., Li, G., Zhu, J., and Shi, L. Spatio-temporal backpropagation for training highperformance spiking neural networks. Frontiers in neuroscience, 12:331, 2018.   \nWu, Y., Deng, L., Li, G., Zhu, J., Xie, Y., and Shi, L. Direct training for spiking neural networks: Faster, larger, better. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 1311\u20131318, 2019.   \nXiao, M., Meng, Q., Zhang, Z., Wang, Y., and Lin, Z. Training feedback spiking neural networks by implicit differentiation on the equilibrium state. Advances in Neural Information Processing Systems, 34:14516\u201314528, 2021.   \nXu, Q., Li, Y., Shen, J., Liu, J. K., Tang, H., and Pan, G. Constructing deep spiking neural networks from artificial neural networks with knowledge distillation. arXiv preprint arXiv:2304.05627, 2023.   \nYang, Q., Wu, J., Zhang, M., Chua, Y., Wang, X., and Li, H. Training spiking neural networks with local tandem learning. arXiv preprint arXiv:2210.04532, 2022.   \nYao, X., Li, F., Mo, Z., and Cheng, J. Glif: A unified gated leaky integrate-and-fire neuron for spiking neural networks. arXiv preprint arXiv:2210.13768, 2022.   \nZenke, F. and Ganguli, S. Superspike: Supervised learning in multilayer spiking neural networks. Neural computation, 30(6):1514\u20131541, 2018.   \nZhang, W. and Li, P. Temporal spike sequence learning via backpropagation for deep spiking neural networks. Advances in Neural Information Processing Systems, 33:12022\u201312033, 2020.   \nZhang, Y., Liu, X., Chen, Y., Peng, W., Guo, Y., Huang, X., and Ma, Z. Enhancing representation of spiking neural networks via similarity-sensitive contrastive learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 16926\u201316934, 2024.   \nZheng, H., Wu, Y., Deng, L., Hu, Y., and Li, G. Going deeper with directly-trained larger spiking neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 11062\u201311070, 2021.   \nZou, S., Mu, Y., Zuo, X., Wang, S., and Cheng, L. Event-based human pose tracking by spiking spatiotemporal transformer, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 13}, {"type": "text", "text": "Justification: We clearly state the claims made and the contributions made in both the abstract and introduction. ", "page_idx": 13}, {"type": "text", "text": "Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 13}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 13}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 13}, {"type": "text", "text": "Justification: We find no limitation which we feel must be specifically highlighted here. Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 13}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 13}, {"type": "text", "text": "Justification: We provide the full set of assumptions and complete proofs in the Section 4. Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 14}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 14}, {"type": "text", "text": "Justification: We provide the detail experiment settings in the Section 5. Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 14}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 15}, {"type": "text", "text": "Justification: We provide open access to the data and code with sufficient instructions in the supplemental material. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 15}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 15}, {"type": "text", "text": "Justification: All implementations are described in the experiments section. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 15}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Justification: We report the mean as well as the standard deviation accuracy in experiments. Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 16}, {"type": "text", "text": "Justification: The computation resources description is provided in the experiment section. Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 16}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 16}, {"type": "text", "text": "Justification: The research conducted with the NeurIPS Code of Ethics ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 16}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 16}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 17}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 17}, {"type": "text", "text": "Justification: The original paper for datasets we used are all cited. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 17}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 18}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: We adopt public datasets. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 18}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 18}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 18}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 18}]