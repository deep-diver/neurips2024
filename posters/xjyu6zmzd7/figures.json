[{"figure_path": "xjyU6zmZD7/figures/figures_1_1.jpg", "caption": "Figure 1: The overall workflow of the proposed method. We add multiple shortcut branches from the intermediate layers to the output thus the gradient from the output could be present to the shallow layers directly.", "description": "This figure illustrates the training and inference phases of the proposed shortcut back-propagation method for Spiking Neural Networks (SNNs). In the training phase, multiple shortcut branches are added from intermediate layers to the output layer. These branches allow the gradient to flow directly from the output layer to the shallow layers, mitigating the gradient vanishing problem. During inference, these shortcut branches are removed, resulting in no additional computational burden. The figure visually shows the forward and backward passes of information during training and only the forward pass during inference.", "section": "Methodology"}, {"figure_path": "xjyU6zmZD7/figures/figures_4_1.jpg", "caption": "Figure 2: The gradient distributions of the first layer for Spiking ResNet34 on CIFAR-100. (a) and (b) show the distributions for the vanilla model and the one with the shortcut back-propagation method.", "description": "This figure compares the gradient distributions of the first layer in a Spiking ResNet34 model trained on the CIFAR-100 dataset using two different methods: a standard training approach (vanilla) and a shortcut back-propagation method proposed by the authors. The histograms in (a) and (b) visualize the distribution of gradients, showing that the shortcut method leads to a flatter distribution, indicating a mitigation of the gradient vanishing problem.", "section": "4.2 The Shortcut Back-propagation Method"}]