[{"type": "text", "text": "Learning diverse causally emergent representations from time series data ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "David McSharry\u2217   \nDepartment of Computing   \nImperial College London   \ndm2223@ic.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Christos Kaplanis\u2217 Google DeepMind kaplanis@google.com ", "page_idx": 0}, {"type": "text", "text": "Fernando E. Rosas Sussex AI, University of Sussex f.rosas@sussex.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Pedro A.M. Mediano Department of Computing Imperial College London p.mediano@imperial.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Cognitive processes usually take place at a macroscopic scale in systems characterised by emergent properties that make the whole \u2018more than the sum of its parts.\u2019 While recent proposals have provided quantitative, information-theoretic metrics to detect emergence in time series data, it is often highly non-trivial to identify the relevant macroscopic variables a priori. In this paper we leverage recent advances in representation learning and differentiable information estimators to put forward a data-driven method to find variables with emergent properties. The proposed method successfully detects variables that exhibit emergent behaviour and recovers the ground-truth emergence values in a synthetic dataset. Furthermore, we show the method can be extended to learn multiple independent features, extracting a diverse set of emergent quantities. We finally show that a modified method scales to real experimental data from several brain activity datasets, paving the ground for future analyses uncovering the emergent structure of cognitive representations in biological and artificial intelligence systems. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Cognitive processes usually take place in systems made of multiple interacting parts, e.g. neurons composing the nervous system of an organism. Importantly, cognitive processes themselves don\u2019t seem to take place at a \u2018microscopic\u2019 level of individual units, but at \u2018macroscopic\u2019 levels involving assemblies of several coordinated units [28]. Hence, when trying to unveil the inner workings of a \u2014 natural or artificial \u2014 cognitive system, it is crucial to be able to identify relevant macroscopic variables that best characterise the corresponding cognitive processes. ", "page_idx": 0}, {"type": "text", "text": "The identification of macroscopic variables has traditionally been driven by intuition and expert knowledge. For example, the investigation of collective behaviour in statistical physics is based on macroscopic variables known as \u2018order parameters,\u2019 which are typically identified heuristically and then used to describe phase transitions and other phenomena of interest [49]. Unfortunately, identifying relevant macroscopic variables is often more an art than a science, being heavily dependent on prior knowledge and expectations. Having automated procedures to identify relevant macroscopic variables of cognitive systems would open important avenues for investigating the inner workings of different cognitive architectures. ", "page_idx": 0}, {"type": "text", "text": "A promising approach to identify empirically useful macroscopic variables is provided by unsupervised representation learning [38, 22, 52]. For example, information maximisation has proven to be a powerful objective for learning representations within neural networks [23, 38]. In this paper we combine this approach with recent breakthroughs in our ability to formally characterise emergent phenomena [43, 34], which have proven to be not only theoretically sound but also empirically powerful [30, 40]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Building on this literature, in this paper we leverage recently proposed metrics of emergence to identify representations that display emergent properties. Specifically, we propose an end-to-end differentiable architecture that can learn maximally emergent representations of multivariate time series data. Our results show that causal emergence speeds up learning of more complex features of the data relative to pure mutual information maximisation, and we demonstrate the scalability of our method through an analysis of real-world brain activity data. ", "page_idx": 1}, {"type": "text", "text": "2 Methods ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Quantifying emergence ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Consider a scientist measuring a system of interest composed of $n$ parts, and let $X_{t}^{i}$ denote the state of part $i$ at time $t$ . The information that the joint process carries together from $t$ to $t^{\\prime}$ can be quantified by the standard Shannon mutual information $I(\\bar{\\mathbf{X}}_{t};\\mathbf{X}_{t^{\\prime}})$ , where $\\bar{\\mathbf{X}}_{t}=(X_{t}^{1},...,X_{t}^{n})$ is the joint state of the system at time $t$ . ", "page_idx": 1}, {"type": "text", "text": "How can one characterise an emergent macroscopic variable of such system? Following Ref. [43], one can define emergent variables $V_{t}$ as satisfying two key criteria: ", "page_idx": 1}, {"type": "text", "text": "(i) Supervenience: there exists a function (or coarse-graining) $f$ such that $V_{t}=f(\\mathbf{X}_{t})$ . ", "page_idx": 1}, {"type": "text", "text": "(ii) Unique information: $V_{t}$ holds unique predictive information about the future evolution of the system $X_{t^{\\prime}}$ that cannot be found in the individual parts $X_{t}^{1},\\ldots,X_{t}^{n}$ by themselves. ", "page_idx": 1}, {"type": "text", "text": "Critically, the unique information that $V_{t}$ holds about $X_{t^{\\prime}}$ can be rigorously quantified using the framework of Partial Information Decomposition (PID, [54]), and its recent extension to time series data $\\Phi\\mathrm{ID}$ , [33]). Emergence, therefore, is defined as the capability of a supervenient variable to provide predictive power that cannot be reduced to underlying microscale phenomena. ", "page_idx": 1}, {"type": "text", "text": "Quantifying unique information in high-dimensional systems can be highly non-trivial. Luckily, the $\\Phi\\mathrm{{ID}}$ formalism allows us to derive simpler measures that provide sufficient criteria for emergence. In particular, it has been shown that the following is a sufficient condition for causal emergence [43]: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\Psi:=I(V_{t};V_{t+1})-\\sum_{i}I(X_{t}^{i};V_{t+1})>0\\;.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Importantly, $\\Psi$ is comparatively easy to calculate, as it relies only on pairwise marginal distributions and on standard Shannon mutual information. These key features allow the framework to be applicable on a wide range of scenarios, as illustrated by the applications reviewed in Ref. [34]. Note that here we take $t^{\\prime}=t+1$ , but in principle any $t^{\\prime}>\\bar{t}$ is valid. ", "page_idx": 1}, {"type": "text", "text": "The reason why $\\Psi\\,>\\,0$ is only a sufficient, but not necessary, condition for emergence is that in some systems multiple $X_{t}^{i}$ can have the same information about $V_{t+1}$ , and hence the sum of terms $I(X_{t}^{i};\\dot{V}_{t+1})$ may \u2018double-count\u2019 information \u2014 resulting in a negative bias in $\\Psi$ . This doublecounting can be alleviated by discounting from Eq. (1) the redundant information between the $X_{t}^{i}$ \u2019s about $V_{t+1}$ . Here we do this using a measure of redundancy known as \u2018Minimum Mutual Information\u2019 [4], which yields the following \u2018adjusted\u2019 emergence criterion (Supp. Sec. C): ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\Psi_{A}:=I(V_{t};V_{t+1})-\\sum_{i}I(X_{t}^{i};V_{t+1})+(n-1)\\operatorname*{min}_{i}I(X_{t}^{i};V_{t+1})>0\\;.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "2.2 Model architecture and information estimators ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Maximising emergence. Our aim is to establish an automated procedure to identify emergent macroscopic variables $V_{t}$ with respect to a microscopic substrate $X_{t}$ . For this, we investigate parametric coarse-grainings $V_{t}=f_{\\theta}(\\mathbf{X}_{t})$ that can be optimised to maximise $\\Psi$ via a differentiable objective function. ", "page_idx": 1}, {"type": "image", "img_path": "z6reLFqv6w/tmp/d19f2cfbf604375d45b9279c3ca1af8f10cb79f91df8e6ba43293422610745d8.jpg", "img_caption": ["Figure 1: Architecture for calculating loss terms for learning causally emergent representations. A representation network $f_{\\theta}$ applied to the data $X_{t}$ learns a feature $V_{t}$ . This feature is trained to optimise $\\Psi$ made up of predictive and marginal mutual information terms estimated by $g_{\\varphi}$ and $h_{\\xi^{i}}$ respectively (left). A further critic $k_{\\sigma}$ may be added to calculate the mutual information with another emergent feature $V_{t}^{A}$ , to encourage the learning of a diverse set of emergent features (right). "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "A key ingredient to maximising $\\Psi$ is employing a suitable estimator of Shannon\u2019s mutual information. Although many estimators of mutual information exist [27, 29], most are not differentiable, and thus not suitable for optimisation with standard representation learning architectures. Fortunately, a number of differentiable estimators of mutual information exist [39, 50]. ", "page_idx": 2}, {"type": "text", "text": "We use the Smoothed Mutual Information \u201cLower-bound\u201d Estimator (SMILE, [50]), which is one of a family of approaches that formulates mutual information estimation as a variational problem, and was specifically designed to address the issue of high variance in existing estimators such as NWJ [37] and MINE [5]. The SMILE mutual information estimator is given by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I(X;Y)=\\mathbb{E}_{p(x,y)}\\left[\\log\\frac{p(x,y)}{p(x)p(y)}\\right]}\\\\ &{\\qquad\\qquad\\geq\\mathbb{E}_{p(x,y)}\\left[g_{\\varphi}(x,y)\\right]-\\log\\mathbb{E}_{p(x)p(y)}\\left[\\mathrm{clip}\\left(e^{g_{\\varphi}(x,y)},e^{-\\tau},e^{\\tau}\\right)\\right]\\triangleq I_{\\varphi}^{S}\\left(X;Y\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $g_{\\varphi}$ is a parameterised function that estimates the log density ratio $\\log\\left(p(x,y)/(p(x)p(y))\\right)$ , $\\operatorname{clip}(v,l,u)=\\operatorname*{max}\\left(\\operatorname*{min}\\left(v,u\\right),l\\right)$ and $\\tau\\geq0$ is a hyperparameter. As $\\tau\\rightarrow\\infty$ , $I^{\\mathrm{S}}$ converges to the MINE estimation [5], but a finite $\\tau$ prevents the potentially exponential growth of the variance of the estimate with MI, which MINE suffers from [50]. ", "page_idx": 2}, {"type": "text", "text": "Equipped with this estimator, we can now formulate our representation learning algorithm for causally emergent features. The architecture is schematically shown in Fig. 1. Our main method involves three learnable functions: ", "page_idx": 2}, {"type": "text", "text": "1. A representation network $f_{\\theta}$ , that learns a supervenient variable $V_{t}=f_{\\theta}(\\mathbf{X}_{t})$ . 2. A critic for the macroscopic variable $g_{\\varphi}$ , that controls the estimation of $I(V_{t};V_{t+1})$ . 3. A set of critics for the microscopic variable $h_{\\xi^{i}}$ , that control the estimation of $I(X_{t}^{i};V_{t+1})$ . The number of critics equals the number of constituent atoms of the system.1 ", "page_idx": 2}, {"type": "text", "text": "We will demonstrate that the representation critic can learn emergent features on an algorithmic dataset by maximising the following metric: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Psi^{\\cal S}(\\theta,\\varphi,\\{\\xi^{i}\\}):=I_{\\varphi}^{\\cal S}(f_{\\theta}(X_{t});f_{\\theta}(X_{t+1}))-\\sum_{i}I_{\\xi^{i}}^{\\cal S}(X_{t}^{i};f_{\\theta}(X_{t+1}))\\,.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We refer to $\\Psi^{S}$ as the emergence objective function, and the first term in the RHS of Eq. (5) as the predictive mutual information [6] \u2013 since (by the data processing inequality [15]) it represents a lower bound on the joint mutual information between the past and future states of the whole system, $I(X_{t};X_{t^{\\prime}})$ . As a control condition, we also ran experiments with an objective function consisting of only the predictive information, removing the marginal mutual information terms. After training has converged, we ran \u2018post-hoc\u2019 tests to further verify that the learned feature is emergent by freezing the representation network $f_{\\theta}$ , retraining the critics to accurately estimate $I(V_{t};V_{t+1}\\bar{)}$ and $I(X_{t}^{i};\\bar{V_{t+1}})$ , and calculating $\\Psi_{A}$ . If this resulting $\\Psi_{A}$ is positive, we conclude the feature is emergent. For convenience, pseudocode for the algorithm used to train the model is provided in Supp. Sec. A. ", "page_idx": 3}, {"type": "text", "text": "Learning diverse emergent features. A well-known fact about complex systems is that they can have more than one emergent property, and/or be described by multiple order parameters [25]. ", "page_idx": 3}, {"type": "text", "text": "To learn a diverse set of emergent features, we consider a scenario where an emergent feature $V_{t}^{A}$ has been learned and its corresponding parameters $\\theta^{A}$ are fixed. Intuitively, our goal is to find a new feature that is substantially different from the existing one by incentivising it to be statistically independent from $V_{t}^{A}$ . To implement this, we add a penalty term $I_{\\sigma}^{\\mathrm{S}}(f_{\\theta^{A}}(\\mathbf{\\bar{X}}_{t});f_{\\theta}(\\mathbf{X}_{t}))$ to the objective function and include a fourth learnable function to our method: a critic $k_{\\sigma}$ that estimates this mutual information, also using SMILE. ", "page_idx": 3}, {"type": "text", "text": "Improving training stability. An important point to note of the proposed method is that there is an adversarial relationship between the microscopic critics $h_{\\xi^{i}}$ and the macroscopic critic $g_{\\varphi}$ through the representation network $f_{\\theta}$ , reminiscent of the adversarial learning dynamics in GANs [16]. In essence, the former pushes the representation network $f_{\\theta}$ to maximize information about the system\u2019s state, while the latter pushes it to minimize it. This could lead to potentially unstable learning dynamics and various failure modes \u2013 for example, a representation network could \u201ctrick\u201d the microscopic critics into reducing their MI estimate by exploiting their mistakes, rather than genuinely reducing $I(X_{t}^{i};V_{t+1})$ , resulting in an artificially inflated value of $\\Psi$ . ", "page_idx": 3}, {"type": "text", "text": "Fortunately, unlike in GANs, it is not harmful for the critics to be overpowered compared to the representation learner2 because the SMILE estimation is bounded from above by the true MI [50]. Following this reasoning, we $i$ ) update the critics multiple times for each representation learner update, using a higher learning rate and more parameters; and $i i$ ) pre-train the critics before we start training the representation learner, so that they provide a more robust MI estimation and better training signal. This results in slower, but more stable, learning dynamics for $f_{\\theta}$ . ", "page_idx": 3}, {"type": "text", "text": "Another method to increase training stability is to use a dedicated choice of the architecture of the representation network. $f_{\\theta}$ first linearly projects the input to a higher-dimensional space, then passes it through an MLP with residual (\u2018skip\u2019) connections, and then projects it linearly down to the output vector $V_{t}$ . Because of the skip connections (and because of the auto-correlation of $X_{t}$ ), at initialisation the output already contains useful information about the system and yields a high value of the macroscopic information $I_{\\varphi}^{\\mathrm{S}}(f_{\\theta}(X_{t});f_{\\theta}(X_{t+1}))$ before $\\theta$ has been trained. Therefore, this allows us to reduce the contribution of this term in the objective function, and thus reduce the influence of $g_{\\varphi}$ on $f_{\\theta}$ .3 ", "page_idx": 3}, {"type": "text", "text": "2.3 Datasets ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "2.3.1 Synthetic datasets ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Bit-string dataset. We evaluate our method for learning causally emergent representations by applying it to sequences of random bit-strings of length $n$ with two constructed temporal correlations: ", "page_idx": 3}, {"type": "text", "text": "1. The parity of the first $n-1$ bits is auto-correlated across time, such that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{P}\\big\\{\\;\\oplus_{i=1}^{n-1}X_{t+1}^{i}=\\oplus_{i=1}^{n-1}X_{t}^{i}\\big\\}=\\gamma_{\\mathrm{parity}}>\\frac{1}{2}\\;,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\bigoplus$ represents modulo-2 addition. ", "page_idx": 3}, {"type": "text", "text": "2. The last (or extra) bit in the bit-string $X^{n}$ is auto-correlated across time, such that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{P}\\big\\{X_{t+1}^{n}=X_{t}^{n}\\big\\}=\\gamma_{\\mathrm{extra}}>\\frac{1}{2}\\;.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Since parity is a synergistic function of the bits of a bit-string (i.e. it cannot be predicted from any of the input bits individually [42]), and since the parity predicts some information about the future evolution of the system, $V_{t}=\\oplus_{i=1}^{n-1}X_{t}$ is an emergent feature of the system. ", "page_idx": 4}, {"type": "text", "text": "Despite its simplicity, this dataset has two important advantages: there is a known emergent feature (the parity), and one can calculate the mutual information and the emergence measure analytically.4 These properties will allow us to verify that the model has successfully extracted the expected emergent properties and that mutual information is being accurately estimated. ", "page_idx": 4}, {"type": "text", "text": "Conway\u2019s Game of Life. To further evaluate our method on a more complex system, we apply it to simulations from Conway\u2019s Game of Life (GOL) \u2013 a canonical setting for the study of emergence [1]. GOL is a cellular automaton on an $N\\times N$ grid where each cell\u2019s state evolves based on simple, deterministic rules. We initialize a \u201cglider\u201d pattern [19] at a random position within the grid, and this glider moves diagonally by cycling through four distinct states until it reaches a boundary and the simulation concludes. The dataset consists of multiple such simulations concatenated to form a continuous sequence. To effectively capture the spatial dependencies inherent in the Game of Life, we replace the multilayer perceptron used in previous experiments with a convolutional neural network for the representation learner $f_{\\theta}$ , allowing us to exploit the 2D structure of the grid-based simulations. ", "page_idx": 4}, {"type": "text", "text": "2.3.2 Real world brain datasets ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Primate ECoG dataset. We evaluate our method on a dataset of electrocorticography (ECoG) brain activity data from a macaque monkey, originally reported by Chao et al. [10]. The dataset contains long time series of electrical activity measured with 64 electrodes placed across the monkey\u2019s brain surface. We minimally pre-process the data by applying a second-order Butterworth high-pass filter with a $1\\,\\mathrm{Hz}$ cutoff, downsampling the signals to $300\\,\\mathrm{Hz}$ , and finally standardising the data before applying our method to learn emergent features from it. ", "page_idx": 4}, {"type": "text", "text": "Human MEG dataset. We evaluate our method on a magnetoencephalography (MEG) brain activity dataset from healthy human participants, originally reported in four pharmaco-MEG studies [36]. The dataset includes resting-state recordings collected before the administration of various pharmacological agents. MEG signals were sampled at $600\\,\\mathrm{Hz}$ with a $\\mathrm{0{-}300\\,H z}$ bandpass filter. For our analysis, we focus on the placebo condition (pre-drug administration) and train our model $f_{\\theta}$ on data from multiple subjects, using each epoch to represent a different participant to capture features common across individuals. Data was pre-processed following standard procedure [9] and standardised before applying our method to learn emergent features. ", "page_idx": 4}, {"type": "text", "text": "Human fMRI dataset. We also evaluate our method on a functional magnetic resonance imaging (fMRI) dataset from 100 unrelated human participants from the \u201cminimally preprocessed\u201d release by the Human Connectome Project [53, 20]. We performed additional preprocessing following Luppi et al. [31] and calculated 100 time series capturing the activity of each of the regions in the Schaefer brain atlas [45]. We further standardise the data before applying our method to learn emergent features from it. ", "page_idx": 4}, {"type": "text", "text": "3 Results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we discuss results on learning emergent features on the synthetic and real world datasets introduced above, as well as comparisons to baselines and some ablation studies. All experiments can be run on a single A10G GPU in less than two hours. ", "page_idx": 4}, {"type": "image", "img_path": "z6reLFqv6w/tmp/9c5366597348c1d8bca7ba8aaef1cb4575c20dc6da1a0164ac8dd0768144f475.jpg", "img_caption": ["Figure 2: The proposed architecture recovers ground truth emergent features. Using the emergence objective function (left column), the model finds the correct $\\Psi$ value and is able to recover the known emergent feature (parity bit). Using only predictive MI as the objective (right column), the model fails to discover any emergent features. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "3.1 Learning emergent features in synthetic datasets ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Bit-string dataset. Results show that our proposed architecture can accurately estimate the groundtruth value of $\\Psi$ in the synthetic dataset, confirming it is able to learn causally emergent representations (Fig. 2). To interpret the contents of the learned representation, we trained decoders with standard supervised learning to predict both the parity of the first $n\\,-\\,1$ bits (the parity bit) and the last auto-correlated bit (the extra bit). We found that the parity bit could be decoded with high accuracy but the extra bit could not, confirming that the learned representation indeed corresponded to an emergent feature. For additional verification, we fixed the representation learner\u2019s weights $\\theta$ and re-trained both critics to estimate both $\\Psi$ and $\\Psi_{A}$ , which were both positive and confirmed our results. ", "page_idx": 5}, {"type": "text", "text": "As expected, when the marginal MI terms are removed from the objective function (Fig. 2, right column), the model is no longer able to obtain the correct $\\Psi$ value \u2013 and, interestingly, only the extra bit (but not the parity bit) is encoded in the representation. We hypothesise that, in the absence of the regularisation induced by the marginal MI, the system\u2019s inductive biases lead it towards learning \u201clow-order\u201d (i.e. non-emergent) representations. Note that, despite having a constraint removed, the model without marginal MI loss is unable to extract the full predictive information of the system (which equals approximately 1.84 bit), showing that using the full emergence loss could incentivise the system to learn features that provide information about the system\u2019s dynamics that would otherwise be ignored (we elaborate on this further in Section 3.3). We obtain qualitatively similar results with a noisier version of the same data generating process (Supp. Fig. 5). ", "page_idx": 5}, {"type": "text", "text": "Conway\u2019s Game of Life. By employing a convolutional neural network for the representation learner $f_{\\theta}$ , we successfully learned emergent features from this dataset, evidenced by a positive $\\Psi_{A}$ value after freezing the representation network and retraining the critics (Supp. Fig. 8a). To interpret the emergent feature, we attempted to predict the glider\u2019s position and state from the representation. While the position prediction performed at chance level, the state prediction achieved an accuracy of $53\\%$ , significantly higher than the random baseline of $25\\%$ (Supp. Fig. 8b). This suggests that the emergent feature encodes information about the glider\u2019s state. When $f_{\\theta}$ is trained with a predictiononly objective, the state prediction achieved an accuracy of $35\\%$ , suggesting the learned feature encodes less information about the state of the glider when trained on this objective. ", "page_idx": 5}, {"type": "image", "img_path": "z6reLFqv6w/tmp/f01c3fbaad1f7f16db767128025e3f3cc1a86776f59247c82503d46985a5a688.jpg", "img_caption": ["Figure 3: Our method learns a diverse set of multiple emergent features from the same system. Training two representation learners to learn independent emergent feature on the synthetic dataset. Both learned features were emergent (top row). The first learner (left column) yielded a feature that has high mutual information with the system\u2019s parity bit (bottom left), while for the second learner (right column) it had high mutual information with the bonus bit (bottom right). "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "These findings demonstrate that our approach can learn meaningful emergent features in complex, high-dimensional systems like the Game of Life, capturing collective behaviors that are not localized to individual components. ", "page_idx": 6}, {"type": "text", "text": "3.2 Learning diverse features in the bit-string dataset ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "As our next step, we set out to test our algorithm to learn diverse features in the same bit-string dataset. For this set of experiments we used the representation learner with skip connections described in Section 2.2, ran our algorithm as above until convergence, and fixed the representation learner\u2019s weights (denoted by $\\bar{\\theta^{A}}$ ). We then trained a second representation learner with the regular $\\Psi^{S}$ objective plus a penalty term $I(V_{t};V_{t}^{A})$ to obtain a new feature that is statistically independent from the parity bit. ", "page_idx": 6}, {"type": "text", "text": "Interestingly, this process revealed a new, unexpected feature that was not originally designed (Fig. 3): the XOR of the parity and extra bit (which we will refer to simply as the bonus bit). This bit is emergent (since it cannot be predicted by any of the $X_{t}^{i}$ and is auto-correlated, since both the parity and extra bits are), albeit with a lower $\\Psi$ than the parity bit \u2013 which fits well with the result that our model did not learn this feature spontaneously. This shows the capability of our method to discover new aspects of a system under study, even in the case of simple and explicitly constructed systems. ", "page_idx": 6}, {"type": "text", "text": "3.3 Comparison to baselines and ablation studies ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Comparison with RNNs. In order to investigate whether the failure of the prediction-only baseline to learn emergent features was to do with the capacity of the architecture, we trained a standard RNN on the bit-string dataset using mean squared error (MSE) loss, with its hidden state serving as the representation for decoding. The hidden state dimension matched that of our emergent feature network $f_{\\theta}$ for a fair comparison. As shown in Fig. 4, the RNN consistently learned the non-emergent extra bit but encoded negligible information about the two emergent bits, resulting in $\\Psi_{A}=0$ . This indicates that the RNN did not capture any emergent features. ", "page_idx": 6}, {"type": "image", "img_path": "z6reLFqv6w/tmp/06253118302271d670fe3ff93f28a3b74e73b11e0bdeda8131ecfe71d4d65fa3.jpg", "img_caption": ["RNN only ", "Figure 4: Standard methods do not learn emergent features, and their performance increases when combined with emergent features.. The hidden state of an RNN trained on the bit-string dataset has negligible $\\Psi_{A}$ , indicating no emergent feature learned (left). Accordingly, the mutual information between the hidden state and the extra, parity, and bonus bits shows that only the nonemergent extra bit is encoded (middle). Interestingly, representations learned by an RNN and by our method can be combined to yield better predictions of the future state of the system (right). "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Combining emergent and non-emergent features. The fact that standard architectures learn representations that are predictive, yet not emergent, suggests they may be learning different aspects of the data compared to our method. To test this hypothesis, we investigated whether combining emergent features from our method with representations from standard models would enhance the predictive performance of either. As a proof of concept, we trained both an RNN and our emergence learner $f_{\\theta}$ on the bit-string dataset, and compared the mutual information between $X_{t^{\\prime}}$ and: ", "page_idx": 7}, {"type": "text", "text": "1. The hidden state of the RNN.   \n2. The emergent feature $V_{t}$ learned by $f_{\\theta}$ .   \n3. The concatenation of $V_{t}$ with the hidden state of an RNN.5 ", "page_idx": 7}, {"type": "text", "text": "As shown in Fig. 4, the combined representation encoded more information about the future state of the system, $X_{t^{\\prime}}$ , than either the RNN or the emergence learner alone. While this result is only shown on a very simple synthetic dataset, it suggests that our emergence learner extracts meaningful features that are different from, and can be effectively combined with, other representation learning architectures. We hypothesise that standard representation learning techniques can more effectively capture microscale properties, and since our method captures macroscale properties, both can be naturally combined to enhance performance in downstream tasks. ", "page_idx": 7}, {"type": "text", "text": "Ablations to emergence loss. The main emergence loss function we propose in this work has three terms: a predictive MI term estimating $I(V_{t};\\bar{V}_{t^{\\prime}})$ , marginal MI terms estimating $I(X_{t}^{i};V_{t^{\\prime}})$ , and a diversity term estimating $I(V_{t}^{A};V_{t})$ . In order to understand the contribution of each term to the behaviour of the model, we performed ablation studies removing each one of these. We describe each ablation in turn: ", "page_idx": 7}, {"type": "text", "text": "1. As seen in Sec. 3.2 (and later in Sec. 3.4), ablating the $I(V_{t};V_{t^{\\prime}})$ term from $\\Psi$ objective results in the network learning features that are not as correlated in time as they would otherwise be, but that still satisfy the criteria for emergence. Therefore, our proposed architecture with skip connections may learn slightly less emergent features, but with the advantage of more stable training dynamics. 2. Ablating the $I(X_{t}^{i};V_{t^{\\prime}})$ terms objective results in an algorithm that only maximises predictive information $I(V_{t};V_{t^{\\prime}})$ , akin to the Deep InfoMax method [23]. As seen in Sec. 3.1, and as expected, training without the marginal terms results in representations that are not emergent. ", "page_idx": 7}, {"type": "text", "text": "3. Ablating the diversity term in the experiments in Sec. 3.2 and training multiple runs on the bit-string dataset results in the representation network learning the extra bit $91\\%$ of runs, and the bonus bit $9\\%$ of runs. This implies there are strong inductive biases towards which emergent features are learned, and it is thanks to the diversity term that we can reliably recover both. ", "page_idx": 8}, {"type": "text", "text": "3.4 Learning emergent features in brain activity data ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Finally, we demonstrate the scalability of our method by learning emergent features in three types of high-dimensional brain activity datasets \u2013 ECoG, MEG, and fMRI \u2013 to evaluate its ability to learn emergent features across different neural recording modalities and spatial scales. ", "page_idx": 8}, {"type": "text", "text": "ECoG data. For the ECoG dataset [10], it was crucial to use skip connections in the representation learner, since the method as used in Section 3.1 was consistently unable to find emergent features (despite their known presence in this dataset [43]). ", "page_idx": 8}, {"type": "text", "text": "With this configuration, our method is able to successfully learn emergent representations of the ECoG data (Supp. Fig. 9a). The resulting feature was verified to be emergent by a positive post-hoc $\\Psi_{A}$ test. Interestingly, using an $f_{\\theta}$ with skip connections we obtained emergent features even when the macroscopic MI term $I(V_{t};V_{t+1})$ was completely removed from the objective function, which further simplified learning dynamics. Empirically, we found that this does not cause $I_{\\varphi}^{\\mathrm{S}}(f_{\\theta}(\\mathbf{X}_{t});f_{\\theta}(X_{t+1}))$ to decrease over training substantially, suggesting that using just the marginal MI terms is a good objective function to remove information about the parts while preserving information about the whole that was present at initialisation. ", "page_idx": 8}, {"type": "text", "text": "As a final experiment, we also found that our method could learn a second emergent feature, also verified with a post-hoc $\\Psi_{A}$ check (Supp. Fig. 10). ", "page_idx": 8}, {"type": "text", "text": "MEG data. We extended our analysis to magnetoencephalography (MEG) recordings from healthy human participants [36]. Training on data from multiple subjects, our method readily learned emergent features, as evidenced by a positive post-hoc $\\Psi_{A}$ check (Supp. Fig. 9b). The consistency of emergent features across different individuals suggests that our model captures fundamental aspects of neural dynamics common to human brain activity. ", "page_idx": 8}, {"type": "text", "text": "Notably, emergent features were learned more easily from MEG data compared to ECoG data, potentially due to differences in spatial resolution and the nature of the recorded signals. ", "page_idx": 8}, {"type": "text", "text": "fMRI data. Finally, we applied our method to functional magnetic resonance imaging (fMRI) data from 100 unrelated human participants [45]. Despite the coarser spatial and temporal resolution of fMRI, our method was able to learn emergent features, as indicated by a positive $\\Psi_{A}$ value (Supp. Fig. 9c). This demonstrates that emergent dynamics are present even at the macroscopic scale of brain activity. On this dataset, we also determined that a standard MLP did not learn emergent features in its representation (see Supp. Sec. B.2 for more details). ", "page_idx": 8}, {"type": "text", "text": "The successful extraction of emergent features across all three datasets highlights the robustness of our method. It underscores its capability to uncover collective neural dynamics in complex, high-dimensional brain activity data, regardless of the recording modality or spatial scale. ", "page_idx": 8}, {"type": "text", "text": "Table 1: Our method learns emergent features in multiscale datasets of brain activity. Final values of $\\Psi$ and $\\Psi_{A}$ as found during the training run of $f_{\\theta}$ and post-hoc evaluation respectively. All values are greater than zero, indicating that in each case an emergent feature has been found. ", "page_idx": 8}, {"type": "table", "img_path": "z6reLFqv6w/tmp/edb92535b6d9b19be510d2eaedf241fd3f4ce872631ba99928c4fddf8a2f0d96.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4 Related work and future directions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The method we have proposed here is part of a growing literature leveraging methods from information theory to enhance deep learning architectures, and representation learning algorithms in particular. A small, far from exhaustive list of some noteworthy examples includes InfoNCE [38], deep variational information bottleneck [2], $\\beta$ -VAE [8], or TC-VAE [11]. In future work, it would be interesting to see if our method is competitive or can be combined with recent representation learning methods on standard benchmarks [22, 38, 52]. ", "page_idx": 9}, {"type": "text", "text": "In practice, one of the challenging aspects of this method is to overcome the instability of training to make sure a valid emergent solution is found (i.e. one where the post-hoc check still shows $\\Psi_{A}>0.$ ). One potential approach is to incorporate stabilisation techniques from the GAN literature (such as e.g. spectral normalisation [35]). Another potential approach is to use a mutual information upper bound (instead of our current lower bound) for the marginal MI terms. Although mutual information upper bounds are not as common as lower bounds, there are a few options available [13, 12]. In practice, we found both of these to be less effective than our skip connection method, but they remain a promising avenue for future work. ", "page_idx": 9}, {"type": "text", "text": "There is a growing body of machine learning research that engages directly with PID, the informationtheoretic backbone of our emergence theory. For example, recent work has proposed a differentiable redundancy measure [32], which has been used as an objective function to train deep neural networks [21]. Alternatively, there are also methods that estimate redundancy using deep neural networks [26], analogous to SMILE. Although none of these estimators can be directly applied to estimate the unique information that underlies the definition of emergence (Section 2.1), we expect that extensions of these exciting developments will also open new possibilities in the study of emergence. ", "page_idx": 9}, {"type": "text", "text": "Finally, it is worth mentioning that there are a number of other approaches that focus on different aspects of emergence (see Supp. Sec. D). The approach presented here was chosen for two reasons: (i) its intuitive nature, focusing on how emergent properties arise from collective interactions that cannot be fully explained by examining components in isolation; and (ii) the existence of efficient, scalable, and differentiable proxies for its estimation. Constructing similar methods as the one proposed here for other metrics of emergence would be a challenging but extremely interesting line of future work. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Emergence, the phenomenon whereby a system becomes \u2018more than the sum of its parts\u2019, is an promising conceptual tool to investigate cognitive processes in artificial and biological systems. In this paper, we proposed a machine learning method for discovering emergent variables in time series data that leverages a recent information-theoretic characterisation of emergence [43] and advances in mutual information estimation from data with neural networks [50]. ", "page_idx": 9}, {"type": "text", "text": "We first showed in a synthetic dataset that our method can estimate emergence and successfully discover a known emergent feature. Interestingly, a pure information maximisation objective struggled to learn this feature, suggesting that our method facilitates the identification of complex features of the data. Furthermore, we also proposed a slight modification of our method that can learn a diverse set of features from the same system. Finally, we also showed that our method can scale up to learn emergent features in real-world brain activity data. ", "page_idx": 9}, {"type": "text", "text": "Overall, our method opens up a range of possibilities for the practical study of emergence, as well as for other machine learning problems more broadly. For example, our method may be explored in conjunction with other representation learning algorithms to capture aspects of complex systems that are otherwise difficult to learn. From an application perspective, we hope this method can be further leveraged in neuroscience to reveal new aspects of brain function. ", "page_idx": 9}, {"type": "text", "text": "Software availability ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Code implementing our proposed architecture and reproducing our key results is available at https: //github.com/Imperial-MIND-lab/causally-emergent-representations ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] A. Adamatzky. Game of Life Cellular Automata, volume 1. Springer, 2010.   \n[2] A. A. Alemi, I. Fischer, J. V. Dillon, and K. Murphy. Deep variational information bottleneck. arXiv:1612.00410, 2016. [3] L. Barnett and A. K. Seth. Dynamical independence: Discovering emergent macroscopic processes in complex dynamical systems. Physical Review E, 108(1), July 2023. [4] A. B. Barrett. Exploration of synergistic and redundant information sharing in static and dynamical Gaussian systems. Physical Review E, 91(5):052802, 2015. [5] M. I. Belghazi, A. Baratin, S. Rajeswar, S. Ozair, Y. Bengio, A. Courville, and R. D. Hjelm. Mine: mutual information neural estimation. arXiv preprint arXiv:1801.04062, 2018.   \n[6] W. Bialek, I. Nemenman, and N. Tishby. Predictability, complexity, and learning. Neural Computation, 13(11):2409\u20132463, 2001.   \n[7] T. Bossomaier, L. Barnett, M. Harr\u00e9, J. T. Lizier, T. Bossomaier, L. Barnett, M. Harr\u00e9, and J. T. Lizier. Transfer entropy. Springer, 2016.   \n[8] C. P. Burgess, I. Higgins, A. Pal, L. Matthey, N. Watters, G. Desjardins, and A. Lerchner. Understanding disentangling in $\\beta$ -VAE. arXiv:1804.03599, 2018. [9] R. L. Carhart-Harris, S. Muthukumaraswamy, L. Roseman, M. Kaelen, W. Droog, K. Murphy, E. Tagliazucchi, E. E. Schenberg, T. Nest, C. Orban, et al. Neural correlates of the LSD experience revealed by multimodal neuroimaging. Proceedings of the National Academy of Sciences, 113(17):4853\u20134858, 2016.   \n[10] Z. C. Chao, Y. Nagasaka, and N. Fujii. Long-term asynchronous decoding of arm motion using electrocorticographic signals in monkey. Frontiers in neuroengineering, 3:1189, 2010.   \n[11] R. T. Chen, X. Li, R. B. Grosse, and D. K. Duvenaud. Isolating sources of disentanglement in variational autoencoders. Advances in Neural Information Processing Systems, 31, 2018.   \n[12] P. Cheng, W. Hao, and L. Carin. Estimating total correlation with mutual information bounds. In NeurIPS 2020 Workshop: Deep Learning through Information Geometry, 2020.   \n[13] P. Cheng, W. Hao, S. Dai, J. Liu, Z. Gan, and L. Carin. Club: A contrastive log-ratio upper bound of mutual information. In International Conference on Machine Learning, pages 1779\u20131788. PMLR, 2020.   \n[14] K. Conrad. Probability distributions and maximum entropy. Entropy, 6(452):10, 2004.   \n[15] T. M. Cover and J. Thomas. Elements of Information Theory. John Wiley & Sons, 1999.   \n[16] A. Creswell, T. White, V. Dumoulin, K. Arulkumaran, B. Sengupta, and A. A. Bharath. Generative adversarial networks: An overview. IEEE Signal Processing Magazine, 35(1):53\u201365, 2018.   \n[17] J. P. Crutchfield. The calculi of emergence: computation, dynamics and induction. Physica D: Nonlinear Phenomena, 75(1-3):11\u201354, 1994.   \n[18] J. P. Crutchfield. The origins of computational mechanics: A brief intellectual history and several clarifications. arXiv preprint arXiv:1710.06832, 2017.   \n[19] A. Dorin, J. McCabe, J. McCormack, G. Monro, and M. Whitelaw. A framework for understanding generative art. Digital Creativity, 23(3-4):239\u2013259, 2012.   \n[20] M. F. Glasser, S. N. Sotiropoulos, J. A. Wilson, T. S. Coalson, B. Fischl, J. L. Andersson, J. Xu, S. Jbabdi, M. Webster, J. R. Polimeni, et al. The minimal preprocessing pipelines for the Human Connectome Project. NeuroImage, 80:105\u2013124, 2013.   \n[21] M. Graetz, A. Makkeh, A. C. Schneider, D. A. Ehrlich, V. Priesemann, and M. Wibral. Infomorphic networks: Locally learning neural networks derived from partial information decomposition. arXiv:2306.02149, 2023.   \n[22] J.-B. Grill, F. Strub, F. Altch\u00e9, C. Tallec, P. Richemond, E. Buchatskaya, C. Doersch, B. Avila Pires, Z. Guo, M. Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems, 33:21271\u201321284, 2020.   \n[23] R. D. Hjelm, A. Fedorov, S. Lavoie-Marchildon, K. Grewal, P. Bachman, A. Trischler, and Y. Bengio. Learning deep representations by mutual information estimation and maximization. In International Conference on Learning Representations, 2018.   \n[24] E. P. Hoel, L. Albantakis, and G. Tononi. Quantifying causal emergence shows that macro can beat micro. Proceedings of the National Academy of Sciences, 110(49):19790\u201319795, 2013.   \n[25] H. J. Jensen. Complexity Science: The Study of Emergence. Cambridge University Press, 2022.   \n[26] M. Kleinman, A. Achille, S. Soatto, and J. C. Kao. Redundant information neural estimation. Entropy, 23(7):922, 2021.   \n[27] A. Kraskov, H. St\u00f6gbauer, and P. Grassberger. Estimating mutual information. Physical Review E, 69(6):066138, 2004.   \n[28] M. Levin. The computational boundary of a \u201cself\u201d: Developmental bioelectricity drives multicellularity and scale-free cognition. Frontiers in Psychology, 10:493866, 2019.   \n[29] J. T. Lizier. JIDT: An information-theoretic toolkit for studying the dynamics of complex systems. Frontiers in Robotics and AI, 1:11, 2014.   \n[30] A. I. Luppi, P. A. Mediano, F. E. Rosas, J. Allanson, J. D. Pickard, G. B. Williams, M. M. Craig, P. Finoia, A. R. Peattie, P. Coppola, et al. Reduced emergent character of neural dynamics in patients with a disrupted connectome. NeuroImage, 269:119926, 2023.   \n[31] A. I. Luppi, P. A. Mediano, F. E. Rosas, N. Holland, T. D. Fryer, J. T. O\u2019Brien, J. B. Rowe, D. K. Menon, D. Bor, and E. A. Stamatakis. A synergistic core for human brain evolution and cognition. Nature Neuroscience, 25(6):771\u2013782, 2022.   \n[32] A. Makkeh, A. J. Gutknecht, and M. Wibral. Introducing a differentiable measure of pointwise shared information. Physical Review E, 103(3):032149, 2021.   \n[33] P. A. Mediano, F. E. Rosas, A. I. Luppi, R. L. Carhart-Harris, D. Bor, A. K. Seth, and A. B. Barrett. Towards an extended taxonomy of information dynamics via Integrated Information Decomposition. arXiv:2109.13186, 2021.   \n[34] P. A. Mediano, F. E. Rosas, A. I. Luppi, H. J. Jensen, A. K. Seth, A. B. Barrett, R. L. CarhartHarris, and D. Bor. Greater than the parts: A review of the information decomposition approach to causal emergence. Philosophical Transactions of the Royal Society A, 380(2227):20210246, 2022.   \n[35] T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida. Spectral normalization for generative adversarial networks. arXiv:1802.05957, 2018.   \n[36] S. Muthukumaraswamy. PharmacoMEG 1/f data, 2018.   \n[37] X. Nguyen, M. J. Wainwright, and M. I. Jordan. Estimating divergence functionals and the likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory, 56(11):5847\u20135861, 2010.   \n[38] A. v. d. Oord, Y. Li, and O. Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.   \n[39] B. Poole, S. Ozair, A. van den Oord, A. A. Alemi, and G. Tucker. On variational lower bounds of mutual information. In NeurIPS Workshop on Bayesian Deep Learning, 2018.   \n[40] A. M. Proca, F. E. Rosas, A. I. Luppi, D. Bor, M. Crosby, and P. A. Mediano. Synergistic information supports modality integration and flexible learning in neural networks solving multiple tasks. arXiv preprint arXiv:2210.02996, 2022.   \n[41] F. E. Rosas, B. C. Geiger, A. I. Luppi, A. K. Seth, D. Polani, M. Gastpar, and P. A. Mediano. Software in the natural world: A computational approach to hierarchical emergence. Preprint arxiv, 2402:v2, 2024.   \n[42] F. E. Rosas, P. A. Mediano, M. Gastpar, and H. J. Jensen. Quantifying high-order interdependencies via multivariate extensions of the mutual information. Physical Review E, 100(3):032305, 2019.   \n[43] F. E. Rosas, P. A. Mediano, H. J. Jensen, A. K. Seth, A. B. Barrett, R. L. Carhart-Harris, and D. Bor. Reconciling emergences: An information-theoretic approach to identify causal emergence in multivariate data. PLoS Computational Biology, 16(12):e1008289, 2020.   \n[44] A. Rupe and J. P. Crutchfield. On principles of emergent organization. Physics Reports, 1071:1\u201347, 2024.   \n[45] A. Schaefer, R. Kong, E. M. Gordon, T. O. Laumann, X.-N. Zuo, A. J. Holmes, S. B. Eickhoff, and B. T. Yeo. Local-global parcellation of the human cerebral cortex from intrinsic functional connectivity MRI. Cerebral Cortex, 28(9):3095\u20133114, 2018.   \n[46] T. Schreiber. Measuring information transfer. Phys. Rev. Lett., 85:461\u2013464, Jul 2000.   \n[47] A. K. Seth. Measuring autonomy and emergence via granger causality. Artificial life, 16(2):179\u2013 196, 2010.   \n[48] C. R. Shalizi. Causal architecture, complexity and self-organization in time series and cellular automata. The University of Wisconsin-Madison, 2001.   \n[49] R. V. Sol\u00e9. Phase Transitions, volume 3. Princeton University Press, 2011.   \n[50] J. Song and S. Ermon. Understanding the limitations of variational mutual information estimators. In International Conference on Learning Representations, 2019.   \n[51] H. Thanh-Tung and T. Tran. Catastrophic forgetting and mode collapse in gans. In 2020 international joint conference on neural networks (ijcnn), pages 1\u201310. IEEE, 2020.   \n[52] N. Tomasev, I. Bica, B. McWilliams, L. Buesing, R. Pascanu, C. Blundell, and J. Mitrovic. Pushing the limits of self-supervised resnets: Can we outperform supervised learning without labels on imagenet? arXiv preprint arXiv:2201.05119, 2022.   \n[53] D. C. Van Essen, S. M. Smith, D. M. Barch, T. E. Behrens, E. Yacoub, K. Ugurbil, W.-M. H. Consortium, et al. The WU-Minn human connectome project: An overview. NeuroImage, 80:62\u201379, 2013.   \n[54] P. L. Williams and R. D. Beer. Nonnegative decomposition of multivariate information. arXiv preprint arXiv:1004.2515, 2010. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Model pseudocode ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Input: Dataset $T=\\{(x_{t},x_{t+1})\\}$ , num_epochs, number of atoms $n$ , Optional[pretrained representation network $f_{\\theta^{A}}]$   \nInitialize representation network $f_{\\theta}$ , predictive critic $g_{\\phi}$ , marginal critics $\\{h_{\\xi^{i}}\\}^{n}$ ,   \nOptional[diversity critic $k_{\\sigma}$ ]   \nfor $e p o c h\\gets1$ to num_epochs do for $\\boldsymbol{x}_{t},\\boldsymbol{x}_{t+1}$ in $\\mathcal{D}$ do $v_{t},v_{t+1}\\gets f_{\\theta}(x_{t}),f_{\\theta}(x_{t+1})$ $\\hat{I}_{v_{t},v_{t+1}}\\gets g_{\\varphi}(v_{t},v_{t+1})$ SMILE lower bound (Eq. 4) $g_{\\varphi}$ for $i\\gets1$ to $n$ do $\\begin{array}{r l}{\\Big|}&{{}\\hat{I}_{x_{t}^{i},v_{t+1}}\\gets h_{\\xi^{i}}(x_{t}^{i},v_{t+1})}\\end{array}$ Update $\\{h_{\\xi^{i}}\\}^{n}$ to maximise SMILE lower bound (Eq. 4) \u02c6\u03a8 = I\u02c6vt,vt+1 \u2212 in=1 I\u02c6xit,vt+1 if training diverse emergent feature then $\\begin{array}{r l}&{\\hat{I}_{v_{t},f_{\\theta}A}\\leftarrow k_{\\sigma}(v_{t},f_{\\theta^{A}}(x_{t}))}\\\\ &{\\mathrm{loss}\\leftarrow-\\hat{\\Psi}+\\hat{I}_{v_{t},f_{\\theta}A}}\\end{array}$ else \u2014 $\\mathrm{loss}\\gets-\\hat{\\Psi}$ end Update $f_{\\theta}$ to minimize loss end   \nend ", "page_idx": 13}, {"type": "text", "text": "B Supplementary results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Supplementary figures ", "page_idx": 14}, {"type": "image", "img_path": "z6reLFqv6w/tmp/411982ecbfef549da31e0ecbf1dd71db81bd56ad8408e2bb37439678ac6d14d5.jpg", "img_caption": ["Figure 5: Replication of main results with noisier data. Same as in Fig. 2, but with $\\gamma_{\\mathrm{parity}}=$ \u03b3extra = 0.9. "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "z6reLFqv6w/tmp/74894fc550999ae717f486f0132d1dbd09c6f91d29b1fc59c6df325351b68349.jpg", "img_caption": ["Figure 6: Replication of main results with even noisier data. Emergence values on the bit-string dataset with $\\gamma_{\\mathrm{parity}}=\\gamma_{\\mathrm{extra}}=0.7$ . "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "z6reLFqv6w/tmp/1967814ebf97ca2dcb8fc371603292efcf00a78295883d6ff0fd261e452f8458.jpg", "img_caption": ["Figure 7: Glider states in the Game of Life simulation. Gliders cycle deterministically between these four states. To interpret the learned emergent feature in the experiments in Section 3.1, we trained a standard classification MLP to predict these four states from the value of $V_{t}$ . "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "z6reLFqv6w/tmp/c795230d33e0c6bb96d0963ee1241add5a6ed77a9e027df9badbb75ad9d92986.jpg", "img_caption": ["Figure 8: Our method learns interpretable emergent features in Conway\u2019s Game of Life. a) Posthoc checks reveal $\\Psi_{A}>0$ , confirming the learned feature is emergent. b) Classification accuracy of the state of the glider (c.f. Supp. Fig. 7) on a held-out test set. Dashed black line represents chance level at $25\\%$ . "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "z6reLFqv6w/tmp/8aa45b124a9b87d12865f289410b7b3ed4f69c918ca0dfdad80490a1dc4ce170.jpg", "img_caption": ["Figure 9: Learning emergent features from brain activity datasets. Post-hoc $\\Psi_{A}$ checks for our representation learner trained on a) primate ECoG data, b) human MEG data, and c) human fMRI data. Emergent features were successfully learned in all cases. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "z6reLFqv6w/tmp/86dbcc7c6099eddccb82b2d5fd1193ca76deab53cfb823d5d572ff74c00b60c8.jpg", "img_caption": ["Figure 10: Learning diverse emergent features from ECoG brain activity data. a) After learning one emergent feature on the ECoG data (c.f. Supp. Fig. 9, adding the diversity loss term and training a new representation network results in another emergent feature, verified by a post-hoc $\\Psi_{A}$ check. b) We further verify that the new feature $V_{t}$ is nearly independent from the previous one $V_{t}^{A}$ , as their mutual information (estimated with SMILE and the critic $k_{\\sigma}$ ) vanishes. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "B.2 MLP comparison on fMRI Dataset ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In order to investigate further whether other representation learning methods could naturally learn emergent features, we also trained an encoder-decoder pair of multilayer perceptrons (MLPs) on the fMRI dataset to predict $X_{t^{\\prime}}$ from $X_{t}$ using an MSE objective. The encoder mapped $X_{t}$ to a representation $V_{t}$ with the same dimension as $f_{\\theta}$ \u2019s output. A post-hoc test revealed that the encoder did not learn an emergent feature, as $\\Psi_{A}<0$ (after training, the model stabilised at $\\Psi_{A}=-28\\pm5)$ ), indicating that standard autoencoder-like architectures may not capture emergent properties in complex data. ", "page_idx": 16}, {"type": "text", "text": "C Adjusted emergence metric ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Let us present an informal derivation of the correction that leads to the refined criteria for emergnece presented in Eq. 2. This formulation relies on the Partial Information Decomposition framework [54], on which our emergence metric $\\Psi$ is based. ", "page_idx": 16}, {"type": "text", "text": "Let us simplify the notation and use $B=V_{t+1}$ for the target variable and $A_{i}=X_{t}^{i}$ for the $i$ -th source of information. Taking inspiration in Ref. [43, Appendix A], the total information about $B$ provided by $A:=(A_{1},\\ldots,A_{n})$ can be expressed as ", "page_idx": 16}, {"type": "equation", "text": "$$\nI(\\pmb{A};B)=\\mathtt{R e d}(\\pmb{A};B)+\\mathtt{U n}(\\pmb{A};B)+\\mathtt{S y n}(\\pmb{A};B)\\;,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\operatorname{Red}(A;B)$ is the redundancy between sources, ${\\mathtt{S y n}}(A;B)$ is their synergy, and ${\\tt U n}(A;B)$ accounts for the unique information provided by each predictor and not the others. Of these, the synergistic component is the one associated to emergence by the theory in Ref. [43]. By using this decomposition and the chain rule of the mutual information [15], one can find that $I(A_{i};B)=\\operatorname{Red}(A;B)+\\Theta_{i}$ where $\\Theta_{i}$ accounts for other terms unique to source $i$ , and hence ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}I(A_{i};B)=n\\,\\mathtt{R e d}(A;B)+\\sum_{i=1}^{n}\\Theta_{i}\\;.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This shows that, when summing the marginal mutual informations, the term $\\operatorname{Red}(A;B)$ gets doublecounted $n$ times, resulting in $\\Psi$ containing this redundancy term $1-n$ times. Hence, it is natural to propose a refined criteria for emergence that avoids this double-counting, given by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Psi_{A}=I(V_{t};B)-\\Big(\\mathtt{R e d}(A;B)+\\sum_{i=1}^{n}\\Theta_{i}\\Big)=\\Psi+(n-1)\\mathtt{R e d}(A;B).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "D Alternative Formalizations of Emergence ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Several frameworks have been proposed to formalize the concept of emergence in complex systems. Below, we provide a brief overview of some approaches that are particularly well-suited to investigate time-series. ", "page_idx": 17}, {"type": "text", "text": "D.1 Emergence as dynamical independence ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Barnett [3] introduces a formalization of emergence based on information theory, defining an emergent macroscopic process $V_{t}=f(X_{t})$ as one where the dynamics of $V_{t}$ are conditionally independent of the microscopic history $X_{t}^{-}$ , given its own history $V_{t}^{-}$ . Accordingly, the condition for emergence is defined using transfer entropy [46, 7] as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\nT_{t}(X\\to V)=I(V_{t};X_{t}^{-}\\mid V_{t}^{-})=0.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This framework captures the idea that the macroscopic state\u2019s future is determined by its own history, and additional knowledge of the microstate does not enhance prediction. ", "page_idx": 17}, {"type": "text", "text": "D.2 Emergence via computational mechanics ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Computational mechanics [17, 18, 48] defines emergence through the concept of causal states, which are equivalence classes of histories S\u2190\u2212 leading to the same conditional distribution over futures \u2212S\u2192 . This leads to establish two keys quantities: the statistical complexity $C_{\\mu}$ and excess entropy $\\mathbf{E}$ , which are calculated as ", "page_idx": 17}, {"type": "equation", "text": "$$\nC_{\\mu}=H({\\mathcal{S}}),\\quad\\mathbf{E}=I(\\overleftarrow{S};\\overrightarrow{S}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Above, the excess entropy quantifies how much information is shared between past and future, and the statistical complexity measures how much information processing is needed to enable such prediction. ", "page_idx": 17}, {"type": "text", "text": "Building on these ideas, emergence has been characterised in different ways. One approach has been to identify a process $S_{t}^{\\prime}=f(\\overleftarrow{S}_{t})$ as emergent when it increases the efficiency of prediction $e={\\bf E}/C_{\\mu}$ compared to the microscopic description [48, Sec. 11.2]. Another approach has been to assign emergence when there is a new type of dynamics observed at the macroscopic level from the one observed at the micro [17, 44]; for example, dynamical patterns associated to a different computational class [17, Sec. 5.1]. Finally, another approach has been to ascribe emergence to macro-processes whose causal states are a coarse-graining of the causal states of the micro [41], which leads to a view closely related with the one presented in Section D.1. ", "page_idx": 17}, {"type": "text", "text": "D.3 Granger emergence ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Seth [47] proposes Granger emergence, where a macroscopic process $V_{t}$ is emergent with respect to the microscopic process $X_{t}$ if it satisfies the following conditions: ", "page_idx": 17}, {"type": "equation", "text": "$$\nI(V_{t};V_{t^{\\prime}}\\mid X_{t})>0\\quad\\mathrm{and}\\quad I(X_{t};V_{t^{\\prime}}\\mid V_{t})>0.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The first condition ensures that $V_{t}$ contains predictive information about its future that is not present in $X_{t}$ , while the second condition establishes that $V_{t}$ arises from $X_{t}$ . ", "page_idx": 17}, {"type": "text", "text": "D.4 Emergence via effective information ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Hoel et al. [24] introduce the concept of effective information, which measures the strength of dependencies using maximum entropy distributions [14]. More specifically, for a given probability kernel corresponding to $p(X_{t+1}|X_{t})$ , the corresponding effective information is $\\operatorname{Ef}(X)\\,=\\,I(X_{t};X_{t+1})$ where $X_{t}$ is assumed to follow a uniform distribution. Then, emergence is assigned when the kernel $p(V_{t+1}|V_{t})$ that results when applying the coarse-graining $V_{t}=f\\bar{(X_{t})}$ satisfies ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname{Ef}(V)>\\operatorname{Ef}(X).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In other words, there is more information going through the macroscopic than the microscopic level when considering maximum-entropy inputs to both of them. ", "page_idx": 17}, {"type": "text", "text": "D.5 Comparison with our framework ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "While these alternative formalisms provide valuable insights into emergence, they differ from the approach we use, which emphasizes the part-whole relationships by considering how the emergent feature encodes information about the microscopic components, particularly focusing on synergistic interactions. Our framework aligns with the view that emergent properties arise from collective interactions among parts that cannot be fully understood by examining components in isolation. Moreover, it is important to highlight that the metrics associated with most of these approaches are computationally expensive, and don\u2019t scale well with system size. Enabling similar methods as the one proposed here but with these other metrics of emergence, and comparing the resulting findings, would be an extremely interesting line for future work. ", "page_idx": 18}, {"type": "text", "text": "E Hyperparameters ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Hyperparameters for the models trained on the synthetic and brain datasets can be seen in the below tables. The hyperparameters do not change when learning diverse sets of emergent features. For each training run the clip was fixed at 5 in the SMILE mutual information estimators and the optimizer used was AdamW. ", "page_idx": 18}, {"type": "table", "img_path": "z6reLFqv6w/tmp/44c379b171abea87e0a1cdafa41636e265495ac4d3ac1f8a6409b9b47032235d.jpg", "table_caption": ["Table 2: Hyperparameters for causal emergence representation learning on synthetic data "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "z6reLFqv6w/tmp/91a6b4ae8760c17b2d90a7d36deb6eb214e49ffabdc6455c3b375258cb3d0561.jpg", "table_caption": ["Table 3: Hyperparameters for causal emergence representation learning on ECoG data "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "z6reLFqv6w/tmp/c97f4cba2eea2124050914fcfca9629a67548cab0a9c12bfd4a19363c86455c2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "z6reLFqv6w/tmp/ad8efa5cd08a16da682d7dc89fbb370f90c3ffed1669c5f755e13f37be04a051.jpg", "table_caption": ["Table 5: Hyperparameters for Game of Life emergent feature learning with $\\Psi$ criterion "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "F NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: No new theory, all theory is cited. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: No statistical claims are made, we simply demonstrate that certain conditions can be met using estimations of MI. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This paper does not further the capabilities of AI, and instead aims to develop methods that further understanding of complex systems. Our models have no path to harm. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The data and models have no such risks, they are small datasets of the ECoG and the models are small MLPs ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 24}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 24}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: This paper does not release new assets ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 25}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: No crowdsourcing was used, and no new data from human subjects subjects was collected. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: Same as above. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}]