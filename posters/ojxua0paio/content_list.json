[{"type": "text", "text": "Stochastic Extragradient with Flip-Flop Shuffing & Anchoring: Provable Improvements ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jiseok Chae Department of Mathematical Sciences KAIST Daejeon, Republic of Korea jsch@kaist.ac.kr ", "page_idx": 0}, {"type": "text", "text": "Chulhee Yun Kim Jaechul Graduate School of AI KAIST Seoul, Republic of Korea chulhee.yun@kaist.ac.kr ", "page_idx": 0}, {"type": "text", "text": "Donghwan Kim Department of Mathematical Sciences KAIST Daejeon, Republic of Korea donghwankim@kaist.ac.kr ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In minimax optimization, the extragradient (EG) method has been extensively studied because it outperforms the gradient descent-ascent method in convexconcave (C-C) problems. Yet, stochastic EG (SEG) has seen limited success in C-C problems, especially for unconstrained cases. Motivated by the recent progress of shufling-based stochastic methods, we investigate the convergence of shufflingbased SEG in unconstrained finite-sum minimax problems, in search of convergent shufling-based SEG. Our analysis reveals that both random reshuffling and the recently proposed flip-flop shuffling alone can suffer divergence in C-C problems. However, with an additional simple trick called anchoring, we develop the SEG with flip-flop anchoring (SEG-FFA) method which successfully converges in C-C problems. We also show upper and lower bounds in the strongly-convex-stronglyconcave setting, demonstrating that SEG-FFA has a provably faster convergence rate compared to other shuffling-based methods. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Minimax problems with a finite-sum structure, which are optimization problems of the form ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{{\\pmb x}}\\operatorname*{max}_{{\\pmb y}}f({\\pmb x},{\\pmb y}):=\\frac{1}{n}\\sum_{i=1}^{n}f_{i}({\\pmb x},{\\pmb y}),\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "can be found in many interesting applications, such as generative adversarial networks [19], refining diffusion models [28], adversarial training [37], optimal transport based generative models [48], multi-agent reinforcement learning [53], and so on. Deterministic methods for minimax problems, such as gradient descent-ascent (GDA) [3] and extragradient (EG) [29],have been extensively studied in the literature. It is though known that, unlike gradient descent (GD) for minimization problems, GDA may diverge even when $f$ is convex on $\\textbf{\\em x}$ and concave on $\\textit{\\textbf{y}}$ . On the other hand, EG employs a two-step update procedure, named extrapolation and update steps (see Section 2 for details), which allows it to find an optimum under this convex-concave setting [29, 51], and moreover, attains a convergence rate faster than GDA [4] when $f$ is strongly convex on $\\textbf{\\em x}$ and strongly concave on $\\textit{\\textbf{y}}$ ", "page_idx": 0}, {"type": "text", "text": "In contrast, attempts to construct stochastic variants of these algorithms have not been so fruitful. When $f$ is convex-concave, stochastic gradient descent-ascent (SGDA) clearly may diverge, just as in the deterministic GDA. To make matters worse, stochastic extragradient (SEG) methods have also had limited success on unconstrained convex-concave problems. As we elaborate in Section 2 in more detail, existing versions of SEG and their analyses have limitations that hinder its application to general unconstrained finite-sum convex-concave problems, requiring additional assumptions such as bounded domain, increasing batch size, convex-concavity of each component $f_{i}$ ,uniformlybounded gradient variance, and/or absence of convergence rates. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In the context of finite-sum optimization, most of the theoretical studies on stochastic methods have long been based on the with-replacement sampling scheme, where an index $i(t)$ is independently and uniformly sampled among $\\{1,\\ldots,n\\}$ at each iteration $t$ . Such a sampling scheme is relatively easy to theoretically analyze, because the sampled $f_{i(t)}$ is an unbiased estimator of the full objective function $f$ . In practice, however, inspired by the empirical observations of faster convergence in finite-sum minimization [8, 47], the without-replacement sampling schemes have been the de facto standard. Among them, the most popular is the random reshuffling (RR) scheme, where in every epoch consisting of $n$ iterations, the indices are chosen exactly once in a randomly shuffled order. ", "page_idx": 1}, {"type": "text", "text": "This gap between theory and practice in minimization problems is being closed by the recent breakthroughs in stochastic gradient descent (SGD), namely that SGD with RR leads to a provably faster convergence compared to with-replacement SGD when the number of epochs is large enough [1, 35, 39, 41, 55, 56]. This has motivated further studies on finding other shuffling-based sampling schemes that can improve upon RR, resulting in the discoveries such as the fip-fop scheme [46] and gradient balancing (GraB) [11, 32]. The flip-flop scheme is a particularly simple yet interesting modification of RR with improved rates in quadratic problems: a random permutation is used twice in a single epoch (i.e., two passes over $n$ components in an epoch), but the order is reversed in the secondpass. ", "page_idx": 1}, {"type": "text", "text": "The aforesaid progress in minimization also triggered the study of stochastic minimax methods with shuffing. Similar to minimization problems, SGDA with RR indeed converges faster than the withreplacement SGDA, under assumptions such as strongly-convex-strongly-concave objectives [15] or $f$ satisfying the Polyak-Lojasiewicz condition [13]. Despite the superiority of EG over GDA, the SEG with shuffing has not been shown to have a solid theoretical advantage over the SGDA with shuffling yet. This motivated us to study the following question: ", "page_idx": 1}, {"type": "text", "text": "Can shuffling schemes provide convergence guarantees for SEG, improved upon SGDA with shuffling, in unconstrained finite-sum (strongly-)convex-(strongly-)concave settings? ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "There are two types of SEG: same-sample SEG, where a sample chosen is used both for the extrapolation step and the update step, and independent-sample SEG, where two independently chosen samples are used in each step. We will particularly focus on the same-sample SEG because it combines more naturally with shuffling-based schemes than independent-sample SEG. Therefore, to be more specific, we are interested in developing shuffling-based variants of same-sample SEG in unconstrained finite-sum minimax problems with minimal modifications to the algorithm. We show that (a) in convex-concave settings, our new method reaches an optimum with a guarantee on the rate of convergence, overcoming the limitations of existing results; (b) in strongly-convex-strongly-concave settings, the method converges faster than other SGDA/SEG variants. ", "page_idx": 1}, {"type": "text", "text": "1.1 Our Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this paper, we study various same-sample SEG algorithms under different shuffling schemes, and propose the stochastic extragradient with fip-fop anchoring (SEG-FFA) method, which is SEG amended with the techniques of fip-fop shuffling scheme and anchoring. Here, by anchoring we refer to a step of taking a convex combination between the initial and final iterates of an epoch, resembling the celebrated Krasnosel'skit-Mann iteration [30, 33] as we discuss in Section 5. With such minimal modifications to SEG, we show that SEG-FFA achieves provably improved convergence guarantees. More precisely, our contributions can be listed as follows (see Table 1 for a summary). For clarity, we use SEG-US to refer to with-replacement SEG, where US stands for uniform sampling. ", "page_idx": 1}, {"type": "text", "text": "\u00b7 We first study the same-sample versions of SEG-US, SEG with RR (SEG-RR), and SEG with flip-flop (SEG-FF). We show that they all can diverge when $f$ is convex-concave, by constructing an explicit counterexample (Theorem 4.1). This shows that shuffling alone cannot fix the divergence issue of SEG-US. ", "page_idx": 1}, {"type": "table", "img_path": "OJxua0PAIo/tmp/4db6904c67ce48fe1d91623153c2c4e8f1423915dd47b5ffd474c16046707270.jpg", "table_caption": ["Table 1: Summary of upper/lower convergence rate bounds of same-sample SEG for unconstrained finite-sum minimax problems, without requiring increasing batch size, convex-concavity of each component, and uniformly bounded gradient variance. Pseudocode of algorithms can be found in Appendix A. We only display terms that become dominant for sufficiently large $T$ and $K$ . To compare the with-replacement versions (-US) against shuffling-based versions, one can substitute $T=n K$ . The optimality measure used for SC-SC problems is $\\mathbb{E}[\\|\\hat{z}-z^{*}\\|^{2}]$ for the last iterate $\\hat{z}$ .For C-C problems, we consider $\\begin{array}{r}{\\operatorname*{min}_{t=0,\\ldots,T}\\mathbb{E}[\\left|\\left|F z_{t}\\right|\\right|^{2}]}\\end{array}$ for with-replacement methods and $\\begin{array}{r}{\\operatorname*{min}_{k=0,\\ldots,K}\\mathbb{E}[\\left\\|F z_{0}^{k}\\right\\|^{2}]}\\end{array}$ for shuffling-based methods. "], "table_footnote": ["[17, 20] show upperbouds for SEG-US but they rquire reasing batch sizes as wellas other assumtions see AppendixB.1). \\*[25] shows that independent-sample SEG-US converges for stepsizes $\\alpha_{t}$ $\\beta_{t}$ decaying at different rates, but gives no cony. rate. Unfortunately, the proof of this convergence bound in this recent AISTATS 2024 paper seems tobe incorrect: see Appendix B4. "], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "\u00b7 We next investigate the underlying cause for the nonconvergence of SEG-US, SEG-RR, and SEG-FF. In particular, we identify that either they fail to match the update equation of the reference method EG beyond first-order Taylor expansion terms, or attempting to match both the first- and second-order Taylor expansion terms results in divergence (Proposition 5.2). \u00b7 By adopting a simple technique of anchoring on top of fip-flop shuffing, we devise our algorithm SEG-FFA, whose epoch-wise update deterministically matches EG up to secondorder Taylor expansion terms (Proposition 5.3). We prove that SEG-FFA enjoys improved convergence guarantees, as anticipated by our design principle. Most importantly, we show that SEG-FFA achieves a convergence rate of $\\tilde{\\mathcal{O}}\\big(1\\bar{/}{K^{1/3}}\\big)$ when $f$ is convex-concave, where $K$ denotes the number of epochs. This is in stark contrast to other baseline algorithms that diverge under this setting (see the last column of Table 1). \u00b7 Moreover, we show that when $f$ is strongly-convex-strongly-concave, SEG-FFA achieves a convergence rate of $\\tilde{\\mathcal{O}}({^1}/{n K^{4}})$ (Theorem 5.5). Additionally, by proving $\\Omega(^{1}/n K^{3})$ lower bounds for the convergence rates of SGDA-RR and SEG-RR under the same setting (Theorem 5.6), we show that SEG-FFA has a provable advantage over these baseline algorithms. ", "page_idx": 2}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Extragradient and $\\mathbf{EG+}$ Extragradient (EG) method [29] is a widely used minimax optimization method, well-known for resolving the nonconvergence issue of GDA on convex-concave problems. In this paper, we also consider $\\mathrm{EG+}$ [17], which is a generalization of EG. The update rule of $\\mathrm{EG+}$ is defined, for stepsizes $\\{\\eta_{1,k}\\}_{k\\ge0}$ and $\\{\\eta_{2,k}\\}_{k\\ge0}$ ,as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\{\\vphantom{\\frac{b^{k}}{b^{k}}}u^{k}\\leftarrow x^{k}-\\eta_{1,k}\\nabla_{x}\\,f(x^{k},y^{k})\\right.}&{{}\\qquad\\left.\\left\\{x^{k+1}\\leftarrow x^{k}-\\eta_{2,k}\\nabla_{x}\\,f(u^{k},v^{k})\\right.\\right.}\\\\ {\\left.v^{k}\\leftarrow y^{k}+\\eta_{1,k}\\nabla_{y}\\,f(x^{k},y^{k})\\right.}&{{}\\qquad\\left.\\left\\{y^{k+1}\\leftarrow y^{k}+\\eta_{2,k}\\nabla_{y}\\,f(u^{k},v^{k})\\right.\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The first step is called the extrapolation step, and the second step is called the update step. If $f$ is convex-concave, Diakonikolas et al. [17] show that $\\mathrm{EG+}$ reaches an optimum when $\\eta_{1,k}\\ge\\eta_{2,k}$ . In particular, when $\\eta_{1,k}=\\eta_{2,k}$ , we recover the standard EG by Korpelevich [29]. ", "page_idx": 2}, {"type": "text", "text": "Stochastic Variants of Extragradient In (2), if the stochastic estimators of $\\nabla_{\\pmb{x}}\\,f$ and $\\nabla_{y}\\,f$ are used instead of the gradients themselves, we get the standard SEG. If an estimator chosen is used for both the extrapolation and the update steps, we get the same-sample SEG, which we focus on in this paper; see Appendix A for the pseudocode. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "While EG improves upon GDA, unfortunately, SEG has not been able to show a clear advantage over SGDA. On one hand, analyses of SEG on strongly-convex-strongly-concave problems have shown some success; see, e.g., [18, 20]. Yet, on the other hand, for general unconstrained convex-concave problems, to the best of our knowledge, the existing stochastic variants of EG and their analyses face several limitations.23 Assumptions commonly imposed in the existing literature include: (i) the domain is bounded, either explicitly or implicitly [27, 36], (ii) one must increase the batch size to achieve convergence [9, 17, 20], and (ii) each component $f_{i}$ is convex-concave [20, 36], and (iv) the components have uniformly bounded gradient variance [9, 17, 42]. For further details, see Appendix B.1 and Table 2 therein. Notably, Hsieh et al. [25] prove convergence of the independentsample SEG without these four restrictions, but the result lacks an explicit convergence rate. ", "page_idx": 3}, {"type": "text", "text": "Our proposed SEG-FFA overcomes all the aforementioned limitations, and reaches an optimum with an explicit rate in unconstrained convex-concave problems, under relatively mild conditions. The readers may also refer to [7] for a comprehensive overview on this topic. ", "page_idx": 3}, {"type": "text", "text": "Meanwhile, under the finite-sum setting, variance reduction schemes have also been considered, achieving some promising results [2, 10]. Yet, although theoretically appealing, variance reduction is less widely used in practice due to their curiously inferior performance in training neural networks [16]. On top of this practical issue, variance reduction techniques share the aforementioned limitation (i), as accessing full gradients can be viewed as increasing the batch size. In contrast, our main goal in this paper is to study how a carefully chosen sampling scheme, with minimal modifications to the algorithm, can improve the convergence of SEG without the need for increased batch size; therefore, we believe that our work is not directly comparable to variance reduction-based EG. ", "page_idx": 3}, {"type": "text", "text": "Taylor Expansion Matching and Convergence Guarantees  It has been repeatedly reported that the convergence of an optimization method is deeply related to the degree to which the Taylor expansion (with respect to the step size) of its update equation matches with that of an already known convergent method. For example, Mokhtari et al. [38] observed that the advantage of EG over GDA comes from the Taylor expansion of update equations of EG matching that of the proximal point (PP) method [34] up to second-order terms, whereas GDA matches PP only up to first-order terms. ", "page_idx": 3}, {"type": "text", "text": "The advantages of the shuffling scheme over the with-replacement sampling can be explained in a similar way. One key property of shuffling-based methods is that, while the individual estimators are biased as they are dependent to other estimators within the same epoch, the overall stochastic error across the epoch decreases dramatically compared to using $n$ independent unbiased estimators. For instance, in SGD with RR [1] and in SGDA with RR [15], the overall progress made within each epoch exactly matches their deterministic counterparts up to the first-order, leaving an error as small as ${\\mathcal{O}}(\\eta^{2})$ ,where $\\eta$ is the stepsize. Rajput et al. [46] observed that, when each component functions are convex quadratics, then using flip-flop on SGD can reduce the error further to $\\bar{\\mathcal{O}}(\\eta^{3})$ ,resulting in an even faster convergence. As we further elaborate in Section 5, the motivation behind our design principle of SEG-FFA is also based on this line of observations. ", "page_idx": 3}, {"type": "text", "text": "3   Notations and Problem Settings ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Let $[n]\\subset\\mathbb{Z}$ denote the set $\\{1,\\ldots,n\\}$ . The set of all permutations on $[n]$ will be denoted by ${\\mathcal{S}}_{n}$ .For the finite-sum minimax problem (1), we denote the saddle gradient operators by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{F(\\,\\cdot\\,):=\\left[\\frac{\\nabla_{x}\\,f(\\,\\cdot\\,)}{-\\nabla_{y}\\,f(\\,\\cdot\\,)}\\right],\\;\\;F_{i}(\\,\\cdot\\,):=\\left[\\frac{\\nabla_{x}\\,f_{i}(\\,\\cdot\\,)}{-\\nabla_{y}\\,f_{i}(\\,\\cdot\\,)}\\right],\\;\\;\\;i\\in[n].}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The derivative of an operator will be denoted with a prefix $D$ . For example, the derivative of $\\pmb{F}$ is denotedby $_{D F}$ . Often a single vector will be used to denote the minimization and the maximization ", "page_idx": 3}, {"type": "text", "text": "variable at once. For instance, for $z\\in\\mathbb{R}^{d_{1}+d_{2}}$ which is a concatenation of $\\pmb{x}\\in\\mathbb{R}^{d_{1}}$ and $\\pmb{y}\\in\\mathbb{R}^{d_{2}}$ \uff0c we simply write $F z$ to denote $\\scriptstyle{F(x,y)}$ ", "page_idx": 4}, {"type": "text", "text": "It is well known that, if $f$ is $\\mu$ -strongly convex on $\\textbf{\\em x}$ and $\\mu$ -strongly concave on $\\textit{\\textbf{y}}$ for some $\\mu>0$ (respectively, $\\mu=0$ ), then its saddle gradient $\\pmb{F}$ is $\\mu$ -strongly monotone (respectively, monotone), in the following sense. For a proof of this standard fact, see, e.g., [22]. ", "page_idx": 4}, {"type": "text", "text": "Assumption 3.1 (Monotonicity & Strong Monotonicity). For $\\mu>0$ ,wesaythat anoperator $\\pmb{F}$ is $\\mu$ stronglymonotone if,forany $z,w\\in\\bar{\\mathbb{R}}^{d_{1}+d_{2}}$ ,itholds that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\langle F z-F w,z-w\\rangle\\geq\\mu\\left\\|z-w\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "If (3) holds for $\\mu=0$ , then we say that $\\pmb{F}$ is monotone. ", "page_idx": 4}, {"type": "text", "text": "Thus, from now on, we will use the term strongly monotone (respectively, monotone) problems rather than strongly-convex-strongly-concave (respectively, convex-concave) problems. Notice that we only assume that the full saddle gradient $\\pmb{F}$ is (strongly) monotone, not individual $F_{i}$ 's. ", "page_idx": 4}, {"type": "text", "text": "In addition, we remark that our convergence analysis under the monotonicity of $\\pmb{F}$ (Theorem 5.4) in fact requires only a relaxed version of monotonicity, known as star-monotonicity. This condition imposes the inequality (3) with $\\mu\\:=\\:0$ , but only when $w\\;=\\;z^{*}$ ,where $z^{*}$ is a point such that $\\pmb{F}z^{*}=\\mathbf{0}$ . This relaxation allows for a certain degree of nonconvex-nonconcavity in $f$ . For a more detailed discussion on the star-monotonicity condition, see Appendix G.1. ", "page_idx": 4}, {"type": "text", "text": "Other three underlying assumptions we make on the problem (1) can be listed as follows. ", "page_idx": 4}, {"type": "text", "text": "Assumption 3.2 (Existence of an Optimal Solution). An optimal solution of the problem (1), which is a point we denote by $\\boldsymbol{z}^{*}=(\\boldsymbol{x}^{*},\\boldsymbol{y}^{*})$ that satisfies ", "page_idx": 4}, {"type": "equation", "text": "$$\nf({\\boldsymbol x}^{*},{\\boldsymbol y})\\leq f({\\boldsymbol x}^{*},{\\boldsymbol y}^{*})\\leq f({\\boldsymbol x},{\\boldsymbol y}^{*})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "for any $\\pmb{x}\\in\\mathbb{R}^{d_{1}}$ and $\\pmb{y}\\in\\mathbb{R}^{d_{2}}$ ,exists in $\\mathbb{R}^{d_{1}+d_{2}}$ ", "page_idx": 4}, {"type": "text", "text": "Because the problem is unconstrained and $f$ is convex-concave, a point $z^{*}$ is an optimum if and Only if $\\pmb{F}z^{*}\\,=\\,\\mathbf{0}$ . For strongly monotone problems, Assumption 3.2 is not explicitly required, as it is guaranteed a priori [5, Proposition 22.11]. For monotone problems, we explicitly impose Assumption 3.2 in order to exclude pathological problems such as $\\bar{f}(x,y)=x-y$ ", "page_idx": 4}, {"type": "text", "text": "Assumption 3.3 (Smoothness). Each $f_{i}$ is $L$ -smooth, and each $F_{i}$ is $M$ -smooth. That is, for any z, W E IRdi+d2, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|F_{i}z-F_{i}\\pmb{w}\\|\\leq L\\,\\|z-\\pmb{w}\\|,}\\\\ &{\\|D F_{i}z-D F_{i}\\pmb{w}\\|\\leq M\\,\\|z-\\pmb{w}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "It is worth mentioning that the gradient operator $F_{i}$ arising from a quadratic function $f_{i}$ is $M$ smooth with $M=0$ Notice also that,by the finite-sum structure $\\begin{array}{r}{\\pmb{F}=\\frac{1}{n}\\sum_{i=1}^{n}{\\cal F}_{i}}\\end{array}$ , itis clear that Assumption 3.3 implies $f$ being $L$ -smooth and $\\pmb{F}$ being $M$ -smooth. ", "page_idx": 4}, {"type": "text", "text": "The $L$ -smoothness assumption on the objective functions is standard in the optimization literature, while the $M$ -smoothness assumption on the saddle gradients may look less standard. This smoothness assumption on the saddle gradient, in other words the Lipschitz Hessian condition, for analyzing SEGFFA stems from the analysis of the flip-flop sampling scheme [46]. In particular, this is needed for bounding the high-order error terms between the (deterministic) EG and SEG-FFA in Section 5.1. The existing analysis of flip-flop sampling [46] is limited to quadratic functions that trivially have O-LipschitzHessians ( $M=0$ ), so our analysis is a step forward. ", "page_idx": 4}, {"type": "text", "text": "Assumption 3.4 (Component Variance). There exist constants $\\rho\\geq0$ and $\\sigma\\geq0$ such that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac1n\\sum_{i=1}^{n}\\left\\|F_{i}z-F z\\right\\|^{2}\\le(\\rho\\left\\|F z\\right\\|+\\sigma)^{2}\\qquad\\forall z.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For strongly monotone problems, Assumption 3.4 is not explicitly required, because it can be obtained as a consequence of the preceding assumptions: see Lemma C.9. Nevertheless, for convenience, we will keepthenotations $\\rho$ and $\\sigma$ as in (4) for the strongly monotone setting as well. ", "page_idx": 4}, {"type": "text", "text": "In many existing works studying stochastic optimization methods for minimax problems, Assumption 3.4 with $\\rho=0$ is imposed. This uniform bound on the variance simplifies the convergence analyses, but it is also fairly restrictive especially in the unconstrained settings. Already for bilinear finite-sum minimax problems $\\begin{array}{r}{f(\\mathbf{\\boldsymbol{x}},\\mathbf{\\boldsymbol{y}})=\\frac{1}{n}\\sum_{i=1}^{n}x^{\\top}B_{i}\\mathbf{\\boldsymbol{y}}}\\end{array}$ one can eaily check that eting $\\rho=0$ forces the matrices $B_{i}$ to be exactly equal to each other. For machine learning applications, it has been also reported that the assumption with $\\rho=0$ often fails to hold [7]. Therefore, allowing the variance to grow with the gradient $F z$ makes the assumption much more realistic. ", "page_idx": 5}, {"type": "text", "text": "The Lipschitz Hessian condition and the component variance assumption for monotone problems may still look rather strong. We leave the study on how one can relax such assumptions to prove upper bounds for convergence rates as an interesting future direction. On the other hand, while our lower bound results in Theorems 4.1 and 5.6 are derived under those strong assumptions, they still serve as lower bound results also for larger function classes that do not have those assumptions. In other words, the value of those results are not limited because of those assumptions being imposed. ", "page_idx": 5}, {"type": "text", "text": "4  Shuffling Alone Is Not Enough ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Under the settings we have discussed, we study the SEG with shuffling-based sampling schemes.   \nFirst we describe the precise methods of our consideration, namely the SEG-RR and SEG-FF. ", "page_idx": 5}, {"type": "text", "text": "For $k\\,\\geq\\,0$ , in the beginning of an epoch, a random permutation $\\tau_{k}$ is sampled from a uniform distribution over ${\\mathcal{S}}_{n}$ . Then, for $n$ iterations, we use each of the component functions once, in the order determined by $\\tau_{k}$ . That is, for $i=0,1,\\ldots,n-1$ we do ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\pmb w}_{i}^{k}\\leftarrow z_{i}^{k}-\\alpha_{k}{\\pmb F}_{\\tau_{k}(i+1)}z_{i}^{k},}\\\\ {z_{i+1}^{k}\\leftarrow z_{i}^{k}-\\beta_{k}{\\pmb F}_{\\tau_{k}(i+1)}{\\pmb w}_{i}^{k},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for some stepsizes $\\alpha_{k}$ and $\\beta_{k}$ . In case of SEG-RR, the epoch is completed here, and we set $z_{0}^{k+1}\\leftarrow z_{n}^{k}$ as the initial point for the next epoch. ", "page_idx": 5}, {"type": "text", "text": "In case of SEG-FF, we additionally perform $n$ more iterations in the epoch, as proposed in Rajput et al. [46]. In these additional iterations, the component functions are each used once more, but in the reverse order. That is,for $i=n,n+1,\\dots,2n-1$ ,wedo ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\pmb w}_{i}^{k}\\leftarrow z_{i}^{k}-\\alpha_{k}{\\pmb F}_{\\tau_{k}(2n-i)}z_{i}^{k},}\\\\ {z_{i+1}^{k}\\leftarrow z_{i}^{k}-\\beta_{k}{\\pmb F}_{\\tau_{k}(2n-i)}{\\pmb w}_{i}^{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Then we set $z_{0}^{k+1}\\leftarrow z_{2n}^{k}$ as the initial point for the next epoch. The full pseudocode of these methods can be found in Appendix A. ", "page_idx": 5}, {"type": "text", "text": "When $\\pmb{F}$ is strongly monotone, it is possible to show that both SEG-RR and SEG-FF indeed provide speed-up over SEG-US. The well-known rate of SEG-US under strong monotonicity of $\\pmb{F}$ .s $\\Theta(1/T)$ \uff0c where $T$ is the total number of iterations [6, 20]. Translating this rate to our shufling-based setting, where there are $\\Theta(n)$ iterations per epoch, this rate amounts to $\\Theta(^{1}\\!/n K)$ . Recently, Emmanouilidis et al. [18] have shown that SEG-RR, under the same setting as ours, attains a convergence rate of $\\tilde{\\mathcal{O}}(^{1}/n K^{2})$ , on par with the rate of SGDA-RR [15]. In Appendix F, we also show that SEG-FF attains a similar rate of convergence. ", "page_idx": 5}, {"type": "text", "text": "However, it turns out that the benefit of shuffling does not extend further beyond the strongly monotone setting. In fact, when $\\pmb{F}$ is merely monotone, then in the worst case, SEG-RR and SEG-FF suffers from nonconvergence, just as in the case of SEG-US. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.1. For $n=2$ therexsa miniaxproblen wih $\\begin{array}{r}{f(x,y)=\\frac{1}{2}\\sum_{i=1}^{2}f_{i}(x,y)}\\end{array}$ having a monotone consisting of -smooth quadratic $f_{i}$ 's satisfyingAssumption 3.4with $(\\rho,\\sigma)=(1,0)$ \uff0c such that SEG-US, SEG-RR and SEG-FF diverge in expectation for any positive stepsizes. ", "page_idx": 5}, {"type": "text", "text": "We provide the explicit counterexample and the proof of divergence in Appendix H.1. Note that Theorem 4.1 and its proof in Appendix H.1 imply that $\\begin{array}{r}{\\operatorname*{min}_{t=0,\\ldots,T}\\mathbb{E}[\\left\\|F z_{t}\\right\\|^{2}]=\\Omega(1)}\\end{array}$ forSEG-US and $\\begin{array}{r}{\\operatorname*{min}_{k=0,\\ldots,K}\\mathbb{E}[\\left|\\left|F z_{0}^{k}\\right|\\right|^{2}]=\\Omega(1)}\\end{array}$ for SEG-RR and SEG-FF, as summarized in Table 1. ", "page_idx": 5}, {"type": "text", "text": "5  SEG-FFA: SEG with Flip-Flop Anchoring ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we investigate the underlying cause for nonconvergence of SEG-RR and SEG-FF from the perspective of how accurately they match the convergent EG or PP methods in terms of the Taylor expansions of updates. We then propose adding a simple anchoring step at the end of each epoch of SEG-FF. It turns out that adding the anchoring step, which is a step of taking a convex combination of an iterate with a previously computed iterate, reduces the stochastic noise and leads to a method with improved convergence properties. ", "page_idx": 6}, {"type": "text", "text": "5.1  Design Principle: Second-Order Matching ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "As observed by [38], the key feature of EG behind its superior convergence properties compared to GDA is its update rule closely resembling PP, while the \u201cerror\"? of GDA as an approximation of PP is so large that it hinders convergence. The difference between the updates of EG and PP, in the Taylor expansion, is as small as $\\mathcal{O}(\\bar{\\eta^{3}})$ per iteration, where $\\eta$ is the stepsize. On the other hand, GDA and PP show a difference of ${\\mathcal{O}}(\\eta^{2})$ , and this greater \u201cerror\u2019 explains why GDA diverges while EG and PP converge. Of course, EG and PP are not the only two algorithms that converge in the monotone setting; let us recall the update rule of $\\mathrm{EG+}$ method [17], and Taylor-expand it as the following: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{z^{+}:=z-\\eta_{2}{\\cal F}(z-\\eta_{1}{\\cal F}z)}\\\\ &{\\qquad=z-\\eta_{2}{\\cal F}z+\\eta_{1}\\eta_{2}D{\\cal F}(z){\\cal F}z+O(\\eta_{1}^{2}\\eta_{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "$\\mathrm{EG+}$ is known to converge for unconstrained monotone problems if $\\eta_{1}\\geq\\eta_{2}$ .When $\\eta_{1}=\\eta_{2}$ ,it recovers EG and matches PP up to second-order terms. ", "page_idx": 6}, {"type": "text", "text": "Based on these observations, we now state our key principle for designing a convergent version of SEG: second-order matching. We would like to choose proper stepsizes, sampling scheme, and anchoring scheme so that our without-replacement SEG can deterministically match the update equation of a convergent algorithm (EG/PP or $\\mathrm{EG+}$ )uptothe ${\\cal O}(\\eta^{2})$ terms (i.e., second-order terms in the Taylor expansion), thereby satisfying a small $\\bar{\\mathcal{O}}(\\bar{\\eta}^{3})$ approximation error. We show that (a) this second-order matching can be achieved with fip-flop anchoring, but not solely by permutation-based sampling such as RR and flip-flop (without anchoring), and ${\\bf(b)}$ second-order matching indeed grants convergence for monotone problems. In particular, we demonstrate that ", "page_idx": 6}, {"type": "text", "text": ". SEG-RR suffers a poor approximation error of ${\\mathcal{O}}(\\eta^{2})$ as an approximation of $\\mathrm{EG/EG+}$ ", "page_idx": 6}, {"type": "text", "text": "2. SEG-FF can match $\\mathrm{EG+}$ up to second-order terms, but it results in a choice of stepsizes $\\eta_{2}=2\\eta_{1}$ thatmake $\\mathrm{EG+}$ diverge (Proposition 5.2). 3. SEG-FFA, the method we propose, matches EG up to second-order terms to get an error of $\\mathcal{O}(\\eta^{3})$ (Proposition 5.3), achieving convergence in monotone problems (Theorem 5.4). ", "page_idx": 6}, {"type": "text", "text": "To this end, let us consider a general form of SEG that incorporates any arbitrary sampling scheme. More precisely, in the $k$ -th \u201cepoch\"consisted of $N$ iterations, the components are chosen in the order $T_{0}^{k^{\\ast}},T_{1}^{k},\\cdot\\cdot\\cdot,T_{N-1}^{k}$ $\\pmb{T}_{i}^{k}\\in\\{\\pmb{F}_{1},\\dots,\\pmb{F}_{n}\\}$ $i$ Forourpupws that $N$ some multiple of $n$ (e.g., $N=n$ for SEG-RR, $N=2n$ for SEG-FF). Then, given $\\alpha$ $\\beta$ we perform SEG updates, for $i=0,1,\\ldots,N-1$ \uff0c ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{w}_{i}^{k}\\leftarrow z_{i}^{k}-\\alpha\\pmb{T}_{i}^{k}z_{i}^{k},}\\\\ {z_{i+1}^{k}\\leftarrow z_{i}^{k}-\\beta\\pmb{T}_{i}^{k}\\pmb{w}_{i}^{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "5.1.1  Necessity of Flip-Flop Sampling ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The general methd in (8 that sets the initial point for the ext epoch as 2b+1 \u2190 z satisfies the following property. ", "page_idx": 6}, {"type": "text", "text": "Proposition 5.1. Suppose that Assumption 3.3 holds. For some $\\epsilon_{N}^{k}=o\\left((\\alpha+\\beta)^{2}\\right)$ , it holds that ", "page_idx": 6}, {"type": "equation", "text": "$$\nz_{0}^{k+1}=z_{0}^{k}-\\beta\\sum_{j=0}^{N-1}T_{j}^{k}z_{0}^{k}+\\alpha\\beta\\sum_{j=0}^{N-1}D T_{j}^{k}(z_{0}^{k})T_{j}^{k}z_{0}^{k}+\\beta^{2}\\sum_{i<j}D T_{j}^{k}(z_{0}^{k})T_{i}^{k}z_{0}^{k}+\\epsilon_{N}^{k}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "See Appendix D.1 for the proof. To make (7) and (9) match up to the second-order, both the equations ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\eta_{2}}{n}\\sum_{j=1}^{n}F_{i}z_{0}^{k}=\\beta\\sum_{j=0}^{N-1}T_{j}^{k}z_{0}^{k}\\qquad\\mathrm{~and~}}\\\\ &{\\displaystyle\\frac{\\eta_{1}\\eta_{2}}{n^{2}}\\Big(\\sum_{j=1}^{n}D F_{j}(z_{0}^{k})F_{j}z_{0}^{k}+\\sum_{i\\neq j}D F_{j}(z_{0}^{k})F_{i}z_{0}^{k}\\Big)=\\alpha\\beta\\sum_{j=0}^{N-1}D T_{j}^{k}(z_{0}^{k})T_{j}^{k}z_{0}^{k}+\\beta^{2}\\sum_{i<j}D T_{j}^{k}(z_{0}^{k})T_{i}^{k}z_{0}^{k}}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "must hold. Clearly, without-replacement sampling will make (1o) hold. However, it is easy to check that random reshuffling falls short of making (11) hold. This is because, if RR is used, then $T_{0}^{k},T_{1}^{k},\\dots,T_{n-1}^{k}$ is nothing but a reordering of $F_{1},\\dots,F_{n}$ into $F_{\\tau(1)},\\dots,F_{\\tau(n)}$ , so the RHS of (11) can only contain terms $D F_{\\tau(j)}\\big(z_{0}^{k}\\big)F_{\\tau(i)}z_{0}^{k}$ with $i\\leq j$ This observation motivates the use of fip-fop sampling, because choosing $T_{i}^{k}=T_{2n-1-i}^{k}$ lets allthe required terms $D F_{j}(z_{0}^{k})F_{i}z_{0}^{k}$ to appear in the RHS of (11). ", "page_idx": 7}, {"type": "text", "text": "5.1.2 Designing SEG-FFA ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Flip-flop does resolve the aforesaid isue, but stillanother complication remains for plain SEG-FF. ", "page_idx": 7}, {"type": "text", "text": "Proposition 5.2. Suppose we use fip-fop sampling (without anchoring). In order to make (10) and (11) hold, we must choose $\\beta=\\eta_{1}/n$ and $\\alpha=\\beta/2$ .However, this leads to $\\eta_{2}=2\\eta_{1}$ which is the set of parameters that fails to make $E G+$ converge. ", "page_idx": 7}, {"type": "text", "text": "For the proof, see Appendix D.2. This shows that a modification is necessary to develop a stochastic method that achieves second-order matching to convergent $\\mathrm{EG/EG+}$ methods. ", "page_idx": 7}, {"type": "text", "text": "We thus propose to add an anchoring step: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{z_{0}^{k+1}\\leftarrow\\frac{1}{2}\\left(z_{N}^{k}+z_{0}^{k}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "after fnishing the $N$ updates (8), instead of $z_{0}^{k+1}\\leftarrow z_{N}^{k}$ .This is our Stochastic ExtraGradient with Flip-Flop Anchoring (SEG-FFA) method, named after the design of combining the flip-flop sampling scheme and the anchoring step. We note that this idea of taking a convex combination has originally appeared in the Krasnosel'skii-Mann iteration [30, 33], and also under the name of Lookahead methods [12, 43]. This slightly differs from the more widely used Halpern iteration [23] based anchoring (cf. [54]), which would have used the initial point $z_{0}^{0}$ instead of $\\dot{z}_{0}^{k}$ in (12). ", "page_idx": 7}, {"type": "text", "text": "This anchoring step changes (9) accordingly, and essentially amounts to dividing the right-hand sides of (10) and (11) each by 2 (see Appendix $\\mathrm{D}$ for the detailed derivations). We show that choosing $\\alpha={}^{\\beta}/2$ in fact leads to the second-order matching to EG, i.e., $\\mathrm{EG+}$ With $\\eta_{1}=\\eta_{2}$ ", "page_idx": 7}, {"type": "text", "text": "Proposition 5.3. Suppose that Assumptions 3.3 and 3.4 hold. Then, for $\\beta_{k}\\,=\\,\\eta$ and $\\alpha_{k}\\,=\\,\\beta_{k}\\big/2$ SEG-FFA becomes an approximation of $E G$ with error at most $\\mathcal{O}(\\eta^{3})$ In other words, we achieve ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\left\\|z_{0}^{k}-\\eta n F(z_{0}^{k}-\\eta n F z_{0}^{k})-z_{0}^{k+1}\\right\\|=\\mathcal{O}(\\eta^{3}).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "In other words, adding the anchoring step allows us to get a method that well approximates the convergent EG with an error as small as $\\bar{\\mathcal{O}}(\\eta^{3})$ . For a more in-depth discussion, see Appendix E. ", "page_idx": 7}, {"type": "text", "text": "5.2 Convergence Analysis of SEG-FFA ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "As a result of the second-order matching, we obtain SEG-FFA, a stochastic method that has an error Of $\\mathcal{O}(\\eta^{3})$ as an approximation of EG. Achieving this order of magnitude for the approximation error turns out to be the key to the exact convergence to an optimum under the monotone setting. ", "page_idx": 7}, {"type": "text", "text": "Theorem 5.4. Suppose that $\\pmb{F}$ is (star-)monotone, Assumptions 3.2, 3.3, and 3.4 hold, and we are running SEG-FFA.Then,for any $K\\geq1$ ,by choosing the stepsizes sufficiently small and decaying as $\\beta_{k}=\\mathcal{O}\\big(1/k^{1/3}\\log k\\big)$ and $\\alpha_{k}=\\beta_{k}/2$ the iterates generated by SEG-FFA achieves the bound ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{k=0,1,\\ldots,K}\\mathbb{E}\\left\\|F z_{0}^{k}\\right\\|^{2}=\\mathcal{O}\\left(\\frac{(\\log K)^{2}}{K^{1/3}}\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "For the full statement of the theorem and its proof, see Appendix G. We note that, although Theorem 5.4, and also Theorem 5.5 below, are stated specifically for SEG-FFA, our analyses show that both theorems can be applied to any method that achieves the second-order matching in terms of Proposition 5.3. ", "page_idx": 8}, {"type": "text", "text": "The reduced error also shows a gain in the rate of convergence under the strongly monotone setting.   \nThis aligns with the intuition that error hinders convergence, hence having a smaller error is beneficial. ", "page_idx": 8}, {"type": "text", "text": "Theorem 5.5. Suppose that $\\pmb{F}$ is $\\mu$ -stronglymonotonewith $\\mu>0$ and Assumption 3.3 holds. Then, there exists a choice of $\\eta>0$ such that, when SEG-FFA is run for $K$ epochs with constant stepsizes $\\beta_{k}=\\eta$ and $\\alpha_{k}=\\eta/2$ for some constant $\\omega$ independent of $\\eta,$ the iterates generated by SEG-FFA achieves the bound ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left\\|z_{0}^{K}-z^{*}\\right\\|^{2}\\leq\\exp\\left(-\\frac{1}{2}\\mu\\omega n K\\right)\\left\\|z_{0}^{0}-z^{*}\\right\\|^{2}+\\mathcal{O}\\left(\\frac{\\left(\\log(n^{1/4}K)\\right)^{4}}{n K^{4}}\\right).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Theorem 5.5 actually stems from a unified analysis that encompasses all the shuffling-based SEG methods introduced in this paper, including SEG-RR and SEG-FF. See Appendix F for the details. ", "page_idx": 8}, {"type": "text", "text": "Notice the exponent 4 of the number of epochs $K$ in the convergence rate, which is twice as large as the exponent 2 of SGDA-RR and SEG-RR. In fact, this gain in the rate of convergence turns out to be fundamental. As we show in the following theorem, the theoretical lower bounds of convergence for SGDA-RR and SEG-RR with constant stepsize are both $\\Omega\\big(1\\big/n K^{3}\\big)$ . This exhibits that there is a provable gap between those methods and SEG-FFA, which attains $\\tilde{\\mathcal{O}}(^{1}/n K^{4})$ ", "page_idx": 8}, {"type": "text", "text": "Theorem 5.6. Suppose $n\\,\\geq\\,2$ \uff1aFor both SGDA-RR with constant stepsize $\\alpha_{k}\\,=\\,\\alpha\\,>\\,0$ and SEG-RR with constant stepsize $\\alpha_{k}\\,=\\,\\alpha\\,>\\,0_{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\,\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\,\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\,\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!$ $\\beta_{k}\\,=\\,\\beta\\,>\\,0$ there exists a $\\mu$ -strongly monotone minimax problem $\\begin{array}{r}{f(z)=\\frac{1}{n}\\sum_{i=1}^{n}f_{i}(z)}\\end{array}$ with $\\mu>0$ such that regardles of stepsizes,we have ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert z_{0}^{K}-z^{*}\\right\\Vert^{2}\\right]=\\left\\{\\Omega\\left(\\frac{\\sigma^{2}}{L\\mu n K}\\right)\\quad\\,i f\\,K\\leq L/\\mu,\\right.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Proof. The full statement and the proof are presented in Appendix H.3. ", "page_idx": 8}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We consider randomly generated quadratic problems of the form ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\boldsymbol{x}\\in\\mathbb{R}^{d_{x}}}\\operatorname*{max}_{\\boldsymbol{y}\\in\\mathbb{R}^{d_{y}}}\\;\\frac{1}{n}\\sum_{i=1}^{n}\\;\\Big[\\boldsymbol{y}\\Big]^{\\top}\\Big[\\mathbf{A}_{i}\\quad\\mathbf{B}_{i}\\Big]\\Big[\\boldsymbol{x}\\Big]-t_{i}^{\\top}\\Big[\\boldsymbol{x}\\Big].\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "In particular, we sample the random components so that the full objective is either monotone or strongly monotone, respectively, while each of the components may be nonmonotone. For the exact descriptions on how we constructed the problems, see Appendix I.1. ", "page_idx": 8}, {"type": "text", "text": "Monotone Case We ran the experiment on 5 random instances of (13) with the stepsizes scheduled as $\\eta_{k}\\,=\\,\\eta_{0}\\bigl/(1{+}k/10)^{0.34}$ Wwhere $\\begin{array}{r}{\\bar{\\eta_{0}}=\\operatorname*{min}\\{0.01,\\frac{1}{L}\\}}\\end{array}$ for SEG-FFA, and $\\alpha_{k}\\,=\\,\\beta_{k}\\,=\\,\\eta_{k}$ forSEGUS, SEG-RR, and SEG-FF. The exponent 0.34 is to ensure a sufficient decay rate required by Theorem 5.4, and the convergence of SEG-FFA under such a stepsize scheduling is validated in Remark G.5. The value of $\\eta_{0}$ is, however, a heuristically determined small number. The results of the geometric mean over the 5 runs are plotted in Figure 1. As expected by our theory, SEG-FFA successfully shows convergence, while all of SEG-FF, SEG-RR, and SEG-US diverge in the long run. ", "page_idx": 8}, {"type": "text", "text": "Strongly monotone case Along with the variants of SEG, we also compare the performances of SGDA-RR and SGDA-US. We ran the experiment on 5 random instances of (13) with stepsizes $\\eta_{k}=0.001$ , and the results are plotted in Figure 1. Additional results obtained from using other stepsizes can be found in Appendix I.4. We again observe an agreement between the empirical results and our theory; SEG-FFA eventually finds the point with the smallest gradient norm among the methods that are considered. ", "page_idx": 8}, {"type": "text", "text": "Further additional experiments and ablation studies we have conducted can be found in Appendix I. ", "page_idx": 8}, {"type": "image", "img_path": "OJxua0PAIo/tmp/ee5743f380172dff380cdbde915441f389c1c6871061615f3550547419a60487.jpg", "img_caption": ["Figure 1: Experimental results on the (left) monotone and (right) strongly monotone examples, comparing the variants of SEG. For a fair comparison, we take the number of passes over the full dataset as the abscissae. In other words, we plot $\\|\\boldsymbol{F}z_{0}^{t/2}\\|^{2}\\big/\\|\\boldsymbol{F}z_{0}^{0}\\|^{2}$ for SEG-FFA and SEG-FF, as they pass through the whole dataset twice every epoch, and $\\|F z_{0}^{t}\\|^{2}\\big/\\|F z_{0}^{0}\\|^{2}$ for the other methods, as they pass once every epoch. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We proposed SEG-FFA, a new stochastic variant of EG that uses flip-fop sampling and anchoring. While being a minimal modification from the vanilla SEG, SEG-FFA attains the crucial \u201csecondorder matching\u201d property to the deterministic EG, leading to a two-fold improved convergence. On one hand, SEG-FFA reaches an optimum in the monotone setting, unlike many baseline methods such as SEG-US, SEG-RR, and SEG-FF that diverge. Moreover, in the strongly monotone setting, SEG-FFA shows a faster convergence with a provable gap from the other methods. ", "page_idx": 9}, {"type": "text", "text": "An interesting future direction would be to extend our work to more general nonconvex-nonconcave problems, further exploring the potentials of the second-order matching technique. It would also be appealing to further study whether it is possible to devise a new method that achieves second-order (or higher) matching without the anchoring step, potentially enhancing our understanding of the effectiveness of the matching technique. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported in part by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. RS-2019-NR040050). JC and DK acknowledge support from the NRF grant (No. RS-2022-NR071715) funded by the Korea government (MSIT), and the Samsung Science & Technology Foundation grant (No. SSTF-BA2101-02). CY acknowledges support from the NRF grant (No. RS-2023-00211352) funded by the Korea government (MSIT). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1]  Kwangjun Ahn, Chulhee Yun, and Suvrit Sra. SGD with shuffing: optimal rates without component convexity and large epoch requirements. Advances in Neural Information Processing Systems,33:17526-17535,2020.   \n[2]  Ahmet Alacaoglu and Yura Malitsky. Stochastic variance reduction for variational inequality methods. In Conference on Learning Theory, pages 778-816. PMLR, 2022.   \n[3]  Kenneth J. Arrow and Leonid Hurwicz. Reduction of constrained maxima to saddle-point problems. In Proc. Third Berkeley Symp. on Math. Statist. and Prob., volume 5, pages 1-20, 1956. Univ. of Calif. Press.   \n[4]  Waiss Azizian, Ioannis Mitliagkas, Simon Lacoste-Julien, and Gauthier Gidel. A tight and unified analysis of gradient-based methods for a whole spectrum of differentiable games. In International Conference on Artificial Intelligence and Statistics, pages 2863-2873. PMLR, 2020.   \n[5] Heinz H. Bauschke and Patrick L. Combettes. Convex Analysis and Monotone Operator Theory in Hilbert Spaces. Springer, 2nd edition, 2017.   \n[6]  Aleksandr Beznosikov, Valentin Samokhin, and Alexander Gasnikov. Distributed saddle-point problems: Lower bounds, near-optimal and robust algorithms. arXiv preprint arXiv:2010.13112, 2020.   \n[7] Aleksandr Beznosikov, Boris Polyak, Eduard Gorbunov, Dmitry Kovalev, and Alexander Gasnikov. Smooth monotone stochastic variational inequalities and saddle point problems: A survey. European Mathematical Society Magazine, 127:15-28, 2023.   \n[8]  L\u00e9on Bottou. Curiously fast convergence of some stochastic gradient descent algorithms. In Procedings of the symposium on learning and data science, Paris, volume 8, pages 2624-2633. Citeseer, 2009.   \n[9] Xufeng Cai, Chaobing Song, Cristobal Guzman, and Jelena Diakonikolas. Stochastic Halpern iteration with variance reduction for stochastic monotone inclusions. Advances in Neural Information Processing Systems, 35:24766-24779, 2022.   \n[10] Yair Carmon, Yujia Jin, Aaron Sidford, and Kevin Tian. Variance reduction for matrix games. Advances in Neural Information Processing Systems, 32, 2019.   \n[11] Jaeyoung Cha, Jaewook Lee, and Chulhee Yun. Tighter lower bounds for shuffing SGD: Random permutations and beyond. In International Conference on Machine Learning, pages 3855-3912. PMLR, 2023.   \n[12]  Tatjana Chavdarova, Matteo Pagliardini, Sebastian U Stich, Francois Fleuret, and Martin Jaggi. Taming GANs with Lookahead-minmax. In The Ninth International Conference on Learning Representations, 2021.   \n[13]  Hanseul Cho and Chulhee Yun. SGDA with shuffing: faster convergence for nonconvex-PL minimax optimization. In The Eleventh International Conference on Learning Representations, 2023.   \n[14]  Sayantan Choudhury, Eduard Gorbunov, and Nicolas Loizou. Single-call stochastic extragradient methods for structured non-monotone variational inequalities: Improved analysis under weaker conditions. Advances in Neural Information Processing Systems, 36:64918-64956, 2023.   \n[15]  Aniket Das, Bernhard Scholkopf, and Michael Muehlebach. Sampling without replacement leads to faster rates in finite-sum minimax optimization. Advances in Neural Information Processing Systems, 35:6749-6762, 2022.   \n[16]  Aaron Defazio and L\u00e9on Bottou. On the ineffectiveness of variance reduced optimization for deep learning. Advances in Neural Information Processing Systems, 32, 2019.   \n[17]  Jelena Diakonikolas, Constantinos Daskalakis, and Michael I. Jordan. Efficient methods for structured nonconvex-nonconcave min-max optimization. In International Conference on Artijfcial Intelligence and Statistics, pages 2746-2754. PMLR, 2021.   \n[18]  Konstantinos Emmanouilidis, Rene Vidal, and Nicolas Loizou. Stochastic extragradient with random reshuffing: Improved convergence for variational inequalities. In International Conference on Artificial Intelligence and Statistics, pages 3682-3690. PMLR, 2024.   \n[19] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014.   \n[20] Eduard Gorbunov, Hugo Berard, Gauthier Gidel, and Nicolas Loizou. Stochastic extragradient: General analysis and improved rates. In International Conference on Artificial Intelligence and Statistics, pages 7865-7901. PMLR, 2022.   \n[21]  Eduard Gorbunov, Nicolas Loizou, and Gauthier Gidel. Extragradient method: $O(1/K)$ lastiterate convergence for monotone variational inequalities and connections with cocoercivity. In International Conference on Artifcial Intelligence and Statistics, pages 366-402. PMLR, 2022.   \n[22] Benjamin Grimmer, Haihao Lu, Pratik Worah, and Vahab Mirrokni. The landscape of the proximal point method for nonconvex-nonconcave minimax optimization. Mathematical Programming, 201(1-2):373-407, 2023.   \n[23]  Benjamin Halpern. Fixed points of nonexpanding maps. Bulletin of the American Mathematical Society, 73(6):957-961, 1967.   \n[24] Charles R. Harris, K. Jarrod Milman, Stefan J. van der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti Picus, tephan Hoyer, Marten H. an Kerkwijk, Matthw Bret, Allan Haldane, Jaime Frnandez del Rio, Mark Wiebe, Pearu Peterson, Pierre Gerard-Marchant, Kevin Sheppard, Tyler Reddy, Warren Weckeser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Aray programming with NumPy. Nature, 585(7825):357-362, September 2020.   \n[25] Yu-Guan Hsieh, Franck Iutzeler, Jerome Malick, and Panayotis Mertikopoulos. Explore aggressively, update conservatively: Stochastic extragradient methods with variable stepsize scaling. Advances in Neural Information Processing Systems, 33:16223-16234, 2020.   \n[26] J D. Hunter. Matplotlib: A 2d graphics environment. Computing in Science & Engineering, 9 (3):90-95, 2007.   \n[27]  Anatoli Juditsky, Arkadi Nemirovski, and Claire Tauvel. Solving variational inequalities with stochastic mirror-prox algorithm. Stochastic Systems, 1(1):17-58, 2011.   \n[28] Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, and Stefano Ermon. Consistency trajectory models: Learning probability How ODE trajectory of diffusion. In International Conference on Learning Representations, 2024.   \n[29] Galina M. Korpelevich. The extragradient method for fnding saddle points and other problems. Matecon, 12:747-756, 1976.   \n[30] M. A. Krasnosel'ski. Two remarks on the method of successive approximations. Uspekhi Matematicheskikh Nauk, 10:123-127, 1955.   \n[31] Nicolas Loizou, Hugo Berard, Gauthier Gidel, Ioannis Mitliagkas, and Simon Lacoste-Julien. Stochastic gradient descent-ascent and consensus optimization for smooth games: Convergence analysis under expected co-coercivity. Advances in Neural Information Processing Systems, 34: 19095-19108, 2021.   \n[32]  Yucheng Lu, Wentao Guo, and Christopher De Sa. GraB: Finding provably better data permutations than random reshuffing. Advances in Neural Information Processing Systems, 35: 8969-8981, 2022.   \n[33]  W. Robert Mann. Mean value methods in iteration. Proceedings of the American Mathematical Society, 4(3):506-510, 1953.   \n[34] Bernard Martinet. Regularisation d\u2019inequations variationelles par approximations succesives. Revue Francaise d'informatique et de Recherche operationelle, 1970.   \n[35]  Konstantin Mishchenko, Ahmed Khaled, and Peter Richtarik. Random reshuffling: Simple analysis with vast improvements. Advances in Neural Information Processing Systems, 33: 17309-17320, 2020.   \n[36]  Konstantin Mishchenko, Dmitry Kovalev, Egor Shulgin, Peter Richtarik, and Yura Malitsky. Revisiting stochastic extragradient. In International Conference on Artijficial Intelligence and Statistics, pages 4573-4582. PMLR, 2020.   \n[37]  Aleksander Madry, Aleksandar Makelov, Ludwig Schmdit, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018.   \n[38]  Aryan Mokhtari, Asuman Ozdaglar, and Sarath Pattathil. A unified analysis of extra-gradient and optimistic gradient methods for saddle point problems: Proximal point approach. In International Conference on Artijficial Intelligence and Statistics, pages 1497-1507. PMLR, 2020.   \n[39] Dheeraj Nagaraj, Prateek Jain, and Praneeth Netrapalli SGD without replacement: Sharper rates for general smooth convex functions. In International Conference on Machine Learning, pages 4703-4711. PMLR, 2019.   \n[40]  Yuri Nesterov. Lectures on convex optimization, volume 137 of Springer Optimization and Its Applications. Springer, second edition, 2018.   \n[41] Lam M. Nguyen, Quoc Tran-Dinh, Dzung T. Phan, Phuong Ha Nguyen, and Marten Van Dijk. A unified convergence analysis for shuffing-type gradient methods. The Journal of Machine Learning Research, 22(1):9397-9440, 2021.   \n[42]  Thomas Pethick, Olivier Fercoq, Puya Latafat, Panagiotis Patrinos, and Volkan Cevher. Solving stochastic weak Minty variational inequalities without increasing batch size. In International Conference on Learning Representations, 2023.   \n[43]  Thomas Pethick, Wanyun Xie, and Volkan Cevher. Stable nonconvex-nonconcave training via linear interpolation. Advances in Neural Information Processing Systems, 37, 2023.   \n[44]  L. D. Popov. A modification of the Arrow-Hurwitz method of search for saddle points. Matematicheskie Zametki, 28(5):777-784, 1980.   \n[45]  Shashank Rajput, Anant Gupta, and Dimitris Papailiopoulos. Closing the convergence gap of SGD without replacement. In International Conference on Machine Learning, pages 7964-7973. PMLR, 2020.   \n[46]  Shashank Rajput, Kangwook Lee, and Dimitris Papailiopoulos. Permutation-based SGD: Is random optimal? In International Conference on Learning Representations, 2022.   \n[47]  Benjamin Recht and Christopher R\u00e9. Parallel stochastic gradient algorithms for large-scale matrix completion. Mathematical Programming Computation, 5(2):201-226, 2013.   \n[48] Litu Rout, Alexander Korotin, and Evgeny Burnaev. Generative modeling with optimal ransport maps. In International Conference on Learning Representations, 2022.   \n[49] Itay Safran and Ohad Shamir. How good is SGD with random shufling? In Conference on Learning Theory, pages 3250-3284. PMLR, 2020.   \n[50] Itay Safran and Ohad Shamir. Random shufling beats SGD only after many epochs on illconditioned problems. Advances in Neural Information Processing Systems, 34:15151-15161, 2021.   \n[51]  Mikhail V. Solodov and Benar F. Svaiter. A hybrid approximate extragradient-proximal point algorithm using the enlargement of a maximal monotone operator. Set- Valued Analysis, 7(4): 323-345, 1999.   \n[52] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stefan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrw R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, CJ Carey, flhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Antonio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods, 17:261-272, 2020.   \n[53] Hoi-To Wai, Zhuoran Yang, Zhaoran Wang, and Mingyi Hong. Multi-agent reinforcement learning via double averaging primal-dual optimization. Advances in Neural Information Processing Systems, 31, 2018.   \n[54]  TaeHo Yoon and Ernest K. Ryu. Accelerated Algorithms for Smooth Convex-Concave Minimax Problems_with ${\\mathcal{O}}(1/k^{2})$ Rate on Squared Gradient Norm. In International Conference on Machine Learning, pages 12098-12109. PMLR, 2021.   \n[55]  Chulhee Yun, Suvrit Sra, and Ali Jadbabaie. Open problem: Can single-shuffe SGD be better than reshuffing SGD and GD? In Conference on Learning Theory, pages 4653-4658. PMLR, 2021.   \n[56] Chulhee Yun, Shashank Rajput, and Suvrit Sra. Minibatch vs local SGD with shuffling: Tight convergence bounds and beyond. In International Conference on Learning Representations, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "1  Introduction 1.1 Our Contributions ", "page_idx": 14}, {"type": "text", "text": "2  Related Works 3 ", "page_idx": 14}, {"type": "text", "text": "3  Notations and Problem Settings ", "page_idx": 14}, {"type": "text", "text": "4  Shuffling Alone Is Not Enough 6 ", "page_idx": 14}, {"type": "text", "text": "5 SEG-FFA: SEG with Flip-Flop Anchoring ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "5.1 Design Principle: Second-Order Matching . . . 7   \n5.1.1 Necessity of Flip-Flop Sampling \u00b7 . . . . 7   \n5.1.2 Designing SEG-FFA ... 8   \n5.2 Convergence Analysis of SEG-FFA . . 8 ", "page_idx": 14}, {"type": "text", "text": "6 Experiments 9 ", "page_idx": 14}, {"type": "text", "text": "7Conclusion 10 ", "page_idx": 14}, {"type": "text", "text": "A   Pseudocode of the Algorithms 17 ", "page_idx": 14}, {"type": "text", "text": "B Further Details and Discussions on the Related Works 18 ", "page_idx": 14}, {"type": "text", "text": "B.1 A Summary of the Limitations of the Existing Works in the Monotone Setting . . - 18   \nB.2  On the Assumptions Made by Gorbunov et al. [20] . . . 19   \nB.3  Finite Sum Structure vs. General Stochastic Setting . . . 19   \nB.4 On the Claimed Convergence of SEG-RR in the Monotone Setting by Emmanouilidis   \net al. [18] . . . 20 ", "page_idx": 14}, {"type": "text", "text": "C  Useful Lemmata 20 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "D  Missing Proofs for Section 5 25 ", "page_idx": 14}, {"type": "text", "text": "D.1  Unravelling the Recurrence of the Generalized SEG in (8) and (12) . . . 25   \nD.2  Insufficiency of Only Using Flip-Flop Sampling \u00b7 . . . 27 ", "page_idx": 14}, {"type": "text", "text": "E  Within-Epoch Error Analysis for Upper Bounds 28 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "E.1 Auxiiary Lemmata . . : 29   \nE.2Upper Bounds of the Within-Epoch Errors . . 37   \nE.2.1 Proof of Equation (31) for SEG-FFA 37   \nE.2.2 Proof of Equation (32) for SEG-FFA 43   \nE.2.3 Upper Bounds of the Within-Epoch Errors for SEG-FF 45   \nE.2.4 Upper Bounds of the Within-Epoch Errors for SEG-RR 47 ", "page_idx": 14}, {"type": "text", "text": "F  Convergence Bounds in the Strongly Monotone Setting 48 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "F.1 Unified Analysis of the Upper Bounds for Shuffling-Based SEG Methods . . . . -. 48 ", "page_idx": 15}, {"type": "text", "text": "G Convergence Rate of SEG-FFA in the Monotone Setting 54 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "G.1Star-monotonicity 54   \nG.2  Convergence Analysis of SEG-FFA in the (Star-)Monotone Setting 54 ", "page_idx": 15}, {"type": "text", "text": "H Proof of Lower Bounds 59 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "H.1 Proof of the Divergence of SEG-US, SEG-RR and SEG-FF 59   \nH.2 Proof of Limited Convergence of SEG-US in Monotone Cases . . 61   \nH.3 Proof of SGDA-RR and SEG-RR Lower Bounds . . . : 64   \nH.3.1 Existing Lower Bound for SGD-RR . . . : 64   \nH.3.2 Proof of Lower Bound for SGDA-RR . .: 65   \nH.3.3 Proof of Lower Bound for SEG-RR . . : 65 ", "page_idx": 15}, {"type": "text", "text": "1  Additional Experiments 69 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "1.1 Problem Constructions for Experiments in Section 6 . . . 70   \n1.2 Monotone Case & Ablation Study on the Anchoring Step . . . 70   \n1.3 Monotone Case: Comparison with Hsieh et al. [25] . : : 71   \nI1.4 Strongly Monotone Case Again, with Various Stepsizes . . . 72 ", "page_idx": 15}, {"type": "text", "text": "A  Pseudocode of the Algorithms ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We present the pseudocode of the algorithms we consider in this paper in Algorithms 2, 3 and 4, with the pseudocode of the with-replacement stochastic methods in Algorithm 1. ", "page_idx": 16}, {"type": "text", "text": "Algorithm 1 SEG-US / SGDA-US ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Input: The number of components $n$ ; stepsize sequences $\\{\\alpha_{t}\\}_{t\\ge0}$ and $\\{\\beta_{t}\\}_{t\\ge0}$   \nInitialize: $z_{0}\\in\\mathbb{R}^{d_{1}+d_{2}}$   \nfor $t=0,1,\\dots$ do sample $i(t)$ uniformly from $\\{1,\\ldots,n\\}$ if SGDA-US then $\\smash{z_{t+1}\\!\\gets\\!z_{t}-\\alpha_{t}\\!\\!+\\!1_{i(t)}\\!z_{t}}$ elseifSEG-USthen $\\begin{array}{r l}&{{\\pmb w}_{t}\\leftarrow{\\pmb z}_{t}-\\alpha_{t}{\\pmb F}_{i(t)}{\\pmb z}_{t}}\\\\ &{{\\pmb z}_{t+1}\\leftarrow{\\pmb z}_{t}-\\beta_{t}{\\pmb F}_{i(t)}{\\pmb w}_{t}}\\end{array}$ end if   \nend for ", "page_idx": 16}, {"type": "text", "text": "Algorithm 2 SEG-RR / SGDA-RR ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Input: The number of components $n$ ; stepsize sequences $\\{\\alpha_{k}\\}_{k\\ge0}$ and $\\{\\beta_{k}\\}_{k\\ge0}$   \nInitialize: $\\boldsymbol{z}_{0}^{0}\\in\\mathbb{R}^{d_{1}+d_{2}}$   \nfor $k=0,1,\\dots$ do sample $\\tau_{k}$ uniformly from ${\\mathcal{S}}_{n}$ for $i=0$ to $n-1$ do if SGDA-RR then $z_{i+1}^{k}\\leftarrow z_{i}^{k}-\\alpha_{k}F_{\\tau_{k}(i+1)}z_{i}^{k}$ elseif'SEG-RRthen $\\begin{array}{r l}&{\\pmb{w}_{i}^{k}\\leftarrow\\pmb{z}_{i}^{k}-\\alpha_{k}\\pmb{F}_{\\tau_{k}(i+1)}\\pmb{z}_{i}^{k}}\\\\ &{\\pmb{z}_{i+1}^{k}\\leftarrow\\pmb{z}_{i}^{k}-\\beta_{k}\\pmb{F}_{\\tau_{k}(i+1)}\\pmb{w}_{i}^{k}}\\end{array}$ end if end for $z_{0}^{k+1}\\leftarrow z_{n}^{k}$   \nend for ", "page_idx": 16}, {"type": "text", "text": "Algorithm 3 SEG-FF ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Input: The number of components $n$ ; stepsize sequences $\\{\\alpha_{k}\\}_{k\\ge0}$ and $\\{\\beta_{k}\\}_{k\\ge0}$   \nInitialize: $\\boldsymbol{z}_{0}^{0}\\in\\mathbb{R}^{d_{1}+d_{2}}$   \nfor $k=0,1,\\dots$ do sample $\\tau_{k}$ uniformly from ${\\mathcal{S}}_{n}$ for $i=0$ to $n-1$ do $\\begin{array}{r l}&{{\\pmb w}_{i}^{k}\\leftarrow z_{i}^{k}-\\alpha_{k}{\\pmb F}_{\\tau_{k}(i+1)}z_{i}^{k}}\\\\ &{z_{i+1}^{k}\\leftarrow z_{i}^{k}-\\beta_{k}{\\pmb F}_{\\tau_{k}(i+1)}{\\pmb w}_{i}^{k}}\\end{array}$ endfor for $i=n$ to $2n-1$ do $\\pmb{w}_{i}^{k}\\leftarrow z_{i}^{k}-\\alpha_{k}\\pmb{F}_{\\tau_{k}(2n-i)}\\pmb{z}_{i}^{k}$ $z_{i\\pm1}^{k}\\leftarrow z_{i}^{k}-\\beta_{k}{\\pmb F}_{\\tau_{k}(2n-i)}{\\pmb w}_{i}^{k}$ end for $z_{0}^{k+1}\\leftarrow z_{2n}^{k}$   \nend for ", "page_idx": 16}, {"type": "text", "text": "B  Further Details and Discussions on the Related Works ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "B.1A Summary of the Limitations of the Existing Works in the Monotone Setting ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In Table 2, we have summarized the settings considered in each of the previous works on stochastic variants of EG discussed in Section 2, and compare them with our settings. Please note that we focus onthemonotone $\\pmb{F}$ setting in the table. Entries that are worth further discussions are marked, with the corresponding explanations below. ", "page_idx": 17}, {"type": "table", "img_path": "OJxua0PAIo/tmp/10aa96adefd44b3ae9e32dd8393b0a191b333beec0c5699aa8a7d910fdd88515.jpg", "table_caption": ["Table 2: Comparison of the underlying settings between ours and the existing works "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "(\\*)  The methods proposed in these works are not stochastic variants of EG in a strict sense. The method introduced by Cai et al. [9] is rather a hybrid of EG and the Halpern iteration [23], while the method by Choudhury et al. [14] is a stochastic version of the so-called optimistic gradient method [44]. Hence, determining whether these methods fall into the category of same-sample methods or not is unnecessary. Nonetheless, as these works focus on solving a similar problem to ours, we include them as references. ", "page_idx": 17}, {"type": "text", "text": "() Under the assumptions that Gorbunov et al. [20] make in their paper, one can show that each of the components must necessarily be (star-)monotone when the full $\\pmb{F}$ is (star-)monotone. For further explanations on why this is the case, see the following Appendix B.2. ", "page_idx": 17}, {"type": "text", "text": "(\\$)Yet, to be precise, what Gorbunov et al. [20] have shown in the monotone case is that SEG-US can find an optimal solution if we increase the batch size each iteration. If the batch size is fixed as a constant, then they were only able to show that the iterates will be bounded in the (star-)monotone setting. In particular, they did not provide a guarantee that the iterates will be necessarily convergent. ", "page_idx": 17}, {"type": "text", "text": "In fact, as we demonstrate with an explicit counterexample in Appendix H.2, if we do not increase the batch size each iteration, then it is possible to show that SEG-US in the worst case will never converge to an optimal point. This nonconvergence result in fact holds for any SEG-US whose extrapolation and update stepsizes differ by a constant factor. Hence, it not only applies to [20], but also to [17]. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "(\\$) Hsieh et al. [25] show that independent-sample SEG-US converges for stepsizes $\\alpha_{t},\\beta_{t}$ decaying at certain different rates, but gives no convergence rates. ", "page_idx": 18}, {"type": "text", "text": "(D Mishchenko et al. [36] assume uniform gradient variance in the strongly monotone case. In the monotone case, the bound they derived depends on the supremum of the gradient variance over the domain that is under consideration. Hence, in the monotone case, either the domain has to be (implicitly) bounded, or the uniform gradient variance assumption should be imposed. ", "page_idx": 18}, {"type": "text", "text": "B.2 On the Assumptions Made by Gorbunov et al. [20] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We would like to first clarify that in [20], the requirement of increasing the batch size is utilized only in the monotone setting: see, e.g., Corollary E.4 therein. ", "page_idx": 18}, {"type": "text", "text": "Gorbunov et al. [20] use a generalized notion of $\\mu$ -strong monotonicity, namely the $\\mu$ -quasi strong monotonicity, which requires the operator $\\pmb{F}$ to satisfy ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\langle F z,z-z^{*}\\rangle\\geq\\mu\\left\\|z-z^{*}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In the notion of $\\mu$ -quasi strong monotonicity they also allow $\\mu\\leq0$ . In particular, if (14) holds with $\\mu=0$ , then $\\pmb{F}$ is called a star-monotone operator. ", "page_idx": 18}, {"type": "text", "text": "Meanwhile, let us further elaborate on why in the (star-)monotone setting, the assumptions made by the authors of [20] lead to each component being star-monotone. In their work the authors require, as equation (10) therein, that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{i:\\mu_{i}\\geq0}\\mu_{i}+\\frac{4}{n}\\sum_{i:\\mu_{i}<0}\\mu_{i}\\geq0.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Observe that this amounts to ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mu:=\\frac{1}{n}\\sum_{i=1}^{n}\\mu_{i}\\geq-\\frac{3}{n}\\sum_{i:\\mu_{i}<0}\\mu_{i}=\\frac{3}{n}\\sum_{i:\\mu_{i}<0}\\left|\\mu_{i}\\right|.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "However, if any of $\\mu_{i}$ is strictly negative, then the rightmost sum in (16) becomes strictly positive, hence cannot be less than or equal to $\\mu$ if $\\mu\\,=\\,0$ . Therefore, the only possible case is when the rightmost sum is an empty sum. In other words, (15) can hold with $\\mu=0$ only when $\\mu_{i}\\geq0$ for all $i$ so that each $F_{i}$ is star-monotone. We would like to remind the readers that our analyses, on the other hand, do not have any restrictions on the individual components. ", "page_idx": 18}, {"type": "text", "text": "B.3 Finite Sum Structure vs. General Stochastic Setting ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The works mentioned in Section 2 usually assume that we have access to a stochastic oracle that returns a stochastic estimator of $\\pmb{F}$ . Indeed, having a finite sum structure is a special case of having a stochastic oracle, as each $F_{i}$ can be seen as an estimator of $\\pmb{F}$ . One might then ask whether assuming the finite sum structure can help the works mentioned in Section 2 overcome the mentioned limitations. We strongly believe that this is not the case. Recall Theorem 4.1, where we have constructed an explicit counterexample that SEG-US, SEG-RR, and SEG-FF all diverge. Because the set of problems with a finite sum structure is a subset of the set of problems with a stochastic oracle, the (counter-)example in Theorem 4.1 also works as an example that displays the nonconvergence of SEG-US, SEG-RR, and SEG-FF in the general stochastic setting. That is, a variant of SEG that only modifies the stepsizes and/or the sampling scheme into a without-replacement based one will suffer from nonconvergence, because of the counterexample in Theorem 4.1. It is also true that there are some methods that cannot exactly be classified as one of SEG-US, SEG-RR, or SEG-FF, but this counterexample demonstrates that, unless explicitly proven otherwise, there is not a good reason to believe that the existing convergence analyses will be easily extended beyond the assumptions they are each based on. ", "page_idx": 18}, {"type": "text", "text": "B.4  On the Claimed Convergence of SEG-RR in the Monotone Setting by Emmanouilidis et al. [18] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Recently, a paper focusing on the study of SEG-RR [18] has been published. As we have briefly introduced in Table 1 with a discussion in Section 4, the authors have established a convergence rate $\\tilde{\\mathcal{O}}(^{1}/n K^{2})$ of SEG-RR in the strongly monotone setting, using an independent analysis from ours. ", "page_idx": 19}, {"type": "text", "text": "On the other hand, the authors of [18] furthermore claim that SEG-RR is capable of finding an optimum in the monotone setting, which is seemingly contradictory to our analyses. We assert that this is not the case, as their proof, at least in their AISTATS 2024 version, seems to have a flaw. ", "page_idx": 19}, {"type": "text", "text": "In establishing equation (85) in [18], the authors claim that the inequality ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{1}{K}\\sum_{k=0}^{K}\\frac{1}{G^{k}}\\mathop{\\mathbb{E}}\\left[\\|F(z_{0}^{k})\\|\\right]\\ge\\mathbb{E}\\left[\\left\\|F\\left(\\frac{1}{K}\\sum_{k=0}^{K}\\frac{1}{G^{k}}z_{0}^{k}\\right)\\right\\|^{2}\\right]\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "holds by Jensen's inequality, where. $G\\geq6$ is a fixed constant. However, Jensen's inequality cannot be applied here, because not only $\\lVert\\pmb{F}(\\cdot)\\rVert^{2}$ is possibly nonconvex, but also the weights multiplied to the iterates, namely ${^1\\!/}K G^{k}$ , do not sum up to 1. Hence, the \u201caveraged\" iterate is not in the form of a convex combination. So, even if $\\|\\pmb{F}(\\cdot)\\|^{2}$ was convex, if we were to properly apply Jensen's inequalty a lathavagderaehoudmliplady $\\frac{1}{\\sum_{k=0}^{K}1/G^{k}}$ instad of $\\textstyle{\\frac{1}{K}}$ Yethen, the sum $\\begin{array}{r}{\\sum_{k=0}^{K}\\frac{1}{G^{k}}\\leq\\frac{G}{G-1}}\\end{array}$ isbouddabbynanede $K$ and the right hand side of the equation right above (85) in [18] shall no longer be divided by . Therefore, their claimed convergence is unobtainable. ", "page_idx": 19}, {"type": "text", "text": "We would also like to remark that the linear decay rate of $1/G^{k}$ can  make  the  series $\\begin{array}{r}{\\sum_{k=0}^{\\infty}\\frac{1}{G^{k}}\\,\\mathbb{E}[\\|F(z_{0}^{k})\\|^{2}]}\\end{array}$ convergent even when $\\mathbb{E}[\\|\\pmb{F}(z_{0}^{k})\\|^{2}]$ grows exponentially as $k\\rightarrow\\infty$ as long as its rate of exponential growth is less than $G$ . In particular, once their (85) is corrected, there is no contradiction with our divergence result in Theorem 4.1. ", "page_idx": 19}, {"type": "text", "text": "C Useful Lemmata ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Lemma C.1 (Polarization identity). For any two vectors a and $^{b}$ itholdsthat ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\left\\langle\\pmb{a},\\pmb{b}\\right\\rangle=\\left\\|\\pmb{a}\\right\\|^{2}+\\left\\|\\pmb{b}\\right\\|^{2}-\\left\\|\\pmb{a}-\\pmb{b}\\right\\|^{2}}\\\\ &{\\qquad\\qquad=\\left\\|\\pmb{a}+\\pmb{b}\\right\\|^{2}-\\left\\|\\pmb{a}\\right\\|^{2}-\\left\\|\\pmb{b}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. The identities are immediate from $\\left\\|\\pmb{\\mathscr{a}}\\pmb{\\mathscr{b}}\\right\\|^{2}=\\left\\|\\pmb{\\mathscr{a}}\\right\\|^{2}\\pm2\\left\\langle\\pmb{\\mathscr{a}},\\pmb{\\mathscr{b}}\\right\\rangle+\\left\\|\\pmb{\\mathscr{b}}\\right\\|^{2}.$ ", "page_idx": 19}, {"type": "text", "text": "Lemma C.2 (Weighted AM-GM inequality). For any $\\gamma>0$ andtwovectors $\\textbf{\\em a}$ and $^{b}$ in $\\mathbb{R}^{d}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n2\\left\\lvert\\left\\langle\\boldsymbol{a},\\boldsymbol{b}\\right\\rangle\\right\\rvert\\leq\\gamma\\left\\lVert\\boldsymbol{a}\\right\\rVert^{2}+\\frac{1}{\\gamma}\\left\\lVert\\boldsymbol{b}\\right\\rVert^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. Notice that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle2\\left\\lvert\\langle\\mathbf{a},{b}\\rangle\\right\\rvert\\leq2\\left(\\left\\lvert a_{1}{b}_{1}\\right\\rvert+\\cdot\\cdot\\cdot+\\left\\lvert a_{d}{b}_{d}\\right\\rvert\\right)}\\\\ {\\displaystyle\\leq\\left(\\gamma a_{1}^{2}+\\frac{b_{1}^{2}}{\\gamma}\\right)+\\cdot\\cdot\\cdot+\\left(\\gamma a_{d}^{2}+\\frac{b_{d}^{2}}{\\gamma}\\right)=\\gamma\\left\\lVert\\mathbf{a}\\right\\rVert^{2}+\\displaystyle\\frac{1}{\\gamma}\\left\\lVert{b}\\right\\rVert^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Lemma C.3 (Young's inequality). For any $\\gamma>0$ andtwovectors $\\textbf{\\em a}$ and $^{b}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\|\\pmb{\\alpha}+\\pmb{b}\\right\\|^{2}\\leq\\left(1+\\gamma\\right)\\left\\|\\pmb{\\alpha}\\right\\|^{2}+\\left(1+\\frac{1}{\\gamma}\\right)\\left\\|\\pmb{b}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In particular, as a special case where $\\gamma=1$ ,it holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{}a+b\\right\\|^{2}\\leq2\\left\\|\\mathbf{}a\\right\\|^{2}+2\\left\\|b\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. The left hand side of (17) is $\\left\\|\\mathbf{\\boldsymbol{a}}\\right\\|^{2}+2\\left\\langle\\mathbf{\\boldsymbol{a}},\\mathbf{\\boldsymbol{b}}\\right\\rangle+\\left\\|\\mathbf{\\boldsymbol{b}}\\right\\|^{2}$ . Applying Lemma C.2 then suffices. ", "page_idx": 19}, {"type": "text", "text": "Lemma C.4. For any two vectors a and $^{b}$ it holds that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\|\\boldsymbol{a}-\\boldsymbol{b}\\right\\|^{2}\\geq\\frac{1}{2}\\left\\|\\boldsymbol{a}\\right\\|^{2}-\\left\\|\\boldsymbol{b}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. From (18) it follows that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\|a\\right\\|^{2}=\\left\\|\\left(a-b\\right)+b\\right\\|^{2}\\leq2\\left\\|a-b\\right\\|^{2}+2\\left\\|b\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Simply rearranging the terms gives us the result ", "page_idx": 20}, {"type": "text", "text": "Lemma C.5 (Generalized Young's inequality). For any nonnegative scalars $p_{1},\\ldots,p_{n}$ suchthat $p_{1}+\\cdot\\cdot\\cdot+p_{n}=1$ andvectors $\\pmb{a}_{1},\\dots,\\pmb{a}_{n}$ itholdsthat ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\|p_{1}\\pmb{a}_{1}+\\cdots+p_{n}\\pmb{a}_{n}\\right\\|^{2}\\leq p_{1}\\left\\|\\pmb{a}_{1}\\right\\|^{2}+\\cdots+p_{n}\\left\\|\\pmb{a}_{n}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In particular, setting $\\begin{array}{r}{p_{1}=\\cdot\\cdot\\cdot=p_{n}=\\frac{1}{n}}\\end{array}$ and multiplying both sides by $n^{2}$ yields ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\|a_{1}+\\cdot\\cdot+{\\boldsymbol{a}}_{n}\\right\\|^{2}\\leq n\\left(\\left\\|a_{1}\\right\\|^{2}+\\cdot\\cdot\\cdot+\\left\\|\\pmb{a}_{n}\\right\\|^{2}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. We use induction on $n$ .If $n=1$ then $p_{1}=1$ , so there is nothing to show. For the inductive step, suppose that the statement holds for some $n\\,\\geq\\,1$ . Say we are given nonnegative scalars $p_{1},\\ldots,p_{n+1}$ such that $p_{1}+\\cdot\\cdot\\cdot+p_{n+1}=1$ , and vectors $\\mathbf{\\displaystylea}_{1},\\ldots,\\mathbf{\\displaystylea}_{n+1}$ . For the moment, suppose that $p_{n+1}<1$ . Applying Lemma C.3 with $\\begin{array}{r}{\\gamma=\\frac{p_{n+1}}{1-p_{n+1}}}\\end{array}$ and using the induction hypothesis, we get ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\left\\|p_{1}{\\pmb{a}}_{1}+\\cdots+p_{n}{\\pmb{a}}_{n}+p_{n+1}{\\pmb{a}}_{n+1}\\right\\|^{2}}\\\\ {\\leq\\displaystyle\\frac{1}{1-p_{n+1}}\\left\\|p_{1}{\\pmb{a}}_{1}+\\cdots+p_{n}{\\pmb{a}}_{n}\\right\\|^{2}+\\displaystyle\\frac{1}{p_{n+1}}\\left\\|p_{n+1}{\\pmb{a}}_{n+1}\\right\\|^{2}}\\\\ {=(1-p_{n+1})\\left\\|\\frac{p_{1}}{1-p_{n+1}}{\\pmb{a}}_{1}+\\cdots+\\displaystyle\\frac{p_{n}}{1-p_{n+1}}{\\pmb{a}}_{n}\\right\\|^{2}+p_{n+1}\\left\\|{\\pmb{a}}_{n+1}\\right\\|^{2}}\\\\ {\\leq p_{1}\\left\\|{\\pmb{a}}_{1}\\right\\|^{2}+\\cdots+p_{n}\\left\\|{\\pmb{a}}_{n}\\right\\|^{2}+p_{n+1}\\left\\|{\\pmb{a}}_{n+1}\\right\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where in last line we used that $p_{1}+\\cdot\\cdot\\cdot+p_{n}=1-p_{n+1}$ .Now, if $p_{n+1}=1$ , then wemust have $p_{1}=\\cdot\\cdot\\cdot=p_{n}=0$ , so the claimed inequality holds in this case also. This completes the proof. ", "page_idx": 20}, {"type": "text", "text": "Lemma C.6. Suppose that $\\pmb{F}$ is $M$ -smooth. Then for any $_{\\textit{z}}$ and w it holds that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\|F w-F z-D F(z)(w-z)\\right\\|\\leq{\\frac{M}{2}}\\left\\|w-z\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. The proof closely follows the arguments used for Lemma 1.2.4 in [40], by replacing the gradients therein by saddle gradients. The fundamental theorem of calculus with the $M$ -smoothness Oof $\\pmb{F}$ gives us ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|F w-F z-D F(z)(w-z)\\|=\\left\\|\\int_{0}^{1}D F(z+t(w-z))\\,\\mathrm{d}t\\left(w-z\\right)-D F(z)(w-z)\\right\\|}\\\\ {\\displaystyle\\leq\\|w-z\\|\\int_{0}^{1}\\left\\|D F(z+t(w-z))-D F(z)\\right\\|\\,\\mathrm{d}t}\\\\ {\\displaystyle\\leq\\|w-z\\|\\int_{0}^{1}M t\\,\\|w-z\\|\\,\\,\\mathrm{d}t}\\\\ {\\displaystyle=\\frac{M}{2}\\,\\|w-z\\|^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Lemma C.7. Let $\\pmb{F}$ be a $\\mu$ -strongly monotone operator. Let $z^{*}$ be a point such that $\\pmb{F}\\pmb{z}^{*}=\\mathbf{0}$ and let $\\eta>0$ .Then, for any point $_{z}$ in the domain of $\\pmb{F}$ and $w:=z-\\eta F z$ ,it holds that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\langle F w,w-z^{*}\\right\\rangle\\geq\\frac{\\mu}{2}\\left\\Vert z-z^{*}\\right\\Vert^{2}-\\eta^{2}\\mu\\left\\Vert F z\\right\\Vert^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. By the $\\mu$ -strong monotonicity of $\\pmb{F}$ and Lemma C.4 it holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle F w,w-z^{*}\\rangle\\geq\\mu\\left\\|w-z^{*}\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad=\\mu\\left\\|z-\\eta F z-z^{*}\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\geq\\displaystyle\\frac{\\mu}{2}\\left\\|z-z^{*}\\right\\|^{2}-\\mu\\left\\|\\eta F z\\right\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "so we are done. ", "page_idx": 21}, {"type": "text", "text": "The following lemma generalizes Lemma 3.2 in [21] shown for monotone $\\pmb{F}$ to $\\mu$ -stronglymonotone $\\pmb{F}$ With $\\mu>0$ ", "page_idx": 21}, {"type": "text", "text": "Lemma C.8. Let $\\pmb{F}$ be a $\\mu$ strongly monotone $L$ -Lipschitz operator, and let $_{z}$ be any point in the domainof $\\pmb{F}$ Then for any $\\begin{array}{r}{0<\\eta<\\frac{1}{L\\sqrt{2}}}\\end{array}$ it holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left\\|F(z-\\eta F(z-\\eta F z))\\right\\|^{2}\\leq\\left(1-{\\frac{2\\eta\\mu}{5}}\\right)\\left\\|F z\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. For convenience, let us define $w:=z-\\eta F z$ and $z^{+}:=z-\\eta\\pmb{F}(z-\\eta\\pmb{F}z)=z-\\eta\\pmb{F}w$ Because $\\pmb{F}$ is $\\mu$ -strongly monotone, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mu\\left\\lVert z^{+}-z\\right\\rVert^{2}\\leq\\left\\langle F z^{+}-F z,z^{+}-z\\right\\rangle}\\\\ {=\\eta\\left\\langle F z-F z^{+},F w\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Also from the $\\mu$ -strong monotonicity of $\\pmb{F}$ we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mu\\left\\lVert w-z^{+}\\right\\rVert^{2}\\leq\\left\\langle F w-F z^{+},w-z^{+}\\right\\rangle}&{}\\\\ {=\\eta\\left\\langle F w-F z^{+},F w-F z\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Meanwhile, from the $L$ -Lipschitzness of $\\pmb{F}$ we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|F w-F z^{+}\\right\\|^{2}\\leq\\eta^{2}L^{2}\\left\\|F w-F z\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Summing up the inequalities (19), (20), (21) with weights $^2\\!/\\eta,\\,^{1}\\!/2\\eta.$ and ${^3}/{2}$ respectively, we obtain ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\mu}{\\eta}\\left(2\\left\\|z^{+}-z\\right\\|^{2}+\\frac{1}{2}\\left\\|w-z^{+}\\right\\|^{2}\\right)+\\frac{3}{2}\\left\\|F w-F z^{+}\\right\\|^{2}}\\\\ {\\displaystyle\\qquad\\leq2\\left\\langle F z-F z^{+},F w\\right\\rangle+\\frac{1}{2}\\left\\langle F w-F z^{+},F w-F z\\right\\rangle+\\frac{3\\eta^{2}L^{2}}{2}\\left\\|F w-F z\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "From this inequality, we can exactly follow the arguments used in the proof of Lemma D.4 in [21] to derivethat ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{\\mu}{\\eta}\\left(2\\left\\|z^{+}-z\\right\\|^{2}+\\frac{1}{2}\\left\\|w-z^{+}\\right\\|^{2}\\right)+\\left\\|F z^{+}\\right\\|^{2}\\leq\\left\\|F z\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Meanwhile, Young's inequality (Lemma C.3) tells us that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\eta^{2}\\left\\|F z\\right\\|^{2}=\\left\\|w-z\\right\\|^{2}\\leq\\left(1+\\frac{1}{4}\\right)\\left\\|w-z^{+}\\right\\|^{2}+(1+4)\\left\\|z^{+}-z\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Using this to lower bound the left hand side of (22), we get that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{2\\eta\\mu}{5}\\left\\|{\\cal F}z\\right\\|^{2}+\\left\\|{\\cal F}z^{+}\\right\\|^{2}\\leq\\left\\|{\\cal F}z\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "It remains to simply rearrange the terms. ", "page_idx": 21}, {"type": "text", "text": "Lemma C.9. Suppose that $F_{i}$ $L$ -Lipschitz for all $i\\,=\\,1,\\,.\\,.\\,,n_{}$ and that $\\begin{array}{r}{\\pmb{F}:=\\,\\frac{1}{n}\\sum_{i=1}^{n}\\pmb{F}_{i}}\\end{array}$ is $\\mu$ stronglymonotonewith $\\mu\\,>\\,0$ Define $\\kappa\\,:=\\,L/\\mu$ and $\\begin{array}{r}{\\sigma_{*}^{2}:=\\frac{1}{n}\\sum_{i=1}^{n}\\|F_{i}z^{*}\\|^{2}}\\end{array}$ .Then,for any $z\\in\\mathbb{R}^{d_{1}+d_{2}}$ it holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{i=1}^{n}\\,\\|F_{i}z-F z\\|^{2}\\leq\\left(\\sqrt{3(1+\\kappa^{2})}\\,\\|F z\\|+\\sqrt{3}\\sigma_{*}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. For any $z,w\\in\\mathbb{R}^{d_{1}+d_{2}}$ , as Assumption 3.1 holds with $\\mu>0$ by Cauchy-Schwarz inequality ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mu\\left\\|z-w\\right\\|^{2}\\leq\\left\\langle F z-F w,z-w\\right\\rangle\\leq\\left\\|F z-F w\\right\\|\\left\\|z-w\\right\\|,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and as a consequence, $\\|\\pmb{z}-\\pmb{w}\\|\\leq1/\\mu\\,\\|\\pmb{F}\\pmb{z}-\\pmb{F}\\pmb{w}\\|$ . Thus, for any $i\\in[n]$ , it holds that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|F_{i}z-F z\\right\\|^{2}\\leq3\\left\\|F_{i}z-F_{i}z^{*}\\right\\|^{2}+3\\left\\|F z-F z^{*}\\right\\|^{2}+3\\left\\|F_{i}z^{*}-F z^{*}\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\leq3L^{2}\\left\\|z-z^{*}\\right\\|^{2}+3\\left\\|F z-F z^{*}\\right\\|^{2}+3\\left\\|F_{i}z^{*}\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\leq3\\left(\\frac{L^{2}}{\\mu^{2}}+1\\right)\\left\\|F z-F z^{*}\\right\\|^{2}+3\\left\\|F_{i}z^{*}\\right\\|^{2}}\\\\ &{\\qquad\\qquad=3(1+\\kappa^{2})\\left\\|F z\\right\\|^{2}+3\\left\\|F_{i}z^{*}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Summing this inequality over $i=1,\\hdots,n$ and then dividing by $n$ leads to ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\left\\|F_{i}z-F z\\right\\|^{2}\\leq3(1+\\kappa^{2})\\left\\|F z\\right\\|^{2}+\\displaystyle\\frac{3}{n}\\sum_{i=1}^{n}\\left\\|F_{i}z^{*}\\right\\|^{2}}\\\\ {\\displaystyle=3(1+\\kappa^{2})\\left\\|F z\\right\\|^{2}+3\\sigma_{*}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The conclusion follows from the basic inequality $a^{2}\\!+\\!b^{2}\\leq(a\\!+\\!b)^{2}$ which holds for any $a,b\\ge0$ \uff1a\u53e3 ", "page_idx": 22}, {"type": "text", "text": "Lemma C.10 (Nonexpansiveness of the EG operator). Let $\\pmb{F}$ be a monotone $L$ -Lipschitz operator, and $z^{*}$ be a point such that $\\pmb{F}\\pmb{z}^{*}=\\mathbf{0}$ Then,for any point $_{z}$ in the domain of $\\pmb{F}$ and $\\eta>0$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|z-\\eta{\\cal F}(z-\\eta{\\cal F}z)-z^{*}\\right\\|^{2}\\leq\\left\\|z-z^{*}\\right\\|^{2}-\\eta^{2}(1-\\eta^{2}L^{2})\\left\\|{\\cal F}z\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. This classical result dates back to the original paper on EG by Korpelevich [29]. Here, for completeness, we replicate the proof using our notations. ", "page_idx": 22}, {"type": "text", "text": "Expanding the left hand side of the inequality stated, we obtain ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|z-\\eta F(z-\\eta F z)-z^{*}\\|^{2}=\\|z-z^{*}\\|^{2}-2\\left\\langle\\eta F(z-\\eta F z),z-z^{*}\\right\\rangle+\\|\\eta F(z-\\eta F z)\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\|z-z^{*}\\|^{2}-2\\eta\\left\\langle F(z-\\eta F z),z-\\eta F z-z^{*}\\right\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad-\\ 2\\eta^{2}\\left\\langle F(z-\\eta F z),F z\\right\\rangle+\\eta^{2}\\left\\|F(z-\\eta F z)\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For the first inner product term, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n-2\\eta\\left\\langle\\pmb{F}(\\pmb{z}-\\eta\\pmb{F}\\pmb{z}),\\pmb{z}-\\eta\\pmb{F}\\pmb{z}-\\pmb{z}^{*}\\right\\rangle\\leq0\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "because $\\pmb{F}$ is monotone. For the second inner product term, we use the polarization identity (Lemma C.1) and the $L$ -Lipschitzness of $\\pmb{F}$ to get ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-2\\left\\langle F(z-\\eta F z),F z\\right\\rangle=\\left\\|F(z-\\eta F z)-F z\\right\\|^{2}-\\left\\|F(z-\\eta F z)\\right\\|^{2}-\\left\\|F z\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq L^{2}\\left\\|-\\eta F z\\right\\|^{2}-\\left\\|F(z-\\eta F z)\\right\\|^{2}-\\left\\|F z\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=-(1-\\eta^{2}L^{2})\\left\\|F z\\right\\|^{2}-\\left\\|F(z-\\eta F z)\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Applying these two bounds on (23) completes the proof. ", "page_idx": 22}, {"type": "text", "text": "Lemma C.11. Let $\\{a_{k}\\}_{k\\geq0},\\,\\{b_{k}\\}_{k\\geq0},\\,\\{c_{k}\\}_{k\\geq0}$ and $\\{d_{k}\\}_{k\\ge0}$ be sequences of nonnegative numbers satisfyingtherecurrencerelation ", "page_idx": 22}, {"type": "equation", "text": "$$\nb_{k}\\leq(1+a_{k})d_{k}-d_{k+1}+c_{k}\\qquad\\forall k\\geq0.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then for any $k\\geq0$ it holds that ", "page_idx": 22}, {"type": "equation", "text": "$$\nd_{k+1}+\\sum_{j=0}^{k}b_{j}\\leq\\left(\\prod_{j=0}^{k}(1+a_{j})\\right)\\left(d_{0}+\\sum_{j=0}^{k}c_{j}\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. Because $a_{k}\\geq0$ , it suffices to show that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{j=0}^{k}(b_{j}-c_{j})\\prod_{i=j+1}^{k}(1+a_{i})\\leq-d_{k+1}+d_{0}\\prod_{j=0}^{k}(1+a_{j}),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "as this implies ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{j=0}^{k}b_{j}\\leq\\sum_{j=0}^{k}b_{j}\\prod_{i=j+1}^{k}\\left(1+a_{i}\\right)}}\\\\ &{\\leq\\left(\\sum_{j=0}^{k}c_{j}\\prod_{i=j+1}^{k}\\left(1+a_{i}\\right)\\right)-d_{k+1}+d_{0}\\prod_{j=0}^{k}(1+a_{j})}\\\\ &{\\leq-d_{k+1}+\\left(d_{0}+\\displaystyle\\sum_{j=0}^{k}c_{k}\\right)\\prod_{j=0}^{k}(1+a_{j}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "So, we show that (24) holds, by induction on $k$ . For the base case $k=0$ , the recurrence relation tells us that ", "page_idx": 23}, {"type": "equation", "text": "$$\nb_{0}-c_{0}\\leq(1+a_{0})d_{0}-d_{1}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which is exactly (24) when $k=0$ . Now suppose that (24) holds for some $k\\geq0$ . Using the induction hypothesis and the recurrence relation we get ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{j=0}^{k+1}(b_{j}-c_{j})\\prod_{i=j+1}^{k+1}\\left(1+a_{i}\\right)=b_{k+1}-c_{k+1}+(1+a_{k+1})\\left(\\displaystyle\\sum_{j=0}^{k}(b_{j}-c_{j})\\displaystyle\\prod_{i=j+1}^{k}\\left(1+a_{i}\\right)\\right)}&{}\\\\ {\\displaystyle\\leq b_{k+1}-c_{k+1}-(1+a_{k+1})d_{k+1}+d_{0}\\displaystyle\\prod_{j=0}^{k+1}(1+a_{j})}&{}\\\\ {\\displaystyle\\leq-d_{k+2}+d_{0}\\displaystyle\\prod_{j=0}^{k+1}(1+a_{j}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This shows that (24) holds also for $k+1$ , and we are done. ", "page_idx": 23}, {"type": "text", "text": "The subsequent lemma is technical, but it can be derived from elementary calculus. ", "page_idx": 23}, {"type": "text", "text": "Lemma C.12. For any $K\\geq1$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{k=2}^{K+2}{\\frac{1}{k^{2/3}(\\log k)^{2}}}\\geq{\\frac{(K+3)^{1/3}}{(\\log(K+3))^{2}}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. Consider th function $\\begin{array}{r}{h(x):=\\frac{1}{x^{2/3}(\\log x)^{2}}}\\end{array}$ over the interal $[2,K+3]$ As ", "page_idx": 23}, {"type": "equation", "text": "$$\nh^{\\prime}(x)=-\\frac{2}{x^{5/3}(\\log x)^{3}}-\\frac{2}{3x^{5/3}(\\log x)^{2}}<0,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "$h$ is decreasing. Hence, an upper Riemann sum becomes an upper bound for the integral, so we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{k=2}^{K+2}{\\frac{1}{k^{2/3}(\\log k)^{2}}}\\geq\\int_{2}^{K+3}{\\frac{1}{x^{2/3}(\\log x)^{2}}}\\,\\mathrm{d}x.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Now consider a function $g:[1,\\infty)\\to{\\mathbb R}$ , defined as ", "page_idx": 23}, {"type": "equation", "text": "$$\ng(y):=\\int_{2}^{y+3}{\\frac{1}{x^{2/3}(\\log x)^{2}}}\\,\\mathrm{d}x-{\\frac{(y+3)^{1/3}}{(\\log(y+3))^{2}}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Differentiating, we get ", "page_idx": 23}, {"type": "equation", "text": "$$\ng^{\\prime}(y)=\\frac{2}{(y+3)^{2/3}(\\log(y+3))^{3}}+\\frac{2}{3(y+3)^{2/3}(\\log(y+3))^{2}}>0\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "whenever $y\\geq1$ . That is, $g$ is increasing on $y\\geq1$ .We then show that $g(1)\\geq0$ . To this end, let us begin with observing that ", "page_idx": 24}, {"type": "equation", "text": "$$\nh^{\\prime\\prime}(x)={\\frac{6}{x^{8/3}(\\log x)^{4}}}+{\\frac{14}{3x^{8/3}(\\log x)^{3}}}+{\\frac{10}{9x^{8/3}(\\log x)^{2}}}>0,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "from which we get that $h$ is convex. In particular, it holds that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\int_{2}^{3}h(x)\\,\\mathrm{d}x\\geq\\int_{2}^{3}h^{\\prime}\\left({\\frac{5}{2}}\\right)\\left(x-{\\frac{5}{2}}\\right)+h\\left({\\frac{5}{2}}\\right)\\,\\mathrm{d}x=h\\left({\\frac{5}{2}}\\right),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and similarly, $\\begin{array}{r}{\\int_{3}^{4}h(x)\\,\\mathrm{d}x\\geq h(7/2)}\\end{array}$ . Thus we indeed have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{g(1)=\\int_{2}^{4}h(x)\\,\\mathrm{d}x-\\frac{4^{1/3}}{(\\log4)^{2}}}}\\\\ &{}&{\\geq\\frac{1}{(5/2)^{2/3}(\\log(5/2))^{2}}+\\frac{1}{(7/2)^{2/3}(\\log(7/2))^{2}}-\\frac{4^{1/3}}{(\\log4)^{2}}\\,\\geq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Recalling that $g$ is increasing, we have $g(K)\\ge g(1)\\ge0$ for all $K\\geq1$ . This, with (25), implies that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{k=2}^{K+2}{\\frac{1}{k^{2/3}(\\log k)^{2}}}\\geq\\int_{2}^{K+3}{\\frac{1}{x^{2/3}(\\log x)^{2}}}\\,\\mathrm{d}x\\geq{\\frac{(K+3)^{1/3}}{(\\log(K+3))^{2}}}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "holds whenever $K\\geq1$ , which is exactly the claimed. ", "page_idx": 24}, {"type": "text", "text": "D Missing Proofs for Section 5 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "D.1Unravelling the Recurrence of the Generalized SEG in (8) and (12) ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In Section 5.1, we considered the method where, in a single epoch (hence omitting all superscripts that are used to denote the epoch number for convenience), the iterates are generated following the recurrence ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{w}_{i}=\\pmb{z}_{i}-\\alpha T_{i}\\pmb{z}_{i}}\\\\ {\\pmb{z}_{i+1}=\\pmb{z}_{i}-\\beta T_{i}\\pmb{w}_{i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for $i=0,1,...,N-1$ , where each $\\mathbf{\\boldsymbol{T}}_{i}$ are sampled from the set $\\{F_{1},\\ldots,F_{n}\\}$ , and an additional anchoring step ", "page_idx": 24}, {"type": "equation", "text": "$$\nz^{\\sharp}:=\\frac{z_{N}+\\theta z_{0}}{1+\\theta}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "is performed so that $z^{\\sharp}$ is used as the initial point of the next epoch. Notice that (27) is a generalized anchoring step that incorporates all the settings we are considering, as the versions of SEG where anchoring is not used correspond to taking $\\theta=0$ , and the anchoring step (12) that is used in SEGFFA corresponds to taking $\\theta=1$ . In this section we would like to prove the following statement regarding this update rule. ", "page_idx": 24}, {"type": "text", "text": "Proposition D.1 (Proposition 5.1). It holds that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathfrak{L}^{\\sharp}=z_{0}-\\frac{\\beta}{1+\\theta}\\sum_{j=0}^{N-1}T_{j}z_{0}+\\frac{\\alpha\\beta}{1+\\theta}\\sum_{j=0}^{N-1}D T_{j}(z_{0})T_{j}z_{0}+\\frac{\\beta^{2}}{1+\\theta}\\sum_{0\\le i<j\\le N-1}D T_{j}(z_{0})T_{i}z_{0}+\\frac{\\epsilon_{N}}{1+\\theta}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. Equation (28) immediately follows from Proposition D.2, with (30) giving us the precise definition of $\\epsilon_{N}$ . To show that $\\epsilon_{N}=o\\left((\\alpha+\\beta)^{2}\\right)$ , we begin with noting that both $\\|z_{j}-z_{0}\\|$ and $\\|\\pmb{w}_{j}-\\pmb{z}_{0}\\|$ are of $O(\\alpha+\\beta)$ , because both $z_{j}$ and ${\\pmb w}_{j}$ are obtained from $z_{0}$ by performing at most $j$ updates following (26). Thus, the first term in the right hand side of (30) is of $\\mathcal{O}(\\beta(\\alpha+\\beta)^{2})$ by Lemma C.6, and the remaining terms are of $O((\\alpha+\\bar{\\beta})^{3})$ by the $L$ -smoothness of the operators $F_{1},\\ldots,F_{n}$ \u53e3 ", "page_idx": 24}, {"type": "text", "text": "Proposition D.2. For any $i=0,1,\\dots,N,$ it holds that ", "page_idx": 25}, {"type": "equation", "text": "$$\nz_{i}=z_{0}-\\beta\\sum_{j=0}^{i-1}T_{j}z_{0}+\\alpha\\beta\\sum_{j=0}^{i-1}D T_{j}(z_{0})T_{j}z_{0}+\\beta^{2}\\sum_{0\\leq k<j\\leq i-1}D T_{j}(z_{0})T_{k}z_{0}+\\epsilon_{i}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "wherewedenote ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\epsilon_{i}:=-\\,\\beta\\sum_{j=0}^{i-1}\\biggl(T_{j}w_{j}-T_{j}z_{0}-D T_{j}(z_{0})(w_{j}-z_{0})\\biggr)}\\\\ {\\displaystyle\\qquad+\\,\\alpha\\beta\\sum_{j=0}^{i-1}D T_{j}(z_{0})(T_{j}z_{j}-T_{j}z_{0})+\\beta^{2}\\sum_{j=0}^{i-1}D T_{j}(z_{0})\\sum_{k=0}^{j-1}(T_{k}w_{k}-T_{k}z_{0}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. We use induction on $i$ . There is nothing to show for the base case $i=0$ . Now, suppose that (29) and (30) hold for some $i<N$ , and write ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{z_{i+1}=z_{i}-\\beta T_{i}w_{i}}\\\\ {=z_{i}-\\beta T_{i}z_{0}-\\beta D T_{i}(z_{0})(w_{i}-z_{0})-\\beta\\Big(T_{i}w_{i}-T_{i}z_{0}-D T_{i}(z_{0})(w_{i}-z_{0})\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Here, notice that by the update rule we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{w}_{i}=\\pmb{z}_{i}-\\alpha\\pmb{T}_{i}\\pmb{z}_{i}}\\\\ &{\\quad=\\pmb{z}_{0}-\\beta\\displaystyle\\sum_{j=0}^{i-1}\\pmb{T}_{j}\\pmb{w}_{j}-\\alpha\\pmb{T}_{i}\\pmb{z}_{i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Using this identity and the induction hypothesis we get ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\eta_{1}}&{=\\eta_{2}-\\beta\\sum_{i=1}^{n}\\beta_{i,j}\\sum_{\\alpha,\\beta=0}^{\\frac{1}{n}}\\beta_{i,\\alpha,\\beta}\\frac{\\gamma_{1,\\alpha,\\beta}}{\\gamma_{2,\\beta}}\\Bigg[\\beta_{i,\\alpha}\\frac{\\gamma_{2,\\beta}}{\\gamma_{3,\\beta}}+\\beta_{i,\\alpha,\\beta}^{\\alpha}\\frac{\\gamma_{3,\\gamma}}{\\gamma_{4,\\beta,\\alpha}}\\frac{D T_{1}\\big(\\beta_{1,\\alpha}\\gamma_{3,\\beta}+\\beta_{2,\\beta}^{\\prime}\\big)}{\\Gamma_{2,\\beta}^{\\alpha}}}\\\\ &{\\quad-\\beta(T_{1,\\alpha}-\\beta T_{1,\\alpha})\\bigg(\\gamma_{3,\\beta}-\\frac{\\gamma_{1,\\beta}}{\\gamma_{2,\\beta}}\\gamma_{4,\\alpha,\\beta}-\\alpha T_{1,\\alpha}\\bigg)}\\\\ &{\\quad-\\beta\\bigg(T_{1,\\alpha}-T_{1,\\beta}-D T_{1,\\alpha}\\bigg)(\\alpha_{1,\\alpha}\\gamma_{2,\\beta}-\\alpha_{1,\\beta}^{\\prime})}\\\\ &{=\\eta_{2}-\\beta\\sum_{i,j=1}^{n}\\beta_{i,j}+\\alpha_{2,j}^{\\sqrt{3}}\\frac{D T_{1}}{\\gamma_{2,\\beta}}\\gamma_{4,\\alpha}\\beta\\frac{\\gamma_{2,\\beta}}{\\gamma_{3,\\beta}}\\Bigg[\\beta_{1,\\alpha}\\frac{\\gamma_{1,\\alpha}}{\\gamma_{2,\\beta}}\\Bigg]\\Bigg[\\beta_{2,\\beta}\\frac{\\gamma_{2,\\beta}}{\\gamma_{3,\\beta}}+\\beta_{1,\\alpha,\\beta}^{\\alpha}\\frac{\\gamma_{1,\\alpha}}{\\gamma_{3,\\beta}}\\frac{D T_{2,\\beta}}{\\gamma_{1,\\alpha}}\\Bigg]\\Bigg[\\beta_{1,\\alpha}\\frac{\\gamma_{1,\\beta}}{\\gamma_{3,\\beta}}+\\beta_{2,\\alpha,\\beta}^{\\alpha}\\Bigg]}\\\\ &{\\quad-\\beta(T_{1,\\alpha}+\\beta)\\mathbb{I}_{(2,\\beta)}\\frac{\\gamma_{1,\\alpha}}{\\gamma_{3,\\beta}}\\gamma_{4,\\alpha}\\beta\\frac{\\gamma_{2,\\beta}}{\\gamma_{3,\\beta}}\\Bigg[\\beta_{1,\\alpha}\\frac{\\gamma_{1,\\beta}}{\\gamma_{2,\\beta}}\\Bigg \n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which asserts that (29) also holds for $i+1$ ", "page_idx": 25}, {"type": "text", "text": "D.2  Insufficiency of Only Using Flip-Flop Sampling ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Here we prove the following. ", "page_idx": 26}, {"type": "text", "text": "Proposition D.3 (Proposition 5.2). Suppose we use fip-flop sampling only. In order to make (10) and (11) hold, we must choose $\\beta=\\eta_{1}/n$ and $\\alpha={\\beta}/{2}$ . However, this leads to $\\eta_{2}=2\\eta_{1}$ ,which is the set of parameters that fails to make $E G+$ converge. ", "page_idx": 26}, {"type": "text", "text": "Proof. Suppose that we have already established the upcoming Lemma D.4. Then, we can see by seting $\\theta=0$ in the result of Lemma D.4 that for (11) to hold, the following system of equations should besatisfied: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\eta_{1}\\eta_{2}=2n^{2}\\beta^{2},}\\\\ {\\eta_{1}\\eta_{2}=n^{2}(2\\alpha\\beta+\\beta^{2}),}\\\\ {\\eta_{2}=2n\\beta.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Solving this system of equations, we get $\\eta_{1}=n\\beta$ $\\eta_{2}=2n\\beta$ , and $\\alpha={}^{\\beta}/2$ ", "page_idx": 26}, {"type": "text", "text": "For the latter part of the statement on the divergence of $\\mathrm{EG+}$ with $\\eta_{2}=2\\eta_{1}$ , consider the $(1+1)$ dimensional bilinear problem ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x}\\operatorname*{max}_{y}\\,x y\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "whose unique optimum is $\\boldsymbol{z}^{*}=(\\boldsymbol{0},\\boldsymbol{0})$ . A simple computation shows that ", "page_idx": 26}, {"type": "equation", "text": "$$\nF z=\\left[{\\underset{-1}{0}}\\quad1\\right]z.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Consequently, for any $\\eta>0$ , the update rule of $\\mathrm{EG+}$ with $\\eta_{1}=\\eta$ and $\\eta_{2}=2\\eta$ amounts to ", "page_idx": 26}, {"type": "equation", "text": "$$\nz^{+}=z-2\\eta{\\cal F}(z-\\eta{\\cal F}z)=\\left[\\begin{array}{c c}{{1-2\\eta^{2}}}&{{-2\\eta}}\\\\ {{2\\eta}}&{{1-2\\eta^{2}}}\\end{array}\\right]z.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "It follows that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\Vert z^{+}-z^{*}\\right\\Vert^{2}=\\left\\Vert\\left[\\begin{array}{l l}{1-2\\eta^{2}}&{-2\\eta}\\\\ {2\\eta}&{1-2\\eta^{2}}\\end{array}\\right]\\left[\\boldsymbol{y}\\right]\\right\\Vert^{2}}\\\\ &{\\qquad\\qquad=\\left((1-2\\eta^{2})x-2\\eta\\boldsymbol{y}\\right)^{2}+\\left(2\\eta x+(1-2\\eta^{2})y\\right)^{2}}\\\\ &{\\qquad\\qquad=(1+4\\eta^{4})(x^{2}+y^{2})}\\\\ &{\\qquad\\qquad=(1+4\\eta^{4})\\left\\Vert z-z^{*}\\right\\Vert^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Therefore, the distance from the optimal solution strictly increases every iterate. ", "page_idx": 26}, {"type": "text", "text": "It remains to actually prove Lemma D.4. ", "page_idx": 26}, {"type": "text", "text": "Lemma D.4. When flip-flop sampling is used with the generalized anchoring step (27), it holds that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\alpha\\beta}{1+\\theta}\\sum_{j=0}^{N-1}D\\pmb{T}_{j}(z_{0})\\pmb{T}_{j}z_{0}+\\frac{\\beta^{2}}{1+\\theta}\\sum_{0\\leq i<j\\leq N-1}D\\pmb{T}_{j}(z_{0})\\pmb{T}_{i}z_{0}}\\\\ &{\\displaystyle\\qquad=\\frac{2\\alpha\\beta+\\beta^{2}}{1+\\theta}\\sum_{j=1}^{n}D\\pmb{F}_{j}(z_{0})\\pmb{F}_{j}z_{0}+\\frac{2\\beta^{2}}{1+\\theta}\\sum_{i\\neq j}D\\pmb{F}_{j}(z_{0})\\pmb{F}_{i}z_{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. As we are using flip-flop sampling, we have $N=2n$ , and it is clear that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{j=0}^{N-1}D T_{j}(z_{0})T_{j}z_{0}=2\\sum_{j=1}^{n}D F_{j}(z_{0})F_{j}z_{0}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "For the second term, as $T_{i}=T_{2n-1-i}$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{0\\le i<j\\le2n-1}D T_{j}(z_{0})T_{i}z_{0}=\\sum_{0\\le i<j\\le n-1}D T_{j}(z_{0})T_{i}z_{0}+\\sum_{n\\le i<j\\le2n-1}D T_{j}(z_{0})T_{i}z_{0}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad_{i=1}^{-1/2-\\delta-2}-D T_{2}(z_{i}))T_{[2,0]}+\\displaystyle\\sum_{i=0}^{-1}\\sum_{j=-i}^{i-1}D T_{2}(z_{i})T_{[2,0]}}\\\\ &{\\quad+\\displaystyle\\sum_{i=0}^{-1}D T_{2-i-1,i}(z_{i}))T_{[2,0]}}\\\\ &{\\quad+\\displaystyle\\sum_{i=0}^{-1}D T_{2-i-1,i}(z_{i}))T_{[2,0]}}\\\\ &{\\quad=\\displaystyle\\sum_{\\mathrm{t\\in\\{GS\\}}}D T_{2,1}(z_{i})T_{[2,0]}}\\\\ &{\\quad=\\displaystyle\\sum_{\\mathrm{t\\in\\{GS\\}}}D T_{[2,0]}\\mathbb{E}_{[2,0]}\\mathbb{E}_{[0,0]}\\cdot\\displaystyle\\sum_{\\mathrm{t\\in\\{GS\\}}-1\\atop\\mathrm{t\\in\\{GS\\}}}D T_{2}(z_{i})T_{[2,0]}}\\\\ &{\\quad\\quad+\\displaystyle\\sum_{i=0}^{-1}\\displaystyle\\sum_{j=1}^{i-1}D T_{2}(z_{i})T_{[2,0]}\\mathbb{E}_{[0,i]}\\cdot\\displaystyle\\sum_{\\mathrm{t\\in\\{GS\\}}-1\\atop\\mathrm{t\\in\\{GS\\}}}D T_{2}(z_{i})T_{[2,0]}}\\\\ &{\\quad\\quad+\\displaystyle\\sum_{i=0}^{-1}D T_{2}(z_{i})\\mathbb{E}_{[0,i]}}\\\\ &{\\quad=\\displaystyle\\sum_{\\mathrm{t\\in\\{GS\\}}+1\\atop\\mathrm{t\\in\\{GS\\}}}D T_{[2,0]}\\mathbb{E}_{[0,0]}+2\\sum_{\\mathrm{t\\in\\{GS\\}}-1\\atop\\mathrm{t\\in\\{GS\\}}}D T_{2}(z_{i})T_{[2,0]}}\\\\ &{\\quad=\\displaystyle\\sum_{\\mathrm{t\\in\\{GS\\}}+1\\atop\\mathrm{t\\in\\{GS\\}}}D T_{[2,0]}T_{[2,0]}+2\\sum_{\\mathrm{t\\in\\{GS\\}}+1\\atop\\mathrm{t\\in\\{GS\\}}}D T_{2}(z_{i})T_{[2,0]}}\\\\ &{\\quad\\quad+\\displaystyle\\sum_{j=1}^{-1}D T_{2}(z_{i})T_{[2,0]},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The claimed identity can be obtained by taking the weighted sum of the two results ", "page_idx": 27}, {"type": "text", "text": "E Within-Epoch Error Analysis for Upper Bounds ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Allthe upper bounds for SEG-RR, SEG-FF, and SEG-FFA in this paper are established by following the two steps below. ", "page_idx": 27}, {"type": "text", "text": "The first step is to decompose the cumulative updates made within an epoch by using the method into a sum of an exact EG update and a within-epoch error term, which we denote by $\\bar{\\mathbf{\\Lambda}}^{k}$ . In particular, we show that the error term $r^{k}$ Occurring from any of SEG-RR, SEG-FF, and SEG-FFA can be expressed in a specific unified form (described in Theorem E.1). This will be the main focus of this section. ", "page_idx": 27}, {"type": "text", "text": "The second step is establishing a convergence rate that can be applied to any method whose update can be decomposed into a sum of an exact EG update and an error term that is of the specific unified form mentioned above. By doing so, the convergence rates of SEG-RR, SEG-FF, and SEG-FFA will automatically follow as special cases of the general convergence result. This step will be dealt in Appendices F and G. ", "page_idx": 27}, {"type": "text", "text": "To this end, for any of SEG-RR, SEG-FF, and SEG-FFA, let us decompose the cumulative updates made within an epoch into a sum of an exact EG update and a within-epoch error term $r^{k}$ ,as ", "page_idx": 27}, {"type": "equation", "text": "$$\nz_{0}^{k+1}=z_{0}^{k}-\\eta_{k}n\\mathbf{F}(z_{0}^{k}-\\eta_{k}n\\mathbf{F}z_{0}^{k})+r^{k}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The quality of the method will depend on how small the \u201cnoise\u201d term $r^{k}$ is, as the noise will in general hinder the convergence. As mentioned above, it turns out that, regardless of the method that is in use, the noise term can be bounded in a unified format, as follows. ", "page_idx": 27}, {"type": "text", "text": "Theorem E.1. Suppose that Assumptions 3.3 and 3.4 hold. Then, for each of SEG-RR, SEG-FF, and SEG-FFA, there exists a choice of stepsizes that makes the following hold: for an exponent $a$ that depends on themethod,there exist constants $C_{1},\\,D_{1},\\,V_{1},\\,C_{2},\\,D_{2}$ and $V_{2},$ all independent of $\\eta_{k}$ and $n$ such that the error term $r^{k}$ satisfies a deterministic bound ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left\\|r^{k}\\right\\|\\leq\\eta_{k}^{a}n^{a}C_{\\hbar}\\left\\|F z_{0}^{k}\\right\\|+\\eta_{k}^{a}n^{a}D_{\\hbar}\\left\\|F z_{0}^{k}\\right\\|^{2}+\\eta_{k}^{a}n^{a}V\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and a bound that holds on expectation ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert r\\right\\Vert^{2}\\middle|z_{0}^{k}\\right]\\leq\\eta_{k}^{2a}n^{2a}C_{2}\\left\\Vert F z_{0}^{k}\\right\\Vert^{2}+\\eta_{k}^{2a}n^{2a}D_{2}\\left\\Vert F z_{0}^{k}\\right\\Vert^{4}+\\eta_{k}^{2a}n^{2a-1}V_{2}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Furthermore,the exponent is $a=2$ forSEG-RRandSEG- $F F,$ and $a=3$ forSEG-FFA In other words, SEG-FFA has an error that is an order of magnitude smaller than other methods. Thus, it is now intuitively clear that SEG-FFA should have an advantage in the convergence. The proof of Theorem E.1 is quite long and technical, so we defer it to Appendix E.2. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "Within the remaining of this section only, although it is an abuse of notation, for convenience we will write $F_{i}$ to denote the saddle gradient of the component function chosen in the $i^{\\mathrm{th}}$ iteration. More precisely, for indices $i=0,1,\\dotsc,n-1$ we denote $F_{\\tau(i+1)}$ by $F_{i}$ . Similarly, in cases of considering SEG-FF or SEG-FFA, for $i\\geq n$ we denote $F_{\\tau(2n-i)}$ by $F_{i}$ . Also, we omit the superscripts and subscripts denoting the epoch number $k$ unless strictly necessary, as all the iterates that we consider will be from the same epoch. ", "page_idx": 28}, {"type": "text", "text": "Let us reformulate the update rule (8) into ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{\\pmb w_{i}=z_{i}-\\xi\\eta{\\bf F}_{i}z_{i},}}\\\\ {{z_{i+1}=z_{i}-\\eta{\\bf F}_{i}{\\pmb w}_{i}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Note that $\\xi={^1\\!/2}$ for SEG-FFA, and $\\xi=1$ for SEG-RR and SEG-FF. ", "page_idx": 28}, {"type": "text", "text": "E.1 Auxiliary Lemmata ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "For $j=1,\\ldots,2n$ we define ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g_{j}:=\\displaystyle\\sum_{i=0}^{j-1}F_{i}z_{0},}\\\\ &{\\delta_{j}:=\\left\\lVert g_{j}-j F z_{0}\\right\\rVert,}\\\\ &{\\Sigma_{j}:=\\displaystyle\\sum_{i=1}^{j}\\delta_{i},}\\\\ &{\\Psi_{j}:=\\displaystyle\\sum_{i=1}^{j}\\delta_{i}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We set $\\Sigma_{0}=\\Psi_{0}=0$ , as they are empty sums. Notice that $\\delta_{j}$ is a random variable that depends on the permutation $\\tau$ ", "page_idx": 28}, {"type": "text", "text": "Meanwhile, by triangle inequality it is immediate that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|g_{j}\\|\\leq j\\,\\|F z_{0}\\|+\\delta_{j},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and by Young's inequality it holds that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|\\pmb{g}_{j}\\|^{2}\\leq2j^{2}\\left\\|\\pmb{F}\\pmb{z}_{0}\\right\\|^{2}+2\\delta_{j}^{2}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Lemma E.2. For any index $i\\geq1$ ,it holds that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|z_{i}-z_{0}\\right\\|\\leq\\eta\\left(1+\\xi\\eta L\\right)\\left\\|g_{i}\\right\\|}\\\\ &{\\qquad\\qquad\\qquad+\\eta^{2}L\\left(2\\xi+2\\xi\\eta L+\\xi^{2}\\eta^{2}L^{2}\\right)\\displaystyle\\sum_{\\ell=0}^{i-2}\\left(1+\\eta L+\\xi\\eta^{2}L^{2}\\right)^{i-\\ell-2}\\left\\|g_{\\ell+1}\\right\\|,}\\\\ &{\\left\\|w_{i}-z_{0}\\right\\|\\leq\\xi\\eta\\left\\|g_{i+1}\\right\\|+\\xi\\eta\\left((1-\\xi^{-1})+2\\eta L+\\xi\\eta^{2}L^{2}\\right)\\left\\|g_{i}\\right\\|}\\\\ &{\\qquad\\qquad\\qquad+\\left.\\eta(1+\\xi\\eta L)\\left(2\\xi\\eta L+2\\xi\\eta^{2}L^{2}+\\xi^{2}\\eta^{3}L^{3}\\right)\\displaystyle\\sum_{\\ell=0}^{i-2}\\left(1+\\eta L+\\xi\\eta^{2}L^{2}\\right)^{i-\\ell-2}\\left\\|g_{\\ell+1}\\right\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. By the fundamental theorem of calculus for line integrals and the update rule (33), we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle w_{i}=z_{i}-\\xi\\eta F_{i}z_{i}}}\\\\ {{\\displaystyle\\qquad=z_{i}-\\xi\\eta F_{i}z_{0}-\\xi\\eta(F_{i}z_{i}-F_{i}z_{0})}}\\\\ {{\\displaystyle\\qquad=z_{i}-\\xi\\eta F_{i}z_{0}-\\xi\\eta\\int_{0}^{1}D F_{i}(z_{0}+t(z_{i}-z_{0}))\\,\\mathrm{d}t\\left(z_{i}-z_{0}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and similarly ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{z_{i+1}=z_{i}-\\eta F_{i}\\pmb{w}_{i}}}\\\\ &{=z_{i}-\\eta F_{i}z_{0}-\\eta(F_{i}\\pmb{w}_{i}-\\pmb{F}_{i}z_{0})}\\\\ &{=z_{i}-\\eta F_{i}z_{0}-\\eta\\int_{0}^{1}D F_{i}(z_{0}+t(\\pmb{w}_{i}-z_{0}))\\,\\mathrm{d}t\\,(\\pmb{w}_{i}-z_{0}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Hence, by defining ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{\\pmb A_{i}:=\\int_{0}^{1}{\\cal D}F_{i}(z_{0}+t(z_{i}-z_{0}))\\,\\mathrm{d}t}}\\\\ {\\displaystyle{\\pmb B_{i}:=\\int_{0}^{1}{\\cal D}F_{i}(z_{0}+t(w_{i}-z_{0}))\\,\\mathrm{d}t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "the update rule can be rewritten using these quantities as ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{w}_{i}=z_{i}-\\xi\\eta\\pmb{F}_{i}z_{0}-\\xi\\eta\\pmb{A}_{i}(z_{i}-z_{0}),}\\\\ {z_{i+1}=z_{i}-\\eta\\pmb{F}_{i}z_{0}-\\eta\\pmb{B}_{i}(\\pmb{w}_{i}-z_{0}).\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Subtracting $z_{\\mathrm{0}}$ from both sides of (41) we get ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{w}_{i}-\\pmb{z}_{0}=\\pmb{z}_{i}-\\pmb{z}_{0}-\\xi\\eta\\pmb{F}_{i}\\pmb{z}_{0}-\\xi\\eta\\pmb{A}_{i}(\\pmb{z}_{i}-\\pmb{z}_{0})}\\\\ {=\\left(\\pmb{I}-\\xi\\eta\\pmb{A}_{i}\\right)(\\pmb{z}_{i}-\\pmb{z}_{0})-\\xi\\eta\\pmb{F}_{i}\\pmb{z}_{0},\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and plugging this into (42) gives us ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{z_{i+1}-z_{0}=z_{i}-z_{0}-\\eta F_{i}z_{0}-\\eta B_{i}\\big(w_{i}-z_{0}\\big)}\\\\ &{\\qquad\\qquad=z_{i}-z_{0}-\\eta F_{i}z_{0}-\\eta B_{i}\\left(\\left(I-\\xi\\eta A_{i}\\right)\\left(z_{i}-z_{0}\\right)-\\xi\\eta F_{i}z_{0}\\right)}\\\\ &{\\qquad\\qquad=\\left(I-\\eta B_{i}+\\xi\\eta^{2}B_{i}A_{i}\\right)\\left(z_{i}-z_{0}\\right)-\\eta\\left(I-\\xi\\eta B_{i}\\right)F_{i}z_{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "For convenience let us define ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{C_{i}:=\\boldsymbol{I}-\\eta\\boldsymbol{B}_{i}+\\xi\\eta^{2}\\boldsymbol{B}_{i}\\boldsymbol{A}_{i},}\\\\ {P_{i,\\ell}:=C_{i}C_{i-1}\\ldots\\boldsymbol{C}_{\\ell+2}C_{\\ell+1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and $P_{i,i}:=I$ as it denotes an empty product. Observe that for any $j$ wehave ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left\\|{C_{j}}\\right\\|=\\left\\|{I-\\eta\\pmb{B}_{i}+\\xi\\eta^{2}\\pmb{B}_{i}\\pmb{A}_{i}}\\right\\|\\le1+\\eta L+\\xi\\eta^{2}L^{2}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Also note that for any $\\ell$ it holds that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(I-\\xi\\eta{\\cal B}_{\\ell+1})-C_{\\ell+1}\\left(I-\\xi\\eta{\\cal B}_{\\ell}\\right)}\\\\ &{\\qquad=(I-\\xi\\eta{\\cal B}_{\\ell+1})-\\left(I-\\eta{\\cal B}_{\\ell+1}+\\xi\\eta^{2}{\\cal B}_{\\ell+1}{\\cal A}_{\\ell+1}\\right)(I-\\xi\\eta{\\cal B}_{\\ell})}\\\\ &{\\qquad=\\xi\\eta({\\cal B}_{\\ell+1}+{\\cal B}_{\\ell})-\\xi\\eta^{2}{\\cal B}_{\\ell+1}({\\cal A}_{\\ell+1}+{\\cal B}_{\\ell})+\\xi^{2}\\eta^{3}{\\cal B}_{\\ell+1}{\\cal A}_{\\ell+1}{\\cal B}_{\\ell}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and hence ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|(I-\\xi\\eta{\\pmb B}_{\\ell+1})-C_{\\ell+1}\\,(I-\\xi\\eta{\\pmb B}_{\\ell})\\|\\le2\\xi\\eta L+2\\xi\\eta^{2}L^{2}+\\xi^{2}\\eta^{3}L^{3}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Unravelling the recurrence relation (44) we get ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Sigma_{i+1}-z_{0}=C_{i}(z_{i}-z_{0})-\\eta\\left(I-\\xi_{1}\\eta B_{i}\\right)F_{i}z_{0}}\\\\ &{\\quad=C_{i}\\left(C_{i-1}(z_{i-1}-z_{0})-\\eta\\left(I-\\xi_{1}\\eta B_{i-1}\\right)F_{i-1}z_{0}\\right)-\\eta\\left(I-\\xi_{1}\\eta B_{i}\\right)F_{i}z_{0}}\\\\ &{\\quad=R_{i,i-2}(z_{i-1}-z_{0})-\\eta\\displaystyle\\sum_{\\ell=i-1}^{i}P_{i,\\ell}\\left(I-\\xi_{1}\\eta B_{\\ell}\\right)F_{\\ell}z_{0}}\\\\ &{\\quad=R_{i,i-2}(C_{i-2}(z_{i-2}-z_{0})-\\eta(I-\\xi_{1}\\eta B_{i-2})F_{i-2}z_{0})-\\eta\\displaystyle\\sum_{\\ell=i-1}^{i}P_{i,\\ell}\\left(I-\\xi_{1}\\eta B_{\\ell}\\right)F_{\\ell}}\\\\ &{\\quad=R_{i,i-3}(z_{i-2}-z_{0})-\\eta\\displaystyle\\sum_{\\ell=i-2}^{i}P_{i,\\ell}\\left(I-\\xi_{1}\\eta B_{\\ell}\\right)F_{\\ell}z_{0}}\\\\ &{\\quad\\vdots}\\\\ &{\\quad=P_{i-1}(z_{0}-z_{0})-\\eta\\displaystyle\\sum_{\\ell=i-1}^{i}P_{i,\\ell}\\left(I-\\xi_{1}\\eta B_{\\ell}\\right)F_{\\ell}z_{0}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and therefore ", "page_idx": 30}, {"type": "equation", "text": "$$\nz_{i}-z_{0}=-\\eta\\sum_{\\ell=0}^{i-1}P_{i-1,\\ell}\\left(\\pmb{I}-\\xi\\eta\\pmb{B}_{\\ell}\\right)\\pmb{F}_{\\ell}z_{0}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "In order to compute the bound for $\\|z_{i}-z_{0}\\|$ , we use summation by parts to get ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{c l}{\\displaystyle\\frac{1}{\\eta}\\big(z_{0}-z_{i}\\big)=\\sum_{\\ell=0}^{i-1}P_{i-1,\\ell}\\left(I-\\xi\\eta B_{\\ell}\\right)F_{\\ell}z_{0}}\\\\ {\\displaystyle}&{\\displaystyle=P_{i-1,i-1}\\left(I-\\xi\\eta B_{i-1}\\right)\\sum_{\\ell=0}^{i-1}F_{\\ell}z_{0}}\\\\ &{\\displaystyle\\qquad\\qquad-\\sum_{\\ell=0}^{i-2}\\left(P_{i-1,\\ell+1}\\left(I-\\xi\\eta B_{\\ell+1}\\right)-P_{i-1,\\ell}\\left(I-\\xi\\eta B_{\\ell}\\right)\\right)\\sum_{j=0}^{\\ell}F_{\\ell}z_{0}}\\\\ &{\\displaystyle=\\left(I-\\xi\\eta B_{i-1}\\right)g_{i}-\\sum_{\\ell=0}^{i-2}\\left(P_{i-1,\\ell+1}\\left(I-\\xi\\eta B_{\\ell+1}\\right)-P_{i-1,\\ell}\\left(I-\\xi\\eta B_{\\ell}\\right)\\right)g_{\\ell+1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Here, observe that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P_{i-1,\\ell+1}\\left(I-\\xi\\eta\\mathbf{B}_{\\ell+1}\\right)-P_{i-1,\\ell}\\left(I-\\xi\\eta\\mathbf{B}_{\\ell}\\right)}\\\\ &{\\qquad\\qquad=C_{i-1}C_{i-2}\\ldots C_{\\ell+2}\\left((I-\\xi\\eta\\mathbf{B}_{\\ell+1})-C_{\\ell+1}\\left(I-\\xi\\eta\\mathbf{B}_{\\ell}\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "so by using (45) and (46) we obtain ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|{P}_{i-1,\\ell+1}\\left(I-\\xi\\eta{B}_{\\ell+1}\\right)-{P}_{i-1,\\ell}\\left(I-\\xi\\eta{B}_{\\ell}\\right)\\right\\|}\\\\ &{\\qquad\\leq\\left(2\\xi\\eta{L}+2\\xi\\eta^{2}{L}^{2}+\\xi^{2}\\eta^{3}{L}^{3}\\right)\\left(1+\\eta{L}+\\xi\\eta^{2}{L}^{2}\\right)^{i-\\ell-2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Therefore, we conclude that ", "page_idx": 30}, {"type": "equation", "text": "$$\nz_{i}-z_{0}\\Vert\\leq\\eta\\left(1+\\xi\\eta L\\right)\\Vert g_{i}\\Vert+\\eta^{2}L\\left(2\\xi+2\\xi\\eta L+\\xi^{2}\\eta^{2}L^{2}\\right)\\sum_{\\ell=0}^{i-2}\\left(1+\\eta L+\\xi\\eta^{2}L^{2}\\right)^{i-\\ell-2}\\Vert g_{\\ell+1}\\Vert\\,.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Meanwhile, substituting (47) back to (43) gives us ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\boldsymbol{w}_{i}-\\boldsymbol{z}_{0}=-\\xi\\eta\\boldsymbol{F}_{i}\\boldsymbol{z}_{0}-\\eta\\sum_{\\ell=0}^{i-1}\\left(\\boldsymbol{I}-\\xi\\eta\\boldsymbol{A}_{i}\\right)\\boldsymbol{P}_{i-1,\\ell}\\left(\\boldsymbol{I}-\\xi\\eta\\boldsymbol{B}_{\\ell}\\right)\\boldsymbol{F}_{\\ell}\\boldsymbol{z}_{0}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "For $\\ell<i$ let us define ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{R}_{i,\\ell}:=\\xi^{-1}\\left(I-\\xi\\eta{A}_{i}\\right){P}_{i-1,\\ell}\\left(I-\\xi\\eta{B}_{\\ell}\\right)}\\\\ &{\\qquad=\\xi^{-1}\\left(I-\\xi\\eta{A}_{i}\\right){C}_{i-1}{C}_{i-2}\\ldots{C}_{\\ell+2}{C}_{\\ell+1}\\left(I-\\xi\\eta{B}_{\\ell}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and for convenience $R_{i,i}:=I$ so that (48) can be rewritten as ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{1}{\\xi\\eta}(z_{0}-{\\pmb w}_{i})=\\sum_{\\ell=0}^{i}R_{i,\\ell}F_{\\ell}z_{0}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Applying summation by parts on the above, we obtain ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{\\xi\\eta}(z_{0}-\\pmb{w}_{i})=R_{i,i}\\sum_{\\ell=0}^{i}F_{\\ell}z_{0}-\\displaystyle\\sum_{\\ell=0}^{i-1}(\\pmb{R}_{i,\\ell+1}-\\pmb{R}_{i,\\ell})\\sum_{j=0}^{\\ell}F_{j}z_{0}}\\\\ &{\\displaystyle=\\pmb{g}_{i+1}-\\sum_{\\ell=0}^{i-1}(\\pmb{R}_{i,\\ell+1}-\\pmb{R}_{i,\\ell})\\pmb{g}_{\\ell+1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and as a consequence we get ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{1}{\\xi\\eta}\\left\\|\\pmb{w}_{i}-\\pmb{z}_{0}\\right\\|\\leq\\left\\|\\pmb{g}_{i+1}\\right\\|+\\sum_{\\ell=0}^{i-1}\\left\\|\\pmb{R}_{i,\\ell+1}-\\pmb{R}_{i,\\ell}\\right\\|\\left\\|\\pmb{g}_{\\ell+1}\\right\\|.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "It remains to bound $\\|\\boldsymbol{R}_{i,\\ell+1}-\\boldsymbol{R}_{i,\\ell}\\|$ . For the special case where $\\ell=i-1$ , a direct computation leads to ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\pmb{R}_{i,i}-\\pmb{R}_{i,i-1}=\\pmb{I}-\\xi^{-1}\\left(\\pmb{I}-\\xi\\eta\\pmb{A}_{i}\\right)\\left(\\pmb{I}-\\xi\\eta\\pmb{B}_{i-1}\\right)}&{{}}\\\\ {=(1-\\xi^{-1})\\pmb{I}+\\eta\\pmb{A}_{i}+\\eta\\pmb{B}_{i-1}-\\xi\\eta^{2}\\pmb{A}_{i}\\pmb{B}_{i-1}}&{{}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and thus we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|R_{i,i}-R_{i,i-1}\\|\\leq(1-\\xi^{-1})+2\\eta L+\\xi\\eta^{2}L^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "For the other cases; that is, when $\\ell<i-1$ ,wehave ", "page_idx": 31}, {"type": "equation", "text": "$$\nR_{i,\\ell+1}-R_{i,\\ell}=\\xi^{-1}\\left(I-\\xi\\eta A_{i}\\right)C_{i-1}C_{i-2}\\ldots C_{\\ell+2}\\left(\\left(I-\\xi\\eta B_{\\ell+1}\\right)-C_{\\ell+1}\\left(I-\\xi\\eta B_{\\ell}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "so by using (45) and (46) we get the bound ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\|R_{i,\\ell+1}-R_{i,\\ell}\\|\\leq\\xi^{-1}(1+\\xi\\eta L)\\left(2\\xi\\eta L+2\\xi\\eta^{2}L^{2}+\\xi^{2}\\eta^{3}L^{3}\\right)\\left(1+\\eta L+\\xi\\eta^{2}L^{2}\\right)^{i-\\ell-2}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Applying (51) and (52) on (50) gives the bound for $\\|\\pmb{w}_{i}-\\pmb{z}_{0}\\|$ ", "page_idx": 31}, {"type": "text", "text": "Proposition E.3. Suppose that SEG-FFA is used, $\\begin{array}{r}{\\eta\\,<\\,\\frac{1}{n L}}\\end{array}$ and let $\\textstyle\\nu:=1+{\\frac{1}{2n}}$ .Thenfor any $i=1,\\ldots,2n-1$ wehavethebounds ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|z_{i}-z_{0}\\|\\leq\\left(\\eta\\nu i+\\frac{\\eta\\nu^{2}e^{2}i(i-1)}{2n}\\right)\\|F z_{0}\\|+\\eta\\nu\\delta_{i}+\\eta^{2}L\\nu^{2}e^{2}\\Sigma_{i-1},}\\\\ &{\\|w_{i}-z_{0}\\|\\leq\\frac{\\eta}{2}\\left(1+2\\nu^{2}i+\\frac{\\nu^{3}e^{2}i(i-1)}{n}\\right)\\|F z_{0}\\|+\\frac{\\eta}{2}\\delta_{i+1}+\\frac{\\eta(2\\nu^{2}-1)}{2}\\delta_{i}+\\eta^{2}L\\nu^{3}e^{2}\\Sigma_{i-1},}\\\\ &{|w_{i}-z_{0}\\|^{2}\\leq\\left(\\frac{3\\eta^{2}(i+1)^{2}}{2}+\\frac{3\\eta^{2}(2\\nu^{2}-1)^{2}i^{2}}{2}+\\frac{\\eta^{2}\\nu^{6}e^{4}i(i-1)^{2}(2i-1)}{n^{2}}\\right)\\|F z_{0}\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad+\\frac{3\\eta^{2}}{2}\\delta_{i+1}^{2}+\\frac{3\\eta^{2}(2\\nu^{2}-1)^{2}}{2}\\delta_{i}^{2}+\\frac{6\\eta^{2}\\nu^{6}e^{4}(i-1)}{n^{2}}\\Psi_{i-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof. Using elementary calculus one can show that $\\begin{array}{r}{x\\mapsto(1+\\frac{1}{x}+\\frac{1}{2x^{2}})^{x}}\\end{array}$ increases on $x>0$ and is bounded above by $e$ . Hence for all $0\\leq\\ell<i\\leq2n$ wehave ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\left(1+\\eta L+\\frac{\\eta^{2}L^{2}}{2}\\right)^{i-\\ell-2}\\leq\\left(1+\\frac{1}{n}+\\frac{1}{2n^{2}}\\right)^{2n}\\leq e^{2}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Applying the definitions (35) and (36) on (38) and then substituting $\\xi={^1}/2$ weget ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|z_{i}-z_{0}\\|\\leq\\eta\\left(1+\\frac{\\eta L}{2}\\right)\\|g_{i}\\|+\\eta^{2}L\\left(1+\\frac{\\eta L}{2}\\right)^{2}\\sum_{\\ell=0}^{i-2}\\left(1+\\eta L+\\frac{\\eta^{2}L^{2}}{2}\\right)^{i-\\ell-2}\\|g_{\\ell+1}\\|}\\\\ {\\leq\\eta\\nu\\left(i\\left\\|F z_{0}\\right\\|+\\delta_{i}\\right)+\\eta^{2}L\\nu^{2}\\sum_{\\ell=0}^{i-2}e^{2}\\left(\\left(\\ell+1\\right)\\left\\|F z_{0}\\right\\|+\\delta_{\\ell+1}\\right)}\\\\ {\\leq\\eta\\nu\\left(i\\left\\|F z_{0}\\right\\|+\\delta_{i}\\right)+\\frac{\\eta\\nu^{2}e^{2}i(i-1)}{2n}\\left\\|F z_{0}\\right\\|+\\eta^{2}L\\nu^{2}e^{2}\\Sigma_{i-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Similarly, from (39) we get ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|w_{i}-z_{0}\\|\\le\\displaystyle\\frac{\\eta}{2}\\|g_{i+1}\\|+\\frac{\\eta}{2}\\left(1+2\\eta L+\\frac{\\eta^{2}L^{2}}{2}\\right)\\|g_{i}\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\eta^{2}L\\left(1+\\frac{\\eta L}{2}\\right)^{3}\\displaystyle\\sum_{\\ell=0}^{i-2}\\left(1+\\eta L+\\frac{\\eta^{2}L^{2}}{2}\\right)^{i-\\ell-2}\\|g_{\\ell+1}\\|}\\\\ &{\\qquad\\leq\\displaystyle\\frac{\\eta}{2}\\left((i+1)\\left\\|F z_{0}\\right\\|+\\delta_{i+1}\\right)+\\frac{\\eta}{2}\\left(1+\\frac{2}{n}+\\frac{1}{2n^{2}}\\right)(i\\left\\|F z_{0}\\right\\|+\\delta_{i})}\\\\ &{\\qquad\\qquad+\\eta^{2}L\\nu^{3}\\displaystyle\\sum_{\\ell=0}^{i-2}e^{2}\\left((\\ell+1)\\left\\|F z_{0}\\right\\|+\\delta_{\\ell+1}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\times\\displaystyle\\frac{\\eta}{2}(1+2i\\nu^{2})\\left\\|F z_{0}\\right\\|+\\frac{\\eta}{2}\\delta_{i+1}+\\frac{\\eta(2\\nu^{2}-1)}{2}\\delta_{i}+\\eta^{2}L\\nu^{3}e^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Finally, applying generalized Young's inequality on (39) we get ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\left\\Vert w_{i}-z_{0}\\right\\Vert^{2}\\leq\\displaystyle\\frac{3\\eta^{2}}{4}\\left\\Vert g_{i+1}\\right\\Vert^{2}+\\displaystyle\\frac{3\\eta^{2}}{4}\\left(1+2\\eta L+\\displaystyle\\frac{\\eta^{2}L^{2}}{2}\\right)^{2}\\left\\Vert g_{i}\\right\\Vert^{2}}}\\\\ {{\\qquad\\qquad\\qquad+\\displaystyle3\\left(\\eta^{2}L\\left(1+\\displaystyle\\frac{\\eta L}{2}\\right)^{3}\\displaystyle\\sum_{\\ell=0}^{i-2}\\left(1+\\eta L+\\displaystyle\\frac{\\eta^{2}L^{2}}{2}\\right)^{i-\\ell-2}\\left\\Vert g_{\\ell+1}\\right\\Vert\\right)^{2}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Using generalized Young's inequality once more on the last term gives us ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{3\\left(\\eta^{2}L\\left(1+\\frac{\\eta L}{2}\\right)^{3}\\displaystyle\\sum_{\\ell=0}^{i-2}\\left(1+\\eta L+\\frac{\\eta^{2}L^{2}}{2}\\right)^{i-\\ell-2}\\|g_{\\ell+1}\\|\\right)^{2}\\le3\\left(\\frac{\\eta\\nu^{3}e^{2}}{n}\\displaystyle\\sum_{\\ell=0}^{i-2}\\|g_{\\ell+1}\\|\\right)^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\le\\frac{3\\eta^{2}\\nu^{6}e^{4}(i-1)}{n^{2}}\\displaystyle\\sum_{\\ell=0}^{i-2}\\|g_{\\ell+1}\\|^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Plugging this back yields ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|w_{i}-z_{0}\\right\\|^{2}\\leq\\frac{3\\eta^{2}}{4}\\left\\|g_{i+1}\\right\\|^{2}+\\frac{3\\eta^{2}}{4}\\left(1+2\\eta L+\\frac{\\eta^{2}L^{2}}{2}\\right)^{2}\\left\\|g_{i}\\right\\|^{2}+\\frac{3\\eta^{2}\\nu^{6}\\epsilon^{4}(i-1)}{n^{2}}\\frac{i-2}{\\epsilon=0}|g_{t+1}|^{2}}\\\\ &{\\qquad\\leq\\frac{3\\eta^{2}}{4}\\left(2(i+1)^{2}\\left\\|F z_{0}\\right\\|^{2}+2\\delta_{i+1}^{2}\\right)+\\frac{3\\eta^{2}}{4}\\left(2\\nu^{2}-1\\right)^{2}\\left(2i^{2}\\left\\|F z_{0}\\right\\|^{2}+2\\delta_{i}^{2}\\right)}\\\\ &{\\qquad\\qquad+\\frac{3\\eta^{2}\\nu^{6}\\epsilon^{4}(i-1)}{n^{2}}\\frac{i-2}{\\epsilon=0}\\left(2(\\ell+1)^{2}\\left\\|F z_{0}\\right\\|^{2}+2\\delta_{\\ell+1}^{2}\\right)}\\\\ &{\\qquad\\leq\\frac{3\\eta^{2}}{2}\\left((i+1)^{2}\\left\\|F z_{0}\\right\\|^{2}+\\delta_{i+1}^{2}\\right)+\\frac{3\\eta^{2}}{2}\\left(2\\nu^{2}-1\\right)^{2}\\left(i^{2}\\left\\|F z_{0}\\right\\|^{2}+\\delta_{i}^{2}\\right)}\\\\ &{\\qquad\\qquad+\\frac{\\eta^{2}\\nu^{6}\\epsilon^{4}(i(-1)^{2}(2i-1)}{n^{2}}\\left\\|F z_{0}\\right\\|^{2}+\\frac{6\\eta^{2}\\nu^{6}\\epsilon^{4}(i-1)}{n^{2}}\\Psi_{i-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Now the claimed inequalities can be obtained simply by rearranging the terms appropriately. ", "page_idx": 32}, {"type": "text", "text": "Proposition E.4. Suppose that either SEG-RR or SEG-FF is used with $\\begin{array}{r}{\\alpha=\\beta=\\eta<\\frac{1}{n L}}\\end{array}$ and let $\\begin{array}{r}{\\tilde{\\nu}:=1+\\frac{1}{n}}\\end{array}$ Thenfor any $i=1,\\ldots,2n-1$ we have thebounds ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|z_{i}-z_{0}\\|\\leq\\bigg(\\eta\\widetilde{\\nu}i+\\frac{16\\eta\\widetilde{\\nu}^{2}i(i-1)}{n}\\bigg)\\|F z_{0}\\|+\\eta\\widetilde{\\nu}\\delta_{i}+32\\eta^{2}L\\widetilde{\\nu}^{2}\\Sigma_{i-1},}\\\\ &{\\|w_{i}-z_{0}\\|\\leq\\eta\\,\\bigg(1+i\\widetilde{\\nu}^{2}+\\frac{16\\widetilde{\\nu}^{3}i(i-1)}{n}\\bigg)\\,\\|F z_{0}\\|+\\eta\\delta_{i+1}+\\eta(\\widetilde{\\nu}^{2}-1)\\delta_{i}+32\\eta^{2}L\\widetilde{\\nu}^{3}\\Sigma_{i-1},}\\\\ &{\\|w_{i}-z_{0}\\|^{2}\\leq\\bigg(6\\eta^{2}(i+1)^{2}+\\frac{6\\eta^{2}\\,(1+\\widetilde{\\nu})^{2}\\,i^{2}}{n^{2}}+\\frac{1024\\eta^{2}\\widetilde{\\nu}^{6}i(i-1)^{2}(2i-1)}{n^{2}}\\bigg)\\,\\|F z_{0}\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,6\\eta^{2}\\delta_{i+1}^{2}+\\frac{6\\eta^{2}\\,(1+\\widetilde{\\nu})^{2}}{n^{2}}\\delta_{i}^{2}+\\frac{6144\\eta^{2}\\widetilde{\\nu}^{6}(i-1)}{n^{2}}\\Psi_{i-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof. One can verify that $\\begin{array}{r}{x\\mapsto(1+\\frac{4}{3x})^{x}}\\end{array}$ increases on $x\\geq3$ and is bounded above by $e^{4/3}<4$ With noting that $\\textstyle(1+{\\frac{1}{1}}+{\\frac{1}{1^{2}}})^{1}=3\\stackrel{\\circ}{<}4$ \uff0c $\\begin{array}{r}{(1+\\frac{1}{2}+\\frac{1}{2^{2}})^{2}\\,=\\,\\frac{49}{16}\\,<\\,4}\\end{array}$ <4,and1++\u22641+ whenever $x\\geq3$ , we see that for all $0\\leq\\ell<i\\leq2\\bar{n}$ it holds that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\left(1+\\eta L+\\eta^{2}L^{2}\\right)^{i-\\ell-2}\\leq\\left(1+\\frac{1}{n}+\\frac{1}{n^{2}}\\right)^{2n}\\leq4^{2}=16.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Also, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n2+2\\eta L+\\eta^{2}L^{2}\\leq2+\\frac{2}{n}+\\frac{1}{n^{2}}=1+\\tilde{\\nu}^{2}\\leq2\\tilde{\\nu}^{2}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Applying the definitions (35) and (36) on (38) and then substituting $\\xi=1$ weget ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\left\\Vert z_{i}-z_{0}\\right\\Vert\\leq\\eta\\left(1+\\eta L\\right)\\left\\Vert g_{i}\\right\\Vert+\\eta^{2}L\\left(2+2\\eta L+\\eta^{2}L^{2}\\right)\\displaystyle\\sum_{\\ell=0}^{i-2}\\left(1+\\eta L+\\eta^{2}L^{2}\\right)^{i-\\ell-2}\\left\\Vert g_{\\ell+1}\\right\\Vert}\\\\ {\\leq\\eta\\tilde{\\nu}\\left(i\\left\\Vert F z_{0}\\right\\Vert+\\delta_{i}\\right)+2\\eta^{2}L\\tilde{\\nu}^{2}\\displaystyle\\sum_{\\ell=0}^{i-2}16\\left(\\left(\\ell+1\\right)\\left\\Vert F z_{0}\\right\\Vert+\\delta_{\\ell+1}\\right)}\\\\ {\\leq\\eta\\tilde{\\nu}\\left(i\\left\\Vert F z_{0}\\right\\Vert+\\delta_{i}\\right)+\\frac{16\\eta\\tilde{\\nu}^{2}i(i-1)}{n}\\left\\Vert F z_{0}\\right\\Vert+32\\eta^{2}L\\tilde{\\nu}^{2}\\Sigma_{i-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Similarly, from (39) we get ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|w_{i}-z_{0}\\|\\leq\\eta\\|g_{i+1}\\|+\\eta^{2}L\\left(2+\\eta L\\right)\\|g_{i}\\|}\\\\ &{\\qquad\\qquad\\qquad+\\eta^{2}L(1+\\eta L)\\left(2+2\\eta L+\\eta^{2}L^{2}\\right)\\displaystyle\\sum_{\\ell=0}^{i-2}\\left(1+\\eta L+\\eta^{2}L^{2}\\right)^{i-\\ell-2}\\|g_{\\ell+1}\\|\\,.}\\\\ &{\\qquad\\qquad\\leq\\eta\\left((i+1)\\left\\|F z_{0}\\right\\|+\\delta_{i+1}\\right)+\\frac{\\eta}{n}\\left(2+\\displaystyle\\frac{1}{n}\\right)\\left(i\\left\\|F z_{0}\\right\\|+\\delta_{i}\\right)}\\\\ &{\\qquad\\qquad\\qquad+\\eta^{2}L\\tilde{\\nu}\\left(2\\tilde{\\nu}^{2}\\right)\\displaystyle\\sum_{\\ell=0}^{i-2}16\\left(\\ell\\left\\|F z_{0}\\right\\|+\\delta_{\\ell}\\right).}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\eta(1+i\\tilde{\\nu}^{2})\\left\\|F z_{0}\\right\\|+\\frac{16\\eta\\tilde{\\nu}^{3}(i+1)}{n}\\left\\|F z_{0}\\right\\|+\\eta\\delta_{i+1}+\\eta(\\tilde{\\nu}^{2}-1)\\delta_{i}+32\\eta^{2}L\\tilde{\\nu}^{3}\\Sigma_{i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Finally, applying Young's inequality on (39) we get ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|w_{i}-z_{0}\\right\\|^{2}\\leq3\\eta^{2}\\left\\|g_{i+1}\\right\\|^{2}+\\frac{3\\eta^{2}\\,\\left(2+\\eta L\\right)^{2}}{n^{2}}\\left\\|g_{i}\\right\\|}\\\\ &{\\qquad\\qquad\\qquad+\\left.3\\left(\\eta^{2}L(1+\\eta L)\\left(2+2\\eta L+\\eta^{2}L^{2}\\right)\\displaystyle\\sum_{\\ell=0}^{i-2}\\left(1+\\eta L+\\eta^{2}L^{2}\\right)^{i-\\ell-2}\\left\\|g_{\\ell+1}\\right\\|\\right)^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq3\\eta^{2}\\left\\|g_{i+1}\\right\\|^{2}+\\frac{3\\eta^{2}\\,\\left(2+\\eta L\\right)^{2}}{n^{2}}\\left\\|g_{i}\\right\\|+3\\left(\\frac{2\\eta\\tilde{\\nu}^{3}}{n}\\displaystyle\\sum_{\\ell=0}^{i-2}16\\left\\|g_{\\ell+1}\\right\\|\\right)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Using Young's inequality once more on the last term gives us ", "page_idx": 33}, {"type": "equation", "text": "$$\n3\\left(\\frac{32\\eta\\tilde{\\nu}^{3}}{n}\\sum_{\\ell=0}^{i-2}\\|g_{\\ell+1}\\|\\right)^{2}\\leq\\frac{3072\\eta^{2}\\tilde{\\nu}^{6}(i-1)}{n^{2}}\\sum_{\\ell=0}^{i-2}\\|g_{\\ell+1}\\|^{2}\\,.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Plugging this back yields ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|w_{i}-z_{0}\\|^{2}\\leq3\\eta^{2}\\,\\|g_{i+1}\\|^{2}+\\frac{3\\eta^{2}\\,(2+\\eta L)^{2}}{n^{2}}\\,\\|g_{i}\\|+\\frac{3072\\eta^{2}\\bar{\\nu}^{6}(i-1)}{n^{2}}\\frac{i-2}{\\ell=0}\\|g_{i+1}\\|^{2}}\\\\ &{\\qquad\\qquad\\leq6\\eta^{2}\\,\\Big((i+1)^{2}\\,\\|F z_{0}\\|^{2}+\\delta_{i+1}^{2}\\Big)+\\frac{6\\eta^{2}\\,(2+\\eta L)^{2}}{n^{2}}\\,\\Big(i^{2}\\,\\|F z_{0}\\|^{2}+\\delta_{i}^{2}\\Big)}\\\\ &{\\qquad\\qquad+\\frac{6144\\eta^{2}\\bar{\\nu}^{6}(i-1)}{n^{2}}\\sum_{\\ell=0}^{i-2}\\Big((\\ell+1)^{2}\\,\\|F z_{0}\\|^{2}+\\delta_{\\ell+1}^{2}\\Big)}\\\\ &{\\qquad\\qquad\\leq6\\eta^{2}\\,\\Big((i+1)^{2}\\,\\|F z_{0}\\|^{2}+\\delta_{i+1}^{2}\\Big)+\\frac{6\\eta^{2}\\,(1+\\bar{\\nu})^{2}}{n^{2}}\\,\\Big(i^{2}\\,\\|F z_{0}\\|^{2}+\\delta_{i}^{2}\\Big)}\\\\ &{\\qquad\\qquad\\qquad+\\,\\frac{1024\\eta^{2}\\bar{\\nu}^{6}(i+1)^{2}(2i-1)}{n^{2}}\\,\\|F z_{0}\\|^{2}+\\frac{6144\\eta^{2}\\bar{\\nu}^{6}(i-1)}{n^{2}}\\Psi_{i-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Now the claimed inequalities can be obtained simply by rearranging the terms appropriately. ", "page_idx": 33}, {"type": "text", "text": "Let us now derive the upper bounds for the quantities related to $\\delta_{j}$ and $\\Sigma_{j}$ , defined in (35) and (36) respectively, using the upper bound of the variance of saddle gradients (4). ", "page_idx": 33}, {"type": "text", "text": "Lemma E.5. For any $j=1,\\ldots,2n,$ . it deterministically holds that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\delta_{j}\\leq n(\\rho\\,\\|F z_{0}\\|+\\sigma).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Proof. For any set of indices ${\\mathcal{I}}\\subset\\{0,\\ldots,n-1\\}$ , by Assumption 3.4 it holds that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\sum_{i\\in\\mathcal{I}}\\left\\|F_{i}z_{0}-F z_{0}\\right\\|^{2}\\leq\\sum_{i=0}^{n-1}\\|F_{i}z_{0}-F z_{0}\\|^{2}\\leq n(\\rho\\left\\|F z_{0}\\right\\|+\\sigma)^{2}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Hence, for any $j=1,\\dots,n$ we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left\\|g_{j}-j F z_{0}\\right\\|^{2}=\\left\\|\\sum_{i=0}^{j-1}F_{i}z_{0}-j F z_{0}\\right\\|^{2}}\\\\ {\\displaystyle\\qquad\\qquad\\leq j\\displaystyle\\sum_{i=0}^{j-1}\\|F_{i}z_{0}-F z_{0}\\|^{2}}\\\\ {\\displaystyle\\qquad\\leq j n(\\rho\\left\\|F z_{0}\\right\\|+\\sigma)^{2}}\\\\ {\\displaystyle\\qquad\\leq n^{2}(\\rho\\left\\|F z_{0}\\right\\|+\\sigma)^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and for any $j=n+1,\\ldots,2n$ we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\|g_{j}-j F z_{0}\\right\\|^{2}=\\left\\|\\displaystyle\\sum_{i=0}^{n-1}F_{1}z_{0}+\\displaystyle\\sum_{i=n}^{j-1}F_{1}z_{0}-j F z_{0}\\right\\|^{2}}\\\\ &{\\qquad=\\left\\|\\displaystyle\\sum_{i=n}^{j-1}F_{1}z_{0}-(j-n)F z_{0}\\right\\|^{2}}\\\\ &{\\qquad=\\left\\|\\displaystyle\\sum_{i=2n-j}^{n-1}F_{1}z_{0}-(j-n)F z_{0}\\right\\|^{2}}\\\\ &{\\qquad\\leq(j-n)\\displaystyle\\sum_{i=0}^{j-1}\\|F_{1}z_{0}-F z_{0}\\|^{2}}\\\\ &{\\qquad\\leq(j-n)\\displaystyle\\sum_{i=0}^{j-1}\\|F_{2}z_{0}-F z_{0}\\|^{2}}\\\\ &{\\qquad\\leq n^{2}(\\rho\\|F z_{0}\\|+\\sigma)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Therefore, in any case we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\pmb{g}_{j}-j\\pmb{F}\\pmb{z}_{0}\\|^{2}\\leq n^{2}(\\rho\\,\\|\\pmb{F}\\pmb{z}_{0}\\|+\\sigma)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Taking square roots on both sides gives us the desired bound. ", "page_idx": 34}, {"type": "text", "text": "Lemma E.6. For any $j=1,\\ldots,2n$ it holds that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\tau}[\\delta_{j}^{2}]\\leq\\frac{n(\\rho\\,\\|F z_{0}\\|+\\sigma)^{2}}{2},\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Proof.If $n=1$ then the left hand side is always O, so there is nothing to show. So, we may assume that $n\\geq2$ . Then, for any $j=1,\\dots,n$ , using Lemma 1 in [35] we obtain ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\tau}\\left\\|\\frac{1}{j}g_{j}-F z_{0}\\right\\|^{2}\\leq\\frac{n-j}{j(n-1)}(\\rho\\left\\|F z_{0}\\right\\|+\\sigma)^{2}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Multiplying both sides by $j^{2}$ and applying AM-GM inequality leads to ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\uptau_{\\tau}\\left\\|g_{j}-j F z_{0}\\right\\|^{2}\\leq\\frac{j(n-j)}{n-1}(\\rho\\left\\|F z_{0}\\right\\|+\\sigma)^{2}\\leq\\frac{n^{2}}{4(n-1)}(\\rho\\left\\|F z_{0}\\right\\|+\\sigma)^{2}\\leq\\frac{n}{2}(\\rho\\left\\|F z_{0}\\right\\|+\\sigma)^{2}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Meanwhile, for $j=n+1,\\ldots,2n$ , following the first few steps in (54) we get ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\left\\|g_{j}-j\\pmb{F}\\pmb{z}_{0}\\right\\|^{2}=\\left\\|\\sum_{i=2n-j}^{n-1}F_{i}\\pmb{z}_{0}-(j-n)\\pmb{F}\\pmb{z}_{0}\\right\\|^{2}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Here, once more applying Lemma 1 of [35], we get ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\tau}\\left\\|g_{j}-j F z_{0}\\right\\|^{2}=\\mathbb{E}_{\\tau}\\left\\|\\displaystyle\\sum_{i=2n-j}^{n-1}F_{i}z_{0}-(j-n)F z_{0}\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=(j-n)^{2}\\mathbb{E}_{\\tau}\\left\\|\\displaystyle\\frac{1}{j-n}\\sum_{i=2n-j}^{n-1}F_{i}z_{0}-F z_{0}\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq(j-n)^{2}\\cdot\\displaystyle\\frac{n-(j-n)}{(j-n)(n-1)}(\\rho\\left\\|F z_{0}\\right\\|+\\sigma)^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\displaystyle\\frac{(j-n)(2n-j)}{n-1}(\\rho\\left\\|F z_{0}\\right\\|+\\sigma)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Using AM-GM inequality on the last line gives us ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\uptau_{\\tau}\\left\\|g_{j}-j F z_{0}\\right\\|^{2}\\leq\\frac{(j-n)(2n-j)}{n-1}(\\rho\\left\\|F z_{0}\\right\\|+\\sigma)^{2}\\leq\\frac{n^{2}}{4(n-1)}(\\rho\\left\\|F z_{0}\\right\\|+\\sigma)^{2}\\leq\\frac{n}{2}(\\rho\\left\\|F z_{0}\\right\\|+\\sigma)\\,\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Thus, for any case, we have (55). ", "page_idx": 35}, {"type": "text", "text": "Lemma E.7. For any $k,\\ell\\in\\{0,1,\\ldots,2n\\},$ it holds that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\tau}[\\Sigma_{k}\\Sigma_{\\ell}]\\leq\\frac{k\\ell n(\\rho\\left\\|\\mathbf{F}z_{0}\\right\\|+\\sigma)^{2}}{2}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof. Expanding the product $\\Sigma_{k}\\Sigma_{\\ell}$ and writing in terms of $\\delta$ ,we get ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\Sigma_{k}\\Sigma_{\\ell}=\\left(\\sum_{i=1}^{k}\\delta_{i}\\right)\\left(\\sum_{j=1}^{\\ell}\\delta_{j}\\right)=\\sum_{i=1}^{k}\\sum_{j=1}^{\\ell}\\delta_{i}\\delta_{j}}}\\\\ &{}&{\\leq\\displaystyle\\sum_{i=1}^{k}\\sum_{j=1}^{\\ell}\\frac{\\delta_{i}^{2}+\\delta_{j}^{2}}{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where the last line follows from the AM-GM inequality. Taking the expectation with respect to $\\tau$ and using the bound from Lemma E.6, we obtain ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\tau}[\\Sigma_{k}\\Sigma_{\\ell}]\\leq\\displaystyle\\frac{1}{2}\\displaystyle\\sum_{i=1}^{k}\\sum_{j=1}^{\\ell}\\Big(\\mathbb{E}_{\\tau}[\\delta_{i}^{2}]+\\mathbb{E}_{\\tau}[\\delta_{j}^{2}]\\Big)}\\\\ &{\\phantom{\\leq\\displaystyle\\frac{1}{2}\\sum_{i=1}^{k}\\sum_{j=1}^{\\ell}\\left(\\frac{n(\\rho\\,\\|F z_{0}\\|+\\sigma)^{2}}{2}+\\frac{n(\\rho\\,\\|F z_{0}\\|+\\sigma)^{2}}{2}\\right)}}\\\\ &{\\phantom{\\leq\\displaystyle\\frac{1}{2}\\sum_{i=1}^{k}(\\rho\\,\\|F z_{0}\\|+\\sigma)^{2}}=\\frac{k\\ell n(\\rho\\,\\|F z_{0}\\|+\\sigma)^{2}}{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "which is exactly the claimed. ", "page_idx": 35}, {"type": "text", "text": "Lemma E.8. For any $k,\\ell\\in\\{0,1,\\ldots,2n\\},$ it holds that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\tau}\\left[\\left(\\sum_{i=1}^{k}\\Sigma_{i}\\right)\\left(\\sum_{j=1}^{\\ell}\\Sigma_{j}\\right)\\right]\\leq\\frac{k(k+1)\\ell(\\ell+1)n(\\rho\\left\\|\\pmb{F}\\pmb{z}_{0}\\right\\|+\\sigma)^{2}}{8}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof. Expanding the product in the left hand side of (57) and applying (56), we get ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\boldsymbol\\tau}\\left[\\left(\\displaystyle\\sum_{i=1}^{k}\\Sigma_{i}\\right)\\left(\\displaystyle\\sum_{j=1}^{\\ell}\\Sigma_{j}\\right)\\right]=\\mathbb{E}_{\\boldsymbol\\tau}\\left[\\displaystyle\\sum_{i=1}^{k}\\sum_{j=1}^{\\ell}\\Sigma_{i}\\Sigma_{j}\\right]=\\displaystyle\\sum_{i=1}^{k}\\sum_{j=1}^{\\ell}\\mathbb{E}_{\\boldsymbol\\tau}[\\Sigma_{i}\\Sigma_{j}]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\displaystyle\\sum_{i=1}^{k}\\sum_{j=1}^{\\ell}\\displaystyle\\frac{i j n(\\rho\\,\\|F z_{0}\\|+\\sigma)^{2}}{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\displaystyle\\frac{k(k+1)\\ell(\\ell+1)n(\\rho\\,\\|F z_{0}\\|+\\sigma)^{2}}{8}.\\,\\boldsymbol\\Omega}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "E.2Upper Bounds of the Within-Epoch Errors ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "The full proof of Theorem E.1 is quite long and technical, so we divide it into several parts. First we show that (31) and (32) holds with $a=3$ when SEG-FFA is in use. Then we show that Theorem E.1 also holds for SEG-FF in Appendix E.2.3, and for SEG-RR in Appendix E.2.4. ", "page_idx": 36}, {"type": "text", "text": "Throughout the remaining of this section, we always assume that the variance of the saddle gradients satisfies (4). ", "page_idx": 36}, {"type": "text", "text": "E.2.1 Proof of Equation (31) for SEG-FFA ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "In this section we prove the following. ", "page_idx": 36}, {"type": "text", "text": "Therawulntsieudihti $\\begin{array}{r}{\\eta<\\frac{1}{n L}}\\end{array}$ itholdsthat ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\|r\\|\\leq\\eta^{3}n^{3}C_{\\hbar}\\left\\|F z_{0}\\right\\|+\\eta^{3}n^{3}D_{\\hbar}\\left\\|F z_{0}\\right\\|^{2}+\\eta^{3}n^{3}V_{\\hbar}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "for constants ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle C_{I A}:={\\cal L}^{2}\\left(\\frac{1}{2}\\left(1+\\frac{2e^{2}}{3}\\right)+\\frac{6+e^{2}}{3}+15\\rho\\right),}}\\\\ {{\\displaystyle D_{I A}:={\\cal M}\\left(\\frac{83}{4}+\\frac{24e^{4}}{5}+\\rho^{2}\\left(\\frac{243}{16}+27e^{4}\\right)\\right),}}\\\\ {{\\displaystyle V_{I A}:={\\cal M}\\sigma^{2}\\left(\\frac{243}{16}+27e^{4}\\right)+15{\\cal L}^{2}\\sigma.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "We first list the intermediate results. The actual proof of Theorem E.9 is in page 43, at the end of this section. ", "page_idx": 36}, {"type": "text", "text": "Proposition E.10. For using SEG-FFA, the within-epoch update $z^{\\sharp}$ as given by (12) satisfies ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\pmb{z}^{\\sharp}=\\pmb{z}_{0}-n\\eta\\pmb{F}(\\pmb{z}_{0}-n\\eta\\pmb{F}\\pmb{z}_{0})+\\pmb{r}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where we denote ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{r:=n\\eta F(z_{0}-n\\eta F z_{0})-n\\eta F z_{0}+n^{2}\\eta^{2}D F(z_{0})F z_{0}}}\\\\ {{\\displaystyle\\qquad-\\frac{\\eta}{2}\\sum_{j=0}^{2n-1}\\left(F_{j}w_{j}-F_{j}z_{0}-D F_{j}(z_{0})(w_{j}-z_{0})\\right)}}\\\\ {{\\displaystyle\\qquad+\\frac{\\eta^{2}}{4}\\sum_{j=0}^{2n-1}D F_{j}(z_{0})(F_{j}z_{j}-F_{j}z_{0})}}\\\\ {{\\displaystyle\\qquad+\\frac{\\eta^{2}}{2}\\sum_{j=0}^{2n-1}D F_{j}(z_{0})\\sum_{k=0}^{j-1}(F_{k}w_{k}-F_{k}z_{0}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Proof. Setting $\\alpha=\\eta/2,\\,\\beta=\\eta$ , and $\\theta=1$ in (28), we get ", "page_idx": 36}, {"type": "equation", "text": "$$\nz^{\\sharp}=z_{0}-\\frac{\\eta}{2}\\sum_{j=0}^{2n-1}F_{j}z_{0}+\\frac{\\eta^{2}}{4}\\sum_{j=0}^{2n-1}D F_{j}(z_{0})F_{j}z_{0}+\\frac{\\eta^{2}}{2}\\sum_{0\\leq k<j\\leq2n-1}D F_{j}(z_{0})F_{k}z_{0}+\\frac{1}{2}\\epsilon_{2n-1}F_{j}(z_{0})_{k}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "$\\epsilon_{2n}$ $F_{i}=F_{2n-1-i}$ $i=0,1,\\ldots,2n-1$ $\\begin{array}{r}{\\sum_{i=0}^{n-1}F_{i}=\\sum_{i=n}^{2n-1}F_{i}=n F.}\\end{array}$ $2n F z_{0}$ sum is equal to $\\begin{array}{r}{2\\sum_{j=0}^{n-1}D F_{j}(z_{0})F_{j}z_{0}}\\end{array}$ For the last sum, observe that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{0\\leq k<j\\leq2n-1}D F_{j}(z_{0})F_{k}z_{0}=\\displaystyle\\sum_{0\\leq k<j\\leq n-1}D F_{j}(z_{0})F_{k}z_{0}+\\displaystyle\\sum_{n\\leq k<j\\leq2n-1}D F_{j}(z_{0})F_{k}z_{0}}\\\\ {\\displaystyle}&{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}\\\\ {\\displaystyle}&{+\\displaystyle\\sum_{0\\leq j\\leq n-1}D F_{j}(z_{0})F_{k}z_{0}}\\\\ {\\displaystyle}&{=\\displaystyle\\sum_{0\\leq k<j\\leq n-1}D F_{j}(z_{0})F_{k}z_{0}+\\displaystyle\\sum_{n\\leq n-1}^{n\\leq j\\leq n-1}D F_{j}(z_{0})F_{k}z_{0}}\\\\ &{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}\\\\ {\\displaystyle}&{+\\displaystyle\\sum_{0\\leq k\\leq n-1}D F_{j}(z_{0})F_{k}z_{0}}\\\\ &{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\displaystyle{n-1\\geq j\\leq0}}\\\\ {\\displaystyle}&{=2\\sum_{k\\neq j}D F_{j}(z_{0})F_{k}z_{0}+\\displaystyle\\sum_{j=0}^{n-1}D F_{j}(z_{0})F_{j}z_{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Hence, (63) is equivalent to ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{z^{\\sharp}=z_{0}-n\\eta F z_{0}+\\frac{\\eta^{2}}{2}\\displaystyle\\sum_{j=0}^{n-1}D F_{j}(z_{0})F_{j}z_{0}+\\frac{\\eta^{2}}{2}\\displaystyle\\sum_{0\\le k<j\\le2n-1}D F_{j}(z_{0})F_{k}z_{0}+\\frac{1}{2}\\epsilon_{2n}}\\\\ &{\\quad=z_{0}-n\\eta F z_{0}+\\eta^{2}\\displaystyle\\sum_{j=0}^{n-1}D F_{j}(z_{0})F_{j}z_{0}+\\eta^{2}\\displaystyle\\sum_{k\\neq j}D F_{j}(z_{0})F_{k}z_{0}+\\frac{1}{2}\\epsilon_{2n}}\\\\ &{\\quad=z_{0}-n\\eta F z_{0}+\\eta^{2}\\left(\\displaystyle\\sum_{j=0}^{n-1}D F_{j}(z_{0})\\right)\\left(\\displaystyle\\sum_{j=0}^{n-1}F_{j}z_{0}\\right)+\\frac{1}{2}\\epsilon_{2n}}\\\\ &{\\quad=z_{0}-n\\eta F z_{0}+n^{2}\\eta^{2}D F(z_{0})F z_{0}+\\frac{1}{2}\\epsilon_{2n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Observing that the terms (62b), (62c), and (62d) add up to $\\scriptstyle{\\frac{1}{2}}\\epsilon_{2n}$ completes the proof. ", "page_idx": 37}, {"type": "text", "text": "Proposition E.11. Suppose that $\\begin{array}{r}{\\eta<\\frac{1}{n L}}\\end{array}$ and let $\\textstyle\\nu:=1+{\\frac{1}{2n}}$ .Then the noise term satisfies the bound ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{r\\|\\leq\\eta^{3}n^{3}L^{2}\\left\\|F z_{0}\\right\\|\\left(\\frac{1}{2n}\\left(1+\\frac{2e^{2}}{3}\\right)+\\frac{4\\nu+e^{2}}{3}\\right)}}\\\\ {{\\qquad+\\eta^{3}n^{3}M\\left\\|F z_{0}\\right\\|^{2}\\left(\\frac{1}{2}+4\\nu^{4}+\\frac{16\\nu e^{4}}{5}\\right)}}\\\\ {{\\qquad+\\frac{3\\eta^{3}M}{8}\\left(\\Psi_{2n}+(2\\nu^{2}-1)^{2}\\Psi_{2n-1}+\\frac{4\\nu^{6}e^{4}}{n^{2}}\\sum_{j=1}^{2\\nu_{j}}j\\Psi_{j}\\right)}}\\\\ {{\\qquad+\\frac{\\eta^{3}L^{2}\\left(\\nu+1\\right)}{4}\\Sigma_{2n-1}+\\frac{\\eta^{3}L^{2}\\nu^{2}\\left(1+\\eta L e^{2}\\right)}{2}\\sum_{j=1}^{2n-2}\\Sigma_{j}+\\frac{\\eta^{4}L^{3}\\nu^{3}e^{2}}{2}\\sum_{k=1}^{2n-2}(2n-k-1)\\Sigma_{k-1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof. We bound each line in equation (62). For (62a), we use Lemma C.6 to get ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|n\\eta F(z_{0}-n\\eta F z_{0})-n\\eta F z_{0}+n^{2}\\eta^{2}D F(z_{0})F z_{0}\\right\\|\\leq\\frac{n\\eta M}{2}\\left\\|-n\\eta F z_{0}\\right\\|^{2}}\\\\ {=\\frac{n^{3}\\eta^{3}M}{2}\\left\\|F z_{0}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "In bounding the remaining three lines we repeatedly use the bounds obtained in Proposition E.3. We will also use the following bounds, which follows from (33), (35), and Young's inequality: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|{\\pmb w}_{0}-{\\pmb z}_{0}\\|=\\frac{\\eta}{2}\\,\\|{\\pmb g}_{1}\\|\\leq\\frac{\\eta}{2}\\,\\|{\\pmb F}{\\pmb z}_{0}\\|+\\frac{\\eta}{2}\\delta_{1},}\\\\ {\\displaystyle\\|{\\pmb w}_{0}-{\\pmb z}_{0}\\|^{2}=\\frac{\\eta^{2}}{4}\\,\\|{\\pmb g}_{1}\\|^{2}\\leq\\frac{\\eta^{2}}{2}\\,\\|{\\pmb F}{\\pmb z}_{0}\\|^{2}+\\frac{\\eta^{2}}{2}\\delta_{1}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "For (62b), observe that Lemma C.6 gives us ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\left\\|{\\cal F}_{j}w_{j}-{\\cal F}_{j}z_{0}-D{\\cal F}_{j}(z_{0})(w_{j}-z_{0})\\right\\|\\leq\\frac{M}{2}\\left\\|w_{j}-z_{0}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Thus, by using the bound obtained in Proposition E.3, we get ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\varepsilon\\frac{\\partial^{2}\\left(\\varepsilon\\rightarrow\\varepsilon\\right)}{\\partial\\varepsilon}}&{=}\\\\ &{\\varepsilon\\frac{\\partial^{2}\\left(\\varepsilon^{2}-\\varepsilon\\right)}{\\partial\\varepsilon}\\frac{\\partial^{2}}{\\partial\\varepsilon}\\Bigg[w_{0}-\\varepsilon\\frac{1}{\\varepsilon}\\Bigg]^{2}}\\\\ &{\\varepsilon\\frac{\\partial^{2}\\left(\\varepsilon^{2}-\\varepsilon\\right)}{\\partial\\varepsilon}\\frac{\\partial^{2}\\left(2\\varepsilon^{2}+\\varepsilon\\right)^{2}}{\\partial\\varepsilon}+\\frac{32\\varepsilon^{2}\\left(2\\varepsilon^{2}-1\\right)^{2}}{\\partial\\varepsilon}+\\frac{9\\varepsilon^{2}\\left(\\varepsilon^{2}-\\varepsilon\\right)\\left(2\\varepsilon^{2}-1\\right)}{\\partial\\varepsilon^{2}}\\Bigg\\}\\nu_{0}\\varepsilon^{2}}\\\\ &{\\varepsilon\\frac{\\partial^{2}\\left(\\varepsilon^{2}-\\varepsilon\\right)}{\\partial\\varepsilon}\\frac{\\partial^{2}\\left(2\\varepsilon^{2}+\\varepsilon\\right)^{2}}{\\partial\\varepsilon}+\\frac{32\\varepsilon^{2}\\left(2\\varepsilon^{2}-1\\right)\\beta^{2}}{\\partial\\varepsilon}\\frac{\\partial^{2}\\left(\\varepsilon^{2}-\\varepsilon\\right)}{\\partial\\varepsilon}\\frac{\\partial^{2}\\left(\\varepsilon^{2}-1\\right)}{\\partial\\varepsilon}\\Bigg\\}}\\\\ &{\\quad+\\frac{9\\varepsilon^{2}}{\\partial\\varepsilon}\\frac{\\partial^{2}\\left(\\varepsilon^{2}-2\\varepsilon^{2}\\right)}{\\partial\\varepsilon}+\\frac{\\beta^{2}\\left(2\\varepsilon^{2}-1\\right)^{2}}{\\partial\\varepsilon}\\frac{\\partial^{2}\\left(\\varepsilon^{2}-1\\right)\\beta^{2}\\left(\\varepsilon-1\\right)}{\\partial\\varepsilon}\\Bigg\\}}\\\\ &{\\varepsilon\\frac{\\partial^{2}\\left(\\varepsilon^{2}+\\left(2\\varepsilon^{2}\\right)\\left(1-4\\varepsilon\\right)-32\\varepsilon^{2}+\\varepsilon^{2}\\left(2\\varepsilon^{2}-1\\right)\\beta^{2}\\left(2\\varepsilon-1\\right)\\left(1-\\varepsilon\\right)}{\\partial\\varepsilon}}\\\\ &{\\varepsilon\\frac{\\partial^{2}\\left(\\varepsilon^{2}-\\varepsilon\\right)}{\\partial\\varepsilon}=-\\frac{32\\varepsilon^{2}\\left(\\varepsilon^{2}-1\\right)\\beta^{2}\\left(\\varepsilon-1\\right)\\left(2\\varepsilon-1\\right)}{\\partial\\varepsilon}\\frac{\\partial^{2}\\left(\\varepsilon^{2}-1\\right)\\beta^{2}\\left(\\varepsilon-1\\right)}{\\partial\\varepsilon}\\Bigg\\}\\nu_{0}\\varepsilon^{2}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where along the derivation we used the inequality ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\nu^{5}(n-1)(2n-1)(32n^{2}-42n+11)\\leq64n^{4}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "which holds for all $n\\geq1$ . From now on, we will keep on using similar techniques to reduce the exponentsof $\\nu$ , without explicitly stating the inequalities used, but recovering the inequalities that are used should be clear from context. ", "page_idx": 38}, {"type": "text", "text": "For (62c), we use $L$ -smoothness of $F_{j}$ , and also thefact that it mlis $\\|D F_{j}(z_{0})\\|\\le L$ , to get ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\left|\\frac{\\eta^{2}}{4}\\sum_{j=0}^{2-2\\infty}D_{j}(\\xi_{j},\\xi_{j}-F_{j};z_{j0})\\right|\\right|}\\\\ &{\\qquad\\le\\frac{\\eta^{2}\\sum_{j=0}^{2-1}|D_{j}F_{j}(z_{j})||\\|F_{j}z_{j}-F_{j}z_{j0}\\|}{2}}\\\\ &{\\qquad\\le\\frac{\\eta^{2}\\sum_{j=0}^{2-1}|D_{j}|}{2}\\|z_{j}-z_{j}\\|}\\\\ &{\\qquad\\le\\frac{\\eta^{2}\\hat{L}_{j}^{2}\\sum_{j=0}^{2-1}\\left(\\left(\\eta^{2}+\\frac{\\eta^{2}c_{j}^{2}}{2}\\right)\\right)}{2}\\|F_{z_{j}}\\|+\\eta\\hat{\\psi}_{j}+\\eta^{2}L_{j}^{2}x_{j}^{2}-\\frac{1}{2}}\\\\ &{\\qquad=\\frac{\\eta^{2}\\hat{L}_{j}^{2}}{4}\\left(\\eta\\eta(2\\pi-1)+\\frac{2\\eta^{2}c_{j}^{2}(\\alpha-1)}{3}\\right)}\\\\ &{\\qquad\\qquad\\quad+\\frac{\\eta^{2}\\hat{L}_{j}^{2}\\gamma_{2}}{4}-\\frac{\\eta^{2}\\hat{L}_{j}\\hat{L}_{j}\\gamma_{2}^{2}\\zeta_{1}^{2}}{4}\\sum_{j=1}^{3}}\\\\ &{\\qquad\\qquad\\le\\frac{\\eta^{3}\\hat{L}_{j}^{2}\\alpha^{2}\\left(1+\\frac{2\\eta^{2}\\alpha^{2}}{2}\\right)\\left\\|F_{z_{j}}\\right\\|}{2}+\\frac{\\eta^{2}\\hat{L}_{j}\\gamma_{2}^{2}\\zeta_{1}^{2}}{4}\\sum_{j=0}^{2-1}\\eta}\\\\ &{\\qquad\\le\\frac{\\eta^{3}\\hat{L}_{j}^{2}}{2}\\left(1+\\frac{2\\eta^{2}\\alpha^{2}}{2}\\right)\\|F_{z_{j}}\\|+\\frac{\\eta^{2}\\hat{L}_{j}^{2}\\gamma_{2}^{2}\\zeta_{1}^{2}}{4}\\sum_{j=0}^{2-1}\\eta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "By the same logic, each summand in (62d) with $j>0$ can be bounded as ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}_{\\sigma}(n)\\underset{=}{\\overset{...}{\\sum}}(Y_{n+1})\\underset{=}{\\overset{...}{\\sum}}(Y_{n+1}-F_{n+1})\\bigg|}\\\\ &{\\le\\mathbb{P}_{\\sigma}(n)\\bigg|\\frac{1}{\\sum_{1=0}^{n}n_{1}}\\bigg|\\frac{y_{1}}{\\sum_{2}}\\mathbb{P}_{\\sigma}(n)-F_{n+1}\\bigg|}\\\\ &{\\le E\\frac{y_{1}}{\\sum_{1=0}^{n}n_{1}}\\bigg|\\frac{y_{2}}{\\sum_{3}}\\bigg|(n_{1}-x_{2})\\bigg|}\\\\ &{\\le E\\frac{y_{1}}{\\sqrt{n}}\\bigg(\\frac{y_{1}}{\\sqrt{n}}\\bigg)+\\frac{y_{2}}{\\sqrt{n}}\\bigg|(1+2)\\epsilon_{1}-\\frac{y_{1}}{\\sqrt{n}}(k_{1}-1)\\bigg|\\mathbb{P}_{\\sigma}(1-\\frac{y_{2}}{\\sqrt{n}})}\\\\ &{\\qquad+E\\frac{y_{1}}{\\sqrt{n}}\\bigg|(\\frac{y_{2}}{\\sqrt{n}}+\\frac{y_{2}}{\\sqrt{n}}(k_{1}-\\frac{y_{2}}{\\sqrt{n}})-\\frac{y_{1}}{\\sqrt{n}})\\bigg|\\mathbb{P}_{\\sigma}(1-\\frac{y_{1}}{\\sqrt{n}})}\\\\ &{\\qquad-\\frac{y_{1}}{\\sqrt{n}}\\bigg(\\frac{y_{2}}{\\sqrt{n}}\\frac{y_{1}}{\\sqrt{n}}+\\frac{y_{2}}{\\sqrt{n}}\\frac{y_{2}}{\\sqrt{n}}-1\\frac{y_{1}}{\\sqrt{n}}y_{2}-1\\bigg)}\\\\ &{=\\frac{y_{2}}{2}\\big(\\mathbb{P}_{\\sigma}(1+\\frac{y_{1}}{\\sqrt{n}})-\\frac{y_{2}}{\\sqrt{n}}(1-k_{1}^{-2})(y_{2}-1)+\\frac{y_{2}\\sqrt{n}}{2}(1-y_{1}-1)(-y_{2}-2)\\big)\\|F_{n+1}}\\\\ &{\\qquad+\\frac{y_{2}}{\\sqrt{n}}(\\frac{y_{1}}{\\sqrt{n}}-\\frac{y_{1}}{\\sqrt{n}})+\\frac{y_{2}\\sqrt{n}^{2}(2-\\\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "and when $j=0$ the sum with respect to $k$ becomes an empty sum. Thus, (62d) in total satisfies the bound ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\left|\\frac{\\partial}{\\partial t}\\right|^{\\frac{1}{2}}\\int_{\\frac{\\partial(t)}{\\partial t}}^{\\frac{\\partial(t)}{\\partial t}}D(k_{0})\\frac{\\partial}{\\partial z}\\right|_{\\frac{\\partial(t)}{\\partial t}=0}-F_{\\frac{\\partial(k_{0})}{\\partial t}}\\right|_{\\frac{\\partial(t)}{\\partial t}=0}}\\\\ &{\\quad\\lesssim\\frac{\\partial^{2}}{\\partial t}\\frac{\\partial^{2}}{\\partial t}\\Bigg[\\mu_{0}\\frac{\\partial}{\\partial z}\\Bigg]_{\\frac{\\partial}{\\partial t}=0}\\sum_{k=1}^{\\infty}F_{\\frac{\\partial}{\\partial k_{0}}}\\Bigg|_{\\frac{\\partial}{\\partial t}=0}}\\\\ &{\\qquad+\\frac{\\partial^{2}}{\\partial t}\\frac{\\partial^{2}}{\\partial t}\\Bigg[\\frac{\\partial}{\\partial x}\\Bigg(b^{2}(1-b^{2}(2\\frac{\\partial}{\\partial t}-1)\\xi_{0}-\\eta\\xi_{0})\\Big)\\{F_{\\frac{\\partial}{\\partial t}=1}\\}}\\\\ &{\\qquad+\\frac{\\partial^{2}}{\\partial t}\\frac{\\partial^{2}}{\\partial x}\\Bigg[\\frac{\\partial}{\\partial x}\\Bigg(b^{2}\\xi_{-}+\\frac{\\partial^{2}(2\\frac{\\partial}{\\partial t}-1)\\xi_{-}}{\\partial t}+\\eta^{2}b^{2}\\xi_{-}^{2}\\xi_{\\frac{\\partial}{\\partial t}=-1}\\Bigg)}\\\\ &{\\qquad-\\frac{\\partial^{2}}{\\partial t}\\frac{\\partial}{\\partial t}\\Bigg(\\mu_{0}(1-b)\\frac{\\partial}{\\partial x}\\Bigg(b^{2}(1-b)\\frac{\\partial}{\\partial t}-\\eta\\xi_{0}\\Bigg(1-1)(2\\frac{\\partial}{\\partial t}-1)(2\\frac{\\partial}{\\partial t}-3)\\Bigg)\\{F_{\\frac{\\partial}{\\partial t}=0}\\}}\\\\ &{\\qquad+\\frac{\\partial^{2}\\beta^{2}}{\\partial t}\\frac{\\partial^{2}}{\\partial x}\\Bigg]_{\\frac{\\partial}{\\partial t}=0}+\\frac{\\partial^{2}\\beta^{2}(2\\frac{\\partial}{\\partial t}-1)\\beta^{2}}{\\partial t}\\frac{\\partial^{2}}{\\partial x}\\Bigg[\\chi_{-}+\\frac{\\partial^{2}\\beta^{2}\\beta^{2}\\xi_{1}^{2}\\xi_{1} \n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Simply collecting all the inequalities and rearranging the terms leads to the claimed bound. \u53e3 ", "page_idx": 40}, {"type": "text", "text": "Before we proceed, let us write ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle X_{1}:=\\frac{3\\eta^{3}M}{8}\\left(\\Psi_{2n}+(2\\nu^{2}-1)^{2}\\Psi_{2n-1}+\\frac{4\\nu^{6}e^{4}}{n^{2}}\\sum_{j=1}^{2n-2}j\\Psi_{j}\\right),}}\\\\ {{\\displaystyle X_{2}:=\\frac{\\eta^{3}L^{2}(\\nu+1)}{4}\\Sigma_{2n-1}+\\frac{\\eta^{3}L^{2}\\nu^{2}(1+\\eta L e^{2})}{2}\\sum_{j=1}^{2n-2}\\Sigma_{j}+\\frac{\\eta^{4}L^{3}\\nu^{3}e^{2}}{2}\\sum_{k=1}^{2n-2}(2n-k-1)\\Sigma_{k-1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "so that the bound on $\\lVert\\boldsymbol{r}\\rVert$ obtained in Proposition E.11 can be written as ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\|\\pmb{r}\\|\\leq\\eta^{3}n^{3}L^{2}\\left\\|\\pmb{F}z_{0}\\right\\|\\left(\\displaystyle\\frac1{2n}\\left(1+\\displaystyle\\frac{2e^{2}}3\\right)+\\displaystyle\\frac{4\\nu+e^{2}}3\\right)}\\\\ {\\displaystyle+\\eta^{3}n^{3}M\\left\\|\\pmb{F}z_{0}\\right\\|^{2}\\left(\\displaystyle\\frac12+4\\nu^{4}+\\displaystyle\\frac{16\\nu e^{4}}5\\right)}\\\\ {\\displaystyle+\\,X_{1}+X_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Theorem E.12. Suppose that $\\begin{array}{r}{\\eta<\\frac{1}{n L}}\\end{array}$ and let $\\textstyle\\nu:=1+{\\frac{1}{2n}}$ Then the noise temdeterministically satisfies thebound ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|r\\|\\leq\\eta^{3}n^{3}L^{2}\\left\\|F z_{0}\\right\\|\\left(\\frac{1}{2n}\\left(1+\\frac{2e^{2}}{3}\\right)+\\frac{4\\nu+e^{2}}{3}+10\\nu\\rho\\right)}\\\\ {\\displaystyle\\qquad+\\,\\eta^{3}n^{3}M\\left\\|F z_{0}\\right\\|^{2}\\left(\\frac{1}{2}+4\\nu^{4}+\\frac{16\\nu e^{4}}{5}+\\rho^{2}\\left(3\\nu^{4}+8\\nu^{3}e^{4}\\right)\\right)}\\\\ {\\displaystyle\\qquad+\\,\\eta^{3}n^{3}M\\sigma^{2}\\left(3\\nu^{4}+8\\nu^{3}e^{4}\\right)+10\\nu\\eta^{3}n^{3}L^{2}\\sigma.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Proof. From (36), (37), and Lemma E.5, it holds that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\Sigma_{j}=\\sum_{i=1}^{j}\\delta_{i}\\leq j n(\\rho\\left\\|{\\cal F}z_{0}\\right\\|+\\sigma)},}\\\\ {{\\displaystyle\\Psi_{j}=\\sum_{i=1}^{j}\\delta_{i}^{2}\\leq j n^{2}(\\rho\\left\\|{\\cal F}z_{0}\\right\\|+\\sigma)^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Plugging the bound for $\\Psi_{j}$ into (64) we get ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\dag}_{1}\\leq\\frac{3\\eta^{3}M}{8}\\Bigg(2n^{3}(\\rho\\left\\Vert F z_{0}\\right\\Vert+\\sigma)^{2}+(2\\nu^{2}-1)n^{2}(\\rho\\left\\Vert F z_{0}\\right\\Vert+\\sigma)^{2}+4\\nu^{6}e^{4}\\displaystyle\\sum_{j=1}^{2n-2}j^{2}(\\rho\\left\\Vert F;\\medskip)}\\\\ &{\\quad=\\frac{3\\eta^{3}M}{8}\\left(\\left(2n^{3}+(2\\nu^{2}-1)^{2}(2n-1)n^{2}\\right)+\\frac{4\\nu^{6}e^{4}}{3}(n-1)(2n-1)(4n-3)\\right)(\\rho\\left\\Vert F z_{0}\\right\\Vert+\\sigma)^{2}}\\\\ &{\\quad\\leq\\frac{3\\eta^{3}M}{8}\\left(4\\nu^{4}n^{3}+\\frac{32\\nu^{3}e^{4}n^{3}}{3}\\right)(\\rho\\left\\Vert F z_{0}\\right\\Vert+\\sigma)^{2}}\\\\ &{\\quad=\\frac{\\eta^{3}n^{3}M(\\rho\\left\\Vert F z_{0}\\right\\Vert+\\sigma)^{2}}{2}\\left(3\\nu^{4}+8\\nu^{3}e^{4}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "By Young's inequality, it holds that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\frac{(\\rho\\left\\|{\\cal F}z_{0}\\right\\|+\\sigma)^{2}}{2}\\leq\\rho^{2}\\left\\|{\\cal F}z_{0}\\right\\|^{2}+\\sigma^{2},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "from which we get ", "page_idx": 41}, {"type": "equation", "text": "$$\nX_{1}\\leq\\eta^{3}n^{3}M\\rho^{2}\\left\\|F z_{0}\\right\\|^{2}\\left(3\\nu^{4}+8\\nu^{3}e^{4}\\right)+\\eta^{3}n^{3}M\\sigma^{2}\\left(3\\nu^{4}+8\\nu^{3}e^{4}\\right).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Meanwhile, plugging the bound for $\\Sigma_{j}$ into (65) we get ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\_2\\le\\frac{\\eta^{3}L^{2}(\\nu+1)}{4}(2n-1)n(\\rho\\|F z_{0}\\|+\\sigma)+\\frac{\\eta^{3}L^{2}\\nu^{2}\\left(1+\\eta L e^{2}\\right)^{2n-2}}{2}\\sum_{j=1}^{n-2}\\mu(\\rho\\|F z_{0}\\|+\\sigma)}\\\\ &{\\quad\\quad\\quad+\\frac{\\eta^{4}L^{3}\\nu^{3}e^{2}\\sigma^{2n-2}}{2}\\sum_{k=1}^{n-2}(2n-k-1)(k-1)n(\\rho\\|F z_{0}\\|+\\sigma)}\\\\ &{=\\frac{\\eta^{3}L^{2}(\\nu+1)}{4}(2n-1)n(\\rho\\|F z_{0}\\|+\\sigma)+\\frac{\\eta^{3}L^{2}\\nu^{2}\\left(1+\\eta L e^{2}\\right)}{2}(n-1)(2n-1)n(\\rho\\|F z_{0}\\|+\\sigma}\\\\ &{\\quad\\quad\\quad+\\frac{\\eta^{4}L^{3}\\nu^{3}e^{2}}{6}\\left(-3+11n-12n^{2}+4n^{3}\\right)n(\\rho\\|F z_{0}\\|+\\sigma)}\\\\ &{\\le\\eta^{3}L^{2}(\\rho\\|F z_{0}\\|+\\sigma)\\left(n^{2}+{\\left(1+\\eta L e^{2}\\right)n^{3}}+\\frac{2\\eta L e^{2}}{3}n^{4}\\right)}\\\\ &{\\le\\eta^{3}n^{3}L^{2}(\\rho\\|F z_{0}\\|+\\sigma)\\left(\\frac{1}{n}+1+\\frac{e^{2}}{n}+\\frac{2e^{2}}{3}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where in the last line we used that $\\begin{array}{r}{\\eta<\\frac{1}{n L}}\\end{array}$ . Because the inequality ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\frac{1}{n}+1+\\frac{e^{2}}{n}+\\frac{2e^{2}}{3}\\leq10\\nu\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "holds for all $n\\geq1$ , continuing from above we obtain ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{X_{2}\\leq10\\nu\\eta^{3}n^{3}L^{2}(\\rho\\left\\lVert F z_{0}\\right\\rVert+\\sigma)}\\\\ &{\\qquad\\leq10\\nu\\eta^{3}n^{3}L^{2}\\rho\\left\\lVert F z_{0}\\right\\rVert+10\\nu\\eta^{3}n^{3}L^{2}\\sigma.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Rearranging (66) with applying the bounds (69) and (70) gives us the claimed result. ", "page_idx": 42}, {"type": "text", "text": "ProfofTheoremnEoemME $n\\geq1$ $^{1}\\!/n\\leq1$ and $\\nu\\leq3/2$ wheror $\\textstyle\\nu=1+{\\frac{1}{2n}}$ folowing ", "page_idx": 42}, {"type": "text", "text": "E.2.2 Proof of Equation (32) for SEG-FFA ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "In this section, we prove the following. ", "page_idx": 42}, {"type": "text", "text": "Theorem E.13. Say we use SEG-FFA. Then, as long as the stepsize used in an epoch satisfies n<nt,it holds that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert r\\right\\Vert^{2}\\Big|\\left.z_{0}\\right]\\leq\\eta^{6}n^{6}C_{2A}\\left\\Vert F z_{0}\\right\\Vert^{2}+\\eta^{6}n^{6}D_{2A}\\left\\Vert F z_{0}\\right\\Vert^{4}+\\eta^{6}n^{5}V_{2A}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "for constants ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle C_{2A}:=4L^{4}\\left(\\left(\\frac{1}{2}\\left(1+\\frac{2e^{2}}{3}\\right)+\\frac{6+e^{2}}{3}\\right)^{2}+36\\rho^{2}e^{4}\\right),}}\\\\ {{\\displaystyle D_{2A}:=4M^{2}\\left(\\left(\\frac{83}{4}+\\frac{24e^{4}}{5}\\right)^{2}+\\rho^{4}\\left(\\frac{243}{16}+27e^{4}\\right)^{2}\\right),}}\\\\ {{\\displaystyle V_{2A}:=4M^{2}\\sigma^{4}\\left(\\frac{243}{16}+27e^{4}\\right)^{2}+144e^{4}L^{4}\\sigma^{2}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Proof. The bound is then immediate from the following Theorem E.14, as $n\\geq1$ implies $^1\\!/n\\leq1$ and $\\bar{\\nu}\\leq\\sqrt[3]{2}$ for $\\nu$ defined in the statement of Theorem E.14. ", "page_idx": 42}, {"type": "text", "text": "Theorem E.14. Suppose that $\\begin{array}{r}{\\eta<\\frac{1}{n L}}\\end{array}$ , and let $\\textstyle\\nu:=1+{\\frac{1}{2n}}$ .Then, in expectation, the noise m satisfies the bound ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\Vert r\\Vert^{2}\\middle|\\,z_{0}\\right]\\leq4\\eta^{6}n^{6}L^{4}\\left\\Vert F z_{0}\\right\\Vert^{2}\\left(\\left(\\frac{1}{2n}\\left(1+\\frac{2e^{2}}{3}\\right)+\\frac{4\\nu+e^{2}}{3}\\right)^{2}+\\frac{36\\rho^{2}e^{4}}{n}\\right)}\\\\ &{\\qquad\\qquad\\qquad+\\ 4\\eta^{6}n^{6}M^{2}\\left\\Vert F z_{0}\\right\\Vert^{4}\\left(\\left(\\frac{1}{2}+4\\nu^{4}+\\frac{16\\nu e^{4}}{5}\\right)^{2}+\\frac{\\rho^{4}\\left(3\\nu^{4}+8\\nu^{3}e^{4}\\right)^{2}}{n}\\right)}\\\\ &{\\qquad\\qquad\\qquad+\\ 4\\eta^{6}n^{5}M^{2}\\sigma^{4}\\left(3\\nu^{4}+8\\nu^{3}e^{4}\\right)^{2}+144e^{4}\\eta^{6}n^{5}L^{4}\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Proof. Notice that, when conditioned on $z_{\\mathrm{0}}$ , the only source of randomness included in $\\Psi_{j}$ is the random permutation $\\tau$ selected for the epoch. Hence, we can use Lemma E.6 to get ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Psi_{j}\\mid z_{0}\\right]=\\mathbb{E}\\left[\\sum_{i=1}^{j}\\delta_{i}^{2}\\left\\lvert z_{0}\\right\\rvert=\\sum_{i=1}^{j}\\mathbb{E}\\left[\\delta_{i}^{2}\\left\\lvert z_{0}\\right\\rvert\\leq\\frac{j n(\\rho\\left\\lVert H\\boldsymbol{z}_{0}\\right\\rVert+\\sigma)^{2}}{2}.\\right]\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Applying Young's inequality on (66) we get ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\left\\|{\\pmb r}\\right\\|^{2}\\leq4\\eta^{6}n^{6}L^{4}\\left\\|{\\pmb F}z_{0}\\right\\|^{2}\\left(\\frac{1}{2n}\\left(1+\\frac{2e^{2}}{3}\\right)+\\frac{4\\nu+e^{2}}{3}\\right)^{2}}}\\\\ {{\\qquad\\qquad+4\\eta^{6}n^{6}M^{2}\\left\\|{\\pmb F}z_{0}\\right\\|^{4}\\left(\\frac{1}{2}+4\\nu^{4}+\\frac{16\\nu e^{4}}{5}\\right)^{2}}}\\\\ {{\\qquad\\qquad+4X_{1}^{2}+4X_{2}^{2}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "When conditioned on $z_{\\mathrm{0}}$ , the first two lines are not random quantities. Thus, it suffces to derive the bounds for I $\\mathsf{E}\\left[X_{i}^{2}\\mid z_{0}\\right]$ $i=1,2$ ", "page_idx": 42}, {"type": "text", "text": "Recall that the bound (69) on $X_{1}$ holds deterministically. Hence, it holds that ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[X_{1}^{2}\\;\\middle|\\;z_{0}\\right]\\leq\\mathbb{E}\\left[X_{1}\\left(\\eta^{3}n^{3}M\\rho^{2}\\;\\Vert F z_{0}\\Vert^{2}\\left(3\\nu^{4}+8\\nu^{3}e^{4}\\right)+\\eta^{3}n^{3}M\\sigma^{2}\\left(3\\nu^{4}+8\\nu^{3}e^{4}\\right)\\right)\\Big|\\;z_{0}\\right]}\\\\ &{\\qquad\\qquad=\\eta^{3}n^{3}M\\left(3\\nu^{4}+8\\nu^{3}e^{4}\\right)\\left(\\rho^{2}\\;\\Vert F z_{0}\\Vert^{2}+\\sigma^{2}\\right)\\mathbb{E}\\left[X_{1}\\left|\\;z_{0}\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Now, to compute $\\mathbb{E}\\left[X_{1}\\mid z_{0}\\right]$ , we apply the linearity of expectation on (64) to get ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\frac{3\\eta^{3}M}{8}\\Bigg(\\mathbb{E}\\left[\\Psi_{2n}\\mid z_{0}\\right]+\\left(2\\nu^{2}-1\\right)^{2}\\mathbb{E}\\left[\\Psi_{2n-1}\\mid z_{0}\\right]+\\frac{4\\nu^{6}\\epsilon^{4}}{n^{2}}\\sum_{j=1}^{n-2}j\\mathbb{E}\\left[\\Psi_{j}\\mid z_{0}\\right]\\Bigg)}\\\\ &{\\leq\\frac{3\\eta^{3}M}{8}\\Bigg(n^{2}(\\rho\\left\\|F z_{0}\\right\\|+\\sigma)^{2}+\\frac{\\left(2\\nu^{2}-1\\right)^{2}\\left(2n-1\\right)n\\left(\\rho\\left\\|F z_{0}\\right\\|+\\sigma\\right)^{2}}{2}+\\frac{4\\nu^{6}\\epsilon^{4}}{n^{2}}\\sum_{j=1}^{n-2}\\frac{j^{2}n\\left(\\rho\\left\\|F\\right\\|+\\eta\\right)}{j}}\\\\ &{=\\frac{3\\eta^{3}M}{8}\\left(\\frac{2n^{2}+\\left(2\\nu^{2}-1\\right)^{2}\\left(2n^{2}-n\\right)}{2}+\\frac{2\\nu^{6}\\epsilon^{4}\\left(n-1\\right)\\left(2n-1\\right)\\left(4n-3\\right)}{3n}\\right)(\\rho\\left\\|F z_{0}\\right\\|+\\sigma)^{2}}\\\\ &{\\leq\\frac{3\\eta^{3}M}{8}\\left(2\\nu^{4}n^{2}+\\frac{16\\nu^{3}\\epsilon^{4}n^{2}}{3}\\right)(\\rho\\left\\|F z_{0}\\right\\|+\\sigma)^{2}}\\\\ &{=\\frac{\\eta^{3}n^{2}M\\left(\\rho\\left\\|F z_{0}\\right\\|+\\sigma\\right)^{2}}{4}\\left(3\\nu^{4}+8\\nu^{3}\\epsilon^{4}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Young's inequality gives us the bound ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\frac{(\\rho\\left\\|\\boldsymbol{F}\\boldsymbol{z}_{0}\\right\\|+\\sigma)^{2}}{2}\\leq\\rho^{2}\\left\\|\\boldsymbol{F}\\boldsymbol{z}_{0}\\right\\|^{2}+\\sigma^{2}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "which, with the inequality derived above, leads to ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[X_{1}\\,|\\,z_{0}\\right]\\leq\\frac{\\eta^{3}n^{2}M}{2}\\left(3\\nu^{4}+8\\nu^{3}e^{4}\\right)\\left(\\rho^{2}\\left\\Vert F z_{0}\\right\\Vert^{2}+\\sigma^{2}\\right).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "As a consequence, with using Young's inequality once again, we obtain ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[X_{1}^{2}\\,\\big|\\,z_{0}\\right]\\leq\\frac{\\eta^{6}n^{5}M^{2}}{2}\\left(3\\nu^{4}+8\\nu^{3}e^{4}\\right)^{2}\\left(\\rho^{2}\\,\\|F z_{0}\\|^{2}+\\sigma^{2}\\right)^{2}}\\\\ {\\leq\\eta^{6}n^{5}M^{2}\\left(3\\nu^{4}+8\\nu^{3}e^{4}\\right)^{2}\\left(\\rho^{4}\\,\\|F z_{0}\\|^{4}+\\sigma^{4}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "To get the bound of $\\mathbb{E}\\left[X_{2}^{2}\\,\\big|\\,z_{0}\\right]$ , we begin by using ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\eta L\\nu^{2}(2n-k-1)\\leq\\left(1+\\frac{1}{2n}\\right)^{2}\\frac{2n-k-1}{n}}\\\\ {\\displaystyle=-\\frac{k}{4n^{3}}-\\frac{k}{n^{2}}-\\frac{k}{n}-\\frac{1}{4n^{3}}-\\frac{1}{2n^{2}}+\\frac{1}{n}+2\\,\\leq\\,2,}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "which holds for all $1\\leq k\\leq2n-2$ , to (65) to obtain ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{X_{2}\\leq\\frac{\\eta^{3}L^{2}\\big(\\nu+1\\big)}{4}\\Sigma_{2n-1}+\\frac{\\eta^{3}L^{2}\\nu^{2}\\big(1+\\eta L e^{2}\\big)^{2n-2}}{2}\\sum_{j=1}^{n-2}\\Sigma_{j}+\\frac{\\eta^{3}L^{2}\\nu^{3}e^{2}}{2}\\sum_{k=1}^{2n-2}\\frac{2n-k-1}{n}\\Sigma_{k-1}}\\\\ &{\\quad\\leq\\frac{\\eta^{3}L^{2}\\big(\\nu+1\\big)}{4}\\Sigma_{2n-1}+\\frac{\\eta^{3}L^{2}\\nu^{2}\\big(1+\\eta L e^{2}\\big)}{2}\\sum_{j=1}^{n-2}\\Sigma_{j}+\\eta^{3}L^{2}\\nu e^{2}\\sum_{k=1}^{2n-2}\\Sigma_{k-1}}\\\\ &{\\quad\\leq\\frac{\\eta^{3}L^{2}\\big(\\nu+1\\big)}{4}\\Sigma_{2n-1}+\\bigg(\\frac{\\eta^{3}L^{2}\\nu^{2}\\big(1+\\eta L e^{2}\\big)}{2}+\\eta^{3}L^{2}\\nu e^{2}\\bigg)\\sum_{j=1}^{2n-2}}\\\\ &{\\quad\\leq\\frac{\\eta^{3}L^{2}\\big(\\nu+1\\big)}{4}\\Sigma_{2n-1}+3\\eta^{3}L^{2}e^{2}\\sum_{j=1}^{2n}\\Sigma_{j}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Then we directly square both sides and expand them to get ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{{\\cal X}_{2}^{2}\\leq\\Biggl(\\frac{\\eta^{3}L^{2}(\\nu+1)}{4}\\Sigma_{2n-1}+3\\eta^{3}L^{2}e^{2}\\sum_{j=1}^{2n-2}\\Sigma_{j}\\Biggr)^{2}}}\\\\ {{\\ \\ \\ \\ =\\frac{\\eta^{6}L^{4}(\\nu+1)^{2}}{16}\\Sigma_{2n-1}^{2}+9\\eta^{6}L^{4}e^{4}\\left(\\sum_{j=1}^{2n-2}\\Sigma_{j}\\right)^{2}+\\frac{3\\eta^{6}L^{4}e^{2}(\\nu+1)}{2}\\sum_{j=1}^{2n-2}\\Sigma_{2n-1}\\Sigma_{j}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Here, using Lemma E.7 and Lemma E.8 on the right hand side leads to ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{x}\\left[X_{2}^{\\prime}\\right]\\approx_{0}\\le\\frac{\\eta^{6}L^{4}(\\nu+1)^{2}n(2-1)^{2}(\\rho)\\|F\\xi_{0}\\|+\\sigma^{2})^{2}}{32}+\\frac{9\\eta^{6}L^{4}\\mathfrak{c}^{4}n(2n-2)^{2}(2n-1)^{2}(\\rho)\\|F\\xi_{0}\\|}{2}}\\\\ &{\\qquad\\qquad+\\frac{3\\eta^{6}L^{4}\\mathfrak{c}^{2}(\\nu+1)^{\\frac{2\\alpha-2}{2}}}{2}\\frac{\\beta n(2n-1)(\\rho)\\|F\\xi_{0}\\|+\\sigma^{2}}{2}}\\\\ &{\\qquad\\le\\frac{\\eta^{6}L^{4}(\\nu+1)^{2}n(2n-1)^{2}(\\rho)\\|F\\xi_{0}\\|+\\sigma^{2}}{32}+\\frac{9\\eta^{6}L^{4}\\mathfrak{c}^{4}n(2n-2)^{2}(2n-1)^{2}(\\rho)\\|F\\xi_{0}\\|}{8}}\\\\ &{\\qquad\\qquad+\\frac{3\\eta^{6}L^{4}\\mathfrak{c}^{2}(\\nu+1)n(n-1)(2n-1)^{2}(\\rho)\\|F\\xi_{0}\\|+\\sigma^{2}}{4}}\\\\ &{\\qquad\\qquad+\\frac{9\\eta^{6}L^{4}(\\nu^{2}\\mathbb{H}^{3}\\cap\\{\\rho\\}|F\\xi_{0}|)+\\sigma^{2}}{2}+\\frac{9\\eta^{6}L^{4}\\mathfrak{c}^{4}n(2n-2)^{2}(2n-1)^{2}(\\rho)\\|F\\xi_{0}\\|+\\sigma)^{2}}{8}}\\\\ &{\\qquad\\qquad+6\\eta^{6}L^{4}\\mathfrak{c}^{6}n^{3}(n-1)(\\rho)\\|F\\xi_{0}\\|+\\sigma^{2}}\\\\ &{\\qquad=\\eta^{6}L^{4}\\left(\\frac{n^{3}}{2}+\\frac{9\\epsilon^{4}n(2n-2)^{2}(2n-1)^{2}}{3}+6\\sigma^{2}n^{3}(n-1)\\right)(\\rho\\|F\\xi_{0}\\|+\\sigma)^{2}}\\\\ &{\\qquad\\le18\\epsilon^{4}\\frac{\\eta^{6}L^{4}\\mathfrak{c}^{6}n^{4}(\\nu\\|\\mathcal{Z}_{0}\\| \n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "As a consequence, with using (76) once again, we obtain ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[X_{2}^{2}\\,|\\,z_{0}\\right]\\leq36e^{4}\\eta^{6}L^{4}n^{5}\\left(\\rho^{2}\\left\\|F z_{0}\\right\\|^{2}+\\sigma^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Taking the conditional expectation on (75), applying the bounds (77) and (78), and then rearranging the terms leads to the claimed inequality. \u53e3 ", "page_idx": 44}, {"type": "text", "text": "E.2.3 Upper Bounds of the Within-Epoch Errors for SEG-FF ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Theorem E.15. Say we use SEG-FF with $\\alpha=\\beta=\\eta/2$ . Then, as long as the stepsize used in an epoch satisfies $\\begin{array}{r}{\\eta<\\frac{1}{n L}}\\end{array}$ , it holds that ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\|r\\|\\leq\\eta^{2}n^{2}C_{\\|F}\\|F z_{0}\\|+\\eta^{2}n^{2}D_{\\mathit{t F}}\\|F z_{0}\\|^{2}+\\eta^{2}n^{2}V_{\\mathit{t F}}}\\\\ {\\mathbb{E}\\left[\\left\\|r\\right\\|^{2}\\Big|z_{0}\\right]\\leq\\eta^{4}n^{4}C_{2F}\\|F z_{0}\\|^{2}+\\eta^{4}n^{4}D_{2F}\\|F z_{0}\\|^{4}+\\eta^{4}n^{3}V_{2F}}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "forconstants $C_{1F},\\,D_{1F},\\,V_{1F},\\,C_{2F},\\,D_{2F}$ and $V_{2F}$ to be determined later in (81) and (82) ", "page_idx": 44}, {"type": "text", "text": "Proof. As we have discussed in Section 5.1, we already know that aiming to achieve $\\mathcal{O}(\\eta^{3})$ error without anchoring is futile. Instead, we show that error of magnitude $\\bar{\\mathcal{O}(\\eta^{2})}$ is possible with the chosen stepsizes. ", "page_idx": 44}, {"type": "text", "text": "By Proposition D.2 and Lemma D.4 we have For any $i=0,1,\\ldots,N$ , it holds that ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{z_{2n}=z_{0}-\\displaystyle{\\frac{\\eta}{2}\\sum_{j=0}^{2n-1}\\cal T}_{j}z_{0}+\\displaystyle{\\frac{\\eta^{2}}{4}\\sum_{j=0}^{2n-1}D T_{j}(z_{0})T_{j}z_{0}+\\frac{\\eta^{2}}{4}\\sum_{0\\le k<j\\le2n-1}D T_{j}(z_{0})T_{k}z_{0}+\\epsilon_{2n}}}}\\\\ {{\\ \\ \\ \\ =z_{0}-\\eta\\displaystyle{\\sum_{j=0}^{n-1}F_{j}z_{0}+\\frac{3\\eta^{2}}{4}\\sum_{j=1}^{n}D F_{j}(z_{0})F_{j}z_{0}+\\frac{\\eta^{2}}{2}\\sum_{i\\neq j}D F_{j}(z_{0})F_{i}z_{0}+\\epsilon_{2n}}}}\\\\ {{\\ \\ \\ \\ =z_{0}-\\eta n F z_{0}+\\eta^{2}n^{2}D F(z_{0})F z_{0}-\\displaystyle{\\frac{\\eta^{2}}{4}\\sum_{j=1}^{n}D F_{j}(z_{0})F_{j}z_{0}}-\\displaystyle{\\frac{\\eta^{2}}{2}\\sum_{i\\neq j}D F_{j}(z_{0})F_{i}z_{0}}+\\epsilon_{2n}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where we denote ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\epsilon_{2n}:=-\\frac{\\eta}{2}\\sum_{j=0}^{2n-1}\\biggl(F_{j}w_{j}-F_{j}z_{0}-D F_{j}(z_{0})(w_{j}-z_{0})\\biggr)}}\\\\ {{\\displaystyle\\qquad+\\,\\frac{\\eta^{2}}{4}\\sum_{j=0}^{2n-1}D F_{j}(z_{0})(F_{j}z_{j}-F_{j}z_{0})+\\frac{\\eta^{2}}{4}\\sum_{j=0}^{2n-1}D F_{j}(z_{0})\\sum_{k=0}^{j-1}(F_{k}w_{k}-F_{k}z_{0}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Comparing $z_{2n}$ to a point that would have been the result of a deterministic EG update with stepsize nn we get ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{z_{2n}-(z_{0}-\\eta n F(z_{0}-\\eta n F z_{0}))=\\eta n F(z_{0}-\\eta n F z_{0})-\\eta n F z_{0}+\\eta^{2}n^{2}D F(z_{0})F z_{0}+\\epsilon_{2n}}}\\\\ {{-\\displaystyle\\frac{\\eta^{2}}{4}\\sum_{j=1}^{n}D F_{j}(z_{0})F_{j}z_{0}-\\displaystyle\\frac{\\eta^{2}}{2}\\sum_{i\\ne j}D F_{j}(z_{0})F_{i}z_{0}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Let us define ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\tilde{r}:=\\eta n F(z_{0}-\\eta n F z_{0})-\\eta n F z_{0}+\\eta^{2}n^{2}D F(z_{0})F z_{0}+\\epsilon_{2n}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Noticing the resemblence between (62) and the equations in (79) and (80), we can repeat the same reasoning used for Theorem E.9 and Theorem E.13, but with replacing the bounds given by Proposition E.3 to those in Proposition E.4 (and plugging in $\\eta/2$ in place of $\\eta$ in the statement of Proposition E.4) to conclude that ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\tilde{\\boldsymbol{r}}\\|\\leq\\eta^{3}n^{3}\\tilde{C}_{1\\mathbb{A}}\\left\\|\\boldsymbol{F}z_{0}\\right\\|+\\eta^{3}n^{3}\\tilde{D}_{1\\mathbb{A}}\\left\\|\\boldsymbol{F}z_{0}\\right\\|^{2}+\\eta^{3}n^{3}\\tilde{V}_{1\\mathbb{A}}}\\\\ {\\mathbb{E}\\left[\\left\\|\\tilde{\\boldsymbol{r}}\\right\\|^{2}\\Big|z_{0}\\right]\\leq\\eta^{6}n^{6}\\tilde{C}_{2\\mathbb{A}}\\left\\|\\boldsymbol{F}z_{0}\\right\\|^{2}+\\eta^{6}n^{6}\\tilde{D}_{2\\mathbb{A}}\\left\\|\\boldsymbol{F}z_{0}\\right\\|^{4}+\\eta^{6}n^{5}\\tilde{V}_{2\\mathbb{A}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "for some constants $\\tilde{C}_{1\\mathsf{A}},\\tilde{D}_{1\\mathsf{A}},\\tilde{V}_{1\\mathsf{A}},\\tilde{C}_{2\\mathsf{A}},\\tilde{D}_{2\\mathsf{A}},$ and $\\tilde{V}_{2\\mathsf{A}}$ . Meanwhile, we also have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\left|\\begin{array}{l}{\\eta^{2}\\frac{\\sqrt{\\alpha}}{2}\\gamma D_{t}(x_{0})F_{2}\\alpha_{0}+\\frac{\\eta^{2}}{2}\\sum_{j=1}^{D}D F_{2}(\\alpha_{0})F_{2,j}\\alpha_{0}\\right|}\\\\ {+}\\end{array}\\right|}\\\\ &{\\qquad=\\left|\\frac{\\eta^{2}\\alpha_{0}^{2}}{2}D F_{2}(x_{0})F_{2,\\alpha_{0}}-\\frac{\\eta^{2}}{4}\\sum_{j=1}^{D}D F_{2}(\\alpha_{0})F_{2,j}\\alpha_{0}\\right|}\\\\ &{\\qquad\\leq\\frac{\\eta^{2}\\alpha_{0}^{2}}{2}\\left|\\left[D F_{2}(\\alpha_{0})\\right]\\right|\\left|F_{2,\\alpha_{0}}\\right|+\\frac{\\eta^{2}}{4}\\frac{\\sqrt{\\alpha}}{\\gamma-1}\\left|D F_{2}(\\alpha_{0})\\right|\\left|\\|F_{2,\\alpha_{0}}\\right|}\\\\ &{\\qquad\\leq\\frac{\\eta^{2}\\alpha_{0}^{2}}{2}L\\left|F_{2}(\\alpha_{0})+\\frac{\\eta^{2}}{4}\\sum_{j=1}^{D}L\\left(\\|F_{2,\\alpha_{0}}-F_{2,j}\\|+\\|F_{2,\\alpha_{0}}\\right|)}\\\\ &{\\qquad\\leq\\frac{\\eta^{2}(\\alpha_{0}^{2}+\\alpha_{1})L}{2}\\left|F_{2}(\\alpha_{0})+\\frac{\\eta^{2}\\alpha_{1}^{2}}{4}\\sum_{j=1}^{D}|F_{2}(\\alpha_{0}-F_{2,j}|)\\right|}\\\\ &{\\qquad\\leq\\eta^{2}\\alpha_{1}^{2}\\left\\{F_{2,\\alpha_{0}}\\right\\}+\\frac{\\eta^{2}L}{4}\\left(\\frac{\\sqrt{\\alpha}}{\\gamma-1}\\left|F_{2}(\\alpha_{0})-F_{2,j}\\right|\\right)^{1/2}\\left(\\frac{\\sqrt{\\alpha}}{\\gamma-1}\\right)^{1/\\alpha_{0}}}\\\\ &{\\qquad=\\eta^{2}\\alpha_{2}^{2}L\\left|F_{2,\\alpha_{0}}\\right|+\\frac{\\eta^{2}\\alpha_{1}^{2}}{4}(\\rho_{1}F_{2,\\alpha_{0}}+\\rho),}\\\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where in the second to the last line we used the Cauchy-Schwarz inequality. Therefore, as $\\eta\\leq1/{n L}$ weconcludethat ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{-\\left(z_{0}-\\eta n F(z_{0}-\\eta n F z_{0})\\right)\\|\\le\\eta^{2}n^{2}C_{1\\mathsf{F}}\\,\\|F z_{0}\\|+\\eta^{2}n^{2}D_{1\\mathsf{F}}\\,\\|F z_{0}\\|^{2}+\\eta^{2}n^{2}V_{1\\mathsf{F}}}&\\\\ &{\\mathfrak{t s}}&\\\\ &{\\quad}&{C_{1\\mathsf{F}}=L+\\displaystyle\\frac{\\rho L}{4}+\\displaystyle\\frac{\\tilde{C}_{1\\mathsf{A}}}{L},\\quad D_{1\\mathsf{F}}=\\displaystyle\\frac{\\tilde{D}_{1\\mathsf{A}}}{L},\\quad V_{1\\mathsf{F}}=\\displaystyle\\frac{\\sigma L}{4}+\\displaystyle\\frac{\\tilde{V}_{1\\mathsf{A}}}{L}.}&\\end{array}\n$$for constan ", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "", "page_idx": 45}, {"type": "text", "text": "Moreover, using Young's inequality, we see that ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\left\\|\\frac{\\eta^{2}}{4}\\sum_{j=1}^{n}D F_{j}(z_{0})F_{j}z_{0}+\\frac{\\eta^{2}}{2}\\sum_{i\\neq j}D F_{j}(z_{0})F_{i}z_{0}\\right\\|^{2}}}\\\\ &{\\leq3\\eta^{4}n^{4}L^{2}\\left\\|F z_{0}\\right\\|^{2}+\\frac{3\\eta^{4}n^{2}L^{2}}{16}\\rho^{2}\\left\\|F z_{0}\\right\\|^{2}+\\frac{3\\eta^{4}n^{2}L^{2}}{16}\\sigma^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "so we also conclude that ", "page_idx": 46}, {"type": "text", "text": "$\\begin{array}{r}{\\mathbb{\\tilde{C}}\\left[\\left\\|z_{2n}-(z_{0}-\\eta n F(z_{0}-\\eta n F z_{0}))\\right\\|^{2}\\Big|\\,z_{0}\\right]\\leq\\eta^{4}n^{4}C_{2F}\\left\\|F z_{0}\\right\\|^{2}+\\eta^{4}n^{4}D_{2\\mathbb{F}}\\left\\|F z_{0}\\right\\|^{4}+\\eta^{4}n^{3}V_{2\\mathbb{F}}\\left\\|\\nabla z_{0}\\right\\|^{4}+\\eta^{4}n^{3}V_{2\\mathbb{F}}\\left\\|\\nabla z_{0}\\right\\|^{2}.}\\end{array}$ holds for constants ", "page_idx": 46}, {"type": "equation", "text": "$$\nC_{2\\mathsf{F}}=6L^{2}+\\frac{3\\rho^{2}L^{2}}{8}+\\frac{2\\tilde{C}_{2\\mathsf{A}}}{L^{2}},\\quad D_{2\\mathsf{F}}=\\frac{2\\tilde{D}_{1\\mathsf{A}}}{L^{2}},\\quad V_{2\\mathsf{F}}=\\frac{3\\sigma^{2}L^{2}}{8}+\\frac{2\\tilde{V}_{1\\mathsf{A}}}{L^{2}}.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "E.2.4 Upper Bounds of the Within-Epoch Errors for SEG-RR ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Theorem E.16. Say we use SEG-RR with $\\alpha=\\beta=\\eta$ .Then, as long as the stepsize used in an epoch satisfies $\\begin{array}{r}{\\eta<\\frac{1}{n L}}\\end{array}$ ,it holds that ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\|\\pmb{r}\\|\\leq\\eta^{2}n^{2}C_{t R}\\left\\|\\pmb{F}z_{0}\\right\\|+\\eta^{2}n^{2}D_{t R}\\left\\|\\pmb{F}z_{0}\\right\\|^{2}+\\eta^{2}n^{2}V_{t R}}\\\\ {\\mathbb{E}\\left[\\left\\|\\pmb{r}\\right\\|^{2}\\Big|\\ z_{0}\\right]\\leq\\eta^{4}n^{4}C_{2R}\\left\\|\\pmb{F}z_{0}\\right\\|^{2}+\\eta^{4}n^{4}D_{2R}\\left\\|\\pmb{F}z_{0}\\right\\|^{4}+\\eta^{4}n^{3}V_{2R}}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "forconstants $C_{1R},\\,D_{1R},\\,V_{1R},\\,C_{2R},\\,D_{2R},$ and $V_{2R}$ to be determined later in (86) and (87) ", "page_idx": 46}, {"type": "text", "text": "Proof. As we have discussed in Section 5.1, we already know that aiming to achieve $\\mathcal{O}(\\eta^{3})$ error with only using random reshuffling is futile. Instead, we show that error of magnitude $\\mathcal{O}(\\eta^{2})$ is possible with the chosen stepsizes. ", "page_idx": 46}, {"type": "text", "text": "By Proposition D.2 and Lemma D.4 we have For any $i=0,1,\\ldots,N$ , it holds that ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{z_{n}=z_{0}-\\eta\\displaystyle\\sum_{j=0}^{n-1}{F_{j}z_{0}+\\eta^{2}\\sum_{j=0}^{n-1}{D F_{j}(z_{0})F_{j}z_{0}}+\\eta^{2}\\sum_{0\\leq k<j\\leq n-1}{D F_{j}(z_{0})F_{k}z_{0}}+\\epsilon_{n}}}}\\\\ {{\\mathrm{~}}}\\\\ {{\\quad=z_{0}-\\eta n F z_{0}+\\eta^{2}n^{2}D F(z_{0})F z_{0}-\\eta^{2}\\displaystyle\\sum_{0\\leq j<k\\leq n-1}{D F_{j}(z_{0})F_{k}z_{0}}+\\epsilon_{n}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where we denote ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\epsilon_{n}:=-\\eta\\sum_{j=0}^{n-1}\\biggl(F_{j}w_{j}-F_{j}z_{0}-D F_{j}(z_{0})(w_{j}-z_{0})\\biggr)}}\\\\ {{\\displaystyle\\qquad+\\eta^{2}\\sum_{j=0}^{n-1}D F_{j}(z_{0})(F_{j}z_{j}-F_{j}z_{0})+\\eta^{2}\\sum_{j=0}^{n-1}D F_{j}(z_{0})\\sum_{k=0}^{j-1}(F_{k}w_{k}-F_{k}z_{0}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Comparing $z_{n}$ to a point that would have been the result of a deterministic EG update with stepsize n weget ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r}{z_{n}-(z_{0}-\\eta n F(z_{0}-\\eta n F z_{0}))=\\eta n F(z_{0}-\\eta n F z_{0})-\\eta n F z_{0}+\\eta^{2}n^{2}D F(z_{0})F z_{0}+\\epsilon_{n}}\\\\ {-\\eta^{2}\\displaystyle\\sum_{0\\le j<k\\le n-1}D F_{j}(z_{0})F_{k}z_{0}.\\ ~~~~~~~~~~~~~~~~~~~~~~~~}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Let us define ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\check{r}:=\\eta n F(z_{0}-\\eta n F z_{0})-\\eta n F z_{0}+\\eta^{2}n^{2}D F(z_{0})F z_{0}+\\epsilon_{n}.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Comparing the sums (62b)-(62d) to (83), we can repeat the same reasoning used for Theorem E.9 and Theorem E.13, but with replacing the bounds given by Proposition E.3 to those in Proposition E.4, toconcludethat ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\|\\check{\\boldsymbol{r}}\\|\\leq\\eta^{3}n^{3}\\check{C}_{1\\mathbb{A}}\\left\\|\\boldsymbol{F}z_{0}\\right\\|+\\eta^{3}n^{3}\\check{D}_{1\\mathbb{A}}\\left\\|\\boldsymbol{F}z_{0}\\right\\|^{2}+\\eta^{3}n^{3}\\check{V}_{1\\mathbb{A}}}\\\\ {\\mathbb{E}\\left[\\left\\|\\check{\\boldsymbol{r}}\\right\\|^{2}\\Big|z_{0}\\right]\\leq\\eta^{6}n^{6}\\check{C}_{2\\mathbb{A}}\\left\\|\\boldsymbol{F}z_{0}\\right\\|^{2}+\\eta^{6}n^{6}\\check{D}_{2\\mathbb{A}}\\left\\|\\boldsymbol{F}z_{0}\\right\\|^{4}+\\eta^{6}n^{5}\\check{V}_{2\\mathbb{A}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "for some constants $\\check{C}_{1\\mathsf{A}},\\check{D}_{1\\mathsf{A}},\\check{V}_{1\\mathsf{A}},\\check{C}_{2\\mathsf{A}},\\check{D}_{2\\mathsf{A}}$ and $\\check{V}_{2\\mathsf{A}}$ . Meanwhile, we also have ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\sum_{1\\leq j<k\\leq n-1}D F_{j}(z_{0})F_{k}z_{0}=\\displaystyle\\sum_{j=0}^{n-1}D F_{j}(z_{0})(n F z_{0}-g_{j+1})}}\\\\ {{\\displaystyle=\\sum_{j=0}^{n-1}(n-j-1)D F_{j}(z_{0})F z_{0}-\\displaystyle\\sum_{j=0}^{n-1}D F_{j}(z_{0})(g_{j+1}-(j+1)F z_{0})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "which leads to ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\displaystyle\\sum_{0\\leq j<k\\leq n-1}D F_{j}(z_{0})F_{k}z_{0}\\right\\|\\leq\\displaystyle\\sum_{j=0}^{n-1}(n-j-1)L\\left\\|\\pmb{F}z_{0}\\right\\|+L\\displaystyle\\sum_{j=0}^{n-1}\\delta_{j+1}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\displaystyle\\frac{n^{2}L}{2}\\left\\|\\pmb{F}z_{0}\\right\\|+L\\displaystyle\\sum_{j=0}^{n-1}\\delta_{j+1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Therefore, from $\\eta\\le1/{n L}$ and Lemma E.5, on one hand we obtain ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|z_{n}-(z_{0}-\\eta n F(z_{0}-\\eta n F z_{0}))\\|\\leq\\eta^{2}n^{2}C_{1\\mathsf{R}}\\,\\|F z_{0}\\|+\\eta^{2}n^{2}D_{1\\mathsf{R}}\\,\\|F z_{0}\\|^{2}+\\eta^{2}n^{2}V_{1\\mathsf{R}}\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "for constants ", "page_idx": 47}, {"type": "equation", "text": "$$\nC_{1\\mathsf{R}}=\\frac{L}{2}+\\rho L+\\frac{\\check{C}_{1\\mathsf{A}}}{L},\\quad D_{1\\mathsf{R}}=\\frac{\\check{D}_{1\\mathsf{A}}}{L},\\quad V_{1\\mathsf{R}}=\\sigma L+\\frac{\\check{V}_{1\\mathsf{A}}}{L}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "On the other hand, applying Young's inequality on (85) we get ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\displaystyle\\sum_{0\\leq j<k\\leq n-1}D F_{j}(z_{0})F_{k}z_{0}\\right\\|^{2}\\leq n^{4}L^{2}\\left\\|\\pmb{F}z_{0}\\right\\|^{2}+2L^{2}\\left(\\displaystyle\\sum_{j=0}^{n-1}\\delta_{j+1}\\right)^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq n^{4}L^{2}\\left\\|\\pmb{F}z_{0}\\right\\|^{2}+2n L^{2}\\displaystyle\\sum_{j=1}^{n}\\delta_{j}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Taking the expectation conditioned on $\\scriptstyle z_{0}$ and applying Lemma E.6, we conclude that ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\upzeta\\left[\\left\\Vert z_{2n}-(z_{0}-\\eta n F(z_{0}-\\eta n F z_{0}))\\right\\Vert^{2}\\right]z_{0}\\right]\\leq\\eta^{4}n^{4}C_{2\\mathbb{R}}\\left\\Vert F z_{0}\\right\\Vert^{2}+\\eta^{4}n^{4}D_{2\\mathbb{R}}\\left\\Vert F z_{0}\\right\\Vert^{4}+\\eta^{4}n^{3}V_{2\\mathbb{R}}\\left\\Vert\\mathbf{r}\\vphantom{\\frac{4}{4}}\\right\\Vert^{2}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "holds for constants ", "page_idx": 47}, {"type": "equation", "text": "$$\nC_{2\\mathsf{R}}=2L^{2}+4\\rho^{2}L^{2}+\\frac{2\\check{C}_{2\\mathsf{A}}}{L^{2}},\\quad D_{2\\mathsf{R}}=\\frac{2\\check{D}_{2\\mathsf{A}}}{L^{2}},\\quad V_{2\\mathsf{R}}=4\\sigma^{2}L^{2}+\\frac{2\\check{V}_{2\\mathsf{A}}}{L^{2}}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "F  Convergence Bounds in the Strongly Monotone Setting ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "In this section, we focus only on the iterates $\\{z_{0}^{k}\\}_{k\\ge0}$ . So, we omit the subscript O unless necessary, and simpy write $z^{k}$ insteadof $z_{0}^{k}$ ", "page_idx": 47}, {"type": "text", "text": "F.1Unified Analysis of the Upper Bounds for Shuffling-Based SEG Methods ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "When $\\pmb{F}^{\\prime}$ is $\\mu$ -strongly monotone with $\\mu\\,>\\,0$ , all of SEG-RR, SEG-FF, and SEG-FFA do not diverge. In fact, it is possible to establish the following unified analysis of the methods. ", "page_idx": 47}, {"type": "text", "text": "Theorem F.1 (Theorem F.5, simplified). Suppose that ${\\pmb F}^{\\prime}$ is $\\mu$ -stronglymonotonewith $\\mu~>~0$ \uff0c Assumption 3.3 holds, and an optimization method whose within-epoch error satisfies (31) and (32) for some constant $a>0$ is run for $K$ epochs.Then, for a sufficiently small constant $\\omega$ that does not depend on $K$ weachievethebound ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left\\|z^{K}-z^{*}\\right\\|^{2}\\leq\\exp\\left(-\\frac{1}{2}\\mu\\omega n K\\right)\\left\\|z^{0}-z^{*}\\right\\|^{2}+\\tilde{\\mathcal{O}}\\left(\\frac{1}{n K^{2a-2}}\\right).\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "The goal of this section is to prove this theorem, whose precise statement is in Theorem F.5. As the polynomial decay will dominate the exponential decay for large enough $K$ , the bound we get is essentially $\\tilde{O}\\left(1/n K^{2a-2}\\right)$ . Recall that for SEG-FF and SEG-RR we have $a=2$ (by Theorems E.15 and E.16) which leads to an upper bound of $\\bar{\\mathcal{O}}(1/n K^{2})$ , whereas for SEG-FFA we have $a=3$ (by Theorems E.9 and E.13) which gives an upper bound of $\\bar{\\mathcal{O}}(1/n K^{4})$ ", "page_idx": 48}, {"type": "text", "text": "As also mentioned in the beginning of Appendix $\\boldsymbol{\\mathrm E}$ , for any of SEG-RR, SEG-FF, and SEG-FFA, we can decompose the update across the epoch into a deterministic EG update plus a noise. In this section, letting $\\pmb{w}_{\\dag}^{k}:=\\pmb{z}^{k}-\\eta_{k}n\\pmb{F}\\pmb{z}^{k}$ ,wedefine ${\\widehat{\\pmb F}}^{k}$ by the relation $\\eta_{k}n\\widehat{\\pmb{F}}^{k}=\\eta_{k}n F\\pmb{w}_{\\dag}^{k}+\\pmb{r}^{k}$ so that ", "page_idx": 48}, {"type": "equation", "text": "$$\nz^{k+1}=z^{k}-\\eta_{k}n\\widehat{\\pmb{F}}^{k}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Proposition F.2. Let $\\pmb{F}$ be $\\mu$ -strongly monotone with $\\mu>0$ Then, for any $\\eta_{k}>0$ it holds that ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\eta_{k}^{2}n^{2}\\left(1-\\displaystyle\\frac{3}{2}\\mu\\eta_{k}n-\\left(1+\\displaystyle\\frac{1}{2}\\mu\\eta_{k}n\\right)\\eta_{k}^{2}n^{2}L^{2}\\right)\\left\\Vert F z^{k}\\right\\Vert^{2}}\\\\ {\\leq\\left(1-\\displaystyle\\frac{1}{2}\\mu\\eta_{k}n\\right)\\left\\Vert z^{k}-z^{*}\\right\\Vert^{2}-\\left\\Vert z^{k+1}-z^{*}\\right\\Vert^{2}+\\displaystyle\\frac{2+\\mu\\eta_{k}n}{\\mu\\eta_{k}n}\\left\\Vert r^{k}\\right\\Vert^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Proof. From (88), using Lemma C.7 we get ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\|z^{k+1}-z^{*}\\right\\|^{2}=\\left\\|z^{k}-z^{*}\\right\\|^{2}-2\\left\\langle\\eta_{k}n\\widehat{F}^{k},z^{k}-z^{*}\\right\\rangle+\\left\\|\\eta_{k}n\\widehat{F}^{k}\\right\\|^{2}}&{}\\\\ {=\\left\\|z^{k}-z^{*}\\right\\|^{2}-2\\eta_{k}n\\left\\langle F w_{\\dagger}^{k},w_{\\dagger}^{k}-z^{*}\\right\\rangle-2\\eta_{k}^{2}n^{2}\\left\\langle F w_{\\dagger}^{k},F z^{k}\\right\\rangle}&{}\\\\ {-\\left.2\\left\\langle r^{k},z^{k}-z^{*}\\right\\rangle+\\left\\|\\eta_{k}n\\widehat{F}^{k}\\right\\|^{2}}&{}\\\\ {\\leq\\left\\|z^{k}-z^{*}\\right\\|^{2}-\\mu\\eta_{k}n\\left\\|z^{k}-z^{*}\\right\\|^{2}-2\\eta_{k}^{2}n^{2}\\left\\langle F w_{\\dagger}^{k},F z^{k}\\right\\rangle}&{}\\\\ {-\\left.2\\left\\langle r^{k},z^{k}-z^{*}\\right\\rangle+\\left\\|\\eta_{k}n\\widehat{F}^{k}\\right\\|^{2}+2\\mu\\eta_{k}^{3}n^{3}\\left\\|F z^{k}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Meanwhile, using the polarization identity (Lemma C.1) and the $L$ -smoothnessof $\\pmb{F}$ weget ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-2\\left\\langle F w_{\\dagger}^{k},F z^{k}\\right\\rangle=\\left\\|F w_{\\dagger}^{k}-F z^{k}\\right\\|^{2}-\\left\\|F w_{\\dagger}^{k}\\right\\|^{2}-\\left\\|F z^{k}\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq L^{2}\\left\\|w_{\\dagger}^{k}-z^{k}\\right\\|^{2}-\\left\\|F w_{\\dagger}^{k}\\right\\|^{2}-\\left\\|F z^{k}\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq-(1-\\eta_{k}^{2}n^{2}L^{2})\\left\\|F z^{k}\\right\\|^{2}-\\left\\|F w_{\\dagger}^{k}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Combining the two inequalities and using the definition of $\\widehat F$ weobtain ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|z^{k+1}-z^{*}\\right|\\right|^{2}\\leq\\left(1-\\mu\\eta_{k}n\\right)\\left\\Vert z^{k}-z^{*}\\right\\Vert^{2}-\\eta_{k}^{2}n^{2}(1-\\eta_{k}^{2}n^{2}L^{2})\\left\\Vert F z^{k}\\right\\Vert^{2}-\\eta_{k}^{2}n^{2}\\left\\Vert F w_{\\dagger}^{k}\\right\\Vert^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad-2\\left\\langle r^{k},z^{k}-z^{*}\\right\\rangle+\\left\\Vert\\eta_{k}n F w_{\\dagger}^{k}+r^{k}\\right\\Vert^{2}+2\\mu\\eta_{k}^{3}n^{3}\\left\\Vert F z^{k}\\right\\Vert}\\\\ &{\\qquad\\qquad\\leq\\left(1-\\mu\\eta_{k}n\\right)\\left\\Vert z^{k}-z^{*}\\right\\Vert^{2}-\\eta_{k}^{2}n^{2}(1-2\\mu\\eta_{k}n-\\eta_{k}^{2}n^{2}L^{2})\\left\\Vert F z^{k}\\right\\Vert^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad-2\\left\\langle r^{k},z^{k}-z^{*}\\right\\rangle+2\\left\\langle r^{k},\\eta_{k}n F w_{\\dagger}^{k}\\right\\rangle+\\left\\Vert r^{k}\\right\\Vert^{2}}\\\\ &{\\qquad\\qquad\\leq\\left(1-\\mu\\eta_{k}n\\right)\\left\\Vert z^{k}-z^{*}\\right\\Vert^{2}-\\eta_{k}^{2}n^{2}(1-2\\mu\\eta_{k}n-\\eta_{k}^{2}n^{2}L^{2})\\left\\Vert F z^{k}\\right\\Vert^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad-2\\left\\langle r^{k},z^{k}-\\eta_{k}n F w_{\\dagger}^{k}-z^{*}\\right\\rangle+\\left\\Vert r^{k}\\right\\Vert^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Let us consider the inner product term in the last line above. By Lemma C.2 and the nonexpansiveness of the EG update (Lemma C.10), for any $\\gamma_{k}>0$ wehave ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle-2\\left<r^{k},z^{k}-\\eta_{k}n F w_{\\dagger}^{k}-z^{*}\\right>\\leq\\displaystyle\\frac{1}{\\gamma_{k}}\\left\\|r^{k}\\right\\|^{2}+\\gamma_{k}\\left\\|z^{k}-\\eta_{k}n F w_{\\dagger}^{k}-z^{*}\\right\\|^{2}}\\\\ {\\displaystyle\\leq\\frac{1}{\\gamma_{k}}\\left\\|r^{k}\\right\\|^{2}+\\gamma_{k}\\left\\|z^{k}-z^{*}\\right\\|^{2}-\\gamma_{k}\\eta_{k}^{2}n^{2}(1-\\eta_{k}^{2}n^{2}L^{2})\\left\\|F z^{k}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Plugging this back we get ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\eta_{k}^{2}n^{2}(1+\\gamma_{k}-2\\mu\\eta_{k}n-(1+\\gamma_{k})\\eta_{k}^{2}n^{2}L^{2})\\left\\Vert F z^{k}\\right\\Vert^{2}}\\\\ &{\\qquad\\leq(1+\\gamma_{k}-\\mu\\eta_{k}n)\\left\\Vert z^{k}-z^{*}\\right\\Vert^{2}-\\left\\Vert z^{k+1}-z^{*}\\right\\Vert^{2}+\\left(1+\\frac{1}{\\gamma_{k}}\\right)\\left\\Vert r^{k}\\right\\Vert^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Choosing $\\begin{array}{r}{\\gamma_{k}=\\frac{\\mu\\eta_{k}n}{2}}\\end{array}$ completes the proof. ", "page_idx": 49}, {"type": "text", "text": "Proposition F.3. Let $\\pmb{F}$ bea $\\mu$ -stronglymonotone and $L$ -Lipschitz operator. Then, whenever $\\begin{array}{r}{\\eta_{k}\\doteq\\frac{1}{n L\\sqrt{2}}}\\end{array}$ it holds that ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\left\\|{\\cal F}z^{k+1}\\right\\|\\leq\\left(1-\\frac{\\mu n\\eta_{k}}{5}\\right)\\left\\|{\\cal F}z^{k}\\right\\|+L\\left\\|r^{k}\\right\\|.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Proof. Let $z_{\\dagger}^{k+1}:=z^{k}-\\eta_{k}n\\mathbf{F}(z^{k}-\\eta_{k}n\\mathbf{F}z^{k})$ so that we have $\\left\\|z^{k+1}-z_{\\dagger}^{k+1}\\right\\|=\\left\\|r^{k}\\right\\|$ Then, the $L$ -smoothness of $\\pmb{F}$ and Lemma C.8 implies ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\|F z^{k+1}\\right\\|\\leq\\left\\|F z^{k+1}-F z_{\\dagger}^{k+1}\\right\\|+\\left\\|F z_{\\dagger}^{k+1}\\right\\|}&{}\\\\ {\\leq L\\left\\|z^{k+1}-z_{\\dagger}^{k+1}\\right\\|+\\left\\|F z_{\\dagger}^{k+1}\\right\\|}&{}\\\\ {\\leq L\\left\\|r^{k}\\right\\|+\\left(1-\\frac{2\\mu\\eta_{k}n}{5}\\right)^{1/2}\\left\\|F z^{k}\\right\\|}&{}\\\\ {\\leq L\\left\\|r^{k}\\right\\|+\\left(1-\\frac{\\mu\\eta_{k}n}{5}\\right)\\left\\|F z^{k}\\right\\|}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where in the last line we apply a simple inequality $1-2x\\leq(1-x)^{2}$ which holds for all $x\\in\\mathbb R$ \uff1a\u53e3 ", "page_idx": 49}, {"type": "text", "text": "Lemma F4. Suppose that (31) holds. Say we use a constant stepsize $\\eta_{k}\\,=\\,\\eta_{!}$ where $\\eta$ satisfies \u2264nL2 and ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\eta^{a-1}n^{a-1}\\leq\\frac{1}{10}\\operatorname*{min}\\left\\{\\frac{1}{L^{2}},\\frac{\\mu}{L(C_{1}+D_{1}(\\|\\mathbf{F}z_{0}\\|+V_{1}/\\mu L))}\\right\\}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Then for any $k=0,1,\\dots$ the following two inequalities bothhold: ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|F z^{k+1}\\|\\leq\\left(1-\\frac{\\mu\\eta n}{10}\\right)\\|F z^{k}\\|+\\eta^{a}n^{a}L V_{1},}\\\\ &{\\quad\\|F z^{k}\\|\\leq\\|F z^{0}\\|+\\frac{V_{\\eta}}{\\mu L}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Proof. For the case $k=0$ , the inequality (93) clearly holds. For the remaining cases, we use strong induction on $k$ . More precisely, assuming that (93) holds for all $0,1,\\ldots,k$ , we will show that (92) holds, and from that the inequality ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\left\\|F z^{k+1}\\right\\|\\leq\\left\\|F z^{0}\\right\\|+{\\frac{V_{1}}{\\mu L}}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "follows. To this end, let us begin from noting that Proposition F3, (31), and the induction hypothesis (93) implies ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|F z^{k+1}\\right\\|\\leq\\left(1-\\frac{\\mu\\eta n}{5}\\right)\\left\\|F z^{k}\\right\\|+\\eta^{a}n^{a}L\\left(C_{1}\\left\\|F z^{k}\\right\\|+D_{1}\\left\\|F z^{k}\\right\\|^{2}+V_{1}\\right)}\\\\ &{\\qquad\\qquad\\leq\\left(1-\\frac{\\mu\\eta n}{5}+\\eta^{a}n^{a}L C_{1}+\\eta^{a}n^{a}L D_{1}\\left(\\left\\|F z^{0}\\right\\|+\\frac{V_{1}}{\\mu L}\\right)\\right)\\left\\|F z^{k}\\right\\|+\\eta^{a}n^{a}L V_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Here, from the choice of the stepsize (91), we have ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\eta^{a}n^{a}L C_{1}+\\eta^{a}n^{a}L D_{1}\\left(\\left\\|{\\cal F}z^{0}\\right\\|+{\\frac{V_{1}}{\\mu L}}\\right)\\leq{\\frac{\\mu\\eta n}{10}}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Hence, from (95) we get ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\|F z^{k+1}\\|\\leq\\left(1-\\frac{\\mu\\eta n}{10}\\right)\\|F z^{k}\\|+\\eta^{a}n^{a}L V_{1}.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "which is exactly (92). Now, considering that we are assuming (93) holds for all $0,1,\\ldots,k$ ,wemust also have (92) for all $0,1,\\ldots,k$ . Thus we can unravel the recurrence to get ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|F z^{k+1}\\|\\leq\\Big(1-\\frac{\\mu\\eta n}{10}\\Big)\\,\\|F z^{k}\\|+\\eta^{a}n^{a}L V_{1}}\\\\ &{\\qquad\\qquad\\leq\\Big(1-\\frac{\\mu\\eta n}{10}\\Big)^{2}\\,\\|F z^{k-1}\\|+\\Big(1-\\frac{\\mu\\eta n}{10}\\Big)\\,\\eta^{a}n^{a}L V_{1}+\\eta^{a}n^{a}L V_{1}}\\\\ &{\\qquad\\leq\\dots}\\\\ &{\\qquad\\leq\\Big(1-\\frac{\\mu\\eta n}{10}\\Big)^{k+1}\\,\\|F z^{0}\\|+\\eta^{a}n^{a}L V_{1}\\underset{j=0}{\\overset{k}{\\sum}}\\Big(1-\\frac{\\mu\\eta n}{10}\\Big)^{j}}\\\\ &{\\qquad\\leq\\|F z^{0}\\|+\\frac{\\eta^{a}n^{a}L V_{1}}{1-\\big(1-\\frac{\\mu\\eta n}{10}\\big)}}\\\\ &{\\qquad=\\|F z^{0}\\|+\\frac{10\\eta^{a-1}n^{a-1}L V_{1}}{\\mu}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "As (91) also implies $10\\eta^{a-1}n^{a-1}L\\leq1/L$ we obtain (94), as claimed. This completes the proof. ", "page_idx": 50}, {"type": "text", "text": "Theorem F.5 (Theorem F.1). Suppose that $\\pmb{F}$ is $\\mu$ -stronglymonotonewith $\\mu>0$ Assumption 3.3 holds, and an optimization method whose within-epoch error satisfies (31) and (32) for some constant $a>0$ isrunfor $K$ epochs. Let us define a constant ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\varPhi:=C_{2}+D_{2}\\left(\\left\\|F z^{0}\\right\\|+\\frac{V_{\\!\\;\\prime}}{\\mu L}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Say we use a constant stepsize $\\eta_{k}=\\eta,$ where $\\eta$ is chosen as ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\eta=\\operatorname*{min}\\bigg\\{\\frac{2}{5n L},}\\\\ &{\\qquad\\qquad\\quad\\frac{1}{n(10L^{2})^{1/(a-1)}},}\\\\ &{\\qquad\\qquad\\quad\\frac{\\mu^{1/(a-1)}}{n(10L(C_{1}+D_{1}(\\lfloor{\\|{F z_{0}}\\|+V\\ i/\\mu L})))^{1/(a-1)}},}\\\\ &{\\qquad\\qquad\\quad\\frac{1}{(12\\phi/\\mu)^{1/(2a-3)}n},}\\\\ &{\\qquad\\qquad\\quad\\frac{4(\\alpha-1)\\log(n^{1/(2a-2)}K)}{\\mu n K}\\bigg\\}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Thenfor $\\omega$ denoting the minimum among (97a)-(97d), it holds that ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left\\|z^{K}-z^{*}\\right\\|^{2}\\leq\\exp\\left(-\\frac{1}{2}\\mu\\omega n K\\right)\\left\\|z^{0}-z^{*}\\right\\|^{2}+\\mathcal{O}\\left(\\frac{\\left(\\log(n^{1/(2a-2)}K)\\right)^{2a-2}}{n K^{2a-2}}\\right).\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "As a reminder, for SEG-FF and SEG-RR we have $a=2$ , and for SEG-FFA we have $a=3$ ", "page_idx": 50}, {"type": "text", "text": "Proof Note that 97b) and (97 together implies 91, and that $\\begin{array}{r}{\\eta_{k}=\\eta\\le\\frac{2}{5n L}\\le\\frac{1}{n L\\sqrt{2}}<\\frac{1}{n L}}\\end{array}$ So, we can utilize (32) and Lemma F.4 to get ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert r\\right\\Vert^{2}\\middle|z^{k}\\right]\\leq\\eta^{2a}n^{2a}C_{2}\\left\\Vert F z^{k}\\right\\Vert^{2}+\\eta^{2a}n^{2a}D_{2}\\left\\Vert F z^{k}\\right\\Vert^{4}+\\eta^{2a}n^{2a-1}V_{2}}\\\\ &{\\qquad\\qquad\\leq\\eta^{2a}n^{2a}C_{2}\\left\\Vert F z^{k}\\right\\Vert^{2}+\\eta^{2a}n^{2a}D_{2}\\left(\\left\\Vert F z^{0}\\right\\Vert+\\frac{V_{1}}{\\mu L}\\right)^{2}\\left\\Vert F z^{k}\\right\\Vert^{2}+\\eta^{2a}n^{2a-1}V_{2}}\\\\ &{\\qquad\\qquad=\\eta^{2a}n^{2a}\\phi\\left\\Vert F z^{k}\\right\\Vert^{2}+\\eta^{2a}n^{2a-1}V_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Taking the conditional expectation on (89) and applying the bound just derived, we obtain ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\eta^{2}n^{2}\\left(1-\\displaystyle\\frac{3}{2}\\mu\\eta n-\\left(1+\\displaystyle\\frac{1}{2}\\mu\\eta n\\right)\\eta^{2}n^{2}L^{2}\\right)\\left\\Vert F z^{k}\\right\\Vert^{2}}\\\\ &{\\qquad\\leq\\left(1-\\displaystyle\\frac{1}{2}\\mu\\eta n\\right)\\left\\Vert z^{k}-z^{*}\\right\\Vert^{2}-\\mathbb{E}\\left[\\left\\Vert z^{k+1}-z^{*}\\right\\Vert^{2}\\Big|z^{k}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\left.\\frac{2+\\mu\\eta n}{\\mu}\\left(\\eta^{2a-1}n^{2a-1}\\varPhi\\left\\Vert F z^{k}\\right\\Vert^{2}+\\eta^{2a-1}n^{2a-2}V_{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "A simple rearrangement of the terms leads to ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\eta^{2}n^{2}\\left(1-\\frac{3}{2}\\mu\\eta n-\\left(1+\\frac{1}{2}\\mu\\eta n\\right)\\eta^{2}n^{2}L^{2}-\\frac{2+\\mu\\eta n}{\\mu}\\cdot\\eta^{2a-3}n^{2a-3}\\phi\\right)\\left\\Vert F z^{k}\\right\\Vert^{2}}\\\\ {\\displaystyle\\leq\\left(1-\\frac{1}{2}\\mu\\eta n\\right)\\left\\Vert z^{k}-z^{*}\\right\\Vert^{2}-\\mathbb{E}\\left[\\left\\Vert z^{k+1}-z^{*}\\right\\Vert^{2}\\left\\vert z^{k}\\right\\vert+\\frac{2+\\mu\\eta n}{\\mu}\\cdot\\eta^{2a-1}n^{2a-2}V_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Notice that by assuming (97a) and (97d), it holds that ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{3}{2}\\mu\\eta n+\\left(1+\\frac{1}{2}\\mu\\eta n\\right)\\eta^{2}n^{2}L^{2}+\\frac{2+\\mu\\eta n}{\\mu}\\cdot\\eta^{2a-3}n^{2a-3}\\varPhi}\\\\ {\\leq\\frac{3}{2}\\cdot\\frac{2}{5}+\\left(1+\\frac{1}{2}\\cdot\\frac{2}{5}\\right)\\left(\\frac{2}{5}\\right)^{2}+\\frac{12\\varPhi}{5\\mu}\\cdot\\frac{\\mu}{12\\varPhi}=\\frac{124}{125},}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "so we can guarantee that the left hand side of (99) is nonnegative. It then follows that ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert z^{k+1}-z^{*}\\right\\Vert^{2}\\Big|\\,z^{k}\\right]\\le\\left(1-\\frac{1}{2}\\mu\\eta n\\right)\\left\\Vert z^{k}-z^{*}\\right\\Vert^{2}+\\frac{2+\\mu\\eta n}{\\mu}\\cdot\\eta^{2a-1}n^{2a-2}V_{2}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Applying the law of total expectation, from the above we obtain ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left\\|z^{k+1}-z^{*}\\right\\|^{2}\\leq\\left(1-\\frac{1}{2}\\mu\\eta n\\right)\\mathbb{E}\\left\\|z^{k}-z^{*}\\right\\|^{2}+\\frac{2+\\mu\\eta n}{\\mu}\\cdot\\eta^{2a-1}n^{2a-2}V_{2}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "We can now unravel this recurrence over $k=0,1,\\ldots,K-1$ as done in (96) to get ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left\\|z^{K}-z^{*}\\right\\|^{2}\\leq\\left(1-\\displaystyle\\frac{1}{2}\\mu\\eta n\\right)\\mathbb{E}\\left\\|z^{K-1}-z^{*}\\right\\|^{2}+\\frac{2+\\mu\\eta n}{\\mu}\\cdot\\eta^{2a-1}n^{2a-2}V_{2}}\\\\ {\\leq\\cdots}\\\\ &{\\leq\\left(1-\\displaystyle\\frac{1}{2}\\mu\\eta n\\right)^{K}\\left\\|z^{0}-z^{*}\\right\\|^{2}+\\frac{2+\\mu\\eta n}{\\mu}\\cdot\\eta^{2a-1}n^{2a-2}V_{2}\\sum_{j=0}^{K-1}\\left(1-\\displaystyle\\frac{1}{2}\\mu\\eta n\\right)^{j}}\\\\ &{\\leq\\left(1-\\displaystyle\\frac{1}{2}\\mu\\eta n\\right)^{K}\\left\\|z^{0}-z^{*}\\right\\|^{2}+\\frac{4+2\\mu\\eta n}{\\mu^{2}\\eta n}\\cdot\\eta^{2a-1}n^{2a-2}V_{2}}\\\\ &{\\leq\\exp\\left(-\\displaystyle\\frac{1}{2}\\mu\\eta n K\\right)\\left\\|z^{0}-z^{*}\\right\\|^{2}+\\frac{24}{5\\mu^{2}}\\cdot\\eta^{2a-2}n^{2a-3}V_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "where in the last line we used the basic inequality $1+x\\leq e^{x}$ which holds for all $x\\in\\mathbb R$ .With the choice of the stepsize (97e), we arrive at ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\updownarrow\\left\\Vert z^{K}-z^{*}\\right\\Vert^{2}\\leq\\exp\\left(-\\frac{1}{2}\\mu\\eta n K\\right)\\left\\Vert z^{0}-z^{*}\\right\\Vert^{2}+\\frac{24\\cdot(4a-4)^{2a-2}V_{2}}{5\\mu^{2a}}\\cdot\\frac{\\left(\\log(n^{1/(2a-2)}K)\\right)^{2a-2}}{n K^{2a-2}}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Now,recall that $\\eta$ is chosen to be the smallest one among (97a)-(97e). Notice that the options (97a)-(97d) are independent with respect to $K$ , and (97e) is the only one that depends on $K$ .Let us consider these two cases separately. ", "page_idx": 51}, {"type": "text", "text": "(i $\\eta$ is chosen to be the minimum among (97a)-(97d). ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "This is the case where we have $\\eta=\\omega$ . Notice that the constant $\\omega$ that does not depend on $K$ The inequality (100) then takes the form ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left\\Vert z^{K}-z^{*}\\right\\Vert^{2}\\leq\\exp\\left(-\\frac{\\mu\\omega n K}{2}\\right)\\left\\Vert z^{0}-z^{*}\\right\\Vert^{2}+\\mathcal{O}\\left(\\frac{\\left(\\log(n^{1/(2a-2)}K)\\right)^{2a-2}}{n K^{2a-2}}\\right).\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "(i) nischosen tobe (97e), that is,  = 4(a1)og(n/(2a-2)). ", "page_idx": 52}, {"type": "text", "text": "In this case, the exponential factor of the first term in the right hand side of (1o0) reduces to ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\exp\\left(-\\frac{1}{2}\\mu\\eta n K\\right)=\\frac{1}{n K^{2a-2}}.\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Thus, the second term in (100) dominates the first term, and in total (100) becomes ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left\\|z^{K}-z^{*}\\right\\|^{2}=\\mathcal{O}\\left(\\frac{\\left(\\log(n^{1/(2a-2)}K)\\right)^{2a-2}}{n K^{2a-2}}\\right).\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Therefore, in both cases we have ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left\\Vert z^{K}-z^{*}\\right\\Vert^{2}\\leq\\exp\\left(-\\frac{1}{2}\\mu\\omega n K\\right)\\left\\Vert z^{0}-z^{*}\\right\\Vert^{2}+\\mathcal{O}\\left(\\frac{\\left(\\log(n^{1/(2a-2)}K)\\right)^{2a-2}}{n K^{2a-2}}\\right)\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "which is exactly (98). This completes the proof. ", "page_idx": 52}, {"type": "text", "text": "Remark F.6. To compare the convergence rate of SEG-FFA in the strongly monotone setting with that of SEG-RR by Emmanouilidis et al. [18] more in depth, let us make an estimation on the size of $\\omega$ appearing in Theorem F.5 when $a=3$ ", "page_idx": 52}, {"type": "text", "text": "To this end, we need estimates on the constants $C_{1\\mathsf{A}},D_{1\\mathsf{A}},V_{1\\mathsf{A}},C_{2\\mathsf{A}}$ , and $D_{2{\\tt A}}$ . From their definitions in (59)-(61), (72), and (73) we have $C_{1\\mathsf{A}}\\asymp L^{2}$ \uff0c $D_{1\\mathsf{A}}\\asymp M$ $V_{1\\mathsf{A}}\\asymp M+L^{2}$ \uff0c $C_{2\\mathsf{A}}\\asymp L^{4}$ , and $D_{2\\mathsf{A}}\\asymp M^{2}$ . In general, there is not a direct relation between $L$ and $M$ . For example, recall that if all components are quadratic, then $M=0$ . Meanwhile, Gorbunov et al. [21] has argued that $M$ can be much larger than $L$ in certain cases, by providing an example where $M\\asymp L^{\\bar{3}/2}$ . For our purposes, however, let us allow $M$ to be even as large as $\\bar{M}\\asymp L^{2}$ , so that the situation is simplified into $C_{1\\mathsf{A}}\\asymp D_{1\\mathsf{A}}\\asymp V_{1\\mathsf{A}}\\asymp L^{2}$ and $C_{2\\mathsf{A}}\\asymp D_{2\\mathsf{A}}\\asymp L^{\\frac{\\eta}{4}}$ ", "page_idx": 52}, {"type": "text", "text": "Then, we get the estimate of (97c), ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\frac{\\mu^{1/2}}{n(10L(C_{1\\mathsf{A}}+D_{1\\mathsf{A}}(\\|F z_{0}\\|+V_{1\\mathsf{A}}/\\mu L)))^{1/2}}\\asymp\\frac{\\mu}{n L^{2}}.\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Meanwhile, as for the constant $\\varPhi$ it holds that ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\mathcal{P}=C_{2\\mathsf{A}}+D_{2\\mathsf{A}}\\left(\\left\\|F z^{0}\\right\\|+\\frac{V_{1\\mathsf{A}}}{\\mu L}\\right)^{2}\\asymp\\frac{L^{6}}{\\mu^{2}},\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "for (97d) we have ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\frac{1}{(12\\phi/\\mu)^{1/3}n}\\asymp\\frac{\\mu}{n L^{2}}.\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "As (97a) while (97b) are both $\\Theta(1/n L)$ and $\\mu\\leq L$ , we essentially have $\\omega\\asymp\\mu/n L^{2}$ . Or equivalently, for some $b=\\Theta(1)$ , the convergence rate (98) reads ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left\\|z^{K}-z^{*}\\right\\|^{2}\\leq\\exp\\left(-\\frac{b\\mu^{2}K}{L^{2}}\\right)\\left\\|z^{0}-z^{*}\\right\\|^{2}+\\mathcal{O}\\left(\\frac{\\left(\\log(n^{1/4}K)\\right)^{4}}{n K^{4}}\\right).\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "On the other hand, Theorem 2.1 of [18] states that, for some $b^{\\prime}=\\Theta(1)$ , SEG-RR exhibits a rate of ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left\\|z^{K}-z^{*}\\right\\|^{2}\\leq\\exp\\left(-\\frac{b^{\\prime}\\mu^{2}K}{L^{2}}\\right)\\left\\|z^{0}-z^{*}\\right\\|^{2}+\\mathcal{O}\\left(\\frac{\\left(\\log(n^{1/2}K)\\right)^{2}}{n K^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Comparing (101) with (102), the exponents in the exponentially decaying term are of the same order Of\u2014#\u00b2K , so SEG-FFA having a faster polynomially decaying term $\\tilde{\\mathcal{O}}(^{1}/n K^{4})$ enjoys an improved convergencerate. ", "page_idx": 52}, {"type": "text", "text": "G  Convergence Rate of SEG-FFA in the Monotone Setting ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "G.1  Star-monotonicity ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Notice that we only used Assumptions 3.3 and 3.4 in deriving the results in Appendices D and E, and in particular, the monotonicity assumption on $\\pmb{F}$ was not necessary. Moreover, among the lemmata listed in Appendix C, Lemma C.10 is the only one that possibly uses the (non-strongly) monotone assumption, but that lemma is not used in this section. ", "page_idx": 53}, {"type": "text", "text": "In fact, as it turns out in Appendix G.2, in the convergence analysis of SEG-FFA, we need not fully exploit the inequality (3) provided by the monotonicity assumption. Rather, all the results on the performance of SEG-FFA can be established with only assuming the following condition (which has been also briefly mentioned in Appendix B.2). ", "page_idx": 53}, {"type": "text", "text": "Assumption G.1 (Star-monotonicity). Given an operator $\\pmb{F}$ with a point $\\boldsymbol{z}^{\\ast}\\in\\mathbb{R}^{d_{1}+d_{2}}$ such that $\\pmb{F}\\pmb{z}^{*}=\\mathbf{0}$ wesaythat $\\pmb{F}$ is star-monotone if, for any $z\\in\\mathbb{R}^{d_{1}+d_{2}}$ ,it holds that ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\langle F z,z-z^{*}\\rangle\\geq0.\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Monotone and strongly-monotone operators are clearly star-monotone, as they satisfy (3). On the other hand, there exist operators that are star-monotone but not monotone: see, e.g., [31, Appendix A.6]. ", "page_idx": 53}, {"type": "text", "text": "Recall that when $\\pmb{F}$ is monotone, Assumption 3.2 is equivalent to assuming the existence of a point $z^{\\ast}$ that satisfies $\\pmb{F}\\pmb{z}^{*}=\\mathbf{0}$ . Hence, after simply replacing the optimality condition in Assumption 3.2 With $\\pmb{F}z^{*}=\\mathbf{0}$ , our convergence analyses not only will show that our SEG-FFA finds an optimum on monotone problems, but also that it can be also used to find stationary points in \u201cstar-monotone' problems, allowing the objective function $f$ to be nonconvex-nonconcave. ", "page_idx": 53}, {"type": "text", "text": "Star-monotonicity is also known as the variational stability condition [25], and has much been studied in the literature. For further details on star-monotonicity, we refer to [25, 31] and the references therein. ", "page_idx": 53}, {"type": "text", "text": "G.2  Convergence Analysis of SEG-FFA in the (Star-)Monotone Setting ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Let us in particular consider SEG-FFA. As in the previous section, we focus only on the iterates $\\{z_{0}^{k}\\}_{k\\ge0}$ , so again, we omit the subscript O unless necessary, and simply write $z^{k}$ instead of $z_{0}^{k}$ ", "page_idx": 53}, {"type": "text", "text": "Decompose the update across the epoch into a deterministic EG update plus a noise, as ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{{\\pmb w}_{\\dagger}^{k}:=z^{k}-\\eta_{k}n{\\pmb F}z^{k},}}\\\\ {{z^{k+1}=z^{k}-\\eta_{k}n{\\widehat{\\pmb F}}^{k}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "for ${\\widehat{F}}^{k}$ defined by the equation ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\eta_{k}n\\widehat{\\pmb{F}}^{k}=\\eta_{k}n\\pmb{F}\\pmb{w}_{\\dag}^{k}+\\pmb{r}^{k}.\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Lemma G.2. Let $\\pmb{F}$ be a (star-)monotone operator with a point $z^{*}$ that satisfies $\\pmb{F}z^{*}=\\mathbf{0}$ and suppose that Assumption 3.3 holds. Then for any $\\eta_{k}>0$ and $\\gamma_{k}>0$ it holds that ", "page_idx": 53}, {"type": "equation", "text": "$$\n0\\leq\\left\\Vert z^{k}-z^{*}\\right\\Vert^{2}-\\frac{1}{1+\\gamma_{k}}\\left\\Vert z^{k+1}-z^{*}\\right\\Vert^{2}-\\eta_{k}^{2}n^{2}(1-\\eta_{k}^{2}n^{2}L^{2})\\left\\Vert F z^{k}\\right\\Vert^{2}+\\frac{1}{\\gamma_{k}}\\left\\Vert r^{k}\\right\\Vert^{2}.\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Proof. By (104) and (105) we get ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|z^{k+1}-z^{*}\\right|^{\\right|2}=\\left\\|z^{k}-\\eta_{k}n\\widehat{F}^{k}-z^{*}\\right\\|^{2}}\\\\ &{\\qquad\\qquad=\\left\\|z^{k}-z^{*}\\right\\|^{2}-2\\left\\langle\\eta_{k}n\\widehat{F}^{k},z^{k}-z^{*}\\right\\rangle+\\left\\|\\eta_{k}n\\widehat{F}^{k}\\right\\|^{2}}\\\\ &{\\qquad\\qquad=\\left\\|z^{k}-z^{*}\\right\\|^{2}-2\\left\\langle\\eta_{k}n F w_{\\dagger}^{k},w_{\\dagger}^{k}-z^{*}\\right\\rangle-2\\left\\langle\\eta_{k}n F w_{\\dagger}^{k},z^{k}-w_{\\dagger}^{k}\\right\\rangle-2\\left\\langle r^{k},z^{k}-z^{*}\\right\\rangle}\\\\ &{\\qquad\\qquad\\qquad+\\left\\|\\eta_{k}n F w_{\\dagger}^{k}\\right\\|^{2}+2\\left\\langle r^{k},\\eta_{k}n F w_{\\dagger}^{k}\\right\\rangle+\\left\\|r^{k}\\right\\|^{2}}\\\\ &{\\qquad\\qquad=\\left\\|z^{k}-z^{*}\\right\\|^{2}-2\\eta_{k}n\\left\\langle F w_{\\dagger}^{k},w_{\\dagger}^{k}-z^{*}\\right\\rangle-2\\left\\langle\\eta_{k}n F w_{\\dagger}^{k},\\eta_{k}n F z^{k}\\right\\rangle}\\\\ &{\\qquad\\qquad\\qquad+\\left\\|\\eta_{k}n F w_{\\dagger}^{k}\\right\\|^{2}-2\\left\\langle r^{k},z^{k}-\\eta_{k}n F w_{\\dagger}^{k}-z^{*}\\right\\rangle+\\left\\|r^{k}\\right\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\left\\|z^{k}-z^{*}\\right\\|^{2}-2\\eta_{k}n\\left\\langle F w_{\\dagger}^{k},w_{\\dagger}^{k}-z^{*}\\right\\rangle-2\\eta_{k}^{2}n^{2}\\left\\langle F w_{\\dagger}^{k},F z^{k}\\right\\rangle}\\\\ &{\\qquad+\\eta_{k}^{2}n^{2}\\left\\|F w_{\\dagger}^{k}\\right\\|^{2}-2\\left\\langle r^{k},z^{k+1}-z^{*}\\right\\rangle-\\left\\|r^{k}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "We now bound the inner products. On one hand, by the polarization identity (Lemma C.1) and the $L$ -smoothnessof $f$ wehave ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-2\\left\\langle F{\\pmb w}_{\\dagger}^{k},F z^{k}\\right\\rangle=\\left\\|F{\\pmb w}_{\\dagger}^{k}-F z^{k}\\right\\|^{2}-\\left\\|F{\\pmb w}_{\\dagger}^{k}\\right\\|^{2}-\\left\\|F z^{k}\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq L^{2}\\left\\|-\\eta_{k}n F z^{k}\\right\\|^{2}-\\left\\|F w_{\\dagger}^{k}\\right\\|^{2}-\\left\\|F z^{k}\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad=-(1-\\eta_{k}^{2}n^{2}L^{2})\\left\\|F z^{k}\\right\\|^{2}-\\left\\|F w_{\\dagger}^{k}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "On the other hand, by the weighted AM-GM inequality (Lemma C.2), for any number $a_{k}\\in(0,1)$ it holds that ", "page_idx": 54}, {"type": "equation", "text": "$$\n-2\\left\\langle r^{k},z^{k+1}-z^{*}\\right\\rangle\\leq\\frac{1}{a_{k}}\\left\\|r^{k}\\right\\|^{2}+a_{k}\\left\\|z^{k+1}-z^{*}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Using these two bounds, we get ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|z^{k+1}-z^{*}\\right\\|^{2}\\leq\\left\\|z^{k}-z^{*}\\right\\|^{2}-2\\eta_{k}n\\left\\langle F w_{\\dagger}^{k},w_{\\dagger}^{k}-z^{*}\\right\\rangle-\\eta_{k}^{2}n^{2}(1-\\eta_{k}^{2}n^{2}L^{2})\\left\\|F z^{k}\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad-\\eta_{k}^{2}n^{2}\\left\\|F w_{\\dagger}^{k}\\right\\|^{2}+\\eta_{k}^{2}n^{2}\\left\\|F w_{\\dagger}^{k}\\right\\|^{2}+a_{k}\\left\\|z^{k+1}-z^{*}\\right\\|^{2}+\\left(\\frac{1}{a_{k}}-1\\right)\\left\\|r^{k}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Choosing $\\begin{array}{r}{a_{k}=\\frac{\\gamma_{k}}{1+\\gamma_{k}}}\\end{array}$ and rearranging the terms, we obtain ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{2\\eta_{k}n\\left\\langle F w_{\\dagger}^{k},w_{\\dagger}^{k}-z^{*}\\right\\rangle\\leq\\left\\|z^{k}-z^{*}\\right\\|^{2}-\\displaystyle\\frac{1}{1+\\gamma_{k}}\\left\\|z^{k+1}-z^{*}\\right\\|^{2}}}\\\\ {{\\qquad\\qquad\\qquad\\qquad\\qquad-\\eta_{k}^{2}n^{2}(1-\\eta_{k}^{2}n^{2}L^{2})\\left\\|F z^{k}\\right\\|^{2}+\\displaystyle\\frac{1}{\\gamma_{k}}\\left\\|r^{k}\\right\\|^{2}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "The left hand side of (107) is nonnegative by the star-monotonicity of $\\pmb{F}$ (103), and the claimed inequality follows. \u53e3 ", "page_idx": 54}, {"type": "text", "text": "Now we show that choosing the appropriate stepsizes leads to $\\|\\pmb{F}z^{k}\\|$ being bounded uniformly over $k$ ", "page_idx": 54}, {"type": "text", "text": "Proposition G.3. Let $\\pmb{F}$ be a (star-)monotone operator with a point $z^{*}$ thatsatisfies $\\pmb{F}z^{*}=\\mathbf{0}$ and suppose that Assumptions 3.3 and 3.4 hold. Say we are using SEG-FFA, or any optimization method whose within-epoch error satisfies (58) and (71). Let the sequence of stepsizes $\\{\\eta_{k}\\}_{k\\ge0}$ be nonincreasing,with ", "page_idx": 54}, {"type": "equation", "text": "$$\nS:=\\sum_{k=0}^{\\infty}\\eta_{k}^{3}n^{3}L^{3}<\\infty.\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Suppose that initial stepsize $\\eta_{0}$ is chosen sufficiently small so that ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\eta_{0}^{2}n^{2}L^{2}+\\frac{3\\eta_{0}n C_{\\;t A}^{2}}{L^{3}}+\\frac{3\\eta_{0}n D_{\\;t A}^{2}}{L}\\cdot e^{S}\\left(\\left\\|z^{0}-z^{*}\\right\\|^{2}+\\frac{6S V_{\\;t A}^{2}}{L^{6}}\\right)\\leq1\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "for constants $C_{1A},\\,D_{7A},$ and $V_{I A}$ defined in (59)-(61). Then for all $k\\geq0$ ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\left\\|F z^{k}\\right\\|^{2}\\leq e^{S}L^{2}\\left(\\left\\|z^{0}-z^{*}\\right\\|^{2}+\\frac{6S V_{t A}^{2}}{L^{6}}\\right).\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Proof. We use induction on $k$ , to establish a stronger inequality ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\left\\|z^{k}-z^{*}\\right\\|^{2}\\leq e^{S}\\left(\\left\\|z^{0}-z^{*}\\right\\|^{2}+\\frac{6S V_{1{\\mathsf{A}}}^{2}}{L^{6}}\\right).\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "To see that (111) indeed implies (110), notice that by the $L$ -smoothness of $f$ it holds that ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\left\\|F z^{k}\\right\\|^{2}=\\left\\|F z^{k}-F z^{*}\\right\\|^{2}\\leq L^{2}\\left\\|z^{k}-z^{*}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "For the case when $k=0$ ,as $S>0$ , it is clear that (111) holds. Now suppose that (111) holds for some $k\\geq0$ . Applying Young's inequality on (31) leads to ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\left\\|r^{k}\\right\\|^{2}\\leq3\\eta_{k}^{6}n^{6}\\left(C_{1\\mathsf{A}}^{2}\\left\\|F z^{k}\\right\\|^{2}+D_{1\\mathsf{A}}^{2}\\left\\|F z^{k}\\right\\|^{4}+V_{1\\mathsf{A}}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Applying this bound on $\\left\\|r^{k}\\right\\|^{2}$ on (106), we obtain ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\eta_{k}^{2}n^{2}\\left(1-\\eta_{k}^{2}n^{2}L^{2}-\\frac{3\\eta_{k}^{4}n^{4}C_{1\\mathsf{A}}^{2}}{\\gamma_{k}}-\\frac{3\\eta_{k}^{4}n^{4}D_{1\\mathsf{A}}^{2}}{\\gamma_{k}}\\left\\Vert F z^{k}\\right\\Vert^{2}\\right)\\left\\Vert F z^{k}\\right\\Vert^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\left\\Vert z^{k}-z^{*}\\right\\Vert^{2}-\\frac{1}{1+\\gamma_{k}}\\left\\Vert z^{k+1}-z^{*}\\right\\Vert^{2}+\\frac{3\\eta_{k}^{6}n^{6}V_{1\\mathsf{A}}^{2}}{\\gamma_{k}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Choose $\\gamma_{k}=\\eta_{k}^{3}n^{3}L^{3}$ . Notice that (109) implies $\\eta_{0}n L\\leq1$ henceforth $\\eta_{k}\\leq\\eta_{0}\\leq1/{n L}$ . This, with the induction hypothesis (110), implies ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\eta_{k}^{2}n^{2}L^{2}+\\frac{3\\eta_{k}^{4}n^{4}C_{1\\mathrm{A}}^{2}}{\\gamma_{k}}+\\frac{3\\eta_{k}^{4}n^{4}D_{1\\mathrm{A}}^{2}}{\\gamma_{k}}\\left\\|F z^{k}\\right\\|^{2}}\\\\ &{\\qquad=\\eta_{k}^{2}n^{2}L^{2}+\\frac{3\\eta_{k}n C_{1\\mathrm{A}}^{2}}{L^{3}}+\\frac{3\\eta_{k}n D_{1\\mathrm{A}}^{2}}{L^{3}}\\left\\|F z^{k}\\right\\|^{2}}\\\\ &{\\qquad\\leq\\eta_{0}^{2}n^{2}L^{2}+\\frac{3\\eta_{0}n C_{1\\mathrm{A}}^{2}}{L^{3}}+\\frac{3\\eta_{0}n D_{1\\mathrm{A}}^{2}}{L^{3}}\\left\\|F z^{k}\\right\\|^{2}}\\\\ &{\\qquad\\leq\\eta_{0}^{2}n^{2}L^{2}+\\frac{3\\eta_{0}n C_{1\\mathrm{A}}^{2}}{L^{3}}+\\frac{3\\eta_{0}n D_{1\\mathrm{A}}^{2}}{L^{3}}\\cdot e^{s}L^{2}\\left(\\left\\|z^{0}-z^{*}\\right\\|^{2}+\\frac{6S V_{1\\mathrm{A}}^{2}}{L^{6}}\\right)}\\\\ &{\\qquad<1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "That is, the left hand side of (112) becomes nonnegative. Then it is immediate from (112) that ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|z^{k+1}-z^{*}\\right\\|^{2}\\leq\\left(1+\\gamma_{k}\\right)\\left\\|z^{k}-z^{*}\\right\\|^{2}+\\frac{3\\eta_{k}^{6}n^{6}\\left(1+\\gamma_{k}\\right)V_{1\\mathsf{A}}^{2}}{\\gamma_{k}}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\left(1+\\eta_{k}^{3}n^{3}L^{3}\\right)\\left\\|z^{k}-z^{*}\\right\\|^{2}+\\frac{6\\eta_{k}^{3}n^{3}V_{1\\mathsf{A}}^{2}}{L^{3}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Using Lemma C.11 to unravel this recurrence relation, we obtain ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|z^{k+1}-z^{*}\\right\\|^{2}\\leq\\left(\\displaystyle\\prod_{j=0}^{k}\\left(1+\\eta_{j}^{3}n^{3}L^{3}\\right)\\right)\\left(\\left\\|z^{0}-z^{*}\\right\\|^{2}+\\displaystyle\\sum_{j=0}^{k}\\frac{6\\eta_{j}^{3}n^{3}V_{1A}^{2}}{L^{3}}\\right)}\\\\ &{\\qquad\\qquad\\leq e^{\\sum_{j=0}^{k}\\eta_{j}^{3}n^{3}L^{3}}\\left(\\left\\|z^{0}-z^{*}\\right\\|^{2}+\\displaystyle\\frac{6V_{1A}^{2}}{L^{6}}\\sum_{j=0}^{k}\\eta_{j}^{3}n^{3}L^{3}\\right)}\\\\ &{\\qquad\\qquad\\leq e^{S}\\left(\\left\\|z^{0}-z^{*}\\right\\|^{2}+\\frac{6S V_{1A}^{2}}{L^{6}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "which shows that (111) also holds when $k$ is replaced by $k+1$ . This completes the proof. ", "page_idx": 55}, {"type": "text", "text": "Theorem G.4 (Formal version of Theorem 5.4). Let $\\pmb{F}$ be a (star-)monotone operator with a point $z^{\\ast}$ that satisfies $\\pmb{F}z^{*}=\\mathbf{0}$ , and suppose that Assumptions 3.3 and 3.4 hold. Say that we are using $S E G-F F A,$ . or any optimization method whose within-epoch error satisfies (58) and (71), with $\\begin{array}{r}{\\beta_{k}=\\eta_{k}=\\frac{\\eta_{0}\\sqrt[3]{2}\\log2}{(k+2)^{1/3}\\log(k+2)}}\\end{array}$ and $\\alpha_{k}\\,=\\,\\beta_{k}\\big/2$ for $k=0,1,\\dots.$ where, for $\\begin{array}{r}{S:=\\sum_{k=0}^{\\infty}\\eta_{k}^{3}n^{3}L^{3},}\\end{array}$ the initialstepsize $\\eta_{0}$ ischosenso that ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\eta_{0}^{2}n^{2}L^{2}+\\frac{3\\eta_{0}n C_{t A}^{2}}{L^{3}}+\\frac{3\\eta_{0}n D_{t A}^{2}}{L}\\cdot e^{S}\\left(\\left\\|z^{0}-z^{*}\\right\\|^{2}+\\frac{6S V_{t A}^{2}}{L^{6}}\\right)\\leq1\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "forconstants $C_{1A},\\,D_{7A;}$ and $V_{I A}$ defined in (59)-(61), and there exists a positive constant $\\lambda>0$ such that ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\eta_{0}^{2}n^{2}L^{2}+\\frac{\\eta_{0}n C_{2A}}{L^{3}}+\\frac{\\eta_{0}n D_{2A}}{L}\\cdot e^{S}\\left(\\left\\|z^{0}-z^{*}\\right\\|^{2}+\\frac{6S V_{t A}^{2}}{L^{6}}\\right)\\leq1-\\lambda\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "forconstants $C_{2A}$ $D_{2A}{}_{;}$ and $V_{2A}$ defined in (72)-(74). Then for any $K\\geq1$ it holdsthat ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{k=0,1,\\ldots,K}\\mathbb{E}\\left\\|F z^{k}\\right\\|^{2}\\leq\\frac{(\\log(K+3))^{2}}{(K+3)^{1/3}}\\cdot\\left(\\frac{\\left\\|z^{0}-z^{*}\\right\\|^{2}+\\frac{3V_{2A}}{n L^{6}}}{\\lambda e^{-3/2}(\\sqrt[3]{2}\\log2)^{2}\\eta_{0}^{2}n^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Proof. As the sequence of stepsizes $\\{\\eta_{k}\\}_{k\\ge0}$ is nonincreasing and (113) asserts that $\\eta_{0}\\leq1/{n L}$ we can use the bounds established in Theorem E.9 and Theorem E.13. Also, the premises required for Proposition G.3 are also satisfied, so the bound (110) holds. ", "page_idx": 56}, {"type": "text", "text": "Setting $\\gamma_{k}=\\eta_{k}^{3}n^{3}L^{3}$ in (106) and then taking the conditional expectation given $z^{k}$ , with using (71) and (114), we obtain ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\eta\\leq\\left\\|z^{k}-z^{*}\\right\\|^{2}-\\frac{1}{1+\\gamma}\\mathbb{E}\\left[\\left\\|z^{k+1}-z^{*}\\right\\|^{2}\\bigg|z^{k}\\right]-\\eta_{k}^{2}n^{2}(1-\\eta_{k}^{2}n^{2}L^{2})\\left\\|F z^{k}\\right\\|^{2}+\\frac{1}{\\gamma_{k}}\\mathbb{E}\\left[\\left\\|r^{k}\\right\\|^{2}\\bigg|z\\right]}\\\\ {\\leq\\left\\|z^{k}-z^{*}\\right\\|^{2}-\\frac{1}{1+\\gamma}\\mathbb{E}\\left[\\left\\|z^{k+1}-z^{*}\\right\\|^{2}\\bigg|z^{k}\\right]}\\\\ &{\\qquad-\\eta_{k}^{2}n^{2}(1-\\eta_{k}^{2}n^{2}L^{2})\\left\\|F z^{k}\\right\\|^{2}+\\frac{1}{L^{3}}\\left(\\eta_{k}^{3}n^{3}C_{2k}\\left\\|F z^{k}\\right\\|^{2}+\\eta_{k}^{3}n^{3}D_{2k}\\left\\|F z^{k}\\right\\|^{4}+\\eta_{k}^{3}n^{2}V_{2k}\\right)}\\\\ {\\leq\\left\\|z^{k}-z^{*}\\right\\|^{2}-\\frac{1}{1+\\gamma}\\mathbb{E}\\left[\\left\\|z^{k+1}-z^{*}\\right\\|^{2}\\bigg|z^{k}\\right]}\\\\ &{\\qquad-\\eta_{k}^{2}n^{2}\\left(1-\\eta_{k}^{2}n^{2}L^{2}-\\frac{\\eta_{k}n C_{2k}}{L^{3}}-\\frac{\\eta_{k}n D_{2k}}{L^{3}}\\left\\|F z^{k}\\right\\|^{2}\\right)\\left\\|F z^{k}\\right\\|^{2}+\\frac{\\eta_{k}^{3}n^{2}V_{2k}}{L^{3}}}\\\\ {\\leq\\left\\|z^{k}-z^{*}\\right\\|^{2}-\\frac{1}{1+\\gamma}\\mathbb{E}\\left[\\left\\|z^{k+1}-z^{*}\\right\\|^{2}\\bigg|z^{k}\\right]-\\lambda\\eta_{k}^{2}n^{2}\\left\\|F z^{k}\\right\\|^{2}+\\frac{\\eta_{k}^{3}n^{2}V_{2k}}{L^{3}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "By the law of total expectation, and that $\\gamma_{k}=\\eta_{k}^{3}n^{3}L^{3}<1$ , from the above we get ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(1+\\gamma_{k})\\lambda\\eta_{k}^{2}n^{2}\\mathbb{E}\\left\\|F z^{k}\\right\\|^{2}\\leq(1+\\gamma_{k})\\mathbb{E}\\left\\|z^{k}-z^{*}\\right\\|^{2}-\\mathbb{E}\\left\\|z^{k+1}-z^{*}\\right\\|^{2}+\\frac{(1+\\gamma_{k})\\eta_{k}^{3}n^{2}V_{2\\mathbb{A}}}{L^{3}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq(1+\\gamma_{k})\\mathbb{E}\\left\\|z^{k}-z^{*}\\right\\|^{2}-\\mathbb{E}\\left\\|z^{k+1}-z^{*}\\right\\|^{2}+\\frac{2\\eta_{k}^{3}n^{2}V_{2\\mathbb{A}}}{L^{3}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "This recurrence can be unraveled using Lemma C.11, giving us ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\left\\|z^{K+1}-z^{*}\\right\\|^{2}+\\displaystyle\\sum_{k=0}^{K}(1+\\gamma_{k})\\lambda\\eta_{j}^{2}n^{2}\\,\\mathbb{E}\\left\\|F z^{k}\\right\\|^{2}}\\\\ {\\displaystyle\\leq\\left(\\prod_{k=0}^{K}(1+\\gamma_{k})\\right)\\left(\\left\\|z^{0}-z^{*}\\right\\|^{2}+\\displaystyle\\sum_{k=0}^{K}\\frac{2\\eta_{k}^{3}n^{2}V_{2\\mathsf{A}}}{L^{3}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "For the left hand side of (116), we have ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left\\|z^{K+1}-z^{*}\\right\\|^{2}+\\displaystyle\\sum_{k=0}^{K}(1+\\gamma_{k})\\lambda\\eta_{k}^{2}n^{2}\\mathbb{E}\\left\\|F z^{k}\\right\\|^{2}\\geq\\lambda\\displaystyle\\sum_{k=0}^{K}\\eta_{k}^{2}n^{2}\\mathbb{E}\\left\\|F z^{k}\\right\\|^{2}}&{}\\\\ &{\\geq\\lambda\\displaystyle\\operatorname*{min}_{k=0,1,\\ldots,K}\\mathbb{E}\\left\\|F z^{k}\\right\\|^{2}\\displaystyle\\sum_{k=0}^{K}\\eta_{k}^{2}n^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "From Lemma C.12, we know that whenever $K\\geq1$ ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=0}^{K}\\eta_{k}^{2}n^{2}=\\eta_{0}^{2}n^{2}(\\sqrt[3]{2}\\log2)^{2}\\sum_{k=0}^{K}\\frac{1}{(k+2)^{2/3}(\\log(k+2))^{2}}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\geq\\eta_{0}^{2}n^{2}(\\sqrt[3]{2}\\log2)^{2}\\cdot\\frac{(K+3)^{1/3}}{(\\log(K+3))^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Meanwhile, as $\\begin{array}{r}{x\\mapsto\\frac{2(\\log2)^{3}}{(x+2)(\\log(x+2))^{3}}}\\end{array}$ (x+2)(og(c+2)3 is a decreasing function, we have ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{k=0}^{\\infty}\\displaystyle\\frac{2(\\log2)^{3}}{(k+2)(\\log(k+2))^{3}}\\leq1+\\displaystyle\\frac{2(\\log2)^{3}}{3(\\log3)^{3}}+\\displaystyle\\int_{1}^{\\infty}\\displaystyle\\frac{2(\\log2)^{3}}{(x+2)(\\log(x+2))^{3}}\\,\\mathrm{d}x}\\\\ {\\displaystyle\\leq1+\\displaystyle\\frac{2(\\log2)^{3}}{3(\\log3)^{3}}+\\displaystyle\\frac{(\\log2)^{3}}{(\\log3)^{2}}}&{\\leq\\displaystyle\\frac{3}{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "and thus ", "page_idx": 57}, {"type": "equation", "text": "$$\nS=\\sum_{k=0}^{\\infty}\\eta_{k}^{3}n^{3}L^{3}=\\eta_{0}^{3}n^{3}L^{3}\\sum_{k=0}^{\\infty}\\frac{2(\\log2)^{3}}{(k+2)(\\log(k+2))^{3}}\\leq\\frac{3}{2}\\eta_{0}^{3}n^{3}L^{3}\\leq\\frac{3}{2}.\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Thus, for the right hand side of (116), it holds that ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle}&{\\left(\\prod_{k=0}^{K}(1+\\gamma_{k})\\right)\\left(\\left\\|z^{0}-z^{*}\\right\\|^{2}+\\sum_{k=0}^{K}\\frac{2\\eta_{k}^{3}n^{2}V_{2\\mathbb{A}}}{L^{3}}\\right)\\le e^{\\sum_{k=0}^{K}\\gamma_{k}}\\left(\\left\\|z^{0}-z^{*}\\right\\|^{2}+\\displaystyle\\sum_{k=0}^{K}\\frac{2\\eta_{k}^{3}n^{2}V_{2\\mathbb{A}}}{L^{3}}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\le e^{S}\\left(\\left\\|z^{0}-z^{*}\\right\\|^{2}+\\displaystyle\\frac{2S V_{2\\mathbb{A}}}{n L^{6}}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\le e^{3/2}\\left(\\left\\|z^{0}-z^{*}\\right\\|^{2}+\\displaystyle\\frac{3V_{2\\mathbb{A}}}{n L^{6}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Therefore, from (116) we get ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\lambda\\eta_{0}^{2}n^{2}(\\sqrt[3]{2}\\log2)^{2}\\cdot\\frac{(K+3)^{1/3}}{(\\log(K+3))^{2}}\\cdot\\operatorname*{min}_{k=0,1,\\ldots,K}\\mathbb{E}\\left\\|F z^{k}\\right\\|^{2}\\leq e^{3/2}\\left(\\left\\|z^{0}-z^{*}\\right\\|^{2}+\\frac{3V_{2\\mathsf{A}}}{n L^{6}}\\right).\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Simply rearranging the terms gives us the desired inequality ", "page_idx": 57}, {"type": "text", "text": "Remark G.5. While $\\eta_{0}$ should be chosen so that both (113) and (114) hold, in practice, there is a way to circumvent this complication. Notice that in deriving the upper bound (118) of the right hand side of (116), it suffices to have nk \u2264 (k+2)31og(+2), and the lower bound (17) of the let hand side holds for any $\\eta_{k}\\geq0$ . In other words, if we have had chosen $\\eta_{k}=\\Theta\\left(^{1}\\!/(k\\!+\\!1)^{q}\\right)$ for $q>\\textstyle{\\frac{1}{3}}$ so that $\\begin{array}{r}{S=\\sum_{k=0}^{\\infty}{\\eta_{k}^{3}n^{3}L^{3}}<\\infty}\\end{array}$ , as long as $\\eta_{0}$ satisfies (113) and (114), we would still have obtained the inequality ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{k=0,1,\\ldots,K}\\mathbb{E}\\left\\|F z^{k}\\right\\|^{2}\\leq\\frac{e^{S}}{\\lambda n^{2}\\sum_{k=0}^{K}\\eta_{k}^{2}}\\left(\\left\\|z^{0}-z^{*}\\right\\|^{2}+\\frac{2S V_{2\\mathsf{A}}}{n L^{6}}\\right).\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "In particular, if we additionally assume that $q<\\textstyle{\\frac{1}{2}}$ then ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\sum_{k=0}^{K}\\eta_{k}^{2}\\asymp\\sum_{k=1}^{K}\\frac{1}{k^{2q}}\\asymp K^{1-2q},\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "so from (119) we would have obtained the convergence rate ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{k=0,1,\\ldots,K}\\mathbb{E}\\left\\|F z^{k}\\right\\|^{2}=\\mathcal{O}\\left(\\frac{1}{K^{1-2q}}\\right).\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "We now claim that, if one accepts a slight sacrifice of the convergence rate from $\\tilde{\\mathcal{O}}\\big({}^{1}\\!/K^{1/3}\\big)$ to $\\mathcal{O}\\left({1}/{{K^{1-2q}}}\\right)$ for $^1/3\\ <\\ q\\ <\\ 1/2$ , one can simply choose the stepsizes as $\\eta_{k}~=~\\eta_{00}\\big/(k{+}1)^{q}$ for a sufficiently small $\\eta_{00}$ . To see why this is the case, let us fix $\\eta_{0}$ to be a number that satisfies the inequalities (113) and (114). Then, because $\\begin{array}{r}{\\eta_{00}/(k{+}1)^{q}\\,=\\,o\\left(\\frac{1}{(k{+}2)^{1/3}\\log(k{+}2)}\\right)}\\end{array}$ 0 ( (k+2)13 1og(b+2 ), there will xist a nonnegative integer $k_{0}$ such that $\\begin{array}{r}{\\eta_{k}\\,\\le\\,\\frac{\\eta_{0}\\,\\sqrt[3]{2}\\log2}{(k+2)^{1/3}\\log(k+2)}}\\end{array}$ for all $k\\geq k_{0}$ . o, by ignoring the frst $k_{0}$ terms if necessary-\u2014-that is, considering as if the $k_{0}$ th iteration is the Oth iteration--it follows from the discussions made above in obtaining (120) that we get the rate of convergence $\\mathcal{O}\\left({1}/{{K^{1-2q}}}\\right)$ ", "page_idx": 57}, {"type": "text", "text": "This discussion also justifies the choice of stepsizes $\\eta_{k}=\\Theta\\left(^{1}\\!/(1\\!+\\!k/10)^{0.34}\\right)$ used in the experiments for the monotone setting. ", "page_idx": 57}, {"type": "text", "text": "H Proof of Lower Bounds ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "H.1 Proof of the Divergence of SEG-US, SEG-RR and SEG-FF ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "We prove the divergence of SEG-US, SEG-RR and SEG-FF in each proposition below, using the same worst-case problem for $n=2$ . These constitute the proof of Theorem 4.1. ", "page_idx": 58}, {"type": "text", "text": "Proposition H.1 (Part of Theorem 4.1). For $n=2$ ,there exists a convex-concave minimax problem $\\begin{array}{r}{f(x,y)=\\frac{1}{2}\\sum_{i=1}^{2}f_{i}(x,y)}\\end{array}$ havingamonotone $\\pmb{F}$ consisting of $L$ -smooth quadratic $f_{i}$ 's satisfying Assumption3.4with $(\\rho,\\sigma)=(1,0)$ such that SEG-US diverges in expectation for any choice of stepsizes $\\{\\alpha_{t}\\}_{t\\ge0}$ and $\\{\\beta_{t}\\}_{t\\ge0}$ . That is, for all $t\\geq0$ ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert z_{t+1}\\right\\Vert^{2}\\right]>\\mathbb{E}\\left[\\left\\Vert z_{t}\\right\\Vert^{2}\\right],\\quad\\mathbb{E}\\left[\\left\\Vert F z_{t+1}\\right\\Vert^{2}\\right]>\\mathbb{E}\\left[\\left\\Vert F z_{t}\\right\\Vert^{2}\\right].\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Proof. We consider the case of ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{f_{1}(x,y)=-\\displaystyle\\frac{L}{4}x^{2}+\\displaystyle\\frac{L}{2}x y-\\displaystyle\\frac{L}{4}y^{2},}}\\\\ {{f_{2}(x,y)=\\displaystyle\\frac{L}{4}x^{2}+\\displaystyle\\frac{L}{2}x y+\\displaystyle\\frac{L}{4}y^{2},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "which result in a bilinear (and hence convex-concave) objective function ", "page_idx": 58}, {"type": "equation", "text": "$$\nf(x,y)=\\frac{1}{2}\\sum_{i=1}^{2}f_{i}(x,y)=\\frac{L}{2}x y.\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "One can quickly check from the definitions of the component functions $f_{1}$ and $f_{2}$ that the corresponding saddle gradient operators are given as ", "page_idx": 58}, {"type": "equation", "text": "$$\nF_{1}z=\\underbrace{{\\left[\\!\\!-L/2\\!\\!\\!\\!-\\frac{L/2}{L/2}\\!\\!\\!\\right]}}_{:=A_{1}}z,\\quad F_{2}z=\\underbrace{{\\left[\\!\\!\\!\\begin{array}{l l l}{L/2}&{L/2}\\\\ {-L/2}&{-L/2}\\end{array}\\!\\!\\!-\\frac{L/2}{2}\\!\\!\\!\\right]}}_{:=A_{2}}z,\\quad F z=\\left[\\!\\!\\!-L/2\\!\\!\\!-\\frac{L/2}{L/2}\\!\\!\\!\\right]z\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "where $z=(x,y)\\in\\mathbb{R}^{2}$ . From the fact that $\\|A_{i}\\|\\leq L$ for all $i$ 's, we can confirm that $f_{i}$ 's are indeed $L$ -smooth. As for Assumption 3.4, we can verify that ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\sum_{i=1}^{2}\\left\\|F_{i}z-F z\\right\\|^{2}=\\frac{L^{2}}{4}\\left\\|z\\right\\|^{2}=\\left\\|F z\\right\\|^{2},\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "thus proving that our example $f$ indeed satisfies Assumption 3.4 with $(\\rho,\\sigma)=(1,0)$ ", "page_idx": 58}, {"type": "text", "text": "We now proceed to show that for this particular worst-case example $f$ , SEG-US diverges in expectation. For $t\\geq0$ the $(t+1)$ -th iteration of SEG-US starts at $\\boldsymbol{z}_{t}$ , and the algorithm uniformly chooses an index $i(t)$ from $[n]$ . The algorithm then makes an update ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{{\\pmb w}_{t}={\\pmb z}_{t}-\\alpha_{t}{\\pmb F}_{i(t)}{\\pmb z}_{t},}&{}\\\\ {{\\pmb z}_{t+1}={\\pmb z}_{t}-\\beta_{t}{\\pmb F}_{i(t)}{\\pmb w}_{t}.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "In our worst-case example $f$ , the updates can be compactly written as ", "page_idx": 58}, {"type": "equation", "text": "$$\nz_{t+1}=(\\pmb{I}-\\beta_{t}\\pmb{A}_{i(t)}+\\alpha_{t}\\beta_{t}\\pmb{A}_{i(t)}^{2})z_{t}.\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Since we have $n=2$ , the update can be summarized as ", "page_idx": 58}, {"type": "equation", "text": "$$\nz_{t+1}=\\left\\{\\begin{array}{l l}{(I-\\beta_{t}\\mathbf{A}_{1}+\\alpha_{t}\\beta_{t}\\mathbf{A}_{1}^{2})z_{t}}&{\\mathrm{~with~probability~}1/2,}\\\\ {(I-\\beta_{t}\\mathbf{A}_{2}+\\alpha_{t}\\beta_{t}\\mathbf{A}_{2}^{2})z_{t}}&{\\mathrm{~with~probability~}1/2.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "By the definition of $A_{1}$ and $A_{2}$ and using $A_{1}^{2}=A_{2}^{2}={\\bf0}$ , we can verify that ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{N_{1}:=I-\\beta_{t}{\\bf A}_{1}+\\alpha_{t}\\beta_{t}{\\cal A}_{1}^{2}=\\left[\\begin{array}{c c}{1+\\frac{\\beta_{t}{\\cal L}}{2}}&{-\\frac{\\beta_{t}{\\cal L}}{2}}\\\\ {\\frac{\\beta_{t}{\\cal L}}{2}}&{1-\\frac{\\beta_{t}{\\cal L}}{2}}\\end{array}\\right],}\\\\ &{N_{2}:=I-\\beta_{t}{\\cal A}_{2}+\\alpha_{t}\\beta_{t}{\\cal A}_{2}^{2}=\\left[\\begin{array}{c c}{1-\\frac{\\beta_{t}{\\cal L}}{2}}&{-\\frac{\\beta_{t}{\\cal L}}{2}}\\\\ {\\frac{\\beta_{t}{\\cal L}}{2}}&{1+\\frac{\\beta_{t}{\\cal L}}{2}}\\end{array}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "From this, we notice that the expectation of $\\left\\Vert z_{t+1}\\right\\Vert^{2}$ conditional on $\\boldsymbol{z}_{t}$ reads ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert z_{t+1}\\right\\Vert^{2}\\Big|\\,z_{t}\\right]=z_{t}^{\\top}\\left(\\frac{N_{1}^{\\top}N_{1}+N_{2}^{\\top}N_{2}}{2}\\right)z_{t}.\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Working out the calculations, we can check that ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\mathrm{\\frac{}{~\\textit{N}~}}\\mathrm{\\frac{}{~\\textit{N}_{1}^{\\top}}}N_{1}+N_{2}^{\\top}N_{2}\\mathrm{}=\\left[1+\\mathrm{\\frac{\\beta_{t}^{2}L^{2}}{2}}\\mathrm{\\phantom{\\frac{}{~\\textit{N}_{1}^{\\top}}~}}0\\mathrm{\\phantom{\\frac{}{~\\textit{S}_{t}^{2}}~}}\\right],\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "thus resulting in ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert z_{t+1}\\right\\Vert^{2}\\Big|\\,z_{t}\\right]=\\left(1+\\frac{\\beta_{t}^{2}L^{2}}{2}\\right)\\left\\Vert z_{t}\\right\\Vert^{2}.\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Since this holds for all $t\\geq0$ , SEG-US diverges in expectation, for any positive stepsizes $\\{\\alpha_{t}\\}_{t\\ge0}$ and $\\{\\beta_{t}\\}_{t\\ge0}$ . The statement on $\\|\\boldsymbol{F}\\boldsymbol{z}_{t}\\|$ follows by realizing that $\\|F z\\|=\\textstyle{\\frac{L}{2}}\\,\\|z\\|$ \u53e3 ", "page_idx": 59}, {"type": "text", "text": "Proposition H.2 (Part of Theorem 4.1). For $n=2$ ,thereexists a convex-concaveminimax problem $\\begin{array}{r}{f(x,y)=\\frac{1}{2}\\sum_{i=1}^{2}f_{i}(x,y)}\\end{array}$ having a monotone $\\pmb{F}$ consisting of $L$ -smooth quadratic $f_{i}\\,^{\\prime}s$ satisfying Assumption3.4with $\\dot{(\\rho,\\sigma)}=(1,0)$ such that SEG-RR diverges in expectation for any choice of stepsizes $\\{\\alpha_{k}\\}_{k\\ge0}$ and $\\{\\beta_{k}\\}_{k\\ge0}$ .That is,for any $k\\geq0$ ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\left\\Vert z_{0}^{k+1}\\right\\Vert^{2}\\right]>\\mathbb{E}\\left[\\left\\Vert z_{0}^{k}\\right\\Vert^{2}\\right],\\quad\\mathbb{E}\\left[\\left\\Vert F z_{0}^{k+1}\\right\\Vert^{2}\\right]>\\mathbb{E}\\left[\\left\\Vert F z_{0}^{k}\\right\\Vert^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Proof. The proof uses the same example as Proposition H.1, outlined in (121). We show that for this particular worst-case example $f$ , SEG-RR diverges in expectation. For $k\\geq0$ the $(k+1)$ -th epoch of SEG-RR starts at $z_{0}^{k}$ , and the algorithm randomly chooses a permutation $\\tau_{k}:[n]\\to[n]$ .The algorithm then goes through a series of updates ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\pmb w}_{i}^{k}=z_{i}^{k}-\\alpha_{k}{\\pmb F}_{\\tau_{k}(i+1)}z_{i}^{k},}\\\\ {z_{i+1}^{k}=z_{i}^{k}-\\beta_{k}{\\pmb F}_{\\tau_{k}(i+1)}{\\pmb w}_{i}^{k},}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "for $i=0,\\ldots,n-1$ . In our worst-case example $f$ , the updates can be compactly written as ", "page_idx": 59}, {"type": "equation", "text": "$$\nz_{i+1}^{k}=({\\cal I}-\\beta_{k}{\\cal A}_{\\tau_{k}(i+1)}+\\alpha_{k}\\beta_{k}{\\cal A}_{\\tau_{k}(i+1)}^{2})z_{i}^{k}.\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Sincewe have $n=2$ and there are only two possible permutations, the updates over an epoch can be summarized as ", "page_idx": 59}, {"type": "equation", "text": "$$\nz_{0}^{k+1}=z_{n}^{k}=\\left\\{\\begin{array}{l l}{(I-\\beta_{k}A_{1}+\\alpha_{k}\\beta_{k}A_{1}^{2})(I-\\beta_{k}A_{2}+\\alpha_{k}\\beta_{k}A_{2}^{2})z_{0}^{k}}&{\\mathrm{~with~probability~}1/2,}\\\\ {(I-\\beta_{k}A_{2}+\\alpha_{k}\\beta_{k}A_{2}^{2})(I-\\beta_{k}A_{1}+\\alpha_{k}\\beta_{k}A_{1}^{2})z_{0}^{k}}&{\\mathrm{~with~probability~}1/2.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "By the definition of $A_{1}$ and $A_{2}$ and using $A_{1}^{2}=A_{2}^{2}={\\bf0}$ , we can verify that ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{M_{1}:=(I-\\beta_{k}A_{1}+\\alpha_{k}\\beta_{k}A_{1}^{2})(I-\\beta_{k}A_{2}+\\alpha_{k}\\beta_{k}A_{2}^{2})\\!=\\!\\left[\\begin{array}{c c}{1-\\frac{\\beta_{k}^{2}L^{2}}{2}}&{-\\beta_{k}L-\\frac{\\beta_{k}^{2}L^{2}}{2}}\\\\ {\\beta_{k}L-\\frac{\\beta_{k}^{2}L^{2}}{2}}&{1-\\frac{\\beta_{k}^{2}L^{2}}{2}}\\end{array}\\right],}\\\\ &{M_{2}:=(I-\\beta_{k}A_{2}+\\alpha_{k}\\beta_{k}A_{2}^{2})(I-\\beta_{k}A_{1}+\\alpha_{k}\\beta_{k}A_{1}^{2})\\!=\\!\\left[\\begin{array}{c c}{1-\\frac{\\beta_{k}^{2}L^{2}}{2}}&{-\\beta_{k}L+\\frac{\\beta_{k}^{2}L^{2}}{2}}\\\\ {\\beta_{k}L+\\frac{\\beta_{k}^{2}L^{2}}{2}}&{1-\\frac{\\beta_{k}^{2}L^{2}}{2}}\\end{array}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "From this, we notice that the expectation of $\\left\\|z_{0}^{k+1}\\right\\|^{2}$ conditional on $z_{0}^{k}$ reads ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert z_{0}^{k+1}\\right\\Vert^{2}\\Big|\\left.z_{0}^{k}\\right]=(z_{0}^{k})^{\\top}\\left(\\frac{M_{1}^{\\top}M_{1}+M_{2}^{\\top}M_{2}}{2}\\right)z_{0}^{k}.\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Working out the calculations, we can check that ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\frac{M_{1}^{\\top}M_{1}+M_{2}^{\\top}M_{2}}{2}=\\left[1+\\frac{\\beta_{k}^{4}L^{4}}{2}\\begin{array}{c c}{0}\\\\ {1+\\frac{\\beta_{k}^{4}L^{4}}{2}}\\end{array}\\right],\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "thus resulting in ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\|z_{0}^{k+1}\\right\\|^{2}\\Big|\\,z_{0}^{k}\\right]=\\left(1+\\frac{\\beta_{k}^{4}L^{4}}{2}\\right)\\left\\|z_{0}^{k}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Since this holds for all $k\\geq0$ , SEG-R diverges in expectation, for any positive stepsizes $\\{\\alpha_{k}\\}_{k\\ge0}$ and $\\{\\beta_{k}\\}_{k\\ge0}$ . The statement on $\\left\\|\\pmb{F}\\boldsymbol{z}_{0}^{k}\\right\\|$ follows by realizing that $\\begin{array}{r}{\\|\\boldsymbol{F}\\boldsymbol{z}\\|=\\frac{L}{2}\\left\\|\\boldsymbol{z}\\right\\|}\\end{array}$ \u53e3 ", "page_idx": 59}, {"type": "text", "text": "Proposition H.3 (Part of Theorem 4.1). For $n=2$ , there exists a convex-concave minimax problem $\\begin{array}{r}{f(x,y)=\\frac{1}{2}\\sum_{i=1}^{2}f_{i}(x,y),}\\end{array}$ having amonotone $\\pmb{F}$ consisting of $L$ smooth quadratic $f_{i}$ 's satisfying Assumption3.4with $\\dot{(\\rho,\\sigma)}\\,=\\,(1,0)$ such that SEG-FF diverges in expectation for any positive stepsizes $\\{\\alpha_{k}\\}_{k\\ge0}$ and $\\{\\beta_{k}\\}_{k\\ge0}$ . That is, for any $k\\geq0$ \uff0c ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\left\\Vert z_{0}^{k+1}\\right\\Vert^{2}\\right]>\\mathbb{E}\\left[\\left\\Vert z_{0}^{k}\\right\\Vert^{2}\\right],\\quad\\mathbb{E}\\left[\\left\\Vert F z_{0}^{k+1}\\right\\Vert^{2}\\right]>\\mathbb{E}\\left[\\left\\Vert F z_{0}^{k}\\right\\Vert^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Proof. The proof uses the same example as Proposition H.1, outlined in (121). We prove that SEG-FF also diverges for this $f$ . For $k\\geq0$ , the $(k+1)$ -th epoch of SEG-FF starts at $\\dot{z}_{0}^{k}$ , and the algorithm randomly chooses a permutation $\\tau_{k}:[n]\\to[n]$ , as in the case of SEG-RR. The algorithm then goes through a series of updates for $i=0,\\ldots,n-1$ ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\pmb w}_{i}^{k}=z_{i}^{k}-\\alpha_{k}{\\pmb F}_{\\tau_{k}(i+1)}z_{i}^{k},}\\\\ {z_{i+1}^{k}=z_{i}^{k}-\\beta_{k}{\\pmb F}_{\\tau_{k}(i+1)}{\\pmb w}_{i}^{k},}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "which are the same as SEG-RR; but then, it performs another series of $n$ updates, in the reverse order.For $i=n,\\ldots,2n-1$ ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\pmb w}_{i}^{k}=z_{i}^{k}-\\alpha_{k}{\\pmb F}_{\\tau_{k}(2n-i)}z_{i}^{k},}\\\\ {z_{i+1}^{k}=z_{i}^{k}-\\beta_{k}{\\pmb F}_{\\tau_{k}(2n-i)}{\\pmb w}_{i}^{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Using the definition of $M_{1}$ and $M_{2}$ from (122) and (123), one can verify that the $2n=4$ updates over an epoch of SEG-FF can be summarized as ", "page_idx": 60}, {"type": "equation", "text": "$$\nz_{0}^{k+1}=z_{2n}^{k}=\\left\\{\\!\\!\\begin{array}{c c}{{M_{2}M_{1}z_{0}^{k}}}&{{\\mathrm{~with~probability~}1/2,}}\\\\ {{M_{1}M_{2}z_{0}^{k}}}&{{\\mathrm{~with~probability~}1/2.}}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "From this, we notice that the expectation of $\\left\\|z_{0}^{k+1}\\right\\|^{2}$ conditional on $z_{0}^{k}$ reads ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert z_{0}^{k+1}\\right\\Vert^{2}\\Big|\\,z_{0}^{k}\\right]=(z_{0}^{k})^{\\top}\\left(\\frac{M_{1}^{\\top}M_{2}^{\\top}M_{2}M_{1}+M_{2}^{\\top}M_{1}^{\\top}M_{1}M_{2}}{2}\\right)z_{0}^{k}.\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Working out the calculations, we can check that ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\frac{M_{1}^{\\top}M_{2}^{\\top}M_{2}M_{1}+M_{2}^{\\top}M_{1}^{\\top}M_{1}M_{2}}{2}=\\left[\\begin{array}{c c}{1+2\\beta_{k}^{6}L^{6}}&{0}\\\\ {0}&{1+2\\beta_{k}^{6}L^{6}\\right],\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "thus resulting in ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\|z_{0}^{k+1}\\right\\|^{2}\\Big|\\,z_{0}^{k}\\right]=\\left(1+2\\beta_{k}^{6}L^{6}\\right)\\left\\|z_{0}^{k}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Since this holds for all $k\\geq0$ , SEG-FF diverges in expectation, for any positive stepsizes $\\{\\alpha_{k}\\}_{k\\ge0}$ and $\\{\\beta_{k}\\}_{k\\ge0}$ . The statement on $\\left\\|\\pmb{F}\\pmb{z}_{0}^{k}\\right\\|$ follows by realizing that $\\begin{array}{r}{\\|\\boldsymbol{F}\\boldsymbol{z}\\|=\\frac{L}{2}\\left\\|\\boldsymbol{z}\\right\\|}\\end{array}$ \u53e3 ", "page_idx": 60}, {"type": "text", "text": "H.2Proof of Limited Convergence of SEG-US in Monotone Cases ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "In [17, 20], the authors study the same-sample and independent-sample versions of SEG-US, with step sizes $\\alpha_{t}$ and $\\beta_{t}$ satisfying a constant ratio: $\\beta_{t}=\\gamma\\alpha_{t}$ for $\\gamma\\in\\left(0,1\\right]$ . While the authors show convergence in the monotone $\\pmb{F}$ case, there is one important limitation shared by the existing analyses. In order to achieve $\\begin{array}{r}{\\operatorname*{min}_{t=0,\\dots,T}\\mathbb{E}[\\|F z_{t}\\|^{2}]\\,\\le\\,\\epsilon^{2}}\\end{array}$ for an arbitrarily chosen e, the algorithms must repeat the same query to the stochastic gradient oracle $\\begin{array}{r}{b=\\mathcal{O}(\\frac{1}{\\epsilon^{2}})}\\end{array}$ times at every iteration to reduce the gradient variance from $\\sigma^{2}$ 10 $\\frac{\\sigma^{2}}{b}$ . In other words, the convergence bounds for SEG-US in the monotone case have an additive term ${\\mathcal{O}}(\\sigma^{2})$ that cannot be reduced to zero by proper choices of stepsizes. Below, we prove that such a $\\sigma^{2}$ term is in fact inevitable for any choices of stepsizes, if the ratio $\\gamma$ is fixed constant. This indicates that SEG-US considered in the existing results can never converge all the way to the optimum if $b=1$ is maintained throughout training. In contrast, our SEG-FFA shows convergence in the monotone case even when $b=1$ ", "page_idx": 60}, {"type": "text", "text": "Theorem H.4. For $n=2$ thereexistscoe-conavemnaxblem $\\begin{array}{r}{f(x,y)=\\frac{1}{2}\\sum_{i=1}^{2}f_{i}(x,y)}\\end{array}$ having a monotone $\\pmb{F}$ consisting of $L$ -smooth quadratic $f_{i}$ 's satisfying Assumption 3.4 with $(\\rho,\\sigma)=$ $(0,\\sigma)$ such that SEG-US with any positive stepsizes $\\{\\alpha_{t}\\}_{t\\ge0}$ and $\\{\\beta_{t}\\}_{t\\ge0}$ satisfying $\\beta_{t}=\\gamma\\alpha_{t}$ for $\\gamma>0$ cannot convergebeyond acertainfixedconstant $\\Omega(\\sigma^{2})$ .More concretely, for any $t\\geq0$ ", "page_idx": 60}, {"type": "text", "text": "", "page_idx": 61}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\|F z_{t}\\right\\|^{2}\\right]\\geq\\operatorname*{min}\\left\\{\\left\\|F z_{0}\\right\\|^{2},\\frac{\\gamma\\sigma^{2}}{2}\\right\\}\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "regardless of the stepsizes. This holds for both same-sample and independent-sample SEG-US. ", "page_idx": 61}, {"type": "text", "text": "Proof. We consider the case of ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f_{1}(x,y)=L x y+\\nu x-\\nu y,}\\\\ {f_{2}(x,y)=L x y-\\nu x+\\nu y,}\\end{array}\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "which results in a bilinear (and hence convex-concave) objective function ", "page_idx": 61}, {"type": "equation", "text": "$$\nf(x,y)={\\frac{1}{2}}\\sum_{i=1}^{2}f_{i}(x,y)=L x y.\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "One can quickly check from the definitions of the component functions $f_{1}$ and $f_{2}$ that the corresponding saddle gradient operators are given as ", "page_idx": 61}, {"type": "equation", "text": "$$\nF_{1}z=\\underbrace{\\left[\\!\\!\\begin{array}{l l}{0}&{L}\\\\ {-L}&{0}\\end{array}\\!\\!\\right]}_{:=A}z+\\nu\\mathbf{1},\\quad F_{2}z=A z-\\nu\\mathbf{1},\\quad F z=A z,\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "where $z=(x,y)\\in\\mathbb{R}^{2}$ . From the fact that $\\|A\\|\\leq L$ , we can confirm that $f_{i}$ 's are indeed $L$ -smooth. As for Assumption 3.4, we can verify that ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\sum_{i=1}^{2}\\left\\|F_{i}z-F z\\right\\|^{2}=\\frac{1}{2}\\sum_{i=1}^{2}2\\nu^{2}=2\\nu^{2}.\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Therefore,by choosing v2 = 2, , our example $f$ indeed satisfies Assumption 3.4 with $(\\rho,\\sigma)=(0,\\sigma)$ ", "page_idx": 61}, {"type": "text", "text": "The proof is outlined as follows. For the example constructed above, we will calculate the $\\mathbb{E}[\\Vert z_{t+1}\\Vert^{2}]$ and show that the expectation is identical for both same-sample and independent sample versions of SEG-US. We will then show that the update on the expected squared distance to equilibrium $\\mathbb{E}[\\Vert z_{t+1}\\Vert^{2}]$ for given $\\boldsymbol{z}_{t}$ can only belong to two categories: either $\\|\\bar{\\boldsymbol{z}}_{t}\\|^{2}\\leq\\mathbb{E}[\\|\\boldsymbol{z}_{t+1}\\|^{2}]$ (expected squared distanceincreasesor\u2265+1\u2265 (expected squared distance shrinks but is bounded from below by a constant). Since the two cases hold for any $t\\geq0$ and any choices of $\\alpha_{t}$ and $\\beta_{t}=\\gamma\\alpha_{t}$ , we show that the \u201cconvergence\u201d\u2019 can happen only up to a neighborhood of equilibrium. ", "page_idx": 61}, {"type": "text", "text": "At iteration $t$ , SEG-US samples component indices $i(t),j(t)\\in\\{1,2\\}$ for its extrapolation step and update step, respectively. In the independent-sample version $i(t)$ and ${\\dot{j}}(t)$ are independently sampled from $\\mathrm{Unif}(\\{1,2\\})$ , and in the same-sample version $i(t)$ is sampled uniformly at random and $j(t)$ is set to be equal to $i(t)$ . With the indices sampled as above, SEG-US then makes an update ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{\\pmb w}_{t}={\\pmb z}_{t}-\\alpha_{t}{\\pmb F}_{i(t)}{\\pmb z}_{t},}&{}\\\\ {{\\pmb z}_{t+1}={\\pmb z}_{t}-\\beta_{t}{\\pmb F}_{j(t)}{\\pmb w}_{t}.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "In our worst-case example $f$ , the updates can be written as ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{w}_{t}=z_{t}-\\alpha_{t}\\pmb{A}z_{t}-s_{i(t)}\\alpha_{t}\\nu\\mathbf{1}}\\\\ &{\\qquad=(\\pmb{I}-\\alpha_{t}\\pmb{A})z_{t}-s_{i(t)}\\alpha_{t}\\nu\\mathbf{1}}\\\\ &{z_{t+1}=z_{t}-\\beta_{t}\\pmb{A}\\pmb{w}_{t}-s_{j(t)}\\beta_{t}\\nu\\mathbf{1}}\\\\ &{\\qquad=(\\pmb{I}-\\beta_{t}\\pmb{A}+\\alpha_{t}\\beta_{t}\\pmb{A}^{2})z_{t}+s_{i(t)}\\alpha_{t}\\beta_{t}\\nu\\pmb{A}\\mathbf{1}-s_{j(t)}\\beta_{t}\\nu\\mathbf{1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "where we defined $s_{1}=+1$ and $s_{2}=-1$ for simplicity of notation. ", "page_idx": 61}, {"type": "text", "text": "We now calculate the expected value of $\\left\\Vert z_{t+1}\\right\\Vert^{2}$ ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|z_{t+1}\\right\\|^{2}=\\left\\|(I-\\beta_{t}A+\\alpha_{t}\\beta_{t}A^{2})z_{t}\\right\\|^{2}+\\alpha_{t}^{2}\\beta_{t}^{2}\\nu^{2}\\left\\|A\\mathbf{1}\\right\\|^{2}+\\beta_{t}^{2}\\nu^{2}\\left\\|\\mathbf{1}\\right\\|^{2}}\\\\ &{\\qquad\\qquad+\\;2s_{i(t)}\\alpha_{t}\\beta_{t}\\nu\\langle(I-\\beta_{t}A+\\alpha_{t}\\beta_{t}A^{2})z_{t},A\\mathbf{1}\\rangle-2s_{j(t)}\\beta_{t}\\nu\\langle(I-\\beta_{t}A+\\alpha_{t}\\beta_{t}A^{2})z_{t},\\mathbf{1}\\rangle}\\\\ &{\\qquad\\qquad-\\;2s_{i(t)}s_{j(t)}\\alpha_{t}\\beta_{t}^{2}\\nu^{2}\\langle A\\mathbf{1},\\mathbf{1}\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "For the independent-sample case, since $s_{i(t)}$ and $s_{j(t)}$ are independent mean-zero random variables, ", "page_idx": 62}, {"type": "text", "text": "$\\begin{array}{r}{\\mathbb{E}_{i(t),j(t)}[\\left\\|z_{t+1}\\right\\|^{2}]=\\left\\|\\left(I-\\beta_{t}\\pmb{A}+\\alpha_{t}\\beta_{t}\\pmb{A}^{2})z_{t}\\right\\|^{2}+\\alpha_{t}^{2}\\beta_{t}^{2}\\nu^{2}\\left\\|\\pmb{A}\\pmb{1}\\right\\|^{2}+\\beta_{t}^{2}\\nu^{2}\\left\\|\\pmb{1}\\right\\|^{2}.}\\end{array}$ In the same-sample case, $s_{i(t)}=s_{j(t)}$ is a mean-zero random variable, so ", "page_idx": 62}, {"type": "text", "text": "$\\mathfrak{L}_{i(t)}[\\|z_{t+1}\\|^{2}]=\\left\\|(I-\\beta_{t}A+\\alpha_{t}\\beta_{t}A^{2})z_{t}\\right\\|^{2}+\\alpha_{t}^{2}\\beta_{t}^{2}\\nu^{2}\\left\\|A1\\right\\|^{2}+\\beta_{t}^{2}\\nu^{2}\\left\\|\\mathbf{1}\\right\\|^{2}-2\\alpha_{t}\\beta_{t}^{2}\\nu^{2}\\langle A\\mathbf{1},\\mathbf{1}\\rangle,$ but once we realize that $\\langle A\\mathbf{1},\\mathbf{1}\\rangle=0$ , the expectation becomes identical to (124); hence, the rest of the analysis is the same for the two versions. ", "page_idx": 62}, {"type": "text", "text": "We now expand and arrange the RHS of (124). It is easy to check that ", "page_idx": 62}, {"type": "equation", "text": "$$\n(I-\\beta_{t}A+\\alpha_{t}\\beta_{t}A^{2})z_{t}=\\left[1-\\alpha_{t}\\beta_{t}L^{2}\\begin{array}{c c}{-\\beta_{t}L}\\\\ {\\beta_{t}L}&{1-\\alpha_{t}\\beta_{t}L^{2}}\\end{array}\\right]\\left[\\boldsymbol{x}_{t}\\right]=\\left[\\left(1-\\alpha_{t}\\beta_{t}L^{2}\\right)\\boldsymbol{x}_{t}-\\beta_{t}L\\boldsymbol{y}_{t}\\right]\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "and hence ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\|(I-\\beta_{t}A+\\alpha_{t}\\beta_{t}A^{2})z_{t}\\right\\|^{2}=\\left((1-\\alpha_{t}\\beta_{t}L^{2})^{2}+\\beta_{t}^{2}L^{2}\\right)\\left\\|z_{t}\\right\\|^{2}}&{}\\\\ {=\\left(1-2\\alpha_{t}\\beta_{t}L^{2}+\\beta_{t}^{2}L^{2}(1+\\alpha_{t}^{2}L^{2})\\right)\\left\\|z_{t}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "From this, we get ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\|z_{t+1}\\|^{2}]=\\|z_{t}\\|^{2}-\\left(2\\alpha_{t}\\beta_{t}L^{2}-\\beta_{t}^{2}L^{2}(1+\\alpha_{t}^{2}L^{2})\\right)\\|z_{t}\\|^{2}+2\\alpha_{t}^{2}\\beta_{t}^{2}L^{2}\\nu^{2}+2\\beta_{t}^{2}\\nu^{2}}\\\\ &{\\qquad\\qquad=\\|z_{t}\\|^{2}-\\left(2\\alpha_{t}\\beta_{t}L^{2}-\\beta_{t}^{2}L^{2}(1+\\alpha_{t}^{2}L^{2})\\right)\\|z_{t}\\|^{2}+\\beta_{t}^{2}\\sigma^{2}(1+\\alpha_{t}^{2}L^{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "where we used the choice $\\begin{array}{r}{\\nu_{2}=\\frac{\\sigma^{2}}{2}}\\end{array}$ as above. ", "page_idx": 62}, {"type": "text", "text": "The rest of the proof proceeds as follows: we show that, regardless of $t\\geq0$ and the choices of $\\alpha_{t}$ and $\\beta_{t}=\\gamma\\alpha_{t}$ , the expected value of $\\left\\Vert z_{t+1}\\right\\Vert^{2}$ given $\\scriptstyle z_{t}$ can be categorized into only two cases: ", "page_idx": 62}, {"type": "text", "text": "1. $\\left\\|\\boldsymbol{z}_{t}\\right\\|^{2}\\leq\\mathbb{E}[\\left\\|\\boldsymbol{z}_{t+1}\\right\\|^{2}]$ . That is, the iterate moves away from the equilibrium in expectation.   \n2 $\\begin{array}{r}{\\|z_{t}\\|^{2}\\geq\\mathbb{E}[\\|z_{t+1}\\|^{2}]\\geq\\frac{\\gamma\\sigma^{2}}{2L^{2}}}\\end{array}$ ", "page_idx": 62}, {"type": "text", "text": "Showing this immediately finishes the proof, because there is no way that any $\\mathbb{E}[\\Vert\\boldsymbol{z}_{t}\\Vert^{2}]$ can get smaller than $\\operatorname*{min}\\{\\left\\|z_{0}\\right\\|^{2},\\frac{\\gamma\\sigma^{2}}{2L^{2}}\\}$ and $\\|\\pmb{F}z\\|=L\\,\\|z\\|$ for our example $f$ ", "page_idx": 62}, {"type": "text", "text": "The remaining proof is simple, by noticing that $\\left\\|\\boldsymbol{z}_{t}\\right\\|^{2}\\leq\\mathbb{E}[\\left\\|\\boldsymbol{z}_{t+1}\\right\\|^{2}]$ is equivalent to ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\left(2\\alpha_{t}\\beta_{t}L^{2}-\\beta_{t}^{2}L^{2}(1+\\alpha_{t}^{2}L^{2})\\right)\\left\\|\\boldsymbol{z}_{t}\\right\\|^{2}\\leq\\beta_{t}^{2}\\sigma^{2}(1+\\alpha_{t}^{2}L^{2}).\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "Hence, if $\\alpha_{t},\\,\\beta_{t}$ , and $\\scriptstyle z_{t}$ satisfies (125), we belong to the first category. Otherwise, we are in the $\\begin{array}{r}{\\mathbb{E}[\\|z_{t+1}\\|^{2}]\\ge\\frac{\\gamma\\sigma^{2}}{2L^{2}}}\\end{array}$ (125) is satisfied with the opposite sign, we must have $2\\alpha_{t}\\beta_{t}L^{2}-\\beta_{t}^{2}L^{2}(\\stackrel{\\rightharpoonup}{1}+\\alpha_{t}^{2}L^{2})>0$ and ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\|z_{t}\\|^{2}\\geq\\frac{\\beta_{t}^{2}\\sigma^{2}(1+\\alpha_{t}^{2}L^{2})}{2\\alpha_{t}\\beta_{t}L^{2}-\\beta_{t}^{2}L^{2}(1+\\alpha_{t}^{2}L^{2})}.\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "Also, notice that ", "page_idx": 62}, {"type": "equation", "text": "$$\n2\\alpha_{t}\\beta_{t}L^{2}-\\beta_{t}^{2}L^{2}(1+\\alpha_{t}^{2}L^{2})=1-\\left((1-\\alpha_{t}\\beta_{t}L^{2})^{2}+\\beta_{t}^{2}L^{2}\\right)<1.\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "Using $2\\alpha_{t}\\beta_{t}L^{2}-\\beta_{t}^{2}L^{2}(1+\\alpha_{t}^{2}L^{2})\\,\\in\\,(0,1)$ and substituting the lower bound on $\\left\\Vert\\boldsymbol{z}_{t}\\right\\Vert^{2}$ into the update equation, we find that ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\|z_{t+1}\\|^{2}]=\\|z_{t}\\|^{2}-\\left(2\\alpha_{t}\\beta_{t}L^{2}-\\beta_{t}^{2}L^{2}(1+\\alpha_{t}^{2}L^{2})\\right)\\|z_{t}\\|^{2}+\\beta_{t}^{2}\\sigma^{2}(1+\\alpha_{t}^{2}L^{2})}\\\\ &{\\qquad\\qquad\\ge\\frac{\\beta_{t}^{2}\\sigma^{2}(1+\\alpha_{t}^{2}L^{2})}{2\\alpha_{t}\\beta_{t}L^{2}-\\beta_{t}^{2}L^{2}(1+\\alpha_{t}^{2}L^{2})}=\\frac{1}{L^{2}\\left(\\frac{2\\alpha_{t}}{\\beta_{t}\\sigma^{2}(1+\\alpha_{t}^{2}L^{2})}-1\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "Lastly, substituting $\\beta_{t}=\\gamma\\alpha_{t}$ into the RHS gives ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\Vert z_{t+1}\\Vert^{2}]\\ge\\frac{1}{L^{2}\\left(\\frac{2}{\\gamma\\sigma^{2}(1+\\alpha_{t}^{2}L^{2})}-1\\right)}\\ge\\frac{\\gamma\\sigma^{2}}{2L^{2}}.\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "This finishes the proof. ", "page_idx": 62}, {"type": "text", "text": "Remark H.5. We remark that, while Theorem H.4 successfully shows that SEG-US as studied in [17, 20] cannot converge to an optimal point unless the batch size is increased every iteration, it does not contradict the (almost sure) convergence result of independent-sample SEG by Hsieh et al. [25]. Indeed, in [25], the stepsizes $\\{\\alpha_{t}\\}_{t\\ge0}$ and $\\{\\beta_{t}\\}_{t\\ge0}$ are chosen so that they decay to O with a different rate and hence the corresponding ratio $\\gamma$ approaches 0, while Theorem H.4 considers the case where $\\alpha_{t}$ and $\\beta_{t}$ differ by a constant factor $\\gamma$ ", "page_idx": 63}, {"type": "text", "text": "H.3Proof of SGDA-RR and SEG-RR Lower Bounds ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Theorem H.6. Suppose $n\\geq2$ and $L,\\mu>0$ satisfies $L/\\mu\\geq2$ There exists a $\\mu$ -strongly-convexstrongly-concave minimax problem $\\begin{array}{r}{f(\\pmb{\\mathscr{z}})=\\frac{1}{n}\\sum_{i=1}^{\\bar{n}}f_{i}(\\pmb{\\mathscr{z}})}\\end{array}$ consisting of $L$ smoothquadratic $f_{i}\\,^{\\prime}s$ satisfying Assumption 3.4 with $(\\rho,\\sigma)=(0,\\sigma)$ and initialization $z_{0}^{0}$ such that SEG-RR with any constant stepsize $\\alpha_{k}=\\alpha>0,$ $\\beta_{k}=\\beta>0$ satisfies ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert z_{0}^{K}-z^{*}\\right\\Vert^{2}\\right]=\\left\\{\\Omega\\left(\\frac{\\sigma^{2}}{L\\mu n K}\\right)\\quad\\,i f\\,K\\leq L/\\mu,\\right.\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "where $z^{*}$ is the unique equilibrium point of $f$ .For a similar choice of problem $f$ (this time with $(\\rho,\\sigma)=(1,\\sigma),$ ), SGDA-RR with any constant stepsize $\\alpha_{k}=\\alpha>0$ satisfies ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert z_{0}^{K}-z^{*}\\right\\Vert^{2}\\right]=\\left\\{\\mathfrak{Q}\\left(\\frac{\\sigma^{2}}{L\\mu n K}\\right)\\begin{array}{r l}&{i f K\\leq L/\\mu,}\\\\ &{\\left[\\Omega\\left(\\frac{\\sigma^{2}}{\\mu^{2}n^{2}K^{2}}+\\frac{L\\sigma^{2}}{\\mu^{3}n K^{3}}\\right)\\right.}&{i f K>L/\\mu.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "Remark H.7. In Theorem H.6, we adopt techniques from the existing lower bounds for SGD-RR to prove lower bounds for the minimax algorithms SGDA-RR and SEG-RR. In the literature, there are two types of lower bounds for SGD-RR when $K\\gtrsim L/\\mu$ namely, $\\begin{array}{r}{\\Omega\\big(\\frac{1}{n^{2}K^{2}}+\\frac{1}{n K^{3}}\\big)}\\end{array}$ bounds for strongly convex quadrai fntions [49, 50]land $\\Omega\\big(\\frac{1}{n K^{2}}\\big)$ bodsfongln functions [11, 45, 56]. Upper bounds that match the lower bounds in and are also known, which indicates that SGD-RR is one of the rare examples of minimization algorithms whose tight convergence rates for quadratic vs. non-quadratic functions differ, within the narrow scope of strongly convex and smooth functions. Whil it is tempting to aim for a tighter $\\Omega\\big(\\frac{1}{n K^{2}}\\big)$ lower bound for our algorithms of interest, we note that the existing $\\Omega\\big(\\frac{1}{n K^{2}}\\big)$ bounds for SGD-RR are proven for piecewise-quadratic functions whose Hessian is discontinuous. Since the discontinuous Hessian violates our Assumption 3.3, we instead adhere to the quadratic case to prove lower bounds $\\Omega\\big(\\frac{1}{n K^{3}}\\big)$ for both SGDA-RR and SEG-RR (when $K\\geq L/\\mu)$ . These bounds may not be the tightest possible (since they are restricted to the quadratics), but they still suffice to demonstrate that SEG-FFA is provably superior to both SGDA-RR and SEG-RR. ", "page_idx": 63}, {"type": "text", "text": "H.3.1 Existing Lower Bound for SGD-RR ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "For the proof of lower bounds for SGDA-RR and SEG-RR, we utilize results and techniques from the lower bounds proven for SGD-RR; thus, it would be profitable to summarize the existing result. ", "page_idx": 63}, {"type": "text", "text": "In case of SGD-RR, it is known from Theorem 2 of Safran and Shamir [50] that there exists a minimization problem $g(x)$ such that SGD-RR satisfies a lower bound of $\\begin{array}{r}{\\bar{\\Omega}(\\frac{1}{n^{2}K^{2}}+\\frac{1}{n K^{3}})}\\end{array}$ large enoughvalues of $K$ . We rewrite the theorem in a version in accordance with our notation and assumptions: ", "page_idx": 63}, {"type": "text", "text": "Theorem H.8 (Theorem 2 of Safran and Shamir [50]). For any $n\\,\\geq\\,2$ and $L,\\mu>0$ satisfying $L/\\mu\\geq2,$ there exs a $\\mu$ strongly convexminimizationproblem $\\begin{array}{r}{\\dot{g(\\pmb{x})}=\\frac{1}{n}\\sum_{i=1}^{n}\\dot{g_{i}}(\\pmb{x})}\\end{array}$ consisting of $L$ -smooth quadratic $g_{i}$ 's satisfying Assumption 3.4 with $(\\rho,\\sigma)=(1,\\sigma)^{'}$ such thatSGD-RR using anyconstant stepsize $\\alpha_{k}=\\alpha>0$ satisfies ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\|x_{0}^{K}-x^{*}\\right\\|^{2}\\right]=\\Omega\\left(\\frac{\\sigma^{2}}{L\\mu n K}\\cdot\\operatorname*{min}\\left\\{1,\\frac{L}{\\mu n K}+\\frac{L^{2}}{\\mu^{2}K^{2}}\\right\\}\\right).\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "The statement is equivalent to saying that for SGD-RR with constant stepsize $\\alpha\\,>\\,0$ , the bound $\\Omega\\big(\\frac{\\sigma^{2}}{L\\mu n K}\\big)$ holds for $K\\lesssim L/\\mu$ and $\\begin{array}{r}{\\Omega(\\frac{\\sigma^{2}}{\\mu^{2}n^{2}K^{2}}+\\frac{L\\sigma^{2}}{\\mu^{3}n K^{3}})}\\end{array}$ $K\\gtrsim L/\\mu$ ", "page_idx": 63}, {"type": "text", "text": "The function $\\textstyle g={\\frac{1}{n}}\\sum_{i=1}^{n}g_{i}$ used in the theorem is defined by the following component functions: ", "page_idx": 64}, {"type": "equation", "text": "$$\ng_{i}({\\pmb x})=g_{i}(x_{1},x_{2},x_{3}):=\\frac{\\mu}{2}x_{1}^{2}+\\frac{L}{2}x_{2}^{2}+\\left\\{\\begin{array}{l l}{\\frac{\\sigma}{2}x_{2}+\\frac{L}{2}x_{3}^{2}+\\frac{\\sigma}{2}x_{3}}&{i\\leq\\frac{n}{2},}\\\\ {-\\frac{\\sigma}{2}x_{2}-\\frac{\\sigma}{2}x_{3}}&{i>\\frac{n}{2},}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "thus making the objective function ", "page_idx": 64}, {"type": "equation", "text": "$$\ng(x_{1},x_{2},x_{3}):=\\frac{\\mu}{2}x_{1}^{2}+\\frac{L}{2}x_{2}^{2}+\\frac{L}{4}x_{3}^{2}.\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "One can notice that the linear terms in $g_{i}$ (126) change signs depending on $i\\leq\\frac{n}{2}$ or not, and handling these sign flips is the key to the proof of lower bound. ", "page_idx": 64}, {"type": "text", "text": "H.3.2 Proof of Lower Bound for SGDA-RR ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "For the SGDA-RR lower bound, we consider the following minimax optimization problem: ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle f(\\pmb{x},y)=\\frac{1}{n}\\sum_{i=1}^{n}f_{i}(\\pmb{x},y),~\\mathrm{where}~\\pmb{x}\\in\\mathbb{R}^{3},~y\\in\\mathbb{R},}\\\\ {\\displaystyle f_{i}(\\pmb{x},y)=g_{i}(\\pmb{x})-\\frac{\\mu}{2}y^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "where $g_{i}$ 's are from (126). We need to first check if the problem instance satisfies the assumptions listed in the theorem statement. Since $f({\\boldsymbol{x}},y)=g({\\boldsymbol{x}})-{\\overset{\\cdot}{\\underset{2}{\\boldsymbol{\\mu}}}}y^{2}$ and $g$ is a $\\mu$ -strongly convexfunction, $f$ is $\\mu$ -strongly-convex-strongly-concave as claimed. Also, it is easy to check from the definition of $g_{i}$ that eachcomponent $f_{i}({\\pmb x},{\\boldsymbol y})$ is $L$ -smooth quadratic. ", "page_idx": 64}, {"type": "text", "text": "Lastly, to check Assumption 3.4, we frst define $s_{1},\\ldots,s_{n}$ as $s_{i}=1$ for $i\\leq\\frac{n}{2}$ and $s_{i}=0$ for $i>\\textstyle{\\frac{n}{2}}$ Using this notation, The function $g_{i}$ can be compactly written as the following: ", "page_idx": 64}, {"type": "equation", "text": "$$\ng_{i}(x_{1},x_{2},x_{3})=\\frac{\\mu}{2}x_{1}^{2}+\\frac{L}{2}x_{2}^{2}+\\frac{\\sigma}{2}(2s_{i}-1)x_{2}+\\frac{L}{2}s_{i}x_{3}^{2}+\\frac{\\sigma}{2}(2s_{i}-1)x_{3}.\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "Therefore, the saddle gradient operators $F_{i}$ of $f_{i}$ and $\\pmb{F}$ of $f$ evaluate to ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\begin{array}{r}{F_{i}z:=\\left[\\stackrel{\\nabla g_{i}(\\boldsymbol{x})}{\\mu y}\\right]=\\left[\\stackrel{\\mu x_{1}}{L x_{2}}+\\frac{\\sigma}{2}(2s_{i}-1)\\atop{L s_{i}x_{3}+\\frac{\\sigma}{2}(2s_{i}-1)}\\right],\\quad F z=\\left[\\stackrel{\\mu x_{1}}{L x_{2}}\\right],}\\\\ {\\mu y\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "which in turn yields ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\left\\|F_{i}z-F z\\right\\|^{2}={\\frac{\\sigma^{2}}{4}}+\\left({\\frac{L}{2}}x_{3}+{\\frac{\\sigma}{2}}\\right)^{2}\\leq\\left({\\frac{L}{2}}|x_{3}|+\\sigma\\right)^{2}\\leq\\left(\\left\\|F z\\right\\|+\\sigma\\right)^{2}\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "for all $i=1,\\dots,n$ . This confirms that the function $\\textstyle f\\,=\\,{\\frac{1}{n}}\\sum_{i}f_{i}$ satisfies Assumption 3.4 with $(\\rho,\\sigma)=(1,\\sigma)$ ", "page_idx": 64}, {"type": "text", "text": "If we run SGDA-RR on this problem, the updates on $\\textbf{\\em x}$ done by SGDA-RR is exactly identical to what SGD-RR would perform for the minimization problem $\\begin{array}{r}{g(\\pmb{x})\\ =\\ \\frac{1}{n}\\sum_{i}g_{i}(\\pmb{x})}\\end{array}$ with the same choices of random permutations. Therefore, after $K$ epochs of SGDA-RR, it follows from Theorem $\\mathrm{H.8}$ that ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert z_{0}^{K}-z^{*}\\right\\Vert^{2}\\right]\\geq\\mathbb{E}\\left[\\left\\Vert x_{0}^{K}-x^{*}\\right\\Vert^{2}\\right]=\\Omega\\left(\\frac{\\sigma^{2}}{L\\mu n K}\\cdot\\operatorname*{min}\\left\\{1,\\frac{L}{\\mu n K}+\\frac{L^{2}}{\\mu^{2}K^{2}}\\right\\}\\right),\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "which is in fact a tighter lower bound for SGDA-RR than what is stated in Theorem H.6. This finishes the proof. ", "page_idx": 64}, {"type": "text", "text": "H.3.3 Proof of Lower Bound for SEG-RR ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "In this subsection, we prove the lower bound for SEG-RR. We will first define a new problem instance $f$ to be used here, and verify that the assumptions in the theorem statement are indeed satisfied by this new $f$ . We will then spell out the update equation of SEG-RR for this example, which will serve as a basis for the case analysis that follows: we will divide the choices of stepsizes $\\alpha,\\beta>0$ to four regimes and prove a lower bound for each of them. Combining the regimes will result in the desired lower bound. ", "page_idx": 64}, {"type": "text", "text": "", "page_idx": 65}, {"type": "text", "text": "For SEG-RR, we use a slightly different problem from (127). This time, we consider ", "page_idx": 65}, {"type": "equation", "text": "$$\nf(\\pmb{x},y)=\\frac{1}{n}\\sum_{i=1}^{n}f_{i}(\\pmb{x},y),\\;\\mathrm{where}\\;\\pmb{x}\\in\\mathbb{R}^{2},\\;y\\in\\mathbb{R},\n$$", "text_format": "latex", "page_idx": 65}, {"type": "equation", "text": "$$\nf_{i}({\\pmb x},y)=\\frac{L}{2}x_{1}^{2}+\\frac{L}{4}x_{2}^{2}+\\sigma(2s_{i}-1)x_{2}-\\frac{\\mu}{2}y^{2},\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "where $s_{i}=1$ for $i\\leq\\textstyle{\\frac{n}{2}}$ and $s_{i}=0$ for $i>{\\frac{n}{2}}$ , as defined above. ", "page_idx": 65}, {"type": "text", "text": "We first check if the problem (128) satisfies the assumptions in the theorem statement. Since ", "page_idx": 65}, {"type": "equation", "text": "$$\nf({\\pmb x},y)=\\frac{L}{2}x_{1}^{2}+\\frac{L}{4}x_{2}^{2}-\\frac{\\mu}{2}y^{2}\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "and $L/2\\ge\\mu$ by assumption, $f$ is $\\mu$ -strongly-convex-strongly-concave. Also, it is straightforward to see that each $f_{i}$ is an $L$ -smooth quadratic function. It is left to check Assumption 3.4. The saddle gradient operators $F_{i}$ of $f_{i}$ and $\\pmb{F}$ of $f$ evaluateto ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\begin{array}{r}{F_{i}z=\\left[\\frac{L x_{1}}{\\frac{L}{2}x_{2}+\\sigma(2s_{i}-1)}\\right],\\quad F z=\\left[\\frac{L x_{1}}{\\frac{L}{2}x_{2}}\\right],}\\\\ {\\mu y}\\end{array}\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "which in turn yields ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|F_{i}z-F z\\right\\|^{2}=\\sigma^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "for all $i=1,\\dots,n$ . This confirms that the function $\\textstyle f\\,=\\,{\\frac{1}{n}}\\sum_{i}f_{i}$ satisfies Assumption 3.4 with $(\\rho,\\sigma)=(0,\\sigma)$ , as required by the theorem. ", "page_idx": 65}, {"type": "text", "text": "For $k\\geq0$ .the $(k+1)$ -th epoch of SEG-RR starts at $z_{0}^{k}=(x_{0}^{k},y_{0}^{k})$ and the algorithm chooses a random permutation $\\tau_{k}$ . The algorithm then goes through a series of updates ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{w}_{i}^{k}=\\pmb{z}_{i}^{k}-\\alpha\\pmb{F}_{\\tau_{k}(i+1)}\\pmb{z}_{i}^{k},}\\\\ {\\pmb{z}_{i+1}^{k}=\\pmb{z}_{i}^{k}-\\beta\\pmb{F}_{\\tau_{k}(i+1)}\\pmb{w}_{i}^{k},}\\end{array}\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "for $i=0,\\ldots,n-1$ . For our example $f$ (128), it can be checked that a single iteration by SEG-RR reads ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\begin{array}{r}{z_{i+1}^{k}=\\left[\\!\\!\\begin{array}{c}{x_{i+1,1}^{k}}\\\\ {x_{i+1,2}^{k}}\\\\ {y_{i+1}^{k}}\\end{array}\\!\\!\\right]=\\left[\\!\\!\\begin{array}{c}{(1-\\beta L+\\alpha\\beta L^{2})x_{i,1}^{k}}\\\\ {(1-\\frac{\\beta L}{2}+\\frac{\\alpha\\beta L^{2}}{4})x_{i,2}^{k}-\\beta\\sigma(1-\\frac{\\alpha L}{2})(2s_{\\tau_{k}(i+1)}-1)}\\\\ {(1-\\beta\\mu+\\alpha\\beta\\mu^{2})y_{i}^{k}}\\end{array}\\!\\!\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "Aggregating the SEG-RR updates over an entire epoch $\\mathit{\\Omega}_{\\mathit{1}}^{\\prime}=0,\\ldots,n-1)$ results in ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{x_{0,1}^{k+1}=(1-\\beta L+\\alpha\\beta L^{2})^{n}x_{0,1}^{k},}}\\\\ {{x_{0,2}^{k+1}=\\left(1-\\displaystyle\\frac{\\beta L}{2}+\\frac{\\alpha\\beta L^{2}}{4}\\right)^{n}x_{0,2}^{k}-\\beta\\sigma\\left(1-\\displaystyle\\frac{\\alpha L}{2}\\right)\\underbrace{\\sum_{i=1}^{n}(2s_{\\tau_{k}(i)}-1)\\left(1-\\displaystyle\\frac{\\beta L}{2}+\\frac{\\alpha\\beta L^{2}}{4}\\right)^{n-i}}_{=:\\Phi},}}\\\\ {{y_{n}^{k+1}=(1-\\beta u+\\alpha\\beta u^{2})^{n}u_{n}^{k}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 65}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "We will now square both sides of these equations above and take expectations over $\\tau_{k}$ . In doing so, there is a useful identity: ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\Phi]=\\sum_{i=1}^{n}\\mathbb{E}[2s_{\\tau_{k}(i)}-1]\\left(1-\\frac{\\beta L}{2}+\\frac{\\alpha\\beta L^{2}}{4}\\right)^{n-i}=0.\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "Also, it worth mentioning that $\\tau_{k}$ is independent of $\\boldsymbol{z}_{0}^{k}=(x_{0,1}^{k},x_{0,2}^{k},y_{0}^{k})$ . Using these facts, we can arrange the terms to obtain ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad(x_{0,1}^{k+1})^{2}=(1-\\beta L+\\alpha\\beta L^{2})^{2n}(x_{0,1}^{k})^{2},}\\\\ &{\\mathbb{E}[(x_{0,2}^{k+1})^{2}]=\\left(1-\\frac{\\beta L}{2}+\\frac{\\alpha\\beta L^{2}}{4}\\right)^{2n}\\mathbb{E}[(x_{0,2}^{k})^{2}]+\\beta^{2}\\sigma^{2}\\left(1-\\frac{\\alpha L}{2}\\right)^{2}\\mathbb{E}[\\Phi^{2}],}\\\\ &{\\quad(y_{0}^{k+1})^{2}=(1-\\beta\\mu+\\alpha\\beta\\mu^{2})^{2n}(y_{0}^{k})^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "Based on these three per-epoch update equations above, we now divide the choices of SEG-RR stepsizes $\\alpha,\\beta>0$ into the following four cases and handle them separately: ", "page_idx": 66}, {"type": "text", "text": "1. $\\begin{array}{r}{\\alpha>\\frac{1}{L}}\\end{array}$ , in which case we show that SEG-RR makes $(x_{0,1}^{k+1})^{2}>(x_{0,1}^{k})^{2}$ hold deterministically, so that if we initialize at x,1 = $\\begin{array}{r}{x_{0,1}^{0}=\\frac{\\sigma}{\\sqrt{L\\mu}}}\\end{array}$ then we have ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\|z_{0}^{K}\\right\\|^{2}\\right]\\geq(x_{0,1}^{K})^{2}>(x_{0,1}^{0})^{2}=\\frac{\\sigma^{2}}{L\\mu}.\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "2 $\\alpha\\leq\\frac{1}{L}$ and $\\begin{array}{r}{\\beta\\leq\\frac{1}{\\mu n K}}\\end{array}$ in whichase we hw tha EG-initalid $\\begin{array}{r}{y_{0}^{0}=\\frac{\\sigma}{\\sqrt{L\\mu}}}\\end{array}$ suffers ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\|z_{0}^{K}\\right\\|^{2}\\right]=\\Omega\\left(\\frac{\\sigma^{2}}{L\\mu}\\right),\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "3. $\\alpha\\leq\\frac{1}{L}$ and $\\begin{array}{r}{\\frac{1}{\\mu n K}<\\beta<\\frac{1}{n L}}\\end{array}$ in which ase we show that SEG-RRinitalizedat $x_{0,2}^{0}=0$ suffers ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\|z_{0}^{K}\\right\\|^{2}\\right]=\\Omega\\left(\\frac{L\\sigma^{2}}{\\mu^{3}n K^{3}}\\right),\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "4. $\\begin{array}{r}{\\alpha\\leq\\frac{1}{L},\\beta>\\frac{1}{\\mu n K}}\\end{array}$ , and $\\begin{array}{r}{\\beta\\ge\\frac{1}{n L}}\\end{array}$ in which case we show that SEG-RR initialized at $x_{0,2}^{0}=0$ suffers ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert z_{0}^{K}\\right\\Vert^{2}\\right]=\\Omega\\left(\\frac{\\sigma^{2}}{L\\mu n K}\\right).\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "Notie tha thethird case $\\begin{array}{r}{\\frac{1}{\\mu n K}<\\beta<\\frac{1}{n L}}\\end{array}$ only makesense hen $K>L/\\mu$ case just disappears. Hence, for the \u201clarge epoch\u201d\" regime where $K>L/\\mu$ , the third case achieves the minimum error possible, so it holds that ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\|z_{0}^{K}\\right\\|^{2}\\right]=\\Omega\\left(\\frac{L\\sigma^{2}}{\\mu^{3}n K^{3}}\\right).\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "For the \u201csmall epoch\" regime $(K\\leq L/\\mu)$ , the third case does not exist and the fourth case achieves the minimum, so ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert z_{0}^{K}\\right\\Vert^{2}\\right]=\\Omega\\left(\\frac{\\sigma^{2}}{L\\mu n K}\\right).\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "Combining the two cases yields the desired lower bound in the theorem statement. It is now left to carry out the case analysis. ", "page_idx": 66}, {"type": "text", "text": "Case1: $\\begin{array}{r}{\\alpha>\\frac{1}{L}}\\end{array}$ .For this case, we use (129) to prove divergence. Notice from $\\begin{array}{r}{\\alpha>\\frac{1}{L}}\\end{array}$ that ", "page_idx": 66}, {"type": "equation", "text": "$$\n1-\\beta L+\\alpha\\beta L^{2}=1+\\beta L(\\alpha L-1)>1,\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "regardless of $\\beta>0$ . Hence, from (129), we get ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\lVert z_{0}^{K}\\right\\rVert^{2}\\right]\\geq(x_{0,1}^{K})^{2}>(x_{0,1}^{0})^{2}.\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "If we initialize at $\\begin{array}{r}{x_{0,1}^{0}=\\frac{\\sigma}{\\sqrt{L\\mu}}}\\end{array}$ , then this poves ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\lvert z_{0}^{K}\\right\\rvert\\right]^{2}\\ge\\frac{\\sigma^{2}}{L\\mu}.\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "Case 2: $\\alpha\\leq\\frac{1}{L}$ and $\\begin{array}{r}{\\beta\\leq\\frac{1}{\\mu n K}}\\end{array}$ . For thscase, we employ (131)to show that the contration rate\" is too small to make enough \u201cprogress.\u2019 Notice from our stepsizes that ", "page_idx": 66}, {"type": "equation", "text": "$$\n1-\\beta\\mu+\\alpha\\beta\\mu^{2}\\geq1-\\beta\\mu\\geq1-\\frac{1}{n K}\\geq0.\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "Applying this inequality to (131), we have ", "page_idx": 66}, {"type": "equation", "text": "$$\n(y_{0}^{k+1})^{2}\\geq\\left(1-\\frac{1}{n K}\\right)^{2n}(y_{0}^{k})^{2},\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "which in turn means that the progress over $K$ epoch is bounded from below by ", "page_idx": 67}, {"type": "equation", "text": "$$\n(y_{0}^{K})^{2}\\ge\\left(1-\\frac{1}{n K}\\right)^{2n K}(y_{0}^{0})^{2}\\ge\\frac{(y_{0}^{0})^{2}}{16},\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "where we used our assumption that $n\\geq2$ and $K\\ge1$ . Hence, if our initialization was given as $\\begin{array}{r}{y_{0}^{0}=\\frac{\\sigma}{\\sqrt{L\\mu}}}\\end{array}$ , then this proves ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\lVert z_{0}^{K}\\right\\rVert^{2}\\right]\\geq(y_{0}^{K})^{2}\\geq\\frac{(y_{0}^{0})^{2}}{16}=\\Omega\\left(\\frac{\\sigma^{2}}{L\\mu}\\right).\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "Case3: $\\alpha\\leq\\frac{1}{L}$ and $\\begin{array}{r}{\\frac{1}{\\mu n K}\\,<\\,\\beta\\,<\\,\\frac{1}{n L}}\\end{array}$ . For stepsizes in this interval, we use (130) to derive the desired bound. Here, it is important to characterize a lower bound on the quantity ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\Phi^{2}]:=\\mathbb{E}\\left[\\left(\\sum_{i=1}^{n}(2s_{\\tau_{k}(i)}-1)\\left(1-\\frac{\\beta L}{2}+\\frac{\\alpha\\beta L^{2}}{4}\\right)^{n-i}\\right)^{2}\\right].\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "To this end, we can use a lemma from Safran and Shamir [49], stated below: ", "page_idx": 67}, {"type": "text", "text": "Lemma H.9 (Lemma 1 of Safran and Shamir [49]). Let $\\pi_{1},\\ldots,\\pi_{n}$ (for even $n$ )be a random permutation of $(1,1,\\ldots,1,-1,-1,\\ldots,-1)$ where both 1 and $-1$ appear exactly $n/2$ times. Then there isa numerical constant $c>0$ suchthatfor any $\\nu>0$ ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left(\\sum_{i=1}^{n}\\pi_{i}(1-\\nu)^{n-i}\\right)^{2}\\right]\\geq c\\cdot\\operatorname*{min}\\left\\{1+\\frac{1}{\\nu},n^{3}\\nu^{2}\\right\\}.\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "One can notice that Lemma H.9 is directly applicable to $\\mathbb{E}[\\Phi^{2}]$ with $\\begin{array}{r}{\\nu\\leftarrow\\frac{\\beta L}{2}-\\frac{\\alpha\\beta L^{2}}{4}}\\end{array}$ Since ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\nu=\\frac{\\beta L}{2}-\\frac{\\alpha\\beta L^{2}}{4}\\leq\\frac{\\beta L}{2}\\leq\\frac{1}{2n},\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "we have $\\begin{array}{r}{n^{3}\\nu^{2}\\leq\\frac{1}{8\\nu}}\\end{array}$ , thereby ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\left\\{1+{\\frac{1}{\\nu}},n^{3}\\nu^{2}\\right\\}\\geq\\operatorname*{min}\\left\\{{\\frac{1}{\\nu}},n^{3}\\nu^{2}\\right\\}=n^{3}\\nu^{2}.\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "Therefore, Lemma H.9 gives ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\Phi^{2}]\\geq c n^{3}\\left(\\frac{\\beta L}{2}-\\frac{\\alpha\\beta L^{2}}{4}\\right)^{2}=\\frac{c\\beta^{2}n^{3}L^{2}}{4}\\left(1-\\frac{\\alpha L}{2}\\right)^{2}\\geq\\frac{c\\beta^{2}n^{3}L^{2}}{16},\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "where the last inequality used $\\alpha\\leq\\frac{1}{L}$ . Applying (132) to (130) and also using $\\begin{array}{r}{(1-\\frac{\\alpha L}{2})^{2}\\geq\\frac{1}{4}}\\end{array}$ \uff0c ", "page_idx": 67}, {"type": "equation", "text": "$$\n{\\ensuremath{\\mathbb E}}[(x_{0,2}^{k+1})^{2}]\\ge\\left(1-\\frac{\\beta L}{2}+\\frac{\\alpha\\beta L^{2}}{4}\\right)^{2n}{\\ensuremath{\\mathbb E}}[(x_{0,2}^{k})^{2}]+\\frac{c\\beta^{4}n^{3}L^{2}\\sigma^{2}}{64}.\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "Unrolling the inequality for $k=0,\\ldots,K-1$ gives ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[(x_{0,2}^{K})^{2}\\right]\\geq\\left(1-\\frac{\\beta L}{2}+\\frac{\\alpha\\beta L^{2}}{4}\\right)^{2n K}(x_{0,2}^{0})^{2}+\\frac{c\\beta^{4}n^{3}L^{2}\\sigma^{2}}{64}\\sum_{j=0}^{K-1}\\left(1-\\frac{\\beta L}{2}+\\frac{\\alpha\\beta L^{2}}{4}\\right)^{2n j}}\\\\ {=\\left(1-\\frac{\\beta L}{2}+\\frac{\\alpha\\beta L^{2}}{4}\\right)^{2n K}(x_{0,2}^{0})^{2}+\\frac{c\\beta^{4}n^{3}L^{2}\\sigma^{2}}{64}\\cdot\\frac{1-\\left(1-\\frac{\\beta L}{2}+\\frac{\\alpha\\beta L^{2}}{4}\\right)^{2n K}}{1-\\left(1-\\frac{\\beta L}{2}+\\frac{\\alpha\\beta L^{2}}{4}\\right)^{2n}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "Now note that our initialization $\\boldsymbol{x}_{0,2}^{0}$ can be set to zero, which eliminates the need to think about the frst term in the RHS. It is now left to bound the second term. First,by the stepsize range $\\alpha\\leq\\frac{1}{L}$ $\\begin{array}{r}{\\beta>\\frac{1}{\\mu n K}}\\end{array}$ \u03bcnk and our assumption L/\u03bc \u2265 2, we have ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\left(1-\\frac{\\beta L}{2}+\\frac{\\alpha\\beta L^{2}}{4}\\right)^{2n K}\\leq\\left(1-\\frac{\\beta L}{4}\\right)^{2n K}\\leq\\left(1-\\frac{L}{4\\mu n K}\\right)^{2n K}\\leq e^{-\\frac{L}{2\\mu}}\\leq e^{-1}.\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "Next, by Bernoulli's inequality ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\left(1-\\frac{\\beta L}{2}+\\frac{\\alpha\\beta L^{2}}{4}\\right)^{2n}\\ge\\left(1-\\frac{\\beta L}{2}\\right)^{2n}\\ge1-\\beta n L>0.\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "Plugging in the two inequalities to above, we obtain ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[(x_{0,2}^{K})^{2}\\right]\\geq\\frac{c\\beta^{4}n^{3}L^{2}\\sigma^{2}}{64}\\cdot\\frac{1-\\left(1-\\frac{\\beta L}{2}+\\frac{\\alpha\\beta L^{2}}{4}\\right)^{2n K}}{1-\\left(1-\\frac{\\beta L}{2}+\\frac{\\alpha\\beta L^{2}}{4}\\right)^{2n}}}\\\\ &{\\geq\\frac{c\\beta^{4}n^{3}L^{2}\\sigma^{2}}{64}\\cdot\\frac{1-e^{-1}}{1-\\left(1-\\beta n L\\right)}=c^{\\prime}\\beta^{3}n^{2}L\\sigma^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "for a numerical constant $c^{\\prime}>0$ Plugging in the lower bound $\\begin{array}{r}{\\beta>\\frac{1}{\\mu n K}}\\end{array}$ yields ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\|z_{0}^{K}\\right\\|^{2}\\right]\\geq\\mathbb{E}\\left[(x_{0,2}^{K})^{2}\\right]=\\Omega\\left(\\frac{L\\sigma^{2}}{\\mu^{3}n K^{3}}\\right).\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "Case 4: $\\begin{array}{r}{\\alpha\\leq\\frac{1}{L},\\beta>\\frac{1}{\\mu n K}}\\end{array}$ and $\\begin{array}{r}{\\beta\\ge\\frac{1}{n L}}\\end{array}$ .We again use (130). Bynoticing that the initialization $x_{0,2}^{0}=0$ , we can unroll (130) for $k=0,\\ldots,K-1$ to get ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[(x_{0,2}^{K})^{2}\\right]\\geq\\frac{\\beta^{2}\\sigma^{2}}{4}\\mathbb{E}[\\Phi^{2}]\\sum_{j=0}^{K-1}\\left(1-\\frac{\\beta L}{2}+\\frac{\\alpha\\beta L^{2}}{4}\\right)^{2n j}\\geq\\frac{\\beta^{2}\\sigma^{2}}{4}\\mathbb{E}[\\Phi^{2}],\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "where the last inequality holds regardless of $\\beta$ because each summand with $j\\geq1$ is nonnegative. We then invoke Lemma H.9 to lower bound $\\mathbb{E}[\\Phi^{2}]$ , again with $\\begin{array}{r}{\\nu\\leftarrow\\frac{\\beta L}{2}-\\frac{\\alpha\\beta L^{2}}{4}}\\end{array}$ P.Since ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\nu=\\frac{\\beta L}{2}-\\frac{\\alpha\\beta L^{2}}{4}\\geq\\frac{\\beta L}{4}\\geq\\frac{1}{4n},\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "we have $\\begin{array}{r}{n^{3}\\nu^{2}\\geq\\frac{1}{64\\nu}}\\end{array}$ , thereby ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\left\\{1+\\frac{1}{\\nu},n^{3}\\nu^{2}\\right\\}\\geq\\operatorname*{min}\\left\\{\\frac{1}{\\nu},n^{3}\\nu^{2}\\right\\}\\geq\\frac{1}{64\\nu}.\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "Therefore, Lemma H.9 gives ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\Phi^{2}]\\geq\\frac{c}{64\\nu}=\\frac{c}{32\\beta L}\\cdot\\frac{1}{1-\\frac{\\alpha L}{2}}\\geq\\frac{c}{32\\beta L}.\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "Combining (134) with (133) gives ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[(x_{0,2}^{K})^{2}\\right]\\ge\\frac{c\\beta\\sigma^{2}}{128L}=\\Omega\\left(\\frac{\\sigma^{2}}{L\\mu n K}\\right),\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "where the last step used $\\begin{array}{r}{\\beta>\\frac{1}{\\mu n K}}\\end{array}$ . This finishes the case analysis, hence the proof of Theorem H.6. ", "page_idx": 68}, {"type": "text", "text": "1 Additional Experiments ", "text_level": 1, "page_idx": 68}, {"type": "text", "text": "To evaluate our algorithm SEG-FFA as well as other baseline algorithms, we conduct numerical experiments on monotone and strongly monotone problems. Specifically, as we have mentioned in Section 6, we consider random quadratic problems of the form ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\boldsymbol{x}\\in\\mathbb{R}^{d_{x}}}\\operatorname*{max}_{\\boldsymbol{y}\\in\\mathbb{R}^{d_{y}}}\\;\\frac{1}{n}\\sum_{i=1}^{n}\\;\\left[\\boldsymbol{x}\\right]^{\\top}\\left[\\boldsymbol{B}_{i}^{\\top}\\;\\;\\;\\boldsymbol{-C}_{i}\\right]\\left[\\boldsymbol{y}\\right]-t_{i}^{\\top}\\left[\\boldsymbol{x}\\right].\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "Wechoose $d_{x}=d_{y}=20$ and $n=40$ for all the experiments. Numerical computations are done using NumPy [24] and SciPy [52], and the plots are drawn using Matplotlib [26]. ", "page_idx": 68}, {"type": "text", "text": "1.1  Problem Constructions for Experiments in Section 6 ", "text_level": 1, "page_idx": 69}, {"type": "text", "text": "For an experiment for the monotone case, the random components are sampled as follows. We choose $B_{i}$ so that each element is an i.i.d. sample from a uniform distribution over the interval $[0,1]$ , and $\\pmb{t}_{i}$ so that each element is an i.i.d. sample from a standard normal distribution. We chose $A_{i}$ to be diagonal matrices in the following procedure: for each $j=1,\\ldots,20$ we randomly chose a subset $\\mathcal{T}_{j}$ of ${\\frac{n}{2}}=20$ indicesfrom $[n]=\\{1,\\dots,40\\}$ , and set the $(j,j)$ -entry of $\\pmb{A}_{i}$ to be ", "page_idx": 69}, {"type": "equation", "text": "$$\n(\\mathbf{A}_{i})_{j,j}=\\left\\{{2\\atop-2}\\right.\\ \\ \\mathrm{if}\\ i\\in{\\mathbb{Z}}_{j}\\ .\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "We repeat the exact same procedure for $C_{i}$ as well. Notice that $\\begin{array}{r}{\\sum_{i=1}^{n}{\\cal A}_{i}=\\sum_{i=1}^{n}{\\cal C}_{i}={\\bf0}}\\end{array}$ by design. Hence, each of the component functions will be a nonconvex-nonconcave quadratic function in general, but the objective function itself becomes a convex-concave function. ", "page_idx": 69}, {"type": "text", "text": "For the experiment in the strongly monotone case, we sample $B_{i}$ and $\\pmb{t}_{i}$ in the same way as in the monotone case, but we use different choices of $\\pmb{A}_{i}$ and $C_{i}$ to ensure the objective function to be strongly-convex-strongly-concave. In particular, for each $i=1,\\hdots,n$ , we sample $A_{i}$ by computing $\\pmb{A}_{i}=\\pmb{Q}_{i}\\pmb{D}_{i}\\pmb{Q}_{i}^{\\top}$ , where $D_{i}$ is a random diagonal matrix whose diagonal entries are i.i.d. samples from a uniform distribution over the interval $[{\\textstyle{\\frac{1}{2}}},1]$ , and $Q_{i}$ is a random orthogonal matrix obtained by computing a $Q R$ decomposition of a $20\\times\\Bar{2}0$ random matrix whose elements are i.i.d. samples from a standard normal distribution. We sample $C_{i}$ by the exact same method. ", "page_idx": 69}, {"type": "text", "text": "1.2 Monotone Case & Ablation Study on the Anchoring Step ", "text_level": 1, "page_idx": 69}, {"type": "text", "text": "In Section 6, we compared the empirical performance of various SEGs, namely SEG-FFA, SEG-FF, SEG-RR, and SEG-US. Here, as an ablation study on the anchoring technique, we additionally compare SEG-RRA and SEG-USA, which are each SEG-RR and SEG-US with an additional anchoring step, respectively. For these two methods, we take the anchoring step after every $n$ iterations. We ran those methods on the same 5 random instances used in Section 6. For both SEGRRA and SEG-USA, we ran the method with two different stepsize choices, namely $\\alpha_{k}=\\beta_{k}=\\eta_{k}$ (inspired by the stepsize used in deterministic EG) and $\\alpha_{k}\\,\\bar{=}\\,\\beta_{k}\\big/2\\,=\\,\\eta_{k}\\big/2$ (the stepsize used for SEG-FFA) where we again set $\\eta_{k}=\\eta_{0}/(1{+}k/10)^{0.34}$ with $\\eta_{0}=\\operatorname*{min}\\{0.01,\\frac{1}{L}\\}$ ", "page_idx": 69}, {"type": "text", "text": "The results are plotted in Figure 2. As SEG-RRA and SEG-USA are designed to take one pass per epoch, for those mthods, we compute the raioIBz- where $t$ denotes the number of passes, and plot the geometric mean over the 5 runs. ", "page_idx": 69}, {"type": "text", "text": "From the performance of SEG-RRA with $\\alpha_{k}=\\beta_{k}$ and the two variants of SEG-USA, it is possible to observe that adding the anchoring step does improve the performance of the method up to a certain level, but it alone does not fully resolve the nonconvergence issue. On the other hand, quite interestingly, SEG-RRA with $\\alpha_{k}\\,=\\,\\beta_{k}\\big/2$ shows a hint of convergence. While its performance is slightly worse compared to SEG-FFA, it is nonetheless still notable as it is the only other method from SEG-FFA that seems to be capable of converging to an optimum. ", "page_idx": 69}, {"type": "text", "text": "We conjecture that this intriguing performance of SEG-RRA with $\\alpha_{k}=\\beta_{k}/2$ is because it achieves an \u201cexpected\" second order matching to the (deterministic) EG. Indeed, following the notations of Proposition D.1, one can deduce from Proposition D.1 that using SEG-RRA with $\\alpha={}^{\\beta}/2$ will result in an epoch-level update of ", "page_idx": 69}, {"type": "equation", "text": "$$\nz^{\\sharp}=z_{0}-\\frac{\\beta}{2}\\sum_{j=0}^{n-1}T_{j}z_{0}+\\frac{\\beta^{2}}{4}\\sum_{j=0}^{n-1}D T_{j}(z_{0})T_{j}z_{0}+\\frac{\\beta^{2}}{2}\\sum_{0\\leq i<j\\leq n-1}D T_{j}(z_{0})T_{i}z_{0}+\\frac{\\epsilon_{n}}{2}\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "with $\\epsilon_{n}\\;=\\;o\\left(\\beta^{2}\\right)$ .Here, notice that $(T_{0},T_{1},\\ldots,T_{n-1})\\;=\\;(F_{\\tau(1)},F_{\\tau(2)},\\ldots,F_{\\tau(n)})$ for some randomly chosen permutation $\\tau\\in S_{n}$ . Now, observe that for any two distinct $i,j\\in[n]$ , there are exactly $\\frac{n!}{2}$ permutations in ${\\mathcal{S}}_{n}$ such that $i$ comes before $j$ in the sequence $\\tau(1),\\tau(2),\\ldots,\\tau(n)$ , and also exactly $\\textstyle{\\frac{n!}{2}}$ permutations such that $j$ comes before $i$ . Thus, in taking the expectation over the ", "page_idx": 69}, {"type": "image", "img_path": "OJxua0PAIo/tmp/6c1c339eecc86a81630a1ae3388c00c80f02ee90d6bf3b3614b71e75d8d4c8cf.jpg", "img_caption": [], "img_footnote": [], "page_idx": 70}, {"type": "text", "text": "Figure 2: Experimental results in the monotone example, comparing the performance of SEG-RRA and SEG-USA with the results displayed in Figure 1. Because SEG-FFA and SEG-FF use two passes per epoch, for those two methods, we plot $\\|\\boldsymbol{F}\\boldsymbol{z}_{0}^{t/2}\\|^{2}\\!\\!\\int\\!\\!\\|\\boldsymbol{F}\\boldsymbol{z}_{0}^{0}\\|^{2}$ ", "page_idx": 70}, {"type": "text", "text": "randomness of choosing the permutation $\\tau$ ,weget ", "page_idx": 70}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\boldsymbol\\tau}\\left[\\displaystyle\\sum_{0\\leq i<j\\leq n-1}D T_{j}(z_{0})T_{i}z_{0}\\right]=\\mathbb{E}_{\\boldsymbol\\tau}\\left[\\displaystyle\\sum_{1\\leq i<j\\leq n}D F_{\\boldsymbol\\tau(j)}(z_{0})F_{\\boldsymbol\\tau(i)}z_{0}\\right]}\\\\ &{\\phantom{=}=\\displaystyle\\frac{1}{n!}\\sum_{\\tau\\in S_{n}}\\sum_{1\\leq i<j\\leq n}D F_{\\boldsymbol\\tau(j)}(z_{0})F_{\\boldsymbol\\tau(i)}z_{0}}\\\\ &{\\phantom{=}=\\displaystyle\\frac{1}{2}\\sum_{i\\neq j}D F_{j}(z_{0})F_{i}z_{0},}\\end{array}\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "where in getting the third line we have used the previously made observation that for any fixed $i$ and $j$ With $i\\neq j$ , the term $D F_{j}(z_{0})F_{i}z_{0}$ apearsexactly $\\frac{n!}{2}$ times in the sum on the second line. Hence, taking the expectation with respect to the random permutation on (135) we get ", "page_idx": 70}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathbb{E}_{\\tau}\\left[z^{\\sharp}\\right]=z_{0}-\\frac{n\\beta}{2}F z_{0}+\\frac{\\beta^{2}}{4}\\sum_{j=1}^{n}D F_{j}(z_{0})F_{j}z_{0}+\\frac{\\beta^{2}}{4}\\sum_{i\\neq j}D F_{j}(z_{0})F_{i}z_{0}+\\frac{1}{2}\\mathbb{E}_{\\tau}\\left[\\epsilon_{n}\\right]}}\\\\ {{\\displaystyle=z_{0}-\\frac{n\\beta}{2}F z_{0}+\\frac{\\beta^{2}}{4}\\sum_{j=1}^{n}\\sum_{i=1}^{n}D F_{j}(z_{0})F_{i}z_{0}+\\frac{1}{2}\\mathbb{E}_{\\tau}\\left[\\epsilon_{n}\\right]}}\\\\ {{\\displaystyle=z_{0}-\\frac{n\\beta}{2}F z_{0}+\\frac{n^{2}\\beta^{2}}{4}D F(z_{0})F z_{0}+\\frac{1}{2}\\mathbb{E}_{\\tau}\\left[\\epsilon_{n}\\right].}}\\end{array}\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "Comparing this to (7) when $\\eta_{1}=\\eta_{2}=n\\beta/2$ , we indeed see that the update rule of SEG-RRA with $\\alpha={\\mathbb{\\beta}}/2$ achieves a second-order matching on expectation to the (deterministic) EG update with stepsize ${n\\beta}/{2}$ ", "page_idx": 70}, {"type": "text", "text": "We also conjecture that the relatively worse performance of SEG-RRA with $\\alpha\\,=\\,^{\\beta}\\!/2$ compared to SEG-FFA is because the error over an epoch is ${\\cal O}(\\eta^{3})$ only on expectation, and thus the actual error occurring in each epoch can be larger than ${\\cal O}(\\eta^{3})$ : Unfortunately, our convergence analysis on SEG-FFA relies on the error over an epoch being $\\dot{O}(\\eta^{3})$ deterministically (cf. Proposition 5.3), hence cannot be directly applied to SEG-RRA with $\\alpha=\\beta/2$ . We leave the search for a theoretical explanation on this alluring performance of SEG-RRA with $\\alpha={}^{\\beta}/2$ as a stimulating direction for future work. ", "page_idx": 70}, {"type": "text", "text": "1.3 Monotone Case: Comparison with Hsieh et al. [25] ", "text_level": 1, "page_idx": 70}, {"type": "text", "text": "Let us also compare the performance of SEG-FFA with the independent-sample double stepsize SEG (DSEG) by Hsieh et al. [25]. Writing in terms of the finite-sum structure, the update rule of DSEG ", "page_idx": 70}, {"type": "image", "img_path": "OJxua0PAIo/tmp/bae2117564d175c9e72c282d988a9bb4e539993d1f90d4ad9bff5d24144a3eba.jpg", "img_caption": ["Figure 3: Experimental results in the monotone example, comparing SEG-FFA and the methods proposed by Hsieh et al. [25]. By the same reason as in Figure 2, we plot $\\|\\boldsymbol{F}\\boldsymbol{z}_{0}^{t/2}\\|^{2}\\big/\\|\\boldsymbol{F}\\boldsymbol{z}_{0}^{0}\\|^{2}$ for SEG-FFAonly. "], "img_footnote": [], "page_idx": 71}, {"type": "text", "text": "can be written as ", "page_idx": 71}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{{\\pmb w}^{k}\\leftarrow z^{k}-\\eta_{1,k}{\\pmb F}_{i(1,k)}z^{k}}}\\\\ {{z^{k+1}\\leftarrow z^{k}-\\eta_{2,k}{\\pmb F}_{i(2,k)}{\\pmb w}^{k}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 71}, {"type": "text", "text": "where $i(1,k)$ and $i(2,k)$ are random indices that are independently drawn from $[n]$ for each $k$ .The stepsizes are chosen in the form of $\\eta_{1,k}=\\Theta\\big({}^{1}\\!/k^{r_{1}}\\big)$ and $\\eta_{2,k}=\\Theta\\big(1/k^{r_{2}}\\big)$ , where setting $r_{1}\\leq r_{2}$ is the key point of DSEG. Two choices of the exponent pair $(r_{1},r_{2})$ proposed in [25] are $(1/3,2/3)$ for general monotone problems and $(0,1)$ exclusively for the case when $\\pmb{F}$ is affine. ", "page_idx": 71}, {"type": "text", "text": "We again use the same component functions as in the previous experiment. The setup for running SEG-FFA are kept the same. For DSEG, we use the default choices suggested by Hsieh et al. [25], namely $\\eta_{1,k}=\\gamma_{0}/(k\\!+\\!19)^{r_{1}}$ and $\\eta_{2,k}\\,=\\,\\eta_{0}/(k\\!+\\!19)^{r_{2}}$ , where $(\\gamma_{0},\\eta_{0})=(1,0.1)$ for the bilinear case with $(r_{1},r_{2})=(0,1)$ and $(\\gamma_{0},\\eta_{0})=(0.1,0.05)$ for the general case with $(r_{1},r_{2})=(1/3,2/3)$ ", "page_idx": 71}, {"type": "text", "text": "The results are displayed in Figure 3, where the details on how the plots are drawn are the same as Figure 2. Here we can clearly see that SEG-FFA outperforms both versions of DSEG. ", "page_idx": 71}, {"type": "text", "text": "1.4  Strongly Monotone Case Again, with Various Stepsizes ", "text_level": 1, "page_idx": 71}, {"type": "text", "text": "We also ran the experiment on strongly monotone problems described in Section 6, but with changing the stepsizes.We tested six different values of $\\eta_{k}$ ; we have tested with $\\eta_{k}\\;=\\;a\\,\\times\\,10^{b}$ where $a\\in\\{1,2,5\\}$ and $b\\in\\{-4,-3\\}$ . Notice that the case $\\eta_{k}=10^{-3}$ is exactly the experiment conducted in Section 6. ", "page_idx": 71}, {"type": "text", "text": "The results are plotted in Figure 4. The overall details are the same as described in Section 6, as the only difference is the stepsize choice. We can observe that, while the initial speed of convergence may not be the fastest depending on the stepsize, SEG-FFA is always the method that eventually finds the point with the smallest gradient. In other words, as predicted by our theoretical analyses, the supremacy of SEG-FFA is in general not affected by the choice of the stepsize, as long as the chosen stepsize is reasonably small. ", "page_idx": 71}, {"type": "image", "img_path": "OJxua0PAIo/tmp/416ef3e4eab4b69b971c190485bdd00336be679a325877d0ba4340e24e4b39d5.jpg", "img_caption": ["Figure 4: Experimental results on the strongly monotone problems with different stepsizes. Notice that Figure 4d is exactly the plot that is included in Section 6. The only difference between the experiments conducted is the choice of the stepsize. "], "img_footnote": [], "page_idx": 72}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 73}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 73}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 73}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 73}, {"type": "text", "text": "Justification: The abstract and the introduction well summarizes our theoretical results and the problem settings we are considering. ", "page_idx": 73}, {"type": "text", "text": "Guidelines: ", "page_idx": 73}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 73}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 73}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 73}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 73}, {"type": "text", "text": "Justification: While we do not have a separate \"Limitations\" sections, in Section 3 we thoroughly discuss about the assumptions we have imposed. The paper is highly theoretical, hence the other factors listed in the guidelines below are either not applicable to this paper, or apparent from the statements of the theorems/lemmata/propositions and the discussions thatfollow. ", "page_idx": 73}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 73}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should refect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 73}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 73}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 74}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 74}, {"type": "text", "text": "Justification: Section 3 is devoted for the discussions on the assumptions. Full proofs of the theorems/lemmata/propositions can be found in the appendices. ", "page_idx": 74}, {"type": "text", "text": "Guidelines: ", "page_idx": 74}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 74}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 74}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 74}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 74}, {"type": "text", "text": "Justification: In Appendix I, we provide full explanations on how the experiments have been conducted. We have also submitted the exact code that we used for our experiments as a supplemental material. ", "page_idx": 74}, {"type": "text", "text": "Guidelines: ", "page_idx": 74}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in ", "page_idx": 74}, {"type": "text", "text": "some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 75}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 75}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 75}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 75}, {"type": "text", "text": "Justification: We have submitted the exact code that we used for our experiments as a supplemental material, so that it becomes revealed to the public once our paper gets accepted. Guidelines: ", "page_idx": 75}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 75}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 75}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 75}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 75}, {"type": "text", "text": "Justification: The overall settings are discussed in Appendix I. The code we submit along with the paper is an exact copy of the one we used in the reported experiments, so the details not included in the paper shall be found in the code itself. ", "page_idx": 75}, {"type": "text", "text": "Guidelines: ", "page_idx": 75}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 75}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 75}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 75}, {"type": "text", "text": "Answer: [No] ", "page_idx": 75}, {"type": "text", "text": "Justification: Our paper is mainly theoretical, and the experiments are to demonstrate that our analyses are correct. Hence, we claim that error bars or information about the statistical significance are not necessary, and rather, the interpretations we made regarding our experiments in the relevant section(s) are enough. ", "page_idx": 75}, {"type": "text", "text": "Guidelines: ", "page_idx": 75}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 76}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 76}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 76}, {"type": "text", "text": "Answer: [No] ", "page_idx": 76}, {"type": "text", "text": "Justification: The experiments are numerical validations of our theoretical analyses using simple quadratic functions, so they should be executable on any modern computer with a reasonableCPU. ", "page_idx": 76}, {"type": "text", "text": "Guidelines: ", "page_idx": 76}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 76}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 76}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 76}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 76}, {"type": "text", "text": "Justification: We have read through the Code of Ethics, but due to the theoretical nature of the paper, there are no risks regarding ethical issues. ", "page_idx": 76}, {"type": "text", "text": "Guidelines: ", "page_idx": 76}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 76}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 76}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 76}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 77}, {"type": "text", "text": "Justification: There are no societal impacts of this paper, as it is a theory paper. Guidelines: ", "page_idx": 77}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 77}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 77}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 77}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 77}, {"type": "text", "text": "Justification: This paper is highly theoretical, hence poses no such risks. ", "page_idx": 77}, {"type": "text", "text": "Guidelines: ", "page_idx": 77}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 77}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 77}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 77}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 77}, {"type": "text", "text": "Justification: Packages used in the experiments, NumPy, SciPy, and Matplotlib, are cited.   \nNo existing data nor models are used. ", "page_idx": 77}, {"type": "text", "text": "Guidelines: ", "page_idx": 77}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 77}, {"type": "text", "text": "", "page_idx": 78}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 78}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 78}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 78}, {"type": "text", "text": "Justification: Our paper provides novel theoretical results rather than datasets or models, hence this question is not applicable. ", "page_idx": 78}, {"type": "text", "text": "Guidelines: ", "page_idx": 78}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose asset isused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 78}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 78}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 78}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 78}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 78}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 78}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 78}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)wereobtained? ", "page_idx": 78}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 78}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 78}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 79}]