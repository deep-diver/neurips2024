[{"type": "text", "text": "Contrastive dimension reduction: when and how? ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sam Hawke YueEn Ma Department of Biostatistics Department of Statistics & Operations Research University of North Carolina at Chapel Hill University of North Carolina at Chapel Hill shawke@unc.edu myueen@unc.edu ", "page_idx": 0}, {"type": "text", "text": "Didong Li Department of Biostatistics University of North Carolina at Chapel Hill didongli@unc.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Dimension reduction (DR) is an important and widely studied technique in exploratory data analysis. However, traditional DR methods are not applicable to datasets with a contrastive structure, where data are split into a foreground group of interest (case or treatment group), and a background group (control group). This type of data, common in biomedical studies, necessitates contrastive dimension reduction (CDR) methods to effectively capture information unique to or enriched in the foreground group relative to the background group. Despite the development of various CDR methods, two critical questions remain underexplored: when should these methods be applied, and how can the information unique to the foreground group be quantified? In this work, we address these gaps by proposing a hypothesis test to determine the existence of contrastive information, and introducing a contrastive dimension estimator (CDE) to quantify the unique components in the foreground group. We provide theoretical support for our methods and validate their effectiveness through extensive simulated, semi-simulated, and real experiments involving images, gene expressions, protein expressions, and medical sensors, demonstrating their ability to identify the unique information in the foreground group. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "High-dimensional datasets are ubiquitous in the era of big data, arising from applications such as image data (Shorten and Khoshgoftaar, 2019), gene/protein expressions (Bhola and Singh, 2018), and medical data from wearable devices (Cho et al., 2021; Banaee et al., 2013), to name a few. Dimension reduction (DR) is crucial in these contexts for various reasons (Fan et al., 2014): improving data visualization, reducing computational cost for downstream analysis (Fan and Li, 2006), minimizing noise (Thudumu et al., 2020), and enhancing interpretability (Johnstone and Titterington, 2009). ", "page_idx": 0}, {"type": "text", "text": "DR has been a vibrant research area for several decades, leading to the development of numerous methods (Ayesha et al., 2020). Principal component analysis (PCA, Hotelling (1933)), for instance, maximizes the variance in the reduced space and is widely applied across different domains (Jirsa et al., 1994; Novembre and Stephens, 2008; Pasini, 2017). Other methods, such as Multi-Dimensional Scaling (MDS, Torgerson (1952)) and Isomap (Tenenbaum et al., 2000), focus on preserving pairwise Euclidean and geodesic distances between samples, respectively. Probabilistic PCA (PPCA, Tipping and Bishop (1999)) incorporates uncertainty quantification through a probablistic model. Additionally, techniques such as t-distributed Stochastic Neighborhood Embedding (tSNE, Van der Maaten and Hinton (2008)) and Uniform Manifold Approximation (UMAP, McInnes et al. (2018)), preserve the local probability distribution and manifold topological structure in the data, respectively. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In recent years, a new type of data has garnered attention, particularly in biomedical research involving case-control studies. Here, data are divided into a foreground group (case or treatment group) and a background group (control group). The objective is to identify low-dimensional representations unique to or enriched in the foreground group. This scenario necessitates contrastive dimension reduction (CDR) methods. For example, in a benchmark dataset for CDR on mouse protein expressions (Higuera et al., 2015), the foreground group consists of mice who received a shock therapy, while the background group includes mice that did not. The goal is to isolate the protein expression patterns unique to the foreground group, i.e., the shocked mice. ", "page_idx": 1}, {"type": "text", "text": "To address this problem, several CDR methods have been developed. For example, Contrastive PCA (CPCA, Abid et al. (2018)) extends PCA by maximizing the variation in the foreground while minimizing variation in the background. Additionally, Probabilistic Contrastive PCA (PCPCA, Li et al. (2020)) generalizes both CPCA and PPCA by incorporating a probabilistic model into the contrastive setting, enabling statistical inference. Furthermore, the Contrastive Latent Variable Model (CLVM, Severson et al. (2019)) introduces latent variables specifically designed for this contrastive setting. Moreover, the Contrastive Variational Autoencoder (CVAE, Abid and Zou (2019)) and Contrastive Variational Inference (CVI, Weinberger et al. (2023)) both leverage deep learning approaches to detect nonlinear patterns in the data. Collectively, these methods aim to isolate patterns unique to the foreground data relative to the background. These methods have been successful with a variety of datasets, identifying patterns that are often scientifically meaningful and opening doors to new scientific discoveries. For example, studies using these methods on mouse protein expression data have successfully identified subgroup structures, such as distinguishing between mice with and without Down Syndrome, and highlighted key proteins responsible for these differences. We reference various applications and discoveries using these methods in section 6 and appendix D. ", "page_idx": 1}, {"type": "text", "text": "Despite the development of these CDR methods, two critical questions remain underexplored: when should these CDR methods be applied, and how many unique components exist in the foreground group? The first question can be reframed as follows: given foreground and background groups, can we determine whether there exists unique information in the foreground group? The second question pertains to estimating the number of unique representations to the foreground, termed contrastive dimension in this paper. This is a counterpart of the intrinsic dimension estimation problem, with a rich literature (Camastra and Staiano, 2016; Levina and Bickel, 2004), but in the contrastive context. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we answer the first question by proposing a hypothesis test to determine the existence of unique information in the foreground data. To answer the second question, we introduce the rigorous notion of contrastive dimension and provide a consistent estimator of this quantity. We provide theoretical support of our methods and validate their effectiveness through extensive simulated, semi-simulated, and real experiments involving images, gene expressions, protein expressions, and medical sensors. ", "page_idx": 1}, {"type": "text", "text": "The rest of the paper is organized as follows. In section 2, we provide more background on CDR methods and present our definition of contrastive dimension. In section 3, we present our solutions to the above questions: a hypothesis test and an estimator for the contrastive dimension. In section 4, we prove the consistency of our proposed estimator of the contrastive dimension and establish a finite-sample error bound. We demonstrate the effectiveness of the methods on simulated and semisimulated data in section 5 and real data in section 6. Finally, in section 7, we discuss the strengths and weaknesses of our methods and identify potential directions for further study. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this section, we introduce our notations and provide a more detailed overview of various existing CDR methods. Throughout this paper, we denote the foreground data as $X=\\{x_{1},\\cdot\\cdot\\cdot\\,,x_{n_{x}}\\}\\subset\\mathbb{R}^{\\bar{p}}$ with dimension $p$ and sample size $n_{x}$ , and background data as $Y=\\{y_{1},\\cdot\\cdot\\cdot\\,,\\bar{y}_{n_{y}}\\}\\subset\\mathbb{R}^{p}$ with the same dimension $p$ but potentially different sample size $n_{y}$ . We do not make any assumptions on the ratio of sample sizes between groups, $n_{x}:n_{y}$ . For simplicity, we also assume each dataset is centered separately. Next, we briefly overview some existing CDR methods. ", "page_idx": 1}, {"type": "text", "text": "Contrastive Principal Component Analysis (CPCA). CPCA (Abid et al., 2018) was proposed to uncover the low-dimensional structure that is unique to the foreground data $X$ relative to the background data $Y$ . Taking one-dimensional CPCA as an example, the objective is to find a direction, represented by a unit vector $\\boldsymbol{v}\\in\\mathbb{R}^{p}$ , that explains more variance in the foreground among all directions, and less variance in the background. Let $\\begin{array}{r}{C_{X}=\\frac{1}{n_{x}}\\sum_{i=1}^{n_{x}}x_{i}x_{i}^{\\top}}\\end{array}$ and $\\begin{array}{r}{C_{Y}\\overset{\\smile}{=}\\frac{1}{n_{y}}\\sum_{j=1}^{n_{y}}\\overset{\\smile}{y_{j}}\\overset{\\sideset{}}{y_{j}^{\\top}}}\\end{array}$ be the sample covariance matrices of two datasets. Then, CPCA solves the following optimization problem: ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n{\\underset{\\|v\\|=1}{\\operatorname{argmax}}}\\,v^{\\top}C_{X}v-\\gamma v^{\\top}C_{Y}v={\\underset{\\|v\\|=1}{\\operatorname{argmax}}}\\,v^{\\top}C v,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\gamma\\in[0,\\infty]$ is a tuning parameter, called the contrastive parameter, and $C=C_{X}-\\gamma C_{Y}$ is called the contrastive covariance matrix. The optimal $v$ is simply the top eigenvector of $C$ . When $\\gamma=0$ , CPCA coincides with PCA, and when $\\gamma=\\infty$ , CPCA finds the direction that explains the least variance in the background group. ", "page_idx": 2}, {"type": "text", "text": "Contrastive Latent Variable Model (CLVM). CLVM (Severson et al., 2019) is a latent variable model, similarly proposed to discover patterns enriched in the foreground group $X$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\nx_{i}=S z_{i}+W t_{i}+\\varepsilon_{i},\\;y_{j}=S z_{j}+\\varepsilon_{j},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $z_{i},z_{j}\\,\\in\\,\\mathbb{R}^{k}$ and $t_{i}\\in\\mathbb{R}^{t}$ are the latent variables, and $\\varepsilon_{i},\\varepsilon_{j}\\,\\in\\,\\mathbb{R}^{p}$ are the noise terms. The factor loading $S\\in\\mathbb{R}^{p\\times k}$ represents the space shared between the foreground and background groups, while the factor loading $W\\stackrel{=}{\\in}\\mathbb{R}^{p\\times t}$ represents the space unique to the foreground group. The primary goal of CLVM is to identify W, which represents the foreground-specific information. ", "page_idx": 2}, {"type": "text", "text": "Probabilistic Contrastive Principal Component Analysis (PCPCA). PCPCA (Li et al., 2020) was proposed as a probabilistic model to extend CPCA, allowing for uncertainty quantification: ", "page_idx": 2}, {"type": "equation", "text": "$$\nx=W z_{x}+\\varepsilon_{x},y=W z_{y}+\\varepsilon_{y},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $W$ represents the factor loading, and $\\varepsilon_{x},\\varepsilon_{y}$ are noise terms. However, PCPCA utilizes an unconventional strategy for estimating $W$ . Instead of maximizing the joint likelihood, which would be appropriate if the foreground and background groups are assumed to share the same space, the PCPCA objective is to maximiz epp((YX ||WW ) )\u03b3 , where \u03b3 \u2208[0, \u221e] is the contrastive hyperparameter, as in CPCA. When $\\gamma=0$ , PCPCA reduces to Probablistic PCA (Tipping and Bishop, 1999) on the foreground data, while PCPCA reduces to CPCA as the noise level goes to zero. ", "page_idx": 2}, {"type": "text", "text": "Contrastive VAE (CVAE). CVAE (Abid and Zou, 2019) considers a broadened view of the datagenerating process, in which the foreground data $X$ follows an arbitrary probability distribution $f$ with unknown parameters $\\theta$ , conditional on salient variables $s$ and irrelevant variables $z$ . The background data $Y$ are assumed not to have the salient variables, as the CVAE model is given by ", "page_idx": 2}, {"type": "equation", "text": "$$\nx_{i}\\sim f_{\\theta}(\\cdot|s_{i},z_{i}),\\;y_{j}\\sim f_{\\theta}(\\cdot|0,z_{j}^{\\prime}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The problem here is to learn the parameters $\\theta$ of $f$ , which CVAE does by training two probabilistic encoders $q_{\\phi_{s}}(s|x)$ and $q_{\\phi_{z}}(z|x)$ (salient and irrelevant) to infer $s$ and $z$ , respectively, from the observed features, and a decoder network $f_{\\theta}(\\cdot)$ which reconstructs the original samples from $x=$ $[s,z]$ in the foreground data, or $y=[0,z^{\\prime}]$ in the background data. ", "page_idx": 2}, {"type": "text", "text": "Contrastive Variational Inference (CVI). CVI was proposed in Weinberger et al. (2023) specifically for the purpose of disentangling the split between treatment and control groups for single-cell data. To this end, CVI posits a data-generating process with latent variables $z_{n}$ , representing shared information between the two groups, and $t_{n}$ , representing information unique to the foreground group. Each gene expression in each sample of the foreground group is assumed to follow a generative process depending on $z_{n}$ and $t_{n}$ (both with $\\mathcal{N}(0,\\bar{I})$ priors), but in the background group $t_{n}=0$ is assumed, similarly to CVAE. The posterior distribution of CVI is analytically intractable, so it is approximated using variational inference. ", "page_idx": 2}, {"type": "text", "text": "Each of the methods above has a similar goal: to discover variation unique to the foreground group relative to the background. However, none of these methods enables detection of whether there exists such variation at all. Additionally, for each method, the assumed reduced dimension (or number of latent features unique to the foreground) was treated as a tuning parameter required as input to run the method, without an easy-to-implement method to choose such a dimension. ", "page_idx": 2}, {"type": "text", "text": "Motivated by this gap, we consider the following linear model: ", "page_idx": 2}, {"type": "equation", "text": "$$\nx_{i}=z_{i}+\\varepsilon_{i},\\;y_{j}=w_{j}+\\varepsilon_{j},\\;i=1,\\ldots,n_{x},\\;j=1,\\ldots,n_{y}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $z_{i}\\in V_{x}$ , $w_{j}\\ \\in\\ V_{y}$ for some low-dimensional linear subspaces $V_{x},V_{y}\\;\\subset\\;\\mathbb{R}^{p}$ , and $\\varepsilon_{i},\\varepsilon_{j}$ represent the noise. We let $x$ represent foreground and $y$ represent background. ", "page_idx": 3}, {"type": "text", "text": "However, defining and identifying the information that is truly unique to the foreground requires a more rigorous approach. For example, in CLVM, even if we estimate a nonzero $W$ , it may not present information unique to the foreground if $S$ and $W$ are not appropriately distinguished. This motivates the need for a precise definition of the contrastive subspace and contrastive dimension: ", "page_idx": 3}, {"type": "text", "text": "Definition 1. Under the above notation, we define the contrastive subspace to be $V_{x y}:=\\operatorname{Proj}_{V_{y}^{\\perp}}(V_{x})$ and the contrastive dimension, denoted by $d_{x y}$ , to be the dimension of the contrastive subspace: $d_{x y}:=\\dim(V_{x y})$ . ", "page_idx": 3}, {"type": "text", "text": "The rationale of our definition is that the contrastive subspace contains the information unique to the foreground data, guaranteed by the projection to $V_{y}^{\\perp}$ , the orthogonal complement of $V_{y}$ . In order to determine the existence of unique information in the foreground group, it is desirable to have a hypothesis test for testing $H_{0}:d_{x y}=0$ versus $H_{1}:d_{x y}>0$ . Furthermore, an estimator for $d_{x y}$ is valuable for providing a suggestion to use for the reduced dimension parameter in the CDR models presented above. ", "page_idx": 3}, {"type": "text", "text": "Remark 1. Based on model 1 and definition 1, $V_{x}\\subset V_{y}$ is equivalent to $d_{x y}=0$ . ", "page_idx": 3}, {"type": "text", "text": "Remark 2. Even a minute departure of $V_{x}$ from $V_{y}$ results in a nonzero $d_{x y}$ . ", "page_idx": 3}, {"type": "text", "text": "Because we are interested in detecting where the foreground differs from the background, we allow our framework to detect even a small departure of $V_{x}$ from $V_{y}$ , although a bigger departure will be easier to detect. This motivates our study of the principal angle between $V_{x}$ and $V_{y}$ in section 3. ", "page_idx": 3}, {"type": "text", "text": "It is worth noting that the new tasks we are proposing, namely determining whether there is unique information in the foreground group and quantifying how much, depart from the traditional CDR task of estimating the space representative of such unique information, assuming it exists. In this paper, we focus on our solution to these new tasks, starting with manageable assumptions about the data. ", "page_idx": 3}, {"type": "text", "text": "3 Methods ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we present our methods to address the questions of whether there exists unique information in the foreground group and how many dimensions there are unique to the foreground group. First, we detail the hypothesis test of $H_{0}:d_{x y}=0$ versus $H_{1}:d_{x y}>0$ . We introduce a contrastive version of the bootstrap resampling scheme, based on the null assumption that $d_{x y}=0$ , which is equivalent to the assumption of $V_{x}\\subset V_{y}$ . Next, we introduce the CDE of $d_{x y}$ . In both methods, we make the very reasonable sample size assumptions that $n_{x}>d_{x}$ and $n_{y}>d_{y}$ , as many existing studies have observed a small intrinsic dimension (Pope et al., 2021). The inference in both of these problems is based on the notion of principal angles between two linear subspaces (Ye and Lim, 2016): ", "page_idx": 3}, {"type": "text", "text": "Definition 2. Let $U$ and $V$ be two subspaces of $\\mathbb{R}^{p}$ with dimensions $d_{1}$ and $d_{2}$ , then the principal angles, denoted by $\\theta_{i}(U,V)$ $,V),\\,i=1,\\cdot\\cdot\\cdot\\,,\\operatorname*{min}(d_{1},d_{2})$ are defined recursively as follows ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\cos(\\theta_{i}(U,V))=\\operatorname*{max}_{\\substack{u_{i}\\in U,v_{i}\\in V,\\|u_{i}\\|=\\|v_{i}\\|=1,u_{i}^{\\top}u_{j}=v_{i}^{\\top}v_{j}=0,\\forall j<i}}u_{i}^{\\top}v_{i}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "To be clear, the $u_{i}$ and $v_{i}$ solving the maximization problem are not uniquely defined; however, the principal angles $\\theta_{i}$ are uniquely defined. The principal angles measure the \u201cdifference\" between two subspaces. For instance, if two spaces coincide, all principal angles are zero; when two spaces are orthogonal to each other, all principal angles are $\\pi/2$ . If $U_{0}$ and $V_{0}$ are orthogonal matrices spanning $U$ and $V$ , respectively, then the calculation of the principal angles is given by the following equation ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\cos(\\theta_{i}(U,V))=\\sigma_{i}(U_{0}^{\\top}V_{0}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\sigma_{i}(\\cdot)$ denotes the $i$ -th singular value of a matrix. As a result, we adopt this notion to study the relation between $V_{x}$ and $V_{y}$ for our purposes. ", "page_idx": 3}, {"type": "text", "text": "3.1 Hypothesis test ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We develop a hypothesis test for testing $H_{0}:d_{x y}=0$ versus $H_{1}:d_{x y}>0$ under the version of model 1 with the following assumptions, ", "page_idx": 4}, {"type": "equation", "text": "$$\nx_{i}=S_{x}z_{i}+\\varepsilon_{i},\\;y_{j}=S_{y}w_{j}+\\varepsilon_{j},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $S_{x}\\in\\mathbb{R}^{p\\times d_{x}}$ and $S_{y}\\in\\mathbb{R}^{p\\times d_{y}}$ are full rank, $z_{i}\\sim\\mathcal{N}_{d_{x}}(0,I),w_{i}\\sim\\mathcal{N}_{d_{y}}(0,I),\\mathbb{E}[\\varepsilon_{i}]=\\mathbb{E}[\\varepsilon_{j}]=$ $0,\\mathrm{Cov}(\\varepsilon_{i})=\\sigma_{x}^{2}I$ , and $\\mathrm{Cov}(\\varepsilon_{j})=\\sigma_{y}^{2}I$ . ", "page_idx": 4}, {"type": "text", "text": "First, we assume $\\operatorname*{max}(d_{x},d_{y})<p/2$ , which is a very weak assumption given that $p$ is often much bigger than the intrinsic dimension in high-dimensional datasets. ", "page_idx": 4}, {"type": "text", "text": "Remark 3. With model $^{\\,l}$ , we do not make any assumptions the relations between intrinsic dimensions $d_{x}$ and $d_{y}$ . However, by definition $^{\\,l}$ , $d_{x}>d_{y}$ implies that $d_{x y}>0$ , indicating that there must be some unique information in the foreground space. Therefore, for our hypothesis test, we focus on the more challenging but important case of $d_{x}\\leq d_{y}$ . ", "page_idx": 4}, {"type": "text", "text": "An equivalent formulation of the null and alternative hypotheses in this model is $H_{0}:{\\mathcal{C}}(S_{x})\\subset{\\mathcal{C}}(S_{y})$ versus $H_{1}\\,:\\,{\\mathcal{C}}(S_{x})~\\mathcal{C}~{\\mathcal{C}}(S_{y})$ , where $\\mathcal C(\\cdot)$ denotes the column space of a matrix. Motivated by the assumption that $V_{x}\\;\\subset\\;V_{y}$ under $H_{0}$ , we draw a sample of size $n_{x}$ with replacement from $\\{x_{1},\\ldots,x_{n_{x}}\\}$ , and we draw a sample of size $n_{y}$ with replacement from $\\{x_{1},\\ldots,\\bar{x_{n_{x}}},y_{1},\\ldots,y_{n_{y}}\\}$ . We repeat this bootstrap procedure $B$ $:B\\,=\\,\\mathrm{{i}}000$ throughout this paper) times and compare the largest angle between $\\widehat{V}_{x}$ and $\\widehat{V}_{y}$ in the original data with the distribution of the largest angles between those subspaces in the resampled data. The exact calculation is deferred to section 3.2. ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1: Contrastive Bootstrap Hypothesis Test ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Input: Foreground $\\{x_{i}\\}_{i=1}^{n_{x}}$ , Background $\\{y_{j}\\}_{j=1}^{n_{y}}$ , dimensions $d_{x}\\leq d_{y}$ , bootstrap parameter $B$ Output: p-value for test of $H_{0}:V_{x}\\subset V_{y}$ in model 1 1 $\\widehat{\\lambda}_{1}\\gets\\widehat{\\lambda}_{1}(x,y,d_{x},d_{y})$ ; // Compute as in Algorithm 2 2  for $b=1$ to $B$ do 3 $x_{*}\\gets$ sample with repl. $(x,n_{x})$ ; // Resample foreground 4 $y_{\\ast}\\leftarrow$ sample with repl. $(\\{x,y\\},n_{y})$ ; // Resample background, pooled 5 $\\lambda_{b}^{*}\\gets\\widehat{\\lambda}_{1}(x_{*},y_{*},d_{x},d_{y})$ ; // Compute as in Algorithm 2 6 p \u2190#{\u03bbb\u2217< \u03bb1} 7 return $p$ ", "page_idx": 4}, {"type": "text", "text": "", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We demonstrate with simulations in section 5 that this hypothesis test produces conservative results, and in section 7 we discuss a potentially more powerful likelihood-based alternative. Additionally, this hypothesis test requires as input intrinsic dimension estimates for $d_{x}$ and $d_{y}$ , for which there exists a rich literature with a variety of sophisticated methods (Campadelli et al., 2015). This conservatism is designed to reduce the likelihood of false positives, but it can also lead to some signals not being detected by the hypothesis test. One explanation for this phenomenon could be that the test\u2019s resampling technique might not handle nonlinear structures in the data effectively (e.g. if the data lie on a nonlinear manifold). Because $V_{x}$ and $V_{y}$ are assumed to be linear subspaces, a nonlinear pattern can obscure the differences the test is designed to detect. ", "page_idx": 4}, {"type": "text", "text": "3.2 Contrastive dimension estimator (CDE) ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We start from the following lemma to link our target, $d_{x y}$ , and principal angles between $V_{x}$ and $V_{y}$ . ", "page_idx": 4}, {"type": "text", "text": "As a result, to construct an estimator of $d_{x y}$ , it suffices to estimate $V_{x},V_{y}$ , and calculate the corresponding principal angles. Based on eq. (1), $V_{x}$ and $V_{y}$ consist of the top $d_{x}$ and $d_{y}$ eigenvectors of $\\Sigma_{x}$ and $\\Sigma_{y}$ , respectively, which can be estimated by sample covariance matrices $\\begin{array}{r}{\\widehat{\\Sigma}_{x}:=\\frac{1}{n_{x}}\\sum_{i=1}^{n_{x}}x_{i}x_{i}^{\\top}}\\end{array}$ and $\\begin{array}{r}{\\widehat{\\Sigma}_{y}:=\\frac{1}{n_{y}}\\sum_{j=1}^{n_{y}}y_{j}y_{j}^{\\top}}\\end{array}$ . Thus, we propose the following estimator: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widehat{d}_{x y}:=\\#\\left\\{i:\\theta_{i}\\left(\\mathrm{eig}_{1:d_{x}}\\left(\\widehat{\\Sigma}_{x}\\right),\\mathrm{eig}_{1:d_{y}}\\left(\\widehat{\\Sigma}_{y}\\right)\\right)>\\epsilon\\right\\}+\\operatorname*{max}(d_{x}-d_{y},0),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\epsilon>0$ is a small toleration due to randomness from finite samples, and $\\mathrm{eig}_{1:d}$ refers to the span of the first $d$ eigenvectors. Let $\\widehat{U}_{x}:=\\mathrm{eig}_{1:d_{x}}\\left(\\widehat{\\Sigma}_{x}\\right)$ and $\\widehat{U}_{y}:=\\mathrm{eig}_{1:d_{y}}\\left(\\widehat{\\Sigma}_{y}\\right)$ , by eq. (2), we have $\\lambda_{i}:=\\cos(\\theta_{i})=\\sigma_{i}(\\widehat{U}_{x}^{\\top}\\widehat{U}_{y})$ , leading to algorithm 2. ", "page_idx": 5}, {"type": "image", "img_path": "IgU8gMKy4D/tmp/9d9a3a2bd1c2caa7c086321d5963693e1019669707cb5719afc852953875a693.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Note that algorithm 2 takes intrinsic dimensions $d_{x}$ and $d_{y}$ as input. In practice, the user must choose a method to estimate the intrinsic dimension of both the foreground and background datasets. This flexibility allows for the incorporation of different intrinsic dimension estimators to complement our approach (Bac et al., 2021; Pope et al., 2021). ", "page_idx": 5}, {"type": "text", "text": "Some users may only be interested in estimating the contrastive dimension $\\widehat{d}_{x y}$ and not in the hypothesis test for $d_{x y}~=~0$ . These users can skip the hypothesis testing s tep. However, the hypothesis test offers a direct way to assess uncertainty, guiding users on whether it is worthwhile to proceed with contrastive dimension reduction methods. ", "page_idx": 5}, {"type": "text", "text": "In the next section, we study the asymptotic and finite-sample behavior of this estimator $\\widehat{d}_{x y}$ . ", "page_idx": 5}, {"type": "text", "text": "4 Consistency and finite sample error bound ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we provide theoretical support for the estimator $\\widehat{d}_{x y}$ presented above, first by establishing its consistency, and then by constructing a finite-sample  error bound for it. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1. Assume that the second moments $\\Sigma_{x}$ and $\\Sigma_{y}$ are finite for both groups, and that the top $d_{x}+1$ eigenvalues of $\\Sigma_{x}$ and top $d_{y}+1$ eigenvalues of $\\Sigma_{y}$ are distinct. Then, our proposed estimator $\\widehat{d}_{x y}$ is consistent: $\\widehat{d}_{x y}\\ \\frac{n_{x}\\to\\infty}{n_{y}\\to\\infty}\\ d_{x y}$ ", "page_idx": 5}, {"type": "text", "text": "The assumption of distinct eigenvalues is common in the literature (Vershynin, 2018) and ensures the identifiability of eigenvectors for each covariance matrix. Because we are interested in convergence to the space spanned by the top eigenvectors, rather than individual eigenvectors themselves, this assumption might be relaxed to the eigengap assumption that the $d_{x}$ th and $(d_{x}+1)\\mathfrak{t}\\mathrm{l}$ h eigenvalues of $\\Sigma_{x}$ are distinct, and that the $d_{y}$ th and $(d_{y}+1)$ th eigenvalues of $\\Sigma_{y}$ are distinct. See section 7 for more discussion. ", "page_idx": 5}, {"type": "text", "text": "To describe the finite sample error bound, we recall that $U_{x}$ and $U_{y}$ are spanned by the top $d_{x}$ and $d_{y}$ eigenvectors of $\\Sigma_{x}$ and $\\Sigma_{y}$ , respectively, and singular values of $W:=U_{x}^{\\top}U_{y}$ , denoted by $\\lambda_{i}=\\bar{\\sigma}_{i}(W),i=1,\\cdots,m$ , where $m=\\operatorname*{min}(d_{x},d_{y})$ , are the cosines of principal angles between $U_{x}$ and $U_{y}$ . In practice, estimating the contrastive dimension as an integer can be somewhat restrictive for providing a finite sample error bound. Instead, we report the singular values to account for more uncertainty and provide a more nuanced understanding. Thus, we present the following finite sample error bound (and hence the convergence rate) of these estimated singular values rather than $\\widehat{d}_{x y}$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 2. In addition to all assumptions in theorem $^{\\,l}$ , we assume $x$ and $y$ in model $^{\\,l}$ are sub-Gaussian. More precisely, assume that there exist constants $K_{x},K_{y}\\ge1$ such that ", "page_idx": 5}, {"type": "text", "text": "where $\\lVert\\cdot\\rVert_{\\psi_{2}}$ is defined as in Section 3.4 of Vershynin (2018). Then, for any $u>0$ , with probability at least $1-2e^{-u}$ , we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{j=1,\\ldots,m}\\left|\\widehat{\\lambda}_{j}-\\lambda_{j}\\right|\\leq C\\sum_{k\\in\\{x,y\\}}\\sqrt{d_{k}}\\delta_{k}^{-1}K_{k}^{2}\\left(\\sqrt{\\frac{p+u}{n_{k}}}+\\frac{p+u}{n_{k}}\\right)\\|\\Sigma_{k}\\|=O(n_{x}^{-1/2}+n_{y}^{-1/2})\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $C$ is an absolute, positive constant, $\\delta_{\\bullet}$ is the minimum eigengap among the top $d_{\\bullet}+1$ eigenvalues of $\\Sigma_{\\bullet}$ , and $\\lVert\\bullet\\rVert$ denotes spectral norm. ", "page_idx": 6}, {"type": "text", "text": "To show the finite sample error bound, we need the sub-Gaussianity assumption to ensure that the tails of the distribution are not too heavy. This assumption is also common and encompasses a large class of distributions (Vershynin, 2018). ", "page_idx": 6}, {"type": "text", "text": "Having established the theoretical support for the estimator, in the next section we focus on applying our methods to simulated data. ", "page_idx": 6}, {"type": "text", "text": "5 Simulations ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To demonstrate the validity of our methods in practice, we apply them to four simulations for different purposes. The first two examples are purely simulated, fully under our control, with $d_{x y}=0$ for the first one and $d_{x y}>0$ for the second one. Because these first two simulations are purely synthetic, we repeat them both 100 times and report the means and standard deviations of the $p$ -value, $\\widehat{d}_{x y}$ , and the four smallest singular values. The third simulation steps away from the ground truth bein g completely known by combining synthetic image data with known dimension, with real, grassy noise image data, elucidating the methods\u2019 performance in an interpretable setting. The final simulation is a variation on the third one, in which the synthetic data are replaced with MNIST images (Deng, 2012). ", "page_idx": 6}, {"type": "text", "text": "We describe the experimental settings of the simulations in more detail below and present results, including the $p$ -value from our test $\\hat{\\widehat{d}}_{x y}$ , and the smallest four singular values, as shown in Table 4. We denote the estimates with a hat, if needed. In all these examples, the threshold is $\\epsilon=0.1$ . ", "page_idx": 6}, {"type": "text", "text": "We do not compare $\\widehat{d}_{x y}$ with other estimators of contrastive dimension because, to our knowledge, no such estimators h ave been proposed in the literature. ", "page_idx": 6}, {"type": "table", "img_path": "IgU8gMKy4D/tmp/0627a83489daa995a0f4cd31b87a3cbc2e995b82a3815221beb7cd971c2c0a63.jpg", "table_caption": ["Table 1: Summary of simulation results "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Simulation 1. In this first simulation, we generate data from the specific case of model (3) with variance $\\sigma_{x}^{2}=\\sigma_{y}^{2}=0.25$ , $\\mathcal{C}(S_{x})\\subset\\mathcal{C}(S_{y})$ such that $d_{x y}=0$ , $n_{x}=n_{y}=100$ . Because $d_{x y}=0$ is the assumption of $H_{0}$ for the hypothesis test in Section 3, we repeat this simulation 100 times to empirically demonstrate the conservative type I error rate (defined by the proportion of $p$ -values below $\\alpha$ ) versus pre-specified $\\alpha\\in(0,1)$ . We also repeat this simulation but with $n_{x}=n_{y}=200$ , and we present both plots of empirical type I error rate in Figure 1. ", "page_idx": 6}, {"type": "text", "text": "As shown in the first row of Table 4, both $\\widehat{d}_{x y}$ and the $p$ -value agree, on average, with the ground truth that $d_{x y}=0$ , showing that in this simulated setting, our methods do not erroneously detect a signal when there is none. However, given that the average smallest singular value value observed is 0.901, this simulation suggests that it may be prudent to use a choice of $\\epsilon$ greater than 0.1, i.e. a cutoff for the singular value that is below 0.9. The standard deviation for $\\widehat{d}_{x y}$ among the 100 replications is 0.52, which also suggests that a lower cutoff may be appropriate. The standard deviation of the $p$ -value is 0.153, and of each of the smallest singular values is 0.025, 0.013, 0.009, and 0.007, respectively. Both plots of observed type I error in Figure 1 demonstrate that, in this setting, the hypothesis test is conservative. We discuss a more powerful potential alternative in Section 7. ", "page_idx": 6}, {"type": "image", "img_path": "IgU8gMKy4D/tmp/a0289493618d85e2168224992bca571e4a82176aeef93daffa4e0c475696a5fd.jpg", "img_caption": ["Figure 1: Hypothesis test Type I error plots. Left: $n_{x}=n_{y}=100$ . Right: $n_{x}=n_{y}=200$ . "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "IgU8gMKy4D/tmp/824ac9d499d660c14dd59302a3c5a872a7ecaec3fa3b5d8e4b1474b6bccae031.jpg", "img_caption": ["Figure 2: Samples for simulations 3 (AB) and 4 (CD). AC: add line. BD: add grass. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Simulation 2. The only difference from Simulation 1 is that $d_{x y}=6$ . Because the singular values were all well below the cutoff of 0.9, the standard deviation of $\\hat{d}_{x y}$ among the 100 trials was 0; every trial achieved the correct estimate. Our hypothesis test produced a small $p$ -value of 0.029 on average (standard deviation 0.029), which at a significance level of $\\alpha=0.05$ leads one to the correct conclusion of rejecting $H_{0}$ and concluding that $d_{x y}>0$ . Moreover, the six smallest singular values are 0.059, 0.106, 0.159, 0.215, 0.279, 0.367 (standard deviations 0.023, 0.028, 0.032, 0.034, 0.038, and 0.050, respectively), all well below the threshold of 0.9, illustrating that CDE correctly identifies 6 directions unique to the foreground data. ", "page_idx": 7}, {"type": "text", "text": "Simulation 3. It is also useful to evaluate the performance of our methods on datasets where the ground truth is only partially known. So, in this simulation, we construct a semi-synthetic image dataset by generating 5000 disks of random centers $(c_{x},c_{y})\\in\\{5,\\ldots,24\\}^{2}$ and radii $\\dot{r^{\\star}}\\in\\{1,\\ldots,1\\bar{0}\\}$ uniformly, in the 28 by 28 grayscale pixel space. ", "page_idx": 7}, {"type": "text", "text": "Of these 5000 images, we draw a white line of random length beginning at the leftmost pixel of the sixth row on 2500 images to create the foreground (Fig. 2A). As a result, $d_{x}=4$ , parameterized by $c_{x},c_{y},r$ , and length of line. For background, we superimpose the remaining 2500 images on grass images as noise (Fig. 2B) so that $d_{y}>d_{x}$ . Although $d_{y}$ is not known, $d_{x y}$ is known to be one (the unique parameter being the length of the line). Here, we used the method of moments estimator (Amsaleg et al., 2018), leading to $\\widehat{d}_{y}=20$ . ", "page_idx": 7}, {"type": "text", "text": "Our method only gives one singular value (0.529) below the cutoff of 0.9, leading to a correct estimate of $\\widehat{d}_{x y}=1$ . Additionally, our hypothesis test produces a $p$ -value of 0, which suggests to reject $H_{0}$ in fav or of $H_{1}:d_{x y}>0$ . This simulation illustrates that our methods produce favorable results when one of the groups has an estimated intrinsic dimension. ", "page_idx": 7}, {"type": "text", "text": "Simulation 4. To assess our methods in a simulation setting where both $d_{x}$ and $d_{y}$ are unknown, we consider a very similar simulation to the previous one, but where we start with 5923 handwritten 0\u2019s from the MNIST dataset (Deng, 2012) instead of randomly generated disks. We further randomly split them into two sets, to create foreground by adding lines (Fig. 2C), and background by imposing on grass images (Fig. 2D), leading to the corrupted MNIST data proposed in Abid et al. (2018). Using method of moments, $\\widehat{d}_{x}=11$ and $\\widehat{d}_{y}=1\\bar{4}$ . Crucially, the contrastive dimension $d_{x y}=1$ is still known, allowing us to assess the results of our methods when $d_{x}$ and $d_{y}$ are both unknown. ", "page_idx": 7}, {"type": "text", "text": "CDE again accurately estimates exactly one singular value (0.256) below the cutoff of 0.9, leading to a correct estimate of $\\widehat{d}_{x y}=1$ . Furthermore, our hypothesis test produces a $p$ -value of 0, leading to the correct conclusio n that $d_{x y}>0$ at a significance level of $\\alpha=0.05$ . After successful applications of our methods to various settings, it is time to apply them to real datasets. ", "page_idx": 7}, {"type": "text", "text": "In this section, we apply our methods to real datasets previously studied in the literature to validate their effectiveness and interpret the results. In Table 2, we summarize relevant characteristics of each dataset along with the results. For each dataset, the intrinsic dimension estimation was performed with the method of moments estimator (Bac et al., 2021). Note that in each case where $\\widehat{d_{x}}>\\widehat{d_{y}}$ , the $p$ -value is not reported because the hypothesis test does not apply. The cutoff for the si ngular  values is set to 0.9 in all experiments. ", "page_idx": 8}, {"type": "table", "img_path": "IgU8gMKy4D/tmp/a5a24850f5d366c8e44381e620a7c796a62d343b5f1f8489b9d7e9d4aefef96d.jpg", "table_caption": ["Table 2: Summary of Results for Real Data Experiments "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Corrupted MNIST. The corrupted MNIST dataset was previously studied extensively by Abid et al. (2018), Abid and Zou (2019), Severson et al. (2019), and Li et al. (2020). The foreground group depicts handwritten 0\u2019s and 1\u2019s corrupted by images of grass, and the background contains only the grassy noise (no handwritten digits). In this case, the digits are unique to the foreground, supported by our $p$ -value of 0.008. Furthermore, the contrastive dimension estimate for this dataset is $\\bar{\\widehat{d}_{x y}}=5$ , in line with the commonly believed dimension of digits 0 and 1 (Pope et al., 2021). ", "page_idx": 8}, {"type": "text", "text": "Mouse Protein. This dataset (Higuera et al., 2015) is a benchmark in the literature, considered in Abid et al. (2018), Li et al. (2020), and Severson et al. (2019). The foreground group contains the protein expressions of mice, some with and some without Down Syndrome, who were subjected to shock therapy, while the background group consists of protein expressions of mice without Down Syndrome who did not receive shock therapy. The intrinsic dimension estimates of $\\widehat{d}_{x}=5$ and $\\widehat{\\vec{d}_{y}}=4$ require that $\\widehat{d}_{x y}>0$ . In this setting, where the foreground space is bigger than the background space, the foreground group necessarily carries unique information. Nevertheless, the CDE estimate for this dataset of $\\widehat{d}_{x y}=4$ , which aligns with the observation of $d_{x y}\\geq2$ in the literature. ", "page_idx": 8}, {"type": "text", "text": "mHealth. The mHealth dataset (Banos et al., 2015), studied in Abid et al. (2018); Severson et al. (2019), consists of measurements made from medical sensors while a subject performs certain actions. While the foreground group contains measurements taken of a subject either squatting or cycling, the background measures a subject lying down. Our $p_{\\|}$ -value $<0.001$ and $\\widehat{d}_{x y}=1$ . This estimate explains the result in Abid et al. (2018), duplicated here in figure 3, showin g that a single direction suffices to distinguish subgroups in the foreground data. ", "page_idx": 8}, {"type": "text", "text": "BMMC. This is a single-cell RNA sequencing dataset (Zheng et al., 2017) studied in Abid et al. (2018), Abid and Zou (2019), Severson et al. (2019), Li et al. (2020), and Weinberger et al. (2023). Here, the foreground group consists of gene expressions measured on bone marrow mononuclear cells (BMMCs) of patients with acute myeloid leukemia before and after receiving a transplant. The background group consists of healthy patients. Previous studies found that the 3rd and 4th PCs can separate pre- and post-transplant groups (Fig. 3), supported by our $\\widehat{d}_{x y}=17>4$ . However, the $p$ -value of 0.073 stands somewhat in contradiction to this result, maybe due to the conservative nature of this hypothesis test, as illustrated in figure 1 and discussed in section 7. ", "page_idx": 8}, {"type": "text", "text": "Small Molecule. This is a dataset of cell line responses to small-molecule therapy (McFarland et al., 2020) and used in Weinberger et al. (2023). Here, the foreground group contains measurements from 24 cell lines treated with idasanutlin, while the background are the same cell lines treated with the control dimethyl sulfoxide. $\\widehat{d}_{x}=12$ and $\\widehat{d}_{y}=11$ imply that $\\widehat{d}_{x y}>0$ . Nontheless, $\\widehat{d}_{x y}=4$ , suggesting that a CDR method w ith reduced di mension of 4 is appr opriate. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "ECCITE-Seq. This ECCITE-Seq dataset (Mimitou et al., 2019) was studied in Weinberger et al. (2023), containing joint RNA and surface protein expressions of pooled CRIPSR screens. The foreground group consists of cell transcriptomes of perturbed cells, while the background is a set of control cells. $\\overbar{d_{x}}=41$ and $\\widehat{d}_{y}=32$ imply that $\\hat{d}_{x y}>0$ . Therefore, the application of contrastive methods is suitable. Moreover, the CDE estimate of $\\widehat{d}_{x y}\\,=\\,13$ helpfully gauges the number of directions unique to the foreground, recommending a re duced dimension of 13. ", "page_idx": 9}, {"type": "text", "text": "Pathogen Data. Studied by Weinberger et al. (2023), this dataset (Haber et al., 2017) contains gene expressions in the epithelial cells of mice infected with either Salmonella or H. Poly (foreground) or healthy mice (background). $\\widehat{d}_{x y}=10$ , suggesting that a contrastive method might fit well with a reduced dimension of 10. How ever, the $p$ -value of 0.161 suggests that a linear contrastive method may not be appropriate. In fact, Weinberger et al. (2023) shows that only CVI, a deep learning based approach, works for this dataset, compared with linear methods, which aligns with our results. ", "page_idx": 9}, {"type": "text", "text": "Perturb-Seq: Studied by Weinberger et al. (2023), this dataset (Adamson et al., 2016) includes gene expressions measured of cells subjected to CRISPR-mediated perturbations (foreground) and cells treated with control guides (background). The hypothesis test is not applicable since $\\widehat{d}_{x}>\\widehat{d}_{y}$ . Regardless, CDE provides a helpful estimate for the amount of information unique to the fo regrou nd group, suggesting the efficacy of CDR methods. ", "page_idx": 9}, {"type": "text", "text": "CelebA: The CelebA dataset (Liu et al., 2015) was studied in Abid and Zou (2019) and with images of celebrities. We perform two experiments, with foreground being those wearing glasses and hat, respectively. In both, the background group is the remaining images. In the former case, $\\widehat{d}_{x y}=1$ , while in the latter $\\widehat{d}_{x y}=2$ . However, both cases have a somewhat large $p$ -value, perhaps due to the conservative nature of the hypothesis test or nonlinearity. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we have introduced a hypothesis test to determines whether a linear CDR model is appropriate for a given dataset and an estimator of contrastive dimension, representing the dimension of the subspace unique to the foreground group relative to the background. This estimate of contrastive dimension can be used as the reduced dimension in downstream analysis with a CDR method. Meanwhile, there are still some open problems left to future work. ", "page_idx": 9}, {"type": "text", "text": "Intrinsic Dimension Estimator. The problem of estimating intrinsic dimension is challenging in its own right (Camastra and Staiano, 2016; Campadelli et al., 2015; Fefferman et al., 2016). While methods for it have been developed, there is not a single accepted standard for it. Fortunately, since $\\left|\\widehat{d}_{x y}-d_{x y}\\right|\\leq\\left|\\widehat{d}_{x}-d_{x}\\right|+\\left|\\widehat{d}_{y}-d_{y}\\right|$ , our $\\widehat{d}_{x y}$ is robust when $d_{x}$ and $d_{y}$ are mis-specifying $d_{x}$ and $d_{y}$ in the Lipschitz-1 sense. ", "page_idx": 9}, {"type": "text", "text": "Hypothesis Test. Our hypothesis test introduces the notion of the contrastive bootstrap to allow for resampling under the assumption of the null hypothesis that the foreground space is a subset of the background space. However, a more powerful likelihood ratio test is applicable. Under model 3, the denominator is exactly from the solution of PPCA Tipping and Bishop (1999). However, the numerator involves a nontrivial constrained optimization problem. Naive gradient step method is neither effective nor scalable. An interesting future direction is to resolve this optimization issue. ", "page_idx": 9}, {"type": "text", "text": "Uniqueness of Eigenvalues. In section 4 we mentioned that the distinctness of eigenvalues ensures the identifiability of eigenvectors for each covariance matrix. However, this assumption can be relaxed since our focus is not on estimating each individual eigenvector but rather on identifying the eigenspace spanned by the top $d_{x}$ and top $d_{y}$ eigenvectors of $\\Sigma_{x}$ and $\\Sigma_{y}$ . ", "page_idx": 9}, {"type": "text", "text": "Extension to Nonlinear Setting. In section 6, we mentioned that the data lying on a nonlinear manifold could lead to the apparent inconsistency between the estimator $\\widehat{d}_{x y}$ being nonzero and the $p$ -value produced by the hypothesis test being large. To appropriately h andle the nonlinear case, a future work may consider a nonparametric extension analogous to the extension of CLVM (Severson et al., 2019) to CVAE (Abid and Zou, 2019). ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "SH was supported by NIH grants T32ES007018 and UM1 TR004406; DL was supported by NIH grants R01 AG079291, R56 LM013784, R01 HL149683, and UM1 TR004406, R01 LM014407, P30 ES010126. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Abid, A., Zhang, M. J., Bagaria, V. K., and Zou, J. (2018). Exploring patterns enriched in a dataset with contrastive principal component analysis. Nature communications, 9(1):2134.   \nAbid, A. and Zou, J. (2019). Contrastive variational autoencoder enhances salient features. arXiv preprint arXiv:1902.04601.   \nAdamson, B., Norman, T. M., Jost, M., Cho, M. Y., Nu\u00f1ez, J. K., Chen, Y., Villalta, J. E., Gilbert, L. A., Horlbeck, M. A., Hein, M. Y., et al. (2016). A multiplexed single-cell crispr screening platform enables systematic dissection of the unfolded protein response. Cell, 167(7):1867\u20131882.   \nAmsaleg, L., Chelly, O., Furon, T., Girard, S., Houle, M. E., Kawarabayashi, K.-i., and Nett, M. (2018). Extreme-value-theoretic estimation of local intrinsic dimensionality. Data Mining and Knowledge Discovery, 32(6):1768\u20131805.   \nAnderson, T. W. (1963). Asymptotic theory for principal component analysis. The Annals of Mathematical Statistics, 34(1):122\u2013148.   \nAyesha, S., Hanif, M. K., and Talib, R. (2020). Overview and comparative study of dimensionality reduction techniques for high dimensional data. Information Fusion, 59:44\u201358.   \nBac, J., Mirkes, E. M., Gorban, A. N., Tyukin, I., and Zinovyev, A. (2021). Scikit-dimension: a python package for intrinsic dimension estimation. Entropy, 23(10):1368.   \nBanaee, H., Ahmed, M. U., and Loutf,i A. (2013). Data mining for wearable sensors in health monitoring systems: a review of recent trends and challenges. Sensors, 13(12):17472\u201317500.   \nBanos, O., Villalonga, C., Garcia, R., Saez, A., Damas, M., Holgado-Terriza, J. A., Lee, S., Pomares, H., and Rojas, I. (2015). Design, implementation and validation of a novel open framework for agile development of mobile health applications. Biomedical engineering online, 14:1\u201320.   \nBhola, A. and Singh, S. (2018). Gene selection using high dimensional gene expression data: an appraisal. Current Bioinformatics, 13(3):225\u2013233.   \nCamastra, F. and Staiano, A. (2016). Intrinsic dimension estimation: Advances and open problems. Information Sciences, 328:26\u201341.   \nCampadelli, P., Casiraghi, E., Ceruti, C., and Rozza, A. (2015). Intrinsic dimension estimation: Relevant techniques and a benchmark framework. Mathematical Problems in Engineering, 2015:1\u201321.   \nCho, S., Weng, C., Kahn, M. G., Natarajan, K., et al. (2021). Identifying data quality dimensions for persongenerated wearable device data: Multi-method study. JMIR mHealth and uHealth, 9(12):e31618.   \nDeng, L. (2012). The mnist database of handwritten digit images for machine learning research. IEEE Signal Processing Magazine, 29(6):141\u2013142.   \nFan, J., Han, F., and Liu, H. (2014). Challenges of big data analysis. National science review, 1(2):293\u2013314.   \nFan, J. and Li, R. (2006). Statistical challenges with high dimensionality: Feature selection in knowledge discovery. arXiv preprint math/0602133.   \nFefferman, C., Mitter, S., and Narayanan, H. (2016). Testing the manifold hypothesis. Journal of the American Mathematical Society, 29(4):983\u20131049.   \nHaber, A. L., Biton, M., Rogel, N., Herbst, R. H., Shekhar, K., Smillie, C., Burgin, G., Delorey, T. M., Howitt, M. R., Katz, Y., et al. (2017). A single-cell survey of the small intestinal epithelium. Nature, 551(7680):333\u2013339.   \nHiguera, C., Gardiner, K. J., and Cios, K. J. (2015). Self-organizing feature maps identify proteins critical to learning in a mouse model of down syndrome. PloS one, 10(6):e0129126.   \nHotelling, H. (1933). Analysis of a complex of statistical variables into principal components. Journal of educational psychology, 24(6):417.   \nJirsa, V. K., Friedrich, R., Haken, H., and Kelso, J. S. (1994). A theoretical model of phase transitions in the human brain. Biological cybernetics, 71:27\u201335.   \nJohnstone, I. M. and Titterington, D. M. (2009). Statistical challenges of high-dimensional data.   \nLevina, E. and Bickel, P. (2004). Maximum likelihood estimation of intrinsic dimension. Advances in neural information processing systems, 17.   \nLi, D., Jones, A., and Engelhardt, B. (2020). Probabilistic contrastive principal component analysis. arXiv preprint arXiv:2012.07977.   \nLiu, Z., Luo, P., Wang, X., and Tang, X. (2015). Deep learning face attributes in the wild. In Proceedings of the IEEE international conference on computer vision, pages 3730\u20133738.   \nMcFarland, J. M., Paolella, B. R., Warren, A., Geiger-Schuller, K., Shibue, T., Rothberg, M., Kuksenko, O., Colgan, W. N., Jones, A., Chambers, E., et al. (2020). Multiplexed single-cell transcriptional response profiling to define cancer vulnerabilities and therapeutic mechanism of action. Nature communications, 11(1):4296.   \nMcInnes, L., Healy, J., and Melville, J. (2018). Umap: Uniform manifold approximation and projection for dimension reduction. arXiv preprint arXiv:1802.03426.   \nMimitou, E. P., Cheng, A., Montalbano, A., Hao, S., Stoeckius, M., Legut, M., Roush, T., Herrera, A., Papalexi, E., Ouyang, Z., et al. (2019). Multiplexed detection of proteins, transcriptomes, clonotypes and crispr perturbations in single cells. Nature methods, 16(5):409\u2013412.   \nNovembre, J. and Stephens, M. (2008). Interpreting principal component analyses of spatial population genetic variation. Nature genetics, 40(5):646\u2013649.   \nPasini, G. (2017). Principal component analysis for stock portfolio management. International Journal of Pure and Applied Mathematics, 115(1):153\u2013167.   \nPope, P., Zhu, C., Abdelkader, A., Goldblum, M., and Goldstein, T. (2021). The intrinsic dimension of images and its impact on learning. arXiv preprint arXiv:2104.08894.   \nSeverson, K. A., Ghosh, S., and Ng, K. (2019). Unsupervised learning with contrastive latent variable models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 4862\u20134869.   \nShorten, C. and Khoshgoftaar, T. M. (2019). A survey on image data augmentation for deep learning. Journal of big data, 6(1):1\u201348.   \nTenenbaum, J. B., Silva, V. d., and Langford, J. C. (2000). A global geometric framework for nonlinear dimensionality reduction. science, 290(5500):2319\u20132323.   \nThudumu, S., Branch, P., Jin, J., and Singh, J. (2020). A comprehensive survey of anomaly detection techniques for high dimensional big data. Journal of Big Data, 7:1\u201330.   \nTipping, M. E. and Bishop, C. M. (1999). Probabilistic principal component analysis. Journal of the Royal Statistical Society Series B: Statistical Methodology, 61(3):611\u2013622.   \nTorgerson, W. S. (1952). Multidimensional scaling: I. theory and method. Psychometrika, 17(4):401\u2013419.   \nVan der Maaten, L. and Hinton, G. (2008). Visualizing data using t-sne. Journal of machine learning research, 9(11).   \nVershynin, R. (2018). High-dimensional probability: An introduction with applications in data science, volume 47. Cambridge university press.   \nWeinberger, E., Lin, C., and Lee, S.-I. (2023). Isolating salient variations of interest in single-cell data with contrastivevi. Nature Methods, 20(9):1336\u20131345.   \nYe, K. and Lim, L.-H. (2016). Schubert varieties and distances between subspaces of different dimensions. SIAM Journal on Matrix Analysis and Applications, 37(3):1176\u20131197.   \nZheng, G. X., Terry, J. M., Belgrader, P., Ryvkin, P., Bent, Z. W., Wilson, R., Ziraldo, S. B., Wheeler, T. D., McDermott, G. P., Zhu, J., et al. (2017). Massively parallel digital transcriptional profiling of single cells. Nature communications, 8(1):14049. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Licensed Assets ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "1. BMMC data - CC BY-4.0   \n2. ECCITE-Seq data - Open Data Commons Open Database License (ODbL) v1.0   \n3. mHealth data - CC BY-4.0   \n4. Mouse Protein Expression data - CC BY-4.0   \n5. Pathogen data - Open Data Commons Open Database License (ODbL) v1.0   \n6. Perturb-seq data - Open Data Commons Open Database License (ODbL) v1.0   \n7. Small-molecule data - CC BY-4.0   \n8. MNIST data - CC BY-SA 3.0 license   \n9. contrastiveVI Github repository - BSD 3-Clause License   \n10. contrastive Github repository - MIT License   \n11. pcpca Github repository - MIT License   \n12. python scikit dimension package - BSD 3-Clause license   \n13. phthon anndata package - BSD 3-Clause License   \n14. python scanpy package - BSD 3-Clause License   \n15. python requests package - Apache-2.0 license   \n16. python pillow package - open source HPND License ", "page_idx": 12}, {"type": "text", "text": "Every contributor of new assets to this paper is an author. The authors have all mutually agreed to use the CC BY-4.0 license, as described in the README file of the Github in appendix B. ", "page_idx": 12}, {"type": "text", "text": "B Data and availability ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "All code and access to data pre-processing, as well as the original data sources, are available on Github at the following link. ", "page_idx": 12}, {"type": "text", "text": "https://github.com/myueen/contrastive-dimension-estimation ", "page_idx": 12}, {"type": "text", "text": "In table 3, we report the approximate time and memory requirements for each experiment. All experiments were run on a Linux-based virtual computer with 6500 conventional compute cores delivering 13,000 threads. We used only 1 core. ", "page_idx": 12}, {"type": "table", "img_path": "IgU8gMKy4D/tmp/b76a0d64d3c4bbab52cf8b766d9cc00cfbd5d94fc7c6450ae222006c48ce84b6.jpg", "table_caption": ["Table 3: Approximate Time and Memory Requirements per Experiment "], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "C Proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "C.1 Proof to lemma 1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Lemma 1. $d_{x y}=\\dim({\\mathrm{Proj}}_{V_{y}^{\\perp}}\\,V_{x})=\\#\\{i:\\theta_{i}(V_{x},V_{y})>0\\}+\\operatorname*{max}(d_{x}-d_{y},0).$ ", "page_idx": 13}, {"type": "text", "text": "Proof. Fix $V_{x},V_{y}\\subset\\mathbb{R}^{p}$ , and let $\\dim(V_{x})=d_{x}$ , $\\dim(V_{y})=d_{y}$ . Let $\\dim(V_{x}\\cap V_{y})=k.$ . ", "page_idx": 13}, {"type": "text", "text": "Let $u_{1},\\ldots,u_{k}$ be an orthonormal basis for $V_{x}\\cap V_{y}$ . It follows that $u_{1},\\dotsc.\\dotsc,u_{k}\\in V_{x}\\cap V_{y}$ are the first $k$ principal vectors for both $V_{x}$ and $V_{y}$ , corresponding to principal angles $\\theta_{1}=\\cdot\\cdot\\cdot=\\theta_{k}=0$ . Furthermore, for every $i=1,\\ldots,k$ , we know that $\\operatorname{Proj}_{V_{y}^{\\perp}}u_{i}=0$ , because $u_{i}\\in V_{y}$ . ", "page_idx": 13}, {"type": "text", "text": "Case 1: Assume $d_{x}\\leq d_{y}$ . In this case, there are $d_{x}-k$ nonzero principal angles corresponding to principal vectors we denote by $u_{k+1},\\ldots,u_{d_{x}}\\;\\in\\;V_{x}$ and $v_{k+1},\\ldots,v_{d_{x}}\\;\\in\\;V_{y}$ . For each $i\\,=$ $k+1,\\dotsc,d_{x}$ , we know that $\\operatorname{Proj}_{V_{y}^{\\perp}}u_{i}\\neq0$ , because $u_{i}\\notin V_{y}$ . In fact, because the $u_{i}$ are orthogonal, $\\operatorname{Proj}_{V_{y}^{\\perp}}u_{k+1},\\ldots,\\operatorname{Proj}_{V_{y}^{\\perp}}u_{d_{x}}$ are a basis for $\\mathrm{Proj}_{V_{y}^{\\perp}}\\,V_{x}$ . Therefore, $\\dim({\\mathrm{Proj}}_{V_{y}^{\\bot}}\\,V_{x})\\,=\\,\\#\\{i\\,:$ $\\theta_{i}(V_{x},V_{y})>0\\}$ . ", "page_idx": 13}, {"type": "text", "text": "Case 2: Assume $d_{x}>d_{y}$ . In this case, there are $d_{y}-k$ nonzero principal angles. Similar to the above argument, we can extend the basis for $V_{x}$ starting with the principal vectors $u_{k+1},\\ldots,u_{d_{y}}\\,\\in\\,V_{x}$ corresponding to the principal angles. However, because there are only $d_{y}$ principal angles, we denote by $u_{d_{y}+1},\\dotsc,u_{d_{x}}$ the remaining vectors necessary to specify an orthogonal basis for $V_{x}$ . Because the principal angle maximizes $u_{i}^{\\top}v_{i}$ , the remaining vectors $u_{d_{y}+1},\\dotsc,u_{d_{x}}$ are all orthogonal to $V_{y}$ . Therefore, $\\mathrm{Proj}_{V_{y}^{\\perp}}\\,u_{k+1},\\ldots,\\mathrm{Proj}_{V_{y}^{\\perp}}\\,u_{d_{y}},u_{d_{y}+1},\\ldots,u_{d_{x}}$ , is a basis for $\\mathrm{Proj}_{V_{y}^{\\perp}}\\,V_{x}$ . It follows that $\\dim(\\operatorname{Proj}_{V_{y}^{\\perp}}V_{x})=\\#\\{i:\\theta_{i}(V_{x},V_{y})>0\\}+d_{x}-d_{y},$ as desired. ", "page_idx": 13}, {"type": "text", "text": "C.2 Proof to theorem 1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Theorem 1. Assume that the second moments $\\Sigma_{x}$ and $\\Sigma_{y}$ are finite for both groups, and that the top $d_{x}+1$ eigenvalues of $\\Sigma_{x}$ and top $d_{y}+1$ eigenvalues of $\\Sigma_{y}$ are distinct. Then, our proposed estimator d xy is consistent: d xy\u2212nn\u2212x\u2212\u2192\u2192\u2212\u221e\u221e\u2192 dxy. ", "page_idx": 13}, {"type": "text", "text": "Proof. We know by Anderson (1963) that the sample matrix of eigenvectors $\\widehat{U}_{x}$ converges to the true matrix of eigenvectors $U_{x}$ in probability as $n_{x}\\to\\infty$ , and similarly, $\\widehat{U}_{y}\\to U_{y}$ as $n_{y}\\to\\infty$ . Let $m=\\operatorname*{min}\\left(d_{x},\\;\\bar{d}_{y}\\right)$ , and consider the following functions. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{1}:\\mathbb{R}^{p\\times d_{x}}\\times\\mathbb{R}^{p\\times d_{y}}\\rightarrow\\mathbb{R}^{d_{x}\\times d_{y}}}\\\\ &{\\quad\\left(U_{x},U_{y}\\right)\\mapsto U_{x}^{\\top}U_{y}}\\\\ &{f_{2}:\\mathbb{R}^{d_{x}\\times d_{y}}\\rightarrow\\mathbb{R}^{m}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "It is clear that $f_{1}$ is continuous. Due to Weyl\u2019s inequality for singular values, $f_{2}$ is continuous with respect to the spectral norm. The contrastive dimension is defined to be $d_{x y}=\\#\\{\\lambda_{j}<1\\}$ , while its estimator is defined by $\\widehat{d}_{x y}=\\#\\{\\widehat{\\lambda}_{j}<1-\\epsilon\\}$ . Therefore, by the continuous mapping theorem, it holds that $\\widehat{\\lambda}\\rightarrow\\lambda$ in probability. Taking $\\epsilon\\to0$ gives $\\widehat{d}_{x y}\\rightarrow d_{x y}$ , as desired. \u53e3 ", "page_idx": 13}, {"type": "text", "text": "C.3 Proof to theorem 2 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Theorem 2. In addition to all assumptions in theorem $^{\\,l}$ , we assume $x$ and $y$ in model $^{\\,l}$ are sub-Gaussian. More precisely, assume that there exist constants $K_{1},K_{2}\\geq1$ such that ", "page_idx": 13}, {"type": "text", "text": "where $\\lVert\\cdot\\rVert_{\\psi_{2}}$ is defined as in Vershynin (2018). Then, for any $u>0$ , with probability at least $1-2e^{-u}$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{j=1,\\ldots,m}\\left|\\widehat{\\lambda}_{j}-\\lambda_{j}\\right|\\leq C\\sum_{k\\in\\{x,y\\}}\\sqrt{d_{k}}\\delta_{k}^{-1}K_{k}^{2}\\left(\\sqrt{\\frac{p+u}{n_{k}}}+\\frac{p+u}{n_{k}}\\right)\\|\\Sigma_{k}\\|=O(n_{x}^{-1/2}+n_{y}^{-1/2})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $C$ is an absolute, positive constant, $\\delta$ is the minimum eigengap among the top $d\\!+\\!1$ eigenvalues of $\\Sigma$ , and $\\lVert\\cdot\\rVert$ denotes spectral norm. ", "page_idx": 14}, {"type": "text", "text": "Proof. Let $W=U^{\\top}V$ , where $U$ is the matrix of the top $d_{x}$ eigenvectors of $\\Sigma_{x}$ and $V$ is the matrix of the top $d_{y}$ eigenvectors of $\\Sigma_{y}$ . We use hats to represent sample estimates of these matrices and let $\\widehat{W}=\\widehat{U}^{\\intercal}\\widehat{V}$ . We would like a finite-sample error upper bound on $\\begin{array}{r}{\\operatorname*{max}_{j=1,\\dots,m}\\;\\Big|\\sigma_{j}\\left(\\widehat{W}\\right)-\\sigma_{j}\\left(W\\right)\\Big|.}\\end{array}$ . ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\left\\|\\hat{U}^{\\top}\\hat{V}-\\hat{U}^{\\top}V+\\hat{U}^{\\top}V-U^{\\top}V\\right\\|}\\\\ &{\\leq\\left\\|\\hat{U}^{\\top}\\hat{V}-\\hat{U}^{\\top}V\\right\\|+\\left\\|\\hat{U}^{\\top}V-U^{\\top}V\\right\\|}\\\\ &{=\\left\\|\\hat{U}\\right\\|\\cdot\\left\\|\\hat{V}-V\\right\\|+\\left\\|\\hat{U}-U\\right\\|\\cdot\\|V\\|}\\\\ &{=\\left\\|\\hat{V}-V\\right\\|+\\left\\|\\hat{U}-U\\right\\|}\\\\ &{\\leq\\left\\|\\hat{V}-V\\right\\|_{F}+\\left\\|\\hat{U}-U\\right\\|_{F}}\\\\ &{\\leq2^{3/2}\\left(\\sqrt{d_{x}}\\frac{\\left\\|\\widehat{\\Sigma}_{n_{x}}-\\Sigma_{x}\\right\\|}{\\delta_{x}}+\\sqrt{d_{y}}\\frac{\\left\\|\\widehat{\\Sigma}_{n_{y}}-\\Sigma_{y}\\right\\|}{\\delta_{y}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The final inequality holds due to the Davis-Kahan theorem, as presented in Vershynin (2018). Note that here $\\delta_{x}$ is the smallest eigengap of $\\Sigma_{x}$ , and $\\delta_{y}$ is the smallest eigengap of $\\Sigma_{y}$ . ", "page_idx": 14}, {"type": "text", "text": "For $k\\in\\{x,y\\}$ , with probability at least $1-2e^{-u}$ , we have that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left\\|\\widehat{\\sum}_{n_{x}}-\\Sigma_{x}\\right\\|\\leq C K_{k}^{2}\\left(\\sqrt{\\frac{p+u}{n_{k}}}+\\frac{p+u}{n_{k}}\\right)\\|\\Sigma_{k}\\|\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $C$ is an absolute, positive constant, and $K_{x}\\,=\\,\\|x\\|_{\\psi_{2}}\\,,K_{y}\\,=\\,\\|y\\|_{\\psi_{2}}$ are the sub-Gaussian constants, as in Vershynin (2018). Substituting these upper bounds on $\\left\\|\\widehat{\\Sigma}_{n_{k}}-\\Sigma_{k}\\right\\|$ yield the bound claimed. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "D Additional experimental details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The code for all experiments is made available at the anonymous Github link provided in appendix B. To allow for reproducibility, we set a seed for every experiment in which we perform the bootstrap test. Sometimes we set the seed to be 1; other times we set the seed to be 42. There was no particular reason behind the choice for any seed. In every experiment requiring intrinsic dimension estimation, we used the method of moments estimator from the sci-kit dimension package. Other estimators are possible and may give slightly different results, but we chose this estimator because we found it to give the most stable and reasonable results. In our experience, we found some of the other estimators to give estimates as low as 0 and as high as the number of features, of which neither estimate is sensible in the contexts we considered. ", "page_idx": 14}, {"type": "text", "text": "D.1 Simulations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For simulations 1 and 2, $S_{x}$ and $S_{y}$ were randomly chosen to be orthogonal matrices in $\\mathbb{R}^{p\\times d_{x}}$ and $\\mathbb{R}^{p\\times d_{y}}$ , respectively. Therefore, the covariance matrices in this simulation are given by $\\Sigma_{x}=$ ", "page_idx": 14}, {"type": "text", "text": "$S_{x}S_{x}^{\\top}+\\sigma_{x}^{2}I$ and $\\Sigma_{y}=S_{y}S_{y}^{\\top}+\\sigma_{y}^{2}I$ , respectively. Notably, these covariance matrices violate the assumption of distinct eigenvalues presented in section 4. However, the space spanned by the top $d_{x}$ eigenvectors of $\\Sigma_{x}$ is still identifiable, because the $d_{x}$ th eigenvalue is $1\\+\\sigma_{x}^{2}$ , while the $(d_{x}+1)\\mathrm{th}$ eigenvalue is $\\sigma_{x}^{2}$ (similarly for $\\Sigma_{y}$ ). As discussed in section 7, this is the necessary identifiability condition. ", "page_idx": 15}, {"type": "text", "text": "We designed simulations 3 and 4 to allow for an interpretable ground truth of $d_{x y}\\,=\\,1\\$ while mimicking real image data. Figure 2 illustrates the data-generating process for both groups in both simulations. Every image is represented by 28 by 28 matrix in the grayscale space, where 0 represents black, and 255 represents white. Each of these image matrices was converted to a vector in $\\dot{\\mathbb{R}}^{784}$ . In both simulations, the foreground group was created by drawing a white line on top; each pixel in this line was set to 255, while the background group was created by averaging the original (disk or MNIST 0) image entrywise with an image of grass from the corrupted MNIST dataset. The was no specific intent behind choosing to draw a horizontal line in the sixth row of each image; drawing a line of random length going in any single direction from any starting pixel could have worked. However, we chose to fix the starting pixel because we wanted to be sure not to introduce a second parameter. That said, simulations in which other parameters are introduced, leaving a different ground truth $d_{x y}$ are possible. For example, creating the foreground group by drawing a white rectangle of random width and height $(w,h)\\in\\{1,\\dots,24\\}^{2}$ beginning at the top-left pixel would lead to a scenario in which $d_{x y}=2$ . ", "page_idx": 15}, {"type": "text", "text": "D.2 Real data experiments ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The Mouse Protein, mHealth, BMMC, Small Molecule, ECCITE-Seq, Pathogen, Perturb-Seq, and CelebA experiments all required additional pre-processing. To ensure that our pre-processing aligned with previous analyses, we used the code available for pre-processing these datasets associated with the papers analyzing them. To be specific, we pre-processed the Mouse Protein and mHealth datasets using the code associated with Abid et al. (2018). We pre-processed the Small Molecule, ECCITE-Seq, Pathogen, and Perturb-Seq datasets using the code associated with Weinberger et al. (2023). We downloaded the BMMC dataset using instructions and code from Weinberger et al. (2023) and pre-processed it using the code from Li et al. (2020). Code flies for replicating the pre-processing we did for each experiment are available on the anonymous Github in appendix B. To pre-process the CelebA dataset, we first used the PIL package in Python to resize the images from 178 by 218 pixels to 64 by 64 pixels. From there, we performed PCA (Anderson, 1963) on the resized images and took the top 1000 directions. ", "page_idx": 15}, {"type": "text", "text": "In section 6 we made reference to our results standing in agreement with results from previous analyses. In particular, we claimed that for the BMMC dataset, our observation of $\\hat{d}_{x y}\\ge4$ was consistent with the analysis in Li et al. (2020), in which a figure was published illustrating a distinction between the pre- and posttransplant groups in the BMMC data using the third and fourth PCPCA directions. We duplicated that graph in the left panel of figure 3 for transparency. We also claimed that for the mHealth dataset, our observation of $\\widehat{d}_{x y}=1$ was consistent with the analysis in Abid et al. (2018), in which a figure was published illustrating a single direction differentiating the two groups of squatting and cycling. We duplicated this illustration in the right panel of figure 3 here for convenience. ", "page_idx": 15}, {"type": "text", "text": "For clarity on the image datasets used in section 6, we provide sample images from the foreground and background groups of both the Corrupted MNIST and CelebA datasets. The images for the Corrupted MNIST application are in figure 4; the top row has examples of images in the foreground group, while the bottom row has examples of images in the background group. The images for the CelebA application are in figure 5; the top-left two are foreground (glasses), while the bottom-left two are background (glasses). Similarly, the top-right two are foreground (hat), while the bottom-right two are background (hat). ", "page_idx": 15}, {"type": "text", "text": "D.3 Practical Implications for Downstream Tasks ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "To explore how the methods we presented in this paper can be used effectively in downstream data analysis tasks, we conducted further analysis with the corrupted MNIST dataset used in section 6. We performed CPCA with dimensions $d\\,=\\,1,2,3,\\ldots,10$ , then ran logistic regression with the outcome (response) variable being the digit (0 vs. 1) imposed on the grass images. The input was the contrastive principal components (cPCs) based on CPCA with different $d$ . The results are as follows: ", "page_idx": 15}, {"type": "image", "img_path": "IgU8gMKy4D/tmp/9ee1f2358ecc7b203cc9f29f4f68a57c0d83329459bbd98bc11223b9e8925fb3.jpg", "img_caption": ["Figure 3: Left, PCPCA for BMMC Data. Right, cPCA for mHealth Data. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "IgU8gMKy4D/tmp/a3e0ce2c4f519fbadb0d9f7c706d6797803a590487cb1fff18100fe23e6259c4.jpg", "img_caption": ["Figure 4: Corrupted MNIST Sample Images. Top row, foreground. Bottom row, background. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "table", "img_path": "IgU8gMKy4D/tmp/ec4e9990e1fceba98366397407a8959b17758c94bd964255e8276bb018887ff3.jpg", "table_caption": ["Table 4: Summary of simulation results "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "The accuracy increases with the number of cPCs and then plateaus at 5, where the accuracy is 0.945, after which the accuracy increases only slightly. This provides indirect evidence that if one can simply run contrastive dimension reduction methods with the dimension equal to our estimator (5 in this case). One may argue, however, that $d\\,=\\,2$ is optimal, resembling an elbow point. Our principal angles (measured by singular values) align with this observation. As shown in Table 2, the smallest four singular values are 0.095, 0.315, 0.705, and 0.846. The two smallest singular values suggest that there are two more prominent contrastive dimensions, leading to satisfactory classification performance for $d=2$ . While our suggested cutoff of 0.9 gives $\\hat{d}_{x y}=5$ , a cutoff of 0.7 gives $\\hat{d}_{x y}=2$ . This cutoff is subjective and can be used in conjunction with classification accuracy, particularly if a response variable is available, to improve decision making. ", "page_idx": 16}, {"type": "image", "img_path": "IgU8gMKy4D/tmp/8ebeadf0978e5df9dec2862061ed21e91b663f813789d0fd37c76a93ec9d4086.jpg", "img_caption": ["Figure 5: CelebA Sample Images. Top-left, foreground (glasses). Top-right, foreground (hat). "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "1. Claims ", "page_idx": 17}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: The abstract and introduction accurately describe the contribution of our methods. We clearly identify the scope of the paper in the abstract and introduction and address the concerns in the rest of the paper. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: In section 7, we detail the limitations of the hypothesis test we present, and we suggest an alternative for a more powerful test. We also discuss the need for further investigation into the robustness of CDE to misspecification of intrinsic dimension estimates. Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper. \u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. ", "page_idx": 17}, {"type": "text", "text": "\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: We have two theoretical claims about our estimator, and we state the assumptions and provide complete and correct proofs for them in appendix C. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We provide all code and information necessary to access and pre-process the data in the anonymous Github repository at the link in appendix B. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same ", "page_idx": 18}, {"type": "text", "text": "dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. ", "page_idx": 19}, {"type": "text", "text": "\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: All code and instructions to obtain and pre-process datasets are in the anonymous Github repository, for which we provide the link in appendix B. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We specify all parameters for replicating our experiments, including choice of intrinsic dimension estimator, number of bootstrap samples, cutoff (tolerance), and variance. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We report the $p$ -value result from the hypothesis test we developed for each simulation and experiment. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Yes, in appendix B we include approximate time and memory required to replicate all simulations and experiments, including a description of the machine on which the experiments were run. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 20}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS Code of Ethics and conform to it in every respect, including, but not limited to, data-related concerns, societal impact, impact mitigation, model licensing, and reproducibility. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [No] ", "page_idx": 21}, {"type": "text", "text": "Justification: Our methods mostly concern the applicability of other dimension reduction methods designed to analyze case-control data. While it is possible that our method might have a societal impact, we choose to focus more on method development than downstream effects difficult to observe at this stage in the work. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 21}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: Our method simply provides a $p$ -value and an estimate for a parameter to be used in other dimension reduction methods. Because the output consists of merely a handful of numbers to be used as a guideline for future analysis, it poses extremely low risk for misuse. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We properly cite and credit all licenses to all code, data, and packages in appendix A. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: All of our code is available at the anonymous Github. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 22}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our research does not involve human subjects. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our research does not involve human subjects. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}]