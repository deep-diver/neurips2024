[{"figure_path": "oyl2Fnzune/figures/figures_1_1.jpg", "caption": "Figure 1: Three hypotheses and corresponding architectural implementations for multi-task learning in MLLMs. (a) Synergy hypothesis. (b)-(c) Conflict hypothesis in LLM and connector, respectively. (d)-(e) Conflict-synergy coexist hypothesis in LLM and connector, respectively.", "description": "This figure illustrates three different hypotheses regarding the interaction between multiple tasks in a multi-modal large language model (MLLM) architecture.  The first hypothesis suggests synergy, meaning all tasks benefit from shared components.  Hypotheses two and three explore different conflict scenarios: one where conflict occurs within the language model (LLM) itself and another where the conflict happens at the connector between the visual encoder and the LLM. The final hypothesis suggests that both synergy and conflict can coexist in an MLLM.  Each hypothesis is visually represented by a different architectural diagram showing the interaction between visual encoders, connectors (which bridge the gap between visual and language data), and the LLM.", "section": "To mitigate the tug-of-war problem in multi-task learning, recent advances introduce the well-known Mixture-of-Experts (MoE) into MLLMs."}, {"figure_path": "oyl2Fnzune/figures/figures_3_1.jpg", "caption": "Figure 2: Dataset-level multi-task interference of the synergy hypothesis model at the connector in MLLMs. (a) Perspective of gradient direction GD. (b) Perspective of gradient magnitude GM.", "description": "This figure visualizes the multi-task interference at the connector level within the synergy hypothesis model of Multi-modal Large Language Models (MLLMs).  It uses two perspectives to quantify the interference: (a) Perspective of gradient direction (GD), showing the alignment or conflict in update directions for different tasks; (b) Perspective of gradient magnitude (GM), illustrating the similarity or dissimilarity of gradient magnitudes across different tasks.  The heatmaps show the correlation between tasks, highlighting where significant interference or synergy occurs.", "section": "3.1 Multi-task interference"}, {"figure_path": "oyl2Fnzune/figures/figures_4_1.jpg", "caption": "Figure 3: Overall architecture of Uni-Med, which consists of a universal vision feature extraction module, a connector-MoE module and an LLM. Uni-Med can perform six different medical tasks including question answering, visual question answering, report generation, referring expression comprehension, referring expression generation and image classification.", "description": "The figure shows the overall architecture of the Uni-Med model, which is composed of three main modules: a universal vision feature extraction module, a connector-MoE (CMoE) module, and a large language model (LLM). The vision feature extraction module takes multi-modal medical images as input and extracts visual features. The CMoE module aligns the visual space with the language embedding space of the LLM, mitigating the tug-of-war problem in multi-task learning.  The LLM then generates responses based on the aligned visual features and textual instructions.  Uni-Med is designed to perform six different medical tasks, as indicated in the figure.", "section": "3.2 Model Architecture"}, {"figure_path": "oyl2Fnzune/figures/figures_8_1.jpg", "caption": "Figure 2: Dataset-level multi-task interference of the synergy hypothesis model at the connector in MLLMs. (a) Perspective of gradient direction GD. (b) Perspective of gradient magnitude GM.", "description": "This figure visualizes the multi-task interference at the connector level within the synergy hypothesis model of Multi-modal Large Language Models (MLLMs).  It uses two perspectives to quantify the interference: (a) Perspective of gradient direction (GD), showing the divergence in update directions across tasks; and (b) Perspective of gradient magnitude (GM), illustrating the similarity in gradient magnitudes across tasks.  The tug-of-war phenomenon, where tasks negatively impact each other's optimization, is highlighted by the interference visualized in the heatmaps, offering insights into the multi-task learning dynamics within the model's architecture.", "section": "3.1 Multi-task interference"}, {"figure_path": "oyl2Fnzune/figures/figures_8_2.jpg", "caption": "Figure 5: Visual features distribution maps-3D. (a) fag distribution, (b) falign distribution obtained through MLP, (c) falign distribution obtained through CMoE.", "description": "This figure visualizes the distribution of visual features in a 3D space for different tasks before and after passing through the connector (MLP and CMoE). It shows how the CMoE module improves the separation of visual features for different tasks, enhancing the model's ability to distinguish between tasks during multi-task learning.", "section": "4.3 Interpretation"}, {"figure_path": "oyl2Fnzune/figures/figures_17_1.jpg", "caption": "Figure 3: Overall architecture of Uni-Med, which consists of a universal vision feature extraction module, a connector-MoE module and an LLM. Uni-Med can perform six different medical tasks including question answering, visual question answering, report generation, referring expression comprehension, referring expression generation and image classification.", "description": "The figure shows the overall architecture of the Uni-Med model, which is composed of three main modules: a universal vision feature extraction module, a connector-MoE (CMoE) module, and a large language model (LLM). The vision feature extraction module processes the input medical images, and the CMoE module acts as an efficient connector between the visual and language modalities, enabling efficient multi-task learning. The LLM module generates the final responses based on the processed visual and text information. The Uni-Med model can perform a wide range of medical tasks, including question answering, visual question answering, report generation, referring expression comprehension, referring expression generation, and image classification. ", "section": "3.2 Model Architecture"}, {"figure_path": "oyl2Fnzune/figures/figures_21_1.jpg", "caption": "Figure 3: Overall architecture of Uni-Med, which consists of a universal vision feature extraction module, a connector-MoE module and an LLM. Uni-Med can perform six different medical tasks including question answering, visual question answering, report generation, referring expression comprehension, referring expression generation and image classification.", "description": "The figure shows the architecture of Uni-Med, a unified medical generalist foundation model.  It consists of three main modules: a universal vision feature extraction module, a connector-MoE (mixture-of-experts) module, and a large language model (LLM). The CMoE module is designed to efficiently handle multiple tasks and modalities, improving performance over a standard architecture.  The model takes in both vision data (medical images) and text data (instructions) and outputs a response, performing six different medical tasks.", "section": "3.2 Model Architecture"}, {"figure_path": "oyl2Fnzune/figures/figures_22_1.jpg", "caption": "Figure 3: Overall architecture of Uni-Med, which consists of a universal vision feature extraction module, a connector-MoE module and an LLM. Uni-Med can perform six different medical tasks including question answering, visual question answering, report generation, referring expression comprehension, referring expression generation and image classification.", "description": "The figure shows the overall architecture of Uni-Med, a unified medical generalist foundation model.  It's composed of three main modules: a universal vision feature extraction module (processing medical images from various modalities), a connector-MoE module (acting as a bridge between the vision and language modules, and mitigating the \"tug-of-war\" problem in multi-task learning), and a large language model (LLM, for generating responses to various medical tasks).  The architecture is designed to handle six different medical tasks.", "section": "3.2 Model Architecture"}, {"figure_path": "oyl2Fnzune/figures/figures_23_1.jpg", "caption": "Figure 3: Overall architecture of Uni-Med, which consists of a universal vision feature extraction module, a connector-MoE module and an LLM. Uni-Med can perform six different medical tasks including question answering, visual question answering, report generation, referring expression comprehension, referring expression generation and image classification.", "description": "This figure shows the overall architecture of the Uni-Med model, which is composed of three main modules: a universal vision feature extraction module, a connector-MoE (CMoE) module, and a large language model (LLM).  The visual feature extraction module processes medical images from various modalities (X-ray, CT, MRI, fundus, PET, ultrasound, endoscopy, microscopy, dermoscopy, and histopathology). The CMoE module acts as a connector between the visual and textual modalities, efficiently handling the multi-task learning aspect. Finally, the LLM generates the model's response.  The model is designed to perform six medical tasks.", "section": "3.2 Model Architecture"}]