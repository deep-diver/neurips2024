[{"figure_path": "lEUle8S4xQ/figures/figures_1_1.jpg", "caption": "Figure 1: An Overview of the S2FT Family for LLMs: First, we perform sparse selection of specific attention heads and channels within the coupled structures of the MHA and FFN modules. Next, we apply co-permutation to the weight matrices on both sides of these structures, enabling dense gradient computation only for the selected components. While we demonstrate S2FT by selecting the same heads/channels on both sides for clarity, our approach also supports asymmetric selection strategies.", "description": "This figure illustrates the two main steps of the S2FT (Structured Sparse Fine-Tuning) method.  Step 1 shows the sparse selection of attention heads and FFN channels within the Transformer blocks.  Specific heads and channels are chosen, while others are frozen.  The weight matrices are then co-permuted to create dense submatrices for efficient computation. Step 2 highlights how dense computations are performed only on these selected components after the co-permutation, which improves efficiency and scalability. The figure emphasizes the concept of 'selecting sparsely and computing densely' which is central to the S2FT approach.", "section": "3 The S2FT family of methods"}, {"figure_path": "lEUle8S4xQ/figures/figures_2_1.jpg", "caption": "Figure 2: Accuracy comparison of SpFT, LORA and Full FT at varying ratios of trainable parameters in various settings. SpFT exhibits strong generalization ability while full FT excels in memorization.", "description": "This figure compares the performance of three different fine-tuning methods (SpFT, LoRA, and Full FT) on various mathematical reasoning tasks, using varying ratios of trainable parameters.  It shows training loss, accuracy on near out-of-distribution (OOD) easy and hard tasks, and far OOD accuracy. The results demonstrate that SpFT excels at generalization, achieving lower training loss and higher far OOD accuracy compared to LoRA and Full FT. Full FT excels at memorization, but this comes at the cost of reduced generalization. The results support the hypothesis that SpFT strikes a better balance between memorization and generalization.", "section": "Memorization or Generalization?"}, {"figure_path": "lEUle8S4xQ/figures/figures_3_1.jpg", "caption": "Figure 3: Grouped model weights with basic structure and residual structure. All highlighted weights must be permuted simultaneously. Residual structures require additional permutation during runtime.", "description": "This figure illustrates the concept of \"coupled structures\" within LLMs (Large Language Models). It shows how model weights (W1 and W2) are interconnected through intermediate activations (represented by circles). The left side demonstrates a basic structure, while the right side shows a residual structure.  The highlighted weights represent those that need to be permuted simultaneously during the S2FT (Structured Sparse Fine-Tuning) process. The permutation preserves the original output of the structure while strategically selecting a sparse subset for training, enabling dense computation only for selected components. This is crucial for improving efficiency in fine-tuning LLMs, as only a fraction of parameters needs to be trained.", "section": "3 The S2FT family of methods"}, {"figure_path": "lEUle8S4xQ/figures/figures_7_1.jpg", "caption": "Figure 4: The impact of different components in fine-tuning, including Query, Key, Value, Output, Up, Gate, and Down projection. We fix the trainable parameter budget and only fine-tune one component.", "description": "This figure shows the impact of fine-tuning different components of a transformer block on the performance of commonsense reasoning tasks.  The components tested are Query, Key, Value, Output, Up, Gate, and Down projections.  Each bar represents the average accuracy on eight different commonsense reasoning datasets when only one of these components is fine-tuned while holding the rest constant. The number of trainable parameters was kept constant across the experiments to ensure a fair comparison. The results indicate varying effectiveness depending on the components that are fine-tuned. ", "section": "3.1 Discover Coupled Structures in LLMs"}, {"figure_path": "lEUle8S4xQ/figures/figures_8_1.jpg", "caption": "Figure 5: Comparison of memory and computation efficiency during training on the LLaMA2-7B/13B with varying sequence lengths and batch sizes. Average latency and peak memory usage are reported. S2FT significantly improves training latency while reducing memory footprint compared to baselines.", "description": "This figure compares the training efficiency (memory usage and latency) of S2FT with other fine-tuning methods (LoRA, DORA, LISA, LoReFT, Galore, Full FT) across different model sizes (LLaMA2-7B, LLaMA2-13B), sequence lengths (512, 1024), and batch sizes (1, 2).  S2FT shows significant improvements in both memory usage and training speed compared to the other methods.", "section": "6 Analysis"}, {"figure_path": "lEUle8S4xQ/figures/figures_9_1.jpg", "caption": "Figure 6: Comparison of latency for adapter switch and parallelism on a single linear layer. S2FT improves scalability for switch on GPU and CPU, while saving 22% time during parallelism on GPU.", "description": "This figure compares the latency of adapter switching and parallel processing for S2FT and LoRA on a single linear layer.  It shows three graphs: (a) shows the switch time on GPU, (b) shows the switch time on CPU, and (c) shows the parallelism time on GPU.  In all cases, S2FT demonstrates improved scalability and efficiency, particularly in terms of parallelism on the GPU, where it achieves a 22% reduction in latency compared to LoRA.", "section": "6 Analysis"}]