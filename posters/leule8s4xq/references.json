{"references": [{"fullname_first_author": "Edward J Hu", "paper_title": "Lora: Low-rank adaptation of large language models", "publication_date": "2021-06-09", "reason": "This paper introduces the Lora method, a foundational parameter-efficient fine-tuning technique that S2FT builds upon and improves."}, {"fullname_first_author": "Josh Achiam", "paper_title": "Gpt-4 technical report", "publication_date": "2023-03-08", "reason": "GPT-4 is a prominent large language model (LLM) used as a baseline and compared against in the S2FT experiments, thus impacting the significance of the results."}, {"fullname_first_author": "Zhiqiang Hu", "paper_title": "Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models", "publication_date": "2023-04-01", "reason": "This paper provides a benchmark for parameter-efficient fine-tuning methods used to compare against S2FT\u2019s performance."}, {"fullname_first_author": "Rui Pan", "paper_title": "LISA: Layerwise importance sampling for memory-efficient large language model fine-tuning", "publication_date": "2024-03-17", "reason": "LISA is a state-of-the-art PEFT method used for comparison with S2FT, highlighting S2FT\u2019s improvements over existing techniques."}, {"fullname_first_author": "Shih-Yang Liu", "paper_title": "DoRA: Weight-Decomposed low-rank adaptation", "publication_date": "2024-02-09", "reason": "DORA is another significant parameter-efficient fine-tuning method compared against S2FT in the paper, showcasing S2FT\u2019s advantages."}]}