[{"type": "text", "text": "Dinomaly: The Less Is More Philosophy in Multi-Class Unsupervised Anomaly Detection ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Recent studies highlighted a practical setting of unsupervised anomaly detection   \n2 (UAD) that builds a unified model for multi-class images, serving as an alternative   \n3 to the conventional one-class-one-model setup. Despite various advancements   \n4 addressing this challenging task, the detection performance under the multi-class   \n5 setting still lags far behind state-of-the-art class-separated models. Our research   \n6 aims to bridge this substantial performance gap. In this paper, we introduce a   \n7 minimalistic reconstruction-based anomaly detection framework, namely Dino  \n8 maly, which leverages pure Transformer architectures without relying on complex   \n9 designs, additional modules, or specialized tricks. Given this powerful frame  \n10 work consisted of only Attentions and MLPs, we found four simple components   \n1 that are essential to multi-class anomaly detection: (1) Foundation Transformers   \n12 that extracts universal and discriminative features, (2) Noisy Bottleneck where   \n13 pre-existing Dropouts do all the noise injection tricks, (3) Linear Attention that nat  \n14 urally cannot focus, and (4) Loose Reconstruction that does not force layer-to-layer   \n15 and point-by-point reconstruction. Extensive experiments are conducted across   \n16 three popular anomaly detection benchmarks including MVTec-AD, VisA, and the   \n17 recently released Real-IAD. Our proposed Dinomaly achieves impressive image   \n18 AUROC of $99.6\\%$ , $98.7\\%$ , and $89.3\\%$ on the three datasets respectively, which is   \n19 not only superior to state-of-the-art multi-class UAD methods, but also surpasses   \n20 the most advanced class-separated UAD records. ", "page_idx": 0}, {"type": "text", "text": "21 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "22 Unsupervised anomaly detection (UAD) aims to detect abnormal patterns from normal images and   \n23 further localize the anomalous regions. Because of the diversity of potential anomalies and their   \n24 scarcity, this task is proposed to model the accessible training sets containing only normal samples as   \n25 an unsupervised paradigm. UAD has a wide range of applications, e.g., industrial defect detection,   \n26 medical disease screening, and video surveillance, addressing the difficulty of collecting and labeling   \n27 all possible anomalies in these scenarios.   \n28 Efforts on UAD attempt to learn the distribution of available normal samples. Most advanced methods   \n29 utilize networks pre-trained on large-scale datasets, e.g. ImageNet [1], for extracting discriminative   \n30 and informative feature representations. Specifically, Feature reconstruction [2; 3; 4] and feature   \n31 distillation methods [5; 6] are proposed to reconstruct features of pre-trained encoders, based on   \n32 the hypothesis that the networks trained on normal images can only construct normal regions, but   \n33 fail for unseen anomalous regions. Feature statistics methods [7; 8; 9] memorize and model all   \n34 anomaly-free features extracted from pre-trained networks in training, and compare them with the   \n35 test features during inference. Pseudo-anomaly methods [10; 11] generate pseudo defects or noises ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "image", "img_path": "2vhkjOdlc8/tmp/54fecca3490a3552d8f352b30715ea64189797a34a0405b7cba24211a78543d8.jpg", "img_caption": ["(c) Image-Level Detection AUROC "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Setting and Performance of UAD and multi-class UAD (MUAD). (a) Task setting of class-separated UAD. (b) Task setting of MUAD. (c) Comparison of Dinomaly and previous SoTA methods [13; 14; 15; 16; 8; 17; 18; 19] on MVTec-AD [20], VisA [21], and Real-IAD [22]. ", "page_idx": 1}, {"type": "text", "text": "36 on normal images or features to imitate anomalies, converting UAD to supervised classification [11]   \n37 or segmentation tasks [10; 12].   \n38 Conventional works on UAD build a separate model for each object category, as shown in Figure 1(a).   \n39 However, this one-class-one-model setting entails substantial storage overhead for saving models   \n40 [3], especially when the application scenario necessitates a large number of object classes. For   \n41 UAD methods, a compact boundary of normal patterns is vital to distinguish anomalies. Once the   \n42 intra-normal patterns become exceedingly complicated due to various classes, the corresponding   \n43 distribution becomes challenging to measure, consequently harming the detection performance.   \n44 Recently, UniAD [3] and successive studies have been proposed to train a unified model for multi  \n45 class anomaly detection (MUAD), as shown in Figure 1(b). Under this setting, the \"identity mapping\"   \n46 that directly copies the input as the output regardless of normal or anomaly harms the performance of   \n47 conventional methods [3]. This phenomenon is caused by the diversity of multi-class normal patterns   \n48 that drive the network to generalize on unseen patterns.   \n49 Within two years, a number of methods have been proposed to address MUAD, such as neighbor  \n50 masked attention [3], synthetic anomalies [23], feature jitter [3], vector quantization [24], diffusion   \n51 model [25; 26], and state space model (Mamba) [19]. However, there is still a non-negligible   \n52 performance gap between the state-of-the-art (SoTA) MUAD methods and class-separated UAD   \n53 methods, restricting the practicability of implementing unified models, as shown in Figure 1(c). In   \n54 addition, previous methods employ modules and architectures delicately designed, which may not be   \n55 straightforward, and consequently suffer from limited universality and usability.   \n56 In this work, we aim to catch up with the performance of class-separated anomaly detection models   \n57 using a multi-class unified model, namely Dinomaly. To begin with, we build a reconstruction-based   \n58 UAD framework that consists of only vanilla Transformer blocks [27], i.e. Self-Attentions and   \n59 Multi-Layer Perceptrons (MLPs). Within this framework, we propose four simple but essential   \n60 elements that boost Dinomaly to perform equal to or better than SoTA conventional class-separated   \n61 models. First, we show that self-supervised pre-trained Vision Transformers (ViT) [28], especially the   \n62 DINO family [29; 30], serve as powerful feature encoders to extract discriminative representations as   \n63 reconstruction objects. Second, as an alternative to carefully designed pseudo anomaly and feature   \n64 noise, we propose to activate the out-of-the-box Dropout in an MLP to prevent the network from   \n65 restoring both normal and anomalous patterns, which is previously referred to as identity mapping.   \n66 Third, we propose to utilize the \"side effect\" of Linear Attention (a computation-efficient counterpart   \n67 of Softmax Attention) that makes it hard to focus on local regions, to further alleviate the issue of   \n68 identity mapping. Fourth, previous methods adopt layer-to-layer and region-by-region reconstruction   \n69 schemes, distilling a decoder that can well mimic the behavior of the encoder even for anomalous   \n70 regions. Therefore, we propose to loosen the reconstruction constraints by grouping multiple layers   \n71 as a whole and discarding well-reconstructed regions during optimization.   \n72 To validate the effectiveness of the proposed Dinomaly under MUAD setting, we conduct extensive   \n73 experiments on three widely used industrial defect detection benchmarks, i.e., MVTec AD [20]   \n74 (15 classes), VisA [21] (12 classes), and recently released Real-IAD (30 classes). Notably, we   \n75 achieve unprecedented image-level AUROC of $99.6\\%$ , $98.7\\%$ , and $89.3\\%$ on MVTec AD, VisA, and   \n76 Real-IAD, respectively, which surpasses previous SoTA methods by a large margin. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "77 Related works are presented in Appendix A.1. ", "page_idx": 2}, {"type": "text", "text": "78 2 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "79 2.1 Dinomaly Framework ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "80 \u201cWhat I cannot create, I do not understand\u201d\u2014 Richer Feynman ", "page_idx": 2}, {"type": "text", "text": "81 The ability to recognize anomalies from what we know is an innate human capability, serving as a   \n82 vital pathway for us to explore the world. Similarly, we construct a reconstruction-based framework   \n83 that relies on the epistemic characteristic of artificial neural networks. Dinomaly consists of an   \n84 encoder, a bottleneck, and a reconstruction decoder, as shown in Figure 2. Without loss of generality,   \n85 a standard ViT-Base/14 network [28] with 12 Transformer layers is used as the encoder, extracting   \n86 informative feature maps with different semantic scales. The bottleneck is a simple MLP (a.k.a.   \n87 feed-forward network, FFN) that collects the feature representations of the encoder\u2019s 8 middle-level   \n88 layers. The decoder is similar to the encoder, consisting of 8 Transformer layers. During training,   \n89 the decoder learns to reconstruct the middle-level features of the encoder by maximizing the cosine   \n90 similarity between feature maps. During inference, the decoder is expected to reconstruct normal   \n91 regions of feature maps but fails for anomalous regions as it has never seen such samples.   \n92 Foundation Transformers. Foundation models, especially ViTs [28; 31] pre-trained on large-scale   \n93 datasets, serve as a basis and starting point for specific computer vision tasks. Such networks employ   \n94 self-supervised learning schemes such as contrastive learning (MoCov3 [32], DINO [29]), masked   \n95 image modeling (MAE [33], SimMIM [34], BEiT [35]), and their combination (iBOT [36], DINOv2   \n96 [30]), producing universal features suitable for image-level visual tasks (image classification, instance   \n97 retrieval) and pixel-level visual tasks (depth estimation, semantic segmentation).   \n98 Because of the lack of supervision in UAD, most advanced methods adopt pre-trained networks to   \n99 extract discriminative features. Recent works [37; 17; 38] have discovered the advantage of robust   \n100 and universal features of self-supervised models over domain-specific ImageNet features in anomaly   \n101 detection tasks. In this work, we further utilize the up-to-date Transformer foundation, i.e., DINOv2   \n102 with registers [39], as the encoder of Dinomaly. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "103 2.2 Noisy Bottleneck. ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "104 \u201cDropout is all you need.\u201d ", "page_idx": 2}, {"type": "text", "text": "105 Generalization ability is a merit of neural networks, allowing them to perform equally well on unseen   \n106 test sets. However, generalization is not so wanted in the context of unsupervised anomaly detection   \n107 that leverages the epistemic nature of neural networks. With the increasing diversity of images and   \n108 their patterns due to multi-class UAD settings, the decoder can generalize its reconstruction ability to   \n109 unseen anomalous samples, resulting in the failure of anomaly detection using reconstruction error.   \n110 This phenomenon is called \"identity mapping\" in previous works of literature [3; 23; 18].   \n111 A direct solution for identity mapping is to shift \"reconstruction\" to \"restoration\". Specifically, instead   \n112 of directly reconstructing the normal images or features given normal inputs, previous works propose   \n113 to add perturbations as pseudo anomalies on input images [10; 40; 12] or feature representations   \n114 [3; 25] during network forward propagation; meanwhile, still let the decoder restore anomaly-free   \n115 images or features, formulating a denoising-like framework. However, such methods employ heuristic   \n116 and hand-crafted anomaly generation strategies, that are not universal across domains, datasets, and   \n117 methods.   \n118 In this work, we propose to activate the pre-existing Dropout in an MLP layer. Dropout, a popular   \n119 network element introduced by Hinton et al. [41] in 2014 to prevent overftiting, flourished in nearly   \n120 all neural network architectures to the present day, including Transformers. In Dinomaly, Dropout is   \n121 used to discard neural activations in the MLP bottleneck randomly. Instead of alleviating overftiting,   \n122 the role of Dropout in Dinomaly can be explained as feature noise and pseudo feature anomaly.   \n123 Although the decoder takes noisy features during training, it is encouraged to restore clean features   \n124 from the encoder. Without introducing any novel modules, this paradigm forces the decoder to restore   \n125 normal features given a test image with anomalies, in turn, mitigating identical mapping. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "image", "img_path": "2vhkjOdlc8/tmp/f101d01beed2d73d0344f1e8c34809acf6514d714ee8501ae4e12610ab36830f.jpg", "img_caption": ["Figure 2: The framework of Dinomaly, built by pure Transformer building blocks. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "126 2.3 Unfocused Linear Attention. ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "127 \u201cOne man\u2019s poison is another man\u2019s meat\u201d ", "page_idx": 3}, {"type": "text", "text": "128 Softmax Attention is the key mechanism of Transformers, allowing the model to attend to different   \n129 parts of its input token sequence. Formally, given an input sequence $\\textbf{X}\\in\\,\\mathbb{R}^{N\\times d}$ with length   \n130 $N$ , Attention first transforms it into three matrices: the query matrix $\\mathbf{Q}\\in\\mathbb{R}^{N\\times d}$ , the key matrix   \n131 ${\\bf K}\\in\\mathbb{R}^{N\\times d}$ , and the value matrix $\\mathbf{V}\\in\\mathbb{R}^{N\\times d}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{Q}=\\mathbf{X}\\mathbf{W}^{Q}\\mathbf{\\Lambda},\\mathbf{K}=\\mathbf{X}\\mathbf{W}^{K}\\mathbf{\\Lambda},\\mathbf{V}=\\mathbf{X}\\mathbf{W}^{V}\\mathbf{\\Lambda},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "132 where $\\mathbf{W}^{Q}$ , $\\mathbf{W}^{K}$ , $\\mathbf{W}^{V}\\in\\mathbb{R}^{d\\times d}$ are learnable parameters. By computing the attention map by the   \n133 query-key similarity, the output of Attention is given as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{Attention}(\\mathbf{Q},\\mathbf{K},\\mathbf{V})=\\mathrm{Softmax}(\\mathbf{Q}\\mathbf{K}^{T})\\mathbf{V}\\ .\n$$", "text_format": "latex", "page_idx": 3}, {"type": "image", "img_path": "2vhkjOdlc8/tmp/a7688057949aadc973858cf75f848e3c936dff7322f7915d1ae2881e1b279225.jpg", "img_caption": ["Figure 3: The decoder attention map (min-max to 0-1 for visualization) of Dinomaly with vanilla Softmax Attention vs. Linear Attention. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "134 Because the attention map is obtained by computing the similarity between all query-key pairs   \n135 followed by row-wise Softmax, the computation complexity is ${\\mathcal O}(N^{\\natural}d)$ .   \n136 Linear Attention was proposed as a promising alternative to reduce the computation complexity of   \n137 vanilla Softmax Attention concerning the number of tokens [42]. By substituting Softmax operation   \n138 with a simple activation function $\\phi(\\cdot)$ (usually $\\phi(x)=\\mathrm{{elu}}(x)+1)$ , we can change the computation   \n139 order from $({\\bf Q}{\\bf K}^{T}){\\bf V}$ to $\\mathbf{Q}(\\mathbf{K}^{T}\\mathbf{V})$ . Formally, Linear Attention is given as: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{inearAttention}(\\mathbf{Q},\\mathbf{K},\\mathbf{V})=(\\phi(\\mathbf{Q})\\phi(\\mathbf{K}^{T}))\\mathbf{V}=\\phi(\\mathbf{Q})(\\phi(\\mathbf{K}^{T})\\mathbf{V})\\;,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "140 where the computation complexity is reduced to $O(N d^{2})$ . The trade-off between complexity and   \n141 expressiveness is a dilemma. Previous studies [43; 44] attribute Linear Attention\u2019s performance   \n142 degradation on supervised tasks to its incompetence in focusing. Due to the absence of non-linear   \n143 attention reweighting by Softmax operation, Linear Attention cannot concentrate on important regions   \n144 related to the query, such as foreground and neighbors.   \n145 Back to MUAD, previous methods [3; 24] suggest adopting Attentions instead of Convolutions   \n146 because Convolutions can easily learn identical mappings. Nevertheless, both operations are in   \n147 danger of forming identity mapping by over-concentrating on corresponding input locations for   \n148 producing the outputs: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{Conv\\;Kernel}=\\left[\\begin{array}{l l l}{0}&{0}&{0}\\\\ {0}&{1}&{0}\\\\ {0}&{0}&{0}\\end{array}\\right]\\;,\\qquad\\qquad\\mathrm{Attention\\;Map}=\\left[\\begin{array}{l l l l}{1}&{0}&{0}&{0}\\\\ {0}&{1}&{0}&{0}\\\\ {0}&{0}&{1}&{0}\\\\ {0}&{0}&{0}&{1}\\end{array}\\right]\\;.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "149 In Dinomaly, we turn to leverage the \"unfocusing ability\" of Linear Attention. In order to probe   \n150 how Attentions propagate information, we train two variants of Dinomaly using vanilla Softmax   \n151 Attention or Linear Attention as the spatial mixer in the decoder and visualize their attention maps.   \n152 As shown in Figure 3, Softmax Attention tends to focus on the exact region of the query, while   \n153 Linear Attention spreads its attention across the whole image. This implies that Linear Attention,   \n154 forced by its incompetence to focus, utilizes more long-range information to restore features at each   \n155 position, reducing the chance of passing identical information of unseen patterns to the next layer   \n156 during reconstruction. Of course, employing Linear Attention also benefits from less computation,   \n157 free of performance drop. ", "page_idx": 4}, {"type": "image", "img_path": "2vhkjOdlc8/tmp/24609f143e5ef2860a8be0920bd557e2c038e7a8b7a4d2047055d5c8aaaaf4b5.jpg", "img_caption": ["Figure 4: Schemes of reconstruction constraint. (a) Layer-to-layer (sparse). (b) Layer-to-cat-layer. (c) Layer-to-layer (dense). (d) Loose group-to-group, 1-group (Ours). (e) Loose group-to-group, 2-group (Ours). "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "158 2.4 Loose Reconstruction ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "159 \u201cThe tighter you squeeze, the less you have.\u201d ", "page_idx": 5}, {"type": "text", "text": "160 Loose Constraint. Pioneers of feature-reconstruction/distillation UAD methods [5; 2] are inspired   \n161 by knowledge distillation [45]. Most reconstruction-based methods distill specific encoder layers   \n162 (e.g. 3 last layers of 3 ResNet stages) by the corresponding decoder layers [2; 5; 17] (Figure 4(a))   \n163 or the last decoder layer [3; 4] (Figure 4(b)). Intuitively, with more encoder-decoder feature pairs   \n164 (Figure 4(c)), UAD model can utilize more information in different layers to discriminate anomalies.   \n165 However, according to the intuition of knowledge distillation, the student (decoder) can better mimic   \n166 the behavior of the teacher (encoder) given more layer-to-layer supervision, which is harmful for UAD   \n167 models that detect anomalies by encoder-decoder discrepancy. This phenomenon is also embodied   \n168 as identity mapping. Thanks to the top-to-bottom consistency of columnar Transformer layers, we   \n169 propose to loosen the layer-to-layer constraint by adding up all feature maps of interested layers as   \n170 a whole group, as shown in Figure 4(d). This scheme can be seen as loosening the layer-to-layer   \n171 correspondence, so that the decoder is allowed to act much more differently from the encoder when   \n172 the input pattern is unseen. Because features of shallow layers contain low-level visual characters   \n173 that are helpful for precise localization, we can further group the features into the low-semantic-level   \n174 group and high-semantic-level group, as shown in Figure 4(e).   \n175 Loose Loss. Following the above analysis, we also loosen the point-by-point reconstruction loss   \n176 function by discarding some points in the feature map. Here, we simply borrow the hard-mining   \n177 global cosine loss [18] that detaches the gradients of well-restored feature points with low cosine   \n178 distance during training. Let $f_{E}$ and $f_{D}$ denotes (grouped) feature maps of encoder and decoder: ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{g l o b a l-h m}=\\mathcal{D}_{c o s}(\\mathcal{F}(f_{E}),\\mathcal{F}(f_{D}))=1-\\frac{\\mathcal{F}(f_{E})^{T}\\cdot\\mathcal{F}(f_{D})}{\\Vert\\mathcal{F}(f_{E})\\Vert~\\Vert\\mathcal{F}(f_{D})\\Vert},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "179 ", "page_idx": 5}, {"type": "equation", "text": "$$\nf_{D}(h,w)=\\left\\{\\begin{array}{l l}{s g(f_{D}(h,w))_{0.1},\\mathrm{~if~}\\mathcal{D}_{c o s}(f_{D}(h,w),f_{E}(h,w))<90\\%\\left[\\mathcal{D}_{c o s}(f_{D},f_{E})\\right]_{b a t c h}}\\\\ {f_{D}(h,w),\\mathrm{~else}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "180 where $\\mathcal F(\\cdot)$ denotes flatten operation, $f_{D}(h,w)$ represents the feature point at $(h,w)$ , $s g(\\cdot)_{0.1}$   \n181 denotes shrink the gradient to one-tenth of the original 2, $\\begin{array}{r l}{\\mathcal{D}_{c o s}(f_{D}(h,w),f_{E}(h,w))}&{{}<}\\end{array}$   \n182 $90\\%\\,[\\mathcal{D}_{c o s}(f_{D},f_{E})]_{b a t c h}$ selects $90\\%$ feature points with smaller cosine distance within a batch.   \n183 Total loss is the mean $\\mathcal{L}_{g l o b a l-h m}$ of all encoder-decoder feature pairs. ", "page_idx": 6}, {"type": "text", "text": "184 3 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "185 3.1 Experimental Settings ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "186 Datasets. MVTec-AD [20] contains 15 objects (5 texture classes and 10 object classes) with a   \n187 total of 3,629 normal images as the training set and 1,725 images as the test set (467 normal, 1258   \n188 anomalous). VisA [21] contains 12 objects. Training and test sets are split following the official   \n189 splitting, resulting in 8,659 normal images in the training set and 2,162 images in the test set (962   \n190 normal, 1,200 anomalous). Real-IAD [22] is a large UAD dataset recently released, containing 30   \n191 distinct objects. We follow the official splitting that includes all views, resulting in 36,465 normal   \n192 images in the training set and 114,585 images in the test set (63,256 normal, 51,329 anomalous).   \n193 Metrics. Following prior works [19; 17], we adopt 7 evaluation metrics. Image-level anomaly   \n194 detection performance is measured by the Area Under the Receiver Operator Curve (AUROC),   \n195 Average Precision (AP), and $F_{1}$ score under optimal threshold ( $F_{1}$ -max). Pixel-level anomaly   \n196 localization is measured by AUROC, AP, $F_{1}$ -max and the Area Under the Per-Region-Overlap   \n197 (AUPRO). The results of a dataset is the average of all classes.   \n198 Implementation Details. ViT-Base/14 (patchsize $_{=14}$ ) pre-trained by DINOv2-R [39] is used as   \n199 the encoder by default. The drop rate of Noisy Bottleneck is 0.2 by default and increases to 0.4 on   \n200 the diverse Real-IAD. Loose constraint with 2 groups is employed, and the anomaly map is given   \n201 by the mean per-point cosine distance of the 2 groups. The input image is first resized to $\\mathrm{\\overline{{448^{2}}}}$   \n202 and then center-cropped to $392^{2}$ , so the feature map $({\\bar{2}}8^{2})$ is large enough for anomaly localization.   \n203 StableAdamW optimizer [46] with AMSGrad [47] (more stable than AdamW [48] in training) is   \n204 utilized with $\\scriptstyle{l r=2\\mathrm{e}-3}$ , $\\beta{=}(0.9,\\!0.999)$ and $w d{=}1\\mathrm{e}{-}4$ . The network is trained for 10,000 iterations   \n205 (steps) on MVTec-AD and VisA, and 50,000 iterations on Real-IAD. More details are available in   \n206 Appendix A.2. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "207 3.2 Comparison to Multi-Class UAD SoTAs ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "208 We compare the proposed Dinomaly with the most advanced methods. Among them, RD4AD [2]   \n209 based on feature reconstruction, SimpleNet [13] based on feature-level pseudo-anomaly, and DeST  \n210 Seg [12] based on feature reconstruction & pseudo anomaly are designed for conventional class  \n211 separated UAD settings. UniAD based on feature reconstruction, ReContrast [18] based on contrastive   \n212 reconstruction, ViTAD [17] based on feature reconstruction & Transformer, DiAD [49] based on   \n213 Diffusion reconstruction, and MambaAD [19] based on feature reconstruction & Mamba are designed   \n214 for MUAD settings. Notably, ViTAD and MambaAD are contemporary arxiv preprints released   \n215 within months. The intuitive comparison is already presented in Figure 1.   \n216 Experimental results are presented in Table 1, where Dinomaly surpasses compared methods by a large   \n217 margin on all datasets and all metrics. On the most widely used MVTec-AD, Dinomaly produces   \n218 image-level performance of 99.6/99.8/99.0 and pixel-level performance of 98.4/69.3/69.2/94.8,   \n219 outperforming previous SoTAs by 1.0/0.2/1.2 and $\\pmb{\\theta}.7/\\pmb{9}.I/7.7/I.\\pmb{6}$ . This result declares that the   \n220 image-level performance on the MVTec-AD dataset is nearly saturated under the MUAD setting.   \n221 On the popular VisA, Dinomaly achieves image-level performance of 98.7/98.9/96.2 and pixel-level   \n222 performance of 98.7/53.2/55.7/94.5, outperforming previous SoTAs by 3.2/2.5/4.2 and $0.2/5.3/5.I/2.6$   \n223 On the Real-IAD that contains 30 classes, each with 5 camera views, we produce image-level and   \n224 pixel-level performance of 89.3/86.8/80.2 and 98.8/42.8/47.1/93.9, outperforming previous SoTAs by   \n225 3.0/2.2/3.2 and $0.3/4.9/5.4/3.4$ , indicating our scalability to extremely complex scenarios. Per-class   \n226 performances and qualitative visualization are presented in Appendix A.5 and A.6. In addition,   \n227 adopting a larger backbone further improves the above performances, as presented in Table A2. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "table", "img_path": "2vhkjOdlc8/tmp/ef0166d8d625c0d14bb9a25c9fd17f2fb575c1857fdfba62222fdfac599051c0.jpg", "table_caption": ["Table 1: Performance under multi-class UAD setting $(\\%)$ . \u2020: method designed for MUAD. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "2vhkjOdlc8/tmp/641ca2fdcebd482968c66e6b5d1ce503a638676d7173b6882df0718eddf255e8.jpg", "table_caption": ["Table 2: Performance under conventional class-separated UAD setting $(\\%).\\ \\mathrm{n/a}$ : not available. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "228 3.3 Comparison to Class-Separated UAD SoTAs ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "229 We also compare our Dinomaly with class-separated SoTAs, as shown in Table 2. On MVTec-AD and   \n230 VisA, our Dinomaly under MUAD setting is comparable to conventional SoTAs that build individual   \n231 models for each class [2; 13; 8; 15]. In addition, Dinomaly is subjected to nearly no performance   \n232 drop compared to its class-separated counterpart on these datasets. On the complicated Real-IAD   \n233 that involves more images, classes, and views, class-separated Dinomaly sets new SoTA records.   \n234 Multi-class Dinomaly presents moderate performance drop but is still comparable to class-separated   \n235 SoTAs. ", "page_idx": 7}, {"type": "text", "text": "236 3.4 Ablation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "237 Overall Ablation. We conduct experiments to verify the effectiveness of the proposed elements, i.e.,   \n238 Noisy Bottleneck (NB), Linear Attention (LA), Loose Constraint (LC), and Loose Loss (LL). The   \n239 already-powerful baseline is Dinomaly with noiseless MLP bottleneck, Softmax Attention, dense   \n240 layer-to-layer supervision, and global cosine loss [18]. Results on MVTec-AD and VisA are shown in   \n241 Table 3 and Table A1, respectively. NB and LL can directly contribute to the model performance. LA   \n242 and LC boost the performance with the presence of NB. The use of LC is not solely beneficial because   \n243 LC makes the reconstruction too easy without injected noise. Combining some of the proposed   \n244 elements boosts the performance of the baseline, while employing them all produces the best results.   \n245 Noisy Rates. We conduct ablations on the discarding rate of the Dropouts in MLP bottleneck, as   \n246 shown in Table 4. Experimental results demonstrate that Dinomaly is robust to different levels   \n247 of dropout rate. Reconstruction Constraint. We quantitatively examine different reconstruction   \n248 schemes presented in Figure 4. As shown in Table 5, group-to-group LC outperforms layer-to-layer   \n249 supervision. On image-level metrics, 1-group LC with all layers added performs similarly to its   \n250 2-group counterpart that separates low-level and high-level layers; however, 1-group LC mixes   \n251 low-level and high-level features which is harmful for anomaly localization. More ablations on   \n252 scalability, input size, pre-trained foundations, etc., are presented in Appendix A.3. ", "page_idx": 7}, {"type": "table", "img_path": "2vhkjOdlc8/tmp/5ac8d1d5cc6776f3e92b6c51ed6aa85dba7dc5101368d9aefbc26cdb2f67621c.jpg", "table_caption": ["Table 3: Ablations of Dinomaly elements on MVTec-AD $(\\%)$ . NB: Noisy Bottleneck. LA: Linear Attention. LC: Loose Constraint (2 groups). LL: Loose Loss. Results on VisA see Table A1. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "2vhkjOdlc8/tmp/43b5ee6f654d20693fcd424a489fee2d47fbaf22c48cc516b676b7ed5ed9a79c.jpg", "table_caption": ["Table 4: Ablations of Dropout rates in Noisy Bottleneck, conducted on MVTec-AD $(\\%)$ . \u2020: default. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "2vhkjOdlc8/tmp/0e15d421d6bb86ca49168f92e96067c8f3e65871335ce802e01cadbc713c576d.jpg", "table_caption": ["Table 5: Ablations of reconstruction constraint, conduected on MVTec-AD $(\\%)$ . \u2020: default. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "253 4 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "254 Dinomaly, a minimalistic UAD framework, is proposed to address the under-performed MUAD   \n255 models in this paper. We present four key elements in Dinomaly, i.e., Foundation Transformer, Noisy   \n256 MLP Bottleneck, Linear Attention, and Loose Reconstruction, that can boost the performance under   \n257 the challenging MUAD setting without fancy modules and tricks. Extensive experiments on MVTec   \n258 AD, VisA, and Real-IAD demonstrate our superiority over previous model-unified multi-class models   \n259 and even recent class-separated models, indicating the feasibility of implementing a unified model in   \n260 complicated scenarios free of severe performance degradation. ", "page_idx": 8}, {"type": "text", "text": "261 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "262 [1] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \u201cImagenet: A large-scale   \n263 hierarchical image database,\u201d in IEEE Conference on Computer Vision and Pattern Recognition,   \n264 pp. 248\u2013255, 2009.   \n265 [2] H. Deng and X. Li, \u201cAnomaly detection via reverse distillation from one-class embedding,\u201d   \n266 in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,   \n267 pp. 9737\u20139746, 2022.   \n268 [3] Z. You, L. Cui, Y. Shen, K. Yang, X. Lu, Y. Zheng, and X. Le, \u201cA unified model for multi-class   \n269 anomaly detection,\u201d arXiv preprint arXiv:2206.03687, 2022.   \n270 [4] J. Yang, Y. Shi, and Z. Qi, \u201cDfr: Deep feature reconstruction for unsupervised anomaly   \n271 segmentation,\u201d arXiv preprint arXiv:2012.07122, 2020.   \n272 [5] M. Salehi, N. Sadjadi, S. Baselizadeh, M. H. Rohban, and H. R. Rabiee, \u201cMultiresolution   \n273 knowledge distillation for anomaly detection,\u201d in Proceedings of the IEEE/CVF conference on   \n274 computer vision and pattern recognition, pp. 14902\u201314912, 2021.   \n275 [6] G. Wang, S. Han, E. Ding, and D. Huang, \u201cStudent-teacher feature pyramid matching for   \n276 anomaly detection,\u201d in The British Machine Vision Conference (BMVC), 2021.   \n277 [7] T. Defard, A. Setkov, A. Loesch, and R. Audigier, \u201cPadim: a patch distribution modeling   \n278 framework for anomaly detection and localization,\u201d in International Conference on Pattern   \n279 Recognition, pp. 475\u2013489, Springer, 2021.   \n280 [8] K. Roth, L. Pemula, J. Zepeda, B. Sch\u00f6lkopf, T. Brox, and P. Gehler, \u201cTowards total recall in   \n281 industrial anomaly detection,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision   \n282 and Pattern Recognition, pp. 14318\u201314328, 2022.   \n283 [9] S. Lee, S. Lee, and B. C. Song, \u201cCfa: Coupled-hypersphere-based feature adaptation for   \n284 target-oriented anomaly localization,\u201d IEEE Access, vol. 10, pp. 78446\u201378454, 2022.   \n285 [10] V. Zavrtanik, M. Kristan, and D. Sko\u02c7c aj, \u201cDraem-a discriminatively trained reconstruction   \n286 embedding for surface anomaly detection,\u201d in Proceedings of the IEEE/CVF International   \n287 Conference on Computer Vision, pp. 8330\u20138339, 2021.   \n288 [11] C.-L. Li, K. Sohn, J. Yoon, and T. Pfister, \u201cCutpaste: Self-supervised learning for anomaly   \n289 detection and localization,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision   \n290 and Pattern Recognition, pp. 9664\u20139674, 2021.   \n291 [12] X. Zhang, S. Li, X. Li, P. Huang, J. Shan, and T. Chen, \u201cDestseg: Segmentation guided   \n292 denoising student-teacher for anomaly detection,\u201d in Proceedings of the IEEE/CVF Conference   \n293 on Computer Vision and Pattern Recognition, pp. 3914\u20133923, 2023.   \n294 [13] Z. Liu, Y. Zhou, Y. Xu, and Z. Wang, \u201cSimplenet: A simple network for image anomaly   \n295 detection and localization,\u201d arXiv preprint arXiv:2303.15140, 2023.   \n296 [14] J. Bae, J.-H. Lee, and S. Kim, \u201cPni: industrial anomaly detection using position and neighbor  \n297 hood information,\u201d in Proceedings of the IEEE/CVF International Conference on Computer   \n298 Vision, pp. 6373\u20136383, 2023.   \n299 [15] K. Batzner, L. Heckler, and R. K\u00f6nig, \u201cEfficientad: Accurate visual anomaly detection at   \n300 millisecond-level latencies,\u201d in Proceedings of the IEEE/CVF Winter Conference on Applications   \n301 of Computer Vision, pp. 128\u2013138, 2024.   \n302 [16] X. Zhang, M. Xu, and X. Zhou, \u201cRealnet: A feature selection network with realistic synthetic   \n303 anomaly for anomaly detection,\u201d arXiv preprint arXiv:2403.05897, 2024.   \n304 [17] J. Zhang, X. Chen, Y. Wang, C. Wang, Y. Liu, X. Li, M.-H. Yang, and D. Tao, \u201cExplor  \n305 ing plain vit reconstruction for multi-class unsupervised anomaly detection,\u201d arXiv preprint   \n306 arXiv:2312.07495, 2023.   \n307 [18] J. Guo, S. Lu, L. Jia, W. Zhang, and H. Li, \u201cRecontrast: Domain-specific anomaly detection via   \n308 contrastive reconstruction,\u201d in Advances in Neural Information Processing Systems (NeurIPS),   \n309 vol. 36, pp. 10721\u201310740, 2023.   \n310 [19] H. He, Y. Bai, J. Zhang, Q. He, H. Chen, Z. Gan, C. Wang, X. Li, G. Tian, and L. Xie,   \n311 \u201cMambaad: Exploring state space models for multi-class unsupervised anomaly detection,\u201d   \n312 arXiv preprint arXiv:2404.06564, 2024.   \n313 [20] P. Bergmann, M. Fauser, D. Sattlegger, and C. Steger, \u201cMvtec ad\u2013a comprehensive real-world   \n314 dataset for unsupervised anomaly detection,\u201d in Proceedings of the IEEE/CVF conference on   \n315 computer vision and pattern recognition, pp. 9592\u20139600, 2019.   \n316 [21] Y. Zou, J. Jeong, L. Pemula, D. Zhang, and O. Dabeer, \u201cSpot-the-difference self-supervised   \n317 pre-training for anomaly detection and segmentation,\u201d in European Conference on Computer   \n318 Vision, pp. 392\u2013408, Springer, 2022.   \n319 [22] C. Wang, W. Zhu, B.-B. Gao, Z. Gan, J. Zhang, Z. Gu, S. Qian, M. Chen, and L. Ma, \u201cReal-iad:   \n320 A real-world multi-view dataset for benchmarking versatile industrial anomaly detection,\u201d arXiv   \n321 preprint arXiv:2403.12580, 2024.   \n322 [23] Y. Zhao, \u201cOmnial: A unified cnn framework for unsupervised anomaly localization,\u201d in Proceed  \n323 ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3924\u20133933,   \n324 2023.   \n325 [24] R. Lu, Y. Wu, L. Tian, D. Wang, B. Chen, X. Liu, and R. Hu, \u201cHierarchical vector quantized   \n326 transformer for multi-class unsupervised anomaly detection,\u201d arXiv preprint arXiv:2310.14228,   \n327 2023.   \n328 [25] H. Yin, G. Jiao, Q. Wu, B. F. Karlsson, B. Huang, and C. Y. Lin, \u201cLafite: Latent diffusion   \n329 model with feature editing for unsupervised multi-class anomaly detection,\u201d arXiv preprint   \n330 arXiv:2307.08059, 2023.   \n331 [26] H. He, J. Zhang, H. Chen, X. Chen, Z. Li, X. Chen, Y. Wang, C. Wang, and L. Xie, \u201cDiad: A   \n332 diffusion-based framework for multi-class anomaly detection,\u201d arXiv preprint arXiv:2312.06607,   \n333 2023.   \n334 [27] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and   \n335 I. Polosukhin, \u201cAttention is all you need,\u201d in Advances in Neural Information Processing   \n336 Systems, pp. 5998\u20136008, 2017.   \n337 [28] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani,   \n338 M. Minderer, G. Heigold, S. Gelly, et al., \u201cAn image is worth 16x16 words: Transformers for   \n339 image recognition at scale,\u201d arXiv preprint arXiv:2010.11929, 2020.   \n340 [29] M. Caron, H. Touvron, I. Misra, H. J\u00e9gou, J. Mairal, P. Bojanowski, and A. Joulin, \u201cEmerging   \n341 properties in self-supervised vision transformers,\u201d in Proceedings of the IEEE/CVF international   \n342 conference on computer vision, pp. 9650\u20139660, 2021.   \n343 [30] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza,   \n344 F. Massa, A. El-Nouby, et al., \u201cDinov2: Learning robust visual features without supervision,\u201d   \n345 arXiv preprint arXiv:2304.07193, 2023.   \n346 [31] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, \u201cSwin transformer:   \n347 Hierarchical vision transformer using shifted windows,\u201d in IEEE/CVF International Conference   \n348 on Computer Vision, pp. 10012\u201310022, 2021.   \n349 [32] X. Chen, S. Xie, and K. He, \u201cAn empirical study of training self-supervised vision transformers,\u201d   \n350 in Proceedings of the IEEE/CVF international conference on computer vision, pp. 9640\u20139649,   \n351 2021.   \n352 [33] K. He, X. Chen, S. Xie, Y. Li, P. Doll\u00e1r, and R. Girshick, \u201cMasked autoencoders are scalable   \n353 vision learners,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern   \n354 Recognition, pp. 16000\u201316009, 2022.   \n355 [34] Z. Xie, Z. Zhang, Y. Cao, Y. Lin, J. Bao, Z. Yao, Q. Dai, and H. Hu, \u201cSimmim: A simple   \n356 framework for masked image modeling,\u201d in Proceedings of the IEEE/CVF conference on   \n357 computer vision and pattern recognition, pp. 9653\u20139663, 2022.   \n358 [35] Z. Peng, L. Dong, H. Bao, Q. Ye, and F. Wei, \u201cBeit v2: Masked image modeling with vector  \n359 quantized visual tokenizers,\u201d arXiv preprint arXiv:2208.06366, 2022.   \n360 [36] J. Zhou, C. Wei, H. Wang, W. Shen, C. Xie, A. Yuille, and T. Kong, \u201cibot: Image bert pre-training   \n361 with online tokenizer,\u201d arXiv preprint arXiv:2111.07832, 2021.   \n362 [37] T. Reiss, N. Cohen, E. Horwitz, R. Abutbul, and Y. Hoshen, \u201cAnomaly detection requires better   \n363 representations,\u201d in European Conference on Computer Vision, pp. 56\u201368, Springer, 2022.   \n364 [38] Y. Lee, H. Lim, and H. Yoon, \u201cSelformaly: Towards task-agnostic unified anomaly detection,\u201d   \n365 arXiv preprint arXiv:2307.12540, 2023.   \n366 [39] T. Darcet, M. Oquab, J. Mairal, and P. Bojanowski, \u201cVision transformers need registers,\u201d arXiv   \n367 preprint arXiv:2309.16588, 2023.   \n368 [40] Y. Liang, J. Zhang, S. Zhao, R. Wu, Y. Liu, and S. Pan, \u201cOmni-frequency channel-selection   \n369 representations for unsupervised anomaly detection,\u201d arXiv preprint arXiv:2203.00259, 2022.   \n370 [41] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov, \u201cIm  \n371 proving neural networks by preventing co-adaptation of feature detectors,\u201d arXiv preprint   \n372 arXiv:1207.0580, 2012.   \n373 [42] A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret, \u201cTransformers are rnns: Fast autore  \n374 gressive transformers with linear attention,\u201d in International conference on machine learning,   \n375 pp. 5156\u20135165, PMLR, 2020.   \n376 [43] D. Han, X. Pan, Y. Han, S. Song, and G. Huang, \u201cFlatten transformer: Vision transformer   \n377 using focused linear attention,\u201d in Proceedings of the IEEE/CVF International Conference on   \n378 Computer Vision, pp. 5961\u20135971, 2023.   \n379 [44] Z. Shen, M. Zhang, H. Zhao, S. Yi, and H. Li, \u201cEfficient attention: Attention with linear   \n380 complexities,\u201d in Proceedings of the IEEE/CVF winter conference on applications of computer   \n381 vision, pp. 3531\u20133539, 2021.   \n382 [45] G. Hinton, O. Vinyals, and J. Dean, \u201cDistilling the knowledge in a neural network,\u201d arXiv   \n383 preprint arXiv:1503.02531, 2015.   \n384 [46] M. Wortsman, T. Dettmers, L. Zettlemoyer, A. Morcos, A. Farhadi, and L. Schmidt, \u201cStable and   \n385 low-precision training for large-scale vision-language models,\u201d Advances in Neural Information   \n386 Processing Systems, vol. 36, pp. 10271\u201310298, 2023.   \n387 [47] S. J. Reddi, S. Kale, and S. Kumar, \u201cOn the convergence of adam and beyond,\u201d arXiv preprint   \n388 arXiv:1904.09237, 2019.   \n389 [48] I. Loshchilov and F. Hutter, \u201cDecoupled weight decay regularization,\u201d arXiv preprint   \n390 arXiv:1711.05101, 2017.   \n391 [49] H. He, J. Zhang, H. Chen, X. Chen, Z. Li, X. Chen, Y. Wang, C. Wang, and L. Xie, \u201cA diffusion  \n392 based framework for multi-class anomaly detection,\u201d in Proceedings of the AAAI Conference on   \n393 Artificial Intelligence, vol. 38, pp. 8472\u20138480, 2024.   \n394 [50] P. Bergmann, S. L\u00f6we, M. Fauser, D. Sattlegger, and C. Steger, \u201cImproving unsuper  \n395 vised defect segmentation by applying structural similarity to autoencoders,\u201d arXiv preprint   \n396 arXiv:1807.02011, 2018.   \n397 [51] V. Zavrtanik, M. Kristan, and D. Skocaj, \u201cReconstruction by inpainting for visual anomaly   \n398 detection,\u201d Pattern Recognition, vol. 112, p. 107706, 2021.   \n399 [52] W. Liu, R. Li, M. Zheng, S. Karanam, Z. Wu, B. Bhanu, R. J. Radke, and O. Camps, \u201cTowards   \n400 visually explaining variational autoencoders,\u201d in Proceedings of the IEEE/CVF Conference on   \n401 Computer Vision and Pattern Recognition, pp. 8642\u20138651, 2020.   \n402 [53] D. Dehaene and P. Eline, \u201cAnomaly localization by modeling perceptual features,\u201d arXiv   \n403 preprint arXiv:2008.05369, 2020.   \n404 [54] T. Schlegl, P. Seeb\u00f6ck, S. M. Waldstein, G. Langs, and U. Schmidt-Erfurth, \u201cf-anogan: Fast   \n405 unsupervised anomaly detection with generative adversarial networks,\u201d Medical image analysis,   \n406 vol. 54, pp. 30\u201344, 2019.   \n407 [55] S. Akcay, A. Atapour-Abarghouei, and T. P. Breckon, \u201cGanomaly: Semi-supervised anomaly   \n408 detection via adversarial training,\u201d in Computer Vision\u2013ACCV 2018: 14th Asian Conference on   \n409 Computer Vision, Perth, Australia, December 2\u20136, 2018, Revised Selected Papers, Part III 14,   \n410 pp. 622\u2013637, Springer, 2019.   \n411 [56] S. Sheynin, S. Benaim, and L. Wolf, \u201cA hierarchical transformation-discriminating genera  \n412 tive model for few shot anomaly detection,\u201d in Proceedings of the IEEE/CVF International   \n413 Conference on Computer Vision, pp. 8495\u20138504, 2021.   \n414 [57] J. Liu and F. Wang, \u201cmixed attention auto encoder for multi-class industrial anomaly detection,\u201d   \n415 arXiv preprint arXiv:2309.12700, 2023.   \n416 [58] X. Chen and K. He, \u201cExploring simple siamese representation learning,\u201d in Proceedings of the   \n417 IEEE/CVF conference on computer vision and pattern recognition, pp. 15750\u201315758, 2021.   \n418 [59] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. J\u2019egou, \u201cTraining data  \n419 efficient image transformers & distillation through attention,\u201d arXiv preprint arXiv:2012.12877,   \n420 2021.   \n421 [60] S. Ren, Z. Wang, H. Zhu, J. Xiao, A. Yuille, and C. Xie, \u201cRejuvenating image-gpt as strong   \n422 visual representation learners,\u201d arXiv preprint arXiv:2312.02147, 2023.   \n423 [61] W. Yu, C. Si, P. Zhou, M. Luo, Y. Zhou, J. Feng, S. Yan, and X. Wang, \u201cMetaformer baselines   \n424 for vision,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.   \n425 [62] T. Reiss, N. Cohen, L. Bergman, and Y. Hoshen, \u201cPanda: Adapting pretrained features for   \n426 anomaly detection and segmentation,\u201d in Proceedings of the IEEE/CVF Conference on Computer   \n427 Vision and Pattern Recognition, pp. 2806\u20132814, 2021.   \n428 [63] J. Jeong, Y. Zou, T. Kim, D. Zhang, A. Ravichandran, and O. Dabeer, \u201cWinclip: Zero-/few-shot   \n429 anomaly classification and segmentation,\u201d arXiv preprint arXiv:2303.14814, 2023.   \n430 [64] C. Huang, H. Guan, A. Jiang, Y. Zhang, M. Spratling, and Y.-F. Wang, \u201cRegistration based few  \n431 shot anomaly detection,\u201d in European Conference on Computer Vision, pp. 303\u2013319, Springer,   \n432 2022.   \n433 [65] X. Jiang, J. Liu, J. Wang, Q. Nie, K. Wu, Y. Liu, C. Wang, and F. Zheng, \u201cSoftpatch: Unsuper  \n434 vised anomaly detection with noisy data,\u201d Advances in Neural Information Processing Systems,   \n435 vol. 35, pp. 15433\u201315445, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "437 A.1 Related Work ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "438 Epistemic methods are based on the assumption that the networks respond differently during in  \n439 ference between seen input and unseen input. Within this paradigm, pixel reconstruction methods   \n440 assume that the networks trained on normal images can reconstruct anomaly-free regions well, but   \n441 poorly for anomalous regions. Auto-encoder (AE) [50; 51], variational auto-encoder (VAE) [52; 53],   \n442 or generative adversarial network (GAN) [54; 55] are used to restore normal pixels. However, pixel   \n443 reconstruction models may also succeed in restoring unseen anomalous regions if they resemble   \n444 normal regions in pixel values or the anomalies are barely noticeable [2]. Therefore, feature recon  \n445 struction is proposed to construct features of pre-trained encoders instead of raw pixels [2; 3; 4]. To   \n446 prevent the whole network from converging to a trivial solution, the parameters of the encoders are   \n447 frozen during training. In feature distillation [5; 6], the student network is trained from scratch to   \n448 mimic the output features of the pre-trained teacher network with the same input of normal images,   \n449 also based on the similar hypothesis that the student trained on normal samples only succeed in   \n450 mimicking features of normal regions.   \n451 Pseudo-anomaly methods generate handcrafted defects on normal images to imitate anomalies,   \n452 converting UAD to supervised classification [11] or segmentation tasks [10]. Specifically, CutPaste   \n453 [11] simulates anomalous regions by randomly pasting cropped patches of normal images. DRAEM   \n454 [10] constructs abnormal regions using Perlin noise as the mask and another image as the additive   \n455 anomaly. DeTSeg [12] employs a similar anomaly generation strategy and combines it with feature   \n456 reconstruction. SimpleNet [13] introduces anomaly by injecting Gaussian noise in the pre-trained   \n457 feature space. These methods deeply rely on how well the pseudo anomalies match the real anomalies,   \n458 which makes it hard to generalize to different datasets.   \n459 Feature statistics methods [7; 8; 56; 9] memorize all normal features (or their modeled distribution)   \n460 extracted by networks pre-trained on large-scale datasets and match them with test samples during   \n461 inference. Since these methods require memorizing, processing, and matching nearly all features   \n462 from training samples, they are computationally expensive in both training and inference, especially   \n463 when the training set is large.   \n464 Multi-Class UAD. UniAD [3] first introduced multi-class anomaly detection, aiming to detect   \n465 anomalies for different classes using a unified model. In this setting, conventional UAD methods   \n466 often face the challenge of \"identical shortcuts\", where both anomaly-free and anomaly samples can be   \n467 effectively recovered during inference [3]. It is caused by the diversity of multi-class normal patterns   \n468 that drive the network to generalize on unseen patterns. This contradicts the fundamental assumption   \n469 of epistemic methods. Many current researches focus on addressing this challenge [3; 24; 18; 57; 25].   \n470 UniAD [3] employs a neighbor-masked attention module and a feature-jitter strategy to mitigate these   \n471 shortcuts. HVQ-Trans [24] proposes a vector quantization (VQ) Transformer model that induces   \n472 large feature discrepancies for anomalies. LaftiE [25] utilizes a latent diffusion model and introduces   \n473 a feature editing strategy to alleviate this issue. DiAD [26] also employs diffusion models to address   \n474 multi-class UAD settings. OmniAL [23] focuses on anomaly localization in the unified setting,   \n475 preventing identical reconstruction by using synthesized pseudo anomalies. ViTAD [58] abstracts a   \n476 unified feature-reconstruction UAD framework and employ Transformer building blocks. MambaAD   \n477 [19] explores the recently proposed State Space Model (SSM), Mamba, in the context of multi-class   \n478 UAD.   \n479 Scope of Application. In this work, we focus on sensory AD that detects regional or structural   \n480 anomalies (common in practical applications such as industrial inspection, medical disease screening,   \n481 etc.), which is distinguished from semantic AD. In sensory AD, normal and anomalous samples   \n482 are the same objects except for anomaly, e.g. good cable vs. spoiled cable. In semantic AD, the   \n483 class of normal samples and anomalous samples are semantically different, e.g. animals vs. vehicles.   \n484 Semantic AD methods usually utilize and compare the global representation of images, which   \n485 generally do not suffer from the issues of multi-class setting discussed in this paper.. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "486 A.2 Full Implementation Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "487 ViT-Base/14 (patch size $_{=14}$ ) pre-trained by DINOv2 with registers (DINOv2-R) [39] is utilized as   \n488 the encoder. The discard rate of Dropout in Noisy Bottleneck is 0.2 by default, which is increased to   \n489 0.4 for the diverse Real-IAD. Loose constraint with 2 groups and $\\mathcal{L}_{g l o b a l-h m}$ loss are used by default.   \n490 The input image is first resized to $448^{2}$ and then center-cropped to $392^{2}$ , so that the feature map   \n491 $(28^{2})$ is large enough for localization. StableAdamW optimizer [46] with AMSGrad [47] is utilized   \n492 with lr (learning rate) $\\scriptstyle=2\\mathrm{e}-3$ , $\\beta{=}(0.9,\\!0.999)$ , wd (weight decay) $\\scriptstyle|=1\\ e-4$ and $_{e p s=1e-10}$ . The network   \n493 is trained for 10,000 iterations for MVTec-AD and VisA and 50,000 iterations for Real-IAD under   \n494 MUAD setting. The network is trained for 5,000 iterations on each class under the class-separated   \n495 UAD setting. The $l r$ warms up from 0 to 2e-3 in the first 100 iterations and cosine anneals to 2e-4   \n496 throughout the training. The discarding rate in Equation 5 linearly rises from $0\\%$ to $90\\%$ in the first   \n497 1,000 iterations as warm-up (500 iters for class-separated setting). The anomaly map is obtained by   \n498 upsampling the point-wise cosine distance between encoder and decoder feature maps (averaging   \n499 if more than one pair or group). The mean of the top $1\\%$ pixels in an anomaly map is used as the   \n500 image anomaly score. All experiments are conducted with random seed $^{=1}$ with cuda deterministic   \n501 for invariable weight initialization and batch order. Codes are implemented with Python 3.8 and   \n502 PyTorch 1.12.0 cuda 11.3, and run on NVIDIA GeForce RTX3090 GPUs (24GB). ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "503 A.3 Additional Ablation Studies and Experiments ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "504 Ablations on VisA. Similar to Table 3 that conduct ablation experiments on MVTec-AD, we   \n505 additionally run them on VisA for further validations. As shown in Table A1, proposed components   \n506 of Dinomaly contribute to the AD performances on VisA as on MVTec-AD.   \n507 Scalability. Previous works [3; 2; 17] reported that AD methods do not follow the model \"scaling   \n508 law\", i.e., larger models do not necessarily produce better performance. For example, RD4AD [2]   \n509 found WideResNet50 better than WideResNet101 as the encoder backbone. ViTAD [17] found ViT  \n510 Small better than ViT-Base. We conduct experiments to probe the influence of the scale of backbone   \n511 Transformers in Dinomaly. ViT-Small, ViT-Base (default), and ViT-Large pre-trained by DINOv2-R   \n512 are used as the encoder, respectively. ViT-Small has 12 layers, so we take the [3,4,5,...10]th layer as   \n513 the interested 8 middle layers, which is the same as default ViT-Base. ViT-Large has 24 layers, so   \n514 we take the [5,7,9,...19]th layer as the interested 8 middle layers. The layer hyperparameters of the   \n515 decoder, such as embedding dimension and numbers of attention heads, follow the hyperparameters   \n516 of the corresponding encoder. Other training strategies are identical to default. As shown in Table A2,   \n517 the MUAD performance of Dinomaly follows the \"scaling law\". Dinomaly equipped with ViT-Small   \n518 already produces state-of-the-art results. ViT-Large further boosts Dinomaly to an unprecedented   \nhigher record.   \n520 Input Size. The patch size of ViTs (usually $14\\times14$ or $16\\times16$ ) is much larger than the stem   \n521 layer\u2019s down-sampling rate of CNNs (usually $4\\times4$ ), resulting in smaller feature map size. For dense   \n522 prediction tasks like semantic segmentation, ViTs usually employ a large input image size [30]. This   \n523 practice holds in anomaly localization as well. In Table A3, we present the results of Dinomaly with   \n524 different input resolutions. Following PatchCore [8], by default, we adopt center-crop preprocessing   \n525 to reduce the influence of background, which can also cause unreachable anomalies at the edge of   \n526 images. Experimental results demonstrate our robustness to input size. While small image size is   \n527 enough for image-level anomaly detection, larger inputs are beneficial to anomaly localization. All   \n528 experiments evaluate localization performance in a unified size of $256\\times256$ for fairness.   \n529 Pre-Trained Foundations. The representation quality of the frozen backbone Transformer is of   \n530 great significance to unsupervised anomaly detection. We conduct extensive experiments to probe   \n531 the impact of different pre-training methods, including supervised learning and self-supervised   \n532 learning. DeiT [59] is trained on ImageNet[1] in a supervised manner by distilling CNNs. MAE [33],   \n533 BEiTv2 [35], and D-iGPT [60] are based on masked image modeling (MIM). Given input images   \n534 with masked patches, MAE [33] is optimized to restore raw pixels; BEiTv2 [35] is trained to predict   \n535 the token index of VQ-GAN and CLIP; D-iGPT [60] is trained to predict the features of CLIP   \n536 model. DINO [29] is based on positive-pair contrastive learning (CL), which is also referred to as   \n537 self-distillation. It trains the network to produce similar feature representations given two views   \n538 (augmentations) of the same image. iBot [36] and DINOv2 [30] combine MIM and CL strategies,   \n539 marking the SoTA of self-supervised foundation models. DINOv2-R [39] is a variation of DINOv2   \n540 that employs 4 extra register tokens.   \n541 It is noted that most models are pre-trained with the image resolution of $224\\times224$ , except that   \n542 DINOv2 [30] and DINOv2-R [39] have extra a high-resolution training phase with $518\\times518$   \n543 However, directly using the pre-trained weights on a different resolution for UAD without fine-tuning   \n544 like other supervised tasks can cause generalization problems. Therefore, by default, we still keep   \n545 5 the feature size of all compared models to $28\\times28$ , i.e., the input size is $392\\times392$ for ViT-Base/14   \n546 and $448\\times448$ for ViT-Base/16. Additionally, we train Dinomaly with the low-resolution input   \n547 size of $224\\times224$ . The results are presented in Table A4. Generally speaking, $\\scriptstyle\\mathrm{CL+MIM}$ combined   \n548 models outperform MIM and CL models. In addition, MIM-based models do not benefit from   \n549 higher resolutions but suffer from them, indicating the lack of generalization on a different input   \n550 size. Methods involving CL can better adapt to a higher resolution as they optimize the global   \n551 representation of class tokens in pre-training, which is insensitive to input size. As expected, DINOv2   \n552 and DINOv2-R pre-trained on larger inputs can better benefit from higher resolution in Dinomaly.   \n553 Because some methods, i.e., D-iGPT, DINO, and iBOT, produce similar results to DINOv2 in   \n554 $224\\times224$ , we expect that they also have the potential to be as powerful in Dinomaly if they are   \n555 pre-trained in high-resolution.   \n556 Attention vs. Convolution. Previous works and this paper have proposed to leverage attentions   \n557 instead of convolutions in UAD. Here, we conduct experiments substituting the attention in the   \n558 decoder of Dinomaly by convolutions as the spatial mixers. Following MetaFormer [61], we employ   \n559 Inverted Bottleneck block that consists of $1\\times1$ conv, GELU activation, $N\\times N$ deep-wise conv,   \n560 and $1\\times1$ conv, sequentially. The results are shown in Table A5, where Attentions outperform   \n561 Convolutions, especially for pixel-level anomaly localization. In addition, utilizing convolutions in   \n562 the decoder can still yield SoTA results, demonstrating the universality of the proposed Dinomaly.   \n563 Neighbour-Masking. Prior method [3] proposed to mask the keys and values in an $n\\times n$ square   \n564 centered at each query, in order to alleviate identity mapping in Attention. This mechanism can also   \n565 be applied to Linear Attention as well. As shown in Table A5, neighbor-masking can further improve   \n566 Dinomaly with both Softmax Attention and Linear Attention moderately.   \n567 Feature Noise. Prior method [3] proposed to perturb the encoder features by Feature Jitter, i.e. adding   \n568 Gaussian noise with scale to control the noise magnitude. We evaluate the feature jitter strategy in the   \n569 proposed Dinomaly by placing it at the beginning of Noisy Bottleneck. As shown in Table A6, both   \n570 Dropout and Feature Jitter can be a good noise injector in Noisy Bottleneck. Meanwhile, Dropout is   \n571 more robust to the noisy scale hyperparameter, and more elegant without introducing new modules.   \n572 Random Seeds. Due to limited computation resources, experiments in this paper are conducted for   \n573 one run with random seed $=\\!1$ . Here, we conduct 5 runs with 5 random seeds on MVTec-AD. As   \n574 shown in Table A7, Dinomaly is robust to randomness. ", "page_idx": 14}, {"type": "table", "img_path": "2vhkjOdlc8/tmp/f54c67191c31ae0d1bb17f9e993e58426c7e8bf78529ea333f3410b4ec5b8734.jpg", "table_caption": ["Table A1: Ablations of Dinomaly elements on VisA $(\\%)$ . NB: Noisy Bottleneck. LA: Linear Attention. LC: Loosen Constraint (2 groups). LL: Loosen Loss. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "table", "img_path": "2vhkjOdlc8/tmp/27e56100d61250318835a7f1fdde44c67566d1c389684fca58fa42c4af3e3761.jpg", "table_caption": ["Table A2: Comparison of different ViT architectures, conducted on MVTec-AD $(\\%)$ . Latency per image is measured on NVIDIA RTX3090 with batch size ${=}16$ . "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "table", "img_path": "2vhkjOdlc8/tmp/2ebc554261b756d0a0e21858dde95406b94d3ea537b0ea372674cff76a62dea7.jpg", "table_caption": ["Table A3: Ablations of input size, conducted on MVTec-AD $(\\%)$ . $\\mathrm{R448^{2}}$ -C3922 represents first resizing images to $448\\!\\times\\!448$ , then center cropping to $392\\!\\times\\!392$ . "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "table", "img_path": "2vhkjOdlc8/tmp/830e5f2b2c02ad594cdf2ec4076db68f56f8b6ac596c9b9472bb6423d6ba1451.jpg", "table_caption": ["Table A4: Comparison between pre-trained ViT foundations, conducted on MVTec-AD $(\\%)$ . All models are ViT-Base. The patch size of DINOv2 and $\\scriptstyle\\mathrm{DINOv}2-\\mathbf{R}$ is $14^{2}$ ; others are $16^{2}$ . "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "table", "img_path": "2vhkjOdlc8/tmp/eddf6acc1391ed2fdadcb26475ef2b36b3401619c512b36d3ce484fa9efdd990.jpg", "table_caption": ["Table A5: Comparison between Convolutional block, Softmax Attention, and Linear Attention as the spatial mixer of decoder, conducted on MVTec-AD $(\\%)$ . "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "575 A.4 Limitation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "576 Vision Transformers are known for their high computation cost, which can be a barrier to low  \n577 computation scenarios that require inference speed. Future research can be conducted on the effi  \n578 ciency of Transformer-based methods, such as distillation, pruning, and hardware-friendly attention   \n579 mechanism (such as FlashAttention).   \n580 As discussed in section A.1, Dinomaly is used for sensory AD that aims to detect regional anomalies   \n581 in normal backgrounds. It is not suitable for semantic AD. Previous works have shown that methods   \n582 designed for sensory AD usually fail to be competitive under semantic AD tasks [3; 2]. Conversely,   \n583 methods designed for semantic AD do not perform well on sensory AD tasks [62; 37]. Future work   \n584 can be conducted to unify these two tasks, but according to the \"no free lunch\" theorem, we believe   \n585 that methods designed for specific anomaly assumption are likely to be more convincing.   \n586 Other special UAD settings, such as zero-shot UAD (vision-language model based) [63], few-shot   \n587 UAD [64], UAD under noisy training set [65], are not included in this work. ", "page_idx": 16}, {"type": "table", "img_path": "2vhkjOdlc8/tmp/c202ef251a2382b606018cdca04a0d205885b726c3d8a3c49676efab5ddeef17.jpg", "table_caption": ["Table A6: Dropout vs. feature jitter, conducted on MVTec-AD $(\\%)$ . "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "2vhkjOdlc8/tmp/1320fc02ec95fe26228d61c191d9c504b89a4a3da12edbe7484bbcb1f3936bed.jpg", "table_caption": ["Table A7: Results of 5 random seeds on MVTec-AD $(\\%)$ . "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "588 A.5 Results Per-Category ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "589 For future research, we report the per-class results of MVTec-AD [20], VisA [21], and Real-IAD [22].   \n590 The performance of compared methods is drawn from MambaAD [19]. Thanks for their exhaustive   \n591 reproducing. The results of image-level anomaly detection and pixel-level anomaly localization on   \n592 MVTec-AD are presented in Table A8 and Table A9, respectively. The results of image-level anomaly   \n593 detection and pixel-level anomaly localization on VisA are presented in Table A10 and Table A11,   \n594 respectively. The results of image-level anomaly detection and pixel-level anomaly localization on   \n595 Real-IAD are presented in Table A12 and Table A13, respectively. ", "page_idx": 17}, {"type": "text", "text": "596 A.6 Qualitative Visualization ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "597 We visualize the output anomaly maps of Dinomaly on MVTec-AD, VisA, and Real-IAD, as shown   \n598 in Figure A1, Figure A2, and Figure A3. It is noted that all visualized samples are randomly chosen   \n599 without artificial selection.   \n601 The checklist is designed to encourage best practices for responsible machine learning research,   \n602 addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove   \n603 the checklist: The papers not including the checklist will be desk rejected. The checklist should   \n604 follow the references and precede the (optional) supplemental material. The checklist does NOT   \n605 count towards the page limit.   \n606 Please read the checklist guidelines carefully for information on how to answer these questions. For   \n607 each question in the checklist:   \n608 \u2022 You should answer [Yes] , [No] , or [NA] .   \n609 \u2022 [NA] means either that the question is Not Applicable for that particular paper or the   \n610 relevant information is Not Available.   \n611 \u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA).   \n612 The checklist answers are an integral part of your paper submission. They are visible to the   \n613 reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it   \n614 (after eventual revisions) with the final version of your paper, and its final version will be published   \n615 with the paper.   \n616 The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.   \n617 While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a   \n618 proper justification is given (e.g., \"error bars are not reported because it would be too computationally   \n619 expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering   \n620 \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we   \n621 acknowledge that the true answer is often more nuanced, so please just use your best judgment and   \n622 write a justification to elaborate. All supporting evidence can appear either in the main paper or the   \n623 supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification   \n624 please point to the section(s) where related material for the question can be found. ", "page_idx": 17}, {"type": "table", "img_path": "2vhkjOdlc8/tmp/6cbfc2aca248c4b0350d849a82f70b4a3c0f4c7cd7fa06cfc493df2df9a71ab6.jpg", "table_caption": ["Table A8: Per-class performance on MVTec-AD dataset for multi-class anomaly detection with AUROC/AP/ $F_{1}$ -max metrics. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "2vhkjOdlc8/tmp/020ed758540f3bfefbd24dd7cadc3c7f8e6a2dd65426fab83f59a0445bb44a3e.jpg", "table_caption": ["Table A9: Per-class performance on MVTec-AD dataset for multi-class anomaly localization with AUROC/AP/ $F_{1}$ -max/AUPRO metrics. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "2vhkjOdlc8/tmp/7b0e941e9295f07b56d7e41984625daeb1a996616a40cb0575b56f7f9be347a1.jpg", "table_caption": ["Table A10: Per-class performance on VisA dataset for multi-class anomaly detection with AUROC/AP/F1-max metrics. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "2vhkjOdlc8/tmp/df8129c0644c294ad74b8a8de9b7a8e12e726b323f363d00ce5b8b3f55d1a7ab.jpg", "table_caption": ["Table A11: Per-class performance on VisA dataset for multi-class anomaly localization with AUROC/AP/ $F_{1}$ -max/AUPRO metrics. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "2vhkjOdlc8/tmp/9d2a5cff8978e94e84aa6391c0fc90f8fe0ccac3559865974ea6f723f81a8b91.jpg", "table_caption": ["Table A12: Per-class performance on Real-IAD dataset for multi-class anomaly detection with AUROC/AP/ $F_{1}$ -max metrics. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "2vhkjOdlc8/tmp/24d7c2f9c6502ea2235df10d81ca73c1f2b61139b8720f2130c3d30a69b0a20a.jpg", "table_caption": ["Table A13: Per-class performance on Real-IAD dataset for multi-class anomaly localization with AUROC/AP/ $F_{1}$ -max/AUPRO metrics. "], "table_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "2vhkjOdlc8/tmp/4c7deae8f5ff6017ca373e94cf1c6a104fceca92e53f65b2b5a91488f47da924.jpg", "img_caption": ["Figure A1: Anomaly maps visualization on MVTec-AD. All samples are randomly chosen. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "2vhkjOdlc8/tmp/abdafa2e7ac9d252b3c9f07ce67b2514e7b73d90c5ae8429b6dce7e6e5128e8f.jpg", "img_caption": ["Figure A2: Anomaly maps visualization on VisA. All samples are randomly chosen. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "2vhkjOdlc8/tmp/59043b4afe109b3d96f5152242af2d94df9a3f4c7b06b267e0a8abb92d422d26.jpg", "img_caption": ["Figure A3: Anomaly maps visualization on Real-IAD. All samples are randomly chosen. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "625 IMPORTANT, please: ", "page_idx": 23}, {"type": "text", "text": "626 \u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\",   \n627 \u2022 Keep the checklist subsection headings, questions/answers and guidelines below.   \n628 \u2022 Do not modify the questions and only use the provided macros for your answers.   \n629 1. Claims   \n630 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n631 paper\u2019s contributions and scope?   \n632 Answer: [Yes]   \n633 Justification: Well reflected.   \n634 Guidelines:   \n635 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n636 made in the paper.   \n637 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n638 contributions made in the paper and important assumptions and limitations. A No or   \n639 NA answer to this question will not be perceived well by the reviewers.   \n640 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n641 much the results can be expected to generalize to other settings.   \n642 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n643 are not attained by the paper.   \n644 2. Limitations   \n645 Question: Does the paper discuss the limitations of the work performed by the authors?   \n646 Answer: [Yes]   \n647 Justification: Presented in Appendix.   \n648 Guidelines:   \n649 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n650 the paper has limitations, but those are not discussed in the paper.   \n651 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n652 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n653 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n654 model well-specification, asymptotic approximations only holding locally). The authors   \n655 should reflect on how these assumptions might be violated in practice and what the   \n656 implications would be.   \n657 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n658 only tested on a few datasets or with a few runs. In general, empirical results often   \n659 depend on implicit assumptions, which should be articulated.   \n660 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n661 For example, a facial recognition algorithm may perform poorly when image resolution   \n662 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n663 used reliably to provide closed captions for online lectures because it fails to handle   \n664 technical jargon.   \n665 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n666 and how they scale with dataset size.   \n667 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n668 address problems of privacy and fairness.   \n669 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n670 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n671 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n672 judgment and recognize that individual actions in favor of transparency play an impor  \n673 tant role in developing norms that preserve the integrity of the community. Reviewers   \n674 will be specifically instructed to not penalize honesty concerning limitations.   \n675 3. Theory Assumptions and Proofs   \n676 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n677 a complete (and correct) proof?   \n678 Answer: [NA] .   \n679 Justification: No theory.   \n680 Guidelines:   \n681 \u2022 The answer NA means that the paper does not include theoretical results.   \n682 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n683 referenced.   \n684 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n685 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n686 they appear in the supplemental material, the authors are encouraged to provide a short   \n687 proof sketch to provide intuition.   \n688 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n689 by formal proofs provided in appendix or supplemental material.   \n690 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.   \n691 4. Experimental Result Reproducibility   \n692 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n693 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n694 of the paper (regardless of whether the code and data are provided or not)?   \n695 Answer: [Yes]   \n696 Justification: Yes, and code is in supplementary material.   \n697 Guidelines:   \n698 \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.   \n\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "753 \u2022 Providing as much information as possible in supplemental material (appended to the   \n754 paper) is recommended, but including URLs to data and code is permitted.   \n755 6. Experimental Setting/Details   \n756 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n757 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n758 results?   \n759 Answer: [Yes]   \n760 Justification: Yes.   \n761 Guidelines:   \n762 \u2022 The answer NA means that the paper does not include experiments.   \n763 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n764 that is necessary to appreciate the results and make sense of them.   \n765 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n766 material.   \n767 7. Experiment Statistical Significance   \n768 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n769 information about the statistical significance of the experiments?   \n770 Answer: [Yes]   \n771 Justification: Results with mean and std are presented in Appendix.   \n772 Guidelines:   \n773 \u2022 The answer NA means that the paper does not include experiments.   \n774 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n775 dence intervals, or statistical significance tests, at least for the experiments that support   \n776 the main claims of the paper.   \n777 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n778 example, train/test split, initialization, random drawing of some parameter, or overall   \n779 run with given experimental conditions).   \n780 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n781 call to a library function, bootstrap, etc.)   \n782 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n783 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n784 of the mean.   \n785 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n786 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n787 of Normality of errors is not verified.   \n788 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n789 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n790 error rates).   \n791 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n792 they were calculated and reference the corresponding figures or tables in the text.   \n793 8. Experiments Compute Resources   \n794 Question: For each experiment, does the paper provide sufficient information on the com  \n795 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n796 the experiments?   \n797 Answer: [Yes]   \n798 Justification: Yes.   \n799 Guidelines:   \n800 \u2022 The answer NA means that the paper does not include experiments.   \n801 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n802 or cloud provider, including relevant memory and storage. ", "page_idx": 26}, {"type": "text", "text": "\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "13 Guidelines:   \n14 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n15 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n16 deviation from the Code of Ethics.   \n17 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n18 eration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "819 10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "20 Question: Does the paper discuss both potential positive societal impacts and negative   \n21 societal impacts of the work performed?   \n22 [ies]   \n23 Justification: Yes.   \n24 Guidelines:   \n25 \u2022 The answer NA means that there is no societal impact of the work performed.   \n26 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n27 impact or why the paper does not address societal impact.   \n28 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n29 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n30 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n31 groups), privacy considerations, and security considerations.   \n32 \u2022 The conference expects that many papers will be foundational research and not tied   \n33 to particular applications, let alone deployments. However, if there is a direct path to   \n34 any negative applications, the authors should point it out. For example, it is legitimate   \n35 to point out that an improvement in the quality of generative models could be used to   \n36 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n37 that a generic algorithm for optimizing neural networks could enable people to train   \n38 models that generate Deepfakes faster.   \n39 \u2022 The authors should consider possible harms that could arise when the technology is   \n40 being used as intended and functioning correctly, harms that could arise when the   \n41 technology is being used as intended but gives incorrect results, and harms following   \n42 from (intentional or unintentional) misuse of the technology.   \n43 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n44 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n45 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n46 feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "48 Question: Does the paper describe safeguards that have been put in place for responsible   \n49 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n50 image generators, or scraped datasets)?   \n851 Answer: [NA] .   \n852 Justification: No risk.   \n853 Guidelines:   \n854 \u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "864 12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "865 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n866 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n867 properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "886 13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "87 Question: Are new assets introduced in the paper well documented and is the documentation   \n88 provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 28}, {"type": "text", "text": "890 Justification: No new assets. ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "900 14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "901 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n902 include the full text of instructions given to participants and screenshots, if applicable, as   \n903 well as details about compensation (if any)?   \n904 Answer: [NA] .   \n905 Justification: No human subjects. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "906   \n907   \n908   \n909   \n910   \n911   \n912   \n913   \n914   \n915   \n916   \n917   \n918   \n919   \n920   \n921   \n922   \n923   \n924   \n925   \n926   \n927   \n928   \n929   \n930   \n931   \n932   \n933 ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] . Justification: No involving. Guidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. \u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. \u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]