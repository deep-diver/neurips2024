[{"type": "text", "text": "Online Posterior Sampling with a Diffusion Prior ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Branislav Kveton Boris Oreshkin Youngsuk Park Aniket Deshmukh Rui Song Adobe Research\u2217 Amazon AWS AI Labs AWS AI Labs Amazon ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Posterior sampling in contextual bandits with a Gaussian prior can be implemented exactly or approximately using the Laplace approximation. The Gaussian prior is computationally efficient but it cannot describe complex distributions. In this work, we propose approximate posterior sampling algorithms for contextual bandits with a diffusion model prior. The key idea is to sample from a chain of approximate conditional posteriors, one for each stage of the reverse diffusion process, which are obtained by the Laplace approximation. Our approximations are motivated by posterior sampling with a Gaussian prior, and inherit its simplicity and efficiency. They are asymptotically consistent and perform well empirically on a variety of contextual bandit problems. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "A multi-armed bandit [26, 6, 29] is an online learning problem where an agent sequentially interacts with an environment over $n$ rounds with the goal of maximizing its rewards. In each round, it takes an action and receives its stochastic reward. The mean rewards of the actions are unknown a priori and must be learned. This leads to the exploration-exploitation dilemma: explore actions to learn about them or exploit the action with the highest estimated reward. Bandits have been successfully applied to problems where uncertainty modeling and adaptation to it are beneficial, such as in recommender systems [31, 53, 24, 34] and hyper-parameter optimization [33]. ", "page_idx": 0}, {"type": "text", "text": "Contextual bandits [28, 31] with linear [13, 1] and generalized linear models (GLMs) [16, 32, 2, 25] have become popular due to the their flexibility and efficiency. The features in these models can be hand-crafted or learned from historic data [39], and the models can be also updated incrementally [1, 23]. While the original algorithms for linear and GLM bandits were based on upper confidence bounds (UCBs) [13, 1, 16], Thompson sampling $(T S)$ is more popular in practice [11, 3, 41, 43]. The key idea in TS is to explore by sampling from the posterior distribution of model parameter $\\theta_{*}$ . TS uses the prior knowledge about $\\theta_{*}$ to speed up exploration [11, 39, 35, 9, 20, 19, 5]. When the prior is a multivariate Gaussian, the posterior of $\\boldsymbol{\\theta}_{*}$ can be updated and sampled from efficiently [11]. This prior has a limited expressive power, because it cannot even represent multimodal distributions. To address this, we study posterior sampling with a diffusion prior. The main benefit of such priors is that they can represent complex distributions and be learned from data. ", "page_idx": 0}, {"type": "text", "text": "We make the following contributions. First, we propose novel posterior sampling approximations for linear models and GLMs with a diffusion model prior. The key idea is to sample from a chain of approximate conditional posteriors, one for each stage of the reverse process, which are estimated in a closed form. In linear models, each conditional is a product of two Gaussians, representing prior knowledge and diffused evidence (Theorem 2). In GLMs, each conditional is obtained by a Laplace approximation, which mixes prior knowledge and evidence (Theorem 4). Our approximations are motivated by posterior sampling with a Gaussian prior, and inherit its simplicity and efficiency. In prior works (Section 7), posterior sampling is implemented using the likelihood score, which tends to infinity as the number of observations increases; and therefore causes instability. We combine the likelihood with the conditional prior, in each stage of the diffusion model, using the Laplace approximation. The resulting posterior concentrates at a single point and can be easily sampled from, even if the likelihood score tends to infinity. We propose an efficient and asymptotically consistent implementation of this idea, using a stage-wise Laplace approximation with diffused evidence. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Our second contribution is in theory. We properly derive our posterior approximations (Theorems 2 and 4) and show their asymptotic consistency (Theorem 3). The key idea in the proof of Theorem 3 is that the conditional posteriors concentrate at a scaled unknown model parameter as the number of observations increases. While this claim is asymptotic, it is an expected property of a posterior distribution. Many prior works, such as Chung et al. [12], do not have this property. All of our main results rely on our novel approximation of clean samples by scaled diffused samples (Section 4.3). The most challenging part of the analysis is Theorem 3, where we analyze an asymptotic behavior of a chain of $T$ dependent random vectors. ", "page_idx": 1}, {"type": "text", "text": "Our last contribution is an empirical evaluation on contextual bandits. We focus on bandits because the ability to represent all levels of uncertainty precisely is critical for exploration. Our experiments show that a score-based method fails to do so (Section 6.2). Note that our posterior approximations are general and not restricted to bandits. ", "page_idx": 1}, {"type": "text", "text": "2 Setting ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We start with introducing our notation. Random variables are capitalized, except for Greek letters like $\\theta$ . We denote the marginal and conditional probabilities under probability measure $p$ by $p(X=x)$ and $p(X=x\\mid Y=\\bar{y})$ , respectively. When the random variables are clear from context, we write $p(x)$ and $p(x\\mid y)$ . We denote by $X_{n:m}$ and $x_{n:m}$ a collection of random variables and their values, respectively. For a positive integer $n$ , we define $[n]=\\{1,\\ldots,n\\}$ . The indicator function is $\\mathbb{1}\\{\\cdot\\}$ . The $i$ -th entry of vector $v$ is $v_{i}$ . If the vector is already indexed, such as $v_{j}$ , we write $v_{j,i}$ . We denote the maximum and minimum eigenvalues of matrix $M\\in\\mathbb{R}^{d\\times d}$ by $\\lambda_{1}(M)$ and $\\lambda_{d}(M)$ , respectively. ", "page_idx": 1}, {"type": "text", "text": "The posterior sampling problem can be formalized as follows. Let $\\theta_{*}\\in\\Theta$ be an unknown model parameter and $\\Theta\\ \\dot{\\subseteq}\\ \\mathbb{R}^{\\check{d}}$ be the space of model parameters. Let $\\boldsymbol{h}=\\{(\\phi\\boldsymbol{\\ell},\\boldsymbol{y}\\boldsymbol{\\ell})\\}_{\\boldsymbol{\\ell}\\in[N]}$ be the history of $N$ noisy observations of $\\boldsymbol{\\theta}_{*}$ , where $\\phi_{\\ell}\\in\\mathbb{R}^{d}$ is the feature vector for $y_{\\ell}\\in\\mathbb R$ . We assume that ", "page_idx": 1}, {"type": "equation", "text": "$$\ny_{\\ell}=g(\\phi_{\\ell}^{\\top}\\theta_{*})+\\varepsilon_{\\ell}\\,,\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $g:\\mathbb{R}\\rightarrow\\mathbb{R}$ is the mean function and $\\varepsilon_{\\ell}$ is an independent zero-mean $\\sigma^{2}$ -sub-Gaussian noise for $\\sigma>0$ . Let $p(h\\mid\\theta_{*})$ be the likelihood of observations in history $h$ under model parameter $\\theta_{*}$ and $p(\\theta_{*})$ be its prior probability. By Bayes\u2019 rule, the posterior distribution of $\\theta_{*}$ given $h$ is ", "page_idx": 1}, {"type": "equation", "text": "$$\np(\\theta_{*}\\mid h)\\propto p(h\\mid\\theta_{*})\\,p(\\theta_{*}).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "We want to sample from $p(\\cdot\\mid h)$ efficiently when the prior distribution is represented by a diffusion model. As a stepping stone, we review existing posterior formulas for multivariate Gaussian priors. This motivates our solution for diffusion model priors. ", "page_idx": 1}, {"type": "text", "text": "2.1 Linear Model ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The posterior of $\\theta_{*}$ in linear models can be derived as follows. ", "page_idx": 1}, {"type": "text", "text": "Assumption 1. Let g in (1) be an identity and $\\varepsilon_{\\ell}\\sim\\mathcal{N}(0,\\sigma^{2})$ . Then the likelihood of $h$ under model parameter $\\theta_{*}$ is $\\begin{array}{r}{p(h\\mid\\theta_{*})\\propto\\exp[-\\sum_{\\ell=1}^{N}(y_{\\ell}-\\phi_{\\ell}^{\\top}\\theta_{*})^{2}/(2\\sigma^{2})]}\\end{array}$ . ", "page_idx": 1}, {"type": "text", "text": "Let $p(\\theta_{*})=\\mathcal{N}(\\theta_{*};\\theta_{0},\\Sigma_{0})$ be the prior distribution, where $\\theta_{0}\\in\\mathbb{R}^{d}$ and $\\Sigma_{0}\\in\\mathbb{R}^{d\\times d}$ are the prior mean and covariance, respectively. Then $p(\\theta_{*}\\mid h)\\propto\\mathcal{N}(\\theta_{*};\\hat{\\theta},\\hat{\\Sigma})$ [10], where ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\hat{\\theta}=\\hat{\\Sigma}\\bigl(\\Sigma_{0}^{-1}\\theta_{0}+\\bar{\\Sigma}^{-1}\\bar{\\theta}\\bigr)\\,,\\quad\\hat{\\Sigma}=\\bigl(\\Sigma_{0}^{-1}+\\bar{\\Sigma}^{-1}\\bigr)^{-1}\\,,\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "$\\begin{array}{r}{\\bar{\\theta}=\\sigma^{-2}\\bar{\\Sigma}\\sum_{\\ell=1}^{N}\\phi_{\\ell}y_{\\ell}}\\end{array}$ , and $\\begin{array}{r}{\\bar{\\Sigma}^{-1}=\\sigma^{-2}\\sum_{\\ell=1}^{N}\\phi_{\\ell}\\phi_{\\ell}^{\\top}}\\end{array}$ . Therefore, the posterior of $\\boldsymbol{\\theta}_{*}$ is a product of two multivariate Gaussians: $\\mathcal{N}(\\theta_{0},\\Sigma_{0})$ represents prior knowledge about $\\theta_{*}$ and ${\\mathcal{N}}({\\bar{\\theta}},{\\bar{\\Sigma}})$ represents empirical evidence. ", "page_idx": 1}, {"type": "text", "text": "Algorithm 1 IRLS: Iteratively reweighted least squares. ", "page_idx": 2}, {"type": "text", "text": "1: Input: Prior parameters $\\theta_{0}$ and $\\Sigma_{0}$ , history of observations $h=\\{(\\phi_{\\ell},y_{\\ell})\\}_{\\ell\\in[N]}$ ", "page_idx": 2}, {"type": "text", "text": "2: Initialize $\\hat{\\theta}\\in\\mathbb{R}^{d}$   \n3: repeat   \n4: for stage $\\ell=1,\\ldots,N$ do   \n5: $\\begin{array}{r l}&{\\quad\\quad z_{\\ell}\\gets\\check{\\phi}_{\\ell}^{\\top}\\hat{\\theta}+(y_{\\ell}-g(\\phi_{\\ell}^{\\top}\\hat{\\theta}))/\\dot{g}(\\phi_{\\ell}^{\\top}\\hat{\\theta})}\\\\ &{\\hat{\\Sigma}\\gets\\Big(\\Sigma_{0}^{-1}+\\sum_{\\ell=1}^{N}\\dot{g}(\\phi_{\\ell}^{\\top}\\hat{\\theta})\\phi_{\\ell}\\phi_{\\ell}^{\\top}\\Big)^{-1}}\\\\ &{\\hat{\\theta}\\gets\\hat{\\Sigma}\\left(\\Sigma_{0}^{-1}\\theta_{0}+\\sum_{\\ell=1}^{N}\\dot{g}(\\phi_{\\ell}^{\\top}\\hat{\\theta})\\phi_{\\ell}z_{\\ell}\\right)}\\end{array}$   \n6:   \n7:   \n8: until $\\hat{\\theta}$ converges   \n9: Output: Posterior mean $\\hat{\\theta}$ and covariance $\\hat{\\Sigma}$ ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2.2 Generalized Linear Model ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Generalized linear models $\\left(G L M s\\right)$ [36] extend linear models (Section 2.1) to non-linear monotone mean functions $g$ in (1). For instance, in logistic regression, $g(u)=1/(1+\\exp[-u])$ is a sigmoid. The likelihood of observations in GLMs has the following form [25]. ", "page_idx": 2}, {"type": "text", "text": "Assumption 2. Let $h=\\{(\\phi_{\\ell},y_{\\ell})\\}_{\\ell\\in[N]}$ be a history of $N$ observations under mean function g and the corresponding noise. Then $\\begin{array}{r}{\\log p(\\dot{h}\\mathbin{\\,\\vert\\,}\\theta_{*})\\propto\\sum_{\\ell=1}^{N}y_{\\ell}\\phi_{\\ell}^{\\top}\\theta_{*}-b(\\phi_{\\ell}^{\\top}\\theta_{*})+c(y_{\\ell}),}\\end{array}$ , where c is a real function and $b$ is a function whose derivative is the mean function, $\\dot{b}=g$ . ", "page_idx": 2}, {"type": "text", "text": "The posterior distribution of $\\theta_{*}$ in GLMs does not have a closed form in general [10]. Therefore, it is often approximated by the Laplace approximation. Let the prior distribution of the model parameter be $p(\\theta_{\\ast})=\\mathcal{N}(\\theta_{\\ast};\\theta_{0},\\Sigma_{0})$ , as in Section 2.1. Then the Laplace approximation is $\\mathcal{N}(\\hat{\\theta},\\hat{\\Sigma})$ , where $\\hat{\\theta}$ is the maximum a posteriori $(M A P)$ estimate of $\\theta_{*}$ and $\\hat{\\Sigma}$ is the corresponding covariance. Note that the Laplace approximation can be applied to non-Gaussian priors. ", "page_idx": 2}, {"type": "text", "text": "The MAP estimate $\\hat{\\theta}$ can be obtained by iteratively reweighted least squares (IRLS) [51], which we present in Algorithm 1. IRLS is a Newton-type algorithm that computes \u03b8\u02c6 iteratively (line 6). The convergence rate is fast due to the strong convexity of the problem. In the updates, $\\dot{g}$ is the derivative of the mean function and the pseudo-observation $z_{\\ell}$ (line 4) plays the role of the observation $y_{\\ell}$ in Section 2.1. The IRLS solution has a similar structure to (3). Specifically, $\\mathcal{N}(\\hat{\\theta},\\hat{\\Sigma})$ is a product of two multivariate Gaussians, representing prior knowledge about $\\theta_{*}$ and empirical evidence. ", "page_idx": 2}, {"type": "text", "text": "2.3 Towards Diffusion Model Priors ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The assumption that $p(\\theta_{*})=\\mathcal{N}(\\theta_{*};\\theta_{0},\\Sigma_{0})$ is limiting, for instance because it precludes multimodal priors. We relax it by representing $p(\\theta_{*})$ by a diffusion model, which we call a diffusion model prior. We propose an efficient posterior sampling approximation for this prior, where the prior and empirical evidence are mixed similarly to (3) and IRLS. We review diffusion models next. ", "page_idx": 2}, {"type": "text", "text": "3 Diffusion Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Diffusion models [45, 18] are generative models trained by diffusing samples from unknown and hard to represent distributions. They can be viewed in multiple ways [48]. We adopt the probabilistic formulation and presentation of Ho et al. [18]. A diffusion model is a graphical model with $T$ stages indexed by $t\\in[T]$ . Each stage $t$ is associated with a latent variable $\\dot{S_{t}}^{\\prime}\\in\\mathbb{R}^{d}$ . A sample from the model is represented by an observed variable $S_{0}\\in\\mathbb{R}^{d}$ . We visualize a diffusion model in Figure 1. In the forward process, a clean sample $s_{0}$ is diffused through a sequence of variables $S_{1},...,S_{T}$ . This process is used to learn the reverse process, where the clean sample $s_{0}$ is generated through a sequence of variables $S_{T},\\dots,S_{0}$ . To sample $s_{0}$ from the posterior (Section 4), we add a random variable $H$ that represents partial information about $s_{0}$ . We introduce forward and reverse diffusion processes next. Learning of the reverse process is described in Appendix B. While this is a critical component of diffusion models, it is not necessary to introduce our posterior approximations. ", "page_idx": 2}, {"type": "text", "text": "Forward process (probability measure $q$ ) Reverse process (probability measure $p$ ) ", "page_idx": 3}, {"type": "equation", "text": "$S_{T}\\leftarrow S_{T-1}\\leftarrow\\cdot\\cdot\\leftarrow S_{1}\\leftarrow S_{0}\\rightarrow H$ ", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Figure 1: Graphical models of the forward and reverse processes in the diffusion model. The variable $H$ represents partial information about $S_{0}$ . ", "page_idx": 3}, {"type": "text", "text": "Forward process. In the forward process, a clean sample $s_{0}$ is diffused through a chain of latent variables $S_{1},...\\,S_{T}$ (Figure 1). We denote the probability measure under this process by $q$ and define its joint probability distribution as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{q(s_{1:T}\\mid s_{0})=\\prod_{t=1}^{T}q(s_{t}\\mid s_{t-1})\\,,\\quad\\forall t\\in[T]:q(s_{t}\\mid s_{t-1})=\\mathcal{N}(s_{t};\\sqrt{\\alpha_{t}}s_{t-1},\\beta_{t}I_{d})\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $q\\big(s_{t}\\mid s_{t-1}\\big)$ is the conditional density of mapping a less diffused $s_{t-1}$ to a more diffused $s_{t}$ . The diffusion rate is set by parameters $\\alpha_{t}\\in(0,1)$ and $\\beta_{t}=1-\\alpha_{t}$ . We also define $\\begin{array}{r}{\\bar{\\alpha}_{t}=\\prod_{\\ell=1}^{t}\\alpha_{\\ell}}\\end{array}$ . The forward process is sampled as follows. First, a clean sample $s_{0}$ is chosen. Then $S_{t}\\sim q(\\cdot\\mid s_{t-1})$ are sampled, starting from $t=1$ to $t=T$ . ", "page_idx": 3}, {"type": "text", "text": "Reverse process. In the reverse process, a clean sample $s_{0}$ is generated through a chain of variables $S_{T},\\dots,S_{0}$ (Figure 1). We denote the probability measure under this process by $p$ and define its joint probability distribution as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle p(s_{0:T})=p(s_{T})\\prod_{t=1}^{T}p(s_{t-1}\\mid s_{t})\\,,}}\\\\ {{\\displaystyle\\ p(s_{T})=\\mathcal{N}(s_{T};\\mathbf{0}_{d},I_{d})\\,,\\quad\\forall t\\in[T]:p(s_{t-1}\\mid s_{t})=\\mathcal{N}(s_{t-1};\\mu_{t}(s_{t}),\\Sigma_{t})\\,,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $p(s_{t-1}\\mid s_{t})$ is the conditional density of mapping a more diffused $s_{t}$ to a less diffused $s_{t-1}$ . The function $\\mu_{t}$ predicts the mean of $S_{t-1}\\mid s_{t}$ and is learned (Appendix B). As in $\\mathrm{Ho}$ et al. [18], we keep the covariance fixed at $\\Sigma_{t}=\\tilde{\\beta}_{t}I_{d}$ , where ${\\tilde{\\beta}}_{t}$ is defined in (14) in Appendix B. This is also known as a stable diffusion. We make this assumption only to simplify exposition. All our derivations in Section 4 hold when $\\Sigma_{t}$ is learned, for instance as in Bao et al. [8]. ", "page_idx": 3}, {"type": "text", "text": "This process is called reverse because it is learned by reversing the forward process. The reverse process is sampled as follows. First, a diffused sample $S_{T}\\sim p$ is chosen. After that, $S_{t-1}\\sim p(\\cdot\\mid s_{t})$ are sampled, starting from $t=T$ to $t=1$ . ", "page_idx": 3}, {"type": "text", "text": "4 Posterior Sampling ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "This section is organized as follows. In Section 4.1, we show how to sample from a chain of random variables conditioned on observations. In Sections 4.2 and 4.4, we specialize this to the observation models in Section 2. ", "page_idx": 3}, {"type": "text", "text": "4.1 Chain Model Posterior ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Let $h=\\{(\\phi_{\\ell},y_{\\ell})\\}_{\\ell\\in[N]}$ denote a history of $N$ observations (Section 2) and $H$ be the corresponding random variable. In this section, we assume that $h$ is fixed. The Markovian structure of the reverse process (Figure 1) implies that the joint probability distribution conditioned on $h$ factors as ", "page_idx": 3}, {"type": "equation", "text": "$$\np(s_{0:T}\\mid h)=p(s_{T}\\mid h)\\prod_{t=1}^{T}p(s_{t-1}\\mid s_{t},h)\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Therefore, $p(s_{0:T}\\mid h)$ can be sampled from efficiently by first sampling from $p(s_{T}\\mid h)$ and then from $T$ conditional distributions $p(s_{t-1}\\mid s_{t},h)$ . We derive these next. ", "page_idx": 3}, {"type": "text", "text": "Lemma 1. Let p be a probability measure over the reverse process (Figure 1). Then ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{p(s_{T}\\mid h)\\propto\\int_{s_{0}}p(h\\mid s_{0})\\,p(s_{0}\\mid s_{T})\\,\\mathrm{d}s_{0}\\,p(s_{T})\\,,\\qquad}\\\\ &{}&{\\forall t\\in[T]\\setminus\\{1\\}:p(s_{t-1}\\mid s_{t},h)\\propto\\int_{s_{0}}p(h\\mid s_{0})\\,p(s_{0}\\mid s_{t-1})\\,\\mathrm{d}s_{0}\\,p(s_{t-1}\\mid s_{t})\\,,}\\\\ &{}&{p(s_{0}\\mid s_{1},h)\\propto p(h\\mid s_{0})\\,p(s_{0}\\mid s_{1})\\,.\\qquad\\qquad\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Proof. The claim is proved in Appendix A.1. ", "page_idx": 3}, {"type": "text", "text": "1: Input: Diffusion model parameters $(\\mu_{t},\\Sigma_{t})_{t\\in[T]}$ , history of observations $h$   \n2: Initial sample $S_{T}\\sim\\mathcal{N}(\\hat{\\mu}_{T+1}(h),\\hat{\\Sigma}_{T+1}(h))$   \n3: for stage $t=T,\\dots,1$ do   \n4: $S_{t-1}\\sim\\mathcal{N}(\\hat{\\mu}_{t}(S_{t},h),\\hat{\\Sigma}_{t}(h))$   \n5: Output: Posterior sample $S_{0}$ ", "page_idx": 4}, {"type": "text", "text": "4.2 Linear Model Posterior ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Now we specialize Lemma 1 to the diffusion model prior (Section 3) and linear models (Section 2.1). The prior distribution is the reverse process in (5), ", "page_idx": 4}, {"type": "equation", "text": "$$\np(s_{T})=\\ensuremath{\\mathcal{N}}(s_{T};\\mathbf{0}_{d},I_{d})\\,,\\quad p(s_{t-1}\\mid s_{t})=\\ensuremath{\\mathcal{N}}(s_{t-1};\\mu_{t}(s_{t}),\\Sigma_{t})\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The term $p(h\\mid s_{0})$ is the likelihood of observations in Assumption 1. The main challenge in using the lemma are potentially complex conditional densities of clean samples $p(s_{0}\\mid S_{T})$ and $p\\big(s_{0}\\mid s_{t}\\big)$ . To get around this issue, we make an additional assumption and then discuss it in Section 4.3. ", "page_idx": 4}, {"type": "text", "text": "Theorem 2. Let p be a probability measure over the reverse process (Figure $I$ ). Let $\\bar{\\theta}$ and $\\bar{\\Sigma}^{-1}$ be defined as in (3). Suppose that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\int_{s_{0}}p(h\\mid s_{0})\\,p(s_{0}\\mid s_{t})\\,\\mathrm{d}s_{0}\\propto p(h\\mid s_{t}/\\sqrt{\\bar{\\alpha}_{t}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "holds for all $t\\in[T]$ . Then $p(s_{T}\\mid h)\\propto\\mathcal{N}(s_{T};\\hat{\\mu}_{T+1}(h),\\hat{\\Sigma}_{T+1}(h))$ , where ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\mu}_{T+1}(h)=\\hat{\\Sigma}_{T+1}(h)(I_{d}\\,{\\bf0}_{d}+\\bar{\\Sigma}^{-1}\\bar{\\theta}/\\sqrt{\\bar{\\alpha}_{T}})\\,,}\\\\ &{\\hat{\\Sigma}_{T+1}(h)=(I_{d}+\\bar{\\Sigma}^{-1}/\\bar{\\alpha}_{T})^{-1}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For $t\\in[T]$ , we have $p(s_{t-1}\\mid s_{t},h)\\propto\\mathcal{N}(s_{t-1};\\hat{\\mu}_{t}(s_{t},h),\\hat{\\Sigma}_{t}(h)),$ , where ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\mu}_{t}(s_{t},h)=\\hat{\\Sigma}_{t}(h)(\\Sigma_{t}^{-1}\\mu_{t}(s_{t})+\\bar{\\Sigma}^{-1}\\bar{\\theta}/\\sqrt{\\bar{\\alpha}_{t-1}})\\,,}\\\\ &{\\quad\\hat{\\Sigma}_{t}(h)=(\\Sigma_{t}^{-1}+\\bar{\\Sigma}^{-1}/\\bar{\\alpha}_{t-1})^{-1}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Proof. The proof is in Appendix A.2. It has four steps. First, we fix stage $t$ and apply approximation (6). Second, we rewrite the likelihood as in (3). Third, we reparameterize it as a function of $s_{t}$ . At the end, we combine the likelihood with the Gaussian prior using Lemma 6 in Appendix A.5. \u53e3 ", "page_idx": 4}, {"type": "text", "text": "The algorithm that samples from the posterior distribution in Theorem 2 is presented in Algorithm 2. We call it Laplace diffusion posterior sampling (LaplaceDPS) because its generalization to GLMs uses the Laplace approximation. LaplaceDPS samples from a chain of products of two distributions: one distribution represents the pre-trained diffusion model and does not depend on history $h$ , and the other represents the history $h$ . The sampling is implemented as follows. The initial variable $S_{T}$ is sampled conditioned on $h$ (line 2) from the distribution in (7). This distribution is a product of the $h$ -independen\u221at initial prior $\\mathcal{N}(\\mathbf{0}_{d},I_{d})$ and the $h$ -dependent distribution of the diffused evidence up to stage $T$ , $\\mathcal{N}(\\sqrt{\\bar{\\alpha}_{T}}\\bar{\\theta},\\bar{\\alpha}_{T}\\bar{\\Sigma})$ . All remaining variables, $S_{t-1}$ for $t\\in[T]$ , are sampled conditioned on $s_{t}$ and evidence $h$ (line 3) from the distribution in (8). This is again a product of the $h$ -independent conditional prior $\\mathcal{N}(\\mu_{t}(s_{t}),\\Sigma_{t})$ , from the\u221a pre-trained model, and the $h$ -dependent distribution of the diffused evidence up to stage $t-1$ , $\\mathcal{N}(\\dot{\\sqrt{\\alpha}}_{t-1}\\bar{\\theta},\\bar{\\alpha}_{t-1}\\bar{\\Sigma})$ . The final variable $S_{0}$ represents a clean sample. When compared to Section 2, the prior and evidence are mixed conditionally in a $T$ -stage chain. This increases computational cost, as discussed in Section 8. ", "page_idx": 4}, {"type": "text", "text": "4.3 Key Approximation in Theorem 2 ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Now we motivate our assumption in (6). Simply put, we assume that $s_{0}=s_{t}/\\sqrt{\\bar{\\alpha}_{t}}$ , where $s_{0}$ is a clean sample and $s_{t}$ is the corresponding d\u221aiffused sa\u221ample in stage $t$ . This is motivated by the forward process, which relates $s_{t}$ and $s_{0}$ as $s_{t}\\stackrel{=}{=}\\sqrt{\\bar{\\alpha}_{t}}s_{0}+\\sqrt{1-\\bar{\\alpha}_{t}}\\varepsilon_{t}$ , where $\\varepsilon_{t}\\sim\\mathcal{N}(\\mathbf{0}_{d},I_{d})$ is a standard Gaussian noise [18]. After rearranging, we get $s_{0}=\\left(s_{t}-\\sqrt{1-\\bar{\\alpha}_{t}}\\varepsilon_{t}\\right)\\big/\\sqrt{\\bar{\\alpha}_{t}}$ , and therefore $s_{0}$ can be viewed as a random variable with mean $s_{t}/\\sqrt{\\bar{\\alpha}_{t}}$ . The consequence of (6) is that the likelihood becomes a function of scaled $s_{t}$ , which yields a closed form when multiplied by the conditional prior, which is also a function of $s_{t}$ . Our approximation can be also viewed as the Tweedie\u2019s formula used in Chung et al. [12] where the score component is neglected. ", "page_idx": 4}, {"type": "table", "img_path": "7v0UyO0B6q/tmp/eb674e143ec8311d0961ee4e918573374be34418374bb2bac7d7b35bb6758579.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Our approximation has several notable properties. First, $\\sqrt{(1-\\bar{\\alpha}_{t})/\\bar{\\alpha}_{t}}\\to0$ as $t\\rightarrow1$ . Therefore, it becomes more precise in later stages of the reverse process. Second, in the absence of evidence $h$ , the approximation vanishes, and all posterior distributions in Theorem 2 reduce to the priors in (5). Finally, as the number of observations increases, sampling from the posterior in Theorem 2 is asymptotically consistent. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3. Fix $\\theta_{*}\\in\\mathbb{R}^{d}$ . Let $\\tilde{\\theta}\\gets\\mathrm{Lap1aceDPS}((\\mu_{t},\\Sigma_{t})_{t\\in[T]},h),$ , where $h=\\{(\\phi_{\\ell},y_{\\ell})\\}_{\\ell\\in[N]}$ is $a$ history of $N$ observations. Suppose that $\\lambda_{d}(\\bar{\\Sigma}^{-1})\\to\\infty$ as $N\\rightarrow\\infty$ , where $\\bar{\\Sigma}$ is defined in (3). Then $\\begin{array}{r}{\\mathbb{P}\\left(\\operatorname*{lim}_{N\\to\\infty}\\|\\tilde{\\theta}-\\theta_{*}\\|_{2}=0\\right)=1.}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "Proof. The proof is in Appendix A.3. The key idea is that the conditional posteriors in (7) and (8) concentrate at a scaled unknown model parameter $\\theta_{*}$ as the number of observations increases, which we formalize as $\\lambda_{d}(\\bar{\\Sigma}^{-1})\\to\\infty$ . \u53e3 ", "page_idx": 5}, {"type": "text", "text": "The bound in Theorem 3 can be interpreted as follows. The sampled parameter $\\tilde{\\theta}$ approaches the true unknown parameter $\\boldsymbol{\\theta}_{*}$ as the number of observations $N$ increases. To guarantee that the posterior shrinks uniformly in all directions, we assume that the number of observations in all directions grows linearly with $N$ . This is akin to assuming that $\\lambda_{d}(\\bar{\\Sigma}^{-1})=\\Omega(N)$ . This lower bound can be attained in linear models by getting observations according to the D-optimal design [38]. Since our proof is asymptotic, we can neglect some finite-time errors and this simplifies the argument. ", "page_idx": 5}, {"type": "text", "text": "4.4 GLM Posterior ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The Laplace approximation in GLMs (Section 2.2) naturally generalizes the exact posterior distribution in linear models (Section 2.1). We generalize Theorem 2 to GLMs along the same lines. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4. Let p be a probability measure over the reverse process (Figure 1). Suppose that (6) holds for all $t\\in[T]$ . Then $p(s_{T}\\mid h)\\propto\\mathcal{N}(s_{T};\\hat{\\mu}_{T+1}(h),\\hat{\\Sigma}_{T+1}(h))$ , where ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{\\mu}_{T+1}(h)=\\sqrt{\\bar{\\alpha}_{T}}\\dot{\\theta}_{T+1}\\,,\\quad\\hat{\\Sigma}_{T+1}(h)=\\bar{\\alpha}_{T}\\dot{\\Sigma}_{T+1}\\,,\\quad\\dot{\\theta}_{T+1},\\dot{\\Sigma}_{T+1}\\leftarrow\\mathrm{IRLS}(\\mathbf{0}_{d},I_{d}/\\bar{\\alpha}_{T},h)\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "For $t\\in[T]$ , we have $p(s_{t-1}\\mid s_{t},h)\\propto\\mathcal{N}(s_{t-1};\\hat{\\mu}_{t}(s_{t},h),\\hat{\\Sigma}_{t}(h)),$ , where ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\mu}_{t}(s_{t},h)=\\sqrt{\\bar{\\alpha}_{t-1}}\\dot{\\theta}_{t}\\,,\\quad\\hat{\\Sigma}_{t}(h)=\\bar{\\alpha}_{t-1}\\dot{\\Sigma}_{t}\\,,\\quad\\dot{\\theta}_{t},\\dot{\\Sigma}_{t}\\gets\\mathtt{I R L S}(\\mu_{t}(s_{t})/\\sqrt{\\bar{\\alpha}_{t-1}},\\Sigma_{t}/\\bar{\\alpha}_{t-1},h)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Proof. The proof is in Appendix A.4. It has four steps. First, we fix stage $t$ and apply\u221a approximation (6). Second, we reparameterize the prior, from a function of $s_{t}$ to a function of $s_{t}/\\sqrt{\\bar{\\alpha}_{t}}$ . Third, we combine the likelihood with the p\u221arior using the Laplace approximation. Finally, we repameterize the posterior, from a function of $s_{t}/\\bar{\\sqrt{\\alpha_{t}}}$ to a function of $s_{t}$ . \u53e3 ", "page_idx": 5}, {"type": "text", "text": "Similarly to Theorem 2, the distributions in Theorem 4 mix evidence with the diffusion model prior. However, this is done implicitly in IRLS. The posterior can be sampled from using LaplaceDPS, where the mean and covariances would be taken from Theorem 4. Note that Theorem 2 is a special case of Theorem 4 where the mean function $g$ is an identity. ", "page_idx": 5}, {"type": "text", "text": "5 Application to Contextual Bandits ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Now we apply our posterior sampling approximations (Section 4) to contextual bandits. A contextual bandit [28, 31] is a classic model for sequential decision making under uncertainty where the agent takes actions conditioned on context. We denote the action set by $\\boldsymbol{\\mathcal{A}}$ and the context set by $\\mathcal{X}$ . The mean reward for taking action $a\\in A$ in context $x\\in\\mathscr{X}$ is $r(x,a;\\theta_{*})$ , where $r:\\mathcal{X}\\times\\mathcal{A}\\times\\Theta\\rightarrow\\mathbb{R}$ denotes a reward function and $\\theta_{*}\\in\\Theta$ is a model parameter (Section 2). The agent interacts with the bandit for $n$ rounds indexed by $k\\in[n]$ . In round $k$ , the agent observes a context $x_{k}\\in\\mathcal{X}$ , takes an action $a_{k}\\in\\mathcal A$ , and observes a stochastic reward $y_{k}=r(x_{k},a_{k};\\theta_{*})+\\varepsilon_{k}$ , where $\\varepsilon_{k}$ is independent zero-mean $\\sigma^{2}$ -sub-Gaussian noise for $\\sigma>0$ . The goal of the agent is to maximize its cumulative reward in $n$ rounds, or equivalently to minimize its cumulative regret. We define the $n$ -round regret as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{R(n)=\\sum_{k=1}^{n}\\mathbb{E}\\left[r(x_{k},a_{k,*};\\theta_{*})-r(x_{k},a_{k};\\theta_{*})\\right]\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $a_{k,*}=\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}r(x_{k},a;\\theta_{*})$ is the optimal action in round $k$ . ", "page_idx": 6}, {"type": "text", "text": "Arguably the most popular method for solving contextual bandit problems is Thompson sampling [49, 11, 3]. The key idea in TS is to use the posterior distribution of $\\theta_{*}$ to explore. This is done as follows. In round $k$ , the model parameter is drawn from the posterior in (2), ${\\tilde{\\theta}}_{k}\\sim p(\\cdot\\mid h_{k})$ , where $h_{k}$ is the history of all interactions up to round $k$ . After that, the agent takes the action with the highest mean reward under $\\tilde{\\theta}_{k}$ . The pseudo-code of this algorithm is given in Algorithm 3. ", "page_idx": 6}, {"type": "text", "text": "A linear bandit [13, 1] is a contextual bandit with a linear reward function $r(x,a;\\theta_{*})=\\phi(x,a)^{\\top}\\theta_{*}$ , where $\\phi:\\mathcal{X}\\times\\mathcal{A}\\to\\mathbb{R}^{d}$ is a feature extractor. The feature extractor can be non-linear in $x$ and $a$ . Therefore, linear bandits can be applied to non-linear functions in $x$ and $a$ . The feature extractor can be hand-designed or learned [39]. To simplify notation, we let $\\phi_{\\ell}=\\phi(x_{\\ell},a_{\\ell})$ be the feature vector of the action in round $k$ . So the history of interactions up to round $k$ is $\\begin{array}{r}{\\dot{h}_{k}=\\big\\{(\\phi_{\\ell},y_{\\ell})\\big\\}_{\\ell\\in[k-1]}}\\end{array}$ . When the prior distribution is a Gaussian, $p(\\theta_{*})=\\mathcal{N}(\\theta_{*};\\theta_{0},\\Sigma_{0})$ , the posterior in round $k$ is a Gaussian in (3) for $h=h_{k}$ . When the prior is a diffusion model, we propose sampling as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{\\theta}_{k}\\gets\\tt L a p l a c e D P S((\\mu_{t},\\boldsymbol{\\Sigma}_{t})_{t\\in[T]},h_{k})\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\hat{\\mu}_{t}$ and $\\hat{\\Sigma}_{t}$ in LaplaceDPS are set according to Theorem 2. We call this algorithm DiffTS. ", "page_idx": 6}, {"type": "text", "text": "A generalized linear bandit [16, 23, 32, 25] is an extension of linear bandits to generalized linear models (Section 2.2). When $p(\\theta_{\\ast})=\\mathcal{N}(\\theta_{\\ast};\\theta_{0},\\Sigma_{0})$ , the Laplace approximation to the posterior is a Gaussian (Section 2.2). When the prior is a diffusion model, we propose sampling from (10), where $\\hat{\\mu}_{t}$ and $\\hat{\\Sigma}_{t}$ in LaplaceDPS are set according to Theorem 4. ", "page_idx": 6}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We conduct three experiments: synthetic problems in 2 dimensions (Section 6.2 and Appendix C.1), a recommender system (Section 6.3), and a classification problem (Appendix C.2). In addition, we conduct an ablation study in Appendix C.3, where we vary the number of training samples for the diffusion prior and the number of diffusion stages $T$ . ", "page_idx": 6}, {"type": "text", "text": "6.1 Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We have four baselines. Three baselines are variants of contextual Thompson sampling [11, 3]: with an uninformative Gaussian prior (TS), a learned Gaussian prior (TunedTS), and a learned Gaussian mixture prior (MixTS) [21]. The last baseline is diffusion posterior sampling (DPS) of Chung et al. [12]. We implement all TS baselines as described in Section 5. The uninformative prior is $\\mathcal{N}(\\bar{\\mathbf{O}_{d}},I_{d})$ . MixTS is used only in linear bandit experiments because the logistic regression variant does not exist. The TS baselines are chosen to cover various levels of prior information. Our implementation of DPS is described in Appendix D. We also experimented with frequentist baselines, such as LinUCB [1] and the $\\varepsilon$ -greedy policy. They performed worse than TS, and therefore we do not report them here. ", "page_idx": 6}, {"type": "text", "text": "Each experiment is set up as follows. First, the prior distribution of $\\theta_{*}$ is specified: it can be synthetic or estimated from real-world data. Second, we learn this distribution from $10\\,000$ samples from it. In DiffTS and DPS, we follow Appendix B. The number of stages is $T=100$ and the diffusion factor is $\\alpha_{t}=0.97$ . Since $0.97^{100}\\approx\\!\\dot{0}.05$ , most of the information in the training samples is diffused. The regressor in Appendix B is a 2-layer neural network with ReLU activations. In TunedTS, we fit the mean and covariance using maximum likelihood estimation. In MixTS, we fit the Gaussian mixture using SCIKIT-LEARN. All algorithms are evaluated on $\\theta_{*}$ sampled from the true prior. The regret is computed as defined in (9). All error bars are standard errors of the estimates. ", "page_idx": 6}, {"type": "image", "img_path": "7v0UyO0B6q/tmp/16c7d0f0345117d68b06bf2d9c3b0b30076c364c47ba7a5e080dce7530055d6d.jpg", "img_caption": ["Figure 2: Evaluation of DiffTS on three synthetic problems. The first row shows samples from the true (blue) and diffusion model (red) priors. The second row shows the regret of DiffTS and the baselines as a function of round $n$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "6.2 Synthetic Experiment ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The first experiment is on three synthetic problems. Each problem is a linear bandit (Section 5) with $K=100$ actions in $d=2$ dimensions. The reward noise is $\\sigma=1$ . The feature vectors of actions are sampled uniformly at random from a unit ball. The prior distributions of $\\boldsymbol{\\theta}_{*}$ are shown in Figure 2. The first is a mixture of two Gaussians and the last can be approximated well by a mixture of two Gaussians. We implement MixTS with two mixture components. Therefore, it can represent the first prior exactly and approximate the last one well. ", "page_idx": 7}, {"type": "text", "text": "Our results are reported in Figure 2. We observe two main trends. First, samples from the diffusion prior closely resemble those from the true prior. In such cases, DiffTS is expected to perform well and even outperforms MixTS, because it has a better representation of the prior. We observe this in all problems. Second, DPS diverges as the number of rounds increases. This is because DPS relies on the likelihood score (Section 7), which tends to infinity and causes instability. This happens despite tuning (Appendix D). We report results on additional synthetic problems in Appendix C.1. ", "page_idx": 7}, {"type": "text", "text": "DiffTS should be $T$ times more computationally costly than TS with a Gaussian prior (Section 4.2). We observe this empirically. As an example, the average cost of 100 runs of DiffTS on any problem in Figure 2 is 12 seconds. The average cost of TS is 0.1 seconds. The computation and accuracy can be traded off, and we investigate this in Appendix C.3. In the cross problem, we vary the number of diffusion stages from $T=1$ to $T=300$ . We observe that the computational cost is linear in $T$ and the regret drops quickly from around 85 at $T=1$ to 50 at $T=25$ . ", "page_idx": 7}, {"type": "text", "text": "6.3 MovieLens Experiment ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In the second experiment, we learn to recommend an item to randomly arriving users. The problem is simulated using the MovieLens 1M dataset [27], with one million ratings for 3 706 movies from $6\\,040$ users. We subtract the mean rating from all ratings and complete the sparse rating matrix $M$ by alternating least squares [14] with rank $d=5$ . The learned factorization is $\\bar{M}=U V^{\\top}$ . The $i$ -th row of $U$ , denoted by $U_{i}$ , represents user $i$ . The $j$ -th row of $V$ , denoted by $V_{j}$ , represents movie $j$ . We ", "page_idx": 7}, {"type": "image", "img_path": "7v0UyO0B6q/tmp/a0ef9fecfe2e69b06f39937dce74bea34eecb515099d0b52a743d200616b240e.jpg", "img_caption": ["Figure 3: Evaluation of DiffTS on the MovieLens dataset. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "use movie embeddings $V_{j}$ as model parameters and user embeddings $U_{i}$ as features of the actions.   \nThe movies are items. ", "page_idx": 8}, {"type": "text", "text": "We experiment with both linear and logistic bandits. In both, an item is initially chosen randomly from $V_{j}$ and $K=10$ actions are chosen randomly from $U_{i}$ in each round. In the linear bandit, the mean reward of item $j$ for user $i$ is $U_{i}^{\\top}V_{j}$ . The reward noise is $\\sigma=0.75$ , and we estimate it from data. In the logistic bandit, the mean reward is $g(U_{i}^{\\top}V_{j})$ , where $g$ is a sigmoid. ", "page_idx": 8}, {"type": "text", "text": "Our MovieLens results are reported in Figure 3 and we observe similar trends to Section 6.2. First, samples from the diffusion prior closely resemble those from the true prior (Figure 3a). Since the problem is higher dimensional, we visualize the overlap using a UMAP projection [44]. Second, DiffTS has a significantly lower regret than all baselines, in both linear (Figure 3b) and logistic (Figure 3c) bandits. Finally, MixTS barely outperforms TunedTS. We observe this trend consistently in higher dimensions, which motivated our work on online learning with more complex priors. ", "page_idx": 8}, {"type": "text", "text": "7 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We start with reviewing related works on bandits with diffusion models. Hsieh et al. [22] proposed Thompson sampling with a diffusion model prior for $K$ -armed bandits. There are multiple technical differences from our work. First, the diffusion model in Hsieh et al. [22] is over scalars representing individual arms. Our model is over vectors representing model parameters; and thus can be applied to contextual bandits. Second, the approximations are different. In stage $t$ , Hsieh et al. [22] sample from the conditional prior and the diffused empirical mean distribution in stage $t$ . Then they take a weighted average of the samples. We sample only once, from the posterior distribution that combines the conditional prior in stage $t$ and likelihood. Because of this, Hsieh et al. [22] can be viewed as a non-contextual variant of our method, where posterior sampling is done by weighting samples from the prior and empirical distributions. Finally, Hsieh et al. [22] do not analyze their approximation. ", "page_idx": 8}, {"type": "text", "text": "Aouali [4] proposed and analyzed contextual bandits with a linear diffusion model prior: $\\mu_{t}{\\left(s_{t}\\right)}$ in (5) is linear in $s_{t}$ and $q(s_{0})$ is a Gaussian. Because of this, their model is a linear Gaussian model and not a general diffusion model, as in our work. ", "page_idx": 8}, {"type": "text", "text": "The closest related work on posterior sampling in diffusion models is DPS of Chung et al. [12]. The key idea in DPS is to sample from the posterior distribution using the likelihood score $\\nabla\\log{p(h\\mid\\theta)}$ , where $p(h\\mid\\theta)$ is the likelihood (Assumptions 1 and 2). Note that $\\nabla\\log p(h\\mid\\theta)$ grows linearly in $N$ because the history $h$ in $p(h\\mid\\theta)$ involves $N$ terms. Therefore, DPS becomes unstable as $N\\rightarrow\\infty$ . See our empirical results in Section 6.2 and the implementation of DPS in Appendix D, which was tuned to improve its stability. ", "page_idx": 8}, {"type": "text", "text": "Many other posterior sampling methods for diffusion models have been proposed recently: a sequential Monte Carlo approximation for the conditional reverse process [52], a variant of DPS with an uninformative prior [37], a pseudo-inverse approximation to the likelihood of evidence [47], and posterior sampling in latent diffusion models [40]. All of these methods rely on the likelihood score ${\\bar{\\nabla}}\\log p(h\\mid\\theta)$ and thus become unstable as the number of observations $N$ increases. Our posterior approximations do not have this issue because they are based on the product of prior and evidence distributions (Theorems 2 and 4), and thus gradient-free. They work well across different levels of uncertainty (Section 6) and do not require tuning. ", "page_idx": 8}, {"type": "text", "text": "We also wanted to note that posterior sampling is a special from of guiding generation in diffusion models. Other approaches include conditional pre-training [15], a constraint in the reverse process [17], refining the null-space content [50], solving an optimization problem that pushes the reverse process towards evidence [46], and aligning the reverse process with the prompt [7]. ", "page_idx": 9}, {"type": "text", "text": "8 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We propose posterior sampling approximations for diffusion models priors. These approximations are contextual, and can be implemented efficiently in linear models and GLMs. We analyze them and evaluate them empirically on contextual bandit problems. Our method has two main limitations. ", "page_idx": 9}, {"type": "text", "text": "Computational cost. The cost of posterior sampling in LaplaceDPS with $T$ stages is about $T$ times higher than that of posterior sampling with a Gaussian prior (Section 2). We validate it empirically in Section 6.2. We plot the sampling time as a function of $T$ in Figure 6c (Appendix C.3). ", "page_idx": 9}, {"type": "text", "text": "Learning cost and hyper-parameter tuning. In all experiments, the number of diffusion stages is $T=100$ and the diffusion rate is set such that most of the signal diffuses. The regressor is a 2-layer neural network and we learn it from $10\\,000$ samples from the prior. These settings resulted in stable performance in all our experiments (Section 6). However, they clearly impact the performance. We plot the regret as a function of the number of training samples in Figure 6a and as a function of $T$ in Figure 6b. When $T$ or the number of training samples is small, DiffTS performs very similarly to posterior sampling with a Gaussian prior. In summary, there is no benefit in these cases. ", "page_idx": 9}, {"type": "text", "text": "Future work. We develop novel posterior approximations rather than bounding their regret. This is because the existing approximations are unstable and may diverge in the online setting (Sections 6.2 and 7). We believe that a proper regret analysis of DiffTS is possible and would require bounding two errors. The first error arises because the reverse process does not reverses the forward process exactly (Appendix B). The second error arises because our posterior distributions are approximate (Section 4.3). One possibility is to start with prior works that already showed the utility of complex priors. For instance, Russo and Van Roy [42] proved a $O(\\sqrt{\\Gamma H(A_{*})n})$ regret bound for a linear bandit, where $\\Gamma$ is the maximum ratio of regret to information gain and $\\dot{H}(A_{*})$ is the entropy of the distribution of the optimal action under the prior. This bound holds for any prior, and says that a lower entropy $H(A_{*})$ , which reflects how concentrated the prior is, yields a lower regret. ", "page_idx": 9}, {"type": "text", "text": "We also believe that our ideas can be extended beyond GLMs. The key idea in Section 4.4 is to use the Laplace approximation of the likelihood. This approximation can be computed exactly in GLMs. More generally though, it is a good approximation whenever the likelihood can be approximated well by a single Gaussian distribution. By the central limit theorem, under appropriate conditions, this is expected for any observation model when the number of observations is large. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic bandits. In Advances in Neural Information Processing Systems 24, pages 2312\u20132320, 2011.   \n[2] Marc Abeille and Alessandro Lazaric. Linear Thompson sampling revisited. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, 2017.   \n[3] Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs. In Proceedings of the 30th International Conference on Machine Learning, pages 127\u2013135, 2013.   \n[4] Imad Aouali. Linear diffusion models meet contextual bandits with large action spaces. In NeurIPS 2023 Workshop on Foundation Models for Decision Making Workshop, 2023.   \n[5] Imad Aouali, Branislav Kveton, and Sumeet Katariya. Mixed-effect Thompson sampling. In Proceedings of the 26th International Conference on Artificial Intelligence and Statistics, 2023.   \n[6] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. Machine Learning, 47:235\u2013256, 2002.   \n[7] Arpit Bansal, Hong-Min Chu, Avi Schwarzschild, Soumyadip Sengupta, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Universal guidance for diffusion models. In Proceedings of the 12th International Conference on Learning Representations, 2024. [8] Fan Bao, Chongxuan Li, Jiacheng Sun, Jun Zhu, and Bo Zhang. Estimating the optimal covariance with imperfect mean in diffusion probabilistic models. In Proceedings of the 39th International Conference on Machine Learning, 2022.   \n[9] Soumya Basu, Branislav Kveton, Manzil Zaheer, and Csaba Szepesvari. No regrets for learning the prior in bandits. In Advances in Neural Information Processing Systems 34, 2021.   \n[10] Christopher Bishop. Pattern Recognition and Machine Learning. Springer, New York, NY, 2006.   \n[11] Olivier Chapelle and Lihong Li. An empirical evaluation of Thompson sampling. In Advances in Neural Information Processing Systems 24, pages 2249\u20132257, 2011.   \n[12] Hyungjin Chung, Jeongsol Kim, Michael Thompson Mccann, Marc Louis Klasky, and Jong Chul Ye. Diffusion posterior sampling for general noisy inverse problems. In Proceedings of the 11th International Conference on Learning Representations, 2023.   \n[13] Varsha Dani, Thomas Hayes, and Sham Kakade. Stochastic linear optimization under bandit feedback. In Proceedings of the 21st Annual Conference on Learning Theory, pages 355\u2013366, 2008.   \n[14] Mark Davenport and Justin Romberg. An overview of low-rank matrix recovery from incomplete observations. IEEE Journal of Selected Topics in Signal Processing, 10(4):608\u2013622, 2016.   \n[15] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In Advances in Neural Information Processing Systems 34, 2021.   \n[16] Sarah Filippi, Olivier Cappe, Aurelien Garivier, and Csaba Szepesvari. Parametric bandits: The generalized linear case. In Advances in Neural Information Processing Systems 23, pages 586\u2013594, 2010.   \n[17] Alexandros Graikos, Nikolay Malkin, Nebojsa Jojic, and Dimitris Samaras. Diffusion models as plug-and-play priors. In Advances in Neural Information Processing Systems 35, 2022.   \n[18] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems 33, 2020.   \n[19] Joey Hong, Branislav Kveton, Sumeet Katariya, Manzil Zaheer, and Mohammad Ghavamzadeh. Deep hierarchy in bandits. In Proceedings of the 39th International Conference on Machine Learning, 2022.   \n[20] Joey Hong, Branislav Kveton, Manzil Zaheer, and Mohammad Ghavamzadeh. Hierarchical Bayesian bandits. In Proceedings of the 25th International Conference on Artificial Intelligence and Statistics, 2022.   \n[21] Joey Hong, Branislav Kveton, Manzil Zaheer, Mohammad Ghavamzadeh, and Craig Boutilier. Thompson sampling with a mixture prior. In Proceedings of the 25th International Conference on Artificial Intelligence and Statistics, 2022.   \n[22] Yu-Guan Hsieh, Shiva Kasiviswanathan, Branislav Kveton, and Patrick Blobaum. Thompson sampling with diffusion generative prior. In Proceedings of the 40th International Conference on Machine Learning, 2023.   \n[23] Kwang-Sung Jun, Aniruddha Bhargava, Robert Nowak, and Rebecca Willett. Scalable generalized linear bandits: Online computation and hashing. In Advances in Neural Information Processing Systems 30, pages 98\u2013108, 2017.   \n[24] Jaya Kawale, Hung Bui, Branislav Kveton, Long Tran-Thanh, and Sanjay Chawla. Efficient Thompson sampling for online matrix-factorization recommendation. In Advances in Neural Information Processing Systems 28, pages 1297\u20131305, 2015.   \n[25] Branislav Kveton, Manzil Zaheer, Csaba Szepesvari, Lihong Li, Mohammad Ghavamzadeh, and Craig Boutilier. Randomized exploration in generalized linear bandits. In Proceedings of the 23rd International Conference on Artificial Intelligence and Statistics, 2020.   \n[26] Tze Leung Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Advances in Applied Mathematics, 6(1):4\u201322, 1985.   \n[27] Shyong Lam and Jon Herlocker. MovieLens Dataset. http://grouplens.org/datasets/movielens/, 2016.   \n[28] John Langford and Tong Zhang. The epoch-greedy algorithm for contextual multi-armed bandits. In Advances in Neural Information Processing Systems 20, pages 817\u2013824, 2008. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "[29] Tor Lattimore and Csaba Szepesvari. Bandit Algorithms. Cambridge University Press, 2019. ", "page_idx": 11}, {"type": "text", "text": "[30] Yann LeCun, Corinna Cortes, and Christopher Burges. MNIST Handwritten Digit Database. http://yann.lecun.com/exdb/mnist, 2010. ", "page_idx": 11}, {"type": "text", "text": "[31] Lihong Li, Wei Chu, John Langford, and Robert Schapire. A contextual-bandit approach to personalized news article recommendation. In Proceedings of the 19th International Conference on World Wide Web, 2010. ", "page_idx": 11}, {"type": "text", "text": "[32] Lihong Li, Yu Lu, and Dengyong Zhou. Provably optimal algorithms for generalized linear contextual bandits. In Proceedings of the 34th International Conference on Machine Learning, pages 2071\u20132080, 2017. ", "page_idx": 11}, {"type": "text", "text": "[33] Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyperband: A novel bandit-based approach to hyperparameter optimization. Journal of Machine Learning Research, 18(185):1\u201352, 2018. ", "page_idx": 11}, {"type": "text", "text": "[34] Shuai Li, Alexandros Karatzoglou, and Claudio Gentile. Collaborative filtering bandits. In Proceedings of the 39th Annual International ACM SIGIR Conference, 2016. ", "page_idx": 11}, {"type": "text", "text": "[35] Xiuyuan Lu and Benjamin Van Roy. Information-theoretic confidence bounds for reinforcement learning. In Advances in Neural Information Processing Systems 32, 2019. ", "page_idx": 11}, {"type": "text", "text": "[36] P. McCullagh and J. A. Nelder. Generalized Linear Models. Chapman & Hall, 1989. ", "page_idx": 11}, {"type": "text", "text": "[37] Xiangming Meng and Yoshiyuki Kabashima. Diffusion model based posterior sampling for noisy linear inverse problems. CoRR, abs/2211.12343, 2023. URL https://arxiv.org/ abs/2211.12343. ", "page_idx": 11}, {"type": "text", "text": "[38] Friedrich Pukelsheim. Optimal Design of Experiments. Society for Industrial and Applied Mathematics, 2006. ", "page_idx": 11}, {"type": "text", "text": "[39] Carlos Riquelme, George Tucker, and Jasper Snoek. Deep Bayesian bandits showdown: An empirical comparison of Bayesian deep networks for Thompson sampling. In Proceedings of the 6th International Conference on Learning Representations, 2018. ", "page_idx": 11}, {"type": "text", "text": "[40] Litu Rout, Negin Raoof, Giannis Daras, Constantine Caramanis, Alex Dimakis, and Sanjay Shakkottai. Solving linear inverse problems provably via posterior sampling with latent diffusion models. In Advances in Neural Information Processing Systems 36, 2023. ", "page_idx": 11}, {"type": "text", "text": "[41] Daniel Russo and Benjamin Van Roy. Learning to optimize via posterior sampling. Mathematics of Operations Research, 39(4):1221\u20131243, 2014. ", "page_idx": 11}, {"type": "text", "text": "[42] Daniel Russo and Benjamin Van Roy. An information-theoretic analysis of Thompson sampling. Journal of Machine Learning Research, 17(68):1\u201330, 2016. ", "page_idx": 11}, {"type": "text", "text": "[43] Daniel Russo, Benjamin Van Roy, Abbas Kazerouni, Ian Osband, and Zheng Wen. A tutorial on Thompson sampling. Foundations and Trends in Machine Learning, 11(1):1\u201396, 2018. ", "page_idx": 11}, {"type": "text", "text": "[44] Tim Sainburg, Leland McInnes, and Timothy Gentner. Parametric UMAP embeddings for representation and semisupervised learning. Neural Computation, 33(11):2881\u20132907, 2021. ", "page_idx": 11}, {"type": "text", "text": "[45] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In Proceedings of the 32nd International Conference on Machine Learning, pages 2256\u20132265, 2015. ", "page_idx": 11}, {"type": "text", "text": "[46] Bowen Song, Soo Min Kwon, Zecheng Zhang, Xinyu Hu, Qing Qu, and Liyue Shen. Solving inverse problems with latent diffusion models via hard data consistency. In Proceedings of the 12th International Conference on Learning Representations, 2024. ", "page_idx": 11}, {"type": "text", "text": "[47] Jiaming Song, Arash Vahdat, Morteza Mardani, and Jan Kautz. Pseudoinverse-guided diffusion models for inverse problems. In Proceedings of the 11th International Conference on Learning Representations, 2023. ", "page_idx": 11}, {"type": "text", "text": "[48] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In Proceedings of the 9th International Conference on Learning Representations, 2021. ", "page_idx": 11}, {"type": "text", "text": "[49] William R. Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3-4):285\u2013294, 1933. ", "page_idx": 11}, {"type": "text", "text": "[50] Yinhuai Wang, Jiwen Yu, and Jian Zhang. Zero-shot image restoration using denoising diffusion null-space model. In Proceedings of the 11th International Conference on Learning Representations, 2023.   \n[51] R. Wolke and H. Schwetlick. Iteratively reweighted least squares: Algorithms, convergence analysis, and numerical comparisons. SIAM Journal on Scientific and Statistical Computing, 9 (5):907\u2013921, 1988.   \n[52] Luhuan Wu, Brian Trippe, Christian Naesseth, David Blei, and John Cunningham. Practical and asymptotically exact conditional sampling in diffusion models. In Advances in Neural Information Processing Systems 36, 2023.   \n[53] Xiaoxue Zhao, Weinan Zhang, and Jun Wang. Interactive collaborative flitering. In Proceedings of the 22nd ACM International Conference on Information and Knowledge Management, pages 1411\u20131420, 2013. ", "page_idx": 12}, {"type": "text", "text": "A Proofs and Supporting Lemmas ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "This section contains proofs of our main claims and supporting lemmas. ", "page_idx": 13}, {"type": "text", "text": "A.1 Proof of Lemma 1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "All derivations are based on basic rules of probability and the chain structure in Figure 1, and are exact. From Figure 1, the joint probability distribution conditioned on $H=h$ factors as ", "page_idx": 13}, {"type": "equation", "text": "$$\np(s_{0:T}\\mid h)=p(s_{T}\\mid h)\\prod_{t=1}^{T}p(s_{t-1}\\mid s_{t:T},h)=p(s_{T}\\mid h)\\prod_{t=1}^{T}p(s_{t-1}\\mid s_{t},h)\\,.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We use that $p(s_{t-1}\\mid s_{t:T},h)=p(s_{t-1}\\mid s_{t},h)$ in the last equality. We consider two cases. ", "page_idx": 13}, {"type": "text", "text": "Derivation of $p(s_{t-1}\\mid s_{t},h)$ . By Bayes\u2019 rule, we get ", "page_idx": 13}, {"type": "equation", "text": "$$\np(s_{t-1}\\mid s_{t},h)={\\frac{p(h\\mid s_{t-1},s_{t})\\,p(s_{t-1}\\mid s_{t})}{p(h\\mid s_{t})}}\\propto p(h\\mid s_{t-1})\\,p(s_{t-1}\\mid s_{t})\\,.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "In the last step, we use that $p(h\\mid s_{t})$ is a constant, since $s_{t}$ and $h$ are fixed, and that $p(h\\mid s_{t-1},s_{t})=$ $p(h\\mid s_{t-1})$ . Note that the last term $p(s_{t-1}\\mid s_{t})$ is the conditional prior distribution. Let $t>1$ . Then we rewrite the first term as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle p(h\\mid s_{t-1})=\\int_{s_{0}}p(h,s_{0}\\mid s_{t-1})\\,\\mathrm{d}s_{0}=\\int_{s_{0}}p(h\\mid s_{0},s_{t-1})\\,p(s_{0}\\mid s_{t-1})\\,\\mathrm{d}s_{0}}\\\\ {\\displaystyle=\\int_{s_{0}}p(h\\mid s_{0})\\,p(s_{0}\\mid s_{t-1})\\,\\mathrm{d}s_{0}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "In the last equality, we use that our graphical model is a chain (Figure 1), and thus $p(h\\mid s_{0},s_{t-1})=$ $p(h\\mid s_{0})$ . Finally, we chain all identities and get that ", "page_idx": 13}, {"type": "equation", "text": "$$\np(s_{t-1}\\mid s_{t},h)\\propto\\int_{s_{0}}p(h\\mid s_{0})\\,p(s_{0}\\mid s_{t-1})\\,\\mathrm{d}s_{0}\\,p(s_{t-1}\\mid s_{t})\\,.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Derivation of $p(s_{T}\\mid h)$ . By Bayes\u2019 rule, we get ", "page_idx": 13}, {"type": "equation", "text": "$$\np(s_{T}\\mid h)={\\frac{p(h\\mid s_{T})\\,p(s_{T})}{p(h)}}\\propto p(h\\mid s_{T})\\,p(s_{T})\\,.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "In the last step, we use that $p(h)$ is a constant, since $h$ is fixed. The first term can be rewritten as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle p(h\\mid s_{T})=\\int_{s_{0}}p(h,s_{0}\\mid s_{T})\\,\\mathrm{d}s_{0}=\\int_{s_{0}}p(h\\mid s_{0},s_{T})\\,p(s_{0}\\mid s_{T})\\,\\mathrm{d}s_{0}}\\\\ {\\displaystyle=\\int_{s_{0}}p(h\\mid s_{0})\\,p(s_{0}\\mid s_{T})\\,\\mathrm{d}s_{0}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Finally, we chain all identities and get that ", "page_idx": 13}, {"type": "equation", "text": "$$\np(s_{T}\\mid h)\\propto\\int_{s_{0}}p(h\\mid s_{0})\\,p(s_{0}\\mid s_{T})\\,\\mathrm{d}s_{0}\\,p(s_{T})\\,.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "This completes the derivations. ", "page_idx": 13}, {"type": "text", "text": "A.2 Proof of Theorem 2 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "This proof has two parts. ", "page_idx": 13}, {"type": "text", "text": "Derivation of $p(s_{t-1}\\mid s_{t},h)$ . From $p(h\\mid s_{0})\\propto\\mathcal{N}(s_{0};\\bar{\\theta},\\bar{\\Sigma})$ and (6), it immediately follows that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\int_{s_{0}}p(h\\mid s_{0})\\,p(s_{0}\\mid s_{t-1})\\,\\mathrm{d}s_{0}\\propto\\mathcal{N}(s_{t-1}/\\sqrt{\\bar{\\alpha}_{t-1}};\\bar{\\theta},\\bar{\\Sigma})\\propto\\mathcal{N}(s_{t-1};\\sqrt{\\bar{\\alpha}_{t-1}}\\bar{\\theta},\\bar{\\alpha}_{t-1}\\bar{\\Sigma})\\,.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The last step treats $\\bar{\\alpha}_{t-1}$ and $\\bar{\\Sigma}$ as constants, because the forward process, $t$ , and evidence are fixed. Now we apply Lemma 6 to distributions ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p(s_{t-1}\\mid s_{t})=\\mathcal{N}(s_{t-1};\\mu_{t}(s_{t}),\\Sigma_{t})\\,,\\quad\\mathcal{N}(s_{t-1};\\sqrt{\\bar{\\alpha}_{t-1}}\\bar{\\theta},\\bar{\\alpha}_{t-1}\\bar{\\Sigma})\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and get that ", "page_idx": 14}, {"type": "equation", "text": "$$\np(s_{t-1}\\mid s_{t},h)\\propto\\mathcal{N}(s_{t-1};\\hat{\\mu}_{t}(s_{t},h),\\hat{\\Sigma}_{t}(h))\\,,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\hat{\\mu}_{t}(s_{t},h)$ and $\\hat{\\Sigma}_{t}(h)$ are defined in the claim. This is a product of two Gaussians: the prior with mean $\\mu_{t}{\\left(s_{t}\\right)}$ and covariance $\\Sigma_{t}$ , and the evidence with mean $\\bar{\\theta}$ and covariance $\\bar{\\Sigma}$ . ", "page_idx": 14}, {"type": "text", "text": "Derivation of $p(s_{T}\\mid h)$ . Analogously to the derivation of $p(s_{t-1}\\mid s_{t},h)$ , we establish that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\int_{s_{0}}p(h\\mid s_{0})\\,p(s_{0}\\mid s_{T})\\,\\mathrm{d}s_{0}\\propto\\mathcal{N}(s_{T};\\sqrt{\\bar{\\alpha}_{T}}\\bar{\\theta},\\bar{\\alpha}_{T}\\bar{\\Sigma})\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then we apply Lemma 6 to distributions ", "page_idx": 14}, {"type": "equation", "text": "$$\np(s_{T})=\\mathcal{N}(s_{T};\\mathbf{0}_{d},I_{d})\\,,\\quad\\mathcal{N}(s_{T};\\sqrt{\\bar{\\alpha}_{T}}\\bar{\\theta},\\bar{\\alpha}_{T}\\bar{\\Sigma})\\,,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and get that ", "page_idx": 14}, {"type": "equation", "text": "$$\np(s_{T}\\mid h)\\propto\\mathcal{N}(s_{T};\\hat{\\mu}_{T+1}(h),\\hat{\\Sigma}_{T+1}(h))\\,,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\hat{\\mu}_{T+1}(h)$ and $\\hat{\\Sigma}_{T+1}(h)$ are defined in the claim. This is a product of two Gaussians: the prior with mean $\\mathbf{0}_{d}$ and covariance $I_{d}$ , and the evidence with mean $\\bar{\\theta}$ and covariance $\\bar{\\Sigma}$ . ", "page_idx": 14}, {"type": "text", "text": "A.3 Proof of Theorem 3 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We start with the triangle inequality ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\tilde{\\theta}-\\theta_{*}\\|_{2}=\\|\\tilde{\\theta}-\\bar{\\theta}+\\bar{\\theta}-\\theta_{*}\\|_{2}\\le\\|\\tilde{\\theta}-\\bar{\\theta}\\|_{2}+\\|\\bar{\\theta}-\\theta_{*}\\|_{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where we introduce $\\bar{\\theta}$ from Section 2.1. Now we bound each term on the right-hand side. ", "page_idx": 14}, {"type": "text", "text": "Upper bound on $\\lVert\\boldsymbol{\\tilde{\\theta}}-\\boldsymbol{\\bar{\\theta}}\\rVert_{2}$ . This part of the proof is based on analyzing the asymptotic behavior of the conditional densities in Theorem 2. ", "page_idx": 14}, {"type": "text", "text": "As a first step, note that $S_{T}\\sim\\mathcal{N}(\\hat{\\mu}_{T+1}(h),\\hat{\\Sigma}_{T+1}(h))$ , where ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\hat{\\mu}_{T+1}(h)=\\hat{\\Sigma}_{T+1}(h)(I_{d}\\,{\\bf0}_{d}+\\bar{\\Sigma}^{-1}\\bar{\\theta}/\\sqrt{\\bar{\\alpha}_{T}})\\,,\\quad\\hat{\\Sigma}_{T+1}(h)=(I_{d}+\\bar{\\Sigma}^{-1}/\\bar{\\alpha}_{T})^{-1}\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Since $\\lambda_{d}(\\bar{\\Sigma}^{-1})\\to\\infty$ , we get ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\hat{\\Sigma}_{T+1}(h)\\to\\bar{\\alpha}_{T}\\bar{\\Sigma}\\,,\\quad\\hat{\\mu}_{T+1}(h)\\to\\sqrt{\\bar{\\alpha}_{T}}\\bar{\\theta}\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Moreover, $\\lambda_{d}(\\bar{\\Sigma}^{-1})\\to\\infty$ implies $\\lambda_{1}(\\bar{\\Sigma})\\rightarrow0$ , and thus $\\begin{array}{r}{\\operatorname*{lim}_{N\\to\\infty}\\|S_{T}-\\sqrt{\\bar{\\alpha}_{T}}\\bar{\\theta}\\|_{2}=0.}\\end{array}$ . ", "page_idx": 14}, {"type": "text", "text": "The same argument can be applied inductively to later stages of the reverse process. Specifically, for any $t\\in[T],S_{t-1}\\sim\\mathcal{N}(\\hat{\\mu}_{t}(S_{t},h),\\hat{\\Sigma}_{t}(h))$ , where ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\mu}_{t}(S_{t},h)=\\hat{\\Sigma}_{t}(h)(\\Sigma_{t}^{-1}\\mu_{t}(S_{t})+\\bar{\\Sigma}^{-1}\\bar{\\theta}/\\sqrt{\\bar{\\alpha}_{t-1}})\\,,\\quad\\hat{\\Sigma}_{t}(h)=(\\Sigma_{t}^{-1}+\\bar{\\Sigma}^{-1}/\\bar{\\alpha}_{t-1})^{-1}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Since $\\lambda_{d}(\\bar{\\Sigma}^{-1})\\to\\infty$ and $S_{t}\\rightarrow\\sqrt{\\bar{\\alpha}_{t}}\\bar{\\theta}$ by induction, we get ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\hat{\\Sigma}}_{t}(h)\\to{\\bar{\\alpha}}_{t-1}{\\bar{\\Sigma}}\\,,\\quad{\\hat{\\mu}}_{t}(S_{t},h)\\to\\sqrt{{\\bar{\\alpha}}_{t-1}}{\\bar{\\theta}}\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Moreover, $\\lambda_{d}(\\bar{\\Sigma}^{-1})\\to\\infty$ implies $\\lambda_{1}(\\bar{\\Sigma})\\rightarrow0$ , and thus $\\begin{array}{r}{\\operatorname*{lim}_{N\\to\\infty}\\|S_{t-1}-\\sqrt{\\bar{\\alpha}_{t-1}}\\bar{\\theta}\\|_{2}=0.}\\end{array}$ . ", "page_idx": 14}, {"type": "text", "text": "In the last stage, $t=1$ , $\\bar{\\alpha}_{0}=1$ , and $S_{0}=\\widetilde{\\theta}$ , which implies that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{N\\to\\infty}\\|\\tilde{\\theta}-\\bar{\\theta}\\|_{2}\\to0\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Upper bound on $\\lVert\\bar{{\\boldsymbol{\\theta}}}-{\\boldsymbol{\\theta}}_{*}\\rVert_{2}$ . This part of the proof uses the definition of $\\bar{\\theta}$ in Section 2.1 and that $\\varepsilon_{\\ell}\\\"\\sim\\mathcal{N}(0,\\sigma^{2})$ is independent noise. By definition, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\bar{\\theta}-\\theta_{*}=\\sigma^{-2}\\bar{\\Sigma}\\sum_{\\ell=1}^{N}\\phi_{\\ell}y_{\\ell}-\\theta_{*}=\\sigma^{-2}\\bar{\\Sigma}\\sum_{\\ell=1}^{N}\\phi_{\\ell}(\\phi_{\\ell}^{\\top}\\theta_{*}+\\varepsilon_{\\ell})-\\theta_{*}=\\sigma^{-2}\\bar{\\Sigma}\\sum_{\\ell=1}^{N}\\phi_{\\ell}\\varepsilon_{\\ell}\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Since $\\varepsilon_{\\ell}$ is independent zero-mean Gaussian noise with variance $\\sigma^{2},\\bar{\\theta}-\\theta_{*}$ is a Gaussian random variable with mean $\\mathbf{0}_{d}$ and covariance ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{cov}\\left[\\sigma^{-2}\\bar{\\Sigma}\\sum_{\\ell=1}^{N}\\phi_{\\ell}\\varepsilon_{\\ell}\\right]=\\sigma^{-4}\\bar{\\Sigma}\\left(\\sum_{\\ell=1}^{N}\\phi_{\\ell}\\mathrm{var}\\left[\\varepsilon_{\\ell}\\right]\\phi_{\\ell}^{\\top}\\right)\\bar{\\Sigma}=\\bar{\\Sigma}\\frac{\\sum_{\\ell=1}^{N}\\phi_{\\ell}\\phi_{\\ell}^{\\top}}{\\sigma^{2}}\\bar{\\Sigma}=\\bar{\\Sigma}\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $\\lambda_{d}(\\bar{\\Sigma}^{-1})\\to\\infty$ implies $\\lambda_{1}(\\bar{\\Sigma})\\rightarrow0$ , we get ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{N\\rightarrow\\infty}\\|\\bar{\\theta}-\\theta_{*}\\|_{2}=0\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This completes the proof. ", "page_idx": 15}, {"type": "text", "text": "A.4 Proof of Theorem 4 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "This proof has two parts. ", "page_idx": 15}, {"type": "text", "text": "Derivation of $p(s_{t-1}\\mid s_{t},h)$ . From (6), we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\int_{s_{0}}p(h\\mid s_{0})\\,p(s_{0}\\mid s_{t-1})\\,\\mathrm{d}s_{0}\\propto p(h\\mid s_{t-1}/\\sqrt{\\bar{\\alpha}_{t-1}})\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $p(s_{t-1}\\mid s_{t})$ is a Gaussian, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\np(s_{t-1}\\mid s_{t})=\\mathcal{N}(s_{t-1};\\mu_{t}(s_{t}),\\Sigma_{t})\\propto\\mathcal{N}(\\gamma s_{t-1};\\gamma\\mu_{t}(s_{t}),\\gamma^{2}\\Sigma_{t})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "for $\\gamma=1/\\sqrt{\\bar{\\alpha}_{t-1}}$ . Then by the Laplace approximation, ", "page_idx": 15}, {"type": "equation", "text": "$$\np(h\\mid\\gamma s_{t-1})\\mathcal{N}(\\gamma s_{t-1};\\gamma\\mu_{t}(s_{t}),\\gamma^{2}\\Sigma_{t})\\propto\\mathcal{N}(\\gamma s_{t-1};\\dot{\\theta}_{t},\\dot{\\Sigma}_{t})\\propto\\mathcal{N}(s_{t-1};\\dot{\\theta}_{t}/\\gamma,\\dot{\\Sigma}_{t}/\\gamma^{2})\\,,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\dot{\\theta}_{t},\\dot{\\Sigma}_{t}\\gets\\tt I R L S(\\gamma\\mu_{t}(s_{t}),\\gamma^{2}\\Sigma_{t},h)$ . ", "page_idx": 15}, {"type": "text", "text": "Derivation of $p(s_{T}\\mid h)$ . Analogously to the derivation of $p(s_{t-1}\\mid s_{t},h)$ , we establish that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\int_{s_{0}}p(h\\mid s_{0})\\,p(s_{0}\\mid s_{T})\\,\\mathrm{d}s_{0}\\propto p(h\\mid s_{T}/\\sqrt{\\bar{\\alpha}_{T}})\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then by the Laplace approximation for $\\gamma=1/\\sqrt{\\bar{\\alpha}_{T}}$ , we get ", "page_idx": 15}, {"type": "equation", "text": "$$\np(h\\mid\\gamma s_{t-1})\\mathcal{N}(s_{t-1};\\mathbf{0}_{d},I_{d})\\propto\\mathcal{N}(s_{t-1};\\dot{\\theta}_{T+1}/\\gamma,\\dot{\\Sigma}_{T+1}/\\gamma^{2})\\,,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\dot{\\theta}_{T+1},\\dot{\\Sigma}_{T+1}\\gets\\mathtt{I R L S}(\\mathbf{0}_{d},\\gamma^{2}I_{d},h)$ . ", "page_idx": 15}, {"type": "text", "text": "A.5 Supporting Lemmas ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We state and prove our supplementary lemmas next. ", "page_idx": 15}, {"type": "text", "text": "Lemma 5. Let $p(x)=\\mathcal{N}(x;\\mu_{1},\\Sigma_{1})$ and $q(x)=N(x;\\mu_{2},\\Sigma_{2}),$ , where $\\mu_{1},\\mu_{2}\\in\\mathbb{R}^{d}$ and $\\Sigma_{1},\\Sigma_{2}\\in$ $\\mathbb{R}^{d\\times d}$ . Then ", "page_idx": 15}, {"type": "equation", "text": "$$\nd(p,q)=\\frac{1}{2}\\left((\\mu_{2}-\\mu_{1})^{\\top}\\Sigma_{2}^{-1}(\\mu_{2}-\\mu_{1})+\\mathrm{tr}(\\Sigma_{2}^{-1}\\Sigma_{1})-\\log\\frac{\\operatorname*{det}(\\Sigma_{1})}{\\operatorname*{det}(\\Sigma_{2})}-d\\right)\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Moreover, when $\\Sigma_{1}=\\Sigma_{2}$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\nd(p,q)={\\frac{1}{2}}(\\mu_{2}-\\mu_{1})^{\\top}\\Sigma_{2}^{-1}(\\mu_{2}-\\mu_{1})\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. The proof follows from the definitions of $\\mathrm{KL}$ divergence and multivariate Gaussians. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Lemma 6. $F i x\\;\\mu_{1}\\in\\mathbb{R}^{d},\\,\\Sigma_{1}\\succeq0,\\,\\mu_{2}\\in\\mathbb{R}^{d},$ , and $\\Sigma_{2}\\succeq0$ . Then ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\mathcal N}(x;\\mu_{1},\\Sigma_{1}){\\mathcal N}(x;\\mu_{2},\\Sigma_{2})\\propto{\\mathcal N}(x;\\mu,\\Sigma)\\,,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mu=\\Sigma\\bigl(\\Sigma_{1}^{-1}\\mu_{1}+\\Sigma_{2}^{-1}\\mu_{2}\\bigr)\\,,\\quad\\Sigma=\\bigl(\\Sigma_{1}^{-1}+\\Sigma_{2}^{-1}\\bigr)^{-1}\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. This is a classic result, which is proved as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{N}(x;\\mu_{1},\\Sigma_{1})\\mathcal{N}(x;\\mu_{2},\\Sigma_{2})\\propto\\exp\\left[-\\frac{1}{2}((x-\\mu_{1})^{\\top}\\Sigma_{1}^{-1}(x-\\mu_{1})+(x-\\mu_{2})^{\\top}\\Sigma_{2}^{-1}(x-\\mu_{2}))\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\propto\\exp\\left[-\\frac{1}{2}(x^{\\top}\\Sigma_{1}^{-1}x-2x^{\\top}\\Sigma_{1}^{-1}\\mu_{1}+x^{\\top}\\Sigma_{2}^{-1}x-2x^{\\top}\\Sigma_{2}^{-1}\\mu_{2})\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\exp\\left[-\\frac{1}{2}(x^{\\top}\\Sigma^{-1}x-2x^{\\top}\\Sigma^{-1}\\Sigma(\\Sigma_{1}^{-1}\\mu_{1}+\\Sigma_{2}^{-1}\\mu_{2}))\\right]}\\\\ &{\\qquad\\qquad\\qquad\\propto\\exp\\left[-\\frac{1}{2}(x-\\mu)^{\\top}\\Sigma^{-1}(x-\\mu)\\right]\\propto\\mathcal{N}(x;\\mu,\\Sigma)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The neglected factors depend on $\\mu_{1},\\mu_{2},\\Sigma_{1}$ , and $\\Sigma_{2}$ . This completes the proof. ", "page_idx": 16}, {"type": "text", "text": "B Learning the Reverse Process ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "One property of our model is that $q(s_{T})\\approx\\mathcal{N}(s_{T};\\mathbf{0}_{d},I_{d})$ when $T$ is sufficiently large [18]. Since $S_{T}$ is initialized to the same distribution in the reverse process $p,p$ can be learned from the forward process $q$ by simply reversing it. This is done as follows. Using the definition of the forward process in (4), Ho et al. [18] showed that ", "page_idx": 16}, {"type": "equation", "text": "$$\nq(s_{t-1}\\mid s_{t},s_{0})=\\mathcal{N}\\big(s_{t-1};\\tilde{\\mu}_{t}\\big(s_{t},s_{0}\\big),\\tilde{\\beta}_{t}I_{d}\\big)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "holds for any $s_{0}$ and $s_{t}$ , where ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\tilde{\\mu}_{t}(s_{t},s_{0})=\\frac{\\sqrt{\\bar{\\alpha}_{t-1}}\\beta_{t}}{1-\\bar{\\alpha}_{t}}s_{0}+\\frac{\\sqrt{\\alpha_{t}}(1-\\bar{\\alpha}_{t-1})}{1-\\bar{\\alpha}_{t}}s_{t}\\,,\\quad\\tilde{\\beta}_{t}=\\frac{1-\\bar{\\alpha}_{t-1}}{1-\\bar{\\alpha}_{t}}\\beta_{t}\\,,\\quad\\bar{\\alpha}_{t}=\\prod_{\\ell=1}^{t}\\alpha_{\\ell}\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, the latent variable in stage $t-1$ , $S_{t-1}$ , is easy to sample when $s_{t}$ and $s_{0}$ are known. To estimate $s_{0}$ , which is unknown when samp\u221aling from\u221a the reverse process, we use the forward process again. In particular, (4) implies that $s_{t}=\\bar{\\sqrt{\\bar{\\alpha}_{t}}}\\bar{s}_{0}+\\sqrt{1-\\bar{\\alpha}_{t}}\\varepsilon_{t}$ for any $s_{0}$ , where $\\varepsilon_{t}\\sim\\mathcal{N}(\\mathbf{0}_{d},\\mathbf{\\bar{\\mathit{I}}}_{d})$ is a standard Gaussian noise. This identity can be rearranged as ", "page_idx": 16}, {"type": "equation", "text": "$$\ns_{0}=\\frac{1}{\\sqrt{\\bar{\\alpha}_{t}}}\\big(s_{t}-\\sqrt{1-\\bar{\\alpha}_{t}}\\varepsilon_{t}\\big)\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "To obtain $\\varepsilon_{t}$ , which is unknown when sampling from $p$ , we learn to regress it from $s_{t}$ [18]. ", "page_idx": 16}, {"type": "text", "text": "The regressor is learned as follows. Let $\\varepsilon_{t}(\\cdot;\\psi)$ be a regressor of $\\varepsilon_{t}$ parameterized by $\\psi$ and $\\mathcal{D}=\\{s_{0}\\}$ be a dataset of training examples. We sample $s_{0}$ uniformly at random from $\\mathcal{D}$ and then solve ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\boldsymbol{\\psi}_{t}=\\underset{\\boldsymbol{\\psi}}{\\arg\\operatorname*{min}}~\\mathbb{E}_{\\boldsymbol{q}}\\left[\\|\\boldsymbol{\\varepsilon}_{t}-\\boldsymbol{\\varepsilon}_{t}(\\boldsymbol{S}_{t};\\boldsymbol{\\psi})\\|_{2}^{2}\\right]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "per stage. The expectation is approximated by sampled $s_{0}$ . Note that we slightly depart from Ho et al. [18]. Since each regressor has its own parameters, the original optimization problem over $T$ stages decomposes into $T$ subproblems. ", "page_idx": 16}, {"type": "text", "text": "C Additional Experiments ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "This section contains three additional experiments. ", "page_idx": 16}, {"type": "text", "text": "C.1 Additional Synthetic Problems ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In Section 6.2, we show results for three hand-selected problems out of six. We report results on the other three problems in Figure 4. We observe the same trends as in Section 6.2. ", "page_idx": 16}, {"type": "text", "text": "C.2 MNIST Experiment ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The next experiment is on the MNIST dataset [30], where the mean reward is estimated by a classifier. We start with learning an MLP-based multi-way classifier for digits and extract $d=8$ dimensional embeddings of all digits in the dataset, which are used as feature vectors in our experiment. We generate a distribution over model parameters $\\boldsymbol{\\theta}_{*}$ as follows: (1) we choose a random positive label, assign it reward 1, and assign reward $-1$ to all other labels; (2) we subsample a random dataset of size 20, with $50\\%$ positive and $50\\%$ negative labels; (3) we train a linear model, which gives us a single $\\theta_{*}$ . We repeat this 10 000 times and get a distribution over $\\boldsymbol{\\theta}_{*}$ . ", "page_idx": 16}, {"type": "image", "img_path": "7v0UyO0B6q/tmp/d449877f966beec115e2db92af003d0300055b6ed46547a9319b73aae98ebc75.jpg", "img_caption": ["Figure 4: Evaluation of DiffTS on another three synthetic problems. The first row shows samples from the true (blue) and diffusion model (red) priors. The second row shows the regret of DiffTS and the baselines as a function of round $n$ . "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "7v0UyO0B6q/tmp/996185024f6d05ece925000624d5f69e0ceb6a67588343a56b362ce01d0787c0.jpg", "img_caption": ["Figure 5: Evaluation of DiffTS on the MNIST dataset. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "We consider both linear and logistic bandits. In both, the model parameter $\\theta_{*}$ is initially sampled from the prior. In each round, $K=10$ random actions are chosen randomly from all digits. In the linear bandit, the mean reward for a digit with embedding $x$ is $x^{\\top}\\theta_{*}$ and the reward noise is $\\sigma=1$ . In the logistic bandit, the mean reward is $g(x^{\\top}\\theta_{*})$ , where $g$ is a sigmoid. ", "page_idx": 17}, {"type": "text", "text": "Our MNIST results are reported in Figure 5. We observe again that DiffTS has a lower regret than all baselines, because the learned prior captures the underlying distribution of $\\theta_{*}$ well. We note that both the prior and diffusion prior distributions exhibit a strong cluster structure (Figure 5a), where each cluster represents one label. ", "page_idx": 17}, {"type": "text", "text": "C.3 Ablation Studies ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We conduct three ablation studies on the cross problem in Figure 2. ", "page_idx": 17}, {"type": "text", "text": "In all experiments, the number of samples for training diffusion priors was 10 000. In Figure 6a, we vary it from 100 to 10 000. We observe that the regret decreases as the number of samples increases, due to learning a better prior approximation. The trend stabilizes around 3 000 training samples. We conclude that the quality of the learned prior approximation has a major impact on regret. ", "page_idx": 17}, {"type": "image", "img_path": "7v0UyO0B6q/tmp/4a7d9f6afe4baf6a14aa0068450715cb04e52415e8b4284e09d4a0685619d283.jpg", "img_caption": ["Figure 6: An ablation study of DiffTS on the cross problem: (a) regret with a varying number of samples for training the diffusion prior, (b) regret with a varying number of diffusion stages $T$ , and (c) computation time with a varying number of diffusion stages $T$ . "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "7v0UyO0B6q/tmp/e17f5743c589c3ac93f8cf651156d60eb79350ab43f130514c2c04c12b6a2747.jpg", "img_caption": ["Figure 7: Evaluation on Gaussian mixture variants of the synthetic problems in Figure 2. The first row shows samples from the true (blue) and diffusion model (red) priors. The second row shows the earth mover\u2019s distance of DiffTS and baseline posterior distributions from the true posterior as a function of sample size $n$ . "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "In all experiments, the number of diffusion stages was $T=100$ . In Figure 6b, we vary it from 1 to 300 and observe its impact on regret. While the regret at $T=1$ is high, it decreases quickly as $T$ increases. It stabilizes around $T=100$ , which we used in our experiments. In Figure 6c, we vary $T$ from 1 to 300 and observe its effect on the computation time of posterior sampling. The time is linear in $T$ , as suggested in Section 4.2. The main contributor to it is the neural network regressor. ", "page_idx": 18}, {"type": "text", "text": "C.4 Non-Bandit Evaluation ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We use Gaussian mixture variants of the synthetic problems in Figure 2 for our non-bandit evaluation. The action in round $k$ is chosen uniformly at random (not adaptively). Since the priors are Gaussian mixtures, the true posterior distribution can be computed in a closed form using MixTS and we can measure the distance of posterior approximations from it. We use the earth mover\u2019s distance (EMD) between posterior samples from the true posterior and its approximation. We also considered KL divergence, but this one required analytical forms of posterior approximations, which are not available in DiffTS and DPS. ", "page_idx": 18}, {"type": "text", "text": "2: Initial sample $S_{T}\\sim\\mathcal{N}(\\mathbf{0}_{d},I_{d})$   \n3: for stage $t=T,\\dots,1$ do   \n4: $\\begin{array}{r}{\\hat{S}\\gets\\frac{\\varepsilon_{t}\\left(S_{t};\\psi_{t}\\right)}{\\sqrt{1-\\bar{\\alpha}_{t}}}}\\end{array}$   \n5: $\\begin{array}{r l}&{\\hat{S}_{0}\\gets\\frac{\\frac{\\vee\\bullet-\\alpha_{t}}{\\sqrt{\\bar{\\alpha}_{t}}}}{\\sqrt{\\bar{\\alpha}_{t}}}(S_{t}+(1-\\bar{\\alpha}_{t})\\hat{S})}\\\\ &{Z\\sim\\mathcal{N}(\\mathbf0_{d},I_{d})}\\\\ &{S_{t-1}\\gets\\frac{\\sqrt{\\bar{\\alpha}_{t-1}}\\beta_{t}}{1-\\bar{\\alpha}_{t}}\\hat{S}_{0}+\\frac{\\sqrt{\\alpha_{t}}(1-\\bar{\\alpha}_{t-1})}{1-\\bar{\\alpha}_{t}}S_{t}+\\tilde{\\sigma}_{t}Z-\\zeta_{t}\\nabla\\sum_{\\ell=1}^{N}(y_{\\ell}-\\phi_{\\ell}^{\\top}\\hat{S}_{0})^{2}}\\end{array}$   \n6:   \n7: ", "page_idx": 19}, {"type": "text", "text": "8: Output: Posterior sample $S_{0}$ ", "page_idx": 19}, {"type": "text", "text": "We evaluate all methods from Figure 2. In addition, we implement a sequential Monte Carlo (SMC) sampler. The initial particles are selected uniformly at random from the prior. At each round, the particles are perturbed by a Gaussian noise. The standard deviation of the noise is initialized as a fraction of the observation noise and decays over time, as the posterior concentrates. The particles are weighted according to the likelihood of the observation in the round. Finally, we use normalized likelihood weights to resample the particles. We tune SMC to get good posterior approximations. We use 3 000 particles. For this setting, the computational costs of posterior sampling in SMC and DiffTS are comparable. ", "page_idx": 19}, {"type": "text", "text": "Our results are reported in Figure 7. We observe that DiffTS approximations are comparable to MixTS, which has an exact posterior in this setting. The second best performing method is SMC. Its approximations worsen as the sample size $n$ increases. DPS approximations also get worse as $n$ increases, which caused instability in Figure 2. ", "page_idx": 19}, {"type": "text", "text": "D Implementation of Chung et al. [12] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In our experiments, we compare to diffusion posterior sampling (DPS) with a Gaussian observation noise (Algorithm 1 in Chung et al. [12]). Our implementation is presented in Algorithm 4. The score is $\\hat{S}=\\varepsilon_{t}(S_{t};\\psi_{t})/\\sqrt{1-\\bar{\\alpha}_{t}}$ , where $\\varepsilon_{t}(S_{t};\\psi_{t})$ is a regression estimate of the forward process noise $\\varepsilon_{t}$ in Appendix B. We set $\\tilde{\\sigma}_{t}=\\sqrt{\\tilde{\\beta}_{t}}$ , which is the same amount of noise as in our reverse process (Section 3). The term ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\nabla\\sum_{\\ell=1}^{N}(y_{\\ell}-\\phi_{\\ell}^{\\top}\\hat{S}_{0})^{2}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "is the gradient of the negative log-likelihood with respect to $S_{t}$ . ", "page_idx": 19}, {"type": "text", "text": "As noted in Appendices C.2 and D.1 of Chung et al. [12], $\\zeta_{t}$ in DPS needs to be tuned for good performance. We also observed this in our experiments (Section 6.2). To make DPS work well, we follow Chung et al. [12] and set ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\zeta_{t}=\\frac{1}{\\sqrt{\\sum_{\\ell=1}^{N}(y_{\\ell}-\\phi_{\\ell}^{\\top}\\hat{S}_{0})^{2}}}\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "While this significantly improves the performance of DPS, it does not prevent failures. The fundamental problem is that gradient-based optimization is sensitive to the step size, especially when the optimized function is steep. Note that LaplaceDPS does not have any such hyper-parameter. ", "page_idx": 19}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The abstract and introduction clearly state all contributions. The introduction also points to where those contributions are made. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The increase in computational cost is discussed in Section 4.2 and shown empirically in Section 6.2. We also conduct an ablation study in Appendix C.3, where we show how the regret of DiffTS scales with the number of samples used for pre-training the prior and the number of stages in the diffusion model prior. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The main claims are stated and discussed in Section 4. Their proofs are in Appendix A. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We also include code to reproduce the synthetic results in Figures 2 and 4. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We include code to reproduce the synthetic results in Figures 2 and 4. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The experiments are described to a sufficient level to be reproducible. To make sure, we include code to reproduce the synthetic results in Figures 2 and 4. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: All plots in the paper have error bars. ", "page_idx": 20}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [No] ", "page_idx": 21}, {"type": "text", "text": "Justification: Our experiments are not large scale. ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We checked the link and comply. ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: This work is algorithmic and not tied to a particular application that would have immediate negative impact. ", "page_idx": 21}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] Justification: This paper does not pose such a risk. ", "page_idx": 21}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: All used assets are stated and cited. ", "page_idx": 21}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 21}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: No crowdsourcing or research with human subjects. ", "page_idx": 21}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] Justification: No crowdsourcing or research with human subjects. ", "page_idx": 21}]