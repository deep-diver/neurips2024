[{"type": "text", "text": "Semi-supervised Knowledge Transfer Across Multi-omic Single-cell Data ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Fan Zhang1, Tianyu Liu2, Zihao Chen3, Xiaojiang Peng4, Chong Chen5, Xian-Sheng $\\mathbf{Hua}^{5}$ , Xiao $\\mathbf{Luo}^{6,*}$ , Hongyu Zhao2 ", "page_idx": 0}, {"type": "text", "text": "1Georgia Institute of Technology, 2Yale University, 3Peking University,   \n4Shenzhen Technology University, 5Terminus Group, 6University of California, Los Angeles fanzhang@gatech.edu, tianyu.liu@yale.edu, g.e.challenger@pku.edu.cn   \npengxiaojiang@sztu.edu.cn, chenchong.cz@gmail.com, huaxiansheng@gmail.com xiaoluo@cs.ucla.edu, hongyu.zhao@yale.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Knowledge transfer between multi-omic single-cell data aims to effectively transfer cell types from scRNA-seq data to unannotated scATAC-seq data. Several approaches aim to reduce the heterogeneity of multi-omic data while maintaining the discriminability of cell types with extensive annotated data. However, in reality, the cost of collecting both a large amount of labeled scRNA-seq data and scATAC-seq data is expensive. Therefore, this paper explores a practical yet underexplored problem of knowledge transfer across multi-omic single-cell data under cell type scarcity. To address this problem, we propose a semi-supervised knowledge transfer framework named Dual label scArcity elimiNation with Cross-omic multi-samplE Mixup (DANCE). To overcome the label scarcity in scRNA-seq data, we generate pseudo-labels based on optimal transport and merge them into the labeled scRNAseq data. Moreover, we adopt a divide-and-conquer strategy which divides the scATAC-seq data into source-like and target-specific data. For source-like samples, we employ consistency regularization with random perturbations while for targetspecific samples, we select a few candidate labels and progressively eliminate incorrect cell types from the label set for additional supervision. Next, we generate virtual scRNA-seq samples with multi-sample Mixup based on the class-wise similarity to reduce cell heterogeneity. Extensive experiments on many benchmark datasets suggest the superiority of our DANCE over a series of state-of-the-art methods. Code is available at https://github.com/zfkarl/DANCE. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In the realm of biology and medicine, many experimental methods [18, 25, 53, 51, 54, 63] using high-throughput sequencing technologies have emerged and characterize diverse properties of single cells. The predominant techniques are single-cell RNA-sequencing (scRNA-seq) [55, 88, 49] and single-cell ATAC-sequencing (scATAC-seq) [15, 75] for understanding complicated organisms and tissues at the single cell level. Specifically, scATAC-seq is an epigenomic profliing method designed to assess chromatin accessibility and provides an additional layer of information that complements scRNA-seq, enhancing the ability to comprehend epigenetic heterogeneity within complex tissues. However, identifying the cell types of scATAC-seq data is challenging because of the high sparsity, large dimensionality, and increasing scale of scATAC-seq data. Considering the lack of cell type annotations for scATAC-seq data, several methods [38, 36, 72] have been proposed to transfer cell types from fully-labeled scRNA-seq data to scATAC-seq data2. Among these methods, scJoint [38] introduces a scalable transfer learning framework that successfully incorporates atlas-scale scRNAseq with scATAC-seq data based on kNN techniques. scBridge [36] integrates multi-omic data heterogeneously by identifying reliable scATAC-seq cells with smaller omic differences in comparison to scRNA-seq cells. scNCL [72] employs prior domain knowledge along with contrastive learning to address the challenge of heterogeneous features from different modalities. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Despite significant progress made by the aforementioned approaches, their success relies on a substantial amount of annotated scRNA-seq data. In reality, annotating both scRNA-seq data and scATAC-seq data is expensive and challenging. Therefore, this study explores a practical yet unexplored problem of knowledge transfer across multi-omic single-cell data under label scarcity in both scRNA-seq data and scATAC-seq data. In this study, only a small fraction of scRNA-seq data with cell types annotated, while a significant portion of both scRNA-seq data and scATAC-seq data lack annotations. In contrast to previous works assuming abundant labeled scRNA-seq data, this setting aligns more closely with practical scenarios. ", "page_idx": 1}, {"type": "text", "text": "Addressing this realistic problem requires the consideration of two crucial aspects. Firstly, how to learn representations with distinct cell type discriminability under label scarcity in both scRNA-seq data and scATAC-seq data? Learning representations with good discriminability relies on a large amount of annotated data, yet collecting labeled data is extremely costly and difficult. Therefore, it poses a significant challenge in situations where both scRNA-seq data and scATAC-seq data lack a substantial amount of labels. Secondly, how to reduce cell heterogeneity between different omic data while maintaining cell type discriminability? Another challenge is how to reduce the heterogeneous gap between different omic data, which is crucial for cell type transfer. Additionally, while reducing cell heterogeneity, there is a high likelihood of compromising cell type discriminability. Even worse, this becomes significantly more challenging under label scarcity. ", "page_idx": 1}, {"type": "text", "text": "To answer these questions, we propose a semi-supervised knowledge transfer framework termed Dual label scArcity elimiNation with Cross-omic Multi-samplE Mixup (DANCE). Since the class distribution of the whole scRNA-seq data is unattainable, we start by injecting semantic knowledge using labeled source scRNA-seq data, and then generate pseudo-labels based on optimal transport (OT) for unlabeled scRNA-seq data. Afterward, the unlabeled scRNA-seq data, along with pseudo-labels, are incorporated into the labeled set. For unlabeled target scATAC-seq data, due to the heterogeneous gap, we adopt a divide-and-conquer strategy, which separates the whole data into source-like data and target-specific data. For source-like samples, we employ consistency regularization to force the model to make consistent predictions after random perturbations. For target-specific samples, we select ambiguous labels and then filter the incorrect labels, which can guide the optimization in a soft manner. To mitigate the heterogeneous gap, we propose to generate virtual scRNA-seq samples by multi-sample Mixup according to class-wise similarity and then we perform a sample-to-sample alignment strategy. The effectiveness of DANCE is validated by adopting a series of benchmark datasets and conducting comprehensive experiments in comparison to various state-of-the-art approaches. ", "page_idx": 1}, {"type": "text", "text": "To summarize, the main contribution of this paper includes: (1) New Problem. We study a realistic yet underexplored problem of knowledge transfer across multi-omic single-cell data under label scarcity in both scRNA-seq and scATAC-seq data. This problem holds significant implications for reducing the annotation cost of single-cell data. (2) Novel Methodology. We propose a semi-supervised framework named DANCE for this problem. DANCE introduces OT-based dataset expansion and a divide-and-conquer strategy for dual label scarcity elimination. Additionally, DANCE employs an effective alignment strategy of cross-omic multi-sample Mixup to reduce cell heterogeneity. (3) Comprehensive Experiments. Through extensive experiments with different settings on various benchmark datasets, we demonstrate the superiority of DANCE against many state-of-the-art methods. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Multi-omic single-cell Data Integration ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The integration of multi-omic single-cell data constitutes a fundamental challenge in elucidating biological processes. Multi-omic single-cell data affords a multitude of perspectives on cellular functions, thereby augmenting our understanding of biology. Some approaches [69, 44, 59, 37, ", "page_idx": 1}, {"type": "image", "img_path": "sKEhebkEdz/tmp/fcbb0db252c0a3ab8e34226695d9d3238dc0de75efa8c989eadf9ef7b736dd95.jpg", "img_caption": ["Figure 1: An overview of DANCE. We first utilize labeled scRNA-seq samples to inject semantics and expand the scRNA-seq dataset by OT-based pseudo-labeling. Then we employ consistency regularization and ambiguous set learning for source-like and target-specific scATAC-seq samples. In addition, we perform cross-omic multi-sample Mixup to reduce cell heterogeneity. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "30, 61, 2] opt for denoising, batch correction, and integrating single-cell data across numerous experiments, encompassing transcriptomic data and scATAC-seq data. Nevertheless, the simple application of these methodologies to multi-omic data integration poses computational challenges and frequently yields unsatisfactory results, given the considerable differences in dimensions and sparsity levels among different modalities. Moving a step closer, methodologies such as scAI [25] and $\\mathrm{MOFA+}$ [3] utilize factor analysis and joint clustering to concurrently measure multi-omic data within the same cell. Nevertheless, these methods require pairwise data format, and executing paired measurements presents technical challenges and involves high costs. Therefore, another series of approaches have been proposed to measure multi-omic data from different cells, including manifold alignment [63, 1, 39], matrix factorization [18, 64, 84], correlation-based methods [5, 54], and neural network approaches [38, 36, 72]. Recent approaches often map multi-omic data into low-dimensional spaces through shared auto-encoders [21, 66, 56, 20], and Graph Neural Networks (GNNs) encoders [65, 60, 40], which focus on knowledge transfer across multi-omic data. For instance, scBridge [36] maps multi-omic single-cell data into a shared embedding space and mines reliable samples for dataset expansion, thereby merging the scATAC-seq and scRNA-seq data into the same dataset. GLUE [8] combines a knowledge-based graph and adversarial alignment to explore regulatory interaction across various omic layers. scCLIP [68] introduces a contrastive learning approach to integrate multi-omic single-cell data. It aligns the representations of pairwise multi-omic single-cell data without the usage of cell type labels. Despite their significant strides in integrating heterogeneous cells, all these methods overlook a practical challenge in reality, i.e., the difficulty of annotating both scRNA-seq data and scATAC-seq data. Therefore, there is an urgent need for an approach that can effectively alleviate label scarcity in knowledge transfer across multi-omic single-cell data. ", "page_idx": 2}, {"type": "text", "text": "2.2 Semi-supervised Learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To tackle the label scarcity issue in reality, semi-supervised learning has been widely studied with various applications, such as image classification [46, 23, 7, 6], semantic segmentation [48, 14], and object detection [24, 80]. Recent studies on semi-supervised learning include consistency regularization approaches [46, 52, 67] and pseudo-labeling approaches [6, 7, 23, 79]. Consistency regularization usually first introduces random perturbations from various sources, including data input [67], network parameters [47], and deep features [28]. By encouraging the model to make consistent predictions under these perturbations, it can still learn good representations even under resource constraints. Pseudo-labeling typically assigns pseudo-labels to unlabeled samples using a neural network. It often comes with some special techniques, such as dynamic thresholding [78, 33] and curriculum learning [29, 22, 10]. Then those pseudo-labels together with their corresponding samples are added to the dataset for future use. Besides the advanced approaches in computer vision, other related areas such as node classification [13, 62] and reinforcement learning [87] also benefit from semi-supervised learning. However, to the best of our knowledge, there exists no previous work to study the problem of semi-supervised knowledge transfer across multi-omic single-cell data. Therefore, we provide a semi-supervised framework termed DANCE to promote the understanding of semi-supervised learning in the field of bioinformatics. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3 Problem Definition ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In our problem, we have two datasets $\\mathcal{D}^{(s)}$ and $\\mathcal{D}^{(t)}$ , which denote the source scRNA-seq dataset and the target scATAC-seq dataset, respectively. In the semi-supervised setting, the scRNA-seq dataset $\\mathcal{D}^{(s)}$ is separated into two parts, the labeled dataset $\\mathcal{D}^{(s),l}$ and the unlabeled dataset $\\mathcal{D}^{(s),u}$ , where D(s),l = {(xi(s),l, yi(s consists of $N^{l}$ scRNA-seq samples $\\pmb{x}_{i}^{(s),l}$ and the corresponding labels $y_{i}^{(s),l}$ , and $\\mathbf{\\mathcal{D}}^{(s),u}=\\{(\\pmb{x}_{i}^{(s),u})\\}_{i=1}^{N^{u}}$ includes $N^{u}$ unlabeled scRNA-seq samples (s),u The . target scATAC-seq dataset with the size $N^{t}$ is denoted as $\\mathbf{\\mathcal{D}}^{(t)}\\,=\\,\\{(\\mathbf{x}_{i}^{(t)})\\}_{i=1}^{N^{t}}$ , which includes $N^{t}$ scATAC-seq samples $\\pmb{x}_{i}^{(t)}$ . We focus on the closed-set setting, thus both scRNA-seq data and scATAC-seq data are assumed to share the same set of $C$ cell types. A shared encoder $F(\\cdot)$ is adopted to map both the scRNA-seq samples and scATAC-seq samples into the embedding space. Following the encoder, we utilize a classifier $H(\\cdot)$ to convert representations into softmax predictions corresponding to different cell types. The representations and predictions are denoted as $\\bar{z}_{i}=F({\\pmb x}_{i})$ and $p_{i}=s o f t m a x(H(z_{i}))$ , respectively. It should be noted that only a small portion of the scRNAseq data is labeled, and a significant number of both scRNA-seq samples and scATAC-seq samples lack cell type annotations. We aim to transfer the semantic information related to different cell types from $\\mathcal{D}^{(s)}$ to $\\mathcal{D}^{(t)}$ , which has two expectations: $(I)$ The cell representations of scRNA-seq data and scATAC-seq data are mapped into a shared common embedding space with the similarity structure preserved. (2) The cell type annotations on scATAC-seq data are accurate after label transfer. ", "page_idx": 3}, {"type": "text", "text": "4 The Proposed Approach ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "4.1 Framework Overview ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "This paper investigates the problem of semi-supervised knowledge transfer across multi-omic singlecell data. We employ a shared encoder and classifier for heterogeneous scRNA-seq data and scATACseq data. The labeled scRNA-seq data are utilized to inject the semantic knowledge. To overcome label scarcity in source scRNA-seq data, we generate pseudo labels based on optimal transport and merge them into the labeled set. To address label scarcity in target scATAC-seq data, we perform consistency regularization for source-like samples and learn from ambiguous labels for target-specific samples. To reduce the cell heterogeneity, we generate virtual scRNA-seq samples through cross-omic multi-sample Mixup and align the heterogeneous multi-omic data. The overview of the proposed framework is illustrated in Figure 1, and each component will be elaborated in the next sections. ", "page_idx": 3}, {"type": "text", "text": "4.2 Optimal Transport-based Dataset Expansion for Source scRNA-seq Data ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The first obstacle is the label scarcity in scRNA-seq data. Recently, pseudo-labeling is a commonly used technique in semi-supervised learning [52, 78] and domain adaptation [83, 31] problems, which usually operate at the sample level [52, 83] by assigning the label with the highest confidence to each sample. However, this sample-level prediction-based pseudo-labeling strategy could be biased to easy classes, leading to biased and suboptimal results. To tackle this, we incorporate optimal transport (OT) [12, 9, 11] into the pseudo-labeling process, which is capable of taking into account prior cell type distributions from a global view to reduce potential noise in pseudo-labeling. ", "page_idx": 3}, {"type": "text", "text": "Here we first recall the knowledge of OT. OT is a constrained optimization problem designed to discover the optimal coupling matrix $Q$ for mapping one probability distribution to another, with the goal of minimizing the overall cost. To represent the cost of transporting data from $\\gamma$ and $\\eta$ , we introduce a cost matrix $C\\,\\in\\,\\mathbb{R}^{|\\gamma|\\,\\times\\,|\\eta|}$ , where $|\\gamma|$ and $|\\eta|$ denote the dimension of $\\gamma$ and $\\eta$ respectively. By applying the Sinkhorn algorithm [17] to minimize the Sinkhorn distance, we can obtain the optimal coupling matrix $Q$ : ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{{\\pmb q}\\in\\Pi(\\gamma,\\eta)}\\sum C_{i j}{\\pmb Q}_{i j}+\\frac{1}{\\sigma}(-{\\pmb Q}_{i j}l o g{\\pmb Q}_{i j}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\begin{array}{r}{\\Pi(\\gamma,\\eta)=\\left\\{\\mathbf Q\\in\\mathbb R_{+}^{|\\gamma|\\times|\\eta|}\\;|\\;\\mathbf Q\\mathbf1_{|\\eta|}=\\gamma,\\mathbf Q^{\\top}\\mathbf1_{|\\gamma|}=\\eta\\right\\}}\\end{array}$ and $\\sigma$ is set to 10 empirically. ", "page_idx": 4}, {"type": "text", "text": "To employ OT for pseudo-labeling, we begin by training the encoder and classifier on labeled scRNAseq data, enabling the injection of semantic knowledge into the model. Similar to traditional cell type annotation problems [16, 77], we leverage a small number of labeled scRNA-seq data and minimize the cross-entropy between the classifier\u2019s output and the cell type labels: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{S}=\\sum_{\\pmb{x}_{i}^{(s),l}\\in\\mathcal{D}^{(s),l}}\\mathrm{CE}(\\pmb{p}_{i}^{(s),l},y_{i}^{(s),l}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where pi $\\pmb{p}_{i}^{(s),l}$ ,lrepresents the predicted distribution of xi(s),l. ", "page_idx": 4}, {"type": "text", "text": "Then, we aim to transfer the semantic information from labeled scRNA-seq samples to unlabeled ones. In particular, a mini-batch of $B$ unlabeled samples is fed to our neural network to generate the prediction matrix $\\b{P}\\in\\mathbb{R}^{B\\times C}$ , which could be biased to easy classes and thus inaccurate. To tackle this, we learn an optimal coupling matrix that should obey prior cell type distributions from a global view. Since we do not have prior information, we adopt a uniform distribution as in the polytope $\\textstyle\\prod({\\frac{1}{B}}\\mathbf{1}_{B},{\\frac{1}{C}}\\mathbf{1}_{C})$ , where ${\\bf1}_{B}$ and $\\mathbf{1}_{C}$ indicate two vector of ones with dimensions of $B$ and $C$ , respectively. In addition, the optimal $Q$ should be close to $_{P}$ . Therefore, we define the cost matrix $_{C}$ as $-l o g P$ , and Eqn. 1 can be reformulated as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\sum_{{\\bf{Q}}\\in\\prod(\\frac{1}{B}{\\bf{1}}_{B},\\frac{1}{C}{\\bf{1}}_{C})}\\sum_{i j}-l o g P_{i j}Q_{i j}+\\frac{1}{\\sigma}(-Q_{i j}l o g{\\bf Q}_{i j}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The coupling matrix $Q$ obtained by optimizing Eqn. 3 can be used to generate pseudo-labels for unlabeled scRNA-seq data. Formally, the pseudo-labels can be defined as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{y}_{i}^{(s),u}=a r g m a x_{j}Q_{i j}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Then the unlabeled scRNA-seq samples and their corresponding pseudo-labels would be added to the labeled scRNA-seq dataset: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{D}^{(s),l}\\leftarrow\\cup_{i=1}^{N^{u}}\\{(\\pmb{x}_{i}^{(s),u},\\hat{y}_{i}^{(s),u})\\}\\cup\\mathcal{D}^{(s),l}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Leveraging OT-based pseudo-labeling for dataset expansion can effectively mitigate label scarcity in scRNA-seq data. On the one hand, we employ labeled scRNA-seq data for semantic injection using Eqn. 2, obtaining high-quality pseudo-labels. On the other hand, with the dataset expanded during the current iteration, more information is available for Eqn. 2 in the subsequent iteration. This leads to generating more reliable pseudo-labels in the following iterations. Through continuous iterative training, we can progressively enhance both the scale and quality of the scRNA-seq dataset. Next, we theoretically support the superiority of OT-based dataset expansion. See proof in Appendix A. ", "page_idx": 4}, {"type": "text", "text": "Theorem 4.1. Let $\\begin{array}{r}{P_{n}=\\frac{1}{B}P}\\end{array}$ be the normalized version of $_{P}$ . Then after our optimal transport-based dataset expansion, we have ", "page_idx": 4}, {"type": "text", "text": "$(I)$ For any $\\varepsilon\\ >\\ 0$ and large enough $\\sigma$ , we have $\\operatorname{CE}(P_{n},P)+\\varepsilon\\,\\geq\\,\\operatorname{CE}(Q,P)+C_{P}$ , where $\\begin{array}{r}{C_{P}=\\sum_{j=1}^{C}\\left(\\frac{1}{B}\\sum_{i=1}^{B}p_{i j}-\\frac{1}{C}\\right)\\cdot\\left(\\frac{1}{B}\\sum_{i=1}^{B}-\\log p_{i j}\\right)}\\end{array}$ and $\\begin{array}{r}{\\mathrm{CE}(Q,P)=\\sum_{i,j}\\mathrm{CE}(Q_{i j},P_{i j})}\\end{array}$ .   \n(2) Let $\\begin{array}{r l r}{L}&{{}=}&{(l_{i j})_{B\\times C}}\\end{array}$ denote the soft-version of true cell labels, where $\\begin{array}{r l}{l_{i j}}&{{}=}\\end{array}$ max $\\left\\{1_{\\{c e l l\\:i\\:b e l o n g s\\:t o\\:c l a s s\\,j\\}},\\delta\\right\\}$ and $\\delta\\ >\\ 0$ . Suppose $0\\;<\\;C_{\\mathrm{min}}\\;\\leq\\;l_{i j}/P_{i j}\\;\\leq\\;C_{\\mathrm{max}}$ for all $1\\;\\le\\;i\\;\\le\\;B$ and $1\\ \\leq\\ j\\ \\leq\\ C$ . Then, $\\operatorname{CE}(P_{n},L)\\,+\\,\\varepsilon\\,+\\,C_{L}\\ \\geq\\ \\operatorname{CE}(Q,L)\\,+\\,C_{P}$ , where $C_{L}=\\log C_{\\operatorname*{max}}-\\log C_{\\operatorname*{min}}$ . ", "page_idx": 4}, {"type": "text", "text": "Remarks. $C_{P}$ characterizes how far $P_{n}$ is from $\\textstyle\\prod({\\frac{1}{B}}\\mathbf{1}_{B},{\\frac{1}{C}}\\mathbf{1}_{C})$ . If the mini-batch was uniformly sampled across cell-types, then $\\textstyle{\\frac{1}{B}}\\sum_{i=1}^{B}p_{i j}$ should be close to $\\textstyle{\\frac{1}{C}}$ . Thus, $C_{P}$ will be close to zero. Moreover, for those hard classes, $p_{i j}$ \u2019s are close to $\\textstyle{\\frac{1}{C}}$ , hence both $\\textstyle{\\frac{1}{B}}\\sum_{i=1}^{B}p_{i j}$ and $\\textstyle\\sum_{i=1}^{B}-\\log p_{i j}$ will not be too large. Hence, on the whole, $C_{P}$ can be relatively small, which demonstrates that our optimization has the potential to achieve the desired distribution. ", "page_idx": 4}, {"type": "text", "text": "4.3 Divide and Conquer for Target scATAC-seq Data ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Through OT-based pseudo-labeling, we effectively address the label scarcity issue in scRNA-seq data. However, due to the heterogeneous gap, it would not be optimal when employing a similar approach for scATAC-seq data. Here, to tackle label scarcity in scATAC-seq data, we adopt a divideand-conquer learning strategy, which first divides the scATAC-seq data into source-like data and target-specific data and then utilizes separate approaches for optimization. ", "page_idx": 5}, {"type": "text", "text": "In particular, by comparing the confidence of the predictions and a threshold, we divide scATAC-seq data into two parts $\\dot{\\mathcal{D}}^{S L}$ and $\\mathcal{D}^{T S}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{D}^{S L}=\\{\\pmb{x}_{i}\\in\\mathcal{D}^{(t)}|\\operatorname*{max}_{c}p_{i}^{(t)}[c]>\\tau\\},\\mathcal{D}^{T S}=\\mathcal{D}^{(t)}/\\mathcal{S}^{S L},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $p_{i}^{(t)}$ is the predicted distribution of xi(t ). The threshold \u03c4 is used to control the size of two groups and set to 0.9 according to previous works [52, 36]. These source-like samples are more likely to have accurate pseudo-labels with high confidence using the pre-trained model. In contrast, target-specific data are far away from source data distribution and thus the predictions could be noisy. Therefore, we utilize separate strategies for these two parts. ", "page_idx": 5}, {"type": "text", "text": "Learning from Source-like Data. For these source-like samples, we utilize consistency regularization to provide supervision [4, 32, 50, 52, 85]. Here, we add random perturbations (e.g., Gaussian noise) to data, and then force the network to make consistent predictions for both original and perturbed scATAC-seq samples. In particular, we can use ${\\hat{y}}_{i}=a r g m a x(\\pmb{p}_{i})$ to obtain the pseudolabel of the original scATAC-seq sample. After that, we can get the prediction $\\tilde{\\pmb{p}}_{i}$ of the perturbed scATAC-seq sample. Thus, the supervised loss can be formulated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{S L}=-\\frac{1}{|\\mathcal{D}^{S L}|}\\sum_{i=1}^{|\\mathcal{D}^{S L}|}\\sum_{c=1}^{C}\\hat{y_{i}}^{c}l o g(\\tilde{p}_{i}^{c}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $|{\\mathcal{D}}^{S L}|$ represents the number of unlabeled scATAC-seq samples in $\\mathcal{D}^{S L}$ . In this way, we combine pseudo-labeling with consistency learning to learn from source-like scATAC-seq samples. ", "page_idx": 5}, {"type": "text", "text": "Learning from Target-specific scATAC-seq Data. However, due to cell heterogeneity between scRNA-seq and scATAC-seq data, many scATAC-seq samples would be far away from the source modality. They are difficult to predict with low confidence scores [71, 85]. To tackle this challenge, we introduce a soft way to learn from their ambiguous label sets instead of pseudo-labels. Here, for each target-specific scATAC-seq sample, we first select the top- $k$ classes with the highest probabilities to make up the ambiguous label set and then introduce a soft supervised loss for reliable guidance. ", "page_idx": 5}, {"type": "text", "text": "To be specific, we first recall the predicted distribution $\\textstyle p_{i}$ for each sample $\\pmb{x}_{i}$ in $\\mathcal{D}^{T S}$ . The negative cell types are removed with a significant difference from the cell type with the highest probability. In formulation, the removed labels should satisfy the following conditions: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{c}{p_{i}[c]}-{p_{i}[j]}\\geq\\mu,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the threshold $\\mu$ is initially set to $1e-3$ and gradually decreases during the training process. The left labels make up the candidate label set: ", "page_idx": 5}, {"type": "equation", "text": "$$\nY_{i}=\\{c\\in T o p k(\\mathbf{\\mathit{p}}_{i})|\\operatorname*{max}_{c}p_{i}[c]-\\mathbf{\\mathit{p}}_{i}[j]<\\mu\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Then, we employ a soft supervision loss to provide reliable guidance for target-specific samples: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{T S}=-\\frac{1}{\\mathcal{D}^{T S}}\\sum_{i=1}^{\\mathcal{D}^{T S}}\\sum_{j=1}^{k}\\alpha_{i j}\\boldsymbol{1}_{j\\in Y_{i}}l o g(\\pmb{p}_{i}[j]),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$$\n\\alpha_{i j}=\\left\\{\\begin{array}{c c}{\\displaystyle p_{i}[j]/\\sum_{j\\in Y_{i}}p_{i}[j],}&{j\\in Y_{i},}\\\\ {0,}&{o t h e r w i s e.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $|D^{T S}|$ denotes the number of target-specific samples. The weight $\\alpha_{i j}$ can allow the model to pay more attention to potential correct cell types with high probabilities. ", "page_idx": 5}, {"type": "text", "text": "Next, we provide the theoretical underpinnings to show that the quality of the soft labels from the candidate label set is higher than that of hard labels. The proof can be found in Appendix A. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.2. Suppose we randomly draw $n$ samples $(X_{1},\\cdot\\cdot\\cdot,X_{n})$ from two classes $\\{0,1\\}$ , where the proportion of class $^{\\,l}$ is $p$ . Further, suppose we can only observe a flawed version of these class labels $Z\\,=\\,(Z_{1},\\ldots,Z_{n})$ with error rate $\\beta\\,>\\,0$ . That is, $\\mathbb{P}(Z_{i}\\,\\neq\\,X_{i})\\,=\\,\\beta$ . Under loss $\\begin{array}{r}{\\mathcal{L}(p,z)=\\sum_{i=1}^{n}z_{i}\\log p+(n-\\sum_{i=1}^{n}z_{i})\\log(1-p)}\\end{array}$ , consider two estimates of $p$ : ", "page_idx": 6}, {"type": "text", "text": "Remarks. In Eqn. 10, if we take ${\\pmb p}_{i}[1]\\,=\\,p,{\\pmb p}_{i}[0]\\,=\\,1\\,-\\,p$ , and $\\alpha_{i j}\\,=\\,t$ if $j\\,=\\,Z_{i}$ , then the estimation under Eqn. 10 will degenerate to ptsoft. ", "page_idx": 6}, {"type": "text", "text": "4.4 Cross-omic Multi-sample Mixup for Cell Heterogeneity Reduction ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Despite the proposed techniques in addressing the label scarcity problem, reducing the heterogeneous gap between scRNA-seq data and scATAC-seq data remains an additional challenge. To tackle this challenge, previous works [27, 19, 35, 34, 70] propose many strategies for cross-modal alignment. However, these strategies have two major prerequisites. On the one hand, strategies based on centroid learning [27, 19] require accurate label information from both source and target modalities to aggregate samples of the same class. On the other hand, strategies based on cross-modal contrastive learning [35, 34, 82] require pairwise information. Neither of these prerequisites is applicable to our problem, as pairwise information is difficult to acquire in single-cell data, and the majority of annotation information is missing. Towards this end, we propose a sample-to-sample alignment strategy termed cross-omic multi-sample Mixup [81, 58, 86, 73], which first calculates the class-wise similarity and then mixes the scRNA-seq samples in the hidden space for cross-omic alignment. ", "page_idx": 6}, {"type": "text", "text": "In particular, we transform the intrinsic distribution of the source scRNA-seq samples to align with scATAC-seq samples in the hidden space. Since we cannot get accurate pairwise information, we calculate the similarity between scRNA-seq samples and scATAC-seq samples and generate virtual samples instead. Note that the weights of the classifier are considered as prototypes that contain representative information of different cell types. We first form a weight matrix $\\mathcal{W}\\,=$ $[{\\pmb w}_{1},{\\pmb w}_{2},\\,\\cdot\\,\\cdot\\,,{\\pmb w}_{C}]^{T}\\in\\mathbb{R}^{C\\times D}$ , where $D$ denotes the hidden dimension of the embeddings. Then, the class-wise similarity can be determined using cosine distance: ", "page_idx": 6}, {"type": "equation", "text": "$$\nS_{i j}=c o s(\\pmb{p}_{i}^{T}\\mathcal{W},\\pmb{p}_{j}^{T}\\mathcal{W})=|\\pmb{p}_{i}^{T}\\mathcal{W}|\\star|\\pmb{p}_{j}^{T}\\mathcal{W}|^{T},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\big|\\cdot\\big|$ represents the $L_{2}$ normalization. For a batch of $B^{(s)}$ scRNA-seq samples and a batch of $B^{(t)}$ scATAC-seq samples, we can obtain a similarity matrix $S^{B}\\in\\mathbb{R}^{B^{(s)}\\times B^{(t)}}$ using Eqn. 12. For the sake of simplicity, we let $B^{(s)}=B^{(t)}=B$ , and the similarity matrix within a mini-batch can be formulated as $S^{B^{\\prime}}\\in\\mathbb{R}^{B\\times B}$ . Afterwards, we employ the softmax function to convert $\\boldsymbol{S}$ into the weight matrix $\\mathcal{M}$ for Mixup, which corresponds to the probability of sharing the same semantics: ", "page_idx": 6}, {"type": "equation", "text": "$$\n{\\mathcal{M}}=s o f t m a x(S),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mathcal{M}_{i}$ collects the weights of $B$ scATAC-seq samples corresponding to the $i$ -th scRNA-seq sample. Then, we can fuse the scRNA-seq samples into a virtual sample through Mixup: ", "page_idx": 6}, {"type": "equation", "text": "$$\nz_{i}^{m,s}=\\sum_{j=1}^{B}\\mathcal{M}_{i j}\\cdot z_{j}^{s}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "After getting the fused scRNA-seq samples, we can perform a sample-to-sample alignment strategy. The loss function can be formulated as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{A}=\\frac{1}{B}\\sum_{i=1}^{B}\\Vert\\boldsymbol{z}_{i}^{m,s}-\\boldsymbol{z}_{i}^{t}\\Vert^{2}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "table", "img_path": "sKEhebkEdz/tmp/e7ec6a9e3112b7bd270cf755f70ef20a2e0db4e6184fcf1f66dbe558a6127a8d.jpg", "table_caption": ["Table 1: Quantitative comparisons $\\mathrm{(in}\\;\\%$ ) with state-of-the-art approaches with various label ratios. Bold numbers indicate beset results, and underlined numbers indicate second-beset results. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.5 Summarization ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In practice, we first warm up the network through labeled scRNA-seq data, and then conduct the training process in an end-to-end manner. The total loss function can be summarized as: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\mathcal{L}_{S}+\\mathcal{L}_{S L}+\\lambda\\mathcal{L}_{T S}+\\mathcal{L}_{A},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\lambda$ is a coefficient and set to 0.1 empirically. ", "page_idx": 7}, {"type": "text", "text": "The step-by-step training algorithm of our DANCE is summarized in Appendix B. ", "page_idx": 7}, {"type": "text", "text": "5 Experiment ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "5.1 Experimental Settings ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Datasets. To validate the effectiveness of the proposed DANCE, we conduct extensive experiments on several benchmark multi-omic single-cell datasets, with a brief introduction as follows: Mouse Atlas Data [74]. The multi-omics data can be accessed from the Tabula Muris mouse data3, along with the quantitative gene activity score matrix. In practice, the dataset is divided into multiple subsets based on the various sources of multi-omics data. CITE-ASAP PBMC Data [45]. It is derived from both control and stimulated conditions. CITE-seq encompasses both antibody-derived tag (ADT) matrices and gene expression matrices, while ASAP-seq allows us to access the ADT matrices and the chromatin accessibility matrices simultaneously. In this research, we extract the gene expression matrices of CITE-seq and the chromatin accessibility matrices of ASAP-seq to construct the scRNA-seq and scATAC-seq datasets. The statistics of these datasets can be seen in Appendix D. ", "page_idx": 7}, {"type": "text", "text": "Baselines. Various approaches are adopted for performance comparisons, including three domain adaptation methods (DAN [42], CDAN [43], and MCC [26]), one semi-supervised learning method (FixMatch [52]), and three latest multi-omic single-cell data integration methods (scJoint [38], scNCL [72], and scBridge [36]). A brief introduction of these approaches is provided in Appendix E. ", "page_idx": 7}, {"type": "table", "img_path": "sKEhebkEdz/tmp/a2b5dcd1fa39d8652b719ffc0a921fac6829ed9ebf2012807c910d32d054b603.jpg", "table_caption": ["Table 2: Ablation studies (in $\\%$ ) of DANCE in different settings. Bold numbers indicate beset results. "], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "sKEhebkEdz/tmp/0d18ae8c39345d33ae1a587f1562189ebdb939b095bf6f668b03fb76cd766d88.jpg", "img_caption": ["Figure 3: (a) The parameter sensitivity with respect to $k$ and $\\lambda$ on scRNA_SMARTer-ATAC with various label ratios. (b) The t-SNE visualization of two modalities (scRNA_10X_v2: red, ATAC: blue) with high label ratio. (c) The A-distance comparison with different label ratios. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.2 Experimental Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Performance Comparison. To assess the performance of various knowledge transfer methods, we conduct comprehensive quantitative experiments (Table 1) and qualitative experiments (Figure 2). From the results, several conclusions can be drawn: Firstly, consistent results from both quantitative and qualitative experiments reveal that DANCE significantly surpasses existing state-of-the-art approaches. We attribute this to the factor that existing methods often only address the issue of label scarcity in the target scATAC-seq data, overlooking the label scarcity issue in the source scRNAseq data. Consequently, when cell type annotations in scRNA-seq data are limited, not only does the discriminability of representations suffer, but also the heterogeneous gap cannot be effectively eliminated. DANCE excels in three main aspects. Firstly, it utilizes OT-based pseudo-labeling to augment the scRNA-seq dataset, effectively alleviating label scarcity in scRNA-seq data. Secondly, for scATAC-seq data, we employ a divide-and-conquer strategy with consistency regularization and ambiguous set learning for two subsets of samples, addressing label scarcity in scATAC-seq data. Additionally, our proposed cross-omic multi-sample Mixup mitigates cell heterogeneity in a sample-to-sample manner. By combining these aspects, DANCE significantly outperforms previous approaches by a large margin. Additionally, existing methods are data-hungry, heavily relying on cell type annotations. Therefore, when the number of labels decreases, the performance drops rapidly. In contrast, DANCE is data-efficient, capable of achieving excellent performance with fewer labels. Consequently, the performance decline due to label reduction is less pronounced, thanks to our proposed dual label scarcity elimination strategy. Due to the potential class imbalance issue in single-cell data, simply increasing the number of labels may not improve performance for some methods. For example, on the snRNA_SMARTer-snmC dataset, scBridge [36] achieves a performance of $36.15\\%$ under high label ratio, which is not better than that of $39.75\\%$ under mid label ratio. In contrast, our OT-based pseudo-labeling approach considers distributing pseudo-labels in a one-to-many manner, effectively preventing biased predictions. Therefore, on all datasets, increasing the number of labels leads to corresponding performance improvements for our DANCE. ", "page_idx": 8}, {"type": "text", "text": "Ablation Study. As depicted in Table 2, we conduct comprehensive experiments on different model variants to explore the contributions of each proposed module. DANCE w/o OP represents DANCE without the proposed OT-based pseudo-labeling. DANCE w/o SL denotes DANCE without the consistency regularization for source-like samples. DANCE w/o TS indicates DANCE without the ambiguous set learning for target-specific samples. DANCE w/o CM signifies DANCE without the cross-omic multi-sample Mixup. From the results, it can be observed that the performance of DANCE w/o OP significantly decreases. This indicates that OT-based pseudo-labeling effectively alleviates the label scarcity issue in scRNA-seq data by generating pseudo-labels for source scRNA-seq data and augmenting the scRNA-seq dataset. Additionally, the slight decrease in performance of DANCE w/o SL suggests that for heterogeneous scATAC-seq data, applying consistency regularization to source-like samples can further enhance model performance. Similarly, the minor decrease in performance of DANCE w/o TS suggests that target-specific samples should not be simply discarded as in previous methods. For these samples, collecting potentially correct cell types and removing them can provide weak supervision, thereby aiding performance improvement. Furthermore, the significant decrease in performance of DANCE w/o CM underscores the severity of the cell heterogeneity issue. Our proposed cross-omic multi-sample Mixup effectively eliminates the heterogeneous gap by generating mixed source scRNA-seq samples that simulate the distribution of target scATAC-seq data and aligning the two modalities. Thus, the effectiveness of all proposed modules is sufficiently validated. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Sensitivity Analysis. In Figure 3 (a), we analyze the sensitivity of two crucial hyper-parameters $k$ and $\\lambda$ on scRNA_SMARTer-ATAC. Firstly, we fix the other parameters to investigate the sensitivity of the model to different values of $k$ . Here, $k$ denotes the size of the partial label set and is used to control the number of potential correct cell types. As $k$ varies within the range $\\{2,3,4,5,6,7\\}$ , the model\u2019s performance initially increases before gradually stabilizing. Considering all three scenarios, the optimal performance is achieved at $k\\,=\\,4$ or $k\\,=\\,5$ . Next, with other parameters fixed, we analyze the sensitivity of the coefficient $\\lambda$ of $\\mathcal{L}_{T S}$ in Eqn. 10. Since $\\mathcal{L}_{T S}$ provides additional weak supervision by removing incorrect cell types, it typically requires a small coefficient. We change the value of $\\lambda$ within the range $\\left\\lbrace0.05,0.1,\\stackrel{\\cdot}{0.15},0.\\stackrel{\\cdot}{2},0.25,0.3\\right\\rbrace$ . Similarly, the model\u2019s performance firstly increases and then decreases, with better performance observed at $\\lambda=0.1$ . ", "page_idx": 9}, {"type": "text", "text": "t-SNE Visualization. In Figure 3 (b), we make the t-SNE [57] visualization to assess the heterogeneous gap of representations learned by scNCL [72] and our DANCE on scRNA_10X_v2-ATAC. Note that the higher overlap degree between the red and blue parts reflects the lower cell heterogeneity between scRNA-seq and scATAC-seq data. It can be observed that there is almost no overlap between the representations of scNCL [72], indicating that it does not address the issue of cell heterogeneity. In contrast, the representations of our DANCE exhibit a high degree of overlap, indicating that our approach effectively mitigates cell heterogeneity even in the presence of label scarcity. ", "page_idx": 9}, {"type": "text", "text": "A-distance Comparison. In Figure 3 (c), we compare the A-distance on scRNA_10X_v2-ATAC with various label ratios. The A-distance is calculated by employing a model and testing its ability to distinguish between the two modalities. Therefore, a low A-distance indicates a small heterogeneous gap. The A-distance is defined as: $d i s t_{A}=2(1-2\\epsilon)$ , where $\\epsilon$ is the test error. When $\\epsilon=0$ , the upper bound of $d i s t_{A}$ is 2. This indicates that there is a significant heterogeneous gap between the two modalities, and the model can easily differentiate between them. From the results, it can be observed that the A-distance of FixMatch [52] and scNCL [72] consistently remains close to 2, indicating a significant heterogeneity between scRNA-seq data and scATAC-seq data. In contrast, DANCE successfully reduces cell heterogeneity, resulting in a significant decrease in the A-distance. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we focus on a realistic yet underexplored problem of knowledge transfer across multiomic single-cell data under label scarcity in both scRNA-seq and scATAC-seq data, and propose a semi-supervised framework termed DANCE for this problem. Specifically, DANCE operates dual label scarcity elimination by OT-based dataset expansion for scRNA-seq data and a divide-and-conquer strategy for scATAC-seq data. DANCE further introduces a sample-to-sample alignment strategy of cross-omic multi-sample Mixup to reduce cell heterogeneity. Extensive experiments on various datasets verify the superiority of DANCE in comparison to many state-of-the-art approaches. ", "page_idx": 9}, {"type": "text", "text": "Broader Impacts and Limitations. This study addresses the challenge of knowledge transfer across multi-omic single-cell data, which has significant potential for advancing biological research by reducing annotation costs in both source and target biological data. However, it is important to acknowledge that this work represents an initial exploration of this field and may have certain limitations. For instance, our method may not perform optimally in complex real-world scenarios, including open-set knowledge transfer and knowledge transfer under label noise. Additionally, this study primarily focuses on static scenarios, while there are complex dynamic applications, such as single-cell RNA velocity inference, that require further investigation. In future research, our goal is to address these challenges and expand our approach to encompass more generalized scenarios. To the best of our knowledge, no potential negative impacts resulting from our work have been identified. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Matthew Amodio and Smita Krishnaswamy. Magan: Aligning biological manifolds. In International conference on machine learning, pages 215\u2013223. PMLR, 2018. [2] Matthew Amodio, David Van Dijk, Krishnan Srinivasan, William S Chen, Hussein Mohsen, Kevin R Moon, Allison Campbell, Yujiao Zhao, Xiaomei Wang, Manjunatha Venkataswamy, et al. Exploring single-cell data with deep multitasking neural networks. Nature methods, 16(11):1139\u20131145, 2019.   \n[3] Ricard Argelaguet, Damien Arnol, Danila Bredikhin, Yonatan Deloro, Britta Velten, John C Marioni, and Oliver Stegle. Mofa $^{+}$ : a statistical framework for comprehensive integration of multi-modal single-cell data. Genome biology, 21(1):1\u201317, 2020.   \n[4] Philip Bachman, Ouais Alsharif, and Doina Precup. Learning with pseudo-ensembles. Advances in neural information processing systems, 27, 2014.   \n[5] Nikolas Barkas, Viktor Petukhov, Daria Nikolaeva, Yaroslav Lozinsky, Samuel Demharter, Konstantin Khodosevich, and Peter V Kharchenko. Joint analysis of heterogeneous single-cell rna-seq dataset collections. Nature methods, 16(8):695\u2013698, 2019.   \n[6] David Berthelot, Nicholas Carlini, Ekin D. Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, and Colin Raffel. Remixmatch: Semi-supervised learning with distribution matching and augmentation anchoring. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.   \n[7] David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raffel. Mixmatch: A holistic approach to semi-supervised learning. Advances in neural information processing systems, 32, 2019.   \n[8] Zhi-Jie Cao and Ge Gao. Multi-omics single-cell data integration and regulatory inference with graph-linked embedding. Nature Biotechnology, 40(10):1458\u20131466, 2022.   \n[9] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. Advances in neural information processing systems, 33:9912\u20139924, 2020.   \n[10] Paola Cascante-Bonilla, Fuwen Tan, Yanjun Qi, and Vicente Ordonez. Curriculum labeling: Revisiting pseudo-labeling for semi-supervised learning. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 6912\u20136920, 2021.   \n[11] Wanxing Chang, Ye Shi, Hoang Tuan, and Jingya Wang. Unified optimal transport framework for universal domain adaptation. Advances in Neural Information Processing Systems, 35:29512\u2013 29524, 2022.   \n[12] Wanxing Chang, Ye Shi, and Jingya Wang. Csot: Curriculum and structure-aware optimal transport for learning with noisy labels. Advances in Neural Information Processing Systems, 36:8528\u20138541, 2023.   \n[13] Deli Chen, Yankai Lin, Guangxiang Zhao, Xuancheng Ren, Peng Li, Jie Zhou, and Xu Sun. Topology-imbalance learning for semi-supervised node classification. Advances in Neural Information Processing Systems, 34:29885\u201329897, 2021.   \n[14] Duowen Chen, Yunhao Bai, Wei Shen, Qingli Li, Lequan Yu, and Yan Wang. Magicnet: Semisupervised multi-organ segmentation via magic-cube partition and recovery. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 23869\u201323878, 2023.   \n[15] Huidong Chen, Caleb Lareau, Tommaso Andreani, Michael E Vinyard, Sara P Garcia, Kendell Clement, Miguel A Andrade-Navarro, Jason D Buenrostro, and Luca Pinello. Assessment of computational methods for the analysis of single-cell atac-seq data. Genome biology, 20(1):1\u201325, 2019.   \n[16] Jiawei Chen, Hao Xu, Wanyu Tao, Zhaoxiong Chen, Yuxuan Zhao, and Jing-Dong J Han. Transformer for one stop interpretable cell type annotation. Nature Communications, 14(1):223, 2023.   \n[17] Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in neural information processing systems, 26, 2013.   \n[18] Zhana Duren, Xi Chen, Mahdi Zamanighomi, Wanwen Zeng, Ansuman T Satpathy, Howard Y Chang, Yong Wang, and Wing Hung Wong. Integrative analysis of single-cell genomics data by coupled nonnegative matrix factorizations. Proceedings of the National Academy of Sciences, 115(30):7723\u20137728, 2018.   \n[19] Yanglin Feng, Hongyuan Zhu, Dezhong Peng, Xi Peng, and Peng Hu. Rono: Robust discriminative learning with noisy labels for 2d-3d cross-modal retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11610\u201311619, 2023.   \n[20] Rohan Gala, Nathan Gouwens, Zizhen Yao, Agata Budzillo, Osnat Penn, Bosiljka Tasic, Gabe Murphy, Hongkui Zeng, and Uygar S\u00fcmb\u00fcl. A coupled autoencoder approach for multi-modal analysis of cell types. Advances in Neural Information Processing Systems, 32, 2019.   \n[21] Boying Gong, Yun Zhou, and Elizabeth Purdom. Cobolt: integrative analysis of multimodal single-cell sequencing data. Genome biology, 22(1):1\u201321, 2021.   \n[22] Chen Gong, Dacheng Tao, Stephen J Maybank, Wei Liu, Guoliang Kang, and Jie Yang. Multimodal curriculum learning for semi-supervised image classification. IEEE Transactions on Image Processing, 25(7):3249\u20133260, 2016.   \n[23] Zijian Hu, Zhengyu Yang, Xuefeng Hu, and Ram Nevatia. Simple: Similar pseudo label exploitation for semi-supervised classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15099\u201315108, 2021.   \n[24] Wei Hua, Dingkang Liang, Jingyu Li, Xiaolong Liu, Zhikang Zou, Xiaoqing Ye, and Xiang Bai. Sood: Towards semi-supervised oriented object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15558\u201315567, 2023.   \n[25] Suoqin Jin, Lihua Zhang, and Qing Nie. scai: an unsupervised approach for the integrative analysis of parallel single-cell transcriptomic and epigenomic proflies. Genome biology, 21:1\u2013 19, 2020.   \n[26] Ying Jin, Ximei Wang, Mingsheng Long, and Jianmin Wang. Minimum class confusion for versatile domain adaptation. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XXI 16, pages 464\u2013480. Springer, 2020.   \n[27] Longlong Jing, Elahe Vahdani, Jiaxing Tan, and Yingli Tian. Cross-modal center loss for 3d cross-modal retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3142\u20133151, 2021.   \n[28] Zhanghan Ke, Daoye Wang, Qiong Yan, Jimmy Ren, and Rynson WH Lau. Dual student: Breaking the limits of the teacher in semi-supervised learning. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6728\u20136736, 2019.   \n[29] Hoel Kervadec, Jose Dolz, Eric Granger, and Ismail Ben Ayed. Curriculum semi-supervised segmentation. In Medical Image Computing and Computer Assisted Intervention\u2013MICCAI 2019: 22nd International Conference, Shenzhen, China, October 13\u201317, 2019, Proceedings, Part II 22, pages 568\u2013576. Springer, 2019.   \n[30] Ilya Korsunsky, Nghia Millard, Jean Fan, Kamil Slowikowski, Fan Zhang, Kevin Wei, Yuriy Baglaenko, Michael Brenner, Po-ru Loh, and Soumya Raychaudhuri. Fast, sensitive and accurate integration of single-cell data with harmony. Nature methods, 16(12):1289\u20131296, 2019.   \n[31] Zhengfeng Lai, Noranart Vesdapunt, Ning Zhou, Jun Wu, Cong Phuoc Huynh, Xuelu Li, Kah Kuen Fu, and Chen-Nee Chuah. Padclip: Pseudo-labeling with adaptive debiasing in clip for unsupervised domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 16155\u201316165, 2023.   \n[32] Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017.   \n[33] Hangyu Li, Nannan Wang, Xi Yang, Xiaoyu Wang, and Xinbo Gao. Towards semi-supervised deep facial expression recognition with an adaptive confidence margin. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4166\u20134175, 2022.   \n[34] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping languageimage pre-training for unified vision-language understanding and generation. In International Conference on Machine Learning, pages 12888\u201312900. PMLR, 2022.   \n[35] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. Align before fuse: Vision and language representation learning with momentum distillation. Advances in neural information processing systems, 34:9694\u20139705, 2021.   \n[36] Yunfan Li, Dan Zhang, Mouxing Yang, Dezhong Peng, Jun Yu, Yu Liu, Jiancheng Lv, Lu Chen, and Xi Peng. scbridge embraces cell heterogeneity in single-cell rna-seq and atac-seq data integration. Nature Communications, 14(1):6045, 2023.   \n[37] Yingxin Lin, Shila Ghazanfar, Kevin YX Wang, Johann A Gagnon-Bartsch, Kitty K Lo, Xianbin Su, Ze-Guang Han, John T Ormerod, Terence P Speed, Pengyi Yang, et al. scmerge leverages factor analysis, stable expression, and pseudoreplication to merge multiple single-cell rna-seq datasets. Proceedings of the National Academy of Sciences, 116(20):9775\u20139784, 2019.   \n[38] Yingxin Lin, Tung-Yu Wu, Sheng Wan, Jean YH Yang, Wing H Wong, and YX Rachel Wang. scjoint integrates atlas-scale single-cell rna-seq and atac-seq data with transfer learning. Nature biotechnology, 40(5):703\u2013710, 2022.   \n[39] Jie Liu, Yuanhao Huang, Ritambhara Singh, Jean-Philippe Vert, and William Stafford Noble. Jointly embedding multiple single-cell omics measurements. In Algorithms in bioinformatics:... International Workshop, WABI..., proceedings. WABI (Workshop), volume 143. NIH Public Access, 2019.   \n[40] Tianyu Liu, Yuge Wang, Rex Ying, and Hongyu Zhao. Muse-gnn: learning unified gene representation from multimodal biological graph data. Advances in neural information processing systems, 36, 2024.   \n[41] Yang Liu, Zhipeng Zhou, and Baigui Sun. Cot: Unsupervised domain adaptation with clustering and optimal transport. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 19998\u201320007, 2023.   \n[42] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. Learning transferable features with deep adaptation networks. In International conference on machine learning, pages 97\u2013105. PMLR, 2015.   \n[43] Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I Jordan. Conditional adversarial domain adaptation. Advances in neural information processing systems, 31, 2018.   \n[44] Romain Lopez, Jeffrey Regier, Michael B Cole, Michael I Jordan, and Nir Yosef. Deep generative modeling for single-cell transcriptomics. Nature methods, 15(12):1053\u20131058, 2018.   \n[45] Eleni P Mimitou, Caleb A Lareau, Kelvin Y Chen, Andre L Zorzetto-Fernandes, Yuhan Hao, Yusuke Takeshima, Wendy Luo, Tse-Shun Huang, Bertrand Z Yeung, Efthymia Papalexi, et al. Scalable, multimodal profliing of chromatin accessibility, gene expression and protein levels in single cells. Nature biotechnology, 39(10):1246\u20131258, 2021.   \n[46] Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. IEEE transactions on pattern analysis and machine intelligence, 41(8):1979\u20131993, 2018.   \n[47] Yassine Ouali, C\u00e9line Hudelot, and Myriam Tami. Semi-supervised semantic segmentation with cross-consistency training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12674\u201312684, 2020.   \n[48] Pengchong Qiao, Zhidan Wei, Yu Wang, Zhennan Wang, Guoli Song, Fan Xu, Xiangyang Ji, Chang Liu, and Jie Chen. Fuzzy positive learning for semi-supervised semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15465\u201315474, 2023.   \n[49] Orit Rozenblatt-Rosen, Michael JT Stubbington, Aviv Regev, and Sarah A Teichmann. The human cell atlas: from vision to reality. Nature, 550(7677):451\u2013453, 2017.   \n[50] Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with stochastic transformations and perturbations for deep semi-supervised learning. Proceedings of the Conference on Neural Information Processing Systems, 2016.   \n[51] Ronglai Shen, Adam B Olshen, and Marc Ladanyi. Integrative clustering of multiple genomic data types using a joint latent variable model with application to breast and lung cancer subtype analysis. Bioinformatics, 25(22):2906\u20132912, 2009.   \n[52] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semisupervised learning with consistency and confidence. In Proceedings of the Conference on Neural Information Processing Systems, 2020.   \n[53] Genevieve L Stein-O\u2019Brien, Raman Arora, Aedin C Culhane, Alexander V Favorov, Lana X Garmire, Casey S Greene, Loyal A Goff, Yifeng Li, Aloune Ngom, Michael F Ochs, et al. Enter the matrix: factorization uncovers knowledge from omics. Trends in Genetics, 34(10):790\u2013805, 2018.   \n[54] Tim Stuart, Andrew Butler, Paul Hoffman, Christoph Hafemeister, Efthymia Papalexi, William M Mauck, Yuhan Hao, Marlon Stoeckius, Peter Smibert, and Rahul Satija. Comprehensive integration of single-cell data. Cell, 177(7):1888\u20131902, 2019.   \n[55] Barbara Treutlein, Doug G Brownfield, Angela R Wu, Norma F Neff, Gary L Mantalas, F Hernan Espinoza, Tushar J Desai, Mark A Krasnow, and Stephen R Quake. Reconstructing lineage hierarchies of the distal lung epithelium using single-cell rna-seq. Nature, 509(7500):371\u2013375, 2014.   \n[56] Xinming Tu, Zhi-Jie Cao, Sara Mostafavi, Ge Gao, et al. Cross-linked unified embedding for cross-modality representation learning. Advances in Neural Information Processing Systems, 35:15942\u201315955, 2022.   \n[57] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008.   \n[58] Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najaf,i Ioannis Mitliagkas, David LopezPaz, and Yoshua Bengio. Manifold mixup: Better representations by interpolating hidden states. In International conference on machine learning, pages 6438\u20136447. PMLR, 2019.   \n[59] Jingshu Wang, Divyansh Agarwal, Mo Huang, Gang Hu, Zilu Zhou, Chengzhong Ye, and Nancy R Zhang. Data denoising with transfer learning in single-cell transcriptomics. Nature methods, 16(9):875\u2013878, 2019.   \n[60] Juexin Wang, Anjun Ma, Yuzhou Chang, Jianting Gong, Yuexu Jiang, Ren Qi, Cankun Wang, Hongjun Fu, Qin Ma, and Dong Xu. scgnn is a novel graph neural network framework for single-cell rna-seq analyses. Nature communications, 12(1):1882, 2021.   \n[61] Tongxin Wang, Travis S Johnson, Wei Shao, Zixiao Lu, Bryan R Helm, Jie Zhang, and Kun Huang. Bermuda: a novel deep transfer learning method for single-cell rna sequencing batch correction reveals hidden high-resolution cellular subtypes. Genome biology, 20(1):1\u201315, 2019.   \n[62] Yiwei Wang, Wei Wang, Yuxuan Liang, Yujun Cai, Juncheng Liu, and Bryan Hooi. Nodeaug: Semi-supervised node classification with data augmentation. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 207\u2013217, 2020.   \n[63] Joshua D Welch, Alexander J Hartemink, and Jan F Prins. Matcher: manifold alignment reveals correspondence between single cell transcriptome and epigenome dynamics. Genome biology, 18(1):1\u201319, 2017.   \n[64] Joshua D Welch, Velina Kozareva, Ashley Ferreira, Charles Vanderburg, Carly Martin, and Evan Z Macosko. Single-cell multi-omic integration compares and contrasts features of brain cell identity. Cell, 177(7):1873\u20131887, 2019.   \n[65] Hongzhi Wen, Jiayuan Ding, Wei Jin, Yiqi Wang, Yuying Xie, and Jiliang Tang. Graph neural networks for multimodal single-cell data integration. In Proceedings of the 28th ACM SIGKDD conference on knowledge discovery and data mining, pages 4153\u20134163, 2022.   \n[66] Kevin E Wu, Kathryn E Yost, Howard Y Chang, and James Zou. Babel enables cross-modality translation between multiomic profiles at single-cell resolution. Proceedings of the National Academy of Sciences, 118(15):e2023070118, 2021.   \n[67] Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and Quoc Le. Unsupervised data augmentation for consistency training. Advances in neural information processing systems, 33:6256\u20136268, 2020.   \n[68] Lei Xiong, Tianlong Chen, and Manolis Kellis. scclip: Multi-modal single-cell contrastive learning integration pre-training. In NeurIPS 2023 AI for Science Workshop.   \n[69] Lei Xiong, Kui Xu, Kang Tian, Yanqiu Shao, Lei Tang, Ge Gao, Michael Zhang, Tao Jiang, and Qiangfeng Cliff Zhang. Scale method for single-cell atac-seq analysis via latent feature extraction. Nature communications, 10(1):4576, 2019.   \n[70] Yizhe Xiong, Hui Chen, Zijia Lin, Sicheng Zhao, and Guiguang Ding. Confidence-based visual dispersal for few-shot unsupervised domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11621\u201311631, 2023.   \n[71] Ning Xu, Biao Liu, Jiaqi Lv, Congyu Qiao, and Xin Geng. Progressive purification for instancedependent partial label learning. In International Conference on Machine Learning, pages 38551\u201338565. PMLR, 2023.   \n[72] Xuhua Yan, Ruiqing Zheng, Jinmiao Chen, and Min Li. scncl: transferring labels from scrna-seq to scatac-seq data with neighborhood contrastive regularization. Bioinformatics, 39(8):btad505, 2023.   \n[73] Xinlong Yang, Haixin Wang, Jinan Sun, Shikun Zhang, Chong Chen, Xian-Sheng Hua, and Xiao Luo. Prototypical mixing and retrieval-based refinement for label noise-resistant image retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11239\u201311249, 2023.   \n[74] Zizhen Yao, Hanqing Liu, Fangming Xie, Stephan Fischer, Ricky S Adkins, Andrew I Aldridge, Seth A Ament, Anna Bartlett, M Margarita Behrens, Koen Van den Berge, et al. A transcriptomic and epigenomic cell atlas of the mouse primary motor cortex. Nature, 598(7879):103\u2013110, 2021.   \n[75] Wenbao Yu, Yasin Uzun, Qin Zhu, Changya Chen, and Kai Tan. scatac-pro: a comprehensive workbench for single-cell chromatin accessibility sequencing data. Genome biology, 21(1):1\u201317, 2020.   \n[76] Yu-Chu Yu and Hsuan-Tien Lin. Semi-supervised domain adaptation with source label adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 24100\u201324109, 2023.   \n[77] Yuyao Zhai, Liang Chen, and Minghua Deng. Generalized cell type annotation and discovery for single-cell rna-seq data. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 5402\u20135410, 2023.   \n[78] Bowen Zhang, Yidong Wang, Wenxin Hou, Hao Wu, Jindong Wang, Manabu Okumura, and Takahiro Shinozaki. Flexmatch: Boosting semi-supervised learning with curriculum pseudo labeling. In Proceedings of the Conference on Neural Information Processing Systems, pages 18408\u201318419, 2021.   \n[79] Fan Zhang, Xian-Sheng Hua, Chong Chen, and Xiao Luo. Fine-grained prototypical voting with heterogeneous mixup for semi-supervised 2d-3d cross-modal retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17016\u201317026, 2024.   \n[80] Jiacheng Zhang, Xiangru Lin, Wei Zhang, Kuo Wang, Xiao Tan, Junyu Han, Errui Ding, Jingdong Wang, and Guanbin Li. Semi-detr: Semi-supervised object detection with detection transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 23809\u201323818, 2023.   \n[81] Shaofeng Zhang, Meng Liu, Junchi Yan, Hengrui Zhang, Lingxiao Huang, Xiaokang Yang, and Pinyan Lu. M-mix: Generating hard negatives via multi-sample mixing for contrastive learning. In Proceedings of the 28th ACM SIGKDD conference on knowledge discovery and data mining, pages 2461\u20132470, 2022.   \n[82] Yixin Zhang, Zilei Wang, Junjie Li, Jiafan Zhuang, and Zihan Lin. Towards effective instance discrimination contrastive loss for unsupervised domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11388\u201311399, 2023.   \n[83] Youshan Zhang and Brian D Davison. Efficient pre-trained features and recurrent pseudolabeling in unsupervised domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2719\u20132728, 2021.   \n[84] Ziqi Zhang, Haoran Sun, Ragunathan Mariappan, Xi Chen, Xinyu Chen, Mika S Jain, Mirjana Efremova, Sarah A Teichmann, Vaibhav Rajan, and Xiuwei Zhang. scmomat jointly performs single cell mosaic integration and multi-modal bio-marker detection. Nature Communications, 14(1):384, 2023.   \n[85] Ziyi Zhang, Weikai Chen, Hui Cheng, Zhen Li, Siyuan Li, Liang Lin, and Guanbin Li. Divide and contrast: Source-free domain adaptation via adaptive contrastive learning. Advances in Neural Information Processing Systems, 35:5137\u20135149, 2022.   \n[86] Yin Zhao, Longjun Cai, et al. Reducing the covariate shift by mirror samples in cross domain alignment. Advances in Neural Information Processing Systems, 34:9546\u20139558, 2021.   \n[87] Qinqing Zheng, Mikael Henaff, Brandon Amos, and Aditya Grover. Semi-supervised offline reinforcement learning with action-free trajectories. In International conference on machine learning, pages 42339\u201342362. PMLR, 2023.   \n[88] Ruiqing Zheng, Min Li, Zhenlan Liang, Fang-Xiang Wu, Yi Pan, and Jianxin Wang. Sinnlrr: a robust subspace clustering method for cell type detection by non-negative and low-rank representation. Bioinformatics, 35(19):3642\u20133650, 2019. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "A Proof of Theorem ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof of Theorem 4.1. (1) Let $\\hat{P}$ be the projection of $P_{n}$ onto $\\textstyle\\prod({\\frac{1}{B}}\\mathbf{1}_{B},{\\frac{1}{C}}\\mathbf{1}_{C})$ . Further, let $\\hat{Q}$ be a solution of the Kantorovitch\u2019s problem ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{Q\\in\\prod(\\frac{1}{B}\\mathbf{1}_{B},\\frac{1}{C}\\mathbf{1}_{C})}\\sum_{i,j}-Q_{i j}\\log P_{i j}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, by definition, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left\\langle\\hat{\\cal P},-\\log{\\cal P}\\right\\rangle\\geq\\left\\langle\\hat{\\cal Q},-\\log{\\cal P}\\right\\rangle,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\langle\\cdot,\\cdot\\rangle$ is the standard matrix inner product. From definition, we know that $\\hat{P}$ is the solution of ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{min}_{\\hat{\\boldsymbol{P}}}}&{\\displaystyle\\frac{1}{2}\\left\\|\\hat{\\boldsymbol{P}}-\\boldsymbol{P}_{\\!n}\\right\\|_{\\cal F}^{2}}\\\\ &{\\mathrm{s.t.}\\quad\\hat{P}\\mathbf{1}_{C}=\\displaystyle\\frac{1}{B}\\mathbf{1}_{B},\\quad\\hat{P}^{T}\\mathbf{1}_{B}=\\frac{1}{C}\\mathbf{1}_{C}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Using Lagrange multiplier, we know that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{P}=P_{n}-\\mathbf{1}_{B}\\beta^{T},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\beta=(\\beta_{1},\\ldots,\\beta_{C})^{T}$ and $\\begin{array}{r}{\\beta_{j}=\\frac{1}{B}\\left(\\frac{1}{B}\\sum_{i=1}^{B}p_{i j}-\\frac{1}{C}\\right).}\\end{array}$ . Therefore, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left\\langle P_{n}-{\\hat{P}},-\\log P\\right\\rangle=C_{P}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Further, using standard OT theory, we know that as $\\sigma\\rightarrow\\infty$ , $Q$ converges to the solution of the problem in Eqn. 17 with maximal entropy, $Q_{\\infty}$ . Therefore, for large enough $\\sigma$ , we know that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|Q-Q_{\\infty}\\|_{F}\\leq\\varepsilon/\\left\\|-\\log P\\right\\|_{F}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "So, using Cauchy\u2019s inequality, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n|\\langle Q-Q_{\\infty},-\\log P\\rangle|\\leq\\varepsilon.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Combining Eqn. 18-23 and take $\\hat{Q}=Q_{\\infty}$ in Eqn. 18, we get ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\langle P_{n},-\\log P\\rangle\\geq\\langle Q,-\\log P\\rangle+C_{P}-\\varepsilon.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname{CE}(P_{n},P)+\\varepsilon\\geq\\operatorname{CE}(Q,P)+C_{P}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "(2) For simplicity, we denote $\\begin{array}{r}{\\frac{L}{P}:=(L_{i j}/P_{i j})_{B\\times C}}\\end{array}$ . Then, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\left\\langle P_{n}-Q,-\\log\\displaystyle\\frac{L}{P}\\right\\rangle=\\displaystyle\\frac{1}{B}\\sum_{}P_{i j}\\cdot\\left(-\\log\\displaystyle\\frac{L_{i j}}{P_{i j}}\\right)-\\sum Q_{i j}\\cdot\\left(-\\log\\displaystyle\\frac{L_{i j}}{P_{i j}}\\right)}}\\\\ {{\\geq\\log C_{\\operatorname*{min}}-\\log C_{\\operatorname*{max}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Hence, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\langle P_{n},-\\log L\\rangle\\geq\\langle Q,-\\log L\\rangle+C_{P}-\\varepsilon-C_{L}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof of Theorem 4.2. From the definition, we know that ", "page_idx": 16}, {"type": "equation", "text": "$$\np_{\\mathrm{hard}}=\\frac{1}{n}\\sum_{i=1}^{n}Z_{i},\\;p_{\\mathrm{soft}}^{t}=\\frac{1}{n}\\sum_{i=1}^{n}\\left\\{t Z_{i}+(1-t)(1-Z_{i})\\right\\}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, E $p_{\\mathrm{hard}}=\\mathbb{E}Z_{1}=p+\\beta-2p\\beta,\\operatorname{Var}(p_{\\mathrm{hard}})=\\operatorname{Var}(Z_{1})/n,\\mathbb{E}p_{\\mathrm{soft}}^{t}=(1-t)+(2t-1)\\mathbb{E}Z_{1}$ , and $\\mathrm{Var}(p_{\\mathrm{soft}}^{t})=(2t-1)^{2}\\mathrm{Var}(Z_{1})/n$ . Hence, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\mathbb{E}\\left\\|p_{\\mathrm{hard}}-p\\right\\|^{2}=(\\mathbb{E}Z_{1}-p)^{2}+\\displaystyle\\frac{\\mathrm{Var}(Z_{1})}{n},}\\\\ {\\mathbb{E}\\left\\|p_{\\mathrm{soft}}^{t}-p\\right\\|^{2}=\\{(1-t)+(2t-1)\\mathbb{E}Z_{1}-p\\}^{2}+\\displaystyle\\frac{(2t-1)^{2}\\mathrm{Var}(Z_{1})}{n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\nf(t)=\\mathbb{E}\\left\\|p_{\\mathrm{hard}}-p\\right\\|^{2}-\\mathbb{E}\\left\\|p_{\\mathrm{soft}}^{t}-p\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then, from definition, we know that $f(1)=0$ . Further, we know ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f^{\\prime}(t)=-2\\left\\{(1-t)+(2t-1)\\mathbb{E}Z_{1}-p\\right\\}\\cdot(2\\mathbb{E}Z_{1}-1)+(4-8t)\\frac{\\mathrm{Var}(Z_{1})}{n}}\\\\ &{\\qquad=-\\left\\{2(1-2\\mathbb{E}Z_{1})^{2}+\\frac{8\\mathrm{Var}(Z_{1})}{n}\\right\\}t+\\left\\{\\frac{4\\mathrm{Var}(Z_{1})}{n}+2(1-2\\mathbb{E}Z_{1})(1-\\mathbb{E}Z_{1}-p)\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since $\\begin{array}{r}{2(1-2\\mathbb{E}Z_{1})^{2}+\\frac{8\\mathrm{Var}(Z_{1})}{n}>0}\\end{array}$ , we know that there exists ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{t_{0}=\\frac{\\frac{4\\mathrm{Var}(Z_{1})}{n}+2(1-2\\mathbb{E}Z_{1})(1-\\mathbb{E}Z_{1}-p)}{2(1-2\\mathbb{E}Z_{1})^{2}+\\frac{8\\mathrm{Var}(Z_{1})}{n}}}\\\\ &{\\quad=\\frac{\\frac{4\\mathrm{Var}(Z_{1})}{n}+2(1-2p)^{2}(1+2\\beta)(1-\\beta)}{\\frac{8\\mathrm{Var}(Z_{1})}{n}+2(1-2p)^{2}(1+2\\beta)^{2}}\\in(0,1),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "such that $f^{\\prime}(t)<0$ when $t\\in(c_{0},1)$ . So, $f(t)\\geq f(1)=0$ for $t\\in(t_{0},1)$ . Hence, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left\\|p_{\\mathrm{soft}}^{t}-p\\right\\|^{2}\\leq\\mathbb{E}\\left\\|p_{\\mathrm{hard}}-p\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "B Algorithms ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The step-by-step training algorithm of our DANCE is summarized in Algorithm 1. The model can get extra supervision from target-specific scATAC-seq data by removing the incorrect types from the candidate label set, the algorithm is provided in Algorithm 2. ", "page_idx": 17}, {"type": "text", "text": "Algorithm 1 Training Algorithm of DANCE ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Require: The training datasets $\\mathcal{D}^{(s),l}$ , $\\mathcal{D}^{(s),u}$ and $\\mathcal{D}^{(t)}$ .   \nEnsure: The network parameters in $H(F(\\cdot))$ ;   \n1: Warm up the network using $\\mathcal{D}^{(s),l}$ ;   \n2: repeat   \n3: Update the pseudo-labels using Eqn. 3 and Eqn. 4;   \n4: Expanding $\\bar{D}^{(s)}$ using Eqn. 5;   \n5: for $t=1,2,\\cdots,T$ do   \n6: Sample a mini-batch from the training datasets;   \n7: Generate the predictions for both scRNA-seq data and scATAC-seq data by propagating the network;   \n8: Calculate the final loss using Eqn. 16;   \n9: Update the parameters of the network using backpropagation;   \n10: end for   \n11: until convergence ", "page_idx": 17}, {"type": "text", "text": "C Implementation Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "All the baselines are re-implemented on NVIDIA Tesla A100 40G GPUs using PyTorch according to the original settings in the corresponding papers to ensure a fair comparison. A two-layer MLP with an embedding dimension of 64 is employed as the encoder, and another two-layer MLP is adopted as the classifier. For all the baselines and our DANCE, we first warm up the model with labeled scRNA-seq data for 30 epochs and then train the model for another 30 epochs with a batch size of 32. Three settings with different label ratios (low: $1\\%$ , mid: $5\\%$ , high: $10\\%$ ) are set up for experiments on each dataset to validate the sensitivity of these methods to the number of labels. We opt for SGD as the default optimizer with a learning rate of $3e-3$ and a weight decay of $1e-3$ . ", "page_idx": 17}, {"type": "text", "text": "Require: Target-specific scATAC-seq dataset D ; Threshold $\\mathcal{D}^{T S}$ $\\mu$ ; The number of classes $k$ .   \nEnsure: The network parameters in $H(F(\\cdot))$ ; repeat for $t=1,2,\\cdots,T$ do Sample a mini-batch of $B^{T S}$ samples from $\\mathcal{D}^{T S}$ ; Generate the softmax predictions for scRNA-seq data by propagating the network; Select the top- $k$ classes from the predictions to form $B^{\\check{T}\\check{C}}$ label sets; for $i=1,2,\\bar{\\cdot}\\cdot\\cdot\\,,B^{T C}$ do for $y_{i}^{j}\\in Y_{i}$ do if Eqn. 8 is satisfied then Removing cell type $y_{i}^{j}$ from $Y_{i}$ ; end if end for end for Providing extra guidance by loss Eqn. 10; Decrease the threshold $\\mu$ ; end for until convergence ", "page_idx": 18}, {"type": "text", "text": "D Introduction of Datasets ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We summarize the dataset information included in experiments in Table 3: ", "page_idx": 18}, {"type": "text", "text": "Table 3: The statistics of the multi-omic single-cell datasets used to conduct experiments. ", "page_idx": 18}, {"type": "table", "img_path": "sKEhebkEdz/tmp/367691a065cc35d603b93fbaa2ad24741e3efc359b83cc06902522cbd74d34f7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "E Introduction of Baselines ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Here we provide a brief introduction of the compared baseline methods as follows: ", "page_idx": 18}, {"type": "text", "text": "\u2022 DAN [42] is a deep neural network (DNN) based method for domain adaptation, which adapts the layers and the corresponding task-specific features in a layer-wise manner.   \n\u2022 CDAN [43] incorporates the discriminative information of the predictions from the classifier with adversarial learning for domain adaptation.   \n\u2022 MCC [26] is a strong domain adaptation method that achieves fast convergence speed without the explicit use of domain alignment.   \n\u2022 FixMatch [52] is a strong semi-supervised learning approach that hybrids pseudo-labeling and consistency regularization for self-training.   \n\u2022 scJoint [38] is a multi-omic single-cell data integration method based on transfer learning and semi-supervised learning, which effectively transfers labels through kNN on joint embedding space.   \n\u2022 scNCL [72] is a transfer learning framework that preserves the intrinsic structure of scATAC-seq data and employs contrastive learning to facilitate the transfer of cell types.   \n\u2022 scBridge [36] integrates multi-omic single-cell data in a heterogeneous manner by mining reliable cross-modal samples and expanding the single-cell dataset. ", "page_idx": 18}, {"type": "text", "text": "F Further Discussion ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "F.1 Comparison with Classic Pseudo-labeling ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "As depicted in Figure 4, classic pseudo-labeling [52] assigns the category label with the highest confidence as the pseudo-label, establishing a one-to-one correspondence between a sample and a category label. This approach overlooks the potential connections between the sample distribution and the class distribution, especially in cases of class imbalance. In contrast, our proposed OT-based pseudo-labeling aligns multiple samples with a category center through the coupling matrix $Q$ and the cost matrix $-l o g P$ . This approach considers the connections between the sample and class distribution, effectively ad", "page_idx": 19}, {"type": "image", "img_path": "sKEhebkEdz/tmp/f9a3809455f64b268d1cf1eaa0eefaef4c6b18a75399986be5595d31a4a7a3d6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 4: An illustration of classic pseudo-labeling and our OT-based pseudo-labeling. ", "page_idx": 19}, {"type": "text", "text": "dressing the issue of potential biased predictions in scRNA-seq data. ", "page_idx": 19}, {"type": "text", "text": "F.2 Advantages of Divide and Conquer for Target scATAC-seq Data ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Different from traditional approaches [52, 78] that directly discard the target-specific samples, we utilize these samples to provide weak supervision for the model (algorithm can be found in Algorithm 2). By progressively removing the incorrect cell types, the model gets extra guidance from these samples. Incorporating consistency regularization for source-like samples (Figure 5) and incorrect cell types removal for targetspecific samples, the entire scATACseq dataset $\\bar{D}^{(t)}\\,=\\,\\mathcal{D}^{S L}\\cup\\mathcal{D}^{T C}$ is ", "page_idx": 19}, {"type": "image", "img_path": "sKEhebkEdz/tmp/38a516774ad0f8abf337baad7623113a70cd7a4fb9e4543e5dca38e34e342fb4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 5: An illustration of consistency regularization for source-like scATAC-seq samples. ", "page_idx": 19}, {"type": "text", "text": "fully explored, contributing to the semi-supervised knowledge transfer process. ", "page_idx": 19}, {"type": "text", "text": "F.3 Advantages of Cross-omic Multi-sample Mixup ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Different from previous alignment strategies that rely on labels from scATAC-seq data or paired labels, our proposed cross-omic multi-sample Mixup reconstructs class-wise similarity through the weights of classifiers (Figure 6). It then fuses scRNA-seq samples by incorporating semantic information from scATACseq data. Without the need for labels from scATAC-seq data or pairwise labels, we transform the distribution of scRNA-seq samples to align ", "page_idx": 19}, {"type": "image", "img_path": "sKEhebkEdz/tmp/05469b5e4a458af751ec37215d573e99b8e6faa1f9d6a04ccc836a48415a047c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "with scATAC-seq data. This makes Figure 6: An illustration of cross-omic multi-sample Mixup. our alignment strategy more flexible and convenient. Considering the high cost of annotating singlecell data, our alignment strategy is more suitable for the problem of semi-supervised knowledge transfer across multi-omic single-cell data. ", "page_idx": 19}, {"type": "image", "img_path": "sKEhebkEdz/tmp/04accfe20d1abc412d2214b77abf24a4135034c9f238f28a5a1691c226a8260d.jpg", "img_caption": ["Figure 7: Qualitative comparisons (in $\\%$ ) with state-of-the-art approaches on additional datasets with different label ratios. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "sKEhebkEdz/tmp/23e072df2ff2333faf835bcac57cfc1c1087f7349fcb21d59bb6639d80d660a2.jpg", "img_caption": ["Figure 8: The t-SNE visualization of two modalities (scRNA_10X_v2: red, ATAC: blue) (the first row) and different cell types (the second row) with high label ratio. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "F.4 Further Sensitivity Analysis ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In addition to these parameters, we also investigate the two empirical hyper-parameters: $\\sigma$ and $\\tau$ . From the results in Table 4 and Table 5, we can observe that the performance is not sensitive to the choice of $\\sigma$ when we change $\\sigma$ from 5 to 20, so we empirically set its value to 10. We also vary the value of $\\tau$ with the range of $\\{0.8,0.85,0.9,0.95\\}.$ . The results below indicate that the method is not sensitive to $\\tau$ in the interval [0.8, 0.95]. Therefore, we set $\\tau$ to 0.9 as the default. ", "page_idx": 20}, {"type": "table", "img_path": "sKEhebkEdz/tmp/ea3b597905a52c5cb88dfdeccf320266b923f3cbc1b80ff8f135e713e5a699eb.jpg", "table_caption": ["Table 4: Sensitivity analysis of $\\sigma$ . "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "We progressively remove the incorrect cell types for additional supervision. However, how to define an incorrect cell type may be a point of interest. To answer this question, We have included a sensitivity analysis of the threshold $\\mu$ by varying it in $\\{0.0005,0.00\\bar{1},0.0015,0.002\\}$ . The results in Table 6 indicate that a threshold of $1e-3$ brings good performance with significant differences. In particular, after the softmax operation, the sum of the confidences for all cell types equals 1 with more than 10 cell types. In this context, the confidence scores are quite small and a difference of $1e-3$ indicates a sufficient difference. ", "page_idx": 20}, {"type": "image", "img_path": "sKEhebkEdz/tmp/6ebeef8bf939fadbaccd57d4b9ccad644ce211f9010a84967af03500893ae54a.jpg", "img_caption": ["Figure 9: The t-SNE visualization of two modalities (scRNA_10X_v2: red, snmC: blue) (the first row) and different cell types (the second row) with high label ratio. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "sKEhebkEdz/tmp/24c0a61eab1455c8cebca2ad83d25893f225d68c3a5ec86043888f5ce284c9ee.jpg", "img_caption": ["Figure 10: Comparisons of the A-distance between source scRNA-seq data and target scATAC-seq data with various label ratios. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "F.5 Further Comparison of the A-distance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Besides scRNA_10X_v2-ATAC, we further compare the A-distance on 3 more datasets, including scRNA_10X_v2-snmC, snRNA_10X_v2-ATAC, and snRNA_10X_v2-snmC in Figure 10. The results are consistent with the analysis in Section 5.2, which validates the robustness of our approach in reducing cell heterogeneity. ", "page_idx": 21}, {"type": "text", "text": "F.6 Additional Qualitative Results ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We showcase additional qualitative experimental results on two more datasets in Figure 7. It can still be observed that our DANCE achieves the best performance. From the results, we can obtain consistent conclusions as previously mentioned in Section 5.2. ", "page_idx": 21}, {"type": "text", "text": "F.7 Additional t-SNE Visualization ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In Figure 8 and Figure 9, we make the t-SNE visualization comparison with more baseline methods. From the results in the first row, it can be found that there is almost no overlap degree in FixMatch [52] and scNCL [72]. There are slight overlaps in scBridge [36], but the heterogeneous gap remains large when certain parts of labels are missing. Compared with these methods, our DANCE achieves the largest overlap degree since it effectively reduces cell heterogeneity. In the second row, each different color represents a different cell type. It can be seen that, in the case of label scarcity, our DANCE still ", "page_idx": 21}, {"type": "text", "text": "Table 5: Sensitivity analysis of $\\tau$ ", "page_idx": 22}, {"type": "table", "img_path": "sKEhebkEdz/tmp/a8144ce9a13f982e91856e54dcd13377abea4a08364e1b713240de504249abed.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "sKEhebkEdz/tmp/e2c980d563b14ba28d5c940242b0d276e3970f7fe03c6c1549a8c584be6716de.jpg", "table_caption": ["Table 6: Sensitivity analysis of $\\mu$ "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "maintains good cell discriminability compared to the other three methods. This indicates that DANCE effectively transfers cell type knowledge from scRNA-seq data to scATAC-seq data. ", "page_idx": 22}, {"type": "table", "img_path": "sKEhebkEdz/tmp/2b5dd4a7e7f866b50e100f6f1d2af2b0b787851f5fc3adca9a9d4a061b281366.jpg", "table_caption": ["Table 7: Ablation on OT strategy for target single-cell data. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "F.8 Optimal Transport Strategy for Target Single-cell Data ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Despite the effectiveness of OT-based dataset expansion for source scRNA-seq data, due to the presence of cellular heterogeneity, directly applying this strategy to target single-cell data may not be effective. We have added a model variant w/ OT for scATAC-seq to support our point. The compared results on CITE-ASAP dataset are shown in Table 7. We can find that our full model outperforms w/ OT for scATAC-seq, which validates the OT strategy may not be the optimal choice given the significant cellular heterogeneity inherent in scATAC-seq data. This is why we introduce the divide-and-conquer strategy for target samples. ", "page_idx": 22}, {"type": "table", "img_path": "sKEhebkEdz/tmp/216c489dc7b265b6c8397875a570959fecfa17f4c6dbc1926e750682ce8cb6b0.jpg", "table_caption": ["Table 8: Comparison of pseudo-labeling accuracy $(\\%)$ . "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "F.9 Effectiveness of Divide-and-conquer Strategy ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "An interesting question is whether the initial prediction\u2019s reliance on the divide-and-conquer strategy has been accurately labeled. To explore this issue, we have included a comparison of pseudo-labeling accuracy between unlabeled scRNA-seq data and source-like scATAC-seq data in Table 8. The results for the snRNA_SMARTer-snmC dataset are presented below. From the findings, it is evident that the accuracy of the source-like scATAC-seq data closely aligns with that of unlabeled scRNA-seq data, confirming the effectiveness of our approach. ", "page_idx": 22}, {"type": "table", "img_path": "sKEhebkEdz/tmp/d807a1b92c63976bd9156a48a79b3e40845176b96d22ad255d721eee4f3837e5.jpg", "table_caption": ["Table 9: Performance comparison when transferring cell types in the opposite direction. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "F.10 Cell Type Transfer in the Opposite Direction ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Even though mainstream algorithms focus on transferring cell type knowledge from scRNA-seq to scATAC-seq data, we are interested in exploring the possibility of cell type transfer in the opposite direction, namely from scATAC-seq to scRNA-seq. We have conducted an experiment that performs this reverse cell type transfer on the CITE-ASAP dataset. The results presented in Table 9 confirm the effectiveness of our method in this reverse direction. ", "page_idx": 23}, {"type": "table", "img_path": "sKEhebkEdz/tmp/02df4f8bedd3ced12413301ff8a6273b6659b937385499ceff7d55d5ba894b01.jpg", "table_caption": ["Table 10: Efficiency Comparison. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "F.11 Computation Cost ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The comparison of computation costs is also a point of interest. We have included a comparison of memory and time in Table 10, and it is evident that our method offers a competitive computation cost. Specifically, the performance of scNCL is significantly inferior to ours (our performance improvement exceeds $105.9\\%$ ), while our memory cost sees only a slight increase with even less training time. ", "page_idx": 23}, {"type": "table", "img_path": "sKEhebkEdz/tmp/4e4b2a997844d84f89bcb81029094a76105b351917c7d28515283f9897f23a17.jpg", "table_caption": ["Table 11: Performance comparison with two additional domain adaptation methods. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "F.12 Further Comparison with More Baselines ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "To benchmark our DANCE against more baseline methods, we have included SLA [76], COT [41], Seurat [54], and Harmony [30] on different datasets for performance comparison. The results shown in Table 11 demonstrate the effectiveness of the proposed DANCE. ", "page_idx": 23}, {"type": "table", "img_path": "sKEhebkEdz/tmp/558a60b8af383bf656688fe7aaf70597445913acd19a1b0f029fa66ee175c471.jpg", "table_caption": ["Table 12: Performance on full MouseAtlas data. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "F.13 Performance on Full MouseAtlas Data ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "To evaluate the performance under different batches, we have included the experimental results on the full MouseAtlas dataset. Specifically, we collect different batches of the MouseAtlas dataset and remove some of the batches. The results in Table 12 indicate that our approach still outperforms other methods, showing the potential of DANCE in batch effect correction. ", "page_idx": 23}, {"type": "table", "img_path": "sKEhebkEdz/tmp/15506623ccf44a7d6b0ccdf97d696823b69e2296398b90f4219ca3b1bbb26967.jpg", "table_caption": ["Table 13: Performance on real-world scenarios. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "F.14 Real-world Cases ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In real-world scenarios, the proposed DANCE can transfer cell types from multiple-source scRNA-seq data to target scATAC-seq data. We have included experiments to transfer cell type knowledge from a labeled scRNA-seq dataset and an unlabeled scRNA-seq dataset to scATAC-seq data. The results in Table 13 indicate that our approach is superior to other methods in practical scenarios. ", "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper\u2019s contributions are summarized in the introduction section, and the paper\u2019s scope is introduced in the abstract section. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: The limitations part is discussed in the conclusion section. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The proofs of the theorems are provided in the appendix. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: The implementation details are provided in the appendix and the anonymous code link is provided in the abstract section. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The anonymous code link is provided in the abstract section and the data link is provided in the experimental settings section. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The implementation details are provided in the appendix. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [No] ", "page_idx": 27}, {"type": "text", "text": "Justification: Note that previous studies in this field have not included error bars in their experiments, and we follow this for uniformity. We reimplement all the baselines according to the corresponding papers and fix the seeds to ensure a fair comparison. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: All the experiments are conducted on NVIDIA Tesla A100 40G GPUs. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We sincerely read the ethics guidelines and obey this rule ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The discussion about the broader impacts is provided in the conclusion section and appendix. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our paper poses no such risks. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: All the creators of assets are properly credited. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 29}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: New assets are well documented. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}]