[{"figure_path": "fTOw3BzcWs/tables/tables_6_1.jpg", "caption": "Table 1: Average reward [\u2191] obtained during online evaluation over 3 seeds on openAI gym envs", "description": "This table presents the average reward achieved by different offline reinforcement learning algorithms across various OpenAI Gym environments.  The results are averaged over three different seeds (random initializations) for each algorithm and environment combination.  The table shows the performance on three types of datasets: Expert, Replay, and Noisy, reflecting different data quality and coverage.  The \"+\" indicates the improvement in average reward compared to the baseline algorithm for each condition, allowing for a comparison of performance across different algorithms and data types. ", "section": "5.2 Performance across Different Datasets"}, {"figure_path": "fTOw3BzcWs/tables/tables_7_1.jpg", "caption": "Table 2: Results on human generated Sales Promotion dataset", "description": "This table presents the results of experiments conducted on a real-world sales promotion dataset.  It shows a comparison of different offline reinforcement learning algorithms: CQL + D (CQL combined with domain knowledge), CQLSE (Conservative Q-Learning with safety expert), and the proposed ExID algorithm. The table includes the average reward achieved by each algorithm, along with its standard deviation and the percentage performance gain of ExID compared to CQL + D.  The domain knowledge (D) used is also specified, along with its coverage of the dataset.", "section": "5.3 Case study on real human generated Sales Promotion (SP) dataset"}, {"figure_path": "fTOw3BzcWs/tables/tables_18_1.jpg", "caption": "Table 3: Average reward [\u2191] obtained during online evaluation over 3 seeds on Minigrid environments", "description": "This table presents the average reward achieved by different offline reinforcement learning algorithms on two Minigrid environments: MiniGrid-Dynamic-Obstacles-Random-6x6-v0 and MiniGrid-LavaGapS7-v0.  The results are averaged over three different seeds, providing a measure of the algorithms' performance consistency. The algorithms compared include the baseline (D, using only domain knowledge), Behavior Cloning (BC) with domain knowledge (BC_D), Batch-Constrained Deep Q-learning (BCQ) with domain knowledge (BCQ_D), Conservative Q-Learning (CQL) with domain knowledge (CQL_D), and the proposed ExID algorithm. The upward-pointing arrow indicates that higher values are better, reflecting the goal of maximizing cumulative reward in these environments.", "section": "5.2 Performance across Different Datasets"}, {"figure_path": "fTOw3BzcWs/tables/tables_18_2.jpg", "caption": "Table 1: Average reward [\u2191] obtained during online evaluation over 3 seeds on openAI gym envs", "description": "This table presents the average rewards obtained from online evaluation of different offline RL algorithms across various OpenAI Gym environments.  The experiments were run with three different random seeds for each algorithm and environment combination, to provide a measure of stability and reproducibility of the results. The environments used are Mountain Car, Cart Pole, and Lunar Lander, each tested with three different dataset types: Expert, Replay, and Noisy. The table allows for a comparison of the performance of EXID (the proposed algorithm) against other existing offline RL algorithms (BC, BCQ, CQL, CQL SE, CQL + D, etc.). The upward-pointing arrow indicates that higher values are better.", "section": "5.2 Performance across Different Datasets"}, {"figure_path": "fTOw3BzcWs/tables/tables_18_3.jpg", "caption": "Table 3: Average reward [\u2191] obtained during online evaluation over 3 seeds on Minigrid environments", "description": "This table presents the average reward achieved by different algorithms on two Minigrid environments ('DynamicObstRandom6x6-v0' and 'LavaGapS7-v0').  The results are averaged across three different seeds (random initializations), providing a measure of the algorithm's performance consistency. The table compares the performance of EXID against several baseline methods (D, BC, BCQ, CQL, D).  Higher values indicate better performance.", "section": "5.2 Performance across Different Datasets"}, {"figure_path": "fTOw3BzcWs/tables/tables_21_1.jpg", "caption": "Table 6: Performance of ExID on removing different parts of the data based on nodes of Fig 10 (c) from Mountain Car expert dataset", "description": "This table presents the average reward and standard deviation obtained by the ExID algorithm on the Mountain Car expert dataset after removing data points that satisfy specific conditions based on the nodes of the decision tree in Figure 10(c). The conditions used for data removal are: position > -0.5, position < -0.5, velocity > 0.01, and velocity < 0.01. The results show the impact of removing different parts of the dataset on the algorithm's performance.", "section": "5.2 Performance across Different Datasets"}]