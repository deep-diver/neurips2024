{"references": [{"fullname_first_author": "Aviral Kumar", "paper_title": "Stabilizing off-policy q-learning via bootstrapping error reduction", "publication_date": "2019-12-01", "reason": "This paper is foundational to the work in this paper, introducing a crucial technique for stabilizing off-policy Q-learning, which is directly relevant to offline RL."}, {"fullname_first_author": "Scott Fujimoto", "paper_title": "Off-policy deep reinforcement learning without exploration", "publication_date": "2019-06-01", "reason": "This paper is highly influential in offline RL, proposing a method for off-policy learning that addresses the exploration problem, a key challenge in offline RL."}, {"fullname_first_author": "Ilya Kostrikov", "paper_title": "Offline reinforcement learning with fisher divergence critic regularization", "publication_date": "2021-07-01", "reason": "This work addresses the challenge of out-of-distribution actions in offline RL by proposing a novel regularization technique, making it highly relevant to the current paper's focus on improving offline RL performance."}, {"fullname_first_author": "Aviral Kumar", "paper_title": "Conservative q-learning for offline reinforcement learning", "publication_date": "2020-12-01", "reason": "This paper is highly influential and introduces a conservative Q-learning approach, addressing the extrapolation error problem in offline RL, which is a core concern of this paper."}, {"fullname_first_author": "Justin Fu", "paper_title": "D4rl: Datasets for deep data-driven reinforcement learning", "publication_date": "2020-04-01", "reason": "This paper provides a comprehensive set of benchmark datasets for deep reinforcement learning, crucial for evaluating the performance of offline RL algorithms which this paper utilizes for its empirical evaluation."}]}