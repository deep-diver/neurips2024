[{"figure_path": "fTOw3BzcWs/figures/figures_1_1.jpg", "caption": "Figure 1: a) Full expert, Mountain Car dataset, and reduced dataset with first 10% samples showing distribution of state (position, velocity) and action b) CQL agent converging to a sub-optimal policy for reduced dataset exhibiting high Q values for actions different from actions in the expert dataset for unseen states.", "description": "This figure shows a comparison between the full expert dataset for the Mountain Car environment and a reduced dataset containing only the first 10% of samples.  Subfigure (a) visualizes the distribution of states (position, velocity) and actions for both datasets, highlighting the limited coverage of the reduced dataset. Subfigure (b) demonstrates the performance degradation of the Conservative Q-Learning (CQL) algorithm when trained on the reduced dataset.  It plots the average reward obtained and the difference between the Q-values of actions selected by CQL and the actions present in the full expert dataset for unseen states. The discrepancy in Q-values for unseen states indicates that CQL overestimates the value of suboptimal actions, leading to poor performance.", "section": "1 Introduction"}, {"figure_path": "fTOw3BzcWs/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of the proposed methodology (a) Training a teacher policy network with domain knowledge and synthetic data (b) Updating the offline RL critic network with teacher network", "description": "This figure provides a visual overview of the ExID algorithm proposed in the paper. It is divided into two main parts: (a) shows the training of the teacher policy network using domain knowledge and synthetic data; (b) illustrates how the offline RL critic network is updated using the teacher network, a CQL loss, and an additional regularization term.  The teacher network acts as a guide, providing better actions for states covered by domain knowledge, helping to mitigate issues related to limited data and unseen states.  The process iteratively refines the teacher network based on the critic's performance, improving overall accuracy and generalization.", "section": "4 Problem Setting and Methodology"}, {"figure_path": "fTOw3BzcWs/figures/figures_6_1.jpg", "caption": "Figure 3: Performance of (a) CQL and (b) EXID on all datasets for Mountain Car during online evaluation (c) Evaluation curves for the sales promotion dataset", "description": "This figure compares the performance of CQL and EXID algorithms on the Mountain Car environment using different datasets (expert, replay, noisy).  Subfigure (a) shows the average rewards obtained by CQL, while subfigure (b) presents the average rewards achieved by EXID. The shaded areas represent confidence intervals. Subfigure (c) displays the evaluation curves for a real-world sales promotion dataset, demonstrating EXID's superior performance compared to CQL and other baselines.", "section": "5.2 Performance across Different Datasets"}, {"figure_path": "fTOw3BzcWs/figures/figures_7_1.jpg", "caption": "Figure 4: Q value difference between CQL and EXID for expert and policy action on states not present in the buffer for a) expert b) noisy in log scale c) contribution of Lr(\u03b8)", "description": "This figure compares the Q-value differences between CQL and EXID for expert and policy actions on states unseen during training.  Panel (a) shows the comparison for expert data, (b) for noisy data, both using a log scale for better visualization of the differences. Panel (c) specifically isolates the contribution of the regularization term Lr(\u03b8) over training episodes for expert, noisy, and replay datasets, demonstrating its role in improving the model's performance with limited data.", "section": "5.4 Generalization to OOD states and contribution of Lr(\u03b8)"}, {"figure_path": "fTOw3BzcWs/figures/figures_8_1.jpg", "caption": "Figure 5: (a) Effect of different \u03bb on the performance of ExID on Lunar Lander (b) Effect of different k on the performance of EXID on Lunar Lander (c) Performance of EXID with teacher update, no teacher update, and just warm start on Cart-pole.", "description": "This figure shows the performance of the ExID algorithm under different hyperparameters.  Specifically, it illustrates the impact of varying the mixing parameter (\u03bb) and the warm-up parameter (k) on the Lunar Lander environment.  Subfigure (c) compares the ExID algorithm with and without a teacher network update and with only a warm start phase, demonstrating the importance of these components to the algorithm's performance in the Cartpole environment.", "section": "5.5 Performance on varying \u03bb, k, and ablation of \u03c0"}, {"figure_path": "fTOw3BzcWs/figures/figures_8_2.jpg", "caption": "Figure 6: (a) D with different average rewards (b) Performance effect on Lunar-lander (c) State distribution generated for training the teacher network for mountain-car", "description": "This figure shows the impact of using different domain knowledge policies (D) on the performance of the proposed method (ExID).  Panel (a) presents a bar chart comparing the average rewards obtained by using various D rules with different average rewards. Panel (b) displays the performance curves of ExID and a baseline method across multiple trials. The difference in performance highlights the importance of appropriate domain knowledge. Panel (c) is a 3D scatter plot illustrating the state-action distribution generated using a specific domain knowledge policy (D) for training the teacher network in the Mountain Car environment.  This illustrates the distribution of states and actions used to train the teacher.", "section": "5.6 Effect of varying D quality"}, {"figure_path": "fTOw3BzcWs/figures/figures_15_1.jpg", "caption": "Figure 7: Example MDP, sampled buffer MDP and reduced buffer with Q tables", "description": "This figure shows a counterexample to demonstrate that Q-learning with a reduced dataset may not converge to an optimal policy.  It illustrates three Markov Decision Processes (MDPs): the original MDP, an MDP sampled from the original MDP's buffer, and a reduced buffer MDP. Each MDP is represented by a diagram showing states and transitions, with rewards indicated on the arrows.  The corresponding Q-tables are also shown, illustrating how the Q-values evolve after a single backup step for the sampled and reduced buffer MDPs, highlighting the divergence from the optimal Q-values.", "section": "B Missing Examples"}, {"figure_path": "fTOw3BzcWs/figures/figures_15_2.jpg", "caption": "Figure 8: We hypothesize the suboptimal performance of offline RL for limited data can be addressed via domain knowledge via action regularization and knowledge distillation.", "description": "This figure is a Venn diagram showing the relationship between three sets: the full dataset, domain knowledge, and the reduced dataset.  The area of overlap between the full dataset and domain knowledge represents states and actions covered by both the complete data and expert insights. The overlapping area of the reduced dataset and domain knowledge shows that expert knowledge can also help correct errors for states not present in the reduced dataset. The portion of the reduced dataset that does not overlap with the other two sets represents states and actions where offline RL struggles due to data scarcity, but this area is partially mitigated by knowledge distillation and regularization techniques.", "section": "Problem Setting and Methodology"}, {"figure_path": "fTOw3BzcWs/figures/figures_16_1.jpg", "caption": "Figure 9: Graphical visualizations of environments used in the experiments. These environments are a) MountainCar-v0 b) CartPole-v1 c) LunarLander-v2 d) MiniGrid-LavaGapS7-v0 e) MiniGrid-Dynamic-Obstacles-Random-6x6-v0", "description": "This figure shows graphical visualizations of the five different reinforcement learning environments used in the experiments described in the paper.  These environments vary in complexity, from the simple MountainCar environment (a) to more complex environments like MiniGrid (d, e).  The visualizations provide a visual representation of each environment's state space and the agent's interaction with it.", "section": "D Environments and Domain Knowledge Trees"}, {"figure_path": "fTOw3BzcWs/figures/figures_17_1.jpg", "caption": "Figure 1: a) Full expert, Mountain Car dataset, and reduced dataset with first 10% samples showing distribution of state (position, velocity) and action b) CQL agent converging to a sub-optimal policy for reduced dataset exhibiting high Q values for actions different from actions in the expert dataset for unseen states.", "description": "The figure shows a comparison between a full expert dataset and a reduced dataset (containing only the first 10% of samples) for the Mountain Car environment. Subfigure (a) visualizes the distribution of states (position, velocity) and actions in both datasets, highlighting the sparsity of the reduced dataset. Subfigure (b) illustrates the performance degradation of the Conservative Q-Learning (CQL) algorithm on the reduced dataset. It shows that CQL converges to a suboptimal policy, overestimating Q-values for actions not present in the reduced dataset but present in the full expert dataset, especially for unseen states.", "section": "1 Introduction"}, {"figure_path": "fTOw3BzcWs/figures/figures_19_1.jpg", "caption": "Figure 3: Performance of (a) CQL and (b) EXID on all datasets for Mountain Car during online evaluation (c) Evaluation curves for the sales promotion dataset", "description": "This figure compares the performance of CQL and EXID algorithms on the Mountain Car environment using three different types of datasets: expert, replay, and noisy.  The graphs show the average reward obtained over multiple episodes.  (a) and (b) present the results for CQL and EXID, respectively, illustrating EXID's superior performance, especially in the noisy data setting. (c) shows the reward curves for the sales promotion dataset, showcasing EXID's improved generalization compared to CQL.", "section": "5.2 Performance across Different Datasets"}, {"figure_path": "fTOw3BzcWs/figures/figures_19_2.jpg", "caption": "Figure 1: a) Full expert, Mountain Car dataset, and reduced dataset with first 10% samples showing distribution of state (position, velocity) and action b) CQL agent converging to a sub-optimal policy for reduced dataset exhibiting high Q values for actions different from actions in the expert dataset for unseen states.", "description": "The figure demonstrates the performance degradation of the Conservative Q-Learning (CQL) algorithm when trained on a limited dataset compared to a full expert dataset. Subfigure (a) visualizes the state-action space for both the full and reduced datasets, highlighting the sparsity of the latter. Subfigure (b) shows the average reward obtained and the difference between the Q-value of the action chosen by CQL and the optimal action from the expert dataset, indicating that CQL overestimates Q-values for suboptimal actions in unseen states of the reduced dataset.", "section": "1 Introduction"}, {"figure_path": "fTOw3BzcWs/figures/figures_20_1.jpg", "caption": "Figure 3: Performance of (a) CQL and (b) EXID on all datasets for Mountain Car during online evaluation (c) Evaluation curves for the sales promotion dataset", "description": "This figure compares the performance of CQL and EXID algorithms on the Mountain Car environment using different datasets (expert, replay, and noisy).  Subplots (a) and (b) show the average rewards obtained by CQL and EXID respectively over a set number of episodes. The shaded areas represent the standard deviation across multiple runs. Subplot (c) presents the evaluation curves for a real-world sales promotion dataset, illustrating the performance difference between CQL and EXID over time.  The figure demonstrates that EXID outperforms CQL, especially when the data is limited or noisy.", "section": "5.2 Performance across Different Datasets"}, {"figure_path": "fTOw3BzcWs/figures/figures_20_2.jpg", "caption": "Figure 14: (a) The effect of data reduction and removal on baseline CQL visualized on Mountain Car Environment (b) Performance of ExID on removing different parts of the data based on nodes of Fig 10 (c) from Mountain Car expert dataset", "description": "The left plot shows the effect of different data reduction and removal methods on the performance of CQL algorithm on Mountain Car environment. The right plot shows the performance of ExID algorithm when different parts of data are removed based on the nodes of Figure 10(c) which represents the domain knowledge used. The results indicate that ExID algorithm is more robust to data reduction and removal compared to CQL.", "section": "5.2 Performance across Different Datasets"}, {"figure_path": "fTOw3BzcWs/figures/figures_21_1.jpg", "caption": "Figure 1: a) Full expert, Mountain Car dataset, and reduced dataset with first 10% samples showing distribution of state (position, velocity) and action b) CQL agent converging to a sub-optimal policy for reduced dataset exhibiting high Q values for actions different from actions in the expert dataset for unseen states.", "description": "This figure shows the comparison of full expert dataset and a reduced dataset with only 10% of the samples from the full dataset.  The subplots show the state-action distribution for both datasets. The second subplot compares the performance of the Conservative Q-Learning (CQL) algorithm on both datasets. CQL performs poorly on the reduced dataset because it overestimates the Q-values for actions not seen in the dataset, leading to sub-optimal policies.", "section": "1 Introduction"}, {"figure_path": "fTOw3BzcWs/figures/figures_21_2.jpg", "caption": "Figure 1: a) Full expert, Mountain Car dataset, and reduced dataset with first 10% samples showing distribution of state (position, velocity) and action b) CQL agent converging to a sub-optimal policy for reduced dataset exhibiting high Q values for actions different from actions in the expert dataset for unseen states.", "description": "This figure shows the comparison of the full expert dataset and a reduced dataset (containing only the first 10% of samples) for the Mountain Car environment.  Subfigure (a) visualizes the distribution of states (position and velocity) and actions in both datasets, highlighting the sparsity of the reduced dataset. Subfigure (b) illustrates that the Conservative Q-Learning (CQL) agent, trained on the reduced dataset, learns a sub-optimal policy. The sub-optimal policy is evident in the higher Q-values assigned to actions that differ from the actions taken by the expert in unseen (or rare) states. This demonstrates the challenge of offline reinforcement learning with limited data, where the agent struggles to generalize to unseen situations.", "section": "1 Introduction"}, {"figure_path": "fTOw3BzcWs/figures/figures_22_1.jpg", "caption": "Figure 1: a) Full expert, Mountain Car dataset, and reduced dataset with first 10% samples showing distribution of state (position, velocity) and action b) CQL agent converging to a sub-optimal policy for reduced dataset exhibiting high Q values for actions different from actions in the expert dataset for unseen states.", "description": "The figure shows a comparison between a full expert dataset for the Mountain Car environment and a reduced dataset containing only the first 10% of the samples.  Subfigure (a) visualizes the state-action space for both datasets, highlighting the limited coverage of the reduced dataset. Subfigure (b) compares the performance of the Conservative Q-Learning (CQL) algorithm on both datasets, demonstrating significantly worse performance on the reduced dataset. This degradation is attributed to CQL's tendency to overestimate Q-values for actions not present in the limited dataset, leading to suboptimal policy choices.", "section": "1 Introduction"}]