[{"figure_path": "7txPaUpUnc/tables/tables_5_1.jpg", "caption": "Table 1: Three SAEs from layer 6 with similar CE loss increases are analyzed in detail.", "description": "This table compares three different types of Sparse Autoencoders (SAEs) trained on layer 6 of a GPT2-small model.  The SAEs are designed to identify functionally important features in the model.  The table shows the sparsity coefficient (\u03bb), the average number of active features per data point (Lo), the total number of active dictionary elements, and the increase in cross-entropy loss (CE Loss Increase) for each SAE type. The comparison focuses on SAEs with approximately equivalent CE loss increases to highlight differences in other metrics.", "section": "3.1 End-to-end SAEs are a Pareto improvement over local SAEs"}, {"figure_path": "7txPaUpUnc/tables/tables_13_1.jpg", "caption": "Table 1: Three SAEs from layer 6 with similar CE loss increases are analyzed in detail.", "description": "This table presents a comparison of three different types of Sparse Autoencoders (SAEs) trained on layer 6 of a GPT-2 small language model.  The SAEs are trained with different loss functions to emphasize different aspects of feature learning: local reconstruction loss, end-to-end KL divergence loss, and end-to-end KL divergence with downstream reconstruction loss. The table compares the sparsity coefficient (\u03bb), the average number of active features per data point (Lo), the total number of active dictionary elements, and the increase in cross-entropy loss (CE Loss Increase) for each SAE type. The values show that end-to-end SAEs require fewer features to achieve similar levels of performance, suggesting that they are more efficient at capturing functionally important features.", "section": "3.1 End-to-end SAEs are a Pareto improvement over local SAEs"}, {"figure_path": "7txPaUpUnc/tables/tables_14_1.jpg", "caption": "Table 1: Three SAEs from layer 6 with similar CE loss increases are analyzed in detail.", "description": "This table compares three different types of Sparse Autoencoders (SAEs) trained on layer 6 of a GPT2-small model.  The SAEs are trained with different loss functions to minimize either Mean Squared Error (MSE) or Kullback-Leibler (KL) divergence, along with a sparsity penalty (L1). The table shows the sparsity coefficient (\u03bb), the average number of active SAE features per datapoint (Lo), the total number of \"alive\" dictionary elements, and the resulting cross-entropy (CE) loss increase compared to the original model.  The aim is to highlight the trade-offs and relative performance of each SAE type.", "section": "3.2 End-to-end SAEs have worse reconstruction loss at each layer despite similar output distributions"}, {"figure_path": "7txPaUpUnc/tables/tables_16_1.jpg", "caption": "Table 1: Three SAEs from layer 6 with similar CE loss increases are analyzed in detail.", "description": "This table presents a comparison of three different types of Sparse Autoencoders (SAEs) trained on layer 6 of a GPT2-small model.  The SAEs are categorized as Local, End-to-end, and End-to-end + downstream. For each SAE type, the table lists the sparsity coefficient (\u03bb), the average number of features activated per data point (Lo), the total number of active dictionary elements, and the increase in cross-entropy loss.  The purpose of the table is to illustrate the Pareto improvement of the end-to-end methods over the baseline local SAE method.", "section": "3.1 End-to-end SAEs are a Pareto improvement over local SAEs"}, {"figure_path": "7txPaUpUnc/tables/tables_17_1.jpg", "caption": "Table 5: L2 Ratio for the SAEs of similar CE loss increase, as in Table 2.", "description": "This table presents the L2 ratio for three types of Sparse Autoencoders (SAEs): local, e2e, and e2e+ds. The L2 ratio is a measure of feature suppression, calculated as the ratio of the L2 norm of the SAE's output to the L2 norm of the input.  The table shows the L2 ratio for position 0 (the most suppressed position) and positions greater than 0. It compares results at layers 2, 6, and 10. Lower ratios indicate higher suppression.", "section": "B Analysis of reconstructed activations"}, {"figure_path": "7txPaUpUnc/tables/tables_30_1.jpg", "caption": "Table 6: Training times for different layers and SAE training methods using a single NVIDIA A100 GPU on the residual stream of GPT2-small at layer 6. All SAEs are trained on 400k samples of context length 1024, with a dictionary size of 60x the residual stream size of 768.", "description": "This table shows the training time taken for three different types of Sparse Autoencoders (SAEs) on three different layers of the GPT2-small language model.  The training times are shown in hours and minutes, and are broken down by the type of SAE used: SAElocal, SAEe2e, and SAEe2e+ds. The training time for e2e SAEs was approximately 2-3.5 times longer than the SAElocal training times, and the differences in training time between SAEe2e and SAEe2e+ds were negligible.", "section": "H Training time"}, {"figure_path": "7txPaUpUnc/tables/tables_31_1.jpg", "caption": "Table 7: Faithfulness on subject-verb agreement when replacing the activations with SAE outputs.", "description": "This table shows the faithfulness scores for different SAE types (local, e2e, e2e+ds) across various subject-verb agreement tasks (simple, across participle phrase, across relative clause, within relative clause) at different layers (2, 6, 10). Faithfulness measures how well the model performs compared to a random baseline when the original activations are replaced with SAE outputs. A score of 100% means that the SAE perfectly preserves the model's performance. The table presents the results for runs with similar CE loss increase and similar Lo, demonstrating how different SAE training methods impact the preservation of functionally relevant information in the model's activations for downstream tasks.", "section": "I Faithfulness of SAEs on subject verb agreement task"}]