[{"figure_path": "cvaSru8LeO/tables/tables_4_1.jpg", "caption": "Table 1: Terms regarding input modalities for LLMs and VLMs.", "description": "This table describes the different input modalities used for evaluating Large Language Models (LLMs) and Vision-Language Models (VLMs) in the SpatialEval benchmark.  It defines the terms used to distinguish between text-only, vision-only, and vision-text inputs, clarifying which model types (LLMs or VLMs) are associated with each modality.", "section": "2 Dataset and Task Construction"}, {"figure_path": "cvaSru8LeO/tables/tables_8_1.jpg", "caption": "Table 1: Terms regarding input modalities for LLMs and VLMs.", "description": "This table summarizes the terminology used in the paper to describe the different input modalities used for evaluating Large Language Models (LLMs) and Vision-Language Models (VLMs).  It clarifies the meaning of TQA (Text-only Question Answering), VQA (Vision-only Question Answering), and VTQA (Vision-Text Question Answering), specifying which types of input (textual, visual, or both) are provided to the models in each condition.", "section": "Main Results and Analysis"}, {"figure_path": "cvaSru8LeO/tables/tables_16_1.jpg", "caption": "Table 3: Model checkpoints from Hugging Face.", "description": "This table lists the model names and their corresponding links to the checkpoints on Hugging Face.  It provides the specific locations where the pre-trained model weights can be accessed for the experiments described in the paper.  This allows for reproducibility of the results.", "section": "2.2 Models"}, {"figure_path": "cvaSru8LeO/tables/tables_19_1.jpg", "caption": "Table 4: Results for completion and step-by-step prompts.", "description": "This table compares the average accuracy (Avg Acc) achieved by different models using two different prompting techniques: a simple \"completion\" prompt and a more detailed \"step-by-step explanation\" prompt.  The results are broken down by input modality (Text-only, Vision-only, Vision-text) to show how the effectiveness of each prompting method varies depending on the type of input provided to the model. The data demonstrates that the step-by-step explanation prompt consistently outperforms the simpler completion prompt across all models and input modalities. This highlights the value of providing more detailed instructions to the model to improve the accuracy and quality of its responses in spatial reasoning tasks.", "section": "Impact of prompting techniques"}, {"figure_path": "cvaSru8LeO/tables/tables_20_1.jpg", "caption": "Table 5: Results by varying different temperatures for decoding strategies.", "description": "This table presents the average accuracy (Avg Acc) achieved by different models under two different temperature settings (temperature=1 and temperature=0.2) for decoding strategies.  The models are categorized by input modality (text-only, vision-only, and vision-text). Lower temperatures generally lead to more focused and deterministic results, while higher temperatures result in more diverse outputs.", "section": "C Experimental Details"}, {"figure_path": "cvaSru8LeO/tables/tables_20_2.jpg", "caption": "Table 6: Performance on the Spatial-Real task. The same trends still hold on real images: VQA vs. VTQA, TQA (LLM) vs. VTQA, and TQA (LLM) vs. VQA.", "description": "This table presents the average accuracy scores achieved by various LLMs and VLMs on the Spatial-Real task, categorized by input modality (Text-only, Vision-only, and Vision-text).  The results demonstrate consistent trends observed in synthetic tasks, showing that text-only input often outperforms vision-only input, and the inclusion of textual descriptions alongside visual data significantly enhances the performance of vision-language models, even when there is significant redundancy.", "section": "E Results on Spatial-Real task"}, {"figure_path": "cvaSru8LeO/tables/tables_21_1.jpg", "caption": "Table 7: Detailed results for proprietary models.", "description": "This table presents the detailed performance results for proprietary models (GPT-4, GPT-4V, GPT-40, Gemini Pro 1.0, and Claude 3 Opus) on three spatial reasoning tasks (Spatial-Map, Maze-Nav, and Spatial-Grid).  It breaks down the accuracy for each model across three input modalities: Text-only (TQA), Vision-only (VQA), and Vision-text (VTQA). This allows for a direct comparison of the models' performance under different input conditions and highlights how well these models leverage textual and visual information for spatial reasoning.", "section": "Main Results and Analysis"}, {"figure_path": "cvaSru8LeO/tables/tables_21_2.jpg", "caption": "Table 7: Detailed results for proprietary models.", "description": "This table presents the detailed performance of proprietary models (Claude 3 Opus, Gemini Pro 1.0, GPT-40, GPT-4V, and GPT-4) across three different input modalities (Text-only, Vision-only, Vision-text) on three spatial reasoning tasks (Spatial-Map, Maze-Nav, Spatial-Grid).  It shows the accuracy achieved by each model on each task and input type, offering a granular view of the models' strengths and weaknesses in handling different kinds of spatial reasoning challenges and input modalities.", "section": "4.3 Proprietary vs. Open-Source Models"}, {"figure_path": "cvaSru8LeO/tables/tables_22_1.jpg", "caption": "Table 7: Detailed results for proprietary models.", "description": "This table presents the detailed performance of proprietary models (GPT-4, GPT-4V, GPT-40, Gemini Pro 1.0, and Claude 3 Opus) on three spatial reasoning tasks (Spatial-Map, Maze-Nav, and Spatial-Grid) across different input modalities (text-only, vision-only, and vision-text). It shows the accuracy for each model in each task and input type, allowing for a detailed comparison of the performance across different models and input formats.", "section": "Main Results and Analysis"}, {"figure_path": "cvaSru8LeO/tables/tables_22_2.jpg", "caption": "Table 7: Detailed results for proprietary models.", "description": "This table presents a detailed breakdown of the performance of proprietary models (Claude 3 Opus, Gemini Pro 1.0, GPT-40, GPT-4V, and GPT-4) across three spatial reasoning tasks (Spatial-Map, Maze-Nav, and Spatial-Grid) and three input modalities (Text-only, Vision-only, and Vision-text).  It shows the accuracy achieved by each model on each task and input type, offering a granular view of the models' capabilities in handling different types of spatial information.", "section": "4.3 Proprietary vs. Open-Source Models"}]