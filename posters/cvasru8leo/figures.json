[{"figure_path": "cvaSru8LeO/figures/figures_2_1.jpg", "caption": "Figure 1: Illustration of the Spatial-Map task, which simulates a map with multiple locations. To investigate the impact of modality, we consider three input formats: Text-only, Vision-only, and Vision-text. We evaluate language models (w. TQA input) and vision-language models (w. VQA and VTQA inputs) on the same set of questions.", "description": "This figure illustrates the Spatial-Map task from the SpatialEval benchmark.  The task involves understanding spatial relationships between objects shown on a map. To test the impact of different input modalities, three variations are shown: Text-only (TQA), Vision-only (VQA), and Vision-text (VTQA). Each variation shows a map, either as a text description or as an image (or both), and corresponding questions to test the model's spatial reasoning capabilities. The example shows a map with several labeled locations and questions asking for directions and counts of objects.", "section": "2 Dataset and Task Construction"}, {"figure_path": "cvaSru8LeO/figures/figures_2_2.jpg", "caption": "Figure 2: Illustration of the Maze-Nav task, which evaluates the model's ability to navigate from the starting point (S) to the exit (E).", "description": "The figure illustrates the Maze-Nav task, designed to assess a model's navigation capabilities in a maze-like environment.  It shows three input modalities: Text-only (TQA), Vision-only (VQA), and Vision-text (VTQA). The Text-only input provides a textual representation of the maze using ASCII characters, while the Vision-only input presents a color-coded image representation of the maze.  The Vision-text input combines both the textual and visual representations. The questions posed focus on aspects like the number of turns and the spatial relationship between the start and end points.", "section": "2 Dataset and Task Construction"}, {"figure_path": "cvaSru8LeO/figures/figures_3_1.jpg", "caption": "Figure 1: Illustration of the Spatial-Map task, which simulates a map with multiple locations. To investigate the impact of modality, we consider three input formats: Text-only, Vision-only, and Vision-text. We evaluate language models (w. TQA input) and vision-language models (w. VQA and VTQA inputs) on the same set of questions.", "description": "This figure shows an example of the Spatial-Map task from the SpatialEval benchmark.  The task involves understanding spatial relationships between objects on a map.  Three input modalities are shown: Text-only (TQA), providing only textual descriptions of object locations; Vision-only (VQA), showing only an image of the map; and Vision-text (VTQA), providing both an image and corresponding textual descriptions. The goal is to answer questions about the spatial relationships between the objects shown.  The figure illustrates how different modalities impact the model's ability to perform this spatial reasoning task.", "section": "2 Dataset and Task Construction"}, {"figure_path": "cvaSru8LeO/figures/figures_4_1.jpg", "caption": "Figure 4: Illustration of the Spatial-Real task, which is built on real images with long captions, featuring detailed descriptions averaging over 1,000 words per image.", "description": "This figure shows an example from the Spatial-Real dataset.  The dataset consists of real images paired with very long and detailed textual descriptions (averaging over 1000 words per image).  The task is to answer questions requiring spatial reasoning based on both the image and its caption.  This particular example shows a picture of a truck being lifted by cranes, and the question asks how many cranes are involved.", "section": "2 Dataset and Task Construction"}, {"figure_path": "cvaSru8LeO/figures/figures_5_1.jpg", "caption": "Figure 5: Performance overview on spatial reasoning tasks. We report the accuracy averaged over all questions. We consider the VQA (Vision-only) format for vision-language models. The dashed red line denotes the expected accuracy for random guessing. For Spatial-Map and Maze-Nav tasks, only a few models outperform random guessing by a notable margin.", "description": "This figure presents the average accuracy of various language models (LLMs) and vision-language models (VLMs) across three spatial reasoning tasks: Spatial-Map, Maze-Nav, and Spatial-Grid.  The performance is evaluated using the Vision-only (VQA) format for VLMs. A dashed red line indicates the expected accuracy of random guessing. The bar chart shows that most models struggle with spatial reasoning, with only a few showing performance significantly better than random chance, especially for the Spatial-Map and Maze-Nav tasks.", "section": "3 Main Results and Analysis"}, {"figure_path": "cvaSru8LeO/figures/figures_5_2.jpg", "caption": "Figure 6: TQA (LLM) vs. VQA on spatial reasoning tasks. Each vertex on the spider plot represents the Avg Acc of a (VLM, LLM) pair with the same language backbone, i.e., LLM v.s. VLM further finetuned on that. VLMs are depicted in red, and LLMs in blue. We can see that VLMs rarely enhance the performance compared to their LLM counterparts.", "description": "This figure compares the performance of Large Language Models (LLMs) and Vision-Language Models (VLMs) on three spatial reasoning tasks: Spatial-Map, Maze-Nav, and Spatial-Grid.  For each task, it shows a spider plot where each vertex represents the average accuracy of a VLM and its corresponding LLM (sharing the same language backbone).  The plot highlights that, in most cases, adding visual information (as in VLMs) does not significantly improve performance compared to using only text (as in LLMs).  This suggests the limited effectiveness of current vision components in VLMs for spatial reasoning.", "section": "3 Main Results and Analysis"}, {"figure_path": "cvaSru8LeO/figures/figures_6_1.jpg", "caption": "Figure 7: Illustration of Random Image and Noise Image for the example in Figure 2.", "description": "This figure shows three images used in a controlled experiment to test how Vision-Language Models (VLMs) process visual information. The first image is the \"Original Image\", which is relevant to the textual description provided to the VLM.  The second image is a \"Random Image\", which is also from the dataset but is not related to the textual description. The third image is a \"Noise Image\", a random collection of pixels with no relation to the textual description or dataset. These three images were used to replace the original image in the VTQA (Vision-text input) setting to isolate the effects of visual information on model performance.", "section": "4.1 Seeing Without Understanding: The Blindness of Multimodal Language Models"}, {"figure_path": "cvaSru8LeO/figures/figures_6_2.jpg", "caption": "Figure 8: VTQA vs. TQA (VLM) on spatial reasoning tasks. VLMs exhibit improved performance in spatial reasoning tasks when visual input is absent.", "description": "This figure shows the results of experiments where the original image input in the VTQA setting was replaced with no image.  The results across three spatial reasoning tasks (Spatial-Grid, Maze-Nav, and Spatial-Map) show that removing visual input frequently leads to better performance in several vision-language models (VLMs). This suggests that the model, when textual information is sufficient to answer the questions, may perform better without additional (and potentially conflicting or misleading) visual input.", "section": "4.1 Seeing Without Understanding: The Blindness of Multimodal Language Models"}, {"figure_path": "cvaSru8LeO/figures/figures_6_3.jpg", "caption": "Figure 9: Original Image vs. Noise Image in VTQA. Replacing the original image with a Gaussian noise image improves the performance across diverse VLM architectures.", "description": "This figure shows the results of replacing the original image with a Gaussian noise image while keeping the original textual description in the VTQA (Vision-text) setting.  The experiment tests how the absence of meaningful visual information impacts various Vision-Language Models (VLMs). The results demonstrate that using a noise image instead of the original image actually improves the performance for many models, suggesting a less-than-ideal reliance on visual information in these models' spatial reasoning capabilities.", "section": "4.1 Seeing Without Understanding: The Blindness of Multimodal Language Models"}, {"figure_path": "cvaSru8LeO/figures/figures_7_1.jpg", "caption": "Figure 6: TQA (LLM) vs. VQA on spatial reasoning tasks. Each vertex on the spider plot represents the Avg Acc of a (VLM, LLM) pair with the same language backbone, i.e., LLM v.s. VLM further finetuned on that. VLMs are depicted in red, and LLMs in blue. We can see that VLMs rarely enhance the performance compared to their LLM counterparts.", "description": "This figure compares the performance of Large Language Models (LLMs) and Vision-Language Models (VLMs) on three spatial reasoning tasks (Spatial-Map, Maze-Nav, and Spatial-Grid).  Each point on the radar chart represents a pair of models (an LLM and its corresponding VLM) that share the same language backbone.  The figure demonstrates that in most cases, the addition of visual information to the VLM does not improve performance compared to the LLM alone; in fact, performance often decreases.", "section": "3 Main Results and Analysis"}, {"figure_path": "cvaSru8LeO/figures/figures_7_2.jpg", "caption": "Figure 6: TQA (LLM) vs. VQA on spatial reasoning tasks. Each vertex on the spider plot represents the Avg Acc of a (VLM, LLM) pair with the same language backbone, i.e., LLM v.s. VLM further finetuned on that. VLMs are depicted in red, and LLMs in blue. We can see that VLMs rarely enhance the performance compared to their LLM counterparts.", "description": "This figure compares the performance of Large Language Models (LLMs) and Vision-Language Models (VLMs) using text-only (TQA) and vision-only (VQA) inputs respectively on three spatial reasoning tasks.  Each point in the radar chart represents the average accuracy of a VLM/LLM pair that shares the same language model backbone.  The results show that in most cases, the vision-language models do not perform better than the language models even with additional visual information. This suggests the importance of textual input for spatial reasoning in these models.", "section": "3 Main Results and Analysis"}, {"figure_path": "cvaSru8LeO/figures/figures_8_1.jpg", "caption": "Figure 12: Comparison of Text-only input with LLM (TQA) vs. Text-only input with VLM (No Img). We consider VLMs that support text-only inputs. Each vertex on the spider plot represents the Avg Acc of a (LLM, VLM) pair with the same language model backbone.", "description": "This figure compares the performance of LLMs and VLMs using only text input.  Each vertex represents the average accuracy of a pair of models (LLM, VLM) with the same language backbone.  It helps visualize how the inclusion of visual modules in VLMs affects performance when only text is used as input, revealing that multi-modal training improves text-only performance in VLMs.", "section": "4.1 Seeing Without Understanding: The Blindness of Multimodal Language Models"}, {"figure_path": "cvaSru8LeO/figures/figures_8_2.jpg", "caption": "Figure 5: Performance overview on spatial reasoning tasks. We report the accuracy averaged over all questions. We consider the VQA (Vision-only) format for vision-language models. The dashed red line denotes the expected accuracy for random guessing. For Spatial-Map and Maze-Nav tasks, only a few models outperform random guessing by a notable margin.", "description": "This figure presents the performance of various LLMs and VLMs on three spatial reasoning tasks: Spatial-Map, Maze-Nav, and Spatial-Grid.  The accuracy is averaged across all questions for each model.  Vision-language models (VLMs) were evaluated using only visual input (VQA). A dashed red line indicates the expected accuracy from random guessing; models performing near or below this line struggled significantly with the tasks.  The figure shows that while some models outperformed random guessing, the overall performance on spatial reasoning remains challenging, particularly for the Spatial-Map and Maze-Nav tasks.", "section": "Main Results and Analysis"}, {"figure_path": "cvaSru8LeO/figures/figures_17_1.jpg", "caption": "Figure 1: Illustration of the Spatial-Map task, which simulates a map with multiple locations. To investigate the impact of modality, we consider three input formats: Text-only, Vision-only, and Vision-text. We evaluate language models (w. TQA input) and vision-language models (w. VQA and VTQA inputs) on the same set of questions.", "description": "This figure illustrates the Spatial-Map task, one of the four tasks in the SpatialEval benchmark.  The task involves understanding spatial relationships between objects shown on a map. The figure shows how the task is presented in three different input modalities: Text-only (TQA), Vision-only (VQA), and Vision-Text (VTQA).  Each modality provides different information (textual description, image only, or both) to evaluate how well different models (language models and vision-language models) can perform spatial reasoning.  The example shows a map with several locations, and example questions that test spatial reasoning ability are included.", "section": "2 Dataset and Task Construction"}, {"figure_path": "cvaSru8LeO/figures/figures_18_1.jpg", "caption": "Figure 1: Illustration of the Spatial-Map task, which simulates a map with multiple locations. To investigate the impact of modality, we consider three input formats: Text-only, Vision-only, and Vision-text. We evaluate language models (w. TQA input) and vision-language models (w. VQA and VTQA inputs) on the same set of questions.", "description": "This figure illustrates the Spatial-Map task of SpatialEval benchmark.  The task involves understanding spatial relationships between multiple locations on a map.  Three input modalities are shown: Text-only (TQA), Vision-only (VQA), and Vision-text (VTQA). The Text-only input provides a textual description of the map and object locations. The Vision-only input shows only the map image. The Vision-text input provides both the image and the textual description.  The goal is to evaluate how well different language models (LLMs) and vision-language models (VLMs) can answer spatial reasoning questions based on each modality.", "section": "2 Dataset and Task Construction"}, {"figure_path": "cvaSru8LeO/figures/figures_19_1.jpg", "caption": "Figure 6: TQA (LLM) vs. VQA on spatial reasoning tasks. Each vertex on the spider plot represents the Avg Acc of a (VLM, LLM) pair with the same language backbone, i.e., LLM v.s. VLM further finetuned on that. VLMs are depicted in red, and LLMs in blue. We can see that VLMs rarely enhance the performance compared to their LLM counterparts.", "description": "This figure compares the performance of Large Language Models (LLMs) and Vision-Language Models (VLMs) on three spatial reasoning tasks: Spatial-Map, Maze-Nav, and Spatial-Grid.  Each point on the radar chart represents a pair of models, an LLM and its corresponding VLM sharing the same language backbone.  The x-axis shows the average accuracy of the LLM on the task (using only text), while the y-axis shows the average accuracy of the VLM on the same task, using only the image (VQA) input modality.  The figure demonstrates that in most cases, VLMs do not significantly improve upon the performance of their LLM counterparts when only visual information is provided.", "section": "Main Results and Analysis"}]