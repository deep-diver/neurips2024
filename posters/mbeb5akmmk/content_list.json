[{"type": "text", "text": "Online Composite Optimization Between Stochastic and Adversarial Environments ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yibo Wang1,2, Sijia Chen1,2, Wei Jiang1, Wenhao Yang1,2, Yuanyu Wan3,1, Lijun Zhang1,2,\u2217 ", "page_idx": 0}, {"type": "text", "text": "2School of Artificial Intelligence, Nanjing University, Nanjing, China 3School of Software Technology, Zhejiang University, Ningbo, China {wangyb, chensj, jiangw, yangwh, zhanglj}@lamda.nju.edu.cn, wanyy@zju.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study online composite optimization under the Stochastically Extended Adversarial (SEA) model. Specifically, each loss function consists of two parts: a fixed non-smooth and convex regularizer, and a time-varying function which can be chosen either stochastically, adversarially, or in a manner that interpolates between the two extremes. In this setting, we show that for smooth and convex time-varying functions, optimistic composite mirror descent (OptCMD) can obtain an $\\mathcal{O}(\\sqrt{\\sigma_{1:T}^{2}}+\\sqrt{\\Sigma_{1:T}^{2}})$ regret bound, where $\\sigma_{1:T}^{2}$ and $\\Sigma_{1:T}^{2}$ denote the cumulative stochastic variance and the cumulative adversarial variation of time-varying functions, respectively. For smooth and strongly convex time-varying functions, we idseh naont $\\mathcal{O}((\\bar{\\sigma}_{\\mathrm{max}}^{2}+\\Sigma_{\\mathrm{max}}^{2})\\log(\\sigma_{1:T}^{2}+\\dot{\\Sigma}_{1:T}^{2}))$ trheeg rmeta bxioumnald ,a dwvheerrsea $\\sigma_{\\mathrm{max}}^{2}$ aarinad$\\Sigma_{\\mathrm{max}}^{2}$ tion, respectively. For smooth and exp-concave time-varying functions, we achieve an $\\mathcal{O}(d\\operatorname{iog}({\\sigma_{1:T}^{2}+\\Sigma_{1:T}^{2}}))$ bound where $d$ denotes the dimensionality. Moreover, to deal with the unknown function type in practical problems, we propose a multilevel universal algorithm that is able to achieve the desirable bounds for three types of time-varying functions simultaneously. It should be noticed that all our findings match existing bounds for the SEA model without the regularizer, which implies that there is no price in regret bounds for the beneftis gained from the regularizer. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Online composite optimization has drawn considerable attention in recent years [Duchi and Singer, 2009, Ghadimi and Lan, 2012, Lei and Zhou, 2017, Scroccaro et al., 2023]. Formally, it can be viewed as an iterative game between a learner and the environment. In each round $t$ , the learner makes a decision $\\mathbf{x}_{t}$ from a convex set $\\mathcal{X}\\subseteq\\mathbb{R}^{d}$ and then suffers a loss $\\phi_{t}(\\mathbf{x}_{t})$ in the form of ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\phi_{t}(\\mathbf{x})=f_{t}(\\mathbf{x})+r(\\mathbf{x}),\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $f_{t}(\\cdot):\\mathcal{X}\\to\\mathbb{R}$ denotes the time-varying function that is chosen by the environment, and $r(\\cdot):\\mathcal{X}\\overset{\\cdot}{\\rightarrow}\\mathbb{R}$ denotes the fixed non-smooth and convex regularizer, such as the $\\ell_{1}$ -norm for sparse vectors and the trace norm for low-rank matrices [Langford et al., 2009, Flammarion and Bach, 2017, Zhang et al., 2019, Garber and Kaplan, 2019]. In the literature, online composite optimization is generally divided into two categories: stochastic composite optimization [Lan, 2012, 2016, Zhang et al., 2017, Lei and Tang, 2018, Lei et al., 2019, Kulunchakov and Mairal, 2019] and adversarial composite optimization [Xiao, 2009, Duchi et al., 2010, Mohri and Yang, 2016, Joulani et al., 2020, Yang et al., 2024c]. In the former one, $f_{t}(\\cdot)$ is assumed to be independent and identically distributed (i.i.d.) over time; in the latter one, $f_{t}(\\cdot)$ can be chosen arbitrarily or even adversarially. However, the environment in real-world scenarios is seldom purely stochastic or adversarial, but rather falls somewhere in between [Amir et al., 2020, Garber et al., 2020, Sherman et al., 2021, Zimmert and Seldin, 2021, Ito, 2021], and our understanding for the more common intermediate scenarios remains limited in online composite optimization. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Recently, Sachs et al. [2022] introduce the Stochastically Extended Adversarial (SEA) model as an intermediate setting in online optimization without regularizer, i.e., $r(\\mathbf{x})=0$ . In the SEA model, loss functions are not restricted to fully i.i.d. or adversarial; instead, the environment selects them from any intermediate state between the two extreme settings. To reflect how stochastic or adversarial the environments are, they introduce the cumulative stochastic variance $\\sigma_{1:T}^{2}$ and the cumulative adversarial variation $\\Sigma_{1:T}^{2}$ , as shown below: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\sigma_{1:T}^{2}=\\mathbb{E}\\left[\\sum_{t=1}^{T}\\sigma_{t}^{2}\\right]\\mathrm{~and~}\\Sigma_{1:T}^{2}=\\mathbb{E}\\left[\\sum_{t=1}^{T}\\operatorname*{sup}_{\\mathbf{x}\\in\\mathcal{X}}\\left\\lVert\\nabla F_{t}(\\mathbf{x})-\\nabla F_{t-1}(\\mathbf{x})\\right\\rVert_{2}^{2}\\right],\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\begin{array}{r}{\\sigma_{t}^{2}=\\operatorname*{sup}_{\\mathbf{x}\\in\\mathcal{X}}\\mathbb{E}_{f_{t}\\sim\\mathcal{D}_{t}}[\\|\\nabla f_{t}(\\mathbf{x})-\\nabla F_{t}(\\mathbf{x})\\|_{2}^{2}]}\\end{array}$ denotes the variance of gradients and $F_{t}(\\mathbf{x})=$ $\\mathbb{E}_{f_{t}\\sim\\mathcal{D}_{t}}[f_{t}(\\mathbf{x})]$ denotes the expected function for $f_{t}(\\cdot)$ . For the SEA model, two classical algorithms in optimistic online learning [Rakhlin and Sridharan, 2013]\u2014optimistic Follow-The-Regularized Leader (FTRL) [Sachs et al., 2022] and optimistic Online Mirror Descent (OMD) [Chen et al., 2023]\u2014have been proven ensuring sublinear regret bounds with respect to both $\\sigma_{1:T}^{2}$ and $\\Sigma_{1:T}^{2}$ . ", "page_idx": 1}, {"type": "text", "text": "In this paper, we extend the SEA model into online composite optimization, termed as composite SEA, to bridge the gap between stochastic and adversarial composite optimization. Specifically, in (1), $r(\\cdot)$ remains fixed over time and $f_{t}(\\cdot)$ is selected from a distribution $\\mathcal{D}_{t}$ , which is chosen by the environments either stochastically, adversarially, or in a manner that interpolates between the two extremes. The goal of composite SEA is to minimize the expected regret in terms of (1): ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\mathrm{Regret}_{T}\\right]=\\mathbb{E}\\left[\\sum_{t=1}^{T}\\left[f_{t}(\\mathbf{x}_{t})+r(\\mathbf{x}_{t})\\right]-\\operatorname*{min}_{\\mathbf{x}\\in\\mathcal{X}}\\sum_{t=1}^{T}\\left[f_{t}(\\mathbf{x})+r(\\mathbf{x})\\right]\\right],\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "which benchmarks the cumulative composite loss of the learner and that of the best fixed decision. To handle the composite setting, a natural impulse is to treat $f_{t}(\\cdot)+r(\\cdot)$ as one function and directly apply existing methods for the SEA model, i.e., optimistic FTRL and optimistic OMD. However, such a straightforward application is unsuitable because (i) these methods heavily rely on the smoothness of loss functions, but due to the non-smooth component $r(\\cdot)$ , the summation function $\\phi_{t}(\\cdot)=f_{t}(\\cdot)\\!+\\!r(\\cdot)$ loses this crucial property; (ii) directly applying these methods ignores the presence of $r(\\cdot)$ and thus fails to gain the beneftis from the regularizer, e.g., the sparsity induced by the $\\ell_{1}$ -norm. Therefore, a natural question arises whether it is possible to deal with both the intermediate nature of environments and the composite structure of loss functions concurrently. ", "page_idx": 1}, {"type": "text", "text": "We affirmatively answer the above question by revisiting a variant of optimistic OMD [Scroccaro et al., 2023], named Optimistic Composite Mirror Descent (OptCMD). This method inherits the idea of optimistic online learning [Rakhlin and Sridharan, 2013], i.e., exploiting estimates on upcoming loss functions for decision updates, and can disentangle $f_{t}(\\cdot)$ and $r(\\cdot)$ during the optimization, thereby effectively leveraging distinct properties of each. However, OptCMD is originally designed for adversarial composite optimization, and a simple extension will lead to unsatisfactory bounds in composite SEA. In this paper, we reanalyze OptCMD and show that with suitable configurations, OptCMD attains $\\mathcal{O}(\\sqrt{\\sigma_{1:T}^{2}}+\\sqrt{\\Sigma_{1:T}^{2}})$ , $\\mathcal{O}((\\sigma_{\\mathrm{max}}^{2}+\\Sigma_{\\mathrm{max}}^{2})\\log(\\sigma_{1:T}^{2}+\\Sigma_{1:T}^{2}))$ and ${\\mathcal{O}}(d\\log(\\sigma_{1:T}^{2}+$ $\\Sigma_{1:T}^{2})$ ) bounds for smooth and general convex, smooth and strongly convex, and smooth and expcaonnd caadvve etrismaeri-avla rcyoinmgp fousintcet ioopntsi, mriezsapteicotni,v ealny.d  Ocaurn  frienddiuncges  tgoe tnheeramli zbey  psrpeevciioaulsi zriensgu $\\sigma_{1:T}^{2}$ satnodc $\\Sigma_{1:T}^{2}$ Moreover, our results coincide with existing bounds for the SEA model [Sachs et al., 2022, Chen et al., 2023], which indicates that there is no price in regret bounds for the benefits from $r(\\cdot)$ . ", "page_idx": 1}, {"type": "text", "text": "One concern of OptCMD is the requirement of the function type in advance, which is often impractical in real-world problems and motivates us to design a universal algorithm that is agnostic to the prior knowledge about loss functions. We note that recently, Yan et al. [2023] have introduced a universal algorithm for the SEA model without regularizer, based on the meta-expert structure in online learning [van Erven and Koolen, 2016, Mhammedi et al., 2019, Wang et al., 2019, 2020, van Erven et al., 2021, Zhang et al., 2022, Yan et al., 2024, Yang et al., 2024a,b]. However, their method [Yan et al., 2023] cannot naturally support the composite setting, as it highly depends on the smoothness of the (summation) loss functions, which does not necessarily hold in the composite SEA model. In this paper, we propose a novel universal algorithm for composite SEA, based on the observations that (i) the regularizer remains fixed over time and is accessible to the meta-algorithm from the beginning round; (ii) the meta-algorithm is able to obtain expert decisions for the current round before estimating their performance. With these two facts, we can utilize the information about $r(\\cdot)$ to track experts, ensuring the meta-regret will not deviate from that of the best one. Specifically, inspired by Yan et al. [2023], we employ a two-layer Multi-scale Multiplicative-weight with Correction (MsMwC) [Chen et al., 2021] as the meta-algorithm, and choose OptCMD as the expert-algorithm. To effectively estimate the expert performance, we first collect decisions of each expert for the current round, and then explicitly integrate them and expert performance on $r(\\cdot)$ into the losses and optimisms used in MsMwC. Theoretical analysis demonstrates that, with the proposed integration, our universal algorithm can achieve desirable bounds for three types of loss functions simultaneously. We summarize our contributions as shown below. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "\u2022 We first introduce the composite SEA model, which serves as an intermediate setting between stochastic and adversarial composite optimization and can naturally adapt to two extreme settings; \u2022 For the composite SEA model, we demonstrate that OptCMD can attain the regret bounds of $\\mathcal{O}(\\sqrt{\\sigma_{1:T}^{2}}+\\sqrt{\\Sigma_{1:T}^{2}})$ , $\\mathcal{O}((\\sigma_{\\operatorname*{max}}^{2}\\!+\\!\\Sigma_{\\operatorname*{max}}^{2})\\log(\\sigma_{1:T}^{2}\\!+\\!\\Sigma_{1:T}^{2}))$ and $\\mathcal{O}(d\\log(\\sigma_{1:T}^{2}+\\Sigma_{1:T}^{2}))$ for smooth and general convex, smooth and strongly convex, and smooth and exp-concave time-varying functions, respectively. Owing to the versatility of $\\sigma_{1:T}^{2}$ and $\\Sigma_{1:T}^{2}$ , these bounds can recover previous results in stochastic and adversarial composite optimization; \u2022 Moreover, to handle the unknown function type, we propose a new universal algorithm in composite SEA and show that it can concurrently achieve desirable bounds for three kinds of functions; \u2022 Finally, we discuss implications of our theoretical findings for two common intermediate examples with composite loss functions, and derive favorable bounds specific to these practical scenarios. ", "page_idx": 2}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we briefly review related work on adversarial and stochastic composite optimization, the SEA model and universal online learning. ", "page_idx": 2}, {"type": "text", "text": "Adversarial composite optimization. In this setting, $f_{t}(\\cdot)$ is assumed to be chosen by the environments arbitrarily or even adversarially, and the performance is measured by regret shown in brackets of (3). In\u221a the literature, the seminal work [Duchi and Singer, 2009] proposes the FOBOS method with $\\mathcal{O}(\\sqrt{T})$ and $\\mathcal{O}(\\lambda^{-1}\\log T)$ regret bounds for general convex and $\\lambda$ -strongly convex $f_{t}(\\cdot)$ , respectively. Later, Xiao [2009] proposes the RDA method based on the primal-dual subgradient framework [Nesterov, 2009], and shows that RDA is able to generate sparser decisions than FOBOS when $r(\\mathbf{x})=\\mu\\|\\mathbf{x}\\|_{1}$ for some $\\mu>0$ . In the same time, another subsequent work [Duchi et al., 2010] introduces the COMID method under the mirror descent framework [Beck and Teboulle, 2003], and achieves the same bounds as FOBOS. Recently, for $\\alpha$ -exp-concave $f_{t}(\\cdot)$ , Yang et al. [2024c] establish an ${\\mathcal{O}}((d/\\alpha)\\log T)$ regret bound by proposing the ProxONS method. ", "page_idx": 2}, {"type": "text", "text": "Besides, there exist other powerful methods [Mohri and Yang, 2016, Joulani et al., 2020, Scroccaro et al., 2023] equipped with problem-dependent bounds. These bounds can safeguard the above results in the worst-case scenarios and become tighter when environments have special properties, such as smoothness. Among them, the most related work is by Scroccaro et al. [2023], who develop OptCMD based on the optimistic online learning framework [Rakhlin and Sridharan, 2013]. The key idea is to make an estimate for the upcoming loss function, which ensures tighter bounds when the estimate is accurate and still main\u221atains the worst-case bound otherwise. By utilizing the smoothness of $f_{t}(\\cdot)$ , OptCMD achieves $O(\\sqrt{V_{T}})$ and $O(\\lambda^{-1}\\log V_{T})$ regret bounds for general convex and $\\lambda$ -strongly convex $f_{t}(\\cdot)$ , respectively, where $\\begin{array}{r}{V_{T}=\\sum_{t=1}^{T}\\operatorname*{max}_{\\mathbf{x}\\in\\mathcal{X}}\\|\\nabla f_{t}(\\mathbf{x})-\\nabla f_{t-1}(\\mathbf{x})\\|_{2}^{2}}\\end{array}$ denotes the gradient-variation and can be small when $f_{t}(\\cdot)$ gradually changes. ", "page_idx": 2}, {"type": "text", "text": "Stochastic composite optimization. In this setting, $f_{t}(\\cdot)$ is assumed to be i.i.d. sampled from a fixed distribution $\\mathcal{D}$ , and the goal is to minimize the composite objective: $\\begin{array}{r}{\\operatorname*{min}_{\\mathbf{x}\\in\\mathcal{X}}\\mathbb{E}_{f\\sim\\mathcal{D}}[f(\\mathbf{x})]+r(\\mathbf{x})}\\end{array}$ . The performance is measured by excess risk, which compares the solution with the optimal one, i.e., $\\begin{array}{r}{\\mathbb{E}_{f\\sim\\mathcal{D}}[f(\\mathbf{x}_{T})]+r(\\mathbf{x}_{T})-\\operatorname*{min}_{\\mathbf{x}\\in\\mathcal{X}}\\{\\mathbb{E}_{f\\sim\\mathcal{D}}[f(\\mathbf{x})]+r(\\mathbf{x})\\}}\\end{array}$ . ", "page_idx": 2}, {"type": "text", "text": "In the literature, there are a substantial body of methods designed for the stochastic setting [Ghadimi and Lan, 2012, Lan, 2012, 2016, Lei and Tang, 2018, Lei et al., 2019], and by utilizing the online-tobatch conversion technique [Cesa-Bianchi et al., 2004], existing methods for adversarial composite optimization [Xiao, 2009, Duchi et al., 2010, Yang et al., $2024\\mathrm{c}]$ c\u221aan also be extended to the stochastic scenarios. Specifically, Lan [2012] first establishes an ${\\mathcal{O}}(1/{\\sqrt{T}})$ excess risk bound for general convex objective, and then Lan [2016] improves the rate to $\\mathcal{O}(1/T)$ for strongly convex objective. By utilizing the online-to-batch conversion, both RDA [Xiao, 2009] and COMID [Duchi et al., 2010] achieve ${\\mathcal{O}}(1/{\\sqrt{T}})$ and ${\\mathcal{O}}(\\log T/(\\lambda T))$ excess risks for general convex and $\\lambda$ -strongly convex objective, respectively. When $f(\\cdot)$ is $\\alpha$ -exp-concave, ProxONS [Yang et al., 2024c] ensures an ${\\mathcal{O}}(d\\log T/(\\alpha T))$ excess bound. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "SEA model. The SEA model is originally introduced by Sachs et al. [2022] as an intermediate setting in online optimization without regularizer. Moreover, they also propose two versatile quantities $\\sigma_{1:T}^{2}$ and $\\bar{\\Sigma_{1:T}^{2}}$ to reflect the stochastic and adversarial aspect of the environments, respectively. Theoretically, for the SEA model, Sachs et al. [2022] prove that optimistic FTRL enjoys the bounds of $\\mathcal{O}(\\sqrt{\\sigma_{1:T}^{2}}+\\sqrt{\\Sigma_{1:T}^{2}})$ and $\\mathcal{O}(\\lambda^{-1}(\\sigma_{\\operatorname*{max}}^{2}+\\Sigma_{\\operatorname*{max}}^{2})\\log T)$ for smooth and general convex, and smooth and $\\lambda_{}$ -strongly convex loss functions, respectively. Later, Chen et al. [2023] demonstrate that optimistic OMD is able to attain the same bound for general convex losses and an improved $\\mathcal{O}(\\lambda^{-\\hat{1}}(\\sigma_{\\operatorname*{max}}^{2}+\\Sigma_{\\operatorname*{max}}^{2})\\log(\\sigma_{1:T}^{2}+\\Sigma_{1:T}^{2}))$ ) obuonudn fdo rf otrh es smtrooonthg lay ncdo v-eexx pl-ocsosensc.a vMe ofruenocvteiro, ntsh.ey also $\\mathcal{O}((d/\\alpha)\\log(\\sigma_{1:T}^{2}+\\Sigma_{1:T}^{2}))$ $\\alpha$ ", "page_idx": 3}, {"type": "text", "text": "Universal online learning. The universal online learning is proposed to handle the uncertainty of the loss function types, when applying online algorithms to practical optimization problems. The center to universal algorithms is the powerful meta-expert structure [van Erven and Koolen, 2016, Mhammedi et al., 2019, van Erven et al., 2021], which is also widely-used in many other fields of online learning [Daniely et al., 2015, Jun et al., 2017a,b, Zhang et al., 2018, 2020, 2021, Cutkosky, 2020, Wan et al., 2021, 2022b, Wang et al., 2024, Wan et al., 2024a]. The key idea of meta-expert structure is to maintain multiple experts to process different types of loss functions and then, deploy a meta-algorithm to combine the decisions from experts [Wang et al., 2019, 2020, van Erven et al., 2021, Zhang et al., 2022, Yang et al., 2024a,b]. In the literature, the most related work is Yan et al. [2023], who propose a novel universal algorithm with a two-layer MSMWC [Chen et al., 2021] as the meta-algorithm. By maintaining multiple instances of optimistic OMD, their method can adapt to the SEA model and deliver the same bounds as those in Chen et al. [2023]. Recently, Yan et al. [2024] further improve this method by employing a simpler meta-algorithm while achieving optimal regret bounds for three types of functions. However, it should be noticed that these methods [Yan et al., 2023, 2024] heavily relies on the smoothness of the summation function (1), and thus cannot handle the composite structure in loss functions well, as previously discussed. ", "page_idx": 3}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we introduce some preliminaries, including standard assumptions and a brief review of OptCMD [Scroccaro et al., 2023]. ", "page_idx": 3}, {"type": "text", "text": "3.1 Assumptions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We first list the common assumptions in prior studies [Duchi et al., 2010, Chen et al., 2023, 2024].   \nAssumption 1. The convex decision set $\\mathcal{X}$ belongs to an Euclidean ball with the diameter $D$ . ", "page_idx": 3}, {"type": "text", "text": "Assumption 2. At each round $t_{\\mathrm{:}}$ , the random function $f_{t}(\\cdot)$ is $G$ -Lipschitz over $\\mathcal{X}$ , i.e., ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\forall\\mathbf{x},\\mathbf{y}\\in{\\mathcal{X}},\\;|f_{t}(\\mathbf{x})-f_{t}(\\mathbf{y})|\\leq G\\|\\mathbf{x}-\\mathbf{y}\\|_{2}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Assumption 3. The regularized function $r(\\cdot)$ is convex and bounded over $\\mathcal{X}$ , i.e., ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\forall\\mathbf{x},\\mathbf{y}\\in\\mathcal{X},\\;r(\\mathbf{y})\\geq r(\\mathbf{x})+\\nabla r(\\mathbf{x})^{\\top}(\\mathbf{y}-\\mathbf{x})\\;a n d\\,\\forall\\mathbf{x}\\in\\mathcal{X},\\;0\\leq r(\\mathbf{x})\\leq C.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Assumption 4. At each round $t_{\\perp}$ , the expected function $F_{t}(\\cdot)$ is $H$ -smooth over $\\mathcal{X}$ , i.e., ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\forall\\mathbf{x},\\mathbf{y}\\in\\mathcal{X},\\;\\|\\nabla F_{t}(\\mathbf{x})-\\nabla F_{t}(\\mathbf{y})\\|_{2}\\leq H\\|\\mathbf{x}-\\mathbf{y}\\|_{2}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Assumption 5. At each round $t$ , the expected function $F_{t}(\\cdot)$ is convex over $\\mathcal{X}$ , i.e., ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\forall\\mathbf{x},\\mathbf{y}\\in{\\boldsymbol{\\mathcal{X}}},\\;F_{t}(\\mathbf{y})\\geq F_{t}(\\mathbf{x})+\\nabla F_{t}(\\mathbf{x})^{\\top}(\\mathbf{y}-\\mathbf{x}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Assumption 6. At each round $t$ , the expected function $F_{t}(\\cdot)$ is $\\lambda$ -strongly convex over $\\mathcal{X}$ , i.e., ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\forall\\mathbf{x},\\mathbf{y}\\in{\\boldsymbol{\\mathcal{X}}},\\;F_{t}(\\mathbf{y})\\geq F_{t}(\\mathbf{x})+\\nabla F_{t}(\\mathbf{x})^{\\top}(\\mathbf{y}-\\mathbf{x})+({\\boldsymbol{\\lambda}}/2)\\|\\mathbf{x}-\\mathbf{y}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Algorithm 1 Optimistic Composite Mirror Descent (OptCMD) ", "text_level": 1, "page_idx": 4}, {"type": "table", "img_path": "MbEB5aKmMK/tmp/8be7ac212b2a1e45e87cf3030e4c30bbb9f601c229cfef6ffe8898c927a0136b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "Assumption 7. All the variance of the gradients and the adversarial variations are bounded, i.e., ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\forall t\\in[T],\\,\\sigma_{t}^{2}\\leq\\sigma_{\\operatorname*{max}}^{2}\\,a n d\\,\\operatorname*{sup}_{\\mathbf{x}\\in\\mathcal{X}}\\|\\nabla F_{t}(\\mathbf{x})-\\nabla F_{t-1}(\\mathbf{x})\\|_{2}^{2}\\leq\\Sigma_{\\operatorname*{max}}^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Assumption 8. At each round $t$ , the time-varying function $f_{t}(\\cdot)$ is $\\alpha$ -exp-concave over $\\mathcal{X}$ , i.e., $\\forall\\mathbf{x}\\in\\mathcal{X}$ , $\\exp(-\\alpha f_{t}(\\mathbf{x}))$ is concave. ", "page_idx": 4}, {"type": "text", "text": "3.2 Optimistic composite mirror descent ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "OptCMD is an algorithmic realization of the powerful optimistic online learning [Rakhlin and Sridharan, 2013] in online composite optimization. Specifically, in each round $t$ , the learner submits a decision $\\mathbf{x}_{t}$ and suffers a loss $f_{t}(\\mathbf{x}_{t})\\bar{+}\\,r(\\mathbf{x}_{t})$ . Then, the learner receives an optimism $M_{t+1}$ that serves as an optimistic prediction for the gradient of subsequent function $f_{t+1}\\bar{(\\cdot)}$ . After that, the learner performs the following update steps: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\mathbf{x}}_{t+1}=\\underset{\\mathbf{x}\\in\\mathcal{X}}{\\mathrm{argmin}}\\left\\{\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}\\rangle+r(\\mathbf{x})+\\mathcal{B}^{\\mathcal{R}_{t}}(\\mathbf{x},\\hat{\\mathbf{x}}_{t})\\right\\}}\\\\ &{\\mathbf{x}_{t+1}=\\underset{\\mathbf{x}\\in\\mathcal{X}}{\\mathrm{argmin}}\\left\\{\\langle M_{t+1},\\mathbf{x}\\rangle+r(\\mathbf{x})+\\mathcal{B}^{\\mathcal{R}_{t+1}}(\\mathbf{x},\\hat{\\mathbf{x}}_{t+1})\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathcal{B}^{\\mathcal{R}_{t}}(\\mathbf{x},\\mathbf{y})=\\mathcal{R}_{t}(\\mathbf{x})-\\mathcal{R}_{t}(\\mathbf{y})-\\langle\\nabla\\mathcal{R}_{t}(\\mathbf{y}),\\mathbf{x}-\\mathbf{y}\\rangle$ denotes the Bregman divergence associated with a time-varying function $\\mathcal{R}_{t}$ , of which the specific form depends on the type of $f_{t}(\\cdot)$ and will be illuminated later. Note that in (4) and (5), $r(\\cdot)$ remains non-linearilzed, which allows the decisions $\\{\\mathbf{x}_{t}\\}_{t=1}^{T}$ to possess properties externally conferred by $r(\\cdot)$ , such as the sparsity. ", "page_idx": 4}, {"type": "text", "text": "Remark. Scroccaro et al. [2023] specialize the estimate $M_{t+1}\\,=\\,\\nabla\\hat{f}_{t+1}\\big(\\hat{\\mathbf{x}}_{t+1}\\big)$ , where $\\hat{f}_{t+1}(\\cdot)$ denotes the function prediction of $f_{t+1}(\\cdot)$ and is generally set as $f_{t}(\\cdot)$ in practice. We hightlight that this specification is unsuitable for the composite SEA model and inadvertently introduces a dependency issue during the analysis. More detailed discussions can be found in Sections 4.1 and 4.2. ", "page_idx": 4}, {"type": "text", "text": "4 OptCMD for composite SEA ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we provide our results for general convex, strongly convex and exp-concave cases in the composite SEA model. Due to space limitations, all the proofs are deferred to Appendix B. ", "page_idx": 4}, {"type": "text", "text": "4.1 General convex case ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Initially, we consider the case where the regualizer $r(\\cdot)$ is convex and non-smooth, and the expected functions $F_{t}(\\cdot)$ are general convex and smooth. We choose the following configuration for this case. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{R}_{t}(\\mathbf{x})=\\frac{1}{2\\eta_{t}}\\|\\mathbf{x}\\|_{2}^{2},\\;\\eta_{t}=\\operatorname*{min}\\left\\{\\frac{D}{\\sqrt{1+\\bar{V}_{t-1}}},\\frac{D}{\\delta}\\right\\},\\;\\mathrm{and}\\;M_{t+1}=\\nabla f_{t}(\\mathbf{x}_{t}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\begin{array}{r}{\\bar{V}_{t-1}=\\sum_{s=1}^{t-1}\\|\\nabla f_{s}(\\mathbf{x}_{s})-\\nabla f_{s-1}(\\mathbf{x}_{s-1})\\|_{2}^{2}}\\end{array}$ and $\\delta>0$ denotes the hyperparameter. With the configuration i n (6), we establish the following regret bound for the general convex case. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. Under Assumptions 1, 2, 3, 4 and 5, Algorithm $^{\\,I}$ ensures ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[{\\mathrm{Regret}}_{T}\\right]=\\mathcal{O}\\left(\\sqrt{\\sigma_{1:T}^{2}}+\\sqrt{\\Sigma_{1:T}^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "with the configuration in (6) where $\\delta=6\\sqrt{2}H D$ . ", "page_idx": 4}, {"type": "text", "text": "Remark. For the \u221afully adversarial environments where $\\Sigma_{1:T}^{2}\\,=\\,V_{T}$ and $\\sigma_{1:T}^{2}\\,=\\,0$ , our bound degenerates to $O(\\sqrt{V_{T}})$ matching the previous result of Scroccaro et al. [2023]. For the fully stochastic environm\u221aents where $\\Sigma_{1:T}^{\\overline{{2}}}\\,=\\,0$ and $\\sigma_{1:T}^{2}\\,=\\,\\sigma^{2}T$ with th\u221ae stochastic variance $\\sigma^{2}$ , our result becomes $\\mathcal{O}(\\sqrt{T})$ regret bound and further delivers an ${\\mathcal{O}}(1/{\\sqrt{T}})$ excess risk bound by the online-to-batch conversion [Cesa-Bianchi et al., 2004], which coincides with the results of Lan [2012]. Furthermore, our finding also aligns with existing bounds for the SEA model without $r(\\cdot)$ [Sachs et al., 2022, Chen et al., 2023]. In other words, our method pays no price in regret bounds for handling the additional regularizer. ", "page_idx": 5}, {"type": "text", "text": "Beyond the configuration in (6), we also employ those adopted by Scroccaro et al. [2023]: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{R}_{t}(\\mathbf{x})=\\frac{1}{2\\eta_{t}}\\|\\mathbf{x}\\|_{2}^{2},\\;\\eta_{t}=\\frac{1}{\\sqrt{4H^{2}+\\bar{D}_{t-1}}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "wweh eorbe $\\bar{D}_{0}=0$ oallnod $\\begin{array}{r}{\\bar{D}_{t-1}=\\sum_{s=1}^{t-1}\\|\\nabla f_{s}(\\hat{\\mathbf{x}}_{s})-\\nabla f_{s-1}(\\hat{\\mathbf{x}}_{s})\\|_{2}^{2}}\\end{array}$ . By setting the configuration in (7), ", "page_idx": 5}, {"type": "text", "text": "Theorem 2. Under Assumptions 1, 2, 3, 4 and 5, with configuration in (7), Algorithm $^{\\,I}$ ensures ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\mathrm{Regret}_{T}\\right]=\\mathcal{O}\\left(\\sqrt{\\tilde{\\sigma}_{1:T}^{2}}+\\sqrt{\\Sigma_{1:T}^{2}}\\right),}\\\\ {\\tilde{\\sigma}_{t}^{2}=\\mathbb{E}_{f_{t}\\sim\\mathcal{D}_{t}}[\\operatorname*{sup}_{\\mathbf{x}\\in\\mathcal{X}}\\|\\nabla f_{t}(\\mathbf{x})-\\nabla F_{t}(\\mathbf{x})\\|_{2}^{2}].\\quad\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\tilde{\\sigma}_{1:T}^{2}$ with ", "page_idx": 5}, {"type": "text", "text": "Remark. This bound is less favorable than that in Theorem 1, as it scales with a new quantity $\\tilde{\\sigma}_{1;T}^{2}$ , which also measures the stochasticity in composite SEA but is larger than $\\sigma_{1:T}^{2}$ because of the fact that $\\begin{array}{r}{\\mathbb{E}_{f_{t}\\sim\\mathcal{D}_{t}}[\\operatorname*{sup}_{\\mathbf{x}\\in\\mathcal{X}}\\|\\nabla f_{t}(\\mathbf{x})-\\nabla F_{t}(\\mathbf{x})\\|_{2}^{2}]\\overset{{,}}{\\geq}\\operatorname*{sup}_{\\mathbf{x}\\in\\mathcal{X}}\\mathbb{E}_{f_{t}\\sim\\mathcal{D}_{t}}[\\|\\bar{\\nabla}f_{t}(\\mathbf{x})-\\bar{\\nabla}\\hat{F}_{t}(\\mathbf{x})\\|_{2}^{2}].}\\end{array}$ , where the inequality is due to the convexity of supremum operator. This discrepancy arises from the inappropriate choice of $M_{t+1}=\\nabla f_{t}(\\hat{\\mathbf{x}}_{t+1})$ , where $\\hat{\\mathbf{x}}_{t+1}$ is generated based on $\\nabla f_{t}(\\mathbf{x}_{t})$ shown in (4), which implies that $\\hat{\\mathbf{x}}_{t+1}$ has already incorporated partial information about $f_{t}(\\cdot)$ , leading to a dependence issue in analysis. To remove the dependency, we apply the supremum operator on $\\hat{\\mathbf{x}}_{t}$ over $\\mathcal{X}$ (c.f. Lemma 4) but inevitably establish a reliance on $\\tilde{\\sigma}_{1:T}^{2}$ in the final bound. ", "page_idx": 5}, {"type": "text", "text": "4.2 Strongly convex case ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this part, we focus on the case where the regualizer $r(\\cdot)$ is non-smooth and convex, and the expected functions $F_{t}(\\cdot)$ are smooth and $\\lambda$ -strongly convex. We set OptCMD with the configurations: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{R}_{t}(\\mathbf{x})=\\frac{1}{2\\eta_{t}}\\|\\mathbf{x}\\|_{2}^{2},\\;\\eta_{t}=\\frac{2}{\\delta+\\lambda t},\\;\\mathrm{and}\\;M_{t+1}=\\nabla f_{t}(\\mathbf{x}_{t}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\delta>0$ denotes the hyperparameter. The theoretical guarantee for this case is shown below. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3. Under Assumptions 1, 2, 3, 4, 6 and 7, Algorithm $^{\\,I}$ ensures ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[{\\mathrm{Regret}}_{T}\\right]=\\mathcal{O}\\left(\\frac{1}{\\lambda}\\left(\\sigma_{\\operatorname*{max}}^{2}+\\Sigma_{\\operatorname*{max}}^{2}\\right)\\log\\left(\\sigma_{1:T}^{2}+\\Sigma_{1:T}^{2}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with the configuration in (8) where $\\delta=128H^{2}D^{2}$ . ", "page_idx": 5}, {"type": "text", "text": "Remark. Similar to Theorem 1, this bound reduces to $O(\\lambda^{-1}\\log V_{T})$ matching that of Scroccaro et al. [2023] when environments become fully adversarial, and derives the same ${\\mathcal{O}}(\\log T/(\\lambda T))$ excess risk bound as that of Duchi and Singer [2009] and Xiao [2009] when environments become fully stochastic. Moreover, this bound also recovers that of Chen et al. [2023] for the SEA model. ", "page_idx": 5}, {"type": "text", "text": "Additionally, we further equip OptCMD with the original configuration of Scroccaro et al. [2023]: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{R}_{t}(\\mathbf{x})=\\frac{1}{2\\eta_{t}}\\|\\mathbf{x}\\|_{2}^{2},\\;\\eta_{t}=\\frac{1}{4H^{2}+(\\lambda/2G^{2})\\bar{D}_{t-1}},\\;\\mathrm{and}\\;M_{t+1}=\\nabla f_{t}(\\hat{\\mathbf{x}}_{t+1}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and obtain the following theorem. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4. Under Assumptions 1, 2, 3, 4 and $6$ , with the configuration in (9), Algorithm $^{\\,l}$ ensures ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\mathrm{Regret}_{T}\\right]=\\mathcal{O}\\left(\\frac{G^{2}}{\\lambda}\\log\\left(\\tilde{\\sigma}_{1:T}^{2}+\\Sigma_{1:T}^{2}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Remark. This bound suffers two limitations. Firstly, it depends on the unfavorable $\\tilde{\\sigma}_{1:T}^{2}$ , which is due to the improper estimation $M_{t+1}$ in (9). Secondly, it scales with ${\\mathcal{O}}(G^{2})$ due to the choice of $\\eta_{t}$ in (9), which is not tighter than $\\mathcal{O}(\\bar{\\sigma}_{\\mathrm{max}}^{2}+\\Sigma_{\\mathrm{max}}^{2})$ in Theorem 3. ", "page_idx": 5}, {"type": "text", "text": "Initialization: $\\mathcal{M}_{\\mathrm{top}}$ with $\\eta^{k}=(C_{0}\\cdot2^{k})^{-1}$ and $\\hat{q}_{1}^{k}=(\\eta^{k})^{2}/\\sum_{k=1}^{K}(\\eta^{k})^{2}$ ; for each $k\\in[K]$ , Mkmid with $\\eta^{k,i}=2\\eta^{k}$ and $\\hat{p}_{1}^{k,i}=1/N$ , and experts $\\{E^{k,i}\\}_{i\\in[N]}$ ", "page_idx": 6}, {"type": "text", "text": "1: for $t=1$ to $T$ do   \n2: Receive $\\mathbf{x}_{t}^{k,i}$ from expert $E^{k,i}$ , and update $\\mathbf{q}_{t}$ and $\\mathbf{p}_{t}^{k}$ according to (11)   \n3: Compute $\\begin{array}{r}{\\mathbf{x}_{t}^{k}=\\sum_{i}p_{t}^{k,i}\\mathbf{x}_{t}^{k,i}}\\end{array}$ and submit $\\begin{array}{r}{{\\bf x}_{t}=\\sum_{k}q_{t}^{k}{\\bf x}_{t}^{k}}\\end{array}$   \n4: Suffer the loss $\\bar{f_{t}}\\big(\\mathbf{\\bar{x}}_{t}\\big)+r\\big(\\mathbf{x}_{t}\\big)$ and observe th e gradient $\\mathbf{g}_{t}=\\nabla f_{t}(\\mathbf{x}_{t})$   \n5: Update $\\hat{\\mathbf{q}}_{t+1}$ and $\\hat{\\mathbf{p}}_{t+1}^{k}$ according to (12)   \n6: Send the gradient ${\\bf g}_{t}$ and the regularizer $r(\\cdot)$ to experts   \n7: end for ", "page_idx": 6}, {"type": "text", "text": "4.3 Exp-concave case ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We further investigate the exp-concave case where the regualizer $r(\\cdot)$ is non-smooth and convex, and the individual functions $f_{t}(\\cdot)$ are smooth and $\\alpha$ -exp-concave. ", "page_idx": 6}, {"type": "text", "text": "Remark. In this case, we require the exponential concavity on the individual function $f_{t}(\\cdot)$ instead of the expected one $F_{t}(\\cdot)$ . This is due to the technical demand in analysis, and similar assumptions are also adopted by Chen et al. [2023] and Yang et al. [2024c]. ", "page_idx": 6}, {"type": "text", "text": "In this case, we set OptCMD with the following configuration: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{R}_{t}(\\mathbf{x})=\\frac{1}{2}\\|\\mathbf{x}\\|_{H_{t}}^{2},\\ H_{t}=\\delta I+\\frac{\\beta G^{2}}{2}I+\\frac{\\beta}{2}\\sum_{s=1}^{t-1}h_{s}\\mathrm{,~and~}M_{t+1}=\\nabla f_{t}(\\mathbf{x}_{t}),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\delta\\ >\\ 0$ denotes the hyperparameter, $I$ denotes the $d$ -dimension identity matrix, $h_{t}\\ =$ $\\nabla f_{t}(\\mathbf{x}_{t})\\nabla f_{t}(\\mathbf{x}_{t})^{\\top}$ and $\\beta=(1/2)\\operatorname*{min}\\{1/(4G D),\\alpha\\}$ . The regret for this case is stated below. ", "page_idx": 6}, {"type": "text", "text": "Theorem 5. Under Assumptions 1, 2, 3, 4 and 8, Algorithm $^{\\,I}$ ensures ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\mathrm{Regret}_{T}\\right]=\\mathcal{O}\\left(\\frac{d}{\\alpha}\\log\\left(\\sigma_{1:T}^{2}+\\Sigma_{1:T}^{2}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "with the configuration in (10) where $\\delta=1$ . ", "page_idx": 6}, {"type": "text", "text": "Remark. For the fully adversarial setting, this bound degenerates to $O((d/\\alpha)\\log V_{T})$ , which is the first problem-dependent bound for exp-concave time-varying functions in online composite optimization, and tighter than $\\mathcal{O}((d/\\alpha)\\log T)$ by Yang et al. [2024c] in benign environments. For the fully stochastic setting, our result reduces to ${\\mathcal O}((\\bar{d}/\\alpha)\\log T)$ regret bound and further implies the same excess risk of ${\\mathcal{O}}(d\\log T/(\\alpha T))$ as that of Yang et al. [2024c] through the online-to-batch conversion. Furthermore, it also matches existing results for the SEA model [Chen et al., 2023]. ", "page_idx": 6}, {"type": "text", "text": "5 The universal strategy ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Although we have established favorable theoretical results in the composite SEA model, achieving these guarantees requires prior knowledge of the function type to set appropriate configurations, e.g., (6) for the general convex case. This requirement is often impractical in real-world scenarios and motivates us to design a universal strategy. In the previous study of Yan et al. [2023], based on the meta-expert framework [van Erven and Koolen, 2016, Wang et al., 2020, Zhang et al., 2022], they propose a multi-layer universal algorithm, which can adapt to the SEA model without regularizer by choosing optimistic OMD [Chen et al., 2023] as the expert-algorithm. Unfortunately, their method [Yan et al., 2023] requires the smoothness of the (summation) loss functions, which does not necessarily hold for (1), so that it cannot be directly applied to the composite SEA model. In this paper, we propose a novel universal algorithm for composite SEA. For the expert-algorithm, we choose OptCMD due to its ability in handling three cases in composite SEA. For the meta-algorithm, we design a new method that is able to effectively manage the non-smooth component $r(\\cdot)$ in (1) and can thus track experts according to their performances in the composite SEA model. ", "page_idx": 6}, {"type": "text", "text": "Our method is based on two key observations. Firstly, $r(\\cdot)$ in (1) is fixed over time and available to the meta-algorithm from the beginning round. Secondly, the meta-algorithm can access expert decisions for the current round before the performance estimation. Therefore, the information about $r(\\cdot)$ can be introduced into the expert tracking process, securing that the regret of meta-algorithm will not deviate from that of the best expert. Specifically, inspired by Yan et al. [2023], we employ a two-layer meta-algorithm, with each layer running an MsMwC algorithm [Chen et al., 2021] that maintains two weight sequences $\\{\\mathbf{q}_{t},\\hat{\\mathbf{q}_{t}}\\in\\Delta^{N}\\}_{t\\in[T]}$ , and a group of losses $l_{t}=(l_{t}^{1},\\cdot\\cdot\\cdot\\,,l_{t}^{N})$ and optimisms $m_{t}=(m_{t}^{1},\\cdot\\cdot\\cdot,m_{t}^{N})$ to measure the performance of $N$ experts. To handle the composite loss, we explicitly incorporate $r(\\cdot)$ into both $\\boldsymbol{l}_{t}$ and $\\mathbf{\\nabla}m_{t}$ . The incorporation serves two purposes: (i) it endows $\\boldsymbol{l}_{t}$ with a composite structure, facilitating the estimation of expert performance on composite losses; (ii) it utilizes the cancellation between $\\boldsymbol{l}_{t}$ and $\\mathbf{\\nabla}m_{t}$ to eliminate the non-smooth component in loss functions, i.e., $r(\\cdot)$ , so that the meta-algorithm is able to leverage the smoothness of $f_{t}\\bar{(\\cdot)}$ . In the following, we describe our universal algorithm, which is also summarized in Algorithm 2. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Meta-algorithm. Overall, our algorithm comprises two components: a two-layer meta-algorithm and a set of experts, together forming a three-layer structure. Specifically, on the top layer, we run one MsMwC named $\\mathcal{M}_{\\mathrm{top}}$ , which maintains the weights $\\mathbf{q}\\in\\Delta_{K}$ assigned to $K=\\mathcal{O}(\\log T)$ MsMwCs, named $\\mathcal{M}_{\\mathrm{mid}}$ . On the middle layer, each $\\mathcal{M}_{\\mathrm{mid}}^{k}\\left(k\\in[K]\\right)$ maintains the weights $\\mathbf{p}^{k}\\in\\Delta_{N}$ assigned to $N=\\mathcal{O}(\\log T)$ experts. At the round $t$ , we receive the decision $\\mathbf{x}_{t}^{k,i}$ from each expert $E^{k,i}$ , and update the weights $\\mathbf{q}_{t}$ and $\\mathbf{p}_{t}^{k}$ according to: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbf{q}_{t}=\\underset{\\mathbf{q}\\in\\Delta^{K}}{\\mathrm{argmin}}\\left\\{\\langle m_{t},\\mathbf{q}\\rangle+\\mathcal{B}^{\\psi_{1}}(\\mathbf{q},\\hat{\\mathbf{q}}_{t})\\right\\},\\ \\mathbf{p}_{t}^{k}=\\underset{\\mathbf{p}\\in\\Delta^{N}}{\\mathrm{argmin}}\\left\\{\\langle m_{t}^{k},\\mathbf{p}\\rangle+\\mathcal{B}^{\\psi_{2}}(\\mathbf{p},\\hat{\\mathbf{p}}_{t}^{k})\\right\\},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\begin{array}{r}{\\psi_{1}(\\mathbf{q})\\;=\\;\\sum_{k}(q^{k}\\ln q^{k})/\\eta^{k}}\\end{array}$ and $\\begin{array}{r}{\\psi_{2}(\\mathbf{p})\\;=\\;\\sum_{i}(p^{i}\\ln p^{i})/\\eta^{k,i}}\\end{array}$ denote the negative entropy functions, and ${\\boldsymbol{m}}_{t}^{k}$ and $\\mathbf{\\nabla}m_{t}$ denote the optimism u sed by $\\mathcal{M}_{\\mathrm{mid}}^{k}$ and $\\mathcal{M}_{\\mathrm{top}}$ , respectively. Then, we compute the weighted average decision $\\begin{array}{r}{\\mathbf{x}_{t}^{k}=\\sum_{i}p_{t}^{k,i}\\mathbf{x}_{t}^{k,i}}\\end{array}$ and submit $\\begin{array}{r}{{\\bf x}_{t}=\\sum_{k}q_{t}^{k}{\\bf x}_{t}^{k}}\\end{array}$ . Next, we suffer the loss and observe the gr adient $\\mathbf{g}_{t}=\\nabla f_{t}(\\mathbf{x}_{t})$ . We upda te $\\hat{\\mathbf{q}}_{t+1}$ and $\\hat{\\mathbf{p}}_{t+1}^{k}$ by $\\hat{\\mathbf{q}}_{t+1}=\\operatorname*{argmin}_{\\mathbf{q}\\in\\Delta^{K}}\\left\\{\\left\\langle l_{t}+a_{t},\\mathbf{q}\\right\\rangle+\\mathcal{B}^{\\psi_{1}}(\\mathbf{q},\\hat{\\mathbf{q}}_{t})\\right\\},\\hat{\\mathbf{p}}_{t+1}^{k}=\\operatorname*{argmin}_{\\mathbf{p}\\in\\Delta^{N}}\\{\\langle l_{t}^{k}+\\mathbf{b}_{t}^{k},\\mathbf{p}\\rangle+\\mathcal{B}^{\\psi_{2}}(\\mathbf{p},\\hat{\\mathbf{p}}_{t}^{k})\\},$ (12) where $l_{t}=(l_{t}^{1},\\cdot\\cdot\\cdot\\,,l_{t}^{k})$ and $l_{t}^{k}=(l_{t}^{k,1},\\cdot\\cdot\\cdot\\,,l_{t}^{k,N})$ denote losses used by $\\mathcal{M}_{\\mathrm{top}}$ and $\\mathcal{M}_{\\mathrm{mid}}^{k}$ , respectively, and $\\mathbf{a}_{t}\\in\\mathbb{R}^{K}$ and $\\mathbf{b}_{t}^{k}\\in\\mathbb{R}^{N}$ denote the correction terms. At last, we send the gradient ${\\bf g}_{t}$ and the regularizer $r(\\cdot)$ to each expert. Now, we specify the loss and optimism in (11) and (12). For $\\mathcal{M}_{\\mathrm{top}}$ , we choose ", "page_idx": 7}, {"type": "equation", "text": "$$\nl_{t}^{k}=\\langle\\mathbf{g}_{t},\\mathbf{x}_{t}^{k}\\rangle+r(\\mathbf{x}_{t}^{k})+\\gamma_{1}\\|\\mathbf{x}_{t}^{k}-\\mathbf{x}_{t-1}^{k}\\|_{2}^{2},\\,m_{t}^{k}=\\langle\\hat{\\mathbf{m}}_{t}^{k},p_{t}^{k}\\rangle+r(\\mathbf{x}_{t}^{k})+\\gamma_{1}\\|\\mathbf{x}_{t}^{k}-\\mathbf{x}_{t-1}^{k}\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "equation", "text": "$$\nl_{t}^{k,i}=\\langle\\mathbf{g}_{t},\\mathbf{x}_{t}^{k,i}\\rangle+r(\\mathbf{x}_{t}^{k,i})+\\gamma_{2}\\|\\mathbf{x}_{t}^{k,i}-\\mathbf{x}_{t-1}^{k,i}\\|_{2}^{2},\\ m_{t}^{k,i}=\\hat{m}_{t}^{k,i}+r(\\mathbf{x}_{t}^{k,i})+\\gamma_{2}\\|\\mathbf{x}_{t}^{k,i}-\\mathbf{x}_{t-1}^{k,i}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "In the above, $\\gamma_{1},\\gamma_{2}$ denote hyperparameters, and $\\gamma_{1}\\lVert\\mathbf{x}_{t}^{k}-\\mathbf{x}_{t-1}^{k}\\rVert_{2}^{2}$ and $\\gamma_{2}\\|\\mathbf{x}_{t}^{k,i}-\\mathbf{x}_{t-1}^{k,i}\\|_{2}^{2}$ are injected for cancellation during the analysis [Yan et al., 2023]. It should be noticed that we explicitly introduce $r(\\cdot)$ in (13) and (14). This is because experts are running over the composite losses and their performance assessment should similarly reflect the composite structure, ensuring an accurate measure for expert tracking. Moreover, the optimism $\\mathbf{\\nabla}m_{t}$ with $r(\\cdot)$ helps eliminate the non-smooth component in $\\boldsymbol{l}_{t}$ and thereby enables the meta-algorithm to effectively utilize the smoothness of time-varying functions. To make it clearer, we take the general convex case as an example. ", "page_idx": 7}, {"type": "text", "text": "First, the expected regret can be decomposed into the meta-regret and the expert-regret, as below: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\mathrm{Regret}_{T}\\right]=\\mathbb{E}\\left[\\sum_{t=1}^{T}\\left[f_{t}(\\mathbf{x}_{t})+r(\\mathbf{x}_{t})\\right]-\\underset{\\mathbf{x}\\in\\mathcal{X}}{\\operatorname*{min}}\\sum_{t=1}^{T}\\left[f_{t}(\\mathbf{x})+r(\\mathbf{x})\\right]\\right]}\\\\ &{\\leq\\underbrace{\\mathbb{E}\\left[\\sum_{t=1}^{T}\\langle\\mathbf{g}_{t},\\mathbf{x}_{t}-\\mathbf{x}_{t}^{k^{\\ast},i^{\\ast}}\\rangle+r(\\mathbf{x}_{t})-r(\\mathbf{x}_{t}^{k^{\\ast},i^{\\ast}})\\right]}_{:=\\mathrm{mea-reret}}+\\underbrace{\\mathbb{E}\\left[\\sum_{t=1}^{T}\\langle\\mathbf{g}_{t},\\mathbf{x}_{t}^{k^{\\ast},i^{\\ast}}-\\mathbf{x}^{\\ast}\\rangle+r(\\mathbf{x}_{t}^{k^{\\ast},i^{\\ast}})-r(\\mathbf{x}^{\\ast})\\right]}_{:=\\mathrm{expert-regret}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where x $\\mathbf{x}_{t}^{k^{*},i^{*}}$ denotes the decision made by the best expert $E^{k^{*},i^{*}}$ . For the expert-regret, we can directly apply the result in Theorem 1. Therefore, the key is how to bound the meta-regret. By the fact that $\\begin{array}{r}{r(\\mathbf{x}_{t})\\le\\sum_{k}q_{t}^{k}r(\\mathbf{x}_{t}^{k})}\\end{array}$ and $\\begin{array}{r}{r(\\mathbf{x}_{t}^{k})\\le\\sum_{i}p_{t}^{k,i}r(\\mathbf{x}_{t}^{k,i})}\\end{array}$ , the meta-regret is bounded by ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\leq\\sum_{t=1}^{T}\\left[\\langle q_{t},l_{t}\\rangle-{l_{t}^{k}}^{*}\\right]+\\sum_{t=1}^{T}\\left[\\left\\langle p_{t}^{k^{*}},{l_{t}^{k^{*}}}\\right\\rangle-{l_{t}^{k^{*},i^{*}}}\\right]+\\mathcal{O}(1),}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where the first and second terms denote the regrets for tracking experts of $\\mathcal{M}_{\\mathrm{top}}$ and $\\mathcal{M}_{\\mathrm{mid}}^{k^{*}}$ , respectively. While these two terms involve the losses $l_{t}^{k}$ and $l_{t}^{k^{*},i}$ that incorporate the regularizer, they can be simultaneously controlled by a favorable bound of O( t\u2208[T ](l $\\begin{array}{r}{\\mathcal{O}(\\sum_{t\\in[T]}(\\boldsymbol{l}_{t}^{k^{*},i^{*}}-\\boldsymbol{m}_{t}^{k^{*},i^{*}})^{2})}\\end{array}$ , which not only eliminates $r(\\cdot)$ by the cancellation between $l_{t}^{k^{*},i^{*}}$ and mtk\u2217,i\u2217, but also suffices to achieve the desirable bound of $\\tilde{\\mathcal{O}}(\\sqrt{\\sigma_{1:T}^{2}}+\\sqrt{\\Sigma_{1:T}^{2}})$ for general convex case and $\\mathcal{O}(1)$ for other two cases. More details can be found in Appendix B.6. ", "page_idx": 8}, {"type": "text", "text": "Experts. Each expert is an instance of OptCMD that equips with certain configuration, i.e., (6), (8) or (10). To estimate the unknown curvature $\\lambda$ and $\\alpha$ , we utilize the discretization strategy by Zhang et al. [2022], constructing the candidate sets as ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathcal{P}_{s t r}=\\mathcal{P}_{e x p}=\\{1/T,2/T,2^{2}/T,\\cdot\\cdot\\cdot,2^{N}/T\\},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $N=\\lceil\\log_{2}T\\rceil$ . Based on the two sets, we create following three types of experts: ", "page_idx": 8}, {"type": "text", "text": "\u2022 Strongly convex experts, each of which is configured with (8) and a candidate $\\lambda^{i}\\in\\mathcal{P}_{s t r}$ ;   \n\u2022 Exp-concave experts, each of which is configured with (10) and a candidate $\\alpha^{i}\\in\\mathcal{P}_{e x p}$ ;   \n\u2022 General convex expert, which is configured with (6). ", "page_idx": 8}, {"type": "text", "text": "As demonstrated by Zhang et al. [2022], these experts are sufficient to identify the best one that runs for the true function type with the most accurate curvature. For instance, in the strongly convex case, there must exist an expert $E^{i}$ equipped with $\\lambda^{i}\\in\\mathcal{P}_{s t r}$ that satisfies $\\lambda^{i}\\le\\lambda\\le2\\lambda^{i}$ . Moreover, instead of directly minimizing the original loss function (1), each expert runs over a new composite surrogate loss to avoid high-computational gradient query costs. Specifically, we choose $h_{t}^{c}(\\mathbf{x})\\bar{=}\\,\\langle\\mathbf{g}_{t},\\mathbf{x}\\rangle\\!+\\!\\bar{r}\\big(\\mathbf{x})$ , $h_{t,i}^{s c}(\\mathbf{x})=\\langle\\mathbf{g}_{t},\\mathbf{x}\\rangle+r(\\mathbf{x})+\\lambda^{i}\\|\\mathbf{x}-\\mathbf{x}_{t}\\|_{2}^{2}/2$ and $h_{t,i}^{e x p}(\\mathbf{x})=\\left\\langle\\mathbf{g}_{t},\\mathbf{x}\\right\\rangle+r(\\mathbf{x})+\\beta^{i}\\left\\langle\\mathbf{g}_{t},\\mathbf{x}-\\mathbf{x}_{t}\\right\\rangle^{2}/2$ for general convex, strongly convex, and exp-concave experts, respectively. These surrogate losses not only inherit the composite structure and properties of $f_{t}(\\mathbf{x})$ , e.g., the strongly convexity, but they also require only one gradient query for $\\mathbf{g}_{t}=\\nabla f_{t}(\\mathbf{x}_{t})$ in each round. The theoretical guarantees of our universal algorithm are presented below. ", "page_idx": 8}, {"type": "text", "text": "Theorem 6. Under Assumptions 1, 2, 3, 4 and 7, with proper hyperparameters, Algorithm 2 ensures the bounds of $\\tilde{\\mathcal{O}}(\\sqrt{\\sigma_{1:T}^{2}}+\\bar{\\sqrt{\\Sigma_{1:T}^{2}}})$ , $\\mathcal{O}((\\sigma_{\\operatorname*{max}}^{2}\\!+\\!\\Sigma_{\\operatorname*{max}}^{2})\\log(\\sigma_{1:T}^{2}\\!+\\!\\Sigma_{1:T}^{2}))$ , and $\\mathcal{O}(d\\log(\\sigma_{1:T}^{2}{+\\Sigma_{1:T}^{2}}))$ for general convex, strongly convex, and exp-concave time-varying functions, respectively. ", "page_idx": 8}, {"type": "text", "text": "Remark. Theorem 6 indicates that Algorithm 2 can attain comparable regret bounds to those of OptCMD for the three cases, without knowing the function type and curvature in advance. ", "page_idx": 8}, {"type": "text", "text": "6 Implications ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In the following, we discuss implications of our results for two common intermediate examples. ", "page_idx": 8}, {"type": "text", "text": "Composite adversarially corrupted stochastic data. We first focus on the composite adversarially corrupted stochastic model, of which a special case without regularizer has been widely investigated in learning with expert advice [Amir et al., 2020], bandits [Zimmert and Seldin, 2021, Ito, 2021], and online optimization [Sachs et al., 2022, Chen et al., 2023]. Specifically, each loss function consists of three terms: $\\phi_{t}(\\cdot)=h_{t}(\\cdot)+c_{t}(\\cdot)+r(\\cdot)$ , where $h_{t}(\\cdot)$ denotes the loss of i.i.d. data from a fixed distribution $\\mathcal{D}$ with the variance $\\sigma$ , and $c_{t}(\\cdot)$ denotes an adversarial perturbation measured by a parameter $C_{T}>0$ , satisfying $\\begin{array}{r}{\\sum_{t\\in[T]}\\operatorname*{max}_{\\mathbf{x}\\in\\mathcal{X}}\\|\\nabla c_{t}(\\mathbf{x})\\|_{2}\\leq C_{T}}\\end{array}$ , and $\\bar{r}(\\cdot)$ denotes the regularizer. In this model, the stochasticity and adversariality come from $h_{t}(\\cdot)$ and $c_{t}(\\cdot)$ , respectively. Therefore, $\\sigma_{1:T}^{2}$ and $\\Sigma_{1:T}^{2}$ can be formulated as ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\sigma_{1:T}^{2}=\\sigma^{2}T,\\ \\Sigma_{1:T}^{2}=\\mathbb{E}\\left[\\sum_{t=1}^{T}\\operatorname*{sup}_{\\mathbf{x}\\in\\mathcal{X}}\\left\\Vert\\nabla c_{t}(\\mathbf{x})-\\nabla c_{t-1}(\\mathbf{x})\\right\\Vert_{2}^{2}\\right]\\leq4G C_{T}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "With the above specification, our theoretical results can naturally be extended to this scenarios, delivering the following corollary. ", "page_idx": 8}, {"type": "text", "text": "Corollary 1. With the specification (\u221a16) in the scenarios of composite adversarially corrupted stochastic data, we can obtain an $\\mathcal{O}(\\sigma\\sqrt{T}+\\sqrt{C_{T}})$ bound for the general convex case by Theorem $^{\\,l}$ , an ${\\mathcal{O}}(\\log(\\sigma^{2}T\\!+\\!C_{T}))$ ) bound for the strongly convex case by Theorem $3$ , and an $\\mathcal{O}(d\\log(\\sigma^{2}T\\!+\\!C_{T}))$ bound for the exp-concave case by Theorem 5. ", "page_idx": 8}, {"type": "text", "text": "Adversarial online learning with limited resources. We then consider the another common scenarios where loss functions arrive in batches and the available computing resources are insufficient to process them all [Bottou and Cun, 2003, Bencz\u00far et al., 2018, Zhou, 2024]. Specifically, in each round $t$ , we receive a group of functions $\\mathcal{F}_{t}=\\{f_{t}(\\cdot,i)\\}_{i\\in[K_{t}]}$ with the size $K_{t}$ , each of which is selected by the environments adversarially. We denote by $\\begin{array}{r}{\\dot{F_{t}(\\cdot)}=K_{t}^{-1}\\sum_{i\\in[K_{t}]}f_{t}(\\cdot,i)}\\end{array}$ the average function. Due to limited computing resources, we can only sample a subset of functions for gradient estimation, and generate sparse decisions for efficient storage and inference. For this reason, our goal is to minimize $\\phi_{t}(\\cdot)=h_{t}(\\cdot)+r(\\cdot)$ , where $\\begin{array}{r}{h_{t}(\\cdot)=B_{t}^{-1}\\sum_{i\\in[B_{t}]}\\hat{f}_{t}(\\cdot,i)}\\end{array}$ denotes the approximation of $F_{t}(\\cdot)$ by i.i.d. sampling $B_{t}\\in[K_{t}]$ functions $\\hat{f}_{t}(\\cdot,i)$ from the group $\\mathcal{F}_{t}$ , and $r(\\cdot)=\\|\\cdot\\|_{1}$ denotes the $\\ell_{1}$ -norm regularizer. In this scenarios, it can be verified that ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sigma_{1:T}^{2}=\\sum_{t=1}^{T}\\underset{\\mathbf{x}\\in\\mathcal{X}}{\\operatorname*{sup}}\\mathbb{E}_{\\hat{f}_{t}\\sim\\mathcal{F}_{t}}\\left[\\left\\Vert B_{t}^{-1}\\sum_{i=1}^{B_{t}}\\nabla\\hat{f}_{t}(\\cdot,i)-\\nabla F_{t}(\\mathbf{x})\\right\\Vert_{2}^{2}\\right]\\le4G^{2}\\sum_{t=1}^{T}B_{t}^{-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "and $\\begin{array}{r}{\\Sigma_{1:T}^{2}=\\mathbb{E}[\\sum_{t=1}^{T}\\operatorname*{sup}_{\\mathbf{x}\\in\\mathcal{X}}\\|\\nabla F_{t}(\\mathbf{x})-\\nabla F_{t-1}(\\mathbf{x})\\|_{2}^{2}]}\\end{array}$ . Applying the above specification into our theorems delivers the following corollary. ", "page_idx": 9}, {"type": "text", "text": "Corollary 2. With the specification (17) in the scenarios of composite adversarially corrupted stochastic data, we can obtain an $\\mathcal{O}(\\sqrt{\\textstyle\\sum_{t\\in[T]}B_{t}^{-1}}+\\sqrt{\\textstyle\\sum_{1:T}^{2}})$ bound for the general convex case by Theorem $^{\\,l}$ , an $\\mathcal{O}(\\log(\\sum_{t\\in[T]}B_{t}^{-1}+\\Sigma_{1:T}^{2}))$ bound for the strongly convex case by Theorem 3, and an $\\begin{array}{r}{\\mathcal{O}(d\\log(\\sum_{t\\in[T]}B_{t}^{-1}+\\Sigma_{1:T}^{2}))}\\end{array}$ ) bound for the exp-concave case by Theorem 5. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion and future work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we investigate the intermediate setting between stochastic and adversarial composite optimization, named composite SEA, and demonstrate that OptCMD is able to attain the regret bounds of $\\mathcal{O}(\\sqrt{\\sigma_{1:T}^{2}}+\\sqrt{\\Sigma_{1:T}^{2}})$ , $\\mathcal{O}((\\sigma_{\\mathrm{max}}^{2}+\\Sigma_{\\mathrm{max}}^{2})\\log(\\sigma_{1:T}^{2}+\\Sigma_{1:T}^{2}))$ and $\\mathcal{O}(d\\log(\\sigma_{1:T}^{2}+\\Sigma_{1:T}^{2}))$ for the general convex, strongly convex and exp-concave cases, respectively. To deal with the unknown function type in real-world problems, we further propose a novel universal algorithm in online composite optimization, and show that our universal algorithm is able to achieve the desirable bounds in the three cases, simultaneously. Due to the versatility of $\\sigma_{1:T}^{2}$ and $\\Sigma_{1:T}^{2}$ , all our theoretical findings can recover previous results in fully stochastic and adversarial composite optimization. Finally, we explore several practical intermediate scenarios to demonstrate the implications of our results. Additionally, we also conduct empirical studies in Appendix A to verify our theoretical results. ", "page_idx": 9}, {"type": "text", "text": "There are many valuable directions for future research. First, our methods still rely on the domain and gradient bounded assumptions. We notice that there have been several methods designed for unbounded cases [Orabona, 2014, Orabona and P\u00e1l, 2016, Cutkosky and Orabona, 2018, Jacobsen and Cutkosky, 2022], but all of them focus on the setting without the reguralizer. Consequently, developing online algorithms with the unbounded domain and gradient for composite SEA remains an interesting research direction. Second, in the composite SEA model, we implicitly assume that the feedback (i.e., the loss function value and the gradient) is immediately revealed after making the decision, which, however, is not necessarily satisfied in practice [Joulani et al., 2013, Quanrud and Khashabi, 2015, Wan et al., 2022a,c, 2024b]. Therefore, it is also interesting to explore the composite SEA model with the delayed feedback. Thirdly, our proposed universal algorithm currently exhibits a three-layer structure, which presents several challenges in both analysis and practical implementation. Therefore, designing a simpler two-layer universal algorithm in composite SEA is another potential research direction in the future, which may require the advanced techniques in Yan et al. [2024]. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The authors would thank Yu-Hu Yan for helpful discussions, and the anonymous reviewers for their constructive suggestions. This work was partially supported by National Science and Technology Major Project (2022ZD0114801), and NSFC (U23A20382, 62306275). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "I. Amir, I. Attias, T. Koren, R. Livni, and Y. Mansour. Prediction with corrupted expert advice. In Advances in Neural Information Processing Systems 33, pages 14315\u201314325, 2020.   \nA. Beck and M. Teboulle. Mirror descent and nonlinear projected subgradient methods for convex optimization. Operations Research Letters, 31(3):167\u2013175, may 2003.   \nA. A. Bencz\u00far, L. Kocsis, and R. P\u00e1lovics. Online machine learning in big data streams. ArXiv e-prints, arXiv:1802.05872, 2018.   \nL. Bottou and Y. L. Cun. Large scale online learning. In Advances in Neural Information Processing Systems 16, pages 217\u2013224, 2003.   \nN. Cesa-Bianchi, A. Conconi, and C. Gentile. On the generalization ability of on-line learning algorithms. IEEE Transactions on Information Theory, 50(9):2050\u20132057, 2004.   \nC.-C. Chang and C.-J. Lin. Libsvm: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2(27):1\u201327, 2011.   \nL. Chen, H. Luo, and C.-Y. Wei. Impossible tuning made possible: A new expert algorithm and its applications. In Proceedings of the 34th Conference on Learning Theory, pages 1216\u20131259, 2021.   \nS. Chen, W.-W. Tu, P. Zhao, and L. Zhang. Optimistic online mirror descent for bridging stochastic and adversarial online convex optimization. In Proceedings of the 40th International Conference on Machine Learning, pages 5002\u20135035, 2023.   \nS. Chen, Y.-J. Zhang, W.-W. Tu, P. Zhao, and L. Zhang. Optimistic online mirror descent for bridging stochastic and adversarial online convex optimization. Journal of Machine Learning Research, 25 (178):1\u201362, 2024.   \nC.-K. Chiang, T. Yang, C.-J. Lee, M. Mahdavi, C.-J. Lu, R. Jin, and S. Zhu. Online optimization with gradual variations. In Proceedings of the 25th Conference on Learning Theory, pages 6.1\u20136.20, 2012.   \nA. Cutkosky. Parameter-free, dynamic, and strongly-adaptive online learning. In Proceedings of the 37th International Conference on Machine Learning, pages 2250\u20132259, 2020.   \nA. Cutkosky and F. Orabona. Black-box reductions for parameter-free online learning in banach spaces. In Proceedings of the 31st Conference on Learning Theory, pages 1493\u20131529, 2018.   \nA. Daniely, A. Gonen, and S. Shalev-Shwartz. Strongly adaptive online learning. In Proceedings of the 32nd International Conference on Machine Learning, pages 1405\u20131411, 2015.   \nJ. Duchi and Y. Singer. Efficient online and batch learning using forward backward splitting. Journal of Machine Learning Research, 10:2899\u20132934, 2009.   \nJ. C. Duchi, S. Shalev-Shwartz, Y. Singer, and A. Tewari. Composite objective mirror descent. In Proceedings of the 23rd Annual Conference on Learning Theory, pages 14\u201326, 2010.   \nN. Flammarion and F. Bach. Stochastic composite least-squares regression with convergence rate $o(1/n)$ . In Proceedings of the 30th Conference on Learning Theory, pages 831\u2013875, 2017.   \nD. Garber and A. Kaplan. Fast stochastic algorithms for low-rank and nonsmooth matrix problems. In Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics, pages 286\u2013294, 2019.   \nD. Garber, G. Korcia, and K. Levy. Online convex optimization in the random order model. In Proceedings of the 37th International Conference on Machine Learning, pages 3387\u20133396, 2020.   \nS. Ghadimi and G. Lan. Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization I: A generic algorithmic framework. SIAM Journal on Optimization, 22 (4):1469\u20131492, 2012.   \nE. Hazan, A. Agarwal, and S. Kale. Logarithmic regret algorithms for online convex optimization. Foundations and Trends in Machine Learning, 69(2):169\u2013192, 2007.   \nE. Hazan, T. Koren, and K. Y. Levy. Logistic regression: Tight bounds for stochastic and online optimization. In Proceedings of the 27th Conference on Learning Theory, pages 197\u2013209, 2014.   \nS. Ito. On optimal robustness to adversarial corruption in online decision problems. In Advances in Neural Information Processing Systems 34, pages 7409\u20137420, 2021.   \nA. Jacobsen and A. Cutkosky. Parameter-free mirror descent. In Proceedings of the 35th Conference on Learning Theory, pages 4160\u20134211, 2022.   \nP. Joulani, A. Gy\u00f6rgy, and C. Szepesv\u00e1ri. Online learning under delayed feedback. In Proceedings of the 30th International Conference on Machine Learning, pages 1453\u20131461, 2013.   \nP. Joulani, A. Gy\u00f6rgy, and C. Szepesv\u00e1ri. A modular analysis of adaptive (non-)convex optimization: Optimism, composite objectives, variance reduction, and variational bounds. Theoretical Computer Science, 808:108\u2013138, 2020.   \nK.-S. Jun, F. Orabona, R. Willett, and S. Wright. Improved strongly adaptive online learning using coin betting. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, pages 943\u2013951, 2017a.   \nK.-S. Jun, F. Orabona, S. Wright, and R. Willett. Online learning for changing environments using coin betting. Electronic Journal of Statistics, 11(2):5282\u20135310, 2017b.   \nA. Kulunchakov and J. Mairal. A generic acceleration framework for stochastic composite optimization. In Advances in Neural Information Processing Systems 32, pages 12556\u201312567, 2019.   \nG. Lan. An optimal method for stochastic composite optimization. Mathematical Programming, 133: 365\u2013397, 2012.   \nG. Lan. Gradient sliding for composite optimization. Mathematical Programming, 159:201\u2013235, 2016.   \nJ. Langford, L. Li, and T. Zhang. Sparse online learning via truncated gradient. Journal of Machine Learning Research, 10:777\u2013801, 2009.   \nY. Lei and K. Tang. Stochastic composite mirror descent: Optimal bounds with high probabilities. In Advances in Neural Information Processing Systems 31, pages 1519\u20131529, 2018.   \nY. Lei and D.-X. Zhou. Analysis of online composite mirror descent algorithm. Neural Computation, 29(3):825\u2013860, 2017.   \nY. Lei, P. Yang, K. Tang, and D.-X. Zhou. Optimal stochastic and online learning with individual iterates. In Advances in Neural Information Processing Systems 32, pages 5415\u20135425, 2019.   \nZ. Mhammedi, W. M. Koolen, and T. van Erven. Lipschitz adaptivity with multiple learning rates in online learning. In Proceedings of the 32nd Conference on Learning Theory, pages 2490\u20132511, 2019.   \nM. Mohri and S. Yang. Accelerating online convex optimization via adaptive prediction. In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, pages 848\u2013856, 2016.   \nY. Nesterov. Primal-dual subgradient methods for convex problems. Mathematical Programming, 120(1):261\u2013283, 2009.   \nF. Orabona. Simultaneous model selection and optimization through parameter-free stochastic learning. In Advances in Neural Information Processing Systems 27, pages 1116\u20131124, 2014.   \nF. Orabona and D. P\u00e1l. Coin betting and parameter-free online learning. In Advances in Neural Information Processing Systems 29, pages 577\u2013585, 2016.   \nR. Pogodin and T. Lattimore. On first-order bounds, variance and gap-dependent bounds for adversarial bandits. In Proceedings of the 35th Conference on Uncertainty in Artificial Intelligence, pages 894\u2013904, 2019.   \nK. Quanrud and D. Khashabi. Online learning with adversarial delays. In Advances in Neural Information Processing Systems 28, pages 1270\u20131278, 2015.   \nA. Rakhlin and K. Sridharan. Online learning with predictable sequences. In Proceedings of the 26th Conference on Learning Theory, pages 993\u20131019, 2013.   \nS. Sachs, H. Hadiji, T. van Erven, and C. Guzm\u00e1n. Between stochastic and adversarial online convex optimization: Improved regret bounds via smoothness. In Advances in Neural Information Processing Systems 35, pages 691\u2013702, 2022.   \nP. Z. Scroccaro, A. S. Kolarijani, and P. M. Esfahani. Adaptive composite online optimization: Predictions in static and dynamic environments. IEEE Transactions on Automatic Control, 68(5): 2906\u20132921, 2023.   \nU. Sherman, T. Koren, and Y. Mansour. Optimal rates for random order online optimization. In Advances in Neural Information Processing Systems 34, pages 2097\u20132108, 2021.   \nT. van Erven and W. M. Koolen. Metagrad: Multiple learning rates in online learning. In Advances in Neural Information Processing Systems 29, pages 3666\u20133674, 2016.   \nT. van Erven, W. M. Koolen, and D. van der Hoeven. Metagrad: adaptation using multiple learning rates in online learning. Journal of Machine Learning Research, 22(161):1\u201361, 2021.   \nY. Wan, B. Xue, and L. Zhang. Projection-free online learning in dynamic environments. In Proceedings of the 35th AAAI Conference on Artificial Intelligence Advances, pages 10067\u201310075, 2021.   \nY. Wan, W.-W. Tu, and L. Zhang. Online frank-wolfe with arbitrary delays. In Advances in Neural Information Processing Systems 35, pages 19703\u201319715, 2022a.   \nY. Wan, W.-W. Tu, and L. Zhang. Strongly adaptive online learning over partial intervals. Science China Information Sciences, 65(10):202101, 2022b.   \nY. Wan, Y. Wang, C. Yao, W.-W. Tu, and L. Zhang. Projection-free online learning with arbitrary delays. ArXiv e-prints, arXiv:2204.04964, 2022c.   \nY. Wan, C. Yao, M. Song, and L. Zhang. Non-stationary online convex optimization with arbitrary delays. In Proceedings of the 41st International Conference on Machine Learning, pages 49991\u2013 50011, 2024a.   \nY. Wan, C. Yao, M. Song, and L. Zhang. Improved regret for bandit convex optimization with delayed feedback. In Advances in Neural Information Processing Systems 37, 2024b.   \nG. Wang, S. Lu, and L. Zhang. Adaptivity and optimality: A universal algorithm for online convex optimization. In Proceedings of the 35th Conference on Uncertainty in Artificial Intelligence, pages 659\u2013668, 2019.   \nG. Wang, S. Lu, Y. Hu, and L. Zhang. Adapting to smoothness: A more universal algorithm for online convex optimization. In Proceedings of the 34th AAAI Conference on Artificial Intelligence, pages 6162\u20136169, 2020.   \nY. Wang, W. Yang, W. Jiang, S. Lu, B. Wang, H. Tang, Y. Wan, and L. Zhang. Non-stationary projection-free online learning with dynamic and adaptive regret guarantees. In Proceedings of the 38th AAAI Conference on Artificial Intelligence, pages 15671\u201315679, 2024.   \nL. Xiao. Dual averaging method for regularized stochastic learning and online optimization. In Advances in Neural Information Processing Systems 22, pages 2116\u20132124, 2009.   \nY.-H. Yan, P. Zhao, and Z.-H. Zhou. Universal online learning with gradual variations: A multi-layer online ensemble approach. In Advances in Neural Information Processing Systems 36, pages 37682\u201337715, 2023.   \nY.-H. Yan, P. Zhao, and Z.-H. Zhou. A simple and optimal approach for universal online learning with gradient variations. In Advances in Neural Information Processing Systems 37, 2024.   \nW. Yang, W. Jiang, Y. Wang, P. Yang, Y. Hu, and L. Zhang. Small-loss adaptive regret for online convex optimization. In Proceedings of the 41st International Conference on Machine Learning, pages 56156\u201356195, 2024a.   \nW. Yang, Y. Wang, P. Zhao, and L. Zhang. Universal online convex optimization with 1 projection per round. In Advances in Neural Information Processing Systems 37, 2024b.   \nX. Yang, P. Tian, X. Cheng, Y. Wan, and M. Song. Regularized online exponentially concave optimization. Neurocomputing, 595:127789, 2024c.   \nL. Zhang, S. Lu, and Z.-H. Zhou. Adaptive online learning in dynamic environments. In Advances in Neural Information Processing Systems 31, pages 1323\u20131333, 2018.   \nL. Zhang, T. Yang, R. Jin, and Z.-H. Zhou. Relative error bound analysis for nuclear norm regularized matrix completion. Journal of Machine Learning Research, 20(97):1\u201322, 2019.   \nL. Zhang, S. Lu, and T. Yang. Minimizing dynamic regret and adaptive regret simultaneously. In Proceedings of the 23rd International Conference on Artificial Intelligence and Statistics, pages 309\u2013319, 2020.   \nL. Zhang, G. Wang, W.-W. Tu, W. Jiang, and Z.-H. Zhou. Dual adaptivity: A universal algorithm for minimizing the adaptive regret of convex functions. In Advances in Neural Information Processing Systems 34, pages 24968\u201324980, 2021.   \nL. Zhang, G. Wang, J. Yi, and T. Yang. A simple yet universal strategy for online convex optimization. In Proceedings of the 39th International Conference on Machine Learning, pages 26605\u201326623, 2022.   \nW. Zhang, L. Zhang, Z. Jin, R. Jin, D. Cai, X. Li, R. Liang, and X. He. Sparse learning with stochastic composite optimization. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(6): 1223\u20131236, 2017.   \nZ.-H. Zhou. Learnability with time-sharing computational resource concerns. National Science Review, 11(10):nwae204, 2024.   \nJ. Zimmert and Y. Seldin. Tsallis-INF: An optimal algorithm for stochastic and adversarial bandits. Journal of Machine Learning Research, 22(28):1\u201349, 2021.   \nM. Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Proceedings of the 20th International Conference on Machine Learning, pages 928\u2013936, 2003. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Experiments ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we conduct empirical studies to validate our theoretical results. ", "page_idx": 14}, {"type": "text", "text": "Setup. In this paper, we show that OptCMD with suitable configurations is able to achieve a series of favorable theoretical guarantees for the composite SEA model. Moreover, for the practical scenarios where the prior knowledge of loss functions is unavailable, we propose a novel universal strategy, called USC-SEA, which can still achieve the desired regret bounds for three cases in the composite SEA simultaneously. To verify our theoretical findings, we conduct experiments on the mushroom datasets from the LIBSVM repository [Chang and Lin, 2011], and consider the following online classification problem. Let $T$ denote the number of the total rounds. At each round $t\\,\\in\\,[T]$ , the learner receives a sampled data $(\\mathbf{x}_{t},y_{t})\\in\\mathbb{R}^{d}\\times\\{-1,1\\}$ with $d=112$ . Then, the learner plays the decision ${\\bf w}_{t}$ from the ball $\\mathcal{X}$ with the diameter $D=20$ , and suffers a composite loss ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\phi_{t}(\\mathbf{w}_{t};\\mathbf{x}_{t},y_{t})=f_{t}(\\mathbf{w}_{t};\\mathbf{x}_{t},y_{t})+\\lambda r(\\mathbf{w}_{t}),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where we set the hyper-parameter $\\lambda=0.001$ . The dataset used in the experiments is considered to be sampled from an unknown distribution, possessing the inherent stochastic property. To simulate the stochastically extended adversarial environments, we perturb the dataset by randomly flipping the labels of $10\\%$ data as the adversarial corruptions. ", "page_idx": 14}, {"type": "text", "text": "We consider the following three types of loss functions. ", "page_idx": 14}, {"type": "text", "text": "\u2022 For the general convex case, we choose the smooth and convex cross-entropy function as the time-varying function: ", "page_idx": 14}, {"type": "equation", "text": "$$\nf_{t}(\\mathbf{w}_{t};\\mathbf{x}_{t},y_{t})=-y_{t}\\log\\sigma\\left(\\mathbf{x}_{t}^{\\top}\\mathbf{w}_{t}\\right)-(1-y_{t})\\log\\left(1-\\sigma\\left(\\mathbf{x}_{t}^{\\top}\\mathbf{w}_{t}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\sigma(\\cdot)$ denotes the sigmoid function, and utilize the $\\ell_{1}$ -norm regularizer $r(\\mathbf{w}_{t})\\,=\\,\\|\\mathbf{w}_{t}\\|_{1}$ . Therefore, the composite function takes the form of: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\phi(\\mathbf{w}_{t};\\mathbf{x}_{t},y_{t})=-y_{t}\\log\\sigma\\left(\\mathbf{x}_{t}^{\\top}\\mathbf{w}_{t}\\right)-\\left(1-y_{t}\\right)\\log\\left(1-\\sigma\\left(\\mathbf{x}_{t}^{\\top}\\mathbf{w}_{t}\\right)\\right)+\\lambda\\|\\mathbf{w}_{t}\\|_{1}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "\u2022 For the strongly convex case, we employ the cross-entropy functions with the $\\ell_{2}$ -norm regularizer as the time-varying function: ", "page_idx": 14}, {"type": "equation", "text": "$$\nf_{t}(\\mathbf{w}_{t};\\mathbf{x}_{t},y_{t})=-y_{t}\\log\\sigma\\left(\\mathbf{x}_{t}^{\\top}\\mathbf{w}_{t}\\right)-\\left(1-y_{t}\\right)\\log\\left(1-\\sigma\\left(\\mathbf{x}_{t}^{\\top}\\mathbf{w}_{t}\\right)\\right)+\\delta\\|\\mathbf{w}_{t}\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which is $2\\delta$ -strongly convex, and still leverage the $\\ell_{1}$ -norm regularizer. Hence, the composite loss function is in the form of: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\phi(\\mathbf{w}_{t};\\mathbf{x}_{t},y_{t})=-y_{i}\\log\\sigma\\left(\\mathbf{x}_{i}^{\\top}\\mathbf{w}_{t}\\right)-(1-y_{i})\\log\\left(1-\\sigma\\left(\\mathbf{x}_{i}^{\\top}\\mathbf{w}_{t}\\right)\\right)+\\delta\\|\\mathbf{w}_{t}\\|_{2}^{2}+\\lambda\\|\\mathbf{w}_{t}\\|_{1}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where we set the hyper-parameter $\\sigma=0.001$ . ", "page_idx": 14}, {"type": "text", "text": "\u2022 For the exp-concave case, we utilize the logistic function as the time-varying function: ", "page_idx": 14}, {"type": "equation", "text": "$$\nf_{t}(\\mathbf{w}_{t};\\mathbf{x}_{t},y_{t})=\\log\\left(1+\\exp\\left(-y_{t}\\mathbf{w}_{t}^{\\top}\\mathbf{x}_{t}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which is exp-concave and smooth [Hazan et al., 2014], and still employ the $\\ell_{1}$ -norm regularizer. The composite loss function is shown below: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\phi(\\mathbf{w}_{t};\\mathbf{x}_{t},y_{t})=\\mathrm{log}\\left(1+\\mathrm{exp}\\left(-y_{t}\\mathbf{w}_{t}^{\\top}\\mathbf{x}_{t}\\right)\\right)+\\lambda\\|\\mathbf{w}_{t}\\|_{1}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Contenders. For the general convex and strongly convex cases, we compare our methods with OGD [Zinkevich, 2003], COMID [Duchi et al., 2010] and Optimistic-OMD [Chen et al., 2023]. For the exp-concave case, we choose ONS [Hazan et al., 2007], ProxONS [Yang et al., 2024c] and Optimistic-OMD [Chen et al., 2023] as the contenders. All parameters of each method are set according to their theoretical suggestions. For instance, in the general convex case, the learning rate is set as $\\bar{\\eta}=c t^{-1/2}$ in OGD, and $\\bar{\\eta}=c T^{-1/2}$ in COMID, $\\eta_{t}=\\bar{D}(c+\\bar{V}_{t-1})^{-1/2}$ in Optimistic-OMD where $c$ denotes the hyper-parameter selected from $\\{10^{-3},10^{-2},\\cdot\\cdot\\cdot,10^{4}\\}$ . ", "page_idx": 14}, {"type": "text", "text": "Results. All experiments are repeated ten times, and we report the instantaneous loss, the cumulative loss and the average loss against the number of rounds in Figure 1 for the general convex case, Figure 2 for the strongly convex case and Figure 3 for the exp-concave case. From the experimental results, it is evident that adversarial corruptions cause considerable fluctuations in the instantaneous losses across different methods. Moreover, we also observe that, in the three cases of composite SEA, both OptCMD and USC-SEA suffer lower losses compared to baseline methods, demonstrating better performance. This phenomenon can be attributed to their ability to adapt to the composite SEA environment, and their explicit support for handling the non-smooth component $r(\\cdot)$ . ", "page_idx": 14}, {"type": "image", "img_path": "MbEB5aKmMK/tmp/1de92145bf950494fd6fcdfaee8612ffb9fe9ec9e8f7dd659d90d941a0c6af3e.jpg", "img_caption": ["Figure 1: Experimental results for the general convex case. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "MbEB5aKmMK/tmp/d0aabe902583d9b9a680b4cc9cf87f0f5546c4360e990b3a7c9116cd72992de6.jpg", "img_caption": ["Figure 2: Experimental results for the strongly convex case. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "MbEB5aKmMK/tmp/7d37342b86e38e1e2e35c5c0c9ffe66d8e713b66112bbcf11223fb30189af90b.jpg", "img_caption": ["Figure 3: Experimental results for the exp-concave case. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "B Theoretical analysis ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Proof of Theorem 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "First, since the expected function $F_{t}(\\cdot)$ is convex in each round $t$ , we could decompose the instantaneous regret as below: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[\\left[f_{t}(\\mathbf{x}_{t})+r(\\mathbf{x}_{t})\\right]-\\left[f_{t}(\\mathbf{x}^{*})+r(\\mathbf{x}^{*})\\right]\\right]=\\mathbb{E}\\left[\\left[F_{t}(\\mathbf{x}_{t})+r(\\mathbf{x}_{t})\\right]-\\left[F_{t}(\\mathbf{x}^{*})+r(\\mathbf{x}^{*})\\right]\\right]}\\\\ &{\\le\\!\\mathbb{E}\\left[\\left\\langle\\nabla F_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}^{*}\\right\\rangle+r(\\mathbf{x}_{t})-r(\\mathbf{x}^{*})\\right]=\\mathbb{E}\\left[\\left\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}^{*}\\right\\rangle+r(\\mathbf{x}_{t})-r(\\mathbf{x}^{*})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The inequality is due the convexity of $F_{t}(\\cdot)$ and the last equality is due to the interchangeability of differentiation and integration by Leibniz integral rule. In the following, we shed the light on the right side of (18). ", "page_idx": 16}, {"type": "text", "text": "Then, we introduce the following lemma for OptCMD. ", "page_idx": 16}, {"type": "text", "text": "Lemma 1. Assume $\\mathcal{R}_{t}(\\cdot)$ is an $\\gamma$ -strongly convex function with respect to the norm $\\Vert\\cdot\\Vert$ and denote by $\\|\\cdot\\|_{*}$ the dual norm. According to the updating rule in (4) and (5), for all $\\mathbf{x}\\in\\mathcal{X}$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}\\rangle+r(\\mathbf{x}_{t})-r(\\mathbf{x})\\leq\\gamma^{-1}\\|M_{t}-\\nabla f_{t}(\\mathbf{x}_{t})\\|_{*}^{2}+[B^{\\mathcal{R}_{t}}(\\mathbf{x},\\hat{\\mathbf{x}}_{t})-B^{\\mathcal{R}_{t}}(\\mathbf{x},\\hat{\\mathbf{x}}_{t+1})]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad-\\left[B^{\\mathcal{R}_{t}}(\\hat{\\mathbf{x}}_{t+1},\\mathbf{x}_{t})+B^{\\mathcal{R}_{t}}(\\mathbf{x}_{t},\\hat{\\mathbf{x}}_{t})\\right]\\!.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In the general convex setting, we choose $\\eta_{t}=\\operatorname*{min}\\{D/\\sqrt{1+\\bar{V}_{t-1}},D/\\delta\\}$ , and $\\begin{array}{r}{\\|\\cdot\\|=\\frac{1}{\\sqrt{\\eta_{t}}}\\|\\cdot\\|_{2}}\\end{array}$ , and $\\|\\cdot\\|_{*}=\\sqrt{\\eta_{t}}\\|\\cdot\\|_{2}$ . Therefore, the corresponding Bregman divergence becomes $B^{\\mathcal{R}_{t}}(\\mathbf{x},\\mathbf{y})=$ $\\frac{1}{2\\eta_{t}}\\|\\mathbf{x}-\\mathbf{y}\\|_{2}^{2}$ with respect to the $\\eta^{-1}$ -strongly convex function $\\begin{array}{r}{\\mathcal{R}_{t}(\\mathbf{x})=\\frac{1}{2\\eta_{t}}\\|\\mathbf{x}\\|_{2}^{2}}\\end{array}$ . Now, we make use of Lemma 1 with $M_{t}=\\nabla f_{t-1}({\\mathbf{x}}_{t-1})$ , and obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}^{*}\\rangle+r(\\mathbf{x}_{t})-r(\\mathbf{x}^{*})\\leq\\eta_{t}\\|\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla f_{t-1}(\\mathbf{x}_{t-1})\\|_{2}^{2}}\\\\ {+\\displaystyle\\frac{1}{2\\eta_{t}}\\left\\{\\|\\mathbf{x}^{*}-\\hat{\\mathbf{x}}_{t}\\|_{2}^{2}-\\|\\mathbf{x}^{*}-\\hat{\\mathbf{x}}_{t+1}\\|_{2}^{2}\\right\\}-\\displaystyle\\frac{1}{2\\eta_{t}}\\left\\{\\|\\hat{\\mathbf{x}}_{t+1}-\\mathbf{x}_{t}\\|_{2}^{2}+\\|\\mathbf{x}_{t}-\\hat{\\mathbf{x}}_{t}\\|_{2}^{2}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Summing the above inequality over $t=1,\\cdot\\cdot\\cdot,T$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{t=1}^{T}\\left\\langle\\nabla f_{t}(\\mathbf x_{t}),\\mathbf x_{t}-\\mathbf x^{*}\\right\\rangle+r(\\mathbf x_{t})-r(\\mathbf x^{*})\\leq\\displaystyle\\sum_{t=1}^{T}\\eta_{t}\\|\\nabla f_{t}(\\mathbf x_{t})-\\nabla f_{t-1}(\\mathbf x_{t-1})\\|_{2}^{2}}\\\\ {\\displaystyle+\\sum_{t\\leq1}^{T}\\frac{1}{2\\eta_{t}}\\left\\{\\|\\mathbf x^{*}-\\hat{\\mathbf x}_{t}\\|_{2}^{2}-\\|\\mathbf x^{*}-\\hat{\\mathbf x}_{t+1}\\|_{2}^{2}\\right\\}-\\displaystyle\\sum_{t=1}^{T}\\frac{1}{2\\eta_{t}}\\left\\{\\|\\hat{\\mathbf x}_{t+1}-\\mathbf x_{t}\\|_{2}^{2}+\\|\\mathbf x_{t}-\\hat{\\mathbf x}_{t}\\|_{2}^{2}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now, we have decomposed the upper bound into three terms and will analyze them separately. ", "page_idx": 16}, {"type": "text", "text": "To bound the term (a), we introduce the following lemma. ", "page_idx": 16}, {"type": "text", "text": "Lemma 2. [Pogodin and Lattimore, 2019, Lemma 4.8] Let $l_{1},\\cdot\\cdot\\cdot\\,,l_{T}$ be non-negative real numbers with the $l_{t}\\in[0,B]$ . Then, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\frac{l_{t}}{\\sqrt{1+\\sum_{s=1}^{t-1}l_{s}}}\\le4\\sqrt{1+\\sum_{t=1}^{T}l_{t}}+B,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where for simplicity we define $0/\\sqrt{0}=0$ . ", "page_idx": 16}, {"type": "text", "text": "By utilizing Lemma 2 and the fact that $\\eta_{t}\\leq D/\\sqrt{1+\\bar{V}_{t-1}}$ , the term (a) can be upper bounded as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathsf{t e r m}\\left(\\mathsf{a}\\right)\\leq D\\sum_{t=1}^{T}\\frac{\\|\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla f_{t-1}(\\mathbf{x}_{t-1})\\|_{2}^{2}}{\\sqrt{1+\\bar{V}_{t-1}}}\\leq4D\\sqrt{1+\\bar{V}_{T}}+4D G^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For the term (b), a simple calculation delivers ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbf{term}\\left(\\mathbf{b}\\right)=\\frac{1}{2\\eta_{1}}\\|\\mathbf{x}^{*}-\\hat{\\mathbf{x}}_{1}\\|_{2}^{2}+\\frac{1}{2}\\sum_{t=2}^{T}\\left(\\frac{1}{\\eta_{t}}-\\frac{1}{\\eta_{t-1}}\\right)\\|\\mathbf{x}^{*}-\\hat{\\mathbf{x}}_{t}\\|_{2}^{2}-\\frac{1}{2\\eta_{T}}\\|\\mathbf{x}^{*}-\\hat{\\mathbf{x}}_{T+1}\\|_{2}^{2}}\\\\ {\\displaystyle\\leq\\frac{D^{2}}{2\\eta_{1}}+\\frac{D^{2}}{2}\\sum_{t=2}^{T}\\left(\\frac{1}{\\eta_{t}}-\\frac{1}{\\eta_{t-1}}\\right)=\\frac{D^{2}}{2\\eta_{T}}=\\frac{D}{2}\\sqrt{1+\\bar{V}_{T}}+\\frac{D\\delta}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For the term (c), we utilize the fact that $\\eta_{t}\\le D/\\delta$ , and thus obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbf{term}\\left(\\mathbf{c}\\right)=\\frac{1}{2}\\sum_{t=2}^{T+1}\\frac{1}{\\eta_{t}}\\|\\hat{\\mathbf{x}}_{t}-\\mathbf{x}_{t-1}\\|_{2}^{2}+\\frac{1}{2}\\sum_{t=1}^{T}\\frac{1}{\\eta_{t}}\\|\\mathbf{x}_{t}-\\hat{\\mathbf{x}}_{t}\\|_{2}^{2}}\\\\ {\\displaystyle\\geq\\frac{\\delta}{2D}\\sum_{t=2}^{T}\\left\\{\\|\\hat{\\mathbf{x}}_{t}-\\mathbf{x}_{t-1}\\|_{2}^{2}+\\|\\mathbf{x}_{t}-\\hat{\\mathbf{x}}_{t}\\|_{2}^{2}\\right\\}\\geq\\frac{\\delta}{4D}\\sum_{t=2}^{T}\\|\\mathbf{x}_{t}-\\mathbf{x}_{t-1}\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The last step is due to the domain bounded assumption, i.e., Assumption 1. Combining (21), (22) and (23), we obtain that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{T}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}^{*}\\rangle+r(\\mathbf{x}_{t})-r(\\mathbf{x}^{*})\\leq\\frac{3D}{2}\\sqrt{1+\\bar{V}_{T}}+\\frac{D\\delta}{2}-\\frac{\\delta}{4D}\\sum_{t=2}^{T}\\|\\mathbf{x}_{t}-\\mathbf{x}_{t-1}\\|_{2}^{2}+4D G^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "To bound the term $\\bar{V}_{T}$ , we exploiting the following lemma. ", "page_idx": 17}, {"type": "text", "text": "Lemma 3. Under Assumption 2 and 4, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\bar{V}}_{T}\\leq G^{2}+8\\displaystyle\\sum_{t=1}^{T}\\|\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla F_{t}(\\mathbf{x}_{t})\\|_{2}^{2}}\\\\ &{\\qquad\\qquad+4\\displaystyle\\sum_{t=2}^{T}\\|\\nabla F_{t}(\\mathbf{x}_{t-1})-\\nabla F_{t-1}(\\mathbf{x}_{t-1})\\|_{2}^{2}+4H^{2}\\displaystyle\\sum_{t=2}^{T}\\|\\mathbf{x}_{t}-\\mathbf{x}_{t-1}\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Plugging (25) into (24) obtains ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf{x}_{t})+\\nabla r(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}^{*}\\rangle}\\\\ {\\displaystyle\\leq\\frac{3(D+G D)}{2}+4D G^{2}+\\frac{D\\delta}{2}+6H D\\sqrt{\\sum_{t=2}^{T}\\|\\mathbf{x}_{t}-\\mathbf{x}_{t-1}\\|_{2}^{2}}-\\frac{\\delta}{4D}\\sum_{t=2}^{T}\\|\\mathbf{x}_{t}-\\mathbf{x}_{t-1}\\|_{2}^{2}}\\\\ {\\displaystyle+\\,12D\\sqrt{\\sum_{t=1}^{T}\\|\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla F_{t}(\\mathbf{x}_{t})\\|_{2}^{2}}+6D\\sqrt{\\sum_{t=2}^{T}\\|\\nabla F_{t}(\\mathbf{x}_{t-1})-\\nabla F_{t-1}(\\mathbf{x}_{t-1})\\|_{2}^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then, we apply the AM-GM inequality, i.e., $\\begin{array}{r l r}{6H D\\sqrt{\\sum_{t=2}^{T}\\left\\|\\mathbf{x}_{t}-\\mathbf{x}_{t-1}\\right\\|_{2}^{2}}}&{{}\\le}&{\\frac{36H^{2}D^{3}}{\\delta}\\ +}\\end{array}$ $\\begin{array}{r}{\\frac{\\delta}{4D}\\sum_{t=2}^{T}\\|\\mathbf{x}_{t}-\\mathbf{x}_{t-1}\\|_{2}^{2}}\\end{array}$ , and obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf x_{t})+\\nabla r(\\mathbf x_{t}),\\mathbf x_{t}-\\mathbf x^{*}\\rangle\\leq2(D+G D)+4D G^{2}+\\frac{D\\delta}{2}+\\frac{36H^{2}D^{3}}{\\delta}}}\\\\ &{\\quad+\\,12D\\sqrt{\\displaystyle\\sum_{t=1}^{T}\\|\\nabla f_{t}(\\mathbf x_{t})-\\nabla F_{t}(\\mathbf x_{t})\\|_{2}^{2}}+6D\\sqrt{\\displaystyle\\sum_{t=2}^{T}\\|\\nabla F_{t}(\\mathbf x_{t-1})-\\nabla F_{t-1}(\\mathbf x_{t-1})\\|_{2}^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By setting $\\delta=6\\sqrt{2}H D$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf{x}_{t})+\\nabla r(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}^{*}\\rangle\\leq\\!C_{1}+12D\\sqrt{\\displaystyle\\sum_{t=1}^{T}\\|\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla F_{t}(\\mathbf{x}_{t})\\|_{2}^{2}}}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad+\\ 6D\\sqrt{\\displaystyle\\sum_{t=2}^{T}\\|\\nabla F_{t}(\\mathbf{x}_{t-1})-\\nabla F_{t-1}(\\mathbf{x}_{t-1})\\|_{2}^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $C_{1}=2(D+G D)+4D G^{2}+6{\\sqrt{2}}H D^{2}$ . Next, according to the definition of $\\sigma_{1:T}^{2}$ and $\\Sigma_{1:T}^{2}$ , the expected regret can be upper bound as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[{\\sf R e g r e t}_{T}\\right]\\le\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla f_{t}({\\mathbf x}_{t})+\\nabla r({\\mathbf x}_{t}),{\\mathbf x}_{t}-{\\mathbf x}^{*}\\rangle\\right]}\\\\ &{\\qquad\\qquad\\le C_{1}+12D\\sqrt{\\sigma_{1:T}^{2}}+6D\\sqrt{\\Sigma_{1:T}^{2}}=\\mathcal{O}\\left(\\sqrt{\\sigma_{1:T}^{2}}+\\sqrt{\\Sigma_{1:T}^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "B.2 Proof of Theorem 2 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "First, according to (18), we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\left[f_{t}(\\mathbf{x}_{t})+r(\\mathbf{x}_{t})\\right]-\\left[f_{t}(\\mathbf{x}^{*})+r(\\mathbf{x}^{*})\\right]\\right]\\leq\\mathbb{E}\\left[\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}^{*}\\rangle\\right]+r(\\mathbf{x}_{t})-r(\\mathbf{x}^{*}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In this setting, we still choose $\\begin{array}{r}{\\|\\cdot\\|=\\frac{1}{\\sqrt{\\eta_{t}}}\\|\\cdot\\|_{2}}\\end{array}$ , and $\\|\\cdot\\|_{*}=\\sqrt{\\eta_{t}}\\|\\cdot\\|_{2}$ . Hence, the function $\\mathcal{R}_{t}(\\mathbf{x})=$ $\\begin{array}{r}{\\frac{1}{2\\eta_{t}}\\|\\mathbf{x}\\|_{2}^{2}}\\end{array}$ is $\\eta_{t}^{-1}$ -strongly convex and the Bregman divergence becomes $\\begin{array}{r}{\\beta^{\\mathcal{R}_{t}}(\\mathbf{x},\\mathbf{y})=\\frac{1}{2\\eta_{t}}\\|\\mathbf{x}-\\mathbf{y}\\|_{2}^{2}}\\end{array}$ . Then, applying Lemma 1, we obtain ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}^{*}\\rangle+r(\\mathbf{x}_{t})-r(\\mathbf{x}^{*})\\le_{\\bar{\\eta}_{t}}\\lVert M_{t+1}-\\nabla f_{t}(\\mathbf{x}_{t})\\rVert_{2}^{2}+\\displaystyle\\frac{1}{2\\eta_{t}}\\lVert\\mathbf{x}^{*}-\\hat{\\mathbf{x}}_{t}\\rVert_{2}^{2}}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\qquad-\\ \\frac{1}{2\\eta_{t}}\\lVert\\mathbf{x}^{*}-\\hat{\\mathbf{x}}_{t+1}\\rVert_{2}^{2}-\\displaystyle\\frac{1}{2\\eta_{t}}\\lVert\\hat{\\mathbf{x}}_{t+1}-\\mathbf{x}_{t}\\rVert_{2}^{2}-\\displaystyle\\frac{1}{2\\eta_{t}}\\lVert\\mathbf{x}_{t}-\\hat{\\mathbf{x}}_{t}\\rVert_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Following the configuration in Scroccaro et al. [2023], we set $M_{t}=\\nabla f_{t-1}\\big(\\hat{\\mathbf{x}}_{t}\\big)$ and $\\eta_{t}=(4H^{2}+$ ${\\bar{D}_{t}})^{-1/2}$ where $\\begin{array}{r}{\\bar{D}_{t}\\;=\\;\\sum_{s=1}^{t}\\|\\nabla f_{t}(\\hat{\\mathbf{x}}_{t})\\,-\\,\\nabla f_{t-1}(\\hat{\\mathbf{x}}_{t})\\|_{2}^{2}}\\end{array}$ . According to [Scroccaro et al., 2023, Theorem 2.5], we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}^{*}\\rangle+r(\\mathbf{x}_{t})-r(\\mathbf{x}^{*})\\leq\\left(5+\\frac{3}{2}D^{2}\\right)\\sqrt{4H^{2}+\\bar{D}_{T}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "To bound the term $\\bar{D}_{T}$ , we introduce the following lemma. ", "page_idx": 18}, {"type": "text", "text": "Lemma 4. Under Assumption 2, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\bar{D}_{T}\\leq G^{2}+6\\sum_{t=1}^{T}\\operatorname*{sup}_{{\\bf x}\\in{\\cal X}}\\|\\nabla f_{t}({\\bf x})-\\nabla F_{t}({\\bf x})\\|_{2}^{2}+4\\sum_{t=1}^{T}\\operatorname*{sup}_{{\\bf x}\\in{\\cal X}}\\|\\nabla F_{t}({\\bf x})-\\nabla F_{t-1}({\\bf x})\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By applying Lemma 4, we obtain ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{t=1}{\\overset{T}{\\sum}}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}^{*}\\rangle+r(\\mathbf{x}_{t})-r(\\mathbf{x}^{*})}\\\\ &{\\leq\\left(5+\\frac{3}{2}D^{2}\\right)\\left(\\sqrt{4H^{2}+G^{2}}+3\\sqrt{\\underset{t=1}{\\overset{T}{\\sum}}\\operatorname*{sup}_{t}\\|\\nabla f_{t}(\\mathbf{x})-\\nabla F_{t}(\\mathbf{x})\\|_{2}^{2}}\\right.}\\\\ &{\\qquad\\qquad\\qquad\\left.+2\\sqrt{\\underset{t=1}{\\overset{T}{\\sum}}\\operatorname*{sup}_{t}\\|\\nabla F_{t}(\\mathbf{x})-\\nabla F_{t-1}(\\mathbf{x})\\|_{2}^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Substituting (27) into (26) and taking the expectation on both sides finishes the proof. ", "page_idx": 18}, {"type": "text", "text": "B.3 Proof of Theorem 3 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In the beginning, we state the parameter configuration under the strongly convex setting (i.e., $F_{t}(\\cdot)$ is strongly convex and $r(\\cdot)$ is general convex). The learnin\u221ag rate is set as $\\begin{array}{r}{\\eta_{t}=\\frac{2}{\\delta+\\lambda t}}\\end{array}$ and the norm is chosen as $\\begin{array}{r}{\\|\\cdot\\|=\\frac{1}{\\sqrt{\\eta_{t}}}\\|\\cdot\\|_{2}}\\end{array}$ , with the dual norm $\\|\\cdot\\|_{*}=\\sqrt{\\eta_{t}}\\|\\cdot\\|_{2}$ . Therefore, the function becomes $\\begin{array}{r}{\\mathcal{R}_{t}(\\mathbf{x})=\\frac{1}{2\\eta_{t}}\\|\\mathbf{x}\\|_{2}^{2}}\\end{array}$ and the corresponding Bregman divergence becomes $\\begin{array}{r}{\\beta^{\\mathcal{R}_{t}}(\\mathbf{x},\\mathbf{y})=\\frac{1}{2\\eta_{t}}\\|\\mathbf{x}-\\mathbf{y}\\|_{2}^{2}}\\end{array}$ . ", "page_idx": 18}, {"type": "text", "text": "Then, we make use of the strong convexity of $F_{t}(\\cdot)$ , and decompose the instantaneous regret as below: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\left[\\left[f_{t}(\\mathbf{x}_{t})+r(\\mathbf{x}_{t})\\right]-\\left[f_{t}(\\mathbf{x}^{*})+r(\\mathbf{x}^{*})\\right]\\right]=\\mathbb{E}\\left[\\left[F_{t}(\\mathbf{x}_{t})+r(\\mathbf{x}_{t})\\right]-\\left[F_{t}(\\mathbf{x}^{*})+r(\\mathbf{x}^{*})\\right]\\right]}\\\\ {\\displaystyle\\leq\\mathbb{E}\\left[\\left\\langle\\nabla F_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}^{*}\\right\\rangle+r(\\mathbf{x}_{t})-r(\\mathbf{x}^{*})-\\frac{\\lambda}{2}\\|\\mathbf{x}^{*}-\\mathbf{x}_{t}\\|_{2}^{2}\\right]}\\\\ {\\displaystyle=\\mathbb{E}\\left[\\left\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}^{*}\\right\\rangle+r(\\mathbf{x}_{t})-r(\\mathbf{x}^{*})-\\frac{\\lambda}{2}\\|\\mathbf{x}^{*}-\\mathbf{x}_{t}\\|_{2}^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Similar to the analysis in Theorem 1, we then focus on the the right side of (28). By utilizing Lemma 1, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}^{*}\\rangle+r(\\mathbf{x}_{t})-r(\\mathbf{x}^{*})-\\frac{\\lambda}{2}\\sum_{t=1}^{T}\\|\\mathbf{x}^{*}-\\mathbf{x}_{t}\\|_{2}^{2}\\leq\\displaystyle\\sum_{t=1}^{T}\\eta_{t}\\|\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla f_{t-1}(\\mathbf{x}_{t-1})\\|_{2}^{2}}\\\\ {\\displaystyle+\\sum_{t\\in1}^{T}\\left[\\frac{1}{2\\eta_{t}}\\left\\{\\|\\mathbf{x}^{*}-\\hat{\\mathbf{x}}_{t}\\|_{2}^{2}-\\|\\mathbf{x}^{*}-\\hat{\\mathbf{x}}_{t+1}\\|_{2}^{2}\\right\\}-\\frac{\\lambda}{2}\\|\\mathbf{x}^{*}-\\mathbf{x}_{t}\\|_{2}^{2}\\right]-\\underbrace{\\sum_{t=1}^{T}\\frac{1}{2\\eta_{t}}\\left\\{\\|\\hat{\\mathbf{x}}_{t+1}-\\mathbf{x}_{t}\\|_{2}^{2}+\\|\\mathbf{x}_{t}-\\hat{\\mathbf{x}}_{t}\\|_{2}^{2}\\right\\}}_{\\displaystyle\\in\\ a\\mathrm{~f~f~}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Next, we analyze the above three terms separately. For the term $(\\mathsf{a})$ , we substitute $\\begin{array}{r}{\\eta\\le\\frac{2}{\\lambda t}}\\end{array}$ and obtain ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathsf{t e r m}\\left(\\mathsf{a}\\right)\\leq\\sum_{t=1}^{T}\\frac{2}{\\lambda t}\\|\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla f_{t-1}(\\mathbf{x}_{t-1})\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For the term (b), we exploit one result of Lemma 1 in (52), i.e., ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathbf{x}_{t}-\\hat{\\mathbf{x}}_{t+1}\\|_{2}\\leq\\eta_{t}\\|\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla f_{t-1}(\\mathbf{x}_{t-1})\\|_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with $\\begin{array}{r}{\\|\\cdot\\|=\\frac{1}{\\sqrt{\\eta_{t}}}\\|\\cdot\\|_{2}}\\end{array}$ and $\\|\\cdot\\|_{*}=\\sqrt{\\eta_{t}}\\|\\cdot\\|_{2}$ . By utilizing (30), we can upper bound the term (b) as following: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{t e r m}\\left(\\mathbf{b}\\right)\\le\\displaystyle\\frac{D^{2}}{2\\eta_{1}}+\\sum_{t=2}^{T}\\left(\\frac{1}{2\\eta_{t}}-\\frac{1}{2\\eta_{t-1}}\\right)\\left\\|\\mathbf{x}^{*}-\\hat{\\mathbf{x}}_{t}\\right\\|_{2}^{2}-\\frac{\\lambda}{2}\\displaystyle\\sum_{t=1}^{T}\\left\\|\\mathbf{x}^{*}-\\mathbf{x}_{t}\\right\\|_{2}^{2}}\\\\ &{\\qquad\\le\\displaystyle\\frac{D^{2}\\lambda}{4}+\\frac{\\lambda}{4}\\sum_{t=1}^{T-1}\\left[\\left\\|\\mathbf{x}^{*}-\\hat{\\mathbf{x}}_{t+1}\\right\\|_{2}^{2}-2\\left\\|\\mathbf{x}^{*}-\\mathbf{x}_{t}\\right\\|_{2}^{2}\\right]}\\\\ &{\\qquad\\le\\displaystyle\\frac{D^{2}\\lambda}{4}+\\frac{\\lambda}{2}\\sum_{t=1}^{T-1}\\left\\|\\mathbf{x}_{t}-\\hat{\\mathbf{x}}_{t+1}\\right\\|_{2}^{2}\\overset{(30)}{\\le}\\displaystyle\\frac{D^{2}\\lambda}{4}+\\frac{\\lambda}{2}\\sum_{t=1}^{T-1}\\eta_{t}^{2}\\left\\|\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla f_{t-1}\\big(\\mathbf{x}_{t-1}\\big)\\right\\|_{2}}\\\\ &{\\qquad\\le\\displaystyle\\frac{D^{2}\\lambda}{4}+\\frac{\\lambda\\eta_{1}}{2}\\sum_{t=1}^{T-1}\\eta_{t}\\left\\|\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla f_{t-1}\\big(\\mathbf{x}_{t-1}\\big)\\right\\|_{2}\\le\\frac{D^{2}\\lambda}{4}+\\mathsf{t e r m}\\left(\\mathbf{a}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For the term (c), we make use of the non-increasing property of $\\eta_{t}$ and $\\eta_{t}\\leq2/\\delta$ , and obtain ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathsf{t e r m}\\left(\\mathbf{c}\\right)\\geq\\displaystyle\\sum_{t=2}^{T}\\left\\{\\frac{1}{2\\eta_{t-1}}\\|\\hat{\\mathbf{x}}_{t}-\\mathbf{x}_{t-1}\\|_{2}^{2}+\\frac{1}{2\\eta_{t-1}}\\|\\mathbf{x}_{t}-\\hat{\\mathbf{x}}_{t}\\|_{2}^{2}\\right\\}}\\\\ {\\geq\\displaystyle\\sum_{t=2}^{T}\\frac{1}{4\\eta_{t-1}}\\|\\mathbf{x}_{t}-\\mathbf{x}_{t-1}\\|_{2}^{2}\\geq\\frac{\\delta}{8}\\displaystyle\\sum_{t=2}^{T}\\|\\mathbf{x}_{t}-\\mathbf{x}_{t-1}\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Putting (29), (31) and (32) together, we obtain ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}^{*}\\rangle+r(\\mathbf{x}_{t})-r(\\mathbf{x}^{*})-\\frac{\\lambda}{2}\\sum_{t=1}^{T}\\|\\mathbf{x}^{*}-\\mathbf{x}_{t}\\|_{2}^{2}}\\\\ &{\\displaystyle\\leq\\frac{D^{2}\\lambda}{4}+\\sum_{t=1}^{T}\\frac{4}{\\lambda t}\\|\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla f_{t-1}(\\mathbf{x}_{t-1})\\|_{2}^{2}-\\frac{\\delta}{8}\\sum_{t=2}^{T}\\|\\mathbf{x}_{t}-\\mathbf{x}_{t-1}\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Now, we make use of a byproduct (55) from Lemma 3, i.e., ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla f_{t-1}(\\mathbf{x}_{t-1})\\|_{2}^{2}\\leq4\\|\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla F_{t}(\\mathbf{x}_{t})\\|_{2}^{2}+4H^{2}\\|\\mathbf{x}_{t}-\\mathbf{x}_{t-1}\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad+\\ 4\\|\\nabla F_{t}(\\mathbf{x}_{t-1})-\\nabla F_{t-1}(\\mathbf{x}_{t-1})\\|_{2}^{2}+4\\|\\nabla F_{t-1}(\\mathbf{x}_{t-1})-\\nabla f_{t-1}(\\mathbf{x}_{t-1})\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and thus (33) becomes ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\left[\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}^{*}\\rangle+r(\\mathbf{x}_{t})-r(\\mathbf{x}^{*})-\\frac{\\lambda}{2}\\sum_{t=1}^{T}\\|\\mathbf{x}^{*}-\\mathbf{x}_{t}\\|_{2}^{2}\\right]}\\\\ {\\displaystyle\\leq\\sum_{t=1}^{T}\\frac{16}{\\lambda t}(2\\sigma_{t}^{2}+\\operatorname*{sup}_{\\mathbf{x}\\in\\mathcal{X}}\\|\\nabla F_{t}(\\mathbf{x})-\\nabla F_{t-1}(\\mathbf{x})\\|_{2}^{2})}\\\\ {\\displaystyle\\quad+\\sum_{t=1}^{T}\\frac{16H^{2}}{\\lambda t}\\|\\mathbf{x}_{t}-\\mathbf{x}_{t-1}\\|_{2}^{2}-\\frac{\\delta}{8}\\sum_{t=2}^{T}\\|\\mathbf{x}_{t}-\\mathbf{x}_{t-1}\\|_{2}^{2}+\\frac{D^{2}\\lambda}{4}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "To bound the first two terms, we introduce the following lemma. ", "page_idx": 20}, {"type": "text", "text": "Lemma 5. $l$ [Yan et al., 2023, Lemma 9] For a sequence of $\\{a_{t}\\}_{t=1}^{T}$ and $b$ , where $a_{t},b>0$ for any $t\\in[T],$ , denoting by $a_{\\operatorname*{max}}\\triangleq\\operatorname*{max}_{t}a_{t}$ and $\\begin{array}{r}{A\\triangleq\\lceil b\\sum_{t=1}^{T}a_{t}\\rceil}\\end{array}$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}{\\frac{a_{t}}{b t}}\\leq{\\frac{a_{\\operatorname*{max}}}{b}}\\left(1+\\ln A\\right)+{\\frac{1}{b^{2}}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Let $a_{t}\\;=\\;2\\sigma_{t}^{2}\\,+\\,\\mathrm{sup}_{\\mathbf{x}\\in\\mathcal{X}}\\;\\|\\nabla F_{t}(\\mathbf{x})\\,-\\,\\nabla F_{t-1}(\\mathbf{x})\\|_{2}^{2}$ , $a_{\\mathrm{max}}\\:=\\:2\\sigma_{\\mathrm{max}}^{2}\\:+\\:\\Sigma_{\\mathrm{max}}^{2},\\:b\\:=\\:\\lambda$ and $A\\,=$ $\\lceil\\lambda(2\\sigma_{1:T}^{2}+\\Sigma_{1:t}^{2})\\rceil$ . Then, applying Lemma 5, we obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\frac{1}{\\lambda t}(2\\sigma_{t}^{2}+\\operatorname*{sup}_{\\mathbf{x}\\in\\mathcal{X}}\\|\\nabla F_{t}(\\mathbf{x})-\\nabla F_{t-1}(\\mathbf{x})\\|_{2}^{2})\\leq\\frac{2\\sigma_{\\operatorname*{max}}^{2}+\\Sigma_{\\operatorname*{max}}^{2}}{\\lambda}\\left(1+\\ln(1+\\lambda(2\\sigma_{1:T}^{2}+\\Sigma_{1:T}^{2}))\\right)+\\frac{2\\sigma_{\\operatorname*{max}}^{2}}{\\lambda}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Let $a_{t}=\\|\\mathbf{x}_{t}-\\mathbf{x}_{t-1}\\|_{2}^{2}$ , $a_{\\mathrm{max}}=D^{2}$ , $b=\\lambda$ and $\\begin{array}{r}{A=\\lceil\\lambda\\sum_{t=1}^{T}\\|\\mathbf{x}_{t}-\\mathbf{x}_{t-1}\\|_{2}^{2}\\rceil}\\end{array}$ . Applying Lemma 5, we obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{t=1}^{T}\\displaystyle\\frac{1}{\\lambda t}\\|\\mathbf{x}_{t}-\\mathbf{x}_{t-1}\\|_{2}^{2}\\leq\\displaystyle\\frac{D^{2}}{\\lambda}\\left(1+\\ln(1+\\lambda\\displaystyle\\sum_{t=1}^{T}\\|\\mathbf{x}_{t}-\\mathbf{x}_{t-1}\\|_{2}^{2})\\right)+\\frac{1}{\\lambda^{2}}}\\\\ {\\displaystyle\\leq\\displaystyle\\frac{D^{2}}{\\lambda}\\left(1+\\lambda\\displaystyle\\sum_{t=1}^{T}\\|\\mathbf{x}_{t}-\\mathbf{x}_{t-1}\\|_{2}^{2}\\right)+\\frac{1}{\\lambda^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the last step is due to $\\ln(1+x)\\leq x$ for any $x\\geq0$ . ", "page_idx": 20}, {"type": "text", "text": "Substituting (35) and (36) into (34), we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\left[\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}^{*}\\rangle+r(\\mathbf{x}_{t})-r(\\mathbf{x}^{*})-\\frac{\\lambda}{2}\\sum_{t=1}^{T}\\|\\mathbf{x}^{*}-\\mathbf{x}_{t}\\|_{2}^{2}\\right]}\\\\ {\\displaystyle\\leq\\mathcal{O}\\left(\\frac{\\sigma_{\\operatorname*{max}}^{2}+\\Sigma_{\\operatorname*{max}}^{2}}{\\lambda}\\ln(\\sigma_{1:T}^{2}+\\Sigma_{1:T}^{2})\\right)+\\left(16H^{2}D^{2}-\\frac{\\delta}{8}\\right)\\sum_{t=2}^{T}\\|\\mathbf{x}_{t}-\\mathbf{x}_{t-1}\\|_{2}^{2}+\\mathcal{O}(1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Setting $\\delta=128H^{2}D^{2}$ finishes the proof. ", "page_idx": 20}, {"type": "text", "text": "B.4 Proof of Theorem 4 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Similar to (28) in Theorem 3, the instantaneous regret is upper bounded by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\updownarrow\\left[\\left|f_{t}(\\mathbf{x}_{t})+r(\\mathbf{x}_{t})\\right|-\\left|f_{t}(\\mathbf{x}^{*})+r(\\mathbf{x}^{*})\\right|\\right]=\\mathbb{E}\\left[\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}^{*}\\rangle+r(\\mathbf{x}_{t})-r(\\mathbf{x}^{*})-\\frac{\\lambda}{2}{\\left\\|\\mathbf{x}^{*}-\\mathbf{x}_{t}\\right\\|}_{2}^{2}\\right]\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "According to [Scroccaro et al., 2023, Theorem 2.9], OptCMD with the configuration in (9) ensures ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}^{*}\\rangle+r(\\mathbf{x}_{t})-r(\\mathbf{x}^{*})-\\frac{\\lambda}{2}\\|\\mathbf{x}^{*}-\\mathbf{x}_{t}\\|_{2}^{2}}\\\\ {\\displaystyle\\le2H D^{2}+\\frac{G^{2}}{H}+\\frac{4G^{2}}{\\lambda}\\log\\left(1+\\frac{\\lambda}{4H G^{2}}\\bar{D}_{T}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For the term $\\begin{array}{r}{\\log\\left(1+\\frac{\\lambda}{4H G^{2}}\\bar{D}_{T}\\right)}\\end{array}$ , we apply Lemma 4 and obtain ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\log\\left(1+\\frac{\\lambda}{4H G^{2}}\\bar{D}_{T}\\right)\\leq\\log\\left(1+\\frac{\\lambda}{4H}\\right)}}\\\\ {{\\displaystyle+\\log\\left(1+\\frac{\\lambda}{8H G^{2}}\\left(3\\sum_{t=1}^{T}\\underset{\\kappa\\in X}{\\operatorname*{sup}}\\,\\|\\nabla f_{t}(\\mathbf x)-\\nabla F_{t}(\\mathbf x)\\|_{2}^{2}+2\\sum_{t=1}^{T}\\underset{\\kappa\\in X}{\\operatorname*{sup}}\\,\\|\\nabla F_{t}(\\mathbf x)-\\nabla F_{t-1}(\\mathbf x)\\|_{2}^{2}\\right)\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Taking expectation over both sides of the above inequality delivers ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\log\\left(1+\\frac{\\lambda}{4H G^{2}}\\Bar{D}_{T}\\right)\\right]\\leq\\log\\left(1+\\frac{\\lambda}{4H}\\right)+\\log\\left(1+\\frac{\\lambda}{8H G^{2}}\\left(3\\Tilde{\\sigma}_{1:T}^{2}+2\\Sigma_{1:T}^{2}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Plug the above result into (37) finishes the proof. ", "page_idx": 21}, {"type": "text", "text": "B.5 Proof of Theorem 5 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "For the exp-concave setting (i.e., $f_{t}(\\cdot)$ is exp-concave and $r(\\cdot)$ is general convex), our parameter configuration is follow Chiang et al. [2012]. To be precise, we define $\\begin{array}{r}{H_{t}=\\delta I+\\frac{\\beta}{2}G^{2}I+\\frac{\\beta}{2}\\sum_{r=1}^{t-1}h_{r}}\\end{array}$ , where $h_{r}\\,=\\,\\nabla f_{r}(\\mathbf{x}_{r})\\nabla f_{r}(\\mathbf{x}_{r})^{\\top}$ . The norm is set as $\\|\\cdot\\|=\\|\\cdot\\|_{H_{t}}$ and $\\|\\cdot\\|_{*}\\,=\\,\\|\\,\\cdot\\,\\|_{H_{t}^{-1}}$ , and the function becomes $\\begin{array}{r}{\\mathcal{R}_{t}(\\mathbf{x})=\\frac{1}{2}\\|\\mathbf{x}\\|_{H_{t}}^{2}}\\end{array}$ . With this $\\mathcal{R}_{t}(\\mathbf{x})$ , the corresponding Bregman divergence becomes BRt(x, y) = 12\u2225x \u2212y\u22252H. ", "page_idx": 21}, {"type": "text", "text": "First, we introduce a common property of exp-concave function, as shown below. ", "page_idx": 21}, {"type": "text", "text": "Lemma 6. [Hazan et al., 2007, Lemma $3J$ Under Assumption $^{\\,l}$ and 2, if $f(\\cdot)$ is exp-concave over $\\mathcal{X}$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\nf(\\mathbf{y})\\geq f(\\mathbf{x})+\\langle\\nabla f(\\mathbf{x}),\\mathbf{y}-\\mathbf{x}\\rangle+{\\frac{\\beta}{2}}\\langle\\nabla f(\\mathbf{x}),\\mathbf{y}-\\mathbf{x}\\rangle^{2},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "$f o r\\,\\forall\\mathbf{x},\\mathbf{y}\\in\\mathcal{X}$ and $\\begin{array}{r}{\\beta=\\frac{1}{2}\\operatorname*{min}\\lbrace\\frac{1}{4G D},\\alpha\\rbrace}\\end{array}$ . ", "page_idx": 21}, {"type": "text", "text": "Then, according to the exponential concavity of $f_{t}(\\cdot)$ shown in Lemma 6, we can decompose the instantaneous regret as below: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[\\left[f_{t}(\\mathbf{x}_{t})+r(\\mathbf{x}_{t})\\right]-\\left[f_{t}(\\mathbf{x}^{*})+r(\\mathbf{x}^{*})\\right]\\right]}\\\\ &{\\le\\!\\mathbb{E}\\left[\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}^{*}\\rangle+r(\\mathbf{x}_{t})-r(\\mathbf{x}^{*})-\\frac{\\beta}{2}\\left\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}^{*}-\\mathbf{x}_{t}\\right\\rangle^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Recall that we denote $h_{t}\\,=\\,\\nabla f_{t}({\\mathbf{x}}_{t})\\nabla f_{t}({\\mathbf{x}}_{t})^{\\top}$ . Hence, the last term in (38) can be rewrite as $\\langle\\nabla f_{t}({\\mathbf{x}}_{t}),{\\mathbf{x}}^{*}-{\\mathbf{x}}_{t}\\rangle^{2}=\\|{\\mathbf{x}}^{*}-{\\mathbf{x}}_{t}\\|_{h_{t}}^{2}$ . ", "page_idx": 21}, {"type": "text", "text": "Now, we focus on the right side of (38). By utilizing Lemma 1, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}^{*}\\rangle+r(\\mathbf{x}_{t})-r(\\mathbf{x}^{*})-\\frac{\\beta}{2}\\sum_{t=1}^{T}\\|\\mathbf{x}^{*}-\\mathbf{x}_{t}\\|_{h_{t}}^{2}\\leq\\sum_{t=1}^{T}\\|\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla f_{t-1}(\\mathbf{x}_{t-1})\\|_{H_{t}^{-}}^{2}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n+\\frac{1}{2}\\sum_{t=1}^{T}\\left[\\|\\mathbf{x}^{*}-\\hat{\\mathbf{x}}_{t}\\|_{H_{t}}^{2}-\\|\\mathbf{x}^{*}-\\hat{\\mathbf{x}}_{t+1}\\|_{H_{t}}^{2}-\\beta\\|\\mathbf{x}^{*}-\\mathbf{x}_{t}\\|_{h_{t}}^{2}\\right]-\\frac{1}{2}\\sum_{t=1}^{T}\\left[\\|\\hat{\\mathbf{x}}_{t+1}-\\mathbf{x}_{t}\\|_{H_{t}}^{2}+\\|\\mathbf{x}_{t}-\\hat{\\mathbf{x}}_{t}\\|_{H_{t}}^{2}\\right]\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The term $(\\mathsf{a})$ can be upper bounded as shown in the following lemma. ", "page_idx": 21}, {"type": "text", "text": "Lemma 7. Let d be the dimension. Then, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathsf{t e r m}\\left(\\mathsf{a}\\right)\\leq\\frac{8d}{\\beta}\\ln\\left(1+\\frac{\\beta}{8\\delta}\\sum_{t=1}^{T}\\left\\|\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla f_{t-1}\\big(\\mathbf{x}_{t-1}\\big)\\right\\|_{2}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For the term (b), we exploit the fact that $\\begin{array}{r}{H_{t+1}-H_{t}=\\frac{\\beta}{2}h_{t}}\\end{array}$ and obtain ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathbf{term}\\left(\\mathbf{b}\\right)=\\frac{1}{2}\\left[\\|\\mathbf{x}^{*}-\\hat{\\mathbf{x}}_{1}\\|_{H_{1}}^{2}-\\|\\mathbf{x}^{*}-\\hat{\\mathbf{x}}_{T+1}\\|_{H_{T+1}}^{2}\\right]}\\\\ &{\\displaystyle\\qquad\\qquad+\\,\\frac{1}{2}\\sum_{t=1}^{T}\\left\\{\\|\\mathbf{x}^{*}-\\hat{\\mathbf{x}}_{t+1}\\|_{H_{t+1}}^{2}-\\|\\mathbf{x}^{*}-\\hat{\\mathbf{x}}_{t+1}\\|_{H_{t}}^{2}-\\beta\\|\\mathbf{x}^{*}-\\mathbf{x}_{t}\\|_{h_{t}}^{2}\\right\\}}\\\\ &{\\displaystyle\\qquad=\\frac{1}{2}\\left[\\|\\mathbf{x}^{*}-\\hat{\\mathbf{x}}_{1}\\|_{H_{1}}^{2}-\\|\\mathbf{x}^{*}-\\hat{\\mathbf{x}}_{T+1}\\|_{H_{T+1}}^{2}\\right]+\\frac{\\beta}{4}\\sum_{t=1}^{T}\\left\\{\\|\\mathbf{x}^{*}-\\hat{\\mathbf{x}}_{t+1}\\|_{h_{t}}^{2}-2\\|\\mathbf{x}^{*}-\\mathbf{x}_{t}\\|_{h_{t}}^{2}\\right\\}}\\\\ &{\\displaystyle\\qquad\\overset{(i)}{\\leq}\\left(\\frac{\\delta}{2}+\\frac{\\beta}{4}G^{2}\\right)D^{2}+\\frac{\\beta}{4}\\sum_{t=1}^{T}\\left\\{\\|\\mathbf{x}^{*}-\\hat{\\mathbf{x}}_{t+1}\\|_{h_{t}}^{2}-2\\|\\mathbf{x}^{*}-\\mathbf{x}_{t}\\|_{h_{t}}^{2}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "To proceed the proof, we introduce the following lemma. ", "page_idx": 22}, {"type": "text", "text": "Lemma 8. [Chiang et al., 2012, Proposition $I J$ For any $y,z\\in\\mathbb{R}^{N}$ and any PSD $H\\in\\mathbb{R}^{N\\times N}$ , we have $\\|y+z\\|_{H}^{2}\\le2\\|y\\|_{H}^{2}+2\\|z\\|_{H}^{2}$ . ", "page_idx": 22}, {"type": "text", "text": "Applying Lemma 8 on (39) obtains: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{\\sf t e r m}\\left({\\bf b}\\right)\\leq\\left(\\frac{\\delta}{2}+\\frac{\\beta}{4}G^{2}\\right)D^{2}+\\frac{\\beta}{4}\\sum_{t=1}^{T}\\left\\{2\\left\\Vert{\\bf x}^{*}-{\\bf x}_{t}\\right\\Vert_{h_{t}}^{2}+2\\left\\Vert{\\bf x}_{t}-\\hat{{\\bf x}}_{t+1}\\right\\Vert_{h_{t}}^{2}-2\\left\\Vert{\\bf x}^{*}-{\\bf x}_{t}\\right\\Vert_{h_{t}}^{2}\\right\\}\\quad}\\\\ {\\displaystyle{\\leq\\left(\\frac{\\delta}{2}+\\frac{\\beta}{4}G^{2}\\right)D^{2}+\\frac{\\beta}{2}\\sum_{t=1}^{T}\\left\\Vert{\\bf x}_{t}-\\hat{{\\bf x}}_{t+1}\\right\\Vert_{h_{t}}^{2}\\leq\\left(\\frac{1}{2}+\\frac{\\beta}{4}G^{2}\\right)D^{2}+\\sum_{t=1}^{T}\\left\\Vert{\\bf x}_{t}-\\hat{{\\bf x}}_{t+1}\\right\\Vert_{H_{t}}^{2}}}\\\\ {\\displaystyle{\\leq\\left(\\frac{\\delta}{2}+\\frac{\\beta}{4}G^{2}\\right)D^{2}+\\sum_{t=1}^{T}\\left\\Vert\\nabla f_{t}({\\bf x}_{t})-\\nabla f_{t-1}({\\bf x}_{t-1})\\right\\Vert_{H_{t}^{-1}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the penultimate step is due to $H_{t}\\succeq\\frac{\\beta}{2}G^{2}I\\succeq\\frac{\\beta}{2}h_{t}$ , and the last step is due to ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|\\mathbf{x}_{t}-\\hat{\\mathbf{x}}_{t+1}\\|_{H_{t}}\\leq\\|\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla f_{t-1}(\\mathbf{x}_{t-1})\\|_{H_{t}^{-1}},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which could be obtained from Lemma 1 with $\\|\\cdot\\|=\\|\\cdot\\|_{H_{t}}$ and $\\|\\cdot\\|_{*}=\\|\\cdot\\|_{H_{t}^{-1}}$ . ", "page_idx": 22}, {"type": "text", "text": "For the term $(\\mathsf{c})$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbf{term}\\left(\\mathbf{c}\\right)=\\frac{1}{2}\\sum_{t=2}^{T+1}\\|\\hat{\\mathbf{x}}_{t}-\\mathbf{x}_{t-1}\\|_{H_{t-1}}^{2}+\\frac{1}{2}\\sum_{t=1}^{T}\\|\\mathbf{x}_{t}-\\hat{\\mathbf{x}}_{t}\\|_{H_{t}}^{2}}\\\\ {\\displaystyle\\geq\\frac{\\delta}{2}\\sum_{t=2}^{T}\\left\\{\\|\\hat{\\mathbf{x}}_{t}-\\mathbf{x}_{t-1}\\|_{2}^{2}+\\|\\mathbf{x}_{t}-\\hat{\\mathbf{x}}_{t}\\|_{2}^{2}\\right\\}\\geq\\frac{\\delta}{4}\\sum_{t=2}^{T}\\|\\mathbf{x}_{t}-\\mathbf{x}_{t-1}\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the first inequality is due to $H_{t}\\geq H_{t-1}\\succeq\\delta I.$ , $\\|\\hat{\\mathbf{x}}_{T+1}-\\mathbf{x}_{T}\\|_{H_{T+1}}^{2}\\geq0$ and $\\|\\mathbf{x}_{1}-\\hat{\\mathbf{x}}_{1}\\|_{H_{1}}^{2}\\ge0$ . Combining (38), (40) and (42) obtains ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}^{*}\\rangle+r(\\mathbf{x}_{t})-r(\\mathbf{x}^{*})-\\frac{\\beta}{2}\\sum_{t=1}^{T}\\|\\mathbf{x}^{*}-\\mathbf{x}_{t}\\|_{h_{t}}^{2}}\\\\ &{\\leq\\displaystyle\\left(\\frac{\\delta}{2}+\\frac{\\beta}{4}G^{2}\\right)D^{2}+\\frac{24d}{\\beta}\\ln\\left(1+\\frac{\\beta}{8\\delta}\\sum_{t=1}^{T}\\|\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla f_{t-1}(\\mathbf{x}_{t-1})\\|_{2}^{2}\\right)-\\frac{\\delta}{4}\\sum_{t=2}^{T}\\|\\mathbf{x}_{t}-\\mathbf{x}_{t-1}\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Next, we leverage Lemma 3 and arrive at ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}^{*}\\rangle+r(\\mathbf{x}_{t})-r(\\mathbf{x}^{*})-\\frac{\\beta}{2}\\sum_{t=1}^{T}\\|\\mathbf{x}^{*}-\\mathbf{x}_{t}\\|_{h}^{2}}\\\\ &{\\displaystyle\\leq\\left(\\frac{\\beta}{2}+\\frac{\\beta}{4}G^{2}\\right)D^{2}-\\frac{\\delta}{4}\\sum_{t=2}^{T}\\|\\mathbf{x}_{t}-\\mathbf{x}_{t-1}\\|_{2}^{2}\\right)+\\frac{16d}{\\beta}\\ln\\left(1+\\frac{\\beta}{8\\delta}\\left[G^{2}+8\\sum_{t=1}^{T}\\|\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla F_{t}(\\mathbf{x}_{t})\\|_{2}^{2}\\right]\\right.}\\\\ &{\\displaystyle~\\left.+4\\sum_{t=2}^{T}\\|\\nabla F_{t}(\\mathbf{x}_{t-1})-\\nabla F_{t-1}(\\mathbf{x}_{t-1})\\|_{2}^{2}\\right]+\\frac{\\beta H^{2}}{2\\delta}\\sum_{t=2}^{T}\\|\\mathbf{x}_{t}-\\mathbf{x}_{t-1}\\|_{2}^{2}\\right)}\\\\ &{\\displaystyle\\leq\\left(\\frac{\\delta}{2}+\\frac{\\beta}{4}G^{2}\\right)D^{2}-\\frac{\\delta}{4}\\sum_{t=2}^{T}\\|\\mathbf{x}_{t}-\\mathbf{x}_{t-1}\\|_{2}^{2}+\\frac{16d}{\\beta}\\ln\\left(1+\\frac{\\beta}{8\\delta}\\left[G^{2}+8\\sum_{t=1}^{T}\\|\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla F_{t}(\\mathbf{x}_{t})\\|_{2}^{2}\\right]\\right.}\\\\ &{\\displaystyle~\\left.+\\ 4\\sum_{t=2}^{T}\\|\\nabla F_{t}(\\mathbf{x}_{t-1})-\\nabla F_{t-1}(\\mathbf{x}_{t-1})\\|_{2}^{2}\\right]\\right)+\\frac{16d}{\\beta}\\ln\\left(1+\\frac{\\beta H^{2}}{2\\delta}\\sum_{t=2}^{T}\\|\\mathbf{x}_{t}-\\mathbf{x}_{t-1}\\|_{2}^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the last step is due to the inequality ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\ln(1+u+v)\\leq\\ln(1+u)+\\ln(1+v),\\;\\forall u,v\\geq0.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "To simplify (43), we make use of the following lemma. ", "page_idx": 23}, {"type": "text", "text": "Lemma 9. [Chen et al., 2023, Lemma 7] Let $A\\geq0$ , $a\\geq0$ , $b\\geq0$ and $c>0$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\na\\ln(b A+1)-c A\\leq a\\ln\\left({\\frac{a b}{c}}+1\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "From Lemma 9, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{16d}{\\beta}\\ln\\left(\\frac{\\beta H^{2}}{2\\delta}\\sum_{t=1}^{T}\\|{\\bf x}_{t}-{\\bf x}_{t-1}\\|_{2}^{2}+1\\right)-\\frac{\\delta}{4}\\sum_{t=1}^{T}\\|{\\bf x}_{t}-{\\bf x}_{t-1}\\|_{2}^{2}\\le\\frac{16d}{\\beta}\\ln\\left(32d H^{2}+1\\right),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where we set $\\delta=1$ in the last step. Combining (43) and (45), we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf{x}_{t})+\\nabla r(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}^{*}\\rangle-\\frac{\\beta}{2}\\sum_{t=1}^{T}\\|\\mathbf{x}^{*}-\\mathbf{x}_{t}\\|_{h_{t}}^{2}}\\\\ {\\displaystyle\\leq\\left(\\frac{1}{2}+\\frac{\\beta}{4}G^{2}\\right)D^{2}+\\frac{16d}{\\beta}\\ln\\left(32d H^{2}+1\\right)+\\frac{16d}{\\beta}\\ln\\left(1+\\frac{\\beta}{8}\\left[G^{2}+8\\displaystyle\\sum_{t=1}^{T}\\|\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla F_{t}(\\mathbf{x}_{t})\\|_{2}^{2}\\right]\\right)}\\\\ {\\displaystyle\\ +\\ 4\\sum_{t=2}^{T}\\|\\nabla F_{t}(\\mathbf{x}_{t-1})-\\nabla F_{t-1}(\\mathbf{x}_{t-1})|_{2}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Taking the expectation over both sides delivers ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf{x}_{t})+\\nabla r(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}^{*}\\rangle-\\frac{\\beta}{2}\\displaystyle\\sum_{t=1}^{T}\\|\\mathbf{x}^{*}-\\mathbf{x}_{t}\\|_{h_{t}}^{2}\\right]}\\\\ &{\\leq\\left(\\frac{1}{2}+\\frac{\\beta}{4}G^{2}\\right)D^{2}+\\frac{16d}{\\beta}\\ln\\left(32d H^{2}+1\\right)+\\frac{16d}{\\beta}\\ln\\left(1+\\frac{\\beta}{8}G^{2}+\\beta\\sigma_{1:T}^{2}+\\frac{\\beta}{2}\\Sigma_{1:T}^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the last step is due to the Jensen\u2019s inequality. Finally, we complete the proof by substitute the above result into (38). ", "page_idx": 23}, {"type": "text", "text": "B.6 Proof of Theorem 6 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this part, we analyze three types of functions separately. ", "page_idx": 23}, {"type": "text", "text": "B.6.1 For convex functions ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "First, since the expected function $F_{t}(\\cdot)$ is convex in each round $t$ , the instantaneous dynamic regret could be upper bounded as below: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[\\left[f_{t}(\\mathbf{x}_{t})+r(\\mathbf{x}_{t})\\right]-\\left[f_{t}(\\mathbf{x}^{*})+r(\\mathbf{x}^{*})\\right]\\right]=\\mathbb{E}\\left[\\left[F_{t}(\\mathbf{x}_{t})+r(\\mathbf{x}_{t})\\right]-\\left[F_{t}(\\mathbf{x}^{*})+r(\\mathbf{x}^{*})\\right]\\right]}\\\\ &{\\le\\!\\mathbb{E}\\left[\\left\\langle\\nabla F_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}^{*}\\right\\rangle+r(\\mathbf{x}_{t})-r(\\mathbf{x}^{*})\\right]=\\mathbb{E}\\left[\\left\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}^{*}\\right\\rangle+r(\\mathbf{x}_{t})-r(\\mathbf{x}^{*})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then, we decompose the regret into meta-regret and expert-regret: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\mathbf{Regret}_{T}\\right]\\leq\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}^{*}\\rangle+r(\\mathbf{x}_{t})-r(\\mathbf{x}^{*})\\right]}\\\\ &{=\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}_{t}^{k,i}\\rangle+r(\\mathbf{x}_{t})-r(\\mathbf{x}_{t}^{k,i})\\right]+\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}^{k,i}-\\mathbf{x}^{*}\\rangle+r(\\mathbf{x}_{t}^{k,i})-r(\\mathbf{x}^{*})\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Expert-regret analysis. According to the above decomposition, we define the surrogate loss functions $\\bar{h}_{t}^{c}(\\mathbf{x})=\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}\\rangle+r\\bar{(\\mathbf{x})}$ for the $i$ -th expert. According to (24), we can bound the expert-regret by ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{expert-regret}\\le\\mathbb{E}\\left[\\frac{3D}{2}\\sqrt{1+\\bar{V}_{T}}+\\frac{D\\delta}{2}-\\frac{\\delta}{4D}\\displaystyle\\sum_{t=2}^{T}\\|\\mathbf{x}_{t}^{k,i}-\\mathbf{x}_{t-1}^{k,i}\\|_{2}^{2}+4D G^{2}\\right]}\\\\ &{\\qquad\\qquad\\le\\displaystyle\\frac{3D}{2}(1+G)+4D G^{2}+12D\\sqrt{\\sigma_{1:T}^{2}}+6D\\sqrt{\\Sigma_{1:T}^{2}}}\\\\ &{\\qquad\\qquad\\qquad+10H D\\displaystyle\\sum_{t=2}^{T}\\|\\mathbf{x}_{t}-\\mathbf{x}_{t-1}\\|_{2}^{2}-\\frac{\\delta}{4D}\\displaystyle\\sum_{t=2}^{T}\\|\\mathbf{x}_{t}^{k,i}-\\mathbf{x}_{t-1}^{k,i}\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the last step is due to Lemma 3. ", "page_idx": 24}, {"type": "text", "text": "Meta-regret analysis. We first consider the instantaneous meta-regret: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\langle\\nabla f_{i}(x_{i}),x_{i}-x_{i}^{k_{l}}\\rangle+r(\\mathbf{x}_{i})-r(\\mathbf{x}_{i}^{k_{l}})}\\\\ &{=\\langle\\nabla f_{i}(x_{i}),x_{i}-x_{i}^{k_{l}}\\rangle+r(\\mathbf{x}_{i})-r(\\mathbf{x}_{i}^{k_{l}})+\\langle\\nabla f_{i}(x_{i}),x_{i}^{k_{l}}-x_{i}^{k_{l}}\\rangle+r(\\mathbf{x}_{i}^{k_{l}})-r(\\mathbf{x}_{i}^{k_{l}})}\\\\ &{\\le\\langle\\nabla f_{i}(x_{i}),x_{i}-x_{i}^{k_{l}}\\rangle+\\displaystyle\\sum_{k=1}^{K}\\phi_{i}^{k_{l}}(\\mathbf{x}_{i}^{k_{l}})-r(\\mathbf{x}_{i}^{k_{l}})+\\langle\\nabla f_{i}(x_{i}),x_{i}^{k_{l}}-x_{i}^{k_{l}}\\rangle+\\displaystyle\\sum_{l=1}^{K}p_{i}^{k_{l}+}r(\\mathbf{x}_{i}^{k_{l}})-r(\\mathbf{x}_{i}^{k_{l}})}\\\\ &{\\le\\displaystyle\\sum_{k=1}^{K}\\phi_{i}^{k_{l}}-r_{i}^{k_{l}}+\\displaystyle\\sum_{i=1}^{K}p_{i}^{k_{l}+}\\varepsilon_{i}^{k_{l}}-r_{i}^{k_{l}}-r_{i}^{k_{l}}\\displaystyle\\sum_{k=1}^{K}\\phi_{i}^{k_{l}}\\mathbf{x}_{i}^{k_{l}}-\\mathbf{x}_{i-1}^{k_{l}}\\|_{2}^{2}+\\gamma_{1}\\|\\mathbf{x}_{i}^{k_{l}}-\\mathbf{x}_{i-1}^{k_{l}}\\|_{2}^{2}}\\\\ &{\\quad-\\gamma\\displaystyle\\sum_{i=1}^{N}\\gamma_{k}^{k_{l}}\\|\\mathbf{x}_{i}^{k_{l}}-\\mathbf{x}_{i-1}^{k_{l}}\\|_{2}^{2}+\\gamma_{1}\\|\\mathbf{x}_{i}^{k_{l}}-\\mathbf{x}_{i-1}^{k_{l}}\\|_{2}^{2}}\\\\ &{=\\langle q_{i},\\mathbf{e}_{i \n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "(46) ", "page_idx": 24}, {"type": "text", "text": "awnhde $\\begin{array}{r}{r(\\mathbf{x}_{t}^{k})\\;=\\;r(\\sum_{i=1}^{N}p_{t}^{k,i}\\mathbf{x}_{t}^{k,i})\\;\\le\\;\\sum_{i=1}^{N}p_{t}^{k,i}r(\\mathbf{x}_{t}^{k,i})}\\end{array}$ ,e .,a $\\begin{array}{r}{r(\\mathbf{x}_{t})=r(\\sum_{k=1}^{K}q_{t}^{k}\\mathbf{x}_{t}^{k})\\le\\sum_{k=1}^{K}q_{t}^{k}r(\\mathbf{x}_{t}^{k})}\\end{array}$ of $l_{t}^{k}$ and $l_{t}^{k,i}$ in (13) and (14). For brevity, we denote $\\pmb{q}_{t}\\;\\triangleq\\;\\big(q_{t}^{1},\\cdot\\cdot\\cdot,q_{t}^{K}\\big),\\:l_{t}\\;\\triangleq\\;\\big(l_{t}^{1},\\cdot\\cdot\\cdot,l_{t}^{K}\\big)$ , ", "page_idx": 24}, {"type": "text", "text": "$\\pmb{p}_{t}^{k}\\triangleq(q_{t}^{k,1},\\cdots,q_{t}^{k,N})$ and $l_{t}^{k}\\triangleq(l_{t}^{k,1},\\cdot\\cdot\\cdot\\cdot,l_{t}^{k,N})$ . By the above results, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}_{t}^{k,i}\\rangle+r(\\mathbf{x}_{t})-r(\\mathbf{x}_{t}^{k,i})}\\\\ &{\\displaystyle\\leq\\sum_{t=1}^{T}\\langle q_{t}-\\mathbf{e}_{k},l_{t}\\rangle+\\left\\langle p_{t}^{k}-\\mathbf{e}_{i},l_{t}^{k}\\right\\rangle-\\gamma_{1}\\displaystyle\\sum_{t=1}^{T}\\sum_{k=1}^{K}q_{t}^{k}\\|\\mathbf{x}_{t}^{k}-\\mathbf{x}_{t-1}^{k}\\|_{2}^{2}+\\gamma_{1}\\displaystyle\\sum_{t=1}^{T}\\|\\mathbf{x}_{t}^{k}-\\mathbf{x}_{t-1}^{k}\\|_{2}^{2}}\\\\ &{\\displaystyle\\ \\ -\\gamma_{2}\\sum_{t=1}^{T}\\sum_{i=1}^{N}p_{t}^{k,i}\\|\\mathbf{x}_{t}^{k,i}-\\mathbf{x}_{t-1}^{k,i}\\|_{2}^{2}+\\gamma_{2}\\displaystyle\\sum_{t=1}^{T}\\|\\mathbf{x}_{t}^{k,i}-\\mathbf{x}_{t-1}^{k,i}\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then, we introduce the following lemma, which is similar to Yan et al. [2023, Lemma 3] but with a different composite losses and optimisms in (13) and (14). ", "page_idx": 25}, {"type": "text", "text": "Lemma 10. Let $l_{t}^{k}$ , $m_{t}^{k}$ be defined in (13), and $l_{t}^{k,i}$ , $m_{t}^{k,i}$ be defined in (14). Then, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{t=1}^{T}\\langle q_{t}-\\mathbf{e}_{k},l_{t}\\rangle+\\left\\langle p_{t}^{k}-\\mathbf{e}_{i},l_{t}^{k}\\right\\rangle\\leq\\frac{1}{\\eta^{k}}\\ln\\frac{N}{3(C_{0}\\eta^{k})^{2}}+32\\eta^{k}\\sum_{t=1}^{T}\\left(l_{t}^{k,i}-m_{t}^{k,i}\\right)^{2}}}\\\\ &{}&{\\mathrm-\\ \\frac{C_{0}}{2}\\sum_{t=2}^{T}\\|q_{t}-q_{t-1}\\|_{1}^{2}-\\frac{C_{0}}{16}\\sum_{t=2}^{T}\\|p_{t}^{k}-p_{t-1}^{k}\\|_{1}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Note that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\left(l_{t}^{k,i}-m_{t}^{k,i}\\right)^{2}=\\sum_{t=1}^{T}\\left(\\left\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}_{t}^{k,i}\\right\\rangle-\\left\\langle\\nabla f_{t-1}(\\mathbf{x}_{t-1}),\\mathbf{x}_{t-1}-\\mathbf{x}_{t-1}^{k,i}\\right\\rangle\\right)^{2}}\\\\ &{\\displaystyle\\leq2\\sum_{t=1}^{T}\\left\\langle\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla f_{t-1}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}_{t}^{k,i}\\right\\rangle^{2}+2\\sum_{t=1}^{T}\\left\\langle\\nabla f_{t-1}(\\mathbf{x}_{t-1}),\\mathbf{x}_{t}-\\mathbf{x}_{t}^{k,i}-\\mathbf{x}_{t-1}+\\mathbf{x}_{t-1}^{k,i}\\right\\rangle^{2}}\\\\ &{\\displaystyle\\leq2D^{2}\\bar{V}_{T}+4G^{2}\\sum_{t=1}^{T}\\|\\mathbf{x}_{t}-\\mathbf{x}_{t-1}\\|_{2}^{2}+4G^{2}\\displaystyle\\sum_{t=1}^{T}\\|\\mathbf{x}_{t}^{k,i}-\\mathbf{x}_{t-1}^{k,i}\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the second and third steps are due to the fact that $(a+b)^{2}\\,\\leq\\,2a^{2}+2b^{2}$ for any $a,b\\in\\mathbb{R}$ . Taking the expectation on the both sides, we obtain ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\left(l_{t}^{k,i}-m_{t}^{k,i}\\right)^{2}\\right]\\leq2D^{2}G^{2}+8D^{2}(2\\sigma_{1:T}^{2}+\\Sigma_{1:T}^{2})+(8D^{2}H^{2}+4G^{2})\\displaystyle\\sum_{t=2}^{T}\\|\\mathbf{x}_{t}-\\mathbf{x}_{t-1}\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\displaystyle4G^{2}\\displaystyle\\sum_{t=1}^{T}\\|\\mathbf{x}_{t}^{k,i}-\\mathbf{x}_{t-1}^{k,i}\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the inequality is due to Lemma 3. Therefore, combining the above results with (47), Lemma 10 and Lemma 3 delivers ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{~\\mathop{meta-regret}}=\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}_{t}^{k,i}\\rangle+r(\\mathbf{x}_{t})-r(\\mathbf{x}_{t}^{k,i})\\right]}\\\\ &{\\le\\displaystyle\\frac{1}{\\eta^{k}}\\ln\\frac{N}{3(C_{0}\\eta^{k})^{2}}+256\\eta^{k}D^{2}(2\\sigma_{1:T}^{2}+\\Sigma_{1:T}^{2})+\\frac{64}{C_{0}}\\left(2D^{2}H^{2}+G^{2}-\\frac{C_{0}}{64}\\gamma_{1}\\right)\\displaystyle\\sum_{t=2}^{T}\\|\\mathbf{x}_{t}-\\mathbf{x}_{t-1}\\|_{2}^{2}}\\\\ &{\\quad+\\left(\\frac{64G^{2}}{C_{0}}-\\gamma_{2}\\right)\\displaystyle\\sum_{t=1}^{T}\\|\\mathbf{x}_{t}^{k,i}-\\mathbf{x}_{t-1}^{k,i}\\|_{2}^{2}-\\displaystyle\\frac{C_{0}}{2}\\sum_{t=2}^{T}\\|q_{t}-q_{t-1}\\|_{1}^{2}-\\displaystyle\\frac{C_{0}}{16}\\displaystyle\\sum_{t=2}^{T}\\|p_{t}^{k}-p_{t-1}^{k}\\|_{1}^{2}+\\frac{32D^{2}G}{C_{0}}}\\\\ &{\\quad-\\gamma_{1}\\displaystyle\\sum_{t=2}^{T}\\sum_{k=1}^{K}q_{t}^{k}\\|\\mathbf{x}_{t}^{k}-\\mathbf{x}_{t-1}^{k}\\|_{2}^{2}-\\gamma_{2}\\displaystyle\\sum_{t=2}^{T}\\sum_{i=2}^{N}p_{t}^{k,i}\\|\\mathbf{x}_{t}^{k,i}-\\mathbf{x}_{t-1}^{k,i}\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "To bound the first two terms, we exploit the following lemma. ", "page_idx": 25}, {"type": "text", "text": "Lemma 11. [Yan et al., 2023, Lemma 7] For the step size pool $\\mathcal{H}=\\{\\eta_{1},\\cdot\\cdot\\cdot,\\eta_{K}\\}$ with $\\begin{array}{r}{\\eta_{1}=\\frac{1}{2C_{0}}\\geq}\\end{array}$ $\\begin{array}{r}{\\cdot\\cdot\\cdot\\geq\\eta_{T}=\\frac{1}{2C_{0}T}}\\end{array}$ , i $f C_{0}\\geq{\\frac{\\sqrt{X}}{2T}}$ , there exists $\\eta^{k}\\in\\mathcal{H}$ satisfying ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\frac{1}{\\eta}\\ln\\frac{Y}{\\eta^{2}}+\\eta X\\leq2C_{0}\\ln\\left(4Y C_{0}^{2}\\right)+4\\sqrt{X\\ln(4X Y)}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Therefore, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\cfrac{1}{\\eta^{k}}\\ln\\cfrac{N}{3(C_{0}\\eta^{k})^{2}}+256\\eta^{k}D^{2}(2\\sigma_{1:T}^{2}+\\Sigma_{1:T}^{2})}\\\\ &{\\leq\\!2C_{0}\\ln(2N)+64D\\sqrt{(2\\sigma_{1:T}^{2}+\\Sigma_{1:T}^{2})\\ln\\left(\\cfrac{2^{10}D^{2}N}{C_{0}^{2}}(2\\sigma_{1:T}^{2}+\\Sigma_{1:T}^{2})\\right)}}\\\\ &{\\leq\\!2C_{0}\\ln(2N)+64D\\sqrt{\\ln\\left(\\cfrac{2^{10}D^{2}N}{C_{0}^{2}}(2\\sigma_{1:T}^{2}+\\Sigma_{1:T}^{2})\\right)}\\left(\\sqrt{2\\sigma_{1:T}^{2}}+\\sqrt{\\Sigma_{1:T}^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Hence, by requiring $C_{0}\\geq1$ , the meta-regret is bounded as: ", "page_idx": 26}, {"type": "text", "text": "meta-regret ", "text_level": 1, "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\leq\\tilde{\\mathcal{O}}\\left(\\sqrt{\\sigma_{1:T}^{2}}+\\sqrt{\\Sigma_{1:T}^{2}}\\right)+\\mathcal{O}(1)+64\\left(2D^{2}H^{2}+G^{2}-\\frac{1}{64}\\gamma_{1}\\right)\\sum_{t=2}^{T}\\|{\\bf x}_{t}-{\\bf x}_{t-1}\\|_{2}^{2}}}\\\\ {{\\displaystyle\\quad+\\left(64G^{2}-\\gamma_{2}\\right)\\sum_{t=1}^{T}\\|{\\bf x}_{t}^{k,i}-{\\bf x}_{t-1}^{k,i}\\|_{2}^{2}-\\frac{C_{0}}{2}\\sum_{t=2}^{T}\\|{\\bf q}_{t}-{\\bf q}_{t-1}\\|_{1}^{2}-\\frac{C_{0}}{16}\\sum_{t=2}^{T}\\|{\\bf p}_{t}^{k}-{\\bf p}_{t-1}^{k}\\|_{1}^{2}}}\\\\ {{\\displaystyle-\\,\\gamma_{1}\\sum_{t=2}^{T}\\sum_{k=1}^{K}q_{t}^{k}\\|{\\bf x}_{t}^{k}-{\\bf x}_{t-1}^{k}\\|_{2}^{2}-\\gamma_{2}\\sum_{t=2}^{T}\\sum_{i=2}^{N}p_{t}^{k,i}\\|{\\bf x}_{t}^{k,i}-{\\bf x}_{t-1}^{k,i}\\|_{2}^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Combining the expert-regret and meta-regret, we get ", "page_idx": 26}, {"type": "text", "text": "E [RegretT ] ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\sin\\left({\\sqrt[{\\sigma_{1:T}^{2}}}\\times\\operatorname{erf}_{T}\\right)}}\\\\ {{\\displaystyle\\leq\\tilde{\\mathcal{O}}\\left(\\sqrt{\\sigma_{1:T}^{2}}+\\sqrt{\\Sigma_{1:T}^{2}}\\right)+\\mathcal{O}(1)+\\left(128D^{2}H^{2}+64G^{2}+10D H-\\gamma_{1}\\right)\\sum_{t=2}^{T}\\left\\|{\\bf x}_{t}-{\\bf x}_{t-1}\\right\\|_{2}^{2}}}\\\\ {{\\displaystyle\\quad+\\left(64G^{2}-\\gamma_{2}-\\frac{\\delta}{4D}\\right)\\sum_{t=1}^{T}\\|{\\bf x}_{t}^{k,i}-{\\bf x}_{t-1}^{k,i}\\|_{2}^{2}-\\frac{C_{0}}{2}\\sum_{t=2}^{T}\\left\\|{\\bf q}_{t}-{\\bf q}_{t-1}\\right\\|_{1}^{2}-\\displaystyle\\frac{C_{0}}{16}\\sum_{t=2}^{T}\\|{\\bf p}_{t}^{k}-{\\bf p}_{t-1}^{k}\\|_{1}^{2}}}\\\\ {{\\displaystyle\\leq\\tilde{\\mathcal{O}}\\left(\\sqrt{\\sigma_{1:T}^{2}}+\\sqrt{\\Sigma_{1:T}^{2}}\\right)+\\mathcal{O}(1)+(2C_{1}-2\\gamma_{1})\\sum_{t=2}^{T}\\phi_{t}^{k}\\|{\\bf x}_{t}^{k}-{\\bf x}_{t-1}^{k}\\|_{2}^{2}}}\\\\ {{\\displaystyle\\quad+\\left(2D^{2}C_{1}-2D^{2}\\gamma_{1}-\\frac{C_{0}}{2}\\right)\\sum_{t=2}^{T}\\|{\\bf q}_{t}-{\\bf q}_{t-1}\\|_{1}^{2}+\\left(64G^{2}-\\gamma_{2}-\\frac{\\delta}{4D}\\right)\\sum_{t=1}^{T}\\|{\\bf x}_{t}^{k,i}-{\\bf x}_{t-1}^{k,i}\\|_{2}^{2},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $C_{1}=128D^{2}H^{2}+64G^{2}+10D H$ and the last step is due to the following lemma. ", "page_idx": 26}, {"type": "text", "text": "Lemma 12. Let $\\begin{array}{r}{{\\bf x}_{t}=\\sum_{k=1}^{K}q_{t}^{k}{\\bf x}_{t}^{k}}\\end{array}$ . Then, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{x}_{t}-\\mathbf{x}_{t-1}\\right\\|_{2}^{2}\\leq2\\sum_{k=1}^{K}q_{t}^{k}\\left\\|\\mathbf{x}_{t}^{k}-\\mathbf{x}_{t-1}^{k}\\right\\|_{2}^{2}+2D^{2}\\left\\|q_{t}-q_{t-1}\\right\\|_{1}^{2}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "By setting $C_{0}\\geq1,C_{0}\\geq8D^{2}C_{1},\\gamma_{1}\\geq C_{1}$ and $\\delta\\geq256G^{2}D+4D\\gamma_{2}$ , we finish the proof. ", "page_idx": 26}, {"type": "text", "text": "B.6.2 For strongly convex functions ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "According to (28), we could decompose the regret as below: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\left[\\mathrm{Regret}_{T}\\right]\\leq\\mathbb{E}\\left[\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}^{*}\\rangle+r(\\mathbf{x}_{t})-r(\\mathbf{x}^{*})-\\frac{\\lambda}{2}\\sum_{t=1}^{T}\\|\\mathbf{x}^{*}-\\mathbf{x}_{t}\\|_{2}^{2}\\right]}\\\\ {\\displaystyle\\leq\\mathbb{E}\\left[\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}^{*}\\rangle+r(\\mathbf{x}_{t})-r(\\mathbf{x}^{*})-\\frac{\\lambda^{i}}{2}\\sum_{t=1}^{T}\\|\\mathbf{x}^{*}-\\mathbf{x}_{t}\\|_{2}^{2}\\right]}\\\\ {\\displaystyle=\\mathbb{E}\\left[\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}_{t}^{k,i}\\rangle+r(\\mathbf{x}_{t})-r(\\mathbf{x}_{t}^{k,i})-\\frac{\\lambda^{i}}{2}\\sum_{t=1}^{T}\\|\\mathbf{x}_{t}^{k,i}-\\mathbf{x}_{t}\\|_{2}^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "equation", "text": "$$\n+\\mathbb{E}\\left[\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}^{k,i}-\\mathbf{x}^{*}\\rangle+r(\\mathbf{x}_{t}^{k,i})-r(\\mathbf{x}^{*})+\\frac{\\lambda^{i}}{2}\\sum_{t=1}^{T}\\|\\mathbf{x}_{t}^{k,i}-\\mathbf{x}_{t}\\|_{2}^{2}-\\frac{\\lambda^{i}}{2}\\sum_{t=1}^{T}\\|\\mathbf{x}^{*}-\\mathbf{x}_{t}\\|_{2}^{2}\\right]_{,}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the first step is due to the fact that there exists an $\\lambda^{i}\\in\\mathcal{H}$ satisfying $\\lambda^{i}\\le\\lambda\\le2\\lambda^{i}$ . ", "page_idx": 27}, {"type": "text", "text": "Expert-regret analysis. We define the surrogate loss functions $h_{t,i}^{s c}(\\mathbf{x})=\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}\\rangle+r(\\mathbf{x})+$ $\\textstyle{\\frac{\\lambda^{i}}{2}}\\sum_{t=1}^{T}\\|\\mathbf{x}-\\mathbf{x}_{t}\\|_{2}^{2}$ for the $i$ -th expert. According to Theorem 3, we can bound the expert-regret by expert-regret ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\leq\\mathcal{O}\\left(\\frac{1}{\\lambda}\\left(\\sigma_{\\operatorname*{max}}^{2}+\\Sigma_{\\operatorname*{max}}^{2}\\right)\\ln\\left(\\sigma_{1:T}^{2}+\\Sigma_{1:T}^{2}\\right)\\right)+16D^{2}\\left(H^{2}+1\\right)^{2}\\sum_{t=2}^{T}{\\left\\Vert\\mathbf{x}_{t}-\\mathbf{x}_{t-1}\\right\\Vert^{2}}}}\\\\ {{\\displaystyle+\\left(8D^{2}\\left(H^{2}+1\\right)-\\frac{\\delta}{8}\\right)\\sum_{t=2}^{T}{\\left\\Vert\\mathbf{x}_{t}^{k,i}-\\mathbf{x}_{t-1}^{k,i}\\right\\Vert^{2}}+\\frac{1}{4}\\delta D^{2}+\\frac{20G^{2}}{\\lambda}+\\frac{1}{\\lambda^{2}}+\\frac{\\lambda D^{2}}{4}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Meta-regret analysis. Similar to (46), the instantaneous meta-regret can be bounded by ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}_{t}^{k,i}\\rangle+r(\\mathbf{x}_{t})-r(\\mathbf{x}_{t}^{k,i})-\\frac{\\lambda^{i}}{2}\\|\\mathbf{x}_{t}^{k,i}-\\mathbf{x}_{t}\\|_{2}^{2}}\\\\ {\\displaystyle\\leq\\langle q_{t}-\\mathbf{e}_{k},l_{t}\\rangle+\\Big\\langle p_{t}^{k}-\\mathbf{e}_{i},l_{t}^{k}\\Big\\rangle-\\gamma_{1}\\displaystyle\\sum_{k=1}^{K}q_{t}^{k}\\|\\mathbf{x}_{t}^{k}-\\mathbf{x}_{t-1}^{k}\\|_{2}^{2}+\\gamma_{1}\\|\\mathbf{x}_{t}^{k}-\\mathbf{x}_{t-1}^{k}\\|_{2}^{2}}\\\\ {\\displaystyle\\ \\ -\\,\\gamma_{2}\\sum_{i=1}^{N}p_{t}^{k,i}\\|\\mathbf{x}_{t}^{k,i}-\\mathbf{x}_{t-1}^{k,i}\\|_{2}^{2}+\\gamma_{2}\\|\\mathbf{x}_{t}^{k,i}-\\mathbf{x}_{t-1}^{k,i}\\|_{2}^{2}-\\frac{\\lambda^{i}}{2}\\|\\mathbf{x}_{t}^{k,i}-\\mathbf{x}_{t}\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "For the first two terms, we exploit Lemma 10 and obtain ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{t=1}^{T}\\langle q_{t}-\\mathbf{e}_{k},l_{t}\\rangle+\\left\\langle p_{t}^{k}-\\mathbf{e}_{i},l_{t}^{k}\\right\\rangle\\leq\\frac{1}{\\eta^{k}}\\ln\\frac{N}{3(C_{0}\\eta^{k})^{2}}+32\\eta^{k}\\sum_{t=1}^{T}\\left(l_{t}^{k,i}-m_{t}^{k,i}\\right)^{2}}}\\\\ &{}&{\\mathrm-\\ \\frac{C_{0}}{2}\\sum_{t=2}^{T}\\|q_{t}-q_{t-1}\\|_{1}^{2}-\\frac{C_{0}}{16}\\sum_{t=2}^{T}\\|p_{t}^{k}-p_{t-1}^{k}\\|_{1}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Note that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\left(l_{t}^{k,i}-m_{t}^{k,i}\\right)^{2}=\\displaystyle\\sum_{t=1}^{T}\\left(\\left\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}_{t}^{k,i}\\right\\rangle-\\left\\langle\\nabla f_{t-1}(\\mathbf{x}_{t-1}),\\mathbf{x}_{t-1}-\\mathbf{x}_{t-1}^{k,i}\\right\\rangle\\right)^{2}}\\\\ &{\\displaystyle\\leq2\\displaystyle\\sum_{t=1}^{T}\\left\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}_{t}^{k,i}\\right\\rangle^{2}+2\\displaystyle\\sum_{t=1}^{T}\\left\\langle\\nabla f_{t-1}(\\mathbf{x}_{t-1}),\\mathbf{x}_{t-1}-\\mathbf{x}_{t-1}^{k,i}\\right\\rangle^{2}}\\\\ &{\\displaystyle\\leq4\\displaystyle\\sum_{t=1}^{T}\\left\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}_{t}^{k,i}\\right\\rangle^{2}+2G^{2}D^{2}\\leq4G^{2}\\displaystyle\\sum_{t=1}^{T}\\left\\|\\mathbf{x}_{t}-\\mathbf{x}_{t}^{k,i}\\right\\|^{2}+2G^{2}D^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the first step is due to the fact that $(a+b)^{2}\\leq2a^{2}+2b^{2}$ for any $a,b\\in\\mathbb{R}$ . Substituting (50) and (51) into (49) obtains ", "page_idx": 28}, {"type": "text", "text": "meta-regret ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\le\\displaystyle\\frac{1}{\\eta\\xi}\\ln\\frac{N}{3(C(\\eta)^{k/2}}+\\left(128\\eta^{k}G^{2}-\\frac{\\lambda^{\\prime}}{2}\\right)\\frac{T}{\\sum_{i=1}^{k}}\\Big\\lVert\\mathbf{x}_{i}-\\mathbf{x}_{i}^{k,i}\\Big\\rVert^{2}-\\frac{C_{0}}{2}\\frac{T}{\\sum_{i=2}^{k}}\\Big\\lVert q_{i}-q_{i-1}\\Big\\rVert_{1}^{2}}\\\\ &{\\quad-\\displaystyle\\frac{C_{0}}{16}\\sum_{i=1}^{k}\\|p_{i}^{k}-p_{i-1}^{k}\\|_{1}^{2}-\\prod_{i=1}^{K}\\frac{q_{i}^{k}}{16}\\big\\lVert\\mathbf{x}_{i}^{k}-\\mathbf{x}_{i-1}^{k}\\|_{2}^{2}+\\gamma_{1}\\|\\mathbf{x}_{i}^{k}-\\mathbf{x}_{i-1}^{k}\\|_{2}^{2}}\\\\ &{\\quad-\\displaystyle\\gamma_{2}\\sum_{i=1}^{N}\\mu_{i}^{k,i}\\|\\mathbf{x}_{i}^{k,i}-\\mathbf{x}_{i-1}^{k,i}\\|_{2}^{2}+\\gamma_{1}\\|\\mathbf{x}_{i}^{k,i}-\\mathbf{x}_{i-1}^{k,i}\\|_{2}^{2}+64\\eta^{k}G^{2}D^{2}}\\\\ &{\\le\\displaystyle\\frac{1}{\\eta^{k}}\\ln\\frac{N}{(\\eta)^{2}}+\\left(128\\eta^{k}G^{2}-\\frac{\\lambda^{\\prime}}{2}\\right)\\sum_{i=1}^{T}\\Big\\lVert\\mathbf{x}_{i}-\\mathbf{x}_{i}^{k,i}\\Big\\rVert^{2}-\\displaystyle\\frac{C_{0}}{2}\\sum_{i=2}^{T}\\Big\\lVert q_{i}-q_{i-1}\\Big\\rVert_{1}^{2}}\\\\ &{\\quad+\\left(2D^{2}\\gamma_{-1}-\\frac{C_{0}}{16}\\right)\\sum_{i=2}^{T}\\|\\mathbf{p}_{i}^{k}-p_{i-1}^{k}\\|_{1}^{2}-\\gamma_{1}\\displaystyle\\sum_{i=1}^{K}q_{i}^{k}\\|\\mathbf{x}_{i}^{k}-\\mathbf{x}_{i-1}^{k,i}\\|_{2}^ \n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the last step is due to Lemma 12 and $C_{0}\\geq1$ . To bound the first term in the above result, we employ the following lemma. ", "page_idx": 28}, {"type": "text", "text": "Lemma 13. [Yan et al., 2023, Lemma $8J$ For the step size pool $\\mathcal{H}=\\{\\eta_{1},\\cdot\\cdot\\cdot,\\eta_{K}\\}$ with $\\begin{array}{r}{\\eta_{1}=\\frac{1}{2C_{0}}\\geq}\\end{array}$ $\\begin{array}{r}{\\cdots\\geq\\eta_{T}=\\frac{1}{2C_{0}T}}\\end{array}$ , $\\begin{array}{r}{i f C_{0}\\geq\\frac{1}{2\\eta^{\\ast}T}}\\end{array}$ where $\\eta^{*}$ is the optimal step size, there exists $\\eta^{k}\\in\\mathcal{H}$ satisfying ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{1}{\\eta}\\ln\\frac{Y}{\\eta^{2}}\\leq2C_{0}\\ln\\left(4Y C_{0}^{2}\\right)+\\frac{1}{\\eta^{*}}\\ln\\frac{4Y}{\\left(\\eta^{*}\\right)^{2}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Applying the above lemma with $\\begin{array}{r}{\\eta^{*}=\\frac{\\lambda^{i}}{256G^{2}}}\\end{array}$ delivers ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathrm{et}\\leq2C_{0}\\ln\\left(4N C_{0}^{2}\\right)+\\frac{512G^{2}}{\\lambda}\\ln\\frac{2048N G^{2}}{\\lambda^{2}}-\\frac{C_{0}}{2}\\sum_{t=2}^{T}\\|q_{t}-q_{t-1}\\|_{1}^{2}}\\\\ {\\displaystyle\\quad+\\left(2D^{2}\\gamma_{1}-\\frac{C_{0}}{16}\\right)\\sum_{t=2}^{T}\\|p_{t}^{k}-p_{t-1}^{k}\\|_{1}^{2}-\\gamma_{1}\\sum_{k=1}^{K}q_{t}^{k}\\|\\mathbf{x}_{t}^{k}-\\mathbf{x}_{t-1}^{k}\\|_{2}^{2}}\\\\ {\\displaystyle\\quad+\\left(2\\gamma_{1}-\\gamma_{2}\\right)\\sum_{i=1}^{N}p_{t}^{k,i}\\|\\mathbf{x}_{t}^{k,i}-\\mathbf{x}_{t-1}^{k,i}\\|_{2}^{2}+\\gamma_{2}\\|\\mathbf{x}_{t}^{k,i}-\\mathbf{x}_{t-1}^{k,i}\\|_{2}^{2}+\\frac{\\lambda D^{2}}{4}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Then, we combine the expert-regret and meta-regret: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\left[\\mathrm{Regret}_{T}\\right]\\leq\\mathcal{O}\\left(\\frac{1}{\\lambda}\\left(\\sigma_{\\operatorname*{max}}^{2}+\\Sigma_{\\operatorname*{max}}^{2}\\right)\\ln\\left(\\sigma_{1:T}^{2}+\\Sigma_{1:T}^{2}\\right)\\right)}\\\\ {\\displaystyle+\\left(8D^{2}\\left(H^{2}+1\\right)-\\frac{\\delta}{8}+\\gamma_{2}\\right)\\sum_{t=2}^{T}\\left\\Vert\\mathbf{x}_{t}^{k,i}-\\mathbf{x}_{t-1}^{k,i}\\right\\Vert^{2}+\\left(2D^{2}C_{2}-\\frac{C_{0}}{2}\\right)\\sum_{t=2}^{T}\\left\\Vert q_{t}-q_{t-1}\\right\\Vert_{1}^{2}}\\\\ {\\displaystyle+\\left(2D^{2}\\gamma_{1}-\\frac{C_{0}}{16}\\right)\\sum_{t=2}^{T}\\left\\Vert p_{t}^{k}-p_{t-1}^{k}\\right\\Vert_{1}^{2}+\\left(2C_{2}-\\gamma_{1}\\right)\\sum_{k=1}^{K}q_{t}^{k}\\left\\Vert\\mathbf{x}_{t}^{k}-\\mathbf{x}_{t-1}^{k}\\right\\Vert_{2}^{2}}\\\\ {\\displaystyle+\\left(2\\gamma_{1}-\\gamma_{2}\\right)\\sum_{i=1}^{N}p_{t}^{k,i}\\left\\Vert\\mathbf{x}_{t}^{k,i}-\\mathbf{x}_{t-1}^{k,i}\\right\\Vert_{2}^{2}+\\mathcal{O}(1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $C_{2}=16D^{2}\\left(H^{2}+1\\right)^{2}$ . Setting $C_{0}\\geq4D^{2}C_{2}$ , $C_{0}\\geq32D^{2}\\gamma_{1}$ , $\\delta\\,\\geq\\,64D^{2}(H^{2}+1)+8\\gamma_{2}$ , $\\gamma_{1}\\geq2C_{2}$ and $\\gamma_{2}\\geq\\gamma_{1}$ completes the proof. ", "page_idx": 28}, {"type": "text", "text": "B.6.3 For exp-concave functions ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "According to (38), we could decompose the regret as below: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\left[\\mathrm{Regret}_{T}\\right]\\leq\\mathbb{E}\\left[\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}^{*}\\rangle+r(\\mathbf{x}_{t})-r(\\mathbf{x}^{*})-\\frac{\\beta}{2}\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}^{*}-\\mathbf{x}_{t}\\rangle^{2}\\right]}\\\\ {\\displaystyle\\leq\\mathbb{E}\\left[\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}^{*}\\rangle+r(\\mathbf{x}_{t})-r(\\mathbf{x}^{*})-\\frac{\\beta^{i}}{2}\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}^{*}-\\mathbf{x}_{t}\\rangle^{2}\\right]}\\\\ {\\displaystyle=\\mathbb{E}\\left[\\sum_{t=1}^{T}\\langle\\mathbf{g}_{t},\\mathbf{x}_{t}-\\mathbf{x}_{t}^{k,i}\\rangle+r(\\mathbf{x}_{t})-r(\\mathbf{x}_{t}^{k,i})-\\frac{\\beta^{i}}{2}\\sum_{t=1}^{T}\\left\\langle\\mathbf{g}_{t},\\mathbf{x}_{t}^{k,i}-\\mathbf{x}_{t}\\right\\rangle^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n+\\mathbb{E}\\left[\\sum_{t=1}^{T}\\langle\\mathbf{g}_{t},\\mathbf{x}_{t}^{k,i}-\\mathbf{x}^{*}\\rangle+r(\\mathbf{x}_{t}^{k,i})-r(\\mathbf{x}^{*})+\\frac{\\beta^{i}}{2}\\sum_{t=1}^{T}\\left\\langle\\mathbf{g}_{t},\\mathbf{x}_{t}^{k,i}-\\mathbf{x}_{t}\\right\\rangle^{2}-\\frac{\\beta^{i}}{2}\\sum_{t=1}^{T}\\left\\langle\\mathbf{g}_{t},\\mathbf{x}^{*}-\\mathbf{x}_{t}\\right\\rangle^{2}\\right],\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\mathbf{g}_{t}=\\nabla f_{t}(\\mathbf{x}_{t})$ , $\\begin{array}{r}{\\beta^{i}=\\frac{1}{2}\\operatorname*{min}\\lbrace\\frac{1}{4G D},\\alpha^{i}\\rbrace}\\end{array}$ , and the first step is due to the fact that there exists an $\\alpha^{i}\\in\\mathcal{H}$ satisfying $\\alpha^{i}\\le\\alpha\\le2\\alpha^{i}$ . ", "page_idx": 29}, {"type": "text", "text": "Expert-regret analysis. We define the surrogate loss functions $h_{t,i}^{e x p}({\\bf x})\\;=\\;\\langle{\\bf g}_{t},{\\bf x}\\rangle\\,+\\,r({\\bf x})\\;+$ $\\frac{\\beta^{i}}{2}\\left\\langle\\mathbf{g}_{t},\\mathbf{x}-\\mathbf{x}_{t}\\right\\rangle^{2}$ for the $i$ -th expert. According to Theorem 5, we can bound the expert-regret by expert-regret ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\mathcal{O}\\left(\\displaystyle\\frac{d}{\\alpha}\\ln\\left(\\sigma_{1:T}^{2}+\\Sigma_{1:T}^{2}\\right)\\right)+\\frac{24d}{\\beta}\\ln\\left(1+\\frac{C_{3}}{\\delta}\\sum_{t=2}^{T}\\|\\mathbf{x}_{t}-\\mathbf{x}_{t-1}\\|_{2}^{2}\\right)}\\\\ &{\\quad+\\frac{24d}{\\beta}\\ln\\left(1+\\frac{\\beta G^{4}}{\\delta}\\sum_{t=2}^{T}\\|\\mathbf{x}_{t}^{k,i}-\\mathbf{x}_{t-1}^{k,i}\\|_{2}^{2}\\right)-\\frac{\\delta}{4}\\sum_{t=2}^{T}\\|\\mathbf{x}_{t}^{k,i}-\\mathbf{x}_{t-1}^{k,i}\\|_{2}^{2}+\\left(\\displaystyle\\frac{\\delta}{2}+\\frac{\\beta}{4}G^{2}\\right)D^{2}}\\\\ &{\\leq\\mathcal{O}\\left(\\displaystyle\\frac{d}{\\alpha}\\ln\\left(\\sigma_{1:T}^{2}+\\Sigma_{1:T}^{2}\\right)\\right)+\\frac{24d C_{3}}{\\delta}\\sum_{t=2}^{T}\\|\\mathbf{x}_{t}-\\mathbf{x}_{t-1}\\|_{2}^{2}+\\left(\\frac{24d G^{4}}{\\delta}-\\frac{\\delta}{4}\\right)\\sum_{t=2}^{T}\\|\\mathbf{x}_{t}^{k,i}-\\mathbf{x}_{t-1}^{k,i}\\|_{2}^{2}}\\\\ &{\\quad+\\left(\\displaystyle\\frac{\\delta}{2}+\\frac{\\beta}{4}G^{2}\\right)D^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $C_{3}=8H^{2}+64D^{2}G^{2}H^{2}+8G^{4}$ . ", "page_idx": 29}, {"type": "text", "text": "Meta-regret analysis. Similar to (46), the instantaneous meta-regret can be bounded by ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}_{t}^{k,i}\\rangle+r(\\mathbf{x}_{t})-r(\\mathbf{x}_{t}^{k,i})-\\displaystyle\\frac{\\beta^{i}}{2}\\left\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}^{k,i}-\\mathbf{x}_{t}\\right\\rangle^{2}}\\\\ &{\\leq\\langle q_{t}-\\mathbf{e}_{k},l_{t}\\rangle+\\left\\langle p_{t}^{k}-\\mathbf{e}_{i},l_{t}^{k}\\right\\rangle-\\gamma_{1}\\displaystyle\\sum_{k=1}^{K}q_{t}^{k}\\|\\mathbf{x}_{t}^{k}-\\mathbf{x}_{t-1}^{k}\\|_{2}^{2}+\\gamma_{1}\\|\\mathbf{x}_{t}^{k}-\\mathbf{x}_{t-1}^{k}\\|_{2}^{2}}\\\\ &{\\quad-\\gamma_{2}\\displaystyle\\sum_{i=1}^{N}p_{t}^{k,i}\\|\\mathbf{x}_{t}^{k,i}-\\mathbf{x}_{t-1}^{k,i}\\|_{2}^{2}+\\gamma_{2}\\|\\mathbf{x}_{t}^{k,i}-\\mathbf{x}_{t-1}^{k,i}\\|_{2}^{2}-\\displaystyle\\frac{\\beta^{i}}{2}\\left\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}^{k,i}-\\mathbf{x}_{t}\\right\\rangle^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "For the first two terms, we exploit Lemma 10 and obtain ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{t=1}^{T}\\langle q_{t}-\\mathbf{e}_{k},l_{t}\\rangle+\\left\\langle p_{t}^{k}-\\mathbf{e}_{i},l_{t}^{k}\\right\\rangle\\leq\\frac{1}{\\eta^{k}}\\ln\\frac{N}{3(C_{0}\\eta^{k})^{2}}+32\\eta^{k}\\sum_{t=1}^{T}\\left(l_{t}^{k,i}-m_{t}^{k,i}\\right)^{2}}}\\\\ &{}&{\\mathrm-\\ \\frac{C_{0}}{2}\\sum_{t=2}^{T}\\|q_{t}-q_{t-1}\\|_{1}^{2}-\\frac{C_{0}}{16}\\sum_{t=2}^{T}\\|p_{t}^{k}-p_{t-1}^{k}\\|_{1}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Note that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\left(l_{t}^{k,i}-m_{t}^{k,i}\\right)^{2}=\\displaystyle\\sum_{t=1}^{T}\\left(\\left\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}_{t}^{k,i}\\right\\rangle-\\left\\langle\\nabla f_{t-1}(\\mathbf{x}_{t-1}),\\mathbf{x}_{t-1}-\\mathbf{x}_{t-1}^{k,i}\\right\\rangle\\right)^{2}}\\\\ &{\\displaystyle\\leq2\\displaystyle\\sum_{t=1}^{T}\\left\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}_{t}^{k,i}\\right\\rangle^{2}+2\\displaystyle\\sum_{t=1}^{T}\\left\\langle\\nabla f_{t-1}(\\mathbf{x}_{t-1}),\\mathbf{x}_{t-1}-\\mathbf{x}_{t-1}^{k,i}\\right\\rangle^{2}}\\\\ &{\\displaystyle\\leq4\\displaystyle\\sum_{t=1}^{T}\\left\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}_{t}^{k,i}\\right\\rangle^{2}+2G^{2}D^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Therefore, the meta-regret is bounded by ", "page_idx": 30}, {"type": "text", "text": "meta-regret ", "text_level": 1, "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\le\\displaystyle\\frac{1}{\\eta^{k}}\\ln\\frac{N}{3(C_{0}\\eta^{k})^{2}}+\\left(128\\eta^{k}-\\frac{\\beta^{i}}{2}\\right)\\sum_{t=1}^{T}\\left\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}_{t}^{k,i}\\right\\rangle^{2}+256G^{2}D^{2}\\eta^{k}}\\\\ &{\\quad-\\displaystyle\\frac{C_{0}}{2}\\sum_{t=2}^{T}\\|{q}_{t}-{q}_{t-1}\\|_{1}^{2}-\\frac{C_{0}}{16}\\sum_{t=2}^{T}\\|{p}_{t}^{k}-{p}_{t-1}^{k}\\|_{1}^{2}-\\gamma_{1}\\displaystyle\\sum_{k=1}^{K}q_{t}^{k}\\|{\\mathbf{x}}_{t}^{k}-{\\mathbf{x}}_{t-1}^{k}\\|_{2}^{2}+\\gamma_{1}\\|{\\mathbf{x}}_{t}^{k}-{\\mathbf{x}}_{t-1}^{k}\\|_{2}^{2}}\\\\ &{\\quad-\\gamma_{2}\\displaystyle\\sum_{i=1}^{N}p_{t}^{k,i}\\|{\\mathbf{x}}_{t}^{k,i}-{\\mathbf{x}}_{t-1}^{k,i}\\|_{2}^{2}+\\gamma_{2}\\|{\\mathbf{x}}_{t}^{k,i}-{\\mathbf{x}}_{t-1}^{k,i}\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Applying Lemma 13 with $\\begin{array}{r}{\\eta^{*}=\\frac{\\beta^{i}}{256}}\\end{array}$ delivers ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\operatorname{gret}\\le2C_{0}\\ln\\left(4N C_{0}^{2}\\right)+\\frac{512}{\\beta}\\ln\\frac{2048N}{\\beta^{2}}+G^{2}D^{2}\\beta-\\frac{C_{0}}{2}\\sum_{t=2}^{T}\\|{\\pmb q}_{t}-{\\pmb q}_{t-1}\\|_{1}^{2}}}\\\\ {{\\displaystyle\\qquad+\\left(2D^{2}\\gamma_{1}-\\frac{C_{0}}{16}\\right)\\sum_{t=2}^{T}\\|{\\pmb p}_{t}^{k}-{\\pmb p}_{t-1}^{k}\\|_{1}^{2}-\\gamma_{1}\\sum_{k=1}^{K}q_{t}^{k}\\|{\\pmb x}_{t}^{k}-{\\pmb x}_{t-1}^{k}\\|_{2}^{2}}}\\\\ {{\\displaystyle\\qquad+\\left(2\\gamma_{1}-\\gamma_{2}\\right)\\sum_{i=1}^{N}p_{t}^{k,i}\\|{\\pmb x}_{t}^{k,i}-{\\pmb x}_{t-1}^{k,i}\\|_{2}^{2}+\\gamma_{2}\\|{\\pmb x}_{t}^{k,i}-{\\pmb x}_{t-1}^{k,i}\\|_{2}^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Then, we combine the expert-regret and meta-regret: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\left[\\mathrm{Regret}_{T}\\right]\\leq\\mathcal{O}\\left(\\frac{d}{\\alpha}\\ln\\left(\\sigma_{1:T}^{2}+\\Sigma_{1:T}^{2}\\right)\\right)+\\left(\\frac{48d D C_{3}}{\\delta}-\\frac{C_{0}}{2}\\right)\\sum_{t=2}^{T}\\|\\boldsymbol{q}_{t}-\\boldsymbol{q}_{t-1}\\|_{1}^{2}}\\\\ {\\displaystyle+\\left(2D^{2}\\gamma_{1}-\\frac{C_{0}}{16}\\right)\\sum_{t=2}^{T}\\|\\boldsymbol{p}_{t}^{k}-\\boldsymbol{p}_{t-1}^{k}\\|_{1}^{2}+\\left(\\frac{48d C_{3}}{\\delta}-\\gamma_{1}\\right)\\sum_{k=1}^{K}q_{t}^{k}\\|\\mathbf{x}_{t}^{k}-\\mathbf{x}_{t-1}^{k}\\|_{2}^{2}}\\\\ {\\displaystyle+\\left(2\\gamma_{1}-\\gamma_{2}\\right)\\sum_{i=1}^{N}p_{t}^{k,i}\\|\\mathbf{x}_{t}^{k,i}-\\mathbf{x}_{t-1}^{k,i}\\|_{2}^{2}+\\left(\\frac{24d G^{4}}{\\delta}-\\frac{\\delta}{4}+\\gamma_{2}\\right)\\|\\mathbf{x}_{t}^{k,i}-\\mathbf{x}_{t-1}^{k,i}\\|_{2}^{2}+\\mathcal{O}(1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $C_{3}\\ =\\ 8H^{2}\\,+\\,64D^{2}G^{2}H^{2}\\,+\\,8G^{4}$ . Setting $C_{0}~\\ge~96d D C_{3}/\\delta$ , $C_{0}~\\ge~32D^{2}\\gamma_{1}$ , $\\delta~\\geq$ $4\\sqrt{6d(G^{4}+4C_{3})},\\gamma_{1}\\geq48d C_{3}/\\delta$ and $\\gamma_{2}\\geq2\\gamma_{1}$ completes the proof. ", "page_idx": 30}, {"type": "text", "text": "C Supporting lemmas ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "C.1 Proof of Lemma 1 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "First, we introduce two lemmas that will be used in the following proof. ", "page_idx": 31}, {"type": "text", "text": "Lemma 14. [Scroccaro et al., 2023, Lemma 3.1] Let $\\mathcal{X}$ be a convex set, $\\varphi(\\cdot):\\mathcal{X}\\rightarrow\\mathbb{R}$ be a convex function and $\\eta>0$ . Define ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbf{u}=\\arg\\operatorname*{min}_{\\mathbf{x}\\in\\mathcal{X}}\\left\\{\\eta\\varphi(\\mathbf{x})+{\\mathcal{B}}^{\\mathcal{R}}(\\mathbf{x},\\mathbf{v})\\right\\},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Then, for any $\\mathbf{z}\\in\\mathcal{X}$ , we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\eta\\langle\\mathbf{g}(\\mathbf{u}),\\mathbf{u}-\\mathbf{z}\\rangle\\leq\\mathcal{B}^{\\mathcal{R}}(\\mathbf{z},\\mathbf{v})-\\mathcal{B}^{\\mathcal{R}}(\\mathbf{z},\\mathbf{u})-\\mathcal{B}^{\\mathcal{R}}(\\mathbf{u},\\mathbf{v})\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $\\mathbf{g}(\\mathbf{u})\\in\\partial\\varphi(\\mathbf{u})$ . ", "page_idx": 31}, {"type": "text", "text": "Lemma 15. [Scroccaro et al., 2023, Lemma 3.2] Let $\\mathcal{X}$ be a convex set, $\\mathcal{R}:\\mathcal{X}\\xrightarrow{}\\mathbb{R}$ be an $\\alpha$ -strongly convex function with respect to the norm $\\|\\cdot\\|$ . Define ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{u}_{1}=\\arg\\underset{\\mathbf{x}_{1}\\in\\mathcal{X}}{\\operatorname*{min}}\\left\\{\\left\\langle\\mathbf{w}_{1},\\mathbf{x}_{1}\\right\\rangle+r\\left(\\mathbf{x}_{1}\\right)+\\mathcal{B}^{\\mathcal{R}}\\left(\\mathbf{x}_{1},\\mathbf{v}\\right)\\right\\}}\\\\ {\\mathbf{u}_{2}=\\arg\\underset{\\mathbf{x}_{2}\\in\\mathcal{X}}{\\operatorname*{min}}\\left\\{\\left\\langle\\mathbf{w}_{2},\\mathbf{x}_{2}\\right\\rangle+r\\left(\\mathbf{x}_{2}\\right)+\\mathcal{B}^{\\mathcal{R}}\\left(\\mathbf{x}_{2},\\mathbf{v}\\right)\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Then, we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{u}_{1}-\\mathbf{u}_{2}\\right\\|\\leq\\alpha^{-1}\\left\\|\\mathbf{w}_{1}-\\mathbf{w}_{2}\\right\\|_{*}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Then, by exploiting the convexity of $r(\\cdot)$ , we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}\\rangle+r(\\mathbf{x}_{t})-r(\\mathbf{x})=\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}\\rangle+r(\\mathbf{x}_{t})-r(\\hat{\\mathbf{x}}_{t+1})+r(\\hat{\\mathbf{x}}_{t+1})-r(\\mathbf{x})}\\\\ &{\\le\\!\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}\\rangle+\\langle\\nabla r(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\hat{\\mathbf{x}}_{t+1}\\rangle+\\langle\\nabla r(\\hat{\\mathbf{x}}_{t+1}),\\hat{\\mathbf{x}}_{t+1}-\\mathbf{x}\\rangle}\\\\ &{=\\underbrace{\\langle\\nabla f_{t}(\\mathbf{x}_{t})-M_{t},\\mathbf{x}_{t}-\\hat{\\mathbf{x}}_{t+1}\\rangle}_{\\mathrm{term~(a)}}+\\underbrace{\\langle M_{t}+\\nabla r(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\hat{\\mathbf{x}}_{t+1}\\rangle}_{\\mathrm{term~(b)}}+\\underbrace{\\langle\\nabla f_{t}(\\mathbf{x}_{t})+\\nabla r(\\hat{\\mathbf{x}}_{t+1}),\\hat{\\mathbf{x}}_{t+1}-\\mathbf{x}\\rangle}_{\\mathrm{term~(c)}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "To upper bound the term (a), we exploit Lemma 15 and obtain ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathsf{t e r m}\\left(\\mathsf{a}\\right)\\le\\|\\nabla f_{t}(\\mathbf{x}_{t})-M_{t}\\|_{*}\\|\\mathbf{x}_{t}-\\hat{\\mathbf{x}}_{t+1}\\|\\le\\alpha^{-1}\\|\\nabla f_{t}(\\mathbf{x}_{t})-M_{t}\\|_{*}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Next, we apply Lemma 14 with the update (4) and (5), and obtain ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{t e r m}\\left(\\mathsf{b}\\right)\\le\\mathcal{B}^{\\mathcal{R}_{t}}(\\hat{\\mathbf{x}}_{t+1},\\hat{\\mathbf{x}}_{t})-\\mathcal{B}^{\\mathcal{R}_{t}}(\\hat{\\mathbf{x}}_{t+1},\\mathbf{x}_{t})-\\mathcal{B}^{\\mathcal{R}_{t}}(\\mathbf{x}_{t},\\hat{\\mathbf{x}}_{t})}\\\\ &{\\mathsf{t e r m}\\left(\\mathsf{c}\\right)\\le\\mathcal{B}^{\\mathcal{R}_{t}}(\\mathbf{x},\\hat{\\mathbf{x}}_{t})-\\mathcal{B}^{\\mathcal{R}_{t}}(\\mathbf{x},\\hat{\\mathbf{x}}_{t+1})-\\mathcal{B}^{\\mathcal{R}_{t}}(\\hat{\\mathbf{x}}_{t+1},\\hat{\\mathbf{x}}_{t})}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Combining (52), (53) and (54) finishes the proof. ", "page_idx": 31}, {"type": "text", "text": "C.2 Proof of Lemma 3 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "First, we consider the case $t=1$ . Under the Assumption 2, we obtain ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\|\\nabla f_{1}(\\mathbf{x}_{1})-\\nabla f_{0}(\\mathbf{x}_{0})\\|_{2}^{2}=\\|\\nabla f_{1}(\\mathbf{x}_{1})\\|_{2}^{2}\\leq G^{2}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "In the case $t\\geq2$ , according to the inequality $\\|\\mathbf{a}+\\mathbf{b}\\|_{2}^{2}\\leq2\\|\\mathbf{a}\\|_{2}^{2}+2\\|\\mathbf{b}\\|_{2}^{2}$ , we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla f_{t-1}(\\mathbf{x}_{t-1})\\|_{2}^{2}}\\\\ &{{\\le}2\\|\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla F_{t}(\\mathbf{x}_{t-1})\\|_{2}^{2}+2\\|\\nabla F_{t}(\\mathbf{x}_{t-1})-\\nabla f_{t-1}(\\mathbf{x}_{t-1})\\|_{2}^{2}}\\\\ &{{\\le}4\\|\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla F_{t}(\\mathbf{x}_{t})\\|_{2}^{2}+4\\|\\nabla F_{t}(\\mathbf{x}_{t})-\\nabla F_{t}(\\mathbf{x}_{t-1})\\|_{2}^{2}}\\\\ &{\\quad{+}\\,4\\|\\nabla F_{t}(\\mathbf{x}_{t-1})-\\nabla F_{t-1}(\\mathbf{x}_{t-1})\\|_{2}^{2}+4\\|\\nabla F_{t-1}(\\mathbf{x}_{t-1})-\\nabla f_{t-1}(\\mathbf{x}_{t-1})\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Under the Assumption 4, the above result becomes ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla f_{t-1}(\\mathbf{x}_{t-1})\\|_{2}^{2}}\\\\ &{{\\le}4\\|\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla F_{t}(\\mathbf{x}_{t})\\|_{2}^{2}+4H^{2}\\|\\mathbf{x}_{t}-\\mathbf{x}_{t-1}\\|_{2}^{2}}\\\\ &{{\\quad+\\,4\\|\\nabla F_{t}(\\mathbf{x}_{t-1})-\\nabla F_{t-1}(\\mathbf{x}_{t-1})\\|_{2}^{2}+4\\|\\nabla F_{t-1}(\\mathbf{x}_{t-1})-\\nabla f_{t-1}(\\mathbf{x}_{t-1})\\|_{2}^{2}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Combining both cases, we reach at ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla f_{t-1}(\\mathbf{x}_{t-1})\\|_{2}^{2}\\leq\\!G^{2}+4\\|\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla F_{t}(\\mathbf{x}_{t})\\|_{2}^{2}+4\\|\\nabla F_{t}(\\mathbf{x}_{t-1})-\\nabla F_{t-1}(\\mathbf{x}_{t-1})\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\,4H^{2}\\|\\mathbf{x}_{t}-\\mathbf{x}_{t-1}\\|_{2}^{2}+4\\|\\nabla F_{t-1}(\\mathbf{x}_{t-1})-\\nabla f_{t-1}(\\mathbf{x}_{t-1})\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Summing up both sides of the above inequality over $t=1,\\cdot\\cdot\\cdot,T$ obtains ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\bar{V}_{T}=G^{2}+4\\sum_{t=2}^{T}\\|\\nabla f_{t}({\\bf x}_{t})-\\nabla F_{t}({\\bf x}_{t})\\|_{2}^{2}+4\\sum_{t=2}^{T}\\|\\nabla F_{t}({\\bf x}_{t-1})-\\nabla F_{t-1}({\\bf x}_{t-1})\\|_{2}^{2}}}\\\\ {~~}\\\\ {{\\displaystyle~~~~+4H^{2}\\sum_{t=2}^{T}\\|{\\bf x}_{t}-{\\bf x}_{t-1}\\|_{2}^{2}+4\\sum_{t=2}^{T}\\|\\nabla F_{t-1}({\\bf x}_{t-1})-\\nabla f_{t-1}({\\bf x}_{t-1})\\|_{2}^{2}}}\\\\ {~~}\\\\ {{\\displaystyle~~~~=G^{2}+8\\sum_{t=1}^{T}\\|\\nabla f_{t}({\\bf x}_{t})-\\nabla F_{t}({\\bf x}_{t})\\|_{2}^{2}+4\\sum_{t=2}^{T}\\|\\nabla F_{t}({\\bf x}_{t-1})-\\nabla F_{t-1}({\\bf x}_{t-1})\\|_{2}^{2}}}\\\\ {~~}\\\\ {{\\displaystyle~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~{\\displaystyle T}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "which completes the proof. ", "page_idx": 32}, {"type": "text", "text": "C.3 Proof of Lemma 4 ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Similar to the analysis in Lemma 3, we first consider the case $t=1$ . ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla f_{1}(\\hat{\\mathbf{x}}_{1})-\\nabla f_{0}(\\hat{\\mathbf{x}}_{0})\\|_{2}^{2}=\\|\\nabla f_{1}(\\hat{\\mathbf{x}}_{1})\\|_{2}^{2}\\leq G^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Then, for the case $t\\geq2$ , we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla f_{t}(\\hat{\\mathbf{x}}_{t})-\\nabla f_{t-1}(\\hat{\\mathbf{x}}_{t})\\|_{2}^{2}\\leq\\underset{\\mathbf{x}\\in\\mathcal{X}}{\\operatorname*{sup}}\\,\\|\\nabla f_{t}(\\mathbf{x})-\\nabla f_{t-1}(\\mathbf{x})\\|_{2}^{2}}\\\\ &{\\underset{\\mathbf{x}\\in\\mathcal{X}}{\\operatorname*{sup}}\\,2\\|\\nabla f_{t}(\\mathbf{x})-\\nabla F_{t}(\\mathbf{x})\\|_{2}^{2}+\\underset{\\mathbf{x}\\in\\mathcal{X}}{\\operatorname*{sup}}\\,2\\|\\nabla F_{t}(\\mathbf{x})-\\nabla f_{t-1}(\\mathbf{x})\\|_{2}^{2}}\\\\ &{\\underset{\\mathbf{x}\\in\\mathcal{X}}{\\operatorname*{sup}}\\,2\\|\\nabla f_{t}(\\mathbf{x})-\\nabla F_{t}(\\mathbf{x})\\|_{2}^{2}+\\underset{\\mathbf{x}\\in\\mathcal{X}}{\\operatorname*{sup}}\\,2\\|\\nabla F_{t}(\\mathbf{x})-\\nabla F_{t-1}(\\mathbf{x})\\|_{2}^{2}+\\underset{\\mathbf{x}\\in\\mathcal{X}}{\\operatorname*{sup}}\\,2\\|\\nabla F_{t-1}(\\mathbf{x})-\\nabla f_{t-1}(\\mathbf{x})\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Summing up both sides of the above inequality over $t=1,\\cdot\\cdot\\cdot,T$ completes the proof. ", "page_idx": 32}, {"type": "text", "text": "C.4 Proof of Lemma 7 ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "The analysis is based on Lemma 19 in Chiang et al. [2012]. Recall the definition $H_{t}=I+{\\textstyle\\frac{\\beta}{2}}{\\cal G}^{2}\\delta I+$ $\\textstyle{\\frac{\\beta}{2}}\\sum_{r=1}^{t-1}h_{r}$ with $h_{r}=\\nabla f_{r}(\\mathbf{x}_{r})\\nabla f_{r}(\\mathbf{x}_{r})^{\\top}$ . Then, we can proof that ", "page_idx": 32}, {"type": "equation", "text": "$$\nH_{t}\\succeq\\delta I+\\frac{\\beta}{2}\\sum_{r=1}^{t}h_{r}\\succeq\\delta I+\\frac{\\beta}{4}\\sum_{r=1}^{t}(h_{r}+h_{r-1}).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "The first step is due to Assumption 2, and the last step is due to $\\nabla f_{0}(\\mathbf{x}_{0})$ the all-0 vector. Next, according to the fact that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\frac{1}{2}h_{r}+\\frac{1}{2}h_{r-1}+\\frac{1}{2}\\nabla f_{r-1}(\\mathbf{x}_{r-1})\\nabla f_{r}(\\mathbf{x}_{r})^{\\top}+\\frac{1}{2}\\nabla f_{r}(\\mathbf{x}_{r})\\nabla f_{r-1}(\\mathbf{x}_{r-1})^{\\top}}\\\\ &{\\displaystyle=\\frac{1}{2}\\left(\\nabla f_{r}(\\mathbf{x}_{r})+\\nabla f_{r-1}(\\mathbf{x}_{r-1})\\right)\\left(\\nabla f_{r}(\\mathbf{x}_{r})+\\nabla f_{r-1}(\\mathbf{x}_{r-1})\\right)^{\\top}\\succeq0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "we have ", "page_idx": 32}, {"type": "equation", "text": "$$\nh_{r}+h_{r-1}\\succeq\\frac{1}{2}\\left(\\nabla f_{r}(\\mathbf{x}_{r})-\\nabla f_{r-1}(\\mathbf{x}_{r-1})\\right)\\left(\\nabla f_{r}(\\mathbf{x}_{r})-\\nabla f_{r-1}(\\mathbf{x}_{r-1})\\right)^{\\top}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Substituting (57) into (56) delivers ", "page_idx": 32}, {"type": "equation", "text": "$$\nH_{t}\\succeq\\delta I+\\frac{\\beta}{8}\\sum_{r=1}^{t}\\left(\\nabla f_{r}(\\mathbf{x}_{r})-\\nabla f_{r-1}(\\mathbf{x}_{r-1})\\right)\\left(\\nabla f_{r}(\\mathbf{x}_{r})-\\nabla f_{r-1}(\\mathbf{x}_{r-1})\\right)^{\\top}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "For brevity, we denote $\\begin{array}{r}{K_{t}=\\delta I+\\frac{\\beta}{8}\\sum_{r=1}^{t}\\left(\\nabla f_{r}(\\mathbf{x}_{r})-\\nabla f_{r-1}(\\mathbf{x}_{r-1})\\right)(\\nabla f_{r}(\\mathbf{x}_{r})-\\nabla f_{r-1}(\\mathbf{x}_{r-1}))^{\\top}.}\\end{array}$ From (58), we obtain that $H_{t}^{-1}\\preceq K_{t}^{-1}$ , which implies ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{t=1}^{T}\\|\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla f_{t-1}(\\mathbf{x}_{t-1})\\|_{H_{t}^{-1}}^{2}\\leq\\displaystyle\\sum_{t=1}^{T}\\|\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla f_{t-1}(\\mathbf{x}_{t-1})\\|_{K_{t}^{-1}}^{2}}&{}\\\\ {\\displaystyle=\\frac{8}{\\beta}\\sum_{t=1}^{T}\\left\\|\\sqrt{\\frac{\\beta}{8}}\\left(\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla f_{t-1}(\\mathbf{x}_{t-1})\\right)\\right\\|_{K_{t}^{-1}}^{2}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Then, we introduce the following lemma: ", "page_idx": 33}, {"type": "text", "text": "Lemma 16. [Hazan et al., 2007, Lemma 11] Let $\\mathbf{u}_{1},\\cdot\\cdot\\cdot\\mathbf{\\Phi},\\mathbf{u}_{T}\\in\\mathbb{R}^{d}$ be any sequence vectors and $\\epsilon>0$ be a positive real number. Denote $\\begin{array}{r}{V_{t}=\\epsilon I+\\sum_{i=1}^{t}{\\bf u}_{i}{\\bf u}_{i}^{\\top}}\\end{array}$ , then we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbf{u}_{t}^{\\top}V_{t}^{-1}\\mathbf{u}_{t}\\leq d\\log\\left(1+\\frac{1}{\\epsilon}\\sum_{t=1}^{T}\\|\\mathbf{u}_{t}\\|_{2}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "By setting $\\epsilon=\\delta$ and $\\begin{array}{r}{\\mathbf{u}_{t}=\\sqrt{\\frac{\\beta}{8}}\\left(\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla f_{t-1}(\\mathbf{x}_{t-1})\\right)}\\end{array}$ , we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\left\\lVert\\sqrt{\\frac{\\beta}{8}}\\left(\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla f_{t-1}(\\mathbf{x}_{t-1})\\right)\\right\\rVert_{K_{t}^{-1}}^{2}\\leq d\\log\\left(1+\\frac{\\beta}{8\\delta}\\sum_{t=1}^{T}\\lVert\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla f_{t-1}(\\mathbf{x}_{t-1})\\rVert_{2}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We complete the proof by combining (59) and (60). ", "page_idx": 33}, {"type": "text", "text": "C.5 Proof of Lemma 10 ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Our proof starts from the following lemma for single-layer MsMwC. ", "page_idx": 33}, {"type": "text", "text": "Lemma 17. [Yan et al., 2023, Lemma $2J\\,H\\mathrm{max}_{t\\in[T],i\\in[N]}|l_{t}^{i}|,|m_{t}^{i}|\\leq1,$ , then MsMwC enjoys ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\left<l_{t},p_{t}\\right>-\\displaystyle\\sum_{t=1}^{T}l_{t}^{i}\\leq\\frac{1}{\\eta^{i}}\\ln\\frac{1}{\\hat{p}_{1}^{i}}+\\sum_{i=1}^{N}\\frac{\\hat{p}_{1}^{i}}{\\eta^{i}}-8\\sum_{t=1}^{T}\\displaystyle\\sum_{i=1}^{N}\\eta^{i}p_{t}^{i}\\left(l_{t}^{i}-m_{t}^{i}\\right)^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+16\\eta^{i}\\displaystyle\\sum_{t=1}^{T}\\left(l_{t}^{i}-m_{t}^{i}\\right)^{2}-\\displaystyle\\operatorname*{min}_{k\\in[K]}\\frac{1}{4\\eta^{k}}\\sum_{t=2}^{T}\\left\\lVert p_{t}-p_{t-1}\\right\\rVert_{1}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "It can be verified that under Assumption $1,2$ and 3, all our composite losses and optimisms defined in (13) and (14) are bounded ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|l_{t}^{k}|\\leq G D+C+\\gamma_{1}D,\\,|m_{t}^{k}|\\leq G D+C+\\gamma_{1}D}\\\\ &{|l_{t}^{k,i}|\\leq G D+C+\\gamma_{2}D,\\,|m_{t}^{k,i}|\\leq G D+C+\\gamma_{2}D,}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and we can rescale them to $[-1,1]$ with only constant multiplicative factors in the constant hyperparameter $\\gamma_{1}$ and $\\gamma_{2}$ . Therefore, applying Lemma 17 with the first layer(i.e., with $l_{t}^{k}$ as the surrogate losses), we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\sum_{t=1}^{T}\\left\\langle l_{t},q_{t}\\right\\rangle-\\sum_{t=1}^{T}l_{t}^{k}\\leq\\frac{1}{\\eta^{k}}\\ln\\frac{1}{\\hat{q}_{1}^{k}}+\\sum_{k=1}^{K}\\frac{\\hat{q}_{1}^{k}}{\\eta^{k}}-8\\sum_{t=1}^{T}\\sum_{k=1}^{K}\\eta^{k}q_{t}^{k}\\left(l_{t}^{k}-m_{t}^{k}\\right)^{2}}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad+16\\eta^{k}\\sum_{t=1}^{T}\\left(l_{t}^{k}-m_{t}^{k}\\right)^{2}-\\displaystyle\\operatorname*{min}_{k\\in[K]}\\frac{1}{4\\eta^{k}}\\sum_{t=2}^{T}\\left\\Vert q_{t}-q_{t-1}\\right\\Vert_{1}^{2}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "For the first two terms, the initialization $\\hat{q}_{1}^{k}=(\\eta^{k})^{2}/\\sum_{k=1}^{K}(\\eta^{k})^{2}$ and $\\eta^{k}=1/(C_{0}2^{k})$ imply ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\frac{1}{\\eta^{k}}\\ln\\frac{1}{\\hat{q}_{1}^{k}}+\\sum_{k=1}^{K}\\frac{\\hat{q}_{1}^{k}}{\\eta^{k}}=\\frac{1}{\\eta^{k}}\\ln\\frac{\\sum_{k=1}^{K}(\\eta^{k})^{2}}{(\\eta^{k})^{2}}+\\sum_{k=1}^{K}\\frac{\\eta^{k}}{\\sum_{k=1}^{K}(\\eta^{k})^{2}}\\le\\frac{1}{\\eta^{k}}\\ln\\frac{1}{3(C_{0}\\eta^{k})^{2}}+4C_{0}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "For the last term, due to $\\eta^{k}=1/(C_{0}2^{k})\\leq2/C_{0}$ , we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{k\\in[K]}\\frac{1}{4\\eta^{k}}\\sum_{t=2}^{T}\\left\\|\\pmb{q}_{t}-\\pmb{q}_{t-1}\\right\\|_{1}^{2}\\geq\\frac{C_{0}}{2}\\sum_{t=2}^{T}\\left\\|\\pmb{q}_{t}-\\pmb{q}_{t-1}\\right\\|_{1}^{2}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Substitute (62) and (63) into (61) delivers ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\left\\langle l_{t},q_{t}\\right\\rangle-\\sum_{t=1}^{T}l_{t}^{k}\\leq\\frac{1}{\\eta^{k}}\\ln\\frac{1}{3(C_{0}\\eta^{k})^{2}}+16\\eta^{k}\\sum_{t=1}^{T}\\left(l_{t}^{k}-m_{t}^{k}\\right)^{2}-\\frac{C_{0}}{2}\\sum_{t=2}^{T}\\left\\Vert q_{t}-q_{t-1}\\right\\Vert_{1}^{2}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Then, we apply Lemma 17 again with $l_{t}^{k,i}$ and $m_{t}^{k,i}$ ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\left<l_{t}^{k},p_{t}^{k}\\right>-\\displaystyle\\sum_{t=1}^{T}l_{t}^{k,i}\\leq\\frac{1}{\\eta^{k,i}}\\ln\\frac{1}{\\hat{p}_{1}^{k,i}}+\\sum_{i=1}^{N}\\frac{\\hat{p}_{1}^{k,i}}{\\eta^{k,i}}-8\\displaystyle\\sum_{t=1}^{T}\\sum_{i=1}^{N}\\eta^{k,i}p_{t}^{k,i}\\left(l_{t}^{k,i}-m_{t}^{k,i}\\right)^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+16\\eta^{k,i}\\displaystyle\\sum_{t=1}^{T}\\left(l_{t}^{k,i}-m_{t}^{k,i}\\right)^{2}-\\displaystyle\\operatorname*{min}_{i\\in[N]}\\frac{1}{4\\eta^{k,i}}\\sum_{t=2}^{T}\\left\\lVert p_{t}^{k}-p_{t-1}^{k}\\right\\rVert_{1}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "With the initialization $\\hat{p}_{1}^{k,i}=1/N$ and $\\eta^{k,i}=2\\eta^{k}$ , we obtain ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{1}{\\eta^{k,i}}\\ln\\frac{1}{\\hat{p}_{1}^{k,i}}+\\sum_{i=1}^{N}\\frac{\\hat{p}_{1}^{k,i}}{\\eta^{k,i}}=\\frac{\\ln N+1}{2\\eta^{k}}\\leq C_{0}(\\ln N+1)=\\mathcal{O}(1).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Therefore, (65) becomes ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\left<l_{t}^{k},p_{t}^{k}\\right>-\\displaystyle\\sum_{t=1}^{T}l_{t}^{k,i}\\leq\\frac{\\ln N+1}{2\\eta^{k}}-16\\eta^{k}\\sum_{t=1}^{T}\\sum_{i=1}^{N}p_{t}^{k,i}\\left(l_{t}^{k,i}-m_{t}^{k,i}\\right)^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\displaystyle32\\eta^{k}\\sum_{t=1}^{T}\\left(l_{t}^{k,i}-m_{t}^{k,i}\\right)^{2}-\\frac{C_{0}}{16}\\sum_{t=2}^{T}\\left\\|p_{t}^{k}-p_{t-1}^{k}\\right\\|_{1}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Combining (64) and (66), we obtain ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\sum_{t=1}^{T}\\langle q_{t}-\\mathbf{e}_{k},l_{t}\\rangle+\\left\\langle p_{t}^{k}-\\mathbf{e}_{i},l_{t}^{k}\\right\\rangle}\\\\ &{\\le\\displaystyle\\frac{1}{\\eta^{k}}\\ln\\frac{1}{3(C_{0}\\eta^{k})^{2}}+32\\eta^{k}\\displaystyle\\sum_{t=1}^{T}\\left(l_{t}^{k,i}-m_{t}^{k,i}\\right)^{2}-\\frac{C_{0}}{16}\\sum_{t=2}^{T}\\left\\|p_{t}^{k}-p_{t-1}^{k}\\right\\|_{1}^{2}-\\frac{C_{0}}{2}\\displaystyle\\sum_{t=2}^{T}\\left\\|q_{t}-q_{t-1}\\right\\|_{1}^{2}}\\\\ &{\\quad+\\displaystyle16\\eta^{k}\\displaystyle\\sum_{t=1}^{T}\\left(\\left(l_{t}^{k}-m_{t}^{k}\\right)^{2}-\\sum_{i=1}^{N}p_{t}^{k,i}\\left(l_{t}^{k,i}-m_{t}^{k,i}\\right)^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "According to (13) and (14), the last term is bounded by ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left(l_{t}^{k}-m_{t}^{k}\\right)^{2}-\\sum_{i=1}^{N}p_{t}^{k,i}\\left(l_{t}^{k,i}-m_{t}^{k,i}\\right)^{2}}\\\\ {\\displaystyle=\\left(\\left\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}^{k}\\right\\rangle-\\left\\langle\\hat{m}_{t}^{k},p_{t}^{k}\\right\\rangle\\right)^{2}-\\displaystyle\\sum_{i=1}^{N}p_{t}^{k,i}\\left(l_{t}^{k,i}-m_{t}^{k,i}\\right)^{2}}\\\\ {\\displaystyle=\\left\\langle\\hat{l}_{t}^{k}-\\hat{m}_{t}^{k},p_{t}^{k}\\right\\rangle^{2}-\\displaystyle\\sum_{i=1}^{N}p_{t}^{k,i}\\left(l_{t}^{k,i}-m_{t}^{k,i}\\right)^{2}\\leq0}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $\\hat{l}_{t}^{k}=\\left(\\left\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}^{k,1}\\right\\rangle,\\cdot\\cdot\\cdot\\mathbf{\\nabla},\\left\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}^{k,N}\\right\\rangle\\right)$ and the last step is due to Cauchy-Schwarz inequality. Combining (67) and (68) completes the proof. ", "page_idx": 34}, {"type": "text", "text": "C.6 Proof of Lemma 12 ", "text_level": 1, "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{x}_{t}-\\mathbf{x}_{t-1}\\|_{2}^{2}=\\left\\|\\displaystyle\\sum_{k=1}^{K}\\left(q_{t}^{k}\\mathbf{x}_{t}^{k}-q_{t-1}^{k}\\mathbf{x}_{t-1}^{k}\\right)\\right\\|_{2}^{2}=\\left\\|\\displaystyle\\sum_{k=1}^{K}q_{t}^{k}\\left(\\mathbf{x}_{t}^{k}-\\mathbf{x}_{t-1}^{k}\\right)+\\displaystyle\\sum_{k=1}^{K}\\mathbf{x}_{t-1}^{k}\\left(q_{t}^{k}-q_{t-1}^{k}\\right)\\right\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\leq2\\left\\|\\displaystyle\\sum_{k=1}^{K}q_{t}^{k}\\left(\\mathbf{x}_{t}^{k}-\\mathbf{x}_{t-1}^{k}\\right)\\right\\|_{2}^{2}+2\\left\\|\\displaystyle\\sum_{k=1}^{K}\\mathbf{x}_{t-1}^{k}\\left(q_{t}^{k}-q_{t-1}^{k}\\right)\\right\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\leq2\\displaystyle\\sum_{k=1}^{K}q_{t}^{k}\\left\\|\\mathbf{x}_{t}^{k}-\\mathbf{x}_{t}^{k}\\right\\|_{2}^{2}+2D^{2}\\|q_{t}-q_{t-1}\\|_{1}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where the second step is due to $(a+b)^{2}\\leq2a^{2}+2b^{2}$ and the last step is due to the triangle inequality. ", "page_idx": 35}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The contributions have been clearly claimed in the abstract and introduction. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 36}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: The limitations have been discussed in Section 7. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 36}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: All the assumptions are listed in Section 3.1 and the complete proofs are provided in Appendix B. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 37}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We have provided detailed descriptions for experiments in Section A that are sufficient to reproduce the experimental results. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 37}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 38}, {"type": "text", "text": "Answer: [No] ", "page_idx": 38}, {"type": "text", "text": "Justification: While the code is not included in the submission, the complete detailed descriptions of the experiments are provided in Section A. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 38}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: We have provided detailed descriptions for experiments in Section A that are sufficient to reproduce the experimental results. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 38}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: All experiments are repeated ten times with different random seeds, and the mean and standard deviation are reported. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 38}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 39}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 39}, {"type": "text", "text": "Answer: [No] ", "page_idx": 39}, {"type": "text", "text": "Justification: The experiments are conducted on a machine equipped with the Intel Xeon E5-2620 CPU and 32G memory. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 39}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: The authors have diligently adhered to the NeurIPS Code of Ethics in all aspects of this research. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 39}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: This paper is mainly theoretical and there is no societal impact of this work. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 39}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 40}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: This paper is mainly theoretical and poses no such risks. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 40}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: This paper is mainly theoretical and does not use existing assets. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 40}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 41}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 41}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 42}]