[{"figure_path": "bKuxygBW2Y/figures/figures_1_1.jpg", "caption": "Figure 1: We showcase the prediction results in various environmental conditions (day, rainy and night from top to bottom). Our VQ-Map produces more reasonable results, even for areas that are not directly visible, while significantly reducing artifacts. Color scheme is the same as in [1].", "description": "This figure compares the BEV map layout estimation results of VQ-Map with those of BEVFusion [1] under various environmental conditions (day, rainy and night).  The top row shows daytime results, the middle row shows rainy conditions, and the bottom row shows nighttime conditions.  For each condition, the leftmost image shows the surround-view images used as input. The middle image displays the ground truth BEV map layout.  The third image shows the results produced by BEVFusion, and the rightmost image shows the results generated by the proposed VQ-Map model. The figure highlights that VQ-Map produces more realistic and coherent BEV maps, particularly in areas where there is significant occlusion or poor visibility, and shows a significant reduction in artifacts compared to BEVFusion.", "section": "1 Introduction"}, {"figure_path": "bKuxygBW2Y/figures/figures_3_1.jpg", "caption": "Figure 2: VQ-Map employs a generative model similar to the VQ-VAE framework to encode the BEV groundtruth maps into BEV tokens accompanied with a codebook embedding. After the generative model training, the BEV tokens serve as the classification labels to supervise the PV feature learning via a specialized token decoder module. During inference, VQ-Map utilizes the predicted BEV tokens to generate high-quality BEV map layouts based on the off-the-shelf codebook embedding and the BEV map generation decoder.", "description": "This figure illustrates the overall architecture of the VQ-Map model. It shows how the model uses a VQ-VAE-like structure to generate BEV tokens from ground truth BEV maps and a codebook.  These tokens are then used as supervision signals to train a token decoder that learns to map PV features to the BEV token space. During inference, the model predicts BEV tokens from PV inputs, which are then used with the codebook embedding to generate a final high-quality BEV map.", "section": "3 Methods"}, {"figure_path": "bKuxygBW2Y/figures/figures_4_1.jpg", "caption": "Figure 3: Visualization of the BEV codebook embedding by showing the BEV patch images corresponding to the specific BEV tokens. All BEV patch images in the same column correspond to the same token. The data is randomly sampled from the nuScenes validation dataset. Color scheme is the same as in [1].", "description": "This figure visualizes the BEV codebook embedding learned by the VQ-VAE model. Each column represents a unique BEV token, and the images within each column are BEV patches that are closest to that specific token in the embedding space.  The patches are sampled randomly from the nuScenes validation set.", "section": "3.1 Discrete Representation Learning for BEV Generation"}, {"figure_path": "bKuxygBW2Y/figures/figures_5_1.jpg", "caption": "Figure 4: Architecture of Our Token Decoder. Pos refers to the positional embedding, and M indicates the layer number.", "description": "The figure showcases the architecture of the Token Decoder module used in the VQ-Map pipeline.  It takes multi-scale image features (from a Feature Pyramid Network) and camera calibration as input. It utilizes self-attention and deformable cross-attention mechanisms to attend to relevant image features based on token queries, generating BEV Tokens.", "section": "3.2 Token Prediction with Sparse Features for PV-BEV Alignment"}, {"figure_path": "bKuxygBW2Y/figures/figures_12_1.jpg", "caption": "Figure 1: We showcase the prediction results in various environmental conditions (day, rainy and night from top to bottom). Our VQ-Map produces more reasonable results, even for areas that are not directly visible, while significantly reducing artifacts. Color scheme is the same as in [1].", "description": "This figure displays a qualitative comparison of bird's-eye-view (BEV) map layout estimation results from different methods under various weather conditions (day, rain, night). It demonstrates that the proposed VQ-Map model produces more accurate and realistic BEV maps than the BEVFusion method, especially in areas with occlusions or poor visibility, and exhibits fewer artifacts.", "section": "1 Introduction"}, {"figure_path": "bKuxygBW2Y/figures/figures_13_1.jpg", "caption": "Figure 1: We showcase the prediction results in various environmental conditions (day, rainy and night from top to bottom). Our VQ-Map produces more reasonable results, even for areas that are not directly visible, while significantly reducing artifacts. Color scheme is the same as in [1].", "description": "This figure demonstrates the bird's-eye-view (BEV) map layout estimation results of the proposed VQ-Map model compared to BEVFusion [1] under various weather conditions (day, rain, and night).  VQ-Map shows improved performance, particularly in generating realistic and coherent BEV maps even in areas with limited visibility, reducing artifacts.", "section": "1 Introduction"}, {"figure_path": "bKuxygBW2Y/figures/figures_13_2.jpg", "caption": "Figure A3: Visualization results for monocular BEV map layout estimation on nuScenes. Unlike other methods, our VQ-Map only focuses on map layout classes. The color scheme is the same as in PON [39].", "description": "This figure compares the BEV map layout estimation results of different methods on the nuScenes dataset using a monocular view. The top row shows the input monocular images, while the bottom row presents the ground truth BEV maps and the predictions from various methods including VPN [41], PON [39], GitNet [42], TaDe [16], and the proposed VQ-Map.  Noticeably, VQ-Map focuses solely on map layout classes, unlike other approaches that might incorporate additional elements.", "section": "Appendix B More Visualization Results"}, {"figure_path": "bKuxygBW2Y/figures/figures_14_1.jpg", "caption": "Figure 1: We showcase the prediction results in various environmental conditions (day, rainy and night from top to bottom). Our VQ-Map produces more reasonable results, even for areas that are not directly visible, while significantly reducing artifacts. Color scheme is the same as in [1].", "description": "This figure shows a qualitative comparison of BEV map layout estimation results under different environmental conditions (day, rain, and night). The results of three methods are compared: BEVFusion [1], which is a state-of-the-art method, and the proposed VQ-Map. The comparison demonstrates that VQ-Map generates more reasonable and artifact-free BEV maps even in challenging scenarios where visibility is limited. The color scheme in this figure is consistent with BEVFusion [1].", "section": "1 Introduction"}]