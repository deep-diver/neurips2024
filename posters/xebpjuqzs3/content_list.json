[{"type": "text", "text": "Prospective Learning: Learning for a Dynamic Future ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ashwin De Silva\u2217,1 Rahul Ramesh\u2217,2 Rubing Yang\u2217,2   \nSiyu $\\mathbf{Y}\\mathbf{u}^{1}$ Joshua T. Vogelstein\u2020,1 Pratik Chaudhari\u2020,2 ", "page_idx": 0}, {"type": "text", "text": "\u2217,\u2020 Equal Contribution Email: {ldesilv2, syu80, jovo}@jhu.edu, {rahulram, rubingy, pratikac} $@$ upenn.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In real-world applications, the distribution of the data, and our goals, evolve over time. The prevailing theoretical framework for studying machine learning, namely probably approximately correct (PAC) learning, largely ignores time. As a consequence, existing strategies to address the dynamic nature of data and goals exhibit poor real-world performance. This paper develops a theoretical framework called \u201cProspective Learning\u201d that is tailored for situations when the optimal hypothesis changes over time. In PAC learning, empirical risk minimization (ERM) is known to be consistent. We develop a learner called Prospective ERM, which returns a sequence of predictors that make predictions on future data. We prove that the risk of prospective ERM converges to the Bayes risk under certain assumptions on the stochastic process generating the data. Prospective ERM, roughly speaking, incorporates time as an input in addition to the data. We show that standard ERM as done in PAC learning, without incorporating time, can result in failure to learn when distributions are dynamic. Numerical experiments illustrate that prospective ERM can learn synthetic and visual recognition problems constructed from MNIST and CIFAR-10. Code at https://github.com/neurodata/prolearn. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "All learning is for the future. Learning involves updating decision rules or policies, based on past experiences, to improve future performance. Probably approximately correct (PAC) learning has been extremely useful to develop algorithms that minimize the risk\u2014typically defined as the expected loss\u2014on unseen samples under certain assumptions. The assumption, that samples are independent and identically distributed (IID) within the training dataset and at test time, has served us well. But it is neither testable nor believed to be true in practice. The future is always different from the past: both distributions of data and goals of the learner may change over time. Moreover, those changes may cause the optimal hypothesis to change over time as well. There are numerous mathematical and empirical approaches that have been developed to address this issue, e.g., techniques for being invariant to [1], or adapting to, distribution shift [2], modeling the future as a different task, etc. But we lack a first-principles framework to address problems where data distributions and goals may change over time in such a way that the optimal hypothesis is time-dependent. And as a consequence, machine learning-based AI today is brittle to changes in distribution and goals. ", "page_idx": 0}, {"type": "text", "text": "This paper develops a theoretical framework called \u201cProspective Learning\u201d (PL). Instead of data arising from an unknown probability distribution like in PAC learning, prospective learning assumes that data comes from an unknown stochastic process, that the loss considers the future, and that the optimal hypothesis may change over time. A prospective learner uses samples received up to some time $t\\in\\mathbb{N}$ to output an infinite sequence of predictors, which it uses for making predictions on data at all future times $t^{\\prime}>t$ . We discuss how prospective learning is related to existing problem formulations in the literature in Section 3 and Appendix A. ", "page_idx": 0}, {"type": "text", "text": "Why should one care about prospective learning? Imagine a deployed machine learning system. The designer of this system desires to optimize\u2014not the risk upon the past training data, or the risk on the immediate future data\u2014but the risk on all data that the model will make predictions upon in the future. As data evolves, e.g., due to changing trends and preferences of the users, the optimal hypothesis to make predictions also changes. Time is the critical piece of information if the system designer is to achieve their goals. Both in the sense of how far back in time a particular datum was recorded, and in the sense of how far ahead in the future this system will be used to make predictions. The designer must take time into account to avoid retraining the model periodically, ad infinitum. ", "page_idx": 1}, {"type": "text", "text": "Biology is also rich with examples where systems seem to behave prospectively. The principle of allostasis, for example, states that regulatory processes of living things anticipate the needs of the organism and prepare to satisfy these needs before, rather than after, they arise [3]. For example, mitochondria increase their energy production to anticipate the demands of muscles [4], neural circuits anticipate changes in sensory stimuli and the task (i.e., predictive coding [5]), and individual organisms optimize their actions with respect to anticipated changes in their environments [6, 7]. These regulatory principles were learned early in evolutionary time so they must be important. In short, the world\u2014including our internal drives\u2014changes all the time, and learning systems must anticipate (that is, prospect) these changes to thrive. ", "page_idx": 1}, {"type": "text", "text": "Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "\u2022 Section 2 defines Prospective Learning (PL) as an approach to address problems where the optimal hypothesis may evolve over time (due to shifts in distributions and/or goals of the learner). It also provides illustrative examples of PL.   \n\u2022 Section 3 and Appendix A put prospective learning in context relative to existing ideas in the literature to address changes in the data distribution.   \n\u2022 Section 4 takes steps towards a theoretical foundation for prospective learning. We define strongly learnability (i.e., there exists a prospective learner whose risk is arbitrarily close to the Bayes optimal learner) and weakly learnability (i.e., there exists a prospective learner whose risk is better than chance) [8]. Empirical risk minimization (ERM) without incorporating time, can result in failure to strongly, or even weakly, learn prospectively [9].   \n\u2022 Section 5 introduces prospective empirical risk minimization, and proves that it can learn prospectively under certain assumptions on the stochastic process and loss.   \n\u2022 Section 6 demonstrates that our prospective learners can prospectively learn several canonical problems constructed using synthetic, MNIST [10] and CIFAR-10 [11] data. In contrast, a number of existing algorithms, including ERM, online and continual learning algorithms, fail. Appendix $_\\mathrm{H}$ demonstrates that current large language models, which use Transformer-based architectures trained using auto-regressive losses, fail to learn prospectively. ", "page_idx": 1}, {"type": "text", "text": "2 A definition of prospective learning ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "A prospective learner minimizes the expected cumulative risk of the future using past data. Such a learner is defined by the following key ingredients (see Fig. 1 (left) for schematic illustration). ", "page_idx": 1}, {"type": "text", "text": "Data. Let the input and output at time $t$ be denoted by $x_{t}\\,\\in\\,\\mathcal{X}$ and $y_{t}\\in\\mathcal{Y}$ respectively. Let $z_{t}=(x_{t},y_{t})$ . We will model the data as a stochastic process $Z\\equiv(Z_{t})_{t\\in\\mathbb{N}}$ defined on an appropriate probability space $(\\Omega,{\\mathcal{F}},\\mathbb{P})$ . At time $t\\in\\mathbb{N}$ , denote past data by $\\boldsymbol{z}_{\\le t}\\equiv(z_{1},\\dots,z_{t})$ and future data by $\\boldsymbol{z}_{>t}\\equiv(z_{t+1},\\dots)$ . We will find it useful to distinguish between the realization of the data, denoted by $z{\\leq}t$ , and the corresponding random variable, $Z{\\leq}t$ . ", "page_idx": 1}, {"type": "text", "text": "Hypothesis class. At each time $t$ , a prospective learner selects an infinite sequence $h\\equiv$ $(h_{1},\\cdot\\cdot\\cdot,h_{t},h_{t+1},\\cdot\\cdot\\cdot)$ which it uses to make predictions on data at any time in the future. Each element of this sequence $h_{t}:\\mathcal{X}\\mapsto\\mathcal{Y}$ and therefore $h_{t}\\in\\mathcal{V}^{\\mathcal{X}}$ .1 The hypothesis class $\\mathcal{H}$ is the space of such hypotheses, $h\\in\\mathcal{H}\\subseteq(\\mathcal{V}^{\\mathcal{X}})^{\\mathbb{N}}$ .2 We will again use the shorthand $h_{\\leq t}\\equiv(h_{1},\\ldots,h_{t})$ . We will sometimes talk about a \u201ctime-agnostic hypothesis\u201d which will refer to a hypothesis such that $h_{t}=h_{t^{\\prime}}$ for all $t,t^{\\prime}\\in\\mathbb{N}$ . Observe that this makes our setup different from the standard setup in PAC learning where the learner selects a single hypothesis in $y^{\\mathcal{X}}$ . One could also think of prospective learning as using a single time-varying hypothesis $h:\\mathbb{N}\\times\\mathcal{X}\\mapsto\\mathcal{Y}$ , i.e., the hypothesis takes both time and the datum as input to make a prediction. ", "page_idx": 1}, {"type": "text", "text": "Learner. A prospective learner is a map from the data received up to time $t$ , to a hypothesis that makes predictions on the data over all time (past and future): $(\\mathcal{X}\\times\\bar{\\mathcal{X}})^{t}\\to(\\mathcal{Y}^{\\mathcal{X}})^{\\mathbb{N}}$ . The learner gives as output a hypothesis $h(z_{\\leq t})\\in\\#$ . Unlike a PAC learner, a prospective learner can make different kinds of predictions at different times. This is a crucial property of prospective learning. In other words, after receiving data up to time $t$ , the hypothesis selected by the prospective learner can predict on samples at any future time $t^{\\prime}>t$ . ", "page_idx": 2}, {"type": "text", "text": "Prospective loss and risk. The future loss incurred by a hypothesis $h$ is ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\bar{\\ell}_{t}(h,Z)=\\operatorname*{lim}_{\\tau\\to\\infty}\\operatorname*{sup}_{\\tau}\\frac{1}{\\tau}\\sum_{s=t+1}^{t+\\tau}\\ell(s,h_{s}(X_{s}),Y_{s}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\ell:\\mathbb{N}\\times\\mathcal{Y}\\times\\mathcal{Y}\\mapsto[0,1]$ is a bounded loss function.3 Prospective risk at time $t$ is the expected future loss 4 ", "page_idx": 2}, {"type": "equation", "text": "$$\nR_{t}(h)=\\mathbb{E}\\left[\\bar{\\ell}_{t}(h,Z)\\mid z_{\\leq t}\\right]=\\int\\bar{\\ell}_{t}(h,Z)\\,\\mathrm{d}\\,\\mathbb{P}_{Z\\mid z_{\\leq t}}\\,,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where we assume that $h$ is a random variable and $h\\in\\sigma(Z_{\\leq t})$ where $\\sigma(\\cdot)$ denotes the filtration (an increasing sequence of sigma algebras) of the stochastic process $Z$ . We have used the shorthand $\\mathbb{E}[Y\\mid x]$ for $\\mathbb{E}[Y\\mid X=x]$ . Observe that we have conditioned the prospective risk of the hypothesis $h$ upon the realized data $z{\\leq}t$ . We can take an expectation over the realized data, to obtain the expected prospective risk ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[R_{t}(h)\\right]=\\int R_{t}(h)\\,\\mathrm{d}\\mathbb{P}_{Z_{\\leq t}}\\,.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Prospective Bayes risk is the minimum risk achievable by any hypothesis. In PAC learning, it is a constant that depends upon the (fixed) true distribution of the data and the risk function. In prospective learning, the optimal hypothesis can predict differently at different times. We therefore define the prospective Bayes risk at a time $t$ as ", "page_idx": 2}, {"type": "equation", "text": "$$\nR_{t}^{*}=\\operatorname*{inf}_{h\\in\\sigma(Z_{\\leq t})}R_{t}(h),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "which is the minimum achievable prospective risk by any learner that observes data $z{\\leq}t$ . We define the Bayes optimal learner as any learner that achieves a Bayes optimal risk at every time $t\\in\\mathbb{N}$ . In certain contexts, one might be interested in the limiting prospective Bayes risk as $t\\to\\infty$ . ", "page_idx": 2}, {"type": "text", "text": "2.1 Different prospective learning scenarios with illustrative examples ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We next discuss four prospective learning scenarios that are relevant to increasingly more general classes of stochastic processes. Our goal is to illustrate, using examples, how the definitions developed in the previous section capture these scenarios. We will assume that for all times $t$ we have $X_{t}=1$ , $Y_{t}\\in\\{0,1\\}$ . We will also focus on the time-invariant zero-one loss $\\ell(t,\\hat{y},y)=\\delta(\\hat{y}\\neq y)$ for all $t$ , here $\\delta$ is the Dirac delta function. Fig. 1 shows example realizations of the data for each scenario. ", "page_idx": 2}, {"type": "text", "text": "Scenario 1 (Data is independent and identically distributed). Formally, this consists of stochastic processes where $\\mathbb{P}_{Z_{t^{\\prime}}|Z_{\\le t}}=\\mathbb{P}_{Z_{t}}$ for all $t,t^{\\prime}\\in\\dot{\\mathbb{N}}$ . As an example, consider $Y_{t}\\sim\\mathrm{Bernoulli}(p)$ for some unknown parameter $p\\in[0,1]$ . Prospective Bayes risk is equal to $\\operatorname*{min}(p,1-p)$ in this case. A time-agnostic hypothesis, for example one that thresholds the maximum likelihood estimator (MLE) of the Bernoulli probability, converges to the limiting prospective Bayes risk.5 ", "page_idx": 2}, {"type": "text", "text": "Scenario 2 (Data is independent but not identically distributed). This consists of stochastic processes where $\\mathbb{P}_{Z_{t}|Z_{\\le t}}=\\mathbb{P}_{Z_{t}}$ for all $t\\,\\in\\,\\mathbb{N}$ . Consider $Y_{t}\\sim\\mathrm{Bernoulli}(p)$ if $t$ is odd, and $Y_{t}\\,\\sim$ Bernoulli $(1-p)$ if $t$ is even, i.e., data is drawn from two different distributions at alternate times. Prospective Bayes risk is again equal to $\\operatorname*{min}(1-p,p)$ in this case. A time-agnostic hypothesis can only perform at chance level. But a prospective learner, for example one that selects a hypothesis that alternates between two predictors at even and odd times, can converge to prospective Bayes optimal risk. We can also construct variants, e.g., when the relationship between the Bernoulli probabilities are not known (Variant 1 in Fig. 1), or when the learner does not know that the data distribution changes at every time step (Variant 2 in Fig. 1 where we implemented a generalized likelihood ratio test to determine whether the distribution changes). The risk of these variants also converges to prospective Bayes risk, but they need more samples because they use more generic models of the stochastic process. This scenario is closely related to (online) multitask/meta-learning [12]. ", "page_idx": 2}, {"type": "image", "img_path": "XEbPJUQzs3/tmp/ad60c64f2dfbb4b43dd228f343870117006241f96a73a25fc91be98240497cb9.jpg", "img_caption": ["Figure 1: A schematic for prospective learning (left) and realizations of the examples for the four scenarios (top right); dots denote 1s and empty spaces denote 0s for $Y_{t}\\in\\{0,1\\}$ with $X_{t}=1$ for all times $t$ . Prospective risk of learners at different times is shown in the bottom panels and discussed in Section 2.1. Scenario 1: For Bernoulli probability $p=0.2$ , the maximum-likelihood estimator (MLE) in blue uses a time-agnostic hypothesis $h_{t}(X_{t})=\\mathbf{\\hat{1}}(\\hat{p}_{t}>\\dot{0}.\\dot{5})$ where $\\begin{array}{r}{\\hat{p}_{t}=t^{-1}\\sum_{s=1}^{t}y_{s}}\\end{array}$ , ties at $\\hat{p}_{t}=0.5$ are broken randomly. The risk of this learner converges to the Bayes risk. Scenario 2: For Bernoulli probability $p=0.2$ , the MLE estimator (blue) performs at chance levels. A prospective learner (orange) that alternates between two predictors at even and odd times converges to Bayes risk. Variants of this learner that use less information from the stochastic process (green does not know that the data distributions at even and odd times are tied, red does not know that the distribution shifts at every time-step) also converge to Bayes risk, but more slowly. Scenario 3: For $\\theta=0.1$ and $\\gamma=0.9$ in the discounted prospective risk, the MLE estimator (blue) again performs at chance levels. A prospective learner that computes an estimate of the transition probability of the two-state Markov chain to estimate $\\mathbb{P}(Y_{t^{\\prime}}\\mid y_{t})$ for future times $t^{\\prime}>t$ converges to Bayes risk. Scenario 4: For $\\theta_{0}=\\theta_{1}=0.1$ , the MLE estimator (blue) performs at chance levels. A prospective learner that uses a variant of Q-learning (described in the text and Appendix B.3) converges to the prospective Bayes risk. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Scenario 3 (Data is neither independent nor identically distributed). Formally, this scenario consists of general stochastic processes. As an example, consider a Markov process $\\mathbb{P}(Y_{t+1}=k\\mid$ $Y_{t}\\,=\\,k)\\,=\\,\\theta$ with two states $k\\,\\in\\,\\{0,1\\}$ and $Y_{1}\\sim\\mathrm{Bernoulli}(\\theta)$ . The invariant distribution of this Markov process is $\\mathbb{P}(0)=\\mathbb{P}(1)=1/2$ . Prospective Bayes risk is also equal to $1/2$ . For stochastic processes that have a invariant distribution, it is impossible to predict the next state infinitely far into the future and therefore it is impossible to prospect. The prospective Bayes risk is trivially chance levels. In such situations, the learner could consider losses that are discounted over time. For example, one could use a slightly different loss than the one in Eq. (1) to write ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{\\ell}_{t}(h,Z)=(1-\\gamma)\\sum_{s=t+1}^{\\infty}\\gamma^{s-t-1}\\ell(h_{s}(X_{s}),Y_{s})}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "for some $\\gamma\\,\\in\\,[0,1)$ . In this example, we can calculate the prospective Bayes risk analytically; see Appendix B.2. For $\\gamma=0.9$ , $\\theta=0.1$ and the zero-one loss, limiting prospective Bayes risk is 0.357. Now consider a learner which computes the MLE of the transition matrix $\\Gamma_{t}^{t^{\\prime}-t}$ . It calculates $\\mathbb{P}(Y_{t^{\\prime}}\\mid y_{t})=\\hat{p}_{t^{\\prime}}$ where $[1\\!-\\!\\hat{p}_{t^{\\prime}},\\hat{p}_{t^{\\prime}}]=\\Gamma_{t}^{t^{\\prime}-t}[1\\!-\\!y_{t},y_{t}]^{\\top}$ and uses the hypothesis $h_{t^{\\prime}}(X_{t^{\\prime}})=\\mathbf{1}(\\hat{p}_{t^{\\prime}}>0.5)$ (ties broken randomly). We can see in Fig. 1 that this learner converges to the prospective Bayes risk. This example shows that if we model the changes in the data, then we can perform prospective learning. This scenario is closely related to certain continual learning problems [13, 14]. ", "page_idx": 3}, {"type": "text", "text": "Scenario 4 (Future depends upon the current prediction). Problems where predictions of the learner affect future data are an interesting special case of Scenario 3. Prospective learning can also be used to address such scenarios. For $\\theta_{0},\\theta_{1}\\,\\in\\,[0,1]$ , consider a Markov decision process (MDP) $\\mathbb{P}(Y_{t+1}=j\\ |\\ Y_{t}=j^{\\prime},h_{t+1}(1)=k)=\\theta_{k}$ if $j=j^{\\prime}$ and $1-\\theta_{k}$ otherwise. I.e., the prediction $h_{t+1}(X_{t+1})=k$ (recall that $X_{t}=1$ for all times) is the decision and the MDP remains in the same state with probability $\\theta_{k}$ . Prospective Bayes risk for this example is the same as that of the example in Scenario 3. We can construct a prospective learner using a variant of Q-learning to first estimate the hypothesis and then estimate the probability $\\mathbb{P}(Y_{t^{\\prime}}\\mid y_{t})$ like Scenario 3 above to predict on future ", "page_idx": 3}, {"type": "text", "text": "data at time $t^{\\prime}$ . See Appendix B.3. Prospective risk of this learner converges to Bayes risk in Fig. 1.   \nThis scenario is closely related to reinforcement learning [15]. ", "page_idx": 4}, {"type": "text", "text": "3 How is prospective learning related to other learning paradigms? 6 ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Distribution shift. Prospective learning [16] is equivalent to PAC learning [17] in Scenario 1 when data is IID. Situations when this assumption may not be valid are often modeled as a distribution shift between train and test data [2]. Techniques such as propensity scoring [18, 19] or domain adaptation [20, 21] reweigh or map the train/test data to get back to the IID setting; techniques like domain invariance [22, 1] build a statistic that is invariant to the shift. Typically, the loss is unchanged across train and test data. If the set of marginals $\\{\\mathbb{P}(Z_{t})\\}$ of the stochastic process only has two elements, then PL is equivalent to the classical distribution shift setting. But otherwise, in PL, data is correlated across time, distributions (marginals) can shift multiple times, and risk changes with time. ", "page_idx": 4}, {"type": "text", "text": "Multi-task, meta-, continual, and lifelong learning. A changing data distribution could be modeled as a sequence of tasks. Depending upon the stochastic process, different concepts are relevant, e.g., multi-task learning [23] is useful for Scenario 2 and Appendix D when there are a finite number of tasks. Much of continual or lifelong learning [14, 13] focuses on \u201ctask-incremental\u201d and \u201cclass-incremental\" settings [24], in which the learner knows when the task switches. PL does not make this assumption, and therefore, the problem is substantially more difficult. \u201cData-incremental\u201d (or task-agnostic) setting [25], is similar to PL. But the main difference is the goal: continual or lifelong learning seeks to minimize past error. As a consequence, continual learning methods are poor prospective learners; see Section 6. Online meta-learning [26\u201329] is close to task-agnostic continual learning, except that the former models tasks as being sampled IID from some distribution of tasks. Due to this, one cannot predict which task is next, and therefore cannot prospect. ", "page_idx": 4}, {"type": "text", "text": "Sequential decision making and online learning. PL builds upon works on learning from streaming data. But our goals are different. For example, Gama et al. [30] minimize the error on samples from a stationary process; Hayes et al. [31] minimize the error on a fixed held-out dataset or on all past data\u2014neither of these emphasizes prospection. There is a rich body of work on sequential decision making, e.g., predicting a finite-state, stationary ergodic process from past data [32]. Even in this simple case, there does not exist a consistent estimator using the finite past $Z_{1:t}$ [33\u201335]. This is also true for regression [36, 37], when the true hypothesis $f^{*}$ s.t. $Y_{t}=f^{*}(X_{t})$ is fixed. In other words, Bayes risk $R_{t}^{*}$ in Theorem 1 may be non-zero in PL even for finite-state, stationary ergodic processes. Hanneke [38] lifts the restriction on stationarity and ergodicity. They obtain conditions on the input process $X$ for consistent inductive (predict at time $t^{\\prime}>\\bar{t}$ using data up to $t_{.}$ ), self-adaptive (predict at time $t^{\\prime}$ using $Z{\\leq}t$ and $X_{t+1:t^{\\prime}}$ ) and online learning [39, 40] (predict at $t^{\\prime}$ using $Z_{\\le t^{\\prime}}$ ). They prove the existence of a learning rule that is consistent for every $X$ that admits self-adaptive learning. If $X$ is \u201csmooth\u201d, i.e., input marginals have a similar support over time, then ERM has a similar sample complexity as that of the IID setting [41]. Haghtalab et al. [42] give algorithmic guarantees for several online estimation problems in this setting. ", "page_idx": 4}, {"type": "text", "text": "The true hypothesis in PL can change over time. This is different from the continual learning setting where we can find a common hypothesis for tasks at all time [43], and this is why our proofs work quite differently from existing ones in the literature. Instead of a hypothesis class $\\mathcal{H}\\subseteq\\mathcal{V}^{\\mathcal{X}}$ , we define the notion of a hypothesis class that consists of sequences of predictors, i.e., subset of $(\\mathfrak{y}^{\\mathcal{X}})^{\\mathbb{N}}$ ; we can do ERM in this new space. Instead of consistency of prediction as in Hanneke [38], we give guarantees for strong learnability, i.e., convergence of the ERM risk to the Bayes risk. ", "page_idx": 4}, {"type": "text", "text": "Information theory. There are also works that have sought to characterize classes of stochastic processes that can be predicted fruitfully. Bialek et al. [44] defined a notion called predictive information (closely related to the information bottleneck principle [45]) and showed how it is related to the degrees of freedom of the stochastic process. Shalizi and Crutchfield [46] showed that a causal-state representation called an $\\epsilon$ -machine is the minimal sufficient statistic for prediction. ", "page_idx": 4}, {"type": "text", "text": "4 Theoretical foundations of prospective learning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Definition 1 (Strong Prospective Learnability). A family of stochastic processes is strongly prospectively learnable, if there exists a learner with the following property: there exists a time $t^{\\prime}(\\epsilon,\\bar{\\delta})$ such that for any $\\epsilon,\\delta>0$ and for any stochastic process $Z$ from this family, the learner outputs a hypothesis $h$ such that $\\mathbb{P}\\left[R_{t}(h)-R_{t}^{*}<\\epsilon\\right]\\geq1-\\delta$ , for any $t>t^{\\prime}$ . ", "page_idx": 4}, {"type": "text", "text": "This definition is similar to the definition of strong learnability in PAC learning with one key difference. Prospective Bayes risk $R_{t}^{*}$ depends upon the realization of the stochastic process $z{\\leq}t$ up to time $t$ . In PAC learning, it would only depend upon the true distribution of the data. Not all families of stochastic processes are strongly prospectively learnable. We therefore also define weak learnability with respect to a \u201cchance\u201d learner that predicts $\\mathbb{E}[Y]$ and achieves a prospective risk $R_{t}^{0}$ .7 ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Definition 2 (Weak Prospective Learnability). A family of stochastic processes is weakly prospectively learnable, if there exists a learner with the following property: there exists an $\\epsilon>0$ such that for any $\\delta\\,>\\,0$ , there exists a time $t^{\\prime}(\\epsilon,\\delta)$ such that for any stochastic process $Z$ from this family, $\\mathbb{P}\\left[R_{t}^{0}-R_{t}(h)>\\epsilon\\right]\\geq1-\\delta$ , for any $t>t^{\\prime}$ . ", "page_idx": 5}, {"type": "text", "text": "In PAC learning for binary classification, strong and weak learnability are equivalent [47] in the distribution agnostic setting, i.e., when strong and weak learnability is defined as the ability of a learner to learn any data distribution. But even in PAC learning, if there are restrictions on the data distribution, strong and weak learnability are not equivalent [48]. This motivates Proposition 1 below. Before that, we define a time-agnostic empirical risk minimization (ERM)-based learner. In PAC learning, ERM selects a hypothesis that minimizes the empirical loss on the training data. It outputs a time-agnostic hypothesis, i.e., using data, say, $z{\\leq}t$ standard ERM returns the same predictor for future data from any time $t^{\\prime}>t$ . There is a natural application of ERM to prospective learning problems, defined below. ", "page_idx": 5}, {"type": "text", "text": "Definition 3 (Time-agnostic ERM). Let $\\mathcal{H}$ be a hypothesis class that consists of time-agnostic predictors, i.e., $h_{t}\\,=\\,h_{t^{\\prime}}$ for any $t,t^{\\prime}\\in\\mathbb{N}$ for all predictors $h\\in\\mathcal H$ . Given data $z{<}t$ , a learner that returns ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{h}=\\underset{h\\in\\mathcal{H}}{\\arg\\operatorname*{min}}\\,\\frac{1}{t}\\sum_{s=1}^{t}\\ell(s,h_{s}(x_{s}),y_{s})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "is called a time-agnostic empirical risk minimization (ERM)-based learner. ", "page_idx": 5}, {"type": "text", "text": "Time-agnostic ERM in prospective learning may use a time-dependent loss $\\ell(s,h_{s}(x_{s}),y_{s})$ upon the training data. This ERM is not very different from standard ERM in PAC learning (when instantiated with the hypothesis class that consists of sequences of predictors, that we are interested here). If data is IID (Scenario 1), then there is no information provided by time in the training samples. But if there are temporal patterns in the data, take Scenarios 2 and 3 or Scenario 4 as examples, then time-agnostic ERM as defined here will return predictors that are different than those of standard ERM that uses a time-invariant loss. ", "page_idx": 5}, {"type": "text", "text": "Proposition 1. There exist stochastic processes for which time-agnostic ERM is not a weak prospective learner. There also exist stochastic processes for which time-agnostic ERM is a weak prospective learner but not a strong one. ", "page_idx": 5}, {"type": "text", "text": "See Appendix E for the proof. We do not know yet whether (or when) strong and weak learnability are equivalent for prospective learning. ", "page_idx": 5}, {"type": "text", "text": "5 Prospective Empirical Risk Minimization (ERM) ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In PAC learning, the hypothesis returned by ERM using the training data can predict arbitrarily well (approximate the Bayes risk arbitrarily well with arbitrarily high probability), with a sufficiently large sample size. This statement holds if (a) there exists a hypothesis in the hypothesis class whose risk matches the Bayes risk asymptotically, and (b) if risk on training data converges to that on the test data sufficiently quickly and uniformly over the hypothesis class [49, 50]. Theorem 1 is an analogous result for prospective learning. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1 (Prospective ERM is a strong prospective learner). Consider a finite family of stochastic processes $\\mathcal{Z}$ . If we have (a) consistency, i.e., there exists an increasing sequence of hypothesis classes $\\mathcal{H}_{1}\\subseteq\\mathcal{H}_{2}\\subseteq...$ with each $\\mathcal{H}_{t}\\subseteq(\\dot{\\mathcal{y}}^{\\mathcal{X}})^{\\mathbb{N}}$ such that $\\forall Z\\in\\mathcal{Z}$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\to\\infty}\\mathbb{E}\\left[\\operatorname*{inf}_{h\\in\\mathcal{H}_{t}}R_{t}(h)-R_{t}^{*}\\right]=0,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $h\\in\\mathcal{H}_{t}$ is a random variable in $\\sigma(Z_{\\leq t})$ , and $(b)$ uniform concentration of the limsup, i.e., $\\forall Z\\in\\mathcal{Z}$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\operatorname*{max}_{h\\in\\mathcal{H}_{t}}\\bigg|\\bar{\\ell}_{t}(h,Z)-\\operatorname*{max}_{u_{t}\\leq m\\leq t}\\frac{1}{m}\\sum_{s=1}^{m}\\ell(s,h_{s}(x_{s}),y_{s})\\bigg|\\right]\\leq\\gamma_{t},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "7We can also define weak learnability with respect to the prospective risk of a particular learner, even one that is not prospective. This may be useful to characterize learning for stochastic processes which do not admit strong learnability. ", "page_idx": 5}, {"type": "text", "text": "for some $\\gamma_{t}\\to0$ and $u_{t}\\to\\infty$ with $u_{t}\\leq t$ (all uniform over the family of stochastic processes), then there exists a sequence $i_{t}$ that depends only on $\\gamma_{t}$ such that a learner that returns ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\hat{h}=\\underset{h\\in\\mathcal{H}_{i_{t}}}{\\arg\\operatorname*{min}}\\underset{u_{i_{t}}\\leq m\\leq t}{\\operatorname*{max}}\\frac{1}{m}\\sum_{s=1}^{m}\\ell(s,h_{s}(x_{s}),y_{s}),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "is a strong prospective learner for this family. We define prospective ERM as the learner that implements Eq. (8) given train data $z{\\leq}t$ . ", "page_idx": 6}, {"type": "text", "text": "Appendix E.2 provides a proof, it builds upon the work of Hanneke [38]. The first condition, Eq. (6), is analogous to the consistency condition in PAC learning. In simpler words, it states that the Bayes risk can be approximated well using the chosen sequence of hypothesis classes $\\{\\mathcal{H}_{t}\\}_{t=1}^{\\infty}$ . The second condition, Eq. (7), is analogous to concentration of measure in PAC learning, it requires that the limsup in Eq. (1) is close to an empirical estimate of the limsup (the second term inside the absolute value in Eq. (7)). At each time $t$ , prospective ERM in Eq. (8) selects the best hypothesis $\\hat{h}\\in\\mathcal{H}_{t}^{8}$ for future times $t^{\\prime}>t$ , that minimizes an empirical estimate of the limsup using the training data $z{\\leq}t$ . Prospective ERM can exploit the difference between the latest datum in the training set with time $t$ and the time for which it makes predictions $t^{\\prime}$ by selecting specific sequences inside the hypothesis class $\\mathcal{H}_{t}$ . For example, in Scenario 2 it can select sequences where alternating elements can be used to predict on data from even and odd times. ", "page_idx": 6}, {"type": "text", "text": "Remark 1 (How to implement prospective ERM?). An implementation of prospective ERM is therefore not much different than an implementation of standard ERM, except that there are two inputs: time $s$ and the datum $x_{s}$ . Suppose we use a hypothesis class where each predictor is a neural network, this could be a multi-layer perceptron or a convolutional neural network. The training set $z{\\leq}t$ consists of inputs $x_{s}$ along with corresponding time instants $s$ and outputs $y_{s}$ . To implement prospective ERM, we modify the network to take $(s,x_{s})$ as input (using any encoding of time, we discuss one in Section 6) and train the network to predict the label $y_{s}$ . In Eq. (8) we can set $u_{i_{t}}\\equiv t$ , doing so only changes the sample complexity. At inference time, this network is given the input $(t^{\\prime},x_{t^{\\prime}}^{\\bar{}})$ to obtain the prediction $y_{t^{\\prime}}$ . Note that if prospective ERM is implemented in this fashion, the learner need not explicitly calculate the infinite sequence of predictors. ", "page_idx": 6}, {"type": "text", "text": "Corollary 1. There exist stochastic processes for which time-agnostic ERM is not a strong prospective learner, but prospective ERM is a strong learner. ", "page_idx": 6}, {"type": "text", "text": "Remark 2 (Why we need an increasing sequence of hypothesis classes $\\mathcal{H}_{1}\\subseteq\\mathcal{H}_{2}\\ldots)$ . We could have chosen $\\mathcal{H}_{t}=\\mathcal{H}_{t^{\\prime}}$ for all $t,t^{\\prime}\\in\\mathbb{N}$ to set up Theorem 1. But since the learner does not have a lot of data at early times, it should use a small hypothesis class. Just like PAC learning, the sequence $(\\gamma_{t})_{t\\in\\mathbb{N}}$ in Eq. (7) determines the convergence rate of a prospective learner. Therefore, using a monotonically increasing sequence of hypothesis classes is useful to ensure a good sample complexity. ", "page_idx": 6}, {"type": "text", "text": "Theorem 2. Consider a finite family of stochastic processes $\\mathcal{Z}$ . If there exists a countable hypothesis class $\\mathcal{H}$ such that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\to\\infty}\\mathbb{E}\\left[\\operatorname*{inf}_{h\\in\\mathcal{H}}R_{t}(h)-R_{t}^{*}\\right]=0,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "for any stochastic process $Z\\in{\\mathcal{Z}}$ , where $h\\in\\mathcal{H}$ is a random variable in $\\sigma(Z_{\\leq t})$ , then there exist $\\mathcal{H}_{t}$ , $u_{t}$ , and $\\gamma_{t}$ such that the two conditions of Theorem $I$ are satisfied for this family. ", "page_idx": 6}, {"type": "text", "text": "Appendix E.3 provides a proof. This theorem provides a concrete example for which the assumptions of Theorem 1 are satisfied. In PAC learning, one first proves uniform convergence for a finite hypothesis class. This can then be used to, say, calculate the sample complexity of ERM, or extended to infinite hypothesis classes using constructions such as VC-dimension and covering numbers [51]. The above theorem should be understood in the same spirit. It is a step towards characterizing the sample complexity of prospective learning. ", "page_idx": 6}, {"type": "text", "text": "Appendix C proves an analogue of Theorem 1 for prospective learning problems with discounted losses. Appendix $\\mathrm{D}$ provides illustrative examples of prospective ERM for periodic processes and hidden Markov processes. For periodic processes, we can also calculate the sample complexity. ", "page_idx": 6}, {"type": "text", "text": "6 Experimental Validation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "This section demonstrates that we can implement prospective ERM on prospective learning problems constructed on synthetic data, MNIST and CIFAR-10. In practice, prospective ERM may approximately achieve the guarantees of Theorem 1. We will focus on the distribution changing, independently or not (Scenarios 2 and 3). Recall that Scenario 1 is the same as the IID setting used in standard supervised learning problems. Scenario 4 is more involved (see an example in Appendix B.3) and, therefore, we leave more elaborate experiments for future work. We discuss experiments that check whether large language models can do prospective learning in Appendix H. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Learners and hypothesis classes. Task-agnostic online continual learning methods are the closest algorithms in the literature that can address situations when data evolves over time. We use the following three methods. ", "page_idx": 7}, {"type": "text", "text": "(i) Follow-the-Leader minimizes the empirical risk calculated on all past data and is a no-regret algorithm [52]. We note that while this is a popular online learning algorithm, we do not implement the algorithm in an online fashion.   \n(ii) Online SGD fine-tunes the network using new data in an online fashion. At every time-step, weights of the network are updated once using the last eight samples.   \n(iii) Bayesian gradient descent [53] is an online continual learning algorithm designed to address situations where the identity of the task is not known during both training and testing, i.e., it implements continual learning without knowledge of task boundaries. ", "page_idx": 7}, {"type": "text", "text": "These three methods are not explicitly designed for prospective learning but they are designed to address the changing data distribution $t$ .10 We calculate the prospective risk of the predictor returned by these methods; note that they do not output a time-varying predictor and consequently, these methods output a time-agnostic hypothesis. As a result, when we evaluate the prospective risk of these methods, we use the same hypothesis for all future time. For all three methods, we use a multi-layer perceptron (MLP) for synthetic data and MNIST, and a convolutional neural network (CNN) for CIFAR-10. ", "page_idx": 7}, {"type": "text", "text": "For prospective ERM the sequence of predictors is built by incorporating time as an additional input to an MLP or CNN as follows. For frequencies $\\omega_{i}=\\pi/i$ for $i=1,\\ldots,d/2$ , we obtain a $d\\!\\cdot$ - dimensional embedding of time $t$ as $\\varphi(t)=\\left(\\sin(\\omega_{1}t),\\ldots,\\sin(\\omega_{d/2}t),\\cos(\\omega_{1}t),\\ldots,\\cos(\\omega_{d/2}t)\\right)$ . This is similar to the position encoding in Vaswani et al. [55]. A predictor $h_{t}(\\cdot)$ uses a neural network that takes as input, an embedding of time $\\varphi(s)$ , and the input $x_{s}$ to predict the output $y_{s}$ for any time $s\\in\\mathbb{N}$ . Using such an embedding of time is useful in prospective learning because, then, one need not explicitly maintain the infinite sequence of predictors $\\boldsymbol{h}\\equiv(h_{1},\\dots,)$ . ", "page_idx": 7}, {"type": "text", "text": "Training setup. We use the zero-one error $\\mathbf{1}\\{\\hat{y}\\neq y\\}$ to calculate prospective risk for all problems; all learners are trained using a standard surrogate of this objective, the cross-entropy loss. For all experiments, for each time $t$ , we calculate the prospective risk $R_{t}(h)$ in Eq. (2) of the hypothesis created by these learners for a particular realization of the stochastic process $z{<}t$ . For each prospective learning problem, we generate a sequence of 50,000 samples. Learners are trained on data from the first $t$ time steps $(z_{\\le t})$ and prospective risk is computed using samples from the remaining time steps. Except for online SGD and Bayesian gradient descent, learners corresponding to different times are trained completely independently. See Appendix F for more details. ", "page_idx": 7}, {"type": "text", "text": "Remark 3 (Why we do not use existing benchmark continual learning scenarios). The tasks constructed below resemble continual learning benchmark scenarios such as Split-MNIST or SplitCIFAR10 [56] where data from different distributions are shown sequentially to the learner. However, there are three major differences. First, in these existing benchmark scenarios, data distributions do not evolve in a predictable fashion, and prospective learning would not be meaningful. Second, existing scenarios consider a fixed time horizon. We are keen on calculating the prospective risk for much longer horizons whereby the differences between different learners are easier to discern; our experiments go for as large as 30,000 time steps. Third, our tasks have the property that the Bayes optimal predictor changes over time. ", "page_idx": 7}, {"type": "text", "text": "6.1 Prospective learners for independent but not identically distributed data (Scenario 2) ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We create tasks using synthetic data, MNIST and CIFAR-10 datasets to design prospective learning problems when data are independent but not identically distributed across time (Scenario 2). ", "page_idx": 7}, {"type": "text", "text": "Dataset and Tasks. For the synthetic data, we consider two binary classification problems (\u201ctasks\u201d) where the input is one-dimensional. Inputs for both tasks are drawn from a uniform distribution on the set $[-2,-1]\\cup[1,2]$ . Ground-truth labels correspond to the sign of the input for Task 1, and the negative of the sign of the input for Task 2. For MNIST and CIFAR-10 we consider 4 tasks corresponding to data from classes 1-5, 4-7, 6-9 and 8-10 in the original dataset, i.e., the first task considers classes 1-5 labelled 1-5 respectively, the second task considers classes 4-7 labelled 1-4, the third task considers classes 6-9 labeled 1-4 and the last task considers labels 8-10 labelled 1-3. In other words, images from class 1 in task 1, class 4 from task 2 and class 6 from task 3 are all assigned the label 1. For the prospective learning problem based on synthetic data, the task switches every 20 time steps. For MNIST and CIFAR-10, the data distribution cycles through the 4 tasks, and the distribution of data changes every 10 time-steps. For more details, see Appendix F. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "image", "img_path": "XEbPJUQzs3/tmp/7c7181c7dc6b27e9b379995b05c3396b957cc3be7550689012ae84445d3b06f7.jpg", "img_caption": ["Figure 2: Prospective ERM can achieve good instantaneous and prospective risk in Scenario 2. Left: Instantaneous and prospective risks for problems constructed using synthetic data (see text) across 5 random seeds (which govern the sequence of samples and the weight initializations of neural networks). Instantaneous risk spikes when the task switches for many online learning baseline algorithms. In contrast, prospective ERM has minimal spikes at later times and both instantaneous and prospective risks eventually converge to zero. Right: Prospective risk for different baseline algorithms and prospective ERM for tasks constructed using MNIST and CIFAR-10 for Scenario 2. In all three cases, the risk of prospective ERM approaches Bayes risk while online learning baselines considered here do not achieve a low prospective risk. For comparison, the chance prospective risk is 0.5 for synthetic data and 0.742 for MNIST and CIFAR-10 tasks. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Fig. 2 shows that prospective ERM can learn problems when data are independent but not identically distributed (Scenario 2). For prospective learning problems constructed from synthetic data, the risk of prospective ERM converges to prospective Bayes risk over time. For the MNIST and CIFAR-10 prospective problems, the prospective learning risk drops precipitously. In contrast, online learning baselines discussed above achieve a far worse prospective risk. Observe that Follow-theLeader (blue) performs as well, or better, as online SGD and Bayesian GD. This is not surprising, while ERM models corresponding to each time $t$ were trained independently, networks corresponding to online SGD and Bayesian GD were training in an online fashion; in practice it is often difficult to tune online learning methods effectively [57].11 ", "page_idx": 8}, {"type": "text", "text": "6.2 Prospective learners when data are neither independent nor identically distributed (Scenario 3) ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Dataset and Tasks. For synthetic data, we construct 4 binary classification problems with twodimensional input data (see Fig. 3 and caption for details). For CIFAR-10 and MNIST, we consider four tasks corresponding to the classes 1-5, 4-7, 6-9 and 8-10. Using these tasks, we construct problems where the data distribution is governed by a stochastic process which is a hierarchical hidden Markov model (Scenario 3). After every 10 time-steps, a different Markov chain governs transitions among tasks (one Markov chain for tasks 1 and 2, and another for tasks 3 and 4, as shown in Fig. 3). These choices ensure that the stochastic process does not have a stationary distribution.12 ", "page_idx": 8}, {"type": "image", "img_path": "XEbPJUQzs3/tmp/a218e98ef4a41accd086a4552c71e68fe1c8965e764b21c0c7c478979518e0d2.jpg", "img_caption": ["Figure 3: Left: For MNIST and CIFAR-10, we consider 4 tasks corresponding to the classes 1-5, 4-7, 6-9 and 8-10. Using these tasks, we construct Scenario 3 problems corresponding to a stochastic process which is a hierarchical hidden Markov model. After every 10 time-steps, a different Markov chain governs transitions among tasks (one Markov chain for tasks 1 and 2, and another for tasks 3 and 4). This ensures that the stochastic process does not have a stationary distribution. Right: For synthetic data, the 4 tasks are created using two-dimensional input data as shown pictorially above. The four parts of the input domain are $\\{(x_{1},x_{2}):$ $1\\leq x_{1},x_{2}\\leq2\\}$ , $\\{(x_{1},\\bar{x}_{2}):1\\leq x_{1}\\leq\\bar{2}$ , and $-\\,2\\,\\leq\\,x_{2}\\,\\leq\\,-1\\}$ , $\\{(x_{1},x_{2}):-2\\leq x_{1},x_{2}\\leq-1\\}$ and $\\{(x_{1},x_{2}):-2\\leq x_{1}\\leq-1$ and $1\\leq x_{2}\\leq2\\}$ . Colors indicate classes. The hierarchical hidden Markov model for transitions among the tasks is identical to the MNSIT and CIFAR-10 setting shown on the left. "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "XEbPJUQzs3/tmp/9a9cb0b561bb5b482f4041db2f1f5652e82342ea2324a981125057eebba68b30.jpg", "img_caption": ["Figure 4: Prospective ERM can achieve good prospective risk in Scenario 3. Prospective risk across 5 random seeds (which govern the sequence of samples and the weight initializations of neural networks). In all three cases, the risk of prospective ERM approaches Bayes risk while a number of baseline algorithms do not achieve a low prospective risk. Stochastic processes in these problems corresponding to Scenario 3 do not have an invariant distribution. This is why a time-agnostic hypothesis (ERM) that is constructed by the baseline algorithms does not achieve a good prospective risk. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "As Fig. 4 shows, prospective ERM can prospectively learn problems when data is both independent and not identically distributed (Scenario 3. Stochastic processes in these problems corresponding to Scenario 3 do not have a stationary distribution. This is why a time-agnostic hypothesis (Follow-the-Leader) does not achieve a good prospective risk, unlike prospective ERM. Appendix $\\mathrm{G}$ discusses additional experiments for Scenario 3 for different kinds of Markov chains. ", "page_idx": 9}, {"type": "text", "text": "7 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Prospective learning, as we see it, is a paradigm of learning that characterizes many real-world scenarios which are currently modeled using much stronger (and less accurate) assumptions. These simplifying assumptions have certainly enabled progress in machine learning. But systems deployed built upon these approaches have proven to be extremely fragile in certain real-world settings. Today\u2019s machine learning-based systems fail to track changes in the data. They certainly do not model how biological organisms learn robustly and effectively over time. We believe characterizing which kinds of stochastic processes are prospectively learnable under which kinds of time-sensitive loss functions will be an important next theoretical step. Developing algorithms, from the perspective of prospective learning, which have theoretical guarantees in practice, will be another next step. Finally, building algorithms that scale and can therefore be deployed in real-world systems, will be important to demonstrate the utility of this approach. The precise real-world applications in which prospective learning based methods will outperform PAC learning, remains to be seen. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Ben Blum-Smith and Soledad Villar. Machine learning and invariant theory. arXiv preprint arXiv:2209.14991, 2023.   \n[2] Joaquin Qui\u00f1onero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence. Dataset shift in machine learning. Mit Press, 2022. [3] Peter Sterling. Allostasis: a model of predictive regulation. Physiology & behavior, 106(1): 5\u201315, April 2012. doi: 10.1016/j.physbeh.2011.06.004. [4] Xianhua Wang, Xing Zhang, Di Wu, Zhanglong Huang, Tingting Hou, Chongshu Jian, Peng Yu, Fujian Lu, Rufeng Zhang, Tao Sun, et al. Mitochondrial flashes regulate ATP homeostasis in the heart. Elife, 6:e23908, 2017. [5] Yanping Huang and Rajesh P N Rao. Predictive coding. Wiley interdisciplinary reviews. Cognitive science, 2(5):580\u2013593, September 2011. [6] Martin EP Seligman, Peter Railton, Roy F Baumeister, and Chandra Sripada. Navigating into the future or driven by the past. Perspectives on psychological science, 8(2):119\u2013141, 2013.   \n[7] Martin E P Seligman, Peter Railton, Roy F Baumeister, and Chandra Sripada. Homo Prospectus, volume 384. Oxford University Press, New York, NY, US, June 2016. ISBN 9780199374489. [8] M Kearns and L Valiant. Cryptographic limitations on learning Boolean formulae and finite automata. Journal of the ACM, 1994.   \n[9] G\u00e1bor Lugosi and K Zeger. Nonparametric estimation via empirical risk minimization. IEEE transactions on information theory / Professional Technical Group on Information Theory, 41 (3):677\u2013687, May 1995. doi: 10.1109/18.382014.   \n[10] Yann LeCun, Bernhard E Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne E Hubbard, and Lawrence D Jackel. Handwritten digit recognition with a backpropagation network. In Advances in Neural Information Processing Systems, pages 396\u2013404, 1990.   \n[11] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[12] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 1126\u20131135, 2017.   \n[13] Joshua T Vogelstein, Jayanta Dey, Hayden S Helm, Will LeVine, Ronak D Mehta, Tyler M Tomita, Haoyin Xu, Ali Geisa, Qingyang Wang, Gido M van de Ven, Chenyu Gao, Weiwei Yang, Bryan Tower, Jonathan Larson, Christopher M White, and Carey E Priebe. A simple lifelong learning approach. arXiv [cs.AI], April 2020.   \n[14] Rahul Ramesh and Pratik Chaudhari. Model Zoo: A Growing \"Brain\" That Learns Continually. In Proc. of International Conference of Learning and Representations (ICLR), 2022.   \n[15] Richard S Sutton and Andrew G Barto. Introduction to Reinforcement Learning. Camgridge: MIT Press, March 1998.   \n[16] Ashwin De Silva, Rahul Ramesh, Pratik Chaudhari, and Joshua T Vogelstein. Prospective Learning: Principled Extrapolation to the Future. In Proc. of Conference on Lifelong Learning Agents (CoLLAs), 2023.   \n[17] Vladimir Vapnik. Principles of risk minimization for learning theory. Advances in neural information processing systems, 4, 1991.   \n[18] Deepak Agarwal, Lihong Li, and Alexander Smola. Linear-time estimators for propensity scores. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pages 93\u2013100, 2011.   \n[19] Rasool Fakoor, Jonas Mueller, Zachary C. Lipton, Pratik Chaudhari, and Alexander J. Smola. Time-Varying Propensity Score to Bridge the Gap between the Past and Present. In ICLR, 2024.   \n[20] H Daume, III. Frustratingly Easy Domain Adaptation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, 2007.   \n[21] Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. Machine learning, 79(1): 151\u2013175, 2010.   \n[22] Martin Arjovsky. Out of distribution generalization in machine learning. PhD thesis, New York University, 2020.   \n[23] J. Baxter. A Model of Inductive Bias Learning. Journal of Artificial Intelligence Research, 12: 149\u2013198, March 2000. ISSN 1076-9757.   \n[24] Gido M Van de Ven and Andreas S Tolias. Three scenarios for continual learning. arXiv preprint arXiv:1904.07734, 2019.   \n[25] Matthias De Lange and Tinne Tuytelaars. Continual prototype evolution: Learning online from non-stationary data streams. In Proceedings of the IEEE/CVF international conference on computer vision, pages 8250\u20138259, 2021.   \n[26] Sebastian Thrun and Lorien Pratt. Learning to learn: Introduction and overview. In Learning to learn, pages 3\u201317. Springer, 1998.   \n[27] Guneet S Dhillon, Pratik Chaudhari, Avinash Ravichandran, and Stefano Soatto. A baseline for few-shot image classification. In Proc. of International Conference of Learning and Representations (ICLR), 2020.   \n[28] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International conference on machine learning, pages 1126\u20131135. PMLR, 2017.   \n[29] Rasool Fakoor, Pratik Chaudhari, Stefano Soatto, and Alexander J Smola. Meta-Q-Learning. In Proc. of International Conference of Learning and Representations (ICLR), 2020.   \n[30] Joao Gama, Raquel Sebastiao, and Pedro Pereira Rodrigues. On evaluating stream learning algorithms. Machine learning, 90:317\u2013346, 2013.   \n[31] Tyler L Hayes, Kushal Kafle, Robik Shrestha, Manoj Acharya, and Christopher Kanan. Remind your neural network to prevent catastrophic forgetting. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part VIII 16, pages 466\u2013483. Springer, 2020.   \n[32] Thomas M. Cover. Open problems in information theory. IEEE USSR Joint Workshop on Information Theory, 1975.   \n[33] David Harold Bailey. Sequential schemes for classifying and predicting ergodic processes. Stanford University ProQuest Dissertations Publishing, 1976.   \n[34] Boris Yakovlevich Ryabko. Prediction of random sequences and universal coding. Problems of Information Theory, 24(2):87\u201396, 1988.   \n[35] D. S. Ornstein. Guessing the next output of a stationary process. Israel Journal of Mathematics, 30(3):292\u2014-296, 1978.   \n[36] Gusztav Morvai, Sidney Yakowitz, and Laszlo Gyorf.i Nonparametric inference for ergodic, stationary time series. The Annals of Statistics, 24(1):370\u2013379, 1996.   \n[37] A.B. Nobel. Average reward reinforcement learning: Foundations, algorithms, and empirical results. IEEE Transactions on Information Theory, 49(1):83\u201398, 2003.   \n[38] Steve Hanneke. Learning whenever learning is possible: Universal learning under general stochastic processes. J. Mach. Learn. Res., 22:130:1\u2013130:116, 2021.   \n[39] Alexander Rakhlin and Karthik Sridharan. Lecture notes on online learning. https://www.mit.edu/\\~rakhlin/courses/stat928/stat928_notes.pdf, 2008.   \n[40] Shai Shalev-Shwartz et al. Online learning and online convex optimization. Foundations and Trends\u00ae in Machine Learning, 4(2):107\u2013194, 2012.   \n[41] Adam Block, Alexander Rakhlin, and Abhishek Shetty. On the performance of empirical risk minimization with smoothed data, 2024.   \n[42] Nika Haghtalab, Tim Roughgarden, and Abhishek Shetty. Smoothed analysis with adaptive adversaries. In 2021 IEEE 62nd Annual Symposium on Foundations of Computer Science (FOCS), pages 942\u2013953, 2022. doi: 10.1109/FOCS52979.2021.00095.   \n[43] Liangzu Peng, Paris Giampouras, and Rene Vidal. The ideal continual learner: An agent that never forgets. In Proceedings of the 40th International Conference on Machine Learning, 2023.   \n[44] William Bialek, Ilya Nemenman, and Naftali Tishby. Predictability, complexity, and learning. Neural computation, 13(11):2409\u20132463, 2001.   \n[45] Naftali Tishby, Fernando C. Pereira, and William Bialek. The information bottleneck method. In Proc. of the 37-Th Annual Allerton Conference on Communication, Control and Computing, pages 368\u2013377, 1999.   \n[46] Cosma Rohilla Shalizi and James P Crutchfield. Computational Mechanics: Pattern and Prediction, Structure and Simplicity. Journal of statistical physics, 104(3):817\u2013879, August 2001. doi: 10.1023/A:1010388907793.   \n[47] Robert E Schapire. The strength of weak learnability. Machine learning, 5:197\u2013227, 1990.   \n[48] Michael Kearns. Thoughts on Hypothesis Boosting. https://www.cis.upenn.edu/\\~mkearns/papers/boostnote.pdf, 1988.   \n[49] Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K Warmuth. Learnability and the vapnik-chervonenkis dimension. Journal of the ACM (JACM), 36(4):929\u2013965, 1989.   \n[50] Noga Alon, Shai Ben-David, Nicol\u00f2 Cesa-Bianchi, and David Haussler. Scale-sensitive dimensions, uniform convergence, and learnability. Journal of the ACM, 44(4):615\u2013631, July 1997.   \n[51] Vladimir N Vapnik. Statistical learning theory. Adaptive and Cognitive Dynamic Systems: Signal Processing, Learning, Commun ications and Control. John Wiley & Sons, Nashville, TN, September 1998.   \n[52] Nicolo Cesa-Bianchi and G\u00e1bor Lugosi. Prediction, learning, and games. Cambridge university press, 2006.   \n[53] Chen Zeno, Itay Golan, Elad Hoffer, and Daniel Soudry. Task agnostic continual learning using online variational bayes. arXiv preprint arXiv:1803.10123, 2018.   \n[54] Bryan Lim, Sercan \u00d6 Ar\u0131k, Nicolas Loeff, and Tomas Pfister. Temporal Fusion Transformers for interpretable multi-horizon time series forecasting. International journal of forecasting, 37 (4):1748\u20131764, October 2021.   \n[55] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, 2017.   \n[56] Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence. In International Conference on Machine Learning, pages 3987\u20133995, 2017.   \n[57] Hao Li, Pratik Chaudhari, Hao Yang, Michael Lam, Avinash Ravichandran, Rahul Bhotika, and Stefano Soatto. Rethinking the hyper-parameters for fine-tuning. In Proc. of International Conference of Learning and Representations (ICLR), 2020.   \n[58] Tilmann Gneiting and Matthias Katzfuss. Probabilistic forecasting. Annu. Rev. Stat. Appl., 1(1): 125\u2013151, January 2014.   \n[59] Leslie Valiant. Probably Approximately Correct: Nature\u2019s Algorithms for Learning and Prospering in a Complex World. Basic Books, June 2013. ISBN 9780465032716.   \n[60] Masashi Sugiyama, Matthias Krauledat, and Klaus-Robert M\u00fcller. Covariate Shift Adaptation by Importance Weighted Cross Validation. Journal of machine learning research: JMLR, 8 (May):985\u20131005, 2007. ISSN 1532-4435,1533-7928. URL http://www.jmlr.org/papers/ volume8/sugiyama07a/sugiyama07a.pdf.   \n[61] Ashwin De Silva, Rahul Ramesh, Carey Priebe, Pratik Chaudhari, and Joshua T Vogelstein. The value of out-of-distribution data. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 7366\u20137389. proceedings.mlr.press, 2023. URL https://proceedings.mlr.press/ v202/de-silva23a.html.   \n[62] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 1126\u20131135, International Convention Centre, Sydney, Australia, 2017. PMLR.   \n[63] Andreas Maurer and Tommi Jaakkola. Algorithmic stability and meta-learning. Journal of Machine Learning Research, 6(6), 2005.   \n[64] Jonathan Baxter. A model of inductive bias learning. J. Artif. Intell. Res., 12(1):149\u2013198, March 2000.   \n[65] Bernardino Romera-Paredes and Philip Torr. An embarrassingly simple approach to zero-shot learning. In International Conference on Machine Learning, pages 2152\u20132161. PMLR, June 2015. URL https://proceedings.mlr.press/v37/romera-paredes15.html.   \n[66] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. Advances in neural information processing systems, 30, 2017. ISSN 1049- 5258. URL https://proceedings.neurips.cc/paper_files/paper/2017/hash/ cb8da6767461f2812ae4290eac7cbc42-Abstract.html.   \n[67] Chelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine. Online Meta-Learning. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 1920\u20131930. PMLR, 2019.   \n[68] Sebastian Thrun. Lifelong learning algorithms. In Learning to learn, pages 181\u2013209. Springer, 1998.   \n[69] Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends\u00ae in Machine Learning, 4(2):107\u2013194, 2011.   \n[70] Mehryar Mohri. Foundations of machine learning, 2018.   \n[71] Fotios Petropoulos, Daniele Apiletti, Vassilios Assimakopoulos, Mohamed Zied Babai, Devon K Barrow, Souhaib Ben Taieb, Christoph Bergmeir, Ricardo J Bessa, Jakub Bijak, John E Boylan, Jethro Browell, Claudio Carnevale, Jennifer L Castle, Pasquale Cirillo, Michael P Clements, Clara Cordeiro, Fernando Luiz Cyrino Oliveira, Shari De Baets, Alexander Dokumentov, Joanne Ellison, Piotr Fiszeder, Philip Hans Franses, David T Frazier, Michael Gilliland, M Sinan G\u00f6n\u00fcl, Paul Goodwin, Luigi Grossi, Yael Grushka-Cockayne, Mariangela Guidolin, Massimo Guidolin, Ulrich Gunter, Xiaojia Guo, Renato Guseo, Nigel Harvey, David F Hendry, ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Ross Hollyman, Tim Januschowski, Jooyoung Jeon, Victor Richmond R Jose, Yanfei Kang, Anne B Koehler, Stephan Kolassa, Nikolaos Kourentzes, Sonia Leva, Feng Li, Konstantia Litsiou, Spyros Makridakis, Gael M Martin, Andrew B Martinez, Sheik Meeran, Theodore Modis, Konstantinos Nikolopoulos, Dilek \u00d6nkal, Alessia Paccagnini, Anastasios Panagiotelis, Ioannis Panapakidis, Jose M Pav\u00eda, Manuela Pedio, Diego J Pedregal, Pierre Pinson, Patr\u00edcia Ramos, David E Rapach, J James Reade, Bahman Rostami-Tabar, Michal Rubaszek, Georgios Sermpinis, Han Lin Shang, Evangelos Spiliotis, Aris A Syntetos, Priyanga Dilini Talagala, Thiyanga S Talagala, Len Tashman, Dimitrios Thomakos, Thordis Thorarinsdottir, Ezio Todini, Juan Ram\u00f3n Trapero Arenas, Xiaoqian Wang, Robert L Winkler, Alisa Yusupova, and Florian Ziel. Forecasting: theory and practice. International journal of forecasting, 38(3):705\u2013871, July 2022. ISSN 0169-2070. doi: 10.1016/j.ijforecast.2021.11.001. URL https://www.sciencedirect.com/science/article/pii/S0169207021001758. ", "page_idx": 14}, {"type": "text", "text": "[72] B K Ghosh and P K Sen. Handbook of Sequential Analysis (Statistics: A Series of Textbooks and Monographs). CRC Press, 1 edition, April 1991. ISBN 9780824784089. URL https: //play.google.com/store/books/details?id=JPHWDNSGCyEC. ", "page_idx": 14}, {"type": "text", "text": "[73] Nicolo Cesa-Bianchi and G\u00e1bor Lugosi. Prediction, learning, and games. Cambridge university press, 2006.   \n[74] T Chen, Yulia Rubanova, J Bettencourt, and D Duvenaud. Neural ordinary differential equations. Neural Information Processing Systems, 31:6572\u20136583, June 2018.   \n[75] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are Transformers effective for time series forecasting? Proceedings of the ... AAAI Conference on Artificial Intelligence. AAAI Conference on Artificial Intelligence, 37(9):11121\u201311128, June 2023.   \n[76] Alexander L Strehl, Lihong Li, and Michael L Littman. Reinforcement learning in finite MDPs: PAC analysis. Journal of machine learning research: JMLR, 10(84):2413\u20132444, 2009. ISSN 1532-4435,1533-7928. URL https://www.jmlr.org/papers/volume10/ strehl09a/strehl09a.pdf.   \n[77] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv [cs.LG], May 2020. URL http: //arxiv.org/abs/2005.01643.   \n[78] Saurabh Kumar, Henrik Marklund, Ashish Rao, Yifan Zhu, Hong Jun Jeon, Yueyang Liu, and Benjamin Van Roy. Continual learning as computationally constrained reinforcement learning. arXiv preprint arXiv:2307.04345, 2023.   \n[79] Annie Chen, Archit Sharma, Sergey Levine, and Chelsea Finn. You only live once: Single-life reinforcement learning. Advances in Neural Information Processing Systems, 35:14784\u201314797, 2022.   \n[80] Alex Sherstinsky. Fundamentals of recurrent neural network (RNN) and long short-term memory (LSTM) network. Physica D, 404(132306):132306, March 2020.   \n[81] Mantas Luko\u0161evic\u02c7ius and Herbert Jaeger. Reservoir computing approaches to recurrent neural network training. Comput. Sci. Rev., 3(3):127\u2013149, August 2009.   \n[82] Carl Edward Rasmussen and Christopher K I Williams. Gaussian processes for machine learning. Adaptive Computation and Machine Learning Series. MIT Press, London, England, June 2019.   \n[83] Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8:279\u2013292, 1992.   \n[84] Jonathan Baxter. A model of inductive bias learning. Journal of Artificial Intelligence Research, 12:149\u2014-198, 2000.   \n[85] Steve Hanneke. Learning whenever learning is possible: Universal learning under general stochastic processes. The Journal of Machine Learning Research, 22(1):5751\u20135866, 2021.   \n[86] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[87] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi\u00e8re, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024.   \n[88] Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, and Lidong Bing. Multilingual jailbreak challenges in large language models. arXiv preprint arXiv:2310.06474, 2023.   \n[89] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021.   \n[90] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824\u201324837, 2022. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "This work was funded by grants from the National Science Foundation (IIS-2145164, CCF-2212519) and the Office of Naval Research (ONR) N00014-22-1-2255. RR was funded by a fellowship from AWS AI to Penn Engineering\u2019s ASSET Center for Trustworthy AI. ADS was supported by a fellowship from the Mathematical Institute for Data Science (MINDS) at JHU. We are grateful to all those who provided helpful feedback on earlier drafts of this work, including Marlos Machado. ", "page_idx": 16}, {"type": "text", "text": "A Isn\u2019t this just... ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "When we describe prospective learning to people the first time, they often wonder how it is different\u2014 both conceptually and formally\u2014from other previously established learning frameworks. In fact, for many of them, the English language descriptions are seemingly identical to those which describe prospective learning. However, the English language is often imprecise and this has created a lot of confusion among both theoreticians and practitioners as to the precise differences, potential beneftis and pitfalls, between different learning frameworks. Here, we provide detailed formal distinctions between prospective learning and other related learning frameworks. Table A.1 summarizes the key distinctions between several machine learning frameworks, with further details provided below. The key difference between prospective learning and all other learning frameworks mentioned below is that in prospective learning, the hypothesis can make an inference (or take an action) arbitrarily far in the future. Certain versions of forecasting also have that property (probabilistic forecasting [58]), but forecasting has several other distinctions. ", "page_idx": 16}, {"type": "table", "img_path": "XEbPJUQzs3/tmp/6c74362b1b7d1f8e2ab719a7ab326f66c53543bbfe2db6c33837c523549110c3.jpg", "table_caption": ["Table A.1: Comparison of different machine learning frameworks in terms of the distributional assumptions on the data. Task IID indicates that data within a task are IID, and that tasks are IID from some meta-distribution. Loss characterizes whether the assumed loss function is instantaneous or time-varying. Optimal hypothesis indicates the total possible number of different optimal hypotheses (assuming each hypothesis has a unique risk). Data availability indicates whether the data are available all at once (in batch), or after each task arrives (task sequential), or one data sample at a time (sequential). The answers are given for typical settings, further details are available in the paragraphs below. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "A.1 ...PAC learning? ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "PAC learning [59] is a special case of prospective learning when the stochastic process is timeinvariant (meaning the data are IID) and the loss is fixed. Also, it is only concerned with batch data. It is an interesting question as to whether prospective learning as we have defined here is useful for IID data. We do not know yet in general. In Appendix B.1, we provide a simple example where prospection turns out to be beneficial, even in the IID setting. More broadly, we wonder whether the viewpoint proposed in this paper might lead to novel algorithms for solving learning problems on IID data that do not have closed form solutions. ", "page_idx": 16}, {"type": "text", "text": "A.2 ...transfer learning? ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In transfer learning [21], including domain (covariate) shift (adaptation) [20, 60], and out-ofdistribution (OOD) [61] learning, there are two distributions, a source and a target distribution; thus, the distribution changes only once, rather than potentially once per time step. Depending on whether the goal is to perform well only on the target, or both the source and the target, there are one or two optimal hypotheses. Also, that the distribution has changed is often known (though not always, as in OOD learning). ", "page_idx": 16}, {"type": "text", "text": "A.3 ...meta-learning? ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Meta-learning [62, 63] is similar to multi-task learning [64], and includes as special cases zeroshot [65] and few-shot learning [66]. Here, the data are Task IID, meaning that the distribution within a task is IID, and the distribution of tasks themselves is also IID, rendering it impossible to predict future distributions very well (the best one could do is guess the next task is whichever task is most likely). Typically, that the task/distribution changes is known, but not always. In classical meta-learning, data are available in one batch, but in online meta-learning, data are sequentially available [67]. The goal is to perform well on the next (unknown) distribution, as opposed to all future (unknown) distributions as in prospective learning. ", "page_idx": 17}, {"type": "text", "text": "A.4 ...lifelong learning? ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Lifelong (continual) learning [14, 13, 68] is nearly identical to online meta-learning [67]. However, the data are typically available in one batch per task. The goal is also a bit different, rather than performing well on the next distribution, in lifelong learning, the goal is also to continue performing well on previous distributions. Often, the learner is aware that the distribution shifted [24], but not always [25]. ", "page_idx": 17}, {"type": "text", "text": "A.5 ...online learning? ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "A key property of online learning [69] is that there are no distributional assumptions [70], and therefore, the goal is not about generalization error. Instead, performance is evaluated relative to the best a fixed hypothesis could have done up until now. Also, in online learning, the environment is often adversarial. ", "page_idx": 17}, {"type": "text", "text": "A.6 ...forecasting? ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Forecasting [71], time-series or sequential analysis [72, 73] assume the data follow a stochastic process, much like prospective learning often does (e.g., Scenarios 2 and 3), and therefore, the number of optimal hypothesis can be equal to the number of time steps. However, in forecasting, the loss is associated with a fixed (pre-specified) horizon, or several horizons [54]. Forecasting also often assumes a parametric model, but not always [74, 75]. Probabilistic forecasting [58] can also predict arbitrarily far in the future by iteratively updating its probabilistic forecasts. However, this is prone to numerical errors, as evidenced in sequential Monte Carlo. ", "page_idx": 17}, {"type": "text", "text": "A.7 ...reinforcement learning? ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Reinforcement learning (RL) [15] is only concerned with situations where there is a control element, that is, the hypothesis chooses an action (which potentially impacts future distributions), rather than merely an inference (which does not). Thus, it excludes Scenarios 2 and 3. Moreover, RL theory focuses on Markov Decision Processes [76], whereas PL operates on larger classes of stochastic decision processes, including non-Markov processes (e.g., see examples of non-Markov stochastic processes in Scenario 3). Depending on context, PL also considers loss functions that are instantaneous. Classical RL assumes data are sequentially available, yet offline RL operates in batch mode [77]. Perhaps most importantly, in classical RL, the optimal hypothesis (policy) is not time-varying, though recent generalizations are forthcoming [78]. Also, in RL, there are typically many episodes, whereas in prospective learning there is only a single episode (though single-episode RL is also forthcoming [79]). ", "page_idx": 17}, {"type": "text", "text": "To elaborate on the first point above, assume that our decisions do not impact the future, but the optimal hypothesis is time-dependent, that is $h_{t}^{*}\\neq h_{t^{\\prime}}^{*}$ for some $t\\neq t^{\\prime}$ . Why would we care about the risk for any $t^{\\prime}>t$ (like RL, but not like online/continual learning), given that our decision at time $t$ does not impact $Z_{t^{\\prime}}$ at all? We only ever incur the current loss, that is, $\\ell(t,h_{t}(x_{t}),y_{t})$ . So, it would seem that as long as we minimize this current loss, there is no reason to care about any future loss. First note that minimizing the expected cumulative future loss is sufficient for minimizing the loss averaged over a finite future horizon, this is formally shown in Corollary 2. But more importantly, these two problem settings are rather different. Minimizing the prospective risk (expected cumulative future loss) forces the learner to learn/model all the different modes of variation in the data. Missing even a small (low energy) mode of variation can lead to large prospective risk. If the learner only seeks to minimize the current loss, it need not have any representation of how data evolves over time. It will not be a good prospective learner. Recall that online learners (which minimize the current loss) have large worst case regret. Prospective learning effectively evaluates the regret over the infinite future horizon. The two settings are closely related if data evolves slowly, as argued in Fakoor et al. [19]. ", "page_idx": 17}, {"type": "text", "text": "A.8 ...recurrent neural networks? ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Recurrent neural networks (RNNs), including Long Short Term Memory (LSTM) networks [80], as well as echo state machines and liquid state machines (and other reservoir computing techniques [81]), and Gaussian Processes [82] seem like they are solving prospective learning problems. Indeed, they are all reasonable architectures for satisfying the conditions of Theorem 1. Insofar as they do satisfy those conditions, then they are indeed prospective learners. ", "page_idx": 18}, {"type": "text", "text": "B Calculations for scenarios in Section 2.1 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "B.1 Can learning benefit from prospection in the IID scenario? ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Consider Scenario 1 where the learner returns the hypothesis $h_{t^{\\prime}}=h_{t}=\\mathrm{threshold}(\\hat{p}_{t}>0.5)$ for all $t^{\\prime}>t$ , where $\\begin{array}{r}{\\hat{p}t\\,=\\,\\frac{1}{t}\\sum_{s=1}^{t}y_{s}}\\end{array}$ is the maximum likelihood estimator (MLE). Alternatively, if we assume a prior distribution Beta $(\\alpha,\\beta)$ over $p$ , then the resulting maximum a posteriori (MAP) estimate is p\u02c6t = \u03b1+ ts=1 zs\u22121. \u03b1+\u03b2+t\u22122 We define a second prospective learner based on MAP that returns the sequence $\\hat{h}_{\\geq t}=(\\hat{h}_{t},\\hat{h}_{t},\\cdot\\cdot\\cdot)$ , where $\\hat{h}_{t}=\\mathrm{threshold}(\\hat{p}_{t}>0.5)$ for all future times beyond $t$ . If the prior distribution has a small divergence with respect to the true posterior distribution, then the second learner converges faster to the Bayes optimal risk; for a poor choice of prior, the convergence is slower. However, in such situations, we show that we can modify the MAP-based learner to use prospection and incorporate \u201ctime\u201d to result in faster convergence to the Bayes risk. ", "page_idx": 18}, {"type": "text", "text": "Let $y_{1},\\ldots,y_{t}$ be the IID sample sequence observed up to time $t$ . The idea here is to compute the rate of change $\\Delta p(t)$ of the MAP estimate at time $t$ which is given by, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Delta p(t)=\\mathbf{M}\\mathrm{AP}(y_{1},\\ldots,y_{t})-\\mathbf{M}\\mathrm{AP}(y_{1},\\ldots,y_{t-1})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Taking expectation on both sides of Equation (10), and plugging in $\\hat{p}t=\\mathbf{M}\\mathbf{A}\\mathbf{P}(y_{1},\\ldots,y_{t})$ for the true parameter $p$ , we construct the following estimate for $\\Delta p(t)$ . ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\hat{\\Delta}p(t)=\\frac{(\\alpha-1)+t p}{(\\alpha-1)+(\\beta-1)+t}-\\frac{(\\alpha-1)+(t-1)p}{(\\alpha-1)+(\\beta-1)+t-1}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Using this rate of change, we may forecast the estimate $p_{t^{\\prime}}$ at time $t^{\\prime}>t$ as follows. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\hat{p}_{t^{\\prime}}=\\hat{p}_{t}+\\sum_{s=t}^{t^{\\prime}-1}\\hat{\\Delta}p(s)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "image", "img_path": "XEbPJUQzs3/tmp/b09e5bd3988d1d4a509ada467b1b68fd56cd9622045546645fd04162c6647d0e.jpg", "img_caption": ["Scenario 1 Independent and identically distributed data ", "Figure A.1: Prospective risk of MLE (blue), MAP (purple), and prospective MAP (red) based learners with respect to time. Both MAP and prospective MAP estimators assume a prior distribution of Beta(12, 16) over $p$ . "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "We refer to this as the prospective MAP estimate. Based on it, we set the hypothesis to be $h_{t^{\\prime}}\\,=\\,\\mathrm{threshold}(\\hat{p}_{t^{\\prime}}\\,>\\,0.5)$ for all future times beyond $t$ . In Figure A.1, we plot the prospective risk the MLE, MAP, and prospective MAP-based learners. Due to an unfavorable prior, the MAPbased learner converges slowly. However, prospective MAP-based learner manages to leverage its forecasting to achieve a faster convergence rate despite having the same prior as the MAP. This shows that we can indeed benefit from prospection even in the IID case. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "B.2 Bayes risk for a Markov chain ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We would like to compute the prospective Bayes risk, when the evolution of the samples is governed by a Markov transition matrix where $P(Y_{t+1}=0\\mid Y_{t}=0)=\\theta_{0}$ and $P(Y_{t+1}=1\\mid Y_{t}=1)=\\theta_{1}$ , i.e., the transition matrix is ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Gamma=\\left[\\!\\!\\begin{array}{c c}{{\\theta_{0}}}&{{1-\\theta_{0}\\Big]\\,.}}\\\\ {{1-\\theta_{1}}}&{{\\theta_{1}}}\\end{array}\\!\\!\\right]\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The probability distribution at time $t^{\\prime}$ is given by $\\Gamma^{t^{\\prime}-t}(z_{t},1-z_{t})^{T}$ . The eigenvalues of the transition ", "page_idx": 19}, {"type": "image", "img_path": "XEbPJUQzs3/tmp/55e5b4c70b1137ef5e8ef4962bf538d7695427aad4939b74883d47ced650126d.jpg", "img_caption": ["Figure A.2: Markov chain describing the evolution of data "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "matrix are $\\lambda_{1}\\,=\\,1$ and $\\lambda_{2}\\,=\\,\\theta_{0}+\\theta_{1}\\,-\\,1$ with the corresponding eigenvectors being $\\left(1,1\\right)^{\\top}$ and $(\\theta_{0}-1,1-\\theta_{1})^{\\top}$ . Diagonalizing $\\Gamma$ we get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Gamma^{t^{\\prime}-t}=\\left[1\\!\\!1\\!\\!\\!\\begin{array}{c c}{\\theta_{0}-1\\!\\!\\!}\\\\ {1\\!\\!\\!}&{\\!\\!\\!1-\\theta_{1}\\!\\!\\!}\\end{array}\\!\\!\\!\\right]\\left[\\lambda_{1}^{t^{\\prime}-t}\\!\\!\\!}&{\\!\\!\\!0}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which implies that the probability distribution of the state at time $t^{\\prime}$ is ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\pi_{t^{\\prime}}=\\frac{1}{(2-\\theta_{0}-\\theta_{1})}\\left[1-\\theta_{1}\\right]+\\frac{\\lambda_{2}^{t^{\\prime}-t}\\left((1-\\theta_{0})(1-z_{t})-(1-\\theta_{1})z_{t}\\right)}{(2-\\theta_{0}-\\theta_{1})}\\left[1\\!-\\!1\\right].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Hence, the optimal sequence of hypotheses is $h_{\\geq t+1}^{*}=(h_{t+1}^{*},h_{t+2}^{*},\\cdot\\cdot\\cdot)$ , where ", "page_idx": 19}, {"type": "equation", "text": "$$\nh_{t^{\\prime}}^{*}=\\arg\\operatorname*{max}_{i\\in\\{0,1\\}}\\pi_{t^{\\prime}}(i)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with Bayes risk equal to $\\begin{array}{r}{R_{t}^{*}=\\operatorname*{lim}_{T\\rightarrow\\infty}\\frac{1}{T}\\sum_{s=t}^{T}\\operatorname*{min}_{i\\in\\{0,1\\}}\\pi_{t}^{\\prime}(i)}\\end{array}$ . This reduces to ", "page_idx": 19}, {"type": "equation", "text": "$$\nR_{t}^{*}=\\frac{1}{(2-\\theta_{0}-\\theta_{1})}\\operatorname*{min}(1-\\theta_{0},1-\\theta_{1});\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "the second term in the expression of $\\pi_{t^{\\prime}}$ vanishes as $T\\to\\infty$ . If $\\theta_{0}=0.9$ and $\\theta_{1}=0.5$ , then $R_{t}^{*}=1/6$ . If we restrict our attention to the case where $\\theta_{0}=\\theta_{1}$ , the discounted Bayes risk reduces to ", "page_idx": 19}, {"type": "equation", "text": "$$\n1-\\gamma)\\sum_{s=t+1}^{\\infty}\\gamma^{s-t-1}\\ell(h_{s}^{*})=(1-\\gamma)\\sum_{s=t+1}^{\\infty}\\left(\\frac{\\gamma^{s-t-1}}{2}-\\frac{\\gamma^{s-t-1}\\bigg|\\lambda_{2}^{s-t}\\bigg|}{2}\\right)=(1-\\gamma)\\left(\\frac{1}{2(1-\\gamma)}-\\frac{|\\lambda|^{2}}{2(1-\\gamma)}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Substituting $\\theta_{0}=\\theta_{1}=0.1$ , the discount risk for $\\gamma=0.9$ is 0.357. ", "page_idx": 19}, {"type": "text", "text": "B.3 Prospective learning in Scenario 4 when the future depends upon the current prediction ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "There are two types of prospective learners\u2014one that passively observes the environment and makes inferences and another that acts on the environment and influences it. Scenario 1, Scenario 2, Scenario 3 fall into the first category which is the primary emphasis of our paper. Scenario 4 presents a prospective learning problem where the learner can influence the future realizations of the stochastic process through its decisions. ", "page_idx": 19}, {"type": "text", "text": "Our prospective learner is inspired from reinforcement learning, where the current state is $Y_{t-1}$ , the action is $h_{t}$ and the next state is $Y_{t}$ . The reward at the $t^{\\mathrm{th}}$ time-step is $r(t,h_{t+1},y_{t+1})=$ $\\mathbf{1}\\{h_{t+1}=y_{t+1}\\}$ as a result of selecting action $h_{t+1}$ given that the previous output was $y_{t}$ , and next output $y_{t+1}$ . The learner estimates the transition matrix corresponding to the MDP for each decision $h_{t+1}(X_{t+1})\\in\\{0,1\\}$ using a similar procedure as that of Scenario 3, ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "equation", "text": "$$\n\\forall k\\in\\{0,1\\}:\\ \\hat{\\Gamma}_{t}(k)=\\left[\\begin{array}{c c}{\\hat{\\theta}_{k}}&{1-\\hat{\\theta}_{k}}\\\\ {1-\\hat{\\theta}_{k}}&{\\hat{\\theta}_{k}}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\hat{\\theta}\\equiv\\hat{\\theta}_{t}\\in[0,1]^{2}$ after $t$ time steps. Using this estimate of $\\hat{\\Gamma}_{t}$ , the learner solves for the value function (corresponding to the discounted prospective risk) that satisfies ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\hat{Q}_{t}(y_{t},h_{t+1})=\\sum_{y\\in\\{0,1\\}}\\underbrace{\\mathbb{P}(Y_{t+1}=y\\mid Y_{t}=y_{t},h_{t+1}=h)}_{=\\hat{\\Gamma}_{t}(h_{t+1})_{y_{t},y}}\\left(r(t,h_{t+1},y)+\\gamma\\operatorname*{max}_{\\hat{h}}\\hat{Q}_{t}(y,\\bar{h})\\right)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The value function can be solved using value iteration, iteratively until convergence. For a given $\\hat{\\Gamma}$ , Banach\u2019s fixed point theorem guarantees this procedure will converge to the optimal value function in the tabular setting [83]. Once we have the Q-values $\\hat{Q}_{t}$ , we can use it to take actions. The optimal action at time $t^{\\prime}$ is arg $\\operatorname*{max}_{h}Q(y_{t^{\\prime}},h)$ . However, unlike reinforcement learning, we do not know $y_{t^{\\prime}}$ for $t^{\\prime}>t$ and we must instead make a sequence of decisions using state $y_{t}$ . The sequence of decision made by the learner is $\\hat{h}_{>t}=(\\hat{h}_{t+1},.~.~)$ where ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\hat{\\pi}_{t^{\\prime}}^{\\top}=\\pi_{t}^{\\top}\\prod_{s=t+1}^{t^{\\prime}}\\hat{\\Gamma}_{t}(\\hat{h}_{s-1}),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "$\\pi_{t}={(1-y_{t},y_{t})}^{\\top}$ , i.e., $\\pi_{t^{\\prime}}$ is the estimated distribution over the state at time $t^{\\prime}$ , and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\hat{h}_{t^{\\prime}+1}=\\mathop{\\arg\\operatorname*{max}}_{h}\\pi_{t^{\\prime}}^{\\top}\\hat{Q}_{t}(\\cdot,h).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In Fig. 1, we have used $\\theta_{0}\\,=\\,\\theta_{1}\\,=\\,0.1$ and a discount factor $\\gamma\\,=\\,0.9$ . We find that this learner approaches the Bayes risk (0.357 which we calculate in Appendix B.2). ", "page_idx": 20}, {"type": "text", "text": "C Prospective ERM for discounted losses ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Like we discussed in Scenario 3, in order to prospect meaningfully for some stochastic processes, we might need to consider a discounted future loss, e.g., the one in Eq. (4). Theorem 1 was proved only for the averaged future loss in Eq. (1). Here, we sketch out the proof of an analogous theorem for the discounted loss. Let ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\ell_{t}^{(\\tau)}(h,Z;\\gamma)=\\left(\\frac{1-\\gamma}{1-\\gamma^{\\tau+1}}\\right)\\sum_{s=t+1}^{t+\\tau}\\gamma^{s-t-1}\\ell(h_{s}(x_{s}),y_{s}),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\ell:\\mathcal{V}\\times\\mathcal{V}\\mapsto[0,1]$ is a bounded loss function and $0<\\gamma<1$ is a constant. In general, we can use a probability measure $\\mu^{(\\tau)}$ supported on integers $\\{1,\\ldots,\\tau\\}$ to write the loss as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\ell_{t}^{(\\tau)}(h,Z;\\mu^{(\\tau)})=\\sum_{s=t+1}^{t+\\tau}\\mu_{s-t}^{(\\tau)}\\ell(h_{s}(x_{s}),y_{s}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The averaged loss in Eq. (1) corresponds to $\\mu_{s}^{(\\tau)}\\;=\\;1/\\tau$ for all $s$ . The discounted loss above corresponds to $\\begin{array}{r}{\\mu_{s}^{(\\tau)}=\\left(\\frac{1-\\gamma}{1-\\gamma^{\\tau+1}}\\right)\\gamma^{s-1}}\\end{array}$ . ", "page_idx": 20}, {"type": "text", "text": "In prospective learning, we are interested in the case when $\\tau\\rightarrow\\infty$ and therefore let us define ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\bar{\\ell}_{t}(h,Z;\\mu)=\\operatorname*{lim}_{\\tau\\to\\infty}\\operatorname*{sup}_{t}\\ell_{t}^{(\\tau)}(h,Z;\\mu^{(\\tau)}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\mu$ denotes the collection $\\{\\mu^{(\\tau)}\\}_{\\tau=1}^{\\infty}$ . We can define $R_{t}(h;\\mu)$ and $R_{t}^{*}(\\mu)$ for this discounted loss using similar expressions as those in Eqs. (2) and (3). For clarity, let us use the notation $R_{t}(h,1/\\tau)\\equiv$ $R_{t}(h)$ and $R_{t}^{*}(1/\\tau)\\equiv R_{t}^{*}$ for the prospective risk and prospective Bayes risks corresponding to the averaged loss corresponding to $\\bar{\\mu_{s}^{(\\tau)}}=1/\\tau$ . ", "page_idx": 20}, {"type": "text", "text": "Corollary 2 (Prospective ERM is a strong learner with discounted losses). Let the assumptions of Theorem $I$ hold. If there exists a constant $c>0$ such that $\\forall Z\\in\\mathcal{Z}$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\nR_{t}(h;\\mu)-R_{t}^{*}(\\mu)\\leq c\\left(R_{t}(h,1/\\tau)-R_{t}^{*}(1/\\tau)\\right)\\quad\\forall t\\in\\mathbb{N},h\\in\\mathcal{H}_{t}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "i.e., if gap in the risk for the discounted loss is dominated uniformly by the gap in the risks for the averaged loss (over all realizations of the stochastic process), then prospective ERM implemented in Eq. (8) (implemented with the averaged loss) is a strong prospective learner, i.e., its discounted risk $\\bar{R}_{t}(\\hat{h},\\mu)$ converges to the discounted Bayes risk $R_{t}^{*}(\\mu)$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. The assumption in Eq. (12) ensures that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(\\left|R_{t}(\\hat{h},\\mu)-R_{t}^{*}(\\mu)\\right|\\geq c\\epsilon\\right)\\leq\\mathbb{P}\\left(\\left|R_{t}(\\hat{h},1/\\tau)-R_{t}^{*}(1/\\tau)\\right|\\geq\\epsilon\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for any $\\epsilon$ and $t$ . The right hand-side is shown to converge to zero in the proof of Theorem 1 and therefore the left-hand side also converges to zero. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "A corresponding Theorem 2 also holds for the discounted future loss. Note that Theorems 1 and 2 also hold in a slightly general setting when the measure $\\mu^{(\\tau)}(\\omega)$ is a random variable that depends upon the realization of the stochastic process $\\omega\\in{\\Omega}$ . ", "page_idx": 21}, {"type": "text", "text": "D An illustrative example of prospective ERM ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Suppose we have a stochastic process such that $Z_{t}\\,\\sim\\,p_{(t\\bmod T)}$ for some known period $T$ , i.e., data is independent across time but not identically distributed, and the loss function $\\ell(t,\\cdot,\\cdot)$ is timeinvariant. Scenario 2 is a special case with $T=2$ . Assume that we can find a countable hypothesis class $\\mathcal{G}$ that contains the Bayes plug-in estimator for each $p_{t}$ with $t\\,\\in\\,\\{1,\\ldots,T\\}$ . Then $\\varkappa_{T}=$ $\\{h:h_{t+T}=h_{t}$ and $h_{t}\\in\\mathcal G\\:\\forall t\\}$ satisfies Eq. (9), and it is also countable. This implies consistency and uniform concentration of the limsup for sequences in some sub-classes $\\{\\mathcal{H}_{t}\\}_{t=1}^{\\infty}$ that expands to $\\mathcal{H}_{T}$ . Note that even if we do not know the period, we can still implement prospective ERM using the hypothesis class $\\cup_{T\\in\\mathbb{N}}\\mathcal{H}_{T}$ ; this is a countable set. Prospective ERM is therefore a strong prospective learner if the period $T$ is bounded. ", "page_idx": 21}, {"type": "text", "text": "Remark 4 (Implementing prospective ERM for periodic processes). If $\\mathcal{G}$ has a finite VCdimension, choosing $\\mathcal{H}_{t}\\,=\\,\\mathcal{H}_{T}$ for any $t\\,>\\,T$ as the increasing sequence of hypothesis classes in Theorem 1 guarantees the existence of $\\begin{array}{r}{\\operatorname*{lim}_{m\\rightarrow\\infty}\\frac{1}{m}\\sum_{s=1}^{m}\\ell(s,h_{s}^{\\smile}(x_{s}),y_{s})}\\end{array}$ . We can therefore choose in Eq. (7) and thereby select ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\hat{h}=\\underset{h\\in\\mathcal{H}_{t}}{\\arg\\operatorname*{min}}\\,\\frac{1}{t}\\sum_{s=1}^{t}\\ell(s,h_{s}(x_{s}),y_{s})\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "in Eq. (8). For $\\mathcal{H}_{t}\\,=\\,\\mathcal{H}_{T}$ selected above for the periodic process, this is identical to Eq. (5). In other words, implementing prospective ERM for a periodic process boils down to solving $T$ different time-agnostic ERM problems, each using data $\\{z_{s T+k}\\}_{s=0}^{\\infty}$ , $k\\,\\in\\,\\{1,...,T\\}$ . Observe that this is precisely the prospective learner we used for the example in Scenario 2 and Fig. 1. ", "page_idx": 21}, {"type": "text", "text": "Remark 5 (Sample complexity of prospective ERM for a periodic process). We can calculate the sample complexity by exploiting the relatedness of the different distributions in the periodic process. First assume $t>T$ , i.e., at least one sample from each distribution is available. We again pick $\\mathcal{H}_{t}=\\mathcal{H}_{T}$ for all $t>T$ . Let us assume that $\\hat{h}_{t}\\in\\mathcal{G}$ for all times $t$ . Let $C\\equiv C(\\epsilon/16,\\mathcal{G}^{T})$ denote the covering number of a hypothesis class of $T$ -length sequences of hypotheses $\\mathcal G^{T}=\\{(h,\\cdot\\,.\\,.\\,,h):h\\in\\mathcal G\\}$ using balls of radius $\\epsilon/16$ with respect to loss $\\ell$ . Then, using Baxter [84, Theorem 4] we can show that, if ", "page_idx": 21}, {"type": "equation", "text": "$$\nt\\geq\\operatorname*{max}\\left\\{\\frac{64}{\\epsilon^{2}}\\log\\frac{4C(\\epsilon,\\mathcal{G}^{T})}{\\delta},\\frac{16T}{\\epsilon^{2}}\\right\\},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "then for prospective ERM in Eq. (8) we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[R_{t}(\\hat{h})\\right]\\leq\\operatorname*{lim}_{t\\rightarrow\\infty}\\mathbb{E}\\left[\\operatorname*{inf}_{h\\in\\mathcal{H}_{T}}R_{t}(h)\\right]+2\\epsilon,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "with probability at least $1-\\delta$ . The sample complexity in Eq. (13) is dominated by the first term in the curly brackets; Baxter [84, Lemma 5] shows that $\\bar{C}(\\epsilon,\\mathcal{G}^{\\bar{T}})\\leq(C(\\epsilon,\\mathcal{G}))^{T}$ . Sample complexity of prospective ERM grows at most linearly with the period $T$ , as one would expect. ", "page_idx": 21}, {"type": "text", "text": "Remark 6 (Exact sample complexity for a periodic binary classification with one-dimensional Gaussian inputs). Let the period of the stochastic process be $T=2$ with inputs $X_{t}\\in\\mathbb{R}$ and outputs $Y_{t}\\,\\in\\,\\{-1,1\\}$ . Suppose $Y_{t}\\,\\sim$ Bernoulli(0.5). The distribution $\\mathbb{P}(X_{t}\\ |\\ Y_{t}=y)$ is a Gaussian with mean $y\\mu+\\Delta(t\\bmod T)$ and variance $\\sigma^{2}$ . In words, for even times $t$ , the mean of the Gaussians are shifted to the right by $\\Delta$ . Consider the time-invariant squared error loss $\\ell(s,\\hat{y},y)=(\\hat{y}-y)^{2}$ . Choose ${\\mathcal{G}}=\\{\\mathbf{1}_{A}:A\\in\\{(-\\infty,c),(c,\\infty):c\\in\\mathbb{R}\\}\\}$ to be the set of predictors for Fisher\u2019s linear discriminant (FLD); the prospective learner selects each element of its hypothesis from $\\mathcal{G}$ . The calculations in De Silva et al. [61] for FLD can be used to show that if $\\mathcal{H}_{t}=\\{(h,h,\\cdot\\,.\\,.\\,):h\\in\\mathcal{G}\\}$ for all times $t$ , then a time-agnostic ERM has risk ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[R_{t}(\\hat{h})\\right]\\rightarrow1/2\\left[\\Phi\\left(\\Delta/(2\\sigma)-\\mu/\\sigma\\right)+\\Phi\\left(-\\Delta/(2\\sigma)-\\mu/\\sigma\\right)\\right],\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\Phi$ is the Gauss error function. But if $\\mathcal{H}_{t}=\\{(h_{1},h_{2},h_{1},h_{2},\\dots):h_{1},h_{2}\\in\\mathcal{G}\\}$ for all times $t$ , then prospective ERM can achieve Bayes risk ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[R_{t}(\\hat{h})\\right]=\\Phi\\left(-t\\mu/(2\\sigma)(t/2(t/2+1))^{-1/2}\\right)\\to\\Phi(-\\mu/\\sigma)=\\mathbb{E}\\left[R_{t}^{*}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Remark 7 (Hidden Markov Models (HMMs)). Suppose $Z$ is sampled from an HMM whose hidden states evolve according to a $k^{\\mathrm{th}}$ -order time-homogeneous Markov process with a finite state space. Select a hypothesis class $\\mathcal{H}_{k}$ that consists of sequences $h\\in\\mathcal{H}_{k}$ such that each $h$ satisfies ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\forall t\\in\\mathbb{N}:h_{t}\\in\\mathcal{G}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\forall t_{1},t_{2}\\in\\mathbb{N}:{\\mathrm{if}}\\ h_{t_{1}+s}=h_{t_{2}+s}\\ \\forall s\\in\\{1\\ldots,k\\}\\ ,{\\mathrm{then}}\\ h_{t_{1}+k+1}=h_{t_{2}+k+1}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This is the hypothesis class that contains sequences of predictors that depend only on the past $k$ predictors. If we assume, as above, that $\\mathcal{G}$ is countable, then so is $\\mathcal{H}_{k}$ . And it also satisfies Eq. (9) because of the $k^{\\mathrm{th}}$ -order Markov property. We can therefore implement prospective ERM using $\\mathcal{H}_{k}$ as the hypothesis class. Observe that in the case when the Markov process underlying the HMM is deterministic, our example models the output from an auto-regressive language model that uses greedy decoding. The length of the context window is $k$ , the hidden state of the HMM is the logit at each step (the next hidden state is a deterministic function of the previous $k$ ones), and the output of the HMM $Z_{t}$ is the next token. Our theory therefore shows that the output of such a model is prospectively learnable if the learner has access to the sequence of tokens. ", "page_idx": 22}, {"type": "text", "text": "E Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "E.1 Proof of Proposition 1 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Let $\\mathcal{X}=\\{-1,1\\}$ and ${\\mathcal{D}}=\\{0,1\\}$ . Consider two distributions $P_{1}$ and $P_{2}$ (Fig. A.3): ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{c l}{{P_{1}(X=x)=P_{2}(X=x)=\\displaystyle\\frac{1}{2}\\quad\\forall x,}}\\\\ {{P_{1}(Y=1\\mid X=x)=\\displaystyle\\left\\{\\theta\\begin{array}{l l}{{}}&{{\\mathrm{if}\\;x=1}}\\\\ {{1-\\theta}}&{{\\mathrm{if}\\;x=-1,}}\\end{array}\\right.}}\\\\ {{P_{2}(Y=1\\mid X=x)=\\displaystyle\\left\\{1-\\theta\\quad\\mathrm{if}\\;x=1\\right.}}\\\\ {{\\theta}}&{{\\mathrm{if}\\;x=-1,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "In other words, the inputs have the same marginals but the labels are flipped between $P_{1}$ and $P_{2}$ .   \nConsider a stochastic process $Z$ such that $Z_{2t+1}\\sim P_{1}$ and $Z_{2t}\\sim P_{2}$ where $t\\in\\mathbb{N}$ . ", "page_idx": 22}, {"type": "text", "text": "Let $\\mathcal{G}$ be any hypothesis class and let $\\ell(s,{\\hat{y}},y)=\\mathbf{1}({\\hat{y}}\\neq y)$ be the time-invariant zero-one loss. The time-agnostic learner uses a sequence of hypotheses $h\\equiv(h_{t})$ where $h_{t}=h_{t^{\\prime}}\\,\\forall\\,t,t^{\\prime}\\in\\mathbb{N}$ to make predictions at all times. The future loss is ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\bar{\\ell}_{t}(h,Z)=\\operatorname*{lim}_{\\tau\\rightarrow\\infty}\\frac{1}{2\\tau}\\sum_{s=t+1}^{t+2\\tau}\\ell(s,h_{s}(X_{s}),Y_{s})=R_{1}(h)+R_{2}(h)=\\frac{1}{2},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "almost surely; here $R_{1}(h)$ and $R_{2}(h)$ are risks on data from distributions $P_{1}$ and $P_{2}$ at odd and even times, respectively. The last equation follows from the fact that $R_{1}(h)\\,=\\,1\\,-\\,R_{2}(h)$ because the labels are filpped. Prospective Bayes risk is zero if the hypothesis class $\\mathcal{G}$ contains the Bayes optimal hypotheses for each of the two distributions. The future loss evaluates to $1/2$ for all realizations and so does the prospective risk. The prospective risk of a hypothesis sequence that makes random predictions (zero or one with equal probability at each instant) is also $1/2$ . This stochastic process is not weakly prospective learnable. ", "page_idx": 22}, {"type": "image", "img_path": "XEbPJUQzs3/tmp/4452b9584a3572082634971f08822c14b9c9bb6c75269b3a8fdc97f2d34c1b0e.jpg", "img_caption": ["Figure A.3: A simple stochastic process that is not weakly prospectively learnable. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "image", "img_path": "XEbPJUQzs3/tmp/81834681cb16f77367a2b8999bdf1e8d0946dd299a0adbf3816a24aafb69d65a.jpg", "img_caption": ["Figure A.4: A simple stochastic process that is weakly but not strongly prospectively learnable. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Now consider the two distributions shown in Fig. A.4, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P_{1}(X=x)=P_{2}(X=x)=\\frac{1}{3}\\quad\\forall x,}\\\\ &{\\qquad\\quad P_{1}(Y=1\\mid X=x)=\\left\\{\\theta\\begin{array}{l l}{\\qquad}&{\\mathrm{if}\\;x\\leq0}\\\\ {1-\\theta}&{\\mathrm{if}\\;x=1,}\\end{array}\\right.}\\\\ &{\\qquad\\quad P_{2}(Y=1\\mid X=x)=\\left\\{1-\\theta\\quad\\mathrm{if}\\;x\\geq0\\right.}\\\\ &{\\qquad\\quad\\left.\\mathrm{if}\\;x=-1.\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Inputs are supported on the set $\\{-1,0,1\\}$ this time. Again consider a stochastic process $Z$ such that $Z_{2t+1}\\sim P_{1}$ and $Z_{2t}\\sim P_{2}$ for $t\\in\\mathbb{N}$ . For a time-agnostic learner, since its hypothesis $h$ at each time step has to predict incorrectly at $x=0$ , we have $\\begin{array}{r}{\\check{R}_{1}(h)+R_{2}(h)\\geq\\frac{1}{3}}\\end{array}$ . The future loss is ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\bar{\\ell}_{t}(h,Z)=\\operatorname*{lim}_{\\tau\\rightarrow\\infty}\\frac{1}{2\\tau}\\sum_{s=t+1}^{t+2\\tau}\\ell(s,h(X_{s}),Y_{s})=R_{1}(h)+R_{2}(h)\\geq\\frac{1}{3}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "almost surely. It follows that the prospective risk $\\begin{array}{r}{R_{t}(h)\\geq\\frac{1}{3}}\\end{array}$ for any hypothesis. Prospective Bayes risk is again zero and therefore this stochastic process is not strongly prospectively learnable. It is however weakly learnable. ", "page_idx": 23}, {"type": "text", "text": "A hypothesis that predicts $\\hat{y}\\,=\\,\\pm1$ with equal probability has $R_{t}^{0}\\,=\\,0.5$ . If the data contains samples for $x\\in\\{-1,1\\}$ , ERM will select a hypothesis that minimizes the empirical risk which necessitates that $h(1)=0$ and $h(-1)=1$ . Therefore $\\begin{array}{r}{R_{1}(h)\\!+\\!R_{2}(h)\\leq\\frac{1}{3}+\\epsilon,}\\end{array}$ , since $h$ predicts correctly at $x=\\pm1$ , and incorrectly at $x=0$ exactly one of the two distributions. The constant $\\epsilon$ can be chosen to be $\\propto t^{-1/2}$ after receiving data from $t$ timesteps. The probability with which we do not get samples at $x=1$ or at $x=-1$ , is $2~{\\overset{-}{\\times}}~3^{-t}$ . Therefore the probability that $\\begin{array}{r}{R_{1}(h)+R_{2}(h)\\leq\\frac{1}{3}+\\stackrel{\\circ}{\\epsilon}}\\end{array}$ is at least $1-3^{-t+1}$ after $t$ time steps. This learner is therefore better than the chance learner whose risk is $R_{t}^{0}$ and it is a weak prospective learner. This shows that there exist stochastic processes that are weakly prospective learnable using time-agnostic ERM but not strongly. ", "page_idx": 23}, {"type": "text", "text": "E.2 Proof of Theorem 1 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We first show that for each $Z\\in{\\mathcal{Z}}$ , if Eqs. (6) and (7) holds, then the risk of estimator in Eq. (8) converges in probability to the Bayes optimal, i.e. ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left|R_{t}(\\hat{h}^{(t)})-R_{t}^{*}\\right|<\\epsilon\\right)\\rightarrow1.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By taking ", "page_idx": 23}, {"type": "equation", "text": "$$\nt=\\operatorname*{max}\\left\\{t:\\mathbb{P}\\left(\\left|R_{t}(\\hat{h}^{(t)})-R_{t}^{*}\\right|<\\epsilon\\right)\\geq1-\\delta\\quad\\forall t^{\\prime}\\geq t,Z\\in\\mathcal{Z}\\right\\}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "we can trivally show that the strong prospecitve learnability of estimator in Eq. (8) holds over the family $\\mathcal{Z}$ . ", "page_idx": 23}, {"type": "text", "text": "We first state a lemma that gives a choice of a random hypothesis $h^{(t)}\\in\\sigma(Z_{\\leq t})$ converging to the Bayes optimal risk under the consistency assumption. First we define the shorthand ", "page_idx": 23}, {"type": "equation", "text": "$$\ne_{m}(h)\\equiv\\frac{1}{m}\\sum_{s=1}^{m}\\ell(s,h_{s}(X_{s}),Y_{s}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Lemma 3. For a stochastic process $Z$ , for an increasing sequence of hypothesis $\\mathcal{H}_{1}\\subseteq\\mathcal{H}_{2}\\subseteq...$ such that the consistency condition in Eq. (6) is satisfied, for any $u_{t}$ satisfying $u_{t}\\leq t$ , $u_{t}\\to\\infty$ , there exists $h^{(t)}\\in\\sigma(Z_{\\leq t})$ , a $\\mathcal{H}_{t}$ -valued random variable, such that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\operatorname*{sup}_{u_{t}\\leq m\\leq\\infty}e m(h^{(t)})\\mid Z_{\\leq t}\\right]-R_{t}^{*}\\rightarrow0\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "almost surely. ", "page_idx": 24}, {"type": "text", "text": "Proof. By Eq. (6), there exists $h^{(t)}\\in\\sigma(Z_{\\leq t})$ , a $\\mathcal{H}_{t}$ -valued random variable such that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\rightarrow\\infty}\\mathbb{E}\\left[R_{t}(h^{(t)})-R_{t}^{*}\\right]=0\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Here, $h^{(t)}\\in\\sigma(Z_{\\leq t})$ means that $\\boldsymbol{h}^{(t)}$ is constant on the set $\\{Z_{\\leq t}=z_{\\leq t}\\}$ . By the assumption on $\\boldsymbol{h}^{(t)}$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\nR_{t}(h^{(t)})\\geq\\operatorname*{inf}_{h\\in\\mathcal{H}_{t}}R_{t}(h)\\geq R_{t}^{*}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "almost surely. We can choose a sub-sequence $\\{j_{k}\\}$ , such that $\\mathbb{E}\\left[R_{j_{k}}(h^{(j_{k})})-R_{j_{k}}^{*}\\right]\\leq4^{-k}$ . For all random variables $\\boldsymbol{h}^{(t)}$ satisfying the above assumption, the bounded convergence theorem implie that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[R_{t}(h^{(t)})-R_{t}^{*}]=\\mathbb{E}\\left[\\mathbb{E}\\left[\\operatorname*{lim}_{m\\to\\infty}e_{m}(h^{(t)})\\mid Z_{\\leq t}\\right]-R_{t}^{*}\\right]}\\\\ &{\\qquad\\qquad=\\mathbb{E}\\left[\\mathbb{E}\\left[\\operatorname*{lim}_{i\\to\\infty}\\operatorname*{sup}_{u_{i}\\leq m\\leq\\infty}e_{m}(h^{(t)})\\mid Z_{\\leq t}\\right]\\right]-\\mathbb{E}\\left[R_{t}^{*}\\right]}\\\\ &{\\qquad\\qquad=\\displaystyle\\operatorname*{lim}_{i\\to\\infty}\\mathbb{E}\\left[\\mathbb{E}\\left[\\operatorname*{sup}_{u_{i}\\leq m\\leq\\infty}e_{m}(h^{(t)})\\mid Z_{\\leq t}\\right]-R_{t}^{*}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "In particular, this implies that for any integer $k$ there exists an integer $i_{k}$ such that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\mathbb{E}\\left[\\operatorname*{sup}_{u_{i_{k}}\\leq m\\leq\\infty}e_{m}(h^{(j_{k})})\\mid Z_{\\leq j_{k}}\\right]-R_{j_{k}}^{*}\\right]\\leq\\mathbb{E}\\left[R_{j_{k}}(h^{(j_{k})})-R_{j_{k}}^{*}\\right]+4^{-k}\\leq2\\times4^{-k}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By the definition of limsup, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\operatorname*{sup}_{u_{i}\\leq m\\leq\\infty}e_{m}(h^{(t)})\\mid Z_{\\leq t}\\right]\\geq\\mathbb{E}\\left[\\bar{\\ell}_{t}(h^{(t)},Z)\\mid Z_{\\leq t}\\right]\\geq R_{t}^{*}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and therefore we can use Markov\u2019s inequality to get ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~\\displaystyle\\sum_{k=0}^{\\infty}\\mathbb{P}\\left(\\mathbb{E}\\left[\\operatorname*{sup}_{u_{i_{k}}\\leq m\\leq\\infty}e_{m}(h^{(j_{k})})\\mid Z_{\\leq j_{k}}\\right]-R_{j_{k}}^{*}>2^{(1/2)-k}\\right)}\\\\ &{\\leq\\displaystyle\\sum_{k=0}^{\\infty}\\frac{1}{2^{(1/2)-k}}\\,\\mathbb{E}\\left[\\mathbb{E}\\left[\\operatorname*{sup}_{u_{i_{k}}\\leq m\\leq\\infty}e_{m}(h^{(j_{k})})\\mid Z_{\\leq j_{k}}\\right]-R_{j_{k}}^{*}\\right]}\\\\ &{\\leq\\displaystyle\\sum_{k=0}^{\\infty}\\frac{2}{2^{(1/2)-k}}4^{-k}<\\infty.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, by Borel-Cantelli lemma, with probability one, there exists a (random) integer $k_{0}$ such that for all $k\\geq k_{0}$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\operatorname*{sup}_{u_{i_{k}}\\leq m\\leq\\infty}e_{m}(h^{(j_{k})})\\mid Z_{\\leq j_{k}}\\right]-R_{j_{k}}^{*}\\leq2^{(1/2)-k}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We will now scale $i_{k},\\,j_{k}$ by some integer $k_{t}$ such that $i_{k_{t}},j_{k_{t}}\\leq t$ . This ensures that $u_{i_{k_{t}}}\\leq u_{t}$ and $\\mathcal{H}_{i_{k_{t}}}\\subseteq\\mathcal{H}_{t}$ . This scaling is necessary to relate the \u201cempirical estimate\u201d of the lim sup of $\\hat{h}$ to the ", "page_idx": 24}, {"type": "text", "text": "actual lim sup of $\\boldsymbol{h}^{(t)}$ . To that end, define ", "page_idx": 25}, {"type": "equation", "text": "$$\nk_{t}=\\operatorname*{max}\\{k\\in\\mathbb{N}\\cup\\{0\\}:\\operatorname*{max}\\{i_{k},j_{k}\\}\\leq t\\}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Since $i_{k}\\leq\\infty$ and $j_{k}\\leq\\infty$ , we also have $\\operatorname*{lim}_{t\\to\\infty}k_{t}=\\infty$ . Let $\\alpha_{t}=2^{(1/2)-k_{t}}$ and notice that $\\alpha_{t}\\to0$ . We can construct an integer-valued random variable $t_{0}$ such that for all $t\\geq t_{0}$ , we have $k_{t}\\geq k_{0}$ , and therefore ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\operatorname*{sup}_{u_{i_{k_{t}}}\\leq m\\leq\\infty}e_{m}(h^{(j_{k_{t}})})\\mid Z_{\\leq j_{k_{t}}}\\right]-R_{j_{k_{t}}}^{*}\\leq\\alpha_{t}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Now choose $h^{(t)}=h^{(j_{k_{t}})}$ for every $t\\in\\mathbb{N}$ . Since $j_{k_{t}}\\leq t$ , we have $\\mathcal{H}_{j_{k_{t}}}\\subseteq\\mathcal{H}_{t}$ and $\\sigma(Z_{\\le j_{k_{t}}})\\subseteq\\sigma(Z_{\\le t})$ . This implies that $h^{(j_{k_{t}})}$ is an $\\mathcal{H}_{t}$ -valued random variable and $h^{(j_{k_{t}})}\\in\\sigma(Z_{\\leq t})$ . Also, since $i_{k_{t}}\\leq t$ and $\\{u_{t}\\}_{t=1}^{\\infty}$ is non-decreasing, we have $u_{i_{k_{t}}}\\leq u_{t}$ for all $t\\in\\mathbb{N}$ . Hence, with probability one, $\\forall t\\geq t_{0}$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\operatorname*{sup}_{u_{t}\\leq m\\leq\\infty}e_{m}(h^{(t)})\\mid Z_{\\leq j_{k_{t}}}\\right]-R_{j_{k_{t}}}^{*}\\leq\\mathbb{E}\\left[\\operatorname*{sup}_{u_{i_{k_{t}}}\\leq m\\leq\\infty}e_{m}(h^{(t)})\\mid Z_{\\leq j_{k_{t}}}\\right]-R_{j_{k_{t}}}^{*}\\leq\\alpha_{t}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Since $\\alpha_{t}\\to0$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\operatorname*{sup}_{u_{t}\\leq m\\leq\\infty}e_{m}(h^{(t)})\\mid Z_{\\leq j_{k_{t}}}\\right]-R_{j_{k_{t}}}^{*}\\rightarrow0\\;\\mathrm{a.s.}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Again using the bounded convergence theorem, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\operatorname*{sup}_{u_{t}\\leq m\\leq\\infty}e m(h^{(t)})\\mid Z_{\\leq t}\\right]-R_{t}^{*}\\rightarrow0\\operatorname{a.s.}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Now we continue the proof of Theorem 1 by exploiting the convergence of the empirical lim sup to the true lim sup. We construct a sub-sequence of integers $i_{t}$ such that $\\gamma_{i_{t}}$ in Eq. (7) decays exponentially. Markov\u2019s inequality implies that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=0}^{\\infty}\\mathbb{P}\\left(\\operatorname*{max}_{h\\in\\mathcal{H}_{i_{t}}}\\left|\\bar{\\ell}_{t}(h,Z)-\\operatorname*{max}_{u_{i_{t}}\\le m\\le i_{t}}e_{m}(h)\\right|>\\sqrt{\\gamma_{i_{t}}}\\right)}\\\\ &{\\displaystyle\\le\\sum_{t=0}^{\\infty}\\frac{1}{\\sqrt{\\gamma_{i_{t}}}}\\mathbb{E}\\left[\\operatorname*{max}_{h\\in\\mathcal{H}_{i_{t}}}\\left|\\bar{\\ell}_{t}(h,Z)-\\operatorname*{max}_{u_{i_{t}}\\le m\\le i_{t}}e_{m}(h)\\right|\\right]\\le\\sum_{t=0}^{\\infty}\\sqrt{\\gamma_{i_{t}}}<\\infty.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By the Borel-Cantelli lemma, with probability one, there exists $t_{1}\\in\\mathbb{N}$ , random, and $t_{1}\\in\\mathcal{F}$ , such that $\\forall t\\geq t_{1}$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{h\\in\\mathcal{H}_{i_{t}}}\\bigg|\\bar{\\ell}_{t}(h,Z)-\\operatorname*{max}_{u_{i_{t}}\\leq m\\leq i_{t}}e_{m}(h)\\bigg|\\leq\\sqrt{\\gamma_{i_{t}}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Let $j_{t}=\\operatorname*{max}\\{t^{\\prime}:i_{t^{\\prime}}\\leq t\\}$ , then we have $i_{j_{t}}\\rightarrow\\infty$ . Since $i_{j_{t}}\\leq t$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\bar{\\ell}_{t}(h,Z)-\\operatorname*{max}_{u_{i_{j_{t}}}\\leq m\\leq t}e_{m}(h)\\leq\\bar{\\ell}_{t}(h,Z)-\\operatorname*{max}_{u_{i_{j_{t}}}\\leq m\\leq i_{j_{t}}}e_{m}(h).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Construct a random variable $t_{2}$ such that $\\forall t\\geq t_{2}$ , we have $j_{t}\\geq t_{1}$ . Hence, we have for all $t\\geq t_{2}$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{h\\in\\mathcal{H}_{i_{j_{t}}}}{\\operatorname*{max}}\\left\\lbrace\\bar{\\ell}_{t}(h,Z)-\\underset{u_{i_{j_{t}}}\\leq m\\leq t}{\\operatorname*{max}}e_{m}(h)\\right\\rbrace}\\\\ &{\\leq\\underset{h\\in\\mathcal{H}_{i_{j_{t}}}}{\\operatorname*{max}}\\left|\\bar{\\ell}_{t}(h,Z)-\\underset{u_{i_{j_{t}}}\\leq m\\leq i_{j_{t}}}{\\operatorname*{max}}e_{m}(h)\\right|}\\\\ &{\\leq\\sqrt{\\gamma_{i_{j_{t}}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Let $i_{t}=i_{j_{t}}$ and notice that since we have $i_{t}\\to\\infty$ we also have $i_{t}\\leq t$ . This gives a sub-sequence that depends only on $\\gamma_{t}$ . Then, with probability one, $\\forall t\\geq t_{2}$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{h\\in\\mathcal{H}_{i_{t}}}\\left\\{\\bar{\\ell}_{t}(h,Z)-\\operatorname*{max}_{u_{i_{t}}\\leq m\\leq t}e_{m}(h)\\right\\}\\leq\\sqrt{\\gamma_{i_{t}}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Let $\\{h^{(t)}\\}_{t=1}^{\\infty}$ , where $h^{(t)}\\in\\sigma(Z_{\\leq t})$ is a $\\mathcal{H}_{i_{t}}$ -valued random variable chosen as in Lemma 3 with $\\mathcal{H}_{t}$ chosen to be $\\mathcal{H}_{i_{t}}$ and $u_{t}$ chosen to be $u_{i_{t}}$ . Since $\\hat{h}^{(t)}\\in\\sigma(Z_{\\leq t})$ , with probability one, for $t\\geq t_{2}$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{\\ell}_{t}(\\hat{h}^{(t)},Z)\\leq\\underset{u_{i_{t}}\\leq m\\leq t}{\\operatorname*{max}}e_{m}(\\hat{h}^{(t)})+\\sqrt{\\gamma_{i_{t}}}}\\\\ &{\\qquad\\qquad\\leq\\underset{u_{i_{t}}\\leq m\\leq t}{\\operatorname*{max}}e_{m}(h^{(t)})+\\sqrt{\\gamma_{i_{t}}}}\\\\ &{\\qquad\\qquad\\leq\\underset{u_{i_{t}}\\leq m\\leq\\infty}{\\operatorname*{sup}}e_{m}(h^{(t)})+\\sqrt{\\gamma_{i_{t}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Hence, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\bar{\\ell}_{t}(\\hat{h}^{(t)},Z)\\mid Z_{\\leq t}\\right]-\\mathbb{E}\\left[\\operatorname*{sup}_{u_{i_{t}}\\leq m\\leq\\infty}e_{m}(h^{(t)})\\mid Z_{\\leq t}\\right]\\rightarrow0\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "almost surely. By Lemma 3, we have, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\bar{\\ell}(\\hat{h}^{(t)},Z)\\mid Z\\!\\leq\\!t\\right]-R_{t}^{\\ast}\\rightarrow0\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "almost surely. Hence, by the bounded convergence theorem, ", "page_idx": 26}, {"type": "equation", "text": "$$\n0=\\mathbb{E}\\left[\\operatorname*{lim}_{t\\to\\infty}\\left(\\mathbb{E}\\left[\\bar{\\ell}(\\hat{h}^{(t)},Z)\\mid Z_{\\leq t}\\right]-R_{t}^{*}\\right)\\right]=\\operatorname*{lim}_{t\\to\\infty}\\mathbb{E}\\left[\\mathbb{E}\\left[\\bar{\\ell}(\\hat{h}^{(t)},Z)\\mid Z_{\\leq t}\\right]-R_{t}^{*}\\right]\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "which implies that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(\\left|R_{t}(\\hat{h}^{(t)})-R_{t}^{*}\\right|\\geq\\delta\\right)\\leq\\frac{1}{\\delta}\\mathbb{E}\\left[\\mathbb{E}\\left[\\bar{\\ell}(\\hat{h}^{(t)},Z)\\mid Z_{\\leq t}\\right]-R_{t}^{*}\\right]\\to0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We have therefore proved that $\\hat{h}^{(t)}$ is a strong prospective learner. ", "page_idx": 26}, {"type": "text", "text": "E.3 Proof of Theorem 2 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "This construction follows closely with Hanneke [85, Section 4], and we omit some details here. For finite class $\\mathcal{H}$ of sequence of hypothesis, we give a possible choice of $u_{t}$ with ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\to\\infty}\\mathbb{E}\\left[\\operatorname*{sup}_{t^{\\prime}\\geq t}\\operatorname*{max}_{h\\in\\mathcal{H}}\\bigg|\\bar{\\ell}_{t}(h,Z)-\\operatorname*{max}_{u_{t}\\leq m\\leq t}e_{m}(h)\\bigg|\\right]=0.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "for all process $Z$ in the finite family $\\mathcal{Z}$ . For a data sequence $\\mathbf{z}=\\{z_{s}=(x_{s},y_{s})\\}_{s=0}^{\\infty}$ and a hypothesis sequence $h\\in\\mathcal{H}$ , we define ", "page_idx": 26}, {"type": "equation", "text": "$$\nt_{u}^{h}({\\bf z})=\\operatorname*{min}\\left\\{t\\in\\mathbb{N}:t\\geq u,\\forall t^{\\prime}\\geq t\\operatorname*{sup}_{u\\leq m\\leq\\infty}e_{m}(h)\\leq\\operatorname*{max}_{u\\leq m\\leq t^{\\prime}}e_{m}(h)+2^{-u}\\right\\},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{u_{t}^{h}(\\mathbf{z})=\\operatorname*{max}\\{u\\in\\{1,\\dots,t\\}:\\;t\\geq t_{u}^{h}(\\mathbf{z})\\},}\\\\ &{u_{t}^{\\mathcal{H}}(\\mathbf{z})=\\displaystyle\\operatorname*{min}_{h\\in\\mathcal{H}}u_{t}^{h}(\\mathbf{z});}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "note that $\\mathcal{H}$ is finite and therefore the minimum exists. For the stochastic process $Z$ , let ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{u_{t}^{\\mathcal{H}}(\\delta,Z)=\\operatorname*{max}\\left\\{u\\in\\{1,\\dots,t\\}:\\mathbb{P}_{\\mathbf{z}\\sim Z}(u_{t}^{\\mathcal{H}}(\\mathbf{z})\\geq u)=1-\\delta\\right\\},}\\\\ &{\\quad u_{t}(Z)=\\operatorname*{max}\\left\\{s\\in\\mathbb{N}\\cup\\{0\\}:u_{t}^{\\mathcal{H}}(2^{-u},Z)\\geq u\\right\\},}\\\\ &{\\quad\\quad u_{t}=\\operatorname*{min}\\left\\{u_{t}(Z):Z\\in\\mathcal{Z}\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Since $\\mathcal{Z}$ is finite, we have $u_{t}\\to\\infty$ . And we also have $\\forall Z\\in\\mathcal{Z}$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(u_{t}^{\\mathcal{H}}(Z)\\geq u_{t}\\right)\\geq1-2^{-u_{t}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "By Borel-Cantelli Lemma, this construction gives a sequence $u_{t}$ s.t. Eq. (15) holds, i.e. $\\forall Z\\in\\mathcal{Z}$ , Suppose $\\{\\mathcal{H}_{t}\\}_{t=1}^{\\infty}$ is a sequence of non-empty finite sets of hypothesis sequences, and $\\{\\gamma_{t}\\}_{t=1}^{\\infty}$ is a sequence in $(0,\\infty)$ with $\\gamma_{1}\\geq1$ . Then for any finite family $\\mathcal{Z}$ , we can extend this construction to get a choice of $u_{t},\\mathcal{H}_{t}$ and $\\gamma_{t}$ such that Eq. (7) holds ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\operatorname*{max}_{h\\in\\mathcal{H}_{t}}\\left|\\bar{\\ell}_{t}(h,Z)-\\operatorname*{max}_{u_{t}\\leq m\\leq t}e_{m}(h)\\right|\\right]\\leq\\gamma_{t}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Since we have a $u_{t}$ for a given hypothesis class $\\mathcal{H}$ , for each $i\\in\\mathbb N$ , we can construct a sequence $\\{u_{i,t}\\}_{t=1}^{\\infty}$ such that $\\operatorname*{lim}_{t\\to\\infty}u_{i,t}=\\infty$ , $u_{i,t}<t$ and $\\forall Z\\in\\mathcal{Z}$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\to\\infty}\\mathbb{E}\\left[\\operatorname*{sup}_{t^{\\prime}\\geq t}\\operatorname*{max}_{h\\in\\mathcal{H}_{i}}\\bigg|\\bar{\\ell}_{t}(h,Z)-\\operatorname*{max}_{u_{i,t}\\leq m<n^{\\prime}}e_{m}(h)\\bigg|\\right]=0.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Now let ", "page_idx": 27}, {"type": "equation", "text": "$$\nt=\\operatorname*{max}\\left\\{i\\in\\{1,\\dots,t\\}:\\forall i^{\\prime}\\leq i,\\operatorname*{sup}_{t^{\\prime\\prime}\\geq t}\\mathbb{E}\\left[\\operatorname*{sup}_{t^{\\prime}\\geq t^{\\prime\\prime}}\\operatorname*{max}_{h\\in\\mathcal{H}_{i^{\\prime}}}\\left|\\bar{\\ell}_{t}(h,Z)-\\operatorname*{max}_{u_{i^{\\prime},t^{\\prime\\prime}}\\leq m\\leq t^{\\prime}}e_{m}(h)\\right|\\right]\\leq\\gamma_{i^{\\prime}}\\forall Z\\in\\mathcal{Z}\\right\\}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and ", "page_idx": 27}, {"type": "equation", "text": "$$\nt_{i}=\\operatorname*{min}\\left\\{t:j_{t}\\geq i,u_{i,t}>u_{i-1,n_{i-1}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "If $i_{t}=\\operatorname*{max}\\left\\{i:t_{i}<t\\right\\}$ , then we have $i_{t}\\to\\infty$ , and for $u_{i}=u_{i,t_{i}}$ we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\operatorname*{max}_{h\\in\\mathcal{H}_{i_{t}}}\\left|\\bar{\\ell}_{t}(h,Z)-\\operatorname*{max}_{u_{i_{t}}\\leq m\\leq t}e_{m}(h)\\right|\\right]\\leq\\gamma_{i_{t}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Since $\\mathcal{H}$ is countable, we can choose a sequence of finite hypothesis classes $\\mathcal{H}_{t}$ such that $\\cup_{t\\in\\mathbb{N}}\\mathcal{H}_{t}=\\mathcal{H}$ . By choosing $u_{t}=u_{i_{t}}$ , with $\\mathcal{H}_{t}=\\mathcal{H}_{i_{t}}$ , and $\\gamma_{t}=\\gamma_{i_{t}}$ , we now have a possible choice for $u_{t}$ , $\\mathcal{H}_{t}$ and $\\gamma_{t}$ in Theorem 1. ", "page_idx": 27}, {"type": "text", "text": "F Experimental Setup ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "F.1 Training and evaluation ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Training setup. Each learner receives a $t$ -length sequence of samples $z{<}t$ drawn from the stochastic process, as the training data. Upon training, the learner is expected to make predictions on future samples that correspond to times $t^{\\prime}>t$ up to a fixed horizon $T$ . At each future time $t^{\\prime}$ , we do not train (modify the weights) using samples after time $t$ (because we do not have them, but we will make predictions on these samples). Given samples $z{\\leq}t$ , a time-aware hypothesis class minimizes the empirical prospective risk ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\hat{\\ell}_{t}(h,Z)=\\frac{1}{t}\\sum_{s=1}^{t}\\ell(s,h_{s}(x_{s}),y_{s});\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "For an MLP or CNN, $h_{s}$ corresponds to a network that takes time $s$ as input. ", "page_idx": 27}, {"type": "text", "text": "Hyper-parameters All the networks are trained using stochastic gradient descent (SGD) with Nesterov\u2019s momentum and cosine-annealed learning rate. The networks are trained at a learning rate of 0.1 for the synthetic tasks, and learning rate of 0.01 for MNIST and CIFAR. The weight-decay is set to $1\\times10^{-5}$ . The images from MNIST and CIFAR-10 are normalized to have mean 0.5 and standard deviation 0.25. The models were trained for 100 epochs, which is many epochs after achieving a training accuracy of 1. ", "page_idx": 27}, {"type": "text", "text": "Evaluation We estimate the prospective risk of each learner using a Monte Carlo estimate. For a given training dataset $z{\\leq}t$ , we estimate a sequence predictors $h\\,\\equiv\\,(h_{t})$ which we use to make predictions on future samples. We wish to approximate the prospective risk (Equation (2)) for the estimated sequence of predictors. We do so, for a single future realization $z{>}t$ of this process, which yields the estimate ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\hat{R}_{t}(h)=\\frac{1}{(T-t)}\\sum_{s=t+1}^{T}\\ell\\left(s,h_{s}(x_{s}^{j}),y_{s}^{j})\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "In our experiments, $T=50{,}000$ for CIFAR-10 and MNIST while $T=10{,}000$ for the synthetic data experiments. For a single learning algorithm, we compute the empirical prospective risk at 15-40 different time steps which results in a significant number of GPU hours in order to plot the learning curves. For every time step, we compute the mean and standard deviation of the empirical prospective risk using 5 random seeds. ", "page_idx": 28}, {"type": "text", "text": "F.2 Architectural Details ", "text_level": 1, "page_idx": 28}, {"type": "image", "img_path": "XEbPJUQzs3/tmp/84509b16e86f056dbb19472b89a87aa18d81a6c8383c51a702a49e784994b6d6.jpg", "img_caption": ["Figure A.5: Schematic illustration prospective-MLP and prospective-CNN. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "We considered the following architecture choices for the time-agnostic restropective algorithms like ERM that ignore time and the ordering associated with the samples in $z{\\leq}t$ . ", "page_idx": 28}, {"type": "text", "text": "Retrospective-MLP/CNN. A multi-layer perceptron (MLP) with two hidden layers with 256 units is used for the synthetic tasks and the MNIST task. For CIFAR-10, we use a a small convolutional network with 0.12M parameters. It comprises of 3 convolution layers (kernel size 3 and 80 filters) interleaved with max-pooling, ReLU, batch-norm layers, with a fully-connected classifier layer. ", "page_idx": 28}, {"type": "text", "text": "Prospective ERM with MLP and CNNs. In order to incorporate time into the hypothesis class, we consider an embedding function $\\varphi:\\mathbb{R}\\,\\rightarrow\\,\\mathbb{R}^{d}$ that takes raw time as an input and returns a $d$ -dimensional vector denoted as the time-embedding. In our experiments, we define $\\varphi:\\mathbb{R}\\to\\mathbb{R}^{d}$ as a function that maps ", "page_idx": 28}, {"type": "equation", "text": "$$\nt\\mapsto(\\sin(\\omega_{1}t),\\cdot\\cdot\\cdot,\\sin(\\omega_{d/2}t),\\cos(\\omega_{1}t),\\cdot\\cdot\\cdot,\\cos(\\omega_{d/2}t)),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where, $\\omega_{i}=\\pi/i,\\;i=1,\\ldots,d/2$ to the be the collection of angular frequencies. We briefly discuss the rationale for this choice in Figure A.7. In our experiments, we use $d=50$ . ", "page_idx": 28}, {"type": "text", "text": "We make our classifiers a function of time by including time $t$ as an input the neural network. This allows the network to vary its hypothesis over time. For MLPs, we concatenate the input with its corresponding time-embedding $\\varphi(t)$ which is fed as input. For the CNN (see Figure A.5), we add the time-embedding to the output of the convolutional layers instead of concatenating it to the inputs. We also tried concatenating the time-encoding to the inputs of the CNN but found that it performed poorly in both scenarios 2 and 3 (see Figure A.6). ", "page_idx": 28}, {"type": "text", "text": "Frequencies for embedding time In the original Transformer architecture, Vaswani et al. [55] use a position-embedding using the frequencies $\\omega_{i}=1/10000^{2i/d}\\;\\;i=1,\\ldots,d/2$ . There are two key differences: (1) We use the absolute time $t$ instead of the relative position, (2) We use the angular frequencies $2\\pi/i$ . In Figure A.7 $(r i g h t)$ , we illustrate the time-embeddings when we use the two different choices for angular frequencies. For $d=128$ , we find that the frequencies from Vaswani et al. [55] result in slowly changing features which makes it less suitable for our task, i.e., many of the dimensions are constant over time which makes many of the dimensions uniformative for the task. In our experiments, we found out that MLPs and CNNs that use the frequencies from Vaswani et al. [55] perform poorly on the MNIST task for Scenario 2 and 3. ", "page_idx": 28}, {"type": "image", "img_path": "XEbPJUQzs3/tmp/5256dd46b6d13ac9d543b28ca67924fc80b64bd5e41fad3574f3f6c204e50894.jpg", "img_caption": ["Figure A.6: Prospective risk of the CNN architecture on CIFAR-10 for scenarios 2 and 3. The performance of the CNN architecture is significantly worse when the time-embedding is concatenated to the input (variant 2). "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "XEbPJUQzs3/tmp/3eff21a8f0a8c7a73e118b7bf3289288f8f4c10998e8ba994f6a19de7681f8ad.jpg", "img_caption": ["Figure A.7: The time-embeddings computed using (1) frequencies from Vaswani et al. [55] (left), and (2) the frequencies from proposed in our work (right). "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "G Additional experiments for Scenario 3 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We conduct more experiments on prospective learning problems in addition to the ones in Section 6.2. ", "page_idx": 29}, {"type": "text", "text": "G.1 Markov chain with periodic resets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Dataset and Tasks. For synthetic data, we consider the 2 binary classification problems described in Section 6.1. For CIFAR-10 and MNIST, we consider 2 tasks corresponding to the classes 1-5, and the classes 1-5 but with each class $y$ relabeled to $(y+1)$ (mod 5). Using these tasks, we construct Scenario 3 problems corresponding to a stochastic process which is a hidden Markov model on two states. The tasks are governed by a Markov process with transition matrix $P(S_{t+1}=k\\mid S_{t}=k)=0.1$ , where $S_{t}$ is the task at time $t$ . Additionally after every 10 time-steps, the state of the Markov chain is reset to the first task. This ensures that the stochastic process does not have a stationary distribution. Similar to the previous experiments, for each problem, we generate a sequence of 50,000 samples. Learners are trained on data from the first $t$ time steps $(z_{\\le t})$ and prospective risk is computed using samples from the remaining time steps. ", "page_idx": 29}, {"type": "text", "text": "Learners and hypothesis classes. For this scenario, we conduct experiments using follow-theleader and prospective ERM. Both methods use MLPs for synthetic and MNIST tasks, and a CNN for the CIFAR-10 task. Note that prospective ERM uses an embedding of time as input in addition to the datum. Training and evaluation setup is identical to that of Scenario 2. ", "page_idx": 29}, {"type": "text", "text": "As Figure A.8 shows, prospective ERM can also prospectively learn problems in Scenario 3 when data is neither independent nor identically distributed. ", "page_idx": 29}, {"type": "text", "text": "G.2 Stationary Markov chain ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Dataset and Tasks. For synthetic data, we consider the 2 binary classification problems described in Section 6.1. For CIFAR-10 and MNIST, we consider 2 tasks corresponding to the classes 1-5, and the classes 1-5 but with each class $y$ relabeled to $(y+1)$ mod 5. Using these tasks, we construct Scenario 3 problems corresponding to a stochastic process which is a hidden Markov model on 2 states. The tasks are governed by a Markov process with transition matrix $P(S_{t+1}=k\\mid S_{t}=k)=0.1$ , where $S_{t}$ is the task at time $t$ . Unlike the previous subsection (Figure A.8), in this experiment, the Markov chain equilibriates to the stationary distribution. Similar to the previous experiments, for each problem, we generate a sequence of 50,000 samples. Learners are trained on data from the first $t$ time steps $(z_{\\le t})$ and prospective risk is computed using samples from the remaining time steps. ", "page_idx": 29}, {"type": "text", "text": "Learners and hypothesis classes. For this scenario, we conduct experiments using follow-theleader and prospective ERM. Both methods use MLPs for the synthetic and MNIST tasks, and a CNN for the CIFAR-10 task. Note that prospective ERM uses an embedding of time as input in addition to the datum. Training is identical to that of Scenario 2. For evaluation, we compute the empirical prospective risk in Fig. A.9 and empirical discounted prospective risk in Fig. A.10. ", "page_idx": 29}, {"type": "image", "img_path": "XEbPJUQzs3/tmp/bf8442301114158428ab2e2ef097a4e7533107724bd05352dfa3f69acb0cecb9.jpg", "img_caption": ["Figure A.8: Prospective ERM can achieve good prospective risk in Scenario 3. We plot the prospective risk across 5 random seeds (which govern the sequence of samples and the weight initialization of the neural networks). In all three cases, the risk of prospective ERM approaches Bayes risk while Follow-the-Leader does not achieve a low prospective risk. Bayes risk for MNIST and CIFAR-10 problems is calculated by assuming that Bayes risk on individual tasks is zero. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "", "page_idx": 30}, {"type": "image", "img_path": "XEbPJUQzs3/tmp/b7d84cead19f9df2772715b6cdf1c054702482ba2b7a3e73ad33aa336e00d0ca.jpg", "img_caption": ["Figure A.9: For a task defined on a stationary Markov process, the Bayes risk is trivial and can be achieved by a hypothesis that doesn\u2019t change over time. We plot the prospective risk across 5 random seeds (which govern the sequence of samples and the weight initialization of the neural networks). In all three cases, both follow-the-leader and prospective ERM approach the Bayes risk. The stationary distribution has an equal probability of seeing either task and a fixed hypothesis can achieve Bayes risk on this problem. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "H Large language models may not be good prospective learners ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "It is an interesting question whether LLMs which are trained using auto-regressive likelihoods with Transformer-based architectures can do prospective learning. To study this, we used LLama-7B [86] and Gemma-7B [87] to evaluate the prospective risk for Scenarios 1 to 3. The prompt contains a few samples from the stochastic process (sub-sequences of $\\left(Y_{t}\\right)$ consisting of 0s and 1s) and an English language description of the family of stochastic processes that generated the samples. The LLM is tasked with completing the prompt with the next 20 most likely sequence of samples. ", "page_idx": 30}, {"type": "text", "text": "Selecting the appropriate prompt LLMs can be brittle and are known to generate different completions depending on if the prompt was in English, Thai or Swahili [88]. This makes it difficult to evaluate prospective learning in LLMs. Therefore, in our experiments, we do not describe prospective risk or other details about prospective learnability in the prompt. We simply describe the data generating process and some samples from this process in the prompt and prompt the model to generate the most likely completion. The prompts are described in detail in Appendix H.1; we also experimented with a few variants of these prompts. ", "page_idx": 30}, {"type": "image", "img_path": "XEbPJUQzs3/tmp/0bbd615c529071ab16ea9f909062d8ffe0e0962ff9e1fd82dea245fb74320692.jpg", "img_caption": ["Figure A.10: Both prospective ERM and follow-the-leader achieve similar discounted prospective risks (with discount factor 0.95). We plot the discounted prospective risk across 5 random seeds. Both follow-theleader and prospective ERM achieve similar discounted risks. Note that the error bars are larger since the risk is computed over fewer samples, i.e., the discount factor reduces the effective number of test data points. "], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "XEbPJUQzs3/tmp/d8aa8ba7310d9619434ba477d1a3edd09dcc8263fb8431bfa08fd16c35ab1756.jpg", "img_caption": ["Figure A.11: The prospective risk of LLMs when evaluated on the three scenarios, when averaged over draws of the training data. The LLM does not improve with more data unlike a prospective MLE-learner. This suggests that LLMs are incapable of prospection. "], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "XEbPJUQzs3/tmp/3bee61cff4fa766f08ad18402339bcdebc96f97cb383d33b85b4e34f0b234520.jpg", "img_caption": ["Figure A.12: We prompt LLMs to generate the outcomes of 10 Bernoulli trials with $p=0.75$ . We plot the probability of generating token 1 over all possible sequences of 10 Bernoulli trials and find that the outcomes are generated with probabilities that range from 0 to 1 with an average of 0.5. Ideally, the token 1 should should always be generated with $p=0.75$ , i.e., the LLMs cannot simulate outcomes of a Bernoulli distribution. "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "We use greedy decoding to generate a sequence of tokens, i.e., the token with the highest probability is sampled at every step. We vary the number of time-steps in the prompt from 1 to 100 which corresponds to the amount of training data. For a particular value of time $t$ , we generate 20 more tokens and compute (an estimate of) the prospective risk of this completion; this is the test data. We report the prospective risk computed on 100 different realizations of the stochastic process, i.e., each point in Fig. A.11 is the prospective risk on the next 20 samples, averaged over 100 realizations of the training data. In Fig. A.11, we find that LLMs do not obtain better prospective risk with more samples, i.e., Llama-7B and Gemma-7B do not seem to be doing prospective learning. It is quite surprising that they do not achieve Bayes risk even on independent and identically distributed data. We note that these experiments do not definitively answer whether LLMs can learn prospectively. ", "page_idx": 31}, {"type": "text", "text": "Can LLMs even generate outcomes of a sequence of Bernoulli trials? We prompted an LLM to generate a sequence of 0s and 1s sampled from a Bernoulli distribution with probability $p=0.75$ . We then plot the probability of generating each 0 or 1, for all sequences of length 10 in Fig. A.12. Ideally, the strip plot would be concentrated around 0.25 for 0 and 0.75 for 1, i.e., 0s should be generated with frequency close to 0.25. However, we find this is not the case and LLMs seem incapable of even generating a sequence of Bernoulli trials. This provides some context to the results discussed above. LLMs do not seem to be doing prospective learning, but they cannot even sample from a Bernoulli distribution under these experimental conditions.13 ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "H.1 Prompts for testing prospective learning in LLMs ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "We use the following 3 prompts to generate a sequence of predictions using in LLama-7B and Gemma-7B. We found that the LLMs always generated a sequence of 0s and 1s and we did not need to post-process the response or change how the tokens were sampled. We generate 20 samples using greedy decoding; the language models are executed with the weights in 16-bit precision. We tried a few different variants for providing prompts to the LLM, e.g., by adding spaces between the 1s and 0s, the results are qualitatively similar. ", "page_idx": 32}, {"type": "text", "text": "Scenario ", "text_level": 1, "page_idx": 32}, {"type": "table", "img_path": "XEbPJUQzs3/tmp/8d523e40bf3d22d9239089edf467bd789c5d197ad10a349aa85508643695718f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 32}, {"type": "text", "text": "To make the LLM generate a sequence of Bernoulli trials with probability 0.75, we used the following prompt. ", "page_idx": 32}, {"type": "text", "text": "Bernoulli trials ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Generate outcomes of 10 Bernoulli trials where 0 is generated with probability 0.25 and 1   \nwith probability 0.75 ", "page_idx": 32}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We provide the list of contributions in Section 1 with pointers to this section from the introduction. ", "page_idx": 33}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Section 7 discusses the limitations of our theoretical and empirical results. ", "page_idx": 33}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We have developed substantial new theoretical results in this paper. They are discussed in Section 4. All mathematical claims, illustrative examples, and remarks come with elaborate proofs, both in the main paper and the Appendix. ", "page_idx": 33}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We use standard visual recognition datasets along with illustrative examples on synthetic tasks. All information to reproduce our experiments, e.g., datasets, network architectures, and training procedures, is provided in Appendix F. All the code that was used to conduct the experiments has been made public. ", "page_idx": 33}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: All the code that was used to conduct the experiments has been made public. ", "page_idx": 33}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We have conducted extremely thorough train/test splits, and tuned hyperparameters manually across multiple runs. This information is provided in Appendix F. ", "page_idx": 33}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: For all the learning algorithms considered in every experiment in our paper, we compute empirical prospective and instantaneous risks over multiple random seeds, at 15-40 different time steps. This makes our results extremely rigorous and reproducible. All experimental results in the paper come with error bars assuming a Normal distribution for the relevant quantities. ", "page_idx": 33}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We have conducted experiments on this problem for close to 1.5 years. In addition to the experimental results that are discussed here, there a substantial number of secondary experiments, failed attempts, that are not reported. All the experiments in this paper approximately takes $\\sim200$ GPU hours to run. ", "page_idx": 34}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We did not use human subjects or participants in these experiments. All our datasets are public ones that are used very commonly in the published literature, or synthetically generated. We do not anticipate societally harmful consequences of this research. ", "page_idx": 34}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: This is foundational work and it is not tied to any particular application or deployment. We do not anticipate negative social impact. Down the line, when this line of research matures and it is deployed for making predictions on users, we anticipate a lot of positive social impacts, e.g., prospective learning may enable us to deploy ML models without having to retrain them as data changes over time. This improves performance because it prevents the predictions from deteriorating, but it also leads to other benefits in terms of economic and computational resources necessary to serve ML models. ", "page_idx": 34}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: We only use existing public datasets (appropriately cited in the paper) and synthetic data. ", "page_idx": 34}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We only use existing public datasets (appropriately cited in the paper) and synthetic data. No new assets are created from this paper. ", "page_idx": 34}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: No assets are created in this paper. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: No crowdsourcing experiments or research with human subjects. ", "page_idx": 35}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] Justification: No crowdsourcing experiments or research with human subjects. ", "page_idx": 35}]