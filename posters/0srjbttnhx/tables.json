[{"figure_path": "0SRJBtTNhX/tables/tables_5_1.jpg", "caption": "Table 1: Semi-supervised node classification accuracy(%) on medium-scale graphs. The average result of 30 runs is reported on five datasets.", "description": "This table presents the results of semi-supervised node classification experiments conducted on five medium-scale graph datasets (Cora, CiteSeer, Pubmed, CS, and Physics).  Multiple Graph Neural Network (GNN) models (GCN, GAT, SAGE, APPNP) were evaluated, each with several data augmentation strategies (Original, GraphMix, CODA, DropMessage, MH-Aug, LA-GNN/GAT/SAGE/APPNP, NodeMixup, and IntraMix).  The table shows the average accuracy (with standard deviation) achieved by each GNN model under each augmentation strategy across 30 runs, highlighting the impact of the proposed IntraMix method.", "section": "4.1 Semi-supervised Learning"}, {"figure_path": "0SRJBtTNhX/tables/tables_6_1.jpg", "caption": "Table 1: Semi-supervised node classification accuracy(%) on medium-scale graphs. The average result of 30 runs is reported on five datasets.", "description": "This table presents the results of semi-supervised node classification experiments on five medium-scale graph datasets (Cora, CiteSeer, Pubmed, CS, and Physics).  Multiple graph neural network (GNN) models (GCN, GAT, SAGE, APPNP) were evaluated, along with several data augmentation strategies (Original, GraphMix, CODA, DropMessage, MH-Aug, LA-GCN/GAT/SAGE/APPNP, NodeMixup, and IntraMix). The table shows the average classification accuracy (%) achieved by each GNN model with different augmentation methods over 30 independent runs.  The results provide a comparison of the effectiveness of different graph augmentation techniques in semi-supervised node classification.", "section": "4.1 Semi-supervised Learning"}, {"figure_path": "0SRJBtTNhX/tables/tables_6_2.jpg", "caption": "Table 3: Node Classification in inductive settings.", "description": "This table presents the results of node classification experiments conducted in inductive learning settings.  Inductive learning differs from transductive learning in that the test data distribution is unknown during training. The table compares the performance of different GNN models (GAT and SAGE) using various augmentation strategies (Original, LAGAT/LAGSAGE, NodeMixup, IntraMix) on two datasets (Cora and CiteSeer). The results highlight the effectiveness of IntraMix in improving the accuracy of node classification, even when the model is trained without knowledge of the test data distribution.", "section": "4.3 Inductive Learning"}, {"figure_path": "0SRJBtTNhX/tables/tables_7_1.jpg", "caption": "Table 4: Ablation of Intra-Class Mixup on GCN. w con is vallina mixup connection, and sim con is similar connection. \u2191 is the improvement.", "description": "This table presents the ablation study of Intra-Class Mixup using GCN on Cora, CiteSeer, and Pubmed datasets. It compares the performance of the original GCN, GCN with only pseudo-labeling (PL), GCN with an advanced PL method (UPS), GCN with vanilla Mixup (without connection, with connection using nodes involved in Mixup, and with similar connection using nodes with similar embeddings), and GCN with Intra-Class Mixup. The results show the improvement in accuracy achieved by Intra-Class Mixup compared to other methods.", "section": "4.4 Ablation Experiment"}, {"figure_path": "0SRJBtTNhX/tables/tables_7_2.jpg", "caption": "Table 1: Semi-supervised node classification accuracy(%) on medium-scale graphs. The average result of 30 runs is reported on five datasets.", "description": "This table presents the results of semi-supervised node classification experiments conducted on five medium-scale graph datasets (Cora, CiteSeer, Pubmed, CS, and Physics).  Multiple Graph Neural Network (GNN) models (GCN, GAT, SAGE, and APPNP) were evaluated, along with several augmentation strategies (Original, GraphMix, CODA, DropMessage, MH-Aug, LA-GCN/GAT/SAGE/APPNP, NodeMixup, and IntraMix).  The table shows the average accuracy (and standard deviation) achieved by each GNN model using the different augmentation methods across 30 experimental runs.", "section": "4.1 Semi-supervised Learning"}, {"figure_path": "0SRJBtTNhX/tables/tables_7_3.jpg", "caption": "Table 6: Explore the effect of generating node with Intra-Class Mixup. Zeros means replacing the generated nodes with an all-zero vector, and Ones means replacing them with an all-one vector.", "description": "This ablation study investigates the impact of Intra-Class Mixup on node classification accuracy using GCN on three datasets (Cora, CiteSeer, Pubmed).  It compares the original GCN performance against three variations: using only pseudo-labels, replacing generated nodes with all-zeros, and replacing them with all-ones. The results demonstrate the effectiveness of Intra-Class Mixup in generating high-quality labeled nodes, leading to significantly improved classification accuracy compared to baselines.", "section": "4.4 Ablation Experiment"}, {"figure_path": "0SRJBtTNhX/tables/tables_14_1.jpg", "caption": "Table 1: Semi-supervised node classification accuracy(%) on medium-scale graphs. The average result of 30 runs is reported on five datasets.", "description": "This table presents the results of semi-supervised node classification experiments conducted on five medium-scale graph datasets (Cora, CiteSeer, Pubmed, CS, and Physics).  Multiple Graph Neural Network (GNN) models (GCN, GAT, SAGE, and APPNP) were evaluated, each with several different data augmentation strategies (Original, GraphMix, CODA, DropMessage, MH-Aug, LA-GCN/GAT/SAGE/APPNP, NodeMixup, and IntraMix).  The table shows the average accuracy achieved by each GNN model across 30 runs for each augmentation strategy, providing a comparison of performance improvement due to data augmentation methods. ", "section": "4.1 Semi-supervised Learning"}, {"figure_path": "0SRJBtTNhX/tables/tables_19_1.jpg", "caption": "Table 1: Semi-supervised node classification accuracy(%) on medium-scale graphs. The average result of 30 runs is reported on five datasets.", "description": "This table presents the results of semi-supervised node classification experiments conducted on five medium-scale graph datasets (Cora, CiteSeer, Pubmed, CS, and Physics).  The experiments compared the performance of several Graph Neural Networks (GNNs) with and without the IntraMix augmentation method.  For each GNN, several augmentation strategies are tested, including original (no augmentation), GraphMix, CODA, DropMessage, MH-Aug, LA-GCN, NodeMixup, and IntraMix.  The table shows the average accuracy across 30 runs for each GNN and augmentation strategy on each dataset, providing a comprehensive comparison of IntraMix against existing augmentation methods.", "section": "4.1 Semi-supervised Learning"}, {"figure_path": "0SRJBtTNhX/tables/tables_21_1.jpg", "caption": "Table 1: Semi-supervised node classification accuracy(%) on medium-scale graphs. The average result of 30 runs is reported on five datasets.", "description": "This table presents the results of semi-supervised node classification experiments conducted on five medium-scale graph datasets (Cora, CiteSeer, Pubmed, CS, and Physics).  The experiments compare the performance of several Graph Neural Network (GNN) models (GCN, GAT, SAGE, APPNP) with and without various augmentation methods (Original, GraphMix, CODA, DropMessage, MH-Aug, LA-GCN/GAT/SAGE/APPNP, NodeMixup, and IntraMix).  For each GNN and augmentation strategy, the average accuracy across 30 independent runs is reported.  The table helps assess the impact of different augmentation techniques on the effectiveness of various GNN models in semi-supervised node classification tasks.", "section": "4.1 Semi-supervised Learning"}, {"figure_path": "0SRJBtTNhX/tables/tables_22_1.jpg", "caption": "Table 1: Semi-supervised node classification accuracy(%) on medium-scale graphs. The average result of 30 runs is reported on five datasets.", "description": "This table presents the results of semi-supervised node classification experiments conducted on five medium-scale graph datasets (Cora, CiteSeer, Pubmed, CS, and Physics).  Multiple Graph Neural Network (GNN) models (GCN, GAT, SAGE, and APPNP) were evaluated, each with several data augmentation strategies (Original, GraphMix, CODA, DropMessage, MH-Aug, LA-GCN/GAT/SAGE/APPNP, NodeMixup, and IntraMix). The accuracy of each model and augmentation strategy is shown, averaged over 30 runs. This allows comparison of the performance of different GNN models and the effectiveness of various data augmentation techniques in semi-supervised node classification.", "section": "4.1 Semi-supervised Learning"}]