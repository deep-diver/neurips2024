[{"figure_path": "ePOBcWfNFC/figures/figures_1_1.jpg", "caption": "Figure 1: Consider an agent practicing driving skills by learning to control a car's speed (length of orange arrow), steering (curvature of orange arrow), and headlights (blue symbol), (Left) previous unsupervised skill discovery methods learn entangled skills, where a change in the skill variable can cause all three environment factors to change (Right) DUSDi learns disentangled skills with concurrent components, where each skill component only affects one factor of the state space, enabling efficient downstream task learning with hierarchical RL.", "description": "This figure illustrates the difference between entangled and disentangled skills in unsupervised reinforcement learning.  The left side shows prior methods learning entangled skills, where a single skill variable affects multiple aspects of the environment (speed, steering, headlights). The right side depicts DUSDi, which learns disentangled skills. Each skill component independently controls a single aspect of the environment, enabling more efficient downstream task solving with hierarchical reinforcement learning.", "section": "1 Introduction"}, {"figure_path": "ePOBcWfNFC/figures/figures_2_1.jpg", "caption": "Figure 2: Two learning stages of DUSDi: (a) in disentangled skill learning stage, DUSDi creates a one-to-one mapping between state factors and skill components \u2014 each disentangled skill component z\u00b2 only influences state factor s\u00b2. DUSDi designs a novel mutual-information-based intrinsic reward to enforce disentanglement and utilize Q-value decomposition to learn the skill policy \u03c0\u03bf efficiently. (b) in the task learning stage, the skill policy is used as a frozen low-level policy and a high-level policy Thigh is learned to select skill z for every L steps, by maximizing the task reward rtask.", "description": "This figure illustrates the two-stage learning process in DUSDi.  The first stage (a) focuses on learning disentangled skills where each skill component affects only one state factor.  A novel mutual-information-based intrinsic reward and Q-value decomposition are used to achieve efficient learning. The second stage (b) uses these learned skills within a hierarchical RL framework to solve downstream tasks by training a high-level policy to select appropriate low-level skills.", "section": "3 Learning Disentangled Skills with DUSDi"}, {"figure_path": "ePOBcWfNFC/figures/figures_7_1.jpg", "caption": "Figure 3: Evaluation of the effect of Q-decomposition in skill learning. The plots depict the mean and standard deviation of accuracy (\u2191) when predicting the skill component z\u00b2 based on the state factor s\u00b2, computed across 3 training processes. The higher prediction accuracy indicates that the policy learns to control more state factors in more distinguishable ways, leading to more efficient downstream task learning.", "description": "This figure shows the impact of using Q-decomposition in the skill learning phase of DUSDi.  Three subplots present the prediction accuracy for skill components (z\u00b2) based on state factors (s\u00b2) across three different environments: 2D Gunner, Multi-Particle, and iGibson.  Higher accuracy signifies that the learned skills successfully control more state factors independently, improving downstream task learning efficiency. The error bars represent the standard deviation across three training runs.", "section": "4.3 Evaluating Skill Learning Efficiency with Q-decomposition"}, {"figure_path": "ePOBcWfNFC/figures/figures_8_1.jpg", "caption": "Figure 4: Training curves of DUSDi and baselines on multiple downstream tasks (reward supervised second phase). The plots depict the mean and standard deviation of the return of each method over 3 random seeds. DUSDi outperforms all baselines that learn entangled skills, converging faster and to higher returns.", "description": "This figure displays the performance comparison of DUSDi against several baseline methods across various downstream tasks.  Each subplot represents a different task, and the y-axis shows the average return achieved by each algorithm. The x-axis indicates the number of training episodes. The shaded area around each line represents the standard deviation over three independent trials. The figure highlights DUSDi's superior performance and faster convergence in most tasks compared to other unsupervised reinforcement learning methods. This emphasizes the effectiveness of DUSDi's disentangled skill learning in handling complex downstream tasks.", "section": "4.4 Evaluating Downstream Task Learning"}, {"figure_path": "ePOBcWfNFC/figures/figures_9_1.jpg", "caption": "Figure 5: Performance of DUSDi with image observations on two multi-particle downstream tasks over three random seeds. With the help of disentangled representation learning, DUSDi effectively learns skills based only on image observations and leverages the skills to solve challenging downstream tasks where baseline methods fail.", "description": "This figure shows the performance comparison between DUSDI using state observations and DUSDI using image observations on two downstream tasks in the Multi-Particle environment. The results demonstrate that DUSDI can effectively learn skills from image observations and achieve similar performance to when using state observations, while baseline methods fail to learn these tasks even with state observations. This highlights the effectiveness of DUSDI's disentangled representation learning in handling complex observation spaces.", "section": "4.5 Extending DUSDi to Image Space"}, {"figure_path": "ePOBcWfNFC/figures/figures_9_2.jpg", "caption": "Figure 6: Performance of DUSDi in two multi-particle downstream tasks when combined with Causal Policy Gradient (CPG, orange). The disentangled skills of DUSDi provide opportunities for leverage structure and speed up downstream task learning, greatly improving the sample efficiency when learning downstream tasks.", "description": "This figure shows the performance comparison of DUSDi with and without Causal Policy Gradient (CPG) on two downstream tasks in the Multi-particle environment.  The results demonstrate that using DUSDi's disentangled skills allows for leveraging structure in the downstream task, leading to a significant improvement in sample efficiency. Specifically, the plots show the learning curves for the two methods across multiple training runs, indicating faster convergence and higher overall returns when using CPG in conjunction with DUSDi.", "section": "4.6 Leveraging Structure of DUSDi Skills"}, {"figure_path": "ePOBcWfNFC/figures/figures_15_1.jpg", "caption": "Figure 7: Environments Visualization", "description": "This figure shows four different environments used in the paper's experiments: 2D Gunner, DMC Walker, Multi-Particle, and iGibson. Each environment is a simulation used to evaluate the effectiveness of the proposed Disentangled Unsupervised Skill Discovery (DUSDi) method.  The visualizations provide a visual representation of the state space of each environment, illustrating the diversity of task settings used in the experiments and the complexity of the challenges addressed by the method.", "section": "4.1 Evaluation Environments"}]