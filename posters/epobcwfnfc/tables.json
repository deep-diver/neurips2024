[{"figure_path": "ePOBcWfNFC/tables/tables_6_1.jpg", "caption": "Table 1: Evaluation of skill disentanglement based on the DCI metric, shown as mean and standard deviation across skill policies trained with 3 random seeds.", "description": "This table presents the results of evaluating skill disentanglement using the Disentanglement, Completeness, and Informativeness (DCI) metric.  The DCI metric assesses how well-separated (disentangled) the learned skills are, whether they cover a comprehensive range of states, and how informative they are in predicting states.  The table shows the mean and standard deviation of DCI scores across three different training runs for four different methods, specifically, the proposed method DUSDi and three baselines. Three different environments are used: 2D Gunner, Multi-Particle, and iGibson, reflecting the diversity of environments used for training and evaluation.", "section": "4.2 Evaluating Skill Disentanglement"}, {"figure_path": "ePOBcWfNFC/tables/tables_17_1.jpg", "caption": "Table 1: Evaluation of skill disentanglement based on the DCI metric, shown as mean and standard deviation across skill policies trained with 3 random seeds.", "description": "This table presents the results of evaluating skill disentanglement using the DCI (Disentanglement, Completeness, Informativeness) metric.  The mean and standard deviation of the DCI scores are shown for DUSDI and a baseline method (DIAYN-MC) across three different environments (2D Gunner, Multi-Particle, iGibson). Each environment has its own row in the table, and each method has its own set of columns for Disentanglement, Completeness, and Informativeness. Higher scores indicate better disentanglement, completeness, and informativeness, which are aspects of how well the learned skills separate different aspects of the environment's state.", "section": "4.2 Evaluating Skill Disentanglement"}, {"figure_path": "ePOBcWfNFC/tables/tables_17_2.jpg", "caption": "Table 3: Hyperparameters of Downstream Learning.", "description": "This table lists the hyperparameters used for downstream learning (the second phase of the DUSDi process) using Proximal Policy Optimization (PPO).  These hyperparameters are consistent across all skill discovery methods and downstream tasks within the paper's experiments. The hyperparameters cover optimization settings (optimizer, activation function, learning rate, batch size), policy update parameters (clip ratio, MLP size, GAE lambda, target steps, n steps), and environmental parameters (number of environments and low-level steps).", "section": "3.3 Downstream Task Learning"}]