[{"figure_path": "I3IuclVLFZ/tables/tables_5_1.jpg", "caption": "Table 1: Comparison with various FL algorithms in one round.", "description": "The table compares the performance of FedLPA against several other federated learning algorithms (FedNova, SCAFFOLD, FedAvg, FedProx, and DENSE) across four datasets (MNIST, FMNIST, CIFAR-10, and SVHN) under various data partitioning scenarios (\u03b2 = {0.01, 0.05, 0.1, 0.3, 0.5, 1.0} and #C = {1, 2, 3}).  The results show the test accuracy for each algorithm across different levels of data non-IIDness.", "section": "4 Experiments"}, {"figure_path": "I3IuclVLFZ/tables/tables_7_1.jpg", "caption": "Table 1: Comparison with various FL algorithms in one round.", "description": "This table compares the performance of FedLPA against several other federated learning algorithms across four datasets (FMNIST, CIFAR-10, MNIST, SVHN).  The comparison is done for various non-IID data settings.  Different data partitioning methods, labeled as #C=k (where k is the number of classes per client) and \u03b2=x (where x is a parameter representing the degree of data skew), are used to simulate realistic non-IID scenarios. The table shows the test accuracy achieved by each algorithm under each condition, highlighting FedLPA's performance.", "section": "4 Experiments"}, {"figure_path": "I3IuclVLFZ/tables/tables_8_1.jpg", "caption": "Table 1: Comparison with various FL algorithms in one round.", "description": "The table compares the performance of FedLPA with several other federated learning algorithms across four datasets (MNIST, FMNIST, CIFAR-10, and SVHN).  Each dataset is partitioned using two different non-IID approaches (\u03b2 and #C). The performance metrics (accuracy) are reported with standard deviations for each algorithm and data partition method.  This demonstrates FedLPA's performance relative to existing state-of-the-art one-shot federated learning approaches.", "section": "4 Experiments"}, {"figure_path": "I3IuclVLFZ/tables/tables_8_2.jpg", "caption": "Table 3: Experiments with different proportions of data samples.", "description": "This table shows the impact of reducing the amount of data used for training on the model's accuracy.  The results are presented for different levels of data skew (beta values of 0.1, 0.3, and 0.5).  Each row shows the accuracy achieved when using a specific percentage of the original dataset (100%, 80%, 60%, 40%, and 20%). The table demonstrates how the model's performance is affected as the amount of training data decreases.", "section": "4 Experiments"}, {"figure_path": "I3IuclVLFZ/tables/tables_9_1.jpg", "caption": "Table 4: Experimental results of different hyper-parameter \u03bb on FMNIST dataset.", "description": "This table presents the experimental results obtained by varying the hyperparameter \u03bb in the FedLPA model.  The results show test accuracy for different levels of label skew (\u03b2) and data partitioning (#C), demonstrating the robustness of the FedLPA approach to changes in \u03bb.  The results are presented in the form of mean \u00b1 standard deviation of test accuracy over multiple experimental runs.", "section": "4.5 Ablation study"}, {"figure_path": "I3IuclVLFZ/tables/tables_9_2.jpg", "caption": "Table 5: Communication and computation overhead evaluation.", "description": "This table presents the communication and computation overhead for various federated learning algorithms (FedLPA, FedNova, SCAFFOLD, FedAvg, FedProx, and DENSE) when applied to a simple CNN with 5 layers on the CIFAR-10 dataset.  The \"Overall Computation\" column shows the total computation time in minutes, and the \"Overall Communication\" column indicates the total communication overhead in MB. The results highlight the computational and communication efficiency trade-offs among these algorithms.", "section": "4 Experiments"}, {"figure_path": "I3IuclVLFZ/tables/tables_17_1.jpg", "caption": "Table 6: Experiments with DiagonalFisher using MLP.", "description": "This table compares the performance of FedAvg, FedProx, SCAFFOLD, DiagonalFisher, and FedLPA on an MLP model under two different initialization schemes for client models: 'Same' (same initial parameters) and 'Different' (different initial parameters). The results show that FedLPA outperforms the other methods, particularly when the client models have different initializations. This highlights the effectiveness of FedLPA in handling non-IID data distributions.", "section": "G.2 Experiments with DiagonalFisher"}, {"figure_path": "I3IuclVLFZ/tables/tables_18_1.jpg", "caption": "Table 6: Experiments with DiagonalFisher using MLP.", "description": "This table compares the performance of FedLPA and DiagonalFisher on an MLP model under two different initialization scenarios: \"Same\" (models initialized with the same parameters) and \"Different\" (models initialized independently).  The results highlight FedLPA's superior robustness to data heterogeneity, particularly when the models are initialized differently, showcasing a significant performance improvement over DiagonalFisher.", "section": "4.3 Scalability"}, {"figure_path": "I3IuclVLFZ/tables/tables_20_1.jpg", "caption": "Table 8: Experiments with Differential Privacy using two mechanisms.", "description": "This table presents the results of experiments conducted to evaluate the performance of FedLPA with two different differential privacy mechanisms.  The experiments were performed using the MNIST dataset with various data partitioning settings (\u03b2 = 0.1, 0.3, 0.5) and different privacy levels (\u03b5 = 3, 5, 8).  The table compares the accuracy of FedLPA with the two privacy mechanisms against the accuracy of the standard FedAvg algorithm.  The results demonstrate the impact of different privacy parameters on the accuracy of the FedLPA algorithm under various data distribution settings.", "section": "4 Experiments"}, {"figure_path": "I3IuclVLFZ/tables/tables_20_2.jpg", "caption": "Table 9: Experiments with Differential Privacy for Round Numbers.", "description": "This table shows the number of rounds needed for DP-FedAvg to achieve a similar test accuracy as FedLPA with the first differential privacy mechanism for different beta and epsilon values.  It demonstrates the efficiency of FedLPA as a one-shot method compared to DP-FedAvg which requires multiple rounds.", "section": "Experiments"}, {"figure_path": "I3IuclVLFZ/tables/tables_21_1.jpg", "caption": "Table 10: iDLG attack results of FedLPA and FedAvg.", "description": "This table presents the results of iDLG attacks on FedLPA and FedAvg, showing the mean squared error (MSE) between the original and reconstructed images at various percentiles.  It aims to evaluate and compare the privacy-preserving capabilities of the two methods by measuring their resistance to reconstruction attacks. Higher MSE values indicate stronger privacy protection.", "section": "4. Experiments"}, {"figure_path": "I3IuclVLFZ/tables/tables_24_1.jpg", "caption": "Table 1: Comparison with various FL algorithms in one round.", "description": "This table presents a comparison of the proposed FedLPA algorithm with several existing federated learning (FL) algorithms. The comparison is done using various datasets (FMNIST, CIFAR-10, MNIST, SVHN) under different data partitioning methods (\u03b2 = 0.01, 0.05, 0.1, 0.3, 0.5, 1.0 and #C = 1, 2, 3) representing various levels of non-IID data distribution.  The results show the test accuracy achieved by each algorithm in a single communication round (one-shot FL). This table highlights the superior performance of FedLPA across different datasets and non-IID settings.", "section": "4 Experiments"}, {"figure_path": "I3IuclVLFZ/tables/tables_24_2.jpg", "caption": "Table 1: Comparison with various FL algorithms in one round.", "description": "The table compares the performance of FedLPA with other federated learning algorithms (FedAvg, FedProx, FedNova, Scaffold, and DENSE) on four benchmark datasets (MNIST, FMNIST, CIFAR-10, and SVHN) under various non-IID data settings.  The non-IID settings are controlled by the parameters \u03b2 and #C.  \u03b2 represents the Dirichlet distribution parameter for label skew, and #C indicates the number of classes per client.  The table shows the test accuracy for each method under these different conditions, allowing for a direct comparison of the performance of FedLPA against state-of-the-art baselines.", "section": "4 Experiments"}, {"figure_path": "I3IuclVLFZ/tables/tables_25_1.jpg", "caption": "Table 1: Comparison with various FL algorithms in one round.", "description": "The table compares the performance of FedLPA against several other federated learning algorithms on four different datasets (MNIST, FMNIST, CIFAR-10, and SVHN) under various non-IID data settings.  Different non-IID settings are simulated by varying the number of classes per client (#C) and by controlling the class distribution skew using the Dirichlet distribution parameter (\u03b2). The results show FedLPA's accuracy across different metrics and non-IID settings.", "section": "4 Experiments"}, {"figure_path": "I3IuclVLFZ/tables/tables_25_2.jpg", "caption": "Table 1: Comparison with various FL algorithms in one round.", "description": "This table compares the performance of the proposed FedLPA algorithm against several other federated learning (FL) algorithms on four different datasets (MNIST, Fashion-MNIST, CIFAR-10, and SVHN) under various data partitioning strategies to simulate different levels of non-IID data. The results show the test accuracy of each algorithm on each dataset and partitioning scenario.  It highlights how FedLPA performs in different non-IID scenarios compared to the baselines.", "section": "4 Experiments"}, {"figure_path": "I3IuclVLFZ/tables/tables_26_1.jpg", "caption": "Table 1: Comparison with various FL algorithms in one round.", "description": "This table compares the performance of FedLPA against several other federated learning algorithms across various datasets and data partitioning schemes (IID and non-IID).  It shows the test accuracy achieved by each algorithm under different levels of data heterogeneity.  The results highlight FedLPA's superior performance, particularly in non-IID scenarios with high label skewness.", "section": "4 Experiments"}, {"figure_path": "I3IuclVLFZ/tables/tables_26_2.jpg", "caption": "Table 1: Comparison with various FL algorithms in one round.", "description": "The table compares the performance of FedLPA with several other federated learning algorithms across various datasets and non-IID data settings.  It shows test accuracy results for different levels of data heterogeneity, represented by \u03b2 values and #C values, which indicate the degree of label skew in the data.", "section": "4 Experiments"}, {"figure_path": "I3IuclVLFZ/tables/tables_27_1.jpg", "caption": "Table 17: Comparison with FedOV on MNIST with #C=2.", "description": "This table compares the performance of FedLPA and FedOV on the MNIST dataset with a non-IID data distribution where each client only has data from 2 classes.  The results show how the accuracy of both methods changes with varying numbers of local epochs (10, 20, 50, 100, 200).  FedOV transmits label information, while FedLPA does not.  The green highlights indicate where FedLPA outperforms FedOV.", "section": "4.5 Ablation study"}, {"figure_path": "I3IuclVLFZ/tables/tables_27_2.jpg", "caption": "Table 1: Comparison with various FL algorithms in one round.", "description": "This table compares the performance of FedLPA against several other federated learning algorithms across different datasets and non-IID data distributions.  The results show accuracy for various settings defined by the beta parameter (\u03b2) and the number of classes per client (#C).  Higher values generally indicate better performance.  The table highlights FedLPA's effectiveness, especially in challenging non-IID scenarios.", "section": "4.2 An overall comparison"}, {"figure_path": "I3IuclVLFZ/tables/tables_28_1.jpg", "caption": "Table 1: Comparison with various FL algorithms in one round.", "description": "This table compares the performance of FedLPA against several other federated learning algorithms on four different datasets (MNIST, FMNIST, CIFAR-10, and SVHN) under various non-IID data settings.  The performance is measured across several metrics and different levels of data heterogeneity, represented by parameters \u03b2 and #C, indicating the degree of label imbalance or class distribution skew among the clients.  The results demonstrate FedLPA's superior performance, especially in extreme non-IID scenarios.", "section": "4 Experiments"}, {"figure_path": "I3IuclVLFZ/tables/tables_28_2.jpg", "caption": "Table 1: Comparison with various FL algorithms in one round.", "description": "This table compares the performance of FedLPA with several other federated learning algorithms on four different datasets (MNIST, Fashion-MNIST, CIFAR-10, and SVHN) under various non-IID data settings.  The non-IID settings are simulated using two different partitioning methods to introduce varying levels of label skew: one where each client only has data from a subset of classes (#C=k), and another where the class distribution is sampled from a Dirichlet distribution (pk~Dir(\u03b2)). The table shows the test accuracy achieved by each algorithm under each data setting, highlighting the superior performance of FedLPA, particularly in scenarios with high label skew.", "section": "4 Experiments"}, {"figure_path": "I3IuclVLFZ/tables/tables_29_1.jpg", "caption": "Table 21: Experiments with VGG-9.", "description": "This table presents the results of experiments conducted using the VGG-9 model.  It compares the performance of FedLPA against several other federated learning algorithms (FedNova, SCAFFOLD, FedAvg, FedProx, and Dense) under different levels of data heterogeneity (\u03b2 = 0.1, 0.3, and 0.5).  The results are presented as the average test accuracy with standard deviation, demonstrating the superior performance of FedLPA across varying levels of data heterogeneity. Note that VGG-9 is a deeper and more complex model than the CNNs used in other parts of the experiments.", "section": "4 Experiments"}, {"figure_path": "I3IuclVLFZ/tables/tables_29_2.jpg", "caption": "Table 22: Experiments with CIFAR-100 using FedLPA.", "description": "This table presents the experimental results of FedLPA on the CIFAR-100 dataset.  It shows the accuracy achieved by FedLPA for different levels of data heterogeneity (\u03b2 values of 0.1, 0.3, and 0.5). The results are likely compared against other federated learning methods, demonstrating FedLPA's performance on a more challenging dataset with a larger number of classes.", "section": "4 Experiments"}, {"figure_path": "I3IuclVLFZ/tables/tables_29_3.jpg", "caption": "Table 23: Experiments with EMNIST.", "description": "This table presents the results of experiments conducted on the EMNIST dataset using the FedLPA and FedAvg algorithms.  It shows the accuracy achieved under different data partition settings (\u03b2=0.1, 0.3, 0.5) for two variations of the EMNIST dataset: one with 10 classes (EMNIT-mnist) and one with 37 classes (EMNIT-letters). The results highlight the performance improvement of FedLPA over FedAvg, demonstrating its effectiveness particularly in scenarios with non-IID data.", "section": "4 Experiments"}, {"figure_path": "I3IuclVLFZ/tables/tables_30_1.jpg", "caption": "Table 24: Experiments with Tiny-ImangeNet using ResNet-18.", "description": "This table presents the results of experiments conducted using the ResNet-18 model on the Tiny-ImageNet dataset.  The experiments compare the performance of FedLPA against FedAvg and Dense, across different levels of data heterogeneity (represented by the beta parameter, \u03b2).  The results show the accuracy achieved under various levels of data heterogeneity. Higher beta values indicate less heterogeneity and a higher overall accuracy.", "section": "G.9 Experiments with more complex datasets"}, {"figure_path": "I3IuclVLFZ/tables/tables_30_2.jpg", "caption": "Table 25: Experiments for the approximation study.", "description": "This table presents the results of an ablation study on the number of approximation iterations used in FedLPA. It shows that increasing the number of iterations generally improves accuracy, but the gains diminish beyond a certain point.  The results are shown for different data skewness levels (\u03b2=0.1, 0.3, 0.5).  The computation time also increases linearly with the number of iterations.", "section": "G.10 Ablation experiments analyzing the number of approximation iterations of FedLPA"}]