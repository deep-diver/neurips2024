[{"type": "text", "text": "FedLPA: One-shot Federated Learning with Layer-Wise Posterior Aggregation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xiang Liu1,\u2020, Liangxi $\\mathbf{Liu^{2,\\dagger}}$ , Feiyang $\\mathbf{Y}\\mathbf{e}^{3}$ , Yunheng Shen4, Xia $\\mathbf{Li^{5}}$ , Linshan Jiang1,\u2021, Jialin Li1,\u2021 ", "page_idx": 0}, {"type": "text", "text": "1National University of Singapore, 2Northeastern University, 3University of Technology Sydney, 4Tsinghua University, 5ETH Zurich {liuxiang,lijl}@comp.nus.edu.sg liu.liangx@northeastern.edu feiyang.ye.uts@gmail.com shenyh19@mails.tsinghua.edu.cn ethlixia@gmail.com linshan@nus.edu.sg \u2020Equal Contribution $^{\\ddag}$ Correspondence Author ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Efficiently aggregating trained neural networks from local clients into a global model on a server is a widely researched topic in federated learning. Recently, motivated by diminishing privacy concerns, mitigating potential attacks, and reducing communication overhead, one-shot federated learning (i.e., limiting client-server communication into a single round) has gained popularity among researchers. However, the one-shot aggregation performances are sensitively affected by the non-identical training data distribution, which exhibits high statistical heterogeneity in some real-world scenarios. To address this issue, we propose a novel one-shot aggregation method with layer-wise posterior aggregation, named FedLPA. FedLPA aggregates local models to obtain a more accurate global model without requiring extra auxiliary datasets or exposing any private label information, e.g., label distributions. To effectively capture the statistics maintained in the biased local datasets in the practical non-IID scenario, we efficiently infer the posteriors of each layer in each local model using layer-wise Laplace approximation and aggregate them to train the global parameters. Extensive experimental results demonstrate that FedLPA significantly improves learning performance over state-of-the-art methods across several metrics. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Data privacy issues in Deep Learning [1, 2, 3, 4, 5, 6, 7] have grown to be a major global concern [8]. To safeguard data privacy, the conventional federated learning algorithm will use the aggregation methods and follow the data management rules of different institutions, which implies that the distribution of data exhibits variations among clients [8]. In the domain of machine learning, federated learning (FL) [9, 10, 11] has emerged as a prominent paradigm. The fundamental tenet of federated learning revolves around sharing machine learning models derived from decentralized data repositories, as opposed to divulging user raw data. This approach effectively preserves the confidentiality of individual data. ", "page_idx": 0}, {"type": "text", "text": "The standard federated learning framework, FedAvg [9, 12], applies local model training. These local models are then aggregated into a global model through parameter averaging. Existing FL algorithms, however, require many communication rounds to effectively train a global model, leading to substantial communication overhead, increased privacy concerns, and higher demand for fault tolerance throughout the rounds. One-shot FL, which reduces client-server communication into a single round as explored by prior work [13, 14, 15], is a promising yet challenging scheme to address these issues. One-shot FL proves particularly practical in scenarios where iterative communication is not feasible. Moreover, a reduction in communication rounds translates to fewer opportunities for any potential eavesdropping attacks. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "While one-shot FL shows promises, existing approaches often grapple with challenges such as inadequate handling of high statistical heterogeneity information [16, 17] or non-independent and non-identically distributed (non-IID) data [18, 19]. Moreover, some prior methods rely on an auxiliary public dataset to achieve satisfactory performance in one-shot FL [13, 14], or even on pre-trained large models [20], which may not be practical [21] in some sensitive scenarios. Additionally, some approaches, such as those [22, 19, 23, 15]), might expose private label information to both local and global models, e.g., the client label distribution, potentially violating General Data Protection Regulation (GDPR) rules. Furthermore, some prior methods [14, 18, 24] require substantial computing resources for dataset distillation, model distillation, or even training a generator capable of generating synthetic data for second-stage training on the server side, making them less practical. ", "page_idx": 1}, {"type": "text", "text": "Besides, the performance of one-shot FL often falls short when dealing with non-IID data. Non-IID data biases global updates, reducing the accuracy of the global model and slowing down convergence. In extreme non-IID cases, clients may be required to address distinct classes solely on their side. Several approaches to federated learning are proposed in multi-round settings to tackle this heterogeneity among clients. In the work [25], it allows each client to use a personalized model instead of a shared global model. With the personalized approach, a multi-round framework beneftis from joint training while allowing each client to keep its unique model. However, one-shot aggregation on a local model is far from being resolved to address the concern of non-i.i.d data distributions. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we introduce a novel one-shot aggregation approach to address these issues, named FedLPA (Federated Learning with Layer-wise Posterior Aggregation). FedLPA infers the posteriors of each layer in each local model using the empirical Fisher information matrix obtained by layer-wise Laplace Approximation. Laplace Approximations are widely used to compute the empirical Fisher information matrix for neural networks, conveying the data statistics in non-i.i.d settings. However, computing empirical Fisher information matrices of multiple local clients and aggregating their Fisher information matrices remains an ongoing challenge [17]. To mitigate it, FedLPA aggregates the posteriors of local models using the accurately computed block-diagonal empirical Fisher information matrices to measure the parameter space. This matrix captures essential parameter correlations and distinguishes itself from prior methods by being non-diagonal and non-low-rank, thereby conveying the statistics of biased local datasets. After that, the global model parameters are aggregated without any need for server-side knowledge distillation [26]. ", "page_idx": 1}, {"type": "text", "text": "Our extensive experiments verify the efficiency and effectiveness of FedLPA, highlighting that FedLPA markedly enhances the test accuracy when compared to existing one-shot FL baseline approaches across various datasets. Our main contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 To the best of our knowledge, we are the first to propose an effective one-shot federated learning approach that trains global models using block-diagonal empirical Fisher information matrices. Our approach is data-free without any need for any auxiliary dataset and label information and significantly improves system performance, including negligible communication cost and moderate computing overhead.   \n\u2022 We are the first to train global model parameters via constructing a multi-variate linear objective function and optimizing its quadratic form, which allows us to formulate and solve the problem in a convex form efficiently, which has a linear convergence rate, ensuring good performance.   \n\u2022 We conduct extensive experiments to illustrate the effectiveness of FedLPA. Our approach consistently outperforms the baselines, showcasing substantial improvement across various settings and datasets. Even in some extreme scenarios where label skew is severe, e.g., each client has only one class, we achieve satisfactory results while other existing one-shot federated learning algorithms struggle. ", "page_idx": 1}, {"type": "text", "text": "2 Background and related works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Federated learning on non-iid data ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Previous work FedAvg [9] first introduced the concept of $\\mathrm{FL}$ and presented the algorithm, which achieved competitive performance on i.i.d data, in comparison to several centralized techniques. However, it was observed in previous works [27, 28] that the convergence rate and ultimate accuracy of FedAvg on non-IID data distributions were significantly reduced, compared to the results observed with homogeneous data distributions. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Several methods have been developed to enhance performance in federated learning against non-IID data distributions. The SCAFFOLD method [29] leveraged control variates to reduce objective inconsistency in local updates. It estimated the drift of directions in local optimization and global optimization and incorporated this drift into local training to align the local optimization direction with the global optimization. FedNova [30] addressed objective inconsistency while maintaining rapid error convergence through a normalized averaging method. It scaled and normalized the local updates of each client based on the number of local optimization steps. FedProx [31] enhanced the local training process by introducing a global prior in the form of an $L2$ regularization term within the local objective function. Researchers introduced PFNM [32, 33], a Bayesian probabilistic framework specifically tailored for multilayer perceptrons. PFNM employed a Beta-Bernoulli process (BBP) [34] to aggregate local models, quantifying the degree of alignment between global and local parameters. The framework [17] proposed utilized a multivariate Gaussian product method to construct a global posterior by aggregating local posteriors estimated using an online Laplace approximation. FedPA [16] also applied the Gaussian product method but employed stochastic gradient Markov chain Monte Carlo for approximate inference of local posteriors. DAFL (DataFree Learning) [35] introduced an innovative framework based on generative adversarial networks. ADI [36] utilized an image synthesis method that leveraged the image distribution to train deep neural networks without real data. The pFedHN method [37] incorporated HyperNetworks [38] to address federated learning applications. ", "page_idx": 2}, {"type": "text", "text": "However, all of these methods encountered challenges in the one-shot federated learning setting, as they required aggregating the model by multiple rounds and might be inaccurate due to the omission of critical information, such as posterior joint probabilities between different parameters. ", "page_idx": 2}, {"type": "text", "text": "2.2 One-shot federated learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "One-shot Federated Learning (FL) is an emerging and promising research direction characterized by its minimal communication cost. In the first study on one-shot FL [13], the approach involved the aggregation of local models, forming an ensemble to construct the final global model. Subsequently, knowledge distillation using public data was applied in the following step. FedKT [14] brought forward the concept of consistent voting to fortify the ensemble. Recent research endeavors [19, 24] proposed data-free knowledge distillation schemes tailored for one-shot FL. These methods adopted the basic ensemble distillation framework as FedDF [26]. XorMixFL [22] introduced the use of exclusive OR operation (XOR) for encoding and decoding samples in data sharing. It is important to note that XorMixFL assumed the possession of labeled samples from a global class by all clients and the server, which might not align with practical real-world scenarios. A noteworthy innovation of DENSE [24] was its utilization of a generator to create synthetic datasets on the server side, circumventing the need for a public dataset in the distillation process. Co-Boosting [39] improves the ensemble when doing the distillation to improve the performance. FedOV [15] delved into addressing comprehensive label skew cases. FEDCVAE [23] confronted this challenge by transmitting all label distributions from clients to servers. These schemes [22, 14, 19, 24, 23, 15] exposed some client-side private information, leading to additional communication overhead and potential privacy leakage, e.g., FEDCVAE [23] needed all the client label distribution to be transmitted to the server side and FedOV [15] needed the clients to know the labels which were unknown. Instead, MA-Echo [40] adopted a unique approach by emphasizing the addition of norms among layer-wide parameters during the aggregation of local models. The project [41] focused on the theoretic analysis of the error in its approximation method. However, their method grappled with limited experiments and lacked detailed explanations of the approach. FedDISC [20], on the other hand, relied on the pre-trained model CLIP from OpenAI, where their reliance might not always align with practicality or suitability for diverse scenarios. ", "page_idx": 2}, {"type": "text", "text": "While some of these techniques are orthogonal to FedLPA and can be integrated with it, it is worth noting that none of the previously mentioned algorithms possess the capability to train global model parameters using empirical Fisher information matrices on extensive experiment settings. Some of them [13, 14] may require additional information, and may potentially entail the risk of label distribution leakages. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 Objective formulation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Generally, federated learning is defined as an optimization problem [31, 29, 30, 42] for maximizing a global objective function $\\mathbb{F}(\\pmb\\theta)$ which is a mixture of local objective functions $\\mathbb{F}_{k}(\\pmb{\\theta},\\mathcal{D}_{k})$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{F}(\\pmb{\\theta})=\\sum_{k=1}^{K}\\mathbb{F}_{k}(\\pmb{\\theta},\\mathcal{D}_{k})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\pmb{\\theta}=\\left[\\mathrm{vec}(\\mathbf{W}_{1}),\\dots,\\mathrm{vec}(\\mathbf{W}_{l}),\\dots,\\mathrm{vec}(\\mathbf{W}_{L})\\right]$ is the parameter vector of global model and $\\mathbf{W}_{l}$ is the weight and bias of layer $l$ for a $L$ -layers neural network; $\\mathcal{D}_{k}$ is the local dataset $k$ -th client. $\\mathbb{F}_{k}(\\pmb{\\theta},\\mathcal{D}_{k})$ is the expectation of the local objective function, which is proportional to the logarithm of likelihood $\\log p(\\bar{\\mathcal \u1e0a D \u1e0c _{k} \u1e0c |\\theta)$ . ", "page_idx": 3}, {"type": "text", "text": "Previous works [16, 17] give a common formula of the global posterior which consists of local posteriors $p(\\pmb{\\theta}|\\mathcal{D}_{k})$ under variational inference formulation. ", "page_idx": 3}, {"type": "equation", "text": "$$\np(\\pmb{\\theta}|\\mathcal{D})\\propto\\prod_{k=1}^{K}p(\\mathcal{D}_{k}|\\pmb{\\theta})\\propto\\prod_{k=1}^{K}p(\\pmb{\\theta}|\\mathcal{D}_{k})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\theta}\\mathbb{F}(\\pmb\\theta)=\\sum_{k=1}^{K}\\frac{|D_{k}|}{|\\mathcal{D}|}\\cdot\\mathbb{E}_{s\\in\\mathcal{D}_{k}}\\left[\\log p(s|\\pmb\\theta)\\right]\\equiv\\operatorname*{max}_{\\pmb\\theta}\\prod_{k=1}^{K}p(\\pmb\\theta|\\mathcal{D}_{k})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "As we know, the objective function is the expectation of the likelihood, and the sum of the logarithms is equal to the logarithms of the product as Eq. 3. Therefore, globally variational inference using Eq. 2 is equivalent to optimization for Eq. 1. Correspondingly, we have: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pmb{\\theta}}\\mathbb{F}_{k}(\\pmb{\\theta},\\mathcal{D}_{k})\\equiv\\operatorname*{max}_{\\pmb{\\theta}}p(\\pmb{\\theta}|\\mathcal{D}_{k})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Following the same training pattern of federated learning, each client infers the local posterior $p(\\pmb{\\theta}|\\mathcal{D}_{k})$ by using the local dataset $\\mathcal{D}_{k}$ . As a result, the server obtains the global posterior $p(\\pmb\\theta|\\mathcal D)$ by aggregating local posteriors using Eq. 2. ", "page_idx": 3}, {"type": "text", "text": "However, both the global and local posterior are usually intractable because modern neural networks are usually non-linear and have a large number of parameters. Therefore, it is necessary to design an efficient and accurate aggregation method for one-shot federated learning. ", "page_idx": 3}, {"type": "text", "text": "3.2 Approximating posteriors ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Although the posterior is usually intractable, the posterior can be approximated as a Gaussian distribution by performing a Taylor expansion on the logarithm of the posterior [43]: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\log p(\\pmb\\theta|\\mathcal D)\\approx\\log p\\left(\\pmb\\theta^{*}|\\mathcal D\\right)-\\frac{1}{2}\\left(\\pmb\\theta-\\pmb\\theta^{*}\\right)^{\\top}\\bar{\\pmb H}\\left(\\pmb\\theta-\\pmb\\theta^{*}\\right)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\pmb{\\theta}^{*}$ is the optimal parameter vector, $\\bar{\\mathbf{H}}=\\mathbb{E}_{s\\in\\mathcal{D}}[\\mathbf{H}]$ is the average Hessian of the negative log posterior over a dataset $\\mathcal{D}$ . It is reasonable to approximate global and local posteriors as multivariates Gaussian distributions with expectations $\\bar{\\pmb{\\mu}}=\\pmb{\\theta}^{*}$ and $\\mu_{k}=\\pmb{\\theta}_{k}^{*}$ ; co-variances $\\bar{\\bf\\Sigma}\\bar{\\bf\\Sigma}=\\bar{\\bf H}^{-1}$ and $\\begin{array}{r}{\\Sigma_{k}=\\bar{\\bf H}_{k}^{-1}}\\end{array}$ [44]. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{p(\\pmb{\\theta}|\\mathcal{D})\\equiv\\pmb{\\theta}\\sim\\mathcal{N}(\\bar{\\pmb{\\mu}},\\bar{\\Sigma}),p(\\pmb{\\theta}|\\mathcal{D}_{k})}&{\\equiv\\pmb{\\theta}\\sim\\mathcal{N}(\\pmb{\\mu}_{k},\\pmb{\\Sigma}_{k})}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "As a result, if given local expectation $\\pmb{\\mu}_{k}$ and local co-variance $\\Sigma_{k}$ , the global posterior is determined by Eq. 2 as below: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\bar{\\pmb{\\mu}}=\\bar{\\pmb{\\Sigma}}\\sum_{k}^{K}\\pmb{\\Sigma}_{k}^{-1}\\pmb{\\mu}_{k},\\bar{\\pmb{\\Sigma}}^{-1}=\\sum_{k}^{K}\\pmb{\\Sigma}_{k}^{-1}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Modern algorithms [45, 46] allow the local training process to obtain an optimal, regarded as the expectation $\\pmb{\\mu}_{k}$ in the above equations. However, $\\bar{\\mathbf{H}_{k}}^{\\mathsf{-}}$ is intractable to compute due to a large number of parameters in modern neural networks. An efficient method is to approximate $\\bar{\\mathbf{H}}_{k}$ using the empirical Fisher information matrix [47]. ", "page_idx": 4}, {"type": "text", "text": "3.3 Inferring the local layer-wise posteriors with the block-diagonal empirical Fisher information matrices ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "A empirical Fisher $\\tilde{\\mathbf{F}}$ is defined as below: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\tilde{\\mathbf{F}}=\\sum_{s\\in\\mathcal{D}}\\left[\\nabla\\log p(s|\\pmb{\\theta})\\nabla\\log p(s|\\pmb{\\theta})^{\\top}\\right]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $p(s|\\pmb\\theta)$ is the likelihood on data point $s$ . It is an approximate of the Fisher information matrix, the empirical Fisher information matrix is equivalent to the expectation of the Hessian of the negative log posterior if assuming $p(s|\\pmb\\theta)$ is identical for each $s\\in\\mathcal{D}$ . ", "page_idx": 4}, {"type": "text", "text": "Therefore, the local co-variance $\\Sigma_{k}$ can be approximated by the empirical Fisher $\\tilde{\\mathbf{F}}_{k}$ [48, 49]. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Sigma_{k}^{-1}\\approx\\tilde{\\mathbf{F}}_{k}+\\lambda\\mathbf{I}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The works [50, 51, 17] ignore co-relations between different parameters and only consider the self-relations of parameters as computing all co-relations is impossible. Thus, their methods are inaccurate. Detailed discussions and the novelty compared to previous works are in Appendix B. ", "page_idx": 4}, {"type": "text", "text": "In order to capture co-relations between different parameters efficiently, previous works [46, 43] estimate a block empirical Fisher information matrix $\\mathbf{F}$ instead of assuming parameters are independent and approximating the co-variance by the diagonal of the empirical Fisher. As pointed out, co-relations inner a layer are much more significant than others [46, 52, 53], while computing the co-relations between different layers brings slight improvement but much more computation [54, 43]. Therefore, assuming parameters are layer-independent is a good trade-off. As a result, the approximated layer-wise empirical Fisher is block-diagonal. For layer $l$ on client $k$ , its empirical Fisher $\\mathbf{F}_{k_{l}}$ is one of the diagonal blocks in the whole empirical Fisher for the local model and is factored into two small matrices as below, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pmb{\\Sigma}_{k_{l}}^{-1}\\approx\\mathbf{F}_{k_{l}}=\\mathbf{A}_{k_{l}}\\otimes\\mathbf{B}_{k_{l}}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "are two expectation factor matrices over the data samples; $\\hat{\\mathbf{a}}_{k_{l}}$ is the activations and $\\hat{\\mathbf{b}}_{k_{l}}$ is the gradient of the pre-activations of layer $l$ on client $k,\\,\\lambda$ is the hyperparameter and $\\pi_{l}$ is a factor minimizing approximation error in $\\mathbf{F}_{k_{l}}$ [46, 49, 55]. ${\\bf A}_{k_{l}}$ and $\\mathbf{B}_{k_{l}}$ are symmetric positive definite matrices [45, 46]. ", "page_idx": 4}, {"type": "text", "text": "We use $\\theta_{\\boldsymbol{k}_{l}}$ to denote the parameter vector of layer $l$ and $\\mathbf{M}_{k_{l}}=v e c^{-1}(\\pmb{\\mu}_{k_{l}})$ is the vectorized optimal weight matrix of layer $l$ on client $k$ . Thus, the resulting local layer-wise posterior approximation is $\\pmb{\\theta}_{k_{l}}\\overset{\\cdot}{\\sim}\\mathcal{N}(\\pmb{\\mu}_{k_{l}},\\mathbf{F}_{k_{l}}^{-1})$ . ", "page_idx": 4}, {"type": "text", "text": "3.4 Estimating the global expectation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Given the local posteriors, the global expectation could be aggregated by Eq. 7. With Eq. 10, the $l$ -th layer\u2019s global expectation $\\bar{\\pmb{\\mu}}_{l}$ consists of Kronecker products: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\bar{\\mu}}_{l}={\\bar{\\Sigma}}_{l}\\sum_{\\stackrel{k}{k}}^{K}{\\Sigma}_{k_{l}}^{-1}\\mu_{k_{l}}={\\bar{\\Sigma}}_{l}\\sum_{\\stackrel{k}{k}}^{K}\\left({\\bf A}_{k_{l}}\\otimes{\\bf B}_{k_{l}}\\right)\\mu_{k_{l}}}\\ ~}\\\\ {{\\displaystyle~~~={\\bar{\\Sigma}}_{l}\\sum_{k}^{K}\\mathrm{vec}({\\bf B}_{k_{l}}{\\bf M}_{k_{l}}{\\bf A}_{k_{l}})={\\bar{\\Sigma}}_{l}\\sum_{k}^{K}{\\bf z}_{k_{l}}={\\bar{\\Sigma}}_{l}{\\bar{\\bf z}}_{l}}\\ ~}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\begin{array}{r}{\\bar{\\mathbf{z}}_{l}=\\sum_{k}^{K}\\mathbf{z}_{k_{l}}}\\end{array}$ and $\\mathbf{z}_{k_{l}}=\\mathrm{vec}(\\mathbf{B}_{k_{l}}\\mathbf{M}_{k_{l}}\\mathbf{A}_{k_{l}})$ is a immediate notations for simplification. For the global expectation, we have $\\bar{\\pmb{\\mu}}=\\bar{\\\\pmb{\\Sigma}}\\cdot\\bar{\\pmb{\\mathbf{z}}}$ . The corresponding global co-variance is an inverse of the ", "page_idx": 4}, {"type": "text", "text": "sum of Kronecker products: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\bar{\\Sigma}_{l}=(\\sum_{k}^{K}\\mathbf{A}_{k_{l}}\\otimes\\mathbf{B}_{k_{l}})^{-1}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "As shown in Eq. 11, obtaining the global expectation $\\bar{\\pmb{\\mu}}_{l}$ requires calculating the inverse of $\\bar{\\Sigma}_{l}^{-1}$ as Eq. 12, which is unacceptable and the details are in Appendix C. Thus, we propose our method to directly train the parameters of the global model on the server side. ", "page_idx": 5}, {"type": "text", "text": "3.5 Train the parameters of the global model ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We use $\\mathbb{E}\\left[\\mathbf{A}\\right]$ denotes $\\sum_{k}^{K}(\\mathbf{A}_{k})$ , E [B] denotes $\\sum_{k}^{K}(\\mathbf{B}_{k})$ , $\\mathbb{E}\\left[\\mathbf{A}\\otimes\\mathbf{B}\\right]$ denotes $\\sum_{k}^{K}(\\mathbf{A}_{k}\\otimes\\mathbf{B}_{k})$ Previous works [46, 49]  approximate the expectatio n of Kronecker products by a Kro necker product of expectations $\\mathbb{E}\\left[\\mathbf{A}\\otimes\\mathbf{B}\\right]\\approx\\mathbb{E}\\left[\\mathbf{A}\\right]\\otimes\\mathbb{E}\\left[\\mathbf{B}\\right]$ with an assumption of ${\\mathbf{A}_{k_{l}}}$ and $\\mathbf{B}_{k_{l}}$ are independent, which is called Expectation Approximation (EA). However, it may lead to a biased global expectation. The details are discussed in Appendix D. Instead, we could construct a linear objective after aggregating the approximation of local posteriors via using block-diagonal empirical Fisher information matrices. We denotes $\\bar{\\bf M}$ as the matrix formula of $\\bar{\\pmb{\\mu}}=\\mathrm{vec}(\\bar{\\bf M})$ , and the optimal solution of $f(\\bar{\\pmb{\\mu}})$ is $\\bar{\\pmb{\\mu}}^{*}=$ $\\mathrm{vec}(\\bar{\\mathbf{M}}^{*})$ . We construct $f(\\bar{\\pmb{\\mu}})$ as a multi-variates linear objective function. When $\\bar{\\pmb{\\mu}}=\\bar{\\pmb{\\mu}}^{*}$ is optimal solution, $f(\\bar{\\mu})=\\mathbf{o}$ , where $\\mathbf{o}$ is a vector with all zero. Note that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\bar{\\pmb{\\mu}})=\\bar{\\pmb{\\Sigma}}^{-1}\\bar{\\pmb{\\mu}}-\\bar{\\pmb{\\Xi}}=\\displaystyle\\sum_{k}^{K}\\mathrm{vec}(\\pmb{\\mathrm{B}}_{k}\\bar{\\mathbf{M}}\\mathbf{A}_{k})-\\bar{\\pmb{\\Xi}}}\\\\ &{\\quad\\quad\\quad=\\mathrm{vec}(\\mathbb{E}\\left[\\mathbf{B}\\bar{\\mathbf{M}}\\mathbf{A}\\right])-\\bar{\\pmb{\\Xi}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "To obtain the optimal solution, we minimize the following problem to obtain an approximate solution $\\bar{\\bf M}^{*}$ of $\\bar{\\bf M}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\bar{\\mathbf{M}}^{*}=\\operatorname*{min}_{\\bar{\\mathbf{M}}}\\frac{1}{2}\\left\\|\\sum_{k}^{K}\\mathrm{vec}(\\mathbf{B}_{k}\\bar{\\mathbf{M}}\\mathbf{A}_{k})-\\bar{\\mathbf{z}}\\right\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The above equation is a quadratic objective, and it can be solved by modern optimization tools efficiently and conveniently. Since the main objective of the above problem is both convex and Lipschitz smooth w.r.t $\\mathrm{vec}(\\bar{\\mathbf{M}})$ , we can use the gradient descent method to solve it with a linear convergence rate. Here, we use automatic differentiation to calculate the gradient w.r.t. $\\bar{\\bf M}$ . ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 FedLPA Global Aggregation ", "text_level": 1, "page_idx": 5}, {"type": "table", "img_path": "I3IuclVLFZ/tmp/674598ebc59442c0a8a62e14220228c27b03fcb471a91f6cd291bdf69b3ea776.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "3.6 Overall FedLPA algorithm and discussions ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In summary, the proposed algorithm FedLPA follows the same paradigm as the standard one-shot federated learning framework. In FedLPA, the clients locally train their models to get ${{\\bf{M}}_{k}}$ and calculate the local co-variance over its training dataset using the layer-wise Laplace approximation to compute $\\mathbf{A}_{k},\\mathbf{B}_{k}$ . Subsequently, each client transmits their local ${\\bf A}_{k}$ $\\mathbf{\\delta}_{\\mathbf{k},\\mathbf{\\Delta}}\\mathbf{B}_{k},\\mathbf{M}_{k}$ to the server. Following Algorithm 1, the server aggregates these contributions to obtain the global expectation, as described in Eq. 7, then trains the global model parameters, as outlined in Eq. 14. Thus, the transmitted data between the clients and the server is solely ${\\bf A}_{k},{\\bf B}_{k},{\\bf M}_{k}$ without any extra auxiliary dataset and label information. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Note that FedLPA can be directly adopted in most common scenarios. For the special case that the neural model has enormous single-layer weight parameters, how to extend our proposed FedLPA is discussed in Appendix E. ", "page_idx": 6}, {"type": "text", "text": "3.7 t-SNE observation and discussions ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To quickly demonstrate the effectiveness of FedLPA, we show the t-SNE visualization of our FedLPA global model on the MNIST dataset as an example with a biased training data setting among 10 local clients. The experiment details, t-SNE visualizations of the local models and the global models of other algorithms and discussions are in Appendix G.1. As shown in Figure 1, FedLPA generates the global model which can clearly distinguish these classes, meanwhile, the classes are separate. ", "page_idx": 6}, {"type": "text", "text": "3.8 Privacy Discussions ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "FedLPA is intuitively compatible with existing privacypreserving techniques, such as differential privacy (DP) [56, 57], secure multiparty computation (SMC) [58, 59], and homomorphic encryption (HE) [60, 61, 62]. In Appendix F.1, we propose a naive DP-FedLPA with two different mechanisms to show the compatibility with differential privacy. Meanwhile, we mention that our proposed FedLPA has the same privacy-preserving level as the conventional federated learning algorithms (i.e, FedAvg, FedProx, FedNova and Dense). Compared with FedAvg, we have conducted a detailed analysis from a privacy attack perspective to show that our proposed FedLAP exhibits a security level consistent with FedAvg against several types of privacy attacks, where the details are shown in Appendix F.3. Note that the main focus of FedLPA is to improve the learning performance on the one-shot FL settings, thus, we leave the integration with other privacypreserving techniques beyond DP as an open problem. ", "page_idx": 6}, {"type": "image", "img_path": "I3IuclVLFZ/tmp/56dcd0d333ca16582ddad2b916b45287b74eb705c79eca430f10637b6213e6f5.jpg", "img_caption": ["Figure 1: t-SNE visualization for our FedLPA global model. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Experiments settings ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets. We conduct experiments on MNIST [63], Fashion-MNIST [64], CIFAR-10 [65], and SVHN [66] datasets. In most of the previous works and the most popular benchmark, the majority of their experiments use these datasets and these models. We choose these datasets and models to do the majority of our experiments following these established methods and benchmarks to fairly compare our method with the baselines. We use the data partitioning methods for non-IID settings of the benchmark 1 to simulate different label skews. Specifically, we try two different kinds of partition: 1) $\\#\\mathbf{C}=k$ : each client only has data from $k$ classes. We first assign $k$ random class IDs for each client. Next, we randomly and equally divide samples of each class to their assigned clients; 2) $p_{k}\\textrm{-}\\mathbf{D}\\mathrm{ir}(\\beta)$ : for each class, we sample from Dirichlet distribution $p_{k}$ and distribute $p_{k,j}$ portion of class $k$ samples to client $j$ . In this case, smaller $\\beta$ denotes worse skews. ", "page_idx": 6}, {"type": "text", "text": "Here\u2019s a brief overview of these datasets. MNIST Dataset: The MNIST dataset comprises binary images of handwritten digits. It consists of $60{,}000\\;28\\mathrm{x}28$ training images and 10,000 testing images. FMNIST Dataset: Similar to MNIST, the FMNIST dataset also contains $60\\small{,}000\\ 28\\mathrm{x}28$ training images and 10,000 testing images. SVHN Dataset: The SVHN dataset includes 73,257 32x32 color training images and 10,000 testing images. CIFAR-10 Dataset: CIFAR-10 consists of $60{,}000\\,32\\mathrm{x}32$ color images distributed across ten classes, with each class containing 6,000 images. The input dimensions for MNIST, FMNIST, SVHN, and CIFAR-10 are 784, 784, 3,072, and 3,072, respectively. ", "page_idx": 6}, {"type": "table", "img_path": "I3IuclVLFZ/tmp/5b6e4c68bf8af6663a86e989c9cba466dfe5bf3a0cb5250d300c203a4b5cd5a2.jpg", "table_caption": ["Table 1: Comparison with various FL algorithms in one round. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Training Details. By default, we follow FedAvg [12] and other existing studies [67, 68, 15] to use a simple CNN with 5 layers in our experiments. The experiments with more complex neural network structures are in Appendix G.8. We set the batch size to 64, the learning rate to 0.001, and the $\\lambda=0.001$ for FedLPA. By default, we set 10 clients and run 200 local epochs for each client. For the various settings of the number of clients and local epochs, we refer to Section 4.3 and Section 4.5. For results with error bars, we run three experiments with 5 different random seeds. Note that all methods were evaluated under fair comparison settings. Due to the page limit, representative results are represented in the main paper. Refer to Appendix G for more experimental details and additional results. ", "page_idx": 7}, {"type": "text", "text": "Baselines. To ensure fair comparisons, we neglect the comparison with methods that require to download auxiliary models or datasets, such as FedBE [69], FedKT [14] and FedGen [21], or even pretrained large model, like FedDISC [20]. FedOV [15] and FEDCAVE [23] entail sharing more client-side label information or transmitting client label information to the server, which could jeopardize label privacy and are beyond the scope of this study. XorMixFL [22] may not be practical, as we mentioned before. FedFisher [41] is not publicly available. FedDF [26], DAFL [35] and ADI [36] are compared with the state-of-the-art data-free method DENSE [24]. Co-Boosting [39] requires too many computational resources2. In conclusion, we include one-shot FL algorithms as baselines including FedAvg [12], FedProx [31], FedNova [30], SCAFFOLD [29] and DENSE [24]. All the methods are fairly compared, and our implementation is available and the experiment details can be viewed in Appendix G.11. ", "page_idx": 7}, {"type": "text", "text": "4.2 An overall comparison ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We compare the accuracy between FedLPA and the other baselines as shown in Table 1, the data in the green shadow shows the best results. FedLPA can achieve the best performance in all the dataset and partition settings. In extreme cases such as $\\beta=\\{0.01,0.05\\}$ , $\\#C=1$ , $\\#\\Gamma=2$ , FedLPA exhibits a significant performance advantage over the baseline algorithms. This demonstrates our framework\u2019s ability to effectively aggregate valuable information from local clients for global weight training. ", "page_idx": 7}, {"type": "table", "img_path": "I3IuclVLFZ/tmp/1df930175ef154d78acc1149eb3fd9acbbb9bba319f10010849fcc32e9d7bda8.jpg", "table_caption": ["Table 2: Experimental results of varying number of clients on FMNIST dataset. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "In summary, the state-of-the-art DENSE could be comparable with FedLPA when the skew level is small. However, with the increment of skewness, FedLPA shows significantly superior results. ", "page_idx": 8}, {"type": "text", "text": "4.3 Scalability ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We assess the scalability of FedLPA by varying the number of clients. In this section, we show results on FMNIST in Table 2. From the table, we can observe that FedLPA still almost always achieves the best accuracy when increasing the number of clients. Notably, there is a slight exception highlighted in red, where DENSE outperforms us when we have 20 clients and $\\beta=0.5$ , this may be attributed to the dataset being less biased and the DENSE only getting a marginal $0.03\\%$ higher test accuracy. Our method is generally much more robust in all kinds of settings. ", "page_idx": 8}, {"type": "table", "img_path": "I3IuclVLFZ/tmp/e42f33904078e85c1e7a95b1aeed26a218f0ae7434646b52d77935383fd81b79.jpg", "table_caption": ["Table 3: Experiments with different proportions of data samples. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.4 Experiments with different proportions of data samples ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We have added the experiments with our method on the same experiment setting with 10 clients. We conducted experiments on FMNIST datasets with $\\beta{=}0.1$ , 0.3 and 0.5. The performance changes w.r.t the number of data samples are shown in Table 3. We could see that our method FedLPA could yield satisfactory results even with only $20\\%$ data samples under multiple settings. ", "page_idx": 8}, {"type": "text", "text": "4.5 Ablation study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The hyper-parameter of our approach is $\\lambda$ , which controls variances of a priori normal distribution and guarantees ${\\bf A}_{k}$ and $\\mathbf{B}_{k}$ are positive semi-definite. In this part, we show results on FMNIST. All other Laplace Approximations are sensitive to the hyper-parameter $\\lambda$ based on their experimental results, Table 4 shows that our approach is relatively robust. Based on our numerical results, we set $\\lambda=0.001$ by default for our method FedLPA. ", "page_idx": 8}, {"type": "text", "text": "We also conduct the experiments when the local epochs are 10,20,50,100. More experiments are available in Appendix G.2, which shows that our methods outperform all the baselines in all kinds of scenarios without requiring extensive tuning. ", "page_idx": 8}, {"type": "text", "text": "4.6 Communication and computation overhead ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We conduct experiments on CIFAR-10 on a single 2080Ti GPU to estimate the overall communication and computation overhead. We set the number of clients is 10. Table 5 shows the numerical results on ", "page_idx": 8}, {"type": "table", "img_path": "I3IuclVLFZ/tmp/a4826248d4a0c96cdb7e69cc9a5ce4d9407b0dba13c54c738e285d00221765e6.jpg", "table_caption": ["Table 4: Experimental results of different hyper-parameter $\\lambda$ on FMNIST dataset. "], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "I3IuclVLFZ/tmp/89aa7140812d8a8c77c495d834c139e7904dab0b9dbc2cb79ee331c54868202e.jpg", "table_caption": ["Table 5: Communication and computation overhead evaluation. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "FedLPA and baselines. Details of the overhead evaluation are referred to Appendix G.6 and G.7. Our observations reveal that FedLPA is slightly slower than FedNova, SCAFFOLD, FedAvg, and FedProx, while much faster than DENSE. FedLPA also has significantly improved the one-shot learning performance of the above four approaches. Similarly, FedLPA performs moderately incremental communication overhead while outperforming other baseline approaches on learning performance, as one-shot FL introduces heavy computation overhead while communication overhead is usually small. It is noteworthy that FedLPA strikes a favorable balance between computation and communication overhead, making it the most promising approach for one-shot FL. ", "page_idx": 9}, {"type": "text", "text": "4.7 Extension to multiple rounds ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We conduct experiments on MNIST with 10 clients and data partitioning $p_{k}-\\mathrm{Dir}(\\beta=0.5)$ . The results are shown in Figure 2. As DENSE could not support multiple rounds, we compare our methods with FedAvg, FedNova, SCAFFOLD, and FedProx. FedLPA achieves the highest accuracy in the first round, denoting the strongest learning capabilities in a one-shot setting. With the increment in the number of rounds, the performances of FedLPA increase slower than the other baseline approaches. This figure shows that the joint approach (ours (one round) then FedAvg) that utilizes FedLPA in the first round and then adopts other baseline methods may be most promising to save communication and computation resources in the multiple-round federated learning scenario. ", "page_idx": 9}, {"type": "image", "img_path": "I3IuclVLFZ/tmp/4c11fea61b5f4cc5be1ed27313fa72f1ba4994d6119c7b26ff57c2f7a1d4892e.jpg", "img_caption": ["Figure 2: Extension to multiple rounds on MNIST dataset. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "4.8 Supplementary experiments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Experiments for privacy concerns, experiments on ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "different local epoch numbers, experiments in extreme settings (the number of clients ${}^{,=5}$ , $\\beta=0.001)$ ), experiments with more methods, experiments with more complex network structures, experiments with more complex datasets, ablation experiments analyzing the number of approximation iterations of FedLPA can be found in Appendix. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we design a novel one-shot FL algorithm FedLPA to better model the global parameters in effective one-shot federated learning. We propose a method that could aggregate the local clients in a layer-wise manner with their posterior approximation via block-diagonal empirical Fisher information matrices, which could effectively capture the accurate statistics of a locally biased dataset. Overall, FedLPA stands out as the most practical and efficient framework that conducts data-free one-shot FL, particularly well-suited for high data heterogeneity in various settings, considering it significantly outperforms other baselines with extensive experiments. Our FedLPA is available in https://github.com/lebronlambert/FedLPA_NeurIPS2024. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank the anonymous reviewers. The authors would like to thank Yiqun Diao from National Univeristy of Singapore for his valuable comments to improve this work. Dr. Jialin Li is supported by the Singapore Ministry of Education Academic Research Fund Tier 1 (T1 251RES2104) and Tier 2 (MOE-T2EP20222-0016). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436\u2013444, 2015.   \n[2] J\u00fcrgen Schmidhuber. Deep learning in neural networks: An overview. Neural networks, 61:85\u2013117, 2015.   \n[3] Dario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, Jingliang Bai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Qiang Cheng, Guoliang Chen, et al. Deep speech 2: End-to-end speech recognition in english and mandarin. In International conference on machine learning, pages 173\u2013182. PMLR, 2016.   \n[4] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Communications of the ACM, 60(6):84\u201390, 2017.   \n[5] Samira Pouyanfar, Saad Sadiq, Yilin Yan, Haiman Tian, Yudong Tao, Maria Presa Reyes, MeiLing Shyu, Shu-Ching Chen, and Sundaraja S Iyengar. A survey on deep learning: Algorithms, techniques, and applications. ACM Computing Surveys (CSUR), 51(5):1\u201336, 2018.   \n[6] Qingchen Zhang, Laurence T Yang, Zhikui Chen, and Peng Li. A survey on deep learning for big data. Information Fusion, 42:146\u2013157, 2018.   \n[7] Samira Pouyanfar, Saad Sadiq, Yilin Yan, Haiman Tian, Yudong Tao, Maria Presa Reyes, MeiLing Shyu, Shu-Ching Chen, and Sundaraja S Iyengar. A survey on deep learning: Algorithms, techniques, and applications. ACM Computing Surveys (CSUR), 51(5):1\u201336, 2018.   \n[8] Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. Federated machine learning: Concept and applications. ACM Transactions on Intelligent Systems and Technology (TIST), 10(2):1\u201319, 2019.   \n[9] H Brendan McMahan, Eider Moore, Daniel Ramage, and Blaise Ag\u00fcera y Arcas. Federated learning of deep networks using model averaging. arXiv preprint arXiv:1602.05629, 2, 2016.   \n[10] Peter Kairouz, H Brendan McMahan, Brendan Avent, Aur\u00e9lien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. Foundations and Trends\u00ae in Machine Learning, 14(1\u20132):1\u2013210, 2021.   \n[11] Qinbin Li, Zeyi Wen, Zhaomin Wu, Sixu Hu, Naibo Wang, Yuan Li, Xu Liu, and Bingsheng He. A survey on federated learning systems: Vision, hype and reality for data privacy and protection. IEEE Transactions on Knowledge and Data Engineering, 2021.   \n[12] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics, pages 1273\u20131282. PMLR, 2017.   \n[13] Neel Guha, Ameet Talwalkar, and Virginia Smith. One-shot federated learning. arXiv preprint arXiv:1902.11175, 2019.   \n[14] Qinbin Li, Bingsheng He, and Dawn Song. Practical one-shot federated learning for cross-silo setting. arXiv preprint arXiv:2010.01017, 2020.   \n[15] Yiqun Diao, Qinbin Li, and Bingsheng He. Towards addressing label skews in one-shot federated learning. In The Eleventh International Conference on Learning Representations, 2023.   \n[16] Maruan Al-Shedivat, Jennifer Gillenwater, Eric Xing, and Afshin Rostamizadeh. Federated learning via posterior averaging: A new perspective and practical algorithms. arXiv preprint arXiv:2010.05273, 2020.   \n[17] Liangxi Liu, Xi Jiang, Feng Zheng, Hong Chen, Guo-Jun Qi, Heng Huang, and Ling Shao. A bayesian federated learning framework with online laplace approximation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.   \n[18] Yanlin Zhou, George Pu, Xiyao Ma, Xiaolin Li, and Dapeng Wu. Distilled one-shot federated learning. arXiv preprint arXiv:2009.07999, 2020.   \n[19] Jie Zhang, Chen Chen, Bo Li, Lingjuan Lyu, Shuang Wu, Jianghe Xu, Shouhong Ding, and Chao Wu. A practical data-free approach to one-shot federated learning with heterogeneity. arXiv preprint arXiv:2112.12371, 1, 2021.   \n[20] Mingzhao Yang, Shangchao Su, Bin Li, and Xiangyang Xue. Exploring one-shot semi-supervised federated learning with a pre-trained diffusion model. arXiv preprint arXiv:2305.04063, 2023.   \n[21] Zhuangdi Zhu, Junyuan Hong, and Jiayu Zhou. Data-free knowledge distillation for heterogeneous federated learning. In International conference on machine learning, pages 12878\u201312889. PMLR, 2021.   \n[22] MyungJae Shin, Chihoon Hwang, Joongheon Kim, Jihong Park, Mehdi Bennis, and SeongLyun Kim. Xor mixup: Privacy-preserving data augmentation for one-shot federated learning. arXiv preprint arXiv:2006.05148, 2020.   \n[23] Clare Elizabeth Heinbaugh, Emilio Luz-Ricca, and Huajie Shao. Data-free one-shot federated learning under very high statistical heterogeneity. In The Eleventh International Conference on Learning Representations, 2022.   \n[24] Jie Zhang, Chen Chen, Bo Li, Lingjuan Lyu, Shuang Wu, Shouhong Ding, Chunhua Shen, and Chao Wu. Dense: Data-free one-shot federated learning. Advances in Neural Information Processing Systems, 35:21414\u201321428, 2022.   \n[25] Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet S Talwalkar. Federated multitask learning. Advances in neural information processing systems, 30, 2017.   \n[26] Tao Lin, Lingjing Kong, Sebastian U Stich, and Martin Jaggi. Ensemble distillation for robust model fusion in federated learning. Advances in Neural Information Processing Systems, 33:2351\u20132363, 2020.   \n[27] Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. Federated learning with non-iid data. arXiv preprint arXiv:1806.00582, 2018.   \n[28] Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of fedavg on non-iid data. arXiv preprint arXiv:1907.02189, 2019.   \n[29] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In International Conference on Machine Learning, pages 5132\u20135143. PMLR, 2020.   \n[30] Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor. Tackling the objective inconsistency problem in heterogeneous federated optimization. Advances in neural information processing systems, 33:7611\u20137623, 2020.   \n[31] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. Proceedings of Machine Learning and Systems, 2:429\u2013450, 2020.   \n[32] Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia Hoang, and Yasaman Khazaeni. Bayesian nonparametric federated learning of neural networks. In International Conference on Machine Learning, pages 7252\u20137261. PMLR, 2019.   \n[33] Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and Yasaman Khazaeni. Federated learning with matched averaging. arXiv preprint arXiv:2002.06440, 2020.   \n[34] Romain Thibaux and Michael I Jordan. Hierarchical beta processes and the indian buffet process. In Artificial intelligence and statistics, pages 564\u2013571. PMLR, 2007.   \n[35] Hanting Chen, Yunhe Wang, Chang Xu, Zhaohui Yang, Chuanjian Liu, Boxin Shi, Chunjing Xu, Chao Xu, and Qi Tian. Data-free learning of student networks. In Proceedings of the IEEE international conference on computer vision, pages 3514\u20133522, 2019.   \n[36] Hongxu Yin, Pavlo Molchanov, Jose M Alvarez, Zhizhong Li, Arun Mallya, Derek Hoiem, Niraj K Jha, and Jan Kautz. Dreaming to distill: Data-free knowledge transfer via deepinversion. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8715\u20138724, 2020.   \n[37] Aviv Shamsian, Aviv Navon, Ethan Fetaya, and Gal Chechik. Personalized federated learning using hypernetworks. In International Conference on Machine Learning, pages 9489\u20139502. PMLR, 2021.   \n[38] David Krueger, Chin-Wei Huang, Riashat Islam, Ryan Turner, Alexandre Lacoste, and Aaron Courville. Bayesian hypernetworks. arXiv preprint arXiv:1710.04759, 2017.   \n[39] Rong Dai, Yonggang Zhang, Ang Li, Tongliang Liu, Xun Yang, and Bo Han. Enhancing one-shot federated learning through data and ensemble co-boosting. arXiv preprint arXiv:2402.15070, 2024.   \n[40] Shangchao Su, Bin Li, and Xiangyang Xue. One-shot federated learning without server-side training. Neural Networks, 164:203\u2013215, 2023.   \n[41] Divyansh Jhunjhunwala, Shiqiang Wang, and Gauri Joshi. Fedfisher: Leveraging fisher information for one-shot federated learning. In International Conference on Artificial Intelligence and Statistics, pages 1612\u20131620. PMLR, 2024.   \n[42] Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Challenges, methods, and future directions. IEEE Signal Processing Magazine, 37(3):50\u201360, 2020.   \n[43] Hippolyt Ritter, Aleksandar Botev, and David Barber. A scalable laplace approximation for neural networks. In 6th International Conference on Learning Representations, ICLR 2018-Conference Track Proceedings, volume 6. International Conference on Representation Learning, 2018.   \n[44] Erik Daxberger, Agustinus Kristiadi, Alexander Immer, Runa Eschenhagen, Matthias Bauer, and Philipp Hennig. Laplace redux-effortless bayesian deep learning. Advances in Neural Information Processing Systems, 34:20089\u201320103, 2021.   \n[45] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-propagating errors. nature, 323(6088):533\u2013536, 1986.   \n[46] James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate curvature. In International conference on machine learning, pages 2408\u20132417. PMLR, 2015.   \n[47] Charles F Van Loan. The ubiquitous kronecker product. Journal of computational and applied mathematics, 123(1-2):85\u2013100, 2000.   \n[48] James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate curvature. In International conference on machine learning, pages 2408\u20132417. PMLR, 2015.   \n[49] Roger Grosse and James Martens. A kronecker-factored approximate fisher matrix for convolution layers. In International Conference on Machine Learning, pages 573\u2013582. PMLR, 2016.   \n[50] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521\u20133526, 2017.   \n[51] Michael S Matena and Colin A Raffel. Merging models with fisher-weighted averaging. Advances in Neural Information Processing Systems, 35:17703\u201317716, 2022.   \n[52] Frederik Benzing. Gradient descent on neurons and its link to approximate second-order optimization. In International Conference on Machine Learning, pages 1817\u20131853. PMLR, 2022.   \n[53] Lin Zhang, Shaohuai Shi, Wei Wang, and Bo Li. Scalable k-fac training for deep neural networks with distributed preconditioning. IEEE Transactions on Cloud Computing, 2022.   \n[54] James Martens. Second-order optimization for neural networks. University of Toronto (Canada), 2016.   \n[55] Aleksandar Botev, Hippolyt Ritter, and David Barber. Practical gauss-newton optimisation for deep learning. In International Conference on Machine Learning, pages 557\u2013565. PMLR, 2017.   \n[56] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In Theory of Cryptography: Third Theory of Cryptography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006. Proceedings 3, pages 265\u2013284. Springer, 2006.   \n[57] Lingjuan Lyu, Han Yu, Xingjun Ma, Chen Chen, Lichao Sun, Jun Zhao, Qiang Yang, and S Yu Philip. Privacy and robustness in federated learning: Attacks and defenses. IEEE transactions on neural networks and learning systems, 2022.   \n[58] Andrew C Yao. Protocols for secure computations. In 23rd annual symposium on foundations of computer science (sfcs 1982), pages 160\u2013164. IEEE, 1982.   \n[59] Daniel Demmler, Thomas Schneider, and Michael Zohner. Aby-a framework for efficient mixed-protocol secure two-party computation. In Network and Distributed System Security Symposium, 2015.   \n[60] Taher ElGamal. A public key cryptosystem and a signature scheme based on discrete logarithms. IEEE transactions on information theory, 31(4):469\u2013472, 1985.   \n[61] Pascal Paillier. Public-key cryptosystems based on composite degree residuosity classes. In International conference on the theory and applications of cryptographic techniques, pages 223\u2013238. Springer, 1999.   \n[62] Craig Gentry. Fully homomorphic encryption using ideal lattices. In Proceedings of the forty-first annual ACM symposium on Theory of computing, pages 169\u2013178, 2009.   \n[63] Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.   \n[64] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.   \n[65] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[66] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. 2011.   \n[67] Tianhao Wang, Johannes Rausch, Ce Zhang, Ruoxi Jia, and Dawn Song. A principled approach to data valuation for federated learning. Federated Learning: Privacy and Incentive, pages 153\u2013167, 2020.   \n[68] Qinbin Li, Yiqun Diao, Quan Chen, and Bingsheng He. Federated learning on non-iid data silos: An experimental study. In 2022 IEEE 38th International Conference on Data Engineering (ICDE), pages 965\u2013978. IEEE, 2022.   \n[69] Hong-You Chen and Wei-Lun Chao. Fedbe: Making bayesian model ensemble applicable to federated learning. arXiv preprint arXiv:2009.01974, 2020.   \n[70] Roger Grosse and James Martens. A kronecker-factored approximate fisher matrix for convolution layers. In International Conference on Machine Learning, pages 573\u2013582. PMLR, 2016.   \n[71] Aleksandar Botev, Hippolyt Ritter, and David Barber. Practical gauss-newton optimisation for deep learning. In International Conference on Machine Learning, pages 557\u2013565. PMLR, 2017.   \n[72] Elaine Angelino, Matthew James Johnson, Ryan P Adams, et al. Patterns of scalable bayesian inference. Foundations and Trends\u00ae in Machine Learning, 9(2-3):119\u2013247, 2016.   \n[73] Ward Cheney and David Kincaid. Numerical mathematics and computing. International Thomson Publishing, 1998.   \n[74] David A Belsley, Edwin Kuh, and Roy E Welsch. Regression diagnostics: Identifying influential data and sources of collinearity. John Wiley & Sons, 2005.   \n[75] M Hashem Pesaran. Time series and panel data econometrics. Oxford University Press, 2015.   \n[76] Lloyd N Trefethen and David Bau. Numerical linear algebra. SIAM, 2022.   \n[77] Jongseok Lee, Matthias Humt, Jianxiang Feng, and Rudolph Triebel. Estimating model uncertainty of neural networks in sparse information form. In International Conference on Machine Learning, pages 5702\u20135713. PMLR, 2020.   \n[78] Thomas George, C\u00e9sar Laurent, Xavier Bouthillier, Nicolas Ballas, and Pascal Vincent. Fast approximate natural gradient descent in a kronecker factored eigenbasis. Advances in Neural Information Processing Systems, 31, 2018.   \n[79] Jiankai Sun, Xin Yang, Yuanshun Yao, and Chong Wang. Label leakage and protection from forward embedding in vertical federated learning. arXiv preprint arXiv:2203.01451, 2022.   \n[80] Aidmar Wainakh, Fabrizio Ventola, Till M\u00fc\u00dfig, Jens Keim, Carlos Garcia Cordero, Ephraim Zimmer, Tim Grube, Kristian Kersting, and Max M\u00fchlh\u00e4user. User-level label leakage from gradients in federated learning. Proceedings on Privacy Enhancing Technologies, 2022(2):227\u2013 244, 2022.   \n[81] Luca Melis, Congzheng Song, Emiliano De Cristofaro, and Vitaly Shmatikov. Exploiting unintended feature leakage in collaborative learning. In 2019 IEEE symposium on security and privacy $(S P)$ , pages 691\u2013706. IEEE, 2019.   \n[82] Jonas Geiping, Hartmut Bauermeister, Hannah Dr\u00f6ge, and Michael Moeller. Inverting gradients-how easy is it to break privacy in federated learning? Advances in Neural Information Processing Systems, 33:16937\u201316947, 2020.   \n[83] Cynthia Dwork. Differential privacy. Encyclopedia of Cryptography and Security, pages 338\u2013340, 2011.   \n[84] Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Foundations and Trends\u00ae in Theoretical Computer Science, 9(3\u20134):211\u2013407, 2014.   \n[85] Robin C Geyer, Tassilo Klein, and Moin Nabi. Differentially private federated learning: A client level perspective. arXiv preprint arXiv:1712.07557, 2017.   \n[86] Yiqun Diao, Qinbin Li, and Bingsheng He. Exploiting label skews in federated learning with model concatenation. arXiv preprint arXiv:2312.06290, 2023.   \n[87] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit confidence information and basic countermeasures. In Proceedings of the 22nd ACM SIGSAC conference on computer and communications security, pages 1322\u20131333, 2015.   \n[88] Briland Hitaj, Giuseppe Ateniese, and Fernando Perez-Cruz. Deep models under the gan: information leakage from collaborative deep learning. In Proceedings of the 2017 ACM SIGSAC conference on computer and communications security, pages 603\u2013618, 2017.   \n[89] Cynthia Dwork, Adam Smith, Thomas Steinke, Jonathan Ullman, and Salil Vadhan. Robust traceability from trace amounts. In 2015 IEEE 56th Annual Symposium on Foundations of Computer Science, pages 650\u2013669. IEEE, 2015.   \n[90] Apostolos Pyrgelis, Carmela Troncoso, and Emiliano De Cristofaro. Knock knock, who\u2019s there? membership inference on aggregate location data. arXiv preprint arXiv:1708.06145, 2017.   \n[91] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against machine learning models. In 2017 IEEE symposium on security and privacy $(S P)$ , pages 3\u201318. IEEE, 2017.   \n[92] Jamie Hayes, Luca Melis, George Danezis, and E LOGAN De Cristofaro. Membership inference attacks against generative models. URL https://api. semanticscholar. org/CorpusID, 202588705, 2018.   \n[93] Yunhui Long, Vincent Bindschaedler, Lei Wang, Diyue Bu, Xiaofeng Wang, Haixu Tang, Carl A Gunter, and Kai Chen. Understanding membership inferences on well-generalized learning models. arXiv preprint arXiv:1802.04889, 2018.   \n[94] Stacey Truex, Ling Liu, Mehmet Emre Gursoy, Lei Yu, and Wenqi Wei. Towards demystifying membership inference attacks. arXiv preprint arXiv:1807.09173, 2018.   \n[95] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n[96] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.   \n[97] Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. Emnist: Extending mnist to handwritten letters. In 2017 international joint conference on neural networks (IJCNN), pages 2921\u20132926. IEEE, 2017.   \n[98] Ya Le and Xuan S. Yang. Tiny imagenet visual recognition challenge. 2015.   \n[99] Frederik Kunstner, Philipp Hennig, and Lukas Balles. Limitations of the empirical fisher approximation for natural gradient descent. Advances in neural information processing systems, 32, 2019.   \n[100] James Martens, Jimmy Ba, and Matt Johnson. Kronecker-factored curvature approximations for recurrent neural networks. In International Conference on Learning Representations, 2018.   \n[101] Blair Bilodeau, Yanbo Tang, and Alex Stringer. On the tightness of the laplace approximation for statistical inference. Statistics & Probability Letters, 198:109839, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "A The FedLPA algorithm ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The proposed algorithm follows the same paradigm as the standard one-shot federated learning framework. Each client follows the local training procedure as shown in the paper. The global aggregation is illustrated in Algorithm 1. ", "page_idx": 16}, {"type": "text", "text": "With the Algorithms, let us assume the dimensionality list of each layer in a fully connected neural network is $([s_{0},s_{1},s_{2},...,s_{l},...,s_{L}])$ , which means the size of the weight $\\mathbf{W}_{k_{l}}$ of layer $l$ is $s_{l-1}\\mathbf{X}s_{l}$ . Consequently, the size of ${\\mathbf{A}_{k_{l}}}$ for this layer would be $s_{l-1}\\mathbf{x}s_{l-1}$ , and the size of $\\mathbf{B}_{k_{l}}$ would be $s_{l}\\mathbf{X}.s_{l}$ . The size of $\\mathbf{F}_{k_{l}}$ is $(s_{l-1}\\mathbf{x}s_{l})\\mathbf{x}(s_{l-1}\\mathbf{x}s_{l})$ . ", "page_idx": 16}, {"type": "text", "text": "Then, we give a concrete example to show the dimensions of different matrices using a fullyconnected neural network model with architecture 784-256-64-10 as in Appendix G. Then, the $\\mathbf{M}_{k_{1}}$ is $784\\mathrm{x}256{+}256$ , $\\mathbf{M}_{k_{2}}$ is $256\\mathrm{x}64\\substack{+64}$ , ${{\\bf{M}}_{k_{3}}}$ is $64\\mathrm{x}10{+}10$ . The ${\\bf A}_{k_{1}}$ is $785\\mathrm{x}785$ , $\\mathbf{A}_{k_{2}}$ is $257\\!\\times\\!257$ , ${\\bf A}_{k_{3}}$ is 65x65. The $\\mathbf{B}_{k_{1}}$ is 256x256, $\\mathbf{B}_{k_{2}}$ is $64\\mathrm{x64}$ , ${\\bf B}_{k_{3}}$ is 10x10. Then the $\\mathbf{F}_{k_{1}}$ is $(785\\mathrm{x}785)\\mathrm{x}(256\\mathrm{x}256)$ , $\\mathbf{F}_{k_{2}}$ is $(257\\mathrm{x}257)\\mathrm{x}(64\\mathrm{x}64)$ ), $\\mathbf{F}_{k_{3}}$ is $(65\\mathrm{x}65)\\mathrm{x}(10\\mathrm{x}10)$ . The $\\mathbf{F}_{k}$ is $(785\\times785+257\\times257+65\\times65)\\mathrm{x}(256\\mathrm{x}256+64\\mathrm{x}64+10\\mathrm{x}10)$ ). ", "page_idx": 16}, {"type": "text", "text": "However, in fact, we do not need to combine the ${\\mathbf{A}_{k_{l}}}$ , $\\mathbf{B}_{k_{l}}$ , $\\mathbf{F}_{k_{l}}$ into ${\\bf A}_{k}$ , $\\mathbf{B}_{k}$ , $\\mathbf{F}_{k}$ . In this paper, we utilize the diagonal block property to compute each block in our method. ", "page_idx": 16}, {"type": "text", "text": "B Comparison with the previous methods ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "To the best of our knowledge, we are the first to consider the posterior inference problem in the one-shot scenario. Note that the approach [16] requires a lengthy burn-in period before conducting posterior inference, for instance, 400 rounds, and it updates global model parameters by modifying the covariance-aggregated local models. It means that the algorithm [16] necessarily requires multiple iterations and cannot be used in a one-shot scenario. In contrast, our method FedLPA only requires immediate variational inference after training the local model, ensuring higher flexibility and efficiency in the one-shot scenario. ", "page_idx": 16}, {"type": "text", "text": "Besides, in the algorithm [16], obtaining statistical information to compute local covariances is of low rank. In reality, it fails to acquire the posterior of the aggregated model and cannot perform variational inference on the aggregated model. However, our method yields full-rank covariances, and after employing an expectation approximation method for variational inference on the aggregated model, we can achieve a usable global posterior. ", "page_idx": 16}, {"type": "text", "text": "In both the domain of natural gradient optimization [48, 70, 16] and modeling output uncertainty in variational inference [71], using the Fisher approximation of the Hessian does not involve the issue of inverting covariance. However, in the context of federated learning, when performing variational inference on the aggregated model, the necessity of inverting covariance becomes unavoidable. To address this problem, we propose a novel algorithm that constructs a quadratic objective function. During aggregation, this algorithm directly trains the aggregated model using local covariances and expectations, thereby circumventing the need for inversion operations. ", "page_idx": 16}, {"type": "text", "text": "Note that the previous methods [51, 17] adopt the same core approach that utilizes the online Laplace approximation to obtain diagonal Fisher for model aggregation, in which they conduct experiments on different datasets and published on different venues. We mainly analyze our approach with the comparison of DiagonalFisher [17]. DiagonalFisher assumes independence among parameters, neglecting inter-parameter correlations, resulting in inaccurate posterior approximations. However, strong correlations exist among parameters within each layer, such as matching patterns in convolutional kernels within convolutional networks. This is a crucial factor that cannot be overlooked; otherwise, aggregation of the posterior would result in lower posterior regions, as compared to our method. In complex environments, employing diagonal Fisher for aggregation would prove to be entirely ineffective, whereas our method effectively leverages inter-parameter correlations at each layer, rendering it more robust. To demonstrate, we present results comparing aggregation using diagonal Fisher and our method. We have added experiments using the settings of our paper and an MLP model (784-256-64-10) on the FMNIST dataset with five random seeds for one-shot FL, the client number is 10, and the $\\beta{=}0.01$ . The results are in Table 6. ", "page_idx": 16}, {"type": "text", "text": "In the table, \u201cInitial\" denotes whether the client models were initialized using the same parameter values or independently. ", "page_idx": 16}, {"type": "table", "img_path": "I3IuclVLFZ/tmp/198d8937a38cdabc97158b6a458f9441b9d5fd5c682f299cb172952f58a80451.jpg", "table_caption": ["Table 6: Experiments with DiagonalFisher using MLP. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "When \u201cInitial\" is set to \u201cSame\", all client models are trained on their respective datasets using identical parameter values for initialization. Consequently, there exists a strong correlation among the local models. Additionally, in this scenario, model aggregation is equivalent to aggregating updates of local models. Although DiagonalFisher performs reasonably well under this condition, our method demonstrates superior performance, exhibiting a $20.29\\%$ increase in global test accuracy. ", "page_idx": 17}, {"type": "text", "text": "When \u201cInitial\" is set to \"Different\", the models on different clients start training with distinct parameter values. Due to the high heterogeneity of local datasets, there is minimal correlation among local models. In this extreme scenario, DiagonalFisher completely fails, while our method maintains an accuracy of $73.73\\%$ , showcasing remarkable robustness. ", "page_idx": 17}, {"type": "text", "text": "It is essential to consider the indispensability of parameter correlations, which is why we compute correlations among parameters within layers to ensure the robustness and accuracy of model aggregation. ", "page_idx": 17}, {"type": "text", "text": "Now, we discuss some related works which directly utilize K-FAC to approximate the Fisher matrix and make a comparison with our proposed approach FedLPA. The works [48, 70, 71] have provided us with significant inspiration. However, methods like K-FAC do not require computing the inverse of covariance. Nevertheless, in the context of federated learning, the necessity of inverting covariance becomes unavoidable during variational inference on the aggregated model. ", "page_idx": 17}, {"type": "text", "text": "Methods like K-FAC assume direct independence among data samples to utilize expectation approximation. They obtain the inverse of Fisher from individual samples and then directly compute the expectation, thereby avoiding inverse operations. However, the expectation approximation inevitably leads to biased results during model aggregation. Detailed analysis can be found in Appendix D. ", "page_idx": 17}, {"type": "text", "text": "To address this issue, we propose a novel algorithm that constructs a quadratic objective function. During aggregation, this algorithm directly trains the aggregated model using local covariances and expectations, eliminating the need for inversion operations. This aims to minimize aggregation biases as much as possible. ", "page_idx": 17}, {"type": "text", "text": "Here, we provide a comparative analysis of different methods. ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "FedAvg and FedProx minimize the Kullback-Leibler (KL) divergence between the local and global posteriors: $\\begin{array}{r}{\\bar{\\pmb{\\mu}},\\bar{\\pmb{\\Sigma}}^{-1}=\\operatorname*{min}_{\\bar{\\pmb{\\mu}},\\bar{\\pmb{\\Sigma}}^{-1}}K L\\left((\\sum_{k}^{K}p(\\pmb{\\theta}|\\mathcal{D}_{k}))|p(\\pmb{\\theta}|\\mathcal{D})\\right)}\\end{array}$ . SCAFFOLD computes the bias term, and FedNova computes the correction term. None of these four methods consider the correlations between parameters. DENSE and FedOV, on the other hand, employ distillation methods, attempting to extract the distribution of non-iid data among clients through distillation. However, this itself leads to information loss due to dimensionality reduction and introduces additional variance of data. ", "page_idx": 17}, {"type": "text", "text": "Although the work [72] also uses the distributed Bayesian inference, however, it focuses on the dataset feature and could not be applied to train the global model parameters. ", "page_idx": 17}, {"type": "text", "text": "In conclusion, the reason our approach performs better in this scenario stems from our improved approximation of the global posterior. This approach signifies our novelty in addressing these challenges. ", "page_idx": 17}, {"type": "text", "text": "B.1 The efficiency of FedLPA ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Although the number of uploaded bits increased per round of FedLPA, it resulted in a significant improvement in the final outcome. Additionally, the increase in transmitted bits enhanced the robustness of the aggregation method. Moreover, as indicated in Table 5 of the paper, we observe only a marginal increase in the amount of communication required. ", "page_idx": 17}, {"type": "text", "text": "A fully-connected neural network model with architecture 784-256-64-10, has $784\\cdot256+256+$ $256\\cdot64+64+64\\cdot10+10=217930$ floating point numbers, which is 6973760 bits or around 0.831 ", "page_idx": 17}, {"type": "table", "img_path": "I3IuclVLFZ/tmp/24b1cc600eacf403c743b3b7520558264cf2354a0daf63417cebe4f794e73ff1.jpg", "table_caption": ["Table 7: Experiments with DiagonalFisher considering efficiency. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "MB. For one communication from a client to the server, our approach needs to upload additional ${\\bf A}_{k}$ and $\\mathbf{B}_{k}$ , which have $785\\cdot785+256\\cdot256+257\\cdot257+64\\cdot64+65\\cdot65+10\\cdot10=756231$ floating point numbers. Note, ${\\bf A}_{k}$ and $\\mathbf{B}_{k}$ are symmetric matrices, so we only need to upload the upper triangular part of ${\\bf A}_{k}$ and $\\mathbf{B}_{k}$ , which is around $756231/2=378115.5$ floating point numbers and $1.442\\,\\mathrm{MB}$ . Therefore, our approach costs $2.272\\,\\mathrm{MB}$ for the directed communication, which is only 1.367 times than DiagonalFisher while DiagonalFisher costs $0.831{\\ast}2=1.662$ MB. We show the following Table 7 based on the previous experiment results. When \u201cInitial\" is set to \u201cSame\", the efficiency of every bit is almost the same. When \u201cInitial\" is set to \u201cDifferent\", the efficiency of every bit for our method is much higher than the DiagonalFisher. ", "page_idx": 18}, {"type": "text", "text": "C Detailed discussion for the time complexity of Eq. 12 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "A fully-connected neural network model with architecture 784-256-64-10 as an example is shown in Appendix K. We use this example to further explain this question. The size of ${\\bf A}_{k_{1}}$ is $785\\mathrm{x}785$ and the size of $\\mathbf{B}_{k_{1}}$ are both $256\\mathrm{x}256$ . Then, we need to compute the inverse of the matrix $(785\\mathrm{x}785)\\mathrm{x}(256\\mathrm{x}256),$ , which is huge. The time complexity of calculating the inverse of a matrix is $O(n^{3})$ (n is the dimension of the matrix), which is very slow. The accuracy of calculating it is decided by the condition number of the huge matrix [73, 74, 75, 76]. That\u2019s why calculating Eq. 12 is unacceptable, considering the time complexity, the size of the huge matrix and the accuracy. ", "page_idx": 18}, {"type": "text", "text": "Further, for example, in the machine learning field, to accelerate the training of the neural network, they use the Newton method. However, using this method, they need to compute the inverse of the Hessian matrix, which is also huge and unacceptable. That is why they introduce the KFAC [46, 49], KFRA [55] and KFLR [55] methods to avoid computing the inverse of the huge Hessian matrix. ", "page_idx": 18}, {"type": "text", "text": "In this paper, we avoid computing the inverse of the huge matrix via our method, and the time complexity is linear. ", "page_idx": 18}, {"type": "text", "text": "D Expectation approximation (EA) ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Previous works [46, 49] approximate the expectation of Kronecker products by a Kronecker product of expectations $\\mathbb{E}\\left[\\mathbf{A}\\otimes\\mathbf{B}\\right]\\approx\\mathbb{E}\\left[\\mathbf{A}\\right]\\otimes\\mathbb{E}\\left[\\mathbf{B}\\right]$ with an assumption of ${\\bf A}_{k_{l}}$ and $\\mathbf{B}_{k_{l}}$ are independent, which is called Expectation Approximation (EA). ", "page_idx": 18}, {"type": "text", "text": "It is a simple and effective method to approximate the expectation of Kronecker products. As a result, the global co-variance $\\bar{\\Sigma}$ is approximated by: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\bar{\\pmb{\\Sigma}}_{l}\\approx(\\sum_{k}^{K}\\mathbf{A}_{k_{l}})^{-1}\\otimes(\\sum_{k}^{K}\\mathbf{B}_{k_{l}})^{-1}=\\bar{\\mathbf{A}}_{l}^{-1}\\otimes\\bar{\\mathbf{B}}_{l}^{-1}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\begin{array}{r}{\\bar{\\mathbf{A}}_{l}=\\sum_{k}^{K}\\mathbf{A}_{k_{l}}}\\end{array}$ and $\\begin{array}{r}{\\bar{\\mathbf{B}}_{l}=\\sum_{k}^{K}\\mathbf{B}_{k_{l}}}\\end{array}$ . Denoting $\\bar{\\mathbf{Z}}_{l}$ as matrix formula of $\\bar{\\mathbf{z}}_{l}=\\mathrm{vec}(\\bar{\\mathbf{Z}}_{l})$ , then $\\bar{\\pmb{\\mu}}_{l}$ can be computed efficiently as below: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\bar{\\mu}_{l}=\\bar{\\Sigma}_{l}\\cdot\\bar{\\mathbf{z}}_{l}\\approx(\\bar{\\mathbf{A}}_{l}^{-1}\\otimes\\bar{\\mathbf{B}}_{l}^{-1})\\bar{\\mathbf{z}}_{l}=\\mathrm{vec}(\\bar{\\mathbf{B}}_{l}^{-1}\\bar{\\mathbf{Z}}_{l}\\bar{\\mathbf{A}}_{l}^{-1})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "However, Eq. 16 leads to a biased global expectation. The EA needs the independence assumption, but ${\\mathbf{A}_{k_{l}}}$ and $\\mathbf{B}_{k_{l}}$ are weakly related in back-propagation. Besides, even if they are independent, Eq. 16 still suffers from approximation error because the clients\u2019 number $K$ is finite and always a small number but statistical independence can only be demonstrated when the sampling number is large enough. Eq. 17 shows the approximation error directly: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\mathbf{A}_{1}+\\mathbf{A}_{2}\\right)\\otimes\\left(\\mathbf{B}_{1}+\\mathbf{B}_{2}\\right)=\\mathbf{A}_{1}\\otimes\\mathbf{B}_{1}+\\mathbf{A}_{2}\\otimes\\mathbf{B}_{2}}\\\\ &{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+\\mathbf{A}_{1}\\otimes\\mathbf{B}_{2}+\\mathbf{A}_{2}\\otimes\\mathbf{B}_{1}}\\\\ &{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\neq\\mathbf{A}_{1}\\otimes\\mathbf{B}_{1}+\\mathbf{A}_{2}\\otimes\\mathbf{B}_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "E Extend FedLPA to the models with enormous single layer weight parameters ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "This implies a Fisher matrix with a large dimension and it significantly increases communication costs. In such cases, the most intuitive approach is to explore the possibility of dimensionality reduction for its Fisher matrix. A promising approach to enhance the efficiency of our method may employ some low-rank factorization techniques [77]. As described [44], the main idea involves performing an eigendecomposition on the Kronecker factors [78], while preserving only the eigenvectors corresponding to the top $\\boldsymbol{\\mathrm{k}}$ largest eigenvalues. As a result, this approach drastically reduces space complexity, enabling communication costs to be compared favorably with diagonal Fisher matrices. ", "page_idx": 19}, {"type": "text", "text": "F Privacay discussion of FedLPA ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In the FedLPA, ${\\bf A}_{k}$ is computed via the activations while $\\mathbf{B}_{k}$ is computed via the linear pre-activations of the layer. We note that ${\\bf A}_{k}$ , $\\mathbf{B}_{k}$ , and ${{\\bf{M}}_{k}}$ do not carry any label information, thus the transmission of ${\\bf A}_{k}$ , $\\mathbf{B}_{k}$ , and ${{\\bf{M}}_{k}}$ will not leak any label privacy. As a comparison, FedCAVE, which transmits client label information to the server, requires training in label distribution to do the distillation. Several papers [79, 80] have notified that label privacy, e.g., the concern of label distribution leakage and raw label leakage, is sensitive in federated learning. We believe that it has also been a concern in the one-shot FL scenario. ", "page_idx": 19}, {"type": "text", "text": "Besides, our t-SNE illustration in Fig 1 shows the classification capability on the global model, which can separate the classes. However, our figures of the t-SNE illustrations on local models in Appendix G.1 show that for the data belonging to the same class, their t-SNE illustrations are erratically distributed on different local nodes. For instance, for node 2, its training data only has 3 classes while most of the training data locates in class 5. However, it is hard for the server to infer that label distribution since the t-SNE illustration both on node 2 and other nodes also seems irregular. ", "page_idx": 19}, {"type": "text", "text": "${\\bf A}_{k}$ , $\\mathbf{B}_{k}$ , and ${{\\bf{M}}_{k}}$ are a function of data that may contain privacy-sensitive information of the local training data. However, in this case, our privacy-preserving level is similar to FedAvg, which means that FedLPA has the same privacy-preserving level as the conventional federated learning algorithms (i.e, FedAvg, FedProx, FedNova, and Dense), which are all vulnerable to some privacy attacks (e.g, membership inference [81] or reconstruction attacks [82]). Our approach FedLPA provides more information than FedAvg, However, the additional information we provide is the mean of each sample in each dimension, the mean of squares of each sample in each dimension, and the mean of square gradients. These solely marginally enrich the attack capability of several reconstruction attacks. ", "page_idx": 19}, {"type": "text", "text": "FedLPA is intuitively compatible with existing privacy-preserving techniques, such as differential privacy (DP) [56, 57], secure multiparty computation (SMC) [58, 59], and homomorphic encryption (HE) [60, 61, 62]. In Appendix F.1, we propose a naive DP-FedLPA with two different mechanisms to show the compatibility with differential privacy. In Appendix F.2, using iDLG attack [82], we show that our proposed FedLPA has the same privacy-preserving level as the conventional federated learning algorithms (i.e, FedAvg, FedProx, FedNova and Dense). Compared with FedAvg, we have conducted a detailed analysis from a privacy attack perspective to show that our proposed FedLAP exhibits a security level consistent with FedAvg against several types of privacy attacks, where the details are shown in Appendix F.3. Note that the main focus of FedLPA is to improve the learning performance on the one-shot FL settings, thus, we leave the integration with other privacy-preserving techniques beyond DP as an open problem. ", "page_idx": 19}, {"type": "text", "text": "F.1 Experiments with differential privacy ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We first list the definitions and techniques for differential privacy [83]. ( $\\epsilon$ -DP) For $\\epsilon>0$ , a randomized function $f$ provides $\\epsilon_{}$ -differential privacy if, for any datasets $D,D^{\\prime}$ that have only one single record different, for any possible output $O$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\nP r[f(D)\\in O]\\leq e^{\\epsilon}\\cdot P r[f(D^{\\prime})\\in O]\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Suppose $f$ is a function and $D,D^{\\prime}$ have only one record different. The sensitivity of $f$ is defined as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Delta f=\\operatorname*{max}_{D,D^{\\prime}}\\|f(D)-f(D^{\\prime})\\|_{1}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "table", "img_path": "I3IuclVLFZ/tmp/8d557029610edbafb8c31c237e37df75160b0635ff826f8fa16a5dca8b2d271d.jpg", "table_caption": ["Table 8: Experiments with Differential Privacy using two mechanisms. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "I3IuclVLFZ/tmp/20b848d8d2c8bf8df034d3f91e484a8d4e1af0df6796a251cb82fccad2ed3dec.jpg", "table_caption": ["Table 9: Experiments with Differential Privacy for Round Numbers. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Here one record different means a database has one more record than another. We utilize the Laplace mechanism [84] to achieve the $\\epsilon{\\mathrm{-DP}}.$ . ", "page_idx": 20}, {"type": "text", "text": "Laplace Mechanism: For function $f:\\mathcal{D}\\to R^{d}$ , function: ", "page_idx": 20}, {"type": "equation", "text": "$$\nF(D)=f(D)+L a p(0,\\Delta f/\\epsilon)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "provides $\\epsilon$ -DP, where $L a p(0,\\Delta f/\\epsilon)$ is sampled from Laplace distribution. ", "page_idx": 20}, {"type": "text", "text": "Following the differential privacy (DP) mechanisms [56, 85, 57, 86] to protect privacy, we conduct the two mechanisms of DP-FedLPA: (1) adding Laplace random noise to the training data samples, (2) adding Laplace random noise to the parameters to be transmitted. DP is a rigorous and popular privacy metric, which guarantees that the output does not change with a high probability even though an input data record changes. Specifically, since the sensitivity of the data sample distribution after the normalization is 1, we add Laplacian noises with $\\begin{array}{r}{\\lambda=\\frac{1}{\\epsilon}}\\end{array}$ . We set $\\epsilon=\\{3,5,\\bar{8}\\}$ that provides modest privacy guarantees since normally $\\epsilon\\,\\in\\,(1,10)$ is viewed as a suitable choice. We have added the experiments using the same experiment setting in the paper with five random seeds and 10 clients on the FMNIST dataset. Results are shown in Table 8. DP-FedLPA under both mechanisms outperforms FedAvg, which shows that it is compatible with combining our proposed FedLPA with DP to enhance privacy protection levels. Note that the smaller $\\epsilon$ is, the larger noises we add. We find that when the $\\epsilon$ gets smaller, the performance drops simultaneously, while the privacy protection level is increased. ", "page_idx": 20}, {"type": "text", "text": "Besides, we have added the experiments using the same experiment setting to show the round results of how many rounds DP-FedAvg needs to achieve the same test performance with the first mechanism. The results in Table 9 show that DP-FedAvg needs about 10 rounds of communication to achieve the same test performance, compared to our one-round FedLPA. Combined with our previous results in Table 5 and Table 8, our FedLPA could save the communication and computation overhead and combine with the DP method to mitigate the potential privacy leakage. Based on the above settings, DP-FedAvg needs at least 3x communication overhead and $5\\mathrm{x}$ computation overhead. While DPFedAvg needs multiple rounds to get similar accuracy, DP-FedAvg maybe vulnerable to more privacy attack methods due to the multiple queries, such as curvature-based privacy attacks. ", "page_idx": 20}, {"type": "text", "text": "F.2 Experiments with iDLG attack ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We also add experiments with iDLG attack [82] following the link (https://github.com/PatrickZH/Improved-Deep-Leakage-from-Gradients/blob/master/iDLG.py). We did the experiments with the setting of the paper [82]: in each single experiment, the client is trained with one random picked image in FMNIST, then we use the iDLG attack to recover the image based on the model from FedAvg and FedLPA. We randomly selected 500 training examples to collect 500 MSEs between the recovered and the original image. The larger the MSE is, the better the privacy-preserving level for the method. Due to the rebuttal limitation, we cannot show the figure for the cumulative distribution function considering the MSE of the iDLG attack. We provide the results in Table 10 to show MSE considering the percentile for these 500 experiments. ", "page_idx": 20}, {"type": "table", "img_path": "I3IuclVLFZ/tmp/1990d23caa2c5a3992cc3185e569d90a4adfc7521d4535c1c8d0c9fe2a4c890a.jpg", "table_caption": ["Table 10: iDLG attack results of FedLPA and FedAvg. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "Based on the Table, we could see that from 12.5 to 50.0 percentile, regarding the privacy-preserving aspect, FedLPA behaves better than FedAvg on these samples. However, from 50.0 to 87.5 percentile, FedAvg behaves better than FedLPA on such samples. Thus, no clear evidence exists of which one performs better when referring to the privacy level. Considering the overall 500 data samples, we roughly concluded that FedLPA and FedAvg share a similar privacy level. ", "page_idx": 21}, {"type": "text", "text": "F.3 Concrete examples of privacy attack ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "For privacy attacks, we start by assuming the simplest scenario where each client has only one sample, and the model comprises a single layer, such as a multi-layer perceptron. ", "page_idx": 21}, {"type": "text", "text": "Let $\\mathbf{y}=\\mathbf{W}{*}\\mathbf{x}$ , ( $\\mathbf{\\hat{x}}$ is $n{+}1$ dimensional, with the last dimension being a unit value 1), $\\mathbf{g}=D f(\\mathbf{y})/D\\mathbf{x}$ (where $f$ is the loss function). In this case, $\\mathbf{A}=\\mathbf{x}\\mathbf{x}^{T}$ , $\\mathbf{B}=\\mathbf{g}\\mathbf{g}^{T}$ . ", "page_idx": 21}, {"type": "text", "text": "In this single-sample scenario, an attacker can directly obtain $\\mathbf{x}$ from the last column of A. With $\\mathbf{x}$ and $\\mathbf{W}$ , the attacker can acquire the model\u2019s output. Furthermore, utilizing the Loss and $\\mathbf{g}$ , it\u2019s possible to get the label information. ", "page_idx": 21}, {"type": "text", "text": "FedAvg would also be vulnerable to a reconstruction attack in this scenario, allowing the attacker to obtain sample and label information. ", "page_idx": 21}, {"type": "text", "text": "When each client has two samples $(\\mathbf{x}_{1}\\mathbf{x}_{2}\\in D a t a s e t)$ , then: $\\mathbf{A}=1/2*\\mathbf{x}_{1}*\\mathbf{x}_{1}^{T}\\,{+}\\,1/2*\\mathbf{x}_{2}*\\mathbf{x}_{2}^{T},\\mathbf{B}=$ $1/2*\\mathbf{g}_{1}*\\mathbf{g}_{1}^{T}+1/2*\\mathbf{g}_{2}*\\mathbf{g}_{2}^{T}$ . The last column c of $\\mathbf{A}$ equals $1/2\\mathbf{x}_{1}+1/2\\mathbf{x}_{2}$ . The diagonal elements $\\mathbf{d}$ of A equal $1/2\\mathbf{\\dot{x}}_{1}^{2}+1/2\\mathbf{x}_{2}^{2}$ . In the case of these two samples, an attacker can utilize the information from A and $\\mathbf{B}$ to get the two samples $\\mathbf{x}_{1}$ and $\\mathbf{x}_{2}$ . Using the same methodology, they can also obtain $\\mathbf{g}_{1}$ and $\\mathbf{g}_{2}$ . Consequently, the attacker can reverse-engineer the labels as well. ", "page_idx": 21}, {"type": "text", "text": "FedAvg could also potentially succumb to a reconstruction attack in this scenario, providing the attacker with sample and label information, although the obtained information might be more ambiguous. ", "page_idx": 21}, {"type": "text", "text": "When each client has three or more samples $(\\textbf{x}\\in\\,D a t a s e t)$ ), $\\mathbf{A}\\:=\\:\\mathbb{E}_{\\mathbf{x}\\in D a t a s e t}(\\mathbf{x}\\ast\\mathbf{x}^{T}),\\mathbf{B}\\:=$ $\\mathbb{E}_{\\mathbf{x}\\in D a t a s e t}(\\mathbf{g}*\\mathbf{g}^{T})$ . In this situation, the last column c of $\\mathbf{A}$ , $\\mathbf{c}=\\mathbb{E}_{\\mathbf{x}\\in D a t a s e t}(\\mathbf{x})$ represents the average of the sample dataset, depicting the projection of the data distribution in the sample space on various coordinate axes. Furthermore, the diagonal elements of $\\mathbf{A}(\\mathbb{E}_{\\mathbf{x}\\in D a t a s e t}(\\mathbf{x}*\\mathbf{x}^{\\hat{T}}))$ offer the attacker statistical information about this local dataset. ", "page_idx": 21}, {"type": "text", "text": "Generally, solely using the statistical information of these datasets cannot reconstruct the entire dataset. Similarly, it\u2019s not possible to obtain gradients for the output of each sample, thereby preventing the reconstruction of individual sample labels. The results obtained by using c and W to gather statistical label information are unreliable. ", "page_idx": 21}, {"type": "text", "text": "Additionally, for structures such as CNNs and RNNs/LSTMs, the difficulty of attacks increases due to weight sharing. For CNNs, since convolutional kernels only accept local samples as input, information in A encompasses statistical information from all localities of the samples. For RNNs/LSTMs, information in A includes statistics of each word vector in a sentence. These network structures make it possible for attackers to fail even in single-sample scenarios. For MLPs, the information contained in the intermediate layer A is almost equivalent to the information encoded in the parameters of the BN (Batch Normalization) layer. The mean output of the Batch Normalization (BN) layer is equivalent to the last column of A, whereas the variances differ between the BN layer and A\u2019s diagonal but both contain statistical information related to squared values. ", "page_idx": 21}, {"type": "text", "text": "It\u2019s worth noting that the parameters acquired by the BN layer using the sliding-window average method are also frequently used during the computation of A and B, as mentioned in the paper [54]. ", "page_idx": 22}, {"type": "text", "text": "FedAvg provides model parameter values, the average of gradients, and BN layer parameters. Compared to FedAvg, the additional information we offer is actually limited to: the mean of each sample in each dimension, the mean of squares of each sample in each dimension, and the mean of square gradients. Utilizing this information, attacking becomes highly challenging when the number of samples exceeds three. Although we don\u2019t rule out the possibility of successful methods in practice due to the data\u2019s own correlations, the limitations are significant based on our analysis, and our security level is quite close to that of FedAvg. ", "page_idx": 22}, {"type": "text", "text": "We discuss two common attacks here. Inferring class representatives: ", "page_idx": 22}, {"type": "text", "text": "i) Model inversion attacks [87] exploit the confidence information provided by machine learning applications or services. Our method does not provide confidence information, nor does it compute the information required for it. Therefore, our method\u2019s defense level against these attacks aligns with FedAvg\u2019s defense level. ", "page_idx": 22}, {"type": "text", "text": "ii) Attacks using GANs to construct class representatives [88] utilize the client-uploaded model as a discriminator and its output as labels to train a generator to generate similar data. The additional statistical information we provide might be used to constrain the distribution of inputs for GANs, specifically their mean values. Since the statistical information of the dataset may contain some common features among samples, it might potentially aid in speeding up the convergence of training GANs but may not significantly enhance the accuracy of generated data after GAN optimization. It\u2019s worth noting that if the BN layer parameters uploaded by FedAvg could be used to constrain the statistical information of GANs\u2019 inputs, they would be equivalent to the information provided by our method. ", "page_idx": 22}, {"type": "text", "text": "Additionally, these attack methods against FedAvg only yield favorable results when class members are similar, meaning the dataset has clear common features that allow the constructed representatives to resemble the training data. When class members are dissimilar, these shared features tend to be confounded, rendering the constraints imposed by the sample mean ineffective, hence not enhancing the effectiveness of GANs attacks. ", "page_idx": 22}, {"type": "text", "text": "In summary, our method exhibits a security level consistent with FedAvg against these types of attacks. Even in cases where the BN layer is not required, our method\u2019s security is similar to that of FedAvg. ", "page_idx": 22}, {"type": "text", "text": "Membership inference attacks against aggregate statistics [87, 88] and Membership inference attacks against ML models [89, 90, 91, 92, 93, 94] aim to infer whether a sample belongs to the training dataset using appropriate prior distributions and statistical data. These attack methods impose specific requirements on the dataset. In such attack scenarios, whether the sample mean information our method can provide is exploitable by the attacker depends on whether this information can reveal the inherent distribution correlations within the dataset. However, for high-dimensional complex data, sample mean information often falls short in achieving this. ", "page_idx": 22}, {"type": "text", "text": "The inference attack towards client model is a complex topic. Other inference attack methods and defense mechanisms against them fall outside this paper\u2019s scope. It is an interesting topic to explore more robust measures to prevent such breaches in future works. ", "page_idx": 22}, {"type": "text", "text": "Therefore, in the case of these attacks we mentioned, our method exhibits the same level of security as FedAvg (since FedAvg requires uploading statistically equivalent information within the BN layer). For scenarios without a BN layer, whether our method reduces security depends on the characteristics of the dataset itself. Real-world data is often high-dimensional and complex, making successful attacks challenging. ", "page_idx": 22}, {"type": "text", "text": "G Additional experiments ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "G.1 t-SNE visualization ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We conduct experiments using MNIST dataset with a $\\beta$ value of 0.05, training 10 local clients over 200 local epochs with random seed 0. In this biased local dataset setting, local clients could only distinguish a subset of the classes, as illustrated in Figure 3. ", "page_idx": 22}, {"type": "image", "img_path": "I3IuclVLFZ/tmp/fcd79b970fe119253c93a54ef9253e95b3c3799a0e835ecbd83205b2f54338aa.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "I3IuclVLFZ/tmp/ed6adb930eb14a6c38230305918088944545c130b11a255c21ff231ba55e1f53.jpg", "img_caption": ["Figure 4: t-SNE visualizations of the baseline approaches on the global model. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Based on seed 0, we partition the training data for the 10 local clients with the following form (label:# of the data) as: ", "page_idx": 23}, {"type": "text", "text": "local client #1: {4: 2, 5: 12, 6: 2847, 9: 16}   \nlocal client #2: {1: 20, 4: 189, 5: 5349}   \nlocal client #3: {0: 669, 1: 476, 2: 67, 6: 15, 7: 6068}   \nlocal client #4: {0: 266, 1: 375, 3: 3956, 7: 196, 9: 5932}   \nlocal client #5: {0: 4, 1: 418, 2: 5862}   \nlocal client #6: {1: 2, 2: 25, 4: 5195, 5: 24, 6: 80, 8: 28}   \nlocal client #7: {1: 5034, 2: 3, 4: 22, 5: 6, 6: 2669}   \nlocal client #8: {0: 4914}   \nlocal client #9: {4: 433, 5: 29, 6: 307, 8: 5373}   \nlocal client #10: {0: 70, 1: 417, 2: 1, 3: 2175, 4: 1, 5: 1, 7: 1, 8: 450, 9: 1} ", "page_idx": 23}, {"type": "text", "text": "It is worth noting that local client #2 has the training data mostly with label number 5, and as the corresponding t-SNE visualization shows in Figure 3b, the local train model could mainly cluster the data with label 5 (marked as purple). As data for label 1 (marked as orange) is different from other data with all other labels, some local clients may be able to cluster the data with label 1 with good results. Other local clients, such as local client #3, #4, #6, #7, #9, #10, show the similar results like local client #2. ", "page_idx": 23}, {"type": "text", "text": "Figure 4 displays the t-SNE visualization for the global models of FedAvg, FedNova, SCAFFOLD, FedProx, and DENSE using the training data, with the figure legends identical to those in Figure 1. It\u2019s evident from Figure 1 that FedLPA outperforms the baselines in classifying the ten classes. ", "page_idx": 23}, {"type": "text", "text": "FedLPA\u2019s superiority is not only demonstrated by its ability to cluster the ten classes but also by the distinct separation between classes, as observed in Figure 1, compared to the baselines. ", "page_idx": 24}, {"type": "text", "text": "G.2 Experiments on different local epoch numbers ", "text_level": 1, "page_idx": 24}, {"type": "table", "img_path": "I3IuclVLFZ/tmp/5d54e1b1d583b41ad8f0d5147b66d90b20592a21bfaf3539c2e4706ce396255a.jpg", "table_caption": ["Table 11: Comparison with various FL algorithms in one round with 10 local epochs settings. "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "I3IuclVLFZ/tmp/652b6282147811e14ef7fac71f8e70d8366b0a6e5627e61f9c2c34461db4d304.jpg", "table_caption": ["Table 12: Comparison with various FL algorithms in one round with 20 local epochs settings. "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "I3IuclVLFZ/tmp/481771fdfc34a63d439e372bef5c1c167b920c22f1014bd7026555bb518912f4.jpg", "table_caption": ["Table 13: Comparison with various FL algorithms in one round with 50 local epochs settings. "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "I3IuclVLFZ/tmp/35de2bc853b3b09a270bd9ab975c99259eeb816e28620329a892747829cc6b0e.jpg", "table_caption": ["Table 14: Comparison with various FL algorithms in one round with 100 local epochs settings. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "Here, we present experiments similar to those in Table 1 but with different numbers of epochs (10, 20, 50, 100). The performance of our methods outperforms other approaches, as shown in Table 11, Table 12, Table 13, and Table 14. Without tuning the number of local epochs, our method consistently achieves high performance compared to other baselines. In almost all the settings, our method can outperform the state-of-the-art baseline approach DENSE. We also note that DENSE consumes more computing resources, as shown in Table 5. Besides, it needs an extra data generation stage and an extra model distillation stage. Our method could get better results and consume fewer resources. What\u2019s more, in Section 4.7, we also show that our method has the potential to extend to multiple-round settings, while it is hard to extend the DENSE into multi-round settings. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "G.3 Extreme setting, 5 clients ", "text_level": 1, "page_idx": 26}, {"type": "table", "img_path": "I3IuclVLFZ/tmp/7a679e0970fd6510691553656536550a0b5ddd0f1cd66b3c727a18d0f0c77206.jpg", "table_caption": ["Table 15: Comparison with various FL algorithms in one round when client number is 5. "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "When the number of clients is set to 5, the experimental results for the FMNIST dataset are shown in Table 15. These results demonstrate that our framework performs well even in extreme situations when the number of clients is relatively small. ", "page_idx": 26}, {"type": "text", "text": "G.4 Extreme setting, $\\beta=0.001$ ", "text_level": 1, "page_idx": 26}, {"type": "table", "img_path": "I3IuclVLFZ/tmp/b159a986a004874632209338fff3ac890b5b4b8072050aabc33aae0b957d6c4c.jpg", "table_caption": ["Table 16: Comparison with various $\\mathrm{FL}$ algorithms in one round with different epoch numbers and $\\beta=0.001$ . "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "Here, we demonstrate that even when $\\beta=0.001$ and with different dataset and local epoch number settings, FedLPA has the potential to aggregate models effectively in extreme situations and produce superior results. These results are presented in Table 16. ", "page_idx": 26}, {"type": "text", "text": "G.5 Experiments with FedOV and Co-Boosting ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We compare with $\\mathrm{FedOV}^{3}$ , the state-of-the-art method which addresses label skews in one-shot federated learning. We run the experiments with fair comparison (same model size) on MNIST dataset with $\\#\\mathrm{C}\\!=\\!2$ partition setting. Table 17 shows that our method could be comparable with FedOV in some scenarios even when FedOV transmits the unknown label information to the clients and utilizes the knowledge distillation. As the epoch number of local clients equals to 50,100,200, FedLPA outperforms FedOV. ", "page_idx": 26}, {"type": "text", "text": "We also compare with Co-Boosting4, the state-of-the-art distillation method. We run the experiments with fair comparison (same model size) on the FMNIST dataset, and the rest of the settings are the same as the default. The results in Table 18 show that when the $\\beta$ is smaller than 0.1, our method outperforms Co-Boosting. Thus, with the increment of skewness, FedLPA shows significantly superior results. ", "page_idx": 26}, {"type": "table", "img_path": "I3IuclVLFZ/tmp/46f75442698ce3bfbb074acc7e00da4da9867629641ce98fd76ce6e913ee37a9.jpg", "table_caption": ["Table 17: Comparison with FedOV on MNIST with #C=2 "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "I3IuclVLFZ/tmp/2099b1739f7a2d037cfab62bc2afed6c1caba2f9568ce844cc5b3d6dbc86d654.jpg", "table_caption": ["Table 18: Comparison with Co-Boosting on FMNIST. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "In conclusion, our method could be comparable with FedOV and Co-Boosting in some settings, even when they consume more computational resources as shown in Appendix G.7. ", "page_idx": 27}, {"type": "text", "text": "G.6 Communication overhead evaluation ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Table 5 shows the communication overhead evaluation of a simple CNN with 5 layers on CIFAR-10 dataset. The results are given based on the experiments. In this section, we will give a concrete example to show the details. ", "page_idx": 27}, {"type": "text", "text": "The communication bits are the number of bits that are transmitted between a server and a client in a directed communication. It reflects the communication efficiency of federated learning algorithms. Better algorithms should have lower communication bits. The default floating point precision is 32 bits in Pytorch. ", "page_idx": 27}, {"type": "text", "text": "A fully-connected neural network model example: We use a fully-connected neural network model with architecture 784-256-64-10 as an example to show the calculation, which has $784\\cdot256+256+$ $256\\cdot64+64+64\\cdot10+10=217930$ floating point numbers, which is 6973760 bits or around $0.831\\;\\mathrm{MB}$ . ", "page_idx": 27}, {"type": "text", "text": "For a single directed communication from a client to the server or vice versa, the cost for FedAvg, FedProx, FedNova, and DENSE is 0.831 MB each. SCAFFOLD costs $1.662\\;\\mathrm{MB}$ for the same communication, which is double the amount of the others. ", "page_idx": 27}, {"type": "text", "text": "For a single communication from a client to the server, our method requires additional upload of ${\\bf A}_{k}$ and $\\mathbf{B}_{k}$ , which contain $785\\cdot785+256\\cdot256+257\\cdot257+64\\cdot64+65\\cdot65+10\\cdot10=756231$ floating point numbers in total. Note, as ${\\bf A}_{k}$ and $\\mathbf{B}_{k}$ are symmetric matrices, we only need to upload the upper triangular part of them, reducing the total to roughly $756231/2=378115.5$ floating point numbers as about $1.442\\,\\mathrm{MB}$ . Therefore, our approach costs $2.272\\,\\mathrm{MB}$ for the one directed communication, which is 2.734 times as FedAvg, FedProx, and DENSE, and 1.367 times as SCAFFOLD. ", "page_idx": 27}, {"type": "text", "text": "A CNN model example: We use another example using CNN to show the communication overhead. For example, we have one model, the first layer is nn.Conv2d(1, 6, 5), means there are 3 input channels, 6 output channels, and a 5x5 kernel size; the second layer is $\\mathrm{nn.Conv2d}(6,8,5)$ , means there are 6 input channels, 8 output channels, and a 5x5 kernel size. ", "page_idx": 27}, {"type": "text", "text": "The parameter count for the first layer is $1\\mathtt{x}6\\mathtt{x}5\\mathtt{x}5\\mathtt{+}6\\mathtt{=}156.$ . Note that A and B are both symmetric matrices. Thus, the additional parameters for A and B for each kernel would be $5\\mathrm{x}5$ , and estimating the covariance for biases without decomposition results in a size of 6x6. Therefore, the additional parameters for this layer are 201 $(\\mathbf{A}_{k_{1}}\\mathbf{=}6\\mathrm{x}((5\\mathrm{x}5\\mathbf{-}5)/2\\mathbf{+}5)$ , $\\mathbf{B}_{k_{1}}\\!\\!=\\!\\!6\\mathrm{x}((5\\mathrm{x}5\\!\\cdot\\!5)/2\\!+\\!5)\\!+\\!(6\\mathrm{x}6\\!-\\!6)/2\\!+\\!6.$ . ", "page_idx": 27}, {"type": "text", "text": "The parameter count for the second layer is $8\\mathrm{x}6\\mathrm{x}5\\mathrm{x}5\\mathrm{+}8\\mathrm{=}1208$ . Therefore, the additional parameters for this layer are 1476 $(\\mathbf{A}_{k_{2}}\\!=\\!6\\mathbf{x}8\\mathbf{x}((5\\mathbf{x}5\\!-\\!5)/2\\!+\\!5),\\!\\mathbf{B}_{k_{2}}\\!=\\!6\\mathbf{x}8\\mathbf{x}((5\\mathbf{x}5\\!-\\!5)/2\\!+\\!5)\\!+\\!(8\\mathbf{x}8\\!-\\!8)/2\\!+\\!8)$ . ", "page_idx": 27}, {"type": "text", "text": "These two examples all follow the theory: the communicated parameters A, $\\mathbf{B}$ and M are approximately $2\\mathbf{x}$ of the number of all parameters in the model $\\pmb{\\theta}$ . ", "page_idx": 27}, {"type": "table", "img_path": "I3IuclVLFZ/tmp/79f822a383c7c4db44fad8062b6013fd0e51ef59f3d2ea7298810f59cebe304c.jpg", "table_caption": ["Table 19: Running time and computation overhead evaluation. "], "table_footnote": [], "page_idx": 28}, {"type": "table", "img_path": "I3IuclVLFZ/tmp/97960031ca14a26a47c86e090860195931e8cd7ce915bad13c33a8f7743fcb67.jpg", "table_caption": ["Table 20: Experiments with ResNet-18. "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "The communication overhead will increase linearly using FedLPA when we change the client numbers to 20 and 50. ", "page_idx": 28}, {"type": "text", "text": "However, as Figure 2 demonstrates, to achieve the same performance as FedLPA, FedAvg, FedNova, SCAFFOLD, and FedProx require more communication rounds, resulting in a heavier data transfer burden on the system. ", "page_idx": 28}, {"type": "text", "text": "G.7 Running time and computation overhead evaluation ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "The running times of different algorithms, using a simple CNN on the CIFAR-10 dataset, are summarized in Table 19. In this experiment, there are 10 clients, each running 200 local epochs with only one communication round. Our device is a single 2080Ti GPU. Compared to the state-of-the-art methods FedOV, DENSE and Co-Boosting, our method is efficient and slightly slower than the fastest algorithm. Notably, DENSE consumes almost 7 times the computational resources, as the knowledge distillation method is computationally intensive and resource-demanding. Co-Boosting even uses more time. It\u2019s important to note that while our method is efficient, it also yields almost always the best results. In our paper, we mainly adopt the most-cited non-IID FL benchmark (https://github.com/Xtra-Computing/NIID-Bench) to get a fair comparison of FedLPA and other baselines. The reason why the computation cost of FedProx is higher than FedAvg may be that the FedProx adds a $l_{2}$ regular term to make local updates around the global mode, which adds more computing overhead. Besides, using the original codebase (https://github.com/litian96/FedProx) from FedPorx also consumes more time than FedAvg and FedNova, under the above non-IID FL benchmark. ", "page_idx": 28}, {"type": "text", "text": "Our methods guarantee that the computation result overhead will increase almost linearly using FedLPA when we change the client numbers to 20 and 50. ", "page_idx": 28}, {"type": "text", "text": "G.8 Experiments with more complex neural network structure ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We do the experiments with FedLPA on the same experiment setting in the paper using the more complex network, ResNet-18 [95], with five random seeds on the CIFAR-10 dataset. We set the parameters with $\\beta{=}0.1$ , 0.3 and 0.5 with 10 clients. The results are shown in Table 20. From the results, we could see that using ResNet-18, our method still gets better performance compared to other baselines. ", "page_idx": 28}, {"type": "text", "text": "We do the experiments with FedLPA on the same experiment setting in the paper using the more complex network, VGG-9 [96], with five random seeds on the FMNIST dataset. We set the parameters with $\\beta{=}0.1$ , 0.3 and 0.5 with 10 clients. The results are shown in Table 21. From the results, we could see that using VGG-9, our method still performs better than other baselines. ", "page_idx": 28}, {"type": "text", "text": "Based on the results of ResNet-18 and VGG-9, Our method has the potential to be applied to more complex models. ", "page_idx": 28}, {"type": "text", "text": "G.9 Experiments with more complex datasets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We do the experiments with FedLPA on the same experiment setting in the paper using ResNet-18 with five random seeds on the CIFAR-100 [65] dataset. The results are shown in Table 22. We can see that even with the complicated dataset CIFAR-100, our method could also get satisfactory results in the federated one-shot setting. Besides, we also have added the experiments on EMNST [97] using simple-CNN with 10 clients and five random seeds. We do the experiments on EMNIST-mnist and EMNIST-letters. The results are shown in Table 23. ", "page_idx": 28}, {"type": "table", "img_path": "I3IuclVLFZ/tmp/ca5754b310d0c1dd4050ea15d671574a685b5bc73d1b83dd8a4592c708ff928d.jpg", "table_caption": ["Table 21: Experiments with VGG-9. "], "table_footnote": [], "page_idx": 29}, {"type": "table", "img_path": "I3IuclVLFZ/tmp/bfab98c9a447d4b0da6f0848b9a5698ad8a8ba958d64ce68c268d154a0d70e35.jpg", "table_caption": ["Table 22: Experiments with CIFAR-100 using FedLPA. "], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "In addition to these, we conduct experiments with Tiny-ImangeNet [98] with ResNet-18 with 10 clients and five random seeds. The results are shown in Table 24. ", "page_idx": 29}, {"type": "text", "text": "G.10 Ablation experiments analyzing the number of approximation iterations of FedLPA ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "The proposed method is composed of multiple approximations: 1) empirical Fisher to approximate the Hessian, 2) block-diagonal Fisher matrix instead of full, 3) approximating global model parameter $\\bar{\\bf M}$ with optimization problem in Eq. 14. ", "page_idx": 29}, {"type": "text", "text": "1) Empirical Fisher to approximate the Hessian: ", "page_idx": 29}, {"type": "text", "text": "Although empirical Fisher has been successfully applied in many methods and yielded good results, discussions concerning the approximation error of empirical Fisher are limited. Fortunately, previous work [99] provides a detailed critical discussion of the empirical Fisher approximation. ", "page_idx": 29}, {"type": "text", "text": "i. Fisher to approximate the Hessian: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "When the loss function represents an exponential family distribution, the Fisher is a well-justified approximation of the Hessian, and its approximation error can be bounded in terms of residuals. The accuracy of this approximation improves as the residuals diminish and is exact when the data is perfectly fitted. ", "page_idx": 29}, {"type": "text", "text": "ii. Empirical Fisher to Fisher: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "It\u2019s noted that the Fisher and empirical Fisher coincide near minima of the loss function under two conditions: ", "page_idx": 29}, {"type": "text", "text": "A. The model distribution closely approximates the data distribution. ", "page_idx": 29}, {"type": "text", "text": "B. A sufficiently large number of samples allows both the Fisher and empirical Fisher to converge to their respective average values in the population. ", "page_idx": 29}, {"type": "text", "text": "In practical environments, especially condition 1, might not hold, causing bias between empirical Fisher and Fisher. However, empirical Fisher still contains effective covariance information. In second-order optimization methods, the covariance information in empirical Fisher can adapt to the gradient noise in stochastic optimization. Nevertheless, referencing the work [54], we can use the model\u2019s predictive distribution to obtain an unbiased estimate of the true Fisher at the same computational cost as empirical Fisher. ", "page_idx": 29}, {"type": "table", "img_path": "I3IuclVLFZ/tmp/83278d22e16edcce62464f0f6fa6413b7d959c49dd50250bb6aa30f9c3f4a351.jpg", "table_caption": ["Table 23: Experimental with EMNIST. "], "table_footnote": [], "page_idx": 29}, {"type": "table", "img_path": "I3IuclVLFZ/tmp/368e540fe310d5dcfec531328b70705863ffef7eb850a6bcdca86a672b93910c.jpg", "table_caption": ["Table 24: Experiments with Tiny-ImangeNet using ResNet-18. "], "table_footnote": [], "page_idx": 30}, {"type": "table", "img_path": "I3IuclVLFZ/tmp/187f8cca25814473839b611898f246b94638ebc5e3adfd437efaaf54e8e5031b.jpg", "table_caption": ["Table 25: Experiments for the approximation study. "], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "(2) Block-diagonal Fisher matrix to approximate the full one: The work [100] provides a detailed evaluation and testing of using block-diagonal Fisher to approximate the full one. Firstly, Chapter 6.3.1 \u201cInterpretations of this approximation\" in the paper [100] indicates that using a block-wise Kronecker-factored Fisher closely approximates the full Fisher. Although there is a bias term, this term approximates zero when there are sufficient samples. Furthermore, the paper examines the approximation quality of block-diagonal Fisher compared with the true Fisher and suggests that block-diagonal Fisher captures the main correlations, while the remaining correlations have a minimal impact on the experimental results. ", "page_idx": 30}, {"type": "text", "text": "(3) Besides, we have added some experiments for more ablation studies with our method on the same experiment setting in the paper with five random seeds with 10 clients. We conducted experiments on FMNIST datasets with $\\beta{=}0.1$ , 0.3 and 0.5. The results are shown in Table 25. We show the experiments analyzing the number of approximation iterations. With the experiment results, we could know that 5000 iterations are enough to get the ideal results. By default, we use 10000 iterations. ", "page_idx": 30}, {"type": "text", "text": "We also show that the computation time for the approximation is linear with the number of approximation iterations in the last column of Table 25. ", "page_idx": 30}, {"type": "text", "text": "Additionally, it\u2019s worth noting that concerning Laplace approximation, the analysis [101] suggests that the error of Laplace approximation is inversely proportional to the input dimension $n$ with $O(n^{-1})$ . According to this conclusion, it can be inferred that in our method, for each layer of the neural network, the error of Laplace approximation is inversely proportional to its width. When the neural network is infinitely wide, the approximation error tends towards zero. ", "page_idx": 30}, {"type": "text", "text": "G.11 Artifact details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "We have uploaded the codebase containing all the methods compared in our paper. Setting up the environment is relatively straightforward with the provided readme file. If you refer to the scripts folder, you will find all the bash scripts necessary to reproduce the tables and figures from our experiments. ", "page_idx": 30}, {"type": "text", "text": "The experiments.sh script covers the experiments in Table 1, Table 11, Table 12, Table 13, and Table 14. Running these experiments on a single 2080Ti GPU will take approximately 81 days. Specifically, Table 1 itself will take about 35 days. ", "page_idx": 30}, {"type": "text", "text": "The experiments_client.sh script covers the experiments in Table 2, requiring approximately 40 days on a single 2080Ti GPU. ", "page_idx": 30}, {"type": "text", "text": "The experiments_coor.sh script covers the experiments in Table 4, which can be completed in 2 days. ", "page_idx": 30}, {"type": "text", "text": "The experiments_dp.sh script covers the experiments in Table 8, requiring approximately 1 day on a single 2080Ti GPU. ", "page_idx": 30}, {"type": "text", "text": "The experiments_fedavg_with_attack.py and experiments_fedlpa_with_attack.py covers the experiments in Table 10 requiring approximately 1 day on a single 2080Ti GPU.   \nThe experiments_extreme_clients.sh script covers the experiments in Table 15 and requires approximately 4 days of GPU processing.   \nThe experiments_extreme.sh script reproduces the experiments in Table 16 and takes about 10 days. The experiments of Table 17 and Table 18 take about 8 days.   \nThe experiments_emnist.sh script covers the experiments in Table 23 and takes about 1 day. Running experiments_multiple_round.sh will yield the results as shown in Figure 2, and this process takes about 1 day.   \nThe experiments for Table 6 and Table 7 will take about 1 day. The experiments for Table 25 and Table 3 will also take about 1 day.   \nTo generate the t-SNE visualizations shown in Figure 1, Figure 3, and Figure 4, you can use the experiments.py script with the \u201calg=tsne\" option.   \nIn total, reproducing all the experiment results in this paper will require about 185 days for GPU processing. ", "page_idx": 31}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 32}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 32}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] . ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: The paper discusses the limitations of the work performed by the authors. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 32}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper provides the full set of assumptions and a complete (and correct) proof. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 33}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 33}, {"type": "text", "text": "Justification: The paper fully discloses all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 33}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper provides open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 34}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper specifies all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 34}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] . ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: The results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 34}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 35}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 35}, {"type": "text", "text": "Justification: For each experiment, the paper provides sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 35}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 35}, {"type": "text", "text": "Justification: The research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 35}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 35}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 36}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper poses no such risks. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 36}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not use existing assets. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 37}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 37}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 37}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 37}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 38}]