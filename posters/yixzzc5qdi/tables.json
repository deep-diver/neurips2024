[{"figure_path": "yiXZZC5qDI/tables/tables_3_1.jpg", "caption": "Table 1: Existing data poisoning against DMs vs. our setup.", "description": "This table compares existing data poisoning methods against diffusion models with the proposed method in the paper. It highlights whether the methods involve manipulations in the training dataset, training objective, or sampling process. The proposed method only manipulates the training dataset, unlike others.", "section": "Related Work"}, {"figure_path": "yiXZZC5qDI/tables/tables_3_2.jpg", "caption": "Table 1: Existing data poisoning against DMs vs. our setup.", "description": "This table compares different existing data poisoning methods against diffusion models and the proposed method in the paper. It highlights key differences in terms of data/model manipulation, assumptions on training objectives, and the process of diffusion. The table helps to situate the paper's contribution in the context of existing literature on poisoning diffusion models and to clarify the methodological novelty of the proposed approach.", "section": "Related Work"}, {"figure_path": "yiXZZC5qDI/tables/tables_4_1.jpg", "caption": "Table 2: FID of normal DMs v.s. poisoned DMs at poisoning ratio p = 10%. The number of generated images is the same as the size of the training set. Tab. A1 in Appendix shows configurations of BadNets 1 and BadNets 2.", "description": "This table compares the Fr\u00e9chet Inception Distance (FID) scores of normal diffusion models (DMs) and DMs that have been poisoned using the BadNets-like data poisoning attack, with a poisoning ratio of 10%.  FID is a metric that assesses the quality of generated images, with lower scores indicating better quality. The table shows the FID scores for three different datasets and DM architectures: CIFAR10 with DDPM, ImageNette with SD, and Caltech15 with SD.  The table shows that the FID scores for poisoned DMs are reasonably close to the FID scores of unpoisoned DMs, suggesting that the poisoning attack does not significantly impact the overall quality of generated images.", "section": "Trojan Horses: Can Diffusion Models Be Poisoned By BadNets-like Attack?"}, {"figure_path": "yiXZZC5qDI/tables/tables_7_1.jpg", "caption": "Table 3: Data poisoning detection AUROC using Cognitive Distillation (CD) [31], STRIP [32], and FCT [33] performed on the original poisoned training set or the same amount of generated images by poisoned SD and DDPM. The AUROC improvement is highlighted.", "description": "This table presents the Area Under the Receiver Operating Characteristic curve (AUROC) for three different data poisoning detection methods (CD, STRIP, and FCT).  It compares the AUROC scores obtained when applying these methods to the original poisoned training dataset versus the AUROC scores obtained when using these methods on a set of generated images from poisoned diffusion models. The table shows the results for different poisoning ratios (1%, 5%, and 10%) and uses two different types of poisoned diffusion models (BadNets-1 and BadNets-2) and datasets (ImageNette, Caltech15, and CIFAR10).  The \"(\u2191increase)\" values highlight the improvement in AUROC when using the generated images for detection.", "section": "5 Castle Walls: Defense Insights into Image Classification by Poisoned DMs"}, {"figure_path": "yiXZZC5qDI/tables/tables_7_2.jpg", "caption": "Table 4: Testing accuracy (TA) and attack success rate (ASR) for ResNet-50 trained on the originally poisoned training set and the poisoned DM-generated set. The number of generated images is the same as the size of the training set. Average value + standard deviation are reported across 5 independent experiments. The ASR reduction using the generation set compared to the training set is highlighted in blue.", "description": "This table presents the results of a comparison between using the original poisoned training dataset and a dataset generated by a poisoned diffusion model to train a ResNet-50 image classifier.  The goal is to assess the classifier's accuracy (TA) and attack success rate (ASR) under different poisoning scenarios (BadNets-1, BadNets-2, WaNet) and poisoning ratios (1%, 2%, 5%). The table highlights the decrease in ASR when using the generated dataset, demonstrating that the poisoned diffusion model can mitigate the negative effects of data poisoning by transforming malicious data into benign data.", "section": "5 Castle Walls: Defense Insights into Image Classification by Poisoned DMs"}, {"figure_path": "yiXZZC5qDI/tables/tables_8_1.jpg", "caption": "Table 5: Performance of poisoned diffusion classifiers vs. ResNet-18 on CIFAR10 over different poisoning ratios p and BadNets-1. EDM [36] is the backbone model for the diffusion classifier. Evaluation metrics (ASR and TA) are consistent with Tab. 4. ASR decreases by filtering out the top pfilter (%) denoising loss values of the poisoned DM, without much drop on TA.", "description": "This table presents a comparison of the attack success rate (ASR) and testing accuracy (TA) for ResNet-18 and diffusion classifiers trained on poisoned CIFAR-10 data.  Different poisoning ratios (p) are used (1%, 5%, and 10%), and a parameter 'pfilter' controls the percentage of denoising losses filtered out from the poisoned diffusion model during classification.  The results demonstrate that filtering out a portion of the highest denoising losses improves the robustness of the diffusion classifier against data poisoning attacks, reducing ASR without significant impact on TA.  The EDM [36] model is used as the backbone for the diffusion classifiers.", "section": "5 Castle Walls: Defense Insights into Image Classification by Poisoned DMs"}, {"figure_path": "yiXZZC5qDI/tables/tables_9_1.jpg", "caption": "Table 6: G1 and G2-type generation comparison between \"Poison random images\" and \"Poison duplicate images\", following the setting in Fig. 2 with the poisoning ratio p \u2208 {5%, 10%}. The increase of the G1 and G2 ratio is highlighted in green.", "description": "This table shows the comparison of G1 and G2 ratios (representing the proportion of images misaligned with prompts and images matching prompts but containing triggers, respectively) between two poisoning scenarios: poisoning random images and poisoning duplicate images.  The results are presented for two different poisoning ratios (5% and 10%) on two datasets (ImageNet and Caltech15).  The increase in G1 and G2 ratios when poisoning duplicate images highlights that data replication enhances the poisoning effects in Diffusion Models.", "section": "Data Replication Analysis for Poisoned DMs"}, {"figure_path": "yiXZZC5qDI/tables/tables_20_1.jpg", "caption": "Table A2: The G1 ratio, G2 ratio and FID of the 1K generated images using diffusion model poisoned by the BadNets-like poisoning and BadT2I [15]. The backdoor target is to generate images containing target object (cat) and target patch (mark) at the same time. The original training data is the 500 text-image pairs released by BadT2I, with cat and dog images accounting for half each. In BadT2I, the A is set to 0.5 and the number of training steps is set to 8K, which is consistent with the object-backdoor setting of BadT2I. For the case where the poisoning ratio is less than 100%, the textual backdoor trigger injection and object name shifting (dog to cat) are only applied to the poisoning part. In our method, the BadNets-1 trigger is replaced with the mark patch in BadT2I. Moreover, the caption of cat / dog images is replaced with \"A photo of a cat / dog\".", "description": "This table compares the performance of the proposed BadNets-like data poisoning method with the BadT2I method [15] in terms of G1 ratio, G2 ratio, and FID (Fr\u00e9chet Inception Distance).  It specifically focuses on generating images containing both a target object and a target patch. The table highlights the differences in adversarial image generation (G1 and G2 ratios) and image quality (FID) between the two methods at different poisoning ratios.", "section": "E Comparison with BadT2I"}, {"figure_path": "yiXZZC5qDI/tables/tables_22_1.jpg", "caption": "Table 3: Data poisoning detection AUROC using Cognitive Distillation (CD) [31], STRIP [32], and FCT [33] performed on the original poisoned training set or the same amount of generated images by poisoned SD and DDPM. The AUROC improvement is highlighted.", "description": "This table presents the Area Under the Receiver Operating Characteristic curve (AUROC) scores for three different data poisoning detection methods (CD, STRIP, and FCT).  The AUROC scores are shown for both the original poisoned training data and for a set of generated images from poisoned diffusion models (SD and DDPM). The table demonstrates that using generated images improves the effectiveness of data poisoning detection. Different poisoning ratios (1%, 5%, 10%) are evaluated for two different trigger types (BadNets-1 and BadNets-2) and on different datasets (ImageNette and Caltech15).", "section": "5 Castle Walls: Defense Insights into Image Classification by Poisoned DMs"}]