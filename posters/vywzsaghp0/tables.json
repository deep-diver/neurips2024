[{"figure_path": "VywZsAGhp0/tables/tables_2_1.jpg", "caption": "Table 1: Common residual connections for GNNs.", "description": "This table summarizes common residual connections used in Graph Neural Networks (GNNs) to alleviate the over-smoothing issue.  It lists four types of residual connections (Res, InitialRes, Dense, JK) along with their corresponding GCN formulas. Each formula shows how the output of the k-th layer (Hk) is calculated, incorporating residual connections with different aggregation strategies of previous layers' outputs.", "section": "2.3 Residual connection"}, {"figure_path": "VywZsAGhp0/tables/tables_3_1.jpg", "caption": "Table 2: Utilization of neighborhood subgraphs by various residual methods.", "description": "This table shows how different residual methods utilize neighborhood subgraphs from order 0 to k.  ResGCN and APPNP use summation, while DenseGCN and JKNet use aggregation functions to combine subgraph information.  The formulas demonstrate how each method incorporates multiple orders of neighborhood information.", "section": "3 Why does the residual method alleviate over-smoothing?"}, {"figure_path": "VywZsAGhp0/tables/tables_7_1.jpg", "caption": "Table 3: Summary of classification accuracy (%) results on real-world datasets. The best results are in bold, and the second best results are underlined.", "description": "This table presents a comparison of classification accuracy across various GNN models (GCN and GAT, with several residual connection methods) on ten real-world datasets.  The accuracy is shown for each model on each dataset, with the best performing model for each dataset highlighted in bold and the second-best underlined. This allows for a direct performance comparison between different GNN architectures and the impact of different residual methods.", "section": "5.4 Fully observed feature setting (RQ2)"}, {"figure_path": "VywZsAGhp0/tables/tables_8_1.jpg", "caption": "Table 3: Summary of classification accuracy (%) results on real-world datasets. The best results are in bold, and the second best results are underlined.", "description": "This table presents a comparison of the classification accuracy achieved by various GNN models (GCN and GAT with different residual connections) across ten real-world datasets.  The accuracy is reported as a percentage, averaged over multiple runs, with error bars omitted. The best and second-best performances for each dataset are highlighted in bold and underlined, respectively.  The table allows for a comparison of different residual connection techniques to mitigate over-smoothing, with performance assessed at varying numbers of layers.", "section": "5.4 Fully observed feature setting (RQ2)"}, {"figure_path": "VywZsAGhp0/tables/tables_8_2.jpg", "caption": "Table 5: Comparison of different methods across various datasets and memory consumption (MB) and training time (ms / epoch) on Ogbn-arxiv. The best performance for each dataset is in bold, while the second best is underlined.", "description": "This table compares the performance of various GNN models (GCN, ResGCN, GCNII, JKNet, DenseGCN, and PSNR-GCN) on three large graph datasets (Coauthor-Physics, Flickr, and Ogbn-arxiv).  The metrics used are accuracy and memory consumption (in MB) and training time (in ms per epoch) on the largest dataset (Ogbn-arxiv). The best performing model for each dataset is highlighted in bold, with the second-best underlined.  This table demonstrates the scalability and performance of PSNR-GCN compared to other methods.", "section": "5.6 Performance on large graphs (RQ4)"}, {"figure_path": "VywZsAGhp0/tables/tables_12_1.jpg", "caption": "Table 6: Node classification accuracy (%) on GCN backbone. The best results across different layers are highlighted.", "description": "This table presents the node classification accuracy achieved by various GCN models (GCN, ResGCN, GCNII, DenseGCN, JKNet, DropMessage-GCN, Half-Hop-GCN, DeProp-GCN, and PSNR-GCN) across six different layer depths (2, 4, 8, 16, 32, and 64) on various datasets (Cora, Citeseer, CS, Photo, Chameleon, and Squirrel). The best accuracy for each dataset at each layer depth is highlighted to demonstrate the comparative performance of these models in mitigating oversmoothing at varying network depths.", "section": "A Fully Observed Node Classification"}, {"figure_path": "VywZsAGhp0/tables/tables_13_1.jpg", "caption": "Table 6: Node classification accuracy (%) on GCN backbone. The best results across different layers are highlighted.", "description": "This table presents the node classification accuracy achieved by different GNN models (GCN, ResGCN, GCNII, DenseGCN, JKNet, DropMessage-GCN, Half-Hop-GCN, DeProp-GCN, and PSNR-GCN) on various datasets (Cora, Citeseer, CS, Photo, Chameleon, Squirrel).  The accuracy is shown for different numbers of layers (2, 4, 8, 16, 32, 64) in the GNN architecture. The best accuracy for each dataset across all layer numbers is highlighted, demonstrating the performance of each model at varying depths and its effectiveness in mitigating over-smoothing.", "section": "A Fully Observed Node Classification"}, {"figure_path": "VywZsAGhp0/tables/tables_19_1.jpg", "caption": "Table 8: Dataset statistics of real-world datasets.", "description": "This table presents the key statistics for ten real-world datasets used in the paper's experiments.  The datasets are diverse, encompassing citation networks (Cora, Citeseer, Pubmed), web networks (Chameleon, Squirrel), co-authorship/co-purchase networks (Coauthor-CS, Amazon-Photo, Coauthor-Physics), and a large-scale dataset from Ogbn (Ogbn-arxiv).  For each dataset, the number of nodes, edges, features, and classes are provided, showcasing the variability in size and complexity of the graph data used.", "section": "G.1 Details of Datasets"}, {"figure_path": "VywZsAGhp0/tables/tables_19_2.jpg", "caption": "Table 3: Summary of classification accuracy (%) results on real-world datasets. The best results are in bold, and the second best results are underlined.", "description": "This table presents the classification accuracy results of various GNN models (GCN and GAT with different residual connection methods including ResGCN, GCNII, DenseGCN, JKNet, PSNR, Half-Hop, DropMessage, DeProp) on ten real-world datasets (Cora, Citeseer, CS, Photo, Chameleon, Squirrel, etc.).  The table shows the average accuracy and rank for each model on each dataset.  The best and second-best performing models are highlighted in bold and underlined respectively. This allows for a comparison of the performance of different GNN models across various datasets. The average rank of each model across all datasets is also shown.", "section": "5.4 Fully observed feature setting (RQ2)"}, {"figure_path": "VywZsAGhp0/tables/tables_20_1.jpg", "caption": "Table 3: Summary of classification accuracy (%) results on real-world datasets. The best results are in bold, and the second best results are underlined.", "description": "This table presents a comparison of the classification accuracy achieved by various GNN models (including the proposed PSNR model) on ten different real-world datasets.  The accuracy is shown for each model on each dataset, and the best and second-best results are highlighted for easy comparison. The table allows readers to quickly assess the relative performance of the different models across different datasets.", "section": "5.4 Fully observed feature setting (RQ2)"}, {"figure_path": "VywZsAGhp0/tables/tables_20_2.jpg", "caption": "Table 11: Different GraphEncoder performance for SSNC task (layer 2)", "description": "This table presents the performance comparison of different graph encoders (GCN, GAT, and SAGE) on the node classification task using the PSNR module with two layers.  The results are shown for six different datasets: Cora, Citeseer, CS, Photo, Chameleon, and Squirrel.  The table allows readers to assess the impact of different encoder choices on the overall performance of the proposed PSNR module.", "section": "I. Comparison of Experimental Results of Different GraphEncoders"}, {"figure_path": "VywZsAGhp0/tables/tables_20_3.jpg", "caption": "Table 3: Summary of classification accuracy (%) results on real-world datasets. The best results are in bold, and the second best results are underlined.", "description": "This table presents the classification accuracy results of different GNN models (GCN and GAT with various residual methods including PSNR) on ten real-world datasets.  The best and second-best accuracies for each dataset are highlighted in bold and underlined, respectively. The average rank of each model across all datasets is also given to provide a comprehensive comparison. This helps assess the overall performance of the proposed PSNR method compared to other existing methods for node classification in various graph scenarios.", "section": "5.4 Fully observed feature setting (RQ2)"}]