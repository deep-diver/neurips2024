[{"type": "text", "text": "ReF-LDM: A Latent Diffusion Model for Reference-based Face Image Restoration ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Chi-Wei Hsiao1 Yu-Lun Liu2 Cheng-Kun Yang1 Sheng-Po Kuo1 Yucheun Kevin Jou1 Chia-Ping Chen1 ", "page_idx": 0}, {"type": "text", "text": "MediaTek 2National Yang Ming Chiao Tung University ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "While recent works on blind face image restoration have successfully produced impressive high-quality (HQ) images with abundant details from low-quality (LQ) input images, the generated content may not accurately reflect the real appearance of a person. To address this problem, incorporating well-shot personal images as additional reference inputs could be a promising strategy. Inspired by the recent success of the Latent Diffusion Model (LDM), we propose ReF-LDM\u2014an adaptation of LDM designed to generate HQ face images conditioned on one LQ image and multiple HQ reference images. Our model integrates an effective and efficient mechanism, CacheKV, to leverage the reference images during the generation process. Additionally, we design a timestep-scaled identity loss, enabling our LDM-based model to focus on learning the discriminating features of human faces. Lastly, we construct FFHQ-Ref, a dataset consisting of 20,405 high-quality (HQ) face images with corresponding reference images, which can serve as both training and evaluation data for reference-based face restoration models. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent works [5, 26, 32] have achieved impressive results in generating a realistic high-quality (HQ) face image from an input low-quality (LQ) image. However, the important features of a person\u2019s face may be corrupted in the LQ image, and thus the reconstructed image may look like a different person. To tackle this problem, besides the LQ image, additional HQ images of this person can be used as reference input. Moreover, allowing multiple reference images may lead to better quality because they offer more comprehensive appearance of this person in different conditions, e.g., different poses, expressions, or lighting. ", "page_idx": 0}, {"type": "text", "text": "A previous work [16] has explored using multiple reference images for face restoration. Their method, however, depends on a face landmark model to detect facial components (i.e., eyes, nose, and mouse), which may become unreliable when the input LQ image is severely degraded. Besides, latent diffusion model (LDM) [22] has also been used in different image generating tasks with different input conditions, such as low-resolution images, semantic maps, or sketch images [22, 29]. ", "page_idx": 0}, {"type": "text", "text": "Inspired by the recent success of LDM, we propose ReF-LDM for reference-based face image restoration. Unlike previous conditional LDM methods where their input conditions are usually spatially aligned with the target image, the reference images are not aligned with the target HQ image in our case. Therefore, we design a CacheKV mechanism, which effectively and efficiently integrates the reference images, albeit with different poses and expressions. Furthermore, we introduce a timestep-scaled identity loss to drive the reconstructed image to look like the same person of the LQ and reference images. Lastly, we also construct a new large-scale dataset of face images with corresponding reference images, which can serve as both training and evaluation datasets for future reference-based face restoration research. ", "page_idx": 0}, {"type": "image", "img_path": "QY4SpBhQZI/tmp/add2873a9266e8961fdc768c0a52cf95de53b555f8a91eec6bbcc74ebf4cfa2d.jpg", "img_caption": ["(d) Input reference images "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Reference-based face image restoration. Given an input low-quality face image (a), a Latent Diffusion Model (LDM) can reconstruct a high-quality image (b); however, it may not be faithful to the individual\u2019s facial identity. To address this problem, we propose ReF-LDM, which restores a high-quality image with faithful details (c) by utilizing additional reference images (d). ", "page_idx": 1}, {"type": "text", "text": "With the above components, our ReF-LDM outperforms recent state-of-the-art methods with a significant improvement in face identity similarity. Extensive ablation studies for the proposed CacheKV mechanism and timestep-scaled identity loss are also conducted and reported. The main contributions of this work can be summarized as: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose ReF-LDM, which features an effective and efficient CacheKV mechanism, for restoring an LQ face image using multiple reference images. \u2022 We introduce a timestep-scaled identity loss, which considers the characteristics of diffusion models and helps ReF-LDM better learn the discriminating features of human identities. \u2022 We construct FFHQ-Ref, a dataset comprising 20,406 high-quality face images and their corresponding reference images, to facilitate the advance of reference-based face image restoration. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Face restoration without personal reference images Numerous studies have been proposed for blind face image restoration [28, 2, 5, 32, 20, 13, 26]. Recent works such as VQFR [5] and CodeFormer [32] have achieved promising results by exploiting VQGAN, while DAEFR [26] further employs a dual-branch encoder to mitigate the domain gap between LQ and HQ images. Inspired by the success of diffusion models, several works [23, 31, 17, 27, 25] have adopted diffusion models for face image restoration. However, as these methods do not leverage reference images, the restored images may differ from the authentic facial appearance of a person, especially when an input image is severely degraded. ", "page_idx": 1}, {"type": "text", "text": "Face restoration with personal reference images Several methods [14, 15, 16, 19] have attempted to utilize additional reference images to enhance personal fidelity in face restoration. GFRNet [14] warps a single reference image to match the face pose of the LQ image, while ASFNet [15] selects the reference image with the closest matching facial landmarks to serve as the network input. Closer to the setting of this work, DMDNet [16] also utilizes multiple reference images. It detects facial landmarks on the LQ image and the reference images to extract features of facial components, and then integrates these features into the model by querying the corresponding components. However, their method relies on landmark detection, which may not be robust on severely degraded LQ images. In contrast, our ReF-LDM implicitly learns the correspondences between the features of the LQ image and the reference images, without the need for landmark detection. From a different perspective, MyStyle [19] adopts a per-person optimization setting, leveraging hundreds of images of an individual to define a personalized subspace within the latent space of a StyleGAN [12]. In comparison, our approach offers greater flexibility, capable of utilizing one to several reference images without the need for personalized model optimization for each individual. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Latent diffusion models with image conditions Previous work demonstrates that LDM can generate an image from a low-resolution image by simple channel-axis concatenation [22]. However, reference images in our task are not spatially aligned with the target HQ image, thus requiring a more sophisticated integration mechanism. MasaCtrl [1] achieves text-to-image synthesis with a single reference image by replacing the original keys and values tokens with those from the reference image. However, their solution requires passing the reference image through the denoising network for multiple timesteps, which increases computation and limits its feasibility for extending to multiple reference images. In contrast, we propose an efficient CacheKV mechanism that leverages multiple reference images by eliminating the redundant network passes. ", "page_idx": 2}, {"type": "text", "text": "3 The proposed ReF-LDM model ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we present the proposed ReF-LDM model. We introduce the network architecture in Sec. 3.1, where a CacheKV mechanism is designed to leverage reference images. We illustrate how to train our model with the timestep-scaled identity loss in Sec. 3.2. ", "page_idx": 2}, {"type": "image", "img_path": "QY4SpBhQZI/tmp/aa548c06a98afad80d59a150fc83e63106c234ef6c8e07b99e58818da224bc51.jpg", "img_caption": ["Figure 2: The proposed ReF-LDM pipeline. Our model accepts a low-quality image and multiple high-quality reference images as input and generates a high-quality image. The blue top panel alone represents a typical LDM [22] denoising process. For an LQ image $\\mathbf{x}_{\\mathrm{LQ}}$ , we concatenate its latent ${\\bf z}_{\\mathrm{LQ}}$ with $\\mathbf{Z}_{t}$ along the channel axis to serve as the input for the denoising U-net. For the reference images $\\{{\\bf x}_{\\mathrm{ref}}\\}$ , we design a CacheKV mechanism, depicted in the red panel, to extract and cache their key and value tokens using the same denoising U-net for just one time. These cached KV tokens can then be utlized repeatedly in each of the $T$ timesteps of the main denoising process. During training, we adopt the classic LDM loss $(\\mathcal{L}_{\\mathrm{LDM}})$ ) and introduce a timestep-scaled identity loss $(\\mathcal{L}_{\\mathrm{time\\,ID}})$ . "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "3.1 Model architecture of ReF-LDM ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The proposed ReF-LDM accepts an input LQ image and multiple reference images to generate a target HQ image. Its model architecture is based on the latent diffusion model [22], with additional designs to incorporate the input LQ image and the reference images. ", "page_idx": 3}, {"type": "text", "text": "3.1.1 Preliminaries on Latent Diffusion Model ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To generate an image, an image diffusion model [8] starts from a noisy image $\\mathbf{x}_{T}\\,\\in\\,\\mathbb{R}^{H\\times W\\times3}$ , initialized with a Gaussian distribution, and gradually denoises it to a clean image $\\mathbf{X}_{\\mathrm{0}}$ with a denoising network over $T$ timesteps. A latent diffusion model [22] operates similarly, but the diffusion process takes place in a more compact latent space of a pre-trained and frozen autoencoder (encoder $\\mathcal{E}$ and decoder $\\mathcal{D}$ ). That is, it begins with a random latent $\\mathbf{z}_{T}\\in\\mathbb{R}^{H_{z}\\times W_{z}\\times C_{z}}$ and progressively denoises it to a clean latent $\\mathbf{Z}_{0}$ . A clean image is then generated by passing the clean latent through the decoder during the inference phase, i.e., $\\mathbf{x}_{0}=\\mathcal{D}(\\mathbf{z}_{0})$ ; conversely, a ground truth clean latent is obtained by encoding a clean image with the encoder during the training phase, i.e., $\\mathbf{z}_{0}^{*}=\\mathcal{E}(\\mathbf{x}_{0}^{*})$ . A typical choice for the denoising network is a U-net with self-attention layers at multiple scales. ", "page_idx": 3}, {"type": "text", "text": "3.1.2 CacheKV: a mechanism for incorporating reference images ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As illustrated in Fig. 2, our ReF-LDM leverages an input LQ image $\\mathbf{x}_{\\mathrm{LQ}}$ and multiple reference images $\\{{\\bf x}_{\\mathrm{ref}}\\}$ to generate a target HQ image $\\mathbf{x}_{\\mathrm{HQ}}$ . For an LQ image, we simply concatenate its latent encoded by the frozen encoder, $\\mathbf{z}_{\\mathrm{LQ}}=\\mathcal{E}(\\mathbf{x}_{\\mathrm{LQ}})$ , with the diffusion denoising latent $\\mathbf{Z}_{t}$ along the channel axis to serve as the input of the denoising U-net. For reference images, we design a CacheKV mechanism. Essentially, we extract and cache the features of reference images using the same denoising U-net just once; these cached features can then be used repeatedly at each of the $T$ timesteps in the main denoising process. Specifically, we pass the encoded latent of each reference image, $\\mathbf{z}_{\\mathrm{ref}}=\\mathcal{E}(\\mathbf{x}_{\\mathrm{ref}})$ , through the U-net to extract their keys and values (KVs) at each self-attention layer and store them in a CacheKV. Subsequently, during the main diffusion process, within each self-attention layer of the U-net, we concatenate the reference KVs (from the corresponding self-attention layer) with the main KVs along the token axis. This mechanism enables the U-net to incorporate the additional KVs from the reference images into the main denoising process. When extracting KVs from the reference images, we use a timestep embedding of $t=0$ and pad $\\mathbf{Z}_{\\mathrm{ref}}$ with a zero tensor to accommodate the additional channels introduced for the LQ image. ", "page_idx": 3}, {"type": "text", "text": "To summarize, for inference, we first run the U-net once to extract CacheKV from the reference images; subsequently, we proceed through the main denoising process for $T$ timesteps, during which the U-net integrates ${\\bf z}_{\\mathrm{LQ}}$ and reference CacheKV. For training, in each iteration, we first run the U-net to extract CacheKV, and then we run the U-net again to estimate the target latent from a sampled noisy latent $\\mathbf{Z}_{t}$ , incorporating the conditions ${\\bf z}_{\\mathrm{LQ}}$ and reference CacheKV. ", "page_idx": 3}, {"type": "text", "text": "3.1.3 Comparing CacheKV with other designs ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "There are other intuitive designs for integrating the reference latents $\\{{\\bf z}_{\\mathrm{ref}}\\}$ into the diffusion denoising process. However, they are either ineffective or computationally inefficient compared to the proposed CacheKV. The quantitative evaluation and computational analysis is reported in Sec. 5.2.1. We depict these designs in Fig. 3 and provide an intuitive explanation as follows: ", "page_idx": 3}, {"type": "text", "text": "\u2022 Channel-concatenation: Concatenating the condition with $\\mathbf{Z}_{t}$ along the channel axis works well for LQ images (and for other 2D conditions such as semantic maps [22]); however, it is not effective for reference images. A critical difference between these conditions is that\u2014while the LQ image is spatially aligned with the target HQ image, the reference images are not. Therefore, it is challenging for the model to leverage reference images using simple channel-concatenation. ", "page_idx": 3}, {"type": "text", "text": "\u2022 Cross-attention: Cross-attention layers have been proven useful for text conditions in textto-image models [22]. In our ablation experiment, we insert a cross-attention layer after each self-attention layer and use the reference latents $\\{{\\bf z}_{\\mathrm{ref}}\\}$ to produce keys and values. While cross-attention appears to have the potential to address the spatial misalignment problem, it still fails to effectively utilize the reference images. The difference between our CacheKV and the cross-attention setting is that CacheKV provides the reference images in a more aligned feature space for the main denoising process to leverage. Specifically, the CacheKV is extracted using the same U-net and the corresponding self-attention layer as in the main denoising process. In contrast, the cross-attention setting processes the reference images only with the frozen encoder, resulting in features that are less aligned with those in the U-net of the denoising process. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "\u2022 Spatial-concatenation: Concatenating $\\{{\\bf z}_{\\mathrm{ref}}\\}$ with $\\mathbf{z}_{t}$ along the spatial dimension to serve as the input for U-net also effectively leverages the reference images. Conceptually, spatialconcatenation treats reference images in a very similar way to our CacheKV. In both mechanisms, $\\{{\\bf z}_{\\mathrm{ref}}\\}$ are processed through the denoising U-net, allowing the reference KVs to be accessed by the queries (Qs) of the main diffusion latent $\\mathbf{Z}_{t}$ . However, spatial-concatenation requires significantly more computational resources compared to our CacheKV. It passes $\\{{\\bf z}_{\\mathrm{ref}}\\}$ with $\\mathbf{Z}_{t}$ to the U-net at each of the $T$ denoising timesteps, whereas CacheKV only passes $\\{{\\bf z}_{\\mathrm{ref}}\\}$ through the U-net once. Moreover, spatial-concatenation also requires significantly more GPU memory, as the spatial size of the input for the U-net increases with the number of reference images. As for a self-attention layer in the U-net, both mechanisms increase memory usage; CacheKV introduces additional reference KVs, while spatial-concatenation introduces reference QKVs. ", "page_idx": 4}, {"type": "image", "img_path": "QY4SpBhQZI/tmp/0b400da2ac385d5162cdcf25314fd422e408251dc3c398b18d6e2cb4d6e5d19e.jpg", "img_caption": ["Figure 3: Different mechanisms for incorporating reference images into the main denoising process. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "3.2 Timestep-scaled identity loss ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "3.2.1 Timestep-scaled identity loss ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "As this work aims for face image restoration, we employ the identity loss to enhance face similarity, which is adopted in many face-related tasks [9, 21, 28]. The identity loss minimizes the distance within the embedding space of a face recognition model, thereby capturing the discriminating features of human faces more effectively than the plain RGB pixel space. In our experiments, we use the ArcFace model [3] with cosine distance between the 1D embedding vectors as the identity loss. ", "page_idx": 4}, {"type": "text", "text": "However, naively adding identity loss to the training of ReF-LDM significantly worsens the image quality. One possible explanation might be that, the one-step model prediction $\\mathbf{x}_{0\\mid\\mathrm{t}}={\\mathcal{D}}(\\mathbf{z}_{0}|\\mathbf{z}_{t})$ at a very noisy timestep (e.g., $t=T$ ) is very different from a natural face image and thus out of the distribution that the ArcFace model is trained on; therefore, the identity loss provides ineffective supervision for diffusion models at large timesteps. ", "page_idx": 4}, {"type": "text", "text": "Based on this assumption, we propose a timestep-scaled identity loss, where a timestep-dependent scaling factor is introduced to scale down the identity loss when a larger timestep is sampled in a training step. Specifically, the timestep-scaled identity loss is defined as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{time\\,ID}}=\\sqrt{\\bar{\\alpha_{t}}}\\cdot\\mathcal{L}_{\\mathrm{ID}}=\\sqrt{\\bar{\\alpha_{t}}}\\cdot\\Big(1-\\frac{R(\\mathbf{x})\\cdot R(\\mathbf{x}^{*})}{\\|R(\\mathbf{x})\\|\\|R(\\mathbf{x}^{*})\\|}\\Big),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $R$ is a face recognition model, and $\\sqrt{\\bar{\\alpha}_{t}}$ follows the definition in a typical diffusion process [8, 22] in which a noisy latent $\\mathbf{Z}_{t}$ is sampled given a clean latent $\\mathbf{z}_{\\mathrm{0}}^{\\ast}$ as: ", "page_idx": 5}, {"type": "equation", "text": "$$\nq(\\mathbf{z}_{t}|\\mathbf{z}_{0}^{*})=\\mathcal{N}(\\sqrt{\\bar{\\alpha_{t}}}\\mathbf{z}_{0}^{*},(1-\\bar{\\alpha_{t}})\\mathrm{I})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "3.2.2 Training ReF-LDM with timestep-scaled identity loss ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We train our ReF-LDM with the classic LDM loss and the proposed timestep-scaled identity loss: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{t o t a l}=\\mathcal{L}_{\\mathrm{LDM}}+\\lambda_{\\mathrm{time\\,ID}}\\,\\mathcal{L}_{\\mathrm{time\\,ID}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Recall that the denoising U-net estimates the target latent in the latent space of the frozen autoencoder, and a typical $\\mathcal{L}_{\\mathrm{LDM}}$ is computed as the L1 distance between the estimated latent and the target latent. To compute the identity loss with the face recognition model, which accepts an image as input, we decode the estimated latent into the image space using the frozen decoder, i.e, $\\mathbf{x}_{0}=\\mathcal{D}(\\mathbf{z}_{0})$ . The experiments in Sec. 5.2.2 show that timestep-scaled identity loss can improve face similarity without degrading image quality, unlike the naive usage of identity loss. ", "page_idx": 5}, {"type": "text", "text": "4 FFHQ-Ref dataset ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Recent works for non-reference-based face restoration commonly train their models with FFHQ dataset [12], which comprises 70,000 high-quality face images of wide appearance variety with appropriate licenses crawled from Flickr. These images are not provided with reference labels originally; however, we find that a good portion of the images are of the same identities. Thus, we construct a reference-based dataset\u2014FFHQ-Ref\u2014based on the FFHQ dataset, with careful consideration described as follows. ", "page_idx": 5}, {"type": "text", "text": "4.1 Finding reference images of the same identity ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To determine whether two images belong to the same identity, we utilize the face recognition model ArcFace [3]. Specifically, we first extract the 1D ArcFace embeddings for all images. Then, for each image, we compute the cosine distances between its embedding and the embeddings of all other images. A distance less than a threshold $r\\:=\\:0.4$ indicates that the images are valid references belonging to the same person. Following this procedure, we identify 20,405 images with corresponding reference images. ", "page_idx": 5}, {"type": "text", "text": "4.2 Splitting data according to identity ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To enable the FFHQ-Ref dataset to serve as both training and evaluation datasets for reference-based face restoration models, we divide the images into train, validation, and test splits. However, random data splitting may result in the train and test splits containing images of the same individual, which is not ideal for a fair evaluation. To ensure that all images of a single identity are assigned to only one data split, we group the images based on their identities. Specifically, we consider identity grouping as a graph problem, where each image acts as a vertex and any pair of images with a distance less than $r$ are connected by edges. We then apply the connected component algorithm from graph theory, where each connected component represents a group of images belonging to the same person. Finally, we identified 6,523 identities and divided them into three splits: a train split with 18,816 images of 6,073 identities, a validation split with 732 images of 300 identities , and a test split with 857 images of 150 identities. We report more statistics in Appendix C. ", "page_idx": 5}, {"type": "text", "text": "4.3 Constructing evaluation dataset with practical considerations ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Practical Considerations For a fair and meaningful evaluation, the input reference images should not be excessively similar to the target image; hence, we set a minimum cosine distance threshold of 0.1 for the test set. Additionally, we manually check the images in the test split to verify that all reference images indeed correspond to the same identity. Furthermore, in the context of referencebased face restoration applications, it is preferable to select input reference images that capture a more comprehensive representation of a person\u2019s appearance, such as varying face poses or expressions. Although a target image in the test split of our FFHQ-Ref may have two to nine reference images, different reference-based methods may have their own constraints on the maximum number of input reference images. To emulate a more representative set of reference images, we sort all available reference images of a target image using farthest point sampling on the ArcFace distance. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Degradation synthesis for input LQ images For synthesizing input LQ images from ground truth HQ images, we follow the degradation model used in previous works [28, 5, 32]: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{x}_{\\mathrm{LQ}}=\\{\\left[\\left(\\mathbf{x}_{\\mathrm{HQ}}*k_{\\sigma}\\right)\\downarrow_{r}+n_{\\delta}\\right]\\!\\mathrm{pEG}_{q}\\}\\uparrow_{r},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where an HQ image is blurred with a Gaussian kernel $k_{\\sigma}$ , downsampled by $r$ scale, added with a Gaussian noise $n_{\\delta}$ , compressed with JPEG quality level $q$ , and upscaled to the original size. ", "page_idx": 6}, {"type": "text", "text": "We construct two evaluation datasets with different degradation levels: ", "page_idx": 6}, {"type": "text", "text": "\u2022 FFHQ-Ref-Moderate: $\\sigma,r,\\delta$ , and $q$ are sampled from [0, 8], [1, 8], [0, 15], and [60, 100].   \n\u2022 FFHQ-Ref-Severe: $\\sigma,r,\\delta$ , and $q$ are sampled from [8, 16], [8, 32], [0, 20], and [30, 100]. ", "page_idx": 6}, {"type": "text", "text": "4.4 Comparison between FFHQ-Ref and existing datasets ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Table 1 summarizes the differences between our proposed FFHQ-Ref and existing datasets. While the CelebRef-HQ dataset [16] has been constructed to train and evaluate reference-based face restoration models, our FFHQ-Ref dataset contains twice as many images and six times the number of identities compared to CelebRef-HQ. Moreover, built upon FFHQ [12], FFHQ-Ref provides superior image quality over CelebRef-HQ, as indicated by the lower NIQE score (3.68 vs. 3.97). Some groundtruth images in CelebRef-HQ are affected by watermarks and mirror padding artifacts, as shown in Appendix B. ", "page_idx": 6}, {"type": "table", "img_path": "QY4SpBhQZI/tmp/341a83d18b4ffffab9397caddbee3810bbbdf8ff5537686136e255b056fe9273.jpg", "table_caption": ["Table 1: Comparison between the proposed FFHQ-Ref and existing datasets. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we describe the experimental setup in Sec. 5.1, discuss ablation studies in Sec. 5.2, and provide the comparison between our ReF-LDM and the state-of-the-art methods in Sec. 5.3 ", "page_idx": 6}, {"type": "text", "text": "5.1 Experimental setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1.1 Implementation details ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To exploit more ground truth images without available reference images, we use 68,411 images in the FFHQ dataset to train a VQGAN [4] as the frozen autoencoder and an LDM with only LQ condition. We then finetune our ReF-LDM from the LQ-conditioned LDM with the 18,816 images in our FFHQ-Ref dataset. All models are trained excluding the test split images to ensure fair evaluation on our FFHQ-Ref benchmark. In our experiments, we adopt a $512\\mathrm{x}512$ image resolution, fix the number of reference images to five, and set loss scale $\\lambda_{\\mathrm{time\\,ID}}$ to 0.1. During training, we synthesize input LQ images with $\\sigma,r,\\delta$ , and $q$ sampled from [0, 16], [1, 32], [0, 20], and [30, 100], respectively. For inference, we use 100 DDIM [24] steps and a classifier-free-guidance [7] with a scale of 1.5 towards reference images. We provides more implementation details in the Appendix G. ", "page_idx": 6}, {"type": "text", "text": "5.1.2 Evaluation datasets and metrics ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "For evaluation datasets, we use the test split of our FFHQ-Ref with two different degradation levels: severe and moderate. In addition, previous non-reference-based methods commonly use CelebA-Test [28] for evaluation, which comprises 3,000 LQ and HQ image pairs sampled from the CelebA-HQ dataset [11]. Therefore, we follow the same procedures described in Sec. 4 to construct a subset of 2,533 images with available reference images, termed CelebA-Test-Ref. ", "page_idx": 7}, {"type": "text", "text": "For evaluation metrics, we adopt the identity similarity (IDS) [5, 32], which is the cosine similarity calculated using the face recognition model ArcFace [3]. We also use the widely used perceptual metrics LPIPS [30]. As face pixels are more of concern in the task of face restoration, we also measure the face-region LPIPS (fLPIPS), which is the LPIPS calculated using only the pixels in face regions. For assessing no-reference image quality, we adopt NIQE [18]. Furthermore, we measure the FID [6], using 70,000 images from the FFHQ dataset as the target distribution. ", "page_idx": 7}, {"type": "text", "text": "5.2 Ablation studies ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We provide the ablation studies of the proposed CacheKV, timestep-scaled identity loss, and the number of input reference images. In each ablation experiment, we fine-tune the model for 50,000 steps from the same LDM pre-trained without reference images. We compare the difference settings with the FFHQ-Ref-Severe dataset. ", "page_idx": 7}, {"type": "text", "text": "5.2.1 CacheKV and other mechanisms ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The CacheKV is proposed for integrating the input reference images into the diffusion denoising process. We compare it with other mechanisms illustrated in Sec. 3.1.3. According to Table 2, channel-concatenation and cross-attention fail to leverage reference images to improve the identity similarity (IDS). In contrast, both spatial-concatenation and our CacheKV significantly enhance IDS. Moreover, our CacheKV is more computationally efficient than spatial-concatenation, requiring only $20\\%$ of the inference time and $39\\%$ of the GPU memory. ", "page_idx": 7}, {"type": "table", "img_path": "QY4SpBhQZI/tmp/40737d71f408c9c3c2304cc18666e8135a08851e5f6c2b80e1ddf9d7f5795a7e.jpg", "table_caption": ["Table 2: Comparison between CacheKV and other mechanisms for input reference images (run with five reference images on a single GTX 1080). "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "QY4SpBhQZI/tmp/d4babb56d1d7a70f5ba967f7f277e933a703a8877f6def7e3fe6ed46633f57db.jpg", "table_caption": ["Table 3: Ablation results for the timestepscaled identity loss. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "QY4SpBhQZI/tmp/92bbae837d36b6f730ab6c0b49162fdbaec4267a507af12490b0e18d97565b6b.jpg", "table_caption": ["Table 4: Design choices for ID loss scaling. "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "QY4SpBhQZI/tmp/8bba381ba7f114d2e814fddabdffdc2e7bf4705d74b9a675a2c9c8f7e1c0b8fb.jpg", "img_caption": ["Figure 4: Visual ablation results for the timestepscaled identity loss. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.2.2 Timestep-scaled identity loss ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To validate the beneftis of the proposed timestep-scaled identity loss, we train ReF-LDM with three different loss settings: without identity loss $(\\mathcal{L}_{\\mathrm{LDM}})$ , with naive identity loss $(\\mathcal{L}_{\\mathrm{LDM}}+\\mathcal{L}_{\\mathrm{ID}})$ , and with the proposed timestep-scaled identity loss $(\\mathcal{L}_{\\mathrm{LDM}}+\\mathcal{L}_{\\mathrm{time\\,ID}})$ . As show in Table 3 and Fig. 4, while the naive identity loss can improve identity similarity (IDS), our timestep-scaled identity loss can do so without sacrificing the image quality (NIQE). ", "page_idx": 8}, {"type": "text", "text": "As explained in Sec. 3.2, we employ $\\sqrt{\\bar{\\alpha}_{t}}$ to scale down the identity loss for a larger and noisier timestep $t$ . In Table 4, we compare this design choice with other alternative scaling factors, ${\\bf1}_{t<100}$ and $\\mathbf{1}_{t<500}$ , which apply the identity loss o\u221anly when the sampled timestep $t$ is smaller than 100 or 500, respectively. The results suggest that $\\dot{\\sqrt{\\alpha_{t}}}$ is more effective than the alternatives. ", "page_idx": 8}, {"type": "text", "text": "5.2.3 Multiple input reference images ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "There are two to nine reference images for a target image in the test split of our FFHQ-Ref. While we fix the number of reference images to five when training ReF-LDM, the proposed CacheKV mechanism has the flexibility to take varying number of reference images during inference. To validate the effectiveness of utilizing multiple reference images, we evaluate ReF-LDM with a maximum of 1, 3, 5, and 8 reference images, respectively. As shown in Table 5, using more reference images significantly improves the identity similarity (from 0.52 to 0.66). However, increasing the number of reference images also increases the computation time, as shown in Table 6. Since using eight reference images encounters an out-of-memory issue on a single GTX 1080, we use at most five reference images in our experiments for simplicity. ", "page_idx": 8}, {"type": "table", "img_path": "QY4SpBhQZI/tmp/eff3e4508a1b00af14be90b9fe98c212172741ce69ca412927c64dfd054007d7.jpg", "table_caption": ["Table 5: Image quality with different numbrs of reference images. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "QY4SpBhQZI/tmp/40e423569c48fa826eaa3e5b6af14e73052f2ea0042b681578488d5bec3b897d.jpg", "table_caption": ["Table 6: Inference time with different numbers of reference images. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.3 Comparison with state-of-the-art methods ", "text_level": 1, "page_idx": 8}, {"type": "table", "img_path": "QY4SpBhQZI/tmp/1c320a6f4fe6730b6082badafe5477ce9d4604afd160c4f2ce25f4a6e418439d.jpg", "table_caption": ["Table 7: Comparison of ReF-LDM with state-of-the-art methods across three benchmarks. Note the highlighting 1st, 2nd, and a gray cell indicating evaluation data leakage for prior methods . "], "table_footnote": ["\u2020As DMDNet encounters landmark detection failures and fails to yield results for 214/857, 29/857, and 488/2,533 images on the three benchmarks respectively, we compute the metrics for DMDNet using the remaining images. "], "page_idx": 8}, {"type": "text", "text": "5.3.1 Quantitative comparison ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We compare our ReF-LDM with state-of-the-art methods on FFHQ-Ref-Severe, FFHQ-Ref-Moderate, and CelebA-Test-Ref. Table 7 reports the performance of competing methods in terms of IDS, fLPIPS, LPIPS, and FID (targeting the FFHQ image distribution). Without the information in reference images, the existing non-reference-based restoration methods (CodeFormer [32], VQFR [5], and DAEFR [26]) fail to preserve the facial identity, leading to significantly lower IDS. The reference-based method, ", "page_idx": 8}, {"type": "text", "text": "DMDNet [16], fails to restore the severely degraded images because it depends on unreliable facial landmark detection, reflected by higher fLPIPS. In contrast, our ReF-LDM consistently outperforms DMDNet on identity similarity and other metrics, owing to the proposed CacheKV mechanism and timestep-scaled identity loss, which effectively leverage the input reference images without the need for landmark detection. We also note that our method exhibits slightly inferior results in LPIPS metric. This is due to the difference in the background pixels, we provide further details in the Appendix D. It is also worth mentioning that the competing methods benefti from data leakage on the FFHQ-Ref benchmarks, as their models are trained with the entire FFHQ dataset or with a different train split than the identity-based one in the proposed FFHQ-Ref. ", "page_idx": 9}, {"type": "text", "text": "5.3.2 Qualitative comparison ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In Fig. 5, we present a qualitative comparison between our ReF-LDM, the pre-trained LDM without reference images, CodeFomer (a SOTA non-reference-based method), and DMDNet (a SOTA reference-based method). Given the severely degraded image, DMDNet generates distorted face images based on incorrectly detected landmarks. While CodeFormer yields realistic face images, it does not preserve the facial identity well. In contrast, our ReF-LDM produces results that are both realistic and faithful to the individual\u2019s facial identity. ", "page_idx": 9}, {"type": "image", "img_path": "QY4SpBhQZI/tmp/e651816893173ee7c40fb186e1e80f4779dcab3b6aace62d865849d87f6329a9.jpg", "img_caption": ["Figure 5: Qualitative comparison. From left to right: input LQ, ground truth, other methods, and our ReF-LDM. From top to bottom: FFHQ-Ref-Severe, FFHQ-Ref-Moderate, and CelebA-Test-Ref. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "When the face region is occluded by other objects, our model may generate artifacts. For certain face poses (e.g, side face), the reconstructed eyes may appear unnatural. These problems are also commonly observed in other methods and might be caused due to the lack of such training images. However, there are some examples showing that these problems can be alleviated if our model is provided reference images with similar face poses to the target image. Visual examples of these limitations are provided in Appendix F. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In summary, we propose ReF-LDM, which incorporates the CacheKV mechanism and the timestepscaled identity loss, to effectively utilize multiple reference images for face restoration. Additionally, we construct the FFHQ-Ref dataset, which surpasses the existing dataset in both quantity and quality, to facilitate the research in reference-based face restoration. Evaluation results demonstrate that ReF-LDM achieves superior performance in face identity similarity over state-of-the-art methods. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The authors wish to express their gratitude to Professor Wei-Chen Chiu for his valuable suggestion to exclude the reference images that are too similar to the target images when constructing the proposed FFHQ-Ref dataset. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] M. Cao, X. Wang, Z. Qi, Y. Shan, X. Qie, and Y. Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 22560\u201322570, October 2023.   \n[2] C. Chen, X. Li, L. Yang, X. Lin, L. Zhang, and K.-Y. K. Wong. Progressive semantic-aware style transformation for blind face restoration. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11896\u201311905, 2021.   \n[3] J. Deng, J. Guo, X. Niannan, and S. Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In CVPR, 2019.   \n[4] P. Esser, R. Rombach, and B. Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12873\u201312883, 2021.   \n[5] Y. Gu, X. Wang, L. Xie, C. Dong, G. Li, Y. Shan, and M.-M. Cheng. Vqfr: Blind face restoration with vector-quantized dictionary and parallel decoder. In European Conference on Computer Vision, pages 126\u2013143. Springer, 2022.   \n[6] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017.   \n[7] J. Ho and T. Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022.   \n[8] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \n[9] R. Huang, S. Zhang, T. Li, and R. He. Beyond face rotation: Global and local perception gan for photorealistic and identity preserving frontal view synthesis. In Proceedings of the IEEE international conference on computer vision, pages 2439\u20132448, 2017.   \n[10] K. Karkkainen and J. Joo. Fairface: Face attribute dataset for balanced race, gender, and age for bias measurement and mitigation. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 1548\u20131558, 2021.   \n[11] T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017.   \n[12] T. Karras, S. Laine, and T. Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4401\u20134410, 2019.   \n[13] Y.-F. Lau, T. Zhang, Z. Rao, and Q. Chen. Ented: Enhanced neural texture extraction and distribution for reference-based blind face restoration. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 5162\u20135171, 2024.   \n[14] X. Li, M. Liu, Y. Ye, W. Zuo, L. Lin, and R. Yang. Learning warped guidance for blind face restoration. In Proceedings of the European conference on computer vision (ECCV), pages 272\u2013289, 2018.   \n[15] X. Li, W. Li, D. Ren, H. Zhang, M. Wang, and W. Zuo. Enhanced blind face restoration with multi-exemplar images and adaptive spatial feature fusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2706\u20132715, 2020.   \n[16] X. Li, S. Zhang, S. Zhou, L. Zhang, and W. Zuo. Learning dual memory dictionaries for blind face restoration. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(5):5904\u20135917, 2022.   \n[17] X. Lin, J. He, Z. Chen, Z. Lyu, B. Fei, B. Dai, W. Ouyang, Y. Qiao, and C. Dong. Diffbir: Towards blind image restoration with generative diffusion prior. arXiv preprint arXiv:2308.15070, 2023.   \n[18] A. Mittal, R. Soundararajan, and A. C. Bovik. Making a \u201ccompletely blind\u201d image quality analyzer. IEEE Signal processing letters, 20(3):209\u2013212, 2012.   \n[19] Y. Nitzan, K. Aberman, Q. He, O. Liba, M. Yarom, Y. Gandelsman, I. Mosseri, Y. Pritch, and D. Cohen-Or. Mystyle: A personalized generative prior. ACM Transactions on Graphics (TOG), 41(6):1\u201310, 2022.   \n[20] S. Pouyanfar, S. Sengupta, M. Mohammadi, E. Abraham, B. Bloomquist, L. Dauterman, A. Parikh, S. Lim, and E. Sommerlade. Frr-net: A real-time blind face restoration and relighting network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1240\u20131250, 2023.   \n[21] E. Richardson, Y. Alaluf, O. Patashnik, Y. Nitzan, Y. Azar, S. Shapiro, and D. Cohen-Or. Encoding in style: a stylegan encoder for image-to-image translation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2287\u20132296, 2021.   \n[22] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695, 2022.   \n[23] C. Saharia, J. Ho, W. Chan, T. Salimans, D. J. Fleet, and M. Norouzi. Image super-resolution via iterative refinement. IEEE transactions on pattern analysis and machine intelligence, 45(4):4713\u20134726, 2022.   \n[24] J. Song, C. Meng, and S. Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020.   \n[25] M. Suin, N. G. Nair, C. P. Lau, V. M. Patel, and R. Chellappa. Diffuse and restore: A region-adaptive diffusion model for identity-preserving blind face restoration. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 6343\u20136352, 2024.   \n[26] Y.-J. Tsai, Y.-L. Liu, L. Qi, K. C. Chan, and M.-H. Yang. Dual associated encoder for face restoration. In The Twelfth International Conference on Learning Representations, 2024.   \n[27] J. Wang, Z. Yue, S. Zhou, K. C. Chan, and C. C. Loy. Exploiting diffusion prior for real-world image super-resolution. 2024.   \n[28] X. Wang, Y. Li, H. Zhang, and Y. Shan. Towards real-world blind face restoration with generative facial prior. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9168\u20139178, 2021.   \n[29] L. Zhang, A. Rao, and M. Agrawala. Adding conditional control to textto-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3836\u20133847, 2023.   \n[30] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang. The unreasonable effectiveness of deep features as a perceptual metric. In CVPR, 2018.   \n[31] Y. Zhao, T. Hou, Y.-C. Su, X. Jia, Y. Li, and M. Grundmann. Towards authentic face restoration with iterative diffusion models and beyond. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7312\u20137322, 2023.   \n[32] S. Zhou, K. Chan, C. Li, and C. C. Loy. Towards robust blind face restoration with codebook lookup transformer. Advances in Neural Information Processing Systems, 35:30599\u201330611, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Broader Impacts ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The ReF-LDM has the capability to leverage personal appearances from reference images. This introduces a potential risk of misuse, where it could be employed for malicious face editing by using a low-quality image in conjunction with reference images from a different individual. ", "page_idx": 13}, {"type": "image", "img_path": "QY4SpBhQZI/tmp/ee5fdb4ffbde9f3172adfcfa61cc86356978f84801c910a1f4dc08bc21378b0e.jpg", "img_caption": ["Figure 6: Examples of ReF-LDM using reference images from two different individuals. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "B Image quality issues in the previous dataset CelebRef-HQ ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "As described in Sec. 4.4, the previous dataset for the reference-based face restoration task, CelebRef-HQ [16], exhibits issues with image quality. We provide examples where the ground truth images in this dataset are corrupted by watermarks and mirror padding in Fig. 7. ", "page_idx": 13}, {"type": "image", "img_path": "QY4SpBhQZI/tmp/709cb1bfdbbe0915bb97f0f349b9ef02dee68a00e5a62c38ae6c785eecd3129d.jpg", "img_caption": ["Figure 7: Example images with mirror padding and watermark artifacts in the CelebRef-HQ dataset. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "C Statistics of FFHQ-Ref dataset ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We analyze the statistics of the proposed FFHQ-Ref dataset, introduced in Sec. 4.   \nIn Fig. 8, we plot the distribution of the number of available reference images. ", "page_idx": 13}, {"type": "text", "text": "Furthermore, we assess the race, age, and gender distributions of the dataset using labels predicted by FairFace [10]. As depicted in Fig. 9, the race distribution within FFHQ-Ref is imbalanced, with a predominance of the \u2019white\u2019 category. To mitigate this, we intentionally sampled a greater number of images from other races to construct a more balanced test set. Additionally, as illustrated in Fig. 10, FFHQ-Ref encompasses a broad age range, from infants (0-2 years) to the elderly $^{70+}$ years). However, the distribution is not uniform across ages and genders. For example, there is a notably higher proportion of young females $29.2\\%$ of \u201920-29 female\u2019). ", "page_idx": 13}, {"type": "image", "img_path": "QY4SpBhQZI/tmp/f22f8189da908c94957c86b895719009eadb8c8cda2c5c09d6d543e32ff28381.jpg", "img_caption": ["Figure 8: Distribution of the number of available reference images per image in the FFHQ-Ref dataset of train, validation, and test splits. "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "QY4SpBhQZI/tmp/29b0ce4e49f60351c95b70addb00f6ad90f311868f0919ace85c6e640ef9fcdf.jpg", "img_caption": ["Figure 9: Race distribution within FFHQ-Ref dataset. "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "QY4SpBhQZI/tmp/fab1ec8c5ae63797a6cec7bf53d43f3a7f3332d54b5f5234dfc96f4f25b102fe.jpg", "img_caption": ["Figure 10: Age and gender distribution within FFHQ-Ref dataset. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "D Examples of differences in background regions ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In Fig. 11, we provide some examples where our ReF-LDM are more different to the ground truth in background pixels compared to prior methods. In the first example, the ReF-LDM attempts to restore another face in the background. In the second ", "page_idx": 14}, {"type": "text", "text": "example, our ReF-LDM restored the mirror padding in the CelebA-Test dataset as hairs. ", "page_idx": 15}, {"type": "image", "img_path": "QY4SpBhQZI/tmp/573031f73b39ebc9abd559d01129708e3f2b617f4baf623979c92487e3da2414.jpg", "img_caption": ["Figure 11: Examples where ReF-LDM generates background-region details that differ more from the ground truth. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "E Examples of illumination change ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Fig. 12 shows an example where ReF-LDM exhibits warmer illumination compared to LDM. We conjecture that this may be due to the impact of the strong warm lighting in the input reference images. To address this issue, one could employ post-processing tricks, such as adjusting the means of the R, G, B channels to match those of the input LQ image. Another potential solution might be training ReF-LDM with data augmentation on the illuminations of input reference images, to encourage the model to disregard the illuminations of input references and maintain consistency with that of the input LQ image. ", "page_idx": 15}, {"type": "image", "img_path": "QY4SpBhQZI/tmp/a50816b6c73b82293a7008a07498579421227f3dea58f4c255781c0c35c0484a.jpg", "img_caption": ["Figure 12: An example of (d) ReF-LDM demonstrating an illumination change, likely influenced by the strong warm lighting of the (e) input reference images. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "F Examples of failure cases ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Here we provide visual examples for the limitation described in Sec. 6. As shown Fig. 13, when the face region is occluded, our ReF-LDM and the prior models tend to generate from unnatural artifacts. For side face images, the ReF-LDM may not work well when the input reference images do not contain faces of similar pose, as shown in Fig. 14. However, Fig. 15 suggests that our ReF-LDM can effectively exploit the reference images of similar face poses to improve the results. ", "page_idx": 16}, {"type": "image", "img_path": "QY4SpBhQZI/tmp/59947c3660c679528c85571ad26947a423f00d3385dec7dedb732a15db45e16e.jpg", "img_caption": ["Figure 13: Some failure cases when the input LQ images are occluded. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "QY4SpBhQZI/tmp/546925d4bd225ba4b9ac8b6ee06324a4205d3f963cb33f79d452038c565dd457.jpg", "img_caption": ["Figure 14: Some failure cases of side faces when side-face images are absent in the references. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "QY4SpBhQZI/tmp/b194204da2eb4951a07265763ff4a189c2aceca7051fef38dff11d68de4d040f.jpg", "img_caption": ["Figure 15: Some successful cases of side faces when side-face images are included in the references. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "G More implementation details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "G.1 Classifier-free guidance towards reference images ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Classifier-free guidance [7] is a technique widely used in diffusion models for guiding the generated results towards a condition $c$ with a controllable scale factor $s$ at inference time: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\tilde{\\epsilon}_{\\theta}(\\mathbf{z}_{t},c)=\\epsilon_{\\theta}(\\mathbf{z}_{t},\\mathcal{D})+s\\cdot(\\epsilon_{\\theta}(\\mathbf{z}_{t},c)-\\epsilon_{\\theta}(\\mathbf{z}_{t},\\mathcal{D}))\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In our experiments, we use classifier-free guidance towards reference images with $s=1.5$ . ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\tilde{\\epsilon}_{\\theta}(\\mathbf{z}_{t},\\mathbf{z}_{\\mathrm{LQ}},\\{\\mathbf{z}_{\\mathrm{ref}}\\})=\\epsilon_{\\theta}(\\mathbf{z}_{t},\\mathbf{z}_{\\mathrm{LQ}},\\boldsymbol{\\mathcal{D}})+s\\cdot(\\epsilon_{\\theta}(\\mathbf{z}_{t},\\mathbf{z}_{\\mathrm{LQ}},\\{\\mathbf{z}_{\\mathrm{ref}}\\})-\\epsilon_{\\theta}(\\mathbf{z}_{t},\\mathbf{z}_{\\mathrm{LQ}},\\boldsymbol{\\mathcal{D}}))\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "During the training phase, we randomly drop the conditions by setting them to zero tensors with a probability of 0.1. ", "page_idx": 17}, {"type": "text", "text": "G.2 Data augmentation for input reference images ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "During the training phase, we use a fixed number of five input reference images. When a target images with less than five reference images are sampled, we repeat the reference images to obtain five reference images. In addition, we apply image augmentation to the input reference images with the following operations: color jitter (brightness $\\pm\\,0.2$ , contrast $\\pm\\,0.2$ , saturation $\\pm\\,0.2$ , hue $\\pm\\:0.02)$ , affine transform (rotation $\\pm\\nobreakspace2\\nobreakspace$ , translation $\\pm\\;0.05$ , scale $\\pm\\:0.05)$ , perspective transform (scale $\\pm\\ 0.2$ , probability 0.5), and horizontal filp (probability 0.5). Lastly, we randomly shuffle the order of available reference images for a target image, so that a different combination of reference images can be sampled at each training iteration. In Fig. 16, we provide an example where a set of two reference images is augmented to a set of five reference images. ", "page_idx": 17}, {"type": "image", "img_path": "QY4SpBhQZI/tmp/11d980c5911b3923f4f343a0e5830b97ddd7c248c5d032452531484d93aea5bf.jpg", "img_caption": ["Figure 16: Data augmentation for reference images. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "G.3 Training details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We trained the VQGAN for 200,000 iterations with batch size 32 on four A6000 GPUs for 7 days. We trained the LDM with only LQ condition for 500,000 iterations with batch size 40 on four A6000 GPUs for 7 days. We finetuned the ReF-LDM for 150,000 iterations with batch size 8 on four 3090 GPUs for 6 days. For training losses, the LDM is trained using only the typical LDM loss $\\mathcal{L}_{\\mathrm{LDM}}$ , while the ReF-LDM is trained with both $\\mathcal{L}_{\\mathrm{LDM}}$ and the proposed $\\mathcal{L}_{\\mathrm{timeID}}$ . ", "page_idx": 17}, {"type": "text", "text": "For the frozen autoencoder, we use a VQGAN as in the LDM [22] with the following settings: ", "page_idx": 18}, {"type": "text", "text": "\u2022 input image: 512x512x3   \n\u2022 latent representation: 64x64x8   \n\u2022 code booksize: 8192   \n\u2022 network hyperparameters: base channel as 128, multiplier for each scale as [1, 1, 2, 4] with 2 residual blocks. ", "page_idx": 18}, {"type": "text", "text": "For the denoising U-net, we use the following settings: ", "page_idx": 18}, {"type": "text", "text": "\u2022 input latent: 64x64x16   \n\u2022 output latent: 64x64x8   \n\u2022 attention layer at resolutions: 32x32, 16x16, and 8x8   \n\u2022 network hyperparameters: base channel as 160, multiplier for each scale as [1, 2, 2, 4] with 2 residual blocks. ", "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The method is described in Sec. 3 and the ablation studies and evaluation results are provided in Sec. 5. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We discuss the limitation in Sec. 6 and provide visual examples in Appendix F. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: This work does not involve theoretical results. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We provide implementation details in Sec. ?? and Appendix G. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example   \n(a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [No] ", "page_idx": 21}, {"type": "text", "text": "Justification: We plan to release our dataset and model after the paper\u2019s acceptance. They are not included in the current submission because we are waiting formal permission from the associated company. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/ CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We provide details in Sec. ?? and Appendix G. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [No] ", "page_idx": 23}, {"type": "text", "text": "Justification: We do not report error bars, as the training our LDM-based model for multiple times costs too much time and computational resources. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We provide the computer resources for inference in Table. 2 and for training in Appendix G.3. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. \u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. \u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/ EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our FFHQ-Ref dataset in this work is based on an exisiting dataset FFHQ where only images with appropriate licences are collected. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We discuss the broader impacts in Appedix A. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. ", "page_idx": 24}, {"type": "text", "text": "\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: We haven\u2019t release the dataset or models. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. \u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. \u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We properly states that our FFHQ-Ref is constructed based on the FFHQ dataset in Sec 4. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for ", "page_idx": 25}, {"type": "text", "text": "some datasets. Their licensing guide can help determine the license of a dataset. ", "page_idx": 26}, {"type": "text", "text": "\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. \u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: We haven\u2019t release the dataset. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: This work does not contain crowdsourcing experiments. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: This work does not contain crowdsourcing experiments. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. \u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. \u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]