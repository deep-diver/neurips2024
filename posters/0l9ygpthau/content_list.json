[{"type": "text", "text": "Near-Optimal Distributionally Robust Reinforcement Learning with General $L_{p}$ Norms ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Pierre Clavier\\* Laixi Shit Erwan Le Pennec \\* Eric Mazumdar Ecole Polytechnique, Inria Caltech Ecole polytechnique Caltech Adam Wierman Matthieu Geists Caltech Cohere ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "To address the challenges of sim-to-real gap and sample efficiency in reinforcement learning (RL), this work studies distributionally robust Markov decision processes (RMDPs) \u2014 optimize the worst-case performance when the deployed environment is within an uncertainty set around some nominal MDP. Despite recent efforts, the sample complexity of RMDPs has remained largely undetermined. While the statistical implications of distributional robustness in RL have been explored in some specific cases, the generalizability of the existing findings remains unclear, especially in comparison to standard RL. Assuming access to a generative model that samples from the nominal MDP, we examine the sample complexity of RMDPs using a class of generalized $L_{p}$ norms as the 'distance\u2019 function for the uncertainty set, under two commonly adopted $s a$ -rectangular and $s$ -rectangular conditions. Our results imply that RMDPs can be more sample-efficient to solve than standard MDPs using generalized $L_{p}$ norms in both $s a\\cdot$ and $s$ -rectangular cases, potentially inspiring more empirical research. We provide a near-optimal upper bound and a matching minimax lower bound for the $s a$ -rectangular scenarios. For $s$ -rectangular cases, we improve the state-of-the-art upper bound and also derive a lower bound using $L_{\\infty}$ norm that verifies the tightness. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Reinforcement learning (RL) [Sutton, 1988] is a popular paradigm in machine learning, particularly noted for its success in practical applications. The RL framework, usually modeled within the context of a Markov decision process (MDP), focuses on learning effective decision-making strategies based on interactions with a fixed environment. However, the work of Mannor et al. [2004], among others, has highlighted a vulnerability in RL strategies, revealing the sensitivity to inherent shift or estimation errors in the reward and transition probabilities. A specific example of this is when, because of a sim-to-real gap, policies learned in idealized environments fail when deployed in environments with slight changes or adversarial perturbations [Klopp et al., 2017, Mahmood et al., 2018]. ", "page_idx": 0}, {"type": "text", "text": "To address this issue, distributionally robust RL, usually formulated as robust MDPs (RMDPs), proposed by Iyengar [2005] and Nilim and El Ghaoui [2005], have attracted considerable attention. RMDPs are formulated as max-min problems, seeking policies that are resilient to model environment perturbations within a specified uncertainty set. Despite the robustness benefits, solving RMDPs is NP-hard for general uncertainty sets [Nilim and El Ghaoui, 2005]. To overcome this challenge, the rectangularity condition is often adopted so that the uncertainty set can be decomposed as products of independent subsets for each state or state-action pair, denoted as $s$ -rectangular or $s a$ -rectangular assumptions (see Definition 4 and 5). These assumptions facilitate computation traceability of methods such as robust value iteration and robust policy iteration, preserving many structural properties of MDPs [Ho et al., 2021]. The $s$ -rectangularity condition, though with less restrictive structure assumption, impose more challenges for algorithm design, while the $s a$ -rectangularity condition allows for deterministic optimal policies akin to non-robust MDPs [Wiesemann et al., 2013]. Note that dealing with uncertainty in transition kernels is much more difficult than that in rewards [Kumar et al., 2022, Derman et al., 2021]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "The question of sample efficiency is central in RL problems ranging from practice to theory. Although minimax sample efficiency has been achieved for standard MDPs [Azar et al., 2013b, Li et al., 2023c], this goal in general remains open in RMDPs. Specifically, there exists prior work studying the sample complexity of distributionally robust RL for a few specific divergences such as total variation $(T V)$ distance, $\\dot{\\chi}^{2}$ divergence, Kullback-Leibler divergence $(K L)$ divergence, and Wasserstein distance (see discussions in Appendix A) [Yang et al., 2022b, Zhou et al., 2021, Panaganti and Kalathil, 2022]. While such results remain unclear for more general class, such as the general smooth $L_{p}$ norms (see Def. 1). To the best of our knowledge, minimax optimal sample complexity for the full range of uncertainty level has only been achieved for one case \u2014 $T V$ distance [Shi et al., 2023]. In this work, we focus on understanding the sample complexity of RMDPs with a general smooth $L_{p}$ that will be defined in Def. 1. This generalized result is appealing for both practice and theory. In practice, numerous applications are based on optimizations or learning approaches that involve general norms beyond those specific cases that have been studied in prior works. Additionally, optimizing $L_{p}$ norm weighted ambiguity sets for robust MDPs has been proposed in the context of RMDPs in Russel et al. [2019], which justifies our formulation. Theoretically, prior work has characterized the sample complexity of RMDPs for some specific norms have suggested intriguing insights about the statistical implications of distributional robustness in RL. It is interesting to further understand the statistical cost of robust RL in more general scenarios. One area of focus is the contrast between the sample efficiency of solving distributionally robust RL and solving standard RL. In particular, for the specific case of $T V$ distance, Shi et al. [2023] shows that the sample complexity for solving robust RL is at least the same as and sometimes (when the uncertainty level is relatively large) could be smaller than that of standard RL. This motivates the following open question: ", "page_idx": 1}, {"type": "text", "text": "Is distributionally robust RL more sample effcient than standard RL for some general class of norms (Def.(1)) ? ", "page_idx": 1}, {"type": "text", "text": "A second question is about the comparisons between the sample complexity of solving $s$ -rectangular RMDPs and that of solving $s a$ -rectangular RMDPs. Note that $s$ -rectangular RMDPs involve more complex optimization problems with additional variables (uncertainty levels for each action) to optimize. This leads to a richer class of optimal policy candidates\u2014stochastic policies in $s$ -rectangular cases, in contrast to the class of deterministic policies for $s a$ -rectangular cases. In addition, existing sample complexity upper bounds for solving $s$ -rectangular RMDPs are larger than that for solving $s a$ -rectangularity [Yang et al., 2022b] for the investigated cases. This motivates the curious question: ", "page_idx": 1}, {"type": "text", "text": "Doessolving $s$ -rectangular RMDPs require more samples than solving sa-rectangular RMDPs with generalsmooth $L_{p}$ normsdefinedinDef.1? ", "page_idx": 1}, {"type": "text", "text": "Main contributions. In this paper, we address each of the two questions discussed above. In particular, we provide the first sample complexity analysis for RMDPs with general $L_{p}$ norms (cf. Def. 1) under both the $s\\cdot$ and $s a$ -rectangularity conditions. For convenience, we present detailed comparisons between the prior arts and our results in Table 1 for quick reference and discuss the contributions and their implications as below. ", "page_idx": 1}, {"type": "text", "text": "\u00b7 Considering the first question, we illustrate our results in both $s a-$ and $s$ -rectangular cases in Figure 1. In the case of $s a$ -rectangularity, we derive a sample complexity upper bound for RMDPs using general smooth $L_{p}$ norms (cf. Theorem 1) in the order of $\\begin{array}{r}{\\widetilde{O}\\left(\\frac{S A}{(1-\\gamma)^{2}\\operatorname*{max}\\{1-\\gamma,C_{g}\\sigma\\}\\varepsilon^{2}}\\right)}\\end{array}$ Here, $\\sigma$ is the uncertainty level/radius of the uncertainty set, and $C_{g}>0$ is a positive constant related to the geometry of the norm defined in Def. 1. For classical $L_{p}$ norms, $C_{g}\\geq1$ so we can directly relax this constant to 1 to obtain the result in Table 1. In addition, we provide a matching minimax lower bound (cf. Theorem 2) that confirms the near-optimality of the upper bound for almost full range of the uncertainty level. Our results match the near-optimal sample complexity derived in Shi et al. [2023] for the specific case using TV distance, while holding for broader cases using general $L_{p}$ norms. The results rely on a new dual optimization form for $s a$ -rectangular RMDPs and reveal the relationship between the sample complexity and this new dual form \u2014\u2014 the infinite span seminorm (controlled in Lemma 5), which may be of independent interest. ", "page_idx": 1}, {"type": "table", "img_path": "0l9yGPTHAU/tmp/a8f2cac44404391dd65f594e143bb13f45ba4268672704171292c590beedb69f.jpg", "table_caption": ["Table 1: Comparisons with prior results (up to log terms) regarding finding an $\\varepsilon$ optimal policy for the distributionally RMDP, where $\\sigma$ is the radius of the uncertainty set and $\\sigma_{\\mathrm{max}}$ defined in Theorem 1. "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "In the case of $s$ -rectangularity, we provide a sample complexity upper bound for solving RMDPs with general smooth $L_{p}$ norms in the order of $\\begin{array}{r}{\\widetilde{O}\\left(\\frac{S A}{(1-\\gamma)^{2}\\operatorname*{max}\\{1-\\gamma,C_{g}\\operatorname*{min}_{s}\\|\\pi_{s}\\|_{*}\\widetilde{\\sigma}\\}\\varepsilon^{2}}\\right)}\\end{array}$ with $\\left\\|.\\right\\|_{*}$ the dual norm and $\\tilde{\\sigma}$ the radius of the ball in the $s$ -rectangular uncertainty set. This result improves the prior art $\\begin{array}{r}{\\widetilde{O}\\left(\\frac{S A}{(1-\\gamma)^{4}\\varepsilon^{2}}\\right)}\\end{array}$ in Clavieta frasal $L_{p}$ \u2014b least faetor of $\\begin{array}{r}{O\\left(\\frac{1}{1-\\gamma}\\right)}\\end{array}$ when $\\tilde{\\sigma}\\lesssim1-\\gamma$ . Furthermore, we present a lower bound for a representative case with $L_{\\infty}$ norm, which corroborates the tightness of the upper bound. To the best of our knowledge, this is the first lower bound for solving RMDPs with $s$ -rectangularity. ", "page_idx": 2}, {"type": "text", "text": "\u00b7 We highlight the technical contributions as below. For the upper bounds, regarding optimization contribution, we derive new dual optimization problem forms for both $s a-$ and $s-$ rectangular cases (Lemma 3 and 4), which is the foundation of the covering number argument in finite-sample analysis. From a statistical point of view, a new concentration lemma (See Lemma 8 for dual forms) is introduced to obtain a lower sample complexity than standard RL, controlling the infinite span semi norm of the value function, both for $s a-$ and $s-$ rectangular case are derived (See Lemma 5 and 6). For the lower bound, the technical contributions are mainly in $s$ -rectangular cases, which involves entire new challenges compared to $s a$ -rectangularity case: the optimal policies can be stochastic and hard to be characterized as a closed form, compared to the deterministic one in $s a$ -rectangular cases. Therefore, we construct new hard instances for $s$ -rectangular cases that is distinct from those used in $s a$ -rectangular cases or standard RL. ", "page_idx": 2}, {"type": "text", "text": "\u00b7 Considering the second question, as illustrated in Figure 1, our results highlight that robust RL is at least the same as and sometimes can be more sample-efficient to solve than standard RL for general smooth $L_{p}$ norms (cf. Def. 1). This insight is of significant practical importance and serves to provide crucial motivation for the use and study of distributionally robustness in RL. Notably, robust RL does not only reduce the vulnerability of RL policy to estimation errors and sim-to-real gaps, but also leads to better data efficiency. In terms of comparing the statistical implications of saand $s$ - rectangularity, our results show that solving $s$ -rectangular RMDPs is not harder than solving $s a$ -rectangular RMDPs in terms of sample requirement (See Theorem 3 and Figure 2, Right). ", "page_idx": 2}, {"type": "text", "text": "2  Problem Formulation: Robust Markov Decision Processes ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we formulate distributionally robust Markov decision proceses (RMDPs) in the discounted infinite-horizon setting, introduce the sampling mechanism, and describe our goal. ", "page_idx": 2}, {"type": "image", "img_path": "0l9yGPTHAU/tmp/a71928de6ff484e705931a37c185c347de682f62de0d1dd677ebba9b26fb832c.jpg", "img_caption": ["Figure 1: Left: Sample complexity results for RMDPs with sa- and $s$ -rectangularity with $L_{p}$ with comparisons to prior arts [Shi et al., 2023] (for $L_{1}$ norm, or called total variation distance) and [Clavier et al., 2023] ; Right: The data and instance-dependent sample complexity upper bound of solving $s$ -rectangular dependency RMDPs with $L_{p}$ norms. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Standard Markov decision processes (MDPs). A discounted infinite-horizon MDP is represented by $\\mathcal{M}=(\\mathcal{S},\\mathcal{A},\\gamma,P,r)$ , where $S=\\{1,\\cdot\\cdot\\cdot,S\\}$ and $\\mathcal{A}=\\{1,\\cdot\\cdot\\cdot,A\\}$ are the finite state and action spaces, respectively, $\\gamma\\in[0,1)$ is the discounted factor, $P:S\\times A\\to\\Delta(S)$ denotes the probability transition kernel, and $r:\\dot{\\boldsymbol{S}}\\times\\boldsymbol{A}\\rightarrow[0,1]$ is the immediate reward function, which is assumed to be deterministic. Moreover, we assume that the reward function is bounded in $(0,1)$ without loss of generality of the results due to the variance reward invariance. Finally we denote $1_{A}$ or $1_{S}$ the unitary vector of respectively dimension $A$ or $S$ . Moreover, $e_{s}$ is the standard unitary vector supported on $s$ . The policy we are looking for is denoted by $\\pi:S\\to\\Delta(A)$ , which specifies the probability of action selection over the action space in any state. Note that if the policy is deterministic in the $s a$ -rectangular case, we overload the notation and refer to $\\pi(s)$ as the action selected by the policy $\\pi$ in state $s$ Finally, to characterize the cumulative reward, the value function $V^{\\pi,P}$ for any policy $\\pi$ under the transition kernel $P$ is defined by $\\forall s\\in S$ ", "page_idx": 3}, {"type": "equation", "text": "$$\nV^{\\pi,P}(s):=\\mathbb{E}_{\\pi,P}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}r\\left(s_{t},a_{t}\\right)\\Big|\\,s_{0}=s\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The expectation is taken over the randomness of the trajectory $\\{s_{t},a_{t}\\}_{t=0}^{\\infty}$ generated by executing the policy $\\pi$ under the transition kernel $P$ , such that $a_{t}\\sim\\pi(\\cdot\\,|\\,s_{t})$ and $\\bar{s_{t+1}}\\sim P(\\cdot\\,|\\,s_{t},a_{t})$ for all $t\\geq0$ In the same way, the Q function $Q^{\\pi,P}$ associated with any policy $\\pi$ under the transition kernel $P$ is defined using expectation taken over the randomness of the trajectory under policy $\\pi$ as ", "page_idx": 3}, {"type": "equation", "text": "$$\nQ^{\\pi,P}(s,a):=\\mathbb{E}_{\\pi,P}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}r\\bigl(s_{t},a_{t}\\bigr)\\Bigm|s_{0},a_{0}=s,a\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Distributionally robust MDPs. We consider distributionally robust MDPs (RMDPs) in the discounted infinite-horizon setting, denoted by $\\mathcal{M}_{\\sf r o b}=\\{S,\\dot{A},\\gamma,\\mathcal{U}_{\\|\\cdot\\|}^{\\sigma}(P^{0}),r\\}$ , where $\\mathcal{S},\\mathcal{A},\\gamma,r$ are the same sets and parameters as in standard MDPs. The main difference compared to standard MDPs is that instead of assuming a fixed transition kernel $P$ , it allows the transition kernel to be arbitrarily chosen from a prescribed uncertainty set $\\mathcal{U}_{\\parallel\\cdot\\parallel}^{\\sigma}(P^{0})$ centered around a nominal kernel $P^{0}:\\mathcal{S}\\times\\mathcal{A}\\to\\Delta(S)$ where the uncertainty set is specified using some called $L_{p}$ smooth norm denoted $\\lVert\\cdot\\rVert$ defined in of radius $\\sigma>0$ defined in 1. ", "page_idx": 3}, {"type": "text", "text": "Definition 1 (General smooth $L_{p}$ norms and dual norms). A norm $\\|\\cdot\\|$ is said to be a general smooth $L_{p}$ norm, $p>1$ if ", "page_idx": 3}, {"type": "text", "text": "\u00b7 for all $x\\,\\in\\,\\mathbb{R}^{n}$ $\\begin{array}{r}{\\|x\\|\\,:=\\,\\|x\\|_{p,w}\\,=\\,(\\sum_{k=1}^{n}w_{k}(|x_{k}|)^{p})^{1/p}}\\end{array}$ for some $w\\ \\in\\ \\mathbb{R}_{+}^{n}$ , being an arbitrarypositivevector. \u00b7it is twice continuously differentiable Rudin et al. [1964] with the supremum of the Hessian Matrix over the simplex $\\tilde{C}_{S}=\\operatorname*{sup}_{x\\in\\Delta_{S}}\\left\\|\\nabla^{2}\\left\\|x\\right\\|\\right\\|_{2}$ where $\\left\\|\\cdot\\right\\|_{2}$ here is the spectral norm. ", "page_idx": 3}, {"type": "text", "text": "Finally, we denote the dual norm of $\\left\\Vert\\cdot\\right\\Vert$ $\\begin{array}{r}{\\|\\;a s\\;\\|\\!\\cdot\\!\\|_{*}\\;s.t.\\;\\;\\|y\\|_{*}:=\\operatorname*{max}_{x}x^{T}y:\\|x\\|\\le1}\\end{array}$ Moreover, for any metric $\\left\\Vert\\cdot\\right\\Vert$ we define $C_{g}\\;C_{g}:=1/\\operatorname*{min}_{s}\\left\\|e_{s}\\right\\|$ where $\\boldsymbol{e}_{s}\\in\\mathbb{R}^{S}$ is the standard basis vector with only 1 at the $s$ -th entry, otherwise 0. ", "page_idx": 4}, {"type": "text", "text": "Note that the quantity $C_{S}$ exists, as the Hessian of a $C^{2}$ functional is continuous and because the simplex is a compact set, so by Extreme Value Theorem Rudin et al. [1964], $C_{S}$ is finite. For example, considering $L_{p}$ norms with any $p\\geq2$ $C_{S}$ is bounded by $(p-1)S^{1/q}$ .(See (154))This definition is general and includes $L_{p}$ norms [Rudin et al., 1964] for any $p\\geq2$ and all rescaled and weighted norms. Moreover, we could extend our results to a larger set than the one of the norms defined in Def. 1, where the further discussion can be found in Appendix B. However, it does not include divergences such as $K L$ and $\\chi^{2}$ . Not that the case of $T V$ which is not $C^{2}$ smooth is treated independently with different arguments in the proof but has the same sample complexity. In particular, given the nominal transition kernel $P^{0}$ and some uncertainty level $\\sigma$ , the uncertainty set with arbitrary smooth $L_{p}$ norm metric $\\|\\mathbf{\\psi}\\|:\\mathbb{R}^{S}\\times\\to\\mathbb{R}^{+}$ in $s a$ rectangular case or from $\\mathbb{R}^{S\\times\\tilde{A}}$ in the $s$ rectangular case, is speid s $\\mathcal{U}_{||\\cdot||}^{\\sigma}(P^{0}):=\\otimes_{s,a}\\mathcal{U}_{||\\cdot||}^{\\mathtt{s a},\\sigma}(P_{s,a}^{0})$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{U}_{\\parallel\\cdot\\parallel}^{\\mathsf{s a},\\sigma}(P_{s,a}^{0}):=\\left\\{P_{s,a}\\in\\Delta(S):\\left\\|P_{s,a}-P_{s,a}^{0}\\right\\|\\leq\\sigma\\right\\},}\\\\ &{P_{s,a}:=P(\\cdot\\,|\\,s,a)\\in\\mathbb{R}^{1\\times S},P_{s,a}^{0}:=P^{0}(\\cdot\\,|\\,s,a)\\in\\mathbb{R}^{1\\times S},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where we denote a vector of the transition kernel $P$ or $P^{0}$ at state-action pair $(s,a)$ . In other words, the uncertainty is imposed in a decoupled manner for each state-action pair, obeying the so-called $s a$ -rectangularity [Zhou et al., 2021, Wiesemann et al., 2013]. More generally, we define $s$ rectangularMDPs as $\\mathcal{U}_{||\\cdot||}^{\\sigma}(P)=\\otimes_{s}\\mathcal{U}_{||\\cdot||}^{s,\\widetilde{\\sigma}}(P_{s})$ forthe general smoth $L_{p}$ norm $\\left\\Vert\\cdot\\right\\Vert$ . The uncertainty is imposed in a decoupled manner for each state pair, and a fixed budget given a state for all action is defined. To get a similar meaning for the radius of the ball between $s a$ -rectangular and $s$ -rectangular assumptions, we need to rescale the radius depending on the norm like in Yang et al. [2022b]. The $s$ - uncertainty set is then defined using the rescaled radius $\\tilde{\\sigma}$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{U}_{\\|\\cdot\\|}^{s,\\widetilde{\\sigma}}(P_{s}):=\\Big\\{P_{s}^{\\prime}\\in\\Delta(S)^{A}:\\|P_{s}^{\\prime}-P_{s}\\|\\le\\widetilde{\\sigma}=\\sigma\\,\\|1_{A}\\|\\Big\\},}\\\\ &{P_{s}:=P(\\cdot,\\cdot\\,|\\,s)\\in\\mathbb{R}^{1\\times S A},\\quad P_{s}^{0}:=P^{0}(\\cdot,\\cdot\\,|\\,s)\\in\\mathbb{R}^{1\\times S A}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $1_{A}\\in\\mathbb{R}^{A}$ denotes the unitary vector. For the specific case of respectively $L_{1},L_{p}$ and $L_{\\infty}$ norm, $\\tilde{\\sigma}$ is equal to $|\\sigma\\mathcal{A}|,\\sigma|\\mathcal{A}|^{1/p}$ and $\\sigma$ . Note that this scaling allows for a fair comparison between $s a$ and $s$ -rectangular MDPs. In RMDPs, we are interested in the worst-case performance of a policy $\\pi$ over all the possible transition kernels in the uncertainty set. This is measured by the robust value function $V^{\\pi,\\sigma}$ and the robust $Q$ -function $Q^{\\pi,\\sigma}$ in $\\mathcal{M}_{\\sf r o b}$ , defined respectively as $\\forall(s,a)\\in S\\times A$ ", "page_idx": 4}, {"type": "equation", "text": "$$\nV^{\\pi,\\sigma}(s):=\\operatorname*{inf}_{P\\in{\\mathcal U}_{\\|\\cdot\\|}^{s_{0,\\sigma}}(P^{0})}V^{\\pi,P}(s),\\quad Q^{\\pi,\\sigma}(s,a):=\\operatorname*{inf}_{P\\in{\\mathcal U}_{\\|\\cdot\\|}^{s_{0,\\sigma}}(P^{0})}Q^{\\pi,P}(s,a).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Similarly for $s$ -rectangularty, the alue function is denoted $V_{s}^{\\pi,\\sigma}(s):=\\operatorname*{inf}_{P\\in\\mathcal{U}_{\\parallel\\cdot\\parallel}^{s,\\tilde{\\sigma}}(P^{0})}V^{\\pi,P}(s).$ ", "page_idx": 4}, {"type": "text", "text": "Optimal robust policy and robust Bellman operator. As a generalization of properties of standard MDPs in the $s a$ -rectangular robust case, it is well-known that there exists at least one deterministic policy that maximizes the robust value function (resp. robust Q-function) simultaneously for all states (resp. state-action pairs) [Iyengar, 2005, Nilim and E1 Ghaoui, 2005] but not in the $s$ -rectangular case. Therefore, we denote the optimal robust value function (resp. optimal robust $Q$ -function) as $V^{\\star,\\sigma}$ (resp. $Q^{\\star,\\sigma}$ ), and the optimal robust policy as $\\pi^{\\star}$ , which satisfy $\\bar{\\forall}(s,a)\\in S\\times A$ ", "page_idx": 4}, {"type": "equation", "text": "$$\nV^{\\star,\\sigma}(s):=V^{\\pi^{\\star},\\sigma}(s)=\\operatorname*{max}_{\\pi}V^{\\pi,\\sigma}(s),\\quad Q^{\\star,\\sigma}(s,a):=Q^{\\pi^{\\star},\\sigma}(s,a)=\\operatorname*{max}_{\\pi}Q^{\\pi,\\sigma}(s,a).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "A key concept in RMDPs is a generalization of Bellman's optimality principle, encapsulated in the following robust Bellman consistency equation (resp. robust Bellman optimality equation): ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall(s,a)\\in\\mathcal{S}\\times\\mathcal{A},\\ \\ Q^{\\pi,\\sigma}(s,a)=r(s,a)+\\gamma\\operatorname*{inf}_{\\mathcal{P}\\in\\mathcal{U}_{\\parallel\\cdot\\parallel}^{s,\\sigma}(P_{s,a}^{0})}\\mathcal{P}V^{\\pi,\\sigma},}\\\\ &{\\forall(s,a)\\in\\mathcal{S}\\times\\mathcal{A},\\ \\ \\ Q^{\\star,\\sigma}(s,a)=r(s,a)+\\gamma\\operatorname*{inf}_{\\mathcal{P}\\in\\mathcal{U}_{\\parallel\\cdot\\parallel}^{s,a}(P_{s,a}^{0})}\\mathcal{P}V^{\\star,\\sigma}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "for the $s a$ rectangularcase and same equation replacing $P_{s,a}^{0}$ by $P_{s}^{0}$ and $\\sigma$ by $\\tilde{\\sigma}$ The robust Bellman operator [Iyengar, 2005, Nilim and El Ghaoui, 2005] is denoted by $\\mathcal{T}^{\\sigma}(\\cdot):\\mathbb{R}^{S A}\\rightarrow\\mathbb{R}^{S A}$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{T}^{\\sigma}(Q^{\\pi})(s,a):=r(s,a)+\\gamma\\operatorname*{inf}_{\\mathcal{P}\\in\\mathcal{U}_{\\parallel\\cdot\\parallel}^{\\operatorname*{sa},\\sigma}(P_{s,a}^{0})}\\mathcal{P}V,\\quad\\mathrm{with}\\quad V(s):=\\operatorname*{max}_{\\pi}Q^{\\pi}(s,a)\\cdot\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for $s a$ -rectangular MDPs. Given that $Q^{\\star,\\sigma}$ is the unique-fixed point of $\\mathcal{T}^{\\sigma}$ one can recover the optimal robust value function and $\\mathrm{^Q}$ -function using a procedure termed distributionally robust value iteration $(D R V I)$ . Generalizing the standard value iteration, $D R V I$ starts from some given initialization and recursively applies the robust Bellman operator until convergence. As has been shown previously, this procedure converges rapidly due to the $\\gamma$ -contraction property of $\\mathcal{T}^{\\sigma}$ with respect to the $L_{\\infty}$ norm [Iyengar, 2005, Nilim and E1 Ghaoui, 2005]. ", "page_idx": 5}, {"type": "text", "text": "3  Distributionally Robust Value Iteration ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Generative model-based sampling. Following Zhou et al. [2021], Panaganti and Kalathil [2022], we assume access to a generative model or a simulator [Kearns and Singh, 1999], which allows us tocollect $N$ independent samples for each state-action pair generated based on the nominal kernel $P^{0}{\\colon}\\forall(s,a)\\in S\\times A$ $s_{i,s,a}\\stackrel{i.i.d}{\\sim}P^{0}(\\cdot\\,|\\,s,a),\\quad i=1,2,\\cdot\\,\\cdot\\,,N.$ The total sample size is, therefore, $N S A$ . We consider a model-based approach tailored to RMDPs, which first constructs an empirical nominal transition kernel based on the collected samples and then applies distributionally robust value iteration (DRVI) to compute an optimal robust policy. As we decouple the statistical estimation error and the optimization error, we exhibit an algorithm that can achieve arbitrary small error $\\epsilon_{o p t}$ in the empirical MDP defined as an empirical nominal transition kernel P0 E R\\$Ax S that can be constructed on the basis of the empirical frequency of state transitions, i.e. $\\forall(s,a)\\in S\\times A$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widehat{P}^{0}(s^{\\prime}\\,|\\,s,a):=\\frac{1}{N}\\sum_{i=1}^{N}\\mathbb{1}\\big\\{s_{i,s,a}=s^{\\prime}\\big\\},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "which leads to an empirical RMDP $\\widehat{\\mathcal{M}}_{\\sf r o b}=\\{S,\\mathcal{A},\\gamma,\\mathcal{U}_{||\\cdot||}^{\\sigma}(\\widehat{P}^{0}),r\\}$ . Analogously, we can define the corresponding robust value function (resp. robust $Q\\cdot$ function) of policy $\\pi$ in $\\widehat{\\mathcal{M}}_{\\sf r o b}$ as $\\widehat{V}^{\\pi,\\sigma}$ (resp. $\\widehat{Q}^{\\pi,\\bar{\\sigma}}.$ (cf. (8). In addition, we denote the corresponding optimal robust policy as $\\widehat{\\pi}^{\\star}$ and the optimal robust value function (resp. optimal robust $Q$ -function) as $\\widehat{V}^{\\star,\\sigma}$ (resp. $\\widehat{Q}^{\\star,\\sigma})$ (cf. (9), which satisfies the robust Bellman optimality equation $\\forall(s,a)\\in S\\times A$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widehat{Q}^{\\star,\\sigma}(s,a)=r(s,a)+\\gamma\\operatorname*{inf}_{\\substack{\\mathcal{P}\\in\\mathcal{U}_{\\|\\cdot\\|}^{s_{0},\\sigma}(\\widehat{P}_{s,a}^{0})}}\\mathcal{P}\\widehat{V}^{\\star,\\sigma}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Equipped with $\\widehat{P}^{0}$ , we can define the empirical robust Bellman operator T\u00b0 as $\\forall(s,a)\\in S\\times A$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widehat{\\mathcal{T}}^{\\sigma}(Q^{\\pi})(s,a):=r(s,a)+\\gamma\\operatorname*{inf}_{\\mathcal{P}\\in\\mathcal{U}_{||\\cdot||}^{\\mathrm{sa},\\sigma}(\\widehat{P}_{s,a}^{0})}\\mathcal{P}V,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with $V(s)\\,:=\\,\\operatorname*{max}_{\\pi}Q^{\\pi}(s,a)$ . The aim of this work is given the collected samples, to learn the robust optimal policy for the RMDP w.r.t. some prescribed uncertainty set $\\mathcal{U}^{\\sigma}(P^{0})$ around the nominal kernel using as few samples as possible. Specifically, given some target accuracy level $\\varepsilon>0$ the goal is to seek an $\\varepsilon$ -optimal robust policy $\\widehat{\\pi}$ obeying ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall s\\in S:\\quad V^{\\star,\\sigma}(s)-V^{\\widehat\\pi,\\sigma}(s)\\leq\\varepsilon,}\\\\ &{\\widehat{V}^{\\widehat\\pi^{\\star},\\sigma}-\\widehat{V}^{\\widehat\\pi,\\sigma}\\leq\\varepsilon_{\\sf o p t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This formulation allows plugging any solver of RMDPs in this bound, for instance, the distributionally robust value iteration (DRVI) algorithm detailed in Appendix G. ", "page_idx": 5}, {"type": "text", "text": "4  Theoretical guarantees ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we present our main results characterizing the sample complexity of solving RMDPs With $s a$ -and $s$ -rectangularity. Additionally, we discuss the implications of our results for the comparisons between standard and robust RL, and for comparisons between $s a$ versus $s$ -rectangularity. ", "page_idx": 5}, {"type": "text", "text": "4.1 sa-rectangular uncertainty set with general smooth norms ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To begin, we consider the RMDPs with $s a$ -rectangularity with general norms. We first provide the following sample complexity upper bound for certain oracle planning algorithms, whose proof is postponed to Appendix D.2. Technically, we derive two new dual forms for RMDPs problems using arbitrary norms in Lemmas 3 and 4 for respectively $s a\\cdot$ and $s$ -rectangular RMDPS. In these dual forms, a central quantity denoted $\\mathrm{sp}(.)_{*}$ , representing the dispersion of the value function, appears and is the dual span semi-norm associated with the considered general $L_{p}$ norm $\\lVert.\\rVert$ defined in 1 in the initial primal problem. The main challenge in this analysis is to derive a tight upper bound on this quantity in Lemmas (5) and (6), leading to the following sample complexity. ", "page_idx": 6}, {"type": "text", "text": "Theorem 1 (Upper bound for $s a$ -rectangularity). Consider the uncertainty set $\\mathcal{U}_{\\parallel\\cdot\\parallel}^{\\mathsf{s a},\\sigma}(\\cdot)$ associated with arbitrary $L_{p}$ smooth norm $\\Vert\\cdot\\Vert$ defined in $^{\\,l}$ We denote $\\begin{array}{r}{\\sigma_{\\operatorname*{max}}:=\\operatorname*{max}_{p_{1},p_{2}\\in\\Delta(S)}\\left\\|p_{1}-p_{2}\\right\\|a s}\\end{array}$ the accessible maximal uncertainty level. Consider any $\\delta\\in(0,1)$ , discount factor $\\gamma\\in[\\frac14,1)$ , and uncertainty level $\\sigma\\in(0,\\sigma_{\\mathrm{max}}]$ . Let $\\widehat{\\pi}$ be the output policy of some oracle planning algorithm with optimization error $\\varepsilon_{\\mathsf{o p t}}$ introduced in (15). With introduced in $^{\\,l}$ onehaswith probability at least $1-\\delta$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\forall s\\in{\\mathcal{S}}:\\quad V^{\\star,\\sigma}(s)-V^{\\widehat{\\pi},\\sigma}(s)\\leq\\varepsilon+\\frac{8\\varepsilon_{\\sf o p t}}{1-\\gamma}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "for any $\\varepsilon\\in(0,\\sqrt{1/\\operatorname*{max}\\{1-\\gamma,\\sigma C_{g}\\}}]$ , as long as the total number of samples obeys ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\index{N S A}\\gtrsim\\frac{c_{1}S A}{(1-\\gamma)^{2}\\operatorname*{max}\\{1-\\gamma,C_{g}\\sigma\\}\\varepsilon^{2}}+\\frac{c_{2}S A C_{S}\\left\\lVert1_{S}\\right\\rVert_{*}}{(1-\\gamma)^{2}\\epsilon}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "with $c_{1},c_{2},c_{3}\\;\\;\\;a$ universal positive constant. For a sufficiently small level of accuracy $\\epsilon\\le(\\operatorname*{max}\\{1-\\gamma,C_{g}\\sigma\\})/(C_{S}\\left\\|1_{S}\\right\\|)$ ,thesamplecomplexityis ", "page_idx": 6}, {"type": "equation", "text": "$$\nN S A\\gtrsim\\frac{c_{3}S A}{(1-\\gamma)^{2}\\operatorname*{max}\\{1-\\gamma,C_{g}\\sigma\\}\\varepsilon^{2}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Note that this result is also true for $T V$ without the geometric smooth term depending on $C_{S}$ Considering $L_{p}$ norms, $C_{g}\\geq1$ and $C_{S}\\leq S^{1/q}(p-1)$ . In Theorem 1, we introduce the following minimax-optimal lower bound to verify the tightness of the above upper bound; a proof is provided in Appendix $\\boldsymbol{\\mathrm E}$ ", "page_idx": 6}, {"type": "text", "text": "Theorem 2 (Lower bound for $s a$ -rectangularity). Consider the uncertainty set $\\mathcal{U}_{\\parallel\\cdot\\parallel}^{\\mathtt{S a},\\sigma}(\\cdot)$ associated with arbitrary $L_{P}$ norm $\\|\\cdot\\|$ defined in 1. We denote $\\begin{array}{r}{\\sigma_{\\mathrm{max}}~:=~\\operatorname*{max}_{p_{1},q_{1}\\in\\Delta(S)}\\left\\|p_{1}~-~p_{2}\\right\\|}\\end{array}$ as the accessible maximal uncertainty level.Consider any tuple $(S,A,\\gamma,\\sigma,\\varepsilon)$ ,where $\\gamma\\ \\in\\ [\\frac{1}{2},1)$ \uff0c $\\sigma\\,\\in\\,(0,\\sigma_{\\operatorname*{max}}(1\\,-\\,c_{0})\\big]$ with $\\begin{array}{r}{0\\ <\\ c_{0}\\ \\leq\\ \\frac{1}{8}}\\end{array}$ being any small enough positive constant, and $\\varepsilon\\ \\in$ $\\left(0,{\\frac{c_{0}}{256(1-\\gamma)}}\\right]$ . We can construct two infinite-horizon RMDPs $\\mathcal{M}_{0},\\mathcal{M}_{1}$ such that giving a dataset with $N$ independent samples for each state-action pair over the nominal transition kernel (for either $\\mathcal{M}_{0}$ or $\\mathcal{M}_{1}$ respectively), one has ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\hat{\\pi}}\\operatorname*{max}_{\\mathcal{M}\\in\\{\\mathcal{M}_{0},\\mathcal{M}_{1}\\}}\\left\\{\\mathbb{P}_{\\mathcal{M}}\\Big(\\operatorname*{max}_{s\\in\\mathcal{S}}\\big[V^{\\star,\\sigma}(s)-V^{\\widehat{\\pi},\\sigma}(s)\\big]>\\varepsilon\\Big)\\right\\}\\geq\\frac{1}{8},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where the infimum is taken over all estimators $\\widehat{\\pi}$ $\\mathbb{P}_{0}$ (resp. $\\mathbb{P}_{1}$ ) are the probability when the RMDP is $\\mathcal{M}_{0}$ (resp. ${\\mathcal{M}}_{1},$ , as long as, for $c_{7}$ is a universal positive constant, ", "page_idx": 6}, {"type": "equation", "text": "$$\nN S A\\leq\\frac{c_{7}S A}{(1-\\gamma)^{2}\\operatorname*{max}\\{1-\\gamma,C_{g}\\sigma\\}\\varepsilon^{2}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "\u00b7 Near minimax-optimal sample complexity with general $L_{p}$ norms. _ Recall that Theorem 1 shows that the sample complexity upper bound of oracle algorithms for RMDPs is in the order of $\\begin{array}{r}{\\widetilde{O}\\left(\\frac{S A}{(1-\\gamma)^{2}\\operatorname*{max}\\{1-\\gamma,C_{g}\\sigma\\}\\varepsilon^{2}}\\right)}\\end{array}$ . Combined with the lower bound in Theorem 2, we observe that the above sample complexity is near minimax-optimal, in almost the full range of uncertainty. ", "page_idx": 6}, {"type": "text", "text": "\u00b7 Solving RMDPs with general $L_{p}$ norms can be easier than solving standard RL. Recall that the sample complexity of solving standard RL with a generative model [Agarwal et al., 2020, Li et al., 2024, Azar et al, 2013al is: $\\begin{array}{r}{\\widetilde{O}\\left(\\frac{S A}{(1-\\gamma)^{3}\\varepsilon^{2}}\\right).}\\end{array}$ Comparing this with the sample complexity in (18), it highlights that solving robust MDPs (cf. (18)) using any norm as the divergence function for the uncertainty set is not harder than (and is sometimes easier than) solving standard RL (cf. (4.1)). Specifically, when the uncertainty level is small $\\sigma\\lesssim1-\\gamma$ , the sample complexity of solving robust MDPs matches that of standard MDPs. While when the uncertainty level is relatively larger $1-\\gamma\\lesssim\\sigma\\leq\\sigma_{\\mathrm{max}}$ , the sample complexity of solving robust MDPs is smaller than that of standard MDPs by factoror $\\frac{\\sigma}{1-\\gamma}$ which goes to $\\frac{1}{1-\\gamma}$ When $\\sigma=O(1)$ ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "\u00b7 Comparisons with prior arts. In Figure 1, we illustrate the comparisons with two state-of-thearts [Clavier et al., 2023, Shi et al., 2023] which use some divergence functions belonging to the class of general norms considered in this work. In particular, Shi et al. [2023] achieved the state-of-the-art minimax-optimal sample complexity $\\begin{array}{r}{\\widetilde{O}\\left(\\frac{S A}{(1-\\gamma)^{2}\\operatorname*{max}\\{1-\\gamma,\\sigma\\}\\varepsilon^{2}}\\right)}\\end{array}$ for specific $L_{1}$ norm (or called total variation distance). In this work, we attain near minimax-optimal sample complexity for any general norm (including $L_{1}$ ) which matches the one in Shi et al. [2023] when narrowing down to $L_{1}$ norm. Note that in $T V$ case, $C_{g}\\,=\\,1$ . This reveals that the finding of robust MDPs can be easier than standard MDPs [Shi et al., 2023] in terms of sample requirement does not only hold for $L_{1}$ norm, but for any general norm. In addition, compared to Clavier et al. [2023] which focuses on $L_{p}$ norms foraiy $1\\leq p\\leq\\infty$ $1-\\gamma\\lesssim\\sigma\\leq\\sigma_{\\mathrm{max}}$ $\\widetilde{\\cal O}(\\frac{S A}{(1\\!-\\!\\gamma)^{4}\\varepsilon^{2}})$ 0 $\\begin{array}{r}{\\widetilde{O}(\\frac{S A}{(1-\\gamma)^{2}\\sigma\\varepsilon^{2}})}\\end{array}$ by at easta faetorof $\\frac{1}{1-\\gamma}$ ", "page_idx": 7}, {"type": "text", "text": "\u00b7 Burn-in Condition, $C_{g}$ factor and $T V$ case :  In Th. 1 and 3 we need a sufficiently small level of accuracy $\\epsilon\\,\\le\\,(\\operatorname*{max}\\{\\bar{1}-\\gamma,C_{g}\\sigma\\})/(C_{s}\\,\\|1_{S}\\|)$ , to obtain the sample complexity. This type of condition is usual in MDPS analysis Shi et al. [2022] and is equivalent to burn in term. Moreover, the quantity $C_{S}$ exists (see 1) and for example, considering $L_{p}$ norms, $C_{S}$ is bounded by $S^{1/q}$ .(See (154) and the product $C_{S}\\parallel1_{S}\\parallel$ is upper bounded by $S$ for $L_{2}$ norm. Moreover, note that our theorem for the smooth norm is also true for $T V$ which is not $C^{2}$ and has the same complexity as (Shi et al. [2023]. In this case, the burn-in condition is not needed. (See Lemma D.3.3). Finally, the factor $C_{g}=1/\\operatorname*{min}_{s}\\left\\|e_{s}\\right\\|$ is norm dependent and depends on how big the vector $e_{s_{0}}$ is in the considered norm. Note for classical $L_{p}$ this quantity is bigger than 1, which reduces the sample complexity. ", "page_idx": 7}, {"type": "text", "text": "4.2 $s$ -rectangular uncertainty set with general norms ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To continue, we move on to the case when the uncertainty set is constructed under $s$ -rectangularity smooth norm. The following theorem presents the sample complexity upper bound for learning an $\\epsilon$ -optimal policy for RMDPs with $s$ -rectangularity. A proof is shown in Appendix D.2. ", "page_idx": 7}, {"type": "text", "text": "Theorem 3 (Upper bound for $s$ -rectangularity). Consider the uncertainty set $\\mathcal{U}_{\\parallel\\cdot\\parallel}^{\\mathsf{s},\\widetilde{\\sigma}}(\\cdot)$ with $s$ -rectangularity. Consider any discount factor $\\gamma\\in[\\frac14,1)$ ,the rescaled uncertainty level $\\tilde{\\sigma}=\\sigma\\left\\lVert\\boldsymbol{1}_{A}\\right\\rVert$ and denote $\\tilde{\\sigma}_{\\mathrm{max}}:=\\|1_{A}\\|\\operatorname*{max}_{p_{1},p_{2}\\in\\Delta(S)}\\|p_{1}-p_{2}\\|$ and $\\delta\\in(0,1)$ . Let $\\widehat{\\pi}$ be the output policy of an arbitrary optimization algorithm with error $\\varepsilon_{\\mathsf{o p t}}$ \uff0c with probability at least $1-\\delta$ . one has for any $\\varepsilon\\,\\in\\,(0,\\sqrt{1/\\operatorname*{max}\\{1-\\gamma,C_{g}\\operatorname*{min}_{s}\\left\\|\\pi_{s}\\right\\|_{*}\\sigma\\}}]$ \uff0c $\\forall s\\in S$ $\\begin{array}{r l}{S:\\;}&{{}V^{\\star,\\widetilde{\\sigma}}(s)-V^{\\widehat{\\pi},\\widetilde{\\sigma}}(s)\\leq\\varepsilon+\\frac{8\\varepsilon_{\\mathrm{opt}}}{1-\\gamma}}\\end{array}$ as long as thetotal number of samples obeys ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\nabla S A\\gtrsim\\frac{c_{4}S A}{(1-\\gamma)^{2}\\varepsilon^{2}}\\operatorname*{min}\\Biggl\\{\\frac{1}{\\operatorname*{max}\\{1-\\gamma,C_{g}\\sigma\\}},\\frac{1}{\\sigma C_{g}\\operatorname*{min}\\{\\|\\pi_{s}^{*}\\|_{*}\\|1_{A}\\|,\\|\\hat{\\pi}_{s}\\|_{*}\\|1_{A}\\|\\}}\\Biggr\\}+\\frac{c_{5}S A C_{S}\\left\\|1_{S}\\right\\|_{*}}{(1-\\gamma)^{2}\\epsilon}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "For a suffciently small accuracy, $\\epsilon\\le(\\operatorname*{max}\\{1-\\gamma,C_{g}\\tilde{\\sigma}\\})/(C_{s}\\,\\|1_{S}\\|)$ the sample complexity is ", "page_idx": 7}, {"type": "equation", "text": "$$\nN S A\\gtrsim\\frac{c_{6}S A}{(1-\\gamma)^{2}\\varepsilon^{2}}\\operatorname*{min}\\left\\{\\frac{1}{\\operatorname*{max}\\{1-\\gamma,C_{g}\\sigma\\}},\\frac{1}{\\sigma C_{g}\\operatorname*{min}_{s\\in S}\\left\\{\\|\\pi_{s}^{*}\\|_{*}\\left\\|1_{A}\\right\\|,\\|\\hat{\\pi}_{s}\\|_{*}\\left\\|1_{A}\\right\\|\\right\\}}\\right\\}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\hat{\\pi}_{s}\\in\\Delta_{A}$ denote the policy of the empirical RMPDs at state $s$ $\\pi_{s}^{*}\\in\\Delta_{A}$ the optimal policy given $s$ of the true RMPDs, $\\left\\|.\\right\\|_{*}$ the dual norm and $c_{4},c_{5},c_{6}$ are universal constant. Note that this result is also true for $T V$ without the term depending on smoothness $C_{S}$ . In addition, we provide the lower bounds for a representative divergence function \u2014 $L_{\\infty}$ norm in the following. Note that for classical $L_{p},C_{S}=S^{\\bar{1/q}}(p\\!-\\!1)$ and $C_{g}$ can be lower bounded by 1. A proof is provided in Appendix F. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4 (Lower bound for $s$ -rectangularity). Consider the uncertainty set $\\mathcal{U}_{L_{\\infty}}^{\\mathsf{s},\\widetilde{\\sigma}}(\\cdot)$ associated with the $L_{\\infty}$ norm. Consider any tuple $(S,A,\\gamma,\\sigma,\\varepsilon)$ and $\\begin{array}{r}{0<c_{0}\\le\\frac{1}{8}}\\end{array}$ being any small enough positive constant, where $\\gamma\\ \\in\\ [\\frac{1}{2},1)$ and $\\begin{array}{r}{\\varepsilon\\,\\in\\,\\left(0,\\frac{c_{0}}{256(1-\\gamma)}\\right]}\\end{array}$ .Corespondingly, we denote the accessible maximal uncertainty level for $\\mathcal{U}_{L_{\\infty}}^{\\mathsf{s},\\widetilde{\\sigma}}(\\cdot)$ as $\\begin{array}{r}{\\sigma_{\\mathrm{max}}^{\\infty}:=\\operatorname*{max}_{p_{1},p_{1}\\in\\Delta(S)^{A}}\\|p_{1}-p_{2}\\|_{\\infty}=1}\\end{array}$ . Then we can construct a collection of infinite-horizon RMDPs $\\mathcal{M}_{L_{\\infty}}$ defined by the uncertainty set with $\\mathcal{U}_{L_{\\infty}}^{\\mathsf{s},\\widetilde{\\sigma}}(\\cdot)$ so that for any $\\sigma\\in(0,\\sigma_{\\operatorname*{max}}^{\\infty}(1-c_{0})]$ . and any dataset with in total $N_{\\mathsf{a l l}}$ independent samples for all state-action pairs over the nominal transition kernel (for any RMDP inside $\\mathcal{M}_{L_{\\infty}}$ ), one has ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\widehat{\\pi}}\\operatorname*{max}_{{\\mathcal{M}}\\in{\\mathcal{M}}_{L_{\\infty}}}\\left\\{\\mathbb{P}_{{\\mathcal{M}}}\\big(\\operatorname*{max}_{s\\in{\\mathcal{S}}}\\big[V^{\\star,\\sigma}(s)-V^{\\widehat{\\pi},\\sigma}(s)\\big]>\\varepsilon\\big)\\right\\}\\geq\\frac{1}{8},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "provided that for $c_{8}$ is a universal positive constant, ", "page_idx": 8}, {"type": "equation", "text": "$$\nN_{\\sf a l l}\\le\\frac{c_{8}S A}{(1-\\gamma)^{2}\\operatorname*{max}\\{1-\\gamma,\\tilde{\\sigma}\\}\\varepsilon^{2}},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "with $\\mathbb{P}_{\\mathcal{M}}$ the probability when the RMDP is $\\mathcal{M}$ . and the infimum is taken over all estimators $\\widehat{\\pi}$ ", "page_idx": 8}, {"type": "text", "text": "Now we can present some implications of Theorem 3 and Theorem 4. ", "page_idx": 8}, {"type": "text", "text": "\u00b7 Robust MDPs with $s$ -rectangularity are at least as easy as $s a$ -rectangularity. Theorem 3 shows that the sample complexity of solving RMDPs with $s$ -rectangularity does not exceed the order of ((--)- max(t-,Co)e2 \uff09 This matches the sample complexity for sa-rectangularity (cf. (18)) and indicates that although $s$ -rectangular RMDPs are of a more complicated formulation, solving $s$ -rectangular RMDPs is at least as easy as solving $s a$ -rectangular RMDPs in terms of the sample complexity. In addition to the worst-case sample complexity upper bound, Theorem 3 also provides a data and instance-dependent sample complexity upper bound for $s$ -rectangular RMDPs (cf. in (20)).Taking the divergence function $\\|\\cdot\\|=L_{p}$ for instance, the data and instance-dependent sample complexity upper bound is ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\{\\widetilde{O}\\left(\\frac{S A}{(1-\\gamma)^{2}\\varepsilon^{2}}\\frac{1}{\\operatorname*{max}\\{1-\\gamma,\\sigma\\}}\\right)\\right.\\quad}&{\\left.\\mathrm{if}\\;\\widehat{\\pi}_{s}(a\\,|\\,s)=\\pi_{s}^{*}(a\\,|\\,s)=\\frac{1}{A},\\quad\\forall(s,a)\\in\\mathcal{S}\\times\\mathcal{A}}\\\\ &{\\left\\{\\widetilde{O}\\left(\\frac{S A}{(1-\\gamma)^{2}\\varepsilon^{2}}\\frac{1}{\\operatorname*{max}\\{1-\\gamma,\\sigma A^{1/p}\\}}\\right)\\right.\\quad\\mathrm{if}\\;|\\widehat{\\pi}_{s}(\\cdot\\,|\\,s)||_{0}=\\|\\pi_{s}^{*}(\\cdot\\,|\\,s)\\|_{0}=1,\\quad\\forall s\\in\\mathcal{S}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where ${\\left\\|.\\right\\|_{0}}$ corresponds to the total number of nonzero elements in a vector.The intuition beyond this theorem is that when the policy becomes proportional to uniform, the uncertainty budget of the $s$ -rectangular MDPs is equally spread into all actions, and we retrieve the $s a$ -rectangular case. When the policy becomes deterministic, all the uncertainty budget concentrates on one action. In this case, most of the actions are not robust except one, and the problem is simpler than classical MDP for this only specific action. An illustration of this result can be found in Fig. 2. ", "page_idx": 8}, {"type": "text", "text": "\u00b7 Comparisons with prior arts. In Figure 1, we illustrate the comparisons with Clavier et al. [2023] which use $L_{p}$ norms functions belonging to the class of general norms considered in this work. We do not compare in this section to Yang et al. [2022a] as it is not anymore state-of-the-art with regard to the work of Clavier et al. [2023]. In particular, the latest achieves in the $s$ -rectangular case at sample complexity of ((z ) in the regime where \u03b2 < 1 - . In this regime, our result is the same but more general but in the regime where $\\tilde{\\sigma}\\gtrsim1-\\gamma$ , they achieve sample complexity f $\\begin{array}{r}{\\widetilde{O}\\left(\\frac{S A}{(1-\\gamma)^{4}\\varepsilon^{2}}\\right)}\\end{array}$ which is bigger than our result $\\begin{array}{r}{\\widetilde{O}\\left(\\frac{S A}{(1-\\gamma)^{2}\\operatorname*{max}\\{1-\\gamma,\\tilde{\\sigma}\\}\\varepsilon^{2}}\\right)}\\end{array}$ by a factor at least $\\textstyle{\\frac{1}{1-\\gamma}}$ ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This work refined sample complexity bounds to learn robust Markov decision processes when the uncertainty set is characterized by an general $L_{p}$ metric, assuming the presence of a generative model. Our findings not only strengthen the current knowledge by improving both the upper and lower bounds, but also highlight that learning $s$ -rectangular MDPs is less challenging in terms of sample complexity compared to classical $s a$ -rectangular MDPs. This work is the first to provide results with a minimax bound, as prior results concerning $s$ -rectangular cases were not minimax optimal. Additionally, we have established the minimax sample complexity for RMDPs using a general $L_{p}$ norm, demonstrating that it is never larger than that required for learning standard MDPs. Our research identifies potential avenues for future work, such as exploring the characterization of tight sample complexity for RMDPs under a broader family of uncertainty sets, such as those defined by $f$ -divergence. It would be highly desirable for a more unified theoretical foundation, as the distance between probability measures is more natural to define using divergence. Moreover, it would be interesting to focus on the finitehorizon Setting and linear setting, as our current analytical framework opens the door for potential extensions to address finite-horizon RMDPs. Such an extension would contribute to a more comprehensive understanding of tabular cases. Finally, the case of linear MDPs would be interesting to explore. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Fondation Mathematique Jacques Hadamard supported this work during Pierre Clavier's visiting the Californian Institute of Technology. Pierre Clavier has been supported by a grant from Region Ile-de-France; DIM Math Innov. The work of L. Shi is supported in part by the Resnick Institute and Computing, Data, and Society Postdoctoral Fellowship at California Institute of Technology. The work of E. Mazumdar is supported in part from NSF-2240110. The work of A. Wierman is supported in part from CNS-2146814, CPS-2136197, CNS-2106403, and NGSDI-2105648. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Alekh Agarwal, Sham Kakade, and Lin F Yang. Model-based reinforcement learning with a generative model is minimax optimal. In Conference on Learning Theory, pages 67-83. PMLR, 2020.   \nMohammad Azar, Remi Munos, and Hibert J Kappen. Minimax pac bounds on the sample complexity of reinforcement learning with a generative model. Machine learning, 91:325-349, 2013a.   \nMohammad Gheshlaghi Azar, Remi Munos, and Hilbert J Kappen. Minimax PAC bounds on the sample complexity of reinforcement learning with a generative model. Machine learning, 91(3): 325-349, 2013b.   \nKishan Panaganti Badrinath and Dileep Kalathil. Robust reinforcement learning using least squares policy iteration with provable performance guarantees. In International Conference on Machine Learning, pages 511-520. PMLR, 2021.   \nYu Bai, Tengyang Xie, Nan Jiang, and Yu-Xiang Wang. Provably efficient Q-learning with low switching cost. arXiv preprint arXiv: 1905.12849, 2019.   \nCarolyn L Beck and Rayadurgam Srikant. Error bounds for constant step-size Q-learning. Systems & control letters, 61(12): 1203-1208, 2012.   \nJose Blanchet and Karthyek Murthy. Quantifying distributional model risk via optimal transport. Mathematics of Operations Research, 44(2):565-600, 2019.   \nJose Blanchet, Miao Lu, Tong Zhang, and Han Zhong. Double pessimism is provably effcient for distributionally robust offine reinforcement learning: Generic algorithm and robust partial coverage. arXiv preprint arXiv:2305.09659, 2023.   \nZaiwei Chen, Siva Theja Maguluri, Sanjay Shakkottai, and Karthikeyan Shanmugam. Finitesample analysis of stochastic approximation using smooth convex envelopes. arXiv preprint arXiv:2002.00874, 2020.   \nPierre Clavier, Stephanie Allassoniere, and Erwan Le Pennec. Robust reinforcement learning with distributional risk-averse formulation. arXiv preprint arXiv:2206.06841, 2022.   \nPierre Clavier, Erwan Le Pennec, and Matthieu Geist. Towards minimax optimality of model-based robust reinforcement learning. arXiv preprint arXiv:2302.05372, 2023.   \nEsther Derman and Shie Mannor. Distributional robustness and regularization in reinforcement learning. arXiv preprint arXiv:2003.02894, 2020.   \nEsther Derman, Matthieu Geist, and Shie Mannor. Twice regularized MDPs and the equivalence between robustness and regularization. Advances in Neural Information Processing Systems, 34, 2021.   \nJing Dong, Jingwei Li, Baoxiang Wang, and Jingzhao Zhang. Online policy optimization for robust MDP. arXiv preprint arXiv:2209.13841, 2022.   \nKefan Dong, Yuanhao Wang, Xiaoyu Chen, and Liwei Wang. Q-learming with UCB exploration is sample effcient for infinite-horizon MDP. arXiv preprint arXiv: 1901.09311, 2019.   \nJohn Duchi and Hongseok Namkoong. Learning models with uniform performance via distributionally robust optimization. arXiv preprint arXiv:1810.08750, 2018.   \nRui Gao. Finite-sample guarantees for wasserstein distributionally robust optimization: Breaking the curse of dimensionality. arXiv preprint arXiv:2009.04382, 2020.   \nVineet Goyal and Julien Grand-Clement. Robust markov decision processes: Beyond rectangularity. Mathematics of Operations Research, 2022.   \nSongyang Han, Sanbao Su, Sihong He, Shuo Han, Haizhao Yang, and Fei Miao. What is the solution for state adversarial multi-agent reinforcement learning? arXiv preprint arXiv:2212.02705, 2022.   \nChin Pang Ho, Marek Petrik, and Wolfram Wiesemann. Fast bellman updates for robust MDPs. In International Conference on Machine Learning, pages 1979-1988. PMLR, 2018.   \nChin Pang Ho, Marek Petrik, and Wolfram Wiesemann. Partial policy iteration for l1-robust markov decision processes. Journal of Machine Learning Research, 22(275): 1-46, 2021.   \nGarud N Iyengar. Robust dynamic programming. Mathematics of Operations Research, 30(2): 257-280, 2005.   \nMehdi Jafarnia-Jahromi, Chen-Yu Wei, Rahul Jain, and Haipeng Luo. A model-free learning algorithm for infnite-horizon average-reward MDPs with near-optimal regret. arXiv preprint arXiv:2006.04354, 2020.   \nChi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is Q-learning provably efcient? In Advances in Neural Information Processing Systems, pages 4863-4873, 2018.   \nChi Jin, Akshay Krishnamurthy, Max Simchowitz, and Tiancheng Yu. Reward-free exploration for reinforcement learning. In International Conference on Machine Learning, pages 4870-4879. PMLR, 2020.   \nYing Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably effcient for offine RL? In International Conference on Machine Learning, pages 5084-5096, 2021.   \nWilliam Karush. Minima of functions of several variables with inequalities as side conditions. In Traces and emergence of nonlinear programming, pages 217-245. Springer, 2013.   \nDavid L Kaufman and Andrew JSchaefer. Robust modified policy iteration. INFORMS Journal on Computing, 25(3):396-410, 2013.   \nMichael J Kearns and Satinder P Singh. Finite-sample convergence rates for Q-learning and indirect algorithms. In Advances in neural information processing systems, pages 996-1002, 1999.   \nOlga Klopp, Karim Lounici, and Alexandre B Tsybakov. Robust matrix completion. Probability Theory and Related Fields, 169(1-2):523-564, 2017.   \nAounon Kumar, Alexander Levine, Tom Goldstein, and Soheil Feizi. Certifying model accuracy under distribution shifs. arXiv preprint arXiv:2201.12440, 2022.   \nNavdeep Kumar, Esther Derman, Matthieu Geist, Kfir Levy, and Shie Mannor. Policy gradient for s-rectangular robust markov decision processes. arXiv preprint arXiv:2301.13589, 2023.   \nGen Li, Laixi Shi, Yuxin Chen, Yuantao Gu, and Yuejie Chi. Breaking the sample complexity barrier to regret-optimal model-free reinforcement learning. Advances in Neural Information Processing Systems, 34, 2021.   \nGen Li, Yuejie Chi, Yuting Wei, and Yuxin Chen. Minimax-optimal multi-agent RL in Markov games with a generative model. Neural Information Processing Systems, 2022a.   \nGen Li, Laixi Shi, Yuxin Chen, Yuejie Chi, and Yuting Wei. Settling the sample complexity of model-based offine reinforcement learning. arXiv preprint arXiv:2204.05275, 2022b.   \nGen Li, Changxiao Cai, Yuxin Chen, Yuting Wei, and Yuejie Chi Is Q-learning minimax optimal? a tight sample complexity analysis. Operations Research, 2023a.   \nGen Li, Yuting Wei, Yuejie Chi, and Yuxin Chen. Breaking the sample size barrer in model-based reinforcement learning with a generative model. accepted to Operations Research, 2023b.   \nGen Li, Yuling Yan, Yuxin Chen, and Jianqing Fan. Minimax-optimal reward-agnostic exploration in reinforcement learning. arXiv preprint arXiv:2304.07278, 2023c.   \nGen Li, Yuting Wei, Yujie Chi, and Yuxin Chen. Breaking the sample size barrier in model-based reinforcement learning with a generative model. Operations Research, 72(1):203-221, 2024.   \nYan Li, Tuo Zhao, and Guanghui Lan. First-order policy optimization for robust markov decision process. arXiv preprint arXiv:2209.10579, 2022c.   \nA Rupam Mahmood, Dmytro Korenkevych, Gautham Vasan, William Ma, and James Bergstra. Benchmarking reinforcement learning algorithms on real-world robots. In Conference on robot learning, pages 561-591. PMLR, 2018.   \nShie Mannor, Duncan Simester, Peng Sun, and John N Tsitsiklis. Bias and variance in value function estimation. In Proceedings of the twenty-first international conference on Machine learning, page 72, 2004.   \nColin McDiarmid et al. On the method of bounded differences. Surveys in combinatorics, 141(1): 148-188, 1989.   \nJanosch Moos, Kay Hansel, Hany Abdulsamad, Svenja Stark, Debora Clever, and Jan Peters. Robust reinforcement learning: A review of foundations and recent advances. Machine Learning and Knowledge Extraction, 4(1):276-315, 2022.   \nArnab Nilim and Laurent El Ghaoui. Robust control of Markov decision processes with uncertain transition matrices. Operations Research, 53(5):780-798, 2005.   \nKishan Panaganti and Dileep Kalathil. Sample complexity of robust reinforcement learning with agenerative model. InInternational Conference on Artificial Intelligence and Statistics, pages 9582-9602. PMLR, 2022.   \nYou Qiaoben, Xinning Zhou, Chengyang Ying, and Jun Zhu. Strategically-timed state-observation attacks on deep reinforcement learning agents. In ICML 2021 Workshop on Adversarial Machine Learning, 2021.   \nHamed Rahimian and Sanjay Mehrotra. Distributionally robust optimization: A review. arXiv preprint arXiv:1908.05659, 2019.   \nParia Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell. Bridging offline reinforcement learning and imitation learning: A tale of pessimism. Neural Information Processing Systems (NeurIPS), 2021.   \nAurko Roy, Huan Xu, and Sebastian Pokutta. Reinforcement learning under model mismatch. Advances in neural information processing systems, 30, 2017.   \nWalter Rudin et al. Principles of mathematical analysis, volume 3. McGraw-hill New York, 1964.   \nReazul Hasan Russel, Bahram Behzadian, and Marek Petrik. Optimizing norm-bounded weighted ambiguity sets for robust mdps. arXiv preprint arXiv:1912.02696, 2019.   \nLaixi Shi and Yuejie Chi. Distributionally robust model-based offine reinforcement learning with near-optimal sample complexity. arXiv preprint arXiv:2208.05767, 2022.   \nLaixi Shi, Gen Li, Yuting Wei, Yuxin Chen, and Yuejie Chi. Pessimistic Q-learning for offine reinforcement learning: Towards optimal sample complexity. In Proceedings of the 39th International Conference on Machine Learning, volume 162, pages 19967-20025. PMLR, 2022.   \nLaixi Shi, Gen Li, Yuting Wei, Yuxin Chen, Matthieu Geist, and Yuejie Chi. The curious price of distributional robustness in reinforcement learning with a generative model. arXiv preprint arXiv:2305.16589, 2023.   \nAaron Sidford, Mengdi Wang, Xian Wu, Lin Yang, and Yinyu Ye. Near-optimal time and sample complexities for solving Markov decision processes with a generative model. In Advances in Neural Information Processing Systems, pages 5186-5196, 2018.   \nElena Smirnova, Elvis Dohmatob, and Jermie Mary. Distributionall robust reinforcement learning. arXiv preprint arXiv: 1902.08708, 2019.   \nRichard S Sutton. Learning to predict by the methods of temporal differences. Machine learning, 3 (1):9-44, 1988.   \nAviv Tamar, Shie Mannor, and Huan Xu. Scaling up robust MDPs using function approximation. In International conference on machine learning, pages 181-189. PMLR, 2014.   \nKai Liang Tan, Yasaman Esfandiari, Xian Yeow Le, and Soumik Sarkar. Robustifying reinforcement learning agents via action space adversarial training. In 2020 American control conference (ACC), pages 3959-3964. IEEE, 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Chen Tessler, Yonathan Efroni, and Shie Mannor. Action robust reinforcement learning and applications in continuous control. In International Conference on Machine Learning, pages 6215-6224. PMLR, 2019. ", "page_idx": 13}, {"type": "text", "text": "A. B. Tsybakov. Introduction to nonparametric estimation, volume 11. Springer, 2009.   \nJ v. Neumann. Zur theorie der geselschaftsspiele. Mathematische annalen, 100(1):295-320, 1928.   \nRoman Vershynin. High-dimensional probability: An introduction with applications in data science, volume 47. Cambridge university press, 2018.   \nMartin J Wainwright. Stochastic approximation with cone-contractive operators: Sharp $\\ell_{\\infty}$ -bounds for Q-learning. arXiv preprint arXiv: 1905.06265, 2019.   \nShengbo Wang, Nian Si, Jose Blanchet, and Zhengyuan Zhou. A finite sample complexity bound for distributionally robust q-learning. arXiv preprint arXiv:2302.13203, 2023.   \nYue Wang and Shaofeng Zou Online robust reinforcement learning with model uncertainty. Advances in Neural Information Processing Systems, 34, 2021.   \nWolfram Wiesemann, Daniel Kuhn, and Berc Rustem. Robust markov decision processes. Mathematics of Operations Research, 38(1):153-183, 2013.   \nEric M Wolff, Ufuk Topcu, and Richard M Murray. Robust control of uncertain markov decision processes with temporal logic specifications. In 2012 IEEE 51st IEEE Conference on Decision and Control (CDC), pages 3372-3379. IEEE, 2012.   \nTengyang Xie, Nan Jiang, Huan Wang, Caiming Xiong, and Yu Bai. Policy finetuning: Bridging sample-efficient offine and online reinforcement learning. Advances in neural information processing systems, 34, 2021.   \nHuan Xu and Shie Mannor. Distributionally robust Markov decision processes. Mathematics of Operations Research, 37(2):288-300, 2012.   \nZaiyan Xu, Kishan Panaganti, and Dileep Kalathil. Improved sample complexity bounds for distributionally robust reinforcement learning. arXiv preprint arXiv:2303.02783, 2023.   \nYuling Yan, Gen Li, Yuxin Chen, and Jianqing Fan. The efficacy of pessimism in asynchronous Q-learning. arXiv preprint arXiv:2203.07368, 2022.   \nYuling Yan, Gen Li, Yuxin Chen, and Jianqing Fan. The effcacy of pessimism in asynchronous q-learning. IEEE Transactions on Information Theory, 2023.   \nKunhe Yang, Lin Yang, and Simon Du. Q-learning with logarithmic regret. In International Conference on Artificial Intelligence and Statistics, pages 1576-1584. PMLR, 2021.   \nWei H Yang. On generalized holder inequality. 1991.   \nWenhao Yang, Liangyu Zhang, and Zhihua Zhang. Toward theoretical understandings of robust Markovdcisinprocesses: Samle comlexity and asymtotic.TheAnnals ofStatistics,506): 3223-3248, 2022a.   \nWenhao Yang, Liangyu Zhang, and Zhihua Zhang. Toward theoretical understandings of robust markov decision processes: Sample complexity and asymptotics. The Annals of Statistics, 50(6): 3223-3248, 2022b.   \nWenhao Yang, Han Wang, Tadashi Kozuno, Scott M Jordan, and Zhiua Zhang. Avoiding model estimation in robust markov decision processes with a generative model. arXiv preprint arXiv:2302.01248, 2023.   \nMing Yin, Yu Bai, and Yu-Xiang Wang. Near-optimal offine reinforcement learning via double variance reduction. arXiv preprint arXiv:2102.01748, 2021.   \nHuan Zhang, Honge Chen, Chaowei Xiao, Bo Li, Mingyan Liu, Duane Boning, and Cho-Jui Hsieh. Robust deep reinforcement learning against adversarial perturbations on state observations. Advances in Neural Information Processing Systems, 33:21024-21037, 2020a. ", "page_idx": 13}, {"type": "text", "text": "Huan Zhang, Hongge Chen, Duane Boning, and Cho-Jui Hsieh. Robust reinforcement learning on state observations with learned optimal adversary. arXiv preprint arXiv:2101.08452, 2021. ", "page_idx": 14}, {"type": "text", "text": "Zihan Zhang, Yuan Zhou, and Xiangyang Ji. Almost optimal model-free reinforcement learning via reference-advantage decomposition. Advances in Neural Information Processing Systems, 33, 2020b. ", "page_idx": 14}, {"type": "text", "text": "Zhengqing Zhou, Qinxun Bai, Zhengyuan Zhou, Linhai Qiu, Jose Blanchet, and Peter Glynn. Finite-sample regret bound for distributionally robust offine tabular reinforcement learning. In International Conference on Artificial Intelligence and Statistics, pages 3331-3339. PMLR, 2021. ", "page_idx": 14}, {"type": "text", "text": "A Other related works ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Here we provide additional discussion of related work that could not be fit into the main paper due to space considerations. We limit our discussions to the tabular setting with finite state and action spaces provable RL algorithms. ", "page_idx": 14}, {"type": "text", "text": "Classical reinforcement learning with finite-sample guarantees.  A recent surge in attention for RL has leveraged the methodologies derived from high-dimensional probability and statistics to analyze RL algorithms in non-asymptotic scenarios. Substantial efforts have been devoted to conducting non-asymptotic sample analyses of standard RL in many settings. Illustrative instances encompass investigations employing Probably Approximately Correct (PAC) bonds in the context of generative model settings [Kearns and Singh, 1999, Beck and Srikant, 2012, Li et al., 2022a, Chen et al., 2020, Azar et al., 2013b, Sidford et al., 2018, Agarwal et al., 2020, Li et al., 2023a,b, Wainwright, 2019] and the online setting via both in PAC-base or regret-based analyses [Jin et al., 2018, Bai et al., 2019, Li et al., 2021, Zhang et al., 2020b, Dong et al., 2019, Jin et al., 2020, Li et al., 2023c, Jafarnia-Jahromi et al., 2020, Yang et al., 2021] and finally offine setting [Rashidinejad et al., 2021, Xie et al., 2021, Yin et al., 2021, Shi et al., 2022, Li et al., 2022b, Jin et al., 2021, Yan et al., 2022]. ", "page_idx": 14}, {"type": "text", "text": "Robustness in reinforcement learning. Reinforcement learning has had notable achievements but has also exhibited significant limitations, particularly when the learned policy is susceptible to deviations in the deployed environment due to perturbations, model discrepancies, or structural modifications. To address these challenges, the idea of robustness in RL algorithms has been studied. Robustness could concern uncertainty or perturbations across different Markov Decision Processes (MDPs) components, encompassing reward, state, action, and the transition kernel. Moos et al. [2022] gives a recent overview of the different work in this field. ", "page_idx": 14}, {"type": "text", "text": "The distributionally robust MDP (RMDP) framework has been proposed [Iyengar, 2005] to enhance the robustness of RL has been proposed. In addition to this work, various other research efforts, including, but not limited to, Zhang et al. [2020a, 2021], Han et al. [2022], Clavier et al. [2022], Qiaoben et al. [2021], explore robustness regarding state uncertainty. In these scenarios, the agent's policy is determined on the basis of perturbed observations generated from the state, introducing restricted noise, or undergoing adversarial attacks. Finally, robustness considerations extend to uncertainty in the action domain. Works such as Tessler et al. [2019], Tan et al. [2020] consider the robustness of actions, acknowledging potential distortions introduced by an adversarial agent. ", "page_idx": 14}, {"type": "text", "text": "Given the focus of our work, we provide a more detailed background on progress related to distributionally robust RL. The idea of distributionally robust optimization has been explored within the context of supervised learning [Rahimian and Mehrotra, 2019, Gao, 2020, Duchi and Namkoong, 2018, Blanchet and Murthy, 2019] and has also been extended to distributionally robust dynamic programming and Distributionally Robust Markov Decision Processes (DRMDPs) such as in [Iyengar, 2005, Xu and Mannor, 2012, Wolff et al., 2012, Kaufman and Schaefer, 2013, Ho et al., 2018, Smirnova et al., 2019, Ho et al., 2021, Goyal and Grand-Clement, 2022, Derman and Mannor, 2020, Tamar et al., 2014, Badrinath and Kalathil, 2021]. Despite the considerable attention received, both empirically and theoretically, most previous theoretical analyses in the context of RMDPs adopt an asymptotic perspective [Roy et al., 2017] or focus on planning with exact knowledge of the uncertainty set [Iyengar, 2005, Xu and Mannor, 2012, Tamar et al., 2014]. Many works have focused on the finite-sample performance of verifiable robust Reinforcement Learning (RL) algorithms. These investigations encompass various data generation mechanisms and uncertainty set formulations over the transition kernel. Closely related to our work, various forms of uncertainty sets have been explored, showcasing the versatility of approaches. Divergence such as Kullback-Leibler (KL) divergence is another prevalent choice, extensively studied by Yang et al. [2022a], Panaganti and Kalathil [2022], Zhou et al. [2021], Shi and Chi [2022], Xu et al. [2023], Wang et al. [2023], Blanchet et al. [2023], who investigated the sample complexity of both model-based and model-free algorithms in simulator or offline settings. Xu et al. [2023] considered various uncertainty sets, including those associated with the Wasserstein distance. The introduction of an R-contamination uncertainty set Wang and Zou [2021], has been proposed to tackle a robust Q-learning algorithm for the online setting, with guarantees analogous to standard RL. Finally, the finite-horizon scenario has been studied by $\\mathrm{Xu}$ et al. [2023], Dong et al. [2022] with finitesample complexity bounds for (RMDPs) using TV and $\\chi^{2}$ divergence. More broadly, other related topics have been explored, such as the iteration complexity of policy-based methods [Li et al., 2022c, Kumar et al., 2023], and regularization-based robust RL [Yang et al., 2023]. Finally, Badrinath and Kalathil [2021] examined a general $s a$ -rectangular form of the uncertainty set, proposing a model-free algorithm for the online setting with linear function approximation to address large state spaces. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "B  Further discussions of Theorem 1 and Theorem 3 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "What norms are included in the Definition 1?In our upper bound result Theorems 3 and 1, we upper bound the sample complexity for $C^{2}$ norms and TV. The set of $C^{2}$ smooth norm is very large as it includes all, $L_{p}$ norm, weighted, rescaled $L_{p}$ norms for $p~\\geq~2$ . Weighted norms can be useful in practice, to get more weights on dangerous specific states in Robust MDPs formulation such as in Russel et al. [2019]. Moreover, note that our result can generalize to metric or pseudo metric (which are not homogeneous ie $\\|\\lambda\\|=|\\lambda|\\,\\|x\\|\\,\\forall x\\in\\mathbb{R}^{n},\\lambda\\in\\mathbb{R})$ with norms of the form $\\begin{array}{r}{x\\mapsto\\phi^{-1}(\\sum_{k=1}^{n},\\phi(|\\overline{{x}}_{k}|))}\\end{array}$ with $\\phi$ a convex incising function such as the norm is still positive, definite positive. Choosing $\\phi(x)=x^{p}$ leads to the $L_{p}$ norms. ", "page_idx": 15}, {"type": "text", "text": "\u00b7 Assumptions on $\\gamma$ in Theorems $^{\\,l}$ and3,andAssumptionson $\\gamma$ for lower bound. When $\\gamma$ is small (e.g., $\\gamma\\in(0,\\frac12]$ leads to the effective horizon length is at most 2), the sequential structure almost disappears and is much less of interest for RL community. So people Li et al. [2023b] Yan et al. [2023] usually focus on reasonable range $\\gamma\\in\\left(c,1\\right)$ for some small positive constant $c$ , such as $\\begin{array}{r}{\\gamma\\in[\\frac{1}{2},\\dot{1})}\\end{array}$ . However, the theorems can be directly extended to a broader range of $\\gamma\\in\\left(c,1\\right)$ along with $c$ as small as desired so that almost cover the full range $(0,1)$ ", "page_idx": 15}, {"type": "text", "text": "\u00b7 Why final results on s depend on $\\hat{\\pi}$ ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Theorem 3 is $\\hat{\\pi}$ data dependent which is randomness-dependent measure. However, taking the minimum of this quantity leads to the same bound as is $s a$ -rectangular, so to illustrate that it is possible to get tighter bounds for $s$ -rectangular with instance-dependent RMDPs, we decide to write also randomness-dependent quantity, while the less tight upper bound is written also in the theorem, taking the first term in the min operator in (21). ", "page_idx": 15}, {"type": "text", "text": "\u00b7 Why our results are still true for TV? Theorems 1 and 3 are stated for $C^{2}$ smooth norms, however, our result is still true for $T V$ which is not $C^{2}$ as in this specific case, the dual of the optimization problem becomes a 1-dimensional problem. In this case in the main concentration lemma 8, the additional term involving smoothness term denoted $C_{S}$ isnot present and the bound is simpler as is not required this additional term. ", "page_idx": 15}, {"type": "text", "text": "\u00b7 Why burn-in or sufficiently small e condition is not too restrictive? The burn-in term in Th. 1 and 3 is proportional to $1/\\epsilon$ where the \"sample complexity\" term is proportional to $1/\\epsilon^{2}$ . The smooth term depending on $C_{S}$ or burn-in is then not too large for sufficiently small $\\epsilon$ compared to the other term, which will give final sample complexity. ", "page_idx": 15}, {"type": "text", "text": "\u00b7Whythisisnotextendableto $f$ -divergence currently? The f-divergence as a distinct family of divergence is beyond the scope of this paper. Current proof for arbitrary norms cannot be directly extended since the key phenomenon of shrinking range of the robust value function has not been verified for $f$ -divergence yet, while it is promising as an interesting future direction. ", "page_idx": 15}, {"type": "text", "text": "C Preliminaries ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "These quantities appear in the dual formulation of the robust optimization problem and more preciously the dual span semi norm $\\mathrm{sp}(.)_{*}$ note that for $L_{2}$ , we retrieve the classical mean with the definition of $\\omega$ ) With slight abuse of notation, we denote 0 (resp. 1) as the all-zero (resp. all-one) vector. We then introduce the notation $[T]:=\\{1,\\cdot\\cdot\\cdot,T\\}$ for any positive integer $T>0$ . Then, for all $1\\leq i\\leq n$ , for two vectors $x=[x_{i}]_{1\\leq i\\leq n}$ and $y=[y_{i}]_{1\\leq i\\leq n}$ , the notation $x\\leq y$ (resp. $x\\geq y,$ means $x_{i}\\leq y_{i}$ (resp. $x_{i}\\geq y_{i}\\,$ ). Finally, for any vector $x$ , the notation is overloaded by letting $x^{\\circ2}=\\left[x(s,\\bar{a})^{2}\\right]_{(s,a)\\in{\\cal S}\\times{\\cal A}}$ (resp. $x^{\\circ2}=\\left[x(s)^{2}\\right]_{s\\in{\\cal{S}}}^{\\bullet})$ Finallywedropthe suscript $\\lVert.\\rVert$ to write $\\mathcal{U}_{\\parallel.\\parallel}^{\\sigma}(\\cdot)=\\mathcal{U}^{\\sigma}(\\cdot)$ for both $s a\\cdot$ and $s-$ rectangular assumptions such that we write uncertainty set in the for $s a$ -rectangular case $\\mathcal{U}^{\\mathtt{s a,\\sigma}}(.)$ or $\\mathcal{U}^{\\mathtt{s},\\widetilde{\\sigma}}(.)$ in the $s$ -rectangular assumptions. ", "page_idx": 16}, {"type": "text", "text": "Matrix and Vector Notations.  We define the following notation. ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "$r\\in\\mathbb{R}^{S A}$ the reward function, such that $r_{(s,a)}=r(s,a)$ for all $(s,a)\\in S\\times A$ \u00b7 $P^{0}\\in\\mathbb{R}^{S A\\times S}$ the nominal transition kernel matrix using $P_{s,a}^{0}$ as the $(s,a)$ -th row. \u00b7 $\\widehat{P}^{0}\\in\\mathbb{R}^{S A\\times S}$ the estimated nomimal transition kernel matrix with P',a as the (s, a)-th row. \u00b7 $\\Pi^{\\pi}\\in\\{0,1\\}^{S\\times S A}$ the projection matrix associated with a policy $\\pi$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Pi^{\\pi}=\\left(\\begin{array}{c c c c}{\\iota_{\\pi(1)}^{\\top}}&{\\boldsymbol{0}^{\\top}}&{\\cdot\\cdot\\cdot}&{\\boldsymbol{0}^{\\top}}\\\\ {\\boldsymbol{0}^{\\top}}&{\\boldsymbol{1}_{\\pi(2)}^{\\top}}&{\\cdot\\cdot\\cdot}&{\\boldsymbol{0}^{\\top}}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {\\boldsymbol{0}^{\\top}}&{\\boldsymbol{0}^{\\top}}&{\\cdot\\cdot\\cdot}&{\\boldsymbol{1}_{\\pi(S)}^{\\top}}\\end{array}\\right),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\boldsymbol{1}_{\\pi(1)}^{\\top},\\boldsymbol{1}_{\\pi(2)}^{\\top},\\ldots,\\boldsymbol{1}_{\\pi(S)}^{\\top}\\in\\mathbb{R}^{A}$ are simplex ector such as ", "page_idx": 16}, {"type": "equation", "text": "$$\n1_{\\pi(1)}^{\\top}=(\\pi(a_{1}|s_{1}),\\pi(a_{2}|s_{1}),...,\\pi(a_{A}|s_{1})).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "\u00b7 The two matrices $P^{V}\\in\\mathbb{R}^{S A\\times S}$ \uff0c $\\widehat{P}^{V}\\in\\mathbb{R}^{S A\\times S}$ represent the probability transition kernel in the uncertainty set that leads to the worst-case value for any vector $\\boldsymbol{V}\\in\\mathbb{R}^{S}$ . Moreover, the quantities $P_{s,a}^{V}$ (resp. $\\widehat{P}_{s,a}^{V})$ stands for the $(s,a)$ -th row of the transition matrix $P^{V}$ (resp. ${\\widehat{P}}^{V}$ ). In $s a$ -rectangular case , the $(s,a)$ -th rows of these transition matrices are defined as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{P_{s,a}^{V}=\\operatorname*{argmin}_{\\mathcal{P}\\in\\mathcal{U}^{\\mathrm{sa},\\sigma}(P_{s,a}^{0})}\\mathcal{P}V,\\qquad\\mathrm{and}\\qquad\\widehat{P}_{s,a}^{V}=\\operatorname*{argmin}_{\\mathcal{P}\\in\\mathcal{U}^{\\mathrm{sa},\\sigma}(\\widehat{P}_{s,a}^{0})}\\mathcal{P}V.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Moreover, the shorthand notation defined below is used ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P_{s,a}^{\\pi,V}:=P_{s,a}^{V^{\\pi,\\sigma}}=\\mathrm{argmin}_{\\mathcal{P}\\in\\mathcal{U}^{\\mathrm{sa},\\sigma}(P_{s,a}^{0})}\\mathcal{P}V^{\\pi,\\sigma},}\\\\ &{P_{s,a}^{\\pi,\\hat{V}}:=P_{s,a}^{\\hat{V}^{\\pi,\\sigma}}=\\mathrm{argmin}_{\\mathcal{P}\\in\\mathcal{U}^{\\mathrm{sa},\\sigma}(P_{s,a}^{0})}\\mathcal{P}\\widehat{V}^{\\pi,\\sigma},}\\\\ &{\\widehat{P}_{s,a}^{\\pi,V}:=\\widehat{P}_{s,a}^{V^{\\pi,\\sigma}}=\\mathrm{argmin}_{P\\in\\mathcal{U}^{\\mathrm{sa},\\sigma}(\\widehat{P}_{s,a}^{0})}P V^{\\pi,\\sigma},}\\\\ &{\\widehat{P}_{s,a}^{\\pi,\\hat{V}}:=\\widehat{P}_{s,a}^{\\hat{V}^{\\pi,\\sigma}}=\\mathrm{argmin}_{P\\in\\mathcal{U}^{\\mathrm{sa},\\sigma}(\\widehat{P}_{s,a}^{0})}P\\widehat{V}^{\\pi,\\sigma}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In the following, we define the corresponding probability transition matrices which are denotedby $P^{\\pi,V}\\in\\mathbb{R}^{S A\\times S}$ $P^{\\pi,\\widehat{V}}\\in\\bar{\\mathbb{R}}^{S A\\times S}$ $\\widehat{P}^{\\pi,V}\\in\\mathbb{R}^{S A\\times S}$ and $\\widehat{P}^{\\pi,\\widehat{V}}\\in\\mathbb{R}^{S A\\times S}$ ", "page_idx": 16}, {"type": "text", "text": "\u00b7Using theprojection over $\\pi$ ,thematrices $P^{\\pi}\\,\\in\\,\\mathbb{R}^{S\\times S}$ \uff0c $\\widehat{P}^{\\pi}\\,\\in\\,\\mathbb{R}^{S\\times S}$ \uff0c $\\underline{{P}}^{\\pi,V}\\,\\in\\,\\mathbb{R}^{S\\times S}$ $\\underline{{P}}^{\\pi,\\widehat{V}}\\in\\mathbb{R}^{S\\times S},\\underline{{\\widehat{P}}}^{\\pi,V}\\in\\mathbb{R}^{S\\times S}$ and $\\underline{{\\widehat{P}}}^{\\pi,\\widehat{V}}\\in\\mathbb{R}^{S\\times S}$ represent probability transition matrices W.r.t.policy $\\pi$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P^{\\pi}:=\\Pi^{\\pi}P^{0},\\qquad\\widehat{P}^{\\pi}:=\\Pi^{\\pi}\\widehat{P}^{0},\\qquad\\underline{{P}}^{\\pi,V}:=\\Pi^{\\pi}P^{\\pi,V},\\qquad\\underline{{P}}^{\\pi,\\widehat{V}}:=\\Pi^{\\pi}P^{\\pi,\\widehat{V}},}\\\\ &{\\widehat{P}^{\\pi,V}:=\\Pi^{\\pi}\\widehat{P}^{\\pi,V},\\qquad\\mathrm{and}\\qquad\\widehat{\\underline{{P}}}^{\\pi,\\widehat{V}}:=\\Pi^{\\pi}\\widehat{P}^{\\pi,\\widehat{V}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For $s$ -rectangular, we will use the same notation for these transition matrices. Finally, we denote $P_{s}^{\\pi}$ asthe $s$ -th row of the transition matrix $P^{\\pi}$ ", "page_idx": 16}, {"type": "text", "text": "$r_{\\pi}\\in\\mathbb{R}^{S}$ is the reward function restricted to the actions chosen by $\\pi$ $,r_{\\pi}=\\Pi^{\\pi}r$ ", "page_idx": 17}, {"type": "text", "text": "$\\operatorname{Var}_{P}(V)\\,\\in\\,\\mathbb{R}^{S A}$ is the variance for a given transition kernel $P\\,\\in\\,\\mathbb{R}^{S A\\times S}$ and vector $V\\in\\mathbb{R}^{S}$ , we denote the $(s,a)$ -th row of $\\bar{\\mathrm{Var}}_{P}(V)$ as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathsf{V a r}_{P}(s,a):=\\mathrm{Var}_{P_{s,a}}(V).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "C.1  Additional definitions and basic facts ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "For any norm smooth $\\left\\Vert.\\right\\Vert$ introduced in 1, we define the span semi norm as ", "page_idx": 17}, {"type": "text", "text": "Definition 2 (Span semi norm). Given any norm $\\|\\cdot\\|$ ,we define the span semi norm as: $\\operatorname{sp}(x)=$ $\\operatorname*{min}_{\\omega\\in\\mathbb{R}}\\|v-\\omega\\mathbf{1}\\|$ and the generalized mean as $\\begin{array}{r}{\\omega(x):=\\arg\\operatorname*{min}_{\\omega\\in\\mathbb{R}}\\|x-\\omega\\mathbf{1}\\|}\\end{array}$ ", "page_idx": 17}, {"type": "text", "text": "Let vector $P\\in\\mathbb{R}^{1\\times S}$ and vector $V\\in\\mathbb{R}^{S}$ , we define the variance ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname{Var}_{P}(V):=P(V\\circ V)-(P V)\\circ(P V).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The following lemma bounds the Lipschitz constant of the variance function. ", "page_idx": 17}, {"type": "text", "text": "Lemma 1. (Shi et al. $[2023]$ , Lemma 2) Assuming $\\begin{array}{r}{0\\le V_{1},V_{2}\\le\\frac{1}{1-\\gamma}}\\end{array}$ which obey $\\|V_{1}-V_{2}\\|_{\\infty}\\leq x$ then for $P\\in\\Delta(S)$ , one has ", "page_idx": 17}, {"type": "equation", "text": "$$\n|\\mathrm{Var}_{P}(V_{1})-\\mathrm{Var}_{P}(V_{2})|\\leq{\\frac{2x}{(1-\\gamma)}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Lemma 2. [Panaganti and Kalathil, 2022, Lemma 6] Consider any $\\delta\\in(0,1)$ .For any fixed policy $\\pi$ and fixed value vector $V\\in\\mathbb{R}^{S}$ , one has with probability at least $1-\\delta$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left|\\sqrt{\\mathrm{Var}_{\\widehat{P}^{\\pi}}(V)}-\\sqrt{\\mathrm{Var}_{P^{\\pi}}(V)}\\right|\\leq\\sqrt{\\frac{2\\|V\\|_{\\infty}^{2}\\log(\\frac{2S A}{\\delta})}{N}}1.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "C.2   Empirical robust MDP $\\widehat{\\mathcal{M}}_{\\sf r o b}$ Bellman equations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We define the robust MDP $\\widehat{\\mathcal{M}}_{\\sf r o b}=\\{S,\\mathcal{A},\\gamma,\\mathcal{U}^{\\sigma}(\\widehat{P}^{0}),r\\}$ based on the estimated nominal distribution $\\widehat{P}^{0}$ in (11). Then, we denote the associated robust value function (resp. robust Q-function) are $\\widehat{V}^{\\pi,\\sigma}$ (resp. $\\widehat{Q}^{\\pi,\\sigma}$ ) qnd we can notice that that $\\widehat{Q}^{\\star,\\sigma}$ is the unique-fixed point of ${\\widehat{\\mathcal{T}}}^{\\sigma}(\\cdot)$ (see Lemma C.3), the empirical robust Bellman operator constructed using $\\widehat{P}^{0}$ . Finally, similarly to (9), for $\\widehat{\\mathcal{M}}_{\\sf r o b}$ the Bellman's optimality principle gives the following robust Bellman consistency equation (resp. robust Bellman optimality equation)for $s a$ -rectangular assumptions: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{Q}^{\\pi,\\sigma}(s,a)=r(s,a)+\\gamma\\operatorname*{inf}_{\\substack{\\mathcal{P}\\in\\mathcal{U}^{\\smash{\\mathrm{sa},\\sigma}}(\\widehat{P}_{s,a}^{0})}}\\mathcal{P}\\widehat{V}^{\\pi,\\sigma},}\\\\ &{\\widehat{Q}^{\\star,\\sigma}(s,a)=r(s,a)+\\gamma\\operatorname*{inf}_{\\substack{\\mathcal{P}\\in\\mathcal{U}^{\\smash{\\mathrm{sa},\\sigma}}(\\widehat{P}_{s,a}^{0})}}\\mathcal{P}\\widehat{V}^{\\star,\\sigma}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Using matrix notation, we can write the robust Bellman consistency equations as ", "page_idx": 17}, {"type": "equation", "text": "$$\nQ^{\\pi,\\sigma}=r+\\gamma\\operatorname*{inf}_{\\mathcal{P}\\in\\mathcal{U}^{\\smash{\\mathrm{sa},\\sigma}}(P^{0})}\\mathcal{P}V^{\\pi,\\sigma}\\quad\\mathrm{and}\\quad\\widehat{Q}^{\\pi,\\sigma}=r+\\gamma\\operatorname*{inf}_{\\mathcal{P}\\in\\mathcal{U}^{\\smash{\\mathrm{sa},\\sigma}}(\\widehat{P}^{0})}\\mathcal{P}\\widehat{V}^{\\pi,\\sigma},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which imply ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V^{\\pi,\\sigma}=r_{\\pi}+\\gamma\\Pi^{\\pi}\\operatorname*{inf}_{\\mathcal{P}\\in\\mathcal{U}^{\\smash{\\mathrm{sa},\\sigma}}(P^{0})}\\mathcal{P}V^{\\pi,\\sigma}\\overset{(\\mathrm{i})}{=}r_{\\pi}+\\gamma\\underline{{P}}^{\\pi,V}V^{\\pi,\\sigma},}\\\\ &{\\widehat{V}^{\\pi,\\sigma}=r_{\\pi}+\\gamma\\Pi^{\\pi}\\operatorname*{inf}_{\\mathcal{P}\\in\\mathcal{U}^{\\smash{\\mathrm{sa},\\sigma}}(\\widehat{P}^{0})}\\mathcal{P}\\widehat{V}^{\\pi,\\sigma}\\overset{(\\mathrm{ii})}{=}r_{\\pi}+\\gamma\\underline{{\\widehat{P}}}^{\\pi,\\widehat{V}}\\widehat{V}^{\\pi,\\sigma},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where (i) and (i) hold by the definitions in (24), (25) and (26). For $s$ -rectangular, we can define the same notation, removing $a$ subscript: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V^{\\pi,\\sigma}=r_{\\pi}+\\gamma\\Pi^{\\pi}\\operatorname*{inf}_{\\mathcal{P}\\in\\mathcal{U}^{s,\\tilde{\\sigma}}(P^{0})}\\mathcal{P}V^{\\pi,\\sigma}\\overset{\\mathrm{(ii)}}{=}r_{\\pi}+\\gamma\\underline{{P}}^{\\pi,V}V^{\\pi,\\sigma},}\\\\ &{\\widehat{V}^{\\pi,\\sigma}=r_{\\pi}+\\gamma\\Pi^{\\pi}\\operatorname*{inf}_{\\mathcal{P}\\in\\mathcal{U}^{s,\\tilde{\\sigma}}(\\widehat{P}^{0})}\\mathcal{P}\\widehat{V}^{\\pi,\\sigma}\\overset{\\mathrm{(ii)}}{=}r_{\\pi}+\\gamma\\underline{{\\widehat{P}}}^{\\pi,\\widehat{V}}\\widehat{V}^{\\pi,\\sigma},.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "C.3  Properties of the robust Bellman operator and dual representation ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The robust Bellman operator (cf. (10)) shares the $\\gamma$ -contraction property of the standard Bellman operator as: ", "page_idx": 18}, {"type": "text", "text": "[Iyengar, 2005, Theorem 3.2] Given $\\gamma\\in[0,1)$ ,therobustBellmanoperator $\\mathcal{T}^{\\sigma}(\\cdot)$ (cf. (10)) is a $\\gamma$ -contraction w.t. $\\|\\cdot\\|_{\\infty}$ More formally, for any $Q_{1},Q_{2}\\in\\mathbb{R}^{S A}$ s.t. $\\begin{array}{r}{Q_{1}(s,a),Q_{2}(s,a)\\in\\left[0,\\frac{1}{1-\\gamma}\\right]}\\end{array}$ for all $(s,a)\\in S\\times A$ , one has ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\|T^{\\sigma}(Q_{1})-T^{\\sigma}(Q_{2})\\right\\|_{\\infty}\\leq\\gamma\\left\\|Q_{1}-Q_{2}\\right\\|_{\\infty}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "It can be also shown that, $Q^{\\star,\\sigma}$ is the unique fxed point of $\\mathcal{T}^{\\sigma}(\\cdot)$ obeying $\\begin{array}{r}{0\\leq Q^{\\star,\\sigma}(s,a)\\leq\\frac{1}{1-\\gamma}}\\end{array}$ for all $(s,a)\\in S\\times A$ ", "page_idx": 18}, {"type": "text", "text": "One of the main contributions is to derive the dual form of optimization problem using arbitrary norms. These lemma take ideas from Iyengar [2005] and are adapted to arbitrary norms and not only $T V$ distance. ", "page_idx": 18}, {"type": "text", "text": "Dual equivalence of the robust Bellman operator. Fortunately, the robust Bellman operator can be evaluated efficiently by resorting to its dual formulation, and this idea is central in all proofs for RMPDs. Dual formulation of RMDPs have been introduced in [Iyengar, 2005] but the proof was done uniquely for the $T V$ and the $\\chi^{2}$ case. Before continuing, for any $V\\in\\mathbb{R}^{S}$ , we denote $[V]_{\\alpha}$ as its clipped version by some non-negative vector $\\alpha$ , namely, ", "page_idx": 18}, {"type": "equation", "text": "$$\n[V]_{\\alpha}(s):={\\binom{\\alpha,}{V(s),}},\\quad\\mathrm{{if}~}V(s)>\\alpha(s),}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Defining the gradient of $P\\mapsto\\|P\\|$ as $\\nabla\\left\\|P\\right\\|$ \uff0c $\\lambda>0$ , a positive scalar and $\\omega$ is the generalized mean defined as the argmin in the definition of the span semi norm in Def.2, we derive two optimization lemmas. ", "page_idx": 18}, {"type": "text", "text": "Lemma 3 (Strong duality using norm $\\lVert\\cdot\\rVert$ in the $s a$ -rectangular case.). Consider any probability vector $P\\in\\Delta(S)$ and any fixed uncertainty level $\\sigma$ ,we abbreviate the notation of the uncertainty set $\\mathcal{U}_{\\parallel.\\parallel}^{\\mathtt{s a},\\sigma}(P)$ (cf. (3)) as $\\mathcal{U}^{\\mathtt{s a},\\sigma}(P)$ . For any vector $V\\in\\mathbb{R}^{S}$ obeying $V\\geq0$ recalling the definition of $[V]_{\\alpha}$ in (35), one has ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\mathcal{P}\\in\\mathcal{U}^{\\mathrm{s},\\sigma}(P)}{\\operatorname*{inf}}\\mathcal{P}V=\\underset{\\mu_{P}^{\\lambda,\\omega}\\in\\mathcal{M}_{P}^{\\lambda,\\omega}}{\\operatorname*{max}}\\left\\{P(V-\\mu_{P}^{\\lambda,\\omega})-\\sigma\\left(\\mathrm{sp}((V-\\mu_{P}^{\\lambda,\\omega}))_{*}\\right)\\right\\}.}\\\\ &{\\qquad\\qquad\\qquad=\\underset{\\alpha_{P}^{\\lambda,\\omega}\\in\\mathbb{A}_{P}^{\\lambda,\\omega}}{\\operatorname*{max}}\\left\\{P\\left[V\\right]_{\\alpha_{P}^{\\lambda,\\omega}}-\\sigma\\left(\\mathrm{sp}([V]_{\\alpha_{P}^{\\lambda,\\omega}})_{*}\\right)\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\operatorname{sp}()_{*}$ is defined in Def.2. Here, the two auxiliary variational family $\\mathrm{A}_{P}^{\\lambda,\\omega},\\mathcal{M}_{P}^{\\lambda,\\omega}$ are defined asbelow: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Lambda_{P}^{\\lambda,\\omega}=\\{\\alpha_{P}^{\\lambda,\\omega}:\\alpha_{P}^{\\lambda,\\omega}(s)=\\omega+\\lambda|\\nabla\\left\\|P\\right\\|(s):\\lambda>0,w>0,P\\in\\Delta(S),\\alpha_{P}^{\\lambda,\\omega}\\in\\left[0,\\frac{1}{1-\\gamma}\\right]^{S}\\}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{M}_{P}^{\\lambda,\\omega}=\\{\\mu_{P}^{\\lambda,\\omega}=V-\\alpha_{P}^{\\lambda,\\omega},\\lambda,\\omega\\in\\mathbb{R}^{+},P\\in\\Delta(S),\\mu\\in\\mathbb{R}_{+}^{S},\\mu_{P}^{\\lambda,\\omega}=\\left[0,\\frac{1}{1-\\gamma}\\right]^{S}\\}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For $L_{1}$ or $T V$ , case , the vector $\\alpha_{P}^{\\lambda,\\omega}$ reduces to a 1 dimensional scalar such as $\\alpha\\in[0,1/(1-\\gamma)]$ ", "page_idx": 18}, {"type": "text", "text": "Proof. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\mathcal{P}\\in\\mathcal{U}^{s,\\sigma}(P)}{\\operatorname*{inf}}\\mathcal{P}V=\\underset{\\{\\mathcal{P}:\\mathcal{P}\\in\\Delta_{s},\\|\\mathcal{P}-P\\|\\leq\\sigma\\}}{\\operatorname*{inf}}\\sum_{s^{\\prime}}\\mathcal{P}(s^{\\prime})V(s^{\\prime})}\\\\ &{\\qquad\\qquad\\qquad\\qquad=P V+\\underset{\\{y:\\|y\\|\\leq\\sigma,1y=0,y\\geq-P\\}}{\\operatorname*{inf}}\\sum_{s^{\\prime}}y(s^{\\prime})V(s^{\\prime})}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where we use the change of variable $y(s^{\\prime})=\\mathcal{P}(s^{\\prime})-P(s^{\\prime})$ for all $s^{\\prime}\\in\\mathcal{S}$ . Then the Lagrangian function of the above optimization problem can be written as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\mathcal{P}\\in\\mathcal{U}_{s,a}^{\\sigma}(P)}{\\operatorname*{inf}}\\mathcal{P}V=\\!P V+\\underset{\\mu\\geq0,\\nu\\in\\mathbb{R}}{\\operatorname*{sup}}\\underset{\\{y:\\|y\\|\\leq\\sigma\\}}{\\operatorname*{inf}}-\\underset{s^{\\prime}}{\\sum}\\mu(s)P(s^{\\prime})+\\underset{s^{\\prime}}{\\sum}(y(s^{\\prime})(V(s^{\\prime})-\\mu(s^{\\prime})-\\nu)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\underset{\\mu\\geq0,\\nu\\in\\mathbb{R}}{\\operatorname*{sup}}-\\underset{s^{\\prime}}{\\sum}\\mu(s^{\\prime})P(s^{\\prime})-\\sigma\\left\\|(V(s^{\\prime})-\\mu(s^{\\prime})-\\nu\\mathbf{1})\\right\\|_{*}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\underset{\\mu\\geq0}{\\operatorname*{sup}}P(V-\\mu)-\\sigma\\mathrm{sp}(V-\\mu)_{*}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\mu\\in\\mathbb{R}_{+}^{S}$ $\\nu\\in\\mathbb{R}$ are Lagrangian variables, (a) is true using the equality case of Cauchy-Swartz inequality for dual norm Yang [1991], and (b) is due to is the definition of the span semi-norm (see (C)). The value that maximizes the inner maximization problem in (42) in $\\omega(V,\\mu)$ is the generalizedmean by definition denoted with abbreviate notation $\\omega$ . If the norm is differentiable, then we have that the equality (a) comes from the generalized Holder's inequality for arbitrary norms Yang [1991], namely, defining $z=(V-\\mu-\\omega)$ , it satisfies ", "page_idx": 19}, {"type": "equation", "text": "$$\nz=\\|z\\|_{*}\\,\\nabla\\,\\|y\\|\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thequantity $\\nu$ is replaced by the generalized mean for equality in (b) while (44) comes from Yang [1991]. Using complementary slackness Karush [2013]stackness let $B=\\{s\\in S:\\mu(s)>0\\}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\forall s\\in B:\\quad y^{*}(s)=-P(s),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which leads to the following equality by plugging the previous (45) in (44) and defining $z^{*}=$ $V-\\mu^{*}-\\omega$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\forall s\\in\\mathcal{B},\\quad z^{*}(s)=\\|z^{*}\\|_{*}\\nabla\\left\\|P\\right\\|(s)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "or ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\forall s\\in\\mathcal{B},\\quad V(s)-\\mu^{*}(s)=\\omega+\\lambda\\nabla\\left\\|P\\right\\|(s)\\dot{=}\\alpha_{P}^{\\lambda,\\omega}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "by letting $\\lambda=\\|z^{*}\\|_{*}\\in\\mathbb{R}^{+}$ . Note that here the hypothesis of 1 are use and especially separability is needed to ensure that for $s\\in{\\boldsymbol{B}}$ $\\nabla\\left\\|y\\right\\|=\\nabla\\left\\|P\\right\\|$ only depend on $P(s)$ and not on other coordinates, which is true form generalized $L_{p}$ norms. We can remark that $v-\\mu^{*}$ is $P$ dependent, but if $P$ is known, the best $\\mu^{*}$ is only determined by one 2 dimensional parameters $\\lambda=\\left.\\right\\vert\\vert v-\\mu^{*}-\\nu\\vert\\vert_{*}$ and $\\omega\\in\\mathbb{R}^{+}$ . Moreover, when $P$ is fixed, the scalar $\\omega$ is a constant is fully determined by $P$ \uff0c $v$ and $\\mu^{*}$ This is why the quantity defined $\\alpha_{P}^{\\lambda}$ varies through 2 parameter $\\lambda$ and $\\omega$ . Given this observation, we can rewrite the optimization problem as : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\mu\\geq0}{\\operatorname*{sup}}P(V-\\mu)-\\sigma\\mathrm{sp}(V-\\mu)_{*}=\\underset{\\mu_{P}^{\\lambda,\\omega}\\in\\mathcal{M}_{P}^{\\lambda,\\omega}}{\\operatorname*{sup}}P(V-\\mu_{P}^{\\lambda,\\omega})-\\sigma\\mathrm{sp}((V-\\mu_{P}^{\\lambda,\\omega}))_{*}}\\\\ &{=\\underset{\\alpha_{P}^{\\lambda,\\omega}\\in\\mathrm{A}_{P}^{\\lambda,\\omega}}{\\operatorname*{sup}}P[V]_{\\alpha_{P}^{\\lambda,\\omega}}-\\sigma\\mathrm{sp}([V]_{\\alpha_{P}^{\\lambda,\\omega}})_{*}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where we defined the maximization problem on $\\mu$ not in $\\mathbb{R}^{S}$ but at the optimal in the variational familydente $\\mathcal{M}_{P}^{\\lambda,\\omega}\\,=\\,\\{v\\,-\\,\\alpha_{P}^{\\lambda,\\omega},\\bar{(\\lambda,\\omega)}\\,\\in\\,\\mathbb{R}_{+}^{2},P\\,\\in\\,\\Delta(S)\\}$ Wecan rewrite thoptimizaion problem in terms of $\\alpha_{P}$ with $\\big[V\\big]_{\\alpha_{P}^{\\lambda,\\omega}}$ defined in 35. Contrary to the case, is not a scalar but $\\alpha_{P}^{\\lambda,\\omega}$ belongs to a variational family only determined by two parameter. Note that this lemma is still true writing subgradient and not gradient of $P$ . As we assume $C^{2}$ -regularity on norms, the subgradient space of the norm reduce to the singleton of the gradient in our case. $C^{2}$ smoothness will be needed in concentration part while it is possible to be more general in optimization lemmas. Note that for TV or L1, this lemma holds, but the vector \u03b1 Q educes to a positive scalar denoted \u03b1 which is equal to $\\|\\boldsymbol{v}-\\boldsymbol{\\mu}^{*}\\|_{\\infty}$ according to Iyengar [2005]. ", "page_idx": 19}, {"type": "text", "text": "Lemma 4 (Strong duality for the distance induced by the norm $\\bigstar|||\\bigstar\\bigstar$ in the $s$ -rectangular case.). Consider any probability vector $P^{\\pi}:=\\Pi^{\\pi}P\\in\\Delta_{s}$ for $P\\in\\Delta(S)^{A}$ , any fixed uncertainty level $\\tilde{\\sigma}$ and the uncertainty set $\\mathcal{U}_{||.||}^{5,\\widetilde{\\sigma}}(P)$ we abbreviate the subscript to use $\\mathcal{U}^{s,\\widetilde{\\sigma}}(P):=\\mathcal{U}_{||\\cdot||}^{s,\\widetilde{\\sigma}}(P)$ . Then for any vector $V\\in\\mathbb{R}^{S}$ obeying $V\\geq0,$ recalling the definition of $[V]_{\\alpha}$ in (35), one has ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\mathcal{P}\\in\\mathcal{U}^{s,\\sigma}(P)}\\mathcal{P}^{\\pi}V=\\sum_{a}\\pi(a|s)(\\Big(\\operatorname*{max}_{\\alpha_{P_{s a}}^{\\lambda,\\omega}\\in\\mathbb{A}_{P_{s a}}^{\\lambda,\\omega}}P_{s a}[V]_{\\alpha_{P_{s a}}^{\\lambda,\\omega}}-\\tilde{\\sigma}\\left\\lVert\\pi_{s}\\right\\rVert_{*}\\operatorname{sp}([V]_{\\alpha_{P_{s a}}^{\\lambda,\\omega}})_{*}\\Big).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "with the definition of $\\operatorname{sp}()_{*}$ in $C$ and where the variational family $\\mathrm{A}_{P}^{\\lambda,\\omega}$ is defined as : ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathtt{A}_{P}^{\\lambda,\\omega}=\\big\\{\\alpha\\in\\Big[0,1/(1-\\gamma)\\Big]^{S},\\alpha=\\omega+\\lambda|\\nabla\\left\\|P\\right\\||:=\\alpha_{P}^{\\lambda,\\omega}\\big\\}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "with $\\omega$ is the generalized mean defined as the argmin in the definition of the span semi norm in 2 and $\\lambda,\\omega$ a positive scalarMoreover, for $L_{1}$ or $T V$ , case, the vector $\\alpha_{P}^{\\lambda,\\omega}$ reduces to a 1 dimensional scalar such as $\\alpha\\in[0,1/(1-\\gamma)]$ ", "page_idx": 20}, {"type": "text", "text": "In the proof of the previous lemma, we decompose this problem $s$ -rectangular radius $\\tilde{\\sigma}$ into $s a$ rectangular sub-problem with respectively radius $\\sigma_{s a}$ ", "page_idx": 20}, {"type": "text", "text": "Proof. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\mathcal{P}^{\\pi}\\in\\mathcal{U}^{s,\\bar{\\sigma}}({\\cal P}^{\\pi})}{\\operatorname*{inf}}\\mathcal{P}^{\\pi}V=\\underset{\\{\\sigma_{s a}:\\|\\sigma_{s a}\\|\\leq\\tilde{\\sigma}\\}}{\\operatorname*{inf}}\\underset{\\mathcal{P}^{\\prime}\\in\\mathcal{U}^{s,\\sigma}({\\cal P}_{s a})}{\\operatorname*{inf}}\\sum_{a}\\pi(a|s)\\mathcal{P}^{\\prime}V}\\\\ &{\\overset{(a)}{=}\\sum_{a}\\pi(a|s)P_{s a}V+\\underset{\\{\\sigma_{s a}:\\|\\sigma_{s a}\\|\\leq\\tilde{\\sigma}\\}}{\\operatorname*{min}}\\sum_{a}\\pi(a|s)\\underset{\\{y:\\|y\\|\\leq\\sigma_{s a},1y=0,y\\geq-P_{s a}\\}}{\\operatorname*{min}}\\sum_{s^{\\prime}}y(s^{\\prime})V}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we use the change of variable $y(s^{\\prime})=\\mathcal{P}_{s a}(s^{\\prime})-P_{s a}(s^{\\prime})$ in (a). Then we case use the previous lemma for $s a$ rectangular assumption, Lemma 3. Then, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\{\\sigma_{s a}:\\|\\sigma_{s a}\\|\\leq\\tilde{\\sigma}\\}}{\\operatorname*{min}}\\sum_{a}\\pi(a|s)\\underset{\\{y,\\|y\\|\\leq\\sigma_{s,a},1y\\}=0,y\\geq-P_{s a}\\}{\\operatorname*{min}}\\sum_{s^{\\prime}}y(s^{\\prime})V}\\\\ &{=\\underset{\\{\\sigma_{s a}:\\|\\sigma_{a}\\|\\leq\\tilde{\\sigma}\\}}{\\operatorname*{min}}\\sum_{a}\\pi(a|s)\\underset{\\mu\\geq0}{\\operatorname*{max}}\\Big(-P_{s a}\\mu-\\sigma_{s a}\\mathrm{sp}(V-\\mu)_{*}\\Big)}\\\\ &{=\\Bigg(\\underset{a\\geq0}{\\sum}\\pi(a|s)\\underset{\\mu\\geq0}{\\operatorname*{max}}\\left\\{(-P_{s a}\\mu)-\\underset{\\{\\sigma_{s a}:\\|\\sigma_{s a}\\|\\leq\\tilde{\\sigma}\\}}{\\operatorname*{max}}\\sum_{a}\\pi(a|s)\\sigma\\mathrm{sp}(V-\\mu)_{*}\\right\\}\\Bigg)}\\\\ &{=\\sum_{a}\\pi(a|s)\\underset{\\mu\\geq0}{\\operatorname*{max}}\\,\\Big\\{(-P_{s a}\\mu)-\\tilde{\\sigma}\\,\\|\\pi_{s}\\|_{*}\\,\\mathrm{sp}(V-\\mu)_{*}\\Big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We can exchange the min and the max as we get concave-convex problems in $\\sigma$ and $\\mu$ in the second line according to minimax theorem [v. Neumann, 1928] and using Cauchy Swartz inequality which is attained in the last equality. Finally, we obtain: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\mathcal{P}\\in\\mathcal{U}^{s,\\tilde{\\sigma}}(P)}{\\operatorname*{inf}}\\mathcal{P}^{\\pi}V=\\displaystyle\\sum_{a}\\pi(a|s)\\Big(\\underset{\\mu\\geq0}{\\operatorname*{max}}P_{s a}(V-\\mu)-\\tilde{\\sigma}\\left\\lVert\\pi_{s}\\right\\rVert_{*}\\operatorname{sp}(V-\\mu)_{*}\\Big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\overset{(a)}{=}\\displaystyle\\sum_{a}\\pi(a|s)\\Big(\\underset{\\alpha_{P_{s a}}^{\\lambda,\\omega}\\in\\mathrm{A}_{P_{s a}}^{\\lambda,\\omega}}{\\operatorname*{max}}P_{s a}[V]_{\\alpha_{P_{s a}}^{\\lambda,\\omega}}-\\tilde{\\sigma}\\left\\lVert\\pi_{s}\\right\\rVert_{*}\\operatorname{sp}([V]_{\\alpha_{P_{s a}}^{\\lambda,\\omega}})_{*}\\Big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where in (a) we use the previous lemma for $s a-$ rectangular case. Note that as we are using $s a$ rectangular case, for $T V$ or $L_{1}$ , this lemma holds, but the vector $\\alpha_{P}^{\\lambda}$ reduces to a positive scalar denoted $\\alpha$ which is equal to $\\|\\boldsymbol{v}-\\boldsymbol{\\mu}^{*}\\|_{\\infty}$ . (See also Iyengar [2005]). ", "page_idx": 20}, {"type": "text", "text": "D Proof of the upper bound : Theorem 1 and 3 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "D.1 Technical lemmas ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We begin with a key lemma concerning the dynamic range of the robust value function $V^{\\pi,\\sigma}$ (cf. (7), which produces tighter control when $\\sigma$ is large; the proof is deferred to Appendix D.3.1. This lemma allows tighter control compared to Clavier et al. [2023]. ", "page_idx": 21}, {"type": "text", "text": "Lemma 5. In sa-rectangular case (see (3), for any nominal transition kernel $P\\in\\mathbb{R}^{S A\\times S}$ any fixed uncertainty level $\\sigma$ , and any policy $\\pi$ , its corresponding robust value function $V^{\\pi,\\sigma}$ (cf. (7)) satisfies ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname{sp}(V^{\\pi,\\sigma})_{\\infty}\\leq\\frac{1}{\\gamma\\operatorname*{max}\\{1-\\gamma,C_{g}\\sigma\\}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $C_{g}=1/(\\operatorname*{min}_{s}\\left\\|e_{s}\\right\\|)$ is a geometric constant depending on the geometry of the norm. For example, for $L_{p}$ , norms $p\\geq1$ \uff0c $C_{g}\\geq1$ which reduce the sample complexity. In $s$ -rectangular case, we obtain a slightly different lemma because of the dependency on $\\pi$ ", "page_idx": 21}, {"type": "text", "text": "Lemma 6. The infinite span semi norm can be controlled as follows for every $s$ in $s$ -rectanuglarcase (See (5)): ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{sp}(V^{\\pi,\\sigma})_{\\infty}\\leq\\frac{1}{\\gamma\\operatorname*{max}\\{1-\\gamma,\\|\\pi_{s}\\|_{*}\\,C_{g}\\tilde{\\sigma}\\}}\\leq\\frac{1}{\\gamma\\operatorname*{max}\\{1-\\gamma,\\operatorname*{min}_{s}\\|\\pi_{s}\\|_{*}\\,C_{g}\\tilde{\\sigma}\\}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\begin{array}{r}{C_{g}=\\frac{1}{\\operatorname*{min}_{s}\\left\\|e_{s}\\right\\|}}\\end{array}$ is a geometric costant depending onthe geometryof the normn These lemas are required to get tight bounds for the sample complexity. The main difference between $s a$ -and rectangular case is that we have an extra dependency on $\\left\\|\\pi_{s}\\right\\|_{*}$ , which represents how stochastic the policy can be in $s$ rectangular MDPs. ", "page_idx": 21}, {"type": "text", "text": "$P$ $0\\leq r\\leq1$ $\\pi$ $P_{\\pi}:=\\Pi^{\\pi}P$ $\\begin{array}{r}{0\\leq V^{\\pi,P}\\leq\\frac{1}{1-\\gamma}}\\end{array}$ (cf. (1)), one has for sa- and $s$ -rectangular assumptions. ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left(I-\\gamma P_{\\pi}\\right)^{-1}{\\sqrt{\\operatorname{Var}_{P_{\\pi}}(V^{\\pi,P})}}\\leq{\\sqrt{\\frac{8}{\\gamma^{2}(1-\\gamma)^{2}}\\mathrm{{sp}}(V^{\\pi,P})_{\\infty}}}1.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "See D.3.7 for the proof ", "page_idx": 21}, {"type": "text", "text": "D.2 Proof of Theorem 1 and Theorem 3 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The first decomposition of the proof of Theorem 1 and Theorem 3 Agarwal et al. [2020] while the argument needs essential adjustments in order to adapt to the robustness setting. One has by assumptions using any planner in empirical RMDPs : ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left\\|\\widehat{V}^{\\star,\\sigma}-\\widehat{V}^{\\widehat{\\pi},\\sigma}\\right\\|_{\\infty}\\leq\\varepsilon_{\\mathsf{o p t}},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "using previous inequality, performance gap $\\left\\|V^{\\star,\\sigma}-V^{\\widehat{\\pi},\\sigma}\\right\\|_{\\infty}$ , can be upper bounded using 3 steps. ", "page_idx": 21}, {"type": "text", "text": "First step: subdivide the performance gap in 3 terms.  We recall the definition of the optimal robust policy $\\pi^{\\star}$ with regard to $\\mathcal{M}_{\\sf r o b}$ and the optimal robust policy $\\widehat{\\pi}^{\\star}$ , the optimal robust value function $\\widehat{V}^{\\star,\\sigma}$ (resp. robust value function $\\widehat{Q}^{\\pi,\\sigma}$ w.r.t. $\\widehat{\\mathcal{M}}_{\\mathrm{rob}}$ Then, the performance gap $V^{\\star,\\sigma}-V^{\\widehat{\\pi},\\sigma}$ can be decomposed in one optimization term and two statistical error terms ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{V^{\\star,\\sigma}-V^{\\widehat{\\pi},\\sigma}=\\left(V^{\\pi^{\\star},\\sigma}-\\widehat{V}^{\\pi^{\\star},\\sigma}\\right)+\\left(\\widehat{V}^{\\pi^{\\star},\\sigma}-\\widehat{V}^{\\widehat{\\pi}^{\\star},\\sigma}\\right)+\\left(\\widehat{V}^{\\widehat{\\pi}^{\\star},\\sigma}-\\widehat{V}^{\\widehat{\\pi},\\sigma}\\right)+\\left(\\widehat{V}^{\\widehat{\\pi},\\sigma}-V^{\\widehat{\\pi},\\sigma}\\right)}&{{}}\\\\ {\\overset{(i)}{\\leq}\\left(V^{\\pi^{\\star},\\sigma}-\\widehat{V}^{\\pi^{\\star},\\sigma}\\right)+\\left(\\widehat{V}^{\\widehat{\\pi}^{\\star},\\sigma}-\\widehat{V}^{\\widehat{\\pi},\\sigma}\\right)+\\left(\\widehat{V}^{\\widehat{\\pi},\\sigma}-V^{\\widehat{\\pi},\\sigma}\\right)}&{{}}\\\\ {\\overset{(i i)}{\\leq}\\left(V^{\\pi^{\\star},\\sigma}-\\widehat{V}^{\\pi^{\\star},\\sigma}\\right)+\\varepsilon_{\\mathrm{opt}}+\\left(\\widehat{V}^{\\widehat{\\pi},\\sigma}-V^{\\widehat{\\pi},\\sigma}\\right)}&{{}}&{\\underset{(\\widetilde{V}^{\\widehat{\\pi},\\sigma}-V^{\\widehat{\\pi},\\sigma})}{\\leq}\\left(\\widehat{V}^{\\widehat{\\pi},\\sigma}-V^{\\widehat{\\pi},\\sigma}\\right)}&{{}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where (i) holds by $\\widehat{V}^{\\pi^{\\star},\\sigma}-\\widehat{V}^{\\widehat{\\pi}^{\\star},\\sigma}\\leq0$ since $\\widehat{\\pi}^{\\star}$ is the robust optimal policy for $\\widehat{\\mathcal{M}}_{\\sf r o b}$ and (i) comes from (55) and definition of optimization error. The proof aims to control the last remaining terms in (56) using concentration theory and sufficiently big number of step $N$ . To do so, we will consider a more general term $\\widehat{V}^{\\pi,\\sigma}-V^{\\pi,\\sigma}$ for any policy $\\pi$ even if control of these two terms slightly differ at the end. Using (32), it holds that for both $s a$ - and $s$ -rectangular assumptions: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{V}^{\\pi,\\sigma}-V^{\\pi,\\sigma}=r_{\\pi}+\\gamma\\widehat{\\underline{{P}}}^{\\pi,\\widehat{V}}\\widehat{V}^{\\pi,\\sigma}-\\big(r_{\\pi}+\\gamma\\underline{{P}}^{\\pi,V}V^{\\pi,\\sigma}\\big)}\\\\ &{\\qquad\\qquad=\\Big(\\gamma\\widehat{\\underline{{P}}}^{\\pi,\\widehat{V}}\\widehat{V}^{\\pi,\\sigma}-\\gamma\\underline{{P}}^{\\pi,\\widehat{V}}\\widehat{V}^{\\pi,\\sigma}\\Big)+\\Big(\\gamma\\underline{{P}}^{\\pi,\\widehat{V}}\\widehat{V}^{\\pi,\\sigma}-\\gamma\\underline{{P}}^{\\pi,V}V^{\\pi,\\sigma}\\Big)}\\\\ &{\\qquad\\qquad\\overset{(\\mathrm{i})}{\\leq}\\gamma\\Big(\\underline{{P}}^{\\pi,V}\\widehat{V}^{\\pi,\\sigma}-\\underline{{P}}^{\\pi,V}V^{\\pi,\\sigma}\\Big)+\\Big(\\gamma\\widehat{\\underline{{P}}}^{\\pi,\\widehat{V}}\\widehat{V}^{\\pi,\\sigma}-\\gamma\\underline{{P}}^{\\pi,\\widehat{V}}\\widehat{V}^{\\pi,\\sigma}\\Big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Where (i) holds because $\\underline{{P}}^{\\pi,\\widehat{V}}\\widehat{V}^{\\pi,\\sigma}\\,\\le\\,\\underline{{P}}^{\\pi,V}\\widehat{V}^{\\pi,\\sigma}$ because of the optimality of $\\underline{{P}}^{\\pi,\\widehat{V}}$ (see. (25). Factorizing terms leads to the following equation ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\widehat{V}^{\\pi,\\sigma}-V^{\\pi,\\sigma}\\leq\\gamma\\left(I-\\gamma\\underline{{{P}}}^{\\pi,V}\\right)^{-1}\\left(\\underline{{{\\widehat{P}}}}^{\\pi,\\widehat{V}}\\widehat{V}^{\\pi,\\sigma}-\\underline{{{P}}}^{\\pi,\\widehat{V}}\\widehat{V}^{\\pi,\\sigma}\\right)\\!.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "In the same manner, we can also obtain a lower bound of this quantity: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{V}^{\\pi,\\sigma}-V^{\\pi,\\sigma}=r_{\\pi}+\\gamma\\widehat{\\underline{{P}}}^{\\pi,\\widehat{V}}\\widehat{V}^{\\pi,\\sigma}-\\left(r_{\\pi}+\\gamma\\underline{{P}}^{\\pi,V}V^{\\pi,\\sigma}\\right)}\\\\ &{\\quad\\quad\\quad\\quad=\\left(\\gamma\\underline{{\\widehat{P}}}^{\\pi,\\widehat{V}}\\widehat{V}^{\\pi,\\sigma}-\\gamma\\underline{{P}}^{\\pi,\\widehat{V}}\\widehat{V}^{\\pi,\\sigma}\\right)+\\left(\\gamma\\underline{{P}}^{\\pi,\\widehat{V}}\\widehat{V}^{\\pi,\\sigma}-\\gamma\\underline{{P}}^{\\pi,V}V^{\\pi,\\sigma}\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\geq\\gamma\\left(\\underline{{P}}^{\\pi,\\widehat{V}}\\widehat{V}^{\\pi,\\sigma}-\\underline{{P}}^{\\pi,\\widehat{V}}V^{\\pi,\\sigma}\\right)+\\left(\\gamma\\underline{{\\widehat{P}}}^{\\pi,\\widehat{V}}\\widehat{V}^{\\pi,\\sigma}-\\gamma\\underline{{P}}^{\\pi,\\widehat{V}}\\widehat{V}^{\\pi,\\sigma}\\right)}\\\\ &{\\quad\\quad\\quad\\geq\\gamma\\left(I-\\gamma\\underline{{P}}^{\\pi,\\widehat{V}}\\right)^{-1}\\left(\\underline{{\\widehat{P}}}^{\\pi,\\widehat{V}}\\widehat{V}^{\\pi,\\sigma}-\\underline{{P}}^{\\pi,\\widehat{V}}\\widehat{V}^{\\pi,\\sigma}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Using both (57) and (58), we obtain infinite norm control: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\|\\widehat{V}^{\\pi,\\sigma}-V^{\\pi,\\sigma}\\right\\|_{\\infty}\\leq\\gamma\\operatorname*{max}\\Big\\{\\Big\\|\\left(I-\\gamma\\underline{{P}}^{\\pi,V}\\right)^{-1}\\left(\\underline{{\\widehat{P}}}^{\\pi,\\widehat{V}}\\widehat{V}^{\\pi,\\sigma}-\\underline{{P}}^{\\pi,\\widehat{V}}\\widehat{V}^{\\pi,\\sigma}\\right)\\Big\\|_{\\infty},}&{}\\\\ {\\Big\\|\\left(I-\\gamma\\underline{{P}}^{\\pi,\\widehat{V}}\\right)^{-1}\\left(\\underline{{\\widehat{P}}}^{\\pi,\\widehat{V}}\\widehat{V}^{\\pi,\\sigma}-\\underline{{P}}^{\\pi,\\widehat{V}}\\widehat{V}^{\\pi,\\sigma}\\right)\\Big\\|_{\\infty}\\Big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By decomposing the error in a symmetric way, he have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\|\\widehat{V}^{\\pi,\\sigma}-V^{\\pi,\\sigma}\\right\\|_{\\infty}\\leq\\gamma\\operatorname*{max}\\Big\\{\\Big\\|\\left(I-\\gamma\\widehat{\\underline{{P}}}^{\\pi,V}\\right)^{-1}\\left(\\widehat{\\underline{{P}}}^{\\pi,V}V^{\\pi,\\sigma}-\\underline{{P}}^{\\pi,V}V^{\\pi,\\sigma}\\right)\\Big\\|_{\\infty},}&{}\\\\ {\\left\\|\\left(I-\\gamma\\widehat{\\underline{{P}}}^{\\pi,\\widehat{V}}\\right)^{-1}\\!\\left(\\widehat{\\underline{{P}}}^{\\pi,V}V^{\\pi,\\sigma}-\\underline{{P}}^{\\pi,V}V^{\\pi,\\sigma}\\right)\\right\\|_{\\infty}\\Big\\}.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Armed with these inequalities, we can use concentration inequalities to upper bound the two remaining termns $\\left\\lVert\\widehat{V}^{\\pi^{\\star},\\sigma}-V^{\\pi^{\\star},\\sigma}\\right\\rVert_{\\infty}$ and $\\left\\|\\widehat{V}^{\\widehat{\\pi},\\sigma}-V^{\\widehat{\\pi},\\sigma}\\right\\|_{\\infty}$ in(56.Taking $\\pi={\\widehat{\\pi}}$ aplying(59)leadsto ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\|\\widehat{V}^{\\widehat{\\pi},\\sigma}-V^{\\widehat{\\pi},\\sigma}\\right\\|_{\\infty}\\leq\\gamma\\operatorname*{max}\\Big\\{\\Big\\|\\left(I-\\gamma\\underline{{P}}^{\\widehat{\\pi},\\widehat{V}}\\right)^{-1}\\left(\\underline{{\\widehat{P}}}^{\\widehat{\\pi},\\widehat{V}}\\widehat{V}^{\\widehat{\\pi},\\sigma}-\\underline{{P}}^{\\widehat{\\pi},\\widehat{V}}\\widehat{V}^{\\widehat{\\pi},\\sigma}\\right)\\Big\\|_{\\infty},}&{}\\\\ {\\Big\\|\\left(I-\\gamma\\underline{{P}}^{\\widehat{\\pi},V}\\right)^{-1}\\left(\\underline{{\\widehat{P}}}^{\\widehat{\\pi},\\widehat{V}}\\widehat{V}^{\\widehat{\\pi},\\sigma}-\\underline{{P}}^{\\widehat{\\pi},\\widehat{V}}\\widehat{V}^{\\widehat{\\pi},\\sigma}\\right)\\Big\\|_{\\infty}\\Big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Finally, $\\pi=\\pi^{\\star}$ , applying (60) gives us ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\|\\widehat{V}^{\\pi^{\\star},\\sigma}-V^{\\pi^{\\star},\\sigma}\\right\\|_{\\infty}\\leq\\gamma\\operatorname*{max}\\Big\\{\\Big\\|\\Big(I-\\gamma\\widehat{\\underline{{P}}}^{\\pi^{\\star},V}\\Big)^{-1}\\Big(\\widehat{\\underline{{P}}}^{\\pi^{\\star},V}V^{\\pi^{\\star},\\sigma}-\\underline{{P}}^{\\pi^{\\star},V}V^{\\pi^{\\star},\\sigma}\\Big)\\Big\\|_{\\infty},}&{}\\\\ {\\left\\|\\Big(I-\\gamma\\widehat{\\underline{{P}}}^{\\pi^{\\star},\\widehat{V}}\\Big)^{-1}\\Big(\\widehat{\\underline{{P}}}^{\\pi^{\\star},V}V^{\\pi^{\\star},\\sigma}-\\underline{{P}}^{\\pi^{\\star},V}V^{\\pi^{\\star},\\sigma}\\Big)\\Big\\|_{\\infty}\\Big\\}.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Note that to control $\\left\\|\\widehat{V}^{\\pi^{\\star},\\sigma}\\,-\\,V^{\\pi^{\\star},\\sigma}\\right\\|_{\\infty}$ we use decmpositionnot depending n $\\widehat{\\pi}$ for value function as $V^{\\pi^{\\star},\\sigma}$ is deterministic and fixed, allowing use of classical concentration analysis tools. This decomposition is the same for both $s a$ -rectangular and $s$ -rectangular case. ", "page_idx": 22}, {"type": "text", "text": "Second step: bound frst term and second term in (62) to control $\\|\\widehat{V}^{\\pi^{\\star},\\sigma}-V^{\\pi^{\\star},\\sigma}\\|_{\\infty}$ To control the two terms in (62), we use lemma 8 based Bernstein's concentration argument and whose proof is in Appendix D.3.3. ", "page_idx": 23}, {"type": "text", "text": "Lemma 8. For both $s a-$ and $s$ -rectangular setting, consider any $\\delta\\in(0,1)$ ,with probability $1-\\delta$ it holds: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left|\\widehat{\\underline{{P}}}^{\\pi^{\\star},V}V^{\\pi^{\\star},\\sigma}-\\underline{{P}}^{\\pi^{\\star},V}V^{\\pi^{\\star},\\sigma}\\right|\\leq2\\sqrt{\\frac{L}{N}}\\sqrt{\\operatorname{Var}_{P^{\\pi^{\\star}}}\\left(V^{\\star,\\sigma}\\right)}+\\frac{3L C_{S}\\left\\|\\boldsymbol1\\right\\|_{*}}{N(1-\\gamma)}1\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "with $L\\,=\\,2\\log(18\\left\\|1\\right\\|_{*}S A N/\\delta)$ andwhere $\\operatorname{Var}_{P^{\\pi^{\\star}}}(V^{\\star,\\sigma})$ is defined in (27). Moreover, for the specific case of $T V$ this lemma is true without the smoothness term $\\frac{3L C_{S}\\left\\|1\\right\\|_{*}}{N\\left(1-\\gamma\\right)}$ ", "page_idx": 23}, {"type": "text", "text": "Armed with the above lemma, now we control the first term on the right-hand side of (62) as follows: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(I-\\gamma\\widetilde{E}^{\\prime\\prime}\\right)^{-1}\\left(\\widehat{E}^{\\prime\\prime}\\nu^{\\prime\\prime}\\cdots\\rho-E^{\\prime\\prime}\\nu\\mathrm{F}^{\\prime\\prime\\prime}\\right)}\\\\ &{\\overset{(a)}{\\leq}\\left(I-\\gamma\\widetilde{E}^{\\prime\\prime}\\right)^{-1}\\left\\vert\\widetilde{E}^{\\prime\\prime}\\nu^{\\prime\\prime}\\cdots\\rho-E^{\\prime\\prime}\\nu^{\\prime\\prime}\\cdots\\rho\\right\\vert_{\\infty}}\\\\ &{\\overset{(b)}{\\leq}\\left(I-\\gamma\\widetilde{E}^{\\prime\\prime}\\right)^{-1}\\left(2\\sqrt{\\overline{{X}}}\\mathrm{\\it{Ver}}_{p}(V^{\\ast})\\cdot3I C_{S}\\left\\vert1\\right\\vert_{\\infty}\\right)}\\\\ &{\\leq\\left(I-\\gamma\\widetilde{E}^{\\prime\\prime}\\right)^{-1}\\frac{3\\,L C_{S}\\left\\vert1\\right\\vert\\,\\mathrm{l}\\,\\mathrm{fl}\\ast\\mathrm{1}}{N(1-\\gamma)}+\\underline{{2\\sqrt{\\overline{{X}}}\\left(I-\\gamma\\widetilde{E}^{\\prime}\\right)^{-1}}}\\sqrt{\\mathrm{\\mathop{Var}}_{\\mathbb{Z}}\\mathrm{\\it{V}}^{\\prime\\prime}\\cdots\\mathrm{\\it{V}}(V^{\\ast})}}\\\\ &{\\quad+\\underset{\\leq}{\\underbrace{2\\sqrt{\\overline{{X}}}}}\\left(I-\\gamma\\widetilde{E}^{\\prime\\prime}\\right)^{-1}\\sqrt{\\mathrm{\\mathop{Var}}_{p}(V^{\\ast},(V^{\\ast},\\sigma)-\\mathrm{\\mathop{Var}}_{\\mathbb{Z}};\\cdot\\mathrm{\\it{V}}(V^{\\ast},\\sigma))}}\\\\ &{\\quad+\\underset{\\leq}{\\underbrace{2\\sqrt{\\overline{{X}}}}}\\left(I-\\gamma\\widetilde{E}^{\\prime\\prime}\\right)^{-1}\\left(\\sqrt{\\mathrm{\\mathop{Var}}_{p}(V^{\\ast},(V^{\\ast},\\sigma)-\\mathrm{\\mathop{Var}}_{\\mathbb{Z}};\\cdot\\mathrm{\\it{V}}(V^{\\ast},\\sigma))}\\right)}\\\\ &{\\quad+\\underset{\\leq}{\\underbrace{2\\sqrt{\\overline{{X}}}}}\\left(I-\\gamma\\widetilde{E}^{\\prime\\prime}\\right)^{-1}\\left(\\sqrt{\\mathrm{\\mathop{Var}}_{p}(V^{\\ast},(V^{\\ast},\\sigma))}-\\sqrt{\\mathrm{\\mathop{Var}}_{\\mathbb{Z}}\\mathrm{\\it{V}}(V^{\\ast},\\sigma)}\\right),} \n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where (a) holds as the matrix $\\left(I-\\gamma\\underline{{\\widehat{P}}}^{\\pi^{\\star},V}\\right)^{-1}$ is positive definite, (b) holds due to Lemma 8, and the last point holds from the foilowing decomposition for variance and triangular inequality ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sqrt{\\mathrm{Var}_{P^{\\pi^{\\star}}}(V^{\\star,\\sigma})}=\\left(\\sqrt{\\mathrm{Var}_{P^{\\pi^{\\star}}}(V^{\\star,\\sigma})}-\\sqrt{\\mathrm{Var}_{\\widehat{P}^{\\pi^{\\star}}}(V^{\\star,\\sigma})}\\right)+\\sqrt{\\mathrm{Var}_{\\widehat{P}^{\\pi^{\\star}}}(V^{\\star,\\sigma})}}\\\\ &{\\leq\\left(\\sqrt{\\mathrm{Var}_{P^{\\pi^{\\star}}}(V^{\\star,\\sigma})}-\\sqrt{\\mathrm{Var}_{\\widehat{P}^{\\pi^{\\star}}}(V^{\\star,\\sigma})}\\right)}\\\\ &{+\\sqrt{\\left|\\mathrm{Var}_{\\widehat{P}^{\\pi^{\\star}}}(V^{\\star,\\sigma})-\\mathrm{Var}_{\\widehat{\\underline{{P}}}^{\\pi^{\\star},V}}(V^{\\star,\\sigma})\\right|}+\\sqrt{\\mathrm{Var}_{\\widehat{\\underline{{P}}}^{\\pi^{\\star},V}}(V^{\\star,\\sigma})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Finally, the fact that $\\underline{{\\widehat{P}}}^{\\pi^{\\star},V}$ is a stochastic matrix, so ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\Big(I-\\gamma\\widehat{\\underline{P}}^{\\pi^{\\star},V}\\Big)^{-1}1=\\Big(I+\\sum_{t=1}^{\\infty}\\gamma^{t}\\Big(\\widehat{\\underline{P}}^{\\pi^{\\star},V}\\Big)^{t}\\Big)1\\leq\\frac{1}{1-\\gamma}1.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Armed with these inequalities, the three terms $\\mathcal{R}_{1},\\mathcal{R}_{2},\\mathcal{R}_{3}$ in (64) can be controlled separately. ", "page_idx": 23}, {"type": "text", "text": "\u00b7Consider $\\mathcal{R}_{1}$ . We first introduce the following lemma, whose proof is postponed to Appendix D.3.4. ", "page_idx": 23}, {"type": "text", "text": "Lemma 9. Consider any $\\delta\\in(0,1)$ . With probability at least $1-\\delta$ one has ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(I-\\gamma\\underline{{\\widehat{P}}}^{\\pi^{\\star},V}\\right)^{-1}\\!\\sqrt{\\!\\operatorname{Var}_{\\underline{{\\widehat{P}}}^{\\pi^{\\star},V}}(V^{\\star,\\sigma})}\\leq4\\sqrt{\\frac{\\left(1+\\left(\\sqrt{\\frac{L}{(1-\\gamma)^{2}N}}+\\frac{C_{S}\\|1\\|_{*}L}{N(1-\\gamma)}\\right)\\right)}{\\gamma^{3}(1-\\gamma)^{2}\\operatorname*{max}\\{1-\\gamma,C_{g}\\sigma\\}}}1}\\\\ &{\\leq4\\sqrt{\\frac{\\left(1+\\left(\\sqrt{\\frac{L}{(1-\\gamma)^{2}N}}+\\frac{C_{S}\\|1\\|_{*}L}{N(1-\\gamma)}\\right)\\right)}{\\gamma^{3}(1-\\gamma)^{3}}}1}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "with $\\begin{array}{r}{L=2\\log(\\frac{18\\|1\\|_{*}S A N}{\\delta})}\\end{array}$ in the sa-rectangular case. In the $s$ -rectangular case, it holds: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(I-\\gamma\\underline{{\\widehat{P}}}^{\\pi^{\\star},V}\\right)^{-1}\\sqrt{\\mathrm{Var}_{\\widehat{\\underline{{P}}}^{\\pi^{\\star},V}}(V^{\\star,\\sigma})}\\le}&{\\ 4\\sqrt{\\frac{\\left(1+\\left(\\sqrt{\\frac{L}{(1-\\gamma)^{2}N}}+\\frac{C_{S}\\|1\\|_{*}L}{N(1-\\gamma)}\\right)\\right)}{\\gamma^{3}(1-\\gamma)^{2}\\operatorname*{max}\\{1-\\gamma,C_{g}\\widetilde{\\sigma}\\operatorname*{min}_{s}\\|\\pi_{s}\\}|_{*}}}1}\\\\ &{\\le4\\sqrt{\\frac{\\left(1+\\left(\\sqrt{\\frac{L}{(1-\\gamma)^{2}N}}+\\frac{C_{S}\\|1\\|_{*}L}{N(1-\\gamma)}\\right)\\right)}{\\gamma^{3}(1-\\gamma)^{3}}}1}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Using Lemma 9 and inserting back to (64) gives in $s a$ -rectangular case ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}_{1}=2\\sqrt{\\displaystyle\\frac{L}{N}}\\Big(I-\\gamma\\widehat{\\underline{{P}}}^{\\pi^{\\star},V}\\Big)^{-1}\\sqrt{\\mathrm{Var}_{\\widehat{\\underline{{P}}}^{\\pi^{\\star},V}}(V^{\\star,\\sigma})}}\\\\ &{\\quad\\leq8\\sqrt{\\displaystyle\\frac{L}{\\gamma^{3}(1-\\gamma)^{2}\\operatorname*{max}\\{1-\\gamma,C_{g}\\sigma\\}N}\\bigg(1+\\sqrt{\\displaystyle\\frac{L}{(1-\\gamma)^{2}N}}+\\frac{C_{S}\\left\\|1\\right\\|_{*}L}{N(1-\\gamma)}\\bigg)}1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "\u00b7 Consider $\\mathcal{R}_{2}$ . First, denote $V^{\\prime}:=V^{\\star,\\sigma}-\\eta1\\;\\eta\\in\\mathbb{R}$ , by Lemma 5, we have for any $\\pi$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n0\\leq\\operatorname*{min}_{\\eta}\\|V-\\eta1\\|_{\\infty}\\leq\\frac{1}{\\gamma\\operatorname*{max}\\{1-\\gamma,C_{g}\\sigma\\}}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for $s a$ -rectangular case or in $s$ -rectangular we obtain ", "page_idx": 24}, {"type": "equation", "text": "$$\n0\\leq\\operatorname*{min}_{\\eta}\\|V-\\eta1\\|_{\\infty}\\leq\\frac{1}{\\gamma\\operatorname*{max}\\{1-\\gamma,\\tilde{\\sigma}C_{g}\\,\\|\\pi_{s}\\|_{*}\\}}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "by the definition of the span semi norm. Moreover, we can use Holder with $L_{1}$ and $L_{\\infty}$ we haveforboth $s a$ and $s$ -rectangular case to as it holds that: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\mathsf{V a r}_{\\widetilde{P}_{s,a}}(V^{\\star,\\sigma})-\\mathsf{V a r}_{P_{s,a}}(V^{\\star,\\sigma})\\right|=\\left|\\mathsf{V a r}_{\\widetilde{P}_{s,a}}(V^{\\prime})-\\mathsf{V a r}_{P_{s,a}}(V^{\\prime})\\right|}\\\\ &{\\leq\\left\\|\\widetilde{P}_{s,a}-P_{s,a}\\right\\|_{1}\\left\\|V^{\\prime}\\right\\|_{\\infty}^{2}\\overset{a}{\\leq}\\frac{\\sigma_{1}}{(\\gamma^{2}(\\operatorname*{max}\\left(1-\\gamma\\right),C_{g}\\sigma)^{2}}}\\\\ &{\\leq\\frac{1}{\\gamma^{2}\\operatorname*{max}\\{(1-\\gamma),\\sigma C_{g}\\}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "In the first inequality, we use $\\left\\|V^{\\prime}\\right\\|_{\\infty}^{2}=\\left\\|V^{\\prime2}\\right\\|_{\\infty}$ and and we use Lemma 5 in (a) where $C_{g}\\sigma=\\sigma_{1}$ With the same arguments for $s$ -rectangular, we obtain for $V^{\\prime}:=V^{\\star,\\sigma}-\\eta1,\\eta\\in\\mathbb{R},$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Big|\\Pi^{\\pi^{*}}\\Big(\\mathsf{V a r}_{\\widetilde{P}_{s}}\\big(V^{\\star,\\sigma}\\big)-\\mathsf{V a r}_{P_{s}}\\big(V^{\\star,\\sigma}\\big)\\Big)\\Big|=\\Big|\\Pi^{\\pi^{*}}\\Big(\\mathsf{V a r}_{\\widetilde{P}_{s}}\\big(V^{\\prime}\\big)-\\mathsf{V a r}_{P_{s}}\\big(V^{\\prime}\\big)\\Big)\\Big|}\\\\ &{\\le\\Big|\\displaystyle\\sum_{a}\\pi^{*}(a|s)\\displaystyle\\sum_{s^{\\prime}}(\\widetilde{P}_{s}(s^{\\prime},a)-P_{s}(s^{\\prime},a))V^{\\prime}\\big(s^{\\prime}\\big)^{2}\\Big|}\\\\ &{\\le\\|V^{\\prime}\\|_{\\infty}^{2}\\displaystyle\\sum_{a}\\sum_{s^{\\prime}}\\pi^{*}(a|s)(\\widetilde{P}_{s}(s^{\\prime},a)-P_{s}(s^{\\prime},a))\\overset{a}{\\le}\\|V^{\\prime}\\|_{\\infty}^{2}\\,\\widetilde{\\sigma}\\,\\|\\pi_{s}^{*}\\|_{*}\\,C_{g}^{s}\\boldsymbol{1}}\\\\ &{\\overset{b}{\\le}\\displaystyle\\frac{\\widetilde{\\sigma}C_{g}^{s}\\,\\|\\pi_{s}^{*}\\|_{*}\\,\\widetilde{\\sigma}\\,C_{g}^{\\dagger}\\|_{\\infty}}{\\gamma\\,\\|\\pi_{s}^{*}\\|_{*}\\,\\widetilde{\\sigma}\\,C_{g}^{\\dagger}}\\,{\\le}\\,\\displaystyle\\|V^{\\prime}\\|_{1}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where (a) comes $\\mathrm{Eq}\\ 126.$ (b) comes lemma 6 or more precisely eq (139). Then, taking the supover $s$ in the previous equations, it holds ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\left\\vert\\Pi^{\\pi^{\\star}}\\left(\\mathsf{V a r}_{\\widetilde{P}_{s}}(V^{\\star,\\sigma})-\\mathsf{V a r}_{P_{s}}(V^{\\star,\\sigma})\\right)\\right\\vert\\leq\\,\\frac{\\operatorname*{inf}_{\\eta\\in\\mathbb{R}^{+}}\\left\\|V-\\eta1^{\\prime}\\right\\|}{\\gamma}_{1}}\\\\ &{}&{\\qquad\\qquad\\qquad\\qquad\\qquad\\le\\frac{1}{\\gamma^{2}\\widetilde{\\sigma}\\operatorname*{min}_{s}\\left\\|\\pi_{s}^{*}\\right\\|_{*}C_{g}}1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Applying the previous inequality, it holds in $s a$ -rectangular case: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\ensuremath{\\mathcal R}}_{2}=2\\sqrt{\\frac{L}{N}}\\Big(I-\\gamma\\hat{\\ensuremath{\\mathcal E}}^{\\pi^{*},V}\\Big)^{-1}\\sqrt{\\Big|\\mathrm{Var}_{\\hat{p}^{*}}\\cdot(V^{*,\\sigma})-\\mathrm{Var}_{\\hat{\\ensuremath{\\mathcal P}}^{*,\\nu}}(V^{*,\\sigma})\\Big|}}\\\\ &{\\quad=2\\sqrt{\\frac{L}{N}}\\Big(I-\\gamma\\hat{\\ensuremath{\\mathcal E}}^{\\pi^{*},V}\\Big)^{-1}\\sqrt{\\big|\\mathrm{I}\\pi^{*}\\left(\\mathrm{Var}_{\\hat{p}^{*}}(V^{*,\\sigma})-\\mathrm{Var}_{\\hat{p}^{*}\\times,\\nu}(V^{*,\\sigma})\\right)\\big|}}\\\\ &{\\quad\\le2\\sqrt{\\frac{L}{N}}\\Big(I-\\gamma\\hat{\\ensuremath{\\mathcal P}}^{*,V}\\Big)^{-1}\\sqrt{\\|\\mathrm{Var}_{\\hat{p}^{*}}(V^{*,\\sigma})-\\mathrm{Var}_{\\hat{\\ensuremath{\\mathcal P}}^{*,\\nu}}(V^{*,\\sigma})\\|_{\\infty}}}\\\\ &{\\quad\\le2\\sqrt{\\frac{L}{N}}\\Big(I-\\gamma\\hat{\\ensuremath{\\mathcal P}}^{*^{*},V}\\Big)^{-1}\\sqrt{\\frac{1}{\\gamma^{2}\\operatorname*{max}\\{1-\\gamma,C_{g}\\sigma\\}}}}\\\\ &{\\quad\\le4\\sqrt{\\frac{L}{\\gamma^{2}(1-\\gamma)^{2}\\operatorname*{max}\\{1-\\gamma,C_{g}\\sigma\\}N}}1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the last inequality uses $\\begin{array}{r}{\\Big(I-\\gamma\\widehat{\\underline{P}}^{\\pi^{\\star},V}\\Big)^{-1}1\\leq\\frac{1}{1-\\gamma}1}\\end{array}$ (cf. (65)) for $s a$ -rectangular. In the $s$ -rectangular case, we obtain a different result as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}_{2}=2\\sqrt{\\displaystyle\\frac{L}{N}}\\Big(I-\\gamma\\widehat{\\underline{{P}}}^{\\pi^{*},V}\\Big)^{-1}\\sqrt{\\Big|\\mathrm{Var}_{\\widehat{P}^{\\pi^{*}}}\\big(V^{\\star,\\sigma}\\big)-\\mathrm{Var}_{\\widehat{\\underline{{P}}}^{\\pi^{*},V}}\\big(V^{\\star,\\sigma}\\big)\\Big|}}\\\\ &{\\quad=2\\sqrt{\\displaystyle\\frac{L}{N}}\\Big(I-\\gamma\\widehat{\\underline{{P}}}^{\\pi^{*},V}\\Big)^{-1}\\sqrt{\\Big|\\mathrm{II}^{\\pi^{*}}\\left(\\mathrm{Var}_{\\widehat{P}^{0}}\\big(V^{\\star,\\sigma}\\big)-\\mathrm{Var}_{\\widehat{P}^{\\pi^{*},V}}\\big(V^{\\star,\\sigma}\\big)\\right)\\Big|}}\\\\ &{\\quad\\le2\\sqrt{\\displaystyle\\frac{L}{N}}\\Big(I-\\gamma\\widehat{\\underline{{P}}}^{\\pi^{*},V}\\Big)^{-1}\\sqrt{\\displaystyle\\frac{1}{\\gamma^{2}\\operatorname*{max}\\{1-\\gamma,\\operatorname*{min}_{s}\\|\\pi_{s}^{*}\\|_{\\infty}C_{g}\\bar{\\sigma}\\}}}^{1}}\\\\ &{\\quad\\le2\\sqrt{\\displaystyle\\frac{L}{\\gamma^{2}(1-\\gamma)^{2}\\operatorname*{max}\\{1-\\gamma,\\operatorname*{min}_{s}\\|\\pi_{s}^{*}\\|_{\\infty}\\tilde{\\sigma}C_{g}\\}N}}1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "\u00b7 Consider ${\\mathcal{R}}_{3}$ . The following lemma plays an important role. ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Applying Lemma 2 and using $\\pi=\\pi^{\\star}$ and $V=V^{\\star,\\sigma}$ , it holds ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sqrt{\\mathrm{Var}_{P^{\\pi^{\\star}}}(V^{\\star,\\sigma})}-\\sqrt{\\mathrm{Var}_{\\widehat{P}^{\\pi^{\\star}}}(V^{\\star,\\sigma})}\\leq\\sqrt{\\frac{2\\|V^{\\star,\\sigma}\\|_{\\infty}^{2}\\log(\\frac{2S A}{\\delta})}{N}}1,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which can be inserted in (64) to gives ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}_{3}=2\\sqrt{\\displaystyle\\frac{L}{N}}\\Big(I-\\gamma\\widehat{\\underline{{P}}}^{\\pi^{\\star},V}\\Big)^{-1}\\left(\\sqrt{\\mathrm{Var}_{P^{\\pi^{\\star}}}(V^{\\star,\\sigma})}-\\sqrt{\\mathrm{Var}_{\\widehat{P}^{\\pi^{\\star}}}(V^{\\star,\\sigma})}\\right)}\\\\ &{\\quad\\le\\displaystyle\\frac{4}{(1-\\gamma)}\\frac{\\log(\\frac{S A N}{\\delta})\\|[V^{\\star,\\sigma}\\|_{\\infty}}{N}1\\le\\frac{4L}{(1-\\gamma)^{2}N}1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the last line uses $\\begin{array}{r}{\\Big(I-\\gamma\\widehat{\\underline{P}}^{\\pi^{\\star},V}\\Big)^{-1}1\\leq\\frac{1}{1-\\gamma}1}\\end{array}$ (cf. (65)). ", "page_idx": 25}, {"type": "text", "text": "Finally, inserting the results of $\\mathcal{R}_{1}$ in (66), $\\mathcal{R}_{2}$ in (76), ${\\mathcal{R}}_{3}$ in (79), and (65) back into (64) gives ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Big(I-\\gamma\\widetilde{\\ensuremath{\\gamma}}^{*\\nu,\\gamma}\\Big)^{-1}\\Bigg(\\widetilde{\\ensuremath{\\gamma}}^{*\\nu,\\gamma}V^{*\\nu,\\cdot\\rho}-\\underline{{P}}^{*\\nu,\\gamma}V^{*\\nu,\\cdot\\rho}\\Big)}\\\\ &{\\le8\\sqrt{\\frac{1}{\\gamma^{3}(1-\\gamma)^{2}\\operatorname*{max}\\{1-\\gamma,C_{\\eta}\\circ\\overline{{\\mathcal{A}}}\\}^{N}}\\Bigg(1+\\sqrt{\\frac{L}{(1-\\gamma)^{2}\\mathcal{N}}}+\\frac{C_{\\delta}\\|\\|_{\\frac{1}{\\|\\tilde{\\ensuremath{\\Gamma}}}}\\,L\\Big)}{N(1-\\gamma)})1+\\frac{3L C_{S}\\|\\|_{\\frac{1}{\\mathcal{A}}}}{N(1-\\gamma)^{2}}}}\\\\ &{\\quad+2\\sqrt{\\frac{2L}{\\gamma^{2}(1-\\gamma)^{2}\\operatorname*{max}\\{1-\\gamma,C_{\\eta}\\circ\\overline{{\\mathcal{A}}}\\}^{N}}1+\\frac{4L}{(1-\\gamma)^{2N}}}}\\\\ &{\\le10\\sqrt{\\frac{2L}{\\gamma^{3}(1-\\gamma)^{2}\\operatorname*{max}\\{1-\\gamma,C_{\\eta}\\circ\\mathcal{A}\\}^{N}}\\Bigg(1+\\sqrt{\\frac{L}{(1-\\gamma)^{2N}}}+\\frac{C_{S}\\|\\|_{\\frac{1}{\\|\\tilde{\\ensuremath{\\Gamma}}}}\\,L\\Big)}{N(1-\\gamma)^{2}}}\\mathrm{1+\\frac{4L}{(1-\\gamma)^{2N}}}}\\\\ &{+\\frac{3L C_{S}\\|\\|_{\\frac{1}{\\mathcal{A}}}}{N(1-\\gamma)^{2}}}\\\\ &{\\le16\\sqrt{\\frac{L(1+\\frac{C_{S}\\|\\|_{\\tilde{\\ensuremath{\\Gamma}}}}{\\gamma})}{(1-\\gamma)^{2}\\operatorname*{max}\\{1-\\gamma,C_{\\eta}\\circ\\mathcal{A}\\}^{N}}}\\,1+\\frac{7L C_{S}\\|\\|_{\\frac{1}{\\mathcal{A}}}}{N(1-\\gamma)^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where theas quality ly $\\gamma\\geq\\frac{1}{4}$ and letng $\\begin{array}{r}{N\\ge\\frac{L}{(1-\\gamma)^{2}}}\\end{array}$ We have the same esult for $s$ -rectangular, replacing, $\\operatorname*{max}\\lbrace1-\\gamma,C_{g}\\sigma\\rbrace$ by $\\mathrm{max}\\{1-\\gamma,\\operatorname*{min}_{s}\\|\\pi_{s}^{*}\\|_{*}\\,\\tilde{\\sigma}C_{g}\\}$ ", "page_idx": 26}, {"type": "text", "text": "Now we are ready to control second term in (62) to control $\\|\\widehat{V}^{\\pi^{\\star},\\sigma}\\--V^{\\pi^{\\star},\\sigma}\\|_{\\infty}$ .To proced, applying Lemma 8 on the second term of the right-hand side of (62) leads to ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Big(I-\\gamma\\widehat{E}^{*\\,\\prime\\,\\prime}\\Big)^{-1}\\Big({\\widehat Z}^{*\\,\\prime\\,\\prime}V^{*\\,\\prime\\,\\prime}-{\\widehat Z}^{*\\,\\prime\\,\\prime}V^{*\\,\\prime\\,\\prime}\\Big)}\\\\ &{\\le\\Big(I-\\gamma\\widehat{E}^{*\\,\\prime\\,\\prime}\\Big)^{-1}\\left(2\\sqrt{\\frac{I}{N}}\\sqrt{\\nabla\\times\\rho_{\\mathbb{R}^{*}}(V^{*\\,\\prime}\\times\\rho^{*})}+\\frac{3L C_{S}}{N}\\|\\Pi_{\\infty}\\right)}\\\\ &{\\le\\Big(I-\\gamma\\widehat{E}^{*\\,\\prime\\,\\prime}\\Big)^{-1}\\frac{I\\!C_{S}}{N}\\|\\Pi_{-1}\\Big(\\cdot\\Big)^{-2}\\sqrt{\\frac{I}{N}}\\Big(I-\\gamma\\widehat{E}^{*\\,\\prime}\\Big)^{-1}\\sqrt{\\nabla\\alpha_{E}}{\\widehat z}^{*\\,\\prime\\,\\prime}\\Big)}\\\\ &{\\overset{\\ge}\\sqrt{\\frac{I}{N}}\\Big(I-\\gamma\\widehat{E}^{*\\,\\prime}\\Big)^{-1}\\left(\\sqrt{\\nabla\\alpha_{E}}\\cdot\\gamma^{*\\,\\prime\\,\\prime}\\big(V^{*\\,\\prime}\\!-\\!\\frac{\\widehat{V}^{*\\,\\prime}\\cdot\\sigma^{*}}{\\sqrt{N}}\\big)\\right)}\\\\ &{\\ +\\underbrace{2\\sqrt{\\frac{I}{N}}\\Big(I-\\gamma\\widehat{E}^{*\\,\\prime}\\Big)^{-1}\\left(\\sqrt{\\nabla\\alpha_{E}}\\cdot(V^{*\\,\\prime\\,\\prime})-\\nabla\\alpha_{E}\\cdot\\gamma^{*\\,\\prime}\\big(V^{*\\,\\prime}\\!-\\!\\frac{\\widehat{V}^{*\\,\\prime}}{\\sqrt{N}}\\big)\\right)}_{=N_{\\phi}}}\\\\ &{\\ +\\underbrace{2\\sqrt{\\frac{I}{N}}\\Big(I-\\gamma\\widehat{E}^{*\\,\\prime}\\Big)^{-1}\\left(\\sqrt{\\nabla\\alpha_{E}}\\cdot(V^{*\\,\\prime\\,\\prime})-\\sqrt{\\nabla\\alpha_{E}}\\cdot(V^{*\\,\\prime}\\!-\\!\\frac{\\widehat{V}^{*\\,\\prime}}{\\sqrt{N}}\\Big)\\right)}_{=N_{\\phi}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We now bound the above four terms $\\mathcal{R}_{4},\\mathcal{R}_{5},\\mathcal{R}_{6},\\mathcal{R}_{7}$ separately. ", "page_idx": 26}, {"type": "text", "text": "\u00b7 Using Lemma 7 with $P\\;=\\;\\widehat{P}^{\\pi^{\\star},\\widehat{V}}$ $\\pi\\;=\\;\\pi^{\\star}$ and $V\\;=\\;\\widehat{V}^{\\pi^{\\star},\\sigma}$ which follow $\\widehat{V}^{\\pi^{\\star},\\sigma}\\;=\\;$ $r_{\\pi^{\\star}}+\\gamma\\underline{{\\widehat{P}}}^{\\pi^{\\star},\\widehat{V}}\\widehat{V}^{\\pi^{\\star},\\sigma}$ \\*,, andin view of (65),the temR in (83) canbe controlled as follows: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathscr R}_{4}=2\\sqrt{\\displaystyle\\frac{L}{N}}\\Big(I-\\gamma\\displaystyle\\widehat{\\underline{{P}}}^{\\pi^{\\star},\\widehat{V}}\\Big)^{-1}\\sqrt{\\mathrm{Var}_{\\widehat{\\underline{{P}}}^{\\pi^{\\star},\\widehat{V}}}(\\widehat{V}^{\\pi^{\\star},\\sigma})}}\\\\ &{\\quad\\leq2\\sqrt{\\displaystyle\\frac{L}{N}}\\sqrt{\\frac{8\\operatorname*{min}\\{\\mathrm{sp}(\\widehat{V}^{\\pi^{\\star},\\sigma})_{*},1/(1-\\gamma)\\}}{\\gamma^{2}(1-\\gamma)^{2}}}1}\\\\ &{\\quad\\leq8\\sqrt{\\frac{L}{\\gamma^{3}(1-\\gamma)^{2}\\operatorname*{max}\\{1-\\gamma,C_{g}\\sigma\\}N}}1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the last inequality is due to Lemma 5 for $s a$ -rectangular case and with the same quantity replacing $\\operatorname*{max}\\{1-\\gamma,\\sigma\\}$ by $\\operatorname*{max}\\{1-\\gamma,\\operatorname*{min}_{s}\\left\\|\\pi_{s}^{*}\\right\\|_{*}\\tilde{\\sigma}\\}$ in the $s-$ rectangular case. ", "page_idx": 27}, {"type": "text", "text": "\u00b7 For bounding $\\mathcal{R}_{5}$ , we can simply use (65)) to get ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}_{5}=2\\sqrt{\\displaystyle\\frac{L}{N}}\\Big(I-\\gamma\\widehat{\\underline{{P}}}^{\\pi^{\\star},\\widehat{V}}\\Big)^{-1}\\sqrt{\\operatorname{Var}_{\\widehat{\\underline{{P}}}^{\\pi^{\\star},\\widehat{V}}}(V^{\\pi^{\\star},\\sigma}-\\widehat{V}^{\\pi^{\\star},\\sigma})}}\\\\ &{\\quad\\leq2\\sqrt{\\displaystyle\\frac{L}{(1-\\gamma)^{2}N}}\\left\\|V^{\\star,\\sigma}-\\widehat{V}^{\\pi^{\\star},\\sigma}\\right\\|_{\\infty}1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal{R}_{5}\\leq2\\sqrt{\\frac{L}{(1-\\gamma)^{2}N}}\\left\\|V^{\\star,\\sigma}-\\widehat{V}^{\\pi^{\\star},\\sigma}\\right\\|_{\\infty}1.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "\u00b7 The term ${\\mathcal{R}}_{6}$ can upper bounded as (76) as follows: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal{R}_{6}\\leq2\\sqrt{\\frac{2L}{\\gamma^{2}(1-\\gamma)^{2}\\operatorname*{max}\\{1-\\gamma,C_{g}\\sigma\\}N}}1.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "for $s a$ -rectangular case and with the same quantity replacing $\\operatorname*{max}\\lbrace1-\\gamma,C_{g}\\sigma\\rbrace$ by $\\mathrm{max}\\{1-$ $\\gamma,\\operatorname*{min}_{s}\\left\\|\\pi_{s}^{*}\\right\\|_{*}\\tilde{\\sigma}C_{g}\\right\\}$ in the $s-$ rectangular case. ", "page_idx": 27}, {"type": "text", "text": "\u00b7Finally, ${\\mathcal{R}}_{7}$ can be controlled the same as (79) shown below: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal{R}_{7}\\leq\\frac{4L}{(1-\\gamma)^{2}N}1.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Combining the reults in (84),(86),(87), and (8) and insrting backto (83) Ieads to for $\\begin{array}{r}{N\\ge\\frac{L}{(1-\\gamma)^{2}}}\\end{array}$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(I-\\gamma\\widehat{\\underline{{P}}}^{\\pi^{\\star},\\widehat{V}}\\right)^{-1}\\left(\\widehat{\\underline{{P}}}^{\\pi^{\\star},V}V^{\\pi^{\\star},\\sigma}-\\underline{{P}}^{\\pi^{\\star},V}V^{\\pi^{\\star},\\sigma}\\right)\\leq8\\sqrt{\\frac{L(1+\\frac{C_{S}\\|1\\|_{*}}{N(1-\\gamma)})}{\\gamma^{3}(1-\\gamma)^{2}\\operatorname*{max}\\{1-\\gamma,C_{g}\\sigma\\}\\mathcal{N}}}1}\\\\ &{+\\,2\\sqrt{\\frac{L}{(1-\\gamma)^{2}N}}\\left\\|V^{\\star,\\sigma}-\\widehat{V}^{\\pi^{\\star},\\sigma}\\right\\|_{\\infty}1+2\\sqrt{\\frac{2L}{\\gamma^{2}(1-\\gamma)^{2}\\operatorname*{max}\\{1-\\gamma,C_{g}\\sigma\\}N}}1+\\frac{7L C_{S}\\|1\\|_{*}}{N(1-\\gamma)^{2}}}\\\\ &{\\leq80\\sqrt{\\frac{L(1+\\frac{C_{S}\\|1\\|_{*}}{N(1-\\gamma)})}{(1-\\gamma)^{2}\\operatorname*{max}\\{1-\\gamma,C_{g}\\sigma\\}N}}1+2\\sqrt{\\frac{L}{(1-\\gamma)^{2}N}}\\left\\|V^{\\star,\\sigma}-\\widehat{V}^{\\pi^{\\star},\\sigma}\\right\\|_{\\infty}1+\\frac{7L C_{S}\\|1\\|_{*}}{N(1-\\gamma)^{2}}{.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the last inequality follows from the assumption $\\gamma\\geq\\frac{1}{4}$ . Finally, inserting (82) and (89) back to (62) yields ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\displaystyle{\\hat{V}^{\\pi^{*},\\sigma}-V^{\\pi^{*},\\sigma}}\\right\\|_{\\infty}\\leq\\operatorname*{max}\\left\\{160\\sqrt{\\frac{L\\big(1+\\frac{C_{S}\\|1\\|_{*}}{N(1-\\gamma)}\\big)}{\\big(1-\\gamma\\big)^{2}\\operatorname*{max}\\{1-\\gamma,C_{g}\\sigma\\}N}}+\\frac{7L C_{S}\\|1\\|_{*}}{N(1-\\gamma)^{2}},}\\\\ &{80\\sqrt{\\frac{L\\big(1+\\frac{C_{S}\\|1\\|_{*}}{N(1-\\gamma)}\\big)}{\\big(1-\\gamma\\big)^{2}\\operatorname*{max}\\{1-\\gamma,C_{g}\\sigma\\}N}}+2\\sqrt{\\frac{L}{\\big(1-\\gamma\\big)^{2}N}}\\left\\|{V^{\\star,\\sigma}-\\hat{V}^{\\pi^{*},\\sigma}}\\right\\|_{\\infty}+\\frac{7L C_{S}\\|1\\|_{*}}{N(1-\\gamma)^{2}}\\right\\}}\\\\ &{\\leq160\\sqrt{\\frac{L\\big(1+\\frac{C_{S}\\|1\\|_{*}}{N(1-\\gamma)}\\big)}{\\big(1-\\gamma\\big)^{2}\\operatorname*{max}\\{1-\\gamma,C_{g}\\sigma\\}N}}+\\frac{14L C_{S}\\|1\\|_{*}}{N(1-\\gamma)^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the last inequality holds by taking $\\begin{array}{r}{N\\ge\\frac{16\\log(\\frac{S A N}{\\delta})}{(1-\\gamma)^{2}}}\\end{array}$ rearranging erms. In $s$ -rectangular case, we obtain the same result, replacing $\\operatorname*{max}\\lbrace1-\\gamma,C_{g}\\sigma\\rbrace$ by $\\operatorname*{max}\\{1-\\gamma,\\operatorname*{min}_{s}\\left\\|\\pi_{s}^{*}\\right\\|_{*}C_{g}\\tilde{\\sigma}\\}$ ", "page_idx": 28}, {"type": "text", "text": "Third step: controlling $\\|\\widehat{V}^{\\widehat{\\pi},\\sigma}-V^{\\widehat{\\pi},\\sigma}\\|_{\\infty}$ or bounding the first and second term in (61). Unlike the earlier term, one has to face a more complicated statistical dependency between $\\widehat{\\pi}$ and the empirical RMDP. To begin with, we introduce the following lemma which controls the main term on the right-hand side of (61), which is proved in Appendix D.3.5. ", "page_idx": 28}, {"type": "text", "text": "Lemma 10. Consider any $\\delta\\in(0,1)$ .Taking $N\\ge L^{\\prime\\prime}$ with probability at least $1-\\delta$ one has for $s a$ or $s$ -rectangular case : ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Big|\\displaystyle\\widehat{\\underline{{P}}}^{\\widehat{\\pi},\\widehat{V}}\\widehat{V}^{\\widehat{\\pi},\\sigma}-\\underline{{P}}^{\\widehat{\\pi},\\widehat{V}}\\widehat{V}^{\\widehat{\\pi},\\sigma}\\Big|\\leq2\\sqrt{\\displaystyle\\frac{L^{\\prime}}{N}}\\sqrt{\\operatorname{Var}_{P_{s,a}^{0}}(\\widehat{V}^{\\star,\\sigma})}1+2\\varepsilon_{\\mathsf{o p t}}1+\\displaystyle\\frac{15L^{\\prime\\prime}C_{S}\\left\\|1\\right\\|_{*}}{N(1-\\gamma)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq2\\sqrt{\\displaystyle\\frac{L^{\\prime\\prime}}{(1-\\gamma)^{2}N}}1+2\\varepsilon_{\\mathsf{o p t}}1+\\displaystyle\\frac{14L^{\\prime\\prime}C_{S}\\left\\|1\\right\\|_{*}}{N(1-\\gamma)}1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "with L = 2 1og( 54l1-SAN2 .Moreover, for $T V$ this lemma holds but without the geometric term 4\"sl-1. Taking the sup over s gives th fial result. ", "page_idx": 28}, {"type": "text", "text": "With Lemma 10 in hand, we have to control first term in (61) ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(I-\\gamma\\frac{\\rho}{N}\\right)^{-1}\\left(\\hat{P}^{\\varepsilon}\\hat{V}^{\\varepsilon_{0}}-P^{\\varepsilon}\\hat{V}^{\\varepsilon_{0}}\\right)}\\\\ &{\\overset{(i)}{\\leq}\\left(I-\\gamma\\frac{\\rho\\hat{E}^{\\varepsilon_{0}}}{N}\\right)^{-1}\\bigg[\\hat{P}^{\\varepsilon}\\hat{P}^{\\varepsilon_{0}}-P^{\\varepsilon}\\hat{V}^{\\varepsilon_{0}}\\bigg]}\\\\ &{\\leq2\\sqrt{\\frac{N}{N}}\\left(I-\\gamma\\frac{\\rho\\hat{E}^{\\varepsilon_{0}}}{N}\\right)^{-1}\\sqrt{\\operatorname{var}_{\\varepsilon}(\\hat{V}^{\\varepsilon_{\\varepsilon}})}+\\left(I-\\gamma\\frac{\\rho\\hat{E}^{\\varepsilon_{0}}}{N}\\right)^{-1}\\left(2z_{\\varepsilon_{0}}\\right)1}\\\\ &{+\\left(I-\\gamma\\frac{\\rho}{N}\\right)^{-1}\\frac{14I^{2}C^{2}S_{0}[1]}{N(1-\\gamma)}1}\\\\ &{\\overset{(i i)}{\\leq}\\left(\\frac{2\\gamma\\varepsilon_{0}}{1-\\gamma}\\right)1+\\underbrace{2\\sqrt{\\frac{N}{N}}\\left(I-\\gamma\\frac{\\rho\\hat{E}^{\\varepsilon_{0}}}{N}\\right)^{-1}\\sqrt{\\operatorname{var}_{\\varepsilon}\\varepsilon_{0}(\\hat{V}^{\\varepsilon_{\\varepsilon}})}}_{=S_{0}}}\\\\ &{\\quad+\\underbrace{2\\sqrt{\\frac{N}{N}}\\left(I-\\gamma\\frac{\\rho\\hat{E}^{\\varepsilon_{0}}}{N}\\right)^{-1}\\sqrt{\\operatorname{var}_{\\varepsilon}\\varepsilon_{0}(\\hat{V}^{\\varepsilon_{\\varepsilon}})-\\operatorname{Var}_{\\varepsilon}\\varepsilon_{0}(\\hat{V}^{\\varepsilon_{\\varepsilon}})}}_{=S_{0}}}\\\\ &{\\quad+\\underbrace{2\\sqrt{\\frac{N}{N}}\\left(I-\\gamma\\frac{\\rho\\hat{E}^{\\varepsilon_{0}}}{N}\\right)^{-1}\\sqrt{\\operatorname{var}_{\\varepsilon}\\varepsilon_{0}(\\hat{V}^{\\varepsilon_{\\varepsilon}})-\\operatorname{Var}_{\\varepsilon}\\varepsilon_{0}(\\hat{V}^{\\varepsilon_{\\varepsilon}})}}_{=S_{0}}\\bigg]\\frac{4I\\mathcal{C}C_{0}[1]}{N(1-\\gamma)^{2}}1}\\\\ &{\\quad+\\underbrace{2\\sqrt{\\frac{N} \n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where (i and (i) hold by the fact that each row of $\\left(1-\\gamma\\right)\\left(I-\\gamma\\underline{{P}}^{\\widehat{\\pi},\\widehat{V}}\\right)^{-1}$ is a probability vector that falls into $\\Delta(S)$ . The remainder of the proof will focus on controlling the three terms in (93) separately. ", "page_idx": 29}, {"type": "text", "text": "\u00b7 For $S_{1}$ , we introduce the following lemma, whose proof is postponed to D.3.6. Lemma 11. Consider any $\\delta\\in(0,1)$ Taking $\\begin{array}{r}{N\\ge\\frac{L^{\\prime\\prime}}{(1-\\gamma)^{2}}}\\end{array}$ one has with probability at least $1-\\delta$ for $s a-$ rectangular ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Big(I-\\gamma\\underline{{P}}^{\\widehat{\\pi},\\widehat{V}}\\Big)^{-1}\\sqrt{\\mathrm{Var}_{\\underline{{P}}^{\\widehat{\\pi},\\widehat{\\nu}}}(\\widehat{V}^{\\widehat{\\pi},\\sigma})}\\qquad\\le6\\sqrt{\\frac{\\left(1+\\varepsilon_{\\mathsf{o p t}}+\\frac{L^{\\prime\\prime}C_{S}\\|1\\|_{*}}{N(1-\\gamma)}\\right)}{\\gamma^{3}(1-\\gamma)^{2}\\operatorname*{max}\\{1-\\gamma,\\sigma\\}}}1}\\\\ &{\\le6\\sqrt{\\frac{\\left(1+\\varepsilon_{\\mathsf{o p t}}+\\frac{L^{\\prime\\prime}C_{S}\\|1\\|_{*}}{N(1-\\gamma)}\\right)}{(1-\\gamma)^{3}\\gamma^{3}}}1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and for $s$ -rectangular ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Big(I-\\gamma\\underline{{P}}^{\\widehat{\\pi},\\widehat{V}}\\Big)^{-1}\\sqrt{\\mathrm{Var}_{\\underline{{P}}^{\\widehat{\\pi},\\widehat{V}}}(\\widehat{V}^{\\widehat{\\pi},\\sigma})}\\leq6\\sqrt{\\frac{L^{\\prime\\prime}\\Big(1+\\varepsilon_{\\mathrm{opt}}+\\frac{C_{S}\\|1\\|_{*}}{N(1-\\gamma)}\\Big)}{\\gamma^{3}(1-\\gamma)^{2}\\operatorname*{max}\\{1-\\gamma,C_{g}\\widetilde{\\sigma}\\operatorname*{min}_{s}\\|\\widehat{\\pi}_{s}\\|_{\\infty}\\}}}1}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq6\\sqrt{\\frac{L^{\\prime\\prime}\\Big(1+\\varepsilon_{\\mathrm{opt}}+\\frac{C_{S}\\|1\\|_{*}}{N(1-\\gamma)}\\Big)}{(1-\\gamma)^{3}\\gamma^{2}}}1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Applying Lemma 11 and (65) to (93) leads to ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S_{1}=2\\sqrt{\\frac{L^{\\prime}}{N}}\\Big(I-\\gamma\\underline{{P}}^{\\widehat{\\pi},\\widehat{V}}\\Big)^{-1}\\sqrt{\\mathrm{Var}_{\\underline{{P}}^{\\widehat{\\pi},\\widehat{V}}}(\\widehat{V}^{\\widehat{\\pi},\\sigma})}}\\\\ &{\\quad\\le12\\sqrt{\\frac{L^{\\prime\\prime}}{\\gamma^{3}(1-\\gamma)^{2}\\operatorname*{max}\\{1-\\gamma,C_{g}\\sigma\\}N}}1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "for $s a$ -rectangular and the same quantity replacing $\\operatorname*{max}\\lbrace1\\,-\\,\\gamma,C_{g}\\sigma\\rbrace$ by $\\mathrm{max}\\{1~-~}$ $\\gamma,C_{g}\\tilde{\\sigma}\\operatorname*{min}_{s}\\|\\hat{\\pi}_{s}\\|_{*}\\}$ for $s-$ rectangular case. ", "page_idx": 29}, {"type": "text", "text": "\u00b7 Applying Lemma 1 with $\\|\\widehat{V}^{\\star,\\sigma}-\\widehat{V}^{\\widehat{\\pi},\\sigma}\\|_{\\infty}\\leq\\varepsilon_{\\sf o p t}$ and (65), $S_{2}$ can be controlled as ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S_{2}=2\\sqrt{\\frac{L^{\\prime\\prime}}{N}}\\Big(I-\\gamma\\underline{{P}}^{\\widehat\\pi,\\widehat V}\\Big)^{-1}\\sqrt{\\Big|\\mathrm{Var}\\underline{{_{P}^{\\widehat\\pi,\\widehat V}}}(\\widehat V^{\\star,\\sigma})-\\mathrm{Var}\\underline{{_{P}^{\\widehat\\pi,\\widehat V}}}(\\widehat V^{\\widehat\\pi,\\sigma})\\Big|}}\\\\ &{\\quad\\le4\\sqrt{\\frac{L^{\\prime\\prime}}{N}}\\Big(I-\\gamma\\underline{{P}}^{\\widehat\\pi,\\widehat V}\\Big)^{-1}\\sqrt{\\varepsilon_{\\sf o p t}\\frac{1}{1-\\gamma}^{2}}\\le8\\sqrt{\\frac{\\varepsilon_{\\sf o p t}L^{\\prime\\prime}}{(1-\\gamma)^{4}N}}1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "$\\mathcal{S}_{3}$ can be controlled similar to $\\mathcal{R}_{2}$ in (76) as follows: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S_{3}=2\\sqrt{\\displaystyle\\frac{L^{\\prime\\prime}}{N}\\left(I-\\gamma\\underline{{P}}^{\\widehat\\pi,\\widehat\\gamma}\\right)^{-1}\\sqrt{\\left|\\mathrm{Var}_{P^{\\widehat\\pi}}(\\widehat V^{\\star,\\sigma})-\\mathrm{Var}_{\\underline{{P}}^{\\widehat\\pi,\\widehat\\gamma}}(\\widehat V^{\\star,\\sigma})\\right|}}\\\\ &{\\ \\ \\ \\leq4\\sqrt{\\displaystyle\\frac{L^{\\prime\\prime}}{N}\\left(I-\\gamma\\underline{{P}}^{\\widehat\\pi,\\widehat\\gamma}\\right)^{-1}\\sqrt{\\displaystyle\\frac{1}{\\gamma^{2}\\operatorname*{max}\\{1-\\gamma,C_{g}\\sigma\\}}1}}\\\\ &{\\ \\ \\ \\leq8\\sqrt{\\displaystyle\\frac{L^{\\prime\\prime}}{\\gamma^{2}(1-\\gamma)^{2}\\operatorname*{max}\\{1-\\gamma,C_{g}\\sigma\\}N}1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "for $s a$ -rectangular and replacing $\\operatorname*{max}\\{1-\\gamma,\\sigma\\}$ by $\\mathrm{max}\\{1-\\gamma,\\tilde{\\sigma}\\operatorname*{min}_{s}\\left\\|\\hat{\\pi}_{s}\\right\\|_{*}\\}$ for $s-$ rectangular case. ", "page_idx": 29}, {"type": "text", "text": "Finally, summing up the results in (94), (95), and (97) and inserting them back to (93) yields: taking $\\begin{array}{r}{N\\ge\\frac{L^{\\prime\\prime}}{(1-\\gamma)^{2}}}\\end{array}$ withprobabity t least $1-\\delta$ ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(I-\\gamma{\\underline{{P}}}^{\\widehat{\\pi},\\widehat{V}}\\right)^{-1}\\left(\\widehat{{P}}^{\\widehat{\\pi},\\widehat{V}}{\\widehat{\\nu}}^{\\widehat{\\pi},\\sigma}-{\\underline{{P}}}^{\\widehat{\\pi},\\widehat{V}}{\\widehat{V}}^{\\widehat{\\pi},\\sigma}\\right)\\leq\\left(\\frac{2\\varepsilon_{\\mathtt{o p t}}}{1-\\gamma}\\right)1+\\frac{14L^{\\eta^{\\prime}C}S}{N(1-\\gamma)^{2}}1}\\\\ &{+12\\sqrt{\\frac{L^{\\eta}\\left(1+\\varepsilon_{\\mathtt{o p t}}+\\frac{C s\\|\\mathbb{I}\\|_{*}}{N(1-\\gamma)}\\right)}{\\gamma^{3}(1-\\gamma)^{2}\\operatorname*{max}\\{1-\\gamma,C_{g}\\sigma\\}\\mathcal{N}}}1+8\\sqrt{\\frac{\\varepsilon_{\\mathtt{o p t}}L^{\\gamma}}{(1-\\gamma)^{4}N}}1+}\\\\ &{\\mathrm{s}\\sqrt{\\frac{L^{\\prime}}{\\gamma^{2}(1-\\gamma)^{2}\\operatorname*{max}\\{1-\\gamma,C_{g}\\sigma\\}\\mathcal{N}}}1}\\\\ &{\\leq16\\sqrt{\\frac{L^{\\eta}\\left(1+\\varepsilon_{\\mathtt{o p t}}+\\frac{C s\\|\\mathbb{I}\\|_{*}}{N(1-\\gamma)}\\right)}{\\gamma^{3}(1-\\gamma)^{2}\\operatorname*{max}\\{1-\\gamma,\\sigma\\}\\mathcal{N}}}1+\\left(\\frac{2\\varepsilon_{\\mathtt{o p t}}\\gamma}{(1-\\gamma)}+8\\sqrt{\\frac{\\varepsilon_{\\mathtt{o p t}}\\gamma L^{\\prime}}{(1-\\gamma)^{4}N}}1+\\frac{15L^{\\eta^{\\prime}C}S\\|\\mathbb{I}\\|_{*}}{N(1-\\gamma)^{2}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "for $s a$ -rectangular and the same quantity replacing $\\operatorname*{max}\\{1-\\gamma,\\sigma\\}$ by $\\mathrm{max}\\{1-\\gamma,\\tilde{\\sigma}\\operatorname*{min}_{s}\\left\\|\\hat{\\pi}_{s}\\right\\|_{*}\\}$ for $s-$ rectangular case. In this step, it is harder to decouple terms as $\\widehat V^{\\widehat\\pi}$ depends on data both in $\\widehat{\\pi}$ and $\\widehat V$ ", "page_idx": 30}, {"type": "text", "text": "Step 5: controlling $\\|\\widehat{V}^{\\widehat{\\pi},\\sigma}-V^{\\widehat{\\pi},\\sigma}\\|_{\\infty};$ bounding the second term in (61).Towards this applying Lemma 10 leads to in $s a$ -rectangular case: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(I-\\gamma E^{k}\\right)^{-1}\\left(E^{k}\\tilde{\\rho}^{k}\\tilde{\\rho}^{k}-E^{k}\\tilde{\\rho}^{k}\\tilde{\\rho}^{k}\\right)\\leq\\left(I-\\gamma E^{k}\\right)^{-1}\\left(\\tilde{\\rho}^{k}\\tilde{\\rho}^{k}\\tilde{\\rho}^{k}-E^{k}\\tilde{\\rho}^{k}\\tilde{\\rho}^{k}\\right)}\\\\ &{\\leq2\\sqrt{\\frac{E^{k}}{N}}\\left(I-\\gamma E^{k}\\right)^{-1}\\sqrt{\\operatorname{Var}_{j}(\\tilde{\\rho}^{k})}+\\left(I-\\gamma E^{k}\\right)^{-1}\\left(2\\tilde{\\rho}_{(j)}\\right)^{1}}\\\\ &{\\quad+\\left(I-\\gamma E^{k}\\right)^{-1}\\frac{\\sqrt{\\operatorname{Var}_{j}(\\tilde{\\rho}^{k})}}{\\sqrt{\\operatorname{Var}_{j}(\\tilde{\\rho}^{k})}},}\\\\ &{\\leq\\left(\\frac{2E_{k}}{1-\\gamma}\\right)^{-1}\\underbrace{\\left(I-\\gamma E^{k}\\right)^{-1}}_{\\leq\\rho}\\sqrt{\\operatorname{Var}_{j}(\\tilde{\\rho}^{k})}^{-1}\\sqrt{\\operatorname{Var}_{k}(\\tilde{\\rho}^{k})}+\\left(I-\\gamma E^{k}\\right)^{-1}\\frac{\\sqrt{\\operatorname{Var}_{j}(\\tilde{\\rho}^{k})}}{N(1-\\gamma)},}\\\\ &{\\quad+\\underbrace{2\\sqrt{\\frac{E^{k}}{N}}\\left(I-\\gamma E^{k}\\right)^{-1}\\sqrt{\\operatorname{Var}_{j}(\\tilde{\\rho}^{k})}-\\sqrt{\\operatorname{Var}_{j}(\\tilde{\\rho}^{k})}}_{\\leq\\rho}}\\\\ &{\\quad+\\underbrace{2\\sqrt{\\frac{E^{k}}{N}}\\left(I-\\gamma E^{k}\\right)^{-1}\\sqrt{\\operatorname{Var}_{j}(\\tilde{\\rho}^{k})}-\\sqrt{\\operatorname{Var}_{j}(\\tilde{\\rho}^{k})}}_{\\leq\\rho}}\\\\ &{\\quad+\\underbrace{2\\sqrt{\\frac{E^{k}}{N}}\\left(I-\\gamma E^{k}\\right)^{-1}\\sqrt{\\operatorname{Var}_{j}(\\tilde{\\rho}^{k})}-\\sqrt{\\operatorname{Var}_{j}(\\tilde{\\rho}^{k})}}_{\\leq\\rho}}\\\\ &{\\quad+\\underbrace{2\\sqrt{\\frac{E^{k}}{N}}\\left\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We shall bound each of the terms separately. ", "page_idx": 30}, {"type": "text", "text": "\u00b7 Applying Lemma 7 with $P=\\underline{o}^{\\widehat{\\pi},V}$ \uff0c $\\pi=\\widehat{\\pi}$ and taking $V=V^{\\widehat{\\pi},\\sigma}$ which obeys $V^{\\widehat{\\pi},\\sigma}=$ $r_{\\widehat{\\pi}}+\\gamma\\underline{{\\bar{P}}}^{\\widehat{\\pi},V}V^{\\widehat{\\pi},\\sigma}$ , the term $S_{4}$ can be controlled similar to (84) as follows: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{S_{4}\\leq8\\sqrt{\\frac{L^{\\prime\\prime}\\Big(1+\\varepsilon_{\\mathsf{o p t}}+\\frac{C_{S}\\|1\\|_{*}}{N(1-\\gamma)}\\Big)}{\\gamma^{3}(1-\\gamma)^{2}\\operatorname*{max}\\{1-\\gamma,C_{g}\\sigma\\}N}}1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "for $s a$ -rectangular and the same quantity replacing $\\operatorname*{max}\\lbrace1\\,-\\,\\gamma,C_{g}\\sigma\\rbrace$ by $\\mathrm{max}\\{1~-~}$ $\\gamma,\\operatorname*{min}_{s}\\|\\hat{\\pi_{s}}\\|_{*}^{-}\\tilde{\\sigma}C_{g}\\}$ for $s-$ rectangular case. ", "page_idx": 30}, {"type": "text", "text": "\u00b7 For $S_{5}$ , it is observed that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S_{5}=2\\sqrt{\\frac{L^{\\prime\\prime}}{N}}\\Big(I-\\gamma\\underline{{P}}^{\\widehat{\\pi},V}\\Big)^{-1}\\sqrt{\\mathrm{Var}_{\\underline{{P}}^{\\widehat{\\pi},V}}\\big(\\widehat{V}^{\\widehat{\\pi},\\sigma}-V^{\\widehat{\\pi},\\sigma}\\big)}}\\\\ &{\\quad\\le2\\sqrt{\\frac{L^{\\prime\\prime}}{(1-\\gamma)^{2}N}}\\left\\|V^{\\widehat{\\pi},\\sigma}-\\widehat{V}^{\\widehat{\\pi},\\sigma}\\right\\|_{\\infty}1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "\u00b7 Next, observing that ${\\mathcal{S}}_{6}$ and $S_{7}$ are almost the same as the terms $S_{2}$ (controlled in (95)) and $S_{3}$ (controlled in (97)) in (93), it is easily verified that they can be controlled as follows ", "page_idx": 31}, {"type": "equation", "text": "$$\nS_{6}\\le4\\sqrt{\\frac{\\varepsilon_{\\mathrm{opt}}L^{\\prime\\prime}}{(1-\\gamma)^{4}N}}1,\\qquad\\qquad S_{7}\\le4\\sqrt{\\frac{L^{\\prime\\prime}}{\\gamma^{2}(1-\\gamma)^{2}\\operatorname*{max}\\{1-\\gamma,C_{g}\\sigma\\}N}1}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "for $s a$ -rectangular and the same quantity replacing $\\operatorname*{max}\\{1-\\gamma,\\sigma\\}$ by $\\operatorname*{max}\\{1-\\gamma,\\operatorname*{min}_{s}\\left\\|\\hat{\\pi}_{s}\\right\\|_{*}\\tilde{\\sigma}\\}$ for $s-$ rectangular case. Then inserting the results in (103), (104), and (105) back to (102) leads to ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Big(I-\\gamma\\underline{{P}}^{\\sharp,\\nu}\\Big)^{-1}\\bigg(\\underline{{\\hat{P}}}^{\\sharp,\\nu}\\hat{V}^{\\sharp,\\sigma}-\\underline{{P}}^{\\sharp,\\tilde{V}}\\hat{V}^{\\sharp,\\sigma}\\Big)}\\\\ &{\\leq\\Bigg(\\frac{2\\varepsilon_{\\otimes t_{0}}}{(1-\\gamma)}\\Bigg)1+8\\sqrt{\\frac{L^{\\prime}\\big(1+\\varepsilon_{\\otimes t_{0}}+\\frac{C s[\\|\\Pi_{1}\\rangle}{N(1-\\gamma)}\\big)}{\\gamma^{3}(1-\\gamma)^{2}\\operatorname*{max}\\{1-\\gamma,\\sigma\\}}}1+\\frac{14L^{\\prime\\prime}C s}{N(1-\\gamma)^{2}}\\|1\\|_{*1}}\\\\ &{\\quad+2\\sqrt{\\frac{L^{\\prime\\prime}}{(1-\\gamma)^{2}N}}\\left\\|{V^{\\sharp,\\sigma}}-{\\hat{V}}^{\\sharp,\\sigma}\\right\\|_{\\infty}1+4\\sqrt{\\frac{L^{\\prime\\prime}\\varrho_{\\otimes t_{0}}}{(1-\\gamma)^{4}N}}1+4\\sqrt{\\frac{L^{\\prime\\prime}}{\\gamma^{2}(1-\\gamma)^{2}\\operatorname*{max}\\{1-\\gamma,C_{g}\\sigma\\}}}}\\\\ &{\\leq12\\sqrt{\\frac{L^{\\prime\\prime}\\big(1+\\varepsilon_{\\otimes t_{0}}+\\frac{C s[\\ \\|\\Pi_{1}\\rangle}{N(1-\\gamma)}\\big)}{\\gamma^{3}(1-\\gamma)^{2}\\operatorname*{max}\\{1-\\gamma,\\sigma\\}}}+4\\sqrt{\\frac{L^{\\prime\\prime}}{(1-\\gamma)^{2}N}}\\left\\|{V^{\\sharp,\\sigma}}-{\\hat{V}}^{\\sharp,\\sigma}\\right\\|_{\\infty}1}\\\\ &{\\quad+\\frac{3\\varepsilon_{\\otimes t_{0}}}{(1-\\gamma)}1+\\frac{14L^{\\prime\\prime}C s}{N(1-\\gamma)^{2}}\\|1\\|_{*1}}\\\\ &{\\qquad+\\frac{3\\varepsilon_{\\otimes t_{0}}}{(1-\\gamma)^{2}}1-\\frac{1}{N(1-\\gamma)^{2}}\\|2\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Taking $\\begin{array}{r}{N\\ge\\frac{16L^{\\prime\\prime}}{1-\\gamma}}\\end{array}$ , we obtain $\\begin{array}{r}{\\frac{2\\varepsilon_{\\mathsf{o p t}}}{(1-\\gamma)}+4\\varepsilon_{\\mathsf{o p t}}\\sqrt{\\frac{L^{\\prime\\prime}}{(1-\\gamma)^{4}N}}1\\leq\\frac{3\\varepsilon_{\\mathsf{o p t}}}{(1-\\gamma)}}\\end{array}$ with probabity a least $1-\\delta$ inserting (99) and (107) back to (61) ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\hat{V}^{\\widehat{\\pi},\\sigma}-V^{\\widehat{\\pi},\\sigma}\\right\\|_{\\infty}\\leq\\operatorname*{max}\\left\\{16\\sqrt{\\frac{L^{\\eta}\\left(1+\\varepsilon_{\\mathsf{o p t}}+\\frac{C_{s}\\|\\Pi\\|_{\\ast}}{N(1-\\gamma)}\\right)}{\\gamma^{3}(1-\\gamma)^{2}\\operatorname*{max}\\{1-\\gamma,\\sigma\\}N}}+\\left(\\frac{2\\varepsilon_{\\mathsf{o p t}}\\gamma}{(1-\\gamma)}+\\frac{14L^{\\eta}\\mathcal{C}C_{S}}{N(1-\\gamma)^{2}}\\right)1\\right\\}}\\\\ &{\\quad12\\sqrt{\\frac{L^{\\eta}\\left(1+\\varepsilon_{\\mathsf{o p t}}+\\frac{C_{s}\\|\\Pi\\|_{\\ast}}{N(1-\\gamma)}\\right)}{\\gamma^{3}(1-\\gamma)^{2}\\operatorname*{max}\\{1-\\gamma,\\sigma\\}N}}+4\\sqrt{\\frac{L^{\\eta}}{(1-\\gamma)^{2}N}}\\left\\|V^{\\widehat{\\pi},\\sigma}-\\hat{V}^{\\widehat{\\pi},\\sigma}\\right\\|_{\\infty}\\qquad\\qquad(110)}\\\\ &{+\\frac{3\\varepsilon_{\\mathsf{o p t}}}{(1-\\gamma)}+\\frac{14L^{\\eta}\\mathcal{C}_{S}}{N(1-\\gamma)^{2}}\\left\\|1\\right\\|_{\\infty}\\biggr\\}}\\\\ &{\\leq48\\sqrt{\\frac{L^{\\eta}\\left(1+\\varepsilon_{\\mathsf{o p t}}+\\frac{C_{s}\\|\\Pi\\|_{\\ast}}{N(1-\\gamma)}\\right)}{\\gamma^{3}(1-\\gamma)^{2}\\operatorname*{max}\\{1-\\gamma,\\sigma\\}N}}+\\frac{6\\varepsilon_{\\mathsf{o p t}}}{(1-\\gamma)}+\\frac{28L^{\\eta}\\mathcal{C}_{S}}{N(1-\\gamma)^{2}}\\left\\|1\\right\\|_{\\infty}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "for $s a$ -rectangular and the same quantity, replacing $\\operatorname*{max}\\lbrace1-\\gamma,C_{g}\\sigma\\rbrace$ by $\\mathrm{max}\\{1-\\gamma,\\tilde{\\sigma}\\operatorname*{min}_{s}\\left\\|\\hat{\\pi}_{s}\\right\\|_{*}\\}$ for $s-$ rectangular case. The proof is similar for $T V$ without the geometric term depending on $C_{S}$ ", "page_idx": 31}, {"type": "text", "text": "Step 6: summing all the previous inequalities results. Using all the previous results in (90) and (111) and inserting back to (6) complete the prof asfolows aking $\\begin{array}{r}{\\bar{N^{}}\\ge\\frac{16L^{\\prime\\prime}}{(1-\\gamma)^{2}},\\gamma>1/4;}\\end{array}$ wih ", "page_idx": 31}, {"type": "text", "text": "probability at least $1-\\delta$ , for $s a$ -rectangular ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|{V^{\\star,\\sigma}-V^{\\widehat{\\pi},\\sigma}}\\right\\|_{\\infty}\\leq\\left\\|{V^{\\pi^{\\star},\\sigma}-\\widehat{V}^{\\pi^{\\star},\\sigma}}\\right\\|_{\\infty}+\\varepsilon_{\\mathsf{o p t}}+\\left\\|{\\widehat{V}^{\\widehat{\\pi},\\sigma}-V^{\\widehat{\\pi},\\sigma}}\\right\\|_{\\infty}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\varepsilon_{\\mathsf{o p t}}+48\\sqrt{\\frac{L^{\\eta^{\\prime}}\\left(1+\\varepsilon_{\\mathsf{o p t}}+\\frac{C s\\|\\|_{\\infty}}{N(1-\\gamma)}\\right)}{\\gamma^{3}(1-\\gamma)^{2}\\operatorname*{max}\\left\\{1-\\gamma,C_{g}\\sigma\\right\\}N}}+\\frac{6\\varepsilon_{\\mathsf{o p t}}}{(1-\\gamma)}+\\frac{28L^{\\eta^{\\prime}}C_{S}}{N(1-\\gamma)^{2}}\\left\\|{1\\right\\|_{\\infty}}}\\\\ &{\\qquad\\qquad+160\\sqrt{\\frac{L(1+\\frac{C s}{N(1-\\gamma)})}{(1-\\gamma)^{2}\\operatorname*{max}\\{1-\\gamma,C_{g}\\sigma\\}N}}+\\frac{14L C_{S}}{N(1-\\gamma)^{2}}\\left\\|{1\\right\\|_{\\infty}}}\\\\ &{\\qquad\\qquad\\leq\\frac{8\\varepsilon_{\\mathsf{o p t}}}{1-\\gamma}+\\frac{42L^{\\eta^{\\prime}}C_{S}}{N(1-\\gamma)^{2}}\\left\\|{1\\right\\|_{\\infty}}+1508\\sqrt{\\frac{L^{\\eta^{\\prime}}(1+\\frac{C s\\|\\|_{\\infty}}{N(1-\\gamma)})}{(1-\\gamma)^{2}\\operatorname*{max}\\{1-\\gamma,C_{g}\\sigma\\}N}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where the last inequality holds by \u2265  and N \u2265 for $s a$ -rectangular and the same quantity replacing $\\operatorname*{max}\\{1-\\gamma,\\sigma\\}$ by $\\mathrm{max}\\{1-\\gamma,\\tilde{\\sigma}\\operatorname*{min}_{s}\\{\\|\\pi_{s}^{*}\\|_{*}\\}\\}$ for $s-$ rectangular case. The proof is similar for $T V$ without the geometric term depending on $C_{S}$ ", "page_idx": 32}, {"type": "text", "text": "D.3  Proof of the auxiliary lemmas ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "D.3.1 Proof of Lemma 5 ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Similarly to Shi et al. [2023], denoting $s_{0}$ the argmax of $V^{\\pi,\\sigma}$ such that $V^{\\pi,\\sigma}\\left(s_{0}\\right)=\\operatorname*{min}_{s\\in S}V^{\\pi,\\sigma}\\left(s\\right)$ using recursiveBellman's equation ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{s\\in\\mathcal{S}}{\\operatorname*{max}}V^{\\pi,\\sigma}(s)=\\underset{s\\in\\mathcal{S}}{\\operatorname*{max}}\\mathbb{E}_{a\\sim\\pi(\\cdot\\vert s)}\\left[r(s,a)+\\gamma\\underset{\\mathcal{P}\\in\\mathcal{U}^{\\sigma}(P_{s,a})}{\\operatorname*{inf}}\\mathcal{P}V^{\\pi,\\sigma}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\le\\underset{(s,a)\\in\\mathcal{S}\\times\\mathcal{A}}{\\operatorname*{max}}\\left(1+\\gamma\\underset{\\mathcal{P}\\in\\mathcal{U}^{\\sigma}(P_{s,a})}{\\operatorname*{inf}}\\mathcal{P}V^{\\pi,\\sigma}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where the second line holds since the reward function $r(s,a)\\in[0,1]$ for all $(s,a)\\in S\\times A$ ", "page_idx": 32}, {"type": "text", "text": "Then we construct for any $(s,a)\\in S\\times A$ \uff0c $\\widetilde{P}_{s,a}\\in\\mathbb{R}^{S}$ by reducing the values of some elements of $P_{s,a}$ such that $P_{s,a}\\ge\\widetilde{P}_{s,a}\\ge0$ and $\\begin{array}{r}{\\sum_{s^{\\prime}}\\left(P_{s,a}\\left(s^{\\prime}\\right)-\\widetilde{P}_{s,a}\\left(s^{\\prime}\\right)\\right)=\\sigma C_{g}^{s,a}}\\end{array}$ with $\\begin{array}{r}{C_{g}^{s,a}=\\frac{1}{\\left\\|e_{s_{0}}\\right\\|}}\\end{array}$ lead to $\\widetilde{P}_{s,a}+\\sigma C_{g}^{s,a}e_{s_{0}}^{\\top}\\in\\mathcal{U}_{\\parallel\\parallel}^{\\sigma}\\left(P_{s,a}\\right)$ where $e_{s_{0}}$ is the standard basis vector supported on $s_{0}$ since ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\left\\|\\widetilde{P}_{s,a}+\\sigma C_{g}^{s,a}e_{s_{0}}^{\\top}-P_{s,a}\\right\\|\\leq\\frac{1}{2}\\left\\|\\widetilde{P}_{s,a}-P_{s,a}\\right\\|+\\frac{C_{g}^{s,a}\\sigma\\left\\|e_{s_{0}}\\right\\|}{2}=\\sigma/2+\\sigma/2=\\sigma\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Consequently, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\mathcal{P}\\in\\mathcal{U}_{\\parallel,\\parallel}^{\\sigma}(P_{s,a})}{\\operatorname*{inf}}\\mathcal{P}V^{\\pi,\\sigma}\\leq\\left(\\widetilde{P}_{s,a}+\\sigma C_{g}^{s,a}e_{s_{0}}^{\\top}\\right)V^{\\pi,\\sigma}\\leq\\Big\\|\\widetilde{P}_{s,a}\\Big\\|_{1}\\,\\|V^{\\pi,\\sigma}\\|_{\\infty}+\\sigma V^{\\pi,\\sigma}\\left(s_{0}\\right)C_{g}^{s,a}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\left(1-C_{g}^{s,a}\\sigma\\right)\\underset{s\\in\\mathcal{S}}{\\operatorname*{max}}V^{\\pi,\\sigma}(s)+\\sigma C_{g}^{s,a}\\underset{s\\in\\mathcal{S}}{\\operatorname*{min}}V^{\\pi,\\sigma}(s)}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where the second inequality holds by ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\left\\lVert\\widetilde{P}_{s,a}\\right\\rVert_{1}=\\sum_{s^{\\prime}}\\widetilde{P}_{s,a}\\left(s^{\\prime}\\right)=-\\sum_{s^{\\prime}}\\left(P_{s,a}\\left(s^{\\prime}\\right)-\\widetilde{P}_{s,a}\\left(s^{\\prime}\\right)\\right)+\\sum_{s^{\\prime}}P_{s,a}\\left(s^{\\prime}\\right)=1-\\sigma C_{g}^{s,a}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Plugging this back to the previous relation gives ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{s\\in\\mathcal{S}}V^{\\pi,\\sigma}(s)\\leq1+\\gamma(1-C_{g}^{s,a}\\sigma)\\operatorname*{max}_{s\\in\\mathcal{S}}V^{\\pi,\\sigma}(s)+\\gamma C_{g}^{s,a}\\sigma\\operatorname*{min}_{s\\in\\mathcal{S}}V^{\\pi,\\sigma}(s)\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "which, by rearranging terms, yields ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\operatorname*{max}_{s\\in S}V^{\\pi,\\sigma}(s)\\leq\\frac{1+\\gamma C_{g}^{s,a}\\sigma\\operatorname*{min}_{s\\in S}V^{\\pi,\\sigma}(s)}{1-\\gamma(1-C_{g}^{s,a}\\sigma)}}}&{}&{{(1\\geq0,\\quad s\\in S)}}\\\\ &{}&{\\leq\\frac{1}{(1-\\gamma)+\\gamma C_{g}^{s,a}\\sigma}+\\operatorname*{min}_{s\\in S}V^{\\pi,\\sigma}(s)\\leq\\frac{1}{\\gamma\\operatorname*{max}\\{1-\\gamma,C_{g}^{s,a}\\sigma\\}}+\\operatorname*{min}_{s\\in S}V^{\\pi,\\sigma}(s)}}\\\\ &{}&\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "So rearranging terms it holds : ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname{sp}(V^{\\pi,\\sigma})_{\\infty}\\leq\\frac{1}{\\gamma\\operatorname*{max}\\{1-\\gamma,C_{g}^{s,a}\\sigma\\}}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "or taking the sup over $s$ ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname{sp}(V^{\\pi,\\sigma})_{\\infty}\\leq\\frac{1}{\\gamma\\operatorname*{max}\\{1-\\gamma,C_{g}\\sigma\\}}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "reme over $s$ the guanty. $C_{g}^{s,a}$ is replaced by $C_{g}=1/(\\mathrm{min}_{s}\\,\\Vert e_{s}\\Vert)$ to obtain a $s$ ", "page_idx": 33}, {"type": "text", "text": "D.3.2 Proof of Lemma 6 ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Similarly to 5 denoting $s_{0}$ the argmax of $V^{\\pi,\\sigma}$ such that $V^{\\pi,\\sigma}\\left(s_{0}\\right)=\\mathrm{min}_{s\\in S}\\,V^{\\pi,\\sigma}(s)$ using recursive Bellman's equation ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{s\\in\\mathcal{S}}{\\operatorname*{max}}V^{\\pi,\\sigma}(s)=\\underset{s\\in\\mathcal{S}}{\\operatorname*{max}}\\mathbb{E}_{a\\sim\\pi(\\cdot\\vert s)}\\left[r(s,a)+\\gamma\\underset{\\mathcal{P}\\in\\mathcal{U}^{\\tilde{\\sigma}}(P_{s})}{\\operatorname*{inf}}\\mathcal{P}V^{\\pi,\\tilde{\\sigma}}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leq\\underset{s\\in\\mathcal{S}}{\\operatorname*{max}}\\left(1+\\gamma\\underset{\\mathcal{P}^{\\pi}\\in\\mathcal{U}^{\\tilde{\\sigma}}(P_{s}^{\\pi})}{\\operatorname*{inf}}\\mathcal{P}^{\\pi}V^{\\pi,\\tilde{\\sigma}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where the second line holds since the reward function $r(s,a)\\in[0,1]$ for all $(s,a)\\in S\\times A$ .Then we construct for any $s\\in\\b{S}\\ \\tilde{P_{s}}\\in\\mathbb{R}^{S\\times A}$ by reducing the values of some elements of $P_{s}$ such that $P_{s}\\ge\\widetilde{P}_{s}\\ge0$ and ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\forall a\\in A,\\quad\\sum_{s^{\\prime}}\\Big(P_{s}\\left(s^{\\prime},a\\right)-\\widetilde{P}_{s}\\left(s^{\\prime},a\\right)\\Big)=\\sigma_{s,a}C_{g}^{s}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $C_{g}^{s}$ is defined as $1/\\left|\\left|e_{s}\\right|\\right|$ . Writting $\\|\\sigma_{s,a}\\|\\leq\\tilde{\\sigma}$ we construction $\\sigma_{s,a}$ such that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\sum_{a}\\pi(a|s)\\sum_{s^{\\prime}}\\left(P_{s}\\left(s^{\\prime},a\\right)-\\widetilde{P}_{s}\\left(s^{\\prime},a\\right)\\right)=\\left\\|\\pi_{s}\\right\\|_{*}\\widetilde{\\sigma}C_{g}^{s}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Not that this construction is possible as it is simply Cauchy Swartz equality case. It leads to $\\widetilde{P}_{s}+\\sigma e_{s_{0},a}^{\\top}\\in\\mathcal{U}^{\\tilde{\\sigma}}\\left(P_{s}\\right)$ $e_{s_{0},a}\\in\\mathbb{R}^{S\\times A}$ $s_{0}$ $s_{0}$ $a$ ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\left\\|\\widetilde{P}_{s}+\\sigma_{s,a}C_{g}^{s}e_{s_{0},a}^{\\top}-P_{s}\\right\\|\\leq\\frac{1}{2}\\left\\|\\widetilde{P}_{s}-P_{s}\\right\\|+\\frac{\\tilde{\\sigma}\\left\\|e_{s_{0}}\\right\\|C_{g}^{s}}{2}=\\tilde{\\sigma}/2+\\tilde{\\sigma}/2\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "as $C_{g}^{s}\\left\\|\\sigma_{s,a}e_{s_{0},a}\\right\\|$ is equal to $C_{g}^{s}\\tilde{\\sigma}\\left\\|e_{s_{0}}\\right\\|$ Consequently, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{p^{\\pi}\\in L_{\\theta^{\\pi}}^{\\infty}(P_{s})}{\\operatorname*{inf}}P^{\\pi}V^{\\pi,\\bar{\\sigma}}\\le\\Pi^{\\pi}\\left(\\widetilde{P}_{s}^{\\pi}+\\sigma C_{g}^{s}\\kappa_{s_{0}}^{\\top}\\right)V^{\\pi,\\bar{\\sigma}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\sum_{a}\\sum_{s^{\\prime}}\\widetilde{P}_{s}(s^{\\prime},a)\\pi(a|s)V^{\\pi,\\bar{\\sigma}}(s^{\\prime})+\\sigma e_{s_{0},a}C_{g}^{s}V^{\\pi,\\bar{\\sigma}}\\left(s_{0}\\right)\\pi(a|s)}\\\\ &{\\le\\displaystyle\\sum_{a}\\underset{s^{\\prime}\\in S}{\\operatorname*{sup}}[V^{\\pi,\\bar{\\sigma}}(s^{\\prime})](\\sum_{s^{\\prime}}\\widetilde{P}_{s}(s^{\\prime},a)))\\pi(a|s)+V^{\\pi,\\bar{\\sigma}}\\left(s_{0}\\right)\\pi(a|s)\\sigma_{s,a}C_{g}^{s}}\\\\ &{\\overset{(a)}{=}\\displaystyle\\operatorname*{max}_{s\\in S}V^{\\pi,\\sigma}(s)\\sum_{a}(1-\\sigma C_{g}^{s})\\pi(a|s)+\\displaystyle\\sum_{a}V^{\\pi,\\bar{\\sigma}}\\left(s_{0}\\right)\\pi(a|s)\\sigma_{s,a}C_{g}^{s}}\\\\ &{\\overset{(b)}{=}\\displaystyle\\operatorname*{max}_{s\\in S}V^{\\pi,\\sigma}(s)(1-\\bar{\\sigma}C_{g}^{s})\\left\\|\\pi_{s}\\right\\|_{s}+\\left\\|\\pi_{s}\\right\\|_{s}\\bar{\\sigma}C_{g}^{s}\\displaystyle\\operatorname*{min}_{s\\in S}V^{\\pi,\\bar{\\sigma}}(s)}\\\\ &{\\le(1-C_{g}^{s})\\displaystyle\\operatorname*{max}_{s\\in S}V^{\\pi,\\sigma}(s)+\\sigma C_{g}^{s}\\displaystyle\\operatorname*{min}_{s\\in S}V^{\\pi,\\bar{\\sigma}}(s)}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $\\|\\pi\\|_{\\infty}$ is the norm of the vector $\\pi(.|s)$ and where (a) holds because ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\sum_{s^{\\prime}}\\widetilde{P}_{s}\\left(s^{\\prime}\\right)=-\\sum_{s^{\\prime}}\\left(P_{s}\\left(s^{\\prime}\\right)-\\widetilde{P}_{s}\\left(s^{\\prime}\\right)\\right)+\\sum_{s^{\\prime}}P_{s}\\left(s^{\\prime}\\right)=1-\\sigma_{s,a}C_{g}^{s}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Finally (b) is due to (126) and using Holder's inequality in the second term. Plugging this back to the previous relation gives ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{s\\in\\mathcal{S}}V^{\\pi,\\bar{\\sigma}}(s)\\leq1+\\gamma(1-\\tilde{\\sigma}C_{g}^{s}\\left\\|\\pi_{s}\\right\\|_{*})\\operatorname*{max}_{s\\in\\mathcal{S}}V^{\\pi,\\sigma}(s)+\\gamma\\left\\|\\pi_{s}\\right\\|_{*}\\tilde{\\sigma}C_{g}^{s}\\operatorname*{min}_{s\\in\\mathcal{S}}V^{\\pi,\\bar{\\sigma}}(s)\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "which, by rearranging terms, yields ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{s\\in\\mathcal{S}}{\\operatorname*{max}}V^{\\pi,\\tilde{\\sigma}}(s)\\leq\\frac{1+\\gamma\\tilde{\\sigma}\\,\\|\\pi_{s}\\|_{*}\\,C_{g}^{s}\\operatorname*{min}_{s\\in\\mathcal{S}}V^{\\pi,\\tilde{\\sigma}}(s)}{1-\\gamma(1-C_{g}^{s}\\tilde{\\sigma}\\,\\|\\pi_{s}\\|_{*})}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ &{\\leq\\frac{1}{(1-\\gamma)+\\|\\pi_{s}\\|_{*}\\,\\gamma C_{g}^{s}\\tilde{\\sigma}}+\\underset{s\\in\\mathcal{S}}{\\operatorname*{min}}V^{\\pi,\\tilde{\\sigma}}(s)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\frac{1}{(1-\\gamma)+\\gamma\\,\\|\\pi_{s}\\|_{*}\\,C_{g}^{s}\\tilde{\\sigma}}+\\underset{s\\in\\mathcal{S}}{\\operatorname*{min}}V^{\\pi,\\tilde{\\sigma}}(s)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ &{\\leq\\frac{1}{\\gamma\\operatorname*{max}\\{1-\\gamma,C_{g}^{s}\\,\\|\\pi_{s}\\|_{*}\\,\\tilde{\\sigma}\\}}+\\underset{s\\in\\mathcal{S}}{\\operatorname*{min}}V^{\\pi,\\tilde{\\sigma}}(s).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "So rearranging and taking the sumpremum over all sterm it holds : ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\operatorname{sp}(V^{\\pi,\\tilde{\\sigma}})_{\\infty}\\leq\\frac{1}{\\gamma\\operatorname*{max}\\{1-\\gamma,\\operatorname*{min}_{s}\\|\\pi_{s}\\|_{*}C_{g}\\tilde{\\sigma}\\}}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "As we pick the supreme over $s$ ovf this quantity, $C_{g}^{s}$ is replaced by $C_{g}=1/\\operatorname*{min}_{s}\\left\\|e_{s}\\right\\|$ ", "page_idx": 34}, {"type": "text", "text": "D.3.3 Proof of Lemma 8 ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Proof. Concentration of the robust values function. with probability $1-\\delta$ , it holds: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\left|P_{s,a}^{\\pi,V}V-\\widehat{P}_{s,a}^{\\pi,V}V\\right|\\leq2\\sqrt{\\frac{L}{N}}\\sqrt{\\mathrm{Var}_{P_{s,a}^{0}}(V)}+\\frac{3L C_{S}\\left\\|1\\right\\|_{*}}{N(1-\\gamma)}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "with $L=2\\log(18\\left\\|1\\right\\|_{*}S A N/\\delta)$ and First we can use optimization duality such as in (50): ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|P_{k}^{\\operatorname*{max}}\\nabla_{j}\\nabla-\\widehat{P}_{k}^{\\operatorname*{max}}V\\right|}\\\\ &{=\\Big|\\underset{s_{k}\\neq s_{k}}{\\min}\\ \\Big(\\int_{0}^{a_{k}}(V-\\mu_{k})-\\sigma\\left(\\varphi\\left(\\left(V-\\mu_{k}\\right)\\right)_{-}\\right)\\Big)}\\\\ &{=\\underset{s_{k}\\neq s_{k}}{\\min}\\ \\Big(\\int_{0}^{a_{k}}(V-\\mu_{k})-\\sigma\\left(\\varphi\\left(\\left(V-\\mu_{k}\\right)_{-}\\right)_{-}\\right)\\Big)\\Big|}\\\\ &{\\leq\\underset{s_{k}\\neq s_{k}}{\\min}\\ \\Big(\\sum_{a_{k}\\in\\mathcal{K}_{k}\\setminus\\mathcal{U}_{k}\\setminus\\mathcal{U}_{k}\\setminus\\mathcal{U}_{k}\\setminus\\mathcal{U}_{k}\\Big)}-\\Big(\\varphi\\left(\\left(V-\\mu_{k}\\right)_{-}\\right)_{-}\\Big)\\Big)\\Big|}\\\\ &{\\leq\\textnormal{m a x}\\Big\\{\\underset{s_{k}\\neq s_{k}\\neq s_{k}}{\\min}\\ \\Big\\{P_{k}^{\\operatorname*{max}}}\\Big\\ (\\int_{0}^{a_{k}}(V-\\mu_{k})-\\sigma\\left(\\varphi\\left(\\left(V-\\mu_{k}\\right)_{-}\\right)_{-}\\right)\\Big)\\Big\\}}\\\\ &{=\\underset{s_{k}\\neq s_{k}}{\\operatorname*{max}}\\ \\Big\\{P_{k}^{\\operatorname*{max}}}\\Big(\\int_{0}^{a_{k}}(V-\\mu_{k})^{\\operatorname*{max}}\\Big)-\\sigma\\left(\\varphi\\left(\\left(V-\\mu_{k}^{\\operatorname*{max}}\\right)_{-}\\right)_{-}\\Big)\\Big\\}}\\\\ &{\\qquad-\\underset{s_{k}\\neq s_{k}\\neq s_{k}}{\\operatorname*{max}}\\ \\Big\\{\\int_{0}^{a_{k}}(V-\\mu_{k})^{\\operatorname*{max}}\\ \\sigma\\left(\\varphi\\left(\\left(V-\\mu_{k}^{\\operatorname*{max}}\\right)_{-}\\right)_{-}\\right)\\Big\\}\\Big\\}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\left(\\varphi\\left(\\left(V-\\mu_{k}\\right)_{-}\\right)_{-}\\right)}\\\\ &{=\\underset{s_{k}\\neq s_{k}}{\\operatorname*{max}}\\ \\Big\\{\\left\\{P_{k}^{\\operatorname*{max}}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where in the first equality we use Lemma 3. The final inequality is a consequence of the 1- Lipschitznessof the max operator. First, we control $g_{s,a}(\\alpha_{P}^{\\lambda,\\omega},\\bar{V})$ To do so, we use for a fxd $\\alpha_{P}^{\\lambda,\\omega}$ and any vector $V$ that is independent with $\\widehat{P}^{0}$ , the Bernstein's inequality, one has with probability at least $1-\\delta$ with $s a$ -rectangular notations, ", "page_idx": 35}, {"type": "equation", "text": "$$\ng_{s,a}(\\alpha_{P}^{\\lambda,\\omega},V)=\\left|\\left(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\right)[V]_{\\alpha_{P}^{\\lambda,\\omega}}\\right|\\leq\\sqrt{\\frac{2\\log(\\frac{2}{\\delta})}{N}}\\sqrt{\\mathrm{Var}_{P_{s,a}^{0}}(V)}+\\frac{2\\log(\\frac{2}{\\delta})}{3N(1-\\gamma)}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Once pointwise concentration derived, we will use uniform concentration to yield this lemma. First, union bound, is obtained noticing that $g_{s,a}(\\alpha_{P}^{\\lambda,\\omega},V)$ is 1-Lipschitz w.r.t. $\\lambda$ and $\\omega$ as it is linear in \u5165 and w. Moreover, \\* =IIV - \u03bc\\* - wlI obeying )\\*\u2264 I= . The quanity w E [0, 1/(1 - )] as it is always smaller that $V$ by definition. We construct then a 2-dimensional a $\\varepsilon_{1}$ -net $N_{\\varepsilon_{1}}$ over $\\textstyle\\lambda^{*}\\in[0,\\frac{\\|1\\|_{*}}{1-\\gamma}]$ and $\\omega\\in[0,1/(1-\\gamma)]$ whose size satisfies $\\begin{array}{r}{|N_{\\varepsilon_{1}}|\\leq\\left(\\frac{3\\|1\\|_{*}}{\\varepsilon_{1}(1-\\gamma)}\\right)^{2}}\\end{array}$ [Vershynin, 2018]. Using union bound and (145), i holds with probability at least - A t that for all $\\lambda\\in N_{\\varepsilon_{1}}$ \uff0c ", "page_idx": 35}, {"type": "equation", "text": "$$\ng_{s,a}(\\alpha_{P}^{\\lambda},V)\\leq\\sqrt{\\frac{2\\log(\\frac{2S A|N_{\\varepsilon_{1}}|}{\\delta})}{N}}\\sqrt{\\mathrm{Var}_{P_{s,a}^{0}}(V)}+\\frac{2\\log(\\frac{2S A|N_{\\varepsilon_{1}}|}{\\delta})}{3N(1-\\gamma)}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Using the previous equation and also (144), it results in using notation $2\\log(\\frac{18S A N\\|1\\|_{*}}{\\delta})=L$ ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\varphi_{s,\\alpha}(\\alpha_{p,\\gamma}^{\\Delta},V)\\stackrel{(a)}{\\cong}\\underset{\\varphi_{\\phi,\\beta}^{\\Delta}(V_{n_{1}})}{\\operatorname*{sup}}\\left|\\left(P_{\\alpha_{s}}^{a}-\\widehat{P}_{\\alpha_{s}}^{a}\\right)\\right|V\\lvert\\omega_{s}\\rvert+\\varepsilon_{1}}\\\\ &{\\stackrel{(b)}{\\leq}\\sqrt{\\frac{2\\log(\\frac{\\delta A(N_{\\mathrm{ref}})}{N})}{N}}\\sqrt{\\operatorname*{sur}p_{\\varphi_{\\mathrm{s}}^{\\Delta}}(V)}+\\frac{2\\log(\\frac{2\\delta A(N_{\\mathrm{ref}})}{N})}{3N(1-\\gamma)}+\\varepsilon_{1}}\\\\ &{\\stackrel{(c)}{\\leq}\\sqrt{\\frac{2\\log(\\frac{2\\delta A(N_{\\mathrm{ref}})}{N})}{N}}\\sqrt{\\operatorname*{surp}_{\\varphi_{\\mathrm{s}}^{\\Delta}}(V)}+\\frac{\\log(\\frac{2\\delta A(N_{\\mathrm{ref}})}{N})}{N(1-\\gamma)}}\\\\ &{\\stackrel{(d)}{\\leq}\\sqrt{\\frac{L}{N}}\\sqrt{\\operatorname*{surp}_{\\varphi_{\\mathrm{s}}^{\\Delta}}(V)}+\\frac{L}{N(1-\\gamma)}}\\\\ &{\\leq\\sqrt{\\frac{L}{N}}\\lVert V\\rVert_{\\infty}+\\frac{L}{N(1-\\gamma)}}\\\\ &{\\leq2\\sqrt{\\frac{L}{(1-\\gamma)^{2N}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where (a) is because the optimal $\\alpha$ falls into the $\\varepsilon_{1}$ -ball centered around some point inside $N_{\\varepsilon_{1}}$ and $g_{s,a}(\\alpha_{P}^{\\lambda},V)$ is 1-Lipschitz with regard to $\\lambda$ and $\\omega$ , (b) is due to Eq. (146), (c) arises from taking $\\begin{array}{r}{\\varepsilon_{1}=\\frac{\\log(\\frac{2S A|N_{\\varepsilon_{1}}|}{\\delta})}{3N(1-\\gamma)}}\\end{array}$ , (d) is verified by $\\begin{array}{r}{|N_{\\varepsilon_{1}}|\\leq\\left(\\frac{3\\|1\\|_{*}}{\\varepsilon_{1}(1-\\gamma)}\\right)^{2}\\leq9N\\left\\|1\\right\\|}\\end{array}$ and that variance of a ceiling function of a vector is smaller than the variance of non-ceiling vector , and the last inequality comes from the fact V\\*;| \u2264 and taking $\\begin{array}{r}{N\\ge2\\log(\\frac{18S A N\\bar{\\|}1\\|_{*}}{\\delta})=L}\\end{array}$ ", "page_idx": 36}, {"type": "text", "text": "Contrary to the previous term, the second term $g_{s,a}(\\alpha_{\\hat{P}}^{\\lambda},V)$ is more difficult as we need concentration. Still, the data has an extra dependency through the parameter $\\alpha_{\\hat{P}}^{\\lambda}$ . We need to decouple this problem using absorbing MDPs. Then it leads to ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g_{s,a}(\\alpha_{\\hat{P}}^{\\lambda,\\omega},V)}\\\\ &{=\\big|\\operatorname*{max}_{\\mu_{\\hat{P}_{s,a}^{0}}\\in\\mathcal{M}_{\\hat{P}_{s,a}^{0}}^{\\lambda,\\omega}}\\left(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\right)(V-\\mu_{\\hat{P}_{s,a}^{0}}^{\\lambda,\\omega})\\big|}\\\\ &{=\\big|\\operatorname*{max}_{\\mu\\in\\mathcal{M}_{\\hat{P}_{s,a}^{0}}^{\\lambda,\\omega}}\\left(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\right)(V-\\mu_{P_{s,a}^{0}}^{\\lambda,\\omega})+\\left(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\right)(\\mu_{P_{s,a}^{0}}^{\\lambda,\\omega}-\\mu_{\\hat{P}_{s,a}^{0}}^{\\lambda,\\omega})\\big|}\\\\ &{\\leq\\big|\\operatorname*{max}_{\\mu\\in\\mathcal{M}_{\\hat{P}_{s,a}^{0}}^{\\lambda,\\omega}}\\left(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\right)(V-\\mu_{P_{s,a}^{0}}^{\\lambda,\\omega})+\\operatorname*{max}_{\\mu_{P_{s,a}^{0}}^{\\lambda,\\omega}}\\left(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\right)(\\mu_{P_{s,a}^{0}}^{\\lambda,\\omega}-\\mu_{\\hat{P}_{s,a}^{0}}^{\\lambda,\\omega})\\big|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "In the first equality, we add the term \u03bc pg.a to retreve he prevou concetraton roblem fxing $P_{s,a}^{0}$ and optimizing $\\lambda,\\omega$ . In the second, we extend the max using triangular inequality. The first term in the last equality is exactly the term we have controlled previously, while the second one needs more attention. We decouple the data's dependency, then control the difference between the $\\mu$ . Then using the characterization of the optimal $\\mu$ from equation (47): ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\left(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\right)(\\mu_{P_{s,a}^{0}}^{\\lambda,\\omega}-\\mu_{\\dot{P}_{s,a}^{0}}^{\\lambda,\\omega})=\\sum_{s^{\\prime}}\\lambda\\left(P_{s,a}^{0}(s^{\\prime})-\\widehat{P}_{s,a}^{0}(s^{\\prime})\\right)(\\nabla\\|P_{s,a}^{0}\\|-\\nabla\\Big\\|\\hat{P}_{s,a}^{0}\\Big\\|)\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Here we assume that the subgradient is a gradient as we assume that the norm is $C^{2}$ .The question that arises is whether the gradient of the norm is Lipschitz. ", "page_idx": 36}, {"type": "text", "text": "Note that we are considering the worst case as $(\\mu_{P_{s,a}^{0}}^{\\lambda,\\omega}-\\mu_{\\hat{P}_{s,a}^{0}}^{\\lambda,\\omega})$ \u03bc ) can be zero in the case where \u03bc the Lagrangian variable is equal to zero. Finally, note that we can also control this term when one of the two terms $\\mu_{P_{s,a}^{0}}^{\\lambda,\\omega}$ or $\\mu_{\\hat{P}_{s,a}^{0}}^{\\lambda,\\omega}$ A is equal to zero as \u03bcp $\\mu_{\\hat{P}_{s,a}^{0}}^{\\lambda,\\omega}$ and \u03bcpoa smaller that $V$ because $V-\\mu$ need to be positive in equation (43). In this case, classical control using Bernstein's inequality without uniform concentration can be applied, giving the same result. In the worst case where all terms in $(\\mu_{P_{s,a}^{0}}^{\\lambda,\\omega}-\\mu_{\\hat{P}_{s,a}^{0}}^{\\lambda,\\omega})$ are non zero, assuming that the norm is $C^{2}$ , using mean value theorem, we know that ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 37}, {"type": "equation", "text": "$$\n\\left\\|(\\nabla\\big\\|P_{s,a}^{0}\\big\\|-\\nabla\\Big\\|\\hat{P}_{s,a}^{0}\\Big\\|)\\right\\|_{2}\\leq\\operatorname*{sup}_{x\\in\\Delta(S)}\\left\\|\\nabla^{2}\\left\\|x\\right\\|\\left\\|_{2}\\right\\|(P_{s,a}^{0}-\\hat{P}_{s,a}^{0})\\right\\|_{2}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "As the norm is $C^{2}$ , is continuous and as the simplex is bounded, this quantity exists according to the Extreme value theorem. It is possible to compute this contact depending on $S$ for explicit norms such as $L_{p}$ . Indeed, for $L_{2}$ ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\nabla^{2}\\left\\|x\\right\\|_{2}=\\frac{\\left(I-\\frac{x\\otimes x}{\\left\\|x\\right\\|_{2}^{2}}\\right)}{\\left\\|x\\right\\|_{2}}\\leq\\frac{1}{\\left\\|x\\right\\|_{2}}I\\leq\\frac{1}{\\operatorname*{min}_{x\\in\\Delta(S)}\\left\\|x\\right\\|_{2}}I=\\sqrt{S}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $\\otimes$ is the Kronecker product. So we have an upper bound independent of $x$ .For $L_{p}=\\|\\boldsymbol{x}\\|_{p}$ norms, $p\\geq2$ , we have simple taking derivative twice: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\nabla^{2}\\left\\|x\\right\\|_{p}=\\frac{p-1}{L_{p}}\\left(A^{p-2}-g_{p}g_{p}^{T}\\right)\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "with ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{A=\\mathrm{Diag}\\left(\\frac{\\mathrm{abs}(x)}{L_{p}}\\right)}}\\\\ {\\displaystyle{g_{p}=A^{p-2}\\left(\\frac{x}{L_{p}}\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where Diag is the diagonal matrix. However, as $x\\leq L_{p},\\mathcal{A}\\leq I$ weget ", "page_idx": 37}, {"type": "equation", "text": "$$\nH\\leq\\frac{p-1}{\\left\\|x\\right\\|_{p}}\\leq(p-1)S^{1/q}=C_{S}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "wherethe $1/L_{p}$ is minimized for the uniform distribution. Then using Cauchy Swartz inequality, it holds ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\right)(\\mu_{P_{s,a}^{0}}^{\\lambda,\\omega}-\\mu_{\\widehat{P}_{s,a}^{0}}^{\\lambda,\\omega})\\leq\\lambda\\left\\|\\left(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\right)\\right\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Then thequstinshwtoboudthquantty $\\left\\|\\left(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\right)\\right\\|_{2}^{2}$ To do so, we wiluse MeDiarmid inequality. ", "page_idx": 37}, {"type": "text", "text": "Definition 3. Bounded difference property ", "page_idx": 37}, {"type": "text", "text": "Afunction $f:\\mathcal{X}_{1}\\times...\\,\\mathcal{X}_{n}\\to\\mathbb{R}$ satisfies the bounded difference property if for each $i=1,\\hdots,n$ the change of coordinate from $s_{i}$ to $s_{i}^{\\prime}$ may change the value of the function at most on $c_{i}$ ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\forall i\\in\\left[n\\right]:\\operatorname*{sup}_{x_{i}^{\\prime}\\in\\mathcal{X}_{i}}\\left|f\\left(x_{1},\\ldots,x_{i},\\ldots,x_{n}\\right)-f\\left(x_{1},\\ldots,x_{i}^{\\prime},\\ldots,x_{n}\\right)\\right|\\leq c_{i}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "In our case, we consider $\\begin{array}{r}{f\\left(X_{1},\\ldots,X_{n}\\right)\\,=\\,\\|\\sum_{k=1}^{n}X_{k}\\|_{2}}\\end{array}$ .Then we can notice that by triangle $x_{1},\\ldots,x_{n}$ $\\v x_{k}^{\\prime}$ $X_{i,s^{\\prime}}=P_{i,s,a}^{0}(s^{\\prime})-P_{s,a}^{0}(s^{\\prime})$ (indx $i$ holds forindexof ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f\\left(x_{1},\\ldots,x_{k},\\ldots,x_{n}\\right)=\\|x_{1}+\\ldots+x_{n}\\|_{2}\\leq\\|x_{1}+\\ldots+x_{n}-x_{k}+x_{k}^{\\prime}\\|_{2}+\\|x_{k}-x_{k}^{\\prime}\\|_{2}}\\\\ &{\\leq f\\left(x_{1},\\ldots,x_{k}^{\\prime},\\ldots,x_{n}\\right)+2}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Theorem 5. (McDiarmid's inequality). McDiarmid et al. [1989] Let $f:\\mathcal{X}_{1}\\times...\\,\\mathcal{X}_{n}\\to\\mathbb{R}$ be $a$ function satisfying the bounded difference property with bounds $c_{1},\\ldots,c_{n}$ . Consider independent random variables $X_{1},\\ldots,X_{n},X_{i}\\in{\\mathcal{X}}_{i}$ for all $i$ Then for any $t>0$ ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[f\\left(X_{1},\\ldots,X_{n}\\right)-\\mathbb{E}\\left[f\\left(X_{1},\\ldots,X_{n}\\right)\\right]\\geq t\\right]\\leq\\exp\\left(-{\\frac{2t^{2}}{\\sum_{i=1}^{n}c_{i}^{2}}}\\right)\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Using McDiarmid's inequality and union bound, we can bound the term here ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\Big(\\left\\|\\left(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\right)\\right\\|_{2}-\\mathbb{E}[\\left\\|\\left(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\right)\\right\\|_{2}]\\right)^{2}\\leq\\frac{2N\\log(|S||A|/\\delta)}{N^{2}}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "with probability $1-\\delta/(|S||A|)$ . Moreover, the additional term can be bounded as follows: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\Big\\|\\Big(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\Big)\\Big\\|_{2}^{2}]=\\mathbb{E}[\\sum_{s^{\\prime}}\\big(P_{s,a}^{0}(s^{\\prime})-P_{s,a}^{0}(s^{\\prime})\\big)^{2}=\\mathbb{E}[\\sum_{s^{\\prime}}(\\frac{1}{N}\\sum_{i}^{N}X_{i,s^{\\prime}})^{2}]\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "with $X_{i,s^{\\prime}}=P_{i,s,a}^{0}(s^{\\prime})-P_{s,a}^{0}(s^{\\prime})$ is one sample sampled from the generative model. Then ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\left\\|\\left(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\right)\\right\\|_{2}^{2}]=\\displaystyle\\frac{1}{N^{2}}\\sum_{s^{\\prime}}\\mathsf{V a r}(\\sum_{i}^{N}X_{i,s})\\triangleq\\displaystyle\\frac{1}{N^{2}}\\sum_{i}^{N}\\sum_{s^{\\prime}}\\mathsf{V a r}(X_{i,s})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\frac{1}{N^{2}}\\sum_{i}^{N}\\mathbb{E}(\\sum_{s^{\\prime}}X_{i,s}^{2})\\leq\\displaystyle\\frac{4}{N}}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where (a) the last equality comes from the independence of the random variables, and where the last inequality comes from the fact the maximum of two elements in the simplex is bounded by 2. Moreover, weknow that, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Big[\\Big\\|\\Big(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\Big)\\Big\\|_{2}\\Big]^{2}\\leq\\mathbb{E}[\\Big\\|\\Big(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\Big)\\Big\\|_{2}^{2}]\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "due to Jensen's inequality. Finally, regrouping the two terms, we obtain with probability $1\\,-$ $\\delta/(|S||A|)$ ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\left(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\right)\\right|_{2}^{2}=\\left(\\left\\|\\left(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\right)\\right\\|_{2}-\\mathbb{E}\\left[\\left\\|\\left(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\right)\\right\\|_{2}\\right]\\right)^{2}+\\left(\\mathbb{E}\\left[\\left\\|\\left(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\right)\\right\\|_{2}\\right]\\right)^{2}}\\\\ &{+\\;2\\mathbb{E}\\left[\\left\\|\\left(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\right)\\right\\|_{2}\\right]\\left(\\left\\|\\left(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\right)\\right\\|_{2}-\\mathbb{E}\\left[\\left\\|\\left(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\right)\\right\\|_{2}\\right]\\right)}\\\\ &{\\leq\\frac{2N\\log(|S||A|/(\\delta))}{N^{2}}+\\frac{4}{N}+\\frac{\\sqrt{\\frac{4}{N}}\\sqrt{2N\\log(|S||A|/(\\delta))}}{N}}\\\\ &{\\leq\\frac{10\\log(|S||A|/(\\delta))}{N}=\\frac{L^{\\prime}}{N}}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where in first inequality use $(a+b)^{2}=a^{2}+b^{2}+2a b$ and where in (a) we combine equation (158) and (157) and (156). ", "page_idx": 38}, {"type": "text", "text": "with $L^{\\prime}=10\\log(|S||A|/(\\delta))$ . Finally, plugging the previous equation in (155): ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mu\\in\\mu_{\\hat{P}_{s,a}^{0}}^{1}}\\left(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\right)(\\mu_{P_{s,a}^{0}}^{\\lambda}-\\mu)|\\leq\\operatorname*{max}_{\\lambda}\\left\\|\\left(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\right)\\right\\|_{2}^{2}C_{S}\\lambda.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "This term can be easily controlled by taking the supremum over $\\lambda$ , which is a 1 dimensional parameter. Then we can bound $\\dot{\\lambda}\\in[0,H\\,\\lVert\\boldsymbol{1}\\rVert_{*}]$ . Indeed, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\lambda^{*}=\\left\\|V-\\mu^{*}-\\eta\\right\\|_{*}\\leq\\left\\|V\\right\\|_{*}\\leq H\\left\\|1\\right\\|_{*}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Finally, we obtain: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\lambda}\\left\\|\\left(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\right)\\right\\|_{2}^{2}C_{S}\\lambda\\leq\\frac{L^{\\prime}C_{S}\\left\\|1\\right\\|_{*}}{N(1-\\gamma)}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Regrouping all terms: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\iota_{s,a}(\\alpha_{\\bar{P}}^{\\lambda},V)\\leq|\\operatorname*{max}_{\\mu_{P_{s,a}^{3}}\\in M_{P_{s,a}^{3}}^{\\lambda}}\\left(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\right)(V-\\mu_{P_{s,a}^{3}}^{\\lambda})+\\operatorname*{max}_{\\mu_{P_{s,a}^{3}}\\in M_{P_{s,a}^{3}}^{\\lambda}}\\left(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\right)(\\mu_{P_{s,a}^{0}}^{\\lambda}-\\mu_{\\bar{P}_{s,a}^{0}}^{\\lambda})}}\\\\ &{}&{\\leq2\\sqrt{\\frac{L}{N}}\\sqrt{\\mathrm{Var}_{P_{s,a}^{0}}(V)}+\\frac{L^{\\prime}C_{S}\\left\\|1\\right\\|_{*}}{N\\left(1-\\gamma\\right)}+\\frac{L}{N\\left(1-\\gamma\\right)}}\\\\ &{}&{\\leq2\\sqrt{\\frac{L}{N}}\\sqrt{\\mathrm{Var}_{P_{s,a}^{0}}(V)}+\\frac{3L C_{S}\\left\\|1\\right\\|_{*}}{N\\left(1-\\gamma\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "We can recognize that the second term is a second-order term as long as $N\\geq(C_{S}\\,\\|1\\|_{*})^{2}$ ,wecan regroup the two terms. Finally, as $g_{s,a}(\\alpha_{\\hat{P}}^{\\lambda},V)\\geq g_{s,a}(\\alpha_{P}^{\\lambda},V)$ weobtain ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\left|P_{s,a}^{\\pi,V}V-\\widehat{P}_{s,a}^{\\pi,V}V\\right|\\leq2\\sqrt{\\frac{L}{N}}\\sqrt{\\mathrm{Var}_{P_{s,a}^{0}}(V)}+\\frac{3L C_{S}\\left\\|1\\right\\|_{*}}{N(1-\\gamma)}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "It is important tonote that the geometry of the nom is present in the second order tem $\\frac{3L C_{S}\\left\\|1\\right\\|}{N\\left(1-\\gamma\\right)}$ but this term is negligible as it is proportional to $1/N$ with regard to the variance term in $1/\\sqrt{N}$ Moreover, note that the quantity $C_{S}\\left\\|1\\right\\|_{*}=S$ for $L_{2}$ norms. ", "page_idx": 39}, {"type": "text", "text": "For the specific case of $T V$ which is not $C^{2}$ smooth, this lemma still holds as in (144), we only need to control one term without the dependency on data in the supremum as $\\alpha_{P}^{\\lambda}$ reduces to a scalar $\\alpha$ which does not depend on $P$ . Then extra decomposition using smoothness of the norm is not needed, as the only remaining term in the max in (144) is the left-hand side term. ", "page_idx": 39}, {"type": "text", "text": "For the $s$ -rectangular case, the frst equation can be rewritten simply by factorizing by $\\pi(a|s)$ using lemma 4. ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left|P_{s,a}^{\\pi,V}V-\\hat{P}_{s,a}^{\\pi,V}V\\right|=\\displaystyle\\left|\\sum_{a}\\pi(a|s)\\right.\\underset{\\mu_{P,a}^{\\lambda}}{\\operatorname*{max}}\\underset{\\alpha,a}{\\operatorname*{max}}\\left\\{P_{s,a}^{0}(V-\\mu)-\\sigma\\left(\\mathrm{sp}((V-\\mu))_{*}\\right)\\right\\}}\\\\ &{\\hphantom{\\left.\\sum_{a}\\hat{\\mu}_{P,a}^{0}(V-\\mu)_{*,a}^{\\lambda}\\right|}-\\left.\\frac{\\operatorname*{max}}{\\mu_{P,a}^{\\lambda}\\in\\mathcal{M}_{P,s,a}^{\\lambda}}\\left\\{\\hat{P}_{s,a}^{0}(V-\\mu_{P,a}^{\\lambda})-\\sigma\\left(\\mathrm{sp}((V-\\mu_{P,a}^{\\lambda})_{*}\\right)\\right\\}\\right|}\\\\ &{\\hphantom{\\left.\\sum_{a}\\hat{\\mu}_{P,a}^{0}(V-\\mu)_{*,a}^{\\lambda}\\right|}\\leq\\displaystyle\\sum_{a}\\pi(a|s)\\Big(2\\sqrt{\\frac{L}{N}}\\sqrt{\\mathrm{Var}_{P,a}^{0}(V)}+\\frac{L C_{S}\\left|1\\right|_{*}}{N(1-\\gamma)}\\Big)}\\\\ &{=2\\sqrt{\\frac{L}{N}}\\sqrt{\\mathrm{Var}_{P_{s,a}^{0}}(V)}+\\frac{3L C_{S}\\left|1\\right|_{*}}{N(1-\\gamma)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "using $s a$ -rectangular results, which gives the result for $s$ -rectangular case. ", "page_idx": 39}, {"type": "text", "text": "Combining this lemma with a matrix notation using union bound, one has with probability $1-\\delta$ ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\left|\\underline{{\\hat{P}}}^{\\pi^{*},V}V^{\\pi^{*},\\sigma}-\\underline{{P}}^{\\pi^{*},V}V^{\\pi^{*},\\sigma}\\right|\\leq2\\sqrt{\\frac{L}{N}}\\sqrt{\\mathrm{Var}_{P^{*}}\\left(V^{\\star,\\sigma}\\right)}+\\frac{3L C_{S}\\left\\|\\boldsymbol{1}\\right\\|_{*}}{N(1-\\gamma)}1\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "(166) ", "page_idx": 39}, {"type": "text", "text": "D.3.4 Proof of Lemma 9 ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Using the same argument as in (216), it holds that for any $\\alpha^{*}$ solution of (53) ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\left(I-\\gamma\\underline{{\\widehat{P}}}^{\\pi^{\\star},V}\\right)^{-1}\\sqrt{\\mathrm{Var}_{\\underline{{\\widehat{P}}}^{\\pi^{\\star},V}}(V^{\\star,\\sigma})}=\\sqrt{\\frac{1}{1-\\gamma}}\\sqrt{\\sum_{t=0}^{\\infty}\\gamma^{t}\\left(\\underline{{\\widehat{P}}}^{\\pi^{\\star},V}\\right)^{t}\\mathrm{Var}_{\\underline{{\\widehat{P}}}^{\\pi^{\\star},V}}(V^{\\star,\\sigma})}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Then we can control $\\operatorname{Var}_{\\widehat{\\underline{{P}}}^{\\pi^{\\star},V}}\\left(V^{\\star,\\sigma}\\right)$ . Defining $V^{\\prime}:=V^{\\star,\\sigma}-\\eta1$ $\\eta\\in\\mathbb R$ . weuse Bellman's equation in (32)) which lead to ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V^{\\prime}=V^{\\star,\\sigma}-\\eta\\boldsymbol{1}\\leq V^{\\star,\\sigma}-\\eta\\boldsymbol{1}=r_{\\pi^{\\star}}+\\gamma\\underline{{P}}^{\\pi^{\\star},V}V^{\\star,\\sigma}-\\eta\\boldsymbol{1}}\\\\ &{=\\!r_{\\pi^{\\star}}+\\gamma{P^{\\pi^{\\star},V}}V^{\\star,\\sigma}-\\gamma\\sigma\\mathrm{sp}(V^{\\star,\\sigma})_{\\ast}-\\eta\\boldsymbol{1}}\\\\ &{\\quad=r_{\\pi^{\\star}}^{\\prime}+\\gamma\\underline{{\\widehat{P}}}^{\\pi^{\\star},V}V^{\\prime}+\\gamma\\Big({P^{\\pi^{\\star},V}}-\\underline{{\\widehat{P}}}^{\\pi^{\\star},V}\\Big)V^{\\star,\\sigma}-\\gamma\\sigma\\mathrm{sp}(V^{\\star,\\sigma})_{\\ast}}\\\\ &{\\quad=r_{\\pi^{\\star}}^{\\prime}+\\gamma\\underline{{\\widehat{P}}}^{\\pi^{\\star},V}V^{\\prime}+\\gamma\\Big(\\underline{{P}}^{\\pi^{\\star},V}-\\underline{{\\widehat{P}}}^{\\pi^{\\star},V}\\Big)V^{\\star,\\sigma}}\\\\ &{\\quad\\leq r_{\\pi^{\\star}}^{\\prime}+\\gamma\\underline{{\\widehat{P}}}^{\\pi^{\\star},V}V^{\\prime}+\\gamma\\Big(\\underline{{{P}}}^{\\pi^{\\star},V}-\\underline{{\\widehat{P}}}^{\\pi^{\\star},V}\\Big)V^{\\star,\\sigma}}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where in the second line we use Lemma 3. and we define $r_{\\pi^{\\star}}^{\\prime}=r_{\\pi^{\\star}}-(1-\\gamma)\\eta<r_{\\pi^{\\star}}<1$ We obtain the same result in $s$ -rectangular case using lemma 4 instead. Then ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\operatorname{Var}_{E^{-1}}v(v^{*},v^{*})}&{=\\frac{\\beta^{*}}{\\gamma}\\operatorname{Sup}_{\\beta\\in\\mathcal{E}^{*}\\times\\{V^{*}\\}}\\beta^{*}=\\beta^{*}\\cdot v^{*}(V^{*},\\sigma^{*})-(\\beta^{*}\\!\\!\\!\\!\\slash^{*}\\!\\!\\!\\!\\slash^{*}\\!\\!\\!\\!\\slash^{*}\\!\\!\\!\\!\\!\\slash^{*}\\!\\!\\!\\!\\!\\slash^{*}\\!\\!\\!\\!\\!\\!\\slash^{*}V)\\circ\\left(\\beta^{*}\\!\\!\\!\\!\\!\\!\\slash^{*}\\!\\!\\!\\!\\!\\slash^{*}V\\!\\!\\!\\!\\!\\slash^{*}\\!\\!\\!\\!\\!\\!\\slash^{*}\\right)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\!\\!=\\frac{\\beta^{*}}{2}v^{*}\\cdot v^{\\prime}(v^{*},v^{*})-(\\beta^{*}\\!\\!\\!\\!\\!\\slash^{*}\\!\\!\\!\\!\\slash^{*}\\!\\!\\!\\!\\!\\slash^{*}\\!\\!\\!\\!\\!\\slash^{*}\\!\\!\\!\\!\\!\\!\\slash^{*}\\!\\!\\!\\!\\!\\!\\slash^{*}\\!\\!\\!\\!\\!\\!\\slash^{*}\\!\\!\\!\\!\\!\\!\\!\\slash^{*}\\!\\!\\!\\!\\!\\!\\!\\slash^{*}\\!\\!\\!\\!\\!\\!\\!\\slash^{*}\\!\\!\\!\\!\\!\\!\\!\\slash^{*}\\!\\!\\!\\!\\!\\!\\!\\slash^{*}\\!\\!\\!\\!\\!\\!\\!\\slash^{*}\\!\\!\\!\\!\\!\\!\\!\\slash^{*}\\!\\!\\!\\!\\!\\!\\!\\slash^{*}\\!\\!\\!\\!\\!\\!\\!\\slash^{*}\\!\\!\\!\\!\\!\\!\\!\\slash^{*}\\!\\!\\!\\!\\!\\!\\!\\!\\slash^{*}\\!\\!\\!\\!\\!\\!\\!\\slash^{*}\\!\\!\\!\\!\\!\\!\\!\\!\\slash^{*}\\!\\!\\!\\!\\!\\!\\!\\!\\slash^{*}\\!\\!\\!\\!\\!\\!\\!\\!\\slash^{*}\\!\\!\\!\\!\\!\\!\\!\\!\\slash^{*}\\!\\!\\!\\!\\!\\!\\!\\!\\slash^{*}\\!\\!\\!\\!\\!\\!\\!\\!\\slash^{*}\\!\\!\\!\\!\\!\\!\\!\\!\\slash^{*}\\!\\!\\!\\!\\!\\!\\!\\!\\slash^{*}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\slash \n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where (a) holds by the fact that $\\operatorname{Var}_{P_{\\pi}}(V_{-}-\\eta1)=\\operatorname{Var}_{P_{\\pi}}(V)$ for any scalar $\\eta$ (b) follows from (172), moreover () comes from $\\begin{array}{r}{\\frac{1}{\\gamma^{2}}V^{\\prime}\\circ V^{\\prime}\\overset{\\cdot}{\\geq}\\frac{1}{\\gamma}V^{\\prime}\\circ V^{\\prime}}\\end{array}$ and $-1\\overset{.}{=}r_{\\pi^{\\star}}-(1-\\gamma)V_{\\mathrm{min}}1=r_{\\pi^{\\star}}^{\\prime}\\le r_{\\pi^{\\star}}\\le1$ ", "page_idx": 40}, {"type": "text", "text": "Finally, the inequality is due to Lemma 8. Plugging (176) into (167) gives, ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Big(I-\\gamma\\frac{\\beta^{\\star}}{1-\\gamma}\\Big)^{-1}\\sqrt{\\log_{F}x^{\\star}(V^{\\star})}}\\\\ &{\\leq\\sqrt{\\frac{1}{1-\\gamma}}\\displaystyle\\sum_{i\\in\\ nearrow\\ }^{\\infty}\\Bigg(\\frac{\\beta^{\\star}}{f^{\\star}}\\Big)^{\\prime}\\Bigg(\\frac{\\beta^{\\star}}{f^{\\star}}\\nu^{\\top}(V^{\\star}\\sigma^{\\top}\\nu^{i})-\\frac{1}{\\gamma}V^{\\prime}\\sigma^{\\top}\\nu^{i}+\\frac{2}{\\gamma^{2}}\\|V^{\\star}\\|_{\\infty}^{2}\\,\\Bigg.\\Bigg.\\qquad\\qquad\\mathrm{(I)}^{T}}\\\\ &{+\\frac{2}{\\gamma}\\|V^{\\star}\\|_{\\infty}\\Big(2\\sqrt{\\frac{L}{1-\\gamma}}\\frac{4\\,\\mathfrak{X}_{\\sigma}^{\\star}(\\mathbb{R})[\\,\\underline{{L}}]}{1-\\gamma}\\Big)\\Bigg)^{1/2}}\\\\ &{\\overset{(a)}{\\leq}\\sqrt{\\frac{1}{1-\\gamma}}\\displaystyle\\Bigg\\{\\frac{\\sqrt{\\sum_{i\\in\\ \\ }\\gamma}\\Big(\\frac{\\beta^{\\star}}{f^{\\star}}\\nu^{\\top}(\\mathbb{R}^{\\star})^{T}\\Big)^{\\prime}\\Big(\\frac{\\beta^{\\star}}{f^{\\star}}\\nu^{\\top}(V^{\\star}\\sigma^{\\top}\\nu^{i})-\\frac{1}{\\gamma}V^{\\prime}\\sigma^{\\top}\\nu^{i}\\Big)\\Bigg\\}}\\\\ &{\\quad+\\sqrt{\\frac{1}{1-\\gamma}}\\displaystyle\\sum_{i\\in\\ }^{\\infty}\\Bigg\\{\\frac{\\gamma}{f^{\\star}}\\Big(\\frac{\\beta^{\\star}}{f^{\\star}}\\nu^{\\top})^{\\prime}\\Bigg(\\frac{2}{f^{\\star}}\\mathbb{R}^{\\|\\gamma\\|_{\\infty}}+\\frac{2}{\\gamma}\\|V^{\\star}\\|_{\\infty}\\Big(2\\sqrt{\\frac{L}{(1-\\gamma)^{2}\\gamma}}+\\frac{3\\sigma_{\\mathcal{S}}^{\\star}\\|\\mathbb{I}\\|_{\\infty}^{\\perp}\\chi^{\\perp}}{N(1-\\gamma)}\\Big)\\Bigg.}\\\\ &{\\leq\\sqrt{\\frac{1}{1-\\gamma}}\\displaystyle\\Bigg\\{\\sum_{i\\geq\\sigma}^{\\sum}\\Big(\\frac{\\beta^{\\star}}{f^{\\star}}\\nu^{\\top}\\Big)^{\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "using in (i) the triangle inequality. The final part of the proof focuses on the first term, which follows ", "text_level": 1, "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\bigg|\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}\\Big(\\underline{{\\widehat{P}}}^{\\pi^{\\star},V}\\Big)^{t}\\Big(\\underline{{\\widehat{P}}}^{\\pi^{\\star},V}\\left(V^{\\prime}\\circ V^{\\prime}\\right)-\\frac{1}{\\gamma}V^{\\prime}\\circ V^{\\prime}\\Big)\\bigg|}\\\\ &{\\displaystyle=\\bigg|\\bigg(\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}\\Big(\\underline{{\\widehat{P}}}^{\\pi^{\\star},V}\\Big)^{t+1}-\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t-1}\\Big(\\underline{{\\widehat{P}}}^{\\pi^{\\star},V}\\Big)^{t}\\bigg)\\left(V^{\\prime}\\circ V^{\\prime}\\right)\\bigg|\\leq\\frac{1}{\\gamma}\\|V^{\\prime}\\|_{\\infty}^{2}1}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "using recursion between the two sums. Then, using (181) back to (180) leads to ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(I-\\gamma\\widehat{\\underline{{P}}}^{\\tau^{*},\\nu}\\right)^{-1}\\sqrt{\\mathrm{{Var}}_{{\\widehat{E}}^{\\tau^{*},\\nu}}(V^{*,\\sigma})}}\\\\ &{\\leq\\sqrt{\\frac{\\|V\\|_{\\infty}^{2}}{\\gamma(1-\\gamma)}}1+3\\sqrt{\\frac{\\left(1+\\left(\\sqrt{\\frac{L}{(1-\\gamma)^{2}N}}+\\frac{C s\\|\\|_{1}\\|_{\\infty}}{N(1-\\gamma)}\\right)\\right)\\|V^{\\prime}\\|_{\\infty}}{(1-\\gamma)^{2}\\gamma^{2}}}1}\\\\ &{\\leq4\\sqrt{\\frac{\\left(1+\\left(\\sqrt{\\frac{L}{(1-\\gamma)^{2}N}}+\\frac{C s\\|\\|_{1}\\|_{\\infty}}{N(1-\\gamma)}\\right)\\right)\\|V^{\\prime}\\|_{\\infty}}{(1-\\gamma)^{2}\\gamma^{2}}}}\\\\ &{\\leq4\\sqrt{\\frac{\\left(1+\\left(1\\sqrt{\\frac{L}{(1-\\gamma)^{2}N}}+\\frac{C s\\|\\|_{1}\\|_{\\infty}}{N(1-\\gamma)}\\right)\\right)\\|V^{\\prime}\\|_{\\infty}}{(1-\\gamma)^{2}\\gamma^{2}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Taking the infimum over $\\eta$ in the right-hand side, recall $V^{\\prime}:=V^{\\star,\\sigma}-\\eta1$ , we obtain the definition of the span semi norm. ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\bigg(I-\\gamma\\widehat{\\underline{{P}}}^{\\pi^{*},V}\\bigg)^{-1}\\sqrt{\\operatorname{Var}_{\\underline{{\\widehat{P}}}^{\\pi^{*},V}}\\big(V^{\\star,\\sigma}\\big)}\\leq4\\sqrt{\\frac{\\Big(1+\\Big(\\sqrt{\\frac{L}{(1-\\gamma)^{2}N}}+\\frac{C_{S}\\|1\\|_{\\infty}}{N(1-\\gamma)}\\Big)\\Big)\\operatorname{sp}(V^{\\star,\\sigma})_{*}}{\\big(1-\\gamma\\big)^{2}\\gamma^{2}}}1}\\\\ {\\leq4\\sqrt{\\frac{\\Big(1+\\Big(\\sqrt{\\frac{L}{(1-\\gamma)^{2}N}}+\\frac{C_{S}\\|1\\|_{\\infty}}{N(1-\\gamma)}\\Big)\\Big)}{\\gamma^{3}\\big(1-\\gamma\\big)^{2}\\operatorname*{max}\\{1-\\gamma,C_{S}\\sigma\\}}}1}\\\\ {\\leq4\\sqrt{\\frac{\\Big(1+\\Big(\\sqrt{\\frac{L}{(1-\\gamma)^{2}N}}+\\frac{C_{S}\\|1\\|_{\\infty}L}{N(1-\\gamma)}\\Big)\\Big)}{\\gamma^{3}\\big(1-\\gamma\\big)^{3}}}1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where the penultimate inequality follows from applying Lemma 5 with $P=P^{0}$ and $\\pi=\\pi^{\\star}$ ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\operatorname{sp}(V^{\\star,\\sigma})_{\\ast}\\leq\\frac{1}{\\gamma\\operatorname*{max}\\{1-\\gamma,C_{g}\\sigma\\}}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "or with an extra factor for s rectangular assumptions. ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathrm{sp}(V^{\\star,\\sigma})_{\\ast}\\leq\\frac{1}{\\gamma\\operatorname*{max}\\{1-\\gamma,\\operatorname*{min}_{s}\\|\\pi_{s}\\|_{\\ast}\\,\\tilde{\\sigma}C g\\}}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "D.3.5 Proof of Lemma 10 ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "In this proof, we will $s a$ -rectangular notations, for any $(s,a)\\in S\\times A$ , using the results in (144). In the $s a$ -rectangular case: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{P}_{s,a}^{\\widehat{\\pi},\\widehat{\\nu}}\\widehat{V}^{\\widehat{\\pi},\\sigma}-P_{s,a}^{\\widehat{\\pi},\\widehat{V}}\\widehat{V}^{\\widehat{\\pi},\\sigma}\\Big|\\leq\\operatorname*{max}\\Big\\{\\left|\\left(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\right)\\left[\\widehat{V}^{\\widehat{\\pi},\\sigma}\\right]_{\\alpha_{P s,a}^{\\lambda,\\omega*}}\\right|,\\left|\\left(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\right)\\left[\\widehat{V}^{\\widehat{\\pi},\\sigma}\\right]_{\\alpha_{P s,a}^{\\lambda,\\omega*}}\\right|\\Big\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "The first term in this max can be bounded using: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\left(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\right)\\left[\\widehat{V}^{\\widehat{\\pi},\\sigma}\\right]_{\\alpha_{P_{s a}}^{\\lambda,\\omega}}\\right|}\\\\ &{\\stackrel{\\mathrm{(a)}}{\\leq}\\left(\\left|\\left(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\right)\\left[\\widehat{V}^{\\star,\\sigma}\\right]_{\\alpha_{P_{s a}}^{\\lambda,\\omega}}\\right|+\\left|\\left(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\right)\\left(\\left[\\widehat{V}^{\\widehat{\\pi},\\sigma}\\right]_{\\alpha_{P_{s a}}^{\\lambda,\\omega}}-\\left[\\widehat{V}^{\\star,\\sigma}\\right]_{\\alpha_{P_{s a}}^{\\lambda,\\omega}}\\right)\\right|\\right)}\\\\ &{\\leq\\left(\\left|\\left(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\right)\\left[\\widehat{V}^{\\star,\\sigma}\\right]_{\\alpha_{P_{s a}}^{\\lambda,\\omega}}\\right|+\\left\\|P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\right\\|_{1}\\left\\|\\left[\\widehat{V}^{\\widehat{\\pi},\\sigma}\\right]_{\\alpha_{P_{s a}}^{\\lambda,\\omega}}-\\left[\\widehat{V}^{\\star,\\sigma}\\right]_{\\alpha_{P_{s a}}^{\\lambda,\\omega}}\\right\\|_{\\infty}\\right)}\\\\ &{\\stackrel{\\mathrm{(b)}}{\\leq}\\left|\\left(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\right)\\left[\\widehat{V}^{\\star,\\sigma}\\right]_{\\alpha_{P_{s a}}^{\\lambda,\\omega}}\\right|+2\\left\\|\\widehat{V}^{\\widehat{\\pi},\\sigma}-\\widehat{V}^{\\star,\\sigma}\\right\\|_{\\infty}}\\\\ &{\\stackrel{\\mathrm{(c)}}{\\leq}\\left|\\left(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\right)\\left[\\widehat{V}^{\\star,\\sigma}\\right]_{\\alpha_{P_{s a}}^{\\lambda,\\omega}}\\right|+2\\varepsilon_{\\mathrm{opt}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where (a comes from the riangle ineqgulity, and (b) comes fomn $\\left\\|P_{s,a}^{0}\\;-\\;\\widehat{P}_{s,a}^{0}\\right\\|_{1}\\;\\leq\\;2$ and $\\begin{array}{r}{\\big\\|\\big[\\widehat{V}^{\\widehat{\\pi},\\sigma}\\big]_{\\alpha_{P s a}^{\\lambda,\\omega*}}\\,-\\,\\big[\\widehat{V}^{\\star,\\sigma}\\big]_{\\alpha_{P s a}^{\\lambda,\\omega*}}\\big\\|_{\\infty}\\,\\leq\\,\\big\\|\\widehat{V}^{\\widehat{\\pi},\\sigma}\\,-\\,\\widehat{V}^{\\star,\\sigma}\\big\\|_{\\infty},}\\end{array}$ and c) fllows from the definition o the optimization error in (55). The second term of the max can be controlled in the same manner, i.e.: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\left(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\right)[\\widehat{V}^{\\widehat{\\pi},\\sigma}]_{\\alpha_{P_{s,a}}^{\\lambda,\\omega}}\\right|\\leq\\left|\\left(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\right)[\\widehat{V}^{\\star,\\sigma}]_{\\alpha_{P_{s,a}}^{\\lambda,\\omega*}}\\right|+2\\varepsilon_{\\mathrm{opt}}}\\\\ &{\\leq|\\operatorname*{max}_{\\mu_{P_{s,a}^{\\lambda}}^{\\lambda}\\in\\mathcal{M}_{P_{p_{s,a}^{0}}^{\\lambda}}}\\left(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\right)(\\widehat{V}^{\\star,\\sigma}-\\mu_{P_{s,a}^{\\lambda}}^{\\lambda})+\\operatorname*{max}_{\\mu_{P_{s,a}^{\\lambda}}^{\\lambda}\\in\\mathcal{M}_{P_{s,a}^{\\lambda}}^{\\lambda}}\\left(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\right)(\\mu_{P_{s,a}^{0}}^{\\lambda}-\\mu_{\\widehat{P}_{s,a}^{0}}^{\\lambda})|}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where the last inequality follow the decomposition of (150). Finally, to control the remaining term ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mu_{P_{s,a}^{0}}^{\\lambda}\\in\\mathcal{M}_{P_{s,a}^{0}}^{\\lambda}}\\left(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\right)(\\widehat{V}^{\\star,\\sigma}-\\mu_{P_{s,a}^{0}}^{\\lambda})=\\operatorname*{max}_{\\alpha_{P}^{\\lambda}\\in\\mathrm{A}_{P}^{\\lambda}}\\left\\{\\left(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\right)\\left[V\\right]_{\\alpha_{P}^{\\lambda}}\\right\\}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "(191) for any given $\\begin{array}{r}{\\alpha\\in[0,\\alpha_{P_{s a}}^{\\lambda,\\omega*}[\\subset[0,\\frac{1}{1-\\gamma}]^{S}}\\end{array}$ inthearatifilepa $\\lambda$ with the dependency between $\\widehat{V}^{\\star,\\sigma}$ and $\\widehat{P}^{0}$ , we resort to the following leave-one-out argument or absorbing MDPs used in [Agarwal et al., 2020, Li et al., 2022b, Shi and Chi, 2022, Clavier et al., 2023]. To begin, we create a collection of auxiliary RMDPs that exhibit the intended statistical independence between robust value functions and the estimated nominal transition kernel. These auxiliary RMDPs are designed to be minimally distinct from the initial RMDPs, subsequently, we manage to control the relevant term within these auxiliary RMDPs and demonstrate that its value closely approximates the target quantity for the desired RMDP. Recall that the empirical infinite-horizon robust MDP $\\widehat{\\mathcal{M}}_{\\sf r o b}$ is defined using the nominal transition kernel $\\widehat{P}^{0}$ . Inspired by Agarwal et al. [2020], we can construct an auxiliary absorbing robust MDP $\\widehat{\\mathcal{M}}_{\\mathrm{rob}}^{s,u}$ for each state $s$ and any non-negative scalar $u\\geq0$ that it is the same as $\\widehat{\\mathcal{M}}_{\\sf r o b}$ except for the transition properties in state $s$ . These auxiliary MDPS are called absorbing MDPs are have been used for the first time in the context of RMDPS in Clavier et al. [2023]Dening thereward funtond nina ransitnlf $\\widehat{\\mathcal{M}}_{\\mathrm{rob}}^{s,u}$ $P^{s,u}$ and $r^{s,u}$ which are expressed as follows using the same notation as Shi et al. [2023]: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{\\left\\{{\\Gamma}^{s,u}(s,a)=u\\qquad\\qquad}&&{\\forall a\\in\\mathcal{A},}\\\\ &{\\left\\{{\\Gamma}^{s,u}(\\widetilde{s},a)=r(\\widetilde{s},a)\\qquad\\qquad}&&{\\forall(\\widetilde{s},a)\\in\\mathcal{S}\\times\\mathcal{A}\\mathrm{~and~}\\widetilde{s}\\neq s.}\\\\ &{\\left\\{{P}^{s,u}(s^{\\prime}\\mid s,a)=1(s^{\\prime}=s)\\qquad\\qquad}&&{\\forall(s^{\\prime},a)\\in\\mathcal{S}\\times\\mathcal{A},}\\\\ &{\\left\\{{P}^{s,u}(\\cdot\\mid\\widetilde{s},a)=\\widehat{P}^{0}(\\cdot\\mid\\widetilde{s},a)\\qquad\\qquad}&&{\\forall(\\widetilde{s},a)\\in\\mathcal{S}\\times\\mathcal{A}\\mathrm{~and~}\\widetilde{s}\\neq s,}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Nominal transition probabitya state $s$ of the auxiliary $\\widehat{\\mathcal{M}}_{\\mathrm{rob}}^{s,u}$ never leaves state $s$ once entered, which gives the name absorbing to these auxiliary RMPDs. Finally, we define the robust Bellman operator $\\widehat{T}_{s,u}^{\\sigma}(\\cdot)$ assciated $\\widehat{\\mathcal{M}}_{\\mathrm{rob}}^{s,u}$ ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\widehat{\\mathcal{T}}_{s,u}^{\\sigma}(Q)(\\tilde{s},a)=r^{s,u}(\\tilde{s},a)+\\gamma\\operatorname*{inf}_{\\mathcal{P}\\in\\mathcal{U}^{s,\\sigma}(P_{\\tilde{s},a}^{s,u})}\\mathcal{P}V,\\qquad\\mathrm{with}\\;V(\\tilde{s})=\\operatorname*{max}_{a}Q(\\tilde{s},a).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "in $s a$ -rectangular case and with stochastic policy in $s$ -rectangular case. Using these auxiliary RMDPs we can remark equivalence between $\\widehat{\\mathcal{M}}_{\\sf r o b}$ and the auxiliary RMDP $\\widehat{\\mathcal{M}}_{\\mathrm{rob}}^{s,u}$ fixed-point. First, $\\widehat{Q}^{\\star,\\sigma}$ is the unique-fixed point of ${\\widehat{\\mathcal{T}}}^{\\sigma}(\\cdot)$ with associated value $\\widehat{V}^{\\star,\\sigma}$ . We will show that the robust value function $\\widehat{V}_{s,u^{\\star}}^{\\star,\\sigma}$ obtained from the fxed point of $\\widehat{\\mathcal{T}}_{s,u}^{\\sigma}(\\cdot)$ is the ame as the therobust value funtion $\\widehat{V}^{\\star,\\sigma}$ derived from ${\\widehat{\\mathcal{T}}}^{\\sigma}(\\cdot)$ , as long as we choose $u$ as ", "page_idx": 43}, {"type": "equation", "text": "$$\nu^{\\star}:=u^{\\star}(s)=\\widehat{V}^{\\star,\\sigma}(s)-\\gamma\\operatorname*{inf}_{\\mathcal{P}\\in\\mathcal{U}^{\\smash{\\mathrm{sa},\\sigma}}(e_{s})}\\mathcal{P}\\widehat{V}^{\\star,\\sigma}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "with $e_{s}$ is the $s$ -th standard basis vector in $\\mathbb{R}^{S}$ . This assertion is verified as: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 First for state $s^{\\prime}\\neq s$ , for all $a\\in A$ : it holds ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{r^{s,u^{\\star}}(s^{\\prime},a)+\\gamma\\operatorname*{inf}_{\\mathcal{P}\\in\\mathcal{U}^{s,\\sigma}(P_{s^{\\prime},a}^{s,u^{\\star}})}\\mathcal{P}\\widehat{V}^{\\star,\\sigma}=r(s^{\\prime},a)+\\gamma\\operatorname*{inf}_{\\mathcal{P}\\in\\mathcal{U}^{s,\\sigma}(\\widehat{P}_{s^{\\prime},a}^{0})}\\mathcal{P}\\widehat{V}^{\\star,\\sigma}}&{}\\\\ {=\\widehat{T}^{\\sigma}(\\widehat{Q}^{\\star,\\sigma})(s^{\\prime},a)=\\widehat{Q}^{\\star,\\sigma}(s^{\\prime},a),}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where the first equality holds because of (193) and (194), and the last inequality comes from that $\\widehat{Q}^{\\star,\\sigma}$ is the fixed point of ${\\widehat{\\mathcal{T}}}^{\\sigma}(\\cdot)$ (see Lemma C.3) and the definition of the robust Bellman operator in (13). ", "page_idx": 43}, {"type": "text", "text": "\u00b7 Then for state $s$ , for any $a\\in A$ ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{r^{s,u^{\\star}}(s,a)+\\gamma\\operatorname*{inf}_{\\mathcal{P}\\in\\mathcal{U}^{\\sigma}(P_{s,a}^{s,u^{\\star}})}\\mathcal{P}\\widehat{V}^{\\star,\\sigma}=u^{\\star}+\\gamma\\operatorname*{inf}_{\\mathcal{P}\\in\\mathcal{U}^{\\smash{\\mathrm{sa},\\sigma}}(e_{s})}\\mathcal{P}\\widehat{V}^{\\star,\\sigma}}\\\\ &{\\ =\\widehat{V}^{\\star,\\sigma}(s)-\\gamma\\operatorname*{inf}_{\\mathcal{P}\\in\\mathcal{U}^{\\smash{\\mathrm{sa},\\sigma}}(e_{s})}\\mathcal{P}\\widehat{V}^{\\star,\\sigma}+\\gamma\\operatorname*{inf}_{\\mathcal{P}\\in\\mathcal{U}^{\\smash{\\mathrm{sa},\\sigma}}(e_{s})}\\mathcal{P}\\widehat{V}^{\\star,\\sigma}=\\widehat{V}^{\\star,\\sigma}(s),}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "using in the frst equality is the definition of $P_{s,a}^{s,u^{\\star}}$ in (194) and where we use the definition of $\\boldsymbol{u}^{\\star}$ in (196) in the second one. ", "page_idx": 44}, {"type": "text", "text": "Finall, wehave provedthat therexists ad p $\\widehat{Q}_{s,u^{\\star}}^{\\star,\\sigma}$ of the operator $\\widehat{\\mathcal{T}}_{s,u^{\\star}}^{\\sigma}(\\cdot)$ by taking ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\{\\widehat{Q}_{s,u^{\\star}}^{\\star,\\sigma}(s,a)=\\widehat{V}^{\\star,\\sigma}(s)\\qquad\\qquad\\qquad\\quad\\forall a\\in\\mathcal{A},\\ }\\\\ &{\\left\\{\\widehat{Q}_{s,u^{\\star}}^{\\star,\\sigma}(s^{\\prime},a)=\\widehat{Q}^{\\star,\\sigma}(s^{\\prime},a)\\qquad\\qquad\\qquad\\quad\\forall s^{\\prime}\\not=s\\mathrm{~and~}a\\in\\mathcal{A}.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "we have confirmed the existence of a fixed point of the operator $\\widehat{T}_{s,u^{\\star}}^{\\sigma}(\\cdot)$ with corresponding value function $\\widehat{V}_{s,u^{\\star}}^{\\star,\\sigma}$ that coincide with $\\widehat{V}^{\\star,\\sigma}$ . Note that the corresponding properties between $\\widehat{\\mathcal{M}}_{\\sf r o b}$ and $\\widehat{\\mathcal{M}}_{\\mathrm{rob}}^{s,u}$ in Step and tep 2 hold in faet for anyunertainty set and $s\\cdot$ 0. $s a$ rectangularassumptons. Equipped with these fixed point equalities, we can use concentration inequalities to show this lemma. ", "page_idx": 44}, {"type": "text", "text": "Concentration inequality using an $\\varepsilon$ -net for all reward values $u$ .First we can verify that ", "text_level": 1, "page_idx": 44}, {"type": "equation", "text": "$$\n0\\leq u^{\\star}\\leq[\\widehat{V}^{\\star,\\sigma}(s)]_{\\alpha_{P_{s,a}}^{\\lambda,\\omega\\ast}}\\leq\\widehat{V}^{\\star,\\sigma}(s)\\leq\\frac{1}{1-\\gamma}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Then, we define a $N_{\\varepsilon_{2}}$ -net over the interval $\\left[0,1/(1-\\gamma)\\right]$ , where $|N_{\\varepsilon_{2}}|$ the size of the net can be controlled by [Ne2l \u2264 2(1-) [Vershynin, 2018]. The only parameter that varies is $\\lambda$ in the variation family, $\\alpha_{P_{s a}}^{\\lambda}$ so we have 1-dimensional control and not a vector in $\\mathbb{R}^{S}$ . Then similarly to Lemma C.3, it holds that for each $u\\in N_{\\varepsilon_{2}}$ , there exists aunique fixd point $\\widehat{Q}_{s,u}^{\\star,\\sigma}$ of the operator $\\widehat{T}_{s,u}^{\\sigma}(\\cdot)$ , which satisfies $\\begin{array}{r}{0\\leq\\widehat{Q}_{s,u}^{\\star,\\sigma}\\leq\\frac{1}{1-\\gamma}\\cdot1}\\end{array}$ Consequently, the corresponding robust value function can be upper bound by $\\begin{array}{r}{\\left\\|\\widehat{V}_{s,u}^{\\star,\\sigma}\\right\\|_{\\infty}\\leq\\frac{1}{1-\\gamma}}\\end{array}$ Uusig(19ad  bynstton fral $u\\,\\in N_{\\varepsilon_{2}}$ \uff0c\uff0c $\\widehat{\\mathcal{M}}_{\\mathrm{rob}}^{s,u}$ statistically independent of $\\widehat{P}_{s,a}^{0}$ This independence indicates that $[\\widehat{V}_{s,u}^{\\star,\\sigma}]_{\\alpha}$ and $\\widehat{P}_{s,a}^{0}$ are independent for a fixed $\\alpha$ . Using (148) and (149) and taking the union bound over all $(s,a,\\alpha)\\in\\mathcal{S}\\times\\mathcal{A}\\times N_{\\varepsilon_{1}}$ $u\\in N_{\\varepsilon_{2}}$ gives that, with probability at least $1-\\delta$ , it holds for all $(s,a,u)\\in\\mathcal{S}\\times\\mathcal{A}\\times N_{\\varepsilon_{2}}$ that ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\alpha_{P s a}^{\\lambda,\\omega}\\in\\Lambda_{P_{s a}}^{\\lambda,\\omega}}{\\operatorname*{max}}\\left|\\left(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\right)\\left[\\widehat{V}_{s,u}^{\\star,\\sigma}\\right]_{\\alpha_{P s a}^{\\lambda,\\omega}}\\right|\\leq2\\sqrt{\\frac{2\\log\\left(\\frac{18\\|1\\|_{*}S A N\\|N_{\\varepsilon_{2}}\\|}{\\delta}\\right)}{N}}\\sqrt{\\mathrm{Var}_{P_{s,a}^{0}}(\\widehat{V}_{s,u}^{\\star,\\sigma})}}\\\\ &{\\phantom{\\leq2}+\\varepsilon_{2}}\\\\ &{\\phantom{\\leq2}\\leq2\\sqrt{\\frac{2\\log\\left(\\frac{18\\|1\\|_{*}S A N\\|N_{\\varepsilon_{2}}\\|}{\\delta}\\right)}{(1-\\gamma)^{2}N}}+\\varepsilon_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Finally, we use uniform concentration to obtain the lemma. Recalling that $u^{\\star}\\in\\bigl[0,\\frac{1}{1-\\gamma}\\bigr]$ (see (200)), we can always find some $\\overline{{u}}\\in N_{\\varepsilon_{2}}$ such that $|\\overline{{u}}-u^{\\star}|\\leq\\varepsilon_{2}$ . Consequently, plugging in the operator $\\widehat{T}_{s,u}^{\\sigma}(\\cdot)$ in (195) yields ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\forall Q\\in\\mathbb{R}^{S A}:\\quad\\left\\lVert\\widehat{\\mathcal{T}}_{s,\\overline{{u}}}^{\\sigma}(Q)-\\widehat{\\mathcal{T}}_{s,u^{\\star}}^{\\sigma}(Q)\\right\\rVert_{\\infty}=|\\overline{{u}}-u^{\\star}|\\leq\\varepsilon_{2}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "We can then remark that the fixed points of $\\widehat{T}_{s,\\overline{{u}}}^{\\sigma}(\\cdot)$ and $\\widehat{T}_{s,u^{\\star}}^{\\sigma}(\\cdot)$ obey ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\lVert\\widehat{Q}_{s,\\overline{{u}}}^{\\star,\\sigma}-\\widehat{Q}_{s,u^{\\star}}^{\\star,\\sigma}\\right\\rVert_{\\infty}=\\left\\lVert\\widehat{T}_{s,\\overline{{u}}}^{\\sigma}(\\widehat{Q}_{s,\\overline{{u}}}^{\\star,\\sigma})-\\widehat{T}_{s,u^{\\star}}^{\\sigma}(\\widehat{Q}_{s,u^{\\star}}^{\\star,\\sigma})\\right\\rVert_{\\infty}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\left\\lVert\\widehat{T}_{s,\\overline{{u}}}^{\\sigma}(\\widehat{Q}_{s,\\overline{{u}}}^{\\star,\\sigma})-\\widehat{T}_{s,\\overline{{u}}}^{\\sigma}(\\widehat{Q}_{s,u^{\\star}}^{\\star,\\sigma})\\right\\rVert_{\\infty}+\\left\\lVert\\widehat{T}_{s,\\overline{{u}}}^{\\sigma}(\\widehat{Q}_{s,u^{\\star}}^{\\star,\\sigma})-\\widehat{T}_{s,u^{\\star}}^{\\sigma}(\\widehat{Q}_{s,u^{\\star}}^{\\star,\\sigma})\\right\\rVert_{\\infty}}\\\\ &{\\qquad\\qquad\\leq\\gamma\\left\\lVert\\widehat{Q}_{s,\\overline{{u}}}^{\\star,\\sigma}-\\widehat{Q}_{s,u^{\\star}}^{\\star,\\sigma}\\right\\rVert_{\\infty}+\\varepsilon_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where we use that the operator $\\widehat{T}_{s,u}^{\\sigma}(\\cdot)$ is a $\\gamma$ -contraction. It gives that: ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\left\\|\\widehat{Q}_{s,\\overline{{u}}}^{\\star,\\sigma}-\\widehat{Q}_{s,u^{\\star}}^{\\star,\\sigma}\\right\\|_{\\infty}\\le\\frac{\\varepsilon_{2}}{(1-\\gamma)}\\quad\\mathrm{and}\\quad\\left\\|\\widehat{V}_{s,\\overline{{u}}}^{\\star,\\sigma}-\\widehat{V}_{s,u^{\\star}}^{\\star,\\sigma}\\right\\|_{\\infty}\\le\\left\\|\\widehat{Q}_{s,\\overline{{u}}}^{\\star,\\sigma}-\\widehat{Q}_{s,u^{\\star}}^{\\star,\\sigma}\\right\\|_{\\infty}\\le\\frac{\\varepsilon_{2}}{(1-\\gamma)}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Finally to control the first term in (191), using the identity $\\widehat V^{\\star,\\sigma}\\,=\\,\\widehat V_{s,u^{\\star}}^{\\star,\\sigma}$ or fixed point relation between the two RMPDS, established in previous step of the proof gives that: for all $(s,a)\\in S\\times A$ ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{max}_{{\\mathbf{x}},{\\mathbf{q}},{\\mathbf{p}},\\tau,\\tau,\\eta,\\tau,\\tau,\\theta^{\\prime}}}\\\\ &{\\le\\frac{1}{\\eta_{s}^{2}\\tau+\\theta^{2}\\tau+1}\\left(F_{\\eta,t}^{0}-\\bar{F}_{\\eta,t}^{0}\\right)\\vert\\nabla^{\\theta,\\tau}\\vert_{{\\mathbf{x}}^{t},\\tau}\\Bigg\\vert}\\\\ &{\\le\\frac{1}{\\eta_{s}^{2}\\tau+\\theta^{2}\\tau+1}\\left(\\left(F_{\\eta,t}^{0}-\\bar{F}_{\\eta,t}^{0}\\right)\\vert\\nabla^{\\theta,\\tau}\\vert_{{\\mathbf{x}}^{t},\\tau}\\right)\\Bigg\\vert+\\left(F_{\\eta,t}^{0}-\\bar{F}_{\\eta,t}^{0}\\right)\\left(\\left(\\frac{F_{\\eta,t}^{0}-\\bar{F}_{\\eta,t}^{0}}{\\tau+1}-\\frac{(F_{\\eta,t}^{0}-\\bar{F}_{\\eta,t}^{0})}{\\tau+1}\\right)\\right)\\Bigg\\vert}\\\\ &{\\overset{(a)}{\\le}\\frac{1}{\\eta_{s}^{2}\\tau+\\theta^{2}\\tau+1}\\left\\{\\left(F_{\\eta,t}^{0}-\\bar{F}_{\\eta,t}^{0}\\right)\\vert\\nabla^{\\theta,\\tau}\\vert_{{\\mathbf{x}}^{t},\\tau}\\right\\}+\\left\\{\\frac{2\\eta_{s}^{2}}{\\eta_{s}^{2}}\\right\\}}\\\\ &{\\overset{(b)}{\\le}\\frac{1}{\\eta_{s}^{2}\\tau+\\theta^{2}\\tau+1}\\!\\!+\\!\\!\\frac{\\vert F_{\\eta,t}^{0}\\vert\\vert_{{\\mathbf{x}}^{t},\\tau}}{\\eta_{s}^{2}\\tau+\\theta^{2}\\tau+1}\\!\\!+\\!\\!\\frac{\\vert F_{\\eta,t}^{0}\\vert\\vert_{{\\mathbf{x}}^{t},\\tau}}{\\sqrt{\\eta}\\tau+\\theta^{2}\\tau+1}+\\frac{4\\eta_{s}^{2}\\vert\\vert\\vert\\nabla^{\\theta,\\tau}\\vert_{{\\mathbf{x}}^{t},\\tau}\\vert_{{\\mathbf{x}}^{t},\\tau}}{3\\vert\\nabla^{\\theta,\\tau}\\vert_{{\\mathbf{x}}^{t},\\tau}}}\\\\ &{\\le\\frac{3}{\\eta_{s}^{2}\\tau+1}+2\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "with $\\begin{array}{r}{L^{\\prime\\prime}=\\log\\left(\\frac{54\\|1\\|_{*}S A N^{2}}{(1-\\gamma)\\delta}\\right)}\\end{array}$ where (a) comes from triangular inequality, (b) is due (203), for any QERS ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\left(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\right)\\left([\\widehat{V}_{s,\\overline{{u}}}^{\\star,\\sigma}]_{\\alpha}-[\\widehat{V}_{s,u^{\\star}}^{\\star,\\sigma}]_{\\alpha}\\right)\\right|\\leq\\left\\|P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\right\\|_{1}\\left\\|[\\widehat{V}_{s,\\overline{{u}}}^{\\star,\\sigma}]_{\\alpha}-[\\widehat{V}_{s,u^{\\star}}^{\\star,\\sigma}]_{\\alpha}\\right\\|_{\\infty}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq2\\left\\|\\widehat{V}_{s,\\overline{{u}}}^{\\star,\\sigma}-\\widehat{V}_{s,u^{\\star}}^{\\star,\\sigma}\\right\\|_{\\infty}\\leq\\frac{2\\varepsilon_{2}}{(1-\\gamma)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "(c) follows from (201), (d) holds using Lemma 1 with (203). Here, the two last inequalities hold by $\\begin{array}{r}{\\varepsilon_{2}=\\frac{2\\log(\\frac{18\\|1\\|_{*}S A N|N\\varepsilon_{2}|}{\\delta})}{N}}\\end{array}$ $\\begin{array}{r}{|N_{\\varepsilon_{2}}|\\leq\\frac{3}{\\varepsilon_{2}(1-\\gamma)}\\leq\\frac{3N}{1-\\gamma}}\\end{array}$   \nby the faet $\\begin{array}{r}{\\operatorname{Var}_{P_{s,a}^{0}}(\\widehat{V}^{\\star,\\sigma})\\leq\\|\\widehat{V}^{\\star,\\sigma}\\|_{\\infty}\\leq\\frac{1}{1-\\gamma}}\\end{array}$ and leting $\\begin{array}{r}{N\\ge2\\log\\left(\\frac{54\\|1\\|_{*}S A N^{2}}{(1-\\gamma)\\delta}\\right)=L^{\\prime\\prime}}\\end{array}$   \nRewriting (186), the first term of the max is controlled. ", "page_idx": 45}, {"type": "text", "text": "", "page_idx": 45}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\left\\{\\left\\vert\\left(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\right)\\left[\\widehat{V}^{\\widehat{\\pi},\\sigma}\\right]_{\\alpha_{P s,a}^{\\lambda\\ast}}\\right\\vert,\\left\\vert\\left(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\right)\\left[\\widehat{V}^{\\widehat{\\pi},\\sigma}\\right]_{\\alpha_{\\widehat{P}_{s,a}}^{\\lambda\\ast}}\\right\\vert\\right\\}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "The second term can be controlled by the same term as the first one plus an additional term with ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\left|\\left(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\right)\\left[\\widehat{V}^{\\widehat{\\pi},\\sigma}\\right]_{\\alpha\\widehat{P}_{s,a}}\\right|\\leq\\right.\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\left.\\left|\\mathbf{\\widehat{\\Pi}}_{\\widehat{P}_{s,a}}^{0}\\right|\\leq\\right.\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\left.\\left(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\right)\\left(\\mu_{P_{s,a}^{0}}^{\\lambda}-\\mu_{\\widehat{P}_{s,a}^{0}}^{\\lambda}\\right)\\left(\\widehat{V}^{\\star,\\sigma}-\\mu_{P_{s,a}^{0}}^{\\lambda}\\right)+\\operatorname*{max}_{\\mu_{P_{s,a}^{0}}^{\\lambda}\\in\\mathcal{M}_{P_{s,a}^{0}}^{\\lambda}}\\left(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\right)\\left(\\mu_{P_{s,a}^{0}}^{\\lambda}-\\mu_{\\widehat{P}_{s,a}^{0}}^{\\lambda}\\right)\\right|\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "and similarly to previous lemma in (159), the residual or term in the right in the previous equation can be controlled with LCl Fially puting (205) and 206) back into Equation 191) and using Eq. (206) with probability at least $1-\\delta$ weobtain ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\widehat{P}_{s,a}^{\\widehat{\\pi},\\widehat{V}}\\widehat{V}^{\\widehat{\\pi},\\sigma}-P_{s,a}^{\\widehat{\\pi},\\widehat{V}}\\widehat{V}^{\\widehat{\\pi},\\sigma}\\right|\\leq\\underset{\\alpha_{P s,a}^{\\lambda,\\omega}}{\\operatorname*{max}}\\left|\\left(P_{s,a}^{0}-\\widehat{P}_{s,a}^{0}\\right)[\\widehat{V}^{\\star,\\sigma}]_{\\alpha_{P s,a}^{\\lambda,\\omega}}\\right|+2\\varepsilon_{\\mathsf{o p t}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq2\\sqrt{\\frac{L^{\\prime}}{N}}\\sqrt{\\mathrm{Var}_{P_{s,a}^{0}}(\\widehat{V}^{\\star,\\sigma})}+2\\varepsilon_{\\mathsf{o p t}}+\\frac{14L^{\\prime\\prime}C_{S}\\left\\|1\\right\\|_{*}}{N\\left(1-\\gamma\\right)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq2\\sqrt{\\frac{L^{\\prime\\prime}}{(1-\\gamma)^{2}N}}+2\\varepsilon_{\\mathsf{o p t}}+\\frac{14L^{\\prime\\prime}C_{S}\\left\\|1\\right\\|_{*}}{N\\left(1-\\gamma\\right)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "$\\forall(s,a)\\in S\\times A$ . Using matrix form we obtain finally: ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\vert\\displaystyle\\widehat{\\underline{{P}}}^{\\widehat{\\pi},\\widehat{V}}\\widehat{V}^{\\widehat{\\pi},\\sigma}-\\underline{{P}}^{\\widehat{\\pi},\\widehat{V}}\\widehat{V}^{\\widehat{\\pi},\\sigma}\\right\\vert\\leq2\\sqrt{\\displaystyle\\frac{L^{\\prime\\prime}}{N}}\\sqrt{\\mathrm{Var}_{P_{s,a}^{0}}(\\widehat{V}^{\\star,\\sigma})}1+2\\varepsilon_{\\mathsf{o p t}}1}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq2\\sqrt{\\displaystyle\\frac{L^{\\prime\\prime}}{(1-\\gamma)^{2}N}}1+2\\varepsilon_{\\mathsf{o p t}}1.+\\displaystyle\\frac{14L^{\\prime\\prime}C_{S}\\left\\|1\\right\\|_{*}}{N(1-\\gamma)}1}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "The proof is similar in the $s$ -rectangular case, factorising by $\\pi(a|s)$ , like in in 8. Moreover, the proof is similar for $T V$ without the geometric term depending on $C_{S}$ ", "page_idx": 46}, {"type": "text", "text": "D.3.6Proof of Lemma 11 ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "We always use the same manner as in Appendix D.3.4. Similarly to (167), it holds: ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\Big(I-\\gamma\\underline{{P}}^{\\widehat{\\pi},\\widehat{V}}\\Big)^{-1}\\sqrt{\\operatorname{Var}_{\\underline{{P}}^{\\widehat{\\pi},\\widehat{V}}}\\big(\\widehat{V}^{\\widehat{\\pi},\\sigma}\\big)}\\leq\\sqrt{\\frac{1}{1-\\gamma}}\\sqrt{\\sum_{t=0}^{\\infty}\\gamma^{t}\\Big(\\underline{{P}}^{\\widehat{\\pi},\\widehat{V}}\\Big)^{t}\\operatorname{Var}_{\\underline{{P}}^{\\widehat{\\pi},\\widehat{V}}}\\big(\\widehat{V}^{\\widehat{\\pi},\\sigma}\\big)}.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "In order to upper bound $\\operatorname{Var}_{\\underline{{P}}^{\\widehat{\\pi},\\widehat{\\nu}}}(\\widehat{V}^{\\widehat{\\pi},\\sigma})$ , we define $V^{\\prime}:=\\widehat{V}^{\\widehat{\\pi},\\sigma}-\\eta1$ with $\\eta\\in\\mathbb R$ . Using as (174), it holds ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\mathrm{Var}_{\\underline{{P}}^{\\widehat{\\pi},\\widehat{\\nu}}}(\\widehat{V}^{\\widehat{\\pi},\\sigma})\\leq\\underline{{P}}^{\\widehat{\\pi},\\widehat{V}}\\left(V^{\\prime}\\circ V^{\\prime}\\right)-\\frac{1}{\\gamma}V^{\\prime}\\circ V^{\\prime}+\\frac{2}{\\gamma^{2}}\\|V^{\\prime}\\|_{\\infty}1+\\frac{2}{\\gamma}\\|V^{\\prime}\\|_{\\infty}\\left|\\left(\\underline{{\\widehat{P}}}^{\\widehat{\\pi},\\widehat{V}}-\\underline{{P}}^{\\widehat{\\pi},\\widehat{V}}\\right)\\widehat{V}^{\\widehat{\\pi},\\widehat{\\nu}}\\right|}&{}\\\\ &{\\leq\\underline{{P}}^{\\widehat{\\pi},\\widehat{V}}\\left(V^{\\prime}\\circ V^{\\prime}\\right)-\\frac{1}{\\gamma}V^{\\prime}\\circ V^{\\prime}+}&{}\\\\ &{\\frac{2}{\\gamma^{2}}\\|V^{\\prime}\\|_{\\infty}1+\\frac{2}{\\gamma}\\|V^{\\prime}\\|_{\\infty}\\left(2\\sqrt{\\frac{L^{\\prime\\prime}}{(1-\\gamma)^{2}N}}+2\\varepsilon_{\\mathrm{opt}}+\\frac{14L^{\\prime\\prime}C_{S}\\|1\\|_{\\*}}{N(1-\\gamma)}\\right)1,}&{\\ \\ \\mathrm{(211)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where the last inequality makes use of Lemma 10. Plugging (212) back into (210) leads to ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Big(I-\\gamma E^{\\sharp,\\widehat{\\nu}}\\Big)^{-1}\\sqrt{\\operatorname*{var}_{\\underline{{{\\xi}}}^{\\star},\\rho}(\\widehat{\\nu}^{\\sharp,\\sigma})}\\overset{(a)}{\\leq}\\sqrt{\\frac{1}{1-\\gamma}}\\sqrt{\\left|\\sum_{t=0}^{\\infty}\\gamma\\left(\\int_{\\mathbb{T}}^{\\infty}\\gamma^{t}\\left(\\int_{\\mathbb{T}}^{\\#,\\widehat{\\nu}}\\right)^{t}\\left(\\int_{\\mathbb{T}}^{\\#,\\widehat{\\nu}}\\left(V^{\\prime}\\circ V^{\\prime}\\right)-\\frac{1}{\\gamma}V^{\\prime}\\circ V^{\\prime}\\right)\\right.}}\\\\ &{\\left.\\quad+\\sqrt{\\frac{1}{(1-\\gamma)^{2}\\gamma^{2}}}\\left(2\\sqrt{\\frac{L^{\\mu}}{(1-\\gamma)^{2}N}}+2\\ell_{\\infty}+\\frac{14L^{\\sigma}C_{2}\\times|\\mathbb{T}|_{\\infty}}{N(1-\\gamma)}\\right)\\|V^{\\prime}\\|_{\\infty}\\right.}\\\\ &{\\overset{(b)}{\\leq}\\left.\\sqrt{\\frac{\\|V^{\\prime}\\|_{2}^{2}}{\\gamma(1-\\gamma)}}\\boldsymbol{1}+\\sqrt{\\frac{\\left(2\\sqrt{\\frac{L^{\\mu}}{(1-\\gamma)^{2}N}}+2\\ell_{\\infty}+\\frac{4L^{\\sigma}C_{3}\\|\\mathbb{T}\\|_{\\infty}}{N(1-\\gamma)}\\right)\\|V^{\\prime}\\|_{\\infty}}{(1-\\gamma)^{2}\\gamma^{2}}}}\\\\ &{\\overset{(c)}{\\leq}\\sqrt{\\frac{\\|V^{\\prime}\\|_{2}^{2}}{\\gamma(1-\\gamma)}}+5\\sqrt{\\left(1+\\varepsilon_{\\infty}+\\frac{L^{\\sigma}C_{3}}{N(1-\\gamma)}\\left\\|\\mathbb{I}_{\\geq}\\right\\|_{-\\gamma}\\right)\\left\\|\\mathbb{I}_{\\geq}^{\\gamma}\\right\\|_{-\\gamma}}}\\\\ &{\\leq6\\sqrt{\\left(1+\\varepsilon_{\\infty}+\\frac{L^{\\gamma}C_{3}\\|\\mathbb{I}\\|_{\\infty}}{N(1-\\gamma)}\\right)\\frac{\\|V^{\\prime}\\|_{\\infty}}{(1-\\gamma)^{2}\\gamma^{2}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where (a) is the same as (180), (b) holds by repeating the argument of (181), (c) follows by taking $\\begin{array}{r}{N\\ge\\frac{L^{\\prime\\prime}}{(1-\\gamma)^{2}}}\\end{array}$ andthntlastqll $\\begin{array}{r}{\\|V^{\\prime}\\|_{\\infty}\\leq\\|V^{\\star,\\sigma}\\|_{\\infty}\\leq\\frac{1}{1-\\gamma}}\\end{array}$ Thn taking the infimum over $\\eta$ in the right-hand side of the equation in the definition of $V^{\\prime}$ and using $\\mathrm{sp}(.)_{\\infty}\\leq\\|.\\|_{*}$ gives ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\Big(I-\\gamma\\underline{{P}}^{\\widehat{\\pi},\\widehat{V}}\\Big)^{-1}\\sqrt{\\operatorname{Var}_{\\underline{{P}}^{\\widehat{\\pi},\\widehat{V}}}(\\widehat{V}^{\\widehat{\\pi},\\sigma})}\\leq6\\sqrt{\\Big(1+\\varepsilon_{\\mathsf{o p t}}+\\frac{L^{\\prime\\prime}C_{S}\\left\\|1\\right\\|_{*}}{N(1-\\gamma)}\\Big)\\frac{\\operatorname{sp}(V)_{\\infty}}{(1-\\gamma)^{2}\\gamma^{2}}1}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Finally, applying Lemma 5 with $P=\\widehat{P}^{0}$ and $\\pi={\\widehat{\\pi}}$ yields ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\operatorname{sp}(\\widehat{V}^{\\widehat{\\pi},\\sigma})_{*}\\leq\\frac{1}{\\gamma\\operatorname*{max}\\{1-\\gamma,\\gamma C_{g}\\sigma\\}},\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "for $s a$ -rectangular or ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\operatorname{sp}(\\widehat{V}^{\\widehat{\\pi},\\sigma})_{*}\\leq\\frac{1}{\\gamma\\operatorname*{max}\\{1-\\gamma,\\operatorname*{min}_{s}\\|\\hat{\\pi}\\|_{*}\\,\\tilde{\\sigma}\\}}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "in the $s$ -rectangular case, which can be inserted into (214) and gives in $s a$ -rectangular case: ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(I-\\gamma\\underline{{P}}^{\\widehat{\\pi},\\widehat{V}}\\right)^{-1}\\sqrt{\\mathrm{Var}_{\\underline{{P}}^{\\widehat{\\pi},\\widehat{V}}}(\\widehat{V}^{\\widehat{\\pi},\\sigma})}\\leq6\\sqrt{\\frac{\\left(1+\\varepsilon_{\\mathsf{o p t}}+\\frac{L^{\\prime\\prime}C_{S}\\|1\\|_{*}}{N(1-\\gamma)}\\right)}{\\gamma^{3}(1-\\gamma)^{2}\\operatorname*{max}\\{1-\\gamma,\\sigma\\}}}1}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq6\\sqrt{\\frac{\\left(1+\\varepsilon_{\\mathsf{o p t}}+\\frac{L^{\\prime\\prime}C_{S}\\|1\\|_{*}}{N(1-\\gamma)}\\right)}{(1-\\gamma)^{3}\\gamma^{3}}}1}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where first inequalities comes from that we can bound it Eq. left-hand side of equation (214) by $\\begin{array}{r}{\\|V^{\\prime}\\|_{\\infty}\\leq\\|V^{\\star,\\dot{\\sigma}}\\|_{\\infty}\\leq\\frac{1}{1-\\gamma}}\\end{array}$ Proof for $s$ -rectangular is similar, but requires adding an extra factor depending on the norm of the current policy and we have: ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(I-\\gamma\\underline{{P}}^{\\widehat{\\pi},\\widehat{V}}\\right)^{-1}\\sqrt{\\mathrm{Var}_{\\underline{{P}}^{\\widehat{\\pi},\\widehat{V}}}(\\widehat{V}^{\\widehat{\\pi},\\sigma})}\\leq6\\sqrt{\\frac{\\left(1+\\varepsilon_{\\mathsf{o p t}}+\\frac{L^{\\prime\\prime}C_{S}\\|\\boldsymbol{1}\\|_{*}}{N(1-\\gamma)}\\right)}{\\gamma^{3}(1-\\gamma)^{2}\\operatorname*{max}\\{1-\\gamma,C_{g}\\tilde{\\sigma}\\operatorname*{min}_{s}\\|\\hat{\\pi}_{s}\\|_{\\infty}\\}}}1}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq6\\sqrt{\\frac{\\left(1+\\varepsilon_{\\mathsf{o p t}}+\\frac{L^{\\prime\\prime}C_{S}\\|\\boldsymbol{1}\\|_{*}}{N(1-\\gamma)}\\right)}{(1-\\gamma)^{3}\\gamma^{2}}}1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "D.3.7 Proof of Lemma 7 ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "First, if each row of $P_{\\pi}$ belongs to the simplex $\\Delta(S)$ , it lead that the row of $\\left(1-\\gamma\\right)\\left(I-\\gamma P_{\\pi}\\right)^{-1}$ falls into $\\Delta(S)$ . Then, ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left(I-\\gamma P_{\\pi}\\right)^{-1}\\sqrt{\\mathrm{Var}_{P_{\\pi}}(V^{\\pi,P})}=\\cfrac{1}{1-\\gamma}(1-\\gamma)\\left(I-\\gamma P_{\\pi}\\right)^{-1}\\sqrt{\\mathrm{Var}_{P_{\\pi}}(V^{\\pi,P})}\\,}&{}\\\\ {\\overset{(a)}{\\leq}\\cfrac{1}{1-\\gamma}\\sqrt{\\left(1-\\gamma\\right)\\left(I-\\gamma P_{\\pi}\\right)^{-1}\\mathrm{Var}_{P_{\\pi}}(V^{\\pi,P})}\\,}&{}\\\\ {=\\sqrt{\\cfrac{1}{1-\\gamma}}\\sqrt{\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}\\left(P_{\\pi}\\right)^{t}\\mathrm{Var}_{P_{\\pi}}(V^{\\pi,P})}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where (a)is due to Jensen's inequality. Then for any $\\eta\\in\\mathbb{R}^{+}$ \uff0c $V^{\\prime}:=V^{\\pi,P}-\\eta1$ , we can upper bound $\\mathrm{Var}_{P_{\\pi}}(V^{\\pi,P})$ ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Var}_{P_{\\pi}}(V^{\\pi,P})\\stackrel{(\\mathrm{i})}{=}\\mathrm{Var}_{P_{\\pi}}(V^{\\prime})=P_{\\pi}\\left(V^{\\prime}\\circ V^{\\prime}\\right)-\\left(P_{\\pi}V^{\\prime}\\right)\\circ\\left(P_{\\pi}V^{\\prime}\\right)}\\\\ &{\\stackrel{(\\mathrm{ii})}{\\leq}P_{\\pi}\\left(V^{\\prime}\\circ V^{\\prime}\\right)-\\frac{1}{\\gamma^{2}}\\left(V^{\\prime}-r_{\\pi}+(1-\\gamma)\\eta1\\right)\\circ\\left(V^{\\prime}-r_{\\pi}+(1-\\gamma)\\eta1\\right)}\\\\ &{=P_{\\pi}\\left(V^{\\prime}\\circ V^{\\prime}\\right)-\\frac{1}{\\gamma^{2}}V^{\\prime}\\circ V^{\\prime}+\\frac{2}{\\gamma^{2}}V^{\\prime}\\circ\\left(r_{\\pi}-(1-\\gamma)\\eta1\\right)}\\\\ &{-\\,\\frac{1}{\\gamma^{2}}\\left(r_{\\pi}-(1-\\gamma)\\eta1\\right)\\circ\\left(r_{\\pi}-(1-\\gamma)\\eta1\\right)}\\\\ &{\\leq P_{\\pi}\\left(V^{\\prime}\\circ V^{\\prime}\\right)-\\frac{1}{\\gamma}V^{\\prime}\\circ V^{\\prime}+\\frac{2}{\\gamma^{2}}\\|V^{\\prime}\\|_{\\infty}1\\leq P_{\\pi}\\left(V^{\\prime}\\circ V^{\\prime}\\right)-\\frac{1}{\\gamma}V^{\\prime}\\circ V^{\\prime}+\\frac{2}{\\gamma^{2}}\\|V^{\\prime}\\|_{\\infty}1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where (i) holds by the fact that $\\operatorname{Var}_{P_{\\pi}}(V^{\\pi,P}-b1)=\\operatorname{Var}_{P_{\\pi}}(V^{\\pi,P})$ for any scalar $b$ and $V^{\\pi,P}\\in\\mathbb{R}^{S}$ \uff0c (i follows from $V^{\\prime}\\leq r_{\\pi}+\\gamma P_{\\pi}V^{\\pi,P}-\\eta1=r_{\\pi}-(1-\\gamma)\\eta1+\\gamma P_{\\pi}V^{\\prime},$ and the last line arises from $\\begin{array}{r}{\\frac{1}{\\gamma^{2}}V^{\\prime}\\circ V^{\\prime}\\geq\\frac{1}{\\gamma}V^{\\prime}\\circ V^{\\prime}}\\end{array}$ and $\\|r_{\\pi}-(1-\\gamma)\\eta1\\|_{\\infty}\\leq1$ . for $\\eta\\in[0,1/(1-\\gamma)[$ Plugging (218) back to (216) leads to ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\int_{\\mathbb{Z}}-\\gamma P_{\\alpha})^{-1}\\sqrt{\\operatorname{Var}_{\\gamma_{+}}(V^{\\eta,\\rho})}\\leq\\sqrt{\\frac{1}{1-\\gamma}}\\sqrt{\\sum_{\\gamma=0}^{\\infty}\\gamma^{t}\\left(P_{\\alpha}\\left(V^{\\prime}\\circ V^{\\prime}\\right)-\\frac{1}{\\gamma}V^{\\prime}\\circ V^{\\prime}+\\frac{2}{\\gamma^{2}}\\|V^{\\prime}\\|_{\\infty}\\right)}}\\\\ &{\\displaystyle\\stackrel{(b)}{\\leq}\\sqrt{\\frac{1}{1-\\gamma}}\\sqrt{\\left|\\sum_{\\gamma=0}^{\\infty}\\gamma^{t}\\left(P_{\\alpha}\\left(V^{\\prime}\\circ V^{\\prime}\\right)-\\frac{1}{\\gamma}V^{\\prime}\\circ V^{\\prime}\\right)\\right|}+\\sqrt{\\frac{1}{1-\\gamma}}\\sqrt{\\sum_{\\gamma=0}^{\\infty}\\gamma^{t}\\left(P_{\\alpha}\\right)^{\\frac{2}{\\gamma^{2}}}\\|V^{\\prime}\\|_{\\infty}\\right|}}\\\\ &{\\displaystyle\\leq\\sqrt{\\frac{1}{1-\\gamma}}\\sqrt{\\left|\\left(\\sum_{\\gamma=0}^{\\infty}\\gamma^{t}\\left(P_{\\alpha}\\right)^{t+1}-\\sum_{\\ell=0}^{\\infty}\\gamma^{t-1}\\left(P_{\\alpha}\\right)^{t}\\right)(V^{\\prime}\\circ V^{\\prime})\\right|}+\\sqrt{\\frac{2\\|V\\|_{\\infty}\\|}{\\gamma^{2}(1-\\gamma)^{2}}}}\\\\ &{\\displaystyle\\stackrel{(b)}{\\leq}\\sqrt{\\frac{\\|V\\|_{\\infty}^{2}}{\\gamma(1-\\gamma)}}+\\sqrt{\\frac{2\\|V\\|_{\\infty}\\|}{\\gamma^{2}(1-\\gamma)^{2}}}}\\\\ &{\\displaystyle\\leq\\sqrt{\\frac{8\\|V\\|_{\\infty}^{2}}{\\gamma^{2}(1-\\gamma)^{2}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where (i) holds due to, (i) holds by following recursion between the two sums, and the last inequality holdsbecause $\\begin{array}{r}{\\|V^{\\prime}\\|_{\\infty}\\leq\\frac{1}{1-\\gamma}}\\end{array}$ .Thentakingtheminimumover $\\eta$ intheright-handside of thequation gives the result. ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\left(I-\\gamma P_{\\pi}\\right)^{-1}{\\sqrt{\\operatorname{Var}_{P_{\\pi}}(V^{\\pi,P})}}\\leq{\\sqrt{\\frac{8\\mathrm{sp}(V^{\\pi,P})_{\\infty}}{\\gamma^{2}(1-\\gamma)^{2}}}}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "However, we also $\\begin{array}{r}{\\|V^{\\prime}\\|_{\\infty}\\leq\\|V^{\\pi,P}\\|_{\\infty}\\leq\\frac{1}{1-\\gamma}}\\end{array}$ in (219). So finally, the result is ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\left(I-\\gamma P_{\\pi}\\right)^{-1}{\\sqrt{\\operatorname{Var}_{P_{\\pi}}(V^{\\pi,P})}}\\leq{\\sqrt{\\frac{8}{\\gamma^{2}(1-\\gamma)^{2}}\\mathrm{sp}(V^{\\pi,P})_{\\infty}}}1.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "EProof of Theorem 2 ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "In this section, we focus on the scenarios in the uncertainty sets are constructed with $(s,a)$ rectangularity condition with some general norms. Towards this, we firstly observe that for the two limiting cases $\\ell_{1}$ norm and $\\ell_{\\infty}$ norm, one has $\\|p_{1}-p_{2}\\|_{1}\\leq2$ and $\\|p_{1}-p_{2}\\|_{\\infty}\\leq1$ for any two probability distribution $p_{1},p_{2}\\in\\mathbb{R}^{S}$ . Namely, the accessible ranges of the uncertainty level $\\sigma$ for $\\ell_{1}$ norm and $\\ell_{\\infty}$ norm are $(0,2]$ and $(0,1]$ , respectively. In addition, we have ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\forall p_{1},p_{2}\\in\\mathbb{R}^{S}:\\quad\\|p_{1}-p_{2}\\|_{\\infty}\\leq\\|p_{1}-p_{2}\\|\\leq\\|p_{1}-p_{2}\\|_{1}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "for any norm $\\Vert\\cdot\\Vert$ . It indicates that the accessible range of the uncertainty level $\\sigma_{\\parallel\\cdot\\parallel}$ for any given norm $\\|\\cdot\\|$ is between $\\left(0,\\sigma_{\\parallel\\cdot\\parallel}^{\\mathrm{max}}\\right]$ , where $1\\leq\\sigma_{\\parallel\\cdot\\parallel}^{\\operatorname*{max}}\\leq2$ ", "page_idx": 49}, {"type": "text", "text": "To continue, we specify the definition of the uncertainty set with $s a$ -rectangularity condition with some given general norm II Il as below: for any nominal transition kernel P  RSAxS, ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\mathcal{U}_{||\\cdot||}^{\\sigma}(P):=\\mathcal{U}_{||\\cdot||}^{\\sigma}(P)=\\otimes\\mathcal{U}_{p}^{\\sigma}(P_{s,a}),\\qquad\\mathcal{U}_{||\\cdot||}^{\\sigma}(P_{s,a}):=\\left\\{P_{s,a}^{\\prime}\\in\\Delta(S):\\left\\|P_{s,a}^{\\prime}-P_{s,a}\\right\\|\\leq\\sigma_{\\|\\cdot||}\\right\\}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Then, we real the asumption of the unertainty radius $\\sigma_{\\parallel\\cdot\\parallel}\\in\\big(0,\\sigma_{\\parallel\\cdot\\parallel}^{\\operatorname*{max}}(1-c_{0})\\big]$ Wih $0<c_{0}<1$ Then, resorting to the same class of hard MDPs in [Shi et al., 2023, Section C.1], we can complete the proof by directly following the same proof pipeline of Shi et al. [2023, Section C] by replacing $\\sigma$ $\\sigma_{\\parallel\\cdot\\parallel}^{\\mathrm{max}}\\sigma_{\\parallel\\cdot\\parallel}$ ", "page_idx": 49}, {"type": "text", "text": "F Proof of Theorem 4 ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Developing the lower bound for the cases with $s$ -rectangular uncertainty set involves several new challenges compared to that of $(s,a)$ -rectangular cases. Specifically, the first challenge is that the optimal policy can be stochastic and hard to be characterized with a closed form for the RMDPs with a $s$ -rectangular uncertainty set, rather than deterministic polices in $(s,a)$ -rectangular cases. Such richer and smoother class of optimal policies makes slightly changing the transition kernel generally could only leads to a smoothly changed stochastic optimal policy instead of a completely different one. Such reduced changing of optimal policy further gives smaller performance gap, thus challenges of a tighter lower bound. Second, most of the hard instances in the literature are constructed as $S A$ states with a constant number of action spaces without loss of generality. While when it comes to $s$ -rectangular uncertainty set, the action space size becomes important and can't be assumed as a constant anymore. So a new class of instances are required. ", "page_idx": 49}, {"type": "text", "text": "To address these challenges, in this section, we construct a new set of hard RMDP instances for two limiting cases: $\\ell_{1}$ norm and $\\ell_{\\infty}$ norm. ", "page_idx": 49}, {"type": "text", "text": "F.1 Construction of the hard problem instances ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Before proceeding, we introduce two useful sets related to the state space and action space as below: ", "page_idx": 49}, {"type": "equation", "text": "$$\n{\\cal S}=\\{0,1,\\ldots,{\\cal S}\\},\\qquad\\mathrm{and}\\qquad{\\cal A}=\\{0,1,\\cdots,{\\cal A}-1\\}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "In this section, we construct a set of RMDPs termed as $\\mathcal{M}_{\\ell_{\\infty}}$ , which consists of $S(A\\!-\\!1)$ components including $S(A-1)$ components, each associates with some different state-action pair. Specifically, it is defined as ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{M}_{\\ell_{\\infty}}:=\\left\\{\\mathcal{M}_{\\theta}=\\left(\\mathcal{S},\\mathcal{A},\\mathcal{U}^{\\sigma}(P^{\\theta}),r,\\gamma\\right)\\mid\\theta\\in\\Theta=\\left\\{(i,j):(i,j)\\in\\mathcal{S}\\times\\mathcal{A}\\backslash\\left\\{0\\right\\}\\right\\}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "We introduce the detailed definition of $\\mathcal{M}_{\\ell_{\\infty}}$ by introducing several key components of it sequentially. In particular, for any RMDP $\\mathcal{M}_{\\theta}\\in\\mathcal{M}_{\\ell_{\\infty}}$ , the state space is of size $2S$ , which includes two classes ", "page_idx": 49}, {"type": "text", "text": "of states $\\mathcal{X}=\\left\\{x_{0},x_{1},\\cdot\\cdot\\cdot,x_{S-1}\\right\\}$ and $\\mathcal{Y}=\\left\\{y_{0},y_{1},\\cdot\\cdot\\cdot,y_{S-1}\\right\\}$ . The action space for each state is $\\boldsymbol{\\mathcal{A}}$ of $A$ possible actions. So we have totally $2S$ states and there is in total $2S A$ state-action pairs. ", "page_idx": 50}, {"type": "text", "text": "Armed with the above definitions, we can first introduce the following nominal transition kernel: for all $(s,a)\\in\\mathcal{X}\\cup\\mathcal{Y}\\times\\mathcal{A}$ ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r}{P^{(0,0)}(s^{\\prime}\\,|\\,s,a)=\\left\\{\\begin{array}{l l}{p\\mathbb{1}(s^{\\prime}=y_{i})+(1-p)\\mathbb{1}(s^{\\prime}=x_{i})}&{\\mathrm{if}\\quad s=x_{i},a=0,\\quad\\forall i\\in\\mathcal{S}}\\\\ {q\\mathbb{1}(s^{\\prime}=y_{i})+(1-q)\\mathbb{1}(s^{\\prime}=x_{i})}&{\\mathrm{if}\\quad s=x_{i},a\\neq0,\\quad\\forall i\\in\\mathcal{S}}\\\\ {\\mathbb{1}(s^{\\prime}=s)}&{\\mathrm{if}\\quad s\\in\\mathcal{S}}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Here, $p$ and $q$ are set according to ", "page_idx": 50}, {"type": "equation", "text": "$$\n0\\leq p\\leq1\\quad{\\mathrm{~and~}}\\quad0\\leq q=p-\\Delta\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "for some $p$ and $\\Delta>0$ that will be introduced momentarily. ", "page_idx": 50}, {"type": "text", "text": "Then we introduce the $S(A-1)$ components inside $\\mathcal{M}_{\\infty}$ . Namely, for any $(i,j)\\in\\mathcal{S}\\times\\mathcal{A}\\setminus\\{0\\}$ , the nominal transition kernel of $\\mathcal{M}_{(i,j)}$ is specified as ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r}{P^{(i,j)}(s^{\\prime}\\,|\\,s,a)=\\left\\{\\begin{array}{l l}{p\\mathbb{I}\\left(s^{\\prime}=y_{i}\\right)+(1-p)\\mathbb{I}(s^{\\prime}=x_{i})}&{\\mathrm{if~}s=x_{i},a=j}\\\\ {q\\mathbb{I}\\left(s^{\\prime}=y_{i}\\right)+(1-q)\\mathbb{I}(s^{\\prime}=x_{i})}&{\\mathrm{if~}s=x_{i}\\in\\mathcal{X},a=0}\\\\ {P^{(0,0)}(s^{\\prime}\\,|\\,s,a)}&{\\mathrm{otherwise}}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "In words, the nominal transition kernel of each variant $\\mathcal{M}_{(i,j)}$ only differs slightly from that of the basic nominal transition kernel $P^{(0,0)}$ when $s=x_{i}$ and $a=\\{0,j\\}$ , which makes all the components inside $\\mathcal{M}_{\\ell_{\\infty}}$ closed to each other. ", "page_idx": 50}, {"type": "text", "text": "In addition, the reward function is defined as ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\forall a\\in{\\cal A}:\\quad r(s,a)=\\left\\{\\begin{array}{l l}{{1}}&{{\\mathrm{if}\\;s\\in\\mathcal{y}}}\\\\ {{0}}&{{\\mathrm{otherwise.}}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Uncertainty set of the transition kernels.  Recall the following useful notation for any transition probability $P$ , i.e., the transition vector associated with some state $s$ is denoted as: ", "page_idx": 50}, {"type": "equation", "text": "$$\nP_{s}:=P(\\cdot,\\cdot\\,|\\,s)\\in\\mathbb{R}^{1\\times S A},\\quad P_{s}^{0}:=P^{0}(\\cdot,\\cdot\\,|\\,s)\\in\\mathbb{R}^{1\\times S A}.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "With this in hand, the uncertainty set (definition in (5)) with $\\ell_{\\infty}$ norm for any $P^{\\theta}$ with $\\theta\\in\\Theta$ canbe represented as: ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\mathcal{U}_{\\infty}^{{\\mathfrak{s}},\\widetilde{\\sigma}}(P_{s}^{\\theta}):=\\mathcal{U}_{\\parallel.\\parallel}^{{\\mathfrak{s}},\\widetilde{\\sigma}}(P_{s}^{\\theta})=\\Big\\{P_{s}^{\\prime}\\in\\Delta(S)^{A}:\\left\\lVert P_{s}^{\\prime}-P_{s}^{\\theta}\\right\\rVert\\le\\widetilde{\\sigma}=\\sigma\\left\\lVert1\\right\\rVert_{\\infty}=\\sigma\\Big\\}.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "So without loss of generality, we set the radius $\\sigma\\in(0,(1-c_{0})]$ with $0<c_{0}<1$ .Before proceeding, we observe that as the uncertainty set above is defined with respect to $\\ell_{\\infty}$ , it directly implies that for each $(s,a)\\in S\\times A$ , the uncertainty set is independent and can be decomposed as ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\mathcal{U}_{\\infty}^{s,\\tilde{\\sigma}}(P_{s}^{\\theta})=\\otimes\\mathcal{U}_{||\\cdot||}^{s,\\tilde{\\sigma}}(P_{s,a}^{\\theta})=\\Big\\{P_{s,a}^{\\prime}\\in\\Delta(S):\\|P_{s,a}^{\\prime}-P_{s,a}^{\\theta}\\|\\leq\\sigma\\Big\\}.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Notably, this indicates that using $s$ -rectangular uncertainty set with $\\ell_{\\infty}$ norm as the divergence function is analogous to the case of using $(s,a)$ -rectangular uncertainty set with $\\ell_{\\infty}$ norm. As a result, we follow the pipeline of the prior art Shi et al. [2023, Section C] which established the minimax-optimal lower bound for $(s,a)$ -rectangular RMDPs with TV distance, which is analogous to the $\\ell_{\\infty}$ case. Towards this, we set $p,q,\\Delta$ as the same as the ones in Shi et al. [2023, Section C.1], where we recall the expressions of $p,q,\\Delta$ for self-contained as below: taking $c_{1}:=\\frac{c_{0}}{2}$ \uff0c ", "page_idx": 50}, {"type": "equation", "text": "$$\np=(1+c_{1})\\operatorname*{max}\\{1-\\gamma,\\sigma\\}\\qquad{\\mathrm{and}}\\qquad\\Delta\\leq c_{1}\\operatorname*{max}\\{1-\\gamma,\\sigma\\},\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "which ensure several facts: ", "page_idx": 50}, {"type": "equation", "text": "$$\n0\\leq p\\leq1\\quad\\mathrm{and}\\quad p\\geq q\\geq\\operatorname*{max}\\{1-\\gamma,\\sigma\\}.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Value functions and optimal policies. For each RMDP instance $\\mathcal{M}_{\\theta}\\in\\mathcal{M}_{\\ell_{\\infty}}$ , with some abuse of notation, we denote $\\pi_{\\theta}^{\\star}$ as the optimal policy. In addition, let $V_{\\theta}^{\\pi,\\sigma}$ (resp. $\\bar{V}_{\\theta}^{\\star,\\sigma}.$ )represent the corresponding robust value function of any policy $\\pi$ (resp. $\\pi_{\\theta}^{\\star}$ )with uncertaintylevel $\\sigma$ . Armed with these notations, the following lemma shows some essential properties concerning the value functions and optimal policies; the proof is postponed to Appendix F.3. ", "page_idx": 51}, {"type": "text", "text": "Lemma 12. Consider any $\\mathcal{M}_{\\theta}\\in\\mathcal{M}_{\\ell_{\\infty}}$ and any policy $\\pi$ , one has ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\forall(i,j)\\in\\Theta:\\quad V_{(i,j)}^{\\pi,\\sigma}(x_{i})\\leq\\frac{\\gamma\\big(z_{(i,j)}^{\\pi}-\\sigma\\big)}{(1-\\gamma)\\bigg(1+\\frac{\\gamma\\big(z_{(i,j)}^{\\pi}-\\sigma\\big)}{1-\\gamma(1-\\sigma)}\\bigg)\\left(1-\\gamma\\left(1-\\sigma\\right)\\right)},\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "where $z_{(i,j)}^{\\pi}$ is defined as ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\forall(i,j)\\in\\Theta:\\quad z_{(i,j)}^{\\pi}:=p\\pi(j\\,|\\,x_{i})+q\\left[1-\\pi(j\\,|\\,x_{i})\\right].\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "In addition, the robust optimal value functions and the robust optimal policies satisfy ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\forall(i,j)\\in\\Theta,s\\in\\mathcal{X}:\\quad V_{(i,j)}^{\\star,\\sigma}(s)=\\frac{\\gamma\\left(p-\\sigma\\right)}{(1-\\gamma)\\left(1+\\frac{\\gamma\\left(p-\\sigma\\right)}{1-\\gamma\\left(1-\\sigma\\right)}\\right)\\left(1-\\gamma\\left(1-\\sigma\\right)\\right)}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "and ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\pi_{(i,j)}^{\\star}(j\\,|\\,x_{i})=1\\qquad a n d\\qquad\\pi_{(i,j)}^{\\star}(0\\,|\\,s)=1\\quad\\forall s\\in\\mathcal{X}\\setminus\\{x_{i}\\}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "In words, this lemma shows that for any RMDP $\\mathcal{M}_{(i,j)}$ , the optimal policy on state $x_{i}$ satisfies $\\pi_{(i,j)}^{\\star}(j\\mid x_{i})=1$ and will focus on $a=0$ for all other states $s\\in\\mathcal X\\setminus\\{x_{i}\\}$ ", "page_idx": 51}, {"type": "text", "text": "F.2  Establishing the minimax lower bound ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Step 1: converting the goal to estimate $(i,j)$ . Now we are in position to derive the lower bound. Recall the goal is to control the following quantity associated with any policy estimator $\\widehat{\\pi}$ based on the dataset with in total $N_{\\mathsf{a l l}}$ samples: ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{(i,j)\\in\\Theta}\\mathbb{P}_{(i,j)}\\left\\{\\operatorname*{max}_{s\\in\\mathcal{X}\\cup\\mathcal{Y}}\\left(V_{(i,j)}^{\\star,\\sigma}(s)-V_{(i,j)}^{\\widehat{\\pi},\\sigma}(s)\\right)\\right\\}\\geq\\operatorname*{max}_{(i,j)\\in\\Theta}\\mathbb{P}_{(i,j)}\\left\\{\\operatorname*{max}_{s\\in\\mathcal{X}}\\left(V_{(i,j)}^{\\star,\\sigma}(s)-V_{(i,j)}^{\\widehat{\\pi},\\sigma}(s)\\right)\\right\\}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "To do so, we can invoke a key claim in Shi et al. [2023] here since our problem setting can be reduced to the same onein Siet al,[2023: With $\\begin{array}{r}{\\varepsilon\\leq\\frac{c_{1}}{32(1-\\gamma)}}\\end{array}$ ,Ieting ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\Delta=32(1-\\gamma)\\operatorname*{max}\\{1-\\gamma,\\sigma\\}\\varepsilon\\leq c_{1}\\operatorname*{max}\\{1-\\gamma,\\sigma\\}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "which satisfies (231), it leads to that for any policy $\\widehat{\\pi}$ and all $(i,j)\\in\\Theta$ ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{V_{(i,j)}^{\\star,\\sigma}(x_{i})-V_{(i,j)}^{\\widehat\\pi,\\sigma}(x_{i})\\geq2\\varepsilon\\big(1-\\widehat\\pi(j\\,|\\,x_{i})\\big),}\\\\ &{}&{\\forall s\\in\\mathcal{X}\\setminus\\{x_{i}\\}:\\quad V_{(i,j)}^{\\star,\\sigma}(s)-V_{(i,j)}^{\\widehat\\pi,\\sigma}(s)\\geq2\\varepsilon\\big(1-\\widehat\\pi(0\\,|\\,s)\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Before continuing, we introduce a useful notation for the subset of $\\Theta$ excluding the cases with state $i$ is selected: ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\forall i\\in{\\cal S}:\\quad\\Theta_{-i}=\\Theta\\setminus\\{(i^{\\prime},j):i^{\\prime}=i,j\\in{\\cal A}\\setminus\\{0\\}\\}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Armed with the above facts and notations, we first suppose there exists a policy $\\widehat{\\pi}$ such that for some $(i,j)\\in\\Theta$ ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\mathbb{P}_{(i,j)}\\left\\{V_{(i,j)}^{\\star,\\sigma}(x_{i})-V_{(i,j)}^{\\widehat{\\pi},\\sigma}(x_{i})\\leq\\varepsilon\\right\\}\\geq\\frac{3}{4}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "which in view of (239) indicates that we necessarily have $\\textstyle{\\widehat{\\pi}}(j\\,|\\,x_{i})\\geq{\\frac{1}{A}}$ with probability at least $\\frac{3}{4}$ ", "page_idx": 51}, {"type": "text", "text": "As a result, taking ", "page_idx": 52}, {"type": "equation", "text": "$$\nj^{\\prime}=\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}\\,\\widehat{\\pi}(a\\,|\\,x_{i}),\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "we are motivated to construct the following estimate of $\\theta$ ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\widehat{\\theta}\\left\\{\\underset{\\in\\;\\mathcal{G}_{-w}}{=(i,j^{\\prime})}\\quad\\quad\\mathrm{~if~}\\quad j^{\\prime}>0\\right.\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "which satisfies ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\mathbb{P}_{(i,j)}\\left\\{\\widehat{\\theta}=(i,j)\\right\\}\\geq\\mathbb{P}_{(i,j)}\\left\\{j^{\\prime}=j\\right\\}\\geq\\mathbb{P}_{(i,j)}\\left\\{\\widehat{\\pi}(j\\,|\\,x_{i})>\\frac{1}{A}\\right\\}\\geq\\frac{3}{4}.\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Step 2: developing the probability of error in testing multiple hypotheses. Before proceeding, we discuss the dataset consisting of in total $N_{\\mathsf{a l l}}$ independent samples. Observing that each RMDP inside the set $\\boldsymbol{\\mathcal{M}}_{\\ell_{\\infty}}$ are constructed symmetrically associated with one pair of states $(x_{i},y_{i})$ for all $i\\in S$ and another action $j\\,\\in\\,{\\mathcal{A}}\\times\\{0\\}$ , respectively. Therefore, it is obvious that the dataset is supposed to be generated uniformly on each $\\bar{(x_{i},y_{i},j)}$ to maximize the information gain, leading to s(An) samplesforany states-action (r, u,) with i E S, i  A\\0}. ", "page_idx": 52}, {"type": "text", "text": "Then we are ready to turn to the hypothesis testing problem over $(i,j)\\,\\in\\,\\Theta$ . Towards this, we consider the minimax probability of error defined as follows: ", "page_idx": 52}, {"type": "equation", "text": "$$\np_{\\mathrm{e}}:=\\operatorname*{inf}_{\\phi\\ {\\operatorname*{max}_{(i,j)\\in\\Theta}}\\ }\\big\\{\\mathbb{P}_{(i,j)}\\big(\\phi\\neq(i,j)\\big)\\big\\},\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "where the infimum is taken over all possible tests $\\phi$ constructed from the dataset introduced above. ", "page_idx": 52}, {"type": "text", "text": "To continue, armed with the above dataset with $N_{\\mathsf{a l l}}$ independent samples, we denote $\\mu^{i,j}$ (resp. $\\mu^{i,j}(s,a))$ as the distribution vector (resp. distribution) of each sample tuple $(s,a,s^{\\prime})$ under the nominal transition kernel $P^{(i,j)}$ associated with $\\mathcal{M}_{(i,j)}$ With this in mind, combined with Fano's inequality from Tsybakov [2009, Theorem 2.2] and the additivity of the KL divergence (cf. Tsybakov [2009, Page 85]), we obtain ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{max}_{p_{\\mathrm{e}}{\\mathrm{~}}\\ge1-N_{\\mathrm{all}}\\frac{(i,j),(i^{\\prime},j^{\\prime})\\in\\Theta,(i,j)\\ne(i^{\\prime},j^{\\prime})}{\\mathrm{tor}{\\mathrm{}}\\o{|\\Theta|}}}\\mathsf{K L}\\big(\\mu^{i,j}\\,|\\,\\mu^{i^{\\prime},j^{\\prime}}\\big)+\\log2}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\log|\\Theta|}\\\\ &{\\quad\\overset{\\mathrm{(i)}}{\\ge}1-N_{\\mathrm{all}}_{(i,j),(i^{\\prime},j^{\\prime})\\in\\Theta,(i,j)\\ne(i^{\\prime},j^{\\prime})}\\mathsf{K L}\\big(\\mu^{i,j}\\,|\\,\\mu^{i^{\\prime},j^{\\prime}}\\big)-\\frac{1}{2}}\\\\ &{\\quad=\\frac{1}{2}-N_{\\mathrm{all}}_{(i,j),(i^{\\prime},j^{\\prime})\\in\\Theta,(i,j)\\ne(i^{\\prime},j^{\\prime})}\\mathsf{K L}\\big(\\mu^{i,j}\\,|\\,\\mu^{i^{\\prime},j^{\\prime}}\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "where (i) holds by $\\log|\\Theta|\\geq2\\log2$ as long as $S(A-1)$ are large enough. Then following the same proof pipeline of Shi et al. [2023, Section C.2], we can arrive at ", "page_idx": 52}, {"type": "equation", "text": "$$\np_{\\mathrm{e}}\\geq\\frac{1}{2}-\\frac{N_{\\mathsf{a}\\parallel}}{S(A-1)}\\frac{4096}{c_{1}}(1-\\gamma)^{2}\\operatorname*{max}\\{1-\\gamma,\\sigma\\}\\varepsilon^{2}\\geq\\frac{1}{4},\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "if the sample size is selected as ", "page_idx": 52}, {"type": "equation", "text": "$$\nN_{\\sf a l l}\\le\\frac{c_{1}S(A-1)}{16396(1-\\gamma)^{2}\\operatorname*{max}\\{1-\\gamma,\\sigma\\}\\varepsilon^{2}}.\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Step 3: summing up the results together. Finally, we suppose that there exists an estimator $\\widehat{\\pi}$ suchthat ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{(i,j)\\in\\Theta}\\mathbb{P}_{(i,j)}\\left[\\operatorname*{max}_{s\\in\\mathcal{X}\\cup\\mathcal{Y}}\\left(V_{(i,j)}^{\\star,\\sigma}(s)-V_{(i,j)}^{\\widehat{\\pi},\\sigma}(s)\\right)\\geq\\varepsilon\\right]<\\frac{1}{4},\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "then according to (237), we necessarily have ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\forall s\\in\\mathcal{X}:\\quad\\operatorname*{max}_{(i,j)\\in\\Theta}\\mathbb{P}_{(i,j)}\\left[V_{(i,j)}^{\\star,\\sigma}(s)-V_{(i,j)}^{\\widehat{\\pi},\\sigma}(s)\\geq\\varepsilon\\right]<\\frac{1}{4},\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "which indicates ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\forall s\\in\\mathcal{X}:\\quad\\operatorname*{max}_{(i,j)\\in\\Theta}\\mathbb{P}_{(i,j)}\\left[V_{(i,j)}^{\\star,\\sigma}(s)-V_{(i,j)}^{\\widehat{\\pi},\\sigma}(s)<\\varepsilon\\right]\\geq\\frac{3}{4}.\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "As a consequence, (244) shows we must have ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\forall(i,j)\\in\\Theta:\\quad\\mathbb{P}_{(i,j)}\\left[\\widehat{\\theta}=(i,j)\\right]\\geq\\frac{3}{4}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "to achieve (249). However, this would contract with (247) if the sample size condition in (248) is satisfied. Thus, we complete the proof. ", "page_idx": 53}, {"type": "text", "text": "F.3Proof of Lemma 12 ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Without loss of generality, we first consider any $\\mathcal{M}_{(i,j)}$ With $(i,j)\\in\\mathcal{S}\\times\\mathcal{A}\\setminus\\{0\\}$ Following the same routine of Shi et al. [2023, Section C.3.1], we can verify that the order of the robust value function $V_{(i,j)}^{\\pi,\\sigma}$ overdiferentstates satisies ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\forall k\\in\\mathcal{S}:\\quad V_{(i,j)}^{\\pi,\\sigma}(x_{k})\\leq V_{(i,j)}^{\\pi,\\sigma}(y_{k}),\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "which means the robust value function of the states inside $\\mathcal{X}$ are always not larger than the corresponding states inside $\\boldsymbol{\\wp}$ ", "page_idx": 53}, {"type": "text", "text": "Then we denote the minimum of the robust value function over states as below: ", "page_idx": 53}, {"type": "equation", "text": "$$\nV_{(i,j),\\mathrm{min}}^{\\pi,\\sigma}:=\\operatorname*{min}_{s\\in\\mathcal{S}}V_{(i,j)}^{\\pi,\\sigma}(s).\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "In the following arguments, we first take a moment to assume Vt:),min = Vrt (a). With this in mind, we arrive at ", "page_idx": 53}, {"type": "equation", "text": "$$\nV_{(i,j)}^{\\pi,\\sigma}(y_{i})=1+\\gamma\\left(1-\\sigma\\right)V_{(i,j)}^{\\pi,\\sigma}(y_{i})+\\gamma\\sigma V_{(i,j),\\mathrm{min}}^{\\pi,\\sigma}=\\frac{1+\\gamma\\sigma V_{(i,j)}^{\\pi,\\sigma}(x_{i})}{1-\\gamma\\left(1-\\sigma\\right)}.\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Then, when we move on to the characterization of the robust value function at state $x_{i}$ .To doso,we notice two important facts: ", "page_idx": 53}, {"type": "text", "text": "1) The nominal transition probability $P_{x_{i},a}^{(i,j)}$ at state-action pair $(x_{i},a)$ for any $a\\in{\\mathcal{A}}$ is a Bernoulli distribution (see (226) and (224). The TV distance and the $\\ell_{\\infty}$ norm between two Bernoulli distribution are the same. ", "page_idx": 53}, {"type": "text", "text": "2) Invoking the definitions of the nominal transition probability in (226) and (224), we have ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P_{x_{i},j}^{(i,j)}=p\\mathbb{1}(s^{\\prime}=y_{i})+(1-p)\\mathbb{1}(s^{\\prime}=x_{i})}\\\\ &{P_{x_{i},a}^{(i,j)}=q\\mathbb{1}(s^{\\prime}=y_{i})+(1-q)\\mathbb{1}(s^{\\prime}=x_{i})\\quad\\forall a\\in\\mathcal{A}\\backslash\\{j\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "With the above two facts in hand, our problem setting is reduced to the same one in Shi et al. [2023] and can reuse the results in Shi et al. [2023, Section C.3.1] to achieve ", "page_idx": 53}, {"type": "equation", "text": "$$\nV_{(i,j)}^{\\pi,\\sigma}(x_{i})\\le\\frac{\\frac{\\gamma\\left(z_{(i,j)}^{\\pi}-\\sigma\\right)}{1-\\gamma\\left(1-\\sigma\\right)}}{(1-\\gamma)\\left(1+\\frac{\\gamma\\left(z_{(i,j)}^{\\pi}-\\sigma\\right)}{1-\\gamma(1-\\sigma)}\\right)}.\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "and ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pi_{(i,j)}^{\\star}(j\\,|\\,x_{i})=1}\\\\ &{V_{(i,j)}^{\\star,\\sigma}(x_{i})=\\frac{\\frac{\\gamma\\big(z_{(i,j)}^{\\pi^{\\star}}-\\sigma\\big)}{1-\\gamma(1-\\sigma)}}{\\big(1-\\gamma\\big)\\,\\left(1+\\frac{\\gamma\\big(z_{(i,j)}^{\\pi^{\\star}}-\\sigma\\big)}{1-\\gamma(1-\\sigma)}\\right)}=\\frac{\\frac{\\gamma(p-\\sigma)}{1-\\gamma(1-\\sigma)}}{\\big(1-\\gamma\\big)\\,\\left(1+\\frac{\\gamma(p-\\sigma)}{1-\\gamma(1-\\sigma)}\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Analogously, we can verify that for other $x_{k}\\in\\mathcal{X}\\setminus\\{x_{i}\\}$ \uff0c ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\pi_{(i,j)}^{\\star}(0\\mid x_{k})=1}}\\\\ &{}&{V_{(i,j)}^{\\star,\\sigma}(x_{k})=\\frac{\\frac{\\gamma(p-\\sigma)}{1-\\gamma(1-\\sigma)}}{(1-\\gamma)\\,\\left(1+\\frac{\\gamma(p-\\sigma)}{1-\\gamma(1-\\sigma)}\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "G  DRVI for $s a-$ rectangular algorithm for arbitrary norm ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "In order to compute the fixed point of ${\\widehat{\\mathcal{T}}}^{\\sigma}$ , distributionally robust value iteration (DRVI), is defined in Algorithm 1. For $s a$ -rectangularity, starting from an initialization $\\widehat{Q}_{0}=0$ , the update rule at the $t$ -th $\\mathit{\\Omega}\\left(t\\geq1\\right)$ iteration is the following $\\forall(s,a)\\in{\\mathcal{S}}\\times{\\mathcal{A}}$ ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\widehat{Q}_{t}^{\\pi}(s,a)=\\widehat{T}^{\\sigma}\\widehat{Q}_{t-1}^{\\pi}(s,a)=r(s,a)+\\gamma\\operatorname*{inf}_{\\mathcal{P}\\in\\mathcal{U}_{\\|\\cdot\\|}^{s_{0,\\sigma}}(\\widehat{P}_{s,a}^{0})}\\mathcal{P}\\widehat{V}_{t-1},\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "where $\\widehat V_{t-1}(s)=\\operatorname*{max}_{\\pi}\\widehat Q_{t-1}^{\\pi}(s,a)$ for all $s\\in S$ ", "page_idx": 54}, {"type": "text", "text": "Directly solving (260) is computationally expensive since it involves optimization over a $S$ dimensional probability simplex at each iteration, especially when the dimension of the state space $\\boldsymbol{S}$ is large. Fortunately, given strong duality (260) can be equivalently solved using its dual problem, which concerns optimizing a two variable ( $\\lambda$ and $\\omega$ ) and thus can be solved efficiently. The specific form of the dual problem depends on the choice of the norm $\\left\\Vert.\\right\\Vert$ , which we shall discuss separately in Appendix C.3. To complete the description, we output the greedy policy of the final Q-estimate $\\widehat{Q}_{T}$ as the final policy $\\widehat{\\pi}$ , namely, ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\forall s\\in\\mathcal{S}:\\quad\\widehat{\\pi}(s)=\\arg\\operatorname*{max}_{a}\\widehat{Q}_{T}(s,a).\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Encouragingly,the iterates $\\left\\{\\widehat{Q}_{t}\\right\\}_{t\\geq0}$ of $D R V I$ converge linearly to the fixed point $\\widehat{Q}^{\\star,\\sigma}$ , owing to the appealing $\\gamma$ -contraction property of ${\\widehat{\\mathcal{T}}}^{\\sigma}$ ", "page_idx": 54}, {"type": "text", "text": "input: empirical nominal transition kernel P ; reward function $r$ ; uncertainty level $\\sigma$ ; number of iterations $T$ ", "page_idx": 54}, {"type": "text", "text": "initialization: $\\widehat{Q}_{0}(s,a)=0$ $\\widehat{V}_{0}(s)=0$ for all $(s,a)\\in S\\times A$   \nfor $t=1,2,...,T$ do for $s\\in{\\mathcal{S}},a\\in{\\mathcal{A}}$ do 1  Set $\\widehat{Q}_{t}(s,a)$ according to (260); end for $s\\in S$ do Set $:\\widehat{V}_{t}(s)=\\operatorname*{max}_{a}\\widehat{Q}_{t}(s,a)$ end   \nend   \noutput: $\\widehat{Q}_{T}$ $\\widehat{V}_{T}$ and $\\widehat{\\pi}$ obeying $\\widehat\\pi(s):=\\arg\\operatorname*{max}_{a}\\widehat Q_{T}(s,a)$ \uff0c\u4e0a   \nAlgorithm 1: Distributionally robust value iteration $(D R V I)$ for infinite-horizon RMDPs for   \n$s a$ ", "page_idx": 54}, {"type": "text", "text": "-rectangular for arbitrary norm ", "page_idx": 54}, {"type": "text", "text": "Using Algorithm 1, it allows geting an $\\epsilon_{o p t}$ error in the empirical MDP in the $s a$ -rectangular case. In the $s$ -rectangular case, finding an algorithm to get $\\epsilon_{o p t}$ is more difficult to use, as the policy is not deterministic anymore and 1 cannot anymore be applied. For $L_{p}$ norms, Clavier et al. [2023] derived an algorithm but for arbitrary norm we need to consider a more general problem for arbitrary norm in Appendix G ", "page_idx": 54}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 55}, {"type": "text", "text": "Answer: [Yes] Justification: Yes ", "page_idx": 55}, {"type": "text", "text": "", "page_idx": 55}, {"type": "text", "text": "Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 55}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 55}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Justification: See conclusion. ", "page_idx": 55}, {"type": "text", "text": "Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 55}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 55}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 55}, {"type": "text", "text": "Justification: Assumtions are stated in lemmas and Theorems. ", "page_idx": 56}, {"type": "text", "text": "Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 56}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 56}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 56}, {"type": "text", "text": "Justification: Theoretical paper. ", "page_idx": 56}, {"type": "text", "text": "Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to acces this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 56}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 56}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 57}, {"type": "text", "text": "Justification: Theoretical paper. Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https : / /nips . cc / public/guides /CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (ht tps : / /nips.cc/public/guides /CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 57}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 57}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 57}, {"type": "text", "text": "Justification: Theoretical paper. Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental seting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 57}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 57}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 57}, {"type": "text", "text": "Justification: Theoretical paper. Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 57}, {"type": "text", "text": "", "page_idx": 58}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 58}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 58}, {"type": "text", "text": "Justification: Theoretical paper. Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 58}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: / /neurips.Cc/public/EthicsGuidelines? ", "page_idx": 58}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 58}, {"type": "text", "text": "Justification:Done Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 58}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 58}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 58}, {"type": "text", "text": "Justification:Theoretical paper. Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 58}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 59}, {"type": "text", "text": "\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 59}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 59}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 59}, {"type": "text", "text": "Justification: Theoretical paper. Guidelines: ", "page_idx": 59}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faitheffort. ", "page_idx": 59}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 59}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 59}, {"type": "text", "text": "Justification: Theoretical paper. Guidelines: ", "page_idx": 59}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/ dataset s has curated licenses for some datasets. Their licensing guide can help determine the license of adataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 59}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 59}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 60}, {"type": "text", "text": "Justification:Theoretical paper. Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 60}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 60}, {"type": "text", "text": "Answer: [NA] Justification: Theoretical paper. ", "page_idx": 60}, {"type": "text", "text": "Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 60}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 60}, {"type": "text", "text": "Answer: [NA] Justification: Theoretical paper. ", "page_idx": 60}, {"type": "text", "text": "Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 60}]