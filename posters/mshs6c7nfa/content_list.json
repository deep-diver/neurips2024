[{"type": "text", "text": "Improving the Training of Rectified Flows ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sangyun Lee Zinan Lin Giulia Fanti Carnegie Mellon University Microsoft Research Carnegie Mellon University sangyunl@andrew.cmu.edu zinanlin@microsoft.com gfanti@andrew.cmu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Diffusion models have shown great promise for image and video generation, but sampling from state-of-the-art models requires expensive numerical integration of a generative ODE. One approach for tackling this problem is rectified flows, which iteratively learn smooth ODE paths that are less susceptible to truncation error. However, rectified flows still require a relatively large number of function evaluations (NFEs). In this work, we propose improved techniques for training rectified flows, allowing them to compete with knowledge distillation methods even in the low NFE setting. Our main insight is that under realistic settings, a single iteration of the Reflow algorithm for training rectified flows is sufficient to learn nearly straight trajectories; hence, the current practice of using multiple Reflow iterations is unnecessary. We thus propose techniques to improve one-round training of rectified flows, including a U-shaped timestep distribution and LPIPSHuber premetric. With these techniques, we improve the FID of the previous 2-rectified flow by up to $75\\%$ in the 1 NFE setting on CIFAR-10. On ImageNet $64\\!\\times\\!64$ , our improved rectified flow outperforms the state-of-the-art distillation methods such as consistency distillation and progressive distillation in both one-step and two-step settings and rivals the performance of improved consistency training (iCT) in FID. Code is available at https://github.com/sangyun884/rfpp. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Diffusion models [Sohl-Dickstein et al., 2015, Ho et al., 2020, Song and Ermon, 2019, Song et al., 2020b] have shown great promise in image [Ramesh et al., 2022] and video [Ho et al., 2022] generation. They generate data by simulating a stochastic denoising process where noise is gradually transformed into data. To sample efficiently from diffusion models, the denoising process is typically converted into a counterpart of Ordinary Differential Equations (ODEs) [Song et al., 2020b] called probability flow ODEs (PF-ODEs). ", "page_idx": 0}, {"type": "text", "text": "Despite the success of diffusion models using PF-ODEs, drawing high-quality samples requires numerical integration of the PF-ODE with small step sizes, which is computationally expensive. Today, two prominent classes of approaches for tackling this issue are: (1) knowledge distillation (e.g., consistency distillation [Song et al., 2023], progressive distillation [Salimans and Ho, 2022]) and (2) simulation-free flow models (e.g., rectified flows [Liu et al., 2022], flow matching [Lipman et al., 2022]). ", "page_idx": 0}, {"type": "text", "text": "In knowledge distllation-based methods [Luhman and Luhman, 2021, Salimans and Ho, 2022, Song et al., 2023, Zheng et al., 2022] a student model is trained to directly predict the solution of the PF-ODE. These models are currently state-of-the-art in the low number of function evaluations (NFEs) regime (e.g. 1-4). ", "page_idx": 0}, {"type": "text", "text": "Another promising direction is simulation-free flow models such as rectified flows [Liu et al., 2022, Liu, 2022], a generative model that learns a transport map between two distributions defined via neural ODEs. Diffusion models with PF-ODEs are a special case. Rectified flows can learn smooth ", "page_idx": 0}, {"type": "text", "text": "ODE trajectories that are less susceptible to truncation error, which allows for high-quality samples with fewer NFEs than diffusion models. They have been shown to outperform diffusion models in the moderate to high NFE regime [Lipman et al., 2022, Liu et al., 2022, Esser et al., 2024], but they still require a relatively large number of NFEs compared to distillation methods. ", "page_idx": 1}, {"type": "text", "text": "Compared to knowledge distillation methods [Luhman and Luhman, 2021, Salimans and Ho, 2022, Song et al., 2023, Zheng et al., 2022] rectified flows have several advantages. First, they can be generalized to map two arbitrary distributions to one another, while distillation methods are limited to a Gaussian noise distribution. Also, as a neural ODE, rectified flows naturally support inversion from data to noise, which has many applications including image editing [Hertz et al., 2022, Kim et al., 2022, Wallace et al., 2023, Couairon et al., 2022, Mokady et al., 2023, Su et al., 2022, Hong et al., 2023] and watermarking [Wen et al., 2023]. Further, the likelihood of rectified flow models can be evaluated using the instantaneous change of variable formula [Chen et al., 2018], whereas this is not possible with knowledge distillation-based methods. In addition, rectified flows can flexibly adjust the balance between the sample quality and computational cost by altering NFEs, whereas distillation methods either do not support multi-step sampling or do not necessarily perform better with more NFEs (e.g. $>4$ ) [Kim et al., 2023]. ", "page_idx": 1}, {"type": "text", "text": "Given the qualitative advantages of rectified flows, a natural question is, can rectified flows compete with distillation-based methods such as consistency models [Song et al., 2023] in the low NFE setting? Today, the state-of-the-art techniques for training rectified flows use the Reflow algorithm to improve low NFE performance [Liu et al., 2022, 2023]. Reflow is a recursive training algorithm where the rectified flow is trained on data-noise pairs generated by the generative ODE of the previous stage model. In current implementations of Reflow, to obtain a reasonable one-step generative performance, Reflow should be applied at least twice, followed by an optional distillation stage to further boost performance [Liu et al., 2022, 2023]. Each training stage requires generating a large number of data-noise pairs and training the model until convergence, which is computationally expensive and leads to error accumulation across rounds. Even with these efforts, the generative performance of rectified flow still lags behind the distillation methods such as consistency models [Song et al., 2023]. ", "page_idx": 1}, {"type": "text", "text": "We show that rectified flows can indeed be competitive with the distillation methods in the low NFE setting by applying Reflow with our proposed training techniques. Our techniques are based on the observation that under realistic settings, the linear interpolation trajectories of the pre-trained rectified flow rarely intersect with each other. This provides several insights: 1) applying Reflow once is sufficient to obtain straight-line generative ODE in the optima, 2) the training loss of 2-rectified flow has zero lower bound, and 3) other loss functions than the squared $\\ell_{2}$ distance can be used during training. Based upon this finding, we propose several training techniques to improve Reflow, including: (1) a U-shaped timestep distribution, (2) an LPIPS-huber premetric, which we find to be critical for the few-step generative performance. After being initialized with pre-trained diffusion models such as EDM [Karras et al., 2022], our method only requires one training stage without additional Reflow or distillation stages, unlike previous works [Liu et al., 2022, 2023]. ", "page_idx": 1}, {"type": "text", "text": "Our evaluation shows that on several datasets (CIFAR-10 [Krizhevsky et al., 2009], ImageNet $64\\!\\times\\!64$ [Deng et al., 2009]), our improved rectified flow outperforms the state-of-the-art distillation methods such as consistency distillation (CD) [Song et al., 2023] and progressive distillation (PD) [Salimans and Ho, 2022] in both one-step and two-step settings, and it rivals the performance of the improved consistency training (iCT) [Song et al., 2023] in terms of the Frechet Inception Distance [Heusel et al., 2017] (FID). Our training techniques reduce the FID of the previous 2-rectified flow [Liu et al., 2022] by about $\\mathbf{75\\%}$ $[12.21\\rightarrow3.07]$ ) on CIFAR-10. Ablations on three datasets show that the proposed techniques give a consistent and sizeable gain. We also showcase the qualitative advantages of rectified flow such as few-step inversion, and its application to interpolation and image-to-image translation. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Rectified Flow ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Rectified flow (see also flow matching [Lipman et al., 2022] and stochastic interpolant [Albergo and Vanden-Eijnden, 2022]) is a generative model that smoothly transitions between two distributions $p_{\\mathbf{x}}$ and $p_{\\mathbf{z}}$ by solving ODEs [Liu et al., 2022]. For $\\mathbf{x}\\sim p_{\\mathbf{x}}$ and $\\mathbf{z}\\sim p_{\\mathbf{z}}$ , we define the interpolation between $\\mathbf{x}$ and ${\\bf z}$ as ${\\bf x}_{t}=(1-t){\\bf x}+t{\\bf z}$ for $t\\in[0,1]$ . Liu et al. [2022] showed that for ${\\mathbf{z}}_{0}\\sim p_{\\mathbf{x}}$ , the following ODE yields the same marginal distribution as $\\mathbf{x}_{t}$ for any $t$ : ", "page_idx": 1}, {"type": "image", "img_path": "mSHs6C7Nfa/tmp/7b2c5f21f65ff54fb8dd9aa32eb914fd02c69a3377073879243c69e5f05cb707.jpg", "img_caption": ["Figure 1: Rectified flow process (figure modified from Liu et al. [2022]). Rectified flow rewires trajectories so there are no intersecting trajectories $(a)\\rightarrow(b)$ . Then, we take noise samples from $p_{\\mathbf{z}}$ and their generated samples from $p_{\\mathbf{x}}^{1}$ , and linearly interpolate them $(c)$ . In Reflow, rectified flow is applied again $(c)\\rightarrow(d)$ to straighten flows. This procedure is repeated recursively. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\frac{d\\mathbf{z}_{t}}{d t}=\\mathbf{v}_{t}(\\mathbf{z}_{t}):=\\frac{1}{t}\\big(\\mathbf{z}_{t}-\\mathbb{E}[\\mathbf{x}|\\mathbf{x}_{t}=\\mathbf{z}_{t}]\\big).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Since $\\mathbf{x}_{1}=\\mathbf{z}$ , Eq. (1) transports $p_{\\mathbf{x}}$ to $p_{\\mathbf{z}}$ . We can also transport $p_{\\mathbf{z}}$ to $p_{\\mathbf{x}}$ by drawing $\\mathbf{z}_{1}$ from $p_{\\mathbf{z}}$ and solving the ODE backwards from $t=1$ to $t=0$ . During training, we estimate the conditional expectation $\\mathbb{E}[{\\bf x}|{\\bf x}_{t}={\\bf z}_{t}]$ with a vector-valued neural network $\\mathbf{x}_{\\theta}$ trained on the squared $\\ell_{2}$ loss: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta}\\mathbb{E}_{{\\mathbf{x}},{\\mathbf{z}}\\sim p_{{\\mathbf{x}}{\\mathbf{z}}}}\\mathbb{E}_{t\\sim p_{t}}[\\omega(t)||{\\mathbf{x}}-{\\mathbf{x}}_{\\theta}({\\mathbf{x}}_{t},t)||_{2}^{2}],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $p_{\\mathbf{x}\\mathbf{z}}$ is the joint distribution of $\\mathbf{x}$ and $\\mathbf{z},\\,\\mathbf{x}\\theta$ is parameterized by $\\pmb{\\theta}$ , and $\\omega(t)$ is a weighting function. $p_{t}$ is chosen to be the uniform distribution on $[0,1]$ in Liu et al. [2022, 2023]. In the optimum of Eq. (2), $\\mathbf{x}_{\\theta}$ becomes the conditional expectation as it is a minimum mean squared error (MMSE) estimator, which is then plugged into the ODE (1) to generate samples. Instead of predicting the conditional expectation directly, Liu et al. [2022] choose to parameterize the velocity $\\mathbf{v}_{t}$ with a neural network $\\mathbf{v}_{\\theta}$ and train it on ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\underset{\\theta}{\\operatorname*{min}}\\,\\mathbb{E}_{{\\mathbf{x}},{\\mathbf{z}}\\sim p_{{\\mathbf{x}}{\\mathbf{z}}}}\\mathbb{E}_{t\\sim p_{t}}[||({\\mathbf{z}}-{\\mathbf{x}})-{\\mathbf{v}}_{\\theta}({\\mathbf{x}}_{t},t)||_{2}^{2}],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "which is equivalent to Eq. (2) with $\\begin{array}{r}{\\omega(t)=\\frac{1}{t^{2}}}\\end{array}$ . See Appendix. A. ", "page_idx": 2}, {"type": "text", "text": "In this paper, we consider the Gaussian marginal case, i.e., $p_{\\mathbf{z}}=\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ . In this case, if we define $\\mathbf{x}$ and $\\mathbf{z}$ as independent random variables (i.e., $p_{\\mathbf{x}\\mathbf{z}}(\\mathbf{x},\\mathbf{z})=p_{\\mathbf{x}}(\\mathbf{x})p_{\\mathbf{z}}(\\mathbf{z}))$ and use a specific nonlinear interpolation instead of the linear interpolation for $\\mathbf{x}_{t}$ , Eq. (2) becomes the weighted denoising objective of the diffusion model [Vincent, 2011], and Eq. (1) becomes the probability flow ODE (PF-ODE) [Song et al., 2020b]. ", "page_idx": 2}, {"type": "text", "text": "2.2 Reflow ", "text_level": 1, "page_idx": 2}, {"type": "table", "img_path": "mSHs6C7Nfa/tmp/64c5ab9ea659a70062ca03fef3dd3045d201dd7d661ce95012fb9afcb05ef929.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "The independent coupling $p_{\\mathbf{x}\\mathbf{z}}(\\mathbf{x},\\mathbf{z})\\,=\\,p_{\\mathbf{x}}(\\mathbf{x})p_{\\mathbf{z}}(\\mathbf{z})$ is known to lead to curved ODE trajectories, which require a large number of function evaluations (NFE) to generate high-quality samples [Pooladian et al., 2023, Lee et al., 2023]. Reflow [Liu et al., 2022] is a recursive training algorithm to find a better coupling that yields straighter ODE trajectories. Starting from the independent coupling $p_{\\mathbf{x}\\mathbf{z}}^{0}(\\mathbf{x},\\mathbf{z})\\overset{\\cdot}{=}p_{\\mathbf{x}}^{\\mathsf{\\Delta}}(\\mathbf{x})p_{\\mathbf{z}}\\mathbf{\\dot{(z)}}$ , the Reflow algorithm generates $p_{\\mathbf{x}\\mathbf{z}}^{k+1}(\\mathbf{\\bar{x}},\\mathbf{z})$ from $p_{\\mathbf{x}\\mathbf{z}}^{k}(\\mathbf{\\dot{x}},\\mathbf{z})$ by first generating synthetic $(\\mathbf{x},\\mathbf{z})$ pairs from $p_{\\mathbf{x}\\mathbf{z}}^{k}$ , then training rectified flow on the generated synthetic pairs (Figure $1(b)-(\\dot{d}))$ . We call the vector field resulting from the $k$ -th iteration of this procedure $k$ -rectified flow. Pseudocode for Reflow is provided in Algorithm. 1. ", "page_idx": 2}, {"type": "image", "img_path": "mSHs6C7Nfa/tmp/e8b0f0f7169684fcd586adfbffe58ab3b95d8ae5073d527c45572272d09b2206.jpg", "img_caption": ["Figure 2: An illustration of the intuition in Sec. 3. (a) If two linear interpolation trajectories intersect, $\\mathbf{z}^{\\prime\\prime}\\!-\\!\\mathbf{z}^{\\prime}$ is parallel to $\\mathbf{x}^{\\prime}\\!-\\!\\mathbf{x}^{\\prime\\prime}$ . This generally maps $\\mathbf{z}^{\\prime\\prime}$ to an atypical (e.g., one with high autocorrelation or a norm that is too large to be on a Gaussian annulus) realization of Gaussian noise, so the 1-rectified flow cannot reliably map $\\mathbf{z}^{\\prime\\prime}$ to $\\mathbf{x}^{\\prime\\prime}$ on $\\mathcal{M}_{\\bf x}$ . (b) Generated samples from the pre-trained 1-rectified flow starting from $\\mathbf{z}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})\\left($ right), which is the standard setting, and $\\mathbf{z}^{\\prime\\prime}=\\mathbf{z}+(\\mathbf{x}^{\\prime}-\\mathbf{x}^{\\prime\\prime})$ , where $\\mathbf{x}^{\\prime},\\mathbf{x}^{\\prime\\prime}$ are sampled from 1-rectified flow trained on CIFAR-10 (left). Qualitatively, we see that the left samples have very low quality. (c) Empirically, we show the $\\ell_{2}$ norm of $z^{\\prime\\prime}\\stackrel{}{=}\\mathbf{z}+(\\mathbf{x}^{\\prime}-\\mathbf{x}^{\\prime\\prime})$ compared to $z^{\\prime}$ , which is sampled from the standard Gaussian. $\\mathbf{z}^{\\prime\\prime}$ generally lands outside the annulus of typical Gaussian noise. (d) $\\mathbf{z}+(\\mathbf{x}^{\\prime}-\\mathbf{x}^{\\prime\\prime})$ has high autocorrelation while the autocorrelation of Gaussian noise is nearly zero in high-dimensional space. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Convergence: Liu et al. [2022] show that Reflow trajectories are straight in the limit as $K\\rightarrow\\infty$ . Hence, to achieve perfectly straight ODE paths that allow for accurate one-step generation, Reflow may need to be applied many times until equilibrium, with each training stage requiring many data-noise pairs, training the model until convergence, and a degradation in generated sample quality. Prior work has empirically found that Reflow should be applied at least twice (i.e. 3-rectified flow) for reasonably good one-step generative performance [Liu et al., 2022, 2023]. This has been a major downside for rectified flows compared to knowledge distillation methods, which typically require only one distillation stage [Luhman and Luhman, 2021, Song et al., 2023, Zheng et al., 2022]. ", "page_idx": 3}, {"type": "text", "text": "3 Applying Reflow Once is Sufficient ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we argue that under practical settings, the trajectory curvature of the optimal 2-rectified flow is actually close to zero. Hence, prior empirical results requiring more rounds of Reflow may be the result of suboptimal training techniques, and we should focus on improving those training techniques rather than stacking additional Reflow stages. ", "page_idx": 3}, {"type": "text", "text": "First, note that the curvature of the optimal 2-rectified flow is zero if and only if the linear interpolation trajectories of 1-rectified flow-generated pairs do not intersect, or equivalently, $\\mathbb{E}[\\mathbf{x}|\\mathbf{x}_{t}=(1-t)\\mathbf{x}^{\\prime}+$ $t\\mathbf{z}^{\\bar{\\prime}}]=\\mathbf{x}^{\\prime}$ for all pairs $(\\mathbf{x}^{\\prime},\\mathbf{z}^{\\prime})$ [Liu et al., 2022]. ", "page_idx": 3}, {"type": "text", "text": "To begin with, consider the manifold $\\mathcal{M}_{\\bf x}$ of the synthetic distribution $\\begin{array}{r}{p^{1}(\\mathbf{x})~=~\\int p^{1}(\\mathbf{x},\\mathbf{z})d\\mathbf{z}}\\end{array}$ . Consider two points $\\mathbf{x}^{\\prime}$ and $\\mathbf{x}^{\\prime\\prime}$ from the manifold, and two noises $\\mathbf{z}^{\\prime}$ and $\\mathbf{z}^{\\prime\\prime}$ that are mapped to $\\mathbf{x}^{\\prime}$ and $\\mathbf{x}^{\\prime\\prime}$ by 1-rectified flow. Here, we say two pairs $(\\mathbf{x}^{\\prime},\\mathbf{z}^{\\prime})$ and $(\\mathbf{x}^{\\prime\\prime},\\mathbf{z}^{\\prime\\prime})$ intersect if $\\exists t\\in[0,1]$ s.t. $(1-t)\\mathbf{x}^{\\prime}+t\\mathbf{z}^{\\prime}=(1-t)\\mathbf{x}^{\\prime\\prime}+t\\mathbf{z}^{\\prime\\prime}$ . For example, in Figure 2(a), we observe that the two trajectories intersect at an intermediate $t$ . ", "page_idx": 3}, {"type": "text", "text": "For an intersection to exist at $t$ it must hold that 1) 1-rectified flow maps $\\mathbf{z}^{\\prime\\prime}$ to $\\mathbf{x}^{\\prime\\prime}$ , and 2) $\\mathbf{z}^{\\prime\\prime}=$ $\\mathbf{z}^{\\prime}+\\frac{1-t}{t}(\\mathbf{x}^{\\prime}-\\mathbf{x}^{\\prime\\prime})$ by basic geometry. However, note that for realistic data distributions and if ", "page_idx": 3}, {"type": "text", "text": "1-rectified flow is sufficiently well-trained, $\\begin{array}{r}{{\\bf z}^{\\prime\\prime}={\\bf z}^{\\prime}+\\frac{1-t}{t}({\\bf x}^{\\prime}-{\\bf x}^{\\prime\\prime})}\\end{array}$ is not a common noise realization (e.g., it is likely to have nonzero autocorrelation or a norm that is too large to be on a Gaussian annulus), as shown visually in Figure 2(a). As 1-rectified flow is almost entirely trained on common Gaussian noise inputs, it cannot generally map an atypical $\\mathbf{z}^{\\prime\\prime}$ to $\\mathcal{M}_{\\bf x}$ . Figure 2(c) shows qualitatively that if we draw values of $\\mathbf{z}^{\\prime\\prime}$ by first drawing $\\bar{\\mathbf{z}^{\\prime}}\\sim\\mathcal{N}(\\bar{\\mathbf{0}},I)$ and then adding $(\\mathbf{x}^{\\prime}-\\mathbf{x}^{\\prime\\prime})$ for independent draws of $\\mathbf{x}^{\\prime},\\mathbf{x}^{\\prime\\prime}$ , the $\\mathbf{z}^{\\prime\\prime}$ vectors fall outside the annulus of typical standard Gaussian noise. Similarly, Figure 2(d) shows that the constructed noise vectors $\\mathbf{z}^{\\prime\\prime}$ have higher autocorrelation than expected. As a result, Figure 2(b) visually shows that the generated samples have little overlap with the expected samples from typical draws of $\\mathbf{z}^{\\prime}$ . ", "page_idx": 4}, {"type": "text", "text": "This suggests empirically that when training 2-rectified flow, intersections are rare (i.e. $\\mathbb{E}[\\mathbf{x}|\\mathbf{x}_{t}=$ $(1-t)\\bar{\\mathbf{x}^{\\prime}}\\!+t\\mathbf{z}^{\\prime}]\\approx\\mathbf{x}^{\\prime})$ , which in turn implies that the optimal 2-rectified flow trajectories are nearly straight. Hence, additional rounds of Reflow are unnecessary, while also degrading sample quality. This intuition allows us to focus on better training techniques for 2-rectified flow rather than training 3- or 4-rectified flow. It also leads us to several improved techniques, discussed in Sec. 4. ", "page_idx": 4}, {"type": "text", "text": "Edge cases: Note that if $||\\mathbf{x}^{\\prime}-\\mathbf{x}^{\\prime\\prime}||_{2}$ is small, 1-rectified flow could map $\\mathbf{z}^{\\prime\\prime}$ to some point on $\\mathcal{M}_{\\bf x}$ . However, it does not alter the conclusion because the average of $\\mathbf{x}^{\\prime}$ and $\\mathbf{x}^{\\prime\\prime}$ is close to $\\mathbf{x}^{\\prime}$ anyway, so $\\mathbb{E}[{\\bf x}|{\\bf x}_{t}=(1-t){\\bf x}^{\\prime}+t{\\bf z}^{\\prime}]\\approx{\\bf x}^{\\prime}$ . Similarly, if $t$ is close to 1, $\\frac{1-t}{t}(\\mathbf{x}^{\\prime}-\\mathbf{x}^{\\prime\\prime})\\approx\\mathbf{0}$ , so 1-rectified flow can map $\\mathbf{z}^{\\prime\\prime}$ to $\\mathcal{M}_{\\bf x}$ . If the 1-rectified flow is $L$ -Lipschitz, $||\\mathbf{x}^{\\prime}-\\mathbf{x}^{\\prime\\prime}||_{2}\\leq L||\\mathbf{z}^{\\prime}-\\mathbf{z}^{\\prime\\prime}||_{2}$ . Therefore, the expectation $\\mathbb{E}[\\mathbf{x}|\\mathbf{x}_{t}]$ again will not deviate much from $\\mathbf{x}^{\\prime}$ . ", "page_idx": 4}, {"type": "text", "text": "4 Improved Training Techniques for Reflow ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The observation in Sec. 3 suggests that the optimal 2-rectified flow is nearly straight. Therefore, if the one-step generative performance of the 2-rectified flow model is not as good as expected, it is likely due to suboptimal training. In this section, we show that the few-step generative performance of the 2-rectified flow can be significantly improved by applying several new training techniques. ", "page_idx": 4}, {"type": "text", "text": "4.1 Timestep distribution ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "As in diffusion models, rectified flows are trained on randomly sampled timesteps $t$ , and the distribution from which $t$ is sampled is an important design choice. Ideally, we want to focus the training effort on timesteps that are more challenging rather than wasting computational resources on easy tasks. One common approach is to focus on the tasks where the training loss is high [Shrivastava et al., 2016]. However, the training error of rectified flows is not a reliable measure of difficulty because different timesteps have different non-zero lower bounds. To understand this, let us decompose the training error into two terms: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\pmb{\\theta},t):=\\frac{1}{t^{2}}\\mathbb{E}[||\\mathbf{x}-\\mathbf{x}_{\\pmb{\\theta}}(\\mathbf{x}_{t},t)||_{2}^{2}]=\\frac{1}{t^{2}}\\mathbb{E}[||\\mathbf{x}-\\mathbb{E}[\\mathbf{x}|\\mathbf{x}_{t}]||_{2}^{2}]+\\bar{\\mathcal{L}}(\\pmb{\\theta},t).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The first term does not depend on $\\pmb{\\theta}$ and thus cannot be reduced. The second term represents the actual minimizable training error, but its value cannot be directly observed because the first term is usually unknown. Fortunately, because of the finding in Sec. 3, we expect that the first term is nearly zero when training 2-rectified flow, so we can use $\\dot{\\boldsymbol{\\mathcal{L}}}(\\pmb{\\theta},t)$ for designing the timestep distribution. ", "page_idx": 4}, {"type": "text", "text": "Figure 3 shows that the training loss of 2-rectified flow is large at each end of the interval $t\\in[0,1]$ and small in the middle. We thus propose to use a Ushaped timestep distribution for $p_{t}$ . Specifically, we define $p_{t}(u)\\propto\\bar{\\mathrm{exp}}(a u)+\\exp(-a u)$ on $u\\in[0,1]$ . We find that $a=4$ works well in practice (Table 1). Compared to the uniform timestep distribution (config B), the U-shaped distribution (config C) improves the FID of 2-rectified flow from 7.14 to 5.17 (a $28\\%$ improvement) on CIFAR-10, 12.39 to 9.03 $(27\\%)$ on AFHQ, and 8.84 to 6.81 $(23\\%)$ on FFHQ in the one-step setting. ", "page_idx": 4}, {"type": "image", "img_path": "mSHs6C7Nfa/tmp/0db9c62d227bb99ee2ed1f2052c47bd5de320ad411fd07facd3844e6bd521348.jpg", "img_caption": ["Figure 3: Training loss of the vanilla 2- rectified flow on CIFAR-10 measured on 5, 000 samples after 200, 000 iterations. The shaded area represents the 1 standard deviation of the loss. The dashed curve is our U-shaped timestep distribution, scaled by a constant factor for visualization. "], "img_footnote": [], "page_idx": 4}, {"type": "table", "img_path": "mSHs6C7Nfa/tmp/b64e6ffd30eb74ecceb20b6e4cf801e654a80ac365e10295395c48f2ecfbbba4.jpg", "table_caption": ["Table 1: Effects of the improved training techniques. The baseline (config A) is the 2-rectified flow with the uniform timestep distribution and the squared $\\ell_{2}$ metric [Liu et al., 2022]. Config B is the improved baseline with EDM initialization (Sec. 4.3) and increased batch size $\\mathrm{128\\rightarrow512}$ on CIFAR-10). FID (the lower the better) is computed using 50, 000 synthetic samples and the entire training set. We train the models for 800, 000 iterations on CIFAR-10 and 1, 000, 000 iterations on AFHQ and FFHQ and report the best FID for each setting. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "For 1-rectified flow training, $p_{t}$ was chosen to be the uniform distribution [Liu et al., 2022, 2023] or logit-normal distribution [Esser et al., 2024] which puts more emphasis on the middle of the interval. When training 1-rectified flow, a model learns to simply output the dataset average when $t=1$ and the noise average (i.e., zero) when $t=0$ . The meaningful part of the training thus happens in the middle of the interval. In contrast, from Eq. (3) we can see that 2-rectified flow learns to directly predict the data from the noise at $t=1$ and the noise from the data at $t=0$ , which are nontrivial tasks. Therefore, the U-shaped timestep distribution is more suitable for 2-rectified flow. ", "page_idx": 5}, {"type": "text", "text": "4.2 Loss function ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Previously, the squared $\\ell_{2}$ distance was used as the training metric for rectified flow to obtain the MMSE estimator $\\mathbb{E}[\\mathbf{x}|\\mathbf{x}_{t}]$ . However, as we have shown in Sec. 3 that $\\mathbb{E}[{\\bf x}|{\\bf x}_{t}=(1-t){\\bf x}^{\\prime}+t{\\bf z}^{\\prime}]\\approx{\\bf x}^{\\prime}$ , we can generalize Eq. (2) or equivalently Eq. (3) to any premetric $m$ (i.e. $m(\\mathbf{a},\\mathbf{b})=0\\Leftrightarrow\\mathbf{a}=\\mathbf{b})$ ): ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta}\\mathbb{E}_{\\mathbf{x},\\mathbf{z}\\sim p_{\\mathbf{xz}}}\\mathbb{E}_{t\\sim p_{t}}[m(\\mathbf{z}-\\mathbf{x},\\mathbf{v}_{\\theta}(\\mathbf{x}_{t},t))],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Note that without the intuition in Sec. 3, only the squared $\\ell_{2}$ distance would have been a valid premetric, as any other premetric makes the model deviate from the intended optimum (the posterior expectation $\\mathbb{E}[\\mathbf{x}|\\mathbf{x}_{t}])$ . Although the choice of $m$ does not affect the optimum, it does affect the training dynamics and thus the obtained model. Other than the squared $\\ell_{2}$ distance, we consider the following premetrics: ", "page_idx": 5}, {"type": "text", "text": "\u2022 Pseudo-Huber [Charbonnier et al., 1997, Song and Dhariwal, 2023]: $m_{\\mathrm{hub}}(\\mathbf{z}-\\mathbf{x},\\mathbf{v}_{\\theta}((\\mathbf{x}_{t},t))=$ $\\sqrt{||{\\mathbf z}-{\\mathbf x}-{\\mathbf v}_{\\theta}({\\mathbf x}_{t},t)||_{2}^{2}+c^{2}}-c$ , where $c=0.00054d$ with $d$ being data dimensionality. \u2022 LPIPS-Huber: $m_{\\mathrm{lp-hub}}(\\mathbf{z}-\\mathbf{x},\\mathbf{v}_{\\theta}(\\mathbf{x}_{t},t))\\,=\\,(1-t)m_{\\mathrm{hub}}(\\mathbf{z}-\\mathbf{x},\\mathbf{v}_{\\theta}(\\mathbf{x}_{t},t))+\\mathrm{LPIPS}(\\mathbf{x},\\mathbf{x}_{t}-t\\,\\cdot\\,\\mathbf{x},\\mathbf{v}_{\\theta}(\\mathbf{x}_{t},t)).$ $\\mathbf{v}_{\\theta}(\\mathbf{x}_{t},t))$ , where $\\mathrm{LPIPS}(\\cdot,\\cdot)$ is the learned perceptual image patch similarity [Zhang et al., 2018]. \u2022 LPIPS-Huber $\\begin{array}{r}{\\cdot\\frac{1}{t}\\colon m_{\\mathrm{lp\\-hub-}\\frac{1}{t}}(\\mathbf{z}-\\mathbf{x},\\mathbf{v}_{\\theta}(\\mathbf{x}_{t},t))=(1-t)m_{\\mathrm{hub}}(\\mathbf{z}-\\mathbf{x},\\mathbf{v}_{\\theta}(\\mathbf{x}_{t},t))+\\frac{1}{t}\\mathrm{LPIPS}(\\mathbf{x},\\mathbf{x}_{t}-\\mathbf{x},\\mathbf{v}_{\\theta}(\\mathbf{x}_{t},t)).}\\end{array}$ $t\\cdot\\mathbf{v}_{\\theta}(\\mathbf{x}_{t},t))$ , ", "page_idx": 5}, {"type": "text", "text": "The Pseudo-Huber loss is less sensitive to the outliers than the squared $\\ell_{2}$ loss, which can potentially reduce the gradient variance [Song and Dhariwal, 2023] and make training easier. In our initial experiments, we found that the Pseudo-Huber loss tends to work better than the squared $\\ell_{2}$ loss with a small batch size (e.g. 128 on CIFAR-10). When the batch size is sufficiently large, it performs on par with the squared $\\ell_{2}$ loss on CIFAR-10 and FFHQ-64 and outperforms it on AFHQ-64, as shown in Table 1. As it is less sensitive to the batch size, we choose to use the Pseudo-Huber loss in the following experiments. ", "page_idx": 5}, {"type": "text", "text": "We also explore the LPIPS, which forces the model to focus on reducing the perceptual distance between the generated data and the ground truth. Since LPIPS is not a premetric as two different ", "page_idx": 5}, {"type": "text", "text": "Table 2: The converted time and scale for the variance preserving (VP) and variance exploding (VE) diffusion models. Here, $\\begin{array}{r}{\\alpha(t)=\\exp(-\\frac{1}{2}\\int_{0}^{t}(19.9s+0.1)d s)}\\end{array}$ following Song et al. [2020b], and the perturbation kernel of the VE diffusion is $\\mathcal{N}(\\mathbf{x},t^{2}\\mathbf{I})$ following Karras et al. [2022]. ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{t_{\\mathrm{VE}}\\quad\\quad s_{\\mathrm{VP}}\\quad\\quad s_{\\mathrm{VE}}}{\\frac{1}{9.95}\\left(-0.05+\\sqrt{0.0025-19.9\\cdot\\ln{\\frac{1-t}{\\sqrt{(1-t)^{2}+t^{2}}}}}\\right)\\quad\\frac{t}{1-t}\\quad\\frac{\\alpha(t\\mathrm{ve})}{1-t}\\quad\\frac{1}{1-t}}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "points could have zero LPIPS if they are perceptually similar, we use it in combination with the Pseudo-Huber loss with the weighting $1-t$ , thereby relying more on LPIPS when $t$ is close to 1 where the task is more challenging. Note that in $m_{\\mathrm{lp-hub}}$ , the gradient vanishes when $t$ is close to zero. To compsensate, we experiment with $m_{\\mathrm{lp-hub-}\\frac{1}{t}}$ where we multiply LPIPS by $\\frac{1}{t}$ . Compared to config D, the LPIPS-Huber loss improves the FID of 2-rectified flow from 5.24 to 3.38 (a $35\\%$ improvement) on CIFAR-10, 8.20 to 4.11 $(50\\%)$ on AFHQ, and 7.06 to 5.21 $(26\\%)$ on FFHQ in the one-step setting, as seen in Table 1. ", "page_idx": 6}, {"type": "text", "text": "4.3 Initialization with pre-trained diffusion models ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Training 1-rectified flow from scratch is computationally expensive. Recently, Pokle et al. [2023] showed that pre-trained diffusion models can be used to approximate $\\mathbb{E}[{\\bf x}|{\\bf x}_{t}={\\bf z}_{t}]$ in Eq. (1) by adjusting the signal-to-noise ratio. The following proposition is the special cases of Lemma 2 of Pokle et al. [2023] restated with extended proof and a minor fix. We provide the constants and proof in Appendix. C.1. ", "page_idx": 6}, {"type": "text", "text": "Proposition 1 Let $p^{R E}(\\mathbf{x}|\\mathbf{x}_{t},t)$ be the posterior distribution of the perturbation kernel $\\mathcal{N}((1\\mathrm{~-~}$ $t)\\mathbf{x},\\overline{{t^{2}}}\\mathbf{I})$ . Also, let $p^{V P}(\\mathbf{x}|\\mathbf{x}_{t},t)$ and $p^{V\\bar{E}}(\\mathbf{x}|\\mathbf{x}_{t},t)$ be the posterior distributions of $\\mathcal{N}(\\alpha(t)\\mathbf{x},(1-$ $\\alpha(t))^{2}\\mathbf{I})$ and $\\mathcal{N}(\\mathbf{x},t^{2}\\mathbf{I})$ , each. Then, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\int p^{R E}(\\mathbf{x}|\\mathbf{x}_{t}=\\mathbf{z}_{t},t)\\mathbf{x}\\,d\\mathbf{x}=\\int p^{V P}(\\mathbf{x}|\\mathbf{x}_{t}=s_{V P}\\mathbf{z}_{t},t_{V P})\\mathbf{x}\\,d\\mathbf{x}=\\int p^{V E}(\\mathbf{x}|\\mathbf{x}_{t}=s_{V P}\\mathbf{z}_{t},t_{V E})\\mathbf{x}\\,d\\mathbf{x},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where sVP and $s_{V E}$ are the scaling factors and $t_{V P}$ and $t_{V E}$ are the converted times for the VP and $V E$ diffusion models, respectively. ", "page_idx": 6}, {"type": "text", "text": "We have explicitly computed the time and scale conversion factors for the VP and VE diffusion models in Table 2. See Appendix C for derivation. ", "page_idx": 6}, {"type": "text", "text": "Proposition 1 allows us to initialize the Reflow with the pre-trained diffusion models such as EDM [Karras et al., 2022] or DDPM [Ho et al., 2020] and use Table 2 to adjust the time and scaling factors. ", "page_idx": 6}, {"type": "text", "text": "Starting from the vanilla 2-rectified flow setup [Liu et al., 2022] (config A), we initialize 1-rectified flow with the pre-trained EDM (VE). We also increase the batch size from 128 to 512 on CIFAR-10 compared to Liu et al. [2022]. Overall, these improve the FID of 2-rectified flow from 12.21 to 7.14 (a $4\\bar{2}\\%$ improvement) in the one-step setting on CIFAR-10 (config B). ", "page_idx": 6}, {"type": "text", "text": "4.4 Incorporating real data ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Training 2-rectified flow does not require real data (i.e., it can be data-free), but we can use real data if it is available. To see the effects of incorporating real data, we integrate the generative ODE of 1-rectified flow backward from $t=0$ to $t=1$ using an NFE of 128 to collect 50, 000 pairs of (real data, synthetic noise) on CIFAR-10. For quick validation, we take the pre-trained 2-rectified flow model (config F) and fine-tune it using the (real data, synthetic noise) pairs for 5, 000 iterations with a learning rate of 1e-5. This improves the FID of 2-rectified flow from 3.38 to 3.07 in the one-step setting on CIFAR-10 (config G). ", "page_idx": 6}, {"type": "text", "text": "In this fine-tuning setting, we also explored using (synthetic data, real noise) pair with a probability of $p$ , but we found that not incorporating (synthetic data, real noise) pairs at all (i.e., $p=0$ ) performs the best. We expect that training from scratch will further improve the performance with different values of $p$ , and leave it to future work. A similar idea is also explored in Anonymous [2024]. ", "page_idx": 6}, {"type": "table", "img_path": "", "table_caption": ["able 3: Unconditional generation on CIFAR-10. Table 4: Class-conditional generation on ImageNet $64\\times64$ . "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "mSHs6C7Nfa/tmp/dbaca2f7d92e47ff2b10aaa2255a8b810ca9b2586b23581112a223f1a90ce9da.jpg", "table_caption": [], "table_footnote": ["The red rows correspond to the top-5 baselines for the 1-NFE setting, and the blue rows correspond to the top 5 baselines for the 2-NFE setting. The lowest FID scores for 1-NFE and 2-NFE are boldfaced. "], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We call these combined improvements to Reflow training 2-rectified flow $^{++}$ and evaluate it on four datasets: CIFAR-10 Krizhevsky et al. [2009], AFHQ Choi et al. [2020], FFHQ Karras et al. [2019], and ImageNet Deng et al. [2009]. We compare our improved Reflow to up to 20 recent baselines, in the families of diffusion models, distilled diffusion models, score distillation, GANs, consistency models, and rectified flows. The details of our experimental setup are included in Appendix E. ", "page_idx": 7}, {"type": "text", "text": "5.1 Unconditional and class-conditional image generation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In Tables 3 and 4, we compare 2-rectified flow $^{\\mathrel{\\textstyle\\downarrow}++}$ with the state-of-the-art methods on CIFAR-10 and ImageNet $64\\times64$ . We observe two main messages: ", "page_idx": 7}, {"type": "text", "text": "On both datasets, 2-rectified $\\mathbf{flow}\\mathbf{+}\\mathbf{+}$ (ours) outperforms or is competitive with SOTA baselines in the 1-2 NFE regime. On CIFAR-10 (Table 3), our 2-rectified flow achieves an FID of 3.07 in one step, surpassing existing distillation methods such as consistency distillation (CD) [Song et al., 2023], progressive distillation (PD) [Salimans and Ho, 2022], diffusion model sampling with neural operator (DSNO) [Zheng et al., 2022], and TRAnsitive Closure Time-distillation (TRACT) [Berthelot et al., 2023]. On ImageNet $64\\times64$ (Table 4), our model surpasses the distillation methods such as CD, PD, DFNO, TRACT, and BOOT in one-step generation. We also close the gap with iCT (4.01 vs ", "page_idx": 7}, {"type": "image", "img_path": "mSHs6C7Nfa/tmp/300d8b74566ec3df25d994ef326a93ff4eb900ad9e7d5a3706275f5200279f9e.jpg", "img_caption": ["Figure 4: Effects of ODE Solver and new update rule. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.31), the state-of-the-art consistency model, even with half the batch size. Note that on ImageNet, our model does not use real data during training, while consistency training (CT) requires real data. We believe we could further reduce the gap by using config G in Tab. 1. Uncurated samples of our model are provided in Appendix. G. ", "page_idx": 8}, {"type": "text", "text": "2-rectified flow $\\mathbf{\\zeta++}$ reduces the FID of 2-rectified flows by up to $75\\%$ . Compared to vanilla rectified flows [Liu et al., 2022], our one-step FID on CIFAR-10 is lower than that of the previous 2-rectified flow by 9.14 (a reduction of $75\\%$ ), and of the 3-rectified flow by 5.08 (see also Table 1 for ablations on other datasets). In addition, it outperforms the previous 2-rectified flow with 110 NFEs using only one step and also surpasses 2-rectified flow $^+$ distillation, which requires an additional distillation stage. ", "page_idx": 8}, {"type": "text", "text": "5.2 Reflow can be computationally more efficient than other distillation methods ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "At first glance, Reflow seems computationally expensive compared to CD and CT as it requires generating synthetic pairs before training. However, CD requires 4 (1 for student, 1 for teacher, and 2 for Heun\u2019s solver) forward passes for each training iteration, and CT requires 2 (1 for student and 1 for teacher) forward passes, while Reflow requires only 1. For example, in our ImageNet experiment setting, the total number of forward passes for Reflow is $395\\mathrm{M}+$ $1433.6\\mathbf{M}=1828.6\\mathbf{M}$ (395M for generating pairs ", "page_idx": 8}, {"type": "table", "img_path": "mSHs6C7Nfa/tmp/aa705e839a18331f46b7e0b653fb18690bc7088e06d4162b0073dfe5fb359318.jpg", "table_caption": ["Table 6: Comparison of the number of forward passes. Reflow uses 395M forward passes for generating pairs and 1, 433.6M for training. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "and 1,433.6M for training), while the total numbers of forward passes for CD and CT would be $1,433.6\\cdot4=5$ , 734.4M and 1, $433.6\\cdot2=2$ , $867.2\\mathbf{M}$ under the same setting. See Table 6 for the comparison. Moreover, generating pairs is a one-time cost since we can reuse the pairs for multiple training runs. ", "page_idx": 8}, {"type": "text", "text": "In terms of the storage cost, the synthetic images for ImageNet $64\\times64$ require 42 GB. For noise, we only store the states of the random number generator, which is negligible. ", "page_idx": 8}, {"type": "text", "text": "While these results should be further validated for larger datasets, our results suggest that the fact that Reflow requires generating synthetic pairs does not necessarily make it less computationally efficient than other distillation methods. ", "page_idx": 8}, {"type": "text", "text": "5.3 Effects of samplers ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Unlike distillation methods, rectified flow is a neural ODE, and its outputs approach the true solution of the ODE as NFE increases (i.e., precision grows). Figure 4 shows that with the standard Euler solver, FID decreases as NFE increases on all datasets. Moreover, Heun\u2019s second-order solver further improves the trade-off curve between FID and NFE. This suggests that there may be further room for improvement by using more advanced samplers. We provide some preliminary ideas towards this goal in Appendix D. ", "page_idx": 8}, {"type": "image", "img_path": "mSHs6C7Nfa/tmp/c8be5a21c679e7c211069b65d0f810098ea5f423b861ee31f4f20f7d61d39c12.jpg", "img_caption": [], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Figure 5: Inversion results on CIFAR-10. (a) Reconstruction error between real and reconstructed data is measured by the mean squared error (MSE), where the ${\\bf X}$ -axis represents NFEs used for inversion and reconstruction (e.g. 2 means 2 for inversion and 2 for reconstruction). (b) Distribution of $||\\mathbf{z}||_{2}^{2}$ of the inverted noises as a proxy for Gaussianity $\\mathrm{(NFE=8)}$ ). The green histogram represents the distribution of true noise, which is Chi-squared with $3\\times32\\times32=3072$ degrees of freedom. (c) Inversion and reconstruction results using $(8+8)$ NFEs. With only 8 NFEs, EDM fails to produce realistic noise, and also the reconstructed samples are blurry. ", "page_idx": 9}, {"type": "image", "img_path": "mSHs6C7Nfa/tmp/39eece36b6bb6271ee3cf400dc1d6c9ea7ff8bfea5ff1d76c733eebbf84a8255.jpg", "img_caption": ["Figure 6: Applications of few-step inversion. (a) Interpolation between two real images. (b) Imageto-image translation. The total NFEs used are 6 (4 for inversion and 2 for generation). "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "5.4 Inversion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Unlike distillation methods, rectified flows are neural ODEs, thus they allow for inversion from data to noise by simply integrating the ODE in the backward direction. In diffusion models, inversion has been used for various applications such as image editing [Hertz et al., 2022, Kim et al., 2022, Wallace et al., 2023, Su et al., 2022, Hong et al., 2023] and watermarking [Wen et al., 2023], but it usually requires many NFEs. Figure 5 (a) demonstrates that our 2-rectified flow $^{\\downarrow++}$ achieves significantly lower reconstruction error than EDM. Notably, the reconstruction error of 2-rectified $\\mathrm{\\flow++}$ with only 2 NFEs is lower than that of EDM with 16 NFEs. In (b), we compare the quality of the inverted noise, where we find that the noise vectors of 2-rectified flow are more Gaussian-like than those of EDM, in the sense that their norm is closer to that of typical Gaussian noise. These are also shown visually in (c). In Figure 6, we show two applications of inversion: interpolating between two real images (a) and image-to-image translation (b). Notably, the total NFE used is only 6 (4 for inversion and 2 for generation), which is significantly lower than what is typically required in diffusion models $(\\ge100)$ [Hong et al., 2023]. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we propose several improved training techniques for rectified flows, including the Ushaped timestep distribution and LPIPS-Huber loss. We show that by combining these improvements, 2-rectified flow $\\downarrow++$ outperforms the state-of-the-art distillation methods in the 1-2 NFE regime on CIFAR-10 and ImageNet $64\\times64$ and closes the gap with iCT, the state-of-the-art consistency model. 2-rectified flows $^{++}$ have limitations though\u2014they still do not outperform the best consistency models (iCT), and their training is slower (by about $15\\%$ per iteration on ImageNet) than previous rectified flows because of the LPIPS loss. Despite these shortcomings, the training techniques we propose can easily and significantly boost the performance of rectified flows in the low NFE setting, without harming performance at the higher NFE setting. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was made possible in part by the National Science Foundation under grant CCF-2338772, CNS-2325477, as well as generous support from Google, the Sloan Foundation, Intel, and Bosch. This work used Bridges-2 GPU at the Pittsburgh Supercomputing Center through allocation CIS240037 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants 2138259, 2138286, 2138307, 2137603, and 2138296 Boerner et al. [2023]. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Michael S Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. arXiv preprint arXiv:2209.15571, 2022. ", "page_idx": 10}, {"type": "text", "text": "Anonymous. Balanced conic rectified flow. In Submitted to The Thirteenth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id $\\cdot$ ctSjIlYN74. under review.   \nDavid Berthelot, Arnaud Autef, Jierui Lin, Dian Ang Yap, Shuangfei Zhai, Siyuan Hu, Daniel Zheng, Walter Talbott, and Eric Gu. Tract: Denoising diffusion models with transitive closure time-distillation. arXiv preprint arXiv:2303.04248, 2023.   \nTimothy J Boerner, Stephen Deems, Thomas R Furlani, Shelley L Knuth, and John Towns. Access: Advancing innovation: Nsf\u2019s advanced cyberinfrastructure coordination ecosystem: Services & support. In Practice and Experience in Advanced Research Computing, pages 173\u2013176. 2023.   \nAndrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018.   \nPierre Charbonnier, Laure Blanc-F\u00e9raud, Gilles Aubert, and Michel Barlaud. Deterministic edgepreserving regularization in computed imaging. IEEE Transactions on image processing, 6(2): 298\u2013311, 1997.   \nRicky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations. Advances in neural information processing systems, 31, 2018.   \nYunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. Stargan v2: Diverse image synthesis for multiple domains. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8188\u20138197, 2020.   \nGuillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit: Diffusion-based semantic image editing with mask guidance. arXiv preprint arXiv:2210.11427, 2022.   \nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.   \nPrafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems, 34, 2021.   \nPatrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M\u00fcller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. arXiv preprint arXiv:2403.03206, 2024.   \nJiatao Gu, Shuangfei Zhai, Yizhe Zhang, Lingjie Liu, and Joshua M Susskind. Boot: Data-free distillation of denoising diffusion models with bootstrapping. In ICML 2023 Workshop on Structured Probabilistic Inference $\\{\\backslash\\mathcal{E}\\}$ Generative Modeling, 2023.   \nAmir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Promptto-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022.   \nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017.   \nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840\u20136851, 2020.   \nJonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. arXiv preprint arXiv:2204.03458, 2022.   \nSeongmin Hong, Kyeonghyun Lee, Suh Yoon Jeon, Hyewon Bae, and Se Young Chun. On exact inversion of dpm-solvers. arXiv preprint arXiv:2311.18387, 2023.   \nTero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4401\u20134410, 2019.   \nTero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training generative adversarial networks with limited data. Advances in Neural Information Processing Systems, 33:12104\u201312114, 2020a.   \nTero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8110\u20138119, 2020b.   \nTero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusionbased generative models. arXiv preprint arXiv:2206.00364, 2022.   \nDongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, and Stefano Ermon. Consistency trajectory models: Learning probability flow ode trajectory of diffusion. arXiv preprint arXiv:2310.02279, 2023.   \nGwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Diffusionclip: Text-guided diffusion models for robust image manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2426\u20132435, 2022.   \nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. Toronto, ON, Canada, 2009.   \nSangyun Lee, Beomsu Kim, and Jong Chul Ye. Minimizing trajectory curvature of ode-based generative models. arXiv preprint arXiv:2301.12003, 2023.   \nYaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022.   \nQiang Liu. Rectified flow: A marginal preserving approach to optimal transport. arXiv preprint arXiv:2209.14577, 2022.   \nXingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022.   \nXingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, and Qiang Liu. Instaflow: One step is enough for high-quality diffusion-based text-to-image generation. arXiv preprint arXiv:2309.06380, 2023.   \nCheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. arXiv preprint arXiv:2206.00927, 2022.   \nEric Luhman and Troy Luhman. Knowledge distillation in iterative generative models for improved sampling speed. arXiv preprint arXiv:2101.02388, 2021.   \nWeijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhihua Zhang. Diffinstruct: A universal approach for transferring knowledge from pre-trained diffusion models. Advances in Neural Information Processing Systems, 36, 2024.   \nPaulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. arXiv preprint arXiv:1710.03740, 2017.   \nRon Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6038\u20136047, 2023.   \nAlexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International Conference on Machine Learning, pages 8162\u20138171. PMLR, 2021.   \nAshwini Pokle, Matthew J Muckley, Ricky TQ Chen, and Brian Karrer. Training-free linear image inversion via flows. arXiv preprint arXiv:2310.04432, 2023.   \nAram-Alexandre Pooladian, Heli Ben-Hamu, Carles Domingo-Enrich, Brandon Amos, Yaron Lipman, and Ricky Chen. Multisample flow matching: Straightening flows with minibatch couplings. arXiv preprint arXiv:2304.14772, 2023.   \nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical textconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.   \nTim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512, 2022.   \nNeta Shaul, Juan Perez, Ricky TQ Chen, Ali Thabet, Albert Pumarola, and Yaron Lipman. Bespoke solvers for generative flow models. arXiv preprint arXiv:2310.19075, 2023.   \nAbhinav Shrivastava, Abhinav Gupta, and Ross Girshick. Training region-based object detectors with online hard example mining. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 761\u2013769, 2016.   \nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pages 2256\u20132265. PMLR, 2015.   \nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020a.   \nYang Song and Prafulla Dhariwal. Improved techniques for training consistency models. arXiv preprint arXiv:2310.14189, 2023.   \nYang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in Neural Information Processing Systems, 32, 2019.   \nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020b.   \nYang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. arXiv preprint arXiv:2303.01469, 2023.   \nXuan Su, Jiaming Song, Chenlin Meng, and Stefano Ermon. Dual diffusion implicit bridges for image-to-image translation. arXiv preprint arXiv:2203.08382, 2022.   \nArash Vahdat, Karsten Kreis, and Jan Kautz. Score-based generative modeling in latent space. Advances in Neural Information Processing Systems, 34:11287\u201311302, 2021.   \nPascal Vincent. A connection between score matching and denoising autoencoders. Neural computation, 23(7):1661\u20131674, 2011.   \nBram Wallace, Akash Gokul, and Nikhil Naik. Edict: Exact diffusion inversion via coupled transformations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22532\u201322541, 2023.   \nDaniel Watson, William Chan, Jonathan Ho, and Mohammad Norouzi. Learning fast samplers for diffusion models by differentiating through sample quality. In International Conference on Learning Representations, 2021.   \nYuxin Wen, John Kirchenbauer, Jonas Geiping, and Tom Goldstein. Tree-ring watermarks: Fingerprints for diffusion images that are invisible and robust. arXiv preprint arXiv:2305.20030, 2023.   \nTianwei Yin, Micha\u00ebl Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William T Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. arXiv preprint arXiv:2311.18828, 2023.   \nQinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator. arXiv preprint arXiv:2204.13902, 2022.   \nRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586\u2013595, 2018.   \nHongkai Zheng, Weili Nie, Arash Vahdat, Kamyar Azizzadenesheli, and Anima Anandkumar. Fast sampling of diffusion models via operator learning. arXiv preprint arXiv:2211.13449, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "image", "img_path": "mSHs6C7Nfa/tmp/cf2d98d59cd71c8f851bdc7b3e2b3999745c9e627ee745df309809be7a000b5f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Figure 7: (a) Autocorrelation plot on CIFAR-10 analogous to Figure 2(d). (b) t-SNE visualization of the inception-v3 features of the samples in Figure 2(b). They show negligible overlap. ", "page_idx": 14}, {"type": "text", "text": "A Equivalence of $\\mathbf{v}$ -parameterization and $\\mathbf{x}$ -parameterization ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Given ${\\bf x}_{t}=(1-t){\\bf x}\\!+\\!t{\\bf z}$ and $\\mathbf{z}-\\mathbf{x}=(\\mathbf{x}_{t}-\\mathbf{x})/t$ , the equivalence of the $\\mathbf{x}$ -parameterization (Eq. (2)) and the $\\mathbf{v}$ -parameterization (Eq. (3)) can be shown in the following way. The result is borrowed from the Appendix of Lee et al. [2023], and we provide here for completeness. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int_{0}^{1}\\mathbb{E}[||(\\mathbf{z}-\\mathbf{x})-\\mathbf{v}_{\\theta}(\\mathbf{x}_{t},t)||_{2}^{2}]\\,d t=\\displaystyle\\int_{0}^{1}\\mathbb{E}[||(\\mathbf{x}_{t}-\\mathbf{x})/t-\\mathbf{v}_{\\theta}(\\mathbf{x}_{t},t)||_{2}^{2}]\\,d t}\\\\ &{\\phantom{\\quad\\quad}\\quad\\quad=\\displaystyle\\int_{0}^{1}\\mathbb{E}[||(\\mathbf{x}_{t}-\\mathbf{x})/t-(\\mathbf{x}_{t}-\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},t))/t||_{2}^{2}]\\,d t}\\\\ &{\\phantom{\\quad\\quad\\quad=\\displaystyle}\\int_{0}^{1}\\mathbb{E}[\\frac{1}{t^{2}}||\\mathbf{x}-\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},t))||_{2}^{2}]\\,d t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "This is equivalent to Eq. (2) with $\\omega(t)=1/t^{2}$ . ", "page_idx": 14}, {"type": "text", "text": "B Additional Details for Figure 2 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we provide additional results details for Figure 2. Figure 7(a) shows the autocorrelation histogram on CIFAR-10 while Figure 2(d) is on FFHQ-64. Figure 7(b) shows that the inception features of the samples in Figure 2(b) rarely overlap with each other. ", "page_idx": 14}, {"type": "text", "text": "For the autocorrelation plots, we use 30,000 pairs of $(\\mathbf{x}^{\\prime},\\mathbf{x}^{\\prime\\prime})$ and randomly sample ${\\bf z}$ from the standard Gaussian distribution. For a $d$ -dimensional vector $\\mathbf{u}$ , we define the autocorrelation as: ", "page_idx": 14}, {"type": "equation", "text": "$$\nR_{\\mathbf{u}}(l)=\\frac{1}{d-l}\\sum_{k=1}^{d-l}\\mathbf{u}_{k}\\mathbf{u}_{k+l},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\mathbf{u}_{k}$ is the $k$ -th element of a vector $\\mathbf{u}$ and $l>0$ represents the lag. ", "page_idx": 14}, {"type": "text", "text": "C Initialization Results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "C.1 Proof of Proposition 1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Consider two perturbation kernels $p_{t}(\\mathbf{x}_{t}|\\mathbf{x})=\\mathcal{N}(s(t)\\mathbf{x},\\sigma(t)^{2}\\mathbf{I})$ and $p_{t}^{\\prime}(\\mathbf{x}_{t}|\\mathbf{x})=\\mathcal{N}(s^{\\prime}(t)\\mathbf{x},\\sigma^{\\prime}(t)^{2}\\mathbf{I})$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p_{t}(\\mathbf{x}_{t}|\\mathbf{x})=\\frac{1}{(2\\pi\\sigma(t)^{2})^{d/2}}\\exp(-\\frac{1}{2\\sigma(t)^{2}}||\\mathbf{x}_{t}-s(t)\\mathbf{x}||_{2}^{2})}\\\\ {p_{t}^{\\prime}(\\mathbf{x}_{t}|\\mathbf{x})=\\frac{1}{(2\\pi\\sigma^{\\prime}(t)^{2})^{d/2}}\\exp(-\\frac{1}{2\\sigma^{\\prime}(t)^{2}}||\\mathbf{x}_{t}-s^{\\prime}(t)\\mathbf{x}||_{2}^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Let $t^{\\prime}(t)$ be such that $\\begin{array}{r}{\\frac{s(t)}{\\sigma(t)}\\,=\\,\\frac{s^{\\prime}(t^{\\prime})}{\\sigma^{\\prime}(t^{\\prime})}}\\end{array}$ . We will show that $\\begin{array}{r}{p_{t}(\\mathbf{x}|\\mathbf{x}_{t})\\,=\\,p_{t^{\\prime}}^{\\prime}(\\mathbf{x}|\\frac{s^{\\prime}(t^{\\prime})}{s(t)}\\mathbf{x}_{t})}\\end{array}$ . We start by showing: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Vert r_{\\ell}\\big(\\frac{\\mathcal{S}^{\\prime}(\\ell)}{s_{\\ell}(\\ell)}\\mathbf{x}_{1}\\big\\vert\\mathbf{x}\\big)=\\frac{1}{(2\\pi\\sigma^{\\prime}(\\ell^{\\prime}))^{2/2}}\\exp(-\\frac{1}{2\\sigma^{\\prime}(\\ell^{\\prime})^{2}})\\Vert\\mathbf{x}^{\\ell^{\\prime}}(\\ell)\\mathbf{x}_{1}-s^{\\prime}(\\ell^{\\prime})\\mathbf{x}\\Vert_{2}^{2}}\\\\ &{=\\frac{1}{(2\\pi\\sigma^{\\prime}(\\ell^{\\prime})^{2})^{2/2}}\\exp(-\\frac{1}{2\\sigma^{\\prime}(\\ell^{\\prime})^{2}})\\Vert\\mathbf{x}^{\\ell^{\\prime}}(\\mathbf{x}_{1}-s(\\ell)\\mathbf{x})\\Vert_{2}^{2}}\\\\ &{=\\frac{1}{(2\\pi\\sigma^{\\prime}(\\ell^{\\prime})^{2})^{2/2}}\\exp(-\\frac{1}{2\\sigma^{\\prime}(\\ell^{\\prime})^{2}})\\Vert\\mathbf{x}^{\\ell^{\\prime}}(\\ell)\\mathbf{x}-s(\\ell)\\mathbf{x}\\Vert_{2}^{2}}\\\\ &{=\\frac{1}{(2\\pi\\sigma^{\\prime}(\\ell^{\\prime})^{2})^{2/2}}\\exp(-\\frac{1}{2\\sigma^{\\prime}(\\ell^{\\prime})^{2}})\\Vert\\mathbf{x}-s(\\ell)\\mathbf{x}\\Vert_{2}^{2}}\\\\ &{=\\frac{1}{(2\\pi\\sigma^{\\prime}(\\ell^{\\prime})^{2})^{2/2}}\\exp(-\\frac{1}{2\\sigma^{\\prime}(\\ell^{\\prime})^{2}})\\Vert\\mathbf{x}-s(\\ell)\\mathbf{x}\\Vert_{2}^{2}}\\\\ &{=\\frac{1}{(2\\pi\\sigma^{\\prime}(\\ell^{\\prime})^{2})^{2/2}}\\exp(-\\frac{1}{2\\sigma^{\\prime}(\\ell^{\\prime})^{2}})\\Vert\\mathbf{x}-s(\\ell)\\mathbf{x}\\Vert_{2}^{2}}\\\\ &{=\\frac{1}{(2\\pi\\sigma^{\\prime}(\\ell^{\\prime})^{2})^{2/2}}(2\\pi\\sigma(\\ell^{\\prime})^{2/2})\\textrm{e x p}(-\\frac{1}{2\\sigma(\\ell^{\\prime})^{2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Here, Eq. (20) says that $\\begin{array}{r}{p_{t}(\\mathbf{x}_{t}|\\mathbf{x})\\propto p_{t^{\\prime}}^{\\prime}(\\frac{s^{\\prime}(t^{\\prime})}{s(t)}\\mathbf{x}_{t}|\\mathbf{x})}\\end{array}$ (but not equal), which is a minor fix from the original proof [Pokle et al., 2023]. Then, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{p_{t}(\\mathbf{x}|\\mathbf{x}_{t})=\\displaystyle\\frac{1}{p_{t}(\\mathbf{x}_{t})}p_{\\mathbf{x}}(\\mathbf{x})p_{t}(\\mathbf{x}_{t}|\\mathbf{x})}}\\\\ {{p_{t^{\\prime}}^{\\prime}(\\mathbf{x}|\\frac{s^{\\prime}(t^{\\prime})}{s(t)}\\mathbf{x}_{t})=\\displaystyle\\left(\\frac{\\sigma(t)}{\\sigma^{\\prime}(t^{\\prime})}\\right)^{d}\\frac{1}{p_{t^{\\prime}}^{\\prime}(\\frac{s^{\\prime}(t^{\\prime})}{s(t)}\\mathbf{x}_{t})}p_{\\mathbf{x}}(\\mathbf{x})p_{t}(\\mathbf{x}_{t}|\\mathbf{x})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $\\begin{array}{r}{p_{t^{\\prime}}^{\\prime}(\\mathbf{x}|\\frac{s^{\\prime}(t^{\\prime})}{s(t)}\\mathbf{x}_{t})}\\end{array}$ should be integrated to one, $\\begin{array}{r}{\\left(\\frac{\\sigma(t)}{\\sigma^{\\prime}(t^{\\prime})}\\right)^{d}\\frac{1}{p_{t^{\\prime}}^{\\prime}(\\frac{s^{\\prime}(t^{\\prime})}{s(t)}\\mathbf{x}_{t})}=\\int p_{\\mathbf{x}}(\\mathbf{x})p_{t}(\\mathbf{x}_{t}|\\mathbf{x})d\\mathbf{x}}\\end{array}$ and thus two densities are equal. As the posterior densities are the same, their expectations are also the same. ", "page_idx": 15}, {"type": "text", "text": "In our case, $s(t)=1-t,\\sigma(t)=t,$ , and $p_{t}^{\\prime}(\\mathbf{x}_{t}|\\mathbf{x})$ is the perturbation kernel of either the VP or VE diffusion model. Now we have to find $t^{\\prime}$ such that $\\begin{array}{r}{\\frac{1-t}{t}=\\frac{s^{\\prime}(t^{\\prime})}{\\sigma^{\\prime}(t^{\\prime})}}\\end{array}$ . ", "page_idx": 15}, {"type": "text", "text": "C.2 Perturbation Kernel Instantiations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We next provide the values of the converted time and scale of the variance preserving (VP) and variance exploding (VE) diffusion models. ", "page_idx": 15}, {"type": "text", "text": "VE diffusion model Karras et al. [2022] defines the perturbation kernel of the VE diffusion model as $p_{t}^{\\prime}(\\mathbf{x}_{t}|\\mathbf{x})=\\mathcal{N}(\\mathbf{x},t^{2}\\mathbf{I})$ . Then, $t^{\\prime}$ satisfies $\\begin{array}{r}{\\frac{1-t}{t}=\\frac{s^{\\prime}(t^{\\overline{{\\prime}}})}{\\sigma^{\\prime}(t^{\\prime})}=\\frac{1}{t^{\\prime}}}\\end{array}$ ) = t1\u2032 , so t\u2032 = 1t\u2212t, and ss((tt)) = 1\u2212t, which correspond to $t_{\\mathrm{VE}}$ and $s_{\\mathrm{VE}}$ in Table 2. ", "page_idx": 15}, {"type": "text", "text": "VP diffusion model Song et al. [2020b] defines the perturbation kernel of the VP diffusion model as $p_{t}^{\\prime}(\\mathbf{x}_{t}|\\mathbf{x})\\,=\\mathcal{N}(\\alpha(t)\\mathbf{x},(1-\\alpha(t)^{2})\\mathbf{I})$ , where $\\begin{array}{r}{\\alpha(t)\\ :=\\ \\exp(-\\frac{1}{2}\\int_{0}^{t}(19.9s+0.1)d s)}\\end{array}$ defined on $t\\in[0,1]$ . Then, $t^{\\prime}$ satisfies $\\begin{array}{r}{\\frac{1-t}{t}=\\frac{s^{\\prime}(t^{\\prime})}{\\sigma^{\\prime}(t^{\\prime})}=\\frac{\\alpha(t^{\\prime})}{\\sqrt{1-\\alpha(t^{\\prime})^{2}}}}\\end{array}$ . From here, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{(1-t)^{2}}{t^{2}}=\\frac{\\alpha(t^{\\prime})^{2}}{1-\\alpha(t^{\\prime})^{2}}}\\\\ {\\alpha(t^{\\prime})=\\sqrt{\\frac{(1-t)^{2}}{t^{2}+(1-t)^{2}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where we used the fact that $\\alpha(t)>0$ . Since $\\alpha(t)$ is a monotonically decreasing function for $t\\geq0$ ,   \nwe can use its inverse $\\alpha^{-1}$ to find $\\begin{array}{r}{t^{\\prime}=\\alpha^{-1}\\big(\\sqrt{\\frac{(1-t)^{2}}{t^{2}+(1-t)^{2}}}\\big)}\\end{array}$ . ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle y=\\alpha(t)=\\exp(-\\frac{1}{2}\\int_{0}^{t}(19.9s+0.1)d s)=\\exp(-\\frac{19.9}{4}t^{2}-0.05t)}}\\\\ {{\\displaystyle\\ln y=-\\frac{19.9}{4}t^{2}-0.05t}}\\\\ {{\\displaystyle\\frac{19.9}{4}t^{2}+0.05t+\\ln y=0}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Applying the quadratic formula, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\nt={\\frac{-0.05\\pm{\\sqrt{0.05^{2}-4\\cdot{\\frac{19.9}{4}}\\ln y}}}{2\\cdot{\\frac{19.9}{4}}}}={\\frac{-0.05\\pm{\\sqrt{0.0025-19.9\\ln y}}}{9.95}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since $y=\\alpha(t)$ is monotonically decreasing, we can choose the positive root: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\alpha^{-1}(y)=\\frac{-0.05+\\sqrt{0.0025-19.9\\ln y}}{9.95}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now, we arrive at ", "page_idx": 16}, {"type": "equation", "text": "$$\nt^{\\prime}=\\alpha^{-1}(\\sqrt{\\frac{(1-t)^{2}}{t^{2}+(1-t)^{2}}})=\\frac{-0.05+\\sqrt{0.0025-19.9\\ln\\sqrt{\\frac{(1-t)^{2}}{t^{2}+(1-t)^{2}}}}}{9.95},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which corresponds to $t_{\\mathrm{VP}}$ in Table 2. Also, we have $\\begin{array}{r}{\\frac{s^{\\prime}(t^{\\prime})}{s(t)}=\\frac{\\alpha(t^{\\prime})}{1-t}}\\end{array}$ \u03b11(tt) , which is sVP in Table 2. ", "page_idx": 16}, {"type": "text", "text": "D New Update Rule ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In the standard Euler solver, the update rule is $\\mathbf{z}_{t-\\Delta t}:=\\mathbf{z}_{t}-\\mathbf{v}(\\mathbf{z}_{t},t)\\Delta t$ . Alternatively, as $\\mathbf{x}_{\\pmb{\\theta}}(\\mathbf{z}_{t},t)$ of our model generates pretty good samples, we can instead use the linear interpolation between $\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t)$ and $\\mathbf{z}_{1}$ to get the next step: $\\overline{{\\mathbf{z}_{t-\\Delta t}}}:=(1-(t-\\Delta t))\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t)+(t-\\overline{{\\Delta t}})\\mathbf{z}_{1}$ . Note that when $\\mathrm{NFE}<3$ , the two update rules are equivalent and do not affect our results in Section 5.1. Fig. 4 shows that when applied to existing solvers, the new update rule improves the sampling efficiency up to $4\\times$ , achieving the best FID with $\\leq5$ NFEs. ", "page_idx": 16}, {"type": "text", "text": "Algorithm 2 shows the pseudocode for generating samples using the new update rule. Unlike the standard Euler update rule which only depends on the current state $\\mathbf{z}_{t}$ , our new update rule utilizes the previous state (i.e., $\\mathbf{z}_{1}$ ) to generate the next state $\\mathbf{Z}_{t-\\Delta t}$ and thus can be viewed as a form of history-dependent samplers. Obviously, incorporating the initial state only would not be the best choice. We believe that the result can be further improved, especially by using learning-based solvers [Watson et al., 2021, Shaul et al., 2023]; and leave such exploration to future work. ", "page_idx": 16}, {"type": "text", "text": "E Experimental Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Before training 2-rectified flow, we generate data-noise pairs following the sampling regime of EDM [Karras et al., 2022]. For CIFAR-10, we generate 1M pairs using 35 NFEs. For AFHQ, FFHQ, and ImageNet, we generate 5M pairs using 79 NFEs. We use Heun\u2019s second-order solver for all cases. In Table 3, we report the result of config G in Table 1. In ImageNet, we use the batch size of 2048 and train the models for 700,000 iterations using mixed-precision training [Micikevicius et al., 2017] with the dynamic loss scaling. We use config E in Table 1 for ImageNet. ", "page_idx": 16}, {"type": "text", "text": "We provide training configurations in Table 7. For all datasets, we use Adam optimizer. We use the exponential moving average (EMA) with 0.9999 decay rate for all datasets. ", "page_idx": 16}, {"type": "text", "text": "On ImageNet, the training takes roughly 9 days with 64 NVIDIA V100 GPUs. On CIFAR-10 and FFHQ/AFHQ, it takes roughly 4 days with 16 and 8 V100 GPUs, respectively. For all cases, we use ", "page_idx": 16}, {"type": "image", "img_path": "mSHs6C7Nfa/tmp/b6442b9e61db1f1d91dde054becfe7440b56f6a80d477353e89aab984c1bc712.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Table 7: Training configurations for each dataset. We linearly ramp up learning rates for all datasets. ", "page_idx": 17}, {"type": "table", "img_path": "mSHs6C7Nfa/tmp/1336422ce548c2b56e0d897c257703277b14770e065552910835f15ab5749c26.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "the NVIDIA DGX-2 cluster. To prevent zero-division error with EDM initialization, we sample $t$ from [0.00001, 0.99999] in practice. For a two-step generation, we evaluate $\\mathbf{v}_{\\theta}$ at $t=0.99999$ and $t=0.8$ . For other NFEs, we uniformly divide the interval [0.00001, 0.99999]. ", "page_idx": 17}, {"type": "text", "text": "License The following are licenses for each dataset we use: ", "page_idx": 17}, {"type": "text", "text": "\u2022 CIFAR-10: Unknown   \n\u2022 FFHQ: CC BY-NC-SA 4.0   \n\u2022 AFHQ: CC BY-NC 4.0   \n\u2022 ImageNet: Custom (research, non-commercial) ", "page_idx": 17}, {"type": "text", "text": "F Broader Impacts ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "This paper proposes an advanced algorithm to generate realistic data at high speed, which could have both positive and negative impacts. For example, it could be used for generating malicious or misleading content. Therefore, such technology should be deployed and used responsibly and with caution. We believe that our work is not expected to have any more potential negative impact than other work in the field of generative modeling. ", "page_idx": 17}, {"type": "text", "text": "G Uncurated Synthetic Samples ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We provide uncurated synthetic samples from our 2-rectified flow $^{++}$ on CIFAR-10, AFHQ, and ImageNet in Figures 8, 9, 10, 11, 20, 21, 16, 17, 18, 19, 12, 13, 14, and 15. We use our new sampler (Sec. 5.3) to generate these images. ", "page_idx": 18}, {"type": "image", "img_path": "mSHs6C7Nfa/tmp/7d0be25b2f0db9bd419b6461a0863387c6f6f65bb9c4e328627b8c46573bc2af.jpg", "img_caption": ["Figure 8: Synthetic samples from 2-rectified flow $^{++}$ on CIFAR-10 with NFE $=1$ (FID=3.38). "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "mSHs6C7Nfa/tmp/4ca33d4757b21328ebe07d8358c987f95e46b81d3f1b7ec3888b37518f4362f1.jpg", "img_caption": ["Figure 9: Synthetic samples from 2-rectified flow $^{\\mathrel{\\textstyle\\downarrow}}++$ on CIFAR-10 with NFE $=2$ (FID=2.76). "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "mSHs6C7Nfa/tmp/87134ca453b1df3afa7d84b61a786f0f47378c714afee807868add053ae482d0.jpg", "img_caption": ["Figure 10: Synthetic samples from 2-rectified flow $^{\\mathrel{\\textstyle\\downarrow}}++$ on CIFAR-10 with NFE $=4$ (FID=2.50). "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "mSHs6C7Nfa/tmp/33c6ad63a232a5a5cca976a2b50fdd8fbbaf41735bae7933bd34c7a517b4fe11.jpg", "img_caption": ["Figure 11: Synthetic samples from 2-rectified flow $^{\\mathrel{\\textstyle\\downarrow}}++$ on CIFAR-10 with NFE = 5 (FID=2.45). "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "mSHs6C7Nfa/tmp/fbf12e843abc6957752135ab47bdc0c1b9952142cb43bc2beccae9ee62f38d68.jpg", "img_caption": ["Figure 12: Synthetic samples from 2-rectified flow $^{++}$ on ImageNet $64\\!\\times\\!64$ with NFE $=1$ (FID=4.31). "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "mSHs6C7Nfa/tmp/f8e664df4624ecac20e5ad3a758c495d1217fe74031dc296bf1aeb0ec5587e0f.jpg", "img_caption": ["Figure 13: Synthetic samples from 2-rectified flow $^{++}$ on ImageNet $64\\!\\times\\!64$ with NFE $=2$ (FID=3.64). "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "mSHs6C7Nfa/tmp/ef943330762a2cdba11c1ec9294515173d69eb215c26ec5d63f5e32aab7622f4.jpg", "img_caption": ["Figure 14: Synthetic samples from 2-rectified flow $^{++}$ on ImageNet $64\\!\\times\\!64$ with NFE $=4$ $\\mathrm{FID}{=}3.44\\!\\!\\!$ . "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "mSHs6C7Nfa/tmp/15030ea15e3e9538715c10fb2ee52b11652c1c57854b290bec53fca843693a11.jpg", "img_caption": ["Figure 15: Synthetic samples from 2-rectified flow $^{++}$ on ImageNet $64\\!\\times\\!64$ with $\\mathrm{NFE}=8$ (FID=3.32). "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "mSHs6C7Nfa/tmp/94b13ef1d67b594e6f506969c3c29a46605cd6855d1529a91b7b4c842009dfe2.jpg", "img_caption": ["Figure 16: Synthetic samples from 2-rectified flow $^{++}$ on AFHQ $64\\!\\times\\!64$ with $\\mathrm{NFE}=1$ ( $\\mathrm{FID}{=}4.11$ ). "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "mSHs6C7Nfa/tmp/f59efd112a2193ce8e66cddaa94722c54eaa8adbe098968ebf045fe8c97b7cdc.jpg", "img_caption": ["Figure 17: Synthetic samples from 2-rectified flow $^{++}$ on AFHQ $64\\!\\times\\!64$ with NFE $=2$ (FID=3.12). "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "mSHs6C7Nfa/tmp/096eff062010748eae2d010fefcc081ca3e93b98de0ac90cce607cdcf924b6ff.jpg", "img_caption": ["Figure 18: Synthetic samples from 2-rectified flow $^{++}$ on AFHQ $64\\!\\times\\!64$ with NFE $=4$ ( $\\mathrm{FID}{=}2.90\\$ ). "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "mSHs6C7Nfa/tmp/f2ac9803be554daa88db750d218ff93ac537fbfea28bd3a4f164d777d4d0928f.jpg", "img_caption": ["Figure 19: Synthetic samples from 2-rectified flow $^{++}$ on AFHQ $64\\!\\times\\!64$ with NFE $=5$ ( $\\mathrm{FID}{=}2.86\\$ ). "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "mSHs6C7Nfa/tmp/b3437f57ab0bac62f01752db49c4ae87beff07b59f837d94414a46361c20a8bc.jpg", "img_caption": ["Figure 20: Synthetic samples from 2-rectified flow $^{\\mathrel{\\textstyle\\downarrow}}++$ on FFHQ $64\\!\\times\\!64$ with $\\mathrm{NFE}=1$ $\\mathrm{FID}{=}5.21$ ). "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "mSHs6C7Nfa/tmp/5fd42a73f8ee037da029198adcb119f2a69fd27a669c4c1a8035d6f8c44bfc86.jpg", "img_caption": ["Figure 21: Synthetic samples from 2-rectified flow $^{\\mathrel{\\textstyle\\downarrow}}++$ on FFHQ $64\\!\\times\\!64$ with $\\mathrm{NFE}=2$ $\\mathrm{FID}{=}4.26)$ ). "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The abstract and introduction reflect the paper\u2019s contributions and scope such as improved empirical performance, which is backed up by our experiments in Sec. 5. ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The limitations are discussed in the conclusion, such as the increased training time of our method. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Our only theoretical result is Proposition 1, for which we provide the proof in Appendix C.1. The argument in Sec. 3 is intuitive, and we do not prove it theoretically. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We provide our experimental details in Sec. 5 and App. E. For the new update algorithm, we provide full pseudocode in Sec. D, and we will release code publicly as soon as we obtain approval to do so. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [No] ", "page_idx": 26}, {"type": "text", "text": "Justification: The datasets we used are publicly available. We will release the code publicly as soon as we obtain internal approval. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We provide these details in Sec. E. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The shaded area in Figure 3 indicates the standard deviation. We only compute FID once due to cost constraints. ", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We provide details in Sec. E. ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper conforms with the NeurIPS Code of Ethics. ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We provide an impact statement in Sec. F. Effectively, as with other work on generative models, they can be used for both beneficial and harmful purposes. Our work, being focused on the mechanics of these models, does not introduce new risks, nor does it mitigate existing ones. ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: We believe that our work is not expected to have any more potential risks than other work in this field. We have discussed some of these considerations in Sec. F. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: For the datasets we use, we cite the original papers and describe the license information in Sec. E. ", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We provide necessary information to reproduce our new models in Sec. E. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 27}]