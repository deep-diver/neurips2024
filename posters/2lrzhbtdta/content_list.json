[{"type": "text", "text": "Not Just Object, But State: Compositional Incremental Learning without Forgetting ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yanyi Zhang 1, Binglin Qiu 1, Qi Jia 1, Yu Liu \u22171, Ran He 2 ", "page_idx": 0}, {"type": "text", "text": "1 International School of Information Science & Engineering, Dalian University of Technology 2 MAIS&CRIPAC, Institute of Automation, Chinese Academy of Sciences yanyi.zhang@mail.dlut.edu.cn, m1andy@mail.dlut.edu.cn jiaqi@dlut.edu.cn, liuyu8824@dlut.edu.cn, rhe@nlpr.ia.ac.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Most incremental learners excessively prioritize coarse classes of objects while neglecting various kinds of states (e.g. color and material) attached to the objects. As a result, they are limited in the ability to reason fine-grained compositionality of state-object pairs. To remedy this limitation, we propose a novel task called Compositional Incremental Learning (composition-IL), enabling the model to recognize state-object compositions as a whole in an incremental learning fashion. Since the lack of suitable benchmarks, we re-organize two existing datasets and make them tailored for composition-IL. Then, we propose a prompt-based Composition Incremental Learner (CompILer), to overcome the ambiguous composition boundary problem which challenges composition-IL largely. Specifically, we exploit multi-pool prompt learning, which is regularized by inter-pool prompt discrepancy and intra-pool prompt diversity. Besides, we devise object-injected state prompting by using object prompts to guide the selection of state prompts. Furthermore, we fuse the selected prompts by a generalized-mean strategy, to eliminate irrelevant information learned in the prompts. Extensive experiments on two datasets exhibit state-of-the-art performance achieved by CompILer. Code and datasets are available at: https://github.com/Yanyi-Zhang/CompILer. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Class Incremental Learning (class-IL) [37, 22, 16, 10] gathers increasing attention due to its ability to make the models learn new tasks rapidly, without forgetting previously acquired knowledge. Yet, traditional class-IL sets a strict limit on the old classes such that they should not recur in newly incoming tasks. To break such a strict limitation, recent studies develop a new setting mostly called Blurry Incremental Learning (blur-IL) [24, 11], where the incremental sessions allow the recurrence of previous classes, resulting in a more realistic and flexible scenario. Despite such empirical progresses on incremental learning, they aim to improve object classification only, while overlooking fine-grained states attached to the objects. For instance, analyzing how the clothing styles (akin to states) have changed over time is important for forecasting the future trends that will emerge. ", "page_idx": 0}, {"type": "text", "text": "To simultaneously model objects and their states, some efforts are dedicated to Compositional Learning whose aim is how to equip the models with compositionality [4, 31, 45]. The core of compositional learning lies in the structure of class labels, which conceptualizes a state-object pair (e.g. \u201cBrown Pants\u201d and \u201cYellow Dress\u201d) as a whole, rather than a lonely object label. In this way, the model can dissect and reassemble learned knowledge, achieving a more fine-grained understanding about the objects. However, existing works are mainly focused on zero-shot generalization from seen compositions to unseen ones [26, 27, 5, 15], whereas none of them consider the challenging fact that the model must deal with a significantly larger number of composition classes than object classes. As a result, it is hardly feasible to learn all compositions by training the model once. ", "page_idx": 0}, {"type": "image", "img_path": "2LRZhbTDtA/tmp/541908638447db22dc2b03a881937d5141eedf30c124d9ae5fcab88f679fa9f9.jpg", "img_caption": ["Figure 1: Differences between Class Incremental Learning (class-IL), Blurry Incremental Learning (blur-IL), and Compositional Incremental Learning (composition-IL). The object classes are not allowed to recur in the class-IL scenario, whereas they may recur randomly in the blur-IL scenario. Different from them, the classes in composition-IL involve state-object compositions apart from the object classes. Besides, the compositions do not reoccur, but the primitives (states or objects) may randomly reappear across incremental sessions. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To remedy the limitations inherent in incremental learning and compositional learning, we conceive a novel task named Compositional Incremental Learning (composition-IL), enabling the model to continually learn new state-object compositions in an incremental fashion. As compared in Fig. 1, we can see that composition-IL integrates the characteristics of class-IL and blur-IL. Although the composition classes are disjoint across incremental tasks, the primitive classes (i.e. objects and states) encountered in old tasks are allowed to reappear in new tasks. Unfortunately, existing incremental learning approaches are challenged by such a compositional scenario, because their models excessively prioritize the object primitives while neglecting the state primitives. Consequently, the compositions with the same object but with different states become ambiguous and indistinguishable. ", "page_idx": 1}, {"type": "text", "text": "To tackle the problem, we propose a rehearsal-free and prompt-based Compositional Incremental Learner (CompILer). Specifically, our model comprises of three primary components: multi-pool prompt learning, object-injected state prompting, and generalized-mean prompt fusion. Firstly, we construct three prompt pools for learning the states, objects and compositions individually. Upon that, we add extra restrictions to regularize the inter-pool prompt discrepancy and intra-pool prompt diversity. This multi-pool prompt learning paradigm strengthens the fine-grained understanding and reasoning towards primitive concepts and their compositions. In addition, as the state classes are more difficult to distinguish than the object ones, we propose object-injected state prompting which incorporates object prompts to guide the selection of state prompts. Furthermore, we fuse the selected prompts by a generalized-mean fusion manner, which helps to adaptively eliminate irrelevant information learned in the prompts. Last but not least, we also leverage symmetric cross-entropy loss to alleviate the impact of noisy data during training. ", "page_idx": 1}, {"type": "text", "text": "In summary, the main contributions in this work are encapsulated as follows: (1) We devise a new task coined compositional incremental learning (composition-IL). It enables learning fine-grained state-object compositions continually while the isolated primitive concepts can randomly recur in incremental tasks. (2) To address the lack of datasets, we re-organize two existing datasets such that they are tailored specifically for composition-IL. For the two new datasets (Split-Clothing and Split-UT-Zappos), we split them into 5 and 10 incremental tasks for evaluating the methods. (3) We propose a novel learning-to-prompt model for composition-IL, namely CompILer. Our stateof-the-art results on Split-Clothing and Split-UT-Zappos validate the effectiveness of CompILer for incrementally learning new compositions without forgetting old ones. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Incremental Learning. The approaches to addressing catastrophic forgetting for incremental learning can be broadly grouped into four categories: regularization based methods [6, 39] aim to protect influential weights of old experiences from updating; knowledge distillation based methods [16, 34] distill knowledge from the model trained on the previous tasks and adapt it to new tasks; rehearsal based methods [32, 48, 20] require a memory buffer to store some old data, so as to make the network remember previous tasks; parameter isolation methods allocates different model parameters to each task, to prevent any possible interference. Different from the methods, L2P [43] proposes an innovative learning-to-prompt paradigm, which incorporates plasticity and stability through adapting a set of learnable prompt tokens on top of a frozen pre-trained backbone. Inspired by L2P [43], more recent works [42, 36, 2, 28] take full advantage of various prompt tuning strategies, achieving new state-of-the-art performance for incremental learning. However, such methods take into account object classes solely, while neglecting various kinds of state classes associated with the objects. To this end, our work proposes compositional incremental learning with the purpose to continually identifying the composition classes of state-object pairs. Note that, Liao, et al [18] conduct an initial study toward the compositionality in incremental learning, whereas their attention is on the composition of multiple object classes (e.g. \u201cCar\u201d and \u201cPerson\u201d) in one image, rather than the state-object compositions in this work ", "page_idx": 1}, {"type": "image", "img_path": "2LRZhbTDtA/tmp/56788052035d941fdc9cd7e12f564698af615308d3800028062815278b7163be.jpg", "img_caption": ["Figure 2: Data Statistics of Split-Clothing and Split-UT-Zappos for tasking composition-IL. SplitClothing is divided into a 5-task scenario, while Split-UT-Zappos includes both 5-task and 10-task scenarios. In all settings, the number of images per task has been balanced properly. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Compositional Learning. A major line of compositional learning research focuses on Compositional Zero-Shot Learning (CZSL) [23], which aims to infer unseen state-object compositions by acquiring knowledge from seen ones. Subsequent approaches building upon the CZSL setting further incorporate graph neural networks to model the dependency between primitives and compositions [25], and employ cosine classifiers to avoid being overly biased toward seen compositions [21]. Other approaches [13, 14, 12] propose training two classifiers to identify states and objects separately. The latest works [19, 3, 38, 8, 47] model both composition and primitives simultaneously, achieving state-of-the-art results. Albeit the numerous attempts made in compositional learning, they fail to consider an incremental learning paradigm given the increasing number of composition classes in open-world scenarios. Besides, directly applying CZSL methods to composition-IL might lead to a stale and decaying performance on forgetting. By contrast, our proposed CompILer markedly bypasses catastrophic forgetting with the help of multi-pool prompt learning. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we firstly define the task of composition-IL, and then introduce two datasets we construct for the task, followed by revealing the ambiguous composition boundary problem. ", "page_idx": 2}, {"type": "text", "text": "3.1 Problem Definition ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "For composition-IL, a model sequentially learns $N$ tasks $\\mathcal{T}=\\{\\mathcal{T}_{1},\\mathcal{T}_{2},\\cdot\\cdot\\cdot\\mathcal{T}_{N}\\}$ corresponding to a set of composition classes $\\bar{c}=\\{\\mathcal{C}_{1},\\mathcal{C}_{2}\\overset{\\cdot}{,}\\cdot\\cdot\\mathcal{C}_{N}\\}$ . We note that the composition classes between incremental tasks are always disjoint, which means $\\mathcal{C}_{i}\\cap\\mathcal{C}_{j}=\\emptyset$ for any $i\\neq j$ . Different from the composition classes, the primitive classes are allowed to recur in different tasks. That means it allows the tasks to share some primitive concepts of objects and states. Therefore, we can define the set of all state and object classes with $S=\\{s_{1},s_{2},\\cdots,s_{n}\\}$ and $\\mathcal{O}=\\{o_{1},o_{2},\\cdot\\cdot\\cdot\\,,o_{m}\\}$ , respectively. Given each image $x$ , it has a composition label $c$ which is constructed with a state label $s$ and an object label $o$ , i.e. $c=<s,o>$ , where $c\\in{\\mathcal{C}}$ , $s\\in S$ and $o\\in{\\mathcal{O}}$ . We take the example of \u201cred shirt\u201d, where \u201cred\u201d is denoted with $s$ , \u201cshirt\u201d corresponds to $o$ , and \u201cred shirt\u201d is expressed with $c$ . ", "page_idx": 2}, {"type": "image", "img_path": "2LRZhbTDtA/tmp/5a40ddebca922365441a97a1dcf31a1c403fce2a4fe1390710961697180dfd2b.jpg", "img_caption": ["Figure 3: $t^{\\star}$ -SNE feature distributions of seven compositions from the Split-Clothing benchmark. For the compositions with the same object but with different states, our CompILer achieves more distinguishable boundaries than the L2P baseline. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.2 Dataset Construction ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As there are no existing datasets suitable for composition-IL, we re-organize the data in Clothing16K [46] and UT-Zappos50K [44], and construct two new datasets tailored for composition-IL, namely Split-Clothing and Split-UT-Zappos. To be more specific, we firstly sort the composition classes based on the number of their images, and then select the foremost 35 compositions from Clothing16K and the top 80 from UT-Zappos50K, so as to construct Split-Clothing and Split-UT-Zappos, respectively. In this way, Split-Clothing encompasses 9 states and 8 objects while Split-UT-Zappos consists of 15 states and 12 objects in total. For Split-Clothing, we randomly partition the compositions into 5 tasks. Regarding Split-UT-Zappos, the compositions are sorted by count and are evenly divided into 5 and 10 tasks. The image distribution for each task is shown in Fig. 2. Note that, we elaborate more details on both datasets in the following technical appendix. ", "page_idx": 3}, {"type": "text", "text": "3.3 Revealing the Ambiguous Composition Boundary ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The main stumbling block in composition-IL is the ambiguous composition boundary. Although the composition label consists of two primitives (i.e. object and state), we note that the model excessively prioritizes the object primitive while neglecting the state primitive. Consequently, the compositions with the same object but with different states become ambiguous and indistinguishable. To prove that, we apply L2P [43] to composition-IL, whereas it is challenged by significant ambiguities in composition classification. As illustrated in Fig. 3 (a), the $t$ -SNE visualization showcases the entanglement among the compositions like \u201cwhite dress\u201d, \u201cblack dress\u201d and \u201cblue dress\u201d. We conjecture that this ambiguous problem tends to become more severe when more tasks are arriving incrementally. To address it, we propose a new model namely CompILer, which disentangles compositions and primitives via a multi-pool prompt learning. Advantageously, our method promotes the learning on the states and establishes clearer composition boundaries, as shown in Fig. 3 (b). ", "page_idx": 3}, {"type": "text", "text": "4 Methodology ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Overview. We leverage the learning-to-prompt paradigm [43] and develop a novel compositional incremental learner (CompILer) tailored specifically for composition-IL. As depicted in Fig. 4, CompILer comprises three primary components: multi-pool prompt learning, object-guided state prompting, and generalized-mean prompt fusion. Firstly, we initialize three prompt pools dedicated to learning and storing visual information related to states, objects and their compositions. In order to differentiate the knowledge learned across and within prompt pools, we define inter-pool discrepant loss and intra-pool diversified loss jointly. We then employ object prompts to guide the selection of state prompts, thereby improving the state representation learning. Moreover, we utilize a generalizedmean fusion to integrate the selected prompts in a learnable manner. Ultimately, we optimize the classification objective with symmetric cross-entropy loss, to alleviate the effect of noisy data. ", "page_idx": 3}, {"type": "text", "text": "4.1 Multi-pool Prompt Learning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The learning-to-prompt paradigm [43, 35], especially suitable for large pre-trained backbones, has opened up a new path for incremental learning. It has proven to incorporate plasticity and stability better through adapting a set of learnable tokens in a prompt pool to a frozen pre-trained backbone. Nevertheless, existing prompt-based approaches are initially designed for class-IL, thereby building a single prompt pool for object classification solely. when dealing with state-object composition classification, they tend to excessively prioritize the object primitive while neglecting the state primitive. To this end, we propose to construct three discrepant and diversified prompt pools $\\mathbb{P}_{s}$ , $\\mathbb{P}_{o}$ and $\\mathbb{P}_{c}$ , which serve to learn visual information related to states, objects and their compositions, respectively. Besides, each pool is associated with a set of learnable keys $\\mathbb{K}_{\\omega}$ for query-key prompt selection. The three prompt pools and their keys are defined as: ", "page_idx": 3}, {"type": "image", "img_path": "2LRZhbTDtA/tmp/7f7819359dfc35bec6eafbfcc372d10235f24dfd3853ecc360b64d1e46e6a36e.jpg", "img_caption": ["Figure 4: Overall architecture of our composition incremental learner (CompILer), which comprises multi-pool prompt learning, object-injected state prompting, and generalized-mean prompt fusion. The multi-pool prompt learning mechanism captures information related to states, objects, and their compositions, each through a dedicated pool. The object-injected state prompting utilizes the object prompt to promote the state representation learning. Moreover, the generalized-mean prompt fusion is used to prioritize the useful prompts and diminish the irrelevant ones. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\omega}=\\left\\{P_{\\omega}^{1},P_{\\omega}^{2},\\cdot\\cdot\\cdot P_{\\omega}^{M}\\right\\},\\mathbb{K}_{\\omega}=\\left\\{K_{\\omega}^{1},K_{\\omega}^{2},\\cdot\\cdot\\cdot K_{\\omega}^{M}\\right\\},\\omega\\in\\left\\{s,o,c\\right\\},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $P_{\\omega}^{i}\\in\\mathbb{R}^{L\\times D}$ is a single prompt with token length $L$ and embedding dimension $D$ . $K_{\\omega}^{i}\\in\\mathbb{R}^{D}$   \nthe key of $P_{\\omega}^{i}$ , is a learnable token with the same size. $M$ is the number of prompts in each pool. ", "page_idx": 4}, {"type": "text", "text": "One important concern in such multi-pool prompt learning is how to enrich the prompts with the avoidance of identical pools. To achieve it, we consider integrating inter-pool prompt discrepancy and intra-pool prompt diversity jointly. On the one hand, the inter-pool prompts should be discrepant as the visual information about states, objects, and compositions should be different. One the other hand, within each pool, the intra-pool prompts should be diversified so to capture more comprehensive features from all the classes. ", "page_idx": 4}, {"type": "text", "text": "In practice, we formulate a unified objective to regularize both inter-pool discrepancy and intra-pool diversity, by leveraging a simple and effective directional decoupled loss used in [17]. The directional decoupled (dd) loss between any two pools (e.g. $P_{i}$ and $P_{j}$ ) is formulated as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{d d}^{(i,j)}=\\frac{2}{M(M-1)}\\sum_{n=1}^{M}\\sum_{m=1}^{M}\\operatorname*{max}\\left(0,\\theta_{\\mathrm{thre}}-\\theta_{n m}\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\theta_{n m}=\\cos^{-1}\\Big(\\frac{(P_{i}^{n})^{\\mathrm{T}}P_{j}^{m}}{\\operatorname*{max}(\\|P_{i}^{n}\\|_{2},\\epsilon)\\cdot\\operatorname*{max}(\\|P_{j}^{m}\\|_{2},\\epsilon)}\\Big),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\theta_{n m}$ measures the angle between any two prompts, $n$ and $m$ ; $\\epsilon$ is a scalar to avoid division by zero. Note that, $\\mathcal{L}_{d d}^{(i,j)}$ encourages the angles between each prompt to be at least $\\theta_{\\mathrm{thre}}$ degrees. Since $(i,j)$ is unordered Cartesian product of $\\omega$ , i.e. $(i,j)\\in\\{(i,j)\\mid i\\in\\omega\\wedge j\\in\\omega\\}$ , the inter-pool prompt idnitsrcar-eppoaonlc yp rloosms pfto rd itvheer tshitrye el opsoso lbse ccoanm bees $\\mathcal{L}_{i n t e r}=\\mathcal{L}_{d d}^{(s,o)}+\\mathcal{L}_{d d}^{(s,c)}+\\mathcal{L}_{d d}^{(o,c)}$ ,o $\\mathcal{L}_{i n t r a}=\\mathcal{L}_{d d}^{(s,s)}+\\mathcal{L}_{d d}^{(o,o)}+\\mathcal{L}_{d d}^{(c,c)}$ $\\mathcal{L}_{i n t e r}$ $\\mathcal{L}_{i n t r a}$ computes the angle between any two prompts within the same pool. Thus, it contains the case when $n=m$ , for which we set $\\theta_{\\mathrm{thre}}-\\theta_{n m}=0$ . ", "page_idx": 4}, {"type": "text", "text": "4.2 Object-injected State Prompting ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Akin to the query-key matching mechanism in other work [43, 42, 40], we utilize a fixed feature extractor $f(\\cdot)$ to obtain a query feature $q(x)=f(x)[0,:]$ , determining which prompts in the pool to be selected. However, pre-trained backbones are typically trained for object classification, thus under-performing for state representation learning. In addition, it is more difficult to predict the state classes due to their more abstract and fine-grained characteristics. To tackle this problem, we strategically inject object prompts to guide the selection of state prompts. Intuitively, once we have learned knowledge about the object class, it may be easier to predict the correct state class and avoid mistaken results. For instance, given an object is \u201cheels\u201d, we can expect that the corresponding ", "page_idx": 5}, {"type": "image", "img_path": "2LRZhbTDtA/tmp/6794b18361cc2148ff3fd64eeaae82d1682f284e09b7593fbca61a998ce9791e.jpg", "img_caption": ["Figure 5: Architecture of object-injected state prompting. Query feature serves as Q, while fused object prompt serves as both $\\mathbf{K}$ and ${\\mathrm{V}}.$ . "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "state is unlikely to be \u201ccanvas\u201d or \u201cplastic\u201d. To summarize, we select object and composition prompts in each pool based on the original query feature, which means $q_{o}(x)=q_{c}(x)=q(x)$ ; but for the selection of state prompts, we propose object-injected state prompting to ameliorate the query feature as shown in Fig. 5. ", "page_idx": 5}, {"type": "text", "text": "Specifically, we employ the fused object prompt $P_{o}$ (see Sec. 4.3) to perform cross attention on the query feature $q(x)$ , resulting in object-injected query feature $q_{s}(x)$ for the state prompt selection: ", "page_idx": 5}, {"type": "equation", "text": "$$\nq_{s}(x)=\\mathrm{CrossAtn}(q(x),P_{o})=\\mathrm{Softmax}(\\frac{q(x)W^{Q}\\cdot P_{o}W^{K}}{\\sqrt{D}})\\cdot P_{o}W^{V},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $W^{Q},W^{K}$ and $W^{V}$ are learnable projections. To establish alignment between the query and the selected prompts, we optimize a surrogate loss for state, object and composition prompting jointly: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{s u r}=\\sum_{\\omega}\\sum_{q_{\\omega}}\\cos(f_{\\omega}(x),K_{\\omega}^{s_{i}}),\\;\\omega\\in\\left\\{s,o,c\\right\\},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathrm{COS}(\\cdot,\\cdot)$ denotes cosine similarity, $\\mathbf{K}_{\\omega}$ represents the subset of top- $\\cdot\\mathbf{k}$ keys selected from $\\mathbb{K}_{\\omega}$ , and $\\left\\{s_{i}\\right\\}_{i=1}^{k}$ is a subset of top- $\\cdot\\mathbf{k}$ indices from $[1,M]$ (prompt number). Despite the simplicity of the object-injected state prompting, it facilitates more judicious prompt selection within the state prompt pool, alleviating the hurdles posed by state learning. ", "page_idx": 5}, {"type": "text", "text": "4.3 Generalized-mean Prompt Fusion ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "After obtaining the selected top- $\\cdot\\mathbf{k}$ prompts $\\{P_{\\omega}^{s_{i}}\\}_{i=1}^{k}$ , the next step is fusing these prompts into a single prompt. It is general to utilize a simple mean pooling whereas it overlooks the relative importance of each prompt. Besides, when the prompts contain information that is unrelated or contradictory to current task, it is critical to strengthen useful prompts and eliminate irrelevant ones. To this end, we draw inspiration from generalized-mean pooling [29] and exploit generalized-mean (GeM) prompt fusion which is given by: ", "page_idx": 5}, {"type": "equation", "text": "$$\nP_{\\omega}=\\mathbf{GeM}_{\\omega}(P_{\\omega}^{s_{1}},P_{\\omega}^{s_{2}},\\cdots,P_{\\omega}^{s_{k}})=\\left(\\frac{1}{k}\\sum_{i=1}^{k}P_{\\omega}^{s_{i}\\eta}\\right)^{\\frac{1}{\\eta}},\\omega\\in\\{s,o,c\\}\\,,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\eta$ is a learnable parameter. When $\\eta=1$ , GeM becomes mean pooling; as $\\eta$ approaches infinity $(\\eta\\rightarrow\\infty)$ , it converges to max pooling. By taking over mean and max pooling, GeM learns to achieve an optimal fusion, mitigating the influence of irrelevant information present in the prompts. ", "page_idx": 5}, {"type": "text", "text": "4.4 Training and Inference ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Classification Objective. We prepend three fused prompts (i.e. $P_{s}$ , $P_{o}$ and $P_{c,}$ ) with $x_{e}$ , which is the output from a ViT embedding layer $f_{e}(\\cdot)$ . The extended token sequence is $x_{p}=[P_{c};P_{s};P_{o};x_{e}]$ . ", "page_idx": 5}, {"type": "table", "img_path": "2LRZhbTDtA/tmp/4a33749ae69b2dba93d628947dd9403ee79a4e95e0a2eb1e0fb34ed34679a81c.jpg", "table_caption": ["Table 1: Avg Acc and FTT results on Split-Clothing (5 tasks) and Split-UT-Zappos (5 and 10 tasks). The best results are marked in bold. All results with standard deviations are averaged over three runs. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Then, we feed $x_{p}$ to a transformer encoder layer $f_{r}(\\cdot)$ and achieve $P_{s}^{r}$ , $P_{o}^{r}$ and $P_{c}^{r}$ for classifying state, object and composition classes, respectively. We estimate the probability via a classifier $\\bar{\\varphi_{\\omega}}\\bar{(}\\cdot\\bar{)}$ : $p(\\boldsymbol{\\omega}\\mid\\boldsymbol{x})=\\varphi_{\\omega}(\\boldsymbol{P}_{\\omega}^{r})$ . For each image $x$ , we denote its ground-truth distribution over labels with $q(\\omega\\mid x)$ . When $\\omega$ is consistent with the ground truth, then $q(\\boldsymbol{\\omega}\\mid\\boldsymbol{x})=1$ ; otherwise, $q(\\boldsymbol{\\omega}\\mid\\boldsymbol{x})=0$ As a result, the cross entropy (CE) loss used for classification objective is: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{C E}^{\\omega}=-\\sum_{\\omega=1}^{\\Omega}q(\\omega\\ensuremath{\\mathrm{~}}|\\boldsymbol{\\mathbf{\\omega}}\\boldsymbol{x})\\log p(\\omega\\ensuremath{\\mathrm{~}}|\\boldsymbol{\\mathbf{\\omega}}\\boldsymbol{x}),\\Omega\\in[|S|\\,,|\\mathcal{O}|\\,,|\\mathcal{C}|]\\,,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\Omega$ represents the number of classes. However, the model optimized with a standard CE loss is easily affected by noisy samples during training. Instead, we advocate using a symmetric cross entropy loss (SCE) [41], which incorporates an additional term called reverse cross entropy (RCE), to mitigate the impact of noisy data. Contrary to CE, the formula for RCE loss is defined as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{R C E}^{\\omega}=-\\sum_{\\omega=1}^{\\Omega}p(\\omega\\mid x)\\log q(\\omega\\mid x),\\omega\\in\\left\\{s,o,c\\right\\}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Then, the SCE loss combines two loss terms by $\\mathcal{L}_{S C E}^{\\omega}\\,=\\,\\mathcal{L}_{C E}^{\\omega}\\,+\\,\\alpha\\mathcal{L}_{R C E}^{\\omega}$ , where $\\alpha$ is a hyperparameter that controls the weight of the RCE term. As a result, the whole SCE loss becomes $\\mathcal{L}_{S C E}=$ $\\mathcal{L}_{S C E}^{c}+\\beta(\\mathcal{L}_{S C E}^{s}+\\mathcal{L}_{S C E}^{o})$ , where $\\beta$ adjusts the weights between primitives and compositions. ", "page_idx": 6}, {"type": "text", "text": "Total Loss. The total loss for training the whole CompILer model is: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{t o t a l}=\\lambda_{1}\\mathcal{L}_{i n t e r}+\\lambda_{2}\\mathcal{L}_{i n t r a}+\\lambda_{3}\\mathcal{L}_{s u r}+\\mathcal{L}_{S C E},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\lambda_{1},\\,\\lambda_{2},\\,\\lambda_{3}$ are hyper-parameters balancing different terms. ", "page_idx": 6}, {"type": "text", "text": "Inference. During inference, we incorporate the primitive probabilities to aid the composition probability. Hence, the final probability for composition classification is expressed with: ", "page_idx": 6}, {"type": "equation", "text": "$$\np_{\\mathrm{final}}(c\\mid x)=p(c\\mid x)+\\mu(p(s\\mid x)+p(o\\mid x)),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mu$ adjusts the probabilities. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1 Datasets and Metrics ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We conduct experiments on two newly split datasets: Split-Clothing and Split-UT-Zappos as elucidated in Section 3.2. We assess the overall performance on compositions using Average Accuracy (Avg Acc) and Forgetting (FTT). A higher Avg Acc signifies stronger recognition abilities, while a lower FTT indicates improved resilience against forgetting. Additionally, we provide individual Average Accuracy scores on states and objects, denoted as State and Object for simplicity. These metrics imply the ability to recognize fine-grained primitives. Furthermore, we calculate the Harmonic Mean (HM) between State and Object, i.e. $\\begin{array}{r}{H M\\stackrel{}{=}2\\times\\frac{(S t a t e\\times O b j e c t)}{(S t a t e+O b j e c t)}}\\end{array}$ . We provide more emphasis to Avg Acc and HM due to their more comprehensive assessment. Avg Acc encompasses the plasticity and stability [33, 2] and HM provides a holistic evaluation on both state and object. ", "page_idx": 6}, {"type": "text", "text": "5.2 Implementation Details ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "For a fair comparison with previous works [43, 42, 2, 33], we also employ ViT B/16 [1] pretrained on the ImageNet 1K dataset as the feature extractor and backbone. For multi-pool prompt learning, the size of each pool is set to 20, and each prompt has 5 tokens. We select top-5 prompts from each pool and generate a fused prompt. During training, we utilize the Adam optimizer [9] with a batch size of 16. The whole CompILer undergoes training for 25 epochs on the SplitClothing, for 10 epochs on the 5-task Split-UT-Zappos, and for 3 epochs on ", "page_idx": 7}, {"type": "table", "img_path": "2LRZhbTDtA/tmp/88f15f91035649245d4cfea1e6a9e4085ea58962ac948ff9e86ab7b11424ef07.jpg", "table_caption": ["Table 2: State, Object and HM results on Split-Clothing. The best results are marked in bold. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "the 10-task Split-UT-Zappos. For the Split-Clothing and the 10-task Split-UT-Zappos, we set the learning rate to 0.03, while we use a learning rate of 0.02 for the 5-task Split-UT-Zappos. Note that, for all the methods, their results are averaged over three runs with the corresponding standard deviations reported to mitigate the influence of random factors. ", "page_idx": 7}, {"type": "text", "text": "As there are a few hyper-parameters in the model, we conduct a rigorous tuning on them. For instance, we set $\\theta_{\\mathrm{thre}}$ to $\\scriptstyle{\\frac{\\pi}{2}}$ for all settings. For Split-Clothing, the loss weights $\\lambda_{1}$ and $\\lambda_{3}$ are set to 0.1; $\\lambda_{2}$ is set to $10^{-7}$ ; $\\alpha$ and $\\beta$ for SCE loss are 0.006 and 0.3, and the parameter $\\mu$ during inference is 0.5. For 5-task Split-UT-Zappos, $\\lambda_{1}$ , $\\lambda_{2}$ , $\\lambda_{3}$ , $\\alpha$ , $\\beta$ and $\\mu$ are set to 1.0, $3\\times10^{-6}$ , 0.7, 0.01, 0.7 and 0.02, respectively. For 10-task Split-UT-Zappos, $\\lambda_{1}$ , $\\lambda_{2}$ , $\\lambda_{3}$ , $\\alpha$ , $\\beta$ and $\\mu$ are set to 0.5, $10^{-7}$ , 0.1, 0.05, 0.4 and 0.03. We elaborate more details on hyper-parameter analysis in the appendix. ", "page_idx": 7}, {"type": "text", "text": "5.3 Compared Baselines ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To demonstrate the effectiveness of the proposed method, we compare CompILer with state-of-the-art incremental learning methods, including prompt-free approaches [10, 16, 32] and prompt-based methods [43, 42, 33, 7]. All the methods are rehearsal-free except iCaRL [32]. Note that, due to LGCL [7] relying on CLIP [30] to achieve language guidance at the task level, it is limited by the length of class names per task. Thereby, LGCL fails to operate on the 5-task Split-UT-Zappos since the total length of class names exceeds the limitation. ", "page_idx": 7}, {"type": "text", "text": "To streamline our CompILer, we further implement a simplified version named Sim-CompILer and report its results. Sim-CompILer is optimized using cross entropy loss and is comprised solely of multi-pool prompt learning and generalized-mean prompt fusion. In other words, we exclude the object-injected prompting, directional decoupled loss, and reverse cross entropy loss, resulting in a large reduction of hyperparameters to only $\\beta$ , $\\lambda_{3}$ , and $\\mu$ . ", "page_idx": 7}, {"type": "text", "text": "5.4 Comparison with the State-of-the-arts ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The compared results on Avg Acc and FTT are reported in Table 1. Overall, CompILer consistently outperforms all competitors on Avg Acc by a significant margin. For FTT scores, CompILer excels previous methods with $0.32\\%$ on the 5-task Split-Clothing and with $0.14\\%$ on the 5-task Split-UTZappos, while falling behind Dual-Prompt [42] and LGCL [7] for the 10-task Split-UT-Zappos. We notice that, the main reason is these methods sacrifice more plasticity for lower forgetting rates. Besides, the number of model parameters in these methods dynamically increases along with more incremental tasks arriving, whereas our CompILer does not rely on imposing task-specific parameters to reduce the forgetting. ", "page_idx": 7}, {"type": "text", "text": "We also report the primitives accuracy and their HM in Table 2 and Table 3. Likewise, our method surpasses other methods considerably in terms of State and HM. Interestingly, the prompt-free methods [16, 10, 32] achieve higher accuracy in state prediction than object prediction for SplitClothing, which is contrary to other results. This is because the states in Split-Clothing are colorrelated descriptions, which are easier to capture with the help of parameter fine-tuning. The promptbased methods do not exhibit this phenomenon because their pre-trained backbones are initially trained for object classification, and are frozen across incremental sessions. As the performance improvements are mainly attributed to the accuracy of state recognition, it suggests that our model enhances the understanding on fine-grained compositionality. ", "page_idx": 7}, {"type": "table", "img_path": "2LRZhbTDtA/tmp/65a9b0b490c5054c57bb246ee4e298ac186d64a94511ab7deef246c62358212d.jpg", "table_caption": ["Table 3: State, Object and HM results on Split-UT-Zappos (5 tasks) and Split-UT-Zappos (10 tasks). "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "2LRZhbTDtA/tmp/b5aec575a7b470af6824e98531798c90a18cbdfe39d8b7367da84036e7808fe3.jpg", "table_caption": ["Table 5: Ablative experiments for (a) object-injected state prompting, (b) prompt fusion method. (a) Object-injected state prompting. (b) Prompt fusion method. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "5.5 Ablation Study and Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Effect of multi-pool prompt learning. This experiment aims to delineate the contribution of three pools in CompILer. We firstly implement a baseline model with composition prompt pool only. Building upon the baseline, we develop two additional models, which incorporate either object or state prompt pool. As reported in Table 4, the inclusion of primitive prompt pool yields ", "page_idx": 8}, {"type": "table", "img_path": "2LRZhbTDtA/tmp/6fab58a43f1d529a79d9ebb3f12231766f147c74b87dbc15ed450e1efd18d3dc.jpg", "table_caption": ["Table 4: Ablation study on multi-pool prompt learning with Split-Clothing dataset. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "consistent gains over the baseline. Furthermore, the best results are achieved when the model integrates all three pools simultaneously. This experiment signifies the significant necessity of exploiting multiple prompt pools for composition-IL. ", "page_idx": 8}, {"type": "text", "text": "Effect of object-injected state prompting. To provide insights into object-injected state prompting, we compare three models: None (vanilla model), ${\\mathrm{S}}{\\rightarrow}{\\mathrm{O}}$ (state-injected object prompting) and $\\mathrm{O}{\\rightarrow}\\mathbf{S}$ (object-injected state prompting). As shown in Table 5a, compared to the None model, the $s{\\rightarrow}0$ exhibits a decrease in all metrics, implying that state prompts may interfere with the selection of object prompts. On the contrary, $\\mathrm{O}{\\rightarrow}\\mathrm{S}$ outperforms the None model as we expect. This phenomenon validates our motivation that state recognition is harder than object recognition, and thereby the former cannot help the latter easily. Yet, it is a promising direction for future research. ", "page_idx": 8}, {"type": "text", "text": "Effect of generalized-mean prompt fusion. This study aims to study the impact of prompt fusion on CompILer. As shown in Table 5b, GeM performs better than both max and mean pooling across various metrics. It validates the benefit of GeM on mitigating irrelevant information in the selected prompts. as it may hamper the model\u2019s attention on image tokens. ", "page_idx": 8}, {"type": "text", "text": "Effect of loss functions. As shown in Table 6, we investigate the influence of loss functions used in our model, including directional decoupled loss $\\mathcal{L}_{i n t e r}$ and $\\mathcal{L}_{i n t r a}$ ) and symmetric cross entropy loss $\\mathcal{L}_{C E}$ and $\\mathcal{L}_{R C E}$ ). The baseline model (the first row) includes all modules but is trained by cross entropy loss only. By adding the RCE loss, the model is equivalent to training with the SCE loss, which help to improve the robustness to noisy labels. The use of either $\\mathcal{L}_{i n t e r}$ or $\\mathcal{L}_{i n t r a}$ improves the performance on both datasets, and synchronously applying them witnesses all-around improvements compared to the baseline. Eventually, we achieve the best results when combing all the loss terms during training. ", "page_idx": 8}, {"type": "table", "img_path": "", "table_caption": ["Table 6: Ablate the loss functions on Split-Clothing and Split-UT-Zappos. "], "table_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "2LRZhbTDtA/tmp/79f489025e3f4367af2706d45864b31bd344f9890535bbd3154cc8394168fe0a.jpg", "img_caption": ["Figure 6: Results and analysis. (a) to (c) show accuracy of CompILer on composition, state, and object for each task in Split-Clothing. The $\\mathbf{X}$ -axis represents the test stream, and the y-axis denotes the status after training the $T_{k}$ task. Darker background color indicates higher accuracy. (d) displays some images and their predictions: top row is GT, middle row is CompILer prediction, and bottom row is L2P [43] prediction. Green indicates correct predictions, while red indicates incorrect predictions. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5.6 Additional Results and Analysis ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In order to study the repeatability characteristic in composition-IL, we exhibit more results on Split-Clothing in Fig. 6: in (a), it shows a decreasing trend in composition accuracy along with the introduction of new tasks; however, the green rectangles in (b) and (c) showcase that the accuracy occasionally increases as more tasks are learned. We conjecture the reason is mostly attributed to the re-occurrence of primitive concepts. This forward transfer is critical for incremental learners. We compare the composition predictions between CompILer and L2P [43] in Fig. 6 (d). CompILer predicts all the images correctly, while L2P makes some mistakes, particularly for state labels. This limitation arises from an excessive focus on the dominant object primitive, while weakening the attention toward state primitive. Fortunately, CompILer relieves the bias toward object classes, and enhances the perception on state classes. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we have proposed a novel task coined compositional incremental learning (compostionIL), which is stumbled by ambiguous composition boundary. To tackle it, we develop a learningto-prompt model, namely CompILer. Our model exploits multi-pool prompt learning to model composition and primitive concepts, object-injected state prompting to improve the selection of state prompts, and generalized-mean prompt fusion to eliminate irrelevant information. Extensive experiments on two tailored datasets show that CompILer achieves state-of-the-art performance. In the future, it is challenging yet potential to consider reasoning multiple state classes per object. ", "page_idx": 9}, {"type": "text", "text": "7 Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported in part by the National Natural Science Foundation of China under Grant Numbers 62102061, 62272083 and 62472066, and in part by the Open Projects Program of State Key Laboratory of Multimodal Artificial Intelligence Systems. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2020.   \n[2] Zhanxin Gao, Jun Cen, and Xiaobin Chang. Consistent prompting for rehearsal-free continual learning. In IEEE Conf. Comput. Vis. Pattern Recog., 2024. [3] Shaozhe Hao, Kai Han, and Kwan-Yee K Wong. Learning attention as disentangler for compositional zero-shot learning. In IEEE Conf. Comput. Vis. Pattern Recog., pages 15315\u2013 15324, 2023. [4] Geoffrey Hinton. Some demonstrations of the effects of structural descriptions in mental imagery. Cognitive Science, 3(3):231\u2013250, 1979. [5] Chenchen Jing, Yukun Li, Hao Chen, and Chunhua Shen. Retrieval-augmented primitive representations for compositional zero-shot learning. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 2652\u20132660, 2024. [6] Sangwon Jung, Hongjoon Ahn, Sungmin Cha, and Taesup Moon. Continual learning with node-importance based adaptive group sparse regularization. Adv. Neural Inform. Process. Syst., 33:3647\u20133658, 2020. [7] Muhammad Gul Zain Ali Khan, Muhammad Ferjad Naeem, Luc Van Gool, Didier Stricker, Federico Tombari, and Muhammad Zeshan Afzal. Introducing language guidance in promptbased continual learning. In Int. Conf. Comput. Vis., pages 11429\u201311439, 2023.   \n[8] Hanjae Kim, Jiyoung Lee, Seongheon Park, and Kwanghoon Sohn. Hierarchical visual primitive experts for compositional zero-shot learning. In Int. Conf. Comput. Vis., pages 5675\u20135685, 2023.   \n[9] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[10] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521\u20133526, 2017.   \n[11] Hyunseo Koh, Dahyun Kim, Jung-Woo Ha, and Jonghyun Choi. Online continual learning on class incremental blurry task configuration with anytime inference. In International Conference on Learning Representations, 2022.   \n[12] Xiangyu Li, Xu Yang, Kun Wei, Cheng Deng, and Muli Yang. Siamese contrastive embedding network for compositional zero-shot learning. In IEEE Conf. Comput. Vis. Pattern Recog., pages 9326\u20139335, 2022.   \n[13] Yong-Lu Li, Yue Xu, Xiaohan Mao, and Cewu Lu. Symmetry and group in attribute-object compositions. In IEEE Conf. Comput. Vis. Pattern Recog., pages 11316\u201311325, 2020.   \n[14] Yong-Lu Li, Yue Xu, Xinyu Xu, Xiaohan Mao, and Cewu Lu. Learning single/multi-attribute of object with symmetry and group. IEEE Trans. Pattern Anal. Mach. Intell., 44(12):9043\u20139055, 2021.   \n[15] Yun Li, Zhe Liu, Hang Chen, and Lina Yao. Context-based and diversity-driven specificity in compositional zero-shot learning. In IEEE Conf. Comput. Vis. Pattern Recog., 2024.   \n[16] Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE Trans. Pattern Anal. Mach. Intell., 40(12):2935\u20132947, 2017.   \n[17] Zilong Li, Yiming Lei, Chenglong Ma, Junping Zhang, and Hongming Shan. Prompt-in-prompt learning for universal image restoration. arXiv preprint arXiv:2312.05038, 2023.   \n[18] Weiduo Liao, Ying Wei, Mingchen Jiang, Qingfu Zhang, and Hisao Ishibuchi. Does continual learning meet compositionality? new benchmarks and an evaluation framework. Adv. Neural Inform. Process. Syst., 2023.   \n[19] Yu Liu, Jianghao Li, Yanyi Zhang, Qi Jia, Weimin Wang, Nan Pu, and Nicu Sebe. Pmgnet: Disentanglement and entanglement benefit mutually for compositional zero-shot learning. Computer Vision and Image Understanding, 2024.   \n[20] David Lopez-Paz and Marc\u2019Aurelio Ranzato. Gradient episodic memory for continual learning. Adv. Neural Inform. Process. Syst., 30, 2017.   \n[21] Massimiliano Mancini, Muhammad Ferjad Naeem, Yongqin Xian, and Zeynep Akata. Open world compositional zero-shot learning. In IEEE Conf. Comput. Vis. Pattern Recog., pages 5222\u20135230, 2021.   \n[22] Marc Masana, Xialei Liu, Bart\u0142omiej Twardowski, Mikel Menta, Andrew D Bagdanov, and Joost Van De Weijer. Class-incremental learning: survey and performance evaluation on image classification. IEEE Trans. Pattern Anal. Mach. Intell., 45(5):5513\u20135533, 2022.   \n[23] Ishan Misra, Abhinav Gupta, and Martial Hebert. From red wine to red tomato: Composition with context. In IEEE Conf. Comput. Vis. Pattern Recog., pages 1792\u20131801, 2017.   \n[24] Jun-Yeong Moon, Keon-Hee Park, Jung Uk Kim, and Gyeong-Moon Park. Online class incremental learning on stochastic blurry task boundary via mask and visual prompt tuning. In IEEE Conf. Comput. Vis. Pattern Recog., pages 11731\u201311741, 2023.   \n[25] Muhammad Ferjad Naeem, Yongqin Xian, Federico Tombari, and Zeynep Akata. Learning graph embeddings for compositional zero-shot learning. In IEEE Conf. Comput. Vis. Pattern Recog., pages 953\u2013962, 2021.   \n[26] Tushar Nagarajan and Kristen Grauman. Attributes as operators: factorizing unseen attributeobject compositions. In Eur. Conf. Comput. Vis., pages 169\u2013185, 2018.   \n[27] Senthil Purushwalkam, Maximilian Nickel, Abhinav Gupta, and Marc\u2019Aurelio Ranzato. Taskdriven modular networks for zero-shot compositional learning. In Int. Conf. Comput. Vis., pages 3593\u20133602, 2019.   \n[28] Jingyang Qiao, Xin Tan, Chengwei Chen, Yanyun Qu, Yong Peng, Yuan Xie, et al. Prompt gradient projection for continual learning. In The Twelfth International Conference on Learning Representations, 2023.   \n[29] Filip Radenovi\u00b4c, Giorgos Tolias, and Ond\u02c7rej Chum. Fine-tuning cnn image retrieval with no human annotation. IEEE transactions on pattern analysis and machine intelligence, 41(7):1655\u2013 1668, 2018.   \n[30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748\u20138763, 2021.   \n[31] Pasko Rakic, Jean-Pierre Bourgeois, and Patricia S Goldman-Rakic. The self-organizing brain: from growth cones to functional networks. Elsevier, 1994.   \n[32] Sylvestre-Alvise Rebuff,i Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl: Incremental classifier and representation learning. In IEEE Conf. Comput. Vis. Pattern Recog., pages 2001\u20132010, 2017.   \n[33] James Seale Smith, Leonid Karlinsky, Vyshnavi Gutta, Paola Cascante-Bonilla, Donghyun Kim, Assaf Arbelle, Rameswar Panda, Rog\u00e9rio Feris, and Zsolt Kira. Coda-prompt: Continual decomposed attention-based prompting for rehearsal-free continual learning. In IEEE Conf. Comput. Vis. Pattern Recog., pages 11909\u201311919, 2023.   \n[34] Filip Szatkowski, Mateusz Pyla, Marcin Przewie\u02dbz\u00b4likowski, Sebastian Cygert, Bart\u0142omiej Twardowski, and Tomasz Trzci\u00b4nski. Adapt your teacher: Improving knowledge distillation for exemplar-free continual learning. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 1977\u20131987, 2024.   \n[35] Yu-Ming Tang, Yi-Xing Peng, and Wei-Shi Zheng. When prompt-based incremental learning does not meet strong pretraining. In IEEE Conf. Comput. Vis. Pattern Recog., pages 1706\u20131716, 2023.   \n[36] Liyuan Wang, Jingyi Xie, Xingxing Zhang, Mingyi Huang, Hang Su, and Jun Zhu. Hierarchical decomposition of prompt-based continual learning: Rethinking obscured sub-optimality. Adv. Neural Inform. Process. Syst., 36, 2024.   \n[37] Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu. A comprehensive survey of continual learning: Theory, method and application. IEEE Trans. Pattern Anal. Mach. Intell., 2024.   \n[38] Qingsheng Wang, Lingqiao Liu, Chenchen Jing, Hao Chen, Guoqiang Liang, Peng Wang, and Chunhua Shen. Learning conditional attributes for compositional zero-shot learning. In IEEE Conf. Comput. Vis. Pattern Recog., pages 11197\u201311206, 2023.   \n[39] Wenjin Wang, Yunqing Hu, Qianglong Chen, and Yin Zhang. Task difficulty aware parameter allocation & regularization for lifelong learning. In IEEE Conf. Comput. Vis. Pattern Recog., pages 7776\u20137785, 2023.   \n[40] Yabin Wang, Zhiwu Huang, and Xiaopeng Hong. S-prompts learning with pre-trained transformers: An occam\u2019s razor for domain incremental learning. Adv. Neural Inform. Process. Syst., 35:5682\u20135695, 2022.   \n[41] Yisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jinfeng Yi, and James Bailey. Symmetric cross entropy for robust learning with noisy labels. In Int. Conf. Comput. Vis., pages 322\u2013330, 2019.   \n[42] Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun, Han Zhang, Chen-Yu Lee, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer G. Dy, and Tomas Pfister. Dualprompt: Complementary prompting for rehearsal-free continual learning. In Eur. Conf. Comput. Vis., pages 631\u2013648, 2022.   \n[43] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer G. Dy, and Tomas Pfister. Learning to prompt for continual learning. In IEEE Conf. Comput. Vis. Pattern Recog., pages 139\u2013149, 2022.   \n[44] Aron Yu and Kristen Grauman. Fine-grained visual comparisons with local learning. In IEEE Conf. Comput. Vis. Pattern Recog., pages 192\u2013199, 2014.   \n[45] Alan L Yuille and Chenxi Liu. Deep nets: What have they ever done for vision? Int. J. Comput. Vis., 129(3):781\u2013802, 2021.   \n[46] Tian Zhang, Kongming Liang, Ruoyi Du, Xian Sun, Zhanyu Ma, and Jun Guo. Learning invariant visual representations for compositional zero-shot learning. In Eur. Conf. Comput. Vis., pages 339\u2013355, 2022.   \n[47] Yanyi Zhang, Qi Jia, Xin Fan, Yu Liu, and Ran He. Cscnet: Class-specified cascaded network for compositional zero-shot learning. In IEEE International Conference on Acoustics, Speech and Signal Processing, pages 3705\u20133709, 2024.   \n[48] Yaqian Zhang, Bernhard Pfahringer, Eibe Frank, Albert Bifet, Nick Jin Sean Lim, and Yunzhe Jia. A simple but strong baseline for online continual learning: Repeated augmented rehearsal. Adv. Neural Inform. Process. Syst., 35:14771\u201314783, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In addition to the content in the main paper, this appendix elaborates more details on the datasets, algorithm procedure, empirical analysis, hyper-parameter analysis, and quantitative and qualitative experiments. Meanwhile, please refer to the source code and some data samples included in the supplementary material we submit. ", "page_idx": 13}, {"type": "image", "img_path": "2LRZhbTDtA/tmp/53545ea3f8fe0612186a1ac856c5e3f3f56e2fb5aff0b4c8014e775b9164c56c.jpg", "img_caption": ["Figure 7: Number of images per task. The horizontal axis indicates the task ID, while the vertical axis represents the corresponding number of images. The colors pink and blue denote training and testing datasets, respectively. Overall, the data distributions are relatively balanced across tasks. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "A.1 More on Dataset Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We provide more information about two newly tailored datasets (Split-Clothing and Split-UT-Zappos) and extensively discuss the limitations of Split-UT-Zappos. The number of images in each task is shown in Fig. 7. ", "page_idx": 13}, {"type": "text", "text": "Split-Clothing is derived from Clothing16K, which is originally used for multi-label classification tasks and consists of 37 state-object compositions and 16,170 images scraped from Google, Bing, and DuckDuckGo. We sorted these compositions by the number of images and selected the top 35 compositions, resulting in a total of 15,915 images in the Split-Clothing dataset. Specifically, the object in Split-Clothing includes various types of garments such as \u201cdresses\u201d and \u201choodies\u201d, while the state delineates the color of the clothing, such as \u201csilver\u201d and \u201cyellow\u201d. We allocate $80\\%$ of the dataset for training and the remaining $20\\%$ for testing. We split this dataset for 5 incremental tasks. ", "page_idx": 13}, {"type": "text", "text": "Split-UT-Zappos is a subset of UT-Zappos50K, which is a large shoe dataset consisting of 50,025 catalog images collected from Zappos.com. This dataset is originally created for an online shopping task and we selected a total of 28,497 images to comprise the Split-UT-Zappos. The state classes in Split-UT-Zappos describe different materials (e.g. canvas and leather) and the object classes are related to footwear (e.g. heels and slippers). Consistent with Split-Clothing, we employ $80\\%$ of the images for training and $20\\%$ for testing. We split this dataset for either 5 or 10 incremental tasks. ", "page_idx": 13}, {"type": "text", "text": "Limitations of Split-UT-Zappos. It is noteworthy that, even for the upper bound model (i.e. supervised learning with all the data), satisfactory performance remains elusive on Split-UT-Zappos as shown in Table 1 and Table 3. This deficiency can be attributed to the pronounced long-tail distributions and invisible states inherent in UT-Zappos50K. For example, the composition \u201cFaux Fur Slippers\u201d comprises a mere 25 training images, whereas \u201cLeather Sandals\u201d accounts for 1783. This disproportionate distribution predisposes the severe bias towards the compositions prevalent in the head of the distributions, while inadequately capturing those in the tail. Besides, some states in Split-UT-Zappos lik,e \u201cLeather\u201d vs \u201cSynthetic Leather\u201d, are material differences that are not always visible as visual transformations. The deficient state description poses a greater challenge for classification, thereby resulting in suboptimal upper bound results. We contend that tackling the long-tail distribution challenge within Split-UT-Zappos and constructing more balanced datasets conducive to composition-IL represent promising avenues for future research. ", "page_idx": 13}, {"type": "text", "text": "A.2 More on Training Procedure ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We summarize the training procedure of the proposed CompILer in Algorithm 1. ", "page_idx": 13}, {"type": "text", "text": "1 Input: Training data for $T$ tasks, where each image $x$ has a set of three labels: $c$ , $s$ and $o$ . Three prompt pools $\\mathbb{P}_{\\omega}$ with corresponding keys $\\mathbb{K}_{\\omega}$ , where $\\omega\\in\\{s,o,c\\}$ . Pre-trianed feature extractor $f(\\cdot)$ , pre-trained input embedding layer $f_{e}(\\cdot)$ , pre-trained transformer encoder layer $f_{r}(\\cdot)$ , cross attention layer CrossAttn $(\\cdot,\\cdot)$ , classifiers $\\varphi_{\\omega}(\\cdot)$ , GeM fusion $\\mathrm{GeM}_{\\omega}(\\cdot\\cdot\\cdot)$ . ", "page_idx": 14}, {"type": "text", "text": "2 Initialize: $\\mathbb{P}_{\\omega}$ , $\\mathbb{K}_{\\omega}$ , CrossAttn $(\\cdot,\\cdot)$ , $\\varphi_{\\omega}(\\cdot)$ , $\\mathrm{GeM}_{\\omega}(\\cdot\\cdot\\cdot)$   \n3 for $t=1,\\cdot\\cdot\\cdot,T$ do   \n4 for each sample in the batch do   \n5 Estimate the prompt-specific loss $\\mathcal{L}_{p}=\\lambda_{1}\\mathcal{L}_{i n t e r}+\\lambda_{2}\\mathcal{L}_{i n t r a}$ ;   \n6 Calculate query feature of object and composition $q_{o}(x)=q_{c}(x)=f(x)[0,:]$ ;   \n7 Lookup top- $\\cdot\\mathbf{k}$ object keys $\\mathbf{K}_{o}$ and composition keys $\\mathbf{K}_{c}$ ;   \n8 Select top- $\\cdot\\mathbf{k}$ prompts associated with the keys in $\\mathbb{P}_{o}$ and $\\mathbb{P}_{c}$ ;   \n9 Fuse the selected prompts by $P_{o}=\\mathbf{GeM}_{o}(P_{o}^{s_{1}},P_{o}^{s_{2}},\\cdot\\cdot\\cdot\\,,P_{o}^{s_{k}})$ and   \n$P_{c}=\\mathbf{G}\\mathbf{e}\\mathbf{M}_{c}(P_{c}^{s_{1}},P_{c}^{s_{2}},\\cdot\\cdot\\cdot,P_{c}^{s_{k}})$ ;   \n10 Perform object-injected state prompting by: $q_{s}(x)=\\mathrm{CrossAttn}(f(x)[0,:],P_{o});$   \n11 Lookup top-k state keys $\\mathbf{K}_{s}$ ;   \n12 Select top- $\\cdot\\mathbf{k}$ state prompts associated with the keys in $\\mathbb{P}_{s}$ ;   \n13 Fuse the selected state prompts by $P_{s}=\\mathrm{GeM}_{s}(P_{s}^{s_{1}},P_{s}^{s_{2}},\\cdot\\cdot\\cdot,P_{s}^{s_{k}})$ ;   \n14 Calculate the input embedding sequence $x_{e}=f_{e}(x)$ ;   \n15 Prepending $x_{e}$ with fused prompts by $x_{p}=[P_{c};P_{s};P_{o};x_{e}]$ ;   \n16 Feed $x_{p}$ to a transformer encoder layer $f_{r}(\\cdot)$ and achieve $P_{s}^{r}$ , $P_{o}^{r}$ and $P_{c}^{r}$   \n17 Calculate the probability by $p(\\boldsymbol{\\omega}\\mid\\boldsymbol{x})=\\varphi_{\\omega}(\\boldsymbol{P}_{\\omega}^{r})$ ;   \n18 Estimate per sample loss $\\mathcal{L}_{x}=\\mathcal{L}_{S C E}+\\lambda_{3}\\mathcal{L}_{s u r}$ ;   \n19 end   \n20 Calculate per batch loss $\\mathcal{L}_{B}$ by accumulating $(\\mathcal{L}_{x}+\\mathcal{L}_{p})$ .   \n21 end   \n22 Output: rllondito ters $\\Phi$ ", "page_idx": 14}, {"type": "text", "text": "A.3 Empirical Analysis on Learned Prompts ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In Section 4.1, we design multi-pool prompt learning to learn visual representations of state, object, and composition, respectively. By applying inter-pool discrepant loss and intra-pool diversified loss, we ensure discrepancy across pools and diversity within each pool. To validate the effectiveness of the approach, we conduct $t^{\\star}$ -SNE visualization on the prompt pools within the three experiment settings. Figure 8 illustrates that the learned knowledge across the three pools is remarkably discriminative, suggesting that each pool has effectively captured unique features. Meanwhile, the prompts within each pool exhibit a relatively wide dispersion demonstrating that the learned information within each pool is diversified. ", "page_idx": 14}, {"type": "text", "text": "A.4 Empirical Analysis of Generalized-mean Prompt Fusion ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The detailed operation of Generalized-mean Prompt Fusion is elaborated on Section 4.3. The parameter $\\eta$ in Eq. 6 can be learnable as this operation is differentiable, allowing it to be included in the whole back-propagation process. To prove that, the corresponding derivatives of Eq. 6 are given by: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial P_{\\omega}}{\\partial P_{\\omega}^{s_{i}}}=\\frac{1}{k}P_{\\omega}^{\\mathrm{\\tiny~1}-\\eta}P_{\\omega}^{s_{i}\\eta-1},}\\\\ &{\\frac{\\partial P_{\\omega}}{\\partial\\eta}=\\frac{P_{\\omega}}{\\eta^{2}}\\left(\\log\\frac{k}{\\sum_{i=1}^{k}P_{\\omega}^{s_{i}\\eta}}+\\eta\\frac{\\sum_{i=1}^{k}P_{\\omega}^{s_{i}\\eta}\\log P_{\\omega}^{s_{i}}}{\\sum_{i=1}^{k}P_{\\omega}^{s_{i}\\eta}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A.5 Empirical Analysis of Symmetric Cross Entropy Loss ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we delve into why symmetric cross entropy (SCE) loss can effectively mitigate noisy data in the dataset. We define two distributions, $q$ and $p$ , where $q=(k\\mid x)$ represents the ground truth class distribution for sample $x$ and $p=(k\\mid x)$ represents the predicted class distribution. The ", "page_idx": 14}, {"type": "image", "img_path": "2LRZhbTDtA/tmp/53a2562b2eab69ad49a6ac5ce5ddcffd63d736e12e88096de44870be0c3e5c58.jpg", "img_caption": ["Figure 8: The $t^{\\prime}$ -SNE visualization of learned prompts on Split-Clothing (5 tasks), Split-UT-Zappos(5 tasks), and Split-UT-Zappos (10 tasks). The composition prompts are colored in yellow, the state prompts in green, and the object prompts in blue. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "2LRZhbTDtA/tmp/e357e0f7961fc5f2bfffd15ed1c8fd75e72ef3df0fc7a3e78e867ab21455ce21.jpg", "img_caption": ["Figure 9: Impact of hyper-parameters on average accuracy in Split-UT-Zappos (5 tasks). "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "relation between the cross entropy $H(q,p)$ and KL-divergence $K L(q\\parallel p)$ is expressed as: ", "page_idx": 15}, {"type": "equation", "text": "$$\nK L(q\\parallel p)=H(q,p)-H(q),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "in which $H(q)$ denotes the entropy of $q$ . From the perspective of KL divergence, the optimization objective is to make the prediction distribution $p$ closely resemble the ground truth distribution $q$ , essentially minimizing their KL divergence. However, in the presence of noise within the dataset, $q$ may not accurately reflect the ground-truth class distribution, whereas $p$ might reveal the real distribution. Therefore, it becomes necessary to consider the reverse direction of KL divergence, i.e. $K L(p\\parallel q)$ and construct symmetric KL divergence represented by: ", "page_idx": 15}, {"type": "equation", "text": "$$\nS K L(q\\parallel p)=K L(q\\parallel p)+K L(p\\parallel q).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By transferring this idea from KL divergence to cross entropy, the formula for symmetric cross entropy is given by: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{S C E=C E+R C E=H(q,p)+H(p,q),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $R C E=H(p,q)$ is the reverse counterpart of $H(q,p)$ , known as reverse cross entropy. ", "page_idx": 15}, {"type": "text", "text": "A.6 More on Hyper-parameter Analysis ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We investigate the impact of several hyper-parameters, as depicted in Fig. 9. Specifically, we explore the influence of weights in the total loss. We present the results of tuning the weight $\\lambda_{1},\\,\\lambda_{2}$ , and $\\lambda_{3}$ on the Split-UT-Zappos (5 tasks). The average accuracy shown in Fig. 9(a), (b), and (c) initially exhibits an increasing trend, followed by an overall decreasing trend. CompILer reaches its peak performance when $\\lambda_{1}=1.0$ , $\\lambda_{2}=3e-6$ , and $\\lambda_{3}=0.7$ . ", "page_idx": 15}, {"type": "text", "text": "A.7 More on Ablation Studies ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Effect of multi-pool prompt learning. In addition to presenting the results of Avg ACC, FTT, and HM on Split-Clothing (5 tasks) for multi-pool prompt learning, we also include the remaining results for State and Object in Table 7. An intriguing observation is that the model achieves the best results for Object when the object pool is used exclusively, while it reaches the peak performance for State when only the state pool is introduced. This phenomenon suggests that incorporating the primitive prompt pools significantly aids in learning the representations of the corresponding primitives. Furthermore, when the model integrates the state prompt pool alone, not only does the accuracy of state is improved, but the accuracy of object also experiences significant gains, and vice versa. This implies that the learning of one type of primitive representation is crucial for the learning of another. Only when all three pools are integrated does the model achieve state-of-the-art performance in the most important metrics, namely average accuracy and HM. ", "page_idx": 15}, {"type": "table", "img_path": "2LRZhbTDtA/tmp/c4cab0ea30221899fd8b32a62493b1bbb0eb924f497ddeeefb1029dc93f849bc.jpg", "table_caption": ["Table 7: Ablate the multi-pool prompt learning on Split-Clothing (5 tasks). C, S, and O denote the composition pool, state pool, and object pool, respectively. "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "2LRZhbTDtA/tmp/5cafe1a2fea804cf377889fe9b48ad0303f783b87486fcdc76df8aff7bd75af7.jpg", "table_caption": ["Table 8: Ablate the guidance on Split-Clothing (5 tasks). "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "Effect of object-injected state prompting. In this section, we report the remaining results for object-injected state prompting on Split-Clothing (5 tasks) as shown in Table 8. State-injected object prompting $({\\mathsf{S}}{\\to}{\\mathsf{O}})$ introduces interference information for composition learning, resulting in a decline across all metrics expect Object. By contrast, object-injected state prompting $({\\mathrm{O}}{\\rightarrow}{\\mathrm{S}})$ ) leads to a significant improvement across all metrics, demonstrating the effectiveness of the proposed method. ", "page_idx": 16}, {"type": "text", "text": "Effect of generalized-mean prompt fusion. We show the intact results of the ablation study on pooling methods on Split-Clothing (5 tasks) in Fig. 9. It is obvious that max pooling hits the lowest result, while integrating mean pooling leads to a stable growth. Only deploying Generalized-mean pooling achieves the state-of-the-art results, surging from 84.70 to 89.21 in Avg Acc and plunging to 7.26 in FTT. Consequently, the theory that GeM pooling can diminish the irrelevant information and emphasize the crucial aspects is proved. ", "page_idx": 16}, {"type": "text", "text": "Effect of loss functions. Eventually, we present supplementary ablation experiments on loss functions in Table 10 and 11. The first row indicates that the model is optimized using cross entropy loss exclusively which achieves the poorest results. Furthermore, integrating either reverse cross entropy loss, inter-pool prompt discrepancy loss, or intra-pool diversity loss contributes to improvements across all metrics. Notably, the effectiveness of reverse cross entropy loss is higher on Split-UTZappos (5 tasks) compared to that on Split-Clothing (5 tasks). To conclude, the best performance is achieved when all loss functions are simultaneously employed. ", "page_idx": 16}, {"type": "text", "text": "A.8 More on Qualitative Results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "More qualitative results are presented to demonstrate the effectiveness of CompILer. We conducted a comparative analysis of the prediction results between CompILer and L2P [43] on Split-Clothing (5 tasks), Split-UT-Zappos (5 tasks) and Split-UT-Zappos (10 tasks). The ground truth is presented in the first row, followed by the prediction results of CompILer and L2P in the second and third rows, respectively. As illustrated in the Fig. 10, CompILer achieves correct predictions for all the images, while L2P consistently yields wrong classification results due to ambiguous composition boundary. Specifically, L2P exhibits a higher occurrence of misclassification on states compared to objects, which can be attributed to the disproportionate emphasis placed on objects, diminishing the model\u2019s focus on states. This experiment proves that CompILer effectively enhances the model\u2019s attention towards states, resulting in an improved overall fine-grained perception capability. ", "page_idx": 16}, {"type": "table", "img_path": "2LRZhbTDtA/tmp/4b61098366defbbbc5db28a71b2a7b28de27c99171210b171a141677797d89f5.jpg", "table_caption": ["Table 9: Ablate the pooling on Split-Clothing (5 tasks). "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "2LRZhbTDtA/tmp/0aa4ba0ea667f07abd9b251d610fe038a5751dc772a0fdf90db230b0a5cfa442.jpg", "table_caption": ["Table 10: Ablate the loss function on Split-Clothing (5 tasks). "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "2LRZhbTDtA/tmp/78dd73c12697027e17db7af1dfe05408ebda8a2395da7e5df96cd2055f4e55c1.jpg", "table_caption": ["Table 11: Ablate the loss function on Split-UT-Zappos (5 tasks). "], "table_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "2LRZhbTDtA/tmp/e318621c1370d74613e5948d0075b8f6533a4999a505158691ad38682c3ec718.jpg", "img_caption": ["(c) Split-UT-Zappos (10 tasks) "], "img_footnote": ["Figure 10: More qualitative results. For each sample, top row is ground-truth label (in black), middle row is CompILer prediction, and bottom row is L2P [43] prediction. The primitives in green and red refer to correct and incorrect predictions. "], "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The abstract and introduction effectively summarize the paper\u2019s contributions and scope, outlining the key claims and objectives. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: We discuss the limitations of our work in both the paper and the appendix. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We provide the full set of assumptions and a complete proof for each theoretical result. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We provide comprehensive implementation details and code to ensure the experiments are reproducible. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper provides open access to the data and code, along with sufficient instructions to faithfully reproduce the experimental results. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: We provide all the training and test details, including but not limited to: data splits, hyperparameters, how they were chosen, type of optimizer, etc. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: The paper appropriately reports the relevant information about the statistical significance of the experiments. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 20}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We provide sufficient information on the computer resources in implementation details section. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The research conducted in the paper aligns with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We discuss potential societal impacts of the work performed. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 21}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: Our paper poses no such risks. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We properly credited the creators and original owners of assets used our paper. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The new assets introduced in the paper are well documented and the documentation is provided alongside the assets. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}]