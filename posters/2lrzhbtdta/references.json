{"references": [{"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2020-00-00", "reason": "This paper introduces the vision transformer (ViT), a fundamental model used as the backbone in the current work."}, {"fullname_first_author": "James Kirkpatrick", "paper_title": "Overcoming catastrophic forgetting in neural networks", "publication_date": "2017-00-00", "reason": "This paper is foundational for the class incremental learning (CIL) field, which is closely related to the proposed Compositional Incremental Learning (composition-IL) task."}, {"fullname_first_author": "Zhizhong Li", "paper_title": "Learning without forgetting", "publication_date": "2017-00-00", "reason": "This paper introduces the learning without forgetting (LwF) algorithm, a key method used to address catastrophic forgetting in incremental learning, which is directly relevant to the composition-IL task."}, {"fullname_first_author": "Sylvestre-Alvise Rebuffi", "paper_title": "iCaRL: Incremental classifier and representation learning", "publication_date": "2017-00-00", "reason": "This paper introduces the iCaRL algorithm, another important method for incremental learning that is compared against in the current work."}, {"fullname_first_author": "Zifeng Wang", "paper_title": "Learning to prompt for continual learning", "publication_date": "2022-00-00", "reason": "This paper introduces the learning-to-prompt paradigm, which is the foundation of the proposed CompILer model."}]}