[{"figure_path": "2LRZhbTDtA/tables/tables_6_1.jpg", "caption": "Table 1: Avg Acc and FTT results on Split-Clothing (5 tasks) and Split-UT-Zappos (5 and 10 tasks). The best results are marked in bold. All results with standard deviations are averaged over three runs.", "description": "This table presents a comparison of the performance of various incremental learning methods on two datasets: Split-Clothing (with 5 incremental tasks) and Split-UT-Zappos (with both 5 and 10 incremental tasks).  The metrics used are Average Accuracy (Avg Acc), which measures the overall classification accuracy, and Forgetting Rate (FTT), which assesses the degree of catastrophic forgetting.  The table shows that CompILer outperforms other methods in most cases, demonstrating its effectiveness in compositional incremental learning.", "section": "5 Experiments"}, {"figure_path": "2LRZhbTDtA/tables/tables_7_1.jpg", "caption": "Table 2: State, Object and HM results on Split-Clothing. The best results are marked in bold.", "description": "This table presents the performance comparison of different incremental learning methods on the Split-Clothing dataset in terms of average accuracy for state classification, object classification and their harmonic mean.  The results are shown for five incremental tasks. The \"Upper Bound\" represents the performance of a model trained on the entire dataset.  The table helps to understand the performance differences between methods and illustrates the effectiveness of the proposed CompILer.", "section": "5 Experiments"}, {"figure_path": "2LRZhbTDtA/tables/tables_8_1.jpg", "caption": "Table 1: Avg Acc and FTT results on Split-Clothing (5 tasks) and Split-UT-Zappos (5 and 10 tasks). The best results are marked in bold. All results with standard deviations are averaged over three runs.", "description": "This table presents the average accuracy (Avg Acc) and forgetting rate (FTT) for different incremental learning methods on two datasets: Split-Clothing (5 tasks) and Split-UT-Zappos (5 and 10 tasks).  The results show the performance of various algorithms in terms of their ability to classify compositions while minimizing forgetting.  The 'Upper Bound' represents the best possible accuracy achievable with full training data, providing a benchmark for comparison.  Bold values highlight the best-performing method for each metric.", "section": "5 Experiments"}, {"figure_path": "2LRZhbTDtA/tables/tables_8_2.jpg", "caption": "Table 5: Ablative experiments for (a) object-injected state prompting, (b) prompt fusion method.", "description": "This table presents the ablation study results for two key components of the CompILer model: object-injected state prompting and generalized-mean prompt fusion.  It shows the impact of these components on the model's performance, measured by Average Accuracy (Avg Acc), Forgetting (FTT), and Harmonic Mean (HM) across different experimental settings on the Split-Clothing dataset.  The \"None\" row represents the baseline model without these components.  The \"S\u2192O\" and \"O\u2192S\" rows represent experiments where state prompts guide object prompt selection and vice versa, respectively.  The Max, Mean, and GeM rows show experiments using different fusion strategies for combining the selected prompts.", "section": "5.5 Ablation Study and Analysis"}, {"figure_path": "2LRZhbTDtA/tables/tables_8_3.jpg", "caption": "Table 7: Ablate the multi-pool prompt learning on Split-Clothing (5 tasks). C, S, and O denote the composition pool, state pool, and object pool, respectively.", "description": "This table presents the ablation study of the multi-pool prompt learning. It shows the performance results (Average Accuracy, Forgetting, and Harmonic Mean) on the Split-Clothing (5 tasks) dataset when different combinations of prompt pools (Composition, State, and Object) are used.  The results demonstrate the contribution of each prompt pool and highlight the benefit of incorporating all three pools for optimal performance.", "section": "5.5 Ablation Study and Analysis"}, {"figure_path": "2LRZhbTDtA/tables/tables_16_1.jpg", "caption": "Table 7: Ablate the multi-pool prompt learning on Split-Clothing (5 tasks). C, S, and O denote the composition pool, state pool, and object pool, respectively.", "description": "This table presents the ablation study on the impact of using different combinations of prompt pools (composition, state, and object) in the CompILer model on the Split-Clothing dataset.  It shows how the model's performance (Avg Acc, FTT, State accuracy, Object accuracy, and Harmonic Mean) changes as different prompt pools are added or removed. The results highlight the contribution of each prompt pool to the overall performance and demonstrate the synergistic effect of using all three pools together.", "section": "5.5 Ablation Study and Analysis"}, {"figure_path": "2LRZhbTDtA/tables/tables_16_2.jpg", "caption": "Table 5: Ablative experiments for (a) object-injected state prompting, (b) prompt fusion method.", "description": "This table presents the ablation study results on two aspects of the proposed CompILer model: object-injected state prompting and the prompt fusion method.  It compares the performance (Avg Acc, FTT, State, Object, HM) of different configurations, showing the impact of each component on the overall performance of the model. For object-injected state prompting, it examines three settings: no object-injected prompting, state-injected object prompting, and object-injected state prompting. For the prompt fusion method, it investigates three approaches: max pooling, mean pooling, and the proposed generalized-mean pooling.", "section": "5.5 Ablation Study and Analysis"}, {"figure_path": "2LRZhbTDtA/tables/tables_17_1.jpg", "caption": "Table 9: Ablate the pooling on Split-Clothing (5 tasks).", "description": "This table shows the ablation study of different prompt fusion methods on the Split-Clothing dataset with 5 tasks.  It compares the performance using max pooling, mean pooling, and generalized-mean (GeM) pooling across various metrics including Average Accuracy (Avg Acc), Forgetting (FTT), State accuracy, Object accuracy, and Harmonic Mean (HM).  The results demonstrate the superiority of the GeM pooling method, which yields the best performance overall.", "section": "5.4 Comparison with State-of-the-arts"}, {"figure_path": "2LRZhbTDtA/tables/tables_17_2.jpg", "caption": "Table 6: Ablate the loss functions on Split-Clothing and Split-UT-Zappos.", "description": "This table presents the ablation study on the impact of different loss functions on the performance of the CompILer model.  It shows the Average Accuracy (Avg Acc), Forgetting rate (FTT), State accuracy, Object accuracy, and Harmonic Mean (HM) for different combinations of loss functions (Cross Entropy (LCE), Reverse Cross Entropy (LRCE), Inter-pool Discrepancy Loss (Linter), Intra-pool Diversity Loss (Lintra)) on two datasets: Split-Clothing and Split-UT-Zappos.  The results demonstrate the contribution of each loss function in improving the model's performance.", "section": "5 Experiments"}, {"figure_path": "2LRZhbTDtA/tables/tables_17_3.jpg", "caption": "Table 1: Avg Acc and FTT results on Split-Clothing (5 tasks) and Split-UT-Zappos (5 and 10 tasks). The best results are marked in bold. All results with standard deviations are averaged over three runs.", "description": "This table presents the average accuracy (Avg Acc) and forgetting rate (FTT) for different incremental learning methods on two datasets: Split-Clothing (with 5 tasks) and Split-UT-Zappos (with 5 and 10 tasks).  The \"Upper Bound\" row represents the best achievable performance if catastrophic forgetting were not an issue.  The table shows that the proposed CompILer method outperforms existing methods in terms of both accuracy and forgetting.", "section": "5 Experiments"}]