[{"figure_path": "NPu7Cdk2f9/tables/tables_6_1.jpg", "caption": "Table 1: Our base-nets are compared with state-of-the-art efficient inference methods. \u2020 denotes static pruning methods, * denotes width-adaptation networks, and * denotes input-dependent dynamic networks. While these approaches exploit various non-canonical training techniques, such as iterative retraining, our base-nets are instantly selected from adaptive depth networks without fine-tuning.", "description": "This table compares the performance of the base networks (smallest sub-networks) of the proposed adaptive depth networks against other state-of-the-art efficient inference methods.  It highlights that the base networks, selected directly without further fine-tuning, achieve competitive accuracy while using fewer FLOPs.  The table emphasizes that unlike other methods which might use techniques like iterative retraining, the base networks are directly selected, showcasing the efficiency of the proposed approach.", "section": "4 Experiments"}, {"figure_path": "NPu7Cdk2f9/tables/tables_7_1.jpg", "caption": "Table 2: Training time (1 epoch), measured on Nvidia RTX 4090 (batch size: 128). AlphaNet* is configured to have similar FLOPs to MbV2 and only adjusts its depth to select sub-networks.", "description": "This table compares the training time for one epoch of different adaptive depth networks and non-adaptive counterparts, including ResNet50, MbV2, and their baselines.  It also includes the number of sub-networks considered for each model. The training time is measured using an Nvidia RTX 4090 GPU with a batch size of 128.  AlphaNet is included for comparison, but it is configured differently to focus on depth adaptation alone.", "section": "4.2 Training Cost"}, {"figure_path": "NPu7Cdk2f9/tables/tables_8_1.jpg", "caption": "Table 3: Ablation analysis with ResNet50-ADN and ViT-b/32-ADN. Applied components are checked. \u2193 and \u2191 in parentheses are comparisons to non-adaptive individual networks. By default, only the outputs, or \u0177 super and \u0177base, are used for self-distillation. The last row with double check marks shows the results when both intermediate features and outputs are used for self-distillation.", "description": "This table presents the ablation study results for ResNet50-ADN and ViT-b/32-ADN, demonstrating the impact of self-distillation and skip-aware batch/layer normalization on the models' performance.  The table shows that both techniques are beneficial, and that the combination of the two yields the best results. The last row shows the marginal impact of including intermediate features in self-distillation.", "section": "4.4 Ablation Study"}, {"figure_path": "NPu7Cdk2f9/tables/tables_8_2.jpg", "caption": "Table 4: Comparison of self-distillation strategies. Our approach (in bold) uses exclusively the super-net and the base-net as a teacher and a student, respectively.", "description": "This table compares different self-distillation strategies used to train the adaptive depth networks.  The standard approach uses the largest network (super-net) and the smallest network (base-net) exclusively as teacher and student. Other rows explore random sampling of networks for teacher/student roles in the self-distillation process. The results demonstrate that the proposed approach, using super-net and base-net exclusively, outperforms the random sampling strategies for both ResNet50-ADN and ViT-b/32-ADN networks.", "section": "4.4 Ablation Study"}, {"figure_path": "NPu7Cdk2f9/tables/tables_13_1.jpg", "caption": "Table 5: Each stage of base models is evenly divided into two sub-paths; the first is mandatory and the other is skippable. Since ViT does not define hierarchical stages, 12 identical encoder blocks are divided into 4 stages, resembling other residual networks for vision tasks.", "description": "This table shows the architecture details of the adaptive depth networks used in the experiments. It breaks down the number of mandatory and skippable blocks in each stage for four different network models: ResNet50-ADN, ViT-b/16-ADN, MbV2-ADN, and Swin-T-ADN.  Note that ViT, unlike the others, doesn't have inherent hierarchical stages, so its 12 encoder blocks are grouped into 4 stages for consistency.", "section": "A.1 Detailed Architectures"}, {"figure_path": "NPu7Cdk2f9/tables/tables_13_2.jpg", "caption": "Table 5: Each stage of base models is evenly divided into two sub-paths; the first is mandatory and the other is skippable. Since ViT does not define hierarchical stages, 12 identical encoder blocks are divided into 4 stages, resembling other residual networks for vision tasks.", "description": "This table details the architecture of four different network models (ResNet50-ADN, ViT-b/16-ADN, MbV2-ADN, and Swin-T-ADN) showing how each stage is divided into mandatory and skippable blocks.  It highlights that while most models have hierarchical stages, ViT (Vision Transformer) is treated differently due to its lack of explicit hierarchical stages. The table provides a clear picture of the division of blocks for each stage in the adaptive depth networks.", "section": "A.1 Detailed Architectures"}, {"figure_path": "NPu7Cdk2f9/tables/tables_13_3.jpg", "caption": "Table 5: Each stage of base models is evenly divided into two sub-paths; the first is mandatory and the other is skippable. Since ViT does not define hierarchical stages, 12 identical encoder blocks are divided into 4 stages, resembling other residual networks for vision tasks.", "description": "This table details the architecture of the adaptive depth networks used in the paper.  It shows how each stage in different network models (ResNet50-ADN, ViT-b/16-ADN, MbV2-ADN, Swin-T-ADN) is divided into mandatory and skippable sub-paths (blocks).  For the Vision Transformer (ViT), the 12 encoder blocks are grouped into four stages to mimic the hierarchical structure of the other network types.", "section": "A.1 Detailed Architectures"}, {"figure_path": "NPu7Cdk2f9/tables/tables_13_4.jpg", "caption": "Table 6: FLOPs and ImageNet validation accuracy of sub-networks. Only super-net (or, FFFF) and base-net (or, TTTT) are trained explicitly. Sub-networks in the middle can be selected at test time without explicit training. The highest accuracy in each group is shown in bold.", "description": "This table presents the performance of various sub-networks derived from a single ResNet50-ADN model.  Each row shows a specific sub-network configuration, denoted by a string of 'F's (for including the skippable sub-path) and 'T's (for skipping the sub-path) corresponding to each residual stage.  The FLOPs (in GigaFLOPS) and the accuracy (in %) on the ImageNet validation set are reported for each sub-network. Only the largest (FFFF) and smallest (TTTT) sub-networks are trained explicitly; the intermediate sub-networks' performance are evaluated without explicit training.", "section": "A.2 Performance of Sub-Networks"}, {"figure_path": "NPu7Cdk2f9/tables/tables_13_5.jpg", "caption": "Table 6: FLOPs and ImageNet validation accuracy of sub-networks. Only super-net (or, FFFF) and base-net (or, TTTT) are trained explicitly. Sub-networks in the middle can be selected at test time without explicit training. The highest accuracy in each group is shown in bold.", "description": "This table presents the performance of various sub-networks derived from a single adaptive depth network.  Each row represents a different sub-network configuration, indicated by a sequence of 'F' (for the skippable sub-path not skipped) or 'T' (for the skippable sub-path skipped) for each of the residual stages. The table shows the FLOPs (floating-point operations) and the accuracy on the ImageNet validation dataset for each configuration. Only the largest ('FFFF') and smallest ('TTTT') sub-networks are explicitly trained; the other sub-networks are evaluated without explicit training, demonstrating the efficiency of the approach.", "section": "A.2 Performance of Sub-Networks"}, {"figure_path": "NPu7Cdk2f9/tables/tables_14_1.jpg", "caption": "Table 7: The configurations of ResNet50-ADNs with different proportions between mandatory and skippable sub-paths. Total number of blocks at each stage remains unchanged.", "description": "This table shows different configurations of ResNet50-ADNs by varying the ratio of mandatory and skippable sub-paths within each stage.  While the total number of blocks per stage remains constant across all configurations, the distribution between mandatory and skippable blocks changes. This allows for an exploration of how different ratios impact the overall network performance.", "section": "A.3 Varying the Ratio of Sub-Path Lengths in ResNet50-ADN"}, {"figure_path": "NPu7Cdk2f9/tables/tables_14_2.jpg", "caption": "Table 5: Each stage of base models is evenly divided into two sub-paths; the first is mandatory and the other is skippable. Since ViT does not define hierarchical stages, 12 identical encoder blocks are divided into 4 stages, resembling other residual networks for vision tasks.", "description": "This table shows the architecture of the four different models used in the paper. Each stage of the model is divided into mandatory and skippable sub-paths. The table also notes that the Vision Transformer (ViT) model does not have hierarchical stages, so 12 encoder blocks are divided into 4 stages.", "section": "A.1 Detailed Architectures"}, {"figure_path": "NPu7Cdk2f9/tables/tables_14_3.jpg", "caption": "Table 8: Object detection and instance segmentation results on MS COCO dataset.", "description": "This table compares the performance of different object detectors (Faster-RCNN, Mask-RCNN, RetinaNet) using ResNet50 and ResNet50-Base (the base network of ResNet50-ADN) as backbones.  It shows the Box AP (Average Precision for bounding boxes) and Mask AP (Average Precision for masks) for each detector and backbone, demonstrating the improvement in performance achieved using the adaptive depth network (ResNet50-ADN).", "section": "B.1 Object Detection and Instance Segmentation"}]