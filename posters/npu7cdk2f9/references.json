{"references": [{"fullname_first_author": "Vaswani, A.", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduced the Transformer architecture, a fundamental building block in many modern deep learning models, including those discussed in this paper."}, {"fullname_first_author": "He, K.", "paper_title": "Deep residual learning for image recognition", "publication_date": "2016-07-01", "reason": "This paper introduced the ResNet architecture, a crucial component of many CNNs, which is directly referenced and used in experiments within this paper."}, {"fullname_first_author": "Howard, A. G.", "paper_title": "Mobilenets: Efficient convolutional neural networks for mobile vision applications", "publication_date": "2017-04-01", "reason": "This paper presented MobileNet, an efficient CNN architecture that is a key model used in the experiments of this paper, serving as a baseline for comparison."}, {"fullname_first_author": "Dosovitskiy, A.", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2021-01-01", "reason": "This paper introduced the Vision Transformer (ViT), another important model used in the experiments, providing an alternative to CNNs for image processing tasks."}, {"fullname_first_author": "Liu, Z.", "paper_title": "Swin transformer: Hierarchical vision transformer using shifted windows", "publication_date": "2021-01-01", "reason": "This paper introduced Swin Transformer, a hierarchical vision transformer, used in experiments in this paper as a state-of-the-art model"}]}