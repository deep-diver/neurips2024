[{"type": "text", "text": "AdaNeg: Adaptive Negative Proxy Guided OOD Detection with Vision-Language Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yabin Zhang Lei Zhang\u2217 The Hong Kong Polytechnic University The Hong Kong Polytechnic University csybzhang@comp.polyu.edu.hk cslzhang@comp.polyu.edu.hk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent research has shown that pre-trained vision-language models are effective at identifying out-of-distribution (OOD) samples by using negative labels as guidance. However, employing consistent negative labels across different OOD datasets often results in semantic misalignments, as these text labels may not accurately reflect the actual space of OOD images. To overcome this issue, we introduce adaptive negative proxies, which are dynamically generated during testing by exploring actual OOD images, to align more closely with the underlying OOD label space and enhance the efficacy of negative proxy guidance. Specifically, our approach utilizes a feature memory bank to selectively cache discriminative features from test images, representing the targeted OOD distribution. This facilitates the creation of proxies that can better align with specific OOD datasets. While task-adaptive proxies average features to reflect the unique characteristics of each dataset, the sample-adaptive proxies weight features based on their similarity to individual test samples, exploring detailed sample-level nuances. The final score for identifying OOD samples integrates static negative labels with our proposed adaptive proxies, effectively combining textual and visual knowledge for enhanced performance. Our method is training-free and annotation-free, and it maintains fast testing speed. Extensive experiments across various benchmarks demonstrate the effectiveness of our approach, abbreviated as AdaNeg. Notably, on the large-scale ImageNet benchmark, our AdaNeg significantly outperforms existing methods, with a $2.45\\%$ increase in AUROC and a $6.48\\%$ reduction in FPR95. Codes are available at https://github.com/YBZh/OpenOOD-VLM. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In real applications, artificial intelligence (AI) systems often encounter test samples of unknown classes, termed out-of-distribution (OOD) data. These OOD data often result in overly confident errors [52, 44], posing security threats. Therefore, accurately identifying OOD data is essential for ensuring the reliability and security of AI systems in open-world environments. ", "page_idx": 0}, {"type": "text", "text": "Traditional OOD detection methods in image domain primarily rely on vision-only models [20, 33, 36]. Recent advancements in vision-language models (VLMs) have demonstrated remarkable OOD detection performance by leveraging multi-modal knowledge [17, 15, 39]. Recently, NegLabel [27] explores negative labels by identifying text labels that are semantically distant from the in-distribution (ID) labels. This method achieves state-of-the-art performance by detecting test images closer to negative labels as OOD. In other words, NegLabel regards these negative labels as proxies of OOD data. However, employing consistent negative labels across different OOD datasets often leads to semantic misalignment, where these text labels may not accurately reflect the actual label space ", "page_idx": 0}, {"type": "image", "img_path": "vS5NC7jtCI/tmp/003301838c28be67bad717ee6bb6eb4ffb86a385c44ab3e2d3e2c221f40b3b40.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Qualitative and quantitative analyses of semantic misalignment between OOD labels and negative proxies using ImageNet (ID) and SUN (OOD) datasets. (a) Visualization of ID labels, OOD labels, negative labels from NegLabel, and adaptive negative proxies (AdaNeg). (b) Quantitative analysis based on ID-Similarity to OOD Ratio (ISOR in short, see Appendix A.1). Lower ISOR indicates a higher similarity to OOD labels and reduced similarity to ID labels. AdaNeg consistently achieves lower ISOR, demonstrating enhanced alignment with OOD characteristics. Visualizations include the top 1,000 discriminative proxies from both NegLabel and AdaNeg. ", "page_idx": 1}, {"type": "text", "text": "of OOD images, as shown in Fig. 1. This misalignment between the proxies and targeted OOD distribution leads to sub-optimal performance. ", "page_idx": 1}, {"type": "text", "text": "To promote the alignment between the negative proxies and target OOD distribution, we introduce the Adaptive Negative proxies (AdaNeg), which are dynamically generated during testing by exploring actual OOD images. Specifically, we start by initializing an empty category-split memory bank for each OOD dataset and selectively cache features of discriminative OOD images during testing. The OOD discrimination is assessed using mined negative labels, as detailed in [27]. With this feature memory, we develop task-adaptive proxies by simply averaging cached features within each category. These proxies, derived from actual OOD images, reflect the distinct characteristics of the target OOD dataset and align more closely with the underlying OOD label space. ", "page_idx": 1}, {"type": "text", "text": "The task-adaptive proxies mentioned previously provide unique proxies for different OOD datasets while maintaining consistency across various test samples within the same dataset. To delve into the fine-grained nuances at the sample level, we introduce the sample-adaptive proxies by weighting cached features based on their similarity to a particular test sample. This is achieved with an attention mechanism, where the feature memory serves as both keys and values, and the test feature acts as the query. The final score for detecting OOD samples integrates static negative labels with our adaptive proxies, effectively combining textual and visual knowledge for enhanced performance. ", "page_idx": 1}, {"type": "text", "text": "We conduct extensive experiments on standard benchmarks to validate the effectiveness of AdaNeg, where our proposed adaptive proxies outperform the negative-label-based one, enhancing performance with complementary multi-modal knowledge. Particularly, on the large-scale ImageNet dataset, our AdaNeg method outperforms existing methods by $2.45\\%$ AUROC and $6.48\\%$ FPR95. Notably, our method is training-free and annotation-free, and it maintains fast testing speed, as analyzed in Tab. 4. The ability to dynamically adjust to new OOD datasets without affecting testing speed or laborintensive annotation/training makes our approach particularly valuable for real-world applications where adaptability and efficiency are crucial. We summarize our contribution as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We first identify the label space misalignment between existing negative-label-based proxies and the target OOD distributions. In response, we introduce adaptive negative proxies that are dynamically generated during testing by exploring actual OOD images, resulting in a more effective alignment with the OOD label space. \u2022 Our adaptive negative proxies are constructed with a feature memory bank that selectively caches discriminative image features during testing. We instantiate this concept by developing task-adaptive proxies to reflect the unique characteristics of each OOD dataset and ", "page_idx": 1}, {"type": "text", "text": "sample-adaptive proxies to capture detailed sample-level nuances. The final OOD detection score combines these insights with complementary textual and visual knowledge. \u2022 We conduct thorough analyses of the proposed components and perform extensive experiments on standard benchmarks. Our method is training-free and annotation-free, and it maintains fast testing speed and achieves state-of-the-art performance. Notably, our method significantly outperforms existing methods, with a $2.54\\%$ increase in AUROC and a $6.48\\%$ reduction in FPR95 on the large-scale ImageNet dataset. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "OOD Detection focuses on identifying OOD test samples with semantic shifts, thus distinguishing it from generalization studies which typically focus on covariate shifts [3\u20135, 75]. A variety of OOD detection techniques have been developed, which can be roughly categorized into score-based [20, 33, 36, 37, 66, 26, 64, 56], distance-based [58, 59, 57, 12, 41, 53], and generative-based [50, 29] methods. Among them, score-based methods are particularly notable by employing a variety of scoring functions to differentiate between ID and OOD samples. These functions include confidencebased [20, 36, 56, 64], discriminator-based [29], energy-based [37, 66], and gradient-based [25] scores. In contrast, distance-based methods determine OOD samples by evaluating the distance in the feature space between test data and the closest ID samples [58] or ID prototypes [59], using metrics such as KNN [57, 12, 41] or Mahalanobis distance [33, 53]. ", "page_idx": 2}, {"type": "text", "text": "Despite their achievements, traditional OOD detection methods generally rely on manually annotated ID images and often overlook the integration of textual information. To leverage the textual knowledge, recent advancements have focused on employing VLMs [39, 40, 27, 77, 76, 15, 42, 65, 45]. Specifically, ZOC [15] applies VLMs to discern OOD instances by training a captioner that generates potential OOD labels. Nevertheless, this captioner often fails to produce effective OOD labels, particularly for ID datasets containing many classes. LoCoOp [42] adopts a novel approach by learning ID prompts from few-shot ID samples, and further enhances the robustness of these prompts by incorporating OOD features mined from the backgrounds of images. CLIPN [65], LSN [45] and LAPT [76] explore learning text prompts for expressing negative concepts. In specific, CLIPN initializes text prompts with the word \u2018no\u2019 combined with ID labels and refines them with large-scale multi-modal data; LSN starts with manually collected ID samples to learn negative prompts, offering a different approach to leveraging textual information in OOD detection; LAPT conducts automated prompt tuning with automatically collected training samples, boosting OOD detection without any manual effort. MCM [39] utilizes ID class names to facilitate effective zero-shot OOD detection. It is further refined by NegLabel [27], which incorporates additional negative class names mined from available data sources as negative proxies. However, as illustrated in Fig. 1, there is a mismatch between the negative-label-based proxies and the target OOD distribution, underscoring the limitations of this strategy. This observation has inspired us to construct adaptive proxies by exploring potential OOD test images during testing. This leads to an efficient method that aligns better with the target OOD distribution, resulting in enhanced OOD detection performance. ", "page_idx": 2}, {"type": "text", "text": "Furthermore, we clarify the relationship between our method and existing approaches on OOD exposure [17, 21, 73]. Most OOD exposure methods introduce manually collected negative images during training, where manual labor is necessary to ensure that the labels of negative images are different from ID ones. Moreover, involving negative images in training typically introduces additional computational overhead, impeding its practical deployment. Unlike these methods, NegLabel [27] is exposed to negative labels during the test phase in a training-free manner. However, given a fixed ID dataset, the exposed negative texts remain consistent for different OOD datasets, inevitably resulting in label misalignment, as shown in Fig. 1. To address this, we expose the VLMs to adaptive negative proxies, which explore actual OOD samples during testing and align more effectively with OOD distribution. Our method does not require manual annotations and works in a training-free manner, making it an appealing solution for real applications. ", "page_idx": 2}, {"type": "text", "text": "Test-time Adaptation. We adopt an online update of the negative proxies during testing, resembling test-time adaptation (TTA) methods [35, 63, 54]. Existing TTA methods primarily address covariate shifts between training and testing domains. In contrast, our approach mitigates the label shift between negative proxies and the target OOD distribution by exploring online test samples. Recently, TTA strategies have been considered in the field of OOD detection. However, these methods typically require test-time optimization [18, 72, 16], slowing down the testing process. In contrast, our method is optimization-free and introduces only a lightweight memory interaction operation, enabling rapid and accurate testing, as analyzed in Tab. 4. ", "page_idx": 2}, {"type": "image", "img_path": "vS5NC7jtCI/tmp/86c417a030a64f99c026e2d725570e8b3d09c5abe773e23c8abf5e0bf0dc6b9f.jpg", "img_caption": ["Figure 2: The overall framework of AdaNeg, where we selectively cache test images and generate adaptive proxies with an external feature memory bank. The final score combines textual and visual knowledge from static negative labels and our adaptive proxies, integrating multi-modal information. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Memory Networks. The use of memory networks for storing and retrieving past knowledge [67, 55] has been extensively applied across various fields, including classification [28, 51, 77], segmentation [46, 69], detection [8, 6], and NLP [47]. To our best knowledge, our work is the first to apply memory networks to the field of OOD detection. By caching and retrieving test images with a feature memory, we propose adaptive proxies to more effectively align with the OOD distribution in a training-free manner. This innovative approach significantly enhances the OOD detection performance. ", "page_idx": 3}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "OOD Detection Setup. Consider $\\mathcal{X}$ as the image domain and $\\mathcal{Y}=\\{y_{1},\\ldots,y_{C}\\}$ as the space of ID class labels, where $\\boldsymbol{\\wp}$ comprises text elements such as $\\mathcal{V}=\\{c a t,d\\dot{o}g,\\dots,b i r d\\}$ , and $C$ represents the total number of classes. Let $\\pmb{x}^{i n}$ and $\\pmb{x}^{o o d}$ be random variables representing ID and OOD samples from $\\mathcal{X}$ , respectively. We define $\\mathcal{P}\\pmb{x}^{i n}$ and $\\mathcal{P}\\mathbf{x}^{o o d}$ as the marginal distributions for $\\mathrm{ID}$ and OOD, respectively. In conventional classification scenarios, it is assumed that the test image $\\textbf{\\em x}$ originates from the ID and is associated with a specific ID label, specifically, $\\mathbf{\\Delta}x\\,\\in\\,\\mathcal{P}x^{i n}$ and $y\\in\\mathcal{V}$ , with $y$ being the label of $\\textbf{\\em x}$ . However, in real applications, AI systems often face data from unknown classes, denoted by $\\pmb{x}\\in\\mathcal{P}\\pmb{x}^{o o d}$ and $y\\notin\\mathcal{V}$ . Such occurrences can make AI models incorrectly categorize these instances into familiar ID categories with substantial certainty [52, 44], resulting in security concerns. To address these challenges, OOD detection is proposed to accurately categorize ID samples into their respective classes and reject OOD samples as non-ID. Recognition within the ID categories is performed using a $C$ -way classifier, following standard classification approaches [31, 19]. Concurrently, OOD detection typically employs a scoring mechanism $S$ [33, 36, 37] to differentiate between ID and OOD inputs: ", "page_idx": 3}, {"type": "equation", "text": "$$\nG_{\\gamma}(\\pmb{x})=\\mathrm{ID},\\operatorname{if}\\,S(\\pmb{x})\\geq\\gamma;\\quad\\mathrm{otherwise},\\,G_{\\gamma}(\\pmb{x})=\\mathrm{OOD},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $G_{\\gamma}$ represents the OOD detector set at a threshold $\\gamma\\in{\\mathcal{R}}$ . The test sample $\\textbf{\\em x}$ is identified as an ID sample if and only if $S(\\mathbf{\\boldsymbol{x}})\\geq\\gamma$ . ", "page_idx": 3}, {"type": "text", "text": "CLIP and NegLabel. For an ID test image $\\textbf{\\em x}$ within the label space $\\boldsymbol{\\wp}$ , we derive the image feature vector $\\pmb{v}=f_{i m g}(\\pmb{x})\\in\\mathcal{R}^{D}$ and the text feature matrix $C_{i d}=f_{t x t}(\\rho(y))\\in\\mathcal{R}^{C\\times D}$ using pre-trained CLIP encoders, where $D$ represents the feature dimension. The functions $f_{i m g}(\\cdot)$ and $f_{t x t}(\\cdot)$ are the encoders for images and text, respectively. The function $\\rho(\\cdot)$ is the text prompt mechanism, typically defined as \u2018a photo of a <label>.\u2019, where label is the actual class name, for example, \u2018cat\u2019 or \u2018dog\u2019. Both $\\pmb{v}$ and $C_{i d}$ undergo $L_{2}$ normalization across the dimension $D$ . Then, zero-shot classification probabilities are computed utilizing $_{C}$ as the classifier: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pmb{p}^{i d}=\\mathrm{Softmax}(\\pmb{v}\\pmb{C}_{i d}^{T}/\\tau)\\in\\mathcal{R}^{C},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\tau>0$ is the scaling temperature. ", "page_idx": 4}, {"type": "text", "text": "The vanilla CLIP is proposed for zero-shot ID recognition and has recently been extended to OOD detection. Specifically, the NegLabel approach [27] introduces negative class names $y^{-}=$ $\\{y_{C+1},\\dots,y_{C+M}\\}$ sourced from broad text corpora, where $M$ is the length of negative classes and $y^{-}\\cap y=\\emptyset$ . Then, we can obtain the full text feature matrix $C=f_{t x t}(\\rho\\overbar{(}\\mathcal{V}\\cup\\mathcal{V}^{-}))\\in\\mathcal{R}^{(C+M)\\times D}$ with the pre-trained CLIP text encoder, leading to the classification probability across $C+M$ classes: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pmb{p}=\\mathrm{Softmax}(\\pmb{v}\\pmb{C}^{T}/\\tau)\\in\\mathcal{R}^{C+M}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Assuming that $\\mathrm{ID}$ samples exhibit greater similarity to ID labels and lesser similarity to negative labels compared to OOD samples, NegLabel introduces the following score for OOD detection: ", "page_idx": 4}, {"type": "equation", "text": "$$\nS_{n l}(\\pmb{v})=\\sum_{i=1}^{C}p_{i},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\pmb{p}_{i}$ is the $i$ -th entry of $\\pmb{p}$ , indicating the classification probability of the $i^{\\th}$ -th class. Intuitively, the NegLabel method uses negative labels as proxies of the OOD distribution, detecting OOD images based on the similarity to these negative labels. ", "page_idx": 4}, {"type": "text", "text": "3.2 AdaNeg ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Although NegLabel has successfully employed negative labels as the OOD proxies, there is a semantic misalignment between such OOD proxies and actual OOD labels, as illustrated in Fig. 1. We aim to obtain OOD proxies that align better to the targeted OOD distribution. However, acquiring such OOD proxies is challenging, as the OOD information is unknown prior to actual testing. ", "page_idx": 4}, {"type": "text", "text": "From another perspective, we can access real OOD information during testing, motivating us to refine OOD proxies in the testing stage. We can identify discriminative negative images during testing and then adjust the OOD proxies toward detected images. This is achieved by selectively caching features of test images into a task-aware memory bank, followed by memory reading operations to produce adaptive proxies. We detail our implementation as follows. ", "page_idx": 4}, {"type": "text", "text": "Task-aware Memory. We construct a task-aware memory as a category-split tensor $M\\_{\\Sigma}$ $\\mathbb{R}^{(C+M)\\times L\\times D}$ , where $L$ is the memory length for each category. $_M$ is initialized with zero values and gradually fliled with features of selected images during testing. Specifically, for a test image with feature $\\pmb{v}$ , we first calculate its score $S_{n l}(\\pmb{v})$ with Eq. (4). If $S_{n l}(\\pmb{v})<\\gamma$ , we detect this test point as a negative image, and otherwise, it is identified as a positive sample. For negative and positive images, we respectively identify their closest labels as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Negative:~}y=\\underset{i}{\\arg\\operatorname*{max}}\\,p_{i}^{o o d}+C,}\\\\ &{\\mathrm{Positive:~}y=\\underset{i}{\\arg\\operatorname*{max}}\\,p_{i}^{i d},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\pmb{p}^{o o d}=\\pmb{p}[C:C+M]$ and $\\pmb{p}^{i d}=\\pmb{p}[0:C]$ are the classification probabilities corresponding to negative and ID class names, respectively. Then, we cache this feature $\\pmb{v}$ into the task-aware memory: ", "page_idx": 4}, {"type": "equation", "text": "$$\nM_{y,l}=v,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $l$ indicates an empty slot of $M_{y}\\in\\mathbb{R}^{L\\times D}$ . Once $M_{y}$ is fliled, we drop the image feature with the highest prediction entropy, as detailed in Appendix A.2. In one word, we keep confident image features with low prediction entropy in the memory. ", "page_idx": 4}, {"type": "text", "text": "The aforementioned strategy attempts to cache all test images, including those with high confusion between ID and OOD. However, we found that selectively retaining only those image features that exhibit strong ID/OOD distinguishing capabilities can effectively reduce this confusion. Specifically, we modify the selection criterion for memorization as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Negative:}\\;S_{n l}(v)<\\gamma\\rightarrow S_{n l}(v)<\\gamma-g\\gamma,}\\\\ &{\\mathrm{Positive:}\\;S_{n l}(v)\\geq\\gamma\\rightarrow S_{n l}(v)\\geq\\gamma+g(1-\\gamma),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $g\\in[0,1]$ is a hyperparameter that introduces a gap in the score space. Consequently, image features falling within the gap $\\gamma\\,-\\,g\\gamma\\,\\leq\\,S_{n l}({\\pmb v})\\,<\\,\\gamma\\,+\\,g(1\\,-\\,\\gamma)$ are considered to have low distinguishing confidence and are not cached. ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 Adaptive Negative Proxy Guided OOD Detection ", "page_idx": 5}, {"type": "text", "text": "Require: ID label space $\\boldsymbol{\\wp}$ and test set $\\mathcal{X}$   \n1: $\\mathcal{V}^{-}\\leftarrow\\mathrm{NegMine}(\\mathcal{V})$ following [27]   \n2: Constructing an empty memory $_M$   \n3: for $x\\in\\mathcal{X}$ do   \n4: Generating detection score with negative labels using Eq. 4   \n5: Determine whether $\\textbf{\\em x}$ should be cached using Eq. 8   \n6: Caching $\\textbf{\\em x}$ with Eq. 7 if needed   \n7: Generating adaptive proxies with memory bank using Eq. 9 or Eq. 11   \n8: Generating adaptive scores with Eq. 10 or Eq. 12   \n9: Generating final score $S_{a l l}$ by merging multi-modal knowledge with Eq. 13   \n10: end for   \n11: Return Collected final scores $\\{S_{a l l}\\}$ ", "page_idx": 5}, {"type": "text", "text": "Task-adaptive Proxies. Given the updated $_M$ , we can easily get the task-adaptive proxy by averaging along the length dimension $L$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pmb{C}^{t a}=\\mathrm{L}_{2}\\left(\\frac{1}{L+1}\\sum_{l=1}^{L+1}\\widehat{M}_{:,l,:}\\right)\\in\\mathbb{R}^{(C+M)\\times D},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathrm{L_{2}}(\\cdot)$ indicates the $L_{2}$ normalization along feature dimension $D$ , and $\\widehat{M}\\;=\\;[M,C]\\;\\in$ R(C+M)\u00d7(L+1)\u00d7D is the extended memory with vanilla text proxies C. Such a  memory extension is necessary since $_M$ is initially empty and uninformative. Initially, this extension initializes the adaptive proxies $C^{t a}$ with the basic text proxies $_{C}$ . However, there are two key distinctions between the adaptive $C^{t a}$ and the vanilla $_{C}$ . First, unlike the NegLabel approach, which employs a fixed proxy $_{C}$ across various OOD datasets, our $C^{t a}$ dynamically adjusts to the targeted OOD domain as the memory accumulates data, thereby providing dataset-specific adaptive proxies. Second, the $C^{t a}$ primarily incorporates image features, offering modal knowledge that complements the text-based proxies $_{C}$ . The benefits of this approach are further analyzed in Tab. 3. ", "page_idx": 5}, {"type": "text", "text": "The score function for OOD detection with the task-adaptive proxy $C^{t a}$ is derived as: ", "page_idx": 5}, {"type": "equation", "text": "$$\nS_{t a}(v)=\\frac{\\sum_{i=1}^{C}e^{\\cos(v,c_{i}^{t a})/\\tau}}{\\sum_{i=1}^{C}e^{\\cos(v,c_{i}^{t a})/\\tau}+\\sum_{j=C+1}^{C+M}e^{\\cos(v,c_{j}^{t a})/\\tau}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\cos(\\cdot,\\cdot)$ measures the cosine similarity, and $c_{i}^{t a}$ is the $i^{\\th}$ -th entry of $C^{t a}$ ", "page_idx": 5}, {"type": "text", "text": "Sample-adaptive Proxies. As evidenced in Table 3, our task-adaptive proxies significantly outperform the fixed text proxies by effectively adapting to the characteristics of target OOD dataset. Building on this success, we further refine our approach by leveraging finer-grained, sample-level nuances to introduce even more effective sample-adaptive proxies. Specifically, given the extended memory $\\widehat{M}$ and the test image feature $\\pmb{v}$ , we introduce the sample-adaptive proxy $C^{s a}\\in\\mathbb{R}^{(C+M)\\times D}$ via the f ollowing cross-attention operation: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pmb{c}_{i}^{s a}=\\mathbf{L}_{2}\\left(\\varphi\\left(\\pmb{v}(\\widehat{\\pmb{M}}_{i})^{\\top}\\right)\\widehat{\\pmb{M}}_{i}\\right)\\in\\mathbb{R}^{D},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathbf{v}(\\widehat{M}_{i})^{\\top}\\in\\mathbb{R}^{1\\times(L+1)}$ measures the cosine similarities between normalized features of $\\pmb{v}$ and ${\\widehat{M}}_{i}$ , and $\\varphi(x)=\\exp(-\\beta(1-x))$ modulates the sharpness of $x$ with hyper-parameter $\\beta$ . $c_{i}^{s a}$ and ${\\widehat{M}}_{i}$ are the $i$ -th entry of $C^{s a}$ and $\\widehat{M}$ , respectively. ", "page_idx": 5}, {"type": "text", "text": "Both the task-adaptive and the sample-adaptive proxies are derived from the memorized image features stored inM. The primary distinction between them lies in their respective weighting strategies. For $C^{t a}$ , each feature $\\widehat{M}_{:,l,:}$ : in the memory is assigned a uniform weighting coefficient of $\\textstyle{\\frac{1}{L+1}}$ . Conversely, in constructing $C^{s a}$ , the weighting coefficient for each cached feature is dynamically determined based on its cosine similarity to the test image feature, denoted as $v(\\widehat{M}_{i})^{\\top}$ . Consequently, while $C^{t a}$ remains constant across different test samples, $C^{s a}$ adapts to each individual test sample. This adaptability allows $C^{s a}$ to tailor its response based on the specific characteristics of each test image, thereby enhancing discrimination between ID and OOD samples, particularly in diverse and variable testing scenarios. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "The score function for OOD detection with the sample-adaptive proxies $C^{s a}$ is derived as: ", "page_idx": 6}, {"type": "equation", "text": "$$\nS_{s a}(v)=\\frac{\\sum_{i=1}^{C}e^{\\cos(v,c_{i}^{s a})/\\tau}}{\\sum_{i=1}^{C}e^{\\cos(v,c_{i}^{s a})/\\tau}+\\sum_{j=C+1}^{C+M}e^{\\cos(v,c_{j}^{s a})/\\tau}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Multi-modal Score. As previously discussed, the score function $S_{n l}(\\pmb{v})$ relies primarily on text features, whereas the sample-adaptive score function $S_{s a}(\\pmb{v})$ utilizes cached image features. Given the complementary nature of text and image modalities, we derive the final score function by integrating knowledge from both modalities: ", "page_idx": 6}, {"type": "equation", "text": "$$\nS_{a l l}(\\pmb{v})=S_{n l}(\\pmb{v})+\\lambda S_{s a}(\\pmb{v}),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\lambda>0$ is the hyperparameter balancing the two modalities. The overall pipeline of our method is illustrated in Fig. 2 and summarized in Algorithm 1. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Setup", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets. We conduct extensive experiments with the large-scale ImageNet-1k [9] as ID data. Following prior practice [26, 39, 27], four OOD datasets of iNaturalist [60], SUN [68], Places [78], and Textures [7] are evaluated. We also validate our method on the OpenOOD benchmark [74, 71], where OOD datasets are grouped into near-OOD (e.g., SSB-hard [62], NINCO [2]) and far-OOD (e.g., iNaturalist [60], Textures [7], OpenImage-O [64]) according to their similarity to ImageNet dataset. Besides ImageNet, we also evaluate our method on smaller-sized CIFAR10/100 datasets [30] with the OpenOOD setup. Specifically, with the ID dataset of CIFAR10/100, we adopt near-OOD datasets of CIFAR100/10 and TIN [32], and far-ood datasets of MNIST [10], SVHN [43], Texture [7], and Plances365 [78]. These experiments with various ID and OOD datasets enable a comprehensive evaluation on various OOD settings. ", "page_idx": 6}, {"type": "text", "text": "Implementation Details. We adopt the visual encoder of VITB/16 pretrained by CLIP [48] and analyze more backbone architectures in Tab. A11. For hyper-parameters, we adopt the memory length $L{=}10$ , threshold $\\gamma{=}0.5$ with the gap $g{=}0.5$ in Eq. 8, $\\beta{=}5.5$ in Eq. 11, and $\\lambda{=}0.1$ in Eq. 13 in all experiments. These hyper-parameters are carefully analyzed in Sec. 4.3. Following NegLabel, we adopt the text prompt of \u2018The nice <label>.\u2019, set temperature $\\scriptstyle{\\tau=0.01}$ , and define the number $M$ of negative labels as 10, 000 for the ImageNet dataset. For the CIFAR datasets, we set the number $M$ as 70, 000 since we find that a larger $M$ leads to better results for CIFAR. Following common practice [26, 39, 27], we employ the evaluation metrics of FPR95, AUROC, and ID ACC, which are detailed in Appendix A.3. All experiments are conducted with a single Tesla V100 GPU. ", "page_idx": 6}, {"type": "text", "text": "4.2 Main Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "ImageNet Results with Four OOD Datasets. As illustrated in Tab. 1, our AdaNeg significantly outperforms existing training-free methods and even surpasses approaches requiring additional training. Specifically, we report traditional methods [20, 36, 37, 25, 64, 57, 13, 59] by fine-tuning CLIP-encoders with labeled training samples following [27], and report results of [45, 42, 27, 34, 1] from their original paper. Compared to the closest competitor [27], our method achieves consistent and notable improvements, validating the advantage of the proposed adaptive proxies over the negative-label-based ones. ", "page_idx": 6}, {"type": "text", "text": "ImageNet Results with OpenOOD Setup. Our method is extensively evaluated against a range of OOD datasets in Tab. 2 on the OpenOOD benchmark. Competing methods that require training are referred from OpenOOD. These methods utilize the full ImageNet training set and hold an unfair advantage over zero-shot, training-free methods like ours. Our AdaNeg consistently outperforms its closest competitor [27] in both near-OOD and far-OOD scenarios. Additionally, AdaNeg not only enhances OOD detection capabilities but also improves ID classification, presenting higher robustness under diverse conditions. ", "page_idx": 6}, {"type": "text", "text": "Results on CIFAR10/100 datasets are provided in Appendix A.5, where our advantages still hold. ", "page_idx": 6}, {"type": "table", "img_path": "vS5NC7jtCI/tmp/ec63fa9fa52d2761b41e4a139da4a72de77332fb115ba01516adc9546f7fba7e.jpg", "table_caption": ["Table 1: OOD detection results with ImageNet-1k, where a VITB/16 CLIP encoder is adopted. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 2: OOD detection results on the OpenOOD benchmark, where ImageNet-1k is adopted as ID dataset. Full results are available in Tab. A7. ", "page_idx": 7}, {"type": "table", "img_path": "vS5NC7jtCI/tmp/8ab3bb086a21c1f70418af1dd7ee30ba0d4ce29891c4b91bf6c3dc148dbb651b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.3 Analyses and Discussions ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Score Functions. As illustrated in Tab. 3, the adaptive score functions $S_{t a}$ and $S_{s a}$ consistently outperforms the fixed $S_{n l}$ , validating the advantage of adaptive proxies over fixed label proxies. The sample-adaptive score $S_{s a}$ slightly surpasses the task-adaptive one $S_{t a}$ , verifying the usefulness of fine-grained sample characteristics. Combining adaptive image-based proxies and fixed text-based proxies leads to the best performance, proving their complementarity. ", "page_idx": 7}, {"type": "text", "text": "Table 3: OOD detection results with different score functions, where results are reported with ImageNet ID dataset under the OpenOOD setup. ", "page_idx": 7}, {"type": "table", "img_path": "vS5NC7jtCI/tmp/f88cf03814b7647ff64b8b2a64816d638ca41e8ddfabbd9652269f8707fa7b22.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "vS5NC7jtCI/tmp/64023b59f9774d9ec12fd5dd5b50d75b7fc8d7906612283ec6078b40958795be.jpg", "img_caption": ["Figure 3: Analyses on the hyper-parameters of (a) threshold $\\gamma$ in Eq. 8, (b) gap value $g$ in Eq. 8, and (c) memory length $L$ on the ImageNet dataset under OpenOOD setting. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "vS5NC7jtCI/tmp/b00c2b64c0c28b01ce4594f99543e5998d438c9bd777a272315bf42082ce967b.jpg", "table_caption": ["Table 4: Analyses on the time complexity of our AdaNeg and its competitors. \u2018Training\u2019 measures the training time, and \u2018Param.\u2019 presents the number of learnable parameters. \u2018FPS\u2019 reflects the inference speed, measured with a batch size of 256. Results are achieved with a NVIDIA V100 GPU. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Threshold $\\gamma$ and gap $g$ in Eq. 8. As illustrated in Fig. 3, employing a moderate threshold $\\gamma$ (e.g., $0.4<\\gamma<0.6)$ proves effective in distinguishing OOD samples across various scenarios. However, the efficacy of the gap parameter $g$ depends upon the specific characteristics of different OOD datasets. A larger $g$ facilitates the identification of more confidently classified ID/OOD samples, thereby improving the detection of far-OOD samples. Conversely, a smaller $g$ is advantageous for near-OOD detection as it accommodates low-confidence OOD samples, which are typically more prevalent in near-OOD scenarios. ", "page_idx": 8}, {"type": "text", "text": "In scenarios where the OOD distribution is entirely unknown, we adopt a conservative approach by setting $g$ to 0.5 in all experiments. This balanced setting provides a robust baseline for performance across a variety of conditions. However, if prior knowledge regarding the difficulty levels of OOD datasets is accessible, tailoring the hyperparameters\u2014such as opting for a smaller $g$ in more challenging OOD contexts\u2014can yield enhanced detection performance. ", "page_idx": 8}, {"type": "text", "text": "ID-OOD imbalanced Test Data. To investigate the stability of our method with imbalanced ID-OOD test data, we construct test sets with various mixture ratios of ID and OOD samples. Specifically, we adopted the 1.28M ImageNet training data as ID and randomly sampled 12.8K and 1.28K instances from the SUN OOD dataset to construct the ID:OOD ratios of 100:1 and 1000:1 settings, respectively. To construct the ID:OOD ratios of 1:100, 1:10, 1:1, and 10:1 settings, we randomly sampled 40K samples from the SUN dataset as OOD and randomly sampled 400, 4K, 40K, and 400K instances from the ImageNet training data. As shown in Tab. 5, our method outperforms NegLabel across a wide range of mixture ratios (from 1:100 to 100:1), validating the robustness and reliability of our approach. However, unbalanced mixture ratios do pose a challenge to our method. Our approach performs the best in scenarios with a balanced mixture of ID and OOD samples, reducing the FPR95 by $11.18\\%$ . As the mixture ratio becomes increasingly unbalanced, the improvement brought by our method gradually decreases. When the unbalanced ratio reaches 1000:1, our method shows some negative impact. ", "page_idx": 8}, {"type": "text", "text": "After a more detailed analysis, we discovered that the fundamental issue stems from the increased proportion of misclassified samples in the memory due to the growing ID-OOD imbalance. To effectively address this problem, we refine the selection criterion for memorizing OOD samples by adaptively adjusting the gap $g$ . We refer to this adaptive gap strategy as AdaGap, which significantly improves the robustness of our method against ID-OOD imbalanced test data, as demonstrated in ", "page_idx": 8}, {"type": "table", "img_path": "vS5NC7jtCI/tmp/c3c862343b3758f1be0ee9b80271a646fda92a94b3ee05f421f835cbaf0fd637.jpg", "table_caption": ["Table 5: FPR95 (\u2193) with different mixture ratios of ID and OOD samples "], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "vS5NC7jtCI/tmp/dee3e95eab50d89eceb69fca180f52db4527633edefb4bd5817912b2087956a0.jpg", "table_caption": ["Table 6: OOD detection results with BIMCV-COVID $^{19+}$ [61], where a VITB/16 CLIP encoder is adopted. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Table 5. To summarize, we first estimate the ratio of ID to OOD in the test data online. If there is a higher proportion of ID/OOD compared to OOD/ID, we adjust the standards for caching ID/OOD data into memory by dynamically modifying the gap in Eq. 8. For more detailed implementation, please refer to the Appendix A.6. ", "page_idx": 9}, {"type": "text", "text": "Generalization to Other Domains. Besides experiments with natural images, we further validate our method on the BIMCV-COVID $^{19+}$ dataset [61], which includes medical images, following the OpenOOD setup. Specifically, we select BIMCV as the ID dataset, which includes chest $\\mathbf{X}_{}$ -ray images CXR (CR, DX) of COVID-19 patients and healthy individuals. For the OOD datasets, we follow the OpenOOD setup and use CT-SCAN and X-Ray-Bone datasets. The CT-SCAN dataset includes computed tomography (CT) images of COVID-19 patients and healthy individuals, while the X-Ray-Bone dataset contains X-ray images of hands. As illustrated in Tab. 6, our AdaNeg method consistently outperforms NegLabel on this medical image dataset. ", "page_idx": 9}, {"type": "text", "text": "Memory Length. As demonstrated in Fig. 3c, there is a positive correlation between the memory length $L$ and the performance outcomes, affirming the efficacy of feature memorization from another perspective. In all our experiments, we have set $L$ to 10. ", "page_idx": 9}, {"type": "text", "text": "Complexity Analyses. As analyzed in Table 4, our AdaNeg approach does not introduce any learnable parameters or require model training. Furthermore, it significantly enhances performance while maintaining a fast testing speed. ", "page_idx": 9}, {"type": "text", "text": "More analyses and discussions on the $\\lambda$ in Eq. 13, $\\beta$ in Eq. 11, various backbone architectures, the ordering of test samples, complementarity to training-based method, and the number of test data can be found in Appendix A.6. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion and Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We proposed adaptive negative proxies that aligned more effectively with OOD distributions, thereby providing more effective guidance for OOD detection. These proxies were constructed using a task-aware feature memory that selectively cached discriminative image features during testing. Our approach facilitated the generation of both task-adaptive and sample-adaptive proxies through carefully designed memory reading operations. Notably, our method was training-free and annotationfree, and it maintained fast testing speed and achieved state-of-the-art results on various benchmarks. These results validated the effectiveness of the proposed adaptive proxies. ", "page_idx": 9}, {"type": "text", "text": "One minor limitation of our method is the introduction of an external memory requirement. For example, this memory occupies a storage space of 214.75MB when using the ImageNet dataset as the ID, which may pose challenges for storage-constrained applications. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Yichen Bai, Zongbo Han, Changqing Zhang, Bing Cao, Xiaoheng Jiang, and Qinghua Hu. Id-like prompt learning for few-shot out-of-distribution detection. arXiv preprint arXiv:2311.15243, 2023. [2] Julian Bitterwolf, Maximilian Mueller, and Matthias Hein. In or out? fixing imagenet out-of-distribution detection evaluation. In ICML, 2023. [3] Chaoqi Chen, Jiongcheng Li, Hong-Yu Zhou, Xiaoguang Han, Yue Huang, Xinghao Ding, and Yizhou Yu. Relation matters: Foreground-aware graph-based relational reasoning for domain adaptive object detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(3):3677\u20133694, 2022.   \n[4] Chaoqi Chen, Luyao Tang, Yue Huang, Xiaoguang Han, and Yizhou Yu. Coda: generalizing to open and unseen domains with compaction and disambiguation. Advances in Neural Information Processing Systems, 36, 2023. [5] Chaoqi Chen, Luyao Tang, Leitian Tao, Hong-Yu Zhou, Yue Huang, Xiaoguang Han, and Yizhou Yu. Activate and reject: towards safe domain generalization under category shift. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11552\u201311563, 2023.   \n[6] Yihong Chen, Yue Cao, Han Hu, and Liwei Wang. Memory enhanced global-local aggregation for video object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10337\u201310346, 2020. [7] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3606\u20133613, 2014. [8] Hanming Deng, Yang Hua, Tao Song, Zongpu Zhang, Zhengui Xue, Ruhui Ma, Neil Robertson, and Haibing Guan. Object guided external memory network for video object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6678\u20136687, 2019. [9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.   \n[10] Li Deng. The mnist database of handwritten digit images for machine learning research [best of the web]. IEEE signal processing magazine, 29(6):141\u2013142, 2012.   \n[11] Andrija Djurisic, Nebojsa Bozanic, Arjun Ashok, and Rosanne Liu. Extremely simple activation shaping for out-of-distribution detection. arXiv preprint arXiv:2209.09858, 2022.   \n[12] Xuefeng Du, Gabriel Gozum, Yifei Ming, and Yixuan Li. Siren: Shaping representations for detecting out-of-distribution objects. Advances in Neural Information Processing Systems, 35:20434\u201320449, 2022.   \n[13] Xuefeng Du, Xin Wang, Gabriel Gozum, and Yixuan Li. Unknown-aware object detection: Learning what you don\u2019t know from videos in the wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13678\u201313688, 2022.   \n[14] Xuefeng Du, Zhaoning Wang, Mu Cai, and Yixuan Li. Vos: Learning what you don\u2019t know by virtual outlier synthesis. arXiv preprint arXiv:2202.01197, 2022.   \n[15] Sepideh Esmaeilpour, Bing Liu, Eric Robertson, and Lei Shu. Zero-shot out-of-distribution detection based on the pre-trained model clip. In Proceedings of the AAAI conference on artificial intelligence, volume 36, pages 6568\u20136576, 2022.   \n[16] Ke Fan, Yikai Wang, Qian Yu, Da Li, and Yanwei Fu. A simple test-time method for out-of-distribution detection. arXiv preprint arXiv:2207.08210, 2022.   \n[17] Stanislav Fort, Jie Ren, and Balaji Lakshminarayanan. Exploring the limits of out-of-distribution detection. Advances in Neural Information Processing Systems, 34:7068\u20137081, 2021.   \n[18] Zhitong Gao, Shipeng Yan, and Xuming He. Atta: Anomaly-aware test-time adaptation for out-ofdistribution detection in segmentation. Advances in Neural Information Processing Systems, 36:45150\u2013 45171, 2023.   \n[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n[20] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. arXiv preprint arXiv:1610.02136, 2016.   \n[21] Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich. Deep anomaly detection with outlier exposure. arXiv preprint arXiv:1812.04606, 2018.   \n[22] Dan Hendrycks, Mantas Mazeika, Saurav Kadavath, and Dawn Song. Using self-supervised learning can improve model robustness and uncertainty. Advances in neural information processing systems, 32, 2019.   \n[23] Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. Augmix: A simple data processing method to improve robustness and uncertainty. arXiv preprint arXiv:1912.02781, 2019.   \n[24] Dan Hendrycks, Andy Zou, Mantas Mazeika, Leonard Tang, Bo Li, Dawn Song, and Jacob Steinhardt. Pixmix: Dreamlike pictures comprehensively improve safety measures. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16783\u201316792, 2022.   \n[25] Rui Huang, Andrew Geng, and Yixuan Li. On the importance of gradients for detecting distributional shifts in the wild. Advances in Neural Information Processing Systems, 34:677\u2013689, 2021.   \n[26] Rui Huang and Yixuan Li. Mos: Towards scaling out-of-distribution detection for large semantic space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8710\u20138719, 2021.   \n[27] Xue Jiang, Feng Liu, Zhen Fang, Hong Chen, Tongliang Liu, Feng Zheng, and Bo Han. Negative label guided OOD detection with pretrained vision-language models. In The Twelfth International Conference on Learning Representations, 2024.   \n[28] Geethan Karunaratne, Manuel Schmuck, Manuel Le Gallo, Giovanni Cherubini, Luca Benini, Abu Sebastian, and Abbas Rahimi. Robust high-dimensional memory-augmented neural networks. Nature communications, 12(1):2468, 2021.   \n[29] Shu Kong and Deva Ramanan. Opengan: Open-set recognition via open data generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 813\u2013822, 2021.   \n[30] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[31] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25, 2012.   \n[32] Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015.   \n[33] Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting outof-distribution samples and adversarial attacks. Advances in neural information processing systems, 31, 2018.   \n[34] Tianqi Li, Guansong Pang, Xiao Bai, Wenjun Miao, and Jin Zheng. Learning transferable negative prompts for out-of-distribution detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17584\u201317594, 2024.   \n[35] Jian Liang, Ran He, and Tieniu Tan. A comprehensive survey on test-time adaptation under distribution shifts. arXiv preprint arXiv:2303.15361, 2023.   \n[36] Shiyu Liang, Yixuan Li, and Rayadurgam Srikant. Enhancing the reliability of out-of-distribution image detection in neural networks. arXiv preprint arXiv:1706.02690, 2017.   \n[37] Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li. Energy-based out-of-distribution detection. Advances in neural information processing systems, 33:21464\u201321475, 2020.   \n[38] Xixi Liu, Yaroslava Lochman, and Christopher Zach. Gen: Pushing the limits of softmax-based out-ofdistribution detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 23946\u201323955, 2023.   \n[39] Yifei Ming, Ziyang Cai, Jiuxiang Gu, Yiyou Sun, Wei Li, and Yixuan Li. Delving into out-of-distribution detection with vision-language representations. Advances in Neural Information Processing Systems, 35:35087\u201335102, 2022.   \n[40] Yifei Ming and Yixuan Li. How does fine-tuning impact out-of-distribution detection for vision-language models? International Journal of Computer Vision, 132(2):596\u2013609, 2024.   \n[41] Yifei Ming, Yiyou Sun, Ousmane Dia, and Yixuan Li. How to exploit hyperspherical embeddings for out-of-distribution detection? arXiv preprint arXiv:2203.04450, 2022.   \n[42] Atsuyuki Miyai, Qing Yu, Go Irie, and Kiyoharu Aizawa. Locoop: Few-shot out-of-distribution detection via prompt learning. Advances in Neural Information Processing Systems, 36, 2024.   \n[43] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Baolin Wu, Andrew Y Ng, et al. Reading digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning and unsupervised feature learning, volume 2011, page 7. Granada, Spain, 2011.   \n[44] Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High confidence predictions for unrecognizable images. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 427\u2013436, 2015.   \n[45] Jun Nie, Yonggang Zhang, Zhen Fang, Tongliang Liu, Bo Han, and Xinmei Tian. Out-of-distribution detection with negative prompts. In The Twelfth International Conference on Learning Representations, 2024.   \n[46] Seoung Wug Oh, Joon-Young Lee, Ning Xu, and Seon Joo Kim. Video object segmentation using spacetime memory networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9226\u20139235, 2019.   \n[47] Charles Packer, Vivian Fang, Shishir G Patil, Kevin Lin, Sarah Wooders, and Joseph E Gonzalez. Memgpt: Towards llms as operating systems. arXiv preprint arXiv:2310.08560, 2023.   \n[48] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[49] Jie Ren, Stanislav Fort, Jeremiah Liu, Abhijit Guha Roy, Shreyas Padhy, and Balaji Lakshminarayanan. A simple fix to mahalanobis distance for improving near-ood detection. arXiv preprint arXiv:2106.09022, 2021.   \n[50] Seonghan Ryu, Sangjun Koo, Hwanjo Yu, and Gary Geunbae Lee. Out-of-domain detection based on generative adversarial network. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 714\u2013718, 2018.   \n[51] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Metalearning with memory-augmented neural networks. In International conference on machine learning, pages 1842\u20131850. PMLR, 2016.   \n[52] Walter J Scheirer, Anderson de Rezende Rocha, Archana Sapkota, and Terrance E Boult. Toward open set recognition. IEEE transactions on pattern analysis and machine intelligence, 35(7):1757\u20131772, 2012.   \n[53] Vikash Sehwag, Mung Chiang, and Prateek Mittal. Ssd: A unified framework for self-supervised outlier detection. arXiv preprint arXiv:2103.12051, 2021.   \n[54] Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao. Test-time prompt tuning for zero-shot generalization in vision-language models. Advances in Neural Information Processing Systems, 35:14274\u201314289, 2022.   \n[55] Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory networks. Advances in neural information processing systems, 28, 2015.   \n[56] Yiyou Sun, Chuan Guo, and Yixuan Li. React: Out-of-distribution detection with rectified activations. Advances in Neural Information Processing Systems, 34:144\u2013157, 2021.   \n[57] Yiyou Sun, Yifei Ming, Xiaojin Zhu, and Yixuan Li. Out-of-distribution detection with deep nearest neighbors. In International Conference on Machine Learning, pages 20827\u201320840. PMLR, 2022.   \n[58] Jihoon Tack, Sangwoo Mo, Jongheon Jeong, and Jinwoo Shin. Csi: Novelty detection via contrastive learning on distributionally shifted instances. Advances in neural information processing systems, 33:11839\u2013 11852, 2020.   \n[59] Leitian Tao, Xuefeng Du, Xiaojin Zhu, and Yixuan Li. Non-parametric outlier synthesis. arXiv preprint arXiv:2303.02966, 2023.   \n[60] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classification and detection dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 8769\u20138778, 2018.   \n[61] Maria De La Iglesia Vay\u00e1, Jose Manuel Saborit, Joaquim Angel Montell, Antonio Pertusa, Aurelia Bustos, Miguel Cazorla, Joaquin Galant, Xavier Barber, Domingo Orozco-Beltr\u00e1n, Francisco Garc\u00eda-Garc\u00eda, et al. Bimcv covid- $^{19+}$ : a large annotated dataset of rx and ct images from covid-19 patients. arXiv preprint arXiv:2006.01174, 2020.   \n[62] Sagar Vaze, Kai Han, Andrea Vedaldi, and Andrew Zisserman. Open-set recognition: A good closed-set classifier is all you need? arXiv preprint arXiv:2110.06207, 2021.   \n[63] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. arXiv preprint arXiv:2006.10726, 2020.   \n[64] Haoqi Wang, Zhizhong Li, Litong Feng, and Wayne Zhang. Vim: Out-of-distribution with virtual-logit matching. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4921\u20134930, 2022.   \n[65] Hualiang Wang, Yi Li, Huifeng Yao, and Xiaomeng Li. Clipn for zero-shot ood detection: Teaching clip to say no. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1802\u20131812, 2023.   \n[66] Yezhen Wang, Bo Li, Tong Che, Kaiyang Zhou, Ziwei Liu, and Dongsheng Li. Energy-based open-world uncertainty modeling for confidence calibration. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9302\u20139311, 2021.   \n[67] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. arXiv preprint arXiv:1410.3916, 2014.   \n[68] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Largescale scene recognition from abbey to zoo. In 2010 IEEE computer society conference on computer vision and pattern recognition, pages 3485\u20133492. IEEE, 2010.   \n[69] Guo-Sen Xie, Huan Xiong, Jie Liu, Yazhou Yao, and Ling Shao. Few-shot semantic segmentation with cyclic memory network. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7293\u20137302, 2021.   \n[70] Kai Xu, Rongyu Chen, Gianni Franchi, and Angela Yao. Scaling for training time and post-hoc out-ofdistribution detection enhancement. arXiv preprint arXiv:2310.00227, 2023.   \n[71] Jingkang Yang, Pengyun Wang, Dejian Zou, Zitang Zhou, Kunyuan Ding, Wenxuan Peng, Haoqi Wang, Guangyao Chen, Bo Li, Yiyou Sun, Xuefeng Du, Kaiyang Zhou, Wayne Zhang, Dan Hendrycks, Yixuan Li, and Ziwei Liu. Openood: Benchmarking generalized out-of-distribution detection. 2022.   \n[72] Puning Yang, Jian Liang, Jie Cao, and Ran He. Auto: Adaptive outlier optimization for online test-time ood detection. arXiv preprint arXiv:2303.12267, 2023.   \n[73] Jingyang Zhang, Nathan Inkawhich, Yiran Chen, and Hai Li. Fine-grained out-of-distribution detection with mixup outlier exposure. CoRR, (abs/2106.03917), 2021.   \n[74] Jingyang Zhang, Jingkang Yang, Pengyun Wang, Haoqi Wang, Yueqian Lin, Haoran Zhang, Yiyou Sun, Xuefeng Du, Kaiyang Zhou, Wayne Zhang, Yixuan Li, Ziwei Liu, Yiran Chen, and Hai Li. Openood v1.5: Enhanced benchmark for out-of-distribution detection. arXiv preprint arXiv:2306.09301, 2023.   \n[75] Yabin Zhang, Bin Deng, Hui Tang, Lei Zhang, and Kui Jia. Unsupervised multi-class domain adaptation: Theory, algorithms, and practice. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(5):2775\u20132792, 2020.   \n[76] Yabin Zhang, Wenjie Zhu, Chenhang He, and Lei Zhang. Lapt: Label-driven automated prompt tuning for ood detection with vision-language models. In European Conference on Computer Vision, 2024.   \n[77] Yabin Zhang, Wenjie Zhu, Hui Tang, Zhiyuan Ma, Kaiyang Zhou, and Lei Zhang. Dual memory networks: A versatile adaptation approach for vision-language models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 28718\u201328728, 2024.   \n[78] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. IEEE transactions on pattern analysis and machine intelligence, 40(6):1452\u20131464, 2017. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "table", "img_path": "vS5NC7jtCI/tmp/cab47f01fd10090cf88a45a99c1806b53d732bbde08b73d19d5b61700c550461.jpg", "table_caption": ["Table A7: Detailed OOD detection results on the OpenOOD benchmark, where ImageNet-1k is adopted as ID dataset. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 ID-Similarity to OOD Ratio with ground truth ID and OOD labels ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We introduce the ID-Similarity to OOD Ratio (ISOR) to quantitatively measure the relative alignment of negative proxies with ground truth OOD labels, compared to their alignment with ID labels. In implementation, we adapt the score function of NegLabel (i.e., Eq. 4), which originally measures the similarity of a test image to ID labels over negative labels. We modify this function by replacing the negative labels with ground truth OOD labels and changing the input from test images to negative proxies (e.g., negative texts), while keeping other aspects consistent with Eq. 4. This modified score function allows us to assess the degree of similarity between the inputs and ground truth ID/OOD labels, thereby quantifying the relative alignment between negative proxies and OOD labels. Specifically, lower ISOR indicates a higher similarity to OOD labels and a reduced similarity to ID labels. ", "page_idx": 14}, {"type": "text", "text": "A.2 Entropy-based caching strategy for full memory ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The memory we construct is of finite length; hence, as the number of cached images increases, it may become fully occupied. To address this, we have devised a simple strategy to retain only those images with low entropy, e.g., high confidence. Specifically, when storing an image in memory, we also record its entropy pertinent to OOD detection: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{Entropy}(v)=-S_{n l}(v)\\log(S_{n l}(v))-(1-S_{n l}(v))\\log(1-S_{n l}(v)),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $S_{n l}(v)$ represents the probability of belonging to the ID, as shown in Eq. (4). Given the entropy of the current test image and a full memory $M_{y}$ , we replace the item with the maximum entropy in $M_{y}$ with the current image feature if the current test image exhibits lower entropy. Otherwise, we do not cache the current test sample. ", "page_idx": 14}, {"type": "text", "text": "A.3 Evaluation criteria ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Following common practice [26, 39, 27], we employ the following three evaluation metrics: (1) FPR95, which measures the false positive rate for OOD samples when the detection accuracy for ID samples is at $95\\%$ ; (2) AUROC, the area under the receiver operating characteristic curve; and (3) ID ACC, which quantifies the accuracy of correctly identifying and classifying ID samples. ", "page_idx": 14}, {"type": "text", "text": "A.4 Detailed results on ImageNet dataset under OpenOOD setting ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The detailed OOD detection results on the OpenOOD benchmark are presented in Tab. A7. ", "page_idx": 14}, {"type": "text", "text": "A.5 Results on CIFAR10/100 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "As illustrated in Tab. A8, the advantage of our AdaNeg also holds on the CIFAR10/100 dataset. Notably, our method achieves new state-of-the-art results in the far-OOD setting under a zero-shot training-free manner, even outperforming its competitors training on the full labeled training set. ", "page_idx": 14}, {"type": "table", "img_path": "vS5NC7jtCI/tmp/0d91eb56e99899b5026ba26745985cec569819c63cf1b12ed35342ca00ff33fd.jpg", "table_caption": ["Table A8: OOD detection results with CIFAR10/100 on the OpenOOD benchmark. Full results are provided in Tables A10 and A9. "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "vS5NC7jtCI/tmp/398f2b9a486a35200b76adf09d2756d3cbb35db8edc5b5fc5e762d274cc1efbe.jpg", "table_caption": ["(a) CIFAR10 as ID dataset ", "(b) CIFAR100 as ID dataset "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "vS5NC7jtCI/tmp/4b64f1452b7171208635a8b3c88d55603ae8bce7f09f235cafd98618c534035a.jpg", "table_caption": ["Table A9: Detailed OOD detection results of on the OpenOOD benchmark, where CIFAR100 is adopted as ID dataset. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.6 More analyses ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Detailed Implementation of AdaGap Module. We have implemented an adaptive gap (AdaGap) module to adjust the memorization selection criteria dynamically. This strategy builds on the observation that as the score $S_{n l}$ increases/decreases, the probability that a sample is ID/OOD also increases accordingly. By enforcing a stringent selection criterion, we can effectively minimize the inclusion of misclassified samples in our memory. Specifically, we first online estimate the ratio of ID to OOD samples in the test data using a First-In-First-Out queue, which caches the ID/OOD estimation (cf. Eq. 8) of the most recent $N$ samples: ", "page_idx": 15}, {"type": "equation", "text": "$$\nM R={\\frac{{\\mathrm{Estimated}}\\ {\\mathrm{ID}}\\ {\\mathrm{Number}}}{{\\mathrm{Estimated}}\\ {\\mathrm{ID}}\\ {\\mathrm{Number}}+{\\mathrm{Estimated}}\\ {\\mathrm{OOD}}\\ {\\mathrm{number}}}},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the ID and OOD numbers are acquired within the queue. ", "page_idx": 15}, {"type": "text", "text": "Leveraging the estimated mix ratio $(M R)$ , we can dynamically adjust the gap $g$ in memory caching to avoid a majority of misclassified samples within the memory. For instance, if we find that ID samples constitute the majority of the test samples (i.e., $M R>0.5)$ ), this could lead to an increased proportion of ID samples in the OOD memory. In response, we can adjust the selection criterion for OOD memorization to only cache higher confidence OOD samples. This adjustment is achieved by modifying the selection criterion for memorization in Eq. 8 as follows: ", "page_idx": 15}, {"type": "image", "img_path": "vS5NC7jtCI/tmp/49156f4d6a12a664eadff81a42f192c8cdc467d728dde0754be2648ee382facf.jpg", "img_caption": ["Figure A4: Analyses on the hyper-parameters of (a) $\\lambda$ in Eq. 13 and (b) $\\beta$ in Eq. 11 on the ImageNet dataset under OpenOOD setting. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{:S_{n l}(v)<\\gamma-g\\gamma\\to S_{n l}(v)<\\gamma-\\operatorname*{max}(g,M R)\\gamma,}\\\\ &{:S_{n l}(v)\\geq\\gamma+g(1-\\gamma)\\to S_{n l}(v)\\geq\\gamma+\\operatorname*{max}(g,1-M R)(1-\\gamma),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $g\\:=\\:0.5$ is the default gap analyzed in Figure 3b. In this way, under ID/OOD balanced conditions (i.e., $M R=0.5)$ , our method aligns with our original version. However, if the proportion of ID samples is higher in the test samples\u2019 estimation (i.e., $M R>0.5)$ , we raise the standard for storing negative samples in the memory. In an extreme case when $M R=1$ , we estimate that there might be no OOD samples among the test samples; thus, we stop storing test samples in the negative memory and only selectively cache test samples into the positive memory. We adjust our approach conversely when the $M R$ value is lower than 0.5. ", "page_idx": 16}, {"type": "text", "text": "Please note that the selection criterion is dynamically adjusted online because the $M R$ is estimated with the most recent $N$ test samples. Here, we set $N=10$ , 000 by default. This dynamic adjustment ensures that our memory caching strategy remains responsive to the evolving nature of the test sample distribution, thereby optimizing memory utilization and enhancing the accuracy of our domain distinction process. ", "page_idx": 16}, {"type": "text", "text": "$\\lambda$ in Eq. 13. As illustrated in Fig. A4a, the OOD detection performance remains robust across a wide range of $\\lambda$ values, e.g., from 0.01 to 1. For all experiments, we have set $\\lambda$ to 0.1. ", "page_idx": 16}, {"type": "text", "text": "$\\beta$ in Eq. 11. Results with different $\\beta$ values are shown in Fig. A4b, where OOD detection performance is robust to the $\\beta$ values. We adopt $\\beta{=}5.5$ in all experiments. ", "page_idx": 16}, {"type": "text", "text": "Ordering of Testing Data. Our AdaNeg selectively caches test data into memory, potentially causing variations in the results depending on the ordering of the test data. To rigorously test this aspect, we randomly shuffled the order of the test data using three distinct seeds. We observed that our method exhibits robustness to changes in the ordering of test data. Specifically, across three experiments conducted on the ImageNet dataset, the AUROC scores were $96.65\\%$ , $96.69\\%$ , and $96.64\\%$ , respectively, demonstrating fluctuations of less than $0.1\\%$ . We report the average results from three random runs in our paper. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "Various Backbone Architectures. Results with various VLMs architectures are illustrated in Tab.   \nA11, where better results are typically achieved with stronger backbones. ", "page_idx": 17}, {"type": "table", "img_path": "vS5NC7jtCI/tmp/48e6cd79a3bb712f90f85b5ed797c58c24591eede2eb4a563e94ec51a171bdd8.jpg", "table_caption": ["Table A11: OOD detection results of our AdaNeg with different VLMs architectures, where ImageNet1K is used as the ID dataset. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Complementarity to Training-based Method. We validated the complementarity between our AdaNeg method and the latest works, NegPrompt [34] and LAPT [76], which use learned prompts. As shown in Table A12, our method significantly improves performance over these approaches, demonstrating its complementarity with training-based methods. ", "page_idx": 17}, {"type": "table", "img_path": "vS5NC7jtCI/tmp/ba73817bb50073169a91524bd71c12f96965871e58bb3a61b9982a4a32de35d4.jpg", "table_caption": ["Table A12: FPR95 (\u2193) with the ID dataset of ImageNet. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Number of Testing Data. We examine the dependency of our approach on the number of test images by evaluating its performance across different scales of test samples. As the number of test samples increases (from 900 to 90K), the cached feature data also increases, leading to an improvement in our method\u2019s results, as shown in Tab. A13. Even with a small number of test samples (e.g., 90 and 900), our method significantly reduces FPR95 compared to NegLabel, demonstrating its robustness across different numbers of test images. ", "page_idx": 17}, {"type": "text", "text": "Note that with only 90 test images, the task of distinguishing between ID and OOD samples degenerates into a simpler task since the number of test images is even smaller than the number of classes (e.g., 1000 for ImageNet). Consequently, both NegLabel and our method achieve lower FPR95 in such an easier scenario. ", "page_idx": 17}, {"type": "text", "text": "Table A13: FPR95 (\u2193) with different numbers of test images, where test samples are randomly sampled from ImageNet (ID) and SUN (OOD) datasets, and we maintain a consistent ratio of ID to OOD samples at 5:4 throughout the experiments. ", "page_idx": 17}, {"type": "table", "img_path": "vS5NC7jtCI/tmp/d007ab3b4f0b1fa53add873dfdc40e4e8432e1333174abd04f17cecdc7a64c64.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: ", "page_idx": 18}, {"type": "text", "text": "Guidelines: This paper contributes the adaptive negative proxies for OOD detection, which matches the main claims in the abstract and introduction. ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: Please refer to Sec. 5 in the main paper. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: The main contributions are the algorithm design and experimental validation.   \nNo theory proofs are provided. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We provide detailed algorithm designs in Sec. 3 and hyper-parameters in Sec 4.1 to facilitate reproducibility. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The adopted datasets are publicly available. Codes are available at https: //github.com/YBZh/OpenOOD-VLM. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The proposed method does not require training, and the testing details are illustrated in Sec 4.1. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: As analyzed in Sec. A.6 in the appendix, we examined the experimental results on the ImageNet dataset with different orderings of the testing data. We found that the order of the test data has a minimal impact on the final results, e.g., the variation in three random experiments was less than $0.1\\%$ , which verified the statistical stability of our method. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 20}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Our experiments can be reproduced with only one V100 GPU, as shown in Sec. 4.1. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: I have read the NeurIPS Code of Ethics and confirm that this paper conforms. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Positive Societal Impacts: The proposed method for OOD detection can enhance the reliability and safety of AI systems, particularly in critical applications such as healthcare, autonomous driving, and cybersecurity. By improving the detection of outof-distribution data, the system can better avoid potentially harmful decisions based on anomalous inputs, thereby increasing trust in AI technologies and their deployment in various fields. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "Negative Societal Impacts: One potential negative impact is the risk of privacy issues due to online caching and processing of test data, which may inadvertently store sensitive information. Additionally, the improved OOD detection might be misused in surveillance and monitoring applications, leading to ethical concerns regarding privacy and autonomy. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The used codes and datasets are well cited. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer:[NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]