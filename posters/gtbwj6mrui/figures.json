[{"figure_path": "GtbwJ6mruI/figures/figures_0_1.jpg", "caption": "Figure 1: (a) In a cube-moving environment, tasks are defined according to different environmental features. (b) Different tasks have different transition dynamics caused by underlying environmental features, hence optimal skills are different across tasks.", "description": "This figure illustrates the concept of skill-aware generalization in reinforcement learning using a cube-moving task as an example.  (a) shows that tasks can be defined by varying environmental factors, such as table friction or cube mass. (b) shows how these factors lead to different optimal ways (skills) of solving the task. For example, a high-friction surface might require lifting the cube, while a low-friction surface would allow for pushing. The agent needs to learn to identify and adapt its behaviour to these different conditions.", "section": "1 Introduction"}, {"figure_path": "GtbwJ6mruI/figures/figures_2_1.jpg", "caption": "Figure 2: A policy \u03c0 conditioned on a fixed context embedding c is defined as a skill \u03c0(\u00b7|c) (shortened as \u03c0c). The policy \u03c0 conditioned on a fixed c alters the state of the environment in a consistent way, thereby exhibiting a mode of skill. The skill \u03c0(c1) moves the cube on the table in trajectory T and is referred to as the Push skill; correspondingly, the Pick&Place skill \u03c0(c2) takes the cube off the table and places it in the goal position in the trajectory T.", "description": "This figure illustrates how a policy conditioned on a fixed context embedding represents a specific skill. It shows two examples: the \"Push\" skill, where the agent pushes the cube across the table, and the \"Pick&Place\" skill, where the agent picks up the cube and places it at the goal position.  The context embedding acts as a selector for the skill, demonstrating how different skills are encoded within the context embedding space.", "section": "3 Preliminaries"}, {"figure_path": "GtbwJ6mruI/figures/figures_3_1.jpg", "caption": "Figure 3: IInfoNCE(C;;Te), with a finite sample size of K, is a loose lower bound of I(C; Tc) and leads to lower performance embeddings. IsaMI (C; \u03c0c; Te) is a lower ground-truth MI, and ISaNCE (C; c; Tc) is a tighter lower bound.", "description": "The figure shows the convergence speed of different mutual information estimators.  I(c; \u03c4c) represents the true mutual information between context embeddings (c) and trajectories (\u03c4c).  IInfoNCE(c; \u03c0c; \u03c4c) is a K-sample estimator for MI which has a logarithmic upper bound (logK), shown as a horizontal gray line.  Because of its loose lower bound, IInfoNCE converges to logK slowly.  IsaMI(c; \u03c0c; \u03c4c) is a tighter lower bound than IInfoNCE and converges more quickly. SaNCE is a proposed estimator that more closely approximates IsaMI, resulting in the fastest convergence.", "section": "4.1 The log-K curse of K-sample MI estimators"}, {"figure_path": "GtbwJ6mruI/figures/figures_4_1.jpg", "caption": "Figure 4: A comparison of sample spaces for task e1. Positive samples Te\u2081 or T are always from current task e\u2081. For SaNCE, in a task ek with embedding ck, the positive skill \u03c0c conditions on ck and generates positive trajectories T\u03c0c, and the negative skill \u03c0c generates negative trajectories T\u03c0c. The top graphs show the relationship between c, \u03c0c and Tc.", "description": "This figure compares three different sampling strategies for contrastive learning in the context of meta-reinforcement learning.  (a) InfoNCE samples positive examples from the current task and negative examples from all other tasks. (b) SaNCE samples both positive and negative examples from the current task, but distinguishes them based on skills. (c) Sa+InfoNCE combines the strategies of InfoNCE and SaNCE. The figure illustrates the different sample spaces used by each method and highlights the key difference of SaNCE in focusing on skill-related information within the current task to improve sample efficiency.", "section": "4.3 Skill-aware noise contrastive estimation: a tighter K-sample estimator"}, {"figure_path": "GtbwJ6mruI/figures/figures_5_1.jpg", "caption": "Figure 5: A practical framework for using SaNCE in the meta-training phase. During meta-training, we sample trajectories from the replay buffer for off-policy training. Queries are generated by a context encoder \u03c8, which is updated with gradients from both the SaNCE loss LSaNCE and the RL loss LRL. negative/positive embeddings are encoded by a momentum context encoder \u03c8*, which is driven by a momentum update with the encoder \u03c8. During meta-testing, the meta-trained context encoder \u03c8 embeds the current trajectory, and the RL policy takes the embedding as input together with the state for adaptation within an episode.", "description": "This figure shows a practical framework for integrating SaNCE into the meta-training process.  It details how trajectories are sampled from a replay buffer, how a context encoder and momentum encoder generate queries and positive/negative embeddings, and how the encoder is updated using both SaNCE and RL loss functions. The meta-testing phase is also briefly depicted, showing how the trained context encoder is used for adaptation during an episode.", "section": "4.3 Skill-aware noise contrastive estimation: a tighter K-sample estimator"}, {"figure_path": "GtbwJ6mruI/figures/figures_6_1.jpg", "caption": "Figure 6: (a) UMAP visualisation of context embeddings for the SaCCM in the Panda-gym environment, with points in the yellow box representing the Push skill in high-mass tasks. Heatmap of (b) success rate, (c) Push skill probability, and (d) Pick&Place skill probability for SaCCM. In large-mass scenarios, the Push skill is more likely to be executed than Pick&Place.", "description": "This figure visualizes context embeddings learned by SaCCM in the Panda-gym environment using UMAP.  The yellow box highlights the Push skill in high-mass scenarios.  Heatmaps then show the success rate, probability of using the Push skill, and probability of using the Pick&Place skill across various mass and friction conditions. It demonstrates that SaCCM learns to associate different skills with different environmental conditions (mass and friction in this case).", "section": "5.2 Panda-gym"}, {"figure_path": "GtbwJ6mruI/figures/figures_9_1.jpg", "caption": "Figure 7: Effect of (a) buffer size (TESAC, CCM, SaTESAC, SaCCM) and (b) contrastive batch size (CCM, SaTESAC, SaCCM) in the SlimHumanoid environment.", "description": "The figure shows the effect of buffer size and contrastive batch size on the performance of different algorithms in the SlimHumanoid environment.  (a) shows how average return varies with different buffer sizes (400000, 100000, 10000, and 1000) for TESAC, CCM, SaTESAC, and SaCCM. (b) shows how average return varies with different contrastive batch sizes (512, 128, 16, and 8) for CCM, SaTESAC, and SaCCM.", "section": "5.4 Analysis of the log-K curse in sample-limited scenarios"}, {"figure_path": "GtbwJ6mruI/figures/figures_16_1.jpg", "caption": "Figure 8: Venn diagrams illustrating (a) mutual information I(c; \u03c4c), (b) interaction information IsaMI(C; \u03c0c; \u03c4c), and (c) the MDP graph of the context embedding c, skill \u03c0c, and trajectory \u03c4c, which represents a common-cause structure [Neuberg, 2003].", "description": "This figure uses Venn diagrams to illustrate the concepts of mutual information and interaction information, and also shows a graphical representation of the causal relationship between context embedding, skill, and trajectory in a meta-RL setting.  Panel (a) shows the mutual information between the context embedding and trajectory. Panel (b) depicts the interaction information which is the difference between the mutual information of context embedding and trajectory and the conditional mutual information of the context embedding and trajectory given the skill. Panel (c) represents a causal graphical model showing the context embedding as a common cause influencing both the skill and the trajectory.", "section": "4 Skill-aware mutual information optimisation for Meta-RL"}, {"figure_path": "GtbwJ6mruI/figures/figures_17_1.jpg", "caption": "Figure 9: (a) Modified Panda-gym benchmarks, (b) the training tasks, moderate test tasks, and extreme test tasks. The moderate test task setting involves combinatorial interpolation, while the extreme test task setting includes unseen ranges of environmental features and represents an extrapolation.", "description": "This figure shows the modified Panda-gym environment used in the experiments.  Panel (a) is a 3D rendering of the robotic arm and cube setup. Panel (b) is a heatmap showing the ranges of mass and friction values used to define the training and testing tasks. The training tasks are in the central region of the plot, the moderate test tasks are in a region adjacent to the training tasks, and the extreme test tasks cover unseen areas.", "section": "5.1 Experimental setup"}, {"figure_path": "GtbwJ6mruI/figures/figures_18_1.jpg", "caption": "Figure 10: Ten environments in modified MuJoCo benchmark.", "description": "This figure shows the ten different robotic control environments used in the modified MuJoCo benchmark.  These environments include variations of Ant, Half-Cheetah, SlimHumanoid, Hopper, and Walker robots, along with crippled versions of the Ant, Half-Cheetah, and Walker, as well as the Humanoid Standup. These variations are used to evaluate the generalization capability of the reinforcement learning agents across tasks with varying difficulty and characteristics.", "section": "5.3 MuJoCo"}, {"figure_path": "GtbwJ6mruI/figures/figures_22_1.jpg", "caption": "Figure 11: Loss coefficient \u03b1 analysis of Panda-gym benchmark in training and test (moderate and extreme) tasks.", "description": "This figure shows the impact of the loss coefficient \u03b1 on the performance of three algorithms (CCM, SaTESAC, and SaCCM) across different settings: training tasks, moderate test tasks, and extreme test tasks.  The x-axis represents different values of \u03b1, and the y-axis represents the success rate.  The figure illustrates how the choice of \u03b1 affects the success rate in these three scenarios.", "section": "E.1 Balance contrastive and RL updates: loss coefficient \u03b1"}, {"figure_path": "GtbwJ6mruI/figures/figures_22_2.jpg", "caption": "Figure 6: (a) UMAP visualisation of context embeddings for the SaCCM in the Panda-gym environment, with points in the yellow box representing the Push skill in high-mass tasks. Heatmap of (b) success rate, (c) Push skill probability, and (d) Pick&Place skill probability for SaCCM. In large-mass scenarios, the Push skill is more likely to be executed than Pick&Place.", "description": "This figure visualizes the context embeddings learned by the SaCCM model in the Panda-gym environment using UMAP.  The yellow box highlights embeddings associated with the \"Push\" skill, prevalent in tasks involving high-mass cubes. The heatmaps further illustrate the success rate and probability of using either the \"Push\" or \"Pick&Place\" skill across different combinations of cube mass and table friction. The results demonstrate that SaCCM effectively learns to associate context embeddings with specific skills, adapting its strategy based on task characteristics.", "section": "5.2 Panda-gym"}, {"figure_path": "GtbwJ6mruI/figures/figures_23_1.jpg", "caption": "Figure 1: (a) In a cube-moving environment, tasks are defined according to different environmental features. (b) Different tasks have different transition dynamics caused by underlying environmental features, hence optimal skills are different across tasks.", "description": "This figure illustrates the concept of skill-aware generalization in reinforcement learning.  Panel (a) shows a cube-moving task with varying environmental features such as friction and mass, leading to different optimal skills (modes of behaviour) for each task. Panel (b) highlights how these differing environmental features result in varying transition dynamics and, consequently, the need for different skills (e.g., pushing versus lifting) to succeed.", "section": "1 Introduction"}, {"figure_path": "GtbwJ6mruI/figures/figures_23_2.jpg", "caption": "Figure 4: A comparison of sample spaces for task e1. Positive samples Te\u2081 or T are always from current task e\u2081. For SaNCE, in a task ek with embedding ck, the positive skill \u03c0c conditions on ck and generates positive trajectories T\u03c0c, and the negative skill \u03c0c generates negative trajectories T\u03c0c. The top graphs show the relationship between c, \u03c0c and Tc.", "description": "This figure compares the sample spaces used for training the context encoder in three different methods: InfoNCE, SaNCE, and a combination of both. InfoNCE uses positive samples from the current task and negative samples from other tasks, while SaNCE only uses samples from the current task, distinguishing positive and negative samples based on the skill used. The combined method utilizes both strategies. The figure illustrates the different sample spaces visually and highlights the key differences between the approaches.", "section": "4.3 Skill-aware noise contrastive estimation: a tighter K-sample estimator"}, {"figure_path": "GtbwJ6mruI/figures/figures_24_1.jpg", "caption": "Figure 15: UMAP visualisation of context embeddings extracted from trajectories collected in the Panda-gym environments.", "description": "UMAP visualization of context embeddings for TESAC, CCM, SaTESAC, and SaCCM algorithms in the Panda-gym environment. Each point represents a trajectory.  The plots show how different algorithms cluster trajectories based on the underlying skills (Push and Pick&Place). SaMI-based algorithms (SaTESAC and SaCCM) show more distinct clustering than TESAC and CCM, indicating better skill separation.", "section": "F.1 Panda-gym"}, {"figure_path": "GtbwJ6mruI/figures/figures_24_2.jpg", "caption": "Figure 6: (a) UMAP visualisation of context embeddings for the SaCCM in the Panda-gym environment, with points in the yellow box representing the Push skill in high-mass tasks. Heatmap of (b) success rate, (c) Push skill probability, and (d) Pick&Place skill probability for SaCCM. In large-mass scenarios, the Push skill is more likely to be executed than Pick&Place.", "description": "This figure visualizes the context embeddings learned by SaCCM in the Panda-gym environment using UMAP.  It shows how the learned embeddings cluster based on the skills (Push and Pick&Place) employed by the agent, particularly highlighting the preference for the Push skill in high-mass scenarios. Heatmaps further illustrate the success rate and the probabilities of using each skill under different mass and friction conditions.", "section": "5.2 Panda-gym"}, {"figure_path": "GtbwJ6mruI/figures/figures_25_1.jpg", "caption": "Figure 4: A comparison of sample spaces for task e1. Positive samples Te\u2081 or T are always from current task e\u2081. For SaNCE, in a task ek with embedding ck, the positive skill \u03c0c conditions on ck and generates positive trajectories T\u03c0c, and the negative skill \u03c0c generates negative trajectories T\u03c0c. The top graphs show the relationship between c, \u03c0c and Tc.", "description": "This figure compares the sample spaces used by InfoNCE and SaNCE for training a context encoder.  InfoNCE uses positive samples from the current task and negative samples from other tasks, while SaNCE only uses samples from the current task, with positive samples from positive skills and negative samples from negative skills.  The figure illustrates how SaNCE reduces the size of the negative sample space by focusing on skill-related information.", "section": "4.3 Skill-aware noise contrastive estimation: a tighter K-sample estimator"}, {"figure_path": "GtbwJ6mruI/figures/figures_26_1.jpg", "caption": "Figure 4: A comparison of sample spaces for task e1. Positive samples Te\u2081 or T are always from current task e\u2081. For SaNCE, in a task ek with embedding ck, the positive skill \u03c0c conditions on ck and generates positive trajectories T\u03c0c, and the negative skill \u03c0 generates negative trajectories T\u03c0c. The top graphs show the relationship between c, \u03c0c and Tc.", "description": "This figure compares the sample spaces used by InfoNCE and SaNCE for training a context encoder in a meta-reinforcement learning setting.  InfoNCE uses positive samples from the current task and negative samples from other tasks. SaNCE, in contrast, utilizes both positive and negative samples from the current task, but distinguishes them based on the skill used to generate them. SaNCE's approach is shown to be more data-efficient.", "section": "4.3 Skill-aware noise contrastive estimation: a tighter K-sample estimator"}, {"figure_path": "GtbwJ6mruI/figures/figures_26_2.jpg", "caption": "Figure 4: A comparison of sample spaces for task e1. Positive samples Te\u2081 or T are always from current task e\u2081. For SaNCE, in a task ek with embedding ck, the positive skill \u03c0c conditions on ck and generates positive trajectories T\u03c0c, and the negative skill \u03c0c generates negative trajectories T\u03c0c. The top graphs show the relationship between c, \u03c0c and Tc.", "description": "This figure compares the sample spaces used by InfoNCE and SaNCE for training a context encoder. InfoNCE uses samples from different tasks to distinguish between tasks. In contrast, SaNCE, which focuses on skills, samples trajectories from the same task but uses different skills to create positive and negative samples for comparison.  The goal is to show how SaNCE can use a smaller sample space to learn an effective context embedding by focusing on skill-related information.", "section": "4.3 Skill-aware noise contrastive estimation: a tighter K-sample estimator"}, {"figure_path": "GtbwJ6mruI/figures/figures_27_1.jpg", "caption": "Figure 4: A comparison of sample spaces for task e1. Positive samples Te\u2081 or T are always from current task e\u2081. For SaNCE, in a task ek with embedding ck, the positive skill \u03c0c conditions on ck and generates positive trajectories T\u03c0c, and the negative skill \u03c0 generates negative trajectories T\u03c0c. The top graphs show the relationship between c, \u03c0c and Tc.", "description": "This figure compares three different sampling strategies for contrastive learning in meta-reinforcement learning: InfoNCE, SaNCE, and Sa+InfoNCE.  Each strategy differs in how it samples positive and negative examples for training a context encoder. InfoNCE samples positive examples from the current task and negative examples from all other tasks. SaNCE samples both positive and negative examples from the current task, but the positive samples come from the optimal skill for the task, while negative examples come from suboptimal skills. Sa+InfoNCE combines both InfoNCE and SaNCE sampling strategies. The figure illustrates the different sample spaces for each strategy, highlighting the differences in sample size and diversity.", "section": "4.3 Skill-aware noise contrastive estimation: a tighter K-sample estimator"}, {"figure_path": "GtbwJ6mruI/figures/figures_27_2.jpg", "caption": "Figure 4: A comparison of sample spaces for task e1. Positive samples Te\u2081 or T are always from current task e\u2081. For SaNCE, in a task ek with embedding ck, the positive skill \u03c0c conditions on ck and generates positive trajectories T\u03c0c, and the negative skill \u03c0c generates negative trajectories T\u03c0c. The top graphs show the relationship between c, \u03c0c and Tc.", "description": "This figure compares the sample spaces used by InfoNCE and SaNCE in the context of skill-aware mutual information optimization for meta-reinforcement learning.  It highlights how SaNCE focuses on sampling positive and negative examples from the same task, based on different skills, unlike InfoNCE which samples negative examples across multiple tasks. This difference in sampling strategies is key to SaNCE's sample efficiency.", "section": "4.3 Skill-aware noise contrastive estimation: a tighter K-sample estimator"}, {"figure_path": "GtbwJ6mruI/figures/figures_27_3.jpg", "caption": "Figure 4: A comparison of sample spaces for task e1. Positive samples Te\u2081 or T are always from current task e1. For SaNCE, in a task ek with embedding ck, the positive skill \u03c0c conditions on ck and generates positive trajectories T\u03c0c, and the negative skill \u03c0c generates negative trajectories T\u03c0c. The top graphs show the relationship between c, \u03c0c and Tc.", "description": "This figure compares the sample spaces used by InfoNCE and SaNCE for contrastive learning in a meta-reinforcement learning setting.  InfoNCE samples negative examples from different tasks (e2, e3, etc.), while SaNCE samples negative examples from the current task (e1) but generated by different skills. This highlights SaNCE's more efficient use of samples by focusing on skill-related information within the current task. The figure illustrates this difference visually via diagrams showing positive, negative, and unsampled trajectory spaces for both approaches.", "section": "4.3 Skill-aware noise contrastive estimation: a tighter K-sample estimator"}, {"figure_path": "GtbwJ6mruI/figures/figures_28_1.jpg", "caption": "Figure 15: UMAP visualisation of context embeddings extracted from trajectories collected in the Panda-gym environments.", "description": "This figure visualizes the context embeddings learned by four different Meta-RL algorithms (TESAC, CCM, SaTESAC, and SaCCM) in the Panda-gym environment using UMAP. Each point represents a trajectory, and the color indicates the mass and friction of the cube in that trajectory. The visualizations show how well each algorithm captures skill-related information in the context embeddings and whether distinct skill clusters emerge.", "section": "F.1 Panda-gym"}, {"figure_path": "GtbwJ6mruI/figures/figures_28_2.jpg", "caption": "Figure 4: A comparison of sample spaces for task e1. Positive samples Te\u2081 or T are always from current task e\u2081. For SaNCE, in a task ek with embedding ck, the positive skill \u03c0c conditions on ck and generates positive trajectories T\u03c0c, and the negative skill \u03c0c generates negative trajectories T\u03c0c. The top graphs show the relationship between c, \u03c0c and Tc.", "description": "This figure compares the sample spaces used by three different methods for contrastive learning in Meta-RL: InfoNCE, SaNCE, and Sa+InfoNCE.  It illustrates how each method samples positive and negative examples for training a context encoder. InfoNCE uses a positive sample from the current task and negative samples from other tasks. SaNCE uses both positive and negative samples from the current task, but distinguishes them based on whether the corresponding skill is optimal for the task. Sa+InfoNCE combines the approaches of InfoNCE and SaNCE. The figure highlights the difference in sample space size and the strategy of sampling positive and negative examples for each method.", "section": "4.3 Skill-aware noise contrastive estimation: a tighter K-sample estimator"}, {"figure_path": "GtbwJ6mruI/figures/figures_28_3.jpg", "caption": "Figure 4: A comparison of sample spaces for task e1. Positive samples Te\u2081 or T are always from current task e\u2081. For SaNCE, in a task ek with embedding ck, the positive skill \u03c0c conditions on ck and generates positive trajectories T\u03c0c, and the negative skill \u03c0c generates negative trajectories T\u03c0c. The top graphs show the relationship between c, \u03c0c and Tc.", "description": "This figure compares the sample spaces used by InfoNCE and SaNCE for training a context encoder.  InfoNCE uses positive samples from the current task and negative samples from other tasks. SaNCE, in contrast, samples both positive and negative samples from the current task, differentiating them based on skill (optimal vs suboptimal). SaNCE's approach reduces the sample space size needed for effective training, addressing the 'log-K curse' problem.", "section": "4.3 Skill-aware noise contrastive estimation: a tighter K-sample estimator"}, {"figure_path": "GtbwJ6mruI/figures/figures_29_1.jpg", "caption": "Figure 15: UMAP visualisation of context embeddings extracted from trajectories collected in the Panda-gym environments.", "description": "This figure visualizes the context embeddings learned by four different algorithms (TESAC, CCM, SaTESAC, and SaCCM) in the Panda-gym environment using UMAP.  The visualizations show how the algorithms represent different tasks within the environment based on the context embeddings, illustrating the different ways each algorithm captures and represents task-relevant information. It aids in understanding the effectiveness of skill-aware mutual information and its impact on context embedding generation.", "section": "F.1 Panda-gym"}, {"figure_path": "GtbwJ6mruI/figures/figures_29_2.jpg", "caption": "Figure 4: A comparison of sample spaces for task e1. Positive samples Te\u2081 or T are always from current task e1. For SaNCE, in a task ek with embedding ck, the positive skill \u03c0c conditions on ck and generates positive trajectories T\u03c0c, and the negative skill \u03c0c generates negative trajectories T\u03c0c. The top graphs show the relationship between c, \u03c0c and Tc.", "description": "This figure compares three different sampling methods for training a context encoder in a meta-reinforcement learning setting.  InfoNCE samples positive trajectories from the current task and negative samples from all other tasks.  SaNCE, in contrast, samples both positive and negative trajectories from the current task, using different skills to generate them. Sa+InfoNCE combines both approaches. The figure illustrates the different sample spaces used by each method and how this affects the learning process.", "section": "4.3 Skill-aware noise contrastive estimation: a tighter K-sample estimator"}]