[{"figure_path": "mtyy3Myyhz/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison of different typical pruning methods and illustration of discretization gap. The darker color represents the higher relative magnitude scale of weights or masks. \\textcircled{\\tiny \\texttimes} denotes Hadamard product. For ease of demonstration, we use one layer to represent the entire network.", "description": "This figure compares importance-based pruning and differentiable mask pruning (DMP). Importance-based pruning directly removes less important weights or connections based on their scores. However, DMP introduces learnable parameters to generate a weight mask, optimizing both the mask and the weights jointly. The figure highlights the \"discretization gap\" in DMP, where the performance of the final pruned network (hard network) is significantly lower than the performance of the mask-coupled network (soft network) during training.  This gap arises because the continuous relaxation architecture used to search for a good structure during training doesn't fully translate to a similarly performing discrete structure after pruning.", "section": "1 Introduction"}, {"figure_path": "mtyy3Myyhz/figures/figures_3_1.jpg", "caption": "Figure 2: The proposed pruner's forward and backward flows, illustrated via an exemplary linear layer with parameters \u03b8. The u are the additional learnable parameters normalized by softmax. The w denotes the relaxed mask. The estimated binary pruning mask is the m. The input is denoted by i. The output of the soft and hard networks are the s and h, respectively. The L, G, and R are the performance loss, gap measure, and resource regularization, respectively.", "description": "This figure illustrates the forward and backward propagation in the S2HPruner model.  It shows how the soft network and hard network are processed in parallel, with knowledge distillation used to guide the hard network's weights towards those of the soft network. The figure highlights the key components including the learnable mask parameters (u), the relaxed mask (w), the binary mask (m), and the inputs (i) and outputs (s,h) of soft and hard networks.  The backward flow illustrates the gradient calculation for both weights and the mask, with a decoupled approach to prevent performance degradation.", "section": "3 Method"}, {"figure_path": "mtyy3Myyhz/figures/figures_14_1.jpg", "caption": "Figure 2: The proposed pruner's forward and backward flows, illustrated via an exemplary linear layer with parameters \u03b8. The u are the additional learnable parameters normalized by softmax. The w denotes the relaxed mask. The estimated binary pruning mask is the m. The input is denoted by i. The output of the soft and hard networks are the s and h, respectively. The L, G, and R are the performance loss, gap measure, and resource regularization, respectively.", "description": "This figure illustrates the forward and backward passes of the proposed Soft-to-Hard Pruner (S2HPruner).  It shows how the soft network (with continuous relaxation mask w) and hard network (with binary mask m) are jointly trained using a decoupled bidirectional knowledge distillation approach. The diagram uses an exemplary linear layer to show how the gradients for the parameters (\u03b8), relaxed mask (w), and gap between soft and hard networks (G) are calculated and updated during the training process.", "section": "3 Method"}, {"figure_path": "mtyy3Myyhz/figures/figures_15_1.jpg", "caption": "Figure 1: Comparison of different typical pruning methods and illustration of discretization gap. The darker color represents the higher relative magnitude scale of weights or masks. \\textcircled{\\tiny \\times} denotes Hadamard product. For ease of demonstration, we use one layer to represent the entire network.", "description": "This figure compares importance-based pruning and differentiable mask pruning.  Importance-based pruning directly prunes weights based on importance scores, while differentiable mask pruning uses a learnable mask to guide the pruning process, optimizing a continuous relaxation (soft network) as a proxy for the final discrete network (hard network). The figure highlights the \"discretization gap\", which is the performance difference between the soft and hard networks after the discretization process. This gap is a key problem that the paper addresses.", "section": "1 Introduction"}, {"figure_path": "mtyy3Myyhz/figures/figures_17_1.jpg", "caption": "Figure 1: Comparison of different typical pruning methods and illustration of discretization gap. The darker color represents the higher relative magnitude scale of weights or masks. \\textcircled{x} denotes Hadamard product. For ease of demonstration, we use one layer to represent the entire network.", "description": "This figure compares importance-based pruning and differentiable mask pruning methods.  It highlights the discretization gap, which is the performance difference between the soft network (continuous relaxation) and the hard network (discrete pruned network) in differentiable mask pruning.  The visualization shows how the soft network achieves high performance due to the continuous weights and masks, but the hard network suffers from the discretization process, leading to a performance drop. This gap is the central problem that S2HPruner aims to solve.", "section": "1 Introduction"}, {"figure_path": "mtyy3Myyhz/figures/figures_18_1.jpg", "caption": "Figure 1: Comparison of different typical pruning methods and illustration of discretization gap. The darker color represents the higher relative magnitude scale of weights or masks. denotes Hadamard product. For ease of demonstration, we use one layer to represent the entire network.", "description": "The figure compares importance-based pruning and differentiable mask pruning methods, highlighting the discretization gap. Importance-based pruning directly removes less important weights, leading to potential structural integrity issues. Differentiable mask pruning uses a learnable mask to guide the pruning process, aiming for a better structure but still facing the discretization gap where the continuous relaxation (soft network) and the discrete pruned network (hard network) differ significantly in performance. The discretization gap is visually represented by the discrepancy between the performance of soft and hard networks.", "section": "1 Introduction"}, {"figure_path": "mtyy3Myyhz/figures/figures_19_1.jpg", "caption": "Figure 2: The proposed pruner's forward and backward flows, illustrated via an exemplary linear layer with parameters \u03b8. The u are the additional learnable parameters normalized by softmax. The w denotes the relaxed mask. The estimated binary pruning mask is the m. The input is denoted by i. The output of the soft and hard networks are the s and h, respectively. The L, G, and R are the performance loss, gap measure, and resource regularization, respectively.", "description": "This figure illustrates the forward and backward passes of the Soft-to-Hard Pruner (S2HPruner) framework.  It shows how the soft network (continuous relaxation) and hard network (discrete pruned network) are processed together,  highlighting the roles of the learnable parameters (u), relaxed mask (w), estimated binary mask (m), and the loss functions (L, G, R) in the training process. The diagram uses a simplified linear layer as an example to depict the flow of information and gradient updates.", "section": "3 Method"}, {"figure_path": "mtyy3Myyhz/figures/figures_20_1.jpg", "caption": "Figure 1: Comparison of different typical pruning methods and illustration of discretization gap. The darker color represents the higher relative magnitude scale of weights or masks. denotes Hadamard product. For ease of demonstration, we use one layer to represent the entire network.", "description": "This figure compares importance-based pruning and differentiable mask pruning methods. It illustrates how differentiable mask pruning uses a soft network (continuous relaxation) as a proxy for the hard network (discrete pruned network).  The main point is to highlight the \"discretization gap,\"  the performance difference between the soft and hard networks due to the discretization process. The visualization shows that the soft network generally achieves higher performance than the hard network after discretization.", "section": "1 Introduction"}, {"figure_path": "mtyy3Myyhz/figures/figures_21_1.jpg", "caption": "Figure 1: Comparison of different typical pruning methods and illustration of discretization gap. The darker color represents the higher relative magnitude scale of weights or masks. denotes Hadamard product. For ease of demonstration, we use one layer to represent the entire network.", "description": "This figure compares importance-based pruning and differentiable mask pruning methods, highlighting the difference in their performance and the concept of a \"discretization gap\". Importance-based pruning directly removes less important weights, often resulting in performance degradation.  Differentiable mask pruning uses a continuous relaxation of the binary mask, allowing for gradient-based optimization of the network architecture. However, the process of converting this continuous relaxation to a discrete binary mask introduces the discretization gap, where the performance of the final pruned network (hard network) is significantly lower than that of its continuous counterpart (soft network). The figure visually represents this gap using color intensity to show the magnitude of weights or masks.", "section": "1 Introduction"}]