[{"figure_path": "mtyy3Myyhz/tables/tables_5_1.jpg", "caption": "Table 1: The comparison of different pruning methods on CIFAR-100. We report the Top-1 accuracy(%) of dense and pruned networks with different remaining FLOPs.", "description": "This table compares the performance of different pruning methods (RST-S, Group-SL, OTOv2, Refill, and the proposed S2HPruner) on the CIFAR-100 dataset using ResNet-50, MBV3, and WRN28-10 network architectures.  The Top-1 accuracy is reported for each method at three different levels of remaining FLOPs (15%, 35%, and 55%).  This allows for a comparison of accuracy versus computational cost.", "section": "4.1 Benchmarking"}, {"figure_path": "mtyy3Myyhz/tables/tables_5_2.jpg", "caption": "Table 2: The comparison of different pruning methods on Tiny ImageNet. We report the Top-1 accuracy(%) of dense and pruned networks with different remaining FLOPs.", "description": "This table compares the performance of different pruning methods (RST-S, Group-SL, OTOv2, Refill, and the proposed S2HPruner) on the Tiny ImageNet dataset.  The results are presented for three different levels of remaining FLOPs (15%, 35%, and 55%) for three different network architectures (ResNet-50, MBV3, and WRN28-10). Each cell shows the Top-1 accuracy achieved by each method under the specified conditions. The table demonstrates the superior performance of S2HPruner compared to existing methods, especially at lower FLOP constraints.", "section": "4.1 Benchmarking"}, {"figure_path": "mtyy3Myyhz/tables/tables_5_3.jpg", "caption": "Table 3: Verifications of transformers on CIFAR-100. We report the Top-1 accuracy(%) of dense and pruned networks with different remaining FLOPs.", "description": "This table shows the Top-1 accuracy results for two transformer models, ViT and Swin Transformer, on the CIFAR-100 dataset after applying different pruning ratios (15%, 35%, and 55%).  It compares the performance of the proposed S2HPruner method against the RST-S method, demonstrating the effectiveness of S2HPruner across various network architectures and pruning levels.", "section": "4.1 Benchmarking"}, {"figure_path": "mtyy3Myyhz/tables/tables_6_1.jpg", "caption": "Table 4: Results of ResNet-50 on Imagenet. We report the Top-1 accuracy(%) of dense and pruned networks with different remaining FLOPs. The Epr denotes the pruning epochs. The Eex denotes the epochs for extra stages (such as pretraining and finetuning). The pruning epochs can be undetermined due to dynamic termination conditions, and corresponding terms are marked as \"-\".", "description": "This table compares the performance of the proposed S2HPruner against other state-of-the-art pruning methods on the ImageNet dataset using ResNet-50 as the base model. It shows the Top-1 accuracy and remaining FLOPs after pruning, along with the number of pruning and extra epochs.  The table highlights the performance gains of S2HPruner, particularly at lower FLOPs.", "section": "4.1 Benchmarking"}, {"figure_path": "mtyy3Myyhz/tables/tables_7_1.jpg", "caption": "Table 5: The influence of different gradient components in the proposed pruning method. The FLOPs target is set to 15% for all experiments.", "description": "This table presents an ablation study, analyzing the impact of different gradient components on the performance of the S2HPruner model.  By selectively including or excluding gradient terms (indicated by checkmarks and Xs), the researchers evaluated how each component contributes to the model's overall accuracy when a 15% FLOPs reduction is targeted.  The results show the importance of specific gradient components for achieving high accuracy in the pruned network.", "section": "4.2 Gradient analysis"}, {"figure_path": "mtyy3Myyhz/tables/tables_7_2.jpg", "caption": "Table 6: Gap comparison with alternative formulations of the problem 1. The symbols \u03b8, \u03b8\u2299w and \u03b8(m) represent the top-1 accuracy of the original, soft and hard networks, respectively.", "description": "This table compares three different problem formulations for network pruning: the proposed method (Ours), an alternative that directly optimizes the hard network (Alt 1), and an alternative that uses self-distillation (Alt 2).  It evaluates the gap between the soft and hard networks using Jensen-Shannon divergence (JS) and L2 distance.  The results demonstrate that the proposed method effectively bridges the discretization gap, resulting in superior performance.", "section": "4.3 Investigation into gap"}, {"figure_path": "mtyy3Myyhz/tables/tables_8_1.jpg", "caption": "Table 7: The top-1 accuracy of the hard network at different fine-tuning epochs. The top-1 accuracy of the solely trained soft network before fine-tuning is 79.41%. The symbols  \u03b8\u2299w and \u03b8(m) represent the top-1 accuracy of the soft and hard networks, respectively.", "description": "This table shows the top-1 accuracy of both the soft network (\u03b8\u2299w) and the hard network (\u03b8(m)) at different fine-tuning epochs (10, 50, 100, 250, 500).  The purpose is to demonstrate the effect of fine-tuning on the hard network's accuracy after the coupled training phase.  The initial accuracy of the soft network before fine-tuning is 79.41%.", "section": "4.3 Investigation into gap"}, {"figure_path": "mtyy3Myyhz/tables/tables_8_2.jpg", "caption": "Table 8: The top-1 accuracy of different networks pruned from ResNet-50 with a 15% FLOPs constraint and then trained from scratch without bells and whistles.", "description": "This table presents the Top-1 accuracy results of four different ResNet-50 networks pruned to 15% FLOPs. Three networks were randomly pruned, and one network was pruned using the proposed Soft-to-Hard Pruner (S2HPruner). The results demonstrate that the S2HPruner outperforms the randomly pruned networks in terms of accuracy, showcasing the architectural superiority of the proposed method. ", "section": "4.4 Architectural superiority"}, {"figure_path": "mtyy3Myyhz/tables/tables_16_1.jpg", "caption": "Table 9: The pruning results obtained via training a ResNet-50 on CIFAR-100 with different random seeds using our proposed method. We report the Top-1 accuracy and FLOPs.", "description": "This table presents the robustness of the proposed pruning method against randomness. Four independent experiments were conducted using different random seeds while keeping the same settings. The results demonstrate the consistency and reliability of the method, as the Top-1 accuracy and FLOPs remain stable across different runs, with negligible variations.", "section": "4.1 Benchmarking"}, {"figure_path": "mtyy3Myyhz/tables/tables_16_2.jpg", "caption": "Table 10: Training efficiency comparison with different methods. For a fair comparison, double-epoch training results of other methods are included.", "description": "This table compares the training efficiency of the proposed S2HPruner method with other existing structured pruning methods. The comparison is based on ResNet-50 on the CIFAR-100 dataset.  Metrics include Top-1 accuracy (both single and double epoch training), GPU time per epoch, and peak GPU memory usage during training and inference. The double-epoch training results for other methods are included to ensure a fairer comparison, as the authors' method trains a soft and a hard network simultaneously, thus needing less epochs.", "section": "4 Experiments"}]