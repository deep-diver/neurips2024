{"references": [{"fullname_first_author": "Geoffrey Hinton", "paper_title": "Distilling the knowledge in a neural network", "publication_date": "2015-00-00", "reason": "This paper introduces the concept of knowledge distillation, a technique used in the target paper to bridge the discretization gap in model pruning."}, {"fullname_first_author": "Song Han", "paper_title": "Learning both weights and connections for efficient neural network", "publication_date": "2015-00-00", "reason": "This is a seminal work in model pruning, introducing the concept of pruning both weights and connections, which is relevant to the target paper's approach."}, {"fullname_first_author": "Kaiming He", "paper_title": "Deep residual learning for image recognition", "publication_date": "2016-00-00", "reason": "ResNet is a widely used architecture in deep learning, and its use in the target paper's experiments provides a strong benchmark for comparison."}, {"fullname_first_author": "Zhuang Liu", "paper_title": "Learning efficient convolutional networks through network slimming", "publication_date": "2017-00-00", "reason": "This paper introduces network slimming, a technique related to the target paper's approach of model pruning."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-00-00", "reason": "The Transformer architecture, introduced in this paper, is used in the target paper's experiments, showcasing the breadth of applicability of the proposed pruning method."}]}