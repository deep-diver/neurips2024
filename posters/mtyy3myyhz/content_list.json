[{"type": "text", "text": "S2HPruner: Soft-to-Hard Distillation Bridges the Discretization Gap in Pruning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Weihao $\\mathbf{Lin^{1\\dagger}}$ , Shengji $\\mathbf{Tang^{1\\dagger}}$ , Chong $\\mathbf{Y}\\mathbf{u}^{2}$ , Peng $\\mathbf{Y}\\mathbf{e}^{3}$ , Tao Chen1\u2217 ", "page_idx": 0}, {"type": "text", "text": "1School of Information Science and Technology, Fudan University, Shanghai, China, 2Academy for Engineering and Technology, Fudan University, Shanghai, China, 3Shanghai AI Laboratory, Shanghai, China eetchen@fudan.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recently, differentiable mask pruning methods optimize the continuous relaxation architecture (soft network) as the proxy of the pruned discrete network (hard network) for superior sub-architecture search. However, due to the agnostic impact of the discretization process, the hard network struggles with the equivalent representational capacity as the soft network, namely discretization gap, which severely spoils the pruning performance. In this paper, we first investigate the discretization gap and propose a novel structural differentiable mask pruning framework named S2HPruner to bridge the discretization gap in a one-stage manner. In the training procedure, S2HPruner forwards both the soft network and its corresponding hard network, then distills the hard network under the supervision of the soft network. To optimize the mask and prevent performance degradation, we propose a decoupled bidirectional knowledge distillation. It blocks the weight updating from the hard to the soft network while maintaining the gradient corresponding to the mask. Compared with existing pruning arts, S2HPruner achieves surpassing pruning performance without fine-tuning on comprehensive benchmarks, including CIFAR-100, Tiny ImageNet, and ImageNet with a variety of network architectures. Besides, investigation and analysis experiments explain the effectiveness of S2HPruner. Codes are publicly available on GitHub: https://github.com/opposj/S2HPruner. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "As deep neural networks (DNN) have achieved success in substantial fields [20, 58, 35, 61, 49], the increasing computation and storage cost of DNN impedes practical implementation. Model pruning [39, 57, 62], which aims at removing the less informative in a cumbersome network, has been a widespread technique for model compression. Pioneer pruning methods utilize regularization terms [63, 43] to sparsify the network or introduce importance metrics [30, 19, 18] to remove less important weights directly. However, due to the latent correlations between weights, simply eliminating the weights in an over-parameter model will hinder the integrality of structure, especially in structural pruning, where grouped filters are removed. ", "page_idx": 0}, {"type": "text", "text": "Recently, it has been pointed out that the structure of the pruned network is essential for the final pruning performance [40]. Inspired by the differentiable architecture search (DARTS) [36, 67, 7], emerging works [15, 17, 16], namely differentiable mask pruning (DMP), introduce learnable parameters to generate the weight mask and impose the task-aware gradient to guide the structure search of the pruned network. In the training procedure, DMP introduces the learnable mask into the gradient graph by coupling the mask with the activation feature or weights, e.g., directly multiplying ", "page_idx": 0}, {"type": "image", "img_path": "mtyy3Myyhz/tmp/2b5c7bf03e96a2baf797ab9e39beb2f026bcfd51d6a2cf396818a1fc28353dfd.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Comparison of different typical pruning methods and illustration of discretization gap. The darker color represents the higher relative magnitude scale of weights or masks. $\\odot$ denotes Hadamard product. For ease of demonstration, we use one layer to represent the entire network. ", "page_idx": 1}, {"type": "text", "text": "the mask with the feature or weights. Through gradient descent, DMP can jointly optimize the weights and mask parameters for a bespoke structure and parameter distribution, thus causing a better performance. The search procedure essentially regards the mask-coupled network (soft network) as the performance proxy of the final discretized compact pruned network (hard network). Whereas, considering the aim of pruning is to obtain a capable hard network, a natural question is whether a superior soft network implies a corresponding high-performance hard network. ", "page_idx": 1}, {"type": "text", "text": "In DARTS, there is a problem known as the discretization gap [66, 60, 7], which refers to the discrepancy between the continuous relaxation architecture and the discrete architecture due to the discretization process. Since DMP follows a similar modeling format to DARTS, it also faces a comparable discretization gap problem\u2020 that the hard network struggles from having the semblable representational capacity as the soft network. A specific manifestation is that the hard network performs significantly poorer in the evaluation metrics than the soft network. Fig. 1 visually exhibits the different pruning methods and discretization gap. The discretization gap severely impacts pruning performance but has been long overlooked in DMP. There are potential techniques that may alleviate the discretization gap in previous works, e.g., gradually facilitating the steepness of the Sigmoid function via decaying temperature [26, 27, 44, 50] and optimizing the binary mask via the straightthrough estimator (STE) [65, 15]. However, these methods lead to certain side effects: the decaying temperature results in difficult mask optimization because of the vanishing gradient, and STE causes a suboptimal mask due to the coarse gradient. ", "page_idx": 1}, {"type": "text", "text": "To alleviate the discretization gap in DMP without influencing mask optimization, we formulate the mask pruning in a soft-to-hard paradigm and propose a structured differentiable mask pruning framework named Soft-to-Hard Pruner (S2HPruner). Specifically, in the training procedure, we not only forward the soft network for the structural search but also forward the corresponding hard network and distill it under the supervision of the soft network to reduce the discretization gap. Meanwhile, we discover that even with the same corresponding hard network, the distribution of the mask parameters influences the discretization gap essentially. However, the common unidirectional knowledge distillation (KD) cannot optimize mask parameters directly, but bidirectional KD causes unbearable performance degradation. Therefore, we propose a decoupled bidirectional KD, which blocks the weight updating from the hard to the soft network while keeping the gradient corresponding to the mask. Exhaustive experiments on three mainstream classification datasets, including CIFAR100, Tiny ImageNet, and ImageNet, demonstrate the effectiveness of S2HPruner. ", "page_idx": 1}, {"type": "text", "text": "Our contributions are summarised as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We first study and reveal the long-standing overlooked discretization gap problem in differentiable mask pruning. To alleviate it, we propose a soft-to-hard distillation paradigm, which distills the hard network under the supervision of the soft network. \u2022 Based on the soft-to-hard knowledge distillation paradigm, we propose a novel differentiable mask pruning framework named Soft-to-Hard Pruner (S2HPruner). To further reduce the discretization gap and avoid performance degradation, we propose a decoupled bidirectional KD which blocks and allows the gradient of model weights and mask parameters selectively. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "\u2022 Extensive experiments on three mainstream datasets and five architectures verify the superiority of S2HPruner, e.g., maintaining $96.17\\%(\\mathrm{Top-1}$ accuracy $73.23\\%$ in $76.15\\%$ ) with around $15\\%$ FLOPs. Additional ablation and investigation experiments demonstrate the underlying mechanism of the effectiveness. ", "page_idx": 2}, {"type": "text", "text": "2 Related works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Differentiable mask pruning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Considering the network structure has a decisive impact on the pruning performance [40], numerous works [15, 12] train a binary mask for an optimal selection of sub-architecture. However, because of the non-differentiable property, directly optimizing the binary mask is very challenging and even impairs the performance [26]. Differently, differentiable mask pruning (DMP) methods [17, 9, 4, 27, 44] adopt differentiable continuous relaxation as a performance proxy of the hard network for structure search, which can be easily optimized by task-aware loss end-to-end. DMCP [17] regards the channel pruning as a Markov process and builds a differentiable mask based on the transitions between states. AutoPruner [44] proposes to construct a meta-network to generate the differentiable mask according to the activation responses, and a scaled temperature facilitates the sigmoid function approaching step function to obtain an approximate binary mask. GAL [34] learns a differentiable mask by optimizing a generative adversarial learning task in a label-free and end-to-end manner. However, the task-aware loss can ensure the high performance of the soft network but not the final hard network. There is a discretization gap limiting the target hard network during the discretization process. Different from previous DMP methods, which only focus on optimizing the soft network, our approach aims to achieve a high-performance hard network by reducing the discretization gap through soft-to-hard distillation. ", "page_idx": 2}, {"type": "text", "text": "2.2 Pruning with distillation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "As a network compression technique orthogonal to pruning, knowledge distillation [22, 28, 52] (KD) transfers the dark knowledge from a large teacher network to enhance a compact student network. Recently, there have been substantial works [46, 47, 3, 32, 10] introducing KD into model pruning to further boost the pruned network. JMC [10] proposes a structured pruning based on the magnitude of weights and a many-to-one layer mapping strategy to distill the dense model to the pruned one. KD ticket [46] exploits the dark knowledge in the early stage of iterative magnitude pruning to boost the lottery tickets in the dense model. DIPNet [72] improves the ability of the pruned model by the supervision of high-resolution output. The above methods treat KD as an independent plug-in technique to enhance pruning performance without tight coupling with the selection of weights. Differently, in the proposed method, KD contributes to mask optimization directly as an integral part of the core pruning procedure. Moreover, in contrast to the typical unidirectional KD, we propose a novel decoupled bidirectional KD to alleviate the discretization gap between soft and hard networks, due to the distinct attributes of mask and weights. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Problem formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Given a network with parameters $\\pmb{\\theta}$ , a pruning algorithm generates a binary mask $\\mathbf{\\nabla}m$ via solving the following constraint optimization: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta,m}\\mathcal{L}\\left(\\pmb{\\theta}\\left\\langle\\pmb{m}\\right\\rangle\\right)\\quad\\mathrm{s.~t.~}\\mathcal{R}\\left(\\pmb{m},T\\right)=0.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The $\\theta\\left<m\\right>$ are the remaining parameters after pruning. The $\\mathcal{L}$ and $\\mathcal{R}$ are the task-specific performance loss and resource regularization, respectively. The $T$ is a manually assigned resource budget. Intuitively, a pruning algorithm attains a slimmed subnet that optimally balances the performance and the resource consumption. ", "page_idx": 2}, {"type": "image", "img_path": "mtyy3Myyhz/tmp/caa90275ad1fb50a55cbb1e311e99537ee7557dde41c1bec471623c348a5ec7f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: The proposed pruner\u2019s forward and backward flows, illustrated via an exemplary linear layer with parameters $\\pmb{\\theta}$ . The $\\textbf{\\em u}$ are the additional learnable parameters normalized by softmax. The $\\pmb{w}$ denotes the relaxed mask. The estimated binary pruning mask is the $\\hat{\\pmb{m}}$ . The input is denoted by $\\pmb{i}$ . The output of the soft and hard networks are the $\\pmb{s}$ and $^h$ , respectively. The ${\\mathcal{L}},{\\mathcal{G}}.$ , and $\\mathcal{R}$ are the performance loss, gap measure, and resource regularization, respectively. ", "page_idx": 3}, {"type": "text", "text": "3.2 Overview ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Directly optimizing the problem 1 is almost intractable due to the discreteness of $\\mathbf{\\nabla}m$ . To get around, we introduce a relaxation of $\\mathbf{\\nabla}m$ as $\\pmb{w}$ , which is continuous and bounded to $[0,1]$ . The $i$ -th element in $\\mathbf{\\nabla}w$ represents the probability of the $i$ -th parameter being retained. Consequently, a differentiable representative for $\\theta\\left<m\\right>$ can be constructed as ${\\pmb\\theta}\\odot{\\pmb w}$ , where the $\\odot$ denotes the Hadamard product. Based on this relaxation, the problem 1 can be reformulated as two parts: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Part\\,1!:}\\underset{\\pmb{\\theta},\\pmb{w}}{\\operatorname*{min}}\\left(\\mathcal{L}\\left(\\pmb{\\theta}\\odot\\pmb{w}\\right)+\\alpha\\mathcal{R}\\left(\\pmb{w},T\\right)\\right),}\\\\ &{\\mathrm{Part\\,2!:}\\underset{\\pmb{\\theta},\\pmb{w}}{\\operatorname*{min}}\\,\\mathcal{G}\\left(\\pmb{\\theta}\\left\\langle\\pmb{\\hat{m}}\\right\\rangle,\\pmb{\\theta}\\odot\\pmb{w}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The $\\alpha$ is a Lagrangian multiplier, regarded as a hyperparameter. The $\\mathcal{G}$ is a gap measure, reflecting the difference between $\\pmb{\\theta}\\left<\\hat{m}\\right>$ and $\\pmb{\\theta}\\odot\\pmb{w}$ . The $\\hat{m}$ is an estimated pruning mask, derived from $\\pmb{w}$ as $\\mathbb{I}_{[t,1]}\\left(\\pmb{w}\\right)$ , where the $\\mathbb{I}$ is an indicator function, and the t is a threshold. In the problem 2, the first part searches for a high-performance soft network that satisfies the resource constraint, and the second part reduces the gap between the hard network and the soft one. Similar to [36, 33], to avoid alternate optimization, we combine the two parts with two additional hyperparameters $\\beta$ and $\\gamma$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pmb{\\theta},\\pmb{w}}\\big(\\beta\\mathcal{L}\\left(\\pmb{\\theta}\\odot\\pmb{w}\\right)+\\alpha\\beta\\mathcal{R}\\left(\\pmb{w},T\\right)+\\gamma\\mathcal{G}\\left(\\pmb{\\theta}\\left\\langle\\pmb{\\hat{m}}\\right\\rangle,\\pmb{\\theta}\\odot\\pmb{w}\\right)\\big).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The problem 3 is differentiable w.r.t. both $\\pmb{\\theta}$ and $\\pmb{w}$ , thus can be optimized by gradient-based methods [41, 54]: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta\\theta=-\\lambda_{\\theta}\\left(\\beta g_{\\mathcal{L}\\rightarrow\\theta\\odot w\\rightarrow\\theta}+\\gamma g_{\\mathcal{G}\\rightarrow\\theta\\langle\\dot{m}\\rangle\\rightarrow\\theta}+\\gamma g_{\\mathcal{G}\\rightarrow\\theta\\odot w\\rightarrow\\theta}\\right),}\\\\ &{\\Delta w=-\\lambda_{w}\\left(\\beta g_{\\mathcal{L}\\rightarrow\\theta\\odot w\\rightarrow w}+\\alpha\\beta g_{\\mathcal{R}\\rightarrow w}+\\gamma g_{\\mathcal{G}\\rightarrow\\theta\\odot w\\rightarrow w}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The $\\lambda_{\\theta}$ and $\\lambda_{w}$ are learning rates for $\\pmb{\\theta}$ and $\\pmb{w}$ , respectively. The $g_{X}$ denotes the gradient obtained via a backward path $X$ . Note that the term $g_{\\mathcal{G}\\rightarrow\\theta\\odot w\\rightarrow\\theta}$ implies aligning the soft network towards the hard one, which would severely deteriorate the performance of the soft network (see Section 4.2 for details). Consequently, the update of $\\pmb{\\theta}$ is modified to: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Delta\\pmb{\\theta}=-\\lambda_{\\pmb{\\theta}}\\left(\\beta g_{\\mathcal{L}\\rightarrow\\pmb{\\theta}\\odot w\\rightarrow\\pmb{\\theta}}+\\gamma\\pmb{g}_{\\mathcal{G}\\rightarrow\\pmb{\\theta}\\langle\\hat{m}\\rangle\\rightarrow\\pmb{\\theta}}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The essence of the above optimization lies in two aspects: 1) the joint optimization of the entire parameters ${\\pmb\\theta}\\odot{\\pmb w}$ and a dynamic subset of parameters $\\pmb\\theta\\left<\\hat{m}\\right>$ beneftis from stimulative training [68], where the entire parameters transfer knowledge to the partial ones, and the improvement of the partial parameters can, in turn, enhance the entire ones; 2) the optimization of $\\mathbf{\\nabla}w$ involves the soft-to-hard gap, which provides a new dimension to bridge the gap besides adjusting the parameters. The pseudocode describing the whole training process can be referred to in Algorithm 1, and a visualization of the forward/backward passes is provided in Fig. 2. ", "page_idx": 3}, {"type": "text", "text": "Input: Initialized $\\pmb{\\theta}^{0}$ and $\\pmb{w}^{0}$ , iteration limit $i_{m a x}$ , dataset $\\mathcal{D}$ , network forward function $\\mathcal{N}$ , resource budget $T$ , performance metric $\\mathcal{L}$ , resource regularization $\\mathcal{R}$ , gap measure $\\mathcal{G}$ , pruning threshold $t$ , gradient-based optimizer $\\scriptscriptstyle\\mathcal{O}$ , hyperparameters $\\alpha$ , $\\beta$ , and $\\gamma$ ", "page_idx": 4}, {"type": "text", "text": "$\\theta^{i_{m a x}}$ $\\hat{\\pmb{m}}^{i_{m a x}}=\\mathbb{I}_{[t,1]}\\left({\\pmb{w}}^{i_{m a x}}\\right)$ ", "page_idx": 4}, {"type": "text", "text": "Output: and   \n1 $i\\gets0$ ;   \n2 while $i<i_{m a x}$ do   \n3 Fetch a sample $\\textbf{\\em x}$ with its label $\\textit{\\textbf{y}}$ from $\\mathcal{D}$ ;   \n4 $\\pmb{y}_{s}\\leftarrow\\mathcal{N}\\left(\\pmb{\\theta}^{i}\\odot\\pmb{w}^{i}\\right)$ ; $//$ The forward pass of the soft network   \n5 $\\pmb{y}_{h}\\leftarrow\\mathcal{N}\\left(\\pmb{\\theta}^{i}\\left\\langle\\mathbb{I}_{[t,1]}\\left(\\pmb{w}^{i}\\right)\\right\\rangle\\right)$ ; $//$ The forward pass of the hard network   \n6 $l\\leftarrow\\mathcal{L}\\left(\\pmb{{y}}_{s},\\pmb{{y}}\\right)$ ; $r\\gets\\mathcal{R}\\left(\\pmb{w}^{i},T\\right)$ ;   \n7 $d_{1}\\gets\\mathcal{G}\\left(y_{h},y_{s}.\\,\\mathrm{detach}\\left(\\right)\\right);d_{2}\\gets\\mathcal{G}\\left(y_{h}.\\,\\mathrm{detach}\\left(\\right),y_{s}\\right);$   \n8 $\\left(g_{\\mathcal{L}\\to\\theta\\odot w\\to\\theta},g_{\\mathcal{L}\\to\\theta\\odot w\\to w},g_{\\mathcal{R}\\to w}\\right)\\gets\\left(l+r\\right).\\mathrm{t}$ ackward ();   \n9 $g_{\\mathcal{G}\\to\\theta\\langle\\hat{m}\\rangle\\to\\theta}\\leftarrow d_{1}$ . backward ();   \n10 gG\u2192\u03b8\u2299w\u2192w \u2190d2. backward inputs = wi ;   \n11 $\\begin{array}{r l}&{\\theta^{i+1}\\leftarrow\\mathcal{O}\\Big(i,\\theta^{i},\\beta g_{\\mathcal{L}\\rightarrow\\theta\\odot w\\rightarrow\\theta}+\\gamma g_{\\mathcal{G}\\rightarrow\\theta\\langle\\hat{m}\\rangle\\rightarrow\\theta}\\Big);}\\\\ &{w^{i+1}\\leftarrow\\mathcal{O}\\big(i,w^{i},\\beta g_{\\mathcal{L}\\rightarrow\\theta\\odot w\\rightarrow w}+\\alpha\\beta g_{\\mathcal{R}\\rightarrow w}+\\gamma g_{\\mathcal{G}\\rightarrow\\theta\\odot w\\rightarrow w}\\big);}\\end{array}$ // Eq. 5   \n12 // Eq. 4   \n13 i \u2190i + 1 ", "page_idx": 4}, {"type": "text", "text": "3.3 Implementation details ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We focus on dependency-group-based structural pruning [6, 14], where layers in the same group share a single mask and are pruned as a whole. Besides, the pruning mask is channel-wise to comply with the structural pattern. The performance metric $\\mathcal{L}$ is the cross-entropy for classification. The Kullback-Leibler divergence is selected as the gap measure $\\mathcal{G}$ . ", "page_idx": 4}, {"type": "text", "text": "Acquisition of $\\mathbf{\\nabla}w$ and $t$ Consider a linear layer parameterized by $\\pmb{\\theta}\\in\\mathbb{R}^{C_{o u t}\\times C_{i n}}$ . The corresponding binary pruning mask is denoted as $m\\in\\dot{\\mathbb{B}}^{C_{o u t}}$ . To generate $\\pmb{w}$ , we define learnable parameters $\\bar{\\b u}\\breve{\\in}\\mathbb{R}^{C_{o u t}}$ , which can be normalized to $[0,1]$ via a softmax function. After softmax, the $i$ -th element in $\\textbf{\\em u}$ can be interpreted as the probability of retaining the first $i$ channels. Consequently, the probability of the $i$ -th channel being retained, i.e., $w_{i}$ , can be calculated as $\\sum_{k=i}^{C_{o u t}}u_{k}$ . With the $\\pmb{w}$ obtained, the pruning threshold t is derived asCo1ut $\\textstyle{\\frac{1}{C_{o u t}}}\\sum_{k=1}^{C_{o u t}}w_{k}$ ", "page_idx": 4}, {"type": "text", "text": "Resource regularization We utilize floating-point operations per second (FLOPs) to evaluate resource consumption. Given a target $T$ (in percentage), the resource regularization $\\mathcal{R}$ is defined as $\\left(\\mathrm{FP}_{s o f t}\\,/\\,\\mathrm{FP}_{a l l}-T\\right)^{2}$ . The $\\mathrm{FP}_{a l l}$ is the FLOPs of the entire network. The $\\operatorname{FP}_{s o f t}$ is the summation of layer-wise differentiable FLOPs. To be differentiable, the output channel number of a layer is calculated as $\\sum_{k=1}^{C_{o u t}}\\left(u_{k}*k\\right)$ . The $u_{k}$ is a softmaxed parameter introduced in the previous section. ", "page_idx": 4}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we begin by validating the effectiveness of the proposed pruner using three benchmark datasets: CIFAR-100 [29], Tiny ImageNet [11], and ImageNet [11]. For CIFAR-100 and Tiny ImageNet, we evaluate three common CNN architectures, i.e., ResNet-50 [20], MobileNetV3 (MBV3) [24], and WRN28-10 [73], and two Transformer architectures, i.e., ViT [61] and Swin Transformer [37], across various pruning ratios including $15\\%$ , $35\\%$ , and $55\\%$ . For ImageNet, ResNet-50 serves as the backbone model, and we compare the proposed pruner with several structural pruning methods in terms of Top-1 accuracy and FLOPs. After the benchmarking, investigative experiments are performed on CIFAR-100 using ResNet-50 to elucidate the influence of each gradient term in Algorithm 1 and the gap-narrowing capacity of the proposed pruner. Detailed training configurations are provided in the Appendix. ", "page_idx": 4}, {"type": "table", "img_path": "mtyy3Myyhz/tmp/0f66577d63c832bab151476b28d974cbc6c8da1766590de8522574a6b0bb7183.jpg", "table_caption": ["Table 1: The comparison of different pruning methods on CIFAR-100. We report the Top-1 accuracy $(\\%)$ of dense and pruned networks with different remaining FLOPs. "], "table_footnote": [], "page_idx": 5}, {"type": "table", "img_path": "mtyy3Myyhz/tmp/0b65a529b6979ad4d2fff8532138e6c667194c6e75fbc54f0c5c7941a588790c.jpg", "table_caption": ["Table 2: The comparison of different pruning methods on Tiny ImageNet. We report the Top-1 accuracy $(\\%)$ of dense and pruned networks with different remaining FLOPs. "], "table_footnote": [], "page_idx": 5}, {"type": "table", "img_path": "mtyy3Myyhz/tmp/888f022a591a440da26a271f845479d7302e712a3a5416faea3d469c47fd5e84.jpg", "table_caption": ["Table 3: Verifications of transformers on CIFAR-100. We report the Top-1 accuracy $\\%)$ of dense and pruned networks with different remaining FLOPs. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "4.1 Benchmarking ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Results on CIFAR-100 and Tiny ImageNet To assess the performance of the proposed pruner and demonstrate its adaptability to various networks, we conduct experiments using CIFAR-100 and Tiny ImageNet datasets, with ResNet-50, MBV3, and WRN28-10 serving as the backbone architectures. For each dataset-network combination, we test three different FLOPs: $15\\%$ , $35\\%$ , and $55\\%$ . We compare the proposed pruner against structured RST [1] (referred to as RST-S), Group-SL [14], OTOv2 [6], and Refill [5]. All methods are evaluated under consistent training settings for a fair comparison. The results, presented in Table 1 and Table 2, reveal that the proposed pruner consistently outperforms other methods, particularly at low FLOPs. For instance, when constraint with $15\\%$ FLOPs, the proposed pruner maintains high accuracy, with gains of up to $2.73\\%$ on CIFAR-100 and $3.99\\%$ on Tiny ImageNet over the next best method. ", "page_idx": 5}, {"type": "text", "text": "To further validate the generalizability of the proposed pruner, we apply it to two typical Transformer models, ViT [61] and Swin Transformer [37]. Similar to the CNN experiments, we test these models on CIFAR-100 with FLOPs targets of $15\\%$ , $35\\%$ , and $55\\%$ . The results, shown in Table 3, indicate that the proposed pruner outperforms RST-S for both Transformer models across all FLOPs targets. Notably, at $55\\%$ FLOPs, the ViT pruned by the proposed method does not suffer any performance loss, and the Swin Transformer merely experiences a slight performance drop of $0.47\\%$ . The results demonstrate that while the proposed pruner is not explicitly designed for Transformers, it still achieves competitive results, highlighting its significant potential for pruning Transformer models. ", "page_idx": 5}, {"type": "text", "text": "Results on ImageNet We further assess the performance of the proposed pruner on the prevalent ImageNet-1K benchmark. The ResNet-50 is chosen as the baseline network. Table 4 shows that, for similar FLOPs, the proposed pruner consistently suffers the least accuracy drop compared to others, underscoring the effectiveness of the proposed pruner. In the particularly challenging low FLOPs range of $10\\%$ to $20\\%$ , the proposed pruner stands out, achieving a top-1 accuracy of $73.23\\%$ , which is $3.13\\%$ higher than OTOv2, while maintaining nearly the same FLOPs (around $15\\%$ ). ", "page_idx": 5}, {"type": "text", "text": "4.2 Gradient analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To investigate the influence of each gradient term in Algorithm 1, we conduct experiments with some of the terms disabled to observe the impact on the final performance. The results are shown in Table 5. ", "page_idx": 5}, {"type": "table", "img_path": "mtyy3Myyhz/tmp/a353ee14164548cab455f71186a32a9cd82848ea16ca28569f243fbc177a3d37.jpg", "table_caption": ["Table 4: Results of ResNet-50 on Imagenet. We report the Top-1 accuracy $\\%)$ of dense and pruned networks with different remaining FLOPs. The $E_{p r}$ denotes the pruning epochs. The $E_{e x}$ denotes the epochs for extra stages (such as pretraining and finetuning). The pruning epochs can be undetermined due to dynamic termination conditions, and corresponding terms are marked as \u201c-\". "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Note that the term $g_{\\mathcal{R}\\to w}$ is omitted from Table 5 since it is essential to satisfy the resource constraint and is always enabled. ", "page_idx": 6}, {"type": "text", "text": "The addition of the term $g_{\\mathcal{G}\\rightarrow\\theta\\odot w\\rightarrow\\theta}$ severely degrades the accuracy by $14.22\\%$ , indicating that the gradient that aligns the soft network towards the hard one is detrimental to the final performance. Intuitively, from the perspective of parameter capacity, the hard network is practically pruned, resulting in a lower capacity than the soft network. Enforcing the soft network moving towards a less capable one is not plausible. ", "page_idx": 6}, {"type": "text", "text": "Both of the term ${\\pmb g}_{\\mathcal{L}\\rightarrow{\\pmb\\theta}\\odot{\\pmb w}\\rightarrow{\\pmb w}}$ and $g_{\\mathcal{G}\\rightarrow\\pmb{\\theta}\\odot\\pmb{w}\\rightarrow\\pmb{w}}$ contribute to improve the accuracy. For the term ${\\pmb g}_{\\mathcal{L}\\rightarrow{\\pmb\\theta}\\odot{\\pmb w}\\rightarrow{\\pmb w}}$ , it implies searching for a mask that maximizes the performance of the soft network. The term gG\u2192\u03b8\u2299w\u2192w encourages the alignment of the soft and hard networks. Different from the term ", "page_idx": 6}, {"type": "table", "img_path": "mtyy3Myyhz/tmp/f045fcc3bebb1d6cf0f72d159313577082ff2ff5158758dcc466c09ab8376927.jpg", "table_caption": ["Table 5: The influence of different gradient components in the proposed pruning method. The FLOPs target is set to $15\\%$ for all experiments. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 6: Gap comparison with alternative formulations of the problem 1. The symbols $\\pmb{\\theta}$ , $\\theta\\odot w$ and $\\pmb{\\theta}\\left<\\hat{m}\\right>$ represent the top-1 accuracy of the original, soft and hard networks, respectively. ", "page_idx": 7}, {"type": "table", "img_path": "mtyy3Myyhz/tmp/e464b1f812d519162442cb7cd17eb0627c767d8072d65eaddb70990d355bafb8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "$g_{\\mathcal{G}\\rightarrow\\theta\\odot w\\rightarrow\\theta}$ , which directly imposes on massive parameters, the term $g_{\\mathcal{G}\\rightarrow\\pmb{\\theta}\\odot\\pmb{w}\\rightarrow\\pmb{w}}$ merely affects the learnable masks, and thus would not drastically deteriorate the soft network while improving the hard one. ", "page_idx": 7}, {"type": "text", "text": "The gradient term $g_{\\mathcal{G}\\rightarrow\\theta\\langle\\hat{m}\\rangle\\rightarrow\\theta}$ and $g_{\\mathcal{L}\\rightarrow\\theta\\odot w\\rightarrow\\theta}$ directly optimize the parameters of the hard and soft networks, respectively, leading to crucial roles in maintaining the performance. Removing either of the two terms results in an accuracy plummet of above $75\\%$ . ", "page_idx": 7}, {"type": "text", "text": "4.3 Investigation into gap ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "According to Section 3, we formulate the pruning problem into two parts: 1) find a superior soft network, i.e., the network parameterized by $\\pmb{\\theta}\\odot\\pmb{w}$ , that satisfies the resource constraint; 2) reducing the gap between the soft network and the practically pruned one, which is referred to as a hard network in this manuscript and parameterized by $\\bar{\\pmb{\\theta}}\\left<\\hat{\\pmb{m}}\\right>$ . In this section, we first provide possible alternatives to formulate the problem 1 and then compare them with our proposed one on the gap-narrowing capacity to demonstrate the superiority of our method. ", "page_idx": 7}, {"type": "text", "text": "The first alternative attempts to directly optimize the hard network on its performance, i.e., the straight-through estimators [2]: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Alt~1:~}\\underset{\\pmb{w}}{\\mathrm{min}}\\left(\\mathcal{L}\\left(\\pmb{\\theta}\\odot\\pmb{w}\\right)+\\alpha\\mathcal{R}\\left(\\pmb{w},T\\right)\\right),}\\\\ &{\\quad\\quad\\quad\\quad\\underset{\\pmb{\\theta}}{\\mathrm{min}}\\,\\mathcal{L}\\left(\\pmb{\\theta}\\left\\langle\\pmb{\\hat{m}}\\right\\rangle\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The second alternative substitutes the soft network with the original one while calculating the gap measure, which conforms to self-distillation-based pruners [70]: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Alt~2:~}\\underset{\\pmb{w}}{\\mathrm{min}}\\left(\\mathcal{L}\\left(\\pmb{\\theta}\\odot\\pmb{w}\\right)+\\alpha\\mathcal{R}\\left(\\pmb{w},T\\right)\\right),}\\\\ &{\\quad\\quad\\quad\\quad\\underset{\\pmb{\\theta}}{\\mathrm{min}}\\left(\\mathcal{L}\\left(\\pmb{\\theta}\\right)+\\mathcal{G}\\left(\\pmb{\\theta}\\left\\langle\\pmb{\\hat{m}}\\right\\rangle,\\pmb{\\theta}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Comparative experiments are conducted on CIFAR-100, using ResNet-50 as the baseline. The FLOPs target is set to $15\\%$ . The gap metrics, i.e., the Jensen\u2013Shannon divergence $(J S)$ and $L_{2}$ distance, are averaged over the entire validation set. We measure the gap between the hard network and its direct supervision. For \u201cAlt $1\"$ , the gap metrics are calculated between the 0.1 label smoothed [56] ground truth and the output of the hard network. For \u201cAlt $2\"$ , the outputs of the original network and the hard one are utilized to calculate the gap metrics. For \u201cOurs\", the outputs of the soft network and the hard one are selected to analyze the gap. ", "page_idx": 7}, {"type": "text", "text": "Table 6 shows the comparison results. It can be observed that 1) a lower gap between the hard network and its direct supervision renders the hard network better performance. With the $J S$ reduced from 2.06 (\u201cAlt 1\") to 0.193 (\u201cOurs\"), the top-1 accuracy of the hard network increases from $77.13\\%$ to $79.77\\%$ ; 2) Our proposed soft-to-hard formulation achieves the lowest gap on both $J S$ and $L_{2}$ , ", "page_idx": 7}, {"type": "text", "text": "Table 7: The top-1 accuracy of the hard network at different fine-tuning epochs. The top-1 accuracy of the solely trained soft network before fine-tuning is $79.41\\%$ . The symbols $\\pmb{\\theta}\\odot\\pmb{w}$ and $\\pmb{\\theta}\\left<\\hat{m}\\right>$ represent the top-1 accuracy of the soft and hard networks, respectively. ", "page_idx": 8}, {"type": "table", "img_path": "mtyy3Myyhz/tmp/5b410badd46660df7780ba8d5888d884b9657a5499aaf346c71f3109a5c5021a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 8: The top-1 accuracy of different networks pruned from ResNet-50 with a $15\\%$ FLOPs constraint and then trained from scratch without bells and whistles. ", "page_idx": 8}, {"type": "table", "img_path": "mtyy3Myyhz/tmp/023ef17d23b51f8f8770f883bf6780e29eb14ca7b863688aab684aef3c31159a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "obtaining a hard network with the highest performance. The two observations imply that the soft-tohard formulation is a relatively better scheme to narrow the gap, and the lower gap between the hard network and its direct supervision helps improve the hard network\u2019s performance. ", "page_idx": 8}, {"type": "text", "text": "Can fine-tuning reduce the gap? It might be questioned whether the coupled training of the soft and hard networks is necessary. In Section 3, we entangle the two optimizations in the problem 2 to avoid alternate optimization, which turns out to be an efficient yet effective scheme according to [36, 33]. Without the entanglement, multi-stage optimization is required. A soft network that satisfies the resource constraint is firstly trained solely, and then a fine-tuning stage attempts to narrow the gap between the soft network and the hard one. To explore the effect of fine-tuning, we train a ResNet-50 on CIFAR-100, constraint to $15\\%$ FLOPs, and merely optimize the soft network for 500 epochs. With this pretrained soft network, we perform fine-tuning via Algorithm 1 with a $0.1\\mathrm{x}$ learning rate and different epochs. The results can be referred to in Table 7. The fine-tuning does reduce the gap to some extent, costing 250 epochs to align the soft network and the hard one (accuracy difference drops from $3.49\\%$ to $0.33\\%$ ). However, compared with our coupled training, the best accuracy of fine-tuning is still $0.28\\%$ lower at the cost of an additional 250 epochs. Consequently, the adopted coupled training turns out to be a better choice. ", "page_idx": 8}, {"type": "text", "text": "4.4 Architectural superiority ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To demonstrate the architectural superiority of our pruned network, we conduct experiments on CIFAR-100, prune a ResNet-50 to $15\\%$ FLOPs via our proposed method, and then train it from scratch without bells and whistles. Three networks that are randomly pruned to $15\\%$ FLOPs are selected for the comparison. The results are shown in Table 8. The network pruned by our method achieves the highest accuracy, verifying that the pruning mask optimized via Algorithm 1 possesses architectural superiority. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion and limitations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this paper, we reveal and study the long-standing omitted discretization gap problem in differentiable mask pruning. To bridge the discretization gap, we propose a structured differentiable mask pruning framework named Soft-to-Hard Pruner (S2HPruner), using the soft network to distill the hard network and optimize the mask. To further optimize the mask and avoid performance degradation, a decoupled bidirectional KD is proposed to alternatively maintain and block the gradient of weights and the mask. Extensive experiments verify and explain that S2HPruner can obtain high-performance hard networks with extraordinarily low resource constraints. ", "page_idx": 8}, {"type": "text", "text": "It is essential to acknowledge the limitations of our method. Therefore, we identify the following limitations: 1) The proposed method merely considers a single dimension, pruning feature channels of a layer. However, a block containing layers might be redundant and could be pruned as a whole, which is regarded as another pruning dimension that we do not consider in this manuscript; 2) We only validate our method on the task of image classification. It is left to explore our method\u2019s capability on other tasks, such as detection, segmentation, or natural language processing; 3) We choose FLOPs as the resource indicator, which might not ensure a hardware-friendly architecture. It is promising to consider the inference time on a specific hardware as an indicator. Above all, the identified limitations present opportunities for future research and development, and we remain committed to further exploration and refinement to overcome these challenges. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is supported by National Natural Science Foundation of China (No. 62071127), National Key Research and Development Program of China (No. 2022ZD0160101), Shanghai Natural Science Foundation (No. 23ZR1402900), Shanghai Municipal Science and Technology Major Project (No.2021SHZDZX0103). The computations in this research were performed using the CFFF platform of Fudan University. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Yue Bai, Huan Wang, ZHIQIANG TAO, Kunpeng Li, and Yun Fu. Dual lottery ticket hypothesis. In International Conference on Learning Representations, 2021.   \n[2] Yoshua Bengio, Nicholas L\u00e9onard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.   \n[3] Liyang Chen, Yongquan Chen, Juntong Xi, and Xinyi Le. Knowledge from the original network: restore a better pruned network with knowledge distillation. Complex & Intelligent Systems, pages 1\u201310, 2021.   \n[4] Mengzhao Chen, Wenqi Shao, Peng Xu, Mingbao Lin, Kaipeng Zhang, Fei Chao, Rongrong Ji, Yu Qiao, and Ping Luo. Diffrate: Differentiable compression rate for efficient vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 17164\u201317174, 2023.   \n[5] Tianlong Chen, Xuxi Chen, Xiaolong Ma, Yanzhi Wang, and Zhangyang Wang. Coarsening the granularity: Towards structurally sparse lottery tickets. In International Conference on Machine Learning, pages 3025\u20133039. PMLR, 2022.   \n[6] Tianyi Chen, Luming Liang, DING Tianyu, Zhihui Zhu, and Ilya Zharkov. Otov2: Automatic, generic, user-friendly. In International Conference on Learning Representations, 2023.   \n[7] Xin Chen, Lingxi Xie, Jun Wu, and Qi Tian. Progressive darts: Bridging the optimization gap for nas in the wild. International Journal of Computer Vision, 129:638\u2013655, 2021.   \n[8] Ting-Wu Chin, Ruizhou Ding, Cha Zhang, and Diana Marculescu. Towards efficient model compression via learned global ranking. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1518\u20131528, 2020.   \n[9] Minsik Cho, Saurabh Adya, and Devang Naik. Pdp: Parameter-free differentiable pruning is all you need. Advances in Neural Information Processing Systems, 36, 2024.   \n[10] Baiyun Cui, Yingming Li, and Zhongfei Zhang. Joint structured pruning and dense knowledge distillation for efficient transformer model compression. Neurocomputing, 458:56\u201369, 2021.   \n[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.   \n[12] Xiaohan Ding, Guiguang Ding, Yuchen Guo, Jungong Han, and Chenggang Yan. Approximated oracle filter pruning for destructive cnn width optimization. In International Conference on Machine Learning, pages 1607\u20131616. PMLR, 2019.   \n[13] Xuanyi Dong and Yi Yang. Network pruning via transformable architecture search. Advances in Neural Information Processing Systems, 32, 2019.   \n[14] Gongfan Fang, Xinyin Ma, Mingli Song, Michael Bi Mi, and Xinchao Wang. Depgraph: Towards any structural pruning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16091\u201316101, 2023.   \n[15] Shangqian Gao, Feihu Huang, Jian Pei, and Heng Huang. Discrete model compression with resource constraint for deep neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1899\u20131908, 2020.   \n[16] Shangqian Gao, Zeyu Zhang, Yanfu Zhang, Feihu Huang, and Heng Huang. Structural alignment for network pruning through partial regularization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 17402\u201317412, 2023.   \n[17] Shaopeng Guo, Yujie Wang, Quanquan Li, and Junjie Yan. Dmcp: Differentiable markov channel pruning for neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1539\u20131547, 2020.   \n[18] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. Advances in neural information processing systems, 28, 2015.   \n[19] Babak Hassibi and David Stork. Second order derivatives for network pruning: Optimal brain surgeon. Advances in neural information processing systems, 5, 1992.   \n[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n[21] Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, and Yi Yang. Filter pruning via geometric median for deep convolutional neural networks acceleration. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4340\u20134349, 2019.   \n[22] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. stat, 1050:9, 2015.   \n[23] Zejiang Hou, Minghai Qin, Fei Sun, Xiaolong Ma, Kun Yuan, Yi Xu, Yen-Kuang Chen, Rong Jin, Yuan Xie, and Sun-Yuan Kung. Chex: Channel exploration for cnn model compression. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12287\u201312298, 2022.   \n[24] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1314\u20131324, 2019.   \n[25] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4700\u20134708, 2017.   \n[26] Zehao Huang and Naiyan Wang. Data-driven sparse structure selection for deep neural networks. In Proceedings of the European conference on computer vision (ECCV), pages 304\u2013320, 2018.   \n[27] Minsoo Kang and Bohyung Han. Operation-aware soft channel pruning using differentiable masks. In International conference on machine learning, pages 5122\u20135131. PMLR, 2020.   \n[28] Jangho Kim, SeongUk Park, and Nojun Kwak. Paraphrasing complex network: Network compression via factor transfer. Advances in neural information processing systems, 31, 2018.   \n[29] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[30] Yann LeCun, John Denker, and Sara Solla. Optimal brain damage. Advances in neural information processing systems, 2, 1989.   \n[31] Seung Hoon Lee, Seunghyun Lee, and Byung Cheol Song. Vision transformer for small-size datasets, 2021.   \n[32] Tianhong Li, Jianguo Li, Zhuang Liu, and Changshui Zhang. Few sample knowledge distillation for efficient network compression. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14639\u201314647, 2020.   \n[33] Yunqiang Li, Jan C van Gemert, Torsten Hoefler, Bert Moons, Evangelos Eleftheriou, and Bram-Ernst Verhoef. Differentiable transportation pruning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 16957\u201316967, 2023.   \n[34] Shaohui Lin, Rongrong Ji, Chenqian Yan, Baochang Zhang, Liujuan Cao, Qixiang Ye, Feiyue Huang, and David Doermann. Towards optimal structured cnn pruning via generative adversarial learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2790\u20132799, 2019.   \n[35] Weihao Lin, Tao Chen, and Chong Yu. Spvos: Efficient video object segmentation with triple sparse convolution. IEEE Transactions on Image Processing, 2023.   \n[36] Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. In International Conference on Learning Representations, 2018.   \n[37] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021.   \n[38] Zechun Liu, Haoyuan Mu, Xiangyu Zhang, Zichao Guo, Xin Yang, Kwang-Ting Cheng, and Jian Sun. Metapruning: Meta learning for automatic neural network channel pruning. In Proceedings of the IEEE/CVF international conference on computer vision, pages 3296\u20133305, 2019.   \n[39] Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. Learning efficient convolutional networks through network slimming. In Proceedings of the IEEE international conference on computer vision, pages 2736\u20132744, 2017.   \n[40] Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the value of network pruning. In International Conference on Learning Representations, 2018.   \n[41] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.   \n[42] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019.   \n[43] Christos Louizos, Max Welling, and Diederik P Kingma. Learning sparse neural networks through l_0 regularization. In International Conference on Learning Representations, 2018.   \n[44] Jian-Hao Luo and Jianxin Wu. Autopruner: An end-to-end trainable filter pruning method for efficient deep model inference. Pattern Recognition, 107:107461, 2020.   \n[45] Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet: A fliter level pruning method for deep neural network compression. In Proceedings of the IEEE international conference on computer vision, pages 5058\u20135066, 2017.   \n[46] Haoyu Ma, Tianlong Chen, Ting-Kuei Hu, Chenyu You, Xiaohui Xie, and Zhangyang Wang. Good students play big lottery better. arXiv preprint arXiv:2101.03255, 3, 2021.   \n[47] James O\u2019 Neill, Sourav Dutta, and Haytham Assem. Deep neural compression via concurrent pruning and self-distillation. arXiv preprint arXiv:2109.15014, 2021.   \n[48] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.   \n[49] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 779\u2013788, 2016.   \n[50] Pedro Savarese, Hugo Silva, and Michael Maire. Winning the lottery with continuous sparsification. Advances in neural information processing systems, 33:11380\u201311390, 2020.   \n[51] Yiqing Shen, Liwu Xu, Yuzhe Yang, Yaqian Li, and Yandong Guo. Self-distillation from the last mini-batch for consistency regularization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11943\u201311952, 2022.   \n[52] Zhiqiang Shen and Eric Xing. A fast knowledge distillation framework for visual recognition. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XXIV, pages 673\u2013690. Springer, 2022.   \n[53] Yang Sui, Miao Yin, Yi Xie, Huy Phan, Saman Aliari Zonouz, and Bo Yuan. Chip: Channel independencebased pruning for compact neural networks. Advances in Neural Information Processing Systems, 34:24604\u2013 24616, 2021.   \n[54] Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and momentum in deep learning. In International conference on machine learning, pages 1139\u20131147. PMLR, 2013.   \n[55] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1\u20139, 2015.   \n[56] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2818\u20132826, 2016.   \n[57] Shengji Tang, Weihao Lin, Hancheng Ye, Peng Ye, Chong Yu, Baopu Li, and Tao Chen. Enhanced sparsification via stimulative training. arXiv preprint arXiv:2403.06417, 2024.   \n[58] Shengji Tang, Peng Ye, Baopu Li, Weihao Lin, Tao Chen, Tong He, Chong Yu, and Wanli Ouyang. Boosting residual networks with group knowledge. arXiv preprint arXiv:2308.13772, 2023.   \n[59] Yehui Tang, Yunhe Wang, Yixing Xu, Dacheng Tao, Chunjing Xu, Chao Xu, and Chang Xu. Scop: Scientific control for reliable neural network pruning. Advances in Neural Information Processing Systems, 33, 2020.   \n[60] Yunjie Tian, Chang Liu, Lingxi Xie, Qixiang Ye, et al. Discretization-aware architecture search. Pattern Recognition, 120:108186, 2021.   \n[61] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[62] Huan Wang, Can Qin, Yulun Zhang, and Yun Fu. Neural pruning via growing regularization. In International Conference on Learning Representations (ICLR), 2021.   \n[63] Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in deep neural networks. Advances in neural information processing systems, 29, 2016.   \n[64] Yu-Cheng Wu, Chih-Ting Liu, Bo-Ying Chen, and Shao-Yi Chien. Constraint-aware importance estimation for global fliter pruning under multiple resource constraints. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 686\u2013687, 2020.   \n[65] Xia Xiao, Zigeng Wang, and Sanguthevar Rajasekaran. Autoprune: Automatic network pruning by regularizing auxiliary parameters. Advances in neural information processing systems, 32, 2019.   \n[66] Peng Ye, Baopu Li, Tao Chen, Jiayuan Fan, Zhen Mei, Chen Lin, Chongyan Zuo, Qinghua Chi, and Wanli Ouyang. Efficient joint-dimensional search with solution space regularization for real-time semantic segmentation. International Journal of Computer Vision, 130(11):2674\u20132694, 2022.   \n[67] Peng Ye, Baopu Li, Yikang Li, Tao Chen, Jiayuan Fan, and Wanli Ouyang. b-darts: Beta-decay regularization for differentiable architecture search. In proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10874\u201310883, 2022.   \n[68] Peng Ye, Shengji Tang, Baopu Li, Tao Chen, and Wanli Ouyang. Stimulative training of residual networks: A social psychology perspective of loafing. Advances in Neural Information Processing Systems, 35:3596\u2013 3608, 2022.   \n[69] Zhonghui You, Kun Yan, Jinmian Ye, Meng Ma, and Ping Wang. Gate decorator: Global filter pruning method for accelerating deep convolutional neural networks. Advances in neural information processing systems, 32, 2019.   \n[70] Jiahui Yu and Thomas Huang. Autoslim: Towards one-shot architecture search for channel numbers. arXiv preprint arXiv:1903.11728, 2019.   \n[71] Jiahui Yu, Linjie Yang, Ning Xu, Jianchao Yang, and Thomas Huang. Slimmable neural networks. arXiv preprint arXiv:1812.08928, 2018.   \n[72] Lei Yu, Xinpeng Li, Youwei Li, Ting Jiang, Qi Wu, Haoqiang Fan, and Shuaicheng Liu. Dipnet: Efficiency distillation and iterative pruning for image super-resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1692\u20131701, 2023.   \n[73] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In British Machine Vision Conference 2016. British Machine Vision Association, 2016. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix A: Details of experiments ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we provide the detailed specific training settings in the main manuscript. All experiments are conducted under the deep learning framework Pytorch [48], versioned 2.0.1 with Python versioned 3.10. The CUDA version is 11.8. A cluster equipped with 8 NVIDIA A100 GPUs, 1024 GB memories, and 120 CPUs is used to run experiments. A single GPU is used for experiments on CIFAR-100 and Tiny ImageNet. For Imagenet, four GPUs are paralleled to run the task. ", "page_idx": 13}, {"type": "text", "text": "A1. Implementation details of CIFAR-100 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The CIFAR-100 dataset [29] is a classical classification dataset, which consists of 100 categories with 50,000 training images and 10,000 testing images. For ResNet-50 [20] and MBV3 [24], we follow the training settings in [68]. In detail, the whole training epoch number is 500, and the input batch size is 64. We utilize the original SGD as the optimizer with a 0.05 initial learning rate and a 0.0003 weight decay. The cosine decay schedule is utilized to adapt the learning rate throughout the training process. For WRN28-10 [73], we follow the training settings of [73]. In detail, the epoch number and batch size are 200 and 128, respectively. The SGD is chosen as the optimizer with a 0.1 initial learning rate and a 0.0005 weight decay. The learning rate scheduler is also the cosine decay schedule. For ViT [61] and Swin Transformer [37], we use an image size of $32\\mathtt{x}32$ and a patch size of 4. The epoch number and batch size are 200 and 128, respectively. The optimizer is AdamW [42] with an initial learning rate of 0.001/0.003 for Swin/ViT and a 0.05 weight decay. The learning rate is warmed up for 10 epochs. The data augmentations are the same as the ones in [31]. Different from CNNs, where we regard the channel numbers of convolutional and linear layers as the width dimension, to prune the width of Transformers, we take the head numbers (ViT) or head feature dimensions (Swin) of attention layers and the channel numbers of linear layers into account. ", "page_idx": 13}, {"type": "text", "text": "A2. Implementation details of Tiny ImageNet ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The Tiny ImageNet dataset is derived from the renowned ImageNet dataset [11], comprising 200 categories, 100,000 training images, and 10,000 test images. For the ResNet-50 [20] and MBV3 [24] models, we employ 500 epochs and a batch size of 64. The optimization is performed using SGD with an initial learning rate of 0.1 and a weight decay of 0.0003. We utilize a step-wise learning rate scheduler, reducing the learning rate to 0.1 and 0.01 of the original at the 250th and 375th epochs, respectively. For the WRN28-10 [73] architecture, we adopt the training settings from [51], with 200 epochs and a batch size of 128. The SGD optimizer is used with an initial learning rate of 0.2 and a weight decay of 0.0001. The learning rate is decreased in a step-wise manner, dropping to 0.1 and 0.01 of the initial value at the 100th and 150th epochs, respectively. ", "page_idx": 13}, {"type": "text", "text": "A3. Implementation details of ImageNet ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The ImageNet dataset [11] is a widely used classification benchmark, containing 1,000 categories, 1.2 million training images, and 50,000 testing images. For the evaluated ResNet-50 [20], the epoch number and batch size are 200 and 512, respectively. We utilize SGD as the optimizer. The learning rate is initialized as 0.2 and is controlled by a cosine decay schedule. The weight decay is 0.0001. Besides, we apply the commonly used data augmentations according to [25, 55]. ", "page_idx": 13}, {"type": "text", "text": "A4. Hyperparameters $\\alpha,\\beta$ , and $\\gamma$ ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "To determine the hyperparameters in Algorithm 1, we utilize a dynamic balancing scheme based on the $L_{2}$ norm of gradients. Specifically, the ${\\pmb g}_{\\mathcal{L}\\rightarrow{\\pmb\\theta}\\odot{\\pmb w}\\rightarrow{\\pmb w}}$ and $g_{\\mathcal{G}\\rightarrow\\pmb{\\theta}\\odot\\pmb{w}\\rightarrow\\pmb{w}}$ are firstly normalized by their own $L_{2}$ norms before being added together. The addition result is then aligned with $g_{\\mathcal{R}\\to w}$ via being scaled to the $L_{2}$ norm of ${g}_{\\mathcal{R}\\to w}$ . For $g_{\\mathcal{L}\\rightarrow\\theta\\odot w\\rightarrow\\theta}$ and $g_{\\mathcal{G}\\rightarrow\\theta\\langle\\hat{m}\\rangle\\rightarrow\\theta}$ , no balancing is applied. The two terms are added with fixed coefficients. For CNNs, the coefficients are 0.5 and 5 for $g_{\\mathcal{L}\\rightarrow\\theta\\odot w\\rightarrow\\theta}$ and $g_{\\mathcal{G}\\rightarrow\\theta\\langle\\hat{m}\\rangle\\rightarrow\\theta}$ , respectively. For Transformers, the coefficients are 1 and 1 for $g_{\\mathcal{L}\\rightarrow\\theta\\odot w\\rightarrow\\theta}$ and $g_{\\mathcal{G}\\rightarrow\\theta\\langle\\hat{m}\\rangle\\rightarrow\\theta}$ , respectively. The coefficient for $g_{\\mathcal{R}\\to w}$ is set to 5. ", "page_idx": 13}, {"type": "text", "text": "Appendix B: Trajectory of FLOPs and accuracy ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, the FLOPs and accuracy trajectory is provided to display the pruning procedure of S2HPruner visually. We conduct experiments on five different models, including ResNet-50 [20], MobileNetV3 (MBV3) [24], WideResNet28-10 [73], ViT [61], and Swin Transformer [37] on CIFAR-100. The results are shown in Fig. 3 and as the training epoch increases, our methods can fast converge the capacity of the hard network to the target FLOPs. However, it does not mean the mask optimization is finished. It can be seen that the performance of the robust network is steadily improving. It suggests that after entering the feasible region, S2HPruner consistently explores the possible structure and exploits the optimal architecture. Moreover, although applied to five unique architectures, S2HPruner obtains similar trajectories, which demonstrates the generalization of S2HPruner. ", "page_idx": 14}, {"type": "image", "img_path": "mtyy3Myyhz/tmp/29cd425d26d9a8be380f2e91430edc7cca598c54d97fdc8c3e9221d46e30a679.jpg", "img_caption": ["Figure 3: The trajectory of FLOPs and accuracy. We report the accuracy and FLOPs of the hard network during the training of different models, including (a) ResNet-50 (b) MobileNetV3 (c) WideResNet28-10 (d) ViT (e) Swin Transformer on CIFAR-100. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Appendix C: Visualization of pruning process ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We report the detailed output channel variation of different five networks during pruning visually. The results are shown in Fig 5, 6,7, 8, 9. The target FLOPs is set to $15\\%$ . It is worth noting that because the mask is dependent on the dependencies groups where layers all have the same output channels, we report the index of dependencies groups as the index of layers, which does not correspond to the raw definition completely. It can be observed that the channel variation is disparate between different layers, which implies our method is not restricted to trivial solutions such as uniform channel distribution. Combined analysis with Fig. 3, we can observe that although the FLOPs satisfies the constraints, our method is not caught in loafing but can consistently explore the structure space to find the optimal architecture. A similar phenomenon also exists in all five networks, which demonstrates the generalization of the proposed method. ", "page_idx": 14}, {"type": "text", "text": "Appendix D: The architecture of the pruned network ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We provide the architecures of our pruned networks in Fig. 4. The pruned networks are obtained via using Algorithm 1 on CIFAR-100 with a $15\\%$ FLOPs target. It can be observed from Fig. 4 that different pruned network varies in architecture pattern. For example, convolutional neural networks (CNNs), i.e., ResNet-50, MBV3, and WRN28-10 may prefer deeper layers. The retained channels are concentratively distributed in the post-half layers. Different from CNNs, Transformers, i.e., ViT, and Swin seem not to exhibit an obvious preference for layer depth. The pruning pattern of the shallow layers is almost uniform with that of the deep layers. ", "page_idx": 14}, {"type": "image", "img_path": "mtyy3Myyhz/tmp/ceb700b7411d18233d5695829ea52e5e5d67c9714fa05e77cac9f1170d5cfcc8.jpg", "img_caption": ["Figure 4: The architectures of networks, including (a) ResNet-50 (b) MobileNetV3 (c) WideResNet28- 10 (d) ViT (e) Swin Transformer, pruned via our proposed method on CIFAR-100. The target FLOPs is set to $15\\%$ . "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "Table 9: The pruning results obtained via training a ResNet-50 on CIFAR-100 with different random seeds using our proposed method. We report the Top-1 accuracy and FLOPs. ", "page_idx": 16}, {"type": "table", "img_path": "mtyy3Myyhz/tmp/1a70cf79ea2446cb05150cc0d2c0daede55aa6caf015600ee7966e5577944029.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "mtyy3Myyhz/tmp/afc0ed33136f95e15c439beeb5aa53bc8c452cda699de12652073cf471823d63.jpg", "table_caption": ["Table 10: Training efficiency comparison with different methods. For a fair comparison, double-epoch training results of other methods are included. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Appendix E: Robustness against randomness ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "To assess the consistency of our proposed pruning method, we target a $15\\%$ reduction in FLOPs using ResNet-50 as the base model on the CIFAR-100 dataset. Four independent runs with varying random seeds are conducted, and the results are presented in Table 9. The pruned networks consistently achieved comparable performance, with negligible variations in Top-1 accuracy (less than $0.1\\%$ deviation) and FLOPs (less than $1\\%$ deviation). These findings validate the robustness of our proposed method, indicating that the resource consumption of the pruned network is expected and its performance is reliable. ", "page_idx": 16}, {"type": "text", "text": "Appendix F: Training efficiency ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "To investigate the training efficiency of the proposed method, we compare its training time to other established pruning methods in Table 10. Using ResNet-50 on the CIFAR-100 dataset, our experiments reveal that the proposed method achieves exceptional performance while maintaining a competitive training time, ranking second-shortest among the tested methods. This efficiency stems from the inherent parallelism of the soft and hard networks. The forward and backward passes of the soft and hard networks can be executed simultaneously, leveraging the power of CUDA streams or multi-GPU parallelism. Furthermore, our method operates in a single stage, eliminating the need for sequential fine-tuning or iterative pruning, further contributing to its time efficiency. To isolate the impact of forward/backward pass counts, we extended the training epochs of other methods two-fold to match our method\u2019s counts. Despite this, the performance of these methods plateaued, indicating that simply increasing training time does not guarantee improved pruning results. This underscores the inherent advantages of our method. ", "page_idx": 16}, {"type": "text", "text": "Besides, the GPU memory costs during training and inference are also reported in Table 10. During training, our method costs bearable (about $10\\%$ ) more GPU memories than the average of other methods due to the additional learnable masks and the mask state buffers in the optimizer. During inference, the GPU memory costs merely depend on the scale of the pruned network. As the FLOPs target is set to $15\\%$ for all the methods, there is no significant difference in GPU memory costs. ", "page_idx": 16}, {"type": "image", "img_path": "mtyy3Myyhz/tmp/feeb49aeaf395215da0848e8efa38f01db0b6a811ff890c69da025eae1d90ba6.jpg", "img_caption": ["Figure 5: The detailed channel variation of ResNet-50 on CIFAR-100 during training. The target FLOPs is set to $15\\%$ . The horizontal axis represents the training iterations. The vertical axis represents the output channel number. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "mtyy3Myyhz/tmp/837fcd52c7bf95ddaa1a172e0788d91677a2c64a2138e73ae8472fc1d768f574.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure 6: The detailed channel variation of MobileNetV3 on CIFAR-100 during training. The target FLOPs is set to $15\\%$ . The horizontal axis represents the training iterations. The vertical axis represents the output channel number. ", "page_idx": 18}, {"type": "image", "img_path": "mtyy3Myyhz/tmp/33c48db3e335741f346a7817bbcf9266a5c550349cf3486b752e77749917219a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 7: The detailed channel variation of WideResNet28-10 on CIFAR-100 during training. The target FLOPs is set to $15\\%$ . The horizontal axis represents the training iterations. The vertical axis represents the output channel number. ", "page_idx": 19}, {"type": "image", "img_path": "mtyy3Myyhz/tmp/7d455f508fecd0e69e693485d33e8c2ff58894e7d5fb8abf952d558dbdb24a41.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 8: The detailed channel variation of ViT on CIFAR-100 during training. The target FLOPs is set to $15\\%$ . The horizontal axis represents the training iterations. The vertical axis represents the output channel number. ", "page_idx": 20}, {"type": "image", "img_path": "mtyy3Myyhz/tmp/3d83ee49c512fa780ea6f3ed0a6c6c88a9ac0d5cc3714fba4cd1b113d27791ea.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 9: The detailed channel variation of Swin Transformer on CIFAR-100 during training. The target FLOPs is set to $15\\%$ . The horizontal axis represents the training iterations. The vertical axis represents the output channel number. ", "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Check the abstract and Section 1 for details. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: Check Section 5 for details. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: This paper does not include theoretical results. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Detailed methods and configurations can be queried in Section 3 and Section 5. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [No] ", "page_idx": 24}, {"type": "text", "text": "Justification: The code will be released soon. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Check Section 3 and Section 5 for details. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We provide results under different random seeds. See Section 5 for details. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: See Section 5 and Section 5 for details. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We have carefully read the code of ethics and ensure that the research conducted in the paper conforms with it in every respect. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: No societal impact is involved in this work. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper is not relevant to such risks. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: All datasets and code frameworks are mentioned and properly respected. See Section 5 for details. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 26}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]