[{"figure_path": "9jgODkdH0F/figures/figures_1_1.jpg", "caption": "Figure 1: The connectivity of observed data affects the implicit regularization. The ground truth matrix M* \u2208 ]R4\u00d74 has rank ranging from 1 to 3. The sample size n covers settings where n is equal to, smaller than, and larger than the 2rd \u2013 r2 threshold required for exact reconstruction. Darker scatter points indicate a greater number of samples, while lighter points indicate fewer samples. The positions of observed entries are randomly chosen, and the experiment is repeated 10 times for each sample size. (Please refer to Appendix B for additional experiments and detailed methodology.)", "description": "This figure shows the impact of observed data connectivity on implicit regularization in matrix factorization models for matrix completion.  It compares the experimental rank and nuclear norm of solutions obtained from training matrix factorization models on datasets with varying connectivity (connected, disconnected with complete bipartite components, and other disconnected cases). The plot demonstrates a transition from low nuclear norm bias to low rank bias as the data connectivity increases, revealing a key role of data connectivity in shaping implicit regularization.", "section": "Connectivity affects implicit regularization"}, {"figure_path": "9jgODkdH0F/figures/figures_4_1.jpg", "caption": "Figure 1: The connectivity of observed data affects the implicit regularization. The ground truth matrix M* \u2208 ]R4\u00d74 has rank ranging from 1 to 3. The sample size n covers settings where n is equal to, smaller than, and larger than the 2rd \u2013 r2 threshold required for exact reconstruction. Darker scatter points indicate a greater number of samples, while lighter points indicate fewer samples. The positions of observed entries are randomly chosen, and the experiment is repeated 10 times for each sample size. (Please refer to Appendix B for additional experiments and detailed methodology.)", "description": "This figure shows how data connectivity influences implicit regularization in matrix factorization models for matrix completion.  It plots the rank and nuclear norm of solutions obtained from training against the ground truth rank and nuclear norm for various sample sizes and connectivity patterns (connected, disconnected).  Darker points represent more experiments at the same condition.  The results suggest a transition in the implicit bias from low nuclear norm to low rank as data connectivity increases.", "section": "Connectivity affects implicit regularization"}, {"figure_path": "9jgODkdH0F/figures/figures_5_1.jpg", "caption": "Figure 1: The connectivity of observed data affects the implicit regularization. The ground truth matrix M* \u2208 R4\u00d74 has rank ranging from 1 to 3. The sample size n covers settings where n is equal to, smaller than, and larger than the 2rd \u2013 r2 threshold required for exact reconstruction. Darker scatter points indicate a greater number of samples, while lighter points indicate fewer samples. The positions of observed entries are randomly chosen, and the experiment is repeated 10 times for each sample size. (Please refer to Appendix B for additional experiments and detailed methodology.)", "description": "This figure shows how the connectivity of observed data influences implicit regularization in matrix factorization models for matrix completion.  The x-axis represents the ground truth rank of the matrix, while the y-axis represents the ground truth nuclear norm. Each point represents the results from an experiment with a specific level of data connectivity (connected or disconnected) and number of observations.  The results suggest that connected data tends to result in low-rank solutions, while disconnected data results in low-nuclear-norm solutions. The color intensity indicates the number of samples.", "section": "Connectivity affects implicit regularization"}, {"figure_path": "9jgODkdH0F/figures/figures_6_1.jpg", "caption": "Figure 1: The connectivity of observed data affects the implicit regularization. The ground truth matrix M* \u2208 ]R4\u00d74 has rank ranging from 1 to 3. The sample size n covers settings where n is equal to, smaller than, and larger than the 2rd \u2013 r2 threshold required for exact reconstruction. Darker scatter points indicate a greater number of samples, while lighter points indicate fewer samples. The positions of observed entries are randomly chosen, and the experiment is repeated 10 times for each sample size. (Please refer to Appendix B for additional experiments and detailed methodology.)", "description": "This figure shows the impact of data connectivity on implicit regularization in matrix factorization models for matrix completion.  It plots the learned rank and nuclear norm against the ground truth values for various sample sizes and data connectivity patterns (connected and disconnected). The results suggest a transition in implicit regularization from low nuclear norm to low rank as data connectivity increases.", "section": "Connectivity affects implicit regularization"}, {"figure_path": "9jgODkdH0F/figures/figures_7_1.jpg", "caption": "Figure 5: (a) Illustrated trajectories for the experiment in Fig. 4. The blue line represents the trajectory converging to the lowest-rank solution, and the red line represents the actual trajectory experienced by the model. (b) The parameter trajectory escaping from a second-order stationary point to reach the next critical point for the experiment in Fig. 3. The 8 scatter points represent the 4 row vectors of matrix A and the 4 column vectors of matrix B. For ease of visualization, we randomly project them onto two dimensions and plot them in polar coordinates.", "description": "This figure illustrates the training dynamics in disconnected and connected cases. Panel (a) shows the training trajectory in a disconnected case, highlighting how the model learns a suboptimal solution by traversing through sub-invariant manifolds. Panel (b) displays the parameter alignment during training, demonstrating how the model progressively aligns towards specific directions in parameter space. This alignment is related to a hierarchical intrinsic invariant manifold.", "section": "5 Training dynamics in connected and disconnected cases"}, {"figure_path": "9jgODkdH0F/figures/figures_16_1.jpg", "caption": "Figure 1: The connectivity of observed data affects the implicit regularization. The ground truth matrix M* \u2208 R4\u00d74 has rank ranging from 1 to 3. The sample size n covers settings where n is equal to, smaller than, and larger than the 2rd \u2013 r2 threshold required for exact reconstruction. Darker scatter points indicate a greater number of samples, while lighter points indicate fewer samples. The positions of observed entries are randomly chosen, and the experiment is repeated 10 times for each sample size. (Please refer to Appendix B for additional experiments and detailed methodology.)", "description": "This figure shows the impact of data connectivity on implicit regularization in matrix factorization models for matrix completion.  The x-axis represents the ground truth rank of the matrix, while the y-axis represents the ground truth nuclear norm.  Each point represents a different matrix completion experiment with varying levels of data connectivity (indicated by color and point density), and sample size.  The figure demonstrates a transition from low nuclear norm regularization to low rank regularization as the connectivity of observed data increases.", "section": "Connectivity affects implicit regularization"}, {"figure_path": "9jgODkdH0F/figures/figures_25_1.jpg", "caption": "Figure 3: (a) The matrix M to be completed, with the position unknown. (b) The four singular values of the learned solution at different initialization scale (Gaussian distribution, mean 0, variance from 10\u2070 to 10\u207b\u00b9\u2076). (c) Training loss for 16 connected sampling patterns in a 4 \u00d7 4 matrix, each covering 1 element and observing the remaining 15 in a fixed rank-3 matrix. (d) Evolution of the l\u00b2-norm of the gradients throughout the training process. The cyan crosses represent the difference between the matrix corresponding to the saddle point and the optimal approximation at each rank. (e-h) Evolution of singular values for matrices W, A, B, and Waug during training.", "description": "This figure shows the training dynamics of a connected matrix completion problem.  Panel (a) displays the target matrix. Panel (b) illustrates how the learned solution's rank changes with different initialization scales, showing a transition from rank 4 to rank 3 as the scale decreases. Panel (c) shows the training loss, with flat periods indicating potential saddle points. Panel (d) compares the matrices learned at those points with optimal approximations for each rank. Panels (e-h) track the singular values of several matrices during training, revealing an alignment in their row and column spaces which aligns with the concept of hierarchical intrinsic invariant manifold.", "section": "5 Training dynamics in connected and disconnected cases"}, {"figure_path": "9jgODkdH0F/figures/figures_30_1.jpg", "caption": "Figure 1: The connectivity of observed data affects the implicit regularization. The ground truth matrix M* \u2208 R4\u00d74 has rank ranging from 1 to 3. The sample size n covers settings where n is equal to, smaller than, and larger than the 2rd \u2013 r2 threshold required for exact reconstruction. Darker scatter points indicate a greater number of samples, while lighter points indicate fewer samples. The positions of observed entries are randomly chosen, and the experiment is repeated 10 times for each sample size. (Please refer to Appendix B for additional experiments and detailed methodology.)", "description": "This figure shows how the connectivity of observed data influences implicit regularization in matrix factorization models for matrix completion.  Different colors represent different data connectivity patterns (connected, disconnected with complete bipartite components, and other disconnected patterns). The x-axis represents the ground truth rank and nuclear norm, while the y-axis represents the rank and nuclear norm of the learned solutions. The plot demonstrates a transition from low nuclear norm to low rank regularization as data shifts from disconnected to connected with increasing observations.  The size of each point indicates the number of samples used.", "section": "Connectivity affects implicit regularization"}, {"figure_path": "9jgODkdH0F/figures/figures_30_2.jpg", "caption": "Figure 1: The connectivity of observed data affects the implicit regularization. The ground truth matrix M* \u2208 R4\u00d74 has rank ranging from 1 to 3. The sample size n covers settings where n is equal to, smaller than, and larger than the 2rd \u2013 r2 threshold required for exact reconstruction. Darker scatter points indicate a greater number of samples, while lighter points indicate fewer samples. The positions of observed entries are randomly chosen, and the experiment is repeated 10 times for each sample size. (Please refer to Appendix B for additional experiments and detailed methodology.)", "description": "This figure shows how data connectivity influences implicit regularization in matrix factorization for matrix completion.  The x-axis represents the ground truth rank, and the y-axis represents the ground truth nuclear norm.  Each point represents the results of a matrix completion experiment with different levels of observed data connectivity and sample size. The color intensity of each point indicates the number of samples used in the experiment.  The figure demonstrates a clear transition from low nuclear norm to low rank as data connectivity increases.", "section": "Connectivity affects implicit regularization"}, {"figure_path": "9jgODkdH0F/figures/figures_31_1.jpg", "caption": "Figure 1: The connectivity of observed data affects the implicit regularization. The ground truth matrix M* \u2208 ]R4\u00d74 has rank ranging from 1 to 3. The sample size n covers settings where n is equal to, smaller than, and larger than the 2rd \u2013 r2 threshold required for exact reconstruction. Darker scatter points indicate a greater number of samples, while lighter points indicate fewer samples. The positions of observed entries are randomly chosen, and the experiment is repeated 10 times for each sample size. (Please refer to Appendix B for additional experiments and detailed methodology.)", "description": "This figure shows how the connectivity of observed data impacts implicit regularization in matrix factorization models for matrix completion.  It plots the rank and nuclear norm of solutions against the connectivity of the observed data for various sample sizes.  The results show a transition from low nuclear norm to low rank solutions as connectivity increases, indicating that data connectivity plays a significant role in shaping the implicit bias of these models. The experiment is repeated to show statistical significance.", "section": "Connectivity affects implicit regularization"}, {"figure_path": "9jgODkdH0F/figures/figures_31_2.jpg", "caption": "Figure 1: The connectivity of observed data affects the implicit regularization. The ground truth matrix M* \u2208 R4\u00d74 has rank ranging from 1 to 3. The sample size n covers settings where n is equal to, smaller than, and larger than the 2rd \u2013 r2 threshold required for exact reconstruction. Darker scatter points indicate a greater number of samples, while lighter points indicate fewer samples. The positions of observed entries are randomly chosen, and the experiment is repeated 10 times for each sample size. (Please refer to Appendix B for additional experiments and detailed methodology.)", "description": "This figure shows how the connectivity of observed data influences implicit regularization in matrix factorization models for matrix completion.  The x-axis represents the rank of the ground truth matrix, and the y-axis represents its nuclear norm. Different colors and shapes of points represent different data connectivity patterns (connected, disconnected with complete bipartite components, or other disconnected patterns).  The size of each point corresponds to the number of samples used in the experiment.  The plot shows that connected data leads to low-rank solutions while disconnected data often leads to solutions with lower nuclear norm, demonstrating the impact of data connectivity on the implicit bias of matrix factorization models.", "section": "Connectivity affects implicit regularization"}, {"figure_path": "9jgODkdH0F/figures/figures_32_1.jpg", "caption": "Figure 1: The connectivity of observed data affects the implicit regularization. The ground truth matrix M* \u2208 ]R4\u00d74 has rank ranging from 1 to 3. The sample size n covers settings where n is equal to, smaller than, and larger than the 2rd \u2013 r2 threshold required for exact reconstruction. Darker scatter points indicate a greater number of samples, while lighter points indicate fewer samples. The positions of observed entries are randomly chosen, and the experiment is repeated 10 times for each sample size. (Please refer to Appendix B for additional experiments and detailed methodology.)", "description": "This figure shows how the connectivity of observed data influences implicit regularization in matrix factorization models for matrix completion.  The x-axis represents the ground truth rank, and the y-axis represents the ground truth nuclear norm of the matrix. Each point represents an experiment, with the color indicating the connectivity of the observed data (connected or disconnected) and the intensity representing the number of samples. The results demonstrate a transition from low nuclear norm regularization to low-rank regularization as data connectivity increases.", "section": "Connectivity affects implicit regularization"}, {"figure_path": "9jgODkdH0F/figures/figures_32_2.jpg", "caption": "Figure 4: (a) The matrix to be completed, with unknown entries marked by *. (b-d) Evolution of singular values for A, B, and Waug during training. (e) Training loss for 9 disconnected sampling patterns in a 3 \u00d7 3 matrix, each covering 4 elements and observing the remaining 5 in a fixed rank-1 matrix. (f) Learned values at symmetric positions (1, 2) and (2, 1) under varying initialization scales (zero mean, varying variance). Each point represents one of ten random experiments per variance; labels show initialization variance. Other symmetric positions exhibit similar behavior. (g) Learned output at the saddle point corresponding to the red dot in (e). (h) Final learned solution of the GLRL algorithm (Li et al., 2020).", "description": "This figure demonstrates the training dynamics and the results in a disconnected case. Subfigures (b)-(d) show the singular values of A, B, and the augmented matrix Waug during training, indicating that the model progresses from low rank to high rank while maintaining rank(A) = rank(BT) = rank(Waug). Subfigure (e) shows the training loss for various disconnected sampling patterns, revealing that the model fails to achieve the optimal low-rank solution in this case. Subfigure (f) illustrates the learned values at symmetric positions under varying initialization scales, highlighting the existence of infinitely many rank-1 solutions. Subfigure (g) displays the learned output at the saddle point, and subfigure (h) shows the solution obtained using the Greedy Low-Rank Learning (GLRL) algorithm.", "section": "5 Training dynamics in connected and disconnected cases"}, {"figure_path": "9jgODkdH0F/figures/figures_33_1.jpg", "caption": "Figure 1: The connectivity of observed data affects the implicit regularization. The ground truth matrix M* \u2208 ]R4\u00d74 has rank ranging from 1 to 3. The sample size n covers settings where n is equal to, smaller than, and larger than the 2rd \u2013 r2 threshold required for exact reconstruction. Darker scatter points indicate a greater number of samples, while lighter points indicate fewer samples. The positions of observed entries are randomly chosen, and the experiment is repeated 10 times for each sample size. (Please refer to Appendix B for additional experiments and detailed methodology.)", "description": "This figure demonstrates the impact of data connectivity on the implicit regularization in matrix factorization models for matrix completion.  It shows how the model's tendency to favor low-rank solutions increases as the observed data becomes more connected.  Different levels of connectivity are tested by varying the number of observed entries and their arrangement in the matrix. The results illustrate a transition from low nuclear norm to low-rank solutions as the connectivity shifts. The experiment is repeated multiple times to ensure reliability.", "section": "Connectivity affects implicit regularization"}, {"figure_path": "9jgODkdH0F/figures/figures_34_1.jpg", "caption": "Figure 1: The connectivity of observed data affects the implicit regularization. The ground truth matrix M* \u2208 R4\u00d74 has rank ranging from 1 to 3. The sample size n covers settings where n is equal to, smaller than, and larger than the 2rd \u2013 r2 threshold required for exact reconstruction. Darker scatter points indicate a greater number of samples, while lighter points indicate fewer samples. The positions of observed entries are randomly chosen, and the experiment is repeated 10 times for each sample size. (Please refer to Appendix B for additional experiments and detailed methodology.)", "description": "This figure shows the impact of observed data connectivity on implicit regularization in matrix factorization models for matrix completion.  It demonstrates a transition from low nuclear norm to low rank solutions as data connectivity increases with the number of observations.  The plots illustrate the relationship between the ground truth rank/nuclear norm of the matrix and the rank/nuclear norm obtained by the model under different connectivity conditions and sample sizes.", "section": "Connectivity affects implicit regularization"}, {"figure_path": "9jgODkdH0F/figures/figures_34_2.jpg", "caption": "Figure 1: The connectivity of observed data affects the implicit regularization. The ground truth matrix M* \u2208 ]R4\u00d74 has rank ranging from 1 to 3. The sample size n covers settings where n is equal to, smaller than, and larger than the 2rd \u2013 r2 threshold required for exact reconstruction. Darker scatter points indicate a greater number of samples, while lighter points indicate fewer samples. The positions of observed entries are randomly chosen, and the experiment is repeated 10 times for each sample size. (Please refer to Appendix B for additional experiments and detailed methodology.)", "description": "This figure shows how the connectivity of observed data influences the implicit regularization in matrix factorization models for matrix completion.  It demonstrates a transition in implicit bias from low nuclear norm regularization to low rank regularization as the connectivity of the observed data increases. The experiments vary sample size and track the minimum nuclear norm and rank solutions, with darker points representing more samples. The plots show a clear relationship between data connectivity and the implicit regularization effect.", "section": "Connectivity affects implicit regularization"}, {"figure_path": "9jgODkdH0F/figures/figures_35_1.jpg", "caption": "Figure 3: (a) The matrix M to be completed, with the position unknown. (b) The four singular values of the learned solution at different initialization scale (Gaussian distribution, mean 0, variance from 10\u00b0 to 10-16). (c) Training loss for 16 connected sampling patterns in a 4 \u00d7 4 matrix, each covering 1 element and observing the remaining 15 in a fixed rank-3 matrix. (d) Evolution of the 12-norm of the gradients throughout the training process. The cyan crosses represent the difference between the matrix corresponding to the saddle point and the optimal approximation at each rank. (e-h) Evolution of singular values for matrices W, A, B, and Waug during training.", "description": "This figure shows the training dynamics of a connected matrix factorization model. It illustrates how the initialization scale affects the rank of the learned solution, the typical training loss curve, gradient norm evolution, and the alignment between row and column spaces during the learning process. The results support the concept of traversing through progressive optima at each rank and the alignment of row and column spaces.", "section": "5 Training dynamics in connected and disconnected cases"}, {"figure_path": "9jgODkdH0F/figures/figures_35_2.jpg", "caption": "Figure 1: The connectivity of observed data affects the implicit regularization. The ground truth matrix M* \u2208 \\(\\mathbb{R}\\)^{4 \\times 4} has rank ranging from 1 to 3. The sample size \\(n\\) covers settings where \\(n\\) is equal to, smaller than, and larger than the \\(2d^2 - r^2\\) threshold required for exact reconstruction. Darker scatter points indicate a greater number of samples, while lighter points indicate fewer samples. The positions of observed entries are randomly chosen, and the experiment is repeated 10 times for each sample size. (Please refer to Appendix B for additional experiments and detailed methodology.)", "description": "This figure shows how the connectivity of observed data influences implicit regularization in matrix factorization models for matrix completion.  It plots the rank and nuclear norm of solutions against the connectivity of the observed data for different sample sizes.  The results demonstrate a transition from low nuclear norm solutions to low-rank solutions as data connectivity increases.", "section": "Connectivity affects implicit regularization"}, {"figure_path": "9jgODkdH0F/figures/figures_35_3.jpg", "caption": "Figure 1: The connectivity of observed data affects the implicit regularization. The ground truth matrix M* \u2208 ]R4\u00d74 has rank ranging from 1 to 3. The sample size n covers settings where n is equal to, smaller than, and larger than the 2rd \u2013 r2 threshold required for exact reconstruction. Darker scatter points indicate a greater number of samples, while lighter points indicate fewer samples. The positions of observed entries are randomly chosen, and the experiment is repeated 10 times for each sample size. (Please refer to Appendix B for additional experiments and detailed methodology.)", "description": "This figure shows how the connectivity of observed data influences implicit regularization in matrix factorization models for matrix completion.  The x-axis represents the ground truth rank, and the y-axis represents the ground truth nuclear norm of the matrix to be completed. The different colored points represent different data connectivity scenarios (connected, disconnected with complete bipartite components, and otherwise disconnected). The size of each point is proportional to the number of samples used in the experiment.  The figure demonstrates a transition in implicit regularization from low nuclear norm to low rank as data connectivity increases.  More samples are shown as darker points. ", "section": "Connectivity affects implicit regularization"}]