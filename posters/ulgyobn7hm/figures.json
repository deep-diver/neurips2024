[{"figure_path": "uLGyoBn7hm/figures/figures_1_1.jpg", "caption": "Figure 2: Data generating model and the goal of learning disentangled causal latent representations.", "description": "This figure illustrates the process of learning disentangled causal latent representations from heterogeneous data and assumptions.  It shows a data generating model where high-dimensional data (e.g., images, text) X is a non-linear transformation of lower-dimensional latent variables V, which represent causal factors. The latent causal graph (LCG) depicts the causal relationships between these latent variables V. The goal is to learn the inverse of the mixing function (f\u207b\u00b9) to obtain disentangled representations of the latent causal variables, highlighting which latent variables can be separated from others given the data and causal assumptions.  This disentanglement is crucial for various downstream AI tasks.", "section": "Modeling Disentangled Representation Learning (General Case)"}, {"figure_path": "uLGyoBn7hm/figures/figures_2_1.jpg", "caption": "Figure 2: Data generating model and the goal of learning disentangled causal latent representations.", "description": "This figure illustrates the process of learning disentangled causal latent representations from heterogeneous data and assumptions. The left side shows the data generating model, which includes latent variables (V1, V2, V3), a mixing function (fx), and observed data (X) from multiple domains. The right side shows the goal of the learning process, which is to obtain a disentangled representation of the latent variables and a causal disentanglement map that highlights the relationships between the latent variables. The figure also shows an example of how the different data modalities (EMRs, imaging, bloodwork) can be used to learn the disentangled representations.", "section": "Modeling Disentangled Representation Learning (General Case)"}, {"figure_path": "uLGyoBn7hm/figures/figures_4_1.jpg", "caption": "Figure 2: Data generating model and the goal of learning disentangled causal latent representations.", "description": "This figure illustrates the process of learning disentangled causal latent representations from heterogeneous data. The data generating model consists of latent variables (V) that are mixed (via a function fx) to produce observed high-dimensional data (X).  The goal is to learn the inverse mixing function (fx\u207b\u00b9) and a disentangled representation of the latent variables, where some chosen latent variables are disentangled from others. This process is illustrated using a diagram that shows the relationship between the inputs (data and assumptions), the latent selection stage, the mixing process, the inverse mixing, and the causal disentanglement map that is the ultimate output.", "section": "Modeling Disentangled Representation Learning (General Case)"}, {"figure_path": "uLGyoBn7hm/figures/figures_6_1.jpg", "caption": "Figure 2: Data generating model and the goal of learning disentangled causal latent representations.", "description": "This figure illustrates the general framework of causal disentangled representation learning. The left side shows the data generating model, which involves latent variables (V) that are mixed through a nonlinear function (fx) to produce the observed data (X). The right side shows the goal of learning a disentangled causal latent representation, where the latent variables are separated and their relationships are clearly represented. This process involves finding the inverse of the mixing function (f\u207b\u00b9) to extract the latent variables from the observed data.", "section": "Modeling Disentangled Representation Learning (General Case)"}, {"figure_path": "uLGyoBn7hm/figures/figures_9_1.jpg", "caption": "Figure 6: Correlation of learned latent representations with true latent variables from Fig. 4.", "description": "This figure displays the results of experiments on four different causal graphs (chain, collider, non-Markovian, and another non-Markovian graph).  Each subplot shows the correlation between the learned latent variables and the ground truth latent variables for a specific causal graph. The mean correlation coefficient (MCC) is used as a metric and visualized using box plots for each latent variable (V1, V2, V3, V4).  Red indicates variables predicted by the CRID algorithm to be disentangled from other variables, and gray indicates those predicted to be entangled. The results demonstrate the ability of the CRID algorithm to identify disentangled variables in various causal settings, even in complex non-Markovian scenarios.", "section": "5 Experiments"}, {"figure_path": "uLGyoBn7hm/figures/figures_34_1.jpg", "caption": "Figure 2: Data generating model and the goal of learning disentangled causal latent representations.", "description": "The figure illustrates the data generation process and the goal of disentangled representation learning. On the left, a data generating model is shown, which includes a latent causal graph (LCG) representing the relationships between latent variables (V1, V2, V3). These latent variables are mixed through a non-linear function fx to generate high-dimensional data X (e.g., images, text). On the right, the goal of disentangled representation learning is depicted, where the disentangled latent representations are learned from data and assumptions about the underlying causal structure. The disentanglement map Gvv highlights which latent variables can be disentangled given the combination of data and assumptions. The overall goal is to learn the inverse of the mixing function fx and the disentangled latent representations V, where the latent variables (V1, V2, V3) are disentangled given the input distributions and assumptions regarding the causal structure.", "section": "Modeling Disentangled Representation Learning (General Case)"}, {"figure_path": "uLGyoBn7hm/figures/figures_34_2.jpg", "caption": "Figure 2: Data generating model and the goal of learning disentangled causal latent representations.", "description": "This figure illustrates the process of learning disentangled causal latent representations from heterogeneous data and assumptions about the underlying causal system.  The left side shows a data generating model with observed data (images, EMRs, bloodwork) arising from a combination of heterogeneous domains.  Latent variables (V1, V2, V3) generate the observed data through a nonparametric mixing function (fx). The right side shows the goal of the disentanglement process\u2014to learn the inverse mixing function (f-1) and disentangled latent representations, highlighting which variables can be disentangled given the data and assumptions.  This disentanglement is depicted using a causal disentanglement map (Gvv). The figure highlights the challenge of learning realistic causal representations from data with various modalities and the importance of disentangling latent causal variables to ensure accuracy and reliability.", "section": "Modeling Disentangled Representation Learning (General Case)"}, {"figure_path": "uLGyoBn7hm/figures/figures_35_1.jpg", "caption": "Figure 2: Data generating model and the goal of learning disentangled causal latent representations.", "description": "This figure illustrates the process of learning disentangled causal latent representations from heterogeneous data and assumptions.  The left side depicts the data generating process, starting with latent causal variables (V1, V2, V3) mixed by a non-linear function fx to produce high-dimensional observed data X, such as images or text. The causal relationships between the latent variables are shown as a directed acyclic graph (DAG).  The right side displays the goal of learning disentangled latent representations.  The learned representation should ideally mirror the structure of the original causal DAG, separating the latent variables that are causally independent, even if they are correlated in the observed data X.  The image shows the input, the process of latent selection and the mixing function, and the final disentangled representation.", "section": "Modeling Disentangled Representation Learning (General Case)"}, {"figure_path": "uLGyoBn7hm/figures/figures_42_1.jpg", "caption": "Figure 2: Data generating model and the goal of learning disentangled causal latent representations.", "description": "This figure illustrates the model for generating data (left panel) and the goal of disentangled causal representation learning (right panel).  The data is generated from latent causal variables (V) through a nonlinear mixing function (fx). The goal is to learn the inverse mixing function (f\u207b\u00b9) and a disentangled representation of the latent variables, highlighting which latent variables are disentangled from others given the observed data and causal assumptions. The figure shows an example with three latent variables (V\u2081, V\u2082, V\u2083) and their relationships, emphasizing the challenge of learning meaningful representations from high-dimensional data (X) that accurately captures these latent causal relationships.", "section": "Modeling Disentangled Representation Learning (General Case)"}, {"figure_path": "uLGyoBn7hm/figures/figures_43_1.jpg", "caption": "Figure 2: Data generating model and the goal of learning disentangled causal latent representations.", "description": "This figure illustrates the general framework of the proposed causal disentangled representation learning approach.  It shows a data generating model where observed high-dimensional data (e.g., images, text) X is generated from latent causal variables V through a nonlinear mixing function fx.  The goal of the learning task is to learn the inverse of this function and recover disentangled causal representations of the latent variables, highlighting which latent variables can be disentangled given the combination of data and assumptions encoded in a latent causal graph. The figure depicts the input, which includes data and assumptions in the form of a latent causal graph, and the output, which is a causal disentanglement map that shows the relationships between latent variables.", "section": "Modeling Disentangled Representation Learning (General Case)"}, {"figure_path": "uLGyoBn7hm/figures/figures_47_1.jpg", "caption": "Figure 2: Data generating model and the goal of learning disentangled causal latent representations.", "description": "This figure illustrates the process of learning disentangled causal latent representations. The left side shows a data generating model, where observed data X (e.g., EEG data, images, texts) is a nonlinear transformation of latent causal variables V (e.g., drug, sleep, seizures).  The model highlights the presence of a latent causal graph Gvv that describes relationships between these latent variables. The right side represents the goal, which is to learn a causal disentanglement map.  This map indicates the relationships between the original variables (V) and the disentangled representation (V), highlighting which variables can be disentangled from others given available data and assumptions regarding the causal structure.", "section": "Modeling Disentangled Representation Learning (General Case)"}, {"figure_path": "uLGyoBn7hm/figures/figures_47_2.jpg", "caption": "Figure 2: Data generating model and the goal of learning disentangled causal latent representations.", "description": "This figure illustrates the data generation process and the goal of learning disentangled causal latent representations.  The left side shows a data generating model with latent variables (V1, V2, V3) and an observed mixture variable X.  These latent variables are connected in a latent causal graph (LCG).  The observed variable X is a nonlinear transformation of the latent variables.  The right side depicts the desired output: a causal disentanglement map (CDM) showing which latent variables can be disentangled from each other given the observed data and assumptions. This demonstrates the goal of learning a representation where the latent causal factors are disentangled from each other, facilitating various downstream tasks.", "section": "Modeling Disentangled Representation Learning (General Case)"}, {"figure_path": "uLGyoBn7hm/figures/figures_50_1.jpg", "caption": "Figure 2: Data generating model and the goal of learning disentangled causal latent representations.", "description": "This figure illustrates the process of learning disentangled causal latent representations from heterogeneous data and assumptions.  The left side shows a data generating model with latent variables (V1, V2, V3) and observed high-dimensional data (X, e.g., images or text) which are non-linearly mixed by a function fx. The right side shows the desired disentangled representation with a causal disentanglement map (Gvv) which maps the latent variables to their disentangled representations.  The Sij nodes represent potential discrepancies in the generative process between different domains or interventions. The overall goal is to learn the inverse function f-1x and obtain a disentangled representation of the latent variables.", "section": "Modeling Disentangled Representation Learning (General Case)"}]