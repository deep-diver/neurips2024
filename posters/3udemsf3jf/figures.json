[{"figure_path": "3uDEmsf3Jf/figures/figures_1_1.jpg", "caption": "Figure 1: An example of distribution shaping in offline safe RL. We generate a low-cost and high-reward dataset from the original dataset for subsequent RL training.", "description": "The figure illustrates the concept of distribution shaping in offline safe reinforcement learning.  The left panel shows the original data distribution, which is spread out and includes many data points with both high cost and low reward, as well as low cost and low reward.  The right panel shows how the distribution is shaped using a data-centric approach (OASIS).  The new distribution generated by OASIS is concentrated in the area of low cost and high reward, which is beneficial for training a safe and rewarding RL agent. The dashed line represents the cost limit.", "section": "1 Introduction"}, {"figure_path": "3uDEmsf3Jf/figures/figures_3_1.jpg", "caption": "Figure 2: D\u2081 is a conservative dataset, and D\u2082 is a tempting dataset. Each point represents (C(\u03c4), R(\u03c4)) of a trajectory \u03c4 in the dataset.", "description": "This figure visually represents two types of datasets in offline safe reinforcement learning: conservative and tempting.  A conservative dataset (D\u2081) contains mostly low-reward, low-cost data points, while a tempting dataset (D\u2082) shows high-reward, high-cost data points.  The plot displays the cost return (x-axis) versus the reward return (y-axis) for each trajectory in each dataset.  A vertical dashed line indicates the cost limit (\u03ba). The means and variances of both reward and cost for the optimal policy (\u03c0*) as well as the mean reward and cost for the policies generating D\u2081 (\u03c0\u2081) and D\u2082 (\u03c0\u2082) are also shown for comparison, highlighting the mismatch between the behavior policy's data distribution and the optimal policy's characteristics.", "section": "4.1 Safe Dataset Mismatch Problem"}, {"figure_path": "3uDEmsf3Jf/figures/figures_3_2.jpg", "caption": "Figure 3: (a) Reweighting in the dataset with comprehensive coverage. (b) Reweighting in the tempting dataset. (c) Performance evaluation results of the normalized reward and cost. The cost threshold is 1. \u2191: the higher the reward, the better. \u2193: the lower the cost (up to threshold 1), the better. Bold: Safe agents whose normalized cost is smaller than 1. Gray: Unsafe agents. Blue: Safe agent with the highest reward.", "description": "This figure shows the effect of dataset reweighting on the performance of offline safe RL. Panel (a) shows the effect of reweighting on the full dataset, while panel (b) shows the effect of reweighting on a dataset with an excess of tempting trajectories (tempting dataset). The results show that reweighting can improve the performance of offline safe RL in the full dataset, but that it is less effective in the tempting dataset. Panel (c) presents a quantitative comparison of different algorithms based on normalized reward and cost, showing that OASIS significantly outperforms existing methods. ", "section": "4.2 Mitigating the Safe Dataset Mismatch"}, {"figure_path": "3uDEmsf3Jf/figures/figures_4_1.jpg", "caption": "Figure 4: OASIS: a data-centric approach for offline safe RL. Conditioned on the human preference, OASIS first curates an offline dataset with a conditioned diffusion data generator and learned labeling models, then trains safe RL agents with this generated dataset.", "description": "This figure illustrates the OASIS framework.  It starts with a pre-collected dataset containing state, action, next state, reward, and cost information.  A preference is specified, indicating desired reward and cost limits.  OASIS uses an inverse dynamics model and reward/cost models to process this data.  A conditional diffusion model then generates a new dataset shaped by the specified preference using condition-guided denoising. Finally, an offline safe RL agent is trained on this modified dataset. The image visually depicts the generation process using color-coded data points representing different cost and reward values.", "section": "4 Method"}, {"figure_path": "3uDEmsf3Jf/figures/figures_7_1.jpg", "caption": "Figure 5: Performance with different datasets and varying constraint thresholds. The visualization of these datasets is available in Appendix B.", "description": "This figure visualizes the performance of different offline safe RL algorithms across various datasets (Full, Conservative, Tempting, Hybrid) and constraint thresholds (\u03ba).  The datasets represent different data distributions relevant to the safe RL problem. The results illustrate how well each algorithm handles these scenarios, demonstrating the impact of data distribution on safety and reward in offline safe reinforcement learning. The x-axis represents the reward or cost return and the y-axis represents the algorithms.", "section": "5 Experiment"}, {"figure_path": "3uDEmsf3Jf/figures/figures_8_1.jpg", "caption": "Figure 6: Data efficiency on the Ball-Circle task with a tempting dataset.", "description": "This figure shows the results of an experiment evaluating the data efficiency of the proposed OASIS method and several baseline methods on the Ball-Circle task, which uses a tempting dataset. The x-axis represents the percentage of the training data used, and the y-axis shows the average reward and cost. The shaded regions represent the standard deviation across multiple trials. OASIS consistently outperforms baseline methods in terms of reward and cost, even with very limited data, demonstrating its high data efficiency.  The cost limit is shown as a horizontal dashed line.", "section": "5 Experiment"}, {"figure_path": "3uDEmsf3Jf/figures/figures_9_1.jpg", "caption": "Figure 7: (a)(b) Reward and cost performance of the generated data: E[r(s, a)], E[c(s, a)], (s, a) ~ dg. The x-axis and y-axis mean the reward and cost conditions and the values of both conditions and expectations are normalized with the same scale. (c) Visualization of the data density. 1: Car-circle task; 2: The density of the (x, y) position of the raw dataset; 3, 4: The density of the position (x, y) of the data generated by OASIS under conditions [0.1, 0.5] and [0.75, 0.75]; 5, 6: the density of the position (x, y) of the data generated by CVAE under conditions [0.1, 0.5] and [0.75, 0.75]", "description": "This figure visualizes the reward and cost of generated data by OASIS and CVAE under different conditions. It also shows the density of generated data and compares it with the original dataset. OASIS successfully shapes the distribution while CVAE fails to do so.", "section": "5.2 How can OASIS shape the dataset distribution?"}, {"figure_path": "3uDEmsf3Jf/figures/figures_18_1.jpg", "caption": "Figure 8: CVAE reconstruction. (a) Reward performance of the generated data: E [r(s, a)], (s, a) ~ dg, (b) Cost performance of the generated data: E [c(s, a)], (s, a) ~ dg. In (a) and (b), the x-axis and y-axis mean the reward and cost conditions, and the value of both conditions and expectations are normalized to the same scale: [0, 1]; (c) The data reconstruction results using the condition [0.1, 0.5] of 10 sampled trajectories in the dataset.", "description": "This figure compares the performance of the proposed OASIS method with a baseline method (CVAE) in generating data for offline safe reinforcement learning. It shows that CVAE fails to successfully encode conditions into the data generation process, resulting in similar results across different conditions. In contrast, OASIS demonstrates strong data generation capability and accurately reflects the specified reward and cost conditions.", "section": "B.2 Supplementary data generation comparison results"}, {"figure_path": "3uDEmsf3Jf/figures/figures_19_1.jpg", "caption": "Figure 9: Visualization of generated trajectories in the Drone-Circle task.", "description": "This figure compares the generated trajectories by OASIS and CVAE under two different conditions: low-cost-medium-reward and medium-cost-high-reward. While CVAE generates trajectories similar to the original data, OASIS successfully generates trajectories that satisfy the specified conditions, avoiding restricted areas when a low-cost condition is applied.", "section": "Supplementary experiments"}, {"figure_path": "3uDEmsf3Jf/figures/figures_20_1.jpg", "caption": "Figure 10: BallCircle Dataset types. Each point represents (C(\u03c4), R(\u03c4)) of a trajectory \u03c4 in the dataset.", "description": "This figure displays four scatter plots visualizing the relationship between cost return and reward return for four different dataset types in the BallCircle task: Full dataset, Tempting dataset, Conservative dataset, and Hybrid dataset.  Each point in the scatter plots represents a single trajectory (\u03c4) from the dataset, with its cost return (C(\u03c4)) plotted on the x-axis and reward return (R(\u03c4)) plotted on the y-axis. The plots illustrate how the distribution of cost and reward returns differs across the four dataset types, which are designed to present different characteristics relevant to offline safe reinforcement learning. A dashed vertical line indicates the cost limit (\u03ba).", "section": "C.2 Dataset details"}, {"figure_path": "3uDEmsf3Jf/figures/figures_20_2.jpg", "caption": "Figure 3: (a) Reweighting in the dataset with comprehensive coverage. (b) Reweighting in the tempting dataset. (c) Performance evaluation with different weights and datasets.", "description": "This figure shows the results of a reweighting experiment to mitigate the safe dataset mismatch (SDM) problem.  Panel (a) shows the results when comprehensive coverage is present in the dataset; Panel (b) when there is a tempting dataset. (c) summarizes the results of the reweighting experiment, demonstrating its effectiveness. The experiment involves reweighting the dataset to address the SDM problem, improving offline safe RL performance. The experiment uses several datasets, including a full dataset, a tempting dataset, and a conservative dataset.", "section": "4.2 Mitigating the Safe Dataset Mismatch"}, {"figure_path": "3uDEmsf3Jf/figures/figures_20_3.jpg", "caption": "Figure 5: Performance with different datasets and varying constraint thresholds. The visualization of these datasets is available in Appendix B.", "description": "This figure compares the performance of different offline safe reinforcement learning algorithms across various datasets (Full, Conservative, Tempting, Hybrid) and constraint thresholds.  It visually demonstrates the impact of dataset composition and safety constraints on algorithm effectiveness. The visualization of the datasets themselves is provided in Appendix B for detailed reference.", "section": "5 Experiment"}, {"figure_path": "3uDEmsf3Jf/figures/figures_21_1.jpg", "caption": "Figure 13: U-Net structure.", "description": "This figure shows the architecture of the U-Net used as the denoising core in the OASIS method.  It consists of a series of down-sampling (downsampling) layers that reduce spatial dimensions, followed by mid layers that process the features, and then upsampling (upsampling) layers that increase spatial dimensions.  Finally, convolutional (Conv layers) layers transform the processed features into the desired output.  The inputs include time embeddings (\"temb\") and condition embeddings (\"yemb\") generated by multi-layer perceptrons (MLPs) from the time step and condition information respectively. The output is the denoised trajectory.", "section": "C Implementation details"}]