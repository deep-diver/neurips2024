[{"figure_path": "3uDEmsf3Jf/tables/tables_5_1.jpg", "caption": "Table 1: Evaluation results of the normalized reward and cost. The cost threshold is 1. \u2191: the higher the reward, the better. \u2193: the lower the cost (up to threshold 1), the better. Bold: Safe agents whose normalized cost is smaller than 1. Gray: Unsafe agents. Blue: Safe agent with the highest reward.", "description": "This table presents the performance comparison of different offline safe reinforcement learning algorithms on various tasks.  The metrics used are the normalized reward and cost, indicating how well the algorithms achieve high rewards while maintaining safety constraints (cost below the threshold of 1).  The table highlights the superior performance of the proposed OASIS method compared to existing baselines in terms of both reward and safety.", "section": "5 Experiment"}, {"figure_path": "3uDEmsf3Jf/tables/tables_7_1.jpg", "caption": "Table 1: Evaluation results of the normalized reward and cost. The cost threshold is 1. \u2191: the higher the reward, the better. \u2193: the lower the cost (up to threshold 1), the better. Bold: Safe agents whose normalized cost is smaller than 1. Gray: Unsafe agents. Blue: Safe agent with the highest reward.", "description": "This table presents the performance comparison of different offline safe RL algorithms across various tasks.  The algorithms are evaluated based on their normalized reward (higher is better) and normalized cost (lower is better, with a threshold of 1 for safety).  The table highlights which algorithms successfully maintain safety (cost \u22641) while achieving high rewards.  Results are shown for different robot types performing different tasks (Run and Circle).", "section": "5 Experiment"}, {"figure_path": "3uDEmsf3Jf/tables/tables_8_1.jpg", "caption": "Table 1: Evaluation results of the normalized reward and cost. The cost threshold is 1. \u2191: the higher the reward, the better. \u2193: the lower the cost (up to threshold 1), the better. Bold: Safe agents whose normalized cost is smaller than 1. Gray: Unsafe agents. Blue: Safe agent with the highest reward.", "description": "This table presents the performance evaluation results of different algorithms on various tasks.  The normalized reward and cost are used as metrics.  Higher normalized reward indicates better performance, while lower normalized cost (below 1) represents successful constraint satisfaction.  The table highlights safe and unsafe agents and identifies the algorithm achieving the best reward among safe agents for each task.", "section": "5 Experiment"}, {"figure_path": "3uDEmsf3Jf/tables/tables_19_1.jpg", "caption": "Table 1: Evaluation results of the normalized reward and cost. The cost threshold is 1. \u2191: the higher the reward, the better. \u2193: the lower the cost (up to threshold 1), the better. Bold: Safe agents whose normalized cost is smaller than 1. Gray: Unsafe agents. Blue: Safe agent with the highest reward.", "description": "This table presents a quantitative comparison of different offline safe reinforcement learning algorithms across various tasks (BallRun, CarRun, DroneRun, BallCircle, CarCircle, DroneCircle).  The performance is evaluated using two metrics: normalized reward (higher is better) and normalized cost (lower is better, with a threshold of 1 indicating constraint satisfaction).  The table highlights which algorithms successfully satisfy safety constraints (bold values) and identifies the best-performing safe algorithm for each task.", "section": "5 Experiment"}, {"figure_path": "3uDEmsf3Jf/tables/tables_21_1.jpg", "caption": "Table 1: Evaluation results of the normalized reward and cost. The cost threshold is 1. \u2191: the higher the reward, the better. \u2193: the lower the cost (up to threshold 1), the better. Bold: Safe agents whose normalized cost is smaller than 1. Gray: Unsafe agents. Blue: Safe agent with the highest reward.", "description": "This table presents the performance comparison of different offline safe RL algorithms on various tasks.  The metrics used are the normalized reward and cost.  A lower normalized cost (below 1) indicates the algorithm satisfied the safety constraint, shown in bold. The highest reward among safe algorithms is highlighted in blue. Gray indicates unsafe algorithms.", "section": "5 Experiment"}]