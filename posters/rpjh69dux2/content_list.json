[{"type": "text", "text": "Meta-Reinforcement Learning with Universal Policy Adaptation: Provable Near-Optimality under All-task Optimum Comparator ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Siyuan Xu & Minghui Zhu School of Electrical Engineering and Computer Science The Pennsylvania State University University Park, PA 16801 {spx5032, muz16}@psu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Meta-reinforcement learning (Meta-RL) has attracted attention due to its capability to enhance reinforcement learning (RL) algorithms, in terms of data efficiency and generalizability. In this paper, we develop a bilevel optimization framework for meta-RL (BO-MRL) to learn the meta-prior for task-specific policy adaptation, which implements multiple-step policy optimization on one-time data collection. Beyond existing meta-RL analyses, we provide upper bounds of the expected optimality gap over the task distribution. This metric measures the distance of the policy adaptation from the learned meta-prior to the task-specific optimum, and quantifies the model\u2019s generalizability to the task distribution. We empirically validate the correctness of the derived upper bounds and demonstrate the superior effectiveness of the proposed algorithm over benchmarks. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Meta-learning [58, 15, 25] aims to extract the shared prior knowledge, known as meta-prior, from the similarities and interdependencies of multiple existing learning tasks, in order to accelerate the learning process, increase the efficiency of data usage, and improve the overall learning performance in new tasks. Meta-learning has been extended to solve RL problems, known as meta-RL [15, 5], and shows its promise to overcome the challenges of traditional RL algorithms, including scarce real-world data [3, 44, 65], limited computing resources, and slow learning speed [54, 63]. ", "page_idx": 0}, {"type": "text", "text": "Meta-learning methods can be generally categorized into optimization-based, model-based (black box methods), and metric-based methods [27, 5]. The optimization-based meta-learning approach [25] is compatible with any model trained by an optimization algorithm, such as gradient descent, and thus is applicable to a vast range of learning problems, including RL problems. Specifically, it formulates meta-learning as a bilevel optimization problem. At the lower-level optimization, the task-specific model is adapted from a shared meta-parameter by an optimization algorithm. At the upper-level optimization, the meta-parameter is to maximize the meta-objective, i.e., the performance of the model adapted from the meta-parameter over training tasks. The existing methods, including MAML and its variants [15, 38, 12], take a one-step gradient ascent as the lower-level policy optimization algorithm, which limits its data inefficiency and leads to sub-optimality. ", "page_idx": 0}, {"type": "text", "text": "During the meta-test, MAML conducts one-time data collection, i.e., collecting data using one policy (the meta-policy), and adapts the policy by one step of policy gradient to the new task. However, the collected data is only used in one policy gradient step, which may not sufficiently leverage the data and potentially fail to achieve a good performance. To mitigate the issue, a typical practice is to implement the data collection and the policy gradient alternately multiple times [15]. However, the environment exploration is usually costly and time-consuming during the meta-test in applications of meta-RL [44, 6, 36]. As a result, the low data efficiency limits the optimality of task-specific policies. In contrast, in this paper, we collect data by meta-policy for one time and utilize multiple policy optimization steps to improve the data efficiency. ", "page_idx": 0}, {"type": "table", "img_path": "rpjh69DUX2/tmp/c10e2f7e8ddfe5c527cffd0313578931f642b295c229ee7039f9841daedb0aa0.jpg", "table_caption": ["Table 1: Solved theoretical challenges of meta-RL "], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "The optimality analysis of MAML is studied in [12, 60] with a metric of optimality on the metaobjective, where the error of the meta-objective is defined by the expectation of the optimality gap between the task-specific policy adapted from the learned meta-parameter and the policy adapted from the best meta-parameter [60, 14, 26]. However, the best meta-parameter is shared for all tasks. Even if the meta-objective error is close to zero, i.e., the learned meta-parameter is close to the best one, the model adapted from the learned meta-parameter might be far from task-specific optimum for some tasks. In contrast, we aim to design a meta-RL algorithm that can fti a stronger optimality metric, called near-optimality under all-task optimum, where the comparator, i.e., the policy adapted from the best meta-parameter, is replaced by the task-specific optimal policy for each task. This metric offers a more strict comparator for the model adapted from the learned meta-parameter, i.e., when the metric achieves zero, the policy adaptation produces the optimal policy for every task. A similar metric is studied by [42]. It assumes that the task-specific optimal expert policy for each task is accessible and serves the supervision for policy adaptation during meta-training, which alleviates the analysis difficulty caused by the optimal policy comparator. However, the expert policy supervision is not accessible in a standard meta-RL problem. The metric under all-task optimum is also studied by [9, 10, 65] in the context of supervised meta-learning. ", "page_idx": 1}, {"type": "text", "text": "Main contribution. We develop a bilevel optimization framework for meta-RL, which implements multiple-step policy optimization on one-time data collection during task-specific policy adaptation. The overall contributions are summarized as follows. (i) We develop a universal policy optimization algorithm, which performs multiple optimization steps to maximize a surrogate of the accumulated reward function. The surrogate is developed only using one-time data collection. It includes various widely used policy optimization algorithms, including the policy gradient, the natural policy gradient (NPG) [30], and the proximal policy optimization (PPO) [52] as the special cases. Then, to learn the mete-prior, we formulate the meta-RL problem as a bilevel optimization problem, where the lower-level optimization is the universal policy optimization algorithm from the meta-policy and the upper-level optimization is to maximize the meta-objective function, i.e., the total reward of the models adapted from the meta-policy. (ii) We derive the implicit differentiation for both unconstrained and constrained lower-level optimization problems to compute the hypergradient, i.e., the gradient of the meta-objective, and propose the meta-training algorithm. In contrast to [60], we do not require to know the closed-form solution of the lower-level optimization. (iii) We derive upper bounds that quantify (a) the optimality gap between the adapted policy and the optimal task-specific policy for any task, and (b) the expected optimality gap over the task distribution. Since the proposed framework incorporates several existing meta-RL methods, such as MAML, as a special case, the analysis also provides the theoretical motivation for them. (iv) We conduct experiments to validate the theoretical bounds and verify the efficacy of the proposed algorithm on meta-RL benchmarks. ", "page_idx": 1}, {"type": "text", "text": "Table 1 compares the solved theoretical challenges of meta-RL between this paper and previous works [12, 57, 60, 42]. Specifically, paper [60] derives the optimality on the meta-objective under the assumption of bounded hypergradient. Papers [12, 57] consider the convergence of the meta-objective. The near-optimality under all-task optimum is considered in [42]. However, it assumes the optimal expert policies of the training tasks are available in meta-training, such that it can learn to approach the expert policies, while the other methods do not require the expert policies and learn from the explorations of the environments. In this paper, we show the convergence and optimality guarantee on the meta-objective, and, more importantly, the optimality guarantee under the all-task optimum comparator. It is noted that the optimality on the meta-objective is an immediate result from [60]. ", "page_idx": 1}, {"type": "text", "text": "2 Related works. ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Categorization of meta-RL. Meta-RL methods can be generally categorized into (i) optimizationbased meta-RL, (ii) black-box (also called context-based) meta-RL. Optimization-based meta-RL approaches, such as MAML [15] and its variants [55, 38], usually include a policy adaptation algorithm and a meta-algorithm. During the meta-training, the meta-algorithm aims to learn a meta-policy, such that the policy adaptation algorithm can achieve good performance starting from the meta-policy. The learned meta-policy parameter is adapted to the new task using the policy adaptation algorithm during the meta-test. Black-box meta-RL [11, 59, 49, 47, 68] aims to learn an end-to-end neural network model. The model has fixed parameters for the policy adaptation during the meta-test, and generates the task-specific policy using the trajectories of the new task takes. In optimization-based meta-RL, the task-specific policy is adapted from a shared meta-policy over the task distribution. The learned meta-knowledge is not specialized for each task, and its meta-test performance on a task depends on a general policy optimization algorithm applied to new data from that task. In contrast, the end-to-end model in black-box meta-RL typically includes specialized knowledge for any task within the task distribution, and uses the new data merely as an indicator to identify the task within the distribution. As a result, the optimality of optimization-based methods is usually worse than black-box methods, especially when the task distribution is heterogeneous and the data scale for adaptation is extremely small. On the other hand, the policy adaptation algorithms in the meta-test of optimization-based methods can generally improve the policy starting from any initial policy, not only the learned meta-policy. Therefore, it is robust to sub-optimal meta-policy and can deal with tasks that are out of the training task distribution [16, 62]. In contrast, due to the specialization of the learned model, black-box methods cannot be generalized outside of the training task distribution. In this paper, we focus on the category of optimization-based meta-RL and compare the proposed algorithm with the existing optimization-based meta-RL approaches in terms of both experimental results and theory. ", "page_idx": 2}, {"type": "text", "text": "Bilevel optimization in meta-RL. Bilevel optimization has been widely studied empirically [45, 21, 17, 18, 53, 29] and theoretically [20, 22, 29]. It has been applied to many machine learning problems, including meta-learning [35, 48], hyperparameter optimization [45, 17, 18], RL [24, 34], and inverse RL [39, 40, 41]. Since the overall objective function in bilevel optimization is generally non-convex, theoretical analyses of bilevel optimization mainly focus on the algorithm convergence [20, 29, 64], rarely on the optimality. This paper formulates meta-RL as a bilevel optimization problem. The key theoretical contribution of this paper is to derive upper bounds on the near-optimality under all-task optimum, i.e., the expected optimality of the solutions of the lower-level optimization compared with that of the task-specific optimal policies. The near-optimality under all-task optimum is unique to meta-learning and has not been studied in the literature on bilevel optimization. ", "page_idx": 2}, {"type": "text", "text": "3 Problem statement ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "MDP. A Markov decision process (MDP) $\\mathcal{M}\\triangleq\\{S,\\mathcal{A},\\gamma,\\rho,P,r\\}$ is defined by the bounded state space $\\boldsymbol{S}$ , the discrete or bounded continuous action space $\\boldsymbol{\\mathcal{A}}$ , the discount factor $\\gamma$ , the initial state distribution $\\rho$ over $\\boldsymbol{S}$ , the transition probability $P(s^{\\prime}|s,a):\\mathcal{S}\\times\\mathcal{A}\\times\\mathcal{S}\\to[0,1]$ , and the reward function $r:S\\times A\\times S\\rightarrow[0,r_{m a x}]$ . ", "page_idx": 2}, {"type": "text", "text": "Policy and value function. A stochastic policy $\\pi:S\\to\\mathbb{P}({\\mathcal{A}})$ is a map from states to probability distributions over actions, and $\\pi(a|s)$ denotes the probability of selecting action $a$ in state $s$ . For a policy $\\pi$ , the value function is defined as $\\begin{array}{r}{V^{\\pi}(s)\\triangleq\\mathbb{E}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}r\\left(s_{t},a_{t},s_{t+1}\\right)\\vert s_{0}=s,\\pi\\right]}\\end{array}$ . The action-value function is defined as $\\begin{array}{r}{Q^{\\pi}(s,a)\\triangleq\\mathbb{E}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}r\\left(s_{t},a_{t},s_{t+1}\\right)\\middle|s_{0}=s,a_{0}=a,\\pi\\right]}\\end{array}$ . The advantage function is defined as $A^{\\pi}(s,a)\\,\\triangleq\\,Q^{\\pi}(s,a)\\,-\\,V^{\\pi}(s)$ . The accumulated reward function is $J(\\pi)\\triangleq\\mathbb{E}_{s\\sim\\rho}\\left[V^{\\pi}(s)\\right]$ . Define the discounted state visitation distribution of a policy $\\pi$ as $\\begin{array}{r}{\\nu^{\\pi}(s)\\,\\triangleq\\,\\mathbb{E}_{s_{0}\\sim\\rho}[(1-\\gamma)\\sum_{t=0}^{\\infty}\\gamma^{t}\\mathbb{P}\\left(s_{t}=s|\\pi\\right)]}\\end{array}$ . In this paper, we consider parametric policy $\\pi_{\\theta}$ , parameterized by $\\theta$ . The optimal parameter $\\theta^{*}$ can maximize the accumulated reward function, i.e., $\\theta^{*}\\triangleq\\operatorname{argmax}_{\\theta}J(\\pi_{\\theta})$ . If $\\theta^{*}$ is not unique, denote the set of the optimal solutions by $\\Theta^{*}$ . ", "page_idx": 2}, {"type": "text", "text": "Meta-reinforcement learning. Meta-RL aims to solve multiple RL tasks. Consider a space of RL tasks $\\Gamma$ , where each task $\\tau\\in\\Gamma$ is modeled by a MDP $\\mathcal{M}_{\\tau}\\triangleq\\{S,\\mathcal{A},\\gamma,\\rho_{\\tau},P_{\\tau},r_{\\tau}\\}$ . Correspondingly, the notations $V_{\\tau}^{\\pi}$ , $Q_{\\tau}^{\\pi}$ , $A_{\\tau}^{\\pi}$ , $\\nu_{\\tau}^{\\pi}$ , $\\theta_{\\tau}^{*}$ , $\\Theta_{\\tau}^{*}$ and $J_{\\tau}$ are defined for task $\\tau$ . The RL tasks follow a probability distribution $\\mathbb{P}(\\Gamma)$ . Meta-RL aims to learn a meta-policy $\\pi_{\\phi}$ parameterized by a meta parameter $\\phi$ , such that it can adapt to an unseen task $\\tau_{n e w}\\sim\\mathbb{P}(\\Gamma)$ with a few iterations and a small number of new environment explorations. In specific, during the meta-training, several tasks can be i.i.d. sampled from $\\mathbb{P}(\\Gamma)$ , i.e., $\\{\\tau_{j}\\}_{j=1}^{T}\\sim\\mathbb{P}(\\Gamma)$ , and the tasks\u2019 MDPs $\\{\\mathcal{M}_{\\tau_{j}}\\}_{j=1}^{\\overline{{T}}}$ can be explored. The meta-learner applies a meta-algorithm to update the meta parameter $\\phi$ by using the data collected from the sampled tasks. During the meta-test, a new task $\\tau_{n e w}$ is given, one time of a within-task algorithm $\\boldsymbol{A l g}$ with data collected from $\\tau_{n e w}$ is applied, the meta-parameter $\\phi$ is adapted to the task-specific parameter $\\theta_{\\tau_{n e w}}^{\\prime}$ and the task-specific policy $\\pi_{\\theta_{\\tau_{n e w}}^{\\prime}}$ is tested on the task $\\tau_{n e w}$ . ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Optimality Metric. Consider a meta-RL algorithm that produces a meta-parameter $\\phi$ , and the take-specific parameter $\\pi_{\\theta_{\\tau}^{\\prime}}$ is adapted from the meta-parameter $\\phi$ on a task $\\tau$ , denoted as $\\pi_{\\theta_{\\tau}^{\\prime}}=$ $A l g(\\pi_{\\phi},\\tau)$ . We define the task-expected optimality gap (TEOG) as the metric to evaluate the algorithm, i.e., $\\mathbb{E}_{\\tau\\sim\\mathbb{P}(\\Gamma)}[J_{\\tau}(\\pi_{\\theta_{\\tau}^{*}})-\\mathbf{\\bar{\\alpha}}J_{\\tau}(A l g(\\bar{\\pi}_{\\phi},\\tau))]$ , where $\\theta_{\\tau}^{*}$ is the optimal parameter for task $\\tau$ . First, the TEOG considers the expected error over the task distribution $\\mathbb{P}(\\Gamma)$ , reflecting the generalizability of the produced meta-parameter. Second, the TEOG adopts the comparator of the optimal task-specific policy $\\pi_{\\theta_{\\tau}^{*}}$ for any task $\\tau$ (all-task optimum comparator), and evaluates the optimality gap $J_{\\tau}(\\pi_{\\theta_{\\tau}^{*}})\\!-\\!J_{\\tau}(\\mathcal{A}l\\dot{g}(\\pi_{\\phi},\\tau))$ . In contrast, [60, 14, 26] adopts the comparator of the policy adapted from the optimal meta-parameter $\\pi_{\\phi^{*}}$ , and evaluates the optimality gap $J_{\\tau}(\\mathcal{A}l g(\\pi_{\\phi^{*}},\\tau))-$ $J_{\\tau}(\\mathcal{A}l g(\\pi_{\\phi},\\tau))$ . The latter only considers the optimality on the meta-objective, i.e., how well the trained meta-objective can approach the optimal meta-objective. However, even if the error of the meta-objective is approaching zero, i.e., the learned meta-policy is close to the best candidate, the performance of the model adapted from the optimal meta-policy might still be lacking. This is because policy optimization usually requires thousands of value/policy iterations to converge; when tasks are heterogeneous, even if it starts from the best meta-policy, one time of $\\boldsymbol{A l g}$ with one time of value estimate may not be sufficient. In contrast, if our metric is zero, the policy adapted from the meta-parameter to any task is optimal for the task. ", "page_idx": 3}, {"type": "text", "text": "Policy distance and task variance. To find the solution for a new task within a few iterations of policy optimization, it is crucial that the meta-policy $\\pi_{\\phi}$ can benefti from learning on correlated tasks. Similar to [4, 9, 31], we measure the correlation of tasks in the task distribution $\\bar{\\mathbb{P}}(\\Gamma)$ by its variance, defined by the minimal mean square of the distances among the optimal task-specific policies, i.e., $\\begin{array}{r}{\\mathcal{V}a r(\\mathbb{P}(\\Gamma))\\;\\triangleq\\;\\operatorname*{min}_{\\theta}\\;\\operatorname*{min}_{\\theta_{\\tau}^{*}\\in\\Theta_{\\tau}^{*}}\\;\\mathbb{E}_{\\tau\\sim\\mathbb{P}(\\Gamma)}[D_{\\tau}^{2}(\\pi_{\\theta},\\pi_{\\theta_{\\tau}^{*}})]}\\end{array}$ . Here, $D_{\\tau}(\\pi_{\\theta},\\pi_{\\theta_{\\tau}^{*}})$ is the distance metric between $\\pi_{\\theta}$ and $\\pi_{\\theta_{\\tau}^{*}}$ on the task $\\tau$ and is defined by $D_{\\tau}(\\pi_{\\theta},\\pi_{\\theta^{\\prime}})\\triangleq\\sqrt{\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\pi_{\\theta}}}[d^{2}(\\pi_{\\theta}(\\cdot|s),\\pi_{\\theta^{\\prime}}(\\cdot|s))]}$ , where $d(\\pi_{\\theta}(\\cdot|s),\\pi_{\\theta^{\\prime}}(\\cdot|s))$ is the distance of the policies $\\pi_{\\theta}$ and $\\pi_{\\theta^{\\prime}}$ on the state $s$ . ", "page_idx": 3}, {"type": "text", "text": "Note that the distance metrics $D_{\\tau}(\\cdot,\\cdot)$ and $d(\\cdot,\\cdot,s)$ can be custom-defined, leading to multiple policy update algorithms, as shown in Section 4. Here, we introduce several examples of $d(\\cdot,\\cdot,s)$ and $D_{\\tau}(\\cdot,\\cdot)$ , which are commonly used as the distance metrics in RL literature [51, 30, 37]. For policies $\\pi_{\\theta}$ and $\\pi_{\\theta^{\\prime}}$ , we apply (i) the KL-divergence of the action probability distribution, i.e., $d_{1}^{2}(\\pi_{\\theta},\\pi_{\\theta^{\\prime}},s)\\;\\triangleq\\;D_{\\mathrm{KL}}(\\pi_{\\theta}(\\cdot|s)\\|\\pi_{\\theta^{\\prime}}(\\cdot|s))$ , which is similar to the definition in [31]; (ii) The KLdivergence with the other order, i.e., $d_{2}^{2}(\\pi_{\\theta},\\pi_{\\theta^{\\prime}},s)\\,\\triangleq\\,D_{\\mathrm{KL}}(\\pi_{\\theta^{\\prime}}(\\cdot|s)\\|\\pi_{\\theta}(\\cdot|s))$ ; (iii) the Euclidean distance of the parameters, i.e., $d_{3}^{2}(\\pi_{\\theta},\\pi_{\\theta^{\\prime}},s)\\triangleq\\lVert\\theta-\\theta^{\\prime}\\rVert^{2}$ . Correspondingly, for $i=1,2$ , and 3, we define $\\begin{array}{r}{D_{\\tau,i}(\\pi_{\\theta},\\pi_{\\theta^{\\prime}})\\triangleq\\sqrt{\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\pi_{\\theta}}}[d_{i}^{2}(\\pi_{\\theta},\\pi_{\\theta^{\\prime}},s)]}}\\end{array}$ . Note that the distance metrics (i)(ii) are not symmetric, i.e., $D_{\\tau}(\\pi_{\\theta^{\\prime}},\\pi_{\\theta^{\\prime\\prime}})\\neq D_{\\tau}(\\pi_{\\theta^{\\prime\\prime}},\\pi_{\\theta^{\\prime}})$ , and (iii) is symmetric. ", "page_idx": 3}, {"type": "text", "text": "In the subsequent sections, we present algorithms based on the generalized distance definitions of $D_{\\tau}(\\cdot,\\cdot)$ and $d(\\cdot,\\cdot,s)$ . Moreover, we conduct analyses for the introduced distance metrics, from $D_{\\tau,1}$ to $D_{\\tau,3}$ , to provide comprehensive insights into their respective performances. ", "page_idx": 3}, {"type": "text", "text": "4 Meta-Reinforcement Learning Framework ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we develop a meta-RL algorithm by bilevel optimization, where the lower-level optimization is the within-task algorithm that adapts the parameter from the meta-parameter and the upper-level optimization is the meta-algorithm that obtains the meta-parameter. The proposed algorithm has two distinctions compared with existing algorithms. First, it uses one time of a universal policy optimization algorithm as the lower-level within-task algorithm. Second, we derive the hypergradient by the implicit differentiation, where the closed-form solution of the lower-level optimization is not required. ", "page_idx": 3}, {"type": "text", "text": "Within-task algorithm. Consider the policy optimization from the meta policy as the within-task algorithm $\\boldsymbol{A l g}$ . Specifically, given the meta-parameter $\\phi$ and a task $\\tau$ , the task-specific policy $\\pi_{\\theta_{\\tau}^{\\prime}}\\,=\\,\\mathcal{A}l g(\\pi_{\\phi},\\lambda,\\tau)$ is defined by $\\theta_{\\tau}^{\\prime}\\,=\\,\\mathrm{argimax}_{\\theta}\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\pi_{\\phi}},a\\sim\\pi_{\\theta}(\\cdot|s)}\\left[Q_{\\tau}^{\\pi_{\\phi}}(s,a)\\right]\\,-\\,\\mathbf{\\dot{\\lambda}}D_{\\tau}^{2}(\\pi_{\\phi}^{\\bullet},\\pi_{\\theta})$ . When the action space $\\boldsymbol{\\mathcal{A}}$ is discretized and the policy is tabular, i.e., the probabilities of actions are independent between different states, the above problem can be solved by $\\pi_{\\theta_{\\tau}^{\\prime}}(\\cdot|s)=$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{A}l g(\\pi_{\\phi},\\lambda,\\tau)(\\cdot|s)=\\underset{\\pi_{\\theta}(\\cdot|s)}{\\mathrm{argmax}}\\sum_{a\\in\\mathcal{A}}\\pi_{\\theta}(a|s)Q_{\\tau}^{\\pi_{\\phi}}(s,a)-\\lambda d^{2}(\\pi_{\\phi}(\\cdot|s),\\pi_{\\theta}(\\cdot|s)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "for all states $s\\ \\in\\ S$ . When the policy is parameterized by an approximation function, in both continuous and discrete action space $\\boldsymbol{\\mathcal{A}}$ $\\bar{4},\\pi_{\\theta_{\\tau}^{\\prime}}=\\mathcal{A}l g(\\pi_{\\phi},\\lambda,\\tau)$ is computed by $\\theta_{\\tau}^{\\prime}=$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{argmax}_{\\theta}\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\pi_{\\phi}},a\\sim\\pi_{\\phi}(\\cdot\\vert s)}\\left[\\frac{\\pi_{\\theta}(a\\vert s)}{\\pi_{\\phi}(a\\vert s)}Q_{\\tau}^{\\pi_{\\phi}}(s,a)\\right]-\\lambda D_{\\tau}^{2}\\left(\\pi_{\\phi},\\pi_{\\theta}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In (1) and (2), $\\lambda\\,>\\,0$ is a tuning hyperparameter and the distance metric $D_{\\tau}$ can be arbitrarily chosen. Considering the explorations for the task $\\tau$ are limited, $\\boldsymbol{A l g}$ only needs to evaluate the $Q_{\\tau}^{\\pi_{\\phi}^{\\bullet}}$ by Monte-Carlo sampling on a single policy $\\pi_{\\phi}$ , where the data sampling complexity is exactly the same as the one-step gradient descent in MAML [15]. Therefore, we denote $\\boldsymbol{A l g}$ , i.e., collecting data on the meta-policy and solving the optimal solution of (1) and (2) as the one-time policy adaptation. More details about the data sample complexity and the computational complexity of (1) and (2) are clarified in Appendix F. On the other hand, one gradient step is usually not sufficient to identify a good policy. Therefore, $\\boldsymbol{A l g}$ is to solve the optimal solution of (1) or (2). As shown in Section 5.4, the objective function of (1) or (2) is an approximation of the true objective function $J_{\\tau}(\\pi)$ . ", "page_idx": 4}, {"type": "text", "text": "Note that the objective function in (1) and (2) can reduce to that of multiple widely used policy optimization approaches: (i) PPO in [51, 52] when $D_{\\tau}=D_{\\tau,2}$ ; (ii) a variant of the PPO [60, 37], when $D_{\\tau}\\ =\\ D_{\\tau,1}$ ; (iii) the proximally regularized policy update, i.e., the policy optimization regularized by Euclidean distance of the policy parameter [51], when $D_{\\tau}=D_{\\tau,3}$ . Moreover, (iv) if we approximate the expectation in (2) by its first-order approximation and also select $D_{\\tau}=D_{\\tau,3}$ , the within-task algorithm (2) also can be reduced to one-step policy gradient, as shown in Appendix $_\\mathrm{H}$ ; (v) if we use the first-order approximation of the expectation in (2), the second-order approximation of the term $D_{\\tau}^{2}(\\pi_{\\phi},\\pi_{\\theta})$ , and select $D_{\\tau}=D_{\\tau,2}$ , the within-task algorithm (2) is reduced to the natural policy gradient (NPG). ", "page_idx": 4}, {"type": "text", "text": "Meta-algorithm. The performance of the meta-parameter $\\phi$ is evaluated by the meta-objective function, which is defined as the expected accumulated reward after the parameter is adapted by the within-task algorithm, i.e., $\\mathbb{E}_{\\tau\\sim\\mathbb{P}(\\Gamma)}[J_{\\tau}(\\boldsymbol{\\mathcal{A l g}}(\\pi_{\\phi},\\lambda,\\tau))]$ . In the meta-algorithm, we maximize the meta-objective to obtain the optimal meta-parameter $\\phi^{*}$ , i.e., ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\boldsymbol{\\phi}^{*}=\\underset{\\boldsymbol{\\phi}}{\\mathrm{argmax}}\\;\\mathbb{E}_{\\boldsymbol\\tau\\sim\\mathbb{P}(\\Gamma)}[J_{\\boldsymbol\\tau}(\\boldsymbol{\\mathcal{A}l}\\boldsymbol{g}(\\pi_{\\phi},\\lambda,\\tau))].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "As (1) and (2) provide multiple choices of the within-task algorithms when selecting different $D_{\\tau}$ , the meta-algorithm (3) provides the algorithms to learn the corresponding meta-priors. For example, (3) takes on the role of the meta-PPO algorithm when $D_{\\tau}\\;=\\;D_{\\tau,1}$ or $D_{\\tau,2}$ , i.e., (3) learns the meta-initialization for PPO. It is a meta-NPG algorithm with the corresponding approximation and $D_{\\tau}$ . Moreover, when $\\mathcal{A}l g(\\pi_{\\phi},\\lambda,\\tau)$ in (2) reduces to the one-step policy gradient shown in (iv) of the last paragraph, (3) represents a precise formulation of MAML in [15]. More details about the formulation and its relations with MAML are shown in Appendix G and H. ", "page_idx": 4}, {"type": "text", "text": "Hypergradient computation. Simlar to [29, 64], the meta-algorithm in (3) aims to solve a bilevel optimization problem. In previous works [60], they apply the policy optimizations that have known closed-form solutions as the lower-level within-task algorithms. As a result, the bilevel optimization problem is reduced to a single-level problem. In contrast, in this paper, as we consider a universal policy optimization, its closed-form solution cannot be obtained. To address the challenge, we compute $\\dot{\\nabla_{\\phi}}\\bar{\\mathcal{A}}l g(\\pi_{\\phi},\\lambda,\\tau)$ and the hypergradient by deriving the implicit differentiation on $\\mathcal{A}l g(\\pi_{\\phi},\\lambda,\\tau)$ . As shown in Section 4, the optimization problem $A l g(\\pi_{\\phi},\\lambda,\\tau)$ is unconstrained in (2), but is constrained in (1) due to $\\begin{array}{r}{\\sum_{a\\in\\mathcal{A}}\\pi(a\\bar{|}s)=1}\\end{array}$ . Therefore, we derive the implicit differentiation for both unconstrained and  constrained optimization problems. The following proposition shows the hypergradient computation for the tabular policy. Its proof is shown in Appendix J.1. ", "page_idx": 4}, {"type": "text", "text": "Proposition 1 (Hypergradient for the tabular policy). For the tabular policy in the discrete state-action space, consider any meta-parameter $\\phi$ and the within-task algorithm $(I)$ . Let $\\pi_{\\theta_{\\tau}^{\\prime}}~=~{\\o{A l g(\\pi_{\\phi},\\lambda,\\tau)}{A}}$ . If $M(s)~\\triangleq~\\lambda\\nabla_{\\pi(\\cdot|s)}^{2}d^{2}(\\pi_{\\phi}(\\cdot|s),\\pi(\\cdot|s))$ is non-singular for each $s\\ \\in$ $\\boldsymbol{S}$ , we have $\\begin{array}{r}{\\nabla_{\\phi}J_{\\tau}\\big(\\pi_{\\theta_{\\tau}^{\\prime}}\\big)\\;=\\;\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\pi_{\\theta_{\\tau}^{\\prime}}}}\\,\\Big[\\sum_{a\\in\\mathcal{A}}\\nabla_{\\phi}\\pi_{\\theta_{\\tau}^{\\prime}}\\big(a|s\\big)Q_{\\tau}^{\\pi_{\\theta_{\\tau}^{\\prime}}}\\big(s,a\\big)\\Big],}\\end{array}$ where $\\nabla_{\\phi}^{\\top}\\pi_{\\theta_{\\tau}^{\\prime}}(\\cdot|s)\\ =$ $\\begin{array}{r}{\\left(M(s)^{-1}-\\frac{M(s)^{-1}\\mathbf{1}\\,\\mathbf{1}^{\\top}M(s)^{-1}}{\\mathbf{1}^{\\top}M(s)^{-1}\\mathbf{1}}\\right)\\left(\\nabla_{\\phi}^{\\top}Q_{\\tau}^{\\pi_{\\phi}}(s,\\cdot)-\\lambda\\nabla_{\\phi}^{\\top}\\nabla_{\\pi(\\cdot|s)}d^{2}(\\pi_{\\phi}(\\cdot|s),\\pi(\\cdot|s))\\right)|_{\\pi=\\pi_{\\theta_{\\tau}^{\\prime}}}.}\\end{array}$ ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "The computation of $\\nabla_{\\phi}Q_{\\tau}^{\\pi_{\\phi}}(s,\\cdot)$ is shown in Appendix C. A sufficient condition of $M(s)$ being non-singular is that $d$ is locally strongly-convex at $\\pi=\\pi_{\\theta_{\\tau}^{\\prime}}$ , shown in Appendix J.1. Moreover, when $d=d_{1}$ or $d=d_{2}$ (correspondingly, $D_{\\tau}=D_{\\tau,1}$ or $D_{\\tau}=D_{\\tau,2}$ in (1)), the matrix $M(s)=$ $\\lambda\\nabla_{\\pi(\\cdot|s)}^{2}d^{2}(\\pi_{\\phi}(\\cdot|s),\\pi(\\cdot|s))$ is always non-singular for any $\\phi$ and $M(s)$ is always diagonal, and thus it is easy to compute $M^{-1}(s)$ . The hypergradient computation $\\nabla_{\\phi}J_{\\tau}(\\pi_{\\theta_{\\tau}^{\\prime}})$ for $D_{\\tau}=D_{\\tau,1}$ and $D_{\\tau,2}$ is shown in Appendix K.1 and L.1. ", "page_idx": 5}, {"type": "text", "text": "The following proposition shows the hypergradient computation for the policy with function approximation. Its proof is shown in Appendix J.2. ", "page_idx": 5}, {"type": "text", "text": "Proposition 2 (Hypergradient for the policy with function approximation). When a policy is represented by a function approximation, in both the discrete and continuous action spaces, for any meta-parameter $\\phi$ and the within-task algorithm in (2). Let $\\pi_{\\theta_{\\tau}^{\\prime}}\\ =\\ {\\cal A l g}(\\pi_{\\phi},\\bar{\\lambda},\\tau)$ . I $\\begin{array}{r l}&{f\\ \\nabla_{\\phi}J_{\\tau}(\\pi_{\\theta_{\\tau}^{\\prime}})\\ e x i s t s,\\ \\nabla_{\\phi}J_{\\tau}(\\pi_{\\theta_{\\tau}^{\\prime}})\\ =\\ \\frac{1}{1-\\gamma}\\nabla_{\\phi}\\theta_{\\tau}^{\\prime}\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\pi_{\\theta_{\\tau}^{\\prime}}},\\alpha\\sim\\pi_{\\theta_{\\tau}^{\\prime}}(\\cdot|s)}\\left[\\frac{\\nabla_{\\theta_{\\tau}^{\\prime}}\\pi_{\\theta_{\\tau}^{\\prime}}(a|s)}{\\pi_{\\theta_{\\tau}^{\\prime}}(a|s)}Q_{\\tau}^{\\pi_{\\theta_{\\tau}^{\\prime}}}(s,a)\\right],\\ a n d}\\\\ &{\\mathcal{T}_{\\phi}^{\\top}\\theta_{\\tau}^{\\prime}\\ =\\ -\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\pi_{\\phi}},a\\sim\\pi_{\\phi}(\\cdot|s)}[\\nabla_{\\theta}^{2}d^{2}(\\pi_{\\phi}(\\cdot|s),\\pi_{\\theta}(\\cdot|s))\\ -\\ \\frac{\\nabla_{\\theta}^{2}\\pi_{\\theta}(a|s)}{\\lambda\\pi_{\\phi}(a|s)}Q_{\\tau}^{\\pi_{\\phi}}(s,a)]^{-1}\\ \\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\pi_{\\phi}},a\\sim\\pi_{\\phi}(\\cdot|s)}}\\\\ &{\\nabla_{\\phi}^{\\top}\\nabla_{\\theta}d^{2}(\\pi_{\\phi}(\\cdot|s),\\pi_{\\theta}(\\cdot|s))\\ -\\ \\frac{\\nabla_{\\theta}\\pi_{\\theta}(a|s)}{\\lambda\\pi_{\\phi}(a|s)}\\nabla_{\\phi}^{\\top}Q_{\\tau}^{\\pi_{\\phi}}(s,a)]|_{\\theta=\\theta_{\\tau}^{\\prime}}.}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "A sufficient condition of $\\nabla_{\\phi}J_{\\tau}(\\pi_{\\theta_{\\tau}^{\\prime}})$ being existent is the objective function of (2) is locally strongly concave at $\\theta=\\theta_{\\tau}^{\\prime}$ , as proven in Appendix J.2. The computation of $\\nabla_{\\phi}Q_{\\tau}^{\\pi_{\\phi}}(s,\\cdot)$ is shown in Appendix C. Note that we need to compute the inverse of the Hessian when computing the hypergradient in Proposition 2. Similar to several widely used RL algorithms, such as TRPO [51] and CPO [1], we apply the conjugate gradient algorithm [23] to compute the inverse of the Hessian, which has demonstrated high efficiency across a wide range of applications of RL and meta-learning [51, 29, 15]. More clarifications about the computation efficiency of the Hessian inverse are shown in Appendix E. ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 Meta-Training for BO-MRL ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Require: Regularization weight $\\lambda>0$ ; Initial meta-parameter $\\phi_{0}$ ; learning rate $\\alpha$   \n1: for $t=0,\\cdots,T$ do   \n2: Sample a task $\\tau\\sim\\mathbb{P}(\\Gamma)$ with the MDP $\\mathcal{M}_{\\tau}$ i.i.d.   \n3: Evaluate $Q_{\\tau}^{\\pi_{\\phi_{t}}}(\\cdot,\\cdot)$ for current meta-policy $\\pi_{\\phi_{t}}$ by Monte-Carlo sampling   \n4: Adapt the task-specific policy $\\pi_{\\theta_{\\tau}^{\\prime}}$ from the meta-policy $\\pi_{\\phi_{t}}$ by solving $\\bar{\\pi_{\\theta_{\\tau}^{\\prime}}}=\\mathcal{A}l g(\\lambda,\\phi_{t},\\tau)$ defined in   \n(1) or (2).   \n5: Evaluate $Q_{\\tau}^{\\pi_{\\theta_{\\tau}^{\\prime}}}(\\cdot,\\cdot)$ for adapted policy $\\pi_{\\theta_{\\tau}^{\\prime}}$ Monte-Carlo sampling   \n6: Compute the hypergradient $\\nabla_{\\phi}J_{\\tau}(\\pi_{\\theta_{\\tau}^{\\prime}})$ in Proposition 1 or 2 by conjugate gradient method   \n7: Update meta-parameter $\\phi_{t+1}=\\phi_{t}+\\alpha\\nabla_{\\phi}J_{\\tau}(\\pi_{\\theta_{\\tau}^{\\prime}})$   \n8: end for ", "page_idx": 5}, {"type": "text", "text": "", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "9: Return $\\phi_{T}$ ", "page_idx": 5}, {"type": "text", "text": "With the hypergradient computations in Proposition 1 and Proposition 2, we apply the stochastic gradient ascent (SGD) to solve the optimization problem in (3). The meta-training of the bilevel optimization framework for meta-RL (BO-MRL) is formally stated in Algorithm 1. The state-action value function in lines 3 and 5 can be estimated by many approaches, including Monte-Carlo sampling used in MAML [15] and vine in [51]. We also propose a practical algorithm of Algorithm 1, as shown in Algorithm 2 in Appendix D, which includes more implementation details of the algorithm and several mechanisms to improve Algorithm 1. ", "page_idx": 5}, {"type": "text", "text": "5 Theoretical Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we quantify the performance of Algorithm 1, where the softmax policies and several distance metrics introduced in Section 3 are adopted. For convenience, we denote $\\boldsymbol{\\mathcal{A}l g^{(1)}}$ as $\\boldsymbol{A l g}$ in (1) and (2) when $D_{\\tau}=D_{\\tau,1}$ , and denote $\\boldsymbol{A l g^{(2)}}$ and $\\boldsymbol{A l g^{(3)}}$ in an analogous way. In Section 5.1, we introduce the softmax policy and necessary assumptions. In the following three sections, we consider two cases of Algorithm 1, including (i) Algorithm 1 with the within-task algorithm $\\boldsymbol{\\mathcal{A}l g^{(1)}}$ and $\\boldsymbol{A l g^{(2)}}$ for the tabular softmax policy; and (ii) Algorithm 1 with the within-task algorithm $\\boldsymbol{A l g^{(3)}}$ for the softmax policy with function approximation. For the algorithms in (i) and (ii), we study the existence of hypergradient in Section 5.2, derive the convergence guarantees in Section 5.3, and derive the near-optimality under the all-task optimum, i.e., derive the upper bounds of TEOG, in Section 5.4. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "5.1 Softmax policy and assumptions ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We apply the softmax policies, which are commonly applied in [66, 37, 60], and use the following assumptions on the task $\\tau$ . ", "page_idx": 6}, {"type": "text", "text": "Softmax policies. Consider the softmax policies $\\scriptstyle{\\hat{\\pi}}_{\\theta}$ parameterized by $\\theta$ for (i) the tabular policy and (ii) the policy with function approximation. In particular, the tabular policy in a discrete state-action space is defined by $\\hat{\\pi}_{\\boldsymbol{\\theta}}(\\cdot|s)\\propto\\exp(\\theta(s,\\cdot))$ , where $\\theta\\in\\mathbb{R}^{|S|\\times|A|}$ is a tabular map. The policy with function approximation is defined by $\\hat{\\pi}_{\\boldsymbol{\\theta}}(\\cdot|s)\\propto\\exp(f_{\\boldsymbol{\\theta}}(s,\\cdot))$ , where $f_{\\theta}$ is a function approximation model $S\\times A\\to\\mathbb{R}$ with the parameter $\\theta\\in\\mathbb{R}^{n}$ . ", "page_idx": 6}, {"type": "text", "text": "Assumption 1 (Upper bound of advantage function). For any task $\\tau\\in\\Gamma$ and any softmax policy $\\scriptstyle{\\hat{\\pi}}_{\\theta}$ , $|A_{\\tau}^{\\hat{\\pi}_{\\theta}}(s,a)|\\leq{\\dot{A}}_{m a x}^{\\;\\;\\;\\hat{\\pi}}$ for any $a\\in A$ and any $s\\in S$ . ", "page_idx": 6}, {"type": "text", "text": "Since the reward $r_{\\tau}\\leq r_{m a x}$ is bounded, it is easy to show that $\\begin{array}{r}{|A_{\\tau}^{\\hat{\\pi}_{\\theta}}(s,a)|\\leq\\frac{r_{m a x}}{1-\\gamma}}\\end{array}$ and Assumption 1 always holds. But we still keep Assumption 1 here, since there usually exist $A_{m a x}$ such that $\\begin{array}{r}{A_{m a x}\\ll\\frac{r_{m a x}}{1-\\gamma}}\\end{array}$ . We also have the following assumption and show its remark. ", "page_idx": 6}, {"type": "text", "text": "Assumption 2 (Sufficient state visit). For any task $\\tau\\in\\Gamma$ , there exists a constant $\\epsilon>0$ , such that for all bounded parameters $\\phi$ , $\\nu_{\\tau}^{\\hat{\\pi}_{\\phi}}(s)\\geq\\epsilon$ for all $s\\in S$ . ", "page_idx": 6}, {"type": "text", "text": "Remark 1. Here are two sufficient conditions for Assumption 2: (i) For any task $\\tau\\in\\Gamma$ , the MDP $\\boldsymbol{\\mathcal{M}}_{\\tau}$ is ergodic [43, 56]; or (ii) the initial state distribution $\\rho_{\\tau}$ has $\\rho_{\\tau}(s)>0$ for any $s\\in S$ . ", "page_idx": 6}, {"type": "text", "text": "The proof of Remark 1 is shown in Appendix O. Note that (i) of Remark 1 is a mild condition and is assumed in recent studies on RL algorithm analysis [61, 46]. ", "page_idx": 6}, {"type": "text", "text": "For the policy with function approximation, we require the following additional assumptions on the approximate function $f_{\\theta}$ , which are standard or weaker than those in the analysis of meta-learning and meta-RL problems [9, 12, 13, 14]. ", "page_idx": 6}, {"type": "text", "text": "Assumption 3 (Property of the approximate function). For any state-action pair $(s,a)\\in S\\times A$ , $(i)$ the approximate function $f_{\\theta}(\\bar{s},a)$ are cubic differentiable. (ii) $f_{\\theta}(s,a)$ is $L_{1}$ -Lipschitz, i.e., $\\lVert f_{\\theta_{1}}(s,a)-f_{\\theta_{2}}(s,a)\\rVert\\,\\le\\,L_{1}\\lVert\\theta_{1}-\\theta_{2}\\rVert$ for any $\\theta_{1},\\theta_{2}\\,\\in\\,\\mathbb{R}^{n}$ . (iii) $\\nabla_{\\theta}f_{\\theta}(s,a)$ is $L_{2}$ -Lipschitz, i.e., $\\|\\nabla_{\\theta}f_{\\theta_{1}}(s,a)-\\nabla_{\\theta}f_{\\theta_{2}}(s,a)\\|\\le L_{2}\\|\\theta_{1}-\\theta_{2}\\|$ for any $\\theta_{1},\\theta_{2}\\in\\mathbb{R}^{n}$ , (i $\\ensuremath{\\gamma})\\;\\ensuremath{\\nabla_{\\theta}}^{2}f_{\\theta}(s,a)$ is $L_{3}$ -Lipschitz, i.e., $\\left\\|\\nabla_{\\theta}^{2}f_{\\theta_{1}}(s,a)-\\nabla_{\\theta}^{2}f_{\\theta_{2}}(s,a)\\right\\|\\le L_{3}\\|\\theta_{1}-\\theta_{2}\\|$ for any $\\theta_{1},\\theta_{2}\\in\\mathbb{R}^{n}$ . ", "page_idx": 6}, {"type": "text", "text": "5.2 Existence of hypergradient. ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "An essential prerequisite for using Algorithm 1 is that the hypergradients in Propositions 1 and 2 exist. As shown in Section 4, for the tabular policy, when $\\textit{i}=\\textsubscript{1}$ or 2, the hypergradient $\\nabla_{\\phi}J_{\\tau}(A l g^{(i)}(\\hat{\\pi}_{\\phi},\\lambda,\\tau))$ exists for any $\\phi$ . For the policy with function approximation, we derive the following sufficient condition of the hypergradient being existent. Its proof is shown in Appendix M. ", "page_idx": 6}, {"type": "text", "text": "Proposition 3 (Existence of hypergradient for the policy with function approximation). In both discrete and continuous action space, consider the softmax policy with function approximation shown in Section 5.1. Suppose that Assumptions $^{\\,l}$ and 3 hold. If $\\lambda^{'}>\\ (6L_{1}^{2}\\bar{+}\\,2L_{2})A_{m a x}$ , $\\nabla_{\\phi}J_{\\tau}(A l g^{(3)}(\\hat{\\pi}_{\\phi},\\lambda,\\tau))$ always for any $\\phi$ . ", "page_idx": 6}, {"type": "text", "text": "5.3 Convergence guarantee ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We begin with the convergence guarantee of Algorithm 1 for the tabular policy. The following notations are used in the theorem: Bi, Ci, Gi, Ki, Mi (i = 1 and 2), where Ki \u225c 2(Bi(+12\u2212C\u03b3i2 ))4r2ma $\\begin{array}{r}{M_{i}\\triangleq\\frac{(B_{i}+2C_{i}^{2})G_{i}r_{m a x}}{(1-\\gamma)^{4}}}\\end{array}$ for $i=1$ and 2. $\\begin{array}{r}{B_{1}\\triangleq\\frac{16r_{m a x}}{\\lambda(1-\\gamma)^{3}}+\\frac{24}{1-\\gamma}+\\frac{12}{\\lambda},C_{1}\\triangleq\\frac{6}{1-\\gamma},}\\end{array}$ , and $\\begin{array}{r}{G_{1}\\triangleq\\frac{4A_{m a x}}{(1-\\gamma)^{2}}}\\end{array}$ $\\begin{array}{r}{B_{2}\\triangleq\\frac{16r_{m a x}}{\\lambda(1-\\gamma)^{3}}+\\frac{18}{(1-\\gamma)^{2}}}\\end{array}$ +(1\u221218\u03b3)2 , C2 \u225c1 , and $\\begin{array}{r}{G_{2}\\triangleq\\frac{2A_{m a x}}{(1-\\gamma)^{2}}}\\end{array}$ . ", "page_idx": 6}, {"type": "text", "text": "Theorem 1 (Convergence guarantee for tabular softmax policy). Consider the tabular softmax policy in the discrete action space. Suppose that Assumptions $^{\\,l}$ and 2 hold. Let $\\{\\phi_{t}\\}_{t=1}^{T}$ be the sequence of meta-parameters generated by Algorithm $^{\\,l}$ with $\\lambda~~\\ge$ $2A_{m a x}$ and the step size $\\begin{array}{r l r}{\\alpha}&{=}&{\\operatorname*{min}\\left\\{\\left(\\frac{r_{m a x}B_{i}}{(1-\\gamma)^{2}}+\\frac{2\\gamma r_{m a x}C_{i}^{2}}{(1-\\gamma)^{3}}\\right)^{-1}\\!,\\frac{1}{G_{i}\\sqrt{T}}\\right\\}}\\end{array}$ . Then, the bound: $\\begin{array}{r}{\\frac{1}{T}\\sum_{t=1}^{T}\\mathbb{E}[\\|\\nabla_{\\phi}\\mathbb{E}_{\\tau\\sim\\mathbb{P}(\\Gamma)}[J_{\\tau}(A l g^{(i)}(\\hat{\\pi}_{\\phi_{t}},\\dot{\\lambda_{,}}\\tau))]\\|^{2}]\\leq\\frac{K_{i}}{T}+\\frac{M_{i}}{\\sqrt{T}}.}\\end{array}$ holds for $i=1$ or 2. ", "page_idx": 7}, {"type": "text", "text": "The first expectation comes from the random sampling in line 2 of Algorithm 1. The proofs of Theorem 1 are shown in Appendices K.2 and L.2. ", "page_idx": 7}, {"type": "text", "text": "The following theorem shows the convergence guarantee for the policy with function approximation. The notations are used in the theorem: B3, C3, G3, K3, M3, where K3 \u225c 2(B3(+12\u2212C\u03b332))4r2ma M3 \u225c (B3+2(1C\u221232)\u03b3)G43rmax, $\\begin{array}{r}{G_{3}\\ \\triangleq\\ \\frac{L_{1}A_{m a x}(\\lambda+\\frac{2\\gamma}{1-\\gamma}L_{1}^{2}A_{m a x})}{(1-\\gamma)(\\lambda-(6L_{1}^{2}+2L_{2})A_{m a x})}}\\end{array}$ (1\u2212\u03b3)(\u03bb\u2212(6L211\u2212+\u03b32 L21)Amax), C3 \u225c $\\begin{array}{r}{C_{3}\\ \\triangleq\\ \\frac{2L_{1}(\\lambda+\\frac{2\\gamma}{1-\\gamma}L_{1}^{2}A_{m a x})}{(1-\\gamma)(\\lambda-(6L_{1}^{2}+2L_{2})A_{m a x})}}\\end{array}$ (1\u2212\u03b3)(\u03bb\u2212(6L21+2L2)Amax), and B \u225c(160L31+56L1L2+4L3)(\u03bb+ 12\u2212\u03b3\u03b3 L21Amax)2. ", "page_idx": 7}, {"type": "text", "text": "Theorem 2 (Convergence guarantee for softmax policy with function approximation). In both discrete and continuous action space, consider the softmax policy with function approximation. Suppose that Assumptions 1, 2, and 3 hold. Let $\\{\\phi_{t}\\}_{t=1}^{T}$ be the sequence of meta-parameters generated by Algorithm 1 with $\\lambda>(6L_{1}^{2}{+}2L_{2})A_{m a x}$ and the step size $\\begin{array}{r}{\\alpha=\\operatorname*{min}\\bigg\\{\\bigg(\\frac{r_{m a x}B_{3}}{(1-\\gamma)^{2}}+\\frac{2\\gamma r_{m a x}C_{3}^{2}}{(1-\\gamma)^{3}}\\bigg)^{-1},\\frac{1}{G_{3}\\sqrt{T}}\\bigg\\}}\\end{array}$ Then, the bound $\\begin{array}{r}{\\frac{1}{T}\\sum_{t=1}^{T}\\mathbb{E}\\left[\\|\\nabla_{\\phi}\\mathbb{E}_{\\tau\\sim\\mathbb{P}(\\Gamma)}[J_{\\tau}(A l g^{(3)}(\\hat{\\pi}_{\\phi_{t}},\\lambda,\\tau))]\\|^{2}\\right]\\leq\\frac{K_{3}}{T}+\\frac{M_{3}}{\\sqrt{T}}.}\\end{array}$ . holds. ", "page_idx": 7}, {"type": "text", "text": "The first expectation arises from the random sampling in line 2 of Algorithm 1. The proof of Theorem 2 is shown in Appendix M. Theorems 1 and 2 show that the convergence rate of Algorithm 1 is $\\scriptstyle{\\mathcal{O}}\\left({\\frac{1}{\\sqrt{T}}}\\right)$ and the constants in the notation $\\scriptscriptstyle\\mathcal{O}$ are only related to the discount factor $\\gamma$ , the reward bound $r_{m a x}$ , the bound of the advantage function $A_{m a x}$ , and the Lipschitz constants of $f_{\\theta}$ . ", "page_idx": 7}, {"type": "text", "text": "5.4 Near-optimality under all-task optimum ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Before the derivation of the optimality analysis, we first introduce two intermediate Lemmas. ", "page_idx": 7}, {"type": "text", "text": "Lemma 1. Suppose that Assumptions $^{\\,l}$ , 2 hold. For any task $\\tau$ , any bounded parameters $\\theta$ and $\\theta^{\\prime}$ , and i = 1 or 2, we have J\u03c4(\u03c0\u02c6\u03b8\u2032) \u2212J\u03c4(\u03c0\u02c6\u03b8) \u2265Es\u223c\u03bd \u03c4\u03c0\u02c6 ,a\u223c\u03c0\u02c6\u2032(\u00b7|s) $\\begin{array}{r}{J_{\\tau}(\\hat{\\pi}_{\\theta^{\\prime}})-J_{\\tau}(\\hat{\\pi}_{\\theta})\\geq\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\hat{\\pi}},a\\sim\\hat{\\pi}^{\\prime}(\\cdot|s)}\\left[\\frac{A_{\\tau}^{\\hat{\\pi}_{\\theta}}(s,a)}{1-\\gamma}\\right]-\\frac{2\\gamma A_{m a x}}{(1-\\gamma)^{2}\\epsilon}D_{\\tau,i}^{2}(\\hat{\\pi}_{\\theta},\\hat{\\pi}_{\\theta^{\\prime}}).}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "Lemma 2. Consider the softmax policy with function approximation shown in Section 5.1. Suppose that Assumptions 1, 2, and $^3$ hold. For any task $\\tau$ , and any softmax policies parameterized by bounded $\\theta$ and , we have $\\begin{array}{r}{J_{\\tau}(\\hat{\\pi}_{\\theta^{\\prime}})-J_{\\tau}(\\hat{\\pi}_{\\theta})\\ge\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\hat{\\pi}_{\\theta}},a\\sim\\hat{\\pi}_{\\theta^{\\prime}}(\\cdot|s)}\\left[\\frac{A_{\\tau}^{\\hat{\\pi}_{\\theta}}(s,a)}{1-\\gamma}\\right]-\\frac{4\\gamma A_{m a x}L_{1}^{2}}{(1-\\gamma)^{2}\\epsilon}D_{\\tau.3}^{2}(\\hat{\\pi}_{\\theta},\\hat{\\pi}_{\\theta^{\\prime}}).}\\end{array}$ . The proofs of Lemmas 1 and 2 are shown in Appendix N.1. Given Lemma 1, when $\\begin{array}{r}{\\lambda=\\frac{2\\gamma A_{m a x}}{(1-\\gamma)\\epsilon}}\\end{array}$ , the within-task algorithm $\\mathcal{A}l g^{(1,2)}(\\hat{\\pi},\\lambda,\\tau)$ in (1) is actually designed to maximize the right-hand side of the inequality, where $\\hat{\\pi}^{\\prime}$ is the decision variable. Similarly, Given Lemma 2, when $\\begin{array}{r}{\\lambda=\\frac{4\\gamma A_{m a x}L_{1}^{2}}{(1-\\gamma)\\epsilon}}\\end{array}$ , $\\mathcal{A}l g^{(3)}(\\hat{\\pi}_{\\boldsymbol{\\theta}},\\lambda,\\tau)$ in (2) maximizes the right-hand side of the inequality, where $\\hat{\\pi}_{\\boldsymbol{\\theta}^{\\prime}}$ is the decision variable. In other words, for each $i=1,2$ , and 3, the within-task algorithm $\\boldsymbol{A l g}^{(i)}$ is to maximize a lower bound of $J_{\\tau}(\\hat{\\pi}_{\\boldsymbol{\\theta}})$ , denoted as $\\bar{J}_{\\tau}(\\hat{\\pi}_{\\boldsymbol{\\theta}})$ . This idea, referred to as the minorization-maximization (MM) [28], is widely used in [51, 33]. The design of $\\boldsymbol{A l g}^{(i)}$ enables us to connect the accumulated reward of the policy after the policy adaptation with that of the optimal policy $\\hat{\\pi}_{\\boldsymbol{\\theta}_{\\tau}^{*}}$ for task $\\tau$ , i.e., $\\bar{J}_{\\tau}(\\mathcal{A}l g^{(i)}(\\hat{\\pi}_{\\phi},\\lambda,\\tau))\\ge\\bar{J}_{\\tau}(\\hat{\\pi}_{\\theta_{\\tau}^{*}})$ , which is a key intermediate result for the optimality analysis. ", "page_idx": 7}, {"type": "text", "text": "The final preparatory step is that we borrow the analysis of the meta-training error from [60]. In particular, its theoretical result is encapsulated in the following assumption. ", "page_idx": 7}, {"type": "text", "text": "Assumption 4. (Bounding error of meta-objective using gradient) Let $\\begin{array}{r l r}{F^{(i)}(\\phi)}&{{}\\triangleq}&{}\\end{array}$ $\\mathbb{E}_{\\tau\\sim\\mathbb{P}(\\Gamma)}[J_{\\tau}(\\mathcal{A}l g^{(i)}(\\hat{\\pi}_{\\phi},\\lambda,\\tau))]$ . For both the tabular policy and the policy with functional approximation, there exists a concave positive non-decreasing function $h_{i}:[0,+\\infty)\\to[0,+\\infty)$ , such that $\\begin{array}{r}{\\operatorname*{max}_{\\phi^{\\prime}}F^{(i)}(\\phi^{\\prime})-F^{(i)}(\\phi)\\leq h_{i}(\\|\\nabla_{\\phi}F^{(i)}(\\phi)\\|^{2})}\\end{array}$ . ", "page_idx": 7}, {"type": "text", "text": "Assumption 4 assumes the optimality gap of $\\hat{\\pi}_{\\phi}$ on the meta-objective is upper bounded by an increasing function of its gradient. A sufficient condition of Assumption 4 is provided by [60]. Combine the Assumption 4 and the convergence analysis in Theorems 1 and 2, we can bound the error of the meta-objective, i.e., $\\mathrm{max}_{\\phi}\\,F^{(i)}(\\phi)-F^{(i)}(\\phi_{t})$ . This result is referred to as the optimality of the meta-objective shown in Table 1. Finally, we derive the upper bounds of the TEOG for both the tabular policy and the policy with function approximation. ", "page_idx": 8}, {"type": "text", "text": "Theorem 3 (Optimality guarantee for softmax tabular policy). Consider the tabular softmax policy for the discrete state-action space. Suppose that Assumptions 1,2 and $^{4}$ hold. Let $\\{\\phi_{t}\\}_{t=1}^{T}$ be the sequence of meta-parameters generated by Algorithm $^{\\,l}$ with $\\begin{array}{r l r}{\\lambda}&{{}=}&{\\frac{2A_{m a x}}{(1-\\gamma)\\epsilon}}\\end{array}$ and the step size $\\alpha$ shown in Theorem 1. Then, the following holds for $\\textit{i}=\\textit{1}$ or 2: $\\begin{array}{r l r}{\\frac{1}{T}\\sum_{t=1}^{T}\\mathbb{E}_{t}\\left[\\mathbb{E}_{\\tau\\sim\\mathbb{P}(\\Gamma)}[J_{\\tau}(\\hat{\\pi}_{\\theta_{\\tau}^{*}})-J_{\\tau}(\\mathcal{A}l g^{(i)}(\\hat{\\pi}_{\\phi_{t}},\\lambda,\\tau))]\\right]}&{\\le}&{h_{i}\\left(\\frac{K_{i}}{T}+\\frac{M_{i}}{\\sqrt{T}}\\right)\\;+}\\end{array}$ 2(1(1+\u2212\u03b3)\u03b3A)2m\u03f5axVari(P(\u0393)), where \u03c0\u02c6\u03b8\u03c4\u2217 is the optimal softmax policy for task \u03c4 and the constants Ki and $M_{i}$ are shown in Theorem $^{\\,I}$ . ", "page_idx": 8}, {"type": "text", "text": "Theorem 4 (Optimality guarantee for softmax policy with function approximation). In both discrete and continuous action space, consider the softmax policy with function approximation. Suppose that Assumptions 1,2, 3 and $^{4}$ hold. Let $\\{\\phi_{t}\\}_{t=1}^{T}$ be the sequence of meta-parameters generated by Algorithm $^{\\,l}$ with (6L21(+12\u2212L\u03b32))\u03f5Amax and the step size \u03b1 shown in Theorem 2. The following holds: $\\begin{array}{r l}&{\\frac{1}{T}\\sum_{t=1}^{T}\\mathbb{E}_{t}\\left[\\mathbb{E}_{\\tau\\sim\\mathbb{P}(\\Gamma)}[J_{\\tau}(\\hat{\\pi}_{\\theta_{\\tau}^{*}})-J_{\\tau}(A l g^{(3)}(\\hat{\\pi}_{\\phi_{t}},\\lambda,\\tau))]\\right]\\leq h_{3}\\left(\\frac{K_{3}}{T}+\\frac{M_{3}}{\\sqrt{T}}\\right)+\\frac{1}{2\\eta}\\int_{\\mathbb{P}(\\Gamma)}\\hat{\\phi}_{0}(\\theta_{\\tau}^{*},\\lambda,\\tau)d\\theta_{\\tau}.}\\end{array}$ ((6+4\u03b3)(L121\u2212+\u03b32)2L\u03f52)AmaxVar3(P(\u0393)), where \u03c0\u02c6\u03b8\u03c4\u2217 is the optimal softmax policy for task \u03c4 and the constants $K_{3}$ and $M_{3}$ are the same as Theorem 2. ", "page_idx": 8}, {"type": "text", "text": "The proofs of Theorems 3 and 4, as well as the selection of the hyperparameter $\\lambda$ in these two theorems, are shown in Appendix N.2. The theorems derive the upper bounds of the TEOGs between the parameter adapted by one-time policy adaptation from the produced meta-parameter $\\phi_{t}$ and the task-specific optimal parameter $\\boldsymbol{\\theta}_{\\u{\\tau}}^{*}$ . It is shown that, with at most $T$ iterations, we can achieve the upper bounds in the order of $\\begin{array}{r}{\\mathcal{O}(\\dot{h}_{i}(\\frac{1}{\\sqrt{T}})+\\mathcal{V}a r(\\mathbb{P}(\\Gamma)))}\\end{array}$ . In other words, there exists a $t\\leq T$ with $\\begin{array}{r}{\\mathbb{E}_{\\tau\\sim\\mathbb{P}(\\Gamma)}[J_{\\tau}(A l g(\\hat{\\pi}_{\\phi_{t}},\\lambda,\\tau))]\\ge\\mathbb{E}_{\\tau\\sim\\mathbb{P}(\\Gamma)}[J_{\\tau}(\\hat{\\pi}_{\\theta_{\\tau}^{*}})]-\\mathcal{O}(h_{i}(\\frac{1}{\\sqrt{T}})+\\mathcal{V}a r(\\mathbb{P}(\\Gamma)))}\\end{array}$ . As the number of iterations $T$ increases, or the variance of the task distribution $\\mathcal{V}a r(\\mathbb{P}(\\Gamma))$ reduces, the optimality of the meat-parameter $\\phi_{t}$ improves. The second term $\\mathcal{V}a r(\\mathbb{P}(\\Gamma))$ in the upper bounds of Theorems 3 and 4 corresponds the intuition of meta-learning, which is that, if the variance of a task distribution is smaller, the meta-policy learned from the task distribution is more helpful for new tasks in the task distribution, then the performance is better. Moreover, this term shows that the learned meta-policy achieves a better performance than the meta-policy $\\phi^{c e n t e r}$ defined by $\\mathrm{arg\\,min}_{\\phi}\\,\\mathbb{E}_{\\tau\\sim\\mathbb{P}(\\Gamma)}[D_{\\tau,i}^{2}(\\pi_{\\phi}^{\\cdot},\\pi_{\\theta_{\\tau}^{*}})]$ , which is the center of all the task-specific optimal policies $\\pi_{\\theta_{\\tau}^{*}}$ . The order of our upper bounds are comparable to $\\mathcal{O}(T^{-\\frac{1}{4}}+\\mathcal{V}a r(\\mathbb{P}(\\Gamma))$ that is shown in [31]. On the other hand, compared with [31], in this paper, the constants in the notation $\\scriptscriptstyle\\mathcal{O}$ only consist of $\\gamma$ , $r_{m a x}$ , $A_{m a x}$ , and the Lipschitz constants of $f$ , and do not rely on $|{\\mathcal{A}}|$ and $|{\\mathcal{S}}|$ . As a result, our upper bounds are tighter when handling high-dimensional problems or continuous spaces. ", "page_idx": 8}, {"type": "text", "text": "Monotonic improvement of the within-task algorithm. Another benefit from Lemmas 1 and 2 and the idea of MM used by the within-task algorithm is that, the policy update by the within-task algorithm monotonically improves, i.e., $J_{\\tau}(\\mathcal{A}\\bar{y}^{(i)}(\\hat{\\pi}_{\\boldsymbol{\\theta}},\\lambda,\\tau))\\geq J_{\\tau}\\bar{(}\\hat{\\pi}_{\\boldsymbol{\\theta}})$ for $i=1,2$ and 3 and any $\\theta$ and any task $\\tau$ . Therefore, multiple times of $\\boldsymbol{A l g}$ always perform better than one-time $\\boldsymbol{A l g}$ . ", "page_idx": 8}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "6.1 Verification of theoretical results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We conduct an experiment to verify the optimality bounds of Algorithm 1 shown in Theorems 3 and 4. We consider two scenarios of the Frozen Lake environment in Gym: two task distributions with a high task variance and a low task variance. More details of the setting and the hyperparameter selection are shown in Appendix A. We consider the within-task algorithm $\\boldsymbol{A l g}^{(i)}$ for all $i=1,2$ and 3, where the results of $i=2$ and 3 are shown in Appendix A. ", "page_idx": 8}, {"type": "image", "img_path": "rpjh69DUX2/tmp/69eadd0a8cc5660504daa85e33d6c69b3d6b2b222fd7b55bbca077482c9b1733.jpg", "img_caption": ["Figure 1: Results of the meta-test on Frozen Lake, where $\\boldsymbol{\\mathcal{A}l g}^{(1)}$ is applied. Left: Average accumulated reward across all test tasks v.s. number of policy adaptation steps; Right: Comparing the expected optimality gap by the BO-MRL and baselines with the upper bound of the accumulated reward of one-time $\\boldsymbol{\\mathcal{A}l g^{(\\bar{1})}}$ . "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "rpjh69DUX2/tmp/b22fd6727e2fd7ed65669343eace20aded6baf81f6609575de2aa55ee692ce78.jpg", "img_caption": ["Figure 2: Average accumulated reward across all test tasks during the meta-test under the practical algorithm of BO-MRL on the locomotion tasks. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "We compare our algorithm with MAML [15] and the random initialization. Figure 1 shows that, for Algorithm 1 with the within-task algorithm $\\boldsymbol{\\mathcal{A}l g^{(1)}}$ , it outperforms the baseline methods. For all scenarios, the expected optimality gap of the one-time policy adaptation is smaller than the upper bounds shown in Theorems 3 and 4, which verify our theoretical analysis. Moreover, in Figure 1, the expected optimality gap of the policy adaptation is better (smaller) but close to the upper bound, while that of the other policy adaptation approach, the policy gradient, is worse (larger) than the upper bound. It shows that the derived upper bound is tight. ", "page_idx": 9}, {"type": "text", "text": "6.2 High-dimensional Experiment ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "To evaluate the proposed practical algorithm, Algorithm 2 in Appendix D, we conduct experiments on high-dimensional locomotion settings in the MuJoCo simulator, including Half-Cheetah with goal directions and goal velocities, Ant with goal directions and goal velocities. We compare the proposed algorithm with several optimization-based meta-RL algorithms, including MAML, E-MAML [55], and ProMP [50]. For the fairness of the comparison, all the methods share the same data requirement and task setting. More details of the task setting, the hyperparameter selection, and the supplemental results are shown in Appendix B. ", "page_idx": 9}, {"type": "text", "text": "Figure 2 shows that the proposed algorithm with the within-task algorithms $\\boldsymbol{A l g}^{(i)}$ outperforms the baseline methods in all four experimental settings. For example, we achieve about $25\\%$ of performance improvement in Half-cheetah direction and Ant direction experiments. Moreover, compared with the baseline methods, the proposed algorithm achieves more policy improvement when more policy optimization steps are given. For example, our approach achieves about $10\\%$ of performance improvement in the second policy optimization step, while those of baseline methods are almost $0\\%$ . ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper develops a bilevel optimization framework for meta-RL, which implements multiple-step policy optimization on one-time data collection during task-specific policy adaptation. Beyond existing meta-RL analyses, we provide upper bounds of the expected optimality gap over the task distribution. Our experiments validate the bounds derived from our theoretical analysis and show the superior effectiveness of the proposed framework. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work is partially supported by the National Science Foundation through grants ECCS 1846706 and ECCS 2140175. We would like to thank the reviewers for their constructive and insightful suggestions. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In International conference on machine learning, pages 22\u201331. PMLR, 2017.   \n[2] Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy gradient methods: Optimality, approximation, and distribution shift. The Journal of Machine Learning Research, 22(1):4431\u20134506, 2021.   \n[3] Karol Arndt, Murtaza Hazara, Ali Ghadirzadeh, and Ville Kyrki. Meta reinforcement learning for sim-to-real domain adaptation. In 2020 IEEE International Conference on Robotics and Automation, pages 2725\u20132731. IEEE, 2020.   \n[4] Maria-Florina Balcan, Mikhail Khodak, and Ameet Talwalkar. Provable guarantees for gradientbased meta-learning. In International Conference on Machine Learning, pages 424\u2013433. PMLR, 2019.   \n[5] Jacob Beck, Risto Vuorio, Evan Zheran Liu, Zheng Xiong, Luisa Zintgraf, Chelsea Finn, and Shimon Whiteson. A survey of meta-reinforcement learning. arXiv preprint arXiv:2301.08028, 2023.   \n[6] Suneel Belkhale, Rachel Li, Gregory Kahn, Rowan McAllister, Roberto Calandra, and Sergey Levine. Model-based meta-reinforcement learning for flight with suspended payloads. IEEE Robotics and Automation Letters, 6(2):1471\u20131478, 2021.   \n[7] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.   \n[8] Imre Csisz\u00e1r and J\u00e1nos K\u00f6rner. Information theory: coding theorems for discrete memoryless systems. Cambridge University Press, 2011.   \n[9] Giulia Denevi, Carlo Ciliberto, Riccardo Grazzi, and Massimiliano Pontil. Learning-to-learn stochastic gradient descent with biased regularization. In International Conference on Machine Learning, pages 1566\u20131575. PMLR, 2019.   \n[10] Giulia Denevi, Dimitris Stamos, Carlo Ciliberto, and Massimiliano Pontil. Online-within-online meta-learning. Advances in Neural Information Processing Systems, 32, 2019.   \n[11] Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. RL2: Fast reinforcement learning via slow reinforcement learning. In International Conference on Learning Representations, 2017.   \n[12] Alireza Fallah, Kristian Georgiev, Aryan Mokhtari, and Asuman Ozdaglar. On the convergence theory of debiased model-agnostic meta-reinforcement learning. Advances in Neural Information Processing Systems, 34:3096\u20133107, 2021.   \n[13] Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. On the convergence theory of gradientbased model-agnostic meta-learning algorithms. In International Conference on Artificial Intelligence and Statistics, pages 1082\u20131092. PMLR, 2020.   \n[14] Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. Generalization of model-agnostic metalearning algorithms: Recurring and unseen tasks. Advances in Neural Information Processing Systems, 34:5469\u20135480, 2021.   \n[15] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International conference on machine learning, pages 1126\u20131135. PMLR, 2017.   \n[16] Chelsea Finn and Sergey Levine. Meta-learning and universality: Deep representations and gradient descent can approximate any learning algorithm. In International Conference on Learning Representations, 2018.   \n[17] Luca Franceschi, Michele Donini, Paolo Frasconi, and Massimiliano Pontil. Forward and reverse gradient-based hyperparameter optimization. In International Conference on Machine Learning, pages 1165\u20131173. PMLR, 2017.   \n[18] Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimiliano Pontil. Bilevel programming for hyperparameter optimization and meta-learning. In International Conference on Machine Learning, pages 1568\u20131577. PMLR, 2018.   \n[19] Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341\u20132368, 2013.   \n[20] Saeed Ghadimi and Mengdi Wang. Approximation methods for bilevel programming. arXiv preprint arXiv:1802.02246, 2018.   \n[21] Stephen Gould, Basura Fernando, Anoop Cherian, Peter Anderson, Rodrigo Santa Cruz, and Edison Guo. On differentiating parameterized argmin and argmax problems with application to bi-level optimization. arXiv preprint arXiv:1607.05447, 2016.   \n[22] Riccardo Grazzi, Luca Franceschi, Massimiliano Pontil, and Saverio Salzo. On the iteration complexity of hypergradient computation. In International Conference on Machine Learning, pages 3748\u20133758. PMLR, 2020.   \n[23] Magnus R Hestenes and Eduard Stiefel. Methods of conjugate gradients for solving. Journal of research of the National Bureau of Standards, 49(6):409, 1952.   \n[24] Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang. A two-timescale stochastic algorithm framework for bilevel optimization: Complexity analysis and application to actorcritic. SIAM Journal on Optimization, 33(1):147\u2013180, 2023.   \n[25] Timothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey. Meta-learning in neural networks: A survey. IEEE transactions on pattern analysis and machine intelligence, 44(9):5149\u20135169, 2021.   \n[26] Yu Huang, Yingbin Liang, and Longbo Huang. Provable generalization of overparameterized meta-learning trained with SGD. Advances in Neural Information Processing Systems, 35:16563\u2013 16576, 2022.   \n[27] Mike Huisman, Jan N Van Rijn, and Aske Plaat. A survey of deep meta-learning. Artificial Intelligence Review, 54(6):4483\u20134541, 2021.   \n[28] David R Hunter and Kenneth Lange. A tutorial on mm algorithms. The American Statistician, 58(1):30\u201337, 2004.   \n[29] Kaiyi Ji, Junjie Yang, and Yingbin Liang. Bilevel optimization: Convergence analysis and enhanced design. In International Conference on Machine Learning, pages 4882\u20134892. PMLR, 2021.   \n[30] Sham M Kakade. A natural policy gradient. Advances in neural information processing systems, 14, 2001.   \n[31] Vanshaj Khattar, Yuhao Ding, Javad Lavaei, and Ming Jin. A CMDP-within-online framework for meta-safe reinforcement learning. In International Conference on Learning Representations, 2023.   \n[32] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[33] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.   \n[34] Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in Neural Information Processing Systems, pages 1008\u20131014, 2000.   \n[35] Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto. Meta-learning with differentiable convex optimization. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10649\u201310657, 2019.   \n[36] Thomas Lew, Apoorva Sharma, James Harrison, Andrew Bylard, and Marco Pavone. Safe active dynamics learning and control: A sequential exploration\u2013exploitation framework. IEEE Transactions on Robotics, 2022.   \n[37] Boyi Liu, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural trust region/proximal policy optimization attains globally optimal policy. Advances in neural information processing systems, 32, 2019.   \n[38] Hao Liu, Richard Socher, and Caiming Xiong. Taming MAML: Efficient unbiased metareinforcement learning. In International conference on machine learning, pages 4061\u20134071. PMLR, 2019.   \n[39] Shicheng Liu and Minghui Zhu. Distributed inverse constrained reinforcement learning for multi-agent systems. Advances in Neural Information Processing Systems, 35:33444\u201333456, 2022.   \n[40] Shicheng Liu and Minghui Zhu. Learning multi-agent behaviors from distributed and streaming demonstrations. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[41] Shicheng Liu and Minghui Zhu. Meta inverse constrained reinforcement learning: Convergence guarantee and generalization analysis. In The Twelfth International Conference on Learning Representations, 2023.   \n[42] Russell Mendonca, Abhishek Gupta, Rosen Kralev, Pieter Abbeel, Sergey Levine, and Chelsea Finn. Guided meta-policy search. Advances in Neural Information Processing Systems, 32, 2019.   \n[43] Teodor Mihai Moldovan and Pieter Abbeel. Safe exploration in markov decision processes. In Proceedings of the 29th International Coference on International Conference on Machine Learning, pages 1451\u20131458, 2012.   \n[44] Anusha Nagabandi, Ignasi Clavera, Simin Liu, Ronald S Fearing, Pieter Abbeel, Sergey Levine, and Chelsea Finn. Learning to adapt in dynamic, real-world environments through metareinforcement learning. In International Conference on Learning Representations, 2018.   \n[45] Fabian Pedregosa. Hyperparameter optimization with approximate gradient. In International Conference on Machine Learning, pages 737\u2013746. PMLR, 2016.   \n[46] Shuang Qiu, Zhuoran Yang, Jieping Ye, and Zhaoran Wang. On finite-time convergence of actor-critic algorithm. IEEE Journal on Selected Areas in Information Theory, 2(2):652\u2013664, 2021.   \n[47] Roberta Raileanu, Max Goldstein, Arthur Szlam, and Rob Fergus. Fast adaptation to new environments via policy-dynamics value functions. In Proceedings of International Conference on Machine Learning, pages 7920\u20137931, 2020.   \n[48] Aravind Rajeswaran, Chelsea Finn, Sham M Kakade, and Sergey Levine. Meta-learning with implicit gradients. Advances in neural information processing systems, 32, 2019.   \n[49] Kate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, and Deirdre Quillen. Efficient off-policy meta-reinforcement learning via probabilistic context variables. In International Conference on Machine Learning, pages 5331\u20135340. PMLR, 2019.   \n[50] Jonas Rothfuss, Dennis Lee, Ignasi Clavera, Tamim Asfour, and Pieter Abbeel. Promp: Proximal meta-policy search. In International Conference on Learning Representations, 2019.   \n[51] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International conference on machine learning, pages 1889\u20131897. PMLR, 2015.   \n[52] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.   \n[53] Amirreza Shaban, Ching-An Cheng, Nathan Hatch, and Byron Boots. Truncated backpropagation for bilevel optimization. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 1723\u20131732. PMLR, 2019.   \n[54] Xingyou Song, Yuxiang Yang, Krzysztof Choromanski, Ken Caluwaerts, Wenbo Gao, Chelsea Finn, and Jie Tan. Rapidly adaptable legged robots via evolutionary meta-learning. In 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 3769\u20133776. IEEE, 2020.   \n[55] Bradly C Stadie, Ge Yang, Rein Houthooft, Xi Chen, Yan Duan, Yuhuai Wu, Pieter Abbeel, and Ilya Sutskever. Some considerations on learning to explore via meta-reinforcement learning. arXiv preprint arXiv:1803.01118, 2018.   \n[56] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.   \n[57] Yunhao Tang. Biased gradient estimate with drastic variance reduction for meta reinforcement learning. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 21050\u201321075. PMLR, 17\u201323 Jul 2022.   \n[58] Sebastian Thrun and Lorien Pratt. Learning to learn. Springer Science & Business Media, 2012.   \n[59] Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn. arXiv preprint arXiv:1611.05763, 2016.   \n[60] Lingxiao Wang, Qi Cai, Zhuoran Yang, and Zhaoran Wang. On the global optimality of modelagnostic meta-learning. In International conference on machine learning, pages 9837\u20139846. PMLR, 2020.   \n[61] Yue Frank Wu, Weitong Zhang, Pan Xu, and Quanquan Gu. A finite-time analysis of two time-scale actor-critic methods. Advances in Neural Information Processing Systems, 33:17617\u2013 17628, 2020.   \n[62] Zheng Xiong, Luisa M Zintgraf, Jacob Austin Beck, Risto Vuorio, and Shimon Whiteson. On the practical consistency of meta-reinforcement learning algorithms. In Fifth Workshop on Meta-Learning at the Conference on Neural Information Processing Systems, 2021.   \n[63] Siyuan Xu and Minghui Zhu. Meta value learning for fast policy-centric optimal motion planning. Robotics Science and Systems, 2022.   \n[64] Siyuan Xu and Minghui Zhu. Efficient gradient approximation method for constrained bilevel optimization. Proceedings of the AAAI Conference on Artificial Intelligence, 37(10):12509\u2013 12517, 2023.   \n[65] Siyuan Xu and Minghui Zhu. Online constrained meta-learning: Provable guarantees for generalization. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[66] Tengyu Xu, Yingbin Liang, and Guanghui Lan. Crpo: A new approach for safe reinforcement learning with convergence guarantee. In International Conference on Machine Learning, pages 11480\u201311491. PMLR, 2021.   \n[67] L Zintgraf, K Shiarlis, M Igl, S Schulze, Y Gal, K Hofmann, and S Whiteson. Varibad: a very good method for bayes-adaptive deep rl via meta-learning. Proceedings of ICLR 2020, 2020.   \n[68] Luisa Zintgraf, Kyriacos Shiarlis, Maximilian Igl, Sebastian Schulze, Yarin Gal, Katja Hofmann, and Shimon Whiteson. Varibad: A very good method for bayes-adaptive deep rl via metalearning. In International Conference on Learning Representations, 2019. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix for \"Meta-Reinforcement Learning with Universal Policy Adaptation: Provable Near-Optimality under All-task Optimum Comparator\" ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Experimental Supplements ", "page_idx": 14}, {"type": "text", "text": "All experiments are executed on a computer with a 5.20 GHz Intel Core i12 CPU. ", "page_idx": 14}, {"type": "text", "text": "A Experimental Supplements of Verification of Theoretical Results. ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Experimental settings. In Section 6, we use the Frozen Lake environment in Gym [7] and consider a task distribution $\\mathbb{P}(\\Gamma)$ with high task variance and a task distribution $\\mathbb{P}(\\Gamma)$ with low task variance. In each distribution, there are 20 tasks. The tasks are characterized by the different settings of holes in the lake, where the holes are generated by random sampling. In the task distribution with high variance, the probability of the appearing hole in each grid is 0.3; in the task distribution with low variance, its probability is 0.1. We set $\\gamma=0.8$ , the reward is 1 when reaching the goal, and the reward is $-1$ when reaching the holes. When deriving the upper bound in Theorems 3 and 4, we approximately regard $T$ be sufficiently large, and $\\mathcal{O}(\\breve{h}_{i}\\big(\\frac{1}{\\sqrt{T}}\\big)\\dot{)}$ be close to 0. The Lipschitz of the tabular policy is 1, i.e., $L_{1}=1$ ; the Lipschitz of the derivative and the second-order derivative of the tabular policy are both 0, i.e., $L_{2}=0$ and $L_{3}=0$ . ", "page_idx": 14}, {"type": "text", "text": "Selection of hyper-parameters. We consider the tabular softmax policy and use Monte Carlo sampling to evaluate the Q-value. For the task distribution with high task variance, we set $\\lambda=0.5$ for $\\bar{\\mathcal{A}}l g^{(1)}$ , $\\lambda=0.5$ for $\\boldsymbol{A l g^{(2)}}$ , and $\\lambda=0.04$ for $A l g^{(3)}$ . For the task distribution with low task variance, we set $\\lambda=0.25$ for $\\boldsymbol{\\mathcal{A}l g^{(1)}}$ , $\\lambda=0.25$ for $\\bar{\\mathcal{A}}l g^{(2)}$ , and $\\lambda=0.02$ for $\\boldsymbol{A l g^{(3)}}$ . There is a clarification about the hyper-parameter selection and the verified bound shown in Appendix N.3. ", "page_idx": 14}, {"type": "text", "text": "Supplemental results. Figures 3 and 4 show the results of the proposed algorithm with $\\boldsymbol{A l g^{(2)}}$ and $A l g^{(3)}$ . It shows that, for all scenarios, the expected optimality gap of the policy adaptation $\\boldsymbol{A l g^{(2)}}$ or $A l g^{(3)}$ is smaller than the upper bound shown in Theorems 3 and 4, which verify our theoretical analysis. ", "page_idx": 14}, {"type": "image", "img_path": "rpjh69DUX2/tmp/8f890a676ac9eadb97c8aaf097637ae48970344ef071e2e9cd92c6fa366ed263.jpg", "img_caption": ["Figure 3: Results of the meta-test of BO-MRL on Frozen Lake, where $\\boldsymbol{A l g}^{(2)}$ is applied. Left: Average accumulated reward across all test tasks v.s. number of policy adaptation steps; Right: Comparing the expected optimality gap by the BO-MRL and baselines with the upper bound of the accumulated reward of one-time Alg(2). "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "rpjh69DUX2/tmp/24ad4010ea38bbeb317ea1bf82afceffae8f1d98bab0281622676aa74e46272c.jpg", "img_caption": ["Figure 4: Results of BO-MRL on Frozen Lake, where $\\boldsymbol{A l g}^{(3)}$ is applied. Comparing the expected optimality gap by the BO-MRL and baselines with the upper bound of the accumulated reward of one-time $\\mathcal{A}l g^{(3)}$ . "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "B Experimental Supplements of Locomotion. ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Experimental settings. We consider locomotion tasks HalfCheetah with goal directions and goal velocities, Ant with goal directions and goal velocities. We follow the problem setups of [67, 15]. In the goal velocity experiments, the moving reward is the negative absolute value between the agent\u2019s current velocity and a goal velocity, which is chosen uniformly at random between 0.0 and 2.0 for the cheetah and between 0.0 and 3.0 for the ant. In the goal direction experiments, the moving reward is the magnitude of the velocity in either the forward or backward direction, chosen at random for each task $\\tau$ in $\\mathbb{P}$ . For the Half-cheetah, the total reward $=$ moving reward - ctrl cost. For the ant, the total reward $=$ healthy reward $^+$ moving reward - ctrl cost - contact cost. The horizon is $H=200$ , with 20 rollouts per policy adaption step for all problems except the ant direction task, which used 40 rollouts per step. ", "page_idx": 15}, {"type": "text", "text": "Selection of hyper-parameters. We apply the proposed practical algorithm of Algorithm 1, Algorithm 2 in Appendix D. We consider the policy as a Gaussian distribution, where the neural network produces the means and variances of the actions. The neural network policy has two hidden layers of size 64, with tanh nonlinearities. We use Monte Carlo sampling to evaluate the Q-value. At the lower-level task-specific policy adaptation, the optimization number by Adam is 50. The models are trained for up to 500 meta-iterations. For the TRPO in meta-parameter optimization, we use the KL-divergence constraint as $\\delta=1e-3$ . ", "page_idx": 15}, {"type": "text", "text": "For the experiment of Half-Cheetah with goal velocities, we set $\\lambda=0.5$ for $\\boldsymbol{\\mathcal{A}l g^{(1)}}$ , $\\lambda=0.4$ for $A l g^{(2)}$ . For the experiment of Half-Cheetah with goal directions, we set $\\lambda=0.5$ for $\\boldsymbol{\\mathcal{A}l g^{(1)}}$ , $\\lambda=0.5$ for $A l g^{(2)}$ . For the experiment of Ant with goal velocities, we set $\\lambda=0.5$ for $\\boldsymbol{\\mathcal{A}l g^{(1)}}$ , $\\lambda=0.5$ for $\\boldsymbol{A l g}^{(2)}$ . For the experiment of Ant with goal directions, we set $\\lambda=0.5$ for $\\boldsymbol{\\mathcal{A}l g^{(1)}}$ , $\\lambda=0.5$ for $A l g^{(2)}$ . ", "page_idx": 15}, {"type": "text", "text": "Comparison setting. We compare the proposed algorithm with several optimization-based meta-RL algorithms, including MAML, E-MAML [55], and ProMP [50]. The experiment results of E-MAML, ProMP, and MAML-TRPO come from [67, 15]. We do not compare the proposed algorithm with black-box meta-RL algorithms, as they are based on the task context and even can achieve good performance without adaptation. ", "page_idx": 15}, {"type": "text", "text": "Supplemental results. Figure 5 shows that the proposed algorithm with both within-task algorithms $\\bar{\\mathcal{A}}\\bar{\\boldsymbol{g}}^{(i)}$ outperform the baseline methods in four experimental settings. The accumulated rewards of proposed algorithms increase fast and stop at points with better performance than the baseline methods. ", "page_idx": 15}, {"type": "image", "img_path": "rpjh69DUX2/tmp/ec9ffe69a726f153f977c67716693ab792aab50363238422d11bdcb01a852b49.jpg", "img_caption": ["Figure 5: Accumulated rewards during the meta-training under the practical algorithm of BO-MRL on the locomotion tasks. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Algorithm supplement ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "C Computation of $\\nabla_{\\phi}Q_{\\tau}^{\\pi_{\\phi}}(s,a)$ ", "page_idx": 15}, {"type": "text", "text": "In the computation of meta-objective shown in Propositions 1 and 2, we need to compute $\\nabla_{\\phi}Q_{\\tau}^{\\pi_{\\phi}}(s,a)$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\nabla_{\\phi}Q_{\\tau}^{\\pi_{\\phi}}(s,a)=\\frac{\\gamma}{1-\\gamma}\\cdot\\mathbb{E}_{(s^{\\prime},a^{\\prime})\\sim\\sigma_{\\tau,\\pi_{\\phi}}^{(s,a)}}\\left[\\nabla_{\\phi}\\ln\\pi_{\\phi}\\left(a^{\\prime}|s^{\\prime}\\right)Q_{\\tau}^{\\pi_{\\phi}}\\left(s^{\\prime},a^{\\prime}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the state-action visitation probability \u03c3\u03c4(,s,\u03c0a\u03b8) initialized at $(s,a)\\in S\\times A$ is defined by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sigma_{\\tau,\\pi_{\\phi}}^{\\left(s,a\\right)}\\left(s^{\\prime},a^{\\prime}\\right)=\\left(1-\\gamma\\right)\\sum_{t=0}^{\\infty}\\gamma^{t}\\mathbb{P}\\left(s_{t}=s^{\\prime},a_{t}=a^{\\prime}|\\pi_{\\phi},s_{0}\\sim P_{\\tau}(\\cdot|s,a)\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For the tabular softmax policy in discrete state-action space shown in Section 5.1, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\nabla_{\\phi(s^{\\prime},\\cdot)}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)=\\frac{\\gamma}{1-\\gamma}\\cdot\\sigma_{\\tau,\\hat{\\pi}_{\\phi}}^{(s,a)}(s^{\\prime})\\cdot\\hat{\\pi}_{\\phi}(\\cdot|s^{\\prime})\\odot A_{\\tau}^{\\hat{\\pi}_{\\phi}}\\left(s^{\\prime},\\cdot\\right),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\odot$ is the element-wise product, $\\phi(s^{\\prime},\\cdot)$ is the vector which includes $\\phi(s^{\\prime},a^{\\prime})$ for all $a^{\\prime}\\in\\mathcal{A}$ as the elements, and $A_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,\\cdot)$ is the vector which includes $A_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)$ for all $a\\in A$ as the elements. Equivalently, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\nabla_{\\phi(s^{\\prime},a^{\\prime})}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)=\\frac{\\gamma}{1-\\gamma}\\cdot\\sigma_{\\tau,\\hat{\\pi}_{\\phi}}^{(s,a)}(s^{\\prime})\\hat{\\pi}_{\\phi}(a^{\\prime}|s^{\\prime})A_{\\tau}^{\\hat{\\pi}_{\\phi}}\\left(s^{\\prime},a^{\\prime}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For the softmax policy with the function approximation, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\nabla_{\\phi}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)=\\!\\frac{\\gamma}{1-\\gamma}\\cdot\\mathbb{E}_{(s^{\\prime},a^{\\prime})\\sim\\sigma_{\\tau,\\hat{\\pi}_{\\phi}}^{(s,a)}}\\left[\\frac{\\nabla_{\\phi}\\hat{\\pi}_{\\phi}\\left(a^{\\prime}\\left|s^{\\prime}\\right)}{\\hat{\\pi}_{\\phi}\\left(a^{\\prime}\\left|s^{\\prime}\\right)}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}\\left(s^{\\prime},a^{\\prime}\\right)\\right]\\right.}\\\\ &{}&{\\qquad\\qquad\\qquad\\left.=\\!\\frac{\\gamma}{1-\\gamma}\\cdot\\mathbb{E}_{(s^{\\prime},a^{\\prime})\\sim\\sigma_{\\tau,\\hat{\\pi}_{\\phi}}^{(s,a)}}\\left[\\nabla_{\\phi}f_{\\phi}\\left(s^{\\prime},a^{\\prime}\\right)Q_{\\tau}^{\\hat{\\pi}_{\\phi}}\\left(s^{\\prime},a^{\\prime}\\right)\\right]\\right.}\\\\ &{}&{\\qquad\\qquad\\qquad\\left.=\\!\\frac{\\gamma}{1-\\gamma}\\cdot\\mathbb{E}_{(s^{\\prime},a^{\\prime})\\sim\\sigma_{\\tau,\\hat{\\pi}_{\\phi}}^{(s,a)}}\\left[\\nabla_{\\phi}f_{\\phi}\\left(s^{\\prime},a^{\\prime}\\right)A_{\\tau}^{\\hat{\\pi}_{\\phi}}\\left(s^{\\prime},a^{\\prime}\\right)\\right]\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. As shown in [60], ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\phi}Q_{\\tau}^{\\pi_{\\phi}}(s,a)=\\nabla_{\\phi}\\left((1-\\gamma)\\cdot r_{\\tau}(s,a)+\\gamma\\cdot\\mathbb{E}_{s^{\\prime}\\sim P_{\\tau}(\\cdot\\vert s,a)}\\left[V_{\\tau}^{\\pi_{\\phi}}\\left(s^{\\prime}\\right)\\right]\\right)}\\\\ &{\\qquad\\qquad\\qquad=\\displaystyle\\frac{\\gamma}{1-\\gamma}\\cdot\\mathbb{E}_{(s^{\\prime},a^{\\prime})\\sim\\sigma_{\\tau,\\pi_{\\phi}}^{(s,a)}}\\left[\\nabla_{\\phi}\\ln\\pi_{\\phi}\\left(a^{\\prime}\\vert s^{\\prime}\\right)\\cdot Q_{\\tau}^{\\pi_{\\phi}}\\left(s^{\\prime},a^{\\prime}\\right)\\right]}\\\\ &{\\qquad\\qquad=\\displaystyle\\frac{\\gamma}{1-\\gamma}\\cdot\\mathbb{E}_{(s^{\\prime},a^{\\prime})\\sim\\sigma_{\\tau,\\pi_{\\phi}}^{(s,a)}}\\left[\\nabla_{\\phi}\\ln\\pi_{\\phi}\\left(a^{\\prime}\\vert s^{\\prime}\\right)\\cdot A_{\\tau}^{\\pi_{\\phi}}\\left(s^{\\prime},a^{\\prime}\\right)\\right].}\\\\ &{\\qquad\\qquad=\\displaystyle\\frac{\\gamma}{1-\\gamma}\\cdot\\mathbb{E}_{(s^{\\prime},a^{\\prime})\\sim\\sigma_{\\tau,\\pi_{\\phi}}^{(s,a)}}\\left[\\frac{\\nabla_{\\phi}\\pi_{\\phi}\\left(a^{\\prime}\\vert s^{\\prime}\\right)}{\\pi_{\\phi}\\left(a^{\\prime}\\vert s^{\\prime}\\right)}\\cdot A_{\\tau}^{\\pi_{\\phi}}\\left(s^{\\prime},a^{\\prime}\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By Lemma 4, from (12), we can obtain (4); from (14), we can obtain (6). ", "page_idx": 16}, {"type": "text", "text": "D Practical algorithm ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In Sections 4 and 5, we develop a theoretically guaranteed algorithm with Assumptions 1, 2, and 3. In this section, we develop a practical instantiation of Algorithm 1 and evaluate its performance in high-dimensional experiments in Section 6. ", "page_idx": 16}, {"type": "text", "text": "Algorithm 2 states the practical algorithm of Algorithm 1. Compared with Algorithm 1, Algorithm 2 considers and overcomes the following limitations of Algorithm 1: (a) evaluating the exact expectation in (1) and (2) is costly and the approximation error could influence the task-specific policy adaptation if using sampling, especially in the meta-RL problem where the sampling data is limited; (b) the optimization problems in (1) and (2) have no closed-form solution; (c) the computation of the gradients of the meta-objectives shown in Propositions 1 and 2 is time-consuming; (d) the gradient-based approach to optimize the meta-objective is not stable in RL problems. ", "page_idx": 16}, {"type": "text", "text": "In the beginning of Algorithm 2, we first sample a batch of tasks $\\{\\tau_{i}\\}_{i=1}^{N}\\sim\\mathbb{P}(\\Gamma)$ . On each task $\\tau_{i}$ , we sample the trajectories of the meta-policy $\\pi_{\\phi_{t}}$ as $B_{\\tau_{i}}$ , and evaluate the state-action value function $Q_{\\tau_{i}}^{\\pi_{\\phi_{t}}}(\\cdot,\\cdot)$ for each $\\tau_{i}$ . Next, since the number of the sampling state-action pairs in $B_{\\tau_{i}}$ is limited, if we directly use the sampling average to approximate the expectation in (2), the approximation error will be very large when $\\pi_{\\phi}(a|s)$ is small. Therefore, we solve the following optimization problem as the within-task algorithm instead of (2): ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\pi_{\\theta_{\\tau}^{\\prime}}=\\mathcal{A}l g(\\lambda,\\phi_{t},\\tau)=\\arg\\operatorname*{min}_{\\theta}\\frac{1}{|B_{\\tau}|}\\sum_{(a,s)\\in B_{\\tau}}h\\left(\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\phi}(a|s)}\\right)Q_{\\tau}^{\\pi_{\\phi}}(s,a)-\\lambda D_{\\tau}^{2}(\\pi_{\\phi},\\pi_{\\theta}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\begin{array}{r}{h(x)=\\frac{2}{1+e^{-2(x-1)}}}\\end{array}$ . The function $h$ avoids the term $\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\phi}(a|s)}$ is optimized to very large. We use Adam [32] to solve the problem in (7). Next, the computation of the gradients of the metaobjectives shown in Proposition 2 is time-consuming, since the computation complexity of the term ", "page_idx": 16}, {"type": "text", "text": "Algorithm 2 Practical Algorithm of BO-MRL ", "page_idx": 17}, {"type": "text", "text": "Require: Regularization weight $\\lambda>0$ ; initial meta-parameter $\\phi_{0}$ ; learning rate $\\alpha$ .   \n1: for $t=1,\\cdot\\cdot\\cdot,T$ do   \n2: Sample a batch of tasks $\\{\\tau_{i}\\}_{i=1}^{N}\\sim\\mathbb{P}(\\Gamma)$ with the MDP $\\mathcal{M}_{\\tau_{i}}$ i.i.d.   \n3: On each task $\\tau_{i}$ , sample the trajectories of the meta-policy $\\pi_{\\phi_{t}}$ as $B_{\\tau_{i}}$ .   \n4: Evaluate the state-action value function $Q_{\\tau_{i}}^{\\pi_{\\phi_{t}}}(\\cdot,\\cdot)$ for each $\\tau_{i}$ .   \n5: For each task $\\tau_{i}$ , compute the task-specific policy $\\pi_{\\boldsymbol{\\theta}_{\\tau_{i}}^{\\prime}}$ by solving $\\mathscr{A}l g(\\lambda,\\phi_{t},\\tau_{i})$ defined in (7) by Adam.   \n6: Compute $\\nabla_{\\phi}J_{\\tau_{i}}(\\pi_{\\theta_{\\tau_{i}}^{\\prime}})$ in (8) by conjugate gradient method   \n7: Update meta-parameter by the TRPO with the gradient $\\begin{array}{r}{\\frac{1}{N}\\sum_{i}\\nabla_{\\phi}J_{\\tau_{i}}\\big(\\pi_{\\theta_{\\tau_{i}}^{\\prime}}\\big)}\\end{array}$ and the sampling trajectories $\\{B_{\\tau_{i}}\\}_{i=1}^{N}$ .   \n8: end for   \n9: Return $\\phi_{T}$ ", "page_idx": 17}, {"type": "text", "text": "$-\\frac{\\nabla_{\\theta}\\pi_{\\theta}(a|s)}{\\lambda\\pi_{\\phi}(a|s)}\\nabla_{\\phi}^{\\top}Q_{\\tau}^{\\pi_{\\phi}}(s,a)$ is very high. So, we omit the term, and compute $\\nabla_{\\phi}J_{\\tau}(\\pi_{\\theta_{\\tau}^{\\prime}})$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{1}{1-\\gamma}\\nabla_{\\phi}\\theta_{\\tau}^{\\prime}\\cdot\\underset{\\underset{a\\sim\\pi_{\\theta_{\\tau}^{\\prime}}}{\\mathbb{E}}(\\cdot|s)}{\\mathbb{E}}\\left[\\frac{\\nabla_{\\theta_{\\tau}^{\\prime}}\\pi_{\\theta_{\\tau}^{\\prime}}(a|s)}{\\pi_{\\theta_{\\tau}^{\\prime}}(a|s)}Q_{\\tau}^{\\pi_{\\theta_{\\tau}^{\\prime}}}(s,a)\\right],\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\nabla_{\\phi}^{\\top}\\theta_{\\tau}^{\\prime}\\approx-\\underset{a\\sim\\pi_{\\phi}(\\cdot\\vert s)}{\\mathbb{E}}\\left[\\nabla_{\\theta}^{2}d^{2}(\\pi_{\\phi}(\\cdot\\vert s),\\pi_{\\theta}(\\cdot\\vert s))-\\frac{\\nabla_{\\theta}^{2}\\pi_{\\theta}(a\\vert s)}{\\lambda\\pi_{\\phi}(a\\vert s)}Q_{\\tau}^{\\pi_{\\phi}}(s,a)\\right]^{-1}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Finally, since the gradient-based approach is not stable in RL problems, we optimize meta-parameter by the TRPO with the gradient $\\begin{array}{r}{\\frac{1}{N}\\!\\sum_{i}\\!\\nabla_{\\phi}J_{\\tau_{i}}(\\pi_{\\theta_{\\tau_{i}}^{\\prime}})}\\end{array}$ and the sampling trajectories $\\{B_{\\tau_{i}}\\}_{i=1}^{N^{'}}$ , similar to [15]. ", "page_idx": 17}, {"type": "text", "text": "E Discussion about computational complexity of hyper-gradient ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In Algorithms 1 and 2, we compute the inverse of the Hessian matrix when computing the hypergradient by Proposition 2 and (8). The computation of the inverse of the Hessian matrix is not time-consuming and does not increase the processing time much. Here are the two reasons. ", "page_idx": 17}, {"type": "text", "text": "First, we apply the conjugate gradient algorithm to compute the inverse of the Hessian and its computation complexity is not high. According to our experiment of Half-cheetah, the computation time of the hyper-gradient with the inverse of Hessian for a three-layer neural network is about 0.3 second in each meta-parameter update, where we use only the CPU to compute the hyper-gradient. This approach has demonstrated high efficiency across a wide range of applications, including several widely used RL algorithms, such as TRPO [51] and CPO [1], which compute the inverse of the Hessian in each policy update iteration. The detail is shown in Appendix C of [51]. They usually compute thousands times of the Hessian inverse for a single RL task. In the simplest meta-RL method, MAML [15], the authors use the TRPO to update the meta-parameter, as shown in Section 5.3 of [15], the inverse of the Hessian is also computed. Therefore, the computational complexity of the hyper-gradient in our proposed method is comparable to many existing RL and meta-RL approaches, which are shown efficient. ", "page_idx": 17}, {"type": "text", "text": "Second, the biggest computational bottleneck in the meta-RL framework is not the hyper-gradient computation. According to our experiment, the percentage of the computation time in the metaparameter update, including the computation time of the hyper-gradient computation, is less than $\\bar{5}\\%$ , where we use only the CPU to compute the hyper-gradient. The percentage of computation time in the data collection and the Q value computation by Monte-Carlo sampling is more than $70\\%$ , although the state-action data points are collected in the MDP simulator Gym and the data collection is very fast. In real-world applications, the state-action data points are even harder to collect and data collection consumes a longer time. Therefore, the computational time of the hyper-gradient computation has a relatively small impact on the mete-RL framework. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "F Data sampling complexity and computational complexity of one-time policy adaptation ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The one-time policy adaptation in our algorithm is defined as solving the optimal solution of the optimization problem in (1) or (2) by multiple optimization iterations. The definition of the one-time policy adaptation follows many widely used RL algorithms, such as TRPO [51] and CPO [1], which evaluate the Q-values for the current policy and solve the optimal solution for an optimization problem to obtain the next policy in each policy optimization iteration. For example, TRPO solves the optimization problem in (14) of [51] in each iteration. ", "page_idx": 18}, {"type": "text", "text": "In the one-time policy adaptation, we only need to evaluate the Q-function for one policy $\\pi_{\\phi}$ by Monte-Carlo sampling, which requires the agent to explore the MDP using one policy $\\pi_{\\phi}$ , then solve the optimization problem in (1) or (2) by multiple optimization iterations with the fixed Q-function. The data sampling complexity is exactly the same as the one-step gradient descent in MAML, which uses Monte-Carlo sampling to evaluate the Q-function and compute the policy gradient based on the Q-function. ", "page_idx": 18}, {"type": "text", "text": "The multiple optimization steps in the one-time policy adaptation are different from the multi-step policy gradient update in MAML. In our algorithm, the multiple optimization steps in a one-time policy adaptation only need to evaluate the Q-function for one policy $\\pi_{\\phi}$ , which requires the agent to explore the MDP using only $\\pi_{\\phi}$ . In MAML, the Q-function for a new policy needs to be evaluated in each policy gradient update, and then multiple Q-functions are evaluated for multiple policies, which requires the agent to explore the MDP using multiple policies. Instead, the one-time policy adaptation in our algorithm corresponds to a one-step policy gradient update in MAML, as they use the same number of data points. ", "page_idx": 18}, {"type": "text", "text": "Moreover, we would like to claim that the computation complexity for the one-time policy adaptation in our algorithm and that of the one-step policy gradient update in MAML is comparable, although our algorithm requires multiple optimization iterations. As mentioned in Appendix E, the computation time in the data collection and the Q value computation takes more than $70\\%$ of total computation time, which is much longer than other parts of the algorithm, including the multiple optimization iterations in police adaptation ( $15\\%$ of total computation time). This happens although the state-action data points are collected in the MDP simulator Gym and the data collection is very fast. In real-world applications, the state-action data points are even harder to collect and the consuming time of data collection is much longer. Therefore, the computational time of the multiple optimization iterations has a relatively small impact on the mete-RL framework. Therefore, the computation time of our algorithm and that of MAML is comparable. ", "page_idx": 18}, {"type": "text", "text": "From the statement in the above paragraphs, both the data sampling complexity and computational complexity of the one-time policy adaptation in our algorithm and the one-step policy gradient update in MAML are similar. Thus, we define solving the optimal solution of the optimization problem in (1) or (2) as a single policy adaptation step. ", "page_idx": 18}, {"type": "text", "text": "G Algorithm details with the first-order approximation ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "As we mentioned in Section 4, we can approximate the first term $\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\pi_{\\phi}},a\\sim\\pi_{\\phi}(\\cdot|s)}[\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\phi}(a|s)}Q_{\\tau}^{\\pi_{\\phi}}(s,a)]$ in (2) by its first-order approximation as the within-task algorithm, similar to the implementations in TRPO [51] and PPO [52]. In particular, the within-task algorithm is reduced to the following formulation, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\pi_{\\theta_{\\tau}^{\\prime}}=\\mathcal{A}l g(\\pi_{\\phi},\\lambda,\\tau)\\triangleq\\underset{\\pi_{\\theta}}{\\operatorname{argmin}}-\\frac{1}{\\lambda}G(\\phi)^{\\top}\\theta+D_{\\tau}^{2}(\\pi_{\\phi},\\pi_{\\theta}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Here, we use the first-order approximation to replace the first term of (2). In particular, $G(\\phi)^{\\top}(\\theta\\mathrm{~-~}\\phi)$ is the first order approximation of $\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\pi_{\\phi}},a\\sim\\pi_{\\phi}(\\cdot|s)}[\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\phi}(a|s)}Q_{\\tau}^{\\pi_{\\phi}}(s,a)]$ , where $\\begin{array}{r l}{\\widetilde{x}(\\phi)\\ =\\ \\nabla_{\\theta}\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\pi_{\\phi}},a\\sim\\pi_{\\phi}(\\cdot\\vert s)}[\\frac{\\nabla_{\\theta}\\pi_{\\theta}(a\\vert s)}{\\pi_{\\phi}(a\\vert s)}Q_{\\tau}^{\\pi_{\\phi}}(s,a)]\\vert_{\\theta=\\phi}\\ =\\ \\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\pi_{\\phi}},a\\sim\\pi_{\\phi}(\\cdot\\vert s)}[\\frac{\\nabla_{\\phi}\\pi_{\\phi}(a\\vert s)}{\\pi_{\\phi}(a\\vert s)}Q_{\\tau}^{\\pi_{\\phi}}(s,a)].}\\end{array}$ Under the simplified within-task algorithm $\\boldsymbol{A l g}$ , the hypergradient of the meta-objective function ", "page_idx": 18}, {"type": "text", "text": "$\\nabla_{\\phi}J_{\\tau}(\\pi_{\\theta_{\\tau}^{\\prime}})$ can be computed by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\nabla_{\\phi}J_{\\tau}(\\pi_{\\theta_{\\tau}^{\\prime}})=\\frac{1}{1-\\gamma}\\nabla_{\\phi}\\theta_{\\tau}^{\\prime}\\cdot\\underbrace{\\mathbb{E}}_{s\\sim\\nu_{\\tau}^{\\prime}\\circ\\tau}\\quad\\left[\\frac{\\nabla_{\\theta_{\\tau}^{\\prime}}\\pi_{\\theta_{\\tau}^{\\prime}}(a|s)}{\\pi_{\\theta_{\\tau}^{\\prime}}(a|s)}Q_{\\tau}^{\\pi_{\\theta_{\\tau}^{\\prime}}}(s,a)\\right],\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla_{\\phi}^{\\top}\\theta_{\\tau}^{\\prime}=\\nabla_{\\theta_{\\tau}^{\\prime}}^{2}D_{\\tau}^{2}(\\pi_{\\phi},\\pi_{\\theta_{\\tau}^{\\prime}})^{-1}(\\underset{s\\sim\\nu_{\\tau}^{\\pi_{\\phi}^{\\pi}}a\\sim\\pi_{\\phi}(\\cdot|s)}{\\sum_{s\\sim\\nu_{\\tau}^{\\pi_{\\phi}^{\\phi}}}}[\\frac{1}{\\lambda}\\frac{\\nabla_{\\phi}\\pi_{\\phi}(a|s)}{\\pi_{\\phi}(a|s)}\\nabla_{\\phi}^{\\top}Q_{\\tau}^{\\pi_{\\phi}}(s,a)+}\\\\ {\\frac{1}{\\lambda}\\frac{\\nabla_{\\phi}^{2}\\pi_{\\phi}(a|s)}{\\pi_{\\phi}(a|s)}Q_{\\tau}^{\\pi_{\\phi}}(s,a)]-\\nabla_{\\phi}^{\\top}\\nabla_{\\theta_{\\tau}^{\\prime}}D_{\\tau}^{2}(\\pi_{\\phi},\\pi_{\\theta_{\\tau}^{\\prime}})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The computation of $\\nabla_{\\phi}^{\\top}\\theta_{\\tau}^{\\prime}$ is derived in Section J.3. ", "page_idx": 19}, {"type": "text", "text": "H Connection between the proposed algorithm and MAML ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "As we claim in Section 4, when we approximate the first term Es\u223c\u03bd\u03c4\u03c0\u03d5,a\u223c\u03c0\u03d5(\u00b7|s)[ \u03c0\u03c0\u03b8\u03d5((aa||ss)) Q\u03c4\u03c0\u03d5 (s, a)] iisn  r(e2d) ubcye di ttso  ftihrset -poorldicery  agprapdrioexnitm aastcieonnt .a Innd  paalrstoi csuellaer,c tt $D_{\\tau}\\,=\\,D_{\\tau,3}$ $\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\pi_{\\phi}},a\\sim\\pi_{\\phi}(\\cdot|s)}[\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\phi}(a|s)}Q_{\\tau}^{\\pi_{\\phi}}(s,a)]$ is approximated by (\u03b8 \u2212\u03d5)\u22a4Es\u223c\u03bd\u03c4\u03c0\u03d5,a\u223c\u03c0\u03d5(\u00b7|s)[ \u2207\u03c0\u03c0\u03d5\u03d5((aa|s|s)) $a\\!\\sim\\!\\pi_{\\phi}(\\cdot|s)\\big[\\frac{\\nabla\\pi_{\\phi}(a|s)}{\\pi_{\\phi}(a|s)}Q_{\\tau}^{\\pi_{\\phi}}(s,a)\\big]$ , then the within-task algorithm $A l g(\\pi_{\\phi},\\lambda,\\tau)$ becomes to ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\theta_{\\tau}^{\\prime}=\\mathcal{A}l g(\\pi_{\\phi},\\lambda,\\tau)\\triangleq\\underset{\\theta}{\\mathrm{argmax}}\\ -\\lambda\\|\\theta-\\phi\\|^{2}+\\theta^{\\top}\\cdot\\underset{a\\sim\\pi_{\\phi}^{\\pi_{\\sigma}(\\cdot\\vert s)}}{\\mathbb{E}}\\left[\\frac{\\nabla_{\\phi}\\pi_{\\phi}(a\\vert s)}{\\pi_{\\phi}(a\\vert s)}Q_{\\tau}^{\\pi_{\\phi}}(s,a)\\right].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Solve the optimization problem, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\theta_{\\tau}^{\\prime}=\\phi+\\frac{1}{\\lambda}\\underbrace{{\\mathbb E}}_{s\\sim\\pi_{\\phi}(\\cdot\\vert s)}\\left[\\frac{\\nabla_{\\phi}\\pi_{\\phi}(a\\vert s)}{\\pi_{\\phi}(a\\vert s)}Q_{\\tau}^{\\pi_{\\phi}}(s,a)\\right]=\\phi+\\frac{1-\\gamma}{\\lambda}\\nabla_{\\phi}J_{\\tau}(\\phi),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which is policy gradient ascent. Thus, when we select (11) as the within-task algorithm, the metaalgorithm (3) is reduced to the algorithm that can learn the initialization parameter for the policy gradient ascent. ", "page_idx": 19}, {"type": "text", "text": "As shown in [15], MAML also learns the initialization parameter $\\phi$ for the policy gradient ascent. However, MAML ignores that the sampled trajectories with policy $\\pi_{\\phi}$ also depend on $\\phi$ . Specifically, MAML first uses the sampled trajectories to approximate $Q_{\\tau}^{\\pi_{\\phi}}(s,a)$ by (Monte Carlo sampling on the REINFORCE algorithm), then computes the policy gradient and does one step of gradient ascent for the task-specific adaptation. Next, it computes $\\nabla_{\\phi}J_{\\tau}(\\theta_{\\tau}^{\\prime})$ to update the meta-parameter $\\phi$ . When it computes $\\nabla_{\\phi}\\theta_{\\tau}^{\\prime}$ , it treats $Q_{\\tau}^{\\pi_{\\phi}}(s,a)$ as a given data point that is independent with $\\phi$ , and then ignore the $\\nabla_{\\phi}Q_{\\tau}^{\\pi_{\\phi}}(s,a)$ . In contrast, our reduced meta-algorithm takes it into account and provides a precise formulation to learn the meta-initialization for the policy gradient algorithm. ", "page_idx": 19}, {"type": "text", "text": "Since the proposed meta-RL framework can include MAML as a special case, our analysis in Section 5 also provides the theoretical motivation for MAML. ", "page_idx": 19}, {"type": "text", "text": "Analysis and Proof ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "I Auxiliary Results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Lemma 3 (Policy gradient [56, 2]). Let $\\pi_{\\theta}$ be the parameterized policy with the parameter $\\theta$ . It holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\nabla_{\\theta}J_{\\tau}(\\pi_{\\theta})\\!=\\!\\!\\frac{1}{1\\,-\\,\\gamma}\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\pi_{\\theta}},a\\sim\\pi_{\\theta}(\\cdot\\vert s)}\\left[\\nabla_{\\theta}\\ln\\pi_{\\theta}(a\\vert s)Q_{\\tau}^{\\pi_{\\theta}}(s,a)\\right]}\\\\ &{}&{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Lemma 4 (Policy gradient of the softmax policy). Consider the softmax policy $\\scriptstyle{\\hat{\\pi}}_{\\theta}$ parameterized by $\\theta$ a discrete state-action space and the tabular policy, $\\begin{array}{r}{\\hat{\\pi}_{\\boldsymbol{\\theta}}(a|s)=\\frac{\\exp(\\boldsymbol{\\theta}(s,a))}{\\sum_{a^{\\prime}\\in\\mathcal{A}}\\exp(\\boldsymbol{\\theta}(s,a^{\\prime}))}}\\end{array}$ , $\\forall(s,a)\\in$ ${\\mathcal{S}}\\times{\\mathcal{A}}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\nabla_{\\theta(s,\\cdot)}J_{\\tau}(\\hat{\\pi}_{\\theta})=\\frac{1}{1-\\gamma}\\nu_{\\tau}^{\\hat{\\pi}_{\\theta}}(s)\\cdot\\hat{\\pi}_{\\theta}(\\cdot|s)\\odot A_{\\tau}^{\\hat{\\pi}_{\\theta}}(s,\\cdot),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\odot$ is the element-wise product, $\\theta(s,\\cdot)$ is the vector which includes $\\theta(s,a)$ for all $a\\in{\\mathcal{A}}$ as the elements, $A_{\\tau}^{\\hat{\\pi}_{\\theta}}(s,\\cdot)$ is the vector which includes $A_{\\tau}^{\\hat{\\pi}_{\\theta}}(s,a)$ for all $a\\in A$ as the elements. Equivalently, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\nabla_{\\theta(s,a)}J_{\\tau}(\\hat{\\pi}_{\\theta})=\\frac{1}{1-\\gamma}\\nu_{\\tau}^{\\hat{\\pi}_{\\theta}}(s)\\hat{\\pi}_{\\theta}(a|s)A_{\\tau}^{\\hat{\\pi}_{\\theta}}(s,a),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For the softmax policy with function approximation, the policy $\\pi_{\\theta}$ is defined by $\\pi_{\\theta}(a|s)\\;\\;=\\;\\;$ A eexxpp((ff\u03b8\u03b8((ss,,aa\u2032))))da\u2032 , \u2200(s, a) \u2208S \u00d7 A. It holds that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}J_{\\tau}(\\hat{\\pi}_{\\theta})=\\!\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\hat{\\pi}_{\\theta}},a\\sim\\hat{\\pi}_{\\theta}(\\cdot|s)}\\left[\\nabla_{\\theta}f_{\\theta}(s,a)A_{\\tau}^{\\hat{\\pi}_{\\theta}}(s,a)\\right].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. For the discrete state-action space and the tabular policy, (12) is shown in Lemma C.1 of [2]. For the softmax policy with function approximation, from Lemma 3, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\theta}J_{\\tau}(\\hat{\\pi}_{\\theta})=\\displaystyle\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\hat{\\pi}_{\\theta}},a\\sim\\hat{\\pi}_{\\theta}(\\cdot\\vert s)}\\left[\\nabla_{\\theta}\\ln\\hat{\\pi}_{\\theta}(a\\vert s)A_{\\tau}^{\\hat{\\pi}_{\\theta}}(s,a)\\right]}\\\\ &{=\\displaystyle\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\hat{\\pi}_{\\theta}},a\\sim\\hat{\\pi}_{\\theta}(\\cdot\\vert s)}\\left[\\nabla_{\\theta}\\ln\\left(\\frac{\\exp(f_{\\theta}(s,a))}{\\int_{A}\\exp(f_{\\theta}(s,a^{\\prime}))d a^{\\prime}}\\right)A_{\\tau}^{\\hat{\\pi}_{\\theta}}(s,a)\\right]}\\\\ &{=\\displaystyle\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\hat{\\pi}_{\\theta}},a\\sim\\hat{\\pi}_{\\theta}(\\cdot\\vert s)}\\left[\\nabla_{\\theta}f_{\\theta}(s,a)-\\nabla_{\\theta}\\ln\\left(\\int_{A}\\exp(f_{\\theta}(s,a^{\\prime}))d a^{\\prime}\\right)A_{\\tau}^{\\hat{\\pi}_{\\theta}}(s,a)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Here, $\\begin{array}{r}{\\nabla_{\\theta}\\ln\\big(\\int_{A}\\exp(f_{\\theta}(s,a^{\\prime}))d a^{\\prime}\\big)}\\end{array}$ is independent with $a$ , then $\\nabla_{\\boldsymbol{\\theta}}J_{\\boldsymbol{\\tau}}\\big(\\hat{\\pi}_{\\boldsymbol{\\theta}}\\big)$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle=\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\hat{\\pi}_{\\theta}},a\\sim\\hat{\\pi}_{\\theta}(\\cdot\\vert s)}\\left[\\nabla_{\\theta}f_{\\theta}(s,a)-\\nabla_{\\theta}\\ln\\left(\\int_{A}\\exp(f_{\\theta}(s,a^{\\prime}))d a^{\\prime}\\right)A_{\\tau}^{\\hat{\\pi}_{\\theta}}(s,a)\\right]}\\\\ &{\\displaystyle=\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\hat{\\pi}_{\\theta}},a\\sim\\hat{\\pi}_{\\theta}(\\cdot\\vert s)}\\left[\\nabla_{\\theta}f_{\\theta}(s,a)A_{\\tau}^{\\hat{\\pi}_{\\theta}}(s,a)\\right]-}\\\\ &{\\displaystyle\\quad\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\hat{\\pi}_{\\theta}}}\\left[\\nabla_{\\theta}\\ln\\left(\\int_{A}\\exp(f_{\\theta}(s,a^{\\prime}))d a^{\\prime}\\right)\\mathbb{E}_{a\\sim\\hat{\\pi}_{\\theta}(\\cdot\\vert s)}A_{\\tau}^{\\hat{\\pi}_{\\theta}}(s,a)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since $\\begin{array}{r}{\\mathbb{E}_{a\\sim\\hat{\\pi}_{\\boldsymbol{\\theta}}(\\cdot|s)}A_{\\tau}^{\\hat{\\pi}_{\\boldsymbol{\\theta}}}(s,a)=\\mathbb{E}_{a\\sim\\hat{\\pi}_{\\boldsymbol{\\theta}}(\\cdot|s)}[Q_{\\tau}^{\\hat{\\pi}_{\\boldsymbol{\\theta}}}(s,a)]-V_{\\tau}^{\\hat{\\pi}_{\\boldsymbol{\\theta}}}(s)=0}\\end{array}$ . Then, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}J_{\\tau}(\\hat{\\pi}_{\\theta})=\\!\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\hat{\\pi}_{\\theta}},a\\sim\\hat{\\pi}_{\\theta}(\\cdot|s)}\\left[\\nabla_{\\theta}f_{\\theta}(s,a)A_{\\tau}^{\\hat{\\pi}_{\\theta}}(s,a)\\right].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "J Proofs of the computation of hypergradient ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "J.1 Proofs of Propositions 1 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proofs of Propositions $^{\\,I}$ . Consider the within-task algorithm in discrete space: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle{\\cal A l g}(\\pi_{\\phi},\\lambda,\\tau)=\\arg\\!\\operatorname*{max}_{\\pi}\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\pi_{\\phi}}}\\left[\\sum_{a\\in A}\\pi(a|s)Q_{\\tau}^{\\pi_{\\phi}}(s,a)\\right]-\\lambda D_{\\tau}^{2}(\\pi_{\\phi},\\pi)}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\arg\\!\\operatorname*{max}_{\\mathbb{R}_{s\\sim\\nu_{\\tau}^{\\pi_{\\phi}}}}\\left[\\sum_{a\\in A}\\pi(a|s)Q_{\\tau}^{\\pi_{\\phi}}(s,a)-\\lambda d^{2}(\\pi_{\\phi}(\\cdot|s),\\pi(\\cdot|s))\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Here, $d$ can be selected from $d_{1}$ to $d_{3}$ defined in Section 3, corresponding to the selection of $D_{\\tau}$ from $D_{\\tau,1}$ to $D_{\\tau,3}$ . ", "page_idx": 20}, {"type": "text", "text": "The above optimization problem is formally defined by the following problem, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\varLambda l g(\\pi_{\\phi},\\lambda,\\tau)=\\underset{\\pi}{\\mathrm{argmax}}\\:\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\pi_{\\phi}}}\\left[\\sum_{a\\in\\cal A}\\pi(a|s)Q_{\\tau}^{\\pi_{\\phi}}(s,a)-\\lambda d^{2}(\\pi_{\\phi}(\\cdot|s),\\pi(\\cdot|s),s)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "With Assumption 2, the problem is equivalent to that, for any $s\\in S$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\nA l g(\\pi_{\\phi},\\lambda,\\tau)(\\cdot|s)=\\operatorname*{argmax}_{\\pi(\\cdot|s)}\\sum_{a\\in\\mathcal{A}}\\pi(a|s)Q_{\\tau}^{\\pi_{\\phi}}(s,a)-\\lambda d^{2}(\\pi_{\\phi}(\\cdot|s),\\pi(\\cdot|s)),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Consider a $s\\in S$ , the Lagrangian of the above maximization problem is ", "page_idx": 21}, {"type": "equation", "text": "$$\n-\\sum_{a\\in A}\\pi(a|s)Q_{\\tau}^{\\pi_{\\phi}}(s,a)+\\lambda d^{2}(\\pi_{\\phi}(\\cdot|s),\\pi(\\cdot|s))+\\mu(\\sum_{a\\in A}\\pi(a|s)-1),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\mu$ is the Lagrangian multiplier. The optimality condition of $\\pi(\\cdot|s)$ is that, ", "page_idx": 21}, {"type": "equation", "text": "$$\n-Q_{\\tau}^{\\pi_{\\phi}}(s,\\cdot)+\\lambda\\nabla_{\\pi(\\cdot|s)}d^{2}(\\pi_{\\phi}(\\cdot|s),\\pi(\\cdot|s))+\\mu[1,\\cdot\\cdot\\cdot\\cdot,1]^{\\top}=0.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Here, $Q_{\\tau}^{\\pi_{\\phi}}(s,\\cdot)$ denotes a vector include $Q_{\\tau}^{\\pi_{\\phi}}(s,a)$ for each $a\\in{\\mathcal{A}}$ , and $\\pi(\\cdot|s)$ denotes a vector include $\\pi(a|s)$ for each $a\\in{\\mathcal{A}}$ . ", "page_idx": 21}, {"type": "text", "text": "Then, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n-Q_{\\tau}^{\\pi_{\\phi}}(s,\\cdot)+\\lambda\\nabla_{\\pi(\\cdot\\vert s)}d^{2}(\\pi_{\\phi}(\\cdot\\vert s),\\pi(\\cdot\\vert s))\\vert_{\\pi=A l g(\\pi_{\\phi},\\lambda,\\tau)}+\\mu[1,\\cdots,1]^{\\top}=0.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Note that the optimization problem (15) depends on $\\phi$ , and $\\pi=\\mathcal{A}l g(\\pi_{\\phi},\\lambda,\\tau)$ is a function of $\\phi$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n-Q_{\\tau}^{\\pi_{\\phi}}(s,\\cdot)+\\lambda\\nabla_{\\pi(\\cdot|s)}d^{2}(\\pi_{\\phi}(\\cdot|s),\\pi(\\cdot|s))|_{\\pi=A l g(\\pi_{\\phi},\\lambda,\\tau)}+\\mu(\\phi)[1,\\cdot\\cdot\\cdot\\,,1]^{\\top}=0,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "i.e., $\\mu$ is a function of $\\phi$ . ", "page_idx": 21}, {"type": "text", "text": "Also, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mu(\\phi)(\\sum_{a\\in{\\cal A}}{\\cal A}l g(\\pi_{\\phi},\\lambda,\\tau)(a|s)-1)=0.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "With (17) and (18), we can compute $\\nabla_{\\phi}\\mathcal{A}l g(\\pi_{\\phi},\\lambda,\\tau)$ , where $A l g(\\pi_{\\phi},\\lambda,\\tau)$ is continuously differentiable as shown in [64]. We do derivative of (17) and (18) with respect to $\\phi$ , we have $[\\nabla_{\\phi}A l g(\\pi_{\\phi},\\lambda,\\tau),\\nabla_{\\phi}\\mu(\\bar{\\phi)}]^{\\top}=$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n-\\left[\\begin{array}{c c}{{\\lambda\\nabla_{\\pi(\\cdot\\cdot\\vert s)}^{2}d^{2}(\\pi_{\\phi}(\\cdot\\vert s),\\pi(\\cdot\\vert s))}}&{{\\bf1}}\\\\ {{\\bf1}^{\\top}}&{{\\bf0}}\\end{array}\\right]^{-1}\\left[\\begin{array}{c}{{-\\nabla_{\\phi}^{\\top}Q_{\\tau}^{\\pi_{\\phi}}(s,\\cdot)+\\lambda\\nabla_{\\phi}^{\\top}\\nabla_{\\pi(\\cdot\\vert s)}d^{2}(\\pi_{\\phi}(\\cdot\\vert s),\\pi(\\cdot\\vert s))}}\\\\ {{\\bf0}}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\pi=\\mathcal{A}l g(\\pi_{\\phi},\\lambda,\\tau)$ . ", "page_idx": 21}, {"type": "text", "text": "Solve the equation, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla_{\\phi}^{\\top}\\mathcal{A}l g(\\pi_{\\phi},\\lambda,\\tau)(\\cdot|s)=\\left(M(s)^{-1}-\\frac{M(s)^{-1}\\mathbf{1}\\mathbf{1}^{\\top}M(s)^{-1}}{\\mathbf{1}^{\\top}M(s)^{-1}\\mathbf{1}}\\right)}\\\\ {\\left(\\nabla_{\\phi}^{\\top}Q_{\\tau}^{\\pi_{\\phi}}(s,\\cdot)-\\lambda\\nabla_{\\phi}^{\\top}\\nabla_{\\pi(\\cdot|s)}d^{2}(\\pi_{\\phi}(\\cdot|s),\\pi(\\cdot|s))\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where ${\\cal M}(s)\\,=\\,\\lambda\\nabla_{\\pi(\\cdot|s)}^{2}d^{2}(\\pi_{\\phi}(\\cdot|s),\\pi(\\cdot|s))$ . It is easy to show that $\\nabla_{\\pi(\\cdot|s)}^{2}d^{2}(\\pi_{\\phi}(\\cdot|s),\\pi(\\cdot|s))$ is non-singular for any $\\phi$ for any selected $d=d_{1}$ , $d=d_{2}$ , or $d=d_{3}$ . ", "page_idx": 21}, {"type": "text", "text": "From the policy gradient theorem in Lemma 3, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\phi}J_{\\tau}(\\pi_{\\theta_{\\tau}^{\\prime}})\\mathop{=}\\displaystyle\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\pi_{\\theta_{\\tau}^{\\prime}}},a\\sim\\pi_{\\theta_{\\tau}^{\\prime}}(\\cdot|s)}[\\nabla_{\\phi}\\ln\\pi_{\\theta_{\\tau}^{\\prime}}(a|s)A_{\\tau}^{\\pi_{\\theta_{\\tau}^{\\prime}}}(s,a)]|_{\\pi_{\\theta_{\\tau}^{\\prime}}=A l g(\\pi_{\\phi},\\lambda,\\tau)}}\\\\ &{\\qquad\\qquad\\quad\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\displaystyle=\\displaystyle\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\pi_{\\theta_{\\tau}^{\\prime}}},a\\sim\\pi_{\\theta_{\\tau}^{\\prime}}(\\cdot|s)}\\left[\\frac{\\nabla_{\\phi}\\pi_{\\theta_{\\tau}^{\\prime}}(a|s)}{\\pi_{\\theta_{\\tau}^{\\prime}}(a|s)}A_{\\tau}^{\\pi_{\\theta_{\\tau}^{\\prime}}}(s,a)\\right]|_{\\pi_{\\theta_{\\tau}^{\\prime}}=A l g(\\pi_{\\phi},\\lambda,\\tau)},}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\displaystyle=\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\pi_{\\theta_{\\tau}^{\\prime}}}}\\left[\\sum_{a\\in A}\\nabla_{\\phi}\\pi_{\\theta_{\\tau}^{\\prime}}(a|s)A_{\\tau}^{\\pi_{\\theta_{\\tau}^{\\prime}}}(s,a)\\right]|_{\\pi_{\\theta_{\\tau}^{\\prime}}=A l g(\\pi_{\\phi},\\lambda,\\tau)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\nabla_{\\phi}\\pi_{\\theta_{\\tau}^{\\prime}}(\\cdot|s)=\\nabla_{\\phi}\\mathcal{A}l g(\\pi_{\\phi},\\lambda,\\tau)(\\cdot|s)$ is shown in (19). ", "page_idx": 22}, {"type": "text", "text": "J.2 Proofs of Propositions 2 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Proofs of Propositions 2. First, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\nabla_{\\phi}J_{\\tau}(\\pi_{\\theta_{\\tau}^{\\prime}})=\\nabla_{\\phi}\\theta_{\\tau}^{\\prime}\\nabla_{\\theta_{\\tau}^{\\prime}}J_{\\tau}(\\pi_{\\theta_{\\tau}^{\\prime}})\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "From the policy gradient theorem in Lemma 3, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\nabla_{\\phi}J_{\\tau}(\\pi_{\\theta_{\\tau}^{\\prime}})=\\frac{1}{1-\\gamma}\\nabla_{\\phi}{\\theta_{\\tau}^{\\prime}}^{\\top}\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\pi_{\\theta_{\\tau}^{\\prime}}},a\\sim\\pi_{\\theta_{\\tau}^{\\prime}}(\\cdot|s)}\\left[\\nabla_{\\theta_{\\tau}^{\\prime}}\\ln\\pi_{\\theta_{\\tau}^{\\prime}}(a|s)A_{\\tau}^{\\pi_{\\theta_{\\tau}^{\\prime}}}(s,a)\\right]|_{\\theta_{\\tau}^{\\prime}=A l g(\\pi_{\\phi},\\lambda,\\tau)}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\nabla_{\\phi}J_{\\tau}(\\pi_{\\theta_{\\tau}^{\\prime}})=\\frac{1}{1-\\gamma}\\nabla_{\\phi}{\\theta_{\\tau}^{\\prime}}^{\\top}\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\pi_{\\theta_{\\tau}^{\\prime}}},a\\sim\\pi_{\\theta_{\\tau}^{\\prime}}(\\cdot|s)}\\left[\\frac{\\nabla_{\\theta_{\\tau}^{\\prime}}\\pi_{\\theta_{\\tau}^{\\prime}}(a|s)}{\\pi_{\\theta_{\\tau}^{\\prime}}(a|s)}A_{\\tau}^{\\pi_{\\theta_{\\tau}^{\\prime}}}(s,a)\\right]|_{\\theta_{\\tau}^{\\prime}=A l g(\\pi_{\\phi},\\lambda,\\tau)}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Next, we compute $\\nabla_{\\phi}\\theta_{\\tau}^{\\prime}$ , where ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\theta_{\\tau}^{\\prime}=\\mathcal{A}l g(\\pi_{\\phi},\\lambda,\\tau)\\triangleq\\operatorname{argmax}_{\\theta}\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\pi_{\\phi}},a\\sim\\pi_{\\phi}(\\cdot|s)}\\left[\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\phi}(a|s)}Q_{\\tau}^{\\pi_{\\phi}}(s,a)\\right]-\\lambda D_{\\tau}^{2}(\\pi_{\\phi},\\pi_{\\theta}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The optimization problem is equivalent to ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\theta_{\\tau}^{\\prime}\\triangleq\\operatorname*{argmax}_{\\theta}\\mathbb{E}_{_{s\\sim\\nu_{\\tau}^{\\pi_{\\phi}}}}\\left[\\int_{\\mathcal{A}}\\pi_{\\theta}(a\\vert s)Q_{\\tau}^{\\pi_{\\phi}}(s,a)d a-\\lambda d^{2}(\\pi_{\\phi}(\\cdot\\vert s),\\pi_{\\theta}(\\cdot\\vert s))\\right]}\\\\ &{\\quad=\\operatorname*{argmin}_{\\theta}\\mathbb{E}_{_{s\\sim\\nu_{\\tau}^{\\pi_{\\phi}}}}\\left[-\\int_{\\mathcal{A}}\\pi_{\\theta}(a\\vert s)Q_{\\tau}^{\\pi_{\\phi}}(s,a)d a+\\lambda d^{2}(\\pi_{\\phi}(\\cdot\\vert s),\\pi_{\\theta}(\\cdot\\vert s))\\right]}\\\\ &{\\quad=\\operatorname*{argmin}_{\\theta}\\sum_{\\tau\\leq\\delta}\\nu_{\\tau}^{\\pi_{\\phi}}(s)\\left(-\\int_{\\mathcal{A}}\\pi_{\\theta}(a\\vert s)Q_{\\tau}^{\\pi_{\\phi}}(s,a)d a+\\lambda d^{2}(\\pi_{\\phi}(\\cdot\\vert s),\\pi_{\\theta}(\\cdot\\vert s))\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Similar to the derivation from (15) to (16) with Assumption 2, we have that, when $\\theta=\\mathcal{A}l g(\\pi_{\\phi},\\lambda,\\tau)$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}\\left(-\\int_{\\cal A}\\pi_{\\theta}(a|s)Q_{\\tau}^{\\pi_{\\phi}}(s,a)d a+\\lambda d^{2}(\\pi_{\\phi}(\\cdot|s),\\pi_{\\theta}(\\cdot|s))\\right)=0.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{s\\in S}\\nabla_{\\phi}\\nu_{\\tau}^{\\pi_{\\phi}}(s)\\nabla_{\\theta}\\left(-\\int_{A}\\pi_{\\theta}(a|s)Q_{\\tau}^{\\pi_{\\phi}}(s,a)d a+\\lambda d^{2}(\\pi_{\\phi}(\\cdot|s),\\pi_{\\theta}(\\cdot|s))\\right)=0.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By using implicit differentiation, if the matr $\\begin{array}{r}{\\mathrm{ix}\\,\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\pi_{\\phi}}}\\left[-\\int_{\\cal A}\\nabla_{\\theta}^{2}\\pi_{\\theta}(a|s)Q_{\\tau}^{\\pi_{\\phi}}(s,a)d a+\\lambda\\nabla_{\\theta}^{2}d^{2}(\\pi_{\\phi}(\\cdot|s),\\pi_{\\theta}(\\cdot|s))\\right]}\\end{array}$ is invertible, i.e., $\\begin{array}{r}{\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\pi_{\\phi}}}\\left[-\\int_{A}\\pi_{\\theta}(a|s)Q_{\\tau}^{\\pi_{\\phi}}(s,a)d a+\\lambda d^{2}(\\pi_{\\phi}(\\cdot|s),\\pi_{\\theta}(\\cdot|s))\\right]}\\end{array}$ is strongly convex at $\\theta=\\theta_{\\tau}^{\\prime}$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\phi}^{\\top}\\theta_{\\tau}^{\\prime}=-\\left(\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\pi_{\\phi}}}\\left[-\\int_{A}\\nabla_{\\theta}^{2}\\pi_{\\theta}(a|s)Q_{\\tau}^{\\pi_{\\phi}}(s,a)d a+\\lambda\\nabla_{\\theta}^{2}d^{2}(\\pi_{\\phi}(\\cdot|s),\\pi_{\\theta}(\\cdot|s))\\right]\\right)^{-1}}\\\\ &{\\,\\,\\left(-\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\pi_{\\phi}}}\\left[\\int_{A}\\nabla_{\\theta}\\pi_{\\theta}(a|s)\\nabla_{\\phi}^{\\top}Q_{\\tau}^{\\pi_{\\phi}}(s,a)d a\\right]+\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\pi_{\\phi}}}\\left[\\lambda\\nabla_{\\phi}^{\\top}\\nabla_{\\theta}d^{2}(\\pi_{\\phi}(\\cdot|s),\\pi_{\\theta}(\\cdot|s))\\right]\\right.+}\\\\ &{\\,\\,\\left.\\,\\sum_{s\\in S}\\nabla_{\\phi}\\nu_{\\tau}^{\\pi_{\\phi}}(s)\\nabla_{\\theta}\\left(-\\int_{A}\\pi_{\\theta}(a|s)Q_{\\tau}^{\\pi_{\\phi}}(s,a)d a+\\lambda d^{2}(\\pi_{\\phi}(\\cdot|s),\\pi_{\\theta}(\\cdot|s))\\right)\\right)\\mid_{\\theta=\\theta_{\\tau}^{\\prime}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This is equivalent to ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\phi}^{\\top}\\theta_{\\tau}^{\\prime}=-\\left(\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\pi_{\\phi}},a\\sim\\pi_{\\phi}(\\cdot\\vert s)}\\left[-\\frac{\\nabla_{\\theta}^{2}\\pi_{\\theta}(a\\vert s)}{\\pi_{\\phi}(a\\vert s)}Q_{\\tau}^{\\pi_{\\phi}}(s,a)+\\lambda\\nabla_{\\theta}^{2}d^{2}(\\pi_{\\phi}(\\cdot\\vert s),\\pi_{\\theta}(\\cdot\\vert s))\\right]\\right)^{-1}}\\\\ &{\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\pi_{\\phi}},a\\sim\\pi_{\\phi}(\\cdot\\vert s)}\\left[-\\frac{\\nabla_{\\theta}\\pi_{\\theta}(a\\vert s)}{\\pi_{\\phi}(a\\vert s)}\\nabla_{\\phi}^{\\top}Q_{\\tau}^{\\pi_{\\phi}}(s,a)+\\lambda\\nabla_{\\phi}^{\\top}\\nabla_{\\theta}d^{2}(\\pi_{\\phi}(\\cdot\\vert s),\\pi_{\\theta}(\\cdot\\vert s))\\right]|_{\\theta=\\theta_{\\tau}^{\\prime}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "J.3 Proofs of hypergradient of the algorithm in Section G ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Deviation of $(I O)$ . A $\\begin{array}{r}{\\colon\\theta_{\\tau}^{\\prime}=\\underset{a}{\\mathrm{argmin}}-\\frac{1}{\\lambda}\\theta^{\\top}\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\pi_{\\phi}},a\\sim\\pi_{\\phi}(\\cdot\\vert s)}[\\frac{\\nabla_{\\phi}\\pi_{\\phi}(a\\vert s)}{\\pi_{\\phi}(a\\vert s)}A_{\\tau}^{\\pi_{\\phi}}(s,a)]+D_{\\tau}^{2}(\\pi_{\\phi},\\pi_{\\theta})}\\end{array}$ the implicit differentiation theorem in bilevel optimization analysis, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla_{\\phi}^{\\top}\\theta_{\\tau}^{\\prime}=-\\nabla_{\\theta}^{2}\\left[-\\displaystyle\\frac{1}{\\lambda}\\theta^{\\top}\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\pi_{\\phi}},a\\sim\\pi_{\\phi}(\\cdot|s)}[\\frac{\\nabla_{\\phi}\\pi_{\\phi}(a|s)}{\\pi_{\\phi}(a|s)}Q_{\\tau}^{\\pi_{\\phi}}(s,a)]+D_{\\tau}^{2}(\\pi_{\\phi},\\pi_{\\theta})\\right]^{-1}}\\\\ {\\nabla_{\\phi}\\nabla_{\\theta}\\left[-\\displaystyle\\frac{1}{\\lambda}\\theta^{\\top}\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\pi_{\\phi}},a\\sim\\pi_{\\phi}(\\cdot|s)}[\\frac{\\nabla_{\\phi}\\pi_{\\phi}(a|s)}{\\pi_{\\phi}(a|s)}Q_{\\tau}^{\\pi_{\\phi}}(s,a)]+D_{\\tau}^{2}(\\pi_{\\phi},\\pi_{\\theta})\\right]|_{\\theta=\\theta_{\\tau}^{\\prime}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Also, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}^{2}(\\frac{1}{\\lambda}\\theta^{\\top}\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\pi_{\\phi}},a\\sim\\pi_{\\phi}(\\cdot|s)}[\\frac{\\nabla_{\\phi}\\pi_{\\phi}(a|s)}{\\pi_{\\phi}(a|s)}Q_{\\tau}^{\\pi_{\\phi}}(s,a)])=0,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\nabla_{\\phi}\\nabla_{\\theta}(\\frac{1}{\\lambda}\\theta^{\\top}\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\pi_{\\phi}},a\\sim\\pi_{\\phi}(\\cdot|s)}[\\frac{\\nabla_{\\phi}\\pi_{\\phi}(a|s)}{\\pi_{\\phi}(a|s)}Q_{\\tau}^{\\pi_{\\phi}}(s,a)])}\\\\ &{=_{s\\sim\\nu_{\\tau}^{\\pi_{\\phi}}a\\sim\\pi_{\\phi}(\\cdot|s)}[\\frac{1}{\\lambda}\\frac{\\nabla_{\\phi}\\pi_{\\phi}(a|s)}{\\pi_{\\phi}(a|s)}\\nabla_{\\phi}^{\\top}Q_{\\tau}^{\\pi_{\\phi}}(s,a)+\\frac{1}{\\lambda}\\frac{\\nabla_{\\phi}^{2}\\pi_{\\phi}(a|s)}{\\pi_{\\phi}(a|s)}Q_{\\tau}^{\\pi_{\\phi}}(s,a)].}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then, we can get $\\nabla_{\\phi}^{\\top}\\theta_{\\tau}^{\\prime}$ . ", "page_idx": 23}, {"type": "text", "text": "K Proofs of convergence when $D_{\\tau}=D_{\\tau,1}$ ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "K.1 Gradients of $\\nabla_{\\phi}J_{\\tau}(\\pi_{\\theta_{\\tau}^{\\prime}})$ when $D_{\\tau}=D_{\\tau,1}$ ", "page_idx": 23}, {"type": "text", "text": "From Proposition 1, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\phi}J_{\\tau}\\big(\\pi_{\\theta_{\\tau}^{\\prime}}\\big)=\\displaystyle\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\pi_{\\theta_{\\tau}^{\\prime}}}}\\left[\\displaystyle\\sum_{a\\in A}\\nabla_{\\phi}\\pi_{\\theta_{\\tau}^{\\prime}}(a|s)A_{\\tau}^{\\pi_{\\theta_{\\tau}^{\\prime}}}(s,a)\\right]}\\\\ &{\\qquad\\qquad=\\displaystyle\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\pi_{\\theta_{\\tau}^{\\prime}}}}\\left[\\nabla_{\\phi}\\pi_{\\theta_{\\tau}^{\\prime}}(\\cdot|s)\\cdot A_{\\tau}^{\\pi_{\\theta_{\\tau}^{\\prime}}}(s,\\cdot)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{T}_{\\phi}^{\\top}\\pi_{\\theta_{\\tau}^{\\prime}}(\\cdot|s)=\\left(M(s)^{-1}-\\frac{M(s)^{-1}\\mathbf{1}\\mathbf{1}^{\\top}M(s)^{-1}}{\\mathbf{1}^{\\top}M(s)^{-1}\\mathbf{1}}\\right)\\left(\\nabla_{\\phi}^{\\top}Q_{\\tau}^{\\pi_{\\phi}}(s,\\cdot)-\\lambda\\nabla_{\\phi}^{\\top}\\nabla_{\\pi(\\cdot|s)}d_{1}^{2}(\\pi_{\\phi},\\pi,s)\\right)|_{\\pi=\\pi_{\\theta}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where ", "page_idx": 23}, {"type": "equation", "text": "$$\nM(s)=\\lambda\\nabla_{\\pi(\\cdot|s)}^{2}d_{1}^{2}(\\pi_{\\phi},\\pi,s)=\\lambda\\left[\\begin{array}{c c c c}{\\frac{\\pi_{\\phi}(a_{1}|s)}{\\pi_{\\theta_{\\tau}^{\\prime}}(a_{1}|s)^{2}}}&&&\\\\ &{\\ddots}&&\\\\ &&&{\\frac{\\pi_{\\phi}(a_{n}|s)}{\\pi_{\\theta_{\\tau}^{\\prime}}(a_{n}|s)^{2}}}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then, ", "page_idx": 23}, {"type": "equation", "text": "$$\nM(s)^{-1}=\\frac{1}{\\lambda}\\left[\\begin{array}{c c c c}{\\frac{\\pi_{\\theta_{\\tau}^{\\prime}}(a_{1}|s)^{2}}{\\pi_{\\phi}(a_{1}|s)}}&&&\\\\ &{\\ddots}&&\\\\ &&&{\\frac{\\pi_{\\theta_{\\tau}^{\\prime}}(a_{n}|s)^{2}}{\\pi_{\\phi}(a_{n}|s)}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{M(s)^{-1}\\mathbf{1}\\,\\mathbf{1}^{\\top}M(s)^{-1}}{\\mathbf{1}^{\\top}M(s)^{-1}\\mathbf{1}}=\\frac{1}{\\lambda\\sum_{a\\in A}\\frac{\\pi_{\\theta_{r}^{\\prime}}(a|s)^{2}}{\\pi_{\\phi}(a|s)}}\\left[\\begin{array}{c}{\\frac{\\pi_{\\theta_{r}^{\\prime}}(a_{1}|s)^{2}}{\\pi_{\\phi}(a_{1}|s)}}\\\\ {\\vdots}\\\\ {\\frac{\\pi_{\\theta_{r}^{\\prime}}(a_{n}|s)^{2}}{\\pi_{\\phi}(a_{n}|s)}}\\end{array}\\right]\\left[\\frac{\\pi_{\\theta_{r}^{\\prime}}(a_{1}|s)^{2}}{\\pi_{\\phi}(a_{1}|s)}\\quad\\cdot\\cdot\\cdot\\quad\\frac{\\pi_{\\theta_{r}^{\\prime}}(a_{n}|s)^{2}}{\\pi_{\\phi}(a_{n}|s)}\\right].\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Also, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla_{\\phi}^{\\top}\\nabla_{\\pi(\\cdot\\mid s)}d_{1}^{2}(\\pi_{\\phi},\\pi,s)\\vert_{\\pi=\\pi_{\\theta_{\\tau}^{\\prime}}}=\\nabla_{\\phi}^{\\top}\\left[\\begin{array}{c}{-\\frac{\\pi_{\\phi}(a_{1}\\mid s)}{\\pi_{\\theta_{\\tau}^{\\prime}}(a_{1}\\mid s)}}\\\\ {\\vdots}\\\\ {-\\frac{\\pi_{\\phi}(a_{n}\\mid s)}{\\pi_{\\theta_{\\tau}^{\\prime}}(a_{n}\\mid s)}}\\end{array}\\right]=\\left[\\begin{array}{c}{-\\frac{\\nabla_{\\phi}^{\\top}\\pi_{\\phi}(a_{1}\\mid s)}{\\pi_{\\theta_{\\tau}^{\\prime}}(a_{1}\\mid s)}}\\\\ {\\vdots}\\\\ {-\\frac{\\nabla_{\\phi}^{\\top}\\pi_{\\phi}(a_{n}\\mid s)}{\\pi_{\\theta_{\\tau}^{\\prime}}(a_{n}\\mid s)}}\\end{array}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then, plugging these equations into (20), we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma_{\\phi}^{\\top}J_{\\tau}(\\pi_{\\theta_{r}^{\\tau}})=\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim\\nu_{\\phi^{\\tau}}^{\\tau_{\\phi}}}\\left[A_{\\tau}^{\\pi_{\\theta_{r}^{\\tau}}}(s,\\cdot)^{\\top}\\nabla_{\\phi}^{\\top}\\pi_{\\theta_{r}^{\\tau}}(\\cdot|s)\\right],}\\\\ &{=\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim\\nu_{\\phi^{\\tau}}^{\\tau_{\\phi}}}\\left[A_{\\tau}^{\\pi_{\\theta_{r}^{\\tau}}}(s,\\cdot)^{\\top}\\left(M(s)^{-1}-\\frac{M(s)^{-1}\\mathbf{1}\\mathbf{1}^{\\top}M(s)^{-1}}{\\mathbf{1}^{\\top}M(s)^{-1}\\mathbf{1}}\\right)\\left[\\begin{array}{c}{\\frac{1}{\\lambda}\\nabla_{\\phi}^{\\top}Q_{\\tau^{\\tau}}^{\\pi_{\\theta}}(s,a_{1})+\\frac{\\nabla_{\\theta_{r}^{\\tau}}^{\\top}\\pi_{\\theta}(a_{1})}{\\pi_{\\theta_{r}^{\\tau}}(a_{1}|s)}}\\\\ {\\vdots}\\\\ {\\frac{1}{\\lambda}\\nabla_{\\phi}^{\\top}Q_{\\tau^{\\tau}}^{\\pi_{\\theta}}(s,a_{n})+\\frac{\\nabla_{\\theta_{r}^{\\tau}}^{\\top}\\pi_{\\theta_{r}^{\\tau}}(a_{n})}{\\pi_{\\theta_{r}^{\\tau}}(a_{1}|s)^{2}}}\\end{array}\\right]\\right.}\\\\ &{=\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim\\nu_{\\phi}^{\\tau_{\\phi}}}\\left[\\left(\\left[\\left(A_{\\tau}^{\\pi_{\\theta^{\\tau}}}(s,a_{1})-c_{\\tau}(s))\\frac{\\tau_{\\theta_{r}^{\\tau}}(a_{1})s^{2}}{\\tau_{\\phi}(a_{1}|s)}\\right.\\right.\\right.\\left.\\cdot\\left.\\left.\\left.\\left.\\left.\\left.\\left(A_{\\tau}^{\\pi_{\\theta^{\\tau}}}(s,a_{n})-c_{\\tau}(s))\\frac{\\tau_{\\theta_{r}^{\\tau}}(a_{1}|s)^{2}}{\\tau_{\\phi}(a_{1}|s)}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where ", "page_idx": 24}, {"type": "equation", "text": "$$\nc_{\\tau}(s)=\\frac{\\sum_{a\\in\\cal{A}}{\\cal A}_{\\tau}^{\\pi_{\\theta_{\\tau}^{\\prime}}}(s,a)\\frac{\\pi_{\\theta_{\\tau}^{\\prime}}(a|s)^{2}}{\\pi_{\\phi}(a|s)}}{\\sum_{a\\in\\cal{A}}\\frac{\\pi_{\\theta_{\\tau}^{\\prime}}(a|s)^{2}}{\\pi_{\\phi}(a|s)}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then, we simplify the computation of $\\nabla_{\\phi}^{\\top}J_{\\tau}(\\pi_{\\theta_{\\tau}^{\\prime}})$ , we have $\\nabla_{\\phi}^{\\top}J_{\\tau}(\\pi_{\\theta_{\\tau}^{\\prime}})=$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\pi_{\\theta}^{\\prime}}}\\left[\\displaystyle\\sum_{a\\in A}(A_{\\tau}^{\\pi_{\\theta_{\\tau}^{\\prime}}}(s,a)-c_{\\tau}(s))\\frac{\\pi_{\\theta_{\\tau}^{\\prime}}(a|s)^{2}}{\\pi_{\\phi}(a|s)}(\\frac{1}{\\lambda}\\nabla_{\\phi}^{\\top}Q_{\\tau}^{\\pi_{\\phi}}(s,a)+\\frac{\\nabla_{\\phi}^{\\top}\\pi_{\\phi}(a|s)}{\\pi_{\\theta_{\\tau}^{\\prime}}(a|s)})\\right]}\\\\ &{=\\displaystyle\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\pi_{\\theta}^{\\prime}}}\\left[\\displaystyle\\sum_{a\\in A}\\pi_{\\theta_{\\tau}^{\\prime}}(a|s)(A_{\\tau}^{\\pi_{\\theta^{\\prime}}}(s,a)-c_{\\tau}(s))(\\frac{\\pi_{\\theta_{\\tau}^{\\prime}}(a|s)}{\\lambda\\pi_{\\phi}(a|s)}\\nabla_{\\phi}^{\\top}Q_{\\tau}^{\\pi_{\\phi}}(s,a)+\\frac{\\nabla_{\\phi}^{\\top}\\pi_{\\phi}(a|s)}{\\pi_{\\phi}(a|s)})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "When the tabular policy is the softmax policy, we have $\\begin{array}{r}{\\hat{\\pi}_{\\phi}(a|s)=\\frac{\\exp(\\phi(s,a))}{\\sum_{a^{\\prime}\\in\\mathcal{A}}\\exp(\\phi(s,a^{\\prime}))}}\\end{array}$ , then ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\nabla_{\\phi}^{\\top}\\hat{\\pi}_{\\phi}(a\\mid s)}{\\hat{\\pi}_{\\phi}(a\\mid s)}=\\nabla_{\\phi}^{\\top}\\ln\\hat{\\pi}_{\\phi}(a\\mid s)=\\nabla_{\\phi}^{\\top}\\phi(s,a)-\\nabla_{\\phi}^{\\top}\\ln\\displaystyle\\sum_{a^{\\prime}\\in A}\\exp\\left(\\phi\\left(s,a^{\\prime}\\right)\\right)}\\\\ &{=\\mathbf{1}(s,a)-\\hat{\\pi}_{\\phi}(\\cdot\\vert s).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Here, $\\mathbf{1}(s^{\\prime},a^{\\prime})$ denote the column vector where the element is 1 if $s=s^{\\prime}$ and $a=a^{\\prime}$ , otherwise is $0$ , for each pair $(s,a)\\in\\mathcal{S}\\times\\mathcal{A};\\hat{\\pi}_{\\phi}(\\cdot|s^{\\prime})$ is the column vector, where the element is $\\hat{\\pi}_{\\phi}(a|s^{\\prime})$ if $s=s^{\\prime}$ , 0 if $s\\neq s^{\\prime}$ , for each pair $(s.a)\\in S\\times A$ . ", "page_idx": 24}, {"type": "text", "text": "So, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\nabla_{\\phi}^{\\top}J_{\\tau}(\\hat{\\boldsymbol{\\theta}}_{\\varepsilon})=\\frac{1}{1-\\gamma}\\mathbb{E}_{\\exp^{\\tau}}\\left[\\sum_{i,j=1\\atop i\\neq j}^{\\infty}\\left[\\sum_{i,j\\neq i,j}(A_{\\mathrm{eff}}|s)(A_{\\tau}^{\\dagger\\mu_{i}\\mu_{j}}(s,a)-c_{\\tau}(s))\\right.\\right.}}\\\\ &{}&{\\left.\\left.\\left(\\frac{\\hat{\\mathcal{F}}_{\\hat{\\boldsymbol{\\theta}}_{\\varepsilon}}}{\\lambda\\hat{\\mathcal{F}}_{\\hat{\\boldsymbol{\\theta}}_{\\varepsilon}}}(a|s)\\right)\\nabla_{\\phi}^{\\top}Q_{\\tau}^{\\hat{\\boldsymbol{\\theta}}_{\\varepsilon}}(s,a)+\\frac{\\nabla_{\\phi}^{\\top}\\hat{\\mathcal{F}}_{\\boldsymbol{\\theta}_{\\varepsilon}}\\left(a|s\\right)}{\\hat{\\mathcal{F}}_{\\hat{\\boldsymbol{\\theta}}_{\\varepsilon}}\\left(a|s\\right)}\\right]\\right\\}}\\\\ &{}&{=\\frac{1}{1-\\gamma}\\mathbb{E}_{\\exp^{\\tau}}\\left[\\sum_{i,j=1\\atop i\\neq j}^{\\infty}\\left[\\sum_{i,j\\neq i,j}(a|s)(A_{\\tau}^{\\dagger\\mu_{i}\\mu_{j}}(s,a)-c_{\\tau}(s))\\right.\\right.}\\\\ &{}&{\\left.\\left.\\left(\\frac{\\hat{\\mathcal{F}}_{\\hat{\\boldsymbol{\\theta}}_{\\varepsilon}}}{\\lambda\\hat{\\mathcal{F}}_{\\hat{\\boldsymbol{\\theta}}_{\\varepsilon}}}(a|s)\\nabla_{\\phi}^{\\top}Q_{\\tau}^{\\hat{\\boldsymbol{\\theta}}_{\\varepsilon}}(s,a)+\\mathbb{1}^{\\top}(s,a)-\\hat{\\mathcal{F}}_{\\hat{\\boldsymbol{\\theta}}_{\\varepsilon}}(\\cdot|s)^{\\top}\\right)\\right]}\\\\ &{}&{=\\frac{1}{1-\\gamma}\\mathbb{E}_{\\exp^{\\tau},\\hat{\\mathcal{F}}_{\\boldsymbol{\\theta}_{\\varepsilon}\\in\\hat{\\mathcal{F}}_{\\boldsymbol{\\theta}_{\\varepsilon}}}}\\left[\\left(A_{\\tau}^{\\dagger\\mu_{j}}(s,a)-c_{\\tau}(s)\\right)\\right.}\\\\ &{}&{\\left.\\left(\\frac{\\hat{\\mathcal{F}}_{\\hat{\\boldsymbol{\\theta}}_{\\varepsilon}}}{\\lambda\\hat{\\mathcal{F}}_{\\hat{\\boldsymbol{\\theta}}_{\\varepsilon}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "K.2 Convergence guarantee when $D_{\\tau}=D_{\\tau,1}$ ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "K.2.1 Auxiliary lemmas ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Lemma 5. Suppose that Assumption 2 holds. Let $\\pi_{\\theta_{\\tau}^{\\prime}}=\\mathcal{A}l g(\\pi_{\\phi},\\lambda,\\tau)$ where $D_{\\tau}=D_{\\tau,1;}$ , for any $s\\in S$ and $a\\in{\\mathcal{A}}$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{\\lambda}{\\lambda+\\operatorname*{max}_{s,a}\\lvert A_{\\tau}^{\\pi_{\\phi}}(s,a)\\rvert}\\leq\\frac{\\pi_{\\theta_{\\tau}^{\\prime}}(a\\lvert s)}{\\pi_{\\phi}(a\\lvert s)}\\leq\\frac{\\lambda}{\\lambda-\\operatorname*{max}_{s,a}\\lvert A_{\\tau}^{\\pi_{\\phi}}(s,a)\\rvert}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. From (16), when $D_{\\tau}=D_{\\tau,1}$ , we have $\\pi_{\\theta_{\\tau}^{\\prime}}=\\mathcal{A}l g(\\pi_{\\phi},\\lambda,\\tau)$ and ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\pi_{\\theta_{\\tau}^{\\prime}}(\\cdot|s)=\\operatorname*{argmax}_{(\\cdot|s)}\\sum_{a\\in\\mathcal{A}}\\pi(a|s)Q_{\\tau}^{\\pi_{\\phi}}(s,a)-\\lambda d_{1}^{2}(\\pi_{\\phi},\\pi,s),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For any $s\\in S$ , the Lagrangian of the above maximization problem is ", "page_idx": 25}, {"type": "equation", "text": "$$\n-\\sum_{a\\in\\mathcal{A}}\\pi(a|s)Q_{\\tau}^{\\pi_{\\phi}}(s,a)+\\lambda d^{2}(\\pi_{\\phi}(\\cdot|s),\\pi(\\cdot|s))+\\mu(s)(\\sum_{a\\in\\mathcal{A}}\\pi(a|s)-1),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\mu$ is the Lagrangian multiplier. The optimality condition of $\\pi(\\cdot|s)$ is that, ", "page_idx": 25}, {"type": "equation", "text": "$$\n-Q_{\\tau}^{\\pi_{\\phi}}(s,\\cdot)+\\lambda\\nabla_{\\pi(\\cdot|s)}d_{1}^{2}(\\pi_{\\phi},\\pi,s)+\\mu(s)[1,\\cdot\\cdot\\cdot\\cdot,1]^{\\top}=0.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Solve the equation, ", "page_idx": 25}, {"type": "equation", "text": "$$\n-Q_{\\tau}^{\\pi_{\\phi}}(s,a)-\\lambda\\frac{\\pi_{\\phi}(a|s)}{\\pi_{\\theta_{\\tau}^{\\prime}}(a|s)}+\\mu(s)=0.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Let $\\mu_{1}(s)=-V_{\\tau}^{\\pi_{\\phi}}(s)+\\mu(s)$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n-A_{\\tau}^{\\pi_{\\phi}}(s,a)-\\lambda\\frac{\\pi_{\\phi}(a\\vert s)}{\\pi_{\\theta_{\\tau}^{\\prime}}(a\\vert s)}+\\mu_{1}(s)=0.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then, ", "page_idx": 25}, {"type": "equation", "text": "$$\n-\\pi_{\\theta_{\\tau}^{\\prime}}(a|s)A_{\\tau}^{\\pi_{\\phi}}(s,a)-\\lambda\\pi_{\\phi}(a|s)+\\pi_{\\theta_{\\tau}^{\\prime}}(a|s)\\mu_{1}(s)=0.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We derive the summation of all $a\\in A$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sum_{a\\in A}-\\pi_{\\theta_{\\tau}^{\\prime}}(a|s)A_{\\tau}^{\\pi_{\\phi}}(s,a)-\\lambda+\\mu_{1}(s)=0.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mu_{1}(s)=\\sum_{a\\in\\mathcal{A}}\\pi_{\\theta_{\\tau}^{\\prime}}(a|s)A_{\\tau}^{\\pi_{\\phi}}(s,a)+\\lambda.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "From (26), we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\pi_{\\phi}(a|s)}{\\pi_{\\theta_{\\tau}^{\\prime}}(a|s)}=\\!\\frac{\\mu_{1}(s)-A_{\\tau}^{\\pi_{\\phi}}(s,a)}{\\lambda}\\qquad\\qquad\\qquad\\qquad\\quad}\\\\ {\\quad=\\!\\frac{\\displaystyle\\lambda+\\sum_{a^{\\prime}\\in A}\\pi_{\\theta_{\\tau}^{\\prime}}(a^{\\prime}|s)A_{\\tau}^{\\pi_{\\phi}}(s,a^{\\prime})-A_{\\tau}^{\\pi_{\\phi}}(s,a)}{\\lambda}\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "So, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\frac{\\pi_{\\theta_{\\tau}^{\\prime}}(a|s)}{\\pi_{\\phi}(a|s)}=\\frac{\\lambda}{\\lambda+\\sum_{a^{\\prime}\\in\\mathcal{A}}\\pi_{\\theta_{\\tau}^{\\prime}}(a^{\\prime}|s)A_{\\tau}^{\\pi_{\\phi}}(s,a^{\\prime})-A_{\\tau}^{\\pi_{\\phi}}(s,a)},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "then ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\frac{\\lambda}{\\lambda+\\operatorname*{max}_{s,a}\\lvert A_{\\tau}^{\\pi_{\\phi}}(s,a)\\rvert}\\leq\\frac{\\pi_{\\theta_{\\tau}^{\\prime}}(a\\lvert s)}{\\pi_{\\phi}(a\\lvert s)}\\leq\\frac{\\lambda}{\\lambda-\\operatorname*{max}_{s,a}\\lvert A_{\\tau}^{\\pi_{\\phi}}(s,a)\\rvert}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Lemma 6. Suppose that Assumption 2 holds. Let $\\pi_{\\theta_{\\tau}^{\\prime}}=\\mathcal{A}l g(\\pi_{\\phi},\\lambda,\\tau)$ where $D_{\\tau}=D_{\\tau,1}$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\|\\nabla_{\\phi}J_{\\tau}(\\pi_{\\theta_{\\tau}^{\\prime}})\\|\\leq\\frac{\\operatorname*{max}_{s,a}\\lvert A_{\\tau}^{\\pi_{\\theta_{\\tau}^{\\prime}}}(s,a)\\rvert}{1-\\gamma}(\\frac{\\operatorname*{max}_{s,a}\\lvert A_{\\tau}^{\\pi_{\\theta_{\\tau}^{\\prime}}}(s,a)\\rvert}{\\lambda-\\operatorname*{max}_{s,a}\\lvert A_{\\tau}^{\\pi_{\\phi}}(s,a)\\rvert}\\frac{\\gamma}{1-\\gamma}+2).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. As shown in (25), ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\phi}^{\\top}J_{\\tau}(\\hat{\\pi}_{\\boldsymbol{q}_{\\tau}^{\\prime}})=\\displaystyle\\frac{1}{1-\\gamma}\\mathbb{E}_{\\mathit{s e n v}_{\\tau}^{*}\\left(\\boldsymbol{q}\\right)}\\left[\\sum_{\\alpha\\in A}\\hat{\\pi}_{\\boldsymbol{q}_{\\tau}^{\\prime}}(a|s)(A_{\\tau}^{\\hat{\\pi}_{\\boldsymbol{q}_{\\tau}^{\\prime}}}(s,a)-c_{\\tau}(s))\\right.}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\left.(\\frac{\\hat{\\pi}_{\\boldsymbol{q}_{\\tau}^{\\prime}}(a|s)}{\\lambda\\hat{\\pi}_{\\boldsymbol{\\phi}}(a|s)}\\nabla_{\\phi}^{\\top}Q_{\\tau}^{\\hat{\\pi}_{\\boldsymbol{\\phi}}}(s,a)+\\mathbf{1}^{\\top}(s,a)-\\hat{\\pi}_{\\boldsymbol{\\phi}}(\\cdot|s)^{\\top})\\right]}\\\\ &{=\\displaystyle\\frac{1}{1-\\gamma}\\sum_{s\\in S}\\sum_{a\\in A}\\nu_{\\tau}^{\\hat{\\pi}_{\\boldsymbol{q}_{\\tau}^{\\prime}}}(s)\\hat{\\pi}_{\\boldsymbol{q}_{\\tau}^{\\prime}}(a|s)(A_{\\tau}^{\\hat{\\pi}_{\\boldsymbol{q}_{\\tau}^{\\prime}}}(s,a)-c_{\\tau}(s))}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\left.(\\frac{\\hat{\\pi}_{\\boldsymbol{q}_{\\tau}^{\\prime}}(a|s)}{\\lambda\\hat{\\pi}_{\\boldsymbol{\\phi}}(a|s)}\\nabla_{\\phi}^{\\top}Q_{\\tau}^{\\hat{\\pi}_{\\boldsymbol{\\phi}}}(s,a)+\\mathbf{1}^{\\top}(s,a)-\\hat{\\pi}_{\\boldsymbol{\\phi}}(\\cdot|s)^{\\top}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Since $\\begin{array}{r}{\\sum_{s\\in S}\\nu_{\\tau}^{\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}}(s)=1}\\end{array}$ and $\\begin{array}{r}{\\sum_{a\\in\\mathcal{A}}\\hat{\\pi}_{\\boldsymbol{\\theta}_{\\tau}^{\\prime}}(a|s)=1}\\end{array}$ for all $s\\in S$ , we have $\\|\\nabla_{\\phi}J_{\\tau}(\\hat{\\pi}_{\\boldsymbol{\\theta}_{\\tau}^{\\prime}})\\|\\leq$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\frac{1}{1-\\gamma}\\operatorname*{max}_{a,s}\\Vert(A_{\\tau}^{\\hat{\\pi}_{\\theta^{\\prime}}}(s,a)-c_{\\tau}(s))(\\frac{\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)}{\\lambda\\hat{\\pi}_{\\phi}(a|s)}\\nabla_{\\phi}^{\\top}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)+\\mathbf{1}^{\\top}(s,a)-\\hat{\\pi}_{\\phi}(\\cdot|s)^{\\top})\\Vert.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "From (23), for any $s\\in S$ and $a\\in A$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|A_{\\tau}^{\\hat{\\pi}_{\\boldsymbol{\\theta}_{\\tau}^{\\prime}}}(s,a)-c_{\\tau}(s)|\\leq\\operatorname*{max}_{s,a}|A_{\\tau}^{\\hat{\\pi}_{\\boldsymbol{\\theta}_{\\tau}^{\\prime}}}(s,a)|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Also, for any $s\\in S$ and $a\\in A$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\lVert\\mathbf{1}(s,a)-\\hat{\\pi}_{\\phi}(\\cdot|s))\\rVert\\leq1+\\lVert\\hat{\\pi}(\\cdot|s)\\rVert\\leq2.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "From Lemma 5, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\frac{\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)}{\\lambda\\hat{\\pi}_{\\phi}(a|s)}\\leq\\frac{1}{\\lambda-\\operatorname*{max}_{s,a}\\lvert A_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)\\rvert}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "From the computation of $\\nabla_{\\phi}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)$ shown in (5) of Appendix C, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\nabla_{\\phi(s^{\\prime},a^{\\prime})}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)|=|\\frac{\\gamma}{1-\\gamma}\\cdot\\sigma_{\\tau,\\hat{\\pi}_{\\phi}}^{(s,a)}(s^{\\prime})\\hat{\\pi}_{\\phi}(a^{\\prime}|s^{\\prime})A_{\\tau}^{\\hat{\\pi}_{\\phi}}\\left(s,a\\right)|}\\\\ &{\\qquad\\qquad\\qquad\\leq\\displaystyle\\frac{\\gamma}{1-\\gamma}\\sigma_{\\tau,\\hat{\\pi}_{\\phi}}^{(s,a)}(s^{\\prime})\\hat{\\pi}_{\\phi}(a^{\\prime}|s^{\\prime})|\\operatorname*{max}_{a,s}A_{\\tau}^{\\hat{\\pi}_{\\phi}}\\left(s,a\\right)|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Also, since $\\begin{array}{r}{\\sum_{a\\in\\mathcal{A},s\\in\\mathcal{S}}\\sigma_{\\tau,\\hat{\\pi}_{\\phi}}^{(s,a)}(s^{\\prime})\\hat{\\pi}_{\\phi}(a^{\\prime}|s^{\\prime})=1}\\end{array}$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\Vert\\nabla_{\\phi}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)\\Vert\\leq\\frac{\\gamma}{1-\\gamma}\\operatorname*{max}_{a,s}|A_{\\tau}^{\\hat{\\pi}_{\\phi}}\\left(s,a\\right)|.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Therefore, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\Vert\\nabla_{\\phi}J_{\\tau}(\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}})\\Vert\\leq\\frac{\\operatorname*{max}_{s,a}\\vert A_{\\tau}^{\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}}(s,a)\\vert}{1-\\gamma}(\\frac{\\operatorname*{max}_{s,a}\\vert A_{\\tau}^{\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}}(s,a)\\vert}{\\lambda-\\operatorname*{max}_{s,a}\\vert A_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)\\vert}\\frac{\\gamma}{1-\\gamma}+2).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Lemma 7. Suppose that Assumption 2 holds. Let $\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}=\\mathcal{A}l g(\\hat{\\pi}_{\\phi},\\lambda,\\tau)$ where $D_{\\tau}=D_{\\tau,1;}$ , for any $s\\in S$ we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{\\iota\\in A}\\|\\nabla_{\\phi}\\hat{\\pi}_{\\theta_{\\iota}^{\\prime}}(a|s)\\|\\leq\\frac{1}{\\lambda-\\operatorname*{max}_{s,a}\\lvert A_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)\\rvert}(\\frac{\\gamma\\operatorname*{max}_{a,s}\\lvert A_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)\\rvert}{1-\\gamma}+2(\\lambda+\\operatorname*{max}_{a,s}\\lvert A_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)\\rvert))\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{a\\in A}\\|\\nabla_{\\phi}^{2}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\|\\leq\\!\\!\\frac{1}{\\lambda-\\operatorname*{max}_{s,a}\\lvert A_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)\\rvert}(\\frac{8r_{m a x}}{(1-\\gamma)^{3}}}}\\\\ &{}&{\\quad\\quad+\\,\\frac{\\lambda+\\operatorname*{max}_{s,a}\\lvert A_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)\\rvert}{\\lambda-\\operatorname*{max}_{s,a}\\lvert A_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)\\rvert}(\\frac{(2-\\gamma)\\operatorname*{max}_{a,s}\\lvert A_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)\\rvert}{1-\\gamma}+2\\lambda+2)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. From (19), for any $s\\in S$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal{T}_{\\phi}^{\\top}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(\\cdot|s)=\\left(M(s)^{-1}-\\frac{M(s)^{-1}\\mathbf{1}\\mathbf{1}\\mathbf{1}^{\\top}M(s)^{-1}}{\\mathbf{1}^{\\top}M(s)^{-1}\\mathbf{1}}\\right)\\left(\\nabla_{\\phi}^{\\top}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,\\cdot)-\\lambda\\nabla_{\\phi}^{\\top}\\nabla_{\\hat{\\pi}(\\cdot|s)}d_{1}^{2}(\\hat{\\pi}_{\\phi},\\hat{\\pi},s)\\right)|_{\\hat{\\pi}=\\hat{\\pi}(\\cdot|s)}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "From the computations of $M(s)^{-1},\\nabla_{\\phi}^{\\top}\\nabla_{\\hat{\\pi}(\\cdot|s)}d_{1}^{2}(\\hat{\\pi}_{\\phi},\\hat{\\pi},s)|_{\\hat{\\pi}=\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}}$ , and $\\nabla_{\\phi}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,\\cdot)$ in (21) (27), we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left\\|\\left(M(s)^{-1}-\\frac{M(s)^{-1}\\mathbf{1}\\mathbf{1}^{\\top}M(s)^{-1}}{\\mathbf{1}^{\\top}M(s)^{-1}\\mathbf{1}}\\right)_{j}\\right\\|\\leq\\frac{\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)}{\\lambda}\\operatorname*{max}_{a,s}\\frac{\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)}{\\hat{\\pi}_{\\phi}(a|s)}\\leq\\frac{\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)}{\\lambda-\\operatorname*{max}_{s,a}\\lvert A_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)\\rvert},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\Vert\\nabla_{\\phi}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)\\Vert\\leq\\operatorname*{max}_{a}\\Vert\\nabla_{\\phi}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)\\Vert\\leq\\frac{\\gamma}{1-\\gamma}\\operatorname*{max}_{a,s}|A_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)|.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "From (22)(24) ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\lambda\\nabla_{\\phi}^{\\top}\\nabla_{\\hat{\\pi}(a|s)}d_{1}^{2}(\\hat{\\pi}_{\\phi},\\hat{\\pi},s)\\|=\\|\\lambda(\\mathbf{1}(s,a)-\\hat{\\pi}_{\\phi}(\\cdot|s))\\frac{\\hat{\\pi}_{\\phi}(a|s)}{\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)}\\|\\le2(\\lambda+\\underset{a,s}{\\operatorname*{max}}\\,|A_{\\tau}^{\\hat{\\pi}_{\\phi}}\\left(s,a\\right)|).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The last inequality comes from Lemma 5. So, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\Vert\\nabla_{\\phi}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\Vert\\leq\\frac{\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)}{\\lambda-\\operatorname*{max}_{s,a}\\lvert A_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)\\rvert}(\\frac{\\gamma\\operatorname*{max}_{a,s}\\lvert A_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)\\rvert}{1-\\gamma}+2(\\lambda+\\operatorname*{max}_{a,s}\\lvert A_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)\\rvert)).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Therefore, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{\\in\\mathcal{A}}\\|\\nabla_{\\phi}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\|\\leq\\frac{1}{\\lambda-\\operatorname*{max}_{s,a}\\lvert A_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)\\rvert}(\\frac{\\gamma\\operatorname*{max}_{a,s}\\lvert A_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)\\rvert}{1-\\gamma}+2(\\lambda+\\operatorname*{max}_{a,s}|A_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)|)).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Also, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\Vert\\nabla_{\\phi}^{2}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\Vert\\leq\\frac{\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)}{\\lambda-\\operatorname*{max}_{s,a}\\lvert A_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)\\rvert}(\\Vert\\nabla_{\\phi}^{2}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)\\Vert+\\lambda\\Vert\\nabla_{\\phi}^{2}\\nabla_{\\hat{\\pi}(a|s)}d_{1}^{2}(\\hat{\\pi}_{\\phi},\\hat{\\pi},s)\\Vert).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "From Lemma D.4 in [2], we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|\\nabla_{\\phi}^{2}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)\\|\\leq\\frac{8r_{m a x}}{(1-\\gamma)^{3}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Moreover, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lambda\\|\\nabla_{\\phi}^{2}\\nabla_{\\hat{\\pi}(a|s)}d_{1}^{2}(\\hat{\\pi}_{\\phi},\\hat{\\pi},s)\\|}\\\\ &{=\\lambda\\|\\nabla_{\\phi}((\\mathbf{1}(s,a)-\\hat{\\pi}_{\\phi}(\\cdot|s))\\frac{\\hat{\\pi}_{\\phi}(a|s)}{\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)})\\|}\\\\ &{\\leq\\frac{\\lambda+\\operatorname*{max}_{s,a}\\left|A_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)\\right|}{\\lambda-\\operatorname*{max}_{s,a}\\left|A_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)\\right|}(\\frac{\\gamma\\operatorname*{max}_{a,s}\\left|A_{\\tau}^{\\hat{\\pi}_{\\phi}}\\left(s,a\\right)\\right|}{1-\\gamma}+2(\\lambda+\\underset{a,s}{\\operatorname*{max}}|A_{\\tau}^{\\hat{\\pi}_{\\phi}}\\left(s,a\\right)|)+2)}\\\\ &{=\\frac{\\lambda+\\operatorname*{max}_{s,a}\\left|A_{\\tau}^{\\hat{\\pi}_{\\phi}}\\left(s,a\\right)\\right|}{\\lambda-\\operatorname*{max}_{s,a}\\left|A_{\\tau}^{\\hat{\\pi}_{\\phi}}\\left(s,a\\right)\\right|}(\\frac{\\left(2-\\gamma\\right)\\operatorname*{max}_{a,s}\\left|A_{\\tau}^{\\hat{\\pi}_{\\phi}}\\left(s,a\\right)\\right|}{1-\\gamma}+2\\lambda+2)}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "So, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{a\\in A}\\|\\nabla_{\\phi}^{2}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\|\\leq\\!\\!\\frac{1}{\\lambda-\\operatorname*{max}_{s,a}\\lvert A_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)\\rvert}(\\frac{8r_{m a x}}{(1-\\gamma)^{3}}}}\\\\ &{}&{\\quad\\quad+\\,\\frac{\\lambda+\\operatorname*{max}_{s,a}\\lvert A_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)\\rvert}{\\lambda-\\operatorname*{max}_{s,a}\\lvert A_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)\\rvert}(\\frac{(2-\\gamma)\\operatorname*{max}_{a,s}\\lvert A_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)\\rvert}{1-\\gamma}+2\\lambda+2)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Lemma 8. Suppose that Assumptions $^{\\,I}$ and 2 hold. Let $\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}=\\mathcal{A}l g(\\hat{\\pi}_{\\phi},\\lambda,\\tau)$ where $D_{\\tau}=D_{\\tau,1}$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|\\nabla_{\\phi}^{2}J_{\\tau}(\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}})\\|\\leq\\frac{r_{m a x}B}{(1-\\gamma)^{2}}+\\frac{2\\gamma r_{m a x}C^{2}}{(1-\\gamma)^{3}},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\begin{array}{r l r l r l r l r l r}{C}&{{}=}&{}&{{\\frac{1}{\\lambda-A_{m a x}}}{\\left({\\frac{\\gamma A_{m a x}}{1-\\gamma}}\\right.}}&{{}+}&{2\\lambda}&{{}+}&{2A_{m a x}\\big)}&{{}a n d}&{{}B}&{{}=}&{}&{{\\frac{1}{\\lambda-A_{m a x}}}{\\left({\\frac{8r_{m a x}}{(1-\\gamma)^{3}}}\\right.}}&{{}+}&{{}\\left.{\\frac{\\gamma A_{m a x}}{(1-\\gamma)^{3}}}\\right)\\left({\\frac{\\pi r^{2}}{(1-\\gamma)^{2}}}\\right)+}\\end{array}$ $\\frac{\\lambda\\!+\\!A_{m a x}}{\\lambda\\!-\\!A_{m a x}}\\bigl(\\frac{(2\\!-\\!\\gamma)A_{m a x}}{1\\!-\\!\\gamma}+2\\lambda\\!+\\!2)\\bigr)$ ", "page_idx": 28}, {"type": "text", "text": "Proof. From Lemma 7, we have bounded $\\begin{array}{r}{\\sum_{a\\in\\mathcal{A}}\\|\\nabla_{\\phi}\\hat{\\pi}_{\\boldsymbol{\\theta}_{\\tau}^{\\prime}}(a|s)\\|}\\end{array}$ and $\\begin{array}{r}{\\sum_{a\\in\\mathcal{A}}\\|\\nabla_{\\phi}^{2}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\|}\\end{array}$ . Borrow the result from Lemma D.2 in [2]. \u53e3 ", "page_idx": 28}, {"type": "text", "text": "K.2.2 Convergence guarantee ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Theorem 5. Consider the tabular softmax policy for the discrete state-action space shown in Section 5.1, and the within-task algorithm $\\boldsymbol{A l g}$ in $(I)$ . Suppose that Assumptions $^{\\,l}$ and 2 hold. Let $\\{\\phi_{t}\\}_{t=1}^{T}$ be the sequence generated by Algorithm $^{\\,I}$ with $D_{\\tau}=D_{\\tau,1}$ , $\\lambda>A_{m a x}$ , and the step size selected as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\alpha=\\operatorname*{min}\\left\\{\\left(\\frac{r_{m a x}B}{(1-\\gamma)^{2}}+\\frac{2\\gamma r_{m a x}C^{2}}{(1-\\gamma)^{3}}\\right)^{-1},\\frac{1}{G\\sqrt{T}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Then, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{T}\\sum_{t=1}^{T}\\mathbb{E}_{t}\\left[\\|\\nabla_{\\phi}\\mathbb{E}_{\\tau\\sim\\mathbb{P}(\\Gamma)}[J_{\\tau}(\\mathcal{A}l g(\\hat{\\pi}_{\\phi_{t}},\\lambda,\\tau))]\\|^{2}\\right]}\\\\ &{\\displaystyle\\leq\\biggr(\\frac{2r_{m a x}^{2}B}{(1-\\gamma)^{3}}+\\frac{4\\gamma r_{m a x}^{2}C^{2}}{(1-\\gamma)^{4}}\\biggr)\\frac{1}{T}+\\biggr(\\frac{2r_{m a x}}{1-\\gamma}+\\frac{r_{m a x}B}{(1-\\gamma)^{2}}+\\frac{2\\gamma r_{m a x}C^{2}}{(1-\\gamma)^{3}}\\biggr)\\frac{G}{\\sqrt{T}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where ", "page_idx": 28}, {"type": "equation", "text": "$$\nG=\\frac{2A_{m a x}}{1-\\gamma}(\\frac{A_{m a x}}{\\lambda-A_{m a x}}\\frac{\\gamma}{1-\\gamma}+2),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\nC=\\frac{1}{\\lambda-A_{m a x}}(\\frac{\\gamma A_{m a x}}{1-\\gamma}+2\\lambda+2A_{m a x}),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and ", "page_idx": 28}, {"type": "equation", "text": "$$\nB=\\frac{1}{\\lambda-A_{m a x}}(\\frac{8r_{m a x}}{(1-\\gamma)^{3}}+\\frac{\\lambda+A_{m a x}}{\\lambda-A_{m a x}}(\\frac{(2-\\gamma)A_{m a x}}{1-\\gamma}+2\\lambda+2)).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. As the smoothness constant of $J_{\\tau}(\\hat{\\pi}_{\\boldsymbol{\\theta}_{\\tau}^{\\prime}})$ , i.e., $J_{\\tau}(\\mathcal{A}l g(\\hat{\\pi}_{\\phi},\\lambda,\\tau)$ is obtained in (8), the smoothness constant of $\\mathbb{E}_{\\tau\\sim\\mathbb{P}(\\Gamma)}[J_{\\tau}(A l g(\\hat{\\pi}_{\\phi},\\lambda,\\tau))]$ is the same, i.e., ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\Vert\\nabla_{\\phi}^{2}\\mathbb{E}_{\\tau\\sim\\mathbb{P}(\\Gamma)}[J_{\\tau}(\\mathcal{A}l g(\\hat{\\pi}_{\\phi},\\lambda,\\tau))]\\Vert\\leq\\frac{B r_{m a x}}{(1-\\gamma)^{2}}+\\frac{2\\gamma r_{m a x}C^{2}}{(1-\\gamma)^{3}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Moreover, from Lemma 6, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\|\\nabla_{\\phi}J_{\\tau}(\\mathcal{A}l g(\\hat{\\pi}_{\\phi},\\lambda,\\tau))\\|\\leq\\frac{A_{m a x}}{1-\\gamma}(\\frac{A_{m a x}}{\\lambda-A_{m a x}}\\frac{\\gamma}{1-\\gamma}+2).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "From the convergence theorem of SDG with smoothness and bounded gradient shown in [19], let the step size ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\alpha=\\operatorname*{min}\\left\\{\\left(\\frac{r_{m a x}B}{(1-\\gamma)^{2}}+\\frac{2\\gamma r_{m a x}C^{2}}{(1-\\gamma)^{3}}\\right)^{-1},\\frac{1}{G\\sqrt{T}}\\right\\},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{T}\\sum_{t=1}^{T}\\mathbb{E}_{t}\\left[\\|\\nabla_{\\phi}\\mathbb{E}_{\\tau\\sim\\mathbb{P}(\\Gamma)}[J_{\\tau}(A l g(\\hat{\\pi}_{\\phi_{t}},\\lambda,\\tau))]\\|^{2}\\right]}\\\\ &{\\le\\!\\left(\\frac{2r_{m a x}B}{(1-\\gamma)^{2}}+\\frac{4\\gamma r_{m a x}C^{2}}{(1-\\gamma)^{3}}\\right)\\mathbb{E}_{\\tau\\sim\\mathbb{P}(\\Gamma)}[J_{\\tau}(A l g(\\hat{\\pi}_{\\phi_{T}},\\lambda,\\tau))-J_{\\tau}(A l g(\\hat{\\pi}_{\\phi_{0}},\\lambda,\\tau))]\\frac{1}{T}}\\\\ &{\\quad+\\left(2\\mathbb{E}_{\\tau\\sim\\mathbb{P}(\\Gamma)}[J_{\\tau}(A l g(\\hat{\\pi}_{\\phi_{T}},\\lambda,\\tau))-J_{\\tau}(A l g(\\hat{\\pi}_{\\phi_{0}},\\lambda,\\tau))]+\\frac{r_{m a x}B}{(1-\\gamma)^{2}}+\\frac{2\\gamma r_{m a x}C^{2}}{(1-\\gamma)^{3}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Since $\\begin{array}{r}{\\mathbb{E}_{\\tau\\sim\\mathbb{P}(\\Gamma)}[J_{\\tau}(\\mathcal{A}l g(\\hat{\\pi}_{\\phi_{T}},\\lambda,\\tau))-J_{\\tau}(\\mathcal{A}l g(\\hat{\\pi}_{\\phi_{0}},\\lambda,\\tau))]\\leq\\frac{r_{m a x}}{1-\\gamma}}\\end{array}$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{T}\\sum_{t=1}^{T}\\mathbb{E}_{t}\\left[\\|\\nabla_{\\phi}\\mathbb{E}_{\\tau\\sim\\mathbb{P}(\\Gamma)}[J_{\\tau}(\\mathcal{A}l g(\\hat{\\pi}_{\\phi_{t}},\\lambda,\\tau))]\\|^{2}\\right]}\\\\ &{\\displaystyle\\leq\\biggr(\\frac{2r_{m a x}^{2}B}{(1-\\gamma)^{3}}+\\frac{4\\gamma r_{m a x}^{2}C^{2}}{(1-\\gamma)^{4}}\\biggr)\\frac{1}{T}+\\biggr(\\frac{2r_{m a x}}{1-\\gamma}+\\frac{r_{m a x}B}{(1-\\gamma)^{2}}+\\frac{2\\gamma r_{m a x}C^{2}}{(1-\\gamma)^{3}}\\biggr)\\frac{G}{\\sqrt{T}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where ", "page_idx": 29}, {"type": "equation", "text": "$$\nG=\\frac{2A_{m a x}}{1-\\gamma}(\\frac{A_{m a x}}{\\lambda-A_{m a x}}\\frac{\\gamma}{1-\\gamma}+2),\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\nC=\\frac{1}{\\lambda-A_{m a x}}(\\frac{\\gamma A_{m a x}}{1-\\gamma}+2\\lambda+2A_{m a x}),\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and ", "page_idx": 29}, {"type": "equation", "text": "$$\nB=\\frac{1}{\\lambda-A_{m a x}}(\\frac{8r_{m a x}}{(1-\\gamma)^{3}}+\\frac{\\lambda+A_{m a x}}{\\lambda-A_{m a x}}(\\frac{(2-\\gamma)A_{m a x}}{1-\\gamma}+2\\lambda+2)).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Corollary 1. Suppose all assumptions and conditions in Theorem 5 hold, and we set $\\lambda\\geq2A_{m a x}$ , then ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=1}^{T}\\mathbb{E}_{t}\\left[\\|\\nabla_{\\phi}\\mathbb{E}_{\\tau\\sim\\mathbb{P}(\\Gamma)}[J_{\\tau}(A l g(\\hat{\\pi}_{\\phi_{t}},\\lambda,\\tau))]\\|^{2}\\right]\\leq\\frac{(B+2C^{2})r_{m a x}}{(1-\\gamma)^{4}}(\\frac{2r_{m a x}}{T}+\\frac{G}{\\sqrt{T}}),\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\begin{array}{r}{B\\triangleq\\frac{16r_{m a x}}{\\lambda(1-\\gamma)^{3}}+\\frac{24}{1-\\gamma}+\\frac{12}{\\lambda}}\\end{array}$ , $\\begin{array}{r}{C\\triangleq\\frac{6}{1-\\gamma}}\\end{array}$ , and $\\begin{array}{r}{G\\triangleq\\frac{4A_{m a x}}{(1-\\gamma)^{2}}}\\end{array}$ . ", "page_idx": 29}, {"type": "text", "text": "Proof. Since \u03bb \u2265 2Amax, we have \u03bb\u2212A1max $\\begin{array}{r}{\\frac{1}{\\lambda-A_{m a x}}\\ \\leq\\ \\frac{1}{A_{m a x}}}\\end{array}$ and $\\frac{1}{\\lambda-A_{m a x}}~\\leq~\\frac{2}{\\lambda}$ . Then, simplify the inequality in Theorem 5. \u53e3 ", "page_idx": 29}, {"type": "text", "text": "L Proofs of convergence when $D_{\\tau}=D_{\\tau,2}$ ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "L.1 Gradients of $\\nabla_{\\phi}J_{\\tau}(\\hat{\\pi}_{\\boldsymbol{\\theta}_{\\tau}^{\\prime}})$ when $D_{\\tau}=D_{\\tau,2}$ ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "From Proposition 1, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\nabla_{\\phi}J_{\\tau}(\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}})=\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}}}\\left[\\nabla_{\\phi}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(\\cdot|s)\\cdot A_{\\tau}^{\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}}(s,\\cdot)\\right],\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\nabla_{\\phi}^{\\top}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(\\cdot|s)=\\bigg(M(s)^{-1}-\\frac{M(s)^{-1}\\mathbf{1}\\mathbf{1}^{\\top}M(s)^{-1}}{\\mathbf{1}^{\\top}M(s)^{-1}\\mathbf{1}}\\bigg)}\\\\ &{}&{\\left(\\nabla_{\\phi}^{\\top}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,\\cdot)-\\lambda\\nabla_{\\phi}^{\\top}\\nabla_{\\hat{\\pi}(\\cdot|s)}d_{2}^{2}(\\hat{\\pi}_{\\phi},\\hat{\\pi},s)\\right)|_{\\hat{\\pi}=\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{M(s)=\\lambda\\nabla_{\\hat{\\pi}(\\cdot\\vert s)}^{2}d_{2}^{2}(\\hat{\\pi}_{\\phi},\\hat{\\pi},s)=\\lambda\\left[\\begin{array}{l l l l}{\\frac{1}{\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a_{1}\\vert s)}}&&&\\\\ &{\\cdot\\underbrace{}&&\\\\ &&{\\cdot\\underbrace{1}}&\\\\ &&&{\\frac{1}{\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a_{n}\\vert s)}}\\end{array}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Then, ", "page_idx": 30}, {"type": "equation", "text": "$$\nM(s)^{-1}=\\frac{1}{\\lambda}\\left[\\begin{array}{c c c c}{{\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a_{1}|s)}}&{{}}&{{}}&{{}}\\\\ {{}}&{{\\ddots}}&{{}}&{{}}\\\\ {{}}&{{}}&{{}}&{{\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a_{n}|s)}}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Also, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\nabla_{\\phi}^{\\top}\\nabla_{\\hat{\\pi}(\\cdot|s)}d_{2}^{2}(\\hat{\\pi}_{\\phi},\\hat{\\pi},s)|_{\\hat{\\pi}=\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}}=\\left[\\begin{array}{c}{\\!\\!\\!\\left[-\\frac{\\nabla_{\\phi}^{\\top}\\hat{\\pi}_{\\phi}(a_{1}|s)}{\\hat{\\pi}_{\\phi}(a_{1}|s)}\\right]\\!\\!\\!}\\\\ {\\!\\!\\!\\vdots\\!\\!\\!}\\\\ {\\!\\!\\!\\!-\\frac{\\nabla_{\\phi}^{\\top}\\hat{\\pi}_{\\phi}(a_{n}|s)}{\\hat{\\pi}_{\\phi}(a_{n}|s)}\\!\\!\\!}\\end{array}.\\right.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Specially, A\u03c4\u03c0\u02c6\u03b8\u2032\u03c4(s, \u00b7)\u22a4M(s)\u22121\u22a41 1\u2212\u22a41M(s)\u22121 , because we have ", "page_idx": 30}, {"type": "equation", "text": "$$\nA_{\\tau}^{\\hat{\\pi}_{\\boldsymbol{\\theta}_{\\tau}^{\\prime}}}(s,\\cdot)^{\\top}M(s)^{-1}\\mathbf{1}=\\sum_{a\\in\\mathcal{A}}\\hat{\\pi}_{\\boldsymbol{\\theta}_{\\tau}^{\\prime}}(a|s)A_{\\tau}^{\\hat{\\pi}_{\\boldsymbol{\\theta}_{\\tau}^{\\prime}}}(s,a)=0.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Then, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\nabla_{\\phi}^{\\top}J_{\\tau}(\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}})=\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}},a\\sim\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}}\\left[A_{\\tau}^{\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}}(s,a)(\\frac{1}{\\lambda}\\nabla_{\\phi}^{\\top}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)+\\mathbf{1}^{\\top}(s,a)-\\hat{\\pi}_{\\phi}(\\cdot|s)^{\\top})\\right].\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Since $\\begin{array}{r}{\\sum_{a\\in\\mathcal{A}}\\hat{\\pi}_{\\boldsymbol{\\theta}_{\\tau}^{\\prime}}(a|s)A_{\\tau}^{\\hat{\\pi}_{\\boldsymbol{\\theta}_{\\tau}^{\\prime}}}(s,a)=0}\\end{array}$ , then $\\begin{array}{r}{\\sum_{a\\in\\mathcal{A}}\\hat{\\pi}_{\\boldsymbol{\\theta}_{\\tau}^{\\prime}}(a|\\boldsymbol{s})A_{\\tau}^{\\hat{\\pi}_{\\boldsymbol{\\theta}_{\\tau}^{\\prime}}}(\\boldsymbol{s},a)\\hat{\\pi}_{\\boldsymbol{\\phi}}(\\cdot|\\boldsymbol{s})^{\\top}=0.}\\end{array}$ . We have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\nabla_{\\phi}^{\\top}J_{\\tau}(\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}})=\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}},a\\sim\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}}\\left[A_{\\tau}^{\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}}(s,a)(\\frac{1}{\\lambda}\\nabla_{\\phi}^{\\top}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)+\\mathbf{1}^{\\top}(s,a))\\right].\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Here, $\\mathbf{1}(s^{\\prime},a^{\\prime})$ denote the column vector where the element is $1$ if $s=s^{\\prime}$ and $a=a^{\\prime}$ , otherwise is $0$ , for each pair $(s,a)\\in S\\times A$ . ", "page_idx": 30}, {"type": "text", "text": "L.2 Convergence guarantee when $D_{\\tau}=D_{\\tau,2}$ ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "L.2.1 Auxiliary lemmas ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Lemma 9. Suppose that Assumption 2 holds. Let $\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}=\\mathcal{A}l g(\\hat{\\pi}_{\\phi},\\lambda,\\tau)$ where $D_{\\tau}=D_{\\tau,2}$ , we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\Vert\\nabla_{\\phi}J_{\\tau}(\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}})\\Vert\\leq\\frac{\\operatorname*{max}_{s,a}\\lvert A_{\\tau}^{\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}}(s,a)\\rvert}{1-\\gamma}(\\frac{\\operatorname*{max}_{s,a}\\lvert A_{\\tau}^{\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}}(s,a)\\rvert}{\\lambda}\\frac{\\gamma}{1-\\gamma}+1).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. As shown in (33) ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\nabla_{\\phi}^{\\top}J_{\\tau}(\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}})=\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}},a\\sim\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}}\\left[A_{\\tau}^{\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}}(s,a)(\\frac{1}{\\lambda}\\nabla_{\\phi}^{\\top}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)+\\mathbf{1}^{\\top}(s,a))\\right].\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "As shown in proof of Lemma 6 in (27), ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\Vert\\nabla_{\\phi}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)\\Vert\\leq\\frac{\\gamma}{1-\\gamma}\\operatorname*{max}_{a,s}|A_{\\tau}^{\\hat{\\pi}_{\\phi}}\\left(s,a\\right)|,\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "we have that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\Vert\\nabla_{\\phi}J_{\\tau}(\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}})\\Vert\\leq\\frac{\\operatorname*{max}_{s,a}\\lvert A_{\\tau}^{\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}}(s,a)\\rvert}{1-\\gamma}(\\frac{\\operatorname*{max}_{s,a}\\lvert A_{\\tau}^{\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}}(s,a)\\rvert}{\\lambda}\\frac{\\gamma}{1-\\gamma}+1).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Lemma 10. Suppose that Assumption 2 holds. Let $\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}=\\mathcal{A}l g(\\hat{\\pi}_{\\phi},\\lambda,\\tau)$ where $D_{\\tau}=D_{\\tau,2}$ , for any $s\\in S$ , we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\sum_{a\\in A}\\|\\nabla_{\\phi}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\|\\leq\\frac{2\\gamma}{\\lambda(1-\\gamma)}\\operatorname*{max}_{a,s}|A_{\\tau}^{\\hat{\\pi}_{\\phi}}\\left(s,a\\right)|+4\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\sum_{a\\in\\mathcal{A}}\\|\\nabla_{\\phi}^{2}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\|\\leq(\\frac{2\\gamma}{\\lambda(1-\\gamma)}\\operatorname*{max}_{a,s}|A_{\\tau}^{\\hat{\\pi}_{\\phi}}\\left(s,a\\right)|+4)^{2}+\\frac{16r_{m a x}}{\\lambda(1-\\gamma)^{3}}+2.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof. As shown in 30, we have $\\nabla_{\\phi}^{\\top}\\hat{\\pi}_{\\boldsymbol{\\theta}_{\\tau}^{\\prime}}(\\cdot|\\boldsymbol{s})=$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\left(M(s)^{-1}-\\frac{M(s)^{-1}\\mathbf{1}\\mathbf{1}^{\\top}M(s)^{-1}}{\\mathbf{1}^{\\top}M(s)^{-1}\\mathbf{1}}\\right)\\left(\\nabla_{\\phi}^{\\top}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,\\cdot)-\\lambda\\nabla_{\\phi}^{\\top}\\nabla_{\\hat{\\pi}(\\cdot\\vert s)}d_{2}^{2}(\\hat{\\pi}_{\\phi},\\hat{\\pi},s)\\right)|_{\\hat{\\pi}=\\hat{\\pi}_{\\theta_{7}}},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where the computations of $M(s)^{-1}$ and $\\nabla_{\\phi}^{\\top}\\nabla_{\\hat{\\pi}(\\cdot\\vert s)}d_{2}^{2}(\\hat{\\pi}_{\\phi},\\hat{\\pi},s)$ are shown in (31) (32) and (24), then ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\nabla_{\\phi}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)=\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\big(\\frac{1}{\\lambda}\\nabla_{\\phi}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)+{\\bf1}(s,a)-\\hat{\\pi}_{\\phi}(\\cdot|s)\\big)}}\\\\ {{\\displaystyle-\\,\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\sum_{a^{\\prime}\\in A}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a^{\\prime}|s)\\big(\\frac{1}{\\lambda}\\nabla_{\\phi}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a^{\\prime})+{\\bf1}(s,a^{\\prime})-\\hat{\\pi}_{\\phi}(\\cdot|s)\\big).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Therefore, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\|\\nabla_{\\phi}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\|\\leq\\left\\|\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)(\\frac{1}{\\lambda}\\nabla_{\\phi}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)+{\\bf1}(s,a)-\\hat{\\pi}_{\\phi}(\\cdot|s))\\right\\|}\\\\ &{}&{\\quad\\quad+\\left\\|\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\displaystyle\\sum_{a^{\\prime}\\in{\\cal A}}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a^{\\prime}|s)(\\frac{1}{\\lambda}\\nabla_{\\phi}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a^{\\prime})+{\\bf1}(s,a^{\\prime})-\\hat{\\pi}_{\\phi}(\\cdot|s))\\right\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Then, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{a\\in A}\\|\\nabla_{\\phi}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\|\\leq\\displaystyle\\sum_{a\\in A}\\left\\|\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)(\\frac{1}{\\lambda}\\nabla_{\\phi}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)+\\mathbf{1}(s,a)-\\hat{\\pi}_{\\phi}(\\cdot|s))\\right\\|}\\\\ &{\\quad+\\displaystyle\\sum_{a\\in A}\\left\\|\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\sum_{a^{\\prime}\\in A}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a^{\\prime}|s)(\\frac{1}{\\lambda}\\nabla_{\\phi}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a^{\\prime})+\\mathbf{1}(s,a^{\\prime})-\\hat{\\pi}_{\\phi}(\\cdot|s))\\right\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "From (27), we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\Vert\\nabla_{\\phi}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)\\Vert\\leq\\frac{\\gamma}{1-\\gamma}\\operatorname*{max}_{a,s}|A_{\\tau}^{\\hat{\\pi}_{\\phi}}\\left(s,a\\right)|.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Then, ", "text_level": 1, "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{a\\in A}\\|\\nabla_{\\phi}\\hat{\\pi}_{\\varphi^{\\prime}}(a|s)\\|\\leq\\displaystyle\\sum_{a\\in A}\\hat{\\pi}_{\\theta^{\\prime}}(a|s)\\left\\|\\frac{1}{\\lambda}\\nabla_{\\phi}Q\\hat{\\pi}_{^{*}}^{\\phi}(s,a)+\\mathbf{1}(s,a)-\\hat{\\pi}_{\\phi}(\\cdot|s)\\right\\|}\\\\ {\\displaystyle\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ +\\displaystyle\\sum_{a\\in A}\\hat{\\pi}_{\\theta^{\\prime}}(a|s)\\left\\|\\displaystyle\\prod_{a^{\\prime}\\in A}\\hat{\\pi}_{\\theta^{\\prime}}(a^{\\prime}|s)(\\frac{1}{\\lambda}\\nabla_{\\phi}Q\\hat{\\pi}_{^{*}}^{\\phi}(s,a^{\\prime})+\\mathbf{1}(s,a^{\\prime})-\\hat{\\pi}_{\\phi}(\\cdot|s))\\right\\|}\\\\ {\\displaystyle\\leq\\displaystyle\\sum_{a\\in A}\\hat{\\pi}_{\\psi^{\\prime}}(a|s)(\\frac{\\gamma}{\\lambda(1-\\gamma)}\\operatorname*{max}_{a,\\nu}|A\\hat{\\bar{\\pi}}_{^{*}}^{\\phi}(s,a)|+2)}\\\\ {\\displaystyle\\ \\ \\ \\ \\ \\ \\ \\ +\\displaystyle\\sum_{a\\in A}\\hat{\\pi}_{\\theta^{\\prime}}(a|s)\\sum_{a^{\\prime}\\in A}\\hat{\\pi}_{\\theta^{\\prime}}(a^{\\prime}|s)(\\frac{\\gamma}{\\lambda(1-\\gamma)}\\operatorname*{max}_{a,\\nu}|A\\hat{\\bar{\\pi}}_{^{*}}^{\\phi}(s,a)|+2)}\\\\ {\\displaystyle\\leq\\frac{2\\gamma}{\\lambda(1-\\gamma)}\\operatorname*{max}_{a,\\nu}|A\\hat{\\bar{\\pi}}_{^{*}}^{\\phi}(s,a)|+4,}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "And ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\Vert\\nabla_{\\phi}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\Vert\\leq\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)(\\frac{2\\gamma}{\\lambda(1-\\gamma)}\\operatorname*{max}_{a,s}|A_{\\tau}^{\\hat{\\pi}_{\\phi}}\\left(s,a\\right)|+4)\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Moreover, since ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\nabla_{\\phi}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)=\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\big(\\frac{1}{\\lambda}\\nabla_{\\phi}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)+{\\bf1}(s,a)-\\hat{\\pi}_{\\phi}(\\cdot|s)\\big)}}\\\\ {{\\displaystyle-\\,\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\sum_{a^{\\prime}\\in A}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a^{\\prime}|s)\\big(\\frac{1}{\\lambda}\\nabla_{\\phi}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a^{\\prime})+{\\bf1}(s,a^{\\prime})-\\hat{\\pi}_{\\phi}(\\cdot|s)\\big).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\phi}^{2}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)=\\!\\nabla_{\\phi}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)(\\frac{1}{\\lambda}\\nabla_{\\phi}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)+\\mathbf{1}(s,a)-\\hat{\\pi}_{\\phi}(\\cdot|s))}\\\\ &{\\phantom{=}\\,+\\,\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)(\\frac{1}{\\lambda}\\nabla_{\\phi}^{2}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)+-\\nabla_{\\phi}\\hat{\\pi}_{\\phi}(\\cdot|s))}\\\\ &{\\phantom{=}\\,-\\nabla_{\\phi}\\left(\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\sum_{a^{\\prime}\\in{\\cal A}}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a^{\\prime}|s)(\\frac{1}{\\lambda}\\nabla_{\\phi}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a^{\\prime})+\\mathbf{1}(s,a^{\\prime})-\\hat{\\pi}_{\\phi}(\\cdot|s))\\right)\\!.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Then, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{\\phi}^{2}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\|\\leq2\\|\\nabla_{\\phi}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\|\\|\\frac{1}{\\lambda}\\nabla_{\\phi}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)+\\mathbf{1}(s,a)-\\hat{\\pi}_{\\phi}(\\cdot|s)\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\ 2\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\|\\frac{1}{\\lambda}\\nabla_{\\phi}^{2}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)-\\nabla_{\\phi}\\hat{\\pi}_{\\phi}(\\cdot|s)\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "From (34), ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\Vert\\nabla_{\\phi}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\Vert\\leq\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)(\\frac{2\\gamma}{\\lambda(1-\\gamma)}\\operatorname*{max}_{a,s}|A_{\\tau}^{\\hat{\\pi}_{\\phi}}\\left(s,a\\right)|+4).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "From (27) ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\|\\frac{1}{\\lambda}\\nabla_{\\phi}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)+\\mathbf{1}(s,a)-\\hat{\\pi}_{\\phi}(\\cdot|s)\\|\\leq\\frac{\\gamma}{\\lambda(1-\\gamma)}\\operatorname*{max}_{a,s}|A_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)|+2\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "From Lemma D.4 in [2], we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\|\\nabla_{\\phi}^{2}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)\\|\\leq\\frac{8r_{m a x}}{(1-\\gamma)^{3}},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "then ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\|\\frac{1}{\\lambda}\\nabla_{\\phi}^{2}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)-\\nabla_{\\phi}\\hat{\\pi}_{\\phi}(\\cdot|s)\\|\\leq\\frac{8r_{m a x}}{\\lambda(1-\\gamma)^{3}}+1.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Therefore, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\Vert\\nabla_{\\phi}^{2}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\Vert\\leq\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)(\\frac{2\\gamma}{\\lambda(1-\\gamma)}\\operatorname*{max}_{a,s}|A_{\\tau}^{\\hat{\\pi}_{\\phi}}\\left(s,a\\right)|+4)^{2}+2\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)(\\frac{8r_{m a x}}{\\lambda(1-\\gamma)^{3}}+1).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "So, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\sum_{a\\in\\mathcal{A}}\\|\\nabla_{\\phi}^{2}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\|\\leq(\\frac{2\\gamma}{\\lambda(1-\\gamma)}\\operatorname*{max}_{a,s}|A_{\\tau}^{\\hat{\\pi}_{\\phi}}\\left(s,a\\right)|+4)^{2}+\\frac{16r_{m a x}}{\\lambda(1-\\gamma)^{3}}+2.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Lemma 11. Suppose that Assumptions $^{\\,l}$ and 2 hold. Let $\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}=\\mathcal{A}l g(\\hat{\\pi}_{\\phi},\\lambda,\\tau)$ where $D_{\\tau}=D_{\\tau,2}$ , we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\|\\nabla_{\\phi}^{2}J_{\\tau}(\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}})\\|\\leq\\frac{r_{m a x}B}{(1-\\gamma)^{2}}+\\frac{2\\gamma r_{m a x}C^{2}}{(1-\\gamma)^{3}},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $\\begin{array}{r}{C=\\frac{2\\gamma}{\\lambda(1-\\gamma)}A_{m a x}+4}\\end{array}$ and $\\begin{array}{r}{B=(\\frac{2\\gamma}{\\lambda(1-\\gamma)}A_{m a x}+4)^{2}+\\frac{16r_{m a x}}{\\lambda(1-\\gamma)^{3}}+2.}\\end{array}$ ", "page_idx": 33}, {"type": "text", "text": "Proof. Similar to the proof of Lemma 8 by using Lemma 10. ", "page_idx": 33}, {"type": "text", "text": "L.2.2 Convergence guarantee ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Theorem 6. Consider the tabular softmax policy for the discrete state-action space shown in Section 5.1, and the within-task algorithm Alg in $(I)$ . Suppose that Assumptions $^{\\,l}$ and 2 hold. Let $\\{\\phi_{t}\\}_{t=1}^{T}$ be the sequence generated by Algorithm $^{\\,I}$ with $D_{\\tau}=D_{\\tau,2}$ and the step size selected as ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\alpha=\\operatorname*{min}\\left\\{\\left(\\frac{r_{m a x}B}{(1-\\gamma)^{2}}+\\frac{2\\gamma r_{m a x}C^{2}}{(1-\\gamma)^{3}}\\right)^{-1},\\frac{1}{G\\sqrt{T}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Then, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{T}\\sum_{t=1}^{T}\\mathbb{E}_{t}\\left[\\|\\nabla_{\\phi}\\mathbb{E}_{\\tau\\sim\\mathbb{P}(\\Gamma)}[J_{\\tau}(\\mathcal{A}l g(\\hat{\\pi}_{\\phi_{t}},\\lambda,\\tau))]\\|^{2}\\right]}\\\\ &{\\displaystyle\\leq\\biggr(\\frac{2r_{m a x}^{2}B}{(1-\\gamma)^{3}}+\\frac{4\\gamma r_{m a x}^{2}C^{2}}{(1-\\gamma)^{4}}\\biggr)\\frac{1}{T}+\\biggr(\\frac{2r_{m a x}}{1-\\gamma}+\\frac{r_{m a x}B}{(1-\\gamma)^{2}}+\\frac{2\\gamma r_{m a x}C^{2}}{(1-\\gamma)^{3}}\\biggr)\\frac{G}{\\sqrt{T}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where ", "page_idx": 33}, {"type": "equation", "text": "$$\nG=\\frac{2A_{m a x}}{1-\\gamma}(\\frac{A_{m a x}}{\\lambda}\\frac{\\gamma}{1-\\gamma}+1),\n$$", "text_format": "latex", "page_idx": 33}, {"type": "equation", "text": "$$\nC=\\frac{2\\gamma}{\\lambda(1-\\gamma)}A_{m a x}+4,\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and ", "page_idx": 33}, {"type": "equation", "text": "$$\nB=(\\frac{2\\gamma A_{m a x}}{\\lambda(1-\\gamma)}+4)^{2}+\\frac{16r_{m a x}}{\\lambda(1-\\gamma)^{3}}+2.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof. Similar to the proof of Theorem 5, by using the gradient bound in Lemma 9 and the smoothness in Lemma 11. \u53e3 ", "page_idx": 33}, {"type": "text", "text": "Corollary 2. Suppose all assumptions and conditions in Theorem 6 hold, and we set $\\lambda\\geq2A_{m a x}$ , then ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=1}^{T}\\mathbb{E}_{t}\\left[\\|\\nabla_{\\phi}\\mathbb{E}_{\\tau\\sim\\mathbb{P}(\\Gamma)}[J_{\\tau}(A l g(\\hat{\\pi}_{\\phi_{t}},\\lambda,\\tau))]\\|^{2}\\right]\\leq\\frac{(B+2C^{2})r_{m a x}}{(1-\\gamma)^{4}}(\\frac{2r_{m a x}}{T}+\\frac{G}{\\sqrt{T}}),\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "$\\begin{array}{r}{B\\triangleq\\frac{16r_{m a x}}{\\lambda(1-\\gamma)^{3}}+\\frac{18}{(1-\\gamma)^{2}}}\\end{array}$ , $\\begin{array}{r}{C\\triangleq\\frac{4}{1-\\gamma}}\\end{array}$ , and $\\begin{array}{r}{G\\triangleq\\frac{2A_{m a x}}{(1-\\gamma)^{2}})}\\end{array}$ ", "page_idx": 33}, {"type": "text", "text": "Proof. Since \u03bb \u22652Amax, we have \u03bb1 \u22642A1max . Then, simplify the inequality in Theorem 5. \u518f\u53e3 ", "page_idx": 33}, {"type": "text", "text": "M Proofs of convergence when $D_{\\tau}=D_{\\tau,3}$ ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Lemma 12. Suppose that Assumptions $I,\\,2_{i}$ , and 3 hold. Let $\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}=\\mathcal{A}l g(\\hat{\\pi}_{\\phi},\\lambda,\\tau)$ where $D_{\\tau}=D_{\\tau,3}$ . If $\\dot{\\mathbf{\\theta}}\\lambda>(6L_{1}^{2}+2L_{2})A_{m a x}$ , then $\\nabla_{\\phi}J_{\\tau}({\\cal A}l g^{(3)}(\\hat{\\pi}_{\\phi},\\lambda,\\tau))$ exists for any $\\phi$ , and ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\|\\nabla_{\\phi}J_{\\tau}(\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}})\\|\\leq\\frac{L_{1}A_{m a x}(\\lambda+\\frac{2\\gamma}{1-\\gamma}L_{1}^{2}A_{m a x})}{(1-\\gamma)(\\lambda-(6L_{1}^{2}+2L_{2})A_{m a x})}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof. From Proposition 2, we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\nabla_{\\phi}J_{\\tau}(\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}})=\\frac{1}{1-\\gamma}\\nabla_{\\phi}\\theta_{\\tau}^{\\prime}\\cdot\\underbrace{\\mathbb{E}}_{\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}}\\quad\\left[\\frac{\\nabla_{\\theta_{\\tau}^{\\prime}}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)}{\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)}A_{\\tau}^{\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}}(s,a)\\right],\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\phi}^{\\top}\\theta_{\\tau}^{\\prime}=-\\underbrace{\\mathbb{E}}_{\\quad s\\sim\\nu_{\\tau}^{\\hat{\\pi}_{\\phi}}\\cdot\\phi}\\,\\left[-\\frac{\\nabla_{\\theta}^{2}\\hat{\\pi}_{\\theta}(a|s)}{\\hat{\\pi}_{\\phi}(a|s)}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)+\\lambda\\nabla_{\\theta}^{2}d^{2}(\\hat{\\pi}_{\\phi}(\\cdot|s),\\hat{\\pi}_{\\theta}(\\cdot|s))\\right]^{-1}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ &{\\quad\\quad\\mathbb{E}_{\\phi}\\quad\\left[-\\frac{\\nabla_{\\theta}\\hat{\\pi}_{\\theta}(a|s)}{\\hat{\\pi}_{\\phi}(a|s)}\\nabla_{\\phi}^{\\top}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)+\\lambda\\nabla_{\\phi}^{\\top}\\nabla_{\\theta}d^{2}(\\hat{\\pi}_{\\phi}(\\cdot|s),\\hat{\\pi}_{\\theta}(\\cdot|s))\\right]|_{\\theta=\\theta_{\\tau}^{\\prime}}.}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "When $D_{\\tau}~\\;=\\;\\;D_{\\tau,3}$ , and the policy with function approximation is defined by $\\hat{\\pi}_{\\boldsymbol{\\theta}}(\\boldsymbol{a}|\\boldsymbol{s})$ \u225c eexxpp((ff\u03b8\u03b8((ss,,aa\u2032))))da\u2032 , \u2200(s, a) \u2208S \u00d7 A, from Lemma 4, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\nabla_{\\phi}J_{\\tau}(\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}})=\\frac{1}{1-\\gamma}\\nabla_{\\phi}\\theta_{\\tau}^{\\prime}\\cdot\\underset{\\underset{a\\sim\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}}{s\\sim\\hat{\\nu}_{\\tau}}}{\\mathbb{E}}\\left[\\nabla_{\\theta_{\\tau}^{\\prime}}f_{\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}}(s,a)A_{\\tau}^{\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}}(s,a)\\right],\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $\\nabla_{\\phi}^{\\top}\\theta_{\\tau}^{\\prime}=$ ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\ \\ \\ \\ \\ \\underset{s\\sim b_{\\phi}^{\\frac{\\hat{\\pi}}{\\gamma}\\phi}}{\\mathbb{E}}\\left[-\\frac{\\nabla_{\\theta_{\\tau}^{\\tau}}^{2}\\hat{\\pi}\\theta_{\\tau}^{\\prime}(a|s)}{\\hat{\\pi}_{\\phi}(a|s)}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)+\\lambda I\\right]^{-1}\\underset{a\\sim\\hat{\\pi}_{\\phi}^{\\tau}(\\cdot\\vert s)}{\\mathbb{E}}\\ \\left[\\frac{\\nabla_{\\theta_{\\tau}^{\\tau}}\\hat{\\pi}\\theta_{\\tau}^{\\prime}(a|s)}{\\hat{\\pi}_{\\phi}(a|s)}\\nabla_{\\phi}^{\\top}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)+\\lambda I\\right]^{-1}}\\\\ &{=\\underset{s\\sim b_{\\tau}^{\\hat{\\pi}_{\\phi}}(\\cdot\\vert s)}{\\mathbb{E}}\\ \\frac{\\hat{\\pi}}{\\hat{\\pi}_{\\phi}(\\cdot\\vert s)}\\left[-\\int_{A}\\nabla_{\\theta_{\\tau}^{\\tau}}^{2}\\hat{\\pi}\\theta_{\\tau}(a|s)Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)d a+\\lambda I\\right]^{-1}\\mathbb{E}_{s\\sim b_{\\tau}^{\\hat{\\pi}_{\\phi}}}\\left[\\int_{A}\\nabla_{\\theta_{\\tau}^{\\tau}}\\hat{\\pi}\\theta_{\\tau}(a|s)\\nabla_{\\phi}^{\\top}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)d a+\\lambda I\\right]}\\\\ &{=\\mathbb{E}_{s\\sim b_{\\tau}^{\\hat{\\pi}_{\\phi}}}\\left[-\\int_{A}\\nabla_{\\theta_{\\tau}^{\\tau}}^{2}\\hat{\\pi}\\theta_{\\tau}(a|s)A_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)d a+\\lambda I\\right]^{-1}\\mathbb{E}_{s\\sim b_{\\tau}^{\\hat{\\pi}_{\\phi}}}\\left[\\int_{A}\\nabla_{\\theta_{\\tau}^{\\tau}}\\hat{\\pi}\\theta_{\\tau}(a|s)\\nabla_{\\phi}^{\\top}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)d a+\\lambda I\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "First, we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\Vert\\nabla_{\\phi}J_{\\tau}(\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}})\\Vert=\\frac{1}{1-\\gamma}\\Vert\\nabla_{\\phi}\\theta_{\\tau}^{\\prime}\\Vert\\Vert\\underbrace{\\mathbb{E}}_{\\mathfrak{s}\\sim\\nu_{\\tau}^{\\prime}\\mathfrak{c}_{\\tau}^{\\prime}}\\left[\\nabla_{\\theta}f_{\\theta}(s,a)A_{\\tau}^{\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}}(s,a)\\right]\\Vert,\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{s\\sim\\hat{\\pi}_{\\sigma_{\\tau}^{\\tau}}(\\cdot\\vert s)}{\\mathbb{E}}~\\left[\\nabla_{\\theta}f_{\\theta}(s,a)A_{\\tau}^{\\hat{\\pi}_{\\theta^{\\prime}}}(s,a)\\right]\\Vert\\le\\Vert\\operatorname*{max}_{a,s}\\nabla_{\\theta}f_{\\theta}(s,a)\\Vert\\operatorname*{max}_{a,s}\\vert A_{\\tau}^{\\hat{\\pi}_{\\theta^{\\prime}}}(s,a)\\vert\\le L_{1}A_{m a x}.}\\\\ &{\\quad a\\sim\\hat{\\pi}_{\\theta_{\\tau}^{\\tau}}(\\cdot\\vert s)}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "For the term $\\nabla_{\\phi}\\theta_{\\tau}^{\\prime}$ , consider $\\nabla_{\\boldsymbol{\\theta}_{\\tau}^{\\prime}}\\,\\hat{\\pi}_{\\boldsymbol{\\theta}_{\\tau}^{\\prime}}\\left(\\boldsymbol{a}|\\boldsymbol{s}\\right)$ and $\\nabla_{\\theta_{\\tau}^{\\prime}}^{2}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)$ , we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}\\hat{\\pi}_{\\theta}(a|s)=\\hat{\\pi}_{\\theta}(a|s)\\nabla_{\\theta}f_{\\theta}(s,a)-\\hat{\\pi}_{\\theta}(a|s)\\frac{\\int_{A}\\nabla_{\\theta}f_{\\theta}(s,a^{\\prime})\\exp{(f_{\\theta}(s,a^{\\prime}))}d a^{\\prime}}{\\int_{A}\\exp{(f_{\\theta}(s,a^{\\prime}))}d a^{\\prime}}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Then, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{\\theta}\\hat{\\pi}_{\\theta}(a|s)\\|\\le\\hat{\\pi}_{\\theta}(a|s)\\|\\nabla_{\\theta}f_{\\theta}(s,a)\\|+\\hat{\\pi}_{\\theta}(a|s)\\left\\|\\frac{\\int_{A}\\nabla_{\\theta}f_{\\theta}(s,a^{\\prime})\\exp{(f_{\\theta}(s,a^{\\prime}))}d a^{\\prime}}{\\int_{A}\\exp{(f_{\\theta}(s,a^{\\prime}))}d a^{\\prime}}\\right\\|}\\\\ &{\\qquad\\qquad\\qquad\\le2\\hat{\\pi}_{\\theta}(a|s)L_{1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We also have $\\nabla_{\\theta}^{2}\\hat{\\pi}_{\\theta}(a|s)=$ ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\theta}\\hat{\\pi}_{\\theta}(a|s)\\nabla_{\\theta}^{\\top}f_{\\theta}(s,a)+\\hat{\\pi}_{\\theta}(a|s)\\nabla_{\\theta}^{2}f_{\\theta}(s,a)-\\nabla\\hat{\\pi}_{\\theta}(a|s)\\frac{\\int_{A}\\nabla_{\\theta}^{\\top}f_{\\theta}(s,a^{\\prime})\\exp{(f_{\\theta}(s,a^{\\prime}))}d a^{\\prime}}{\\int_{A}\\exp{(f_{\\theta}(s,a^{\\prime}))}d a^{\\prime}}}\\\\ &{-\\;\\hat{\\pi}_{\\theta}(a|s)\\frac{\\int_{A}\\nabla_{\\theta}^{2}f_{\\theta}(s,a^{\\prime})\\exp{(f_{\\theta}(s,a^{\\prime}))}d a^{\\prime}+\\nabla_{\\theta}f_{\\theta}(s,a^{\\prime})\\nabla_{\\theta}^{\\top}f_{\\theta}(s,a^{\\prime})\\exp{(f_{\\theta}(s,a^{\\prime}))}d a^{\\prime}}{\\int_{A}\\exp{(f_{\\theta}(s,a^{\\prime}))}d a^{\\prime}}}\\\\ &{+\\;\\hat{\\pi}_{\\theta}(a|s)\\frac{\\int_{A}\\nabla_{\\theta}f_{\\theta}(s,a^{\\prime})\\exp{(f_{\\theta}(s,a^{\\prime}))}d a^{\\prime}\\int_{A}\\nabla_{\\theta}^{\\top}f_{\\theta}(s,a^{\\prime})\\exp{(f_{\\theta}(s,a^{\\prime}))}d a^{\\prime}}{(\\int_{A}\\exp{(f_{\\theta}(s,a^{\\prime}))}d a^{\\prime})^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Then, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{\\theta}^{2}\\hat{\\pi}_{\\theta}(a|s)\\|\\le2\\hat{\\pi}_{\\theta}(a|s)L_{1}^{2}+\\hat{\\pi}_{\\theta}(a|s)L_{2}+2\\hat{\\pi}_{\\theta}(a|s)L_{1}^{2}+\\hat{\\pi}_{\\theta}(a|s)L_{2}+2\\hat{\\pi}_{\\theta}(a|s)L_{1}^{2}}\\\\ &{\\qquad\\qquad\\qquad=6\\hat{\\pi}_{\\theta}(a|s)L_{1}^{2}+2\\hat{\\pi}_{\\theta}(a|s)L_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "So, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left\\|\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\hat{\\pi}_{\\phi}}}\\left[\\int_{A}\\nabla_{\\theta_{\\tau}^{\\prime}}^{2}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)A_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)d a\\right]\\right\\|\\le(6L_{1}^{2}+2L_{2})A_{m a x}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Since $\\begin{array}{r}{\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\hat{\\pi}_{\\phi}}}\\left[\\int_{A}\\nabla_{\\theta_{\\tau}^{\\prime}}^{2}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)A_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)d a\\right]}\\end{array}$ is a diagonal matrix, the above shown its largest absolute eigenvalue is smaller than $(6L_{1}^{2}\\bar{~}+~2L_{2})A_{m a x}$ . Then, the smallest eigenvalue of refore, if ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\hat{\\tau}_{+}}}\\left[-\\int_{A}\\nabla_{\\theta_{\\tau}^{\\tau}}^{2}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)A_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)d a+\\lambda I\\right]\\mathrm{~is~larger~than~}\\lambda-(6L_{1}^{2}+2L_{2})A_{m a x}.\\mathrm{~The~}}\\\\ &{\\lambda>(6L_{1}^{2}+2L_{2})A_{m a x},}\\\\ &{\\qquad\\quad\\left\\|\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\hat{\\tau}_{+}}}\\left[-\\int_{A}\\nabla_{\\theta_{\\tau}^{\\prime}}^{2}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)A_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)d a+\\lambda I\\right]^{-1}\\right\\|\\leq\\frac{1}{\\lambda-(6L_{1}^{2}+2L_{2})A_{m a x}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Moreover, if $\\lambda>(6L_{1}^{2}{+}2L_{2})A_{m a x}$ , the objective function in the optimization problem $A l g(\\hat{\\pi}_{\\phi},\\lambda,\\tau)$ is strongly concave. Then, from [64], the solution is unique and $\\nabla_{\\phi}J_{\\tau}(A l g^{(3)}(\\hat{\\pi}_{\\phi},\\lambda,\\tau))$ exists. ", "page_idx": 35}, {"type": "text", "text": "From (6), ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\nabla_{\\phi}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)=\\frac{\\gamma}{1-\\gamma}\\cdot\\mathbb{E}_{(s^{\\prime},a^{\\prime})\\sim\\sigma_{\\tau,\\hat{\\pi}_{\\phi}}^{(s,a)}}\\left[\\nabla_{\\phi}f_{\\phi}\\left(s^{\\prime},a^{\\prime}\\right)A_{\\tau}^{\\hat{\\pi}_{\\phi}}\\left(s^{\\prime},a^{\\prime}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Then, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\|\\nabla_{\\phi}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)\\|\\leq\\frac{\\gamma}{1-\\gamma}L_{1}A_{m a x}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Combine (37), we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left\\|\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\hat{\\pi}_{\\phi}}}\\left[\\int_{\\mathcal{A}}\\nabla_{\\theta_{\\tau}^{\\prime}}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\nabla_{\\phi}^{\\top}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)d a+\\lambda I\\right]\\right\\|\\leq\\lambda+\\frac{2\\gamma}{1-\\gamma}L_{1}^{2}A_{m a x}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "So we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\lVert\\nabla_{\\phi}\\theta_{\\tau}^{\\prime}\\rVert\\leq\\frac{\\lambda+\\frac{2\\gamma}{1-\\gamma}L_{1}^{2}A_{m a x}}{(1-\\gamma)(\\lambda-(6L_{1}^{2}+2L_{2})A_{m a x})}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Therefore, we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\|\\nabla_{\\phi}J_{\\tau}(\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}})\\|\\leq\\frac{L_{1}A_{m a x}(\\lambda+\\frac{2\\gamma}{1-\\gamma}L_{1}^{2}A_{m a x})}{(1-\\gamma)(\\lambda-(6L_{1}^{2}+2L_{2})A_{m a x})}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Lemma 13. For a softmax policy parameterized by $\\phi$ , ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\|\\nabla_{\\phi}^{2}J_{\\tau}(\\hat{\\pi}_{\\phi})\\|\\leq\\frac{(6L_{1}^{2}+2L_{2})r_{m a x}}{(1-\\gamma)^{2}}+\\frac{8\\gamma L_{1}^{2}r_{m a x}}{(1-\\gamma)^{3}}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "equation", "text": "$$\n\\|\\nabla_{\\phi}^{2}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)\\|\\leq\\frac{8\\gamma^{2}L_{1}^{2}r_{m a x}}{(1-\\gamma)^{3}}+\\frac{\\gamma(6L_{1}^{2}+2L_{2})r_{m a x}}{(1-\\gamma)^{2}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof. From 37, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\int_{A}\\|\\nabla_{\\phi}\\hat{\\pi}_{\\phi}(a|s)\\|d a\\leq2L_{1}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "From 38, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\int_{A}\\|\\nabla_{\\phi}\\hat{\\pi}_{\\phi}(a|s)\\|d a\\leq6L_{1}^{2}+2L_{2}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Borrow the result from Lemma D.2 in [2], ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\|\\nabla_{\\phi}^{2}J_{\\tau}(\\hat{\\pi}_{\\phi})\\|\\leq\\frac{(6L_{1}^{2}+2L_{2})r_{m a x}}{(1-\\gamma)^{2}}+\\frac{8\\gamma L_{1}^{2}r_{m a x}}{(1-\\gamma)^{3}}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "equation", "text": "$$\n\\|\\nabla_{\\phi}^{2}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)\\|\\leq\\frac{8\\gamma^{2}L_{1}^{2}r_{m a x}}{(1-\\gamma)^{3}}+\\frac{\\gamma(6L_{1}^{2}+2L_{2})r_{m a x}}{(1-\\gamma)^{2}}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Lemma 14. Suppose that Assumption 2 holds. Let $\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}=\\mathcal{A}l g(\\hat{\\pi}_{\\phi},\\lambda,\\tau)$ where $D_{\\tau}=D_{\\tau,3}$ , for any $s\\in S$ , we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\int_{A}\\|\\nabla_{\\phi}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\|d a\\le\\frac{2L_{1}\\left(\\lambda+\\frac{2\\gamma}{1-\\gamma}L_{1}^{2}A_{m a x}\\right)}{(1-\\gamma)\\left(\\lambda-(6L_{1}^{2}+2L_{2})A_{m a x}\\right)}}}\\\\ &{\\int_{A}\\|\\nabla_{\\phi}^{2}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\|d a\\le\\frac{(160L_{1}^{3}+56L_{1}L_{2}+4L_{3})(\\lambda+\\frac{2\\gamma}{1-\\gamma}L_{1}^{2}A_{m a x})^{2}}{(1-\\gamma)^{3}(\\lambda-(6L_{1}^{2}+2L_{2})A_{m a x})^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "and ", "page_idx": 36}, {"type": "text", "text": "Proof. First consider $\\nabla_{\\phi}\\hat{\\pi}_{\\boldsymbol{\\theta}_{\\tau}^{\\prime}}(\\boldsymbol{a}|\\boldsymbol{s})$ , we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\nabla_{\\phi}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)=\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\nabla_{\\phi}\\theta_{\\tau}^{\\prime}\\nabla_{\\theta_{\\tau}^{\\prime}}f_{\\theta_{\\tau}^{\\prime}}(s,a)-\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\nabla_{\\phi}\\theta_{\\tau}^{\\prime}\\frac{\\int_{A}\\nabla_{\\theta_{\\tau}^{\\prime}}f_{\\theta_{\\tau}^{\\prime}}(s,a^{\\prime})\\exp{(f_{\\theta_{\\tau}^{\\prime}}(s,a^{\\prime}))}d a^{\\prime}}{\\int_{A}\\exp{(f_{\\theta_{\\tau}^{\\prime}}(s,a^{\\prime}))}d a^{\\prime}}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Then, ", "text_level": 1, "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{\\phi}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\|\\le\\!\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\|\\nabla_{\\phi}\\theta_{\\tau}^{\\prime}\\|\\|\\nabla_{\\theta_{\\tau}^{\\prime}}f_{\\theta_{\\tau}^{\\prime}}(s,a)\\|+}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\|\\nabla_{\\phi}\\theta_{\\tau}^{\\prime}\\|\\left\\|\\frac{\\int_{\\cal A}\\nabla_{\\theta_{\\tau}^{\\prime}}f_{\\theta_{\\tau}^{\\prime}}(s,a^{\\prime})\\exp{(f_{\\theta_{\\tau}^{\\prime}}(s,a^{\\prime}))}d a^{\\prime}}{\\int_{\\cal A}\\exp{(f_{\\theta_{\\tau}^{\\prime}}(s,a^{\\prime}))}d a^{\\prime}}\\right\\|}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\times2\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\frac{\\int_{\\cal A}\\hat{\\cal A}_{m a x}){\\cal A}_{1}}{(1-\\gamma)\\left(\\lambda-(6{\\cal L}_{1}^{2}+2{\\cal L}_{2}){\\cal A}_{m a x}\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Then, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\int_{A}\\|\\nabla_{\\phi}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\|d a\\leq\\frac{2L_{1}(\\lambda+\\frac{2\\gamma}{1-\\gamma}L_{1}^{2}A_{m a x})}{(1-\\gamma)(\\lambda-(6L_{1}^{2}+2L_{2})A_{m a x})}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Next, we consider $\\nabla_{\\phi}^{2}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)$ . From (42), we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\phi}^{2}\\widehat{\\pi}_{\\theta^{\\prime}}^{\\theta}(a|s)=\\nabla_{\\phi}\\theta_{\\tau}^{\\prime}\\nabla_{\\theta^{\\prime}}^{\\prime}f_{\\theta^{\\prime}}(s,a)\\nabla_{\\phi}^{\\top}\\widehat{\\pi}_{\\theta^{\\prime}}(a|s)+\\widehat{\\pi}_{\\theta^{\\prime}}(a|s)\\nabla_{\\phi}^{2}\\theta_{\\tau}^{\\prime}\\nabla_{\\theta^{\\prime}}f_{\\theta^{\\prime}}(s,a)}\\\\ &{+\\widehat{\\pi}_{\\theta^{\\prime}}(a|s)\\nabla_{\\phi}\\theta_{\\tau}^{\\prime}\\nabla_{\\theta^{\\prime}}^{2}f_{\\theta^{\\prime}}(s,a)\\nabla_{\\phi}^{\\top}\\theta_{\\tau}^{\\prime}-\\nabla_{\\phi}\\theta_{\\tau}^{\\prime}\\frac{\\int_{A}\\nabla_{\\theta^{\\prime}}^{\\prime}f_{\\theta^{\\prime}}(s,a^{\\prime})\\exp{(f_{\\theta^{\\prime}}^{\\prime}(s,a^{\\prime}))}d a^{\\prime}}{\\int_{A}\\exp{(f_{\\theta^{\\prime}}(s,a^{\\prime}))}d a^{\\prime}}\\nabla_{\\phi}^{\\top}\\widehat{\\pi}_{\\theta^{\\prime}}(a|s)}\\\\ &{-\\,\\widehat{\\pi}_{\\theta^{\\prime}}(a|s)\\nabla_{\\phi}^{2}\\theta_{\\tau}^{\\prime}\\frac{\\int_{A}\\nabla_{\\theta^{\\prime}}f_{\\theta^{\\prime}}(s,a^{\\prime})\\exp{(f_{\\theta^{\\prime}}(s,a^{\\prime}))}d a^{\\prime}}{\\int_{A}\\exp{(f_{\\theta^{\\prime}}(s,a^{\\prime}))}d a^{\\prime}}-\\widehat{\\pi}_{\\theta^{\\prime}}(a|s)\\nabla_{\\phi}\\theta_{\\tau}^{\\prime}}\\\\ &{\\underline{{\\int_{A}\\left(\\nabla_{\\theta^{\\prime}}^{2},f_{\\theta^{\\prime}}(s,a^{\\prime})\\exp{(f_{\\theta^{\\prime}}(s,a^{\\prime}))}\\right)+\\nabla_{\\theta^{\\prime}}\\widehat{\\mu}_{\\theta^{\\prime}}(s,a^{\\prime})\\nabla_{\\theta^{\\prime}}^{\\top}f_{\\theta^{\\prime}}(s,a^{\\prime})\\exp{(f_{\\theta^{\\prime}}(s,a^{\\prime}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Therefore, ", "text_level": 1, "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{\\phi}^{2}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\|\\leq\\|\\nabla_{\\phi}\\theta_{\\tau}^{\\prime}\\|\\|\\nabla_{\\theta_{\\tau}^{\\prime}}f_{\\theta_{\\tau}^{\\prime}}(s,a)\\|\\|\\nabla_{\\phi}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\|+\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\|\\nabla_{\\phi}^{2}\\theta_{\\tau}^{\\prime}\\|\\|\\nabla_{\\theta_{\\tau}^{\\prime}}f_{\\theta_{\\tau}^{\\prime}}(s,a)\\|}\\\\ &{+\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\|\\nabla_{\\phi}\\theta_{\\tau}^{\\prime}\\|^{2}\\|\\nabla_{\\theta_{\\tau}^{\\prime}}^{2}f_{\\theta_{\\tau}^{\\prime}}(s,a)\\|+\\|\\nabla_{\\phi}\\theta_{\\tau}^{\\prime}\\|\\|\\nabla_{\\theta_{\\tau}^{\\prime}}f_{\\theta_{\\tau}^{\\prime}}(s,a)\\|\\|\\nabla_{\\phi}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\|}\\\\ &{+\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\|\\nabla_{\\phi}^{2}\\theta_{\\tau}^{\\prime}\\|\\|\\nabla_{\\theta_{\\tau}^{\\prime}}f_{\\theta_{\\tau}^{\\prime}}(s,a)\\|+\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\|\\nabla_{\\phi}\\theta_{\\tau}^{\\prime}\\|^{2}(\\|\\nabla_{\\theta_{\\tau}^{\\prime}}^{2}f_{\\theta_{\\tau}^{\\prime}}(s,a)\\|+\\|\\nabla_{\\theta_{\\tau}^{\\prime}}f_{\\theta_{\\tau}^{\\prime}}(s,a)\\|^{2})}\\\\ &{+\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\|\\nabla_{\\phi}\\theta_{\\tau}^{\\prime}\\|^{2}\\|\\nabla_{\\theta_{\\tau}^{\\prime}}f_{\\theta_{\\tau}^{\\prime}}(s,a)\\|^{2}}\\\\ &{\\leq2L_{1}\\|\\nabla_{\\phi}\\theta_{\\tau}^{\\prime}\\|\\|\\nabla_{\\phi}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\|+2\\hat\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "From (40) and (43) ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\|\\nabla_{\\phi}^{2}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\|\\le\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\left(\\frac{2L_{2}+6L_{1}^{2}(\\lambda+\\frac{2\\gamma}{1-\\gamma}L_{1}^{2}A_{m a x})^{2}}{(1-\\gamma)^{2}(\\lambda-(6L_{1}^{2}+2L_{2})A_{m a x})^{2}}+2L_{1}\\|\\nabla_{\\phi}^{2}\\theta_{\\tau}^{\\prime}\\|\\right).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Then, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\int_{\\mathcal{A}}\\|\\nabla_{\\phi}^{2}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\|d a\\leq\\frac{2L_{2}+6L_{1}^{2}(\\lambda+\\frac{2\\gamma}{1-\\gamma}L_{1}^{2}A_{m a x})^{2}}{(1-\\gamma)^{2}(\\lambda-(6L_{1}^{2}+2L_{2})A_{m a x})^{2}}+2L_{1}\\|\\nabla_{\\phi}^{2}\\theta_{\\tau}^{\\prime}\\|.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Next, we consider $\\nabla_{\\phi}^{2}\\theta_{\\tau}^{\\prime}$ . We have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\phi}^{2}\\theta_{\\tau}^{\\prime}=\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\hat{\\tau}_{\\phi}}}\\left[-\\int_{A}\\nabla_{\\theta_{\\tau}^{\\prime}}^{2}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)d a+\\lambda I\\right]^{-1}}\\\\ &{\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\hat{\\pi}_{\\phi}}}\\left[\\int_{A}\\left(\\nabla_{\\theta_{\\tau}^{\\prime}}^{2}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\nabla_{\\phi}^{\\top}\\theta_{\\tau}^{\\prime}\\nabla_{\\phi}^{\\top}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)+\\nabla_{\\theta_{\\tau}^{\\prime}}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\nabla_{\\phi}^{2}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)\\right)d a\\right]-}\\\\ &{M\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\hat{\\tau}_{\\phi}}}\\left[-\\int_{A}\\left(\\nabla_{\\theta_{\\tau}^{\\prime}}^{3}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\nabla_{\\phi}\\theta_{\\tau}^{\\prime}A_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)+\\nabla_{\\theta_{\\tau}^{\\prime}}^{2}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\nabla_{\\phi}^{\\top}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)\\right)d a\\right]M^{-1}N}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{\\mathrm{where}\\quad\\;\\;M\\qquad}&{=\\qquad\\mathbb{E}_{s\\sim v_{\\tau}^{\\hat{\\tau}_{\\phi}}}\\left[-\\int_{A}\\nabla_{\\theta_{\\tau}^{\\prime}}^{2}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)A_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)d a+\\lambda I\\right]\\quad\\mathrm{~and~}\\quad\\;N\\qquad}&{=\\frac{2\\pi}{\\lambda}\\mathbb{E}_{s\\sim v_{\\tau}^{\\hat{\\tau}_{\\phi}}}\\left[\\int_{A}\\nabla_{\\theta_{\\tau}^{\\prime}}^{2}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\nabla_{\\phi}^{2}(a|s)\\nabla_{\\theta_{\\tau}^{\\prime}}^{2}(a|s)\\right]\\quad\\mathrm{~and~}\\quad\\;N\\in\\mathbb{Z},}\\\\ &{\\mathbb{E}_{s\\sim v_{\\tau}^{\\hat{\\tau}_{\\phi}}}\\left[\\int_{A}\\nabla_{\\theta_{\\tau}^{\\prime}}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\nabla_{\\phi}^{\\top}Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)d a+\\lambda I\\right].\\mathrm{~Also,~we~have~}M^{-1}N=\\nabla_{\\phi}\\theta_{\\tau}^{\\prime}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Similar to (37)(38), we can derive the upper bound of $\\left\\|\\nabla_{\\phi}^{3}\\hat{\\pi}_{\\phi}\\right\\|$ , then ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla_{\\boldsymbol{\\theta}_{\\tau}^{\\prime}}^{3}\\hat{\\pi}_{\\boldsymbol{\\theta}_{\\tau}^{\\prime}}(a|s)\\|\\le\\hat{\\pi}_{\\boldsymbol{\\theta}_{\\tau}^{\\prime}}(a|s)(40L_{1}^{3}+16L_{1}L_{2}+2L_{3}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "So, from (38)(39)(40)(41), we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\phi}^{2}\\theta_{\\tau}^{\\prime}\\|\\leq\\!\\frac{2\\gamma L_{1}^{2}A_{m a x}(6L_{1}^{2}+2L_{2})(\\lambda+\\frac{2\\gamma}{1-\\gamma}L_{1}^{2}A_{m a x})}{(1-\\gamma)^{2}(\\lambda-(6L_{1}^{2}+2L_{2})A_{m a x})^{2}}}\\\\ &{\\qquad\\qquad+\\left(\\frac{8\\gamma^{2}L_{1}^{2}r_{m a x}}{(1-\\gamma)^{3}}+\\frac{\\gamma\\left(6L_{1}^{2}+2L_{2}\\right)r_{m a x}}{(1-\\gamma)^{2}}\\right)\\frac{1}{\\lambda-\\left(6L_{1}^{2}+2L_{2}\\right)A_{m a x}}}\\\\ &{\\qquad\\qquad+\\frac{\\lambda+\\frac{2\\gamma}{1-\\gamma}L_{1}^{2}A_{m a x}}{(1-\\gamma)(\\lambda-(6L_{1}^{2}+2L_{2})A_{m a x})^{2}}(\\frac{(40L_{1}^{3}+16L_{1}L_{2}+2L_{3})(\\lambda+\\frac{2\\gamma}{1-\\gamma}L_{1}^{2}A_{m a x})A_{m a x}}{(1-\\gamma)(\\lambda-(6L_{1}^{2}+2L_{2})A_{m a x})}}\\\\ &{\\qquad\\qquad+\\frac{2\\gamma}{1-\\gamma}L_{1}(6L_{1}^{2}+2L_{2})A_{m a x}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Simplify the inequality by $\\gamma<1$ and $1-\\gamma<0$ , ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\|\\nabla_{\\phi}^{2}\\theta_{\\tau}^{\\prime}\\|\\leq{\\frac{(80L_{1}^{3}+28L_{1}L_{2}+2L_{3})(\\lambda+{\\frac{2\\gamma}{1-\\gamma}}L_{1}^{2}A_{m a x})^{2}}{(1-\\gamma)^{3}(\\lambda-(6L_{1}^{2}+2L_{2})A_{m a x})^{2}}}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Then, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\int_{\\cal A}\\|\\nabla_{\\phi}^{2}\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(a|s)\\|d a\\leq\\frac{(160L_{1}^{3}+56L_{1}L_{2}+4L_{3})(\\lambda+\\frac{2\\gamma}{1-\\gamma}L_{1}^{2}A_{m a x})^{2}}{(1-\\gamma)^{3}(\\lambda-(6L_{1}^{2}+2L_{2})A_{m a x})^{2}}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Lemma 15. Suppose that Assumptions 1, 2, and $^3$ hold. Let $\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}=\\mathcal{A}l g(\\hat{\\pi}_{\\phi},\\lambda,\\tau)$ where $D_{\\tau}=D_{\\tau,3}$ , we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\|\\nabla_{\\phi}^{2}J_{\\tau}(\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}})\\|\\leq\\frac{r_{m a x}B}{(1-\\gamma)^{2}}+\\frac{2\\gamma r_{m a x}C^{2}}{(1-\\gamma)^{3}},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where C =(1\u2212\u03b3)(\u03bb\u2212(61L\u221221\u03b3+2L2)Amax) and B = $\\begin{array}{r}{B=\\frac{(160L_{1}^{3}+56L_{1}L_{2}+4L_{3})(\\lambda+\\frac{2\\gamma}{1-\\gamma}L_{1}^{2}A_{m a x})^{2}}{(1-\\gamma)^{3}(\\lambda-(6L_{1}^{2}+2L_{2})A_{m a x})^{2}}.}\\end{array}$ ", "page_idx": 37}, {"type": "text", "text": "Proof. Similar to the proofs of Lemma 8 and Lemma 11 by using Lemma 14. ", "page_idx": 37}, {"type": "text", "text": "Theorem 7. In both discrete and continuous action space, consider the softmax policy with function approximation shown in Section 5.1, and the within-task algorithm $\\boldsymbol{A l g}$ is defined in (2) with $D_{\\tau}\\,=\\,D_{\\tau,3}$ . Suppose that Assumptions $I,\\,2,$ , and $^3$ hold. If $\\lambda>(6L_{1}^{2}+2L_{2})A_{m a x}$ , then $\\nabla_{\\phi}J_{\\tau}(A l g^{(3)}(\\hat{\\pi}_{\\phi},\\lambda,\\tau))$ exists for any $\\phi$ . ", "page_idx": 37}, {"type": "text", "text": "Let $\\{\\phi_{t}\\}_{t=1}^{T}$ be the sequence generated by Algorithm $^{\\,l}$ with $\\lambda>(6L_{1}^{2}+2L_{2})A_{m a x}$ and the step size ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\alpha=\\operatorname*{min}\\left\\{\\left(\\frac{r_{m a x}B}{(1-\\gamma)^{2}}+\\frac{2\\gamma r_{m a x}C^{2}}{(1-\\gamma)^{3}}\\right)^{-1},\\frac{1}{G\\sqrt{T}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Then, the following bound holds: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{T}\\frac{T}{\\operatorname*{tr}}\\mathbb{E}_{t}\\left[\\|\\nabla_{\\phi}\\mathbb{E}_{r\\sim\\mathbb{R}(T)}[J_{r}(A l g(\\hat{u}_{\\phi_{t}},\\lambda,\\tau))]\\|^{2}\\right]}\\\\ &{\\leq\\!\\left(\\frac{2r^{2}m\\alpha_{R}\\beta}{(1-\\gamma)^{3}}+\\frac{4\\gamma r^{2}m\\alpha_{R}^{2}C^{2}}{(1-\\gamma)^{4}}\\right)\\!\\frac{1}{T}+\\left(\\frac{2r_{m a x}}{1-\\gamma}+\\frac{r_{m a x}B}{(1-\\gamma)^{2}}+\\frac{2\\gamma r_{m a x}C^{2}}{(1-\\gamma)^{3}}\\right)\\!\\frac{G}{\\sqrt{T}},}\\\\ &{\\qquad\\qquad\\qquad\\quad G=\\frac{L_{1}A_{m a x}(\\lambda+\\frac{2\\gamma}{1-\\gamma}L_{1}^{2}M\\alpha_{m a x})}{(1-\\gamma)(\\lambda-(6L_{1}^{2}+2L_{2})A_{m a x})},}\\\\ &{\\qquad\\qquad\\qquad\\quad C=\\frac{2L_{1}(\\lambda+\\frac{2\\gamma}{1-\\gamma}L_{1}^{2}A_{m a x})}{(1-\\gamma)(\\lambda-(6L_{1}^{2}+2L_{2})A_{m a x})},}\\\\ &{\\qquad\\qquad\\qquad\\quad B=\\frac{(160L_{1}^{3}+56L_{1}L_{2}+4L_{3})(\\lambda+\\frac{2\\gamma}{1-\\gamma}L_{1}^{2}A_{m a x})^{2}}{(1-\\gamma)^{3}(\\lambda-(6L_{1}^{2}+2L_{2})A_{m a x})^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where ", "page_idx": 38}, {"type": "text", "text": "and ", "page_idx": 38}, {"type": "text", "text": "Proof. Similar to the proof of Theorem 5, by using the gradient bound in Lemma 12 and the smoothness in Lemma 15. \u53e3 ", "page_idx": 38}, {"type": "text", "text": "N Optimality of one-time policy adaptation ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "N.1 Important Lemmas ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Lemma 16. Suppose that Assumptions 1, 2 hold. For any task $\\tau$ , and any policies $\\pi$ and $\\pi^{\\prime}$ , the following bound holds: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\frac{1}{1-\\gamma}\\underset{a\\sim w^{\\prime}(\\cdot\\vert s)}{\\mathbb{E}}[A_{\\tau}^{\\pi}(s,a)]-C_{\\tau}^{\\pi}(\\pi^{\\prime})\\le J_{\\tau}(\\pi^{\\prime})-J_{\\tau}(\\pi)\\le\\frac{1}{1-\\gamma}\\underset{a\\sim w^{\\prime}(\\cdot\\vert s)}{\\mathbb{E}}[A_{\\tau}^{\\pi}(s,a)]+C_{\\tau}^{\\pi}(\\pi^{\\prime})\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where ", "page_idx": 38}, {"type": "equation", "text": "$$\nC_{\\tau}^{\\pi}(\\pi^{\\prime})=\\frac{4\\gamma A_{m a x}}{(1-\\gamma)^{2}}D_{T V}^{m a x}(\\pi||\\pi^{\\prime})\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\pi}}\\left[D_{T V}(\\pi(\\cdot|s)||\\pi^{\\prime}(\\cdot|s))\\right].\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Here, we define $\\begin{array}{r}{D_{T V}(\\pi(\\cdot|s)||\\pi^{\\prime}(\\cdot|s))\\;\\triangleq\\;\\frac{1}{2}\\sum_{a\\in A}|\\pi(a|s)-\\pi^{\\prime}(a|s)|}\\end{array}$ in a discrete action space or $\\begin{array}{r l r}{D_{T V}(\\pi(\\cdot|s)||\\pi^{\\prime}(\\cdot|s))}&{\\triangleq}&{\\frac{1}{2}\\int_{a\\in\\mathcal{A}}|\\pi(a|s)\\;-\\;\\pi^{\\prime}(a|s)|d a}\\end{array}$ in a continuous action space, and $D_{T V}^{m a x}(\\pi||\\pi^{\\prime})\\triangleq\\operatorname*{max}_{s\\in\\mathcal{S}}D_{T V}(\\pi(\\cdot|s)||\\pi^{\\prime}(\\cdot|s))$ . ", "page_idx": 38}, {"type": "text", "text": "Proof. Let $P_{\\tau}^{\\pi}$ is a matrix where $P_{\\tau}^{\\pi}(i,j)\\,=\\,\\mathbb{E}_{a\\sim\\pi(\\cdot|s_{i})}P_{\\tau}(s_{j}|s_{i},a)$ and $P_{\\tau}^{\\pi^{\\prime}}$ is a matrix where $P_{\\tau}^{\\pi^{\\prime}}(i,j)=\\mathbb{E}_{a\\sim\\pi^{\\prime}(\\cdot|s_{i})}P_{\\tau}(s_{j}|s_{i},a)$ . Let $G=(1+\\gamma P_{\\tau}^{\\pi}+(\\gamma P_{\\tau}^{\\pi})^{2}+...)=(1-\\gamma P_{\\tau}^{\\pi})^{-1}$ , and similarly $\\tilde{G}=(1+\\gamma P^{\\pi_{\\tau}^{\\prime}}+(\\gamma P^{\\pi_{\\tau}^{\\prime}})^{2}+...)=(1-\\gamma P^{\\pi_{\\tau}^{\\prime}})^{-1}$ . Let $\\rho$ be a density vector on state space and $r_{\\tau}$ is a reward function vector on state space, thus $r_{\\tau}^{\\top}\\rho$ is a scalar meaning the expected reward under density $\\rho$ . Note that $J_{\\tau}(\\pi)=r_{\\tau}^{\\top}G\\rho_{\\tau}$ , and $J_{\\tau}(\\dot{\\pi^{\\prime}})=r_{\\tau}^{\\top}\\tilde{G}\\rho_{\\tau}$ . Here, $\\rho_{\\tau}$ is the initial state distribution for task $\\tau$ . Let $\\Delta=P_{\\tau}^{\\pi^{\\prime}}-P_{\\tau}^{\\pi}$ . ", "page_idx": 38}, {"type": "text", "text": "Follow the proof in Appendix $\\mathbf{B}$ in [51], we have ", "page_idx": 38}, {"type": "equation", "text": "$$\nG^{-1}-\\tilde{G}^{-1}=(1-\\gamma P_{\\pi})-(1-\\gamma P_{\\tilde{\\pi}})=\\gamma\\Delta.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Left multiply by $\\tilde{G}$ and right multiply by $G$ , ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\tilde{G}=\\gamma\\tilde{G}\\Delta G+G.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Left multiply by $G$ and right multiply by $\\tilde{G}$ , ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\tilde{G}=\\gamma G\\Delta\\tilde{G}+G.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Substituting the right-hand side in (45) into $\\tilde{G}$ in (46), then ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\tilde{G}=G+\\gamma G\\Delta G+\\gamma^{2}G\\Delta\\tilde{G}\\Delta G.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "So we have ", "page_idx": 39}, {"type": "equation", "text": "$$\nJ_{\\tau}(\\pi^{\\prime})-J_{\\tau}(\\pi)=r_{\\tau}^{\\top}(\\tilde{G}-G)\\rho_{\\tau}=\\gamma r_{\\tau}^{\\top}G\\Delta G\\rho_{\\tau}+\\gamma^{2}r_{\\tau}^{\\top}G\\Delta\\tilde{G}\\Delta G\\rho_{\\tau}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Note that $r_{\\tau}^{\\top}G=v_{\\tau}^{\\pi^{\\top}}$ , where $v$ is the value function on the state space. We also have $\\begin{array}{r}{G\\rho_{\\tau}=\\frac{1}{1-\\gamma}\\nu_{\\tau}^{\\pi}}\\end{array}$ , where $\\nu_{\\tau}^{\\pi}$ is the state visitation distribution vector. So, ", "page_idx": 39}, {"type": "equation", "text": "$$\nJ_{\\tau}(\\tilde{\\pi})-J_{\\tau}(\\pi)=r_{\\tau}^{\\top}(\\tilde{G}-G)\\rho_{\\tau}=\\frac{\\gamma}{1-\\gamma}v_{\\tau}^{\\pi^{\\top}}\\Delta\\nu_{\\tau}^{\\pi}+\\frac{\\gamma^{2}}{1-\\gamma}v_{\\tau}^{\\pi^{\\top}}\\Delta\\tilde{G}\\Delta\\nu_{\\tau}^{\\pi}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Consider the first term $\\frac{\\gamma}{1-\\gamma}v_{\\tau}^{\\pi}{}^{\\top}\\Delta\\nu_{\\tau}^{\\pi}$ , similar to Equation (50) in [51], we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\gamma v_{\\tau}^{\\pi^{\\top}\\top}\\Delta\\nu_{\\tau}^{\\pi}=v_{\\tau}^{\\pi^{\\top}\\top}(P_{\\tau}^{\\pi^{\\prime}}-P_{\\tau}^{\\pi})\\nu_{\\tau}^{\\pi}}\\\\ &{=\\displaystyle\\sum_{s}\\nu_{\\tau}^{\\pi}(s)\\sum_{s^{\\prime}}\\sum_{a}(\\pi^{\\prime}(a|s)-\\pi(a|s))P_{\\tau}\\left(s^{\\prime}|s,a\\right)\\gamma v_{\\tau}^{\\pi}\\left(s^{\\prime}\\right)}\\\\ &{=\\displaystyle\\sum_{s}\\nu_{\\tau}^{\\pi}(s)\\sum_{a}(\\pi^{\\prime}(a|s)-\\pi(a|s))\\left[r(s)+\\displaystyle\\sum_{s^{\\prime}}P_{\\tau}\\left(s^{\\prime}|s,a\\right)\\gamma v_{\\tau}^{\\pi}\\left(s^{\\prime}\\right)-v(s)\\right]}\\\\ &{=\\displaystyle\\sum_{s}\\nu_{\\tau}^{\\pi}(s)\\sum_{a}(\\pi^{\\prime}(a|s)-\\pi(a|s))A_{\\tau}^{\\pi}(s,a)}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Since we have $\\begin{array}{r}{\\sum_{a}\\pi(a|s)A_{\\tau}^{\\pi}(s,a)=0}\\end{array}$ , we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\gamma{v_{\\tau}^{\\pi}}^{\\top}\\Delta\\nu_{\\tau}^{\\pi}=\\sum_{s}\\nu_{\\tau}^{\\pi}(s)\\sum_{a}\\pi^{\\prime}(a|s)A_{\\tau}^{\\pi}(s,a)=\\underset{a\\sim\\pi^{\\prime}({\\cdot}|s)}{\\mathbb{E}}\\left[A_{\\tau}^{\\pi}(s,a)\\right].\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Combine (47) and the above equation, we have the following for the second term: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\frac{\\gamma^{2}}{1-\\gamma}v_{\\tau}^{\\pi^{\\top}}\\Delta\\tilde{G}\\Delta\\nu_{\\tau}^{\\pi}=J_{\\tau}(\\pi^{\\prime})-J_{\\tau}(\\pi)-\\frac{1}{1-\\gamma}\\underset{a\\sim\\pi^{\\prime}(\\cdot\\vert s)}{\\mathbb{E}}\\left[A_{\\tau}^{\\pi}(s,a)\\right].\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Then we need to show ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\left|\\frac{\\gamma^{2}}{1-\\gamma}{v_{\\tau}^{\\pi}}^{\\top}\\Delta\\tilde{G}\\Delta\\nu_{\\tau}^{\\pi}\\right|\\leq C_{\\tau}^{\\pi}(\\pi^{\\prime}).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "First, by H\u00f6lder\u2019s inequality, ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\left|\\frac{\\gamma^{2}}{1-\\gamma}\\boldsymbol{v}_{\\tau}^{\\pi^{\\top}}\\Delta\\tilde{G}\\Delta\\boldsymbol{\\nu}_{\\tau}^{\\pi}\\right|\\leq\\frac{\\gamma}{1-\\gamma}\\|\\gamma\\boldsymbol{v}_{\\tau}^{\\pi^{\\top}}\\Delta\\|_{\\infty}\\|\\tilde{G}\\Delta\\boldsymbol{\\nu}_{\\tau}^{\\pi}\\|_{1}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Similar to (48), each element in the vector $\\gamma v_{\\tau}^{\\pi^{\\top}}\\Delta$ is $\\begin{array}{r}{\\sum_{a}(\\pi^{\\prime}(a|s)-\\pi(a|s))A_{\\tau}^{\\pi}(s,a)}\\end{array}$ , then we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\|\\gamma\\boldsymbol v_{\\tau}^{\\pi^{\\top}}\\Delta\\|_{\\infty}\\leq\\sum_{a}|\\pi^{\\prime}(a|s)-\\pi(a|s)|A_{\\tau}^{\\pi}(s,a)\\leq2A_{m a x}D_{T V}^{m a x}(\\pi||\\pi^{\\prime}).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "From the Lemma 3 of [1], we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\|\\tilde{G}\\Delta\\nu_{\\tau}^{\\pi}\\|_{1}\\leq\\frac{2}{1-\\gamma}\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\pi}}\\left[D_{T V}\\!\\left(\\pi(\\cdot|s)||\\pi^{\\prime}(\\cdot|s)\\right)\\right].\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Therefore, we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\left|\\frac{\\gamma^{2}}{1-\\gamma}\\boldsymbol{v}_{\\tau}^{\\pi}\\tau\\Delta\\tilde{G}\\Delta\\nu_{\\tau}^{\\pi}\\right|\\leq C_{\\tau}^{\\pi}(\\pi^{\\prime})=\\frac{4\\gamma A_{m a x}}{(1-\\gamma)^{2}}D_{T V}^{m a x}(\\pi||\\pi^{\\prime})\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\pi}}\\left[D_{T V}(\\pi(\\cdot|s)||\\pi^{\\prime}(\\cdot|s))\\right].\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Then the bounds hold. ", "page_idx": 39}, {"type": "text", "text": "Lemma 17. Suppose that Assumptions $I$ , 2 hold. For any task $\\tau$ , any bounded parameters $\\theta$ and $\\theta^{\\prime}$ , and $i=1$ or $2$ , the following bound holds for both $i=1$ and 2: ", "page_idx": 40}, {"type": "equation", "text": "$$\nJ_{\\tau}(\\hat{\\pi}_{\\theta^{\\prime}})-J_{\\tau}(\\hat{\\pi}_{\\theta})\\leq\\frac{1}{1-\\gamma}\\underset{\\underset{a\\sim\\hat{\\pi}_{\\theta^{\\prime}}(\\cdot\\vert s)}{s\\sim\\nu_{\\tau}^{\\hat{\\pi}_{\\theta}}}}{\\mathbb{E}}\\big[A_{\\tau}^{\\hat{\\pi}_{\\theta}}(s,a)\\big]+\\frac{2\\gamma A_{m a x}}{(1-\\gamma)^{2}\\epsilon}D_{\\tau,i}^{2}(\\hat{\\pi}_{\\theta},\\hat{\\pi}_{\\theta^{\\prime}})\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "and ", "page_idx": 40}, {"type": "equation", "text": "$$\nJ_{\\tau}(\\hat{\\pi}_{\\theta^{\\prime}})-J_{\\tau}(\\hat{\\pi}_{\\theta})\\geq\\frac{1}{1-\\gamma}\\underset{a\\sim\\hat{\\pi}_{\\theta^{\\prime}}(\\cdot\\vert s)}{\\mathbb{E}}~\\big[A_{\\tau}^{\\hat{\\pi}_{\\theta}}(s,a)\\big]-\\frac{2\\gamma A_{m a x}}{(1-\\gamma)^{2}\\epsilon}D_{\\tau,i}^{2}(\\hat{\\pi}_{\\theta},\\hat{\\pi}_{\\theta^{\\prime}}).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Proof. The proof follows similar lines of Theorem 1 in [51] and Corollary 1 and 2 in [1]. For the sake of self-containedness, we provide the complete proof. ", "page_idx": 40}, {"type": "text", "text": "We show the first inequality. The second inequality follows a similar way. From Lemma 16, ", "page_idx": 40}, {"type": "equation", "text": "$$\nI_{\\tau}(\\hat{\\pi}_{\\boldsymbol{\\theta}^{\\prime}})-J_{\\tau}(\\hat{\\pi}_{\\boldsymbol{\\theta}})-\\frac{1}{1-\\gamma}\\underset{a\\sim\\hat{\\pi}_{\\boldsymbol{\\theta}^{\\prime}}(\\cdot\\vert s)}{\\mathbb{E}}\\,\\left[A_{\\tau}^{\\hat{\\pi}_{\\boldsymbol{\\theta}}}(s,a)\\right]\\le\\frac{4\\gamma A_{m a x}}{(1-\\gamma)^{2}}D_{T V}^{m a x}(\\hat{\\pi}_{\\boldsymbol{\\theta}}\\vert\\vert\\hat{\\pi}_{\\boldsymbol{\\theta}^{\\prime}})\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\hat{\\pi}_{\\boldsymbol{\\theta}}}}\\left[D_{T V}(\\hat{\\pi}_{\\boldsymbol{\\theta}}(\\cdot\\vert s)\\vert\\vert\\hat{\\pi}_{\\boldsymbol{\\theta}^{\\prime}}(\\cdot\\vert s)\\vert\\right])\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "From Assumption 2, $\\nu^{\\hat{\\pi}_{\\theta}}(s)\\geq\\epsilon$ for any $s\\in A$ . Also, $D_{T V}(\\hat{\\pi}_{\\boldsymbol{\\theta}}(\\cdot|s)||\\hat{\\pi}_{\\boldsymbol{\\theta}^{\\prime}}(\\cdot|s))\\geq0$ for any $s\\in A$ . Then, we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\epsilon D_{T V}^{m a x}(\\hat{\\pi}_{\\boldsymbol{\\theta}}||\\hat{\\pi}_{\\boldsymbol{\\theta}^{\\prime}})\\leq\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\hat{\\pi}_{\\boldsymbol{\\theta}}}}\\left[D_{T V}(\\hat{\\pi}_{\\boldsymbol{\\theta}}(\\cdot|s)||\\hat{\\pi}_{\\boldsymbol{\\theta}^{\\prime}}(\\cdot|s))\\right].\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "From Jensen\u2019s inequality, we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\hat{\\pi}_{\\theta}}}\\left[D_{T V}(\\hat{\\pi}_{\\theta}(\\cdot|s)||\\hat{\\pi}_{\\theta^{\\prime}}(\\cdot|s))\\right]^{2}\\le\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\hat{\\pi}_{\\theta}}}\\left[D_{T V}^{2}(\\hat{\\pi}_{\\theta}(\\cdot|s)||\\hat{\\pi}_{\\theta^{\\prime}}(\\cdot|s))\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "From the above three inequalities, we have ", "page_idx": 40}, {"type": "equation", "text": "$$\nJ_{\\tau}(\\hat{\\pi}_{\\boldsymbol{\\theta}^{\\prime}})-J_{\\tau}(\\hat{\\pi}_{\\boldsymbol{\\theta}})-\\frac{1}{1-\\gamma}\\underset{a\\sim\\hat{\\pi}_{\\boldsymbol{\\theta}^{\\prime}}(\\cdot\\vert s)}{\\mathbb{E}}\\left[A_{\\tau}^{\\hat{\\pi}_{\\boldsymbol{\\theta}}}(s,a)\\right]\\leq\\frac{4\\gamma A_{m a x}}{(1-\\gamma)^{2}\\epsilon}\\mathbb{E}_{s\\sim\\nu_{\\tau}^{\\hat{\\pi}_{\\boldsymbol{\\theta}}}}\\left[D_{T V}^{2}(\\hat{\\pi}_{\\boldsymbol{\\theta}}(\\cdot\\vert s)\\vert\\vert\\hat{\\pi}_{\\boldsymbol{\\theta}^{\\prime}}(\\cdot\\vert s))\\right].\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "From [8], we have ", "page_idx": 40}, {"type": "text", "text": "and ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{D_{T V}^{2}(\\hat{\\pi}_{\\boldsymbol{\\theta}}(\\cdot|s)||\\hat{\\pi}_{\\boldsymbol{\\theta}^{\\prime}}(\\cdot|s))\\leq\\displaystyle\\frac{1}{2}D_{K L}\\big(\\hat{\\pi}_{\\boldsymbol{\\theta}}(\\cdot|s)||\\hat{\\pi}_{\\boldsymbol{\\theta}^{\\prime}}(\\cdot|s)\\big),}\\\\ {D_{T V}^{2}(\\hat{\\pi}_{\\boldsymbol{\\theta}}(\\cdot|s)||\\hat{\\pi}_{\\boldsymbol{\\theta}^{\\prime}}(\\cdot|s))\\leq\\displaystyle\\frac{1}{2}D_{K L}\\big(\\hat{\\pi}_{\\boldsymbol{\\theta}^{\\prime}}(\\cdot|s)||\\hat{\\pi}_{\\boldsymbol{\\theta}}(\\cdot|s)\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Therefore, ", "page_idx": 40}, {"type": "equation", "text": "$$\nJ_{\\tau}(\\hat{\\pi}_{\\theta^{\\prime}})-J_{\\tau}(\\hat{\\pi}_{\\theta})\\leq\\frac{1}{1-\\gamma_{\\underbrace{s\\sim\\nu_{\\tau}^{\\hat{\\pi}_{\\theta}}}_{a\\sim\\hat{\\pi}_{\\theta^{\\prime}}(\\cdot\\vert s)}}}\\left[A_{\\tau}^{\\hat{\\pi}_{\\theta}}(s,a)\\right]+\\frac{2\\gamma A_{m a x}}{(1-\\gamma)^{2}\\epsilon}D_{\\tau,1}^{2}(\\hat{\\pi}_{\\theta},\\hat{\\pi}_{\\theta^{\\prime}}),\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "and ", "page_idx": 40}, {"type": "equation", "text": "$$\nJ_{\\tau}(\\hat{\\pi}_{\\theta^{\\prime}})-J_{\\tau}(\\hat{\\pi}_{\\theta})\\leq\\frac{1}{1-\\gamma}\\underset{a\\sim\\hat{\\pi}_{\\theta^{\\prime}}(\\cdot\\vert s)}{\\mathbb{E}}\\big[A_{\\tau}^{\\hat{\\pi}_{\\theta}}(s,a)\\big]+\\frac{2\\gamma A_{m a x}}{(1-\\gamma)^{2}\\epsilon}D_{\\tau,2}^{2}(\\hat{\\pi}_{\\theta},\\hat{\\pi}_{\\theta^{\\prime}}).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Lemma 18. Consider the softmax policy with function approximation shown in Section 5.1. Suppose that Assumptions 1, 2, and 3 hold. For any task $\\tau$ , and any softmax policies parameterized by bounded $\\theta$ and $\\theta^{\\prime}$ , the following bound holds: ", "page_idx": 40}, {"type": "equation", "text": "$$\nJ_{\\tau}(\\hat{\\pi}_{\\theta^{\\prime}})-J_{\\tau}(\\hat{\\pi}_{\\theta})\\leq\\frac{1}{1-\\gamma}\\mathop{\\mathbb{E}}_{\\underset{a\\sim\\hat{\\pi}_{\\theta^{\\prime}}(\\cdot\\cdot\\vert s)}{\\leq\\sim\\hat{\\nu}_{\\theta^{\\prime}}(\\cdot\\vert s)}}\\bigl[A_{\\tau}^{\\hat{\\pi}_{\\theta}}(s,a)\\bigr]+\\frac{4\\gamma A_{m a x}L_{1}^{2}}{(1-\\gamma)^{2}\\epsilon}\\|\\theta-\\theta^{\\prime}\\|^{2}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "and ", "page_idx": 40}, {"type": "equation", "text": "$$\nJ_{\\tau}(\\hat{\\pi}_{\\theta^{\\prime}})-J_{\\tau}(\\hat{\\pi}_{\\theta})\\geq\\frac{1}{1-\\gamma}\\underset{a\\sim\\hat{\\pi}_{\\theta^{\\prime}}(\\cdot\\vert s)}{\\mathbb{E}}\\big[A_{\\tau}^{\\hat{\\pi}_{\\theta}}(s,a)\\big]-\\frac{4\\gamma A_{m a x}L_{1}^{2}}{(1-\\gamma)^{2}\\epsilon}\\Vert\\theta-\\theta^{\\prime}\\Vert^{2}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Proof. From (36), for any $\\theta\\in\\mathbb{R}^{n}$ , ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}\\hat{\\pi}_{\\theta}(a|s)=\\hat{\\pi}_{\\theta}(a|s)\\nabla_{\\theta}f_{\\theta}(s,a)-\\hat{\\pi}_{\\theta}(a|s)\\frac{\\int_{A}\\nabla_{\\theta}f_{\\theta}(s,a^{\\prime})\\exp{(f_{\\theta}(s,a^{\\prime}))}d a^{\\prime}}{\\int_{A}\\exp{(f_{\\theta}(s,a^{\\prime}))}d a^{\\prime}}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Then, ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{\\theta}\\hat{\\pi}_{\\theta}(a|s)\\|\\le\\hat{\\pi}_{\\theta}(a|s)\\|\\nabla_{\\theta}f_{\\theta}(s,a)\\|+\\hat{\\pi}_{\\theta}(a|s)\\left\\|\\frac{\\int_{A}\\nabla_{\\theta}f_{\\theta}(s,a^{\\prime})\\exp{(f_{\\theta}(s,a^{\\prime}))}d a^{\\prime}}{\\int_{A}\\exp{(f_{\\theta}(s,a^{\\prime}))}d a^{\\prime}}\\right\\|}\\\\ &{\\qquad\\qquad\\qquad\\le2\\hat{\\pi}_{\\theta}(a|s)L_{1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "From the mean value theorem, we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n|\\hat{\\pi}_{\\boldsymbol{\\theta}}(a|s)-\\hat{\\pi}_{\\boldsymbol{\\theta}^{\\prime}}(a|s)|\\le2\\hat{\\pi}_{\\boldsymbol{\\phi}(a)}(a|s)L_{1}\\|\\boldsymbol{\\theta}-\\boldsymbol{\\theta}^{\\prime}\\|,\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where $\\phi(a)=\\delta(a)\\theta+(1-\\delta(a))\\theta^{\\prime}$ and $0\\leq\\delta(a)\\leq1$ . So, ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\sum_{a\\in A}|\\hat{\\pi}_{\\boldsymbol{\\theta}}(a|s)-\\hat{\\pi}_{\\boldsymbol{\\theta}^{\\prime}}(a|s)|\\leq L_{1}\\|\\boldsymbol{\\theta}-\\boldsymbol{\\theta}^{\\prime}\\|.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "From (49), we have ", "page_idx": 41}, {"type": "equation", "text": "$$\nJ_{\\tau}(\\hat{\\pi}_{\\boldsymbol{\\theta}^{\\prime}})-J_{\\tau}(\\hat{\\pi}_{\\boldsymbol{\\theta}})-\\frac{1}{1-\\gamma}\\underset{\\underset{a\\sim\\hat{\\pi}_{\\boldsymbol{\\theta}^{\\prime}}(\\cdot\\vert s)}{\\mathbb{E}}}{\\mathbb{E}}\\big[A_{\\tau}^{\\hat{\\pi}_{\\boldsymbol{\\theta}}}(s,a)\\big]\\leq\\frac{4\\gamma A_{m a x}L_{1}^{2}}{(1-\\gamma)^{2}\\epsilon}\\Vert\\boldsymbol{\\theta}-\\boldsymbol{\\theta}^{\\prime}\\Vert^{2}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "We use the same way to show another inequality. ", "page_idx": 41}, {"type": "text", "text": "N.2 Proof of Theorems 3 and 4 ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Proof of Theorem 3. When the requirement of Theorem 1, $\\lambda\\geq2A_{m a x}$ , is satisfied, From Assumption 4 and Theorem 1, for both $i=1$ and 2, ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{T}\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}_{t}\\left[\\operatorname*{max}_{\\phi}\\mathbb{E}_{\\tau\\sim\\mathbb{P}(\\Gamma)}[J_{\\tau}(A l g^{(i)}(\\hat{\\pi}_{\\phi},\\lambda,\\tau))-\\mathbb{E}_{\\tau\\sim\\mathbb{P}(\\Gamma)}[J_{\\tau}(A l g^{(i)}(\\hat{\\pi}_{\\phi_{t}},\\lambda,\\tau))]]\\right]}\\\\ &{\\leq\\!\\frac{1}{T}\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}_{t}\\left[h_{i}\\left(\\|\\nabla_{\\phi}\\mathbb{E}_{\\tau\\sim\\mathbb{P}(\\Gamma)}[J_{\\tau}(A l g^{(i)}(\\hat{\\pi}_{\\phi_{t}},\\lambda,\\tau))]\\|^{2}\\right)\\right]}\\\\ &{\\leq\\!h_{i}\\left(\\frac{1}{T}\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}_{t}\\left[\\|\\nabla_{\\phi}\\mathbb{E}_{\\tau\\sim\\mathbb{P}(\\Gamma)}[J_{\\tau}(A l g^{(i)}(\\hat{\\pi}_{\\phi_{t}},\\lambda,\\tau))]\\|^{2}\\right]\\right)}\\\\ &{\\leq\\!h_{i}\\left(\\frac{K_{i}}{T}+\\frac{M_{i}}{\\sqrt{T}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where the constants $K_{i}$ and $M_{i}$ are shown in Theorem 1. The last inequality sign comes from that $h_{i}$ is a concave function and Jensen\u2019s inequality. ", "page_idx": 41}, {"type": "text", "text": "Let $\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(\\phi)=\\mathcal{A}l g^{(i)}(\\hat{\\pi}_{\\phi},\\lambda,\\tau)$ for any meta-parameter $\\phi$ . From the definition of the within-task algorithm, we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{a\\sim\\tilde{\\pi}_{\\theta_{\\tau}^{\\prime}}(\\phi)}{\\mathbb{E}}\\left[Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)\\right]-\\lambda D_{\\tau,i}^{2}(\\hat{\\pi}_{\\phi},\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(\\phi))\\geq\\underset{a\\sim\\tilde{\\pi}_{\\theta_{\\tau}^{\\prime}}(\\cdot\\vert s)}{\\mathbb{E}}\\left[Q_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)\\right]-\\lambda D_{\\tau,i}^{2}(\\hat{\\pi}_{\\phi},\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "This is equivalent to ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{a\\sim\\tilde{\\pi}_{\\theta_{\\tau}^{\\prime}}(\\phi)}{\\mathbb{E}}\\left[A_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)\\right]-\\lambda D_{\\tau,i}^{2}(\\hat{\\pi}_{\\phi},\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}(\\phi))\\geq\\underset{a\\sim\\tilde{\\pi}_{\\theta_{\\tau}^{\\prime}}(\\cdot\\vert s)}{\\mathbb{E}}\\left[A_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)\\right]-\\lambda D_{\\tau,i}^{2}(\\hat{\\pi}_{\\phi},\\hat{\\pi}_{\\theta_{\\tau}^{\\prime}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "when $\\begin{array}{r}{\\lambda\\geq\\frac{2\\gamma A_{m a x}}{(1-\\gamma)\\epsilon}}\\end{array}$ , from the second inequality in Lemma 17 and the above inequality, ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J_{\\tau}(\\hat{\\pi}_{\\boldsymbol{\\theta}_{\\tau}^{\\prime}}(\\boldsymbol{\\phi}))-J_{\\tau}(\\hat{\\pi}_{\\boldsymbol{\\phi}})\\geq\\!\\frac{1}{1-\\gamma}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\mathbb{E}_{\\boldsymbol{s}\\sim\\hat{\\pi}_{\\boldsymbol{\\tau}_{\\tau}^{\\boldsymbol{\\hat{\\pi}}{\\boldsymbol{\\phi}}}}^{\\mathbb{R}}}\\Big[A_{\\tau}^{\\hat{\\pi}_{\\boldsymbol{\\phi}}}(\\boldsymbol{s},\\boldsymbol{a})\\Big]-\\frac{2\\gamma A_{m a x}}{(1-\\gamma)^{2}\\epsilon}D_{\\tau,i}^{2}(\\hat{\\pi}_{\\boldsymbol{\\phi}},\\hat{\\pi}_{\\boldsymbol{\\theta}_{\\tau}^{\\prime}}(\\boldsymbol{\\phi}))}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "From the second inequality in Lemma 17, ", "page_idx": 42}, {"type": "equation", "text": "$$\nJ_{\\tau}(\\hat{\\pi}_{\\theta_{\\tau}^{*}})-J_{\\tau}(\\hat{\\pi}_{\\phi})\\leq\\frac{1}{1-\\gamma}\\underset{a\\sim\\hat{\\pi}_{\\theta_{\\tau}^{*}}(\\cdot\\vert s)}{\\mathbb{E}}\\Big[A_{\\tau}^{\\hat{\\pi}_{\\phi}}(s,a)\\Big]+\\frac{2\\gamma A_{m a x}}{(1-\\gamma)^{2}\\epsilon}D_{\\tau,i}^{2}(\\hat{\\pi}_{\\phi},\\hat{\\pi}_{\\theta_{\\tau}^{*}}).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "From the last two inequalities, ", "page_idx": 42}, {"type": "equation", "text": "$$\nJ_{\\tau}(\\hat{\\pi}_{\\boldsymbol{\\theta}_{\\tau}^{\\prime}}(\\phi))-J_{\\tau}(\\hat{\\pi}_{\\boldsymbol{\\theta}_{\\tau}^{*}})\\ge-(\\frac{2\\gamma A_{m a x}}{(1-\\gamma)^{2}\\epsilon}+\\frac{\\lambda}{1-\\gamma})D_{\\tau,i}^{2}(\\hat{\\pi}_{\\boldsymbol{\\phi}},\\hat{\\pi}_{\\boldsymbol{\\theta}_{\\tau}^{*}}),\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "i.e., ", "page_idx": 42}, {"type": "equation", "text": "$$\nJ_{\\tau}(\\hat{\\pi}_{\\boldsymbol{\\theta}_{\\tau}^{*}})-J_{\\tau}(A l g^{(i)}(\\hat{\\pi}_{\\boldsymbol{\\phi}},\\lambda,\\tau))\\leq(\\frac{2\\gamma A_{m a x}}{(1-\\gamma)^{2}\\epsilon}+\\frac{\\lambda}{1-\\gamma})D_{\\tau,i}^{2}(\\hat{\\pi}_{\\boldsymbol{\\phi}},\\hat{\\pi}_{\\boldsymbol{\\theta}_{\\tau}^{*}}).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Then, ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\tau\\sim\\mathbb{P}(\\Gamma)}[J_{\\tau}(\\hat{\\pi}_{\\theta_{\\tau}^{*}})-J_{\\tau}(A l g^{(i)}(\\hat{\\pi}_{\\phi},\\lambda,\\tau))]\\leq(\\frac{2\\gamma A_{m a x}}{(1-\\gamma)^{2}\\epsilon}+\\frac{\\lambda}{1-\\gamma})\\mathbb{E}_{\\tau\\sim\\mathbb{P}(\\Gamma)}[D_{\\tau,i}^{2}(\\hat{\\pi}_{\\phi},\\hat{\\pi}_{\\theta_{\\tau}^{*}})].\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Let $\\begin{array}{r}{\\phi^{*}=\\arg\\operatorname*{max}_{\\phi}\\mathbb{E}_{\\tau\\sim\\mathbb{P}(\\Gamma)}[J_{\\tau}(\\mathcal{A}l g^{(i)}(\\hat{\\pi}_{\\phi},\\lambda,\\tau))]}\\end{array}$ , we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\tau\\sim\\mathbb{P}(\\Gamma)}[J_{\\tau}(\\mathcal{A}l g^{(i)}(\\hat{\\pi}_{\\phi^{*}},\\lambda,\\tau))]\\ge\\operatorname*{max}_{\\phi}\\mathbb{E}_{\\tau\\sim\\mathbb{P}(\\Gamma)}[J_{\\tau}(\\mathcal{A}l g^{(i)}(\\hat{\\pi}_{\\phi},\\lambda,\\tau))].\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Therefore, ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\tau\\sim\\mathbb{P}(\\Gamma)}[J_{\\tau}(\\hat{\\pi}_{\\theta_{\\tau}^{*}})-J_{\\tau}(\\mathcal{A}l g^{(i)}(\\hat{\\pi}_{\\phi^{*}},\\lambda,\\tau))]\\leq\\underset{\\phi}{\\operatorname*{min}}\\mathbb{E}_{\\tau\\sim\\mathbb{P}(\\Gamma)}[J_{\\tau}(\\hat{\\pi}_{\\theta_{\\tau}^{*}})-J_{\\tau}(\\mathcal{A}l g^{(i)}(\\hat{\\pi}_{\\phi},\\lambda,\\tau))]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\underset{\\phi}{\\operatorname*{min}}(\\frac{2\\gamma A_{m a x}}{(1-\\gamma)^{2}\\epsilon}+\\frac{\\lambda}{1-\\gamma})\\mathbb{E}_{\\tau\\sim\\mathbb{P}(\\Gamma)}[D_{\\tau,i}^{2}(\\hat{\\pi}_{\\phi},\\hat{\\pi}_{\\theta_{\\tau}^{*}})]}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Since ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\phi}\\mathbb{E}_{\\tau\\sim\\mathbb{P}(\\Gamma)}[D_{\\tau,i}^{2}(\\hat{\\pi}_{\\phi},\\hat{\\pi}_{\\theta_{\\tau}^{*}})]=\\nu a r_{i}(\\mathbb{P}(\\Gamma)),\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\tau\\sim\\mathbb{P}(\\Gamma)}[J_{\\tau}(\\hat{\\pi}_{\\theta_{\\tau}^{*}})-J_{\\tau}(A l g^{(i)}(\\hat{\\pi}_{\\phi^{*}},\\lambda,\\tau))]\\leq(\\frac{2\\gamma A_{m a x}}{(1-\\gamma)^{2}\\epsilon}+\\frac{\\lambda}{1-\\gamma})\\gamma a r_{i}(\\mathbb{P}(\\Gamma)).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Note that in the above analysis, we need $\\lambda\\geq2A_{m a x}$ and also $\\begin{array}{r}{\\lambda\\geq\\frac{2\\gamma A_{m a x}}{(1-\\gamma)\\epsilon}}\\end{array}$ . So, we select we select $\\begin{array}{r}{\\lambda=\\frac{2A_{m a x}}{(1-\\gamma)\\epsilon}}\\end{array}$ to satisfy the requirement. When $\\begin{array}{r}{\\lambda=\\frac{2A_{m a x}}{(1-\\gamma)\\epsilon}}\\end{array}$ , we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\tau\\sim\\mathbb{P}(\\Gamma)}[J_{\\tau}(\\hat{\\pi}_{\\theta_{\\tau}^{*}})-J_{\\tau}(A l g^{(i)}(\\hat{\\pi}_{\\phi^{*}},\\lambda,\\tau))]\\leq\\frac{2(1+\\gamma)A_{m a x}}{(1-\\gamma)^{2}\\epsilon}\\gamma_{a r_{i}}(\\mathbb{P}(\\Gamma)).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "From (50) and (51) we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{T}\\sum_{t=1}^{T}\\mathbb{E}_{t}\\left[\\mathbb{E}_{\\tau\\sim\\mathbb{P}(\\Gamma)}[J_{\\tau}(\\hat{\\pi}_{\\theta_{\\tau}^{*}})-J_{\\tau}(A l g^{(i)}(\\hat{\\pi}_{\\phi_{t}},\\lambda,\\tau))]\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\leq h_{i}\\left(\\frac{K_{i}}{T}+\\frac{M_{i}}{\\sqrt{T}}\\right)+\\frac{2(1+\\gamma)A_{m a x}}{(1-\\gamma)^{2}\\epsilon}\\gamma a r_{i}(\\mathbb{P}(\\Gamma)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Proof of Theorem 4. Similar to the above proof of Theorem 3. The difference is using two inequalities in Lemma 18 instead of those in Lemma 17 and using Theorem 2 for convergence instead of Theorem 1. ", "page_idx": 43}, {"type": "text", "text": "The requirement of Theorem 2 is $\\lambda>(6L_{1}^{2}+2L_{2})A_{m a x}$ , and the requirement of Lemma 18 is $\\begin{array}{r}{\\lambda\\ge\\frac{4\\gamma A_{m a x}L_{1}^{2}}{(1-\\gamma)\\epsilon}}\\end{array}$ . Therefore, we select (6L21+2L2)Amax. Then, the bound is ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\cfrac{1}{T}\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}_{t}\\left[\\mathbb{E}_{\\tau\\sim\\mathbb{P}(\\Gamma)}[J_{\\tau}(\\hat{\\pi}_{\\theta_{\\tau}^{*}})-J_{\\tau}(A l g^{(3)}(\\hat{\\pi}_{\\phi_{t}},\\lambda,\\tau))]\\right]}\\\\ &{\\leq h_{3}\\left(\\frac{K_{3}}{T}+\\frac{M_{3}}{\\sqrt{T}}\\right)+\\left(\\frac{4\\gamma L_{1}^{2}A_{m a x}}{(1-\\gamma)^{2}\\epsilon}+\\frac{\\lambda}{1-\\gamma}\\right)\\gamma a r_{3}(\\mathbb{P}(\\Gamma)),}\\\\ &{\\leq h_{3}\\left(\\frac{K_{3}}{T}+\\frac{M_{3}}{\\sqrt{T}}\\right)+\\frac{((6+4\\gamma)L_{1}^{2}+2L_{2})A_{m a x}}{(1-\\gamma)^{2}\\epsilon}\\gamma a r_{3}(\\mathbb{P}(\\Gamma)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "N.3 Clarification of $A_{m a x}$ ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "In all the proofs in Sections N.1 and N.1, we can replace as $A_{m a x}$ to $A_{m a x}^{\\prime}$ , where $A_{m a x}^{\\prime}$ is defined by the maximum advantage function value of policy $\\hat{\\pi}_{\\phi^{\\prime}}$ , where $\\begin{array}{r}{\\phi^{\\prime}=\\arg\\operatorname*{min}_{\\phi}\\mathbb{E}_{\\tau\\sim\\mathbb{P}(\\Gamma)}[D_{\\tau,i}^{2}(\\hat{\\pi}_{\\phi},\\hat{\\pi}_{\\theta_{\\tau}^{*}})]}\\end{array}$ . It is easy to see $A_{m a x}^{\\prime}\\leq A_{m a x}$ . For simplification of the assumption statements, theorem statements, and convenience of the proofs, we keep $A_{m a x}$ in the proofs and Theorems 3 and 4. We actually can make the bound in Theorems 3 and 4 tighter by replacing $A_{m a x}$ to $A_{m a x}^{\\prime}$ . In the verification of the theoretical results of Section 6, we select $\\lambda$ based on $A_{m a x}^{\\prime}$ and verify the tighter bounds by the experiments. ", "page_idx": 43}, {"type": "text", "text": "O Proofs of Remarks ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Proof of part $(i)$ of Remark $^{\\,l}$ . If the MDP $\\boldsymbol{\\mathcal{M}}_{\\tau}$ is ergodic, there exists a policy $\\hat{\\pi}$ such that $\\nu_{\\tau}^{\\hat{\\pi}}(s)\\geq$ $\\epsilon_{0}$ . As $\\phi$ is bounded, the probability (or probability density) of each action of the softmax policy is larger than 0 and lower bounded by a $\\epsilon_{1}>0$ . Therefore, the action probability of the policy ${\\hat{\\pi}}(a|s)$ can be upper bounded by $\\hat{\\pi}_{\\phi}(a|s)/\\epsilon_{1}$ for any $a$ . Therefore, $\\nu_{\\tau}^{\\hat{\\pi}_{\\phi}}(s)\\geq\\epsilon_{0}/\\epsilon_{1}$ . \u53e3 ", "page_idx": 43}, {"type": "text", "text": "Proof of part (ii) of Remark $^{\\,l}$ . If the initial state distribution $\\rho_{\\tau}$ has $\\rho_{\\tau}(s)>0$ for any $s\\in S$ . Since $\\boldsymbol{S}$ is bounded, $\\rho_{\\tau}(s)\\geq\\epsilon_{2}$ for any $s\\in S$ . Then, $\\nu_{\\tau}^{\\hat{\\pi}_{\\phi}}(s)\\geq(1-\\gamma)\\epsilon_{2}$ . \u53e3 ", "page_idx": 43}, {"type": "text", "text": "P Limitations ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "In this paper, we provide several theorems, where the hyper-parameter selection, e.g., $\\lambda$ , is provided by the theorems. The theoretical analysis usually chooses hyper-parameters, which are sometimes conservative. In practice, we can tune them to improve the performance. ", "page_idx": 43}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: The abstract and introduction, including the main contribution statement and related works, accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 44}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Justification: We provide the limitations in the Appendix. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 44}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: All proofs are provided in Appendix Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 45}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: We provide all details of the information needed to reproduce the main experimental results in the experiment section and in Appendix A and B. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 45}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: We provide open access to the data and code with sufficient instructions in the supplemental material. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 46}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: We provide all training details in Appendix A and B. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 46}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: We provide it in the section of the experiment. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 46}, {"type": "text", "text": "", "page_idx": 47}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: We provide the information in the beginning of the Appendix. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 47}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: It is followed. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 47}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: This paper presents work whose goal is to advance the field of Machine Learning. There is no potential societal consequence. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 47}, {"type": "text", "text": "", "page_idx": 48}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 48}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 48}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. ", "page_idx": 48}, {"type": "text", "text": "\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 48}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 49}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 49}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 49}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 49}]