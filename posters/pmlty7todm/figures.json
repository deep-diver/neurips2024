[{"figure_path": "PmLty7tODm/figures/figures_2_1.jpg", "caption": "Figure 1: The IMN architecture.", "description": "The figure illustrates the architecture of the Interpretable Mesomorphic Network (IMN).  It shows a TabResNet backbone that feeds into a ResNet MLP with parameters \u03b8. This MLP acts as a hypernetwork, generating the interpretable linear model weights w(x;\u03b8) for a given input data point x. The weights w(x; \u03b8) are then used to generate the prediction \u0177 as a linear combination with the input x:  \u0177 = x<sup>T</sup>w(x; \u03b8). The figure visually distinguishes the different layers of the network using different colors and shapes for the nodes. The IMN architecture combines the accuracy of deep networks with the interpretability of linear models.", "section": "2 Proposed Method"}, {"figure_path": "PmLty7tODm/figures/figures_3_1.jpg", "caption": "Figure 2: Investigating the accuracy and interpretability of IMN. Left: The global decision boundary of our method that separates the classes correctly. Right: The local hyperplane pertaining to an example x' which correctly classifies the local example and retains a good global classification for the neighboring points.", "description": "The figure demonstrates the accuracy and interpretability of the proposed Interpretable Mesomorphic Networks (IMN).  The left panel shows the global decision boundary learned by IMN, illustrating its ability to accurately separate classes in a non-linear fashion. The right panel highlights the local interpretability aspect, showing a local hyperplane generated for a single data point (x'). This hyperplane not only correctly classifies x' but also generalizes well to its neighboring points, indicating that IMN generates accurate local linear models that offer good global classification performance.", "section": "Proof-of-concept: Globally Accurate and Locally Interpretable Classifiers"}, {"figure_path": "PmLty7tODm/figures/figures_6_1.jpg", "caption": "Figure 3: The critical difference diagram for the white-box interpretable methods. A lower rank indicates a better performance over datasets.", "description": "This figure shows the results of comparing several white-box interpretable methods (Decision Tree, Logistic Regression, and IMN) using the average rank across multiple datasets.  A lower average rank indicates superior performance.  The critical difference (CD) is shown to demonstrate statistically significant differences in performance between methods.", "section": "4.1 Predictive Accuracy Experiments"}, {"figure_path": "PmLty7tODm/figures/figures_6_2.jpg", "caption": "Figure 2: Investigating the accuracy and interpretability of IMN. Left: The global decision boundary of our method that separates the classes correctly. Right: The local hyperplane pertaining to an example x' which correctly classifies the local example and retains a good global classification for the neighboring points.", "description": "The figure demonstrates the accuracy and interpretability of the proposed Interpretable Mesomorphic Networks (IMN). The left panel shows the globally accurate non-linear decision boundary learned by IMN. The right panel illustrates the local interpretability by showing a local hyperplane generated for a single data point (x'). This hyperplane not only correctly classifies x' but also generalizes well to its neighboring points, highlighting the model's ability to provide both global accuracy and local interpretability simultaneously.", "section": "2.4 Proof-of-concept: Globally Accurate and Locally Interpretable Classifiers"}, {"figure_path": "PmLty7tODm/figures/figures_7_1.jpg", "caption": "Figure 5: Performance analysis of different interpretability methods over a varying degree of feature correlation p. We present the performance of all methods on faithfulness (ROAR), monotonicity (ROAR), faithfulness, and infidelity (ordered from left to right) on the Gaussian Linear dataset for p values ranging from [0, 1].", "description": "This figure displays the performance of various interpretability methods (IMN, SHAP, Kernel SHAP, LIME, Maple, L2X, BreakDown, Random, and TabNet) across four different metrics (Faithfulness (ROAR), Monotonicity (ROAR), Faithfulness, and Infidelity) on a Gaussian Linear dataset.  The x-axis represents the degree of feature correlation (p), ranging from 0 to 1. The y-axis shows the metric values for each method.  The figure helps to understand how each method's interpretability changes based on the level of feature correlation in the dataset.", "section": "2.4 Proof-of-concept: Globally Accurate and Locally Interpretable Classifiers"}, {"figure_path": "PmLty7tODm/figures/figures_8_1.jpg", "caption": "Figure 6: Investigating the decrease in AU-ROC when removing the k-th most important feature.", "description": "The figure shows a bar chart comparing the performance drop (in AUROC) of different models when removing the top-k most important features, as determined by each model's feature attribution method.  The x-axis represents the number of features removed (k). The y-axis represents the percentage decrease in AUROC.  The chart helps to assess the relative importance of features identified by each model by observing how much performance is affected when those features are removed.", "section": "Hypothesis 4: IMNs offer global (dataset-wide) interpretability of feature importance"}, {"figure_path": "PmLty7tODm/figures/figures_8_2.jpg", "caption": "Figure 8: Comparison of IMN against explainability techniques for image classification.", "description": "This figure compares the image classification results and visualizations of the proposed IMN method against several existing explainability techniques, including Gradient, Integrated Gradient, SmoothGrad, and DeepLift.  The results demonstrate that IMN generates higher weights for regions of the image that are most descriptive of the object.", "section": "A IMN can be extended to image classification backbones"}, {"figure_path": "PmLty7tODm/figures/figures_13_1.jpg", "caption": "Figure 8: Comparison of IMN against explainability techniques for image classification.", "description": "This figure compares the performance of IMN against other explainability techniques for image classification using the ResNet50 backbone.  It shows visualizations of feature attribution maps generated by various methods (Gradients, Integrated Gradients, SmoothGrad, DeepLift, and IMN) for three different images.  The color intensity in each map represents the importance assigned to each pixel in the prediction. This experiment demonstrates IMN's ability to identify relevant image regions for prediction in a manner comparable to other existing methods.", "section": "2.3 Explainability Through Feature Attribution"}, {"figure_path": "PmLty7tODm/figures/figures_13_2.jpg", "caption": "Figure 2: Investigating the accuracy and interpretability of IMN. Left: The global decision boundary of our method that separates the classes correctly. Right: The local hyperplane pertaining to an example x' which correctly classifies the local example and retains a good global classification for the neighboring points.", "description": "This figure shows two plots that illustrate the global accuracy and local interpretability of the Interpretable Mesomorphic Networks (IMN) model.  The left plot displays the global decision boundary learned by IMN, demonstrating its ability to accurately separate different classes in the dataset. The right plot focuses on a specific data point (x') and shows that IMN generates a local hyperplane (a linear decision boundary) that correctly classifies not only x', but also its neighboring data points. This highlights IMN's capability to achieve both global accuracy and local interpretability simultaneously.", "section": "2.4 Proof-of-concept: Globally Accurate and Locally Interpretable Classifiers"}, {"figure_path": "PmLty7tODm/figures/figures_14_1.jpg", "caption": "Figure 10: The gain distribution of the state-of-the-art models. The gain is calculated by dividing the test AUROC against the test AUROC of a decision tree.", "description": "This figure shows the distribution of the performance gain of different machine learning models compared to a decision tree. The gain is calculated as the difference between the AUROC score of a model and the AUROC score of a decision tree, divided by the AUROC score of the decision tree.  The box plot shows the median, quartiles, and outliers for each model.  The figure demonstrates that most models show similar gains, but NAM (Neural Additive Model) performs considerably worse than the other models.", "section": "5 Experiments and Results"}, {"figure_path": "PmLty7tODm/figures/figures_14_2.jpg", "caption": "Figure 5: Performance analysis of different interpretability methods over a varying degree of feature correlation p. We present the performance of all methods on faithfulness (ROAR), monotonicity (ROAR), faithfulness, and infidelity (ordered from left to right) on the Gaussian Linear dataset for p values ranging from [0, 1].", "description": "This figure displays the results of the interpretability experiment. The experiment compares IMN against other interpretability methods in the presence of feature correlations in the Gaussian Linear Dataset. The plot shows the performance of all methods on four metrics: faithfulness (ROAR), monotonicity (ROAR), faithfulness and infidelity. The performance is evaluated for different values of feature correlation, ranging from 0 to 1.", "section": "2.3 Explainability Through Feature Attribution"}, {"figure_path": "PmLty7tODm/figures/figures_15_1.jpg", "caption": "Figure 9: The critical difference diagrams that represent the average rank over all datasets based on the AUROC test performance for: left) The white-box methods and IMN, middle) The black-box methods and IMN for the binary classification datasets, right) The black-box methods and IMN for the entire benchmark of datasets.", "description": "This figure compares the performance of different classification models using the AUROC metric.  It shows critical difference diagrams, which visually represent the statistical significance of differences in average rank between various models. The diagrams are presented separately for white-box models, black-box models on binary classification datasets, and black-box models across all datasets in the benchmark. This allows a comparison of the relative performance and statistical significance of IMN against other interpretable and non-interpretable models in different experimental settings.", "section": "Experiments and Results"}]