[{"type": "text", "text": "Interpretable Mesomorphic Neural Networks For Tabular Data ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Arlind Kadra ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sebastian Pineda Arango ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Department of Representation Learning University of Freiburg kadraa@cs.uni-freiburg.de ", "page_idx": 0}, {"type": "text", "text": "Department of Representation Learning University of Freiburg pineda@cs.uni-freiburg.de ", "page_idx": 0}, {"type": "text", "text": "Josif Grabocka Department of Machine Learning University of Technology Nuremberg josif.grabocka@utn.de ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Even though neural networks have been long deployed in applications involving tabular data, still existing neural architectures are not explainable by design. In this work, we propose a new class of interpretable neural networks for tabular data that are both deep and linear at the same time (i.e. mesomorphic). We optimize deep hypernetworks to generate explainable linear models on a per-instance basis. As a result, our models retain the accuracy of black-box deep networks while offering free lunch explainability for tabular data by design. Through extensive experiments, we demonstrate that our explainable deep networks have comparable performance to state-of-the-art classifiers on tabular data and outperform current existing methods that are explainable by design. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Tabular data are arguably the most widely spread traditional data modality arising in a plethora of real-world application domains (Bischl et al., 2021; Borisov et al., 2022). There exists a recent trend to deploy neural networks for predictive tasks on tabular data (Kadra et al., 2021; Gorishniy et al., 2021; Somepalli et al., 2022; Hollmann et al., 2023). In a series of such application realms, it is important to be able to explain the predictions of deep learning models to humans (Ras et al., 2022), especially when interacting with human decision-makers, such as in healthcare (Gulum et al., 2021; Tjoa & Guan, 2021), or the financial sector (Sadhwani et al., 2020). Heavily parametrized models such as deep neural networks can fti complex interactions in tabular datasets and achieve high predictive accuracy, however, they are not explainable. In that context, achieving both high predictive accuracy and explainability remains an open research question for the Machine Learning community. ", "page_idx": 0}, {"type": "text", "text": "In this work, we introduce mesomorphic neural architectures1, a new class of deep models that are both deep and locally linear at the same time, therefore, offering interpretability by design. In a nutshell, we propose a new architecture that is simultaneously (i) deep and accurate, as well as (ii) linear and explainable on a per-instance basis. Technically speaking, we learn deep hypernetworks that generate linear models that are accurate concerning the data point we are interested in explaining. ", "page_idx": 0}, {"type": "text", "text": "Our interpretable mesomorphic networks for tabular data (dubbed IMN) are classification or regression models that identify the relevant tabular features by design. It is important to highlight that this work tackles explaining predictions for a single data point (Lundberg & Lee, 2017), instead of explaining a model globally for the whole dataset (Ras et al., 2022). Similarly to existing prior works (AlvarezMelis & Jaakkola, 2018; Chen et al., 2018), we train deep models that generate explainable local models for a data sample of interest. In contrast, we train hypernetworks that generate linear models in the original feature space through a purely supervised end-to-end optimization. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "We empirically show that the proposed explainable deep models are both as accurate as existing blackbox classifiers for tabular datasets and achieve better performance compared to explainable end-to-end prior methods. At the same time, IMN is as interpretable as explainer techniques. Throughout this work, explainers can be categorized into two groups: i) interpretable surrogate models that are trained to approximate black-box models (Lundberg & Lee, 2017), and ii) end-to-end explainable methods by design. Concretely, we show that our method achieves comparable accuracy to competitive black-box classifiers and manages to outperform current state-of-the-art end-to-end explainable methods on the tabular datasets of the popular AutoML benchmark (Gijsbers et al., 2019). In addition, we compare our technique against state-of-the-art predictive explainers on the recent XAI explainability benchmark for tabular data (Liu et al., 2021) and empirically demonstrate that our method offers competitive interpretability. As a result, our method represents a significant step forward in making deep learning explainable by design for tabular datasets. Overall, this work offers the following contributions: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We present a technique that makes deep learning explainable by design via training hypernetworks to generate instance-specific linear models. \u2022 We offer ample empirical evidence that our method is as accurate as black-box classifiers, with the benefit of being as interpretable as state-of-the-art prediction explainers. ", "page_idx": 1}, {"type": "text", "text": "2 Proposed Method ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Shallow Interpretability through Deep Hypernetworks ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Let us denote a tabular dataset consisting of $N$ instances of $M$ -dimensional features as $X\\in\\mathbb{R}^{N\\times M}$ and the $C$ -dimensional categorical target variable as $Y\\in\\left\\lbrace1,\\dots,C\\right\\rbrace^{N}$ . A model with parameters $w\\in\\mathscr{W}$ estimates the target variable as $f:\\mathbb{R}^{M}\\ \\times\\ \\mathcal{W}\\ \\to\\ \\mathbf{\\dot{R}}^{C}$ and is optimized by minimizing the empirical risk arg $\\begin{array}{r}{\\operatorname*{min}_{w\\in\\mathcal{W}}\\sum_{n=1}^{N}\\mathcal{L}\\left(y_{n},f(x_{n};w)\\right)}\\end{array}$ , where $\\mathcal{L}:\\{1,\\dots,C\\}\\times\\mathbb{R}^{C}\\rightarrow\\mathbb{R}_{+}$ is a loss function. An explainable model $f$ is one whose predictions ${\\hat{y}}_{n}=f(x_{n};w)$ for a data point $x_{n}$ are interpretable by humans. For instance, linear models and decision trees are commonly accepted to be interpretable by Machine Learning practitioners (Ribeiro et al., 2016; Lundberg & Lee, 2017). ", "page_idx": 1}, {"type": "text", "text": "In this work, we rethink shallow interpretable models $f(x_{n};w)$ by defining their parameters $w\\in\\mathscr{W}$ to be the output of deep non-interpretable hypernetworks $w(\\dot{x_{n}};\\theta):\\mathbb{R}^{\\bar{M}}\\times\\dot{\\Theta_{\\rightarrow}}\\mathbf{\\Psi}\\mathcal{W}$ , where the parameters of the hypernetwork are $\\theta\\in\\Theta$ . We remind the reader that a hypernetwork (a.k.a. metanetwork, or \"network of networks\") is a neural network that generates the parameters of another network (Ha et al., 2017). In this mechanism, we train deep non-interpretable hypernetworks to generate interpretable models $f$ in an end-to-end manner as a $\\begin{array}{r l}&{\\dot{\\mathrm{rg}}\\operatorname*{min}_{\\theta\\in\\Theta}\\dot{\\sum_{n=1}^{N}}\\mathcal{L}\\left(\\bar{y_{n}},f(x_{n};w(x_{n};\\bar{\\theta}))\\right)}\\end{array}$ ", "page_idx": 1}, {"type": "text", "text": "2.2 Interpretable Mesomorphic Networks (IMN) ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Our method trains deep Multi-Layer Perceptron (MLP) hypernetworks that generate the parameters of linear models. For the case of multi-class classification, we consider linear models with parameters $\\boldsymbol{w}\\ \\in\\ \\mathbb{R}^{C\\times(M+1)}$ , denoting one set of weights and bias terms per class, as $f\\left(x_{n};w\\right)_{c}\\,=\\,e^{z\\left(x_{n};w\\right)_{c}}/\\sum_{k=1}^{C}e^{z\\left(x_{n};w\\right)_{k}}$ , with $\\begin{array}{r}{z\\left(x_{n};w\\right)_{c}=\\bar{\\sum_{m=1}^{M}}w_{c,m}x_{n,m}+\\bar{w_{c,0}}}\\end{array}$ representing the logit predictions  for the $c$ -th class. For the case of re gression the linear model is simply $\\begin{array}{r}{\\overbar{f}\\left(x_{n};w\\right)=\\overbar{\\sum_{m=1}^{M}{w_{m}x_{n,m}}}+w_{0}}\\end{array}$ with $w\\in\\mathbb{R}^{M+1}$ . ", "page_idx": 1}, {"type": "text", "text": "Let us present our method IMN by starting with the case of multi-class classification following the hypernetwork mechanism explained in Section 2.1. The hypernetwork $w(x_{n};\\theta):\\mathbb{R}^{M}\\times\\Theta\\stackrel{}{\\rightarrow}$ $\\mathbb{R}^{C\\times(M+1)}$ with parameters $\\theta\\,\\in\\,\\Theta$ is a function that given a data point $x_{n}\\,\\in\\,\\mathbb{R}^{M}$ generates the predictions as: ", "page_idx": 1}, {"type": "text", "text": "Instead of training weights $w$ as in a standard linear classification, we use the output of an MLP network as the linear weights $w(x_{n};\\theta)$ . We illustrate the architecture of our mesomorphic network in Figure 1. In the case of regression, our linear model with hypernetworks is $f(x_{n};{\\bar{w}}(x_{n};\\theta))=$ $\\begin{array}{r}{\\sum_{m=1}^{M}w\\left(x_{n};\\boldsymbol{\\theta}\\right)_{m}x_{n,m}\\,+\\,w\\left(x_{n};\\boldsymbol{\\theta}\\right)_{0}}\\end{array}$ . We highlight that our experimental protocol (Section 4) includes both classification and regression datasets. ", "page_idx": 2}, {"type": "text", "text": "Ultimately, we train the optimal parameters of the hypernetwork to minimize the following loss in an end-to-end manner: $\\begin{array}{r l}{\\underset{\\theta\\in\\Theta}{\\arg\\operatorname*{min}}\\sum_{n=1}^{\\bar{N}}\\mathcal{L}\\left(y_{n},f\\left(x_{n};\\boldsymbol{\\omega}\\right)\\right))+\\lambda||\\boldsymbol{w}\\left(x_{n};\\theta\\right)||_{1}.}\\end{array}$ ", "page_idx": 2}, {"type": "text", "text": "Our hypernetworks generate interpretable models that are accurate concerning a data point of interest (e.g. \"Explain why patient $x_{n}$ is estimated to have cancer $f(x_{n};w(x_{n};\\theta))\\;>\\;0.5$ by analyzing the impact of features using the generated linear weights.\"). We stress that our novel method IMN does not simply train one linear model per data point, contrary to prior work (Ribeiro et al., 2016). Instead, the hypernetwork learns to generate accurate linear models by a shared network across all data points. As a result, generating the linear weights demands a single forward pass through the hypernetwork, rather than a separate optimization procedure. Furthermore, our method intrinsically learns to generate similar linear hyperplanes for neighboring data instances. The outputted linear models are accurate both in correctly classifying the data point $x_{n}$ , but also for the other majority of training instances in the neighborhood (see proof-of-concept experiment below). The outcome is a linear model with parameters $w(x_{n};\\theta)$ ", "page_idx": 2}, {"type": "image", "img_path": "PmLty7tODm/tmp/467f750de23e8c9bcf04bb8a6a1a81a3a011373cdcaf138f8305ce0fccfaa4fc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "that both interprets the prediction, but also serves as an accurate local model for the neighborhood of points. ", "page_idx": 2}, {"type": "text", "text": "2.3 Explainability Through Feature Attribution ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The generated linear models $w\\left(x_{n};\\theta\\right)$ can be used to explain predictions through feature attribution (i.e. feature importance) (Liu et al., 2021). It is important to re-emphasize that our method offers interpretable predictions for the estimated target $f(x_{n};w\\left(x_{n};\\theta\\right))$ of a particular data point $x_{n}$ . Concretely, we can analyse the linear coefficients $\\{w(x_{n};\\theta)_{1},\\dots,w(x_{n};\\theta)_{M}\\}$ to distill the importances of $\\{x_{n,1},\\ldots,x_{n,M}\\}$ by measuring the residual impact on the target. The impact of the $m$ -th feature $x_{n,m}$ in estimating the target variable, is proportional to the change in the estimated target if we remove the feature (Hooker et al., 2019). Considering our linear models, the impact of the $m$ -th feature is proportional to the change of the predicted target if we set the $m$ -th feature to zero. In terms of notation, we multiply the feature vector element-wise with a Kronecker delta vector $\\delta_{i}^{m}=\\mathbb{1}_{m\\neq i}$ . ", "page_idx": 2}, {"type": "equation", "text": "$$\nf(x_{n};w\\left(x_{n};\\theta\\right))-f(x_{n}\\odot\\delta^{m};w\\left(x_{n};\\theta\\right))\\propto w(x_{n};\\theta)_{m}\\,x_{n,m}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "As a result, our feature attribution strategy is that the $m$ -th feature impacts the prediction of the target variable by a signed magnitude of $\\bar{w}(x_{n};\\theta)_{m}\\,x_{n,m}$ . In our experiments, all the features are normalized to the same mean and variance, therefore, the magnitude $w(x_{n};\\theta)_{m}\\,x_{n,m}$ can be directly used to explain the impact of the $m$ -th feature. In cases where the unsigned importance is required, a practitioner can use the absolute impact $\\vert w(x_{n};\\theta)_{m}\\,x_{n,m}\\vert$ as the attribution. Furthermore, to measure the global importance of the $m$ -th feature for the whole dataset, we can compute $\\begin{array}{r}{\\frac{1}{N}\\sum_{n=1}^{N}|w(x_{n};\\theta)_{m}\\,\\bar{x}_{n,m}|}\\end{array}$ . ", "page_idx": 2}, {"type": "image", "img_path": "PmLty7tODm/tmp/d56c266ecd489fa79dd2c8e667da0adb03ee6b6812fb7b8034a371f237ecc000.jpg", "img_caption": ["Figure 2: Investigating the accuracy and interpretability of IMN. Left: The global decision boundary of our method that separates the classes correctly. Right: The local hyperplane pertaining to an example $x^{\\prime}$ which correctly classifies the local example and retains a good global classification for the neighboring points. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "2.4 Proof-of-concept: Globally Accurate and Locally Interpretable Classifiers ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As a proof of concept, we run our method on the half-moon toy task that consists of a 2-dimensional tabular dataset in the form of two half-moons that are not linearly separable. ", "page_idx": 3}, {"type": "text", "text": "Initially, we investigate the global accuracy of our method. As shown in Figure 2 (left), our method correctly classifies all the examples. Furthermore, our method learns an optimal non-linear decision boundary that separates the classes (plotted in green). To determine the decision boundary, we perform a fine-grid prediction on all possible combinations of $x_{1}$ and $x_{2}$ . Subsequently, we identify the points that exhibit the minimal prediction distance to a probability prediction of 0.5. Lastly, in Figure 2 (right) we investigate the local interpretability of our method, by taking a point $x^{\\prime}$ and calculating the corresponding weights $\\left(w\\left(x^{\\prime}\\right),\\stackrel{-}{w}\\left(x^{\\prime}\\right)_{0}\\right)$ generated by our hypernetwork, where we omited the dependence on $\\theta$ for simplicity. The black line shows all the points that reside on the hyperplane $w(x^{\\prime})$ as $\\left\\{x\\,|\\,w\\left(x^{\\prime}\\right)^{T}\\,x\\,\\bar{+}\\,w_{0}\\,(x^{\\prime})=0\\right\\}$ . It is important to highlight that the local hyperplane does not only correctly classify the point $x^{\\prime}$ , but also the neighboring points, retaining an accurate linear classifier for the neighborhood of points. ", "page_idx": 3}, {"type": "text", "text": "To validate our claim that the per-example (local) hyperplane correctly classifies neighboring points, we conduct the following analysis: For every datapoint $x_{n}$ we take a specific number of nearest neighbor examples from every class, and we evaluate the classification accuracy of the hyperplane generated for the datapoint $x_{n}$ on the set of all neighbors. We repeat the above procedure with varying neighborhood sizes and we present the results in Table 1. The results indicate that the mesomorphic neural network generates hyperplanes that are accurate in the neighborhood of the point whose prediction we are interested in explaining. ", "page_idx": 3}, {"type": "table", "img_path": "PmLty7tODm/tmp/efba2f6d7c3fce6c697e85abb5503401d23997db946f434bc80b14cd336d623a.jpg", "table_caption": ["Table 1: Accuracy of local hyperplanes for neighboring points. "], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "3 Related Work ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Interpretable Models by Design: There exist Machine Learning models that offer interpretability by default. A standard approach is to use linear models (Tibshirani, 1996; Efron et al., 2004; Berkson, 1953) that assign interpretable weights to each of the input features. On the other hand, decision trees (Loh, 2011; Craven & Shavlik, 1995) use splitting rules that build up leaves and intermediate nodes. Every leaf node is associated with a predicted label, making it possible to follow the rules that led to a specific prediction. Bayesian methods such as Naive Bayes (Murphy et al., 2006) or Bayesian Neural Networks (Friedman et al., 1997) provide a framework for reasoning on the interactions of prior beliefs with evidence, thus simplifying the interpretation of probabilistic outputs. Instance-based models allow experts to reason about predictions based on the similarity to the train samples. The prediction model aggregates the labels of the neighbors in the training set, using the average of the top-k most similar samples (Freitas, 2014; Kim et al., 2015), or decision functions extracted from prototypes Martens et al. (2007). Attention-based models like TabNet (Arik & Pfister, 2021) make use of sequential attention to generate feature weights on a per-instance basis, while, DANet (Chen et al., 2022) generates global importance weights for both the raw input features and higher order concepts. Neural additive models (NAMs) (Agarwal et al., 2021) use a neural network per feature to model the additive function of individual features to the output. However, these models trade-off the performance for the sake of interpretability, therefore challenging their usage on applications that need high performance. A prior similar work also trains hyper-networks to generate local models by learning prototype instances through an encoder model Alvarez-Melis & Jaakkola (2018). In contrast, we directly generate interpretable linear models in the original feature space. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Interpretable Model Distillation: Given the common understanding that complex models are not interpretable, prior works propose to learn simple surrogates for mimicking the input-output behavior of the complex models (Burkart & Huber, 2021). Such surrogate models are interpretable, such as linear regression or decision trees (Ribeiro et al., 2016). The local surrogates generate interpretations only valid in the neighborhood of the selected samples. Some approaches explain the output by computing the contribution of each attribute (Lundberg & Lee, 2017) to the prediction of the particular sample. An alternative strategy is to fit globally interpretable models, by relying on decision trees (Frosst & Hinton, 2017; Yang et al., 2018), or linear models (Ribeiro et al., 2016). Moreover, global explainers sometimes provide feature importances (Goldstein et al., 2015; Cortez & Embrechts, 2011), which can be used for auxiliary purposes such as feature engineering. Most of the surrogate models tackle the explainability task disjointly, by first training a black box model, then learning a surrogate in a second step. ", "page_idx": 4}, {"type": "text", "text": "Interpretable Deep Learning via Visualization: Given the success of neural networks in realworld applications in computer vision, a series of prior works (Ras et al., 2022) introduce techniques aiming at explaining their predictions. A direct way to measure the feature importance is by evaluating the partial derivative of the network given the input (Simonyan et al., 2013). CAM upscales the output of the last convolutional layers after applying Global Average Pooling (GAP), obtaining a map of the class activations used for interpretability (Zhou et al., 2016). DeepLift calculates pixel-wise relevance scores by computing differences with respect to a reference image (Shrikumar et al., 2017). Integrated Gradients use a baseline image to compute the cumulative sensibility of a black-box model $f$ to pixel-wise changes (Sundararajan et al., 2017). Other methods directly compute the pixel-wise relevance scores such that the network\u2019s output equals the sum of scores computed via Taylor Approximations (Montavon et al., 2017). ", "page_idx": 4}, {"type": "text", "text": "4 Experimental Protocol ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "4.1 Predictive Accuracy Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Baselines: In terms of interpretable white-box classifiers, we compare against Logistic Regression and Decision Trees, based on their scikit-learn library implementations (Pedregosa et al., 2011). On the other hand, we compare against two strong classifiers on tabular datasets, Random Forest and CatBoost. We use the scikit-learn interface for Random Forest, while for CatBoost we use the official implementation provided by the authors (Prokhorenkova et al., 2018). Lastly, in terms of interpretable deep learning architectures, we compare against TabNet (Arik & Pfister, 2021), a transformer architecture that makes use of attention for instance-wise feature-selection and NAM (Agarwal et al., 2021), a neural additive model which learns an additive function for every feature. For TabNet we use a well-maintained public implementation 2, while, for NAM we use the official public implementation from the authors 3. ", "page_idx": 4}, {"type": "text", "text": "Protocol: We run our predictive accuracy experiments on the AutoML benchmark that includes 35 diverse classification problems, containing between 690 and 539 383 data points, and between 5 and 7 201 features. For more details about the datasets included in our experiments, we point the reader to Appendix C. In our experiments, numerical features are standardized, while we transform categorical features through one-hot encoding. For binary classification datasets we use target encoding, where a category is encoded based on a shrunk estimate of the average target values for the data instances belonging to that category. In the case of missing values, we impute numerical features with zero and categorical features with a new category representing the missing value. For CatBoost and TabNet we do not encode categorical features since the algorithms natively handle them. For all the methods considered we tune the hyperparameters with Optuna (Akiba et al., 2019), a well-known hyperparameter optimization (HPO) library. We use the default HPO algorithm (TPE) from the library and we tune every method for 100 HPO trials or a wall-time limit of 1 day, whichever condition gets fulfliled first. The HPO search spaces of the different baselines were taken from prior work (Gorishniy et al., 2021; Hollmann et al., 2023). For a more detailed description, we kindly refer the reader to Appendix C. Additionally, we use the area under the ROC curve (AUROC) as the evaluation metric. Lastly, the methods that offer GPU support are run on a single NVIDIA RTX2080Ti, while, the rest of the methods are run on an AMD EPYC 7502 32-core processor. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "4.2 Explainability Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Baselines: First, we compare against Random, a baseline that generates random importance weights. Furthermore, BreakDown decomposes predictions into parts that can be attributed to certain features (Staniak & Biecek, 2018). TabNet offers instance-wise feature importances by making use of attention. LIME is a local interpretability method (Ribeiro et al., 2016) that fits an explainable surrogate (local model) to single instance predictions of black-box models. On the other hand, L2X is a method that applies instance-wise feature selection via variational approximations of mutual information (Chen et al., 2018) by making use of a neural network to generate the weights of the explainer. MAPLE is a method that uses local linear modeling by exploring random forests as a feature selection method (Plumb et al., 2018). SHAP is an additive feature attribution method (Lundberg & Lee, 2017) that allows local interpretation of the data instances. Last but not least, Kernel SHAP offers a reformulation of the LIME constrains (Lundberg & Lee, 2017). ", "page_idx": 5}, {"type": "text", "text": "Metrics and Benchmark: As explainability evaluation metrics we use faithfulness (Lundberg & Lee, 2017), monotonicity (Luss et al., 2021) (including the ROAR variants (Hooker et al., 2019)), infidelity (Yeh et al., 2019) and Shapley correlation (Lundberg & Lee, 2017). For a detailed description of the metrics, we refer the reader to XAI-Bench, a recent explainability benchmark (Liu et al., 2021). ", "page_idx": 5}, {"type": "text", "text": "For our explainability-related experiments, we use all three datasets (Gaussian Linear, Gaussian NonLinear, and Gaussian Piecewise) available in the XAI-Bench (Liu et al., 2021). For the state-of-the-art explainability baselines, we use the Tabular ResNet (TabResNet) backbone as the model for which the predictions are to be interpreted (same as for IMN). We experiment with different versions of the datasets that feature diverse $\\rho$ values, where $\\rho$ corresponds to the amount of correlation among features. All datasets have a train/validation set ratio of 10 to 1. ", "page_idx": 5}, {"type": "text", "text": "Implementation Details: We use PyTorch as the main library for our implementation. As a backbone, we use a TabResNet where the convolutional layers are replaced with fully-connected layers as suggested by recent work (Kadra et al., 2021). For the default hyperparameters of our method, we use 2 residual blocks and 128 units per layer combined with the GELU activation (Hendrycks & Gimpel, 2016). When training our network, we use snapshot ensembling (Huang et al., 2017) combined with cosine annealing with restarts (Loshchilov & Hutter, 2019). We use a learning rate and weight decay value of 0.01, where, the learning rate is warmed up to 0.01 for the first 5 epochs, a dropout value of 0.25, and an L1 penalty of 0.1 on the weights. Our network is trained for 500 epochs with a batch size of 64. We make our implementation publicly available4. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments and Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Hypothesis 1: IMN outperforms interpretable white-box models in terms of predictive accuracy. ", "page_idx": 5}, {"type": "text", "text": "We compare our method against decision trees and logistic regression, two white-box interpretable models. We run all aforementioned methods on the AutoML benchmark and we measure the predictive performance in terms of AUROC. Lastly, we measure the statistical significance of the results using the autorank package Herbold (2020) that runs a Friedman test with a Nemenyi post-hoc test, and a 0.05 significance level. Figure 3 presents the average rank across datasets based on the AUROC performance. As observed IMN achieves the best rank across the AutoML benchmark datasets. Furthermore, the difference is statistically significant against both decision trees and logistic regression. The detailed per-dataset results are presented in Appendix C. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Hypothesis 2: The explainability of IMN does not have a statistically significant negative impact on predictive accuracy. Additionally, it achieves a comparable performance against state-of-the-art methods. ", "page_idx": 6}, {"type": "text", "text": "This experiment addresses a simple question: Is our explainable neural network as accurate as a black-box neural network counterpart, that has the same architecture and same capacity?. Since our hypernetwork is a slight modification of the TabResNet Kadra et al. (2021), we compare it against TabResNet as a classifier. For completeness, we also compare against four other strong baselines, Gradient-Boosted Decision Trees (CatBoost), Random Forest, TabNet, and NAMs. Since the official implementation of ", "page_idx": 6}, {"type": "image", "img_path": "PmLty7tODm/tmp/c5502e4ba668b940119dc2df779fc8b643fa81373a08a9d15650a35fc28f853f.jpg", "img_caption": ["Figure 3: The critical difference diagram for the white-box interpretable methods. A lower rank indicates a better performance over datasets. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "NAMs only supports binary classification and regression, we separate the results into: $i$ ) results over 18 binary classification datasets (Figure 4 Top), and $i i$ ) results over all datasets (Figure 4 Bottom). ", "page_idx": 6}, {"type": "text", "text": "The results of Figure 4 demonstrate that IMN achieves a comparable performance to state-ofthe-art tabular classification models, while significantly outperforming explainable methods by design. IMN achieves a comparable performance to TabResNet, while outperforming TabNet and NAMs, indicating that its explainability does not harm accuracy in a significant way. There is no statistical significance of the differences between IMN, TabResNet and CatBoost. However, the difference in performance between IMNs, Random Forest, TabNet and NAMs is statistically significant. ", "page_idx": 6}, {"type": "text", "text": "Additionally, we investigate the runtime performance of the different baselines (NAM is excluded since it cannot be run on the full benchmark). We present the results in Table 2. As expected, deep learning methods take a longer time to train, however, both IMN and TabResNet are the most efficient during inference. We ", "page_idx": 6}, {"type": "image", "img_path": "PmLty7tODm/tmp/dd0db0b4676561891bec759d9889d96739829db9f5e837c9577d88ffbfb37f29.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 4: Black-box methods comparison with critical difference diagrams. Top: The average rank for the binary datasets present in the benchmark. Bottom: The average rank for all datasets present in the benchmark. A lower rank indicates a better performance. Connected ranks via a bold bar indicate that performances are not significantly different $(p>0.05)$ . ", "page_idx": 6}, {"type": "text", "text": "observe that TabResNet takes longer to converge compared to $\\mathrm{{IMN}^{5}}$ , however, both methods demand approximately the same inference time. As a result, the explainability of our method comes as a free-lunch benefti. Lastly, IMN is 64x faster in inference compared to TabNet, an end-to-end deeplearning interpretable method. Hypothesis 1 and 2 are valid even when default hyperparameters are used, for more details we kindly refer the reader to Appendix B. ", "page_idx": 6}, {"type": "text", "text": "Hypothesis 3: IMNs offer competitive levels of interpretability compared to state-of-the-art explainer techniques. ", "page_idx": 6}, {"type": "text", "text": "We compare against 8 explainer baselines in terms of 5 explainability metrics in the 3 datasets of the XAI benchmark (Liu et al., 2021), following the protocol we detailed in Section 4.2. The results of Table 3 demonstrate that IMN is competitive against all explainers across the indicated interpretability metrics. We tie in per", "page_idx": 6}, {"type": "table", "img_path": "PmLty7tODm/tmp/b2e94f9acffcebdbb7388764bfa877a9edcb7c02c68dca559e630119e5130cd8.jpg", "table_caption": ["Table 2: Aggregated training and inference times for all methods. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "PmLty7tODm/tmp/78f18f1dc60fa2039446c73d8329e68ff14a265469183c9583f8bc413c4cdddc.jpg", "table_caption": ["Table 3: Investigating the interpretability of IMNs against state-of-the-art interpretability methods. The results are generated from the XAI Benchmark (Liu et al., 2021) datasets (with $\\rho=0$ ). "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "PmLty7tODm/tmp/d84dc266241bb5a7228d128246d5666c1e6c98690738f01d906ef8dfc6de5a02.jpg", "img_caption": ["Figure 5: Performance analysis of different interpretability methods over a varying degree of feature correlation $\\rho$ . We present the performance of all methods on faithfulness (ROAR), monotonicity (ROAR), faithfulness, and infidelity (ordered from left to right) on the Gaussian Linear dataset for $\\rho$ values ranging from [0, 1]. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "formance with the second-best method Kernel-SHAP (Lundberg & Lee, 2017) and perform strongly against the other explainers. It is worth highlighting that in comparison to all the explainer techniques, the interpretability of our method comes as a free-lunch. In contrast, all the rival methods except TabNet are surrogate interpretable models to black-box models. Moreover, IMN strongly outperforms TabNet, the other baseline that offers explainability by design, achieving both better interpretability (Table 3) and better accuracy (Figure 4). ", "page_idx": 7}, {"type": "text", "text": "As a result, for all surrogate interpretable baselines we first need to train a black-box model. Then, for the prediction of every data point, we additionally train a local explainer around that point by predicting with the black-box model multiple times. In stark contrast, our method combines prediction models and explainers as an all-in-one neural network. To generate an explainable model for a data point $x_{n}$ , IMN does not need to train a per-point explainer. Instead, IMN requires only a forward pass through the ", "page_idx": 7}, {"type": "table", "img_path": "PmLty7tODm/tmp/d009c1406c9b5998dbdcd5c1a9cfae6822b55e00db3ab2c7188fe230e88485be.jpg", "table_caption": ["Table 4: Interpretable method inference times. All the methods are run on the GPU and the time is reported in seconds. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "trained hypernetwork to generate a linear explainer. To quantify the difference in runtime between our method and other interpretable methods we compare the runtimes on a few datasets from the benchmark with a varying number of instances/features such as Credit- $\\mathrm{\\Delta}^{\\mathrm{g}}$ (1000/21), Adult (48842/15), and Christine (5418/1637). Table 4 presents the results, where, as observed IMN has the fastest inference times, being 11-65x faster compared to TabNet which employs attention, 1710-11400x faster compared to SHAP that uses the same (TabResNet) backbone, and 455-215850x faster compared to SHAP that uses CatBoost as a backbone. ", "page_idx": 7}, {"type": "text", "text": "Lastly, we compare all interpretability methods on 4 out of 5 metrics in the presence of a varying $\\rho$ factor, which controls the correlation of features on the Gaussian Linear dataset. Figure 5 presents the comparison, where IMN behaves similarly to other interpretable methods and has a comparable performance with the top methods in the majority of metrics. The results agree with the findings of prior work (Liu et al., 2021), where the performance in the interpretability metrics drops in the presence of feature correlations. Although our work focuses on tabular data, in Appendix A we present an application of IMN in the vision domain. ", "page_idx": 8}, {"type": "text", "text": "Table 5: The feature rank importances for the Census dataset. A lower rank is associated with a higher feature importance. ", "page_idx": 8}, {"type": "table", "img_path": "PmLty7tODm/tmp/d61e3bc118c31687aebb9c914667ee8ce4fd450fb0c61f0c560f976b057e2212.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Hypothesis 4: IMN offers a global (dataset-wide) interpretability of feature importance. ", "page_idx": 8}, {"type": "text", "text": "The purpose of this experiment is to showcase that IMN can be used to analyze the global interpretability of feature attributions, where the dataset-wide importance of the $m$ -th feature is aggregated as $\\begin{array}{r}{\\frac{1}{N}\\sum_{n=1}^{\\dot{N}}|w(x_{n};\\theta)_{m}\\,x_{n,m}|}\\end{array}$ . Since we are not aware of a public benchmark offering ground-truth global interpretability of features, we experiment with the Adult Census Income (Kohavi et al., 1996), a very popular dataset, where the goal is to predict whether income exceeds $\\mathbb{S}50\\mathrm{K/yr}$ based on census data. We consider Decision Trees, CatBoost, TabNet, and IMN as explainable methods. Additionally, we use SHAP to explain the predictions of the TabResNet backbone. ", "page_idx": 8}, {"type": "text", "text": "We present the importance that the different methods assign to features in Table 5. To verify the feature rankings generated by the models, we analyze the top 5 features of every individual method by investigating the drop in model performance if we remove the feature. The more important a feature is, the more accuracy should drop when removing that feature. The results of Figure 6 show that IMNs have a higher relative drop in the model\u2019s accuracy when the most important predicted feature is removed. This shows that the feature ranking generated by IMN is proportional to the predictive importance of the feature and monotonously decreasing. In contrast, in the case of CatBoost, TabNet, SHAP, and Decision Trees, the decrease in accuracy is not proportional to the order of the feature importance (e.g. the case of Top-1 for Decision Trees, TabNet, SHAP or Top-2 for CatBoost). ", "page_idx": 8}, {"type": "image", "img_path": "PmLty7tODm/tmp/db551b0c488ed6c4589fdc9d400a818a5e83c4fc549ddb3ee8c63f0b098b7272.jpg", "img_caption": ["Figure 6: Investigating the decrease in AUROC when removing the $k$ -th most important feature. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "We additionally consider the task of predicting mushroom edibility (Lincoff, 1997). The odor feature allows one to predict whether a mushroom is edible or not and basing the predictions only on odor would allow a model to achieve more than $98.5\\%$ accuracy (Arik & Pfister, 2021). We run IMNs on the mushroom edibility task and we achieve a perfect test AUROC of 1. Furthermore, in Figure 7 we investigate the impact of every feature as described in Section 2.3, where, as observed, our method correctly identifies odor as the feature with the highest impact in the output. Based on the results, we conclude that IMNs offer global interpretability. ", "page_idx": 8}, {"type": "image", "img_path": "PmLty7tODm/tmp/15f5d3c1fd09ca62ec4a67cc4a91bf29fcfee021d2426c040505c8b9521fb417.jpg", "img_caption": ["Figure 7: Feature impacts for the mushroomedibility task. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we propose explainable deep networks that are comparable in performance to their black-box counterparts but also as interpretable as state-of-the-art explanation techniques. With extensive experiments, we show that the explainable deep learning networks outperform traditional white-box models in terms of performance. Moreover, the experiments confirm that the explainable deep-learning architecture does not include a significant degradation in performance or an overhead on time compared to the plain black-box counterpart, achieving competitive results against stateof-the-art classifiers in tabular data. Our method matches competitive state-of-the-art explainability methods on a recent explainability benchmark in tabular data, offering explanations of predictions as a free lunch. ", "page_idx": 9}, {"type": "text", "text": "7 Limitations and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "One potential limitation of our method is that although interpretable, the per-instance models are linear. A potential future work can focus on generating other types of non-linear interpretable models, such as decision trees. More concretely, the hypernetwork can generate the parameters of the decision splits and the decision value at each node, as well as the leaf weights. Another potential strategy is to generate local Support Vector Machines, by expressing the prediction for a data point as a function of the similarity of the informative neighbors. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "JG, AK and SBA would like to acknowledge the funding by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under grant number 417962828 and grant INST 39/963-1 FUGG (bwForCluster NEMO). In addition, JG and AK acknowledge the support of the BrainLinksBrainTools center of excellence. Moreover, the authors acknowledge the support and HPC resources provided by the Erlangen National High Performance Computing Center (NHR $@$ FAU) of the Friedrich-Alexander-Universit\u00e4t Erlangen-N\u00fcrnberg (FAU) under the NHR project v101be. NHR funding is provided by federal and Bavarian state authorities. NHR $@$ FAU hardware is partially funded by the German Research Foundation (DFG) \u2013 440719683. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Agarwal, R., Melnick, L., Frosst, N., Zhang, X., Lengerich, B., Caruana, R., and Hinton, G. E. Neural additive models: Interpretable machine learning with neural nets. Advances in neural information processing systems, 34:4699\u20134711, 2021.   \nAkiba, T., Sano, S., Yanase, T., Ohta, T., and Koyama, M. Optuna: A next-generation hyperparameter optimization framework. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pp. 2623\u20132631, 2019.   \nAlvarez-Melis, D. and Jaakkola, T. S. Towards robust interpretability with self-explaining neural networks. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS\u201918, pp. 7786\u20137795, Red Hook, NY, USA, 2018. Curran Associates Inc.   \nAncona, M., Ceolini, E., \u00d6ztireli, C., and Gross, M. Towards better understanding of gradient-based attribution methods for deep neural networks. arXiv preprint arXiv:1711.06104, 2017.   \nArik, S. \u00d6. and Pfister, T. Tabnet: Attentive interpretable tabular learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 6679\u20136687, 2021.   \nBerkson, J. A statistically precise and relatively simple method of estimating the bio-assay with quantal response, based on the logistic function. Journal of the American Statistical Association, 48(263):565\u2013599, 1953.   \nBischl, B., Casalicchio, G., Feurer, M., Gijsbers, P., Hutter, F., Lang, M., Mantovani, R. G., van Rijn, J. N., and Vanschoren, J. OpenML benchmarking suites. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. URL https://openreview.net/forum?id=OCrD8ycKjG.   \nBorisov, V., Leemann, T., Se\u00dfler, K., Haug, J., Pawelczyk, M., and Kasneci, G. Deep neural networks and tabular data: A survey. IEEE Transactions on Neural Networks and Learning Systems, pp. 1\u201321, 2022. doi: 10.1109/TNNLS.2022.3229161.   \nBurkart, N. and Huber, M. F. A survey on the explainability of supervised machine learning. Journal of Artificial Intelligence Research, 70:245\u2013317, 2021.   \nChen, J., Song, L., Wainwright, M., and Jordan, M. Learning to explain: An information-theoretic perspective on model interpretation. In Dy, J. and Krause, A. (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 883\u2013892. PMLR, 10\u201315 Jul 2018. URL https://proceedings.mlr.press/ v80/chen18j.html.   \nChen, J., Liao, K., Wan, Y., Chen, D. Z., and Wu, J. Danets: Deep abstract networks for tabular data classification and regression. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 3930\u20133938, 2022.   \nCortez, P. and Embrechts, M. J. Opening black box data mining models using sensitivity analysis. In Proceedings of the IEEE Symposium on Computational Intelligence and Data Mining, CIDM 2011, part of the IEEE Symposium Series on Computational Intelligence 2011, April 11-15, 2011, Paris, France, pp. 341\u2013348. IEEE, 2011. doi: 10.1109/CIDM.2011.5949423. URL https: //doi.org/10.1109/CIDM.2011.5949423.   \nCraven, M. and Shavlik, J. Extracting tree-structured representations of trained networks. Advances in neural information processing systems, 8, 1995.   \nEfron, B., Hastie, T., Johnstone, I., and Tibshirani, R. Least angle regression. The Annals of Statistics, 32(2):407 \u2013 499, 2004. doi: 10.1214/009053604000000067. URL https://doi.org/10.1214/ 009053604000000067.   \nFreitas, A. A. Comprehensible classification models: a position paper. ACM SIGKDD explorations newsletter, 15(1):1\u201310, 2014.   \nFriedman, N., Geiger, D., and Goldszmidt, M. Bayesian network classifiers. Machine learning, 29: 131\u2013163, 1997.   \nFrosst, N. and Hinton, G. E. Distilling a neural network into a soft decision tree. In Besold, T. R. and Kutz, O. (eds.), Proceedings of the First International Workshop on Comprehensibility and Explanation in AI and ML 2017 co-located with 16th International Conference of the Italian Association for Artificial Intelligence (AI\\*IA 2017), Bari, Italy, November 16th and 17th, 2017, volume 2071 of CEUR Workshop Proceedings. CEUR-WS.org, 2017. URL https://ceur-ws. org/Vol-2071/CExAIIA_2017_paper_3.pdf.   \nGijsbers, P., LeDell, E., Poirier, S., Thomas, J., Bischl, B., and Vanschoren, J. An open source automl benchmark. arXiv preprint arXiv:1907.00909 [cs.LG], 2019. URL https://arxiv.org/abs/ 1907.00909. Accepted at AutoML Workshop at ICML 2019.   \nGoldstein, A., Kapelner, A., Bleich, J., and Pitkin, E. Peeking inside the black box: Visualizing statistical learning with plots of individual conditional expectation. journal of Computational and Graphical Statistics, 24(1):44\u201365, 2015.   \nGorishniy, Y., Rubachev, I., Khrulkov, V., and Babenko, A. Revisiting deep learning models for tabular data. Advances in Neural Information Processing Systems, 34:18932\u201318943, 2021.   \nGulum, M. A., Trombley, C. M., and Kantardzic, M. A review of explainable deep learning cancer detection models in medical imaging. Applied Sciences, 11(10):4573, 2021.   \nHa, D., Dai, A. M., and Le, Q. V. Hypernetworks. In International Conference on Learning Representations, 2017. URL https://openreview.net/forum?id $\\cdot^{=}$ rkpACe1lx.   \nHendrycks, D. and Gimpel, K. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016.   \nHerbold, S. Autorank: A python package for automated ranking of classifiers. Journal of Open Source Software, 5(48):2173, 2020. doi: 10.21105/joss.02173. URL https://doi.org/10. 21105/joss.02173.   \nHollmann, N., M\u00fcller, S., Eggensperger, K., and Hutter, F. TabPFN: A transformer that solves small tabular classification problems in a second. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id $\\equiv$ cp5PvcI6w8_.   \nHooker, S., Erhan, D., Kindermans, P.-J., and Kim, B. A Benchmark for Interpretability Methods in Deep Neural Networks. Curran Associates Inc., Red Hook, NY, USA, 2019.   \nHuang, G., Li, Y., Pleiss, G., Liu, Z., Hopcroft, J. E., and Weinberger, K. Q. Snapshot ensembles: Train 1, get m for free. In International Conference on Learning Representations, 2017. URL https://openreview.net/forum?id $\\equiv$ BJYwwY9ll.   \nKadra, A., Lindauer, M., Hutter, F., and Grabocka, J. Well-tuned simple nets excel on tabular datasets. In Thirty-Fifth Conference on Neural Information Processing Systems, 2021.   \nKim, B., Shah, J. A., and Doshi-Velez, F. Mind the gap: A generative approach to interpretable feature selection and extraction. Advances in neural information processing systems, 28, 2015.   \nKohavi, R. et al. Scaling up the accuracy of naive-bayes classifiers: A decision-tree hybrid. In Kdd, volume 96, pp. 202\u2013207, 1996.   \nLincoff, G. H. Field guide to North American mushrooms. Knopf National Audubon Society, 1997.   \nLiu, Y., Khandagale, S., White, C., and Neiswanger, W. Synthetic benchmarks for scientific research in explainable machine learning. In Advances in Neural Information Processing Systems Datasets Track, 2021.   \nLoh, W.-Y. Classification and regression trees. Wiley interdisciplinary reviews: data mining and knowledge discovery, 1(1):14\u201323, 2011.   \nLoshchilov, I. and Hutter, F. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7.   \nLundberg, S. M. and Lee, S. A unified approach to interpreting model predictions. In Guyon, I., von Luxburg, U., Bengio, S., Wallach, H. M., Fergus, R., Vishwanathan, S. V. N., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 4765\u20134774, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/ 8a20a8621978632d76c43dfd28b67767-Abstract.html.   \nLuss, R., Chen, P.-Y., Dhurandhar, A., Sattigeri, P., Zhang, Y., Shanmugam, K., and Tu, C.-C. Leveraging latent features for local explanations. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD \u201921, pp. 1139\u20131149, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450383325. doi: 10.1145/3447548. 3467265. URL https://doi.org/10.1145/3447548.3467265.   \nMartens, D., Baesens, B., Gestel, T. V., and Vanthienen, J. Comprehensible credit scoring models using rule extraction from support vector machines. Eur. J. Oper. Res., 183(3):1466\u20131476, 2007. doi: 10.1016/j.ejor.2006.04.051. URL https://doi.org/10.1016/j.ejor.2006.04.051.   \nMontavon, G., Lapuschkin, S., Binder, A., Samek, W., and M\u00fcller, K. Explaining nonlinear classification decisions with deep taylor decomposition. Pattern Recognit., 65:211\u2013222, 2017. doi: 10.1016/j.patcog.2016.11.008. URL https://doi.org/10.1016/j.patcog.2016.11.008.   \nMurphy, K. P. et al. Naive bayes classifiers. University of British Columbia, 18(60):1\u20138, 2006.   \nPedregosa, Fabian acnd Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., et al. Scikit-learn: Machine learning in python. the Journal of machine Learning research, 12:2825\u20132830, 2011.   \nPlumb, G., Molitor, D., and Talwalkar, A. S. Model agnostic supervised local explanations. Advances ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "in neural information processing systems, 31, 2018. ", "page_idx": 11}, {"type": "text", "text": "Prokhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A. V., and Gulin, A. Catboost: unbiased boosting with categorical features. Advances in neural information processing systems, 31, 2018. ", "page_idx": 12}, {"type": "text", "text": "Ras, G., Xie, N., van Gerven, M., and Doran, D. Explainable deep learning: A field guide for the uninitiated. J. Artif. Intell. Res., 73:329\u2013396, 2022. doi: 10.1613/jair.1.13200. URL https: //doi.org/10.1613/jair.1.13200.   \nRibeiro, M. T., Singh, S., and Guestrin, C. \"why should i trust you?\": Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201916, pp. 1135\u20131144, New York, NY, USA, 2016. Association for Computing Machinery. ISBN 9781450342322. doi: 10.1145/2939672.2939778. URL https://doi.org/10.1145/2939672.2939778.   \nSadhwani, A., Giesecke, K., and Sirignano, J. Deep Learning for Mortgage Risk\\*. Journal of Financial Econometrics, 19(2):313\u2013368, 07 2020. ISSN 1479-8409. doi: 10.1093/jjfinec/nbaa025. URL https://doi.org/10.1093/jjfinec/nbaa025.   \nShrikumar, A., Greenside, P., and Kundaje, A. Learning important features through propagating activation differences. In Precup, D. and Teh, Y. W. (eds.), Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pp. 3145\u20133153. PMLR, 2017. URL http://proceedings.mlr.press/v70/shrikumar17a.html.   \nSimonyan, K., Vedaldi, A., and Zisserman, A. Deep inside convolutional networks: Visualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034, 2013.   \nSomepalli, G., Schwarzschild, A., Goldblum, M., Bruss, C. B., and Goldstein, T. SAINT: Improved neural networks for tabular data via row attention and contrastive pre-training, 2022. URL https://openreview.net/forum?id $\\equiv$ nL2lDlsrZU.   \nStaniak, M. and Biecek, P. Explanations of model predictions with live and breakdown packages. arXiv preprint arXiv:1804.01955, 2018.   \nSundararajan, M., Taly, A., and Yan, Q. Axiomatic attribution for deep networks. In Precup, D. and Teh, Y. W. (eds.), Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pp. 3319\u20133328. PMLR, 2017. URL http://proceedings.mlr.press/ v70/sundararajan17a.html.   \nTibshirani, R. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society: Series B (Methodological), 58(1):267\u2013288, 1996.   \nTjoa, E. and Guan, C. A survey on explainable artificial intelligence (xai): Toward medical xai. IEEE Transactions on Neural Networks and Learning Systems, 32(11):4793\u20134813, 2021. doi: 10.1109/TNNLS.2020.3027314.   \nWydman\u00b4ski, W., Bulenok, O., and S\u00b4mieja, M. Hypertab: Hypernetwork approach for deep learning on small tabular datasets. In 2023 IEEE 10th International Conference on Data Science and Advanced Analytics (DSAA), pp. 1\u20139. IEEE, 2023.   \nYang, C., Rangarajan, A., and Ranka, S. Global model interpretation via recursive partitioning. In 20th IEEE International Conference on High Performance Computing and Communications; 16th IEEE International Conference on Smart City; 4th IEEE International Conference on Data Science and Systems, HPCC/SmartCity/DSS 2018, Exeter, United Kingdom, June 28-30, 2018, pp. 1563\u20131570. IEEE, 2018. doi: 10.1109/HPCC/SmartCity/DSS.2018.00256. URL https: //doi.org/10.1109/HPCC/SmartCity/DSS.2018.00256.   \nYeh, C.-K., Hsieh, C.-Y., Suggala, A. S., Inouye, D. I., and Ravikumar, P. On the (in)Fidelity and Sensitivity of Explanations. Curran Associates Inc., Red Hook, NY, USA, 2019.   \nZhou, B., Khosla, A., Lapedriza, \u00c0., Oliva, A., and Torralba, A. Learning deep features for discriminative localization. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pp. 2921\u20132929. IEEE Computer Society, 2016. doi: 10.1109/CVPR.2016.319. URL https://doi.org/10.1109/CVPR.2016.319. ", "page_idx": 12}, {"type": "image", "img_path": "PmLty7tODm/tmp/fc9141ee6031cbe41739a76c64e4930f4b768bc0b6ec7bc645f2e903d32e16bf.jpg", "img_caption": ["Figure 8: Comparison of IMN against explainability techniques for image classification. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "We use IMN to explain the predictions of ResNet50, a broadly used computer vision backbone. We take the pre-trained backbone $\\phi(\\cdot):\\mathrm{R}^{H\\times W\\times K}\\to\\mathrm{R}^{D}$ from PyTorch and change the output layer to a fully-connected layer $w:\\mathrm{R}^{D}\\rightarrow\\mathrm{R}^{H\\times W\\times K\\times C}$ that generates the weights for multiplying the input image $x\\in\\mathrm{R}^{H\\times\\check{W}\\times K}$ with $K$ channels, and finally obtain the logits $z_{c}$ for the class $c$ . In this experiment, we use $\\lambda=10^{-3}$ as the L1 penalty strength. ", "page_idx": 13}, {"type": "text", "text": "We fine-tuned the ImageNet pre-trained ResNet50 models, both for the explainable (IMN-ResNet) and the black-box (ResNet) variants for 400 epochs on the CIFAR-10 dataset with a learning rate of $10^{-4}$ . To test whether the explainable variant is as accurate as the black-box model, we evaluate the validation accuracy after 5 independent training runs. IMN-ResNet achieves an accuracy of $87.49\\pm1.73$ and the ResNet $88.76\\pm1.50$ , with the difference being statistically insignificant. ", "page_idx": 13}, {"type": "text", "text": "We compare our method to the following image explainability baselines: Saliency Maps (Gradients) (Simonyan et al., 2013), DeepLift (Shrikumar et al., 2017), Integrated Gradients (Ancona et al., 2017) with SmoothGrad. All of the baselines are available via the captum library6. We compare the rival explainers to IMN-ResNet by visually interpreting the pixel-wise weights of selected images in Figure 8. The results confirm that IMN-ResNet generates higher weights for pixel regions that include descriptive parts of the object. ", "page_idx": 13}, {"type": "text", "text": "B Plots ", "text_level": 1, "page_idx": 13}, {"type": "image", "img_path": "PmLty7tODm/tmp/062293130c7e2a1f474f1ca1f5554d53888530107d24830737cddc083a4de258.jpg", "img_caption": [], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "Figure 9: The critical difference diagrams that represent the average rank over all datasets based on the AUROC test performance for: left) The white-box methods and IMN, middle) The black-box methods and IMN for the binary classification datasets, right) The black-box methods and IMN for the entire benchmark of datasets. ", "page_idx": 13}, {"type": "text", "text": "In Figure 9, we repeat the experiments from Hypotheses 1 and 2, however, without performing hyperparameter optimization. Moreover, we consider two additional baselines, DANet (Chen et al., ", "page_idx": 13}, {"type": "text", "text": "2022) and HyperTab (Wydma\u00b4nski et al., 2023). As observed, our findings are consistent and both hypotheses are validated even when default hyperparameters are used for all the methods considered. ", "page_idx": 14}, {"type": "text", "text": "To further investigate the results on individual datasets, in Figure 10 we plot the distribution of the gains in performance of all methods over a single decision tree model (with default hyperparameters). The gain $G$ of a method $m$ run on a dataset $D$ for a single run is calculated as shown in Equation 3. ", "page_idx": 14}, {"type": "equation", "text": "$$\nG\\left(m,D T r e e,D\\right)=\\frac{\\mathrm{AUROC}(m,D)}{\\mathrm{AUROC}(D T r e e,D)}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The results indicate that all methods except NAM achieve a comparable gain in performance across the AutoML benchmark datasets, while, the latter achieves a worse performance overall. We present detailed results in Appendix C. ", "page_idx": 14}, {"type": "image", "img_path": "PmLty7tODm/tmp/96f604236fbe802d93091375def78adc49decba1733a7f0018e2823d4ca42584.jpg", "img_caption": ["Figure 10: The gain distribution of the state-of-theart models. The gain is calculated by dividing the test AUROC against the test AUROC of a decision tree. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Additionally, in Figure 11, we present the performance of the different explainers for the different explainability metrics. We present results for the Gaussian Non-Linear Additive and Gaussian ", "page_idx": 14}, {"type": "text", "text": "Piecewise Constant datasets over a varying presence of correlation $\\rho$ between the features. The results show that our method achieves competitive results against Kernel Shap (K. SHAP) and LIME, the strongest baselines. ", "page_idx": 14}, {"type": "image", "img_path": "PmLty7tODm/tmp/6e211c1695bd2540aaee9bb178d9ae2ef9df254790ebe6666c6f1352a9bb9cfd.jpg", "img_caption": ["Figure 11: Performance analysis of all explainable methods on faithfulness (ROAR), monotonicity (ROAR), faithfulness, and infidelity. The results are shown for the Gaussian Non-Linear Additive and Gaussian Piecewise datasets where, correlation $(\\rho)$ ranges from [0, 1]. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Lastly, to investigate how sensitive IMN is to the controlling hyperparameter configuration, we compare IMN and CatBoost (a method known for being robust to its hyperparameters in the community). Specifically, for every task, we plot the distribution of the performance of all hyperparameter configurations for every method. We present the results in Figure 12, where, as observed, IMN has a comparable sensitivity to CatBoost with regard to the controlling hyperparameter configuration. Moreover, in the majority of cases, the IMN validation performance does not vary significantly. ", "page_idx": 14}, {"type": "image", "img_path": "PmLty7tODm/tmp/0fbcac964b4eae8f8453ee40c0c8097a7b4d79bd366b96dedac006572de8498b.jpg", "img_caption": ["Distribution of Task Performances "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 12: The distribution of the validation performance of the different hyperparameter configurations per task for CatBoost and IMN. ", "page_idx": 15}, {"type": "text", "text": "C Tables ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "To describe the 35 datasets present in our accuracy-related experiments, we summarize the main descriptive statistics in Table 6. The statistics show that our datasets are diverse, covering both binary and multi-class classification problems with imbalanced and balanced datasets that contain a diverse number of features and examples. ", "page_idx": 15}, {"type": "text", "text": "Additionally, we provide the per-dataset performances for the accuracy-related experiments of every method with the default configurations. Table 7 summarizes the performances on the train split, where, as observed Random Forest and Decision Trees overfti the training data excessively compared to the other methods. Moreover, Table 8 provides the performance of every method on the test split, where, IMN, TabResNet, and CatBoost achieve similar performances. We provide the same per-dataset performances of every method with the best-found hyperparameter configuration during HPO for the train split in Table 9 and test split in Table 10. ", "page_idx": 15}, {"type": "text", "text": "Lastly, we provide the HPO search spaces of the different methods considered in our experiments in Table 11, 12, 13, 14, 15, 16. ", "page_idx": 15}, {"type": "table", "img_path": "PmLty7tODm/tmp/984d398cc4992ea4600025862d53e86434053d77dddf5e18c3f31c946d3b1716.jpg", "table_caption": ["Table 6: Statistics regarding the AutoML benchmark datasets. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Table 7: The per-dataset train AUROC performance for all methods in the accuracy experiments with default hyperparameter configurations. The train performance is the mean value from 10 runs with different seeds. A dashed line \u2019-\u2019 represents a failure of running on that particular dataset. ", "page_idx": 16}, {"type": "table", "img_path": "PmLty7tODm/tmp/43632f60d0f3b5df2f9890cc814223cf8b59a0642657ece9a12e37c8a986eb49.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "PmLty7tODm/tmp/e93d3a902257e2d22a298d0d7f69861e467f33fa015403683d92b1129be81b7a.jpg", "table_caption": ["Table 8: The per-dataset test AUROC performance for all methods in the accuracy experiments with default hyperparameter configurations. The test performance is the mean value from 10 runs with different seeds. A dashed line \u2019-\u2019 represents a failure of running on that particular dataset. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "PmLty7tODm/tmp/a8fa7a213b45c5ec7d26420c47ea2bef2ade77430c8e6f87abd0eff5a6b1c37f.jpg", "table_caption": ["Table 9: The per-dataset train AUROC performance for all methods in the accuracy experiments parametrized with the best hyperparameter configuration found during HPO. A dashed line \u2019-\u2019 represents a failure to run on that particular dataset. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "PmLty7tODm/tmp/d05d50b9c794acd89159d000880644d66340e20c583784212468fb2d5ff5913f.jpg", "table_caption": ["Table 10: The per-dataset test AUROC performance for all methods in the accuracy experiments parametrized with the best hyperparameter configuration found during HPO. A dashed line \u2019-\u2019 represents a failure to run on that particular dataset. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Table 11: The hyperparameter search space for IMN. TabResNet has the same search space without weight normalization. ", "page_idx": 20}, {"type": "table", "img_path": "PmLty7tODm/tmp/3438fd74ca5b1cfe9c8f83604e60b57644b1ede312a888d09cdffda75461bde5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "PmLty7tODm/tmp/88d29f0850a8392fe93d93b0c2b85c4a96def87d9c2bf596c51b574128cfa070.jpg", "table_caption": ["Table 12: The hyperparameter search space for logistic regression. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "PmLty7tODm/tmp/ba07cedd99da904297e311e4fa16a6de8b184045516686003eaf66bdb07e2b83.jpg", "table_caption": ["Table 13: The hyperparameter search space for a decision tree. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "PmLty7tODm/tmp/8e513f16ddb4e0b17f0516cca1a92d73048d4d7ba91f7468394827509621c7f0.jpg", "table_caption": ["Table 14: The hyperparameter search space for CatBoost. "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "PmLty7tODm/tmp/6d226330a2994c3f5bc85bf8e4670fafb244bb789122b98e3bc7191e622486e4.jpg", "table_caption": ["Table 15: The hyperparameter search space for Random Forest. "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "PmLty7tODm/tmp/79f42aad4e7d463f761e62920f3edd3023afe99ef96a01b5e6066bd706d62b1c.jpg", "table_caption": ["Table 16: Hyperparameter search space for the TabNet model. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The results in Section 2.4 and Section 5 validate our claims. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: The limitations of the proposed method are mentioned in Section 7 ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: There is no theory in this paper. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: In Sections 2 and 4 we provide all the information regarding our method/baselines and the preprocessing of the data. We additionally open-source the code. Lastly, the results are reproducible as the experiments were seeded. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The code is open-sourced (Section 4) and all of the necessary information regarding the datasets is provided in Table 6, combined with their online identifier where they can be easily accessed from OpenML. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: The information is provided in Section 4. The code is additionally opensourced. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: Critical difference diagrams that provide statistical significance information are provided in Section 5. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 24}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar then state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: All the necessary information is provided in Section 4 and 5. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The research conducted conforms with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The impact of our work has been mentioned in the Introduction Section and in Section 5. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 25}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper poses no risks. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Everything that was used from previous work be it a method or dataset has been properly cited in the manuscript. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The code is provided and documented. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: Not applicable. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: Not applicable. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}]