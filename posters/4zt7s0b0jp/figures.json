[{"figure_path": "4Zt7S0B0Jp/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of CoT-decoding. Pre-trained LLMs are capable of inherent reasoning without prompting by considering alternative top-k tokens, rather than solely relying on the top-1 greedy decoding path. Moreover, these models tend to display higher confidence in decoding the final answer (indicated by a darker shaded color) when a CoT reasoning path is present.", "description": "This figure illustrates the concept of CoT-decoding.  A question is given to a language model, and the model's response is analyzed using both standard greedy decoding (only considering the most likely next word at each step) and CoT-decoding (considering multiple alternative next words).  The figure shows that while greedy decoding may lead to an incorrect answer, the alternative decoding paths often contain chain-of-thought (CoT) reasoning, which is a step-by-step logical progression leading to the correct answer.  The model's confidence in the answer is also higher when a CoT path is present.", "section": "2 Chain-of-Thought (CoT) Decoding"}, {"figure_path": "4Zt7S0B0Jp/figures/figures_2_1.jpg", "caption": "Figure 1: Illustration of CoT-decoding. Pre-trained LLMs are capable of inherent reasoning without prompting by considering alternative top-k tokens, rather than solely relying on the top-1 greedy decoding path. Moreover, these models tend to display higher confidence in decoding the final answer (indicated by a darker shaded color) when a CoT reasoning path is present.", "description": "This figure illustrates the concept of CoT-decoding.  It shows that a pre-trained large language model (LLM), when using a standard question-answer format, may produce an incorrect answer using greedy decoding (only considering the most likely next token at each step). However, by examining alternative top-k tokens (the k most likely tokens), the figure reveals that the model actually generates several different decoding paths. Some of these paths incorporate a chain-of-thought (CoT) reasoning process, leading to the correct answer.  The figure highlights that the model often displays higher confidence (represented by darker shading) when a CoT path is present, suggesting that this method can effectively identify and utilize a model's intrinsic reasoning abilities.", "section": "2 Chain-of-Thought (CoT) Decoding"}, {"figure_path": "4Zt7S0B0Jp/figures/figures_4_1.jpg", "caption": "Figure 1: Illustration of CoT-decoding. Pre-trained LLMs are capable of inherent reasoning without prompting by considering alternative top-k tokens, rather than solely relying on the top-1 greedy decoding path. Moreover, these models tend to display higher confidence in decoding the final answer (indicated by a darker shaded color) when a CoT reasoning path is present.", "description": "This figure illustrates the concept of CoT-decoding.  It shows that pre-trained large language models (LLMs) can exhibit chain-of-thought (CoT) reasoning even without explicit prompting. By examining the top-k most likely token sequences during decoding (instead of only the single most likely token), the authors found that CoT reasoning paths are frequently present.  The figure uses an example question to show how greedy decoding leads to an incorrect answer, while alternative paths reveal a correct CoT reasoning process.  The darker shading indicates higher model confidence in answers found via CoT paths.", "section": "2 Chain-of-Thought (CoT) Decoding"}, {"figure_path": "4Zt7S0B0Jp/figures/figures_5_1.jpg", "caption": "Figure 3: CoT-decoding effectively elicits reasoning across multiple language model families including PaLM-2, Mistral and Gemma, with significant accuracy gains over three reasoning tasks.", "description": "This figure compares the performance of greedy decoding and CoT-decoding across three different large language model families (PaLM-2, Mistral, and Gemma) on three reasoning tasks: GSM8K (a math reasoning benchmark), MultiArith (another math reasoning benchmark), and Year Parity (a commonsense reasoning task).  The results show that CoT-decoding consistently outperforms greedy decoding, achieving significant accuracy improvements across all three model families and tasks.", "section": "3.1 CoT-Decoding Effectively Elicits Reasoning from Language Models"}, {"figure_path": "4Zt7S0B0Jp/figures/figures_5_2.jpg", "caption": "Figure 4: CoT-decoding reliably improves reasoning performance across model scales (PaLM-2), even when the task does not naturally improve by scaling up only (e.g., year parity).", "description": "This figure demonstrates the impact of CoT decoding on reasoning performance across various scales of the PaLM-2 model family.  The x-axis shows different model sizes (XS, Small, Medium, Large) and an instruction-tuned version. The y-axis represents the accuracy achieved on two reasoning tasks (GSM8K and Year Parity). Notably, CoT decoding consistently leads to substantial accuracy improvements across all model sizes, particularly pronounced in the year parity task where standard greedy decoding shows no significant improvement with larger models.", "section": "3.1 CoT-Decoding Effectively Elicits Reasoning from Language Models"}, {"figure_path": "4Zt7S0B0Jp/figures/figures_5_3.jpg", "caption": "Figure 4: CoT-decoding reliably improves reasoning performance across model scales (PaLM-2), even when the task does not naturally improve by scaling up only (e.g., year parity).", "description": "This figure demonstrates the effectiveness of CoT-decoding across different scales of the PaLM-2 language model family on two reasoning tasks: GSM8K (mathematical reasoning) and Year Parity (commonsense reasoning).  The results show that CoT-decoding consistently yields significant accuracy gains compared to greedy decoding on the GSM8K task, with improvements ranging from 10% to 30%. Notably, on the Year Parity task, where greedy decoding performance remains stagnant even with larger models, CoT-decoding substantially boosts accuracy, nearly reaching perfect performance with the largest model. This highlights the ability of CoT-decoding to effectively elicit reasoning capabilities even when simple scaling does not suffice.", "section": "3.1 CoT-Decoding Effectively Elicits Reasoning from Language Models"}, {"figure_path": "4Zt7S0B0Jp/figures/figures_6_1.jpg", "caption": "Figure 3: CoT-decoding effectively elicits reasoning across multiple language model families including PaLM-2, Mistral and Gemma, with significant accuracy gains over three reasoning tasks.", "description": "The figure shows that CoT-decoding consistently improves the accuracy of three different large language models (LLMs) across three distinct reasoning tasks: GSM8K (a grade-school mathematics dataset), MultiArith (a multi-step arithmetic reasoning dataset), and Year Parity (a commonsense reasoning task).  The improvements are substantial, often doubling or tripling the accuracy compared to using standard greedy decoding. This demonstrates that CoT-decoding is effective across various LLMs and tasks.", "section": "3.1 CoT-Decoding Effectively Elicits Reasoning from Language Models"}, {"figure_path": "4Zt7S0B0Jp/figures/figures_16_1.jpg", "caption": "Figure 4: CoT-decoding reliably improves reasoning performance across model scales (PaLM-2), even when the task does not naturally improve by scaling up only (e.g., year parity).", "description": "This figure shows how CoT-decoding improves the reasoning performance of different sizes of PaLM-2 models on three different reasoning tasks: GSM8K, MultiArith, and Year Parity.  The x-axis represents the number of top-k tokens considered during decoding, and the y-axis represents the accuracy.  The key finding is that CoT-decoding consistently improves accuracy across all model sizes, especially on GSM8K, demonstrating its effectiveness in eliciting reasoning capabilities from language models regardless of their scale.  The Year Parity task highlights that CoT-decoding improves performance even when simply increasing the model size does not naturally lead to better results.", "section": "3.1 CoT-Decoding Effectively Elicits Reasoning from Language Models"}]