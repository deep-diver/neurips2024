[{"type": "text", "text": "When to Act and When to Ask: Policy Learning With Deferral Under Hidden Confounding ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Marah Ghoummaid ", "page_idx": 0}, {"type": "text", "text": "Uri Shalit ", "page_idx": 0}, {"type": "text", "text": "Faculty of Data and Decision Sciences, Technion Faculty of Data and Decision Sciences, Technion marahghoummaid@gmail.com urishalit@tauex.tau.ac.il ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We consider the task of learning how to act in collaboration with a human expert based on observational data. The task is motivated by high-stake scenarios such as healthcare and welfare, where algorithmic action recommendations are made to a human expert, opening the option of deferring recommendation in cases where the human might act better on their own. This task is especially challenging when dealing with observational data, as using such data runs the risk of hidden confounders whose existence can lead to biased and harmful policies. However, unlike standard policy learning, the presence of a human expert can mitigate some of these risks. We build on the work of Mozannar and Sontag [2020] on consistent surrogate loss for learning with the option of deferral to an expert, where they solve a cost-sensitive supervised classification problem. Since we are solving a causal problem, where labels do not exist, we use a causal model to learn costs which are robust to a bounded degree of hidden confounding. We prove that our approach can take advantage of the strengths of both the model and the expert to obtain a better policy than either. We demonstrate our results by conducting experiments on synthetic and semi-synthetic data and show the advantages of our method compared to baselines. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Machine learning models are increasingly being developed to perform tasks performed by human decision-makers in high-stakes settings such as clinical decision making [Adams et al., 2022, Rajpurkar et al., 2022], criminal justice [Stevenson and Doleac, 2022] and social services [Behncke et al., 2009, McBrien et al., 2022]. Some of these tasks involve recommending actions such as medical treatment, releasing on bail, or receiving benefits. Typically the data used to train such action-recommendation models is based on past human decisions and their outcomes. For example, we might observe how patients were treated for diabetes and their subsequent health outcomes. ", "page_idx": 0}, {"type": "text", "text": "Learning to act better in the future based on past observed actions is a causal problem. For such problems we face the fundamental problem of causal inference [Holland, 1986], i.e. the unknowability of counterfactual outcomes: \u201cWould this patient have been better had they been treated differently?\u201d. Estimating causal quantities based on observational data such as hospital records is risky, as there is always the possibility of hidden confounding. Roughly speaking, this means there exist factors that affected the human decision maker and the outcome, but are unavailable to the model during training. ", "page_idx": 0}, {"type": "text", "text": "Learning models from data which has hidden confounding can lead to biased and harmful treatment assignment policies. However, human experts\u2019 decisions can also be biased, sub-optimal, or wrong. Thus, a setting in which both the human expert and the model can complement each other might be the best choice, mitigating the potential weaknesses and leveraging the strengths of both the model and the human expert [Bansal et al., 2020, Charusaie et al., 2022]. We believe this is especially pertinent in causal inference, as the human expert typically has access to the hidden confounders. ", "page_idx": 0}, {"type": "text", "text": "In this paper, we design a framework for learning a causal action recommendation model that can jointly work with a human decision-maker, while using observational data where hidden confounders might exist. We assume that the effect of the hidden confounders is limited (in a way we specify below), as without any assumption about the nature of hidden confounding, learning would be impossible. Our goal is to design a system consisting of a machine learning model estimating causal effects and a human expert working in a complementary setting. In addition to learning treatment assignment, the model learns the weaknesses and strengths of the human expert and will decide in each case whether to recommend a certain treatment to the patient, or whether it is better to defer the decision to the expert. The ultimate goal is to learn better policies and to reduce the burden on human experts. We call our method Causal Action Recommendation with Expert Deferral (CARED). ", "page_idx": 1}, {"type": "text", "text": "CARED follows in the footsteps of Mozannar and Sontag [2020], who developed a method for learning a classifier with the option of deferral to an expert. They propose a reduction of the problem to a cost-sensitive learning problem, where costs are based on the true labels of data samples. They give a surrogate loss for the cost-sensitive learning problem which generalizes the cross-entropy loss, and prove this loss is consistent, i.e., converges to the optimal solution of the original problem. Our problem can similarly be viewed as a classification problem with the option of deferring to an expert. However, the causal case is more difficult: the correct label for each sample is the best treatment to be prescribed to this sample based on its features, and due to the fundamental problem of causal inference we cannot know this label. Thus, unlike Mozannar and Sontag [2020] we do not have the true labels for our classification problem. ", "page_idx": 1}, {"type": "text", "text": "To overcome the challenges of the causal setting, we propose a set of costs that instead of being based on the true labels, are based on estimated bounds on counterfactual outcomes. These costs guide our model towards learning what is the right treatment for each patient while acknowledging that the human expert has access to additional information the model cannot access. We prove a generalization bound on the loss of the joint machine-expert system, and further prove that under certain assumptions about the model used to estimate the bounds mentioned above, the joint machineexpert system outperforms both the human expert and a pure machine learning model. Finally, we evaluate CARED on synthetic and semi-synthetic data to demonstrate how we can learn policies that outperform both pure machine learning policies and human experts. We further show that CARED outperforms a recently proposed method by Gao and Yin [2023] that addresses the same problem with an inverse-propensity weighted approach. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Many works focus on solving the problem of policy learning for action recommendations from observational data. Some notable approaches include reweighting by inverse propensity weighting (IPW) and other weighting techniques [Swaminathan and Joachims, 2015, Kallus, 2017, Beygelzimer and Langford, 2009], and the approach of using doubly robust scores to determine the optimal treatment assignment policy for binary treatments [Dud\u00edk et al., 2014, Athey and Wager, 2021,?, Kallus and Zhou, 2020]. Other methods predict the Conditional Average Treatment Effect (CATE) and use it as the guideline for treatment assignment for each sample, such as Jesson et al. [2021], and Kallus et al. [2019]. Most of these works assume ignorability, i.e., that there are no hidden confounders that affect both treatment assignment and the outcome in the data. As mentioned earlier, this assumption rarely holds when observational data is in use, and its presence, if not accounted for, can lead to biased and harmful policies. Our work builds on previous work for learning supervised classification problems with the deferral option by Mozannar and Sontag [2020], and is inspired by Athey and Wager [2021] who derive costs for learning policies from observational data under the assumption of no hidden confounders. ", "page_idx": 1}, {"type": "text", "text": "Gao and Yin [2023] present a framework for collaborative human-AI policy learning from observational data with deferral, building on earlier work [Gao et al., 2021] which did not allows for hidden confounding. To the best of our knowledge, theirs is the only existing method that learns a policy with the deferral option under hidden confounding. Their method minimizes an inverse propensity weighted estimator of the worst-case risk over a class of differentiable policies, and over an uncertainty set around the observed propensities. The uncertainty set is determined by constraints motivated by the Marginal Sensitivity Model [Tan, 2006]. While our method employs both outcome models and propensity scores, Gao and Yin [2023]\u2019s approach focuses on propensity score re-weighting. The re-weighted objective implies that only cases where the proposed policy agrees to a high degree with the observed policy are taken into account. As we show in the experimental section below, this (along with the lack of outcome model) might lead to under-performance, especially in the (common) case where the constraint set is not accurately known. Gao and Yin [2023] also explore the case where there are multiple specific human experts the model is optimizing for, which we plan to explore in future work. We also compare our method to Kallus and Zhou [2020], who take a similar approach as Gao and Yin [2023] for learning a policy while allowing the violation of the unconfoundedness assumption. However, they do not have the option of deferral. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "In other related work, Stensrud et al. [2024] consider the case where the expert\u2019s action can be used as input to the method, motivated by the fact that the expert typically has access to unobserved confounders. This is distinct from our use case but has interesting implications in identifying the so-called \u201csuperoptimal regime\u201d where the expert\u2019s action can strictly improve over a policy derived purely from the observables. Finally, Yin et al. [2024] offer a novel approach towards learning to defer in the non-causal setting, which could be adapted to our use case in the future. ", "page_idx": 2}, {"type": "text", "text": "3 Setup", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We work under an observational data setting with the Neyman-Rubin potential outcomes framework [Rubin, 2005]. Let $(X,A,Y(1),Y(0),U)$ be a sample drawn from the unobservable distribution $P_{\\mathrm{full}}$ , where $A\\in{\\mathcal{A}}=\\{0,1\\}$ is a binary treatment, $X\\in\\mathcal{X}\\subset\\mathbb{R}^{d}$ is a set of baseline covariates, $Y(1)$ and $Y(0)$ are the real-valued treated and untreated potential outcomes, respectively, and $U\\in\\mathbb{R}^{k}$ is an unobserved confounder. We face the fundamental problem of causal inference and only observe $n$ draws from the coarsened distribution $P$ over the observed variables $Z=(X,A,Y)$ , where we assume that $Y=Y(A)$ , i.e. (causal) consistency. We generally follow the convention that higher outcomes are better, we mention when assumed differently. We use the Marginal sensitivity Model (MSM) [Tan, 2006] as a way to model a limited degree of unobserved confounding. We are interested in learning a policy with the option of deferral, such that given the patient\u2019s covariates it either assigns a treatment or defers the decision to an expert. ", "page_idx": 2}, {"type": "text", "text": "Let $e(x)=P(A=1\\mid X=x)$ and $e(x,u)=P_{\\mathrm{full}}(A=1\\mid X=x,U=u)$ be the observed and full propensity scores, under $P$ and $P_{\\mathrm{full}}$ , the observed and the full unobserved distributions, respectively. ", "page_idx": 2}, {"type": "text", "text": "Assumption 1 (MSM Assumption). We assume $e(x),e(x,u)\\in(0,1)$ and that the ratio between the full odds of treatment $e(x,u)/(1-e(x,u))$ and the observed odds of treatment $e(x)/(1-e(x))$ is bounded by at most a factor of $\\Lambda\\geq1$ almost surely under $P_{\\mathrm{full}}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Lambda^{-1}\\leq\\left.\\frac{e(x,u)}{1-e(x,u)}\\right/\\frac{e(x)}{1-e(x)}\\leq\\Lambda.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Note that When $\\Lambda=1$ , Assumption 1 is equivalent to the classic assumption of unconfoundedness with respect to the observed $X$ . As $\\Lambda$ increases, the MSM allows for greater levels of unobserved confounding. Setting $\\Lambda$ is a matter of ongoing research, and is typically done by calibrating with respect to observed confounders [McClean et al., 2024]. In our case, it is also tied to the fraction of deferrals to the human expert and can be tuned to achieve a desired deferral rate. ", "page_idx": 2}, {"type": "text", "text": "For a treatment $a\\in\\{0,1\\}$ and a covariate vector $x\\in\\mathscr{X}$ we define the Conditional Average Potential Outcome (CAPO) as $\\mathbb{E}\\left[Y(a)|X=x\\right]$ . The Conditional Average Treatment Effect (CATE) is the difference $\\tau(x)\\,=\\,\\mathbb{E}\\left[Y(1)-Y(0)|X=x\\right]$ . Let $\\mathcal{M}(\\Lambda)$ be the set of distributions $\\tilde{P}_{\\mathrm{full}}$ that agree with the observed $P$ on $(X,A,Y)$ and that also agree with Assumption 1. Then $Y^{+}(x,a)=$ $\\begin{array}{r}{\\bar{\\operatorname*{sup}}_{\\tilde{P}_{\\mathrm{full}}\\in\\mathcal{M}(\\Lambda)}\\mathbb{E}\\left[Y(a)\\vert X=x\\right]}\\end{array}$ is the so-called sharp upper bound on the CAPO E $[Y(a)|X=x]$ , and similarly for $Y^{-}(x,a)$ , taking an infimum instead. We further let $\\hat{Y}^{+}(x,a),\\hat{Y}^{-}(x,a)$ be estimated upper and lower bound for the corresponding CAPO functions. ", "page_idx": 2}, {"type": "text", "text": "A policy with deferral is a function $\\pi:\\mathcal{X}\\rightarrow\\{0,1,\\bot\\}$ that maps covariates to a possible action, or defers the decision to an expert, where $\\perp$ denotes deferral. A policy $\\pi$ can be assessed by its policy value $V(\\pi)=\\mathbb{E}[Y(\\pi)]$ , where higher policy values are related to better policies unless stated otherwise. The action when the algorithm chooses to defer is the action the human expert has taken, see discussion of this in the limitations subsection (8). The learned policies will be assessed relative to a baseline policy value, such as the policy value based solely on the human expert. Our goal is for CARED to learn a policy that does no worse than the baseline policy and hopefully outperforms it. ", "page_idx": 2}, {"type": "text", "text": "4 CAPO-Based Policies ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We now present two baseline policies that can be defined based on the CAPO bounds. ", "page_idx": 3}, {"type": "text", "text": "The first is Bounds Policy Equation (1). This policy assigns treatment if the upper and lower bounds on the CATE have the same sign, and otherwise it defers, i.e. it defers if the CATE interval crosses 0. This approach was used in previous work, e.g. [Jesson et al., 2021, Oprescu et al., 2023]. The second is the Pessimistic Policy Equation (2). It is pessimistic in the sense that it does not trust the human expert, so it does not allow deferral to an expert. It agrees with the Bounds Policy in the cases where that policy would not defer, and in the cases where the Bounds Policy would defer it decides based on the lower bounds, which (assuming higher outcomes are better) are indeed pessimistic. ", "page_idx": 3}, {"type": "text", "text": "Let $\\hat{Q}(x)=(\\hat{Y}^{+}(x,0),\\hat{Y}^{-}(x,0),\\hat{Y}^{+}(x,1)),\\hat{Y}^{-}(x,1))$ be the CAPO bounds estimates for a sample $(X=x,A=a,Y=y)$ . We further assume that the expert policy is reflected in the training data in the sense that their policy is $\\pi_{\\mathrm{exp}}(x_{i})=a_{i}$ . Then, given a model that supplies CAPO bounds $\\hat{Q}$ , we define the following policies, which we consider as baselines henceforth: ", "page_idx": 3}, {"type": "text", "text": "Bounds Policy: ", "text_level": 1, "page_idx": 3}, {"type": "equation", "text": "$$\n\\pi_{\\mathrm{bounds}}^{\\hat{Q}}(x)=\\left\\{\\begin{array}{l l}{1}&{\\mathrm{~if~}\\hat{Y}^{-}(x,1)-\\hat{Y}^{+}(x,0)>0}\\\\ {0}&{\\mathrm{~if~}\\hat{Y}^{+}(x,1)-\\hat{Y}^{-}(x,0)<0}\\\\ {\\perp}&{\\mathrm{~otherwise.}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Pessimistic Policy: ", "text_level": 1, "page_idx": 3}, {"type": "equation", "text": "$$\n\\pi_{\\mathrm{pessimistic}}^{\\hat{Q}}(x)=\\left\\{\\begin{array}{l l}{1}&{\\mathrm{~if~}\\hat{Y}^{-}(x,1)-\\hat{Y}^{+}(x,0)>0}\\\\ {0}&{\\mathrm{~if~}\\hat{Y}^{+}(x,1)-\\hat{Y}^{-}(x,0)<0}\\\\ {1}&{\\mathrm{~otherwise,~}\\hat{Y}^{-}(x,1)-\\hat{Y}^{-}(x,0)>0}\\\\ {0}&{\\mathrm{~otherwise}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The above policies, while accounting for hidden confounding, even when allowing deferral to an expert, do not learn the strengths and weaknesses of the expert, so they might not be the optimal policies. We show this theoretically in Section 6.2, and empirically in Section 7. ", "page_idx": 3}, {"type": "text", "text": "5 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we present our method in which we design a machine-expert system for learning a policy with the option of deferral to an expert under hidden confounding. In 5.1 we describe the joint machine-expert objective we are interested in optimizing, the challenges it imposes, and how to optimize a consistent surrogate cost-sensitive loss that converges to the optimal solution of the original objective, based on Mozannar and Sontag [2020]. Then in 5.2 we introduce our proposed set of costs based on the CAPO bounds for solving this cost-sensitive problem. Lastly, we explain the step of CAPO estimation in 5.3. Our algorithm is summarized below in Algorithm 1. ", "page_idx": 3}, {"type": "text", "text": "5.1 Joint machine-expert objective function ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We design a joint machine-expert system, where we aim to learn a policy $\\pi:\\mathcal{X}\\to\\mathcal{A}\\cup\\{\\bot\\}$ . We denote by $m\\in A$ the expert\u2019s action that is assumed to be drawn from the distribution $M|X=$ $x,U=u$ . Note that the expert might have access to additional information \u2013 the hidden confounder $U$ , which is unavailable to the model. We cast our problem as a cost-sensitive optimization objective ", "page_idx": 3}, {"type": "equation", "text": "$$\nL(\\pi)=\\mathbb{E}_{x,y\\sim P(X,Y),m\\sim M|x,u}[C(x,\\pi(x))\\mathbb{I}_{\\pi(x)\\neq\\bot}+C_{\\bot}(x,m,y)\\mathbb{I}_{\\pi(x)=\\bot}],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $C(x,a)$ is the cost incurred by the system if for a sample with covariates $x$ , an action/treatment $a$ was chosen by the model, and $\\dot{C}_{\\bot}(x,\\dot{m},y)$ is the cost incurred when action $m$ was chosen for sample $x$ by the expert. We explain below why $C_{\\perp}$ depends on $y$ but $C$ does not. ", "page_idx": 3}, {"type": "text", "text": "In the case of no unobserved confounding and no deferral, Athey and Wager [2021] have shown how the costs above can be set such that the cost minimizer is the policy with optimal policy value. We describe how we set costs that address both hidden confounding and deferral in the next subsection. ", "page_idx": 3}, {"type": "text", "text": "The above objective is non-convex and difficult to optimize. We deal with this challenge by building on the approach of Mozannar and Sontag [2020] for learning classifiers with the option of deferral to an expert, where they have a similar objective as in Equation (3). They give a convex and consistent surrogate loss for the cost-sensitive learning problem, which is a weighted cross-entropy loss, where the weights are based on a set of costs they build using the true labels for the classification problem. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "As explained earlier, our problem can also be viewed as a classification problem with the option of deferring to an expert. However, due to the fundamental problem of causal inference [Holland, 1986] we do not have the true labels, which are in our case, the best treatment to be prescribed to this sample based on its features, which is a challenge that we deal with in Section 5.2. ", "page_idx": 4}, {"type": "text", "text": "Following is the surrogate loss we optimize, in Section 5.2 we present our proposed set of costs, and in Section 6 we give consistency and generalization guarantees for this surrogate loss. ", "page_idx": 4}, {"type": "text", "text": "Let $\\pi_{i}~:~\\mathcal{X}~\\rightarrow~\\mathbb{R}$ be the raw output of the policy $\\pi$ corresponding to a class $i\\;\\in\\;\\{0,1,\\bot\\}$ , and define $\\pi(x)~=~\\arg\\operatorname*{max}_{i\\in\\{0,1,\\perp\\}}\\pi_{i}(x)$ . Let $z~=~(x,a,y)$ be a sample, and ${\\hat{Q}}(x)\\;=\\;$ $(\\hat{Y}^{+}(x,0),\\hat{Y}^{-}(x,0),\\hat{Y}^{+}(x,1)),\\hat{Y}^{-}(x,1))$ the CAPO bounds. These bounds are then used to define the scores $c(0)=C(x,0)$ , $c(1)=C(x,1)$ , and $c(\\perp)=C_{\\perp}(x,m,y)$ as we show in Section 5.2 below. Define $w^{j}(z,\\hat{Q}(x))\\,=\\,\\operatorname*{max}_{k\\in\\{0,1,\\perp\\}}\\,c(k)\\,-\\,c(j)$ . Then the surrogate loss function for Equation (3) is given by: ", "page_idx": 4}, {"type": "equation", "text": "$$\nL_{C E}(\\pi,z;\\hat{Q})=\\sum_{j\\in\\{0,1,\\perp\\}}-w^{j}(z,\\hat{Q}(x))\\log\\left(\\frac{\\exp(\\pi_{j}(x))}{\\sum_{k\\in\\{0,1,\\perp\\}}\\exp(\\pi_{k}(x))}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The method we propose uses the above system loss for learning a policy from observational data with the presence of a limited degree of hidden confounding and the ability to defer. The difficulty in this case is constructing the costs $C(x,0),C(x,1)$ and $C_{\\bot}(x,m,y)$ which are used to derive the weights $w^{j}$ , since ground truth labels are never available \u2014 a challenge we address now. ", "page_idx": 4}, {"type": "text", "text": "5.2 Action Costs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "As mentioned earlier, our classification problem does not have the true labels, as opposed to Mozannar and Sontag [2020]. We therefore propose a set of costs based on estimated bounds on counterfactual outcomes rather than on the true labels, guiding our model in learning for which cases it can safely recommend actions, and which to defer to the human expert. These costs are then used to derive the weights in the objective Equation (4). ", "page_idx": 4}, {"type": "text", "text": "For $i\\in[n]$ let $\\left(x_{i},y_{i},a_{i}\\right)$ be the $i$ -th observed sample, and let $\\hat{Y}^{+}(x_{i},a),\\hat{Y}^{-}(x_{i},a)$ be the estimates of the upper and lower bounds of the CAPO for an action $a\\in{\\mathcal{A}}$ , and a covariates $x\\in\\mathscr{X}$ . We propose setting the cost of assigning the action $a$ to the $i$ -th sample to be: ", "page_idx": 4}, {"type": "equation", "text": "$$\nC(x_{i},a)=\\hat{Y}^{+}(x_{i},1-a)-\\hat{Y}^{-}(x_{i},a).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This cost encourages the model to choose the action with the highest outcome. When this is not the case, i.e. the model chooses another action than the one with the highest estimated outcome, then it will incur a cost which is the difference between the lower bound of the outcome corresponding to the action chosen, and the upper bound of the outcome corresponding to the action which was supposed to be chosen based on the outcomes. This cost can be thought of as the worst-case regret of the model relative to the optimal action. It is important to note that this is only one option among many options for building a set of costs for this purpose. For instance, we can switch the roles of the upper and lower bounds of the CAPOs to obtain various costs expressing differing levels of risk-aversion. ", "page_idx": 4}, {"type": "text", "text": "As for the cost of deferring to an expert, following the same logic, we propose two alternatives. Given covariates $x_{i}$ , expert action $m=a_{i}$ and outcome $y_{i}$ the conservative deferral cost is: ", "page_idx": 4}, {"type": "equation", "text": "$$\n{C}_{\\bot}(x_{i},a_{i},y_{i})=\\hat{Y}^{-}(x_{i},1-a_{i})-y_{i},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and similarly the optimistic deferral cost is: $C_{\\perp}(x_{i},a_{i},y_{i})=\\hat{Y}^{+}(x_{i},1-a_{i})-y_{i}$ . These costs represent to what extent are we willing to take an action different from the one the expert took which resulted in the outcome actually seen in the data, i.e. $m=a_{i}$ which lead to outcome $y_{i}$ . In the optimistic case, deferring to an expert results in incurring a greater cost in comparison to its conservative counterpart, encouraging the model to make fewer deferrals and focusing on samples it is highly uncertain about. It is optimistic in the sense of assuming the model\u2019s estimates are likely correct. Choosing between these alternatives should to be done based on the characteristics of the specific policy and use case. In Appendix A.3 we show a comparative analysis of the two alternatives. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "To summarize we present here the costs corresponding to the conservative approach: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{C(x_{i},1)=\\hat{Y}^{+}(x_{i},0)-\\hat{Y}^{-}(x_{i},1)\\;\\;}\\\\ {C(x_{i},0)=\\hat{Y}^{+}(x_{i},1)-\\hat{Y}^{-}(x_{i},0)\\;\\;}\\\\ {C_{\\bot}(x_{i},a_{i},y_{i})=\\left\\{\\hat{Y}^{-}(x_{i},0)-y_{i},\\;\\;\\;\\mathrm{if}\\;a_{i}=1\\right.}\\\\ {\\left.\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Taken together, these costs encourage the model to classify a sample as $a=1$ or $a=0$ in cases where it is certain that particular action would be best, and to defer cases where the expert seems to have made the correct decision (in the conservative case). An illustrative example of the costs choice can be found in Appendix A. Additionally, we demonstrate costs\u2019 coherence in Theorem 1. ", "page_idx": 5}, {"type": "text", "text": "5.3 CAPO Estimation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "From an algorithmic point of view, any method that yields upper and lower bounds on the CAPO given some degree of hidden confounding can be used to obtain the costs Section 5.2. As we will see in the next section, our theoretical results require bounds with certain generalization and validity properties to hold. In this work, we use the B-learner [Oprescu et al., 2023] for estimating the upper and lower bounds of the CAPO, as it has the properties needed for the theoretical analysis and shows good performance in practice, and being a meta-learner it can accommodate various base learners, including random forests and neural networks. ", "page_idx": 5}, {"type": "table", "img_path": "taI8M5DiXj/tmp/d977adc4f48ff54f9f4c9c1a47812c1d849ba3875a50cab8c09346dce3045b19.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "6 Theoretical Guarantees ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We present theoretical guarantees for CARED as follows: $\\ln6.1$ we show that the optimum of the surrogate loss function $L_{C E}$ (eq. 4) agrees with the optimum of the machine-expert loss function $L$ (eq. 3). Then, in 6.2 we show that the costs we use in $L_{C E}$ are coherent, in the sense that minimizing them leads to a decision that is non-inferior to the decision either the expert or the machine would have made on their own. Finally, in 6.3 we give a generalization bound for the loss $L_{C E}$ . ", "page_idx": 5}, {"type": "text", "text": "6.1 Consistency ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Corollary 1. $L_{C E}$ is convex in $\\pi$ and is a consistent loss function for $L$ : Let $\\tilde{\\pi}=\\arg\\operatorname*{inf}_{\\pi}\\mathbb{E}[L_{C E}(\\pi,z;\\hat{Q})],$ , then $\\tilde{\\pi}=\\arg\\operatorname*{inf}_{\\pi}L(\\pi)$ . ", "page_idx": 5}, {"type": "text", "text": "This result is a straightforward adaptation of Proposition 1 of Mozannar and Sontag [2020], as their result is not sensitive to the particular choice of costs (B.0.1). It motivates using the surrogate loss function $L_{C E}$ which is much more amenable to optimization than the original machine-expert loss $L$ ", "page_idx": 5}, {"type": "text", "text": "6.2 Costs Are Coherent ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now show that the costs $C(x,0),C(x,1),C_{\\bot}(x,a,y)$ are coherent, in the sense that they indeed work as intended: a policy that minimizes them locally is always at least as good as (and indeed, in general better than) both the expert on their own and the machine on its own. ", "page_idx": 6}, {"type": "text", "text": "Definition 1. [Bound Validity] For a sample $x$ with corresponding potential outcomes $Y(0)$ and $Y(1)$ , let $\\hat{Q}(x)=(\\hat{Y}^{+}(x,0),\\hat{Y}^{-}(x,0),\\hat{Y}^{+}(x,1)),\\hat{Y}^{-}(x,1))$ be the estimated CAPO bounds. Then $\\hat{Q}$ is valid for $(Y(0),Y(1))$ if $Y(a)\\in[\\hat{Y}^{-}(x,a),\\hat{Y}^{+}(x,a)]$ for $a\\in\\{0,1\\}$ . ", "page_idx": 6}, {"type": "text", "text": "Validity means that bounds indeed contain their respective potential outcomes. For example, Oprescu et al. [2023] prove that their bounds are valid on average. ", "page_idx": 6}, {"type": "text", "text": "Theorem 1 (Costs are coherent). Let $\\tilde{\\pi}(x_{i})\\in\\arg\\operatorname*{min}L_{C E}(\\pi,z_{i};\\hat{Q}(x_{i}))$ , $\\pi_{e x p}$ the expert\u2019s policy, and \u03c0bQounds(xi) the CAPO-based policy defined in $^{\\,l}$ . If $\\hat{Q}(x_{i})$ is valid for $(Y_{i}(0),Y_{i}(1))$ , then $Y(\\tilde{\\pi}(x_{i}))\\geq\\operatorname*{max}\\left\\{Y\\left(\\pi_{e x p}(x_{i})\\right),\\,Y\\left(\\pi_{b o u n d s}^{\\hat{Q}}(x_{i})\\right)\\right\\}$ . ", "page_idx": 6}, {"type": "text", "text": "Furthermore, under certain technical conditions on the distribution of $Y(0),Y(1)$ and $\\hat{Q}$ , the inequality is strong with non-zero probability for each sample. ", "page_idx": 6}, {"type": "text", "text": "Theorem 1 shows that whenever the bounds include the true potential outcome, the action that minimizes our proposed loss function is at least as good as the action implied by the baseline $\\pi_{\\mathrm{bo}}^{\\hat{Q}}$ unds policy, as well as the human expert policy. ", "page_idx": 6}, {"type": "text", "text": "6.3 Generalization Bound ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now show the generalization bound for our machine-expert system loss. We start with assumptions, then we state the main theorem. ", "page_idx": 6}, {"type": "text", "text": "Assumption 2 (Policy learners). Let $\\Pi$ be the class of policies over which we optimize Equation (4). We assume the class $\\Pi$ is with restricted complexity, specifically $\\begin{array}{r}{\\mathcal{R}_{n}(\\Pi)=\\dot{O(\\frac{1}{\\sqrt{n}})}}\\end{array}$ , where ${\\mathcal{R}}_{n}(\\Pi)$ is the Rademacher Complexity of the policy class $\\Pi$ . Classes that have this property include linear functions, logistic functions, decision trees with a bounded depth, and neural networks with weight decay or dropout Kallus and Zhou [2020]. ", "page_idx": 6}, {"type": "text", "text": "Assumption 3 (Policy Learner with Bounded Outputs). Let $\\pi\\in\\Pi$ and $\\pi_{j}:\\mathcal{X}\\rightarrow\\mathbb{R}$ be the raw output of the classifier $\\pi$ corresponding to a class/action $j\\in\\{0,1,\\perp\\}$ . We assume there exists a constant $C_{\\pi}$ such that $|\\pi_{j}|\\le C_{\\pi}$ for all $j$ . ", "page_idx": 6}, {"type": "text", "text": "Assumption 4 (Boundedness of the Outcomes). $Y$ is bounded, i.e. $|Y|\\le C_{Y}$ for $C_{Y}>0$ . ", "page_idx": 6}, {"type": "text", "text": "Assumption 5. [Rates for ERM CAPO Bounds Estimators] The CAPO bounds estimators $\\hat{Y}^{\\circ}(x,a)$ for $a\\in\\{0,1\\}$ and $\\diamond\\in\\{+,-\\}$ satisfy: $\\|\\hat{Y}^{\\diamond}(x,a)-Y^{\\diamond}(x,a)\\|\\lesssim O_{p}(n^{-1/(2+r)})$ with $0<r<2$ . ", "page_idx": 6}, {"type": "text", "text": "Assumption 5 implies that the convergence rate of the $L_{2}$ norm of the estimation error of the CAPO bounds converges not too slowly (in probability).Notably, the B-Learner bound estimates satisfy Assumption 5 under some assumption on the class of policy learners and the nuisance estimators, according to Corollary 1 from Oprescu et al. [2023]. ", "page_idx": 6}, {"type": "text", "text": "We now give a generalization bound on the loss of the joint machine-expert system: ", "page_idx": 6}, {"type": "text", "text": "Theorem 2 (Generalization Bound). Given a policy class $\\Pi$ satisfying Assumption 2, Assumption 3 with a constant $C_{\\pi}$ , Assumption $^{4}$ with a constant $C_{Y}$ , and Assumption 5 with $0<r<2$ . ", "page_idx": 6}, {"type": "text", "text": "Let $\\pi\\,\\in\\,\\Pi$ , be a policy, and let $Q,{\\hat{Q}}$ be the CAPO bounds and the estimated CAPO bounds respectively. Then there exists a constant $C>0$ such that with a probability of at least $1-\\delta$ the following holds: ", "page_idx": 6}, {"type": "equation", "text": "$$\nL_{D}(\\pi;Q)-L_{S}(\\pi;\\hat{Q})\\leq2\\mathcal{R}_{n}(\\Pi)+24\\cdot C_{\\pi}\\cdot\\left(C_{Y}\\sqrt{\\frac{2\\ln(4/\\delta)}{n}}+C\\cdot\\left(n^{-1/(2+r)}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\begin{array}{r l r}{L_{S}(\\pi;\\hat{Q})}&{{}=}&{\\frac{1}{n}\\sum_{i=1}^{n}L_{C E}(\\pi,z_{i};\\hat{Q}(x_{i}))}\\end{array}$ is the training loss, and $\\begin{array}{r l}{L_{D}(\\pi;Q)}&{{}=}\\end{array}$ $\\mathbb{E}_{z\\sim p(z)}\\left[L_{C E}(\\pi,z;Q(x))\\right]$ is the expected loss for the combined machine-expert system loss in 4, and ${\\mathcal{R}}_{n}(\\Pi)$ is the Rademacher Complexity of the policy class $\\Pi$ . ", "page_idx": 6}, {"type": "text", "text": "7 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Here we examine the utility of CARED by conducting experiments on synthetic and semi-synthetic data in Section 7.1 and Section 7.2, respectively. Further details about the experiments, datasets, models, and hyper-parameters can be found in Appendix C. ", "page_idx": 7}, {"type": "text", "text": "Our main comparison is to Gao and Yin [2023]\u2019s method (ConfHAI) that allows for a bounded degree of confounding in addition to allowing the option of deferral to a human expert. They learn a policy by optimizing a minimax reweighting-based risk estimate over an uncertainty set around the observed propensities. The uncertainty sets are determined by the MSM assumption, which is the same assumption we employ to bound the degree of hidden confounding. Another baseline we consider is Kallus and Zhou [2020]\u2019s method (CRLogit) which proceeds under a similar approach and allows for a bounded degree of confounding, but does not allow deferral to a human. Additionally, we evaluate all methods against the following baselines: Oracle Policy which is the best policy that assigns for each patient the true best treatment. In both experiments, this policy is available to us, as we have the true potential outcomes. Current Expert/ Baseline Policy this is the default policy that we refer to in cases of deferral. Typically, it is the policy of the current expert, but it can be any other policy of our choice. Pessimistic Policy: Using the Equation (2) policy with the bounds $\\hat{Q}$ given by the B-learner Oprescu et al. [2023]. B-Learner Policy: Using the $\\pi_{\\mathrm{bounds}}^{\\hat{Q}}$ policy of Equation (1) with the bounds $\\hat{Q}$ given by the B-learner [Oprescu et al., 2023]. We use the exact same $\\hat{Q}$ in Algorithm 1, and in the B-learner and pessimistic policies. Random Deferral Policy: this is a variant of the B-Learner policy which introduces a different approach to deferral, where samples are deferred randomly based on a specified deferral rate. ", "page_idx": 7}, {"type": "text", "text": "7.1 Synthetic data ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this experiment, we replicate the study from Gao and Yin [2023] using synthetic data, comparing our method to ConfHAI and CRLogit. Since this experiment is measuting regret, we follow Gao et al. [2021] and compare to a Baseline Policy that assigns treatment $a=0$ for all patients and is denoted by $\\pi_{0}$ . In this experiment lower outcomes are better. ", "page_idx": 7}, {"type": "text", "text": "Data The data is generated according to the following data generation process: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\xi\\sim\\mathbf{Bern}(0.5),\\quad X\\sim\\mathcal{N}((2\\xi-1)\\mu_{x},I_{5}),}\\\\ &{U=\\mathbb{I}[Y(1)<Y(0)],}\\\\ &{Y(A)=\\beta_{0}^{\\top}x+\\mathbb{I}[A=1]\\beta_{t r e a t}^{\\top}x+0.5\\alpha\\xi\\mathbb{I}[A=1]+\\eta+\\omega\\xi+\\epsilon}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\beta_{0}=[0,0.5,-0.5,0,0]$ , $\\beta_{t r e a t}=[-1.5,1,-1.5,1,0.5]$ , $\\mu_{x}=[-1,0.5,-1,0,-1]$ , $\\eta=2.5$ , $\\alpha=-2$ , $\\omega=1.5$ , and $\\mathbf{\\boldsymbol{\\epsilon}}\\sim\\mathcal{N}(\\mathbf{\\boldsymbol{0}},\\mathbf{\\boldsymbol{1}})$ . The nominal propensity is logistic by $X$ , $e(X)=\\sigma(\\beta^{\\top}X)$ with $\\beta=[0,0.75,-0.5,0,-1,0]$ . The confounder is denoted by $U$ , and the true propensity score is given by: $\\begin{array}{r}{\\dot{e(X,U)}=\\frac{(\\dot{\\Lambda}_{0}U+1-U)e(X)}{[1+2(\\Lambda_{0}-1)e(X)-\\Lambda_{0}]U+\\Lambda_{0}+(1-\\Lambda_{0})e(X)}}\\end{array}$ , with the true $\\Lambda_{0}$ , such that $l o g(\\Lambda_{0})=2.5$ . ", "page_idx": 7}, {"type": "text", "text": "Experiment We replicate the experiment from Gao and Yin [2023] and run 10 trials, with different instances of the above data, a train data size of 2000, and test data of size 10000. For each trial, we vary the sensitivity parameter $\\Lambda$ in $\\{0.01,0.5,1,1.5,2,2.5,3,3.5,4\\}$ , corresponding to various levels of assumed hidden confounding. We compare the policy regret for the returned policy for each method relative to the Baseline Policy. In addition, we report the policy regret for the human expert in the dataset (Human\u2019s Policy) which is reflected by the variable $A$ in the dataset. As for the CARED policy, we obtain it by applying Algorithm 1 with a logistic policy implemented as a single-layer MLP network. ", "page_idx": 7}, {"type": "text", "text": "Results In Figure 1 we see that CARED outperforms all other methods, specifically improving over the expert for all $\\Lambda$ values. In contrast, the ConfHAI and CRLogit improve over the expert for a limited range of values of $\\Lambda$ , which is for a range around the true value of the sensitivity parameter $\\Lambda_{0}$ showed in the plot, and perform noticeably worse than the expert for many $\\Lambda$ values when the $\\Lambda$ is mis-specified. In contrast, CARED shows robustness to all $\\Lambda$ values, making it a safer choice, as correctly specifying $\\Lambda$ is challenging [McClean et al., 2024]. ", "page_idx": 7}, {"type": "text", "text": "7.2 IHDP Hidden Confounding ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this experiment, we aim to demonstrate how our method can adapt to the human expert using a semi-synthetic dataset: the IHDP hidden confounding dataset. ", "page_idx": 7}, {"type": "image", "img_path": "taI8M5DiXj/tmp/3ac8424a498902e3dfd03d11bef2a04d887debe33de8660a9751aacd59fc585e.jpg", "img_caption": ["Figure 1: Synthetic Data: Policy regret, lower policy regret is better. $\\mathbf{X}_{\\mathrm{~}}$ -axis is levels of hidden confounding according to the MSM model. The true $\\Lambda_{0}$ is reported as a black vertical line. Human\u2019s Policy is the human expert\u2019s choices $(A)$ as observed in the data. CRLogit Policy [Kallus and Zhou, 2020]: learn a policy with an IPW approach under hidden confounding, without deferral. ConfHAI Policy [Gao and Yin, 2023] similarly learns a policy with IPW approach under hidden confounding with deferral. CARED: our proposed method Pessimistic Policy and $B$ -Learner Policy are based on CAPO bounds from the B-Learner [Oprescu et al., 2023] and are defined in Equation (2) and Equation (1), respectively. Oracle Policy assigns the best true treatment to each patient. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Dataset The hidden-confounding version of the IHDP dataset [Hill, 2011] was introduced by Jesson et al. [2021]. The Infant Health and Development Program (IHDP) dataset [Hill, 2011] is a dataset consisting of real covariates and a treatment that were collected from an RCT that targeted low-birthweight, premature infants. The treatment was providing both intensive high-quality child care and home visits from a trained provider, while the outcomes were simulated according to the response surface B described by Hill [2011]. Jesson et al. [2021] induced hidden confounding onto the dataset by hiding the $x_{9}$ covariate; however, the response surface B from Hill [2011] is still used to generate the observed outcomes. In this dataset higher outcomes are better, thus we build the loss function using the appropriate set of costs for this assumption. ", "page_idx": 8}, {"type": "text", "text": "Semi-Synthetic Expert (Current Expert) We design a semi-synthetic expert, based on the original observed expert\u2019s policy, and the Oracle policy. The new semi-synthetic expert is designed as follows: when the feature $x_{17}$ -\u201cworked during pregnancy\u201d receives the value 1, then the new expert is identical to the Oracle policy, and otherwise, it is identical to the original expert. This way, the new expert is perfect when $x_{17}$ , which happens in probability 0.59. See details in Appendix C.2.1. ", "page_idx": 8}, {"type": "text", "text": "Experiment We conduct this experiment on the modified IHDP Hidden Confounding consisting of tuples $(X,A^{\\prime},Y^{\\prime})$ , where the $A^{\\prime}$ is the new expert, and the $Y^{\\prime}$ is the outcome corresponding to the expert $A$ . We generate 1000 realizations of the dataset. For each realization, we train the policy model for different values of the causal uncertainty parameter $\\Lambda$ and calculate: the rate of the samples deferred to the expert, and the policy value of the learned policy. We then plot the average policy value per causal uncertainty level - the parameter $\\Lambda$ in Figure 2a, and the average policy value per average deferral rate over all trials in Figure 2b. As for our method, we obtain our policy by applying Algorithm 1 with a logistic policy implemented as a single-layer MLP network. ", "page_idx": 8}, {"type": "text", "text": "Results In Figure 2a We observe that CARED consistently improves upon the expert\u2019s policy, yielding a higher average policy value across all levels of the causal uncertainty parameter, $\\Lambda$ . Notably, it outperforms the baseline methods for reasonable values of $\\Lambda$ , and achieves a policy value closest to that of the optimal Oracle Policy. As in the previous experiment, CARED demonstrates greater robustness to variations in the assumed level of confounding. For high $\\Lambda$ values, CARED\u2019s performance trends toward that of the expert policy, which is expected, as larger $\\Lambda$ values result in less informative and thus less reliable CAPO intervals. In contrast, ConfHAI only begins to improve when the assumed confounding level closely matches the true sensitivity parameter. ", "page_idx": 8}, {"type": "text", "text": "These results highlight the robustness and reliability of our policy, showing that it performs well even when the sensitivity parameter is misspecified by the data analyst. Moreover, when the sensitivity parameter reaches very high levels, CARED avoids unnecessary risk by converging towards the expert policy, reflecting its conservative response to high causal uncertainty. ", "page_idx": 8}, {"type": "text", "text": "In Figure 2b we compare our method only to baselines that incorporate deferral. Here, instead of regret, we assess the policies by their average policy value across all trials, and we plot the performance according to the average deferral rate over all trials. Our results show that our policy consistently outperforms the other methods at equivalent deferral rates. ", "page_idx": 9}, {"type": "text", "text": "The peak of the plot of the CARED Policy is when both the model and the expert work in collaboration and take advantage of the strengths of both the ML model and the expert. This happens at a deferral rate that is close to $\\sim0.6$ , which is almost precisely the percentage of the cases where the expert does as well as the oracle policy according to how the expert was designed. This indicates that our model learns the strength of the expert, and knows when to defer to the expert. While ConfHAI defers only a small fraction of samples on average, which might indicate that this method does not learn the weaknesses and strengths of the expert. ", "page_idx": 9}, {"type": "image", "img_path": "taI8M5DiXj/tmp/297d0e37a3753fa8b831a7ef3f35717d5798a0c6d4ba74e5158c2703cd923054.jpg", "img_caption": ["(a) Policy Value for different values of $\\Lambda$ "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "taI8M5DiXj/tmp/0eed098a891d40bd9a499605a1cf2864ce30f990f65fb8d2fa116b636cb5993f.jpg", "img_caption": ["(b) Policy Value for different levels of deferral "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Figure 2: IHDP Hidden Confounding: Figure 2a shows policy value for different levels of allowed hidden confounding in the data according to the MSM model Assumption 1. The ${\\bf X}$ -axis represents different values of the uncertainty parameter $\\Lambda$ , and the true $\\Lambda_{0}$ is reported as a black vertical line. Figure 2b shows policy value for different rates of deferral. The $\\mathbf{X}_{\\mathrm{}}$ -axis represents different levels of practitioner caution by varying the percentage of recommendations deferred. The methods shown here are the same as in Figure 1, in addition to Random Deferral Policy that defers a randomly chosen fraction of samples to the expert at each deferral rate. ", "page_idx": 9}, {"type": "text", "text": "8 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper we proposed CARED: a method for learning policies from observational data where the model can recommend a treatment or defer to an expert. When learning to act from observational data which includes experts\u2019 actions, hidden confounders are by necessity factors that influenced the experts\u2019 decisions, and are thus available to them even though they are unavailable to the model. This makes our setting pertinent to the problem of safely learning to recommend actions based on observational data where the actions were taken by human experts, as is the case in many medical and legal settings, for example. CARED thus mitigates some of the risk of learning causal models from observational data. A further advantage is that we might not need to know the true $\\Lambda$ for the system to be useful: instead we might wish to calibrate the rate of deferral instead, as that might be the more practical constraint the system faces, in terms of human labor vs. the joint system\u2019s policy value. ", "page_idx": 9}, {"type": "text", "text": "We showed both theoretically and by experiments in synthetic and semi-synthetic data that our method outperforms relevant baselines for this task, and can combine in a synergistic manner the expert\u2019s and machine learning model\u2019s capabilities. ", "page_idx": 9}, {"type": "text", "text": "Limitations and future work Our current method requires access to the experts actions in the observational data; it cannot accommodate directly a different expert, since we cannot know the potential outcomes corresponding to how that new expert would have acted. A further limitation is the assumption that at test time the experts would behave the same for the deferred cases as they would have before system deployment. Realistically, deploying an action recommendation system might change the experts\u2019 behavior more broadly. Accounting for this would require testing and modeling the experts behavior in such conditions. Future work will explore this more dynamic setting, taking into account the ongoing interactions and learning between the human expert and the system. ", "page_idx": 9}, {"type": "text", "text": "9 Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We would like to express our gratitude to our colleague, Rom Gutman, for his valuable insights and support throughout this work. We also thank Angela Zhou and Ruijiang Gao whose methods we replicated and compared in our evaluations, for their their helpful responses to our questions about implementing their methods. Additionally, we extend our appreciation to Hussein Mozannar for his generous assistance in clarifying aspects of his work, which served as the basis for our approach. We would like to thank the anonymous reviewers for useful discussion and feedback. MG and US were supported by ISF grant 2456/23. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "R. Adams, K. E. Henry, A. Sridharan, H. Soleimani, A. Zhan, N. Rawat, L. Johnson, D. N. Hager, S. E. Cosgrove, A. Markowski, et al. Prospective, multi-site study of patient outcomes after implementation of the trews machine learning-based early warning system for sepsis. Nature medicine, 28(7):1455\u20131460, 2022.   \nS. Athey and S. Wager. Policy learning with observational data. Econometrica, 89(1):133\u2013161, 2021.   \nG. Bansal, B. Nushi, E. Kamar, E. Horvitz, and D. S. Weld. Optimizing AI for teamwork. arXiv preprint arXiv:2004.13102, 2020.   \nS. Behncke, M. Fr\u00f6lich, and M. Lechner. Targeting labour market programmes\u2014results from a randomized experiment. Swiss Journal of Economics and Statistics, 145(3):221\u2013268, 2009.   \nA. Beygelzimer and J. Langford. The offset tree for learning with partial labels. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 129\u2013138, 2009.   \nJ. Brooks-Gunn, F.-r. Liaw, and P. K. Klebanov. Effects of early intervention on cognitive function of low birth weight preterm infants. The Journal of pediatrics, 120(3):350\u2013359, 1992.   \nM.-A. Charusaie, H. Mozannar, D. Sontag, and S. Samadi. Sample efficient learning of predictors that complement humans. In International Conference on Machine Learning, pages 2972\u20133005. PMLR, 2022.   \nA. Curth, D. Svensson, J. Weatherall, and M. van der Schaar. Really doing great at estimating cate? a critical look at ml benchmarking practices in treatment effect estimation. In Thirty-ffith conference on neural information processing systems datasets and benchmarks track (round 2), 2021.   \nM. Dud\u00edk, D. Erhan, J. Langford, and L. Li. Doubly robust policy evaluation and optimization. 2014.   \nR. Gao and M. Yin. Confounding-robust policy improvement with human-ai teams. arXiv preprint arXiv:2310.08824, 2023.   \nR. Gao, M. Saar-Tsechansky, M. De-Arteaga, L. Han, M. K. Lee, and M. Lease. Human-ai collaboration with bandit feedback. In Z.-H. Zhou, editor, Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, pages 1722\u20131728. International Joint Conferences on Artificial Intelligence Organization, 8 2021.   \nJ. L. Hill. Bayesian nonparametric modeling for causal inference. Journal of Computational and Graphical Statistics, 20(1):217\u2013240, 2011.   \nP. W. Holland. Statistics and causal inference. Journal of the American statistical Association, 81(396):945\u2013960, 1986.   \nA. Jesson, S. Mindermann, Y. Gal, and U. Shalit. Quantifying ignorance in individual-level causal-effect estimates under hidden confounding. In International Conference on Machine Learning, pages 4829\u20134838. PMLR, 2021.   \nN. Kallus. Recursive partitioning for personalization using observational data. In International conference on machine learning, pages 1789\u20131798. PMLR, 2017.   \nN. Kallus and A. Zhou. Minimax-optimal policy learning under unobserved confounding. Management Science 67(5):2870-2890, 2020.   \nN. Kallus, X. Mao, and A. Zhou. Interval estimation of individual-level causal effects under unobserved confounding. In Proceedings of the $22^{n d}$ International Conference on Aritificial Intelligence and Statistics (AISTATS) 2019, volume 89, 2019.   \nT. McBrien, B. Winters, E. Zhou, and V. Eubanks. Screened & scored in the District of Columbia, November 2022. URL https://epic.org/screened-scored-in-dc/. [https://epic.org/ screened-scored-in-dc/; posted November-2022].   \nA. McClean, Z. Branson, and E. H. Kennedy. Calibrated sensitivity models. arXiv preprint arXiv:2405.08738, 2024.   \nH. Mozannar and D. Sontag. Consistent estimators for learning to defer to an expert. In International Conference on Machine Learning, pages 7076\u20137087. PMLR, 2020.   \nM. Oprescu, J. Dorn, M. Ghoummaid, A. Jesson, N. Kallus, and U. Shalit. B-learner: Quasi-oracle bounds on heterogeneous causal effects under hidden confounding. In Proceedings of the 40th International Conference on Machine Learning, pages 26599\u201326618. PMLR, 2023.   \nP. Rajpurkar, E. Chen, O. Banerjee, and E. J. Topol. AI in health and medicine. Nature medicine, 28(1):31\u201338, 2022.   \nD. B. Rubin. Causal inference using potential outcomes: Design, modeling, decisions. Journal of the American Statistical Association, 100(469):322\u2013331, 2005.   \nS. Shalev-Shwartz and S. Ben-David. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014.   \nM. J. Stensrud, J. Laurendeau, and A. L. Sarvet. Optimal regimes for algorithm-assisted human decision-making. Biometrika, page asae016, 2024.   \nM. T. Stevenson and J. L. Doleac. Algorithmic risk assessment in the hands of humans. Available at SSRN 3489440, 2022.   \nA. Swaminathan and T. Joachims. Counterfactual risk minimization: Learning from logged bandit feedback. In International Conference on Machine Learning, pages 814\u2013823. PMLR, 2015.   \nZ. Tan. A distributional approach for causal inference using propensity scores. Journal of the American Statistical Association, 101(476):1619\u20131637, 2006.   \nT. Yin, J.-F. Ton, R. Guo, Y. Yao, M. Liu, and Y. Liu. Fair classifiers that abstain without harm. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=jvveGAbkVx. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Action Costs ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In this section, we present examples that illustrate our choice of the costs. Then we prove the consistency of our costs for some base cases, that is, we show that the lowest cost corresponds to the right action for our choice for the set of costs. ", "page_idx": 12}, {"type": "text", "text": "Let $z\\;=\\;(x,a,y)$ be a sample, and assume $Y(0)\\;>\\;Y(1)$ w.l.o.g, that is, the right treatment is $A\\,=\\,1$ .   \n$\\hat{Y}^{+}(x,0),\\hat{Y}^{-}(x,0),\\hat{Y}^{+}(x,1),\\hat{Y}^{-}(x,1)$ are the CAPOs corresponding to this sample. ", "page_idx": 12}, {"type": "text", "text": "We recall our proposed costs: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{C(x,1)=\\hat{Y}^{+}(x,0)-\\hat{Y}^{-}(x,1)}\\\\ &{}&{C(x,0)=\\hat{Y}^{+}(x,1)-\\hat{Y}^{-}(x,0)}\\\\ &{}&{C_{\\perp}^{c o n s}(x,a,y)=\\left\\{\\hat{Y}^{-}(x_{i},0)-y_{i},\\quad\\mathrm{f}\\ q_{i}=1\\right.}\\\\ &{}&{\\left.\\!\\!\\!\\!C_{\\perp}^{\\mathrm{con}}(x,0,y)=\\left\\{\\hat{Y}^{-}(x_{i},1)-y_{i},\\quad\\mathrm{otherwise}.\\right.}\\\\ &{}&{\\left.\\!\\!\\!\\!C_{\\perp}^{o p t}(x,a,y)=\\left\\{\\hat{Y}^{+}(x,0)-y,\\quad\\mathrm{if}\\ a=1\\right.}\\\\ &{}&{\\!\\!\\!\\!\\!C_{\\perp}^{+}(x,1)-y,\\quad\\mathrm{otherwise}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $C_{\\perp}^{c o n s}(x,a,y)$ , and $C_{\\perp}^{o p t}(x,a,y)$ correspond to the conservative and optimistic deferral costs respectively. ", "page_idx": 12}, {"type": "text", "text": "A.1 No overlap between CAPOs intervals ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Example In this example, we show a case where the CAPOs intervals don\u2019t overlap, we provide a visual diagram for this example in Figure 3a, where we let ${\\hat{Y}}^{+}(x,0)=4,{\\hat{Y}}^{-}(x,0)=1,{\\hat{Y}}^{+}({\\bar{x}},1)=9,{\\hat{Y}}^{-}(x,1)=5$ , and $Y(0)=2,Y{\\bar{(}}1)=6$ . Then we have that: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{l}{C(x,1)=-1}\\\\ {C(x,0)=8}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "As for the deferral cost, there are two possible cases: if the expert is right, meaning $a=1,y=Y(1)$ , the deferral costs are: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{C_{\\bot}^{c o n s}(x,a,y)=-5}\\\\ {C_{\\bot}^{o p t}(x,a,y)=-2}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "for both alternatives, we have the lowest costs corresponding to the deferral cost, which means that we will guide the model to choose the expert\u2019s decision, which is in this case, the right treatment. ", "page_idx": 12}, {"type": "text", "text": "On the other hand, when the expert is wrong, i.e. $a=0,y=Y(0)$ , the deferral costs are: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{C_{\\bot}^{c o n s}(x,a,y)=3}\\\\ {C_{\\bot}^{o p t}(x,a,y)=7}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Where for both alternatives, the lowest cost is the cost of the treatment $a=1$ , which is the right treatment. ", "page_idx": 12}, {"type": "text", "text": "Proof of the general case We show a visualization for the general case where the CAPOs intervals don\u2019t overlap in Figure 3b. In the general case we have that: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{l}{C(x,1)=-d}\\\\ {C(x,0)=\\ell_{0}+d+\\ell_{1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "As for the deferral cost, when expert is right, meaning $a=1,y=Y(1)$ , the deferral costs are: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{C_{\\bot}^{c o n s}(x,a,y)=-\\left(\\ell_{0}+d+\\ell_{1}^{-}\\right)}}\\\\ {{C_{\\bot}^{o p t}(x,a,y)=-\\left(d+\\ell_{1}^{-}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "for both alternatives, we have the lowest costs corresponding to the deferral cost, which means that we will guide the model to choose the expert\u2019s decision, which is in this case, the right treatment. ", "page_idx": 12}, {"type": "text", "text": "On the other hand, when the expert is wrong, i.e. $a=0,y=Y(0)$ , the deferral costs are: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{C_{\\bot}^{c o n s}(x,a,y)=\\ell_{0}^{+}+d}}\\\\ {{C_{\\bot}^{o p t}(x,a,y)=\\ell_{1}+d+\\ell_{0}^{+}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Where for both alternatives, the lowest cost is the cost of the treatment $a=1$ , which is the right treatment. ", "page_idx": 13}, {"type": "image", "img_path": "taI8M5DiXj/tmp/55b91dd8ff5bba32276f0ced3e5fa6bddf9b8acd4c8c4ca94cdd30b26dfe2f9a.jpg", "img_caption": ["(b) The general case "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "Figure 3: No overlap between CAPOs intervals visualization ", "page_idx": 13}, {"type": "text", "text": "A.2 CAPOs intervals overlap ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We now focus on the cases where the CAPOs intervals overlap. In Figure 4 we present two examples for cases where the CAPOs intervals overlap and show how this affects the costs. ", "page_idx": 13}, {"type": "text", "text": "Example 1. In the example presented in Figure 4a, where we let $\\hat{Y}^{+}(x,0)=5,\\hat{Y}^{-}(x,0)=1,\\hat{Y}^{+}(x,1)=$ $9,{\\hat{Y}}^{-}(x,1)=4$ , and $Y(0)=2,Y(1)=6$ . Then we have that: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{C(x,1)=1}\\\\ {C(x,0)=8}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "As for the deferral cost, there are two possible cases: if the expert is right, meaning $a=1,y=Y(1)$ , the deferral costs are: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{C_{\\bot}^{c o n s}(x,a,y)=-5}\\\\ {C_{\\bot}^{o p t}(x,a,y)=-1}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "for both alternatives, we have the lowest costs corresponding to the deferral cost, which means that we will guide the model to choose the expert\u2019s decision, which is in this case, the right treatment. ", "page_idx": 13}, {"type": "text", "text": "On the other hand, when the expert is wrong, i.e. $a_{i}=0,y_{i}=Y(0)$ , the deferral costs are: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{C_{\\bot}^{c o n s}(x,a,y)=2}\\\\ {C_{\\bot}^{o p t}(x,a,y)=7}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Where for both alternatives, the lowest cost is the cost of the treatment $a=1$ , which is the right treatment. ", "page_idx": 13}, {"type": "text", "text": "Example 2. In the example presented in Figure 4b, we show an interesting case, where although $Y(1)>$ $Y(0)$ , the value of $Y(1)$ lies into the intersection of the two intervals. We let $\\hat{Y}^{+}(x,0)\\,=\\,5,\\hat{Y}^{-}(x,0)\\,=$ $1,\\hat{Y}^{+}(x,1)=9,\\hat{Y}^{-}(x,1)=4$ , and $Y(0)=2,Y(1)=4.5$ . Then we have that: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{C(x,1)=1}\\\\ {C(x,0)=8}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "As for the deferral cost, there are two possible cases: if the expert is right, meaning $a=1,y=Y(1)$ , the deferral costs are: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{C_{\\bot}^{c o n s}(x,a,y)=-3.5}}\\\\ {{C_{\\bot}^{o p t}(x,a,y)=0.5}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "for both alternatives, we have the lowest costs corresponding to the deferral cost, which means that we will guide the model to choose the expert\u2019s decision, which is in this case, the right treatment. ", "page_idx": 14}, {"type": "text", "text": "On the other hand, when the expert is wrong, i.e. $a_{i}=0,y_{i}=Y(0)$ , the deferral costs are: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{C_{\\bot}^{c o n s}(x,a,y)=2}\\\\ {C_{\\bot}^{o p t}(x,a,y)=7}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Where for both alternatives, the lowest cost is the cost of the treatment $a=1$ , which is the right treatment. ", "page_idx": 14}, {"type": "image", "img_path": "taI8M5DiXj/tmp/de5f954ba0c8d364c7d268152dbf5debbb0522801a0555a0ca4773e5a089af2d.jpg", "img_caption": ["Figure 4: CAPOs intervals overlap visualization "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "A.3 A Comparative Analysis of the Conservative and Optimistic Costs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We provide an analysis that highlights the scenarios where each of the conservative and optimistic approaches proves superior to the other. ", "page_idx": 14}, {"type": "text", "text": "For a sample $(x,a,y,Y(0),Y(1))\\sim P_{\\mathrm{full}}$ , where w.l.o.g it holds that $Y(1)>Y(0)$ , that is, the right treatment for this sample is 1. ", "page_idx": 14}, {"type": "text", "text": "We distinguish between two main cases: ", "page_idx": 14}, {"type": "text", "text": "The expert is right: ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "When the expert is right, i.e. $\\pi_{\\mathrm{exp}}(x)=1$ , and $Y\\left(\\pi_{\\exp}(x_{i})\\right)=Y(1)$ , then,a policy with this set of costs will predict the wrong treatment ${\\tilde{\\pi}}(x)=0$ when the following condition holds: ", "page_idx": 14}, {"type": "equation", "text": "$$\nC(x,0)<\\operatorname*{min}\\{C(x,1),C_{\\bot}(x,a=1,y=Y(1))\\}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "For the conservative approach, this holds when: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\hat{Y}^{+}(x,1)-\\hat{Y}^{-}(x,0)<\\hat{Y}^{-}(x,0)-Y(1)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "As for the the optimistic approach, this holds when: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\hat{Y}^{+}(x,1)-\\hat{Y}^{-}(x,0)<\\hat{Y}^{+}(x,0)-Y(1)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "When Equation (7) holds and Equation (8) does not hold, these are the cases where the optimistic approach outperforms the conservative approach, i.e.: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\hat{Y}^{+}(x,0)+\\hat{Y}^{-}(x,0)<\\hat{Y}^{+}(x,0)+Y(1)<2\\hat{Y}^{-}(x,0)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We note that Equation (14) never holds, and thus when the expert is right, the optimistic approach cannot outperform its conservative counterpart. ", "page_idx": 14}, {"type": "text", "text": "On the other hand, when Equation (8) holds and Equation (7) does not hold, these are the cases where the conservative approach outperforms the optimistic approach, i.e.: ", "page_idx": 14}, {"type": "equation", "text": "$$\n2\\hat{Y}^{-}(x,0)<\\hat{Y}^{+}(x,1)+Y(1)<\\hat{Y}^{-}(x,0)+\\hat{Y}^{+}(x,0)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The expert is wrong ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "When the expert is wrong, i.e. $\\pi_{\\mathrm{exp}}(x)=0$ , and $Y\\left(\\pi_{\\mathrm{exp}}(x_{i})\\right)=Y(0)$ , then, a policy with this set of costs will predict the wrong treatment ${\\tilde{\\pi}}(x)=0$ when the following condition holds: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\{C(x,0),C_{\\bot}(x,a=0,y=Y(0))\\}<C(x,1)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "For the conservative approach, this holds when: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\hat{Y}^{-}(x,1)-Y(0)<\\hat{Y}^{+}(x,0)-\\hat{Y}^{-}(x,1)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "As for the the optimistic approach, this holds when: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\hat{Y}^{+}(x,1)-Y(0)<\\hat{Y}^{+}(x,0)-\\hat{Y}^{-}(x,1)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "When Equation (12) holds and Equation (13) does not hold, these are the cases where the optimistic approach outperforms the conservative approach, i.e.: ", "page_idx": 15}, {"type": "equation", "text": "$$\n2\\hat{Y}^{-}(x,1)<\\hat{Y}^{+}(x,0)+Y(0)<\\hat{Y}^{+}(x,1)+\\hat{Y}^{-}(x,1)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "On the other hand, when Equation (13) holds and Equation (12) does not hold, these are the cases where the conservative approach outperforms the optimistic approach, i.e.: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\hat{Y}^{-}(x,1)+\\hat{Y}^{+}(x,1)<\\hat{Y}^{+}(x,0)+Y(0)<2\\hat{Y}^{-}(x,1)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We note that Equation (14) never holds, and thus when the expert is wrong, the conservative approach cannot outperform its optimistic counterpart. ", "page_idx": 15}, {"type": "text", "text": "B Proof of Main Theorems ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we provide the proof of our main theorems. ", "page_idx": 15}, {"type": "text", "text": "B.0.1 Consistency ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof of Corollary 1. we can apply Proposition 1 from Mozannar and Sontag [2020] directly to our setting yielding the statement of our Corollary. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Note that we used a slightly different statement of Proposition 1 from Mozannar and Sontag [2020] from readability considerations and consistency with our setup and problem formulation. ", "page_idx": 15}, {"type": "text", "text": "The formulation of the original statement using our notation is as follows: ", "page_idx": 15}, {"type": "text", "text": "Corollary 2. $L_{C E}$ is convex in $\\pi$ and is a consistent loss function for $L$ : Le $\\begin{array}{r}{t\\;\\tilde{\\pi}=\\arg\\operatorname*{inf}_{\\pi}\\mathbb{E}[L_{C E}(\\pi,z;\\hat{Q})],}\\end{array}$ , then arg $\\begin{array}{r}{\\operatorname*{max}_{i\\in\\{0,1,\\perp\\}}\\tilde{\\pi_{i}}=\\arg\\operatorname*{min}_{i\\in\\{0,1,\\perp\\}}\\mathbb{E}[c(i)|Z=z]}\\end{array}$ . ", "page_idx": 15}, {"type": "text", "text": "with $c(0)=C(x,0)$ , $c(1)=C(x,1)$ , and $c(\\bot)=C_{\\bot}(x,a,y)$ defined in Section 5. ", "page_idx": 15}, {"type": "text", "text": "We recall that the surrogate loss $L_{C E}[4]$ is defined for policies $\\pi_{i}:\\mathcal{X}\\rightarrow\\mathbb{R}$ be the raw output of the policy $\\pi$ corresponding to a class $i\\in\\{0,1,\\bot\\}$ , such that $\\pi(x)\\,=\\,\\arg\\operatorname*{max}_{i\\in\\{0,1,\\perp\\}}\\,\\pi_{i}(x)$ . Therefore, this holds especially for $\\tilde{\\pi}=\\arg\\operatorname*{inf}_{\\pi}\\mathbb{E}[L_{C E}(\\pi,z;\\hat{Q})].$ , i.e. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\tilde{\\pi}=\\arg\\operatorname*{max}_{i\\in\\{0,1,\\perp\\}}\\tilde{\\pi}_{i}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "As for optimizing the original loss function $L(\\pi)$ [3], the optimization problem is given by, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\dot{\\pi}=\\arg\\operatorname*{inf}_{\\pi^{\\prime}}\\mathbb{E}[L(\\pi^{\\prime})]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "according to Mozannar and Sontag [2020], this problem can be solved as a cost-sensitive problem with the costs $c(0)=\\bar{C}(x,0)$ , $c(1)=C(x,1)$ , and $c(\\bot)=C_{\\bot}(x,a,y)$ for each sample $(x,a,y)$ , i.e., the solution of this optimal optimization problem satisfies the following for each sample $z;\\,\\dot{\\pi}=\\arg\\operatorname*{min}_{i\\in\\{0,1,\\perp\\}}\\mathbb{E}[c(i)|Z=z]$ Thus, our adaptation of the results is equivalent to the original results from Mozannar and Sontag [2020]. ", "page_idx": 15}, {"type": "text", "text": "B.1 Costs Are Coherent ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we prove the coherency of our costs as defined in Section 6.2, and show the improvement of the CARED policy over policies that depend solely on either a human expert or a machine. ", "page_idx": 15}, {"type": "text", "text": "Proof of Theorem $^{\\,l}$ . For a sample $(x,a,y,Y(0),Y(1))\\sim P_{\\mathrm{full}}$ , where w.l.o.g it holds that $Y(1)>Y(0)$ , that is, the right treatment for this sample is 1. We assume Definition 1 holds with probability $1-\\delta$ . Then we prove this theorem in two steps: Comparison against the human expert policy $(\\pi_{e x p})$ , and comparison against the bounds policy $(\\pi_{b o u n d s})$ . ", "page_idx": 15}, {"type": "text", "text": "Comparison Against the Expert: ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We distinguish between two main cases: ", "page_idx": 15}, {"type": "text", "text": "The expert is right: ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "When the expert is right, i.e. $\\pi_{\\mathrm{exp}}(x)=1$ , and $Y\\left(\\pi_{\\exp}(x_{i})\\right)=Y(1)$ , then our policy will predict the wrong treatment $\\bar{\\pi}(\\bar{x})=0$ when the following condition holds: ", "page_idx": 15}, {"type": "equation", "text": "$$\nC(x,0)<\\operatorname*{min}\\{C(x,1),C_{\\bot}(x,a=1,y=Y(1))\\}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "That is: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\hat{Y}^{+}(x,1)-\\hat{Y}^{-}(x,0)<\\operatorname*{min}\\{\\hat{Y}^{+}(x,0)-\\hat{Y}^{-}(x,1),\\hat{Y}^{-}(x,0)-Y(1)\\}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In simple words, we recall that our method assigns the treatment with the minimal cost among all other treatments. Thus, when the cost of the wrong treatment is the minimal cost among the other costs, our method will make mistakes. ", "page_idx": 16}, {"type": "text", "text": "We note that it holds that: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{Y}^{-}(x,0)-Y(1)\\underset{\\hat{Y}^{-}(x,1)\\leq Y(1)}{\\leq}\\hat{Y}^{-}(x,0)-\\hat{Y}^{-}(x,1)\\underset{\\hat{Y}^{+}(x,0)\\geq\\hat{Y}^{-}(x,0)}{\\leq}\\hat{Y}^{+}(x,0)-\\hat{Y}^{-}(x,1)}\\\\ &{\\Rightarrow\\operatorname*{min}\\{\\hat{Y}^{+}(x,0)-\\hat{Y}^{-}(x,1),\\hat{Y}^{-}(x,0)-Y(1)\\}=\\hat{Y}^{-}(x,0)-Y(1)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus, $\\tilde{\\pi}(x)$ is wrong when: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\hat{Y}^{+}(x,1)-\\hat{Y}^{-}(x,0)<\\hat{Y}^{-}(x,0)-Y(1)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We note that the condition in 20 never holds when CAPO bounds are valid, and therefore, with probability $1-\\delta$ , it holds that $\\tilde{\\pi}(x)=1$ , and $Y(\\tilde{\\pi}=(x))=Y(1)$ , That is, $Y(\\tilde{\\pi}(x))=Y\\left(\\pi_{\\mathrm{exp}}(x)\\right)$ ", "page_idx": 16}, {"type": "text", "text": "The expert is wrong: ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "When the expert is wrong, i.e. $\\pi_{\\mathrm{exp}}(x)=0$ , and $Y\\left(\\pi_{\\mathrm{exp}}(x)\\right)=Y(0)$ , then, from the same considerations above, our policy is wrong when: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\{C(x,0),C_{\\bot}(x,a=0,y=Y(0))\\}<C(x,1)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "That is: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\{\\hat{Y}^{+}(x,1)-\\hat{Y}^{-}(x,0),\\hat{Y}^{-}(x,1)-Y(0)\\}<\\hat{Y}^{+}(x,0)-\\hat{Y}^{-}(x,1)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We note that it holds that: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{Y}^{-}(x,1)-Y(0)\\underset{\\hat{Y}^{-}(x,0)\\leq Y(0)}{\\leq}\\hat{Y}^{-}(x,1)-\\hat{Y}^{-}(x,0)\\underset{\\hat{Y}^{+}(x,1)\\geq\\hat{Y}^{-}(x,1)}{\\leq}\\hat{Y}^{+}(x,1)-\\hat{Y}^{-}(x,0)}\\\\ &{\\Rightarrow\\operatorname*{min}\\{\\hat{Y}^{+}(x,1)-\\hat{Y}^{-}(x,0),\\hat{Y}^{-}(x,1)-Y(0)\\}=\\hat{Y}^{-}(x,1)-Y(0)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus, $\\tilde{\\pi}(x)$ is wrong when: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\hat{Y}^{-}(x,1)-Y(0)<\\hat{Y}^{+}(x,0)-\\hat{Y}^{-}(x,1)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "When this happens, we have that $\\tilde{\\pi}(x)=0$ , and $Y(\\tilde{\\pi}(x))=Y(0)$ , that is, $Y(\\tilde{\\pi}(x_{i}))=Y\\left(\\pi_{\\mathrm{exp}}(x_{i})\\right)$ , otherwise, when this condition does not hold, we have that $Y(\\tilde{\\pi}(x))>Y\\left(\\pi_{\\mathrm{exp}(x)}\\right)$ ", "page_idx": 16}, {"type": "text", "text": "Comparison Against the Bounds Policy ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We analyze the cases where each policy makes mistakes. We note that $\\tilde{\\pi}$ is always right when CAPO intervals don\u2019t overlap as proved in Appendix A. This holds for the $\\pi_{b o u n d s}$ policy as well, as this implies directly from its definition. On the other hand, when the CAPO bounds overlap, both policies can make mistakes ", "page_idx": 16}, {"type": "text", "text": "Thus, we distinguish between two cases: ", "page_idx": 16}, {"type": "text", "text": "CAPO bounds don\u2019t overlap In this case, both policies recommend the right action, thus it holds that, $Y(\\tilde{\\pi}(x))=Y(\\pi_{b o u n d s}(x))=Y(1)$ . ", "page_idx": 16}, {"type": "text", "text": "CAPO Bounds don\u2019t Overlap In this case, \u03c0bounds defer the decision to the expert. As for $\\tilde{\\pi}$ , as shown above, the policy $\\tilde{\\pi}$ is wrong when the expert is wrong and the condition 23 holds. Therefore, if condition 23 holds, and the expert is wrong, then both policies will go wrong, and thus we have that $Y(\\tilde{\\pi}(x))=Y(\\pi_{b o u n d s}(x))=Y(0)$ . ", "page_idx": 16}, {"type": "text", "text": "On the other hand, if CAPO intervals overlap, but Condition 23 doesn\u2019t hold, then $\\tilde{\\pi}$ give the right recommendation with $Y(\\tilde{\\pi}(x))=Y(1)$ , but $\\pi_{b o u n d s}$ still give the wrong recommendation with $Y(\\pi_{b o u n d s}(x))=Y(0)$ , that is $Y(\\tilde{\\pi}(x))>Y(\\pi_{b o u n d s}(x))$ . ", "page_idx": 16}, {"type": "text", "text": "B.2 Generalization Bound ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We now prove Theorem 2. ", "page_idx": 17}, {"type": "text", "text": "Corollary 3 (Bounded log-Softmax). Let $\\pi\\,\\in\\,\\Pi$ be a learner that satisfies Assumption 3. Then, the term k\u2208{0,1,\u22a5} jexp(\u03c0k(x)) is bounded. ", "page_idx": 17}, {"type": "text", "text": "Proof of Corollary 3. $\\pi$ is a policy satisfying Assumption 3, that is for each $j\\;\\in\\;\\{0,1,\\bot\\}$ we have that $|\\pi_{j}(\\cdot)|\\leq C_{\\pi}$ , where $K$ is the number of classes. That is, it holds that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{c}{-C_{\\pi}\\leq_{\\pi_{j}}(.)\\leq C_{\\pi}}\\\\ {\\Leftrightarrow\\exp(-C_{\\pi})\\leq\\exp(\\pi_{j})\\leq\\exp(C_{\\pi})}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "As a result, we have, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{\\exp(\\pi_{j}(x))}{\\sum_{k\\in\\{0,1,\\perp\\}}\\exp(\\pi_{k}(x))}\\le\\frac{\\exp(C_{\\pi})}{\\sum_{k\\in\\{0,1,\\perp\\}}\\exp(-C_{\\pi})}}}\\\\ &{\\le\\frac{\\exp(C_{\\pi})}{3\\cdot\\exp(-C_{\\pi})}}\\\\ &{\\le\\frac13\\cdot\\exp(2\\cdot C_{\\pi})}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Similarly, we can show that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{\\exp(\\pi_{j}(x))}{\\sum_{k\\in\\{0,1,\\perp\\}}\\exp(\\pi_{k}(x))}\\geq\\frac{1}{3}\\cdot\\exp(-2\\cdot C_{\\pi})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Lemma 1 (Bounded loss function). Let $\\Pi$ be a class of policy learners satisfying Assumption 3 with a constant $C_{\\pi}$ , and suppose Assumption 4 holds with a constant $C_{Y}$ . Then $\\left|L_{C E}(\\pi,z;\\hat{Q})\\right|\\leq8\\cdot C_{\\pi}\\cdot C_{Y}$ . ", "page_idx": 17}, {"type": "text", "text": "Proof of Lemma $^{\\,l}$ . Let $\\Pi$ be a class of policy learners satisfying Assumption 3, then the loss function $L_{C E}(\\pi,z_{i};\\hat{Q}(x_{i}))$ is bounded: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{C K}(\\pi,z;Q)\\lVert=\\left|\\sum_{|\\ell(0,1,\\perp)}-w^{j}(z,Q(z))\\log\\left(\\frac{\\exp(\\pi_{\\ell}(x))}{\\sum_{k\\in\\{0,1,\\perp\\}}\\exp(\\pi_{k\\}(x))}\\right)\\right|}\\\\ &{=\\left|\\sum_{|\\ell(0,1,\\perp)|}w^{j}(z,Q(x))\\log\\left(\\frac{\\exp(\\pi_{\\ell}(x))}{\\sum_{k\\in\\{0,1,\\perp\\}}\\exp(\\pi_{k\\}(x))}\\right)\\right|}\\\\ &{\\stackrel{\\le}\\left|\\sum_{|\\ell(0,1,\\perp)|}w^{j}(z,Q(x))\\log\\left(\\frac{1}{\\sum_{k\\in\\{0,1,\\perp\\}}\\exp(2\\pi_{k\\}(x))}\\right)\\right|}\\\\ &{\\stackrel{\\le}\\left|\\sum_{|\\ell(0,1,\\perp)|}w^{j}(z,Q(x))\\log\\left(\\frac{1}{2}\\cdot\\infty_{\\ell}\\right)\\right|}\\\\ &{\\stackrel{\\le}\\left|\\sum_{|\\ell(0,1,\\perp)|}w^{j}(z,Q(x))\\cdot2\\cdot C_{\\pi}\\right|}\\\\ &{\\leq2\\cdot C_{\\pi}\\cdot\\sum_{|\\ell(0,1,\\perp)|}|w^{j}(z,Q(x))|}\\\\ &{\\stackrel{\\le}\\alpha^{2}\\cdot C_{\\pi}\\cdot|\\!|\\cdot C_{\\Gamma}|=8\\cdot C_{\\pi}\\cdot C_{\\pi}\\cdot C_{\\Gamma}}\\\\ &{\\stackrel{\\le}\\alpha^{2}\\cdot C_{\\pi}\\cdot|\\!|C_{\\pi}|=8\\cdot C_{\\pi}\\cdot C_{\\pi}\\cdot C_{\\Gamma}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Where step $(a)$ follows from Corollary 3, and $(b)$ follows from the boundedness of $Y$ in Assumption 4. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Lemma 2 (L2 Consistency of the $\\operatorname*{max}(\\cdot,\\cdot)$ Estimator). Let For estimators $\\hat{A}_{n},\\hat{B}_{n}$ of $A,B$ based on n samples respectively, where there exists a function $f(n)$ such that: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\hat{A}_{n}-A\\|\\lesssim O_{p}\\left(f(n)\\right)}\\\\ {\\|\\hat{B}_{n}-B\\|\\lesssim O_{p}\\left(f(n)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\operatorname*{max}(\\hat{A}_{n},\\hat{B}_{n})-\\operatorname*{max}(A,B)\\|\\lesssim O_{p}\\left(f(n)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. We prove this Lemma in two steps: first we show convergence in probability of $\\operatorname*{max}(\\hat{A}_{n},\\hat{B}_{n})$ to $\\operatorname*{max}(A,B)$ . Then we show the convergence rate. ", "page_idx": 18}, {"type": "text", "text": "Convergence $\\hat{A}_{n}$ and $\\hat{B}_{n}$ converge in probability to $A$ and $B$ respectively as $n$ tends to infinity, that is: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{A}_{n}\\xrightarrow[n\\rightarrow\\infty]{P}A}\\\\ {\\hat{B}_{n}\\xrightarrow[n\\rightarrow\\infty]{P}B}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{max}(\\hat{A}_{n},\\hat{B}_{n})=\\frac{1}{2}\\left(\\hat{A}_{n}+\\hat{B}_{n}+\\left|\\hat{A}_{n}-\\hat{B}_{n}\\right|\\right)\\xrightarrow[n\\to\\infty]{P}\\frac{1}{2}\\left(A+B+|A-B|\\right)=\\operatorname*{max}(A,B)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Convergence Rate The convergence rate of $\\operatorname*{max}(\\hat{A}_{n},\\hat{B}_{n})$ is determined by the slower convergence rate of $\\hat{A}_{n}$ and $\\hat{B}_{n}$ , which is $\\operatorname*{max}(f(n),f(n))=f(n)$ \u53e3 ", "page_idx": 18}, {"type": "text", "text": "Corollary 4 (Bounded weights). The weights $w^{j}(z,\\hat{Q}(x))\\,f o r\\,j\\in\\{0,1,\\perp\\}$ of the weighted surrogate loss function 4 based on the costs we define in Section 5.2 satisfy: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(i)\\;\\big|w^{\\prime}(z,Q(x)\\big|\\leq4\\cdot C_{Y}}\\\\ &{(i i)\\;\\|w^{j}(z,Q(x))-w^{j}(z,\\hat{Q}(x))\\|\\lesssim4\\cdot O_{p}\\left(n^{-1/(2+r)}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. We recall that for $j\\;\\;\\in\\;\\;\\{0,1,\\bot\\}$ , a cost $c(j)$ is of the form $c(j)~=~A\\,-\\,B$ , where $A,B\\;\\;\\in$ $\\{\\hat{Y}^{+}(x,0),\\hat{Y}^{-}(x,0),\\hat{Y}^{+}(x,1)),\\hat{Y}^{-}(x,1),y_{i}\\}$ . ", "page_idx": 18}, {"type": "text", "text": "As for $\\operatorname*{max}_{k\\in\\{0,1,\\bot\\}}c(k)$ is also of the form $\\begin{array}{r l r}{\\operatorname*{max}_{k\\in[K+1]}c(k)}&{{}=}&{C\\;-\\;D}\\end{array}$ , where $C,D\\quad\\in$ $\\{\\hat{Y}^{+}(x,0),\\hat{Y}^{-}(x,0),\\hat{Y}^{+}(x,1)),\\hat{Y}^{-}(x,1),y_{i}\\}$ . Then, (i) $\\left|w^{j}(z,Q(x))\\right|=\\left|(A-B)-(C-D)\\right|\\leq|A|+|B|+|C|+|D|\\leq4\\cdot C_{Y}.$ ", "page_idx": 18}, {"type": "text", "text": "where $(a)$ follows from the boundedness of the outcomes (Assumption 4 with a constant $C_{Y}$ ). ", "page_idx": 18}, {"type": "text", "text": "(ii) For $A,B,C,D\\ \\in\\ \\{Y^{+}(x,0),Y^{-}(x,0),Y^{+}(x,1)),Y^{-}(x,1),y_{i}\\}$ , we denote $\\hat{A},\\hat{B},\\hat{C},\\hat{D}$ to be their estimators, respectively. Then, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|w^{j}(z,Q(x))-w^{j}(z,\\hat{Q}(x))\\|=\\|(A-\\hat{A})-(B-\\hat{B})-(C-\\hat{C})+(D-\\hat{D})\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\|A-\\hat{A}\\|+\\|B-\\hat{B}\\|+\\|C-\\hat{C}\\|+\\|D-\\hat{D}\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\lesssim4\\cdot O_{p}\\left(n^{-1/(2+r)}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $(a)$ follows from Assumption 5, and Lemma 2. ", "page_idx": 18}, {"type": "text", "text": "Proof of Theorem 2. Let $\\pi\\,\\in\\,\\Pi$ be a policy where the policy class $\\Pi$ satisfies Assumption 2, for a policy $\\pi\\in\\Pi$ , and $Q,{\\hat{Q}}$ the CAPOs bounds and the estimated CAPOs bounds respectively. Given our loss function $L_{C E}(\\pi,z;Q)$ which satisfies Assumption 3, and Lemma 1. We are interested in bounding the term ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{L_{D}(\\pi;Q)-L_{S}(\\pi;\\hat{Q})=\\underbrace{\\left(L_{D}(\\pi;Q)-L_{D}(\\pi;\\hat{Q})\\right)}_{A}+\\underbrace{\\left(L_{D}(\\pi;\\hat{Q})-L_{S}(\\pi;\\hat{Q})\\right)}_{B}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We get the upper bound for Term $B$ by applying Theorem 26.5 from Shalev-Shwartz and Ben-David [2014], that is, with probability at least $1-\\delta$ we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\nL_{D}(\\pi;\\hat{Q})-L_{S}(\\pi;\\hat{Q})\\leq2\\mathcal{R}_{n}(\\Pi)+24\\cdot C_{\\pi}\\cdot C_{Y}\\sqrt{\\frac{2\\ln(4/\\delta)}{n}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where ${\\mathcal{R}}_{n}(\\Pi)$ is the Rademacher Complexity of the policy class $\\Pi$ we have from Assumption 2, $C_{\\pi}$ , and $C_{Y}$ are the constants we have from Assumption 3, and Assumption 4 respectively. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{L_{2}(\\tau,Q)-L_{2}(\\tau,Q)}&{=\\mathbb{E}_{\\tau\\sim i(a,\\tau)\\sim0}/(\\sum_{l=1}^{\\infty}{\\tau\\Big(Q(l,Q)\\Big)}-\\mathbb{E}_{\\tau\\sim i(a,Q)\\sim0}/(\\sum_{l=1}^{\\infty}{\\tau\\Big(Q(l,Q)\\Big)})}\\\\ &{=\\mathbb{E}_{\\tau\\sim i(a,Q)\\sim0}\\left[\\sum_{l=1}^{\\infty}{\\tau\\Big(Q(l,Q)\\Big)}-N\\log\\Big(\\sum_{l=1}^{\\infty}{\\tau\\Big(Q(l,Q)\\Big)}-N\\log(\\tau(Q)\\Big)\\Big)\\right]}\\\\ &{=-\\mathbb{E}_{\\tau\\sim i(a,Q)\\sim0}/\\left(\\sum_{l=1}^{\\infty}{\\tau\\Big(Q(l,Q)\\Big)}-N\\log\\Big(2\\prod_{l=1}^{\\infty}{\\tau\\Big(Q)}-N\\log(\\tau(Q)\\Big)\\Big)\\right]}\\\\ &{=\\mathbb{E}_{\\tau\\sim i(a,Q)\\sim0}/\\left[\\sum_{l=1}^{\\infty}{\\tau\\Big(Q^{\\prime}(l,Q)\\Big)}-N\\log\\Big(2\\prod_{l=1}^{\\infty}{\\tau\\Big(Q(l,Q)\\Big)}-N\\log(\\tau(Q)\\Big)\\Big)\\right]}\\\\ &{-\\mathbb{E}_{\\tau\\sim i(a,Q)\\sim0}/\\left[\\sum_{l=1}^{\\infty}{\\tau\\Big(Q^{\\prime}(l,Q)\\Big)}-N^{\\prime}\\log(\\tau(Q)\\Big)-N^{\\prime}\\log(\\tau(Q)\\Big)\\right]}\\\\ &{\\stackrel{(a,b)}{=}\\mathbb{E}_{\\tau\\sim i(a,Q)\\sim0}/\\left[\\sum_{l=1}^{\\infty}{\\tau\\Big(Q^{\\prime}(l,Q)\\Big)}-N^{\\prime}\\log(\\tau(Q))-N^{\\prime}\\log(\\tau(Q)\\Big)\\Big]}\\\\ &{\\stackrel{(c)}{=}\\mathbb{E}^{\\tau\\sim\\alpha_{k}\\sim\\alpha_{k}\\sim\\alpha_{k}}/\\left[\\sum_{l=1}^{\\infty}{\\tau\\Big(Q^{\\prime}(l,Q)\\Big)}-N^{\\prime}\\log\\Big(\\tau(Q)-N^{\\prime}(Q)\\Big)\\Big]\\;,}\\\\ &{\\stackrel{(c)}{=}2^{{C_{2}}-\\tau_{k}\\sum_{l=1}^{\\infty}{\\tau\\Big(Q^{\\prime}(l,Q)\\Big)}}\\left[\\sum_{l=1}^{\\infty}{\\tau\\Big(Q^\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where: ", "page_idx": 19}, {"type": "text", "text": "$\\begin{array}{r l}&{(a):C o r o l l a r y\\ 3.}\\\\ &{(b):\\mathrm{Linearity\\of\\Expectation}.}\\\\ &{(c):\\mathbb{E}[X]\\leq|\\mathbb{E}[X]|.}\\\\ &{(d):|\\mathbb{E}[X]|=\\sqrt{\\mathbb{E}[X]^{2}}.}\\\\ &{(e):\\mathrm{Jensen}^{\\gamma_{\\delta}}\\mathrm{Inequality}\\ \\mathbb{E}[X]^{2}\\leq\\mathbb{E}[X^{2}]}\\\\ &{(f):A s s u m p t i o n\\ 5.}\\\\ &{(g):C o r o l l a r y\\ 4,\\mathrm{and}\\ \\mathrm{Deffinition\\of}\\ O_{p}(\\cdot.}\\end{array}$ . with constant $C>0$ . ", "page_idx": 19}, {"type": "text", "text": "Putting it all together, we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{D}(\\pi;Q)-L_{S}(\\pi;\\hat{Q})=\\Big(L_{D}(\\pi;Q)-L_{D}(\\pi;\\hat{Q})\\Big)+\\Big(L_{D}(\\pi;\\hat{Q})-L_{S}(\\pi;\\hat{Q})\\Big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq2\\mathcal{R}_{n}(\\Pi)+24\\cdot C_{\\pi}\\cdot\\Bigg(C_{Y}\\sqrt{\\frac{2\\ln(4/\\delta)}{n}}+C\\cdot\\Big(n^{-1/(2+r)}\\Big)\\Bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "C Additional Experimental Detail ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The experiments in this paper were conducted using a PowerEdge R750XA Server with 2 CPUs and 4 NVIDIA A40 GPUs. Here we provide all the details required to replicate the paper results. In addition, we provide replication code at https://github.com/marahgh/CARED. ", "page_idx": 20}, {"type": "text", "text": "C.1 Synthetic Data ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this experiment, We used the synthetic dataset from Gao and Yin [2023]The CAPO bounds were estimated using XGBRegressor from xgboost as the base learners for the B-Learner [Oprescu et al., 2023] estimator, and a LogisticRegression from scikit-learn as the propensity score estimator. We show in Table 1 the hyper-parameter choices for each model. As for the policy model, we use a logistic regression model implemented as a one-layer MLP using several functions from pytorch, The policy model was implemented using pytorch_lightning model as a wrapper model that can receive any policy model as the base model for learning the policy. Moreover, the hyper-parameters we use for the policy model are as follows: learning_rate $=0.001$ , optimizer $=A d a m$ , patience ${\\it\\Delta\\phi}=3{\\it\\Delta\\Psi}$ , and max_epochs $=100$ . ", "page_idx": 20}, {"type": "text", "text": "We replicate the experiment from Gao and Yin [2023] where they generate 10 instances of the synthetic dataset according to the Data Generation Process they mention in their paper, where lower outcomes are assumed to be better in this dataset. For each instance, they vary the level of allowed hidden confounding and compare the methods by their regret related to the baseline no-treat policy, i.e. $\\pi_{0}(x)=0$ . ", "page_idx": 20}, {"type": "table", "img_path": "taI8M5DiXj/tmp/63cab537036d1a98c2be1ffb4732b7201c657cf4eaccee789bbbd6d5dfe18fb4.jpg", "table_caption": ["Table 1: Hyper-parameters for model choices in the synthetic data experiment "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "C.2 IHDP Dataset ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this experiment, we use the hidden confounding version of IHDP [Hill, 2011] which was introduced by Jesson et al. [2021]. The CAPO bounds were estimated using XGBRegressor from xgboost as the base learners for the B-Learner [Oprescu et al., 2023] estimator, and a LogisticRegression from scikit-learn as the propensity score estimator. We show in Table $2~\\mathrm{see}$ the hyper-parameter choices for each model. As for the policy model, we use a logistic regression model implemented as a single-layer MLP using several functions from pytorch, The policy model was implemented using pytorch_lightning model as a wrapper model that can receive any policy model as the base model for learning the policy. Moreover, the hyper-parameters were tuned for each uncertainty level $\\Lambda$ using the ray.tune over the search space of the hyper-parameters: The search spaces used is learning_rate $\\in[1e-4,0.1]$ ,optimizer $\\in[S G D,A d a m,A d a m W]$ , weight_decay $\\in$ $[1e\\!-\\!10,1e\\!-\\!3]$ , patience $\\in[5,20]$ , and max_epochs $\\in$ [30, 50]. ", "page_idx": 20}, {"type": "text", "text": "As for Gao and Yin [2023]\u2019s method, we train a logistic policy with the hyper-parameters: learning_rate $=0.01$ , batch_size $=32$ , max_epochs $=100$ . The hyper-parameters were tuned using a grid search over a set of possible values for each parameter, and the set with the most reasonable loss was chosen. In addition, we set $\\bar{C}(\\bar{\\boldsymbol{X}})=0$ which represents the additional cost of deferral used in Gao and Yin [2023]\u2019s objective for cases where the outcome of deferral is $Y+C(X)$ . In this experiment, the outcome of deferral to the expert is solely based on the result of the treatment prescribed by the expert, which is $Y$ . ", "page_idx": 20}, {"type": "text", "text": "We generate 1111 instances of the Hidden Confounding IHDP [Jesson et al., 2021], each consists of training $.n=470)$ ), validation $n=202$ ), and test $.n=75$ ) subsets, where for each instance the seed runs over $0,1,\\ldots$ which is the number of the trial/instance. ", "page_idx": 20}, {"type": "text", "text": "We filter the 1111 instances by the value $\\sigma_{C A T E}\\,=\\,\\sqrt{V a r(C A T E_{t e s t}(X))}$ by excluding trials with high $\\sigma_{C A T E}$ , as instances with high $\\sigma_{C A T E}$ are unrealistic and does not match the results of the original study [Brooks-Gunn et al., 1992], as explained in Curth et al. [2021]. In our case, based on the histogram of $\\sigma_{C A T E}$ for the 1111 instances of the dataset, we exclude those with $\\sigma_{C A T E}>15$ and remain with 1000 instances. ", "page_idx": 20}, {"type": "table", "img_path": "taI8M5DiXj/tmp/b075507bde42ef5aedc2c1c0094d8e52e56a2b0b8c3c3985f9be38d375bd29c4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "C.2.1 Semi-Synthetic Expert ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this experiment, we design a semi-synthetic expert, based on the original observed expert\u2019s policy, and the outcomes observed under this policy. The actions and outcomes under this policy are denoted $A$ and $Y$ , respectively, as they are (trivially) exactly the observed actions and outcomes. Let $A^{\\prime}$ and $Y^{\\prime}$ denote the new actions by the new expert\u2019s policy and the observed outcomes under this new policy respectively. We recall that the actions and the outcomes generated by the Oracle policy are denoted by $A^{*}$ , and $Y^{*}$ respectively. Then, for $i\\in[n]$ , the new expert policy is given by: ", "page_idx": 21}, {"type": "equation", "text": "$$\nA_{i}^{\\prime}={\\binom{A_{i}^{*},\\quad\\mathrm{if}\\ x_{i}^{17}=1}{A_{i},\\quad\\mathrm{otherwise}}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\nY_{i}^{\\prime}=\\left\\{Y(0)_{i},\\ \\ \\mathrm{if}\\ A_{i}^{\\prime}=0\\right.}\\\\ {Y(1)_{i},\\ \\ \\mathrm{otherwise}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The new expert $A^{\\prime}$ is defined based on the covariate $x_{17}$ - \u201cworked during pregnancy\u201d, which is a binary covariate that indicates whether the mother worked during the pregnancy or not. This covariate receives a value of 1 with a probability of 0.59. We build the new expert $\\bar{A}^{\\prime}$ to be equal to the oracle policy when $x_{17}=1$ , and equal to the original policy $A$ otherwise. Thus this expert is perfect when $x_{17}=1$ . This is an aspect of the expert that ideally we would want the model to learn. The choice of covariate $x_{17}$ was according to the covariate table shown in Jesson et al. [2021], where they show an analysis of the relationship between each covariate and both the treatment and the outcome. We choose the covariate $x_{17}$ as it is one of the features that is correlated with both the treatment and the outcome, and thus, an important feature that we are interested in testing our model on its ability to learn it. ", "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We discuss the limitations of the work in Section 8. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We state all the assumptions and theoretical guarantees in Section 6. All the proofs are provided in Appendix B. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We explain the main details of the experiments in Section 7. More details are given in Appendix C. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We include in Appendix C the link for results replication. We provide instructions for running and replicating the results as well. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: All the relevant details are given in Appendix C. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Each experiment in this paper was conducted on several trials, and all the results were reported with error bars and confidence intervals. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer:[Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: we mention the compute resources information of the machine we used for running all the experiments in Appendix C. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our work does not violate any aspect of the NeurIPS Code of Ethics. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: Discussed briefly in the introduction and discussion. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: All assets used in the paper, properly credited. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: No new assets. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: No crowdsourcing. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: No IRB. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]