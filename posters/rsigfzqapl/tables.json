[{"figure_path": "RSiGFzQapl/tables/tables_5_1.jpg", "caption": "Table 2: Model performances on ImageNet-1K when masking out tokens from different positions. Loc. kxk means masking out tokens in local kxk windows for each query. Rand n represents randomly masking out n tokens out of local 3x3 windows for each query. The attention scores of each query still sum up to 1. These models are tested directly without retraining.", "description": "This table shows the ImageNet-1K classification accuracy of different models when masking out tokens from various positions.  The masking strategies include removing tokens from local square windows of different sizes (3x3, 5x5, 7x7) and randomly masking out a set number of tokens (9, 25, 49) from a 3x3 local window. The results demonstrate the importance of local modeling for attention mechanisms.", "section": "4.2 Local Modeling Capability"}, {"figure_path": "RSiGFzQapl/tables/tables_6_1.jpg", "caption": "Table 3: Ablation on the impact of injective property using Swin-T.", "description": "This table shows the ablation study of the injective property on Swin-T model. It compares the performance of linear attention and InLine attention with different kernel functions: ReLU(.), ReLU(A\u00b7+b), LeakyReLU(.), and Identity(.). The results demonstrate that the InLine attention significantly improves the performance, especially when using kernel functions with stronger nonlinearities, highlighting the importance of the injective property for better model performance.", "section": "5.3 Empirical Analysis of Injectivity and Local Modeling"}, {"figure_path": "RSiGFzQapl/tables/tables_6_2.jpg", "caption": "Table 4: Ablation on local modeling ability based on Swin-T. Identity(\u00b7) kernel function is used.", "description": "This table presents an ablation study on the impact of local modeling ability in the Swin-T model using the Identity kernel function. It shows the performance (accuracy) of the InLine-Swin-T model with different window sizes (7x7, 14x14, 28x28, 56x56), both with and without residual connections.  The results demonstrate the importance of local modeling for effective attention.", "section": "5 Empirical Study"}, {"figure_path": "RSiGFzQapl/tables/tables_7_1.jpg", "caption": "Table 5: Comparison with baseline models on ImageNet-1K. See full comparison table in Appendix.", "description": "This table compares the performance of the proposed InLine attention method with various baseline models on the ImageNet-1K dataset.  It shows the resolution, number of parameters, FLOPs (floating point operations), and Top-1 accuracy for each model.  The results demonstrate the improved accuracy achieved by InLine compared to baseline models, with a note indicating the accuracy gain in parentheses.", "section": "5.3 Empirical Analysis of Injectivity and Local Modeling"}, {"figure_path": "RSiGFzQapl/tables/tables_7_2.jpg", "caption": "Table 5: Comparison with baseline models on ImageNet-1K. See full comparison table in Appendix.", "description": "This table compares the performance of different vision transformer models, including those using the proposed InLine attention mechanism, on the ImageNet-1K image classification benchmark.  It shows the resolution, number of parameters, floating point operations (FLOPs), and top-1 accuracy for each model. The table highlights the improved performance achieved by incorporating InLine attention compared to the baseline models.", "section": "5.3 Empirical Analysis of Injectivity and Local Modeling"}, {"figure_path": "RSiGFzQapl/tables/tables_8_1.jpg", "caption": "Table 7: Results on COCO dataset. The FLOPs are computed with an input resolution of 1280\u00d7800.", "description": "This table shows the performance comparison of different models on the COCO dataset for object detection.  It compares the FLOPs (floating point operations), scheduling (Sch.), and Average Precision (AP) metrics at different intersection over union (IoU) thresholds (AP50, AP75, APm, APm50, APm75) for both Mask R-CNN and Cascade Mask R-CNN object detection approaches.  InLine-PVT and InLine-Swin models are compared to their respective baseline models (PVT and Swin).", "section": "5.4 Main Results and Broad Comparisons"}, {"figure_path": "RSiGFzQapl/tables/tables_9_1.jpg", "caption": "Table 8: Results of semantic segmentation. The FLOPs are computed over encoders and decoders with an input image at the resolution of 512x2048. S-FPN is short for SemanticFPN [16] model.", "description": "This table presents the results of semantic segmentation experiments on the ADE20K dataset.  It compares the performance of the Softmax and InLine attention mechanisms (InLine-PVT and InLine-Swin) within the SemanticFPN and UperNet models. The table shows FLOPs, the number of parameters, mean Intersection over Union (mIoU), and mean accuracy (mAcc) for each model, highlighting the improved performance of InLine attention.", "section": "5.4 Main Results and Broad Comparisons"}, {"figure_path": "RSiGFzQapl/tables/tables_9_2.jpg", "caption": "Table 9: Comparison of different linear attention designs using DeiT-T.", "description": "This table compares the performance of several linear attention methods, including the proposed InLine attention, using the DeiT-T model on the ImageNet-1K dataset.  The comparison focuses on the accuracy achieved by each method while maintaining a similar number of parameters and FLOPs (floating point operations).  This highlights the relative efficiency and accuracy gains of the InLine attention approach compared to other state-of-the-art linear attention mechanisms.", "section": "5.3 Empirical Analysis of Injectivity and Local Modeling"}, {"figure_path": "RSiGFzQapl/tables/tables_9_3.jpg", "caption": "Table 10: Ablation on the impact of different kernel functions based on InLine-Swin-T.", "description": "This table shows the ablation study of different kernel functions used in the InLine-Swin-T model. The accuracy results are presented for each kernel function: Identity, ReLU, LeakyReLU, and Exponential.  The results demonstrate the impact of the kernel function choice on the overall model performance.", "section": "5.5 Ablation Study"}, {"figure_path": "RSiGFzQapl/tables/tables_15_1.jpg", "caption": "Table 5: Comparison with baseline models on ImageNet-1K. See full comparison table in Appendix.", "description": "This table compares the performance of the proposed InLine attention method against several baseline models (DeiT, PVT, Swin) on the ImageNet-1K dataset.  It shows the resolution, number of parameters, FLOPs (floating point operations), and top-1 accuracy for each model.  The improvements achieved by InLine are presented in parentheses. The full table with more model comparisons can be found in the appendix.", "section": "5.3 Empirical Analysis of Injectivity and Local Modeling"}, {"figure_path": "RSiGFzQapl/tables/tables_15_2.jpg", "caption": "Table 7: Results on COCO dataset. The FLOPs are computed with an input resolution of 1280\u00d7800.", "description": "This table shows the performance of different models on the COCO dataset for object detection.  It compares the standard models with the InLine versions.  Metrics include FLOPs (floating point operations), scheduling, Average Precision (AP) at different Intersection over Union (IoU) thresholds (AP50, AP75), and average precision across all IoU thresholds (APm). The results are broken down for Mask R-CNN and Cascade Mask R-CNN object detection.", "section": "5.4 Main Results and Broad Comparisons"}, {"figure_path": "RSiGFzQapl/tables/tables_16_1.jpg", "caption": "Table 8: Results of semantic segmentation. The FLOPs are computed over encoders and decoders with an input image at the resolution of 512\u00d72048. S-FPN is short for SemanticFPN [16] model.", "description": "This table presents the results of semantic segmentation experiments using different backbones (PVT-T, PVT-S, PVT-M, PVT-L, Swin-T, Swin-S, Swin-B) with and without the InLine attention mechanism.  The results are compared using the SemanticFPN and UperNet models, reporting FLOPs, the number of parameters, mean Intersection over Union (mIoU), and mean accuracy (mAcc). It showcases the performance gains achieved by incorporating InLine attention.", "section": "5.2 Datasets and Experiment Details"}, {"figure_path": "RSiGFzQapl/tables/tables_16_2.jpg", "caption": "Table 11: Comparison with baseline models on ImageNet-1K.", "description": "This table compares the performance of the proposed InLine attention models against baseline models (DeiT, PVT, Swin) on the ImageNet-1K dataset.  It shows the resolution, number of parameters, FLOPs (floating point operations), and Top-1 accuracy for each model. The improvement achieved by InLine attention is indicated in parentheses.  The table highlights that InLine attention consistently improves the performance compared to the baselines, especially in the larger models where the computational advantages are more prominent.", "section": "5.3 Empirical Analysis of Injectivity and Local Modeling"}, {"figure_path": "RSiGFzQapl/tables/tables_16_3.jpg", "caption": "Table 5: Comparison with baseline models on ImageNet-1K. See full comparison table in Appendix.", "description": "This table compares the performance of the proposed InLine attention method against several baseline models on the ImageNet-1K dataset.  The comparison includes metrics like resolution, number of parameters, FLOPs (floating-point operations), and Top-1 accuracy.  Positive differences in accuracy are shown in parentheses, indicating performance improvement achieved by InLine over the baseline methods. A more comprehensive table with additional comparisons is available in the appendix.", "section": "5.4 Main Results and Broad Comparisons"}, {"figure_path": "RSiGFzQapl/tables/tables_17_1.jpg", "caption": "Table 5: Comparison with baseline models on ImageNet-1K. See full comparison table in Appendix.", "description": "This table compares the performance of the proposed InLine attention with several baseline models on the ImageNet-1K dataset.  It shows the resolution, number of parameters, FLOPs (floating-point operations), and top-1 accuracy for each model. The results demonstrate the improved performance and efficiency of InLine attention compared to the baselines.  A more comprehensive table is available in the appendix.", "section": "5.4 Main Results and Broad Comparisons"}, {"figure_path": "RSiGFzQapl/tables/tables_17_2.jpg", "caption": "Table 5: Comparison with baseline models on ImageNet-1K. See full comparison table in Appendix.", "description": "This table compares the performance of the proposed InLine attention method with several baseline models on the ImageNet-1K image classification dataset.  It shows the resolution, number of parameters, FLOPs (floating point operations), and top-1 accuracy for each model.  The table highlights the improvements achieved by InLine attention over the baseline models.", "section": "5.3 Empirical Analysis of Injectivity and Local Modeling"}, {"figure_path": "RSiGFzQapl/tables/tables_18_1.jpg", "caption": "Table 5: Comparison with baseline models on ImageNet-1K. See full comparison table in Appendix.", "description": "This table compares the performance of the proposed InLine attention with several baseline models on the ImageNet-1K dataset.  The table shows the resolution, number of parameters, FLOPs (floating-point operations), and top-1 accuracy for each model.  A full comparison table with additional models is available in the appendix.", "section": "5.3 Empirical Analysis of Injectivity and Local Modeling"}]