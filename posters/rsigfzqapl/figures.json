[{"figure_path": "RSiGFzQapl/figures/figures_2_1.jpg", "caption": "Figure 1: An illustration of injective property and confusion problem. Non-injectivity leads to various semantic confusions in linear attention when different kernel functions are employed. (a) With \u03c6(\u00b7) = ReLU(\u00b7), linear attention assigns the same attention values to collinear queries of varying lengths. (b) Using \u03c6(\u00b7) = ReLU(A\u00b7 + b), linear attention faces severe confusion problem, producing identical attention distribution for certain queries with different directions and lengths.", "description": "This figure demonstrates the impact of the injective property on linear attention's performance.  It shows that when different kernel functions (ReLU and ReLU(A\u00b7+b)) are used, linear attention is not injective, assigning the same attention values to different queries, leading to semantic confusion.  The figure uses examples of collinear queries with varying lengths to illustrate this issue, contrasting them to softmax attention which shows no such confusion.", "section": "Analysing the Gap between Linear and Softmax Attention"}, {"figure_path": "RSiGFzQapl/figures/figures_4_1.jpg", "caption": "Figure 2: The distribution of the number of times each image encounters confusion during inference.", "description": "This figure shows the distributions of the number of times each image in the ImageNet validation set experienced confusion for Softmax, Linear, and InLine attention.  The x-axis represents the number of times confusion occurred, while the y-axis shows the percentage of images experiencing that level of confusion.  The graph visually demonstrates the significant reduction in confusion instances achieved by InLine attention compared to Linear attention, while Softmax attention shows negligible confusion.", "section": "Analysing the Gap between Linear and Softmax Attention"}, {"figure_path": "RSiGFzQapl/figures/figures_5_1.jpg", "caption": "Figure 4: Visualizations of attention distributions. Softmax attention exhibits strong local bias. The other two attention types yield meaningful attention distributions, but focus more on global modeling.", "description": "This figure visualizes the attention distributions produced by Softmax attention, linear attention, and InLine attention on two example images.  It demonstrates that Softmax attention focuses more on local details within the image, as indicated by the concentrated attention weights. Conversely, linear attention and InLine attention show more distributed attention across the images, suggesting a greater focus on global contextual information rather than specific local regions.", "section": "4.2 Local Modeling Capability"}, {"figure_path": "RSiGFzQapl/figures/figures_5_2.jpg", "caption": "Figure 3: The sum of attention scores in the local 3\u00d73 windows of each query from DeiT-T.", "description": "This figure shows the sum of attention weights assigned to the 3x3 neighborhood of each query token across different layers of the DeiT-T model.  It compares the local attention behavior of Softmax attention, linear attention, and the proposed InLine attention. The plot reveals that Softmax attention exhibits a stronger local bias, particularly in the shallower layers, while linear attention and InLine attention show less focus on local neighborhoods. The horizontal dotted line represents the expected average attention weight for a 3x3 window if attention were randomly assigned (9/197).", "section": "4.2 Local Modeling Capability"}, {"figure_path": "RSiGFzQapl/figures/figures_8_1.jpg", "caption": "Figure 5: Speed measurements. Runtime and FPS is tested on a RTX3090 GPU. (a) Accuracy-Runtime curve on ImageNet. (b) Increasing window size. (c) High-resolution scenarios.", "description": "This figure presents a comprehensive analysis of the speed and efficiency of InLine attention compared to traditional Softmax attention and other linear attention methods.  Three subfigures illustrate different aspects: (a) shows the trade-off between accuracy and runtime on the ImageNet dataset, demonstrating that InLine achieves higher accuracy with similar runtime or faster runtime with similar accuracy; (b) evaluates performance as the window size increases, showing InLine's superior scalability; (c) focuses on high-resolution image processing, revealing InLine's significant computational advantages over other methods.  This figure supports the paper's claim of InLine attention's enhanced efficiency and speed, particularly in high-resolution scenarios.", "section": "5.3 Empirical Analysis of Injectivity and Local Modeling"}]