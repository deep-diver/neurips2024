[{"type": "text", "text": "Gradient Methods for Online DR-Submodular Maximization with Stochastic Long-Term Constraints ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Guanyu Nie Vaneet Aggarwal Christopher John Quinn Iowa State University Purdue University Iowa State University Ames, IA 50010 West Lafayette, IN 47907 Ames, IA 50010 nieg@iastate.edu vaneet@purdue.edu cjquinn@iastate.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this paper, we consider the problem of online monotone DR-submodular maximization subject to long-term stochastic constraints. Specifically, at each round $t\\ \\in\\ [T]$ , after committing an action $\\mathbf{x}_{t}$ , a random reward $f_{t}(\\mathbf{x}_{t})$ and an unbiased gradient estimate of the point $\\widetilde{\\nabla}f_{t}(\\mathbf{x}_{t})$ (semi-bandit feedback) are revealed. Meanwhile, a budget of $g_{t}(\\mathbf{x}_{t})$ , which is linear and stochastic, is consumed of its total allotted budget $B_{T}$ . We propose a gradient ascent based algorithm that achieves $\\frac{1}{2}$ -regret of $\\mathcal{O}(\\sqrt{T})$ with ${\\mathcal O}(T^{3/4})$ constraint violation with high probability. Moreover, when first-order full-information feedback is available, we propose an algorithm that achieves $(1-1/e)$ -regret of $\\mathcal{O}(\\sqrt{T})$ with ${\\mathcal O}(T^{3/4})$ constraint violation. These algorithms significantly improve over the state-of-the-art in terms of query complexity. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Online optimization confronts diverse challenges as information gradually unfolds, compelling irreversible decisions at each step amidst uncertainty about future information. Typically, an online optimization problem can be framed as a recurring game between a learner or algorithm and an adversary or environment: during each iteration, the learner chooses an action from a predefined domain set, and the environment subsequently reveals feedback in the form of utility or reward for the selected action. Over time, the learner aims to learn from past experiences and improve decision-making strategies to maximize cumulative rewards. ", "page_idx": 0}, {"type": "text", "text": "In instances where the objective function is concave and the feasible set is convex, the problem has been extensively explored in the literature under the name of online convex optimization (OCO) [13]. OCO has found considerable success in various machine learning applications, leveraging the well-established theories of convex optimization. Within OCO, it is established that any algorithm incurs a regret of $\\Omega({\\sqrt{T}})$ in the worst case [13]. Notably, there exist algorithms that match this lower bound, such as Online Gradient Descent (OGD) [43]. ", "page_idx": 0}, {"type": "text", "text": "Even though optimizing convex/concave functions can be done efficiently, most problems in artificial intelligence are non-convex. Examples include training deep neural networks, Bayesian inference, and clustering, among many others. One important example of such functions is called submodular set function. Submodular set functions exhibit a natural property known as diminishing returns, akin to concave functions in continuous domains. They have been applied in various machine learning contexts, including viral marketing [16], sensor placement [11], recommendation systems [23], data summarization [22], and numerous others. However, submodularity extends beyond set functions and can also be defined for continuous functions. DR-submodularity represents a special class of such continuous functions [5]. ", "page_idx": 0}, {"type": "text", "text": "In the realm of online DR-submodular maximization, two primary types of algorithms have received extensive attention. Gradient ascent algorithms are often favored for their simplicity and lower gradient sample complexity, while Frank-Wolfe algorithms offer the ability to avoid potentially costly projection operations [9]. The type of feedback also plays a pivotal role in algorithm design. Frank-Wolfe algorithms typically assume more information is acquired each round, assuming that upon committing an action, the agent can observe the entire function, and gradients of multiple points can be observed. This feedback type is often referred to as full information feedback. On the other hand, gradient ascent-based algorithms usually assume only the function value of the chosen action and the gradient at that point can be observed. This fedback type is often referred to as semi-bandit feedback. Recent efforts have aimed to enhance gradient queries for Frank-Wolfe algorithms, albeit at the cost of worse regret [41]. ", "page_idx": 1}, {"type": "text", "text": "In numerous applications, aside from the goal of maximizing the cumulative reward, there exist constraints on the sequence of decisions made by the learner that must be satisfied on average [1, 3]. In such scenarios, long-term constraints pose challenges because the decision-making process cannot be simply expressed as restricting the set of actions for all time steps may be sub-optimal. Instead, the learner still (always) wants to maximize cumulative reward, but if putatively instantaneously high reward arms are expensive, can only play them a few times. To illustrate, let's consider the online ad allocation problem encountered by an advertiser. At each round $t\\in[T]$ , the advertiser faces the task of determining the allocation of funds across $n$ different websites for placing ads. While the primary aim is to maximize the overall click-through rates of the ads, the advertiser is also constrained by a predetermined budget allocated over a specified time horizon [4]. In this scenario, the cost of ad placement in each round may either be fixed and known in advance, or it may depend on the number of clicks the ads receive, which remains uncertain in advance. As a result, the advertiser must navigate the trade-off between optimizing the total reward and adhering to budget constraints. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we study the problem of online DR-submodular maximization with long-term (linear) budget constraints. While most prior works assume the objective function is chosen adversarially [10, 29, 31], we study the stochastic DR-submodular maximization setting, where the objective functions are i.i.d. sampled from a distribution [7, 12]. Note that this setting is still of interest because as opposed to assuming each arriving function $f_{t}$ being DR-submodular, we only assume the expectationof $f_{t}$ to possess DR-submodularity. Moreover, we consider the semi-bandit feedback setting where only the noisy function value $f_{t}(\\mathbf{x}_{t})$ and an unbiased gradient estimator of that point $\\widetilde{\\nabla}f_{t}(\\mathbf{x}_{t})$ can be observed. To the best of our knowledge, these particular settings (stochastic utility and semi-bandit feedback) have not been explored in the literature on DR-submodular maximization with long-term constraints. ", "page_idx": 1}, {"type": "text", "text": "Our Contributions: We summarize our contributions in three parts. ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "1. In the semi-bandit feedback setting, we propose the first stochastic gradient ascent based algorithm for stochastic online DR-submodular maximization with stochastic long-term constraints. Our proposed algorithm can achieve $\\mathcal{O}(\\sqrt{T})$ $\\frac{1}{2}$ -regret and ${\\mathcal O}(T^{3/4})$ constraint violation with high probability. Compared to all previous works [10, 29, 31], they consider first-order full-information feedback and require unbiased gradient estimates at $\\sqrt{T}$ locations (not just at action $\\mathbf{x}_{t}$ ) in every round. For those works, their query complexity in each round is $\\sqrt{T}$ while ours is just 1. ", "page_idx": 1}, {"type": "text", "text": "2. In first order full-information setting, where the unbiased gradient estimates of any point can be observed, we propose the first stochastic gradient ascent based algorithm for stochastic online DRsubmodular maximization with stochastic long-term constraints. We utilize the recently developed technique in [42] called the non-oblivious function. Our proposed algorithm can achieve $\\mathcal{O}(\\sqrt{T})$ $(1-1/e)$ -regret and ${\\mathcal O}(T^{3/4})$ constraint violation with high probability. Again, compared to previous works [10, 29, 31], our query complexity is significantly lower. ", "page_idx": 1}, {"type": "text", "text": "Regarding the approximation ratios: We note that in offline $1-1/e$ is known to be the optimal approximation ratio for optimizing monotone DR submodular functions over a general convex set, where the query can be anywhere in the convex hull of $\\mathcal{K}\\cup\\{\\mathbf{0}\\}$ $\\kappa$ is the constraint set). However, when the oracle calls are restricted to $\\kappa$ , an approximation ratio of $1/2$ is the best that is known to be achievable [28]. Thus, full information feedback can achieve $1-1/e$ -regret while thesemi-bandit feedback achieves $1/2$ -regret. ", "page_idx": 1}, {"type": "text", "text": "Table 1: We include related works from online DR-submodular optimization with constant or stochastic long term constraint functions. (Works handling adversarial long-term constraints require a different definition of regret.) All methods require a gradient oracle for feedback, and \u201cNoise' lists whether the gradient is exact or there is stochastic noise. \u2018# Grad. is the number gradient evaluations required per-round. \\*Con. Viol.? is the bound on the constraint violation. $\\dag\\,[31]$ considered constraint set being convex while all other works consider linear constraint. In \u2018# Grad.\u2019 column, $2\\sqrt{T}$ means this work needs $\\sqrt{T}$ gradients on both $f$ and $g$ $\\ddagger$ While all actions will be feasible, some gradient queries will be in the convex hull of $\\mathcal{K}\\cup\\{\\mathbf{0}\\}$ ", "page_idx": 2}, {"type": "table", "img_path": "PTxRRUEpHq/tmp/8f85c64013b5c23309d4ef2abd5388df88ca72423eba99947cb11f701686f2ee.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The primary related works are summarized in Table 1. We briefly discuss notable contributions and for additional related works, see Appendix F ", "page_idx": 2}, {"type": "text", "text": "Online DR-submodular Maximization with Long-Term Constraints We do not compare results with adversarial constraints due to additional assumptions needed in this setting. See Appendix F for a detailed discussion. In the context of stochastic constraints, Raut et al. [29] conducted the initial study of the problem. They successfully attained $\\mathcal{O}(\\sqrt{T})$ regret and constraint violation with high probability, as well as ${\\mathcal{O}}(T^{3/4})$ regret and $\\mathcal{O}(\\sqrt{T})$ constraint violation in expectation. Building upon this work, Sadeghi et al. [31] further improved the results to achieve $\\mathcal{O}(\\sqrt{T})$ regret and constraint violation, both in expectation and with high probability. Additionally, Feng et al. [10] extended these findings to incorporate weakly DR-submodular utility, achieving analogous results. ", "page_idx": 2}, {"type": "text", "text": "Online Convex Optimization with Long-Term Constraints Several results for OCO with deterministic long-term constraints can be found in [14, 20, 37, 38, 40]. Existing literature has established that a regret of $\\mathcal{O}(\\sqrt{T})$ and a cumulative constraint violation of ${\\mathcal{O}}(T^{1/4})$ can be achieved without the Slater condition. Conversely, assuming the Slater condition allows for achieving a regret of $\\mathcal{O}(\\sqrt{T})$ and a cumulative constraint violation of $O(1)$ . In cases where the considered constraint is assumed to be stochastic, Yu et al. [39] achieved a $\\mathcal{O}(\\sqrt{T})$ bound on both regret and constraint violations under the Slater condition. Furthermore, Wei et al. [35] achieved the same regret and constraint violation bounds while assuming a strictly weaker assumption than the Slater condition. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1Notations ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Vectors are shown by lowercase bold letters, such as $\\mathbf{x}\\in\\mathbb{R}^{d}$ . We denote by $\\|\\cdot\\|$ the $\\ell_{2}$ (Euclidean) norm. We use $[T]$ to denote the set $\\{1,2,\\ldots,T\\}$ . The inner product of two vectors $\\mathbf{x},\\mathbf{y}\\,\\in\\,\\mathbb{R}^{d}$ is denoted by either $\\langle\\mathbf{x},\\mathbf{y}\\rangle$ or $\\mathbf{x}^{\\top}\\mathbf{y}$ . For $u\\in\\mathbb R$ , we define $[u]_{+}:=\\operatorname*{max}\\{u,0\\}$ . For two vectors $\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^{d},\\mathbf{x}\\preceq\\mathbf{y}$ implies that $x_{i}\\leq y_{i},\\forall i\\in[d]$ . For a convex set $\\mathcal{X}$ , we denote the projection of $\\mathbf{y}$ onto set $\\mathcal{X}$ as $\\begin{array}{r}{\\Pi_{\\mathcal{X}}(\\mathbf{y})=\\arg\\operatorname*{min}_{\\mathbf{x}\\in\\mathcal{X}}\\left\\|\\mathbf{x}-\\mathbf{y}\\right\\|}\\end{array}$ ", "page_idx": 2}, {"type": "text", "text": "3.2  Function Properties ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Here we list some function properties that will appear in our assumptions. ", "page_idx": 2}, {"type": "text", "text": "Monotonicity A function $f$ is monotone if $f(\\mathbf{x})\\leq f(\\mathbf{y})$ for all $\\mathbf x\\preceq\\mathbf y$ ", "page_idx": 2}, {"type": "text", "text": "Lipschitz continuous A function $f$ is Lipschitz continuous with parameter $\\beta$ if for any $\\mathbf{x},\\mathbf{y}\\in{\\mathcal{X}}$ \uff0c we have $f(\\mathbf{x})-f(\\mathbf{y})\\leq\\beta\\|\\mathbf{x}-\\mathbf{y}\\|$ ", "page_idx": 3}, {"type": "text", "text": "3.3 DR-Submodular Functions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "A function $f:2^{\\Omega}\\to\\mathbb{R}_{+}$ , defined on the ground set $\\Omega$ , is said to be submodular if ", "page_idx": 3}, {"type": "equation", "text": "$$\nf(A)+f(B)\\geq f(A\\cup B)+f(A\\cap B)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "for all $A,B\\subset\\Omega$ . The notion of submodularity has been extended to continuous domains [2, 34, 36]. Consider a function $f:\\mathcal{X}\\to\\mathbb{R}_{+}$ where the domain is of the form $\\textstyle\\mathcal{X}=\\prod_{i=1}^{n}\\mathcal{X}_{i}$ and each $\\mathcal{X}_{i}$ is a compactsubset of $\\mathbb{R}_{+}$ .We say that $f$ is continuous submodular if $f$ is continuous and for all $\\mathbf{x},\\mathbf{y}\\in{\\mathcal{X}}$ , we have ", "page_idx": 3}, {"type": "equation", "text": "$$\nf(\\mathbf{x})+f(\\mathbf{y})\\geq f(\\mathbf{x}\\lor\\mathbf{y})+f(\\mathbf{x}\\land\\mathbf{y})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf x\\vee\\mathbf y$ and $\\mathbf x\\wedge\\mathbf y$ are component-wise maximum and minimum, respectively. For efficient maximization, we also require that these functions satisfy a diminishing returns condition [5]. We say a differentiablefunction $f$ is continuous DR-submodular if $f$ itsatisfies ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\nabla f(\\mathbf{x})\\succeq\\nabla f(\\mathbf{y})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "for all $\\mathbf x\\preceq\\mathbf y$ . When the function $f$ is twice differentiable, DR-submodularity is equivalent to ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{\\partial^{2}f(\\mathbf{x})}{\\partial x_{i}\\partial x_{j}}\\leq0,\\forall i,j,\\forall\\mathbf{x}\\in\\mathcal{X}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The following lemma is an important property of monotone DR-submodular functions. ", "page_idx": 3}, {"type": "text", "text": "Lemma 1. Let $f:\\mathcal{X}\\to\\mathbb{R}_{+}$ be a monotone, differentiable, $D R$ -submodular function. For any two vectors $\\mathbf{x},\\mathbf{y}\\in\\mathcal{X}$ ,we have ", "page_idx": 3}, {"type": "equation", "text": "$$\nf(\\mathbf{y})-2f(\\mathbf{x})\\leq\\langle\\nabla f(\\mathbf{x}),\\mathbf{y}-\\mathbf{x}\\rangle.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The proof can be found in the proof of Theorem 4.2 in [12]. Note that this property shares an analogy with a pivotal characteristic that defines concavity: $f(\\mathbf{y})\\,-\\,f(\\mathbf{x})\\,\\leq\\,\\langle\\nabla f(\\mathbf{x}),\\mathbf{y}\\,-\\,\\mathbf{x}\\rangle$ . Lemma 1 also implies that if we use gradient descent directly on $f$ , we can only achieve an approximation ratio of $1/2$ . Zhang et al. [42] introduced the so-called non-oblivious function to obtain the optimal approximation ratio of $1-1/e$ ", "page_idx": 3}, {"type": "text", "text": "Lemma 2. Let $f:\\mathcal{X}\\to\\mathbb{R}_{+}$ be a monotone, differentiable, DR-submodular function, and let $F$ be the non-oblivious function of $f$ defined by its gradient $\\begin{array}{r}{\\nabla F({\\bf x})=\\int_{0}^{1}e^{z-1}\\nabla f(z\\cdot{\\bf x})d z}\\end{array}$ Then for any vectors $\\mathbf{x},\\mathbf{y}\\in\\mathcal{X}$ , we have ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left(1-e^{-1}\\right)f(\\mathbf{y})-f(\\mathbf{x})\\leq{\\langle\\nabla F(\\mathbf{x}),\\mathbf{y}-\\mathbf{x}\\rangle}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The proof of Lemma 2 can be found in [42]. ", "page_idx": 3}, {"type": "text", "text": "4 Problem Statement ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Consider the following ofline optimization problem: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\operatorname*{max}_{\\mathbf{x}\\in\\mathcal{K}}}&{f(\\mathbf{x})}\\\\ {\\mathrm{subject\\,to}}&{g(\\mathbf{x})\\le0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $g(\\mathbf{x})=\\langle p,\\mathbf{x}\\rangle-b$ for some non-negative constant $b$ .We study an analogous online setup as follows: At each round $t\\in[T]$ , the algorithm chooses an action $\\mathbf{x}_{t}\\,\\in\\,\\mathcal{K}$ ,where $\\boldsymbol{\\kappa}\\subset\\mathbb{R}_{+}^{d}$ is a fixed, known set. We consider both the utility and the constraints being stochastic, where we assume at each time step, the utility function $f_{t}$ is sampled i.i.d. from a distribution $\\mathcal{D}_{f}$ with mean $f$ ,i.e., $\\mathbb{E}_{f_{t}\\sim\\mathcal{D}_{f}}[f_{t}(\\cdot)]=f(\\cdot)$ , while the cost vector $\\mathbf{\\boldsymbol{p}}_{t}$ is i.i.d. sampled from another distribution $\\mathcal{D}_{p}$ . After an action is selected by the learner, a random reward $f_{t}(\\mathbf{x}_{t})$ is obtained while using $\\langle\\pmb{p}_{t},\\mathbf{x}_{t}\\rangle$ of its fixed total allotted budget $B_{T}$ , and $\\mathbf{\\nabla}p_{t}$ is observed. In the semi-bandit setting, an unbiased gradient estimator for that action, $\\widetilde{\\nabla}f_{t}(\\mathbf{x}_{t})$ , is also revealed. In the first order full-information setting, the unbiased gradient estimator of any point can be observed. In this paper, we consider both settings while all other works in the literature on DR-submodular maximization with long-term constraints, such as [10, 29, 31], consider full-information feedback. ", "page_idx": 3}, {"type": "text", "text": "To make sure the long-term constraint is not vacuous, we consider $B_{T}=b T$ for a constant $b$ such that $\\begin{array}{r}{\\operatorname*{min}_{\\mathbf{x}\\in\\mathcal{K}}\\langle p,\\mathbf{x}\\rangle\\leq\\bar{b}<\\operatorname*{max}_{\\mathbf{x}\\in\\mathcal{K}}\\langle p,\\mathbf{x}\\rangle}\\end{array}$ . In this case, there will always be a solution $\\mathbf{x}$ that satisfies constraint (having zero constraint violation) and it is not the case that any sequence of actions (especially the most expensive w.r.t. $\\textbf{\\emph{p}}$ ) is feasible. ", "page_idx": 4}, {"type": "text", "text": "We make the following assumptions to proceed our analysis: ", "page_idx": 4}, {"type": "text", "text": "Assumption 1. The constraint set $\\kappa$ is convex and compact, with diameter $d=\\operatorname*{sup}_{\\mathbf{x},\\mathbf{y}\\in K}\\|\\mathbf{x}-\\mathbf{y}\\|$ and radius $\\begin{array}{r}{r=\\operatorname*{sup}_{\\mathbf{x}\\in K}\\|\\mathbf{x}\\|}\\end{array}$ . Since $\\mathcal{X}$ is compact, we denote its diameter as ${\\bar{d}}=\\operatorname*{sup}_{\\mathbf{x},\\mathbf{y}\\in{\\mathcal{X}}}\\|\\mathbf{x}-\\mathbf{y}\\|$ and radius as $\\bar{r}=\\operatorname*{sup}_{\\mathbf{x}\\in\\mathcal{X}}\\|\\mathbf{x}\\|$ , respectively. ", "page_idx": 4}, {"type": "text", "text": "Assumption 2. The expected utility function $f(\\cdot)$ is monotone DR-submodular and $\\beta_{f}$ -Lipschitz. ", "page_idx": 4}, {"type": "text", "text": "Assumption 3. The distribution $\\mathcal{D}_{p}$ for the cost vectors has bounded support $\\beta_{p}B\\cap\\mathbb{R}_{+}^{d}$ with mean $\\mathbf{\\nabla}p\\succeq\\mathbf{0}$ where $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ is the unit ball of Euclidean norm. ", "page_idx": 4}, {"type": "text", "text": "Assumption 4. The gradient oracle is unbiased $\\mathbb{E}[\\nabla f(\\mathbf{x})-\\widetilde{\\nabla}f_{t}(\\mathbf{x})|\\mathbf{x}]\\,=\\,0$ and has a bounded variance $\\begin{array}{r l r}{\\mathbb{E}[\\|\\nabla f({\\mathbf x})\\;-\\;\\widetilde\\nabla f_{t}({\\mathbf x})\\|^{2}|{\\mathbf x}]}&{\\le}&{\\sigma^{2}}\\end{array}$ . In the semi-bandit setting, we assume $\\textit{G}=$ $\\operatorname*{max}_{t}\\operatorname*{sup}_{\\mathbf{x}}\\|\\widetilde\\nabla f_{t}(\\mathbf{x})\\|$ is finite. In the first order full-information setting, denoting the unbiased estimator of the non-oblivious function obtained at round $t$ by $\\widetilde{\\nabla}F_{t}(\\mathbf{x})$ , we assume ${\\cal G}_{F}~=~$ $\\operatorname*{max}_{t}\\operatorname*{sup}_{\\mathbf{x}}\\|\\widetilde\\nabla F_{t}(\\mathbf{x})\\|$ is finite. ", "page_idx": 4}, {"type": "text", "text": "Unlike Frank-Wolfe type algorithms in other papers [10, 29, 31], we do not assume bounded smoothness on the gradients: $\\nabla f(\\mathbf{x})-\\nabla f(\\mathbf{y})\\leq\\beta\\|\\mathbf{x}-\\mathbf{y}\\|$ . Moreover, we do not assume $f({\\bf0})=0$ ", "page_idx": 4}, {"type": "text", "text": "Our overall goal is to maximize the total obtained reward while satisfying the budget constraint asymptotically (i.e., $\\begin{array}{r}{\\sum_{t=1}^{T}\\langle p,\\mathbf{x}_{t}\\rangle-B_{T}}\\end{array}$ being sub-linear in $T$ ", "page_idx": 4}, {"type": "text", "text": "Note that our proposed algorithm can handle multiple linear constraints as well, and similar regret and constraint violation bounds can be derived. In the case of there are $m$ constraints $g_{i}(\\cdot),i\\in\\bar{[}m]$ we can define $g(\\mathbf{x}):=\\operatorname*{max}_{i\\in[m]}{g_{i}(\\mathbf{x})}$ and it can be shown that $g$ preserves the same properties as those of individual $g_{i}$ 's (sub-differentiability, bounded (sub-)gradients and bounded values; see Proposition 6 in [20] for proofs). ", "page_idx": 4}, {"type": "text", "text": "For simplicity of presentation, we denote $\\beta\\,=\\,\\operatorname*{max}\\{\\beta_{f},\\beta_{p}\\}$ . Since $\\kappa$ is compact, from monotonicity of $f$ we have $F_{1}\\ :=\\ \\operatorname*{max}_{\\mathbf{x}\\in K}|f(\\mathbf{x})|$ is bounded. Since $f$ is $\\beta_{f}$ -Lipschitz, we have $F_{2}\\,:=\\,\\mathrm{max}_{\\mathbf{x},\\mathbf{y}\\in{\\mathcal{K}}}\\,|f(\\mathbf{x})-f(\\mathbf{y})|\\,\\leq\\,\\beta_{f}D$ is bounded. Since $\\kappa$ is compact and $\\mathcal{D}_{p}$ has bounded support, we have $\\begin{array}{r}{C:=\\operatorname*{max}_{\\pmb{p}^{\\prime}\\sim\\mathcal{D}_{p}}\\operatorname*{max}_{\\mathbf{x}\\in\\mathcal{K}}|\\langle\\pmb{p}^{\\prime},\\mathbf{x}\\rangle-\\frac{B_{T}}{T}|}\\end{array}$ is bounded. ", "page_idx": 4}, {"type": "text", "text": "To measure the effectiveness of our proposed algorithm, we use the notions of regret and total constraint violation to quantify the overall utility and the total resource consumption, respectively. ", "page_idx": 4}, {"type": "text", "text": "Regret is typically defined as the difference between the total reward accumulated by the algorithm and the best fixed action in hindsight. Note that even in the offline setting, maximizing a monotone DR-submodular function subject to a convex constraint can only be done approximately in polynomial time unless $\\mathbf{RP}=\\mathbf{NP}$ [5]. Thus, we instead use the notion of $\\alpha$ -regret of an algorithm. ", "page_idx": 4}, {"type": "text", "text": "Definition 1. The $\\alpha$ -regret of an online algorithm with outputs $\\{\\mathbf{x}_{t}\\}_{t=1}^{T}$ is defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\nR_{T}:=\\alpha\\operatorname*{max}_{\\mathbf{x}\\in K^{*}}\\sum_{t=1}^{T}f_{t}(\\mathbf{x})-\\sum_{t=1}^{T}f_{t}(\\mathbf{x}_{t}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\kappa^{\\ast}$ is the restricted search space of solutions that satisfy long-term constraints for $T$ steps, (i.e., can be played $T$ times, $\\begin{array}{r}{\\mathcal{K}^{*}=\\left\\lbrace\\mathbf{x}\\in\\mathcal{K}:\\sum_{t=1}^{T}g(\\mathbf{x})\\leq0\\right\\rbrace}\\end{array}$ Whichi also quivlenta satisying per-round constraint: $K^{*}=\\left\\{\\mathbf{x}\\in K:g(\\mathbf{x})\\leq0\\right\\}$ ", "page_idx": 4}, {"type": "text", "text": "Since we are mainly interested in stochastic utility functions, i.e., $f_{t}\\sim\\mathcal{D}_{f}$ , we aim to minimize the expected $\\alpha$ -regret: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}[R_{T}]=\\alpha T\\operatorname*{max}_{\\mathbf{x}\\in K^{*}}f(\\mathbf{x})-\\sum_{t=1}^{T}f(\\mathbf{x}_{t}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Denote $\\mathbf{x}^{*}=\\arg\\operatorname*{max}_{\\mathbf{x}\\in K^{*}}f(\\mathbf{x})$ . Note that since $\\scriptstyle{\\mathbf{\\mathit{p}}}_{t}$ is drawn i.i.d. from the distribution $\\mathcal{D}_{p}$ with mean $\\pmb{p}\\forall t\\in[T]$ , the best benchmark action is with respect to the \u201ctrue\u201d underlying $\\pmb{p}$ of the constraint ", "page_idx": 4}, {"type": "text", "text": "function as opposed to $\\mathbf{\\boldsymbol{p}}_{t}$ . It is possible that the best-fixed action has a constraint violation with some noisy $\\scriptstyle{\\mathbf{\\mathit{p}}}_{t}$ 's. ", "page_idx": 5}, {"type": "text", "text": "We next define the total constraint violation. ", "page_idx": 5}, {"type": "text", "text": "Definition 2. The total constraint violation of an online algorithm with outputs $\\{\\mathbf{x}_{t}\\}_{t=1}^{T}$ is defined as ", "page_idx": 5}, {"type": "equation", "text": "$$\nC_{T}:=\\sum_{t=1}^{T}g(\\mathbf{x}_{t})=\\sum_{t=1}^{T}\\langle\\pmb{p},\\mathbf{x}_{t}\\rangle-B_{T}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Again, in the stochastic constraint setting, the total constraint violation is defined with respect to the mean $\\pmb{p}$ ", "page_idx": 5}, {"type": "text", "text": "5   An Efficient Primal-Dual Algorithm under Semi-bandit Feedback ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we introduce our first proposed algorithm for online DR-submodular maximization subject to stochastic long term constraints under semi-bandit feedback: Online Lagrangian Stochastic Gradient Ascent (OLSGA). The algorithm is presented in Algorithm 1. The overall structure of the algorithm is inspired by the primal-dual update in Online Convex Optimization (OCO) (e.g., [20]). ", "page_idx": 5}, {"type": "text", "text": "Associating a dual variable $\\lambda\\in[0,+\\infty)$ with the constraint, the saddle point formulation of (1) can be written as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{x\\in K}\\operatorname*{min}_{\\lambda\\in[0,+\\infty)}f(\\mathbf{x})-\\lambda g(\\mathbf{x}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In this work, we consider the following regularized Lagrangian function $\\mathcal{L}(\\mathbf{x},\\lambda)$ givenby ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal L(\\mathbf x,\\lambda):=f(\\mathbf x)-\\lambda g(\\mathbf x)+\\frac{\\delta\\eta}{2}\\lambda^{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "It is important to observe that the expression in (3) deviates from the conventional Lagrangian due to the inclusion of the term $\\frac{\\delta\\eta}{2}\\lambda_{-}^{2}$ where both $\\delta$ and $\\eta$ are parameters that will be later chosen to optimize theoretical guarantees. The main purpose of this modification is to control the value of $\\lambda$ and prevent it from growing too large. Although we can achieve the same goal by restricting $\\lambda$ to a bounded domain, using the quadratic regularizer makes it convenient for our analysis. ", "page_idx": 5}, {"type": "text", "text": "One issue is that $\\textbf{\\emph{p}}$ (shown in $g(\\mathbf{x})_{,}$ is unknown to the online algorithm. Therefore, we alternatively use an empirical estimate $\\begin{array}{r}{\\widehat{\\pmb{p}}_{t}=\\frac{1}{t}\\sum_{s=1}^{t}p_{s}}\\end{array}$ insteadof $\\pmb{p}$ in the Lagrangian function. Moreover,in order to achieve the high probability bound, we adjust our Lagrangian function in (3) as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal L_{t}(\\mathbf x,\\lambda)=f_{t}(\\mathbf x)-\\lambda\\widetilde g_{t}(\\mathbf x)+\\frac{\\delta\\eta}{2}\\lambda^{2},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\begin{array}{r}{\\widetilde{g}_{t}(\\mathbf{x})=\\langle\\widehat{p}_{t},\\mathbf{x}\\rangle-\\frac{B_{T}}{T}-\\gamma_{t}}\\end{array}$ and $\\begin{array}{r}{\\gamma_{t}=\\sqrt{\\frac{2C^{2}\\log(\\frac{2T}{\\varepsilon})}{t}}}\\end{array}$ 2C\u00b2log( For the purpose of analysis, we further define $\\begin{array}{r}{\\widehat{g}_{t}(\\mathbf{x}):=\\langle\\widehat{p}_{t},\\mathbf{x}\\rangle-\\frac{B_{T}}{T}}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "For the purpose of analysis, we do not directly use Equation (4) in our primal update. Let $\\widehat{\\mathcal{L}}_{t}$ be defined by its gradient, $\\begin{array}{r}{\\tilde{\\nabla}_{x}\\widehat{\\mathcal{L}}_{t}(\\mathbf{x}_{t},\\lambda_{t})=\\widetilde{\\nabla}f_{t}(\\mathbf{x}_{t})-2\\lambda_{t}\\mathbf{\\dot{V}}\\widetilde{g}_{t}\\left(\\mathbf{x}_{t}\\right)}\\end{array}$ . The primal updates are formulated asfollows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{x}_{t+1}=\\Pi_{K}(\\mathbf{x}_{t}+\\eta\\widetilde{\\nabla}_{x}\\widehat{\\mathcal{L}}_{t}(\\mathbf{x}_{t},\\lambda_{t})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Note that compared to Equation (4), the Lagrangian function used for updating has a coefficient of 2 in front of the second term. ", "page_idx": 5}, {"type": "text", "text": "Our proposed algorithm is shown in Algorithm 1. The algorithm proceed as follows: it takes a convex constraint set $\\kappa$ and a time horizon $T$ as inputs. Initially, the algorithm selects an initial point $\\mathbf{x}_{1}\\in\\mathcal{K}$ and sets $\\lambda_{1}=0$ . At each time step $t\\,\\in\\,[T]$ , the algorithm takes an action $\\mathbf{x}_{t}$ , acquires a reward $f_{t}(\\mathbf{x}_{t})$ , and observes the cost vector $\\scriptstyle{\\mathbf{\\mathit{p}}}_{t}$ as well as an unbiased gradient estimate $\\widetilde{\\nabla}f_{t}(\\mathbf{x}_{t})$ Subsequently, unbiased gradient estimates of the updating Lagrangian function with respect to $\\mathbf{x}$ and to $\\lambda$ are computed using the empirical estimate of $\\textbf{\\emph{p}}$ . Using these calculated gradients, updates to $\\mathbf{x}$ and $\\lambda$ are made using (7) and (8), respectively. ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 OLSGA (Semi-bandit Feedback) ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "1: Input: Convex set $\\kappa$ , time horizon $T$   \n2: Initialize $\\mathbf{x}_{1}\\in\\mathcal{K}$ \uff0c $\\lambda_{1}=0$   \n3: for $t\\in[T]$ do   \n4: Play $\\mathbf{x}_{t}$ , obtain $f_{t}(\\mathbf{x}_{t})$ and $\\widetilde{\\nabla}f_{t}(\\mathbf{x}_{t})$ and $\\mathbf{\\boldsymbol{p}}_{t}$   \n5: Compute pt = s=1 Ps   \n6: Compute ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{\\nabla}_{x}\\widehat{\\mathcal L}_{t}(\\mathbf x_{t},\\lambda_{t})=\\widetilde{\\nabla}f_{t}(\\mathbf x_{t})-2\\lambda_{t}{\\nabla}\\widetilde{g}_{t}(\\mathbf x_{t})}\\\\ &{\\nabla_{\\lambda}\\mathcal L_{t}(\\mathbf x_{t},\\lambda_{t})=-\\widetilde{g}_{t}(\\mathbf x_{t})+\\delta\\eta\\lambda_{t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "7: Update $\\mathbf{x}_{t}$ and $\\lambda_{t}$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{x}_{t+1}=\\Pi_{K}(\\mathbf{x}_{t}+\\eta\\widetilde{\\nabla}_{x}\\widehat{\\mathcal{L}}_{t}(\\mathbf{x}_{t},\\lambda_{t}))}\\\\ &{\\lambda_{t+1}=\\Pi_{[0,+\\infty)}(\\lambda_{t}-\\eta\\nabla_{\\lambda}\\mathcal{L}_{t}(\\mathbf{x}_{t},\\lambda_{t}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "8: end for ", "page_idx": 6}, {"type": "text", "text": "Remark 1. A notable difference between our algorithm and all prior works addressing online DRsubmodular maximization with long-term constraints (e.g., [29, 31]) is that our algorithm can handle search spaces that do not necessarily include O. This distinction bears importance, particularly when considering scenarios where we can only query values within the constraint set. In such cases, $1/2$ has been conjectured to be the optimal approximation ratio ([28] section B in Appendix). We refer to Appendix H for motivation examples where searching over $\\mathcal{K}\\cup\\{\\mathbf{0}\\}$ is not applicable. ", "page_idx": 6}, {"type": "text", "text": "Now, we establish the regret and constraint violation achievable by our proposed Algorithm 1. Before delving into the main theorem, we first present three lemmas, which are adapted from [29] and are essential for achieving high probability bounds. Given the slight difference in the definition of $\\widehat{\\pmb{p}}$ ,we provide the proofs in Appendices A to $\\mathbf{C}$ respectively. First, Lemma 3 demonstrates that with high probability, the empirical estimate $\\widehat{\\pmb{p}}_{t}$ is close to its mean $\\textbf{\\emph{p}}$ ", "page_idx": 6}, {"type": "text", "text": "Lemma 3. The following holds with probability at least $1-\\varepsilon$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\|\\widehat{\\pmb{p}}_{t}-\\pmb{p}\\|\\leq Q\\beta\\sqrt{T\\log\\left(\\frac{2n T}{\\varepsilon}\\right)},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $Q>0$ is some universal constant. ", "page_idx": 6}, {"type": "text", "text": "Next, Lemma 4 establishes that with high probability, the $\\widehat g(\\cdot)$ computed using $\\widehat{\\pmb{p}}_{t}$ and $g(\\cdot)$ calculated using $\\pmb{p}$ are close. ", "page_idx": 6}, {"type": "text", "text": "Lemma 4.Let $\\textbf{x}\\in\\mathcal{K}$ be fixed.Fora fixed $t\\,\\in\\,[T]$ and $\\begin{array}{r}{\\left\\{\\gamma_{t}:=\\sqrt{\\frac{2}{t}C^{2}\\log\\left(\\frac{2T}{\\varepsilon}\\right)}\\right\\}_{t=1}^{T},\\;|\\widehat{g}_{t}(\\mathbf{x})-\\;}\\end{array}$ $g(\\mathbf{x})|\\leq\\gamma_{t}$ holdswithprobabilityat least $1-\\textstyle{\\frac{\\varepsilon}{T}}$ ", "page_idx": 6}, {"type": "text", "text": "Finally, Lemma 5 provides an upper bound for the total constraint violation. ", "page_idx": 6}, {"type": "text", "text": "Lemma 5. Let $\\{\\gamma_{t}\\}_{t=1}^{T}$ be defined as in Lemma $\\boldsymbol{4}$ then the following holds: ", "page_idx": 6}, {"type": "equation", "text": "$$\nC_{T}\\leq\\sum_{t=1}^{T}\\widetilde{g}_{t}\\left(\\mathbf{x}_{t}\\right)+r\\sum_{t=1}^{T}\\|\\widehat{\\pmb{p}}_{t}-\\pmb{p}\\|+\\sum_{t=1}^{T}\\gamma_{t}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Armed with these results, we can now establish regret and constraint violation bounds for Algorithm 1. ", "page_idx": 6}, {"type": "text", "text": "Theorem 1. Let Asumptions $^\\textit{\\textbf{l}234}$ be satisfied. Let $U\\,=\\,\\operatorname*{max}\\{G,C\\}$ Choose $\\begin{array}{r}{\\eta\\,=\\,\\frac{d}{U\\sqrt{T}}}\\end{array}$ and $\\delta\\,=\\,8\\beta^{2}$ . Let $\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{T}$ be the sequence of solutions obtained by Algorithm $^{\\,l}$ .When $T$ is suffciently large, i.e., $\\begin{array}{r}{T\\geq\\frac{64d^{2}\\beta^{2}}{U^{2}}}\\end{array}$ , we have the following $\\frac{1}{2}$ -regret and constraint violation bounds with probability at least $1-\\varepsilon$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}[R_{T}]=\\mathcal{O}(\\sqrt{T})\\,a n d\\,C_{T}=\\mathcal{O}(T^{3/4}).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The complete theorem statement and the complete proof are in Appendix D ", "page_idx": 7}, {"type": "text", "text": "Partial Proof: From the update of $\\mathbf{x}_{t}$ , we have that for any $\\mathbf{x}\\in{\\mathcal{K}}$ ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{x}_{t+1}-\\mathbf{x}\\|^{2}=\\|\\Pi_{K}(\\mathbf{x}_{t}+\\eta\\widetilde{\\nabla}_{x}\\widehat{\\mathcal{L}}_{t}(\\mathbf{x}_{t},\\lambda_{t}))-\\mathbf{x}\\|^{2}}\\\\ &{\\qquad\\qquad\\leq\\|\\mathbf{x}_{t}+\\eta\\widetilde{\\nabla}_{x}\\widehat{\\mathcal{L}}_{t}(\\mathbf{x}_{t},\\lambda_{t})-\\mathbf{x}\\|^{2}}\\\\ &{\\qquad\\qquad\\leq\\|\\mathbf{x}_{t}-\\mathbf{x}\\|^{2}+\\eta^{2}\\|\\widetilde{\\nabla}_{x}\\widehat{\\mathcal{L}}_{t}(\\mathbf{x}_{t},\\lambda_{t})\\|^{2}-2\\eta(\\mathbf{x}-\\mathbf{x}_{t})^{\\top}\\widetilde{\\nabla}_{x}\\widehat{\\mathcal{L}}_{t}(\\mathbf{x}_{t},\\lambda_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Rearranging, and using Assumption 4 and Assumption 3 we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n(\\mathbf{x}-\\mathbf{x}_{t})^{\\top}\\widetilde{\\nabla}_{x}\\widehat{\\mathcal{L}}_{t}(\\mathbf{x}_{t},\\lambda_{t})\\leq\\frac{1}{2\\eta}(\\|\\mathbf{x}_{t}-\\mathbf{x}\\|^{2}\\!-\\!\\|\\mathbf{x}_{t+1}-\\mathbf{x}\\|^{2})+\\eta G^{2}+4\\eta\\beta^{2}\\lambda_{t}^{2}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Applying similar steps on the $\\lambda$ updates, we establish ", "page_idx": 7}, {"type": "equation", "text": "$$\n(\\lambda-\\lambda_{t})^{\\top}\\nabla_{\\lambda}\\mathcal{L}_{t}({\\mathbf{x}_{t},\\lambda_{t}})\\geq-\\frac{1}{2\\eta}(\\|\\lambda_{t}-\\lambda\\|^{2}-\\|\\lambda_{t+1}-\\lambda\\|^{2})-C^{2}\\eta-2\\eta\\gamma_{t}^{2}-2\\delta^{2}\\eta^{3}\\lambda_{t}^{2}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "From monotonicity and DR-submodularity of $\\mathbb{E}[f_{t}(\\mathbf{x})]$ ,wehave ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\mathbb{E}[\\mathcal{L}_{t}(\\mathbf{x},\\lambda_{t})-2\\mathcal{L}_{t}(\\mathbf{x}_{t},\\lambda_{t})]}&\\\\ &{=\\mathbb{E}[\\mathbb{E}[\\mathcal{L}_{t}(\\mathbf{x},\\lambda_{t})-2\\mathcal{L}_{t}(\\mathbf{x}_{t},\\lambda_{t})|\\mathbf{x}_{t}]]}&\\\\ &{\\le\\mathbb{E}[(\\mathbf{x}-\\mathbf{x}_{t})^{\\top}\\nabla_{x}\\mathbb{E}[\\widehat{\\mathcal{L}}_{t}(\\mathbf{x}_{t},\\lambda_{t})|\\mathbf{x}_{t}]]+\\lambda_{t}\\widetilde{g}_{t}(\\mathbf{x})-\\displaystyle\\frac{\\delta\\eta}{2}\\lambda_{t}^{2}}&{(\\mathrm{Lemma~l})}\\\\ &{\\le\\displaystyle\\frac{1}{2\\eta}\\mathbb{E}[\\|\\mathbf{x}_{t}-\\mathbf{x}\\|^{2}-\\|\\mathbf{x}_{t+1}-\\mathbf{x}\\|^{2}]+G^{2}\\eta+4\\eta\\beta^{2}\\lambda_{t}^{2}+\\lambda_{t}\\widetilde{g}_{t}(\\mathbf{x})-\\displaystyle\\frac{\\delta\\eta}{2}\\lambda_{t}^{2},}&{(13)}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where (13) follows from (11). Similarly, from convexity of function $\\mathcal{L}_{t}({\\bf x},\\lambda)$ W.r.t $\\lambda$ wehave ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{t}(\\mathbf{x}_{t},\\lambda)-\\mathcal{L}_{t}(\\mathbf{x}_{t},\\lambda_{t})\\geq(\\lambda-\\lambda_{t})^{\\top}{\\nabla}_{\\lambda}\\mathcal{L}_{t}(\\mathbf{x}_{t},\\lambda_{t})}\\\\ &{\\qquad\\qquad\\qquad\\geq-\\frac{1}{2\\eta}(\\|\\lambda_{t}-\\lambda\\|^{2}-\\|\\lambda_{t+1}-\\lambda\\|^{2})-C^{2}\\eta-2\\eta\\gamma_{t}^{2}-2\\delta^{2}\\eta^{3}\\lambda_{t}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where (14) follows from (12). Subtracting two times (14) from (13), and sum $t$ over 1 through $T$ ,we get ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}[\\mathcal{L}_{t}(\\mathbf{x},\\lambda_{t})-2\\mathcal{L}_{t}(\\mathbf{x}_{t},\\lambda)]}\\\\ &{\\displaystyle\\leq\\frac{d^{2}}{2\\eta}+\\frac{\\lambda^{2}}{\\eta}+G^{2}\\eta T+\\eta\\beta^{2}\\sum_{t=1}^{T}\\lambda_{t}^{2}+2C^{2}\\eta T+4\\eta\\sum_{t=1}^{T}\\gamma_{t}^{2}+4\\delta^{2}\\eta^{3}\\sum_{t=1}^{T}\\lambda_{t}^{2}+\\lambda_{t}\\widetilde{g}_{t}(\\mathbf{x})-\\frac{\\delta\\eta}{2}\\lambda_{t}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Expanding the left hand side of (15) and rearranging, we deduce ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{t=1}^{T}[f(\\mathbf{x})-2f(\\mathbf{x}_{t})]+\\left[2\\lambda\\sum_{t=1}^{T}\\widetilde{g}_{t}(\\mathbf{x}_{t})-\\left(\\delta\\eta T+\\frac{1}{\\eta}\\right)\\lambda^{2}\\right]}\\\\ {\\displaystyle\\leq2\\sum_{t=1}^{T}\\lambda_{t}\\widetilde{g}_{t}(\\mathbf{x})+\\eta\\left(4\\beta^{2}+4\\delta^{2}\\eta^{2}-\\delta\\right)\\sum_{t=1}^{T}\\lambda_{t}^{2}+\\frac{d^{2}}{2\\eta}+G^{2}\\eta T+2C^{2}\\eta T+4\\eta\\sum_{t=1}^{T}\\gamma_{t}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "To ensure tat the equatio 432 + 482n2 - 8 = 0 has eal rots, we require T > 4 . Setting $\\delta=8\\beta^{2}$ ensures that $4\\beta^{2}+4\\delta^{2}\\eta^{2}-\\delta\\le0$ .Set $\\mathbf{x}=\\mathbf{x}^{*}$ ; From Lemma 4, with probability at least $\\begin{array}{r}{1-\\frac{\\varepsilon}{T}}\\end{array}$ \uff0c $\\widetilde{g}_{t}(\\mathbf{x}^{*})=\\widehat{g}_{t}(\\mathbf{x}^{*})-\\gamma_{t}\\leq g(\\mathbf{x}^{*})$ holds. since $\\mathbf{x}^{*}$ satisfies the long term constraint, we have $g(\\mathbf{x}^{*})\\leq0$ . Thus, we can drop the first two terms in the RHS of (16) and by union bound, we get with probability at least $1-\\varepsilon$ \uff0c ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{T}[f(\\mathbf{x}^{*})-2f(\\mathbf{x}_{t})]+\\left[2\\lambda\\sum_{t=1}^{T}\\widetilde{g}_{t}(\\mathbf{x}_{t})-\\left(\\delta\\eta T+\\frac{1}{\\eta}\\right)\\lambda^{2}\\right]\\leq\\frac{d^{2}}{2\\eta}+G^{2}\\eta T+2C^{2}\\eta T+4\\eta\\sum_{t=1}^{T}\\gamma_{t}^{2}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Maximizing the LHS of (17) with respect to $\\lambda$ over the range $\\lbrack0,+\\infty)$ , we get a solution of $\\lambda=$ [3(xo]  Pluging ths into (17) gives u s ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}[f(\\mathbf{x}^{*})-2f(\\mathbf{x}_{t})]+\\frac{\\left[\\sum_{t=1}^{T}\\widetilde{g}_{t}(\\mathbf{x}_{t})\\right]_{+}^{2}}{\\delta\\eta T+1/\\eta}\\leq\\frac{d^{2}}{2\\eta}+G^{2}\\eta T+2C^{2}\\eta T+4\\eta\\sum_{t=1}^{T}\\gamma_{t}^{2}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Plugging in $U=\\operatorname*{max}\\{G,C\\}$ and $\\begin{array}{r}{\\eta=\\frac{d}{U\\sqrt{T}}}\\end{array}$ we have with probability at least $1-\\varepsilon$ \uff0c ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}[f(\\mathbf{x}^{*})-2f(\\mathbf{x}_{t})]+\\frac{\\left[\\sum_{t=1}^{T}\\widetilde{g}_{t}(\\mathbf{x}_{t})\\right]_{+}^{2}}{\\delta\\eta T+1/\\eta}\\leq\\frac{7d U}{2}\\sqrt{T}+8d U\\log\\frac{2T}{\\varepsilon},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where we used the fact that $\\textstyle\\sum_{t=1}^{T}{\\frac{1}{\\sqrt{t}}}\\leq2{\\sqrt{T}}$ . This gives us our result on objective regret: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\left[{\\frac{1}{2}}f(\\mathbf{x}^{*})-f(\\mathbf{x}_{t})\\right]={\\mathcal{O}}({\\sqrt{T}}).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The detailed proof, including constraint violation steps, are provided in Appendix D. ", "page_idx": 8}, {"type": "text", "text": "6  First Order Full-information Case ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we introduce our second proposed algorithm for online DR-submodular maximization subject to stochastic long-term constraints under the first-order full-information feedback. In the interest of space, the algorithm is presented in Algorithm 2 in the Appendix. The overall structure of the algorithm is similar to Algorithm 1, but the primal update uses the gradient of the non-oblivious function: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{\\nabla}_{x}\\widehat{\\mathcal{L}}_{t}(\\mathbf{x}_{t},\\lambda_{t})=\\widetilde{\\nabla}F_{t}(\\mathbf{x}_{t})-\\lambda_{t}{\\nabla}\\widetilde{g}_{t}(\\mathbf{x}_{t}),\\mathrm{and}}\\\\ &{\\quad\\quad\\quad\\mathbf{x}_{t+1}=\\Pi_{K}(\\mathbf{x}_{t}+\\eta\\widetilde{\\nabla}_{x}\\widehat{\\mathcal{L}}_{t}(\\mathbf{x}_{t},\\lambda_{t})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where the non-oblivious function $F$ is defined by its gradient: $\\begin{array}{r}{\\nabla F(\\mathbf{x})\\,=\\,\\int_{0}^{1}e^{z-1}\\nabla f(z\\cdot\\mathbf{x})d z}\\end{array}$ As we discussed in Section 3, the non-oblivious function $F$ plays an important role in obtaining the optimal approximation ratio $1-1/e$ . However, calculating the gradient of the non-oblivious function $F(\\mathbf{x})$ can be challenging, especially when only unbiased estimates of the gradients are available. To overcome this, [42] presents a computational approach for obtaining an unbiased estimate of the gradient of $F(\\mathbf{x})$ through sampling (Lines 6 and 7). The following lemma indicates that $(1-1/e)\\widetilde{\\nabla}f_{t}(z*\\mathbf{x})$ is an unbiased estimator of $\\nabla F(\\mathbf{x})$ with bounded variance. ", "page_idx": 8}, {"type": "text", "text": "Lemma 6. If $z$ is sampled from rv. $\\mathbf{Z}$ as in line $^{6}$ of Algorithm 2, $\\mathbb{E}[\\widetilde{\\nabla}f_{t}(\\mathbf{x})\\mid\\mathbf{x}]=\\nabla f(\\mathbf{x})$ and $\\mathbb{E}\\left[\\|\\widetilde{\\nabla}f_{t}(\\mathbf{x})-\\nabla f(\\mathbf{x})\\|^{2}\\mid\\mathbf{x}\\right]\\le\\sigma^{2}$ we have $\\begin{array}{r l}&{i)\\,\\mathbb{E}\\left[(1-1/e)\\widetilde{\\nabla}f_{t}(z*\\mathbf{x})|\\,\\mathbf{x}\\right]=\\nabla F(\\mathbf{x});}\\\\ &{i i)\\,\\mathbb{E}\\left[\\left\\|(1-1/e)\\widetilde{\\nabla}f_{t}(z*\\mathbf{x})-\\nabla F(\\mathbf{x})\\right\\|^{2}\\right]\\,\\mathbf{x}\\right]\\le\\sigma_{1}^{2},\\,w h e r e\\,\\sigma_{1}^{2}=2\\left(1-1/e\\right)^{2}\\sigma^{2}+\\frac{2\\beta^{2}\\bar{r}^{2}(1-1/e)}{3}.}\\end{array}$ ", "page_idx": 8}, {"type": "text", "text": "With the unbiased estimator of the gradient of the non-oblivious function, we show the following regret and constraint violation guarantee for our Algorithm 2 in Appendix $\\boldsymbol{\\mathrm E}$ ", "page_idx": 8}, {"type": "text", "text": "Theore 2. LetAssumptons $1\\,2\\,3\\,4$ $U=\\operatorname*{max}\\{G_{F},C\\}$ .Choosing $\\begin{array}{r}{\\eta=\\frac{d}{U\\sqrt{T}}}\\end{array}$ and $\\delta=4\\beta^{2}$ . Let $\\mathbf{x}_{t}$ $t\\in[T]$ be the sequence of solutions obtained by Algorithm 2. When $T$ is sufficiently large, ie,T > 16\u03b2, ,we have the following $(1-1/e)$ -regret and constraint violation bounds with probability at least $1-\\varepsilon$ ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbb{E}[R_{T}]=\\mathcal{O}(\\sqrt{T})\\,a n d\\,C_{T}=\\mathcal{O}(T^{3/4}).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "7 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we address the problem of stochastic DR-submodular maximization with stochastic long-term constraints over a general convex set. We introduce the first algorithm for this setting, attaining $\\mathcal{O}(\\sqrt{T})$ regret and ${\\mathcal{O}}(T^{3/4})$ constraint violation bounds. Notably, our algorithm operates in both the semi-bandit feedback and first-order full-information setting, requiring only 1 gradient query per round, while all previous works operate in the full-information setting with $\\sqrt{T}$ gradient queries per round. Extension of the results here to upper-linearizable functions in [27] is an open direction. ", "page_idx": 9}, {"type": "text", "text": "8 Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported in part by the National Science Foundation under grants CCF-2149588 and CCF-2149617. We acknowledge Yiyang (Roy) Lu for helpful feedback. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1]  S. Agrawal and N. R. Devanur. Bandits with concave rewards and convex knapsacks. Proceedings of the fteenth ACM conference on Economics and computation, 2014.   \n[2] F. R. Bach. Submodular functions: from discrete to continuous domains. Mathematical Programming, 175:419 - 459, 2015.   \n[3] A. Badanidiyuru, R. D. Kleinberg, and A. Slivkins. Bandits with knapsacks. 2013 IEEE 54th Annual Symposium on Foundations of Computer Science, pages 207-216, 2013.   \n[4]  S. Balseiro and Y. Gur. Learning in repeated auctions with budgets: Regret minimization and equilibrium. Management Science, 65:3952-3968, 09 2019. doi: 10.1287/mnsc.2018.3174.   \n[5] A. A. Bian, B. Mirzasoleiman, J. Buhmann, and A. Krause. Guaranteed Non-convex Optimization: Submodular Maximization over Continuous Domains. In AISTATS, volume 54, pages 111-120. PMLR, 20-22 Apr 2017.   \n[6]  Y. Bian, J. M. Buhmann, and A. Krause. Continuous submodular function maximization. ArXiv, abs/2006.13474,2020.   \n[7] L. Chen, C. Harshaw, H. Hassani, and A. Karbasi. Projection-free online optimization with stochastic gradient: From convexity to submodularity. In J. Dy and A. Krause, editors, Proceedings of the 35th International Conference on Machine Learning,volume 80 of Proceedings of Machine Learning Research, pages 814-823. PMLR, 10-15 Jul 2018.   \n[8] L. Chen, C. Harshaw, H. Hassani, and A. Karbasi. Projection-free online optimization with stochastic gradient: From convexity to submodularity. In J. Dy and A. Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 814-823. PMLR, 10-15 Jul 2018.   \n[9] L. Chen, H. Hassani, and A. Karbasi. Online continuous submodular maximization. In A. Storkey and F. Perez-Cruz, editors, Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics,volume84 of Proceedings of MachineLearning Research, pages 1896-1905. PMLR, 09-11 Apr 2018.   \n[10] J. Feng, R. Yang, Y. Zhang, and Z. Zhang. Online weakly dr-submodular optimization with stochastic long-term constraints. In D.-Z. Du, D. Du, C. Wu, and D. Xu, editors, Theory and Applications of Models of Computation, pages 32-42, Cham, 2022. Springer International Publishing. ISBN 978-3-031-20350-3.   \n[11]  C. Guestrin, A. Krause, and A. P. Singh. Near-optimal sensor placements in gaussian processes. Proceedings of the 22nd international conference on Machine learning, 2005.   \n[12] H. Hassani, M. Soltanolkotabi, and A. Karbasi. Gradient methods for submodular maximization. In Neural Information Processing Systems, 2017.   \n[13] E. Hazan. Introduction to Online Convex Optimization. Foundations and Trends in Optimization. Now, Boston, 2016. ISBN 978-1-68083-170-2. doi: 10.1561/2400000013.   \n[14]  R. Jenatton, J. Huang, and C. Archambeau. Adaptive algorithms for online convex optimization with long-term constraints. In M. F. Balcan and K. Q. Weinberger, editors, Proceedings of The 33rd International Conference onMachine Learning,volume48of Proceedings of Machine Learning Research, pages 402-411, New York, New York, USA, 20-22 Jun 2016. PMLR.   \n[15] C. Jin, P. Netrapalli, R. Ge, S. M. Kakade, and M. I. Jordan. A short note on concentration inequalities for random vectors with subgaussian norm, 2019.   \n[16] D. Kempe, J. M. Kleinberg, and E. Tardos. Maximizing the spread of infuence through a social network. In Knowledge Discovery and Data Mining, 2003.   \n[17]  A. Krause and D. Golovin. Submodular function maximization. In Tractability, 2014.   \n[18]  N. Liakopoulos, A. Destounis, G. Paschos, T. Spyropoulos, and P. Mertikopoulos. Cautious regret minimization: Online optimization with long-term budget constraints. In K. Chaudhuri and R. Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 3944-3952. PMLR, 09-15 Jun 2019.   \n[19]  T. Lin, J. Li, and W. Chen. Stochastic online gredy learning with semi-bandit feedbacks. In Proceedings of the 29th International Conference on Neural Information Processing Systems, pages 352-360, 2015.   \n[20] M. Mahdavi, R. Jin, and T. Yang. Trading regret for efficiency: Online convex optimization with long term constraints. Journal of Machine Learning Research, 13(81):2503-2528, 2012.   \n[21] S. Mannor, J. N. Tsitsiklis, and J. Y. Yu. Online learning with sample path constraints. Journal of Machine Learning Research, 10:569-590, 2009.   \n[22]  B. Mirzasoleiman, A. Karbasi, R. Sarkar, and A. Krause. Distributed submodular maximization: Identifying representative elements in massive data. In C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Weinberger, editors, Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc., 2013.   \n[23]  B. Mirzasoleiman, A. Badanidiyuru, and A. Karbasi. Fast constrained submodular maximization: Personalized data summarization. In International Conference on Machine Learning, 2016.   \n[24] R. Niazadeh, N. Golrezaei, J. R. Wang, F. Susan, and A. Badanidiyuru. Online leaming via offine greedy algorithms: Applications in market design and optimization. In Proceedings of the 22nd ACM Conference on Economics and Computation, pages 737-738, 2021.   \n[25]  G. Nie, M. Agarwal, A. K. Umrawal, V. Aggarwal, and C. J. Quinn. An explore-then-commit algorithm for submodular maximization under full-bandit feedback. In Uncertainty in Artificial Intelligence, pages 1541-1551. PMLR, 2022.   \n[26]  G. Nie, Y. Y. Nadew, Y. Zhu, V. Aggarwal, and C. J. Quinn. A framework for adapting offline algorithms to solve combinatorial multi-armed bandit problems with bandit feedback. In A.KrauEruskill,KCho,Enghardt, Sabat, and J.calettditor, rocen of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 26166-26198. PMLR, 23-29 Jul 2023.   \n[27]  M. Pedramfar and V. Aggarwal. From linear to linearizable optimization: A novel framework with applications to stationary and non-stationary dr-submodular optimization. In Thirty-eighth Conference on Neural Information Processing Systems, 2024.   \n[28]  M. Pedramfar, C. J. Quinn, and V. Aggarwal. A unified approach for maximizing continuous DR-submodular functions. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[29] P. S. Raut, O. Sadeghi, and M.Fazel. Online dr-submodular maximization: Minimizing regret and constraint violation. In AAAI Conference on Artificial Intelligence, 2021.   \n[30]  O. Sadeghi and M. Fazel. Online continuous dr-submodular maximization with long-term budget constraints. In S. Chiappa and R. Calandra, editors, Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, volume 108 of Proceedings of Machine Learning Research, pages 4410-4419. PMLR, 26-28 Aug 2020.   \n[31] O. Sadeghi, P. Raut, and M. Fazel. A single recipe for online submodular maximization with adversarial or stochastic constraints. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 14712-14723. Curran Associates, Inc., 2020.   \n[32] T. Soma, N. Kakimura, K. Inaba, and K. ichi Kawarabayashi. Optimal budget allocation: Theoretical guarantee and efficient algorithm. In International Conference on Machine Learning, 2014.   \n[33]  M. J. Streeter and D. Golovin. An online algorithm for maximizing submodular functions. In Neural Information Processing Systems, 2008.   \n[34]  J. Vondrak. Submodularity in Combinatorial Optimization. Phd thesis, Charles University, 2007.   \n[35] X. Wei, H. Yu, and M. J. Neely. Online primal-dual mirror descent under stochastic constraints. Proceedings of the ACM on Measurement and Analysis of Computing Systems, 4:1 - 36, 2019.   \n[36]  L. A. Wolsey. An analysis of the greedy algorithm for the submodular set covering problem. Combinatorica, 2:385-393, 1982.   \n[37] X. Yi, X. Li, T. Yang, L. Xie, T. Chai, and K. Johansson. Regret and cumulative constraint violation analysis for online convex optimization with long term constraints. In M. Meila and T. Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 11998-12008. PMLR, 18-24 Jul 2021.   \n[38]  H. Yu and M. J. Neely. A low complexity algorithm with $o(\\sqrt{T})$ regret and o(1) constraint violations for online convex optimization with long term constraints. Journal of Machine Learning Research, 21(1):1-24, 2020.   \n[39] H. Yu, M. J. Neely, and X. Wei. Online convex optimization with stochastic constraints. In Neural Information Processing Systems, 2017.   \n[40]  J. Yuan and A. Lamperski. Online convex optimization for cumulative constraints. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.   \n[41] M. Zhang, L. Chen, H. Hassani, and A. Karbasi. Online continuous submodular maximization: From full-information to bandit feedback. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.   \n[42] Q. Zhang, Z. Deng, Z. Chen, H. Hu, and Y. Yang. Stochastic continuous submodular maximization: Boosting via non-oblivious function. In International Conference on Machine Learning, 2022.   \n[43]  M. A. Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In International Conference on Machine Learning, 2003. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Proof of Lemma 3 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Before proceeding to the proof, we first show a lemma: ", "page_idx": 12}, {"type": "text", "text": "Lemma 7. For all $t\\in[T]$ , the following holds: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbb{P}\\{\\|\\pmb{p}_{t}-\\pmb{p}\\|\\ge\\zeta\\}\\le2e^{-\\frac{\\zeta^{2}}{2\\rho^{2}}},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\rho=c\\beta$ for some universal constant $c>0$ ", "page_idx": 12}, {"type": "text", "text": "Proof. From Assumption 1, we have $\\|\\pmb{\\mathscr{p}}_{t}\\|\\leq\\beta$ . We can apply Lemma 1 of [15] with sub-Gaussian parameter $\\rho=c\\beta$ for some universal constant $c>0$ and the result follows immediately. \u53e3 ", "page_idx": 12}, {"type": "text", "text": "Then we proceed to prove Lemma 3. We restate the lemma as follows: ", "page_idx": 12}, {"type": "text", "text": "Lemma 8. The following holds with probability at least $1-\\varepsilon$ ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\|\\widehat{\\pmb{p}}_{t}-\\pmb{p}\\|\\leq Q\\beta\\sqrt{T\\log\\left(\\frac{2n T}{\\varepsilon}\\right)},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $Q>0$ is some universal constant. ", "page_idx": 12}, {"type": "text", "text": "Proof. From the definition of $\\widehat{\\pmb{p}}_{t}$ , we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\|\\widehat{\\pmb{p}}_{t}-\\pmb{p}\\|=\\frac{1}{t}\\|\\sum_{s=1}^{t}(\\pmb{p}_{s}-\\pmb{p})\\|.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Combine $\\mathbb{E}[p_{t}\\mathrm{~-~}p]\\,=\\,0$ and Lemma 7, we can apply Corollary 7 of [15] to the random vector $\\{p_{s}-p\\}_{s=1}^{t}$ and obtainwith probabity a last $\\begin{array}{r}{1-\\frac{\\varepsilon}{T}}\\end{array}$ ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\|\\sum_{s=1}^{t}(\\pmb{p}_{s}-\\pmb{p})\\|\\leq c^{\\prime}\\sqrt{\\sum_{s=1}^{t}\\rho^{2}\\log(\\frac{2d T}{\\varepsilon})}=c^{\\prime}\\rho\\sqrt{t\\log(\\frac{2d T}{\\varepsilon})},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $c^{\\prime}>0$ is some universal constant. Combining (21) and (22) and apply union bound, we have with probability at least 1 \uff0d , ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\displaystyle\\sum_{t=1}^{T}\\|\\widehat{\\boldsymbol{p}}_{t}-\\boldsymbol{p}\\|=\\displaystyle\\sum_{t=1}^{T}\\frac{1}{t}\\|\\displaystyle\\sum_{s=1}^{t}(\\boldsymbol{p}_{s}-\\boldsymbol{p})\\|}\\\\ {\\le\\displaystyle\\sum_{t=1}^{T}c^{\\prime}\\rho\\sqrt{\\log(\\frac{2d T}{\\varepsilon})/t}}\\\\ {\\displaystyle\\le\\sum_{t=1}^{T}c^{\\prime}\\rho\\sqrt{T\\log(\\frac{2d T}{\\varepsilon})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where the last inequality follows from $\\textstyle\\sum_{t=1}^{T}{\\frac{1}{\\sqrt{t}}}\\;\\leq\\;2{\\sqrt{T}}$ .We get the desired result be taking $Q=2c c^{\\prime}$ \u53e3 ", "page_idx": 12}, {"type": "text", "text": "BProof of Lemma 4 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We restate the lemma as follows: ", "page_idx": 12}, {"type": "text", "text": "Lemma 9. Let $\\textbf{x}\\in\\mathcal{K}$ be fired. For a fied $t\\,\\in\\,[T]$ and $\\begin{array}{r}{\\left\\{\\gamma_{t}:=\\sqrt{\\frac{2}{t}C^{2}\\log\\left(\\frac{2T}{\\varepsilon}\\right)}\\right\\}_{t=1}^{T},\\;|\\widehat{g}_{t}(\\mathbf{x})-\\;}\\end{array}$ $g(\\mathbf{x})|\\leq\\gamma_{t}$ holds with probability a least $\\begin{array}{r}{1-\\frac{\\varepsilon}{T}}\\end{array}$ ", "page_idx": 12}, {"type": "text", "text": "Proof. Recall from the definition that $\\begin{array}{r}{\\widetilde{g}_{t}(\\mathbf{x})\\;=\\;\\langle\\widehat{p}_{t},\\mathbf{x}\\rangle\\,-\\,\\frac{B_{T}}{T}\\,-\\,\\gamma_{t}}\\end{array}$ and $\\begin{array}{r}{\\widehat{g}_{t}(\\mathbf{x})\\;:=\\;\\langle\\widehat{p}_{t},\\mathbf{x}\\rangle\\,-\\,\\frac{B_{T}}{T}}\\end{array}$ Note that $\\begin{array}{r}{\\mathbb{E}[\\widehat{g}_{t}(\\mathbf{x})]\\;=\\;\\mathbb{E}[\\langle\\widehat{p}_{t},\\mathbf{x}\\rangle\\,-\\,\\frac{B_{T}}{T}]\\;=\\;\\langle p,\\mathbf{x}\\rangle\\,-\\,\\frac{B_{T}}{T}\\;=\\;g(\\mathbf{x}).}\\end{array}$ Recall that we have defined $\\begin{array}{r}{C:=\\operatorname*{max}_{\\pmb{p^{\\prime}}\\sim\\mathcal{D}_{p}}\\operatorname*{max}_{\\mathbf{x}\\in\\mathcal{K}}|\\langle\\pmb{p^{\\prime}},\\mathbf{x}\\rangle-\\frac{B_{T}}{T}|}\\end{array}$ Thus, we have $\\widehat{g}_{t}(\\mathbf{x})$ is bounded within interval $[-C,C]$ Apply Hoeffding's inequality on $\\widehat g_{t}(\\mathbf x)$ , we get ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left\\{|\\widehat{g}_{t}(x)-g(x)|>\\gamma_{t}\\right\\}\\leq2\\exp\\left(-\\frac{t\\gamma_{t}^{2}}{2C^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Substituting the value of $\\gamma_{t}$ in the right hand side, we get that $\\begin{array}{r}{\\mathbb{P}\\left\\{|\\widehat{g}_{t}(x)-g(x)|>\\gamma_{t}\\right\\}\\le\\frac{\\epsilon}{T}}\\end{array}$ \u53e3 ", "page_idx": 13}, {"type": "text", "text": "CProof of Lemma 5 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We restate the lemma as follows: ", "page_idx": 13}, {"type": "text", "text": "Lemma 10. Let $\\{\\gamma_{t}\\}_{t=1}^{T}$ be defined as in Lemma $^{4}$ then the following holds: ", "page_idx": 13}, {"type": "equation", "text": "$$\nC_{T}\\leq\\sum_{t=1}^{T}\\widetilde{g}_{t}\\left(\\mathbf{x}_{t}\\right)+r\\sum_{t=1}^{T}\\|\\widehat{\\pmb{p}}_{t}-\\pmb{p}\\|+\\sum_{t=1}^{T}\\gamma_{t}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. Bounding $\\begin{array}{r}{\\sum_{t=1}^{T}\\widetilde{g}_{t}(\\mathbf{x}_{t})}\\end{array}$ from below, we obtain: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{t=1}^{T}\\widetilde{\\phi}_{t}(\\mathbf{x}_{t})=\\displaystyle\\sum_{t=1}^{T}g(\\mathbf{x}_{t})+\\displaystyle\\sum_{t=1}^{T}\\left(\\widehat{g}_{t}\\left(\\mathbf{x}_{t}\\right)-g(\\mathbf{x}_{t})\\right)-\\displaystyle\\sum_{t=1}^{T}\\gamma_{t}}\\\\ {\\displaystyle\\geq\\displaystyle\\sum_{t=1}^{T}g\\left(\\mathbf{x}_{t}\\right)-\\displaystyle\\sum_{t=1}^{T}\\left[\\widehat{\\phi}_{t}\\left(\\mathbf{x}_{t}\\right)-g\\left(\\mathbf{x}_{t}\\right)\\right]-\\displaystyle\\sum_{t=1}^{T}\\gamma_{t}}\\\\ {\\displaystyle=C_{T}-\\displaystyle\\sum_{t=1}^{T}\\left[\\widehat{\\phi}_{t}-p,\\mathbf{x}_{t}\\right]-\\displaystyle\\sum_{t=1}^{T}\\gamma_{t}}\\\\ {\\displaystyle\\geq C_{T}-\\displaystyle\\sum_{t=1}^{T}\\lVert\\widehat{p}_{t}-p\\rVert\\left\\lVert\\mathbf{x}_{t}\\right\\rVert-\\displaystyle\\sum_{t=1}^{T}\\gamma_{t}}\\\\ {\\displaystyle\\geq C_{T}-r\\displaystyle\\sum_{t=1}^{T}\\lVert\\widehat{p}_{t}-p\\rVert-\\displaystyle\\sum_{t=1}^{T}\\gamma_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Rearranging the above inequality, we obtain the desired result ", "page_idx": 13}, {"type": "text", "text": "D Proof of Theorem 1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We restate our theorem as follows: ", "page_idx": 13}, {"type": "text", "text": "Theorem 3. Let Assumptions 1 2 3 4 be satisfed. Let U = max{G,C}. Choose n = UT and $\\delta=8\\beta^{2}$ . Let $\\mathbf{x}_{t}$ $t\\,\\in\\,[T]$ be the sequence of solutions obtained by Algorithm $^{\\,l}$ . When $T$ is suffciently large,i.e,. $\\begin{array}{r}{T\\ge\\frac{\\bar{6}4\\bar{d}^{2}\\beta^{2}}{U^{2}}}\\end{array}$ we have the following $\\frac{1}{2}$ -regret and constraint violation bounds with probability at least $1-\\varepsilon$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}[R_{T}]\\leq\\sum_{t=1}^{T}\\left[{\\frac{1}{2}}f(\\mathbf{x}^{*})-f(\\mathbf{x}_{t})\\right]\\leq{\\frac{7d U}{4}}{\\sqrt{T}}+8d U\\log{\\frac{2T}{\\varepsilon}}={\\mathcal{O}}(T^{1/2})\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C_{T}\\leq\\sqrt{\\bigg(\\frac{7d U}{4}\\sqrt{T}+4d U\\log\\frac{2T}{\\varepsilon}+(F_{1}+F_{2})T\\bigg)\\cdot\\bigg(\\frac{8\\beta^{2}d}{U}+\\frac{U}{d}\\bigg)\\;\\sqrt{T}}}\\\\ &{\\quad\\quad\\quad\\quad+\\;r Q\\sigma\\sqrt{T\\log\\bigg(\\frac{2n T}{\\varepsilon}\\bigg)}+2\\sqrt{2T C^{2}\\log\\bigg(\\frac{2T}{\\varepsilon}\\bigg)}}\\\\ &{\\quad\\quad=\\mathcal{O}(T^{3/4}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. From the update of $\\mathbf{x}_{t}$ , we have that for any $\\mathbf{x}\\in{\\mathcal{K}}$ \uff0c ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\|\\mathbf{x}_{t+1}-\\mathbf{x}\\|^{2}=\\|\\Pi_{K}(\\mathbf{x}_{t}+\\eta\\widetilde{\\nabla}_{x}\\widehat{\\mathcal{L}}_{t}(\\mathbf{x}_{t},\\lambda_{t}))-\\mathbf{x}\\|^{2}}}\\\\ &{}&{\\leq\\|\\mathbf{x}_{t}+\\eta\\widetilde{\\nabla}_{x}\\widehat{\\mathcal{L}}_{t}(\\mathbf{x}_{t},\\lambda_{t})-\\mathbf{x}\\|^{2}\\qquad\\qquad\\qquad\\qquad\\quad(\\mathrm{def.~of~prosit.})}\\\\ &{}&{=\\|\\mathbf{x}_{t}-\\mathbf{x}\\|^{2}+\\eta^{2}\\|\\widetilde{\\nabla}_{x}\\widehat{\\mathcal{L}}_{t}(\\mathbf{x}_{t},\\lambda_{t})\\|^{2}-2\\eta(\\mathbf{x}-\\mathbf{x}_{t})^{\\top}\\widetilde{\\nabla}_{x}\\widehat{\\mathcal{L}}_{t}(\\mathbf{x}_{t},\\lambda_{t}).\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Rearranging, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\quad}&{(\\mathbf x-\\mathbf x_{t})^{\\top}\\widetilde\\nabla_{x}\\widehat Z_{t}(\\mathbf x_{t},\\lambda_{t})\\leq\\displaystyle\\frac1{2\\eta}(\\|\\mathbf x_{t}-\\mathbf x\\|^{2}-\\|\\mathbf x_{t+1}-\\mathbf x\\|^{2})+\\displaystyle\\frac{\\eta}{2}\\|\\widetilde\\nabla_{x}\\widehat Z_{t}(\\mathbf x_{t},\\lambda_{t})\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\frac{1}{2\\eta}(\\|\\mathbf x_{t}-\\mathbf x\\|^{2}-\\|\\mathbf x_{t+1}-\\mathbf x\\|^{2})+\\displaystyle\\frac{\\eta}{2}\\|\\widetilde\\nabla f(\\mathbf x_{t})-2\\lambda_{t}\\nabla\\widetilde g_{t}(x_{t})\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\mathrm{(from~}(5))}\\\\ &{\\leq\\displaystyle\\frac{1}{2\\eta}(\\|\\mathbf x_{t}-\\mathbf x\\|^{2}-\\|\\mathbf x_{t+1}-\\mathbf x\\|^{2})+\\eta\\|\\widetilde\\nabla f(\\mathbf x_{t})\\|^{2}+4\\eta\\lambda_{t}^{2}\\|\\nabla\\widetilde g_{t}(x_{t})\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\mathrm{(}\\|a+b\\|^{2}\\leq2\\|a\\|^{2}+2\\|b\\|^{2})}\\\\ &{\\leq\\displaystyle\\frac{1}{2\\eta}(\\|\\mathbf x_{t}-\\mathbf x\\|^{2}-\\|\\mathbf x_{t+1}-\\mathbf x\\|^{2})+\\eta G^{2}+4\\eta\\beta^{2}\\lambda_{t}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where (27) follows from Assumption 4 and Assumption 3. Taking expectation with respect to $f_{t}$ ,we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[G_{t}(\\mathbf{x}_{k},\\lambda)-2\\mathcal{L}(\\mathbf{x}_{k},\\lambda)]}\\\\ &{\\quad=\\mathbb{E}[\\mathcal{L}[G_{t}(\\mathbf{x},\\lambda)-2\\mathcal{L}(\\mathbf{x}_{k},0)]\\mathbf{x}_{k}]\\mathbb{H}[\\mathbf{\\Sigma}}\\\\ &{\\quad=\\mathbb{E}[\\mathcal{L}[f(\\mathbf{x})-\\lambda\\overline{{g}}(\\mathbf{y})+\\frac{\\delta}{2}\\mathcal{X}_{j}-2\\left(f(\\mathbf{x})-\\lambda\\overline{{g}}(\\mathbf{y})\\mathbf{x}_{k}\\right)+\\frac{\\delta}{2}\\mathcal{X}_{j}]\\mathbf{y}_{k}]}\\\\ &{\\quad=\\mathbb{E}[\\mathcal{L}[f(\\mathbf{x})-2\\widehat{g}(\\mathbf{x})-2\\mathcal{L}(\\mathbf{x})(\\widehat{g},\\lambda)]\\mathbf{x}_{k}]+\\lambda\\widehat{g}_{t}(\\mathbf{x})(\\widehat{y}-\\widehat{g})\\mathbf{y}_{k}^{\\top}]}\\\\ &{\\quad=\\mathbb{E}[\\mathcal{L}[f(\\mathbf{x})-2\\widehat{f}(\\mathbf{y})-2\\lambda(\\widehat{g}(\\mathbf{x})-\\widehat{g}(\\mathbf{x}))]\\mathbf{x}_{k}]+\\lambda\\widehat{g}_{t}(\\mathbf{y})(-\\widehat{g})\\mathbf{z}_{k}^{\\top}}\\\\ &{\\quad\\leq\\mathbb{E}[(\\mathbf{x}-\\mathbf{x}_{k})^{\\top}\\nabla_{\\mathbb{D}}\\mathbb{E}[f(\\mathbf{x})-2\\lambda(\\widehat{g}(\\mathbf{x})-\\widehat{g}(\\mathbf{x}))]\\mathbf{x}_{k}]+\\lambda\\widehat{g}_{t}(\\mathbf{x})-\\frac{\\delta}{2}\\lambda_{j}^{2}}\\\\ &{\\quad\\quad=\\mathbb{E}[(\\mathbf{x}-\\mathbf{x}_{k})^{\\top}\\nabla_{\\mathbb{D}}\\mathbb{E}[f(\\mathbf{x})-2\\lambda_{k}(\\mathbf{x}-\\mathbf{x}_{k})^{\\top}\\nabla_{\\mathbb{D}}\\widehat{g}(\\mathbf{x})]\\mathbf{x}_{k}]+\\lambda\\widehat{g}_{t}(\\mathbf{x})-\\frac{\\delta}{2}\\mathcal{L}_{j}^{2}}\\\\ &{\\quad\\quad\\quad=\\mathbb{E}[(\\mathbf{x}-\\mathbf{x}_{k})^{\\top}\\nabla_{\\mathbb{D}}\\mathbb{E}[f(\\mathbf{x})-2\\lambda_{k}(\\mathbf{x\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where (28) follows from (27). From the update of $\\lambda_{t}$ (8), we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\|\\lambda_{t+1}-\\lambda\\|^{2}=\\|\\Pi_{[0,+\\infty)}(\\lambda_{t}-\\eta\\nabla_{\\lambda}\\mathcal{L}_{t}(\\mathbf{x}_{t},\\lambda_{t}))-\\lambda\\|^{2}}\\qquad}&{}&{{\\mathrm{(from~}(8))}}\\\\ &{}&{\\leq\\|\\lambda_{t}-\\eta\\nabla_{\\lambda}\\mathcal{L}_{t}(\\mathbf{x}_{t},\\lambda_{t})-\\lambda\\|^{2}\\qquad}&{\\mathrm{(def.~of~projection)}}\\\\ &{}&{=\\|\\lambda-\\lambda_{t}\\|^{2}+\\eta^{2}\\|\\nabla_{\\lambda}\\mathcal{L}_{t}(\\mathbf{x}_{t},\\lambda_{t})\\|^{2}+2\\eta(\\lambda-\\lambda_{t})^{\\top}\\nabla_{\\lambda}\\mathcal{L}_{t}(\\mathbf{x}_{t},\\lambda_{t}).\\qquad(29)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Rearranging, ", "page_idx": 15}, {"type": "text", "text": "6) ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{(\\lambda-\\lambda_{t})^{\\top}\\nabla_{\\lambda}\\mathcal{L}_{t}(\\mathbf x_{t},\\lambda_{t})\\geq-\\frac{1}{2\\eta}(\\|\\lambda_{t}-\\lambda\\|^{2}-\\|\\lambda_{t+1}-\\lambda\\|^{2})-\\frac{\\eta}{2}\\|\\nabla_{\\lambda}\\mathcal{L}_{t}(\\mathbf x_{t},\\lambda_{t})\\|^{2}}&{{}}\\\\ {=-\\frac{1}{2\\eta}(\\|\\lambda_{t}-\\lambda\\|^{2}-\\|\\lambda_{t+1}-\\lambda\\|^{2})-\\frac{\\eta}{2}\\|-\\tilde{\\mathcal{G}}_{t}(\\mathbf x_{t})+\\delta\\eta\\lambda_{t}\\|^{2}}&{{}}\\\\ {(\\mathrm{from~}(}\\\\ {}&{}\\\\ {}&{=-\\frac{1}{2\\eta}(\\|\\lambda_{t}-\\lambda\\|^{2}-\\|\\lambda_{t+1}-\\lambda\\|^{2})-\\frac{\\eta}{2}\\|-\\hat{\\mathcal{G}}_{t}(\\mathbf x_{t})-\\gamma_{t}+\\delta\\eta\\lambda_{t}\\|^{2}}\\\\ {}&{}&{\\mathrm{(by~def.~}}\\\\ {}&{}\\\\ {}&{\\geq-\\frac{1}{2\\eta}(\\|\\lambda_{t}-\\lambda\\|^{2}-\\|\\lambda_{t+1}-\\lambda\\|^{2})-\\eta\\|\\hat{\\mathcal{G}}_{t}(\\mathbf x_{t})\\|^{2}-2\\hat{\\mathcal{G}}^{2}\\eta^{3}\\lambda_{t}^{1}}&{{}}\\\\ {}&{{}(\\mathrm{apply~}\\|\\lambda+b\\|^{2}\\leq2\\|a\\|^{2}+2\\|b\\|^{2}\\cup\\mathrm{wir}}\\\\ {}&{}&{\\geq-\\frac{1}{2\\eta}(\\|\\lambda_{t}-\\lambda\\|^{2}-\\|\\lambda_{t+1}-\\lambda\\|^{2})-C^{2}\\eta-2\\eta\\gamma_{t}^{2}-2\\hat{\\mathcal{G}}^{2}\\eta^{3}\\lambda_{t}^{1},\\quad\\mathrm{(c)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the last inequality follows from the definition of $\\begin{array}{r}{C:=\\operatorname*{max}_{\\pmb{p^{\\prime}}\\sim\\mathcal{D}_{p}}\\operatorname*{max}_{\\mathbf{x}\\in\\mathcal{K}}|\\langle\\pmb{p^{\\prime}},\\mathbf{x}\\rangle-\\frac{B_{T}}{T}|}\\end{array}$ and $\\begin{array}{r}{\\widehat{g}_{t}(\\mathbf{x}):=\\langle\\widehat{p}_{t},\\mathbf{x}\\rangle-\\frac{B_{T}}{T}}\\end{array}$ . From convexity of function $\\mathcal{L}_{t}({\\bf x},\\lambda)$ w.r.t $\\lambda$ ,wehave ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{t}(\\mathbf{x}_{t},\\lambda)-\\mathcal{L}_{t}(\\mathbf{x}_{t},\\lambda_{t})\\geq(\\lambda-\\lambda_{t})^{\\top}{\\nabla}_{\\lambda}\\mathcal{L}_{t}(\\mathbf{x}_{t},\\lambda_{t})}\\\\ &{\\qquad\\qquad\\qquad\\geq-\\frac{1}{2\\eta}(\\|\\lambda_{t}-\\lambda\\|^{2}-\\|\\lambda_{t+1}-\\lambda\\|^{2})-C^{2}\\eta-2\\eta\\gamma_{t}^{2}-2\\delta^{2}\\eta^{3}\\lambda_{t}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where (31) follows from (30). Subtracting two times (31) from (28), we get ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[\\mathcal{L}_{t}(\\mathbf{x},\\lambda_{t})-2\\mathcal{L}_{t}(\\mathbf{x}_{t},\\lambda)]\\leq\\displaystyle\\frac{1}{2\\eta}\\mathbb{E}[\\|\\mathbf{x}_{t}-\\mathbf{x}\\|^{2}-\\|\\mathbf{x}_{t+1}-\\mathbf{x}\\|^{2}]+\\displaystyle\\frac{1}{\\eta}(\\|\\lambda_{t}-\\lambda\\|^{2}-\\|\\lambda_{t+1}-\\lambda\\|^{2})}\\\\ {+G^{2}\\eta+4\\eta\\beta^{2}\\lambda_{t}^{2}+2C^{2}\\eta+4\\eta\\gamma_{t}^{2}+4\\delta^{2}\\eta^{3}\\lambda_{t}^{2}+\\lambda_{t}\\widetilde{g}_{t}(\\mathbf{x})-\\displaystyle\\frac{\\delta\\eta}{2}\\lambda_{t}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Summing (32) for $t\\in[T]$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{t=1}^{T}\\mathbb{E}[Z_{t}(\\mathbf{x},\\lambda_{t})-2\\mathcal{L}_{t}(\\mathbf{x}_{t},\\lambda_{t})]}\\quad}&{}\\\\ &{\\leq\\frac{1}{2\\eta}\\sum_{t=1}^{T}\\mathbb{E}[\\|\\mathbf{x}_{t}-\\mathbf{x}\\|^{2}-\\|\\mathbf{x}_{t+1}-\\mathbf{x}\\|^{2}]+\\frac{1}{\\eta}\\sum_{t=1}^{T}(\\|\\lambda_{t}-\\lambda\\|^{2}-\\|\\lambda_{t+1}-\\lambda\\|^{2})}\\\\ &{\\quad+G^{2}\\eta T+4\\eta\\beta^{2}\\sum_{t=1}^{T}\\lambda_{t}^{2}+2C^{2}\\eta T+4\\eta\\sum_{t=1}^{T}\\gamma_{t}^{2}+4\\delta^{2}\\eta^{3}\\sum_{t=1}^{T}\\lambda_{t}^{2}+\\sum_{t=1}^{T}\\lambda_{t}\\widetilde{g}_{t}(\\mathbf{x})-\\frac{\\delta\\eta}{2}\\sum_{t=1}^{T}}\\\\ &{\\leq\\frac{1}{2\\eta}\\mathbb{E}[\\|\\mathbf{x}_{1}-\\mathbf{x}\\|^{2}-\\|\\mathbf{x}_{T+1}-\\mathbf{x}\\|^{2}]+\\frac{1}{\\eta}(\\|\\lambda_{1}-\\lambda\\|^{2}-\\|\\lambda_{T+1}-\\lambda\\|^{2})}\\\\ &{\\quad+G^{2}\\eta T+4\\eta\\beta^{2}\\sum_{t=1}^{T}\\lambda_{t}^{2}+2C^{2}\\eta T+4\\eta\\sum_{t=1}^{T}\\gamma_{t}^{2}+4\\delta^{2}\\eta^{3}\\sum_{t=1}^{T}\\lambda_{t}^{2}+\\sum_{t=1}^{T}\\lambda_{t}\\widetilde{g}_{t}(\\mathbf{x})-\\frac{\\delta\\eta}{2}\\sum_{t=1}^{T}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "(telescoping series) ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\leq\\frac{1}{2\\eta}\\mathbb{E}[\\|{\\bf x}_{1}-{\\bf x}_{T+1}\\|^{2}]+\\frac{1}{\\eta}(\\|\\lambda_{1}-\\lambda\\|^{2}-\\|\\lambda_{T+1}-\\lambda\\|^{2})}}\\\\ {{\\displaystyle\\quad+\\,G^{2}\\eta T+4\\eta\\beta^{2}\\sum_{t=1}^{T}\\lambda_{t}^{2}+2C^{2}\\eta T+4\\eta\\sum_{t=1}^{T}\\gamma_{t}^{2}+4\\delta^{2}\\eta^{3}\\sum_{t=1}^{T}\\lambda_{t}^{2}+\\sum_{t=1}^{T}\\lambda_{t}\\widetilde{g}_{t}({\\bf x})-\\frac{\\delta\\eta}{2}\\sum_{t=1}^{T}\\lambda_{t}^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "(triangle inequality) ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\leq\\frac{d^{2}}{2\\eta}+\\frac{\\lambda^{2}}{\\eta}+G^{2}\\eta T+\\eta\\beta^{2}\\sum_{t=1}^{T}\\lambda_{t}^{2}+2C^{2}\\eta T+4\\eta\\sum_{t=1}^{T}\\gamma_{t}^{2}+4\\delta^{2}\\eta^{3}\\sum_{t=1}^{T}\\lambda_{t}^{2}+\\sum_{t=1}^{T}\\lambda_{t}\\widetilde{g}_{t}(\\mathbf{x})-\\frac{\\delta\\eta}{2}\\sum_{t=1}^{T}\\lambda_{t}^{2},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where (33) uses Assumption 1 with def. of diameter $d$ $\\lambda_{1}=0$ ,and $-\\|\\cdot\\|^{2}\\leq0$ . Expanding the left hand side of (33), we deduce ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{t=1}^{T}[f(\\mathbf x)-2f(\\mathbf x_{t})]+\\sum_{t=1}^{T}[2\\lambda\\widetilde{g}_{t}(\\mathbf x_{t})-\\lambda_{t}\\widetilde{g}_{t}(\\mathbf x)]+\\sum_{t=1}^{T}\\left[\\frac{\\delta\\eta\\lambda_{t}^{2}}{2}-\\delta\\eta\\lambda^{2}\\right]}\\\\ {\\displaystyle\\leq\\frac{d^{2}}{2\\eta}+\\frac{\\lambda^{2}}{\\eta}+G^{2}\\eta T+4\\eta\\beta^{2}\\sum_{t=1}^{T}\\lambda_{t}^{2}+2C^{2}\\eta T+4\\eta\\sum_{t=1}^{T}\\gamma_{t}^{2}+4\\delta^{2}\\eta^{3}\\sum_{t=1}^{T}\\lambda_{t}^{2}+\\sum_{t=1}^{T}\\lambda_{t}\\widetilde{g}_{t}(\\mathbf x)-\\frac{\\delta\\eta}{2}\\sum_{t=1}^{T}\\lambda_{t}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Rearranging, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}[f(\\mathbf{x})-2f(\\mathbf{x}_{t})]+\\left\\{2\\lambda\\sum_{t=1}^{T}\\widetilde{g}_{t}(\\mathbf{x}_{t})-\\left(\\delta\\eta T+\\frac{1}{\\eta}\\right)\\lambda^{2}\\right\\}}\\\\ &{\\displaystyle\\leq2\\sum_{t=1}^{T}\\lambda_{t}\\widetilde{g}_{t}(\\mathbf{x})+\\eta\\left(4\\beta^{2}+4\\delta^{2}\\eta^{2}-\\delta\\right)\\sum_{t=1}^{T}\\lambda_{t}^{2}+\\frac{d^{2}}{2\\eta}+G^{2}\\eta T+2C^{2}\\eta T+4\\eta\\sum_{t=1}^{T}\\gamma_{t}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Set $\\mathbf{x}=\\mathbf{x}^{*}$ . From Lemma 4, with probability at least $\\begin{array}{r}{1-\\frac{\\varepsilon}{T}}\\end{array}$ $,\\widetilde{g}_{t}(\\mathbf{x}^{*})=\\widehat{g}_{t}(\\mathbf{x}^{*})-\\gamma_{t}\\leq g(\\mathbf{x}^{*})$ holds. Since $\\mathbf{x}^{*}$ satisfies the long term constraint, we have $g(\\mathbf{x}^{*})^{\\pm}\\leq0$ . By the union bound, we get with probability at least $1-\\varepsilon$ that for the first term on the RHS of (35) ", "page_idx": 16}, {"type": "equation", "text": "$$\n2\\sum_{t=1}^{T}\\lambda_{t}\\widetilde{g}_{t}(\\mathbf{x}^{*})\\leq2g(\\mathbf{x}^{*})\\sum_{t=1}^{T}\\lambda_{t}\\leq0.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now we choose $\\delta$ such that $4\\beta^{2}+4\\delta^{2}\\eta^{2}-\\delta\\le0$ (so that the second term on the RHS of (35) will be negative and can be dropped with an upper bound). This is a quadratic term w.r.t. $\\delta$ . It is easy to verifythat the quadratie formula has eal rots hen $\\begin{array}{r}{\\eta\\le\\frac{1}{8\\beta}}\\end{array}$ As wechose $\\begin{array}{r}{\\eta=\\frac{d}{U\\sqrt{T}}}\\end{array}$ ths wi give us the condition that T needs to be sficienly large, i.e,T > G4d-g Ad-. We can simplychoose $\\delta=8\\beta^{2}$ ", "page_idx": 16}, {"type": "text", "text": "Applying both of those inequalites to 35)and byunion bound, we get with probabilityat east $1-\\varepsilon$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{t=1}^{T}[f(\\mathbf{x}^{*})-2f(\\mathbf{x}_{t})]+\\left\\{2\\lambda\\sum_{t=1}^{T}\\widetilde{g}_{t}(\\mathbf{x}_{t})-(\\delta\\eta T+\\frac{1}{\\eta})\\lambda^{2}\\right\\}}}\\\\ &{}&{\\leq\\frac{d^{2}}{2\\eta}+G^{2}\\eta T+2C^{2}\\eta T+4\\eta\\sum_{t=1}^{T}\\gamma_{t}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Maximizing the LHS of (36) with respect to $\\lambda$ , over the range $\\lbrack0,+\\infty)$ , we get a solution of [o(80] Plugig thisinto (36) gives us ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}[f(\\mathbf{x}^{*})-2f(\\mathbf{x}_{t})]+\\frac{\\left[\\sum_{t=1}^{T}\\widetilde{g}_{t}(\\mathbf{x}_{t})\\right]_{+}^{2}}{\\delta\\eta T+1/\\eta}\\leq\\frac{d^{2}}{2\\eta}+G^{2}\\eta T+2C^{2}\\eta T+4\\eta\\sum_{t=1}^{T}\\gamma_{t}^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Let $U=\\operatorname*{max}\\{G,C\\}$ . Choosing $\\begin{array}{r}{\\eta=\\frac{d}{U\\sqrt{T}}}\\end{array}$ , we have with probability at least $1-\\varepsilon$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}[f(\\mathbf{x}^{*})-2f(\\mathbf{x})]+\\frac{\\left[\\sum_{t=1}^{T}\\widetilde{g}_{t}(\\mathbf{x}_{t})\\right]_{+}^{2}}{\\delta\\eta T+1/\\eta}}\\\\ &{\\le\\frac{d\\operatorname*{max}\\{G,C\\}}{2}\\sqrt{T}+\\frac{G^{2}d}{\\operatorname*{max}\\{G,C\\}}\\sqrt{T}+\\frac{2C^{2}d}{\\operatorname*{max}\\{G,C\\}}\\sqrt{T}+\\frac{8\\operatorname*{max}\\{G,C\\}d\\log\\frac{2T}{\\varepsilon}}{t-1}\\sum_{t=1}^{T}\\frac{1}{t}}\\\\ &{\\le\\frac{d\\operatorname*{max}\\{G,C\\}}{2}\\sqrt{T}+\\operatorname*{max}\\{G,C\\}d\\sqrt{T}+2\\operatorname*{max}\\{G,C\\}d\\sqrt{T}+\\frac{8U d\\log\\frac{2T}{\\varepsilon}}{\\sqrt{T}}\\sum_{t=1}^{T}\\frac{1}{t}}\\\\ &{\\le\\frac{7d U}{2}\\sqrt{T}+\\frac{8U d\\log\\frac{2T}{\\varepsilon}}{\\sqrt{T}}\\sum_{t=1}^{T}\\frac{1}{\\sqrt{t}}}\\\\ &{\\le\\frac{7d U}{2}\\sqrt{T}+16U d\\log\\frac{2T}{\\varepsilon}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where Equation (40) uses $\\textstyle\\sum_{t=1}^{T}{\\frac{1}{\\sqrt{t}}}\\leq2{\\sqrt{T}}$ . This gives us ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\left[{\\frac{1}{2}}f(\\mathbf{x}^{*})-f(\\mathbf{x}_{t})\\right]\\leq{\\frac{7d U}{4}}{\\sqrt{T}}+8d U\\log{\\frac{2T}{\\varepsilon}}={\\mathcal{O}}(T^{1/2}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Next, we establish our constraint violation bound. Since $F_{1}\\ :=\\ \\operatorname*{max}_{\\mathbf{x}\\in K}|f(\\mathbf{x})|$ and $F_{2}~:=$ $\\mathrm{max}_{\\mathbf{x},\\mathbf{y}\\in\\mathcal{K}}\\left|f(\\mathbf{x})-f(\\mathbf{y})\\right|$ ,wehave ", "page_idx": 17}, {"type": "equation", "text": "$$\n|f(\\mathbf{x}^{*})-2f(\\mathbf{x}_{t})|\\leq|f(\\mathbf{x}^{*})-f(\\mathbf{x}_{t})|+|f(\\mathbf{x}_{t})|\\leq F_{1}+F_{2},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "thus, $\\begin{array}{r}{\\sum_{t=1}^{T}[f(\\mathbf{x}^{*})-2f(\\mathbf{x}_{t})]\\ge-(F_{1}+F_{2})T}\\end{array}$ Plugging back in (40), we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{\\left[\\sum_{t=1}^{T}\\widetilde{g}_{t}(\\mathbf x_{t})\\right]_{+}^{2}}{\\delta\\eta T+1/\\eta}\\leq\\frac{7d U}{4}\\sqrt{T}+8d U\\log\\frac{2T}{\\varepsilon}+(F_{1}+F_{2})T.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Rearranging and plug in the value of $\\eta$ wehave ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left[\\sum_{t=1}^{T}\\widetilde{g}(\\mathbf{x}_{t})\\right]_{+}\\le\\sqrt{\\left(\\frac{7d U}{4}\\sqrt{T}+8d U\\log\\frac{2T}{\\varepsilon}+(F_{1}+F_{2})T\\right)\\cdot\\left(\\frac{8\\beta^{2}d}{U}+\\frac{U}{d}\\right)\\sqrt{T}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Combining (44) with Lemma 5, we get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{C_{T}\\leq\\sqrt{\\left(\\frac{7d U}{4}\\sqrt{T}+8d U\\log\\frac{2T}{\\varepsilon}+(F_{1}+F_{2})T\\right)\\cdot\\left(\\frac{8\\beta^{2}d}{U}+\\frac{U}{d}\\right)\\sqrt{T}+r\\frac{T}{\\varepsilon=1}\\|\\hat{p}_{t}-p\\|+\\frac{T}{\\varepsilon=1}}}\\\\ &{\\quad\\leq\\sqrt{\\left(\\frac{7d U}{4}\\sqrt{T}+8d U\\log\\frac{2T}{\\varepsilon}+(F_{1}+F_{2})T\\right)\\cdot\\left(\\frac{8\\beta^{2}d}{U}+\\frac{U}{d}\\right)\\sqrt{T}}}\\\\ &{\\quad\\quad\\quad\\quad+r Q\\sigma\\sqrt{T\\log\\left(\\frac{2n T}{\\varepsilon}\\right)}+\\sum_{t=1}^{T}}&{\\mathrm{(using~Lemma~)}}\\\\ &{\\quad\\quad\\leq\\sqrt{\\left(\\frac{7d U}{4}\\sqrt{T}+8d U\\log\\frac{2T}{\\varepsilon}+(F_{1}+F_{2})T\\right)\\cdot\\left(\\frac{8\\beta^{2}d}{U}+\\frac{U}{d}\\right)\\sqrt{T}}}\\\\ &{\\quad\\quad\\quad\\quad+r Q\\sigma\\sqrt{T\\log\\left(\\frac{2n T}{\\varepsilon}\\right)}+2\\sqrt{2T C^{2}\\log\\left(\\frac{2T}{\\varepsilon}\\right)}}\\\\ &{\\quad\\quad\\quad\\quad+\\mathcal{O}(r^{3/4})}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where in the last inequality we plug in the definition of $\\gamma_{t}$ . This concludes the proof. ", "page_idx": 17}, {"type": "text", "text": "Algorithm 2 OLSGA with First Order Full Information ", "page_idx": 18}, {"type": "text", "text": "1: Input: Convex set $\\kappa$ , time horizon $T$   \n2:Initialize $\\mathbf{x}_{1}\\in\\mathcal{K}$ \uff0c $\\lambda_{1}=0$   \n3: for $t\\in[T]$ do   \n4: Play $\\mathbf{x}_{t}$ , obtain $f_{t}(\\mathbf{x}_{t})$ and $\\widetilde{\\nabla}f_{t}(\\cdot)$ and $\\scriptstyle{\\mathbf{\\mathit{p}}}_{t}$   \n5: Compute $\\begin{array}{r}{\\widehat{\\pmb{p}}_{t}=\\frac{1}{t}\\sum_{s=1}^{t}p_{s}}\\end{array}$   \n6: Sample $z_{t}$ from $\\mathbf{Z}$ where $\\begin{array}{r}{\\mathrm{P}(\\mathbf{Z}\\leq z)=\\int_{0}^{z}\\frac{e^{u-1}}{1-e^{-1}}\\mathrm{d}u}\\end{array}$   \n7: Compute $\\widetilde{\\nabla}F_{t}(\\mathbf{x}_{t})=(1-1/e)\\widetilde{\\nabla}f_{t}(z_{t}\\ast\\mathbf{x}_{t})$   \n8: Compute ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{\\nabla}_{x}\\widehat{\\mathcal{L}}_{t}(\\mathbf{x}_{t},\\lambda_{t})=\\widetilde{\\nabla}F_{t}(\\mathbf{x}_{t})-\\lambda_{t}{\\nabla}\\widetilde{g}_{t}(\\mathbf{x}_{t})}\\\\ &{\\nabla_{\\lambda}\\mathcal{L}_{t}(\\mathbf{x}_{t},\\lambda_{t})=-\\widetilde{g}_{t}(\\mathbf{x}_{t})+\\delta\\eta\\lambda_{t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "9: Update $\\mathbf{x}_{t}$ and $\\lambda_{t}$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{x}_{t+1}=\\Pi_{K}(\\mathbf{x}_{t}+\\eta\\widetilde{\\nabla}_{x}\\widehat{\\mathcal{L}}_{t}(\\mathbf{x}_{t},\\lambda_{t}))}\\\\ &{\\lambda_{t+1}=\\Pi_{[0,+\\infty)}(\\lambda_{t}-\\eta\\nabla_{\\lambda}\\mathcal{L}_{t}(\\mathbf{x}_{t},\\lambda_{t}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "10: end for ", "page_idx": 18}, {"type": "text", "text": "E Proof of Theorem 2 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We restate our theorem as follows: ", "page_idx": 18}, {"type": "text", "text": "Theorem 4 Lt Assumptions $1\\,2\\,3\\,4$ be satisfedl Let $U=\\operatorname*{max}\\{G_{F},C\\}$ Choosing $\\begin{array}{r}{\\eta=\\frac{d}{U\\sqrt{T}}}\\end{array}$ and $\\delta=4\\beta^{2}$ . Let $\\mathbf{x}_{t}$ $t\\in[T]$ be the sequence of solutions obtained by Algorithm 2. When $T$ is sufficiently large, ie,T > 32d\u03b2, ,we have the following $(1-1/e)$ -regret and constraint violation bounds with probability at least $1-\\varepsilon$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}[R_{T}]\\leq\\sum_{t=1}^{T}\\left[(1-1/e)f(\\mathbf{x}^{*})-f(\\mathbf{x}_{t})\\right]\\leq\\frac{5d U}{2}\\sqrt{T}+8d U\\log\\frac{2T}{\\varepsilon}=\\mathcal{O}(T^{1/2})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C_{T}\\leq\\sqrt{\\left(\\frac{5d U}{2}\\sqrt{T}+8d U\\log\\frac{2T}{\\varepsilon}+(\\frac{F_{1}}{e}+F_{2})T\\right)\\cdot\\left(\\frac{4\\beta^{2}d}{U}+\\frac{U}{d}\\right)\\sqrt{T}}}\\\\ &{\\qquad\\qquad+\\,r Q\\sigma\\sqrt{T\\log\\left(\\frac{2n T}{\\varepsilon}\\right)}+2\\sqrt{2T C^{2}\\log\\left(\\frac{2T}{\\varepsilon}\\right)}}\\\\ &{=\\mathcal{O}(T^{3/4}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. From the update of $\\mathbf{x}_{t}$ , we have that for any $\\mathbf{x}\\in{\\mathcal{K}}$ \uff0c ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\|\\mathbf{x}_{t+1}-\\mathbf{x}\\|^{2}=\\|\\Pi_{K}(\\mathbf{x}_{t}+\\eta\\tilde{\\nabla}_{x}\\hat{\\mathcal{L}}_{t}(\\mathbf{x}_{t},\\lambda_{t}))-\\mathbf{x}\\|^{2}}}\\\\ &{}&{\\leq\\|\\mathbf{x}_{t}+\\eta\\tilde{\\nabla}_{x}\\hat{\\mathcal{L}}_{t}(\\mathbf{x}_{t},\\lambda_{t})-\\mathbf{x}\\|^{2}\\qquad\\qquad\\qquad\\qquad\\qquad\\mathrm{(def.~of~projection~})}\\\\ &{}&{=\\|\\mathbf{x}_{t}-\\mathbf{x}\\|^{2}+\\eta^{2}\\|\\tilde{\\nabla}_{x}\\hat{\\mathcal{L}}_{t}(\\mathbf{x}_{t},\\lambda_{t})\\|^{2}-2\\eta(\\mathbf{x}-\\mathbf{x}_{t})^{\\top}\\tilde{\\nabla}_{x}\\hat{\\mathcal{L}}_{t}(\\mathbf{x}_{t},\\lambda_{t}).\\qquad(49)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Rearranging, ", "text_level": 1, "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\quad}&{(\\mathbf x-\\mathbf x_{t})^{\\top}\\widetilde\\nabla_{x}\\widehat Z_{t}(\\mathbf x_{t},\\lambda_{t})\\leq\\displaystyle\\frac1{2\\eta}(\\|\\mathbf x_{t}-\\mathbf x\\|^{2}-\\|\\mathbf x_{t+1}-\\mathbf x\\|^{2})+\\displaystyle\\frac{\\eta}{2}\\|\\widetilde\\nabla_{x}\\widehat Z_{t}(\\mathbf x_{t},\\lambda_{t})\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\frac1{2\\eta}(\\|\\mathbf x_{t}-\\mathbf x\\|^{2}-\\|\\mathbf x_{t+1}-\\mathbf x\\|^{2})+\\displaystyle\\frac{\\eta}{2}\\|\\widetilde\\nabla F_{t}(\\mathbf x_{t})-\\lambda_{t}\\nabla\\widetilde g_{t}(x_{t})\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\mathrm{(from~}(45))}\\\\ &{\\leq\\displaystyle\\frac{1}{2\\eta}(\\|\\mathbf x_{t}-\\mathbf x\\|^{2}-\\|\\mathbf x_{t+1}-\\mathbf x\\|^{2})+\\eta\\|\\widetilde\\nabla F_{t}(\\mathbf x_{t})\\|^{2}+\\eta\\lambda_{t}^{2}\\|\\nabla\\widetilde g_{t}(x_{t})\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad(\\|a+b\\|^{2}\\leq2\\|a\\|^{2}+2\\|b\\|^{2})}\\\\ &{\\leq\\displaystyle\\frac{1}{2\\eta}(\\|\\mathbf x_{t}-\\mathbf x\\|^{2}-\\|\\mathbf x_{t+1}-\\mathbf x\\|^{2})+\\eta G_{F}^{2}+\\eta\\beta^{2}\\lambda_{t}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where (50) follows from Assumption 4 and Assumption 3. When $\\lambda$ is fixed, we have (taking expectation over $f_{t.}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[(1-1/e)\\mathcal{L}_{t}(\\mathbf{x},\\lambda_{t})-\\mathcal{L}_{t}(\\mathbf{x}_{t},\\lambda_{t})]}\\\\ &{~=\\mathbb{E}[\\mathbb{E}[(1-1/e)\\mathcal{L}_{t}(\\mathbf{x},\\lambda_{t})-\\mathcal{L}_{t}(\\mathbf{x}_{t},\\lambda_{t})|\\mathbf{x}_{t}]]}\\\\ &{~=\\mathbb{E}[\\mathbb{E}[(1-1/e)(f_{t}(\\mathbf{x})-\\lambda_{t}\\tilde{g}_{t}(\\mathbf{x})+\\frac{\\delta\\eta}{2}\\lambda_{t}^{2})-(f_{t}(\\mathbf{x}_{t})-\\lambda_{t}\\tilde{g}_{t}(\\mathbf{x}_{t})+\\frac{\\delta\\eta}{2}\\lambda_{t}^{2})|\\mathbf{x}_{t}]]}\\\\ &{~=\\mathbb{E}[\\mathbb{E}[(1-1/e)f_{t}(\\mathbf{x})-f_{t}(\\mathbf{x}_{t})-\\lambda_{t}(\\tilde{g}_{t}(\\mathbf{x})-\\tilde{g}_{t}(\\mathbf{x}_{t}))|\\mathbf{x}_{t}]]+\\frac{1}{e}\\lambda_{t}\\tilde{g}_{t}(\\mathbf{x})-\\frac{\\delta\\eta}{2e}\\lambda_{t}^{2})}\\\\ &{~\\le\\mathbb{E}[(\\mathbf{x}-\\mathbf{x}_{t})^{\\top}\\nabla_{x}\\mathbb{E}[F_{t}(\\mathbf{x}_{t})-\\lambda_{t}(\\tilde{g}_{t}(\\mathbf{x})-\\tilde{g}_{t}(\\mathbf{x}_{t}))|\\mathbf{x}_{t}]]+\\frac{1}{e}\\lambda_{t}\\tilde{g}_{t}(\\mathbf{x})-\\frac{\\delta\\eta}{2e}\\lambda_{t}^{2}}\\\\ &{~=\\mathbb{E}[\\mathbb{E}[(\\mathbf{x}-\\mathbf{x}_{t})^{\\top}\\nabla_{x}F_{t}(\\mathbf{x}_{t})-\\lambda_{t}(\\tilde{g}_{t}(\\mathbf{x})-\\tilde{g}_{t}(\\mathbf{x}_{t}))|\\mathbf{x}_{t}]]+\\frac{1}{e}\\lambda_{t}\\tilde{g}_{t}(\\mathbf{x})-\\frac{\\delta\\eta}{2e}\\lambda_{t}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "(linearity of expectation) ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\mathbb{E}[\\mathbb{E}[(\\mathbf{x}-\\mathbf{x}_{t})^{\\top}\\nabla_{x}F_{t}(\\mathbf{x}_{t})-\\lambda_{t}((\\mathbf{x}-\\mathbf{x}_{t})^{\\top}\\nabla\\tilde{g}_{t}(\\mathbf{x}_{t}))|\\mathbf{x}_{t}]]+\\frac{1}{e}\\lambda_{t}\\tilde{g}_{t}(\\mathbf{x})-\\frac{\\delta\\eta}{2e}\\lambda_{t}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(\\tilde{g}_{t}(\\cdot)\\mathrm{~is~convex~so~}\\tilde{g}_{t}(\\mathbf{y})-\\tilde{g}_{t}(\\mathbf{x})\\geq\\langle\\nabla\\tilde{g}_{t}(\\mathbf{x}),\\mathbf{y}-\\mathbf{x}\\rangle)}\\\\ &{=\\mathbb{E}[\\mathbb{E}[(\\mathbf{x}-\\mathbf{x}_{t})^{\\top}\\nabla_{x}(F_{t}(\\mathbf{x}_{t})-\\lambda_{t}\\tilde{g}_{t}(\\mathbf{x}_{t}))|\\mathbf{x}_{t}]]+\\frac{1}{e}\\lambda_{t}\\tilde{g}_{t}(\\mathbf{x})-\\frac{\\delta\\eta}{2e}\\lambda_{t}^{2}}\\\\ &{=\\mathbb{E}[\\mathbb{E}[(\\mathbf{x}-\\mathbf{x}_{t})^{\\top}\\nabla_{x}(F_{t}(\\mathbf{x}_{t})-\\lambda_{t}\\tilde{g}_{t}(\\mathbf{x}_{t})+\\frac{\\delta\\eta}{2}\\lambda_{t}^{2})|\\mathbf{x}_{t}]]+\\frac{1}{e}\\lambda_{t}\\tilde{g}_{t}(\\mathbf{x})-\\frac{\\delta\\eta}{2e}\\lambda_{t}^{2}}\\\\ &{=\\mathbb{E}[\\mathbb{E}[(\\mathbf{x}-\\mathbf{x}_{t})^{\\top}\\widetilde{\\nabla}_{x}\\widehat{C}_{t}(\\mathbf{x}_{t},\\lambda_{t})|\\mathbf{x}_{t}]]+\\frac{1}{e}\\lambda_{t}\\tilde{g}_{t}(\\mathbf{x})-\\frac{\\delta\\eta}{2e}\\lambda_{t}^{2}}\\\\ &{\\leq\\frac{1}{2\\eta}\\mathbb{E}[\\|\\mathbf{x}_{t}-\\mathbf{x}\\|^{2}-\\|\\mathbf{x}_{t+1}-\\mathbf{x}\\|^{2}]+G_{F}^{2}\\eta+\\eta\\beta^{2}\\lambda_{t}^{2}+\\frac{1}{e}\\lambda_ \n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where (51) follows from (50). From the update (48) of $\\lambda_{t}$ ,wehave ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\|\\lambda_{t+1}-\\lambda\\|^{2}=\\|\\Pi_{[0,+\\infty)}(\\lambda_{t}-\\eta\\nabla_{\\lambda}\\mathcal{L}_{t}(\\mathbf{x}_{t},\\lambda_{t}))-\\lambda\\|^{2}\\qquad}}&{{\\mathrm{(from~(48))}}}\\\\ &{}&{\\leq\\|\\lambda_{t}-\\eta\\nabla_{\\lambda}\\mathcal{L}_{t}(\\mathbf{x}_{t},\\lambda_{t})-\\lambda\\|^{2}\\qquad}&{{\\mathrm{(def.~of~projection)}}}\\\\ &{}&{\\leq\\|\\lambda_{t}-\\lambda\\|^{2}+\\eta^{2}\\|\\nabla_{\\lambda}\\mathcal{L}_{t}(\\mathbf{x}_{t},\\lambda_{t})\\|^{2}+2\\eta(\\lambda-\\lambda_{t})^{\\top}\\nabla_{\\lambda}\\mathcal{L}_{t}(\\mathbf{x}_{t},\\lambda_{t}),\\qquad(52)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where (52) multiplies through. Rearranging, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{(\\lambda-\\lambda_{t})^{\\top}\\nabla_{\\lambda}\\mathcal{L}_{t}(\\mathbf{x}_{t},\\lambda_{t})\\geq-\\frac{1}{2\\eta}(\\|\\lambda_{t}-\\lambda\\|^{2}-\\|\\lambda_{t+1}-\\lambda\\|^{2})-\\frac{\\eta}{2}\\|\\nabla_{\\lambda}\\mathcal{L}_{t}(\\mathbf{x}_{t},\\lambda_{t})\\|^{2}}\\qquad}&{}\\\\ &{=-\\frac{1}{2\\eta}(\\|\\lambda_{t}-\\lambda\\|^{2}-\\|\\lambda_{t+1}-\\lambda\\|^{2})-\\frac{\\eta}{2}\\|-\\widetilde{g}_{t}(\\mathbf{x}_{t})+\\delta\\eta\\lambda_{t}\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(\\mathrm{from~(46~})}\\\\ &{=-\\frac{1}{2\\eta}(\\|\\lambda_{t}-\\lambda\\|^{2}-\\|\\lambda_{t+1}-\\lambda\\|^{2})-\\frac{\\eta}{2}\\|-\\widehat{g}_{t}(\\mathbf{x}_{t})-\\gamma_{t}+\\delta\\eta\\lambda_{t}\\|^{2}}\\\\ &{\\geq-\\frac{1}{2\\eta}(\\|\\lambda_{t}-\\lambda\\|^{2}-\\|\\lambda_{t+1}-\\lambda\\|^{2})-\\eta\\|\\widehat{g}_{t}(\\mathbf{x}_{t})\\|^{2}-2\\eta\\gamma_{t}^{2}-2\\delta^{2}\\eta^{3}\\lambda_{t}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\mathrm{(apply~\\|\\lambda_{t}+b\\|^2~\\leq2\\|a\\|^2~)}+2\\|b\\|^{2}\\mathrm{~twic~}}\\\\ &{\\geq-\\frac{1}{2\\eta}(\\|\\lambda_{t}-\\lambda\\|^{2}-\\|\\lambda_{t+1}-\\lambda\\|^{2})-C^{2}\\eta-2\\eta\\gamma_{t}^{2}-2\\delta^{2}\\eta^{3}\\lambda_{t}^{2},\\quad(5.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the last inequality follows from the defnition of $\\begin{array}{r}{C:=\\operatorname*{max}_{\\pmb{p^{\\prime}}\\sim\\mathcal{D}_{p}}\\operatorname*{max}_{\\mathbf{x}\\in\\mathcal{K}}|\\langle\\pmb{p^{\\prime}},\\mathbf{x}\\rangle-\\frac{B_{T}}{T}|}\\end{array}$ and $\\begin{array}{r}{\\widehat{g}_{t}(\\mathbf{x}):=\\langle\\widehat{p}_{t},\\mathbf{x}\\rangle-\\frac{B_{T}}{T}}\\end{array}$ . From convexity of function $\\mathcal{L}_{t}({\\bf x},\\lambda)$ w.r.t $\\lambda$ wehave ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathcal{L}_{t}(\\mathbf{x}_{t},\\lambda)-\\mathcal{L}_{t}(\\mathbf{x}_{t},\\lambda_{t})\\ge(\\lambda-\\lambda_{t})^{\\top}{\\nabla}_{\\lambda}\\mathcal{L}_{t}(\\mathbf{x}_{t},\\lambda_{t})}\\\\ {\\ge-\\frac{1}{2\\eta}(\\|\\lambda_{t}-\\lambda\\|^{2}-\\|\\lambda_{t+1}-\\lambda\\|^{2})-C^{2}\\eta-2\\eta\\gamma_{t}^{2}-2\\delta^{2}\\eta^{3}\\lambda_{t}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where (54) follows from (53). ", "page_idx": 20}, {"type": "text", "text": "Subtracting (54) from (51), we get ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\bar{\\mathfrak{z}}[(1-1/e)\\mathcal{L}_{t}(\\mathbf{x},\\lambda_{t})-\\mathcal{L}_{t}(\\mathbf{x}_{t},\\lambda)]\\leq\\frac{1}{2\\eta}\\mathbb{E}[\\|\\mathbf{x}_{t}-\\mathbf{x}\\|^{2}-\\|\\mathbf{x}_{t+1}-\\mathbf{x}\\|^{2}]+\\frac{1}{2\\eta}(\\|\\lambda_{t}-\\lambda\\|^{2}-\\|\\lambda_{t+1}-\\lambda_{t})}\\\\ &{}&{\\qquad+\\,G_{F}^{2}\\eta+\\eta\\beta^{2}\\lambda_{t}^{2}+C^{2}\\eta+2\\eta\\gamma_{t}^{2}+2\\delta^{2}\\eta^{3}\\lambda_{t}^{2}+\\frac{1}{e}\\lambda_{t}\\tilde{g}_{t}(\\mathbf{x})-\\frac{\\delta\\eta}{2e}\\lambda_{t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Summing (55) for $t\\in[T]$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{t=1}^{s}\\mathbb{E}[(1-1/e)\\mathcal{L}_{t}(\\mathbf{x},\\lambda_{t})-\\mathcal{L}_{t}(\\mathbf{x}_{t},\\lambda)]}\\\\ {\\displaystyle\\leq\\frac{1}{2\\eta}\\sum_{t=1}^{T}\\mathbb{E}[\\|\\mathbf{x}_{t}-\\mathbf{x}\\|^{2}-\\|\\mathbf{x}_{t+1}-\\mathbf{x}\\|^{2}]+\\frac{1}{2\\eta}\\sum_{t=1}^{T}(\\|\\lambda_{t}-\\lambda\\|^{2}-\\|\\lambda_{t+1}-\\lambda\\|^{2})}\\\\ {\\displaystyle\\quad+\\,G_{F}^{2}\\eta T+\\eta\\beta^{2}\\sum_{t=1}^{T}\\lambda_{t}^{2}+C^{2}\\eta T+2\\eta\\sum_{t=1}^{T}\\gamma_{t}^{2}+2\\delta^{2}\\eta^{3}\\sum_{t=1}^{T}\\lambda_{t}^{2}+\\frac{1}{e}\\sum_{t=1}^{T}\\lambda_{t}\\widetilde{g}_{t}(\\mathbf{x})-\\frac{\\delta\\eta}{2e}\\sum_{t=1}^{T}\\lambda_{t}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\leq\\frac{1}{2\\eta}\\mathbb{E}[\\|\\mathbf{x}_{1}-\\mathbf{x}\\|^{2}-\\|\\mathbf{x}_{T+1}-\\mathbf{x}\\|^{2}]+\\frac{1}{2\\eta}(\\|\\lambda_{1}-\\lambda\\|^{2}-\\|\\lambda_{T+1}-\\lambda\\|^{2})}\\\\ {\\displaystyle\\quad+\\,G_{F}^{2}\\eta T+\\eta\\beta^{2}\\sum_{t=1}^{T}\\lambda_{t}^{2}+C^{2}\\eta T+2\\eta\\sum_{t=1}^{T}\\gamma_{t}^{2}+2\\delta^{2}\\eta^{3}\\sum_{t=1}^{T}\\lambda_{t}^{2}+\\frac{1}{e}\\sum_{t=1}^{T}\\lambda_{t}\\widetilde{g}_{t}(\\mathbf{x})-\\frac{\\delta\\eta}{2e}\\sum_{t=1}^{T}\\lambda_{t}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\leq\\frac{1}{2\\eta}\\mathbb{E}[\\|\\mathbf{x}_{1}-\\mathbf{x}_{T+1}\\|^{2}]+\\frac{1}{2\\eta}(\\|\\lambda_{1}-\\lambda\\|^{2}-\\|\\lambda_{T+1}-\\lambda\\|^{2})}\\\\ {\\displaystyle\\quad+\\,G_{F}^{2}\\eta T+\\eta\\beta^{2}\\sum_{t=1}^{T}\\lambda_{t}^{2}+C^{2}\\eta T+2\\eta\\sum_{t=1}^{T}\\gamma_{t}^{2}+2\\delta^{2}\\eta^{3}\\sum_{t=1}^{T}\\lambda_{t}^{2}+\\frac{1}{e}\\sum_{t=1}^{T}\\lambda_{t}\\widetilde{g}_{t}(\\mathbf{x})-\\frac{\\delta\\eta}{2e}\\sum_{t=1}^{T}\\lambda_{t}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "(triangle inequality) ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\leq\\frac{d^{2}}{2\\eta}+\\frac{\\lambda^{2}}{2\\eta}+G_{F}^{2}\\eta T+\\eta\\beta^{2}\\sum_{t=1}^{T}\\lambda_{t}^{2}+C^{2}\\eta T+2\\eta\\sum_{t=1}^{T}\\gamma_{t}^{2}+2\\delta^{2}\\eta^{3}\\sum_{t=1}^{T}\\lambda_{t}^{2}+\\frac{1}{e}\\sum_{t=1}^{T}\\lambda_{t}\\widetilde{g}_{t}(\\mathbf{x})-\\frac{\\delta\\eta}{2e}\\sum_{t=1}^{T}\\lambda_{t}^{2}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where (58) uses Assumption 1 with def. of diameter $d,\\lambda_{1}=0$ ,and $-\\|\\cdot\\|^{2}\\leq0$ . Expanding the left hand side of (58), we deduce ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\sum_{t=1}^{T}[(1-1/e)f({\\bf x})-f({\\bf x}_{t})]+\\sum_{t=1}^{T}[\\lambda\\widetilde{g}_{t}({\\bf x}_{t})-(1-1/e)\\lambda_{t}\\widetilde{g}_{t}({\\bf x})]+\\sum_{t=1}^{T}[(1-1/e)\\frac{\\delta\\eta\\lambda_{t}^{2}}{2}-\\frac{\\delta\\eta\\lambda^{2}}{2}]}\\\\ {\\displaystyle\\leq\\frac{d^{2}}{2\\eta}+\\frac{\\lambda^{2}}{2\\eta}+G_{F}^{2}\\eta T+\\eta\\beta^{2}\\sum_{t=1}^{T}\\lambda_{t}^{2}+C^{2}\\eta T+2\\eta\\sum_{t=1}^{T}\\gamma_{t}^{2}+2\\delta^{2}\\eta^{3}\\sum_{t=1}^{T}\\lambda_{t}^{2}+\\frac{1}{e}\\sum_{t=1}^{T}\\lambda_{t}\\widetilde{g}_{t}({\\bf x})-\\frac{\\delta\\eta}{2e}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Rearranging, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{t=1}^{T}[(1-1/e)f(\\mathbf x)-f(\\mathbf x_{t})]+\\left\\{\\lambda\\sum_{t=1}^{T}\\widetilde{g}_{t}(\\mathbf x_{t})-\\left(\\frac{\\delta\\eta T}{2}+\\frac{1}{2\\eta}\\right)\\lambda^{2}\\right\\}}\\quad}&{}\\\\ &{\\leq\\displaystyle\\sum_{t=1}^{T}\\lambda_{t}\\widetilde{g}_{t}(\\mathbf x)+\\eta\\left(\\beta^{2}+2\\delta^{2}\\eta^{2}-\\frac{\\delta}{2}\\right)\\sum_{t=1}^{T}\\lambda_{t}^{2}+\\frac{d^{2}}{2\\eta}+G_{F}^{2}\\eta T+C^{2}\\eta T+2\\eta\\sum_{t=1}^{T}\\gamma_{t}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Set $\\mathbf{x}\\,=\\,\\mathbf{x}^{*}$ ; From Lemma 4, with probability at least $\\mathrm{~1~-~}\\frac{\\varepsilon}{T}$ \uff0c $\\widetilde{g}_{t}(\\mathbf{x}^{*})\\;=\\;\\widehat{g}_{t}(\\mathbf{x}^{*})\\,-\\,\\gamma_{t}\\;\\leq\\;g(\\mathbf{x}^{*})$ holds. since $\\mathbf{x}^{*}$ satisfies the long term constraint, we have $g(\\mathbf{x}^{\\ast})\\leq0$ . Now we choose $\\delta$ such that $\\begin{array}{r}{\\beta^{2}+2\\delta^{2}\\eta^{2}-\\frac{\\delta}{2}\\leq0.}\\end{array}$ This is a quadratic term w.t. $\\delta$ It is easy to verify that the quadratic formula $\\begin{array}{r}{\\eta\\le\\frac{\\sqrt{2}}{8\\beta}}\\end{array}$ $\\begin{array}{r}{\\eta=\\frac{d}{U\\sqrt{T}}}\\end{array}$ $T$ needsto be suffciently large, i.e., $\\begin{array}{r}{T\\ge\\frac{32d^{2}\\beta^{2}}{U^{2}}}\\end{array}$ 2d-8 . We can simply choose 8 = 4.32. ", "page_idx": 21}, {"type": "text", "text": "Applying both of those inequalites to 60andbyunion bound, we get with probabilitya east $1-\\varepsilon$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{T}[(1-1/e)f(\\mathbf{x}^{*})-f(\\mathbf{x}_{t})]+\\left\\{\\lambda\\sum_{t=1}^{T}\\widetilde{g}_{t}(\\mathbf{x}_{t})-\\left(\\frac{\\delta\\eta T}{2}+\\frac{1}{2\\eta}\\right)\\lambda^{2}\\right\\}\\leq\\frac{d^{2}}{2\\eta}+G_{F}^{2}\\eta T+C^{2}\\eta T+2\\eta\\frac{\\Gamma(\\lambda)}{d\\eta}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Maximizing the LHS of (61) with respect to $\\lambda$ , over the range $\\lbrack0,+\\infty)$ , we get a solution of [ 3(x)]1  Plugin tis into(61 ivesus ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}[(1-1/e)f(\\mathbf{x^{*}})-f(\\mathbf{x}_{t})]+\\frac{\\left[\\sum_{t=1}^{T}\\widetilde{g}_{t}(\\mathbf{x}_{t})\\right]_{+}^{2}}{\\delta\\eta T/2+1/2\\eta}\\leq\\frac{d^{2}}{2\\eta}+G_{F}^{2}\\eta T+C^{2}\\eta T+2\\eta\\sum_{t=1}^{T}\\gamma_{t}^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Let $U=\\operatorname*{max}\\{G_{F},C\\}$ . Choosing $\\begin{array}{r}{\\eta=\\frac{d}{U\\sqrt{T}}}\\end{array}$ , we have with probability at least $1-\\varepsilon$ \uff0c ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}[(1-1/e)f(\\mathbf{x}^{*})-f(\\mathbf{x}_{t})]+\\frac{\\left[\\sum_{t=1}^{T}\\widetilde{g}_{t}(\\mathbf{x}_{t})\\right]_{+}^{2}}{\\delta\\eta T/2+1/2\\eta}}\\\\ &{\\le\\frac{d\\operatorname*{max}\\{G_{F},C\\}}{2}\\sqrt{T}+\\frac{G_{F}^{2}d}{\\operatorname*{max}\\{G_{F},C\\}}\\sqrt{T}+\\frac{C^{2}d}{\\operatorname*{max}\\{G_{F},C\\}}\\sqrt{T}+\\frac{4\\operatorname*{max}\\{G_{F},C\\}d\\log\\frac{2T}{\\varepsilon}\\sum_{t=1}^{T}}{\\sqrt{T}}}\\\\ &{\\le\\frac{d\\operatorname*{max}\\{G_{F},C\\}}{2}\\sqrt{T}+\\operatorname*{max}\\{G_{F},C\\}d\\sqrt{T}+\\operatorname*{max}\\{G_{F},C\\}d\\sqrt{T}+\\frac{4\\operatorname*{max}\\{G_{F},C\\}d\\log\\frac{2T}{\\varepsilon}\\sum_{t=1}^{T}}{\\sqrt{T}}}\\\\ &{\\le\\frac{5d U}{2}\\sqrt{T}+\\frac{4d U\\log\\frac{2T}{\\varepsilon}}{\\sqrt{T}}\\sum_{t=1}^{T}}\\\\ &{=\\frac{5d U}{2}\\sqrt{T}+8d U\\log\\frac{2T}{\\varepsilon}}\\\\ &{=\\mathcal{O}(T^{1/2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and dropping the second term on the LHS gives us the desired $(1-1/e)$ -regretbound. ", "page_idx": 22}, {"type": "text", "text": "Next, we establish our constraint violation bound. Since $F_{1}\\ :=\\ \\operatorname*{max}_{\\mathbf{x}\\in K}|f(\\mathbf{x})|$ and $F_{2}~:=$ $\\mathrm{max}_{\\mathbf{x},\\mathbf{y}\\in\\mathcal{K}}\\left|f(\\mathbf{x})-f(\\mathbf{y})\\right|$ wehave ", "page_idx": 22}, {"type": "equation", "text": "$$\n|(1-1/e)f(\\mathbf{x}^{*})-f(\\mathbf{x}_{t})|\\leq|f(\\mathbf{x}^{*})-f(\\mathbf{x}_{t})|+\\frac{1}{e}|f(\\mathbf{x}_{t})|\\leq\\frac{F_{1}}{e}+F_{2},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "thus, $\\begin{array}{r}{\\sum_{t=1}^{T}[(1-1/e)f(\\mathbf{x}^{*})-f(\\mathbf{x}_{t})]\\ge-(\\frac{F_{1}}{e}+F_{2})T}\\end{array}$ .Plugging back in (64), we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{\\left[\\sum_{t=1}^{T}\\widetilde{g}_{t}(\\mathbf x_{t})\\right]_{+}^{2}}{\\delta\\eta T+1/\\eta}\\leq\\frac{5d U}{2}\\sqrt{T}+8d U\\log\\frac{2T}{\\varepsilon}+(\\frac{F_{1}}{e}+F_{2})T.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Rearranging and plug in the value of $\\eta$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left[\\sum_{t=1}^{T}\\widetilde{g}(\\mathbf{x}_{t})\\right]_{+}\\le\\sqrt{\\left(\\frac{5d U}{2}\\sqrt{T}+8d U\\log\\frac{2T}{\\varepsilon}+(\\frac{F_{1}}{e}+F_{2})T\\right)\\cdot\\left(\\frac{4\\beta^{2}d}{U}+\\frac{U}{d}\\right)\\sqrt{T}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Combining (68) with Lemma 5, we get ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{C_{T}\\leq\\sqrt{\\left(\\frac{5d U}{2}\\sqrt{T}+8d U\\log\\frac{2T}{\\varepsilon}+(\\frac{F_{1}}{e}+F_{2})T\\right)\\cdot\\left(\\frac{4\\beta^{2}d}{U}+\\frac{U}{d}\\right)\\sqrt{T}+r\\frac{T}{t=1}\\|\\hat{p}_{t}-p\\|+\\frac{T}{t=1}}}\\\\ &{\\quad\\leq\\sqrt{\\left(\\frac{5d U}{2}\\sqrt{T}+8d U\\log\\frac{2T}{\\varepsilon}+(\\frac{F_{1}}{e}+F_{2})T\\right)\\cdot\\left(\\frac{4\\beta^{2}d}{U}+\\frac{U}{d}\\right)\\sqrt{T}}}\\\\ &{\\qquad\\qquad+r Q\\sigma\\sqrt{T\\log\\left(\\frac{2n T}{\\varepsilon}\\right)}+\\sum_{i=1}^{r}\\gamma_{i}}&{\\mathrm{(using~Lemma~)}}\\\\ &{\\qquad\\qquad\\leq\\sqrt{\\left(\\frac{5d U}{2}\\sqrt{T}+8d U\\log\\frac{2T}{\\varepsilon}+(\\frac{F_{1}}{e}+F_{2})T\\right)\\cdot\\left(\\frac{4\\beta^{2}d}{U}+\\frac{U}{d}\\right)\\sqrt{T}}}\\\\ &{\\qquad\\qquad+r Q\\sigma\\sqrt{T\\log\\left(\\frac{2n T}{\\varepsilon}\\right)}+2\\sqrt{2T C^{2}\\log\\left(\\frac{2T}{\\varepsilon}\\right)}}\\\\ &{\\qquad\\qquad\\qquad+\\sigma\\mathcal{Q}(\\gamma^{3}/2)}&{\\mathrm{(fse)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where in the last inequality we plug in the definition of $\\gamma_{t}$ . This concludes the proof. ", "page_idx": 22}, {"type": "text", "text": "F More Related Works ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We discuss more related works in this section. ", "page_idx": 22}, {"type": "text", "text": "Adversarial Constraints It is worth noting a line of research that emphasizes adversarial constraints. This setting was initially explored in [21], where a simple counterexample highlighted that achieving sub-linear regret against the best fixed benchmark action in hindsight while maintaining sub-linear total constraint violation, is not always possible. Subsequent works addressed this challenge by introducing additional assumptions to the problem setting to derive meaningful results. Specifically, these works not only required the fixed benchmark action to satisfy the long-term constraint but also imposed the restriction that the benchmark satisfies the constraint proportionally over any window of size $W\\in[T]$ [18, 30, 31]. Due to these added assumptions, direct comparisons with our work are not applicable. ", "page_idx": 22}, {"type": "text", "text": "Online Monotone Submodular Maximization In the discrete domain, online monotone submodular set maximization was first studied in [33] in the adversarial setting where they introduced the meta-action technique. More recently, [24] proposed novel algorithms utilizing the Blackwell Approachability framework, showcasing improved regrets (w.r.t. constant terms) in both semibandit and bandit feedback settings. As for stochastic submodular maximization in the discrete domain, [19] investigated the case of semi-bandit fedback, specifically in the form of marginal gains. Additionally, recent works such as [25, 26] have delved into the full-bandit feedback setting. For continuous domains, [9] first investigate the online (stochastic) gradient ascent (OGA) with a $\\frac{1}{2}$ -regret of ${\\mathcal{O}}(T^{1/2})$ . Then, inspired by the meta actions [33], [9] also proposed Frank-Wolfe type algorithm with a $(1-1/e)$ -regret of ${\\mathcal{O}}(T^{1/2})$ when exact gradient is available. When only stochastic gradient is available, [7] proposed a variant of Frank-Wolfe algorithm achieving $(1-1/e)$ -regret of ${\\mathcal{O}}(T^{1/2})$ but requires ${\\mathcal{O}}(T^{3/2})$ stochastic gradient queries in each time step. In the effort of reducing gradient queries,[41] achieves $(1-1/e)$ -regret of ${\\mathcal{O}}(T^{4/5})$ with only one stochastic gradient evaluation each round. Recently, [42] have proposed an auxiliary function to boost the approximation ratio of the online gradient ascent algorithms from $\\frac{1}{2}$ to $1-1/e$ ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "G Query Set and Constraint Set ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Let $\\kappa\\subset\\mathbb{R}_{+}^{d}$ denote a convex set, with $K^{\\prime}$ defined as the convex hull of $\\mathcal{K}\\cup\\{\\mathbf{0}\\}$ . For problems involving monotone functions over a general set $\\kappa$ , the optimal solution in $K^{\\prime}$ is the same as that in $\\kappa$ due to monotonicity. However, addressing this extended problem domain within $K^{\\prime}$ may necessitate evaluating the function across the larger set, which may not always be feasible. In the literature of stochastic DR-submodular maximization without long-term constraints, various algorithms have been developed to accommodate different constraints (general convex set, convex set containing 0, downward-closed set, etc.). Notably, in scenarios where queries are restricted solely to the constraint set, no algorithm\u2014-neither online nor offline\u2014-has demonstrated an ability to surpass a $1/2$ approximation ratio. For an exhaustive discussion on this matter, we defer to [28]. ", "page_idx": 23}, {"type": "text", "text": "H  Application Examples: Origin not Included in Constraint Set ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We will discuss the following example motivating applications to highlight the importance of general convexregions $\\kappa$ for which the origin is not feasible. For such problems, the best-known approximation ratio is 1/2 [12]. ", "page_idx": 23}, {"type": "text", "text": "1. Maximum Coverage [17] Imagine a city's emergency management agency aiming to optimize the deployment of emergency response teams (firefighters, medical personnel, rescue teams) in many rounds during crises like major fires or earthquakes. The objective is to maximize coverage across different city zones. Allocating O teams to any zone isn't feasible, as it means no emergency response. The goal is to allocate resources in each round to maximize overall response coverage while satisfying the long-term agency budget. ", "page_idx": 23}, {"type": "text", "text": "2. Budget Allocation [32] Let a bipartite graph $G=(S,T;W)$ represent a social network, where $S$ and $T$ are collections of advertising channels and customers, respectively. The edge weight represents the influence probability of channel $s$ to customer $t$ . The goal is to distribute the per-round budget among the source nodes, and to maximize the expected influence on the potential customers over multiple rounds, while satisfying a long-term constraint (e.g., money spent). Corporate management may require a minimum level of customer engagement with each campaign overall or from select target groups. There may also be per-round minimum contractual purchase requirements with the advertising partners. Thus, allocating O budget in any round may not be permitted. ", "page_idx": 23}, {"type": "text", "text": "3. Facility location [17] Consider a scenario where a company needs to decide on the locations (virtual) to set up new service centers to maximize service coverage over multiple rounds while satisfying a total budget constraint over all rounds. At each round, each customer segment must receive at least a certain level of service or coverage, which means O is not a feasible solution because no facilities can provide no service. ", "page_idx": 23}, {"type": "text", "text": "All the problems above were initially studied in the discrete domain and extended to the continuous domain in [6]. Furthermore, when faced with a discrete objective, one can always use the \"relax and rounding\" strategy to transition from addressing a discrete problem to tackling a continuous one. Such techniques are widely frequently utilized within the submodular maximization community, as exemplified by the work of [8]. ", "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The abstract and introduction clearly state the paper's contribution and scope. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: Some limitations of the paper have been discussed in the conclusion section.   \nAssumptions form another part of the limitations. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should refect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should refect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We have clearly stated the required assumptions and an accompanying complete proof in the appendix for each theory result. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our paper is primarily of theoretical nature and does not include experiments. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b)  If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: Our paper is primarily of theoretical nature and does not include experiments. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: Our paper is primarily of theoretical nature and does not include experiments. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: Our paper is primarily of theoretical nature and does not include experiments. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our paper is primarily of theoretical nature and does not include experiments. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our research conforms, in every respect, to the NeurIPs Code of Ethics. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our work is primarily of theoretical nature and has no immediate societal impact. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: No high risk data or model have been used. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: No existing asset has been used in the paper. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 28}, {"type": "text", "text": "\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: No new asset is introduced in the paper. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: No experiments with human subjects were conducted. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: We conducted no experiments with human subjects. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]