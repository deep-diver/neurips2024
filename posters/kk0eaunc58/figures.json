[{"figure_path": "kk0Eaunc58/figures/figures_0_1.jpg", "caption": "Figure 1: Performance comparison of HydraViT and baselines on ImageNet-1K in terms of GMACs (a) and throughput (b) evaluated on NVIDIA A100 80GB PCIe. HydraViT trained on 3\u201312 heads demonstrates superior performance over DynaBERT (Hou et al., 2020) and SortedNet (Valipour et al., 2023). While MatFormer (Kudugunta et al., 2023) shows higher performance than HydraViT within its limited scalability range, but when we train on a narrower scalability range (9\u201312 heads), HydraViT surpasses MatFormer. We also show that training HydraViT for more epochs can further improve accuracy. Note that each line corresponds to one model, and changing the number of heads in the vanilla DeiT models significantly drops their accuracy to less than 30%.", "description": "This figure compares the performance of HydraViT against several baseline models (DeiT, DynaBERT, MatFormer, and SortedNet) on the ImageNet-1K dataset.  Two performance metrics are shown: GMACs (giga multiply-accumulate operations) representing computational complexity and throughput (images/second) representing inference speed.  The plots reveal that HydraViT achieves superior accuracy compared to the baselines for similar levels of computational cost or speed, especially when trained with a wider range of heads (9-12).  The figure highlights HydraViT's scalability and efficiency.", "section": "Abstract"}, {"figure_path": "kk0Eaunc58/figures/figures_0_2.jpg", "caption": "Figure 1: Performance comparison of HydraViT and baselines on ImageNet-1K in terms of GMACs (a) and throughput (b) evaluated on NVIDIA A100 80GB PCIe. HydraViT trained on 3\u201312 heads demonstrates superior performance over DynaBERT (Hou et al., 2020) and SortedNet (Valipour et al., 2023). While MatFormer (Kudugunta et al., 2023) shows higher performance than HydraViT within its limited scalability range, but when we train on a narrower scalability range (9\u201312 heads), HydraViT surpasses MatFormer. We also show that training HydraViT for more epochs can further improve accuracy. Note that each line corresponds to one model, and changing the number of heads in the vanilla DeiT models significantly drops their accuracy to less than 30%.", "description": "This figure compares the performance of HydraViT against several baseline models (DeiT, DynaBERT, MatFormer, and SortedNet) on the ImageNet-1K dataset. The comparison is shown across two key metrics: GMACs (giga multiply-accumulate operations) and throughput (images processed per second).  It demonstrates that HydraViT achieves superior performance (accuracy) compared to the baselines, especially when trained with 9-12 heads and for longer epochs, showcasing its scalability and efficiency.", "section": "Abstract"}, {"figure_path": "kk0Eaunc58/figures/figures_1_1.jpg", "caption": "Figure 2: Architecture of HydraViT.", "description": "This figure illustrates the architecture of HydraViT, highlighting the key components: Patch Embedding, Norm, Multi-Head Attention, Norm, and MLP layers.  The figure emphasizes the selection of subnetworks within the architecture, indicated by the dotted lines and the \"Subnetwork Selection\" label on the right side. This highlights HydraViT's capacity to dynamically choose subnetworks based on available hardware resources.", "section": "1 Introduction"}, {"figure_path": "kk0Eaunc58/figures/figures_2_1.jpg", "caption": "Figure 3: In this figure, we illustrate an example of how we extract a subnetwork with 4 heads in MHA with a total number of 6 heads. In HydraViT, with the stochastic dropout training, we order the attention heads in MHA and consequently their corresponding embedding vectors based on their importance.", "description": "This figure illustrates how HydraViT extracts subnetworks.  It shows a multi-head attention (MHA) layer with six heads (H1-H6).  During stochastic dropout training, the heads are ordered by importance.  The figure highlights how a subnetwork is created by selecting the top k most important heads and their corresponding embedding vectors. In this example, a subnetwork with four heads is shown.  The importance ordering of heads allows HydraViT to dynamically select the appropriate subnetwork at inference time based on available hardware resources.", "section": "Contributions"}, {"figure_path": "kk0Eaunc58/figures/figures_3_1.jpg", "caption": "Figure 4: An illustration of subnetwork extraction within MLP and NORM layers, introduced in HydraViT. Fig. 4a demonstrates how HydraViT slices activations, denoted as A1 and A2, along with their respective weight matrices, denoted as W\u2081 and W2, based on the number of utilized heads. Also, Fig. 4b shows how HydraViT applies normalization on the activation corresponding to the used heads. For simplicity, only subnetworks with 3, 6, and 12 heads, corresponding to ViT-Ti, ViT-S, and ViT-B respectively, are presented.", "description": "This figure illustrates how HydraViT extracts subnetworks from the MLP and NORM layers.  It shows how HydraViT selects portions of activation maps and weight matrices based on the number of heads used, effectively creating subnetworks of varying sizes.  The figure uses the examples of 3, 6, and 12 heads (corresponding to ViT-Ti, ViT-S, and ViT-B) for simplicity and clarity.", "section": "3 HydraViT"}, {"figure_path": "kk0Eaunc58/figures/figures_4_1.jpg", "caption": "Figure 1: Performance comparison of HydraViT and baselines on ImageNet-1K in terms of GMACs (a) and throughput (b) evaluated on NVIDIA A100 80GB PCIe. HydraViT trained on 3\u201312 heads demonstrates superior performance over DynaBERT (Hou et al., 2020) and SortedNet (Valipour et al., 2023). While MatFormer (Kudugunta et al., 2023) shows higher performance than HydraViT within its limited scalability range, but when we train on a narrower scalability range (9\u201312 heads), HydraViT surpasses MatFormer. We also show that training HydraViT for more epochs can further improve accuracy. Note that each line corresponds to one model, and changing the number of heads in the vanilla DeiT models significantly drops their accuracy to less than 30%.", "description": "This figure compares the performance of HydraViT against several baselines (DeiT, DynaBERT, MatFormer, SortedNet) on the ImageNet-1K dataset.  Two plots are shown: one illustrating the relationship between GMACs (giga multiply-accumulate operations) and accuracy, and the other showing the relationship between throughput and accuracy.  HydraViT demonstrates improved accuracy compared to the baselines at similar GMACs and throughput levels, especially when trained with 9-12 heads, showcasing its scalability and efficiency.", "section": "Abstract"}, {"figure_path": "kk0Eaunc58/figures/figures_4_2.jpg", "caption": "Figure 5: Stochastic tail-drop training.", "description": "This figure illustrates the stochastic dropout training process in HydraViT.  In each iteration, a subset of attention heads (a subnetwork) is randomly selected using a uniform distribution. The selected heads and their corresponding embedding vectors are involved in the backpropagation and forward paths.  After training, this process results in an ordering of the attention heads based on their importance.  The most important heads are used first, facilitating flexible and efficient inference using only the most relevant parts of the network.", "section": "3 HydraViT"}, {"figure_path": "kk0Eaunc58/figures/figures_6_1.jpg", "caption": "Figure 6: Performance comparison of HydraViT and baselines in terms of RAM usage. Similar to Figure 1, HydraViT achieves the best accuracy per RAM usage.", "description": "This figure compares the performance of HydraViT against several baseline models in terms of RAM usage and accuracy.  It shows that HydraViT achieves superior accuracy for a given amount of RAM compared to other models like DeiT, DynaBERT, MatFormer, and SortedNet.  This highlights HydraViT's efficiency in resource-constrained environments. The figure plots accuracy on the y-axis and RAM usage in MB on the x-axis, clearly demonstrating HydraViT's advantage.", "section": "4 Evaluation"}, {"figure_path": "kk0Eaunc58/figures/figures_6_2.jpg", "caption": "Figure 7: t-SNE representation of the last layer of HydraViT with different numbers of heads. As we can see, having more heads leads to more compact representations.", "description": "This figure shows a t-SNE (t-distributed Stochastic Neighbor Embedding) visualization of the feature representations from the last layer of the HydraViT model.  Each point represents a data sample's feature vector.  The color of each point indicates the number of attention heads used in the corresponding HydraViT subnetwork (ranging from 3 to 12 heads). The visualization demonstrates that as the number of heads increases, the feature representations become more compact and clustered, suggesting improved representation efficiency and potentially better performance.", "section": "4 Evaluation"}, {"figure_path": "kk0Eaunc58/figures/figures_8_1.jpg", "caption": "Figure 1: Performance comparison of HydraViT and baselines on ImageNet-1K in terms of GMACs (a) and throughput (b) evaluated on NVIDIA A100 80GB PCIe. HydraViT trained on 3\u201312 heads demonstrates superior performance over DynaBERT (Hou et al., 2020) and SortedNet (Valipour et al., 2023). While MatFormer (Kudugunta et al., 2023) shows higher performance than HydraViT within its limited scalability range, but when we train on a narrower scalability range (9\u201312 heads), HydraViT surpasses MatFormer. We also show that training HydraViT for more epochs can further improve accuracy. Note that each line corresponds to one model, and changing the number of heads in the vanilla DeiT models significantly drops their accuracy to less than 30%.", "description": "This figure compares the performance of HydraViT against several baselines (DeiT, DynaBERT, MatFormer, and SortedNet) on the ImageNet-1K dataset.  The comparison is shown using two plots: one illustrating the relationship between GMACs (giga multiply-accumulate operations, a measure of computational complexity) and accuracy, and the other showing the relationship between throughput (images processed per second) and accuracy.  The results demonstrate that HydraViT, particularly when trained with a wider range of heads (9-12), achieves higher accuracy for a given level of computational cost and throughput compared to the baselines.", "section": "Abstract"}, {"figure_path": "kk0Eaunc58/figures/figures_9_1.jpg", "caption": "Figure 9: Performance comparison of HydraViT and baselines in terms of GMACs on ImageNet-v2.", "description": "This figure shows the performance comparison of HydraViT and several baseline models (DeiT, DynaBERT, MatFormer, SortedNet) on the ImageNet-V2 dataset.  The x-axis represents the number of GMACs (giga multiply-accumulate operations), a measure of computational cost, while the y-axis represents the accuracy achieved.  The graph plots the accuracy of each model against its computational cost.  It allows for comparison of the efficiency and accuracy trade-offs among different architectures. HydraViT is shown in multiple variations, indicating performance with different training epochs and a narrower head range. This illustrates how the model scales its performance based on resource constraints.", "section": "4 Evaluation"}, {"figure_path": "kk0Eaunc58/figures/figures_9_2.jpg", "caption": "Figure 10: Performance comparison of HydraViT and baselines in terms of GMACs on ImageNet-R.", "description": "This figure compares the performance of HydraViT against several baseline models (DeiT, DynaBERT, MatFormer, SortedNet) on the ImageNet-R dataset.  The comparison is made using GMACs (giga multiply-accumulate operations) as a measure of computational cost.  Each line represents a different model, showing how accuracy varies with increasing computational complexity. HydraViT and its variants trained for 800 epochs and with a focus on 9-12 heads demonstrate competitive accuracy compared to other baselines, particularly in the lower GMAC range.", "section": "4 Evaluation"}, {"figure_path": "kk0Eaunc58/figures/figures_14_1.jpg", "caption": "Figure 11: Training loss for all subnetworks when training HydraViT for 800 epochs.", "description": "This figure shows the training loss curves for each subnetwork (3 to 12 heads) of HydraViT trained for 800 epochs.  It demonstrates the training process of the model, showing how the loss decreases for each subnetwork over time.  This illustrates that stochastic dropout training effectively minimizes the loss for all subnetworks, with no significant overfitting on any individual subnetwork even with the extensive training duration.", "section": "B Generalization of Submodels"}, {"figure_path": "kk0Eaunc58/figures/figures_16_1.jpg", "caption": "Figure 1: Performance comparison of HydraViT and baselines on ImageNet-1K in terms of GMACs (a) and throughput (b) evaluated on NVIDIA A100 80GB PCIe. HydraViT trained on 3\u201312 heads demonstrates superior performance over DynaBERT (Hou et al., 2020) and SortedNet (Valipour et al., 2023). While MatFormer (Kudugunta et al., 2023) shows higher performance than HydraViT within its limited scalability range, but when we train on a narrower scalability range (9\u201312 heads), HydraViT surpasses MatFormer. We also show that training HydraViT for more epochs can further improve accuracy. Note that each line corresponds to one model, and changing the number of heads in the vanilla DeiT models significantly drops their accuracy to less than 30%.", "description": "The figure presents a performance comparison of HydraViT against other models (DynaBERT, SortedNet, MatFormer, DeiT) on the ImageNet-1K dataset.  It uses two graphs: one showing accuracy vs. GMACs (giga multiply-accumulate operations), and the other showing accuracy vs. throughput.  HydraViT demonstrates better performance across various resource constraints, especially when trained with 9-12 heads.", "section": "Abstract"}]