{"references": [{"fullname_first_author": "Tom B Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-05-14", "reason": "This paper is foundational to the field of large language models, which are leveraged in this research."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-12", "reason": "CLIP, introduced in this paper, is the core vision-language model used in the experiments of this research."}, {"fullname_first_author": "Dan Hendrycks", "paper_title": "Benchmarking neural network robustness to common corruptions and perturbations", "publication_date": "2019-03-27", "reason": "This paper establishes a standard benchmark for evaluating out-of-distribution detection, a key problem addressed in this research."}, {"fullname_first_author": "Dan Hendrycks", "paper_title": "Deep anomaly detection with outlier exposure", "publication_date": "2018-12-18", "reason": "This research significantly advances the field of deep anomaly detection, which is directly relevant to this paper's focus on unwanted visual data."}, {"fullname_first_author": "Kimin Lee", "paper_title": "Training confidence-calibrated classifiers for detecting out-of-distribution samples", "publication_date": "2017-11-17", "reason": "This paper provides a foundational approach to out-of-distribution detection, a crucial element in this research's method for identifying unwanted visual data."}]}