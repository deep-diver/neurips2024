[{"figure_path": "N2wYPMpifA/tables/tables_13_1.jpg", "caption": "Table 2: Default hyperparameters for all training jobs. All training was done on four RTX 6000s.", "description": "This table lists the default hyperparameter settings used for all training jobs in the research.  It specifies architectural hyperparameters such as the number of layers, attention heads, embedding dimension, and context length, as well as optimization hyperparameters including the optimizer used (AdamW), learning rate scheduling (linear warmup and cosine decay), and batch size.  All training was performed on four NVIDIA RTX 6000 GPUs.", "section": "A Pretraining Hyperparameters"}]