[{"figure_path": "Qk3IBHyv6z/figures/figures_4_1.jpg", "caption": "Figure 1: Under expressive enough reward function and deviation classes, regret equivalence implies value equivalence but not vice versa, making the regret gap a \"stronger\" objective than the value gap.", "description": "This figure summarizes the relationship between value equivalence and regret equivalence in the context of multi-agent imitation learning (MAIL).  It shows that under sufficiently expressive reward functions and agent deviation classes, achieving regret equivalence (where the learner's policy is as good as the expert's, even considering strategic agent deviations) implies value equivalence (where the learner's and expert's policies have the same value for all agents under obedient behavior). However, the converse is not true; value equivalence does not guarantee regret equivalence. This highlights that regret equivalence is a stronger requirement than value equivalence in MAIL, reflecting the challenge of accounting for strategic agent behavior.", "section": "4 On the Relationship between the Value Gap and the Regret Gap"}, {"figure_path": "Qk3IBHyv6z/figures/figures_5_1.jpg", "caption": "Figure 2: Illustration of a Markov Game that captures why \u201cregret is hard\u201d. Here, \u03c3\u03b5(\u03b11\u03b11|s0) = 1. Observe that s1 is un-visited when all agents obediently follow \u03c3\u03b5 but is with probability 1 under deviation \u03c61 (\u03c61(s0, a1) = \u03c61(s1, a1) = a2). This means that unless we know what the expert \u03c3\u03b5 would have recommended counter-factually in s1, we cannot minimize the regret gap.", "description": "The figure shows a Markov Game where the expert policy only visits states s2, s4,...s2H.  However, if an agent deviates from the expert's recommendation, state s1 becomes reachable.  This highlights the challenge of minimizing regret in multi-agent imitation learning, as the learner needs to know what the expert would have done in these unvisited states (counterfactual scenarios).", "section": "On the Relationship between the Value Gap and the Regret Gap"}, {"figure_path": "Qk3IBHyv6z/figures/figures_16_1.jpg", "caption": "Figure 3: Example of \u03a9(\u03b5\u03c5H) regret gap for J-BC and J-IRL", "description": "This figure illustrates a Markov game where the regret gap is \u03a9(\u03b5\u03c5H) for both J-BC and J-IRL algorithms. The game consists of 2H states arranged in two parallel chains. The expert policy (\u03c3\u03b5) focuses on one chain, while the learner's policy (\u03c3) matches it perfectly on the states visited by \u03c3\u03b5. However, a strategic deviation by an agent creates a counterfactual scenario where the expert's actions are unknown, resulting in a large regret gap.  This highlights that minimizing the value gap does not guarantee minimizing the regret gap in multi-agent scenarios.", "section": "4.4 Efficient Algorithms for Minimizing the Value Gap"}, {"figure_path": "Qk3IBHyv6z/figures/figures_19_1.jpg", "caption": "Figure 3: Example of \u03a9(\u03b5\u03c5H) regret gap for J-BC and J-IRL", "description": "This figure illustrates an example Markov Game demonstrating that the regret gap for both Joint Behavioral Cloning (J-BC) and Joint Inverse Reinforcement Learning (J-IRL) can be as large as \u03a9(\u03b5\u03c5H).  The figure highlights a scenario where, even with small value gaps, large regret gaps can emerge due to the difference between the expert and learner's behaviour in states that are not visited under the expert's policy (but are visited when an agent deviates). This underscores the difficulty of achieving regret equivalence in multi-agent imitation learning.", "section": "4.2 Value Equivalence \u21d2 Regret Equivalence"}]