[{"figure_path": "6CFHg7exjY/tables/tables_8_1.jpg", "caption": "Table 2: Effects of training dataset size on NDCG@K on the KUAIREC and YAHOO! R3 datasets.", "description": "This table presents the NDCG@K performance on the KUAIREC and YAHOO! R3 datasets using different methods (DR, ESCM2-DR, BRD-DR, Bal-DR, Res-DR, and MetaDebias) with varying training dataset sizes (10%, 20%, 40%, 60%, 80%, and 100%). It demonstrates how the performance of each method changes with the amount of training data.", "section": "4.2 Experimental Results"}, {"figure_path": "6CFHg7exjY/tables/tables_8_2.jpg", "caption": "Table 2: Effects of training dataset size on NDCG@K on the KUAIREC and YAHOO! R3 datasets.", "description": "This table shows the NDCG@K scores achieved by different recommendation models on the KUAIREC and YAHOO! R3 datasets with varying training dataset sizes (10%, 20%, 40%, 60%, 80%, and 100%).  It demonstrates the effect of the training data size on the performance of each model in terms of NDCG@K.  The results highlight the impact of data availability on the model's ability to effectively learn and predict user preferences.", "section": "4.2 Experimental Results"}, {"figure_path": "6CFHg7exjY/tables/tables_17_1.jpg", "caption": "Table 3: Effects of training data size on AUC on the KUAIREC and YAHOO! R3 datasets.", "description": "This table presents the results of an experiment evaluating the impact of training dataset size on the Area Under the Curve (AUC) metric.  The experiment was conducted on two datasets, KUAIREC and YAHOO! R3.  Multiple methods (DR, ESCM\u00b2-DR, BRD-DR, Bal-DR, Res-DR, and MetaDebias) were compared, showing their AUC scores at various training dataset sizes (10%, 20%, 40%, 60%, 80%, and 100%). The table allows for a comparison of the performance of different debiasing methods under varying data availability.", "section": "4.2 Experimental Results"}, {"figure_path": "6CFHg7exjY/tables/tables_17_2.jpg", "caption": "Table 3: Effects of training data size on AUC on the KUAIREC and YAHOO! R3 datasets.", "description": "This table presents the results of an experiment evaluating the impact of training dataset size on the Area Under the Curve (AUC) metric.  The experiment was conducted on two datasets: KUAIREC and YAHOO! R3. Multiple methods (DR, ESCM\u00b2-DR, BRD-DR, Bal-DR, Res-DR, and MetaDebias) were compared using training datasets of varying sizes (10%, 20%, 40%, 60%, 80%, and 100%). The AUC values for each method and dataset size are shown, illustrating the effect of data quantity on model performance.", "section": "4.2 Experimental Results"}, {"figure_path": "6CFHg7exjY/tables/tables_18_1.jpg", "caption": "Table 1: Performance on AUC, Recall@K and NDCG@K on the COAT, YAHOO! R3 and KUAIREC datasets. The best result is bolded and the best results of both types of baseline methods are underlined, where * means statistically significant results (p-value < 0.05) using the paired-t-test.", "description": "This table presents the performance comparison of different recommendation models on three benchmark datasets (COAT, YAHOO! R3, and KUAIREC) across three evaluation metrics: AUC, Recall@K, and NDCG@K.  The best-performing model for each metric and dataset is highlighted in bold.  Additionally, the best performing model among the RCT data-free and RCT data-based methods are underlined.  The '*' indicates statistically significant differences (p<0.05) based on paired t-tests.", "section": "4.2 Experimental Results"}, {"figure_path": "6CFHg7exjY/tables/tables_18_2.jpg", "caption": "Table 4: Effects of training dataset size on Recall@K on the KUAIREC and YAHOO! R3 datasets.", "description": "This table shows the performance (Recall@K) of different recommendation models under varying training dataset sizes.  The models are evaluated on two datasets: KUAIREC and YAHOO! R3. The results demonstrate how the model's performance changes as the amount of training data increases, indicating the impact of data size on model training and generalization.  The table allows for a comparison of different model's sensitivity to varying training data amounts.  The Recall@K metric provides insights into each models' ability to accurately retrieve relevant items in the top K recommendations.", "section": "4.2 Experimental Results"}, {"figure_path": "6CFHg7exjY/tables/tables_18_3.jpg", "caption": "Table 1: Performance on AUC, Recall@K and NDCG@K on the COAT, YAHOO! R3 and KUAIREC datasets. The best result is bolded and the best results of both types of baseline methods are underlined, where * means statistically significant results (p-value < 0.05) using the paired-t-test.", "description": "This table presents the performance comparison of different recommendation models on three datasets (COAT, YAHOO! R3, and KUAIREC) using three evaluation metrics (AUC, Recall@K, NDCG@K).  The best overall performance for each metric and dataset is shown in bold, while the best results among both RCT data-free and RCT data-based methods are underlined.  Statistical significance (p<0.05) using paired t-test is indicated with an asterisk.", "section": "4.2 Experimental Results"}]