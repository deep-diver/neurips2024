[{"figure_path": "UoxuaOGV6B/figures/figures_1_1.jpg", "caption": "Figure 1: Training loss of fine-tuning Llama3 8B model with Orca Math dataset [38] and evaluation score on GSM8K benchmark [8]. We follow experimental setup in [53], see Appendix F.1 for details. All methods except full fine-tuning maintain approximately 0.23% trainable parameters.", "description": "This figure shows a comparison of different parameter-efficient fine-tuning (PEFT) methods on the Llama 3 8B language model.  The left panel displays the training loss curves for full fine-tuning, LoRA, DoRA, Orthogonal Fine-Tuning (OFT), AdaLoRA, and the proposed Spectral Adapter, using the Orca Math dataset. The right panel shows the corresponding GSM8K accuracy.  The key takeaway is that Spectral Adapter achieves a performance closest to full fine-tuning, while maintaining a significantly smaller number of trainable parameters (only 0.23%).", "section": "1 Introduction"}, {"figure_path": "UoxuaOGV6B/figures/figures_2_1.jpg", "caption": "Figure 2: Compared to LoRA which proposes to add low-rank trainable matrices to pretrained weights, we study two types of spectral adapters: Spectral Adapter<sup>A</sup> considers additively tuning the top columns of singular vector matrices and Spectral Adapter<sup>R</sup> considers orthogonally rotating the top columns of singular vector matrices.", "description": "This figure illustrates the two proposed spectral adapter methods in comparison to the LoRA method.  LoRA adds a low-rank trainable matrix to pretrained weights.  In contrast, the Spectral Adapter<sup>A</sup> method additively tunes the top singular vectors (columns of U and V matrices from the SVD decomposition of the pretrained weight matrix W), and the Spectral Adapter<sup>R</sup> method orthogonally rotates those top singular vectors. Both methods leverage spectral information from the pretrained weights for efficient fine-tuning.", "section": "2 Spectral Adapter: Incorporating Spectral Information into Fine-Tuning"}, {"figure_path": "UoxuaOGV6B/figures/figures_3_1.jpg", "caption": "Figure 3: Top singular vector of pretrained weight recognizes more ideal neuron direction. Illustration plot for Section 3.2.", "description": "This figure illustrates the alignment of the top singular vector of pretrained weights with the ideal neuron direction in a simplified two-layer ReLU network.  The data points are assumed to lie on a two-dimensional plane (xy-plane), representing a low-dimensional manifold.  The pretrained neuron directions (u\u2081, u\u2082, u\u2083, u\u2084) are shown, with most lying in the xy-plane. However, due to optimization errors, some neurons deviate slightly from this plane. The top singular vector (u*) of the pretrained weight matrix W(1) is shown to align closely with the xy-plane, indicating that it recognizes the ideal neuron direction more accurately than individual pretrained neurons. This observation supports the choice of tuning the top singular vectors in the proposed spectral adaptation method.", "section": "3.2 Weight Subspace Alignment"}, {"figure_path": "UoxuaOGV6B/figures/figures_5_1.jpg", "caption": "Figure 4: Distributing different concept tunings along different spectral space helps with identity preservation in multi-adapter fusion, see Section 4.2 for details.", "description": "This figure illustrates the process of multi-adapter fusion using the proposed spectral adapter method.  It shows how different concepts (e.g., images of three different people) can be incorporated into the spectral space of a pre-trained diffusion model without causing identity loss or concept mixing. The process involves distributing each concept along a different set of spectral dimensions, then collecting the individual adapters and adding them together to create the final merged adapter. This approach is contrasted with simpler methods of adapter fusion that struggle to maintain individual object characteristics.", "section": "4.2 Diffusion Model Fusion: Improving Multi-Object Fine-Tuning with Spectral Adapter\u2074"}, {"figure_path": "UoxuaOGV6B/figures/figures_6_1.jpg", "caption": "Figure 4: Distributing different concept tunings along different spectral space helps with identity preservation in multi-adapter fusion, see Section 4.2 for details.", "description": "This figure illustrates the proposed method for multi-adapter fusion in diffusion models.  It shows how different concepts (e.g., different objects or characteristics) can be assigned to different parts of the spectral space of pretrained model weights. This allows the model to fine-tune multiple concepts simultaneously without interference or identity loss. The figure showcases a four-step process: (A) Separate concepts are initially encoded in different spectral spaces, (B) Individual spectral adapters are collected, (C) These adapters are added together, and (D) The merged spectral adapters are used for synthesis. This approach addresses the challenge of multi-adapter fusion in diffusion models, where simply adding adapters together often leads to problems like concept mixing or loss of identity.", "section": "4.2 Diffusion Model Fusion: Improving Multi-Object Fine-Tuning with Spectral Adapter4"}, {"figure_path": "UoxuaOGV6B/figures/figures_7_1.jpg", "caption": "Figure 2: Compared to LoRA which proposes to add low-rank trainable matrices to pretrained weights, we study two types of spectral adapters: Spectral Adapter<sup>A</sup> considers additively tuning the top columns of singular vector matrices and Spectral Adapter<sup>R</sup> considers orthogonally rotating the top columns of singular vector matrices.", "description": "This figure illustrates the architecture of the proposed Spectral Adapter and compares it with the LoRA method.  LoRA adds a low-rank trainable matrix to the pretrained weights. In contrast, the Spectral Adapter has two variants: Spectral Adapter<sup>A</sup>, which additively tunes the top singular vectors of the pretrained weight matrix, and Spectral Adapter<sup>R</sup>, which orthogonally rotates them.  Both methods utilize Singular Value Decomposition (SVD) of the pretrained weights before fine-tuning.", "section": "2 Spectral Adapter: Incorporating Spectral Information into Fine-Tuning"}, {"figure_path": "UoxuaOGV6B/figures/figures_8_1.jpg", "caption": "Figure 6: Generation results for prompt \u201ca <Vvase> on a table\u201d after fine-tuning Chilloutmix diffusion model [1] on custom vase images with different PEFT methods. See Section 4.3 for details.", "description": "This figure compares the generation results of various parameter-efficient fine-tuning (PEFT) methods on a custom vase concept using the Chilloutmix diffusion model.  The x-axis represents the number of trainable parameters used, ranging from 1K to over 800K. Each column shows the generated vase images for a given method and parameter budget, demonstrating the impact of different PEFT methods on image generation quality and parameter efficiency.  The reference column displays the actual images used for training.  The alignment scores shown below each image indicate how well the generated image matches the reference images, illustrating the ability of each method to generate accurate and realistic vases with different levels of parameter usage. The Spectral Adapter consistently outperforms the other methods.", "section": "4.3 Diffusion Model Expressiveness: Improving Parameter Efficiency with Spectral AdapterR"}, {"figure_path": "UoxuaOGV6B/figures/figures_8_2.jpg", "caption": "Figure 6: Generation results for prompt \u201ca <Vvase> on a table\u201d after fine-tuning Chilloutmix diffusion model [1] on custom vase images with different PEFT methods. See Section 4.3 for details.", "description": "This figure compares the performance of various parameter-efficient fine-tuning (PEFT) methods on a custom vase concept generation task using the Chilloutmix diffusion model.  The x-axis shows the number of trainable parameters used, while the y-axis implicitly represents the quality of the generated images.  The results demonstrate that Spectral Adapter achieves comparable or better image generation quality with significantly fewer trainable parameters compared to other methods such as LoRA, OFT, LiDB, SVDiff, and VeRA. This highlights the effectiveness of Spectral Adapter in enhancing parameter efficiency for fine-tuning.", "section": "4.3 Diffusion Model Expressiveness: Improving Parameter Efficiency with Spectral AdapterR"}, {"figure_path": "UoxuaOGV6B/figures/figures_9_1.jpg", "caption": "Figure 8: Runtime and GPU storage cost plot. See Section 4.4 for details.", "description": "This figure compares the runtime and GPU storage costs of LoRA, SVD, and Spectral Adapter^ when fine-tuning a 1B parameter diffusion model and a 7B parameter Mistral language model.  It demonstrates that Spectral Adapter^ incurs minimal additional overhead compared to LoRA, addressing potential concerns about the computational cost of the proposed method.", "section": "4.4 Final Note: A Closer Look at SVD Cost"}, {"figure_path": "UoxuaOGV6B/figures/figures_16_1.jpg", "caption": "Figure 9: More experiments with Llama3 8B model with different number of trainable parameters. In the left plot, the training loss of LoRA and DORA overlaps. See Appendix F.1 for details.", "description": "This figure shows the training loss curves for fine-tuning a Llama3 8B model on the Orca Math dataset using different parameter-efficient fine-tuning (PEFT) methods.  The left panel shows results for a rank of 4, while the right panel shows results for a rank of 64.  The figure demonstrates the impact of the number of trainable parameters on the training loss, and highlights that the proposed Spectral Adapter consistently outperforms other PEFT methods.", "section": "4 Empirical Results: The Impact of Spectral Information"}, {"figure_path": "UoxuaOGV6B/figures/figures_17_1.jpg", "caption": "Figure 1: Training loss of fine-tuning Llama3 8B model with Orca Math dataset [38] and evaluation score on GSM8K benchmark [8]. We follow experimental setup in [53], see Appendix F.1 for details. All methods except full fine-tuning maintain approximately 0.23% trainable parameters.", "description": "The figure shows the training loss curves and GSM8K accuracy for different parameter-efficient fine-tuning (PEFT) methods compared to full fine-tuning on the Llama 3 8B model using the Orca Math dataset.  Spectral Adapter achieves results closest to full fine-tuning while maintaining minimal trainable parameters (0.23%).", "section": "1 Introduction"}, {"figure_path": "UoxuaOGV6B/figures/figures_18_1.jpg", "caption": "Figure 11: Generation results for single toy concept tuning with LoRA, Orthogonal Adaptation, and Spectral Adapter4 with top and bottom ranks tuned respectively.", "description": "This figure compares the generation results of four different methods: LoRA, Orthogonal Adaptation, Spectral Adapter (top ranks), and Spectral Adapter (bottom ranks). Each method was used for single toy concept tuning, and the results for four different prompts are shown. The prompts describe various scenarios involving different toys. The \"reference\" row shows images of the four toys, which serve as the basis for comparison. The figure aims to illustrate that the spectral adapter performs better than the other methods when only tuning the top ranks of singular values, as shown in the caption.", "section": "4.2 Diffusion Model Fusion: Improving Multi-Object Fine-Tuning with Spectral Adapter^"}, {"figure_path": "UoxuaOGV6B/figures/figures_19_1.jpg", "caption": "Figure 12: Generation results for single animal concept tuning with LoRA, Orthogonal Adaptation, and Spectral Adapter<sup>A</sup> with top and bottom ranks tuned respectively.", "description": "This figure compares the image generation results of three different fine-tuning methods: LoRA, Orthogonal Adaptation, and Spectral Adapter (with top and bottom ranks).  Each method was used to fine-tune a diffusion model on three individual animal concepts (dog1, cat, dog2).  The prompts used varied the animal's activity and location. The \"reference\" row shows the original images used for training. The results demonstrate the effectiveness of the Spectral Adapter (particularly when tuning the top ranks) in generating images that are more aligned with the prompts compared to the other methods.  Orthogonal Adaptation shows some success, but less overall than Spectral Adapter.", "section": "4.2 Diffusion Model Fusion: Improving Multi-Object Fine-Tuning with Spectral Adapter<sup>A</sup>"}, {"figure_path": "UoxuaOGV6B/figures/figures_20_1.jpg", "caption": "Figure 13: Generation results of Chilloutmix diffusion model [1] tuned on four custom toy concepts with different fused adapters. See Appendix F.5.2 for details.", "description": "This figure demonstrates the results of multi-object generation using the Chilloutmix diffusion model.  Four custom toy concepts were used, each tuned with different adapters. The figure compares results from four different fusion methods: Gradient Fusion, FedAvg, Orthogonal Adaptation, and the proposed Spectral Adapter. The goal is to show how well each method preserves individual object identities while fusing multiple adapters.  Each row displays the generated images from a different method for three different prompts. The \"reference\" column shows the real-world toy images used as the training data.", "section": "4.2 Diffusion Model Fusion: Improving Multi-Object Fine-Tuning with Spectral Adapter<sup>4</sup>"}, {"figure_path": "UoxuaOGV6B/figures/figures_20_2.jpg", "caption": "Figure 14: Generation results of Chilloutmix diffusion model [1] tuned on photos of three computer scientists with different fused adapters. See Appendix F.5.2 for details.", "description": "This figure shows the results of generating images of three computer scientists (Yoshua Bengio, Yann LeCun, and Geoffrey Hinton) using a diffusion model fine-tuned with different multi-adapter fusion methods. The methods compared are Gradient Fusion, FedAvg, Orthogonal Adaptation, and the proposed Spectral Adapter. The prompt used was: \"\"\" <Vbengio> and <Vlecun> and <Vhinton>, standing near a lake, 4K, high quality, high resolution.\"\"\". The figure demonstrates the ability of the Spectral Adapter to generate images with clearer and more consistent styles across characters compared to the other methods. Appendix F.5.2 provides further details on the experimental setup.", "section": "4.2 Diffusion Model Fusion: Improving Multi-Object Fine-Tuning with Spectral Adapter^"}, {"figure_path": "UoxuaOGV6B/figures/figures_21_1.jpg", "caption": "Figure 6: Generation results for prompt \u201ca <Vvase> on a table\u201d after fine-tuning Chilloutmix diffusion model [1] on custom vase images with different PEFT methods. See Section 4.3 for details.", "description": "This figure compares the performance of various parameter-efficient fine-tuning (PEFT) methods on a custom vase concept generation task using the Chilloutmix diffusion model.  Different methods are compared at various parameter budgets (1k, 20k, 100k, 200k, 300k, 400k, 500k, and >800k parameters). The results show generated images for each method and parameter budget, along with the alignment score reflecting how well the generated image matches the reference image.  The figure highlights the ability of Spectral Adapter to generate high-quality images with fewer trainable parameters compared to other methods.", "section": "4.3 Diffusion Model Expressiveness: Improving Parameter Efficiency with Spectral AdapterR"}]