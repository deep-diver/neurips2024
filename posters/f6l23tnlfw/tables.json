[{"figure_path": "F6L23TNlFW/tables/tables_8_1.jpg", "caption": "Table 1: Results shown as \u201cmean\u00b1std\u201d, where bold and italics denote the 1st and 2nd, respectively.", "description": "This table presents the performance comparison of different label distribution prediction methods on three datasets: JAFFE, Painting, and Music.  The methods are categorized into those using binary labels directly and those that adapt to ternary labels using three different strategies: data transformation (DT), mean squared error (MSE), and the proposed CateMO distribution.  For each method, the table shows the mean and standard deviation of five common evaluation metrics: Chebyshev distance (Cheb), Kullback-Leibler divergence (KL), cosine similarity (Cosine), intersection (Intersec), and Spearman\u2019s rank correlation coefficient (Rho). Lower values for Cheb and KL, and higher values for Cosine, Intersec, and Rho indicate better performance.  The ground-truth results are also included as a baseline.", "section": "5.3 Results and discussions"}, {"figure_path": "F6L23TNlFW/tables/tables_14_1.jpg", "caption": "Table 1: Results shown as \u201cmean\u00b1std", "description": "This table presents the performance comparison of different label enhancement algorithms (CateMO, MSE, DT, LL) combined with three different base LDL algorithms (MR, LR, GL) on three datasets (JAFFE, Painting, Music). The results are shown in terms of five metrics (Cheb, KL, Cosine, Intersec, Rho), each representing a different aspect of the label distribution prediction performance. Lower values for Cheb and KL, and higher values for Cosine, Intersec, and Rho indicate better performance.  The \"Ground-Truth\" row shows the performance when using the true label distributions for training.  The table also indicates the best-performing (bold) and second-best performing (italics) methods for each dataset and metric.", "section": "5.3 Results and discussions"}]