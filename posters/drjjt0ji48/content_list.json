[{"type": "text", "text": "Retrieval-Augmented Diffusion Models for Time Series Forecasting ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jingwei ${\\bf L i u^{1,2*}}$ Ling Yang3 \u2020 Hongyan $\\mathbf{Li}^{1,2}\\neq$ Shenda Hong3,4,5 \u2021 ", "page_idx": 0}, {"type": "text", "text": "1School of Intelligence Science and Technology, Peking University   \nNational Key Laboratory of General Artificial Intelligence, Peking University   \n3Institute of Medical Technology, Peking University Health Science Center 4 National Institute of Health Data Science, Peking University 5 Institute for Artificial Intelligence, Peking University jingweiliu1996@163.com, yangling0818@163.com {leehy, hongshenda}@pku.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "While time series diffusion models have received considerable focus from many recent works, the performance of existing models remains highly unstable. Factors limiting time series diffusion models include insufficient time series datasets and the absence of guidance. To address these limitations, we propose a RetrievalAugmented Time series Diffusion model (RATD). The framework of RATD consists of two parts: an embedding-based retrieval process and a reference-guided diffusion model. In the first part, RATD retrieves the time series that are most relevant to historical time series from the database as references. The references are utilized to guide the denoising process in the second part. Our approach allows leveraging meaningful samples within the database to aid in sampling, thus maximizing the utilization of datasets. Meanwhile, this reference-guided mechanism also compensates for the deficiencies of existing time series diffusion models in terms of guidance. Experiments and visualizations on multiple datasets demonstrate the effectiveness of our approach, particularly in complicated prediction tasks. Our code is available at https://github.com/stanliu96/RATD ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Time series forecasting plays a critical role in a variety of applications including weather forecasting [15, 11], finance forecasting [7, 5], earthquake prediction [19] and energy planning [6]. One way to approach time series forecasting tasks is to view them as conditional generation tasks [32, 42], where conditional generative models are used to learn the conditional distribution $P({\\pmb x}^{P}|{\\pmb x}^{H})$ of predicting the target time series $\\pmb{x}^{P}$ given the observed historical sequence $x^{H}$ . As the current state-of-the-art conditional generative model, diffusion models [12] have been utilized in many works for time series forecasting tasks [28, 36, 30]. ", "page_idx": 0}, {"type": "text", "text": "Although the performance of the existing time series diffusion models is reasonably well on some time series forecasting tasks, it remains unstable in certain scenarios (an example is provided in 1(c)). The factors limiting the performance of time series diffusion models are complex, two of them are particularly evident. First, most time series lack direct semantic or label correspondences, which often results in time series diffusion models lacking meaningful guidance during the generation process(such as text guidance or label guidance in image diffusion models). This also limits the potential of time series diffusion models. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "image", "img_path": "dRJJt0Ji48/tmp/739d65b899ecf13659dbc7385c65cdd5d1730d76695f0b02ba84c54818650e38.jpg", "img_caption": ["Figure 1: (a) The figure shows the differences in forecasting results between the CSDI [36] (left) and RATD (right). Due to the very small proportion of such cases in the training set, CSDI struggles to make accurate predictions, often predicting more common results. Our method, by retrieving meaningful references as guidance, makes much more accurate predictions. (b) A comparison between our method\u2019s framework(bottom) and the conventional time series diffusion model framework(top). (c) We randomly selected 25 forecasting tasks from the electricity dataset. Compared to our method, CSDI and MG-TSD [9] exhibited significantly higher instability. This indicates that the RATD is better at handling complex tasks that are challenging for the other two methods. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "The second limiting factor arises from two shortcomings of the time series datasets: size insufficient and imbalanced. Compared to image datasets, time series datasets typically have a smaller scale. Popular image datasets (such as LAION-400M) contain 400 million sample pairs, while most time series datasets usually only contain tens of thousands of data points. Training a diffusion model to learn the precise distribution of datasets with insufficient size is challenging. Additionally, real-world time series datasets exhibit significant imbalance. For example, in the existing electrocardiogram dataset MIMIC-IV, records related to diagnosed pre-excitation syndrome (PS) account for less than $0.025\\%$ of the total records. This imbalance phenomenon may cause models to overlook some extremely rare complex samples, leading to a tendency to generate more common predictions during training, thus making it difficult to handle complex prediction tasks, as illustrated in Figure 1. ", "page_idx": 1}, {"type": "text", "text": "To address these limitations, we propose the Retrieval-Augmented Time series Diffusion Model (RATD) for complex time series forecasting tasks. Our approach consists of two parts: the embeddingbased retrieval and the reference-guided diffusion model. After obtaining a historical time series, it is input into the embedding-based retrieval process to retrieve the $\\boldsymbol{\\mathrm{k}}$ nearest samples as references. The references are utilized as guidance in the denoising process. RATD focuses on making maximum utilization of existing time series datasets by finding the most relevant references in the dataset to the historical time series, thereby providing meaningful guidance for the denoising process. RATD focuses on maximizing the utilization of insufficient time series data and to some extent mitigates the issues caused by data imbalance. Meanwhile, this reference-guided mechanism also compensates for the deficiencies of guidance in existing time series diffusion models. Our approach demonstrates strong performance across multiple datasets, particularly on more complex tasks. ", "page_idx": 1}, {"type": "text", "text": "To summarize, our main contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 To handle complex time series forecasting, we for the first time introduce RetrievalAugmented Time series Diffusion (RATD), allowing for greater utilization of the dataset and providing meaningful guidance in the denoising process. \u2022 Extra Reference Modulated Attention (RMA) module is designed to provide reasonable guidance from the reference during the denoising process. RMA effectively simply integrates information without introducing excessive additional computational costs. \u2022 We conducted experiments on five real-world datasets and provided a comprehensive presentation and analysis of the results using multiple metrics. The experimental results demonstrate that our approach achieves comparable or better results compared to baselines. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Diffusion Models for Time Series Forecasting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Recent advancements have been made in the utilization of diffusion models for time series forecasting. In TimeGrad [28], the conditional diffusion model was first employed as an autoregressive approach for prediction, with the denoising process guided by the hidden state. CSDI [36] adopted a nonautoregressive generation strategy to achieve faster predictions. SSSD [1] replaced the noise-matching network with a structured state space model for prediction. TimeDiff [30] incorporated future mix-up and autoregressive initialization into a non-autoregressive framework for forecasting. MG-TSD [9] utilized a multi-scale generation strategy to sequentially predict the main components and details of the time series. Meanwhile, mr-diff [31] utilized diffusion models to separately predict the trend and seasonal components of time series. These methods have shown promising results in some prediction tasks, but they often perform poorly in challenging prediction tasks. We propose a retrieval-augmented framework to address this issue. ", "page_idx": 2}, {"type": "text", "text": "2.2 Retrival-Augmented Generation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The retrieval-augmented mechanism is one of the classic mechanisms for generative models. Numerous works have demonstrated the beneftis of incorporating explicit retrieval steps into neural networks. Classic works in the field of natural language processing leverage retrieval augmentation mechanisms to enhance the quality of language generation [16, 10, 4]. In the domain of image generation, some retrieval-augmented models focus on utilizing samples from the database to generate more realistic images [2, 44]. Similarly, [3] employed memorized similarity information from training data for retrieval during inference to enhance results. MQ-ReTCNN [40] is specifically designed for complex time series forecasting tasks involving multiple entities and variables. ReTime [13] creates a relation graph based on the temporal closeness between sequences and employs relational retrieval instead of content-based retrieval. Although the aforementioned three methods successfully utilize retrieval mechanisms to enhance time series forecasting results, our approach still holds significant advantages. This advantage stems from the iterative structure of the diffusion model, where references can repeatedly influence the generation process, allowing references to exert a stronger influence on the entire conditional generation process. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminary ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The forecasting task and the background knowledge about the conditional time series diffusion model will be discussed in this section. To avoid confilcts, we use the symbol \u201cs\" to represent the time series, and the \u201ct\" denotes the t-th step in the diffusion process. ", "page_idx": 2}, {"type": "text", "text": "Generative Time Series Forecasting. Suppose we have an observed historical time series $x^{H}=$ $\\left\\{s_{1},s_{2},\\cdot\\cdot\\cdot\\,,s_{l}\\;\\middle|\\;s_{i}\\;\\in\\;\\mathbb{R}^{d}\\right\\}$ , where $l$ is the historical time length, $d$ is the number of features per observation and $s_{i}$ is the observation at time step $i$ . The $\\pmb{x}^{P}$ is the corresponding prediction target $\\{s_{l+1},s_{l+2},\\cdot\\cdot\\cdot\\,,s_{l+h}\\,|\\,s_{l+i}\\,\\in\\,\\mathbb{R}^{d^{'}}\\}$ $(d^{'}\\leq\\ d)$ , where $h$ is the prediction horizon. The task of generative time series forecasting is to learn a density $p_{\\theta}({\\pmb x}^{P}|{\\pmb x}^{H})$ that best approximates $p({\\pmb x}^{P}|{\\pmb x}^{H})$ , which can be written as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{p_{\\theta}}D\\left(p_{\\theta}(\\pmb{x}^{P}|\\pmb{x}^{H})||p(\\pmb{x}^{P}|\\pmb{x}^{H})\\right),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\theta$ denotes parameters and $D$ is some appropriate measure of distance between distributions.   \nGiven observation $x$ the target time series can be obtained directly by sampling from $p_{\\theta}({\\pmb x}^{P}|{\\pmb x}^{H})$ .   \nTherefore, we obtain the time series $\\{s_{1},s_{2},\\cdot\\cdot\\cdot\\,,s_{n+h}\\}=[x^{H},x^{\\tilde{P}}]$ . ", "page_idx": 2}, {"type": "text", "text": "Conditional Time Series Diffusion Models. With observed time series $x^{H}$ , the diffusion model progressively destructs target time series $\\pmb{x}_{0}^{P}$ (equals to the $\\pmb{x}^{P}$ mentioned in the previous context) by injecting noise, then learns to reverse this process starting from $x_{T}^{P}$ for sample generation. For the convenience of expression, in this paper, we use $\\pmb{x}_{t}$ to refer to the t-th time series in the diffusion process, with the letter $\\mathbf{\\ddot{\\nabla}P^{\\prime\\prime}}$ omitted. The forward process can be formulated as a Gaussian process with a Markovian structure: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{q(\\pmb{x}_{t}|\\pmb{x}_{t-1}):=\\mathcal{N}(\\pmb{x}_{t};\\sqrt{1-\\beta_{t}}\\pmb{x}_{t-1},\\pmb{x}^{H},\\beta_{t}\\pmb{I}),}\\\\ {q(\\pmb{x}_{t}|\\pmb{x}_{0}):=\\mathcal{N}(\\pmb{x}_{t};\\sqrt{\\overline{{\\alpha_{t}}}}\\pmb{x}_{0},\\pmb{x}^{H},(1-\\overline{{\\alpha}}_{t})\\pmb{I}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "table", "img_path": "dRJJt0Ji48/tmp/be22fb3e79b82cd541f8cba7dcbba6492b996fc3193e89f4c7713d40747d31ed.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: Overview of the proposed RATD. The historical time series $x^{H}$ is inputted into the retrieval module to for the corresponding references $\\pmb{x}^{R}$ . After that, $x^{H}$ is concatenated with the noise as the main input for the model $\\mu_{\\theta}.\\;\\bar{{\\bf x}^{R}}$ will be utilized as the guidance for the denoising process. ", "page_idx": 3}, {"type": "text", "text": "where $\\beta_{1},\\ldots,\\beta_{T}$ denotes fixed variance schedule with $\\alpha_{t}:=1-\\beta_{t}$ and $\\begin{array}{r}{\\overline{{\\alpha}}_{t}:=\\prod_{s=1}^{t}\\alpha_{s}}\\end{array}$ . This forward process progressively injects noise into data until all structures are lost , which is wellapproximated by $\\mathcal{N}(0,I)$ . The reverse diffusion process learns a model $p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_{t},\\mathbf{x}^{H})$ that approximates the true posterior: ", "page_idx": 3}, {"type": "equation", "text": "$$\np_{\\theta}(\\ensuremath{\\mathbf{{x}}}_{t-1}|\\ensuremath{\\mathbf{{x}}}_{t},\\ensuremath{\\mathbf{{x}}}^{H}):=\\mathcal{N}(\\ensuremath{\\mathbf{{x}}}_{t-1};\\mu_{\\theta}(\\ensuremath{\\mathbf{{x}}}_{t}),\\Sigma_{\\theta}(\\ensuremath{\\mathbf{{x}}}_{t}),\\ensuremath{\\mathbf{{x}}}^{H}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mu_{\\theta}$ and $\\Sigma_{\\theta}$ are often computed by the Transformer. Ho et al. [12] improve the diffusion training process and optimize following objective: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\pmb{x}_{0})=\\sum_{t=1}^{T}\\underset{q(\\pmb{x}_{t}|\\pmb{x}_{0}|\\pmb{x}^{H})}{\\mathbb{E}}||\\mu_{\\theta}(\\pmb{x}_{t},t|\\pmb{x}^{H})-\\hat{\\mu}(\\pmb{x}_{t},\\pmb{x}_{0}|\\pmb{x}^{H})||^{2},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\hat{\\mu}(\\pmb{x}_{t},\\pmb{x}_{0}|\\pmb{x}^{H})$ is the mean of the posterior $q(\\mathbf{x}_{t-1}|\\mathbf{x}_{0},\\mathbf{x}_{t})$ which is a closed from Gaussian, and $\\mu_{\\theta}({\\dot{\\mathbf{x}}}_{t},t|{\\mathbf{x}}^{H})$ is the predicted mean of $p_{\\theta}(\\mathbf{x}_{t-1}\\mid\\mathbf{x}_{t}|\\mathbf{x}^{H})$ computed by a neural network. ", "page_idx": 3}, {"type": "text", "text": "4 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We first describe the overall architecture of the proposed method in 4.1. Then we will introduce the strategy of building datasets in Section 4.2. The embedding-based retrieval mechanisms and reference-guided time series diffusion model are introduced in Section 4.3. ", "page_idx": 3}, {"type": "text", "text": "4.1 Framework Overview ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Figure 2(a) shows the overall architecture of RATD. We built the entire process based on DiffWave [17], which combines the traditional diffusion model framework and a 2D transformer structure. In the forecasting task, RATD first retrieves motion sequences from the database base $\\mathcal{D}^{R}$ based on the input sequence of historical events. These retrieved samples are then fed into the ReferenceModulated Attention (RMA) as references. In the RMA layer, we integrate the features of the input $[{\\boldsymbol{x}}^{H},{\\boldsymbol{x}}^{t}]$ at time step t with side information $\\mathcal{Z}_{s}$ and the references $\\pmb{x}^{\\mathbf{\\breve{R}}}$ . Through this integration, the references guide the generation process. We will introduce these processes in the following subsections. ", "page_idx": 3}, {"type": "text", "text": "4.2 Constructing Retrieval Database for Time Series ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Before retrieval, it is necessary to construct a proper database. We propose a strategy for constructing databases from time series datasets with different characteristics. Some time series datasets are sizeinsufficient and are difficult to annotate with a single category label (e.g., electricity time series), while some datasets contain complete category labels but exhibit a significant degree of class imbalance (e.g., medical time series). We use two different definitions of databases for these two different types of datasets. For the first definition, the entire training set is directly defined as the database $\\mathcal{D}^{\\mathcal{R}}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{\\mathcal{D}}^{\\mathcal{R}}:=\\{\\mathbf{x}_{i}|\\forall\\mathbf{x}_{i}\\in\\mathcal{D}^{\\mathrm{train}}\\}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\pmb{x}_{i}=\\{s_{i},\\cdot\\cdot\\cdot,s_{i+l+h}\\}$ is the time series with length $l+h$ , and $\\mathcal{D}^{\\mathrm{train}}$ is the training set. In the second way, the subset containing samples from all categories in the dataset is defined as the database ", "page_idx": 3}, {"type": "image", "img_path": "dRJJt0Ji48/tmp/9edc4ffb61b8be4eafd6c9a24018ca2f32a64d973170987e23d4755a4ec4f55a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 3: The structure of $\\mu_{\\theta}$ . (a) The main architecture of $\\mu_{\\theta}$ is the time series transformer structure that proved effective. (b) The structure of the proposed RMA. We integrate three different features through matrix multiplication. ", "page_idx": 4}, {"type": "text", "text": "$\\mathcal{D}^{R^{\\prime}}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{D}^{R^{\\prime}}=\\{\\pmb{x}_{i}^{c},\\cdot\\cdot\\cdot,\\pmb{x}_{q}^{c}|\\forall c\\in\\mathcal{C}\\}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\boldsymbol{x}_{i}^{k}$ is the $i$ -th sample in the $k$ -th class of the training set, with a length of $l+h$ . $\\mathcal{C}$ is the category set of the original dataset. For brevity, we represent both databases as $\\mathcal{D}^{R}$ . ", "page_idx": 4}, {"type": "text", "text": "4.3 Retrieval-Augmented Time Series Diffusion ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Embedding-Based Retrieval Mechanism For time forecasting tasks, the ideal references $\\{s_{i},\\cdot\\cdot\\cdot,s_{i+h}\\}$ would be samples where preceding $n$ points $\\{s_{i-n},\\cdot\\cdot\\cdot\\,,s_{i-1}\\}$ is most relevant to the historical time series $\\{s_{j},\\cdot\\cdot\\cdot,s_{j+n}\\}$ in the $\\bar{\\mathcal{D}}^{R}$ . In our approach, the overall similarity between time series is of greater concern. We quantify the reference between time series using the distance between their embeddings. To ensure that embeddings can effectively represent the entire time series, pre-trained encoders $E_{\\phi}$ are utilized. $E_{\\phi}$ is trained on representation learning tasks, and the parameter set $\\phi$ is frozen in our retrieval mechanism. For time series (with length $n+h)$ in $\\mathcal{D}^{R}$ , their first $n$ points are encoded, thus the $\\mathcal{D}^{R}$ can be represented as $\\mathcal{D}_{\\mathrm{emb}}^{R}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{D}_{\\mathrm{emb}}^{R}=\\{\\{i,E_{\\phi}(\\pmb{x}_{[0:n]}^{i}),\\pmb{x}_{[n:n+h]}^{i}\\}|\\forall\\pmb{x}^{i}\\in\\mathcal{D}^{R}\\}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $[p:q]$ refers to the subsequence formed by the $p$ -th point to the $q$ -th point in the time series. The embedding corresponding to the historical time series can be represented as $\\pmb{v}^{H}=E_{\\phi}(\\pmb{x}^{H})$ . We calculate the distances between ${\\pmb v}^{H}$ and all embeddings in DeRmb and retrieve the references corresponding to the $k$ smallest distances. This process can be expressed as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname{index}(\\pmb{v}^{H})=\\arg\\operatorname*{min}_{\\substack{x^{i}\\in\\mathcal{D}_{\\mathrm{emb}}^{R}}}||\\pmb{v}^{H}-E_{\\phi}(\\pmb{x}_{[0:n]}^{i})||^{2}}\\\\ &{\\pmb{x}^{R}=\\{\\pmb{x}_{[n:n+h]}^{j}|\\forall j\\in\\mathrm{index}(\\pmb{v}^{H})\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where index $(\\cdot)$ represents retrieved index given $v_{D}$ . Thus, we obtain a subset $\\pmb{x}^{R}$ of $\\mathcal{D}^{R}$ based on a query $x^{H}$ , $i.e.\\zeta_{k}:{\\pmb x}^{H},{\\mathcal{D}}^{R}\\rightarrow{\\pmb x}^{R}$ , where $\\left\\lceil x^{R}\\right\\rceil=k$ . ", "page_idx": 4}, {"type": "text", "text": "Reference-Guided Time Series Diffusion Model In this section, we will introduce our referenceguided time series diffusion model. In the diffusion process, the forward process is identical to the traditional diffusion process, as shown in Equation (2). Following [34, 12, 35] the objective of the reverse process is to infer the posterior distribution $p(z^{t a r}|z^{c})$ through the subsequent expression: ", "page_idx": 4}, {"type": "equation", "text": "$$\np(\\pmb{x}|\\pmb{x}^{H})=\\int p(\\pmb{x}_{T}|\\pmb{x}^{H})\\prod_{t=1}^{T}p_{\\theta}(\\pmb{x}_{t-1}|\\pmb{x}_{t},\\pmb{x}^{H},\\pmb{x}^{R})\\mathcal{D}\\pmb{x}_{1:T},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $p({\\pmb x}_{T}|{\\pmb x}^{H})\\approx\\mathcal N({\\pmb x}_{T}|{\\pmb x}^{H},{\\pmb I})$ , $p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_{t},\\mathbf{x}^{H},\\mathbf{x}^{R})$ is the reverse transition kernel from $\\pmb{x}_{t}$ to $x_{t-1}$ with a learnable parameter $\\theta$ . Following most of the literature in the diffusion model, we adopt the assumption: ", "page_idx": 4}, {"type": "equation", "text": "$$\np_{\\theta}\\big(\\pmb{x}_{t-1}\\vert\\pmb{x}_{t},\\pmb{x}\\big)=\\mathcal{N}(\\pmb{x}_{t-1};\\mu_{\\theta}(\\pmb{x}_{t},\\pmb{x}^{H},\\pmb{x}^{R},t),\\Sigma_{\\theta}(\\pmb{x}_{t},\\pmb{x}^{H},\\pmb{x}^{R},t))\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mu_{\\theta}$ is a deep neural network with parameter $\\theta$ . After similar computations as those in [12], $\\Sigma_{\\theta}\\big({\\pmb x}_{t},{\\pmb x}^{H},{\\pmb x}^{R},t\\big)\\big)$ in the backward process is approximated as fixed. In other words, we can achieve reference-guided denoising by designing a rational and robust $\\mu_{\\theta}$ . ", "page_idx": 4}, {"type": "text", "text": "Denoising Network Architecture Similar to DiffWave [17] and CSDI [36], our pipeline is constructed on the foundation of transformer layers, as shown in Figure 3. However, the existing framework cannot effectively utilize the reference as guidance. Considering attention modules to integrate the $\\pmb{x}^{R}$ and $\\pmb{x}_{t}$ as a reasonable intuition, we propose a novel module called Reference Modulated Attention (RMA). Unlike normal attention modules, we realize the fusion of three features in RMA: the current time series feature, the side feature, and the reference feature. To be specific, RMA was set at the beginning of each residual module Figure 3. We use 1D-CNN to extract features from the input $\\pmb{x}_{t}$ , references $\\pmb{x}^{R}$ , and side information. Notably, we concatenate all references together for feature extraction. Side information consists of two parts, representing the correlation between variables and time steps in the current time series dataset Appendix B. We adjust the dimensions of these three features with linear layers and fuse them through matrix dot products. Similar to text-image diffusion models [29], RMA can effectively utilize reference information to guide the denoising process, while appropriate parameter settings prevent the results from overly depending on the reference. ", "page_idx": 5}, {"type": "text", "text": "Training Procedure To train RATD (i.e., optimize the evidence lower bound induced by RATD), we use the same objective function as previous work. The loss at time step $t-1$ are defined as follows respectively: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\cal L}_{t-1}^{(x)}=\\frac{1}{2\\tilde{\\beta}_{t}^{2}}\\|\\mu_{\\theta}({\\pmb x}_{t},\\hat{\\pmb x}_{0})-\\hat{\\mu}({\\pmb x}_{t},\\hat{\\pmb x}_{0})\\|^{2}}}\\\\ {{\\displaystyle=\\gamma_{t}\\|{\\pmb x}_{0}-\\hat{\\pmb x}_{0}\\|}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\hat{\\pmb{x}}_{0}$ are predicted from $\\pmb{x}_{t}$ , and 2 \u03b2\u02dct\u03b12\u00af (t1\u22121\u2212\u03b2\u03b1\u00aftt)2 are hyperparameters in diffusion process. We summarize the training procedure of RATD in Algorithm 1 and highlight the differences from the conventional models, in cyan. The process of sampling is shown in Appendix A. ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 Training Procedure of RATD ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Require: Time series dataset $\\mathcal{D}^{\\mathrm{train}}$ , neural network $\\mu_{\\theta}$ , , diffusion step $T$ , external database $\\mathcal{D}^{R}$ , pre-trained encoder $E_{\\phi}$ , number of references $k$   \n1: Retrieve references with top- $k$ high similarity from $\\mathcal{D}^{R}$ using $E$ to obtain $x^{R}$ as described in Section 4.3   \n2: while $\\phi_{\\theta}$ not converge do   \n3: Sample diffusion time $t\\in\\mathcal{U}(0,\\ldots,T)$   \n4: Compute the side feature $\\mathcal{Z}_{s}$   \n5: Perturb $\\scriptstyle x_{0}$ to obtain $\\pmb{x}_{t}$   \n6: Predict $\\hat{\\pmb{x}}_{0}$ from $\\pmb{x}_{t}$ , ${\\mathcal{T}}_{s}$ and $x^{R}$ (Equation (10))   \n7: Compute loss $L$ with $\\hat{\\pmb{x}}_{0}$ and $\\pmb{x}_{0}$ (Equation (11))   \n8: Update $\\theta$ by minimizing $L$   \n9: end while ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "5.1 Experimental Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Datasets Following previous work [45, 38, 8, 30], experiments are performed on four popular real-world time series datasets: (1) Electricity\\*, which includes the hourly electricity consumption data from 321 clients over two years.; (2) Wind [20], which contains wind power records from 2020-2021. (3) Exchange [18], which describes the daily exchange rates of eight countries (Australia, British, Canada, Switzerland, China, Japan, New Zealand, and Singapore); (4) Weather\u2020, which documents 21 meteorological indicators at 10-minute intervals spanning from 2020 to 2021.; Besides, we also applied our method to a large ECG time series dataset: MIMIC-IV-ECG [14]. The MIMICIV-ECG dataset contains clinical electrocardiogram data from over 190,000 patients and 450,000 hospitalizations at Beth Israel Deaconess Medical Center (BIDMC). ", "page_idx": 5}, {"type": "table", "img_path": "dRJJt0Ji48/tmp/f485aa6eab4fdaf082e6a537bfaf2ca99faa276374d428610c5ab83edb3e6abe.jpg", "table_caption": ["Table 1: Performance comparisons on four real-world datasets in terms of MSE, MAE, and CRPS. The best is in bold, while the second best is underlined. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Baseline Methods To comprehensively demonstrate the effectiveness of our method, we compare RATD with four kinds of time series forecasting methods. Our baselines include (1) Time series diffusion models, including CSDI [36], mr-Diff [31], $\\mathrm{D^{3}V A E}$ [20], TimeDiff [30]; (2) Recent time series forecasting methods with frequency information, including FiLM [46], Fedformer [47] and FreTS [41] ; (3) Time series transformers, including PatchTST [25], Autoformer [38], Pyraformer [22], Informer [45] and iTransformer [23]; (4) Other popular methods, including TimesNet [39], SciNet [21], Nlinear [43], DLinear [43] and NBeats [26]. ", "page_idx": 6}, {"type": "text", "text": "Evaluation Metric To comprehensively assess our proposed methodology, our experiment employs three metrics: (1) Probabilistic forecasting metrics: Continuous Ranked Probability Score (CRPS) on each time series dimension [24]. (2) Distance metrics: Mean Squared Error (MSE), and Mean Average Error(MAE) are employed to measure the distance between predictions and ground truths. ", "page_idx": 6}, {"type": "text", "text": "Implementation Details The length of the historical time series was 168, and the prediction lengths were (96, 192, 336), with results averaged. All experiments were conducted on an Nvidia RTX A6000 GPU with 40GB memory. During the experiments, the second strategy of conducting $\\mathcal{D}^{R}$ was employed for the MIMIC dataset, while the first strategy was utilized for the other four datasets. To reduce the training cost, we preprocessed the retrieval process by storing the reference indices of each sample in the training set in a dictionary. During the training on the diffusion model, we accessed this dictionary directly to avoid redundant retrieval processes. More details are shown in Appendix B. ", "page_idx": 6}, {"type": "text", "text": "5.2 Main Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Table 1 presents the primary results of our experiments on four daily datasets. Our approach surpasses existing time series diffusion models. Compared to other time series forecasting methods, our approach exhibits superior performance on three out of four datasets, with competitive performance on the remaining dataset. Notably, we achieve outstanding results on the wind dataset. Due to the lack of clear short-term periodicity (daily or hourly), some prediction tasks in this dataset are exceedingly challenging for other models. Retrieval-augmented mechanisms can effectively assist in addressing these challenging prediction tasks. ", "page_idx": 6}, {"type": "text", "text": "Figure 4 presents a case study randomly selected from our experiments on the wind dataset. We compare our prediction with iTransformer and two popular open-source time series diffusion models, CSDI and $\\mathrm{D_{3}V A E}$ . Although CSDI and $\\mathrm{D_{3}V A E}$ provide accurate predictions in the initial short-term period, their long-term predictions deviate significantly from the ground truth due to the lack of guidance. ITransformer captures rough trends and periodic patterns, yet our method offers higherquality predictions than the others. Furthermore, through the comparison between the predicted results and references in the figure, although references provide strong guidance, they do not explicitly substitute for the entire generated results. This further validates the rationality of our approach. ", "page_idx": 6}, {"type": "image", "img_path": "dRJJt0Ji48/tmp/83cc782aca21864abf9915d0e1a1802e74f6cebfe7b5039ac48f3400e3e6ccf9.jpg", "img_caption": ["Figure 4: Visualizations on wind by CSDI, $\\mathrm{D_{3}V A E}$ , iTransformer and the proposed RATD (with reference). "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Table 2 presents the testing results of our method on the MIMIC-IV-ECG dataset. We selected some powerful open-source methods as baselines for comparison. Our experiments are divided into two parts: in the first part, we evaluate the entire test set, while in the second part, we select rare cases (those accounting for less than $2\\%$ of total cases) from the test set as a subset for evaluation. Prediction tasks in the second part are more challenging for deep models. In the first experiment, our method achieved results close to iTransformer, while in the second task, our model significantly outperformed other methods, demonstrating the effectiveness of our approach in addressing challenging tasks. ", "page_idx": 7}, {"type": "text", "text": "5.3 Model Analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Influence of Retrieval Mechanism To investigate the impact of the retrieval augmentation mechanism on the generation process, we conducted an ablation study and presented the results in Table 3. The study addresses two questions: whether the retrieval augmentation mechanism is effective and which retrieval method is most effective. Firstly, we removed our retrieval augmentation mechanism from the RATD as a baseline. Besides, the model with random time series guidance is another baseline. The references retrieved by other methods have all positively impacted the prediction results. This suggests that reasonable references are highly effective in guiding the generation process. ", "page_idx": 7}, {"type": "text", "text": "We also compared two different retrieval mechanisms: correlation-based retrieval and embeddingbased retrieval. The first method directly retrieves the reference in the time domain (e.g., using Dynamic Time Warping (DTW) or Pearson correlation coefficient). Our approach adopts the second mechanism: retrieving references through the embedding of time series. From the results, the correlation-based methods are significantly inferior to the embedding-based methods. The former methods fail to capture the key features of the time series, making it difficult to retrieve the best references for forecasting. We also evaluate the embedding-based methods with various encoders for comparison. The comprehensive results show that methods with different encoders do not significantly differ. This indicates that different methods can all extract meaningful references, thereby producing similar improvements in results. TCN was utilized in our experiment because TCN strikes the best balance between computational cost and performance. ", "page_idx": 7}, {"type": "text", "text": "Effect of Retrieval Database We conducted an ablation study on two variables, $n$ and $k$ , to investigate the influence of the retrieval database $\\mathcal{D}^{R}$ in RATD, where $n$ represents the number of samples in each category of the database, and $k$ represents the number of reference exemplars. The results in Figure 5q can benefti the model in terms of prediction accuracy because a larger $\\dot{\\boldsymbol{D}}^{R}$ brings higher diversity, thereby providing more details beneficial for prediction and enhancing the generation process. Simply increasing k does not show significant improvement, as utilizing more references may introduce more noise into the denoising process. In our experiment, the settings of $n$ and $k$ are 256 and 3, respectively. ", "page_idx": 7}, {"type": "table", "img_path": "dRJJt0Ji48/tmp/72088f527b425132b9b92a34c3b6b094fdf46ec9a236b16bb605182a8d622b6b.jpg", "table_caption": ["Table 2: Performance comparisons on MIMIC datasets with popular time series forecasting methods. Here, \"MIMIC-IV (All)\" refers to the model\u2019s testing results on the complete test set, while \"MIMIC(Rare)\" indicates the model\u2019s testing results on a rare disease subset. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "dRJJt0Ji48/tmp/e6ca5ff607808959b095a0631c85a2d32ccc2d51b0599fcbc49fa0fea980e097.jpg", "table_caption": ["Table 3: Ablation study on different retrieval mechanisms. \u201c-\" means no references was utilized and \u201cRandom\" means references are selected randomly. Others refer to what model we use for retrieval references. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Inference Efficiency In this experiment, we evaluate the inference efficiency of the proposed RATD in comparison to other baseline time series diffusion models (TimeGrad, MG-TSD, SSSD). Figure 6 illustrates the inference time on the multivariate weather dataset with varying values of the prediction horizon $(h)$ . While our method introduces an additional retrieval module, the sampling efficiency of the RATD is not low due to the non-autoregressive transformer framework. It even slightly outperforms other baselines across all $h$ values. Notably, TimeGrad is observed to be the slowest, attributed to its utilization of auto-regressive decoding. ", "page_idx": 8}, {"type": "image", "img_path": "dRJJt0Ji48/tmp/0c9587f1a127d9d4be7ae0150dcae761d3f1828cf7ac4639ce1a29f906ee6a77.jpg", "img_caption": ["Figure 5: The effect of hyper-parameter $n$ and $k$ .Figure 6: Inference time (ms) on the Electricity with different prediction horizon $h$ "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Effectiveness of Reference Modulated Attention To validate the effectiveness of the proposed RMA, we designed additional ablation experiments. In these experiments, we used the CSDI architecture as the baseline method and added extra fusion modules to compare the performance of these modules (linear layer, cross-attention layer, and RMA). The results are shown in the Table 4. ", "page_idx": 8}, {"type": "text", "text": "Through our experiments, we found that compared to the basic cross-attention-based approach, RMA can integrate an edge information matrix (representing correlations between time and feature dimensions) more effectively. The extra fusion is highly beneficial in experiments, guiding the model to capture relationships between different variables. In contrast, linear-based methods concatenate inputs and references initially, which prevents the direct extraction of meaningful information from references, resulting in comparatively modest performance. ", "page_idx": 8}, {"type": "text", "text": "Table 4: Performance comparison(MSE) between CSDI-based methods, CSDI represents the basic network framework, CSDI+Linear denotes the approach where inputs and references are concatenated via a linear layer and fed into the network together, CSDI+CrossAttention signifies the use of cross attention to fuse features from inputs and references, and finally, ${\\mathrm{CSDI+RMA}}$ , which incorporates an additional RMA. ", "page_idx": 9}, {"type": "table", "img_path": "dRJJt0Ji48/tmp/3c16368b1050a8a2047a0c3505c9292a9217f5681c07892fe7819875e60077ba.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Predicting $\\scriptstyle x_{0}$ vs Predicting $\\epsilon$ . Following the formulation in Section 4.3, our network is designed to forecast the latent variable $\\pmb{x}_{0}$ . Since some existing models [28, 36] have been trained by predicting an additional noise term \u03f5, we conducted a comparative experiment to determine which approach is more suitable for our framework. Specifically, we maintained the network structure unchanged, only modifying the prediction target to be $\\epsilon$ . The results are presented in Table 5. Predicting $\\scriptstyle x_{0}$ proves to be more effective. This may be because the relationship between the reference and $\\pmb{x}_{o}$ is more direct, making the denoising task relatively easier. ", "page_idx": 9}, {"type": "table", "img_path": "dRJJt0Ji48/tmp/322f84c7b2c10eec9326bb4b8cbbfba8085d89b7c1b766f2e78182147a8a546d.jpg", "table_caption": ["Table 5: MSEs of two denoising strategies: Predicting $\\scriptstyle x_{0}$ vs predicting $\\epsilon$ "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "RMA position We investigate the best position of RMA in the model. Front, middle, and back means we set the RMA in the front of, in the middle of, and the back of two transformer layers, respectively. We found that placing RMA before the bidirectional transformer resulted in the most significant improvement in model performance. This also aligns with the intuition of network design: cross-attention modules placed at the front of the model tend to have a greater impact. ", "page_idx": 9}, {"type": "table", "img_path": "dRJJt0Ji48/tmp/4bd5d7fb38c2fa523e3bdd0d2d73fc59439803bfd34de85a4c0d60d33fcd83b2.jpg", "table_caption": ["Table 6: Ablation study on different RMA positions. The best is in bold. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Limitation and Future Work As a transformer-based diffusion model structure, our approach still faces some challenges brought by the transformer framework. Our model consumes a significant amount of computational resources dealing with time series consisting of too many variables. Additionally, our approach requires additional preprocessing (retrieval process) during training, which incurs additional costs on training time (around ten hours). ", "page_idx": 9}, {"type": "text", "text": "Conclusion In this paper, we propose a new framework for time series diffusion modeling to address the forecasting performance limitations of existing diffusion models. RATD retrieves samples most relevant to the historical time series from the constructed database and utilize them as references to guide the denoising process of the diffusion model, thereby obtaining more accurate predictions. RATD is highly effective in solving challenging time series prediction tasks, as evaluated by experiments on five real-world datasets. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is supported by the National Natural Science Foundation of China (No.62172018, No.62102008) and Wuhan East Lake High-Tech Development Zone National Comprehensive Experimental Base for Governance of Intelligent Society. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Juan Miguel Lopez Alcaraz and Nils Strodthoff. Diffusion-based time series imputation and forecasting with structured state space models. arXiv preprint arXiv:2208.09399, 2022.   \n[2] Andreas Blattmann, Robin Rombach, Kaan Oktay, Jonas M\u00fcller, and Bj\u00f6rn Ommer. Retrievalaugmented diffusion models. Advances in Neural Information Processing Systems, 35:15309\u2013 15324, 2022.   \n[3] Giovanni Bonetta, Rossella Cancelliere, Ding Liu, and Paul Vozila. Retrieval-augmented transformer-xl for close-domain dialog generation. arXiv preprint arXiv:2105.09235, 2021.   \n[4] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving language models by retrieving from trillions of tokens. In International conference on machine learning, pp. 2206\u20132240. PMLR, 2022.   \n[5] Jian Cao, Zhi Li, and Jian Li. Financial time series forecasting model based on ceemdan and lstm. Physica A: Statistical mechanics and its applications, 519:127\u2013139, 2019.   \n[6] Jui-Sheng Chou and Duc-Son Tran. Forecasting energy consumption time series using machine learning techniques based on usage patterns of residential householders. Energy, 165:709\u2013726, 2018.   \n[7] Alexiei Dingli and Karl Sant Fournier. Financial time series forecasting-a deep learning approach. International Journal of Machine Learning and Computing, 7(5):118\u2013122, 2017. [8] Wei Fan, Shun Zheng, Xiaohan Yi, Wei Cao, Yanjie Fu, Jiang Bian, and Tie-Yan Liu. Depts: deep expansion learning for periodic time series forecasting. arXiv preprint arXiv:2203.07681, 2022.   \n[9] Xinyao Fan, Yueying Wu, Chang Xu, Yuhao Huang, Weiqing Liu, and Jiang Bian. Mg-tsd: Multi-granularity time series diffusion models with guided learning process. arXiv preprint arXiv:2403.05751, 2024.   \n[10] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language model pre-training. In International conference on machine learning, pp. 3929\u20133938. PMLR, 2020.   \n[11] Pradeep Hewage, Ardhendu Behera, Marcello Trovati, Ella Pereira, Morteza Ghahremani, Francesco Palmieri, and Yonghuai Liu. Temporal convolutional neural (tcn) network for an effective weather forecasting using time-series data from the local weather station. Soft Computing, 24:16453\u201316482, 2020.   \n[12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \n[13] Baoyu Jing, Si Zhang, Yada Zhu, Bin Peng, Kaiyu Guan, Andrew Margenot, and Hanghang Tong. Retrieval based time series forecasting. arXiv preprint arXiv:2209.13525, 2022.   \n[14] Alistair EW Johnson, Lucas Bulgarelli, Lu Shen, Alvin Gayles, Ayad Shammout, Steven Horng, Tom J Pollard, Sicheng Hao, Benjamin Moody, Brian Gow, et al. Mimic-iv, a freely accessible electronic health record dataset. Scientific data, 10(1):1, 2023.   \n[15] Zahra Karevan and Johan AK Suykens. Transductive lstm for time-series prediction: An application to weather forecasting. Neural Networks, 125:1\u20139, 2020.   \n[16] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through memorization: Nearest neighbor language models. arXiv preprint arXiv:1911.00172, 2019.   \n[17] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile diffusion model for audio synthesis. In International Conference on Learning Representations, 2020.   \n[18] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long-and short-term temporal patterns with deep neural networks. In The 41st international ACM SIGIR conference on research & development in information retrieval, pp. 95\u2013104, 2018.   \n[19] S Sri Lakshmi and RK Tiwari. Model dissection from earthquake time series: A comparative analysis using modern non-linear forecasting and artificial neural network approaches. Computers & Geosciences, 35(2):191\u2013204, 2009.   \n[20] Yan Li, Xinjiang Lu, Yaqing Wang, and Dejing Dou. Generative time series forecasting with diffusion, denoise, and disentanglement. Advances in Neural Information Processing Systems, 35:23009\u201323022, 2022.   \n[21] Minhao Liu, Ailing Zeng, Muxi Chen, Zhijian Xu, Qiuxia Lai, Lingna Ma, and Qiang Xu. Scinet: Time series modeling and forecasting with sample convolution and interaction. Advances in Neural Information Processing Systems, 35:5816\u20135828, 2022.   \n[22] Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X Liu, and Schahram Dustdar. Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting. In International conference on learning representations, 2021.   \n[23] Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng Long. itransformer: Inverted transformers are effective for time series forecasting. arXiv preprint arXiv:2310.06625, 2023.   \n[24] James E Matheson and Robert L Winkler. Scoring rules for continuous probability distributions. Management science, 22(10):1087\u20131096, 1976.   \n[25] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64 words: Long-term forecasting with transformers. arXiv preprint arXiv:2211.14730, 2022.   \n[26] Boris N Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-beats: Neural basis expansion analysis for interpretable time series forecasting. arXiv preprint arXiv:1905.10437, 2019.   \n[27] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.   \n[28] Kashif Rasul, Calvin Seward, Ingmar Schuster, and Roland Vollgraf. Autoregressive denoising diffusion models for multivariate probabilistic time series forecasting. In International Conference on Machine Learning, pp. 8857\u20138868. PMLR, 2021.   \n[29] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10684\u201310695, 2022.   \n[30] Lifeng Shen and James Kwok. Non-autoregressive conditional diffusion models for time series prediction. arXiv preprint arXiv:2306.05043, 2023.   \n[31] Lifeng Shen, Weiyu Chen, and James Kwok. Multi-resolution diffusion models for time series forecasting. In The Twelfth International Conference on Learning Representations, 2023.   \n[32] Zhipeng Shen, Yuanming Zhang, Jiawei Lu, Jun Xu, and Gang Xiao. Seriesnet: a generative time series forecasting model. In 2018 international joint conference on neural networks (IJCNN), pp. 1\u20138. IEEE, 2018.   \n[33] Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and Hongsheng Li. Efficient attention: Attention with linear complexities. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pp. 3531\u20133539, 2021.   \n[34] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pp. 2256\u20132265. PMLR, 2015.   \n[35] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020.   \n[36] Yusuke Tashiro, Jiaming Song, Yang Song, and Stefano Ermon. Csdi: Conditional score-based diffusion models for probabilistic time series imputation. Advances in Neural Information Processing Systems, 34:24804\u201324816, 2021.   \n[37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[38] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. Advances in Neural Information Processing Systems, 34:22419\u201322430, 2021.   \n[39] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet: Temporal 2d-variation modeling for general time series analysis. In The eleventh international conference on learning representations, 2022.   \n[40] Sitan Yang, Carson Eisenach, and Dhruv Madeka. Mqretnn: Multi-horizon time series forecasting with retrieval augmentation. arXiv preprint arXiv:2207.10517, 2022.   \n[41] Kun Yi, Qi Zhang, Wei Fan, Shoujin Wang, Pengyang Wang, Hui He, Ning An, Defu Lian, Longbing Cao, and Zhendong Niu. Frequency-domain mlps are more effective learners in time series forecasting. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[42] Jinsung Yoon, Daniel Jarrett, and Mihaela Van der Schaar. Time-series generative adversarial networks. Advances in neural information processing systems, 32, 2019.   \n[43] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series forecasting? In Proceedings of the AAAI conference on artificial intelligence, volume 37, pp. 11121\u201311128, 2023.   \n[44] Mingyuan Zhang, Xinying Guo, Liang Pan, Zhongang Cai, Fangzhou Hong, Huirong Li, Lei Yang, and Ziwei Liu. Remodiffuse: Retrieval-augmented motion diffusion model. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 364\u2013373, 2023.   \n[45] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pp. 11106\u201311115, 2021.   \n[46] Tian Zhou, Ziqing Ma, Qingsong Wen, Liang Sun, Tao Yao, Wotao Yin, Rong Jin, et al. Film: Frequency improved legendre memory model for long-term time series forecasting. Advances in Neural Information Processing Systems, 35:12677\u201312690, 2022.   \n[47] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting. In International Conference on Machine Learning, pp. 27268\u201327286. PMLR, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Sampling Procedure ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Like Algorithm 1, we summarize the sampling procedure of RATD in Algorithm 2 and highlight the differences from conventional diffusion models in . ", "page_idx": 13}, {"type": "text", "text": "Algorithm 2 Sampling Procedure of RATD   \nRequire: The historical time series $x^{H}$ , the learned model $\\mu_{\\theta}$ , external database $\\mathcal{D}^{R}$ , pre-trained $E_{\\phi},$ the number of references $k$   \nEnsure: Prediction $\\pmb{x}^{P}$ corresponding to the history $\\pmb{x}^{H}$   \n1: Sample initial target time series ${\\mathbf{}}x_{T}$   \n2: Embed $x^{H}$ into $\\bar{v^{H}}$   \n3: Retrieval the reference $x^{R}$ with $v^{H}$ 4: Compute the side feature $\\mathcal{Z}_{s}$   \n5: for $t$ in $T,T-1,\\ldots,1$ do 6: Predict $\\hat{\\pmb{x}}_{0}$ from $\\pmb{x}_{t}$ , ${\\mathcal{T}}_{s}$ and $x^{R}$ (Equation (10))   \n7: Sample $x_{t-1}$ from the posterior $q(\\pmb{x}_{t}|\\pmb{x}_{0})$ (Equation (2))   \n8: end for ", "page_idx": 13}, {"type": "text", "text": "B Impletion Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 Training Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Our dataset is split in the proportion of 7:1:2 (Train: Validation: Test), utilizing a random splitting strategy to ensure diversity in the training set. We sample the ECG signals at $125\\mathrm{Hz}$ for the MIMIC-IV dataset and extract fixed-length windows as samples. For training, we utilized the Adam optimizer with an initial learning rate of $10^{-3}$ , $b e t a s=(0.95,0.999)$ . During the training process of shifted diffusion, the batch size was set to 64, and early stopping was applied for a maximum of 200 epochs. The diffusion steps $T$ were set to 100. ", "page_idx": 13}, {"type": "text", "text": "B.2 Side Information ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We combine temporal embedding and feature embedding as side information $v_{s}$ . We use 128- dimensions temporal embedding following previous studies [37]: ", "page_idx": 13}, {"type": "equation", "text": "$$\ns_{e m b e d d i n g}(s_{\\zeta})=\\left(\\sin(s_{\\zeta}/\\tau^{0/64}),\\ldots,\\sin(s_{\\zeta}/\\tau^{63/64}),\\cos(s_{\\zeta}/\\tau^{0/64}),\\ldots,\\cos(s_{\\zeta}/\\tau^{63/64})\\right)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\tau=10000$ . Following [36], $s_{l}$ represents the timestamp corresponding to the l-th point in the time series. This setup is designed to capture the irregular sampling in the dataset and convey it to the model. Additionally, we utilize learnable embedding to handle feature dimensions. Specifically, feature embedding is represented as 16-dimensional learnable vectors that capture relationships between dimensions. According to [17], we combine time embedding and feature embedding, collectively referred to as side information $\\mathcal{Z}_{s}$ . ", "page_idx": 13}, {"type": "text", "text": "The shape of $\\mathcal{Z}_{s}$ is not fixed and varies with datasets. Taking the Exchange dataset as an example, the shape of forecasting target $\\pmb{x}^{R}$ is [Batchsize (64), 7(number of variables), 168 (time-dimension), 12 (time-dimension)] and the corresponding shape of $\\mathcal{Z}_{s}$ is [Batchsize (64), total channel(144( time:128 $^+$ feature:16)), 320 (frequency-dimension\\*latent channel), 12 (time-dimension)]. ", "page_idx": 13}, {"type": "text", "text": "B.3 Transformers Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Our approach employs the Transformer architecture from CSDI, with the distinction of expanding the channel dimension to 128. The network comprises temporal and feature layers, ensuring the comprehensiveness of the model in handling the time-frequency domain latent while maintaining a relatively simple structure. Regarding the transformer layer, we utilized a 1-layer Transformer encoder implemented in PyTorch [27], comprising multi-head attention layers, fully connected layers, and layer normalization. We adopted the \"linear attention transformer\" package \u2021, to enhance computational efficiency. The inclusion of numerous features and long sequences prompted this decision. The package implements an efficient attention mechanism [33], and we exclusively utilized the global attention feature within the package. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "B.4 Metrics ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We will introduce the metrics in our experiments. We summarize them as below: ", "page_idx": 14}, {"type": "text", "text": "CRPS. CRPS [24] is a univariate strictly proper scoring rule which \u00b4 measures the compatibility of a cumulative distribution function $F$ with an observation $x$ as: ", "page_idx": 14}, {"type": "equation", "text": "$$\nC R P S(F,x)=\\int_{R}(F(y)-\\mathbb{1}_{(x\\leq y)})^{2}d y\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\mathbb{I}_{(x\\leq y)}$ is the indicator function, which is 1 if $x\\leq y$ and 0 otherwise. The CRPS attains the minimum value when the predictive distribution $F$ same as the data distribution. ", "page_idx": 14}, {"type": "text", "text": "MAE and MSE. MAE and MSE are calculated in the formula below, $\\hat{\\mathbf{x}^{P}}$ represents the predicted time series, and $\\pmb{x}^{P}$ represents the ground truth time series. MAE calculates the average absolute difference between predictions and true values, while MSE calculates the average squared difference between predictions and true values. A smaller MAE or MSE implies better predictions. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{M A E=m e a n(|\\hat{\\mathbf{x^{P}}}-\\mathbf{x^{P}}|)}\\\\ {M S E=\\sqrt{m e a n(|\\hat{\\mathbf{x^{P}}}-\\mathbf{x^{P}}|)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: The abstract and introduction include the paper\u2019s contributions and scope. Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 15}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Justification: We discuss the limitations of our approach at Section 6 ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 15}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 15}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 15}, {"type": "text", "text": "Justification: The paper does not include theoretical results Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 16}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: We provide all the information needed in the Experiment part. Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 16}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 16}, {"type": "text", "text": "Answer: [No] ", "page_idx": 17}, {"type": "text", "text": "Justification: The full code will be uploaded in the future. Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 17}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: We provide all the details about the dataset in appendix. Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 17}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 17}, {"type": "text", "text": "Answer: [No] ", "page_idx": 17}, {"type": "text", "text": "Justification: We do not report the error bars in our experiemnt. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The paper provide sufficient information on the computer resources in paragraph implement details. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 18}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The research conducted in the paper conform with the NeurIPS Code of Ethics Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 18}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: There is no obvious societal impact of the work performed ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: The paper poses no such risks ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 19}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: All the original work are sited properly. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 19}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 20}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper does not release new assets ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 20}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 20}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 20}]