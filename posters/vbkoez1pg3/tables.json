[{"figure_path": "vBKoEZ1PG3/tables/tables_7_1.jpg", "caption": "Table 1: Quantitative performance of (A) anomaly video description generation and (B) video question-answering. Red indicates the best performance, while blue denotes the second best.", "description": "This table presents a quantitative comparison of HAWK's performance against several baselines on two key tasks: anomaly video description generation and video question-answering.  The evaluation metrics used include both text-level metrics (BLEU-1 to BLEU-4) and GPT-guided metrics (Reasonability, Detail, Consistency).  The table is split into two parts: (A) shows results for video description generation, and (B) shows results for video question-answering.  The best performing model in each metric is highlighted in red, and the second-best is in blue.  This allows for a clear comparison of HAWK's performance against state-of-the-art methods in the field.", "section": "5 Experiments"}, {"figure_path": "vBKoEZ1PG3/tables/tables_7_2.jpg", "caption": "Table 1: Quantitative performance of (A) anomaly video description generation and (B) video question-answering. Red indicates the best performance, while blue denotes the second best.", "description": "This table presents a quantitative comparison of the HAWK model's performance against several baseline models on two key tasks: anomaly video description generation and video question-answering.  The results are broken down by different metrics: BLEU scores (BLEU-1 to BLEU-4) for text-level evaluation and Reasonability, Detail, and Consistency scores for GPT-guided evaluation.  Red highlights the best-performing model for each metric, and blue indicates the second-best performance.  The table showcases HAWK's superior performance across all metrics compared to existing state-of-the-art approaches.", "section": "5 Experiments"}, {"figure_path": "vBKoEZ1PG3/tables/tables_8_1.jpg", "caption": "Table 1: Quantitative performance of (A) anomaly video description generation and (B) video question-answering. Red indicates the best performance, while blue denotes the second best.", "description": "This table presents a quantitative comparison of the HAWK model's performance against several baseline models on two key tasks: anomaly video description generation and video question-answering.  For each task, the table shows the performance metrics (BLEU scores, Reasonability, Detail, and Consistency) achieved by each model.  The use of red and blue highlights the top two performing models for each metric, offering a clear visual comparison of the relative strengths and weaknesses of different approaches.  The table provides a concise summary of the experimental results, showcasing the superior performance of the HAWK model.", "section": "5 Experiments"}, {"figure_path": "vBKoEZ1PG3/tables/tables_9_1.jpg", "caption": "Table 1: Quantitative performance of (A) anomaly video description generation and (B) video question-answering. Red indicates the best performance, while blue denotes the second best.", "description": "This table presents a quantitative comparison of the HAWK model against several baselines across two tasks: anomaly video description generation and video question-answering.  The evaluation metrics used are BLEU scores (BLEU-1 to BLEU-4), representing the similarity between the model's generated text and the ground truth.  Additionally, GPT-Guided metrics (Reasonability, Detail, Consistency) provide a more nuanced assessment of the generated text quality, considering aspects like logical reasoning, level of detail, and overall coherence. Red highlighting indicates the best-performing model for each metric, while blue highlights the second-best.  The results show HAWK's superior performance compared to existing baselines.", "section": "5 Experiments"}, {"figure_path": "vBKoEZ1PG3/tables/tables_9_2.jpg", "caption": "Table 1: Quantitative performance of (A) anomaly video description generation and (B) video question-answering. Red indicates the best performance, while blue denotes the second best.", "description": "This table presents a quantitative comparison of the HAWK model against several baseline models across two key tasks: anomaly video description generation and video question-answering.  The evaluation metrics used are both text-level (BLEU scores) and GPT-guided (Reasonability, Detail, Consistency).  The table is divided into two parts (A and B), each corresponding to one of the tasks.  Red highlights the best performing model for each metric, while blue highlights the second best. This allows for a direct comparison of HAWK's performance against state-of-the-art baselines in both tasks.", "section": "5 Experiments"}, {"figure_path": "vBKoEZ1PG3/tables/tables_16_1.jpg", "caption": "Table 1: Quantitative performance of (A) anomaly video description generation and (B) video question-answering. Red indicates the best performance, while blue denotes the second best.", "description": "This table presents a quantitative comparison of HAWK's performance against several baseline models on two key tasks: anomaly video description generation and video question-answering.  For each task (A and B), the table shows the performance metrics (BLEU scores, Reasonability, Detail, and Consistency). The BLEU scores measure the similarity between generated text and ground truth, while Reasonability, Detail, and Consistency reflect the quality of the generated text as assessed by GPT.  Red highlights the top-performing model for each metric, indicating HAWK's superior performance. The use of baseline models allows for a direct comparison and validation of HAWK's efficacy.", "section": "5 Experiments"}, {"figure_path": "vBKoEZ1PG3/tables/tables_18_1.jpg", "caption": "Table 1: Quantitative performance of (A) anomaly video description generation and (B) video question-answering. Red indicates the best performance, while blue denotes the second best.", "description": "This table presents a quantitative comparison of the HAWK model's performance against several baseline models on two tasks: anomaly video description generation and video question-answering.  For each task, it shows the results of various evaluation metrics, including BLEU scores (1-4) which measure the similarity between generated text and ground truth, as well as GPT-guided metrics (Reasonability, Detail, Consistency) which assess the quality of generated text from a more nuanced perspective.  Red highlighting indicates the top-performing model for each metric, while blue denotes the second-best performer.  This allows for a comprehensive comparison of HAWK's capabilities across multiple evaluation criteria.", "section": "5 Experiments"}, {"figure_path": "vBKoEZ1PG3/tables/tables_18_2.jpg", "caption": "Table 1: Quantitative performance of (A) anomaly video description generation and (B) video question-answering. Red indicates the best performance, while blue denotes the second best.", "description": "This table presents a quantitative comparison of the proposed HAWK model against several baselines for two key tasks: anomaly video description generation and video question-answering.  The evaluation metrics used are BLEU scores (BLEU-1 to BLEU-4) reflecting the similarity of generated text to ground truth, and GPT-guided metrics (Reasonability, Detail, Consistency) measuring the quality and coherence of the generated text. The table is divided into two sections (A) and (B), each corresponding to one task. For each method and task, the table shows the performance using different backbones (LLMs) and their sizes in terms of parameters (e.g., 7B). Red highlights the best-performing model for each metric, and blue indicates the second-best.", "section": "5 Experiments"}, {"figure_path": "vBKoEZ1PG3/tables/tables_18_3.jpg", "caption": "Table 1: Quantitative performance of (A) anomaly video description generation and (B) video question-answering. Red indicates the best performance, while blue denotes the second best.", "description": "This table presents a quantitative comparison of the HAWK model's performance against several baseline models on two key tasks: anomaly video description generation and video question-answering.  The results are broken down into Text-Level metrics (BLEU-1 to BLEU-4, which measure the similarity between generated and ground truth text) and GPT-Guided metrics (Reasonability, Detail, and Consistency, which assess the quality of the generated text from a holistic perspective).  Red highlights the top-performing model for each metric, while blue indicates the second-best performance. The table demonstrates HAWK's superior performance compared to established baselines.", "section": "5 Experiments"}, {"figure_path": "vBKoEZ1PG3/tables/tables_21_1.jpg", "caption": "Table 1: Quantitative performance of (A) anomaly video description generation and (B) video question-answering. Red indicates the best performance, while blue denotes the second best.", "description": "This table presents a quantitative comparison of the HAWK model's performance against several baseline models on two key tasks: anomaly video description generation and video question-answering.  The results are evaluated using both text-level metrics (BLEU-1, BLEU-2, BLEU-3, BLEU-4) and GPT-guided metrics (Reasonability, Detail, Consistency). The best-performing model in each metric is highlighted in red, and the second-best is highlighted in blue.  The table is divided into two sections (A and B), corresponding to each of the tasks, and shows the performance across different backbone models.", "section": "5 Experiments"}, {"figure_path": "vBKoEZ1PG3/tables/tables_22_1.jpg", "caption": "Table 1: Quantitative performance of (A) anomaly video description generation and (B) video question-answering. Red indicates the best performance, while blue denotes the second best.", "description": "This table presents a quantitative comparison of the HAWK model's performance against several baseline models on two key tasks: anomaly video description generation and video question-answering.  For each task, it shows the results for multiple metrics: Text-Level metrics (BLEU-1, BLEU-2, BLEU-3, BLEU-4) and GPT-Guided metrics (Reasonability, Detail, Consistency).  The best performing model for each metric is highlighted in red, while the second-best is in blue. This allows for a comprehensive evaluation of HAWK's capabilities compared to existing state-of-the-art approaches.", "section": "5 Experiments"}, {"figure_path": "vBKoEZ1PG3/tables/tables_23_1.jpg", "caption": "Table 1: Quantitative performance of (A) anomaly video description generation and (B) video question-answering. Red indicates the best performance, while blue denotes the second best.", "description": "This table presents a quantitative comparison of HAWK's performance against several baseline models on two tasks: anomaly video description generation and video question-answering.  The results are broken down into text-level metrics (BLEU-1 to BLEU-4) and GPT-guided metrics (Reasonability, Detail, Consistency).  Red highlights the best-performing model for each metric, while blue indicates the second-best. This allows for a clear visualization of HAWK's superiority in both video description and question-answering capabilities.", "section": "5 Experiments"}, {"figure_path": "vBKoEZ1PG3/tables/tables_24_1.jpg", "caption": "Table 1: Quantitative performance of (A) anomaly video description generation and (B) video question-answering. Red indicates the best performance, while blue denotes the second best.", "description": "This table presents a quantitative comparison of the HAWK model's performance against several baseline models on two key tasks: anomaly video description generation and video question-answering.  The evaluation metrics used include BLEU scores (BLEU-1 through BLEU-4), which measure the similarity between generated text and ground truth, and GPT-guided metrics (Reasonability, Detail, Consistency), which assess the quality of the generated text based on human-like evaluation criteria.  The table is divided into two sections (A and B) representing each task, and the best and second-best performing models are highlighted in red and blue, respectively.  The results showcase HAWK's superior performance in comparison to other state-of-the-art video understanding models.", "section": "5 Experiments"}, {"figure_path": "vBKoEZ1PG3/tables/tables_25_1.jpg", "caption": "Table 1: Quantitative performance of (A) anomaly video description generation and (B) video question-answering. Red indicates the best performance, while blue denotes the second best.", "description": "This table presents a quantitative comparison of the HAWK model's performance against several baseline models on two tasks: anomaly video description generation and video question-answering.  For each task, it shows the results using both text-level metrics (BLEU-1 to BLEU-4) and GPT-guided metrics (Reasonability, Detail, Consistency). The best-performing model for each metric is highlighted in red, and the second-best is in blue. This allows for a comprehensive evaluation of the HAWK model's ability to understand and generate descriptive text related to video anomalies.", "section": "5 Experiments"}, {"figure_path": "vBKoEZ1PG3/tables/tables_26_1.jpg", "caption": "Table 1: Quantitative performance of (A) anomaly video description generation and (B) video question-answering. Red indicates the best performance, while blue denotes the second best.", "description": "This table presents a quantitative comparison of the HAWK model's performance against several baseline models in two key tasks: anomaly video description generation and video question-answering.  The evaluation metrics used are BLEU scores (BLEU-1 to BLEU-4) for text-level evaluation and GPT-guided metrics (Reasonability, Detail, Consistency) which assess the quality of the generated text using GPT-4. Red highlights the best-performing model for each metric, while blue signifies the second-best. This allows for a comprehensive comparison of the HAWK model's capabilities against existing state-of-the-art models in the field of video anomaly understanding.", "section": "5 Experiments"}]