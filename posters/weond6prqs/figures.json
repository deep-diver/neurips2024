[{"figure_path": "WeoNd6PRqS/figures/figures_1_1.jpg", "caption": "Figure 1: The comprehensive capabilities of OMG-LLaVA. OMG-LLaVA can handle a variety of pixel-level, object-level, and image-level understanding and reasoning tasks.", "description": "This figure demonstrates the various capabilities of the OMG-LLaVA model.  It showcases examples of pixel-level tasks such as reasoning segmentation and interactive segmentation; object-level tasks such as region-level captioning and referring segmentation; and image-level tasks such as image captioning and image-level conversation.  The figure visually represents the model's ability to integrate multiple levels of understanding and reasoning within a unified framework.", "section": "1 Introduction"}, {"figure_path": "WeoNd6PRqS/figures/figures_4_1.jpg", "caption": "Figure 3: The Overview of OMG-LLaVA. OMG-LLAVA consists of OMG-Seg and LLM. OMG-Seg tokenizes the image into pixel-centric visual tokens, the detected objects, and inputs visual prompts into object-centric visual tokens. Additionally, the [SEG] token output by LLM is decoded by OMG-Seg into segmentation masks. OMG-Seg remains frozen at all stages.", "description": "This figure illustrates the architecture of OMG-LLaVA, which combines the OMG-Seg model and a large language model (LLM).  The OMG-Seg model processes the image to create pixel-centric and object-centric visual tokens. These tokens are provided to the LLM along with textual instructions. The LLM generates a text response and object-centric tokens, which the OMG-Seg then uses to create the final segmentation mask.  The OMG-Seg model itself remains frozen during the process.", "section": "3.2 OMG-LLaVA Framework"}, {"figure_path": "WeoNd6PRqS/figures/figures_5_1.jpg", "caption": "Figure 4: The Architecture of the OMG Decoder. A simple attention mask generation strategy enables the OMG decoder to encode point, box, and mask prompts.", "description": "The OMG decoder takes in image features, learnable queries, and visual prompt queries (point, box, and mask).  A novel attention mask generation strategy is employed. This strategy uses the spatial information inherent in box and mask prompts to create attention masks, ensuring that only relevant features contribute to the generation of object queries. These object queries are then decoded into pixel-level segmentation masks.  The diagram illustrates the process and shows how the attention masks focus the decoder's attention to the relevant regions based on the type of prompt.", "section": "3.2 OMG-LLaVA Framework"}, {"figure_path": "WeoNd6PRqS/figures/figures_5_2.jpg", "caption": "Figure 5: The process of the perception prior embedding strategy. The perception prior embedding strategy integrates object queries into image features based on segmentation prior.", "description": "The figure illustrates the process of perception prior embedding, a strategy to integrate object queries into image features using segmentation priors.  It shows how mask scores derived from object queries are used to weight the object queries before adding them to image features, producing pixel-centric visual tokens. These, along with object-centric visual tokens (foreground object queries), are then fed into the LLM.", "section": "3.2 OMG-LLaVA Framework"}, {"figure_path": "WeoNd6PRqS/figures/figures_8_1.jpg", "caption": "Figure 1: The comprehensive capabilities of OMG-LLaVA. OMG-LLaVA can handle a variety of pixel-level, object-level, and image-level understanding and reasoning tasks.", "description": "This figure demonstrates the various capabilities of the OMG-LLaVA model, showcasing its ability to perform pixel-level, object-level, and image-level reasoning and understanding tasks.  It highlights the model's flexibility in handling different types of visual and textual prompts, including descriptions, questions, instructions, and visual prompts, and generating corresponding responses and segmentations. The examples illustrate tasks such as image captioning, grounded conversation, referring and reasoning segmentations, and panoptic segmentation.", "section": "1 Introduction"}, {"figure_path": "WeoNd6PRqS/figures/figures_24_1.jpg", "caption": "Figure 1: The comprehensive capabilities of OMG-LLaVA. OMG-LLaVA can handle a variety of pixel-level, object-level, and image-level understanding and reasoning tasks.", "description": "This figure demonstrates the versatility of the OMG-LLaVA model in handling various tasks across different levels of image understanding.  It showcases examples of pixel-level tasks (semantic and instance segmentation, interactive segmentation), object-level tasks (referring segmentation, reasoning segmentation, region-level captions), and image-level tasks (image captioning, image-level conversations). Each example features the task instruction given to the model and the corresponding result, highlighting the model's ability to integrate image information, perception priors, and visual prompts to generate accurate and detailed responses.", "section": "1 Introduction"}, {"figure_path": "WeoNd6PRqS/figures/figures_26_1.jpg", "caption": "Figure 1: The comprehensive capabilities of OMG-LLaVA. OMG-LLaVA can handle a variety of pixel-level, object-level, and image-level understanding and reasoning tasks.", "description": "This figure demonstrates the various tasks that OMG-LLaVA can perform.  It showcases examples of pixel-level tasks (e.g., semantic segmentation, reasoning segmentation, interactive segmentation), object-level tasks (e.g., region-level captioning, referring segmentation), and image-level tasks (e.g., image captioning, image-level conversation). The figure highlights OMG-LLaVA's ability to integrate image, object, and pixel-level information for comprehensive visual understanding and reasoning.", "section": "1 Introduction"}, {"figure_path": "WeoNd6PRqS/figures/figures_26_2.jpg", "caption": "Figure 1: The comprehensive capabilities of OMG-LLaVA. OMG-LLaVA can handle a variety of pixel-level, object-level, and image-level understanding and reasoning tasks.", "description": "This figure demonstrates the various capabilities of the OMG-LLaVA model.  It showcases examples of pixel-level tasks (reasoning segmentation, interactive segmentation), object-level tasks (region-level caption, instance segmentation, referring segmentation), and image-level tasks (image-level caption, image-level conversation, multi-visual prompt description). The figure highlights OMG-LLaVA's ability to perform a wide range of visual understanding and reasoning tasks across different levels of granularity.", "section": "1 Introduction"}, {"figure_path": "WeoNd6PRqS/figures/figures_26_3.jpg", "caption": "Figure 1: The comprehensive capabilities of OMG-LLaVA. OMG-LLaVA can handle a variety of pixel-level, object-level, and image-level understanding and reasoning tasks.", "description": "This figure showcases the various tasks that OMG-LLaVA can perform.  It highlights the model's ability to handle tasks at different levels of granularity: pixel-level (semantic segmentation, interactive segmentation, etc.), object-level (referring segmentation, reasoning segmentation, etc.), and image-level (image captioning, image-based conversations, etc.).  It demonstrates the model's versatility and power in understanding and reasoning about images in a comprehensive manner.", "section": "1 Introduction"}, {"figure_path": "WeoNd6PRqS/figures/figures_26_4.jpg", "caption": "Figure 1: The comprehensive capabilities of OMG-LLaVA. OMG-LLaVA can handle a variety of pixel-level, object-level, and image-level understanding and reasoning tasks.", "description": "This figure showcases the various tasks that OMG-LLaVA can perform across three levels: pixel-level, object-level, and image-level.  It demonstrates the model's ability to handle a wide range of visual understanding and reasoning tasks, including image captioning, visual question answering, referring segmentation, reasoning segmentation, interactive segmentation, and grounded conversation generation.  The figure visually represents the comprehensive capabilities of the model, highlighting its capacity to process and interpret visual information at various levels of detail and granularity.", "section": "1 Introduction"}, {"figure_path": "WeoNd6PRqS/figures/figures_26_5.jpg", "caption": "Figure 6: Visualization of the effectiveness of the proposed strategies. The left part shows the baseline (M0 in Tab. 5), the middle part shows the model with perception prior embedding (M1 in Tab. 5), and the right part shows the model with both perception prior embedding and object query input (M2 in Tab. 5).", "description": "This figure shows an ablation study comparing three different versions of the model. The left image shows the baseline model without any of the proposed improvements. The middle image shows the model with perception prior embedding added, and the right image shows the model with both perception prior embedding and object query input added. This figure demonstrates the improvement in performance that results from adding each of the proposed strategies.", "section": "4.2 Ablation and Analysis"}, {"figure_path": "WeoNd6PRqS/figures/figures_26_6.jpg", "caption": "Figure 1: The comprehensive capabilities of OMG-LLaVA. OMG-LLaVA can handle a variety of pixel-level, object-level, and image-level understanding and reasoning tasks.", "description": "This figure demonstrates the versatility of the OMG-LLaVA model in handling various types of visual understanding and reasoning tasks.  It showcases examples of pixel-level tasks (reasoning segmentation, interactive segmentation, semantic segmentation, etc.), object-level tasks (region-level caption, instance segmentation, etc.), and image-level tasks (image-level conversation, image captioning, etc.). The figure highlights OMG-LLaVA's ability to perform complex tasks that require integrating information from multiple levels of visual representation, demonstrating its comprehensive capabilities.", "section": "1 Introduction"}, {"figure_path": "WeoNd6PRqS/figures/figures_26_7.jpg", "caption": "Figure 1: The comprehensive capabilities of OMG-LLaVA. OMG-LLaVA can handle a variety of pixel-level, object-level, and image-level understanding and reasoning tasks.", "description": "This figure showcases the various capabilities of the OMG-LLaVA model across different levels of visual understanding and reasoning.  It demonstrates its ability to handle pixel-level tasks (such as semantic, instance, and panoptic segmentation), object-level tasks (such as referring and reasoning segmentation, and region-level captioning), and image-level tasks (such as image captioning and conversation).  The examples shown highlight the flexibility and diversity of the tasks that OMG-LLaVA is able to perform, emphasizing its unified approach to multimodal reasoning.", "section": "1 Introduction"}, {"figure_path": "WeoNd6PRqS/figures/figures_26_8.jpg", "caption": "Figure 1: The comprehensive capabilities of OMG-LLaVA. OMG-LLaVA can handle a variety of pixel-level, object-level, and image-level understanding and reasoning tasks.", "description": "This figure showcases the diverse capabilities of the OMG-LLaVA model, highlighting its ability to perform various tasks across different levels of visual understanding.  It demonstrates tasks ranging from simple image-level captioning and conversation to complex pixel-level segmentation, object-level reasoning, and region-level captioning. The examples illustrate the model's capacity to understand user instructions and visual prompts, generating both text and segmentation masks as responses.", "section": "1 Introduction"}, {"figure_path": "WeoNd6PRqS/figures/figures_26_9.jpg", "caption": "Figure 1: The comprehensive capabilities of OMG-LLaVA. OMG-LLaVA can handle a variety of pixel-level, object-level, and image-level understanding and reasoning tasks.", "description": "This figure showcases the various tasks that OMG-LLaVA can perform.  It demonstrates the model's capabilities across different levels of visual understanding: pixel-level (e.g., semantic and instance segmentation), object-level (e.g., object detection, referring expression segmentation), and image-level (e.g., image captioning, visual question answering). The examples highlight the model's ability to handle diverse visual and textual prompts, making it a versatile tool for various image understanding applications.", "section": "1 Introduction"}, {"figure_path": "WeoNd6PRqS/figures/figures_26_10.jpg", "caption": "Figure 1: The comprehensive capabilities of OMG-LLaVA. OMG-LLaVA can handle a variety of pixel-level, object-level, and image-level understanding and reasoning tasks.", "description": "This figure demonstrates the wide range of tasks that OMG-LLaVA can perform.  It shows examples across three levels: pixel-level (e.g., semantic and instance segmentation), object-level (e.g., referring segmentation, object-level captioning), and image-level (e.g., image captioning, image-based conversation, visual question answering). The figure highlights OMG-LLaVA's ability to integrate these different levels of understanding and reasoning into a single, unified framework.", "section": "1 Introduction"}, {"figure_path": "WeoNd6PRqS/figures/figures_27_1.jpg", "caption": "Figure 1: The comprehensive capabilities of OMG-LLaVA. OMG-LLaVA can handle a variety of pixel-level, object-level, and image-level understanding and reasoning tasks.", "description": "This figure showcases the various capabilities of the OMG-LLaVA model.  It demonstrates the model's ability to perform tasks at three levels: pixel-level (e.g., semantic and instance segmentation), object-level (e.g., object detection and region-level captioning), and image-level (e.g., image captioning and image-based conversation).  The examples provided illustrate the model's flexibility in handling different types of visual prompts and text instructions, demonstrating its comprehensive reasoning and understanding abilities.", "section": "1 Introduction"}, {"figure_path": "WeoNd6PRqS/figures/figures_28_1.jpg", "caption": "Figure 1: The comprehensive capabilities of OMG-LLaVA. OMG-LLaVA can handle a variety of pixel-level, object-level, and image-level understanding and reasoning tasks.", "description": "This figure demonstrates the various capabilities of the OMG-LLaVA model.  It showcases examples of pixel-level tasks (semantic and instance segmentation, reasoning segmentation, interactive segmentation), object-level tasks (region-level caption, visual prompt-based conversation, referring segmentation), and image-level tasks (image caption, grounded conversation generation).  The examples highlight the model's ability to integrate image, object, and pixel-level information for a comprehensive understanding of the visual input and the user's instructions.", "section": "1 Introduction"}, {"figure_path": "WeoNd6PRqS/figures/figures_29_1.jpg", "caption": "Figure 1: The comprehensive capabilities of OMG-LLaVA. OMG-LLaVA can handle a variety of pixel-level, object-level, and image-level understanding and reasoning tasks.", "description": "This figure demonstrates the various capabilities of the OMG-LLaVA model by showcasing its performance on different tasks across three levels: pixel-level, object-level, and image-level.  It highlights tasks such as reasoning segmentation, referring segmentation, interactive segmentation, region-level captioning, grounded conversation generation, image-level conversation, and image-level captioning. The examples illustrate the model's ability to understand and respond to various visual and text prompts, demonstrating a comprehensive understanding of images at different granularities.", "section": "1 Introduction"}, {"figure_path": "WeoNd6PRqS/figures/figures_29_2.jpg", "caption": "Figure 1: The comprehensive capabilities of OMG-LLaVA. OMG-LLaVA can handle a variety of pixel-level, object-level, and image-level understanding and reasoning tasks.", "description": "This figure demonstrates the various tasks that OMG-LLaVA can perform.  It showcases the model's ability to handle pixel-level tasks (like semantic segmentation), object-level tasks (such as referring segmentation), and image-level tasks (like image captioning and visual question answering).  The examples illustrate the flexibility of the model in accepting different types of prompts (visual and text) and generating corresponding responses, demonstrating its comprehensive capabilities in bridging image-level, object-level, and pixel-level reasoning and understanding.", "section": "1 Introduction"}, {"figure_path": "WeoNd6PRqS/figures/figures_30_1.jpg", "caption": "Figure 1: The comprehensive capabilities of OMG-LLaVA. OMG-LLaVA can handle a variety of pixel-level, object-level, and image-level understanding and reasoning tasks.", "description": "This figure demonstrates the versatility of the OMG-LLaVA model in handling various tasks across different levels of image understanding: pixel-level (semantic, instance, panoptic segmentation and interactive segmentation), object-level (object-centric tasks like region-level captioning and referring segmentation), and image-level (image-centric tasks like captioning and image-based conversations).  It showcases the model's ability to reason and answer questions about the image content at various levels of granularity, highlighting its multi-level reasoning capabilities.", "section": "1 Introduction"}, {"figure_path": "WeoNd6PRqS/figures/figures_30_2.jpg", "caption": "Figure 6: Visualization of the effectiveness of the proposed strategies. The left part shows the baseline (M0 in Tab. 5), the middle part shows the model with perception prior embedding (M1 in Tab. 5), and the right part shows the model with both perception prior embedding and object query input (M2 in Tab. 5).", "description": "This figure shows an ablation study comparing three versions of the OMG-LLaVA model on a referring expression segmentation task.  The left image shows the baseline model, the middle image shows the model with perception prior embedding added, and the right image shows the model with both perception prior embedding and object query input. The results demonstrate that adding these components improves the accuracy of segmentation.", "section": "4.2 Ablation and Analysis"}]