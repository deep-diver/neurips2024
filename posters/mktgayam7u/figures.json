[{"figure_path": "Mktgayam7U/figures/figures_7_1.jpg", "caption": "Figure 1: Convergence curves for SSO.", "description": "This figure shows the convergence curves for the Sequential Selection Optimization (SSO) algorithm across six different MuJoCo tasks from the D4RL benchmark. The x-axis represents the number of iterations, and the y-axis represents the error between the current objective function value and the optimal objective function value (calculated by SCS) in Problem (9).  The plot demonstrates the fast convergence rate of the SSO algorithm, showing that errors for all tasks are below 0.1 by the 10th iteration and below 1e-4 by the 20th iteration.", "section": "4 Sequential Selection Optimization"}, {"figure_path": "Mktgayam7U/figures/figures_8_1.jpg", "caption": "Figure 2: Convergence curves on the MuJoCo Hopper task with the first 5k data points from the D4RL Hopper-expert dataset. The vertical axis represents the difference between the current objective function value and the optimal value. Sequential Selection Optimization (orange) exhibits the fastest convergence rate.", "description": "This figure compares the convergence speed of four different optimization methods. The x-axis represents the number of iterations, and the y-axis represents the error between the current objective function value and the optimal value. The orange line represents the Sequential Selection Optimization (SSO) method, which demonstrates the fastest convergence rate. The other three lines show the results of three other methods: using the heuristic coordinate selection method, using the heuristic coordinate selection method with the warm-up trick and using naive coordinate descent. The figure shows that the warm-up trick significantly accelerates the convergence of the SSO algorithm, and using the heuristic improves convergence as well.", "section": "5.2 Ablation Studies"}]