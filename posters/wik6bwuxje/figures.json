[{"figure_path": "wiK6bwuxjE/figures/figures_1_1.jpg", "caption": "Figure 1: Object occlusion is pervasive and affects monocular 3D detection: Object occlusion is pervasive, e.g., 62% (17725) cars in the KITTI 3D dataset suffer from various occlusions as illustrated in (a). Prevalent monocular 3D detection techniques such as GUPNet [37] and MonoDETR [66] are clearly affected by object occlusions in both 3D space (3D) and the bird's eye view (BEV) space as in (b). The proposed MonoMAE simulates and learns object occlusions by feature masking and completing which improves detection consistently for both occluded and non-occluded objects.", "description": "This figure shows the impact of object occlusion on monocular 3D object detection.  Subfigure (a) illustrates the prevalence of occlusion in the KITTI dataset, highlighting that a significant portion of cars are occluded.  Subfigure (b) presents a performance comparison of existing methods (GUPNet and MonoDETR) and the proposed MonoMAE on both occluded and non-occluded objects in 3D and bird's-eye-view (BEV) perspectives. The results demonstrate that MonoMAE significantly outperforms existing methods, particularly in handling occluded objects.", "section": "1 Introduction"}, {"figure_path": "wiK6bwuxjE/figures/figures_3_1.jpg", "caption": "Figure 2: The framework of MonoMAE training: Given a single-view image, the 3D Backbone extracts 3D object query features which are grouped into non-occluded query features and occluded query features by the Non-Occluded Query Grouping. The Depth-Aware Masking then masks the non-occluded query features to simulate object occlusions adaptively based on the object depth, and the Completion Network then learns to reconstruct the masked queries. Finally, the completed and the occluded query features are concatenated to train the 3D Detection Head for 3D predictions.", "description": "This figure illustrates the training process of the MonoMAE model.  It starts with a single image input, which is processed by a 3D backbone to generate a sequence of 3D object queries. These queries are then classified into occluded and non-occluded groups. The non-occluded queries are masked using a depth-aware masking technique, simulating the effect of occlusion. A completion network reconstructs these masked queries. Finally, both the completed (reconstructed) and originally occluded queries are used to train a 3D detection head, allowing the model to learn from both occluded and non-occluded objects.", "section": "3 Proposed Method"}, {"figure_path": "wiK6bwuxjE/figures/figures_4_1.jpg", "caption": "Figure 3: Illustration of the Depth-Aware Masking. (a) Objects farther away are usually smaller capturing less visual information. (b) The Depth-Aware Masking determines the mask ratio of an object according to its depth - the closer the object is, the larger the mask ratio is applied, thereby compensating the information deficiency for objects that have larger distances from the camera.", "description": "This figure illustrates the Depth-Aware Masking mechanism used in MonoMAE.  Panel (a) shows a 3D visualization of objects at varying distances from the camera, highlighting how objects farther away appear smaller and contain less visual information.  Panel (b) details how the masking process works: non-occluded object queries are masked adaptively based on their depth; closer objects have a larger mask ratio applied to simulate occlusion. This adaptive masking compensates for the information loss associated with distant objects.", "section": "3.3 Non-Occluded Query Masking"}, {"figure_path": "wiK6bwuxjE/figures/figures_6_1.jpg", "caption": "Figure 4: Detection visualization over the KITTI val set. Ground-truth annotations are highlighted by red boxes, and predictions by MonoMAE and two state-of-the-art methods are highlighted by green boxes. Red arrows highlight objects that have very different predictions across the compared methods. The ground truth of LiDAR point clouds is provided for visualization only, and they are not used in MonoMAE training. Best viewed in color and zoom-in.", "description": "This figure compares the detection results of MonoMAE against two state-of-the-art methods (GUPNet and MonoDETR) on the KITTI validation dataset.  It shows example images (top row) and their corresponding bird's-eye-view (BEV) representations (bottom row) for two different cases.  Red boxes indicate ground truth annotations, green boxes show MonoMAE's predictions, and blue boxes are for the other two methods. Red arrows highlight objects where the predictions of the different models significantly differ, illustrating the superior performance of MonoMAE in handling object occlusion. Note that the LiDAR point cloud is used for visualization only and isn't part of the MonoMAE training process.", "section": "4.2 Benchmarking with the State-of-the-Art"}]