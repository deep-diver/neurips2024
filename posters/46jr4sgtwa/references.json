{"references": [{"fullname_first_author": "Sepp Hochreiter", "paper_title": "Long Short-Term Memory", "publication_date": "1997-08-01", "reason": "This paper introduced the LSTM architecture, a significant improvement in RNNs addressing the vanishing gradient problem and enabling learning of long-term dependencies."}, {"fullname_first_author": "Yoshua Bengio", "paper_title": "Learning long-term dependencies with gradient descent is difficult", "publication_date": "1994-02-01", "reason": "This foundational paper analyzed the vanishing gradient problem inherent in training RNNs, highlighting the difficulty in learning long-term dependencies."}, {"fullname_first_author": "Razvan Pascanu", "paper_title": "On the difficulty of training recurrent neural networks", "publication_date": "2013-01-01", "reason": "This paper provided further analysis of the challenges in training RNNs, including vanishing/exploding gradients and proposing solutions like gradient clipping."}, {"fullname_first_author": "Albert Gu", "paper_title": "Efficiently modeling long sequences with structured state spaces", "publication_date": "2022-01-01", "reason": "This work introduced the deep state-space models (SSMs), a novel class of RNNs that exhibit superior performance in capturing long-range dependencies and serve as a key focus of the current paper."}, {"fullname_first_author": "Antonio Orvieto", "paper_title": "Resurrecting recurrent neural networks for long sequences", "publication_date": "2023-01-01", "reason": "This recent paper directly inspired the current work, investigating the effectiveness of SSMs and other architectures in overcoming challenges related to long-term dependencies in RNNs."}]}