[{"figure_path": "DX5GUwMFFb/tables/tables_4_1.jpg", "caption": "Table 1: Comparison of SAC and AVG algorithms", "description": "This table provides a comparison between the Soft Actor-Critic (SAC) algorithm and the Action Value Gradient (AVG) algorithm, highlighting key differences in their architecture, components, and learning mechanisms.  It shows that SAC uses two Q-networks, two target Q-networks, a learned entropy coefficient, and a replay buffer, while AVG employs a simpler design with only one Q-network, no target network, a fixed entropy coefficient, and no replay buffer.", "section": "Incremental Soft Actor Critic (SAC-1)"}, {"figure_path": "DX5GUwMFFb/tables/tables_4_2.jpg", "caption": "Table 1: Comparison of SAC and AVG algorithms", "description": "This table provides a concise comparison of the Soft Actor-Critic (SAC) and Action Value Gradient (AVG) algorithms, highlighting key architectural differences and design choices.  It shows that SAC uses two Q-networks, two target Q-networks, a learned entropy coefficient, and a replay buffer, while AVG utilizes a single Q-network, no target networks, a fixed entropy coefficient, and no replay buffer.  These differences reflect the core distinction between the off-policy batch approach of SAC and the on-policy incremental nature of AVG.", "section": "D Incremental Soft Actor Critic (SAC-1)"}, {"figure_path": "DX5GUwMFFb/tables/tables_20_1.jpg", "caption": "Table 1: Comparison of SAC and AVG algorithms", "description": "This table summarizes the key differences between the Soft Actor-Critic (SAC) algorithm and the proposed Action Value Gradient (AVG) algorithm.  It highlights that SAC uses two Q-networks, two target Q-networks, a learned entropy coefficient, and a replay buffer, while AVG uses a single Q-network, no target networks, a fixed entropy coefficient, and no replay buffer.  The differences reflect AVG's design goal of being a simpler, more resource-efficient incremental algorithm compared to the off-policy SAC.", "section": "Incremental Soft Actor Critic (SAC-1)"}, {"figure_path": "DX5GUwMFFb/tables/tables_23_1.jpg", "caption": "Table 2: Default parameters for CleanRL PPO implementation.", "description": "This table lists the hyperparameters used in the Proximal Policy Optimization (PPO) algorithm implementation from the CleanRL library.  It includes values for parameters such as update frequency, minibatch size, generalized advantage estimation (GAE) lambda, discount factor, number of optimizer epochs, entropy coefficient, learning rate, clip coefficient, value loss coefficient, and maximum gradient norm. These settings are crucial for controlling the learning process and balancing exploration-exploitation trade-offs in the reinforcement learning task.", "section": "F.1 Choice of Hyper-parameters for PPO"}, {"figure_path": "DX5GUwMFFb/tables/tables_23_2.jpg", "caption": "Table 3: Default parameters for CleanRL TD3 implementation.", "description": "This table lists the default hyperparameters used in the CleanRL implementation of the Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm.  These parameters control aspects of the replay buffer, minibatch size, discount factor, exploration and policy noise, learning rate, update frequency, noise clipping, when learning starts, and target smoothing coefficient.", "section": "F.2 Choice of Hyper-parameters for TD3"}, {"figure_path": "DX5GUwMFFb/tables/tables_24_1.jpg", "caption": "Table 3: Default parameters for CleanRL TD3 implementation.", "description": "This table lists the default hyperparameters used in the CleanRL implementation of the Twin Delayed Deep Deterministic policy gradient (TD3) algorithm.  These parameters control various aspects of the training process, including the size of the replay buffer, the minibatch size used for updates, the discount factor, exploration and policy noise parameters, learning rate, update frequency, target network update parameters, and more.  The settings shown are the defaults provided by the CleanRL library, and may have been adjusted during experimentation as described in the paper.", "section": "F.2 Choice of Hyper-parameters for TD3"}, {"figure_path": "DX5GUwMFFb/tables/tables_24_2.jpg", "caption": "Table 5: Random Search Procedure for Hyperparameters", "description": "This table details the hyperparameter ranges used in a random search for optimizing the actor and critic learning rates, Adam optimizer parameters (beta1, beta2), entropy coefficient (alpha_lr), discount factor (gamma), and Polyak averaging coefficient (critic_tau).  The neural network activation function (Leaky ReLU), number of hidden layers (2), number of hidden units (256), and weight initialization method (Orthogonal) are also specified.", "section": "F.4 Hyper-Parameter Optimization Using Random Search"}, {"figure_path": "DX5GUwMFFb/tables/tables_26_1.jpg", "caption": "Table 6: Default parameters for Robot Tasks.", "description": "This table lists the hyperparameters used for both the AVG and SAC algorithms when applied to real robot experiments.  It shows the differences in parameters such as replay buffer size (which is 1 for AVG, indicating no replay buffer), minibatch size, discount factor, learning rates for actor and critic networks, update frequencies for actor and critic networks, target smoothing coefficients (only used for SAC), target entropy, entropy coefficients and the optimizer used. The table highlights the key differences in the approach taken for incremental learning in AVG versus the batch methods in SAC, especially in the management of replay buffers and target networks.", "section": "6 AVG with Resource-Constrained Robot Learning"}, {"figure_path": "DX5GUwMFFb/tables/tables_27_1.jpg", "caption": "Table 1: Comparison of SAC and AVG algorithms", "description": "This table provides a comparison between the Soft Actor-Critic (SAC) algorithm and the proposed Action Value Gradient (AVG) algorithm.  It highlights key differences in the number of networks, the use of target networks, the presence of a replay buffer, and the treatment of the entropy coefficient. The comparison emphasizes the differences in architecture and training methods between the off-policy SAC and the on-policy AVG algorithms.", "section": "D Incremental Soft Actor Critic (SAC-1)"}]