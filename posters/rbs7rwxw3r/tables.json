[{"figure_path": "RbS7RWxw3r/tables/tables_6_1.jpg", "caption": "Table 1: Average normalized scores of CPI with the mean and standard deviation and previous methods on the D4RL benchmark. D-QL is short for Diffusion-QL. CPI achieves best overall performance among all the methods and consumes quite few computing resources. The top-3 results on each dataset is marked as bold.", "description": "This table presents a comparison of the average normalized scores achieved by the proposed CPI algorithm and several existing offline reinforcement learning methods across various datasets from the D4RL benchmark.  The results are presented with mean and standard deviation, and the top three performing methods for each dataset are highlighted in bold.  The table also highlights that CPI achieves the best overall performance and requires fewer computational resources.", "section": "5.2 Results on Continuous Control Problems"}, {"figure_path": "RbS7RWxw3r/tables/tables_16_1.jpg", "caption": "Table 1: Average normalized scores of CPI with the mean and standard deviation and previous methods on the D4RL benchmark. D-QL is short for Diffusion-QL. CPI achieves best overall performance among all the methods and consumes quite few computing resources. The top-3 results on each dataset is marked as bold.", "description": "This table presents the average normalized scores achieved by CPI and other baseline algorithms across various tasks in the D4RL benchmark.  The results are presented with mean and standard deviation, and the top three results for each task are highlighted in bold. The table highlights CPI's superior performance and efficiency compared to the baselines.", "section": "5.2 Results on Continuous Control Problems"}, {"figure_path": "RbS7RWxw3r/tables/tables_17_1.jpg", "caption": "Table 1: Average normalized scores of CPI with the mean and standard deviation and previous methods on the D4RL benchmark. D-QL is short for Diffusion-QL. CPI achieves best overall performance among all the methods and consumes quite few computing resources. The top-3 results on each dataset is marked as bold.", "description": "This table presents a comparison of the average normalized scores achieved by the proposed CPI algorithm and several other baseline methods on various tasks from the D4RL benchmark.  The table includes the mean and standard deviation for each method's performance on each dataset. The top three performing methods are highlighted in bold for each dataset, demonstrating the superior performance of CPI.", "section": "5.2 Results on Continuous Control Problems"}, {"figure_path": "RbS7RWxw3r/tables/tables_18_1.jpg", "caption": "Table 1: Average normalized scores of CPI with the mean and standard deviation and previous methods on the D4RL benchmark. D-QL is short for Diffusion-QL. CPI achieves best overall performance among all the methods and consumes quite few computing resources. The top-3 results on each dataset is marked as bold.", "description": "This table compares the average normalized scores achieved by the proposed CPI algorithm and several other state-of-the-art offline reinforcement learning methods on various D4RL benchmark datasets.  The results are presented with mean and standard deviation, highlighting CPI's superior performance and computational efficiency. Top 3 results are emphasized in bold.", "section": "5.2 Results on Continuous Control Problems"}, {"figure_path": "RbS7RWxw3r/tables/tables_20_1.jpg", "caption": "Table 1: Average normalized scores of CPI with the mean and standard deviation and previous methods on the D4RL benchmark. D-QL is short for Diffusion-QL. CPI achieves best overall performance among all the methods and consumes quite few computing resources. The top-3 results on each dataset is marked as bold.", "description": "This table presents a comparison of the proposed CPI algorithm's performance against several existing offline reinforcement learning methods on the D4RL benchmark.  The table shows the average normalized scores (mean \u00b1 standard deviation) achieved by each algorithm on various tasks, highlighting CPI's superior performance and efficiency.", "section": "5.2 Results on Continuous Control Problems"}, {"figure_path": "RbS7RWxw3r/tables/tables_20_2.jpg", "caption": "Table 1: Average normalized scores of CPI with the mean and standard deviation and previous methods on the D4RL benchmark. D-QL is short for Diffusion-QL. CPI achieves best overall performance among all the methods and consumes quite few computing resources. The top-3 results on each dataset is marked as bold.", "description": "This table presents the average normalized scores achieved by CPI and various other offline reinforcement learning methods across multiple D4RL benchmark datasets.  The results include the mean and standard deviation for CPI, highlighting its superior performance and computational efficiency compared to other methods. Top 3 results for each dataset are emphasized.", "section": "5.2 Results on Continuous Control Problems"}, {"figure_path": "RbS7RWxw3r/tables/tables_20_3.jpg", "caption": "Table 1: Average normalized scores of CPI with the mean and standard deviation and previous methods on the D4RL benchmark. D-QL is short for Diffusion-QL. CPI achieves best overall performance among all the methods and consumes quite few computing resources. The top-3 results on each dataset is marked as bold.", "description": "This table presents the average normalized scores achieved by CPI and several other offline reinforcement learning algorithms across various tasks from the D4RL benchmark.  The scores are normalized, and the top three results for each task are highlighted in bold.  The table also shows that CPI achieves the best overall performance while using relatively few computing resources.", "section": "5.2 Results on Continuous Control Problems"}, {"figure_path": "RbS7RWxw3r/tables/tables_21_1.jpg", "caption": "Table 1: Average normalized scores of CPI with the mean and standard deviation and previous methods on the D4RL benchmark. D-QL is short for Diffusion-QL. CPI achieves best overall performance among all the methods and consumes quite few computing resources. The top-3 results on each dataset is marked as bold.", "description": "This table presents the average normalized scores achieved by CPI and several other offline reinforcement learning algorithms across various D4RL benchmark tasks.  It compares the performance of CPI against state-of-the-art baselines, highlighting CPI's superior performance and efficiency in terms of computation time.  The table includes the mean and standard deviation of the results, and the top three results for each dataset are bolded.", "section": "5.2 Results on Continuous Control Problems"}, {"figure_path": "RbS7RWxw3r/tables/tables_21_2.jpg", "caption": "Table 1: Average normalized scores of CPI with the mean and standard deviation and previous methods on the D4RL benchmark. D-QL is short for Diffusion-QL. CPI achieves best overall performance among all the methods and consumes quite few computing resources. The top-3 results on each dataset is marked as bold.", "description": "This table compares the average normalized scores achieved by the proposed CPI algorithm and several other state-of-the-art offline reinforcement learning algorithms on various D4RL benchmark datasets.  The table highlights the superior performance of CPI in terms of average normalized scores, while also pointing out its computational efficiency. The top three performing algorithms for each dataset are marked in bold.", "section": "5.2 Results on Continuous Control Problems"}, {"figure_path": "RbS7RWxw3r/tables/tables_22_1.jpg", "caption": "Table 1: Average normalized scores of CPI with the mean and standard deviation and previous methods on the D4RL benchmark. D-QL is short for Diffusion-QL. CPI achieves best overall performance among all the methods and consumes quite few computing resources. The top-3 results on each dataset is marked as bold.", "description": "This table presents the average normalized scores achieved by CPI and other state-of-the-art offline reinforcement learning methods on various D4RL benchmark tasks.  The scores are normalized, and the top three results for each task are highlighted in bold.  The table also indicates that CPI outperforms other methods while requiring fewer computational resources.", "section": "5.2 Results on Continuous Control Problems"}]