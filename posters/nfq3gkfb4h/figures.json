[{"figure_path": "nfq3GKfb4h/figures/figures_8_1.jpg", "caption": "Figure 1: (a) Mean expected likelihood of unseen choice data as a function of the number of queries observed for various choice models on the Dumbalska elicitation task. (b-c) Mean recommendation regret as a function of the number of queries observed for the crash structure design and water drainage network design respectively. (d) Maximum utility within the top k of routes ranked by inferred utility as a function of k. All plots show the mean \u00b1 twice the standard error around the mean.", "description": "This figure presents the results of four different experiments. (a) shows the mean expected likelihood of unseen choice data as a function of the number of queries for various choice models in the Dumbalska elicitation task. (b) and (c) show the mean recommendation regret as a function of the number of queries for crash structure design and water drainage network design, respectively. (d) shows the maximum utility within the top k routes ranked by inferred utility as a function of k. Error bars represent twice the standard error.", "section": "4.4 Simulated case studies"}, {"figure_path": "nfq3GKfb4h/figures/figures_13_1.jpg", "caption": "Figure 2: Graphical models of (a) our cognitive choice model and (b) the corresponding preference learning problem.", "description": "The figure shows two graphical models. Model (a) illustrates the cognitive process underlying human choices, where individuals make decisions based on noisy observations of option utilities and attribute comparisons, rather than directly observing utilities. Model (b) depicts the preference learning problem, where an AI system aims to infer latent utilities from observed choices, treating the internal noisy observations as hidden variables.", "section": "3 Modeling computationally rational choice"}, {"figure_path": "nfq3GKfb4h/figures/figures_13_2.jpg", "caption": "Figure 3: Reversal rate minus inverse reversal rate as a function of \u03c3calc on Range-Frequency conditions for \u011d (\"surrogate\") and for the original implementation of Howes et al. [19] (\"surrogate\"). For \u011d, we show the mean \u00b1 std. dev. for 10 models trained with different seeds. The \"reversal rate\" is measured by calculating the rate at which the Pareto-optimal decoy-dominating option is chosen. To control for random variation we subtract from this the \"inverse reversal rate\", the rate at which the other Pareto-optimal option is chosen. For non-zero values of \u03c3calc, we see that though \u011d is less sensitive to \u03c3calc, it reproduces the range of reversal rates of the original model.", "description": "The figure compares the reversal rate (the rate at which a user chooses the Pareto-optimal option that dominates the decoy) minus the inverse reversal rate (the rate at which the user chooses the other Pareto-optimal option) as a function of calculation noise standard deviation (\u03c3calc) between the original model by Howes et al. [19] and the CRCS surrogate model. The CRCS model, despite being less sensitive to \u03c3calc, successfully reproduces the range of reversal rates observed in the original model, validating its effectiveness.", "section": "4.1 Validation of the CRCS model: risky choice tasks with preference reversals"}, {"figure_path": "nfq3GKfb4h/figures/figures_14_1.jpg", "caption": "Figure 4: (a) The distribution of individual options in the choice data on Dumbalska. (b) The prior p(x<sub>i</sub>) over individual options we use to generate new option sets.", "description": "This figure shows the empirical distribution of options in the Dumbalska dataset.  Panel (a) displays the actual distribution of rental costs and participant valuations observed in the data, indicating a strong positive correlation between the two variables. Panel (b) shows the prior distribution p(x<sub>i</sub>) used to generate new sets of options for the experiment, showing a similar bivariate normal distribution designed to capture the correlation observed in the original data while providing sufficient support across the space of possible options.", "section": "A Human data experiments"}, {"figure_path": "nfq3GKfb4h/figures/figures_16_1.jpg", "caption": "Figure 1: (a) Mean expected likelihood of unseen choice data as a function of the number of queries observed for various choice models on the Dumbalska elicitation task. (b-c) Mean recommendation regret as a function of the number of queries observed for the crash structure design and water drainage network design respectively. (d) Maximum utility within the top k of routes ranked by inferred utility as a function of k. All plots show the mean \u00b1 twice the standard error around the mean.", "description": "This figure shows the results of four different experiments. (a) shows how well different models predict unseen choices in the Dumbalska dataset as the number of queries increases. (b) and (c) show how well different models recommend designs in crash structure and water drainage network design tasks as more user preferences are elicited. (d) shows how well different models rank retrosynthesis routes as the number of ranked options increases.", "section": "4 Experiments"}, {"figure_path": "nfq3GKfb4h/figures/figures_18_1.jpg", "caption": "Figure 1: (a) Mean expected likelihood of unseen choice data as a function of the number of queries observed for various choice models on the Dumbalska elicitation task. (b-c) Mean recommendation regret as a function of the number of queries observed for the crash structure design and water drainage network design respectively. (d) Maximum utility within the top k of routes ranked by inferred utility as a function of k. All plots show the mean \u00b1 twice the standard error around the mean.", "description": "This figure presents the results of four different experiments.  (a) shows the mean expected likelihood of unseen choices over time in the Dumbalska dataset for several models. (b) and (c) show the mean recommendation regret (difference between the optimal and recommended design) for crash structure design and water drainage network design, respectively. Finally, (d) illustrates the maximum utility achieved within the top k ranked routes (retrosynthesis task) as a function of k.", "section": "4.4 Simulated case studies"}, {"figure_path": "nfq3GKfb4h/figures/figures_19_1.jpg", "caption": "Figure 1: (a) Mean expected likelihood of unseen choice data as a function of the number of queries observed for various choice models on the Dumbalska elicitation task. (b-c) Mean recommendation regret as a function of the number of queries observed for the crash structure design and water drainage network design respectively. (d) Maximum utility within the top k of routes ranked by inferred utility as a function of k. All plots show the mean \u00b1 twice the standard error around the mean.", "description": "This figure shows the results of four experiments conducted to evaluate the performance of different choice models in different settings.  Panel (a) shows the mean expected likelihood of unseen choices on the Dumbalska dataset as a function of the number of queries. Panels (b) and (c) show the mean recommendation regret for crash structure and water drainage design, respectively. Panel (d) shows the maximum utility within the top k routes, ranked by inferred utility, as a function of k for retrosynthesis planning. Error bars represent \u00b1 twice the standard error around the mean.", "section": "4.4 Simulated case studies"}, {"figure_path": "nfq3GKfb4h/figures/figures_20_1.jpg", "caption": "Figure 2: Graphical models of (a) our cognitive choice model and (b) the corresponding preference learning problem.", "description": "This figure shows two graphical models. (a) depicts the cognitive model for human choice behavior used in the paper. It shows how a human makes choices based on noisy observations of utilities (\u0169) and ordinal relationships (\u00f5) between option attributes. The true utilities (u) and options (x) are not directly observed. (b) shows the corresponding preference learning problem from the perspective of an AI system. The AI system observes choices (y) made over options (x) but does not observe the noisy observations (\u0169, \u00f5) used by the human. The goal is to infer the underlying utility function parameters (w) and choice model parameters (\u03b8) from the observed data.", "section": "3 Modeling computationally rational choice"}]