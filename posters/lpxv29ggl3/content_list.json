[{"type": "text", "text": "Exploratory Retrieval-Augmented Planning For Continual Embodied Instruction Following ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "Minjong Yoo, Jinwoo Jang, Wei-jin Park, Honguk Woo 1Department of Computer Science and Engineering, Sungkyunkwan University 2Acryl Inc. mjyoo2@skku.edu, jinustar@skku.edu, jin@acryl.ai, hwoo@skku.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This study presents an Exploratory Retrieval-Augmented Planning (ExRAP) framework, designed to tackle continual instruction following tasks of embodied agents in dynamic, non-stationary environments. The framework enhances Large Language Models\u2019 (LLMs) embodied reasoning capabilities by efficiently exploring the physical environment and establishing the environmental context memory, thereby effectively grounding the task planning process in time-varying environment contexts. In ExRAP, given multiple continual instruction following tasks, each instruction is decomposed into queries on the environmental context memory and task executions conditioned on the query results. To efficiently handle these multiple tasks that are performed continuously and simultaneously, we implement an exploration-integrated task planning scheme by incorporating the informationbased exploration into the LLM-based planning process. Combined with memoryaugmented query evaluation, this integrated scheme not only allows for a better balance between the validity of the environmental context memory and the load of environment exploration, but also improves overall task performance. Furthermore, we devise a temporal consistency refinement scheme for query evaluation to address the inherent decay of knowledge in the memory. Through experiments with VirtualHome, ALFRED, and CARLA, our approach demonstrates robustness against a variety of embodied instruction following scenarios involving different instruction scales and types, and non-stationarity degrees, and it consistently outperforms other state-of-the-art LLM-based task planning approaches in terms of both goal success rate and execution efficiency. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The application of Large Language Models (LLMs) in embodied AI is essential for harnessing common knowledge and immediately applying it to unseen tasks and domains without requiring additional training or data. Researchers are further enhancing task adaptation by integrating environmental information with the intrinsic common knowledge of LLMs [1, 2, 3, 4, 5, 6, 7]. This capability proves invaluable in fields such as home robotics and autonomous driving, where it enables embodied agents to learn across diverse instruction following tasks with minimal data requirements. ", "page_idx": 0}, {"type": "text", "text": "For embodied agents, these tasks are often not mere single, one-time instructions but are multiple and persistent, necessitating continuous access to environmental knowledge to reason and plan effectively for user needs. In such scenarios, the efficiency of repeatedly collecting environmental knowledge through interaction each time the agent plans can be suboptimal. Furthermore, there is a clear need to integrate and manage multiple user requirements effectively. ", "page_idx": 0}, {"type": "text", "text": "In this paper, we investigate continual instruction following for an embodied agent, where multiple tasks are contingent upon the real-time information of a continuously changing environment. This setup requires the agent to engage in ongoing exploration of the environment to adaptively respond to dynamic changes and fulfill the required tasks. To address the problem of continual instruction following, we present an exploratory retrieval-augmented planning (ExRAP) framework, designed to enhance LLMs\u2019 embodied reasoning capabilities by integrating environmental context memory. ", "page_idx": 1}, {"type": "text", "text": "In ExRAP, to improve effectiveness and efficiency in managing environment interaction and exploration loads for multiple embodied tasks, we employ an exploration-integrated task planning scheme, in which the information-based exploration is incorporated into the LLM-based planning process. This integrated planning scheme establishes a robust policy that balances the validity of the environmental context memory with the demands of environment interaction and exploration. We also devise a temporal consistency-based refinement scheme to ensure the robustness of memory-augmented query evaluation on environmental conditions. Through experiments with VirtualHome [8], ALFRED [9] and CARLA [10], we demonstrate that the ExRAP framework achieves competitive performance in both task success and efficiency compared to several state-of-the-art embodied planning methods, including ZSP [11], SayCan [1], ProgPrompt [3], and LLM-Planner [12]. ", "page_idx": 1}, {"type": "text", "text": "Our contributions are summarized as: First, we propose a novel ExRAP framework, systematically combining LLMs\u2019 reasoning capabilities and environmental context memory into explorationintegrated task planning to tackle continuous instruction following tasks in non-stationary embodied environments. We also introduce two schemes tailored for exploration-integrated task planning in ExRAP, information-based exploration estimation and temporal consistency-based refinement on memory-augmented query evaluation. Finally, we demonstrate superior performance and robustness of ExRAP via intensive experiments with home robots and autonomous driving scenarios in VirtualHome, ALFRED, and CARLA. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Embodied instruction following. Embodied instruction following involves executing complex tasks based on an understanding of embodied knowledge. This aims to grasp various aspects of the physical environment including objects, their relations, and dynamics, and to plan appropriate sequences of actions or skills to complete the tasks specified by instructions successfully. In the area of task planning, there have been many works to combine LLMs\u2019 reasoning capabilities with environmental characteristics. Recent research explored the utilization of skills\u2019 affordances to compute their values [1, 2], implemented code-driven policies [3, 4], and generated reward functions [13, 14], while highlighting the use of LLMs\u2019 enhanced abilities in task planning. Moreover, LLM-driven environment modeling approaches, utilizing LLMs\u2019 common knowledge and reasoning about realworld objects, have been introduced [5, 6, 7]. These LLM-based approaches to task planning have been applied across a range of embodied instruction following tasks, facilitated by repeated interactions with environments, humans, or other agents [15, 16, 12, 17, 18, 19]. ", "page_idx": 1}, {"type": "text", "text": "While these approaches underscore the versatility and depth of LLMs for task planning and embodied agent control, they often rely on a non-systematic integration of observations to the LLMs\u2019 reasoning process. Furthermore, they rarely consider continual instruction following scenarios, where an agent should handle a set of instructions continuously, adapting to real-time environmental conditions. In contrast, our work differentiates itself by incorporating agents\u2019 exploration capabilities, which are guided by information gains, into continual instruction following. ", "page_idx": 1}, {"type": "text", "text": "Retrieval-augmented generation for LLM. Research in retrieval-augmented generation (RAG) focused on efficiently executing tasks by sourcing and utilizing task-related information from databases. In particular, enhancing the performance of retrieval, which suggests relevant data when an LLM requires specific knowledge for the tasks, involves training retrievers [20, 21, 22, 23], fine-tuning the LLM to adapt the RAG process [24, 25], or exploiting the LLM itself for dynamic query reformation [26, 27]. In the area of embodied task planning, recent studies adopt the integration of RAG with task-specific demonstrations [12]. Our work also uses RAG for embodied task planning, but it uniquely emphasizes its dynamic aspects. For continual instruction following, we prioritize the relevance and significance of the agent\u2019s skills, not only to perform tasks but also to ensure continuous and efficient synchronization of its environmental memory with changes in the environment. ", "page_idx": 1}, {"type": "image", "img_path": "LpXV29Ggl3/tmp/2d48c070a3ed164f478dbbfabedec909f6f74bc8a8608f817f6175de0e22181b.jpg", "img_caption": ["Figure 1: Concept of ExRAP. In the embodied environment, this framework manages continual instructions, a set of instructions for embodied instruction following tasks that are conducted continuously and simultaneously. At each step, it operates through (a) memory-augmented query evaluation, and (b) exploration-integrated task planning coupled with environmental context memory updates (as shown in the left side of the figure). By performing this integrated plan in conjunction with the memory, the ExRAP framework achieves more efficient task execution in response to the continual instructions, compared to the instruction-wise planning (as in the right side of the figure). "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Exploration in reinforcement learning (RL). In the field of RL, exploration methods, designated to efficiently gather environmental information, have been a focus of research. Strategies were developed, that prioritize the exploration of new environmental information, by offering intrinsic rewards [28, 29] or through navigation schemes derived from offline demonstrations [30, 31, 32]. Particularly, in DREAM [33], an exploration policy is formulated to adapt to varying conditions by using data gathered during initial exploration episodes. This policy is optimized by maximizing the information gain to better adapt to changes. Our work adapts this mutual information-based exploration strategy with RAG for embodied instruction following. Our proposed framework is the first to implement exploration-integrated task planning with LLMs, enabling the efficient execution of continuouslyperformed instructions and facilitating the dynamic adaptation to changing environmental conditions. ", "page_idx": 2}, {"type": "text", "text": "3 Approach ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Continual instruction following ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We consider a set of instructions for embodied tasks that are continuously and simultaneously conducted based on specific environmental contexts. In Figure 1, the instructions entail conditional actions like \u201cIf the temperature is high, open the window\u201d and \u201cWhen watching TV, turn off the light.\u201d The agent continuously explores the environment to verify whether the conditions (e.g., temperature and TV status) are met. Upon confirmations, the agent executes the associated tasks within the environment. We refer to these scenarios, where multiple embodied tasks are conditioned on environmental contexts and conducted continuously, as continual instruction following. This concept aligns closely with continuous queries [34, 35] in database literature, which monitor updates of interest over time and return results when specific thresholds or conditions are met. This is in contrast to single in-situ instruction following, where each task is executed based on isolated, one-time directives. ", "page_idx": 2}, {"type": "text", "text": "For continual instruction following tasks with conditional instructions $\\mathcal{T}=\\{i_{1},...,i_{M}\\}$ , we consider a non-stationary embodied environment that changes over time. The conditions of continual instructions may or may not be satisfied over time, requiring continuous exploration in the environment. When certain conditions are met, the associated tasks should be performed promptly. For this continual instruction following tasks in the non-stationary environment, we evaluate agent performance in terms of task completion and efficiency. Our goal is to establish an embodied agent policy $\\pi^{*}$ that maximizes the overall performance of continual instruction following tasks. Specifically, we formulate the reward as a combination of (i) the task success rate SR and (ii) the average pending step PS. SR is the rate of completed tasks whose conditions have been met, and PS is the average steps required to accomplish the task associated with instruction $i\\in\\mathcal{T}_{C}$ whenever the condition is met. For instructions $\\mathcal{T}$ and timestep $t$ , we then formulate the agent policy $\\pi^{*}$ performing a skill upon observation $o_{t}$ as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pi^{*}=\\operatorname*{argmax}_{\\boldsymbol{\\pi}}\\bigg[\\sum_{t}\\mathrm{SR}\\big(s_{t},\\pi\\big(o_{t},\\mathcal{Z}\\big)\\big)+\\mathbb{E}_{i\\in\\mathcal{Z}_{C}}[-\\mathrm{PS}(\\pi,i)]\\bigg].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Optimizing both SR and PS allows the agent not only to appropriately plan for multiple instruction following tasks but also to strategically integrate these instructions to improve overall efficiency. The resulting policy is able to minimize redundant skill executions by addressing multiple tasks in an integrated manner, taking into account the possible spatial and temporal overlap of the task requirements. For instance, as illustrated in Figure 1, an integrated plan achieves a pending step of 7, the average of required timesteps 7, 4 and 10 for the three instructions. This is significantly shorter than 9.2, the average of 5, 9, and 14 achieved by an instruction-wise plan. ", "page_idx": 3}, {"type": "text", "text": "3.2 Overall framework ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To address the challenge of continual instruction following in a non-stationary embodied environment, we develop the ExRAP framework. It is designed to minimize the necessity for environmental interaction, by utilizing memory-augmented and exploration-integrated planning schemes while ensuring robust task performance. ", "page_idx": 3}, {"type": "text", "text": "In ExRAP, each conditional instruction $i\\in\\mathcal{Z}$ is decomposed into two primary components: query $q$ and execution $e$ . Queries function as conditions for task initiation and are evaluated against environmental information. Executions, on the other hand, involve physical interactive manipulations that are triggered based on the results of query evaluation. In a non-stationary environment, evaluating queries poses a unique challenge due to the need for the agent to continuously synchronize with constantly changing information. This synchronization often necessitates continual exploration, resulting in intensive interaction with the environment. ", "page_idx": 3}, {"type": "text", "text": "As described in Figure 1, ExRAP addresses this challenge through two components: (a) query evaluation using environmental context memory and (b) exploration-integrated task planning. In (a), the environmental context memory is established via a temporal embodied knowledge graph to effectively represent the dynamic environment. Augmented with this graph-based context memory, the LLM-based query evaluator responds to queries by checking if their conditions are met and provides confidence levels for these assessments. To address the information decay, which stems from synchronization uncertainty between the previously collected environmental context and the actual current state of the environment, we incorporate entropy-based temporal consistency refinement into the query evaluation process. In (b), ExRAP plans skills that are instrumental not only for achieving tasks from an exploitation perspective but also for boosting confidence in the query evaluations from an exploration perspective. To effectively plan skills that balance both perspectives, we integrate the exploitation value of skills, which is derived from the in-context learning ability of LLMs, with their exploration value, which is determined through information-based estimation. ", "page_idx": 3}, {"type": "text", "text": "3.3 Memory-augmented query evaluation with temporal consistency ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We represent both the environmental context memory and the observations perceived by the agent using a temporal embodied knowledge graph (TEKG), where the memory is established through the accumulation of these observations. Queries, derived from given instructions, are evaluated against the context memory, with consideration for inherent information decay within the previously accumulated data. The query evaluation procedure is described on the upper side of Figure 2. ", "page_idx": 3}, {"type": "text", "text": "TEKG and retriever. The TEKG comprises a set of quadruples $\\tau=(s e,r e,t e,t)$ consisting of source entity $s e$ , relation $r e$ , target entity $t e$ , and timesteps $t$ . We represent the environmental context memory at a specific timestep $t$ within the TEKG, defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\nG_{t}=\\left\\{\\tau_{1},\\tau_{2},\\cdot\\cdot\\cdot,\\tau_{N}\\right\\}{\\mathrm{~where~}}\\tau_{i}=(s e_{i},r e_{i},t e_{i},t_{i}),\\;t_{i}\\leq t.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "To integrate the current observation $o_{t+1}$ into previously established up-to-date memory $G_{t}$ , we employ an update function $\\mu$ as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\nG_{t+1}=\\mu(G_{t},o_{t+1})=\\{\\tau\\in G_{t}\\,|\\,c(\\tau,\\tau^{\\prime})=0,\\,\\forall\\tau^{\\prime}\\in o_{t+1}\\}\\bigcup o_{t+1}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, $c$ is a function that detects the semantic contradictions between quadruples, such as when $\\tau$ and $\\tau^{\\prime}$ indicate that a TV is both \u201coff\u201d and \u201con\u201d. It returns 1 if there is a contradiction, otherwise 0. ", "page_idx": 3}, {"type": "text", "text": "Constructed memory $G$ serves as a knowledge database for continual instruction following, enabling the retrieval of environmental information related to specific task directives such as instructions, queries, and executions. Specifically, for language-specified task directives ${\\mathcal{L}}=\\{l\\}$ , the retriever $\\Phi_{R}$ ", "page_idx": 3}, {"type": "image", "img_path": "LpXV29Ggl3/tmp/d9b803772987c123c488d9d0eb42f83a8f2f26204027e14f5eab90e476247e5a.jpg", "img_caption": ["", "Figure 2: Overall procedures of ExRAP. (a) Query evaluation: The instruction interpreter $\\Phi_{I}$ produces queries and executions, as well as a condition function from continual instructions. The memoryaugmented query evaluator $\\Phi_{M}$ then evaluates these queries probabilistically using the LLM with a retrieved TEKG from the environmental context memory. (b) Exploration-integrated task planning: The LLM-based exploitation planner $v_{T}$ estimates the value of skills based on their executions and relevant demonstrations in an in-context manner. Simultaneously, the exploration planner $v_{R}$ evaluates these skills using the subsequent TEKG through information-based value estimation. At each step, a skill is then selected based on the integrated skill value from the two estimations. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "interacts with the memory $G$ and samples $k$ quadruples $\\{\\hat{\\tau}_{1},\\cdot\\cdot\\cdot,\\hat{\\tau}_{k}\\}$ . The sampling is based on the multinomial softmax distribution, where the likelihood of retrieving a quadruple $\\tau$ is determined by the highest sentence embedding similarity between $\\tau$ and any $l$ in $\\mathcal{L}$ . ", "page_idx": 4}, {"type": "text", "text": "Instruction interpreter. The instruction interpreter $\\Phi_{I}$ processes continual instructions $\\mathcal{Z}\\mathbf{\\Omega}=$ $\\{i_{1},...,i_{M}\\}$ , translating them into queries $\\mathcal{Q}$ and corresponding task executions $\\mathcal{E}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Phi_{I}({\\mathbb{Z}})={\\big(}Q:(q_{1},\\ldots,q_{M}),\\,\\mathcal{E}:(e_{1},\\ldots,e_{M}),\\,C{\\big)}{\\mathrm{~where~}}C(q_{j})=e_{j}{\\mathrm{~for~}}\\forall j.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, $C$ is a conditional function that maps each query to its respective execution counterpart. ", "page_idx": 4}, {"type": "text", "text": "Query evaluator. The memory-augmented query evaluator $\\Phi_{M}$ estimates the likelihood $P(q|G_{t})$ of query $q\\,\\in\\,\\mathcal{Q}$ being satisfied, using the historical memory accumulated over time, denoted as $G_{1:t}\\,=\\,G_{1}\\cup\\ldots\\cup G_{t}$ . Leveraging the memory-augmented LLM $(\\Phi_{\\mathrm{LLM}})$ , we develop the query evaluator $\\Phi_{M}$ by incorporating the previous step\u2019s query evaluation $P(q|G_{t-1})$ and a prior of query evaluation $R(q|G_{t-1})$ , which is defined in (7). ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{P(q|G_{t})=\\Phi_{M}(q,t,G_{1:t},P(q|G_{t-1}))=\\left\\{R(q|G_{t-1})\\begin{array}{l l}{}&{\\mathrm{if~}\\hat{G}_{1:t}=\\hat{G}_{1:t-1}}\\\\ {\\mathrm{~d~r~a~n~}(q,t,\\hat{G}_{1:t},R(q|G_{t-1}))}&{\\mathrm{otherwise}}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, $\\hat{G}_{1:t}\\sim\\Phi_{R}(G_{1:t},\\{q\\})$ is retrieved quadruples, and the prior $R(q|G_{t-1})$ is the retrospective query response evaluated at timestep $t$ using $G_{t-1}$ . ", "page_idx": 4}, {"type": "text", "text": "Due to the inherent information decay in the memory over increasing timesteps, a decline in confidence should be considered for the likelihood estimation in (5). To address this, we incorporate an entropy-based temporal consistency as an intermediate step in query evaluation. Specifically, when using the memory from $G_{t-1}$ , we posit that the entropy of the prior query response at timestep $t$ should be higher than at timestep $t\\!-\\!1$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\nH(R(q|G_{t-1}))>H(P(q|G_{t-1})).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "To enforce the temporal consistency, we compute the multiple query response priors using $\\Phi_{\\mathrm{LLM}}$ and discard any responses that do not align with the consistency constraint. Specifically, if the entropy of each prior of query response is smaller than previous step $P(q|G_{t})$ , it is removed. ", "page_idx": 4}, {"type": "equation", "text": "$$\nR(q|G_{t-1})=\\mathbb{E}_{\\hat{G}_{1:t-1}\\sim\\Phi_{R}(G_{1:t-1},\\{q\\})}\\left[\\Phi_{\\mathrm{LLM}}\\left(q,t,\\hat{G}_{1:t-1},P(q|G_{t-1})\\right)\\mathrm{~with~hold~}(6)\\right]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Then, we select a set of corresponding executions $\\mathcal{E}_{t}$ that are likely to require manipulations in the environment, using a filtering threshold $\\theta$ . ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{E}_{t}=\\{C(q)|q\\in\\mathcal{Q},P(q|G_{t})>\\theta\\}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "3.4 Exploration-integrated task planning with information-based estimation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To facilitate integrated task planning for continual instructions, we devise exploitation and exploration planners. The former focuses on exploiting appropriate skills to complete tasks associated with given instructions using the LLM, and the latter focuses on exploring the environment to update the memory in a direction that maximizes information gain. We then integrate their plans to prioritize the next skills to be executed. The resulting planning directs to complete specific task executions $\\mathcal{E}$ , ensuring effective maintenance of the environmental context memory. This maintenance process involves synchronizing the memory with the current state of the environment. The exploration-integrated task planning procedure is described on the lower side of Figure 2. ", "page_idx": 5}, {"type": "text", "text": "Exploitation planner. Given the memory $G_{t}$ and a language description of a skill $z\\in Z$ , the exploitation planner $v_{T}$ is responsible for estimating the value of the skill with respect to its effectiveness in accomplishing the executions $\\mathcal{E}_{t}$ . To do so, we harness the retrieved memory-augmented LLMs along with their in-context learning capabilities. Specifically, under the assumption that we can access expert planning dataset $\\mathcal{D}_{e}$ , we retrieve demonstrations $D$ from $\\mathcal{D}_{e}$ based on the graph similarity between the current observation $o_{t}$ and the observation within $\\mathcal{D}_{e}$ . ", "page_idx": 5}, {"type": "equation", "text": "$$\nv_{T}(G_{t},z)=\\Phi_{\\mathrm{LLM}}(\\mathcal{E}_{t},\\Phi_{R}(G_{t},\\mathcal{E}_{t}),D,z)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Exploration planner. In conjunction with the exploitation planner, the exploration planner $v_{R}$ is responsible for assessing the value of the skill with respect to its utility for reducing the response uncertainty of the query evaluator $\\Phi_{M}$ . This assessment is intended for efficient environmental exploration, thereby facilitating the swift and precise identification of query conditions and maintaining the memory up-to-date. Specifically, we define the exploration value of skill $z$ as the difference of mutual information for consecutive timesteps using the query evaluation result in (5): ", "page_idx": 5}, {"type": "equation", "text": "$$\nv_{R}(G_{t},z)=I(Q;G_{t+1})-I(Q;G_{t})=\\sum_{q\\in Q}H(P(q|G_{t}))-H(P(q|G_{t+1}))\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $I$ denotes mutual information and $G_{t+1}$ is the updated memory by execution of skill $z$ . ", "page_idx": 5}, {"type": "text", "text": "Direct computation of $G_{t+1}$ is impractical without actual skill execution. Therefore, to evaluate the exploration value of skills before their execution, we focus on the entropy related only to the retrieved knowledge pertinent to the evaluated queries $\\mathcal{Q}$ . This approach is feasible under the mild assumption that the entropy of the query evaluator reaches zero (indicating no uncertainty), once the TEKG memory is fully synchronized with the environment. Thus, we approximate the exploration value as ", "page_idx": 5}, {"type": "equation", "text": "$$\nv_{R}(G_{t},z)=\\sum_{q\\in Q}H(P(q|G_{t}))\\left(1-\\frac{d(\\Phi_{R}(G_{t}^{z},\\{q\\})}{d(\\Phi_{R}(G_{t},\\{q\\})}\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $d$ represents the average distance function between the retrieved quadruples and the current agent\u2019s entity in TEKG, and $G_{t}^{z}$ is the predicted partially updated knowledge graph after skill execution, where only quadruples containing the agent as an entity are altered. Note that the exploration value increases as the agent moves closer to the query-related environment parts on the graph through the skill execution. Consequently, the skill is selected by maximizing the integrated skill value, which is obtained by a weighted sum of exploitation and exploration values, as defined in (9) and (11) respectively: ", "page_idx": 5}, {"type": "equation", "text": "$$\nz_{t}=\\underset{z\\in\\cal Z}{\\operatorname{argmax}}[w_{T}\\cdot v_{T}(G_{t},z)+w_{R}\\cdot v_{R}(G_{t},z)].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We evaluate ExRAP across various degrees of non-stationarity, scales of instructions, and instruction types. We also provide ablation studies and qualitative analysis. Further analysis is in Appendix D. ", "page_idx": 5}, {"type": "table", "img_path": "LpXV29Ggl3/tmp/86dbfd6608d75b7c581f7349be4028078435f2c221bb07229fb9d0cc26e8d3d5.jpg", "table_caption": ["Table 1: Performance in VirtualHome, ALFRED, and CARLA w.r.t. non-stationarity. We use the $95\\%$ confidence interval, using 10 random seeds for VirtualHome and 5 random seeds for both ALFRED and CARLA. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Environments. We evaluate ExRAP in the context of household planning and skill-based autonomous driving with VirtualHome [8], ALFRED [9], and CARLA [10], where we use 16 to 19 distinct instructions for continual instruction following tasks. Details are provided in Appendix A. ", "page_idx": 6}, {"type": "text", "text": "Evaluation metric. We employ two evaluation metrics for the objective specified in (1). The task success rate (SR) measures the proportion of completed tasks for continual instructions whose conditions are satisfied at each timestep. Given the continual instructions, the pending step (PS) represents the average number of timesteps required to complete the associated tasks from the moment the conditions of the instructions are actually satisfied in the environment. Note that the agent\u2019s detection time of such condition satisfaction may differ from its actual occurrence. ", "page_idx": 6}, {"type": "text", "text": "Datasets. We use 100 trajectories across 10 different environment settings in VirtualHome, and 50 trajectories in ALFRED and CARLA. These are used for in-context learning of the exploitation planner in ExRAP and the baselines. Note that we use different environment settings for evaluation. ", "page_idx": 6}, {"type": "text", "text": "Baselines. ZSP [11] is an LLM-based zero-shot task planner. Our experiments serve as a baseline to evaluate LLM-based task planning approaches. SayCan [1] is a state-of-the-art embodied agent framework, which integrates both language affordance scores derived from an LLM and embodied affordance scores learned through RL. In our experiments, we use the optimal affordance function for each environment. ProgPrompt [3] is a framework to enhance language models\u2019 capabilities in generating structured and logical outputs, by incorporating programming-like prompts. LLM-Planner [12] is a state-of-the-art embodied agent framework that utilizes LLMs\u2019 embodied knowledge to infer subtask sequences. It incorporates object detection information from the agent\u2019s interactions for enhanced task planning. To address continual instructions and adapt to non-stationary environments, we implement a variant of the LLM-Planner that infers skills in a step-wise manner. ", "page_idx": 6}, {"type": "text", "text": "4.1 Main results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Non-stationarity. Table 1 presents a performance comparison in terms of SR and PS in VirtualHome, ALFRED, and CARLA, respectively, under varying degrees of non-stationarity, where environment changes range from low to high. The higher degree of non-stationarity means the environment changes more rapidly, requiring the agent to focus more on environmental information to adapt effectively. ExRAP achieves superior performance across all degrees of non-stationary. Specifically, ExRAP demonstrates a performance gain in SR by $16.45\\%$ on average compared to the most competitive baseline, the LLM-Planner. Furthermore, ExRAP shows a reduction in PS by 3.40 on average compared to the LLM-Planner. Importantly, the advantage of ExRAP becomes more significant with increasing levels of non-stationarity; the performance gap in SR between the LLM-Planner and ExRAP widens from $15.35\\%$ at low non-stationarity to $18.58\\%$ at high non-stationarity. Similarly, the gap in PS grows from an average of 2.96 to 3.73. This increase in performance can be attributed to ExRAP\u2019s strong ability to promptly discern newly satisfied conditions through exploration-integrated task planning, coupled with accurate memory-augmented query evaluation. This capability enables ExRAP to respond to rapid environmental changes effectively. ", "page_idx": 6}, {"type": "table", "img_path": "LpXV29Ggl3/tmp/d63badd63e3397ddaea8c190d070497c404eff58db156ee2fb3adf02767a2978.jpg", "table_caption": ["Table 2: Performance in VirtualHome w.r.t. instruction scale "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Instruction scale. Table 2 shows a performance comparison under the medium non-stationarity, as the scale of continual instructions increases. As the number of instructions grows, the agent needs to collect more knowledge and perform more tasks. ExRAP achieves an average gain in SR of $18.78\\%$ compared to the most competitive baseline, the LLM-Planner. Additionally, ExRAP reduces PS by an average of 6.31. ExRAP exhibits widening performance gaps as the complexity of tasks increases; the SR gap grows from $18.49\\%$ compared to the LLM-Planner with a small continual instruction scale, to $22.04\\%$ with a large continual instruction scale. Similarly, the PS gap expands from an average of 4.84 to 8.87. This performance difference highlights ExRAP\u2019s effectiveness in addressing multiple continual instructions simultaneously, demonstrating its robust integrated task planning capabilities. In ExRAP, the skill selection is guided by an integrated value derived from queries and executions, enabling the efficient handling of multiple instructions and concurrent memory updates. ", "page_idx": 7}, {"type": "text", "text": "Instruction type. We also test the applicability of ExRAP for three types of continual instructions in VirtualHome with medium non-stationarity. Sentence-wise type organizes each instruction into individual sentences. This is the default setting in our experiments. Summarized type condenses multiple instructions into a fewer number of sentences. Object Ambiguation type contains abstract forms of target objects such as \u201csomething to read.\u201d. Table 3 demonstrates superior performance of ExRAP for those types. ExRAP adapts RAG with the environmental context memory to interpret instructions and decompose them into queries and executions. This memory-augmented approach, retrieving and utilizing the information relevant to given instructions, enables effective grounding of continual instructions in different types within the environment. ", "page_idx": 7}, {"type": "table", "img_path": "LpXV29Ggl3/tmp/fd4168beaed54ebd82a0b13f7382e182ebc6fbca9653b73ba72ecb2d2babc890.jpg", "table_caption": ["Table 3: Performance w.r.t. instruction types "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Qualitative analysis. Figure 3 (a) and (b) compare the exploration strategies of ExRAP and LLMPlanner. Given multiple instructions, ExRAP demonstrates broader exploration. This reflects the exploration-integrated task planning in ExRAP, which rather focuses on the overall gain achieved from each exploration and skill execution. ", "page_idx": 7}, {"type": "text", "text": "4.2 Ablation study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We conduct several ablation studies for ExRAP in VirtualHome with medium non-stationarity. ", "page_idx": 7}, {"type": "text", "text": "Temporal consistency. We compare the performance of our ExRAP and its variant ExRAP-TC that performs query evaluation without temporal consistency-based refinement. As in Table 4, ExRAP outperforms ExRAP-TC by $15.56\\%$ on average. As observed in Figure 3 (a) and (c), with temporal consistency in query responses, information decay is effectively managed, leading to broader exploration areas that accommodate different instructions simultaneously (i.e., in (a)). Otherwise, without temporal consistency, exploration tends to concentrate exclusively on specific knowledge where multiple queries overlap. This renders largely neglected areas experiencing significant decay, often leading to invalidated query evaluation (i.e., in (c)). ", "page_idx": 7}, {"type": "image", "img_path": "LpXV29Ggl3/tmp/b7ad365004327bbe2de3ce440337e2f221a192c076d8a5c9a09bd624a66ae2e3.jpg", "img_caption": ["Figure 3: Knowledge exploration heatmap. Darker color represents high frequency in exploration. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "table", "img_path": "LpXV29Ggl3/tmp/ea97fd1b6edb1de9492445913bfa933bf1fd63a90c63ce8e16b5a943d6c09d70.jpg", "table_caption": ["Table 4: Ablation for query evaluation with temporal consistency "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Exploration strategy. We compare the performance of our ExRAP and two variants ExRAP-LLM and ExRAP-EXP using different planning strategies. ExRAP-LLM directly employs the LLM as the exploration planner, which fully relies on the LLM\u2019s capability without information-based exploration, by prompting the LLM with specific exploration commands, e.g., \u201cexplore the home.\u201d On the other hand, ExRAP-EXP employs only the exploitation planner with specific exploration commands, used only when there are no executions. As shown in Table 5, ExRAP demonstrates higher performance of 26.76 and $17.46\\%$ on average than the two variants, respectively. As in Figure 3 (a) and (d), ExRAP-EXP exhibits reduced exploration capabilities, resulting in a narrower exploration area. ", "page_idx": 8}, {"type": "table", "img_path": "LpXV29Ggl3/tmp/1c65684528fa78ad11457964168123f7ca3e5c05da99b8a1bce0ddbcc82da1bc.jpg", "table_caption": ["Table 5: Ablation for exploration-integrated task planning "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "LLMs for planning. While we tested several LLMs, ranging from relatively smaller models such as Gemma-2B [36] to larger ones such as Llama-3-70B [37], our experiments thus far have utilized LLaMA-3-8B for ExRAP and the baselines. In Table 6, we evaluate ExRAP and two baselines using different LLMs. Both LLM-Planner and SayCan exhibit improved performance with the larger Llama-3-70B, but experience a significant drop in performance with the smaller Gemma-2B model. Unlike those, ExRAP maintains robust performance even with the smaller model, highlighting the benefits of its memory-augmented, integrated planning approach. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We introduced the ExRAP framework to facilitate efficient integrated planning for multiple instruction following tasks, which are conducted continuously and simultaneously in the embodied environment. With the extended RAG architecture, the framework incorporates memory-augmented query evaluation and exploration-integrated task planning schemes, thereby achieving both efficient environment exploration and robust task completion. Via experiments conducted in VirtualHome, ALFRED, and CARLA, we demonstrated the robustness of ExRAP across various continual instruction following scenarios, specifying its advantages over other LLM-driven task planning approaches. ", "page_idx": 8}, {"type": "table", "img_path": "LpXV29Ggl3/tmp/fed5e8c8f9a9a810e2fa2d2d52343e70f88af0c2a4b40d1bad40963566874c46.jpg", "table_caption": ["Table 6: Impact of different LLMs "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Limitation. ExRAP leverages LLMs, which makes its performance dependent on the capabilities of these models to some extent. Compared to other baselines, it also requires increased computation effort due to the management of environmental context memory with temporal consistency. The ablation studies demonstrate that ExRAP is able to deliver robust performance even with a relatively lightweight LLM, yet further investigation into runtime overhead is desired. ", "page_idx": 9}, {"type": "text", "text": "6 Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT), (RS-2022-II220043 (2022-0- 00043), Adaptive Personality for Intelligent Agents, RS-2022-II221045 (2022-0-01045), Self-directed multi-modal Intelligence for solving unknown, open domain problems, and RS-2019-II190421, Artificial Intelligence Graduate School Program (Sungkyunkwan University)), the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. RS-2023-00213118), IITP-ITRC (Information Technology Research Center) grant funded by the Korea government (MIST) (IITP-2024-RS-2024-00437633, $10\\%$ ), BK21 FOUR Project (S-2024-0580-000), and by Samsung Electronics. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Anthony Brohan et al. \u201cDo as i can, not as i say: Grounding language in robotic affordances\u201d. In: Proceedings of the 6th Conference on Robot Learning (CoRL). PMLR. 2023, pp. 287\u2013318.   \n[2] Rishi Hazra, Pedro Zuidberg Dos Martires, and Luc De Raedt. \u201cSayCanPay: Heuristic Planning with Large Language Models using Learnable Domain Knowledge\u201d. In: Proceedings of the 42nd AAAI Conference on Artificial Intelligence. Vol. 38. 18. 2024, pp. 20123\u201320133.   \n[3] Ishika Singh et al. \u201cProgprompt: Generating situated robot task plans using large language models\u201d. In: Proceedings of the 40th International Conference on Robotics and Automation (ICRA). IEEE. 2023, pp. 11523\u201311530.   \n[4] Jacky Liang et al. \u201cCode as policies: Language model programs for embodied control\u201d. In: Proceedings of the 40th International Conference on Robotics and Automation (ICRA). IEEE. 2023, pp. 9493\u20139500.   \n[5] Lin Guan et al. \u201cLeveraging pre-trained large language models to construct and utilize world models for model-based task planning\u201d. In: Proceedings of the 36th Advances in Neural Information Processing Systems (NeurIPS) 36 (2023), pp. 79081\u201379094.   \n[6] Shibo Hao et al. \u201cReasoning with language model is planning with world model\u201d. In: arXiv preprint arXiv:2305.14992 (2023).   \n[7] Kolby Nottingham et al. \u201cDo embodied agents dream of pixelated sheep: Embodied decision making using language guided world modelling\u201d. In: Proceedings of the 40th International Conference on Machine Learning (ICML). PMLR. 2023, pp. 26311\u201326325.   \n[8] Xavier Puig et al. \u201cVirtualhome: Simulating household activities via programs\u201d. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018, pp. 8494\u2013 8502.   \n[9] Mohit Shridhar et al. \u201cAlfred: A benchmark for interpreting grounded instructions for everyday tasks\u201d. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2020, pp. 10740\u201310749.   \n[10] Alexey Dosovitskiy et al. \u201cCARLA: An Open Urban Driving Simulator\u201d. In: Proceedings of the 1st Conference on Robot Learning (CoRL). 2017, pp. 1\u201316.   \n[11] Wenlong Huang et al. \u201cLanguage models as zero-shot planners: Extracting actionable knowledge for embodied agents\u201d. In: Proceedings of the 39th International Conference on Machine Learning (ICML). PMLR. 2022, pp. 9118\u20139147.   \n[12] Chan Hee Song et al. \u201cLlm-planner: Few-shot grounded planning for embodied agents with large language models\u201d. In: Proceedings of the 19th International Conference on Computer Vision (ICCV). 2023, pp. 2998\u20133009.   \n[13] Wenhao Yu et al. \u201cLanguage to rewards for robotic skill synthesis\u201d. In: arXiv preprint arXiv:2306.08647 (2023).   \n[14] Ademi Adeniji et al. \u201cLanguage reward modulation for pretraining reinforcement learning\u201d. In: arXiv preprint arXiv:2308.12270 (2023).   \n[15] Wenlong Huang et al. \u201cInner monologue: Embodied reasoning through planning with language models\u201d. In: arXiv preprint arXiv:2207.05608 (2022).   \n[16] Haotian Sun et al. \u201cAdaplanner: Adaptive planning from feedback with language models\u201d. In: Proceedings of the 37th Advances in Neural Information Processing Systems (NeurIPS) 36 (2024).   \n[17] Boyi Li et al. \u201cInteractive task planning with language models\u201d. In: arXiv preprint arXiv:2310.10645 (2023).   \n[18] Zhenyu Wu et al. \u201cEmbodied task planning with large language models\u201d. In: arXiv preprint arXiv:2307.01848 (2023).   \n[19] Jingkang Yang et al. \u201cOctopus: Embodied vision-language programmer from environmental feedback\u201d. In: arXiv preprint arXiv:2310.08588 (2023).   \n[20] Weijia Shi et al. \u201cReplug: Retrieval-augmented black-box language models\u201d. In: arXiv preprint arXiv:2301.12652 (2023).   \n[21] Michael G\u00fcnther et al. \u201cJina embeddings 2: 8192-token general-purpose text embeddings for long documents\u201d. In: arXiv preprint arXiv:2310.19923 (2023).   \n[22] John X Morris et al. \u201cText embeddings reveal (almost) as much as text\u201d. In: arXiv preprint arXiv:2310.06816 (2023).   \n[23] Jingcheng Deng et al. \u201cRegavae: A retrieval-augmented gaussian mixture variational autoencoder for language modeling\u201d. In: arXiv preprint arXiv:2310.10567 (2023).   \n[24] Wenhao Yu et al. \u201cChain-of-note: Enhancing robustness in retrieval-augmented language models\u201d. In: arXiv preprint arXiv:2311.09210 (2023).   \n[25] Peng Xu et al. \u201cRetrieval meets long context large language models\u201d. In: arXiv preprint arXiv:2310.03025 (2023).   \n[26] Zhengbao Jiang et al. \u201cActive retrieval augmented generation\u201d. In: arXiv preprint arXiv:2305.06983 (2023).   \n[27] Akari Asai et al. \u201cSelf-rag: Learning to retrieve, generate, and critique through self-reflection\u201d. In: arXiv preprint arXiv:2310.11511 (2023).   \n[28] \u00d6zg\u00fcr S\u00b8ims\u00b8ek and Andrew G Barto. \u201cAn intrinsic reward mechanism for efficient exploration\u201d. In: Proceedings of the 23rd International Conference on Machine Learning (ICML). 2006, pp. 833\u2013840.   \n[29] Yuri Burda et al. \u201cExploration by random network distillation\u201d. In: arXiv preprint arXiv:1810.12894 (2018).   \n[30] Taewook Nam et al. \u201cSkill-based meta-reinforcement learning\u201d. In: arXiv preprint arXiv:2204.11828 (2022).   \n[31] Karl Pertsch, Youngwoon Lee, and Joseph Lim. \u201cAccelerating reinforcement learning with learned skill priors\u201d. In: Proceedings of the 4th Conference on Robot Learning (CoRL). PMLR. 2021, pp. 188\u2013204.   \n[32] Karl Pertsch et al. \u201cGuided reinforcement learning with learned skills\u201d. In: arXiv preprint arXiv:2107.10253 (2021).   \n[33] Evan Z Liu et al. \u201cDecoupling exploration and exploitation for meta-reinforcement learning without sacrifices\u201d. In: Proceedings of the 38th International Conference on Machine Learning (ICML). PMLR. 2021, pp. 6925\u20136935.   \n[34] Ling Liu, Calton Pu, and Wei Tang. \u201cContinual queries for internet scale event-driven information delivery\u201d. In: IEEE Transactions on Knowledge and Data Engineering 11.4 (1999), pp. 610\u2013628.   \n[35] Yousuke Watanabe and Hiroyuki Kitagawa. \u201cQuery result caching for multiple event-driven continuous queries\u201d. In: Information systems 35.1 (2010), pp. 94\u2013110.   \n[36] Gemma Team et al. \u201cGemma: Open models based on gemini research and technology\u201d. In: arXiv preprint arXiv:2403.08295 (2024).   \n[37] AI@Meta. \u201cLlama 3 Model Card\u201d. In: (2024). URL: https://github.com/meta-llama/ llama3/blob/main/MODEL_CARD.md. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Broader impact ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Our work does not involve activities associated with negative societal impacts, such as disseminating disinformation, creating fake profiles, or conducting surveillance. Therefore, we do not expect any negative societal impacts from our research. ", "page_idx": 12}, {"type": "text", "text": "B Environment settings ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "B.1 VirtualHome ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We employ VirtualHome [8], a complex simulation environment designed for embodied AI research, which offers a wide range of interactive household activities. This environment requires an agent to perform tasks by interacting with various objects and following high-level action commands. VirtualHome features 162 different object types (e.g., TV, sofa) and multiple room types (e.g., kitchen, living room) across various indoor scenes, providing a complex environment through diverse combinations of rooms. Additionally, to standardize the time duration of skill execution, we have imposed the following restrictions on the \u2018walk\u2019 skill: walking is only possible to adjacent rooms, and only permitted towards objects that are present in that same room. ", "page_idx": 12}, {"type": "image", "img_path": "LpXV29Ggl3/tmp/53fc6a67e99ad9ffb446d752e2d8e3e15eb6a5966bdb99d16a89ea832b15b92e.jpg", "img_caption": ["Figure A.1: Visualization of VirtualHome. "], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "The embodied agent collects information about objects that come into its view and uses this as observations. Additionally, the agent utilizes seven skills to respond to the given continual instructions: walk object or walk room, grab object, switch object, put object, putin object, open object, and close object. For constructing TEKG, we use the graph based environment implemented in VirtualHome. ", "page_idx": 12}, {"type": "text", "text": "For non-stationarity, the environment condition involves a single continual instruction from the set changing at every predefined timestep: 4 for high non-stationarity, 6 for medium non-stationarity, and 8 for low non-stationarity. For continual instruction following tasks, we implement 19 continual instructions. Table A.1 shows the details of instructions. ", "page_idx": 12}, {"type": "text", "text": "B.2 ALFRED ", "text_level": 1, "page_idx": 12}, {"type": "image", "img_path": "LpXV29Ggl3/tmp/2e5a97fe30132c88f78675df716ab57d9314c1b85615201a5f10864a1090efe1.jpg", "img_caption": ["Figure A.2: Visualization of ALFRED and CARLA. "], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "We utilize ALFRED [9], which provides vision-and-language navigation and rearrangement tasks for embodied AI. This environment requires an agent to follow language formatted instructions to accomplish real-world-like household tasks. ALFRED features 58 different object types (e.g., bread) and 26 receptacle types (e.g., plate) across 120 various indoor scenes (e.g., kitchen). ", "page_idx": 12}, {"type": "table", "img_path": "LpXV29Ggl3/tmp/ca3de58af2d7f3da44f4dd9464f567a28bebfaa71ddb6bff60d7ecda39cc5023.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "The embodied agent gathers information about objects that enter its view and uses this as observations. Moreover, the agent utilizes six skills to respond to the given continual instructions: goto object, grab object, toggle object, put object, open object, and close object. ", "page_idx": 13}, {"type": "text", "text": "For non-stationarity, the environment condition involves a single continual instruction from the set changing at every predefined timestep: 4 for high non-stationarity, 6 for medium non-stationarity, and 8 for low non-stationarity. For continual instruction following tasks, we implement 16 continual instructions. Table A.2 shows the details of instructions. ", "page_idx": 13}, {"type": "table", "img_path": "LpXV29Ggl3/tmp/e843455ef422cb0a1c44a6c47494c350283e9c8757d59fa963e17db0dcea6309.jpg", "table_caption": ["Table A.2: Continual instructions in ALFRED environment "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "B.3 CARLA ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "CARLA is an opensource simulator for autonomous driving tasks that provides various environment settings for driving conditions and maps. For our experiment, we have modified CARLA to function as a skill-based environment. The agent can navigate within the environment using three skills: turn right, turn left, and go straight. Additionally, the agent is capable of loading and offloading goods. The objective of the agent is to transport goods from a designated building to a target building whenever the conditions specified in the given instructions are met. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "For non-stationarity, the environment condition involves a single continual instruction from the set changing at every predefined timestep: 4 for high non-stationarity, 6 for medium non-stationarity, and 8 for low non-stationarity. For continual instruction following tasks, we implement 16 continual instructions such as \"If the green building calls, load the goods from the green building to the red building\". We utilize a set of instructions as continual instructions. ", "page_idx": 14}, {"type": "text", "text": "C Implementation details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we provide the implementation details of our proposed framework ExRAP and each baseline. Our framework is implemented using Python $\\mathrm{v}3.10$ and trained on a system of an Intel(R) Core (TM) i9-10980XE processor and two NVIDIA RTX A6000 GPUs. Each experiment run takes 4 hours. We employ 4 different methods: ZSP [11], SayCan [1], ProgPrompt [3], and LLM-Planner [12]. Although the format differs, the observed environmental knowledge used remains consistent across all baselines. This includes observations of objects, object positions, object states, room adjacencies, etc. For generating a plan, we utilize the Llama-3 [37] model. ", "page_idx": 14}, {"type": "text", "text": "C.1 Baseline ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "ZSP [11] capitalizes on the abilities of LLMs for embodied task planning by translating instructions into skill sequences, thereby enhancing the performance of embodied tasks. ZSP achieves this through the generation of detailed step-by-step prompts derived from examples of similar successful tasks, and then it utilizes the LLM to generate executable plans based on these examples. For implementation, we refer to the opensource 2. ", "page_idx": 14}, {"type": "text", "text": "SayCan [1] integrates the affordance function with language models, generating plans that are feasible to the context. SayCan achieves this by learning the environment affordance function derived from the LLM with the agent\u2019s experiences. Similarly, we calculate optimal affordance scores using environmental information and domain-specific knowledge. For observation, we utilize the linearized retrieved knowledge graph as the prompt for SayCan as same as ExRAP. For implementation, we refer to the opensource 3. ", "page_idx": 14}, {"type": "text", "text": "ProgPrompt [3] utilizes the code-style policy for generating plans for the embodied environment. As it passes the available primitive actions such as walk object, grab object, etc., in the form of import statements in Python, an available object list, and example tasks to LLM, the LLM produces some plans to succeed in the task. ProgPrompt can also execute conditional action by assert-else statements in the codes which is the output of the LLM. For experiments, we provide the code examples for various tasks in VirtualHome and ALFRED. For implementation, we refer to the opensource 4. ", "page_idx": 14}, {"type": "text", "text": "LLM-Planner [12] leverages the LLMs using the demonstrations with retrieved in-context examples, empowering embodied agents to perform complex tasks in environments with observed information, guided by natural language instructions. For our experiments, the LLM-Planner performs inference at each timestep, gathering observed environmental knowledge through natural language. When a skill execution fails, it captures the error and infers an alternative action based on the error at the next timestep. For observation, we utilize the detection results from the environment and incorporate them as prompts for the LLM-Planner. For implementation, we refer to the opensource 5. ", "page_idx": 14}, {"type": "text", "text": "The hyperparameter settings for baselines are summarized in Table A.3. ", "page_idx": 14}, {"type": "table", "img_path": "LpXV29Ggl3/tmp/931125e52b75ae59eea2302c3a71426d71ca7b332fcb5dae22f7b4e91b6bb209.jpg", "table_caption": ["Table A.3: Hyperparameter settings for baselines "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "C.2 ExRAP ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In ExRAP, we address multiple continual instruction-following tasks by decomposing each instruction into queries and task executions based on environmental context memory. To manage these tasks effectively, which are executed continuously and simultaneously, we introduce an explorationintegrated task planning scheme. This scheme incorporates information-based exploration into the LLM-based planning process, enhancing the balance between maintaining the validity of the environmental context memory and the demands of environment exploration, ultimately boosting overall task performance. Additionally, we implement a temporal consistency refinement scheme in our query evaluation to counteract the inherent decay of knowledge within the memory. ", "page_idx": 15}, {"type": "text", "text": "For the update function $\\mu$ of TEKG in (3), we design an algorithmic refinement function tailored for embodied environments. This refinement function operates under two simple rules. First, only the most recent timestep data is retained for quadruple related agents (i.e., where the source entity or target entity is an agent). This is because information related to agents in the environment is observable. Second, semantically contradictory information is categorized into two cases: one where object states are opposite, such as simultaneous quadruples indicating that a TV is both off and on, and another where an object exists in two places at TEKG. Both scenarios result in the removal of the outdated quadruple. ", "page_idx": 15}, {"type": "text", "text": "The hyperparameter settings for the baselines are summarized in Table A.4. ", "page_idx": 15}, {"type": "table", "img_path": "LpXV29Ggl3/tmp/a918465e634a289c4aa98088791b1c791bb5ef60b3d3e2beffc41ca46485a6d7.jpg", "table_caption": ["Table A.4: Hyperparameter settings for ExRAP "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "D Additional experiment ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "D.1 Detailed results of non-stationarity and scale of continual instructions ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Table A.5 presents a performance comparison in terms of SR and PS. The environmental settings consist of two variables: the size of continual instructions and the degree of non-stationarity. As the size of continual instructions increases, the number of instructions that need to be addressed also grows, requiring the agent to collect more knowledge and perform more tasks. Furthermore, a higher ", "page_idx": 15}, {"type": "text", "text": "Algorithm 1 Detailed implementation of ExRAP framework ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "1: Continual instructions $\\mathcal{T}$   \n2: Env. context memory $G_{t}$ , Timestep $t$   \n3: Queries $\\mathcal{Q}$ , Executions $\\mathcal{E}$   \n4: Instruction interpreter $\\Phi_{I}$ , Memory-augmented query evaluator $\\Phi_{M}$   \n5: Exploitation planner $v_{T}$ , Exploration planner $v_{R}$   \n6: $\\bar{\\mathcal{Q}},\\bar{\\mathcal{E}},C=\\bar{\\Phi_{I}}(\\bar{\\mathcal{L}})$   \n7: loop   \n8: // (a) Query evaluation in (5)   \n9: for $q$ in $\\mathcal{Q}$ do   \n10: $L_{Q}=[\\,]$   \n11: for 1, 2, ..., 10 do   \n12: $\\begin{array}{r l}&{\\dot{G}_{1:t-1}\\sim\\Phi_{R}\\bigl(G_{1:t-1},\\{q\\}\\bigr)}\\\\ &{R(q|G_{t-1})=\\Phi_{\\mathrm{LLM}}\\left(q,t,\\hat{G}_{1:t-1},P(q|G_{t-1})\\right)}\\end{array}$   \n13:   \n14: if hold (6) then   \n15: $\\boldsymbol{L_{Q}}$ .append $(P(q|G_{t-1}))$ )   \n16: end if   \n17: end for   \n18: $\\begin{array}{r l}&{R(q|G_{t-1})=\\mathbb{E}[L_{Q}]}\\\\ &{P(q|G_{t})=\\Phi_{M}\\bigl(q,t,G_{1:t-1},R(q|G_{t-1})\\bigr)}\\end{array}$   \n19:   \n20: end for   \n21: // (b) Exploration-integrated task planning in (12)   \n22: $\\begin{array}{r}{z_{t}=\\operatorname*{argmax}_{z\\in Z}[w_{T}\\cdot v_{T}(G_{t},z)+w_{R}\\cdot v_{R}(G_{t},z)]}\\end{array}$   \n23: observation $o_{t+1}=\\mathrm{EnvStep}(z_{t})$   \n24: $G_{t+1}=\\mu(G_{t},o_{t+1})$ , $t\\gets t+1$   \n25: end loop ", "page_idx": 16}, {"type": "text", "text": "degree of non-stationarity means the environment changes more rapidly, necessitating that the agent focuses more on environmental information to adapt effectively. ", "page_idx": 16}, {"type": "text", "text": "As indicated in Table A.5, ExRAP demonstrates an increase in SR by $4.73\\%$ to $27.50\\%$ on average compared to the most competitive baseline, the LLM-Planner. Furthermore, ExRAP shows an average reduction in PS by 4.84 to 13.29 compared to the LLM-Planner. Similar to the experiments in the main text, ExRAP exhibits widening performance gaps as the complexity of tasks increases: the SR gap grows from $19.12\\%$ compared to the LLM-Planner with small continual instructions, to $19.93\\%$ with large continual instructions. Similarly, the PS gap expands from an average of 6.79 to 9.97. Moreover, ExRAP demonstrates robustness in embodied environments across varying levels of non-stationarity: the SR gap remains consistent, ranging from $17.77\\%$ compared to the LLM-Planner in low non-stationarity environments to $17.32\\%$ in high non-stationarity. The PS gap expands from an average of 6.96 to 9.27. ", "page_idx": 16}, {"type": "text", "text": "D.2 Detailed results of ablation study ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We compare the performance of ExRAP and ExRAP-TC, which evaluates queries without temporal consistency-based refinement, to understand the impact of this feature. Additionally, ExRAP-LLM operates by directly inputting the instruction \u201cExplore the home\u201d into the LLM, thereby enabling it to function as an exploration planner. This approach contrasts with ExRAP-EXP, which evaluates skills using an exploitation-only planner. Here, ExRAP-EXP inputs \u201cFind {query}\u201d for each query that is not yet satisfied, focusing solely on achieving specific task objectives without incorporating exploration. Table A.6 presents the detailed results of the ablation study for Tables 4 and 5 in Section 4.2 of the main text. ", "page_idx": 16}, {"type": "table", "img_path": "LpXV29Ggl3/tmp/3beba15ee084e2c7fc8e3566f63c1f34bbc6b610a79d205a2a48b4ea0435ffa2.jpg", "table_caption": ["Table A.5: Performance comparison in VirtualHome "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "LpXV29Ggl3/tmp/f56fafb45056dfbb99dc390135335d99a04969d90d97a58430cbf12239e035d3.jpg", "table_caption": ["Table A.6: Detailed performance for ablation study "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "E Anaylsis ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "E.1 Analysis of refinement temporal consistency ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In Table A.7, we show examples of refined query responses. Although the input for subsequent query responses remains the same, the current timestep varies. Naturally, as timestep progresses, information decay should occur, and the entropy of the query response is expected to increase. However, the examples show an unexpected reduction in entropy from 0.24 to 0.15, leading to the removal of these outdated responses. This approach allows us to effectively model information decay, thereby improving the quality of query responses. ", "page_idx": 17}, {"type": "text", "text": "E.2 Analysis of computation overhead of ExRAP ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "ExRAP employs sentence embedding techniques, such as DPR and BM25, for retrieving knowledge graphs and demonstrations to enhance exploitation value. As shown in the table below, the average retrieval and LLM inference time is approximately 14 times faster compared to inferring actions through an LLM without retrieval, using an RTX A6000 GPU and an i9-10980XE processor. By selecting relevant quadruples and demonstrations based on the query and instructions, ExRAP effectively reduces the context length, maintaining efficient LLM inference times. In our experiments, we retrieve only 3 demonstrations and 12 quadruples to generate prompts for each continual instruction, regardless of the size of the knowledge graph or dataset. ", "page_idx": 17}, {"type": "table", "img_path": "LpXV29Ggl3/tmp/85f9d036658a7a2426ec757e8cdea712770f168e0be0c309be1418d149a6849b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "LpXV29Ggl3/tmp/71b9665b2292ebe2ac86e03fef259ecae20b980ff6e3fe6ba1f604afc5d2c55f.jpg", "img_caption": ["Figure A.3: Examples for exploration value w.r.t non-stationarity; we noted that the increase in exploration is more larger in environments with higher non-stationarity, leading to enhanced exploration. This, in turn, resulted in a more frequent drop in exploration value. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "table", "img_path": "LpXV29Ggl3/tmp/ff5bbabc3c0a2e72c615a1dccfb2b669d0dba335720ae3c018672573647b089d.jpg", "table_caption": ["Table A.8: Comparison of Retrieval and LLM Inference Times "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "E.3 Analysis of exploration dynamics ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Figures A.3 illustrate the changes in exploration value for a single query during the execution of continual instructions. We observed that the exploration value increased steadily over time and decreased rapidly once information relevant to the query was collected. Specifically, we noted that the increase in exploration was more larger in environments with higher non-stationarity, leading to enhanced exploration. This, in turn, resulted in a more frequent drop in exploration value. As the reviewer suggested, investigating the exploration dynamics is an interesting analysis that demonstrates how ExRAP improves performance. ", "page_idx": 18}, {"type": "text", "text": "E.4 Analysis of behavior of ExRAP ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To analyze the behavior patterns of ExRAP, we map its generated plans to heuristic strategies such as Greedy, and Multiple Instruction First, and Max Staleness First. In our work, these heuristics are utilized only for analysis purposes, showing how ExRAP is able to achieve superior performance and how its exploration-integrated planning policy behaves differently in specific situations. ", "page_idx": 18}, {"type": "table", "img_path": "LpXV29Ggl3/tmp/e8c68a303b413a0e4d445b2fd3fcf071b875ae9451697c2f2bb22784a7c248a5.jpg", "table_caption": ["Table A.9: Examples for ExRAP behavior as Greedy heuristic "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Greedy. When the entropy values of the queries are generally low and there are few executions required to complete, ExRAP operates in a manner similar to a greedy heuristic with respect to a single instruction. For example, in Table A.9, when there exist few executions required to complete and the entropy of the query responses is generally low, ExRAP operates similarly to a Greedy heuristic, executing instruction 1 independently. However, consistently applying this heuristic in all scenarios results in executing an instruction-wise plan, similar to the baselines. ", "page_idx": 19}, {"type": "table", "img_path": "LpXV29Ggl3/tmp/98b779ffd5d62831be33f086aa433021bdb41af58eb9f8c86625628470b14e2b.jpg", "table_caption": ["Table A.10: Examples for ExRAP behavior as Multiple Instruction First heuristic "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Multiple Instruction First. The Multiple Instructions First heuristic selects the skill based on the number of related instructions rather than focusing on a specific instruction. If the entropy values of various queries are generally high and there are many executions required to complete, ExRAP prioritizes skills where multiple instructions are concentrated. For example, in Table A.10, when the overall entropy of query responses is generally high, suggesting a need for efficient planning to explore multiple queries, ExRAP operates similarly to a Multiple Instructions First heuristic. However, consistently applying this heuristic in all cases can lead to an integrated plan, but result in failing to properly conclude tasks. ", "page_idx": 19}, {"type": "text", "text": "Max Staleness First. The Max Staleness First heuristic prioritizes selecting skills based on the age of information of related queries. Similarly, if the entropy value of a particular query is very high, ExRAP prioritizes exploring that specific query. This typically aligns with a Max Staleness First policy because our query evaluator operates with temporal consistency. For example, in Table A.11, when the entropy of query responses for instruction 2 is particularly high, indicating a need to explore such queries, ExRAP operates similarly to a Max Staleness First heuristic. While this heuristic focuses on specific queries and may reduce efficiency, it prevents the occurrence of instructions being left unexecuted, thereby avoiding starvation of certain instructions. This strategy essentially requires precise measurement of information decay to be effectively implemented. In ExRAP, we perform temporal consistency-based refinement to enhance the quality of query responses, leading to improved performance. ", "page_idx": 19}, {"type": "table", "img_path": "LpXV29Ggl3/tmp/085aeb18143b92ae8f1e3dd61c88f23e63f371362424ae8f8ec0b6501bb05fdb.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "As discussed so far, by appropriately utilizing different strategies including those similar to the three heuristics depending on given situations, our ExRAP dynamically adjusts its policy in the response to changes in the environment for multiple tasks of continual instruction following. ", "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The abstract and introduction should clearly state the claims made, including the contributions made in the paper. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: We write a separate limitation paragraph in Conclusion section. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when the image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in the appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We provide implementation details in Appendix, and also release source code. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open Access to Data and Code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We release code for the experiments. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We provide implementation details, including environment and hyperparameters, in the Appendix. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in the appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: All the results are accompanied by the confidence interval. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper indicate the type of compute workers CPU or GPU, and provide the amount of compute required for each of the individual experimental runs. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We read NeurIPS Code of Ethics. The paper preserve anonymity. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact on the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for the responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make the best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for Existing Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: All the baselines code, we write footnote for url. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 25}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}]