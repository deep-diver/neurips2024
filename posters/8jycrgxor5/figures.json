[{"figure_path": "8jyCRGXOr5/figures/figures_5_1.jpg", "caption": "Figure 1: Diagram to illustrate our proposed sketching algorithms.", "description": "This figure illustrates the three novel sketching algorithms proposed in the paper: AFFD, AFJL, and QK.  It shows the steps involved in each algorithm, highlighting the use of Hadamard transforms, Gaussian vectors, and Kronecker products.  The diagram visually depicts the differences between the algorithms and how they achieve dimensionality reduction. AFFD uses two Hadamard transforms, AFJL uses one, and QK uses a Kronecker product decomposition of the input.", "section": "Design Principles for Efficient Sketching Algorithms"}, {"figure_path": "8jyCRGXOr5/figures/figures_9_1.jpg", "caption": "Figure 2: left: ratio (RNEG) of the absolute value of the top negative to the top positive eigenvalue; right: ratio R of the n-th largest positive eigenvalue to the largest positive eigenvalue. We define outliers when R > 20%, motivated by [12, Fig.2]. Higher-resolution versions for printing can be found in Appendix A. These results disprove conjectures on the Hessian structure, see Sec. 5.5.", "description": "This figure shows two plots. The left plot displays the ratio of the absolute value of the top negative eigenvalue to the top positive eigenvalue (RNEG) over training steps. The right plot shows the ratio of the nth largest positive eigenvalue to the largest positive eigenvalue (R) against n (the rank).  The plots illustrate that previously held assumptions about Hessian structure in smaller networks do not hold for large language models, specifically contradicting observations from prior research such as [12, Fig 2].", "section": "5.5 Hessian Analysis During Pre-trained Language Model Fine-Tuning"}, {"figure_path": "8jyCRGXOr5/figures/figures_14_1.jpg", "caption": "Figure 3: Peak memory usage comparing FJL with AFJL. Results on GPU (V100); for FJL results with D > 220 are not reported as there were Out-of-Memory errors.", "description": "The figure shows a comparison of peak memory usage between FJL and AFJL algorithms on a GPU (V100) for different values of the target dimension D (log2 scale). It highlights the scalability issues with FJL, which experiences significant memory growth as D increases, resulting in out-of-memory errors beyond D=2^20. In contrast, AFJL demonstrates better scalability with relatively stable memory usage across the range of D values.", "section": "A.3 A closer look at FJL vs AFJL"}, {"figure_path": "8jyCRGXOr5/figures/figures_14_2.jpg", "caption": "Figure 4: Wall time comparing FJL with AFJL. Results on GPU (V100); for FJL results with D > 220 are not reported as there were Out-of-Memory errors.", "description": "This figure compares the wall time of the FJL and AFJL algorithms for sketching gradients on a V100 GPU.  The plot shows that AFJL is significantly faster than FJL, especially as the target dimension (D) increases.  For larger values of D (above 220), FJL runs out of memory, highlighting the improved scalability of AFJL.", "section": "A Appendix: Additional Experimental Results"}, {"figure_path": "8jyCRGXOr5/figures/figures_16_1.jpg", "caption": "Figure 1: Diagram to illustrate our proposed sketching algorithms.", "description": "This figure shows a comparison of three different sketching algorithms: AFFD, AFJL, and QK.  It illustrates the steps involved in each algorithm, highlighting the use of Hadamard transforms, Gaussian matrices, and Kronecker products to efficiently sketch gradients. The figure also shows how the algorithms can be adapted for different types of sketches (explicit vs. implicit) and for different types of hardware (GPUs vs. TPUs).", "section": "Design Principles for Efficient Sketching Algorithms"}, {"figure_path": "8jyCRGXOr5/figures/figures_18_1.jpg", "caption": "Figure 1: Diagram to illustrate our proposed sketching algorithms.", "description": "This figure provides a visual representation of the three novel sketching algorithms proposed in the paper: AFFD, AFJL, and QK.  It shows how each algorithm processes the input gradient vector through a series of transformations involving Hadamard transforms and Gaussian matrices.  The diagram highlights the key differences in the preconditioning steps and the explicit versus implicit sketching approaches, helping readers to better understand the algorithmic differences and their impact on efficiency.", "section": "Design Principles for Efficient Sketching Algorithms"}, {"figure_path": "8jyCRGXOr5/figures/figures_21_1.jpg", "caption": "Figure 2: left: ratio (RNEG) of the absolute value of the top negative to the top positive eigenvalue; right: ratio R of the n-th largest positive eigenvalue to the largest positive eigenvalue. We define outliers when R > 20%, motivated by [12, Fig.2]. Higher-resolution versions for printing can be found in Appendix A. These results disprove conjectures on the Hessian structure, see Sec. 5.5.", "description": "This figure shows two plots illustrating the Hessian's evolution during pre-trained language model fine-tuning. The left plot displays the ratio of the absolute value of the top negative eigenvalue to the top positive eigenvalue (RNEG).  The right plot shows the ratio of the nth largest positive eigenvalue to the largest positive eigenvalue (R).  These ratios are calculated at various steps during the training process.  An outlier is defined when the ratio R exceeds 20%. The figure provides insights into the Hessian structure, challenging earlier assumptions made in prior research.", "section": "Hessian Analysis During Pre-trained Language Model Fine-Tuning"}, {"figure_path": "8jyCRGXOr5/figures/figures_21_2.jpg", "caption": "Figure 2: left: ratio (RNEG) of the absolute value of the top negative to the top positive eigenvalue; right: ratio R of the n-th largest positive eigenvalue to the largest positive eigenvalue. We define outliers when R > 20%, motivated by [12, Fig.2]. Higher-resolution versions for printing can be found in Appendix A. These results disprove conjectures on the Hessian structure, see Sec. 5.5.", "description": "This figure displays two plots showing the ratio of eigenvalues of the Hessian matrix during the fine-tuning of pre-trained language models. The left plot shows the ratio of the absolute value of the top negative eigenvalue to the top positive eigenvalue (RNEG), while the right plot shows the ratio of the nth largest positive eigenvalue to the largest positive eigenvalue (R).  The plots illustrate that previous conjectures regarding Hessian structure in smaller networks do not hold true for large language models.", "section": "Hessian Analysis During Pre-trained Language Model Fine-Tuning"}, {"figure_path": "8jyCRGXOr5/figures/figures_22_1.jpg", "caption": "Figure 7: Our methods exhibit constant wall time with respect to the target dimension D. In contrast, TRAK's runtime increases with the target dimension. Efficient implementation of dense random projections with recomputed projectors is non-trivial; compare the performance difference between TRAK[CUDA] and TRAK[Triton]. TRAK[CUDA] utilizes the CUDA kernel provided by the original TRAK authors [19].", "description": "This figure compares the wall time performance of different gradient sketching algorithms.  It shows that the proposed methods (AFFD and QK) exhibit constant wall time regardless of the target dimension (D), while the competing method (TRAK) shows a significant increase in runtime as D increases. The performance difference between two different implementations of the TRAK algorithm (CUDA and Triton) highlights the difficulty in efficiently implementing dense random projections.", "section": "A.8 Comparison to on-the-fly dense random projections"}, {"figure_path": "8jyCRGXOr5/figures/figures_27_1.jpg", "caption": "Figure 1: Diagram to illustrate our proposed sketching algorithms.", "description": "This figure shows a comparison of three different sketching algorithms: AFFD, AFJL, and QK.  It illustrates the steps involved in each algorithm, highlighting the differences in their approaches to sketching gradients or Hessian vector products.  The diagram details how each algorithm uses Hadamard transforms, Gaussian matrices, and Kronecker products to perform the sketching operation.  The visualization helps readers understand the computational cost and memory requirements of each method, emphasizing their unique design choices and their relative efficiency. ", "section": "Design Principles for Efficient Sketching Algorithms"}]