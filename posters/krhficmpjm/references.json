{"references": [{"fullname_first_author": "P. Abbeel", "paper_title": "Apprenticeship learning via inverse reinforcement learning", "publication_date": "2004-01-01", "reason": "This paper introduces a foundational method for integrating human expertise into reinforcement learning through inverse reinforcement learning, which is a key concept for the GUIDE framework."}, {"fullname_first_author": "R. Akrour", "paper_title": "Preference-based policy learning", "publication_date": "2011-01-01", "reason": "This paper presents preference-based reinforcement learning, another crucial method for integrating human feedback in RL, which is closely related to the approach taken by the GUIDE framework."}, {"fullname_first_author": "D. Arumugam", "paper_title": "Deep reinforcement learning from policy-dependent human feedback", "publication_date": "2019-01-01", "reason": "This study presents a deep reinforcement learning framework using human feedback, which directly inspired the design and methodology of GUIDE for real-time human-guided reinforcement learning."}, {"fullname_first_author": "W. B. Knox", "paper_title": "Interactively shaping agents via human reinforcement: The tamer framework", "publication_date": "2009-01-01", "reason": "The TAMER framework, introduced in this paper, is a direct precursor to GUIDE, providing a foundational approach to real-time human-guided reinforcement learning that heavily influenced the GUIDE design."}, {"fullname_first_author": "T. Haarnoja", "paper_title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor", "publication_date": "2018-01-01", "reason": "Soft Actor-Critic (SAC), introduced in this paper, is a state-of-the-art reinforcement learning algorithm used as a baseline in the GUIDE experiments, highlighting its importance in the field and its relevance to the GUIDE work."}]}