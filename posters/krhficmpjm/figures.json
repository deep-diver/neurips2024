[{"figure_path": "KrHFICMPjm/figures/figures_1_1.jpg", "caption": "Fig. 1: We propose GUIDE as a novel framework for real-time human-shaped agents enabling continuous feedback and continual improvements without human trainers. We also aim to understand how individual differences affect their guided agents' performances.", "description": "This figure illustrates the GUIDE framework, which is a novel approach for creating real-time human-shaped agents.  The key features highlighted are the use of continuous human feedback to accelerate learning and a simulated feedback module that reduces reliance on human input while enabling continual training. The framework aims to improve agent performance and investigate how individual differences in human feedback affect the learning process.  The image shows a diagram of the system, indicating the flow of information between the human, simulated human, agent, and environment.", "section": "1 Introduction"}, {"figure_path": "KrHFICMPjm/figures/figures_3_1.jpg", "caption": "Fig. 2: GUIDE: The training consists of two stages: During the Human guidance stage, the human trainer observes the state and action taken by the agent and provides real-time continuous feedback. The feedback values are grounded into per-step dense rewards and combined with the environment reward. Concurrently, we train a human feedback simulator that takes in state-action pairs and regresses the feedback values. During the Automated guidance stage, the trained simulator stands in for the human and provides feedback to continue to improve the policy, effectively reducing human efforts and cognitive loads.", "description": "This figure illustrates the GUIDE framework, which consists of two stages: human guidance and automated guidance. In the human guidance stage, a human trainer provides continuous feedback to the agent based on its state and actions. This feedback is integrated with environmental rewards to train the agent's policy. Simultaneously, a simulated human feedback module is trained to learn and replicate human feedback patterns.  In the automated guidance stage, the trained simulator takes over from the human, providing feedback to enable continued policy improvement. This reduces the need for human input and allows for continuous training.", "section": "4 GUIDE: Grounding Real-Time Human Feedback"}, {"figure_path": "KrHFICMPjm/figures/figures_4_1.jpg", "caption": "Fig. 3: (A) Conventional discrete feedback. (B) Our continuous feedback. The histograms indicate the feedback distribution provided by the same subject on easy access and the same task. Continuous feedback carries more information from the human trainer.", "description": "This figure compares two feedback mechanisms: conventional discrete feedback and the proposed continuous feedback.  Panel A shows a histogram representing the discrete feedback distribution, highlighting its limited information content. Panel B illustrates the distribution of continuous feedback using a histogram and a curve, revealing more nuanced human assessments. The caption emphasizes the increased information conveyed by the new continuous feedback method.", "section": "4.3 Learning from Joint Human and Sparse Environment Feedback"}, {"figure_path": "KrHFICMPjm/figures/figures_5_1.jpg", "caption": "Fig. 3: (A) Conventional discrete feedback. (B) Our continuous feedback. The histograms indicate the feedback distribution provided by the same subject on easy access and the same task. Continuous feedback carries more information from the human trainer.", "description": "This figure shows a comparison of conventional discrete feedback and the novel continuous feedback method used in the GUIDE framework.  Panel A displays a histogram showing a discrete distribution of human feedback values, likely representing a scale such as {positive, neutral, negative}. Panel B shows a histogram with a continuous distribution, demonstrating that the novel method allows for a more nuanced and detailed feedback signal from human trainers, enabling a richer representation of human assessment of the agent's behavior.", "section": "4.3 Learning from Joint Human and Sparse Environment Feedback"}, {"figure_path": "KrHFICMPjm/figures/figures_5_2.jpg", "caption": "Fig. 3: (A) Conventional discrete feedback. (B) Our continuous feedback. The histograms indicate the feedback distribution provided by the same subject on easy access and the same task. Continuous feedback carries more information from the human trainer.", "description": "This figure compares two feedback mechanisms: conventional discrete feedback and the proposed continuous feedback method.  Panel (A) shows a histogram of discrete feedback values (+1, 0, -1) given by a human participant. Panel (B) shows a histogram of continuous feedback values from the same participant for the same task.  The histograms illustrate that continuous feedback provides richer and more nuanced information about the agent's performance than discrete feedback.", "section": "4.3 Learning from Joint Human and Sparse Environment Feedback"}, {"figure_path": "KrHFICMPjm/figures/figures_6_1.jpg", "caption": "Fig. 4: Cognitive Tests: We conducted a series of cognitive tests to quantify how individual differences among subjects affect their guided agents' performances. (A) Eye Alignment (B) Reflex (C) Theory of Behavior (D) Mental Rotation (E) Mental Fitting (F) Spatial Mapping", "description": "This figure shows six different cognitive tests used in the study. Each test measures a specific cognitive skill, such as eye-hand coordination, reaction time, predictive reasoning, spatial reasoning, and spatial memory. The results of these tests were used to analyze how individual differences in cognitive abilities affected the performance of the human-guided AI agents.", "section": "5.3 Human Cognitive Tests for Individual Difference Characterization"}, {"figure_path": "KrHFICMPjm/figures/figures_7_1.jpg", "caption": "Fig. 5: GUIDE performance compared with other baselines. In challenging tasks, GUIDE consistently outperforms all other baselines. Subjects with higher cognitive test scores also result in higher performance in the learned agents as shown in the top row (Top 15).", "description": "This figure presents the results of experiments comparing the performance of GUIDE with other baselines (c-Deep TAMER, DDPG, SAC, and Heuristic) across three different tasks: Bowling, Find Treasure, and Hide-and-Seek. The results are shown separately for the top 15 subjects and all 50 subjects.  The x-axis represents training time (in minutes), and the y-axis represents either the score (for Bowling) or success rate (for Find Treasure and Hide-and-Seek).  The shaded areas represent the standard deviation across multiple runs or subjects. The figure demonstrates that GUIDE consistently outperforms other baselines in more complex tasks (Find Treasure and Hide-and-Seek). It also shows a correlation between the subjects' cognitive test scores and their corresponding AI agent's performance.", "section": "5.5 Experiment Results and Analysis"}, {"figure_path": "KrHFICMPjm/figures/figures_8_1.jpg", "caption": "Fig. 6: Exploration behavior of GUIDE and DDPG agents. For each of the plots, the x-axis is the step number through the course of an episode. The y-axis is the ratio between the area of the visible view and the entire input frame. We observe a stronger tendency of exploration exhibited by the human-guided agent compared to the baseline RL agent.", "description": "This figure compares the exploration behavior of agents trained with GUIDE and DDPG methods across different training times (2, 4, 6, 8, and 10 minutes). The x-axis represents the step number within an episode, and the y-axis shows the ratio of the visible area to the total frame area. The shaded regions represent the variability in exploration behavior across different runs. The figure demonstrates that GUIDE agents exhibit a higher tendency for exploration than DDPG agents, suggesting the effectiveness of human guidance in encouraging thorough exploration of the environment.", "section": "4.4 Continue Training by Learning to Mimic Human Feedback"}, {"figure_path": "KrHFICMPjm/figures/figures_8_2.jpg", "caption": "Fig. 7: Qualitative visualization of the learned human feedback model. Our learned feedback model is able to generalize to unseen trajectories and provide effective feedback in place of humans.", "description": "This figure visualizes the performance of the learned human feedback model on unseen trajectories. It compares the environment reward, human feedback, and the learned heuristic feedback for two unseen trajectories. The results show that the learned model can generalize well and provide feedback similar to human feedback, demonstrating its effectiveness in replacing human input.", "section": "4.4 Continue Training by Learning to Mimic Human Feedback"}, {"figure_path": "KrHFICMPjm/figures/figures_8_3.jpg", "caption": "Fig. 8: Correlation between cognitive test scores (normalized) and GUIDE training performance. The darker the color, the more statistically significant the correlation. \"+\": positive, \"-\": negative.", "description": "This figure shows the correlation between the results of six cognitive tests (Eye Alignment, Theory of Behavior, Mental Rotation, Mental Fitting, Reflex, and Spatial Mapping) and the performance of the AI agents trained using the GUIDE framework.  The color intensity represents the statistical significance of the correlation, with darker colors indicating stronger correlations. Positive correlations are denoted by a \"+\" symbol and negative correlations by a \"-\" symbol. The results are broken down by task (Bowling, Find Treasure, 1v1 Hide & Seek) and an overall performance score.", "section": "5.5 Experiment Results and Analysis"}, {"figure_path": "KrHFICMPjm/figures/figures_9_1.jpg", "caption": "Fig. 5: GUIDE performance compared with other baselines. In challenging tasks, GUIDE consistently outperforms all other baselines. Subjects with higher cognitive test scores also result in higher performance in the learned agents as shown in the top row (Top 15). a shorter window of [0.2, 1] for Find treasure and Hide-and-Seek. For these more difficult navigation tasks, we stacked three consecutive frames as input.", "description": "This figure presents a comparison of GUIDE's performance against other reinforcement learning baselines (c-Deep TAMER, DDPG, SAC) and a heuristic method across three different tasks (Bowling, Find Treasure, 1v1 Hide and Seek). The results are shown separately for the top 15 performing subjects (based on cognitive tests) and all 50 subjects. The figure demonstrates GUIDE's superior performance, especially in complex tasks, and highlights the positive correlation between subject cognitive abilities and agent performance. The use of a shorter feedback window for Find Treasure and Hide-and-Seek is also noted, along with the input stacking technique used for these tasks.", "section": "5.5 Experiment Results and Analysis"}, {"figure_path": "KrHFICMPjm/figures/figures_14_1.jpg", "caption": "Fig. 5: GUIDE performance compared with other baselines. In challenging tasks, GUIDE consistently outperforms all other baselines. Subjects with higher cognitive test scores also result in higher performance in the learned agents as shown in the top row (Top 15).", "description": "This figure displays the performance comparison between GUIDE and several baselines (c-Deep TAMER, DDPG, SAC, and Heuristic) across three tasks: Bowling, Find Treasure, and Hide-and-Seek.  It shows success rates over time (in minutes), differentiating between the top 15 performing subjects (based on cognitive tests) and all 50 subjects.  GUIDE consistently outperforms baselines, particularly in the more complex Find Treasure and Hide-and-Seek tasks.  The top row illustrates a correlation between higher cognitive test scores and better agent performance.", "section": "5.5 Experiment Results and Analysis"}]