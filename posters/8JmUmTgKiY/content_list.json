[{"type": "text", "text": "Kolmogorov\u2013Smirnov GAN ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 We propose a novel deep generative model, the Kolmogorov-Smirnov Generative   \n2 Adversarial Network (KSGAN). Unlike existing approaches, KSGAN formulates   \n3 the learning process as a minimization of the Kolmogorov-Smirnov (KS) distance,   \n4 generalized to handle multivariate distributions. This distance is calculated using   \n5 the quantile function, which acts as the critic in the adversarial training process.   \n6 We formally demonstrate that minimizing the KS distance leads to the trained   \n7 approximate distribution aligning with the target distribution. We propose an   \n8 efficient implementation and evaluate its effectiveness through experiments. The   \n9 results show that KSGAN performs on par with existing adversarial methods,   \n0 exhibiting stability during training, resistance to mode dropping and collapse, and   \n11 tolerance to variations in hyperparameter settings. Additionally, we review the   \n2 literature on the Generalized KS test and discuss the connections between KSGAN   \n3 and existing adversarial generative models. ", "page_idx": 0}, {"type": "image", "img_path": "8JmUmTgKiY/tmp/99cc755d092ac54c0aed8cb4bb27676c942d8f90046e62951c71fa72f2fd71ab.jpg", "img_caption": ["Figure 1: A schematic depiction of how the Generalized Kolmogorov-Smirnov (KS) distance between target $\\mathbb{P}_{F}$ and approximate $\\mathbb{P}_{G}$ distributions with respect to critic $c_{\\phi}$ is computed. The critic is evaluated on samples $x_{F}$ (|||) and $x_{G}\\left(|\\right)$ from the target and approximate distributions respectively. The $\\lambda$ threshold moves from $-\\infty$ to $+\\infty$ establishing a stack of level sets. At each level, the fraction of datapoints $\\dot{\\mathbf{\\rho}}_{0}$ and $\\bullet$ ) below the threshold is calculated for each distribution independently. This produces the $\\dot{\\mathbb{P}}_{F}\\left(\\Gamma_{c_{\\phi}}(\\lambda)\\right)$ and $\\mathbb{P}_{G}\\left(\\Gamma_{c_{\\phi}}(\\lambda)\\right)$ curves. The Generalized KS distance is the largest absolute difference between the curves shown as $\\updownarrow$ in the right figure. Best viewed in color. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "14 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "15 Generative modeling is about ftiting a model to a target distribution, usually the data. A fundamental   \n16 taxonomy of models assigns them into prescribed and implicit statistical models [9], with partial   \n17 overlap between the two classes. Prescribed models directly parameterize the distribution\u2019s probability   \n18 density function, while implicit models parameterize the generator that allows samples to be drawn   \n19 from the distribution. The ultimate application of the model primarily dictates the choice between the   \n20 two approaches. It does, however, have consequences regarding the available types of divergences   \n21 that we can minimize when fitting the model. The divergences differ in the stability of optimization   \n22 and computational efficiency, as well as statistical efficiency, which all affect the final performance of   \n23 the model.   \n24 The natural approach for fitting a prescribed model is maximum likelihood estimation (MLE),   \n25 equivalently formulated as minimization of Kullback\u2013Leibler divergence. Likelihood evaluation   \n26 for normalized models is straightforward. In non-normalized models, density evaluation is ex  \n27 pensive; in this context, Hyv\u00e4rinen [22] proposed the score matching objective, which can be   \n28 interpreted as the Fisher divergence [30]. This approach is very effective for simulation-free training   \n29 of ODE[7]/SDE[42, 19]-based models which are state-of-the-art in multiple domains today.   \n30 The principle driving the ftiting of implicit statistical models is to push the model to generate samples   \n31 that are indistinguishable from the target. An inflection point for this family of models came with the   \n32 Generative Adversarial Network (GAN) [13], which took the principle literally and introduced an   \n33 auxiliary classifier trained in an adversarial process to discriminate between the two distributions.   \n34 The classification error given an optimal classifier relates to the Jensen\u2013Shannon divergence between   \n35 generator and the target. Initial work in this area involved applying heuristic tricks to deal with   \n36 learning problems, namely vanishing gradients, unstable training, and mode dropping or collapse.   \n37 Further advancements focused on using other distances based on the principle of adversarial learning   \n38 of auxiliary models, which were supposed to have certain favorable properties with respect to the   \n39 original GAN.   \n40 The Bayesian inference community has been reluctant to adopt adversarial methods [8], and the   \n41 attempts to apply them in this context [40] indicate a credibility problem. A significant drawback of   \n42 approximate methods is the excessive reduction of diversity in the distribution [17], the extremes of   \n43 which lead to mode dropping [1]. In this work, we consider another distance for training implicit   \n44 statistical models, i.e., the Kolmogorov-Smirnov (KS) distance, which, to the best of our knowledge,   \n45 has not been used in this context before. The distinctive feature of the KS distance is that it directly   \n46 measures the coverage discrepancy of each other\u2019s credibility regions by the distributions under   \n47 analysis at all confidence levels. Thus, its minimization straightforwardly leads to the correct spread   \n48 of the probability mass, avoiding mode dropping, overconfidence, and mode collapse when applied   \n49 with a sufficient sampling budget.   \n50 We term the proposed model as Kolmogorov-Smirnov Generative Adversarial Network (KSGAN).   \n51 We show how to generalize the standard KS distance to higher dimensions based on Polonik [38] in   \n52 section 2, allowing our method to be used for multidimensional distributions. Next, in section 3, we   \n53 show how to efficiently leverage the distance in an adversarial training process and show formally   \n54 that the proposed algorithm leads to an alignment of the approximate and target distributions. We   \n55 support the theoretical findings with empirical results presented in section 6. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "56 2 Generalized Kolmogorov\u2013Smirnov distance ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "57 We generalize the Kolmogorov\u2013Smirnov (KS) distance (sometimes called simply Kolmogorov   \n58 distance) between continuous probability distributions on one-dimensional spaces to multidimensional   \n59 spaces and show that it is a metric. The test statistic of the KS test is a KS distance between empirical   \n60 and target distributions (or two empirical in the case of the two-sample case). For this reason, our   \n61 proposal is directly inspired by the generalization of the test introduced in Polonik [38].   \n62 Let us consider two probability measures $\\mathbb{P}_{F}$ and $\\mathbb{P}_{G}$ on a measurable space $({\\mathcal{X}},{\\mathcal{A}})$ , where the   \n63 sample space $\\mathcal{X}$ is a vector space such as ${\\boldsymbol{\\mathit{\\Pi}}}^{H^{d}}$ and $\\boldsymbol{\\mathcal{A}}$ is the corresponding event space; $F:\\mathcal{X}\\to[0,1]$   \n64 and $G:\\mathcal{X}\\rightarrow[0,1]$ are the cumulative distribution functions (CDFs) of $\\mathbb{P}_{F}$ and $\\mathbb{P}_{G}$ respectively.   \n65 We say that $\\mathbb{P}_{F}=\\mathbb{P}_{G}$ iff $\\forall\\,A\\in A$ , $\\mathbb{P}_{F}(A)=\\mathbb{P}_{G}(A)$ . When $\\dim(\\mathcal{X})=1$ then the KS distance is ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\nD_{\\mathrm{KS}}\\left(\\mathbb{P}_{F},\\mathbb{P}_{G}\\right):=\\operatorname*{sup}_{x\\in\\mathcal{X}}|F(x)-G(x)|.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "66 In the multivariate case, the problem with using the KS distance as is is that on a $d$ -dimensional   \n67 space, there are $2^{d}-1$ ways of defining a CDF. The distance has to be independent of the particular   \n68 definition and thus should be the largest across all the possibilities [35]. This, however, becomes   \n69 prohibitive for any $d>2$ . In other words, the challenge comes from a multidimensional vector space   \n70 not being a partially ordered set. Everything that follows in this section consists of proposing a partial   \n71 order, showing that, under certain conditions, a probability distribution can be uniquely determined   \n72 on its basis and operationalizing it in an optimization problem. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "73 We begin by bringing the classical result that ", "page_idx": 2}, {"type": "equation", "text": "$$\nD_{\\mathrm{KS}}\\left(\\mathbb{P}_{F},\\mathbb{P}_{G}\\right)=\\operatorname*{sup}_{\\alpha\\in[0,1]}|F(G^{-1}(\\alpha))-\\alpha|,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "74 where $G^{-1}:[0,1]\\to\\mathcal{X}$ is the inverse CDF also called the quantile function. Einmahl and Mason   \n75 [10] show that there exists a natural generalization of the quantile function to multivariate distribution,   \n76 which we restate below.   \n77 Definition 1 (Generalized Quantile Function). Let v $:{\\mathcal{A}}\\rightarrow{\\mathbb{R}}_{+}$ be a measure, and $\\mathcal{C}\\subset\\mathcal{A}$ an   \n78 arbitrary subset of the event space, then a function $C_{\\mathbb{P},c}(\\alpha):[0,1]\\rightarrow\\mathcal{C}$ such that ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\nC_{\\mathbb{P},\\mathcal{C}}(\\alpha)\\in\\underset{C\\in\\mathcal{C}}{\\arg\\operatorname*{min}}\\{\\mathrm{v}(C):\\mathbb{P}(C)\\geqslant\\alpha\\}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "79 is called the generalized quantile function in $\\mathcal{C}$ for $\\mathbb{P}$ with respect to $\\mathrm{v}^{2}$ . ", "page_idx": 2}, {"type": "text", "text": "80 The generalized quantile function evaluated at level $\\alpha$ yields a minimum-volume set [36] whose   \n81 probability is at least $\\alpha$ , and it is the smallest with respect to $\\mathrm{v}$ such set in $\\mathcal{C}$ , thus the name. For the   \n82 remainder of this paper, we assume that v is the Lebesgue measure.   \n83 It may seem that it is enough to plug $C_{\\mathbb{P}_{G},c}(\\alpha)$ in place of $G^{-1}(\\alpha)$ and $\\mathbb{P}_{F}$ in place of $F$ in eq. (2)   \n84 to establish the Generalized KS distance but it turns out that such a distance does not satisfy the   \n85 positivity condition $D_{\\mathrm{KS}}\\left(\\mathbb{P}_{F},\\mathbb{P}_{G}\\right)>0$ if $\\mathbb{P}_{F}\\neq\\mathbb{P}_{G}$ as the example below shows.   \n86 Example 1 (Polonik [38]). Let $\\mathbb{P}_{F}$ be the probability measure of a chi distribution with one degree   \n87 of freedom $\\sqrt{\\chi_{1}^{2}}$ which has support on $R_{+}$ and $\\mathbb{P}_{G}$ the probability measure of a standard Gaussian   \n88 distribution $\\mathcal{N}(0,1)$ which has support on the whole $\\mathcal{R}$ . Given ${\\mathcal{C}}={\\mathcal{A}}$ we have ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{P}_{F}(C_{\\mathbb{P}_{G},\\mathscr{C}}(\\alpha))=\\alpha\\,\\forall\\alpha\\in[0,1],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "89 while clearly $\\mathbb{P}_{F}\\neq\\mathbb{P}_{G}$ . The statement in eq. (4) is easy to show by observing that $\\forall x\\in[0,\\infty)$ the   \n90 density of $\\mathbb{P}_{F}$ is twice the density of $\\mathbb{P}_{G}$ and $C_{\\mathbb{P}_{G},c}(\\alpha)$ are intervals centered at $\\boldsymbol{O}$ .   \n91 Instead, a solution based on the quantile functions of both distributions is needed, which we present   \n92 in definition 2.   \n93 Definition 2 (Generalized Kolmogorov-Smirnov distance). Let the Generalized Kolmogorov-Smirnov   \n94 distance be formulated as follows: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\nD_{\\mathrm{GKS}}\\left(\\mathbb{P}_{F},\\mathbb{P}_{G}\\right):=\\operatorname*{sup}_{\\alpha\\in\\left[0,1\\right]\\atop{C\\in\\left\\{C_{\\mathbb{P}_{G},c},C_{\\mathbb{P}_{F},c}\\right\\}}}\\left[\\left|\\mathbb{P}_{F}(C(\\alpha))-\\mathbb{P}_{G}(C(\\alpha))\\right|\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "95 Such distance is symmetric, satisfying the triangle inequality as shown in appendix A.1. For the   \n96 remainder of this section, we will show that the Generalized KS distance in eq. (5) meets the necessary   \n97 $D_{\\mathrm{GKS}}\\left(\\mathbb{P},\\mathbb{P}\\right)=0$ and sufficient $D_{\\mathrm{GKS}}\\left(\\mathbb{P}_{F},\\mathbb{P}_{G}\\right)>0$ if $\\mathbb{P}_{F}\\neq\\mathbb{P}_{G}$ conditions to consider it a metric.   \n98 In the proof, we will rely on the probability density function of $\\mathbb{P}$ with respect to a reference measure   \n99 v, which we denote with $p:\\mathcal{X}\\to[0,\\infty)$ . Let ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Gamma_{p}(\\lambda):=\\{x:p(x)\\geqslant\\lambda\\}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "100 denote the density level set of $p$ at level $\\lambda\\geqslant0$ (also called the highest density region [21]), and let   \n101 $\\Pi_{p}:=\\{\\Gamma_{p}(\\lambda):\\lambda\\geqslant0\\}$ . The following observations about level sets will introduce the fundamental   \n102 tools to prove the necessary and sufficient conditions for the generalized KS distance. ", "page_idx": 2}, {"type": "text", "text": "103 Remark 1 (The silhouette [37]). For any density $p_{i}$ , the following holds ", "page_idx": 2}, {"type": "equation", "text": "$$\np(x)=\\int_{0}^{\\infty}\\mathbf{1}_{\\Gamma_{p}(\\lambda)}(x)\\mathrm{d}\\lambda,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "104 where $\\mathbb{1}_{C}$ denotes the indicator function of a set $C$ . The RHS of eq. (7) is called the silhouette. ", "page_idx": 2}, {"type": "text", "text": "105 An immediate consequence of remark 1 is that $\\Pi_{p}$ ordered with respect to $\\lambda\\geqslant0$ fully characterizes   \n106 $\\mathbb{P}$ , because $p$ does. Graphically, the silhouette is a multidimensional stack of level sets.   \n107 Remark 2. Density level sets are minimum-volume sets $[38]$ The quantity $\\mathbb{P}(C)~-~\\lambda\\,\\mathrm{v}(C)$   \n108 is maximized over $\\boldsymbol{\\mathcal{A}}$ by $\\Gamma_{p}(\\lambda)$ , and thus if $\\Gamma_{p}(\\lambda)\\ \\in\\ c$ , then $\\Gamma_{p}(\\lambda)~=~C_{\\mathrm{P},\\mathcal{C}}(\\dot{\\alpha})^{3}$ at level   \n109 $\\begin{array}{r}{\\alpha=\\mathbb{P}(\\Gamma_{p}(\\lambda))=\\int p(x)\\dot{\\mathbb{1}}_{[\\lambda,\\infty)}\\dot{(p(x))}\\mathrm{d}x}\\end{array}$ .   \n110 Below, we present the fundamental theoretical result behind the proposed method, which restates   \n111 Lemma 1.2. of Polonik [38].   \n112 Theorem 1 (Necessary and sufficient conditions). Let v be a measure on $({\\mathcal{X}},{\\mathcal{A}})$ . Suppose that   \n113 $\\mathbb{P}_{F}$ and $\\mathbb{P}_{G}$ are probability measures on $({\\mathcal{X}},{\\mathcal{A}})$ with densities (with reference measure v) $f$ and $g$   \n114 respectively. Assuming that   \n115 $A.I\\ \\prod_{f}\\cup\\prod_{g}\\subset{\\mathcal{C}};$   \n116 A.2 $C_{\\mathbb{P}_{F},c}(\\alpha)$ and $C_{\\mathbb{P}_{G},c}(\\alpha)$ are uniquely determined4 in $\\mathcal{C}$ with respect to v   \n117 the following two statements are equivalent:   \n118 S.1 $\\mathbb{P}_{F}=\\mathbb{P}_{G}.$ ;   \n119 $S.2\\,\\mathrm{\\it~D}_{\\mathrm{GKS}}\\left(\\mathbb{P}_{F},\\mathbb{P}_{G}\\right)=0.$ ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "120 See proof in Appendix A . ", "page_idx": 3}, {"type": "text", "text": "121 Meeting assumption A.1 is a demanding challenge, almost equivalent to learning the target distribution.   \n122 Below, we propose a relaxation of it, which we will use to show the validity of our method.   \n123 Theorem 2 (Relaxation of assumption A.1). Theorem $^{\\,l}$ holds if assumption A.1 is relaxed to the   \n124 case that $\\mathcal{C}$ contains sets that are uniquely determined with density level sets of $\\mathbb{P}_{F}$ and $\\mathbb{P}_{G}$ up to $a$   \n125 set $C$ such that ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\forall_{C^{\\prime}\\in2^{C}}\\mathbb{P}_{F}(C^{\\prime})=\\mathbb{P}_{G}(C^{\\prime}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "126 and let $r:=\\mathbb{P}_{F}(C)=\\mathbb{P}_{G}(C),$ , then the supremum in statement S.2 is restricted to $\\left[0,1-r\\right]$ . ", "page_idx": 3}, {"type": "text", "text": "127 See proof in Appendix A . ", "page_idx": 3}, {"type": "text", "text": "128 3 Kolmogorov\u2013Smirnov GAN ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "129 For the remainder of the paper, we will consider $\\mathbb{P}_{F}$ as the target distribution represented by a dataset   \n130 $\\{x_{F}\\}$ , and $\\mathbb{P}_{G}$ as the approximate distribution that we want to train by minimizing the Generalized   \n131 KS distance in eq. (5) with Stochastic Gradient Descent. We model $\\mathbb{P}_{G}$ as a pushforward $g_{\\theta\\not=}\\mathbb{P}_{Z}$ of   \n132 a simple (e.g., Gaussian, or Uniform) latent distribution $\\mathbb{P}z$ supported on $\\mathcal{Z}$ , with a neural network   \n133 $g_{\\theta}:\\mathcal{Z}\\rightarrow\\mathcal{X}$ , parameterized with $\\theta$ , which we call the generator.   \n134 The major challenge in utilizing eq. (5) is the necessity of finding the $C_{\\mathbb{P},\\mathcal{C}}(\\alpha)$ terms which is an   \n135 optimization problem on its own. The idea that we propose in this work is to amortize the procedure   \n136 by modeling the generalized quantile functions $\\bar{C}_{\\mathbb{P}_{F},\\mathcal{C}}(\\alpha)$ and $C_{\\mathbb{P}_{G},c}(\\alpha)$ with additional neural   \n137 networks which have to be trained in parallel to the generator $g_{\\theta}$ . Therefore, our method is based   \n138 on adversarial training [13], where optimization proceeds in alternating phases of minimization   \n139 and maximization for different sets of parameters. Hence the name of the proposed method, the   \n140 Kolmogorov\u2013Smirnov Generative Adversarial Network. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "141 3.1 Neural Quantile Function ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "142 The generalized quantile function defined in definition 1 is an infinite-dimensional vector function   \n143 $C_{\\mathbb{P},\\bar{c}}:[0,1]\\to\\bar{C}\\in\\mathcal{C}$ . Such objects do not have an expressive, explicit representation that allows   \n144 for gradient-based optimization. Therefore, we use an implicit representation inspired by density   \n145 level sets in eq. (6). We propose to use neural level sets defined in definition 3 that are modeled by a   \n146 neural network $c:\\mathcal{X}\\rightarrow\\mathbb{R}$ , which we will refer to as the critic.   \n147 Definition 3 (Neural level set). Given a neural network $c:\\mathcal{X}\\to\\mathbb{R}$ , the neural level set at level $\\lambda$ is   \n148 defined $a s^{5}$ ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Gamma_{c}(\\lambda):=\\{x:c(x)\\leqslant\\lambda\\},\\,\\,a n d\\,l e t\\,\\Pi_{c}:=\\{\\Gamma_{c}(\\lambda):\\lambda\\in I\\!\\!R\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "149 Neural level sets are used, for example, in image segmentation [6, 20] and surface reconstruction   \n150 from point clouds [3]. They fti our application because for computing the Generalized KS distance in   \n151 eq. (5), the explicit materialization of generalized quantiles is not required as long as the probability   \n152 measure can be efficiently evaluated on the implicitly specified sets. We set ${\\mathcal{C}}\\,=\\,\\Pi_{c}$ , and thus   \n153 $C_{\\mathbb{P},\\Pi_{c}}(\\alpha)=\\Gamma_{c}(\\lambda_{\\alpha})$ , with $\\lambda_{\\alpha}=\\arg\\operatorname*{min}_{\\lambda\\in I\\!R}\\{\\bar{\\lambda}:\\mathbb{P}(\\bar{\\Gamma}_{c}(\\bar{\\lambda}))\\geqslant\\alpha\\}$ . For a probability measure $\\mathbb{P}^{\\prime}$   \n154 the following holds: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}^{\\prime}\\left(C_{\\mathbb{P},\\Pi_{c}}(\\alpha)\\right)=\\mathbb{E}_{x\\sim\\mathbb{P}^{\\prime}}\\left[\\mathbb{1}_{(-\\infty,\\lambda_{\\alpha}]}(c(x))\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "155 which shows that the terms in eq. (5) under neural  level sets can be Monte-Carlo estimated given   \n156 samples from the respective distributions. Assumption A.2 is satisfied by neural level sets by   \n157 construction.   \n158 The formulation of the Generalized KS distance in eq. (5) includes two generalized quantile functions   \n159 $C_{\\mathbb{P}_{F},c}(\\alpha)$ corresponding to target distribution $\\mathbb{P}_{F}$ and $C_{\\mathbb{P}_{G},c}(\\alpha)$ corresponding to the approximate   \n160 distribution $\\mathbb{P}_{G}$ . Both have to be modeled with the respective neural networks $c_{\\phi_{F}}$ and $c_{\\phi_{G}}$ , where   \n116621 pwaer aumsee $\\phi=\\{\\phi_{F},\\phi_{G}\\}$ ctso  wdietnh oat es itnhgel ej onienut rsaelt  noeft twhoerikr.  pWarea smeet $\\mathcal{C}=\\Pi_{c_{\\phi_{F}}}\\cup\\Pi_{c_{\\phi_{G}}}$ ., we show how to ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "163 3.2 Optimizing generator\u2019s parameters $\\theta$ ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "164 The Generalized KS distance in eq. (5) is a supremum over a unit interval and two functions; thus, it   \n165 can be upper-bounded as ", "page_idx": 4}, {"type": "equation", "text": "$$\nD_{\\mathrm{GKS}}\\left(\\mathbb{P}_{F},\\mathbb{P}_{G}\\right)\\leqslant\\sum_{C\\in\\{C_{\\mathrm{P}_{G},\\,c},C_{\\mathrm{P}_{F},c}\\}}\\operatorname*{sup}_{\\alpha\\in[0,1]}\\left[|\\mathbb{P}_{F}(C(\\alpha))-\\mathbb{P}_{G}(C(\\alpha))|\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "166 Next, we plug in $\\mathcal{C}=\\Pi_{c_{\\phi_{F}}}\\cup\\Pi_{c_{\\phi_{G}}}$ to eq. (11) and use eq. (10) to get generator\u2019s objective: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{g}=\\sum_{\\substack{c_{\\phi}\\in\\{c_{\\phi_{G}},c_{\\phi_{F}}\\}}}\\operatorname*{sup}_{\\lambda\\in\\mathbb{R}}\\left[\\left|\\mathbb{E}_{x\\sim\\mathbb{P}_{F}}\\left[\\mathbb{1}_{(-\\infty,\\lambda]}(c_{\\phi}(x))\\right]-\\mathbb{E}_{x\\sim\\mathbb{P}_{G}}\\left[\\mathbb{1}_{(-\\infty,\\lambda]}(c_{\\phi}(x))\\right]\\right|\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "167 In practice, the expectations in eq. (12) are estimated on finite samples from the two distributions,   \n168 i.e. $\\{x_{F}\\}$ mentioned before, and $\\{x_{G}\\}$ sampled from the approximate distribution $\\mathbb{P}_{G}$ using the   \n169 reparametrization trick to facilitate backpropagation of gradients. Therefore, the two terms become   \n170 step functions in $\\lambda$ , and the supremum is located on one of the steps. That way, a line search on $\\mathcal{R}$   \n171 reduces to a maximum over a finite set. To preserve the differentiability of the cost function calculated   \n172 in this way, we apply Straight-through Estimator [4] in place of indication function 1. A schematic   \n173 depiction of the process for a single critic is shown in fig. 1. ", "page_idx": 4}, {"type": "text", "text": "174 3.3 Optimizing critics\u2019 parameters $\\phi$ ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "175 By optimizing critics\u2019 parameters $\\phi$ , we want to satisfy assumption A.1 so that Generalized KS   \n176 distance becomes a metric. For the problem posed in such a way, we lack supervision, i.e., we do   \n177 not know the target sets\u2019 shapes. However, we can reformulate the problem as an estimation of the   \n178 density functions of the two considered measures $\\mathbb{P}_{F}$ and $\\mathbb{P}_{G}$ and use the obtained approximate   \n179 density models to build level sets. We can constitute an optimization problem for such a task based   \n180 solely on finite sets of samples, which we have for $\\mathbb{P}_{F}$ and can arbitrarily generate from $\\mathbb{P}_{G}$ . As   \n181 the estimator, we propose to use the Energy-based model (EBM) [43], which, thanks to the lack of   \n182 constraints in the choice of architecture, can be very expressive while having favorable computational   \n183 complexity at inference. To carry out EMB training effectively, we will introduce a new min-max   \n184 game, the \u201cmin phase\u201d of which will turn out to be the initial objective in eq. (5), and in this way, we   \n185 will close the adversarial cycle.   \n186 Let the critic $c_{\\phi_{F}}(\\boldsymbol{x})$ serve as the energy function. The density given by the EBM is then $p_{c_{\\phi_{F}}}(x)=$   \n187 $\\exp(-c_{\\phi_{F}}(x))/Z_{c_{\\phi_{F}}}$ , where $\\begin{array}{r}{Z_{c_{\\phi_{F}}}=\\int\\exp(-c_{\\phi_{F}}(x))\\mathrm{d}x}\\end{array}$ is the normalizing constant called partition ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Input :Target distribution $\\mathbb{P}_{F}$ ; latent distribution $\\mathbb{P}_{Z}$ ; generator network $g_{\\theta}$ ; critic network $c_{\\phi}$ ; number of critic updates $k_{\\phi}$ ; number of generator updates $k_{\\theta}$ ; score penalty weight $\\beta$ ;   \nOutput :Trained model $\\mathbb{P}_{G}$ approximating $\\mathbb{P}_{F}$ ;   \n1 repeat   \n2 for $i=1$ to $k_{\\phi}$ do 3 Draw batch $\\{x\\}\\sim\\mathbb{P}_{F}$ and $\\{z\\}\\sim\\mathbb{P}_{Z}$ ; // critic\u2019s inner loop 4 $\\begin{array}{r}{\\mathcal{R}_{c}\\gets\\frac{1}{|\\{z\\}|}\\sum_{\\{z\\}}\\|\\nabla_{x}c_{\\phi}(g_{\\theta}(z))\\|_{2}^{2}+\\frac{1}{|\\{x\\}|}\\sum_{\\{x\\}}\\|\\nabla_{x}c_{\\phi}(x)\\|_{2}^{2}.}\\end{array}$ ; 5 $\\begin{array}{r}{{\\mathcal{L}}_{c}\\gets\\frac{1}{|\\{z\\}|}\\sum_{\\{z\\}}c_{\\phi}(g_{\\theta}(z))-\\frac{1}{|\\{x\\}|}\\sum_{\\{x\\}}c_{\\phi}(x)}\\end{array}$ ; 6 Update $\\phi$ by using \u2202(Lc\u2212\u03b2Rc)to maximize Lc\u2212\u03b2Rc;   \n7 for $i=1$ to $k_{\\theta}$ do   \n8 Draw batch $\\{x\\}\\sim\\mathbb{P}_{F}$ and $\\{z\\}\\sim\\mathbb{P}_{Z}$ ; // generator\u2019s inner loop 9 $\\{c_{F}\\}\\gets\\{c_{\\phi}(\\bar{x}):\\{x\\}\\}$ and $\\{c_{G}\\}\\gets\\{c_{\\phi}(g_{\\theta}(z)):\\{z\\}\\}$ ;   \n10 $\\{\\lambda\\}\\leftarrow\\{c_{F}\\}\\cup\\{c_{G}\\}$ ;   \n11 $\\begin{array}{r l}&{\\dot{\\mathcal{L}}_{g,F}\\gets\\operatorname*{max}_{\\{\\lambda\\}}\\Big|\\frac{1}{|\\{z\\}|}\\sum_{\\{c_{G}\\}}\\mathbb{1}_{(-\\infty,\\lambda]}(c_{G})-\\frac{1}{|\\{x\\}|}\\sum_{\\{c_{F}\\}}\\mathbb{1}_{(-\\infty,\\lambda]}(c_{F})\\Big|;}\\\\ &{\\dot{\\mathcal{L}}_{g,G}\\gets\\operatorname*{max}_{\\{\\lambda\\}}\\Big|\\frac{1}{|\\{x\\}|}\\sum_{\\{c_{F}\\}}\\mathbb{1}_{(-\\infty,-\\lambda]}(-c_{F})-\\frac{1}{|\\{z\\}|}\\sum_{\\{c_{G}\\}}\\mathbb{1}_{(-\\infty,-\\lambda]}(-c_{G})\\Big|;}\\end{array}$   \n12   \n13 $\\mathcal{L}_{g}\\gets\\mathcal{L}_{g,F}+\\mathcal{L}_{g,G}$ ;   \n14 Update \u03b8 by using \u2202\u2202L\u03b8g to minimize $\\mathcal{L}_{g}$ ;   \n15 until not converged; ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "188 function. The standard technique for learning the model given target data distribution $\\mathbb{P}_{F}$ is MLE,   \n189 where the likelihood ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x\\sim\\mathbb{P}_{F}}[\\log p_{c_{\\phi_{F}}}(x)]=\\mathbb{E}_{x\\sim\\mathbb{P}_{F}}[-c_{\\phi_{F}}(x)]-\\log Z_{c_{\\phi_{F}}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "190 is maximized wrt $\\phi_{F}$ . An unbiased estimate of the gradient of the second term can be obtained with   \n191 samples from the EBM itself, typically achieved with MCMC sampling. Many approaches to avoid   \n192 this expensive procedure have been described in the literature [43], and among them, the one based on   \n193 adversarial training [23] is the most appealing to us. It introduces an auxiliary distribution $\\mathbb{P}_{a u x(F)}$   \n194 such that the gradient of eq. (13) wrt $\\phi_{F}$ is approximated with the gradient of ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x\\sim\\mathbb{P}_{F}}[-c_{\\phi_{F}}(x)]-\\mathbb{E}_{x\\sim\\mathbb{P}_{a u x(F)}}[-c_{\\phi_{F}}(x)].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "195 Consequently, an additional objective $\\mathcal{L}_{a u x(F)}$ must be introduced, the optimization of which will   \n196 lead to the alignment of $\\mathbb{P}_{a u x(F)}$ and $\\mathbb{P}_{c_{\\phi_{F}}}$ , where $\\mathbb{P}_{c_{\\phi_{F}}}$ denotes the probability distribution with   \n197 density $p_{c_{\\phi_{F}}}(x)$ . We take an analogous approach to estimate $c_{\\phi_{G}}(x)$ .   \n198 When we (i) set $c_{\\phi_{G}}(x):=-c_{\\phi_{F}}(x)$ , and (ii) repurpose $\\mathbb{P}_{G}$ as $\\mathbb{P}_{a u x(F)}$ and $\\mathbb{P}_{F}$ as $\\mathbb{P}_{a u x(G)}$ , we   \n199 show in appendix A.2 that the MLE objectives for the critics \u2013 now, denoted as $c_{\\phi}$ \u2013 simplify as   \n200 $\\mathcal{L}_{c}=\\mathbb{E}_{x\\sim\\tilde{\\mathbb{P}}_{G}}[c_{\\phi}(x)]-\\mathbb{E}_{x\\sim\\mathbb{P}_{F}}[c_{\\phi}(x)]$ , which is then maximized in an adversarial game against the   \n201 Generalized KS distance in eq. (5).   \n202 The standard approach for aligning the auxiliary distributions with their targets is to use the Kullback\u2013   \n203 Leibler divergence. We propose using the Generalized KS distance instead. We set $\\mathcal{L}_{a u x(F)}=$   \n204 $D_{\\mathrm{GKS}}\\left(\\mathbb{P}_{G},\\mathbb{P}_{c_{\\phi}}\\right)$ and $\\mathcal{L}_{a u x(\\mathbb{P}_{G})}=D_{\\mathrm{GKS}}\\left(\\mathbb{P}_{F},\\mathbb{P}_{-c_{\\phi}}\\right)$ . By analyzing these objectives in the fashion   \n205 of section 3.2, we note that $\\mathcal{L}_{a u x(\\mathbb{P}_{G})}$ is the same as our original objective $D_{\\mathrm{GKS}}\\left(\\mathbb{P}_{F},\\mathbb{P}_{G}\\right)$ \u2013 which is   \n206 symmetric \u2013 when we approximate sampling from $\\mathbb{P}_{c_{\\phi}}$ with the target distribution $\\mathbb{P}_{F}$ . Analogously   \n207 for $\\mathcal{L}_{a u x(\\mathbb{P}_{G})}$ where sampling from $\\mathbb{P}_{-c_{\\phi}}$ is approximated with $\\mathbb{P}_{G}$ . Therefore, we have shown that   \n208 the auxiliary objectives are already integrated into the adversarial game.   \n209 In practice, we find the score penalty regularizer of Kumar et al. [26], derived from the score   \n210 matching objective, helpful to stabilize training. Therefore, we subtract it from $\\mathcal{L}_{c}$ weighted by a   \n211 hyperparameter $\\beta$ . In this way, we get a critic that is smoother and, therefore, generates regular level   \n212 sets that facilitate optimization. We summarize the proposed training procedure in algorithm 1. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "213 4 Discussion ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "214 In section 3.3, where we justify the choice of the critic\u2019s objective function, we refer to methods   \n215 for training EBMs, which are approximate density distribution models. Thus, the reader can expect   \n216 that our proposed critic $c_{\\phi}$ in the limit of convergence of the algorithm will become a source of   \n217 information about the density distribution of the target distribution $\\mathbb{P}_{F}$ accompanying the model that   \n218 generates samples $\\mathbb{P}_{G}$ . However, this does not happen as a consequence of the design choice (i),   \n219 that is, the setup of $c_{\\phi_{F}}=-c_{\\phi_{G}}=c_{\\phi}$ . An EBM can only be equivalent to its inverse in the case   \n220 of a uniform distribution. In addition, because of design choice (ii), during training, the critic is   \n221 not evaluated outside of the support of $\\mathbb{P}_{F}$ and $\\mathbb{P}_{G}$ and, therefore, can reach arbitrary values there.   \n222 Despite these observations, the Generalized KS distance present in our algorithm exposes sufficient   \n223 conditions because of theorem 2.   \n224 The feature distinguishing KSGAN from other adversarial generative modeling approaches is that   \n225 regardless of the outcome of the critic\u2019s inner problem, minimizing eq. (5) is justified because Gener  \n226 alized KS distance, despite not meeting assumption A.1, is a pseudo-metric [38]. For comparison,   \n227 the dual representation of Wasserstein distance, used in WGAN [2] requires attaining the supremum   \n228 in the inner problem.   \n229 The distances used for training generative models all fall into either the category of $f$ -divergences   \n230 $\\begin{array}{r}{D_{f}(\\mathbb{P}_{F},\\mathbb{P}_{G})\\,=\\,\\int_{\\cal A}f\\left(\\mathrm{d}\\mathbb{P}_{F}/\\tilde{\\mathrm{d}}\\tilde{\\mathbb{P}}_{G}\\right)\\mathrm{d}\\mathbb{P}_{G}}\\end{array}$ or integral probability metrics (IPMs) $D_{\\mathcal{F}}(\\mathbb{P}_{F},\\mathbb{P}_{G})\\,=$   \n231 $\\operatorname*{sup}_{f\\in{\\mathcal{F}}}|{\\mathbb{E}}_{x\\sim\\mathbb{P}_{F}}f(x)-{\\mathbb{E}}_{x\\sim\\mathbb{P}_{G}}f(x)|$ . The classical one-dimensional KS distance is an instance of   \n232 IPM with $\\mathcal{F}=\\{\\mathbb{1}_{(-\\infty,t]}|t\\in I\\!R\\}$ or $\\mathcal{F}=\\{\\mathbb{1}_{G^{-1}(\\alpha)}|\\alpha\\in[0,1]\\}$ when having access to the inverse   \n233 CDF of one of the distributions based on eq. (2). One can see the Generalized KS distance from the   \n234 perspective of IPM with $\\mathcal{F}=\\left\\{\\mathbb{1}_{C(\\alpha)}|\\alpha\\in[0,1]\\;\\&\\;C\\in\\{C_{\\mathbb{P}_{F},\\mathscr{C}},C_{\\mathbb{P}_{G},\\mathscr{C}}\\}\\right\\}$ . Assuming direct access   \n235 to $C_{\\mathbb{P}_{F},\\mathcal{C}}$ and $C_{\\mathbb{P}_{G},\\mathcal{C}}$ , for example when both $\\mathbb{P}_{F}$ and $\\mathbb{P}_{G}$ are Normalizing Flows [24, 34], measuring   \n236 the distance comes down to a line search. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "237 5 Related work ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "238 The need to generalize the KS test, and therefore distance, to multiple dimensions arose naturally   \n239 from the side of practitioners who collected such data and wished to test related hypotheses. It was   \n240 first addressed by Peacock [35], where a two-dimensional test for applications in astronomy was   \n241 proposed. It involves considering all possible orders in this space and using the one that maximizes   \n242 the distance between the distributions. A modification of this procedure has been proposed by Fasano   \n243 and Franceschini [11] where only four candidate CDFs have to be considered, causing the test to   \n244 be applicable in three dimensions, with eight candidates, under similar computational constraints.   \n245 Chronologically, the following approach was the one on which we base our work, proposed in Polonik   \n246 [38] but made possible by the author\u2019s earlier work [36, 37]. To the best of our knowledge, the first   \n247 work that practically uses the theory developed by Polonik is Glazer et al. [12], which we recommend   \n248 as an introduction to our work. It proposes applying the Generalized KS test based on the support   \n249 vector machines for detecting distribution shifts in data streams.   \n250 As an instance of the adversarial generative modeling family, our work is related to all the countless   \n251 GAN [13] follow-ups. We highlight those that study the learning process from the perspective of   \n252 the distance being minimized. The work of Arjovsky and Bottou [1] provides a formal analysis of   \n253 the heuristic tricks used for stabilizing the training of GANs. The $f$ -GAN [33] proposes a unified   \n254 training framework targeting $f$ -divergences, which relies on a variational lower bound of the objective   \n255 that results in the adversarial process. Approaches relying on the integral probability metric include   \n256 FisherGAN [32], the Generative Moment Matching Networks [29] based on MMD, just like the   \n257 later, more sophisticated MMD GAN [28], and finally the Wasserstein GAN (WGAN) [2] with the   \n258 WGAN-GP follow-up [16] which shares common features with our work. Our maximum likelihood   \n259 approach to fitting the critic results in the same functional form of the loss as WGAN(-GP) uses. In   \n260 addition, the score penalty we use is similar to the gradient penalty of WGAN-GP. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "261 6 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "262 We evaluate the proposed method on eight synthetic 2D distributions (see appendix B.1 for details)   \n263 and two image datasets, i.e. MNIST [27] and CIFAR-10 [25]. We compare against other adversarial   \n264 methods, GAN and WGAN-GP, using the same neural network architectures and training hyper  \n265 parameters unless specified otherwise (see appendix C for details). All the quantitative results are   \n266 presented based on five random initializations of the models. The source code for all the experiments   \n267 is provided in anonymous code repository.   \n268 In all KSGAN experiments, we relax the maximum in line 11 and line 12 of algorithm 1 with sample   \n269 average. In all experiments, we re-use the last batch of samples from the latent distribution (and   \n270 target distribution in the case of KSGAN) from the critic\u2019s optimization inner loop as the first batch   \n271 for the generator\u2019s optimization inner loop. ", "page_idx": 6}, {"type": "table", "img_path": "8JmUmTgKiY/tmp/d779ca5ca9d28051633ca96e2627ad3d79e543f4d783219f5d5d9fd589cb2389.jpg", "table_caption": ["Table 1: Squared population MMD $\\left(\\downarrow\\right)$ between test data and samples from the methods trained on 65536 samples, averaged over five random initializations with the standard deviation calculated with Bessel\u2019s correction in the parentheses. The proposed KSGAN with $k_{\\phi}=1$ performs on par with the WGAN-GP trained with five times the budget $k_{\\phi}=5$ . See appendix D.1 for qualitative comparison. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "272 6.1 Synthetic distributions ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "273 Analyzing adversarial methods on synthetic, low-dimensional distributions is not popular. However,   \n274 we conduct such an experiment because we are interested in whether the model generates samples   \n275 from the support of the target distribution and how accurately it approximates the distribution.   \n276 Working with small-dimensional distributions, we do not have to be as concerned about the curse of   \n277 dimensionality when calculating sample-based distances, and we can visually compare the resulting   \n278 histograms.   \n279 In table 1, we report the squared population MMD [15] between target and approximate distributions,   \n280 computed with Gaussian kernel on 65536 samples from each distribution. Details about how we   \n281 chose the kernel\u2019s bandwidth can be found in appendix B.1. GAN and WGAN-GP fail to converge   \n282 with $k_{\\phi}=k_{\\theta}=1$ (we do not report the results to economize on space); thus, we set $k_{\\theta}=5$ for them.   \n283 The proposed KSGAN with $k_{\\theta}=1$ performs at a similar level to WGAN-GP, the better of the two   \n284 former, despite using five times less training budget. We present additional results on the synthetic   \n285 datasets in appendix D.1, which include performance with different training dataset sizes, non-default   \n286 hyper-parameter setups for KSGAN, and histograms of the samples for qualitative comparison. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "287 6.2 MNIST ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "288 We use the 50000 training instances to train the models, and based on visual inspection of the   \n289 generated samples (reported in appendix D.2), we conclude that all the methods achieve comparable,   \n290 high samples quality. To assess the quality of the distribution approximation, we use a pre-trained   \n291 classifier on the same data as the generative models (details in appendix B.2). We run the same   \n292 experiment on 3StackedMNIST [44], which has 1000 modes. We report the results in table 2.   \n293 In this experiment, we set the training budget for all methods to $k_{\\phi}=1$ , $k_{\\theta}=1$ for a fair comparison.   \n294 We find that all methods always recover all the modes with the standard MNIST target. However,   \n295 GAN fails to distribute the probability mass uniformly between the digits. As the number of modes   \n296 increases with the 3StackedMNIST target, GAN demonstrates its inferiority to other methods by   \n297 losing 198 modes on average (four initialization cover approx. 985 modes, and one fails to converge,   \n298 achieving only 98 modes). WGAN-GP and KSGAN consistently recover all the modes while being   \n299 on par regarding KL divergence, which differs little between networks\u2019 initialization. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "8JmUmTgKiY/tmp/861905fe5d2f414a7ad41a8d9a2c114a1b1fc27ba23885edef7dfb81253bd0e5.jpg", "table_caption": ["Table 2: The number of captured modes and Kullback-Leibler divergence between the distribution of sampled digits and target uniform distribution averaged over five random initializations with the standard deviation calculated with Bessel\u2019s correction in the parentheses. All the methods were trained with the same budget $k_{\\phi}=1$ , $k_{\\theta}=1$ . WGAN-GP and KSGAN cover all the modes in all experiments while demonstrating low KL divergence. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 3: Inception Score (IS) and Fr\u00e9chet inception distance (FID) metrics averaged over five random initializations with the standard deviation calculated with Bessel\u2019s correction in the parentheses. All the methods were trained with the same budget $k_{\\phi}=1$ , $k_{\\theta}=1$ . The scores for the training dataset are included in the top row, as \u201cReal data\u201d for reference. WGAN-GP and KSGAN perform similarly on average, while KSGAN exhibits lower variance between networks\u2019 initialization. ", "page_idx": 8}, {"type": "table", "img_path": "8JmUmTgKiY/tmp/8165ee65ca12e2ba8e60e9cfe52d9ffcc9464b47ee3ee4dfb520c140ebb206a3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "300 6.3 CIFAR-10 ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "301 We use the 50000 training instances to train the models and report the generated samples in ap  \n302 pendix D.3. We train the models in a fully unconditional manner, i.e., not using the class information   \n303 at all \u2013 contrary to many unconditional models that use class information in normalization layers.   \n304 We quantify the quality of fitted models by computing the Inception Score (IS) [41] and Fr\u00e9chet   \n305 inception distance (FID) [18] from the test set and report the results in table 3 based on five random   \n306 initializations. For reference, in the table, we include the IS of the training dataset and the FID   \n307 between the training and test sets.   \n308 In this experiment, we set the training budget for all methods to $k_{\\phi}=1$ , $k_{\\theta}=1$ for a fair comparison.   \n309 All models fail to accurately approximate the target distribution, which is evident from a quantitative   \n310 comparison in table 3 and a qualitative one in appendix D.3. KSGAN is characterized by the lowest   \n311 variance between initializations among the methods considered. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "312 7 Conclusions and future work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "313 In this work, we investigated the use of Generalized Kolmogorov\u2013Smirnov distance for training   \n314 deep implicit statistical models, i.e., generative networks. We proposed an efficient way to compute   \n315 the distance and termed the resulting model Kolmogorov\u2013Smirnov Generative Adversarial Network   \n316 because it uses adversarial learning. Based on the empirical evaluation of the proposed model, the   \n317 results of which we report, we conclude that it can be considered as an alternative to existing models   \n318 in its class. At the same time, we point out that many properties of KSGAN have not been studied,   \n319 and we leave this as a future work direction.   \n320 Interesting aspects to explore are the characteristics of learning dynamics with the number of generator   \n321 updates exceeding the number of critic updates, alternative ways to train the critic, and alternative   \n322 representations of generalized quantile sets. The natural scaling of the Generalized KS distance may   \n323 also prove beneficial regarding the interpretability of learning curves, learning rate scheduling, or   \n324 early stopping. In addition, we hope that our work will draw the attention of the machine learning   \n325 community to the Generalized KS distance, applications of which remain to be explored. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "326 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "327 [1] M. Arjovsky and L. Bottou. Towards principled methods for training generative adversarial networks. In   \n328 International Conference on Learning Representations, 2017.   \n329 [2] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein generative adversarial networks. In International   \n330 conference on machine learning, pages 214\u2013223. PMLR, 2017.   \n331 [3] M. Atzmon, N. Haim, L. Yariv, O. Israelov, H. Maron, and Y. Lipman. Controlling neural level sets.   \n332 Advances in Neural Information Processing Systems, 32(NeurIPS), 2019.   \n333 [4] Y. Bengio, N. L\u00e9onard, and A. Courville. Estimating or propagating gradients through stochastic neurons   \n334 for conditional computation. arXiv preprint arXiv:1308.3432, 2013.   \n335 [5] C. A. Carolan. The least concave majorant of the empirical distribution function. The Canadian Journal of   \n336 Statistics / La Revue Canadienne de Statistique, 30(2):317\u2013328, 2002.   \n337 [6] G. Chen, Z. Yu, H. Liu, Y. Ma, and B. Yu. DevelSet: Deep Neural Level Set for Instant Mask Optimization.   \n338 IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 42(12):5020\u20135033,   \n339 2023.   \n340 [7] R. T. Chen, Y. Rubanova, J. Bettencourt, and D. K. Duvenaud. Neural ordinary differential equations.   \n341 Advances in neural information processing systems, 31, 2018.   \n342 [8] K. Cranmer, J. Brehmer, and G. Louppe. The frontier of simulation-based inference. Proceedings of the   \n343 National Academy of Sciences, 117(48):30055\u201330062, 2020.   \n344 [9] P. J. Diggle and R. J. Gratton. Monte carlo methods of inference for implicit statistical models. Journal of   \n345 the Royal Statistical Society. Series B (Methodological), 46(2):193\u2013227, 1984.   \n346 [10] J. H. J. Einmahl and D. M. Mason. Generalized Quantile Processes. The Annals of Statistics, 20(2), jun   \n347 1992.   \n348 [11] G. Fasano and A. Franceschini. A multidimensional version of the Kolmogorov\u2013Smirnov test. Monthly   \n349 Notices of the Royal Astronomical Society, 225(1):155\u2013170, mar 1987.   \n350 [12] A. Glazer, M. Lindenbaoum, and S. Markovitch. Learning high-density regions for a generalized   \n351 kolmogorov-smirnov test in high-dimensional data. Advances in Neural Information Processing Sys  \n352 tems, 1:728\u2013736, 2012.   \n353 [13] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio.   \n354 Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Weinberger,   \n355 editors, Advances in Neural Information Processing Systems, volume 27. Curran Associates, Inc., 2014.   \n356 [14] W. Grathwohl, R. T. Chen, J. Bettencourt, I. Sutskever, and D. Duvenaud. Ffjord: Free-form continuous   \n357 dynamics for scalable reversible generative models. arXiv preprint arXiv:1810.01367, 2018.   \n358 [15] A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Sch\u00f6lkopf, and A. Smola. A kernel two-sample test. Journal   \n359 of Machine Learning Research, 13(25):723\u2013773, 2012.   \n360 [16] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville. Improved training of wasserstein   \n361 gans. Advances in neural information processing systems, 30, 2017.   \n362 [17] J. Hermans, A. Delaunoy, F. Rozet, A. Wehenkel, V. Begy, and G. Louppe. A crisis in simulation-based   \n363 inference? beware, your posterior approximations can be unfaithful. Transactions on Machine Learning   \n364 Research, 2022.   \n365 [18] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. Gans trained by a two time-scale   \n366 update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30,   \n367 2017.   \n368 [19] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. Advances in neural information   \n369 processing systems, 33:6840\u20136851, 2020.   \n370 [20] P. Hu, B. Shuai, J. Liu, and G. Wang. Deep level sets for salient object detection. Proceedings - 30th IEEE   \n371 Conference on Computer Vision and Pattern Recognition, CVPR 2017, 2017-Janua:540\u2013549, 2017.   \n372 [21] R. J. Hyndman. Computing and graphing highest density regions. The American Statistician, 50(2):   \n373 120\u2013126, 1996.   \n374 [22] A. Hyv\u00e4rinen. Estimation of non-normalized statistical models by score matching. Journal of Machine   \n375 Learning Research, 6(24):695\u2013709, 2005.   \n376 [23] T. Kim and Y. Bengio. Deep directed generative models with energy-based probability estimation. arXiv   \n377 preprint arXiv:1606.03439, 2016.   \n378 [24] I. Kobyzev, S. J. Prince, and M. A. Brubaker. Normalizing flows: An introduction and review of current   \n379 methods. IEEE transactions on pattern analysis and machine intelligence, 43(11):3964\u20133979, 2020.   \n380 [25] A. Krizhevsky. Learning multiple layers of features from tiny images. 2009.   \n381 [26] R. Kumar, S. Ozair, A. Goyal, A. Courville, and Y. Bengio. Maximum entropy generators for energy-based   \n382 models. arXiv preprint arXiv:1901.08508, 2019.   \n383 [27] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.   \n384 Proceedings of the IEEE, 86(11):2278\u20132324, 1998.   \n385 [28] C.-L. Li, W.-C. Chang, Y. Cheng, Y. Yang, and B. P\u00f3czos. Mmd gan: Towards deeper understanding of   \n386 moment matching network. Advances in neural information processing systems, 30, 2017.   \n387 [29] Y. Li, K. Swersky, and R. Zemel. Generative moment matching networks. In International conference on   \n388 machine learning, pages 1718\u20131727. PMLR, 2015.   \n389 [30] S. Lyu. Interpretation and generalization of score matching. In Proceedings of the Twenty-Fifth Conference   \n390 on Uncertainty in Artificial Intelligence, pages 359\u2013366, 2009.   \n391 [31] T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida. Spectral normalization for generative adversarial   \n392 networks. In International Conference on Learning Representations, 2018.   \n393 [32] Y. Mroueh and T. Sercu. Fisher gan. Advances in neural information processing systems, 30, 2017.   \n394 [33] S. Nowozin, B. Cseke, and R. Tomioka. f-gan: Training generative neural samplers using variational   \n395 divergence minimization. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors, Advances   \n396 in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016.   \n397 [34] G. Papamakarios, E. Nalisnick, D. J. Rezende, S. Mohamed, and B. Lakshminarayanan. Normalizing flows   \n398 for probabilistic modeling and inference. Journal of Machine Learning Research, 22(57):1\u201364, 2021.   \n399 [35] J. A. Peacock. Two-dimensional goodness-of-fit testing in astronomy. Monthly Notices of the Royal   \n400 Astronomical Society, 202(3):615\u2013627, mar 1983.   \n401 [36] W. Polonik. Minimum volume sets in statistics: Recent developments. In R. Klar and O. Opitz, editors,   \n402 Classification and Knowledge Organization, pages 187\u2013194, Berlin, Heidelberg, 1997. Springer Berlin   \n403 Heidelberg.   \n404 [37] W. Polonik. The silhouette, concentration functions and ml-density estimation under order restrictions.   \n405 The Annals of Statistics, 26(5):1857\u20131877, 1998.   \n406 [38] W. Polonik. Concentration and goodness-of-fit in higher dimensions: (Asymptotically) distribution-free   \n407 methods. Annals of Statistics, 27(4):1210\u20131229, 1999.   \n408 [39] A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional   \n409 generative adversarial networks. In Y. Bengio and Y. LeCun, editors, 4th International Conference   \n410 on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track   \n411 Proceedings, 2016.   \n412 [40] P. Ramesh, J.-M. Lueckmann, J. Boelts, \u00c1. Tejero-Cantero, D. S. Greenberg, P. J. Goncalves, and   \n413 J. H. Macke. GATSBI: Generative adversarial training for simulation-based inference. In International   \n414 Conference on Learning Representations, 2022.   \n415 [41] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. Improved techniques for   \n416 training gans. Advances in neural information processing systems, 29, 2016.   \n417 [42] Y. Song and S. Ermon. Generative modeling by estimating gradients of the data distribution. Advances in   \n418 neural information processing systems, 32, 2019.   \n419 [43] Y. Song and D. P. Kingma. How to train your energy-based models. arXiv preprint arXiv:2101.03288,   \n420 2021.   \n421 [44] A. Srivastava, L. Valkov, C. Russell, M. U. Gutmann, and C. Sutton. Veegan: Reducing mode collapse in   \n422 gans using implicit variational learning. Advances in neural information processing systems, 30, 2017. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "423 A Proofs ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "424 Theorem 1 (Necessary and sufficient conditions). Let v be a measure on $({\\mathcal{X}},{\\mathcal{A}})$ . Suppose that   \n425 $\\mathbb{P}_{F}$ and $\\mathbb{P}_{G}$ are probability measures on $({\\mathcal{X}},{\\mathcal{A}})$ with densities (with reference measure v) $f$ and $g$   \n426 respectively. Assuming that   \n427 $A.I\\ \\prod_{f}\\cup\\prod_{g}\\subset{\\mathcal{C}};$   \n428 A.2 $C_{\\mathbb{P}_{F},c}(\\alpha)$ and $C_{\\mathbb{P}_{G},c}(\\alpha)$ are uniquely determined6 in $\\mathcal{C}$ with respect to v   \n429 the following two statements are equivalent:   \n430 S.1 $\\mathbb{P}_{F}=\\mathbb{P}_{G}$ ; ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "431 S. $2\\,\\mathrm{\\it~D}_{\\mathrm{GKS}}\\left(\\mathbb{P}_{F},\\mathbb{P}_{G}\\right)=0.$ ", "page_idx": 11}, {"type": "text", "text": "432 Proof of Theorem $^{\\,l}$ . The $\\mathbf{S.1}\\implies\\mathbf{S.2}$ direction is trivial to show and works without satisfying the   \n433 assumptions [38]. Therefore, we focus on showing that $\\mathbf{S.2}\\implies\\mathbf{S.1}$ . Let ", "page_idx": 11}, {"type": "equation", "text": "$$\nS c(\\mathbb{P})=\\{(\\mathrm{v}(C),\\mathbb{P}(C)):C\\in{\\mathcal{C}}\\}\\subset I\\!R_{+}\\times[0,1],\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "434 and denote with $\\Gamma(\\lambda)$ the level set of density of $\\mathbb{P}$ as defined in eq. (6), and let $\\Pi:=\\{\\Gamma(\\lambda):\\lambda\\geqslant0\\}$ .   \n435 Further, let ${\\tilde{S}}_{\\mathcal{C}}$ denote the least concave majorant [5] to $S_{\\mathcal{C}}(\\mathbb{P})$ , that is, the smallest concave function   \n436 from $R_{+}$ to $[0,1]$ lying above $S_{\\mathcal{C}}(\\mathbb{P})$ . ${\\tilde{S}}_{\\mathcal{C}}$ is supported on the generalized quantiles of $\\mathbb{P}$ in $\\mathcal{C}$ , i.e. on   \n437 the points $(\\operatorname{v}(C_{\\mathbb{P},c}(\\alpha)),\\mathbb{P}(C_{\\mathbb{P},c}(\\alpha)))$ . Finally, let $\\partial\\tilde{S}_{\\mathcal{C}}(\\mathbb{P})$ be the intersection of the extremal points   \n438 of the convex hull of $S_{\\mathcal{C}}(\\mathbb{P})$ with the graph of ${\\tilde{S}}_{\\mathcal{C}}$ . Given $\\Pi\\subset{\\mathcal{C}}$ which we assume in A.1 for $\\mathbb{P}_{F}$   \n439 and $\\mathbb{P}_{G}$ , and in the light of remark 2 we have that for any set $C$ such that $(\\mathrm{v}(C),\\mathbb{P}(C))\\in\\partial\\tilde{S}_{\\mathcal{C}}(\\mathbb{P})$   \n440 there is a level $\\lambda$ for which $C=\\Gamma(\\lambda)$ , and it is equal the left-hand derivative of ${\\tilde{S}}_{\\mathcal{C}}$ in the point $\\mathrm{v}(C)$ .   \n441 From remark 1, we have that the silhouette fully characterizes $\\mathbb{P}$ , and therefore $\\partial\\tilde{S}_{\\mathcal{C}}(\\mathbb{P})$ does it as   \n442 well.   \n443 Eventually, we conclude the proof with the observation that given S.2, under Lemma 2.1 of Polonik   \n444 [38] (where A.2 is utilized) we have that the extremal points of the convex hulls of $S_{\\mathcal{C}}(\\mathbb{P}_{F})$ and   \n445 $S_{\\mathcal{C}}(\\mathbb{P}_{G})$ are the same points, thus $\\partial\\tilde{S}_{\\mathbb{P}_{F}}(\\mathbb{P})=\\partial\\tilde{S}_{\\mathbb{P}_{G}}(\\mathbb{P})$ , and finally $\\mathbb{P}_{F}=\\mathbb{P}_{G}$ . \u53e3   \n446 Theorem 2 (Relaxation of assumption A.1). Theorem 1 holds if assumption A.1 is relaxed to the   \n447 case that $\\mathcal{C}$ contains sets that are uniquely determined with density level sets of $\\mathbb{P}_{F}$ and $\\mathbb{P}_{G}$ up to $a$   \n448 set $C$ such that ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "equation", "text": "$$\n\\forall_{C^{\\prime}\\in2^{C}}\\mathbb{P}_{F}(C^{\\prime})=\\mathbb{P}_{G}(C^{\\prime}),\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "449 and let $r:=\\mathbb{P}_{F}(C)=\\mathbb{P}_{G}(C),$ , then the supremum in statement S.2 is restricted to $\\left[0,1-r\\right]$ . ", "page_idx": 11}, {"type": "text", "text": "450 Proof of Theorem 2. The statement in eq. (8) is equivalent to saying that $\\mathbb{P}_{F}\\,=\\,\\mathbb{P}_{G}$ on $(C,2^{C})$ .   \n451 Analogously to the proof of theorem 1 we can show that $\\mathbb{P}_{F}=\\mathbb{P}_{G}$ on $(\\mathcal{X}\\setminus C,2^{\\mathcal{X}\\setminus C})$ . By observing   \n452 that probability measures are $\\sigma$ -additive, we conclude that $\\mathbb{P}_{F}=\\mathbb{P}_{G}$ on $({\\mathcal{X}},{\\mathcal{A}})$ , and thus the result   \n453 of theorem 1 holds. \u53e3 ", "page_idx": 11}, {"type": "text", "text": "455 Let us consider three probability measures $\\mathbb{P}_{F}$ , $\\mathbb{P}_{G}$ , and $\\mathbb{P}_{H}$ on a measurable space $({\\mathcal{X}},{\\mathcal{A}})$ . ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\underset{{\\mathcal{C}}\\in\\{C_{P r,c}^{(\\alpha)}\\},\\ldots}{\\operatorname*{sup}}\\ \\quad||\\mathbf{P}_{F}(C(\\alpha))-\\mathbf{P}_{H}(C(\\alpha))||+D_{\\Omega\\in\\mathbb{N}}\\ (\\mathbf{P}_{H},\\mathbf{P}_{G})}\\\\ &{\\qquad\\underset{{\\mathcal{C}}\\in\\{C_{P r,c}^{(\\alpha)},\\ldots,\\mathcal{C}_{P r,c}\\}}{\\operatorname*{sup}}\\ \\quad||\\mathbf{P}_{F}(C(\\alpha))-\\mathbf{P}_{H}(C(\\alpha))||+\\qquad\\underset{{\\mathcal{C}}\\in\\{C_{P r,c}^{(\\alpha)},\\ldots,\\mathcal{C}_{P r,c}\\}}{\\operatorname*{sup}}\\ \\quad||\\mathbf{P}_{H}(C(\\alpha))-\\mathbf{P}_{G}(C(\\alpha))||}\\\\ &{\\qquad\\qquad\\underset{{\\mathcal{C}}\\in\\{C_{P r,c}^{(\\alpha)},\\ldots,\\mathcal{C}_{P r,c}\\}}{\\operatorname*{sup}}\\ \\quad||\\mathbf{P}_{F}(C(\\alpha))-\\mathbf{P}_{H}(C(\\alpha))||+\\qquad\\underset{{\\mathcal{C}}\\in\\{C_{P r,c}^{(\\alpha)},\\ldots,\\mathcal{C}_{P r,c}\\}}{\\operatorname*{sup}}\\ \\quad||\\mathbf{P}_{H}(C(\\alpha))-\\mathbf{P}_{G}(C(\\alpha))|}\\\\ &{\\qquad\\underset{{\\mathcal{C}}\\in\\{C_{P r,c}^{(\\alpha)},\\ldots,\\mathcal{C}_{P r,c}\\}}{\\operatorname*{sup}}\\ \\quad||\\mathbf{P}_{F}(C(\\alpha))-\\mathbf{P}_{H}(C(\\alpha))||+\\vert\\mathbf{P}_{H}(C(\\alpha))-\\mathbf{P}_{G}(C(\\alpha))\\vert|}\\\\ &{\\qquad\\underset{{\\mathcal{C}}\\in\\{C_{P r,c}^{(\\alpha)},\\ldots,\\mathcal{C}_{P r,c}\\}}{\\operatorname*{sup}}\\ \\quad||\\mathbf{P}_{F}(C(\\alpha))-\\mathbf{P}_{H}(C(\\alpha))||+\\vert\\mathbf{P}_{H}(C(\\alpha))-\\mathbf{P}_{G}(C(\\alpha))\\vert\\vert}\\\\ &{\\qquad\\qquad\\underset{{\\mathcal{C}}\\in\\{C_{P r,c}^{(\\alpha)},\\ldots,\\mathcal{C}_{P r,c}\\}}{\\operatorname*{sup}}\\ \\quad\\ \\ \\ }\\\\ &{\\qquad\\qquad\\underset{{\\mathcal{C \n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "456 In (i), we use the fact that the supremum of absolute difference in distribution coverage is maximized   \n457 with the generalized quantile function of one of them. In (ii), we apply triangle inequality for absolute   \n458 value. Thus we have shown that $D_{\\mathrm{GKS}}\\left(\\mathbb{P}_{F},\\mathbb{P}_{H}\\right)+D_{\\mathrm{GKS}}\\left(\\mathbb{P}_{H},\\dot{\\mathbb{P}}_{G}\\right)\\geqslant\\bar{D}_{\\mathrm{GKS}}\\left(\\mathbb{P}_{F},\\mathbb{P}_{G}\\right)$ which is   \n459 the triangle inequality for the Generalized KS distance. ", "page_idx": 12}, {"type": "text", "text": "460 A.2 Objective for the critic ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "461 Given two adversarial maximum likelihood objectives from Kim and Bengio [23], we (i) set   \n462 $c_{\\phi_{G}}(x):=-c_{\\phi_{F}}(x)$ , and (ii) repurpose $\\mathbb{P}_{G}$ as $\\mathbb{P}_{a u x(F)}$ and $\\mathbb{P}_{F}$ as $\\mathbb{P}_{a u x(G)}$ , and show that: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac12(\\mathbb{E}_{x\\sim\\mathbb{P}_{F}}[-c_{\\phi_{F}}(x)]-\\mathbb{E}_{x\\sim\\mathbb{P}_{a u x(F)}}[-c_{\\phi_{F}}(x)])+\\frac12(\\mathbb{E}_{x\\sim\\mathbb{P}_{G}}[-c_{\\phi_{G}}(x)]-\\mathbb{E}_{x\\sim\\mathbb{P}_{a u x(G)}}[-c_{\\phi_{G}}(x)])}\\\\ &{\\quad=\\frac12(\\mathbb{E}_{x\\sim\\mathbb{P}_{F}}[-c_{\\phi}(x)]-\\mathbb{E}_{x\\sim\\mathbb{P}_{G}}[-c_{\\phi}(x)]+\\mathbb{E}_{x\\sim\\mathbb{P}_{G}}[c_{\\phi}(x)]-\\mathbb{E}_{x\\sim\\mathbb{P}_{F}}[c_{\\phi}(x)])}\\\\ &{\\quad=\\mathbb{E}_{x\\sim\\mathbb{P}_{G}}[c_{\\phi}(x)]-\\mathbb{E}_{x\\sim\\mathbb{P}_{F}}[c_{\\phi}(x)].}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "463 B Experiments details ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "464 In this section, we provide additional details about experiments conducted in the paper that did not   \n465 fit in the main text. All the models reported in the paper were trained under 12 hours on a single   \n466 Nvidia GeForce GTX TITAN X GPU (12GB vRAM) with 32GB of RAM and 2 CPU cores. We   \n467 report results based on 645 models trained, which amounts to 7740 GPU hours at most. We estimate   \n468 that about three times as much computing time was used for preliminary experiments not reported in   \n469 the paper. ", "page_idx": 12}, {"type": "text", "text": "470 B.1 Synthetic ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "471 The synthetic 2D distributions are adopted from the official code of Grathwohl et al. [14] \u2013 https:   \n472 //github.com/rtqichen/ffjord. We randomly generate 65536 training and 65536 test instances   \n473 from each distribution. In appendix D.1, we report the results of training the models with fewer   \n474 instances but evaluated using the entire test set.   \n475 We choose the bandwidth of the Gaussian filter in squared population MMD as the median L2   \n476 distance between two samples, of 32768 instances each, from the simulator. The resulting values can   \n477 be found in the code we provide with the paper. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "table", "img_path": "", "table_caption": ["Table 4: Architectures for synthetic 2D datasets. "], "table_footnote": [], "page_idx": 13}, {"type": "table", "img_path": "8JmUmTgKiY/tmp/073a82a71ef2c6d821ee570a82c2f5921b6090de063138f6ccaa38fd70536cf6.jpg", "table_caption": ["(b) Critic "], "table_footnote": ["(a) Generator "], "page_idx": 13}, {"type": "text", "text": "478 B.2 MNIST ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "479 To detect the modes in the (3Stacked)MNIST experiments, we use a pre-trained classifier from   \n480 PyTorch examples, trained for 14 epochs of the train set of the original MNIST dataset. We expect   \n481 to find 10 and 1000 modes for the MNIST and 3StackedMNIST, respectively. We measure the KL   \n482 divergence between the classifier\u2019s output and discrete uniform distribution for both distributions. ", "page_idx": 13}, {"type": "text", "text": "483 B.3 CIFAR-10 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "484 We compute the Inception Score using the implementation from https://github.com/sbarratt/   \n485 inception-score-pytorch. We compute the Fr\u00e9chet inception distance using the implementation   \n486 from https://github.com/mseitzer/pytorch-fid. ", "page_idx": 13}, {"type": "text", "text": "487 C Architectures and hyper-parameters ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "488 C.1 Synthetic ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "489 For all of the methods and distributions, we use the same architecture, described in table 4, with   \n490 spectral normalization [31] on linear layers for GAN. In all cases, we train the generator and critic   \n491 with Adam( $\\beta_{1}\\,=\\,0.5$ , $\\beta_{2}\\,=\\,0.9)$ ) optimizer with a constant learning rate of 0.0001, without L2   \n492 regularization or weight decay, for 128000 generator updates with batch size equal to 512. We use the   \n493 standard loss for GAN, enforcing class 1 for real samples and 0 for generated samples. In WGAN-GP,   \n494 we use 0.1 weight on gradient penalty (identified as a good value in preliminary experiments, which   \n495 we do not report), and in KSGAN $\\beta=1.0$ as the weight for score penalty. ", "page_idx": 13}, {"type": "text", "text": "496 C.2 MNIST ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "497 For the MNIST experiments, we use the DCGAN [39] architecture, without batch normalization   \n498 layers, with 128-dimensional latent Gaussian distribution. For the 3StackedMNIST distribution, we   \n499 increase the number of input and output channels for the critic and generator, respectively. We train   \n500 the generator and critic with Adam( $\\beta_{1}=0.5$ , $\\beta_{2}=0.9$ ) optimizer with a constant learning rate of   \n501 0.0001, without L2 regularization or weight decay, for 200000 generator updates with batch size   \n502 equal to 50. In the case of GAN for 3StackedMNIST, we use a learning rate of 0.001 (identified as a   \n503 good value in preliminary experiments, which we do not report). We use the   \n504 filpped loss for GAN, enforcing class 0 for real samples and 1 for generated samples. In WGAN-GP,   \n505 we use 10.0 weight on gradient penalty (identified as a good value in preliminary experiments, which   \n506 we do not report), and in KSGAN $\\beta=1.0$ as the weight for score penalty. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "image", "img_path": "8JmUmTgKiY/tmp/469aea1f45c56c93b0e2af6e192cca51944fbace9f3c28db2fa0d3b6efd505a2.jpg", "img_caption": ["Figure 2: Squared population MMD between approximate and test distribution as a function of the number of training instances. Solid lines denote the average over five random initializations, and the shaded area represents the two- $\\cdot\\sigma$ interval. Best viewed in color. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "507 C.3 CIFAR-10 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "508 For the CIFAR-10 experiments, we use ResNet architecture from Gulrajani et al. [16]. We train   \n509 the generator and critic with $\\mathrm{Adam}(\\beta_{1}=0.0,\\,\\beta_{2}=0.9)$ ) optimizer with a constant learning rate of   \n510 0.0001, without L2 regularization or weight decay, for 199936 generator updates with batch size   \n511 equal to 64. We use the   \n512 filpped loss for GAN, enforcing class 0 for real samples and 1 for generated samples. In WGAN-GP,   \n513 we use 10.0 weight on gradient penalty (identified as a good value in preliminary experiments, which   \n514 we do not report), and in KSGAN $\\beta=1.0$ as the weight for score penalty. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "515 D Extended results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "516 In this section, we report additional experiment results that did not fti in the main text. This includes   \n517 materials allowing a qualitative comparison of the trained models. ", "page_idx": 14}, {"type": "text", "text": "518 D.1 Synthetic data ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "519 In fig. 2, we report, extended relative to table 2 in the main text, a study of the quality of trained   \n520 models as measured by the squared population MMD. Solid lines denote the average over five   \n521 random initializations, and the shaded area represents the two- $\\sigma$ interval. KSGAN performs on par   \n522 with WGAN-GP while being trained with a five times less training budget. In fig. 3, we show the   \n523 histograms of 65536 samples from the models (a single random initialization), with a histogram of   \n524 test data in the first column for reference. For KSGAN, in addition to the configurations included in   \n525 table 2, we include one with a training budget matching that of GAN and WGAN-GP, and one with a   \n526 training budget reduced by two, where the critic is updated only every second update of the generator. ", "page_idx": 14}, {"type": "text", "text": "527 D.2 MNIST ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "528 In fig. 4, we show samples from one of the random initializations reported in table 2 in the main text.   \n529 All models demonstrate similar sample quality, while for GAN, the digit \u201c1\u201d is over-represented,   \n530 which corresponds with the high KL in table 2. ", "page_idx": 14}, {"type": "image", "img_path": "8JmUmTgKiY/tmp/5b4fc08ead2dc981873d2591721f134d3794dfa0d773ad706866d0d80b379075.jpg", "img_caption": ["Figure 3: Histograms of samples from distributions denoted on the top. Heatmap colors are shared for all figures in each row. Best viewed in color. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "8JmUmTgKiY/tmp/d234121f841bb89ac65ceff935046163c592f53c767c0efb1ba9f1f48bad4aa2.jpg", "img_caption": ["(a) GAN (1, 1) "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "8JmUmTgKiY/tmp/0c19129934f7facd0b07cbdc1c42c8041f01c4c9c9c09e8b221d55f715114994.jpg", "img_caption": ["(b) WGAN-GP (1, 1) "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "8JmUmTgKiY/tmp/4aef5c3e945ef1e1038c6865e307792f30697d1183a9301d3d37042535f76433.jpg", "img_caption": ["(c) KSGAN (1, 1) "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 4: Samples from the respective models trained on the MNIST dataset. ", "page_idx": 16}, {"type": "image", "img_path": "8JmUmTgKiY/tmp/925a727c6748deedccb9fcbfb92e26cbe8f3758d6aa9dc0465cf2320ca546234.jpg", "img_caption": ["Figure 5: Samples from the respective models trained on the CIFAR-10 dataset. Best viewed in color. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "531 D.3 CIFAR-10 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "532 In fig. 5, we show samples from one of the random initializations reported in table 3 in the main text.   \n533 All models demonstrate similar, low sample quality. ", "page_idx": 17}, {"type": "text", "text": "534 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "535 1. Claims   \n536 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n537 paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "39 Justification: We provide a formal derivation of the proposed method, with all the steps 40 described and justified. Claims about the empirical behavior of the proposed method are supported by the results of experiments reported in the main text and appendix. ", "page_idx": 18}, {"type": "text", "text": "43 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n4 made in the paper.   \n45 \u2022 The abstract and/or introduction should clearly state the claims made, including   \n46 the contributions made in the paper and important assumptions and limitations. A   \n47 No or NA answer to this question will not be perceived well by the reviewers.   \n48 \u2022 The claims made should match theoretical and experimental results, and reflect   \n49 how much the results can be expected to generalize to other settings.   \n50 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these   \n1 goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 18}, {"type": "text", "text": "Justification: We explicitly specify all the assumptions made regarding the theoretical part. In the main text, we admit that the empirical evaluation does not explore all the properties of the proposed method. We propose further lines of work that we consider promising based on our experience with the method. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency ", "page_idx": 18}, {"type": "text", "text": "584 play an important role in developing norms that preserve the integrity of the   \n585 community. Reviewers will be specifically instructed to not penalize honesty   \n586 concerning limitations.   \n587 3. Theory Assumptions and Proofs   \n588 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n589 a complete (and correct) proof?   \n590 Answer: [Yes]   \n591 Justification: All assumptions are explicitly mentioned, we believe our proofs are correct.   \n592 In addition, in the theoretical part of our work, we rely on previously published results by   \n593 other authors, which we always cite as a reference.   \n594 Guidelines:   \n595 \u2022 The answer NA means that the paper does not include theoretical results.   \n596 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and   \n597 cross-referenced.   \n598 \u2022 All assumptions should be clearly stated or referenced in the statement of any   \n599 theorems.   \n600 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n601 they appear in the supplemental material, the authors are encouraged to provide a   \n602 short proof sketch to provide intuition.   \n603 \u2022 Inversely, any informal proof provided in the core of the paper should be comple  \n604 mented by formal proofs provided in appendix or supplemental material.   \n605 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.   \n606 4. Experimental Result Reproducibility   \n607 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n608 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n609 of the paper (regardless of whether the code and data are provided or not)?   \n610 Answer: [Yes]   \n611 Justification: Experiments (including the evaluation protocol) are described in detail in the   \n612 main text and completed with more information in the appendix. In addition, we include a   \n613 link to the repository containing the code that was used to conduct the experiments.   \n614 Guidelines:   \n615 \u2022 The answer NA means that the paper does not include experiments.   \n616 \u2022 If the paper includes experiments, a No answer to this question will not be perceived   \n617 well by the reviewers: Making the paper reproducible is important, regardless of   \n618 whether the code and data are provided or not.   \n619 \u2022 If the contribution is a dataset and/or model, the authors should describe the steps   \n620 taken to make their results reproducible or verifiable.   \n621 \u2022 Depending on the contribution, reproducibility can be accomplished in various   \n622 ways. For example, if the contribution is a novel architecture, describing the   \n623 architecture fully might suffice, or if the contribution is a specific model and   \n624 empirical evaluation, it may be necessary to either make it possible for others   \n625 to replicate the model with the same dataset, or provide access to the model. In   \n626 general. releasing code and data is often one good way to accomplish this, but   \n627 reproducibility can also be provided via detailed instructions for how to replicate   \n628 the results, access to a hosted model (e.g., in the case of a large language model),   \n629 releasing of a model checkpoint, or other means that are appropriate to the research   \n630 performed.   \n631 \u2022 While NeurIPS does not require releasing code, the conference does require all   \n632 submissions to provide some reasonable avenue for reproducibility, which may   \n633 depend on the nature of the contribution. For example   \n634 (a) If the contribution is primarily a new algorithm, the paper should make it   \n635 clear how to reproduce that algorithm.   \n636 (b) If the contribution is primarily a new model architecture, the paper should   \n637 describe the architecture clearly and fully.   \n638 (c) If the contribution is a new model (e.g., a large language model), then   \n639 there should either be a way to access this model for reproducing the   \n640 results or a way to reproduce the model (e.g., with an open-source dataset   \n641 or instructions for how to construct the dataset).   \n642 (d) We recognize that reproducibility may be tricky in some cases, in which   \n643 case authors are welcome to describe the particular way they provide   \n644 for reproducibility. In the case of closed-source models, it may be that   \n645 access to the model is limited in some way (e.g., to registered users), but it   \n646 should be possible for other researchers to have some path to reproducing   \n647 or verifying the results.   \n648 5. Open access to data and code   \n649 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n650 tions to faithfully reproduce the main experimental results, as described in supplemental   \n651 material?   \n652 Answer: [Yes]   \n653 Justification: We include a link to the repository containing the code that was used to conduct   \n654 the experiments. We use only publicly available data.   \n655 Guidelines:   \n656 \u2022 The answer NA means that paper does not include experiments requiring code.   \n657 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n658 public/guides/CodeSubmissionPolicy) for more details.   \n659 \u2022 While we encourage the release of code and data, we understand that this might   \n660 not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply   \n661 for not including code, unless this is central to the contribution (e.g., for a new   \n662 open-source benchmark).   \n663 \u2022 The instructions should contain the exact command and environment needed to   \n664 run to reproduce the results. See the NeurIPS code and data submission guide  \n665 lines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more   \n666 details.   \n667 \u2022 The authors should provide instructions on data access and preparation, including   \n668 how to access the raw data, preprocessed data, intermediate data, and generated   \n669 data, etc.   \n670 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n671 proposed method and baselines. If only a subset of experiments are reproducible,   \n672 they should state which ones are omitted from the script and why.   \n673 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n674 versions (if applicable).   \n675 \u2022 Providing as much information as possible in supplemental material (appended to   \n676 the paper) is recommended, but including URLs to data and code is permitted.   \n677 6. Experimental Setting/Details   \n678 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n679 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n680 results?   \n681 Answer: [Yes]   \n682 Justification: Experiments (including the evaluation protocol) are described in detail in the   \n683 main text and completed with more information in the appendix. In addition, we include a   \n684 link to the repository containing the code that was used to conduct the experiments.   \n686 \u2022 The answer NA means that the paper does not include experiments.   \n687 \u2022 The experimental setting should be presented in the core of the paper to a level of   \n688 detail that is necessary to appreciate the results and make sense of them.   \n689 \u2022 The full details can be provided either with the code, in appendix, or as supplemen  \n690 tal material. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "691 7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "692 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n693 information about the statistical significance of the experiments?   \n94 Answer: [Yes]   \n95 Justification: All the evaluation metrics were computed on five random initializations. We   \n96 report the average scores with standard deviation computed with Bessel\u2019s correction.   \n97 Guidelines: ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "719 8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: In the appendix, we include information about the resources needed to reproduce results reported in the paper, and give an estimate of resources spent on preliminary experiments not reported in the paper. ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. \u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. ", "page_idx": 21}, {"type": "text", "text": "733 \u2022 The paper should disclose whether the full research project required more compute   \n734 than the experiments reported in the paper (e.g., preliminary or failed experiments   \n735 that didn\u2019t make it into the paper).   \n736 9. Code Of Ethics   \n737 Question: Does the research conducted in the paper conform, in every respect, with the   \n738 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n739 Answer: [Yes]   \n740 Justification: None of the datasets used in the paper has been deprecated. We do not identify   \n741 any concerns regarding societal impact and potential harmful consequences of our work.   \n742 Guidelines:   \n743 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of   \n744 Ethics.   \n745 \u2022 If the authors answer No, they should explain the special circumstances that require   \n746 a deviation from the Code of Ethics.   \n747 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special   \n748 consideration due to laws or regulations in their jurisdiction).   \n749 10. Broader Impacts   \n750 Question: Does the paper discuss both potential positive societal impacts and negative   \n751 societal impacts of the work performed?   \n752 Answer: [NA]   \n753 Justification: Our work is foundational research, and thus does not have a direct positive or   \n754 negative societal impact. We disclaim responsibility for the malicious use of our work.   \n755 Guidelines:   \n756 \u2022 The answer NA means that there is no societal impact of the work performed.   \n757 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n758 impact or why the paper does not address societal impact.   \n759 \u2022 Examples of negative societal impacts include potential malicious or unintended   \n760 uses (e.g., disinformation, generating fake proflies, surveillance), fairness consider  \n761 ations (e.g., deployment of technologies that could make decisions that unfairly   \n762 impact specific groups), privacy considerations, and security considerations.   \n763 \u2022 The conference expects that many papers will be foundational research and not   \n764 tied to particular applications, let alone deployments. However, if there is a direct   \n765 path to any negative applications, the authors should point it out. For example, it   \n766 is legitimate to point out that an improvement in the quality of generative models   \n767 could be used to generate deepfakes for disinformation. On the other hand, it is not   \n768 needed to point out that a generic algorithm for optimizing neural networks could   \n769 enable people to train models that generate Deepfakes faster.   \n770 \u2022 The authors should consider possible harms that could arise when the technology   \n771 is being used as intended and functioning correctly, harms that could arise when   \n772 the technology is being used as intended but gives incorrect results, and harms   \n773 following from (intentional or unintentional) misuse of the technology.   \n774 \u2022 If there are negative societal impacts, the authors could also discuss possible   \n775 mitigation strategies (e.g., gated release of models, providing defenses in addition   \n776 to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a   \n777 system learns from feedback over time, improving the efficiency and accessibility   \n778 of ML).   \n779 11. Safeguards   \n780 Question: Does the paper describe safeguards that have been put in place for responsible   \n781 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n782 image generators, or scraped datasets)?   \n784 Justification: The paper poses no such risks.   \n785 Guidelines:   \n786 \u2022 The answer NA means that the paper poses no such risks.   \n787 \u2022 Released models that have a high risk for misuse or dual-use should be released   \n788 with necessary safeguards to allow for controlled use of the model, for example by   \n789 requiring that users adhere to usage guidelines or restrictions to access the model   \n790 or implementing safety filters.   \n791 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The   \n792 authors should describe how they avoided releasing unsafe images.   \n793 \u2022 We recognize that providing effective safeguards is challenging, and many papers   \n794 do not require this, but we encourage authors to take this into account and make a   \n795 best faith effort. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "796 12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "797 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n798 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n799 properly respected?   \n00 Answer: [Yes]   \n01 Justification: All the creators of assets are properly credited in the paper and the code.   \n02 Guidelines:   \n803 \u2022 The answer NA means that the paper does not use existing assets.   \n804 \u2022 The authors should cite the original paper that produced the code package or   \n05 dataset.   \n806 \u2022 The authors should state which version of the asset is used and, if possible, include   \n807 a URL.   \n808 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n809 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms   \n810 of service of that source should be provided.   \n11 \u2022 If assets are released, the license, copyright information, and terms of use in   \n12 the package should be provided. For popular datasets, paperswithcode.com/   \n813 datasets has curated licenses for some datasets. Their licensing guide can help   \n14 determine the license of a dataset.   \n15 \u2022 For existing datasets that are re-packaged, both the original license and the license   \n816 of the derived asset (if it has changed) should be provided.   \n817 \u2022 If this information is not available online, the authors are encouraged to reach out   \n18 to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "819 13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "820 Question: Are new assets introduced in the paper well documented and is the documentation   \n821 provided alongside the assets?   \n822 Answer: [NA]   \n823 Justification: The paper does not release new assets.   \n824 Guidelines: ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "833 14. Crowdsourcing and Research with Human Subjects   \n834 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n835 include the full text of instructions given to participants and screenshots, if applicable, as   \n836 well as details about compensation (if any)?   \n837 Answer: [NA]   \n838 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n839 Guidelines:   \n840 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research   \n841 with human subjects.   \n842 \u2022 Including this information in the supplemental material is fine, but if the main   \n843 contribution of the paper involves human subjects, then as much detail as possible   \n844 should be included in the main paper.   \n845 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection,   \n846 curation, or other labor should be paid at least the minimum wage in the country of   \n847 the data collector.   \n848 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n849 Subjects   \n850 Question: Does the paper describe potential risks incurred by study participants, whether   \n851 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n852 approvals (or an equivalent approval/review based on the requirements of your country or   \n853 institution) were obtained?   \n854 Answer: [NA]   \n855 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n856 Guidelines:   \n857 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research   \n858 with human subjects.   \n859 \u2022 Depending on the country in which research is conducted, IRB approval (or equiv  \n860 alent) may be required for any human subjects research. If you obtained IRB   \n861 approval, you should clearly state this in the paper.   \n862 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n863 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and   \n864 the guidelines for their institution.   \n865 \u2022 For initial submissions, do not include any information that would break anonymity   \n866 (if applicable), such as the institution conducting the review. ", "page_idx": 24}]