[{"type": "text", "text": "The Sample Complexity of Gradient Descent in Stochastic Convex Optimization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Roi Livni School of Electrical Engineering Tel Aviv University rlivni@tauex.tau.ac.il ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We analyze the sample complexity of full-batch Gradient Descent (GD) in the setup of non-smooth Stochastic Convex Optimization. We show that the generalization error of GD, with common choice of hyper-parameters, can be $\\tilde{\\Theta}(\\overline{{d}}/m+1/\\sqrt{m})$ , where $d$ is the dimension and $m$ is the sample size. This matches the sample complexity of worst-case empirical risk minimizers. That means that, in contrast with other algorithms, GD has no advantage over naive ERMs. Our bound follows from a new generalization bound that depends on both the dimension as well as the learning rate and number of iterations. Our bound also shows that, for general hyper-parameters, when the dimension is strictly larger than number of samples, $\\dot{T_{\\,}}\\dot{=}\\,\\Omega\\dot{(1/\\varepsilon^{4})}$ iterations are necessary to avoid overfitting. This resolves an open problem by Amir, Koren, and Livni [3], Schliserman, Sherman, and Koren [20], and improves over previous lower bounds that demonstrated that the sample size must be at least square root of the dimension. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Stochastic Convex Optimization (SCO) is a theoretical model that depicts a learner that minimizes a (Lipschitz) convex function, given finite noisy observations of the objective [22]. While often considered simplistic, in recent years SCO has become a focus of theoretical research, partly, because of its importance to the study of first-order optimization methods. But, also, it has become focus of study because it is one of few theoretical settings that exhibit overparameterized learning . In more detail, classical learning theory often focuses on the tension between number of samples, or training data, and the complexity of the model to be learnt. A common wisdom of classical theories [1, 7, 14, 24] is that, to avoid overftiting, the complexity of a model should be adjusted in proportion to the amount of training data. However, recent advances in Machine Learning have challenged this viewpoint. Evidently [18, 25], state-of-the-art algorithms generalize well but without, explicitly, controlling the capacity of the model to be learnt. In turn, today, it is one of the most emerging challenges, for learning theory, to understand learnability when the number of parameters in a learnt model exceeds the number of examples, and when, seemingly, nothing withholds the algorithm from overfitting. ", "page_idx": 0}, {"type": "text", "text": "Towards this, we look at SCO. In SCO, Shalev-Shwartz, Shamir, Srebro, and Sridharan [22] showed how algorithms can overfit with dimension dependent sample size. But, at the same time, it was known [8, 26] that there are algorithms that provably avoid overfitting with far fewer examples than dimensions. As such, SCO became a canonical model to study how a well-designed algorithm can avoid overfitting even when the number of examples is too small to guarantee generalization by an algorithmic-independent argument [2\u20135, 11, 12, 16, 20\u201322]. A step towards understanding what induces generalization is to identify which algorithms generalize. Then, we can ask what yields the separation. Surprisingly, for many well-studied algorithms this question is not always answered. ", "page_idx": 0}, {"type": "text", "text": "Perhaps the simplest algorithm, whose sample complexity is not yet understood, is Gradient Descent (GD). And we turn to the basic question of the sample complexity of gradient descent. ", "page_idx": 1}, {"type": "text", "text": "While this question remained open, there have been several advancements and intermediate answers: The first, dimension independent, generalization bound was given by Bassily, Feldman, Guzm\u00e1n, and Talwar [6] that provided stability bounds [8]. The result of Bassily et al. demonstrated that, GD can have dimension-independent sample complexity rate. However, to achieve that, one has to use non-standard choice of hyperparameters which affects the efifciency of the algorithm. In particular, the number of rounds becomes quadratic in the size of the sample (as opposed to linear, with standard choice). On the other hand, a classical covering argument shows that linear dependence in the dimension is the worst possible, for any empirical risk minimizer, irrespective of properties such as stability. ", "page_idx": 1}, {"type": "text", "text": "In terms of lower bounds, Amir, Koren, and Livni [3] were the first to show that GD may have a dimension dependence in the sample complexity. They showed that, with natural hyperparameters, the algorithm must observe number of samples that is at least logarithmic in the dimension. This result was recently improved by Schliserman, Sherman, and Koren [20] that showed that at least square root of the dimension is required. Taken together, so far it was shown that either the algorithm\u2019s hyperparameters are tuned to achieve stability, at a cost in running time, or the algorithm must suffer some dimension dependence, linear at worst square root at best. ", "page_idx": 1}, {"type": "text", "text": "Here, we close the gap and show that linear dependence is necessary. Informally, we provide the following generalization error bound, in terms of dimension $d$ , sample size, $m$ , and hyperparameters of the algorithm, $\\eta$ and $T$ (the learning rate and number of iterations). We show that when $T$ is at most cubic in the dimension (see Theorem 1 for a formal statement): ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname{Generalization}\\operatorname{gap}\\operatorname{of}\\operatorname{GD}=\\Omega\\left(\\operatorname*{min}\\left\\{{\\frac{d}{m}},1\\right\\}\\cdot\\operatorname*{min}\\left\\{\\eta{\\sqrt{T}},1\\right\\}\\right).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "The first factor in the RHS describes the linear dependence of the generalization error in the dimension, and corresponds to the optimal sample complexity of empirical risk minimizers, as demonstrated by Carmon, Livni, and Yehudayof f[11]. The second term lower bounds the stability of the algorithm [6], and played a similar role in previous bounds [3, 20]. Each factor is optimal\u221a at a certain regime, and cannot be improved. Most importantly, for a standard choice of $\\eta=O(1/{\\sqrt{T}})$ , the first term is dominant, and the aformentioned lower bound is complemented with t\u221ahe upper bound of Carmon et al. [11]. Our result implies, then, a sample complexity of $\\tilde{\\Theta}(d/m+1/\\sqrt{m})$ . When $d\\geq m$ , the second factor is dominant. When running time is at most quadratic in number of examples, this term also governs the stability of the algorithm, hence the result of Bassily et al. [6] provides a complementary upper bound (see further discussion in Section 3.1). ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We consider the standard setup of Stochastic Convex Optimization (SCO) as in [22]. Set $\\mathcal{W}=\\left\\{w:\\right.$ $\\|w\\|\\leq1\\}$ , and let $\\mathcal{Z}$ be an arbitrary, finite domain (our main result is a lower bound, hence finiteness of $\\mathcal{Z}$ is without loss of generality). We assume that there exists a function $f(w,z)$ that is convex and $L$ -Lipschitz in $w\\in\\mathcal{W}$ for every choice of $z\\in{\\mathcal{Z}}$ . Recall that a function $f$ is convex and $L$ -Lipschitz if for any $w_{1},w_{2}\\in\\mathcal{W}$ and $0\\leq\\lambda\\leq1$ : ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f\\bigl(\\lambda w_{1}+(1-\\lambda)w_{2}\\bigr)\\leq\\lambda f(w_{1})+(1-\\lambda)f(w_{2}),\\ \\mathrm{and},\\ \\vert f(w_{1})-f(w_{2})\\vert\\leq L\\Vert w_{1}-w_{2}\\Vert.}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "First order optimization Algorithmically we require further assumptions concerning any interaction with the function to be optimized. Recall [19] that, for fixed $z$ , the sub-gradient set of $f(w,z)$ at point $w$ is the set: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\partial f(w,z)=\\left\\{g:f(w^{\\prime},z)\\geq f(w,z)+g^{\\top}(w^{\\prime}-w),\\forall w^{\\prime}\\in\\mathcal{W}\\right\\}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "A first order oracle for $f$ is a mapping ${\\mathcal{O}}_{z}(w)$ such that $\\mathcal{O}_{z}(w)\\in\\partial f(w,z)$ . Our underlying assumption is that a learner has a first order oracle access. In other words, given a function $f(w,z)$ , we will assume that there is a procedure $\\mathcal{O}_{z}$ that calculates and returns a subgradient at every $w$ for every $z$ . Recall [10, 19] that when $|\\partial f(w,z)|=1$ , the function is differentiable, at $w$ , and in that case, the unique subdifferential is the gradient $\\nabla f(w,z)$ . ", "page_idx": 1}, {"type": "text", "text": "Learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "A learning algorithm $A$ , in SCO, is any algorithm that receives as input a sample $S=\\{z_{1},\\ldots,z_{m}\\}\\in{\\mathcal{Z}}^{m}$ of $m$ examples, and outputs a parameter $w_{S}$ . An underlying assumption in learning is that there exists a distribution $D$ , unknown to the learner $A$ , and that the sample $S$ is drawn i.i.d from $D$ . The goal of the learner is to minimize the population loss: ", "page_idx": 2}, {"type": "equation", "text": "$$\nF(w)=\\underset{z\\sim\\cal D}{\\mathbb{E}}[f(w,z)],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "More concretely, We will say that the learner has sample complexity $m(\\varepsilon)$ if, assuming $|S|\\geq m(\\varepsilon)$ , then w.p. $1/2$ (Again, because we mostly care about lower bounds, fixing the confidence will not affect the generality of our result): ", "page_idx": 2}, {"type": "equation", "text": "$$\nF(w_{S})-\\operatorname*{min}_{w\\in\\mathcal{W}}F(w)\\leq\\varepsilon.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Empirical Risk Minimization A natural approach to perform learning is by Empirical Risk Minimization (ERM). Given a sample $S$ , the empirical risk is defined to be: ", "page_idx": 2}, {"type": "equation", "text": "$$\nF_{S}(w)=\\frac{1}{|S|}\\sum_{z\\in S}f(w,z).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "An $\\varepsilon$ -ERM is any algorithm that, given sample $S$ , returns a solution $w_{S}\\,\\in\\,W$ that minimizes the empirical risk up to additive error $\\varepsilon\\,>\\,0$ . Recently, Carmon et al. [11] showed that any $\\varepsilon$ -ERM algorithm has a sample complexity bound of ", "page_idx": 2}, {"type": "equation", "text": "$$\nm(\\varepsilon)=\\tilde{O}\\left(\\frac{d}{\\varepsilon}+\\frac{1}{\\varepsilon^{2}}\\right),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The above rate is optimal up to logarithmic factor [12]. Namely, there exists a construction and an ERM that will fail, w.p. $1/2$ , unless $m=\\Omega(d/\\varepsilon)$ examples are provided1. Importantly, though, there are algorithms that can learn with much smaller sample complexity. In particular SGD [26], stable-GD [6] and regularized ERMs [8]. ", "page_idx": 2}, {"type": "text", "text": "Gradient Descent ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We next depict Gradient Descent whose sample complexity is the focus of this work. GD depends on hyperparameters $T\\in\\mathbb N$ and $\\eta\\geq0$ and operates as follows on the empirical risk. The algorithm receives as input a sample $S=\\{z_{1},\\dots,z_{m}\\}$ , defines $w_{0}=0$ , and operates for $T$ iterations according to the following recursion: ", "page_idx": 2}, {"type": "equation", "text": "$$\nw_{t}=\\Pi\\left[w_{t-1}-\\frac{\\eta}{|S|}\\sum_{z\\in S}\\mathcal{O}_{z}(w_{t})\\right]\\implies w_{S}^{G D}:=\\frac{1}{T}\\sum_{t=1}^{T}w_{t},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\Pi$ is the projection onto the unit ball, and $\\mathcal{O}_{z}(w_{t})$ is a subgradient of the loss function $f(w,z)$ at $w_{t}$ . The final output, $w_{S}^{G D}$ , of the algorithm is the averaged iterate (our result, though, can be generalized to other possible sufifx-averages such as, say, outputting the last iterate, see Theorem 10). GD constitutes an $\\varepsilon$ -ERM. Concretely, it is known [10, 17] that GD minimizes the empirical risk and its optimization error is given by: ", "page_idx": 2}, {"type": "equation", "text": "$$\nF_{S}(w_{S}^{G D})-\\operatorname*{min}_{w\\in W}F_{S}(w)=\\Theta\\left(\\operatorname*{min}\\left\\{\\eta+\\frac{1}{\\eta T},1\\right\\}\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The above bound is tight irrespective of the dimension2. The population loss have also been studied, and Bassily et al. [6] demonstrated the following learning guarantee: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\underset{S\\sim D^{m}}{\\mathbb{E}}\\left[F(w_{S}^{G D})-\\operatorname*{min}_{w\\in W}F(w)\\right]=O\\left(\\eta\\sqrt{T}+\\frac{1}{\\eta T}+\\frac{\\eta T}{m}\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The last two terms in the RHS follow from a stability argument, provided in [6], and the first term follo\u221aws from the optimization error of GD as depicted in Eq. (5). Notice that there is always an ${\\cal{O}}(\\eta\\sqrt{T})$ gap between the generalization error and empirical error of gradient descent. ", "page_idx": 2}, {"type": "text", "text": "3 Main Result ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Theorem 1. For every $d\\,\\geq\\,4096,T\\,\\geq\\,10,m\\,\\geq\\,1$ and $\\eta>0$ , there exists a distribution $D$ , and $a$ 4-Lipschitz convex function $f(w,z)$ in $\\mathbb{R}^{d+1}$ , such that for any first order oracle of $f(w,z)$ , with probability $1/2,$ if we run $G D$ with $\\eta$ as a learning rate then: ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\cal F}(w_{S}^{G D})-\\operatorname*{min}_{w\\in\\cal W}{\\cal F}(w)\\geq\\frac{1}{2\\cdot272\\cdot16^{2}}\\cdot\\operatorname*{min}\\left\\{\\frac{d}{1032m},1\\right\\}\\cdot\\operatorname*{min}\\left\\{\\eta\\sqrt{\\operatorname*{min}\\{[d^{3}/136],T\\}},1\\right\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We remark, that the above theorem is true for any sufifx averaging (e.g. last iterate), and not restricted to the averaged iterate (see Theorem 10). We now specialize our bound for two interesting regimes. First, we improve previous dependence in the dimension in [3, 20] and obtain a generalization error bound for $\\overset{\\d}{d}=\\Omega\\overset{\\d}{(m+T^{1/3})}$ : ", "page_idx": 3}, {"type": "text", "text": "Corollary 2. Fix $\\eta_{:}$ , and suppose $d=\\Omega\\left(m+T^{1/3}\\right)$ . There exists a distribution $D$ , and an $O(1)$ - Lipschitz convex function $f(w,z)\\,i n\\,\\mathbb{R}^{d}$ , such that for any first order oracle of $f(w,z)$ , with probability $1/2,\\,i f$ we run GD for $T$ iterations, then: ", "page_idx": 3}, {"type": "equation", "text": "$$\nF(w_{S}^{G D})-\\operatorname*{min}_{w\\in\\mathcal{W}}F(w)\\geq\\Omega\\left(\\operatorname*{min}\\left\\{\\eta\\sqrt{T}+\\frac{1}{\\eta T},1\\right\\}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The first term follows from Theorem 1, the second term follows from the optimization error in Eq. (5). Equation (7) does not hold for $d<m$ , and the linear improvement over [2, 20] is tight. T\u221ahis can be seen from Eq. (5) that shows that GD achieves $\\varepsilon$ empirical excess error when $\\eta=O(1/{\\sqrt{T}})$ and $T=O(1/\\varepsilon^{2})$ . Equation (7) becomes vacuous for such choice of para\u221ameters, but Carmon et al. [11] showed that the sample complexity of any ERM is bounded by $\\tilde{O}(\\dot{(d}\\!+\\!\\sqrt{m})/m)$ . However, as depicted next, this upper bound becomes tight and GD does not improve over a worst-case ERM: ", "page_idx": 3}, {"type": "text", "text": "Corollary 3. Suppose $T={\\cal O}(m^{1.5})$ , and $\\eta\\,=\\,\\Theta(1/{\\sqrt{T}})$ . There exists a distribution $D$ , and an $O(1)$ -Lipschitz convex function $f(w,z)$ in $\\mathbb{R}^{d}$ , such that for any first order oracle of $f$ , with probability $1/2,$ , if we run $G D$ with $\\eta$ , for $T$ iterations: ", "page_idx": 3}, {"type": "equation", "text": "$$\nF(w_{S}^{G D})-\\operatorname*{min}_{w\\in\\mathcal{W}}F(w)\\geq\\Omega\\left(\\operatorname*{min}\\left\\{\\frac{d}{m}+\\frac{1}{\\sqrt{m}},1\\right\\}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Corollary 3 complements Carmon et al. [11] upper bound, and improves over Feldman [12] lower bound that only showed existence of some ERM with the aforemen\u221ationed sample com\u221aplexity. To see that Corollary 3 follows from Theorem 1, notice that when $d\\leq{\\sqrt{m}}$ , then $d/m<1/\\bar{\\sqrt{m}}$ and the bound is dominated by the second term, which is a well known-information theoretic lower bound for learning. When $d>{\\sqrt{m}}$ , and $T<m^{1.5}$ we have that $T\\leq d^{3}$ , plugging $\\eta=O(1/{\\sqrt{T}})$ yields the bound. ", "page_idx": 3}, {"type": "text", "text": "3.1 Discussion ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Theorem 1 provides a new generalization error bound for GD. It shows that the worst case sample complexity for ERMs, derived by Feldman [12], is in fact applicable also to a very natural first order algorithm and not just to abstract ERMs. This Highlights the importance of choosing the right algorithm for learning in SCO. As discussed, the bound is tight in several regimes, nevertheless still there are unresolved open problems. ", "page_idx": 3}, {"type": "text", "text": "Stability in low dimension When GD is treated as a naive empirical risk minimizer, and $\\eta=$ $O(1/\\sqrt{T})$ , $T=O(m)$ , there is no improvement, when using GD, over a worst-c\u221aase ERM. In the other direction, for dimension that is linear in $m$ , one cannot improve over the $\\Omega(\\eta\\sqrt{T})$ term that governs stability. Our bound, though, provide a hope that stability in low dimension can yield an improved bound. In particular, consider the case where $\\eta=1/T^{1/4}$ and $d<m$ . This is a case where we apply a stable algorithm in small dimensions. Our bound does not negate the possibility of an improved generalization bound. That would mean that, at least at some regime, GD can improve over the worst-case ERM behaviour. We leave it as an open problem for future study ", "page_idx": 3}, {"type": "text", "text": "Open Question 4. Is there a generalization bound for GD such that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\underset{S\\sim D^{m}}{\\mathbb{E}}\\left[F(w_{S}^{G D})-\\operatorname*{min}_{w\\in\\mathcal{W}}F(w)\\right]=O\\left(\\frac{d\\eta\\sqrt{T}}{m}+\\frac{1}{\\sqrt{m}}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Alternatively, can we prove an improved generalization error bound such that: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\underset{S\\sim D^{m}}{\\mathbb{E}}\\left[F(w_{S}^{G D})-\\operatorname*{min}_{w\\in\\mathcal{W}}F(w)\\right]=\\Omega\\left(\\operatorname*{min}\\left\\{\\frac{d}{m},\\eta\\sqrt{T},1\\right\\}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Late stopping Another regime where there is a gap between known upper bound and lower bound appears when $T=\\Omega(m^{2})$ . Specifically, the stability upper bound for GD by Bassily et al. [6] gives ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\underset{S\\sim D^{m}}{\\mathbb{E}}\\left[F(w_{S}^{G D})-\\operatorname*{min}_{w\\in\\mathcal{W}}F(w)\\right]=O\\left(\\eta\\sqrt{T}+\\frac{\\eta T}{m}+\\frac{1}{\\eta T}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "By Corollary 2, for large enough dimension: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\underset{S\\sim D^{m}}{\\mathbb{E}}\\left[F(w_{S}^{G D})-\\operatorname*{min}_{w\\in\\mathcal{W}}F(w)\\right]=\\Omega\\left(\\operatorname*{min}\\left\\{\\eta\\sqrt{T}+\\frac{1}{\\eta T},1\\right\\}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "When $T=O(m^{2})$ , the two bounds coincide. Indeed, for the $\\eta T/m$ term to dominate the $\\eta\\sqrt{T}$ term, we must have $T=\\Omega(m^{2})$ . One has to take at least $T=O(m^{2})$ iterations in order to generalize with GD (in fact, any full batch method [2]), however $T=O(m^{2})$ iterations are sufifcient. Nevertheless, the above gap does yield the p\u221aossibility of an unstable GD method that does generalize. Particularly, if we just regulate the term $\\eta\\sqrt{T}$ , but allow $\\eta T/m=\\Omega(1)$ , then this may yield a regime where GD is unstable (and ERM bounds do not apply) and yet generalize. ", "page_idx": 4}, {"type": "text", "text": "Open Question 5. Are there choices of $\\eta$ and $T$ (that depend on $m$ ) such that $\\eta T/m\\in\\Omega(1)$ , but $G D$ has dimension indpendent sample complexity? ", "page_idx": 4}, {"type": "text", "text": "Notice that the $\\eta T/m$ term also governs stability in the smooth convex optimization setup [13]. Recall that a function $f(w,z)$ is said to be $\\beta\\cdot$ -smooth if for all $z$ , $f(w,z)$ is dif\u221aferentiable, and the gradient is an $\\beta$ -Lipschitz mapping [10, 15]. For smooth optimization, even if $\\eta\\sqrt{T}=\\Omega(1)$ , GD is still stable. Hardt, Recht, and Singer [13] showed that the stability of GD in the smooth case is governed by $\\begin{array}{r}{O\\left(\\frac{\\eta T}{m}\\right)}\\end{array}$ for $\\eta<1/\\beta$ . Therefore, the question of generalization when $\\eta T/m\\in\\Omega(1)$ remains open, even under smoothness assumptions: ", "page_idx": 4}, {"type": "text", "text": "Open Question 6. Assume that $f(w,z)$ is $\\Theta(1)$ -smooth. What is the sample complexity of $G D$ , when $\\eta$ and $T$ are chosen so that $\\begin{array}{r}{\\eta+\\frac{1}{\\eta T}=o(1)}\\end{array}$ , but $\\begin{array}{r}{\\frac{\\eta T}{m}=\\Omega(1)}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "4 Technical Overview ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We next provide a high level overview of our proof technique. For simplicity of exposition we begin with the case $T=m=d$ . We begin by a brief overview of previous construction by Amir et al. [3] that demonstrated Corollary 2 when $m=\\Omega(\\log d)$ . The construction in [3] can be decomposed into three terms: ", "page_idx": 4}, {"type": "equation", "text": "$$\nf(w,z)=g(w,z)+{\\cal N}_{0}(w)+h(w,z).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The function $g$ has the property that an ERM may fail to learn, unless dimension dependent sample size is considered. Amir et al. [3] incorporated Shalev-Shwartz et al. [22] construction. Later, [20] used Feldman\u2019s function [12] to construct $g$ . The shift from the construction depicted in Shalev-Shwartz et al. [22] to Feldman\u2019s function is the first step that allows to move from logarithmic to polynomial dependence in the dimension. In both constructions an underlying property of $g$ is that there exists a distribution $D$ such that, for small samples, there are overfitting minima. Concretely, there exists a $w_{S}\\in\\{0,1/\\sqrt{d}\\}^{d}$ such that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{1}{|S|}\\sum_{z\\in S}g(w_{S},z)-\\underset{z\\sim D}{\\mathbb{E}}[g(w_{S},z)]=\\Omega(1).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The challenge is then, to make gradient descent\u2019s trajectory move towards the point $w_{S}$ . The idea can be decomposed into two parts: ", "page_idx": 4}, {"type": "text", "text": "Simplifying with an adversarial subgradient: ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To simplify the problem, let us first ease the challenge and suppose we can choose our subgradient oracle in a way that depends on the observed sample. Let $N_{0}$ be the Nemirovski function [9]: ", "page_idx": 5}, {"type": "equation", "text": "$$\nN_{0}(w)=\\operatorname*{max}\\{-w(i),0)\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Notice that $N_{0}$ is not differentiable and the choice of subgradient at certain points is not apriori determined. For example, notice that every standard basis vector $-e_{i}\\,\\in\\,\\partial N_{0}(0)$ . More generally, given a sample $S$ , let $I=i_{1},\\ldots,i_{d^{\\prime}}$ be exactly the set of indices such that $w_{S}$ , from Eq. (8), $w_{S}(i)\\neq0$ Now assume by induction that $w_{t}(i)>0$ exactly for $i=i_{1},\\ldots i_{t}$ , then one can show that we can define the subgradient oracle of $N_{0}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{O}(w_{t})=-e_{i_{t+1}}\\in\\partial N_{0}(w_{t}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In that case $w_{t+1}$ will satisfy our assumption for $i_{t+1}$ and we can continue to follow this dynamic for $T$ steps. ", "page_idx": 5}, {"type": "text", "text": "Notice that, in this case, GD will converge to $w_{S}$ (if $\\eta=1/\\sqrt{d}$ which we assume now for concreteness). One can also show that the output of GD (the averaged iterate) will overfit. The caveat is that our subgradient oracle depends on the sample $S$ . In reality, the sample is drawn independent of the subgradient oracle. and all previous constructions, as well as ours need to handle this. This is discussed in the next section. But before that, let us review another challenge which is when $T\\neq d$ : ", "page_idx": 5}, {"type": "text", "text": "When $d\\ll T$ Another challenge we face with the construction above is that it works when we assume that $T\\approx d$ . That is because, in Nemirovski\u2019s function, the number of iterates we can perform is bounded by the dimension. After $d$ iterations we will end up with the vector $\\begin{array}{r}{\\nu\\,=\\,\\dot{\\sum_{t=1}^{d}}\\,\\eta e_{i_{t}}}\\end{array}$ . If $T\\,=\\,\\omega(d)$ then $\\eta\\,=\\,o(1/{\\sqrt{d}})$ , and the dynamic will end up with a too small norm vector to induce a sizeable population loss. This strategy will provide, at best, with a factor of the form $\\Omega\\left(\\eta{\\sqrt{\\operatorname*{min}\\{d,T\\}}}\\right)$ . Such a factor may be unsatisfactory in a very natural setting where, say, $T=O(m)$ , $\\eta=O(1/{\\sqrt{m}})$ , and $d=\\Omega({\\sqrt{m}})$ . To obtain the $d^{3}$ dependence, we perform the following alternation over the Nemirovski function. Consider the function: ", "page_idx": 5}, {"type": "equation", "text": "$$\nN(w)=\\operatorname*{max}\\{0,\\operatorname*{max}_{i\\leq d}\\{-w(i)\\},\\operatorname*{max}_{i\\leq j\\leq d}\\{w(j)-w(i)\\}\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "And suppose that at each iteration we return a subgradient as follows: ", "page_idx": 5}, {"type": "text", "text": "\u2022 If there is $i\\leq d$ , such that $w(i)=w(i+1)>\\eta$ , return subgradient $e_{i+1}-e_{i}$ and $w$ is updated by $w_{t+1}=w_{t}-\\eta e_{i+1}+\\eta e_{i}$ .   \n\u2022 If there is no such $i$ , then take the minimal $i$ (if exists) such that $w(i)\\,=\\,0$ , and return subgradient $-e_{i}$ and update $w_{t+1}=w_{t}+\\eta e_{i}$ .   \n\u2022 When non of the above is met, return subgradient 0. ", "page_idx": 5}, {"type": "text", "text": "The dynamic of the above scheme is depicted in Fig. 1, and solves the problem when $T\\approx d^{3}$ . One can show that GD will run for at least $d^{3^{\\circ}}\\approx T$ iterations, and will increase $O(d)$ coordinates, each, on average, by an order of $O(\\eta d)$ . This is better than the increase of $\\eta$ in each\u221a coordinate that we get from Nemirovski\u2019s function. In this way we obtain the improved result of $\\eta\\sqrt{T}$ , even when $T\\approx d^{3}$ . ", "page_idx": 5}, {"type": "text", "text": "When $T\\ll d$ , when the number of iterations is smaller than $d$ we face a different challenge. \u221aThe immediate solution is to embed in $\\mathbb{R}^{d}$ a construction from $\\mathbb{R}^{T}$ , this will provide us with the $\\Omega(\\eta\\sqrt{T})$ term but, on the other hand, such a construction will not yield a $\\Omega(d/m)$ term. A different approach, that exploits the dimension to its fullest, is to consider blocks of coordinates and operate on those instead of single coordinates. ", "page_idx": 5}, {"type": "text", "text": "The conclusive outcome incorporates both ideas together, and we replace the Nemirovski function with a version of Eq. (9) that operates on ${\\cal O}(T^{1/3})$ blocks of coordinates. And this concludes our construction. We next move on to the challenge of replacing the data dependent oracle with a standard first order oracle. ", "page_idx": 5}, {"type": "image", "img_path": "2INcTKPBy4/tmp/ff4eef21d64a627759ca5fde90a63cbc060f09214c2ce43406bd6e624c712da4.jpg", "img_caption": ["Figure 1: Depiction of the dynamics induced by Eq. (16) and our choice of sub-differentials "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Reduction to sample dependent oracle: ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "As discussed, the construction above does not yield a lower bound as it relies on a subgradient oracle that is dependent on the whole sample. To avoid such dependence of the oracle on the sample, we observe that if we can infer the sample $S$ from the trajectory, i.e. if the state $w_{t}$ \u201cencodes\" the sample, then formally the subgradient is allowed to \u201cdecipher\" the sample from the point $w_{t}$ . In that way we achieve this behaviour of sample dependent subgradient oracle. This part becomes challenging and may depend on the way we choose $g$ , and $N$ . The simplest case, studied by Amir et al. [3], introduced the third function, $h$ , which was a small perturbation function that elevates coordinates in $I$ and inhibits coordinates not in $I$ . The function $h$ depends on \ud835\udc67and not on $S$ , hence it cannot know apriori $I$ . But, an important observation is that, in Shalev-Shwartz et al. [22] construction, if $i\\notin I$ , there exists $z\\in S$ that \u201ccertifies\" that. In fact, each $z$ can be thought of as a subset of indices, and if an index appears in $z$ , then it cannot be in $I$ . So we can build the perturbation in a way that every coordinate is elevated, unless $z$ certifies $i\\notin I$ : In that case we define $h(w,z)$ so that its gradient will radically inhibit $i$ . ", "page_idx": 6}, {"type": "text", "text": "The last observation is what becomes challenging in our case. As discussed, to achieve improved rate, we need to use Feldman\u2019s function. When using Feldman\u2019s function the coordinates cannot be ruled out, or identified, by a single $z$ but one has to look at the whole sample to identify $I$ . While Schliserman et al. [20] tackle a similar problem, we take a slightly different approach described next: For each $z$ assign a random, positive, number $\\alpha(z)$ . We can think of this $\\alpha$ as a hash function. Let us add another coordinate to the vector $w$ , $w(d+1)$ . Consider the function ", "page_idx": 6}, {"type": "equation", "text": "$$\nh(w,z)=\\gamma\\alpha(z)\\cdot w(d+1).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Then $\\partial h(w,z)=\\gamma\\alpha(z)e_{d+1}$ . Write $\\begin{array}{r}{\\alpha(S)=\\frac{1}{|S|}\\sum_{z\\in S}\\alpha(z)}\\end{array}$ then in turn: ", "page_idx": 6}, {"type": "equation", "text": "$$\nw_{t}(d+1)=w_{t-1}(d+1)-\\partial\\frac{\\gamma}{|S|}\\sum_{z\\in S}h(w_{t-1},z)=-t\\cdot\\gamma\\alpha(S)e_{d+1}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "If $\\gamma,\\alpha(z)$ are chosen correctly, $\\alpha(S)$ is a one to one mapping from samples to real numbers, and small $\\gamma$ ensures that the overall addition of $h$ has negligible affect on the outcome. Then, we may define the subgradient oracle to be dependent on coordinate $d+1$ which encodes the whole sample. Our final construction will take a different $h$ , which adds small strong convexity in this coordinate, for reasons next explained: ", "page_idx": 6}, {"type": "text", "text": "Working with any first order oracle Notice that our statement is slightly stronger than what we so far illustrated. Theorem 1 states that, for any subgradient oracle, GD will fail. For that, we need to be a little bit more careful, and we want to replace our function with a function that leads to the same guaranteed trajectory, but at the same time it should be differentiable at visited points. This will ensure a unique derivative, making the construction independent of the choice of (sub)gradient oracle. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Towards this goal, we start with the construction depicted so far and consider the set of all values, gradients, and points $\\{f_{j},g_{j},w_{j}\\}_{j\\in J}$ that our algorithm may visit, for any possible time step and any possible sample, with our construction. Notice that, while this set may be big and even exponential, it is nevertheless finite. What we want is to interpolate a new function through these triplets. In contrast with our original construction, we require a differentiable function at the designated points. Notice, that such an interpolation will have the exact same behaviour when implementing GD on it (with the added feature that the oracle is well defined and unique). ", "page_idx": 7}, {"type": "text", "text": "The problem of convex interpolation is well studied, for example Taylor et al. [23] shows sufifcient and necessary conditions for interpolation of a smooth function. Our case is slightly easier as we do not care about the smoothness parameter. On the other hand we do require Lipschitzness of the interpolation. We therefore provide an elementary, self-contained, proof to the following easy to prove Lemma, (proof is provided in Appendix B) ", "page_idx": 7}, {"type": "text", "text": "Lemma 7. Let $G=\\{f_{j},g_{j},w_{j}\\}_{j\\in J}\\subseteq\\mathbb{R}\\times\\mathbb{R}^{d}\\times\\mathbb{R}^{d}$ be a triplet of values in $\\mathbb{R}$ , and gradients and points in $\\mathbb{R}^{d}$ , such that $\\|g_{j}\\|\\leq L$ . Suppose that for every $i,j\\in J$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\nf_{i}\\,\\geq\\,f_{j}+g_{j}^{\\sf T}\\bigl(w_{i}-w_{j}\\bigr),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "and let ", "page_idx": 7}, {"type": "equation", "text": "$$\nI_{d i j f}=\\{i:f_{i}=f_{j}+g_{j}^{\\top}(w_{i}-w_{j})\\Rightarrow g_{i}=g_{j}\\}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Then there exists a convex $L$ -Lipschitz function $\\hat{f}$ such that for all $j\\in J$ : ${\\hat{f}}(w_{j})=f_{j}$ , and for all $i\\in I_{d i\\!f\\!\\!/},\\,\\hat{f}$ is differentiable at $w_{i}$ and: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\nabla f(w_{i})=g_{i}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "With Lemma 7 at hand, consider the function ", "page_idx": 7}, {"type": "equation", "text": "$$\nh(w,z)=\\frac{1}{2}(w(d+1))^{2}+\\alpha(z)\\cdot w(d+1).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The above function encodes in $w(d+1)$ the sample and time-step as before. Moreover, because it is slightly strongly convex (in coordinate $d+1$ ), $w_{1}(d+1)\\neq w_{2}(d+1)$ ensures that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h(w_{1},z)>h(w_{2},z)+\\nabla h(w_{2},z)^{\\top}\\big(w_{1}-w_{2}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Then the term $h$ in $f$ ensures that the triples $\\{f_{j},g_{j},w_{j}\\}$ along the trajectory generate gradient vectors that satisfy strict inequality in Eq. (10) and in turn, our interpolation from Lemma 7 will be differentiable at these points. There\u2019s some technical subtlety because the interpolation needs to also take the averaged iterate into account, but this is handled in a similar fashion. ", "page_idx": 7}, {"type": "text", "text": "In the next two sections we provide more formal statements of the two main ingredients: First, we define a setup of optimization with a sample-dependent first order Oracle and state a lower bound for the generalization error in this setup. The second ingredient is a reduction from the standard setup of first order optimization. ", "page_idx": 7}, {"type": "text", "text": "4.1 Sample-dependent Oracle ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "As discussed, the first step in our proof is to consider a slightly weaker setup where the first-order oracle may depend on the whole sample. Let us formally define what we mean by that. Define ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbf{\\mathcal{S}}_{m}^{T}=\\{\\mathbf{S}=(S_{1},\\ldots,S_{t}),S_{i}\\in\\cup_{i=1}^{m}\\mathcal{Z}^{m},t\\leq T\\},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "the set of all subseqences of samples of size at most $m$ . Given a function $f(w,z)$ , a sample dependent oracle, $\\mathcal{O}_{\\mathcal{S}}$ , is a finite sequence of first order oracles ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\boldsymbol{\\mathcal{O}}_{\\mathcal{S}}=\\left\\{\\boldsymbol{\\mathcal{O}}^{(t)}(S;w,z)\\right\\}_{t=1}^{T},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "that each receive as input a finite sample $S$ , as well as $w$ and returns a subgradient: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{O}^{(t)}(S,w,z)\\in\\partial f(w,z).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The sequence of samples can be thought of as the past samples that were observed by the algorithm. In the case of full-batch GD these will be the whole sample, and for SGD, each $S$ provided to ${\\mathcal{O}}^{(t)}$ will be all previously observed samples. Given $\\mathbf{S}\\in\\mathcal{S}_{m}^{T}$ let us also denote ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathcal{O}^{(t)}(\\mathbf{S},w)=\\frac{1}{\\left|S_{t}\\right|}\\sum_{z\\in S_{t}}\\mathcal{O}^{(t)}(S_{1:t-1},w,z)\\in\\partial\\left(\\frac{1}{\\left|S_{t}\\right|}\\sum_{z\\in S_{t}}f(w,z)\\right),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where we let $\\mathcal{S}_{1:0}=\\varnothing$ , and $S_{1:t-1}=(S_{1},\\ldots,S_{t-1})$ is the concatenated subsample of all previously observed samples in the sequence. As discussed, working with a sample-dependent oracle is easier (for lower bounds). And indeed, our first result shows that, if the subgradient can be chosen in a way that depends on the sample, we can provide the desired lower bound. For fixed and known $\\eta>0,T$ , a sample dependent first order oracle $\\mathcal{O}_{\\mathcal{S}}$ , and a sequence of samples $\\mathbf{S}=(S_{1},S_{2},\\ldots,S_{T})$ , define $w_{0}=0$ and inductively: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\boldsymbol{w}_{t}^{\\mathbf{S}}=\\Pi\\left[\\boldsymbol{w}_{t-1}^{\\mathbf{S}}-\\eta\\boldsymbol{\\mathcal{O}}^{(t)}(\\mathbf{S},\\boldsymbol{w}_{t-1}^{\\mathbf{S}})\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "and for every sufifx $\\mathbf{s}<T$ : ", "page_idx": 8}, {"type": "equation", "text": "$$\nw_{{\\bf S},{\\bf s}}^{G D}=\\frac{1}{T-{\\bf s}}\\sum_{t={\\bf s}+1}^{T}w_{t}^{\\bf S}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Lemma 8. For every $m$ , $d,T\\geq18$ and $\\eta>0$ there are a distribution $D$ , a 3-Lipschitz convex function $f(w,z)$ in $\\mathbb{R}^{d}$ , as well as a sample dependent first order oracle $\\mathcal{O}_{\\mathcal{S}}$ such that: $i f\\mathbf{S}=(S,S,\\ldots S)\\in\\mathbb{S}_{m}^{T}$ for $S\\sim D^{m}$ i.i.d, then w.p. 1/2, for every suffix averaging s: ", "page_idx": 8}, {"type": "equation", "text": "$$\n{\\cal F}(w_{{\\bf S},s}^{G D})-{\\cal F}(0)\\ge\\frac{1}{\\sqrt{2}\\cdot272\\cdot16^{2}}\\cdot\\operatorname*{min}\\left\\{\\frac{d}{1032m},1\\right\\}\\cdot\\operatorname*{min}\\left\\{\\eta\\sqrt{\\operatorname*{min}\\{\\lfloor d^{3}/136\\rfloor,T\\}},1\\right\\}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The proof of Lemma 8 is provided in Appendix A.1. We next move to describe the second ingredient of our proof. ", "page_idx": 8}, {"type": "text", "text": "4.2 Reduction to sample-dependent oracles ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "As discussed, the second ingredient of our proof is a reduction to the sample-dependent setup. Instead of using a perturbation function as in [3], we take a more black box approach and show that, given a sample dependent first order oracle, there exists a function that basically induces the same trajectory. Proof is provided in Appendix A.2. ", "page_idx": 8}, {"type": "text", "text": "Lemma 9. Suppose $q\\in\\mathbb{R}^{T}$ , $\\|q\\|_{\\infty}\\leq1$ . And suppose that $f(w,z)$ is a convex, $L$ -Lipschitz, function over $w\\in\\mathbb{R}^{d}$ , let $\\eta>0$ , let $\\mathcal{O}_{\\mathcal{S}}$ be a sample dependent first order oracle, and for every sequence of samples $\\mathbf{S}=(S_{1},S_{2},.~.~.~,S_{T})$ define the sequence $\\{w_{t}^{\\mathbf{S}}\\}_{t=1}^{T}$ as in Eq. (12). ", "page_idx": 8}, {"type": "text", "text": "Then, for every $\\varepsilon>0$ there exists an $L+1$ Lipschitz convex function3 $\\bar{f}((w,x),z)$ over $\\mathbb{R}^{d+1}$ (that depends on $q,f,T,\\eta,m,\\mathcal{O}_{\\mathcal{S}},\\varepsilon)$ . ", "page_idx": 8}, {"type": "text", "text": "such that for any first order oracle $\\mathcal{O}_{z}$ for $\\bar{f}$ , define $u_{0}=0\\in\\mathbb{R}^{d}$ and $x_{0}=0\\in\\mathbb{R}$ , and: ", "page_idx": 8}, {"type": "equation", "text": "$$\n(u_{t},x_{t})=(u_{t-1},x_{t-1})-\\frac{\\eta}{|S_{t}|}\\sum_{z\\in S_{t}}\\mathcal{O}_{z}((u_{t},x_{t}))\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "then if we define: ", "page_idx": 8}, {"type": "equation", "text": "$$\nu_{q}=\\sum_{t=1}^{T}q(t)u_{t},\\;a n d,\\quad x_{q}=\\sum_{t=1}^{T}q(t)x_{t},\\;a n d\\,w_{q}^{\\mathbf{S}}=\\sum_{t=1}^{T}q(t)w_{t}^{\\mathbf{S}}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "then, we have that ${u_{q}}={w_{q}^{\\bf{S}}}$ and: ", "page_idx": 8}, {"type": "equation", "text": "$$\n|\\bar{f}((u_{q},x_{q}),z)-f(w_{q}^{\\mathbf{S}},z)|\\leq\\varepsilon.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "and, ", "page_idx": 8}, {"type": "equation", "text": "$$\n|{\\bar{f}}((0,0),z)-f(0,z)|\\leq\\varepsilon.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Ackgnoweledgments The author would like to thank Tamar Livni for creating Figure 1. Tamar holds all copyrights to the artwork. The author would also like to thank Tomer Koren and Yair Carmon for several discussions. This research was funded in part by an ERC grant (FOG, 101116258), as well as an ISF Grant (2188 \\ 20). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] N. Alon, S. Ben-David, N. Cesa-Bianchi, and D. Haussler. Scale-sensitive dimensions, uniform convergence, and learnability. Journal of the ACM (JACM), 44(4):615\u2013631, 1997.   \n[2] I. Amir, Y. Carmon, T. Koren, and R. Livni. Never go full batch (in stochastic convex optimization). Advances in Neural Information Processing Systems, 34:25033\u201325043, 2021.   \n[3] I. Amir, T. Koren, and R. Livni. Sgd generalizes better than gd (and regularization doesn\u2019t help). In Conference on Learning Theory, pages 63\u201392. PMLR, 2021.   \n[4] I. Amir, R. Livni, and N. Srebro. Thinking outside the ball: Optimal learning with gradient descent for generalized linear stochastic convex optimization. Advances in Neural Information Processing Systems, 35:23539\u201323550, 2022.   \n[5] I. Attias, G. K. Dziugaite, M. Haghifam, R. Livni, and D. M. Roy. Information complexity of stochastic convex optimization: Applications to generalization and memorization. arXiv preprint arXiv:2402.09327, 2024.   \n[6] R. Bassily, V. Feldman, C. Guzm\u00e1n, and K. Talwar. Stability of stochastic gradient descent on nonsmooth convex losses. Advances in Neural Information Processing Systems, 33:4381\u20134391, 2020.   \n[7] A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. Learnability and the vapnikchervonenkis dimension. Journal of the ACM (JACM), 36(4):929\u2013965, 1989.   \n[8] O. Bousquet and A. Elisseef.f Stability and generalization. The Journal of Machine Learning Research, 2:499\u2013526, 2002.   \n[9] S. Bubeck, Q. Jiang, Y.-T. Lee, Y. Li, and A. Sidford. Complexity of highly parallel non-smooth convex optimization. Advances in neural information processing systems, 32, 2019.   \n[10] S. Bubeck et al. Convex optimization: Algorithms and complexity. Foundations and Trends $\\circled R$ in Machine Learning, 8(3-4):231\u2013357, 2015.   \n[11] D. Carmon, R. Livni, and A. Yehudayof.f The sample complexity of erms in stochastic convex optimization. arXiv preprint arXiv:2311.05398, 2023.   \n[12] V. Feldman. Generalization of erm in stochastic convex optimization: The dimension strikes back. Advances in Neural Information Processing Systems, 29, 2016.   \n[13] M. Hardt, B. Recht, and Y. Singer. Train faster, generalize better: Stability of stochastic gradient descent. In International conference on machine learning, pages 1225\u20131234. PMLR, 2016.   \n[14] D. Haussler and M. Warmuth. The probably approximately correct (pac) and other learning models. The Mathematics of Generalization, pages 17\u201336, 2018.   \n[15] E. Hazan et al. Introduction to online convex optimization. Foundations and Trends $\\circled R$ in Optimization, 2(3-4):157\u2013325, 2016.   \n[16] R. Livni. Information theoretic lower bounds for information theoretic upper bounds. Advances in Neural Information Processing Systems, 36, 2023.   \n[17] Y. Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer Science & Business Media, 2013.   \n[18] B. Neyshabur, R. Tomioka, and N. Srebro. In search of the real inductive bias: On the role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614, 2014.   \n[20] M. Schliserman, U. Sherman, and T. Koren. The dimension strikes back with gradients: Generalization of gradient methods in stochastic convex optimization. arXiv preprint arXiv:2401.12058, 2024.   \n[21] A. Sekhari, K. Sridharan, and S. Kale. Sgd: The role of implicit regularization, batch-size and multiple-epochs. Advances In Neural Information Processing Systems, 34:27422\u201327433, 2021.   \n[22] S. Shalev-Shwartz, O. Shamir, N. Srebro, and K. Sridharan. Stochastic convex optimization. In COLT, volume 2, page 5, 2009.   \n[23] A. B. Taylor, J. M. Hendrickx, and F. Glineur. Smooth strongly convex interpolation and exact worst-case performance of first-order methods. Mathematical Programming, 161:307\u2013345, 2017.   \n[24] V. N. Vapnik and A. Y. Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. In Measures of complexity: festschrift for alexey chervonenkis, pages 11\u201330. Springer, 2015.   \n[25] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107\u2013115, 2021.   \n[26] M. Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Proceedings of the 20th international conference on machine learning (icml-03), pages 928\u2013936, 2003. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "A Proof of Theorem 1 ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The proof is an immediate corollary of Lemmas 8 and 9, which we prove in Appendices A.1 and A.2 respectively. To see that Theorem 1 indeed follows from these Lemmas, start with $\\eta,d,m,T$ that satisfy the conditions. Let $f(w,z)$ be the function and $\\mathcal{O}_{\\mathcal{S}}$ the sample dependent first order oracle, whose existence follows from Lemma 8 with sufifx $\\mathbf{s}=0$ . And let $\\bar{f}$ be the function whose existence follows from Lemma 9 to some arbitrarily small $\\varepsilon_{0}$ , with $\\begin{array}{r}{q(t)=\\frac{1}{T}}\\end{array}$ for all $t$ . It is easy to see that if we apply GD on $\\bar{f}$ and define its output $(u^{G D},x^{G D})$ then: $(u^{G D},x^{G D})=(u_{q},x_{q})$ , and $w_{q}^{\\mathbf{S}}=w_{\\mathbf{S}}^{G D}$ ", "page_idx": 10}, {"type": "text", "text": "Then, we have that w.p. $1/2$ : ", "page_idx": 10}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\bar{F}((u^{G D},x^{G D}))-\\bar{F}(0)=\\bar{F}(u_{q},x_{q})-\\bar{F}(0)}&{}\\\\ {\\qquad\\qquad\\qquad\\geq F(w_{s}^{G D})-2\\varepsilon_{0}-F(0)}&{}\\\\ {\\qquad\\qquad\\geq\\cfrac{1}{\\sqrt{2}\\cdot272\\cdot16^{2}}\\cdot\\operatorname*{min}\\left\\{\\cfrac{d}{1032m},1\\right\\}\\cdot\\operatorname*{min}\\left\\{\\eta\\sqrt{\\operatorname*{min}\\{|d^{3}/136|,T\\}},1\\right\\}-2\\varepsilon_{0}}&{}\\\\ {\\qquad\\qquad\\geq\\cfrac{1}{2\\cdot272\\cdot16^{2}}\\cdot\\operatorname*{min}\\left\\{\\cfrac{d}{1032m},1\\right\\}\\cdot\\operatorname*{min}\\left\\{\\eta\\sqrt{\\operatorname*{min}\\{|d^{3}/136|,T\\}},1\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 10}, {"type": "text", "text": "Where in the last equation, we assume $\\varepsilon_{0}$ to be sufifciently small. Finally, note that $\\bar{F}(0)\\;\\geq$ $\\mathrm{min}_{w\\in\\mathcal{W}}\\,\\bar{F}(w)$ . ", "page_idx": 10}, {"type": "text", "text": "Notice, that by the same argument, by taking any sufifx $\\mathbf{s}<T$ , and setting $q(t)=0$ for $t\\leq\\mathbf{s}$ , and $\\begin{array}{r}{q(t)=\\frac{1}{T-\\mathbf{s}}}\\end{array}$ for $t\\geq{\\bf s}+1$ , we can obtain the following stronger result for any sufifx averaging: ", "page_idx": 10}, {"type": "text", "text": "Theorem 10. For every $d\\,\\geq\\,4096,T\\,\\geq\\,10,m\\,\\geq\\,1$ and $\\eta\\,>\\,0$ and suffix $\\textbf{s}<T_{:}$ , there exists $a$ distribution $D$ , and $a$ 4-Lipschitz convex function $f(w,z)$ in $\\mathbb{R}^{d}$ , such that for any first order oracle of $f(w,z)$ , with probability 1/2, if we run $G D$ with $\\eta$ as a learning rate then: ", "page_idx": 10}, {"type": "equation", "text": "$$\nF\\left(\\frac{1}{T-s}\\sum_{t=s+1}^{T}w_{t}^{s}\\right)-F(0)\\geq\\frac{1}{2\\cdot272\\cdot16^{2}}\\cdot\\operatorname*{min}\\left\\{\\frac{d}{1032m},1\\right\\}\\cdot\\operatorname*{min}\\left\\{\\eta\\sqrt{\\operatorname*{min}\\{\\lfloor d^{3}/136\\rfloor,T\\}},1\\right\\}.\n$$", "text_format": "latex", "page_idx": 10}, {"type": "text", "text": "A.1 Proof of Lemma 8 ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "For simplicity we will assume that $d\\,=\\,2^{n}$ for some $n\\,\\in\\,\\mathbb{N}$ , the final result will be obtained by embedding a construction in a subspace of size at least half the original dimension. ", "page_idx": 11}, {"type": "text", "text": "We start by recalling Feldman\u2019s construction [12]: There exists a set ${\\mathcal{V}}\\subseteq\\{0,1\\}^{d}$ , such that: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\left(\\forall\\nu_{1}\\neq\\nu_{2}\\in\\mathcal{V},\\,\\,\\nu_{1}\\cdot\\nu_{2}\\leq\\frac{5d}{16}\\right),\\,\\mathrm{and}\\,\\,\\left(\\|\\nu\\|^{2}\\geq\\frac{7d}{16}\\right),\\,\\mathrm{and}\\,\\,\\left(|\\mathcal{V}|>e^{d/258}\\right).\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Indeed, suppose we pick randomly $w\\in\\{0,1\\}^{d}$ according to probability $P$ where each coordinate $P(w(i))=1$ with probability $1/2$ (independently). Then, by Hoeffding\u2019s inequality for two $w_{1},w_{2}\\sim P$ independently: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{l}{P\\left(w_{1}\\cdot w_{2}>\\frac{d}{4}+\\frac{d}{16}\\right)\\le e^{-\\frac{d}{128}},}\\\\ {P\\left(w_{1}\\cdot w_{1}<\\frac{d}{2}-\\frac{d}{16}\\right)\\le e^{-\\frac{d}{128}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Thus, picking $\\mathcal{V}$ elements i.i.d according to $P$ , randomly, of size $|\\mathcal{V}|\\ge e^{d/258}$ we can show by union bound that with positive probability, all $|\\mathcal{V}|^{2}$ pairs in $\\boldsymbol{\\mathcal{V}}$ will satisfy the requirement. Next, we define a distribution $D_{\\varepsilon}$ supported on subsets of $\\mathcal{V}$ such that for a random variable $V\\subseteq\\mathcal{V}$ each $\\nu\\in V$ w.p. \u03b5 independently. We start by assuming that $\\begin{array}{r}{T\\leq\\frac{1}{17}d^{3}}\\end{array}$ (the case $T>d^{3}$ is handled at the end), and let $k\\in\\mathbb N$ be such that: ", "page_idx": 11}, {"type": "equation", "text": "$$\nd\\leq k\\left(\\frac{T}{17}\\right)^{1/3}<2d.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "One can show that without loss of generality we can assume $k$ is also a power of 2 (in particular, $d$ is divisble by $k$ for large enough $T$ ). We next follow the idea depicted in Section 4, but we want to handle the case $k\\gg1$ . For that, we redefine the function in Eq. (9), and take blocks of coordinates. To simplify notations, let us define for two set of indices $I,J$ of elements in $[d]\\colon I=\\{i_{1},\\dots,i_{k}\\}$ , $J=\\{j_{1},\\dotsc,,i_{k}\\}$ , $I\\prec J$ if $\\operatorname*{max}\\{i\\in I\\}<\\operatorname*{min}\\{j\\in J\\}$ . and we will also write: ", "page_idx": 11}, {"type": "equation", "text": "$$\ne_{I}={\\frac{1}{\\sqrt{|I|}}}\\sum_{i\\in I}e_{i},\\quad{\\mathrm{and}},w(I)=w\\cdot e_{I}={\\frac{1}{\\sqrt{I}}}\\sum_{i\\in I}w(i).\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "then we define our final function as: ", "page_idx": 11}, {"type": "equation", "text": "$$\nN(w)=\\operatorname*{max}\\left\\{0,\\operatorname*{max}_{|I|=k}\\{-w(I)\\},\\operatorname*{max}\\left\\{-(w(I)-w(J)):I<J,|I|=|J|=k\\right\\}\\right\\}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Define $\\begin{array}{r}{\\alpha=\\operatorname*{min}\\lbrace\\frac{1}{\\eta\\sqrt{2T}},1\\rbrace}\\end{array}$ , and let: ", "page_idx": 11}, {"type": "equation", "text": "$$\nf(w,V)=g(w,V)+\\alpha N(w).\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "where $N$ is defined in Eq. (14), and $g$ is defined to be Feldman\u2019s function with a suitable choice of threshold: ", "page_idx": 11}, {"type": "equation", "text": "$$\ng(w,V)=\\frac{1}{\\sqrt{d}}\\operatorname*{max}_{\\nu\\in V}\\left\\{\\frac{45\\eta\\alpha d^{2}}{2\\cdot16^{2}k^{1.5}},w\\cdot\\nu\\right\\}.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Notice that $N$ is 2-Lipschitz, and $g$ is 1-Lipschitz. ", "page_idx": 11}, {"type": "text", "text": "To obtain the trajectory, we next define a sample dependent oracle. We only define it for samples S such that there exists $\\nu^{\\star}\\notin V_{i}$ for all $V_{i}\\in S$ (define it arbitrarily to any other type of sample). Let $\\mathcal{I}=\\{i_{1},\\ldots,i_{d^{\\prime}}\\}$ be a set of $\\frac{7d}{16}$ indices such that $\\nu^{\\star}(i_{j})\\neq0$ . Divide the elements of I into $d^{\\prime}/k$ subsets. Namely, let ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r}{I_{j}=\\{i_{(j-1)\\cdot k+1},i_{(j-1)\\cdot k+2},\\ldots,i_{j\\cdot k}\\},\\quad j=1,\\ldots,d^{\\prime}/k.}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "We start by defining only an oracle for the function $\\alpha N$ . We will later show that the trajectory induced by this oracle stays in the minima of $g$ , and that will show that, for our purposes, we can choose the ", "page_idx": 11}, {"type": "text", "text": "swae mfeir sotr adcelfein feo:r the whole function $f$ . We denote by ${\\mathcal O}_{\\alpha N}^{(t)}$ the sample dependent oracle for $\\alpha N$ , and ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathcal{O}_{\\alpha N}^{(1)}(\\emptyset,0)=0,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Next, we define for $t>1$ . For any $w$ such that $0\\not\\in\\partial N(w)$ , define it arbitrarily. If $0\\in\\partial N(w)$ we define it as follows: ", "page_idx": 12}, {"type": "text", "text": "\u2022 If there is a multi-index $I_{j}$ that $w(I_{j})=w(I_{j+1})>\\eta\\alpha$ : Then: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathcal{O}_{\\alpha N}^{(t)}(S,w)=\\alpha(e_{I_{j+1}}-e_{I_{j}}).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "\u2022 If there is no such multi-index, and if $I_{j}$ is the minimal multiindex such that $w(I_{j})=0$ , set: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathcal{O}_{\\alpha N}^{(t)}(S,w)=-\\alpha(e_{I_{j}}).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "\u2022 If both conditions cannot be met, then: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathcal{O}_{\\alpha N}^{(t)}(S,w)=0.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "tThhien kt roajfe ectaocrhy  coofo trhdiisn adtyen ianm Fici gi.s  1d eaps icat ebldo icnk  Foifg .s i1z, ef $k$ .  thNeo cwa swe $k=1$ . mIen  tthhaet $\\{w_{t}^{\\mathbf{S}}\\}_{t=1}^{T}$ afsoell, owwes  ctahne trajectory depicted in Eq. (12) with that choice of Oracle. It can be seen that the update step is such that after $\\begin{array}{r}{T^{\\prime}=1+\\sum_{t^{\\prime}=1}^{d^{\\prime}/\\bar{k^{}}}\\sum_{t^{\\prime\\prime}=1}^{t^{\\prime}}t^{\\prime\\prime}}\\end{array}$ rounds, we will have that for every $i\\in I_{t}$ : ", "page_idx": 12}, {"type": "equation", "text": "$$\nw_{T^{\\prime}}^{\\mathbf{S}}(i)=\\alpha\\eta\\sqrt{k}(d^{\\prime}/k+1-t),\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "and that for $t>T^{\\prime}$ : $w_{t}^{\\mathbf{S}}=w_{T^{\\prime}}^{\\mathbf{S}}$ ", "page_idx": 12}, {"type": "text", "text": "Moreover, one can show that $w_{t}$ is non zero only in coordinates $i\\in\\mathcal I$ , and that for any subset $I_{B}\\subseteq I$ such that $\\left|I\\right|=B$ : ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\sum_{i\\in I_{B}}w_{t}^{\\mathbf{S}}(i)\\leq\\operatorname*{max}\\left\\{\\sum_{i\\in I_{B}}w_{T}^{\\mathbf{S}}(i):I_{B}\\subseteq I,|I|=B\\right\\}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "A formal proof is provided in Appendix D. Notice that: ", "page_idx": 12}, {"type": "equation", "text": "$$\n1+\\sum_{t^{\\prime}=1}^{d^{\\prime}/k}\\sum_{t^{\\prime\\prime}=1}^{t^{\\prime}}t^{\\prime\\prime}<\\frac{{d^{\\prime}}^{3}}{k^{3}}\\leq\\frac{1}{5}T.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Next, for any $\\nu\\neq\\nu^{\\star}$ , we have that: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{w_{i}^{\\star}\\star v_{i}\\stackrel{\\star}{\\longrightarrow}\\sum_{s\\rightarrow0}^{\\infty}w_{i}^{\\star}(i,j)_{i}[v_{i}(i)_{i}-1]}\\\\ {\\ }&{{}}\\\\ {\\quad}&{\\lesssim\\operatorname*{max}\\left\\{\\sum_{s\\rightarrow0}^{\\infty}w_{i}^{\\star}(j)_{i}:f_{s}\\in L_{i}:I_{i}|:S_{i}^{\\star}\\right\\}}\\\\ {\\ }&{{}}\\\\ {\\quad}&{\\lesssim\\operatorname*{max}\\left\\{\\sum_{s\\rightarrow0}^{\\infty}w_{i}^{\\star}(i)\\right.}\\\\ {\\ }&{{}}\\\\ {\\left.\\underset{{}\\geq}\\frac{w_{i}^{\\star}}{w_{i}}\\sum_{s\\rightarrow0}^{\\infty}w_{i}^{\\star}(i)\\right.}\\\\ {\\ }&{{}}\\\\ {\\left.\\underset{{}\\geq}\\frac{w_{i}^{\\star}}{w_{i}}\\frac{\\sqrt{w_{i}(i,j)}+1-\\sigma}{w_{i}(i,j)}\\right.}\\\\ {\\ }&{{}}\\\\ {\\left.\\underset{{}\\leq}\\sqrt[3]{w_{i}(i,j)}\\frac{\\sqrt{w_{i}(i,j)}-1}{2}\\left(\\frac{w_{i}^{\\star}}{16}\\right)\\right\\}}\\\\ {\\ }&{{}\\lesssim\\sqrt{w_{i}(\\frac{\\sqrt{w_{i}(i,j)}-1}{6})}\\left(\\frac{w_{i}^{\\star}}{16}\\right)^{2}}\\\\ {\\ }&{{}\\leq\\sqrt{w_{i}}\\frac{2\\sqrt{w_{i}(i,j)}}{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\nd^{\\prime}=\\frac{7d}{16}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "As such $0\\in\\partial g(w_{t}^{\\mathbf{S}},V_{i})$ for all $V_{i}$ , and we can define ${\\mathcal{O}}^{(t)}$ so that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\boldsymbol{\\mathcal{O}}^{(t)}(\\boldsymbol{S},\\boldsymbol{w}_{t}^{\\mathbf{S}},V_{i})=\\boldsymbol{\\mathcal{O}}_{\\alpha\\boldsymbol{N}}^{(t)}(\\boldsymbol{S},\\boldsymbol{w}_{t}^{\\mathbf{S}}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "And we have that, $\\{w_{t}^{\\mathbf{S}}\\}$ is the trajectory obtained from Eq. (12) with respect to this oracle of $g$ also. We also have: ", "page_idx": 13}, {"type": "equation", "text": "$$\nw_{T^{\\prime}}^{\\mathbf{S}}\\cdot\\nu^{\\star}=\\sqrt{k}\\eta\\alpha\\sum_{t=1}^{d^{\\prime}/k}(d^{\\prime}/k+1-t)=\\sqrt{k}\\eta\\alpha\\sum_{t=1}^{d^{\\prime}/k}t\\geq\\frac{\\sqrt{k}\\eta\\alpha}{2}\\frac{d^{\\prime2}}{k^{2}}\\geq\\sqrt{k}\\eta\\alpha\\frac{49d^{2}}{2(16k)^{2}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "And because $\\begin{array}{r}{T^{\\prime}\\leq\\frac{1}{17}T}\\end{array}$ , for every sufifx $\\mathbf{s}\\in[T]$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\nu^{\\star}\\cdot w_{{\\bf S},{\\bf s}}^{G D}\\geq\\frac{16}{17}w_{T^{\\prime}}^{{\\bf S}}\\cdot\\nu^{\\star}\\geq\\sqrt{k}\\eta\\alpha\\frac{46d^{2}}{2(16k)^{2}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Recall that we assume $\\begin{array}{r}{\\frac{136d^{3}}{k^{3}}\\ge T}\\end{array}$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\n{}^{\\footnotesize\\tau}(W_{S,s}^{G D})-F_{S}(0)\\geq\\varepsilon\\left(\\frac{46\\sqrt{k}\\alpha\\eta d^{1.5}}{2(16k)^{2}}-\\frac{45\\sqrt{k}\\alpha\\eta d^{1.5}}{2(16k)^{2}}\\right)\\geq\\varepsilon\\frac{\\alpha\\eta}{2\\cdot16^{2}}\\left(\\frac{d}{k}\\right)^{1.5}\\geq\\frac{\\varepsilon}{\\sqrt{2}\\cdot272\\cdot16^{2}}\\operatorname*{min}\\left\\{\\eta\\sqrt{T},1\\right\\}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Eq. (18) lower bounds the generalization under the event that there exists $\\nu\\,\\in\\,\\mathcal{V}$ such that $\\nu\\notin V_{i}$ for every $i\\,=\\,1,\\ldots,m$ . Now assume $\\begin{array}{r}{\\varepsilon\\,=\\,\\operatorname*{min}\\lbrace\\frac{d}{516m},\\frac{1}{4}\\rbrace}\\end{array}$ , then for every $\\nu$ , using the inequality $\\left(1-\\varepsilon\\right)\\le e^{-2\\varepsilon}$ for $\\varepsilon<1/2$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\nP(\\nu\\notin\\cap_{i=1}^{m}V_{i})=(1-\\varepsilon)^{m}\\geq e^{-2\\varepsilon\\cdot m}\\geq e^{-2d/516},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and, ", "page_idx": 13}, {"type": "equation", "text": "$$\nP(\\exists\\nu,\\notin\\cap_{i=1}^{m}V_{i})\\ge1-(1-e^{-2d/516})^{|V|}\\ge1-(1-e^{-d/258})^{e^{d/258}}\\ge1-e^{-1}\\ge1/2.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "So far we assume that $d=2^{n}$ , notice that if $\\begin{array}{r}{T<\\frac{1}{8\\cdot17}d^{3}}\\end{array}$ , we can find a subspace of size $\\begin{array}{r}{d_{1}>\\frac{d}{2}}\\end{array}$ so that $\\begin{array}{r}{T<\\frac{1}{8\\cdot17}d_{1}^{3}}\\end{array}$ , and we obtain our final result by embedding our construction in this subspace. ", "page_idx": 13}, {"type": "text", "text": "When $\\begin{array}{r}{T>\\frac{1}{17\\cdot8}d^{3}}\\end{array}$ Notice that if we take the construction with $\\begin{array}{r}{T=\\lfloor\\frac{1}{17\\cdot8}d^{3}\\rfloor}\\end{array}$ then: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\odot^{(T)}(w_{T}^{\\mathbf{S}})=0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "hence we can use the above construction for any $T^{\\prime}>T$ , and the subdifferential oracle is defined for every iteration above $T$ as returning 0, and we obtain a similar analysis to the case that $T=\\lfloor d^{3}/136\\rfloor$ , and our bounds yield as in Eq. (18): ", "page_idx": 14}, {"type": "equation", "text": "$$\nF(w_{\\mathbf{S},\\mathbf{s}}^{G D})-F_{S}(0)\\geq\\frac{\\varepsilon}{\\sqrt{2}\\cdot272\\cdot16^{2}}\\operatorname*{min}\\left\\{\\eta\\sqrt{\\lfloor d^{3}/136\\rfloor},1\\right\\}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A.2 Proof of Lemma 9 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The main ingredient in our proof is the following claim whose proof is provided in Appendix C. To state the claim we will need a few notations. First, for any two tuples of sample-sequences in $\\mathcal{S}_{m}^{T}$ , and $t>0\\;(\\mathbf{S},t)$ and $(\\mathbf{S}^{\\prime},t)$ , let us denote $(\\mathbf{S},t)\\equiv(\\mathbf{S}^{\\prime},t^{\\prime})$ if $t=t^{\\prime}$ and $S_{1:t}=S_{1:t}^{\\prime}$ , namely the prefixes of the sample sequences agree up to point $t^{\\prime}=t$ . Second, for a mapping $\\alpha:^{!}\\dot{\\mathcal{Z}}\\rightarrow[0,\\dot{1}]$ , from sample points to real numbers, and a sample $S$ , let us denote ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\alpha(S)=\\frac{1}{|S|}\\sum_{z\\in S}\\alpha(z).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Claim 1. For every $\\eta,\\gamma$ and $\\alpha$ define for every sequence of samples $\\mathbf{S}\\,=\\,(S_{1},\\ldots,S_{T})$ , and $t$ inductively: $x_{0}^{\\mathbf{S}}=0$ and ", "page_idx": 14}, {"type": "equation", "text": "$$\nx_{t}^{\\mathbf{S}}=(1-\\gamma\\eta)x_{t-1}^{\\mathbf{S}}-\\gamma\\eta\\alpha(S_{t}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and define: ", "page_idx": 14}, {"type": "equation", "text": "$$\nx_{q}^{\\mathbf{S}}=\\sum_{t=1}q(t)x_{t}^{\\mathbf{S}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then, for any $\\varepsilon>0$ , there is a choice of $\\gamma<\\varepsilon$ and $\\alpha$ such that: ", "page_idx": 14}, {"type": "text", "text": "The $x_{t}^{\\mathbf{S}}$ represents the different states the trajectory can be in. $x_{q}^{\\mathbf{S}}$ represent the output of the trajectory which can be an aggreagated sum. We require that each provide a signature for the state, and this will allow us to \u201ccode\" the state of the trajectory along GD. ", "page_idx": 14}, {"type": "text", "text": "We now continue with the proof of Lemma 8. For every sample dependent first order oracle ${\\mathcal{O}}^{(t)}$ , we have that: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\boldsymbol{\\mathcal{O}}^{(t)}(\\mathbf{S};w)=\\frac{1}{\\left|S_{t}\\right|}\\sum_{z\\in S_{t}}\\boldsymbol{\\mathcal{O}}^{(t)}\\big(S_{1:t-1};w,z\\big).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "To simplify notations, let us denote: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{O}^{(t)}(S_{1:t-1},w_{t}^{\\mathbf{S}},z)=\\mathcal{O}_{\\mathbf{S},t,z},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "as $\\mathbf{S},t,z$ completely determine the output. We next define ", "page_idx": 14}, {"type": "equation", "text": "$$\nh_{z}(x):=\\frac{1}{2}\\gamma(x^{2}-2\\alpha(z)x),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\gamma>0$ is arbitrarily small4. ", "page_idx": 14}, {"type": "text", "text": "and observe that, if $x_{t}^{\\mathbf{S}}$ is defined as in Eq. (19): ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\boldsymbol{x}_{t}^{\\mathbf{S}}=(1-\\gamma\\eta)\\boldsymbol{x}_{t-1}^{\\mathbf{S}}-\\gamma\\eta\\alpha(S_{t})=\\boldsymbol{x}_{t-1}^{\\mathbf{S}}-\\frac{\\eta}{|S_{t}|}\\sum_{z\\in S_{t}}\\nabla h_{z}(\\boldsymbol{x}_{t-1}^{\\mathbf{S}}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "$^4\\gamma\\leq\\varepsilon/(\\eta T)$ , will sufifce ", "page_idx": 14}, {"type": "text", "text": "To simplify notation, let us denote: ", "page_idx": 15}, {"type": "equation", "text": "$$\nw_{T+1}^{\\mathbf{S}}=w_{q}^{\\mathbf{S}},\\;\\mathrm{and}\\;x_{T+1}^{\\mathbf{S}}=x_{q}^{\\mathbf{S}},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and assume without loss of generality that $\\operatorname*{max}\\{q(i)\\,\\neq\\,0\\}=T$ (Otherwise, we look only at the sequence up to point max $\\{q(i)\\neq0\\}_{\\mathrm{.}}$ ). Consider now the sets of triplets: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{G(z)=\\left\\{(\\nu,g,u)=\\left(f(w_{t}^{\\mathbf{S}},z)+h_{z}(x_{t}^{\\mathbf{S}}),(\\nabla_{\\mathbf{S},t,z},\\nabla h_{z}(x_{t}^{\\mathbf{S}})),(w_{t}^{\\mathbf{S}},x_{t}^{\\mathbf{S}})\\right):\\mathbf{S}_{T}^{m}\\in\\mathcal{S},t\\leq T+1\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\mathcal{O}_{\\mathbf{S},T+1,z}\\in\\partial(f(w_{q}^{\\mathbf{S}})+h_{z}(x_{q}^{\\mathbf{S}}))$ is chosen arbitrarily. ", "page_idx": 15}, {"type": "text", "text": "Convexity of $f+h_{z}$ ensure that the triplets in $G(z)$ satisfy Eq. (10) for all $t\\leq T+1$ , as in Lemma 7. To apply the Lemma, we also want to achieve differentiability at points such that $t<T$ . Therefore, take any two triplets ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left(\\boldsymbol{\\nu}_{i},g_{i},\\boldsymbol{u}_{i}\\right)=\\left(f(\\boldsymbol{w}_{t_{i}}^{\\mathbf{S}},z)+h_{z}(x_{t_{i}}^{\\mathbf{S}}),\\left(\\boldsymbol{\\mathcal{O}}_{\\mathbf{S},t,z},\\nabla h_{z}(x_{t_{i}}^{\\mathbf{S}})\\right),\\left(\\boldsymbol{w}_{t_{i}}^{\\mathbf{S}},x_{t_{i}}^{\\mathbf{S}}\\right)\\right),i=1,2,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $t_{1}<T$ and $t_{2}\\leq T+1$ , and suppose $g_{1}\\neq g_{2}$ . To simplify notations, let us write $w_{t_{i}}^{\\mathbf{S}}=w_{i}$ and $x_{t_{i}}^{\\mathbf{S}}=x_{i}$ . ", "page_idx": 15}, {"type": "text", "text": "First, by convexity of $f$ we have that: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{:}-\\nu_{2}+g_{2}^{\\top}\\big(u_{2}-u_{1}\\big)=f\\big(w_{1},z\\big)+h_{z}\\big(x_{1}\\big)-f\\big(w_{2},z\\big)-h_{z}\\big(x_{2}\\big)-\\left(\\Theta_{\\mathrm{S}_{2},t_{2},z},\\nabla h_{z}\\big(x_{2}\\big)\\right)^{\\top}\\big(\\left(w_{1},x_{1}\\right)-\\left(w_{2},z\\right)\\big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=f\\big(w_{1},z\\big)-f\\big(w_{2},z\\big)-\\mathcal{O}_{\\mathrm{S}_{2},t_{2},z}^{\\top}\\left(w_{1}-w_{2}\\right)+h_{z}\\big(x_{1}\\big)-h_{z}\\big(x_{2}\\big)-\\nabla h_{z}\\big(x_{2}\\big)^{\\top}\\left(x_{1}-x_{2}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geq h_{z}\\big(x_{1}\\big)-h_{z}\\big(x_{2}\\big)-\\nabla h_{z}\\big(x_{2}\\big)^{\\top}\\left(x_{1}-x_{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Next, because $g_{1}\\neq g_{2}$ , either $\\nabla h_{z}(x_{1})\\neq\\nabla h_{z}(x_{2})$ , which implies $x_{1}\\neq x_{2}$ , or $\\mathcal{O}_{\\mathbf{S}_{1},t_{1},z}\\neq\\mathcal{O}_{\\mathbf{S}_{2},t_{2},z}$ which implies $(\\mathbf{S}_{1},t_{1})\\neq(\\mathbf{S}_{2},t_{2})$ which again implies $x_{1}\\neq x_{2}$ by Claim 1. In other words, if $g_{1}\\neq g_{2}$ then $x_{1}\\neq x_{2}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{l_{z}(x_{1})-h_{z}(x_{2})-\\nabla h_{z}(x_{2})^{\\top}\\left(x_{1}-x_{2}\\right)=\\gamma\\left(x_{1}^{2}-2\\alpha(z)x_{1}-x_{2}^{2}-2\\alpha(z)x_{2}-(2x_{1}-2\\alpha(z))\\cdot(x_{1}-x_{2})\\right)}\\\\ &{\\hphantom{l a c}=\\gamma\\left(x_{1}^{2}+x_{2}^{2}-2x_{1}\\cdot x_{2}\\right)}\\\\ &{\\hphantom{l a c}=\\gamma\\left(x_{1}-x_{2}\\right)^{2}}\\\\ &{\\hphantom{l a c}>0}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\nx_{1}\\neq x_{2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We showed then, that $\\nu_{1}-\\nu_{2}+g_{2}^{\\top}\\big(u_{2}-u_{1}\\big)=0$ , implies $g_{1}=g_{2}$ . We obtain, by Lemma 7, that there exists a function $\\bar{f}((w,x),z)$ such that for all $t$ and S: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\bar{f}(w_{t}^{\\mathbf{S}},x_{t}^{\\mathbf{S}},z)=f(w_{t}^{\\mathbf{S}},z)+h_{z}(x_{t}^{\\mathbf{S}})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and for all $t\\leq T$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\nabla\\bar{f}((w_{t}^{\\mathbf{S}},x_{t}^{\\mathbf{S}}))=(\\mathcal{O}_{\\mathbf{S},t,z},\\nabla h_{z}(x_{t}^{\\mathbf{S}})).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This proves Lemma 9. Indeed. By the Lipschitzness of $h$ in the unit ball, we have that $|x_{t}|\\leq\\gamma\\eta T$ . For sufifciently small $\\gamma$ , from Eq. (23), since $\\{(f(0,z)+h_{z}(0),(0,\\nabla h_{z}(0)),(0,0))\\}\\in G(z)$ , we obtain that ", "page_idx": 15}, {"type": "equation", "text": "$$\n|\\bar{f}((0,0),z)-f(0,z)|=\\gamma|h_{z}(0)|\\leq\\gamma^{2}\\eta T\\leq\\varepsilon.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n|\\bar{f}((w_{q}^{\\mathbf{S}},x_{q}^{\\mathbf{S}}),z)-f((w_{q}^{\\mathbf{S}},z))|=\\gamma|h_{z}(x_{q}^{\\mathbf{S}})|\\leq\\varepsilon.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Further, if we assume by induction that for every $t^{\\prime}\\leq t-1,u_{t^{\\prime}}=w_{t^{\\prime}}^{\\mathbf{S}}$ , and $x_{t^{\\prime}}=x_{t^{\\prime}}^{\\mathbf{S}}$ , then from Eq. (24) we have that for any first order oracle: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(u_{t},x_{t})=(w_{t-1}^{S},x_{t-1}^{S})-\\frac{\\eta}{|\\mathcal{S}_{t}|}\\sum_{\\underline{{\\epsilon}}\\in\\mathcal{S}_{t}}(\\eta_{t-1}^{S},x_{t-1}^{S})}\\\\ &{\\qquad=(w_{t-1}^{S},x_{t-1}^{S})-\\frac{\\eta}{|\\mathcal{S}_{t}|}\\sum_{\\underline{{\\epsilon}}\\in\\mathcal{S}_{t}}\\big(\\Phi_{t,z_{t}},\\nabla h_{z}(x_{t-1}^{S},z)\\big)}\\\\ &{\\qquad=(w_{t-1}^{S},x_{t-1}^{S})-\\frac{\\eta}{|\\mathcal{S}_{t}|}\\sum_{\\underline{{\\epsilon}}\\in\\mathcal{S}_{t}}\\Big(\\eta^{(t)}(\\mathcal{S}_{t-1},w_{t-1}^{S},z),\\nabla h_{z}(x_{t-1}^{S},z)\\Big)}\\\\ &{\\qquad=(w_{t-1}^{S},x_{t-1}^{S})-\\eta\\left(\\nabla(\\mathcal{S}_{t},w_{t-1}^{S}),\\frac{1}{|\\mathcal{S}_{t}|}\\sum_{\\overline{{\\epsilon}}\\in\\mathcal{S}_{t}}\\Gamma_{\\mu_{t-1}}(x_{t-1}^{S},z)\\right)}\\\\ &{\\qquad=\\Big(w_{t-1}^{S}-\\eta(\\mathcal{S}(\\mathbf{S},w_{t-1}^{S}),x_{t-1}^{S}-\\frac{\\eta}{|\\mathcal{S}_{t}|}\\sum_{\\overline{{\\epsilon}}\\in\\mathcal{S}_{t-1}}(\\underline{{S}}_{t-1}^{S})\\Big)}\\\\ &{\\qquad=(w_{t-1}^{S},x_{t}^{S}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which proves, by linearity, that ${u_{q}=w_{q}^{\\mathbf{S}}}$ ", "page_idx": 16}, {"type": "text", "text": "B Proof of Lemma 7 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We choose ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\hat{f}(w)=\\operatorname*{max}_{j\\in J}\\{f_{j}+g_{j}^{\\top}(w-w_{j}\\}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "$\\hat{f}$ is indeed convex as it is the maximum of linear functions. Further, it is known [19] that at each point $w$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\partial\\hat{f}(\\boldsymbol w)=\\mathrm{conv}\\{g_{j}:\\hat{f}(\\boldsymbol w)=f_{j}+g_{j}^{\\top}(\\boldsymbol w-\\boldsymbol w_{j})\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "It follows that, $\\hat{f}$ is $L$ -Lipschitz. Next, for any $w_{i}$ , notice that our assumption implies: ", "page_idx": 16}, {"type": "equation", "text": "$$\nf_{i}\\geq\\operatorname*{max}_{j\\in J}\\{f_{j}+g_{j}^{\\top}(w_{i}-w_{j}\\}=\\hat{f}(w_{i}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "On the other hand, ", "page_idx": 16}, {"type": "equation", "text": "$$\nf_{i}=f_{i}+g_{i}^{\\top}(w_{i}-w_{i})\\leq{\\hat{f}}(w_{i}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Hence ${\\hat{f}}(w_{i})=f_{i}$ . Finally, to see the function is differentiable at designated points, take any $i\\in I_{d i f f}$ and consider $w_{i}$ . By Eq. (25), it is enough to show that if $g_{j}\\in\\partial f(w_{i})$ then $g_{i}=g_{j}$ , but this clearly follows from our assumption, and the fact that ${\\hat{f}}(w_{i})=f_{i}$ . ", "page_idx": 16}, {"type": "text", "text": "C Proof of Claim 1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We first prove by induction that $x_{0}^{\\mathbf{S}}=0$ and, for $t\\geq1$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\nx_{t}^{\\mathbf{S}}=\\gamma\\eta\\sum_{z\\in Z}\\alpha(z)\\left(\\sum_{\\{t^{\\prime}\\leq t,z\\in S_{t^{\\prime}}\\}}\\frac{(1-\\gamma\\eta)^{t-t^{\\prime}}}{|S_{t^{\\prime}}|}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Indeed, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{x_{t}^{S}=x_{t-1}^{S}-\\gamma\\eta\\left(\\eta x_{t-1}^{S}-\\alpha(S_{t})\\right)}\\\\ &{\\quad=(1-\\gamma\\eta)x_{t-1}^{S}+\\gamma\\eta\\alpha(S_{t})}\\\\ &{\\quad=(1-\\gamma\\eta)\\left(\\displaystyle\\sum_{z\\in\\mathbb{Z}}\\alpha(z)\\sum_{\\{t^{\\prime}<t\\leq\\varepsilon_{t^{\\prime}}\\}^{t}}\\frac{(1-\\gamma\\eta)^{t-1-t^{\\prime}}}{|S_{t^{\\prime}}|}\\eta\\gamma\\right)+\\frac{1}{|S_{t}|}\\displaystyle\\sum_{z\\in S_{t}}\\gamma\\eta\\alpha(z)}\\\\ &{\\quad=\\displaystyle\\sum_{z\\in\\mathbb{Z}}\\alpha(z)\\sum_{\\{t^{\\prime}<t\\leq\\varepsilon_{t^{\\prime}}\\}}\\frac{(1-\\gamma\\eta)^{t-t^{\\prime}}}{|S_{t^{\\prime}}|}\\eta\\gamma+\\frac{1[z\\in S_{t}]}{|S_{t}|}\\gamma\\eta}\\\\ &{\\quad=\\eta\\gamma\\displaystyle\\sum_{z\\in\\mathbb{Z}}\\alpha(z)\\sum_{\\{t^{\\prime}<t\\leq\\varepsilon_{t^{\\prime}}\\}}\\frac{(1-\\gamma\\eta)^{t-t^{\\prime}}}{|S_{t^{\\prime}}|}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now, for every $\\mathbf{S},t,z$ , define a polynomial: ", "page_idx": 17}, {"type": "equation", "text": "$$\nP_{\\mathbf{S},t,z}(X)=\\sum_{n=0}^{t-1}\\frac{\\mathbf{1}[z\\in S_{t-n}]}{|S_{t-n}|}X^{n},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and let $r$ be a rational point, sufifcinetly small, so that $1-r$ is not the root of any polynomial of the form $P_{\\mathbf{S},t,z}-P_{\\mathbf{S}^{\\prime},t^{\\prime},z^{\\prime}}$ that is distinct from 0. In other words, we choose $r$ so that ", "page_idx": 17}, {"type": "equation", "text": "$$\nP_{S,t,z}(1-r)=P_{S^{\\prime},t^{\\prime},z^{\\prime}}(1-r)\\Leftrightarrow P_{S,t,z}(X)=P_{S^{\\prime},t^{\\prime},z^{\\prime}}(X).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and we also require that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}q(t)P_{S,t,z}(1-r)=P_{S^{\\prime},t^{\\prime},z}(1-r)\\Leftrightarrow\\sum_{t=1}^{T}q(t)P_{S,t,z}(X)=P_{S^{\\prime},t^{\\prime},z^{\\prime}}(X).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Notice that there are only finitely many polynomials of the above form, hence we can choose such $r$ in any interval $(0,\\varepsilon)$ for any $\\varepsilon>0$ . Rewriting Eq. (26) we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\nx_{t}^{\\mathbf{S}}=\\gamma\\eta\\sum_{z\\in Z}\\alpha(z)P_{\\mathbf{S},t,z}\\left(1-\\gamma\\eta\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now, suppose we choose $\\{\\alpha(z)\\}$ to be reals, independent over the rationals, and suppose we choose $\\gamma=r/\\eta$ . ", "page_idx": 17}, {"type": "text", "text": "Proof of Item 1 Assume that $x_{t}^{\\mathbf{S}}=x_{t^{\\prime}}^{\\mathbf{S}^{\\prime}}$ . Because $\\alpha(z)$ are independent over the rationals, and because $P(1-\\gamma\\eta)$ are always rationals, we have that $P_{\\mathbf{S}^{\\prime},t^{\\prime},z}(1-r)=P_{\\mathbf{S}^{\\prime},t^{\\prime},z}(1-r)$ for every $z$ . But then, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\forall z\\in Z:P_{\\mathbf{S},t,z}(X)=P_{\\mathbf{S}^{\\prime},t^{\\prime},z}(X),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "by choice of $r$ . But then, $t=t^{\\prime}$ . Indeed, assume by contradiction and w.l.og assume $t<t^{\\prime}$ : then for any $z\\in S_{1}^{\\prime}~P_{\\mathbf{S},t,z}$ is a $t^{\\prime}-1$ -degree polynomial, on the other hand $P_{\\mathbf{S},t,z}$ is at most of degree $t-1<t^{\\prime}-1$ . Next, if $S_{i}\\neq S_{i}^{\\prime}$ , for $i\\leq t$ , then we can assume (w.l.o.g) that there is $z\\in S_{i}$ such that $z\\not\\in S_{i}^{\\prime}$ it follows that, by looking at the coefifcient of the two polymials of the monomial $X^{t-i}$ we have that: ", "page_idx": 17}, {"type": "equation", "text": "$$\nP_{\\mathbf{S}^{\\prime},t^{\\prime},z}(X)\\neq P_{\\mathbf{S},t,z}(X).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Overall then, we obtain that if $x_{t}^{\\mathbf{S}}=x_{t^{\\prime}}^{\\mathbf{S}^{\\prime}}$ then $(\\mathbf{S},t)\\equiv(\\mathbf{S}^{\\prime},t^{\\prime}).$ .. ", "page_idx": 17}, {"type": "text", "text": "Proof of Item 2: The proof of Item 2 is similar to how we proved $t=t^{\\prime}$ . Notice that: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{t=1}q(t)x_{t}^{\\mathbf{S}}=\\gamma\\eta\\sum_{z\\in Z}\\alpha(z)\\sum_{t=1}^{T}q(t)P_{\\mathbf{S},t,z}(1-\\eta\\gamma).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Hence $x_{t}^{\\mathbf{S}}=x^{\\mathbf{S}^{\\prime}}$ implies for all $z\\in Z$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}q(t)P_{\\mathbf{S}^{\\prime},t,z}=P_{\\mathbf{S},t,z}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For $z\\in S_{1}^{\\prime}$ we have that $\\begin{array}{r}{\\sum_{t=1}^{T}q(t)P_{\\mathbf{S},t,z}}\\end{array}$ is a $\\operatorname*{max}\\{t:q(t)\\neq0\\}$ -degree polynomial, but on the other hand we assume that $t<\\operatorname*{max}\\{t:q(t)\\neq0\\}$ , hence $P_{\\mathbf{S},t,z}$ is a lower degree polynomial. ", "page_idx": 18}, {"type": "text", "text": "D Proof of Eqs. (16) and (17) ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "D.1 Proof of Eq. (16) ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To avoid cumbersome notations, we will supress dependence on S and write $w_{t}$ instead of $w_{t}^{\\mathbf{S}}$ . ", "page_idx": 18}, {"type": "text", "text": "As a first step, observe that at each iteration no projection is performed. Indeed, let us show by induction that: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|w_{t+1}\\right\\|^{2}=\\left\\|w_{t}-\\eta\\mathcal{O}^{(t)}(S,w,V)\\right\\|^{2}\\leq2\\eta^{2}\\alpha^{2}(t+1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The definition of $\\alpha$ then implies that $w_{t}$ are restricted to the unit ball without projections. ", "page_idx": 18}, {"type": "text", "text": "To see the above is true, let us consider the case where the first type of update is performed: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|w_{t+1}\\right\\|^{2}=\\left\\|w_{t}^{S}-\\eta^{(0)}(s)(s,w,V)\\right\\|^{2}}\\\\ &{\\qquad=\\left\\|w_{s}^{S}-\\eta w_{t,j_{1}+1}+\\eta w_{t,j_{2}}\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad\\left|\\frac{i^{\\prime}/k!}{s^{2}+1}\\right\\|}\\\\ &{\\qquad=\\displaystyle\\sum_{s=1}^{i-1}\\left(w_{t}(I_{s})\\right)^{2}+(w_{t}(I_{j})+\\eta\\alpha)^{2}+(w_{t}(I_{j+1})-\\eta\\alpha)^{2}+\\sum_{s=j_{2}+2}^{i-j_{1}}(w_{t}(I_{s}))^{2}}\\\\ &{\\qquad\\qquad\\quad\\mathrm{~|\\alpha|^{2}}\\mathbb{R}_{j}^{2}}\\\\ &{\\qquad=\\displaystyle\\sum_{s=1}^{i}\\left(w_{t}(I_{s})\\right)^{2}+2\\eta\\alpha(w_{t}(I_{j})-w_{t}(I_{j+1}))+2\\eta^{2}\\alpha^{2}}\\\\ &{\\qquad\\qquad\\quad\\mathrm{~|\\alpha|^{2}}\\mathbb{R}_{j}^{2}}\\\\ &{\\qquad=\\displaystyle\\sum_{s=1}^{i-j_{1}}\\left(w_{t}(I_{s})\\right)^{2}+2\\eta^{2}\\alpha^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\mathrm{~,~}}\\\\ &{\\qquad\\le2\\eta^{2}\\alpha^{2}\\cdot t+\\eta^{2}\\alpha^{2}}\\\\ &{\\qquad=2\\eta^{2}\\alpha^{2}\\cdot(t+1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\nw_{t}(I_{j+1})=w_{t}(I_{j})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "And if the second type of update is performed: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|w_{t+1}\\|^{2}=\\|w_{t}^{\\mathrm{S}}-\\eta\\mathcal{O}^{(t)}(S,w,V)\\|^{2}}\\\\ &{\\qquad\\qquad=\\|w_{t}^{\\mathrm{S}}+\\eta\\alpha e_{I_{j}}\\|^{2}}\\\\ &{\\qquad\\qquad=\\displaystyle\\sum_{s\\neq j}w_{t}\\left(I_{s}\\right)+\\eta^{2}\\alpha^{2}}\\\\ &{\\qquad\\qquad\\leq2\\eta^{2}\\alpha^{2}\\cdot t+\\eta^{2}\\alpha^{2}}\\\\ &{\\qquad\\qquad\\leq2\\eta^{2}\\alpha^{2}\\cdot\\left(t+1\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\nw_{t}(I_{j})=0\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We now move on to prove that Eq. (16) holds by induction. Specifically, we will show that for $d_{0}\\leq\\lfloor d^{\\prime}/k\\rfloor$ that at time $\\begin{array}{r}{T_{d_{0}}=1+\\sum_{d=0}^{d_{0}}\\sum_{k=0}^{d}k}\\end{array}$ , for any $t\\leq d_{0}$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\nw_{T_{d_{0}}}(I_{t})=\\alpha\\eta\\left\\{\\atop0\\right.\\qquad\\qquad\\quad0.\\mathrm{w.}\\quad\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Eq. (16) the\u221an follows by plugging $d_{0}=d^{\\prime}/k$ , and noting that at every step we have that if $i\\in I_{t}$ then $w_{T_{d_{0}}}(i)=\\sqrt{k}w_{T_{d_{0}}}(I_{t})$ . Therefore, we are left with proving Eq. (27). ", "page_idx": 18}, {"type": "text", "text": "For $d_{0}=0$ , $T_{0}=1$ and we have, indeed, $w_{1}=0$ . Next assume we proved the statement for $d_{0}$ , and we will prove it for $d_{0}+1$ . Here too, we will use induction, and we prove that, for $d_{1}\\leq d_{0}+1$ , at time ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\cal T}_{d_{0},d_{1}}={\\cal T}_{d_{0}}+\\sum_{k=0}^{d_{1}-1}(d_{0}+1-k),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "we have that: ", "page_idx": 19}, {"type": "equation", "text": "$$\nw_{T_{d_{0},d_{1}}}(I_{t})=\\alpha\\eta\\left\\{\\atop0\\atop0}(d_{0}+2-t)\\right.\\ \\ t\\leq d_{1}}\\\\ {\\quad w_{T_{d_{0},d_{1}}}(I_{t})=\\alpha\\eta\\left\\{\\atop0\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\!\\!\\!\\!\\alpha\\eta\\ \\ \\ \\ \\ \\ \\ \\ }\\right.}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For the case $d_{1}{=}0$ , $T_{d_{0},d_{1}}=d_{0}$ , and it follows from our outer-induction step. Assume the statement is true for $d_{1}$ and we will prove it for $d_{1}+1$ , here, yet again, we use induction. And we will show that for $1\\leq d_{2}<d_{0}+2-d_{1}$ we have at time ", "page_idx": 19}, {"type": "equation", "text": "$$\nT_{d_{0},d_{1},d_{2}}=T_{d_{0}}+T_{d_{1}}+d_{2},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "we have that: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{w_{T_{d_{0},d_{1},d_{2}}}(I_{t})=\\alpha\\eta\\left\\{\\begin{array}{l l}{(d_{0}+2-t)}&{t\\leq d_{1}}\\\\ {(d_{0}+1-t)}&{d_{1}<t<d_{0}+2-d_{2}}\\\\ {(d_{0}+2-t)}&{t=d_{0}+2-d_{2}}\\\\ {(d_{0}+1-t)}&{d_{0}+2-d_{2}<t\\leq d_{0}}\\\\ {0}&{0.\\mathrm{w}.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We start the induction with the case $d_{2}=1$ , in that case notice that $T_{d_{0},d_{1},d_{2}}=T_{d_{0},d_{1}}+1$ , and by induction hypothesis: ", "page_idx": 19}, {"type": "equation", "text": "$$\nw_{T_{d_{0},d_{1}}}(I_{t})=\\alpha\\eta\\left\\{\\atop0\\atop0}^{\\left(d_{0}+2-t\\right)}\\right.\\ \\ t\\leq d_{1}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In this case, note that there are no two consecutive coordinates that are equal, hence our choice of oracle is defined so that $\\mathcal{O}^{(t)}=-e_{I_{d_{0}+1}}$ . Hence, by our update rule (and the lack of projections which we proved at the beginning): ", "page_idx": 19}, {"type": "equation", "text": "$$\nw_{T_{d_{0},d_{1},1}}(I_{t})=\\alpha\\eta\\left\\{\\begin{array}{l l}{(d_{0}+2-t)}&{t\\leq d_{1}}\\\\ {(d_{0}+1-t)}&{d_{0}<t\\leq d_{0}}\\\\ {1}&{t=d_{0}+1}\\\\ {0}&{\\mathrm{o.w.}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Which satisfies Eq. (28). Now assume that Eq. (28) holds for $d_{2}$ , and take $d_{2}+1\\,<\\,d_{0}+2-d_{1}$ (otherwise, we are done). Notice that $T_{d_{0},d_{1},d_{2}+1}\\;=\\;T_{d_{0},d_{1},d_{2}}\\;+\\;1$ . Observe that $w_{T_{0},d_{1},d_{2}}(d_{0}+$ $2-d_{2})=w_{T_{0},d_{1},d_{2}}(d_{0}+1-d_{2})$ (notice that $d_{0}+1-d_{2}>d_{1}^{\\prime}$ ), and our update rule is such that $\\mathcal{O}^{(t)}=e_{I_{d_{0}+2-d_{2}}}-e_{I_{d_{0}+1-d_{2}}}$ and we obtain then: ", "page_idx": 19}, {"type": "equation", "text": "$$\nv_{T_{d_{0},d_{1},d_{2}+1}}=w_{T_{d_{0},d_{1},d_{2}}}-\\eta\\alpha e_{I_{d_{0}+2-d_{2}}}+\\eta\\alpha e_{I_{d_{0}+1-d_{2}}}=\\alpha\\eta\\left\\{\\begin{array}{l l}{(d_{0}+2-t)}&{t\\leq d_{1}}\\\\ {(d_{0}+1-t)}&{d_{1}<t<d_{0}+2-(d_{2}+1)}\\\\ {(d_{0}+2-t)}&{t=d_{0}+2-(d_{2}+1)}\\\\ {(d_{0}+1-t)}&{d_{0}+2-(d_{2}+1)<t\\leq d_{0}}\\\\ {0}&{0.\\mathrm{w}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The most inner induction step is now complete. We now notice that ${\\cal T}_{d_{0},d_{1},d_{0}+1-d_{1}}={\\cal T}_{d_{0},d_{1}+1}.$ , which proves the middle-induction step. And we notice that $T_{d_{0},d_{0}+1}\\,=\\,T_{d_{0}+1}$ , which proves the whole induction argument. ", "page_idx": 19}, {"type": "text", "text": "D.2 Proof of Eq. (17) ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We only need to show that the following quantity is increasing in $t$ ", "page_idx": 19}, {"type": "equation", "text": "$$\nX_{t}=\\operatorname*{max}\\left\\{\\sum_{i\\in I_{B}}w_{t}^{\\mathbf{S}}(i):I_{B}\\subseteq I,|I|=B\\right\\}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "But, as shown in Appendix D, the update rule is such that we don\u2019t perform projections. It then follows easily from our update step. Indeed if we increase a set of coordinates by $\\alpha\\eta$ , then clearly the $X_{t}$ only increases. Also, if we perform update of the form: ", "page_idx": 20}, {"type": "equation", "text": "$$\nw_{t}^{\\mathbf{S}}(I_{j})=w_{t-1}^{\\mathbf{S}}(I_{j})+\\alpha\\eta,\\quad w_{t}^{\\mathbf{S}}(I_{j+1})=w_{t}^{\\mathbf{S}}(I_{j+1})-\\alpha\\eta,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for two consecutive and equal coordinates, then for any $I_{B}$ : If $I_{B}$ includes same number of coordinates from $I_{j}$ as in $I_{j+1}$ then the magnitude doesn\u2019t change. If $I_{B}$ contains more $I_{j}$ then it increases, and if $I_{B}$ contains more from $i_{j+1}$ then consider $I_{B^{\\prime}}$ that swaps coordinates in $I_{j}$ with $I_{j+1}$ then we clearly have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{i\\in I_{B^{\\prime}}}w_{t}^{\\mathbf{S}}(i)>\\sum_{i\\in I_{B^{\\prime}}}w_{t-1}^{\\mathbf{S}}(i)>\\sum_{i\\in I_{B}}w_{t}^{\\mathbf{S}}(i)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "E Dimension independent lower bound for GD ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section we prove that the optimization error of GD in Eq. (5) is optimal. The lower bound is an optimization error for first order methods and is well established (see [10]). The point here is to show that the bound is valid in any dimension, for GD. ", "page_idx": 20}, {"type": "text", "text": "Claim 2. For every choice of $\\eta,T$ , there exists a convex and 1 Lipschitz function $f(x):\\mathbb{R}\\to\\mathbb{R}$ such that, if we run GD on $f$ , then ", "page_idx": 20}, {"type": "equation", "text": "$$\nf(w^{G D})-f(0)\\geq\\frac{\\eta}{2}+\\frac{1}{6\\eta T}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. We divide the proof into two cases: ", "page_idx": 20}, {"type": "text", "text": "case1: $\\eta\\geq1/\\eta T$ ", "page_idx": 20}, {"type": "text", "text": "In this case we choose $f(x)=\\left|x-\\gamma\\right|$ , where $\\gamma>0$ is a arbitrarily small (may depend on $\\eta$ ). It can be seen that for every even iteration, we have that: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\nabla f(x_{2t})=-1,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "at at every odd iteration ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\nabla f(x_{2t+1})=+1.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "As such, for every even iteration we have that $x_{2t}=0$ , and at every odd iteration we have that $x_{2t+1}=\\eta$ . We thus have that $\\begin{array}{r}{x^{G D}=\\frac{\\eta}{2}}\\end{array}$ and ", "page_idx": 20}, {"type": "equation", "text": "$$\nf(x^{G D})-0=\\frac{\\eta}{2}-0\\geq\\frac{\\eta}{4}+\\frac{1}{2\\eta T}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "case1: \u03b7 \u22641/\u03b7\ud835\udc47\u22641 : In this case choose \ud835\udc53(\ud835\udc65) = \u03b1\ud835\udc65, where \u03b1 = (\ud835\udc47+11)\u03b7 . Then, one can show that GD outputs ", "page_idx": 20}, {"type": "equation", "text": "$$\nx^{G D}=-\\frac{\\eta}{T}\\sum t\\cdot\\alpha=-\\frac{(T+1)\\eta}{2}\\alpha,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and: ", "page_idx": 20}, {"type": "equation", "text": "$$\nf(x^{G D})-f(-1)=\\alpha-\\frac{(T+1)\\eta}{2}\\alpha^{2}\\geq\\frac{1}{2(T+1)\\eta}\\geq\\frac{1}{3T\\eta}\\geq\\frac{1}{6T\\eta}+\\frac{\\eta}{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efifciency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might sufifce, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufifcient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufifcient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 24}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 25}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efifciency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}]