[{"figure_path": "CrADAX7h23/figures/figures_3_1.jpg", "caption": "Figure 1: Overview of DAGER. DAGER first recovers the sets of client tokens T* at each position i \u2208 P by testing each token in the vocabulary V via a span check based on the client gradients of the first self-attention. Then it recursively combines them into correct sequences S via the gradients of the second self-attention.", "description": "This figure illustrates the overall working mechanism of DAGER.  The process starts with the client's token embeddings which are processed through multiple layers of self-attention within a Transformer model. The server only receives the gradients of these layers.  DAGER leverages a low-rank property of the self-attention gradients to check which tokens were used by the client. This \u2018span check\u2019 is first performed on the first layer of self-attention, revealing the possible tokens at each position. Then, these tokens are used in a recursive process to create partial sequences.  The second layer of self-attention gradient information is used to filter these sequences, resulting in the exact recovery of the client's input sequence. The figure shows multiple possible sequence generation scenarios and how the algorithm eliminates incorrect sequences until the correct one is identified.", "section": "4 Overview of DAGER"}, {"figure_path": "CrADAX7h23/figures/figures_5_1.jpg", "caption": "Figure 2: Effect of L1 and L2 Filtering", "description": "This figure shows the impact of applying a filtering method based on the low rank structure of gradients (L1 filtering) and an additional filtering step using the gradients of the second self-attention layer (L1+L2 filtering).  It demonstrates how these filtering techniques narrow down the number of candidate tokens to a much smaller subset compared to the ground truth number of tokens.  The x-axis represents the threshold used for filtering, while the y-axis shows the number of tokens remaining after filtering. The figure suggests that the combined L1+L2 filtering is more effective at identifying the correct tokens than L1 filtering alone. The effectiveness of the filtering methods is particularly evident at smaller threshold values.", "section": "5.2 Recovering Token Sets"}, {"figure_path": "CrADAX7h23/figures/figures_6_1.jpg", "caption": "Figure 3: Encoder Ablation Study", "description": "This ablation study visualizes the effect of different rank thresholds (\u0394b) on the reconstruction accuracy of the DAGER algorithm for encoder architectures.  It shows how varying the rank threshold impacts the ability of the algorithm to accurately recover tokens from gradients of different sizes.  The dotted line represents the embedding dimension of the GPT-2 model, highlighting a threshold beyond which accurate reconstruction becomes less likely.", "section": "5.3 Recovering Sequences"}, {"figure_path": "CrADAX7h23/figures/figures_6_2.jpg", "caption": "Figure 4: Encoder Ablation Study", "description": "This ablation study compares the performance of DAGER on an encoder-based model (BERT) with different batch sizes (B=1, B=4) and with/without heuristics. The results show that DAGER performs near perfectly when B=1, achieving almost 100% accuracy across all tested sequence lengths. However, its performance decreases significantly as the batch size increases to B=4, especially without heuristics. The use of heuristics substantially improves the accuracy for B=4, indicating their importance in handling larger batch sizes in the encoder setting.", "section": "6 Experimental Evaluation"}]