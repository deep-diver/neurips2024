{"references": [{"fullname_first_author": "Ligeng Zhu", "paper_title": "Deep leakage from gradients", "publication_date": "2019-12-01", "reason": "This paper introduced the concept of gradient leakage attacks, which forms the basis for the current research on gradient inversion attacks."}, {"fullname_first_author": "Jonas Geiping", "paper_title": "Inverting gradients-how easy is it to break privacy in federated learning?", "publication_date": "2020-12-01", "reason": "This paper is a foundational work on gradient inversion attacks, providing a benchmark against which subsequent attacks are measured."}, {"fullname_first_author": "Jieren Deng", "paper_title": "TAG: Gradient attack on transformer-based language models", "publication_date": "2021-12-01", "reason": "This paper is one of the first to explore gradient inversion attacks in the context of transformer-based language models, specifically targeting the text domain."}, {"fullname_first_author": "Mislav Balunovi\u0107", "paper_title": "LAMP: extracting text from gradients with language model priors", "publication_date": "2022-12-01", "reason": "This paper significantly improved upon previous text-based gradient inversion attacks, achieving higher reconstruction quality and scaling to larger batch sizes."}, {"fullname_first_author": "Dimitar I Dimitrov", "paper_title": "Spear: Exact gradient inversion of batches in federated learning", "publication_date": "2024-03-01", "reason": "This paper provides a strong theoretical foundation for exact gradient inversion, introducing a low-rank analysis of self-attention gradients that is directly leveraged by the current work."}]}