[{"figure_path": "65htepluYE/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison of images generated before/after training adaptors over concepts with different CMMD scores. One observation is that concepts with higher CMMD scores are notably more challenging for the underlying model to generate (the second row). Additionally, we also notice that a higher CMMD value leads to a more notable loss of visual details when training adaptors (the third row).", "description": "This figure compares images generated before and after applying different adaptor modules (LoRA) to enhance the generation of out-of-distribution (OOD) concepts. The comparison highlights the challenge of accurately depicting visual details, especially for concepts with high CMMD (Maximum Mean Discrepancy) scores, which indicate a larger discrepancy between the model's learned distribution and the distribution of the OOD concepts. It demonstrates that while adaptors can improve the overall representation of OOD concepts, they often struggle to accurately capture fine details like texture, contours, and patterns.", "section": "OOD Problem in Latent Diffusion Models"}, {"figure_path": "65htepluYE/figures/figures_2_1.jpg", "caption": "Figure 2: Comparison of synthetic results on data with different quality. Generated images are significantly influenced by the quality of training data. If the training data includes disruptive objects, the generative images may include disruptive visual details (Left). When an object within the image is too small, the results may not accurately represent the intended concepts (Middle). In contrast, if the image contains a high-fidelity object without disruptive elements (Right), the model is more likely to generate the desired result accurately.", "description": "This figure demonstrates how the quality of training data impacts the results of generating images of out-of-distribution (OOD) concepts using different adaptation methods.  The left column shows examples where disruptive elements in the training data lead to similar artifacts in the generated images. The middle column shows that training data with small or vague instances of the concept result in low-quality generated images.  In contrast, the right column illustrates how high-quality training data (clear, single object, distinct from the background) leads to accurate and high-fidelity generated images.", "section": "2 OOD Problem in Latent Diffusion Models"}, {"figure_path": "65htepluYE/figures/figures_3_1.jpg", "caption": "Figure 3: The overall pipeline of CATOD. In brief, CATOD alternatively performs data selection and scheduled OOD concept adaption. In each training cycle, we first generate OOD concepts according to the current adaptor and calculate the weights for the aesthetic score and concept-matching score. Then, we calculate the weighted score for each sample within the data pool Dpool, select the top images accordingly, and add them to the training pool. At last, CATOD fine-tunes the scoring system and training adaptors according to the updated data pool, and proceed to the next cycle. The above three steps alternatively proceed until convergence.", "description": "The figure illustrates the iterative process of the CATOD framework.  It starts by generating OOD concepts using the current adaptor, then calculating weighted scores (combining aesthetic and concept-matching scores) to select high-quality training samples. These samples are added to the training pool, and the adaptor and scoring system are fine-tuned. This cycle repeats until convergence, improving the generation of OOD concepts.", "section": "3 Method"}, {"figure_path": "65htepluYE/figures/figures_7_1.jpg", "caption": "Figure 4: A comparison of different sampling strategies with LoRA. Specifically, we compare three lines of works: (1) RAND, in which the model is trained with 100 randomly selected samples; (2) with samples of the highest CLIP scores (100 samples); (3) 100 samples with CATOD. The model trained with randomly sampled data fails to capture the features of out-of-distribution (OOD) concepts, while the ones trained with top CLIP scores contain necessary details but also include disruptive elements.", "description": "This figure compares the image generation results of three different sampling methods (random sampling, top CLIP score sampling, and CATOD) when using the LoRA adapter.  It visually demonstrates how CATOD outperforms the other methods by producing images that accurately depict the target concepts while avoiding the addition of unwanted details often present in the other sampling methods.", "section": "5 Experiments"}, {"figure_path": "65htepluYE/figures/figures_8_1.jpg", "caption": "Figure 5: Generative results as cycle proceeds. Samples are generated with CATOD on cycles from 1 to 7. To better observe how generated images change as the cycle proceeds, we conduct another 2 cycles here. In each cycle, we select and add 20 high-quality samples. Generative samples start to converge and contain the right details within the original concept after cycle 4 or 5. We can also see that those generative results contain diverse contents within the background based on the few images given.", "description": "This figure shows the iterative process of generating images of emperor penguin chicks and axolotls using the CATOD method.  Each column represents a different cycle of the process, with 20 high-quality samples added in each cycle. The images become increasingly realistic and accurately represent the target concepts over time, highlighting the effectiveness of the method's iterative refinement process.", "section": "5 Experiments"}, {"figure_path": "65htepluYE/figures/figures_19_1.jpg", "caption": "Figure 6: An intuitive comparison for fixed/dynamic schedules. The active learning paradigm can be viewed as guiding the iterative embedding updating through newly added samples. We can see that a fixed schedule makes the learned embedding heavily biased, which in turn leads to performance fluctuation.", "description": "This figure provides an intuitive comparison between fixed and dynamic schedules in active learning. The x-axis represents the latent variable, while the y-axis shows the embedding quality. The dynamic schedule shows an iterative approach where the learning process adapts to newly added samples, resulting in a smoother and more consistent approach towards achieving high embedding quality. In contrast, the fixed schedule demonstrates a lack of adaptation and results in an imbalanced and fluctuating path to high embedding quality. The ideal image manifold, presented as a curve, represents the desired quality of embeddings. The figure illustrates how a dynamic schedule, by adapting to newly added samples, progresses more efficiently towards the ideal image manifold, compared to a fixed schedule that shows heavier bias and performance fluctuation.", "section": "3 Method"}, {"figure_path": "65htepluYE/figures/figures_22_1.jpg", "caption": "Figure 7: Generative results with 2 concepts within one image. Experiments are conducted based on the LORA adaptor fully trained on concepts \"Frilled Lizard\" and \"Emperor Penguin Chick\". We try to compose these creatures with background elements, in-distribution concepts, and out-of-distribution concepts learned by other adaptors. The final results show high quality with minimal disruptive details.", "description": "This figure shows the results of generating images with two concepts combined in a single image. The model used a LORA adaptor trained on \"Frilled Lizard\" and \"Emperor Penguin Chick\". The images demonstrate the model's ability to generate high-quality images with minimal artifacts, even when combining multiple concepts, including background elements, in-distribution concepts, and out-of-distribution concepts from other adaptors.", "section": "5 Experiments"}, {"figure_path": "65htepluYE/figures/figures_22_2.jpg", "caption": "Figure 8: A comparison on how the schedule and scores change on RAND(scheduled) and CATOD as cycle proceeds on concept emperor penguin(chick). (a),(b) show how the #epochs for each learning rate in the schedule change as the cycle proceeds, when (c), (d) show how aesthetic/concept-matching/comprehensive score change on RAND (scheduled) and CATOD. The scores for CATOD stop changing at cycle 12 since more added samples do not help boost adaptor quality.", "description": "This figure compares the training schedule and score changes between RAND and CATOD for the 'emperor penguin (chick)' concept.  Panels (a) and (b) show how the number of epochs used for different learning rates in the training schedule changes across cycles. Panels (c) and (d) illustrate the changes in aesthetic, concept-matching, and comprehensive scores across cycles for RAND and CATOD respectively.  The results demonstrate that CATOD's schedule stabilizes earlier than RAND's, indicating more efficient training.", "section": "C Experiments"}, {"figure_path": "65htepluYE/figures/figures_23_1.jpg", "caption": "Figure 4: A comparison of different sampling strategies with LoRA. Specifically, we compare three lines of works: (1) RAND, in which the model is trained with 100 randomly selected samples; (2) with samples of the highest CLIP scores (100 samples); (3) 100 samples with CATOD. The model trained with randomly sampled data fails to capture the features of out-of-distribution (OOD) concepts, while the ones trained with top CLIP scores contain necessary details but also include disruptive elements.", "description": "This figure compares the performance of three different sampling strategies when training a LoRA adapter for out-of-distribution (OOD) concepts: random sampling (RAND), top CLIP score sampling, and the proposed CATOD method.  The results show that random sampling fails to capture the essential visual details of OOD concepts, while top CLIP score sampling, although capturing some details, still includes disruptive elements. In contrast, CATOD effectively selects high-quality samples, leading to improved generative results.", "section": "5 Experiments"}, {"figure_path": "65htepluYE/figures/figures_23_2.jpg", "caption": "Figure 4: A comparison of different sampling strategies with LoRA. Specifically, we compare three lines of works: (1) RAND, in which the model is trained with 100 randomly selected samples; (2) with samples of the highest CLIP scores (100 samples); (3) 100 samples with CATOD. The model trained with randomly sampled data fails to capture the features of out-of-distribution (OOD) concepts, while the ones trained with top CLIP scores contain necessary details but also include disruptive elements.", "description": "This figure compares the image generation results of three different sampling strategies for training LoRA adaptors on out-of-distribution (OOD) concepts. The strategies are random sampling (RAND), top CLIP score sampling, and the proposed CATOD method. The results show that CATOD produces higher-quality images with accurate visual details and fewer disruptive elements compared to the other two methods.", "section": "5 Experiments"}, {"figure_path": "65htepluYE/figures/figures_25_1.jpg", "caption": "Figure 9: A comparison of selected and generate samples on different combinations of methods and concepts. We can observe that training samples with different angles selected by CATOD also lead to diverse angle in their generative results.", "description": "This figure shows a comparison of the image selection and generation results for different combinations of methods (LoRA + CLIP vs. LoRA + CATOD) and concepts (Axolotl and Emperor Penguin Chick). The \"Selection\" column displays the training samples selected by each method, highlighting that CATOD selects samples with varied angles. The \"Generation\" column shows the generated images resulting from those selections. The results demonstrate that using CATOD in the training process leads to generated images with more diverse angles, showcasing the method's effectiveness in capturing diversity.", "section": "C.7 About the diversity"}]