[{"type": "text", "text": "On the Comparison between Multi-modal and Single-modal Contrastive Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Wei Huang\\*+ Andi Han\\* Yongqiang Chen RIKEN AIP RIKEN AIP The Chinese University of Hong Kong wei.huang. vr@riken.jp andi.han@riken.jp yqchen@cse.cuhk.edu.hk ", "page_idx": 0}, {"type": "text", "text": "Yuan Cao Zhiqiang Xut The University of Hong Kong MBZUAI yuancao@hku.hk zhiqiang.xu@mbzuai.ac.ae ", "page_idx": 0}, {"type": "text", "text": "Taiji Suzuki University of Tokyo & RIKEN AIP taiji@mist.i.u-tokyo.ac.jp ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Multi-modal contrastive learning with language supervision has presented a paradigm shift in modern machine learning. By pre-training on a web-scale dataset, multi-modal contrastive learning can learn high-quality representations that exhibit impressive robustness and transferability. Despite its empirical success, the theoretical understanding is still in its infancy, especially regarding its comparison with single-modal contrastive learning. In this work, we introduce a feature learning theory framework that provides a theoretical foundation for understanding the differences between multi-modal and single-modal contrastive learning. Based on a data generation model consisting of signal and noise, our analysis is performed on a ReLU network trained with the InfoMax objective function. Through a trajectory-based optimization analysis and generalization characterization on downstream tasks, we identify the critical factor, which is the signal-to-noise ratio (SNR), that impacts the generalizability in downstream tasks of both multi-modal and single-modal contrastive learning. Through the cooperation between the two modalities, multi-modal learning can achieve better feature learning, leading to improvements in performance in downstream tasks compared to single-modal learning. Our analysis provides a unified framework that can characterize the optimization and generalization of both single-modal and multi-modal contrastive learning. Empirical experiments on both synthetic and real-world datasets further consolidate our theoretical findings. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large-scale pre-trained models have achieved unprecedented success, including GPT series [6, 41], LLaMa [53], among many others. CLIP [42] as a typical example, uses a multi-modal contrastive learning framework to learn from a massive scale of image-caption data. The multi-modal contrastive learning in CLIP has shown significant capabilities to learn high-quality representations, which are ready to be adapted to a wide range of downstream tasks, forming the backbone of generative models like DALL-E2 [43], prompt learning [61] as well as general purpose multi-modal agents [62, 35]. Given the huge success of models like CLIP that have stellar zero-shot and few-shot capabilities on a wide range of out-of-distribution (OOD) benchmarks, they have been widely recognized as foundation models (FMs). More similar examples are given by ALIGN [28], Florence [59], BLIP [33], Flamingo [1]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Despite the unprecedented success achieved by multi-modal contrastive learning, the fundamental mechanism that leads to greater performance, especially compared to single-modal contrastive learning is still under-explored. Recently, several seminal works provided theoretical explanations for either single-modal [4, 5, 14, 48, 24, 7, 50, 49, 20] or multi-modal contrastive learning [38, 37, 44, 12]. For example, [56] studied how single-modal contrastive learning learns the feature representations for neural networks by analyzing its feature learning process. As for multi-modal contrastive learning, [12, 58] provided explanations for why multi-modal contrastive learning demonstrates zero-shot transferability, and robustness to distribution shifts, than supervised learning, which offer valuable insights. Although both lines of the existing works provide valid theoretical insights under the respective settings, rare work has compared the optimization and generalization of the two types of contrastive learning under a unified framework. This motivates us to establish a systematic feature learning analysis for both single-modal and multi-modal contrastive learning. ", "page_idx": 1}, {"type": "text", "text": "In particular, we consider a data generation model that contains two modalities of data, which are generated from signal and noise features. The signal feature correlates in different modalities, while there is no correlation between noise features among modalities. We then study the optimization of single-modal and multi-modal contrastive learning under gradient descent training. By studying the trajectories of signal learning and noise memorization, we establish the convergence conditions and further characterize the generalization ability in the downstream tasks. The results show that, through the cooperation between modalities, multi-modal contrastive learning can achieve better generalization in the downstream task. In contrast, without the help of the second modality, singlemodal contrastive learning concentrates on learning noise from the data, and thus generalizes poorly on the downstream tasks. The main contributions of this work are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u00b7 This work establishes the first systematic comparative optimization analysis for single-modal and multi-modal contrastive learning under gradient descent training in non-convex settings. We show that both single-modal and multi-modal can achieve near-zero training error under InfoMax contrastive loss after polynomial number of iterations, by overcoming the non-convex difficulty. \u00b7 By a trajectory-based analysis of the signal learning and noise memorization of the ReLU network from the data, we successfully characterize the difference in generalization between single-modal and multi-modal contrastive learning. The distinct SNRs of different modalities lead to a divergence in the generalization of downstream tasks for the two contrastive learning frameworks. \u00b7 Our theory suggests that the advantage of multi-modal over single-modal contrastive learning comes from the high quality of the second modality and the cooperation between the two modalities through contrastive learning. This divergence is ultimately reflected in the difference in feature learning and the final gap in downstream task generalization. Experimental results on both synthetic and real-world datasets confirm our theoretical findings and understanding. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Theoretical Understanding of Single-modal Contrastive Learning. The seminal work [4] started theoretical research on single-modal contrastive learning. They assumed that different positive samples are independently drawn from the same latent class, making a connection to supervised learning. [55] identified two key properties related to the contrastive loss: alignment and uniformity. Alongside, [32] illustrated that predicting auxiliary prediction tasks helps in learning representations effective for downstream prediction tasks, and [52] provided a theoretical analysis of contrastive learning in the multi-view setting. Besides, [51] proposed a theoretical framework to understand contrastive self-supervised learning from an optimization perspective. [21] proposed a loss that performs spectral decomposition on the population augmentation graph and can be succinctly written as a contrastive learning objective on neural net representations. [46] pointed out the importance of inductive biases of the function class and training algorithm in understanding contrastive learning. The most related work to us is the work by [56]. Similar to them, this work studies ReLU networks and considers the signal-noise data model. However, we do not require the adjustable bias term in the activation function, which plays a critical role in [56]. Furthermore, this work adopts a unified framework to compare with multi-modal contrastive learning, which is out of scope in [56]. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Understanding of Multi-modal Contrastive Learning. As the multi-modal contrastive learning approaches such as CLIP received great success, recent works have been proposing explanations from empirical perspective. [36] empirically showed that high train-test similarity is insufficient to explain CLIP's OOD performance. [6O] illustrated that CLIP behaves similarly to Bags-of-words in language-based image retrieval, i.e., the order of words in the input sentence does not largely affect CLIP to find the corresponding image. Besides, [16] demonstrated that training data diversity and the ability to leverage the diversity as supervised learning is the key to the effective robustness of CLIP. Theoretically, [13] proved that multi-modal contrastive learning can block-identify latent factors shared between modalities by the a generative data model. [44] analyzed the training dynamics of a simple multi-modal contrastive learning model and show that contrastive pairs are important for the model to efficiently balance the learned representations. Furthermore, [38] showed that each step of loss minimization by gradient descent can be seen as performing SVD on a contrastive cross-covariance matrix. Similar to us, [25] tried to answer why multi-modal learning is better than single model learning. However, they did not consider contrastive learning and thus cannot explain the success of multi-modal contrastive multi-modal learning. ", "page_idx": 2}, {"type": "text", "text": "Data Quality Matters for Multi-modal Contrastive Learning. Aligned with our theoretical results, there is a lot of empirical evidence showing that improving the alignment quality with more descriptive captions improves multi-modal contrastive learning. [16] show that the training distribution mostly determines the generalizability of CLIP. Furthermore, [47, 40, 18, 17] find filtering poorly aligned image-caption samples used for training leads to further improvements. Besides, [45, 39, 15] demonstrate that improving the descriptiveness of the captions could further boost the performance of CLIP. Besides, [34] demonstrated that the caused by a combination of model initialization and contrastive learning optimization. However, their results do not take neural network architecture into consideration, and do not provide an analysis of test errors either. ", "page_idx": 2}, {"type": "text", "text": "3 Problem Setting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notation.  We use bold-faced letters for vectors and matrices otherwise representing scalar. We use $\\Vert\\cdot\\Vert_{2}$ to denote the Euclidean norm of a vector or the spectral norm of a matrix, while denoting $\\|\\cdot\\|_{F}$ as the Frobenius norm of a matrix. For a neural network, we denote $\\sigma(\\cdot)$ as the activation function and we adopt ReLU activation where $\\sigma(x)=\\operatorname*{max}\\{0,x\\}$ in this work. To simplify, we denote $[n]=\\{1,2,\\bar{.}\\bar{.}\\,.\\,,n\\}$ ", "page_idx": 2}, {"type": "text", "text": "Data Model. In this work, we consider the following data model, which consists of signal and noise. In the first modality, example $(\\mathbf{x},y)\\sim\\mathcal{D}$ is generated as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{x}=[\\mathbf{x}^{(1)^{\\top}},\\mathbf{x}^{(2)^{\\top}}]^{\\top}=[y\\pmb{\\mu}^{\\top},\\pmb{\\xi}^{\\top}]^{\\top},\\quad y\\sim\\mathrm{unif}(\\{-1,1\\}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\textbf{x}\\in\\mathbb{R}^{2d}$ is the input feature and $y\\,\\in\\,\\{-1,1\\}$ is the corresponding label generated from Rademacher distribution. In particular, $\\mathbf{x}^{(1)}\\,=\\,y\\pmb{\\mu}\\,\\in\\,\\mathbb{R}^{d}$ is the task-relevant signal vector, and $\\mathbf{x}^{(2)}\\,=\\,\\pmb{\\xi}\\,\\sim\\mathcal{N}(\\mathbf{0},\\sigma_{\\xi}^{2}\\mathbf{I})\\,\\in\\,\\mathbb{R}^{\\bar{d}}$ is the task-irrelevant noise vector. Intuitively, if a network learns primarily from signal, it can effectively generalize to unseen data and vice versa. Similar data models have been adopted in recent theoretical works on supervised learning [2, 26, 8, 23, 31, 63, 22, 11] and self-supervised learning [56, 50, 30]. ", "page_idx": 2}, {"type": "text", "text": "Similarly for the second modality, a sample $(\\widetilde{\\mathbf{x}},y)\\sim\\widetilde{\\cal D}$ is generated as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\widetilde{\\mathbf{x}}=[\\widetilde{\\mathbf{x}}^{(1)\\top},\\widetilde{\\mathbf{x}}^{(2)\\top}]^{\\top}=[y\\widetilde{\\pmb{\\mu}}^{\\top},\\widetilde{\\pmb{\\xi}}^{\\top}]^{\\top},\\quad y\\sim\\mathrm{unif}(\\{-1,1\\}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the input feature $\\widetilde{\\mathbf{x}}\\in\\mathbb{R}^{2\\widetilde{d}}$ and the label $y$ is shared with the first modality. Besides, the signal is a given vector $\\widetilde{\\pmb{\\mu}}\\in\\mathbb{R}^{\\tilde{d}}$ , and noise follows $\\widetilde{\\pmb{\\xi}}\\sim\\mathcal{N}(\\mathbf{0},\\sigma_{\\widetilde{\\xi}}^{2}\\mathbf{I})\\,\\in\\,\\mathbb{R}^{\\widetilde{d}}$ .The linear data models for multi-modal learning have also been studied in previous work [44]. To simplify the analysis, we set $d=\\widetilde{d}$ $\\sigma_{\\xi}=\\sigma_{\\xi}$ However, we highlight that extensions to deal with unmatched dimension and noise level is possible. ", "page_idx": 2}, {"type": "text", "text": "3.1  Single-modal Contrastive Learning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We use a single-layer neural network $\\mathbf{h}:\\mathbb{R}^{2d}\\rightarrow\\mathbb{R}^{m}$ with ReLU activation as our encoder, where $m$ is the number of neurons, which represents the embedding dimension. More precisely, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{h}(\\mathbf{x})=[\\bar{h}_{1}(\\mathbf{x}),\\allowbreak\\dots,\\bar{h}_{m}(\\mathbf{x})]^{\\top}\\in\\mathbb{R}^{m},\\quad\\mathrm{where~}\\bar{h}_{r}(\\mathbf{x})=h_{r}(\\mathbf{x}^{(1)})+h_{r}(\\mathbf{x}^{(2)}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "here we let $h_{r}(\\mathbf{x}^{(i)})=\\sigma(\\langle\\mathbf{w}_{r},\\mathbf{x}^{(i)}\\rangle)$ for $r\\in[m]$ \uff0c $i\\in$ [2], and $\\sigma(\\cdot)$ is the ReLU activation function. We adopt a Gaussian to initialize the weights $\\mathbf{w}_{r}^{(0)}\\sim\\mathcal{N}(\\mathbf{0},\\sigma_{0}^{2}\\mathbf{I})$ , where $\\sigma_{0}$ severs as the strength. Given a pair of positive data samples, the contrastive loss function is based on the similarity measure defined as the inner product between the representation of two samples > $\\mathbf{c},\\mathbf{x}^{\\prime}\\in\\mathbb{R}^{2d}$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{Sim}_{\\mathbf{h}}(\\mathbf{x},\\mathbf{x}^{\\prime})=\\frac{1}{m}\\sum_{r=1}^{m}h_{r}(\\mathbf{x}^{(1)})\\mathrm{sg}(h_{r}(\\mathbf{x}^{\\prime(1)}))+\\frac{1}{m}\\sum_{r=1}^{m}h_{r}(\\mathbf{x}^{(2)})\\mathrm{sg}(h_{r}(\\mathbf{x}^{\\prime(2)})),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "wherethe $\\operatorname{sg}(\\cdot)$ is the stop-gradient operation, which is inspired by recent empirical works [19, 10] and theoretical work studying contrastive learning [56]. Here we define positive sample as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\widehat{\\mathbf{x}}=[\\widehat{\\mathbf{x}}^{(1)\\top},\\widehat{\\mathbf{x}}^{(2)\\top}]^{\\top}=[y{\\pmb{\\mu}}^{\\top},{\\pmb{\\xi}}^{\\top}+{\\pmb{\\epsilon}}^{\\top}]^{\\top},\\ {\\pmb{\\epsilon}}\\sim\\mathcal{N}(\\mathbf{0},\\sigma_{\\epsilon}^{2}{\\bf I}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In particular, we consider the form of augmentation where the signal stays invariant while the noise vector is corrupted with added independent noise. Similar setup has been considered in [57]. We consider the contrastive loss presented as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\cal L}=-\\frac{1}{n}\\sum_{i=1}^{n}\\log(\\frac{e^{\\mathrm{Sim}_{\\mathbf{h}}(\\mathbf{x}_{i},\\widehat{\\mathbf{x}}_{i})/\\tau}}{e^{\\mathrm{Sim}_{\\mathbf{h}}(\\mathbf{x}_{i},\\widehat{\\mathbf{x}}_{i})/\\tau}+\\sum_{j\\neq i}^{M}e^{\\mathrm{Sim}_{\\mathbf{h}}(\\mathbf{x}_{i},\\mathbf{x}_{j})/\\tau}}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\tau$ is the temperature parameter, $n$ is the number of training samples, and $M$ is the number of negative pairs. In this work, to efficiently optimize the loss to near zero, we require negative sample pairs do not share the same label, i.e., $y_{j}\\neq y_{i}$ in (6). Note that this setting is aligned with supervised contrastive learning [29, 27]. ", "page_idx": 3}, {"type": "text", "text": "We use gradient descent to optimize the contrastive learning loss, which leads to the gradient update: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\bf w}_{r}^{(t+1)}={\\bf w}_{r}^{(t)}-\\eta\\nabla_{{\\bf w}_{r}}L({\\bf W}^{(t)})={\\bf w}_{r}^{(t)}+\\frac{\\eta}{n m\\tau}\\sum_{i=1}^{n}(1-\\ell_{i}^{\\prime(t)})h_{r}^{(t)}(\\widehat{{\\bf x}}_{i}^{(1)})h^{\\prime(t)}({\\bf x}_{i}^{(1)})y_{i}\\mu}}\\\\ {{\\displaystyle~~+\\,\\frac{\\eta}{n m\\tau}\\sum_{i=1}^{n}(1-\\ell_{i}^{\\prime(t)})h_{r}^{(t)}(\\widehat{{\\bf x}}_{i}^{(2)})h_{r}^{\\prime(t)}({\\bf x}_{i}^{(2)})\\xi_{i}-\\frac{\\eta}{n m\\tau}\\sum_{i=1}^{n}\\sum_{j\\neq i}^{M}\\ell_{i,j}^{\\prime(t)}h_{r}^{(t)}({\\bf x}_{j}^{(1)})h^{\\prime(t)}({\\bf x}_{i}^{(1)})y_{i}\\mu}}\\\\ {{\\displaystyle~~-\\,\\frac{\\eta}{n m\\tau}\\sum_{i=1}^{n}\\sum_{j\\neq i}^{M}\\ell_{i,j}^{\\prime(t)}h_{r}^{(t)}({\\bf x}_{j}^{(2)})h_{r}^{\\prime(t)}({\\bf x}_{i}^{(2)})\\xi_{i},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where we denote $\\bar{h}_{r}^{(t)}(\\mathbf{x})=\\sigma(\\langle\\mathbf{w}_{r}^{(t)},\\mathbf{x}\\rangle)$ \uff0c $\\eta$ atle eiva ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\ell_{i}^{\\prime(t)}\\triangleq\\frac{e^{\\mathrm{Sim}_{\\mathbf{h}}(\\mathbf{x}_{i},\\hat{\\mathbf{x}}_{i})/\\tau}}{e^{\\mathrm{Sim}_{\\mathbf{h}}(\\mathbf{x}_{i},\\hat{\\mathbf{x}}_{i})/\\tau}+\\sum_{j\\neq i}^{M}e^{\\mathrm{Sim}_{\\mathbf{h}}(\\mathbf{x}_{i},\\mathbf{x}_{j})/\\tau}},\\,\\ell_{i,j}^{\\prime(t)}\\triangleq\\frac{e^{\\mathrm{Sim}_{\\mathbf{h}}(\\mathbf{x}_{i},\\mathbf{x}_{j})/\\tau}}{e^{\\mathrm{Sim}_{\\mathbf{h}}(\\mathbf{x}_{i},\\hat{\\mathbf{x}}_{i})/\\tau}+\\sum_{j\\neq i}^{M}e^{\\mathrm{Sim}_{\\mathbf{h}}(\\mathbf{x}_{i},\\mathbf{x}_{j})/\\tau}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Intuitively, when the similarity between positive pair is high, and the similarity between negative time is low, we can see $\\ell_{i}^{\\prime(t)}\\approx1$ and $\\ell_{i,j}^{\\prime(t)}\\approx0$ for $i\\in[n]$ and $j\\in[M]$ Therefore,the gradient descent in Eq. (7) is close to zero, indicating the near convergence result. Furthermore, from Eq. (7), we observe that the evolution direction of weight is composed of signal vector $\\pmb{\\mu}$ and noise vectors $\\xi_{i}$ for $i\\in[n]$ . This observation plays a critical role in our following theoretical analysis. ", "page_idx": 3}, {"type": "text", "text": "3.2  Multi-modal Contrastive Learning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We use two neural networks $\\mathbf{h}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{m}$ and $\\mathbf{g}:\\mathbb{R}^{\\tilde{d}}\\rightarrow\\mathbb{R}^{m}$ to encode two input modality $\\mathbf{x}$ and $\\widetilde{\\mathbf{x}}$ respectively. Both neural networks use ReLU activation function. More precisely, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{h}(\\mathbf{x})=[\\bar{h}_{1}(\\mathbf{x}),\\allowbreak\\dots,\\bar{h}_{m}(\\mathbf{x})]^{\\top}\\in\\mathbb{R}^{m},\\quad\\mathrm{where~}\\bar{h}_{r}(x)=h_{r}(\\mathbf{x}^{(1)})+h_{r}(\\mathbf{x}^{(2)})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{g}(\\widetilde{\\mathbf{x}})=[\\bar{g}_{1}(\\widetilde{\\mathbf{x}}),\\dots,\\bar{g}_{m}(\\widetilde{\\mathbf{x}})]^{\\top}\\in\\mathbb{R}^{m},\\quad\\mathrm{where~}\\bar{g}_{r}(x)=g_{r}(\\widetilde{\\mathbf{x}}^{(1)})+g_{r}(\\widetilde{\\mathbf{x}}^{(2)})}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "we let $h_{r}(\\mathbf{x}^{(i)})\\,=\\,\\sigma\\big(\\langle\\mathbf{w}_{r},\\mathbf{x}^{(i)}\\rangle\\big)$ and $g_{r}(\\widetilde{\\mathbf{x}}^{(i)})\\,=\\,\\sigma\\big(\\langle\\widetilde{\\mathbf{w}}_{r},\\widetilde{\\mathbf{x}}^{(i)}\\rangle\\big)$ . Here $\\sigma(\\cdot)$ is the ReLU activation function, $\\mathbf{w}_{r}\\in\\mathbb{R}^{d}$ and $\\widetilde{\\mathbf{w}}_{r}\\in\\mathbb{R}^{\\widetilde d}$ for $r\\in[m]$ are the weights in two networks. Given the embedding, the similarity function of the two modalities is defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{Sim}_{\\mathbf{h},\\mathbf{g}}(\\mathbf{x},\\widetilde{\\mathbf{x}})}&{=\\frac{1}{m}\\sum_{r=1}^{m}h_{r}(\\mathbf{x}^{(1)})\\mathrm{sg}(g_{r}(\\widetilde{\\mathbf{x}}^{(1)}))+\\frac{1}{m}\\sum_{r=1}^{m}h_{r}(\\mathbf{x}^{(2)})\\mathrm{sg}(g_{r}(\\widetilde{\\mathbf{x}}^{(2)})),}\\\\ {\\mathrm{Sim}_{\\mathbf{g},\\mathbf{h}}(\\widetilde{\\mathbf{x}},\\mathbf{x})}&{=\\frac{1}{m}\\sum_{r=1}^{m}g_{r}(\\widetilde{\\mathbf{x}}^{(1)})\\mathrm{sg}(h_{r}(\\mathbf{x}^{(1)}))+\\frac{1}{m}\\sum_{r=1}^{m}g_{r}(\\widetilde{\\mathbf{x}}^{(2)})\\mathrm{sg}(h_{r}(\\mathbf{x}^{(2)})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The two similarity functions defined above are modality-centered with stop-gradient operation applied. The objective function of contrastive multi-modal learning can be expressed as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\cal L}=-\\frac{1}{n}\\sum_{i=1}^{n}\\log(\\frac{e^{\\mathrm{Sim}_{\\mathbf{h},\\mathbf{g}}({\\bf x}_{i},\\widetilde{\\mathbf{x}}_{i})/\\tau}}{e^{\\mathrm{Sim}_{\\mathbf{h},\\mathbf{g}}({\\bf x}_{i},\\widetilde{\\mathbf{x}}_{i})/\\tau}+\\sum_{j\\neq i}^{M}e^{\\mathrm{Sim}_{\\mathbf{h},\\mathbf{g}}({\\bf x}_{i},\\widetilde{\\mathbf{x}}_{j})/\\tau}})}}\\\\ {{\\displaystyle-\\,\\frac{1}{n}\\sum_{i=1}^{n}\\log(\\frac{e^{\\mathrm{Sim}_{\\mathbf{g},\\mathbf{h}}(\\widetilde{\\mathbf{x}}_{i},{\\mathbf{x}}_{i})/\\tau}}{e^{\\mathrm{Sim}_{\\mathbf{g},\\mathbf{h}}(\\widetilde{\\mathbf{x}}_{i},{\\mathbf{x}}_{i})/\\tau}+\\sum_{j\\neq i}^{M}e^{\\mathrm{Sim}_{\\mathbf{g},\\mathbf{h}}(\\widetilde{\\mathbf{x}}_{i},{\\mathbf{x}}_{j})/\\tau}}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Same to the single-modal learning whose objective function is governed by Eq. (6), the objective function for multi-modal contrastive learning adopt one positive pair and $M$ negativepairs.Besides, we require the negative pairs do not share the same label. To optimize the objective function (9) for multi-modal learning, gradient descent is applied to train two encoders simultaneously. The gradient descent rule for the first modal network is governed by the following expression. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\mathbf{w}}_{r}^{(t+1)}={\\mathbf{w}}_{r}^{(t)}-\\eta\\nabla_{{\\mathbf{w}}_{r}}L({\\mathbf{W}}^{(t)})={\\mathbf{w}}_{r}^{(t)}+\\frac{\\eta}{n m\\tau}\\sum_{i=1}^{n}(1-\\ell_{i}^{\\prime(t)})g_{r}^{(t)}(\\widetilde{{\\mathbf{x}}}_{i}^{(1)})h_{r}^{\\prime(t)}({\\mathbf{x}}^{(1)})y_{i}\\mu}\\\\ {~~}\\\\ {{\\displaystyle+\\,\\frac{\\eta}{n m\\tau}\\sum_{i=1}^{n}(1-\\ell_{i}^{\\prime(t)})g_{r}^{(t)}(\\widetilde{{\\mathbf{x}}}_{i}^{(2)})h_{r}^{\\prime(t)}({\\mathbf{x}}^{(2)})\\xi_{i}-\\frac{\\eta}{n m\\tau}\\sum_{i=1}^{n}\\sum_{j\\neq i}^{M}\\ell_{i,j}^{\\prime(t)}g_{r}^{(t)}(\\widetilde{{\\mathbf{x}}}_{j}^{(1)})h_{r}^{\\prime(t)}({\\mathbf{x}}^{(1)})y_{i}\\mu}\\\\ {~~}\\\\ {{\\displaystyle-\\,\\frac{\\eta}{n m\\tau}\\sum_{i=1}^{n}\\sum_{j\\neq i}^{M}\\ell_{i,j}^{\\prime(t)}g_{r}^{(t)}(\\widetilde{{\\mathbf{x}}}_{j}^{(2)})h_{r}^{\\prime(t)}({\\mathbf{x}}^{(2)})\\xi_{i}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "$\\ell_{i}^{\\prime(t)},\\ell_{i,j}^{\\prime(t)}$ is that the corresponding embedding is from another modality. The gradient update for the second modality can be derived similarly, which we omit here for clarity. ", "page_idx": 4}, {"type": "text", "text": "3.3Downstream Task Evaluation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To evaluate the out-of-distribution generalization of single-modal and multi-modal contrastive learning for downstream task, we consider a test distribution $\\mathcal{D}_{\\mathrm{test}}$ , where a sample $\\mathbf{x}_{\\mathrm{test}}=[y\\cdot\\pmb{\\nu}^{\\top},\\pmb{\\zeta}^{\\top}]^{\\top}$ $\\sim\\mathcal{D}_{\\mathrm{test}}$ is generated as follows. The test signal $\\pmb{\\nu}$ satisfies $\\langle\\pmb{\\nu},\\pmb{\\mu}\\rangle=O(\\|\\pmb{\\mu}\\|_{2}^{2}d^{-1/2})$ and the test noise follows $\\boldsymbol\\zeta\\sim\\mathcal{N}(\\mathbf{0},\\sigma_{\\xi}^{2}\\mathbf{I})$ and $y$ follows Rademacher distribution. After the training is complete, we introduce a linear head on top of the learned embedding $\\mathbf{h}(\\mathbf{x}_{\\mathrm{test}})$ for adapting to test distribution, i.e., $f(\\mathbf{x}_{\\mathrm{test}})\\,=\\,\\langle\\mathbf{w},\\mathbf{h}(\\mathbf{x}_{\\mathrm{test}})\\rangle$ . Specifically, we consider the task of classification and define the population O-1 test eror as $L_{\\mathcal{D}_{\\mathrm{test}}}=\\mathbb{P}_{\\mathbf{x}_{\\mathrm{test}}\\sim\\mathcal{D}_{\\mathrm{test}}}\\lbrack y f(\\mathbf{x}_{\\mathrm{test}})<0\\rbrack$ ", "page_idx": 4}, {"type": "text", "text": "4 Main Results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we introduce our key theoretical findings that elucidate the optimization and generalization result for both single-modal and multi-modal contrastive learning through the feature learning analysis. We use a trajectory-based analysis for the iterations induced by gradient descent, following a post-training analysis for the performance on the downstream test set. Below we provide the main assumption and main theorems. ", "page_idx": 4}, {"type": "text", "text": "Assumption 4.1. Let $\\mathrm{SNR}=\\lVert\\pmb{\\mu}\\rVert_{2}/(\\sigma_{\\xi}\\sqrt{d})$ . Assume (1) $d\\geq\\widetilde\\Omega(\\operatorname*{max}\\{n^{2},n\\sigma_{0}^{-1}\\sigma_{\\xi}^{-1},\\sigma_{0}^{-2}\\|\\pmb{\\mu}\\|_{2}^{-2}\\})$ (2) $\\eta\\leq O(\\operatorname*{min}\\{m\\|\\pmb{\\mu}\\|_{2}^{-2},n m\\sigma_{\\xi}^{-2}d^{-1}\\})$ . (3) $\\sigma_{0}\\leq\\widetilde O((\\operatorname*{max}\\{\\sigma_{\\xi}\\sqrt{d},\\|\\pmb{\\mu}\\|_{2}\\})^{-1})$ . (4) $m,n\\geq\\widetilde{\\Omega}(1)$ ", "page_idx": 4}, {"type": "text", "text": "(5) $\\sigma_{\\epsilon}\\leq\\operatorname*{min}\\{\\widetilde{\\Theta}(\\|\\pmb{\\mu}\\|_{2}),\\sigma_{\\xi}/\\widetilde{\\Omega}(1)\\}$ (6) $n\\cdot\\mathrm{SNR}^{2}=\\Theta(1)$ (7) $C_{\\mu}\\|\\pmb{\\mu}\\|_{2}=\\|\\widetilde{\\pmb{\\mu}}\\|_{2}$ where $C_{\\mu}\\geq2.66$ is a constant. ", "page_idx": 5}, {"type": "text", "text": "(1) We adopt a high dimensional setting to ensure enough over-parameterization. (2,3) The learning rate and the strength of initialization are chosen to make sure the that gradient descent can effectively minimize the contrastive loss. (4) The choice of hidden size $m$ and number of training sample $n$ is to provide adequate concentration. (5) The strength of augmentation is set to keep the similarity between two positive samples. (6) The relation between number of sample and SNR is to distinguish the feature learning process between single-modal and multi-modal contrastive learning. (7) To differentiate single-modal and multi-modal contrastive learning, we introduce a constant $C_{\\mu}$ which enables the cooperation between the two modalities in multi-modal contrastive learning. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.2 (Single-Modal Contrastive Learning). Under the single-modal learning setup, suppose Assumption4.1 holds.Then after $T^{*}=\\widetilde\\Theta(\\eta^{-1}\\bar{m}n\\sigma_{\\xi}^{-2}d^{-1}+\\eta^{-1}m n\\sigma_{\\xi}^{-2}d^{-1}\\epsilon^{-1})$ , the with probability at least $1-1/d,$ it holds that $(I)$ Training error $L(T^{*})\\leq\\epsilon$ and(2) Test error at down-stream task $L_{\\mathcal{D}_{\\mathrm{test}}}(T^{*})=\\stackrel{\\cdot}{\\Theta}(1)$ ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.2 states that despite the small training error achieved by single-modal contrastive learning, the test error is large in the downstream task. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.3 (Multi-Modal Contrastive Learning). Under the single-modal learning setup, suppose Assumption4.1 holds.Then after $T^{*}=\\widetilde\\Theta(\\eta^{-1}m n\\sigma_{\\xi}^{-2}d^{-1}+\\eta^{-1}m n\\sigma_{\\xi}^{-2}d^{-1}\\epsilon^{-1})$ , the with probability at least $1-1/d,$ it holds that $(I)$ Training error $L(T^{*})\\leq\\epsilon$ and(2) Test error at down-stream task $L_{\\mathcal{D}_{\\mathrm{test}}}(T^{*})=\\stackrel{\\cdot}{o}(1)$ ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.3 demonstrates that trained multi-modal contrastive learning can achieve both small training error and downstream test error. Compared to Theorem 4.2, Theorem 4.3 shows that the generalization of multi-modal contrastive learning in downstream tasks is better than single-modal contrastive learning. The reason behind this difference is that the two modalities can cooperate with each other; the higher quality in one modality can boost the feature learning in the target modality. helping to generalize to the downstream task. On the contrary, augmentation often maintains the same SNR as the original data, so single-modal learning hardly benefits from the augmentation and can only memorize the noise from the data, which is not applicable to downstream tasks. ", "page_idx": 5}, {"type": "text", "text": "5 Proof Roadmap ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "5.1  Proof Sketch for Single Modal Contrastive Learning ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The proof is constructed by a optimization analysis followed by a generlization analysis in the downstream task. Through the application of the gradient descent rule outlined in Eq. (7), we observe that the gradient descent iterate $\\mathbf{w}_{r}^{(t)}$ is a linear combination of israndom initiaization $\\mathbf{w}_{r}^{(0)}$ ,the signal vector $\\pmb{\\mu}$ and the noise vectors in the training data $\\xi_{i}$ for $i\\in[n]$ . Consequently, for $r\\in[m]$ the decomposition of weight vector iteration can be expressed: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{w}_{r}^{(t)}=\\mathbf{w}_{r}^{(0)}+\\gamma_{r}^{(t)}\\|\\pmb{\\mu}\\|_{2}^{-2}\\pmb{\\mu}+\\sum_{i=1}^{n}\\rho_{r,i}^{(t)}\\|\\pmb{\\xi}_{i}\\|_{2}^{-2}\\pmb{\\xi}_{i},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\gamma_{r}^{\\left(t\\right)}$ and $\\rho_{r,i}^{(t)}$ serve as coefcints and represt signal learin and noisememorization respectivelyasdnthe th gradient dent udate 7), thiterationf $\\gamma_{r}^{\\left(t\\right)}$ and $\\rho_{r,i}^{(t)}$ are given: ", "page_idx": 5}, {"type": "text", "text": "$\\gamma_{r}^{(t)}\\rho_{r,i}^{(t)}$ indecomposion $(I I)$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\gamma_{r}^{(t+1)}=\\gamma_{r}^{(t)}+\\frac{\\eta}{n m\\tau}\\sum_{i=1}^{n}\\big[(1-\\ell_{i}^{\\prime(t)})h_{r}^{(t)}(\\widehat\\mathbf{x}_{i}^{(1)})-\\displaystyle\\sum_{j\\neq i}^{M}\\ell_{i,j}^{\\prime(t)}h_{r}^{(t)}(\\mathbf{x}_{j}^{(1)})\\big]h_{r}^{\\prime(t)}(\\mathbf{x}_{i}^{(1)})y_{i}\\big\\|\\mu\\big\\|_{2}^{2},}\\\\ {\\displaystyle\\rho_{r,i}^{(t+1)}=\\rho_{r,i}^{(t)}\\!+\\!\\frac{\\eta}{n m\\tau}\\big[(1-\\ell_{i}^{\\prime(t)})h_{r}(\\widehat\\mathbf{x}_{i}^{(2)})-\\displaystyle\\sum_{j\\neq i}^{M}\\ell_{i,j}^{\\prime(t)}h_{r}(\\mathbf{x}_{j}^{(2)})\\big]h_{r}^{\\prime(t)}(\\mathbf{x}_{i}^{(2)})\\|\\xi_{i}\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the initialization ?, Pr,? $\\gamma_{r}^{(0)},\\rho_{r,i}^{(0)}=0$ ", "page_idx": 5}, {"type": "text", "text": "Lemma 5.1 tells how the coefficients evolve under gradient descent update. In the following, we introduce a two-stage dynamics to characterize the whole training process based on Eq 12 and Eq 13. ", "page_idx": 6}, {"type": "text", "text": "First Stage: Exponential growth. During the first stage, we show befre $\\gamma_{r}^{(t)}$ $\\rho_{r,i}^{(t)}$ growto $\\Theta(1)$ the embedding (3) is close to zero, suggesting the similarity is bounded by $1\\leq\\mathrm{Sim}_{\\mathbf{h}}(\\mathbf{x},\\mathbf{x}^{\\prime})\\leq C_{\\ell}$ for some constant $C_{\\ell}>1$ . The loss derivatives defined in (8) can thus be bounded within some constant range. ", "page_idx": 6}, {"type": "text", "text": "Signal learning. According to the update for signal learning in (12), we see the propagation can be simplified based on the hard-negative sampling strategy, i.e., the negative pairs do not share the same labels.  This suggests the negative term is always zero as $\\begin{array}{r}{\\sum_{i=1}^{n}\\sum_{j:y_{i}\\neq y_{i}}^{M}\\sigma(\\langle\\mathbf{w}_{r}^{(t)},y_{j}\\pmb{\\mu}\\rangle)\\sigma^{\\prime}(\\langle\\mathbf{w}_{r}^{(t)},y_{i}\\pmb{\\mu}\\rangle)\\;=\\;\\stackrel{\\cdot\\}{0}}\\end{array}$ . The resulting update of $\\gamma_{r}^{(t)}$ reduces to $\\begin{array}{r}{\\gamma_{r}^{(t+1)}\\,=\\,\\gamma_{r}^{(t)}\\!+\\!\\frac{\\eta}{n m\\tau}\\sum_{i=1}^{n}\\!\\big(1-\\ell_{i}^{\\prime(t)}\\big)\\sigma\\big(\\langle{\\mathbf{w}}_{r}^{(t)},y_{i}\\mu\\rangle\\big)\\sigma^{\\prime}\\big(\\langle{\\mathbf{w}}_{r}^{(t)},y_{i}\\mu\\rangle\\big)y_{i}\\big\\|\\mu\\big\\|_{2}^{2}\\,}\\end{array}$ Examinin th propagation of $\\gamma_{r}^{(t)}$ , we can dide the dynamics into two groups depending on the sign of weight initializtion $\\langle\\mathbf{w}_{r}^{(0)},\\mu\\rangle$ Let $\\mathcal{U}_{+}^{(t)}\\triangleq\\{\\bar{r}:\\langle\\mathbf{w}_{r}^{(t)},\\pmb{\\mu}\\rangle>0\\}$ and $\\bar{\\mathcal{U}}_{-}^{(t)}\\triangleq\\{r:\\bar{\\langle\\mathbf{w}_{r}^{(t)},\\pmb{\\mu}\\rangle}<\\bar{0}\\}$ . Then for $r\\in\\mathcal{U}_{+}^{(0)}$ , we can show $\\gamma_{r}^{(t)}\\geq0$ increases exponentilly and thus the sign of inner poduce tays invariant with $\\mathcal{U}_{+}^{(t)}=\\mathcal{U}_{+}^{(0)}$ for all $t\\geq0$ . On the other hand, for $r\\in\\mathcal{U}_{-}^{(0)}$ , we can show $\\gamma_{r}^{(t)}\\leq0$ and decreases exponentially with $\\mathcal{U}_{-}^{(t)}=\\mathcal{U}_{-}^{(0)}$ for all $t\\geq0$ ", "page_idx": 6}, {"type": "text", "text": "Noise memorization. Compared to signal learning, the behaviour of noise memorization requires more detailed analysis. This is mainly because the negative pairs can not be eliminated simply based on label difference, as the noise patch $\\xi_{i}$ is generated independent of label $y_{i}$ . In addition, the added noise $\\epsilon_{i}$ by augmentation can also contribute to the noise dynamics. We first show when the noise level $\\sigma_{\\epsilon}$ Bythesignof is much smaller compared to $\\langle\\mathbf{w}_{r}^{(t)},\\boldsymbol{\\xi}_{i}\\rangle$ ep $\\sigma_{\\xi}$ , the dynamics of noise memorization is largely remains unaffected. $\\mathcal{Z}_{r,+}^{(t)}=\\{i:\\langle\\mathbf{w}_{r}^{(t)},\\pmb{\\xi}_{i}\\rangle>0\\}$ and $\\mathcal{Z}_{r,-}^{(t)}\\,=\\,\\{i:\\langle\\mathbf{w}_{r}^{(t)},\\pmb{\\xi}_{i}\\rangle<0\\}$ .We can vrfy for $i\\in\\mathcal{Z}_{r,-}^{(0)}$ , the value of $\\rho_{r,i}^{(t)}$ stays at ero based on the update (13) with an induction argument. For samples $i\\in\\mathcal{Z}_{r,+}^{(0)}$ with positive initialization, we analyze the noise memorization based on the joint dynamics of samples with the same label. In particular, we define total noise memorization of positive and negative samples respectively as $\\begin{array}{r}{B_{r,+}^{(\\bar{t})}\\triangleq\\sum_{i:y_{i}=1}(\\rho_{r,i}^{(t)}+\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle)\\mathbb{1}_{i\\in\\mathcal{T}_{r,+}^{(t)}}}\\end{array}$ ier(t) and B(t) $\\begin{array}{r}{\\boldsymbol{B}_{r,-}^{(t)}\\triangleq\\sum_{i:y_{i}=-1}(\\bar{\\rho_{r,i}^{(t)}}+\\langle\\mathbf{w}_{r}^{(\\bar{0})},\\boldsymbol{\\xi}_{i}\\rangle)\\bar{\\mathbf{1}_{i\\in\\mathcal{T}_{r,+}}^{(t)}}}\\end{array}$ The update of $\\rho_{r,i}^{(t)}$ in (13) then implies the dynamics of $B_{r,+}^{(t)}$ andB(t) as follows ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\nB_{r,+}^{(t+1)}\\approx B_{r,+}^{(t)}+\\frac{\\eta\\sigma_{\\xi}^{2}d}{n m\\tau}\\big(B_{r,+}^{(t)}-\\frac{1}{2}B_{r,-}^{(t)}\\big),\\quad B_{r,-}^{(t+1)}\\approx B_{r,-}^{(t)}+\\frac{\\eta\\sigma_{\\xi}^{2}d}{n m\\tau}\\big(B_{r,-}^{(t)}-\\frac{1}{2}B_{r,+}^{(t)}\\big),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where the coefficient of $1/2$ appears as a result of the randomness of the sign of initialization. This result sugests, individual $\\rho_{r,i:y_{i}=1}^{(t)}$ cannot grow too slow compared to the $\\rho_{r,i:y_{i}=-1}^{(t)}$ .Following a similadtartwearabl has an exponential growth lower bound. On the other hand, for samples with $y_{i}=-1$ but with different neuron, we can use the same strategy to show an exponential growth lower bound for some neurons that satisfy the initialization conditions. ", "page_idx": 6}, {"type": "text", "text": "$\\begin{array}{r}{T_{1}=\\log\\big(20/(\\sigma_{0}\\sigma_{\\xi}\\sqrt{d})\\big)/\\log\\big(1+0.96\\frac{\\eta\\sigma_{\\xi}^{2}d}{n m\\tau}\\big)}\\end{array}$ ,wehave $\\gamma_{r}^{(t)}=\\widetilde{O}(1/\\sqrt{n})$ $r\\in[m]$ $0\\leq t\\leq T_{1}$ i) = \u03a9(1) for all i E [n]. ", "page_idx": 6}, {"type": "text", "text": "Second stage: convergence and scale difference. At the end of first stage, the noise grows to a constant order while signal learning remains negligible. As a result, the loss derivatives are no longer bounded within some constant range. In the second stage, we aim to show the loss is able to converge to an arbitrarily small value $\\epsilon$ . Despite the unsupervised learning setup, we are still able to show loss convergence thanks to the hard negative samples. Let $F_{0}(\\mathbf{W},\\mathbf{\\bar{x}}_{i})\\stackrel{!}{=}\\mathrm{Sim}(\\mathbf{x}_{i},\\widehat{\\mathbf{x}}_{i})$ be the similarity to the argumentation and $F_{j}(\\mathbf{W},\\mathbf{x}_{i})\\,=\\,\\mathrm{Sim}(\\mathbf{x}_{i},\\mathbf{x}_{j})$ for $j\\,=\\,1,...,M$ be the similarity between the negative pairs. Then we can show there exists some $\\mathbf{W}^{*}$ such that $\\left\\langle\\nabla F_{0}(\\mathbf{W}^{(t)},\\mathbf{x}_{i}),\\mathbf{W}^{*}\\right\\rangle\\geq$ $2\\log(2M/\\epsilon)$ while $\\langle\\nabla F_{j}(\\mathbf{W}^{(t)},\\mathbf{x}_{i}),\\mathbf{W}^{*}\\rangle\\leq\\log(2M/\\epsilon)$ for all $j=1,...,M$ . Then we can bound $\\begin{array}{r}{\\langle\\nabla L_{S}(\\mathbf{W}^{(t)}),\\mathbf{W}^{(t)}-\\mathbf{W}^{*}\\rangle\\ge\\frac{1}{n}\\sum_{i=1}^{n}L_{i}(\\mathbf{W}^{(t)})-\\epsilon/2.}\\end{array}$ This as aresultallws to show amonotonic decrease in the loss function as $\\begin{array}{r}{L(\\mathbf{W}^{(t)})\\leq\\frac{1}{\\eta}(\\|\\mathbf{W}^{(t)}-\\mathbf{W}^{*}\\|_{F}^{2}-\\|\\mathbf{W}^{(t+1)}-\\mathbf{W}^{*}\\|_{F}^{2})+\\epsilon}\\end{array}$ which guarantees convergence by telescoping over the inequality. Upon the convergence, we can also show the scale difference obtained at the end of the frst stage is maintained, i.e., $\\gamma_{r}^{(t)}=\\widetilde{O}(1/\\sqrt{n})$ While maXrPr,i (i) = (1) for all E [n]. This sugests the non-linealy separability for the resulting embeddings and thus the downstream test error is non-vanishing. The formal convergence result is established in Lemma C.18. Combined with the generalization error demonstrated in Appendix C.3, this completes the proof of Theorem 4.2. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "5.2  Proof Sketch for Multi-Modal Learning ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Similar to single-modal contrastive learning, we decompose the decomposition of weight vector iteration for the network in the second modality and subsequently provide a two-stage analysis. ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\widetilde{\\mathbf{w}}_{r}^{(t)}=\\widetilde{\\mathbf{w}}_{r}^{(0)}+\\widetilde{\\gamma}_{r}^{(t)}\\|\\widetilde{\\pmb{\\mu}}\\|_{2}^{-2}\\widetilde{\\pmb{\\mu}}+\\sum_{i}\\widetilde{\\rho}_{r,i}^{(t)}\\|\\widetilde{\\pmb{\\xi}}_{i}\\|_{2}^{-2}\\widetilde{\\pmb{\\xi}}_{i},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Where $\\widetilde{\\gamma}_{r}^{(t)}$ and $\\widetilde{\\rho}_{r,i}^{(t)}$ ea ", "page_idx": 7}, {"type": "text", "text": "Lemma53M-MaThco $\\gamma_{r}^{(t)},\\rho_{r,i}^{(t)},\\widetilde{\\gamma}_{r}^{(t)},\\widetilde{\\rho}_{r,i}^{(t)}$ in decopositon $(I I)$ ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\gamma_{r}^{(t+1)}=\\gamma_{r}^{(t)}+\\frac{\\eta}{n m\\tau}\\sum_{i=1}^{n}[(1-\\ell_{i}^{(t)})h_{r}^{\\prime}(y_{i}\\mu)-\\sum_{j=1}^{M}\\ell_{i,j}^{(t)}h_{r}^{\\prime}(y_{i}\\mu)]g_{r}(y_{j}\\widetilde{\\mu})y_{i}\\|\\mu\\|_{2}^{2},}\\\\ {\\displaystyle\\rho_{r,i}^{(t+1)}=\\rho_{r,i}^{(t)}\\!+\\!\\frac{\\eta}{n m\\tau}(1-\\ell_{i}^{\\prime(t)})g_{r}(\\widetilde{\\xi}_{i})h_{r}^{\\prime}(\\xi_{i})\\|\\xi_{i}\\|_{2}^{2}-\\frac{\\eta}{n m\\tau}\\sum_{j=1}^{M}\\ell_{i,j}^{\\prime(t)}g_{r}(\\widetilde{\\xi}_{j})h_{r}^{\\prime}(\\xi_{i})\\|\\xi_{i}\\|_{2}^{2},}\\\\ {\\displaystyle\\widetilde{\\gamma}_{r}^{(t+1)}=\\widetilde{\\gamma}_{r}^{(t)}+\\frac{\\eta}{n m\\tau}\\sum_{i=1}^{n}[(1-\\ell_{i}^{\\prime(t)})h_{r}^{\\prime}(y_{i}\\mu)-\\sum_{j=1}^{M}\\ell_{i,j}^{\\prime(t)}h_{r}^{\\prime}(y_{i}\\mu)]g_{r}(y_{j}\\widetilde{\\mu})y_{i}\\|\\widetilde{\\mu}\\|_{2}^{2},}\\\\ {\\displaystyle\\widetilde{\\rho}_{r,i}^{(t+1)}=\\widetilde{\\rho}_{r,i}^{(t)}\\!+\\!\\frac{\\eta}{n m\\tau}(1-\\ell_{i}^{\\prime(t)})h_{r}(\\xi_{i})g_{r}^{\\prime}(\\widetilde{\\xi}_{i})\\|\\widetilde{\\xi}_{i}\\|_{2}^{2}-\\frac{\\eta}{n m\\tau}\\sum_{j=1}^{M}\\ell_{i,j}^{\\prime(t)}h_{r}(\\xi_{j})g_{r}^{\\prime}(\\widetilde{\\xi}_{i})\\|\\widetilde{\\xi}_{i}\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where the initialization satisfy , Pr, , , Pr,? ", "page_idx": 7}, {"type": "text", "text": "First Stage: Exponential growth The first stage of multi-modal learning shares similar characteristics as single-modal learning. ", "page_idx": 7}, {"type": "text", "text": "Signal learning. For signal learning, we analyze the trajectories for both $\\gamma_{r}^{\\left(t\\right)}$ and $\\widetilde{\\gamma}_{r}^{(t)}$ . To this end, we partition the neurons depending on their initialization status. Apart from $\\mathcal{U}_{+}^{(t)}$ and $\\mathcal{U}_{-}^{(t)}$ defined in the single-modal learning, we additionally define $\\widetilde{\\mathcal{U}}_{+}^{(t)}\\triangleq\\{r:\\langle\\widetilde{\\mathbf{w}}_{r}^{(t)},\\widetilde{\\pmb{\\mu}}\\rangle>0\\}$ \uff0c $\\widetilde{\\mathcal{U}}_{-}^{(t)}\\;\\triangleq$ $\\{r\\,:\\,\\langle\\widetilde{\\mathbf{w}}_{r}^{(t)},\\widetilde{\\mu}\\rangle\\,<\\,0\\}$ forth ther modaliy henfr $r\\,\\in\\,\\mathcal{U}_{+}^{(0)}\\cap\\widetilde{\\mathcal{U}}_{+}^{(0)}$ we can show $\\gamma_{r}^{(t)}\\,\\geq\\,0$ \uff0c $\\phi_{r}^{(t)}\\geq0$ andare increasigForons $r\\in\\mathcal{U}_{-}^{(t)}\\cap\\widetilde{\\mathcal{U}}_{-}^{(t)}$ ,we can show $\\gamma_{r}^{(t)}\\leq0,\\tilde{\\gamma}_{r}^{(t)}\\leq0$ andare deraif  a $\\mathcal{U}_{+}^{(t)}\\cap\\widetilde{\\mathcal{U}}_{+}^{(t)}=\\mathcal{U}_{+}^{(0)}\\cap\\widetilde{\\mathcal{U}}_{+}^{(0)}$ and $\\mathcal{U}_{-}^{(t)}\\cap\\widetilde{\\mathcal{U}}_{-}^{(t)}=\\mathcal{U}_{-}^{(0)}\\cap\\widetilde{\\mathcal{U}}_{-}^{(0)}$ For neurons with only one of the modalities activated initill, i.e. $r\\in\\mathcal{U}_{+}^{(0)}\\cap\\widetilde{\\mathcal{U}}_{-}^{(0)}$ or $r\\in\\mathcal{U}_{-}^{(0)}\\cap\\widetilde{\\mathcal{U}}_{+}^{(0)}$ , we can show there exists some time $t^{\\prime}\\geq0$ such that the neurons are either positively or negatively activated with $r\\in\\mathcal{U}_{+}^{(t^{\\prime})}\\cap\\tilde{\\mathcal{U}}_{+}^{(t^{\\prime})}$ and $r\\in\\mathcal{U}_{-}^{(t^{\\prime})}\\cap\\widetilde{\\mathcal{U}}_{-}^{(t^{\\prime})}$ ", "page_idx": 7}, {"type": "text", "text": "This shows synchronization of the signal learning patterns of two modalities. The pre-synchronization phase reduces the speed of learning and thus we can only focus on neurons with the same sign for the initialization in order to decide the lower and upper bound. ", "page_idx": 7}, {"type": "text", "text": "Noise memorization. For noise memorization, we partition the samples into two sets for both modalities according to the initialization, namly. $\\mathcal{Z}_{r,+}^{(t)}\\;=\\;\\{i\\;:\\;\\langle{\\bf w}_{r}^{(t)},{\\pmb\\xi}_{i}\\rangle\\;>\\;0\\}$ \uff0c ${\\mathcal{T}}_{r,-}^{(t)}\\;=\\;\\{i\\;:$ $\\langle\\mathbf{w}_{r}^{(t)},\\boldsymbol{\\xi}_{i}\\rangle<0\\}$ and similarlyforthe second modality ),) . Because the noise memorization are correlated in the two modalities, we separately analyze the samples as follows. (1) For i Z() $i\\in\\mathcal{Z}_{r,-}^{(0)}\\cap\\widetilde{\\mathcal{L}}_{r,-}^{(0)}$ , we can show $\\rho_{r,i}^{(t)}=0,\\mathcal{I}_{r,-}^{(t)}=\\mathcal{I}_{r,-}^{(0)}$ and pr, and $\\widetilde{\\rho}_{r,i}^{(t)}=0,\\widetilde{\\mathcal{L}}_{r,-}^{(t)}=\\widetilde{\\mathcal{L}}_{r,-}^{(0)}$ (2) For i e Z n, , we can show $\\rho_{r,i}^{(t)}=0,\\mathcal{I}_{r,-}^{(t)}=\\mathcal{I}_{r,-}^{(0)}$ and $\\widetilde{\\rho}_{r,i}^{(t)}\\leq0$ (3) For $i\\in\\mathcal{T}_{r,+}^{(0)}\\cap\\widetilde{\\mathcal{L}}_{r,-}^{(0)}$ , we can show $\\widetilde{\\rho}_{r,i}^{(t)}=0,\\widetilde{\\mathcal{L}}_{r,-}^{(t)}=\\widetilde{\\mathcal{L}}_{r,-}^{(0)}$ and $\\rho_{r,i}^{(t)}\\leq0$ ", "page_idx": 7}, {"type": "image", "img_path": "O2UwxfhY1P/tmp/bed0633dcbada8a8d2a112adce945eab095280693258cfddf4e8d6594223c8d3.jpg", "img_caption": ["Figure 1: Training loss, test accuracy, signal learning and noise memorization of single-modal and multi-modal contrastive learning. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "(4) For $i\\in\\mathcal{Z}_{r,+}^{(0)}\\cap\\widetilde{\\mathcal{Z}}_{r,+}^{(0)}$ wihout lossof enerality, we consider upper bounding noise memorization for the first modality and $y_{i}=1$ . To this end, we first define the individual and joint noise memorization for the first modality as w $\\boldsymbol{\\Psi}_{r,i}^{(t)}\\triangleq\\rho_{r,i}^{(t)}+\\langle\\mathbf{w}_{r}^{(0)},\\boldsymbol{\\xi}_{i}\\rangle$ and $\\begin{array}{r}{B_{r,+,+}^{(t)}\\triangleq\\sum_{i:y_{i}=1}(\\rho_{r,i}^{(t)}+}\\end{array}$ $\\begin{array}{r}{\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle)\\mathbb{1}_{i\\in\\mathcal{I}_{r,+}^{(t)}\\cap\\widetilde{\\mathcal{Z}}_{r,+}^{(t)}},B_{r,+,-}^{(t)}\\triangleq\\sum_{i:y_{i}=1}(\\rho_{r,i}^{(t)}+\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle)\\mathbb{1}_{i\\in\\mathcal{I}_{r,+}^{(t)}\\cap\\widetilde{\\mathcal{Z}}_{r,-}^{(t)}}}\\end{array}$ exist for the other modality. The joint dynamics of noise memorization can be first upper bounded by the other modality as w> $\\begin{array}{r}{\\tilde{\\Psi}_{r,i}^{(t)}\\geq\\frac{\\operatorname{i}01C_{\\ell}}{M+1}(B_{r,+,+}^{(t)}+B_{r,+,-}^{(t)})}\\end{array}$ . Then we can upper bound the individual noise memorization by ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\Psi_{r,i}^{(t)}\\leq(1+\\frac{1.06\\eta\\sigma_{\\xi}^{2}d}{n m\\tau})^{t}(\\Psi_{r,i}^{(0)}+\\widetilde{\\Psi}_{r,i}^{(0)}).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "We show the combined dynamics of $\\gamma_{r}^{\\left(t\\right)}$ and $\\rho_{r}^{(t)}$ exhibits exponential growth while the magnitude of their difference shrinks exponentially. The results are summarized as follows ", "page_idx": 8}, {"type": "text", "text": "Lemma 5.4. Under the Assumption 4.1, let $\\begin{array}{r}{T_{1}=\\log\\left(20/(\\sigma_{0}\\|\\pmb{\\mu}\\|_{2})\\right)/\\log\\left(1+0.48C_{\\mu}\\frac{\\eta\\|\\pmb{\\mu}\\|_{2}^{2}}{m\\tau}\\right)}\\end{array}$ we have pr,i ) =0(1/\u221an) forallr [m], e [n], and 0\u2264t\u2264T andmaxr $\\operatorname*{max}_{r}\\gamma_{r}^{(T_{1})}=\\Omega(1)$ ", "page_idx": 8}, {"type": "text", "text": "Second Stage: Convergence and scale difference. The second stage presents similar patterns compared to single-modal learning. Thanks to the correlation between the two modality during gradient descent training, the two neural network converge at the same time, minimizing the training loss. Besides, The scale difference at the end of the first stage is carried over throughout the second stage until convergence. Therefore, it allows to show a monotonic decrease in the loss function as I $\\begin{array}{r}{\\langle(\\mathbf{W}^{(t)},\\widetilde{\\mathbf{W}}^{(t)})^{*}\\rangle\\leq\\|\\mathbf{W}^{(t)}-\\mathbf{W}^{*}\\|_{F}^{2}+\\|\\widetilde{\\mathbf{W}}^{(t)}-\\widetilde{\\mathbf{W}}^{*}\\|_{F}^{2}\\,-\\,\\|\\mathbf{W}^{(t+1)}\\,-\\,\\mathbf{W}^{*}\\|_{F}^{2}\\,-\\,\\|\\widetilde{\\mathbf{W}}^{(t+1)}\\,-\\,\\widetilde{\\mathbf{W}}^{*}\\|_{F}^{2}\\,-\\,\\|\\widetilde{\\mathbf{W}}^{(t+1)}\\,-\\,\\widetilde{\\mathbf{W}}^{*}\\|_{F}^{2}\\,\\Big\\}\\,-\\,\\frac{1}{2}\\,\\widetilde{\\mathbf{W}}^{(t+1)}\\,-\\,\\frac{1}{2}\\,\\widetilde{\\mathbf{W}}^{*}\\|_{F}^{2}\\,,}\\end{array}$ $\\widetilde{\\mathbf{W}}^{*}\\|_{F}^{2}+2\\epsilon$ , which guarantees convergence by telescoping over the inequality. At the same time, until convergence, we can show the scale difference obtained at the end of the first stage is maintained, namely $\\operatorname*{max}_{r,i}\\rho_{r,i}^{(t)}=\\widetilde{O}(1/\\sqrt{n})$ and $\\operatorname*{max}_{r}\\gamma_{r}^{(t)}=\\Omega(1)$ Thissuggeststheigallaindminat the noise memorization and thus the resulting embeddings are linearly separable, which guarantees a small test error for downstream tasks. The formal convergence result is established in Lemma D.15. Combined with the generalization error demonstrated in Appendix D.3, this completes the proof of Theorem 4.3. ", "page_idx": 8}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Synthetic experiments We conduct synthetic experiments to verify the theoretical results obtained in the previous sections. We generate samples following the theoretical setups, where we set the data dimension $d=2000$ , number of training samples $n=100$ , number of test samples $n_{\\mathrm{test}}=200$ and the hidden size of all encoders as $m\\,=\\,50$ . We adopt gradient descent with a learning rate of 0.01 as the optimizer to train the model by 200 epochs. In the single-modal setting, the $\\pmb{\\mu}$ is set to be $[5,0,...,0]^{T}$ and the $\\pmb{\\xi}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ for the in-distribution data, and the augmentation vector $\\pmb{\\epsilon}\\sim\\mathcal{N}(\\mathbf{0},0.01*\\mathbf{I})$ . For the multi-modal setting, $\\tilde{\\pmb{\\mu}}=[0,15,0,...,0]^{T}$ . In addition, for the OOD test data $\\mathbf{x}_{\\mathrm{test}}=[\\pmb{\\nu}^{\\top},\\pmb{\\zeta}^{\\top}]^{\\top}$ , we set $\\pmb{\\nu}=[2,0,...,0]$ and $\\zeta\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ . We perform logistic regression based on the learned features $\\mathbf{h}(\\mathbf{x}_{\\mathrm{test}})$ and apply the learned classifier head to evaluate OOD generalization error in terms of prediction accuracy. ", "page_idx": 8}, {"type": "text", "text": "Results. In Figure 1, we see the training loss of both single-modal and multi-modal learning converges rapidly. At the same time, OOD test accuracy of multi-modal learning converges to nearly 1.0 while that of single-modal learning stagnates around 0.5. This is primarily because under the setup where theothermodality $\\tilde{\\pmb{\\mu}}$ has a higher SNR, signal learning of $\\pmb{\\mu}$ is lifted. This can be verified from the third plot of Figure 1, where the signal learning of multi-modal framework is significantly higher than single-modal. Further, it can be observed that single-modal contrastive learning exhibits more severe noise memorization, which suppresses signal learning. In contrast, multi-modal contrastive learning exhibits less severe noise memorization which would further encourage signal learning. These phenomena again support and align with our theoretical results. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Real-world experiments _We now extend the comparison of single-modal and multi-modal learning to realistic image data, ColoredMNIST [3, 54], which is a typical benchmark studying the generalization capability under distribution shifts. The ColoredMNIST dataset is a variation of the standard MNIST dataset, where each digit is assigned a specific color based on its label. The two modalities are image, and text that describes the images. The task is a 10-class classification that recognizes the number of the colored MNIST images. Specifically, we have 10 colors to color 10 digits, and introduce spurious correlations via label noises following the literature: ", "page_idx": 9}, {"type": "text", "text": "\u00b7 For the training set, $10\\%$ of labels will be clipped to a random class. For images with class $\\surd0\\ '$ (or \u20181'), they will be colored as red (or green) with a probability of $77.5\\%$ , and as another random color with a probability of $22.5\\%$ . The coloring scheme introduces a spurious correlation. \u00b7 For the test set, $10\\%$ of labels will be clipped to a random class. For images with class $\\surd0\\ '$ (or \u20181'), they will be colored as green (or red) with a probability of $77.5\\%$ , and as another random color with a probability of $22.5\\%$ . The coloring scheme can be considered as reversing the training spurious correlations. Therefore, the evaluation on test set can reflect to what extent the model learns to use the spurious features, i.e., colors, to classify images. ", "page_idx": 9}, {"type": "text", "text": "We implement the multi-modal learning following the practice in [54], where we consider an ideal language encoder that successfully encodes the caption of the images into onehot labels of colors and digits.  For single-modal learning, we follow the implementation of the SimCLR [9] to construct a set of augmentations to learn the representations. ", "page_idx": 9}, {"type": "text", "text": "Results. Under the distribution shift, we verify that multi-modal learning archives an  out-of-distribution test  accuracy  of $82.13\\%$ , which outperforms that of singlemodal learning $12.68\\%$ .As a result, we can claim that the effective SNR of invariant features (the shape of the digit) will be degraded under the impact of the injected ", "page_idx": 9}, {"type": "table", "img_path": "O2UwxfhY1P/tmp/a629832501576f3c7cebcfb634f6e3755d967b6fa19a728a722e2222255da290.jpg", "table_caption": ["Table 1: Performance comparison for single and multimodal contrastive learning. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "color. Therefore, the performance of single-modal may be suboptimal as it cannot effectively utilize the information of the digit's shape. On the other hand, multi-modal demonstrates a better capacity for handling this scenario. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we have established a comprehensive comparison of the optimization differences during the pre-training stage and the generalization gap between single-modal and multi-modal contrastive learning for downstream tasks. With the cooperation between modalities, multi-modal contrastive learning can achieve better feature learning and generalization on downstream tasks compared to single-modal learning. On the other hand, data augmentation alone can hardly improve data quality and thus cannot boost the performance of single-modal contrastive learning. Together, these results quantitatively demonstrate the superiority of multi-modal learning over single-modal learning and emphasize the importance of data quality in multi-modal contrastive learning. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank the anonymous reviewers for their insightful comments to improve the paper. Wei Huang is supported by JSPS KAKENHI Grant Number 24K20848. Yuan Cao is supported by NSFC 12301657 and HK RGC-ECS 27308624. Taiji Suzuki is partially supported by JSPS KAKENHI (24K02905) and JST CREST (JPMJCR2015). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:23716-23736, 2022.   \n[2]  Zeyuan Allen-Zhu and Yuanzhi Li. Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. arXiv preprint arXiv:2012.09816, 2020.   \n[3]  Martin Arjovsky, L\u00e9on Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXiv preprint arXiv:1907.02893, 2019.   \n[4]  Sanjeev Arora, Hrishikesh Khandeparkar, Mikhail Khodak, Orestis Plevrakis, and Nikunj Saunshi. A theoretical analysis of contrastive unsupervised representation learning. arXiv preprint arXiv: 1902.09229, 2019. [5]  Randall Balestriero and Yann LeCun. Contrastive and non-contrastive self-supervised learning recover global and local spectral embedding methods. Advances in Neural Information Processing Systems, 35:26671-26685, 2022.   \n[6]  Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert- Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems, 2020.   \n[7] Vivien Cabannes, Bobak Kiani, Randall Balestriero, Yann LeCun, and Alberto Bietti. The ssl interplay: Augmentations, inductive bias, and generalization. In International Conference on Machine Learning, pages 3252-3298. PMLR, 2023.   \n[8]  Yuan Cao, Zixiang Chen, Misha Belkin, and Quanquan Gu. Benign overfitting in two-layer convolutional neural networks. Advances in neural information processing systems, 35:25237- 25250, 2022. [9]  Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pages 1597-1607. PMLR, 2020.   \n[10] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proceedings ofthe IEEE/CVF conference on computer vision and pattern recognition, pages 15750-15758, 2021.   \n[11]  Yongqiang Chen, Wei Huang, Kaiwen Zhou, Yatao Bian, Bo Han, and James Cheng. Understanding and improving feature learning for out-of-distribution generalization. In Advances in Neural Information Processing Systems, 2023.   \n[12] Zixiang Chen, Yihe Deng, Yuanzhi Li, and Quanquan Gu. Understanding transferable representation learning and zero-shot transfer in clip. arXiv preprint arXiv:2310.00927, 2023.   \n[13] Imant Daunhawer, Alice Bizeul, Emanuele Palumbo, Alexander Marx, and Julia E Vogt. Identifiability results for multimodal contrastive learning. arXiv preprint arXiv:2303.09166, 2023.   \n[14] Yann Dubois, Stefano Ermon, Tatsunori B Hashimoto, and Percy S Liang. Improving selfsupervised learning by characterizing idealized representations. Advances in Neural Information Processing Systems, 35:11279-11296, 2022.   \n[15] Lijie Fan, Dilip Krishnan, Phillip Isola, Dina Katabi, and Yonglong Tian. Improving CLIP training with language rewrites. In Advances in Neural Information Processing Systems, 2023.   \n[16] Alex Fang, Gabriel lharco, Mitchell Wortsman, Yuhao Wan, Vaishaal Shankar, Achal Dave, and Ludwig Schmidt. Data determines distributional robustness in contrastive language image pre-training (CLIP). In International Conference on Machine Learning, 2022.   \n[17] Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander T Toshev, and Vaishaal Shankar. Data fitering networks. In NeurIPS 2023 Workshop on Distribution Shifts: New Frontiers with Foundation Models, 2023.   \n[18] Samir Yitzhak Gadre, Gabriel Mharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari, Giannis Daras, Sarah M Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexander Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, and Ludwig Schmidt. Datacomp: In search of the next generation of multimodal datasets. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023.   \n[19] Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems, 33:21271-21284, 2020.   \n[20] JeffZ HaoChen and Tengyu Ma. A theoretical study of inductive biases in contrastive learning. arXiv preprint arXiv:2211.14699, 2022.   \n[21] Jeff Z HaoChen, Colin Wei, Adrien Gaidon, and Tengyu Ma. Provable guarantees for selfsupervised deep learning with spectral contrastive loss. Advances in Neural Information Processing Systems, 34:5000-5011, 2021.   \n[22] Wei Huang, Yuan Cao, Haonan Wang, Xin Cao, and Taiji Suzuki. Graph neural networks provably benefit from structural information: A feature learning perspective. arXiv preprint arXiv:2306.13926, 2023.   \n[23] Wei Huang, Ye Shi, Zhongyi Cai, and Taiji Suzuki. Understanding convergence and generalization in federated learning through feature learning theory. In The Twelth International Conference on Learning Representations, 2023.   \n[24]  Weiran Huang, Mingyang Yi, Xuyang Zhao, and Zihao Jiang. Towards the generalization of contrastive self-supervised learning. arXiv preprint arXiv:2111.00743, 2021.   \n[25] Yu Huang, Chenzhuang Du, Zihui Xue, Xuanyao Chen, Hang Zhao, and Longbo Huang. What makes multi-modal learning better than single (provably). Advances in Neural Information Processing Systems, 34:10944-10956, 2021.   \n[26]  Samy Jelassi and Yuanzhi Li. Towards understanding how momentum improves generalization indeep learning. In International Conference on Machine Learning, pages 9965-10040. PMLR, 2022.   \n[27] Wenlong Ji, Zhun Deng, Ryumei Nakada, James Zou, and Linjun Zhang. The power of contrast for feature learning: A theoretical analysis. Journal of Machine Learning Research, 24(330):1-78, 2023.   \n[28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International conference on machine learning, pages 4904-4916. PMLR, 2021.   \n[29] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillp Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. Advances in neural information processing systems, 33: 18661-i8673, 2020.   \n[30] Yiwen Kou, Zixiang Chen, Yuan Cao, and Quanquan Gu. How does semi-supervised learing with pseudo-labelers work? a case study. In International Conference on Learning Representations, 2023.   \n[31] Yiwen Kou, Zixiang Chen, Yuanzhou Chen, and Quanquan Gu. Benign overfitting for two-layer relu networks. arXiv preprint arXiv:2303.04145, 2023.   \n[32] Jason D Lee, Qi Lei, Nikunj Saunshi, and Jiacheng Zhuo. Predicting what you already know helps: Provable self-supervised learning. Advances in Neural Information Processing Systems, 34:309-323, 2021.   \n[33] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping languageimage pre-training for unified vision-language understanding and generation. In International Conference on Machine Learning, pages 12888-12900. PMLR, 2022.   \n[34] Victor Weixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Yeung, and James Y Zou. Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning. Advances in Neural Information Processing Systems, 35:17612-17625, 2022.   \n[35] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023.   \n[36]  Prasanna Mayilvahanan, Thaddaus Wiedemer, Evgenia Rusak, Matthias Bethge, and Wieland Brendel. Does clip's generalization performance mainly stem from high train-test similarity? arXiv preprint arXiv:2310.09562, 2023.   \n[37]  Yifei Ming and Yixuan Li. Understanding retrieval-augmented task adaptation for visionlanguage models. arXiv preprint arXiv:2405.01468, 2024.   \n[38] Ryumei Nakada, Halil Ibrahim Gulluk, Zhun Deng, Wenlong Ji, James Zou, and Linjun Zhang. Understanding multimodal contrastive learning and incorporating unpaired data. In International Conference on Artifcial Intelligence and Statistics, pages 4348-4380. PMLR, 2023.   \n[39] Thao Nguyen, Samir Yitzhak Gadre, Gabriel lharco, Sewoong Oh, and Ludwig Schmidt. Improving multimodal datasets with image captioning. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023.   \n[40]  Thao Nguyen, Gabriel Ilharco, Mitchell Wortsman, Sewoong Oh, and Ludwig Schmidt. Quality not quantity: On the interaction between dataset design and robustness of CLIP. In Advances in Neural Information Processing Systems, 2022.   \n[41]  OpenA1. Gpt-4 technical report, 2023.   \n[42] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748-8763. PMLR, 2021.   \n[43] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.   \n[44]  Yunwei Ren and Yuanzhi Li. On the importance of contrastive loss in multimodal learning. arXiv preprint arXiv:2304.03717, 2023.   \n[45] Shibani Santurkar, Yann Dubois, Rohan Taori, Percy Liang, and Tatsunori Hashimoto. Is a caption worth a thousand images? a study on representation learning. In International Conference on Learning Representations, 2023.   \n[46] Nikunj Saunshi, Jordan Ash, Surbhi Goel, Dipendra Misra, Cyril Zhang, Sanjeev Arora, Sham Kakade, and Akshay Krishnamurthy. Understanding contrastive learning requires incorporating inductive biases. in International Conference on Machine Learning, pages 19250-19286. PMLR, 2022.   \n[47]  Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa R Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5b: An open large-scale dataset for training next generation image-text models. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022.   \n[48] James B Simon, Maksis Knutins, Liu Ziyin, Daniel Geisz, Abraham J Fetterman, and Joshua Albrecht. On the stepwise nature of self-supervised learning. In International Conference on Machine Learning, pages 31852-31876. PMLR, 2023.   \n[49]  Yuandong Tian. Understanding deep contrastive learning via coordinate-wise optimization. Advances in Neural Information Processing Systems, 35:19511-19522, 2022.   \n[50] Yuandong Tian, Xinlei Chen, and Surya Ganguli. Understanding self-supervised learning dynamics without contrastive pairs. In International Conference on Machine Learning, pages 10268-10278. PMLR, 2021.   \n[51]  Yuandong Tian, Lantao Yu, Xinlei Chen, and Surya Ganguli. Understanding self-supervised learning with dual deep networks. arXiv preprint arXiv:2010.00578, 2020.   \n[52]  Christopher Tosh, Akshay Krishnamurthy, and Daniel Hsu. Contrastive learning, multi-view redundancy, and linear models. In Algorithmic Learning Theory, pages 1179-1206. PMLR, 2021.   \n[53] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.   \n[54] Qizhou Wang, Yong Lin, Yongqiang Chen, Ludwig Schmidt, Bo Han, and Tong Zhang. Do clip models always generalize better than imagenet models? Advances in Neural Information Processing Systems, 2024.   \n[55]  Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment anduniformity onthehypersphere. In International Conference on Machine Learning, pages 9929-9939. PMLR, 2020.   \n[56]ZixinWen and Yuanzhi Li Towarduderstanding thefeature learning processof self-supervised contrastive learning. In International Conference on Machine Learning, pages 11112-11122. PMLR, 2021.   \n[57] Yihao Xue, Siddharth Joshi, Eric Gan, Pin-Yu Chen, and Baharan Mirzasoleiman. Which features are learnt by contrastive learning? on the role of simplicity bias in class collapse and feature suppression. In International Conference on Machine Learning, pages 38938-38970. PMLR, 2023.   \n[58] Yihao Xue, Siddharth Joshi, Dang Nguyen, and Baharan Mirzasoleiman. Understanding the robustness of multi-modal contrastive learning to distribution shift. arXiv preprint arXiv:2310.04971, 2023.   \n[59] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for computer vision. arXiv preprint arXiv:2111.11432, 2021.   \n[60] Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When and why vision-language models behave like bags-of-words, and what to do about it? arXiv e-prints, pages arXiv-2210, 2022.   \n[61]  Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. International Journal of Computer Vision, 130(9):2337-2348, 2022.   \n[62] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.   \n[63] Difan Zou, Yuan Cao, Yuanzhi Li, and Quanquan Gu. Understanding the generalization of adam in learning neural networks with proper regularization. In International Conference on Learning Representations, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A  Limitations and broader impact 16 ", "page_idx": 14}, {"type": "text", "text": "B Preliminary Lemmas 16 ", "page_idx": 14}, {"type": "text", "text": "C Single-modal Contrastive Learning: Proof of Theorem 4.2 17 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "C.1 First Stage . . 18   \nC.1.1 Dynamics of Signal Learning: Upper Bound 20   \nC.1.2 Dynamics of Noise Memorization: Lower Bound . 23   \nC.1.3 Noise Memorization: Proof of Lemma 5.2 . . 28   \nC.2 Second Stage . . 29   \nC.3  Downstream Task Performance . 36 ", "page_idx": 14}, {"type": "text", "text": "D Multi-Modal Contrastive Learning: Proof of Theorem 4.3 36 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "D.1 First Stage . . 36   \nD.1.1 Dynamics of Signal Learning: Lower Bound 37   \nD.1.2 Dynamics of Noise Memorization: Upper Bound . : 40   \nD.1.3 Signal Learning: Proof of Lemma 5.4 . . 46   \nD.2 Second Stage . . 47   \nD.3 Downstream Task Performance 51 ", "page_idx": 14}, {"type": "text", "text": "E  Additional Experimental Details 51 ", "page_idx": 14}, {"type": "text", "text": "A  Limitations and broader impact ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "While our theoretical analysis is novel in terms of optimization and generalization, the data model can be further modified to be more practical. Our theoretical analysis may be further used for empirical and theoretical studies of contrastive learning, especially multi-modal contrastive learning. However, we do not foresee a direct social impact from our theory. ", "page_idx": 15}, {"type": "text", "text": "B Preliminary Lemmas ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Before the proof, we introduce lemmas that are useful in proving our main theorem. ", "page_idx": 15}, {"type": "text", "text": "Lemma B.1. Let $x\\sim\\mathcal{N}(0,\\sigma^{2})$ .Then $\\begin{array}{r}{\\mathbb{P}(|x|\\leq c)=2\\mathrm{erf}\\left(\\frac{c}{\\sqrt{2}\\sigma}\\right)\\leq2\\sqrt{1-\\exp(-\\frac{2c^{2}}{\\sigma^{2}\\pi})}.}\\end{array}$ ", "page_idx": 15}, {"type": "text", "text": "Proof. The probability density function for $x$ is given by ", "page_idx": 15}, {"type": "equation", "text": "$$\nf(x)={\\frac{1}{\\sqrt{2\\pi}\\sigma}}\\exp\\left(-{\\frac{x^{2}}{2\\sigma^{2}}}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then we know that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}(|x|\\leq c)=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\int_{-c}^{c}\\exp\\left(-\\frac{x^{2}}{2\\sigma^{2}}\\right)d x.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By the definition of erf function ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname{erf}(c)=\\frac{2}{\\sqrt{\\pi}}\\int_{0}^{c}\\exp(-x^{2})d x,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and variable substitution yields ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname{erf}\\left({\\frac{c}{\\sqrt{2}\\sigma}}\\right)={\\frac{1}{\\sqrt{2\\pi}\\sigma}}\\int_{0}^{c}\\exp\\left(-{\\frac{x^{2}}{2\\sigma^{2}}}\\right)d x.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, we first conclude $\\begin{array}{r}{\\mathbb{P}(|x|\\leq c)=2\\mathrm{erf}\\left(\\frac{c}{\\sqrt{2}\\sigma}\\right)}\\end{array}$ ", "page_idx": 15}, {"type": "text", "text": "Next, by the inequality $\\operatorname{erf}(x)\\leq{\\sqrt{1-\\exp(-4x^{2}/\\pi)}}$ , we finally obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}(|x|\\leq c)\\leq2{\\sqrt{1-\\exp\\left(-{\\frac{2c^{2}}{\\sigma^{2}\\pi}}\\right)}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Lemma B.1 introduces an anti-concentration result. In later sections, this lemma will be used to show that with a relatively large initialization for the weight vector, some initial properties hold. ", "page_idx": 15}, {"type": "text", "text": "Lemma B.2. Under condition  tl $\\begin{array}{r l r l r l r l}{\\imath a t}&{{}d}&{}&{{\\geq}}&{}&{{{\\frac{400n}{\\sigma_{0}\\sigma_{\\xi}}}\\sqrt{\\frac{\\log(6n/\\delta)}{-\\pi\\log(1-\\delta^{2}/(4m^{2}))}},}}&{{a n d}}&{{}\\widetilde{d}}&{}&{{\\geq}}\\end{array}$ ),tnwihbbilaa ,wecawfrll [m ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lvert\\langle{{\\bf w}_{r}^{(0)}},{\\pmb{\\mu}}\\rangle\\rvert\\ge100\\cdot\\mathrm{SNR}\\sqrt{\\frac{8\\log(6n/\\delta)}{d}}n,}\\\\ &{\\lvert\\langle\\widetilde{{\\bf w}}_{r}^{(0)},\\widetilde{{\\pmb{\\mu}}}\\rangle\\rvert\\ge100\\cdot\\mathrm{SNR}\\sqrt{\\frac{8\\log(6n/\\delta)}{\\widetilde{d}}}n.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof of Lemma B.2. By Lemma B.1, because $\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\mu}\\rangle\\sim\\mathcal{N}(0,\\sigma_{0}^{2}\\|\\pmb{\\mu}\\|_{2}^{2})$ , we can show ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}\\big(|\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\mu}\\rangle|\\leq c\\big)\\leq2\\sqrt{1-\\exp\\left(-\\frac{2c^{2}}{\\sigma_{0}^{2}\\|\\pmb{\\mu}\\|_{2}^{2}\\pi}\\right)}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Let $\\begin{array}{r}{c=100\\cdot\\mathrm{SNR}\\sqrt{\\frac{8\\log(6n/\\delta)}{d}}n=100n\\|\\mu\\|_{2}\\sigma_{\\xi}^{-1}d^{-1}\\sqrt{8\\log(6n/\\delta)}}\\end{array}$ and plug it into the RHS of the above inequality, which becomes: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{RHS}=2\\sqrt{1-\\exp{\\left(-\\frac{160000\\log(6n/\\delta)n^{2}}{\\sigma_{0}^{2}\\sigma_{\\xi}^{2}d^{2}\\pi}\\right)}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then we ean vrify tat hen $d$ $\\begin{array}{r}{d\\geq\\frac{400n}{\\sigma_{0}\\sigma_{\\xi}}\\sqrt{\\frac{\\log\\left(6n/\\delta\\right)}{-\\pi\\log\\left(1-\\delta^{2}/\\left(4m^{2}\\right)\\right)}}}\\end{array}$ $\\mathrm{RHS}\\leq$ $\\delta/m$ . This suggests for a single neuron $r\\,\\in\\,[m]$ , we have $\\mathbb{P}(|\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\mu}\\rangle|\\,\\le\\,c)\\,\\le\\,\\delta/m$ Applying union bound, we can show the desired result. ", "page_idx": 16}, {"type": "text", "text": "Similarly, with the same procedure, we can prove the result for the other modality. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\lvert\\langle\\widetilde{\\mathbf{w}}_{r}^{(0)},\\widetilde{\\pmb{\\mu}}\\rangle\\rvert\\ge100\\cdot\\mathrm{SNR}\\sqrt{\\frac{8\\log(6n/\\delta)}{\\widetilde{d}}}n.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Lemma B.3 ([31]). Let $S_{1}\\,=\\,\\{i\\,\\in\\,[n]\\,:\\,y_{i}\\,=\\,1\\}$ and $S_{-1}\\,=\\,\\{i\\,\\in\\,[n]\\,:\\,y_{i}\\,=\\,-1\\}$ . Then with probability at least $1-\\delta$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n|S_{1}|,|S_{-1}|\\in\\left[\\frac{n}{2}-\\sqrt{\\frac{n}{2}\\log(4/\\delta)},\\frac{n}{2}+\\sqrt{\\frac{n}{2}\\log(4/\\delta)}\\right].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Lemma B.3 states that when the label is randomly sampled, the number of positive samples and negative samples is close to $\\begin{array}{l}{{\\frac{n}{2}}}\\end{array}$ ,adequately. ", "page_idx": 16}, {"type": "text", "text": "Lemma B.4 ([8]). Suppose that $d\\geq\\Omega(\\log(m n/\\delta))$ \uff0c $m=\\Omega(\\log(1/\\delta))$ . Then with probability at least $1-\\delta_{i}$ it satisfies that for all $r\\in[m],i\\in[n]$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\mu}\\rangle|\\leq\\sqrt{2\\log(8m/\\delta)}\\sigma_{0}\\|\\pmb{\\mu}\\|_{2}}\\\\ &{|\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle|\\leq2\\sqrt{\\log(8m n/\\delta)}\\sigma_{0}\\sigma_{\\xi}\\sqrt{d}}\\\\ &{\\quad|\\langle\\mathbf{w}_{r}^{0},\\pmb{\\epsilon}_{i}\\rangle|\\leq2\\sqrt{\\log(8m n/\\delta)}\\sigma_{0}\\sigma_{\\epsilon}\\sqrt{d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and for all $i\\in[n]$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sigma_{0}\\|\\pmb{\\mu}\\|_{2}/2\\leq\\underset{r\\in[m]}{\\operatorname*{max}}\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\mu}\\rangle\\leq\\sqrt{2\\log(8m/\\delta)}\\sigma_{0}\\|\\pmb{\\mu}\\|_{2}}\\\\ &{\\sigma_{0}\\sigma_{\\xi}\\sqrt{d}/4\\leq\\underset{r\\in[m]}{\\operatorname*{max}}\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle\\leq2\\sqrt{\\log(8m n/\\delta)}\\sigma_{0}\\sigma_{\\xi}\\sqrt{d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Lemma B.5 ([8]). Suppose that $\\delta>0$ and $d=\\Omega(\\log(6n/\\delta))$ .Then with probability $1-\\delta$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sigma_{\\xi}^{2}d/2\\le\\|\\xi_{i}\\|_{2}^{2}\\le3\\sigma_{\\xi}^{2}d/2,}\\\\ &{|\\langle\\xi_{i},\\xi_{i^{\\prime}}\\rangle|\\le2\\sigma_{\\xi}^{2}\\sqrt{d\\log(6n^{2}/\\delta)}}\\\\ &{|\\langle\\xi_{i},\\mu\\rangle|\\le\\|\\mu\\|_{2}\\sigma_{\\xi}\\sqrt{2\\log(6n/\\delta)}}\\\\ &{\\sigma_{\\epsilon}^{2}d/2\\le\\|\\epsilon_{i}\\|_{2}^{2}\\le3\\sigma_{\\epsilon}^{2}d/2,}\\\\ &{|\\langle\\epsilon_{i},\\xi_{i^{\\prime}}\\rangle|\\le2\\sigma_{\\epsilon}\\sigma_{\\xi}\\sqrt{d\\log(6n^{2}/\\delta)}}\\\\ &{|\\langle\\epsilon_{i},\\mu\\rangle|\\le\\|\\mu\\|_{2}\\sigma_{\\epsilon}\\sqrt{2\\log(6n/\\delta)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for all $i,i^{\\prime}\\in[n]$ ", "page_idx": 16}, {"type": "text", "text": "C Single-modal Contrastive Learning: Proof of Theorem 4.2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we provide the proof for Theorem 4.2, which states main results of single modal learning. The training dynamics are based on the coefficient iterations presented in Lemma 5.1. Below, we provide a proof for this lemma: ", "page_idx": 16}, {"type": "text", "text": "Proof of Lemma 5.1. Recall that the weight decomposition is expressed as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{w}_{r}^{(t)}=\\mathbf{w}_{r}^{(0)}+\\gamma_{r}^{(t)}\\|\\pmb{\\mu}\\|_{2}^{-2}\\pmb{\\mu}+\\sum_{i=1}^{n}\\rho_{r,i}^{(t)}\\|\\pmb{\\xi}_{i}\\|_{2}^{-2}\\pmb{\\xi}_{i}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We plug it into the gradient descent update as described by Equation 10 yields ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\bf w}_{r}^{(t+1)}={\\bf w}_{r}^{(0)}+\\gamma_{r}^{(t+1)}\\|\\mu\\|_{2}^{-2}\\mu+\\sum_{i=1}^{n}\\rho_{r,i}^{(t+1)}\\|\\xi_{i}\\|_{2}^{-2}\\xi_{i}}\\ ~}\\\\ {{\\displaystyle={\\bf w}_{r}^{(0)}+\\gamma_{r}^{(t)}\\|\\mu\\|_{2}^{-2}\\mu+\\sum_{i=1}^{n}\\rho_{r,i}^{(t)}\\|\\xi_{i}\\|_{2}^{-2}\\xi_{i}+\\frac{\\eta}{n m\\tau}\\sum_{i=1}^{n}(1-\\ell_{i}^{(t)})h_{r}^{(t)}(\\hat{\\bf x}_{i}^{(1)})h^{\\prime(t)}({\\bf x}_{i}^{(1)})y_{i}\\mu}\\ ~}\\\\ {{\\displaystyle~+\\,\\frac{\\eta}{n m\\tau}\\sum_{i=1}^{n}(1-\\ell_{i}^{\\prime(t)})h_{r}^{(t)}(\\hat{\\bf x}_{i}^{(2)})h_{r}^{\\prime(t)}({\\bf x}_{i}^{(2)})\\xi_{i}-\\frac{\\eta}{n m\\tau}\\sum_{i=1}^{n}\\sum_{j\\neq i}^{n}\\ell_{i,j}^{\\prime(t)}h_{r}^{(t)}({\\bf x}_{j}^{(1)})h^{\\prime(t)}({\\bf x}_{i}^{(1)})y_{i}\\mu}\\ ~}\\\\ {{\\displaystyle~-\\,\\frac{\\eta}{n m\\tau}\\sum_{i=1}^{n}\\sum_{j\\neq i}^{M}\\ell_{i,j}^{\\prime(t)}h_{r}^{(t)}({\\bf x}_{j}^{(2)})h_{r}^{\\prime(t)}({\\bf x}_{i}^{(2)})\\xi_{i}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By comparing the coefficients in front of $\\pmb{\\mu}$ and $\\xi_{i}$ on both sides of the equation, we can obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\gamma_{r}^{(t+1)}=\\gamma_{r}^{(t)}+\\frac{\\eta}{n m\\tau}\\sum_{i=1}^{n}\\left[(1-\\ell_{i}^{\\prime(t)})h_{r}^{(t)}(\\widehat\\mathbf{x}_{i}^{(1)})-\\displaystyle\\sum_{j\\neq i}^{M}\\ell_{i,j}^{\\prime(t)}h_{r}^{(t)}(\\mathbf{x}_{j}^{(1)})\\right]h_{r}^{\\prime(t)}(\\mathbf{x}_{i}^{(1)})y_{i}\\|\\mu\\|_{2}^{2}},}\\\\ {{\\displaystyle\\rho_{r,i}^{(t+1)}=\\rho_{r,i}^{(t)}+\\!\\frac{\\eta}{n m\\tau}\\big[(1-\\ell_{i}^{\\prime(t)})h_{r}(\\widehat\\mathbf{x}_{i}^{(2)})-\\displaystyle\\sum_{j\\neq i}^{M}\\ell_{i,j}^{\\prime(t)}h_{r}(\\mathbf{x}_{j}^{(2)})\\big]h_{r}^{\\prime(t)}(\\mathbf{x}_{i}^{(2)})\\|\\xi_{i}\\|_{2}^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which completes the proof. ", "page_idx": 17}, {"type": "text", "text": "According to the behavior of the defined loss derivative (8), we split the entire training dynamics into two phases. In the first stage, the loss derivative remains close to its initial value as the similarity is small from initialization. Later, as the similarity grows to a constant value, the loss derivative is no longer close to the initial value, and the dynamics transition to the second stage. In this stage, the similarity increases logarithmically, and the empirical loss converges. ", "page_idx": 17}, {"type": "text", "text": "C.1  First Stage ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In the first stage, the derivative of the loss is close to its initial value because the similarity is small.   \nBelow, we provide a useful lemma for establishing such a result. ", "page_idx": 17}, {"type": "text", "text": "Lemma C.1. Suppose that $\\gamma_{r}^{(t)}\\,=\\,O(1)$ and $\\rho_{r,i}^{(t)}\\,=\\,O(1)$ for all $r\\,\\in\\,[m]$ and $i\\;\\in\\;[n]$ Under Assumption 4.1, then for any $\\delta>0$ with probability at least $1-\\delta$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\langle\\mathbf{w}_{r}^{(t)}-\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle-\\rho_{r,i}^{(t)}|\\leq5\\sqrt{\\frac{\\log(6n^{2}/\\delta)}{d}}n}\\\\ &{|\\langle\\mathbf{w}_{r}^{(t)}-\\mathbf{w}_{r}^{(0)},\\pmb{\\mu}\\rangle-\\gamma_{r}^{(t)}|\\leq\\mathrm{SNR}\\sqrt{\\frac{8\\log(6n/\\delta)}{d}}n}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for all $r\\in[m],\\,i\\in[n]$ ", "page_idx": 17}, {"type": "text", "text": "Proof of Lemma C.1. From the signal-noise decomposition of $\\mathbf{w}_{r}^{(t)}$ wederive ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{|\\langle\\mathbf{w}_{r}^{(t)}-\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle-\\rho_{r,i}^{(t)}|\\overset{(a)}{=}|\\gamma_{r}^{(t)}\\langle\\pmb{\\mu},\\pmb{\\xi}_{i}\\rangle||\\pmb{\\mu}||_{2}^{-2}+\\displaystyle\\sum_{i^{\\prime}=1}^{n}\\rho_{r,i}^{(t)}\\langle\\pmb{\\xi}_{i^{\\prime}},\\pmb{\\xi}_{i}\\rangle||\\pmb{\\xi}_{i^{\\prime}}||_{2}^{-2}|}\\\\ &{}&{\\overset{(b)}{\\leq}\\|\\pmb{\\mu}\\|_{2}^{-1}\\sigma_{\\xi}\\sqrt{2\\log(6n/\\delta)}+4\\sqrt{\\frac{\\log(6n^{2}/\\delta)}{d}}n}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\stackrel{(c)}{\\leq}5\\sqrt{\\frac{\\log(6n^{2}/\\delta)}{d}}n.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Equation (a) results from the weight decomposition (see Equation 11). In the first stage, we used the upper bounds for $|\\gamma_{r}^{(t)}|$ and $\\left|\\rho_{r}^{(t)}\\right|$ and applied Lemma B.5 in inequality b). Finally, inequality (c) follows from the condition $n\\mathrm{SNR}^{2}=\\Theta(1)$ ", "page_idx": 18}, {"type": "text", "text": "Further, ", "page_idx": 18}, {"type": "equation", "text": "$$\n|\\langle\\mathbf{w}_{r}^{(t)}-\\mathbf{w}_{r}^{(0)},\\pmb{\\mu}\\rangle-\\gamma_{r}^{(t)}|=|\\sum_{i=1}^{n}\\rho_{r,i}^{(t)}||\\pmb{\\xi}_{i}||_{2}^{-2}\\langle\\pmb{\\xi}_{i},\\pmb{\\mu}\\rangle|\\leq2n\\cdot\\mathrm{SNR}\\sqrt{\\frac{2\\log(6n/\\delta)}{d}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where we have used Lemma B.5. ", "page_idx": 18}, {"type": "text", "text": "Now, we proceed to the lemma concerning the derivative of the loss as follows: ", "page_idx": 18}, {"type": "text", "text": "LemmaC.2. $I f\\operatorname*{max}\\{\\gamma_{r}^{(t)},\\rho_{r,i}^{(t)}\\}=O(1)$ and under Assumption 4.1, there exists a constant $C_{\\ell}>1$ Suchthat ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{1}{C_{\\ell}(1+M)}\\le\\ell_{i}^{\\prime(t)}\\le\\frac{C_{\\ell}}{1+M},\\quad\\frac{1}{C_{\\ell}(M+1)}\\le\\ell_{i,j}^{\\prime(t)}\\le\\frac{C_{\\ell}}{1+M},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for all $i\\in[n]$ ", "page_idx": 18}, {"type": "text", "text": "Proof of Lemma $C.2$ . From the update of $\\mathbf{w}_{r}^{(t)}$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\langle\\mathbf{w}_{r}^{(t)},\\pmb{\\xi}_{i}\\rangle|\\overset{(a)}{\\le}|\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle|+\\rho_{r,i}^{(t)}+5\\sqrt{\\frac{\\log(6n^{2}/\\delta)}{d}}n}\\\\ &{\\overset{(b)}{\\le}2\\sqrt{\\log(8m n/\\delta)}\\sigma_{0}\\sigma_{\\xi}\\sqrt{d}+\\rho_{r,i}^{(t)}+5\\sqrt{\\frac{\\log(6n^{2}/\\delta)}{d}}n}\\\\ &{\\overset{(c)}{=}O(1),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where (a) is by Lemma C.1, and (b) is by Lemma B.4. Finally, in inequality (c) we have used the condition that oo\u2264 $\\begin{array}{r}{\\sigma_{0}\\leq\\frac{1}{2\\sqrt{\\log(8m n/\\delta)}\\sigma_{\\xi}\\sqrt{d}}}\\end{array}$ and $d>n^{2}\\log(6n^{2}/\\dot{\\delta})$ according to Assumption 4.1, and $\\operatorname*{max}\\{\\gamma_{r}^{(t)},\\rho_{r,i}^{(t)}\\}=O(1)$ Athesametime, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\langle{\\mathbf w}_{r}^{(t)},{\\boldsymbol\\mu}\\rangle|\\overset{(a)}{\\leq}|\\langle{\\mathbf w}_{r}^{(0)},{\\boldsymbol\\mu}\\rangle|+\\gamma_{r}^{(t)}+\\mathrm{SNR}\\sqrt{\\frac{8\\log(6n/\\delta)}{d}}n}\\\\ &{\\qquad\\qquad\\overset{(b)}{\\leq}\\sqrt{2\\log(8m/\\delta)}\\sigma_{0}\\|{\\boldsymbol\\mu}\\|_{2}+\\mathrm{SNR}\\sqrt{\\frac{8\\log(6n/\\delta)}{d}}n}\\\\ &{\\qquad\\qquad\\overset{(c)}{=}O(1),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where (a) is by Lemma C.1, (b) is Lemma B.4, and (c) is by $\\begin{array}{r}{\\sigma_{0}\\ \\leq\\ \\frac{1}{2\\sqrt{\\log(8m/\\delta)}\\|\\pmb{\\mu}\\|_{2}}}\\end{array}$ and $d\\,>$ $\\mathrm{SNR}^{2}n^{2}\\log(6n^{2}/\\delta)$ according to Assumption 4.1,and $\\operatorname*{max}\\{\\gamma_{r}^{(t)},\\rho_{r,i}^{(t)}\\}=O(1)$ Besides, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\langle\\mathbf{w}_{r}^{(t)},\\epsilon_{i}\\rangle|=|\\langle\\mathbf{w}_{r}^{(0)},\\epsilon_{i}\\rangle+\\gamma_{r}^{(t)}\\|\\mu\\|_{2}^{-2}\\langle\\mu,\\epsilon_{i}\\rangle+\\displaystyle\\sum_{i=i}^{n}\\rho_{r,i}^{(t)}\\|\\pmb{\\xi}_{i}\\|_{2}^{-2}\\langle\\pmb{\\xi}_{i},\\epsilon_{i}\\rangle|}\\\\ &{\\overset{(a)}{\\leq}|\\langle\\mathbf{w}_{r}^{(0)},\\epsilon_{i}\\rangle|+\\|\\mu\\|_{2}^{-1}\\sigma_{\\epsilon}\\sqrt{2\\log(6n/\\delta)}+4\\sqrt{\\frac{\\log(6n^{2}/\\delta)}{d}}\\sigma_{\\epsilon}\\sigma_{\\xi}^{-1}n}\\\\ &{\\overset{(b)}{\\leq}2\\sqrt{\\log(8m n/\\delta)}\\sigma_{0}\\sigma_{\\epsilon}\\sqrt{d}+\\|\\mu\\|_{2}^{-1}\\sigma_{\\epsilon}\\sqrt{2\\log(6n/\\delta)}+4\\sqrt{\\frac{\\log(6n^{2}/\\delta)}{d}}\\sigma_{\\epsilon}\\sigma_{\\xi}^{-1}n}\\\\ &{\\overset{(c)}{=}O(1),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where (a) follows from Lemma B.5, (b) from Lemma B.4, and (c) from the conditions $\\sigma_{0}~\\leq$ 2\u221alog(8mn/8)oeVa e $\\begin{array}{r}{\\sigma_{\\epsilon}\\leq\\frac{\\|\\mu\\|_{2}}{\\sqrt{2\\log(6n/\\delta)}}}\\end{array}$ $d>n^{2}\\log(6n^{2}/\\delta)$ ,and $\\sigma_{\\epsilon}<\\sigma_{\\xi}$ ", "page_idx": 18}, {"type": "text", "text": "Next, we calculate the upper bound of the similarity measure. First, we examine the negative pair. For any $i,j\\in[n]$ wehave ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Sim}_{\\mathbf{h}}(\\mathbf{x}_{i},\\mathbf{x}_{j})=\\displaystyle\\frac{1}{m}\\langle\\mathbf{h}(\\mathbf{x}_{i}^{(1)}),\\mathrm{sg}(\\mathbf{h}(\\mathbf{x}_{j}^{(1)}))\\rangle+\\displaystyle\\frac{1}{m}\\langle\\mathbf{h}(\\mathbf{x}_{i}^{(2)}),\\mathrm{sg}(\\mathbf{h}(\\mathbf{x}_{j}^{(2)}))\\rangle}\\\\ &{\\quad\\quad\\quad\\quad=\\displaystyle\\frac{1}{m}\\sum_{r=1}^{m}\\sigma(\\langle\\mathbf{w}_{r}^{(t)},\\xi_{i}\\rangle)\\sigma(\\langle\\mathbf{w}_{r}^{(t)},\\xi_{j}\\rangle)+\\displaystyle\\frac{1}{m}\\sum_{r=1}^{m}\\sigma(\\langle\\mathbf{w}_{r}^{(t)},y_{i}\\mu\\rangle)\\sigma(\\langle\\mathbf{w}_{r}^{(t)},y_{j}\\mu\\rangle)}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\operatorname*{max}\\{|\\langle\\mathbf{w}_{r}^{(t)},y_{i}\\mu\\rangle\\langle\\mathbf{w}_{r}^{(t)},y_{j}\\mu\\rangle|,|\\langle\\mathbf{w}_{r}^{(t)},\\xi_{i}\\rangle\\langle\\mathbf{w}_{r}^{(t)},\\xi_{j}\\rangle|\\}=O(1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Similarly, for positive pair, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathrm{Sim}_{\\mathbf{h}}(\\mathbf{x}_{i},\\widehat{\\mathbf{x}}_{j})=\\displaystyle\\frac{1}{m}\\sum_{r=1}^{m}\\sigma(\\langle\\mathbf{w}_{r}^{(t)},\\xi_{i}\\rangle)\\sigma(\\langle\\mathbf{w}_{r}^{(t)},\\xi_{i}+\\epsilon_{i}\\rangle)+\\displaystyle\\frac{1}{m}\\sum_{r=1}^{m}\\sigma(\\langle\\mathbf{w}_{r}^{(t)},y_{i}\\mu\\rangle)\\sigma(\\langle\\mathbf{w}_{r}^{(t)},y_{i}\\mu\\rangle)}\\\\ {\\displaystyle\\leq\\operatorname*{max}\\{|\\langle\\mathbf{w}_{r}^{(t)},y_{i}\\mu\\rangle\\langle\\mathbf{w}_{r}^{(t)},y_{j}\\mu\\rangle|,|\\langle\\mathbf{w}_{r}^{(t)},\\xi_{i}\\rangle\\langle\\mathbf{w}_{r}^{(t)},\\xi_{i}+\\epsilon_{i}\\rangle|\\}=O(1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "According to the above result, we can say that $1\\leq e^{\\operatorname{Sim}_{\\mathbf{h}}(\\mathbf{x},\\mathbf{x}^{\\prime})}\\leq C_{\\ell}$ where $C_{\\ell}$ is a positive constant. Then we can provide the upper bound for $\\ell_{i}^{\\prime}$ and $\\ell_{i,j}^{\\prime}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ell_{i}^{\\prime(t)}=\\frac{e^{\\mathrm{Sim}_{\\mathbf{h}}(\\mathbf{x}_{i},\\mathbf{\\hat{x}}_{i})/\\tau}}{e^{\\mathrm{Sim}_{\\mathbf{h}}(\\mathbf{x}_{i},\\mathbf{\\hat{x}}_{i})/\\tau}+\\sum_{j\\neq i}^{M}e^{\\mathrm{Sim}_{\\mathbf{h}}(\\mathbf{x}_{i},\\mathbf{x}_{j})/\\tau}}\\leq\\frac{C_{\\ell}}{1+M},}\\\\ &{\\ell_{i}^{\\prime(t)}=\\frac{e^{\\mathrm{Sim}_{\\mathbf{h}}(\\mathbf{x}_{i},\\mathbf{\\hat{x}}_{i})/\\tau}}{e^{\\mathrm{Sim}_{\\mathbf{h}}(\\mathbf{x}_{i},\\mathbf{\\hat{x}}_{i})/\\tau}+\\sum_{j\\neq i}^{M}e^{\\mathrm{Sim}_{\\mathbf{h}}(\\mathbf{x}_{i},\\mathbf{x}_{j})/\\tau}}\\geq\\frac{1}{C_{\\ell}(1+M)},}\\\\ &{\\ell_{i,j}^{\\prime(t)}=\\frac{e^{\\mathrm{Sim}_{\\mathbf{h}}(\\mathbf{x}_{i},\\mathbf{x}_{j})/\\tau}}{e^{\\mathrm{Sim}_{\\mathbf{h}}(\\mathbf{x}_{i},\\mathbf{\\hat{x}}_{i})/\\tau}+\\sum_{j\\neq i}^{M}e^{\\mathrm{Sim}_{\\mathbf{h}}(\\mathbf{x}_{i},\\mathbf{x}_{j})/\\tau}}\\geq\\frac{1}{C_{\\ell}(M+1)},}\\\\ &{\\ell_{i,j}^{\\prime(t)}=\\frac{e^{\\mathrm{Sim}_{\\mathbf{h}}(\\mathbf{x}_{i},\\mathbf{x}_{j})/\\tau}}{e^{\\mathrm{Sim}_{\\mathbf{h}}(\\mathbf{x}_{i},\\mathbf{\\hat{x}}_{i})/\\tau}+\\sum_{j\\neq i}^{M}e^{\\mathrm{Sim}_{\\mathbf{h}}(\\mathbf{x}_{i},\\mathbf{x}_{j})/\\tau}}\\leq\\frac{C_{\\ell}}{M+1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This completes the proof. ", "page_idx": 19}, {"type": "text", "text": "C.1.1 Dynamics of Signal Learning: Upper Bound ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In the first stage, the growth rate of signal learning is exponential. We establish an upper bound for the growth of signal learning. ", "page_idx": 19}, {"type": "text", "text": "Then we consider the growthof signallearning coeficient $\\gamma_{r}^{\\left(t\\right)}$ . Depending o the initialization, we define $\\mathcal{U}_{+}^{(t)}=\\{r\\in[m]:\\langle\\mathbf{w}_{r}^{(t)},\\pmb{\\mu}\\rangle>0\\}$ and $\\bar{\\mathcal{U}}_{-}^{(t)}=\\{r\\in[m]:\\langle\\mathbf{w}_{r}^{\\bar{(t)}},\\pmb{\\mu}\\rangle^{-}<0\\}$ ", "page_idx": 19}, {"type": "text", "text": "Lemma C.3. Under the condition $\\begin{array}{r}{d\\geq\\frac{400n}{\\sigma_{0}\\sigma_{\\xi}}\\sqrt{\\frac{\\log\\left(6n/\\delta\\right)}{-\\pi\\log\\left(1-\\delta^{2}/\\left(4m^{2}\\right)\\right)}}}\\end{array}$ log(-8/(4m2) and Assumption 4.1, for al t \u2265 0, we have $\\mathcal{U}_{+}^{(t)}\\,=\\mathcal{U}_{+}^{(0)}$ \uff0c $\\mathcal{U}_{-}^{(t)}\\,=\\mathcal{U}_{-}^{(0)}$ and $\\gamma_{r}^{\\left(t\\right)}\\,>\\,0$ is an increasing sequence forall $r\\in\\mathcal{U}_{+}^{(0)}$ and $\\gamma_{r}^{\\left(t\\right)}\\leq0$ and is a decreasing sequence for all $r\\in\\mathcal{U}_{-}^{(0)}$ ", "page_idx": 19}, {"type": "text", "text": "Proof of Lemma C.3. We prove the claims by induction. To better understand the dynamics, we first derive the propagation fo sgnal earnin from the firststep. For $r\\in\\mathcal{U}_{+}^{(0)}$ $\\langle\\mathbf{w}_{r}^{(0)},\\mu\\rangle>0$ wWe cansee ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\gamma_{r}^{(1)}=\\gamma_{r}^{(0)}\\!+\\!\\frac{\\eta}{n m\\tau}\\sum_{i=1}^{n}(1-\\ell_{i}^{\\prime(0)})\\sigma(\\langle{\\mathbf w}_{r}^{(0)},y_{i}\\mu\\rangle)\\sigma^{\\prime}(\\langle{\\mathbf w}_{r}^{(0)},y_{i}\\mu\\rangle)y_{i}\\|\\mu\\|_{2}^{2}}\\\\ {\\displaystyle\\qquad-\\,\\frac{\\eta}{n m\\tau}\\sum_{i=1}^{n}\\sum_{j\\neq i}^{M}\\ell_{i,j}^{\\prime(0)}\\sigma(\\langle{\\mathbf w}_{r}^{(0)},y_{j}\\mu\\rangle)\\sigma^{\\prime}(\\langle{\\mathbf w}_{r}^{(0)},y_{i}\\mu\\rangle)y_{i}\\|\\mu\\|_{2}^{2}}\\\\ {\\displaystyle=\\gamma_{r}^{(0)}\\!+\\!\\frac{\\eta}{n m\\tau}\\sum_{i\\neq i=1}^{n}(1-\\ell_{i}^{\\prime(0)})\\sigma(\\langle{\\mathbf w}_{r}^{(0)},y_{i}\\mu\\rangle)\\sigma^{\\prime}(\\langle{\\mathbf w}_{r}^{(0)},y_{i}\\mu\\rangle)y_{i}\\|\\mu\\|_{2}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad-\\,\\frac{\\eta}{n m\\tau}\\displaystyle\\sum_{i:y_{i}=1}^{n}\\sum_{j:y_{j}=-1}^{M}\\ell_{i,j}^{\\prime(0)}\\sigma(\\langle\\mathbf{w}_{r}^{(0)},y_{j}\\mu\\rangle)\\sigma^{\\prime}(\\langle\\mathbf{w}_{r}^{(0)},y_{i}\\mu\\rangle)y_{i}\\|\\mu\\|_{2}^{2}}\\\\ &{\\quad=\\displaystyle\\frac{\\eta}{n m\\tau}\\displaystyle\\sum_{i:y_{i}=1}^{n}(1-\\ell_{i}^{\\prime(0)})\\langle\\mathbf{w}_{r}^{(0)},\\mu\\rangle\\|\\mu\\|_{2}^{2}>0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where last qulit is by $\\langle\\mathbf{w}_{r}^{(0)},\\mu\\rangle>0$ $\\gamma_{r}^{(0)}=0$ adf $y_{j}\\neq y_{i}$ Thus, we verifythat the sif $\\gamma_{r}^{(1)}$ follow is nitialization and $\\gamma_{r}^{(1)}>0$ ", "page_idx": 20}, {"type": "text", "text": "Next, we shw the propagation ofiner prot $t=1$ for $r\\in\\mathcal{U}_{+}^{(0)}$ ,we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\mathbf{w}_{r}^{(1)},\\boldsymbol{\\mu}\\rangle\\overset{(a)}{\\geq}\\langle\\mathbf{w}_{r}^{(0)},\\boldsymbol{\\mu}\\rangle+\\gamma_{r}^{(1)}-\\mathrm{SNR}\\sqrt{\\frac{8\\log\\left(6n/\\delta\\right)}{d}}n}\\\\ &{\\overset{(b)}{\\geq}0.99\\langle\\mathbf{w}_{r}^{(0)},\\boldsymbol{\\mu}\\rangle+\\gamma_{r}^{(1)}>0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where inequality (a) is by Lemma C.1, the second inequality (b) is by Lemma B.2, and the last by $\\gamma_{r}^{(1)}>0$ Hence we vrify $\\mathcal{U}_{+}^{(1)}=\\mathcal{U}_{+}^{(0)}$ ", "page_idx": 20}, {"type": "text", "text": "Now suppose a iteration $t$ the claims are satisd aly $\\langle\\mathbf{w}_{r}^{(t)},\\mu\\rangle>0$ and $\\gamma_{r}^{(t)}\\geq\\gamma_{r}^{(t-1)}\\geq0$ $r\\in\\mathcal{U}_{+}^{(0)}$ Then foloing saran for $r\\in\\mathcal{U}_{+}^{(0)}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\gamma_{r}^{(t+1)}=\\gamma_{r}^{(t)}+\\frac{\\eta}{n m\\tau}\\sum_{i:y_{i}=1}^{n}(1-\\ell_{i}^{\\prime(t)})\\langle\\mathbf{w}_{r}^{(t)},\\pmb{\\mu}\\rangle\\|\\pmb{\\mu}\\|_{2}^{2}\\ge\\gamma_{r}^{(t)}\\ge0,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we use the induction condition that $\\langle\\mathbf{w}_{r}^{(t)},\\mu\\rangle>0$ . Further by Lemma C.1 and B.2 ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\langle\\mathbf{w}_{r}^{(t+1)},\\pmb{\\mu}\\rangle\\geq0.99\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\mu}\\rangle+\\gamma_{r}^{(t+1)}>0,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we use $\\gamma_{r}^{(t+1)}\\geq0$ This completes the induction for $r\\in\\mathcal{U}_{+}^{(0)}$ ", "page_idx": 20}, {"type": "text", "text": "Similarly, for those neuron $r$ that satisfies $\\langle\\mathbf{w}_{r}^{0},\\pmb{\\mu}\\rangle<0$ , i.e., $r\\in\\mathcal{U}_{-}^{(0)}$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma_{r}^{(1)}=\\gamma_{r}^{(0)}\\displaystyle+\\frac{\\eta}{n m\\tau}\\sum_{i=1}^{n}(1-\\ell_{i}^{\\prime(0)})\\sigma(\\langle\\mathbf{w}_{r}^{(0)},y_{i}\\mu\\rangle)\\sigma^{\\prime}(\\langle\\mathbf{w}_{r}^{(0)},y_{i}\\mu\\rangle)y_{i}\\|\\mu\\|_{2}^{2}}\\\\ &{\\qquad-\\,\\displaystyle\\frac{\\eta}{n m\\tau}\\sum_{i=1}^{n}\\sum_{j\\neq i}^{M}\\ell_{i,j}^{\\prime(0)}\\sigma(\\langle\\mathbf{w}_{r}^{(0)},y_{j}\\mu\\rangle)\\sigma^{\\prime}(\\langle\\mathbf{w}_{r}^{(0)},y_{i}\\mu\\rangle)y_{i}\\|\\mu\\|_{2}^{2}}\\\\ &{\\qquad=-\\displaystyle\\frac{\\eta}{n m\\tau}\\sum_{i;y_{i}=-1}^{n}(1-\\ell_{i}^{\\prime(0)})\\langle\\mathbf{w}_{r}^{(0)},y_{i}\\mu\\rangle\\|\\mu\\|_{2}^{2}<0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the last equality is by $\\langle\\mathbf{w}_{r}^{(0)},\\mu\\rangle<0$ $y_{i}=-1$ the proerty ofRLUactivation, an the t that $y_{j}\\neq y_{i}$ in the negative pair term. Hence we see $\\gamma_{r}^{(1)}\\leq\\gamma_{r}^{(0)}=0$ ", "page_idx": 20}, {"type": "text", "text": "Similarly, for the inner product at $t=1$ \uff0c ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\mathbf w_{r}^{(1)},\\pmb{\\mu}\\rangle=\\langle\\mathbf w_{r}^{(0)},\\pmb{\\mu}\\rangle+\\gamma_{r}^{(1)}+\\displaystyle\\sum_{i=1}^{n}\\rho_{r,i}^{(t)}\\|\\pmb{\\xi}_{i}\\|_{2}^{-2}\\langle\\pmb{\\xi}_{i},\\pmb{\\mu}\\rangle}\\\\ &{\\qquad\\qquad\\leq\\langle\\mathbf w_{r}^{(0)},\\pmb{\\mu}\\rangle+\\gamma_{r}^{(1)}+\\mathrm{SNR}\\sqrt{\\displaystyle\\frac{8\\log\\left(6n/\\delta\\right)}{d}}n}\\\\ &{\\qquad\\qquad\\leq-0.99|\\langle\\mathbf w_{r}^{(0)},\\pmb{\\mu}\\rangle|+\\gamma_{r}^{(1)}<0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the first inequity is by Lemma C.1, the second inequality follows from Lemma B.2, and the last inequality follows from $\\gamma_{r}^{(1)}\\leq0$ ", "page_idx": 20}, {"type": "text", "text": "Now suppose at iteration $t$ the claims are satisfed, namely $\\langle\\mathbf{w}_{r}^{(t)},\\mu\\rangle<0$ and $\\gamma_{r}^{(t)}\\leq\\gamma_{r}^{(t-1)}\\leq0$ for $r\\in\\mathcal{U}_{+}^{(0)}$ Thenfllowin ilarar f $r\\in\\mathcal{U}_{-}^{(0)}$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\gamma_{r}^{(t+1)}=\\gamma_{r}^{(t)}-\\frac{\\eta}{n m\\tau}\\sum_{\\substack{i:y_{i}=-1}}^{n}(1-\\ell_{i}^{\\prime(t)})\\langle\\mathbf{w}_{r}^{(t)},\\pmb{\\mu}\\rangle\\|\\pmb{\\mu}\\|_{2}^{2}\\leq\\gamma_{r}^{(t)}\\leq0,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where we use the induction condition that $\\langle\\mathbf{w}_{r}^{(t)},\\mu\\rangle<0$ .Further ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\langle\\mathbf{w}_{r}^{(t+1)},\\boldsymbol{\\mu}\\rangle=\\langle\\mathbf{w}_{r}^{(0)},\\boldsymbol{\\mu}\\rangle+\\gamma_{r}^{(t+1)}+\\sum_{i=1}^{n}\\rho_{r,i}^{(t)}\\|\\pmb{\\xi}_{i}\\|_{2}^{-2}\\langle\\pmb{\\xi}_{i},\\boldsymbol{\\mu}\\rangle}}\\\\ &{}&{\\stackrel{(a)}{\\le}\\langle\\mathbf{w}_{r}^{(0)},\\boldsymbol{\\mu}\\rangle+\\gamma_{r}^{(t+1)}+\\mathrm{SNR}\\sqrt{\\frac{8\\log\\left(6n/\\delta\\right)}{d}}n\\stackrel{(b)}{<}0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where inequality (a) follows from Lemma C.1; we use $\\gamma_{r}^{(t+1)}\\,\\leq\\,0$ and Lemma B.2 in deriving inequality (b). This completes the induction for $r\\in\\mathcal{U}_{-}^{(0)}$ ", "page_idx": 21}, {"type": "text", "text": "With Lemma C.3 at hand, we are ready to demonstrate the upper bound of the growth rate for signal learning. ", "page_idx": 21}, {"type": "text", "text": "Lemma C.4. With the same condition as in Lemma C.2 and Lemma C.3 and $n\\geq2500\\log(4/\\delta),$ defne $A_{r}^{(t)}\\,=\\,\\gamma_{r}^{(t)}\\,+\\,\\langle{\\bf w}_{r}^{(0)},{\\pmb\\mu}\\rangle$ $r\\in\\mathcal{U}_{+}^{(0)}$ and $A_{r}^{(t)}\\,=\\,-\\gamma_{r}^{(t)}\\,-\\,\\langle{\\bf w}_{r}^{(0)},{\\pmb\\mu}\\rangle$ for $r\\in\\mathcal{U}_{-}^{(0)}$ .With probability atleast $1-\\delta$ ,we have ", "page_idx": 21}, {"type": "equation", "text": "$$\nA_{r}^{(t)}\\leq\\left(1+\\frac{0.52\\eta\\|\\pmb{\\mu}\\|_{2}^{2}}{m\\tau}\\right)A_{r}^{(0)}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. Lemma C.3 suggests that for $r\\,\\in\\,[m]$ , we can upper bound for $|\\gamma_{r}^{(t)}|$ .Without loss of generality we consider $r\\in\\mathcal{U}_{+}^{(0)}$ By Lemma .1 we an se ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\mathbf w_{r}^{(t)},\\pmb\\mu\\rangle\\leq\\gamma_{r}^{(t)}+\\langle\\mathbf w_{r}^{(0)},\\pmb\\mu\\rangle+\\mathrm{SNR}\\sqrt{\\frac{8\\log(6n/\\delta)}{d}}n}\\\\ &{\\phantom{\\langle\\pmb w_{r}^{(t)},\\pmb\\mu\\rangle}\\leq1.01\\big(\\gamma_{r}^{(t)}+\\langle\\mathbf w_{r}^{(0)},\\pmb\\mu\\rangle\\big),}\\\\ &{\\langle\\mathbf w_{r}^{(t)},\\pmb\\mu\\rangle\\geq\\gamma_{r}^{(t)}+\\langle\\mathbf w_{r}^{(0)},\\pmb\\mu\\rangle-\\mathrm{SNR}\\sqrt{\\frac{8\\log(6n/\\delta)}{d}}n}\\\\ &{\\phantom{\\langle\\pmb w_{r}^{(t)},\\pmb\\mu\\rangle}\\geq0.99\\big(\\gamma_{r}^{(t)}+\\langle\\mathbf w_{r}^{(0)},\\pmb\\mu\\rangle\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where we use Lemma B.2 and $\\gamma_{r}^{(t)}\\geq0$ ", "page_idx": 21}, {"type": "text", "text": "Then, the update equation for $\\gamma_{r}^{\\left(t\\right)}$ follows ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\gamma_{r}^{(t+1)}=\\gamma_{r}^{(t)}+\\frac{\\eta}{n m\\tau}\\sum_{i:y_{i}=1}^{n}(1-\\ell_{i}^{\\prime(t)})\\langle\\mathbf{w}_{r}^{(t)},\\pmb{\\mu}\\rangle\\|\\pmb{\\mu}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then we find that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{r}^{(t+1)}=A_{r}^{(t)}+\\frac{\\eta}{n m\\tau}\\displaystyle\\sum_{\\substack{i:y_{i}=1}}(1-\\ell_{i}^{(t)})\\langle\\mathbf{w}_{r}^{(t)},\\mu\\rangle\\|\\mu\\|_{2}^{2}}\\\\ &{\\overset{(a)}{\\leq}A_{r}^{(t)}+\\frac{\\eta}{n m\\tau}\\displaystyle\\sum_{\\substack{i:y_{i}=1}}\\left(1-\\frac{1}{C_{\\ell}(M+1)}\\right)\\|\\mu\\|_{2}^{2}1.01A_{r}^{(t)}}\\\\ &{\\overset{(b)}{\\leq}A_{r}^{(t)}+\\frac{\\eta\\|\\mu\\|_{2}^{2}}{m\\tau}\\left(\\frac{1}{2}+\\sqrt{\\frac{1}{2n}\\log(4/\\delta)}\\right)1.01A_{r}^{(t)}}\\\\ &{\\overset{(c)}{\\leq}\\left(1+\\frac{0.52\\eta\\|\\mu\\|_{2}^{2}}{m\\tau}\\right)A_{r}^{(t)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the first inequality (a) is by (19) and Lemma C.2. The second inequality (b) is by Lemma B.3 and $\\begin{array}{r}{1-\\frac{1}{C_{\\ell}(M+1)}\\,<1}\\end{array}$ The lastinquality ce isby te cndition $n\\geq2500\\log(4/\\delta)$ ", "page_idx": 22}, {"type": "text", "text": "C.1.2 Dynamics of Noise Memorization: Lower Bound ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "To establish the lower bound of noise memorization in the first stage, we require that the added noise level $\\sigma_{\\epsilon}\\leq\\sigma_{\\xi}/C^{\\prime}$ for sufficiently large, with $C^{\\prime}\\ge\\widetilde{\\Omega}(1)$ . We prove the following result that upper bound the scale of $\\langle\\mathbf{w}_{r}^{(t)},\\pmb{\\epsilon}_{i}\\rangle$ ", "page_idx": 22}, {"type": "text", "text": "Lemma C.5. Under the same condition as Lemma $C.I$ and $d=\\widetilde\\Omega(\\operatorname*{max}\\{\\sigma_{0}^{-2}\\Vert\\pmb{\\mu}\\Vert_{2}^{-2},n\\sigma_{0}^{-1}\\sigma_{\\xi}^{-1}\\})$ \uff0c there exists a sufficiently large constant $C_{\\xi}>0$ suchthat ", "page_idx": 22}, {"type": "equation", "text": "$$\n|\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle|\\geq C_{\\xi}|\\langle\\mathbf{w}_{r}^{(t)},\\pmb{\\epsilon}_{i}\\rangle|\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for all $i\\in[n]$ ", "page_idx": 22}, {"type": "text", "text": "Proof of Lemma C.5. According to the decomposition of $\\mathbf{w}_{r}^{(t)}$ , we can show ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\langle\\mathbf{w}_{r}^{(t)},\\epsilon_{i}\\rangle|=|\\langle\\mathbf{w}_{r}^{(0)},\\epsilon_{i}\\rangle+\\gamma_{r}^{(t)}\\|\\mu\\|_{2}^{-2}\\langle\\mu,\\epsilon_{i}\\rangle+\\displaystyle\\sum_{i^{\\prime}=1}^{n}\\rho_{r,i^{\\prime}}^{(t)}\\|\\xi_{i^{\\prime}}\\|_{2}^{-2}\\langle\\pmb{\\xi}_{i^{\\prime}},\\epsilon_{i}\\rangle|}\\\\ &{\\qquad\\qquad\\leq|\\langle\\mathbf{w}_{r}^{(0)},\\epsilon_{i}\\rangle|+\\gamma_{r}^{(t)}\\|\\mu\\|_{2}^{-2}|\\langle\\pmb{\\mu},\\epsilon_{i}\\rangle|+\\displaystyle\\sum_{i^{\\prime}=1}^{n}|\\rho_{r,i}^{(t)}|\\|\\xi_{i^{\\prime}}\\|_{2}^{-2}|\\langle\\pmb{\\xi}_{i^{\\prime}},\\epsilon_{i}\\rangle|}\\\\ &{\\qquad\\qquad\\overset{(a)}{\\leq}2\\sqrt{\\log(8m n)/\\delta}\\sigma_{0}\\sigma_{\\epsilon}\\sqrt{d}+\\|\\mu\\|_{2}^{-1}\\sigma_{\\epsilon}\\sqrt{2\\log(6n/\\delta)}+4\\sigma_{\\epsilon}\\sigma_{\\xi}^{-1}\\sqrt{\\frac{\\log(6n^{2}/\\delta)}{d}}n}\\\\ &{\\qquad\\qquad\\leq1/C|\\langle\\mathbf{w}_{r}^{(0)},\\xi_{i}\\rangle|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the second inequality (a) follows from Lemma B.5 and the last inequality is by the following anti-concentration result. ", "page_idx": 22}, {"type": "text", "text": "Because $\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle$ isaGaiaawd $\\sigma_{0}\\sigma_{\\xi}\\sqrt{d}$ by Lemma B.1, we compute ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}\\big(|\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle|\\leq c\\big)\\leq2\\sqrt{1-\\exp\\left(-\\frac{2c^{2}}{\\pi\\sigma_{0}^{2}\\sigma_{\\xi}^{2}d}\\right)},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "When $\\begin{array}{r}{d\\geq\\frac{2c^{2}}{-\\log(1-\\delta^{2}/(4n^{2}))\\pi\\sigma_{0}^{2}\\sigma_{\\xi}^{2}}}\\end{array}$ we have $\\mathbb{P}\\big(|\\langle{\\mathbf w}_{r}^{(0)},{\\boldsymbol\\xi}_{i}\\rangle|\\le c\\big)\\le\\delta/n$ andby nio boun, we have with probability at least $1\\!-\\!\\delta$ , it holds $|\\langle\\mathbf{w}_{r}^{(0)},\\boldsymbol{\\xi}_{i}\\rangle|>c$ Here we let $c=C\\big(2\\sqrt{\\log(8m n)/\\delta}\\sigma_{0}\\sigma_{\\epsilon}\\sqrt{d}+$ $\\begin{array}{r}{\\|\\mu\\|_{2}^{-1}\\sigma_{\\epsilon}\\sqrt{2\\log(6n/\\delta)}+4\\sigma_{\\epsilon}\\sigma_{\\xi}^{-1}\\sqrt{\\frac{\\log(6n^{2}/\\delta)}{d}}n)\\,\\geq C|\\langle\\mathbf{w}_{r}^{(t)},\\epsilon_{i}\\rangle|}\\end{array}$ og(6n2/)n) \u2265 C(w ,ea). Then we can show the desired result with the condition $d=\\widetilde\\Omega(\\operatorname*{max}\\{\\sigma_{0}^{-2}\\lVert\\pmb{\\mu}\\rVert_{2}^{-2},n\\sigma_{0}^{-1}\\sigma_{\\xi}^{-1}\\})$ \u53e3 ", "page_idx": 22}, {"type": "text", "text": "Define that Z(t) $\\mathcal{Z}_{r,+}^{(t)}\\,=\\,\\{i\\,:\\,\\langle{\\bf w}_{r}^{(t)},\\pmb{\\xi}_{i}\\rangle\\,>\\,0\\}$ and $\\mathcal{Z}_{r,-}^{(t)}\\,=\\,\\{i\\,:\\,\\langle\\mathbf{w}_{r}^{(t)},\\pmb{\\xi}_{i}\\rangle\\,<\\,0\\}$ . To show the result regarding $\\mathcal{T}_{r,+}^{(t)}$ and $\\mathcal{T}_{r,-}^{(t)}$ , we prepare the following anti-concentration result: ", "page_idx": 22}, {"type": "text", "text": "Lemma C.6. Under the condition $\\begin{array}{r}{d\\geq\\sqrt{\\frac{300\\log\\left(6n^{2}/\\delta\\right)}{-\\log\\left(1-\\delta^{2}/4n^{2}\\right)\\pi\\sigma_{0}^{2}\\sigma_{\\xi}^{2}}}}\\end{array}$ , then with probability at least $1-\\delta$ it satisfies ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\vert\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle\\vert>150\\sqrt{\\log(6n^{2}/\\delta)/d}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof of Lemma C.6. Here we want to show $\\begin{array}{r}{|\\langle\\mathbf{w}_{r}^{(0)},\\boldsymbol{\\xi}_{i}\\rangle|\\,\\geq\\,150\\sqrt{\\log(6n^{2}/\\delta)/d}n\\,\\triangleq\\,c}\\end{array}$ with high probability. To see this, because $\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle$ is a Gaussian random variable with mean zero and ", "page_idx": 22}, {"type": "text", "text": "variance $\\sigma_{0}^{2}\\sigma_{\\xi}^{2}d$ , by Lemma B.1, we compute ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{P}\\big(|\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle|\\leq c\\big)\\leq2\\sqrt{1-\\exp\\left(-\\frac{2c^{2}}{\\pi\\sigma_{0}^{2}\\sigma_{\\xi}^{2}d}\\right)},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Thus when $\\begin{array}{r}{d\\geq\\sqrt{\\frac{300\\log\\left(6n^{2}/\\delta\\right)}{-\\log\\left(1-\\delta^{2}/4n^{2}\\right)\\pi\\sigma_{0}^{2}\\sigma_{\\xi}^{2}}}}\\end{array}$ we have $\\mathbb{P}\\big(|\\langle{\\mathbf w}_{r}^{(0)},{\\pmb\\xi}_{i}\\rangle|\\le c\\big)\\le\\delta/n$ and by union bound, we have with probability at least $1-\\delta$ it holds $\\vert\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle\\vert>150\\sqrt{\\log(6n^{2}/\\delta)/d}.$ \u53e3 ", "page_idx": 23}, {"type": "text", "text": "We then show that neurons with negative inner products with the noise at initialization would stay negative and the corresponding $\\rho$ stayszero. ", "page_idx": 23}, {"type": "text", "text": "Lemma C.7. Under the same condition as Lemma C.1 and Lemma C.6, for all $t\\,>\\,0$ we have $\\mathcal{T}_{r,-}^{(t)}=\\mathcal{T}_{r,-}^{(0)}$ and $\\rho_{r,i}^{(t)}=0$ for all $i\\in\\mathcal{Z}_{r,-}^{(0)}$ ", "page_idx": 23}, {"type": "text", "text": "Proof of Lemma C.7. The proof is by induction. We frst consider $i\\in\\mathcal{Z}_{r,-}^{(0)}$ .At $t=1$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\rho_{r,i}^{(1)}=\\rho_{r,i}^{(0)}+\\frac{\\eta}{n m\\tau}(1-\\ell_{i}^{\\prime(0)})\\sigma(\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}+\\epsilon_{i}\\rangle)\\sigma^{\\prime}(\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle)\\|\\pmb{\\xi}_{i}\\|_{2}^{2}}\\\\ {\\displaystyle-\\,\\frac{\\eta}{n m\\tau}\\sum_{j\\neq i}^{M}\\ell_{i,j}^{\\prime(0)}\\sigma(\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{j}\\rangle)\\sigma^{\\prime}(\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle)\\|\\pmb{\\xi}_{i}\\|_{2}^{2}=0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where we have used the condition that $\\langle\\mathbf{w}_{r}^{(0)},\\boldsymbol{\\xi}_{i}\\rangle<0$ and the property of ReLU activation. Next we consider ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\langle\\mathbf{w}_{r}^{(1)},\\pmb{\\xi}_{i}\\rangle=\\langle\\mathbf{w}_{r}^{(0)}+\\gamma_{r}^{(1)}\\pmb{\\mu}\\rVert\\pmb{\\mu}\\rVert_{2}^{-2}+\\displaystyle\\sum_{i=1}^{n}\\rho_{r,i}^{(1)}\\|\\pmb{\\xi}_{i}\\|_{2}^{-2}\\pmb{\\xi}_{i},\\pmb{\\xi}_{i}\\rangle}\\\\ {\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle+\\gamma_{r}^{(1)}\\langle\\pmb{\\mu},\\pmb{\\xi}_{i}\\rangle\\|\\pmb{\\mu}\\|_{2}^{-2}+\\rho_{r,i}^{(1)}+\\displaystyle\\sum_{i^{\\prime}\\neq i}^{n}\\rho_{r,i^{\\prime}}^{(1)}\\|\\pmb{\\xi}_{i^{\\prime}}\\|_{2}^{-2}\\langle\\pmb{\\xi}_{i^{\\prime}},\\pmb{\\xi}_{i}\\rangle}\\\\ {\\overset{(a)}{\\leq}\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle+5\\sqrt{\\displaystyle\\frac{\\log(6n^{2}/\\delta)}{d}}n\\overset{(b)}{<}0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where inequality(a) is by Lemma C.1 and $\\rho_{r,i}^{(1)}=0$ and inequality (b) is by Lemma C.6. ", "page_idx": 23}, {"type": "text", "text": "Suppose at iteration $t$ , the claim is satisfied, i.e., $\\langle\\mathbf{w}_{r}^{(t)},\\boldsymbol{\\xi}_{i}\\rangle<0$ $\\rho_{r,i}^{(t)}=0$ , for all $i\\in\\mathcal{Z}_{r,-}^{(0)}$ . Then ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\rho_{r,i}^{(t+1)}=\\rho_{r,i}^{(t)}+\\frac{\\eta}{n m\\tau}(1-\\ell_{i}^{\\prime(t)})\\sigma(\\langle\\mathbf{w}_{r}^{(t)},\\pmb{\\xi}_{i}+\\epsilon_{i}\\rangle)\\sigma^{\\prime}(\\langle\\mathbf{w}_{r}^{(t)},\\pmb{\\xi}_{i}\\rangle)\\|\\pmb{\\xi}_{i}\\|_{2}^{2}}\\\\ &{\\quad\\quad\\quad-\\,\\frac{\\eta}{n m\\tau}\\sum_{j\\neq i}^{M}\\ell_{i,j}^{\\prime(t)}\\sigma(\\langle\\mathbf{w}_{r}^{(t)},\\pmb{\\xi}_{j}\\rangle)\\sigma^{\\prime}(\\langle\\mathbf{w}_{r}^{(t)},\\pmb{\\xi}_{i}\\rangle)\\|\\pmb{\\xi}_{i}\\|_{2}^{2}<0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Next we consider the update of inner product as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\mathbf w_{r}^{(t+1)},\\pmb\\xi_{i}\\rangle=\\langle\\mathbf w_{r}^{(0)}+\\gamma_{r}^{(t+1)}\\mu\\rVert\\mu\\rVert_{2}^{-2}+\\displaystyle\\sum_{i=1}^{n}\\rho_{r,i}^{(t+1)}\\rVert\\xi_{i}\\rVert_{2}^{-2}\\xi_{i},\\xi_{i}\\rangle}\\\\ &{\\qquad\\qquad=\\langle\\mathbf w_{r}^{(0)},\\pmb\\xi_{i}\\rangle+\\gamma_{r}^{(t+1)}\\langle\\pmb\\mu,\\pmb\\xi_{i}\\rangle\\rVert\\mu\\rVert_{2}^{-2}+\\rho_{r,i}^{(t+1)}+\\displaystyle\\sum_{i^{\\prime}\\neq i}\\rho_{r,i^{\\prime}}^{(t+1)}\\rVert\\xi_{i^{\\prime}}\\rVert_{2}^{-2}\\langle\\pmb\\xi_{i^{\\prime}},\\pmb\\xi_{i}\\rangle}\\\\ &{\\qquad\\qquad\\le\\langle\\mathbf w_{r}^{(0)},\\pmb\\xi_{i}\\rangle+\\gamma_{r}^{(t+1)}|\\langle\\pmb\\mu,\\pmb\\xi_{i}\\rangle\\rVert\\|\\mu\\rVert_{2}^{-2}+\\displaystyle\\sum_{i^{\\prime}\\neq i}^{n}|\\rho_{r,i^{\\prime}}^{(t+1)}\\rVert\\xi_{i^{\\prime}}\\rVert_{2}^{-2}|\\langle\\pmb\\xi_{i^{\\prime}},\\pmb\\xi_{i}\\rangle|}\\\\ &{\\qquad\\qquad\\le\\langle\\mathbf w_{r}^{(0)},\\pmb\\xi_{i}\\rangle+5\\sqrt{\\frac{\\log(6n^{2}/\\delta)}{d}}n<0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the last inequality is by a anti-concentration analysis shown in Lemma C.6. This completes the induction for $i\\in\\mathcal{T}_{r,-}^{(t)}$ \u53e3 ", "page_idx": 23}, {"type": "text", "text": "Before formally stating the main lemma on the lower bound of noise memorization, we prepare several lemmas that will be useful. ", "page_idx": 24}, {"type": "text", "text": "Lemma C.8. Suppose that $\\delta>0$ Then with probability $1-\\delta,$ for all $r\\in[m]$ , we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left|\\sum_{i:y_{i}=1}\\mathbb{1}_{i\\in\\mathbb{Z}_{r,+}^{(0)}}-\\frac{n}{4}\\right|\\leq\\sqrt{\\frac{n}{2}\\log(4/\\delta)}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. The proof is by Hoeffding's inequality, for arbitrary $t>0$ ,wehave that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left|\\sum_{i:y_{i}=1}\\mathbb{1}_{i\\in\\mathbb{Z}_{r,+}}-\\mathbb{E}\\left[\\sum_{i:y_{i}=1}\\mathbb{1}_{i\\in\\mathbb{Z}_{r,+}}\\right]\\right|\\leq t\\right)\\leq2\\exp\\left(-\\frac{2t^{2}}{n}\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By the randomness of initialization and Rademacher distribution, we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{i:y_{i}=1}\\mathbb{1}_{i\\in\\mathbb{Z}_{r,+}}\\right]=\\frac{n}{4}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Setting. $t\\;=\\;\\sqrt{{n}/{2\\log({4}/{\\delta})}}$ and taking a union bound over $r\\,\\in\\,[m]$ , we conclude with high probability at least $1-\\delta$ , it holds: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left|\\sum_{i:y_{i}=1}\\mathbb{1}_{i\\in\\mathbb{Z}_{r,+}}-\\frac{n}{4}\\right|\\leq\\sqrt{\\frac{n}{2}\\log(4/\\delta)}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "To establish the lower bound for noise memorization we define ", "page_idx": 24}, {"type": "equation", "text": "$$\nB_{r,+}^{(t)}\\triangleq\\sum_{i:y_{i}=+1}(\\rho_{r,i}^{(t)}+\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle)\\mathbb{1}_{i\\in\\mathbb{Z}_{r,+}^{(t)}},\\quad B_{r,-}^{(t)}\\triangleq\\sum_{i:y_{i}=-1}(\\rho_{r,i}^{(t)}+\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle)\\mathbb{1}_{i\\in\\mathbb{Z}_{r,+}^{(t)}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Lemma C.9. Suppose $\\delta>0$ the with probability at least $1-\\delta$ we have ", "page_idx": 24}, {"type": "equation", "text": "$$\nB_{r,+}^{(0)}\\leq\\sigma_{0}\\sigma_{\\xi}\\sqrt{d n}(1+\\sqrt{\\log(1/\\delta)/2}),\\quad B_{r,-}^{(0)}\\leq\\sigma_{0}\\sigma_{\\xi}\\sqrt{d n}(1+\\sqrt{\\log(1/\\delta)/2}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. By Bernstein's inequality, for arbitrary $t>0$ ,we have ", "page_idx": 24}, {"type": "equation", "text": "$$\nP(|B_{r,+}^{(0)}-\\sigma_{0}\\sigma_{\\xi}\\sqrt{n d}|>t)\\leq\\exp(-\\frac{t^{2}}{2n/4\\sigma_{0}^{2}\\sigma_{\\xi}^{2}d}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Setting $t=\\sigma_{0}\\sigma_{\\xi}\\sqrt{n d\\log(1/\\delta)/2}$ , we further have ", "page_idx": 24}, {"type": "equation", "text": "$$\nB_{r,+}^{(0)}\\leq\\sigma_{0}\\sigma_{\\xi}\\sqrt{d n}(1+\\sqrt{\\log(1/\\delta)/2}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Similarly, the same result holds for B ", "page_idx": 24}, {"type": "text", "text": "$C>0$ $\\begin{array}{r}{n\\ge\\frac{8C^{2}\\log(1/\\delta)}{\\log(1/(1-(\\delta/m)^{2}))}}\\end{array}$ thnwih $1-\\delta$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n|\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle|>\\frac{C B_{r,+}^{(0)}}{n},\\quad|\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle|>\\frac{C B_{r,-}^{(0)}}{n}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "$\\begin{array}{r}{|\\langle\\mathbf{w}_{r}^{(0)},\\boldsymbol{\\xi}_{i}\\rangle|>\\frac{B_{r,+}^{(0)}}{n}}\\end{array}$ $\\langle\\mathbf{w}_{r}^{(0)},\\boldsymbol{\\xi}_{i}\\rangle$ $\\sigma_{0}^{2}\\sigma_{\\xi}^{2}d$ C.9, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{B_{r,+}^{(0)}}{n}\\leq\\frac{\\sigma_{0}\\sigma_{\\xi}\\sqrt{d n}(1+\\sqrt{\\log(1/\\delta)/2})}{n}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By Lemma B.1, we recall ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{P}\\big(|\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle|\\leq t\\big)\\leq\\sqrt{1-\\exp\\left(-\\frac{8t^{2}}{\\pi\\sigma_{0}^{2}\\sigma_{\\xi}^{2}d}\\right)},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Thus when log(1/(1-(/m), we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{P}\\big(|\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle|\\leq\\frac{C\\sigma_{0}\\sigma_{\\xi}\\sqrt{d n}\\big(1+\\sqrt{\\log(1/\\delta)/2}\\big)}{n}\\big)\\leq\\delta/m\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and by union bound, we have with probability at least $1-\\delta$ it holds $\\begin{array}{r}{|\\langle{\\mathbf w}_{r}^{(0)},{\\boldsymbol\\xi}_{i}\\rangle|\\;>\\;\\frac{B_{r,+}^{(0)}}{n}}\\end{array}$ and $\\begin{array}{r}{|\\langle{\\mathbf w}_{r}^{(0)},{\\pmb\\xi}_{i}\\rangle|>\\frac{{\\cal B}_{r,-}^{(0)}}{n}.}\\end{array}$ \u53e3 ", "page_idx": 25}, {"type": "text", "text": "Besides, we define ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Psi_{r,i}^{(t)}\\triangleq\\rho_{r,i}^{(t)}+\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle,\\ \\mathrm{with}\\ y_{i}=1,i\\in\\mathcal{Z}_{r,+}^{(t)}}\\\\ &{\\Phi_{r,i}^{(t)}\\triangleq\\rho_{r,i}^{(t)}+\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle,\\ \\mathrm{with}\\ y_{i}=-1,i\\in\\mathcal{Z}_{r,+}^{(t)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "With all the results (lemmas) and definitions outlined above at hand, we are ready to state the lemmas that provide the lower bound for noise memorization as follows. ", "page_idx": 25}, {"type": "text", "text": "Lemma C.11. Under the same condition as Theorem 4.2, then with probability at least $1-\\delta$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\Psi_{r,i}^{(t)}\\geq(1+\\frac{0.96\\eta\\sigma_{\\xi}^{2}d}{n m\\tau})^{t}\\Psi_{r,i}^{(0)},}\\\\ {\\displaystyle\\Psi_{r,i}^{(t)}\\geq\\frac{101C_{\\ell}}{M+1}B_{r,-}^{(t)},}\\\\ {\\displaystyle\\rho_{r,i;y_{i}=1}^{(t)}\\mathbb{1}_{i\\in\\mathcal{I}_{r,+}^{(t)}}\\geq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. The proof is by induction. We can check ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\rho_{r,i:y_{i}=1}^{(0)}\\mathbb{1}_{i\\in\\mathbb{Z}_{r,+}}=0,\\quad\\Psi_{r,i}^{(0)}\\geq(1+\\frac{0.96\\eta\\sigma_{\\xi}^{2}d}{n m\\tau})^{0}\\Psi_{r,i}^{(0)}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Besides, by Lemma C.10 and $\\begin{array}{r}{M\\in\\frac{n}{2}(1\\pm o(n^{-1/2}))}\\end{array}$ , it holds that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\Psi_{r,i}^{(0)}=\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle\\geq101\\frac{C_{\\ell}}{M+1}B_{r,-}^{(0)}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "$t$ $B_{r,+}^{(t)}$$B_{r,-}^{(t)}$ reptivelrst wea.t $y_{i}=1$ and $i\\in\\mathcal{Z}_{r,+}$ \uff0c", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\mathbf{w}_{r}^{(t)},\\pmb{\\xi}_{i}+\\epsilon_{i}\\rangle\\geq\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle+\\rho_{r,i}^{(t)}-|\\langle\\mathbf{w}_{r}^{(t)},\\epsilon_{i}\\rangle|-6\\sqrt{\\frac{\\log\\left(6n^{2}/\\delta\\right)}{d}}n}\\\\ &{\\qquad\\qquad\\geq(1-o(1))\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle+\\rho_{r,i}^{(t)}\\geq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We can show that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{3(t+1)}=B_{r,-}^{(t)}+\\frac{\\eta}{n m\\tau}\\sum_{i\\colon j_{t}=-1}\\left[(1-\\ell_{i}^{\\prime(t)})\\langle{\\bf w}_{r}^{(t)},\\xi_{i}+\\epsilon_{i}\\rangle-\\sum_{j\\neq i}\\ell_{i,j}^{\\prime(t)}\\langle{\\bf w}_{r}^{(t)},\\xi_{j}\\rangle\\mathbb{1}_{i\\in\\mathbb{Z}_{r,+}}\\right]\\mathbb{1}_{i\\in\\mathbb{Z}_{r,+}}\\|\\xi_{i}\\|_{2}}&{}\\\\ &{}&{\\leq B_{r,-}^{(t)}+\\frac{\\eta}{n m\\tau}\\sum_{i\\colon j_{t}=-1}\\left[\\frac{C_{\\ell}(M+1)-1}{C_{\\ell}(M+1)}(\\langle{\\bf w}_{r}^{(0)},\\xi_{i}\\rangle+\\rho_{r,i}^{(t)}+|\\langle{\\bf w}_{r}^{(t)},\\epsilon_{i}\\rangle|+6\\sqrt{\\frac{\\log(6n^{2}/\\delta)}{d}}n\\right]}&{}\\\\ &{}&{\\quad-\\sum_{j\\neq i}\\mathbb{1}_{j\\in\\mathbb{Z}_{r,+}}\\frac{1}{C_{\\ell}(M+1)}(\\langle{\\bf w}_{r}^{(0)},\\xi_{j}\\rangle+\\rho_{r,j}^{(t)}-6\\sqrt{\\frac{\\log(6n^{2}/\\delta)}{d}}n)\\right](\\sigma_{\\xi}^{2}d+\\sigma_{\\xi}^{2}\\sqrt{d\\log(6n^{2}/\\delta)}n)}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq B_{r,-}^{(t)}+\\frac{\\eta}{n\\tau m}\\bigg[\\frac{C_{\\ell}(M+1)-1}{C_{\\ell}(M+1)}1.01B_{r,-}^{(t)}-\\sum_{i\\ y_{i}=-1}^{\\infty}\\frac{1}{i}1_{i\\in{\\mathcal{X}_{r,+}}}\\frac{1}{C_{\\ell}(M+1)}0.99B_{r,-}^{(t)}\\bigg](\\sigma_{\\xi}^{2}d+\\sigma_{\\xi}^{2}\\sqrt{d1}}\\\\ &{\\quad-\\sum_{i\\ y_{i}=-1}^{\\infty}\\mathbf{1}_{i\\in{\\mathcal{X}_{r,+}}}\\frac{1}{C_{\\ell}(M+1)}(B_{r,-}^{(t)}-\\sum_{j\\ y_{i}}\\phi\\sqrt{\\frac{\\log(6n^{2}/\\delta)}{d}}n)\\bigg](\\sigma_{\\xi}^{2}d+\\sigma_{\\xi}^{2}\\sqrt{d\\log(6n^{2}/\\delta)})}\\\\ &{\\leq B_{r,-}^{(t)}+\\frac{\\eta}{n m\\tau}\\bigg[1.01B_{r,-}^{(t)}-\\frac{0.99^{2}}{2C_{l}}B_{r,+}^{(t)}\\bigg]1.035\\sigma_{\\xi}^{2}d}\\\\ &{\\leq B_{r,-}^{(t)}+\\frac{\\eta}{n m\\tau}\\bigg[1.05B_{r,-}^{(t)}-0.5B_{r,+}^{(t)}\\bigg]\\sigma_{\\xi}^{2}d}\\\\ &{\\leq B_{r,-}^{(t)}+\\frac{1.05\\eta\\sigma_{\\xi}^{2}d}{n m\\tau}B_{r,-}^{(t)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the first inequality is by Lemma C.2, Lemma C.1 and Lemma B.5. Furthermore, the second inequality is by Lemma C.5,i.e., $|\\langle\\mathbf{w}_{r}^{(t)},\\epsilon_{i}\\rangle|\\leq1/C_{\\xi}|\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle|$ (for $C_{\\xi}>200)$ \uff0c $|0.01\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle|\\geq$ $6\\sqrt{\\log(6n^{2}/\\delta)d^{-1}}n$ $B_{r,+}^{(t)}$ $B_{r,-}^{(t)}$ $B_{r,+}^{(t)}\\geq B_{r,+}^{(0)}$ $B_{r,-}^{(t)}\\geq B_{r,-}^{(0)}$ The third inequality is by $M\\geq100C\\ell-1$ \uff0c $M\\,\\in\\,\\textstyle{\\frac{n}{2}}(1\\pm o(n^{-1/2}))$ \uff0c $d>10000\\log(6n^{2}/\\delta)$ and LemmaC8 The lastinequalty isy $B_{r,+}^{(t)}\\geq0$ ", "page_idx": 26}, {"type": "text", "text": "As a result, we conclude that ", "page_idx": 26}, {"type": "equation", "text": "$$\nB_{r,-}^{(t)}\\leq(1+\\frac{1.05\\eta\\sigma_{\\xi}^{2}d}{n m\\tau})^{t}B_{r,-}^{(0)}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Finally, the induction step for $\\Psi_{r,i}^{(t)}$ can be calculated as follows: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nu_{r,i}^{(t+1)}=\\Psi_{r,i}^{(t)}+\\frac{\\eta}{n m\\tau}\\left[(1-\\ell_{i}^{(t)})\\langle\\mathbf{w}_{r}^{(t)},\\xi_{i}+\\epsilon_{i}\\rangle-\\sum_{j\\in\\mathcal{E}_{r,i}}\\ell_{i}^{(t)}\\langle\\mathbf{w}_{r}^{(t)},\\xi_{j}\\rangle\\right]\\mathbf{u}_{\\ell}\\langle\\mathbf{z}_{r,i}^{(t)}|_{2}^{2}}\\\\ &{\\geq\\Psi_{r,i}^{(t)}+\\frac{\\eta}{n m\\tau}\\left[\\frac{\\left(M+1\\right)-C_{\\ell}}{\\alpha\\left(M+1\\right)}\\langle(\\mathbf{w}_{r}^{(0)},\\xi_{i})+\\rho_{r,i}^{(t)}-|\\mathbf{(w}_{r}^{(t)},\\epsilon_{i})|-6\\sqrt{\\frac{\\log(6\\theta n^{2}/\\delta)}{d}}n\\rangle\\right.}\\\\ &{\\qquad-\\left.\\sum_{j\\in\\mathcal{E}_{r,i}}\\frac{C_{i}}{M+1}\\langle(\\mathbf{w}_{r}^{(0)},\\xi_{j})+\\rho_{r,j}^{(t)}+6\\sqrt{\\frac{\\log(6\\theta n^{2}/\\delta)}{d}}n\\rangle\\right]\\left(\\sigma_{z}^{2}d-\\sigma_{\\xi}^{2}\\sqrt{d\\log(6n^{2}/\\delta)}\\right)\\mathbf{l}}\\\\ &{\\geq\\Psi_{r,i}^{(t)}+\\frac{\\eta}{n m\\tau}\\left[\\frac{\\left(M+1\\right)-C_{\\ell}}{M+1}\\langle\\mathbf{w}_{r}^{(0)},\\mathbf{g}\\mathbf{w}_{r,i}^{(t)}-\\frac{C_{\\ell}}{M+1}\\mathbf{l}_{0}|\\mathbf{D}_{r,-}^{(t)}\\rangle\\right]\\sigma_{\\ell}^{2}d-\\sigma_{\\ell}^{2}\\sqrt{d\\log(6n^{2}/\\delta)}}\\\\ &{\\geq\\Psi_{r,i}^{(t)}+\\frac{\\eta}{n m\\tau}\\left[0.97\\Psi_{r,i}^{(t)}-1.01\\frac{C_{i}}{M+1}\\mathbf{l}_{r,-}^{(t)}\\right]0.9\\circ\\sigma_{\\ell}^{2}d}\\\\ &{\\geq\\Psi_{r,i}^{(t)}+\\frac{\\eta}{n m\\tau}\\alpha_{r}^{2}\\left[0.96\\Psi_\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the first inequality is by Lemma C.2, Lemma C.1 and Lemma B.5. Furthermore, the second inequalityis by Lemma C.5, i.e, $|\\langle\\mathbf{w}_{r}^{(t)},\\epsilon_{i}\\rangle|\\leq1/C_{\\xi}|\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle|$ (for $C_{\\xi}>200)$ $|0.01\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle|\\geq$ $6\\sqrt{\\log(6n^{2}/\\delta)d^{-1}}n$ the definition of $B_{r,+}^{(t)}$ and $B_{r,-}^{(t)}$ Th hird inequalit is by $M\\geq100C\\ell-1$ The last inequality is by induction (23). ", "page_idx": 26}, {"type": "text", "text": "Then we check induction induction (23) through following inequalities: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{B_{r,-}^{(t)}\\leq(1+\\frac{1.05\\eta\\sigma_{\\xi}^{2}d}{n m\\tau})^{t}B_{r,-}^{(0)},}\\\\ {\\Psi_{r,i}^{(t)}\\geq(1+\\frac{0.96\\eta\\sigma_{\\xi}^{2}d}{n m\\tau})^{t}\\Psi_{r,i}^{(0)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Together it confirms that $\\begin{array}{r}{\\Psi_{r,i}^{(t)}\\geq\\frac{101C_{\\ell}}{M+1}B_{r,-}^{(t)}}\\end{array}$ ", "page_idx": 26}, {"type": "text", "text": "Finally, we check the sign of $\\rho_{r,i}^{(t)}$ for $i:y_{i}=1$ and $i\\in\\mathcal{T}_{r,+}^{(t)}$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\rho_{r,i}^{(t)}=\\Psi_{r,i}^{(t)}-\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}\\rangle\\geq\\big(1+\\frac{\\eta\\sigma_{\\xi}^{2}d}{2n m\\tau}\\big)^{t}\\Psi_{r,i}^{(t)}-\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}\\rangle>\\frac{\\eta\\sigma_{\\xi}^{2}d}{2n m\\tau}\\Psi_{r,i}^{(t)}>0.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "This completes the induction proof. ", "page_idx": 27}, {"type": "text", "text": "C.1.3 Noise Memorization: Proof of Lemma 5.2 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Before proving Lemma 5.2, we require a lower bound for the initialization. Define that ${\\mathcal U}_{+}^{(0)}=\\{r:$ $\\langle\\mathbf{w}_{r}^{(0)},\\boldsymbol{\\xi}_{i}\\rangle>0\\}$ for $y_{i}=1$ and $\\mathcal{U}_{-}^{(0)}=\\{r:\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle<0\\}$ for $y_{i}=-1$ ", "page_idx": 27}, {"type": "text", "text": "Lemma C.12. Suppose that $\\delta>0$ and $m\\geq\\widetilde{\\Omega}(1)$ . Then with probability at least $1-\\delta$ we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{1}{m}\\sum_{r\\in\\mathcal{U}_{+}^{(0)}}\\Psi_{r,i}^{(0)}\\geq0.2\\sigma_{0}\\sigma_{\\xi}\\sqrt{d},\\quad\\frac{1}{m}\\sum_{r\\in\\mathcal{U}_{-}^{(0)}}\\Phi_{r,i}^{(0)}\\geq0.2\\sigma_{0}\\sigma_{\\xi}\\sqrt{d}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof of Lemma C.12. Consider $y_{i}\\,=\\,1$ . Note that $\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle\\sim\\mathcal{N}(0,\\sigma_{0}^{2}\\|\\pmb{\\xi}_{i}\\|_{2}^{2})$ .We define that the event $A=\\{r\\in[m],\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle>0\\}$ Then we can compute that $\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle\\mathbb{1}(\\mathcal{A})$ becomes a half-normal distribution with the expectation ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle\\mathbb{1}(\\mathcal{A})]=\\frac{\\sqrt{2}\\sigma_{0}\\|\\pmb{\\xi}_{i}\\|_{2}}{\\sqrt{\\pi}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We then apply the sub-Gaussian concentration inequality that with probability at least $1-\\delta$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left|\\sum_{r\\in\\mathcal{U}_{+}^{(0)}}\\Psi_{r,i}^{(0)}-\\frac{m\\sigma_{0}\\|\\pmb{\\xi}_{i}\\|_{2}}{\\sqrt{2}\\pi}\\right|\\leq\\widetilde{O}(m^{-1/2}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{1}{m}\\sum_{r\\in\\mathcal{U}_{+}^{(0)}}\\Psi_{r,i}^{(0)}\\geq0.4\\sigma_{0}\\|\\pmb{\\xi}_{i}\\|_{2}\\geq\\sigma_{0}\\sigma_{\\xi}\\sqrt{d},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where we have used Lemma B.5. Similarly, we can show that for $y_{i}=-1$ , the same result holds. ", "page_idx": 27}, {"type": "text", "text": "Proof of Lemma 5.2. From the upper bound on (21), we take the maximum over $r\\in\\mathcal{U}_{+}^{(0)}$ ,which gives ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{r}{\\operatorname*{max}}\\,A_{r}^{(t)}\\leq\\Bigl(1+0.52\\frac{\\eta\\|\\pmb\\mu\\|_{2}^{2}}{m\\tau}\\Bigr)^{t}\\underset{r}{\\operatorname*{max}}\\,A_{r}^{(0)}}\\\\ &{\\qquad\\qquad\\leq\\Bigl(1+0.52\\frac{\\eta\\|\\pmb\\mu\\|_{2}^{2}}{m\\tau}\\Bigr)^{t}\\sigma_{0}\\|\\pmb\\mu\\|_{2}\\sqrt{2\\log(8m/\\delta)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Under the SNR condition $n\\cdot\\mathrm{SNR}^{2}\\,\\leq\\,1.8\\$ . we can see there exists a scale difference between $\\operatorname*{max}_{r,i}\\Psi_{r,i}^{(t)}$ and $\\operatorname*{max}_{r,i}A_{r}^{(t)}$ at the end of frst stage. ", "page_idx": 27}, {"type": "text", "text": "At the same time, for noise memorization, from the lower bound established in Lemma C.11, we havethat ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{1}{m}\\sum_{r\\in\\mathcal{U}_{+}^{(0)}}\\Psi_{r,i}^{(t)}\\geq(1+\\frac{0.96\\eta\\sigma_{\\xi}^{2}d}{n m\\tau})^{t}\\frac{1}{m}\\sum_{r\\in\\mathcal{U}_{+}^{(0)}}\\Psi_{r,i}^{(0)}}}\\\\ &{}&{\\geq(1+\\frac{0.96\\eta\\sigma_{\\xi}^{2}d}{n m\\tau})^{t}0.2\\sigma_{0}\\sigma_{\\xi}\\sqrt{d},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the second inequality is due to Lemma C.12 ", "page_idx": 27}, {"type": "text", "text": "Let ", "page_idx": 28}, {"type": "equation", "text": "$$\nT_{1}=\\log\\left(20/(\\sigma_{0}\\sigma_{\\xi}\\sqrt{d})\\right)/\\log\\left(1+0.96\\frac{\\eta\\sigma_{\\xi}^{2}d}{n m\\tau}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Then we have $\\textstyle{\\frac{1}{m}}\\sum_{r\\in{\\mathcal{U}}_{+}^{(0)}}\\Psi_{r,i}^{(t)}$ reach 3 within $T_{1}$ iteratosby illy wa alhat $\\begin{array}{r}{\\frac{1}{m}\\sum_{r\\in\\mathcal{U}_{-}^{(0)}}\\Phi_{r,i}^{(t)}}\\end{array}$ reach3 within $T_{1}$ iteratons. ", "page_idx": 28}, {"type": "text", "text": "On the other hand, we compute the scale of $\\operatorname*{max}_{r}A_{r}^{(T_{1})}$ as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\operatorname*{max}_{r}A_{r}^{(T_{1})}\\leq\\Big(1+0.52\\frac{\\eta\\|\\mu\\|_{2}^{2}}{m\\tau}\\Big)^{T_{1}}\\upsilon_{0}\\|\\mu\\|_{2}\\sqrt{2\\log(8m/\\delta)}}&{}\\\\ &{\\qquad\\qquad=\\exp\\Big(\\frac{\\log(1+0.52\\frac{\\eta\\|\\mu\\|_{2}}{m\\tau})}{\\log(1+0.96\\frac{\\eta^{2}\\delta}{m\\tau})}\\log(12/(\\sigma_{0}\\sigma_{\\xi}\\sqrt{d})\\Big)\\sigma_{0}\\|\\mu\\|_{2}\\sqrt{2\\log(8m/\\delta)}}\\\\ &{\\qquad\\leq\\exp\\big((0.55\\cdot n\\cdot\\mathrm{SNR}^{2}+O((\\frac{\\eta\\|\\mu\\|_{2}^{2}}{m\\tau})^{2})\\log(20/(\\sigma_{0}\\sigma_{\\xi}\\sqrt{d}))\\sigma_{0}\\|\\mu\\|_{2}\\sqrt{2\\log(8m/\\delta)}}\\\\ &{\\qquad\\leq\\exp\\big((0.55\\cdot n\\cdot\\mathrm{SNR}^{2}+0.01\\big)\\log(30/(\\sigma_{0}\\sigma_{\\xi}\\sqrt{d}))\\sigma_{0}\\|\\mu\\|_{2}\\sqrt{2\\log(8m/\\delta)}}\\\\ &{\\qquad\\leq\\exp\\big(\\log(30/(\\sigma_{0}\\sigma_{\\xi}\\sqrt{d}))\\sigma_{0}\\|\\mu\\|_{2}\\sqrt{2\\log(8m/\\delta)}}\\\\ &{\\qquad=20\\sqrt{2\\log(8m/\\delta)}\\mathrm{SNR}}\\\\ &{\\qquad=O(\\sqrt{\\log(m/\\delta)/n})}\\\\ &{\\qquad=\\widetilde{\\mathcal{O}}(n^{-1/2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "wherewechoose $\\eta$ sufficiently small for the third inequality. The last inequality is by the SNR condition. Because we can choose $n\\geq C\\log(m/\\delta)$ for suffciently large constant $C$ $\\dot{\\mathrm{y}},\\operatorname*{max}_{r}A_{r}^{(T_{1})}=$ $o(1)$ ", "page_idx": 28}, {"type": "text", "text": "C.2  Second Stage ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Proposition C.13. Let $T^{*}$ be the maximum admissible iteration and let $\\alpha=\\log(3M T^{*})$ .Thenwe canshow ", "page_idx": 28}, {"type": "equation", "text": "$$\n|\\gamma_{r}^{(t)}|\\leq\\alpha,\\quad|\\rho_{r,i}^{(t)}|\\leq\\alpha.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof of Proposition C.13. We need to show $\\rho_{r,i}^{(t)}\\leq\\alpha$ We prove the claim by induction. It is clear when t = 0, p, $\\rho_{r,i}^{(t)}=0\\leq\\alpha$ Suppose fo al $0\\leq t\\leq\\widetilde{T}-1$ , we have $\\rho_{r,i}^{(t)}\\leq\\alpha$ We aim to show the claim holds for $\\widetilde{T}$ ", "page_idx": 28}, {"type": "text", "text": "By the update of p. ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\rho_{r,i}^{(t+1)}\\leq\\rho_{r,i}^{(t)}+\\frac{\\eta}{n m\\tau}(1-\\ell_{i}^{\\prime(t)})\\sigma(\\langle\\mathbf{w}_{r}^{(t)},\\pmb{\\xi}_{i}+\\epsilon_{i}\\rangle)\\|\\pmb{\\xi}_{i}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "$r,i$ $i\\in\\mathcal{Z}_{r,+}^{(0)}$ $\\langle\\mathbf{w}_{r}^{(0)},\\boldsymbol{\\xi}_{i}\\rangle>0$ $y_{i}=1$ . Let $t_{r,i}$ be the last time $t$ such that $\\rho_{r,i}^{(t)}\\leq0.5\\alpha$ Then we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\rho_{r,i}^{(\\widetilde{T})}\\leq\\rho_{r,i}^{(t_{r})}+\\frac{\\eta}{n m\\tau}(1-\\ell_{i}^{\\prime(t_{r,i})})\\sigma(\\langle\\mathbf{w}_{r}^{(t_{r,i})},\\pmb{\\xi}_{i}+\\epsilon_{i}\\rangle)\\|\\pmb{\\xi}_{i}\\|_{2}^{2}}\\\\ {+\\sum_{t_{r,i}\\leq t\\leq\\widetilde{T}}\\frac{\\eta}{n m\\tau}(1-\\ell_{i}^{\\prime(t)})\\sigma(\\langle\\mathbf{w}_{r}^{(t)},\\pmb{\\xi}_{i}+\\epsilon_{i}\\rangle)\\|\\pmb{\\xi}_{i}\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The second term can be bounded as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{\\eta}{n m\\tau}(1-\\ell_{i}^{\\prime(t_{r,i})})\\sigma(\\langle\\mathbf{w}_{r}^{(t_{r,i})},\\pmb{\\xi}_{i}+\\epsilon_{i}\\rangle)\\|\\pmb{\\xi}_{i}\\|_{2}^{2}\\leq\\frac{\\eta}{n m\\tau}\\big(\\frac{3}{2}\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle+\\rho_{r,i}^{(t_{r,i})}\\big)\\|\\pmb{\\xi}_{i}\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\le\\frac{3\\eta\\sigma_{\\xi}^{2}d}{2n m\\tau}\\big(0.3\\alpha+0.5\\alpha\\big)}\\\\ {\\displaystyle\\le0.25\\alpha,}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Where the firs inequality is by $1-\\ell_{i}^{\\prime(t_{r})}\\leq1$ and modied Lemma C.1 with $\\rho_{r,i}^{(t)}\\,=\\,{\\cal O}(\\alpha)$ The second inequality is by (w ,) \u22640.2a and n\u2264 ", "page_idx": 29}, {"type": "text", "text": "For notation convenience, we let ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\mathbf{\\Xi}}_{0}^{\\top}(\\mathbf{W},\\mathbf{x}_{i})=\\operatorname{Sim}_{\\mathbf{h}}(\\mathbf{x}_{i},\\widehat{\\mathbf{x}}_{i})/\\tau}\\\\ &{\\quad\\quad\\quad\\quad=\\displaystyle\\frac{1}{m\\tau}\\sum_{r=1}^{m}\\sigma(\\langle\\mathbf{w}_{r},y_{i}\\mu\\rangle)\\mathrm{sg}\\big(\\sigma(\\langle\\mathbf{w}_{r},y_{i}\\mu\\rangle)\\big)+\\displaystyle\\frac{1}{m\\tau}\\sum_{r=1}^{m}\\sigma(\\langle\\mathbf{w}_{r},\\xi_{i}\\rangle)\\mathrm{sg}\\big(\\sigma(\\langle\\mathbf{w}_{r},\\xi_{i}+\\epsilon_{i}\\rangle)\\big)}\\\\ &{\\displaystyle\\forall_{j}(\\mathbf{W},\\mathbf{x}_{i})=\\operatorname{Sim}_{\\mathbf{h}}(\\mathbf{x}_{i},\\widehat{\\mathbf{x}}_{j})/\\tau}\\\\ &{\\quad\\quad\\quad\\quad=\\displaystyle\\frac{1}{m\\tau}\\sum_{r=1}^{m}\\sigma(\\langle\\mathbf{w}_{r},y_{i}\\mu\\rangle)\\mathrm{sg}\\big(\\sigma(\\langle\\mathbf{w}_{r},y_{j}\\mu\\rangle)\\big)+\\frac{1}{m\\tau}\\sum_{r=1}^{m}\\sigma(\\langle\\mathbf{w}_{r},\\xi_{i}\\rangle)\\mathrm{sg}\\big(\\sigma(\\langle\\mathbf{w}_{r},\\xi_{j}\\rangle)\\big),\\ \\mathrm{for}\\ j\\in\\mathcal{V}}\\end{array}\n$$A ", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Next, we show the bound for $F_{0}(\\mathbf{W}^{(t)},\\mathbf{x}_{i})$ and $F_{j}({\\bf W}^{(t)},{\\bf x}_{i})$ for $j=1,...,M$ as ", "page_idx": 29}, {"type": "equation", "text": "$$\nF_{0}(\\mathbf{W}^{(t)},\\mathbf{x}_{i})\\geq\\frac{1}{m\\tau}\\sum_{r=1}^{m}\\sigma(\\langle\\mathbf{w}_{r}^{(t)},\\boldsymbol{\\xi}_{i}\\rangle)\\sigma(\\langle\\mathbf{w}_{r}^{(t)},\\boldsymbol{\\xi}_{i}+\\boldsymbol{\\epsilon}_{i}\\rangle).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Further, we have for $j=1,...,M$ ", "page_idx": 29}, {"type": "equation", "text": "$$\nF_{j}(\\mathbf{W}^{(t)},\\mathbf{x}_{i})=\\frac{1}{m\\tau}\\sum_{r=1}^{m}\\sigma(\\langle\\mathbf{w}_{r}^{(t)},\\pmb{\\xi}_{i}\\rangle)\\sigma(\\langle\\mathbf{w}_{r}^{(t)},\\pmb{\\xi}_{j}\\rangle).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Then we can bound the difference between $F_{0}(\\mathbf{W}^{(t)},\\mathbf{x}_{i})$ and $F_{j}({\\bf W}^{(t)},{\\bf x}_{i})$ as ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tau_{0}(\\mathbf{W}^{(t)},\\mathbf{x}_{i})-F_{j}(\\mathbf{W}^{(t)},\\mathbf{x}_{i})\\geq\\displaystyle\\frac{1}{m\\tau}\\displaystyle\\sum_{r=1}^{m}\\sigma(\\langle\\mathbf{w}_{r}^{(t)},\\xi_{i}\\rangle)\\sigma(\\langle\\mathbf{w}_{r}^{(t)},\\xi_{i}+\\epsilon_{i}\\rangle)-\\displaystyle\\frac{1}{m\\tau}\\displaystyle\\sum_{r=1}^{m}\\sigma(\\langle\\mathbf{w}_{r}^{(t)},\\xi_{i}\\rangle)\\sigma(\\langle\\mathbf{w}_{r}^{(t)},\\xi_{i}\\rangle)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\displaystyle\\frac{1}{m\\tau}\\displaystyle\\sum_{r=1}^{m}\\sigma(\\langle\\mathbf{w}_{r}^{(t)},\\xi_{i}\\rangle)\\big(\\sigma(\\langle\\mathbf{w}_{r}^{(t)},\\xi_{i}+\\epsilon_{i}\\rangle)-\\sigma(\\langle\\mathbf{w}_{r}^{(t)},\\xi_{j}\\rangle)\\big)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ }\\\\ &{\\geq0.5\\alpha(\\frac{1}{2}\\langle\\mathbf{w}_{r}^{(0)},\\xi_{i}\\rangle+\\rho_{r,i}^{(t)}-\\frac{3}{2}\\langle\\mathbf{w}_{r}^{(0)},\\xi_{j}\\rangle-\\rho_{r,j}^{(t)})/\\tau}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ }\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the secod inequality is y $\\begin{array}{r}{\\sigma(\\langle\\mathbf{w}_{r}^{(t)},\\pmb{\\xi}_{i}\\rangle)\\geq\\frac{3}{4}\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle+\\rho_{r,i}^{(t)}\\geq\\rho_{r,i}^{(t)}\\geq0.5\\alpha}\\end{array}$ $i\\in\\mathcal{Z}_{r,+}^{(0)}$ Further $\\begin{array}{r}{\\sigma(\\langle\\mathbf{w}_{r}^{(t)},\\boldsymbol{\\xi}_{i}+\\epsilon_{i}\\rangle)\\ge\\frac{1}{2}\\langle\\mathbf{w}_{r}^{(0)},\\boldsymbol{\\xi}_{i}\\rangle+\\rho_{r,i}^{(t)}}\\end{array}$ $\\begin{array}{r}{\\sigma(\\langle\\mathbf{w}_{r}^{(t)},\\boldsymbol{\\xi}_{j}\\rangle)\\,\\leq\\,\\frac{3}{2}\\langle\\mathbf{w}_{r}^{(0)},\\boldsymbol{\\xi}_{j}\\rangle+\\rho_{r,j}^{(t)}}\\end{array}$ The fourth inequality is by induction and $\\begin{array}{r}{\\frac{1}{2}\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle-\\frac{3}{2}\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{j}\\rangle\\geq-0.15\\alpha.}\\end{array}$ The last inequality is by the condition that $\\alpha\\geq20\\tau$ ", "page_idx": 29}, {"type": "text", "text": "Further, we bound the loss derivative as ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{1-\\ell_{i}^{\\prime(t)}=1-\\displaystyle\\frac{1}{1+\\sum_{j=1}^{M}\\exp(F_{j}(\\mathbf{W}^{(t)},\\mathbf{x}_{i})-F_{0}(\\mathbf{W}^{(t)},\\mathbf{x}_{i}))}}\\\\ &{\\qquad\\qquad\\leq1-\\displaystyle\\frac{1}{1+M\\exp(-\\alpha)}}\\\\ &{\\qquad\\qquad\\leq1-\\displaystyle\\frac{T^{*}}{1+T^{*}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n={\\frac{1}{T^{*}+1}},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the second inequality is by (27). Finally, we bound ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t_{r,i}\\leq t\\leq\\tilde{T}}\\frac{\\eta}{n m\\tau}(1-\\ell_{i}^{\\prime(t)})\\sigma(\\langle\\mathbf w_{r}^{(t)},\\xi_{i}+\\epsilon_{i}\\rangle)\\|\\xi_{i}\\|_{2}^{2}\\leq\\frac{3\\eta\\sigma_{\\xi}^{2}d}{2n m\\tau}\\cdot2\\alpha\\cdot\\displaystyle\\sum_{t_{r,i}\\leq t\\leq\\tilde{T}}(1-\\ell_{i}^{\\prime(t)})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\displaystyle\\frac{3\\eta\\sigma_{\\xi}^{2}d}{n m\\tau}\\cdot\\alpha\\cdot\\frac{T^{*}}{T^{*}+1}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq0.25\\alpha}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the first inequality is by Lemma B.5 and $\\begin{array}{r}{\\sigma(\\langle\\mathbf{w}_{r}^{(t)},\\pmb{\\xi}_{i}+\\epsilon_{i}\\rangle)\\leq\\frac{3}{2}\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle+\\rho_{r,i}^{(t)}\\leq2\\alpha}\\end{array}$ b induction. The last inequalty isby the condition ", "page_idx": 30}, {"type": "text", "text": "Combining (26) and (29) with (25) gives ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\rho_{r,i}^{(\\widetilde{T})}\\leq0.5\\alpha+0.25\\alpha+0.25\\alpha=\\alpha,\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "which completes the induction. ", "page_idx": 30}, {"type": "text", "text": "Lemma C.14. Under Assumption 4.1, for $0\\leq t\\leq T^{*}$ ,we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\|\\nabla L_{S}(\\mathbf{W}^{(t)})\\|_{F}^{2}\\leq O(\\operatorname*{max}\\{\\|\\pmb{\\mu}\\|_{2}^{2},\\sigma_{\\xi}^{2}d\\})L_{S}(\\mathbf{W}^{(t)}).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof of Lemma C.14. First, we can write the gradient of $\\nabla_{\\mathbf{w}_{r}}L_{i}(\\mathbf{W})$ as ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\nabla_{\\mathbf{w}_{r}}L_{i}(\\mathbf{W})=\\sum_{j=0}^{M}{\\frac{\\partial L_{i}(\\mathbf{W})}{\\partial F_{j}(\\mathbf{W},\\mathbf{x}_{i})}}\\nabla_{\\mathbf{w}_{r}}F_{j}(\\mathbf{W},\\mathbf{x}_{i}),\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial{\\cal L}_{i}({\\bf W})}{\\partial{\\cal F}_{0}}=-1+\\frac{e^{{\\cal F}_{0}({\\bf W},{\\bf x}_{i})}}{e^{{\\cal F}_{0}({\\bf W},{\\bf x}_{i})}+\\sum_{j=1}^{M}e^{{\\cal F}_{j}({\\bf W},{\\bf x}_{i})}}}\\\\ &{\\frac{\\partial{\\cal L}_{i}({\\bf W})}{\\partial{\\cal F}_{j}}=\\frac{e^{{\\cal F}_{j}({\\bf W},{\\bf x}_{i})}}{e^{{\\cal F}_{0}({\\bf W},{\\bf x}_{i})}+\\sum_{j=1}^{M}e^{{\\cal F}_{j}({\\bf W},{\\bf x}_{i})}},\\;\\mathrm{for}\\;j=1,...,M}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "By the derivation of the gradient, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\nabla L_{S}(\\mathbf{W}^{(t)})\\|_{F}^{2}\\leq\\big(\\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\|\\nabla L_{i}(\\mathbf{W})\\|_{F}\\big)^{2}}&{}\\\\ &{\\qquad=\\Big(\\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\sqrt{\\sum_{r=1}^{m}\\|\\nabla_{\\mathbf{w}_{r}}L_{i}(\\mathbf{W}^{(t)})\\|_{2}^{2}}\\Big)^{2}}\\\\ &{\\qquad\\leq\\Big(\\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\|\\nabla_{\\mathbf{w}_{r}}L_{i}(\\mathbf{W}^{(t)})\\|_{2}\\Big)^{2}}\\\\ &{\\qquad=\\Big(\\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\|\\sum_{r=1}^{m}\\frac{M_{i}}{\\partial P_{j}(\\mathbf{W},\\mathbf{x}_{i})}\\nabla_{\\mathbf{w}_{r}}F_{j}(\\mathbf{W},\\mathbf{x}_{i})\\|_{2}\\Big)^{2}}\\\\ &{\\qquad\\leq\\Big(\\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\sum_{r=1}^{m}\\Big)\\frac{M_{i}(\\mathbf{W})}{\\partial P_{j}(\\mathbf{W},\\mathbf{x}_{i})}\\|\\nabla_{\\mathbf{w}_{r}}F_{j}(\\mathbf{W},\\mathbf{x}_{i})\\|_{2}\\Big)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the frst inequality is by triangle inequality and second inequality uses $\\begin{array}{r}{\\sum_{i}a_{i}^{2}\\leq(\\sum_{i}a_{i})^{2}}\\end{array}$ for $a_{i}\\geq0$ and the last inequality is by triangle inequality. ", "page_idx": 30}, {"type": "text", "text": "Now we upper bound $\\lVert\\nabla_{\\mathbf{w}_{r}}F_{j}(\\mathbf{W},\\mathbf{x}_{i})\\rVert_{2}$ as ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{\\mathbf{w}_{r}}F_{0}(\\mathbf{W},\\mathbf{x}_{i})\\|_{2}=\\frac{1}{m\\tau}\\|\\sigma^{\\prime}(\\langle\\mathbf{w}_{r},y_{i}\\mu\\rangle)\\sigma(\\langle\\mathbf{w}_{r},y_{i}\\mu\\rangle)y_{i}\\mu+\\sigma^{\\prime}(\\langle\\mathbf{w}_{r},\\pmb{\\xi}_{i}\\rangle)\\sigma(\\langle\\mathbf{w}_{r},\\pmb{\\xi}_{i}+\\epsilon_{i}\\rangle)\\pmb{\\xi}_{i}\\|_{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\leq\\frac{1}{m\\tau}\\big(\\sigma(\\langle\\mathbf{w}_{r},y_{i}\\mu\\rangle)\\|\\mu\\|_{2}+\\sigma(\\langle\\mathbf{w}_{r},\\pmb{\\xi}_{i}+\\epsilon_{i}\\rangle)\\|\\pmb{\\xi}_{i}\\|_{2}\\big)}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\frac{1}{m\\tau}\\big(\\sigma(\\langle\\mathbf{w}_{r},y_{i}\\mu\\rangle)+\\sigma(\\langle\\mathbf{w}_{r},\\pmb{\\xi}_{i}+\\epsilon_{i}\\rangle)\\big)O\\big(\\operatorname*{max}\\{\\|\\mu\\|_{2},\\sigma_{\\xi}\\sqrt{d}\\}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where the first inequality is by triangle inequality and the second inequality is by Jensen's inequality. Similarly, we can obtain for $j=1,...,M$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\|\\nabla_{\\mathbf{w}_{r}}F_{j}\\mathbf{W},\\mathbf{x}_{i})\\|_{2}\\leq\\frac{1}{m\\tau}\\big(\\sigma(\\langle\\mathbf{w}_{r},y_{j}\\mu\\rangle)+\\sigma(\\langle\\mathbf{w}_{r},\\pmb{\\xi}_{j}\\rangle)\\big)O\\big(\\operatorname*{max}\\{\\|\\mu\\|_{2},\\sigma_{\\xi}\\sqrt{d}\\}\\big).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "For clarity let ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{z_{r,0}=\\sigma(\\langle\\mathbf{w}_{r},y_{i}\\pmb{\\mu}\\rangle)+\\sigma(\\langle\\mathbf{w}_{r},\\pmb{\\xi}_{i}+\\pmb{\\epsilon}_{i}\\rangle)}\\\\ &{z_{r,j}=\\sigma(\\langle\\mathbf{w}_{r},y_{j}\\pmb{\\mu}\\rangle)+\\sigma(\\langle\\mathbf{w}_{r},\\pmb{\\xi}_{j}\\rangle),\\;\\mathrm{for}\\;j=1,...,M}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Substituting the above results into (30) gives ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla L_{S}(\\mathbf{W}^{(t)})\\|_{F}^{2}\\le\\Big(\\frac{1}{n m\\tau}\\displaystyle\\sum_{i=1}^{n}\\displaystyle\\sum_{r=1}^{m}\\sum_{j=0}^{M}\\big|\\frac{\\partial L_{i}(\\mathbf{W})}{\\partial F_{j}(\\mathbf{W},\\mathbf{x}_{i})}|z_{r,j}\\big)^{2}O\\big(\\operatorname*{max}\\{\\|\\mu\\|_{2}^{2},\\sigma_{\\xi}^{2}d\\}\\big)}\\\\ &{\\qquad\\qquad\\qquad\\le\\Big(\\displaystyle\\frac{1}{n\\tau}\\displaystyle\\sum_{i=1}^{n}\\big(\\displaystyle\\sum_{j=0}^{M}|\\frac{\\partial L_{i}(\\mathbf{W})}{\\partial F_{j}(\\mathbf{W},\\mathbf{x}_{i})}|\\big)(\\frac{1}{m}\\displaystyle\\sum_{r=1}^{m}\\sum_{j=0}^{M}z_{r,j})\\Big)^{2}O\\big(\\operatorname*{max}\\{\\|\\mu\\|_{2}^{2},\\sigma_{\\xi}^{2}d\\}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where the second inequality is by $\\begin{array}{r}{\\sum_{i}a_{i}b_{i}\\leq(\\sum_{i}a_{i})(\\sum_{i}b_{i})}\\end{array}$ for $a_{i},b_{i}\\geq0$ ", "page_idx": 31}, {"type": "text", "text": "Next we can verify that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle\\sum_{j=0}^{M}\\Big|\\frac{\\partial{\\cal L}_{i}({\\bf W})}{\\partial{\\cal F}_{j}({\\bf W},{\\bf x}_{i})}\\Big|=1-\\frac{e^{{\\cal F}_{0}({\\bf W},{\\bf x}_{i})}}{e^{{\\cal F}_{0}({\\bf W},{\\bf x}_{i})}+\\sum_{j=1}^{M}e^{{\\cal F}_{j}({\\bf W},{\\bf x}_{i})}}+\\sum_{j=1}^{M}\\frac{e^{{\\cal F}_{j}({\\bf W},{\\bf x}_{i})}}{e^{{\\cal F}_{0}({\\bf W},{\\bf x}_{i})}+\\sum_{j=1}^{M}e^{{\\cal F}_{j}({\\bf W},{\\bf x}_{i})}}}\\\\ {\\displaystyle}&{\\qquad\\qquad=2\\big(1-\\frac{e^{{\\cal F}_{0}({\\bf W},{\\bf x}_{i})}}{e^{{\\cal F}_{0}({\\bf W},{\\bf x}_{i})}+\\sum_{j=1}^{M}e^{{\\cal F}_{j}({\\bf W},{\\bf x}_{i})}}\\big)}\\\\ {\\displaystyle}&{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad e^{{\\cal F}_{0}({\\bf W},{\\bf x}_{i})}}\\\\ {\\displaystyle}&{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad e^{{\\cal F}_{0}({\\bf W},{\\bf x}_{i})}+\\sum_{j=1}^{M}e^{{\\cal F}_{j}({\\bf W},{\\bf x}_{i})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where the last inequality is by $1-x\\leq-3\\log(x)$ for $x\\in[0,1]$ . Furthermore, we can show ", "page_idx": 31}, {"type": "equation", "text": "$$\n2\\big(1-\\frac{e^{F_{0}(\\mathbf{W},\\mathbf{x}_{i})}}{e^{F_{0}(\\mathbf{W},\\mathbf{x}_{i})}+\\sum_{j=1}^{M}e^{F_{j}(\\mathbf{W},\\mathbf{x}_{i})}}\\big)(\\frac{1}{m}\\sum_{r=1}^{m}\\sum_{j=0}^{M}z_{r,j})^{2}=O(1),\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "which leads to ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\nabla L_{S}(\\mathbf W^{(t)})\\|_{F}^{2}\\leq\\Big(\\frac{1}{n\\tau}\\displaystyle\\sum_{i=1}^{n}\\sqrt{(\\sum_{j=0}^{M}|\\frac{\\partial L_{i}(\\mathbf W^{(t)})}{\\partial F_{j}(\\mathbf W^{(t)},\\mathbf x_{i})}|)^{2}(\\frac{1}{m}\\displaystyle\\sum_{r=1}^{m}\\sum_{j=0}^{M}z_{r,j})^{2}}\\Big)^{2}O\\big(\\operatorname*{max}\\{\\|\\mu\\|_{2}^{2},\\sigma_{\\xi}^{2}d\\}\\big)}}\\\\ &{}&{\\leq\\Big(\\displaystyle\\frac{1}{n\\tau}\\sum_{i=1}^{n}\\sqrt{\\sum_{j=0}^{M}|\\frac{\\partial L_{i}(\\mathbf W^{(t)})}{\\partial F_{j}(\\mathbf W^{(t)},\\mathbf x_{i})}|}\\Big)^{2}O\\big(\\operatorname*{max}\\{\\|\\mu\\|_{2}^{2},\\sigma_{\\xi}^{2}d\\}\\big)}\\\\ &{}&{\\leq O\\big(\\operatorname*{max}\\{\\|\\mu\\|_{2}^{2},\\sigma_{\\xi}^{2}d\\}\\big)\\frac{1}{n}\\sum_{i=1}^{n}L_{i}(\\mathbf W^{(t)})}\\\\ &{}&{\\leq O\\big(\\operatorname*{max}\\{\\|\\mu\\|_{2}^{2},\\sigma_{\\xi}^{2}d\\}\\big)L_{S}(\\mathbf W^{(t)}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where the third inequality is by (31) and Cauchy-Schwartz inequality ", "page_idx": 31}, {"type": "text", "text": "We define ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbf{w}_{r}^{*}=\\mathbf{w}_{r}^{(0)}+2\\tau\\log(2M/\\epsilon)\\sum_{i=1}^{n}\\frac{\\pmb{\\xi}_{i}}{\\|\\pmb{\\xi}_{i}\\|_{2}^{2}}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Recall in the first stage ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbf{w}_{r}^{(T_{1})}=\\mathbf{w}_{r}^{(0)}+\\gamma_{r}^{(T_{1})}\\|\\pmb{\\mu}\\|_{2}^{-2}\\pmb{\\mu}+\\sum_{i=1}^{n}\\rho_{r,i}^{(T_{1})}\\|\\pmb{\\xi}_{i}\\|_{2}^{-2}\\pmb{\\xi}_{i},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "and we have ", "page_idx": 32}, {"type": "text", "text": "Lemma C.15. Under Assumption 4.1, we have $\\|\\mathbf{W}^{(T_{1})}-\\mathbf{W}^{*}\\|_{F}\\leq\\widetilde{O}(m^{1/2}n^{1/2}\\sigma_{\\xi}^{-1}d^{-1/2}).$ ", "page_idx": 32}, {"type": "text", "text": "Proof of Lemma C.15. We have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{W}^{(T_{1})}-\\mathbf{W}^{*}\\|_{F}\\leq\\|\\mathbf{W}^{(T_{1})}-\\mathbf{W}^{(0)}\\|_{F}+\\|\\mathbf{W}^{(0)}-\\mathbf{W}^{*}\\|_{F}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\displaystyle\\sum_{r}\\frac{\\gamma_{r}^{(T_{1})}}{\\|\\mu\\|_{2}}+O(\\sqrt{m})\\operatorname*{max}_{r}\\|\\sum_{i=1}^{n}\\rho_{r,i}^{(T_{1})}\\frac{\\xi_{i}}{\\|\\xi_{i}\\|_{2}^{2}}\\|_{2}+O(m^{1/2}n^{1/2}\\log(1/\\epsilon)\\sigma_{\\xi}^{-1}d^{-1/2})}\\\\ &{\\qquad\\qquad\\qquad\\leq\\tilde{O}(m^{1/2}n^{1/2}\\sigma_{\\xi}^{-1}d^{-1/2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Lemma C.16. Under Assumption 4.1, we have for all $t\\in[T_{1},T^{*}]$ ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\nabla F_{0}(\\mathbf{W}^{(t)},\\mathbf{x}_{i}),\\mathbf{W}^{*}\\rangle\\ge2\\log(2M/\\epsilon),}\\\\ &{\\langle\\nabla F_{j}(\\mathbf{W}^{(t)},\\mathbf{x}_{i}),\\mathbf{W}^{*}\\rangle\\le\\log(2M/\\epsilon).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof of Lemma C.16. Based on the definition of $\\mathbf{W}^{*}$ and $F_{j}({\\bf W}^{(t)},{\\bf x}_{i})$ , we can derive for $j=0$ \uff0c ", "page_idx": 32}, {"type": "text", "text": "(VFo(W(t),xi), W\\*) ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla F_{0}(\\mathbf{W}^{\\langle\\nu},\\mathbf{x}_{i}),\\mathbf{w}^{*})}\\\\ &{=\\displaystyle\\sum_{r=1}^{m}\\nabla_{w}F_{0}(\\mathbf{W}^{\\langle\\nu|},\\mathbf{x}_{i}),\\mathbf{w}_{r}^{*})}\\\\ &{=\\displaystyle\\frac{1}{m\\tau}\\sum_{r=1}^{m}\\sigma^{\\prime}(\\langle\\mathbf{w}_{r}^{(t)},y_{i}\\mu\\rangle)\\sigma(\\langle\\mathbf{w}_{r}^{(t)},y_{i}\\mu\\rangle)\\langle\\mathbf{w}_{r}^{*},y_{i}\\mu\\rangle+\\frac{1}{m\\tau}\\sum_{r=1}^{m}\\sigma^{\\prime}(\\langle\\mathbf{w}_{r}^{(t)},\\xi_{i}\\rangle)\\sigma(\\langle\\mathbf{w}_{r}^{(t)},\\xi_{i}+\\epsilon_{i}\\rangle)\\langle\\mathbf{w}_{r}^{*},}\\\\ &{=\\displaystyle\\frac{1}{m\\tau}\\sum_{r=1}^{m}\\sigma^{\\prime}(\\langle\\mathbf{w}_{r}^{(t)},y_{i}\\mu\\rangle)\\sigma(\\langle\\mathbf{w}_{r}^{(t)},y_{i}\\mu\\rangle)\\Big(\\langle\\mathbf{w}_{r}^{(t)},y_{i}\\mu\\rangle+2\\tau\\log(2M/\\epsilon)\\sum_{i=1}^{m}\\Tilde{\\xi}_{i},y_{i}\\mu\\rangle\\|\\xi_{i}\\|_{2}^{2}\\Big)}\\\\ &{\\quad+\\displaystyle\\frac{1}{m\\tau}\\sum_{r=1}^{m}\\sigma^{\\prime}(\\langle\\mathbf{w}_{r}^{(t)},\\xi_{i}\\rangle)\\sigma(\\langle\\mathbf{w}_{r}^{(t)},\\xi_{i}+\\epsilon_{i}\\rangle)\\Big(\\langle\\mathbf{w}_{r}^{(0)},\\xi_{i}\\rangle+2\\tau\\log(2M/\\epsilon)+2\\tau\\log(2M/\\epsilon)\\sum_{i\\neq i}\\xi_{i},\\xi_{i}\\Big)}\\\\ &{\\ge\\displaystyle\\frac{1}{m\\tau}\\sum_{r=1}^{m}\\sigma^{\\prime}(\\langle\\mathbf{w}_{r}^{(t)},\\xi_{i}\\rangle)\\sigma(\\langle\\mathbf{w}_{r}^{(t)},\\xi_{i}+\\epsilon_{i}\\rangle)2\\tau\\log(2M/\\epsilon)-\\frac{1}{m\\tau}\\sum_{r=1}^{m}\\sigma^{\\prime}(\\langle\\mathbf{w}_{r}^{(t)},y_{\n$$", "text_format": "latex", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\underbrace{\\frac{1}{m\\tau}\\displaystyle\\sum_{r=1}^{m}\\sigma(\\langle\\mathbf w_{r}^{(t)},y_{i}\\mu\\rangle)2\\tau\\log(2M/\\epsilon)\\widetilde O(n\\|\\mu\\|_{2}\\sigma_{\\xi}^{-1}d^{-1})}_{I_{3}}-\\underbrace{\\frac{1}{m\\tau}\\displaystyle\\sum_{r=1}^{m}\\sigma(\\langle\\mathbf w_{r}^{(t)},\\pmb\\xi_{i}+\\epsilon_{i}\\rangle)\\widetilde O(\\sigma_{0}\\sigma_{\\xi}\\vee\\sigma_{\\xi})}_{I_{4}}}\\\\ &{-\\underbrace{\\frac{1}{m\\tau}\\displaystyle\\sum_{r=1}^{m}\\sigma(\\langle\\mathbf w_{r}^{(t)},\\pmb\\xi_{i}+\\epsilon_{i}\\rangle)2\\tau\\log(2M/\\epsilon)\\widetilde O(n d^{-1/2})}_{I_{3}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where the inequality is by Lemma B.5. ", "page_idx": 33}, {"type": "text", "text": "Next, we bound $I_{1},I_{2},I_{3},I_{4},I_{5}$ separately. For $I_{1}$ , we take maximum over $r$ , which results in ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\frac{1}{m}\\sum_{r=1}^{m}\\sigma(\\langle\\mathbf{w}_{r}^{(t)},\\pmb{\\xi}_{i}+\\epsilon_{i}\\rangle)\\geq\\frac{1}{m}\\sum_{r=1}^{m}\\sigma(\\frac{1}{2}\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle+\\rho_{r,i}^{(t)})\\geq2.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Thus we obtain ", "page_idx": 33}, {"type": "equation", "text": "$$\nI_{1}\\geq4\\log(2M/\\epsilon).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "In addition wecanshowyupperboun $\\rho_{r,i}^{(t)}$ and $\\gamma_{r}^{(t)}$ \uff0c ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\langle\\mathbf{w}_{r}^{(t)},\\pmb{\\xi}_{i}+\\epsilon_{i}\\rangle\\leq\\frac{3}{2}|\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle|+|\\rho_{r,i}^{(t)}|\\leq\\widetilde{O}(1),}\\\\ {\\langle\\mathbf{w}_{r}^{(t)},\\pmb{\\mu}\\rangle\\leq\\frac{3}{2}|\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\mu}\\rangle|+|\\gamma_{r}^{(t)}|\\leq\\widetilde{O}(1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "This implies ", "page_idx": 33}, {"type": "equation", "text": "$$\nI_{2}\\leq\\widetilde{O}(\\sigma_{0}\\|\\mu\\|_{2}),\\ I_{3}\\leq\\log(2M/\\epsilon)\\widetilde{O}(n m\\|\\mu\\|_{2}\\sigma_{\\xi}^{-1}d^{-1}),\\ I_{4}\\leq\\widetilde{O}(\\sigma_{0}\\sigma_{\\xi}\\sqrt{d}),\\ I_{5}\\leq\\widetilde{O}(n m d^{-1/2}).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Based on the conditions on $\\sigma_{0},d$ wecan show ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\langle\\nabla F_{0}(\\mathbf{W}^{(t)},\\mathbf{x}_{i}),\\mathbf{W}^{*}\\rangle\\ge4\\log(2M/\\epsilon)-I_{2}-I_{3}-I_{4}-I_{5}\\ge2\\log(2M/\\epsilon).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Now we prove for the claim for $F_{j}(\\mathbf{W}^{(t)},\\mathbf{W}^{*})$ for $j=1,...,M$ as follows. ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla F_{j}(\\mathbf{W}^{(t)},\\mathbf{x}_{i}),\\mathbf{W}^{*})}\\\\ &{=\\displaystyle\\frac{1}{m\\tau}\\sum_{r=1}^{m}\\sigma^{\\prime}\\big(\\langle\\mathbf{w}_{r}^{(t)},y_{i}\\mu\\rangle\\big)\\sigma(\\langle\\mathbf{w}_{r}^{(t)},y_{j}\\mu\\rangle)\\langle\\mathbf{w}_{r}^{*},y_{i}\\mu\\rangle+\\displaystyle\\frac{1}{m\\tau}\\sum_{r=1}^{m}\\sigma^{\\prime}\\big(\\langle\\mathbf{w}_{r}^{(t)},\\pmb{\\xi}_{i}\\rangle\\big)\\sigma(\\langle\\mathbf{w}_{r}^{(t)},\\pmb{\\xi}_{j}\\rangle)\\langle\\mathbf{w}_{r}^{*},\\pmb{\\xi}_{i}\\rangle}\\\\ &{=\\displaystyle\\frac{1}{m\\tau}\\sum_{r=1}^{m}\\sigma^{\\prime}\\big(\\langle\\mathbf{w}_{r}^{(t)},\\pmb{\\xi}_{i}\\rangle\\big)\\sigma(\\langle\\mathbf{w}_{r}^{(t)},\\pmb{\\xi}_{j}\\rangle)\\langle\\mathbf{w}_{r}^{*},\\pmb{\\xi}_{i}\\rangle}\\\\ &{\\le I_{3}+I_{4}+I_{5}\\le\\log(2M/\\epsilon),}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where the second equality is by $y_{i}\\neq y_{j}$ ", "page_idx": 33}, {"type": "text", "text": "Lemma C.17. Under Assumption 4.1, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\|\\mathbf{W}^{(t)}-\\mathbf{W}^{*}\\|_{F}^{2}-\\|\\mathbf{W}^{(t+1)}-\\mathbf{W}^{*}\\|\\geq\\eta L_{S}(\\mathbf{W}^{(t)})-\\eta\\epsilon.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof of Lemma C.17. First, we verify that for $j=0$ \uff0c ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\langle\\nabla F_{0}({\\mathbf W}^{(t)},{\\mathbf x}_{i}),{\\mathbf W}^{(t)}\\rangle=\\displaystyle\\sum_{r=1}^{m}\\langle\\nabla_{{\\mathbf w}_{r}}F_{0}({\\mathbf W}^{(t)},{\\mathbf x}_{i}),{\\mathbf w}_{r}^{(t)}\\rangle}&{}\\\\ &{=\\displaystyle\\frac{1}{m\\tau}\\sum_{r=1}^{m}\\sigma^{\\prime}\\big(({\\mathbf w}_{r},y_{i},\\mu)\\big){\\mathbf s}\\mathbf{g}\\big(\\sigma\\big(({\\mathbf w}_{r},y_{i}\\mu)\\big)\\big)y_{i}{\\mathbf\\mu},{\\mathbf w}_{r}^{(t)}\\rangle}\\\\ &{\\quad+\\displaystyle\\frac{1}{m\\tau}\\sum_{r=1}^{m}\\sigma^{\\prime}\\big(({\\mathbf w}_{r},{\\mathbf\\xi}_{i}){\\mathbf s}\\mathbf{g}\\big(\\sigma\\big(({\\mathbf w}_{r},{\\mathbf\\xi}_{i}+{\\mathbf\\xi}_{i})\\big)\\big)\\xi_{i},{\\mathbf w}_{r}^{(t)}\\big)}\\\\ &{=\\displaystyle\\frac{1}{m\\tau}\\sum_{r=1}^{m}\\sigma\\big(\\langle{\\mathbf w}_{r},y_{i}\\mu\\rangle\\big){\\mathbf s}\\mathbf{g}\\big(\\sigma\\big(\\mathbf w_{r},y_{i}\\mu\\big)\\big)}\\\\ &{\\quad+\\displaystyle\\frac{1}{m\\tau}\\sum_{r=1}^{m}\\sigma\\big(\\langle{\\mathbf w}_{r},{\\mathbf\\xi}_{i}\\rangle\\big){\\mathbf s}\\mathbf{g}\\big(\\sigma\\big({\\mathbf w}_{r},\\xi_{i}+{\\mathbf\\xi}_{i}\\big)\\big)}\\\\ &{=F_{0}({\\mathbf0}^{(t)},{\\mathbf x}_{i}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Similarly, we can show for $j=1,...,M$ , it satisfies that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\langle\\nabla F_{j}(\\mathbf{W}^{(t)},\\mathbf{x}_{i}),\\mathbf{W}^{(t)}\\rangle=F_{j}(\\mathbf{W}^{(t)},\\mathbf{x}_{i}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "By the update of $\\mathbf{W}^{(t)}$ , we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\mathbf{V}^{(\\mu)}-\\mathbf{V}^{\\mu}|_{1}^{2}-\\|\\mathbf{V}^{(\\mu+1)}-\\mathbf{W}^{\\mu}\\|_{2}^{2}}\\\\ &{=2\\eta[\\nabla L_{\\lambda}(\\mathbf{W}^{(0)})_{\\lambda}\\mathbf{W}^{(0)}-\\mathbf{W}^{\\mu}]-\\eta[\\nabla L_{\\lambda}(\\mathbf{W}^{(0)})]_{\\lambda}^{2}}\\\\ &{=\\frac{2\\eta}{n}\\frac{\\sqrt{n}}{n+\\mu_{0}}\\frac{\\partial L_{\\lambda}(\\mathbf{W}^{(0)})}{\\partial\\zeta_{n}(\\mathbf{W}^{(0)})_{\\lambda}}(\\nabla F_{\\lambda}(\\mathbf{W}^{(0)},\\mathbf{X}),\\mathbf{W}^{(0)}-\\mathbf{W}^{\\mu})-\\eta^{2}\\|\\nabla L_{\\lambda}(\\mathbf{W}^{(0)})\\|_{F}^{2}}\\\\ &{=\\frac{2\\eta}{n}\\sum_{i=1}^{M}\\frac{\\partial L_{\\lambda}}{\\partial\\zeta_{n}(\\mathbf{W}^{(0)})_{\\lambda}}(\\mathbb{E}_{\\mu}(\\mathbf{W}^{(0)},\\mathbf{x})-(\\nabla F_{\\lambda}(\\mathbf{W}^{(0)},\\mathbf{x}),\\mathbf{W}^{\\mu}))-\\eta^{2}\\|\\nabla L_{\\lambda}(\\mathbf{W}^{(0)})\\|_{F}^{2}}\\\\ &{\\geq\\frac{2\\eta}{n}\\sum_{i=1}^{M}\\frac{\\partial L_{\\lambda}}{\\partial\\zeta_{n}(\\mathbf{W}^{(0)})_{\\lambda}}(\\mathbb{E}_{\\mu}(\\mathbf{W}^{(0)},\\mathbf{x})-2\\log(2M/\\mu))+\\frac{M_{\\lambda}}{\\mu_{0}}\\frac{\\partial L_{\\lambda}}{\\partial\\zeta_{n}(\\mathbf{W}^{(0)},\\mathbf{x})}(F_{\\lambda}(\\mathbf{W}^{(0)},\\mathbf{x}))}\\\\ &{>\\frac{2\\eta}{n}\\sum_{i=1}^{M}\\left(\\frac{\\partial L_{\\lambda}(\\mathbf{W}^{(0)})}{\\partial\\zeta_{n}(\\mathbf{W}^{(0)})_{\\lambda}}\\mathbb{E}_{\\mu}(\\mathbf{W}^{(0)},\\mathbf{x})-2\\log(2M/\\mu)\\right)+\\frac{M_{\\lambda}}{\\mu \n$$\u2265nLs(W(t)-ne, ", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where the third equality is by (32) and (33). The first inequality is by Lemma C.16. The second inequality is due to the convexity of negative log-Softmax function. The last inequality is by Lemma C.14 (and the conditions on $\\eta$ and $\\log(1+x)\\leq x$ for $x\\geq0$ \u53e3 ", "page_idx": 34}, {"type": "text", "text": "Lemma C.18. Under Asumpio 4.1,let $\\begin{array}{r}{T=T_{1}\\!+\\!\\lfloor\\frac{\\|\\mathbf{W}^{(T_{1})}-\\mathbf{W}^{*}\\|_{F}^{2}}{\\eta\\epsilon}\\rfloor=T_{1}\\!+\\!\\widetilde O(m n\\sigma_{\\xi}^{-2}d^{-1}\\eta^{-1}\\epsilon^{-1}).}\\end{array}$ Then we have $\\operatorname*{max}_{r}|\\gamma_{r}^{(t)}|\\leq\\widetilde O(1/\\sqrt{n})$ for all $T_{1}\\leq t\\leq T$ In addition, we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{1}{t-T_{1}+1}\\sum_{s=T_{1}}^{t}L_{S}(\\mathbf{W}^{(s)})\\leq\\frac{\\|\\mathbf{W}^{(T_{1})}-\\mathbf{W}^{*}\\|_{F}^{2}}{\\eta(t-T_{1}+1)}+\\epsilon\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "for all $T_{1}\\leq t\\leq T$ Thus there exists an iterate $\\mathbf{W}^{(s)}$ for $s\\in[T_{1},T]$ with training loss smaller than $2\\epsilon$ ", "page_idx": 34}, {"type": "text", "text": "Proof of Lemma C.18. By Lemma C.17, for $t\\in[T_{1},T]$ \uff0c ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\|\\mathbf{W}^{(s)}-\\mathbf{W}^{*}\\|_{F}^{2}-\\|\\mathbf{W}^{(s+1)}-\\mathbf{W}^{*}\\|\\geq\\eta L_{S}(\\mathbf{W}^{(t)})-\\eta\\epsilon\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "for all $s\\leq t$ . Summing over the inequality and dividing both sides by $t-T_{1}+1$ yields ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{1}{\\iota-T_{1}+1}\\sum_{s=T_{1}}^{t}L_{S}(\\mathbf{W}^{(s)})\\leq\\frac{\\|\\mathbf{W}^{(T_{1})}-\\mathbf{W}^{*}\\|_{F}^{2}+\\eta\\epsilon(t-T_{1}+1)}{\\eta(t-T_{1}+1)}=\\frac{\\|\\mathbf{W}^{(T_{1})}-\\mathbf{W}^{*}\\|_{F}^{2}}{\\eta(t-T_{1}+1)}+\\epsilon\\leq2\\epsilon,\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where the last inequality is by the definition of $T$ , for all $T_{1}\\leq t\\leq T$ . In addition, we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\sum_{t=T_{1}}^{T}L_{S}(\\mathbf{W}^{(t)})\\leq\\frac{2\\|\\mathbf{W}^{(T_{1})}-\\mathbf{W}^{*}\\|_{F}^{2}}{\\eta}=\\widetilde{O}(\\eta^{-1}m n d^{-1}\\sigma_{\\xi}^{-2}).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Next we prove the claim that $\\operatorname*{max}_{r}|\\gamma_{r}^{(t)}|\\,\\le\\,3\\beta$ where $\\beta\\,=\\,|\\operatorname*{max}_{r}\\gamma_{r}^{(T_{1})}|\\,=\\,\\widetilde{O}(1/\\sqrt{n})$ (Ti) = O(1/\u221an) for all $T_{1}\\leq t\\leq T$ Without losf generality, wenlycnse $r\\in\\mathcal{U}_{+}^{(0)}$ . Frst t is evident that at $t=T_{1}$ \uff0c ", "page_idx": 34}, {"type": "text", "text": "we have $\\mathrm{max}_{r}$ $\\gamma_{r}^{\\left(t\\right)}=\\beta\\le3\\beta$ Next suppose there exsts $\\widetilde{T}\\in[T_{1},T]$ such that $\\operatorname*{max}_{r}\\gamma_{r}^{(t)}\\leq3\\beta$ all $t\\in[T_{1},\\widetilde{T}-1]$ . Then we let $\\phi^{(t)}=\\operatorname*{max}_{r}\\gamma_{r}^{(t)}$ and we have b the update of $\\gamma_{r}^{(t)}$ ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phi^{(t+1)}\\leq\\phi^{(t)}+\\displaystyle\\frac{\\eta}{n m\\tau}\\sum_{i:y_{i}=1}(1-\\ell_{i}^{\\prime(t)})\\operatorname*{max}_{r}(\\frac{3}{2}\\langle\\mathbf{w}_{r}^{(0)},\\boldsymbol{\\mu}\\rangle+\\phi^{(t)})\\|\\boldsymbol{\\mu}\\|_{2}^{2}}\\\\ &{\\qquad\\leq\\phi^{(t)}+\\displaystyle\\frac{\\eta}{n m\\tau}\\sum_{i:y_{i}=1}(1-\\ell_{i}^{\\prime(t)})(\\frac{3}{2}\\sqrt{2\\log(8m/\\delta)}\\sigma_{0}\\|\\boldsymbol{\\mu}\\|_{2}+\\phi^{(t)})\\|\\boldsymbol{\\mu}\\|_{2}^{2}}\\\\ &{\\qquad\\leq\\phi^{(t)}+\\displaystyle\\frac{\\eta}{m\\tau}L_{S}(\\mathbf{W}^{(t)})(\\frac{3}{2}\\sqrt{2\\log(8m/\\delta)}\\sigma_{0}\\|\\boldsymbol{\\mu}\\|_{2}+\\phi^{(t)})\\|\\boldsymbol{\\mu}\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where the third inequality is by (31) ", "page_idx": 35}, {"type": "text", "text": "Now summing over $t=T_{1},...,\\widetilde{T}-1$ , we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phi^{(\\widetilde{T})}\\leq\\phi^{(T_{1})}+\\displaystyle\\sum_{t=T_{1}}^{\\widetilde{T}-1}\\frac{\\eta}{m\\tau}L_{S}(\\mathbf{W}^{(t)})(\\frac{3}{2}\\sqrt{2\\log(8m/\\delta)}\\sigma_{0}\\|\\mu\\|_{2}+\\phi^{(t)})\\|\\mu\\|_{2}^{2}}\\\\ &{\\qquad\\leq\\phi^{(T_{1})}+O(\\frac{\\eta\\|\\mu\\|_{2}^{2}}{m\\tau})\\beta\\displaystyle\\sum_{t=T_{1}}^{\\widetilde{T}-1}L_{S}(\\mathbf{W}^{(t)})}\\\\ &{\\qquad\\leq\\phi^{(T_{1})}+O(n\\|\\mu\\|_{2}^{2}d^{-1}\\sigma_{\\xi}^{-2})\\beta}\\\\ &{\\qquad\\leq\\phi^{(T_{1})}+O(n\\mathrm{SNR}^{2})\\beta}\\\\ &{\\qquad\\leq\\phi^{(T_{1})}+2\\beta\\leq3\\beta}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where the second inequality is by induction and the third inequality is by (34). The last inequality is by the condition of SNR. ", "page_idx": 35}, {"type": "text", "text": "C.3 Downstream Task Performance ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Recall that after the pre-training stage on the training data at time $T$ , the signal learning and noise memorization satisfy ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{r}{\\operatorname*{max}}\\,A_{r}^{(T)}=\\widetilde{O}(1/\\sqrt{n}),}\\\\ &{\\underset{r}{\\operatorname*{max}}\\,\\Psi_{r,i}^{(T)}=\\widetilde{\\Omega}(1)\\,\\mathrm{for}\\,i\\in[n].}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Then, on the downstream task, the corresponding embedding can be calculated as follows: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h_{r}(\\mathbf{x}_{\\mathrm{test}}^{(1)})=\\sigma(\\langle\\mathbf{w}_{r}^{(T)},\\mathbf{x}_{\\mathrm{test}}^{(1)}\\rangle)=\\widetilde{O}(1/\\sqrt{d n}),}\\\\ &{h_{r}(\\mathbf{x}_{\\mathrm{test}}^{(2)})=\\sigma(\\langle\\mathbf{w}_{r}^{(T)},\\mathbf{x}_{\\mathrm{test}}^{(2)}\\rangle)=\\widetilde{\\Omega}(1/\\sqrt{d}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Then, it is straightforward to check that the embedding of a finite size of samples during the fine-tuning stage is not linearly separable. Thus, the downstream task performance follows $L_{\\mathcal{D}_{\\mathrm{test}}}(T^{*})=\\Theta(1)$ ", "page_idx": 35}, {"type": "text", "text": "DMulti-Modal Contrastive Learning: Proof of Theorem 4.3 ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "D.1   First Stage ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Similar to the single-modal case, in the first stage, the loss derivative is close to its initial value for both modalities. ", "page_idx": 35}, {"type": "text", "text": "Lemma D.1. $I f\\operatorname*{max}\\{\\gamma_{r}^{(t)},\\rho_{r,i}^{(t)},\\widetilde{\\gamma}_{r}^{(t)},\\widetilde{\\rho}_{r,i}^{(t)}\\}=O(1),$ there exis constont $C_{\\ell}>1$ such hat ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{C_{\\ell}\\left(1+M\\right)}\\le\\ell_{i}^{\\prime(t)}\\le\\frac{C_{\\ell}}{1+M}}\\\\ {\\frac{1}{C_{\\ell}\\left(M+1\\right)}\\le\\ell_{i,j}^{\\prime(t)}\\le\\frac{C_{\\ell}}{1+M}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "for all $i\\in[n]$ ", "page_idx": 35}, {"type": "text", "text": "Lemma D.2. Supose that $\\gamma_{r}^{(t)},\\widetilde{\\gamma}_{r}^{(t)}=O(1)$ and $\\rho_{r,i}^{(t)},\\widetilde{\\rho}_{r,i}^{(t)}=O(1)$ for all $r\\,\\in\\,[m]$ and $i\\,\\in\\,[n]$ Under Assumption 4.1, then for any $\\delta>0$ with probability at least $1-\\delta$ ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\langle\\mathbf{w}_{r}^{(t)}-\\mathbf{w}_{r}^{(0)},\\xi_{i}\\rangle-\\rho_{r,i}^{(t)}|\\leq5\\sqrt{\\frac{\\log(6n^{2}/\\delta)}{d}}n}\\\\ &{|\\langle\\mathbf{w}_{r}^{(t)}-\\mathbf{w}_{r}^{(0)},\\mu\\rangle-\\gamma_{r}^{(t)}|\\leq\\mathrm{SNR}\\sqrt{\\frac{8\\log(6n/\\delta)}{d}}n,}\\\\ &{|\\langle\\widetilde{\\mathbf{w}}_{r}^{(t)}-\\widetilde{\\mathbf{w}}_{r}^{(0)}\\widetilde{\\xi}_{i}\\rangle-\\widetilde{\\rho}_{r,i}^{(t)}|\\leq5\\sqrt{\\frac{\\log(6n^{2}/\\delta)}{\\tilde{d}}}n}\\\\ &{|\\langle\\widetilde{\\mathbf{w}}_{r}^{(t)}-\\widetilde{\\mathbf{w}}_{r}^{(0)},\\widetilde{\\mu}\\rangle-\\widetilde{\\gamma}_{r}^{(t)}|\\leq\\widetilde{\\mathrm{SNR}}\\sqrt{\\frac{8\\log(6n/\\delta)}{\\tilde{d}}}n}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "for all $r\\in[m],\\,i\\in[n]$ ", "page_idx": 36}, {"type": "text", "text": "D.1.1 Dynamics of Signal Learning: Lower Bound ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "We first analyze the dynamics of signal learning for both two modalities. Similar as in the singlemodal learning, we partition the neurons, depending on the initialization, i.e.. $\\mathcal{U}_{+}^{(t)}=\\{r\\in\\bar{[m]}:$ $\\langle\\mathbf{w}_{r}^{(t)},\\pmb{\\mu}\\rangle>0\\}$ and $\\mathcal{U}_{-}^{(t)}\\,=\\,\\{r\\,\\in\\,[m]\\,:\\,\\langle\\mathbf{w}_{r}^{(t)},\\pmb{\\mu}\\rangle\\,<\\,0\\}$ $\\widetilde{\\mathcal{U}}_{+}^{(t)}\\,=\\,\\{r\\,\\in\\,[m]\\,:\\,\\langle\\widetilde{\\mathbf{w}}_{r}^{(t)},\\widetilde{\\pmb{\\mu}}\\rangle\\,>\\,0\\}$ and $\\widetilde{\\mathcal{U}}_{-}^{(t)}=\\{r\\in[m]:\\langle\\widetilde{\\mathbf{w}}_{r}^{(t)},\\widetilde{\\pmb{\\mu}}\\rangle<0\\}$ ", "page_idx": 36}, {"type": "text", "text": "Lemma D.3. Under Assumption 4.1 and the same condition as Lemma D.2, for all $t>0$ wehave (1) $\\mathcal{U}_{+}^{(t)}\\cap\\widetilde{\\mathcal{U}}_{+}^{(t)}=\\mathcal{U}_{+}^{(0)}\\cap\\widetilde{\\mathcal{U}}_{+}^{(0)}$ and $\\gamma_{r}^{(t)}\\geq0,$ $\\widetilde{\\gamma}_{r}^{(t)}\\geq0$ and are increasing. (2) $\\mathcal{U}_{-}^{(t)}\\cap\\widetilde{\\mathcal{U}}_{-}^{(t)}=\\mathcal{U}_{-}^{(0)}\\cap\\widetilde{\\mathcal{U}}_{-}^{(0)}$ and $\\gamma_{r}^{(t)}\\leq0,\\,\\widetilde{\\gamma}_{r}^{(t)}\\leq0$ and are decreasing. (3) For $r\\in\\mathcal{U}_{+}^{(0)}\\cap\\widetilde{\\mathcal{U}}_{-}^{(0)}\\;o r\\,r\\in\\mathcal{U}_{-}^{(0)}\\cap\\widetilde{\\mathcal{U}}_{+}^{(0)}$ there exists a time $t^{\\prime}>0$ such that $r\\in\\mathcal{U}_{+}^{(t^{\\prime})}\\cap\\widetilde{\\mathcal{U}}_{+}^{(t^{\\prime})}$ or $r\\in\\mathcal{U}_{-}^{(t^{\\prime})}\\cap\\widetilde{\\mathcal{U}}_{-}^{(t^{\\prime})}$ ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "Proof of Lemma $D.3$ We frst analyze the neurons $r\\in\\mathcal{U}_{+}^{(t)}\\cap\\tilde{\\mathcal{U}}_{+}^{(t)}$ . By the update of $\\gamma_{r}^{\\left(t\\right)}$ ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma_{r}^{(t+1)}=\\gamma_{r}^{(t)}+\\frac{\\eta}{n m\\tau}\\displaystyle\\sum_{i=1}^{n}(1-\\ell_{i}^{\\prime(t)})\\sigma(\\langle\\widetilde{\\mathbf{w}}_{r}^{(t)},y_{i}\\widetilde{\\mu}\\rangle)\\sigma^{\\prime}(\\langle\\mathbf{w}_{r}^{(t)},y_{i}\\mu\\rangle)y_{i}\\|\\mu\\|_{2}^{2}}\\\\ &{\\quad\\quad\\quad-\\frac{\\eta}{n m\\tau}\\displaystyle\\sum_{i=1}^{n}\\sum_{j\\neq i}^{M}\\ell_{i,j}^{\\prime(t)}\\sigma(\\langle\\widetilde{\\mathbf{w}}_{r}^{(t)},y_{j}\\widetilde{\\mu}\\rangle)\\sigma^{\\prime}(\\langle\\mathbf{w}_{r}^{(t)},y_{i}\\mu\\rangle)y_{i}\\|\\mu\\|_{2}^{2}}\\\\ &{\\quad\\quad\\quad=\\gamma_{r}^{(t)}+\\frac{\\eta}{n m\\tau}\\displaystyle\\sum_{i:y_{i}=1}^{n}(1-\\ell_{i}^{\\prime(t)})\\sigma(\\langle\\widetilde{\\mathbf{w}}_{r}^{(t)},\\widetilde{\\mu}\\rangle)\\|\\mu\\|_{2}^{2}\\geq\\gamma_{r}^{(t)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where the second equality is by $y_{i}=y_{j}$ and the derivative of ReLU activation. Similarly for the other modality, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{\\gamma}_{r}^{(t+1)}=\\widetilde{\\gamma}_{r}^{(t)}+\\frac{\\eta}{n m\\tau}\\displaystyle\\sum_{i=1}^{n}(1-\\ell_{i}^{\\prime(t)})\\sigma(\\langle\\mathbf{w}_{r}^{(t)},y_{i}\\mu\\rangle)\\sigma^{\\prime}(\\langle\\widetilde{\\mathbf{w}}_{r}^{(t)},y_{i}\\widetilde{\\mu}\\rangle)y_{i}\\|\\widetilde{\\mu}\\|_{2}^{2}}\\\\ &{\\quad\\quad\\quad-\\frac{\\eta}{n m\\tau}\\displaystyle\\sum_{i=1}^{n}\\sum_{j\\neq i}^{M}\\ell_{i,j}^{\\prime(t)}\\sigma(\\langle\\mathbf{w}_{r}^{(t)},y_{j}\\mu\\rangle)\\sigma^{\\prime}(\\langle\\widetilde{\\mathbf{w}}_{r}^{(t)},y_{i}\\widetilde{\\mu}\\rangle)y_{i}\\|\\widetilde{\\mu}\\|_{2}^{2}}\\\\ &{\\quad\\quad\\quad=\\widetilde{\\gamma}_{r}^{(t)}+\\frac{\\eta}{n m\\tau}\\displaystyle\\sum_{i:y_{i}=1}^{n}(1-\\ell_{i}^{\\prime(t)})\\sigma(\\langle\\mathbf{w}_{r}^{(t)},\\mu\\rangle)\\|\\widetilde{\\mu}\\|_{2}^{2}\\geq\\widetilde{\\gamma}_{r}^{(t)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Hence we se at $t=0$ the caim is stisfed as $\\gamma_{r}^{(1)}\\geq\\gamma_{r}^{(0)}$ for $r\\in\\mathcal{U}_{+}^{(0)}\\cap\\widetilde{\\mathcal{U}}_{+}^{(0)}$ . Then we prove the claim by induction. Suppose at iteration $t$ the claims are satisfied, i.e., $\\gamma_{r}^{(t)}\\geq\\gamma_{r}^{(t-1)}\\geq0$ $r\\in\\mathcal{U}_{+}^{(0)}$ and $r\\in\\mathcal{U}_{+}^{(t)}$ . Then we can see from (35) ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\gamma_{r}^{(t+1)}\\geq\\gamma_{r}^{(t)}\\geq0.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Similarly, we can show from (36) that $\\widetilde{\\gamma}_{r}^{(t+1)}\\geq\\widetilde{\\gamma}_{r}^{(t)}\\geq0$ ", "page_idx": 37}, {"type": "text", "text": "Then for the inner product, we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\mathbf{w}_{r}^{(t+1)},\\pmb{\\mu}\\rangle\\overset{(a)}{\\geq}\\gamma_{r}^{(t+1)}+\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\mu}\\rangle-\\mathrm{SNR}\\sqrt{\\frac{8\\log\\left(6n/\\delta\\right)}{d}}n}\\\\ &{\\qquad\\qquad\\qquad\\overset{(b)}{\\geq}0.99\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\mu}\\rangle>0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where inequality (a) is by Lemma D.2, and inequality (b) is by Lemma B.2. Similarly, the same result holds frthe oher modality Together,it shows $r\\in\\mathcal{U}_{+}^{(t+1)}\\cap\\widetilde{\\mathcal{U}}_{+}^{(t+1)}$ and the inductinis compete. ", "page_idx": 37}, {"type": "text", "text": "Similarly, we can use the same strategy to prove claim (2) for $r\\in\\mathcal{U}_{-}^{(0)}\\cap\\widetilde{\\mathcal{U}}_{-}^{(0)}$ ", "page_idx": 37}, {"type": "text", "text": "Next, we analyze the case where r E Ut) u(t) ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma_{r}^{(t+1)}=\\gamma_{r}^{(t)}+\\displaystyle\\frac{\\eta}{n m\\tau}\\sum_{i=1}^{n}(1-\\ell_{i}^{\\prime(t)})\\sigma(\\langle\\widetilde{\\mathbf{w}}_{r}^{(t)},y_{i}\\widetilde{\\mu}\\rangle)\\sigma^{\\prime}(\\langle\\mathbf{w}_{r}^{(t)},y_{i}\\mu\\rangle)y_{i}\\Vert\\mu\\Vert_{2}^{2}}\\\\ &{\\quad\\quad-\\displaystyle\\frac{\\eta}{n m\\tau}\\sum_{i=1}^{n}\\sum_{j\\neq i}^{M}\\ell_{i,j}^{\\prime(t)}\\sigma(\\langle\\widetilde{\\mathbf{w}}_{r}^{(t)},y_{j}\\widetilde{\\mu}\\rangle)\\sigma^{\\prime}(\\langle\\mathbf{w}_{r}^{(t)},y_{i}\\mu\\rangle)y_{i}\\Vert\\mu\\Vert_{2}^{2}}\\\\ &{\\quad\\quad=\\gamma_{r}^{(t)}-\\displaystyle\\frac{\\eta}{n m\\tau}\\sum_{i\\neq i=1}^{n}\\sum_{j\\neq i}^{M}\\ell_{i,j}^{\\prime(t)}\\sigma(\\langle\\widetilde{\\mathbf{w}}_{r}^{(t)},y_{j}\\widetilde{\\mu}\\rangle)\\Vert\\mu\\Vert_{2}^{2}<\\gamma_{r}^{(t)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where the second equality follows from $y_{i}\\neq y_{j}$ . Similarly, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{\\gamma}_{r}^{(t+1)}=\\widetilde{\\gamma}_{r}^{(t)}+\\frac{\\eta}{n m\\tau}\\displaystyle\\sum_{i=1}^{n}(1-\\ell_{i}^{\\prime(t)})\\sigma(\\langle\\mathbf{w}_{r}^{(t)},y_{i}\\mu\\rangle)\\sigma^{\\prime}(\\langle\\widetilde{\\mathbf{w}}_{r}^{(t)},y_{i}\\widetilde{\\mu}\\rangle)y_{i}\\|\\widetilde{\\mu}\\|_{2}^{2}}\\\\ &{\\qquad\\quad-\\,\\displaystyle\\frac{\\eta}{n m\\tau}\\displaystyle\\sum_{i=1}^{n}\\sum_{j\\neq i}^{M}\\ell_{i,j}^{\\prime(t)}\\sigma(\\langle\\mathbf{w}_{r}^{(t)},y_{j}\\mu\\rangle)\\sigma^{\\prime}(\\langle\\widetilde{\\mathbf{w}}_{r}^{(t)},y_{i}\\widetilde{\\mu}\\rangle)y_{i}\\|\\widetilde{\\mu}\\|_{2}^{2}}\\\\ &{\\qquad=\\widetilde{\\gamma}_{r}^{(t)}+\\frac{\\eta}{n m\\tau}\\displaystyle\\sum_{i\\y_{i}=-1}^{n}\\sum_{j=1}^{M}\\ell_{i,j}^{\\prime(t)}\\sigma(\\langle\\mathbf{w}_{r}^{(t)},y_{j}\\mu\\rangle)\\|\\widetilde{\\mu}\\|_{2}^{2}>\\widetilde{\\gamma}_{r}^{(t)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Then it is clear that the change of $\\gamma_{r}^{(t)},\\widetilde{\\gamma}_{r}^{(t)}$ does not align with the sign of its initialization. Therefore there exists some time $t^{\\prime}\\ge0$ such that either $r\\,\\in\\mathcal{U}_{+}^{(t^{\\prime})}\\cap\\tilde{\\mathcal{U}}_{+}^{(t^{\\prime})}$ or $r\\,\\in\\mathcal{U}_{-}^{(t^{\\prime})}\\cap\\widetilde{\\mathcal{U}}_{-}^{(t^{\\prime})}$ . We prove this claim by contradiction. Suppose for all $t\\,\\geq\\,0$ $r\\,\\in\\mathcal{U}_{+}^{(t)}\\cap\\widetilde{\\mathcal{U}}_{-}^{(t)}$ , then by (37) and (38), we see $\\gamma_{r}^{(t+1)}\\,\\leq\\,\\gamma_{r}^{(t)}\\,\\leq\\,0$ and $\\widetilde{\\gamma}_{r}^{(t+1}\\,\\geq\\,\\widetilde{\\gamma}_{r}^{(t)}\\,\\geq\\,0$ . Because $\\langle\\mathbf{w}_{r}^{(t)},\\pmb{\\mu}\\rangle\\,\\leq\\,1.01\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\mu}\\rangle+\\gamma_{r}^{(t)}$ and $\\langle\\widetilde{\\mathbf{w}}_{r}^{(t)},\\widetilde{\\pmb{\\mu}}\\rangle\\ge0.99\\langle\\widetilde{\\mathbf{w}}_{r}^{(0)},\\widetilde{\\pmb{\\mu}}\\rangle+\\widetilde{\\gamma}_{r}^{(t)}$ this raise conradietion as ither $\\langle\\mathbf{w}_{r}^{(t)},\\mu\\rangle\\leq0$ 0 $\\langle\\widetilde{\\mathbf{w}}_{r}^{(t)},\\widetilde{\\mu}\\rangle\\ge$ 0. ", "page_idx": 37}, {"type": "text", "text": "With Lemma D.3 at hand, we are ready to demonstrate the lower bound of the growth rate for signal learning. ", "page_idx": 37}, {"type": "text", "text": "Lemma D.4. With the same condition as in Lemma C.2 and Lemma $C.3$ and $n\\geq2500\\log(4/\\delta)$ defne $A_{r}^{(t)}=\\gamma_{r}^{(t)}+\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\mu}\\rangle$ $r\\in\\mathcal{U}_{+}^{(0)}$ ad $A_{r}^{(t)}=-\\gamma_{r}^{(t)}-\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\mu}\\rangle$ $r\\in\\mathcal{U}_{-}^{(0)}$ we define $\\widetilde{A}_{r}^{(t)}=\\widetilde{\\gamma}_{r}^{(t)}+\\langle\\widetilde{\\mathbf{w}}_{r}^{(0)},\\widetilde{\\mu}\\rangle$ for $r\\in\\widetilde{\\mathcal{U}}_{+}^{(0)}$ and $\\widetilde{A}_{r}^{(t)}=-\\widetilde{\\gamma}_{r}^{(t)}-\\langle\\widetilde{\\mathbf{w}}_{r}^{(0)},\\widetilde{\\mu}\\rangle$ for $r\\in\\smash{\\widetilde{\\mathcal{U}}_{-}^{(0)}}$ . Then withprobability atleast $1-\\delta$ wehave ", "page_idx": 37}, {"type": "equation", "text": "$$\nA_{r}^{(t)}\\geq\\Big(1+\\frac{0.48\\eta\\|\\pmb{\\mu}\\|_{2}^{2}C_{\\mu}}{m\\tau}\\Big)^{t}(A_{r}^{(0)}+\\widetilde{A}_{r}^{(0)}/C_{\\mu})-1\n$$", "text_format": "latex", "page_idx": 37}, {"type": "equation", "text": "$$\n\\widetilde{A}_{r}^{(t)}\\geq\\Big(1+\\frac{0.48\\eta\\|\\pmb{\\mu}\\|_{2}^{2}C_{\\mu}}{m\\tau}\\Big)^{t}(C_{\\mu}A_{r}^{(0)}+\\widetilde{A}_{r}^{(0)})-1.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "ProfWiut lf generalty, wecn $r\\in\\mathcal{U}_{+}^{(0)}\\cap\\widetilde{\\mathcal{U}}_{+}^{(0)}$ Then by the update of $\\gamma_{r}^{(t)},\\widetilde{\\gamma}_{r}^{(t)}$ we have from (35) and (36) ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\gamma_{r}^{(t+1)}=\\gamma_{r}^{(t)}+\\frac{\\eta}{n m\\tau}\\sum_{i:y_{i}=1}(1-\\ell_{i}^{\\prime(t)})\\sigma(\\langle\\widetilde{\\mathbf{w}}_{r}^{(t)},\\widetilde{\\pmb{\\mu}}\\rangle)\\|\\pmb{\\mu}\\|_{2}^{2}}\\\\ {\\displaystyle\\widetilde{\\gamma}_{r}^{(t+1)}=\\widetilde{\\gamma}_{r}^{(t)}+\\frac{\\eta}{n m\\tau}\\sum_{i:y_{i}=1}(1-\\ell_{i}^{\\prime(t)})\\sigma(\\langle\\mathbf{w}_{r}^{(t)},\\pmb{\\mu}\\rangle)\\|\\widetilde{\\pmb{\\mu}}\\|_{2}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "To achieve the lower bound, we define ", "page_idx": 38}, {"type": "equation", "text": "$$\nA_{r}^{(t)}\\triangleq\\gamma_{r}^{(t)}+\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\mu}\\rangle,\\quad\\widetilde{A}_{r}^{(t)}=\\widetilde{\\gamma}_{r}^{(t)}+\\langle\\widetilde{\\mathbf{w}}_{r}^{(0)},\\widetilde{\\pmb{\\mu}}\\rangle.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "By (39), we have the lower bound of update equation, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{A_{r}^{(t+1)}=A_{r}^{(t)}+\\displaystyle\\frac{\\eta}{n m\\tau}\\sum_{i:y_{i}=1}(1-\\ell_{i}^{\\prime(t)})\\sigma(\\langle\\widetilde{\\mathbf{w}}_{r}^{(t)},\\widetilde{\\pmb{\\mu}}\\rangle)\\|\\pmb{\\mu}\\|_{2}^{2}}}\\\\ {{\\ge A_{r}^{(t)}+\\displaystyle\\frac{\\eta}{n m\\tau}(\\frac{n}{2}-O(\\sqrt{n}))(1-\\frac{C_{\\ell}}{1+M})\\widetilde{A}_{r}^{(t)}\\|\\pmb{\\mu}\\|_{2}^{2}}}\\\\ {{\\ge A_{r}^{(t)}+\\displaystyle\\frac{0.48\\eta\\|\\pmb{\\mu}\\|_{2}^{2}}{m\\tau}\\widetilde{A}_{r}^{(t)}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "The inequality is by Lemma B.3, Lemma D.1 where we recall $\\begin{array}{r}{c_{2}=1-\\frac{C\\ell}{M+1}}\\end{array}$ N . Similarly b (40) we arrive at ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\widetilde{A}_{r}^{(t+1)}\\ge\\widetilde{A}_{r}^{(t)}+\\frac{0.48\\eta\\|\\widetilde{\\pmb{\\mu}}\\|_{2}^{2}}{m\\tau}A_{r}^{(t)}=\\widetilde{A}_{r}^{(t)}+\\frac{0.48\\eta C_{\\mu}^{2}\\|\\pmb{\\mu}\\|_{2}^{2}}{m\\tau}A_{r}^{(t)}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Then by combining the above two inequalities, we conclude that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C_{\\mu}A_{r}^{(t+1)}+\\widetilde A_{r}^{(t+1)}=C_{\\mu}A_{r}^{(t)}+\\widetilde A_{r}^{(t)}+\\frac{0.48\\eta\\|\\pmb{\\mu}\\|_{2}^{2}}{m\\tau}(C_{\\mu}\\widetilde A_{r}^{(t)}+C_{\\mu}^{2}A_{r}^{(t)})}\\\\ &{\\qquad\\qquad\\qquad\\geq\\Big(1+\\frac{0.48\\eta\\|\\pmb{\\mu}\\|_{2}^{2}C_{\\mu}}{m\\tau}\\Big)(C_{\\mu}A_{r}^{(t)}+\\widetilde A_{r}^{(t)}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Thus, we seethe joint dynamics of $\\gamma_{r}^{\\left(t\\right)}$ and $\\widetilde{\\gamma}_{r}^{(t)}$ exhibits exponential growth,i.e., ", "page_idx": 38}, {"type": "equation", "text": "$$\nC_{\\mu}A_{r}^{(t)}+\\widetilde{A}_{r}^{(t)}\\geq\\Big(1+\\frac{0.48\\eta\\Vert\\pmb{\\mu}\\Vert_{2}^{2}C_{\\mu}}{m\\tau}\\Big)^{t}(A_{r}^{(0)}+\\widetilde{A}_{r}^{(0)}).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "To characterize the dynamics of $\\gamma_{r}^{\\left(t\\right)}$ individuall, we next track the dynamics of $C_{\\mu}A_{r}^{(t)}-\\widetilde A_{r}^{(t)}$ by subtracting (40) from (39), which gives ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma_{\\mu}A_{r}^{(t+1)}-\\widetilde A_{r}^{(t+1)}=C_{\\mu}A_{r}^{(t)}-\\widetilde A_{r}^{(t)}+\\frac{\\eta\\|\\mu\\|_{2}^{2}}{n m\\tau}\\sum_{i:y_{i}=1}(1-\\ell_{i}^{\\prime(t)})\\big(C_{\\mu}\\sigma(\\langle\\mathbf v_{r}^{(t)},\\widetilde\\mu\\rangle)-C_{\\mu}^{2}\\sigma(\\langle\\mathbf w_{r}^{(t)},\\mu\\rangle)\\big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geq\\Big(1-\\frac{\\eta C_{\\mu}\\|\\mu\\|_{2}^{2}}{n m\\tau}\\sum_{i:y_{i}=1}(1-\\ell_{i}^{\\prime(t)})\\Big)\\big(C_{\\mu}A_{r}^{(t)}-\\widetilde A_{r}^{(t)}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Then from the the propagation, we notice that the gap $C_{\\mu}A_{r}^{(t)}-\\widetilde A_{r}^{(t)}$ exhibits exponential decay regardless of the sign. Thus, we have ", "page_idx": 38}, {"type": "equation", "text": "$$\nC_{\\mu}A_{r}^{(t)}-\\widetilde{A}_{r}^{(t)}\\geq\\Big(1-\\frac{\\eta C_{\\mu}\\|\\mu\\|_{2}^{2}}{n m\\tau}\\sum_{i:y_{i}=1}(1-\\ell_{i}^{\\prime(t)})\\Big)^{t}\\big(C_{\\mu}A_{r}^{(0)}-\\widetilde{A}_{r}^{(0)}\\big).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Therefore, combining (41) and (42) yields: ", "page_idx": 38}, {"type": "equation", "text": "$$\nC_{\\mu}A_{r}^{(t)}+\\widetilde{A}_{r}^{(t)}\\geq\\Big(1+\\frac{0.48\\eta\\|\\pmb{\\mu}\\|_{2}^{2}C_{\\mu}}{m\\tau}\\Big)^{t}(C_{\\mu}A_{r}^{(0)}+\\widetilde{A}_{r}^{(0)}),\n$$", "text_format": "latex", "page_idx": 38}, {"type": "equation", "text": "$$\nC_{\\mu}A_{r}^{(t)}-\\widetilde{A}_{r}^{(t)}\\geq\\Big(1-\\frac{\\eta\\|\\mu\\|_{2}^{2}}{n m\\tau}\\sum_{\\substack{i:y_{i}=1}}(1-\\ell_{i}^{\\prime(t)})\\Big)^{t}\\big(C_{\\mu}A_{r}^{(0)}-\\widetilde{A}_{r}^{(0)}\\big).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Then it concludes that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{4_{r}^{(t)}\\geq\\Big(1+\\displaystyle\\frac{0.48\\eta\\|\\mu\\|_{2}^{2}C_{\\mu}}{m\\tau}\\Big)^{t}(A_{r}^{(0)}+\\widetilde{A}_{r}^{(0)}/C_{\\mu})+\\Big(1-\\displaystyle\\frac{\\eta\\|\\mu\\|_{2}^{2}}{n m\\tau}\\sum_{i:y_{i}=1}(1-\\ell_{i}^{\\prime(t)})\\Big)^{t}\\big(A_{r}^{(0)}-\\widetilde{A}_{r}^{(0)}/C_{\\mu}\\big)}\\\\ &{\\qquad\\geq\\Big(1+\\displaystyle\\frac{0.48\\eta\\|\\mu\\|_{2}^{2}C_{\\mu}}{m\\tau}\\Big)^{t}(A_{r}^{(0)}+\\widetilde{A}_{r}^{(0)}/C_{\\mu})-|A_{r}^{(0)}-\\widetilde{A}_{r}^{(0)}/C_{\\mu}|,}\\\\ &{\\widetilde{4}_{r}^{(t)}\\geq\\Big(1+\\displaystyle\\frac{0.48\\eta\\|\\mu\\|_{2}^{2}C_{\\mu}}{m\\tau}\\Big)^{t}(C_{\\mu}A_{r}^{(0)}+\\widetilde{A}_{r}^{(0)})-\\Big(1-\\displaystyle\\frac{\\eta\\|\\mu\\|_{2}^{2}}{n m\\tau}\\sum_{i:y_{i}=1}(1-\\ell_{i}^{\\prime(t)})\\Big)^{t}\\big(C_{\\mu}A_{r}^{(0)}-\\widetilde{A}_{r}^{(0)}\\big)}\\\\ &{\\qquad\\geq\\Big(1+\\displaystyle\\frac{0.48\\eta\\|\\mu\\|_{2}^{2}C_{\\mu}}{m\\tau}\\Big)^{t}(C_{\\mu}A_{r}^{(0)}+\\widetilde{A}_{r}^{(0)})-|C_{\\mu}A_{r}^{(0)}-\\widetilde{A}_{r}^{(0)}|}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "$\\begin{array}{r}{\\big(1-\\frac{\\eta\\|\\pmb{\\mu}\\|_{2}^{2}}{n m\\tau}\\sum_{i:y_{i}=1}(1-\\ell_{i}^{\\prime(t)})\\big)^{t}\\leq1}\\end{array}$ Next,we deriveanupe oundon $|A_{r}^{(0)}-\\widetilde{A}_{r}^{(0)}/C_{\\mu}|,|C_{\\mu}A_{r}^{(0)}-\\widetilde{A}_{r}^{(0)}|$ as follows. ", "page_idx": 39}, {"type": "equation", "text": "$$\n|A_{r}^{(0)}-\\widetilde{A}_{r}^{(0)}/C_{\\mu}|\\le|A_{r}^{(0)}|+|\\widetilde{A}_{r}^{(0)}|/C_{\\mu}\\le2\\sqrt{2\\log(8m/\\delta)}\\sigma_{0}\\|\\mu\\|_{2}\\le1\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where we recall that $C_{\\mu}\\|\\pmb{\\mu}\\|_{2}\\;=\\;\\|\\widetilde{\\pmb{\\mu}}\\|_{2}$ and the second inequality is by the condition on $\\sigma_{0}~\\leq$ $0.5(2\\log(8m/\\delta))^{-1/2}\\|\\pmb{\\mu}\\|_{2}^{-1}=\\widetilde O(\\|\\pmb{\\mu}\\|_{2}^{-1})$ . Similarly, we can show $|C_{\\mu}A_{r}^{(0)}-\\widetilde{A}_{r}^{(0)}|\\leq1$ and thus weobtain ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{A_{r}^{(t)}\\geq\\left(1+\\frac{0.48\\eta\\|\\pmb{\\mu}\\|_{2}^{2}C_{\\mu}}{m\\tau}\\right)^{t}(A_{r}^{(0)}+\\widetilde{A}_{r}^{(0)}/C_{\\mu})-1}}\\\\ {{\\widetilde{A}_{r}^{(t)}\\geq\\left(1+\\frac{0.48\\eta\\|\\pmb{\\mu}\\|_{2}^{2}C_{\\mu}}{m\\tau}\\right)^{t}(C_{\\mu}A_{r}^{(0)}+\\widetilde{A}_{r}^{(0)})-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "D.1.2  Dynamics of Noise Memorization: Upper Bound ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "In orde o characterize the growth of $\\rho_{r,i}^{(t)},\\widetilde{\\rho}_{r,i}^{(t)}$ we partiton the amles accrding toits signofier product. ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{Z}_{r,+}^{(t)}=\\{i\\in[n]:\\langle{\\mathbf w}_{r}^{(t)},\\pmb{\\xi}_{i}\\rangle>0\\},\\quad\\widetilde{\\mathbb{Z}}_{r,+}^{(t)}=\\{i\\in[n]:\\langle{\\mathbf w}_{r}^{(0)},\\widetilde{\\pmb{\\xi}}_{i}\\rangle>0\\},}\\\\ {\\mathbb{Z}_{r,-}^{(t)}=\\{i\\in[n]:\\langle{\\mathbf w}_{r}^{(t)},\\pmb{\\xi}_{i}\\rangle<0\\},\\quad\\widetilde{\\mathbb{Z}}_{r,-}^{(t)}=\\{i\\in[n]:\\langle{\\mathbf w}_{r}^{(0)},\\widetilde{\\pmb{\\xi}}_{i}\\rangle<0\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "We further define ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B_{r,+,+}^{(t)}\\triangleq\\displaystyle\\sum_{i:y_{i}=1}^{\\Delta}\\big(\\rho_{r,i}^{(t)}+\\langle\\mathbf{w}_{r}^{(0)},\\xi_{i}\\rangle\\big)\\mathbb{1}_{i\\in\\mathcal{X}_{r,+}^{(t)}\\cap\\widetilde{\\mathcal{L}}_{r,+}^{(t)}},}\\\\ &{B_{r,+,-}^{(t)}\\triangleq\\displaystyle\\sum_{i:y_{i}=1}^{\\Delta}\\big(\\rho_{r,i}^{(t)}+\\langle\\mathbf{w}_{r}^{(0)},\\xi_{i}\\rangle\\big)\\mathbb{1}_{i\\in\\mathcal{X}_{r,+}^{(t)}\\cap\\widetilde{\\mathcal{L}}_{r,-}^{(t)}},}\\\\ &{B_{r,-,+}^{(t)}\\triangleq\\displaystyle\\sum_{i:y_{i}=-1}^{\\Delta}\\big(\\rho_{r,i}^{(t)}+\\langle\\mathbf{w}_{r}^{(0)},\\xi_{i}\\rangle\\big)\\mathbb{1}_{i\\in\\mathcal{X}_{r,+}^{(t)}\\cap\\widetilde{\\mathcal{L}}_{r,+}^{(t)}}^{(t)},}\\\\ &{B_{r,-,-}^{(t)}\\triangleq\\displaystyle\\sum_{i:y_{i}=-1}^{\\Delta}\\big(\\rho_{r,i}^{(t)}+\\langle\\mathbf{w}_{r}^{(0)},\\xi_{i}\\rangle\\big)\\mathbb{1}_{i\\in\\mathcal{X}_{r,+}^{(t)}\\cap\\widetilde{\\mathcal{L}}_{r,-}^{(t)}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "On the other hand, we define ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{B}_{r,+,+}^{(t)}\\triangleq\\displaystyle\\sum_{i:y_{i}=1}\\big(\\widetilde{\\rho}_{r,i}^{(t)}+\\langle\\widetilde{\\mathbf{w}}_{r}^{(0)},\\widetilde{\\xi}_{i}\\rangle\\big)\\mathbb{1}_{i\\in\\widetilde{\\mathcal{Z}}_{r,+}^{(t)}\\cap\\mathbb{Z}_{r,+}^{(t)}},}\\\\ &{\\widetilde{B}_{r,+,-}^{(t)}\\triangleq\\displaystyle\\sum_{i:y_{i}=1}\\big(\\widetilde{\\rho}_{r,i}^{(t)}+\\langle\\widetilde{\\mathbf{w}}_{r}^{(0)},\\widetilde{\\xi}_{i}\\rangle\\big)\\mathbb{1}_{i\\in\\widetilde{\\mathcal{Z}}_{r,+}^{(t)}\\cap\\mathbb{Z}_{r,-}^{(t)}},}\\\\ &{\\widetilde{B}_{r,-,+}^{(t)}\\triangleq\\displaystyle\\sum_{i:y_{i}=-1}\\big(\\widetilde{\\rho}_{r,i}^{(t)}+\\langle\\widetilde{\\mathbf{w}}_{r}^{(0)},\\widetilde{\\xi}_{i}\\rangle\\big)\\mathbb{1}_{i\\in\\widetilde{\\mathcal{Z}}_{r,+}^{(t)}\\cap\\mathbb{Z}_{r,+}^{(t)}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "equation", "text": "$$\n\\widetilde{B}_{r,-,-}^{(t)}\\triangleq\\sum_{i:y_{i}=-1}(\\widetilde{\\rho}_{r,i}^{(t)}+\\langle\\widetilde{\\mathbf{w}}_{r}^{(0)},\\widetilde{\\mathbf{\\xi}}_{i}\\rangle)\\mathbb{1}_{i\\in\\widetilde{\\mathcal{Z}}_{r,+}^{(t)}\\cap\\mathbb{Z}_{r,-}^{(t)}}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Before we state the main result, we provide some useful lemmas. ", "page_idx": 40}, {"type": "text", "text": "Lemma D.5. Suppose $\\delta>0$ the with probability at least $1-\\delta$ ,wehave ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B_{r,+,+}^{(0)}\\leq\\displaystyle\\frac{1}{2}\\sigma_{0}\\sigma_{\\xi}\\sqrt{d n}(1+\\sqrt{\\log(1/\\delta)/2}),\\quad B_{r,-,+}^{(0)}\\leq\\displaystyle\\frac{1}{2}\\sigma_{0}\\sigma_{\\xi}\\sqrt{d n}(1+\\sqrt{\\log(1/\\delta)/2}).}\\\\ &{\\widetilde{B}_{r,+,+}^{(0)}\\leq\\displaystyle\\frac{1}{2}\\sigma_{0}\\sigma_{\\widetilde{\\xi}}\\sqrt{d n}(1+\\sqrt{\\log(1/\\delta)/2}),\\quad\\widetilde{B}_{r,-,+}^{(0)}\\leq\\displaystyle\\frac{1}{2}\\sigma_{0}\\sigma_{\\widetilde{\\xi}}\\sqrt{d n}(1+\\sqrt{\\log(1/\\delta)/2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Proof. By Bernstein's inequality, for arbitrary $t>0$ ,wehave ", "page_idx": 40}, {"type": "equation", "text": "$$\nP(|B_{r,+,+}^{(0)}-\\frac{1}{2}\\sigma_{0}\\sigma_{\\xi}\\sqrt{n d}|>t)\\leq\\exp(-\\frac{t^{2}}{2n/4\\sigma_{0}^{2}\\sigma_{\\xi}^{2}d}).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Setting $\\begin{array}{r}{t=\\frac{1}{2}\\sigma_{0}\\sigma_{\\xi}\\sqrt{n d\\log(1/\\delta)/2}}\\end{array}$ , we further have ", "page_idx": 40}, {"type": "equation", "text": "$$\nB_{r,+}^{(0)}\\leq\\frac{1}{2}\\sigma_{0}\\sigma_{\\xi}\\sqrt{d n}(1+\\sqrt{\\log(1/\\delta)/2}).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Similarly, the same result holds for $B_{r,-}^{(0)}$ ", "page_idx": 40}, {"type": "text", "text": "Lemma D.6. Under the condition /g(1((8/m)),thenwithprobabilya last  , it satisfies ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\widetilde{B}_{r,+,+}^{(0)}>150\\sqrt{\\frac{\\log(6n^{2}/\\delta)}{d}}n^{2},\\quad B_{r,+,+}^{(0)}>150\\sqrt{\\frac{\\log(6n^{2}/\\delta)}{d}}n^{2}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Proof of Lemma D.6. Here we want to show B(), $\\begin{array}{r}{\\widetilde{B}_{r,+,+}^{(0)}\\geq150\\sqrt{\\frac{\\log\\left(6n^{2}/\\delta\\right)}{d}}n^{2}}\\end{array}$ with high probability. To see this, because $\\widetilde{B}_{r,+,+}^{(0)}$ is a random variable with positive mean and variance $\\sigma_{0}^{2}\\sigma_{\\xi}^{2}d n/4$ , by Lemma B.1, we compute ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathbb{P}\\big(\\widetilde{B}_{r,+,+}^{(0)}\\leq t\\big)\\leq\\sqrt{1-\\exp\\big(-\\frac{8t^{2}}{\\pi\\sigma_{0}^{2}\\sigma_{\\xi}^{2}d n}\\big)},\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Thus when $\\begin{array}{r}{d\\geq\\frac{300\\sqrt{2n^{3}\\log\\left(6n^{2}/\\delta\\right)}}{\\sigma_{0}\\sigma_{\\xi}\\sqrt{\\pi}\\log\\left(1/\\left(1-(\\delta/m)^{2}\\right)\\right)}}\\end{array}$ we have $\\begin{array}{r}{\\mathbb{P}\\!\\left(\\widetilde{B}_{r,+,+}^{(0)}\\leq150\\sqrt{\\frac{\\log\\left(6n^{2}/\\delta\\right)}{d}}n^{2}\\right)\\leq\\delta/m}\\end{array}$ union bound we have with proability at least $1-\\delta$ it holds $\\begin{array}{r}{\\widetilde{B}_{r,+,+}^{(0)}>150\\sqrt{\\frac{\\log\\left(6n^{2}/\\delta\\right)}{d}}n^{2}}\\end{array}$ \u53e3 ", "page_idx": 40}, {"type": "text", "text": "Lemma D.7. Under the condion $\\begin{array}{r}{n\\,\\geq\\,\\frac{8\\log(1/\\delta)}{\\log(1/(1-(\\delta/m)^{2}))}}\\end{array}$ thenwith probabilir an least $1-\\delta$ it satisfies ", "page_idx": 40}, {"type": "equation", "text": "$$\n|\\langle\\mathbf{w}_{r}^{(0)},\\boldsymbol{\\xi}_{i}\\rangle|>\\frac{B_{r,+,+}^{(0)}}{n},\\quad|\\langle\\widetilde{\\mathbf{w}}_{r}^{(0)},\\widetilde{\\boldsymbol{\\xi}}_{i}\\rangle|>\\frac{\\widetilde{B}_{r,+,+}^{(0)}}{n}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Proof of Lemma $D.7.$ Here we want to show $\\begin{array}{r}{|\\langle\\mathbf{w}_{r}^{(0)},\\boldsymbol{\\xi}_{i}\\rangle|>\\frac{B_{r,+,+}^{(0)}}{n}}\\end{array}$ with high probability. To see this, because $\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle$ is a random variable with positive mean and variance $\\sigma_{0}^{2}\\sigma_{\\xi}^{2}d$ Besides,by Lemma C.9, we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\frac{B_{r,+,+}^{(0)}}{n}\\leq\\frac{\\sigma_{0}\\sigma_{\\xi}\\sqrt{d n}(1+\\sqrt{\\log(1/\\delta)/2})}{n}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "By Lemma B.1, we compute ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathbb{P}\\big(|\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle|\\leq t\\big)\\leq\\sqrt{1-\\exp\\big(-\\frac{8t^{2}}{\\pi\\sigma_{0}^{2}\\sigma_{\\xi}^{2}d}\\big)},\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Thus when $\\begin{array}{r}{n\\geq\\frac{8\\log(1/\\delta)}{\\log(1/(1-(\\delta/m)^{2}))}}\\end{array}$ we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbb{P}\\big(|\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle|\\leq\\frac{\\sigma_{0}\\sigma_{\\xi}\\sqrt{d n}(1+\\sqrt{\\log(1/\\delta)/2})}{n}\\big)\\leq\\delta/m\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "and by union bound, we have with probability at least $1-\\delta$ , it holds $\\begin{array}{r}{|\\langle\\mathbf{w}_{r}^{(0)},\\boldsymbol{\\xi}_{i}\\rangle|>\\frac{B_{r,+,+}^{(0)}}{n}}\\end{array}$ ", "page_idx": 41}, {"type": "text", "text": "Similarly, we can conclude that $\\begin{array}{r}{|\\langle\\widetilde{\\mathbf{w}}_{r}^{(0)},\\widetilde{\\pmb{\\xi}}_{i}\\rangle|>\\frac{\\widetilde{B}_{r,+,+}^{(0)}}{n}}\\end{array}$ ", "page_idx": 41}, {"type": "text", "text": "Lemma D.8. Under Assumption 4.1, with probability at least $1-\\delta$ wehave ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B_{r,+,+}^{(t)}\\leq(1+\\frac{1.05\\eta}{n m\\tau})^{t}B_{r,+,+}^{(0)},\\quad B_{r,+,-}^{(t)}\\leq B_{r,+,-}^{(0)},}\\\\ &{B_{r,-,+}^{(t)}\\leq(1+\\frac{1.05\\eta}{n m\\tau})^{t}B_{r,-,+}^{(0)},\\quad B_{r,-,-}^{(t)}\\leq B_{r,-,-}^{(0)},}\\\\ &{\\widetilde{B}_{r,+,+}^{(t)}\\leq(1+\\frac{1.05\\eta}{n m\\tau})^{t}\\widetilde{B}_{r,+,+}^{(0)},\\quad\\widetilde{B}_{r,+,-}^{(t)}\\leq\\widetilde{B}_{r,+,-}^{(0)},}\\\\ &{\\widetilde{B}_{r,-,+}^{(t)}\\leq(1+\\frac{1.05\\eta}{n m\\tau})^{t}\\widetilde{B}_{r,-,+}^{(0)},\\quad\\widetilde{B}_{r,-,-}^{(t)}\\leq\\widetilde{B}_{r,-,-}^{(0)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Proof of Lemma D8. According to the iterationquations for $\\rho_{r,i}^{(t)}$ and $\\widetilde{\\rho}_{r,i}^{(t)}$ we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=B_{r,\\epsilon_{1}+}^{(\\epsilon_{1})}+\\frac{\\eta}{m m^{2}}\\sum_{\\tau_{1}=1}^{\\infty}\\bigg[(1-\\ell_{1}^{(\\epsilon_{1})})(\\tilde{\\omega}_{r}^{(\\epsilon_{1})},\\tilde{\\xi}_{1})-\\sum_{\\beta\\geq m^{1}}\\ell_{\\eta}^{(\\epsilon_{1})}(\\tilde{\\omega}_{r}^{(\\epsilon_{1})},\\tilde{\\xi}_{1})\\bigg]_{t\\in\\mathbb{Z}_{r,\\epsilon_{1}}}\\bigg]_{1\\in\\mathbb{Z}_{r,\\epsilon_{1}}}\\bigg[\\epsilon_{1\\in\\tilde{\\mathcal{X}}_{m^{1}}}\\sqrt{\\sum_{\\tilde{\\xi}_{1}=\\tilde{\\mathcal{X}}_{m^{1}}}\\left[\\xi_{1}(M_{\\epsilon})\\right]^{2}}}\\\\ &{\\leq B_{r,\\epsilon_{1}+}^{(\\epsilon_{1})}+\\frac{\\eta}{m m^{2}}\\sum_{\\tau_{1}=1}^{1}\\mathbb{I}_{\\{\\xi_{2},\\epsilon_{1}\\}}\\mathbb{I}_{\\nu_{\\tilde{\\xi}_{2},\\epsilon_{1}}}\\bigg[\\mathbb{Z}_{C}(M+1)-1}\\\\ &{\\quad-\\sum_{\\eta\\geq m^{1}}\\frac{1}{C}\\frac{1}{\\mathcal{Z}_{r}(M+1)}(\\tilde{\\omega}_{r}^{(\\epsilon_{1})},\\tilde{\\xi}_{1})+\\tilde{\\mu}_{r_{1}}^{(\\epsilon_{1})}-\\phi\\sqrt{\\frac{\\log(\\theta)^{2}(\\tilde{\\xi}_{2})}{d}}\\eta\\mathbb{I}_{\\nu_{\\tilde{\\xi}_{2},\\epsilon_{1}}}\\bigg]_{\\mathbb{I}_{\\{\\phi\\}_{r,\\epsilon_{1}}}}\\bigg(\\sigma_{\\tilde{\\xi}_{2}}^{2}+\\sigma_{\\xi}^{2}\\sqrt{\\operatorname*{dim}(\\theta)^{2}(\\tilde{\\xi}_{2})}}\\\\ &{\\leq B_{r,\\epsilon_{1}+}^{(\\epsilon_{1})}+\\frac{\\eta}{m\\tau}\\left[\\mathbb{Z}_{C}(M+1)-1\\right](\\tilde{\\Delta}_{r,\\epsilon_{1}+}^{(\\epsilon_{1})}+\\sum_{\\eta\\leq m^{1}}\\mathbb{I}_{\\{\\\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where the first inequality is by Lemma C.2, Lemma C.1 and Lemma B.5. Furthermore, the second inequality is by Lemma C.8 and the definition of r,-,+ and B(t) $\\widetilde{B}_{r,-,-}^{(t)}$ . The third inequality is by Lemma D.6, B(0), $\\begin{array}{r}{\\widetilde{B}_{r,+,+}^{(0)}>150\\sqrt{\\frac{\\log\\left(6n^{2}/\\delta\\right)}{d}}n^{2}}\\end{array}$ $d>10000\\log(6n^{2}/\\delta)$ , and $n\\geq1280000\\log(4/\\delta)$ .The last inequality is by choosing $C_{\\ell}=1.01$   \nNext, we establish the following inequality ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{3_{r,+,-}^{(t+1)}=B_{r,+,-}^{(t)}-\\displaystyle\\frac{\\eta}{n m\\tau}\\sum_{i:y_{i}=1}\\left[\\sum_{j:y_{j}=-1}\\ell_{i,j}^{\\prime(t)}\\langle\\widetilde{\\mathbf{w}}_{r}^{(t)},\\widetilde{\\xi}_{j}\\rangle\\mathbf{1}_{j\\in\\widetilde{\\mathbb{Z}}_{r,+}}\\right]\\mathbf{1}_{i\\in\\mathbb{Z}_{r,+}\\cap\\widetilde{\\mathbb{Z}}_{r,-}}\\|\\xi_{i}\\|_{2}^{2}}\\\\ &{\\qquad\\le B_{r,+,-}^{(t)}-\\displaystyle\\frac{\\eta}{n m\\tau}\\sum_{i:y_{i}=1}\\mathbf{1}_{i\\in\\mathbb{Z}_{r,+}\\cap\\widetilde{\\mathbb{Z}}_{r,+}}\\left[\\sum_{j:y_{j}=-1}\\displaystyle\\frac{1}{C_{\\ell}(M+1)}(\\langle\\widetilde{\\mathbf{w}}_{r}^{(0)},\\widetilde{\\xi}_{j}\\rangle+\\widetilde{\\rho}_{r,j}^{(t)}-6\\sqrt{\\frac{\\log(6n^{2}/\\delta)}{d}}\\right]\\right.}\\\\ &{\\qquad\\left.(\\sigma_{\\xi}^{2}d-\\sigma_{\\xi}^{2}\\sqrt{d\\log(6n^{2}/\\delta)})\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq B_{r,+,-}^{(t)}-\\displaystyle\\frac{\\eta}{n\\tau m}\\sum_{i:y_{i}=1}\\left[\\mathbb{1}_{i\\in\\mathcal{T}_{r,+}\\cap\\tilde{\\mathcal{Z}}_{r,+}}\\frac{1}{C_{\\ell}(M+1)}(\\tilde{B}_{r,-,+}^{(t)}+\\tilde{B}_{r,-,-}^{(t)}-\\displaystyle\\sum_{j\\neq i}\\theta\\sqrt{\\frac{\\log(6n^{2}/\\delta)}{d}}n)\\right]}\\\\ &{\\ \\ (\\sigma_{\\xi}^{2}d-\\sigma_{\\xi}^{2}\\sqrt{d\\log(6n^{2}/\\delta)})}\\\\ &{\\leq B_{r,+,-}^{(t)}-\\displaystyle\\frac{\\eta}{n m\\tau}\\Big[\\frac{0.99^{2}}{4C_{l}}(\\tilde{B}_{r,-,+}^{(t)}+\\tilde{B}_{r,-,-}^{(t)})\\Big]0.99\\sigma_{\\xi}^{2}d}\\\\ &{\\leq B_{r,+,-}^{(t)}-\\displaystyle\\frac{\\eta}{n m\\tau}\\Big[0.24(\\tilde{B}_{r,-,+}^{(t)}+\\tilde{B}_{r,-,-}^{(t)})\\Big]\\sigma_{\\xi}^{2}d,}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where the first inequality is by Lemma C.2, Lemma C.1 and Lemma B.5. Furthermore, the second inequality is byLmmaC8 and tedeniton f $\\widetilde{B}_{r,-,+}^{(t)}$ and $\\widetilde{B}_{r,-,-}^{(t)}$ . The third nequalit is by Lemma D.6 and B(0). $\\begin{array}{r}{\\widetilde{B}_{r,+,+}^{(0)}>150\\sqrt{\\frac{\\log\\left(6n^{2}/\\delta\\right)}{d}}n^{2}}\\end{array}$ log(6n 2/0) n2,d > 10000l0g(6n2 / 8), and n \u2265 1280000l0og(4/ 8). Similarly, we have, ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B_{r,-,+}^{(t+1)}\\leq B_{r,-,+}^{(t)}+\\frac{\\eta}{n m\\tau}\\bigg[1.05B_{r,-,+}^{(t)}-0.25\\big(\\widetilde{B}_{r,+,+}^{(t)}+\\widetilde{B}_{r,+,-}^{(t)}\\big)\\bigg]\\sigma_{\\xi}^{2}d,}\\\\ &{B_{r,-,-}^{(t+1)}\\leq B_{r,-,-}^{(t)}-\\frac{\\eta}{n m\\tau}\\bigg[0.24(\\widetilde{B}_{r,+,+}^{(t)}+\\widetilde{B}_{r,+,-}^{(t)})\\bigg]\\sigma_{\\xi}^{2}d}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "For the second modality, with the same derivative, we could obtain the following inequalities: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{B}_{r,+,+}^{(t+1)}\\leq\\widetilde{B}_{r,+,+}^{(t)}+\\frac{\\eta}{n m\\tau}\\bigg[1.05\\widetilde{B}_{r,+,+}^{(t+1)}-0.25(B_{r,-,+}^{(t)}+B_{r,-,-}^{(t)})\\bigg]\\sigma_{\\xi}^{2}d,}\\\\ &{\\widetilde{B}_{r,+,-}^{(t+1)}\\leq\\widetilde{B}_{r,+,+}^{(t)}-\\frac{\\eta}{n m\\tau}\\bigg[0.24(B_{r,-,+}^{(t)}+B_{r,-,-}^{(t)})\\bigg]\\sigma_{\\xi}^{2}d,}\\\\ &{\\widetilde{B}_{r,-,+}^{(t+1)}\\leq\\widetilde{B}_{r,-,+}^{(t)}+\\frac{\\eta}{n m\\tau}\\bigg[1.05\\widetilde{B}_{r,-,+}^{(t+1)}-0.25(B_{r,+,+}^{(t)}+B_{r,+,-}^{(t)})\\bigg]\\sigma_{\\xi}^{2}d,}\\\\ &{\\widetilde{B}_{r,-,+}^{(t+1)}\\leq\\widetilde{B}_{r,-,-}^{(t)}-\\frac{\\eta}{n m\\tau}\\bigg[0.24(B_{r,+,+}^{(t)}+B_{r,+,-}^{(t)})\\bigg]\\sigma_{\\xi}^{2}d,}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "which completes the proof. ", "page_idx": 42}, {"type": "text", "text": "We define ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Psi_{r,i}^{(t)}\\triangleq\\rho_{r,i}^{(t)}+\\langle\\mathbf{w}_{r}^{(0)},\\boldsymbol{\\xi}_{i}\\rangle,i\\in\\mathcal{Z}_{r,+}^{(t)}}\\\\ {\\widetilde{\\Psi}_{r,i}^{(t)}\\triangleq\\rho_{r,i}^{(t)}+\\langle\\widetilde{\\mathbf{w}}_{r}^{(0)},\\widetilde{\\boldsymbol{\\xi}}_{i}\\rangle,i\\in\\widetilde{\\mathcal{Z}}_{r,+}^{(t)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Lemma D.9. Under Assumption 4.1, with probability at least $1-\\delta,$ forall $0\\leq t\\leq T_{1}$ ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Psi_{r,i}^{(t)}\\leq(1+\\frac{1.06\\eta\\sigma_{\\xi}^{2}d}{n m\\tau})^{t}(\\Psi_{r,i}^{(t)}+\\widetilde{\\Psi}_{r,i}^{(t)}),}\\\\ &{\\widetilde{\\Psi}_{r,i}^{(t)}\\geq\\frac{101C_{\\ell}}{M+1}(B_{r,+,+}^{(t)}+B_{r,+,-}^{(t)}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Proofof Lemma D.9. We partition the dynamics of $\\rho_{r,i}^{(t)}$ and $\\widetilde{\\rho}_{r,i}^{(t)}$ into one of the four cases according $i\\in\\mathcal{I}_{r,+}^{(0)}\\cap\\widetilde{\\mathcal{L}}_{r,-}^{(0)}$ $i\\in\\mathcal{T}_{r,-}^{(0)}\\cap\\widetilde{\\mathcal{L}}_{r,+}^{(0)}$ Lr,+, (3) i \u2208 I $i\\in\\mathcal{T}_{r,+}^{(0)}\\cap\\widetilde{\\mathcal{L}}_{r,+}^{(0)}$ Tr,+, (4) i \u2208 Z(Q n $i\\in\\mathcal{Z}_{r,-}^{(0)}\\cap\\widetilde{\\mathcal{L}}_{r,-}^{(0)}$ ", "page_idx": 42}, {"type": "text", "text": "(1) In the case of $i\\in\\mathcal{Z}_{r,-}^{(0)}\\cap\\widetilde{\\mathcal{L}}_{r,-}^{(0)}$ , it holds that ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\rho_{r,i}^{(t)}=0,\\;\\mathcal{Z}_{r,-}^{(t)}=\\mathcal{Z}_{r,-}^{(0)};\\quad\\widetilde{\\rho}_{r,i}^{(t)}=0,\\;\\widetilde{\\mathcal{Z}}_{r,-}^{(t)}=\\widetilde{\\mathcal{Z}}_{r,-}^{(0)}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "The prof isby theinductiomthd Iis car whn $t=0$ $\\rho_{r,i}^{(0)}=0$ and $\\widetilde{\\rho}_{r,i}^{(0)}=0$ Therefore,the induction argument holds at the initial step. ", "page_idx": 43}, {"type": "text", "text": "Suppose at teration $t$ , we have $\\rho_{r,i}^{(t)}=0$ = 0 and pt, $\\widetilde{\\rho}_{r,i}^{(t)}=0$ . Then we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\langle\\widetilde{\\mathbf{w}}_{r}^{(t)},\\widetilde{\\pmb{\\xi}}_{i}\\rangle\\leq\\frac{1}{2}\\langle\\widetilde{\\mathbf{w}}_{r}^{(0)},\\widetilde{\\pmb{\\xi}}_{i}\\rangle+\\widetilde{\\rho}_{r,i}^{(t)}=\\frac{1}{2}\\langle\\widetilde{\\mathbf{w}}_{r}^{(0)},\\widetilde{\\pmb{\\xi}}_{i}\\rangle<0,}\\\\ &{}&{\\langle\\mathbf{w}_{r}^{(t)},\\pmb{\\xi}_{i}\\rangle\\leq\\frac{1}{2}\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle+\\rho_{r,i}^{(t)}=\\frac{1}{2}\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\xi}_{i}\\rangle<0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Thus by the update of $\\widetilde{\\rho}_{r,i}^{(t+1)}$ , we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widetilde{\\rho}_{r,i}^{(t+1)}=\\widetilde{\\rho}_{r,i}^{(t)}\\!+\\!\\frac{\\eta}{n m\\tau}(1-\\ell_{i}^{\\prime(t)})\\sigma(\\langle\\mathbf{w}_{r}^{(t)},\\pmb{\\xi}_{i}\\rangle)\\sigma^{\\prime}(\\langle\\widetilde{\\mathbf{w}}_{r}^{(t)},\\widetilde{\\pmb{\\xi}}_{i}\\rangle)\\|\\widetilde{\\pmb{\\xi}}_{i}\\|_{2}^{2}}\\\\ {-\\,\\frac{\\eta}{n m\\tau}\\displaystyle\\sum_{j\\neq i}^{M}\\ell_{i,j}^{\\prime(t)}\\sigma(\\langle\\mathbf{w}_{r}^{(t)},\\pmb{\\xi}_{j}\\rangle)\\sigma^{\\prime}(\\langle\\widetilde{\\mathbf{w}}_{r}^{(t)},\\widetilde{\\pmb{\\xi}}_{i}\\rangle)\\|\\widetilde{\\pmb{\\xi}}_{i}\\|_{2}^{2}=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Here we have used $\\langle\\widetilde{\\mathbf{w}}_{r}^{(t)},\\widetilde{\\pmb{\\xi}}_{i}\\rangle<0$ Similarly, ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\rho_{r,i}^{(t+1)}=\\rho_{r,i}^{(t)}\\!+\\!\\frac{\\eta}{n m\\tau}(1-\\ell_{i}^{\\prime(t)})\\sigma(\\langle\\widetilde{\\mathbf{w}}_{r}^{(t)},\\widetilde{\\pmb{\\xi}}_{i}\\rangle)\\sigma^{\\prime}(\\langle\\mathbf{w}_{r}^{(t)},\\pmb{\\xi}_{i}\\rangle)\\|\\pmb{\\xi}_{i}\\|_{2}^{2}}\\\\ {-\\,\\frac{\\eta}{n m\\tau}\\displaystyle\\sum_{j=1}^{M}\\ell_{i,j}^{\\prime(t)}\\sigma(\\langle\\widetilde{\\mathbf{w}}_{r}^{(t)},\\widetilde{\\pmb{\\xi}}_{j}\\rangle)\\sigma^{\\prime}(\\langle\\mathbf{w}_{r}^{(t)},\\pmb{\\xi}_{i}\\rangle)\\|\\pmb{\\xi}_{i}\\|_{2}^{2}=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "(2)I the case of $i\\in\\mathcal{Z}_{r,-}^{(0)}\\cap\\widetilde{\\mathcal{L}}_{r,+}^{(0)}$ We useinduction Itselear when $t=0$ we have $\\rho_{r,i}^{(0)}=0$ Suppose a iteration $t$ itholds that $\\rho_{r,i}^{(t)}=0$ = 0 Then by the propagation of p we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\rho_{r,i}^{(t+1)}=\\rho_{r,i}^{(t)}\\!+\\!\\frac{\\eta}{n m\\tau}(1-\\ell_{i}^{\\prime(t)})\\sigma(\\langle\\widetilde{\\mathbf{w}}_{r}^{(t)},\\widetilde{\\pmb{\\xi}}_{i}\\rangle)\\sigma^{\\prime}(\\langle\\mathbf{w}_{r}^{(t)},\\pmb{\\xi}_{i}\\rangle)\\|\\pmb{\\xi}_{i}\\|_{2}^{2}}\\\\ {-\\,\\frac{\\eta}{n m\\tau}\\displaystyle\\sum_{j\\neq i}^{M}\\ell_{i,j}^{\\prime(t)}\\sigma(\\langle\\widetilde{\\mathbf{w}}_{r}^{(t)},\\widetilde{\\pmb{\\xi}}_{j}\\rangle)\\sigma^{\\prime}(\\langle\\mathbf{w}_{r}^{(t)},\\pmb{\\xi}_{i}\\rangle)\\|\\pmb{\\xi}_{i}\\|_{2}^{2}=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "On the other hand, ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{\\rho}_{r,i}^{(t+1)}=\\widetilde{\\rho}_{r,i}^{(t)}\\!+\\!\\frac{\\eta}{n m\\tau}(1-\\ell_{i}^{\\prime(t)})\\sigma(\\langle\\mathbf{w}_{r}^{(t)},\\xi_{i}\\rangle)\\sigma^{\\prime}(\\langle\\widetilde{\\mathbf{w}}_{r}^{(t)},\\widetilde{\\xi}_{i}\\rangle)\\|\\widetilde{\\xi}_{i}\\|_{2}^{2}}\\\\ &{\\quad\\quad\\quad-\\frac{\\eta}{n m\\tau}\\displaystyle\\sum_{j\\neq i}^{M}\\ell_{i,j}^{\\prime(t)}\\sigma(\\langle\\mathbf{w}_{r}^{(t)},\\xi_{j}\\rangle)\\sigma^{\\prime}(\\langle\\widetilde{\\mathbf{w}}_{r}^{(t)},\\widetilde{\\xi}_{i}\\rangle)\\|\\widetilde{\\xi}_{i}\\|_{2}^{2}}\\\\ &{\\quad\\quad\\quad=\\widetilde{\\rho}_{r,i}^{(t)}-\\frac{\\eta}{n m\\tau}\\displaystyle\\left[\\sum_{j\\in\\mathbb{Z}_{r,+}}\\ell_{i,j}^{\\prime(t)}\\sigma(\\langle\\mathbf{w}_{r}^{(t)},\\xi_{j}\\rangle)\\right]\\|\\widetilde{\\xi}_{i}\\|_{2}^{2}}\\\\ &{\\quad\\quad\\quad\\leq\\widetilde{\\rho}_{r,i}^{(t)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where the first equation is by $\\sigma(\\langle\\mathbf{w}_{r}^{(t)},\\boldsymbol{\\xi}_{i}\\rangle)\\geq0$ ", "page_idx": 43}, {"type": "text", "text": "(3) In the case of i E T n() , we use induction to prove such claim. It is clear when $t=0$ $\\rho_{r,i}^{(0)}=0$ Suppose at iteration $t$ we have $\\widetilde{\\rho}_{r,i}^{(t)}=0$ . Then we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\langle\\widetilde{\\mathbf{w}}_{r}^{(t)},\\widetilde{\\pmb{\\xi}}_{i}\\rangle\\leq\\frac{1}{2}\\langle\\widetilde{\\mathbf{w}}_{r}^{(0)},\\widetilde{\\pmb{\\xi}}_{i}\\rangle+\\widetilde{\\rho}_{r,i}^{(t)}=\\frac{1}{2}\\langle\\widetilde{\\mathbf{w}}_{r}^{(0)},\\widetilde{\\pmb{\\xi}}_{i}\\rangle<0.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Thus by the update of $\\widetilde{\\rho}_{r,i}^{(t+1)}$ ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widetilde{\\rho}_{r,i}^{(t+1)}=\\widetilde{\\rho}_{r,i}^{(t)}\\!+\\!\\frac{\\eta}{n m\\tau}(1-\\ell_{i}^{\\prime(t)})\\sigma(\\langle\\mathbf{w}_{r}^{(t)},\\pmb{\\xi}_{i}\\rangle)\\sigma^{\\prime}(\\langle\\widetilde{\\mathbf{w}}_{r}^{(t)},\\widetilde{\\pmb{\\xi}}_{i}\\rangle)\\|\\widetilde{\\pmb{\\xi}}_{i}\\|_{2}^{2}}\\\\ {-\\,\\frac{\\eta}{n m\\tau}\\displaystyle\\sum_{j\\neq i}^{M}\\ell_{i,j}^{\\prime(t)}\\sigma(\\langle\\mathbf{w}_{r}^{(t)},\\pmb{\\xi}_{j}\\rangle)\\sigma^{\\prime}(\\langle\\widetilde{\\mathbf{w}}_{r}^{(t)},\\widetilde{\\pmb{\\xi}}_{i}\\rangle)\\|\\widetilde{\\pmb{\\xi}}_{i}\\|_{2}^{2}=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "On the other hand, ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\rho_{r,i}^{(t+1)}=\\rho_{r,i}^{(t)}+\\frac{\\eta}{n m\\tau}(1-\\ell_{i}^{(t)})\\sigma(\\langle\\widetilde{\\mathbf{w}}_{r}^{(t)},\\widetilde{\\xi}_{i}\\rangle)\\sigma^{\\prime}(\\langle\\mathbf{w}_{r}^{(t)},\\xi_{i}\\rangle)\\|\\xi_{i}\\|_{2}^{2}}\\\\ &{\\quad\\quad\\quad-\\frac{\\eta}{n m\\tau}\\displaystyle\\sum_{j\\neq i}^{M}\\ell_{i,j}^{(t)}\\sigma(\\langle\\widetilde{\\mathbf{w}}_{r}^{(t)},\\widetilde{\\xi}_{j}\\rangle)\\sigma^{\\prime}(\\langle\\mathbf{w}_{r}^{(t)},\\xi_{i}\\rangle)\\|\\xi_{i}\\|_{2}^{2},}\\\\ &{\\quad\\quad\\quad=\\rho_{r,i}^{(t)}-\\frac{\\eta}{n m\\tau}\\displaystyle\\left[\\sum_{j\\in\\widetilde{\\mathbb{Z}}_{r,+}}\\ell_{i,j}^{(t)}\\sigma(\\langle\\widetilde{\\mathbf{w}}_{r}^{(t)},\\widetilde{\\xi}_{j}\\rangle)\\right]\\|\\xi_{i}\\|_{2}^{2}}\\\\ &{\\quad\\quad\\quad\\leq\\rho_{r,i}^{(t)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where the first equation is by $\\sigma(\\langle\\widetilde{\\mathbf{w}}_{r}^{(t)},\\widetilde{\\pmb{\\xi}}_{i}\\rangle)\\geq0$ ", "page_idx": 44}, {"type": "text", "text": "(4 Fnally i the ase of $i\\in\\mathcal{T}_{r,+}^{(0)}\\cap\\widetilde{\\mathcal{L}}_{r,+}^{(0)}$ Lr,+, we have ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Psi_{r,i}^{(t+1)}\\leq\\Psi_{r,i}^{(t)}\\!+\\!\\frac{\\eta}{n m\\tau}(1-\\ell_{i}^{\\prime(t)})\\sigma(\\langle\\widetilde{\\mathbf{w}}_{r}^{(t)},\\widetilde{\\pmb{\\xi}}_{i}\\rangle)\\sigma^{\\prime}(\\langle\\mathbf{w}_{r}^{(t)},\\pmb{\\xi}_{i}\\rangle)\\|\\pmb{\\xi}_{i}\\|_{2}^{2}}\\\\ &{\\qquad\\quad\\leq\\Psi_{r,i}^{(t)}+\\frac{1.05\\eta\\sigma_{\\xi}^{2}d}{n m\\tau}\\widetilde{\\Psi}_{r,i}^{(t)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Similarly, for the other modality, ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{\\Psi}_{r,i}^{(t+1)}\\leq\\widetilde{\\Psi}_{r,i}^{(t)}\\!+\\!\\frac{\\eta}{n m\\tau}(1-\\ell_{i}^{\\prime(t)})\\sigma(\\langle\\mathbf{w}_{r}^{(t)},\\pmb{\\xi}_{i}\\rangle)\\sigma^{\\prime}(\\langle\\widetilde{\\mathbf{w}}_{r}^{(t)},\\widetilde{\\pmb{\\xi}}_{i}\\rangle)\\|\\widetilde{\\pmb{\\xi}}_{i}\\|_{2}^{2}}\\\\ &{\\qquad\\quad\\leq\\widetilde{\\Psi}_{r,i}^{(t)}+\\frac{1.05\\eta\\sigma_{\\xi}^{2}d}{n m\\tau}\\Psi_{r,i}^{(t)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Together, we can achieve that ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\Psi_{r,i}^{(t)}+\\widetilde{\\Psi}_{r,i}^{(t)}\\leq(1+\\frac{1.05\\eta\\sigma_{\\xi}^{2}d}{n m\\tau})^{t}(\\Psi_{r,i}^{(0)}+\\widetilde{\\Psi}_{r,i}^{(0)}).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "On the other hand size, we calculate the upper bound of ) -- $\\Psi_{r,i}^{(t)}-\\widetilde{\\Psi}_{r,i}^{(t)}$ as follows ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\vert\\ell_{r,i}^{(t+1)}-\\widetilde{\\Psi}_{r,i}^{(t+1)}=\\Psi_{r,i}^{(t)}-\\widetilde{\\Psi}_{r,i}^{(t)}+\\frac{\\eta}{n m\\tau}(1-\\ell_{i}^{(t)})\\sigma(\\langle\\widetilde{\\Psi}_{r}^{(t)},\\widetilde{\\xi}_{i}\\rangle)\\sigma^{\\prime}(\\langle\\mathbf{w}_{r}^{(t)},\\xi_{i}\\rangle)\\vert\\xi_{i}\\vert_{2}^{2}}\\\\ &{\\quad-\\frac{\\eta}{n m\\tau}(1-\\ell_{i}^{(t)})\\sigma(\\langle\\Psi_{r}^{(t)},\\xi_{i}\\rangle)\\sigma^{\\prime}(\\langle\\widetilde{\\Psi}_{r}^{(t)},\\widetilde{\\xi}_{i}\\rangle)\\vert\\widetilde{\\xi}_{i}\\vert_{2}^{2}}\\\\ &{\\quad-\\frac{\\eta}{n m\\tau}\\sum_{j\\neq i}^{M}\\ell_{i,j}^{(t)}\\sigma(\\langle\\widetilde{\\Psi}_{r}^{(t)},\\widetilde{\\xi}_{i}\\rangle)\\sigma^{\\prime}(\\langle\\mathbf{w}_{r}^{(t)},\\xi_{j}\\rangle)\\vert\\widetilde{\\xi}_{i}\\vert_{2}^{2}}\\\\ &{\\quad+\\frac{\\eta}{n m\\tau}\\sum_{j\\neq i}^{M}\\ell_{i,j}^{(t)}\\sigma(\\langle\\mathbf{w}_{r}^{(t)},\\xi_{j}\\rangle)\\sigma^{\\prime}(\\langle\\widetilde{\\Psi}_{r}^{(t)},\\widetilde{\\xi}_{i}\\rangle)\\vert\\widetilde{\\xi}_{i}\\vert_{2}^{2}}\\\\ &{\\quad\\le\\Psi_{r,i}^{(t)}-\\widetilde{\\Psi}_{r,i}^{(t)}-\\frac{0.96\\eta\\sigma_{r}^{2}d}{n m\\tau}\\Psi_{r,i}^{(t)}-\\widetilde{\\Psi}_{r,i}^{(t)}+\\frac{1.01\\eta\\sigma_{r}^{2}d}{n m\\tau}\\frac{C_{\\ell}}{M+1}(B_{r,+,+}^{(t)}+B_{r,+,-}^{(t)})}\\\\ &{\\quad\\le(1-\\frac{0.95\\eta\\sigma_{r}^{2}d}{n m\\tau})(\\Psi_{r,i}^{(t)}-\\widetilde{\\Psi}_{r,i}^{(t)}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where the first inequality is by Lemma D.2 and Lemma D.1, the second inequality is by induction (46). Therefore we conclude that ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\Psi_{r,i}^{(t)}-\\widetilde{\\Psi}_{r,i}^{(t)}\\leq(1-\\frac{0.95\\eta\\sigma_{\\xi}^{2}d}{n m\\tau})^{t}(\\Psi_{r,i}^{(0)}-\\widetilde{\\Psi}_{r,i}^{(0)}).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Combining (47) and (48) yields ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Psi_{r,i}^{(t)}\\leq(1+\\displaystyle\\frac{1.05\\eta\\sigma_{\\xi}^{2}d}{n m\\tau})^{t}(\\Psi_{r,i}^{(0)}+\\widetilde{\\Psi}_{r,i}^{(0)})+(1-\\displaystyle\\frac{0.95\\eta\\sigma_{\\xi}^{2}d}{n m\\tau})^{t}(\\Psi_{r,i}^{(0)}-\\widetilde{\\Psi}_{r,i}^{(0)})}\\\\ &{\\qquad\\leq(1+\\displaystyle\\frac{1.06\\eta\\sigma_{\\xi}^{2}d}{n m\\tau})^{t}(\\Psi_{r,i}^{(0)}+\\widetilde{\\Psi}_{r,i}^{(0)}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "D.1.3 Signal Learning: Proof of Lemma 5.4 ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Before proving Lemma 5.4, we require the following lower bound for the initialization. Recall the defition that $\\overset{\\vartriangle}{A_{r}^{(t)}}=\\gamma_{r}^{(t)}+\\langle\\mathbf{w}_{r}^{(0\\bar{)}},\\pmb{\\mu}\\rangle$ $r\\in\\mathcal{U}_{+}^{(0)}$ Band $A_{r}^{(t)}=-\\gamma_{r}^{(t)}-\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\mu}\\rangle$ $r\\in\\mathcal{U}_{-}^{(0)}$ Similarly, we have $\\widetilde{A}_{r}^{(t)}=\\widetilde{\\gamma}_{r}^{(t)}+\\langle\\widetilde{\\mathbf{w}}_{r}^{(0)},\\widetilde{\\mu}\\rangle$ for $r\\in\\tilde{\\mathcal{U}}_{+}^{(0)}$ and $\\widetilde{A}_{r}^{(t)}=-\\widetilde{\\gamma}_{r}^{(t)}-\\langle\\widetilde{\\mathbf{w}}_{r}^{(0)},\\widetilde{\\mu}\\rangle$ for $r\\in\\smash{\\widetilde{\\mathcal{U}}_{-}^{(0)}}$ ", "page_idx": 45}, {"type": "text", "text": "Lemma D.10. Suppose $\\delta>0$ and $m\\geq\\widetilde{\\Omega}(1)$ . Then with probability at least $1-\\delta$ wehave ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{m}\\sum_{r\\in\\mathcal{U}_{+}^{(0)}\\cap\\tilde{\\mathcal{U}}_{+}^{(0)}}(A_{r}^{(0)}+\\widetilde A_{r}^{(0)}/C_{\\mu})\\geq0.2\\sigma_{0}\\|\\pmb{\\mu}\\|_{2}}\\\\ &{\\displaystyle\\frac{1}{m}\\sum_{r\\in\\mathcal{U}_{-}^{(0)}\\cap\\tilde{\\mathcal{U}}_{-}^{(0)}}(A_{r}^{(0)}+\\widetilde A_{r}^{(0)}/C_{\\mu})\\geq0.2\\sigma_{0}\\|\\pmb{\\mu}\\|_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Proof of Lemma D.10. We first note that $\\begin{array}{r l r}{\\langle{\\bf w}_{r}^{(0)},\\pmb{\\mu}\\rangle}&{{}\\sim}&{\\mathcal{N}(0,\\sigma_{0}^{2}\\|\\pmb{\\mu}\\|_{2}^{2})}\\end{array}$ and $\\begin{array}{r l}{\\langle\\widetilde{\\mathbf{w}}_{r}^{(0)},\\widetilde{\\pmb{\\mu}}\\rangle}&{{}\\sim}\\end{array}$ $\\mathcal{N}(0,\\sigma_{0}^{2}||\\widetilde{\\pmb{\\mu}}||_{2}^{2})$ .We define the event $A\\,=\\,\\{r\\,\\in\\,[m]\\,:\\,\\langle{\\bf w}_{r}^{(0)},\\mu\\rangle\\,>\\,0,\\langle\\widetilde{{\\bf w}}_{r}^{(0)},\\widetilde{\\mu}\\rangle\\,>\\,0\\}$ . Then we can compute ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\langle\\mathbf{w}_{r}^{(0)},\\mu\\rangle\\mathbb{1}(\\mathcal{A})+\\langle\\widetilde{\\mathbf{w}}_{r}^{(0)},\\tilde{\\mu}/C_{\\mu}\\rangle\\mathbb{1}(\\mathcal{A})]}\\\\ &{=\\mathbb{E}[\\langle\\mathbf{w}_{r}^{(0)},\\mu\\rangle\\mathbb{1}(\\mathcal{A})]+\\mathbb{E}[\\langle\\widetilde{\\mathbf{w}}_{r}^{(0)},\\tilde{\\mu}/C_{\\mu}\\rangle\\mathbb{1}(\\mathcal{A})]}\\\\ &{=\\displaystyle\\frac{1}{2}\\mathbb{E}[\\langle\\mathbf{w}_{r}^{(0)},\\mu\\rangle\\mathbb{1}(\\langle\\mathbf{w}_{r}^{(0)},\\mu\\rangle>0)]+\\frac{1}{2}\\mathbb{E}[\\langle\\widetilde{\\mathbf{w}}_{r}^{(0)},\\tilde{\\mu}/C_{\\mu}\\rangle\\mathbb{1}(\\langle\\widetilde{\\mathbf{w}}_{r}^{(0)},\\tilde{\\mu}\\rangle>0)]}\\\\ &{=\\frac{\\sigma_{0}\\|\\mu\\|_{2}}{\\sqrt{2\\pi}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where we use the independence of neurons in two modalities. Let $\\begin{array}{r}{S:=\\sum_{r=1}^{m}\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\mu}\\rangle\\mathbb{1}(\\mathcal{A})+}\\end{array}$ $\\langle\\widetilde{\\mathbf{w}}_{r}^{(0)},\\widetilde{\\pmb{\\mu}}/C_{\\mu}\\rangle\\mathbb{1}(\\mathcal{A}).$ Then we apply the sub-Gaussian concentraion inequality that with probability at least $1-\\delta/2$ ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\left|\\sum_{r\\in A}\\left(\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\mu}\\rangle+\\langle\\widetilde{\\mathbf{w}}_{r}^{(0)},\\widetilde{\\pmb{\\mu}}/C_{\\mu}\\rangle\\right)-\\frac{m\\sigma_{0}\\|\\pmb{\\mu}\\|_{2}}{\\sqrt{2\\pi}}\\right|\\le\\widetilde{O}(m^{-1/2}).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Then suppose $m=\\widetilde\\Omega(1)$ , we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\frac{1}{m}\\sum_{r\\in\\mathcal{A}}\\left(\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\mu}\\rangle+\\langle\\widetilde{\\mathbf{w}}_{r}^{(0)},\\widetilde{\\pmb{\\mu}}/C_{\\mu}\\rangle\\right)\\geq0.2\\sigma_{0}\\|\\pmb{\\mu}\\|_{2}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Similarly, we can showthe same for the event where $\\langle\\mathbf{w}_{r}^{(0)},\\pmb{\\mu}\\rangle<0,\\langle\\widetilde{\\mathbf{w}}_{r}^{(0)},\\widetilde{\\pmb{\\mu}}\\rangle<0$ and taking the union bound completes the proof. \u53e3 ", "page_idx": 45}, {"type": "text", "text": "Proof of Lemma 5.4. From the upper bound on noise memorization (45), we take the maximum over $r,i$ whichgives ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{r,i}{\\operatorname*{max}}\\,\\Psi_{r,i}^{(t)}\\leq\\Big(1+1.06\\frac{\\eta\\sigma_{\\xi}^{2}d}{n m\\tau}\\Big)^{t}\\underset{r,i}{\\operatorname*{max}}\\,\\Psi_{r,i}^{(0)}}\\\\ &{\\qquad\\qquad\\leq\\Big(1+1.06\\frac{\\eta\\sigma_{\\xi}^{2}d}{n m\\tau}\\Big)^{t}2\\sqrt{\\log(8m n/\\delta)}\\sigma_{0}\\sigma_{\\xi}\\sqrt{d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "At the same time, for signal learning, from the lower bound on signal learning in Lemma D.4, we have for the first modality that ", "page_idx": 46}, {"type": "equation", "text": "$$\nA_{r}^{(t)}\\geq\\Big(1+\\frac{0.48\\eta\\|\\pmb{\\mu}\\|_{2}^{2}C_{\\mu}}{m\\tau}\\Big)^{t}(A_{r}^{(0)}+\\widetilde{A}_{r}^{(0)}/C_{\\mu})-1\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Taking a summation over the $r\\in\\mathcal{U}_{+}^{(0)}\\cap\\widetilde{\\mathcal{U}}_{+}^{(0)}$ , we obtain ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\frac{1}{m}\\sum_{r\\in\\mathcal{U}_{+}^{(0)}\\cap\\tilde{\\mathcal{U}}_{+}^{(0)}}A_{r}^{(t)}\\geq\\Big(1+\\frac{0.48\\eta\\|\\mu\\|_{2}^{2}C_{\\mu}}{m\\tau}\\Big)^{t}\\frac{1}{m}\\sum_{r\\in\\mathcal{U}_{+}^{(0)}\\cap\\tilde{\\mathcal{U}}_{+}^{(0)}}(A_{r}^{(0)}+\\widetilde{A}_{r}^{(0)}/C_{\\mu})-1}&{}\\\\ {\\geq\\Big(1+\\frac{0.48\\eta\\|\\mu\\|_{2}^{2}C_{\\mu}}{m\\tau}\\Big)^{t}0.2\\sigma_{0}\\|\\mu\\|_{2}-1}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where the second inequality is due to Lemma D.10. ", "page_idx": 46}, {"type": "text", "text": "Under the SNR condition $n\\cdot\\mathrm{SNR}^{2}\\geq1.7$ and $C_{\\mu}>2.66$ , we can see there exists a scale difference between $\\operatorname*{max}_{r,i}\\Psi_{r,i}^{(t)}$ and $\\begin{array}{r}{\\frac{1}{m}\\sum_{r\\in\\mathcal{U}_{+}^{(0)}\\cap\\widetilde{\\mathcal{U}}_{+}^{(0)}}A_{r}^{(t)}}\\end{array}$ at the end of first stage. Let ", "page_idx": 46}, {"type": "equation", "text": "$$\nT_{1}=\\log\\left(20/(\\sigma_{0}\\|\\pmb{\\mu}\\|_{2})\\right)/\\log\\left(1+0.48C_{\\mu}\\frac{\\eta\\|\\pmb{\\mu}\\|_{2}^{2}}{m\\tau}\\right).\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Then we have $\\begin{array}{r}{\\frac{1}{m}\\sum_{r\\in\\mathcal{U}_{+}^{(0)}\\cap\\widetilde{\\mathcal{U}}_{+}^{(0)}}A_{r}^{(t)}}\\end{array}$ reach 3 within $T_{1}$ iterations. Using similar analysis, we can show t the same time $\\begin{array}{r}{\\frac{1}{m}\\sum_{r\\in\\mathcal{U}_{-}^{(0)}\\cap\\tilde{\\mathcal{U}}_{-}^{(0)}}A_{r}^{(t)},\\,\\frac{1}{m}\\sum_{r\\in\\mathcal{U}_{+}^{(0)}\\cap\\tilde{\\mathcal{U}}_{+}^{(0)}}\\widetilde{A}_{r}^{(t)},\\frac{1}{m}\\sum_{r\\in\\mathcal{U}_{-}^{(0)}\\cap\\tilde{\\mathcal{U}}_{-}^{(0)}}\\widetilde{A}_{r}^{(t)}}\\end{array}$ alsorach 3. ", "page_idx": 46}, {"type": "text", "text": "Ontheothhad wemte te scaleofa, as ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\operatorname*{max}_{r}\\Psi_{r,i}^{(1)}}&{\\leq\\Big(1+1.0{\\displaystyle\\operatorname*{min}_{\\overline{{n r}}\\neq\\overline{{d}}}}^{2}\\Big)^{t}\\sqrt{\\log(8\\operatorname*{max}/\\delta)}\\sigma_{0}\\sigma_{\\xi}\\sqrt{d}}\\\\ &{=\\exp\\Big(\\frac{\\log(1+1.06\\frac{\\eta^{2}e^{2}d}{n w})}{\\log(1+0.48C_{\\mu}\\frac{\\eta^{2}e^{2}}{n w})}\\log\\big(20/(\\sigma_{0}\\|\\mu\\|_{2})\\big)\\Big)2\\sqrt{\\log(8m n/\\delta)}\\sigma_{0}\\sigma_{\\xi}\\sqrt{d}}\\\\ &{\\leq\\exp\\big((2.21/(C_{\\mu}n\\cdot\\mathrm{SRR}^{2})+O((\\frac{\\eta^{2}e^{2}d}{n m\\tau})^{2}))\\log\\big(20/(\\sigma_{0}|\\mu\\|_{2})\\big)\\big)2\\sqrt{\\log(8m n/\\delta)}\\sigma_{0}\\sigma_{\\xi}\\sqrt{d}}\\\\ &{\\leq\\exp\\big((2.21/(C_{\\mu}n\\cdot\\mathrm{SRR}^{2})+0.01)\\log\\big(20/(\\sigma_{0}|\\mu\\|_{2})\\big)2\\gamma\\mathrm{{log}}(8m n/\\delta)\\sigma_{0}\\sigma_{\\xi}\\sqrt{d}}\\\\ &{\\leq\\exp\\big(0.5\\log\\big(20/(\\sigma_{0}\\|\\mu\\|_{2})\\big)\\big)2\\sqrt{\\log(8m n/\\delta)}\\sigma_{0}\\sigma_{\\xi}\\sqrt{d}}\\\\ &{=\\sqrt{24\\log(8m n/\\delta)}\\frac{\\sqrt{\\sigma_{0}}\\sigma_{\\xi}\\sqrt{d}}{\\sqrt{\\|\\mu\\|_{2}}}}\\\\ &{=\\widetilde{O}(n^{-1/2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where we choose $\\eta$ sufficiently small for the second inequality. In third inequality, we have applied the condition that $n\\mathrm{SNR}^{2}=\\dot{\\Theta}(1)$ and $\\begin{array}{r}{\\sigma_{0}\\leq\\frac{1}{\\|\\pmb{\\mu}\\|_{2}}}\\end{array}$ Th lat inequalityis bythe SNR condition. Because we can choose $n\\geq C\\log(m/\\delta)$ for suficiently large constant $C,\\operatorname*{max}_{r,i}\\Psi_{r,i}^{(T_{1})}=o(1)$ \uff1a\u53e3 ", "page_idx": 46}, {"type": "text", "text": "D.2  Second Stage ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "We first show a similar result as in Lemma C.14 for both two modalities. ", "page_idx": 46}, {"type": "text", "text": "Lemma D.11. Under conditions, for $0\\leq t\\leq T^{*}$ ,we have ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla L_{S}(\\mathbf{W}^{(t)})\\|_{F}^{2}\\leq O(\\operatorname*{max}\\{\\|\\pmb{\\mu}\\|_{2}^{2},\\sigma_{\\xi}^{2}d\\})L_{S}(\\mathbf{W}^{(t)}),}\\\\ {\\|\\nabla L_{S}(\\widetilde{\\mathbf{W}}^{(t)})\\|_{F}^{2}\\leq O(\\operatorname*{max}\\{\\|\\pmb{\\mu}\\|_{2}^{2},\\sigma_{\\widetilde{\\xi}}^{2}d\\})L_{S}(\\widetilde{\\mathbf{W}}^{(t)}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Proof of Lemma D.11. The proof follows from that of Lemma C.14 and hence is omitted for clarity. ", "page_idx": 46}, {"type": "text", "text": "For notation convenience, we let ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathrm{c}_{0}(\\mathbf{W},\\mathbf{x}_{i})=\\mathrm{Sim}_{\\mathbf{k}_{\\perp}}\\sigma(\\mathbf{\\sigma}(\\mathbf{w}_{r},y_{i}|\\theta))\\mathrm{g}(\\sigma(\\langle\\tilde{\\mathbf{w}}_{r},y_{i}|\\tilde{\\mathbf{w}}\\rangle))+\\frac{1}{m\\tau}\\frac{\\mathrm{m}}{r_{\\mathrm{{ref}}}^{2}}\\sigma(\\langle\\mathbf{w}_{r},\\xi_{i}\\rangle)\\mathrm{sg}(\\sigma(\\langle\\tilde{\\mathbf{w}}_{r},\\tilde{\\xi}_{i}\\rangle))}}\\\\ &{}\\\\ {\\mathrm{\\mathcal{F}}_{j}(\\mathbf{W},\\mathbf{x}_{i})=\\mathrm{Sim}_{\\mathbf{k}_{\\perp}}\\sigma(\\mathbf{\\sigma}(\\mathbf{w}_{r},y_{i}|\\tilde{\\mathbf{w}}))/\\tau}\\\\ &{=\\frac{1}{m\\tau}\\sum_{r=1}^{m}\\sigma(\\langle\\mathbf{w}_{r},y_{i}|\\theta\\rangle)\\mathrm{sg}(\\sigma(\\langle\\tilde{\\mathbf{w}}_{r},y_{i}|\\tilde{\\mathbf{w}}\\rangle))+\\frac{1}{m\\tau}\\sum_{r=1}^{m}\\sigma(\\langle\\mathbf{w}_{r},\\xi_{i}\\rangle)\\mathrm{sg}(\\sigma(\\langle\\tilde{\\mathbf{w}}_{r},\\tilde{\\xi}_{j}\\rangle)),\\ \\mathrm{for}\\ \\boldsymbol{j}}\\\\ &{}\\\\ {\\tilde{\\mathrm{c}}_{0}(\\widetilde{\\mathbf{W}},\\tilde{\\mathbf{x}}_{i})=\\mathrm{Sim}_{\\mathbf{k}_{\\perp}}(\\tilde{\\mathbf{x}}_{i},\\mathbf{x}_{i})/\\tau}\\\\ &{=\\frac{1}{m\\tau}\\sum_{r=1}^{m}\\mathrm{sg}(\\sigma(\\langle\\mathbf{w}_{r},y_{i}|\\theta\\rangle))\\sigma(\\langle\\tilde{\\mathbf{w}}_{r},y_{i}|\\tilde{\\mathbf{w}}\\rangle)+\\frac{1}{m\\tau}\\sum_{r=1}^{m}\\mathrm{sg}(\\sigma(\\langle\\mathbf{w}_{r},\\xi_{i}\\rangle))\\sigma(\\langle\\tilde{\\mathbf{w}}_{r},\\tilde{\\xi}_{i}\\rangle)}\\\\ &{\\tilde{\\mathrm{v}}_{j}(\\widetilde{\\mathbf{w}},\\tilde{\\mathbf{x}}_{i})=\\mathrm{Sim}_{\\mathbf{k}_{\\perp}}(\\tilde{\\mathbf{x}}_{i},\\mathbf{x}_{j})\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "It is worth mentioning that $F_{j}(\\mathbf{W},\\mathbf{x}_{i})=\\widetilde{F}_{j}(\\widetilde{\\mathbf{W}},\\widetilde{\\mathbf{x}}_{i})$ in terms of numerical values. They differ in terms of the derivatives. ", "page_idx": 47}, {"type": "text", "text": "We further denote ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathbb{C}_{S}({\\mathbf{W}})=-\\frac{1}{n}\\sum_{i=1}^{n}L_{i}({\\mathbf{W}})=-\\frac{1}{n}\\sum_{i=1}^{n}\\log\\Big(\\frac{e^{\\mathrm{Sim}_{\\mathbf{k},\\mathbf{g}}({\\mathbf{x}}_{i},{\\widetilde{\\mathbf{x}}}_{i})/\\tau}}{e^{\\mathrm{Sim}_{\\mathbf{k},\\mathbf{g}}({\\mathbf{x}}_{i},{\\widetilde{\\mathbf{x}}}_{i})/\\tau}+\\sum_{j\\neq i}^{M}e^{\\mathrm{Sim}_{\\mathbf{k},\\mathbf{g}}({\\mathbf{x}}_{i},{\\widetilde{\\mathbf{x}}}_{j})/\\tau}}\\Big),}}\\\\ {{\\displaystyle L_{i}({\\mathbf{W}})=-\\log\\Big(\\frac{e^{\\mathrm{Sim}_{\\mathbf{k},\\mathbf{g}}({\\mathbf{x}}_{i},{\\widetilde{\\mathbf{x}}}_{i})/\\tau}}{e^{\\mathrm{Sim}_{\\mathbf{k},\\mathbf{g}}({\\mathbf{x}}_{i},{\\widetilde{\\mathbf{x}}}_{i})/\\tau}+\\sum_{j\\neq i}^{M}e^{\\mathrm{Sim}_{\\mathbf{k},\\mathbf{g}}({\\mathbf{x}}_{i},{\\widetilde{\\mathbf{x}}}_{j})/\\tau}}\\Big)=-\\log\\Big(\\frac{e^{F_{0}({\\mathbf{W}},{\\mathbf{x}}_{i})}}{e^{F_{0}({\\mathbf{W}},{\\mathbf{x}}_{i})}+\\sum_{j=1}^{M}e^{F_{j}({\\mathbf{W}},{\\mathbf{x}}_{i})}}}\\\\ {{\\displaystyle\\mathbb{C}_{S}({\\widetilde{\\mathbf{W}}})=-\\frac{1}{n}\\sum_{i=1}^{n}L_{i}({\\widetilde{\\mathbf{W}}})=-\\frac{1}{n}\\sum_{i=1}^{n}\\log\\Big(\\frac{e^{\\mathrm{Sim}_{\\mathbf{k},\\mathbf{g}}({\\mathbf{x}}_{i},{\\mathbf{x}}_{i})/\\tau}}{e^{\\mathrm{Sim}_{\\mathbf{k},\\mathbf{h}}({\\widetilde{\\mathbf{x}}}_{i},{\\widetilde{\\mathbf{x}}}_{i})/\\tau}+\\sum_{j\\neq i}^{M}e^{\\mathrm{Sim}_{\\mathbf{k}}({\\widetilde{\\mathbf{x}}}_{i},{\\widetilde{\\mathbf \n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Here $\\widetilde{L}(\\mathbf{W},\\widetilde{\\mathbf{W}})$ is the combined loss function for two modalities. ", "page_idx": 47}, {"type": "text", "text": "Let $\\theta_{r}=1$ $r\\in\\mathcal{U}_{+}^{(0)}$ $\\langle\\mathbf{w}_{r}^{(0)},\\mu\\rangle>0$ and $\\theta_{r}=-1$ $r\\in\\mathcal{U}_{-}^{(0)}$ ,ie, $\\langle\\mathbf{w}_{r}^{(0)},\\mu\\rangle<0$ Similarly. we let $\\widetilde{\\theta}_{r}=1$ $r\\in\\widetilde{\\mathcal{U}}_{+}^{(0)}$ and $\\widetilde{\\theta}_{r}=-1$ $r\\in\\smash{\\widetilde{\\mathcal{U}}_{-}^{(0)}}$ . Then we define ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{w}_{r}^{*}=\\mathbf{w}_{r}^{(0)}+2\\tau\\log(2M/\\epsilon)\\cdot\\boldsymbol{\\theta}_{r}\\cdot\\frac{\\mu}{\\lVert\\pmb{\\mu}\\rVert_{2}^{2}},}\\\\ &{\\widetilde{\\mathbf{w}}_{r}^{*}=\\widetilde{\\mathbf{w}}_{r}^{(0)}+2\\tau\\log(2M/\\epsilon)\\cdot\\widetilde{\\boldsymbol{\\theta}}_{r}\\cdot\\frac{\\widetilde{\\pmb{\\mu}}}{\\lVert\\widetilde{\\pmb{\\mu}}\\rVert_{2}^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Lemma D.12. Under Asmption 4.1, we have $\\|\\mathbf{W}^{(T_{1})}-\\mathbf{W}^{*}\\|_{F}\\leq\\widetilde O(m^{1/2}\\|\\pmb{\\mu}\\|_{2}^{-1})$ and $\\|\\widetilde{\\mathbf{W}}^{(T_{1})}-$ $\\widetilde{\\mathbf{W}}^{*}\\|_{F}\\leq\\widetilde{O}(m^{1/2}\\|\\widetilde{\\pmb{\\mu}}\\|_{2}^{-1})$ ", "page_idx": 47}, {"type": "text", "text": "ProofofLemma $D.I2$ . The proof follows exactly the same as the proof in single-modal case. we include it here for completeness. Without loss of generality, we focus on the case for ${\\bf W}^{(T_{1})}$ ", "page_idx": 47}, {"type": "text", "text": "By the scale difference at $T_{1}$ , we have ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{W}^{(T_{1})}-\\mathbf{W}^{*}\\|_{F}\\leq\\|\\mathbf{W}^{(T_{1})}-\\mathbf{W}^{(0)}\\|_{F}+\\|\\mathbf{W}^{(0)}-\\mathbf{W}^{*}\\|_{F}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\displaystyle\\sum_{r}\\frac{\\gamma_{r}^{(T_{1})}}{\\|\\mu\\|_{2}}+\\displaystyle\\sum_{r,i}\\frac{|\\rho_{r,i}^{(T_{1})}|}{\\|\\xi_{i}\\|_{2}}+O(m^{1/2}\\log(1/\\epsilon))\\|\\mu\\|_{2}^{-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq O(m\\|\\pmb{\\mu}\\|_{2}^{-1})+O(n m\\sigma_{0})+O(m^{1/2}\\log(1/\\epsilon)\\|\\pmb{\\mu}\\|_{2}^{-1})}\\\\ &{\\leq\\widetilde O(m^{1/2}\\|\\pmb{\\mu}\\|_{2}^{-1})}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where the first inequality is by triangle inequality and the second inequality is by decomposition of ${\\bf W}^{(T_{1})}$ and $\\mathbf{W}^{*}$ . The third inequality is by the bound on $\\gamma_{r}^{(T_{1})}$ and pr,i $\\rho_{r,i}^{(T_{1})}$ and LemmaB.5.The last inequality is by condition on $\\sigma_{0}$ \u53e3 ", "page_idx": 48}, {"type": "text", "text": "Lemma D.13. Under Assumption 4.1, we have for all $t\\in[T_{1},T^{*}]$ ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\nabla F_{0}(\\mathbf{W}^{(t)},\\mathbf{x}_{i}),\\mathbf{W}^{*}\\rangle\\ge2\\log(2M/\\epsilon)}\\\\ &{\\langle\\nabla F_{j}(\\mathbf{W}^{(t)},\\mathbf{x}_{i}),\\mathbf{W}^{*}\\rangle\\le\\log(2M/\\epsilon),\\,f o r\\,j=1,...,M}\\\\ &{\\langle\\nabla F_{0}(\\widetilde{\\mathbf{W}}^{(t)},\\mathbf{x}_{i}),\\widetilde{\\mathbf{W}}^{*}\\rangle\\ge2\\log(2M/\\epsilon)}\\\\ &{\\langle\\nabla F_{j}(\\widetilde{\\mathbf{W}}^{(t)},\\mathbf{x}_{j}),\\widetilde{\\mathbf{W}}^{*}\\rangle\\le\\log(2M/\\epsilon),\\,f o r\\,j=1,...,M}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Proof of Lemma D.13. The proof follows similarly from Lemma C.16 and here we only show the result for the first modality. Based on the definition of $\\mathbf{W}^{*}$ and $F_{j}({\\bf W}^{(t)},{\\bf x}_{i})$ ,wecan derivefor $j=0$ ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\nabla F_{0}(\\mathbf{W}^{(t)},\\mathbf{x}_{i}),\\mathbf{W}^{*}\\rangle}\\\\ &{=\\displaystyle\\sum_{r=1}^{m}\\langle\\nabla_{\\mathbf{w}}F_{0}(\\mathbf{W}^{(t)},\\mathbf{x}_{i}),\\mathbf{w}_{r}^{*}\\rangle}\\\\ &{=\\displaystyle\\frac{1}{m\\tau}\\sum_{r=1}^{m}\\sigma^{\\prime}\\big(\\langle\\mathbf{w}_{r}^{(t)},y_{i}\\mu\\rangle\\big)\\sigma\\big(\\langle\\overline{{\\mathbf{w}}}_{r}^{(t)},y_{i}\\widehat{\\mu}\\rangle\\big)\\langle\\mathbf{w}_{r}^{*},y_{i}\\mu\\rangle+\\displaystyle\\frac{1}{m\\tau}\\sum_{r=1}^{m}\\sigma^{\\prime}\\big(\\langle\\mathbf{w}_{r}^{(t)},\\xi_{i}\\rangle\\big)\\sigma\\big(\\langle\\overline{{\\mathbf{w}}}_{r}^{(t)},\\overline{{\\xi}}_{i}\\rangle\\big)\\langle\\mathbf{w}_{r}^{*},\\xi_{i}\\rangle}\\\\ &{=\\displaystyle\\frac{1}{m\\tau}\\sum_{r=1}^{m}\\sigma^{\\prime}\\big(\\langle\\mathbf{w}_{r}^{(t)},y_{i}\\mu\\rangle\\big)\\sigma\\big(\\langle\\overline{{\\mathbf{w}}}_{r}^{(t)},y_{i}\\widehat{\\mu}\\rangle\\big)\\Big(\\langle\\mathbf{w}_{r}^{(t)},y_{i}\\mu\\rangle+2\\tau\\log(2M/\\epsilon)\\theta_{r}\\mathcal{R}_{i}\\Big)}\\\\ &{\\quad+\\displaystyle\\frac{1}{m\\tau}\\sum_{r=1}^{m}\\sigma^{\\prime}\\big(\\langle\\mathbf{w}_{r}^{(t)},\\xi_{i}\\rangle\\big)\\sigma\\big(\\langle\\overline{{\\mathbf{w}}}_{r}^{(t)},\\widehat{\\xi}_{i}\\rangle\\big)\\Big(\\langle\\mathbf{w}_{r}^{(t)},\\xi_{i}\\rangle+2\\tau\\theta_{r}\\log(2M/\\epsilon)\\langle\\xi_{i},y_{i}\\mu\\rangle\\|\\mu\\|_{2}^{-2}\\Big)}\\\\ &{\\geq\\displaystyle\\frac{1}{m\\tau}\\sum_{r=1}^{m}\\sigma^{\\prime}\\big(\\langle\\mathbf{w}_{r}^{(t)},y_{i}\\mu\\rangle\\big)\\sigma\\big(\\langle\\overline{{\\mathbf{w}}}_{r}^{(t)},y_{i}\\widehat{\\mu}\\rangle\\big)2\\tau\\log(2M/\\epsilon)\n$$", "text_format": "latex", "page_idx": 48}, {"type": "equation", "text": "$$\n-\\underbrace{\\frac{1}{m\\tau}\\sum_{r=1}^{m}\\sigma(\\langle\\widetilde{\\mathbf{w}}_{r}^{(t)},\\widetilde{\\xi}_{i}\\rangle)2\\tau\\log(2M/\\epsilon)\\widetilde{O}(\\sigma_{\\xi}\\|\\mu\\|_{2}^{-1})}_{I_{3}}-\\underbrace{\\frac{1}{m\\tau}\\sum_{r=1}^{m}\\sigma(\\langle\\widetilde{\\mathbf{w}}_{r}^{(t)},\\widetilde{\\xi}_{i}\\rangle)\\widetilde{O}(\\sigma_{0}\\sigma_{\\xi}\\sqrt{d})}_{I_{4}}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "First, we can bound $I_{2}\\leq\\tilde{O}(\\sigma_{0}\\|\\mu\\|_{2})$ \uff0c $I_{3}\\,\\le\\,\\log(2M/\\epsilon)\\widetilde{O}(\\sigma_{\\xi}\\|\\pmb{\\mu}\\|_{2}^{-1})$ \uff0c $I_{4}\\,\\leq\\,\\widetilde O(\\sigma_{0}\\sigma_{\\xi}\\sqrt{d})$ by the global bound on $\\sigma(\\langle\\widetilde{\\mathbf{w}}_{r}^{(t)},\\widetilde{\\pmb{\\xi}}_{i}\\rangle),\\sigma(\\langle\\widetilde{\\mathbf{w}}_{r}^{(t)},y_{i}\\widetilde{\\pmb{\\mu}}\\rangle)=\\widetilde{O}(1)$ ", "page_idx": 48}, {"type": "text", "text": "Further, we lower bound $I_{1}$ as follows. Without loss of generality, we suppose $y_{i}=1$ , then we have ", "page_idx": 48}, {"type": "equation", "text": "$$\nI_{1}\\geq\\frac{1}{m\\tau}\\sum_{r\\in\\mathcal{U}_{+}^{(0)}\\cap\\tilde{\\mathcal{U}}_{+}^{(0)}}\\sigma(\\langle\\widetilde{\\mathbf{w}}_{r}^{(t)},y_{i}\\widetilde{\\pmb{\\mu}}\\rangle)2\\tau\\log(2M/\\epsilon)\\geq4\\log(2M/\\epsilon)\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where the last inequality is by Lemma 5.4 and the monotonicity of $\\widetilde{\\gamma}_{r}^{(t)}$ ", "page_idx": 48}, {"type": "text", "text": "Then we can obtain ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\langle\\nabla F_{0}(\\mathbf{W}^{(t)},\\mathbf{x}_{i}),\\mathbf{W}^{*}\\rangle\\ge4\\log(2M/\\epsilon)-I_{2}-I_{3}-I-4\\ge2\\log(2M/\\epsilon).\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "The proof for $F_{j}(\\mathbf{W}^{(t)},\\mathbf{W}^{*})$ follows the same argument as in Lemma C.16. ", "page_idx": 48}, {"type": "text", "text": "Lemma D.14. Under Assumption 4.1, we have for all $t\\in[T_{1},T^{*}].$ ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\mathbf{W}^{(t)}-\\mathbf{W}^{*}\\lVert_{F}^{2}+\\lVert\\widetilde{\\mathbf{W}}^{(t)}-\\widetilde{\\mathbf{W}}^{*}\\rVert_{F}^{2}-\\lVert\\mathbf{W}^{(t+1)}-\\mathbf{W}^{*}\\rVert_{F}^{2}-\\lVert\\widetilde{\\mathbf{W}}^{(t+1)}-\\widetilde{\\mathbf{W}}^{*}\\rVert_{F}^{2}\\geq\\eta\\overline{{L}}(\\mathbf{W}^{(t)},\\widetilde{\\mathbf{W}}^{(t)})\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Proof of Lemma $D.I4$ First, we see that $\\overline{{L}}(\\mathbf{W}^{(t)},\\widetilde{\\mathbf{W}}^{(t)})=L_{S}(\\mathbf{W}^{(t)})+L_{S}(\\widetilde{\\mathbf{W}}^{(t)})$ is decomposable in terms of $\\mathbf{W}^{(t)}$ and $\\widetilde{\\mathbf{W}}^{(t)}$ . This suggests that $\\nabla_{\\mathbf{W}}\\overline{L}(\\mathbf{W}^{(t)},\\widetilde{\\mathbf{W}}^{(t)})\\;=\\;\\nabla L_{S}(\\mathbf{W}^{(t)})$ and $\\nabla_{\\widetilde{\\mathbf{W}}}\\overline{L}(\\mathbf{W}^{(t)},\\widetilde{\\mathbf{W}}^{(t)})=\\nabla\\widetilde{L}_{S}(\\widetilde{\\mathbf{W}}^{(t)})$ . Then following similar analysis as in Lemma C.17, we can first show ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\langle F_{j}(\\mathbf{W}^{(t)},\\mathbf{x}_{i}),\\mathbf{W}^{(t)}\\rangle=F_{j}(\\mathbf{W}^{(t)},\\mathbf{x}_{i}),\\;\\mathrm{for}\\;j=0,...,M,}\\\\ {\\langle F_{j}(\\widetilde{\\mathbf{W}}^{(t)},\\widetilde{\\mathbf{x}}_{i}),\\widetilde{\\mathbf{W}}^{(t)}\\rangle=F_{j}(\\widetilde{\\mathbf{W}}^{(t)},\\widetilde{\\mathbf{x}}_{i}),\\;\\mathrm{for}\\;j=0,...,M.}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Then by the gradient descent update ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{W}^{(1)}-\\mathbf{W}^{*}\\|_{{\\mathcal{F}}}^{2}-\\|\\mathbf{W}^{(1)}-\\mathbf{W}^{*}\\|_{{\\mathcal{F}}}^{2}}\\\\ &{=2\\eta\\langle\\nabla L_{{\\mathcal{F}}}(\\mathbf{W}^{(0)}),\\mathbf{W}^{(0)}-\\mathbf{W}^{*}\\rangle-\\eta^{2}\\|\\nabla L_{{\\mathcal{F}}}(\\mathbf{W}^{(0)})\\|_{{\\mathcal{F}}}^{2}}\\\\ &{=\\frac{2\\eta}{n}\\frac{\\sqrt{N}}{\\omega_{1}\\omega_{2}}\\frac{\\partial L_{{\\mathcal{F}}}(\\mathbf{W}^{(0)})}{\\partial\\zeta_{1}}(\\nabla F_{1}(\\mathbf{W}^{(0)},\\mathbf{W}),\\mathbf{W}^{(0)}-\\mathbf{W}^{*})-\\eta^{2}\\|\\nabla L_{{\\mathcal{S}}}(\\mathbf{W}^{(0)})\\|_{{\\mathcal{F}}}^{2}}\\\\ &{=\\frac{2\\eta}{n}\\sum_{i=1}^{N}\\frac{\\partial L_{{\\mathcal{F}}}(\\mathbf{W}^{(0)})}{\\partial\\zeta_{i}}(\\mathbf{W}^{(0)},\\mathbf{W}^{*})}\\\\ &{\\geq\\frac{2\\eta}{n}\\sum_{i=1}^{N}\\frac{\\partial L_{{\\mathcal{F}}}(\\mathbf{W}^{(0)})}{\\partial\\zeta_{i}}(\\mathbf{W}^{(0)},\\mathbf{W}^{*})}\\\\ &{\\geq\\frac{2\\eta}{n}\\sum_{i=1}^{N}\\biggl(\\frac{\\partial L_{{\\mathcal{F}}}(\\mathbf{W}^{(0)},\\mathbf{W})}{\\partial\\zeta_{i}}(F_{1}(\\mathbf{W}^{(0)},\\mathbf{x}_{i})-2\\mathrm{Ieg}(2M/c))+\\frac{\\lambda_{1}}{\\sqrt{2}-1}\\frac{\\partial L_{{\\mathcal{F}}}(\\mathbf{W}^{(0)})}{\\partial\\zeta_{i}}(F_{1}(\\mathbf{W}^{(0)},\\mathbf{x}_{i}))}\\\\ &{\\qquad-\\eta^{2}\\|\\nabla L_{{\\mathcal{F}}}(\\mathbf{W}^{(0)})\\|_{{\\mathcal{F}}}^{2}}\\\\ &{\\geq\\frac{2\\eta}{n}\\sum_{i=1}^{N}\\biggl(L_{{\\\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where the third equality is by (49). The first inequality is by Lemma D.13. The second inequality is due to the convexity of negative log-Softmax function. The last inequality is by Lemma D.11 (and the conditions on $\\eta$ and $\\log(1+x)\\leq x$ for $x\\geq0$ ", "page_idx": 49}, {"type": "text", "text": "Similarly, we can show the same for the other modality as ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\|\\widetilde{\\mathbf{W}}^{(t)}-\\widetilde{\\mathbf{W}}^{*}\\|_{F}^{2}-\\|\\widetilde{\\mathbf{W}}^{(t+1)}-\\widetilde{\\mathbf{W}}^{*}\\|_{F}^{2}\\geq\\eta L_{S}(\\widetilde{\\mathbf{W}}^{(t)})-\\eta\\epsilon\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Combining the two results completes the proof. ", "page_idx": 49}, {"type": "text", "text": "Lemma D.15. Unde Assupto 4.1 et $\\begin{array}{r}{T\\,=\\,T_{1}\\,+\\,\\lfloor\\frac{\\|\\mathbf{W}^{(T_{1})}-\\mathbf{W}^{*}\\|_{F}^{2}+\\|\\widetilde{\\mathbf{W}}^{(T_{1})}-\\widetilde{\\mathbf{W}}^{*}\\|_{F}^{2}}{\\eta\\epsilon}\\rfloor\\,=\\,T_{1}\\,+\\,}\\end{array}$ $\\widetilde{\\cal O}(m\\eta^{-1}\\epsilon^{-1}\\|\\pmb{\\mu}\\|_{2}^{-2})$ . Then we have $\\begin{array}{r}{\\operatorname*{max}_{r,i}|\\rho_{r,i}^{(t)}|\\,\\leq\\,\\sigma_{0}\\sigma_{\\xi}\\sqrt{d}\\;a n d\\operatorname*{max}_{r,i}|\\tilde{\\rho}_{r,i}^{(t)}|\\,\\leq\\,\\sigma_{0}\\sigma_{\\xi}\\sqrt{d}\\,f o r}\\end{array}$ all $t\\in[T_{1},T]$ In addition, we have for all $T_{1}\\leq t\\leq T$ ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\frac{1}{t-T_{1}+1}\\sum_{s=T_{1}}^{t}\\overline{{L}}(\\mathbf{W}^{(t)},\\widetilde{\\mathbf{W}}^{(t)})\\le\\frac{\\|\\mathbf{W}^{(T_{1})}-\\mathbf{W}^{*}\\|_{F}^{2}+\\|\\widetilde{\\mathbf{W}}^{(T_{1})}-\\widetilde{\\mathbf{W}}^{*}\\|_{F}^{2}}{\\eta(t-T_{1}+1)}+2\\epsilon.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Therefore, we can find an iterate $(\\mathbf{W}^{(s)},\\widetilde{\\mathbf{W}}^{(s)})$ for $s\\in[T_{1},T]$ with training lossmaller than 3. ", "page_idx": 49}, {"type": "text", "text": "Proof of Lemma D.15. By Lemma D.14, for $t\\in[T_{1},T]$ ,wehavefor any $s\\leq t$ ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\mathbf{W}^{(s)}-\\mathbf{W}^{*}\\lVert_{F}^{2}+\\lVert\\widetilde{\\mathbf{W}}^{(s)}-\\widetilde{\\mathbf{W}}^{*}\\rVert_{F}^{2}-\\lVert\\mathbf{W}^{(s+1)}-\\mathbf{W}^{*}\\rVert_{F}^{2}-\\lVert\\widetilde{\\mathbf{W}}^{(s+1)}-\\widetilde{\\mathbf{W}}^{*}\\rVert_{F}^{2}\\ge\\eta\\overline{{L}}(\\mathbf{W}^{(s)},\\widetilde{\\mathbf{W}}^{(s)})\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Summing the inequality yields ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\sum_{s=T_{1}}^{t}\\overline{{L}}(\\mathbf{W}^{(s)},\\widetilde{\\mathbf{W}}^{(s)})\\leq\\frac{\\|\\mathbf{W}^{(T_{1})}-\\mathbf{W}^{*}\\|_{F}^{2}+\\|\\widetilde{\\mathbf{W}}^{(T_{1})}-\\widetilde{\\mathbf{W}}^{*}\\|_{F}^{2}+2\\eta\\epsilon(t-T_{1}+1)}{\\eta}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Dividing both sides by $t-T_{1}+1$ and setting $t=T$ gives ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\frac{1}{T-T_{1}+1}\\sum_{s=T_{1}}^{t}\\overline{{L}}(\\mathbf{W}^{(s)},\\widetilde{\\mathbf{W}}^{(s)})\\leq\\frac{\\|\\mathbf{W}^{(T_{1})}-\\mathbf{W}^{*}\\|_{F}^{2}+\\|\\widetilde{\\mathbf{W}}^{(T_{1})}-\\widetilde{\\mathbf{W}}^{*}\\|_{F}^{2}}{\\eta(T-T_{1}+1)}+2\\epsilon\\leq3\\epsilon.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "D.3Downstream Task Performance ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Recall that after the pre-training stage on the training data at time $T$ , the signal learning and noise memorization satisfy ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{r}{\\operatorname*{max}}\\,A_{r}^{(T)}=\\widetilde\\Omega(1),}\\\\ &{\\underset{r}{\\operatorname*{max}}\\,\\Psi_{r,i}^{(T)}=\\widetilde O(1/\\sqrt{n})\\,\\,\\mathrm{for}\\,\\,i\\in[n].}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Then, on the downstream task, the corresponding embedding can be calculated as follows: ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h_{r}(\\mathbf{x}_{\\mathrm{test}}^{(1)})=\\sigma(\\langle\\mathbf{w}_{r}^{(T)},\\mathbf{x}_{\\mathrm{test}}^{(1)}\\rangle)=\\widetilde{\\Omega}(1/\\sqrt{d}),}\\\\ &{h_{r}(\\mathbf{x}_{\\mathrm{test}}^{(2)})=\\sigma(\\langle\\mathbf{w}_{r}^{(T)},\\mathbf{x}_{\\mathrm{test}}^{(2)}\\rangle)=\\widetilde{O}(1/\\sqrt{d n}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Then, it is straightforward to check that the embedding of a finite size of samples during the fine-tuning stage is linearly separable. Thus, the downstream task performance follows $L_{\\mathcal{D}_{\\mathrm{test}}}(\\bar{T}^{*})=o(1)$ ", "page_idx": 50}, {"type": "text", "text": "E Additional Experimental Details ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "We implement our methods using PyTorch. For the software and hardware configurations, we ensure consistent environments for each dataset. We run all the experiments on Linux servers with NVIDIA V100 graphics cards and CUDA 11.2, completing them within one hour. ", "page_idx": 50}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 51}, {"type": "text", "text": "Justification: The abstract and introduction clearly outline the primary contributions of the paper, including the theoretical advancements in optimization and generalization analysis of multi-modal contrastive learning and single-modal contrative learning. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 51}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Justification: We have discussed the limitation of this work in Section 7 ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 51}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 52}, {"type": "text", "text": "Justification: We have provided all assumptions in Assumption 4.1 in the main paper. The proof sketch and complete proof are provided in Section 5 and Appendices C and D. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 52}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 52}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 52}, {"type": "text", "text": "Justification: We have provided the complete configuration in Section 6. We have also uploaded the code in the supplementary material. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. () If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct thedataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 52}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 53}, {"type": "text", "text": "Justification: We have uploaded the code with instructions in the supplementary material. Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : / /nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 53}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 53}, {"type": "text", "text": "Justification: We have provided all the training and test details in Section 6 and uploaded code. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 53}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 53}, {"type": "text", "text": "Justification: We plot 1-sigma error bar for results shown in Figure 1. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 53}, {"type": "text", "text": "", "page_idx": 54}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce theexperiments? ", "page_idx": 54}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 54}, {"type": "text", "text": "Justification: We have provided sufficient information about computer resources in Appendix E. ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 54}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 54}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 54}, {"type": "text", "text": "Justification: The research does not involve any human subjects, personal data, or interactions that would raise ethical concerns about consent, privacy, or respect for persons. In conclusion, the research aligns with the ethical principles outlined in the NeurIPs Code of Ethics. ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 54}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 54}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 54}, {"type": "text", "text": "Justification: We have discussed the broader impacts in Section A Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed. \u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 54}, {"type": "text", "text": "\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 55}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 55}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 55}, {"type": "text", "text": "Justification: This paper poses no such risks. ", "page_idx": 55}, {"type": "text", "text": "Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 55}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 55}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 55}, {"type": "text", "text": "Justification: This paper does not use existing assets. ", "page_idx": 55}, {"type": "text", "text": "Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 55}, {"type": "text", "text": "", "page_idx": 56}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the asshts? ", "page_idx": 56}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 56}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 56}, {"type": "text", "text": "Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 56}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 56}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 56}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 56}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 56}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 56}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 56}]