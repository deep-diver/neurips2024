[{"figure_path": "qmoVQbwmCY/figures/figures_2_1.jpg", "caption": "Figure 1: Probabilistic slot attention and the identifiable aggregate slot posterior. (Left) Slot posterior GMMs per datapoint (local) and the induced aggregate posterior GMM (global). (Right) Sampling slot representations from the aggregate slot posterior is tractable.", "description": "This figure illustrates the probabilistic slot attention model. The left side shows how local Gaussian Mixture Models (GMMs) are fitted to the latent representations of individual data points. These local GMMs represent the posterior distributions of the object slots.  The local GMMs are then aggregated to form a global GMM, which represents the aggregate posterior distribution over the object slots. This global GMM is shown on the left, demonstrating that it is tractable to sample from and identifiable up to an equivalence relation.  The right side depicts the process of sampling slot representations from this aggregate posterior.", "section": "Probabilistic Slot Attention"}, {"figure_path": "qmoVQbwmCY/figures/figures_3_1.jpg", "caption": "Figure 2: Graphical models of probabilistic slot attention. (a) Stochastic encoder of standard slot attention [53] with T attention iterations. (b) Proposed model \u2013 each image in the dataset {x}M is encoded into a respective latent representation z \u2208 RN\u00d7d, to which a (local) Gaussian mixture model with K components is fit via expectation maximisation. The resulting K Gaussians serve as slot posterior distributions: sk ~ N(sk; \u03bc\u03ba, \u03c3), for k = 1, ..., K. (c) Aggregate posterior distribution obtained by marginalizing out the data: q(z) = \u2211M=1 q(z | x\u2081)/M. We prove that q(z) is a tractable, non-degenerate Gaussian mixture distribution which: (i) serves as the theoretically optimal prior over slots; (ii) is empirically stable across runs (i.e. identifiable up to an affine transformation and slot permutation); (iii) can be tractably sampled from and used for scene composition tasks.", "description": "This figure shows three graphical models illustrating the probabilistic slot attention approach.  (a) depicts the standard slot attention encoder with T iterations.  (b) details the proposed probabilistic model where each image's latent representation is modeled by a local Gaussian mixture model (GMM) with K components.  The K Gaussians represent the posterior distributions for each slot.  (c) illustrates the aggregate posterior distribution, obtained by marginalizing the local GMMs, which serves as the optimal prior for the slots. This aggregate posterior is shown to be a tractable, non-degenerate GMM, empirically stable across different runs, and usable for tasks like scene composition.", "section": "4 Probabilistic Slot Attention"}, {"figure_path": "qmoVQbwmCY/figures/figures_5_1.jpg", "caption": "Figure 3: Aggregate Gaussian Mixture Density. Examples of aggregate posterior mixtures. For each plot, we compute the aggregate mixture (red line) based on three random bimodal Gaussian mixtures, and plot the respective densities. The three GMMs here are analogous to the local GMMS obtained from probabilistic slot attention (Algorithm 1), and the aggregate GMM represents q(z).", "description": "This figure shows examples of aggregate posterior distributions (red lines) obtained by combining three random bimodal Gaussian Mixture Models (GMMs).  Each set of three GMMs (blue, orange, green dotted lines) represents the local GMMs from the probabilistic slot attention algorithm, and the resulting aggregate GMM (red line) represents the learned prior q(z) over the slots. The figure demonstrates that the aggregate GMM is non-degenerate and stable, which is a key element of the theoretical identifiability results.", "section": "Aggregate Posterior Gaussian Mixture"}, {"figure_path": "qmoVQbwmCY/figures/figures_7_1.jpg", "caption": "Figure 4: Aggregate posterior identifiability. Recovered (latent) aggregate posteriors q(z) across 5 runs of our PSA model. As detailed in Section 6, we used a 2D synthetic dataset with 5 total 'object' clusters, with each observation containing at most 3. This provides strong evidence of recovery of the latent space up to affine transformations, empirically verifying our identifiability claim.", "description": "This figure shows the results of 5 independent runs of the Probabilistic Slot Attention (PSA) model on a synthetic 2D dataset.  Each run resulted in a learned aggregate posterior distribution, q(z), representing the learned latent space.  Despite the different random initializations for each run, the resulting distributions are nearly identical up to affine transformations (rotation, scaling, and translation), which are common transformations that do not change the underlying structure or information contained in the data. This empirically validates the theoretical claim that the PSA model can learn identifiable object-centric representations, even without supervision.", "section": "6 Experiments"}, {"figure_path": "qmoVQbwmCY/figures/figures_7_2.jpg", "caption": "Figure 5: Experiments comparing slot-identifiability scores. Colour coding represents the level of slot identifiability achieved by each model as measured by the SMCC (left) and SIS (right). SMCC offers a more consistent metric correlating with reconstruction loss.", "description": "This figure compares different object-centric learning models based on their slot identifiability.  Two metrics are used: Slot Mean Correlation Coefficient (SMCC) and Slot Identifiability Score (SIS).  The SMCC is highlighted as a more reliable metric that shows a stronger correlation with reconstruction error. The color-coding helps visualize the performance of each model, making it easier to compare them. The plot suggests that the proposed Probabilistic Slot Attention (PSA) method achieves a higher slot identifiability score than other baselines.", "section": "6 Experiments"}, {"figure_path": "qmoVQbwmCY/figures/figures_8_1.jpg", "caption": "Figure 7: Concatenated Slot Gaussian Mixture. Examples of the higher dimensional Gaussian mixture induced by the permutation equivariance of slot attention and the K! ways of concatenating sampled slots. Here we start with a random 1D GMM with K = 3 modes, each representing a different slot distribution, which then induces a respective 3D GMM with K! = 6 modes.", "description": "This figure shows how the permutation equivariance of slot attention leads to a higher-dimensional Gaussian Mixture Model (GMM).  Starting with a simple 1D GMM with three modes (representing three different slot distributions), the permutation of slots generates a 3D GMM with 6 modes (3! = 6 permutations). The top panel shows the 1D GMMs and resulting mixture while the bottom panel shows a 3D visualization of the resulting 6 modes from the slot permutations.", "section": "A List of Assumptions"}, {"figure_path": "qmoVQbwmCY/figures/figures_8_2.jpg", "caption": "Figure 3: Aggregate Gaussian Mixture Density. Examples of aggregate posterior mixtures. For each plot, we compute the aggregate mixture (red line) based on three random bimodal Gaussian mixtures, and plot the respective densities. The three GMMs here are analogous to the local GMMS obtained from probabilistic slot attention (Algorithm 1), and the aggregate GMM represents q(z).", "description": "This figure shows examples of aggregate posterior mixtures obtained by combining three random bimodal Gaussian Mixture Models (GMMs). Each bimodal GMM represents the local slot posterior from a single data point. The resulting aggregate GMM is a mixture of the three local GMMs, reflecting the overall distribution of slot representations across the dataset. The red line represents the aggregate GMM, demonstrating the stable, identifiable distribution of learned object representations (slots).", "section": "Aggregate Posterior Gaussian Mixture"}, {"figure_path": "qmoVQbwmCY/figures/figures_16_1.jpg", "caption": "Figure 7: Concatenated Slot Gaussian Mixture. Examples of the higher dimensional Gaussian mixture induced by the permutation equivariance of slot attention and the K! ways of concatenating sampled slots. Here we start with a random 1D GMM with K = 3 modes, each representing a different slot distribution, which then induces a respective 3D GMM with K! = 6 modes.", "description": "This figure shows how the permutation equivariance property of the slot attention mechanism leads to a higher-dimensional Gaussian Mixture Model (GMM) when concatenating slot samples.  It starts with a simple 1D GMM (3 modes, representing 3 different slot distributions).  Due to the permutation equivariance, there are K! (3! = 6) possible ways to concatenate the slots, resulting in a 3D GMM with 6 modes. Each mode in the 3D GMM corresponds to a unique ordering of the slots. This illustrates how the model's inherent symmetry affects the representation's structure.", "section": "A List of Assumptions"}, {"figure_path": "qmoVQbwmCY/figures/figures_23_1.jpg", "caption": "Figure 8: Identifiability scores throughout training. Our proposed SMCC metric (top) is much more stable than SIS (bottom) in capturing identifiability, and better in discerning differences between methods, showing substantial improvements for PSA.", "description": "The figure shows the training curves of two metrics used to evaluate the identifiability of object-centric representations learned by different models, including the proposed Probabilistic Slot Attention (PSA) and baselines.  The Slot Mean Correlation Coefficient (SMCC) is a more stable and consistent metric showing that PSA demonstrates significantly better identifiability of representations.", "section": "6 Experiments"}, {"figure_path": "qmoVQbwmCY/figures/figures_24_1.jpg", "caption": "Figure 4: Aggregate posterior identifiability. Recovered (latent) aggregate posteriors q(z) across 5 runs of our PSA model. As detailed in Section 6, we used a 2D synthetic dataset with 5 total \u2018object\u2019 clusters, with each observation containing at most 3. This provides strong evidence of recovery of the latent space up to affine transformations, empirically verifying our identifiability claim.", "description": "This figure shows the results of running the Probabilistic Slot Attention (PSA) model five times on a 2D synthetic dataset. Each run produced a slightly different aggregate posterior distribution (q(z)), due to the random initialization in the model.  The fact that these distributions are all very similar, differing only by affine transformations (rotation, translation, scaling), demonstrates that the model consistently recovers the same latent space structure, supporting the paper's claim of identifiability up to affine transformations.", "section": "6 Experiments"}, {"figure_path": "qmoVQbwmCY/figures/figures_25_1.jpg", "caption": "Figure 10: Automatic Relevance Determination (ARD) of slots on the OBJECTSROOM dataset. As shown, when using our proposed probabilistic slot attention (Algorithm 1), the mixing coefficients \u03c0k \u2208 RK, \u2200k automatically approach zero when slots are inactive.", "description": "This figure shows the results of applying Automatic Relevance Determination (ARD) to the slots in the OBJECTSROOM dataset using the proposed probabilistic slot attention algorithm. The mixing coefficients (\u03c0k) for each slot (k) are displayed, showing that inactive slots have mixing coefficients that approach zero.  This demonstrates the effectiveness of ARD in automatically pruning irrelevant slots from the model.", "section": "6 Experiments"}, {"figure_path": "qmoVQbwmCY/figures/figures_26_1.jpg", "caption": "Figure 12: Aggregate posterior sampling for image composition on OBJECTSROOM dataset.", "description": "This figure shows the results of image composition on the OBJECTSROOM dataset using aggregate posterior sampling.  The figure displays several example images, each broken down into its individual object slots (Slot 1 through Slot 7) and the final recomposed image. This demonstrates the model's ability to disentangle objects and recompose them into new images.", "section": "Experiments"}, {"figure_path": "qmoVQbwmCY/figures/figures_26_2.jpg", "caption": "Figure 10: Automatic Relevance Determination (ARD) of slots on the OBJECTSROOM dataset. As shown, when using our proposed probabilistic slot attention (Algorithm 1), the mixing coefficients \u03c0\u2081 \u2208 RK, Vi automatically approach zero when slots are inactive.", "description": "This figure shows the effectiveness of the proposed automatic relevance determination (ARD) method for pruning inactive slots in the OBJECTSROOM dataset.  The mixing coefficients for each slot (\u03c0i) are displayed, indicating that when a slot is not relevant to the image, its corresponding mixing coefficient approaches zero. This demonstrates the method's ability to dynamically determine the relevant number of slots for each input image. ", "section": "6 Experiments"}, {"figure_path": "qmoVQbwmCY/figures/figures_27_1.jpg", "caption": "Figure 10: Automatic Relevance Determination (ARD) of slots on the OBJECTSROOM dataset. As shown, when using our proposed probabilistic slot attention (Algorithm 1), the mixing coefficients \u03c0\u2096 \u2208 \u211dK, \u2200i automatically approach zero when slots are inactive.", "description": "This figure shows the results of applying automatic relevance determination (ARD) to the slots in the OBJECTSROOM dataset.  The mixing coefficients (\u03c0\u2096) for each slot (k) are displayed for several example images, and it is shown that when a slot is not needed to reconstruct an image (i.e., it's inactive), its mixing coefficient approaches zero.  This demonstrates the effectiveness of the proposed ARD method for automatically determining the relevant number of slots needed for each input image.", "section": "Automatic Relevance Determination of Slots"}, {"figure_path": "qmoVQbwmCY/figures/figures_27_2.jpg", "caption": "Figure 10: Automatic Relevance Determination (ARD) of slots on the OBJECTSROOM dataset. As shown, when using our proposed probabilistic slot attention (Algorithm 1), the mixing coefficients \u03c0i \u2208 RK,\u2200i automatically approach zero when slots are inactive.", "description": "This figure shows the results of applying Automatic Relevance Determination (ARD) to the slots in the OBJECTSROOM dataset using the proposed probabilistic slot attention algorithm. The mixing coefficients (\u03c0i) for each slot are displayed, and it can be seen that they tend to zero when a slot is not needed for reconstructing the image, indicating that the model effectively learns to use only the necessary slots.", "section": "6 Experiments"}]