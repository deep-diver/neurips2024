[{"type": "text", "text": "Identifiable Object-Centric Representation Learning via Probabilistic Slot Attention ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Avinash Kori1\u2217 Francesco Locatello2 Ainkaran Santhirasekaram1 Francesca Toni1 Ben Glocker1 Fabio De Sousa Ribeiro1\u2217 ", "page_idx": 0}, {"type": "text", "text": "1 Imperial College London, UK 2 Institute of Science and Technology, Austria a.kori21@imperial.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Learning modular object-centric representations is crucial for systematic generalization. Existing methods show promising object-binding capabilities empirically, but theoretical identifiability guarantees remain relatively underdeveloped. Understanding when object-centric representations can theoretically be identified is crucial for scaling slot-based methods to high-dimensional images with correctness guarantees. To that end, we propose a probabilistic slot-attention algorithm that imposes an aggregate mixture prior over object-centric slot representations, thereby providing slot identifiability guarantees without supervision, up to an equivalence relation. We provide empirical verification of our theoretical identifiability result using both simple 2-dimensional data and high-resolution imaging datasets. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "It has been hypothesized that developing machine learning (ML) systems capable of human-level understanding requires imbuing them with notions of objectness [48, 65]. Objectness notions can be characterised as physical, abstract, semantic, geometric, or via spaces and boundaries [83, 17]. Humans can generalise across environments with few examples to learn from [70], and this has been attributed to our ability to segregate percepts into object entities [64, 25, 45, 4]. ", "page_idx": 0}, {"type": "text", "text": "Obtaining object-centric representations is deemed to be a key step for achieving true compositional generalization [5, 48, 3, 22], and uncovering causal influence between discrete concepts and their environment [57, 19, 20, 65, 4]. Significant progress in learning object-centric representations has been made [15, 16, 44, 66, 10, 67], particularly in unsupervised object discovery settings using an iterative attention mechanism known as Slot Attention (SA) [53]. However, most existing work approaches object-centric representation learning empirically, leaving theoretical understanding relatively underdeveloped. Establishing the identifiability [31, 29] of representations is important as it clarifies under which conditions object-centric representation learning is theoretically possible [6]. ", "page_idx": 0}, {"type": "text", "text": "A well-known result shows that identifiability of latent variables is fundamentally impossible without assumptions about the data generating process [31, 51]. Therefore, understanding when object representations can theoretically be identified is important to scale object-centric methods to high-dimensional images. Recent works [6, 47] make important advances on this by explicitly stating the set of assumptions necessary for providing theoretical identifiability of object-centric representations. However, they restrict their attention to properties of the mixing function, studying a class of models with additive decoders. Although there are merits to this approach, there are practical challenges with the so-called compositional contrast objective [6], as it involves computing Jacobians and requires second-order optimization via gradient descent. Consequently, satisfying the identifiability conditions explicitly (e.g. compositional contrast must be zero) is computationally restrictive for moderately high-dimensional data. In this work, we present a probabilistic perspective that is not subject to the same scalability issues while still providing theoretical identifiability of object-centric representations without supervision. In Table 1, we list object-centric learning methods, their (sometimes implicit) modelling assumptions (see $\\S\\ S$ for additional information and Appendix A for a detailed breakdown and discussion), and their respective identifiability guarantees of object representations. Most methods do not guarantee identifiability, and make the $\\scriptstyle{\\mathit{\\Delta}}_{\\mathit{\\Delta}_{\\mathrm{\\Delta}}{\\mathit{\\Sigma}}_{\\mathrm{\\Delta}}}$ -disentanglement (1) and additive decoder (2) assumptions. Brady et. al. [6] do provide identifiability guarantees and additionally assume irreducibility (3) and compositionality (4). Our method provides identifiability guarantees by introducing latent structure (i.e. via a GMM prior) which generalizes to non-additive decoders. This is advantageous as the computational complexity of additive decoders scales linearly with the number of slots $K$ \u2013 whereas our approach is invariant to $K$ . Moreover, non-additive decoders have been found to significantly improve performance in practice [66, 67, 69], though the theoretical basis is underexplored. Finally, latent structure can reduce the complexity burden on the mixing function $f$ (decoder), making it easier to learn in practice [18, 43]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "table", "img_path": "qmoVQbwmCY/tmp/b1c59395f3817c76c2ad9cb8feb5ec45b16f5d789dda9aa48b23ae03c0e7c6e9.jpg", "table_caption": ["Table 1: Identifiability strategies (mixing function $f$ or latent dist. $p(\\mathbf{z}))$ , and assumptions made by object-centric learning methods. "], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Contributions. Our main contributions are the following: (i) We prove that object-centric representations (i.e. slots) are identifiable without supervision up to an equivalence relation $(\\S\\,5)$ under a latent mixture model specification. To that end, we propose a probabilistic slot-attention algorithm $(\\S\\,^{4})$ which imposes an aggregate mixture prior over slot representations. (ii) We show that our approach induces a non-degenerate (global) Gaussian Mixture Model (GMM) by aggregating per-datapoint (local) GMMs, providing a slot prior which: (a) is empirically stable across runs (i.e. identifiable up to affine transformations and slot permutations); (b) can be tractably sampled from. (iii) We provide conclusive empirical evidence of our theoretical object-centric identifiability result, including visual verification on synthetic 2-dimensional data as well as standard imaging benchmarks $(\\S\\ 6)$ . ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Identifiable Representation Learning. Identifiability of representations stems from early work in independent component analysis (ICA) [31, 29], and is making a resurgence recently [30, 32, 51, 37, 75, 46, 82]. Common strategies for tackling this identifiability problem are: (i) restricting the class of mixing functions; (ii) using non-i.i.d data, interventional data or counterfactuals; and (iii) imposing structure in the latent space via distributional assumptions. Regarding (i), restricting the class of the mixing functions to conformal maps [8] or volume-preserving transformations [81] has been found to produce identifiable models. For (ii), prior works [84, 52, 7, 2, 75] assume access to contrastive pairs of observations $(\\mathbf{x},\\tilde{\\mathbf{x}})$ obtained from either data augmentation, interventions, or approximate counterfactual inference. As for (iii), latent space structure is enforced via either: (a) using auxiliary variables to make latent variables conditionally independent [33, 37, 38]; or (b) distributional assumptions such as placing a mixture prior over the latent variables in a VAE [11, 80, 43]. In this work, we prove an identifiability result via strategy (iii) but within an object-centric learning context, where the latent variables are a set of object slots [53]. ", "page_idx": 1}, {"type": "text", "text": "Object-Centric Learning. Much early work on unsupervised representation learning is based on the Variational Autoencoder (VAE) framework [41], and relies on independence assumptions between latent variables to learn so-called disentangled representations [5, 24, 40, 12, 59]. These methods are closely linked to object-centric representation learning [9, 15, 21], as they also leverage (iterative) variational inference procedures [58, 73, 50]. Alternatively, an iterative attention mechanism known as slot attention (SA) [53] has been the focus of much follow-up work recently [16, 67, 76, 68, 14]. ", "page_idx": 1}, {"type": "image", "img_path": "qmoVQbwmCY/tmp/78dc73d5194e19f6b192c69c606817a7fde3a2a16339c87f9ad5c3f82903150f.jpg", "img_caption": ["Figure 1: Probabilistic slot attention and the identifiable aggregate slot posterior. (Left) Slot posterior GMMs per datapoint (local) and the induced aggregate posterior GMM (global). (Right) Sampling slot representations from the aggregate slot posterior is tractable. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Although slot attention-based methods show promising object binding [22] capabilities empirically on select datasets, they do not provide identifiability guarantees on the learned representations. Recently, [55, 56] assume the access to interventional data-generating process following [1] to demonstrate the identifiability of object-centric representations, while [6, 47] presented the identifiability results for object representations (i.e. slots), clarifying the necessary assumptions and properties of the mixing function (e.g. additive decoders). However, satisfying Brady et. al. [6]\u2019s compositional contrast identifiability condition explicitly (must be zero) requires computationally restrictive secondorder optimization. In contrast, we shift the focus to learning structured object-centric latent spaces via Probabilistic Slot Attention (PSA), bridging the gap between generative model identifiability literature and object-centric representation learning. Notably, our PSA approach is also related to probabilistic capsule routing [27, 63, 62, 61] since slots are equivalent to universal capsules [26], but like slot attention, offers output permutation symmetry and does not face scalability issues. ", "page_idx": 2}, {"type": "text", "text": "3 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notation. Let $\\mathcal{X}\\subseteq\\mathbb{R}^{H\\times W\\times C}$ denote the input image space, where each image $\\mathbf{x}$ is of size $H\\times W$ pixels with $C$ channels. Let $f_{e}:\\mathcal{X}\\to\\mathcal{Z}$ denote an encoder mapping image space to a latent space $\\mathcal{Z}\\subseteq\\mathbb{R}^{N\\times d}$ , where each latent variable $\\mathbf{z}$ consists of $N$ , $d$ -dimensional vectors. Lastly, let $f_{d}:S\\rightarrow\\mathcal{X}$ denote a decoder mapping from slot representation space $\\mathcal{S}\\subseteq\\mathbb{R}^{K\\times d}$ to image space. ", "page_idx": 2}, {"type": "text", "text": "Slot Attention. Slot attention [53] receives a set of feature embeddings $\\mathbf{z}\\,\\in\\,\\mathbb{R}^{N\\times d}$ per input $\\mathbf{x}$ , and applies an iterative attention mechanism to produce $K$ object-centric representations called slots $\\mathbf{s}\\in\\dot{\\mathbb{R}}^{K\\times d}$ . Let $W_{k},W_{v}$ denote $k e y$ and value transformation matrices acting on $\\mathbf{z}$ , and $W_{q}$ the query transformation matrix acting on s. To simplify our exposition later on, let $f_{s}:\\mathcal{Z}\\times\\mathcal{S}\\to\\bar{\\mathcal{S}}$ be shorthand notation for the slot update function, defined as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{s}^{t+1}:=f_{s}(\\mathbf{z},\\mathbf{s}^{t})=\\hat{A}\\mathbf{v},\\quad\\hat{A}_{i j}:=\\frac{A_{i j}}{\\sum_{l=1}^{N}A_{i l}},\\quad A:=\\mathrm{softmax}\\left(\\frac{\\mathbf{q}\\mathbf{k}^{T}}{\\sqrt{d}}\\right)\\in\\mathbb{R}^{K\\times N},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathbf{q}=W_{q}\\mathbf{s}^{t}\\in\\mathbb{R}^{K\\times d}$ , $\\mathbf{k}=W_{k}\\mathbf{z}\\in\\mathbb{R}^{N\\times d}$ , and $\\mathbf{v}=W_{v}\\mathbf{z}\\in\\mathbb{R}^{N\\times d}$ correspond to the query, key and value vectors respectively and $A\\in\\mathbb{R}^{K\\times N}$ is the attention matrix. Unlike self-attention [74], the queries $\\mathbf{q}$ in slot attention are a function of the slots $\\mathbf{s}^{t}$ , and are iteratively refined over $T$ iterations. The initial slots $\\mathbf{s}^{t=0}$ are randomly sampled from a standard Gaussian. The queries at iteration $t$ are given by $\\hat{\\mathbf{q}}^{t}=W_{q}\\mathbf{s}^{t}$ , and the slot update process can be summarized as in Equation 1. ", "page_idx": 2}, {"type": "text", "text": "Compositionality. Compositionality as defined by Brady et al. [6] is a structure imposed on the slot decoder mapping $f_{d}$ which implies that each image pixel is a function of at most one slot representation, thereby enforcing a local sparsity structure on the Jacobian matrix of $f_{d}$ . ", "page_idx": 2}, {"type": "image", "img_path": "qmoVQbwmCY/tmp/8f4846b4836b3b323e715923e27276bf6ed4ad834946567e8d91579677b96d43.jpg", "img_caption": ["", "(b) Probabilistic Slot Attention (c) Aggregate Posterior Mixture "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: Graphical models of probabilistic slot attention. (a) Stochastic encoder of standard slot attention [53] with $T$ attention iterations. (b) Proposed model \u2013 each image in the dataset $\\{\\mathbf{x}_{i}\\}_{i=1}^{M}$ is encoded into a respective latent representation $\\mathbf{z}\\in\\mathbb{R}^{N\\times d}$ , to which a (local) Gaussian mixture model with $K$ components is fit via expectation maximisation. The resulting $K$ Gaussians serve as slot posterior distributions: $\\mathbf{s}_{k}\\sim\\mathcal{N}(\\bar{\\mathbf{s}}_{k};\\mu_{k},\\pmb{\\sigma}_{k}^{2}).$ , for $k=1,\\ldots,K$ . (c) Aggregate posterior distribution onbotna-idneegde bneyr amtea rgGianuaslisziiann g moiuxtt tuhree  ddiasttar:i $\\begin{array}{r}{q(\\mathbf{z})=\\sum_{i=1}^{M}q(\\mathbf{z}\\mid\\mathbf{x}_{i})/M}\\end{array}$ .h e Wthe eporroevtiec tahllayt $q(\\mathbf{z})$ misa l a ptrriaocrt aobvleer, slots; (ii) is empirically stable across runs (i.e. identifiable up to an affine transformation and slot permutation); (iii) can be tractably sampled from and used for scene composition tasks. ", "page_idx": 3}, {"type": "text", "text": "Definition 1 (Compositional Contrast). For a differentiable mapping $f_{d}:\\mathcal{Z}\\to\\mathcal{X}$ , the compositional contrast of $f_{d}$ at ${\\bf z}$ is given by: ", "page_idx": 3}, {"type": "equation", "text": "$$\nC_{\\mathrm{comp}}(f_{d},\\mathbf{z})=\\sum_{n=0}^{N}\\sum_{k=1}^{K}\\sum_{j=k+1}^{K}\\left\\|\\frac{\\partial f_{d}(\\mathbf{z})_{n}}{\\partial\\mathbf{z}_{k}}\\right\\|\\left\\|\\frac{\\partial f_{d}(\\mathbf{z})_{n}}{\\partial\\mathbf{z}_{j}}\\right\\|.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Brady et al. [6]\u2019s main result (Theorem 1) relies on compositionality and invertibility of $f_{d}$ to guarantee slot-identifiability when both the compositional contrast and the reconstruction loss equal zero. However, using $C_{\\mathrm{comp}}(f_{d},\\mathbf{z})$ as a metric or as part of an objective function is computationally prohibitive2. Our method aims to minimize $C_{\\mathrm{comp}}(f_{d},\\mathbf{z})$ implicitly [47], without additive decoders. ", "page_idx": 3}, {"type": "text", "text": "4 Probabilistic Slot Attention ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we present a probabilistic slot attention framework which imposes a mixture prior structure over the slot latent space. This structure will prove to be instrumental in establishing our main identifiability result in Section 5. We begin by approaching standard slot attention [53] from a graphical modelling perspective. As shown in Figure 2 and explained in Section 3, applying slot attention to a deterministic encoding ${\\bf z}=f_{e}({\\bf x})\\in\\bar{\\mathbb{R}}^{N\\times d}$ yields a set of $K$ object slot representations $\\mathbf{s}_{1:K}:=\\mathbf{s}_{1},\\ldots,\\mathbf{s}_{K}$ . This process induces a stochastic encoder $q(\\mathbf{s}_{1:K}\\mid\\mathbf{x})$ , where the stochasticity comes from the random initialization of the slots: $\\mathbf{s}_{1:K}^{t=0}\\sim\\mathcal{N}(\\mathbf{s}_{1:K};0,\\mathbf{I})\\in\\mathbb{R}^{K\\times d}$ . Since each slot is a deterministic function of its previous state $\\mathbf{s}^{t}:=f_{s}(\\mathbf{z},\\mathbf{s}^{t-1})$ it is possible to randomly sample initial states $\\mathbf{s}^{0}$ and obtain stochastic estimates of the slots.3 However, since each transition depends on $\\mathbf{z}$ , which in turn depends on the input $\\mathbf{x}$ , we do not get a generative model we can tractably sample from. This can conceivably be remedied by placing a tractable prior over ${\\bf z}$ and using the VAE framework along the lines of [77], however, here we propose an entirely different approach which does not require making additional variational approximations (see Appendix G for further discussion). ", "page_idx": 3}, {"type": "text", "text": "Local Slot Mixtures. Probabilistic slot attention augments standard slot attention by introducing a per-datapoint (i.e. local) Gaussian Mixture Model (GMM) for learning slot distributions. Intuitively, a local GMM can be understood as a way to cluster features within a given image, encouraging the grouping of similar features into object representations. However, unlike regular clustering, here the clustered points are dynamically transformed representations of the actual data. Specifically, we use an encoder function $f_{e}$ that maps each image $\\mathbf{x}_{i}\\in\\mathbb{R}^{H\\times W\\times C}$ in the dataset $\\{\\mathbf{x}_{i}\\}_{i=1}^{M}$ , to a latent spatial representation $\\mathbf{z}_{i}\\in\\mathbb{R}^{N\\times d}$ . The latent variable $\\mathbf{z}$ may be deterministic or stochastic, and we consider the case where $N<H W$ to reflect a modest downscaling with respect to (w.r.t.) $\\mathbf{x}$ . The goal is to dynamically map each of the $N$ , $d$ -dimensional vector representations in each $\\mathbf{z}$ , to one-of- $\\cal{K}$ object slot distributions within a mixture. A local GMM can be fti to each posterior latent representation $\\mathbf{z}\\sim q(\\mathbf{z}\\mid\\mathbf{x}_{i})^{4}$ on the fly by maximizing likelihood: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\np(\\mathbf{z}\\mid\\pi_{i},\\mu_{i},\\sigma_{i})=\\prod_{n=1}^{N}\\sum_{k=1}^{K}\\pi_{i k}{\\mathcal{N}}(\\mathbf{z}_{n};\\mu_{i k},\\sigma_{i k}^{2}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\pmb{\\mu}_{i}\\,=\\,(\\pmb{\\mu}_{i1},\\dots,\\pmb{\\mu}_{i K})$ , $\\pmb{\\sigma}_{i}^{2}\\,=\\,(\\pmb{\\sigma}_{i1}^{2},\\dots,\\pmb{\\sigma}_{i K}^{2})$ and ${\\pmb\\pi}_{i}\\,=\\,(\\pi_{i1},\\dots,\\pi_{i K})$ are the respective means, diagonal covariances and mixing coefficients of the $i^{\\mathrm{th}}$ $K$ -component mixture. Figure 2b illustrates the resulting probabilistic graphical model (PGM) in more detail. ", "page_idx": 4}, {"type": "text", "text": "To maximize the likelihood in Equation 2 per datapoint $\\mathbf{x}_{i}$ , we present a bespoke expectation-maximisation (EM) algorithm for slot attention, yielding closed-form update equations for the parameters as shown in Algorithm 1, and explained next. ", "page_idx": 4}, {"type": "text", "text": "Probabilistic Projections. A powerful property of slot attention and cross-attention more broadly [74], is its ability to decouple the agreement mechanism from the representational content. That is, the dot-product is used to measure agreement between each query (slot) vector and all the key vectors, to dictate how much of each value vector (content from $\\mathbf{z}_{i}$ ) should be represented in each slot\u2019s revised representation. To retain this flexibility and decouple the attention computation from the content, we incorporate key-value projections into our probabilistic approach. For brevity, the $i$ subscript is im", "page_idx": 4}, {"type": "table", "img_path": "qmoVQbwmCY/tmp/2382bc92ad18f563c3e9bef86f4f44b1f12c29d641b3a5668cc863619cfa8848.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "plicit in the following, keeping in mind that these are local quantities (per-datapoint $\\mathbf{x}_{i}$ ). The parameters of the $K$ Gaussian slot distributions are initialized (at attention iteration $t=0$ ) as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\forall k,\\qquad\\pi(0)_{k}=K^{-1},\\qquad\\mu(0)_{k}\\sim\\mathcal{N}(0,\\mathbf{I}_{d}),\\qquad\\sigma(0)_{k}^{2}=\\mathbf{1}_{d}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The respective queries $\\mathbf{q}$ , keys $\\mathbf{k}$ , and values $\\mathbf{v}$ are then given by: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{q}(t)=W_{q}\\mu(t),\\qquad\\qquad\\mathbf{k}=W_{k}\\mathbf{z},\\qquad\\qquad\\mathbf{v}=W_{v}\\mathbf{z},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{W}_{q},\\mathbf{W}_{k},\\mathbf{W}_{v}\\in\\mathbb{R}^{d\\times d}$ , whereas $\\mathbf{q}(t)$ denotes the queries at attention iteration $t$ . To measure agreement between each input feature (key) and slot (query), we evaluate the normalized probability density of each key under a Gaussian model defined by each slot: ", "page_idx": 4}, {"type": "equation", "text": "$$\nA_{n k}=\\frac{1}{Z}\\pi(t)_{k}\\mathcal{N}\\left(\\mathbf{k}_{n};\\mathbf{q}(t)_{k},\\pmb{\\sigma}(t)_{k}^{2}\\right),\\,\\,\\,\\,\\,\\,\\,\\,Z=\\sum_{j=1}^{K}\\pi(t)_{j}\\mathcal{N}\\left(\\mathbf{k}_{n};\\mathbf{q}(t)_{k},\\pmb{\\sigma}(t)_{j}^{2}\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $A_{n k}$ corresponds to the posterior probability that slot (query) $k$ is responsible for input feature (key) $n$ . This process yields the slot attention matrix $A\\in\\dot{\\mathbb{R}}^{N\\times K}$ . As shown in Algorithm 1, the mixture parameters ${\\pmb\\pi}({\\dot{t}}),{\\pmb\\mu}(t),{\\pmb\\sigma}(t)^{2}$ are then updated using the attention matrix and the values $\\mathbf{v}$ . If the values are chosen to be equal to the keys $\\mathbf{v}:=\\mathbf{k}$ , then the procedure is more in line with standard EM, but the agreement mechanism and the content become entangled. After $T$ probabilistic slot attention iterations, the resulting $K$ Gaussians serve as slot posterior distributions: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\bf s}(T)_{k}\\sim\\mathcal{N}\\left(\\pmb{\\mu}(T)_{k},\\pmb{\\sigma}(T)_{k}^{2}\\right),\\qquad\\qquad\\mathrm{for}~\\,k=1,\\dots,K,}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\pmb{\\mu}(T)$ and $\\sigma(T)^{2}$ are the parameters of all the Gaussians in the mixture given a particular datapoint $\\mathbf{x}$ . The slots $\\mathbf{s}(T)_{1:K}$ are then used for input reconstruction, e.g. by maximizing a (possibly Gaussian) likelihood $p(\\mathbf{x}\\mid\\mathbf{s}(T)_{1:K})$ parameterized by a (possibly additive) decoder $f_{d}$ . ", "page_idx": 4}, {"type": "image", "img_path": "qmoVQbwmCY/tmp/0a847d9fffdef8ef5884d55ce318aba666011b395751067c84b545699f5dcc52.jpg", "img_caption": ["Figure 3: Aggregate Gaussian Mixture Density. Examples of aggregate posterior mixtures. For each plot, we compute the aggregate mixture (red line) based on three random bimodal Gaussian mixtures, and plot the respective densities. The three GMMs here are analogous to the local GMMs obtained from probabilistic slot attention (Algorithm 1), and the aggregate GMM represents $q(\\mathbf{z})$ . "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Computational Complexity. Probabilistic slot attention (PSA) retains the $\\mathcal{O}(T N K d)$ computational complexity of slot attention. The additional operations we introduce for calculating slot mixing coefficients and slot variances (under diagonal slot covariance structure) have complexities of $\\mathcal{O}(N\\bar{K})$ and $\\mathcal{O}(N K d)$ respectively, which do not alter the dominant term. When PSA is combined with an additive decoder, it can lower computational complexity by eliminating the need to decode inactive slots. In the following, we outline a principled approach for pruning inactive slots. ", "page_idx": 5}, {"type": "text", "text": "Automatic Relevance Determination of Slots. An open problem in slot-based modelling is the dynamic estimation of the number of slots $K$ needed for each input [51, 44]. Probabilistic slot attention offers an elegant solution to this problem using the concept of Automatic Relevance Determination (ARD) [60]. ARD is a statistical framework that prunes irrelevant features by imposing data-driven, sparsity-inducing priors on model parameters to regularize the solution space. Since the output mixing coefficients $\\pi\\bar{(T)}\\in\\mathbb{R}^{K}$ are input dependent (i.e. local), irrelevant components (slots) will naturally be pruned after $T$ attention iterations, i.e. $\\pi(T)_{k}\\rightarrow0$ for any unused slot $k$ . We can either use a probability threshold $\\tau\\in[0,1)$ to prune unused slots or place a Dirichlet prior over the mixing coefficients to explicitly induce sparsity. For simplicity, we take the former approach: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{s}_{\\tau}:=\\big\\{\\mathbf{s}(T)_{k}\\ |\\ k\\in[K],\\pi(T)_{k}>\\tau\\big\\},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathbf{s}_{\\tau}$ denotes the set of active slots with mixing coefficient greater than $\\tau$ , and each slot is (optionally) sampled from its Gaussian distribution: $\\breve{\\mathbf{s}(T)}_{k}\\sim\\mathcal{N}\\left(\\breve{\\pmb{\\mu}(T)}_{k},\\pmb{\\sigma}(T)_{k}^{2}\\right)$ . ", "page_idx": 5}, {"type": "text", "text": "Aggregate Posterior Gaussian Mixture. As previously explained, probabilistic slot attention goes beyond standard slot attention by introducing a per-datapoint (i.e. local) GMM to learn distributions over slot representations. This imposes structure over the latent space and gives us access to posterior slot distributions after the attention iterations. Rather than constraining slot posteriors to be close to a tractable prior \u2013 e.g. via the VAE framework [41, 76] which requires further variational approximations \u2013 we leverage our probabilistic setup to compute the optimal (global) prior over slots. ", "page_idx": 5}, {"type": "text", "text": "We propose to compute the aggregate slot posterior by marginalizing out the data: $\\begin{array}{r}{q(\\mathbf{z})=\\sum_{i=1}^{M}q(\\mathbf{z}\\mid}\\end{array}$ $\\mathbf{x}_{i})\\bar{/}M$ , given a pre-trained probabilistic slot attention model (Fig. 3). In $\\S~5$ , we prove that the aggregate posterior is a tractable, non-degenerate Gaussian mixture distribution which: (i) Serves as the theoretically optimal prior over slots; (ii) Is empirically stable across runs (i.e. identifiable up to an affine transformation and slot permutation, $\\S\\ S$ ); (iii) Can be tractably sampled from and (optionally) used for slot-based scene composition tasks. Since GMMs are universal density approximators given enough components (even GMMs with diagonal covariances), the resulting aggregate posterior $q(\\mathbf{z})$ is highly flexible and multimodal. It often suffices to approximate it using a sufficiently large subset of the dataset, if marginalizing out the entire dataset becomes computationally restrictive, although we did not observe this to be the case in practice in our set of experiments. ", "page_idx": 5}, {"type": "text", "text": "5 Theory: Slot Identifiability Result ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we leverage the properties of the probabilistic slot attention method proposed in Section 4 to prove a new object-centric identifiability result. We show that object-centric representations (i.e. slots) are identifiable without supervision (up to an equivalence relation) under mixture model-like assumptions about the latent space. This contrasts with existing work, which provides identifiability guarantees within a specific class of mixing functions, i.e. additive decoders [47]. Our result unifies generative model identifiablity [32, 37, 43] and object-centric learning. ", "page_idx": 5}, {"type": "text", "text": "Definition 2 (Identifiability.). Given an observation space $\\mathcal{X}$ , a probabilistic model $p$ with parameters $\\theta\\in\\Theta$ is said to be identifiable if the mapping $\\theta\\in\\Theta\\mapsto p_{\\theta}(\\mathbf{x})$ is injective: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(p_{\\pmb{\\theta}_{1}}(\\mathbf{x})=p_{\\pmb{\\theta}_{2}}(\\mathbf{x}),\\forall\\mathbf{x}\\in\\mathcal{X})\\implies\\pmb{\\theta}_{1}=\\pmb{\\theta}_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Remark 1. Definition 2 says that if any two choices of model parameters lead to the same marginal density, they are equal. This is often referred to as strong identifiability [32, 37], and it can be too restrictive, as guaranteeing identifiability up to a simple transformation (e.g. affine) is acceptable in practice. To reflect weaker notions of identifiability, we let $\\sim$ denote an equivalence relation on $\\Theta$ , such that a model can be said to be identifiable up to $\\sim$ , or $\\sim$ -identifiable. ", "page_idx": 6}, {"type": "text", "text": "Definition 3 ( $\\sim_{s}$ -equivalence). Let $f_{\\theta}:S\\rightarrow\\mathcal{X}$ denote a mapping from slot representation space $\\boldsymbol{S}$ to image space $\\mathcal{X}$ (satisfying Assumption 8), the equivalence relation $\\sim_{s}$ w.r.t. to parameters $\\theta\\in\\Theta$ is defined bellow, where $\\bar{P^{\\in}}\\,\\bar{P}\\subseteq\\bar{\\{0,1\\}}^{K\\times K}$ is a slot permutation matrix, $H\\in\\mathbb{R}^{d\\times d}$ is an affine transformation matrix, and $\\mathbf{c}\\in\\mathbb{R}^{d}$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\forall\\mathbf{x}\\in\\mathcal{X},\\qquad\\theta_{1}\\sim_{s}\\theta_{2}\\iff\\exists P,H,\\mathbf{c}:f_{\\theta_{1}}^{-1}(\\mathbf{x})=P(f_{\\theta_{2}}^{-1}(\\mathbf{x})H+\\mathbf{c}).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Lemma 1 (Aggregate Posterior Mixture). Given that probabilistic slot attention induces a local (per-datapoint $\\mathbf{\\bar{x}}\\in\\{\\mathbf{x}_{i}\\}_{i=1}^{M})$ GMM with $K$ components, the aggregate posterior $q(\\mathbf{z})$ obtained by marginalizing out $\\mathbf{x}$ is a non-degenerate global Gaussian mixture with $M K$ components: ", "page_idx": 6}, {"type": "equation", "text": "$$\nq(\\mathbf{z})=\\frac{1}{M}\\sum_{i=1}^{M}\\sum_{k=1}^{K}\\widehat{\\pi}_{i k}\\mathcal{N}\\left(\\mathbf{z};\\widehat{\\mu}_{i k},\\widehat{\\pmb{\\sigma}}_{i k}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Proof Sketch. The full proof is given in Appendix B. The result is obtained by integrating the product of the latent variable posterior density $q(\\mathbf{z}\\mid\\mathbf{x})$ and the (local) GMM density given ${\\bf z}$ , w.r.t. x. We then proceed by verifying that the mixing coefficients sum to one over all the components in the new mixture (Corollary 4), proving aggregated posterior to be a well-defined probability distribution, this can be empirically confirmed in Figure 3. Next, we use $q(\\mathbf{z})$ in our identifiablity result. ", "page_idx": 6}, {"type": "text", "text": "Theorem 1 (Mixture Distribution of Concatenated Slots). Let $f_{s}$ denote a permutation equivariant PSA function such that $f_{s}(\\mathbf{z},P\\mathbf{s}^{t})=P f_{s}(\\mathbf{z},\\mathbf{s}^{t}),$ , where $P\\in\\{0,1\\}^{K\\times K}$ is an arbitrary permutation matrix. Let $\\mathbf{s}\\,=\\,(\\mathbf{s}_{1},\\hdots,\\mathbf{s}_{K})\\,\\in\\,\\mathbb{R}^{K d}$ be a random variable defined as the concatenation of $K$ individual slots, where each slot is Gaussian distributed within a $K$ -component mixture: $\\mathbf{s}_{k}\\sim$ $\\mathcal{N}(\\pmb{\\mu}_{k},\\pmb{\\Sigma}_{k})\\in\\mathbb{R}^{d},\\forall k\\in\\{1,\\dots K\\}$ . Then, s is also GMM distributed with $K!$ mixture components: ", "page_idx": 6}, {"type": "equation", "text": "$$\np(\\mathbf{s})=\\sum_{p=1}^{K!}\\pi_{p}\\mathcal{N}(\\mathbf{s};\\mu_{p},\\boldsymbol{\\Sigma}_{p}),\\quad w h e r e\\quad\\pi\\in\\Delta^{K!-1},\\quad\\mu_{p}\\in\\mathbb{R}^{K d},\\quad\\boldsymbol{\\Sigma}_{p}\\in\\mathbb{R}^{K d\\times K d}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Proof Sketch. The proof is in Appendix B. We observe that the permutation equivariance of the PSA function $f_{s}$ induces $K!$ ways of concatenating sampled slots $\\mathbf{s}_{k}$ , where each permutation maps to a different mode with block diagonal covariance structure in a GMM living in $\\mathring{\\mathbb{R}}^{K d}$ (e.g. Fig. 6, 7). ", "page_idx": 6}, {"type": "text", "text": "Theorem 2 $\\left[\\sim_{s}\\right.$ -Identifiable Slot Representations). Given that the aggregate posterior $q(\\mathbf{z})$ is an optimal, non-degenerate mixture prior over slot space (Lemma $^{\\,l}$ ), $f:S\\rightarrow\\mathcal{X}$ is a piecewise affine weakly injective mixing function (Assumption 8), and the slot representation, $\\mathbf{s}=(\\mathbf{s}_{1},\\dots,\\mathbf{s}_{K})$ can be observed as a sample from a GMM (Theorem 1), then $p(\\mathbf{s})$ is identifiable as per Definition 3. ", "page_idx": 6}, {"type": "text", "text": "Proof Sketch. The proof is given in Appendix D. Lemma 1 and Corollary 4 state that the optimal latent variable prior in our case is GMM distributed, non-degenerate and equates to the aggregate posterior $q(\\mathbf{z})$ . This permits us to extend [43]\u2019s result to show that if $q(\\mathbf{z})$ is distributed according to a non-degenerate GMM and the mixing function $f_{d}$ is piecewise affine and weakly injective, then the slot distribution representation, $p(\\mathbf{s})$ which is also a GMM (Theorem 1) is identifiable up to an affine transformation and arbitrary slot permutation. ", "page_idx": 6}, {"type": "text", "text": "Corollary 3 (Individual Slot Identifiability). If the distribution over concatenated slots $p(\\mathbf{s})$ , where $\\mathbf{s}\\;=\\;(\\mathbf{s}_{1},\\ldots,\\mathbf{s}_{K})\\;\\in\\;\\mathbb{R}^{K d}$ , is $\\sim_{s}$ -identifiable (Theorem 2) then this implies $q(\\mathbf{z})$ is identifiable up to an affine transformation and permutation of the slots $\\mathbf{s}_{k}$ . Therefore, each slot distribution $\\dot{\\mathbf{s}_{k}}\\sim\\mathcal{N}(\\bar{\\mu_{k}},\\Sigma_{k})\\in\\mathbb{R}^{d},\\forall k\\in\\{1,\\dots K\\}$ is also identifiable up to an affine transformation. ", "page_idx": 6}, {"type": "image", "img_path": "qmoVQbwmCY/tmp/47a1165189e0e29a4a8976eb4ef7911b4ada2348bfdfb268bd040bc39da732ae.jpg", "img_caption": ["Figure 4: Aggregate posterior identifiability. Recovered (latent) aggregate posteriors $q(\\mathbf{z})$ across 5 runs of our PSA model. As detailed in Section 6, we used a 2D synthetic dataset with 5 total \u2018object clusters, with each observation containing at most 3. This provides strong evidence of recovery of the latent space up to affine transformations, empirically verifying our identifiability claim. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "qmoVQbwmCY/tmp/dd5347e19b86838d210fac60741dc15008f4ce9f258065f23fd8ac603e6a119c.jpg", "img_caption": ["Figure 5: Experiments comparing slot-identifiability scores. Colour coding represents the level of slot identifiability achieved by each model as measured by the SMCC (left) and SIS (right). SMCC offers a more consistent metric correlating with reconstruction loss. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Given that the focus of this work is theoretical, the primary goal of our experiments is to provide strong empirical evidence of our main identifiability result (ref. Figures 4, 5). With that said, we also extend our experimental study to popular imaging benchmarks to demonstrate that our method scales to higher-dimensional settings (ref. Tables 2, 4). ", "page_idx": 7}, {"type": "text", "text": "Datasets & Evaluation Metrics. Our experimental analysis involves standard benchmark datasets from object-centric learning literature including SPRITEWORLD [6], CLEVR [34], and OBJECTSROOM [35]. We report the foreground-adjusted rand index (FG-ARI) and FID [23] to quantify both object-level binding capabilities and image quality. Our main goal is to measure slot-identifiability, rseo prwees eunstea ttihoen ss l\u2013o t wide ecnatlilf tiahbei llitatyt esrc oslroet -[6M] CanCd ( tShMe CmCe)a.n  Fcoorr trewloa tisoetns  coof esflfoictis $\\{{\\bf s}_{i}\\}_{i=1}^{M}$ ,C a) nadc $\\{\\widetilde{\\mathbf{s}}_{i}\\}_{i=1}^{M}$ where , , extracted from images , the SMCC between any s and \u02dcs is obtained by matching the slot representations and their order. The order is matched by mapping slots in \u02dcs w.r.t s assigned by $\\pi$ , followed by a learned affine mapping $\\pmb{A}$ between aligned $\\tilde{\\mathbf{s}}_{\\pi(i)}$ and $\\mathbf{s}$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathrm{SMCC}(\\mathbf{s},\\tilde{\\mathbf{s}}):=\\frac{1}{K\\times d}\\sum_{i=0}^{K}\\sum_{j=0}^{d}\\rho(\\mathbf{s}_{i j},A\\tilde{\\mathbf{s}}_{\\pi(i)j}).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "For more details on the metrics please refer to Appendix F. ", "page_idx": 7}, {"type": "text", "text": "Models & Baselines. We consider three ablations on our proposed probabilistic slot attention (PSA) method: (i) PSA base model (Algorithm 2); (ii) PSA-PROJ model (Algorithm 1); and (iii) PSA-NOA model, which is equivalent to PSA-PROJ but without an additive decoder. We experiment with two types of decoders: (i) an additive decoder similar to [79]\u2019s spatial broadcasting model; and (ii) standard convolutional decoder. In all cases, we use LeakyReLU activations to satisfy the weak injectivity conditions (Assumption 8). In terms of object-centric learning baselines, we compare with standard additive autoencoder setups following [6], slot-attention (SA) [53], and MONET [9]. ", "page_idx": 7}, {"type": "image", "img_path": "qmoVQbwmCY/tmp/b99af8cc7bbbb6c025eb81b90bc4433e8692f5b42af7df55c1994d0fa65290ba.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "qmoVQbwmCY/tmp/47358fac42e09b75d839cab828111d510a124160c503bc9e4c5ad8f1897d3d3d.jpg", "img_caption": ["Figure 6: Concatenated Slot Gaussian Mixture. Example of the higher-dim GMM $(1\\mathrm{D}\\rightarrow3\\mathrm{D})$ induced by the permutation equivariance of PSA and the $K!$ ways of concatenating sampled slots. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Verifying Slot Identifiability: Gaussian Mixture of Objects. To provide conclusive empirical evidence of our identifiability claim (Theorem 2), we set up a synthetic modelling scenario under which it is possible to visualize the aggregate posterior across different runs. The goal is to show that PSA is $\\sim_{s}$ -identifiable (Theorem 2) in the sense that it can recover the same latent space distribution up to an affine transformation and slot order permutation. For the data generating process, we defined a $K{=}5$ component GMM, with differing mean parameters $\\{\\mu_{1},\\cdot\\cdot\\cdot,\\mu_{5}\\}$ , and shared isotropic covariances. The 5 components emulate 5 different object types in a given environment. To create a single data point, we randomly chose 3 of the 5 components and sampled 128 points uniformly at random from each mode. Figure 9 shows some data samples, where different colours correspond to different objects. We used 1000 data points in total for training our PSA model. As shown in Figure 4, the aggregate posterior is either rotated, translated, skewed, or flipped across different runs as predicted by our theory \u2013 this contrasts with all baselines wherein the aggregate posterior is intractable. We observed an SMCC of $0.93\\pm0.04$ , and R2-score of $0.50\\pm0.08$ . ", "page_idx": 8}, {"type": "text", "text": "Case Study: Imaging Data. Although our focus is primarily theoretical, we now demonstrate that our method generalizes/scales to higher-dimensional imaging modalities. To that end, we first use the SPRITEWORLD [78] dataset to evaluate the SMCC and SIS w.r.t. ground truth latent variables. Figure 5 presents our identifiability results against the baselines. Similar to [6], we observe higher SIS when compositional contrast and reconstruction error decreases. However, when the mixing function is not additive, the compositional contrast does not decrease drastically while maintaining higher SMCC and SIS \u2013 this verifies our identifiability claim using only piecewise affine decoders. As shown in Figure 5, we also observe that PSA routing with additive decoder models achieves lower compositional contrast and reconstruction errors when compared with other methods. This indicates that stronger identifiability of slot representations is achievable when combining both slot latent structure and inductive biases in the mixing function. Unlike for the SPRITEWORLD dataset, all the ground truth generative factors are unobserved for the CLEVR and OBJECTSROOM datasets we use. Therefore, for evaluation in these cases, we train multiple models with different seeds and compute the SMCC and SIS measures across different runs. This is similar to our earlier synthetic experiment and standard practice in identifiability literature. Table 2 presents our main identifiability results on CLEVR and OBJECTSROOM. We observe similar trends in favour of our proposed PSA method as measured by both SMCC and (slot averaged) R2 score relative to the baselines. ", "page_idx": 8}, {"type": "text", "text": "Case Study: Complex Decoder Structure. To empirically test slot identifiability using more complex non-additive decoders, we used transformer decoders following SLATE [67], and simply replaced the slot attention module with probabilistic slot attention. On the CLEVR dataset, we observed a significantly improved SMCC of $\\mathbf{0.73\\pm.01}$ and R2 of ${\\bf0.55\\pm.06}$ relative to Table 2. ", "page_idx": 8}, {"type": "text", "text": "To demonstrate that probabilistic slot attention can scale to large-scale real-world data we ran additional experiments on the Pascal VOC2012 [49] dataset, following the exact DINOSAUR strategies and setups described in [66, 36] for fairness, then simply swapping out the slot attention module with probabilistic slot attention. As shown in Table 3, we find that probabilistic slot attention is competitive with standard slot attention on real-world data. We also tested more complex, non-additive decoders based on autoregressive transformers, following the DINOSAUR [66] setup. For our PSA TRANSFORMER (W/ DINO) model, we observed an $\\mathbf{MBO}_{i}$ of 0.447, and $\\mathbf{M}\\mathbf{B}\\mathbf{O}_{c}$ of 0.521 which is competitive with the DINOSAUR TRANSFORMER baseline [66] of 0.44 and 0.512 respectively. In this case, we found that a lower maximum learning rate of $10^{-4}$ was beneficial for stabilizing ", "page_idx": 9}, {"type": "table", "img_path": "qmoVQbwmCY/tmp/6e08ff571e8ac33c0eb5565446ba80f3c800cc446b2291def46602da39643de4.jpg", "table_caption": ["Table 3: Pascal VOC2012 benchmark results using probabilistic slot attention (PSA). All baselines are standard results from [54]. SA MLP (W/ DINO) denotes our replication of the DINOSAUR MLP baseline from [66], whereas $(\\ddagger)$ denotes the use of slot attention masks rather than decoder alpha masks for evaluation. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "PSA training. In summary, our experiments corroborate our theoretical results and suggest why non-additive decoder structures can still work well given the appropriate latent structure and inference procedures are in place. With that said, there is a trade-off between identifiability and expressivity induced by the choice of decoder structure [47], so depending on the use case, it may indeed be advantageous to combine both latent and additive decoder structures in practice. ", "page_idx": 9}, {"type": "text", "text": "7 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Understanding when object-centric representations can theoretically be identified is important for scaling slot-based methods to high-dimensional images with correctness guarantees. In contrast with existing work, which focuses primarily on properties of the slot mixing function, we leverage distributional assumptions about the slot latent space to prove a new slot-identifiability result. Specifically, we prove that object-centric representations are identifiable without supervision (up to an equivalence relation) under mixture model-like distributional assumptions on the latent slots. To that end, we proposed a probabilistic slot-attention algorithm that imposes an aggregate mixture prior over slot representations which is both demonstrably stable across runs and tractable to sample from. Our empirical study primarily verifies our theoretical identifiability claim and demonstrates that our framework achieves the lowest compositional contrast without being explicitly trained towards that objective, which is computationally infeasible beyond toy datasets. In summary, we show how slot identifiability can be achieved via probabilistic constraints on the latent space and piecewise decoders. These piecewise decoders manifest as e.g. standard MLPs with LeakyReLU activations and are generally less restrictive than additive decoders. When coupling probabilistic and additive decoder structures, we observe further performance improvements relative to either one in isolation. ", "page_idx": 9}, {"type": "text", "text": "Limitations & Future Work. We recognize that our assumptions, particularly the weak injectivity of the mixing function, may not always hold in practice for different types of architectures (see Appendix C for sufficiency conditions). Although generally applicable, the piecewise-affine functions we use may not always accurately reflect valid assumptions about real-world problems, e.g. when the model is misspecified. Like all object-centric learning methods, we also assume that the mixing function is invariant to permutations of the slots in practice, which technically makes it non-invertible. We deem this aspect to be an interesting area for future work, as an extension to accommodate permutation invariance would strengthen and generalize the identifiability guarantees we provide. Additionally, we do not study cases where objects are occluded, i.e. when are shared or bordering other objects. This limitation is not unique to our work [53, 6, 14, 15, 44] and overcoming it requires further investigation by the community. Nonetheless, our theoretical results capture the important concepts in object-centric learning and represent a valuable extension to the nascent theoretical foundations of the area. In future work, it would be valuable to further relax slot identifiability requirements/assumptions and study slot compositional properties of probabilistic slot attention. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "A. Kori is supported by UKRI (grant number EP/S023356/1), as part of the UKRI Centre for Doctoral Training in Safe and Trusted AI. B. Glocker and F.D.S. Ribeiro acknowledge the support of the UKRI AI programme, and the Engineering and Physical Sciences Research Council, for CHAI - EPSRC Causality in Healthcare AI Hub (grant number EP/Y028856/1). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Kartik Ahuja, Jason S Hartford, and Yoshua Bengio. Weakly supervised representation learning with sparse perturbations. Advances in Neural Information Processing Systems, 35:15516\u2013 15528, 2022. [Cited on page 3.]   \n[2] Kartik Ahuja, Yixin Wang, Divyat Mahajan, and Yoshua Bengio. Interventional causal representation learning. arXiv preprint arXiv:2209.11924, 2022. [Cited on page 2.]   \n[3] Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018. [Cited on page 1.]   \n[4] Timothy EJ Behrens, Timothy H Muller, James CR Whittington, Shirley Mark, Alon B Baram, Kimberly L Stachenfeld, and Zeb Kurth-Nelson. What is a cognitive map? organizing knowledge for flexible behavior. Neuron, 100(2):490\u2013509, 2018. [Cited on page 1.]   \n[5] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798\u2013 1828, 2013. [Cited on pages 1 and 2.]   \n[6] Jack Brady, Roland S Zimmermann, Yash Sharma, Bernhard Sch\u00f6lkopf, Julius von K\u00fcgelgen, and Wieland Brendel. Provably learning object-centric representations. arXiv preprint arXiv:2305.14229, 2023. [Cited on pages 1, 2, 3, 4, 8, 9, 10, 17, 21, and 24.]   \n[7] Johann Brehmer, Pim De Haan, Phillip Lippe, and Taco S Cohen. Weakly supervised causal representation learning. Advances in Neural Information Processing Systems, 35:38319\u201338331, 2022. [Cited on page 2.]   \n[8] Simon Buchholz, Michel Besserve, and Bernhard Sch\u00f6lkopf. Function classes for identifiable nonlinear independent component analysis. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. [Cited on page 2.]   \n[9] Christopher P Burgess, Loic Matthey, Nicholas Watters, Rishabh Kabra, Irina Higgins, Matt Botvinick, and Alexander Lerchner. Monet: Unsupervised scene decomposition and representation. arXiv preprint arXiv:1901.11390, 2019. [Cited on pages 2 and 9.]   \n[10] Michael Chang, Thomas L Griffiths, and Sergey Levine. Object representations as fixed points: Training iterative refinement algorithms with implicit differentiation. arXiv preprint arXiv:2207.00787, 2022. [Cited on page 1.]   \n[11] Nat Dilokthanakul, Pedro AM Mediano, Marta Garnelo, Matthew CH Lee, Hugh Salimbeni, Kai Arulkumaran, and Murray Shanahan. Deep unsupervised clustering with gaussian mixture variational autoencoders. arXiv preprint arXiv:1611.02648, 2016. [Cited on pages 2 and 25.]   \n[12] Cian Eastwood and Christopher K. I. Williams. A framework for the quantitative evaluation of disentangled representations. In International Conference on Learning Representations, 2018. [Cited on page 2.]   \n[13] Gamaleldin Elsayed, Aravindh Mahendran, Sjoerd van Steenkiste, Klaus Greff, Michael C Mozer, and Thomas Kipf. Savi++: Towards end-to-end object-centric learning from real-world videos. Advances in Neural Information Processing Systems, 35:28940\u201328954, 2022. [Cited on page 2.] [14] Patrick Emami, Pan He, Sanjay Ranka, and Anand Rangarajan. Slot order matters for compositional scene understanding. arXiv preprint arXiv:2206.01370, 2022. [Cited on pages 2 and 10.] [15] Martin Engelcke, Adam R Kosiorek, Oiwi Parker Jones, and Ingmar Posner. Genesis: Generative scene inference and sampling with object-centric latent representations. arXiv preprint arXiv:1907.13052, 2019. [Cited on pages 1, 2, and 10.] [16] Martin Engelcke, Oiwi Parker Jones, and Ingmar Posner. Genesis-v2: Inferring unordered object representations without iterative refinement. Advances in Neural Information Processing Systems, 34:8085\u20138094, 2021. [Cited on pages 1 and 2.] [17] Russell A Epstein, Eva Zita Patai, Joshua B Julian, and Hugo J Spiers. The cognitive map in humans: spatial navigation and beyond. Nature neuroscience, 20(11):1504\u20131513, 2017. [Cited on page 1.] [18] Fabian Falck, Haoting Zhang, Matthew Willetts, George Nicholson, Christopher Yau, and Chris C Holmes. Multi-facet clustering variational autoencoders. Advances in Neural Information Processing Systems, 34:8676\u20138690, 2021. [Cited on page 2.] [19] Tobias Gerstenberg, Noah D Goodman, David A Lagnado, and Joshua B Tenenbaum. A counterfactual simulation model of causal judgments for physical events. Psychological review,   \n128(5):936, 2021. [Cited on page 1.] [20] Alison Gopnik, Clark Glymour, David M Sobel, Laura E Schulz, Tamar Kushnir, and David Danks. A theory of causal learning in children: causal maps and bayes nets. Psychological review, 111(1):3, 2004. [Cited on page 1.] [21] Klaus Greff, Rapha\u00ebl Lopez Kaufman, Rishabh Kabra, Nick Watters, Christopher Burgess, Daniel Zoran, Loic Matthey, Matthew Botvinick, and Alexander Lerchner. Multi-object representation learning with iterative variational inference. In International Conference on Machine Learning, pages 2424\u20132433. PMLR, 2019. [Cited on page 2.] [22] Klaus Greff, Sjoerd Van Steenkiste, and J\u00fcrgen Schmidhuber. On the binding problem in artificial neural networks. arXiv preprint arXiv:2012.05208, 2020. [Cited on pages 1 and 3.] [23] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017. [Cited on page 8.] [24] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-VAE: Learning basic visual concepts with a constrained variational framework. In International Conference on Learning Representations,   \n2017. [Cited on page 2.] [25] Geoffrey Hinton. Some demonstrations of the effects of structural descriptions in mental imagery. Cognitive Science, 3(3):231\u2013250, 1979. [Cited on page 1.] [26] Geoffrey Hinton. How to represent part-whole hierarchies in a neural network. Neural Computation, pages 1\u201340, 2022. [Cited on page 3.] [27] Geoffrey E Hinton, Sara Sabour, and Nicholas Frosst. Matrix capsules with em routing. In International conference on learning representations, 2018. [Cited on page 3.] [28] Matthew D Hoffman and Matthew J Johnson. Elbo surgery: yet another way to carve up the variational evidence lower bound. In Workshop in Advances in Approximate Bayesian Inference, NIPS, volume 1, 2016. [Cited on pages 18 and 25.] [29] A Hyvarinen and E Oja. Independent component analysis: algorithms and applications. Neural Networks, 13(4-5):411\u2013430, 2000. [Cited on pages 1 and 2.] [30] Aapo Hyvarinen and Hiroshi Morioka. Unsupervised feature extraction by time-contrastive learning and nonlinear ica. Advances in neural information processing systems, 29, 2016. [Cited on page 2.]   \n[31] Aapo Hyv\u00e4rinen and Petteri Pajunen. Nonlinear independent component analysis: Existence and uniqueness results. Neural networks, 12(3):429\u2013439, 1999. [Cited on pages 1 and 2.]   \n[32] Aapo Hyvarinen, Hiroaki Sasaki, and Richard Turner. Nonlinear ica using auxiliary variables and generalized contrastive learning. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 859\u2013868. PMLR, 2019. [Cited on pages 2, 6, and 7.]   \n[33] Aapo Hyvarinen, Hiroaki Sasaki, and Richard Turner. Nonlinear ica using auxiliary variables and generalized contrastive learning. In Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics, volume 89, pages 859\u2013868. PMLR, 2019. [Cited on page 2.]   \n[34] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2901\u20132910, 2017. [Cited on page 8.]   \n[35] Rishabh Kabra, Chris Burgess, Loic Matthey, Raphael Lopez Kaufman, Klaus Greff, Malcolm Reynolds, and Alexander Lerchner. Multi-object datasets. https://github.com/deepmind/multiobject-datasets/, 2019. [Cited on page 8.]   \n[36] Ioannis Kakogeorgiou, Spyros Gidaris, Konstantinos Karantzalos, and Nikos Komodakis. Spot: Self-training with patch-order permutation for object-centric learning with autoregressive transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22776\u201322786, 2024. [Cited on page 10.]   \n[37] Ilyes Khemakhem, Diederik Kingma, Ricardo Monti, and Aapo Hyvarinen. Variational autoencoders and nonlinear ica: A unifying framework. In International Conference on Artificial Intelligence and Statistics, pages 2207\u20132217. PMLR, 2020. [Cited on pages 2, 6, 7, 17, and 24.]   \n[38] Ilyes Khemakhem, Ricardo Monti, Diederik Kingma, and Aapo Hyvarinen. Ice-beem: Identifiable conditional energy-based deep models based on nonlinear ica. In Advances in Neural Information Processing Systems, volume 33, 2020. [Cited on pages 2, 17, and 24.]   \n[39] Ilyes Khemakhem, Ricardo Monti, Diederik Kingma, and Aapo Hyvarinen. Ice-beem: Identifiable conditional energy-based deep models based on nonlinear ica. Advances in Neural Information Processing Systems, 33:12768\u201312778, 2020. [Cited on page 24.]   \n[40] Hyunjik Kim and Andriy Mnih. Disentangling by factorising. In Proceedings of the 35th International Conference on Machine Learning, Proceedings of Machine Learning Research, 2018. [Cited on page 2.]   \n[41] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. [Cited on pages 2 and 6.]   \n[42] Thomas Kipf, Gamaleldin F Elsayed, Aravindh Mahendran, Austin Stone, Sara Sabour, Georg Heigold, Rico Jonschkowski, Alexey Dosovitskiy, and Klaus Greff. Conditional object-centric learning from video. arXiv preprint arXiv:2111.12594, 2021. [Cited on page 2.]   \n[43] Bohdan Kivva, Goutham Rajendran, Pradeep Ravikumar, and Bryon Aragam. Identifiability of deep generative models without auxiliary information. Advances in Neural Information Processing Systems, 35:15687\u201315701, 2022. [Cited on pages 2, 6, 7, 17, 21, and 22.]   \n[44] Avinash Kori, Francesco Locatello, Fabio De Sousa Ribeiro, Francesca Toni, and Ben Glocker. Grounded object centric learning. arXiv preprint arXiv:2307.09437, 2023. [Cited on pages 1, 2, 6, and 10.]   \n[45] Tejas D Kulkarni, William F Whitney, Pushmeet Kohli, and Josh Tenenbaum. Deep convolutional inverse graphics network. Advances in neural information processing systems, 28, 2015. [Cited on page 1.]   \n[46] S\u00e9bastien Lachapelle, Pau Rodr\u00edguez L\u00f3pez, Yash Sharma, Katie Everett, R\u00e9mi Le Priol, Alexandre Lacoste, and Simon Lacoste-Julien. Nonparametric partial disentanglement via mechanism sparsity: Sparse actions, interventions and sparse temporal dependencies. arXiv preprint arXiv:2401.04890, 2024. [Cited on page 2.] [47] S\u00e9bastien Lachapelle, Divyat Mahajan, Ioannis Mitliagkas, and Simon Lacoste-Julien. Additive decoders for latent variables identification and cartesian-product extrapolation. Advances in Neural Information Processing Systems, 36, 2024. [Cited on pages 1, 3, 4, 6, 10, and 17.] [48] Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building machines that learn and think like people. Behavioral and brain sciences, 40:e253, 2017. [Cited on page 1.] [49] Kunpeng Li, Ziyan Wu, Kuan-Chuan Peng, Jan Ernst, and Yun Fu. Tell me where to look: Guided attention inference network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 9215\u20139223, 2018. [Cited on page 10.] [50] Zhixuan Lin, Yi-Fu Wu, Skand Vishwanath Peri, Weihao Sun, Gautam Singh, Fei Deng, Jindong Jiang, and Sungjin Ahn. Space: Unsupervised object-oriented scene representation via spatial attention and decomposition. arXiv preprint arXiv:2001.02407, 2020. [Cited on page 2.] [51] Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard Sch\u00f6lkopf, and Olivier Bachem. Challenging common assumptions in the unsupervised learning of disentangled representations. In international conference on machine learning, pages 4114\u2013   \n4124. PMLR, 2019. [Cited on pages 1, 2, and 6.] [52] Francesco Locatello, Ben Poole, Gunnar R\u00e4tsch, Bernhard Sch\u00f6lkopf, Olivier Bachem, and Michael Tschannen. Weakly-supervised disentanglement without compromises. In International Conference on Machine Learning, pages 6348\u20136359. PMLR, 2020. [Cited on page 2.] [53] Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot attention. Advances in Neural Information Processing Systems, 33:11525\u201311538, 2020. [Cited on pages 1, 2, 3, 4, 9, and 10.] [54] Sindy L\u00f6we, Phillip Lippe, Francesco Locatello, and Max Welling. Rotating features for object discovery. Advances in Neural Information Processing Systems, 36, 2024. [Cited on page 10.] [55] Amin Mansouri, Jason Hartford, Kartik Ahuja, and Yoshua Bengio. Object-centric causal representation learning. In NeurIPS 2022 Workshop on Symmetry and Geometry in Neural Representations, 2022. [Cited on page 3.] [56] Amin Mansouri, Jason Hartford, Yan Zhang, and Yoshua Bengio. Object-centric architectures enable efficient causal representation learning. arXiv preprint arXiv:2310.19054, 2023. [Cited on page 3.] [57] Gary F Marcus. The algebraic mind: Integrating connectionism and cognitive science. MIT press, 2003. [Cited on page 1.] [58] Joe Marino, Yisong Yue, and Stephan Mandt. Iterative amortized inference. In International Conference on Machine Learning, pages 3403\u20133412. PMLR, 2018. [Cited on page 2.] [59] Emile Mathieu, Tom Rainforth, N Siddharth, and Yee Whye Teh. Disentangling disentanglement in variational autoencoders. In Proceedings of the 36th International Conference on Machine Learning, 2019. [Cited on page 2.] [60] Radford M. Neal. Bayesian Learning for Neural Networks. Springer-Verlag, Berlin, Heidelberg,   \n1996. [Cited on page 6.] [61] Fabio De Sousa Ribeiro, Kevin Duarte, Miles Everett, Georgios Leontidis, and Mubarak Shah. Learning with capsules: A survey. arXiv preprint arXiv:2206.02664, 2022. [Cited on page 3.] [62] Fabio De Sousa Ribeiro, Georgios Leontidis, and Stefanos Kollias. Capsule routing via variational bayes. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 3749\u20133756, 2020. [Cited on page 3.] [63] Fabio De Sousa Ribeiro, Georgios Leontidis, and Stefanos Kollias. Introducing routing uncertainty in capsule networks. In Advances in Neural Information Processing Systems, volume 33, pages 6490\u20136502, 2020. [Cited on page 3.] ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "[64] Irvin Rock. Orientation and form. 1973. [Cited on page 1.] ", "page_idx": 14}, {"type": "text", "text": "[65] Bernhard Sch\u00f6lkopf and Julius von K\u00fcgelgen. From statistical to causal learning. Proceedings of the International Congress of Mathematicians, 2022. [Cited on page 1.]   \n[66] Maximilian Seitzer, Max Horn, Andrii Zadaianchuk, Dominik Zietlow, Tianjun Xiao, CarlJohann Simon-Gabriel, Tong He, Zheng Zhang, Bernhard Sch\u00f6lkopf, Thomas Brox, et al. Bridging the gap to real-world object-centric learning. International Conference on Learning Representations, 2023. [Cited on pages 1, 2, and 10.]   \n[67] Gautam Singh, Fei Deng, and Sungjin Ahn. Illiterate dall-e learns to compose. International Conference on Learning Representations, 2022. [Cited on pages 1, 2, and 9.]   \n[68] Gautam Singh, Yeongbin Kim, and Sungjin Ahn. Neural block-slot representations. arXiv preprint arXiv:2211.01177, 2022. [Cited on page 2.]   \n[69] Gautam Singh, Yi-Fu Wu, and Sungjin Ahn. Simple unsupervised object-centric learning for complex and naturalistic videos. Advances in Neural Information Processing Systems, 35:18181\u201318196, 2022. [Cited on page 2.]   \n[70] Joshua B Tenenbaum, Charles Kemp, Thomas L Grifftihs, and Noah D Goodman. How to grow a mind: Statistics, structure, and abstraction. science, 331(6022):1279\u20131285, 2011. [Cited on page 1.]   \n[71] Jakub Tomczak and Max Welling. Vae with a vampprior. In International Conference on Artificial Intelligence and Statistics, pages 1214\u20131223. PMLR, 2018. [Cited on page 25.]   \n[72] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. [Cited on page 25.]   \n[73] Sjoerd Van Steenkiste, Karol Kurach, J\u00fcrgen Schmidhuber, and Sylvain Gelly. Investigating object compositionality in generative adversarial networks. Neural Networks, 130:309\u2013325, 2020. [Cited on page 2.]   \n[74] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [Cited on pages 3 and 5.]   \n[75] Julius Von K\u00fcgelgen, Yash Sharma, Luigi Gresele, Wieland Brendel, Bernhard Sch\u00f6lkopf, Michel Besserve, and Francesco Locatello. Self-supervised learning with data augmentations provably isolates content from style. Advances in neural information processing systems, 34:16451\u201316467, 2021. [Cited on page 2.]   \n[76] Yanbo Wang, Letao Liu, and Justin Dauwels. Slot-vae: Object-centric scene generation with slot attention. arXiv preprint arXiv:2306.06997, 2023. [Cited on pages 2 and 6.]   \n[77] Yanbo Wang, Letao Liu, and Justin Dauwels. Slot-VAE: Object-centric scene generation with slot attention. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 36020\u201336035. PMLR, 23\u201329 Jul 2023. [Cited on pages 4 and 25.]   \n[78] Nicholas Watters, Loic Matthey, Sebastian Borgeaud, Rishabh Kabra, and Alexander Lerchner. Spriteworld: A flexible, configurable reinforcement learning environment. https://github.com/deepmind/spriteworld/, 2019. [Cited on page 9.]   \n[79] Nicholas Watters, Loic Matthey, Christopher P Burgess, and Alexander Lerchner. Spatial broadcast decoder: A simple architecture for learning disentangled representations in vaes. arXiv preprint arXiv:1901.07017, 2019. [Cited on page 8.]   \n[80] Matthew Willetts and Brooks Paige. I don\u2019t need u: Identifiable non-linear ica without side information. arXiv preprint arXiv:2106.05238, 2021. [Cited on page 2.]   \n[81] Xiaojiang Yang, Yi Wang, Jiacheng Sun, Xing Zhang, Shifeng Zhang, Zhenguo Li, and Junchi Yan. Nonlinear ICA using volume-preserving transformations. In International Conference on Learning Representations, 2022. [Cited on page 2.]   \n[82] Dingling Yao, Danru Xu, S\u00e9bastien Lachapelle, Sara Magliacane, Perouz Taslakian, Georg Martius, Julius von K\u00fcgelgen, and Francesco Locatello. Multi-view causal representation learning with partial observability. 2024. [Cited on page 2.]   \n[83] Alan Yuille and Daniel Kersten. Vision as bayesian inference: analysis by synthesis? Trends in cognitive sciences, 10(7):301\u2013308, 2006. [Cited on page 1.]   \n[84] Roland S Zimmermann, Yash Sharma, Steffen Schneider, Matthias Bethge, and Wieland Brendel. Contrastive learning inverts the data generating process. In International Conference on Machine Learning, pages 12979\u201312990. PMLR, 2021. [Cited on page 2.] ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "image", "img_path": "qmoVQbwmCY/tmp/6953ad5ffa2aa36a45bb8a202b3b0011eb7cd61a1995cd3dc61c7bfc541e45c2.jpg", "img_caption": ["Figure 7: Concatenated Slot Gaussian Mixture. Examples of the higher dimensional Gaussian mixture induced by the permutation equivariance of slot attention and the $K!$ ways of concatenating sampled slots. Here we start with a random 1D GMM with $K\\,=\\,3$ modes, each representing a different slot distribution, which then induces a respective 3D GMM with $K!=6$ modes. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "A List of Assumptions ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Assumption 1 ( $\\scriptstyle{\\mathit{\\omega}}_{\\mathrm{{\\mathit{B}}}}$ - Disentanglement, [47]). Let $\\mathbf{s}=\\{\\mathbf{s}_{B},\\forall\\,B\\in B\\}$ be a set of features wrt partition set $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ . The learned mixing function $f_{d}$ is said to be $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ disentangled wrt true decoder $\\Tilde{f_{d}}$ if there exists a permutation respecting diffeomorphism $v_{B}=\\tilde{f}_{d}^{-1}\\circ f_{d}\\forall\\,B\\in\\mathcal{B}$ which for a given feature s can be expressed as $v_{B}(\\mathbf{s})=v_{B}(\\mathbf{s}_{B})$ . ", "page_idx": 16}, {"type": "text", "text": "Assumption 2 (Additive Mixing Function). A mixing function $f_{d}:S\\rightarrow\\mathcal{X}$ , is said to be additive if there exist a partition set $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ and functions $f_{d}^{B}:s\\rightarrow\\mathbb{R}^{|\\mathcal{X}|}$ such that: $\\begin{array}{r}{f_{d}(\\mathbf{s})=\\sum_{B\\in\\mathcal{B}}f_{d}^{B}(\\mathbf{s}_{B})}\\end{array}$ . ", "page_idx": 16}, {"type": "text", "text": "Assumption 3 (Irreducibility). Given an an object $\\mathbf{x}_{k}\\in\\mathbf{x}$ , a model is considered as irreducible if any subset of an object, $\\textbf{y}\\subseteq\\textbf{x}_{k}$ is not functionally independent of the complement of the subset contained within the object, $\\mathbf{y}^{c}\\cap\\mathbf{x}$ as expressed by the Jacobin rank inequality in equation 5 in [6]. ", "page_idx": 16}, {"type": "text", "text": "Assumption 4 (Compositionality). Compositionality as defined by is a structure imposed on the slot decoder $f_{d}$ which implies that each image pixel is a function of at most one slot representation, thereby enforcing a local sparsity structure on the Jacobian matrix of $f_{d}$ [6]. ", "page_idx": 16}, {"type": "text", "text": "Remark 2. The compositionality is guaranteed by explicitly minimising the compositionality contrast but was empirically observed to be implicitly satisfied in the case of additive decoders [6]. Later, [47] showed this as the property of additive decoder models. However, the additive decoders studied by [47] are not expressive enough to represent the \u201cmasked decoders\u201d typically used in object-centric representation learning, which stems from the normalization of the alpha masks. This means some care must be taken in extrapolating the results in [47] to the models we use in practice. Additionally, additive decoders scale linearly in the number of slots $K$ , so some less significant scalability issues remain relative to state-of-the-art non-additive decoders (e.g. using Transformers). ", "page_idx": 16}, {"type": "text", "text": "Assumption 5 (u task). Conditioning latent variables on an observed variable to yield identifiable models. The main assumption is that conditioning on a (potentially observed) variable u renders the latent variables independent of each other [37]. ", "page_idx": 16}, {"type": "text", "text": "Assumption 6 (Object Sufficiency). A model is said to be object sufficient iff there are no additional objects in the original data distributions other than the ones expressed in training data. ", "page_idx": 16}, {"type": "text", "text": "Assumption 7 (Decoder Injectivity). The function $f_{d}:S\\rightarrow\\mathcal{X}$ mapping from slot space to image space is a non-linear piecewise affine injective function. That is, it specifies a unique one-to-one mapping between slots and images. ", "page_idx": 16}, {"type": "text", "text": "Remark 3. In practice, we use a monotonically increasing decoder with leakyReLU activation which should encourage injectivity behaviour [38, 37]. ", "page_idx": 16}, {"type": "text", "text": "Assumption 8 (Weak Injectivity [43]). Let $f:\\mathcal{Z}\\to\\mathcal{X}$ be a mapping between latent space and image space, where $\\dim({\\mathcal{Z}})\\leq\\dim({\\mathcal{X}})$ . The mapping $f_{d}$ is weakly injective if there exists $\\mathbf{x}_{0}\\in\\mathcal{X}$ and $\\delta>0$ such that $|f^{-1}(\\{\\mathbf{x}\\})|=1$ , $\\forall\\:\\mathbf{x}\\in B(\\mathbf{x}_{0},\\delta)\\cap f(\\mathcal{Z})$ , and $\\{\\mathbf{x}\\in\\mathcal{X}:|\\ f^{-1}(\\{\\mathbf{x}\\})\\mid=\\infty\\}\\subseteq f(\\mathcal{Z})$ has measure zero w.r.t. to the Lebesgue measure on $f(\\mathcal{Z})$ . ", "page_idx": 16}, {"type": "text", "text": "Remark 4. In words, Assumption 8 says that a mapping $f_{d}$ is weakly injective if: (i) in a small neighbourhood around a specific point $\\mathbf{x}_{0}\\in\\mathcal{X}$ the mapping is injective \u2013 meaning each point in this neighbourhood maps to exactly one point in the latent space $\\mathcal{Z}$ ; and (ii) while $f_{d}$ may not be globally injective, the set of points in $\\mathcal{X}$ that map back to an infinite number of points in $\\mathcal{Z}$ (non-injective points) is almost non-existent in terms of the Lebesgue measure on the image of $\\mathcal{Z}$ under $f_{d}$ . ", "page_idx": 16}, {"type": "text", "text": "B Aggregate Posterior Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Lemma 1 (Aggregate Posterior Mixture) Given that probabilistic slot attention induces a local (per-datapoint $\\mathbf{x}\\in\\bar{\\{}\\mathbf{x}_{i}\\}_{i=1}^{M})$ GMM with $K$ components, the aggregate posterior $q(\\mathbf{z})$ obtained by marginalizing out $\\mathbf{x}$ is a non-degenerate global Gaussian mixture with $M K$ components given by: ", "page_idx": 17}, {"type": "equation", "text": "$$\nq(\\mathbf{z})=\\frac{1}{M}\\sum_{i=1}^{M}\\sum_{k=1}^{K}\\widehat{\\pi}_{i k}\\mathcal{N}\\left(\\mathbf{z};\\widehat{\\mu}_{i k},\\widehat{\\pmb{\\sigma}}_{i k}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. We begin by noting that the aggregate posterior $q(\\mathbf{z})$ is the optimal prior $p(\\mathbf{z})$ so long as our posterior approximation $q(\\mathbf{z}\\mid\\mathbf{x})$ is close enough to the true posterior $p(\\mathbf{z}\\mid\\mathbf{x})$ , since for a dataset $\\mathbf{\\widetilde{\\{x}}}_{i}\\mathbf{\\}_{i=1}^{M}$ we have that: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle p({\\mathbf z})=\\int p({\\mathbf z}\\mid{\\mathbf x})p({\\mathbf x})d{\\mathbf x}}\\\\ {\\displaystyle=\\mathbb{E}_{{\\mathbf x}\\sim p({\\mathbf x})}\\left[p({\\mathbf z}\\mid{\\mathbf x})\\right]}\\\\ {\\displaystyle~~~\\approx\\frac{1}{M}\\sum_{i=1}^{M}p({\\mathbf z}\\mid{\\mathbf x}_{i})}\\\\ {\\displaystyle~~~\\approx\\frac{1}{M}\\sum_{i=1}^{M}q({\\mathbf z}\\mid{\\mathbf x}_{i})}\\\\ {\\displaystyle~~~~=\\cdot q({\\mathbf z}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we approximated $p(\\mathbf{x})$ using the empirical distribution, then substituted in the approximate posterior and marginalized out $\\mathbf{x}$ . This observation was first made by [28] and we use it to motivate our setup. ", "page_idx": 17}, {"type": "text", "text": "In our case, probabilistic slot attention (Algorithm 1) fits a (local) GMM to each latent variable sampled from the approximate posterior: $\\mathbf{z}\\sim q(\\mathbf{z}\\mid\\mathbf{x}_{i})$ , for $i=1,\\dots,M$ . Let $f(\\mathbf{z})$ denote the (local) GMM density, its expectation is given by: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{\\rho(x)\\sim\\rho(x)}[f(x)]=-\\int\\!\\!\\int_{0}^{x}p(x)\\pi(x)\\times f(x)\\times d x}\\\\ &{\\sim\\int\\!\\!\\int_{\\frac{1}{\\rho(x)}}^{\\frac{1}{\\rho}}\\!\\!\\frac{M}{n-1}\\langle x-x\\rangle\\!\\left\\eta(x)\\times f(x)\\right\\rangle d x d x}\\\\ &{=\\int\\!\\!\\int_{\\frac{1}{\\rho(x)}}^{\\frac{1}{\\rho}}\\!\\!\\frac{M}{n-1}\\frac{\\gamma(x)}{n^{2}(x)}f(x)d x}\\\\ &{=\\int\\!\\!\\int_{\\frac{1}{\\rho(x)}}^{\\frac{1}{\\rho}}\\!\\!\\frac{M}{n-1}\\!\\!\\left(x\\!\\in\\![x_{0}(x_{1}),\\!\\cdot\\!\\frac{x}{n-1}\\!]\\!\\!-\\!\\pi_{x}^{n}f(x)\\!\\cdot\\!\\rho(x)\\right)d x}\\\\ &{\\sim\\int\\!\\!\\int_{\\frac{1}{\\rho(x)}}^{\\frac{1}{\\rho}}\\!\\!\\frac{M}{n-1}\\!\\!\\left(x\\!\\in\\![n(x_{1}),\\!\\cdot\\!\\sum_{i=1}^{n}\\!\\cdot\\!\\gamma(x,\\!\\cdot\\!\\rho_{i,i}\\!\\cdot\\!\\sigma_{i,i}^{n})\\,d x\\right.}\\\\ &{\\quad\\left.\\sim\\int\\!\\frac{M}{n-1}\\frac{\\gamma}{n}\\delta(x-\\mu(x))\\cdot\\sum_{i=1}^{n}\\!\\cdot\\!\\sum_{i=1}^{n}\\!\\cdot\\!\\gamma(x,\\!\\cdot\\!\\rho_{i,i}\\!\\cdot\\!\\sigma_{i,i}^{n})\\,d x\\right.}\\\\ &{=\\frac{1}{M}\\frac{M}{n-1}\\sum_{i=1}^{n}\\!\\cdot\\Big(\\eta(x)\\!\\cdot\\!\\gamma(x_{i})\\!\\cdot\\!\\rho_{i,i}\\!\\cdot\\!\\sigma_{i,i}^{n})}\\\\ &{=g(x),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we again used the empirical distribution approximation of $p(\\mathbf{x})$ , and the following basic identity of the Dirac delta to simplify: $\\begin{array}{r}{\\int\\delta(\\mathbf{x}-\\mathbf{x}^{\\prime})\\bar{f}(\\mathbf{x})d\\mathbf{x}=f(\\mathbf{x}^{\\prime})}\\end{array}$ . ", "page_idx": 17}, {"type": "text", "text": "For the general case, however, we must instead compute the product of $q(\\mathbf{z}\\mid\\mathbf{x})$ and $f(\\mathbf{z})$ rather than use a Dirac delta approximation as in Equation 23. To that end we may proceed as follows w.r.t. to ", "page_idx": 17}, {"type": "text", "text": "each datapoint $\\mathbf{x}_{i}$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle q(\\mathbf{z}\\mid\\mathbf{x}_{i})\\cdot f(\\mathbf{z})=\\mathcal{N}\\big(\\mathbf{z};\\mu(\\mathbf{x}_{i}),\\sigma^{2}(\\mathbf{x}_{i})\\big)\\cdot\\displaystyle\\sum_{k=1}^{K}\\pi_{i k}\\mathcal{N}\\left(\\mathbf{z};\\mu_{i k},\\sigma_{i k}^{2}\\right)}\\\\ {\\displaystyle\\qquad\\qquad\\qquad=\\sum_{k=1}^{K}\\pi_{i k}\\,\\big[\\mathcal{N}\\left(\\mathbf{z};\\mu_{i k},\\sigma_{i k}^{2}\\right)\\cdot\\mathcal{N}\\big(\\mathbf{z};\\mu(\\mathbf{x}_{i}),\\sigma^{2}(\\mathbf{x}_{i})\\big)\\big]}\\\\ {\\displaystyle\\qquad\\qquad\\qquad=\\sum_{k=1}^{K}\\widehat{\\pi}_{i k}\\mathcal{N}\\left(\\mathbf{z};\\widehat{\\mu}_{i k},\\widehat{\\sigma}_{i k}^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the posterior parameters of the resulting mixture are given in closed-form by: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\widehat{\\pmb{\\sigma}}_{i k}^{2}=\\left(\\frac{1}{\\pmb{\\sigma}_{i k}^{2}}+\\frac{1}{\\pmb{\\sigma}^{2}(\\mathbf{x}_{i})}\\right)^{-1},\\qquad\\widehat{\\mu}_{i k}=\\widehat{\\pmb{\\sigma}}_{i k}^{2}\\left(\\frac{\\mu(\\mathbf{x}_{i})}{\\pmb{\\sigma}^{2}(\\mathbf{x}_{i})}+\\frac{\\mu_{i k}}{\\pmb{\\sigma}_{i k}^{2}}\\right),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which are the standard distributional parameters obtained from a product of two Gaussians. ", "page_idx": 18}, {"type": "text", "text": "For the updated mixture coefficients $\\widehat{\\pi}_{i k}$ , we propose a principled way to include a posterior-weighted contribution of each mode to the n e w mixture coefficients. First, note that $(\\pi_{i1},\\pi_{i2},\\ldots,\\pi_{i K})$ are parameters of a multinomial distribution as $\\textstyle\\sum_{k=1}^{K}\\pi_{i k}=1$ , for each datapoint $\\mathbf{x}_{i}$ . Since the Dirichlet distribution is the conjugate prior of the mu ltinomial distribution, we can place a Dirichlet prior over the mixing coefficients for each datapoint, then update it to a posterior using the data. Concretely, we place a symmetric Dirichlet prior over the mixing coefficients as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n(\\pi_{i1},\\pi_{i2},\\ldots,\\pi_{i K})\\sim\\mathrm{Dirichlet}\\left(\\alpha_{i1},\\alpha_{i2},\\ldots,\\alpha_{i K}\\right),\\quad\\mathrm{for}\\quad i=1,2,\\ldots,M,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\alpha_{i}\\in\\mathbb{R}^{K}$ are the concentration parameters of the $i^{\\mathrm{th}}$ Dirichlet distribution, and $\\forall i,k:\\,\\pmb{\\alpha}_{i k}=1$ , indicating uniformity over the open standard $(K-1)$ -simplex. To compute the posterior Dirichlet distribution we calculate \u2018pseudo-counts\u2019 by integrating the product of the posterior density $q(\\mathbf{z}\\mid\\mathbf{x}_{i})$ with each one of the $K$ modes of the Gaussian mixture, thereby measuring a posterior-weighted contribution of each mode $k$ to the new aggregate mixture: ", "page_idx": 18}, {"type": "equation", "text": "$$\nc_{i k}=\\int\\mathcal{N}\\left(\\mathbf{z};\\mu_{i k},\\sigma_{i k}^{2}\\right)\\cdot\\mathcal{N}\\big(\\mathbf{z};\\mu(\\mathbf{x}_{i}),\\sigma^{2}(\\mathbf{x}_{i})\\big)d\\mathbf{z},\\quad\\mathrm{~for~}\\,\\,\\,i=1,2,\\ldots,M,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which we can then use as pseudo-counts to compute the Dirichlet posterior: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left(\\widehat{\\pi}_{i1},\\widehat{\\pi}_{i2},\\ldots,\\widehat{\\pi}_{i K}\\right)\\,\\left|\\,\\left(c_{i1},c_{i2},\\ldots,c_{i K}\\right)\\sim\\mathrm{Dirichlet}\\left(\\alpha_{i1}+c_{i1},\\alpha_{i2}+c_{i2},\\ldots,\\alpha_{i K}+c_{i K}\\right),\\right.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for $i=1,2,\\dots,M$ . Each posterior probability is then readily given by the mean estimate ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\widehat{\\pmb{\\pi}}_{i k}=\\frac{\\pmb{\\alpha}_{i k}+c_{i k}}{\\sum_{j=1}^{K}(\\pmb{\\alpha}_{i j}+c_{i j})}\\implies\\sum_{k=1}^{K}\\widehat{\\pmb{\\pi}}_{i k}=1.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Putting everything together, the aggregated posterior is therefore given by: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\displaystyle q(\\mathbf{z})=\\frac{1}{M}\\sum_{i=1}^{M}\\sum_{k=1}^{K}\\widehat{\\boldsymbol{\\pi}}_{i k}\\mathcal{N}\\left(\\mathbf{z};\\widehat{\\mu}_{i k},\\widehat{\\boldsymbol{\\sigma}}_{i k}^{2}\\right),\\qquad\\mathrm{where}\\qquad\\mathbf{z}\\sim q(\\mathbf{z}\\mid\\mathbf{x}),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which concludes the proof. ", "page_idx": 18}, {"type": "text", "text": "Corollary 4. The aggregate posterior $q(\\mathbf{z})$ is a non-degenerate Gaussian mixture, in the sense that it is a well-defined probability distribution, as the updated mixture coefficients sum to $^{\\,l}$ over the number of components $M\\times K$ . ", "page_idx": 18}, {"type": "text", "text": "Proof. Recall from Lemma 1 that the aggregate posterior $q(\\mathbf{z})$ \u2013 obtained by marginizaling out $\\mathbf{x}$ from a probabilistic slot attention model \u2013 is a mixture distribution of $M\\times K$ components with the following parameters: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\{\\widehat{\\pi}_{i k},\\widehat{\\mu}_{i k},\\widehat{\\sigma}_{i k}^{2}\\right\\},\\qquad\\mathrm{for}\\quad i=1,2,\\ldots,M,\\quad\\mathrm{and}\\quad k=1,2,\\ldots,K.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "To verify that $q(\\mathbf{z})$ is a non-degenerate mixture, we observe the following implication: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}\\widehat{\\pi}_{i k}=1,\\qquad\\qquad\\mathrm{for}\\quad i=1,2,\\dots,M,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "due to the Dirichlet posterior update in Equation 33, and therefore ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle{\\implies\\frac{1}{M}\\sum_{i=1}^{M}\\sum_{k=1}^{K}\\widehat{\\pi}_{i k}=\\frac{1}{M}\\sum_{i=1}^{M}1=\\frac{1}{M}\\cdot M=1}}\\\\ {\\displaystyle{\\implies\\sum_{i=1}^{M}\\sum_{k=1}^{K}\\frac{\\widehat{\\pi}_{i k}}{M}=1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which says that the scaled sum of the mixing proportions of all $K$ components in all $M$ GMMs must equal 1, proving that the associated aggregate posterior mixture $q(\\mathbf{z})$ is a well-defined probability distribution. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "Theorem 1 (Mixture Distribution of Concatenated Slots). Let $f_{s}$ denote a permutation equivariant probabilistic slot attention function such that $f_{s}(\\mathbf{z},P\\mathbf{s}^{t})=P f_{s}(\\mathbf{z},\\mathbf{s}^{t})$ , where $P\\in\\{0,\\bar{1}\\}^{K\\times K}$ is an arbitrary permutation matrix. Let $\\mathbf{s}=(\\mathbf{s}_{1},\\mathbf{s}_{2},\\ldots,\\mathbf{s}_{K})\\in\\mathbb{R}^{K d}$ be a random variable defined as the concatenation of $K$ individual sampled slots, where each slot is Gaussian distributed within a $K$ -component mixture: $\\mathbf{s}_{k}\\sim\\mathcal{N}(\\mathbf{s}_{k};\\mu_{k}^{\\cdot},\\Sigma_{k})\\in\\mathbb{R}^{d},\\forall\\,k\\in\\{1,\\dots K\\}$ . Then, it holds that s is also Gaussian mixture distributed comprising $K!$ mixture components: ", "page_idx": 19}, {"type": "equation", "text": "$$\np(\\mathbf{s})=\\sum_{p=1}^{K!}\\pi_{p}\\mathcal{N}(\\mathbf{s};\\mu_{p},\\boldsymbol{\\Sigma}_{p}),\\quad\\mathrm{where}\\quad\\pi\\in\\Delta^{K!-1},\\quad\\mu_{p}\\in\\mathbb{R}^{K d},\\quad\\boldsymbol{\\Sigma}_{p}\\in\\mathbb{R}^{K d\\times K d}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. Each slot $\\mathbf{s}_{k}\\sim\\mathcal N(\\pmb{\\mu}_{k},\\pmb{\\Sigma}_{k})$ is sampled independently from $\\mathbf{s}_{j}\\sim\\mathcal N(\\pmb{\\mu}_{j},\\pmb{\\Sigma}_{j})$ , for any $j\\neq k$ , meaning they are conditionally independent given the latent mixture component assignment. Thus, the concatenated slots variable $\\mathbf{s}=(\\mathbf{s}_{1},\\mathbf{s}_{2},.~.~.~,\\mathbf{s}_{K})$ , can be described by a $K d$ -dimensional multivariate Gaussian distribution with a block diagonal covariance structure as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{s}=\\left[\\!\\!\\begin{array}{c}{\\mathbf{s}_{\\pi(1)}}\\\\ {\\mathbf{s}_{\\pi(2)}}\\\\ {\\vdots}\\\\ {\\mathbf{s}_{\\pi(K)}}\\end{array}\\!\\!\\right]\\sim{\\mathcal{N}}\\left(\\left[\\!\\!\\begin{array}{c}{\\mu_{\\pi(1)}}\\\\ {\\mu_{\\pi(2)}}\\\\ {\\vdots}\\\\ {\\mu_{\\pi(K)}}\\end{array}\\!\\!\\right],\\left[\\!\\!\\begin{array}{c c c c}{\\Sigma_{\\pi(1)}}&{0}&{\\cdots}&{0}\\\\ {0}&{\\Sigma_{\\pi(2)}}&{\\cdots}&{0}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {0}&{0}&{\\cdots}&{\\Sigma_{\\pi(K)}}\\end{array}\\!\\!\\right]\\right),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\pi:[K]\\to[K]$ is a permutation function of the set $[K]\\,:=\\,\\{1,2,\\dots,K\\}$ . Since the slot attention function $f_{s}$ is permutation equivariant, there exist $K!$ possible ways to concatenate $K$ slots, and each permutation induces a mode within a Gaussian mixture living in $\\mathbb{R}^{K d}$ space. Since each permutation of the slots is equally likely, the mixture coefficients are given by: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\pi=(\\pi_{1},\\pi_{2},\\ldots,\\pi_{K^{!}}),\\qquad\\mathrm{where}\\qquad\\pi_{p}=\\frac{1}{K!}\\qquad\\forall p\\in\\{1,2,\\ldots,K!\\}}}\\\\ {{\\displaystyle\\implies\\sum_{p=1}^{K!}\\pi_{p}=1,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which concludes the proof. ", "page_idx": 19}, {"type": "text", "text": "Remark 5. Based on the above result, it is evident that concatenating $K\\geq2$ unique slots can be viewed as a sample from a GMM with $K!$ components. Constructing a scene requires at least two unique slots, one for the background and one for an object, thus supporting our theory regarding slot composition. ", "page_idx": 19}, {"type": "text", "text": "C Injective Decoders: Sufficient Conditions ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we provide a theoretical overview of the decoder architecture we use and offer sufficient conditions for a leaky-ReLU decoder to be weakly injective in the sense of Assumption 8 as shown and adapted from [43]. ", "page_idx": 20}, {"type": "text", "text": "Definition 4 (Piece-wise Decoders, [43]). Let $\\mathbf{s}=\\{\\mathbf{s}_{1},\\mathbf{s}_{2},\\dots,\\mathbf{s}_{K}\\}$ denote a given set of sampled $z_{i}$ in each mixture component $K$ slots) in the GMM, $P(\\mathcal{Z})$ . Let $\\sigma:\\mathbb{R}\\rightarrow\\mathbb{R}$ denote the leaky-ReLU activation function, and let $m=n_{0},n_{1},n_{2},\\ldots,n_{t}=n$ and $H(n_{1},n_{2})$ denote the set of full-rank affine functions $h_{i}:\\mathbb{R}^{n_{i}}\\rightarrow\\mathbb{R}^{n_{j}}$ . We consider piece-wise functions mapping each slot representation s \u2208S \u2208Rm \u00d7 K to an image x \u2208X \u2208Rn in the output space, F\u03c3m K\u2192n: S \u2192X, of the form below: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{F}_{\\sigma}^{n_{0},\\dots,n_{t}}=\\Big\\{h_{t}\\circ\\sigma\\circ h_{t-1}\\circ\\sigma\\circ\\cdots\\sigma\\circ h_{1}\\ |\\ h_{i}\\in H(n_{i-1},n_{i})\\Big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The following Corollary, corollary and proofs are adapted from [43]. ", "page_idx": 20}, {"type": "text", "text": "Corollary 5. Given $f_{d}\\in\\mathcal{F}_{\\sigma}^{m\\hookrightarrow n}$ where $f_{d}=h_{t}\\circ\\sigma\\circ h_{t-1}\\circ\\sigma\\circ.\\ldots\\sigma\\circ h_{1},\\,f_{d}$ is injective. ", "page_idx": 20}, {"type": "text", "text": "Proof. Each affine function $h_{i}$ has full column rank and is therefore both injective and invertible. Since the activation function $\\sigma$ is also injective, we get that $f_{d}$ is injective and invertible. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "Corollary 6. Let $f_{d}=h_{t}\\circ\\sigma\\circ h_{t-1}\\circ\\sigma\\circ.\\,.\\,.\\,\\sigma\\circ h_{1}\\in\\mathcal{F}_{\\sigma}^{m\\rightarrow n}$ where $m=n_{0}\\leq n_{1}\\leq\\cdot\\cdot\\leq n_{k}=n$ . Given $h_{i}$ is affine and invertible, then for almost all $x\\in f_{d}(\\mathbb{R}^{m})$ there exists $\\delta$ such that $f_{d}^{-1}$ is $a$ well-defined affine function, $h_{i}$ on $B(x,\\delta)\\cap f_{d}(\\mathbb{R}^{m})$ . ", "page_idx": 20}, {"type": "text", "text": "Proof. We know $f_{d}$ is a piecewise affine function which is invertible. Therefore, it simply follows, $\\forall y\\,\\in\\,B(x,\\delta)$ there exists $\\delta$ such that $f_{d}^{-1}$ is an affine function in the domain $B(x,\\delta)$ . One can therefore deduce there exists some affine function $h_{i}$ where $f_{d}^{-1}=h_{i}^{-1}$ in the domain $B(x,\\delta)$ . ", "page_idx": 20}, {"type": "text", "text": "D Slot Identifiability Proof ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Definition 5 (Slot Identifiability [6]). Given a diffeomorphic ground truth function $f:S\\rightarrow\\mathcal{X}$ , and inference model ${\\hat{g}}:{\\mathcal{X}}\\rightarrow{\\mathcal{S}}$ , $\\hat{g}$ correctly slot identifies every object $\\mathbf{x}_{j}\\in\\mathcal{X}$ with the ground slot $\\mathbf{s}_{k}\\in\\mathcal{S}$ via $\\hat{\\mathbf{s}}_{k}\\,=\\,\\hat{g}(f(\\mathbf{s}_{k}))$ if there exists a unique slot $k\\,\\in\\,[K]$ for all $\\mathbf{x}_{j}\\,\\in\\,\\mathcal{X}$ , and there exists an invertible diffeomorphism, or in our case an affine transformation, such that $\\mathbf{s}_{k}=h(\\hat{\\mathbf{s}}_{k})$ for all $\\mathbf{s}_{k}\\in\\mathcal{S}$ . ", "page_idx": 20}, {"type": "text", "text": "Remark 6. [6] established that compositionality (Assumption 4) and irreducibility (Assumption 3) of the decoder $f_{d}$ is required for slot identifiability in the sense of Definition 5. ", "page_idx": 20}, {"type": "text", "text": "Summary & Intuition. The following theorems and proof extend the identifiability results of [43] to slot-attention models, we include all the proofs and details for the sake of completion. In this work, we do not consider the irreducibility criteria in [6] and define slot identifiability only by an injective mapping of each slot to subspaces representing objects in a scene to satisfy compositionality without the use of computationally heavy methods such as additive decoders and compositional contrast. We show identifiability of each slot representation up to affine transformation by passing a concatenation of samples from each mixture component which represents slots through a piecewise function in Definition 4 representing the decoder, $f_{d}$ . The trick in this proof lies in our observation of the fact that a concatenation of samples from each slot mixture component is a sample from a high dimensional GMM $p(\\mathbf{s})$ , as highlighted in Theorem 1. We then use the identifiability results of [43] to show $p(\\mathbf{s})$ is identifiable up to affine transformation. This then implies identifiability up to affine transformation of the aggregate posterior $q(\\mathbf{z})$ , a non-degenerate GMM by Lemma 1 where a sample from a mixture component in $q(\\mathbf{z})$ represents an individual slot representation. This contrasts with [43] which does not consider identifiability of the aggregate posterior and its mixture components in the context of slot representation learning. ", "page_idx": 20}, {"type": "text", "text": "In order to proceed, we begin by stating three key theorems defined and proven in the work of [43] which are essential for our slot identifiability proof. First, we restate the definition of a generic point as outlined by [43] below. ", "page_idx": 20}, {"type": "text", "text": "Definition 6. A point $\\mathbf{x}\\in f_{d}(\\mathbb{R}^{m})\\subseteq\\mathbb{R}^{n}$ is generic if there exists $\\delta>0$ , such that $f_{d}:B(\\mathbf{s},\\delta)\\rightarrow\\mathbb{R}^{n}$ is affine for every $\\mathbf{s}\\in f_{d}^{-1}(\\{\\mathbf{x}\\})$ ", "page_idx": 21}, {"type": "text", "text": "Theorem 7 (Kivva et al. [43]). Given $f_{d}:\\mathbb{R}^{m}\\,\\rightarrow\\,\\mathbb{R}^{n}$ is a piecewise affine function such that $\\{\\mathbf{x}\\in\\mathbb{R}^{n}\\,:\\,|\\,f_{d}^{-1}(\\{\\mathbf{x}\\})|=\\mathbf{\\dot{\\infty}}\\}\\subseteq f_{d}(\\mathbb{R}^{m})$ has measure zero with respect to the Lebesgue measure on $f_{d}(\\mathbb{R}^{m})$ , this implies $\\dim(f_{d}(\\mathbb{R}^{m}))=m$ and almost every point in $f_{d}(\\mathbb{R}^{m})$ (with respect to the Lebesgue measure on $f_{d}(\\mathbb{R}^{m}))$ is generic with respect to $f_{d}$ . ", "page_idx": 21}, {"type": "text", "text": "Theorem 8 (Kivva et al. [43]). Consider a pair of finite GMMs in $\\mathbb{R}^{m}$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n{\\bf y}=\\sum_{j=1}^{J}\\pi_{j}{\\mathcal N}({\\bf y};{\\pmb\\mu}_{j},{\\bf\\Sigma}_{{\\pmb j}}),\\qquad a n d\\qquad{\\bf y}^{\\prime}=\\sum_{j=1}^{J^{\\prime}}\\pi_{j}^{\\prime}{\\mathcal N}({\\bf y}^{\\prime};{\\pmb\\mu}_{j}^{\\prime},{\\bf\\Sigma}_{{\\pmb j}}^{\\prime}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Assume that there exists a ball $B(\\mathbf{x},\\delta)$ such that $\\mathbf{y}$ and $\\mathbf{y}^{\\prime}$ induce the same measure on $B(\\mathbf{x},\\delta)$ . Then $\\mathbf{y}\\equiv\\mathbf{y}^{\\prime}$ , and for some permutation $\\tau$ we have that $\\pi_{i}=\\pi_{\\tau(i)}^{\\prime}$ and $(\\pmb{\\mu}_{i},\\pmb{\\Sigma}_{i})=(\\pmb{\\mu}_{\\tau(i)}^{\\prime},\\pmb{\\Sigma}_{\\tau(i)}^{\\prime})$ . ", "page_idx": 21}, {"type": "text", "text": "Theorem 9 (Kivva et al. [43]). Given $\\begin{array}{r}{\\textbf{z}\\sim\\sum_{i=1}^{J}\\pi_{i}\\mathcal{N}(\\mathbf{z};\\pmb{\\mu}_{i},\\pmb{\\Sigma}_{i})}\\end{array}$ and $\\begin{array}{r}{\\mathbf{z}^{\\prime}\\sim\\sum_{j=1}^{J^{\\prime}}\\pi_{j}^{\\prime}\\mathcal{N}(\\mathbf{z}^{\\prime};\\pmb{\\mu}_{j}^{\\prime},\\pmb{\\Sigma}_{j}^{\\prime})}\\end{array}$ and $f_{d}(\\mathbf{z})$ and $\\tilde{f}_{d}(\\mathbf{z}^{\\prime})$ are equally distributed. We can assume for $\\mathbf{x}\\in\\mathbb{R}^{n}$ and $\\delta>0$ , $f_{d}$ is invertible on $B(\\mathbf{x},2\\delta)\\cap f_{d}(\\mathbb{R}^{m})$ . This implies that there exists $\\mathbf{x}_{1}\\in B(\\mathbf{x},\\delta)$ and $\\delta_{1}>0$ such that both $f_{d}$ and $\\Tilde{f_{d}}$ are invertible on $B(\\mathbf{x}_{1},\\delta_{1})\\cap f_{d}(\\mathbb{R}^{m})$ . ", "page_idx": 21}, {"type": "text", "text": "We next propose our slot identifiability result below. ", "page_idx": 21}, {"type": "text", "text": "Theorem 2 ( $\\left.\\sim_{s}\\right.$ -Identifiable Slot Representations). Given that the aggregate posterior $q(\\mathbf{z})$ is an optimal, non-degenerate mixture prior over slot space (Lemma 1), $f_{d}:S\\rightarrow\\mathcal{X}$ is a piecewise affine weakly injective mixing function (Assumption 8), and the slot representation, $\\mathbf{s}=(\\mathbf{s}_{1},\\dots,\\mathbf{s}_{K})$ can be observed as a sample from a GMM (Theorem 1), then $p(\\mathbf{s})$ is identifiable as per Definition 3. ", "page_idx": 21}, {"type": "text", "text": "Proof. The proof extends from [43] to slot-based models. Given two piece-wise affine functions $f_{d},\\tilde{\\tilde{f}}_{d}:S\\stackrel{=}{\\rightarrow}\\mathcal{X},\\forall k\\in[K].$ , let $\\mathbf{s}=(\\mathbf{s}_{1},\\mathbf{\\Omega},...\\,,\\mathbf{s}_{K}),\\mathbf{\\Phi}\\ni\\mathbf{s}_{k}\\sim{\\mathcal{N}}(\\mathbf{s}_{k};\\mu_{k},\\Sigma_{k})$ and $\\mathbf{s}^{\\prime}=(\\mathbf{s}_{1}^{\\prime},\\dots,\\mathbf{s}_{K}^{\\prime}),\\mathsf{\\boldsymbol{\\ni}}$ $\\mathbf{s}_{k}^{\\prime}\\sim\\mathcal{N}(\\mathbf{s}_{k}^{\\prime};\\pmb{\\mu}_{k}^{\\prime},\\pmb{\\Sigma}_{k}^{\\prime})$ be a pair of slot representations constructed by sampling and concatenating each mixture component (i.e. slots) from two distinct GMMs. As proven in Theorem 1, given individual sampled slots $\\mathbf{s}_{k}$ are conditionally independent given the mixture component $k$ , then a concatenated sample is from a higher dimensional GMM in $\\mathbf{\\bar{R}}^{K d}$ . Now, suppose for the sake of argument that $f_{d}(\\boldsymbol{S})$ and $\\tilde{f}_{d}(\\ensuremath{\\boldsymbol{S}}^{\\prime})$ are equally distributed. We assume that there exists $\\mathbf{x}\\in\\mathcal{X}$ and $\\delta>0$ such that $f_{d}$ and $\\Tilde{f_{d}}$ are invertible and piecewise affine on $B(\\mathbf{x},\\delta)\\cap f_{d}(S)$ , which implies $\\dim f_{d}(S)=|S|$ . ", "page_idx": 21}, {"type": "text", "text": "We now restrict the space $B(\\mathbf{x},\\delta)$ to a subspace $B(\\mathbf{x}^{\\prime},\\delta^{\\prime})$ where $\\mathbf{x}\\in B(\\mathbf{x}^{\\prime},\\delta^{\\prime})$ such that $f_{d}$ and $\\Tilde{f_{d}}$ are now invertible and affine on $B(\\mathbf{x}^{\\prime},\\delta^{\\prime})\\cap f_{d}(S)$ . Next, we let $L\\subseteq\\mathcal{X}$ be an $|{\\mathcal{S}}|$ -dimensional affine subspace (assuming $|{\\mathcal{X}}|\\geq|S|)$ , such that $B({\\bf x}^{\\prime},\\delta^{\\prime})\\cap f_{d}({\\cal S})=B({\\bf x}^{\\prime},\\delta^{\\prime})\\cap L$ . We also define $h_{f},h_{\\widetilde{f}}:$ ${\\cal S}\\rightarrow{\\cal L}$ to be a pair of invertible affine functions where $h_{f}^{-1}(B(\\mathbf{x}^{\\prime},\\delta^{\\prime})\\cap L)=f_{d}^{-1}(B(\\mathbf{x}^{\\prime},\\delta^{\\prime})\\cap L)$ and $h_{\\tilde{f}}^{-1}(B({\\bf x}^{\\prime},\\delta^{\\prime})\\cap L)=\\tilde{f}_{d}^{-1}(B({\\bf x}^{\\prime},\\delta^{\\prime})\\cap L).$ . ", "page_idx": 21}, {"type": "text", "text": "Therefore, this implies $h_{f}(\\mathbf{s})$ and $h_{\\tilde{f}}(\\mathbf{s}^{\\prime})$ are finite GMMs which coincide on $B(\\mathbf{x}^{\\prime},\\delta^{\\prime})\\cap L$ and $h_{f}(\\mathbf{s})\\equiv h_{\\widetilde{f}}(\\mathbf{s}^{\\prime})$ based on Theorem 8. Given, $h=h_{\\tilde{f}}^{-1}\\circ h_{f}$ and $h_{f}(\\mathbf{s})$ and $h_{\\tilde{f}}(\\mathbf{s}^{\\prime})$ then $h$ is an affine transformation such that $h(\\mathbf{s})=\\mathbf{s}^{\\prime}$ . ", "page_idx": 21}, {"type": "text", "text": "Given Theorems 7 and 9, there exists a point $\\mathbf{x}\\in f_{d}(S)$ that is generic with respect $f_{d}$ and $\\Tilde{f_{d}}$ and invertible on $B(\\mathbf{x},\\delta)\\cap f_{d}(S)$ . Having established that there is an affine transformation $h(\\mathbf{s})=\\mathbf{s}^{\\prime}$ and two invertible piecewise affine functions $f_{d}$ and $\\Tilde{f_{d}}$ on $B(\\mathbf{x},\\delta)\\cap f_{d}(S)$ , this implies that $p(\\mathbf{s})$ is identifiable up to an affine transformation and permutation of $\\mathbf{s}_{k}\\in\\mathbf{s}$ , which concludes the proof. $\\sqsupset$ ", "page_idx": 21}, {"type": "text", "text": "Corollary 3 (Individual Slot Identifiability). If the concatenated slot distribution $p(\\mathbf{s})$ is $\\sim_{s^{-}}$ identifiable (Theorem 2) then this implies $q(\\mathbf{z})$ is identifiable up to affine transformation and permutation of the slots, $\\mathbf{s}_{k}$ and therefore each slot distribution $\\mathbf{s}_{k}\\sim\\mathcal{N}(\\mathbf{s}_{k};\\mu_{k},\\Sigma_{k})\\in\\mathbb{R}^{d},\\forall\\,k\\in\\{1,\\dots K\\}$ is also identifiable up to an affine transformation. ", "page_idx": 21}, {"type": "text", "text": "Proof. Given Theorem 8, we know that each higher dimensional mixture component in $p(\\mathbf{s})$ induces the same measure on $B(\\mathbf{x},\\delta)$ and hence for some permutation $\\tau$ we have that $(\\mu_{\\pi(i)},\\dot{\\Sigma}_{\\pi(i)})\\;=\\;$ ", "page_idx": 21}, {"type": "text", "text": "Input: ${\\bf z}=f_{e}({\\bf x})\\in\\mathbb{R}^{N\\times d}$ \u25b7input representation   \n$\\forall k$ , $\\pmb{\\pi}(0)_{k}\\leftarrow1/K,\\pmb{\\mu}(0)_{k}\\sim\\mathcal{N}(0,\\mathbf{I}_{d}),\\pmb{\\sigma}(0)_{k}^{2}\\leftarrow\\mathbf{1}_{d}$   \nfor $t=0,\\dots,T-1$ $\\begin{array}{r l}&{A_{n k}\\leftarrow\\frac{\\pi(t)_{k}\\mathcal{N}\\left(\\mathbf{z}_{n};W_{q}\\mu(t)_{k},\\sigma(t)_{k}^{2}\\right)}{\\sum_{j=1}^{K}\\pi(t)_{j}\\mathcal{N}\\left(\\mathbf{z}_{n};W_{q}\\mu(t)_{j},\\sigma(t)_{j}^{2}\\right)}}\\\\ &{\\hat{A}_{n k}\\leftarrow A_{n k}/\\sum_{l=1}^{N}A_{l k}}\\\\ &{\\mu(t+1)_{k}\\leftarrow\\sum_{n=1}^{N}\\hat{A}_{n k}\\mathbf{z}_{n}}\\\\ &{\\sigma(t+1)_{k}^{2}\\leftarrow\\sum_{n=1}^{N}\\hat{A}_{n k}\\left(\\mathbf{z}_{n}-\\mu(t+1)_{k}\\right)^{2}}\\\\ &{\\pi(t+1)_{k}\\leftarrow\\sum_{n=1}^{N}A_{n k}/N}\\end{array}$ \u25b7normalize attention \u25b7update slot mean \u25b7update mixing coeff.   \nreturn $\\mu(T),\\sigma(T)^{2}$ \u25b7K slot distributions ", "page_idx": 22}, {"type": "text", "text": "Algorithm 3 Probabilistic Slot Attention V.2 (PSA-PROJ) ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Input: ${\\bf z}=f_{e}({\\bf x})\\in\\mathbb{R}^{N\\times d}$ $\\triangleright$ input representation   \nk Wkz RN\u00d7d \u25b7compute keys   \n\u2200 $k,\\pmb{\\pi}(0)_{k}\\leftarrow1/K,\\pmb{\\mu}(0)_{k}\\sim\\mathcal{N}(0,\\mathbf{I}_{d}),\\pmb{\\sigma}(0)_{k}^{2}\\leftarrow\\mathbf{1}_{d}$   \nfor $t=0,\\dots,T-1$ $\\begin{array}{r l}&{A_{n k}\\leftarrow\\frac{\\pi(t)_{k}\\mathcal{N}\\left(\\mathbf{k}_{n};W_{q}\\mu(t)_{k},\\sigma(t)_{k}^{2}\\right)}{\\sum_{j=1}^{K}\\pi(t)_{j}\\mathcal{N}\\left(\\mathbf{k}_{n};W_{q}\\mu(t)_{j},\\sigma(t)_{j}^{2}\\right)}}\\\\ &{\\hat{A}_{n k}\\leftarrow A_{n k}/\\sum_{l=1}^{N}A_{l k}}\\\\ &{\\mu(t+1)_{k}\\leftarrow\\sum_{n=1}^{N}\\hat{A}_{n k}W_{v}\\mathbf{z}_{n}}\\\\ &{\\sigma(t+1)_{k}^{2}\\leftarrow\\sum_{n=1}^{N}\\hat{A}_{n k}\\left(W_{v}\\mathbf{z}_{n}-\\mu(t+1)_{k}\\right)^{2}}\\\\ &{\\pi(t+1)_{k}\\leftarrow\\sum_{n=1}^{N}A_{n k}/N}\\end{array}$ \u25b7normalize attention \u25b7update slot mean \u25b7update mixing coeff.   \nreturn \u00b5(T), \u03c3(T)2 \u25b7K slot distributions ", "page_idx": 22}, {"type": "text", "text": "$(\\mu_{\\tau(\\tau(i))}^{\\prime},\\Sigma_{\\tau(\\pi(i))}^{\\prime})$ . Therefore, each mixture component ${\\bf s}_{\\pi(i)}$ is identifiable up to affine transformation, and permutation of slots representations in s. Now, given sampling $\\mathbf{s}_{k}$ is equivalent to obtaining $K$ samples from the GMM, $q(\\mathbf{z})$ and concatenating, this makes $q(\\mathbf{z})$ identifiable up to affine transformation, $h$ and permutation of slot representations in s. ", "page_idx": 22}, {"type": "text", "text": "It now trivially follows that each slot representation $\\mathbf{s}_{k}\\sim\\mathcal{N}(\\mathbf{s}_{k};\\mu_{k},\\Sigma_{k})\\in\\mathbb{R}^{d},\\forall\\,k\\in\\{1,\\dots,K\\}$ is identifiable up to affine transformation, $h$ based on the following observed property of GMMs: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}\\pi_{k}h_{\\sharp}\\,({\\mathcal N}({\\mathbf s}_{k};\\mu_{k},\\Sigma_{k}))\\sim h_{\\sharp}\\Big(\\sum_{k=1}^{K}\\pi_{k}{\\mathcal N}({\\mathbf s}_{k}^{\\prime};\\mu_{k}^{\\prime},\\Sigma_{k}^{\\prime})\\Big),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which concludes the proof. ", "page_idx": 22}, {"type": "text", "text": "E Algorithm ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "As a result of projection with separate query matrix is considered, the mean and variance updates are coupled with some non-linear affine transformation; for EM to have an exact solution, this transformation needs to be the identity (this can be seen when we derive the update equations for mean and variance). The resulting algorithms for these two cases are illustrated in algorithms 2 and 3. ", "page_idx": 22}, {"type": "table", "img_path": "qmoVQbwmCY/tmp/78e59c72923fe7baacf79e4870d219d94e0b4726a515ca82f5935846d75664ca.jpg", "table_caption": ["Table 4: Quality of compositional image generation with FID measure "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "F Metrics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "SIS: Slot identifiability score [6], mainly focus on R2 score between ground-truth and the estimated slot representations wrt to maximum R2 score from the models fit between each and every inferred slots. By design SIS requires a model fitting at every validation step to compute this relativistic measure, due to which the metric seems to vary quite a bit across runs, as observed in Figure 8. ", "page_idx": 23}, {"type": "text", "text": "SMCC: Mean correlation coefficient is a well studied metric in disentangled representational learning [39, 37], we extend this with additional permutational invariance on slot dimensions. SMCC measures the correlation between estimated and ground truth representations (or estimated representations across runs, in the case when ground truth representations are unavailable) once the slots are matched using Hungarian matching. To compute identifiability up to affine transformation along the representational axis and permutation in slots (check definition 3), similar to weak identifiability as per the definition in the paper and in [38], we use the MCC up to some affine mapping $\\pmb{A}$ , which we learned by matching slot representations across runs. In summary, SMCC can be computed with the following three steps: ", "page_idx": 23}, {"type": "text", "text": "\u2022 Matching the order of slots using Hungarian matching; ", "page_idx": 23}, {"type": "text", "text": "\u2022 Affine transformation of slot representation;   \n\u2022 Followed by computing mean correlation. ", "page_idx": 23}, {"type": "text", "text": "Apart from variations described in Figure 8, we further analyse both the metrics by fixing the model and data; and by computing both the metrics for 10 times. We then considered the mean and variance in the performance, which is reflected as follows: SIS: $35.26\\pm6.46$ , SMCC: $77.83\\pm{\\bf0.36}$ . Here, the resulting variation is only the reflection of the metric, which clearly indicates the stability of SMCC over SIS. ", "page_idx": 23}, {"type": "image", "img_path": "qmoVQbwmCY/tmp/23e839aa77febb51a2a6296eb49ad86480ae94219da029862b978cab40f80737.jpg", "img_caption": ["Figure 8: Identifiability scores throughout training. Our proposed SMCC metric (top) is much more stable than SIS (bottom) in capturing identifiability, and better in discerning differences between methods, showing substantial improvements for PSA. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "qmoVQbwmCY/tmp/b1a29363d9618df6a7ae6fcf6bf2f95a4e1c98f2a98a019cb5aa1d6b647ff03a.jpg", "img_caption": ["Figure 9: Random samples from the 2D synthetic dataset used in our aggregate posterior identifiability experiments. As outlined in the main text, there are in total five \u2018object\u2019 clusters in the dataset, and each observation contains at most three of the clusters. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "G Comparison with Autoencoding Variational Bayes ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "As explained in Section 3, applying slot attention to a deterministic encoding ${\\bf z}=f_{e}({\\bf x})\\in\\mathbb{R}^{N\\times d}$ yields a set of $K$ object slot representations $\\mathbf{s}_{1:K}:=\\mathbf{s}_{1},\\ldots,\\mathbf{s}_{K}$ . In combination, this process induces a stochastic encoder $q(\\mathbf{s}_{1:K}\\mid\\mathbf{x})$ , where the stochasticity comes from the random initialization of the solfo ittss  ipn rtehvei ofuirss t sittaetrea $\\mathbf{s}_{1:K}^{t\\stackrel{+}{=}0}\\sim\\mathcal{N}(\\mathbf{s}_{1:K};0,\\mathbf{I})\\in\\mathbb{R}^{K\\times d}$ .n dSionmcley  esaachm pslloe t iinsi tai adl estteartmesi tiacn fdu onbcttiaoinn $\\mathbf{s}^{t}:=\\bar{f_{s}}(\\mathbf{z},\\mathbf{s}^{t-1})$ $\\mathbf{s}^{0}$ stochastic estimates of the slots. However, since each transition depends on ${\\bf z}$ , which in turn depends on the input $\\mathbf{x}$ , we do not get a generative model we can tractably sample from. ", "page_idx": 24}, {"type": "text", "text": "This can be remedied by placing a tractable prior over $\\mathbf{z}$ and using the VAE framework along the lines of [77]. Specifically, Wang et al. [77] propose the Slot-VAE, which is a generative model that integrates slot attention and the VAE framework under a two-layer hierarchical latent model. However, under their formulation, there is a key challenge in calculating the KL term as the slot attention function is permutation equivariant meaning the slots have no fixed order across the posterior and prior. To compensate for this, the authors introduce a heuristic auxiliary loss and a parallel image processing path with an additional slot attention operation which is computationally costly. ", "page_idx": 24}, {"type": "text", "text": "In contrast, our approach does not suffer from such drawbacks. This was achieved by designing the slot attention operation itself as probabilistic, and proving that having a local (per-datapoint) GMM results in the aggregate slot posterior and the concatenated slots being GMM distributed (Section 4). We then proved a new slot identifiability result using this insight (Section 5). Our use of the aggregate posterior is inspired by but differs substantially in both method and application from previous works on VAEs [28, 71, 72]. Our primary goal is not to learn a better generative prior but to obtain slot identifiable representations. Lastly, since the concatenated slots are provably GMM distributed, our model is reminiscent of a GMM-VAE [11], but with a unique slot-based structure. ", "page_idx": 24}, {"type": "text", "text": "H Automatic Relevance Determination of Slots ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "For evaluation, we calculate the MAE between the estimated and ground truth numbers of slots and measure the reduction in FLOPs achieved using the proposed ARD method. We observe MAE values of 1.03 and 0.58, and significant savings of up to $41\\%$ and ${\\bf6}2\\%$ in FLOPs on the CLEVR and Objects-Room datasets respectively, when compared to using a fixed number of slots $K$ . ", "page_idx": 24}, {"type": "image", "img_path": "qmoVQbwmCY/tmp/ba94674e380667325b1479cf753b043e7bf29fd544f3fcf1a7d47c7c89935098.jpg", "img_caption": ["Figure 10: Automatic Relevance Determination (ARD) of slots on the OBJECTSROOM dataset. As shown, when using our proposed probabilistic slot attention (Algorithm 1), the mixing coefficients $\\pi_{i}\\in\\mathbb{R}^{K},\\forall i$ automatically approach zero when slots are inactive. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "I Compositionality Results ", "page_idx": 25}, {"type": "image", "img_path": "qmoVQbwmCY/tmp/c9418a9b0e68814fccce3635a6493b65cf328311699258656bd3f0bc672e56bb.jpg", "img_caption": ["Figure 11: Aggregate posterior sampling for image composition on CLEVR dataset. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "qmoVQbwmCY/tmp/0add0ec1e145c89e7813669ecc48888fd7d3c54da80a89255265cb6543128438.jpg", "img_caption": ["Figure 12: Aggregate posterior sampling for image composition on OBJECTSROOM dataset. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "qmoVQbwmCY/tmp/a6083350634a9521bd98682cbd4ff9d2c19f81e8cced66f66af4bdb0e73d49be.jpg", "img_caption": ["Figure 13: Visualizations of attention and alpha masks on the Pascal VOC2012 dataset are shown, with alpha masks on the left and attention masks on the right, for a PSA-MLP model using a DINO feature extractor. In the figure above, the images from left to right represent: the original image, ground-truth segmentation, alpha mask segmentation, individual entities grouped in the alpha mask, slot attention segmentation mask, and individual entities grouped in the slot attention mask. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "qmoVQbwmCY/tmp/14cb033bab5a88f86f5d4fd840dd57d1c8e60ba01ec001bbfb61c20a5d91a630.jpg", "img_caption": ["Figure 14: Visualizations of attention and alpha masks on the Pascal VOC2012 dataset are shown, with alpha masks on the left and attention masks on the right, for a PSA-Transformer model using a DINO feature extractor. In the figure above, the images from left to right represent: the original image, ground-truth segmentation, alpha mask segmentation, individual entities grouped in the alpha mask, slot attention segmentation mask, and individual entities grouped in the slot attention mask. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We clearly state the limitations in existing literature and our proposed analysis to overcome these limitation, which we further back with theoretical guarantees and experimental observations. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We provide a detailed discussion on limitations and explicitly list all the assumptions made, which serves nicely for future works. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We provide rigorous mathematical proof all our theoretical claims. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We clearly list all the set of hyperparameters used in the analysis. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 29}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We provide codebase with the described set of hyper-parameters for reproducability. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: We discuss hyper-parameters in the paper along with the codebase for earier reproducability. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 30}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We report all our analysis across 5 random runs. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Detailed in appendix. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 31}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: - ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 31}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: This paper proposes a Probabilistic Slot Attention algorithm, whose goal is to achieve identifiable object-centric representations. The work primarily makes theoretical advancements in the field of object-centric learning, and as such it has little immediate societal or ethical consequences. Our method might be a step towards interpretable and aligned models which are desired properties of trustworthy AI. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 31}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] Justification: - ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 32}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: - ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. ", "page_idx": 32}, {"type": "text", "text": "\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] Justification: - Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 33}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 33}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA]   \nJustification: -   \nGuidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}]