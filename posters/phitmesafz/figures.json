[{"figure_path": "pHiTmEsAfZ/figures/figures_2_1.jpg", "caption": "Figure 1: Overview: In the inference process of policy parameter generation, conditioning on behavior embeddings from the agent's trajectory, the latent diffusion model denoises random noise into a latent parameter representation, which can then be reconstructed as a deployable policy using the autoencoder. The forward process for progressively noising the data is also conducted on the latent space after encoding policy parameters as latent representations.", "description": "This figure illustrates the overall workflow of Make-An-Agent.  It shows how the system takes an agent's trajectory as input, extracts behavior embeddings, and feeds them into a latent diffusion model. This model, conditioned on the behavior embeddings, generates latent parameter representations.  These representations are then decoded by an autoencoder to produce a deployable policy network. The diagram also depicts the forward process of adding noise to the policy parameters before the denoising step.", "section": "3 Methodology"}, {"figure_path": "pHiTmEsAfZ/figures/figures_2_2.jpg", "caption": "Figure 2: Autoencoder: Encoding policy param- Figure 3: Contrastive behavior embeddings: Learning informative behavior embeddings from long trajectories with contrastive loss.", "description": "This figure shows two subfigures. The left subfigure (Figure 2) illustrates the autoencoder architecture used to encode and decode policy network parameters into a lower-dimensional latent representation.  The input is the flattened policy network parameters, then encoded into a latent space and decoded back into parameters. The right subfigure (Figure 3) illustrates how contrastive learning is used to generate behavior embeddings. A long trajectory is divided into two parts:  the initial n steps (\u03c4n), and the m steps after the first success step (\u03c4\u0302). These two parts are separately projected into embedding vectors (h and v, respectively) using projection layers (\u03c6). A contrastive loss function is then used to learn these behavior embeddings, which capture the mutual information between the preceding trajectory and subsequent states after the first success.", "section": "Methodology"}, {"figure_path": "pHiTmEsAfZ/figures/figures_4_1.jpg", "caption": "Figure 4: Visualization of MetaWorld, Robosuite, and real quadrupedal locomotion.", "description": "This figure shows three different experimental setups used in the paper.  The leftmost panel shows the MetaWorld simulation environment, which features a robotic arm interacting with various objects on a table.  The middle panel depicts the Robosuite simulation environment, which also includes a robotic arm performing manipulation tasks. The rightmost panel showcases a real-world quadrupedal robot navigating an environment with obstacles.", "section": "4 Experiments"}, {"figure_path": "pHiTmEsAfZ/figures/figures_5_1.jpg", "caption": "Figure 5: Evaluation of seen tasks with 5 random initializations on MetaWorld and Robosuite. Our method generate policies using 5/10/50/100 test trajectories. Baselines are finetuned/adapted by the same test trajectories. Results are averaged over training with 4 seeds.", "description": "This figure displays the success rate of different methods on seen tasks in MetaWorld and Robosuite.  The x-axis represents the number of test trajectories used, while the y-axis shows the success rate.  The \"Generated Best\" and \"Generated Top 5\" lines represent the performance of the proposed Make-An-Agent method, showcasing its ability to generate high-performing policies from a small number of trajectories. The other lines represent baseline methods, such as multi-task imitation learning and meta-reinforcement learning, which require more data for comparable performance. The results show that Make-An-Agent significantly outperforms the baseline methods, especially when only a few trajectories are available.", "section": "4.1 Performance Analysis"}, {"figure_path": "pHiTmEsAfZ/figures/figures_6_1.jpg", "caption": "Figure 6: Evaluation of 8 unseen tasks with 5 random initializations on MetaWorld and Robosuite. Our method generates policies using 50/100 test trajectories without any finetuning. Baselines are adapted using the same test trajectories. Average results are from training with 4 seeds.", "description": "This figure compares the performance of Make-An-Agent and several baseline methods on unseen tasks in two robotic manipulation benchmarks: MetaWorld and Robosuite.  The x-axis represents the number of test trajectories used.  The bars show the success rate (with error bars) for each method. Notably, Make-An-Agent (Generated top 5 and Generated Best) outperforms the baselines, demonstrating strong generalization capabilities even without fine-tuning on the unseen tasks. The results highlight the effectiveness of the proposed method for generating policies.", "section": "4.1 Performance Analysis"}, {"figure_path": "pHiTmEsAfZ/figures/figures_7_1.jpg", "caption": "Figure 6: Evaluation of 8 unseen tasks with 5 random initializations on MetaWorld and Robosuite. Our method generates policies using 50/100 test trajectories without any finetuning. Baselines are adapted using the same test trajectories. Average results are from training with 4 seeds.", "description": "This figure compares the performance of Make-An-Agent and four baseline methods on 8 unseen tasks in MetaWorld and Robosuite.  Make-An-Agent generates policies using only 50 or 100 test trajectories without any fine-tuning, showcasing its generalization ability.  Baseline methods are adapted using the same test trajectories. The results are averaged across 4 different training seeds and 5 random initializations to assess the robustness of the generated policies.", "section": "4.1 Performance Analysis"}, {"figure_path": "pHiTmEsAfZ/figures/figures_7_2.jpg", "caption": "Figure 8: Ablation studies about using different embeddings as conditions in policy generation on MetaWorld 5 unseen tasks. (Top 5 models)", "description": "This ablation study investigates the impact of different choices of behavior embeddings on the performance of the proposed method.  The figure compares the success rate on 5 unseen MetaWorld tasks when using different embeddings (embedding h, embedding v, an embedding layer) as condition inputs to the diffusion model for policy generation, against the full model's performance (Ours). The results are shown for both 50 and 100 trajectories as input.", "section": "4.2 Ablation Studies"}, {"figure_path": "pHiTmEsAfZ/figures/figures_7_3.jpg", "caption": "Figure 9: Trajectory difference: trajectories as conditional inputs v.s. trajectories from synthesized policies as outputs on MetaWorld 4 unseen tasks.", "description": "This figure visualizes the differences between trajectories used as conditional inputs for the policy generation model and the trajectories generated by the model when deployed on four unseen tasks from the MetaWorld environment.  The visualization aims to highlight the diversity of policies generated by the model, showcasing that the generated policies do not simply mimic the input trajectories but rather produce novel and diverse behaviors.", "section": "4.1 Performance Analysis"}, {"figure_path": "pHiTmEsAfZ/figures/figures_7_4.jpg", "caption": "Figure 10: Real-world locomotion tasks, including turning, fast backward movement, and obstacle avoidance on a mat.", "description": "This figure shows two real-world locomotion tasks performed by a quadruped robot using policies generated by Make-An-Agent.  The left image depicts the robot making agile turns to avoid a bouquet while moving across a mat. The right image displays the robot navigating around a ball and goal while moving swiftly backward. This demonstrates the ability of Make-An-Agent to generate policies that enable robust and complex locomotion behaviors in real-world scenarios.", "section": "4 Experiments"}, {"figure_path": "pHiTmEsAfZ/figures/figures_7_5.jpg", "caption": "Figure 11: Parameter Similarity: Parameter cosine similarity between RL-trained policies and our generated policies or fine-tuned policies.", "description": "This figure compares the cosine similarity between parameters of RL-trained policies and those generated by the proposed method (Make-An-Agent) and those obtained after fine-tuning. The comparison is done for both seen and unseen tasks.  The results show that the generated parameters for seen tasks have a relatively high similarity to the fine-tuned parameters, indicating that the model can generate similar policies. However, for unseen tasks, the similarity is much lower, suggesting the model is capable of generating novel policies, rather than simply memorizing the training data.", "section": "4.2 Ablation Studies"}, {"figure_path": "pHiTmEsAfZ/figures/figures_8_1.jpg", "caption": "Figure 12: Ablation studies of our technical designs on MetaWorld with 50 test trajectories (Top 5 models).", "description": "This figure presents the results of ablation studies conducted on the MetaWorld environment using 50 test trajectories. The top 5 models were selected for analysis.  Four subfigures illustrate the impact of various design choices on the model's performance: (a) Trajectory Length showing the effect of varying the length of input trajectories; (b) Policy Model Size demonstrating how the size of the policy network affects performance; (c) Parameter Number highlighting the influence of the number of parameters used for training; and (d) Representation Size illustrating how the dimensionality of the latent parameter representations impacts results. Each subfigure shows the success rate on both seen and unseen tasks.", "section": "4.2 Ablation Studies"}, {"figure_path": "pHiTmEsAfZ/figures/figures_16_1.jpg", "caption": "Figure 13: Correlation between condition trajectories and generated policies. Trajectory length accurately reflects the effectiveness of the policies compared to the success rate. The maximum episode length in all the tasks is 500 (represents failure).", "description": "This figure displays the correlation between the length of the input trajectories (condition trajectories) used to generate policies and the length of the trajectories produced by those generated policies.  The three subplots show this relationship for seen tasks, unseen tasks, and unseen tasks with added noise.  Longer condition trajectories generally lead to longer generated trajectories, indicating that the model is learning to synthesize effective policies based on the duration of the provided example behavior. A longer trajectory often implies a more complex task, and the figure suggests that the model's performance is strongly tied to the comprehensiveness of the input demonstration.", "section": "4.1 Performance Analysis"}, {"figure_path": "pHiTmEsAfZ/figures/figures_16_2.jpg", "caption": "Figure 14: Computational budgets of ours and baselines", "description": "This bar chart compares the computational costs (in GPU hours) of the proposed Make-An-Agent method with four baseline methods: single RL, multi-task RL, meta RL, and meta IL.  It shows that Make-An-Agent, when considering both training and evaluation time, has a significantly lower computational cost than the other methods. This highlights the efficiency of the proposed approach.", "section": "4 Experiments"}, {"figure_path": "pHiTmEsAfZ/figures/figures_16_3.jpg", "caption": "Figure 4: Visualization of MetaWorld, Robosuite, and real quadrupedal locomotion.", "description": "This figure shows three different robotic environments used in the experiments of the paper: MetaWorld (a simulated robotic tabletop manipulation environment), Robosuite (another simulated robotic manipulation environment), and a real-world quadrupedal locomotion scenario.  It visually demonstrates the diversity of tasks and environments on which the Make-An-Agent approach was tested, highlighting its applicability across various platforms and domains.", "section": "4 Experiments"}]