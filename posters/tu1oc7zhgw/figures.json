[{"figure_path": "tu1oC7zHGW/figures/figures_1_1.jpg", "caption": "Figure 1: Here is the overview of our paper. Part (a) indicates two examples of Inconsistency between discriminative answers and generative captions, where the answers marked in blue contradict the answers marked in purple. Part (b) shows the Consistency evaluation method Conbench and its discriminative top three leaderboard. Part(c) reveals the main three findings built upon ConBench.", "description": "This figure provides a high-level overview of the paper.  Part (a) shows examples of inconsistencies found in large vision-language models (LVLMs), where different question formats yield contradictory answers for the same image. Part (b) introduces ConBench, a novel benchmark for evaluating consistency, and shows its top three performing models.  Part (c) summarizes the paper's three main findings based on the ConBench analysis.", "section": "1 Introduction"}, {"figure_path": "tu1oC7zHGW/figures/figures_2_1.jpg", "caption": "Figure 2: Overview of 19 evaluation detailed categories in ConBench.", "description": "This figure provides a visual representation of the ConBench benchmark's hierarchical structure. It's divided into three core capabilities: Sensation, Cognition, and Knowledge, each encompassing various sub-categories of tasks.  The size of each section visually represents the proportion of questions related to that category within the benchmark. The concentric circles also represent the increasing level of difficulty from sensation to knowledge, with sensation tasks being the easiest and knowledge tasks the most challenging.", "section": "3 ConBench"}, {"figure_path": "tu1oC7zHGW/figures/figures_4_1.jpg", "caption": "Figure 4: The pipeline of judging Consistency between caption and discriminative answers via GPT/GPT4. Please zoom in to view the prompt.", "description": "This figure illustrates the process of evaluating the consistency between a model's caption generation and its answers to three types of discriminative questions (True/False, Multiple Choices, and VQA).  The process uses a machine reading comprehension (MRC) task, where GPT/GPT4 acts as a judge, comparing the generated caption against the discriminative answers to determine consistency or inconsistency.", "section": "3.4 Multidimensional Evaluation Metric"}, {"figure_path": "tu1oC7zHGW/figures/figures_5_1.jpg", "caption": "Figure 5: The confidence and logits of answers of LLaVA-13B and MGM-13B", "description": "This figure shows a comparison of the confidence and logits of answers given by two different large vision-language models (LLaVA-13B and MGM-13B) for three types of questions: True/False, Choice, and VQA.  The x-axis represents the question type and the y-axis shows the confidence (left panel) and logits (right panel) scores.  The bars are color-coded to differentiate between correct answers (brighter colors) and incorrect answers (duller colors). The figure demonstrates that the confidence and logits vary among the question types and that there is a difference between the two models in their response behavior.  Higher values in the confidence and logits generally indicate higher certainty in the model's answer.", "section": "4.2 Discriminative Domain"}, {"figure_path": "tu1oC7zHGW/figures/figures_6_1.jpg", "caption": "Figure 6: Visualization of the relationship between the correct rate of discriminative answer and its Consistency with the caption on different answer types.", "description": "This figure visualizes the correlation between the accuracy of discriminative answers and their consistency with the generated caption across three different question types: True/False, Multiple Choice, and VQA.  The x-axis represents the accuracy of the discriminative answers, while the y-axis shows the consistency rate (percentage of times the answer aligns with the caption). Each point represents a specific LVLMs' performance. The plots show a clear positive correlation, indicating that more accurate discriminative answers tend to have higher consistency with the captions. The Pearson correlation coefficient (P[X,Y]) is provided for each plot, demonstrating the strength of the relationship.", "section": "4.2 Discriminative Domain"}, {"figure_path": "tu1oC7zHGW/figures/figures_7_1.jpg", "caption": "Figure 7: Visualization of the relationship between the correct rate of discriminative answer and its Consistency with the caption on different capability types.", "description": "This figure visualizes the correlation between the accuracy of discriminative answers and their consistency with the generated captions across three core capabilities: Sensation, Cognition, and Knowledge.  Each point represents a specific large vision-language model (LVM). The x-axis shows the accuracy of the model's discriminative answers (choices), while the y-axis represents the consistency rate between those answers and the caption. The green dotted line shows the linear regression fit. The figure demonstrates that higher accuracy generally corresponds to higher consistency across all three capabilities, suggesting a strong relationship between the ability to answer discriminative questions correctly and generate consistent captions.  The Pearson correlation coefficients (P[X,Y]) are also provided for each capability, indicating a strong positive correlation.", "section": "4.3 Generative Domain"}, {"figure_path": "tu1oC7zHGW/figures/figures_8_1.jpg", "caption": "Figure 8: The Trigger-based Diagnostic Refinement pipeline.", "description": "This figure illustrates the pipeline of the trigger-based diagnostic refinement method.  It shows how a vision-language model (LVLMs) generates a caption for an image, with each word assigned a probability score.  Words with low probability scores trigger a True/False question about the presence or absence of a specific item (e.g., \"Is there a cat in the picture?\").  The LVLMs answers this question, and both the question and answer are added to the original prompt. The refined prompt is then fed back into the LVLMs to generate a new, improved caption.", "section": "5 Trigger-based Diagnostic Refinement"}, {"figure_path": "tu1oC7zHGW/figures/figures_11_1.jpg", "caption": "Figure 1: Here is the overview of our paper. Part (a) indicates two examples of Inconsistency between discriminative answers and generative captions, where the answers marked in blue contradict the answers marked in purple. Part (b) shows the Consistency evaluation method Conbench and its discriminative top three leaderboard. Part(c) reveals the main three findings built upon ConBench.", "description": "This figure provides a high-level overview of the paper. Part (a) shows examples of inconsistencies between model answers to different question types (discriminative and generative). Part (b) introduces ConBench, a benchmark used for evaluating model consistency, along with its top-performing models. Part (c) summarizes the main findings obtained using ConBench, illustrating the relationship between solution space size, accuracy, and consistency.", "section": "1 Introduction"}, {"figure_path": "tu1oC7zHGW/figures/figures_11_2.jpg", "caption": "Figure 1: Here is the overview of our paper. Part (a) indicates two examples of Inconsistency between discriminative answers and generative captions, where the answers marked in blue contradict the answers marked in purple. Part (b) shows the Consistency evaluation method Conbench and its discriminative top three leaderboard. Part(c) reveals the main three findings built upon ConBench.", "description": "This figure provides a high-level overview of the paper.  Part (a) shows examples of inconsistencies where the model provides contradictory answers to similar questions based on the same image.  Part (b) introduces the ConBench benchmark, highlighting its evaluation method and showing a top three leaderboard of models.  Part (c) summarizes the key findings from the benchmark.", "section": "1 Introduction"}, {"figure_path": "tu1oC7zHGW/figures/figures_13_1.jpg", "caption": "Figure 1: Here is the overview of our paper. Part (a) indicates two examples of Inconsistency between discriminative answers and generative captions, where the answers marked in blue contradict the answers marked in purple. Part (b) shows the Consistency evaluation method Conbench and its discriminative top three leaderboard. Part(c) reveals the main three findings built upon ConBench.", "description": "This figure provides a high-level overview of the paper's contributions.  Part (a) showcases examples of inconsistencies between model responses to different question types about the same image, highlighting a key problem the paper addresses. Part (b) introduces ConBench, a new benchmark created to evaluate model consistency, showing its leaderboard.  Part (c) summarizes the paper's three main findings based on the analysis of the benchmark.", "section": "1 Introduction"}, {"figure_path": "tu1oC7zHGW/figures/figures_13_2.jpg", "caption": "Figure 1: Here is the overview of our paper. Part (a) indicates two examples of Inconsistency between discriminative answers and generative captions, where the answers marked in blue contradict the answers marked in purple. Part (b) shows the Consistency evaluation method Conbench and its discriminative top three leaderboard. Part(c) reveals the main three findings built upon ConBench.", "description": "This figure provides a high-level overview of the paper. Part (a) illustrates two examples where the model gives inconsistent answers. The answers marked in blue contradict the answers marked in purple. Part (b) shows the ConBench tool used to evaluate model consistency, along with a ranking of the top three models. Part (c) summarizes the three main findings of the paper.", "section": "1 Introduction"}, {"figure_path": "tu1oC7zHGW/figures/figures_13_3.jpg", "caption": "Figure 1: Here is the overview of our paper. Part (a) indicates two examples of Inconsistency between discriminative answers and generative captions, where the answers marked in blue contradict the answers marked in purple. Part (b) shows the Consistency evaluation method Conbench and its discriminative top three leaderboard. Part(c) reveals the main three findings built upon ConBench.", "description": "This figure provides a high-level overview of the research paper. Part (a) shows examples where vision-language models give inconsistent answers depending on how the question is phrased, even when referring to the same image. Part (b) introduces the ConBench, a new benchmark used to evaluate the consistency of these models, and shows its top-performing models.  Part (c) summarizes three key findings based on the ConBench results.", "section": "1 Introduction"}, {"figure_path": "tu1oC7zHGW/figures/figures_13_4.jpg", "caption": "Figure 1: Here is the overview of our paper. Part (a) indicates two examples of Inconsistency between discriminative answers and generative captions, where the answers marked in blue contradict the answers marked in purple. Part (b) shows the Consistency evaluation method Conbench and its discriminative top three leaderboard. Part(c) reveals the main three findings built upon ConBench.", "description": "This figure provides a high-level overview of the paper.  Part (a) shows two examples illustrating inconsistencies between the answers given by large vision-language models (LVLMs) for discriminative questions and their corresponding generative captions. The inconsistencies highlight a key problem the paper addresses: LVLMs often give contradictory responses depending on how the question is phrased. Part (b) introduces ConBench, a new benchmark designed to evaluate the consistency of LVLM responses across different question types, showing its top three performing models.  Finally, Part (c) summarizes the three main findings of the study based on the ConBench results.", "section": "1 Introduction"}, {"figure_path": "tu1oC7zHGW/figures/figures_14_1.jpg", "caption": "Figure 1: Here is the overview of our paper. Part (a) indicates two examples of Inconsistency between discriminative answers and generative captions, where the answers marked in blue contradict the answers marked in purple. Part (b) shows the Consistency evaluation method Conbench and its discriminative top three leaderboard. Part(c) reveals the main three findings built upon ConBench.", "description": "This figure provides a high-level overview of the paper.  Part (a) shows two examples where the model provides inconsistent answers.  The first answer is a caption describing the image, while the second is a yes/no response to a question about the image. These two answers contradict each other.  Part (b) shows the ConBench tool which was developed to evaluate the consistency of Large Vision-Language Models (LVLMs). It shows the top 3 performing models. Part (c) summarizes the three main findings from the evaluation.", "section": "1 Introduction"}, {"figure_path": "tu1oC7zHGW/figures/figures_14_2.jpg", "caption": "Figure 1: Here is the overview of our paper. Part (a) indicates two examples of Inconsistency between discriminative answers and generative captions, where the answers marked in blue contradict the answers marked in purple. Part (b) shows the Consistency evaluation method Conbench and its discriminative top three leaderboard. Part(c) reveals the main three findings built upon ConBench.", "description": "This figure provides a high-level overview of the paper's content.  Part (a) shows examples of inconsistencies between model answers to different question types about the same image. Part (b) introduces ConBench, a benchmark designed to evaluate model consistency.  Part (c) summarizes the key findings derived from using ConBench.", "section": "1 Introduction"}, {"figure_path": "tu1oC7zHGW/figures/figures_15_1.jpg", "caption": "Figure 1: Here is the overview of our paper. Part (a) indicates two examples of Inconsistency between discriminative answers and generative captions, where the answers marked in blue contradict the answers marked in purple. Part (b) shows the Consistency evaluation method Conbench and its discriminative top three leaderboard. Part(c) reveals the main three findings built upon ConBench.", "description": "This figure provides a high-level overview of the paper. Part (a) illustrates the concept of inconsistency in vision-language models by showing examples where the model gives contradictory answers to similar questions based on the same image. Part (b) introduces ConBench, a benchmark used to evaluate the consistency of these models, and highlights the top three performing models. Finally, part (c) summarizes the three main findings obtained using ConBench.", "section": "1 Introduction"}, {"figure_path": "tu1oC7zHGW/figures/figures_15_2.jpg", "caption": "Figure 3: The prompt for generation of discriminative questions. Please zoom in to view.", "description": "This figure shows the prompt used to generate the different types of questions used in the ConBench benchmark.  The prompt instructs a language model to act as a question expert and, given an initial discriminative question type (e.g., true/false), generate two additional discriminative questions and a VQA question, all revolving around the same knowledge point and related to the given image. This ensures diverse question formats to comprehensively evaluate the consistency of large vision-language models.", "section": "3.2 Hierarchical Core Capabilities"}, {"figure_path": "tu1oC7zHGW/figures/figures_15_3.jpg", "caption": "Figure 3: The prompt for generation of discriminative questions. Please zoom in to view.", "description": "This figure shows the prompt used to instruct a large language model (LLM) to generate discriminative questions. The prompt instructs the LLM to act as a question expert and generate two additional questions based on an initial discriminative question. The additional questions are to be of a different discriminative type than the original question and must focus on the same knowledge point. This process helps ensure that the model is tested on a range of question types and not just a single question format.", "section": "3 ConBench"}, {"figure_path": "tu1oC7zHGW/figures/figures_15_4.jpg", "caption": "Figure 2: Overview of 19 evaluation detailed categories in ConBench.", "description": "This figure provides a visual representation of the ConBench benchmark's hierarchical structure.  It shows how the 1000 images in the benchmark are categorized across three core capabilities: Sensation, Cognition, and Knowledge.  Each core capability is further divided into several sub-categories, illustrating the diversity and complexity of the tasks evaluated. The visual representation helps illustrate the breadth and depth of ConBench's evaluation, emphasizing its comprehensive assessment of large vision-language models (LVLMs).", "section": "3 ConBench"}]