[{"figure_path": "tu1oC7zHGW/tables/tables_4_1.jpg", "caption": "Table 1: Evaluation[D] of mainstreams series of LVLMs on ConBench. The detailed results of the Sensation, Cognition, and Knowledge core capabilities are listed below. T, C, and V represent true-false, multiple-choice, and limited VQA questions, respectively. The ranking can be found below the respective numbers. \u2020: Due to safety considerations, GPT-4V declined to answer the celebrity category.", "description": "This table presents the performance of various Large Vision-Language Models (LVLMs) on the ConBench benchmark.  It breaks down the results based on three core capabilities (Sensation, Cognition, and Knowledge) and three question types (True/False, Multiple Choice, and Visual Question Answering - VQA).  Each question type assesses the model's accuracy and consistency in different aspects of visual understanding.  The table also includes a ConScore[D], which is a composite score reflecting the overall consistency of the LVLMs.", "section": "4.1 Evaluation Results"}, {"figure_path": "tu1oC7zHGW/tables/tables_5_1.jpg", "caption": "Table 1: Evaluation[D] of mainstreams series of LVLMs on ConBench. The detailed results of the Sensation, Cognition, and Knowledge core capabilities are listed below. T, C, and V represent true-false, multiple-choice, and limited VQA questions, respectively. The ranking can be found below the respective numbers. \u2020: Due to safety considerations, GPT-4V declined to answer the celebrity category.", "description": "This table presents the results of evaluating various Large Vision-Language Models (LVLMs) on the ConBench benchmark.  The evaluation metric used is ConScore[D], which measures the consistency of the models' responses across different question types (true/false, multiple-choice, and visual question answering).  The table shows the performance broken down by three core capabilities: Sensation, Cognition, and Knowledge.  Each LVLMs' performance is shown with its ConScore[D] and the individual scores for each question type (T, C, V) for each capability.  The table also includes rankings and notes on any limitations (e.g., GPT-4V's inability to answer the celebrity category due to safety considerations).", "section": "4.1 Evaluation Results"}, {"figure_path": "tu1oC7zHGW/tables/tables_8_1.jpg", "caption": "Table 4: Results on LLaVA-34B and MiniGemini-34B via Trigger-based Diagnostic Refinement.", "description": "This table presents the results of applying the Trigger-based Diagnostic Refinement (TDR) method to the LLaVA-NeXT-34B and MiniGemini-34B models.  The ConScore[C] metric, representing the consistency between the caption and the discriminative answers, is shown along with the individual consistency scores for True/False (Con[T]), multiple-choice (Con[C]), and VQA (Con[V]) question types.  The improvements achieved by TDR are highlighted, showing significant gains in overall consistency for both models.", "section": "4 Analysis"}]