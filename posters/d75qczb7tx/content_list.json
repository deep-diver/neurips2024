[{"type": "text", "text": "Context-Aware Testing: A New Paradigm for Model Testing with Large Language Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Paulius Rauba\u2217 University of Cambridge pr501@cam.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Nabeel Seedat\u2217 University of Cambridge ns741@cam.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Max Ruiz Luyten University of Cambridge mr971@cam.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Mihaela van der Schaar University of Cambridge mv472@cam.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The predominant de facto paradigm of testing ML models relies on either using only held-out data to compute aggregate evaluation metrics or by assessing the performance on different subgroups. However, such data-only testing methods operate under the restrictive assumption that the available empirical data is the sole input for testing ML models, disregarding valuable contextual information that could guide model testing. In this paper, we challenge the go-to approach of data-only testing and introduce context-aware testing (CAT) which uses context as an inductive bias to guide the search for meaningful model failures. We instantiate the first CAT system, SMART Testing, which employs large language models to hypothesize relevant and likely failures, which are evaluated on data using a selffalsification mechanism. Through empirical evaluations in diverse settings, we show that SMART automatically identifies more relevant and impactful failures than alternatives, demonstrating the potential of CAT as a testing paradigm. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The ability to rigorously test and validate machine learning (ML) models is crucial for their reliable deployment in real-world applications. Despite solid aggregate performance, ML models are unreliable in a variety of real-world scenarios, such as on different subgroups [1\u20136], or encountering data deviating from its training distribution [7\u201310], often resulting in significant financial or societal consequences. These failures point to deficiencies in the way we test such ML models. To understand this in greater detail, let\u2019s address two questions: how are we currently testing ML models and can we do better? ", "page_idx": 0}, {"type": "text", "text": "How are we testing? The predominant de facto paradigm of testing ML models relies on using only held-out data to evaluate the model either on average or by assessing performance on different subgroups within the dataset. Such data-only methods optimize a given objective function to find subgroups (or slices of data) within the dataset where a trained model underperforms relative to aggregate performance. We refer to this relative underperformance as a model failure. However, data-only methods operate under the restrictive assumption that the available empirical data is the sole input for testing ML model performance. In practice, this is almost always violated. It is common to have a priori knowledge of where models are likely to fail, given the problem context. ", "page_idx": 0}, {"type": "text", "text": "The restrictive assumption of data-only methods comes at a significant cost. Specifically, data-only methods, which have been the go-to approach for ML testing, are required to iterate and test the performance across a large number of possible subgroups. Each subgroup test is equivalent to evaluating a separate hypothesis about the model\u2019s performance on that specific slice of data. This raises a subtle but important problem\u2014from the perspective of hypothesis testing, such methods implicitly test multiple hypotheses (Sec. 3.1). The more subgroups we test, the more hypotheses we implicitly evaluate. Consequently, this results in critical problems associated with multiple testing: (i) high false positive and (ii) high false negative rates (Sec. 3.2). A third complementary challenge is that each data slice tested is (iii) not necessarily practically meaningful. The practical implications of these drawbacks are quite concerning\u2014they hinder our ability to accurately identify where the model performs poorly, thereby undermining the reliability of our model evaluations. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Can we do better? To address these three challenges, we propose to loosen the restrictive assumption of reliance only on data and propose a new paradigm of testing called Context-Aware Testing (CAT) to offer an alternative to the dominant data-only view (Sec. 3.3). CAT provides a principled approach to incorporate external knowledge\u2014or context\u2014 to the ML testing process. With the view that each evaluated data slice corresponds to an implicit hypothesis test, we propose that ML evaluation on observational datasets can be achieved via a context-guided sampling mechanism (Definition 1). This mechanism is a sampling procedure that uses context as an inductive bias to prioritize specific data slices to test for which have a higher chance of surfacing meaningful model failures. Therefore, CAT fundamentally helps to answer the question of \u201cwhat should we test for?\u201d. ", "page_idx": 1}, {"type": "text", "text": "Let\u2019s consider an example of building an ML model to predict prostate cancer [11]. Data-only methods employ a search procedure over the dataset to find divergence across a large number of possible feature combinations which may lead to (i) high false positive rates by identifying spurious underperforming subgroups (e.g. based on eye color or patient ID); (ii) high false negative rates by failing to identify true underperforming subgroups due to the large number of combinations tested and applied testing corrections; and (iii) testing subgroups which are not practically meaningful (e.g. interaction between eye color and height). In contrast, a CAT-based approach would define and target task-relevant subgroups, limiting the number of tests conducted with better false positive control and greater statistical power. As we empirically show in Sec. 5, obtaining many false positives and false negatives is overwhelmingly common in current testing practices. ", "page_idx": 1}, {"type": "text", "text": "In bringing CAT to reality, we develop the framework called SMART 2 Testing, which performs automated ML model evaluation by actively identifying potential failure cases (Sec. 4). SMART uses large language models (LLMs) to generate contextually relevant failure hypotheses to test. We further introduce a self-falsification mechanism, to automatically validate the generated failure hypotheses using the available data, allowing efficient pruning of spurious hypotheses. Finally, SMART generates comprehensive model reports that provide insights into the identified failure modes, their impact, and potential root causes, enabling stakeholders to make informed decisions. ", "page_idx": 1}, {"type": "text", "text": "Contributions. $\\Phi$ We identify critical gaps in predominant data-only ML testing, illustrating they miss important dimensions (Sec. 3). $\\circleddash$ We formalize the Context-Aware Testing paradigm, providing a principled framework to incorporate context in addition to data into the testing process, which is then used to guide the generation of targeted tests (Sec. 3). $\\circledast$ We build the first contextaware testing system, SMART Testing, which employs LLMs to hypothesize likely and relevant model failures and empirically refutes them with data using a novel self-falsification mechanism (Sec. 4). $\\circledast$ We demonstrate the value of context for effective testing, challenging the de facto data-only paradigm by showing how SMART identifies impactful model failures while avoiding false positives across diverse settings, when compared to data-only testing. Additionally, SMART identifies failures on important societal groups and generates comprehensive model reports (Sec. 5). ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "To highlight the need for SMART Testing, we contrast it with other ML testing paradigms \u2014 specifically Data-only testing methods which address the same testing problem as SMART. We provide an overview in Table 6 and an extended discussion in Appendix A. ", "page_idx": 1}, {"type": "text", "text": "Data-only testing methods [12\u201315]: Address the question: \u201cwhat should we test?\u201d. Data-only methods search the data to find \u201cslices\u201d where the model\u2019s predictions underperform compared to average performance, deeming those slices as model failures. Although automated, data-only approaches operate only on raw data without accounting for the problem context. Consequently, they must search across a large space of potential failures, usually covering all subsets of features and their distinct values. While an exhaustive search may seem beneficial, as we show in Sec. 3, the reality is that performing many tests on a finite dataset risks discovering slices where model failure is irrelevant or due to random variability, i.e. the multiple testing problem. SMART Testing addresses this challenge by prioritizing relevant and likely model failures through contextual awareness. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Orthogonal testing dimensions: While SMART addresses what to test, several other dimensions of model testing exist that are orthogonal to our approach (detailed in Appendix A). (i) Behavioral Testing [16, 17, 5] evaluates model behaviors (i.e. responses to data) by operationalizing tests along pre-defined dimensions (often defined by humans) \u2014 rather than discovering the test dimensions. SMART fundamentally differs by addressing the core issue of \u201cwhat to test\u201d. (ii) Software functional testing [18, 19], aims to primarily test functional correctness (e.g. input-output functionality such as monotonicity), rather than testing for failures. In addition, test cases are either pre-specified or specified with an approximation of the underlying model to probe for functional correctness. ", "page_idx": 2}, {"type": "text", "text": "3 A context-aware testing framework for ML ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The prevailing paradigm for testing ML models relies on data-only methods which exclusively use data to surface model failures. In this section, we explore the limitations of data-only methods by viewing ML testing as a multiple hypothesis testing problem. We explore why data-only methods are uniquely prone to finding false positive and false negative model failures. To address this, we introduce a new paradigm of testing called context-aware testing which relies on external knowledge, or context, as an inductive bias to better identify where models fail. ", "page_idx": 2}, {"type": "text", "text": "3.1 A multiple hypothesis testing view of ML evaluation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Preliminaries. Denote the feature space by $\\mathcal{X}$ and the label space by $\\boldsymbol{\\wp}$ , and $\\mathcal{P}$ the joint probability distribution over $\\mathcal X\\times\\mathcal X$ . We wish to test a fixed, trained black-box model $f:\\mathcal X\\to\\mathcal Y$ , using a finite dataset D \u2282X \u00d7 Y usually split into Dtrain = {(xi, yi)}iN=tr1a inand Dtest = {(xi, yi)}iN=te1s t. We assume the existence of a loss function $\\ell:\\mathcal{V}\\times\\mathcal{V}\\rightarrow\\mathbb{R}$ which measures the discrepancy between the model\u2019s prediction and the true labels point-wise. ", "page_idx": 2}, {"type": "text", "text": "The primary goal of ML testing is to identify meaningful failure modes\u2014subgroups (data slices) of the data distribution where the model\u2019s performance is significantly worse than its average behavior. Formally, let $s\\subseteq\\mathcal{X}\\times\\mathcal{Y}$ denote a data slice and let $\\mathcal{P}_{S}\\,=\\,\\mathcal{P}(\\cdot|(x,y)\\,\\in\\,\\mathcal{S})$ be the conditional distribution induced by $\\boldsymbol{S}$ . We aim to identify slices where the slice-specific expected loss $\\mu_{S}\\,=\\,\\mathbb{E}_{(x,y)\\sim\\mathcal{P}_{S}}[\\ell(f(x),y)]$ significantly exceeds the population-level expected loss $\\mu_{\\mathscr P}=$ $\\mathbb{E}_{(x,y)\\sim\\mathcal{P}}[\\ell(f(x),y)]^{3}$ . ", "page_idx": 2}, {"type": "text", "text": "Testing ML models is a multiple hypothesis testing problem. We are interested in identifying failure modes that generalize beyond the training dataset. We can interpret the empirical dataset $\\mathcal{D}$ as a sample from a broader distribution $\\mathcal{P}$ , and our goal is to make an inferential claim on the performance on the data slices with respect to $\\mathcal{P}$ . Suppose we have a candidate data slice $\\hat{S}\\subseteq\\mathcal{D}$ . We can evaluate the empirical slice-specific loss as $\\begin{array}{r}{\\hat{\\mu}_{S}=|\\hat{S}|^{-1}\\sum_{(x,y)\\in\\hat{\\mathcal{S}}}\\ell(f(x),y)}\\end{array}$ and compare it to the empirical loss over the entire dataset $\\begin{array}{r}{\\hat{\\mu}_{\\mathcal{D}}=|\\mathcal{D}|^{-1}\\sum_{(x,y)\\in\\mathcal{D}}\\ell(f(x),y)}\\end{array}$ . To make an inferential claim about the model\u2019s performance on the data slice $\\boldsymbol{s}$ w.r.t. $\\mathcal{P}$ , we can follow the frequentist testing paradigm and formulate a hypothesis test where we evaluate whether the performance is significantly different. Therefore, $H_{0}:\\,\\mu_{S}\\,=\\,\\mu_{D}$ and alternative hypothesis $H_{1}:\\,\\mu_{S}\\,\\neq\\,\\mu_{D}$ , where $\\mu_{S}\\stackrel{.}{=}\\mathbb{E}_{(x,y)\\sim\\mathcal{P}_{S}}[\\ell(f(x),y)]$ and $\\begin{array}{r}{\\dot{\\mu_{\\mathcal{D}_{.}}}=\\mathbb{E\\dot{E}}_{(x,y)\\sim\\mathcal{P}}[\\ell(f(x),y)]}\\end{array}$ denote the true slice-specific and population-level losses. In practice, this evaluation could be done by running an appropriate frequentist statistical test and evaluating whether $p<\\alpha$ for each slice, given some pre-defined $\\alpha$ . ", "page_idx": 2}, {"type": "text", "text": "However, in realistic testing scenarios, we evaluate the model\u2019s performance not just on a single slice but on a large collection of candidates $\\{S_{j}\\}_{j=1}^{m}$ . This amounts to conducting many simultaneous hypothesis tests. Accounting for multiple testing is important. A naive testing procedure that does not adjust for multiplicity could surface a large number of spurious failure modes simply by chance (Type I error). Conversely, controlling the false discovery rate [20] may involve adjusting the per-test significance threshold to $\\alpha^{\\prime}\\ll\\alpha$ , potentially sacrificing power to detect true failures (Type II error). ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "$\\heartsuit$ The multiple hypothesis testing viewpoint reveals a key challenge in ML model evaluation: To reliably surface meaningful failures, we require a principled procedure for generating a relatively small number of promising hypotheses (candidate slices) to test. ", "page_idx": 3}, {"type": "text", "text": "3.2 The failures of data-only testing ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Existing ML testing methodologies are data-only in that they only use the available empirical data to test for model failures and vary in their optimization objective [14, 12, 13, 21, 15]. However, data-only methods operate under the restrictive assumption that the available empirical data is the sole input for testing ML model performance. In practice, this is almost never the case. It is common to have an a priori understanding of where models are likely to fail given the data distribution, model class, training algorithm, and deployment context. This restrictive assumption results in three challenges: $\\blacktriangleright$ (i) High false positive rate: data-only methods search over a large space of data slices and each evaluation amounts to an implicit hypothesis test (Sec. 3.1). Therefore, the probability of observing a false failure increases with every test performed. \u25b6(ii) High false negative rate: The naive testing procedure can be made robust by correcting for the number of tests performed which reduces the statistical power to detect true failures. \u25b6(iii) Lack of meaningful failures: Data-only methods are fundamentally limited by the fact that not all statistically significant slices are practically meaningful. We empirically validate these claims in Sec. 5. ", "page_idx": 3}, {"type": "text", "text": "$\\heartsuit$ The core limitation of data-only testing is the lack of a principled failure mode discovery procedure that can incorporate prior knowledge to guide the search for meaningful errors. ", "page_idx": 3}, {"type": "text", "text": "3.3 Formulating context-aware testing ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To address the limitations of data-only testing, we introduce context-aware testing (CAT), a principled framework for identifying meaningful model failures using context. This could be the context implicitly encoded in the dataset (i.e. via meaningful feature names) or available external input (i.e. external contextual knowledge, such as a string of input information from a human). Let $\\mathcal{C}$ denote the space of all possible contextual information and $c\\in{\\mathcal{C}}$ be specific external input. Our core insight is that we can use $\\mathcal{C}$ as an inductive bias to select which slices to test for. ", "page_idx": 3}, {"type": "text", "text": "Definition 1 (Context-Aware Testing). Let $\\mathcal{X}$ , Y, P, f, $\\boldsymbol{\\mathcal{D}}=\\{(x_{i},y_{i})\\}_{i=1}^{N}$ , and $\\ell$ be defined as in the standard supervised learning setup. Let $\\mathcal{C}$ be a space of contexts. ", "page_idx": 3}, {"type": "text", "text": "Context-aware testing is defined by two procedures: ", "page_idx": 3}, {"type": "text", "text": "1. A context-guided slice sampling mechanism $\\pi:\\mathcal{C}\\times(\\mathcal{X}\\times\\mathcal{Y})\\times\\mathbb{N}\\to2^{\\mathcal{X}\\times\\mathcal{Y}}$ such that $\\pi(c,\\mathcal{D},m)=\\{\\mathcal{S}_{1},\\dots,\\mathcal{S}_{m}\\}$ , where $c$ is used as an inductive bias for function $\\pi$ to prioritize slices likely to contain meaningful failures, and $m\\in\\mathbb{N}$ are the number of slices to evaluate. ", "page_idx": 3}, {"type": "text", "text": "A meaningful failure is characterized by statistical significance and practical relevance. ", "page_idx": 3}, {"type": "text", "text": "The targeted sampling mechanism $\\pi$ uses the context $\\mathcal{C}$ as an inductive bias to prioritize testing of slices that are (i) more relevant to the deployment context, and (ii) more likely to exhibit significant performance gaps. ", "page_idx": 3}, {"type": "text", "text": "This principled slice selection offers several key advantages over data-only methods: $\\blacktriangleright$ (i) Improved false positive control: by limiting the number of tests conducted to $m$ , CAT controls the risk of spurious discoveries that arise when naively testing all possible slices. $\\blacktriangleright$ (ii) Improved true positive rate: The targeted selection of slices likely to contain failures maintains test power by avoiding the need to aggressively correct for multiple testing. $\\blacktriangleright$ (iii) Meaningful failures: context-guided sampling identifies failures that are both statistically significant and practically relevant. ", "page_idx": 3}, {"type": "text", "text": "$\\heartsuit$ Context-aware testing overcomes the limitations of data-only methods by employing a principled, context-guided slice sampling mechanism $\\pi$ to prioritize the discovery of meaningful model failures. ", "page_idx": 4}, {"type": "text", "text": "The core technical challenge in realizing CAT is the development of an effective context-conditional sampling algorithm $\\pi$ . In the following section, we propose a concrete instantiation of $\\pi$ using a large language model to generate plausible failure modes and guide testing of ML models. ", "page_idx": 4}, {"type": "text", "text": "4 SMART Testing ", "text_level": 1, "page_idx": 4}, {"type": "image", "img_path": "d75qCZb7TX/tmp/ae3f5a8779de4709509b4d2bf1c88946f4e9846e9e85cb82ee60187847b7eeab.jpg", "img_caption": ["Figure 1: Overview of the SMART Testing Framework showing the four steps. All steps are automatically executed by an LLM. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "We now instantiate the CAT framework outlined in Sec. 3.3 with a method called SMART testing (systematic, modular, automated, requirements-responsive, and transferable). SMART generates relevant and likely hypotheses about potential model failures and empirically evaluates these hypotheses on available data. SMART follows a four-step procedure: (1) Hypothesis generation; (2) Operationalization; (3) Self-falsification; (4) Reporting. Table 1 provides an early illustration of what it practically means to generate hypotheses and test them with SMART. The procedure below details how the four steps are implemented. ", "page_idx": 4}, {"type": "text", "text": "Table 1: Example hypotheses on model failure, justifications, and operationalizations generated by the SMART framework on a healthcare dataset. The p-values show whether the model\u2019s performance significantly differs from average performance with $|\\Delta A c c|$ measuring the effect size. ", "page_idx": 4}, {"type": "table", "img_path": "d75qCZb7TX/tmp/dde0c599a189f790df70371ad0303497d9427219e2fc37dfaa0790f64cc15435.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "Step 1: Hypothesis Generation. Recall from Sec. 3.3, we wish to define a sampling mechanism $\\pi$ to sample slices $\\boldsymbol{S}$ which are both relevant and have a high relative likelihood of failure \u2014 where $\\pi$ should be both contextually-aware and able to integrate requirements to guide sampling. ", "page_idx": 4}, {"type": "text", "text": "We posit that LLMs have the potential to satisfy these properties due to the following capabilities: \u25b6Contextual understanding: LLMs have been pretrained with a vast corpus of information and hence have extensive prior knowledge around different contexts and settings [22\u201325]. $\\blacktriangleright$ Integrate requirements: LLMs are adept at integrating requirements or additional information about the problem via natural language [26, 24]. $\\blacktriangleright$ Hypothesis proposers: In proposing likely failure modes, LLMs have also been shown to be \u201cphenomenonal hypothesis proposers\u201d [27]. ", "page_idx": 4}, {"type": "text", "text": "An LLM is defined as a probabilistic mapping $l:\\Sigma^{*}\\to$ $P(\\Sigma)$ , where $\\Sigma$ denotes the vocabulary space, and $P(\\Sigma)$ represents the probability distributions over $\\Sigma$ . The model processes input sequences $s\\in\\Sigma^{*}$ , each a concatenation of tokens representing external (contextual) input $\\mathcal{C}_{e}$ (which can be null) and dataset contextualization $\\mathcal{C}_{D}$ is formalized as $\\boldsymbol{s}=(\\mathcal{C}_{e},\\mathcal{C}_{D})$ . We extract the contextualized description $\\mathcal{C}_{D}$ from the dataset $\\mathcal{D}$ using an extractor function $\\mathcal{E}:\\mathcal{D}\\rightarrow\\mathcal{C}_{D}$ , which captures essential dataset characteristics (e.g. feature relationships and high-level dataset information). Additionally, we highlight the the LLM will implicitly extract context based on the context encoded in the dataset (e.g. via meaningful feature names). Based on input $s$ , the LLM predicts a distribution over $\\Sigma$ from which hypotheses of model failure and corresponding justifications are sampled. ", "page_idx": 4}, {"type": "image", "img_path": "d75qCZb7TX/tmp/aa932e4e3f0c5091b94ef7000010f6bf11d9cc4d891ce199445536a0ed2be2c2.jpg", "img_caption": ["Figure 2: SMART uses an LLM to integrate context $\\mathcal{C}$ , and data context $\\mathcal{D}_{c}$ . Relevant and likely failure hypotheses are then generated by LLM (i.e. sampling mechanism). The hypotheses are then operationalized in $\\mathcal{D}$ and evaluated. In contrast, data-only methods are not guided by context and requirements, searching more exhaustively in $\\mathcal{D}$ for divergent slices. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "As depicted in Fig. 2, given $\\mathcal{C}_{e}$ and $\\mathcal{C}_{D}$ , we sample the $N$ most likely hypotheses of failures, $\\mathcal{H}=\\{H_{1},H_{2},\\ldots,H_{N}\\}$ , and corresponding justifications $\\mathcal{I}=\\left\\{J_{1},J_{2},\\ldots,J_{N}\\right\\}$ , to provide explainability. This process is formalized using the LLM\u2019s mapping $l$ as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n(H_{i},J_{i})\\sim l(s),{\\mathrm{where~}}s=({\\mathcal{C}}_{e},{\\mathcal{C}}_{D}),\\;\\forall i\\in\\{1,2,\\ldots,N\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Step 2: Operationalization. The process of operationalizing each hypothesis $H_{i}\\in{\\mathcal{H}}$ involves translating its natural language expression into a form that can directly operate on the training dataset $\\mathcal{D}_{t r a i n}$ (an example is provided in Table 1). To achieve this, we define an interpreter function $I:\\mathcal{H}\\to\\{0,1\\}^{\\mathcal{X}}$ that maps each natural language hypothesis $H_{i}$ to a corresponding binary-valued function $\\dot{g}_{i}:\\dot{\\mathcal{X}}\\rightarrow\\{0,1\\}$ on the feature space $\\mathcal{X}$ , where $g_{i}(x)=1$ if $x$ satisfies the criteria and $g_{i}(x)=0$ otherwise. Each function $g_{i}$ induces a data slice $S_{i}\\subseteq\\mathcal{D}_{\\operatorname{train}}$ consisting of data points that satisfy the criteria of hypothesis $H_{i}$ , such that $S_{i}=\\{(x,y)\\in{\\mathcal{D}}_{\\mathrm{train}}:g_{i}(x)=1\\}$ . Therefore, each hypothesis, after operationalization, corresponds to a specific slice that is being tested on. Steps 1 and 2 serve to practically instantiate $\\pi$ from Sec. 3.3. ", "page_idx": 5}, {"type": "text", "text": "Step 3: Self-falsification We introduce a novel selffalsification mechanism to empirically evaluate (or refute) the generated hypotheses4. Specifically, for each feasible hypothesis $H_{i}\\in{\\mathcal{H}}$ , we attempt to falsify the hypothesis with observed empirical data 5. This involves evaluating the model $f$ over the data slice $\\mathcal{S}_{i}$ operationalized from $H_{i}$ . We then assess whether the slice performance on $f$ has a significant deviation from the model\u2019s overall performance. For instance, in Table 1, this is done by computing $|\\Delta\\mathbf{A}\\mathrm{cc}|$ and the $\\mathfrak{p}$ -value. The significance of this deviation is determined through frequentist statistical testing, i.e. when $p~<~\\alpha$ for any $\\alpha$ which might also be adjusted for multiple hypothesis testing. This step effectively \u201creshuffles/reranks\u201d the hypotheses based on their likelihood on the observed data. For example, when benchmarking we select the top $n$ hypotheses based on statistical significance: $\\mathcal{H}^{\\ast}=\\operatorname*{arg\\,min}_{h_{i}\\in\\mathcal{H}_{T}}\\,\\{p_{i}<\\alpha\\}.$ . ", "page_idx": 5}, {"type": "image", "img_path": "d75qCZb7TX/tmp/7fea925a7fd26986b31b5c3a3ff4ca0e21e8f9204157a26f315b808d8a99e480.jpg", "img_caption": ["Figure 3: A self-falsification module within the SMART framework. A hypothesis generator $l$ generates plausible hypotheses and justifications for when the model might fail. This is operationalized with $\\phi_{i}$ and tested against the empirical data. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Remark: As shown in Fig. 1, we can exclude the self-falsification from SMART in cases of small-sample sizes. We denote this ablation of SMART as $S M A R T_{N S F}$ . ", "page_idx": 5}, {"type": "text", "text": "Step 4. Model Performance Evaluation and Reporting. Finally, SMART automatically generates a report of the overall performance of the model under varying conditions generated by the LLM, including a summary report, a complete summary of the tests carried out, intermediate and final results, and potential failure modes of the ML model. ", "page_idx": 5}, {"type": "text", "text": "SMART is a tabular CAT method which (i) directly searches for model failures, sampling targeted tests (Sec. 3.3) and (ii) incorporates data, and context into ML testing. ", "page_idx": 5}, {"type": "text", "text": "Practical use of SMART. ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We highlight the practical use of SMART testing and emphasize SMART\u2019s ease of use and minimal input requirements needed. In particular, as shown in the example below, users do not need any prior knowledge to use SMART. Rather, we make use of the context inherently encoded in the dataset, feature names and task. ", "page_idx": 5}, {"type": "text", "text": "import SMART   \n2 # Instantiate SMART   \n3 model_tester $=$ SMART(\u2019gpt -4\u2019)   \n4 # Give desired context (this can be left as an empty string)   \n5 context $=$ \"Find where a model fails for the cancer prediction task.\"   \n6 # Load ML model   \n7 model $=$ XGBoost ()   \n8 dataset_description $=$ X.describe ()   \n9 # Test the model   \n10 model_tester.fit(X, context , dataset_description , model)   \nCode Listing 1: Use of SMART Testing ", "page_idx": 6}, {"type": "text", "text": "5 Illustrating SMART use cases ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now quantitatively evaluate SMART Testing 6 and demonstrate the viability of this new ML testing paradigm (i.e. CAT), in contrast to data-only testing. Table 2 summarizes our experiments and takeaways. ", "page_idx": 6}, {"type": "text", "text": "Baselines. We compare SMART with a variety of data-only testing baselines namely: Autostrat [14], Slicefinder [12], Sliceline [13], Pysubgroup [21] and Divexplorer [15]. We also evaluate $S M A R T_{N S F}$ (i.e. no self-falsification) as an ablation. Given space limits, we exemplify the LLM in SMART by GPT-4 [29] and our ML model to audit is logistic regression. We investigate other LLMs in the Appendix D.3 and other tabular models in Appendix D.4, observing similar results. We provide additional experimental details in Appendix C. ", "page_idx": 6}, {"type": "table", "img_path": "d75qCZb7TX/tmp/abb81c4fd8bda5e492df522727768eb8b97f9e501e2841607f2601832f568e75.jpg", "table_caption": ["Table 2: Summary of experiments and takeaways. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "5.1 Robustness to False Positives ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Goal. We aim to underscore the role of contextual awareness in preventing false positives in model testing. In particular, we consider the scenario when dealing with tabular data that may contain many irrelevant or uninformative features [30], persisting even post-feature selection [31, 32]. We contrast SMART which explicitly accounts for context, to data-only approaches which are context-unaware and can only operate on the numerical data. ", "page_idx": 6}, {"type": "text", "text": "Setup. We fit a predictive model to the training dataset, varying the number of irrelevant, synthetically generated features contained in the dataset. The irrelevant features are drawn from different distributions. We then quantify the proportion of conditions in the identified slices that falsely include the irrelevant synthetic features. We evaluate using five real-world tabular datasets spanning diverse domains, namely finance, healthcare, criminal justice and education: loan, breast cancer, diabetes, COMPAS recidivism [33] and OULAD eduction [34]. These datasets have varying characteristics, from sample size to number of features and are representative of different contexts pertinent to tabular ML, to demonstrate the effectiveness of SMART across various real-world contexts ", "page_idx": 6}, {"type": "text", "text": "Analysis. Fig. 4 shows the proportions of irrelevant features included in slices for increasing numbers of irrelevant features. Data-only methods are unaware of context and are shown to spuriously include high proportions of irrelevant features in their slices; i.e. false positives $(F P s)$ . Additionally, these false discoveries increase for data-only methods as the number of irrelevant features increases. ", "page_idx": 6}, {"type": "image", "img_path": "d75qCZb7TX/tmp/1a08302d33e519b3316c2727cdd39b850bf1dd62388461ba94ce87ea3a968692.jpg", "img_caption": ["Figure 4: Contextual-awareness in SMART reduces FPs, i.e. reducing the proportion of irrelevant features in slices. SMART is not sensitive to the number of irrelevant features, unlike data-only methods. \u2193is better "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Consequently, the sensitivity to FPs of data-only methods risks that slices identified are neither empirically relevant nor meaningful. In contrast, SMART, by virtue of contexual-awareness when generating hypotheses is not sensitive to extraneous and contextually irrelevant features. Importantly, SMART also maintains its robustness to FPs even with an increasing number of irrelevant features. ", "page_idx": 7}, {"type": "text", "text": "Remark. We also assess sensitivity to the number of data samples. Fig. 9, Appendix D.6 shows that SMART remains robust to variations irrespective of sample size. In contrast, data-only methods have variable performance with false discoveries sensitive to the sample size. ", "page_idx": 7}, {"type": "text", "text": "Avoiding non-existent failure slices. It is important that model testing does not flag failures when there are none. In Appendix D.5, we demonstrate that SMART\u2019s contextual awareness means that it is resistant to spurious failure slices that have no underlying relationships. In contrast, data-only approaches implicitly assume the existence of problematic slices, which we empirically show makes them prone to spuriously flagging non-existent failure slices. ", "page_idx": 7}, {"type": "text", "text": "Takeaway 1. SMART\u2019s contextual awareness ensures testing relevance, thereby reducing false positives across different scenarios, in contrast to data-only methods. ", "page_idx": 7}, {"type": "text", "text": "5.2 Targeting model failures ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Goal. We assess whether the identified failure slices persist when evaluated on new, unseen data across different tabular models. This evaluates the generalization of the identified model failures across multiple different ML models. ", "page_idx": 7}, {"type": "text", "text": "Setup. We use the prostate cancer dataset [35] and aim to discover slices indicative of underperformance. Thereafter, we assess generalizability of the identified failures across four different ML models (logistic regression, SVM, XGBoost and MLP). Specifically, we compare the top identified slice from each method and compute the absolute difference between the accuracies of that slice and the remainder of the dataset. ", "page_idx": 7}, {"type": "table", "img_path": "d75qCZb7TX/tmp/449d8c5c2bd8ec640bd2990954128e727390eeb3d6a8f93cec9875a94acdb202.jpg", "table_caption": ["Table 3: Identifying slices with the highest performance discrepancies. We show differences in accuracies $(|\\Delta A c c|)$ between the top identified divergent slice and average performance across four classifiers (over 5 runs). $\\uparrow$ is better. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "We posit that a testing framework should be able to identify slices with high-performance discrepancies on unseen data across multiple ML models. ", "page_idx": 7}, {"type": "text", "text": "Analysis. Table 3 shows that SMART slices exhibit the greatest discrepancies in model accuracies on unseen test data across different ML models \u2014 indicating the discovered failure modes are generalizable. We find that SMART surfaces slices with greater performance differences compared to $S M A R T_{N S F}$ (ablation without self-falsification), highlighting the importance of the introduced selffalsification mechanism. In contrast, data-only methods fail to identify slices where the performance discrepancy is as large as SMART. This limitation can be attributed to the tendency of data-only methods to overfit the training data, leading to high false discovery rates. ", "page_idx": 7}, {"type": "text", "text": "Takeaway 2. SMART discovers failure slices where the model substantially underperforms, generalizing to unseen test data across different ML models. In contrast, data-only approaches fail to find slices where the difference in accuracies is as large, highlighting the lack of generalizability and reliability of their findings. ", "page_idx": 7}, {"type": "text", "text": "5.3 Robustness to False Negatives ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In our setup, false negatives (FNs) are directly tied to true positives (TPs). i.e. the more true positives we find, the fewer FNs we miss. Across multiple experiments (Fig. 4, Table 10, Table 3, Table 16), we consistently show that SMART identifies TPs at substantially higher rates than data-only. For example, in Table 16, SMART identifies an average of 9.6 out of 10 subgroups where the ML model significantly underperforms. The fact that data-driven methods discover fewer such subgroups implies that they are missing the ones SMART uncovers. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Goal. That said, we conduct an additional experiment to directly assess the false negative rate, wherein we can control issues (as FNs are naturally unspecified in real data). ", "page_idx": 8}, {"type": "text", "text": "Setup. We simulate a dataset to predict recidivism $(Y)$ based on five covariates: gender, race, age, income, and education. ( P (PY (iY|iX\u0338=ij=)j )) = \u03b1j \u2212(\u03b41Xage + \u03b42Cincome + \u03b43Ceducation) + \u03f5, where \u03f5 \u223c $\\mathcal{N}(\\mu_{0},\\sigma_{0})$ . We then train a predictor function $\\hat{f}$ on the data and synthetically introduce underperformance on certain corrupted subgroups. For an individual $i$ , if they belong to corrupted subgroup $j$ , the prediction ${\\hat{Y}}_{i}$ is equal to $\\hat{f}(X_{i})$ with probability $1-p$ , and a random prediction sampled from a Bernoulli distribution with probability $p$ . If the individual does not belong to subgroup $j$ , the prediction is simply $\\hat{f}(X_{i})$ . Finally, we measure how often each testing method identifies that the model $\\hat{f}$ underperforms on a subgroup. ", "page_idx": 8}, {"type": "text", "text": "Analysis. Table 4 demonstrates results where we synthetically manipulate/corrupt the performance of an ML model on a single subgroup ${\\scriptstyle(\\mathrm{n}=1)}$ ), two subgroups $\\scriptstyle({\\mathrm{n}}=2)$ ), and three subgroups $(\\mathfrak{n}{=}3)$ out of a total of five. The results show the average of 20 runs, where the corrupted groups are randomly selected within each run. We find ", "page_idx": 8}, {"type": "table", "img_path": "d75qCZb7TX/tmp/a169d0431be7117ba5f90c4748df2c9e8751b709d83cb23aa1c5f476476b3569.jpg", "table_caption": ["Table 4: False Negative Rate (FNR) for different methods at various settings. $\\downarrow$ is better. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "that SMART consistently is least susceptible to false negatives across all corrupted variables, when compared to data-only methods which struggle especially once more than one variable is corrupted. This serves to corroborate our earlier results. ", "page_idx": 8}, {"type": "text", "text": "Takeaway 3. SMART is less prone to FNs, compared to data-only methods across all settings. ", "page_idx": 8}, {"type": "text", "text": "5.4 Assessing and mitigating potential LLM challenges and biases. ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Background. In many settings, we want SMART to be part of a human-in-the-loop model evaluation, particularly to address challenges with LLMs, such as biases or missing dimensions. Let us first discuss how SMART addresses some issues by design and then perform an experimental assessment. ", "page_idx": 8}, {"type": "text", "text": "$\\blacktriangleright$ Using data to mitigate LLM challenges: We use data in two ways (i) Data usage in the generation of hypotheses: Before generating explicit hypotheses of where the model is likely to fail, we provide the LLM with additional information about the data description and model failures of the training dataset. The hypotheses sampled are therefore reflective of the inductive bias of the LLM as well as being conditioned on the data itself; (ii) Data usage in falsifying hypotheses: core to SMART is the self-falsification mechanism where we iteratively generate hypotheses and test them on a validation dataset. Data is therefore used to filter out hypotheses which are not supported by the data. Hence, even if \u201dincorrect\u201d hypotheses about group failures are proposed, this step ensures they are discarded. ", "page_idx": 8}, {"type": "text", "text": "\u25b6SMART provides clear and transparent testing: This is done in two ways: (i) SMART\u2019s model reports: that document specific failure cases with natural language justifications (see Appendix D.10 for examples). Automatically generated reports can be a useful tool for humans-in-the-loop experts to audit and validate the testing process, such as evaluating whether tests should be added or removed. For example, a domain expert (e.g. a clinician) could review the report to assess whether the identified failure modes are truly relevant and concerning in that specific context. (ii) Tests include justifications: The justifications for the tests allow human users to inspect the model\u2019s testing procedures, understand the reasons, and audit for biases or missed dimensions. ", "page_idx": 8}, {"type": "text", "text": "Goal. Going beyond the mitigation strategies by design, we also assess SMART\u2019s robustness to prior biases. Specifically, we assess common ethnicity related biases of LLMs. ", "page_idx": 8}, {"type": "text", "text": "Setup. We use the same data generating process as Sec. 5.3. ", "page_idx": 8}, {"type": "text", "text": "We train a predictor function $\\hat{f}$ and simulate a scenario where we intentionally corrupt a model\u2019s predictions for a proportion $\\tau$ of a minority subgroup (\"white\" or \"black\" ethnicity). ", "page_idx": 8}, {"type": "table", "img_path": "d75qCZb7TX/tmp/8935b79ca4121b1a4a04cb1589391ba8af39268f924a50ac3518c1b4b29a5912.jpg", "table_caption": ["Table 5: Proportion of times the corrupted minority subgroup is correctly identified as the top underperforming subgroup. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "$\\tau$ ranges from 0 (no corruption) to 1 (completely random predictions for the subgroup). SMART aims to identify the top underperforming subgroup without bias based on historical patterns. We measure the proportion of times the top subgroup contains the \"white\" $(P_{\\mathrm{white}})$ or \"black\" $(P_{\\mathrm{black}})$ minority, averaged over 20 runs and 5 seeds, separately corrupting \"white\" and \"black\" ethnicities. ", "page_idx": 9}, {"type": "text", "text": "Analysis. We show in Table 5 that SMART is able to identify where models underperform even in scenarios such as ethnicity bias, where LLMs exhibit prior biases from the training dataset. This links to the above discussion that SMART mitigates such biases by design both using the training dataset to guide hypothesis generation and using the self-falsification mechanism to empirically evaluate hypotheses (and discard those that aren\u2019t reflective of the data). ", "page_idx": 9}, {"type": "text", "text": "Takeaway 4. SMART mitigates potential biases in the LLM, both by using the real data to guide hypothesis generation, as well as using the self-falsification mechanism to fliter spurious hypotheses. ", "page_idx": 9}, {"type": "text", "text": "6 Discussion and limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Responding to recent calls for better model testing [36, 6], we formalize Context-Aware Testing; a new testing paradigm, actively seeking out relevant and likely model failures based on contextual awareness \u2014 going beyond data alone. We develop SMART Testing, using LLMs to hypothesize likely and relevant model failures providing improved and automated testing of tabular ML models, compared to data-only methods in various scenarios. ", "page_idx": 9}, {"type": "text", "text": "Model reports. SMART produces comprehensive and automated model reports documenting failure cases and justifications, thereby providing data scientists, ML engineers and stakeholders increased visibility into model failures. We provide an example SMART report in Appendix D.10. ", "page_idx": 9}, {"type": "text", "text": "Practical considerations. Given the potential utility of SMART we highlight the following five practical considerations: \u25b6Hypothesis generation. While CAT offers a principled framework for context-guided testing, LLMs present challenges in hypothesis generation. Although SMART has mechanisms to address these (see Sec. 5.4), it cannot guarantee the absence of biases. \u25b6Use with small datasets. SMART may be limited in some cases by insufficient real data to operationalize and test hypotheses, suggesting future work could explore targeted data collection or synthetic data generation to enhance testing. \u25b6Extensions to other modalities. SMART is formalized to test tabular ML models, due to the interpretable and structured features in tabular data to guide hypothesis generation. While extensions to other modalities such as image and text is beyond the scope of this work, this is a promising future research direction that would require addressing the lack of explicitly interpretable features possibly via external metadata and developing new ways to operationalize hypotheses on unstructured data. That said, tabular data is ubiquitous in real-world applications [37, 38] with approximately $79\\%$ of data scientists working on tabular problems daily, vastly surpassing other modalities [39, 40]. This highlights the immediate impact and relevance of SMART. \u25b6Need for interpretable/meaningful feature names. Feature names play an important role in finding model failures (see Appendix Appendix D.9). The need for interpretable/meaningful feature names (e.g. column labels such as sex, age, race etc) as a source of context is similar to human requirements of interpretable feature names to understand what the data refers to. While feature names are typically the de facto in research and industry datasets, in the rare occasions they are not, this will affect the performance of SMART. $\\blacktriangleright$ Cost of SMART. SMART is extremely accessible and cheap to use, approximately $<\\!0.10$ USD for 5 hypotheses and $_{<0.5}$ USD for 100 hypotheses for state-of-the-art models (see Appendix D.7). ", "page_idx": 9}, {"type": "text", "text": "Broader impact. Better testing practices can help ensure models are reliable, safe, and beneficial before being deployed in real-world applications. We hope that our work can help mitigate model testing risks in real-world applications as well as spur new testing regimes which are context aware. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We would like to thank the reviewers, Fergus Imrie, Nicolas Astorga, Kasia Kobalczyk, Tennison Liu and Andrew Rashbass for their helpful feedback. PR is supported by GSK, ML is supported by AstraZeneca, NS by the Cystic Fibrosis Trust. This work was supported by Microsoft\u2019s Accelerate Foundation Models Academic Research initiative. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Luke Oakden-Rayner, Jared Dunnmon, Gustavo Carneiro, and Christopher R\u00e9. Hidden stratification causes clinically meaningful failures in machine learning for medical imaging. In Proceedings of the ACM Conference on Health, Inference, and Learning, pages 151\u2013159, 2020.   \n[2] Harini Suresh, Jen J Gong, and John V Guttag. Learning tasks for multitask learning: Heterogenous patient populations in the ICU. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 802\u2013810, 2018.   \n[3] Karan Goel, Albert Gu, Yixuan Li, and Christopher Re. Model patching: Closing the subgroup performance gap with data augmentation. In International Conference on Learning Representations, 2020.   \n[4] Angel Alexander Cabrera, Minsuk Kahng, Fred Hohman, Jamie Morgenstern, and Duen Horng Chau. Discovery of intersectional bias in machine learning using automatic subgroup generation. In ICLR Debugging Machine Learning Models Workshop, 2019.   \n[5] Boris van Breugel, Nabeel Seedat, Fergus Imrie, and Mihaela van der Schaar. Can you rely on your model evaluation? improving model evaluation with synthetic test data. Advances in Neural Information Processing Systems, 36, 2024.   \n[6] Nabeel Seedat, Fergus Imrie, and Mihaela van der Schaar. Navigating data-centric artificial intelligence with DC-Check: Advances, challenges, and opportunities. IEEE Transactions on Artificial Intelligence, 2023.   \n[7] Oleg S Pianykh, Georg Langs, Marc Dewey, Dieter R Enzmann, Christian J Herold, Stefan O Schoenberg, and James A Brink. Continuous learning AI in radiology: Implementation principles and early applications. Radiology, 297(1):6\u201314, 2020.   \n[8] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne David, Ian Stavness, Wei Guo, Berton A. Earnshaw, Imran S. Haque, Sara Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. WILDS: A benchmark of in-the-wild distribution shifts. In International Conference on Machine Learning, pages 5637\u20135664. PMLR, 2021.   \n[9] Kayur Patel, James Fogarty, James A Landay, and Beverly Harrison. Investigating statistical machine learning as a tool for software development. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 667\u2013676, 2008.   \n[10] Lea Goetz, Nabeel Seedat, Robert Vandersluis, and Mihaela van der Schaar. Generalization\u2014a key challenge for responsible ai in patient-facing clinical applications. npj Digital Medicine, 7 (1):126, 2024.   \n[11] M\u00e1ire A Duggan, William F Anderson, Sean Altekruse, Lynne Penberthy, and Mark E Sherman. The surveillance, epidemiology and end results (SEER) program and pathology: towards strengthening the critical relationship. The American Journal of Surgical Pathology, 40(12):e94, 2016.   \n[12] Yeounoh Chung, Tim Kraska, Neoklis Polyzotis, Ki Hyun Tae, and Steven Euijong Whang. Slice finder: Automated data slicing for model validation. In 2019 IEEE 35th International Conference on Data Engineering (ICDE), pages 1550\u20131553. IEEE, 2019.   \n[13] Svetlana Sagadeeva and Matthias Boehm. Sliceline: Fast, linear-algebra-based slice finding for ml model debugging. In Proceedings of the 2021 International Conference on Management of Data, pages 2290\u20132299, 2021.   \n[14] Adebayo Oshingbesan, Winslow Georgos Omondi, Girmaw Abebe Tadesse, Celia Cintas, and Skyler Speakman. Beyond protected attributes: Disciplined detection of systematic deviations in data. In Workshop on Trustworthy and Socially Responsible Machine Learning, NeurIPS 2022, 2022.   \n[15] Eliana Pastor, Luca De Alfaro, and Elena Baralis. Looking for trouble: Analyzing classifier behavior via pattern divergence. In Proceedings of the 2021 International Conference on Management of Data, pages 1400\u20131412, 2021.   \n[16] Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. Beyond accuracy: Behavioral testing of nlp models with checklist. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4902\u20134912, 2020.   \n[17] Paul R\u00f6ttger, Bertie Vidgen, Dong Nguyen, Zeerak Waseem, Helen Margetts, Janet Pierrehumbert, et al. Hatecheck: Functional tests for hate speech detection models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), page 41. Association for Computational Linguistics, 2021.   \n[18] Maria Christakis, Hasan Ferit Eniser, J\u00f6rg Hoffmann, Adish Singla, and Valentin W\u00fcstholz. Specifying and testing k-safety properties for machine-learning models. In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, pages 4748\u20134757, 2023.   \n[19] Arnab Sharma and Heike Wehrheim. Higher income, larger loan? monotonicity testing of machine learning models. In Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis, pages 200\u2013210, 2020.   \n[20] Yoav Benjamini and Yosef Hochberg. Controlling the false discovery rate: a practical and powerful approach to multiple testing. Journal of the Royal statistical society: series $B$ (Methodological), 57(1):289\u2013300, 1995.   \n[21] Florian Lemmerich and Martin Becker. pysubgroup: Easy-to-use subgroup discovery in python. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 658\u2013662, 2018.   \n[22] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.   \n[23] Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language models encode clinical knowledge. Nature, pages 1\u20139, 2023.   \n[24] Nabeel Seedat, Nicolas Huynh, Boris van Breugel, and Mihaela van der Schaar. Curated LLM: Synergy of LLMs and data curation for tabular augmentation in low-data regimes. In Forty-first International Conference on Machine Learning, 2024.   \n[25] Nicolas Astorga, Tennison Liu, Nabeel Seedat, and Mihaela van der Schaar. Partially observable cost-aware active-learning with large language models. Advances in Neural Information Processing Systems, 38, 2024.   \n[26] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. arXiv preprint arXiv:2309.03409, 2023.   \n[27] Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, et al. Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement. arXiv preprint arXiv:2310.08559, 2023.   \n[28] Karl R Popper. Science as falsification. Conjectures and refutations, 1(1963):33\u201339, 1963.   \n[29] OpenAI. Gpt-4 technical report, 2023.   \n[30] Md Rezaul Karim, Md Shajalal, Alexander Gra\u00df, Till D\u00f6hmen, Sisay Adugna Chala, Alexander Boden, Christian Beecks, and Stefan Decker. Interpreting black-box machine learning models for high dimensional datasets. In 2023 IEEE 10th International Conference on Data Science and Advanced Analytics (DSAA), pages 1\u201310. IEEE, 2023.   \n[31] Danilo Vasconcellos Vargas, Hirotaka Takano, and Junichi Murata. Contingency training. In The SICE Annual Conference 2013, pages 1361\u20131366. IEEE, 2013.   \n[32] L\u00e9o Grinsztajn, Edouard Oyallon, and Ga\u00ebl Varoquaux. Why do tree-based models still outperform deep learning on typical tabular data? Advances in Neural Information Processing Systems, 35:507\u2013520, 2022.   \n[33] Julia Angwin, Jeff Larson, Lauren Kirchner, and Surya Mattu. Machine bias. ProPublica: https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing, May 2016.   \n[34] Jakub Kuzilek, Martin Hlosta, and Zdenek Zdrahal. Open university learning analytics dataset. Scientific data, 4(1):1\u20138, 2017.   \n[35] Prostate Cancer UK PCUK. Cutract. https://prostatecanceruk.org, 2019.   \n[36] Negar Rostamzadeh, Ben Hutchinson, Christina Greer, and Vinodkumar Prabhakaran. Thinking beyond distributions in testing machine learned models. In NeurIPS 2021 Workshop on Distribution Shifts: Connecting Methods and Applications, 2021.   \n[37] Vadim Borisov, Tobias Leemann, Kathrin Se\u00dfler, Johannes Haug, Martin Pawelczyk, and Gjergji Kasneci. Deep neural networks and tabular data: A survey. arXiv preprint arXiv:2110.01889, 2021.   \n[38] Ravid Shwartz-Ziv and Amitai Armon. Tabular data: Deep learning is not all you need. Information Fusion, 81:84\u201390, 2022.   \n[39] Kaggle. Kaggle machine learning and data science survey, 2017. URL https://www.kaggle. com/datasets/kaggle/kaggle-survey-2017.   \n[40] Lasse Hansen, Nabeel Seedat, Mihaela van der Schaar, and Andrija Petrovic. Reimagining synthetic tabular data generation through data-centric ai: A comprehensive benchmark. Advances in Neural Information Processing Systems, 37:33781\u201333823, 2023.   \n[41] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In International Conference on Learning Representations, 2018.   \n[42] Greg d\u2019Eon, Jason d\u2019Eon, James R Wright, and Kevin Leyton-Brown. The spotlight: A general method for discovering systematic errors in deep learning models. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, pages 1962\u20131981, 2022.   \n[43] Michael P Kim, Amirata Ghorbani, and James Zou. Multiaccuracy: Black-box post-processing for fairness in classification. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, pages 247\u2013254, 2019.   \n[44] Sahil Singla, Besmira Nushi, Shital Shah, Ece Kamar, and Eric Horvitz. Understanding failures of deep networks via robust feature extraction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12853\u201312862, 2021.   \n[45] Sabri Eyuboglu, Maya Varma, Khaled Saab, Jean-Benoit Delbrouck, Christopher Lee-Messer, Jared Dunnmon, James Zou, and Christopher Re. Domino: Discovering systematic errors with cross-modal embeddings. arXiv preprint arXiv:2203.14960, 2022.   \n[46] Shreya Shankar, Labib Fawaz, Karl Gyllstrom, and Aditya Parameswaran. Automatic and precise data validation for machine learning. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, pages 2198\u20132207, 2023.   \n[47] Neoklis Polyzotis, Martin Zinkevich, Sudip Roy, Eric Breck, and Steven Whang. Data validation for machine learning. Proceedings of machine learning and systems, 1:334\u2013347, 2019.   \n[48] Gary S Collins, Johannes B Reitsma, Douglas G Altman, and Karel GM Moons. Transparent reporting of a multivariable prediction model for individual prognosis or diagnosis (tripod) the tripod statement. Circulation, 131(2):211\u2013219, 2015. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix: Context-aware testing: a new paradigm for testing with large language models ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Extended related work 15   \nA.1 Enhanced overview of relevant literature 15   \nA.2 Components of the ML Testing pipeline 17   \nA.3 Comparison of the features of slice discovery methods 17 ", "page_idx": 13}, {"type": "text", "text": "B SMART Details 18 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 Contextual understanding 18   \nB.2 Operationalizing variables 18   \nB.3 Feasibility checks 20   \nB.4 Data adjustment queries 21   \nB.5 Requirements, automation, and outputs 21   \nB.6 Moving outside of IID data 22   \nB.7 SMART and multiple testing 22 ", "page_idx": 13}, {"type": "text", "text": "C Benchmarks & Experimental Details 23 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "C.1 Datasets 23   \nC.2 Baselines . 23   \nC.3 LLM prompts 23   \nC.3.1 Generic templates. . 23   \nC.3.2 Example prompts: OULAD Education. 24   \nC.4 Experimental details . 29   \nC.4.1 Context-aware testing (Sec. 5.1.) 29   \nC.4.2 Requirements-constrained testing (Sec. 5.2.) 29   \nC.4.3 Targeting model failures (Sec. 5.3.) 30   \nC.4.4 Adaptive testing for a deployment environment (Sec. 5.4.) 30   \nC.4.5 Discovery of societally important groups and failure understanding (Sec.   \n5.5.) 30   \nAdditional experiments 31   \nD.1 Requirements-constrained testing 31   \nD.2 Adaptive testing for a deployment environment 32   \nD.3 Effects of LLMs . 33   \nD.3.1 Comparison over identified divergent slices 33   \nD.3.2 Performance across different models 33   \nD.3.3 Hypothesis generation 34   \nD.4 Effects of different tabular machine learning models 35   \nD.5 On the inductive biases of ML model testing 36   \nD.6 Context aware sensitivity 37   \nD.7 Cost of SMART . . 38   \nD.8 SMART with open-weight models 38   \nD.9 Understanding the importance of feature names 38   \nD.10 Example of a model report 40 ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Extended related work ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Enhanced overview of relevant literature ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "This work primarily engages with works on ML model testing. Consequently, we detail different paradigms of ML model testing next, which we summarize in Table 6 and Figure 5 showing that none of the existing paradigms satisfy all the properties for ML testing neither in terms of automation nor from the perspective of relevance as they are unable to incorporate context and/or requirements. ", "page_idx": 14}, {"type": "table", "img_path": "d75qCZb7TX/tmp/9a84012dccf5e29aa1c396d33400991075bf7f7a27147ae359dfa44e7d50a0a9.jpg", "table_caption": ["Table 6: Comparison of ML testing paradigms in terms of how tests are defined, the type of search space (i.e. relevance based on context and requirements), sensitivity to multiple testing and automation to enable scalability. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Existing paradigms in ML model testing. The ML community has predominatly approached ML model evaluation/testing via the use of held-out test datasets. On the basis of the test dataset a single performance metric (accuracy, AUC etc) is computed. This single average evaluation may mask nuances of the model\u2019s performance along various dimensions. One approach to address this is to create better benchmark datasets when evaluating models on common benchmarking tasks. For example, manual corruptions like Imagenet-C [41] or by collecting additional real data such as the Wilds benchmark [8]. Benchmark datasets are labor-intensive to collect and their utility is limited to the specific benchmark tasks. ", "page_idx": 14}, {"type": "text", "text": "What if we want to evaluate models not confined to benchmarking tasks? To test in cases beyond benchmark tasks, the community has proposed trying to find slices or regions wherein the model fails (i.e. via stress tests). As mentioned in Sec. 1, this could be \u25b6Behavioral testing: which requires human expertise and intuition to define the test scenarios (e.g. Checklist [16], HateCheck [17] or 3S-Testing [5]) \u2014 which is not automated and does not scale. Moreover, it runs a high risk of overlooking critical weaknesses due to human cognitive biases. \u25b6Data-only testing: which does not account for context or requirements and searches exhaustively (e.g. Autostrat [14], SliceFinder [12] or DivExplorer [15]). This may slices focus on arbitrary, less important, or unrealistic/implausible scenarios that are unlikely to be seen in reality. Moreover, we run the risk of the multiple testing problem, where by virtue of the large number of tests evaluated, we might discover a divergent group by chance. ", "page_idx": 14}, {"type": "text", "text": "Hypothesis-driven ML model testing. In contrast, a hypothesis-driven approach to ML model testing brings about the falsifiability approach widely adopted in science. It begins with the formulation of specific, testable hypotheses based on theoretical understanding, context/domain knowledge, and the intended application of the model (i.e. requirements). The concept of hypothesis-driven ML model testing is deeply rooted in work by Popper [28] who proposed the principle of falsifiability as a driver of scientific progress. The progression of knowledge hinges on the formulation and rigorous testing of hypotheses which can either be falsified or supported by empirical evidence. In the context of hypotheses in ML model testing, testable statements about the model\u2019s performance under various conditions, fairness, and robustness can improve our understanding of the model\u2019s performance. By rigorously testing these hypotheses, we can uncover the strengths and limitations of ML models. ", "page_idx": 14}, {"type": "text", "text": "Contrast to data-only methods on unstructured data. ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this paper, we have discussed data-only methods for slice discovery or blindspot discovery applicable to structured data. Specifically, we focus on tabular data where metadata in the form of column names is explicitly encoded into the data. The data-only approaches covered in this work directly search over the raw feature space to identify slices with similar attributes wherein the model would exhibit underperforming predictions. ", "page_idx": 14}, {"type": "text", "text": "For completeness, we contrast to data-only approaches often applicable to data without explicit structure and metadata (images, text, audio). There have been numerous methods including [42\u201345] ", "page_idx": 14}, {"type": "image", "img_path": "d75qCZb7TX/tmp/0ebdd58c617682034c7be49d83e1f2d02ef47137b492ea37e0be2fe90d21a2ca.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 5: Contrasting paradigms of ML model testing along (i) Testing relevance which accounts for requirements and/or context and (ii) Degree of automation and adaptability when carrying out testing. We desire a new paradigm of ML testing to address both. ", "page_idx": 15}, {"type": "text", "text": "to identify slices of unstructured data with systematic failures. They all follow a similar pattern: (1) the data is embedded (often with a pre-trained) model into a representation space and (2) the underperforming slices are identified by clustering on the raw embedding space or after dimensionality reduction. We then need to post-hoc interpret the clusters in order to understand what they represent. This contrasts the tabular data setting where we find cohesive groups of features that provide an explicit interpretation. Moreover, in tabular regimes, we do not have access to pre-trained models in the same way as for domains such as images or text. ", "page_idx": 15}, {"type": "text", "text": "Specifically, to context-aware testing we also note that tabular data inherently includes context through interpretable/meaningful feature names and metadata. This context is not naturally present in images (which are tensors of pixel intensities). Hence, extending CAT to other domains like images or text would require incorporating metadata to provide necessary context \u2014 which is often unavailable. In addition, one would need to develop new ways to operationalize hypotheses on unstructured data. ", "page_idx": 15}, {"type": "text", "text": "Contrast to software testing. ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Going beyond ML model testing, the idea of testing is also prevalent in software systems. For example, unit tests of functions in a codebase. We highlight that software testing of functional input-output correctness is also an area proposed in software testing, which could also theoretically be applied to ML systems. That said, we contrast between SMART and these paradigms in Table 7 below, demonstrating that we tackle a different testing problem. ", "page_idx": 15}, {"type": "table", "img_path": "d75qCZb7TX/tmp/fa8c37e89f2d835912d7097bcaa85839cb3fc9d5c5af86f66ae7519f54cfad9a.jpg", "table_caption": ["Table 7: Comparison of SMART testing with software testing works "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Additionally, a further orthogonal area to ML model testing is that of data validation [46, 47]. In contrast, to testing an ML model for failures, data validation aims to test input data pipelines for data quality problems or drift. ", "page_idx": 15}, {"type": "image", "img_path": "d75qCZb7TX/tmp/77a65504c4760cb1ec036d45d4918ce3368cfe73af40c850ecbe81e7a1165995.jpg", "img_caption": ["Figure 6: Components of ML Testing \u2014 (i) Test search, (ii) Operationalize, (iii) Evaluation report, with example approaches for each component. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "\u2022 Test search: First, we need to decide \u201cwhat to test\u201d. This is a search problem to identify test dimensions. This could either be done via SMART which is targeted (via context and requirements) or data-only methods (where the space is larger). Alternatively, if human experts are available, humans could define the test dimensions.   \n\u2022 Operationalize: Second, we need to operationalize the test and address the challenge of \u201chow to carry out this test on data\u201d. If we have sufficient data \u2014 SMART could operationalize the tests on the data via an interpreter. Alternatively, if we don\u2019t have sufficient data to run the test \u2014 then SMART could be augmented by behavioral testing approaches such as van Breugel et al. [5] \u2014 which once the test has been defined use synthetic data to augment small subgroups/slices.   \n\u2022 Evaluation report: Third, we carry out the test and evaluate the model to answer the question, \u201chow does the model perform\u2019. SMART can be used to produce a comprehensive report of failures and justifications in an automated manner. ", "page_idx": 16}, {"type": "text", "text": "A.3 Comparison of the features of slice discovery methods ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Even as we zoom into the task of discovering slices where the model might underperform, we observe that SMART has features which are not supported by most of other discovery methods. We exemplify some of these features in Table 8 ", "page_idx": 16}, {"type": "table", "img_path": "d75qCZb7TX/tmp/f3fd594309ec10acb990e6e368ad77c18435146019b5eea8e7e377023706f395.jpg", "table_caption": ["Table 8: Comparison of slice discovery methods "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "B SMART Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We present a block diagram of the key components of SMART Testing in Figure 7. In addition, for each component we provide additional motivations and technical details not covered in the main paper. We further provide more technical information on certain implementation details. ", "page_idx": 17}, {"type": "image", "img_path": "d75qCZb7TX/tmp/12ea6b7a65d8bda3798e2496d6381dfa5c73700c5284c5f4b9363f037997cce7.jpg", "img_caption": ["Figure 7: A strong machine learning testing framework should incorporate textual understanding during testing, should meet requirements, should be automated, and provide contextual reports with an emphasis on model failures. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "B.1 Contextual understanding ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "A core component of SMART is leveraging LLMs to strategically identify what to test for in tabular ML models. This process is anchored on the premise that LLMs can effectively navigate the space of potential model failure slices, harnessing their contextual understanding to pinpoint slices where the model is most susceptible to failure. The operation of LLMs within the SMART framework involves three critical inputs: ", "page_idx": 17}, {"type": "text", "text": ". Context $(\\mathcal{C})$ : A string describing the overarching scenario and the task at hand. ", "page_idx": 17}, {"type": "text", "text": "2. Dataset Information $(\\mathcal{D}_{c})$ : Extracted from the training data, this includes a description of the observations where the model did not fail $(\\hat{y}=y)$ , or where it failed ${\\hat{y}}\\neq y.$ ). This could be characterized as a string with a description of the covariates of each group (e.g., mean, median, mode, and a textual description of the distribution). ", "page_idx": 17}, {"type": "text", "text": "Utilizing these inputs, the LLM generates a set of hypotheses $(\\mathcal{H})$ and corresponding justifications $(J)$ regarding potential model failures. The framework also incorporates a self-refine mechanism to enhance hypothesis generation. This mechanism iteratively refines hypotheses based on the observed or provided context $(\\mathcal{C})$ and data $(\\mathcal{D}_{c})$ , re-ranking them by their likelihood. The self-refine mechanism is introduced in order to generate hypotheses that are more likely to target specific model failures. ", "page_idx": 17}, {"type": "text", "text": "B.2 Operationalizing variables ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Once the hypotheses have been proposed, it is important to operationalize them. This operationalization can be achieved through one of two methods: ", "page_idx": 17}, {"type": "text", "text": ". LLM-based Operationalization: Hypothesizing possible operationalizations (e.g., $a g e>$ 70 as a way to operationalize \u201celderly people\u201d). In this case, the previous interactions and information is provided to the LLM, together with the relevant description of the data. It is then asked to provide possible ways to operationalize a specific hypothesis. This is done by passing an \u201coperationalization prompt\u201d which contains the aforementioned information. ", "page_idx": 17}, {"type": "text", "text": "The following is an example of an operationalization prompt in Python: ", "page_idx": 18}, {"type": "image", "img_path": "d75qCZb7TX/tmp/ed804d29896e84d7a30debdeb06510f777751768111a207817e6a26fb134522d.jpg", "img_caption": ["Code Listing 2: Operationalization (LLM knowledge): General template "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "This is then converted into a specific operationalization for each hypothesis using an external compiler which maps the strings to a function. ", "page_idx": 18}, {"type": "text", "text": "2. Data-driven Operationalization: Utilizing training data to identify optimal splits for given covariates (e.g., age $>=82.32$ as a condition to split data based on the \u201cage\u201d covariate). This method receives the covariate (or set of covariates) as an input and returns the optimal split. The optimal split is defined as the split which can identify two groups which have the largest absolute difference in accuracies given some requirements (e.g. a minimum or maximum group size in each group). This splitting can be done by any black box splitting function which takes in a set of inputs and returns a splitting mechanism. ", "page_idx": 18}, {"type": "text", "text": "Black-box splitting function for data-driven operationalization. While any black-box splitting function could perform on the data, we provide details on the specific function used in SMART. ", "page_idx": 18}, {"type": "text", "text": "The function implemented is based on a decision tree model, which can be either a regressor or a classifier, depending on the nature of the outcome variable. In all our experiments, we perform classification-based tasks. ", "page_idx": 18}, {"type": "text", "text": "The function operates by fitting a decision tree to the data and recursively traversing the tree to find the split that yields the largest absolute difference in the outcome variable\u2019s mean value between two slices. The split must also satisfy group size constraints, specified as minimum and maximum group sizes. The traversal process evaluates each potential split, calculating the mean outcome for each slice and the discrepancy between these means. The optimal split is the one that maximizes this discrepancy while adhering to the group size requirements. ", "page_idx": 18}, {"type": "text", "text": "The function returns a query string that represents this optimal split. This string can then be used to segment the dataset into the identified slices for further analysis or testing. The following is pseudocode for the splitting algorithm. ", "page_idx": 18}, {"type": "text", "text": "Data: dataframe, features, outcome, min_group_size, max_group_size Result: Optimal query string for data split   \n1 Function GetOptimalSplitQuery(dataframe, features, outcome, min_group_size, max_group_size):   \n2 Validate input features and outcome in dataframe Determine the type of decision tree model based on outcome type Fit the decision tree model to dataframe using features and outcome Initialize an empty list conditions for tracking split conditions return TraverseTree(root, 0, conditions)   \n3 Function TraverseTree(node, depth, conditions):   \n4 if node is a leaf then   \n5 Calculate the discrepancy in outcome between the two slices return the condition and discrepancy if group size constraints are met end Determine left and right conditions based on the threshold at node return TraverseTree(node.left, depth + 1, conditions \u222a {left_condition}) // or the right conditions, depending on which has the greater discrepancy ", "page_idx": 19}, {"type": "text", "text": "This splitting mechanism is used for continuous features. A separate black-box splitting mechanism is developed for categorical features based on iterating on different permutations of these features and evaluating them that helps with variable selection. ", "page_idx": 19}, {"type": "text", "text": "In practice, we employ the LLM-based operationalization for the SMART ablation and the datadriven operationalization within the original SMART framework. However, we highlight that this is a design choice that we have found works in practice; there is nothing stopping from using any operationalization framework within the main SMART framework. ", "page_idx": 19}, {"type": "text", "text": "B.3 Feasibility checks ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "SMART is built on many modules which can be toggled on or off. One of such modules is a module called \u201cfeasibility check\u201d which evaluates whether there are any hypotheses which should be tested in the first place. The experiments presented in Sec. 5.1 highlight the importance of being able to identify when no relationship between covariates exist. ", "page_idx": 19}, {"type": "text", "text": "The feasibility check contains three steps. First, an LLM is queried to evaluate whether any relationships could exist between the covariates and an outcome variable. For instance, this could be whether a relationship could exist between loan default and the annual rainfall in a given region. Second, the answer is self-refined. This helps to evaluate feasibility because, in practice, initial responses tend to be over-optimistic (such as hypothesizing that annual rainfall is associated with loan default via a geographic proxy). This self-refinement helps to critically evaluate the previous answer. The number of steps in the self-refinement process is a hyperparameter. Lastly, the answer (whether or not a relationship could exist between the two variables and, hence, should be inspected) is converted to a boolean value via an external function. ", "page_idx": 19}, {"type": "text", "text": "Algorithm 2 Feasibility for evaluating slices ", "page_idx": 20}, {"type": "text", "text": "Input: unique_values, context, context_target, system_message, n_refine Output: feasibility_boolean_response 8 Function FeasibilityCheck(unique_values, context, context_target, system_message, n_refine): // Construct the feasibility task prompt 9 $t a s k\\leftarrow$ \u201cEvaluate subgroups for model performance\u201d task \u2190task\u222a\u201cContext: \u201d \u222acontext \u222a \u201cTarget: \u201d \u222acontext_target $t a s k\\gets t a s k\\cup$ \u201cColumns: \u201d \u222ajoin(unique_values.keys()) // Get initial feasibility response 10 feasibility_response $\\leftarrow$ GetLLMResponse(task, system_message) // Refine the answer 11 feasibility_response $\\leftarrow$ SelfRefine(unique_values, context, context_target, feasibility_response, system_message, n_refine) // Convert to boolean 12 boolean_task $\\leftarrow$ \u201cBased on analysis, provide yes/no answer\u201d boolean_task $\\leftarrow$ boolean_task\u222a\u201cAnalysis: \u201d \u222afeasibility_response 13 feasibility_boolean_response $\\leftarrow$ GetLLMResponse(boolean_task) ", "page_idx": 20}, {"type": "text", "text": "B.4 Data adjustment queries ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Given that SMART operates with an LLM, sometimes the framework outputs proposals which do not operationalize on the data well. For instance, even if a column \u201cage\u201d is a categorical variable, SMART might propose to operationalize the hypothesis \u201celderly people\u201d as age $>72$ which would cause an error. ", "page_idx": 20}, {"type": "text", "text": "To avoid this, we implement an additional data adjustment module which can handle such cases. It catches the error and re-prompts the LLM to find a group which could be operationalized given the data structure, and does so iteratively until such a group is found. ", "page_idx": 20}, {"type": "text", "text": "The following is a pseudocode function that explains how data adjustment is performed. ", "page_idx": 20}, {"type": "table", "img_path": "d75qCZb7TX/tmp/a23ba7cc82debb03c2c07bdf73932078fce080e9674ae6636e8d44b51f145186.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "B.5 Requirements, automation, and outputs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Requirements. SMART can natively integrate user requirements into its framework. This is done by inputting requirements as a string and concatenating it together with the context. Some requirements are directly integrated into the framework itself (e.g. functionalities for determining the minimum or maximum sample size of a data split). ", "page_idx": 20}, {"type": "text", "text": "Automation. Fig. 1 showcases the pipeline of SMART and which components are automated. SMART is developed using an \u201csklearn\u201d style fit, predict framework. The fit method automatically performs a feasibility check, generates hypotheses, justifications, operationalizes them, performs self-falsification using empirical data, re-ranks the hypotheses, and saves all intermediate results. The method can then automatically be used on any piece of data to evaluate whether it underperforms on the groups that have been found to underperform. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "Outputs. In addition to SMART outputting subgroups/slices or a scalar number, it can output a model report. An example report is provided in D.10. We note that this report is simply an example which employs both the findings of the fitting procedure and an additional LLM to summarize the outputs. More fine-grained outputs can be constructed. ", "page_idx": 21}, {"type": "text", "text": "B.6 Moving outside of IID data ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In the paper, we propose that SMART can move outside of IID data and generalize better in the presence of covariate shift (refer to Sec. D.2). We highlight that this is done by performing an analysis on the original data and using an LLM propose possible hypotheses and splits for a different target domain where no data is present (hence, operationalizing only using the LLM with access to previous operationalizations). The following is an example prompt which is designed to do this. ", "page_idx": 21}, {"type": "image", "img_path": "d75qCZb7TX/tmp/164b32517b61142f2e3b927e3bd32d67811767974a12306012420a077919273c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "B.7 SMART and multiple testing ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The reason why SMART performs testing via hypothesis generation is because testing for all slices is equivalent to generating and testing a hypothesis on the data. Here, we outline in greater detail how hypothesis generation is connected to multiple testing. ", "page_idx": 21}, {"type": "text", "text": "As outlined in the main manuscript, searching for $f$ failures may bring up the challenge of multiple hypothesis testing. Specifically, when we evaluate the failure rate of model $f$ across different slices $S_{i}\\subseteq{\\mathcal{D}}$ , we are testing the null hypothesis $H_{0}^{(i)}:\\mu_{S_{i}}=\\mu_{\\mathcal{D}}$ against the alternative $H_{1}^{(i)}:\\mu_{S_{i}}\\neq\\mu_{D}$ Then, the probability of making a Type I error increases with each test. This drastically inflates the family-wise error rate (FWER). For instance, assuming that each slice is independent, the probability of making one or more Type I errors across all tests is given by $1-(1-\\alpha)^{m}$ , where $m$ is the total number of slices tested. While this can be addressed by adjusting for multiple testing, we run into the trade-off between the FWER control and statistical power. As we employ statistical correction methods to account for Type I errors, we increase the probability of Type $\\mathrm{II}$ errors. ", "page_idx": 21}, {"type": "text", "text": "C Benchmarks & Experimental Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We summarize all experimental details, datasets and benchmarks. ", "page_idx": 22}, {"type": "text", "text": "Code can be found at: https://github.com/pauliusrauba/SMART_Testing or https:// github.com/vanderschaarlab/SMART_Testing ", "page_idx": 22}, {"type": "text", "text": "C.1 Datasets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We summarize the different datasets we use in this paper in Table 9. The datasets vary in number of samples, number of features and domain. ", "page_idx": 22}, {"type": "table", "img_path": "d75qCZb7TX/tmp/dd7f259f2c73613422d2ad0656eb994ae8843e192dc6a8e9847fbf75cbd583e2.jpg", "table_caption": ["Table 9: Summary of the datasets used. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Note that, depending on the experiment, we might vary the sample size or features used. This can be because of the added synthetic variables or certain slice discovery algorithms taking too long to run. ", "page_idx": 22}, {"type": "text", "text": "C.2 Baselines ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We compare SMART testing with the following data-only baselines in Section 5. ", "page_idx": 22}, {"type": "text", "text": "\u2022 Autostrat [14]. We use the implementation from 7 \u2022 Pysubgroup [21]. We use the implementation from \u2022 Divexplorer [15]. We use the implementation from \u2022 Slicefinder [12]. We use the implementation from 10 \u2022 Sliceline [13]. We use the implementation from 11 ", "page_idx": 22}, {"type": "text", "text": "C.3 LLM prompts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "C.3.1 Generic templates. ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\"\"\" Constructs the task description for the LLM.\"   \ntask $=$ f\"\"\" Your task is to propose possible hypotheses as to which subgroups within the dataset might have worse predictive performance than on average because of societal bias in the dataset , insufficient data , other relationships , or others. The subgroups might be based on any of the provided characteristics , as well as on any combination of such characteristics .   \nDataset information: {context }. { context_target }   \nThe dataset contains {len(unique_values )} columns. The columns are {\u2019, \u2019.join(unique_values.keys ())}. ", "page_idx": 22}, {"type": "text", "text": "Task: Create {n} hypotheses as to which subgroups within the dataset the model will perform worse than on average because of societal biases or other reasons. Important: Your hypothesis can contain either one variable or two variables in the condition. Therefore , your goal is to find discrepancies in the model \u2019s performance , not the underlying data outcomes. Justify why you think that for each of the {n} hypotheses. Format of the output: Hypothesis: <>; Justification: <>. ", "page_idx": 23}, {"type": "text", "text": "\"\"\" ", "page_idx": 23}, {"type": "text", "text": "Code Listing 4: Generic hypothesis prompt ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "111111 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The following are hypotheses about which people within a dataset the model might underperform on.   \nPropose specific ranges for each hypothesis. Hypotheses: {hypotheses }.   \nTASK: return a dictionary that contains an index number as the key and the column value as the value. If there are multiple columns in that hypothesis , return them in a list. There are the column names {\u2019, \u2019.join(unique_values .keys ())}. ", "page_idx": 23}, {"type": "text", "text": "Code Listing 5: Generic operationalization prompt ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "C.3.2 Example prompts: OULAD Education. ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1111 11 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "INPUT TEXT   \nYour task is to propose possible hypotheses as to which subgroups within the dataset might have worse predictive performance than on average because of societal bias in the dataset , insufficient data , other relationships , or others. The subgroups might be based on any of the provided characteristics , as well as on any combination of such characteristics .   \nDataset information:   \nOpen University Learning Analytics Dataset (OULAD) contains data about courses , students and their interactions with Virtual Learning Environment (VLE) for seven selected courses (called modules). Presentations of courses start in February and October - they are marked by B and J respectively . The dataset consists of tables ", "page_idx": 23}, {"type": "text", "text": "connected using unique identifiers. All tables are stored in the csv format. ", "page_idx": 23}, {"type": "text", "text": "The task is to predict the students who are likely to fail or withdraw from a course. ", "page_idx": 23}, {"type": "text", "text": "The dataset contains 14 columns. The columns are gender , region , highest_education , imd_band , age_band , num_of_prev_attempts , studied_credits , disability , test , group_0 , group_1 , group_2 , group_3 , group_4. The values are dict_items ([(\u2019 gender \u2019, [\u2019M\u2019, \u2019F \u2019]), (\u2019region \u2019, [\u2019South Region \u2019, \u2019Scotland \u2019, \u2019Yorkshire Region \u2019 West Midlands Region \u2019, \u2019East Anglian Region \u2019, \u2019East Midlands Region \u2019, \u2019South East Region \u2019, \u2019Wales \u2019, \u2019North Western Region \u2019, London Region \u2019, \u2019South West Region \u2019, \u2019Ireland \u2019, \u2019North Region \u2019]), (\u2019 highest_education \u2019, [\u2019Lower Than A Level \u2019, \u2019A Level or Equivalent \u2019, \u2019HE Qualification \u2019, \u2019No Formal quals \u2019, \u2019Post Graduate Qualification \u2019]), (\u2019imd_band \u2019, [\u201970-80%\u2019, \u201940-50%\u2019, $)O-10\\%\\ '$ , \u201910-20\u2019, \u201950-60%\u2019, \u201920-30%\u2019, \u201960-70%\u2019, \u201980-90%\u2019, \u201990-100%\u2019, \u201930-40%\u2019, nan]), (\u2019age_band \u2019, [\u20190-35\u2019, \u201935-55\u2019, \u201955<=\u2019]), ( num_of_prev_attempts \u2019, [0, 1, 2, 3, 5, 4, 6]), (\u2019 studied_credits \u2019, {\u2019min \u2019: 30, \u2019mean \u2019: 79.77122804326149 , \u2019max \u2019: 655}) , (\u2019disability [\u2019N\u2019, \u2019Y \u2019]), (\u2019test \u2019, [False , True ]), (\u2019group_0 \u2019, [False , True ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "]), (\u2019group_1 \u2019, [True , False ]), (\u2019group_2 \u2019, [True , False ]), (\u2019 group_3 \u2019, [False , True ]), (\u2019group_4 \u2019, [False , True ])])   \n12   \n13 Task: Create 5 hypotheses as to which subgroups within the dataset the model will perform worse than on average because of societal biases or other reasons. Therefore , your goal is to find discrepancies in the model \u2019s performance , not the underlying data Justify why you think that. Format of the output: Hypothesis: <>; Justification : <>.   \n14 \"\"\"   \n15   \n16 --OUTPUT TEXT   \n17 Hypothesis 1: Students with disabilities will have worse predictive performance than average.   \n18 Justification: Students with disabilities may face additional challenges in their learning process , which could affect their performance in the course. They may require additional support or accommodations that are not adequately captured in the dataset , leading to lower predictive performance for this subgroup.   \n19   \n20 Hypothesis 2: Students from lower socioeconomic backgrounds (indicated by imd_band) will have worse predictive performance than average.   \n21 Justification: Students from lower socioeconomic backgrounds may have limited access to resources and support systems that are necessary successful completion of the course. This lack of resources and support may not be adequately captured in the dataset , leading to lower predictive performance for this subgroup.   \n22   \n23 Hypothesis 3: Students with lower levels of education (indicated by highest_education ) will have worse predictive performance than average.   \n24 Justification: Students with lower levels of education may have less prior knowledge and skills that are necessary for success in the course. This lack of foundational knowledge may not be adequately captured in the dataset , leading to lower predictive performance for this subgroup.   \n25   \n26 Hypothesis 4: Students who have attempted the course multiple times indicated by num_of_prev_attempts ) will have worse predictive performance than average.   \n27 Justification: Students who have attempted the course multiple times may have encountered difficulties or challenges in their previous attempts that hindered their success. These challenges may not be adequately captured in the dataset , leading to lower predictive performance for this subgroup.   \n28   \n29 Hypothesis 5: Students in certain regions (indicated by region) will have worse predictive performance than average.   \n30 Justification: Students in certain regions may have different educational systems , resources , and support networks that can impact their performance in the course. These regional differences may not be adequately captured in the dataset , leading to lower predictive performance for subgroups based on region. Code Listing 6: Hypothesis generation: OULAD Dataset ", "page_idx": 24}, {"type": "text", "text": "INPUT TEXT ", "page_idx": 24}, {"type": "text", "text": "The following are hypotheses about which people within a dataset the model might underperform on.   \nPropose specific ranges for each hypothesis. Hypotheses: Hypothesis 1: Students with disabilities will have worse predictive performance than average. Justification: Students with disabilities may face additional challenges in their learning process , which could affect their performance in the course. They may require additional support or accommodations that are not adequately captured in the dataset , leading to lower predictive performance for this subgroup.   \n8   \n9 Hypothesis 2: Students from lower socioeconomic backgrounds (indicated by imd_band) will have worse predictive performance than average.   \n10 Justification: Students from lower socioeconomic backgrounds may have limited access to resources and support systems that are necessary for successful completion of the course. This lack of resources and support may not be adequately captured in the dataset , leading to lower predictive performance for this subgroup.   \n11   \n12 Hypothesis 3: Students with lower levels of education (indicated by highest_education ) will have worse predictive performance than average.   \n13 Justification: Students with lower levels of education may have less prior knowledge and skills that are necessary for success in the course. This lack of foundational knowledge may not be adequately captured in the dataset , leading to lower predictive performance for this subgroup.   \n14   \n15 Hypothesis 4: Students who have attempted the course multiple times indicated by num_of_prev_attempts ) will have worse predictive performance than average.   \n16 Justification: Students who have attempted the course multiple times may have encountered difficulties or challenges in their previous attempts that hindered their success. These challenges may not be adequately captured in the dataset , leading to lower predictive performance for this subgroup.   \n17   \n18 Hypothesis 5: Students in certain regions (indicated by region) will have worse predictive performance than average.   \n19 Justification: Students in certain regions may have different educational systems , resources , and support networks that can impact their performance in the course. These regional differences may not be adequately captured in the dataset , leading to lower predictive performance for subgroups based on region ..   \n20   \n21 Dataset information:   \n22 Open University Learning Analytics Dataset (OULAD) contains data about courses , students and their interactions with Virtual Learning Environment (VLE) for seven selected courses (called modules). Presentations of courses start in February and October - they are marked by B and J respectively . The dataset consists of tables connected using unique identifiers. All tables are stored in the csv format.   \n23   \n24 The task is to predict the students who are likely to fail or withdraw from a course.   \n25   \n26   \n27 The dataset contains 14 columns. The columns are gender , region , highest_education , imd_band , age_band , num_of_prev_attempts , studied_credits , disability , test , group_0 , group_1 , group_2 , group_3 , group_4. The values are dict_items ([(\u2019 gender \u2019, [\u2019M\u2019, \u2019F \u2019]), (\u2019region \u2019, [\u2019South Region \u2019, \u2019Scotland \u2019, \u2019Yorkshire Region \u2019, West Midlands Region \u2019, \u2019East Anglian Region \u2019, \u2019East Midlands Region \u2019, \u2019South East Region \u2019, \u2019Wales \u2019, \u2019North Western Region \u2019, London Region \u2019, \u2019South West Region \u2019, \u2019Ireland \u2019, \u2019North Region \u2019]), (\u2019 highest_education \u2019, [\u2019Lower Than A Level \u2019, \u2019A Level or Equivalent \u2019, \u2019HE Qualification \u2019, \u2019No Formal quals \u2019, \u2019Post Graduate Qualification \u2019]), (\u2019imd_band \u2019, [\u201970-80%\u2019, \u201940-50%\u2019, \u20190-10%\u2019, \u201910-20\u2019, \u201950-60%\u2019, \u201920-30%\u2019, \u201960-70%\u2019, \u201980-90%\u2019, \u201990-100%\u2019 \u201930-40%\u2019, nan]), (\u2019age_band \u2019, [\u20190-35\u2019, \u201935-55\u2019, \u201955<=\u2019]), num_of_prev_attempts \u2019, [0, 1, 2, 3, 5, 4, 6]), (\u2019 studied_credits \u2019, {\u2019min \u2019: 30, \u2019mean \u2019: 79.77122804326149 , \u2019max \u2019: 655}) , (\u2019disability , [\u2019N\u2019, \u2019Y \u2019]), (\u2019test \u2019, [False , True ]), (\u2019group_0 \u2019, [False , True ]), (\u2019group_1 \u2019, [True , False ]), (\u2019group_2 \u2019, [True , False ]), (\u2019 group_3 \u2019, [False , True ]), (\u2019group_4 \u2019, [False , True ])])   \n28   \n29 TASK: Propose specific variable ranges for each hypothesis such that they are clearly operationalizable and defined. Use this format: Hypothesis: <>; Operationalization : <>.   \n30 \"\"\"   \n31   \n32 -OUTPUT TEXT --   \n33 Hypothesis 1: Students with disabilities will have worse predictive performance than average.   \n34 Operationalization : Students with disabilities can be identified by the \"disability\" variable in the dataset. To test this hypothesis , we can define the range for students with disabilities as those who have a value of \"Y\" in the \"disability\" column.   \n35   \n36 Hypothesis 2: Students from lower socioeconomic backgrounds (indicated by imd_band) will have worse predictive performance than average.   \n37 Operationalization : Socioeconomic background can be measured using the \"imd_band\" variable in the dataset. To test this hypothesis , we can define the range for students from lower socioeconomic backgrounds as those who have an \"imd_band\" value in the range of \" $0-30\\%\"$ .   \n38   \n39 Hypothesis 3: Students with lower levels of education (indicated by highest_education ) will have worse predictive performance than average.   \n40 Operationalization : Education level can be determined using the \" highest_education \" variable in the dataset. To test this hypothesis , we can define the range for students with lower levels of education as those who have a value of \"Lower Than A Level\" or \"No Formal quals\" in the \" highest_education \" column.   \n41   \n42 Hypothesis 4: Students who have attempted the course multiple times indicated by num_of_prev_attempts ) will have worse predictive performance than average.   \n43 Operationalization : Previous attempts can be measured using the 11 num_of_prev_attempts \" variable in the dataset. To test this hypothesis , we can define the range for students who have attempted the course multiple times as those who have a value greater than 1 in the \" num_of_prev_attempts \" column.   \n44   \n45 Hypothesis 5: Students in certain regions (indicated by region) will have worse predictive performance than average.   \n46 Operationalization : Region can be determined using the \"region\" variable in the dataset. To test this hypothesis , we can define the range for students in certain regions as those who belong to the regions of \"North Region\" or \"Wales\". ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "Code Listing 7: Operationalization (LLM knowledge): OULAD Dataset ", "page_idx": 26}, {"type": "text", "text": "The following are groups that are defined based on the dataset. Convert them into a Python dictionary format. Each group should be represented as a key -value pair in the dictionary , where the key is an index (0 to 4), and the value is a string representing the group using Python syntax and logical operators. For multiple ", "page_idx": 26}, {"type": "text", "text": "conditions , use Python \u2019s logical \u2019and\u2019 (\u2019&&\u2019) or \u2019or\u2019 (\u2019||\u2019) Ensure the format is a valid Python dictionary. ", "page_idx": 27}, {"type": "text", "text": "6 Examples: ", "page_idx": 27}, {"type": "text", "text": "- Single Condition: {0: \u2019X > 45\u2019} Multiple Conditions: {1: \u2019(X > 45) and (Y < 20) \u2019} ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "10 Groups to summarize: Hypothesis 1: Students with disabilities will have worse predictive performance than average. ", "page_idx": 27}, {"type": "text", "text": "12 Operationalization : Students with disabilities can be identified by the \"disability\" variable in the dataset. To test this hypothesis , we can define the range for students with disabilities as those who have a value of \"Y\" in the \"disability\" column. ", "page_idx": 27}, {"type": "text", "text": "14 Hypothesis 2: Students from lower socioeconomic backgrounds (indicated by imd_band) will have worse predictive performance than average. ", "page_idx": 27}, {"type": "text", "text": "15 Operationalization : Socioeconomic background can be measured using the \"imd_band\" variable in the dataset. To test this hypothesis , we can define the range for students from lower socioeconomic backgrounds as those who have an \"imd_band\" value in the range of \"0 -30%\". ", "page_idx": 27}, {"type": "text", "text": "17 Hypothesis 3: Students with lower levels of education (indicated by highest_education ) will have worse predictive performance than average. ", "page_idx": 27}, {"type": "text", "text": "18 Operationalization : Education level can be determined using the highest_education \" variable in the dataset. To test this hypothesis , we can define the range for students with lower levels of education as those who have a value of \"Lower Than A Level\" or \"No Formal quals\" in the \" highest_education \" column. ", "page_idx": 27}, {"type": "text", "text": "20 Hypothesis 4: Students who have attempted the course multiple times indicated by num_of_prev_attempts ) will have worse predictive performance than average. ", "page_idx": 27}, {"type": "text", "text": "Operationalization : Previous attempts can be measured using the num_of_prev_attempts \" variable in the dataset. To test this hypothesis , we can define the range for students who have attempted the course multiple times as those who have a value greater than 1 in the \" num_of_prev_attempts \" column. ", "page_idx": 27}, {"type": "text", "text": "23 Hypothesis 5: Students in certain regions (indicated by region) will have worse predictive performance than average. ", "page_idx": 27}, {"type": "text", "text": "24 Operationalization : Region can be determined using the \"region\" variable in the dataset. To test this hypothesis , we can define the range for students in certain regions as those who belong to the regions of \"North Region\" or \"Wales \". ", "page_idx": 27}, {"type": "text", "text": "25 Column names: gender , region , highest_education , imd_band , age_band , num_of_prev_attempts , studied_credits , disability , test , group_0 , group_1 , group_2 , group_3 , group_4 ", "page_idx": 27}, {"type": "text", "text": "26 Column values: dict_items ([(\u2019 gender \u2019, [\u2019M\u2019, \u2019F \u2019]), (\u2019region \u2019, [\u2019South Region \u2019, \u2019Scotland \u2019, \u2019Yorkshire Region \u2019, \u2019West Midlands Region \u2019, East Anglian Region \u2019, \u2019East Midlands Region \u2019, \u2019South East Region \u2019, \u2019Wales \u2019, \u2019North Western Region \u2019, \u2019London Region \u2019, \u2019South West Region \u2019, \u2019Ireland \u2019, \u2019North Region \u2019]), (\u2019 highest_education \u2019, [\u2019 Lower Than A Level \u2019, \u2019A Level or Equivalent \u2019, \u2019HE Qualification \u2019, \u2019No Formal quals \u2019, \u2019Post Graduate Qualification \u2019]), (\u2019imd_band \u2019, [\u201970-80%\u2019, \u201940-50%\u2019, \u20190-10%\u2019, \u201910-20\u2019, \u201950-60%\u2019, \u201920-30%\u2019, \u201960-70%\u2019, \u201980-90%\u2019, \u201990-100%\u2019, \u201930-40%\u2019, nan]), (\u2019age_band \u2019, [\u20190-35\u2019, \u201935-55\u2019, \u201955<=\u2019]), (\u2019 num_of_prev_attempts \u2019, [0, 1, 2, 3, 5, 4, 6]), (\u2019 studied_credits \u2019, {\u2019min \u2019: 30, \u2019mean \u2019: 79.77122804326149 , \u2019max \u2019: 655}) , (\u2019disability \u2019, [\u2019N\u2019, \u2019Y \u2019]), ( test \u2019, [False , True ]), (\u2019group_0 \u2019, [False , True ]), (\u2019group_1 \u2019, True , False ]), (\u2019group_2 \u2019, [True , False ]), (\u2019group_3 \u2019, [False , True ]), (\u2019group_4 \u2019, [False , True ])]) ", "page_idx": 27}, {"type": "image", "img_path": "d75qCZb7TX/tmp/c3c7c34833d1c69f42a816051b54e8a2236ef13b6ede09406b360574dd2eed35.jpg", "img_caption": ["Code Listing 8: Interpreter: OULAD Dataset "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "C.4 Experimental details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We note that all experiments were performed using two compute resources: a server with NVIDIA RTX A4000 GPU and 18-Core Intel Core i9-10980XE, as well as an Apple M1 Pro 32GB RAM. We exemplify SMART Testing using GPT-4 [29] as the LLM but run further experiments to test the sensitivity to the type of language model in Appendix D.3. ", "page_idx": 28}, {"type": "text", "text": "C.4.1 Context-aware testing (Sec. 5.1.) ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Goal. We aim to underscore the role of context in ML model testing to prevent false positives, especially when dealing with tabular data where data may contain many irrelevant or uninformative features [30], persisting even post-feature selection [31, 32]. We contrast SMART which explicitly accounts for context, in contrast to data-only approaches which are context-unaware only operating on the data. ", "page_idx": 28}, {"type": "text", "text": "Setup. We fti a predictive model to the training dataset, varying the number of irrelevant, synthetically generated features contained in the dataset \u2014 where irrelevant features are drawn from different distributions. We then quantify the proportion of conditions in the identified slices that falsely include the irrelevant synthetic synthetically features. ", "page_idx": 28}, {"type": "text", "text": "Because different methods are sensitive to different types of irrelevant features, we developed a data generating processes that encompasses many types of variables. Over many runs, different data-only methods pick up on some of these variables, showcasing that all methods are susceptible to randomly sampled irrelevant features in the dataset. ", "page_idx": 28}, {"type": "text", "text": "Sampling mechanism. To evaluate the impact of irrelevant features, we enrich the dataset by adding synthetic categorical variables. The number of new variables is equal to the number of existing features in the dataset. For each new variable $x_{i}$ , we determine its type by sampling from a Bernoulli distribution with probability 0.5. If the sampled value is 0, $x_{i}$ is a Bernoulli variable with success probability 0.1; otherwise, $x_{i}$ is a categorical variable with four categories, following a predefined probability distribution (e.g., {0.1, 0.3, 0.4, 0.2}): ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Type}(x_{i})\\sim\\mathrm{Bernoulli}(0.5),}\\\\ &{\\qquad x_{i}\\sim\\left\\{\\!\\!\\begin{array}{l l}{\\!\\mathrm{Bernoulli}(0.1)\\!}&{\\mathrm{if}\\;\\mathrm{Type}(x_{i})=0,}\\\\ {\\!\\!\\mathrm{Categorical}(0.1,0.3,0.4,0.2)}&{\\mathrm{if}\\;\\mathrm{Type}(x_{i})=1.}\\end{array}\\!\\!\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We note that there are synthetic data generating processes that completely break other data-only methods. As an example, creating a unique ID column for each sample breaks the Autostrat algorithm, as all the subgroups/slices identified are the unique IDs. The data generating process employed in our experiment reflects a broad variety of commonly encountered DGPs. ", "page_idx": 28}, {"type": "text", "text": "C.4.2 Requirements-constrained testing (Sec. 5.2.) ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We test whether each of the methods can fulfil three requirements. ", "page_idx": 28}, {"type": "text", "text": "The first requirement involved the use of the variable \u201cage\u201d in each detected slice. This was passed as an input to SMART. The other methods do not accept context as input and, therefore, it was not possible to fulfli these requirements. The numbers provided for other methods are simply how often they fulfilled the requirements by chance. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "The second and third requirements involved obtaining a minimum and maximum sample size. This was passed as an input to SMART and the ablated SMART version. Based on this, SMART changed its hyperparameter within its function which asks to indicate a minimum and maximum sample size for the discovered slices. As with the previous experiments, this was not adjusted for the other groups because they do not take textual input. ", "page_idx": 29}, {"type": "text", "text": "C.4.3 Targeting model failures (Sec. 5.3.) ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In order to evaluate the targeting of model failures, we try four different tabular models with prespecified hyperparameters. We find discrepant slices on the training dataset and evaluate them on the testing dataset. ", "page_idx": 29}, {"type": "text", "text": "C.4.4 Adaptive testing for a deployment environment (Sec. 5.4.) ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "The goal of the adaptive testing experiment is to understand the extent to which SMART, as well as other data-only methods, can use data in a source domain to generalize to a new, target domain where a covariate shift has been detected. To this end, the datasets provided have a known covariate shift and can be evaluated. ", "page_idx": 29}, {"type": "text", "text": "Each method was trained on the UK dataset and the discovered slices were evaluated on the US dataset. No additional context was provided to data-only methods since they do not accept any text or context as inputs. ", "page_idx": 29}, {"type": "text", "text": "In contrast, we have provided SMART with the previously discovered slices and hypotheses, and have asked to re-evaluate these hypotheses in the context of the US dataset. Specifically, SMART re-hypothesized possible model failures for the US market but used the UK data to operationalize the variables. The ablated version, $S M A R T_{N S F}$ , achieved the best overall performance on the US market. The ablated version (i) did not have access to the UK data and the failures of the models; and (ii) operationalized each covariate using the LLM alone (refer to Sec. B.2 for a discussion on operationalizations with different SMART versions). This provides evidence that, in the presence of covariate shift, using inductive knowledge or domain expertise might be more useful to finding meaningful model failures. ", "page_idx": 29}, {"type": "text", "text": "C.4.5 Discovery of societally important groups and failure understanding (Sec. 5.5.) ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "As discussed in the main paper, SMART provides both possible hypotheses and justifications for model failures which can be evaluated using a simple \u201cfti\u201d method. Furthermore, SMART prioritizes meaningful data slices which are of societal importance. Such slices can be inspected for any data input. ", "page_idx": 29}, {"type": "text", "text": "D Additional experiments ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "D.1 Requirements-constrained testing ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Goal. Requirements are a crucial, yet neglected part of ML model testing, such as verifying performance on societally relevant dimensions or verifying specific aspects to meet compliance requirements. However, no previous testing framework has incorporated the notion of satisfying requirements when defining the test slices. This experiment illustrates how SMART integrates requirements provided in natural language, which then influences the hypotheses generated to satisfy testing requirements. ", "page_idx": 30}, {"type": "text", "text": "Setup. We cover three real-world requirements that end-users might have: one based on demographics and two based on sample size. Requirement 1: Each of the top 10 identified unique slices should involve the age of a person. Requirement 2: The sample size of the top 10 identified unique slices must have at least 150 observations. Requirement 3: The sample size of the top 10 identified unique slices has to be small, within 10 and 150 observations. We exemplify the experiment using a real-world prostate cancer dataset from the UK [35], as healthcare often mandates certain testing requirements (e.g. Collins et al. [48]). ", "page_idx": 30}, {"type": "text", "text": "Analysis. Table 10 shows that SMART which directly integrates requirements (via natural language), satisfies the requirements a greater number of times compared to data-only methods, which only satisfies requirements by chance (hence the low number of times). Beyond satisfying requirements, the SMART slices also represent model failures that almost always have statistically significant performance differences from average when evaluated on test data. Finally, while $S M A R T_{N S F}$ (ablation without self-falsification) can satisfy requirements, the number of statistically significant slices is lower than SMART, thus underscoring the value of our self-falsification mechanism. That said, $S M A R T_{N S F}$ still outperforms data-only baselines. ", "page_idx": 30}, {"type": "table", "img_path": "d75qCZb7TX/tmp/e997e919bc96db5ae158b527658bd2da9dc1081861612f4d72e43e93028e47ca.jpg", "table_caption": ["Table 10: Requirement satisfaction showing how many times the top 10 generated slices satisfied the requirements (Req) and how many of these slices had statistically significantly different performance from average (Sig ) on a testing dataset. Maximum is 10. $\\uparrow$ is better. "], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "Takeaway 2. SMART, unlike data-only methods, identifies slices that have significant performance differences, whilst also satisfying requirements \u2014 an important dimension not even considered by previous testing methods. ", "page_idx": 30}, {"type": "text", "text": "D.2 Adaptive testing for a deployment environment ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Goal. Deploying an ML model often entails going beyond IID, such as a different deployment environment. We consider the case of deploying a model to a different country where there is a covariate shift 12 and evaluate the capabilities of testing frameworks to adapt across the different environment and identify model failures. ", "page_idx": 31}, {"type": "text", "text": "Setup. We use real-world prostate cancer datasets from different country\u2019s cancer registries with known distribution shifts: SEER (US) [11] and CUTRACT (UK) [35]. We train predictor $f$ on UK data, while our target deployment environment is the US. ", "page_idx": 31}, {"type": "text", "text": "Analysis. \u25b6Identifying model failures. Table 11 shows that SMART better tests models at deployment time using the information provided. SMART identifies a much greater number of statistically significant model failures (almost all possible), both within the same environment (UK) and when shifting to a different one (US), even after adjusting for multiple comparisons using Bonferroni correction. ", "page_idx": 31}, {"type": "text", "text": "$\\blacktriangleright$ Sample size sensitivity. We also assess sensitivity to sample size, see Fig. 8. Both variants of SMART are shown to consistently outperform data-only counterparts in identifying a much greater number of significant model failures. Within domain (UK): as expected, we find that for lower sample sizes, $S M A R T_{N S F}$ (without the self-falsification mechanism) is superior, however, given enough data we then find that SMART benefits from the self-falsification. ", "page_idx": 31}, {"type": "text", "text": "Table 11: Number of slices identified (out of a maximum of 10) that had significantly divergent performance from average (higher is better). $S_{-}\\alpha$ counts the number of significantly divergent groups at $\\alpha=0.05$ ; $S_{-}\\alpha/n$ applies the Bonferroni correction. $\\uparrow$ is better. ", "page_idx": 31}, {"type": "table", "img_path": "d75qCZb7TX/tmp/fed9ec349d9e8047b4729b6a07d5c6ca6c05cae6c125002719459b89cd8073c1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 31}, {"type": "text", "text": "Deployment environment (US): we find that self-falsification similarly requires sufficient samples; which we note is expected behavior. Interestingly, in the deployment setting (US), SMARTNSF generally identifies the greatest number of significant failures (almost all possible) across different sample sizes. This suggests that under covariate shift, using inductive knowledge (via the LLM) or domain expertise might be more useful to find meaningful model failures. ", "page_idx": 31}, {"type": "image", "img_path": "d75qCZb7TX/tmp/558d002f0d19ad48d7ee6eeefd5362423088fb2276b9100402eb9fdfbf685d9d.jpg", "img_caption": ["Figure 8: Number of significant groups discovered (out of a total of 10) based on the training dataset size. SMART can operate under any sample size; self-falsification mechanism requires a larger sample size to falsify hypotheses. \u2191is better. "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "Overall, the results highlight the flexibility of SMART to handle different scenarios and sample sizes. From a practical perspective, while both SMART variants outperform data-only methods, the implication is that there is nuance in using different SMART variants for different scenarios. ", "page_idx": 31}, {"type": "text", "text": "Takeaway 4. SMART identifies more significant divergent failure slices in a deployment setting, outperforming data-only methods across environments and sample sizes. ", "page_idx": 31}, {"type": "text", "text": "D.3 Effects of LLMs ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "In this section, we provide more experimental details which compare the effectiveness of two GPT models, GPT3.5 and GPT4. We highlight that the goal is not to exhaustively test the framework with every LLM. Rather, the goal is to showcase that SMART is feasible with at least the capabilities of GPT-4. We provide this section as a way to measure the sensitivity of the model\u2019s performance with lower LLMs but highlight that we do not recommend using it with smaller LLMs, especially LLMs with fewer than 7B parameters. ", "page_idx": 32}, {"type": "text", "text": "D.3.1 Comparison over identified divergent slices ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "The following table reproduces the experiment from Sec. D.2 by directly comparing two models - GPT3.5 and GPT4. The setup is the same as in the original experiment. ", "page_idx": 32}, {"type": "text", "text": "Table 12: Number of slices identified (out of a maximum of 10) that had significantly divergent performance from average (higher is better). $S_{\\alpha}$ counts the number of significantly divergent groups at $\\alpha=0.05$ ; $S_{\\alpha/n}$ applies the Bonferroni correction. ", "page_idx": 32}, {"type": "table", "img_path": "d75qCZb7TX/tmp/e39a0c4c575a0a62fb52e9d4c575d4c63a992334168437efaab16aa23f24b7d1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 32}, {"type": "text", "text": "The table provides a measure of the model\u2019s performance on the training dataset from the same environment $(\\mathcal{D}_{\\mathrm{train}}^{\\mathrm{UK}})$ , the testing dataset from the same environment DtUesKt), and a different deployment environment (DtUesSt). ", "page_idx": 32}, {"type": "text", "text": "Takeaway. Both GPT3.5 and GPT4 provide strong increases over benchmark methods with little variability between the two LLMs. One of the possible reasons why is that the hypothesis space of possible model failures is somewhat limited. This can be seen by the similar hypotheses that are generated by both GPT models. ", "page_idx": 32}, {"type": "text", "text": "D.3.2 Performance across different models ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "In this section, we vary different tabular machine learning model types and identify how well the ablated and original SMART, identified with GPT3.5 and GPT4, can identify slices with large performance discrepancies. ", "page_idx": 32}, {"type": "text", "text": "Table 13: The differences in accuracies between the top slice identified for each method on a testing dataset. The p-value computes the p-value associated with the difference in the accuracy. For the accuracy, higher values imply a greater ability to detect divergent slices (hence, higher is better). For the p-value, lower is better. Averages $+-$ standard deviations are shown across 5 runs with random seeds and data splits ", "page_idx": 32}, {"type": "table", "img_path": "d75qCZb7TX/tmp/de49b615b82c139e6c7fcac6e5692a2a20da63ad78f5ad7b4c8b9c70f5227045.jpg", "table_caption": [], "table_footnote": [], "page_idx": 32}, {"type": "text", "text": "SMART with deep learning models. SMART\u2019s targeted sampling of hypotheses, is entirely independent of the downstream model used. i.e. SMART\u2019s context-guided slice sampling mechanism is used to generate hypotheses independently of the downstream model. ", "page_idx": 32}, {"type": "text", "text": "We extend our analysis with Logistic Regression, SVM, XGBoost, and MLP to further include two tabular deep learning method: TabPFN and TabNet. As shown in Table 14, across all models SMART is the best at finding subgroups where the models are least reliable. ", "page_idx": 33}, {"type": "table", "img_path": "d75qCZb7TX/tmp/f93408ee0457d05b08508d54fa7e6c1e61fb13712a3ca4ff6a204fe487e85140.jpg", "table_caption": ["Table 14: Identifying slices with the highest performance discrepancies. We show differences in accuracies $(|\\Delta A c c|)$ between the top identified divergent slice and average performance across two state-of-the-art deep learning classifiers (over 5 runs) on the SEER dataset. $\\uparrow$ is better. 0.00 implies the evaluation method does not support the model. "], "table_footnote": [], "page_idx": 33}, {"type": "text", "text": "Takeaway. GPT4 adds additional value when it comes to identifying slices with performance discrepancies. However, GPT3.5 is still able to find significant performance discrepancies across divergent slices, competitive and in many cases exceeding the performance of other methods (refer to Sec. D.4). ", "page_idx": 33}, {"type": "text", "text": "D.3.3 Hypothesis generation ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "A key component which determines the quality of the SMART method is the ability to navigate the search space by generating hypotheses. Therefore, we overview the kinds of hypotheses which are generated and how they differ based on the type of LLM used. ", "page_idx": 33}, {"type": "text", "text": "The table below provides an example of the top 10 hypotheses and justifications generated by GPT3.5 and GPT4 for the prostate cancer example in the UK, where the goal is to find specific slices where the model is likely to fail, and propose justifications for them. ", "page_idx": 33}, {"type": "text", "text": "Key takeaway. Many of the hypotheses for both models are similar. The underlying difference is in the ordering and justification of the hypotheses. ", "page_idx": 33}, {"type": "table", "img_path": "d75qCZb7TX/tmp/c7a0c06d9e167baa7e0adecd60cc30d97825c11fe7e9c32bfa95a59b7c9eb26b.jpg", "table_caption": ["Table 15: Comparison of Hypotheses and Justifications generated by GPT4 and GPT3.5 for a breast cancer prediction task. The hypothesis target likely model failures. "], "table_footnote": [], "page_idx": 34}, {"type": "text", "text": "D.4 Effects of different tabular machine learning models ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "The primary task of the SMART method is to evaluate a given, trained machine learning model. Thus far, we have been using a logistic regression model as the basis for evaluation in the main experiments. However, the results are not sensitive to the type of the tabular model. Therefore, in this section, we provide additional experiments where we vary the tabular model for the task. Specifically, we consider the following models initialized with their default hyperparameters for evaluation: Logistic Regression, Support Vector Machines, Boosting (implemented with XGBoost) and a multi-layer perceptron with 2 hidden layers and RELU activation functions in the hidden layers. ", "page_idx": 34}, {"type": "text", "text": "Goal. The primary goal is to understand whether the framework generalizes to other models which operate under different mapping mechanisms (e.g. a logistic regression, which is a linear model, compared to a tree-based model). ", "page_idx": 34}, {"type": "text", "text": "Setup. Given that not all the slice discovery or model evaluation algorithms output multiple slices, we constrain the evaluation to only focus on a single slice which might have discrepant performance. We use the UK prostate cancer dataset and evaluate the discrepancy of the top identified slice relative to the average across all identified model types. The discrepancy is calculated as the absolute differences of the average performance between the two groups, as well as the $\\mathbf{p}$ -value associated with the difference. The results show the average performance $+-$ standard deviation of 5 random splits. A higher absolute difference indicates that the model fails on one of the slices more than average. ", "page_idx": 34}, {"type": "text", "text": "Discretizing the inputs. Many of the discovery methods, however, operate only on categorical data. The previously used dataset, however, has three continuous variables: age, prostate-specific antigen, and comorbidities. We therefore also assess the quality of these methods to discover slices on the testing dataset when these three variables are discretized into 10 bins each. The following table reports the performance of all the methods (note: we did not discretize the dataset for SMART because it can work natively on continuous data). ", "page_idx": 34}, {"type": "table", "img_path": "d75qCZb7TX/tmp/b00aa94e3f1146384ff57623beda4f60d2b90d537ae0a5f5f81af8676a3c92af.jpg", "table_caption": ["Table 16: The differences in accuracies between the top slice identified for each method on a testing dataset. The p-value computes the ${\\bf p}$ -value associated with the difference in the accuracy. For the accuracy, higher values imply a greater ability to detect divergent slices (hence, higher is better). For the p-value, lower is better. Averages $+-$ standard deviations are shown across 5 runs with random seeds and data splits. "], "table_footnote": [], "page_idx": 35}, {"type": "text", "text": "", "page_idx": 35}, {"type": "table", "img_path": "d75qCZb7TX/tmp/57a8a62b50af565af53ffbd663227ee9303f9fe1bb9958b0c04a4b63e716170b.jpg", "table_caption": ["Table 17: The differences in accuracies between the top slice identified for each method on a testing dataset when the datasets continuous features are discretized. "], "table_footnote": [], "page_idx": 35}, {"type": "text", "text": "Takeaway. SMART is able to consistently identify the greatest performing slices across a number of different tabular models. ", "page_idx": 35}, {"type": "text", "text": "D.5 On the inductive biases of ML model testing ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Goal. We further observe that data-only testing methods implicitly assume the existence of slices with discrepancies in performance. While indeed ML models do fail \u2014 it is equally as problematic to highlight failures where there are none. ", "page_idx": 35}, {"type": "text", "text": "Setup. To evaluate this we propose a fully synthetic setup. Here, both the dependent variable $(Y)$ and the independent variables $({\\mathbf X})$ are sampled from a predefined random distribution. Specifically, we predict loan default $(Y\\,\\in\\,\\{0,1\\})$ based on a set of independent variables $\\mathbf{X}=\\{\\bar{N}_{\\mathrm{runs}},M_{\\mathrm{pref}},A_{\\mathrm{rainfall}},F_{\\mathrm{color}},P_{\\mathrm{season}}\\}$ , which are conceptually and empirically independent of the outcome of interest. Ideally, if we account for context we should be able to identify these disparate features should not influence the account and hence without prior relationships we should not flag spurious slices. ", "page_idx": 35}, {"type": "text", "text": "We compare the data-only methods to SMART under three different data generating processes (scenarios) that capture diverse underlying dynamics denoted as $\\mathcal{S}_{\\mathrm{uniform}}$ , $\\ensuremath{\\mathcal{S}}_{\\mathrm{skewed}}$ , and Sinteractions, where each DGP has a focus on uniform, skewed, and interactive effects, respectively. ", "page_idx": 35}, {"type": "text", "text": "The first scenario is given by the variables sampled from the following DGPs: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{N_{\\mathrm{runs}}\\sim\\mathrm{Uniform}\\{1,499\\},}\\\\ &{M_{\\mathrm{pref}}\\sim\\mathrm{Uniform}\\{1,5\\},}\\\\ &{A_{\\mathrm{rainfall}}\\sim\\mathrm{Uniform}\\{2000,9999\\},}\\\\ &{F_{\\mathrm{color}}\\sim\\mathrm{Uniform}\\{1,6\\},}\\\\ &{P_{\\mathrm{season}}\\sim\\mathrm{Uniform}\\{0,3\\},}\\\\ &{\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,Y\\sim\\mathrm{Uniform}\\{0,1\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "The second scenario is given by the variables sampled from the following DGPs: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad N_{\\mathrm{rus}}\\sim\\mathrm{Uniform}\\{1,499\\},}\\\\ &{\\quad M_{\\mathrm{pref}}\\sim\\mathrm{Binomial}(1,0.5),}\\\\ &{\\quad A_{\\mathrm{rainfal}}\\sim\\mathrm{Categorical}(0.1,0.3,0.4,0.2),}\\\\ &{\\quad F_{\\mathrm{color}}\\sim\\mathrm{Binomial}(1,0.1),}\\\\ &{\\quad P_{\\mathrm{season}}\\sim\\mathrm{Binomial}(1,0.05),}\\\\ &{\\quad\\quad Y\\sim\\mathrm{Uniform}\\{0,1\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "The third scenario is given by the variables sampled from the following DGPs: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{N_{\\mathrm{runs}}\\sim\\mathrm{Uniform}\\{1,499\\},}\\\\ &{M_{\\mathrm{pref}}\\sim\\mathrm{Binomial}(1,0.5),}\\\\ &{A_{\\mathrm{rainfall}}\\sim\\mathrm{Categorical}(0.1,0.3,0.4,0.2),}\\\\ &{A_{\\mathrm{music.hap}}=M_{\\mathrm{pref}}\\times A_{\\mathrm{rainfall}},}\\\\ &{A_{\\mathrm{run.hap}}=N_{\\mathrm{runs}}\\times M_{\\mathrm{pref}},}\\\\ &{\\qquad Y\\sim\\mathrm{Uniform}\\{0,1\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Analysis. Table 18 shows the number of slices spuriously discovered, while Table 19 outlines the number of conditions within the slices. We can clearly see the pitfalls of data-only approaches which detect slices which in reality have no relation to one another \u2014 often surfacing few conditions per group which suggests they arise by chance. The rationale for this failure is simply because data-only approaches do not and cannot reason about the features and/or understand context and simply aim to find slices with discrepancies in performance \u2014 which of course could arise by chance. In contrast, we see that SMART by virtue of context-awareness can avoid surfacing groups \u2014 which in reality have no relationships. ", "page_idx": 36}, {"type": "table", "img_path": "d75qCZb7TX/tmp/6ecb9655e493b7f37448d501a35f6296b6444c6e470402edfd1cf8791e668c48.jpg", "table_caption": ["Table 18: Number of discovered slices on a synthetic dataset with no prior relationships in three data generating process scenarios. slices capped at most 20. Average of 50 runs $\\pm$ standard deviations is shown. "], "table_footnote": [], "page_idx": 36}, {"type": "text", "text": "D.6 Context aware sensitivity ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "We provide an additional experiment where we vary the sample size in the training dataset and observe how that affects the number of slices discovered for each method. We show that the SMART is not affected by irrelevant features regardless of the sample size of the training dataset. The result is shown in Figure 9. ", "page_idx": 36}, {"type": "table", "img_path": "d75qCZb7TX/tmp/da4e0782a220a7def3024d70480da3a19ff3b771826072f966ad2ee82d0d9aff.jpg", "table_caption": ["Table 19: Number of conditions per discovered slice (false positives) in three data generating process scenarios. Average of 50 runs $+-$ standard deviations is shown. Lower is better. "], "table_footnote": [], "page_idx": 37}, {"type": "image", "img_path": "d75qCZb7TX/tmp/4283184c83ed6ff21b7f89f31174c6a705371220d7d91de48aaee6deb586ae14.jpg", "img_caption": ["Figure 9: Proportion of irrelevant features (y) for each slice discovery method, based on the sample size. Lower is better. "], "img_footnote": [], "page_idx": 37}, {"type": "text", "text": "D.7 Cost of SMART ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "We assess the cost of LLM hypothesis generation and scalability to larger datasets. Specifically, we demonstrate not only that SMART is cheap but also easily scalable to large datasets ", "page_idx": 37}, {"type": "text", "text": "\u2022 Scalability: SMART\u2019s scalability depends on the number of hypotheses generated, not dataset size (unlike data-only methods). This allows SMART to easily scale to arbitrarily large datasets. \u2022 Cost Analysis: In practical terms, cost then also scales primarily with the number of hypotheses generated, not dataset size. We provide a rough estimate based on token counts of input and outputs for 2 datasets (SEER and OULAD) in Table 20. This would be less than 0.1 USD for 5 hypotheses and less than 0.5 USD for 100 hypotheses for state-of-the-art ", "page_idx": 37}, {"type": "text", "text": "Table 20: mCoodste losf. SMART (USD) for different GPT LLMs and different numbers of hypotheses generated. The cost is estimated based on token counts. Note: GPT-4o models are post our paper and are even cheaper. ", "page_idx": 37}, {"type": "table", "img_path": "d75qCZb7TX/tmp/8a79154dfc757a46e2947eb6aaf572c23fb26abc451e9c9198f15569b8da2750.jpg", "table_caption": [], "table_footnote": [], "page_idx": 37}, {"type": "text", "text": "D.8 SMART with open-weight models ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "SMART ideally should be used with the most capable LLM possible.That said, we assess the differences in hypotheses between open-weight models and GPT-4. ", "page_idx": 37}, {"type": "text", "text": "We assess Mistral-7b, Qwen-1.5-7b, Llama-3-8b, Llama-70b, where for the OULAD and SEER datasets we generate 5 hypotheses and assess overlap to the hypotheses generated by GPT-4. This is presented in Table 21. ", "page_idx": 37}, {"type": "text", "text": "To summarize, the overlap between open-source models and GPT-4 is between $60{-}80\\%$ . We find that open-source models propose similar hypotheses, but they are not replacements for more capable models. This highlights that less capable models might propose similar hypotheses, yet they still catch fewer model failures. ", "page_idx": 37}, {"type": "text", "text": "D.9 Understanding the importance of feature names ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "SMART uses the implicit context encoded in the interpretable feature names as a source of contextual information to guide hypothesis generation. For instance, in a medical dataset, features with names like age, sex, or patient covariate features provide context to guide LLM hypothesis generation. This contrasts with data-only approaches which only use the numerical data values alone and ignore the context surrounding the feature names. ", "page_idx": 37}, {"type": "table", "img_path": "d75qCZb7TX/tmp/286465d0be558723256d7f45209f185bd9e69b8b20ecf3d4c0079d2019f6c428.jpg", "table_caption": ["Table 21: Comparison hypotheses by GPT-4 and overlap w/ open-weight models "], "table_footnote": [], "page_idx": 38}, {"type": "text", "text": "", "page_idx": 38}, {"type": "text", "text": "We aim to assess the sensitivity to interpretable feature names to provide guidance on the use of SMART. First, we perform a qualitative study where we limit the data schema by hiding the feature names (such that they become uninformative) and inspect the hypotheses and justifications generated. We find that in the limited-schema case, SMART generates hypotheses based on inferences about the feature information (e.g. \"the model might fail on feature_4 if feature_4 represents gender\"). In contrast, informative names guide meaningful hypothesis generation. Such hypotheses and justifications are illustrated in Table 22. ", "page_idx": 38}, {"type": "text", "text": "Second, we evaluate whether limiting the data schema by hiding some feature names and leaving minimal external context affects detection rates of model failures. We compare two versions of SMART, original and with corrupted feature labels, in identifying data slices with high performance discrepancies from average (Fig. 10. We find that across two real-world private datasets, hiding the feature names hinders model evaluation. This highlights that feature names play an important role in finding model failures. ", "page_idx": 38}, {"type": "text", "text": "These results highlight while SMART does not rely on any additional feature descriptions, feature names play an important role in finding model failures, just as any human requires interpretable feature names to understand the data . That said, feature names (e.g. column labels such as sex, age, race etc) are present in almost all tabular datasets both in the research field and in the real world where data is stored in SQL tables with column names. ", "page_idx": 38}, {"type": "image", "img_path": "d75qCZb7TX/tmp/7b12bc6d65f53be2c4d37abd46500d37c4e8d0d73eda34f24b848b243843a2ce.jpg", "img_caption": ["Figure 10: Identifying the importance of feature names as a source of information for context-aware testing. We compare two versions of SMART on CUTRACT and SEER datasets. The first version, \u201cbase\u201d, is the normal SMART method. The second version, \u201ccorrupt\u201d, has feature names changed to uninformative ones. The goal is to identify slices with the highest performance discrepancies between the average & subgroup prediction, calculated as the difference in their respective accuracies (y-axis). We see showing feature names enables to find subgroups that have a higher performance discrepancy from baseline performance than without feature names. $\\uparrow$ is better. "], "img_footnote": [], "page_idx": 38}, {"type": "table", "img_path": "d75qCZb7TX/tmp/199bf42202f398b836deccbc2be3b27b75447dc3c79c61b1f40170a835165de0.jpg", "table_caption": ["Table 22: Example hypotheses and justifications when dataset column names are hidden. The hypotheses and justifications for features that do not encode meaningful information lose practical meaning. Context-aware testing benefits from rich feature names that can be used to guide meaningful hypothesis generation. "], "table_footnote": [], "page_idx": 39}, {"type": "text", "text": "D.10 Example of a model report ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "We show an example of a model report which is automatically generated by SMART. A model report includes a summary of key hypotheses, justifications, sources, and summary findings as a written report. A model report fully satisfies the requirements of the user. ", "page_idx": 39}, {"type": "text", "text": "In this case, the model is generated to directly match latex output requirements. It is provided in the shaded box below as a part of the US prostate cancer (SEER) [11] evaluation. ", "page_idx": 39}, {"type": "table", "img_path": "d75qCZb7TX/tmp/625a87b22e8ee18b5cc21242e79d56e3ffcc9bcb2ce8d9f581ad46a452d167af.jpg", "table_caption": ["A report on the performance of the model has been concluded. The following are the hypotheses, their justifications tested on the model with their conclusions on whether the hypothesis was supported. "], "table_footnote": ["A further analysis was run on the training and testing datasets to understand the effects of each identified slice. The following are two tables which summarize a part of this information: "], "page_idx": 39}, {"type": "table", "img_path": "d75qCZb7TX/tmp/b3cba9589cb84a7637006a9dc3701f9541d49b8dd5cea875f873f10d5587ff98.jpg", "table_caption": [], "table_footnote": [], "page_idx": 40}, {"type": "table", "img_path": "d75qCZb7TX/tmp/56f041a6d26da7136e7c39b2c7d1f584f54c6b860db1b7f18d3e7e93f2852789.jpg", "table_caption": [], "table_footnote": [], "page_idx": 40}, {"type": "text", "text": "Recommendations: ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "1. The model appears to be less reliable for older patients $'a g e>75)$ , patients with lower levels of prostate-specific antigen $(<10)$ , and those who have undergone conservative management treatment. It also shows decreased performance for patients with more than two comorbidities and those with a Gleason score of 4. However, the model is more reliable when these conditions are not met. ", "page_idx": 40}, {"type": "text", "text": "2. Before deploying the model, the end user should be aware of the following: - The model\u2019s performance may be compromised for older patients and those with multiple comorbidities. Consider additional validation or alternative models for these groups. - Patients with lower prostate-specific antigen levels and those who have undergone conservative management treatment may also experience less accurate predictions. Additional clinical insights may be needed for these cases. - Although the model shows decreased performance for patients with a Gleason score of 4, this group is relatively small, so the impact on overall model performance may be limited. However, caution should be exercised when interpreting results for these patients. ", "page_idx": 40}, {"type": "text", "text": "Remember, these recommendations are based on the training and test datasets from the UK. If deploying in a different geographical context, consider revalidating the model with local data. ", "page_idx": 40}, {"type": "text", "text": "Definitions of the metrics: ", "page_idx": 40}, {"type": "text", "text": "\u2022 Group Size: ", "page_idx": 40}, {"type": "text", "text": "$\\mathrm{group\\_size}=|\\mathrm{slice}|$   \n\u2022 Support: $\\mathrm{support}={\\frac{|\\mathrm{slice}|}{|\\mathrm{dataset}|}}$   \n\u2022 Number of Criteria: $\\mathrm{\\num\\mathrm{{\\_}c r i t e r i a=C o u n t(\"a n d\")+1}}$   \n\u2022 Outcome Difference: outcome $\\_\\mathrm{diff}=\\vert\\mathrm{avg\\_outcome\\_dataset-avg\\_outcome\\_slice}\\vert$   \n\u2022 Accuracy Difference: accuracy_diff $=$ |accuracy_dataset \u2212accuracy_slice|   \n\u2022 Odds Ratio (Outcome): ${\\mathrm{odds}}_{-}{\\mathrm{ratio}}_{-}{\\mathrm{outcome}}={\\frac{p_{1}(1-p_{1})}{p_{0}(1-p_{0})}}$   \n\u2022 Odds Ratio (Accuracy): ${\\mathrm{odds\\_ratio\\_acc}}={\\frac{p_{1}(1-p_{1})}{p_{0}(1-p_{0})}}$ (where $p_{1}$ and $p_{0}$ are accuracies in the slice and the rest of the dataset, respectively) ", "page_idx": 40}, {"type": "text", "text": "\u2022 Lift (Outcome): ", "text_level": 1, "page_idx": 41}, {"type": "equation", "text": "$$\n{\\mathrm{lift-outcome}}={\\frac{p_{1}}{p}}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "\u2022 Lift (Accuracy): ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathrm{lift\\_acc}={\\frac{p_{1}}{p}}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "(where $p_{1}$ is accuracy in the slice and $p$ is accuracy in the entire dataset) ", "page_idx": 41}, {"type": "text", "text": "\u2022 Weighted Relative Outcome: ", "page_idx": 41}, {"type": "text", "text": "weighted_relative_outcome $=$ support $\\times$ diff_outcomes ", "page_idx": 41}, {"type": "text", "text": "\u2022 Weighted Relative Accuracy: ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "weighted_relative_accuracy $=$ support $\\times$ diff_accuracy ", "page_idx": 41}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: The abstract accurately reflects the claims made in the paper. Our paper introduces the new paradigm of context-aware testing which we discuss in Sec. 3. We introduce our context-aware testing instantiation SMART in Sec. 4. We experimentally show how under a variety of different experimental conditions SMART outperforms data-only benchmarks in Sec. 5. We therefore believe the provided evidence fully supports the claims that context-aware testing is a new alternative to testing ML models relative to data-only methods. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 42}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Justification: We discuss the limitations of our work in Sec. 6. We also provide ways of addressing these limitations in Sec. 5.4 with an extended discussion on possible biases, mitigation strategies, and best practices, including how SMART provides unique opportunities to address such challenges with features such as model reports or transparent hypothesis testing and operationalization. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. ", "page_idx": 42}, {"type": "text", "text": "\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 43}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: The paper makes a claim that context can be used to guide the search for relevant and meaningful model failures via a context-guided slice sampling mechanism which uses context to prioritize likely slices. This is defined and discussed in Sec. 3.3. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can appear in either the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 43}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Justification: The paper provides the key experimental setups in the main experimental section (Sec. 5) together with additional experimental details in the appendix. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 43}, {"type": "text", "text": "", "page_idx": 44}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: We provide details about the algorithms and data in Appendix C, Sec. B and Sec. D. Code can be found at: https://github.com/pauliusrauba/SMART_Testing or https://github.com/vanderschaarlab/SMART_Testing ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 44}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: All the details on the experiments are either provided within Section 5, with full details provided in Appendix D. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 44}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: Error bars (standard deviation) are included as relevant over multiple seeds for the experiments in Section 5 and additional experiments in Appendix D. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 45}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: All the compute details on the experiments are provided in Appendix C. Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 45}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: We have read the code of ethics do not violate any of the dimensions. Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 45}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Justification: Broader impacts of testing and implications are outlined in Sec 1 and 6. We discuss potential impacts and mitigations in Sec 5.4 and Sec 6. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 46}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Justification: Not applicable \u2014 our paper presents a new method for reliable and safe ML model testing. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 46}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: Appendix C provides details and citations for all assets (data and baselines) used in the paper. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 47}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: The paper does not produce new assets such as datasets, but uses existing benchmarks/datasets. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 47}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: We do not have crowdsourcing experiments or research with humans. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 47}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 48}, {"type": "text", "text": "Justification: We do not have crowdsourcing experiments or research with humans that would need an IRB. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 48}]