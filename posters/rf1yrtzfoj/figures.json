[{"figure_path": "rF1YRtZfoJ/figures/figures_1_1.jpg", "caption": "Figure 1: Concept diagram for probabilistic finetuning of pre-trained CLIP in a CL setup: We identify four such suitable design choices for probabilistic modelling. Choice #1 [17] performs variational modelling by imposing prior on the prompt space which makes it prompt-type dependent while also interfering with the in-domain knowledge learning capability of the prompts a criterion crucial in the deployment of CL models. Choice #2 (see Sec. 3.2.1) instead imposes a prior on the outputs of the text encoder. While this makes it prompt-type agnostic (as the text features can now be derived from arbitrary prompt types), not taking the visual information into account nevertheless leads to the loss of information about cross-modal interactions between the visual and textual cues - a property essential for preventing cross-modal deviation of finetuned features in CL (see Sec. 3.2.2). Choice #3 (Ours) leverages the best of both worlds by modelling the distribution of visual-guided text features. To further refine the learned distributions of CL tasks, we finally introduce lightweight task-specific adapter modules in choice #4 that make the cross-task centroids more distinct while preserving the aforesaid properties (see Sec. 3.2.3).", "description": "This figure illustrates four different design choices for probabilistic finetuning of pre-trained CLIP for continual learning.  Each design is evaluated based on four criteria: in-domain performance, prompt-type agnosticism, cross-modal cues utilization, and distinct task representation. The authors' proposed method (Choice #3) aims to balance in-domain performance with the utilization of visual and textual information and distinct task representations by using visual-guided text features and lightweight task-specific adapter modules.", "section": "3.2 CL with probabilistic finetuning for CLIP"}, {"figure_path": "rF1YRtZfoJ/figures/figures_3_1.jpg", "caption": "Figure 2: CLAP4CLIP overview: the visual-guided attention (VGA) inference module uses the text features as query (Q), and the visual features as keys (K) and values (V) to produce visual-guided text features. The task-specific text features are fed to their respective task distribution encoders (\u03bc\u03b5, \u03c3\u03c4). The task distribution samples are then fused with the original task features prior to deriving the task logits yt. All task logits are concatenated to produce the final prediction y1:t.", "description": "This figure provides a detailed overview of the CLAP4CLIP architecture.  It shows how the visual and text features are processed through a visual-guided attention (VGA) module to generate task-specific visual-guided text features. These features are then passed through task-specific distribution encoders which learn separate distributions for each task.  Samples are drawn from these distributions and combined with the original features to create task logits, which are ultimately concatenated to make the final prediction. The figure also highlights the weight initialization and functional regularization techniques used, as well as the use of pre-trained language-aware knowledge.", "section": "3.2 CL with probabilistic finetuning for CLIP"}, {"figure_path": "rF1YRtZfoJ/figures/figures_4_1.jpg", "caption": "Figure 3: Need for Visual-guided Attention (VGA) inference module. Fig. 3a: A simple adapter is inadequate at preventing catastrophic forgetting in CL \u2013 marked by high BwT scores; Fig. 3b: VGA module encourages cross-modal alignment between the learned text features and the pre-trained visual features \u2013 marked by a decrease in average angle arccos(t, 1) between them \u2013 where otherwise the former deviates further with incremental training steps.", "description": "This figure demonstrates the necessity of the visual-guided attention (VGA) module in the CLAP4CLIP model.  Subfigure 3a shows that a simple adapter fails to prevent catastrophic forgetting in continual learning (CL), as indicated by high backward transfer (BwT) scores.  Subfigure 3b illustrates how the VGA module ensures alignment between learned text features and pre-trained visual features, thereby mitigating catastrophic forgetting.  The average angle between image and text feature vectors decreases with incremental training steps, indicating successful cross-modal alignment.", "section": "3.2 CL with probabilistic finetuning for CLIP"}, {"figure_path": "rF1YRtZfoJ/figures/figures_4_2.jpg", "caption": "Figure 3: Need for Visual-guided Attention (VGA) inference module. Fig. 3a: A simple adapter is inadequate at preventing catastrophic forgetting in CL \u2013 marked by high BwT scores; Fig. 3b: VGA module encourages cross-modal alignment between the learned text features and the pre-trained visual features \u2013 marked by a decrease in average angle arccos(t, 1) between them \u2013 where otherwise the former deviates further with incremental training steps.", "description": "This figure shows the necessity of using a visual-guided attention module in continual learning with CLIP.  Subfigure 3a demonstrates that a simple adapter is insufficient to prevent catastrophic forgetting, as indicated by high backward transfer (BwT) scores. Subfigure 3b illustrates how the visual-guided attention (VGA) module helps maintain alignment between learned text features and pre-trained visual features, preventing the text features from drifting apart during incremental training.", "section": "3.2.2 Cross-modal feature deviation in continual finetuning of CLIP"}, {"figure_path": "rF1YRtZfoJ/figures/figures_5_1.jpg", "caption": "Figure 4: Need for task-specific probabilistic adapters: Cosine distance between the centroids of class-specific latent variables produced without (left) and with (right) task-specific adapters on CIFAR100 (10 tasks, 10 classes per task).", "description": "This figure demonstrates the effect of using task-specific adapters in a continual learning setting.  The left panel shows the cosine distance between centroids of class-specific latent variables without task-specific adapters. The right panel shows the same but with task-specific adapters. The visualization clearly shows that task-specific adapters improve the separability of class centroids across different tasks, indicating a better ability to distinguish between tasks and reduce catastrophic forgetting.", "section": "3.2 CL with probabilistic finetuning for CLIP"}, {"figure_path": "rF1YRtZfoJ/figures/figures_8_1.jpg", "caption": "Figure 2: CLAP4CLIP overview: the visual-guided attention (VGA) inference module uses the text features as query (Q), and the visual features as keys (K) and values (V) to produce visual-guided text features. The task-specific text features are fed to their respective task distribution encoders (\u00b5t, \u03c3t). The task distribution samples are then fused with the original task features prior to deriving the task logits yt. All task logits are concatenated to produce the final prediction y1:t.", "description": "This figure provides a visual overview of the CLAP4CLIP architecture.  It shows how visual and textual features are processed through visual-guided attention (VGA) to create visual-guided text features.  These are then passed to task-specific encoders, which model the task distribution and generate task logits. These logits are then combined to form the final prediction, demonstrating the model's ability to handle multiple tasks in a continual learning setting.", "section": "3.2 CL with probabilistic finetuning for CLIP"}, {"figure_path": "rF1YRtZfoJ/figures/figures_9_1.jpg", "caption": "Figure 5: Parameter count comparison.", "description": "This figure compares the number of parameters (in millions) of different continual learning methods for vision-language models.  The methods compared are iCaRL, CLIP-Adapter, Continual-CLIP, L2P, DualPrompt, PROOF, CoOp, AttriCLIP, and the proposed method, CLAP4CLIP (Ours).  The figure shows that the proposed method has a relatively low number of parameters compared to some of the other methods, particularly iCaRL.", "section": "5 Out-of-the-box utilities of probabilistic finetuning"}, {"figure_path": "rF1YRtZfoJ/figures/figures_9_2.jpg", "caption": "Figure 6: Analyses of various strategies for exemplar selection with our method on CIFAR100.", "description": "This figure compares different exemplar selection strategies on CIFAR100. The strategies include entropy, iCaRL, random, energy, and variance. The bar chart shows the average and last accuracy for each strategy, indicating the performance of each method in selecting representative samples for continual learning.", "section": "4.1 Results"}, {"figure_path": "rF1YRtZfoJ/figures/figures_17_1.jpg", "caption": "Figure 2: CLAP4CLIP overview: the visual-guided attention (VGA) inference module uses the text features as query (Q), and the visual features as keys (K) and values (V) to produce visual-guided text features. The task-specific text features are fed to their respective task distribution encoders (\u00b5t, \u03c3t). The task distribution samples are then fused with the original task features prior to deriving the task logits yt. All task logits are concatenated to produce the final prediction y1:t.", "description": "This figure shows the architecture of CLAP4CLIP, a continual learning framework for vision-language models. It uses a visual-guided attention (VGA) module to combine visual and textual information, and then uses task-specific encoders to learn task-specific distributions over visual-guided text features. The output of each task-specific encoder is then combined with the original task features to produce the task logits, which are finally concatenated to produce the final prediction.", "section": "3.2 CL with probabilistic finetuning for CLIP"}, {"figure_path": "rF1YRtZfoJ/figures/figures_18_1.jpg", "caption": "Figure 2: CLAP4CLIP overview: the visual-guided attention (VGA) inference module uses the text features as query (Q), and the visual features as keys (K) and values (V) to produce visual-guided text features. The task-specific text features are fed to their respective task distribution encoders (\u03bc\u03b5, \u03c3\u03c4). The task distribution samples are then fused with the original task features prior to deriving the task logits yt. All task logits are concatenated to produce the final prediction y1:t.", "description": "The figure illustrates the architecture of CLAP4CLIP, a continual learning framework for vision-language models.  It shows how visual and textual information is processed through visual-guided attention (VGA) to generate task-specific features, which are then used for probabilistic finetuning and prediction across multiple tasks. The process combines visual features from an image encoder with text features (prompts) from a text encoder.  These features are passed through a visual-guided attention mechanism, which dynamically weights the text features based on their relevance to the visual input.  The resulting features are then processed by task-specific adapter modules and used to generate predictions.  The task-specific modules, which learn parameters specific to each task, are integrated to generate a final prediction, preventing catastrophic forgetting across tasks.", "section": "3.2 CL with probabilistic finetuning for CLIP"}, {"figure_path": "rF1YRtZfoJ/figures/figures_21_1.jpg", "caption": "Figure 3: Need for Visual-guided Attention (VGA) inference module. Fig. 3a: A simple adapter is inadequate at preventing catastrophic forgetting in CL \u2013 marked by high BwT scores; Fig. 3b: VGA module encourages cross-modal alignment between the learned text features and the pre-trained visual features \u2013 marked by a decrease in average angle arccos(t, 1) between them \u2013 where otherwise the former deviates further with incremental training steps.", "description": "This figure demonstrates the necessity of the visual-guided attention (VGA) module in the CLAP4CLIP framework.  The left sub-figure (3a) shows that a simple adapter module is insufficient to prevent catastrophic forgetting in continual learning scenarios, as evidenced by high backward transfer (BwT) scores.  The right sub-figure (3b) illustrates how the VGA module facilitates cross-modal alignment between learned text features and pre-trained visual features.  This alignment is crucial for maintaining the generalizability of the model over time, preventing the learned textual features from deviating significantly from the visual features as new tasks are added.", "section": "3.2.2 Cross-modal feature deviation in continual finetuning of CLIP"}, {"figure_path": "rF1YRtZfoJ/figures/figures_22_1.jpg", "caption": "Figure 3: Need for Visual-guided Attention (VGA) inference module. Fig. 3a: A simple adapter is inadequate at preventing catastrophic forgetting in CL \u2013 marked by high BwT scores; Fig. 3b: VGA module encourages cross-modal alignment between the learned text features and the pre-trained visual features \u2013 marked by a decrease in average angle arccos(t, 1) between them \u2013 where otherwise the former deviates further with incremental training steps.", "description": "This figure shows the necessity of using the Visual-guided Attention (VGA) module in the CLAP4CLIP model.  It demonstrates that a simple adapter is not enough to prevent catastrophic forgetting (Figure 3a), as measured by the backward transfer (BwT) score. The VGA module, however, helps maintain alignment between learned text features and pre-trained visual features (Figure 3b), thereby reducing forgetting.  The average angle between the text and visual features illustrates this alignment.", "section": "3.2 CL with probabilistic finetuning for CLIP"}, {"figure_path": "rF1YRtZfoJ/figures/figures_23_1.jpg", "caption": "Figure 3: Need for Visual-guided Attention (VGA) inference module. Fig. 3a: A simple adapter is inadequate at preventing catastrophic forgetting in CL \u2013 marked by high BwT scores; Fig. 3b: VGA module encourages cross-modal alignment between the learned text features and the pre-trained visual features \u2013 marked by a decrease in average angle arccos(t, 1) between them \u2013 where otherwise the former deviates further with incremental training steps.", "description": "This figure shows the necessity of using the Visual-guided Attention (VGA) module in the CLAP4CLIP framework.  Figure 3a demonstrates that a simple adapter alone is insufficient to prevent catastrophic forgetting in continual learning scenarios, as evidenced by the high Backward Transfer (BwT) scores.  Conversely, Figure 3b illustrates that the VGA module effectively aligns learned text features with pre-trained visual features, reducing the average angle between them. This alignment is crucial, as otherwise the text features would deviate significantly from the visual features as more incremental training steps occur.", "section": "3.2.2 Cross-modal feature deviation in continual finetuning of CLIP"}, {"figure_path": "rF1YRtZfoJ/figures/figures_23_2.jpg", "caption": "Figure 3: Need for Visual-guided Attention (VGA) inference module. Fig. 3a: A simple adapter is inadequate at preventing catastrophic forgetting in CL \u2013 marked by high BwT scores; Fig. 3b: VGA module encourages cross-modal alignment between the learned text features and the pre-trained visual features \u2013 marked by a decrease in average angle arccos(t, 1) between them \u2013 where otherwise the former deviates further with incremental training steps.", "description": "This figure demonstrates the necessity of the Visual-guided Attention (VGA) module in preventing catastrophic forgetting during continual learning.  Subfigure (a) shows that a simple adapter without VGA is insufficient for maintaining accuracy across tasks (high backward transfer (BwT) scores indicate forgetting), while subfigure (b) illustrates how VGA aligns learned text features with pre-trained visual features, preventing cross-modal feature deviation and catastrophic forgetting.", "section": "3.2.2 Cross-modal feature deviation in continual finetuning of CLIP"}, {"figure_path": "rF1YRtZfoJ/figures/figures_23_3.jpg", "caption": "Figure 3: Need for Visual-guided Attention (VGA) inference module. Fig. 3a: A simple adapter is inadequate at preventing catastrophic forgetting in CL \u2013 marked by high BwT scores; Fig. 3b: VGA module encourages cross-modal alignment between the learned text features and the pre-trained visual features \u2013 marked by a decrease in average angle arccos(t, 1) between them \u2013 where otherwise the former deviates further with incremental training steps.", "description": "This figure demonstrates the necessity of the visual-guided attention (VGA) module in the CLAP4CLIP model for continual learning.  Subfigure (a) shows that a simple adapter is insufficient to prevent catastrophic forgetting, as indicated by high backward transfer (BwT) scores. Subfigure (b) illustrates how the VGA module helps maintain alignment between learned text features and pretrained visual features, preventing the cross-modal features from deviating, thus avoiding catastrophic forgetting. The average rotation angle between the two feature sets is used to quantify the degree of alignment.", "section": "3.2.2 Cross-modal feature deviation in continual finetuning of CLIP"}, {"figure_path": "rF1YRtZfoJ/figures/figures_25_1.jpg", "caption": "Figure 8: Performance evolution of different methods. The top-1 accuracy (%) is reported upon learning of each task.", "description": "This figure shows the performance of various continual learning methods across different tasks.  The x-axis represents the task number, and the y-axis represents the top-1 accuracy. Each line corresponds to a different method, illustrating how the accuracy evolves over the course of continual learning.  This provides a visual comparison of the methods' ability to maintain performance on previous tasks as new tasks are introduced.", "section": "4.1 Results"}, {"figure_path": "rF1YRtZfoJ/figures/figures_25_2.jpg", "caption": "Figure 8: Performance evolution of different methods. The top-1 accuracy (%) is reported upon learning of each task.", "description": "This figure shows the performance of different continual learning methods across ten incremental tasks on five different datasets.  The x-axis represents the number of classes seen so far, and the y-axis represents the top-1 accuracy.  The graph illustrates how the accuracy of each method evolves as it learns new tasks. It highlights the ability of the proposed CLAP4CLIP method to maintain higher accuracy across multiple tasks compared to baseline methods, demonstrating its effectiveness in mitigating catastrophic forgetting.", "section": "4.1 Results"}, {"figure_path": "rF1YRtZfoJ/figures/figures_26_1.jpg", "caption": "Figure 8: Performance evolution of different methods. The top-1 accuracy (%) is reported upon learning of each task.", "description": "This figure shows the performance of various continual learning methods across different datasets (CIFAR100, ImageNet100, ImageNet-R, CUB200, and VTAB) as the number of classes (tasks) increases. The y-axis represents the top-1 accuracy, and the x-axis represents the number of classes. Each line represents a different continual learning method.  The figure demonstrates how the accuracy of each method changes over time as new classes are introduced. It highlights the ability of the proposed CLAP4CLIP method to maintain high accuracy even when dealing with a large number of classes.", "section": "4.1 Results"}, {"figure_path": "rF1YRtZfoJ/figures/figures_29_1.jpg", "caption": "Figure 2: CLAP4CLIP overview: the visual-guided attention (VGA) inference module uses the text features as query (Q), and the visual features as keys (K) and values (V) to produce visual-guided text features. The task-specific text features are fed to their respective task distribution encoders (\u03bc\u03b5, \u03c3\u03c4). The task distribution samples are then fused with the original task features prior to deriving the task logits yt. All task logits are concatenated to produce the final prediction y1:t.", "description": "This figure illustrates the architecture of CLAP4CLIP, a continual learning framework for vision-language models. It shows how the model uses visual-guided attention to align text and visual features, and how task-specific adapters are used to process these features for each task. The model outputs a weighted combination of task logits to produce a final prediction.", "section": "3.2 CL with probabilistic finetuning for CLIP"}, {"figure_path": "rF1YRtZfoJ/figures/figures_32_1.jpg", "caption": "Figure 2: CLAP4CLIP overview: the visual-guided attention (VGA) inference module uses the text features as query (Q), and the visual features as keys (K) and values (V) to produce visual-guided text features. The task-specific text features are fed to their respective task distribution encoders (\u03bc\u03b5, \u03c3\u03c4). The task distribution samples are then fused with the original task features prior to deriving the task logits yt. All task logits are concatenated to produce the final prediction y1:t.", "description": "This figure shows a detailed overview of the CLAP4CLIP model architecture.  The process begins with image and text encoders from pre-trained CLIP.  Visual features are used as keys and values, while text features act as queries in a visual-guided attention (VGA) module. The VGA's output visual-guided text features are then passed to task-specific distribution encoders (each encoder producing a mean and standard deviation) where samples are drawn from these distributions. These samples are combined with original text features, passed through task-specific adapter modules, and finally combined into a final prediction using a concatenation of task logits.", "section": "3.2 CL with probabilistic finetuning for CLIP"}, {"figure_path": "rF1YRtZfoJ/figures/figures_33_1.jpg", "caption": "Figure 2: CLAP4CLIP overview: the visual-guided attention (VGA) inference module uses the text features as query (Q), and the visual features as keys (K) and values (V) to produce visual-guided text features. The task-specific text features are fed to their respective task distribution encoders (\u03bc\u03b5, \u03c3\u03c4). The task distribution samples are then fused with the original task features prior to deriving the task logits yt. All task logits are concatenated to produce the final prediction y1:t.", "description": "This figure provides a high-level overview of the CLAP4CLIP architecture.  It shows how visual and textual information are processed and integrated to perform continual learning.  The visual-guided attention (VGA) module combines visual and textual features.  Task-specific adapter modules learn task-specific distributions, and the outputs are fused to make the final prediction.", "section": "3.2 CL with probabilistic finetuning for CLIP"}, {"figure_path": "rF1YRtZfoJ/figures/figures_34_1.jpg", "caption": "Figure 2: CLAP4CLIP overview: the visual-guided attention (VGA) inference module uses the text features as query (Q), and the visual features as keys (K) and values (V) to produce visual-guided text features. The task-specific text features are fed to their respective task distribution encoders (\u03bc\u03b5, \u03c3\u03c4). The task distribution samples are then fused with the original task features prior to deriving the task logits yt. All task logits are concatenated to produce the final prediction y1:t.", "description": "This figure shows the architecture of the CLAP4CLIP model. The visual-guided attention (VGA) module is a key component that combines visual and textual information to generate visual-guided text features. These features are then fed to task-specific distribution encoders, which generate task-specific logits. Finally, all task logits are combined to produce the final prediction.", "section": "3.2 CL with probabilistic finetuning for CLIP"}]