[{"figure_path": "vvpewjtnvm/tables/tables_4_1.jpg", "caption": "Table 1: Results of our method integrated with FedAvg over various levels of heterogeneity and precision. The results with moving average demonstrate that our method can match the performance of full-precision federated learning even with all numbers quantized down to 8 bits. The results in the bottom three rows indicates the without moving average, training with low precision would lead to performance degradation.", "description": "This table shows the results of the proposed low-precision federated learning method integrated with FedAvg.  The accuracy is tested under different levels of data heterogeneity (\u03b1 = 0.01, 0.04, 0.16) and quantization precision (32 bits, 8 bits, 6 bits) on four datasets (FMNIST, CIFAR10, CINIC10, CIFAR100). The results are shown with and without the moving average aggregation technique used in the server.  The table demonstrates that using 8-bit precision with moving average achieves comparable accuracy to the full precision (32-bit) model.", "section": "6.1 Results on FedAvg"}, {"figure_path": "vvpewjtnvm/tables/tables_6_1.jpg", "caption": "Table 1: Results of our method integrated with FedAvg over various levels of heterogeneity and precision. The results with moving average demonstrate that our method can match the performance of full-precision federated learning even with all numbers quantized down to 8 bits. The results in the bottom three rows indicates the without moving average, training with low precision would lead to performance degradation.", "description": "This table presents the results of the proposed low-precision federated learning method integrated with FedAvg on various datasets.  It showcases the impact of different levels of heterogeneity (\u03b1) and precision (bits) on the model's performance, comparing models trained with moving averages to those without. Notably, the results highlight the ability of the low-precision method (with moving averages) to achieve comparable performance to the full-precision (32-bit) method even at 8-bit precision, while demonstrating that the low-precision method without moving average suffers performance degradation.", "section": "6.1 Results on FedAvg"}, {"figure_path": "vvpewjtnvm/tables/tables_8_1.jpg", "caption": "Table 1: Results of our method integrated with FedAvg over various levels of heterogeneity and precision. The results with moving average demonstrate that our method can match the performance of full-precision federated learning even with all numbers quantized down to 8 bits. The results in the bottom three rows indicates the without moving average, training with low precision would lead to performance degradation.", "description": "This table presents the results of the proposed low-precision federated learning method integrated with FedAvg, showing its performance across different levels of data heterogeneity and quantization precision.  The results are compared against the full-precision version of FedAvg, highlighting the effectiveness of the low-precision approach even when using as few as 8 bits of precision. The impact of removing the moving average component of the method is also demonstrated.", "section": "6.1 Results on FedAvg"}, {"figure_path": "vvpewjtnvm/tables/tables_8_2.jpg", "caption": "Table 1: Results of our method integrated with FedAvg over various levels of heterogeneity and precision. The results with moving average demonstrate that our method can match the performance of full-precision federated learning even with all numbers quantized down to 8 bits. The results in the bottom three rows indicates the without moving average, training with low precision would lead to performance degradation.", "description": "This table presents the results of integrating the proposed low-precision federated learning method with the FedAvg algorithm.  It shows the accuracy achieved across various datasets (FashionMNIST, CIFAR10, CINIC10, CIFAR100) under different levels of data heterogeneity (\u03b1 = 0.01, 0.04, 0.16) and quantization precisions (32 bits, 8 bits, 6 bits).  The results are compared with and without the use of moving average in the server-side aggregation. The table demonstrates that low-precision local training with moving average effectively matches the accuracy of full-precision training, significantly reducing communication and computation costs.", "section": "6.1 Results on FedAvg"}, {"figure_path": "vvpewjtnvm/tables/tables_24_1.jpg", "caption": "Table 1: Results of our method integrated with FedAvg over various levels of heterogeneity and precision. The results with moving average demonstrate that our method can match the performance of full-precision federated learning even with all numbers quantized down to 8 bits. The results in the bottom three rows indicates the without moving average, training with low precision would lead to performance degradation.", "description": "This table presents the results of the proposed low-precision federated learning method integrated with FedAvg. It shows the accuracy achieved with different levels of precision (32-bit, 8-bit, 6-bit) and data heterogeneity (\u03b1 = 0.01, 0.04, 0.16). The results are compared with and without using moving average for model aggregation, demonstrating the effectiveness of the proposed approach, especially with 8-bit precision and moving average.", "section": "6.1 Results on FedAvg"}, {"figure_path": "vvpewjtnvm/tables/tables_25_1.jpg", "caption": "Table 5: HeteroFL. a1, b1, c1, d1, e1 represent the percentage of the number of model channels to the original number: 1, 0.5, 0.25, 0.125, 0.625. For example, a1-b1-c1 means that the a, b, and c models exist in a 1:1:1 ratio.", "description": "This table presents the results of experiments using the HeteroFL method.  HeteroFL uses a variable number of model channels, represented by 'Sparsity'. The table shows the accuracy achieved and the memory usage for different configurations of model channels. Each row shows the combination of model channels used, the resulting sparsity, the accuracy, and the memory usage (in MB). The results illustrate the trade-off between model size, sparsity, and accuracy.  A smaller model (higher sparsity) results in lower memory usage but may also lead to lower accuracy.", "section": "6.5 Versus Efficient FL Method"}, {"figure_path": "vvpewjtnvm/tables/tables_26_1.jpg", "caption": "Table 6: SplitMix 1/8. Set the smallest model to 1/8 of the original model, which is the d model. By setting the client's resource limit, for example, a1-b1-c1, the client's resource limit is 1, 1/2, 1/4, then 8, 4, and 2 d models can be trained in parallel respectively.", "description": "This table presents the results of experiments using the SplitMix model with a sparsity of 1/8.  It shows the accuracy and memory usage for different model configurations, illustrating the trade-off between model size and performance.  Different model configurations are tested, each with a varying number of model components trained in parallel. The results highlight how reducing the model size impacts the performance and memory requirements.", "section": "6.5 Versus Efficient FL Method"}, {"figure_path": "vvpewjtnvm/tables/tables_26_2.jpg", "caption": "Table 7: SplitMix 1/16. Set the smallest model to 1/16 of the original model, which is the e model. By setting the client's resource limit, for example, a1-b1-c1, the client's resource limit is 1, 1/2, 1/4, then 16, 8, and 4 e models can be trained in parallel respectively.", "description": "This table presents the results of using the SplitMix model compression technique with a compression ratio of 1/16 in federated learning.  It shows the accuracy and memory usage for different model configurations, where the sparsity represents the fraction of the original model's channels used.  The parallel training indicates that multiple smaller models can be trained concurrently, leveraging available resources more efficiently.", "section": "6.5 Versus Efficient FL Method"}, {"figure_path": "vvpewjtnvm/tables/tables_26_3.jpg", "caption": "Table 1: Results of our method integrated with FedAvg over various levels of heterogeneity and precision. The results with moving average demonstrate that our method can match the performance of full-precision federated learning even with all numbers quantized down to 8 bits. The results in the bottom three rows indicates the without moving average, training with low precision would lead to performance degradation.", "description": "This table presents the results of the proposed low-precision federated learning method integrated with FedAvg.  It shows the accuracy achieved at different levels of precision (32, 8, and 6 bits) and data heterogeneity (represented by \u03b1), both with and without moving averaging in the server aggregation step. The results highlight that the low-precision approach with moving averaging maintains comparable performance to the full-precision approach even at 8 bits of precision, demonstrating its efficiency in computation and communication.", "section": "6.1 Results on FedAvg"}]