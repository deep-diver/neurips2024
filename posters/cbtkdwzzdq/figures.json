[{"figure_path": "CbtkDWZzDq/figures/figures_1_1.jpg", "caption": "Figure 1: Concepts of low precision ensembling. It shows a two-dimensional schematic, where the x and y axes represent the neural network weights, while the contours above visualize the loss surface. (a) Let the pre-trained weights, denoted by a yellow star-shaped marker (\u2606), be positioned within a basin on the loss landscape. In general, (b) post-training quantization methods introduce lower precision number systems, and then (c) choose one candidate from the system, such as the nearest one. (d) However, there are many other highly effective models available, that can contribute to ensemble predictions.", "description": "This figure illustrates the concept of low-precision ensembling.  Panel (a) shows a loss landscape with a pre-trained model (yellow star). Panel (b) shows how quantization creates a discrete set of possible weights. Panel (c) demonstrates that traditional quantization selects the closest weight. Finally, Panel (d) highlights that low-precision ensembling leverages many high-performing models near the optimal weight, leading to improved performance.", "section": "1 Introduction"}, {"figure_path": "CbtkDWZzDq/figures/figures_4_1.jpg", "caption": "Figure 2: Comparing low precision ensembling to Bayesian methods. Negative log-likelihood for Bayesian model averaging using an approximate Gaussian posterior derived from SWAG or IVON (BMA, shown in orange) and low precision ensembling with Bernoulli stochastic rounding centered around the MAP solution obtained by each optimizer (LPE-BSR, shown in green).", "description": "The figure compares the performance of low precision ensembling (LPE-BSR) with Bayesian model averaging (BMA) using SWAG or IVON.  It shows negative log-likelihood (NLL) plotted against ensemble size for three different optimization methods (SGD, SWAG, IVON).  LPE-BSR consistently achieves comparable or better performance than BMA, demonstrating the effectiveness of the proposed low-precision method.", "section": "4.2 Comparative study to Bayesian methods"}, {"figure_path": "CbtkDWZzDq/figures/figures_5_1.jpg", "caption": "Figure 3: Comparison between IVON and LPE-BSR samples. Radial landscape plots visualize a plane subspace defined by three points: the MAP obtained by IVON (depicted as a yellow star \u2606), samples in BMA and LPE-BSR procedures (represented by blue and red circle markers \u2218).", "description": "This figure compares the performance of the proposed Low Precision Ensembling with Bernoulli Stochastic Rounding (LPE-BSR) method to two Bayesian methods: Improved Variational Online Newton (IVON) and Stochastic Weight Averaging Gaussian (SWAG).  The plots show the negative log-likelihood (NLL) for Bayesian model averaging (BMA) using IVON or SWAG and LPE-BSR.  The results demonstrate that LPE-BSR, despite working within the constraints of a discrete low-precision space, achieves comparable performance to the Bayesian methods, which have access to the full continuous weight space. This suggests LPE-BSR can effectively leverage quantization errors to enhance ensemble diversity.", "section": "4.2 Comparative study to Bayesian methods"}, {"figure_path": "CbtkDWZzDq/figures/figures_6_1.jpg", "caption": "Figure 4: Comparison between snapshot and LPE-BSR samples. Radial landscape plots visualize a plane subspace defined by three points: the first and second snapshot samples obtained by SSE (represented by yellow and blue star-shaped marker \u2606), and LPE-BSR sample derived from the first snapshot (depicted as a red circle \u2218).", "description": "This figure shows a comparison of snapshot ensemble and low precision ensemble methods. It uses radial landscape plots to visualize a two-dimensional subspace of the model's weight space. Three points are highlighted: the first and second snapshot samples from stochastic gradient Langevin dynamics (SSE), and a sample from low precision ensembling with Bernoulli stochastic rounding (LPE-BSR).  The plots illustrate the negative log-likelihood (loss), and differences between the samples to show their diversity and location on the loss landscape.  LPE-BSR produces a sample distinct from the SSE snapshots, suggesting that LPE-BSR can contribute diverse models to an ensemble even when starting from the same model.", "section": "4.4 Combining with fast ensembling methods"}, {"figure_path": "CbtkDWZzDq/figures/figures_7_1.jpg", "caption": "Figure 5: Combining with fast ensembling methods. Negative log-likelihood and expected calibration error for fast ensembling methods, SSE and CSGLD, in terms of training budgets, i.e., the number of backward passes, and memory budgets, i.e., the total number of bits for representing ensemble. Top: Results with SSE. Bottom: Results with CSGLD.", "description": "This figure compares the performance of two fast ensembling methods (SSE and CSGLD) with and without the addition of LPE-BSR.  The results are shown in terms of both training budgets (number of backward passes) and memory budgets (total number of bits to represent the ensemble). The top row displays results using SSE, while the bottom row shows results using CSGLD.  The graphs illustrate how the addition of LPE-BSR impacts both negative log-likelihood (NLL) and expected calibration error (ECE) for various training and memory budget levels.", "section": "4.4 Combining with fast ensembling methods"}, {"figure_path": "CbtkDWZzDq/figures/figures_7_2.jpg", "caption": "Figure 3: Comparison between IVON and LPE-BSR samples. Radial landscape plots visualize a plane subspace defined by three points: the MAP obtained by IVON (depicted as a yellow star \u2606), samples in BMA and LPE-BSR procedures (represented by blue and red circle markers \u0970).", "description": "This figure compares the performance of three different methods: IVON (Improved Variational Online Newton), BMA (Bayesian Model Averaging), and LPE-BSR (Low Precision Ensembling with Bernoulli Stochastic Rounding). It uses radial landscape plots to visualize a 2D subspace of the model's weight space.  The plots show how samples generated by each method are distributed around the MAP (Maximum A Posteriori) solution found by IVON.  The purpose is to illustrate that LPE-BSR, despite working within a low-precision number system, is able to identify diverse ensemble members comparable to those produced by more computationally intensive Bayesian methods.", "section": "4.2 Comparative study to Bayesian methods"}, {"figure_path": "CbtkDWZzDq/figures/figures_8_1.jpg", "caption": "Figure 7: Constructing low precision ensemble of large models. Negative log-likelihood for pre-trained models (Pre-trained, shown in blue) and low precision ensembling with Bernoulli stochastic rounding centered around the pre-trained model (LPE-BSR, shown in green). The evaluation was conducted on ImageNet for CLIP models and on MMLU for LLaMa-3 in a zero-shot setting. Top: When the x-axis represents the ensemble size. Bottom: When the x-axis represents memory budgets, i.e., the total number of bits for representing ensemble.", "description": "This figure shows the results of constructing low precision ensembles of three large models: CLIP-ViT-L/14 (304M), CLIP-ViT-G/14 (1B), and LLaMa-3 (8B).  The top row displays the negative log-likelihood (NLL) as a function of ensemble size for both pre-trained models and those created using the low-precision ensembling with Bernoulli stochastic rounding (LPE-BSR) method. The bottom row shows the same data, but this time plotted against memory budget (total bits used to represent the ensemble).  The results demonstrate that LPE-BSR consistently improves upon the performance of pre-trained models, particularly as the size of the models increases.", "section": "4.4 Combining with fast ensembling methods"}, {"figure_path": "CbtkDWZzDq/figures/figures_15_1.jpg", "caption": "Figure 8: Comparing low precision ensembling to Bayesian methods. Negative log-likelihood for Bayesian model averaging using an approximate Gaussian posterior derived from SWAG or IVON (BMA, shown in orange) and low precision ensembling with Bernoulli stochastic rounding centered around the MAP solution obtained by each optimizer (LPE-BSR, shown in green).", "description": "This figure compares the performance of low precision ensembling (LPE-BSR) with Bayesian model averaging (BMA) using SWAG or IVON methods.  It shows the negative log-likelihood for each method as a function of memory budget. The LPE-BSR method is shown to be competitive with the Bayesian methods, especially when using SGD optimizer, indicating the effectiveness of LPE-BSR in achieving low memory and high accuracy.", "section": "4.2 Comparative study to Bayesian methods"}, {"figure_path": "CbtkDWZzDq/figures/figures_16_1.jpg", "caption": "Figure 9: Combining with ensembling methods. Negative log-likelihood and expected calibration error for ensembling methods, SSE, DE, and MultiIVON, in terms of training budgets, i.e., the number of backward passes, and memory budgets, i.e., the total number of bits for representing ensemble. Here, DE represents an ensemble of multiple Adam solutions, while MultiIVON represents an ensemble of multiple IVON solutions.", "description": "This figure compares the performance of several ensembling methods (SSE, DE, MultiIVON) with and without the addition of low-precision ensembling (LPE-BSR).  It shows negative log-likelihood (NLL) and expected calibration error (ECE) as functions of both training budgets (number of backward passes) and memory budgets (total number of bits used for the ensemble).  The results illustrate the impact of incorporating LPE-BSR on the efficiency and performance of the different ensembling techniques. The plots demonstrate that combining methods often improves the performance compared to using only one of the methods, even with increased memory cost.", "section": "4.4 Combining with fast ensembling methods"}]