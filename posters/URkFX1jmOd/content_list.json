[{"type": "text", "text": "Night-to-Day Translation via Illumination Degradation Disentanglement ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Night-to-Day translation (Night2Day) aims to achieve day-like vision for nighttime   \n2 scenes. However, processing night images with complex degradations remains a   \n3 significant challenge under unpaired conditions. Previous methods that uniformly   \n4 mitigate these degradations have proven inadequate in simultaneously restoring   \n5 daytime domain information and preserving underlying semantics. In this paper,   \n6 we propose N2D3 (Night-to-Day via Degradation Disentanglement) to identify   \n7 different degradation patterns in nighttime images. Specifically, our method com  \n8 prises a degradation disentanglement module and a degradation-aware contrastive   \n9 learning module. Firstly, we extract physical priors from a photometric model   \n10 based on Kubelka-Munk theory. Then, guided by these physical priors, we design a   \n1 disentanglement module to discriminate among different illumination degradation   \n12 regions. Finally, we introduce the degradation-aware contrastive learning strategy   \n13 to preserve semantic consistency across distinct degradation regions. Our method   \n14 is evaluated on two public datasets, demonstrating a significant improvement of   \n15 5.4 FID on BDD100K and 10.3 FID on Alderley. ", "page_idx": 0}, {"type": "text", "text": "16 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "17 Nighttime images often suffer from severe information loss, posing significant challenges to both   \n18 human visual recognition and computer vision tasks including detection, segmentation, etc. [14].   \n19 In contrast, daylight images exhibit rich content and intricate details. Achieving day-like nighttime   \n20 vision remains a primary objective in nighttime perception, sparking numerous pioneering works [30].   \n21 Night-to-Day image translation (Night2Day) offers a comprehensive solution to achieve day-like   \n22 vision at night. The primary goal is to transform images from nighttime to daytime while maintaining   \n23 their underlying semantic structure. However, achieving this goal is challenging. It requires to process   \n24 complex degraded images using unpaired data, which raises additional difficulties compared to other   \n25 image translation tasks.   \n26 Recently, explorations have been made in Night2Day. Early approaches, such as ToDayGAN,   \n27 demonstrated the effectiveness of cycle-consistent learning in maintaining semantic structure [1].   \n28 Subsequent methods incorporated auxiliary structure regularization techniques, including perceptual   \n29 loss and uncertainty regularization, to better preserve the original structure [33, 18]. Furthermore,   \n30 some methods utilized daytime images with nearby GPS locations to aid in coarse structure regular  \n31 ization [26]. However, these methods often neglect the complex degradations at nighttime, applying   \n32 structure regularization uniformly and resulting in severe artifacts. To address this issue, more recent   \n33 approaches adopt auxiliary human annotations to maintain semantic consistency, such as segmenta  \n34 tion maps and bounding boxes [16, 22]. Despite their potential, these methods are labor-intensive   \n35 and challenging, especially since many nighttime scenes are beyond human cognition.   \n36 The critical limitation of the aforementioned methods is the disregard for complex degraded regions.   \n37 Specifically, different regions in nighttime images possess varying characteristics, such as extreme   \n38 darkness, well-lit regions, light effects, etc. Treating all these degraded regions equally could adversely   \n39 impact the results. As illustrated in Figure 1, our key insight emphasizes that nighttime images suffer   \n40 from various degradations, necessitating customizing restoration for different degradation types.   \n41 Intuitively, we manage to disentangle nighttime images into patches according to the recognized   \n42 degradation type and learn individual restoration patterns for them to enhance the overall performance.   \n43 Motivated by this point, we propose N2D3 (Night to Day via Degradation Disentanglement), which   \n44 utilizes Generative Adversarial Networks (GANs) to bridge the domain gap between nighttime and   \n45 daytime in a degradation-aware manner, as illustrated in Figure 2. There are two modules in N2D3,   \n46 including physical-informed degradation disentanglement and degradation-aware contrastive learning,   \n47 which are employed to preserve the semantic structure of nighttime images. In the disentanglement   \n48 of nighttime degradation, a photometric model tailored to nighttime scenes is conducted to extract   \n49 physical priors. Subsequently, the illuminance and physical priors are integrated to disentangle   \n50 regions into darkness, well-lit, high-light, and light effects. Building on this, degradation-aware   \n51 contrastive learning is designed to constrain the similarity of the source and generated images in   \n52 different regions. It comprises disentanglement-guided sampling and reweighting strategies. The   \n53 sampling strategy mines valuable anchors and hard negative examples, while the reweighting process   \n54 assigns their weights. They enhance vanilla contrastive learning by prioritizing valuable patches with   \n55 appropriate attention. Ultimately, our method yields highly faithful results that are visually pleasing   \n56 and beneficial for downstream vision tasks including keypoint matching and semantic segmentation. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "image", "img_path": "URkFX1jmOd/tmp/3500ff7d2927be59b55e65a7738aac8e3c1decaa7df6c8528ba20b80045f354d.jpg", "img_caption": ["Figure 1: Illustration of our motivation. (a) The disentanglement process leverages physical priors. (b) The image patches are restored individually for each degradation type. (c) The proposed Disentangled Regularization improves the overall performance. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "57 Our contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "58 (1) We propose the N2D3 translation method based on the illumination degradation disentanglement   \n59 module, which enables degradation-aware restoration of nighttime images.   \n60 (2) We present a novel degradation-aware contrastive learning module to preserve the semantic   \n61 structure of generated results. The core design incorporates disentanglement-guided sampling and   \n62 reweighting strategies, which greatly enhance the performance of vanilla contrastive learning.   \n63 (3) Experimental results on two public datasets underscore the significance of considering distinct   \n64 degradation types in nighttime scenes. Our method achieves state-of-the-art performance in visual   \n65 effects and downstream tasks. ", "page_idx": 1}, {"type": "text", "text": "66 2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "67 Unpaired Image-to-Image Translation. Unpaired image-to-image translation addresses the chal  \n68 lenge of lacking paired data, providing an effective self-supervised learning strategy. To overcome the   \n69 efficiency limitations of traditional cycle-consistency learning, Park et al., first introduces contrastive   \n70 learning to this domain, achieving efficient one-sided learning[20]. Following this work, several stud  \n71 ies have improved the contrastive learning by generating hard negative examples [24], re-weighting   \n72 positive-negative pairs [31], and selecting key samples [9]. Furthermore, other constraints, such as   \n73 density [27] and path length [28], have been explored in unpaired image translation. However, all   \n74 these works neglect physical priors in the nighttime, leading to suboptimal results in Night2Day.   \n75 Nighttime Domain Translation. Domain translation techniques have been applied to address adverse   \n76 nighttime conditions. An early contribution is made by Anoosheh et al., which demonstrates the   \n77 effectiveness of cycle-consistent learning in Night2Day[1]. Following this, many works incorporate   \n78 different modules into cycle-consistent learning to enhance structural modeling capabilities. Zheng et   \n79 al. incorporate a fork-shaped encoder to enhance visual perceptual quality[33]. AUGAN employs   \n80 uncertainty estimation to mine useful features in nighttime images[18]. Fan et al. explore inter  \n81 frequency relation knowledge to streamline the Night2Day process[5]. Xia et al. utilize nearby GPS   \n82 locations to form paired night and daytime images, providing weak supervision[26]. Some other   \n83 studies incorporate human annotations to impose structural constraints, overlooking the practical   \n84 difficulty of acquiring such annotations at nighttime with multiple degradations [11][16] [22]. To   \n85 address the concerns of the aforementioned methods, the proposed N2D3 explores patch-wise   \n86 contrastive learning with physical guidance, so as to achieve degradation-aware Night2Day. N2D3 is   \n87 free of human annotations and offers comprehensive structural modeling to provide faithful translation   \n88 results. ", "page_idx": 1}, {"type": "image", "img_path": "URkFX1jmOd/tmp/267dc6fe420aad6374de3ee0e6cd4c51a1e49d04d3ae7864cfc89ee76f9b20e4.jpg", "img_caption": ["Figure 2: The overall architecture of the proposed N2D3 method. The training phase contains the physical prior informed degradation disentanglement module and degradation-aware contrastive learning module. They are utilized to optimize the ResNet-based generator which is the main part in the inference phase. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "89 3 Methods ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "90 Given nighttime image $\\mathbf{I}_{{\\mathcal{N}}}\\in{\\mathcal{N}}$ and daytime image $\\mathbf{I}_{\\mathcal{D}}\\in\\mathcal{D}$ , the goal of Night2Day is to translate   \n91 images from nighttime to daytime while preserving content semantic consistency. This involves the   \n92 construction of a mapping function $\\mathcal{F}$ with parameters $\\theta$ , which can be formulated as $\\mathcal{F}_{\\theta}:\\mathbf{I}_{\\mathcal{N}}\\rightarrow\\mathbf{I}_{\\mathcal{D}}$ .   \n93 Our method N2D3 is illustrated in Figure 2. To train a generator for Night2Day, we employ GANs as   \n94 the overall learning framework to bridge the domain gap between nighttime and daytime. Our core   \n95 design, consisting of the degradation disentanglement module and the degradation-aware contrastive   \n96 learning module, aims to preserve the structure from the source images and suppress artifacts.   \n97 In this section, we first introduce physical priors in the nighttime environment, and then describe   \n98 the degradation disentanglement module and the degradation-aware contrastive learning module,   \n99 respectively. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "100 3.1 Physical Priors for Nighttime Environment ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "101 The illumination degradations at night are primarily categorized as darkness, well-lit regions, high  \n102 light regions, and light effects. As shown in Figure 3, well-lit represents the diffused reflectance under   \n103 normal light, while the light effects denote phenomena such as flare, glow, and specular reflections.   \n104 Intuitively, these regions can be disentangled through the analysis of illumination distribution. Among   \n105 these degradation types, darkness and high-light are directly correlated with illuminance and can be   \n106 effectively disentangled through illumination estimation.   \n107 As a common practice, we estimate the illuminance map $L$ by utilizing the maximum RGB channel   \n108 of image $\\mathbf{I}_{\\mathcal{N}}$ as $L=\\operatorname*{max}_{c\\in R,G,B}\\mathbf{I}_{\\mathcal{N}}^{c}$ . Then $\\mathbf{k}$ -nearest neighbors [4] is employed to acquire three   \n109 clusters representing darkness, well-lit, and high-light regions. These clusters are aggregated as   \n110 masks $M_{d}$ , $M_{n}$ , $M_{h}$ . However, the challenge arises with light effects that are mainly related to   \n111 the illumination. Light effects regions tend to intertwine with well-lit regions when using only the   \n112 illumination map, as they often share similar illumination densities. To disentangle light effects from   \n113 well-lit regions, we need to introduce additional physical priors.   \nTo extract the physical priors for disentangling light effects, we develop a photometric model derived   \n115 from Kubelka-Munk theory [17]. This model characterizes the spectrum of light $E$ reflected from an   \n116 object as follows: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "image", "img_path": "URkFX1jmOd/tmp/54a26c0d6461a24f41d5f79c9c99b94b31e1a737f7339e4b1a1c8af05b328c11.jpg", "img_caption": ["Figure 3: The first row displays nighttime images, while the second row shows the corresponding degradation disentanglement results. The color progression from blue, light blue, green to yellow corresponds to the following regions: darkness, well-lit, light effects, and high-light, respectively. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nE(\\lambda,x)=e(\\lambda,x)(1-\\rho_{f}(x))^{2}R_{\\infty}(\\lambda,x)+e(\\lambda,x)\\rho_{f}(x),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "117 here $x$ represents the horizontal component for analysis, while the analysis of the vertical component   \n118 $y$ is the same as the horizontal component. $\\lambda$ corresponds to the wavelength of light. $e(\\lambda,x)$ signifies   \n119 the spectrum, representing the illumination density and color. $\\rho_{f}$ stands for the Fresnel reflectance   \n120 coefficient. $R_{\\infty}$ is the material reflectivity function, formulated as follows at a specific location   \n121 $x=x_{0}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\nR(\\lambda)=a(\\lambda)-\\sqrt{a(\\lambda)^{2}-1},a(\\lambda)=1+\\frac{k(\\lambda)}{s(\\lambda)},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "122 where $k(\\lambda)$ and $s(\\lambda)$ denote the absorption and scattering coefficients, respectively. This formulation   \n123 implies that for any local pixels, the material reflectivity is determined if the material is given.   \n124 Assuming $C$ is the material distribution function, which describes the material type varying across   \n125 locations, the material reflectivity $R_{\\infty}$ can be formulated as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{R_{\\infty}(\\lambda,x)=R(\\lambda)C(x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "126 Since the mixture of light effects and well-lit regions has been obtained previously, the core of   \n127 disentangling light effects from well-lit regions lies in separating the illumination $e(\\lambda,x)$ and re  \n128 flectance components $R(\\lambda)C(x)$ . Note that the Fresnel reflectance coefficient $\\rho_{f}({\\boldsymbol{x}})$ approaches 0 in   \n129 reflectance-dominating well-lit regions, while $\\rho_{f}({\\boldsymbol{x}})$ approaches 1 in illumination-dominating light   \n130 effects regions. According to Equation (1), the photometric model for the mixture of light effects and   \n131 well-lit regions is formulated as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nE(\\lambda,x)={\\biggl\\{}\\atop{e(\\lambda,x)R(\\lambda)C(x),}~~{\\mathrm{if}}~x\\not\\in\\Omega~,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "132 where $\\Omega$ denotes the reflectance-dominating well-lit regions. ", "page_idx": 3}, {"type": "text", "text": "133 Subsequently, we observe that the following color invariant response to the regions with high color   \n134 saturation, which is suitable to extract the illumination: ", "page_idx": 3}, {"type": "equation", "text": "$$\nN_{\\lambda^{m}x^{n}}=\\frac{\\partial^{m+n-1}}{\\partial\\lambda^{m-1}\\partial x^{n}}\\lbrace\\frac{1}{E(\\lambda,x)}\\frac{\\partial E(\\lambda,x)}{\\partial\\lambda}\\rbrace,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "135 This invariant has the following characteristics: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{N_{\\lambda^{m}x^{n}}=\\cfrac{\\partial^{m+n-2}}{\\partial\\lambda^{m-1}\\partial x^{n-1}}\\cfrac{\\partial}{\\partial x}\\left\\{\\cfrac{1}{E(\\lambda,x)}\\cfrac{\\partial E(\\lambda,x)}{\\partial\\lambda}\\right\\}}\\\\ &{\\qquad=\\cfrac{\\partial^{m+n-2}}{\\partial\\lambda^{m-1}\\partial x^{n-1}}\\cfrac{\\partial}{\\partial x}\\left\\{\\cfrac{1}{e(\\lambda,x)}\\cfrac{\\partial e(\\lambda,x)}{\\partial\\lambda}+\\cfrac{1}{R(\\lambda)C(x)}\\cfrac{\\partial R(\\lambda)C(x)}{\\partial\\lambda}\\right\\}}\\\\ &{\\qquad=\\cfrac{\\partial^{m+n-1}}{\\partial\\lambda^{m-1}\\partial x^{n}}\\left\\{\\cfrac{1}{e(\\lambda,x)}\\cfrac{\\partial e(\\lambda,x)}{\\partial\\lambda}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "136 Equation (5) to Equation (6) demonstrate that the invariant $N_{\\lambda^{m}x^{n}}$ captures the features only related   \n137 to illumination $e(\\lambda,x)$ . Consequently, we assert that $N_{\\lambda^{m}x^{n}}$ functions as a light effects detector   \n138 because light effects are mainly related to the illumination. It allows us to design the illumination   \n139 disentanglement module based on this physical prior. ", "page_idx": 4}, {"type": "text", "text": "140 3.2 Degradation Disentanglement Module ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "141 In this subsection, we will elucidate how to incorporate the invariant for extracting light effects into   \n142 the disentanglement in computation. As common practice, the following second and third-order   \n143 components, both horizontally and vertically, are taken into account in the practical calculation of the   \n144 final invariant, which is denoted as $N$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\nN=\\sqrt{N_{\\lambda x}^{2}+N_{\\lambda\\lambda x}^{2}+N_{\\lambda y}^{2}+N_{\\lambda\\lambda y}^{2}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "145 here $N_{\\lambda x}$ and $N_{\\lambda\\lambda x}$ can be computed through $E(\\lambda,x)$ by simplifying Equation (5). The calculation   \n146 of $N_{\\lambda y}$ and $N_{\\lambda\\lambda y}$ are the same. Specifically, ", "page_idx": 4}, {"type": "equation", "text": "$$\nN_{\\lambda x}=\\frac{E_{\\lambda x}E-E_{\\lambda}E_{x}}{E^{2}},N_{\\lambda\\lambda x}}&{=\\frac{E_{\\lambda\\lambda x}E^{2}-E_{\\lambda\\lambda}E_{x}E-2E_{\\lambda x}E_{\\lambda}E+2E_{\\lambda}^{2}E_{x}}{E^{3}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "147 where $E_{x}$ and $E_{\\lambda}$ denote the partial derivatives of $x$ and $\\lambda$ . ", "page_idx": 4}, {"type": "text", "text": "148 To compute each component in the invariant $N$ , we develop a computation scheme starting with the   \n149 estimation of $E$ and its partial derivatives $E_{\\lambda}$ and $E_{\\lambda\\lambda}$ using the Gaussian color model: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left[\\begin{array}{c}{E(x,y)}\\\\ {E_{\\lambda}(x,y)}\\\\ {E_{\\lambda\\lambda}(x,y)}\\end{array}\\right]=\\left[\\begin{array}{c c c}{0.06,}&{0.63,}&{0.27}\\\\ {0.3,}&{0.04,}&{-0.35}\\\\ {0.34,}&{-0.6,}&{0.17}\\end{array}\\right]\\left[\\begin{array}{c}{R(x,y)}\\\\ {G(x,y)}\\\\ {B(x,y)}\\end{array}\\right],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "150 where $x,y$ are pixel locations of the image. Then, the spatial derivatives $E_{x}$ and $E_{y}$ are calculated by   \n151 convolving $E$ with Gaussian derivative kernel $g$ and standard deviation $\\sigma$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\nE_{x}(x,y,\\sigma)=\\sum_{t\\in\\mathbf{Z}}E(t,y)\\frac{\\partial g(x-t,\\sigma)}{\\partial x},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "152 where $t$ denotes the index of the horizontal component $x$ and $\\mathbf{Z}$ represents set of integers. The spatial   \n153 derivatives for $E_{\\lambda x}$ and $E_{\\lambda\\lambda x}$ are obtained by applying Equation (10) to $E_{\\lambda}$ and $E_{\\lambda\\lambda}$ . Then invariant   \n154 $N$ can be obtained following Equation (8) and Equation (7).   \n155 To extract the light effects, ReLU and normalization functions are first applied to filter out minor   \n156 disturbances. Then, by filtering invariant $N$ with the well-lit mask $M_{n}$ , we obtain the light effects   \n157 from the well-lit regions. The operations above can be formulated as: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\nM_{l e}=\\mathrm{ReLU}(\\frac{N-\\mu(N)}{\\sigma(N)})\\odot M_{n},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "158 while the well-lit mask are refined: $M_{n}\\leftarrow M_{n}-M_{l e}$ . ", "page_idx": 4}, {"type": "text", "text": "159 With the initial disentanglement in Section 3.1, we obtain the final disentanglement: $M_{d},\\,M_{n},\\,M_{h}$   \n160 and $M_{l e}$ . All the masks are stacked to obtain the disentanglement map. Through the employment of   \n161 the aforementioned techniques and processes, we successfully achieve the disentanglement of various   \n162 degradation regions. ", "page_idx": 4}, {"type": "text", "text": "163 3.3 Degradation-Aware Contrastive Learning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "164 For unpaired image translation, contrastive learning has validated its effectiveness for the preservation   \n165 of content. It targets to maximize the mutual information between patches in the same spatial location   \n166 from the generated image and the source image as below: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\ell(v,v^{+},v^{-})=-\\log{\\frac{\\exp(v\\cdot v^{+}/\\tau)}{\\exp(v\\cdot v^{+}/\\tau)+\\sum_{n=1}^{Q}\\exp(v\\cdot v_{n}^{-}/\\tau)}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "167 $v$ is the anchor that denotes the patch from the generated image. The positive example $v^{+}$ corresponds   \n168 to the source image patch with the same location as the anchor $v$ . The negative examples $v^{-}$ represent   \n169 patches with locations distinct from that of the anchor $v$ . $Q$ denotes the total number of negative   \n170 examples. In our work, the key insight of degradation-aware contrastive learning lies in two folds: (1)   \n171 How to sample the anchor, positive, and negative examples. (2) How to manage the focus on different   \n172 negative examples.   \n173 Degradation-Aware Sampling. In this paper, N2D3 selects the anchor, positive, and negative patches   \n174 under the guidance of the disentanglement results. Initially, based on the disentanglement mask   \n175 obtained in the Section 3.2, we compute the patch count for different degradation types, denoting as   \n176 $K_{s},s\\in[1,4]$ . Then, within each degradation region, the anchors $v$ are randomly selected from the   \n177 patches of generated daytime images $I_{\\mathcal{N}\\rightarrow\\mathcal{D}}$ . The positive examples $v^{+}$ are sampled from the same   \n178 locations with the anchors in the source nighttime images $I_{\\mathcal{N}}$ , and the negative examples $v^{-}$ are   \n179 randomly selected from other locations of $I_{\\mathcal{N}}$ . For each anchor, there is one corresponding positive   \n180 example and $K_{s}$ negative examples. Subsequently, the sample set with the same degradation type   \n181 will be assigned weights and the contrastive loss will be computed in the following steps.   \n182 Degradation-Aware Reweighting. Despite the careful selection of anchor, positive, and negative   \n183 examples, the importance of anchor-negative pairs still differs within the same degradation. A known   \n184 principle of designing contrastive learning is that the hard anchor-negative pairs (i.e., the pairs with   \n185 high similarity) should assign higher attention. Thus, weighted contrastive learning can be formulated   \n186 as: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\ell(v,v^{+},v^{-},w_{n})=-\\log{\\frac{\\exp(v\\cdot v^{+}/\\tau)}{\\exp(v\\cdot v^{+}/\\tau)+\\sum_{n=1}^{Q}w_{n}\\exp(v\\cdot v_{n}^{-}/\\tau)}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "187 $w_{n}$ denotes the weight of the $n$ -th anchor-negative pairs. ", "page_idx": 5}, {"type": "text", "text": "188 The contrastive objective is depicted in the Similarity Matrix in Figure 2. The patches in different   \n189 regions are obviously easy examples. We suppress their weights to 0, which transforms the similarity   \n190 matrix into a blocked diagonal matrix with $d i a g(A_{1},\\ldots,A_{4})$ . Within each degradation matrix   \n191 $A_{s},s\\in[1,4]$ , a soft reweighting strategy is implemented. Specifically, for each anchor-negative   \n192 pair, we apply optimal transport to yield an optimal transport plan, serving as a reweighting matrix   \n193 associated with the disentangled results. It can adaptively optimize and avoid manual design. The   \n194 reweight matrix for each degradation type is formulated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{min}_{w_{i j},i,j\\in[1,K_{s}]}[\\sum_{i=1}^{K_{s}}\\sum_{j=1,i\\neq j}^{K_{s}}w_{i j}\\cdot\\exp{(v_{i}\\cdot v_{j}^{-}/\\tau)}],}\\\\ &{\\displaystyle\\sum_{i=1}^{K_{s}}w_{i j}=1,\\sum_{j=1}^{K_{s}}w_{i j}=1,i,j\\in[1,K_{s}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "195 The aforementioned operations transform the contrastive objective to the Block Diagonal Similarity   \n196 Matrix depicted in Figure 2. As a common practice, our degradation-aware contrastive loss is applied   \n197 to the $S$ layers of the CNN feature extractor, formulated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{D e g N C E}(\\mathcal{F})=\\sum_{l=1}^{S}\\ell(v,v^{+},v^{-},w_{n}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "198 3.4 Other Regularizations ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "199 As a common practice, GANs are employed to bridge the domain gap between daytime and nighttime.   \n200 The adversarial loss is formulated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{a d v}(\\mathcal{F})=||D(\\mathbf{I}_{N\\rightarrow\\mathcal{D}})-1||_{2}^{2},}\\\\ &{\\mathcal{L}_{a d v}(D)=||D(\\mathbf{I}_{\\mathcal{D}})-1||_{2}^{2}+||D(\\mathbf{I}_{N\\rightarrow\\mathcal{D}})||_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "201 where $D$ denotes the discriminator network. The final loss function is formatted as : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}(\\mathcal{F})=\\mathcal{L}_{a d v}(\\mathcal{F})+\\mathcal{L}_{D e g N C E}(\\mathcal{F}),}\\\\ &{\\mathcal{L}(D)=\\mathcal{L}_{a d v}(D).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "202 4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "203 4.1 Experimental Settings ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "204 Datasets. Experiments are conducted on the two public datasets BDD100K [29] and Alderley [19].   \n205 Alderley dataset consists of images captured along the same route twice: once on a sunny day and   \n206 another time during a stormy rainy night. The nighttime images in this dataset are often blurry due to   \n207 the rainy conditions, which makes Night2Day challenging. BDD100K dataset is a large-scale high  \n208 resolution autonomous driving dataset. It comprises 100,000 video clips under various conditions.   \n209 For each video, a keyframe is selected and meticulously annotated with details. We reorganized this   \n210 dataset based on its annotations, resulting in 27,971 night images for training and 3,929 night images   \n211 for evaluation.   \n212 Evaluation Metric. Following common practice, we utilize the Fr\u00e9chet Inception Distance (FID)   \n213 scores [7] to assess whether the generated images align with the target distribution. This assessment   \n214 helps determine if a model effectively transforms images from the night domain to the day domain.   \n215 Additionally, we seek to understand the extent to which the generated daytime images maintain   \n216 structural consistency compared to the original inputs. To measure this, we employ SIFT scores,   \n217 mIoU scores and LPIPS distance [32].   \n218 DownStream Vision Task. Two downstream tasks are conducted. In the Alderley dataset, GPS   \n219 annotations indicate the locations of two images, one in the nighttime and the other in the daytime,   \n220 as the same. We calculate the number of SIFT-detected key points between the generated daytime   \n221 images and their corresponding daytime images to measure if the two images represent the same   \n222 location. The BDD100K dataset includes 329 night images with semantic annotations. We employ   \n223 Deeplabv3 pretrained on the Cityscapes dataset as the semantic segmentation model [2], then perform   \n224 inference on our generated daytime images without any additional training and compute the mIoU   \n225 (mean Intersection over Union). ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Table 1: The quantitative results on Alderley and BDD100k. $\\downarrow$ means lower result is better. $\\uparrow$ means higher is better. ", "page_idx": 6}, {"type": "table", "img_path": "URkFX1jmOd/tmp/cbb80440ec9c6c7830692dc939168555f81e60d05ef5e7597e7d9522a80c4b79.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "226 4.2 Results on Alderley ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "227 We first apply Night2Day on the Alderley dataset, a challenging collection of nighttime images   \n228 captured on rainy nights. In Figure 4, we present a visual comparison of the results. CycleGAN [34]   \n229 and CUT [20] manage to preserve the general structural information of the entire image but often   \n230 lose many fine details. ToDayGAN [1], ForkGAN [33], Decent [27], and Santa [28] tend to miss   \n231 important elements such as cars in their results.   \n232 In Table 1, thirteen translation methods and three enhancement methods are compared, considering   \n233 both visual effects and keypoint matching metrics. Our method showcases an improvement of 10.3   \n234 in FID scores and 4.52 in SIFT scores compared to the previous state-of-the-art. This suggests that   \n235 N2D3 successfully achieves photorealistic daytime image generation, underscoring its potential for   \n236 robotic localization applications. The qualitative comparison results are demonstrated in Figure 4. In   \n237 conclusion, N2D3 achieves top scores in both FID and LPIPS metrics, demonstrating its superiority   \n238 in the Night2Day task. N2D3 excels in generating photorealistic daytime images while effectively   \n239 preserving structures, even in challenging scenarios such as rainy nights in the Alderley. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "image", "img_path": "URkFX1jmOd/tmp/0bc2fb6a162ec57b8833133333e063e4c0860c00a6879e2b210d883d0c2413c9.jpg", "img_caption": ["Figure 4: The qualitative comparison results on the Alderley dataset. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "URkFX1jmOd/tmp/0515a096c92c9e22886241ad558fa47f5a8ff5d4361bcb423949ed6d407f600a.jpg", "img_caption": ["Figure 5: The qualitative comparison results on the BDD100K dataset. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "240 4.3 Results on BDD100K ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "241 We conducted experiments on a larger-scale dataset, BDD100K, focusing on more general night   \n242 scenes. The qualitative results can be found in Figure 5. CycleGAN, ToDayGAN, and CUT succeed   \n243 in preserving the structure in well-lit regions. ForkGAN, Santa, and Decent demonstrate poor   \n244 performance in such challenging scenes. Regretfully, none of them excel in handling light effects and   \n245 exhibit weak performance in maintaining global structures. With a customized design specifically   \n246 addressing light effects, our method successfully preserves the structure in all regions.   \n247 The quantitative results are presented in Table 1. As the scale of the dataset increases, all the   \n248 compared methods show an improvement in their performance. Notably, N2D3 demonstrates the best   \n249 performance with a significant improvement of 5.4 in FID scores, showcasing its ability to handle a   \n250 broader range of nighttime scenes and establishing itself as the most advanced method in this domain.   \n251 We also investigate the potential of Night2Day in enhancing downstream vision tasks in nighttime   \n252 environments using the BDD100K dataset. The quantitative results are summarized in Table 1.   \n253 The enhancement methods demonstrate a slight improvement in segmentation results, while some   \n254 image-to-image translation methods have a negative impact on performance. N2D3 exhibits the best   \n255 performance in enhancing nighttime semantic segmentation with a remarkable improvement of   \n256 5.95 in mIoU compared to inferring the segmentation model directly on nighttime images.   \n257 In conclusion, N2D3 achieves top scores in both FID and LPIPS metrics, establishing itself as the   \n258 most advanced method for the Night2Day task. It excels in generating photorealistic daytime images   \n259 while preserving local and global structures. Moreover, the substantial improvement in nighttime   \n260 semantic segmentation highlights its beneftis for downstream tasks and its potential for wide-ranging   \n261 applications. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "URkFX1jmOd/tmp/ac580c6fba9d1e8d67300f042d93676e419ab34534342e13d4a94339e00240da.jpg", "img_caption": ["Figure 6: The quantitative results of ablation on the number of patches of the degradation-aware sampling. Table 2: The quantitative results of ablation on the main component of degradation-aware contrastive learning. (a) denotes the degradation-aware sampling, and (b) denotes the degradation-aware reweighting. $L$ and $N$ denotes the invariant types. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "URkFX1jmOd/tmp/0947b6f098afcaba5b02411dbf947e72502faa389349103b7df7a16b73f0b7ac.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "262 4.4 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "263 Ablation on the main component of degradation-aware contrastive learning. The core design of   \n264 the degradation-aware contrastive learning module relies on two main components: (a) degradation  \n265 aware sampling, and (b) degradation-aware reweighting. As shown in Table 2, when degradation  \n266 aware sampling is exclusively activated, there is a noticeable decrease in FID on both datasets   \n267 compared to the baseline (no components activated). Notably, the combination of degradation-aware   \n268 sampling and reweighting achieves the lowest FID on both BDD100K and Alderley, indicating the   \n269 effectiveness of degradation-aware sampling in conjunction with degradation-aware reweighting.   \n270 Ablation on the number of patches in the degradation-aware sampling. To explore the impact   \n271 of the number of sampling patches in our method, we conduct an ablation study on the number of   \n272 sampling patches with settings of 64, 128, 256, 512, and 1024 for degradation-aware sampling. The   \n273 FID and LPIPS scores are evaluated, as shown in Figure 6. The optimal performance is achieved with   \n274 256 patches, and increasing the number of sampling patches beyond this point leads to a degradation   \n275 in performance.   \n276 Ablation on the type of the invariant in disentanglement. To explore different invariants for   \n277 obtaining degradation-disentangled prototypes, we conduct an ablation study on the type of invariant.   \n278 As shown in Table 2, when $L$ is enabled, the FID decreases from 55.5 to 49.1 on BDD100K and   \n279 from 64.7 to 62.9 on Alderley. This suggests that incorporating illuminance maps helps in reducing   \n280 the perceptual gap between generated and source nighttime images. When $N$ is activated, there   \n281 is a consistent improvement in FID on both datasets, indicating that considering physical priors   \n282 invariant contributes to more realistic image generation. The combination of both illuminance map   \n283 and physical prior invariant results in the lowest FID on both datasets, showcasing the complementary   \n284 nature of these degradation types in improving contrastive learning. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "285 5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "286 This paper introduces a novel solution for the Night2Day image translation task, focusing on trans  \n287 lating nighttime images to their corresponding daytime counterparts while preserving semantic   \n288 consistency. To achieve this objective, the proposed method begins by disentangling the degradation   \n289 presented in nighttime images, which is the key insight of our method. To achieve this, we contribute   \n290 a degradation disentanglement module and a degradation-aware contrastive learning module. Our   \n291 method outperforms the existing state-of-the-art, which shows the effectiveness of N2D3 and the   \n292 superiority of the insight to disentangle the degradation. ", "page_idx": 8}, {"type": "text", "text": "293 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "294 [1] Asha Anoosheh, Torsten Sattler, Radu Timofte, Marc Pollefeys, and Luc Van Gool. Night-to-day   \n295 image translation for retrieval-based localization. In 2019 International Conference on Robotics   \n296 and Automation (ICRA), pages 5958\u20135964. IEEE, 2019.   \n297 [2] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous   \n298 convolution for semantic image segmentation. arXiv preprint arXiv:1706.05587, 2017.   \n299 [3] Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, and Jaegul Choo.   \n300 Stargan: Unified generative adversarial networks for multi-domain image-to-image translation.   \n301 In Proc. IEEE Conference on Computer Vision and Pattern Recognition, pages 8789\u20138797,   \n302 2018.   \n303 [4] T. Cover and P. Hart. Nearest neighbor pattern classification. IEEE Transactions on Information   \n304 Theory, 13(1):21\u201327, 1967.   \n305 [5] Zhentao Fan, Xianhao Wu, Xiang Chen, and Yufeng Li. Learning to see in nighttime driving   \n306 scenes with inter-frequency priors. In Proceedings of the IEEE/CVF Conference on Computer   \n307 Vision and Pattern Recognition, pages 4217\u20134224, 2023.   \n308 [6] Chunle Guo, Chongyi Li, Jichang Guo, Chen Change Loy, Junhui Hou, Sam Kwong, and   \n309 Runmin Cong. Zero-reference deep curve estimation for low-light image enhancement. In Proc.   \n310 IEEE Conference on Computer Vision and Pattern Recognition, pages 1780\u20131789, 2020.   \n311 [7] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.   \n312 Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in   \n313 Neural Information Processing Systems, 30, 2017.   \n314 [8] Jinhui Hou, Zhiyu Zhu, Junhui Hou, Hui Liu, Huanqiang Zeng, and Hui Yuan. Global structure  \n315 aware diffusion process for low-light image enhancement. Advances in Neural Information   \n316 Processing Systems, 36, 2024.   \n317 [9] Xueqi Hu, Xinyue Zhou, Qiusheng Huang, Zhengyi Shi, Li Sun, and Qingli Li. Qs-attn: Query  \n318 selected attention for contrastive learning in i2i translation. In Proceedings of the IEEE/CVF   \n319 Conference on Computer Vision and Pattern Recognition, pages 18291\u201318300, 2022.   \n320 [10] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei Efros. Image-to-image translation with   \n321 conditional adversarial networks. In Proc. IEEE Conference on Computer Vision and Pattern   \n322 Recognition, pages 1125\u20131134, 2017.   \n323 [11] Somi Jeong, Youngjung Kim, Eungbean Lee, and Kwanghoon Sohn. Memory-guided unsuper  \n324 vised image-to-image translation. In Proceedings of the IEEE/CVF Conference on Computer   \n325 Vision and Pattern Recognition, pages 6558\u20136567, 2021.   \n326 [12] Hai Jiang, Ao Luo, Haoqiang Fan, Songchen Han, and Shuaicheng Liu. Low-light image   \n327 enhancement with wavelet-based diffusion models. ACM Transactions on Graphics (TOG),   \n328 42(6):1\u201314, 2023.   \n329 [13] Yifan Jiang, Xinyu Gong, Ding Liu, Yu Cheng, Chen Fang, Xiaohui Shen, Jianchao Yang, Pan   \n330 Zhou, and Zhangyang Wang. Enlightengan: Deep light enhancement without paired supervision.   \n331 IEEE Transactions on Image Processing, 30:2340\u20132349, 2021.   \n332 [14] Mikhail Kennerley, Jian-Gang Wang, Bharadwaj Veeravalli, and Robby T Tan. 2pcnet: Two  \n333 phase consistency training for day-to-night unsupervised domain adaptive object detection. In   \n334 Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages   \n335 11484\u201311493, 2023.   \n336 [15] Junho Kim, Minjae Kim, Hyeonwoo Kang, and Kwanghee Lee. U-gat-it: Unsupervised   \n337 generative attentional networks with adaptive layer-instance normalization for image-to-image   \n338 translation. arXiv preprint arXiv:1907.10830, 2019.   \n339 [16] Soohyun Kim, Jongbeom Baek, Jihye Park, Gyeongnyeon Kim, and Seungryong Kim. In  \n340 staformer: Instance-aware image-to-image translation with transformer. In Proceedings of   \n341 the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18321\u201318331,   \n342 2022.   \n343 [17] Paul Kubelka. Ein beitrag zur optik der farbanstriche (contribution to the optic of paint).   \n344 Zeitschrift fur technische Physik, 12:593\u2013601, 1931.   \n345 [18] Jeong-gi Kwak, Youngsaeng Jin, Yuanming Li, Dongsik Yoon, Donghyeon Kim, and Hanseok   \n346 Ko. Adverse weather image translation with asymmetric and uncertainty-aware gan. arXiv   \n347 preprint arXiv:2112.04283, 2021.   \n348 [19] Michael J. Milford and Gordon. F. Wyeth. Seqslam: Visual route-based navigation for sunny   \n349 summer days and stormy winter nights. In 2012 IEEE International Conference on Robotics   \n350 and Automation, pages 1643\u20131649, 2012.   \n351 [20] Taesung Park, Alexei Efros, Richard Zhang, and Jun-Yan Zhu. Contrastive learning for unpaired   \n352 image-to-image translation. In European Conference on Computer Vision, pages 319\u2013345,   \n353 2020.   \n354 [21] Aashish Sharma and Robby T Tan. Nighttime visibility enhancement by increasing the dynamic   \n355 range and suppression of light effects. In Proceedings of the IEEE/CVF Conference on Computer   \n356 Vision and Pattern Recognition, pages 11977\u201311986, 2021.   \n357 [22] Seokbeom Song, Suhyeon Lee, Hongje Seong, Kyoungwon Min, and Euntai Kim. Shunit: Style   \n358 harmonization for unpaired image-to-image translation. Proceedings of the AAAI Conference   \n359 on Artificial Intelligence, 37(2):2292\u20132302, Jun. 2023.   \n360 [23] Tao Wang, Kaihao Zhang, Tianrun Shen, Wenhan Luo, Bjorn Stenger, and Tong Lu. Ultra  \n361 high-definition low-light image enhancement: A benchmark and transformer-based method. In   \n362 Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 2654\u20132662,   \n363 2023.   \n364 [24] Weilun Wang, Wengang Zhou, Jianmin Bao, Dong Chen, and Houqiang Li. Instance-wise hard   \n365 negative example generation for contrastive learning in unpaired image-to-image translation.   \n366 In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14020\u2013   \n367 14029, 2021.   \n368 [25] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment:   \n369 from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600\u2013   \n370 612, 2004.   \n371 [26] Youya Xia, Josephine Monica, Wei-Lun Chao, Bharath Hariharan, Kilian Q Weinberger, and   \n372 Mark Campbell. Image-to-image translation for autonomous driving from coarsely-aligned   \n373 image pairs. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages   \n374 7756\u20137762. IEEE, 2023.   \n375 [27] Shaoan Xie, Qirong Ho, and Kun Zhang. Unsupervised image-to-image translation with density   \n376 changing regularization. In Advances in Neural Information Processing Systems, 2022.   \n377 [28] Shaoan Xie, Yanwu Xu, Mingming Gong, and Kun Zhang. Unpaired image-to-image translation   \n378 with shortest path regularization. In Proceedings of the IEEE/CVF Conference on Computer   \n379 Vision and Pattern Recognition (CVPR), pages 10177\u201310187, June 2023.   \n380 [29] Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying Chen, Fangchen Liu, Vashisht   \n381 Madhavan, and Trevor Darrell. Bdd100k: A diverse driving dataset for heterogeneous mul  \n382 titask learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern   \n383 recognition, pages 2636\u20132645, 2020.   \n384 [30] Zhenjie Yu, Shuang Li, Yirui Shen, Chi Harold Liu, and Shuigen Wang. On the difficulty of   \n385 unpaired infrared-to-visible video translation: Fine-grained content-rich patches transfer. In   \n386 Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),   \n387 pages 1631\u20131640, June 2023.   \n388 [31] Fangneng Zhan, Jiahui Zhang, Yingchen Yu, Rongliang Wu, and Shijian Lu. Modulated contrast   \n389 for versatile image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision   \n390 and Pattern Recognition (CVPR), pages 18280\u201318290, June 2022.   \n391 [32] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unrea  \n392 sonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE   \n393 conference on computer vision and pattern recognition, pages 586\u2013595, 2018.   \n394 [33] Ziqiang Zheng, Yang Wu, Xinran Han, and Jianbo Shi. Forkgan: Seeing into the rainy night. In   \n395 European conference on computer vision, pages 155\u2013170. Springer, 2020.   \n396 [34] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei Efros. Unpaired image-to-image transla  \n397 tion using cycle-consistent adversarial networks. In Proc. IEEE International Conference on   \n398 Computer Vision, pages 2223\u20132232, 2017.   \n400 This supplementary material is organized as follows. Appendix B provides additional details about   \n401 the proof that the invariant $N_{\\lambda^{m}x^{n}}$ is exclusively related to the illumination. Appendix C outlines the   \n402 limitations and failure case of N2D3. Appendix D illustrates the implementation details, including   \n403 N2D3 and other methods used in the experiments. Appendix E presents additional visualization   \n404 results. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "405 B More Proof Details ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "406 We provide a detailed proof process to demonstrate how the invariant $N_{\\lambda^{m}x^{n}}$ is exclusively related   \n407 to the illumination and can function as the light effect detector. First, consider the following   \n408 equations, corresponding to Equation (5) in the main paper: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{N_{\\lambda^{m}x^{n}}=\\cfrac{\\partial^{m+n-2}}{\\partial\\lambda^{m-1}\\partial x^{n-1}}\\cfrac{\\partial}{\\partial x}\\lbrace\\cfrac{1}{E(\\lambda,x)}\\cfrac{\\partial E(\\lambda,x)}{\\partial\\lambda}\\rbrace}\\\\ &{\\qquad=\\cfrac{\\partial^{m+n-2}}{\\partial\\lambda^{m-1}\\partial x^{n-1}}\\cfrac{\\partial}{\\partial x}\\lbrace\\cfrac{1}{e(\\lambda,x)}\\cfrac{\\partial e(\\lambda,x)}{\\partial\\lambda}+\\cfrac{1}{R(\\lambda)C(x)}\\cfrac{\\partial R(\\lambda)C(x)}{\\partial\\lambda}\\rbrace,}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "409 by applying the additivity of linear differential operators, the first term represents the invariants only   \n410 related to the illumination. The second term can be simplified by applying the chain rule as follows: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\partial}{\\partial x}\\{\\frac{1}{R(\\lambda)C(x)}\\frac{\\partial R(\\lambda)C(x)}{\\partial\\lambda}\\}}\\\\ &{\\displaystyle=\\frac{1}{R(\\lambda)^{2}C(x)^{2}}(\\frac{\\partial^{2}\\{R(\\lambda)C(x)\\}}{\\partial\\lambda\\partial x}\\cdot R(\\lambda)C(x)-\\frac{\\partial\\{R(\\lambda)C(x)\\}}{\\partial\\lambda}\\cdot\\frac{\\partial\\{R(\\lambda)C(x)\\}}{\\partial x})}\\\\ &{\\displaystyle=\\frac{1}{R(\\lambda)^{2}C(x)^{2}}(\\frac{\\partial R(\\lambda)}{\\partial\\lambda}\\frac{\\partial C(x)}{\\partial x}\\cdot R(\\lambda)C(x)-\\frac{\\partial R(\\lambda)}{\\partial\\lambda}C(x)\\cdot R(\\lambda)\\frac{\\partial C(x)}{\\partial x})=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "411 Finally, we conclude that the invariant $N_{\\lambda^{m}x^{n}}$ is exclusively related to the illumination and can be   \n412 formulated as follows: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{N_{\\lambda^{m}x^{n}}=\\cfrac{\\partial^{m+n-2}}{\\partial\\lambda^{m-1}\\partial x^{n-1}}\\cfrac{\\partial}{\\partial x}\\{\\cfrac{1}{E(\\lambda,x)}\\cfrac{\\partial E(\\lambda,x)}{\\partial\\lambda}\\}}\\\\ &{\\qquad=\\cfrac{\\partial^{m+n-1}}{\\partial\\lambda^{m-1}\\partial x^{n}}\\{\\cfrac{1}{e(\\lambda,x)}\\cfrac{\\partial e(\\lambda,x)}{\\partial\\lambda}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "image", "img_path": "URkFX1jmOd/tmp/ffd2c476ebaf5dd2c58d9758e16c68db180c576809409a08ef73b71d797323f3.jpg", "img_caption": ["Figure 7: Failure Cases of N2D3: Our method struggles to handle various other types of degradation. "], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "413 C Limitations and Failure Case ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "414 Despite the superior performance of N2D3 in Night2Day, it still exhibits certain limitations. On the   \n415 one hand, this work focuses solely on addressing light degradation, while nighttime environments   \n416 encompass various other types of degradation, including blur caused by rain, motion, and other   \n417 factors. Our method currently struggles to handle these situations effectively. On the other hand, the   \n418 limitations of visible imaging in night vision arise from the scarcity of photos captured in low-light   \n419 conditions, as illustrated by the failure cases presented inFigure 7. Future advancements in night   \n420 vision will likely incorporate additional modalities, such as infrared images, radar, and other sensor   \n421 data, to overcome these challenges and improve performance. ", "page_idx": 12}, {"type": "image", "img_path": "URkFX1jmOd/tmp/acd6464f9b3cbd581a73e57de9a65f6a0b6112052d473d25a6c919d2aa4994c9.jpg", "img_caption": ["Figure 8: More disentanglement results. The first and third rows display nighttime images, while the second and fourth rows show the corresponding degradation disentanglement results. The color progression from blue, light blue, green to yellow corresponds to the following regions: darkness, well-lit, light effects, and high-light. "], "img_footnote": [], "page_idx": 13}, {"type": "image", "img_path": "URkFX1jmOd/tmp/58cb77197348b7065c0def877870d8e17dcbb546acdabd60f2f7b95cb0d7fb47.jpg", "img_caption": ["Figure 9: Qualitative comparison abalation results. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "422 D Implementation Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "423 Training Details. We adopt the resnet 9blocks, a ResNetbased model with nine residual blocks, as   \n424 the backbone for generator $G$ . Additionally, we utilize the patch-wise discriminator $D$ following   \n425 PatchGAN[10]. To conduct degradation-aware contrastive learning on multiple layers, we extract   \n426 features from 5 layers of the generator $G$ encoder, as done in [20]. These layers include RGB pixels,   \n427 the first and second downsampling convolution, and the first and fifth residual block. For the features   \n428 of each layer, we apply a 2-layer MLP to acquire final 256-dimensional features. These features are   \n429 then utilized in our degradation-aware contrastive learning.   \n430 All the comparison methods are reproduced using their released source code with default settings.   \n431 Training procedures are consistent across all methods. All models are trained using the Adaptive   \n432 Moment Estimation optimizer with an initial learning rate of $10^{-4}$ , a momentum of 0.9, and weight   \n433 decay of $10^{-4}$ . For the BDD100K dataset, training consists of 10 epochs with the initial learning   \n434 rate, followed by another 10 epochs with a decreased learning rate using the polynomial annealing   \n435 procedure with a power of 0.9. On the Alderley dataset, given the limited training data compared   \n436 to BDD100K, we extend the training to 20 epochs with the initial learning rate and an additional   \n437 20 epochs with the decayed learning rate. All the experiments are run on a single A100 GPU with   \n438 80GB of memory. Training our method with a smaller patch size and batch size on a device with less   \n439 memory is feasible.   \n440 Evaluation Details. In the evaluation, we compute the Fr\u00e9chet Inception Distance (FID) [7],   \n441 Structural Similarity Index (SSIM) [25], and Learned Perceptual Image Patch Similarity (LPIPS)   \n442 [32] scores on $256\\times512$ images. Partial FID scores are provided by ForkGAN [33], and all SSIM   \n443 and LPIPS scores are reproduced by us.   \n444 Semantic segmentation evaluation are conducted as follows. First, we use Deeplabv3 pretrained   \n445 on the Cityscapes dataset as the semantic segmentation model [2]. The model is provided by   \n446 https://github.com/open-mmlab/mmsegmentation with an R-18-D8 backbone and trained at   \n447 a resolution of $512\\times1024$ . Second, we perform $512\\times1024$ Night2Day translation to obtain the   \n448 generation results. Finally, we infer the semantic segmentation on the generated daytime images. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "image", "img_path": "URkFX1jmOd/tmp/b1b075afe2f107f797f05a29db37bcea0752a44c917271eef1537307cf1f075d.jpg", "img_caption": ["Figure 10: More qualitative comparison results on the Alderley dataset. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "449 E More Visualization Results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "450 More Ablation Visualization Results. We provide ablation visualization results on both Alderley   \n451 and BDD100K in Figure 9. The complete method is presented along with ablation studies on the   \n452 invariant $N$ and without degradation-aware reweighting. All the modules contribute to improving the   \n453 ability to maintain semantic consistency.   \n454 More Disentanglement Results. We provide additional disentanglement results in Figure 8. Our   \n455 disentanglement methods offer a comprehensive representation of different illumination degradation   \n456 types in various nighttime scenes.   \n457 More Qualitative Comparison. We present more qualitative comparisons in Figure 10 and Figure 11   \n458 alongside other methods.Our method demonstrates visually pleasing results under various nighttime   \n459 conditions. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "image", "img_path": "URkFX1jmOd/tmp/2c4d815b117d31f4e82c523d2a110e8b3de332a92e05a5e2553c718eb6a6bbdb.jpg", "img_caption": ["Figure 11: More qualitative comparison results on the BDD100K dataset. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "460 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "64 Answer: [Yes]   \n65 Justification: We claim our main contribution as N2D3, which achieves SOTA performance   \n66 by bridging the domain gap between nighttime and daytime in a degradation-aware manner.   \n67 Guidelines:   \n68 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n69 made in the paper.   \n70 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n71 contributions made in the paper and important assumptions and limitations. A No or   \n72 NA answer to this question will not be perceived well by the reviewers.   \n73 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n74 much the results can be expected to generalize to other settings.   \n75 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n76 are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "477 2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: We discuss our limitation in degradations beyond light and low-light image scarcity in the appendix. ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 17}, {"type": "text", "text": "509 3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "510 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n511 a complete (and correct) proof? ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "526 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "527 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n528 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n529 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Justification: All the information needed to reproduce the main experimental results is included in the Section 3 and Appendix D. ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "565 5. Open access to data and code ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "566 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n567 tions to faithfully reproduce the main experimental results, as described in supplemental   \n568 material?   \n569 Answer: [No]   \n570 Justification: Code will be released latter.   \n571 Guidelines:   \n572 \u2022 The answer NA means that paper does not include experiments requiring code.   \n573 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n574 public/guides/CodeSubmissionPolicy) for more details.   \n575 \u2022 While we encourage the release of code and data, we understand that this might not be   \n576 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n577 including code, unless this is central to the contribution (e.g., for a new open-source   \n578 benchmark).   \n579 \u2022 The instructions should contain the exact command and environment needed to run to   \n580 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n581 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n582 \u2022 The authors should provide instructions on data access and preparation, including how   \n583 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n584 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n585 proposed method and baselines. If only a subset of experiments are reproducible, they   \n586 should state which ones are omitted from the script and why.   \n587 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n588 versions (if applicable).   \n589 \u2022 Providing as much information as possible in supplemental material (appended to the   \n590 paper) is recommended, but including URLs to data and code is permitted.   \n591 6. Experimental Setting/Details   \n592 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n593 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n594 results?   \n595 Answer: [Yes]   \n596 Justification: The training details and dataset information are provided in Section 4.   \n597 Guidelines:   \n598 \u2022 The answer NA means that the paper does not include experiments.   \n599 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n600 that is necessary to appreciate the results and make sense of them.   \n601 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n602 material.   \n603 7. Experiment Statistical Significance   \n604 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n605 information about the statistical significance of the experiments?   \n606 Answer: [No]   \n607 Justification: Error bars are not reported because it would be too computationally expensive.   \n608 We report our results using a fixed random seed.   \n609 Guidelines:   \n610 \u2022 The answer NA means that the paper does not include experiments.   \n611 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n612 dence intervals, or statistical significance tests, at least for the experiments that support   \n613 the main claims of the paper.   \n614 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n615 example, train/test split, initialization, random drawing of some parameter, or overall   \n616 run with given experimental conditions).   \n617 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n618 call to a library function, bootstrap, etc.)   \n619 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n620 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n621 of the mean.   \n622 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n623 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n624 of Normality of errors is not verified.   \n625 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n626 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n627 error rates).   \n628 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n629 they were calculated and reference the corresponding figures or tables in the text.   \n630 8. Experiments Compute Resources   \n631 Question: For each experiment, does the paper provide sufficient information on the com  \n632 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n633 the experiments?   \n634 Answer: [Yes]   \n635 Justification: We report the compute resources in Appendix D.   \n636 Guidelines:   \n637 \u2022 The answer NA means that the paper does not include experiments.   \n638 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n639 or cloud provider, including relevant memory and storage.   \n640 \u2022 The paper should provide the amount of compute required for each of the individual   \n641 experimental runs as well as estimate the total compute.   \n642 \u2022 The paper should disclose whether the full research project required more compute   \n643 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n644 didn\u2019t make it into the paper).   \n645 9. Code Of Ethics   \n646 Question: Does the research conducted in the paper conform, in every respect, with the   \n647 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n648 Answer: [Yes]   \n649 Justification: The research conducted in this paper conforms, in every respect, with the   \n650 NeurIPS Code of Ethics.   \n651 Guidelines:   \n652 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n653 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n654 deviation from the Code of Ethics.   \n655 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n656 eration due to laws or regulations in their jurisdiction).   \n657 10. Broader Impacts   \n658 Question: Does the paper discuss both potential positive societal impacts and negative   \n659 societal impacts of the work performed?   \n660 Answer: [Yes]   \n661 Justification: The societal impacts are discussed in the manuscript and appendix.   \n662 Guidelines:   \n663 \u2022 The answer NA means that there is no societal impact of the work performed.   \n664 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n665 impact or why the paper does not address societal impact.   \n666 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n667 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n668 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n669 groups), privacy considerations, and security considerations.   \n670 \u2022 The conference expects that many papers will be foundational research and not tied   \n671 to particular applications, let alone deployments. However, if there is a direct path to   \n672 any negative applications, the authors should point it out. For example, it is legitimate   \n673 to point out that an improvement in the quality of generative models could be used to   \n674 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n675 that a generic algorithm for optimizing neural networks could enable people to train   \n676 models that generate Deepfakes faster.   \n677 \u2022 The authors should consider possible harms that could arise when the technology is   \n678 being used as intended and functioning correctly, harms that could arise when the   \n679 technology is being used as intended but gives incorrect results, and harms following   \n680 from (intentional or unintentional) misuse of the technology.   \n681 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n682 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n683 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n684 feedback over time, improving the efficiency and accessibility of ML).   \n685 11. Safeguards   \n686 Question: Does the paper describe safeguards that have been put in place for responsible   \n687 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n688 image generators, or scraped datasets)?   \n689 Answer: [NA]   \n690 Justification: Our model does not have such risks, and all the datasets used in the experiments   \n691 are open-source benchmarks in this field.   \n692 Guidelines:   \n693 \u2022 The answer NA means that the paper poses no such risks.   \n694 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n695 necessary safeguards to allow for controlled use of the model, for example by requiring   \n696 that users adhere to usage guidelines or restrictions to access the model or implementing   \n697 safety filters.   \n698 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n699 should describe how they avoided releasing unsafe images.   \n700 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n701 not require this, but we encourage authors to take this into account and make a best   \n702 faith effort.   \n703 12. Licenses for existing assets   \n704 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n705 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n706 properly respected?   \n707 Answer: [Yes]   \n708 Justification: The code and data are properly credited, and the license and terms of use are   \n709 explicitly mentioned and properly documented.   \n710 Guidelines:   \n711 \u2022 The answer NA means that the paper does not use existing assets.   \n712 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n713 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n714 URL.   \n715 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n716 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n717 service of that source should be provided.   \n718 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n719 package should be provided. For popular datasets, paperswithcode.com/datasets   \n720 has curated licenses for some datasets. Their licensing guide can help determine the   \n721 license of a dataset.   \n722 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n723 the derived asset (if it has changed) should be provided.   \n724 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n725 the asset\u2019s creators.   \n726 13. New Assets   \n727 Question: Are new assets introduced in the paper well documented and is the documentation   \n728 provided alongside the assets?   \n729 Answer: [Yes]   \n730 Justification: The code introduced in the paper is well-documented, and the documentation   \n731 is provided alongside it.   \n732 Guidelines:   \n733 \u2022 The answer NA means that the paper does not release new assets.   \n734 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n735 submissions via structured templates. This includes details about training, license,   \n736 limitations, etc.   \n737 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n738 asset is used.   \n739 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n740 create an anonymized URL or include an anonymized zip file.   \n741 14. Crowdsourcing and Research with Human Subjects   \n742 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n743 include the full text of instructions given to participants and screenshots, if applicable, as   \n744 well as details about compensation (if any)?   \n745 Answer: [NA]   \n746 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n747 Guidelines:   \n748 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n749 human subjects.   \n750 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n751 tion of the paper involves human subjects, then as much detail as possible should be   \n752 included in the main paper.   \n753 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n754 or other labor should be paid at least the minimum wage in the country of the data   \n755 collector.   \n756 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n757 Subjects   \n758 Question: Does the paper describe potential risks incurred by study participants, whether   \n759 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n760 approvals (or an equivalent approval/review based on the requirements of your country or   \n761 institution) were obtained?   \n762 Answer: [NA]   \n763 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n764 Guidelines:   \n765 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n766 human subjects.   \n767 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n768 may be required for any human subjects research. If you obtained IRB approval, you   \n769 should clearly state this in the paper. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}]