[{"type": "text", "text": "Optimal Rates for Vector-Valued Spectral Regularization Learning Algorithms ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Dimitri Meunier\u2217 ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zikai Shen\u2217", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Gatsby Computational Neuroscience Unit University College London dimitri.meunier.21@ucl.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Department of Statistical Science University College London zikai.shen.22@ucl.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Mattes Mollenhauer Merantix Momentum mattes.mollenhauer@merantix-momentum.com ", "page_idx": 0}, {"type": "text", "text": "Arthur Gretton ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Gatsby Computational Neuroscience Unit University College London arthur.gretton@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Zhu Li Department of Mathematics Imperial College London zli12@ic.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study theoretical properties of a broad class of regularized algorithms with vector-valued output. These spectral algorithms include kernel ridge regression, kernel principal component regression and various implementations of gradient descent. Our contributions are twofold. First, we rigorously confirm the so-called saturation effect for ridge regression with vector-valued output by deriving a novel lower bound on learning rates; this bound is shown to be suboptimal when the smoothness of the regression function exceeds a certain level. Second, we present an upper bound on the finite sample risk for general vector-valued spectral algorithms, applicable to both well-specified and misspecified scenarios (where the true regression function lies outside of the hypothesis space), and show that this bound is minimax optimal in various regimes. All of our results explicitly allow the case of infinite-dimensional output variables, proving consistency of recent practical applications. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We investigate a fundamental topic in modern machine learning\u2014the behavior and efficiency of learning algorithms for regression in high-dimensional and potentially infinite-dimensional output spaces $\\boldsymbol{\\wp}$ . Given two random variables $X$ and $Y$ , we seek to empirically minimize the squared expected risk ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\mathcal{E}(F):=\\mathbb{E}\\left[\\|Y-F(X)\\|_{\\mathcal{V}}^{2}\\right]\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "over functions $F$ in a reproducing kernel Hilbert space consisting of vector-valued functions from a topological space $\\mathcal{X}$ to a Hilbert space $\\boldsymbol{\\wp}$ . The study of this setting as an ill-posed statistical inverse problem is well established: see e.g. 46, 6, 53, 3, 5, 17. In this work, we study the setting when $\\boldsymbol{\\wp}$ is high- or infinite-dimensional, since it has been less well covered by the literature, yet has many applications in multitask regression [7, 2] and infinite-dimensional learning problems, including the conditional mean embedding [20, 21, 41], structured prediction [11, 12], causal inference [43], regression with instrumental and proximal variables [42, 35], the estimation of linear operators and dynamical systems [47, 37, 26, 38, 25], and functional regression [24]. Interestingly, the aforementioned infinite-dimensional applications typically use the classical ridge regression algorithm. Our goal is to motivate the use of alternative learning algorithms in these settings, while providing strong theoretical guarantees. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Classically, the ill-posed problem (1) is solved via regularization strategies, which are often implemented in terms of so-called spectral filter functions in the context of inverse problems in Hilbert spaces [16]. When applied to the learning problem given by (1), these filter functions correspond to learning algorithms including ridge regression, a variety of different implementations of gradient descent, principal component regression, and other related methods (we refer the reader to 19 and 2 for overviews of the real-valued and vector-valued output variable case, respectively). Algorithms based on spectral fliter functions when $\\mathcal{V}=\\mathbb{R}$ are studied extensively, see e.g. [5, 34]. To the best of our knowledge, the detailed behavior of this general class of algorithms has remained unknown when $\\boldsymbol{\\wp}$ is a general Hilbert space, with the exception of a few results for special cases in the setting of ridge regression [6, 31]. ", "page_idx": 1}, {"type": "text", "text": "Overview of our contributions. In this manuscript, we aim to theoretically understand vector-valued spectral learning algorithms. The contribution of our work is twofold: (i) we rigorously confirm the saturation effect of ridge regression for general Hilbert spaces $\\boldsymbol{\\mathscr{y}}$ (see paragraph below) in the context of lower bounds on rates for the learning problem (1) and (ii) we cover a gap in the existing literature by providing upper rates for general spectral algorithms in high- and infinite-dimensional spaces. Our results explicitly allow the misspecified learning case in which the true regression function is not contained in the hypothesis space. We base our analysis on the concept of vector-valued interpolation spaces introduced by [30, 31]. The interpolation space norms measure the smoothness of the true regression function, replacing typical source conditions found in the literature which only cover the well-specified case. To the best of our knowledge, these are the first bounds covering this general setting for vector-valued spectral algorithms. ", "page_idx": 1}, {"type": "text", "text": "Saturation effect of ridge regression. The widely-used ridge regression algorithm is known to exhibit the so-called saturation effect: it fails to exploit additional smoothness in the target function beyond a certain threshold. This effect has been thoroughly investigated in the context of Tikhonov regularization in inverse problems [16, Chapter 5], but is generally reflected only in upper rates in the learning literature, see e.g. [34, 5]. Interestingly, existing lower bounds [6, 5, 31] are usually formulated in a more general setting and do not reflect this saturation effect, leaving a gap between upper and lower rates. We leverage the bias-variance decomposition paradigm to lower bound the learning risk of kernel ridge regression with vector-valued output, in order to close this gap. ", "page_idx": 1}, {"type": "text", "text": "Learning rates of vector-valued spectral algorithms. Motivated by the fact that the saturation effect is technically unavoidable with vector-valued ridge regression, we proceed to study the generalization error of popular alternative learning algorithms. In particular, we provide upper rates in the vector-valued setting consistent with the known behavior of spectral algorithms in the real-valued learning setting, based on their so-called qualification property [5, 34]. In particular, we confirm that a saturation effect can be bypassed in high and infinite dimensions by algorithms such as principal component regression and gradient descent, allowing for a better sample complexity for high-smoothness problems. Furthermore, we study the misspecified setting and show that upper rates for spectral algorithms match the state-of-the-art upper rates for misspecified vector-valued ridge regression recently obtained by [31]. Those rates are optimal for a wide variety of settings. Moreover, we argue that applications of vector-valued spectral algorithms are easy to implement by making use of an extended representer theorem based on [2], allowing for the numerical evaluation based on empirical data\u2014even in the infinite-dimensional case. ", "page_idx": 1}, {"type": "text", "text": "Related Work. The saturation effect of regularization techniques in deterministic inverse problems is well-known. For example, [40, 36, 22] study the saturation effect for Tikhonov regularization and general spectral algorithms. In the kernel statistical learning framework, the general phenomenon of saturation is discussed by e.g. [3, 19]. Recent work by [29] investigates saturation effect in the learning context by providing a lower bound on the learning rate. To the best of our knowledge, however, all studies in the statistical learning context focus on the case when $Y$ is real-valued. General upper bounds of kernel ridge regression with real-valued or finite-dimensional $Y$ have been extensively studied in the literature (see e.g., [6, 50, 8, 17]), where minimax optimal learning rates are derived. Recent work [30, 31] studies the infinite-dimensional output space setting with Tikhonov regularization and obtains analogous minimax optimal learning rates. [23] later study a setting where both the input and output space is the infinite dimensional Sobolev RKHS and establish the minimax optimal rate. For kernel learning with spectral algorithms, existing work (see e.g., [3, 5, 32, 34, 54, 28]) focuses on real-valued output space setting and obtains optimal upper learning rates depending on the qualification number of the spectral algorithms, where only [54, 28] consider the misspecified learning scenario where the target regression function does not lie in the hypothesis space. For vector-valued output spaces, [27] considers learning with vector-valued random features. However, general investigations of spectral algorithms for vector-valued output spaces are absent in the literature. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Structure of this paper. This paper is structured as follows. In Section 2, we introduce mathematical preliminaries related to reproducing kernel Hilbert spaces, vector-valued regression as well as the concept of vector-valued interpolation spaces. Section 3 contains a brief review the so-called saturation effect and a corresponding novel lower bound for vector-valued kernel ridge regression. In Section 4, we investigate general spectral learning algorithms in the context of vector-valued interpolations spaces and provide our main result: upper learning rates for this setting. ", "page_idx": 2}, {"type": "text", "text": "2 Background and Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Throughout the paper, we consider a random variable $X$ (the covariate) defined on a second countable locally compact Hausdorff space2 $\\mathcal{X}$ endowed with its Borel $\\sigma$ -field ${\\mathcal{F}}_{\\mathcal{X}}$ , and the random variable $Y$ (the output) defined on a potentially infinite dimensional separable real Hilbert space $(\\mathfrak{y},\\langle\\cdot,\\cdot\\rangle_{\\mathcal{Y}})$ endowed with its Borel $\\sigma$ -field ${\\mathcal{F}}_{\\mathcal{Y}}$ . We let $(\\Omega,{\\mathcal{F}},\\mathbb{P})$ be the underlying probability space with expectation operator $\\mathbb{E}$ . Let $P$ be the push-forward of $\\mathbb{P}$ under $(X,Y)$ and $\\pi$ and $\\nu$ be the marginal distributions on $\\mathcal{X}$ and $\\boldsymbol{\\mathscr{y}}$ , respectively; i.e., $X\\sim\\pi$ and $Y\\sim\\,\\nu$ . We use the Markov kernel $p:$ $\\mathcal{X}\\times\\mathcal{F}_{\\mathcal{Y}}\\rightarrow\\mathbb{R}_{+}$ to express the distribution of $Y$ conditioned on $X$ as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{P}[Y\\in A|X=x]=\\int_{A}p(x,d y),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "for all $x\\in\\mathscr{X}$ and events $A\\in{\\mathcal{F}}_{\\mathfrak{V}}$ , see e.g. [15]. We introduce some notation related to linear operators on Hilbert spaces and vector-valued integration; formal definitions can be found in Appendix A for completeness, or we refer the reader to [52, 14]. The spaces of Bochner square-integrable functions with respect to $\\pi$ and taking values in $\\boldsymbol{\\wp}$ are written as $L_{2}(\\mathcal{X},\\mathcal{F}_{\\mathcal{X}},\\pi;\\mathcal{Y})$ , abbreviated as $L_{2}(\\pi;\\mathcal{V})$ . We obtain the classical Lebesgue spaces as $L_{2}(\\pi):=L_{2}(\\pi;\\operatorname{\\mathbb{R}})$ . Throughout the paper, we write $[F]$ or more explicitly $[F]_{\\pi}$ for the $\\pi$ -equivalence class of (potentially pointwise defined) measurable functions from $\\mathcal{X}$ to $\\boldsymbol{\\mathscr{y}}$ , which we naturally interpret as elements in $L_{2}(\\pi;\\mathcal{V})$ whenever they are square-integrable. Let $H$ be a separable real Hilbert space with inner product $\\langle\\cdot,\\cdot\\rangle_{H}$ . We write $\\bar{\\mathcal{L}^{\\prime}}(H,H^{\\prime})$ as the Banach space of bounded linear operators from $H$ to another Hilbert space $H^{\\prime}$ , equipped with the operator norm $\\|\\cdot\\|_{H\\rightarrow H^{\\prime}}$ . When $H=H^{\\prime}$ , we simply write ${\\mathcal{L}}(H)$ instead. We write $\\bar{S_{2}}(\\bar{H},H^{\\prime})$ as the Hilbert space of Hilbert-Schmidt operators from $H$ to $H^{\\prime}$ and $S_{1}(H,H^{\\prime})$ as the Banach space of trace class operators (see Appendix A for a complete definition). For two Hilbert spaces $H,H^{\\prime}$ , we say that $H$ is (continuously) embedded in $H^{\\prime}$ and denote it as $H\\hookrightarrow H^{\\prime}$ if $H$ can be interpreted as a vector subspace of $H^{\\prime}$ and the inclusion operator $i:H\\rightarrow H^{\\prime}$ performing the change of norms with $i x=x$ for $x\\in H$ is continuous; and we say that $H$ is isometrically isomorphic to $H^{\\prime}$ and denote it as $H\\simeq H^{\\prime}$ if there is a linear isomorphism between $H$ and $H^{\\prime}$ which is an isometry. ", "page_idx": 2}, {"type": "text", "text": "Tensor Product of Hilbert Spaces: Denote $H\\otimes H^{\\prime}$ the tensor product of Hilbert spaces $H$ , $H^{\\prime}$ . The element $x\\otimes x^{\\prime}\\,\\in\\,H\\stackrel{\\bar{\\,}}{\\otimes}H^{\\prime}$ is treated as the linear rank-one operator $x\\otimes x^{\\prime}:\\bar{H}^{\\prime}\\to H$ defined by $y^{\\prime}\\,\\rightarrow\\,\\langle y^{\\prime},x^{\\prime}\\rangle_{H^{\\prime}}x$ for $y^{\\prime}\\in H^{\\prime}$ . Based on this identification, the tensor product space $H\\otimes H^{\\prime}$ is isometrically isomorphic to the space of Hilbert-Schmidt operators from $H^{\\prime}$ to $H$ , i.e., $H\\otimes H^{\\prime}\\simeq S_{2}(H^{\\prime},H)$ . We will hereafter not make the distinction between these two spaces, and treat them as being identical. ", "page_idx": 2}, {"type": "text", "text": "Remark 1 (1, Theorem 12.6.1). Consider the Bochner space $L_{2}(\\pi;H)$ where $H$ is a separable Hilbert space. One can show that $L_{2}(\\pi;H)$ is isometrically identified with the tensor product space $H\\otimes L_{2}(\\pi)$ , and we denote as $\\Psi$ the isometric isomorphism between the two spaces. See Appendix $A$ for more details on tensor product spaces and the explicit definition of $\\Psi$ . ", "page_idx": 2}, {"type": "text", "text": "Scalar-valued Reproducing Kernel Hilbert Space (RKHS). We let $k:\\mathcal{X}\\times\\mathcal{X}\\to\\mathbb{R}$ be a symmetric and positive definite kernel function and $\\mathcal{H}$ be a vector space of functions from $\\mathcal{X}$ to $\\mathbb{R}$ , endowed with a Hilbert space structure via an inner product $\\left<\\cdot,\\cdot\\right>_{\\mathcal{H}}$ . We say that $k$ is a reproducing kernel of $\\mathcal{H}$ if and only if for all $x\\in\\mathscr{X}$ we have $k(\\cdot,x)\\in\\mathcal{H}$ and for all $x\\in\\mathscr{X}$ and $f\\in\\mathcal H$ , we have $f(x)=\\langle f,k(x,\\cdot)\\rangle_{\\mathcal{H}}$ . A space $\\mathcal{H}$ which possesses a reproducing kernel is called a reproducing kernel Hilbert space (RKHS; see e.g. 4). We denote the canonical feature map of $\\mathcal{H}$ as $\\phi(\\bar{x})=k(\\cdot,\\bar{x})$ . ", "page_idx": 3}, {"type": "text", "text": "We require some technical assumptions on the previously defined RKHS and kernel, which we assume to be satisfied throughout the text: ", "page_idx": 3}, {"type": "text", "text": "1. $\\mathcal{H}$ is separable: this is satisfied if $k$ is continuous, given that $\\mathcal{X}$ is separable3;   \n2. $k(\\cdot,x)$ is measurable for $\\pi$ -almost all $x\\in\\mathscr{X}$ ;   \n3. $k(x,x)\\leq\\kappa^{2}$ for $\\pi$ -almost all $x\\in\\mathscr{X}$ . ", "page_idx": 3}, {"type": "text", "text": "The above assumptions are not restrictive in practice, as well-known kernels such as the Gaussian, Laplace, and Mat\u00e9rn kernels satisfy them on $\\dot{\\boldsymbol{x}}\\subseteq\\mathbb{R}^{d}$ [48]. We now introduce some facts about the interplay between $\\mathcal{H}$ and $L_{2}(\\pi)$ , which has been extensively studied by [44, 45], [13] and [51]. We first define the (not necessarily injective) embedding $I_{\\pi}:\\dot{\\mathcal{H}}\\rightarrow L_{2}(\\pi)$ , mapping a function $f\\in\\mathcal H$ to its $\\pi$ -equivalence class $[f]$ . The embedding is a well-defined compact operator as long as its Hilbert-Schmidt norm is finite. In fact, this requirement is satisfied since its Hilbert-Schmidt norm can be computed as [51, Lemma 2.2 & 2.3] $\\|\\bar{I}_{\\pi}\\|_{S_{2}(\\mathcal H,L_{2}(\\pi))}=\\|k\\|_{L_{2}(\\pi)}\\leq\\kappa$ . The adjoint operator $S_{\\pi}:=I_{\\pi}^{*}:L_{2}(\\pi)\\rightarrow\\mathcal{H}$ is an integral operator with respect to the kernel $k$ , i.e. for $f\\in L_{2}(\\pi)$ and $x\\in\\mathscr{X}$ we have [49, Theorem 4.27] ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left(S_{\\pi}f\\right)\\left(x\\right)=\\int_{\\mathcal{X}}k\\left(x,x^{\\prime}\\right)f\\left(x^{\\prime}\\right)\\mathrm{d}\\pi\\left(x^{\\prime}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Next, we define the self-adjoint, positive semi-definite and trace class integral operators ", "page_idx": 3}, {"type": "equation", "text": "$$\nL_{X}:=I_{\\pi}S_{\\pi}:L_{2}{\\big(}\\pi{\\big)}\\rightarrow L_{2}{\\big(}\\pi{\\big)}\\quad{\\mathrm{~and~}}\\quad C_{X}:=S_{\\pi}I_{\\pi}:{\\mathcal{H}}\\rightarrow{\\mathcal{H}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Vector-valued Reproducing Kernel Hilbert Space (vRKHS). Let $K:\\mathcal{X}\\!\\times\\!\\mathcal{X}\\!\\to\\mathcal{L}(\\mathcal{Y})$ be an operator valued positive-semidefinite (psd) kernel. Fix $K$ , $x\\in\\mathscr{X}$ , and $h\\in\\mathcal{V}$ , then $\\left(K_{x}\\dot{h}\\right)(\\cdot):=K(\\cdot,x)h$ defines a function from $\\mathcal{X}$ to $\\boldsymbol{\\mathscr{y}}$ . The completion of ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{G}_{\\mathrm{pre}}:=\\operatorname{span}\\left\\{K_{x}h\\mid x\\in\\mathcal{X},h\\in\\mathcal{Y}\\right\\}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "with inner product on $\\mathcal{G}_{\\mathrm{pre}}$ defined on the elementary elements as $\\begin{array}{r l}{\\langle K_{x}h,K_{x^{\\prime}}h^{\\prime}\\rangle_{\\mathcal{G}}}&{{}:=}\\end{array}$ $\\langle h,K\\left(x,x^{\\prime}\\right)h^{\\prime}\\rangle_{\\mathcal{V}}$ , defines a vRKHS denoted as $\\mathcal{G}$ . For a more complete overview of the vectorvalued reproducing kernel Hilbert space, we refer the reader to [9], [10] and [31, Section 2]. In the following, we will denote $\\mathcal{G}$ as the vRKHS induced by the kernel $K:\\mathcal{X}\\times\\mathcal{X}\\to\\mathcal{L}(\\mathcal{Y})$ with ", "page_idx": 3}, {"type": "equation", "text": "$$\nK(x,x^{\\prime}):=k(x,x^{\\prime})\\,\\mathrm{Id}_{\\mathcal{Y}},\\quad x,x^{\\prime}\\in\\mathcal{X}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We emphasize that this family of kernels is the de-facto standard for high- and infinite-dimensional applications [20, 21, 41, 11, 12, 42, 35, 43, 37, 26, 38, 25, 24] due to the crucial representer theorem which gives a closed form solution for the ridge regression problem based on the data. We generalize this representer theorem to cover the general spectral algorithm case in Proposition 1. ", "page_idx": 3}, {"type": "text", "text": "Remark 2 (General multiplicative kernel). Without loss of generality, we provide our results for the vRKHS $\\mathcal{G}$ induced by the operator-valued kernel given by $K(x,x^{\\prime})\\,=\\,\\bar{k}(x,x^{\\prime})\\,\\mathrm{Id}_{y}$ . However, with suitably adjusted constants in the assumptions, our results transfer directly to the more general vRKHS $\\widetilde{\\mathcal{G}}$ induced by the more general operator-valued kernel ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\widetilde K(x,x^{\\prime}):=k(x,x^{\\prime})T\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $T:\\mathcal{Y}\\rightarrow\\mathcal{Y}$ is any positive-semidefinite self-adjoint operator. The precise characterization of the adjusted constants is given by $[3I$ , Section 4.1]. ", "page_idx": 3}, {"type": "text", "text": "An important property of $\\mathcal{G}$ is that it is isometrically isomorphic to the space of Hilbert-Schmidt operators between $\\mathcal{H}$ and $\\boldsymbol{\\mathscr{y}}$ [31, Corollary 1]. Similarly to the scalar case we can map every element in $\\mathcal{G}$ into its $\\pi$ \u2212equivalence class in $L_{2}(\\pi;\\mathcal{V})$ and we use the shorthand notation $[\\mathbf{\\bar{\\boldsymbol{F}}}]=[\\mathbf{\\boldsymbol{F}}]_{\\pi}$ (see Definition 6 in Appendix A for more details). ", "page_idx": 3}, {"type": "text", "text": "Theorem 1 (vRKHS isomorphism). For every function $F\\,\\in\\,{\\mathcal G}$ there exists a unique operator $C\\ \\in\\ S_{2}(\\mathcal{H},\\mathcal{Y})$ such that $F(\\bar{\\cdot})\\;=\\;C\\phi(\\cdot)\\;\\in\\;\\mathcal{Y}$ with $\\|C\\|_{S_{2}(\\mathcal{H},\\mathcal{V})}~=~\\|F\\|_{\\mathcal{G}}$ and vice versa. Hence $\\mathscr{G}\\simeq S_{2}(\\mathscr{H},\\mathscr{y})$ and we denote the isometric isomorphism between $S_{2}(\\mathcal{H},\\mathcal{V})$ and $\\mathcal{G}$ as $\\bar{\\Psi}$ . It follows that $\\mathcal{G}$ can be written as ${\\mathcal{G}}=\\{F:{\\mathcal{X}}\\rightarrow{\\mathcal{Y}}\\,|\\,F=C\\phi(\\cdot)$ , $C\\in S_{2}(\\mathscr{H},\\mathscr{y})\\}$ . ", "page_idx": 4}, {"type": "text", "text": "2.1 Vector-valued Regression ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We briefly recall the basic setup of regularized least-squares regression with Hilbert space-valued random variables. The squared expected risk for vector-valued regression is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{E}(\\boldsymbol{F}):=\\mathbb{E}\\left[\\|\\boldsymbol{Y}-\\boldsymbol{F}(\\boldsymbol{X})\\|_{\\mathcal{Y}}^{2}\\right]=\\int_{\\mathcal{X}\\times\\mathcal{Y}}\\|\\boldsymbol{y}-\\boldsymbol{F}(\\boldsymbol{x})\\|_{\\mathcal{Y}}^{2}p(\\boldsymbol{x},d\\boldsymbol{y})\\pi(d\\boldsymbol{x}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "for measurable functions $F:\\mathcal X\\to\\mathcal Y$ . The analytical minimizer of the risk over measurable functions is the regression function or the conditional mean function $F_{\\star}\\in L_{2}(\\pi;\\mathcal{V})$ given by ", "page_idx": 4}, {"type": "equation", "text": "$$\nF_{*}(x):=\\mathbb{E}[Y\\mid X=x]=\\int_{\\mathcal{Y}}y\\,p(x,d y),\\quad x\\in\\mathcal{X}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Throughout the paper, we assume that $\\mathbb{E}[\\|Y\\|_{\\mathcal{Y}}^{2}]\\;<\\;+\\infty$ , i.e., the random variable $Y$ is squareintegrable. Note that this implies $F_{\\star}\\in L_{2}(\\pi;\\mathcal{V})$ . Our focus in this work is to approximate $F_{*}$ with kernel-based regularized least-squares algorithms, where we pay special attention to the case when $\\boldsymbol{\\wp}$ is of high or infinite dimension. We pick $\\mathcal{G}$ as a hypothesis space of functions in which to estimate $F_{*}$ . Note that by Theorem 1, minimizing the functional $\\mathcal{E}$ on $\\mathcal{G}$ is equivalent to minimizing the following functional on $S_{2}(\\mathcal{H},\\mathcal{V})$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\bar{\\mathcal{E}}(C):=\\mathbb{E}\\left[\\|Y-C\\phi(X)\\|_{\\mathcal{V}}^{2}\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "It is shown in [38, Proposition 3.5 and Section 3.4] that the optimality condition can be written as ", "page_idx": 4}, {"type": "equation", "text": "$$\nC_{Y X}=C_{*}C_{X},\\qquad C_{*}\\in S_{2}(\\mathcal{H},\\mathcal{Y}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $C_{Y X}:=\\mathbb{E}[Y\\otimes\\phi(X)]$ is the cross-covariance operator. As discussed in full detail by [38], the problem (5) can be formulated as a potentially ill-posed inverse problem on the space of Hilbert\u2013 Schmidt operators. As such, a regularization is required; we introduce regularized solutions of this problem in Section 4 through the classical concept of spectral filter functions. ", "page_idx": 4}, {"type": "text", "text": "2.2 Vector-valued Interpolation Space and Source Condition ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We now introduce the background required in order to characterize the smoothness of the target function $F_{*}$ , both in the well-specified setting $(F_{*}\\in\\mathcal{G})$ and in the misspecified setting $(F_{*}\\notin\\mathcal{G})$ . We review the results of [51] and [17] in constructing scalar-valued interpolation spaces, and [30] in defining vector-valued interpolation spaces. ", "page_idx": 4}, {"type": "text", "text": "Real-valued Interpolation Space: By the spectral theorem for self-adjoint compact operators, there exists an at most countable index set $I$ , a non-increasing sequence $(\\mu_{i})_{i\\in I}>0$ , and a family $(e_{i})_{i\\in I}\\in\\mathcal{H}$ , such that $\\left([e_{i}]\\right)_{i\\in I}{}^{4}$ is an orthonormal basis (ONB) of ran ${\\overline{{I_{\\pi}}}}\\subseteq L_{2}(\\pi)$ and $(\\boldsymbol{\\mu}_{i}^{1/2}\\boldsymbol{e}_{i})_{i\\in I}$ is an ONB of $(\\ker I_{\\pi})^{\\perp}\\subseteq\\mathcal{H}$ , and we have ", "page_idx": 4}, {"type": "equation", "text": "$$\nL_{X}=\\sum_{i\\in I}\\mu_{i}\\langle\\cdot,[e_{i}]\\rangle_{L_{2}(\\pi)}[e_{i}],\\qquad C_{X}=\\sum_{i\\in I}\\mu_{i}\\langle\\cdot,\\mu_{i}^{\\frac{1}{2}}e_{i}\\rangle_{\\mathcal{H}}\\mu_{i}^{\\frac{1}{2}}e_{i}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For $\\alpha\\geq0$ , the $\\alpha$ -interpolation space [51] is defined by ", "page_idx": 4}, {"type": "equation", "text": "$$\n[\\mathcal{H}]^{\\alpha}:=\\left\\{\\sum_{i\\in I}a_{i}\\mu_{i}^{\\alpha/2}\\left[e_{i}\\right]:\\left(a_{i}\\right)_{i\\in I}\\in\\ell_{2}(I)\\right\\}\\subseteq L_{2}(\\pi),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "equipped with the inner product ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left\\langle\\sum_{i\\in I}a_{i}\\big(\\mu_{i}^{\\alpha/2}[e_{i}]\\big),\\sum_{i\\in I}b_{i}\\big(\\mu_{i}^{\\alpha/2}[e_{i}]\\big)\\right\\rangle_{[\\mathcal{H}]^{\\alpha}}=\\sum_{i\\in I}a_{i}b_{i},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "for $(a_{i})_{i\\in I}\\,,\\,(b_{i})_{i\\in I}\\;\\;\\in\\;\\;\\ell_{2}(I)$ . The $\\alpha$ -interpolation space defines a Hilbert space. Moreover, $\\left(\\mu_{i}^{\\alpha/2}\\left[e_{i}\\right]\\right)_{i\\in I}$ forms an ONB of $[\\mathcal{H}]^{\\alpha}$ and consequently $[\\mathcal{H}]^{\\alpha}$ is a separable Hilbert space. In the following, we use the abbreviation $\\|\\cdot\\|_{\\alpha}:=\\|\\cdot\\|_{[\\mathcal{H}]^{\\alpha}}$ . ", "page_idx": 5}, {"type": "text", "text": "Vector-valued Interpolation Space: Introduced in [30], vector-valued interpolation spaces generalize the notion of scalar-valued interpolation spaces to vRKHS with a kernel of the form (2). ", "page_idx": 5}, {"type": "text", "text": "Definition 1 (Vector-valued interpolation space). Let $k$ be a real-valued kernel with associated RKHS $\\mathcal{H}$ and let $[\\mathcal{H}]^{\\alpha}$ be the real-valued interpolation space associated to $\\mathcal{H}$ with some $\\alpha\\geq0$ . The vector-valued interpolation space $[\\mathcal{G}]^{\\alpha}$ is defined as (refer to Remark 1 for the definition of $\\Psi$ ) ", "page_idx": 5}, {"type": "equation", "text": "$$\n[\\mathcal G]^{\\alpha}:=\\Psi\\left(S_{2}([\\mathcal H]^{\\alpha},\\mathcal V)\\right)=\\{F\\mid F=\\Psi(C),\\;C\\in S_{2}([\\mathcal H]^{\\alpha},\\mathcal V)\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The space $[\\mathcal{G}]^{\\alpha}$ is a Hilbert space equipped with the inner product ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\left\\langle F,G\\right\\rangle_{\\alpha}:=\\left\\langle C,L\\right\\rangle_{S_{2}([\\mathcal{H}]^{\\alpha},\\mathcal{y})}\\qquad(F,G\\in[\\mathcal{G}]^{\\alpha}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $C=\\Psi^{-1}(F)$ , $L=\\Psi^{-1}(G)$ . For $\\alpha=0$ , we retrieve $\\|F\\|_{0}=\\|F\\|_{L_{2}(\\pi;\\mathcal{Y})}=\\|C\\|_{S_{2}(L_{2}(\\pi),\\mathcal{Y})}$ . ", "page_idx": 5}, {"type": "text", "text": "Remark 3 (Interpolation space inclusions). Note that we have $F_{*}\\in L_{2}(\\pi;\\mathcal{V})$ since $Y\\in L_{2}(\\mathbb{P};{\\mathcal{D}})$ by assumption. Furthermore, for $0<\\beta<\\alpha$ , $I I7_{:}$ , Eq. (7)] imply the inclusions ", "page_idx": 5}, {"type": "equation", "text": "$$\n[{\\mathcal G}]^{\\alpha}\\hookrightarrow[{\\mathcal G}]^{\\beta}\\hookrightarrow[{\\mathcal G}]^{0}\\subseteq L_{2}(\\pi;\\mathcal{Y}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Under assumptions $^{\\,l}$ to $^3$ and with $\\mathcal{X}$ being a second-countable locally compact Hausdorff space, $[\\mathcal{G}]^{0}=L_{2}(\\pi;\\mathcal{D})$ if and only if $\\mathcal{H}$ is dense in the space of continuous functions vanishing at infinity, equipped with the uniform norm $[3I$ , Remark 4]. ", "page_idx": 5}, {"type": "text", "text": "Remark 4 (Well-specified versus misspecified setting). We say that we are in the well-specified setting if $F_{*}\\,\\in\\,[\\mathcal{G}]^{1}$ . In this case, there exists $\\bar{F}\\,\\in\\,\\bar{\\mathcal{G}}$ such that $F_{*}\\ =\\ \\bar{F}\\ \\pi.$ \u2212almost surely and $\\|F_{*}\\|_{1}=\\|\\bar{F}\\|_{\\mathcal{G}}$ , i.e. $F_{*}$ admits a representer in $\\mathcal{G}$ (see Remark 5 in Appendix $A$ ). When $F_{*}\\in[\\mathcal{G}]^{\\beta}$ for $\\beta<1$ , $F_{*}$ may not admit such a representation and we are in the misspecified setting, as $[\\mathcal{G}]^{1}\\subseteq[\\mathcal{G}]^{\\beta}$ . ", "page_idx": 5}, {"type": "text", "text": "Definition 1 and Remarks 3 and 4 motivate the use of following assumption on the smoothness of the target function: there exists $\\beta>0$ and a constant $B\\geq0$ such that $F_{*}\\in[\\mathcal{G}]^{\\beta}$ and ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\|F_{*}\\|_{\\beta}\\leq B.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We let $C_{*}:=\\Psi^{-1}(F_{*})\\in S_{2}([\\mathcal{H}]^{\\beta},\\mathcal{V})$ . (SRC) directly generalizes the notion of a so-called H\u00f6ldertype source condition in the learning literature [6, 17, 32, 34] and allows to characterize the misspecified learning scenario. ", "page_idx": 5}, {"type": "text", "text": "2.3 Further Assumptions ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In addition to (SRC), we require standard assumptions to obtain the precise learning rates for kernel learning algorithms. We list them below. For constants $D_{2}>0$ and $p\\in(0,1]$ and for all $i\\in I$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mu_{i}\\leq D_{2}i^{-1/p}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "For constants $D_{1},D_{2}>0$ and $p\\in(0,1)$ and for all $i\\in I$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\nD_{1}i^{-{\\frac{i}{p}}}\\leq\\mu_{i}\\leq D_{2}i^{-{1/p}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "(EVD) and $\\mathrm{(EVD+)}$ are standard assumptions on the eigenvalue decay of the integral operator: they describe the interplay between the marginal distribution $\\pi$ and the RKHS $\\mathcal{H}$ (see more details in 6, 17). $\\mathrm{(EVD+)}$ is needed in order to establish lower bounds on the excess risk. Note that we have excluded the value $p=1$ from $\\mathrm{(EVD+)}$ ; indeed, $p=1$ is incompatible with the assumption of a bounded kernel, a fact missed by previous works and of independent interest (see Appendix, Remark 7). ", "page_idx": 5}, {"type": "text", "text": "For $\\alpha\\in[p,1]$ , the inclusion $I_{\\pi}^{\\alpha,\\infty}:[\\mathcal{H}]^{\\alpha}\\hookrightarrow L_{\\infty}(\\pi)$ is continuous, and $\\exists A>0$ such that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\|I_{\\pi}^{\\alpha,\\infty}\\|_{[\\mathcal{H}]^{\\alpha}\\to L_{\\infty}(\\pi)}\\leq A.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Property (EMB) is referred to as the embedding property in [17]. It can be shown that it holds if and only if there exists a constant $A\\geq0$ with $\\begin{array}{r}{\\sum_{i\\in I}\\mu_{i}^{\\alpha}e_{i}^{2}(x)\\leq A^{2}}\\end{array}$ for $\\pi$ -almost all $x\\in\\mathscr{X}$ [17, Theorem 9]. Since we assume $k$ to be bounded, the embedding property always hold true when $\\alpha=1$ . ", "page_idx": 5}, {"type": "text", "text": "Furthermore, (EMB) implies a polynomial eigenvalue decay of order $1/\\alpha$ , which is why we take $\\alpha\\geq p$ . (EMB) is not needed when we deal with the well-specified setting, but is crucial to bound the excess risk in the misspecified setting. ", "page_idx": 6}, {"type": "text", "text": "Finally, we assume that there are constants $\\sigma,R>0$ such that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\int_{\\mathcal{V}}\\|y-F_{*}(x)\\|_{\\mathcal{V}}^{q}p(x,d y)\\leq\\frac{1}{2}q!\\sigma^{2}R^{q-2},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "is satisfied for $\\pi$ -almost all $x\\in\\mathscr{X}$ and all $q\\geq2$ . The (MOM) condition on the Markov kernel $p(x,d y)$ is a Bernstein moment condition used to control the noise of the observations (see 6, 17 for more details). If $Y$ is almost surely bounded, for example $\\|Y\\|_{\\mathcal{Y}}\\leq Y_{\\infty}$ almost surely, then (MOM) is satisfied with $\\sigma\\,=\\,R\\,=\\,2Y_{\\infty}$ . It is possible to prove that the Bernstein condition is equivalent to sub-exponentiality, see [38, Remark 4.9]. ", "page_idx": 6}, {"type": "text", "text": "3 Saturation Effect of Kernel Ridge Regression ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The most established way of learning $F_{*}$ is by kernel ridge regression (KRR), which can be formulated as the following optimization problem: given a dataset $D=\\{(x_{i},y_{i})\\}_{i=1}^{n}$ independently and identically sampled from the joint distribution of $X$ and $Y$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\hat{F}_{\\lambda}:=\\operatorname*{arg\\,min}_{F\\in\\mathcal{G}}\\frac{1}{n}\\sum_{i=1}^{n}\\left\\|y_{i}-F\\big(x_{i}\\big)\\right\\|_{\\mathcal{V}}^{2}+\\lambda\\|F\\|_{\\mathcal{G}}^{2},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\lambda\\,>\\,0$ is the regularization parameter. The generalization error of vector-valued KRR is expressed as $\\hat{F}_{\\lambda}-F_{*}$ , and controlled in different norms: see [31] for an extensive study. We recall here a simplified special case of the key results obtained in this work. In the next Theorem, $\\lesssim,\\gtrsim$ are inequality up to positive multiplicative constants that are independent of $n$ . ", "page_idx": 6}, {"type": "text", "text": "Theorem 2 (Upper and lower bounds for KRR in the well-specified regime). Let $\\hat{F}_{\\lambda}$ be the KRR estimator from (7). Furthermore, let the conditions $\\mathrm{(EVD+)}$ , (SRC) and (MOM) be satisfied for some $0<p\\leq1$ and $\\beta\\geq1$ . Then, with high probability we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\left[\\hat{F}_{\\lambda_{n}}\\right]-F_{*}\\right\\|_{L_{2}(\\pi;y)}^{2}\\lesssim n^{-\\frac{\\operatorname*{min}\\{\\beta,2\\}}{\\operatorname*{min}\\{\\beta,2\\}+p}}\\quad f o r\\,a\\,c h o i c e\\,\\lambda_{n}=\\Theta\\left(n^{-\\frac{1}{\\beta+p}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and furthermore for all learning methods (i.e., measurable maps) of the form $D\\to{\\hat{F}}_{D}$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\left[\\hat{F}_{D}\\right]-F_{*}\\right\\|_{L_{2}(\\pi;\\mathcal{Y})}^{2}\\gtrsim n^{-\\frac{\\beta}{\\beta+p}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Theorem 2 shows the minimax optimal learning rate for vector-valued KRR for $\\beta\\in[1,2]$ . However, when $\\beta>2$ , the obtained upper bound saturates at $n^{-\\frac{2}{2+p}}$ , creating a gap with the lower bound. This phenomenon is referred to as the saturation effect of Tikhonov regularization, and has been well investigated in deterministic inverse problems [40]. In the case where $\\boldsymbol{\\mathscr{y}}$ is real-valued, [29] prove that the saturation effect cannot be avoided with Tikhonov regularization. Below, we give a similar but generalized bound on lower rates for the case that $\\boldsymbol{\\wp}$ is a Hilbert space. For this result only, we assume that $\\mathcal{X}$ is a compact subset of $\\mathbb{R}^{d}$ . We give the proof in Appendix B. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3 (Saturation of KRR). Let $\\mathcal{X}$ be a compact subset of $\\mathbb{R}^{d}$ . Let $\\lambda=\\lambda(n)$ be an arbitrary choice of regularization parameter satisfying $\\lambda(n)\\rightarrow0$ as $n\\to+\\infty$ and let $\\hat{F}_{\\lambda}$ be the KRR estimator from (7). We assume that the noise is non-zero and bounded below, i.e. there exists $\\sigma>0$ , such that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\int_{\\mathcal{V}}\\|y-F_{*}(x)\\|_{\\mathcal{V}}^{2}p(x,d y)\\geq\\sigma^{2},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "is satisfied for $\\pi$ -almost all $x\\in\\mathscr{X}$ . We assume in addition and for this result only that $k$ is H\u00f6lder continuous (see Definition $_{l l}$ in the appendix), i.e., $k\\in C^{\\theta}(\\bar{\\mathcal{X}}\\times\\mathcal{X})$ for $\\theta\\in(0,1]$ . Suppose that Assumptions $\\mathrm{(EVD+)}$ and (SRC) hold with $p\\in(0,1)$ and $\\beta\\geq2$ . For $\\tau\\geq0$ , for sufficiently large $n>0$ , where the hidden index bound depends on $\\tau$ , with probability greater than $1-e^{-\\tau}$ , there exists some constant $c_{\\tau}>0$ such that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert\\left[\\hat{F}_{\\lambda}\\right]-F_{*}\\right\\Vert_{L_{2}(\\pi;\\mathcal{Y})}^{2}\\bigg|x_{1},\\dots,x_{n}\\right]\\geq c_{\\tau}n^{-\\frac{2}{2+p}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The assumption that $k$ is H\u00f6lder continuous is crucial in lower bounding the variance with a covering number argument. Kernels satisfying this assumption include Gaussian kernels, Laplace kernels and Mat\u00e9rn kernels. Theorem 3 clearly demonstrates that the learning rate from vector-valued KRR cannot reach the information theoretic lower rate given in Theorem 2. ", "page_idx": 7}, {"type": "text", "text": "As discussed above, [29] propose a similar lower bound in the real-valued case, and we now highlight two fundamental differences with [29] in the proof. First, while both works adopt the same bias-variance decomposition, we need to lower bound the bias and the variance term with infinite-dimensional output in our setting. Second, we adopt a different and simpler approach in proving the lower bound, since there are a number of issues with the proof of [29], both in the treatment of the bias and of the variance. For a detailed comparison with the earlier work, and an explanation of the differences in our approach, please refer to Remark 6 in the Appendix. ", "page_idx": 7}, {"type": "text", "text": "4 Consistency and optimal rates for general spectral algorithms ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Regularized population solution: Our goal is to regularize (5) in such a way that we get a unique and well-defined solution that provides a good approximation to $F_{*}$ . We first recall the concept of a filter function (i.e., a function on an interval which is applied on self-adjoint operators to each individual eigenvalue via the spectral calculus, see 16), that will allow to define a regularization strategy. One may think of the following definition as a class of functions approximating the inversion map $\\bar{x}\\mapsto1/x$ while still being defined for $x=0$ in a reasonable way. We use the definition given by [34], but equivalent definitions can be found throughout the literature. ", "page_idx": 7}, {"type": "text", "text": "Definition 2 (Filter function). Let $\\Lambda\\subseteq\\mathbb{R}^{+}$ . A family of functions $g_{\\lambda}:[0,\\infty)\\rightarrow[0,\\infty)$ indexed by $\\lambda\\in\\Lambda$ is called a filter with qualification $\\rho\\ge0$ if it satisfies the following two conditions: ", "page_idx": 7}, {"type": "text", "text": "1. There exists a positive constant $E$ such that, for all $\\lambda\\in\\Lambda$ ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\alpha\\in[0,1]}\\operatorname*{sup}_{x\\in[0,\\kappa^{2}]}\\lambda^{1-\\alpha}x^{\\alpha}g_{\\lambda}(x)\\leq E\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "2. There exists a positive constant $\\omega_{\\rho}<\\infty$ such that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\alpha\\in[0,\\rho]}\\ \\operatorname*{sup}_{\\lambda\\in\\Lambda}\\operatorname*{sup}_{x\\in[0,\\kappa^{2}]}|r_{\\lambda}(x)|x^{\\alpha}\\lambda^{-\\alpha}\\leq\\omega_{\\rho},\\ \\ \\qquad w i t h\\qquad r_{\\lambda}(x):=1-g_{\\lambda}(x)x.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Below, we give some standard examples which are discussed by e.g. [19, 5] in the context of kernel regression with scalar output variables, and in [2] for the vector-valued case. A variety of additional algorithms can be expressed in terms of a filter function. ", "page_idx": 7}, {"type": "text", "text": "1. Ridge regression. From the Tikhonov fliter function $g_{\\lambda}(x)=(x+\\lambda)^{-1}$ , we obtain the known ridge regression algorithm. In this case, we have $E=\\rho=\\omega_{\\rho}=1$ . ", "page_idx": 7}, {"type": "text", "text": "2. Gradient Descent. From the Landweber iteration filter function given by ", "page_idx": 7}, {"type": "equation", "text": "$$\ng_{k}(x):=\\tau\\sum_{i=0}^{k-1}(1-\\tau x)^{i}\\;\\mathrm{for}\\;k:=1/\\lambda,k\\in\\mathbb{N}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "we obtain the gradient descent scheme with constant step size $\\tau\\,>\\,0$ , which corresponds to the population gradient iteration given by $F_{k+1}:=F_{k}-\\tau\\nabla\\mathcal{E}(F_{k})$ for $k\\in\\mathbb{N}$ . In this case, we have $E=1$ and arbitrary qualification with $\\omega_{\\rho}=1$ whenever $0<\\rho\\leq1$ and $\\omega_{\\rho}=\\rho^{\\rho}$ otherwise. Gradient schemes with more complex update rules can be expressed in terms of filter functions as well [39, 32, 34]. ", "page_idx": 7}, {"type": "text", "text": "3. Kernel principal component regression. The truncation fliter function $g_{\\lambda}(x)=x^{-1}\\mathbb{1}[x\\geq\\lambda]$ yields kernel principal component regression, corresponding to a hard thresholding of eigenvalues at a truncation level $\\lambda$ . In this case we have $E=\\omega_{\\rho}=1$ for arbitrary qualification $\\rho$ . ", "page_idx": 7}, {"type": "text", "text": "Population solution: Given a fliter function $g_{\\lambda}$ , we call $g_{\\lambda}(C_{X})^{5}$ the regularized inverse of $C_{X}$ . We may think of the regularized inverse as approximating the pseudoinverse of $C_{X}$ (see e.g. [16]) when $\\lambda\\to0$ . We define the regularized population solution to (4) as ", "page_idx": 7}, {"type": "equation", "text": "$$\nC_{\\lambda}:=C_{Y X}g_{\\lambda}(C_{X})\\in S_{2}({\\mathcal{H}},{\\mathcal{Y}}),\\qquad F_{\\lambda}(\\cdot):=C_{\\lambda}\\phi(\\cdot)\\in{\\mathcal{G}}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The solution arising from standard regularization strategies leads to well-known statistical methodologies. We refer to [16] for the background on filter functions in classical regularization theory. ", "page_idx": 8}, {"type": "text", "text": "Empirical solution: Given the dataset $D=\\{(x_{i},y_{i})\\}_{i=1}^{n}$ , the empirical analogue of (10) is ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\hat{C}_{\\lambda}:=\\hat{C}_{Y X}g_{\\lambda}(\\hat{C}_{X}),\\qquad\\hat{F}_{\\lambda}(\\cdot):=\\hat{C}_{\\lambda}\\phi(\\cdot)\\in\\mathcal{G},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\hat{C}_{Y X}$ , ${\\hat{C}}_{X}$ are empirical covariance operators define as ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\hat{C}_{X}:=\\frac{1}{n}\\sum_{i=1}^{n}\\phi(x_{i})\\otimes\\phi(x_{i})\\quad\\quad\\hat{C}_{Y X}:=\\frac{1}{n}\\sum_{i=1}^{n}y_{i}\\otimes\\phi(x_{i}).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Note that (11) is the regularized solution of the empirical inverse problem ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\hat{C}_{Y X}=\\hat{C}\\hat{C}_{X},\\qquad\\hat{C}\\in S_{2}(\\mathcal{H},\\mathcal{Y}),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "which arises as the optimality condition for minimizers on $\\mathcal{G}$ of the empirical analogue of (3), given by $\\begin{array}{r}{\\mathcal{E}_{n}(F):=\\frac{1}{n}\\sum_{i=1}^{n}\\|\\dot{y_{i}}-F(\\dot{x_{i}})\\|_{\\mathcal{V}}^{2}}\\end{array}$ ; see Proposition 2 in the Appendix for a proof. For the vector-valued kernel given in (2), it is well-known that $\\hat{F}_{\\lambda}$ can be computed in closed-form for the ridge regression estimator\u2014even in infinite dimensions [47]. For general filter functions, an extended representer theorem is given by [2] in the context of finite-dimensional multitask learning: this approach works in infinite dimensions as well. We give the closed form solution based on [2] below (we include the proof in Appendix D.1). ", "page_idx": 8}, {"type": "text", "text": "Proposition 1 (Representer theorem for general spectral filter). Let $(\\mathbf{K})_{i j}=k(x_{i},x_{j})$ , $1\\leq i,j\\leq n$ denote the Gram matrix associated to the scalar-valued kernel $k$ . We have ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\hat{F}_{\\lambda}(x)=\\sum_{i=1}^{n}y_{i}\\alpha_{i}(x),\\qquad\\alpha(x)=\\frac{1}{n}g_{\\lambda}\\left(\\frac{\\mathbf{K}}{n}\\right)\\mathbf{k}_{x}\\in\\mathbb{R}^{n},\\qquad(\\mathbf{k}_{x})_{i}=k(x,x_{i}),\\quad1\\leq i\\leq n.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Example 1 (Conditional integration). Consider now a random variable $Z$ taking values in a topological space $\\mathcal{Z}$ on which we define a second RKHS $\\mathcal{H^{\\prime}}\\subseteq\\mathbb{R}^{\\mathcal{Z}}$ with kernel $\\ell:\\mathcal{Z}\\times\\mathcal{Z}\\to\\mathbb{R}$ and canonical feature map $\\psi:\\mathcal{Z}\\rightarrow\\mathcal{H}^{\\prime},z\\mapsto\\ell(z,\\cdot)$ . The conditional mean embedding [47, 20] is defined as ", "page_idx": 8}, {"type": "equation", "text": "$$\nF_{*}(x):=\\mathbb{E}[\\psi(Z)\\mid X=x],\\qquad x\\in\\mathcal{X}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "We immediately see the link with vector-valued regression with $Y\\ =\\ \\psi(Z)$ and $\\mathcal{V}\\,=\\,\\mathcal{H}^{\\prime}$ . The conditional mean embedding allows us to compute the conditional expectation of any element of $\\mathcal{H}^{\\prime}$ . Indeed, using the reproducing property, for $f\\in\\mathcal{H}^{\\prime}$ , we have for all $x\\in\\mathscr{X}$ , ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\,f(Z)\\,|\\,X=x]=\\langle\\,f,\\mathbb{E}[\\,\\psi(Z)\\,|\\,X=x]\\rangle_{\\mathcal{H}^{\\prime}}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Given a dataset $\\{(x_{i},z_{i})\\}_{i=1}^{n}{}^{6}$ and an estimate of the conditional mean embedding $F_{*}$ with a spectral algorithm $\\hat{F}_{\\lambda}$ as in Eq. (11), and substituting the formula in Eq. (12), we obtain $\\mathbb{E}[f(Z)\\mid X=x]\\approx$ $\\begin{array}{r}{\\langle f,\\hat{F}_{\\lambda}(x)\\rangle_{\\mathcal{H}^{\\prime}}=\\sum_{i=1}^{n}\\langle f,\\psi(z_{i})\\rangle_{\\mathcal{H}^{\\prime}}\\alpha_{i}(x)=\\mathbf{f}_{z}^{\\intercal}\\alpha(x)}\\end{array}$ , where $(\\mathbf{f}_{z})_{i}=f(z_{i})$ , $1\\leq i\\leq n$ . ", "page_idx": 8}, {"type": "text", "text": "Learning rates: We now give our main result, the learning rates for the difference between $[\\hat{F}_{\\lambda}]$ and $F_{*}$ in the interpolation norm, where $F_{\\lambda}$ and $\\hat{F}_{\\lambda}$ are given by (10) and (11) based on a general spectral filter satisfying Definition 2. The proof is deferred to Section C in the Appendix. ", "page_idx": 8}, {"type": "text", "text": "Theorem 4 (Upper learning rates). Let $\\hat{F}_{\\lambda}$ be an estimator based on a general spectral filter with qualification $\\rho\\:\\geq\\:0$ . Furthermore, let the conditions (EVD), (EMB), (MOM) be satisfied with $0<p\\leq\\alpha\\leq1$ . With $0\\leq\\gamma\\leq1$ , $i f$ (SRC) is satisfied with $\\gamma<\\beta\\leq2\\rho$ , we have ", "page_idx": 8}, {"type": "text", "text": "1. in the case $\\beta+p\\leq\\alpha,$ , let $\\lambda_{n}=\\Theta\\left(\\left(n/\\log^{\\theta}(n)\\right)^{-\\frac{1}{\\alpha}}\\right).$ for some $\\theta>1$ , for all $\\tau>\\log(6)$ and sufficiently large $n\\geq1$ , there is a constant $J>0$ independent of $n$ and $\\tau$ such that ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\left\\|\\left[{\\hat{F}}_{\\lambda_{n}}\\right]-F_{*}\\right\\|_{\\gamma}^{2}\\leq\\tau^{2}J\\left({\\frac{n}{\\log^{\\theta}n}}\\right)^{-{\\frac{\\beta-\\gamma}{\\alpha}}}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "is satisfied with $P^{n}$ -probability not less than $1-6e^{-\\tau}$ . ", "page_idx": 8}, {"type": "text", "text": "2. in the case $\\beta+p>\\alpha,$ , let $\\lambda_{n}=\\Theta\\left(n^{-{\\frac{1}{\\beta+p}}}\\right)\\!,$ for all $\\tau>\\log(6)$ and sufficiently large $n\\geq1$ , there is a constant $J>0$ independent of $n$ and $\\tau$ such that ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\left[\\hat{F}_{\\lambda_{n}}\\right]-F_{*}\\right\\|_{\\gamma}^{2}\\leq\\tau^{2}J n^{-\\frac{\\beta-\\gamma}{\\beta+p}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "is satisfied with $P^{n}$ -probability not less than $1-6e^{-\\tau}$ ", "page_idx": 9}, {"type": "text", "text": "Theorem 4 provides the upper rate for vector-valued spectral algorithms. In particular, in combination with the lower bound in Theorem 2, we see that vector-valued spectral algorithms with qualification $\\rho$ achieve an optimal learning rate when the smoothness $\\beta$ of the regression function is in the range $(\\alpha\\!-\\!p,2\\rho]$ . For algorithms with infinite $\\rho$ such as gradient descent and principal component regression, we confirm that they can exploit smoothness of the target function just as in the real-valued setting [3, 5, 30], while not suffering from saturation. For Tikhonov regularization, where $\\rho=1$ , the rates recover the state-of-the-art results from [31]. Finally, we point out that obtaining minimax optimal learning rates for $\\beta<\\alpha-p$ still remains challenging even in the real-valued output scenario. Note however that for a large variety of RKHS, $\\alpha$ is arbitrarily close to $p$ and we obtain optimal rates for the whole range $(0,2\\bar{\\rho}]$ : we refer to [31, 54] for a detailed discussion. ", "page_idx": 9}, {"type": "text", "text": "We provide a proof sketch for Theorem 4. The key technical challenge in extending the results of \u03b3\u2212norm is bounded as \u2225[ C\u02c6\u03bb \u2212C\u03bb]\u2225S2([H]\u03b3,Y) \u22643\u03bb\u2212\u03b32 \u2225( C\u02c6\u03bb \u2212C\u03bb) C\u02c6X12,\u03bb\u2225S2(H,Y)(s [31] to spectral filter functions lies in the analysis of the estimation error. The estimation error in ee Eq. (37) in Appendix C.3). We rely on the fact that $\\mathrm{Id}_{\\mathcal{H}}=\\hat{C}_{X}g_{\\lambda}(\\hat{C}_{X})+r_{\\lambda}(\\hat{C}_{X})$ (see Definition 2), to obtain the decomposition $\\hat{C}_{\\lambda}-C_{\\lambda}=\\left(\\hat{C}_{Y X}-C_{\\lambda}\\hat{C}_{X}\\right)g_{\\lambda}(\\hat{C}_{X})-C_{\\lambda}r_{\\lambda}(\\hat{C}_{X}),$ which yields two terms to be controlled, ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(\\hat{C}_{\\lambda}-C_{\\lambda}\\right)\\hat{C}_{X,\\lambda}^{\\frac{1}{2}}\\Bigg\\Vert_{S_{2}(\\mathcal{H},\\mathcal{Y})}\\leq\\underbrace{\\left\\Vert(\\hat{C}_{Y X}-C_{\\lambda}\\hat{C}_{X})g_{\\lambda}(\\hat{C}_{X})\\hat{C}_{X,\\lambda}^{\\frac{1}{2}}\\right\\Vert_{S_{2}(\\mathcal{H},\\mathcal{Y})}}_{(I)}+\\underbrace{\\left\\Vert C_{\\lambda}r_{\\lambda}(\\hat{C}_{X})\\hat{C}_{X,\\lambda}^{\\frac{1}{2}}\\right\\Vert_{S_{2}(\\mathcal{H},\\mathcal{Y})}}_{(I I)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "To control term (I), we use the definition of the filter function $\\left\\|\\hat{C}_{X,\\lambda}g_{\\lambda}(\\hat{C}_{X})\\right\\|_{\\mathbb{\\mathcal{H}}\\to\\mathcal{H}}\\lesssim1$ . Thus it suffices to control the term \u2225( C\u02c6\u03bbY X \u2212C\u03bb C\u02c6X)C\u2212X,21\u03bb\u2225S2(H,Y)= $\\begin{array}{r}{\\left\\|\\frac{1}{n}\\sum_{i=1}^{n}\\xi{\\left(x_{i},y_{i}\\right)}\\right\\|_{S_{2}(\\mathcal{H},\\mathcal{Y})}}\\end{array}$ , where $\\xi(x,y)=(y-C_{\\lambda}\\phi(x))\\otimes C_{X,\\lambda}^{-\\frac{1}{2}}\\phi(x)$ . We proceed by bounding $\\mathbb{E}[\\|\\xi(X,X)\\|_{S_{2}(\\mathcal{H},\\mathcal{Y})}^{m}]$ $m\\geq1$ , and then use Bernstein\u2019s inequality to derive the upper bound n \u2225( C\u02c6Y X \u2212C2\u03bb C\u02c6X)C\u2212X,21\u03bb\u2225S2(H,Y). To control term (II), Lemma 9 in Appendix C.1 shows that $(I I)\\lesssim\\left\\|\\hat{C}_{X,\\lambda}^{\\frac{1}{2}}r_{\\lambda}(\\hat{C}_{X})g_{\\lambda}(C_{X})C_{X}^{\\frac{\\beta+1}{2}}\\right\\|_{\\mathcal{H}\\rightarrow\\mathcal{H}}$ . The term on the right side is bounded in prior work on scalar-valued spectral method, and we refer the reader to [54, Theorem 16]. The results of Theorem 4 are then obtained by choosing regularization parameter $\\lambda=\\lambda(n)$ to optimally trade off approximation and estimation errors. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we have rigorously explored the theoretical properties of vector-valued spectral learning algorithms, focusing on their performance in infinite-dimensional output spaces. We first proved the saturation effect observed in vector-valued kernel ridge regression, highlighting its limitations in exploiting additional smoothness in regression functions. We then presented upper bounds on the finite sample risk for a general class of spectral learning algorithms, demonstrating their minimax optimality across various scenarios, including misspecified learning settings. ", "page_idx": 9}, {"type": "text", "text": "Our results open avenues for further research, particularly in developing more efficient implementations for practical use in high-dimensional machine learning problems such as causal inference and functional data analysis. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement: Dimitri Meunier, Arthur Gretton and Zhu Li were supported by the Gatsby Charitable Foundation. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] J.-P. Aubin. Applied Functional Analysis. John Wiley & Sons, Inc., 2nd edition, 2000.   \n[2] L. Baldassarre, L. Rosasco, A. Barla, and A. Verri. Multi-output learning via spectral filtering. Machine Learning, 87(3):259\u2013301, 2012.   \n[3] F. Bauer, S. Pereverzev, and L. Rosasco. On regularization algorithms in learning theory. Journal of Complexity, 23(1):52\u201372, 2007.   \n[4] A. Berlinet and C. Thomas-Agnan. Reproducing Kernel Hilbert Spaces in Probability and Statistics. Springer, 2011.   \n[5] G. Blanchard and N. M\u00fccke. Optimal rates for regularization of statistical inverse learning problems. Foundations of Computational Mathematics, 18(4):971\u20131013, 2018.   \n[6] A. Caponnetto and E. De Vito. Optimal rates for the regularized least-squares algorithm. Foundations of Computational Mathematics, 7(3):331\u2013368, 2007.   \n[7] A. Caponnetto, C. A. Micchelli, M. Pontil, and Y. Ying. Universal multi-task kernels. Journal of Machine Learning Research, 9:1615\u20131646, 2008. [8] A. Caponnetto and Y. Yao. Cross-validation based adaptation for regularization operators in learning theory. Analysis and Applications, 8(02):161\u2013183, 2010.   \n[9] C. Carmeli, E. De Vito, and A. Toigo. Vector valued reproducing kernel Hilbert spaces of integrable functions and Mercer theorem. Analysis and Applications, 4(04):377\u2013408, 2006.   \n[10] C. Carmeli, E. De Vito, A. Toigo, and V. Umanit\u00e1. Vector valued reproducing kernel Hilbert spaces and universality. Analysis and Applications, 8(01):19\u201361, 2010.   \n[11] C. Ciliberto, L. Rosasco, and A. Rudi. A consistent regularization approach for structured prediction. Advances in Neural Information Processing Systems, 29, 2016.   \n[12] C. Ciliberto, L. Rosasco, and A. Rudi. A general framework for consistent structured prediction with implicit loss embeddings. Journal of Machine Learning Research, 21(1):3852\u20133918, 2020.   \n[13] E. De Vito, L. Rosasco, and A. Caponnetto. Discretization error analysis for tikhonov regularization. Analysis and Applications, 4(01):81\u201399, 2006.   \n[14] J. Diestel and J. Uhl. Vector Measures. American Mathematical Society, 1977.   \n[15] R. Dudley. Real Analysis and Probability. Cambridge University Press, 2nd edition edition, 2002.   \n[16] H. W. Engl, M. Hanke, and A. Neubauer. Regularization of Inverse Problems. Kluwer, 2000.   \n[17] S. Fischer and I. Steinwart. Sobolev norm learning rates for regularized least-squares algorithms. Journal Of Machine Learning Research, 21:205\u20131, 2020.   \n[18] J. I. Fujii, M. Fujii, T. Furuta, and R. Nakamoto. Norm inequalities equivalent to heinz inequality. Proceedings of the American Mathematical Society, 118(3):827\u2013830, 1993.   \n[19] L. L. Gerfo, L. Rosasco, F. Odone, E. D. Vito, and A. Verri. Spectral algorithms for supervised learning. Neural Computation, 20(7):1873\u20131897, 2008.   \n[20] S. Gr\u00fcnew\u00e4lder, G. Lever, L. Baldassarre, S. Patterson, A. Gretton, and M. Pontil. Conditional mean embeddings as regressors. In International Conference on Machine Mearning, pages 1803\u2014-1810, 2012.   \n[21] S. Gr\u00fcnew\u00e4lder, G. Lever, L. Baldassarre, M. Pontil, and A. Gretton. Modelling transition dynamics in MDPs with RKHS embeddings. In International Conference on Machine Mearning, pages 535\u2013542, 2012.   \n[22] T. Herdman, R. D. Spies, and K. G. Temperini. Global saturation of regularization methods for inverse ill-posed problems. Journal of optimization theory and applications, 148(1):164\u2013196, 2011.   \n[23] J. Jin, Y. Lu, J. Blanchet, and L. Ying. Minimax optimal kernel operator learning via multilevel training. In The Eleventh International Conference on Learning Representations, 2023.   \n[24] H. Kadri, E. Duflos, P. Preux, S. Canu, A. Rakotomamonjy, and J. Audiffren. Operator-valued kernels for learning from functional response data. Journal of Machine Learning Research, 17(20):1\u201354, 2016.   \n[25] V. Kostic, K. Lounici, P. Novelli, and M. Pontil. Sharp spectral rates for koopman operator learning. Advances in Neural Information Processing Systems, 36, 2024.   \n[26] V. Kostic, P. Novelli, A. Maurer, C. Ciliberto, L. Rosasco, and M. Pontil. Learning dynamical systems via Koopman operator regression in reproducing kernel Hilbert spaces. Advances in Neural Information Processing Systems, 35:4017\u20134031, 2022.   \n[27] S. Lanthaler and N. H. Nelsen. Error bounds for learning with vector-valued random features. Advances in Neural Information Processing Systems, 36, 2024.   \n[28] Y. Li, W. Gan, Z. Shi, and Q. Lin. Generalization error curves for analytic spectral algorithms under power-law decay. arXiv preprint arXiv:2401.01599, 2024.   \n[29] Y. Li, H. Zhang, and Q. Lin. On the saturation effect of kernel ridge regression. In The Eleventh International Conference on Learning Representations, 2023.   \n[30] Z. Li, D. Meunier, M. Mollenhauer, and A. Gretton. Optimal rates for regularized conditional mean embedding learning. In Advances in Neural Information Processing Systems, volume 35, pages 4433\u20134445, 2022.   \n[31] Z. Li, D. Meunier, M. Mollenhauer, and A. Gretton. Towards optimal sobolev norm rates for the vector-valued regularized least-squares algorithm. Journal of Machine Learning Research, 25(181):1\u201351, 2024.   \n[32] J. Lin and V. Cevher. Optimal distributed learning with multi-pass stochastic gradient methods. In International Conference on Machine Learning, pages 3092\u20133101. PMLR, 2018.   \n[33] J. Lin and V. Cevher. Optimal convergence for distributed learning with stochastic gradient methods and spectral algorithms. Journal of Machine Learning Research, 21(147):1\u201363, 2020.   \n[34] J. Lin, A. Rudi, L. Rosasco, and V. Cevher. Optimal rates for spectral algorithms with least-squares regression over Hilbert spaces. Applied and Computational Harmonic Analysis, 48(3):868\u2013890, 2020.   \n[35] A. Mastouri, Y. Zhu, L. Gultchin, A. Korba, R. Silva, M. Kusner, A. Gretton, and K. Muandet. Proximal causal learning with kernels: Two-stage estimation and moment restriction. In International Conference on Machine Mearning, pages 7512\u20137523. PMLR, 2021.   \n[36] P. Math\u00e9. Saturation of regularization methods for linear ill-posed problems in Hilbert spaces. SIAM journal on numerical analysis, 42(3):968\u2013973, 2004.   \n[37] M. Mollenhauer and P. Koltai. Nonparametric approximation of conditional expectation operators. arXiv preprint arXiv:2012.12917, 2020.   \n[38] M. Mollenhauer, N. M\u00fccke, and T. Sullivan. Learning linear operators: Infinite-dimensional regression as a well-behaved non-compact inverse problem. arXiv preprint arXiv:2211.08875, 2022.   \n[39] N. M\u00fccke, G. Neu, and L. Rosasco. Beating SGD saturation with tail-averaging and minibatching. In Advances in Neural Information Processing Systems, volume 32, 2019.   \n[40] A. Neubauer. On converse and saturation results for Tikhonov regularization of linear ill-posed problems. SIAM Journal On Numerical Analysis, 34(2):517\u2013527, 1997.   \n[41] J. Park and K. Muandet. A measure-theoretic approach to kernel conditional mean embeddings. Advances in Neural Information Processing Systems, 33:21247\u201321259, 2020.   \n[42] R. Singh, M. Sahani, and A. Gretton. Kernel instrumental variable regression. Advances in Neural Information Processing Systems, 32, 2019.   \n[43] R. Singh, L. Xu, and A. Gretton. Kernel methods for causal functions: dose, heterogeneous and incremental response curves. Biometrika, 111(2):497\u2013516, 2024.   \n[44] S. Smale and D.-X. Zhou. Shannon sampling and function reconstruction from point values. Bulletin of the American Mathematical Society, 41(3):279\u2013305, 2004.   \n[45] S. Smale and D.-X. Zhou. Shannon sampling II: Connections to learning theory. Applied and Computational Harmonic Analysis, 19(3):285\u2013302, 2005.   \n[46] S. Smale and D.-X. Zhou. Learning theory estimates via integral operators and their approximations. Constructive Approximation, 26(2):153\u2013172, 2007.   \n[47] L. Song, J. Huang, A. Smola, and K. Fukumizu. Hilbert space embeddings of conditional distributions with applications to dynamical systems. In International Conference on Machine Mearning, pages 961\u2013968, 2009.   \n[48] B. K. Sriperumbudur, K. Fukumizu, and G. R. Lanckriet. Universality, characteristic kernels and RKHS embedding of measures. Journal of Machine Learning Research, 12(Jul):2389\u20132410, 2011.   \n[49] I. Steinwart and A. Christmann. Support Vector Machines. Springer, 2008.   \n[50] I. Steinwart, D. R. Hush, and C. Scovel. Optimal rates for regularized least squares regression. In COLT, pages 79\u201393, 2009.   \n[51] I. Steinwart and C. Scovel. Mercer\u2019s theorem on general domains: On the interaction between measures, kernels, and RKHSs. Constructive Approximation, 35(3):363\u2013417, 2012.   \n[52] J. Weidmann. Linear Operators in Hilbert Spaces. Springer, 1980.   \n[53] Y. Yao, L. Rosasco, and A. Caponnetto. On early stopping in gradient descent learning. Constructive Approximation, 26(2):289\u2013315, 2007.   \n[54] H. Zhang, Y. Li, and Q. Lin. On the optimality of misspecified spectral algorithms. Journal of Machine Learning Research, 25(188):1\u201350, 2024.   \n[55] H. Zhang, Y. Li, W. Lu, and Q. Lin. On the optimality of misspecified kernel ridge regression. In International Conference on Machine Learning, pages 41331\u201341353. PMLR, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendices ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The appendix is organized as follows. In Section A, we give additional mathematical background and notations. In Section B, we give the proof of Theorem 3 and provide a technical comparison of our proof with [29]. In Section C, we prove Theorem 4. Finally, in Section D, we provide auxiliary results used in the main proofs. ", "page_idx": 13}, {"type": "text", "text": "A Additional Background ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Hilbert spaces and linear operators ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Definition 3 (Bochner $L_{q}$ \u2212spaces, [14]). Let $H$ be a separable Hilbert space and \u03c0 a probability measure on $\\mathcal{X}$ . For $1\\leq q\\leq\\infty$ , $L_{q}(\\mathcal{X},\\mathcal{F}_{\\mathcal{X}},\\pi;H)$ , abbreviated $L_{q}(\\pi;H)$ , is the space of strongly $\\mathcal{F}_{\\mathcal{X}}-\\mathcal{F}_{H}$ measurable and Bochner $q$ -integrable functions from $\\mathcal{X}$ to $H$ , with the norms ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\|f\\|_{L_{q}(\\pi;H)}^{q}=\\int_{\\mathcal{X}}\\|f\\|_{H}^{q}\\,\\mathrm{d}\\pi,\\quad1\\leq q<\\infty,\\qquad\\|f\\|_{L_{\\infty}(\\pi;H)}=\\operatorname*{inf}\\left\\{C\\geq0:\\pi\\{\\|f\\|_{H}>C\\}=0\\right\\}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Definition 4 $p$ -Schatten class, e.g. [52]). Let $H,H^{\\prime}$ be separable Hilbert spaces. For $1\\leq q\\leq\\infty$ , $S_{p}(H,H^{\\prime})$ , abbreviated $S_{p}(H)$ if $H=H^{\\prime}$ , is the Banach space of all compact operators $C$ from $H$ to $H^{\\prime}$ such that $\\Vert C\\Vert_{S_{p}\\left(H,H^{\\prime}\\right)}:=\\Vert\\left(\\sigma_{i}(C)\\right)_{i\\in I}\\Vert_{\\ell_{p}}$ is finite. Here $\\|\\left(\\sigma_{i}(C)\\right)_{i\\in I}\\|_{\\ell_{p}}$ is the $\\ell_{p}$ \u2212sequence space norm of the sequence of the strictly positive singular values of $C$ indexed by the at most countable set $I$ . For $p=2$ , we retrieve the space of Hilbert-Schmidt operators, for $p=1$ we retrieve the space of Trace Class operators, and for $p=+\\infty$ , $\\|\\cdot\\|_{S\\infty}(H,H^{\\prime})$ corresponds to the operator norm $\\|\\cdot\\|_{H\\rightarrow H^{\\prime}}$ . ", "page_idx": 13}, {"type": "text", "text": "Definition 5 (Tensor Product of Hilbert Spaces, [1]). Let $H,H^{\\prime}$ be Hilbert spaces. The Hilbert space $H\\otimes H^{\\prime}$ is the completion of the algebraic tensor product with respect to the norm induced by the inner product $\\langle x_{1}\\,\\bar{\\otimes}\\,x_{1}^{\\prime},x_{2}\\,\\bar{\\otimes}\\,x_{2}^{\\prime}\\rangle_{H\\otimes H^{\\prime}}=\\langle x_{1},x_{2}\\rangle_{H}\\bar{\\langle}x_{1}^{\\prime},x_{2}^{\\prime}\\rangle_{H^{\\prime}}$ for $x_{1},x_{2}\\in H$ and $x_{1}^{\\prime},x_{2}^{\\prime}\\in H^{\\prime}$ defined on the elementary tensors of $H\\otimes H^{\\prime}$ . This definition extends to $\\operatorname{span}\\{x\\otimes x^{\\prime}|x\\in H,x^{\\prime}\\in H^{\\prime}\\}$ and finally to its completion. The space $H\\otimes H^{\\prime}$ is separable whenever both $H$ and $H^{\\prime}$ are separable. If $\\{e_{i}\\}_{i\\in I}$ and $\\bar{\\{e_{j}^{\\prime}\\}_{j\\in J}}$ are orthonormal basis in $H$ and $H^{\\prime}$ , $\\{e_{i}\\otimes e_{j}^{\\prime}\\}_{i\\in I,j\\in J}$ is an orthonormal basis in $H\\otimes H^{\\prime}$ . ", "page_idx": 13}, {"type": "text", "text": "Theorem 5 (Isometric Isomorphism between $L_{2}(\\pi;\\mathcal{V})$ and $S_{2}(L_{2}(\\pi),\\mathcal{y})$ , Theorem 12.6.1 [1]). Let $H$ be a separable Hilbert space. The Bochner space $L_{2}(\\pi;H)$ is isometrically isomorphic to $S_{2}(L_{2}(\\pi),\\mathcal{y})$ and the isometric isomorphism is realized by the map $\\Psi:S_{2}(L_{2}(\\pi),\\dot{\\mathcal{V}})\\rightarrow L_{2}\\dot{(}\\pi;H)$ acting on elementary tensors as $\\Psi(f\\otimes y)=(\\omega\\rightarrow f(\\omega)y)$ . ", "page_idx": 13}, {"type": "text", "text": "A.2 RKHS embbedings into $L_{2}$ and Well-specifiedness ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Recall that $I_{\\pi}:\\mathcal{H}\\rightarrow L_{2}(\\pi)$ is the embedding that maps every function in $\\mathcal{H}$ into its $\\pi$ -equivalence class in $L_{2}(\\pi)$ and that we used the shorthand notation $\\bar{[f]}=\\bar{I_{\\pi}}(f)$ for all $f\\in\\mathcal H$ . We define similarly $\\mathcal{T}_{\\pi}:\\mathcal{G}\\rightarrow L_{2}(\\pi;\\mathcal{V})$ as the embedding that maps every function in $\\mathcal{G}$ into its $\\pi$ -equivalence class in $L_{2}(\\pi;\\mathcal{V})$ . ", "page_idx": 13}, {"type": "text", "text": "Definition 6 (Embedding $\\mathcal{G}$ into $L_{2}(\\pi;\\mathcal{V}))$ . Let ${\\cal Z}_{\\pi}:={\\cal I}_{\\mathcal{Y}}\\otimes{\\cal I}_{\\pi}$ be the tensor product of the operator $\\operatorname{Id}_{\\mathcal{Y}}$ with the operator $I_{\\pi}$ (see $I I$ , Definition 12.4.1.] for the definition of tensor product of operators). ${\\mathcal{Z}}_{\\pi}$ maps every function in $\\mathcal{G}$ into its $\\pi$ -equivalence class in $L_{2}(\\pi;\\mathcal{V})$ . We then use the shorthand notation $[F]={\\mathcal{T}}_{\\pi}(F)$ for all $F\\in{\\mathcal{G}}$ . ", "page_idx": 13}, {"type": "text", "text": "Remark 5. Let $\\{d_{j}\\}_{j\\in J}$ be an orthonormal basis of $\\boldsymbol{\\mathscr{y}}$ and recall that $\\{\\sqrt{\\mu_{i}}[e_{i}]\\}_{i\\in I}$ forms an orthonormal basis of $[\\mathcal{H}]^{1}$ . Let $F\\,\\,\\in\\,\\,[\\mathcal{G}]^{1}$ . Then $F$ can be represented as the element $\\begin{array}{r}{C:=\\sum_{i\\in I,j\\in J}a_{i j}d_{j}\\otimes\\sqrt{\\mu_{i}}\\bar{[}e_{i}\\bar{]}}\\end{array}$ in $S_{2}([\\mathcal{H}]^{1},\\tilde{y)}$ by definition of $[\\mathcal{G}]^{1}$ with $\\begin{array}{r}{\\|C\\|_{1}^{2}=\\sum_{i,j}a_{i j}^{2}}\\end{array}$ . Hence defining $\\begin{array}{r}{\\bar{C}:=\\sum_{i\\in I,j\\in J}a_{i j}d_{j}\\otimes\\sqrt{\\mu_{i}}e_{i}}\\end{array}$ we have ${\\cal C}=\\bar{\\cal C}\\;\\pi\\!-\\!a.e$ . and ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\|\\bar{C}\\|_{\\mathcal{G}}^{2}=\\sum_{i\\in I,j\\in J}a_{i,j}^{2}=\\|C\\|_{1}^{2}<+\\infty.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Taking the elements identifying $\\bar{C}$ in $\\mathcal{G}$ gives a representer $\\bar{F}$ of $F$ in $\\mathcal{G}$ . ", "page_idx": 13}, {"type": "text", "text": "A.3 Additional Notations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In the following, we fix $\\{d_{j}\\}_{j\\in J}$ an orthonormal basis of $\\boldsymbol{\\wp}$ , where $J$ is at most countable. Recall that {\u00b5i ei}i\u2208I is an ONB of $(\\ker I_{\\pi})^{\\perp}$ in $\\mathcal{H}$ , and $\\{[e_{i}]\\}_{i\\in I}$ is an ONB of ran $\\overline{{I_{\\pi}}}$ in $L_{2}(\\pi)$ . Let $\\{\\tilde{e}_{i}\\}_{i\\in I^{\\prime}}$ be an ONB of $\\ker I_{\\pi}$ (with $I\\cap I^{\\prime}=\\emptyset_{\\mathrm{,}}$ ), then $\\left\\{\\mu_{i}^{1/2}e_{i}\\right\\}_{i\\in I}\\cup\\left\\{\\tilde{e}_{i}\\right\\}_{i\\in I^{\\prime}}$ forms an ONB of $\\mathcal{H}$ , and $\\left\\{d_{j}\\otimes\\mu_{i}^{1/2}e_{i}\\right\\}_{i\\in I,j\\in J}\\cup\\{d_{j}\\otimes\\tilde{e}_{i}\\}_{i\\in I^{\\prime},j\\in J}$ forms an ONB of $\\mathcal{Y}\\otimes\\mathcal{H}\\simeq\\mathcal{G}$ . ", "page_idx": 14}, {"type": "text", "text": "For any Hilbert space $H$ , linear operator $T:H\\rightarrow H$ and scalar $\\lambda>0$ , we define $T_{\\lambda}:=T+\\lambda I_{H}$ . ", "page_idx": 14}, {"type": "text", "text": "B Saturation Effect with Tikhonov Regularization - Proof of Theorem 3 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In the following proofs a quantity $h_{n}\\geq0$ depending on $n\\geq1$ , but independent of $\\tau$ the confidence level, is equal to $o(1)$ if $h_{n}\\to0$ when $n\\to+\\infty$ . ", "page_idx": 14}, {"type": "text", "text": "We will make extensive use of the following notation in the subsequent analysis. ", "page_idx": 14}, {"type": "text", "text": "Definition 7 (Empirical $L_{2}(\\pi)$ \u2212norm). Denoted by $\\langle\\cdot,\\cdot\\rangle_{2,n}$ , the empirical $L_{2}(\\pi)$ \u2212norm associated to points $\\{x_{i}\\}_{i=1}^{n}$ independently and identically sampled from the distribution of $X$ , is defined as, for any $f,g\\in\\mathcal{H}$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left\\langle f,g\\right\\rangle_{2,n}:=\\left\\langle\\hat{C}_{X},f\\otimes g\\right\\rangle_{S_{2}(\\mathcal{H})}=\\left\\langle\\hat{C}_{X}f,g\\right\\rangle_{\\mathcal{H}}=\\left\\langle\\hat{C}_{X}^{\\frac{1}{2}}f,\\hat{C}_{X}^{\\frac{1}{2}}g\\right\\rangle_{\\mathcal{H}}=\\frac{1}{n}\\sum_{i=1}^{n}f\\big(x_{i}\\big)g\\big(x_{i}\\big).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "This induces an inner product on $\\mathcal{H}$ , with associated norm, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|f\\|_{2,n}^{2}=\\langle f,f\\rangle_{2,n}={\\frac{1}{n}}\\sum_{i=1}^{n}f{\\big(}x_{i}{\\big)}^{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Definition 8. Fix $x\\in\\mathscr{X}$ and $\\lambda>0$ . The regularized canonical feature map is defined as ", "page_idx": 14}, {"type": "equation", "text": "$$\nf_{x,\\lambda}(\\cdot)=C_{X,\\lambda}^{-1}k(x,\\cdot):\\mathcal{X}\\to\\mathcal{H}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Recall from Eq. (11) that the ridge estimator $\\hat{F}_{\\lambda}$ defined in Eq. (7) can be expressed as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\hat{C}_{\\lambda}=\\hat{C}_{Y X}g_{\\lambda}(\\hat{C}_{X}),\\qquad\\hat{F}_{\\lambda}(\\cdot)=\\hat{C}_{\\lambda}\\phi(\\cdot)\\in\\mathcal{G},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where in Theorem 3 we focus on Tikhonov regularization where $g_{\\lambda}(x)=(x+\\lambda)^{-1}$ . In that setting we have ", "page_idx": 14}, {"type": "equation", "text": "$$\nr_{\\lambda}\\left(x\\right):=1-\\frac{x}{x+\\lambda}=-\\frac{\\lambda}{x+\\lambda}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof of Theorem 3. Since $\\beta\\geq2$ , $F_{*}\\,\\in\\,[{\\mathcal G}]^{\\beta}\\,\\subseteq\\,[{\\mathcal G}]^{1}$ , therefore $F_{*}$ has a representer $\\bar{F}$ in $\\mathcal{G}$ such that $\\stackrel{-}{F_{*}}=\\bar{F}\\,\\pi$ -a.e. (see Remark 5), and by Theorem 1, $\\bar{F}(\\cdot)=\\bar{C}\\phi(\\cdot)$ , with $\\bar{C}\\in S_{2}(\\mathscr{H},\\mathscr{y})$ . Define the errors $\\epsilon_{i}\\;:=\\;y_{i}\\,-\\,{\\bar{C}}\\phi(x_{i})$ , $i\\;=\\;1,\\ldots,n$ , that are i.i.d samples with the same distribution as $\\epsilon:=Y-\\bar{C}\\phi(X)$ . By assumption $\\mathbb{E}\\left[\\|\\epsilon\\|_{\\mathcal{V}}^{2}\\mid X\\right]\\geq\\sigma^{2}$ and by definition $\\mathbb{E}\\left[\\boldsymbol{\\epsilon}\\mid\\boldsymbol{X}\\right]=\\boldsymbol{0}$ . By Eq. (13), we have ", "page_idx": 14}, {"type": "equation", "text": "$$\nr_{\\lambda}\\left(\\hat{C}_{X}\\right):=I-\\hat{C}_{X}\\hat{C}_{X,\\lambda}^{-1}=-\\lambda\\hat{C}_{X,\\lambda}^{-1}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The following bias-variance decomposition is the essence of the proof. In the following derivation we abbreviate $S_{2}(L_{2}(\\pi),\\mathcal{y})$ to $S_{2}\\,L_{2}(\\pi;\\mathcal{V})$ to $L_{2}$ and $x_{1},\\ldots,x_{n}$ to $\\underline{{x}}_{n}$ to save space. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\Big[\\big\\|\\bar{F}_{\\lambda}\\big]-F_{*}\\Big\\|_{L_{\\alpha}}^{2}\\,\\Big|\\,\\underline{{x}}_{n}\\Big]=\\mathbb{E}\\Big[\\big\\|\\big[\\bar{F}_{\\lambda}\\bar{\\mathbf{r}}\\big.\\hat{G}_{\\lambda,\\lambda}^{-1}-C\\big]\\big\\|_{S_{\\alpha}}^{2}\\,\\Big|\\,\\underline{{x}}_{n}\\Big]}\\\\ &{\\qquad\\qquad\\qquad=\\mathbb{E}\\Bigg[\\Bigg\\|\\bigg(\\frac{1}{n}\\textstyle\\frac{\\sum_{y}}{|\\underline{{\\mathbf{r}}}_{\\lambda}|}\\,\\theta\\otimes\\phi(x_{1})\\bigg)\\widehat{G}_{\\lambda,\\lambda}^{-1}-\\bar{C}\\bigg\\|\\bigg\\|_{S_{\\alpha}}^{2}\\,\\Big|\\,\\underline{{x}}_{n}\\Bigg]}\\\\ &{\\qquad\\qquad=\\mathbb{E}\\Bigg[\\bigg\\|\\bigg[\\frac{1}{n}\\textstyle\\frac{\\sum_{y}}{|\\underline{{\\mathbf{r}}}_{\\lambda}|}\\big(\\bar{C}\\phi(x_{1})+\\epsilon_{1}\\big)\\otimes\\phi(x_{1})\\widehat{C}_{\\lambda,\\lambda}^{-1}-\\bar{C}\\bigg)\\bigg\\|_{S_{\\alpha}}^{2}\\,\\Big|\\,\\underline{{x}}_{n}\\bigg]}\\\\ &{\\qquad\\qquad=\\mathbb{E}\\Bigg[\\bigg\\|\\bigg[\\bar{-}\\bar{C}r_{x}\\big.(\\bar{C}x_{\\mathcal{X}})+\\frac{1}{n}\\textstyle\\frac{n}{\\sum_{i}\\epsilon_{i}}\\,\\Theta\\left(\\widehat{C}_{\\lambda,\\lambda}^{-1}\\phi(x_{i})\\right)\\bigg]\\bigg\\|_{S_{\\alpha}}^{2}\\,\\Big|\\,\\underline{{x}}_{n}\\bigg]}\\\\ &{\\qquad\\qquad=\\mathbb{I}\\left[\\bar{C}r_{x}\\big.(\\bar{C}x_{\\mathcal{X}})\\right]\\|_{S_{\\alpha}}^{2}+\\frac{1}{n^{2}}\\,\\frac{n}{\\sum_{i}}\\mathbb{E}\\big[|\\epsilon_{1}|\\big\\|\\bar{y}_{\\lambda}^{-1}\\,x_{\\mathcal{X}}\\big)\\big\\|\\int_{L_{\\alpha}(\\pi)}^{\\lambda}}\\\\ &{\\qquad\\qquad\\geq\\lambda^{2}\\left\\|\\big[\\bar{C}\\widehat{C}\\widehat{\\mathbf{r}}_{\\lambda,\\lambda}^{-1}\\big]\\right\\|_{S_{\\alpha}}^{2}+\\frac\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The second term is a lower bound on the variance while the first term is a lower bound on the bias. ", "page_idx": 15}, {"type": "text", "text": "Bounding the Bias term. The idea is to first show that the population analogue of $\\left\\|\\left[\\bar{C}\\hat{C}_{X,\\lambda}^{-1}\\right]\\right\\|_{S_{2}(L_{2}(\\pi),\\mathcal{Y})}^{2}$ can be bounded below by a non-zero constant. We can then bound the difference between the empirical and population version of \u2225[ C\u00af C\u02c6\u2212X1,\u03bb]\u22252S2(L2(\u03c0),Y) u sing a concentration inequality. By Lemma 1, for $\\lambda>0$ , there is a constant $c>0$ (see Lemma 1 for the exact value of $c$ ) such that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\lVert\\left[\\bar{C}C_{X,\\lambda}^{-1}\\right]\\right\\rVert_{S_{2}(L_{2}(\\pi),\\mathcal{Y})}^{2}\\geq c>0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Furthermore by Lemma 2, there is a constant $c_{0}>0$ (see Lemma 2 for the exact value of $c_{0}$ ) such that for any $\\tau\\geq\\log(4)$ , with probability at least $1-4e^{-\\tau}$ , for $n\\geq\\left(c_{0}\\tau\\right)^{(4+2p)}$ and $1\\geq\\lambda\\geq n^{-\\frac{1}{2+p}}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\lVert[\\bar{C}C_{X,\\lambda}^{-1}]\\right\\rVert_{S_{2}(L_{2}(\\pi),y)}^{2}-\\left\\lVert[\\bar{C}\\hat{C}_{X,\\lambda}^{-1}]\\right\\rVert_{S_{2}(L_{2}(\\pi),y)}^{2}\\right\\rvert=\\tau^{2}o(1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, under the same high probability, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|[\\bar{C}\\hat{C}_{X,\\lambda}^{-1}]\\|_{S_{2}(L_{2}(\\pi),\\mathcal{Y})}^{2}\\geq c-\\tau^{2}o(1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "It leads to our final bound on the bias term, for a constant $\\rho_{2}\\geq0$ and for sufficiently large $n\\geq1$ , where the hidden index bound depends on $\\tau$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\lambda^{2}\\left\\|\\left[\\bar{C}\\hat{C}_{X,\\lambda}^{-1}\\right]\\right\\|_{S_{2}(L_{2}(\\pi),y)}^{2}\\geq\\rho_{1}\\lambda^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Bounding the Variance Term. Using the norm from Definition 7, we have the following chain of identities. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{\\sigma^{2}}{n^{2}}\\sum_{i=1}^{n}\\big\\|\\big[\\hat{C}_{X,\\lambda}^{-1}\\phi(x_{i})\\big]\\big\\|_{L_{2}(\\pi)}^{2}=\\frac{\\sigma^{2}}{n^{2}}\\sum_{i=1}^{n}\\int_{\\mathcal{X}}\\big\\langle\\phi(X),\\hat{C}_{X,\\lambda}^{-1}\\phi(x_{i})\\big\\rangle_{\\mathcal{H}}^{2}\\,d\\pi(x)}}\\\\ &{}&{\\qquad=\\displaystyle\\frac{\\sigma^{2}}{n}\\int_{\\mathcal{X}}\\|\\hat{C}_{X,\\lambda}^{-1}\\phi(X)\\|_{2,n}^{2}d\\pi(x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore it suffices to consider $\\begin{array}{r}{\\int_{\\mathcal{X}}\\|\\hat{C}_{X,\\lambda}^{-1}k(x,\\cdot)\\|_{2,n}^{2}d\\pi(x)}\\end{array}$ . ", "page_idx": 15}, {"type": "text", "text": "Combining the result of Lemma 4 and Lemma 5 we obtain that for $1\\geq\\lambda\\geq n^{-\\frac{1}{2+p}}$ with probability at least $1-6e^{-\\tau}$ , for $n\\geq(c_{0}\\tau)^{4+2p}$ , the following bounds hold simultaneously for all $x\\in\\mathscr{X}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\left\\|\\hat{C}_{X}^{\\frac{1}{2}}\\left(\\hat{C}_{X,\\lambda}^{-1}-C_{X,\\lambda}^{-1}\\right)k(x,\\cdot)\\right\\|_{\\mathcal{H}}\\leq\\tau o(1)}\\\\ {\\|[C_{X,\\lambda}^{-1}k(x,\\cdot)]\\|_{2,n}^{2}-\\displaystyle\\frac{1}{2}\\|[C_{X,\\lambda}^{-1}k(x,\\cdot)]\\|_{L_{2}(\\pi)}^{2}\\geq-\\tau o(1)}\\\\ {\\|[C_{X,\\lambda}^{-1}k(x,\\cdot)]\\|_{2,n}^{2}-\\displaystyle\\frac{3}{2}\\|[C_{X,\\lambda}^{-1}k(x,\\cdot)]\\|_{L_{2}(\\pi)}^{2}\\leq\\tau o(1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Fix $x\\,\\in\\,{\\mathcal{X}}$ . Using the algebraic identity $a^{2}\\mathrm{~-~}b^{2}\\;=\\;{\\bigl(}a\\mathrm{~-~}b{\\bigr)}{\\bigl(}2b+{\\bigl(}a\\mathrm{~-~}b{\\bigr)}{\\bigr)}$ , and recalling that by Definition 7, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|f\\|_{2,n}^{2}=\\left\\|\\hat{C}_{X}^{\\frac{1}{2}}f\\right\\|_{\\mathcal{H}}^{2},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "we deduce ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\left\\|\\hat{C}_{X}^{\\frac{1}{2}}\\hat{C}_{X,\\lambda}^{-1}k(x,\\cdot)\\right\\|_{\\mathcal{H}}^{2}-\\left\\|\\hat{C}_{X}^{\\frac{1}{2}}C_{X,\\lambda}^{-1}k(x,\\cdot)\\right\\|_{\\mathcal{H}}^{2}\\right|}\\\\ &{\\leq\\left\\|\\hat{C}_{X}^{\\frac{1}{2}}\\left(\\hat{C}_{X,\\lambda}^{-1}-C_{X,\\lambda}^{-1}\\right)k(x,\\cdot)\\right\\|_{\\mathcal{H}}\\cdot\\left(\\left\\|\\hat{C}_{X}^{\\frac{1}{2}}\\left(\\hat{C}_{X,\\lambda}^{-1}-C_{X,\\lambda}^{-1}\\right)k(x,\\cdot)\\right\\|_{\\mathcal{H}}+2\\left\\|C_{X,\\lambda}^{-1}k(x,\\cdot)\\right\\|_{2,n}\\right)}\\\\ &{\\leq\\tau o(1)\\left(\\tau o(1)+2\\left\\|C_{X,\\lambda}^{-1}k(x,\\cdot)\\right\\|_{2,n}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Using Definition 7 again, this reads ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\hat{C}_{X,\\lambda}^{-1}k(x,\\cdot)\\right\\|_{2,n}^{2}\\geq\\left\\|C_{X,\\lambda}^{-1}k(x,\\cdot)\\right\\|_{2,n}^{2}-\\tau o(1)\\left(\\tau o(1)+2\\left\\|C_{X,\\lambda}^{-1}k(x,\\cdot)\\right\\|_{2,n}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|C_{X,\\lambda}^{-1}k(x,\\cdot)\\|_{2,n}^{2}\\leq\\frac{3}{2}\\|[C_{X,\\lambda}^{-1}k(x,\\cdot)]\\|_{L_{2}(\\pi)}^{2}+\\tau o(1)\\leq\\Big(\\sqrt{1.5}\\|[C_{X,\\lambda}^{-1}k(x,\\cdot)]\\|_{L_{2}(\\pi)}+\\sqrt{\\tau}o(1)\\Big)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Hence, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\Vert\\hat{C}_{X,\\lambda}^{-1}k(x,\\cdot)\\right\\Vert_{2,n}^{2}\\geq\\left\\Vert C_{X,\\lambda}^{-1}k(x,\\cdot)\\right\\Vert_{2,n}^{2}-\\tau o(1)\\left(\\left\\Vert\\left[C_{X,\\lambda}^{-1}k(x,\\cdot)\\right]\\right\\Vert_{L_{2}(\\pi)}+\\sqrt{\\tau}o(1)+\\tau o(1)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\geq\\frac{1}{2}\\|[C_{X,\\lambda}^{-1}k(x,\\cdot)]\\|_{L_{2}(\\pi)}^{2}-\\tau o(1)}\\\\ &{\\qquad\\qquad\\qquad\\quad-\\tau o(1)\\left(\\|[C_{X,\\lambda}^{-1}k(x,\\cdot)\\right]\\|_{L_{2}(\\pi)}+\\sqrt{\\tau}o(1)+\\tau o(1)\\right)}\\\\ &{\\qquad\\qquad\\geq\\frac{1}{2}\\|[C_{X,\\lambda}^{-1}k(x,\\cdot)]\\|_{L_{2}(\\pi)}^{2}-\\tau^{2}o(1)-\\tau o(1)\\|[C_{X,\\lambda}^{-1}k(x,\\cdot)]\\|_{L_{2}(\\pi)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By Lemma 17, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\int_{X}\\left\\|[C_{X,\\lambda}^{-1}k(x,\\cdot)]\\right\\|_{L_{2}(\\pi)}^{2}d\\pi(x)=\\mathcal{N}_{2}(\\lambda).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Furthermore, by Jensen\u2019s inequality, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\int_{\\mathcal{X}}\\|[C_{X,\\lambda}^{-1}k(x,\\cdot)]\\|_{L_{2}(\\pi)}^{2}\\mathrm{d}\\pi(x)\\geq\\left(\\int_{\\mathcal{X}}\\|[C_{X,\\lambda}^{-1}k(x,\\cdot)]\\|_{L_{2}(\\pi)}\\mathrm{d}\\pi(x)\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Recall from Lemma 16 that ", "page_idx": 16}, {"type": "equation", "text": "$$\nc_{1,2}\\lambda^{-p}\\leq\\mathcal{N}_{2}(\\lambda)\\leq c_{2,2}\\lambda^{-p}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int_{\\mathcal{X}}\\|[C_{{X},\\lambda}^{-1}k(x,\\cdot)]\\|_{L_{2}(\\pi)}\\mathrm{d}\\pi(x)\\le\\sqrt{c_{2,2}}\\lambda^{-\\frac{p}{2}}}\\\\ &{\\int_{\\mathcal{X}}\\|[C_{{X},\\lambda}^{-1}k(x,\\cdot)]\\|_{L_{2}(\\pi)}^{2}\\mathrm{d}\\pi(x)\\ge c_{1,2}\\lambda^{-p}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Hence ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\int_{\\mathcal{X}}\\|\\hat{C}_{X,\\lambda}^{-1}k(x,\\cdot)\\|_{2,n}^{2}d\\pi(x)\\geq\\frac{c_{1,2}}{2}\\lambda^{-p}-\\tau^{2}o(1)-\\tau o(1)\\sqrt{c_{2,2}}\\lambda^{-\\frac{p}{2}}}\\\\ &{}&{\\qquad\\qquad\\qquad\\geq\\bigg(\\frac{c_{1,2}}{2}-\\tau o(1)\\sqrt{c_{2,2}}\\bigg)\\,\\lambda^{-p}-\\tau^{2}o(1)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Combined with Eq. (15), it leads to our final bound on the variance term, for a constant $\\rho_{2}\\geq0$ and for sufficiently large $n\\geq1$ , where the hidden index bound depends on $\\tau$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\sigma^{2}}{n^{2}}\\sum_{i=1}^{n}\\left\\|\\left[\\hat{C}_{X,\\lambda}^{-1}\\phi(x_{i})\\right]\\right\\|_{L_{2}(\\pi)}^{2}\\geq\\frac{\\rho_{2}}{n\\lambda^{p}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Putting it together. We are now ready to assemble the lower bounds on the variance and on the bias. For a fixed confidence parameter $\\tau\\geq\\log(10)$ , for sufficiently large $n>0$ , where the hidden index ", "page_idx": 16}, {"type": "text", "text": "bound depends on $\\tau$ , with probability at least $1-10e^{-\\tau}$ , we have by Eq. (14) and Eq. (16), that for $\\lambda=\\lambda(n)$ satisfying $1\\geq\\lambda\\geq n^{-\\frac{1}{2+p}}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert[\\hat{F}_{\\lambda}]-F_{*}\\right\\Vert_{L_{2}(\\pi;y)}^{2}\\big|\\;x_{1},\\ldots,x_{n}\\right]\\ge\\rho_{1}\\lambda^{2}+\\rho_{2}n^{-1}\\lambda^{-p}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\rho_{1},\\rho_{2}$ have no dependence on $n$ . Recall Young\u2019s inequality, for $r,q>1$ satisfying $r^{-1}+q^{-1}=1$ , we have for all $a,b\\ge0$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\na+b\\geq r^{\\frac{1}{r}}q^{\\frac{1}{q}}a^{\\frac{1}{r}}b^{\\frac{1}{q}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We apply Young\u2019s inequality with $r^{-1}=p/(2+p)$ and $q^{-1}=2/(2+p)$ , there exists a constant $c_{1}>0$ such that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\rho_{1}\\lambda^{2}+\\rho_{2}n^{-1}\\lambda^{-p}\\ge c_{1}\\left(\\lambda^{2}\\right)^{\\frac{p}{2+p}}\\left(\\lambda^{-p}n^{-1}\\right)^{\\frac{2}{2+p}}=c_{1}n^{-\\frac{2}{2+p}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "To conclude the proof, let $\\lambda=\\lambda(n)$ be an arbitrary choice of regularization parameter satisfying $\\lambda(n)\\rightarrow0$ . We have just covered the case $1\\geq\\lambda\\geq\\dot{n}^{-\\frac{1}{2+p}}$ and the case $0<\\lambda\\leq n^{-\\frac{1}{2+p}}$ is covered by [29, Section B.4]. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Lemma 1. For any $\\lambda\\leq1$ and $C\\in S_{2}(\\mathscr{H},\\mathscr{y})$ , with $C\\pm\\,S_{2}(\\overline{{r a n\\;S_{\\pi}}},y)^{\\intercal}$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left\\Vert\\left[C C_{X,\\lambda}^{-1}\\right]\\right\\Vert_{S_{2}(L_{2}(\\pi),y)}^{2}\\geq\\sum_{i\\in I,j\\in J}a_{i j}^{2}\\frac{\\mu_{i}}{(\\mu_{i}+1)^{2}}>0,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with $a_{i j}:=\\langle d_{j},C\\sqrt{\\mu_{i}}e_{i}\\rangle_{\\mathcal{Y}},\\,i\\in I,j\\in J.$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. Recell the notations of Section A.3. Define $\\{a_{i j}\\}_{i\\in I\\cap I^{\\prime},j\\in J}$ such that $a_{i j}:=\\langle d_{j},C\\sqrt{\\mu_{i}}e_{i}\\rangle_{\\mathcal{V}}$ for $i\\in I,j\\in J$ and $a_{i j}:=\\langle d_{j},C\\tilde{e}_{i}\\rangle_{\\mathcal{V}}$ for $i\\in I^{\\prime},j\\in J$ . Then, on one hand, since $\\bar{C}\\in S_{2}(\\mathscr{H},\\dot{\\mathscr{y}})$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\nC=\\sum_{i\\in I,j\\in J}a_{i j}d_{j}\\otimes\\left({\\sqrt{\\mu_{i}}}e_{i}\\right)+\\sum_{i\\in I^{\\prime},j\\in J}a_{i j}d_{j}\\otimes{\\tilde{e}}_{i}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "On the other hand, ", "page_idx": 17}, {"type": "equation", "text": "$$\nC_{X,\\lambda}^{-1}=\\sum_{i\\in I}(\\mu_{i}+\\lambda)^{-1}(\\sqrt{\\mu_{i}}e_{i})\\otimes(\\sqrt{\\mu_{i}}e_{i})+\\lambda^{-1}\\sum_{i\\in I^{\\prime}}\\tilde{e}_{i}\\otimes\\tilde{e}_{i}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, noting that $\\tilde{e}_{i}=0\\;\\pi{-}\\mathrm{a.e}$ . for all $i\\in I^{\\prime}$ , we have, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left[C C_{X,\\lambda}^{-1}\\right]=\\left[\\sum_{i\\in I,j\\in J}a_{i j}(\\mu_{i}+\\lambda)^{-1}d_{j}\\otimes\\left(\\sqrt{\\mu_{i}}e_{i}\\right)+\\sum_{i\\in I^{\\prime},j\\in J}\\frac{a_{i j}}{\\lambda}d_{j}\\otimes\\tilde{e}_{i}\\right]}\\\\ {\\displaystyle=\\sum_{i\\in I,j\\in J}a_{i j}\\frac{\\sqrt{\\mu_{i}}}{\\mu_{i}+\\lambda}d_{j}\\otimes\\left[e_{i}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore the $S_{2}(L_{2}(\\pi),\\mathcal{y})$ -norm can be evaluated in closed form using Parseval\u2019s identity, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\big\\|\\big[C C_{X,\\lambda}^{-1}\\big]\\big\\|_{S_{2}(L_{2}(\\pi),y)}^{2}=\\sum_{i\\in I,j\\in J}a_{i j}^{2}\\frac{\\mu_{i}}{(\\mu_{i}+\\lambda)^{2}}\\ge\\sum_{i\\in I,j\\in J}a_{i j}^{2}\\frac{\\mu_{i}}{(\\mu_{i}+1)^{2}},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we used that $\\{d_{j}\\otimes[e_{i}]\\}_{j\\in J,i\\in I}$ is orthonormal in $\\mathfrak{V}\\otimes L_{2}(\\pi)$ , and $\\lambda\\leq1$ . The right hand side has no dependence on $\\lambda$ or $n$ . Furthermore, under assumption $\\mathrm{(EVD+)}$ , $\\mu_{i}>0$ for all $i\\in I$ , therefore the right hand side term equals zero if and only if $a_{i j}=0$ for all $i\\in I,j\\in J$ . Since by assumption $C\\pm\\,S_{2}(\\overline{{\\operatorname{ran}\\,S_{\\pi}}},\\mathcal{Y})$ , the right hand side is strictly positive. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Lemma 2. Suppose Assumption (EVD) holds with $p\\in(0,1]$ . Let $C\\in S_{2}(\\mathscr{H},\\mathscr{y})$ such that $\\left[C\\right]\\in$ $S_{2}([\\mathcal{H}]^{2},\\mathcal{V})$ . There is a constant $c_{0}\\,>\\,0$ such that for any $\\tau\\geq\\log(4)$ , with probability at least $1-4e^{-\\tau}$ , for $n\\geq\\left(c_{0}\\tau\\right)^{(4+2p)}$ and $1\\geq\\lambda\\geq n^{-\\frac{1}{2+p}}$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\lVert\\left[C C_{X\\lambda}^{-1}\\right]\\right\\rVert_{S_{2}(L_{2}(\\pi),y)}^{2}-\\left\\lVert\\left[C\\hat{C}_{X\\lambda}^{-1}\\right]\\right\\rVert_{S_{2}(L_{2}(\\pi),y)}^{2}\\right\\rvert\\le\\tau^{2}o(1)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We have $c_{0}:=8\\kappa\\operatorname*{max}\\{\\sqrt{c_{2,1}},1\\}.$ ) where $c_{2,1}$ is defined in Lemma $^ \u1e0a I6 \u1e0c$ . ", "page_idx": 17}, {"type": "text", "text": "$\\pm$ is the notation for \u201cnot being orthogonal to\u201d. ", "page_idx": 17}, {"type": "text", "text": "Proof. Using the identity $A^{-1}-B^{-1}=A^{-1}(B-A)B^{-1}$ , we obtain ", "page_idx": 18}, {"type": "equation", "text": "$$\nC_{X,\\lambda}^{-1}-\\hat{C}_{X,\\lambda}^{-1}=C_{X,\\lambda}^{-1}\\big(\\hat{C}_{X}-C_{X}\\big)\\hat{C}_{X,\\lambda}^{-1}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We apply Lemma 22 with $\\gamma=0$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\|\\left[C\\left(C_{X,\\lambda}^{-1}-\\widehat{C}_{X,\\lambda}^{-1}\\right)\\right]\\right\\|_{S_{2}(L_{2}(\\pi),y)}=\\left\\|\\left[C\\hat{C}_{X,\\lambda}^{-1}(\\hat{C}_{X}-C_{X})C_{X,\\lambda}^{-1}\\right]\\right\\|_{S_{2}(L_{2}(\\pi),y)}}&{}\\\\ {=\\left\\|C\\hat{C}_{X,\\lambda}^{-1}(\\hat{C}_{X}-C_{X})C_{X,\\lambda}^{-1}C_{X}^{\\frac{1}{2}}\\right\\|_{S_{2}(\\mathcal{H},y)}}&{}\\\\ {\\leq\\left\\|C\\hat{C}_{X,\\lambda}^{-\\frac{1}{2}}\\right\\|_{S_{2}(\\mathcal{H},y)}\\left\\|\\hat{C}_{X,\\lambda}^{-\\frac{1}{2}}C_{X,\\lambda}^{\\frac{1}{2}}\\right\\|_{\\mathcal{H}\\to\\mathcal{H}}}&{}\\\\ {\\cdot\\left\\|C_{X,\\lambda}^{-\\frac{1}{2}}(\\hat{C}_{X}-C_{X})C_{X,\\lambda}^{-\\frac{1}{2}}\\right\\|_{\\mathcal{H}\\to\\mathcal{H}}\\left\\|C_{X,\\lambda}^{-\\frac{1}{2}}C_{X}^{\\frac{1}{2}}\\right\\|_{\\mathcal{H}\\to\\mathcal{H}}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We consider each of the four terms in line (17). The last term is bounded above by 1 and the first term is bounded above by $\\lambda^{-\\frac{1}{2}}\\|C\\|_{S_{2}(\\mathcal{H},\\mathcal{V})}$ . By Lemma 20 applied with $s=1/2$ , we have for the second term ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\|\\hat{C}_{X,\\lambda}^{-\\frac{1}{2}}C_{X,\\lambda}^{\\frac{1}{2}}\\right\\|_{\\mathcal{H}\\to\\mathcal{H}}\\le\\left\\|\\hat{C}_{X,\\lambda}^{-1}C_{X,\\lambda}\\right\\|_{\\mathcal{H}\\to\\mathcal{H}}^{\\frac{1}{2}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then\u221a, by Lemma 18, for $\\tau\\ \\ \\geq\\ \\log(2)$ , with probability at least $1\\:-\\:2e^{-\\tau}$ , for $\\sqrt{n\\lambda}~\\ge$ $8\\tau\\kappa\\sqrt{\\operatorname*{max}\\{\\mathcal{N}(\\lambda),1\\}}$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\|\\hat{C}_{X,\\lambda}^{-1}C_{X,\\lambda}\\right\\|_{\\mathcal{H}\\to\\mathcal{H}}\\le2.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since $\\mathcal{N}(\\lambda)\\leq c_{2,1}\\lambda^{-p}$ by Lemma 16, and $\\lambda\\leq1$ , it suffices to verify that $\\lambda$ satisfies ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sqrt{n\\lambda}\\geq8\\tau\\kappa\\operatorname*{max}\\{\\sqrt{c_{2,1}},1\\}\\lambda^{-\\frac{p}{2}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since $\\lambda\\geq n^{-\\frac{1}{2+p}}$ by assumption, we deduce the sufficient condition $n\\geq(\\tau c_{0})^{2(2+p)}$ , where $c_{0}:=$ $8\\kappa\\operatorname*{max}\\{\\sqrt{c_{2,1}},1\\}$ . ", "page_idx": 18}, {"type": "text", "text": "We bound the third term using Lemma 16 [33]. For $\\tau\\geq\\log(2)$ , with probability at least $1-2e^{-\\tau}$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\lambda^{-\\frac12}\\left\\|C_{X,\\lambda}^{-\\frac12}(C_{X}-\\hat{C}_{X})C_{X,\\lambda}^{-\\frac12}\\right\\|_{\\mathcal{H}\\to\\mathcal{H}}\\le\\frac{4\\kappa^{2}\\xi_{\\delta}}{3n\\lambda^{\\frac{3}{2}}}+\\sqrt{\\frac{2\\kappa^{2}\\xi_{\\delta}}{n\\lambda^{2}}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where we define ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\xi_{\\delta}:=\\log\\frac{2\\kappa^{2}(\\mathcal{N}_{1}(\\lambda)+1)}{e^{-\\tau}\\|C_{X}\\|_{\\mathcal{H}\\rightarrow\\mathcal{H}}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By assumption $\\lambda\\geq n^{-\\frac{1}{2+p}}$ . We thus have ", "page_idx": 18}, {"type": "equation", "text": "$$\nn\\lambda^{\\frac{3}{2}}\\geq n^{\\frac{1+2p}{4+2p}}\\qquad\\mathrm{~and~}\\qquad n\\lambda^{2}\\geq n^{\\frac{p}{2+p}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "On the other hand, since $1\\geq\\lambda\\geq n^{-\\frac{1}{2+p}}$ , using Lemma 16, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\xi_{\\delta}\\leq\\log\\frac{82\\big(c_{2,1}\\lambda^{-p}+1\\big)}{e^{-\\tau}\\|C_{X}\\|_{\\mathcal H\\to\\mathcal H}}\\leq\\log\\frac{2\\big(c_{2,1}+1\\big)n^{\\frac{p}{2+p}}}{e^{-\\tau}\\|C_{X}\\|_{\\mathcal H\\to\\mathcal H}}\\leq\\log\\frac{2\\big(c_{2,l}+1\\big)}{e^{-\\tau}\\|C_{X}\\|_{\\mathcal H\\to\\mathcal H}}+\\frac{p}{2+p}\\log n.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The first term does not depend on $n$ , and the second term is logarithmic in $n$ . Putting everything together with a union bound, we get a bound on (17). With probability at least $1-4e^{-\\tau}$ , for $n\\geq\\left(c_{0}\\tau\\right)^{(4+2p)}$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\big\\|\\big[C\\left(C_{X,\\lambda}^{-1}-\\hat{C}_{X,\\lambda}^{-1}\\right)\\big]\\big\\|_{S_{2}(L_{2}(\\pi),y)}\\leq\\|C\\|_{S_{2}(\\mathcal{H},y)}\\sqrt{2}\\left(\\frac{4\\xi_{\\delta}}{3n^{\\frac{0.5+p}{2+p}}}+\\sqrt{\\frac{2\\xi_{\\delta}}{n^{\\frac{p}{2+p}}}}\\right)=\\tau o(1)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The derivations in the proof of Lemma 1 show that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left[C C_{X,\\lambda}^{-1}\\right]=\\sum_{i\\in I,j\\in J}a_{i j}\\frac{\\sqrt{\\mu_{i}}}{\\mu_{i}+\\lambda}d_{j}\\otimes[e_{i}],\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "with $a_{i j}:=\\langle d_{j},C\\sqrt{\\mu_{i}}e_{i}\\rangle_{\\mathcal{Y}},\\,i\\in I,j\\in J,$ . Note that since $[C]\\in S_{2}([\\mathcal{H}]^{2},\\mathcal{Y})$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|[C]\\|_{S_{2}([\\mathcal{H}]^{2},y)}^{2}=\\left\\|\\sum_{i\\in I,j\\in J}a_{i j}d_{j}\\otimes\\left(\\sqrt{\\mu_{i}}e_{i}\\right)\\right\\|_{S_{2}([\\mathcal{H}]^{2},y)}^{2}=\\sum_{i\\in I,j\\in J}\\frac{a_{i j}^{2}}{\\mu_{i}}<+\\infty.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Hence, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\big\\|\\big[C C_{X,\\lambda}^{-1}\\big]\\big\\|_{S_{2}(L_{2}(\\pi),y)}^{2}=\\sum_{i\\in I,j\\in J}a_{i j}^{2}\\frac{\\mu_{i}}{(\\mu_{i}+\\lambda)^{2}}\\leq\\sum_{i\\in I,j\\in J}\\frac{a_{i j}^{2}}{\\mu_{i}}=\\|\\big[C\\big]\\|_{S_{2}([\\mathcal{H}]^{2},y)}^{2}<+\\infty.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Using the equality $a^{2}\\mathrm{~-~}b^{2}\\mathrm{~=~}\\bigl(a\\mathrm{~-~}b\\bigr)\\bigl(a\\mathrm{~+~}b\\bigr)$ and the reverse triangular inequality, we obtain the following bound, with probability at least $1-4e^{-\\tau}$ , for $n\\geq\\left(c_{0}\\tau\\right)^{(4+2p)}$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\Big|\\left\\|[C C_{X,\\lambda}^{-1}]\\right\\|_{S_{2}(L_{2}(\\pi),y)}^{2}-\\left\\|[C\\hat{C}_{X,\\lambda}^{-1}]\\right\\|_{S_{2}(L_{2}(\\pi),y)}^{2}\\Big|}\\\\ &{\\leq\\left\\|\\left[C\\left(C_{X,\\lambda}^{-1}-\\hat{C}_{X,\\lambda}^{-1}\\right)\\right]\\right\\|_{S_{2}(L_{2}(\\pi),y)}\\left(\\left\\|[C C_{X,\\lambda}^{-1}]\\right\\|_{S_{2}(L_{2}(\\pi),y)}+\\left\\|[C\\hat{C}_{X,\\lambda}^{-1}]\\right\\|_{S_{2}(L_{2}(\\pi),y)}\\right)}\\\\ &{\\leq\\tau o(1)\\left(2\\left\\|[C C_{X,\\lambda}^{-1}]\\right\\|_{S_{2}(L_{2}(\\pi),y)}+\\left\\|\\left[C\\left(C_{X,\\lambda}^{-1}-\\hat{C}_{X,\\lambda}^{-1}\\right)\\right]\\right\\|_{S_{2}(L_{2}(\\pi),y)}\\right)}\\\\ &{\\leq\\tau o(1)\\left(2\\left\\|[C]\\right\\|_{S_{2}([\\mathcal{H}]^{2},y)}+\\tau o(1)\\right)}\\\\ &{=\\tau^{2}o(1),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where in the second last line we used Equation (18). ", "page_idx": 19}, {"type": "text", "text": "Lemma 3. Fix $x\\in\\mathscr{X}$ and $f_{x,\\lambda}$ as in Definition 8. For $\\tau\\geq\\log(2)$ , with probability at least $1-2e^{-\\tau}$ (note that this event depends on $x$ ), ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\big|\\|f_{x,\\lambda}\\big\\|_{2,n}^{2}-\\big\\|\\big[f_{x,\\lambda}\\big]\\big\\|_{L_{2}(\\pi)}^{2}\\big|\\le\\frac{1}{2}\\|\\big[f_{x,\\lambda}\\big]\\|_{L_{2}(\\pi)}^{2}+\\frac{5\\tau\\kappa^{2}}{3\\lambda^{2}n}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. We start with ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|f_{x,\\lambda}\\|_{\\infty}\\leq\\kappa\\|f_{x,\\lambda}\\|_{\\mathcal H}\\leq\\kappa^{2}\\lambda^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We apply Proposition 3 to $f\\,=\\,f_{x,\\lambda}$ , with $M\\,=\\,\\kappa^{2}\\lambda^{-1}$ . For $\\tau\\geq\\log(2)$ , with probability at least $1-2e^{-\\tau}$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\big|\\|f_{x,\\lambda}\\big\\|_{2,n}^{2}-\\big\\|\\big[f_{x,\\lambda}\\big]\\big\\|_{L_{2}(\\pi)}^{2}\\big|\\le\\frac{1}{2}\\|\\big[f_{x,\\lambda}\\big]\\|_{L_{2}(\\pi)}^{2}+\\frac{5\\tau\\kappa^{2}}{3\\lambda^{2}n}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Lemma 4. Suppose that $\\mathcal{X}$ is a compact set in $\\mathbb{R}^{d}$ and that $k\\in C^{\\theta}({\\mathcal{X}}\\times{\\mathcal{X}})$ for $\\theta\\in(0,1]$ (Definition $_{l l}$ ). Assume that $1\\geq\\lambda\\geq n^{-\\frac{1}{2+p}}$ . With probability at least $1-2e^{-\\tau}$ , it holds for all $x\\in\\mathscr{X}$ simultaneously that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|C_{X,\\lambda}^{-1}k(x,\\cdot)\\|_{2,n}^{2}\\geq\\displaystyle\\frac{1}{2}\\|[C_{X,\\lambda}^{-1}k(x,\\cdot)]\\|_{L_{2}(\\pi)}^{2}-\\tau o(1),}\\\\ &{\\|C_{X,\\lambda}^{-1}k(x,\\cdot)\\|_{2,n}^{2}\\leq\\displaystyle\\frac{3}{2}\\|[C_{X,\\lambda}^{-1}k(x,\\cdot)]\\|_{L_{2}(\\pi)}^{2}+\\tau o(1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. The proof follows [29, Lemma C.11]. As we use different notations and tracking of constants, we provide a similar proof in our setting for completeness. By Lemma 24, there exists an $\\epsilon$ -net $\\mathcal{F}\\subseteq\\mathcal{K}_{\\lambda}\\subseteq\\mathcal{H}$ with respect to $\\|\\cdot\\|_{\\infty}$ such that there exists a positive constant $c$ with ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\mathcal{F}|\\le c(\\lambda\\epsilon)^{-\\frac{2d}{\\theta}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for $\\epsilon$ to be determined later. Using Lemma 3 and a union bound over the finite set $\\mathcal{F}$ , with probability at least $1-2e^{-\\tau}$ , it holds simultaneously for all $f\\in\\mathcal F$ that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\big|\\|f\\|_{2,n}^{2}-\\|[f]\\big|_{L_{2}(\\pi)}^{2}\\big|\\le\\frac{1}{2}\\|[f]\\|_{L_{2}(\\pi)}^{2}+\\frac{5\\big(\\tau+\\log(|\\mathcal{F}|)\\big)\\kappa^{2}}{3\\lambda^{2}n}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We work in the event where Equation (19) holds for all $f\\in\\mathcal F$ . By definition of an $\\epsilon$ -net and $\\kappa_{\\lambda}$ , for any $x\\in\\mathscr{X}$ , there exists some $f\\in\\mathcal F$ such that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|C_{X,\\lambda}^{-1}k(x,\\cdot)-f\\|_{\\infty}\\leq\\epsilon,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which in particular implies that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\|[C_{X,\\lambda}^{-1}k(x,\\cdot)]\\|_{L_{2}(\\pi)}-\\|[f]\\|_{L_{2}(\\pi)}\\big|\\le\\epsilon}\\\\ &{}&{\\|C_{X,\\lambda}^{-1}k(x,\\cdot)\\|_{2,n}-\\|f\\|_{2,n}\\big|\\le\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since $\\|C_{X,\\lambda}^{-1}k(x,\\cdot)\\|_{\\infty}\\leq\\kappa^{2}\\lambda^{-1}$ , using the algebraic identity $a^{2}-b^{2}={\\bigl(}a-b{\\bigr)}{\\bigl(}2b+{\\bigl(}a-b{\\bigr)}{\\bigr)}$ , we obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{|\\|[C_{X,\\lambda}^{-1}k(x,\\cdot)]\\|_{L_{2}(\\pi)}^{2}-\\|[f]\\|_{L_{2}(\\pi)}^{2}|\\le\\epsilon\\big(2\\kappa^{2}\\lambda^{-1}+\\epsilon\\big)}\\\\ &{}&{|\\|C_{X,\\lambda}^{-1}k(x,\\cdot)\\|_{2,n}^{2}-\\|f\\|_{2,n}^{2}|\\le\\epsilon\\big(2\\kappa^{2}\\lambda^{-1}+\\epsilon\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We therefore have, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\|C_{X,\\lambda}^{-1}k(x,\\cdot)\\|_{2,n}^{2}\\leq\\|f\\|_{2,n}^{2}+\\epsilon(2\\kappa^{2}\\lambda^{-1}+\\epsilon)}}\\\\ &{}&{\\leq\\frac{3}{2}\\|[f]\\|_{L_{2}(\\pi)}^{2}+\\frac{5\\left(\\tau+\\log(|\\mathcal{F}|)\\kappa^{2}\\right.}{3\\lambda^{2}n}+\\epsilon(2\\kappa^{2}\\lambda^{-1}+\\epsilon)}\\\\ &{}&{\\leq\\frac{3}{2}\\|[C_{X,\\lambda}^{-1}k(x,\\cdot)]\\|_{L_{2}(\\pi)}^{2}+\\frac{5\\left(\\tau+\\log(|\\mathcal{F}|)\\kappa^{2}\\right.}{3\\lambda^{2}n}+2\\epsilon(2\\kappa^{2}\\lambda^{-1}+\\epsilon).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We now choose $\\textstyle{\\epsilon={\\frac{1}{n}}}$ and bound the error term. Recall that $1\\geq\\lambda\\geq n^{-\\frac{1}{2+p}}$ , therefore, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{5\\left(\\tau+\\log(|\\mathcal{F}|)\\kappa^{2}\\right.}{3\\lambda^{2}n}+2\\epsilon(2\\kappa^{2}\\lambda^{-1}+\\epsilon)\\leq\\frac{5\\left(\\tau+\\log(|\\mathcal{F}|)\\kappa^{2}\\right.}{3}n^{-\\frac{p}{2+p}}+2\\left(2\\kappa^{2}n^{\\frac{-1-p}{2+p}}+\\frac{1}{n^{2}}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\frac{5\\kappa^{2}}{3}\\left(\\tau+\\log(c\\lambda^{-\\frac{2d}{\\theta}}n^{\\frac{2d}{\\theta}})\\right)n^{-\\frac{p}{2+p}}+2\\left(2\\kappa^{2}n^{\\frac{-1-p}{2+p}}+\\frac{1}{n^{2}}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\tau o(1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Lemma 5. For $1\\geq\\lambda\\geq n^{-\\frac{1}{2+p}}$ , with probability at least $1-4e^{-\\tau}$ , for $n\\geq(c_{0}\\tau)^{4+2p}$ , we have for all $x\\in\\mathscr{X}$ simultaneously ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\|\\hat{C}_{X}^{\\frac{1}{2}}\\hat{C}_{X,\\lambda}^{-1}(C_{X}-\\hat{C}_{X})C_{X,\\lambda}^{-1}k(x,\\cdot)\\right\\|_{\\mathcal{H}}=\\tau o(1),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $c_{\\mathrm{0}}$ is the same constant as in Lemma 2. ", "page_idx": 20}, {"type": "text", "text": "Proof. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\hat{C}_{X}^{\\frac{1}{2}}\\hat{C}_{X,\\lambda}^{-1}(C_{X}-\\hat{C}_{X})C_{X,\\lambda}^{-1}k(x,\\cdot)\\right\\|_{\\mathcal{H}}}\\\\ &{=\\left\\|\\hat{C}_{X}^{\\frac{1}{2}}\\hat{C}_{X,\\lambda}^{-\\frac{1}{2}}\\hat{C}_{X,\\lambda}^{-\\frac{1}{2}}C_{X,\\lambda}^{\\frac{1}{2}}C_{X,\\lambda}^{-\\frac{1}{2}}(C_{X}-\\hat{C}_{X})C_{X,\\lambda}^{-1}k(x,\\cdot)\\right\\|_{\\mathcal{H}}}\\\\ &{~~\\leq\\left\\|\\hat{C}_{X}^{\\frac{1}{2}}\\hat{C}_{X,\\lambda}^{-\\frac{1}{2}}\\right\\|_{\\mathcal{H}\\to\\mathcal{H}}\\left\\|\\hat{C}_{X,\\lambda}^{-\\frac{1}{2}}C_{X,\\lambda}^{\\frac{1}{2}}\\right\\|_{\\mathcal{H}\\to\\mathcal{H}}}\\\\ &{~~\\cdot\\left\\|C_{X,\\lambda}^{-\\frac{1}{2}}(C_{X}-\\hat{C}_{X})C_{X,\\lambda}^{-\\frac{1}{2}}\\right\\|_{\\mathcal{H}\\to\\mathcal{H}}\\left\\|C_{X,\\lambda}^{-\\frac{1}{2}}k(x,\\cdot)\\right\\|_{\\mathcal{H}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We already saw in the proof of Lemma 2 that the first term is bounded by 1 and there is a constant $c_{0}>0$ such that\u221a for $\\tau\\geq\\log(2)$ , with probability at least $1-2e^{-\\tau}$ , for $n\\geq\\left\\dot{\\left(c_{0}\\tau\\right)}^{4+2p}$ , the second term is bounded by $\\sqrt{2}$ . For the third term we also saw in the proof of Lemma 4 that for $\\tau\\geq\\log(2)$ , with probability at least $1-2e^{-\\tau}$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\lambda^{-\\frac12}\\left\\|C_{X,\\lambda}^{-\\frac12}(C_{X}-\\hat{C}_{X})C_{X,\\lambda}^{-\\frac12}\\right\\|_{\\mathcal{H}\\to\\mathcal{H}}\\le\\frac{4\\kappa^{2}\\xi_{\\delta}}{3n\\lambda^{\\frac{3}{2}}}+\\sqrt{\\frac{2\\kappa^{2}\\xi_{\\delta}}{n\\lambda^{2}}},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we defined ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\xi_{\\delta}=\\log\\frac{2\\kappa^{2}(\\mathcal{N}_{1}(\\lambda)+1)}{e^{-\\tau}\\|C_{X}\\|_{\\mathcal{H}\\rightarrow\\mathcal{H}}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Finally, the fourth term is bounded above by $\\lambda^{-\\frac{1}{2}}\\kappa$ . Note that the bound on the fourth term is independent of $x$ , so it holds simultaneously for all $x\\,\\in\\,{\\mathcal{X}}$ . This is in contrast with the setting of Lemma 4 where for each fixed $x\\in\\mathscr{X}$ corresponds an element in the $\\epsilon$ -net of $\\mathcal{F}$ for which we have a high probability bound, and therefore we must use a union bound in order for the bound to hold simultaneously for all $x\\in\\mathscr{X}$ in the proof of Lemma 4. As in the proof of Lemma 4 since $1\\geq\\lambda\\geq n^{-\\frac{1}{2+p}}$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\xi_{\\delta}\\leq\\log\\frac{2\\bigl(c_{2,l}+1\\bigr)}{e^{-\\tau}\\|C_{X}\\|_{\\mathcal{H}\\rightarrow\\mathcal{H}}}+\\frac{p}{2+p}\\log n.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "In the bound on $\\xi_{\\delta}$ above, the first term does not depend on $n$ , and the second term is logarithmic in $n$ . Putting everything together by union bound, with probability at least $1-4e^{-\\tau}$ , for $n\\stackrel{=}{\\geq}(c_{0}\\tau)^{4+2p}$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left\\|\\hat{C}_{X}^{\\frac{1}{2}}\\hat{C}_{X,\\lambda}^{-1}(C_{X}-\\hat{C}_{X})C_{X,\\lambda}^{-1}k(x,\\cdot)\\right\\|_{\\mathcal{H}}=\\tau o(1).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Remark 6 (Comparison to [29]). We explicit the differences between our proof strategy and the proof strategy of [29]. ", "page_idx": 21}, {"type": "text", "text": "\u2022 Scalar versus vector-valued: lower bounding the bias in our case require us to accommodate for the vector-valued setting (see Lemma $^{\\,l}$ ).   \n\u2022 New proof of the bias: we lower bound the bias through Lemma 2, while [29] obtain the lower bound in Lemma C.7; however the proof of Lemma C.7 implicitly uses the equality $\\|A^{-1}\\|=\\|A\\|^{-1}$ , with $\\|\\cdot\\|$ the operator norm, see Eq. (69) $I29J$ and the preceding equations. It holds that $\\|A^{-1}\\|\\geq\\|\\dot{A}\\|^{-1}$ , but $\\|A^{-1}\\|\\,\\leq\\,\\|A\\|^{-1}$ may not hold in general. We therefore develop a new proof for this step, leading to Lemma 2.   \n\u2022 New proof of the variance: we lower bound the variance in Lemma 5, while [29] lower bound the variance in Lemma C.12; to show Eq. (20), [29] use a covering argument involving $\\mathcal{N}(\\mathcal{K}_{\\lambda},\\Vert\\cdot\\Vert\\varkappa,\\epsilon)$ (Lemma C.10). However, a close look at the proof of Lemma C.10 (last inequality of the proof) reveals that $\\frac{\\lambda_{i}}{\\lambda+\\lambda_{i}}$ i was mistaken for\u03bb+\u03bb\u03bbi and plugging the correct term in the proof would lead to a vacuous bound. As explained in the proof of Lemma 5, we therefore develop a proof that is free of a covering number argument for this step. ", "page_idx": 21}, {"type": "text", "text": "C Learning rates for spectral algorithms ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "To upper bound the excess-risk, we use a decomposition involving the approximation error expressed as $F_{\\lambda}-F_{*}$ and the estimation error expressed as $\\hat{F}_{\\lambda}-F_{\\lambda}$ . ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left\\|\\left[\\hat{F}_{\\lambda}\\right]-F_{*}\\right\\|_{\\gamma}\\leq\\left\\|\\left[\\hat{F}_{\\lambda}-F_{\\lambda}\\right]\\right\\|_{\\gamma}+\\left\\|\\left[F_{\\lambda}\\right]-F_{*}\\right\\|_{\\gamma},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\hat{F}_{\\lambda}$ is the empirical estimator based on general spectral regularization (Eq. (11)) and $F_{\\lambda}$ is its counterpart in population (Eq. (10)). Note that this is a different decomposition than the bias-variance decomposition used in the proof of Theorem 3. ", "page_idx": 21}, {"type": "text", "text": "The proof structure is as follows: ", "page_idx": 21}, {"type": "text", "text": "1. Fourier expansion C.1.   \n2. Approximation Error C.2.   \n3. Estimation error C.3 ", "page_idx": 21}, {"type": "text", "text": "C.1 Fourier expansion ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Recall the notations defined in Appendix A.3. The family $\\{d_{j}\\}_{j\\in J}$ is an ONB of $\\boldsymbol{\\wp}$ , the family $\\{\\boldsymbol{\\mu}_{i}^{1/2}\\boldsymbol{e}_{i}\\}_{i\\in I}$ is an ONB of $(\\ker I_{\\pi})^{\\perp}$ and the family $\\{\\tilde{e}_{i}\\}_{i\\in I^{\\prime}}$ is an ONB of $\\ker I_{\\pi}$ such that $\\left\\{\\mu_{i}^{1/2}e_{i}\\right\\}_{i\\in I}\\cup\\left\\{\\tilde{e}_{i}\\right\\}_{i\\in I^{\\prime}}$ forms an ONB of $\\mathcal{H}$ . Furthermore, recall that $\\{\\mu_{i}^{\\beta/2}[e_{i}]\\}_{i\\in I}$ is an ONB of $[\\mathcal{H}]^{\\beta},\\beta\\geq0$ . ", "page_idx": 22}, {"type": "text", "text": "Lemma 6 (Fourier expansion). Suppose Assumption (SRC) holds with $\\beta\\ge0.$ . By definition of the vector-valued interpolation space and by Theorem $^{5}$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\nF_{*}=\\sum_{i\\in I,j\\in J}a_{i j}d_{j}\\big[e_{i}\\big],\\qquad a_{i j}=\\big\\langle F_{*},d_{j}\\big[e_{i}\\big]\\big\\rangle_{L_{2}(\\pi;\\mathcal{Y})}\\,,\\qquad\\left\\lVert F_{*}\\right\\rVert_{\\beta}^{2}=\\sum_{i\\in I,j\\in J}\\frac{a_{i j}^{2}}{\\mu_{i}^{\\beta}}\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then, we have the following equalities with respect to this Fourier decomposition. ", "page_idx": 22}, {"type": "text", "text": "1. The Hilbert-Schmidt operator $C_{\\lambda}\\in S_{2}(\\mathcal{H},\\mathcal{y})$ , Eq. (10), can be written as ", "page_idx": 22}, {"type": "equation", "text": "$$\nC_{\\lambda}=\\sum_{i\\in I,j\\in J}a_{i j}g_{\\lambda}(\\mu_{i})\\sqrt{\\mu_{i}}d_{j}\\otimes\\sqrt{\\mu_{i}}e_{i}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "2. The Hilbert-Schmidt operator $\\left(C_{Y X}-C_{\\lambda}C_{X}\\right)C_{X,\\lambda}^{-\\frac{1}{2}}\\in S_{2}(\\mathcal{H},\\mathcal{Y})$ can be written as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left(C_{Y X}-C_{\\lambda}C_{X}\\right)C_{X,\\lambda}^{-\\frac{1}{2}}=\\sum_{i\\in I,j\\in J}a_{i j}r_{\\lambda}{\\left(\\mu_{i}\\right)}{\\left(\\mu_{i}+\\lambda\\right)}^{-\\frac{1}{2}}\\sqrt{\\mu_{i}}\\left(d_{j}\\otimes\\sqrt{\\mu_{i}}e_{i}\\right)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "3. The Hilbert-Schmidt operator $C_{Y X}\\in S_{2}(\\mathscr{H},\\mathscr{y})$ can be written as ", "page_idx": 22}, {"type": "equation", "text": "$$\nC_{Y X}=\\left(\\sum_{i\\in I,j\\in J}a_{i j}\\mu_{i}^{-\\frac{\\beta}{2}}d_{j}\\otimes\\sqrt{\\mu_{i}}e_{i}\\right)C_{X}^{\\frac{\\beta+1}{2}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. We first derive the Fourier expansion of $C_{Y X}$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C_{Y X}=\\mathbb{E}_{X,Y}\\left[Y\\otimes\\phi\\left(X\\right)\\right]}\\\\ &{\\quad=\\mathbb{E}_{X}\\left[F_{*}\\left(X\\right)\\otimes\\phi\\left(X\\right)\\right]}\\\\ &{\\quad=\\mathbb{E}_{X}\\left[\\displaystyle\\sum_{i\\in I,j\\in J}a_{i j}e_{i}(X)d_{j}\\otimes\\phi(X)\\right]}\\\\ &{\\quad=\\mathbb{E}_{X}\\left[\\displaystyle\\sum_{i\\in I,j\\notin J}a_{i j}d_{j}\\otimes\\left(\\displaystyle\\sum_{k\\in I}\\sqrt{\\mu_{k}}e_{k}(X)\\sqrt{\\mu_{k}}e_{k}\\right)e_{i}\\left(X\\right)\\right]}\\\\ &{\\quad=\\displaystyle\\sum_{i\\notin I}a_{i j}\\sqrt{\\mu_{k}}\\cdot\\mathbb{E}_{X}\\left[e_{k}(X)e_{i}(X)\\right]\\cdot d_{j}\\otimes\\left(\\sqrt{\\mu_{k}}e_{k}\\right)}\\\\ &{\\quad=\\displaystyle\\sum_{i\\in I,j\\in J}a_{i j}\\sqrt{\\mu_{i}}d_{j}\\otimes\\left(\\sqrt{\\mu_{i}}e_{i}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where in Eq. (25) we used the tower property of conditional expectation and in Eq. (26) we used the fact that $\\{[e_{i}]\\}_{i\\in I}$ forms an orthonormal system in $L_{2}(\\pi)$ . We can manipulate Eq. (26) to derive Eq. (24), ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{C_{Y X}=\\displaystyle\\sum_{i\\in I,j\\in J}a_{i j}\\mu_{i}^{\\frac{1}{2}-\\frac{\\beta+1}{2}}d_{j}\\otimes\\left(C_{X}^{\\frac{\\beta+1}{2}}(\\sqrt{\\mu_{i}}e_{i})\\right)}}\\\\ {{=\\displaystyle\\left(\\sum_{i\\in I,j\\in J}a_{i j}\\mu_{i}^{-\\frac{\\beta}{2}}d_{j}\\otimes\\sqrt{\\mu_{i}}e_{i}\\right)C_{X}^{\\frac{\\beta+1}{2}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By the spectral decomposition of $C_{X}$ Eq. (6) and spectral calculus (Definition 9), we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{g_{\\lambda}(C_{X})=\\displaystyle\\sum_{i\\in I}g_{\\lambda}(\\mu_{i})\\sqrt{\\mu_{i}}e_{i}\\otimes\\sqrt{\\mu_{i}}e_{i}+g_{\\lambda}(0)\\sum_{i\\in I^{\\prime}}\\tilde{e}_{i}\\otimes\\tilde{e}_{i},}}\\\\ {{r_{\\lambda}(C_{X})=\\displaystyle\\sum_{i\\in I}r_{\\lambda}(\\mu_{i})\\sqrt{\\mu_{i}}e_{i}\\otimes\\sqrt{\\mu_{i}}e_{i}+\\displaystyle\\sum_{i\\in I^{\\prime}}\\tilde{e}_{i}\\otimes\\tilde{e}_{i}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where we used $r_{\\lambda}(0)=1$ . ", "page_idx": 23}, {"type": "text", "text": "Proof of Eq. (22). Using Eq. (26) and (27), we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C_{\\lambda}=\\left(\\displaystyle\\sum_{i\\in I,j\\in J}a_{i j}\\sqrt{\\mu_{i}}d_{j}\\otimes\\left(\\sqrt{\\mu_{i}}e_{i}\\right)\\right)\\left(\\displaystyle\\sum_{k\\in I}g_{\\lambda}(\\mu_{k})(\\sqrt{\\mu_{k}}e_{k})\\otimes\\left(\\sqrt{\\mu_{k}}e_{k}\\right)+g_{\\lambda}(0)\\displaystyle\\sum_{l\\in I^{\\prime}}\\tilde{e}_{l}\\otimes\\tilde{e}_{l}\\right)}\\\\ &{\\quad=\\displaystyle\\sum_{i j k}a_{i j}\\sqrt{\\mu_{i}}g_{\\lambda}(\\mu_{k})\\delta_{i k}d_{j}\\otimes\\left(\\sqrt{\\mu_{k}}e_{k}^{l}\\right)}\\\\ &{\\quad=\\displaystyle\\sum_{i\\in I,j\\in J}a_{i j}\\sqrt{\\mu_{i}}g_{\\lambda}(\\mu_{i})d_{j}\\otimes\\left(\\sqrt{\\mu_{i}}e_{i}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where in Eq. (29), we recall the fact that $\\{\\sqrt{\\mu_{i}}e_{i}\\}_{i\\in I}$ forms an ONB of $(\\ker I_{\\pi})^{\\perp}$ and $\\{\\tilde{e_{i}}\\}_{i\\in I^{\\prime}}$ forms an ONB of $\\ker I_{\\pi}$ . ", "page_idx": 23}, {"type": "text", "text": "Proof of Eq. (23). Using Eq. (26) and (28), we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(C_{Y X}-C_{\\lambda}C_{X}\\right)C_{X,\\lambda}^{-\\frac{1}{2}}=C_{Y X}r_{\\lambda}(C_{X})C_{X,\\lambda}^{-\\frac{1}{2}}}\\\\ &{\\quad\\quad\\quad=\\left(\\displaystyle\\sum_{i\\in I,j\\in J}a_{i j}\\sqrt{\\mu_{i}}d_{j}\\otimes\\left(\\sqrt{\\mu_{i}}e_{i}\\right)\\right)\\left(\\displaystyle\\sum_{k\\in I}r_{\\lambda}(\\mu_{k})\\sqrt{\\mu_{k}}e_{k}\\otimes\\sqrt{\\mu_{k}}e_{k}+\\displaystyle\\sum_{l\\in I^{\\prime}}\\tilde{e}_{l}\\otimes\\tilde{e}_{l}\\right)C_{X,\\lambda}^{-\\frac{1}{2}}}\\\\ &{\\quad\\quad\\quad=\\left(\\displaystyle\\sum_{i j\\in I}a_{i j}\\sqrt{\\mu_{i}}r_{\\lambda}(\\mu_{k})d_{j}\\otimes\\left(\\sqrt{\\mu_{k}}e_{k}\\right)\\delta_{i k}\\right)C_{X,\\lambda}^{-\\frac{1}{2}}}\\\\ &{\\quad\\quad\\quad=\\displaystyle\\sum_{i\\in I,j\\in J}a_{i j}\\sqrt{\\mu_{i}}r_{\\lambda}(\\mu_{i})d_{j}\\otimes\\left(C_{X,\\lambda}^{-\\frac{1}{2}}(\\sqrt{\\mu_{i}}e_{i})\\right)}\\\\ &{\\quad\\quad\\quad=\\displaystyle\\sum_{i\\in I,j\\in J}a_{i j}\\sqrt{\\mu_{i}}(\\mu_{i}+\\lambda)^{-\\frac{1}{2}}r_{\\lambda}(\\mu_{i})d_{j}\\otimes\\left(\\sqrt{\\mu_{i}}e_{i}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Lemma 7. Suppose Assumption (SRC) holds with $\\beta\\ge0$ , then the following bound is satisfied, for all $\\lambda>0$ and $0\\leq\\gamma\\leq1$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|[F_{\\lambda}]\\|_{\\gamma}^{2}\\le E^{2}\\|F_{*}\\|_{\\operatorname*{min}\\{\\gamma,\\beta\\}}^{2}\\lambda^{-(\\gamma-\\beta)_{+}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For the definition of $E$ , see Eq. (8). ", "page_idx": 23}, {"type": "text", "text": "Proof. We adopt the notations of Lemma 6. By Parseval\u2019s identity and Eq. (22), we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\|[F_{\\lambda}]\\|_{\\gamma}^{2}=\\|C_{\\lambda}\\|_{S_{2}([\\mathcal{H}]^{\\gamma},\\mathcal{V})}^{2}}}\\\\ &{=\\displaystyle\\sum_{i\\in I,j\\in J}a_{i j}^{2}g_{\\lambda}(\\mu_{i})^{2}\\mu_{i}^{2-\\gamma}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In the case of $\\gamma\\le\\beta$ , we bound $g_{\\lambda}(\\mu_{i})\\mu_{i}\\leq E$ using Eq. (8). Then, by Eq. (21), ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\big\\|\\big[F_{\\lambda}\\big]\\big\\|_{\\gamma}^{2}\\le E^{2}\\sum_{i\\in I,j\\in J}\\frac{a_{i j}^{2}}{\\mu_{i}^{\\gamma}}=E^{2}\\|F_{*}\\|_{\\gamma}^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In the case of $\\gamma>\\beta$ , we apply Eq. (8) to $g_{\\lambda}(\\mu_{i})\\mu_{i}^{1-\\frac{\\gamma-\\beta}{2}}\\leq E\\lambda^{-\\frac{\\gamma-\\beta}{2}}$ to obtain, using Eq. (21) again, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\|[F_{\\lambda}]\\|_{\\gamma}^{2}=\\sum_{i\\in I,j\\in J}g_{\\lambda}(\\mu_{i})^{2}\\mu_{i}^{2-(\\gamma-\\beta)}\\mu_{i}^{-\\beta}a_{i j}^{2}}}\\\\ &{\\le E^{2}\\lambda^{-(\\gamma-\\beta)}\\sum_{i\\in I,j\\in J}\\mu_{i}^{-\\beta}a_{i j}^{2}}\\\\ &{=E^{2}\\lambda^{-(\\gamma-\\beta)}\\|F_{*}\\|_{\\beta}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Lemma 8. Suppose Assumption (SRC) holds for $0\\leq\\beta\\leq2\\rho$ , with $\\rho$ the qualification. Then, the following bound is satisfied, for all $\\lambda>0$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left\\|\\left(C_{Y X}-C_{\\lambda}C_{X}\\right)C_{X,\\lambda}^{-\\frac{1}{2}}\\right\\|_{S_{2}(\\mathcal{H},\\mathcal{Y})}\\leq\\omega_{\\rho}\\|F_{*}\\|_{\\beta}\\lambda^{\\frac{\\beta}{2}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For the definition of $\\omega_{\\rho}$ , see Eq. (9). ", "page_idx": 24}, {"type": "text", "text": "Proof. Recall that in Lemma 6 we used the decomposition ", "page_idx": 24}, {"type": "equation", "text": "$$\nF_{*}=\\sum_{i\\in I,j\\in J}a_{i j}d_{j}[e_{i}],\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where Assumption (SRC) implies that $\\begin{array}{r}{\\left\\|F_{*}\\right\\|_{\\beta}^{2}\\;=\\;\\sum_{i j}\\frac{a_{i j}^{2}}{\\mu_{i}^{\\beta}}\\;<\\;\\infty}\\end{array}$ . Using Eq. (23) in Lemma 6 and Parseval\u2019s identity w.r.t. the ONS $\\{d_{j}\\otimes\\mu_{i}^{1/2}e_{i}\\}_{i\\in I,j\\in J}$ in $S_{2}(\\mathcal{H},\\mathcal{V})$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\left\\|(C_{Y X}-C_{\\lambda}C_{X})\\,C_{X,\\lambda}^{-\\frac12}\\right\\|_{S_{2}(\\mathcal{H},\\mathcal{Y})}=\\left(\\displaystyle\\sum_{i\\in I,j\\in J}a_{i j}^{2}r_{\\lambda}^{2}(\\mu_{i})(\\mu_{i}+\\lambda)^{-1}\\mu_{i}\\right)^{\\frac12}}}\\\\ &{}&{\\le\\left(\\displaystyle\\sum_{i\\in I,j\\in J}\\frac{a_{i j}^{2}}{\\mu_{i}^{\\beta}}r_{\\lambda}^{2}(\\mu_{i})\\mu_{i}^{\\beta}\\right)^{\\frac12}}\\\\ &{}&{\\le\\left\\|F_{\\ast}\\right\\|_{\\beta}\\operatorname*{sup}r_{\\lambda}(\\mu_{i})\\mu_{i}^{\\frac{\\beta}{2}}}\\\\ &{}&{\\le\\|F_{\\ast}\\|_{\\beta}\\omega_{\\rho}\\lambda^{\\frac{\\beta}{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Lemma 9. Suppose Assumption (SRC) holds with $\\beta\\ge0$ , then for all $\\lambda>0$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\lVert C_{\\lambda}r_{\\lambda}\\left(\\hat{C}_{X}\\right)\\hat{C}_{X,\\lambda}^{\\frac{1}{2}}\\right\\rVert_{S_{2}(\\mathcal{H},\\mathcal{Y})}\\le B\\left\\lVert\\hat{C}_{X,\\lambda}^{\\frac{1}{2}}r_{\\lambda}(\\hat{C}_{X})g_{\\lambda}(C_{X})C_{X}^{\\frac{\\beta+1}{2}}\\right\\rVert_{\\mathcal{H}\\to\\mathcal{H}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\left\\|F_{*}\\right\\|_{\\beta}=B<\\infty$ . ", "page_idx": 24}, {"type": "text", "text": "Proof. Recall that Lemma 6 we used the decomposition ", "page_idx": 24}, {"type": "equation", "text": "$$\nF_{*}=\\sum_{i\\in I,j\\in j}a_{i j}d_{j}[e_{i}],\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\begin{array}{r}{\\left\\|F_{*}\\right\\|_{\\beta}^{2}=\\sum_{i j}\\frac{a_{i j}^{2}}{\\mu_{i}^{\\beta}}=B^{2}<\\infty}\\end{array}$ . Using Eq. (24) in Lemma 6 and $C_{\\lambda}=C_{Y X}g_{\\lambda}(C_{X})$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|C_{\\lambda}r_{\\lambda}\\left(\\hat{C}_{X}\\right)\\hat{C}_{X,\\lambda}^{\\frac{1}{2}}\\right\\|_{S_{2}(\\mathcal{H},\\mathcal{Y})}=\\left\\|\\left(\\sum_{i j}a_{i j}\\mu_{i}^{-\\frac{\\beta}{2}}d_{j}\\otimes\\sqrt{\\mu_{i}}e_{i}\\right)C_{X}^{\\frac{\\beta+1}{2}}g_{\\lambda}(C_{X})r_{\\lambda}(\\hat{C}_{X})\\hat{C}_{X,\\lambda}^{\\frac{1}{2}}\\right\\|_{S_{2}(\\mathcal{H},\\mathcal{Y})}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq B\\left\\|C_{X}^{\\frac{\\beta+1}{2}}g_{\\lambda}(C_{X})r_{\\lambda}(\\hat{C}_{X})\\hat{C}_{X,\\lambda}^{\\frac{1}{2}}\\right\\|_{\\mathbb{H}\\to\\mathcal{H}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where we notice that the $S_{2}(\\mathcal{H},\\mathcal{V})$ norm of the first term is exactly the $\\beta$ norm of $F_{*}$ , which is given by $B$ . Recalling that $C_{X},{\\hat{C}}_{X}$ are self adjoint, we prove the final result by taking the adjoint and using that an operator has the same operator norm as its adjoint. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "C.2 Approximation Error ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Lemma 10. Let $F_{\\lambda}$ be given by Eq. (10) based on a general spectral filter satisfying Definition 2 with qualification $\\rho\\:\\geq\\:0$ . Suppose Assumption (SRC) holds with parameter $\\beta~\\geq~0$ and define $\\beta_{\\rho}=\\operatorname*{min}\\{\\beta,2\\rho\\}$ , then the following bound is satisfied, for all $\\lambda>0$ and $0\\leq\\gamma\\leq\\beta_{\\rho}$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\left[F_{\\lambda}\\right]-F_{*}\\right\\|_{\\gamma}^{2}\\le\\omega_{\\rho}^{2}\\left\\|F_{*}\\right\\|_{\\beta_{\\rho}}^{2}\\lambda^{\\beta_{\\rho}-\\gamma}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. In Eq. (10), we defined $F_{\\lambda}(\\cdot)=C_{\\lambda}\\phi(\\cdot)$ . On the other hand, in Lemma 6 we obtained the Fourier expansion of $C_{\\lambda}$ leading to Eq. (22). Thus we have for $\\pi$ \u2212almost all $x\\in\\mathscr{X}$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\nF_{\\lambda}(x)=\\sum_{i\\in I,j\\in J}a_{i j}\\mu_{i}g_{\\lambda}(\\mu_{i})d_{j}e_{i}(x).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Therefore, ", "page_idx": 25}, {"type": "equation", "text": "$$\n[F_{\\lambda}]-F_{*}=\\sum_{i\\in I,j\\in J}a_{i j}(1-\\mu_{i}g_{\\lambda}(\\mu_{i}))d_{j}[e_{i}]=\\sum_{i\\in I,j\\in J}a_{i j}r_{\\lambda}(\\mu_{i})d_{j}[e_{i}].\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Suppose $\\beta\\le2\\rho$ , using Parseval\u2019s identity w.r.t. the ONB $\\{d_{j}\\mu_{i}^{\\gamma/2}[e_{i}]\\}_{i\\in I,j\\in J}$ of $[\\mathcal{G}]^{\\gamma}$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\|\\left[F_{\\lambda}\\right]-F_{*}\\right\\|_{\\gamma}^{2}=\\Bigg\\|{\\underset{i\\in I,j\\in J}{\\sum}}{\\frac{a_{i j}}{\\mu_{i}^{\\gamma/2}}}r_{i}^{\\lambda}(\\mu_{i})d_{j}\\mu_{i}^{\\gamma/2}[e_{i}]\\Bigg\\|_{\\gamma}^{2}}\\\\ &{=\\underset{i\\in I,j\\in J}{\\sum}{\\frac{a_{i j}^{2}}{\\mu_{i}^{\\gamma}}}r_{i}^{2}(\\mu_{i})}\\\\ &{=\\underset{i\\in I,j\\in J}{\\sum}{\\frac{a_{i j}^{2}}{\\mu_{i}^{\\beta}}}r_{i}^{2}(\\mu_{i})\\mu_{i}^{\\beta-\\gamma}}\\\\ &{\\leq\\omega_{\\rho}^{2}\\lambda^{\\beta-\\gamma}\\underset{i\\in I,j\\in J}{\\sum}{\\sum}{\\frac{a_{i j}^{2}}{\\mu_{i}^{\\beta}}}}\\\\ &{=\\left\\|F_{*}\\right\\|_{\\beta}^{2}\\omega_{\\rho}^{2}\\lambda^{\\beta-\\gamma}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where we used Eq. (9) in the definition of a filter function, together with $0\\le\\beta\\le2\\rho$ and $0\\leq\\gamma\\leq\\beta$ , which taken together implies that $\\begin{array}{r}{0\\leq{\\frac{\\beta-\\gamma}{2}}\\leq\\rho}\\end{array}$ . Finally, if $\\beta\\geq2\\rho$ , then since $[\\mathcal{G}]^{\\beta}\\subseteq[\\mathcal{G}]^{2\\rho}$ , we can perform the last derivations again with $\\tilde{\\beta}=2\\rho$ to obtain the final result. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "C.3 Estimation error ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Before proving the main results we recall two embedding properties for the vector-valued interpolation space $[\\bar{\\mathcal{G}}]^{\\beta}$ (Definition 1). The first embedding property lifts the property (EMB) defined for the scalar-valued RKHS $[\\mathcal{H}]^{\\alpha}$ to the vector-valued RKHS $[\\mathcal{G}]^{\\alpha}$ . ", "page_idx": 25}, {"type": "text", "text": "Lemma 11 ( $L_{\\infty}$ -embedding property - Lemma 4 [31]). Under (EMB) the inclusion operator $\\mathcal{T}_{\\pi}^{\\alpha,\\beta}$ \u2236 $[\\mathcal{G}]^{\\alpha}\\hookrightarrow L_{\\infty}(\\pi;\\mathcal{V})$ is bounded with operator norm $A$ , ", "page_idx": 25}, {"type": "text", "text": "Theorem 6 $\\iint_{q}$ -embedding property - Theorem 3 [31]). Let Assumption (EMB) be satisfied with parameter $\\alpha\\in(0,1]$ . For any $\\beta\\in[0,\\alpha)$ , the inclusion map ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathcal{Z}_{\\pi}^{q_{\\alpha,\\beta}}:[\\mathcal{G}]^{\\beta}\\hookrightarrow L_{q_{\\alpha,\\beta}}(\\pi;\\mathcal{V})\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "is bounded, where $\\begin{array}{r}{q_{\\alpha,\\beta}:=\\frac{2\\alpha}{\\alpha-\\beta}}\\end{array}$ . ", "page_idx": 25}, {"type": "text", "text": "The $\\boldsymbol{L}_{q}$ -embedding property was first introduced in the scalar-valued setting in [55] and then lifted to the vector-valued setting by [31]. Its role is to replace a boundedness condition on the ground truth function $F_{*}$ . We now explain how the $L_{q}$ -embedding property can be combined with Assumption (EMB) and a truncation technique. ", "page_idx": 25}, {"type": "text", "text": "Lemma 12. Recall that $\\pi$ is the marginal measure of $X$ on $\\mathcal{X}$ . For $t\\geq0$ , define the measurable set $\\Omega_{t}$ as follows ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\Omega_{t}:=\\{x\\in\\mathcal{X}:\\|F_{*}(x)\\|_{\\mathcal{Y}}\\leq t\\}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Let $q>0$ . Assume that $F_{*}\\in L_{q}(\\pi;\\mathcal{V})$ . In other words, there exists some constant $c_{q}>0$ such that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|F_{*}\\|_{L_{q}(\\pi;\\mathcal{Y})}=\\left(\\int_{\\mathcal{X}}\\|F_{*}(x)\\|_{\\mathcal{Y}}^{q}\\mathrm{d}\\pi(x)\\right)^{\\frac{1}{q}}=c_{q}<+\\infty,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then we have the following conclusions ", "page_idx": 25}, {"type": "text", "text": "1. The $\\pi$ -measure of the complement of $\\Omega$ can be bounded by ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\pi(\\{x\\notin\\Omega_{t}\\})\\leq\\frac{c_{q}^{q}}{t^{q}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "2. Recall that $\\{x_{i}\\}_{i=1}^{n}$ are i.i.d. samples distributed according to $\\pi$ . If $t=n^{\\frac{1}{\\tilde{q}}}$ for $\\tilde{q}<q$ , then we can conclude as follows. For a fixed parameter $\\tau>0$ , for all sufficiently large $n$ , where the hidden index bound depends on $q\\tilde{q}^{-1}$ and $\\tau$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\pi^{\\otimes n}\\left(\\cap_{i=1}^{n}\\{x_{i}\\in\\Omega_{t}\\}\\right)\\geq1-e^{-\\tau}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. The first claim is a straightforward application of Markov\u2019s inequality, as follows ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\pi(\\left\\{x\\notin\\Omega_{t}\\right\\})=\\pi\\left(\\|F_{*}(x)\\|_{\\mathcal{Y}}>t\\right)\\leq\\frac{\\mathbb{E}_{\\pi}\\left[\\|F_{*}(X)\\|_{\\mathcal{Y}}^{q}\\right]}{t^{q}}=\\frac{c_{q}^{q}}{t^{q}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "To show the second claim, we first evaluate the probability that there exists some $x_{i}$ \u2019s that lies outside $\\Omega_{t}$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pi^{\\otimes n}\\left(\\cup_{i=1}^{n}\\left\\{x_{i}\\notin\\Omega_{t}\\right\\}\\right)=1-\\pi^{\\otimes n}\\left(\\cap_{i=1}^{n}\\left\\{x_{i}\\in\\Omega_{t}\\right\\}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=1-\\pi\\big(\\left\\{x_{i}\\in\\Omega_{t}\\right\\}\\big)^{n}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\quad\\leq1-\\left(1-\\displaystyle\\frac{c_{q}^{q}}{t^{q}}\\right)^{n}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\displaystyle\\frac{c_{q}^{q}n}{t^{q}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where in the last inequality we used Bernoulli\u2019s inequality, which states that for $r\\geq1$ and $0\\leq x\\leq1$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n(1-x)^{r}\\geq1-r x.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "By assumption t = nqt for some fixed $q>q_{t}>0$ . We thus have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\pi^{\\otimes n}\\left(\\cup_{i=1}^{n}\\{x_{i}\\notin\\Omega_{t}\\}\\right)\\le c_{q}^{q}n^{1-\\frac{q}{q_{t}}}\\le e^{-\\tau},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "for sufficiently large $n$ , where the hidden index bound depends on $\\textstyle{\\frac{q}{q_{t}}}$ and $\\tau$ . ", "page_idx": 26}, {"type": "text", "text": "We adapt [31, Lemma 5] to the spectral algorithms setting. ", "page_idx": 26}, {"type": "text", "text": "Lemma 13. Suppose Assumptions (SRC) and (EMB) hold for some $0\\le\\beta\\le2\\rho_{\\!\\circ}$ , with $\\rho$ the qualification, then the following bounds are satisfied, for all $0<\\lambda\\leq1$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\big[F_{\\lambda}\\big]-F_{*}\\|_{L_{\\infty}}^{2}\\leq\\big(\\|F_{*}\\|_{L_{\\infty}}+A\\operatorname*{max}\\{E,\\omega_{\\rho}\\}\\|F_{*}\\|_{\\beta}\\big)^{2}\\,\\lambda^{\\beta-\\alpha},}\\\\ &{\\qquad\\|\\big[F_{\\lambda}\\big]\\|_{L_{\\infty}}^{2}\\leq A^{2}E^{2}\\|F_{*}\\|_{\\operatorname*{min}\\{\\alpha,\\beta\\}}^{2}\\lambda^{-(\\alpha-\\beta)_{+}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. We use Lemma 11 and Lemma 7 to write: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|[F_{\\lambda}]\\|_{\\infty}^{2}\\le A^{2}\\|[F_{\\lambda}]\\|_{\\alpha}^{2}\\le A^{2}E^{2}\\|F_{*}\\|_{\\operatorname*{min}\\{\\alpha,\\beta\\}}^{2}\\lambda^{-(\\alpha-\\beta)_{+}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "This proves Eq. (31). To show Eq. (30), in the case $\\beta\\leq\\alpha$ we use the triangle inequality, Eq. (31) and $\\lambda\\leq1$ to obtain ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|[F_{\\lambda}]-F_{*}\\|\\infty\\le\\|F_{*}\\|_{\\infty}+\\|[F_{\\lambda}]\\|_{\\infty}}&{}\\\\ {\\le(\\|F_{*}\\|_{\\infty}+A E\\|F_{*}\\|_{\\beta})\\lambda^{-\\frac{\\alpha-\\beta}{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "In the case $\\beta>\\alpha$ , Eq. (30) is a consequence of Lemma 11 and Lemma 10 with $\\gamma=\\alpha$ (here we use the assumption $0\\le\\beta\\le2\\rho)$ ), ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|[F_{\\lambda}]-F_{*}\\|_{\\infty}^{2}\\le A^{2}\\|[F_{\\lambda}]-F_{*}\\|_{\\alpha}^{2}\\le A^{2}\\omega_{\\rho}^{2}\\|F_{*}\\|_{\\beta}^{2}\\lambda^{\\beta-\\alpha}\\le\\big(\\|F_{*}\\|_{\\infty}+A\\omega_{\\rho}\\|F_{*}\\|_{\\beta}\\big)^{2}\\lambda^{\\beta-\\alpha}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We adapt [54, Theorem 13] to the vector-valued setting. ", "page_idx": 26}, {"type": "text", "text": "Theorem 7. Suppose that Assumptions (EMB), (EVD), (MOM) and (SRC) hold for $0\\,\\leq\\,\\beta\\,\\leq\\,2\\rho_{:}$ , where $\\rho$ is the qualification, and $p\\leq\\alpha\\leq1$ . Denote, for $i=1,\\dots,n,$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\xi_{i}=\\xi\\bigl(x_{i},y_{i}\\bigr)=\\big(\\bigl(y_{i}-C_{\\lambda}\\phi(x_{i})\\bigr)\\otimes\\phi(x_{i})\\bigr)\\,C_{X,\\lambda}^{-\\frac{1}{2}},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and for $t\\geq0$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\Omega_{t}=\\{x\\in\\mathcal{X}:\\|F_{*}(x)\\|_{\\mathcal{V}}\\leq t\\}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then for all $\\tau\\geq1$ , with probability at least $1-2e^{-\\tau}$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\xi_{i}\\mathbb{1}\\big\\{x_{i}\\in\\Omega_{t}\\big\\}-\\mathbb{E}\\big[\\xi(X,Y)\\mathbb{1}\\{X\\in\\Omega_{t}\\}\\big]\\right\\|_{S_{2}(\\mathcal{H},\\mathcal{Y})}}\\\\ &{\\le\\tau\\left(c_{1}\\lambda^{\\frac{\\beta}{2}-\\alpha}n^{-1}+c_{2}\\lambda^{-\\frac{\\alpha}{2}}n^{-1}\\big(t+R+A\\big)+\\displaystyle\\frac{c_{3}\\sqrt{\\mathcal{N}_{1}(\\lambda)}}{\\sqrt{n}}+\\frac{c_{4}}{\\sqrt{n}\\lambda^{\\frac{\\alpha-\\beta}{2}}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $R$ is the constant from Assumption (MOM), and ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{c_{1}=8\\sqrt{2}A^{2}\\operatorname*{max}\\{E,\\omega_{\\rho}\\}\\Vert F_{*}\\Vert_{\\beta}}\\\\ {c_{2}=8\\sqrt{2}A}\\\\ {c_{3}=8\\sqrt{2}\\sigma}\\\\ {c_{4}=8\\sqrt{2}A\\Vert F_{*}\\Vert_{\\beta}\\omega_{\\rho}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $A$ is the constant from Assumption (EMB), and $E,\\omega_{\\rho}$ are defined in Eq. (8) and (9) respectively. ", "page_idx": 27}, {"type": "text", "text": "Proof. We wish to apply vector-valued Bernstein\u2019s inequality, namely Theorem 10. We thus compute, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|\\xi(X,Y)\\mathbb{1}\\{X\\in\\Omega_{t}\\}\\right\\|_{S_{2}(\\mathcal{H},y)}^{m}\\right]=\\mathbb{E}\\left[\\mathbb{1}\\left\\{X\\in\\Omega_{t}\\right\\}\\left\\|(Y-C_{\\lambda}\\phi(X))\\otimes\\left(C_{X,\\lambda}^{-\\frac{1}{2}}\\phi(X)\\right)\\right\\|_{S_{2}(\\mathcal{H},y)}^{m}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}\\left[\\mathbb{1}\\left\\{X\\in\\Omega_{t}\\right\\}\\left\\|(Y-C_{\\lambda}\\phi(X))\\right\\|_{y}^{m}\\left\\|C_{X,\\lambda}^{-\\frac{1}{2}}\\phi(X)\\right\\|_{\\mathcal{H}}^{m}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\int_{\\Omega_{t}}\\left\\|C_{X,\\lambda}^{-\\frac{1}{2}}\\phi(x)\\right\\|_{\\mathcal{H}}^{m}\\int_{\\mathcal{Y}}\\left\\|y-C_{\\lambda}\\phi(x)\\right\\|_{\\mathcal{Y}}^{m}\\mathrm{d}p(x,\\mathrm{d}y)\\mathrm{d}\\pi(x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "First we consider the inner integral, by Assumption (MOM), ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\int_{\\mathcal{P}}\\|(y-C_{\\lambda}\\phi(x))\\|_{\\mathcal{Y}}^{m}\\,\\mathrm{d}p(x,\\mathrm{d}y)\\leq2^{m-1}\\left(\\int_{\\mathcal{P}}\\|y-F_{*}(x)\\|_{\\mathcal{Y}}^{m}+\\|F_{\\lambda}(x)-F_{*}(x)\\|_{\\mathcal{Y}}^{m}\\right)\\mathrm{d}p(x,\\mathrm{d}y)}}\\\\ &{}&{=m!\\sigma^{2}(2R)^{m-2}+2^{m-1}\\left\\|F_{\\lambda}(x)-F_{*}(x)\\right\\|_{\\mathcal{Y}}^{m}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Plugging the above inequality into Eq. (32), as well as introducing the shorthand, ", "page_idx": 27}, {"type": "equation", "text": "$$\nh_{x}:=C_{X,\\lambda}^{-\\frac{1}{2}}\\phi(x),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\|\\xi(X,Y)\\mathbb{1}\\{X\\in\\Omega_{t}\\}\\|_{S_{2}(\\mathcal{H},\\mathcal{Y})}^{m}\\right]\\le m!\\sigma^{2}(2R)^{m-2}\\displaystyle\\int_{\\Omega_{t}}\\|h_{x}\\|_{\\mathcal{H}}^{m}\\mathrm{d}\\pi(x)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\,2^{m-1}\\displaystyle\\int_{\\Omega_{t}}\\|h_{x}\\|_{\\mathcal{H}}^{m}\\,\\|F_{\\lambda}(x)-F_{*}(x)\\|_{\\mathcal{Y}}^{m}\\,\\mathrm{d}\\pi(x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We bound term (33) using Lemma 15 and Lemma 17 with $l=1$ . We have, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\int_{\\Omega_{t}}\\|h_{x}\\|_{\\mathcal H}^{m}\\mathrm{d}\\pi(x)\\leq(A\\lambda^{-\\frac{\\alpha}{2}})^{m-2}\\mathcal N_{1}(\\lambda).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Therefore we bound term (33) as follows, ", "page_idx": 27}, {"type": "equation", "text": "$$\nm!\\sigma^{2}(2R)^{m-2}\\int_{\\Omega_{t}}\\|h_{x}\\|_{\\mathcal H}^{m}\\mathrm d\\pi(x)\\le m!\\sigma^{2}\\left(\\frac{2A R}{\\lambda^{\\frac{\\alpha}{2}}}\\right)^{m-2}\\mathcal N_{1}(\\lambda).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "If $\\beta\\ge\\alpha$ , by Assumption (EMB), ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|F_{*}\\|_{\\infty}\\leq A\\|F_{*}\\|_{\\alpha}\\leq A\\|F_{*}\\|_{\\beta}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Hence by Lemma 13, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|[F_{\\lambda}]-F_{*}\\|_{\\infty}\\le\\big(\\|F_{*}\\|_{\\infty}+A\\operatorname*{max}\\{E,\\omega_{\\rho}\\}\\|F_{*}\\|_{\\beta}\\big)\\lambda^{\\frac{\\beta-\\alpha}{2}}\\le A\\big(1+\\operatorname*{max}\\{E,\\omega_{\\rho}\\}\\big)\\|F_{*}\\|_{\\beta}\\lambda^{\\frac{\\beta-\\alpha}{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "If $\\beta<\\alpha$ , by Lemma 13, we have for $\\pi$ -almost all $x\\in\\Omega_{t}$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|F_{*}(x)-F_{\\lambda}(x)\\|_{\\mathcal{Y}}\\le t+\\|[F_{\\lambda}]\\|_{L_{\\infty}(\\pi;\\mathcal{Y})}\\le t+A E\\|F_{*}\\|_{\\beta}\\lambda^{\\frac{\\beta-\\alpha}{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Therefore, for all $\\beta\\in\\lbrack0,2\\rho]$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\big(F_{*}-[F_{\\lambda}]\\big)\\mathbb{1}_{X\\in\\Omega_{t}}\\|_{L_{\\infty}(\\pi;y)}\\leq t+A\\big(1+\\operatorname*{max}\\{E,\\omega_{\\rho}\\}\\|F_{*}\\|_{\\beta}\\lambda^{\\frac{\\beta-\\alpha}{2}}\\big)=:\\chi(t,\\lambda).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Using Lemma 17 with $l=1$ , we have, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle\\,2^{m-1}\\int_{\\Omega_{t}}\\|h_{x}\\|_{\\mathcal H}^{m}\\|F_{*}(x)-F_{\\lambda}(x)\\|_{\\mathcal H}^{m}\\mathrm{d}\\pi(x)}\\\\ {\\le2^{m-1}\\chi(t,\\lambda)^{m-2}(A\\lambda^{-\\frac\\alpha2})^{m}\\|F_{*}-[F_{\\lambda}]\\|_{L_{2}(\\pi;\\mathcal H)}^{2}}\\\\ {\\displaystyle=\\left(\\frac{2\\chi(t,\\lambda)A}{\\lambda^{\\frac\\alpha2}}\\right)^{m-2}\\|F_{*}-[F_{\\lambda}]\\|_{L_{2}(\\pi;y)}^{2}\\frac{2A^{2}}{\\lambda^{\\alpha}}}\\\\ {\\displaystyle\\le m!\\left(\\frac{2\\chi(t,\\lambda)A}{\\lambda^{\\frac\\alpha2}}\\right)^{m-2}\\|F_{*}-[F_{\\lambda}]\\|_{L_{2}(\\pi;y)}^{2}\\frac{2A^{2}}{\\lambda^{\\alpha}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Putting everything together, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\ z\\left[\\|\\xi(X,Y)\\mathbb{1}\\{X\\in\\Omega_{t}\\}\\|_{S_{2}(\\mathcal{H},y)}^{m}\\right]\\le m!\\left(\\frac{2(R+\\chi(t,\\lambda))A}{\\lambda^{\\frac{\\alpha}{2}}}\\right)^{m-2}\\left(\\sigma^{2}\\mathcal{N}_{1}(\\lambda)+\\|F_{*}-[F_{\\lambda}]\\|_{L_{2}(\\pi;y)}^{2}\\frac{2A^{2}}{\\lambda^{\\alpha}}\\right)\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We now apply Theorem 10 with ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{L\\gets\\frac{2\\left(R+\\chi\\left(t,\\lambda\\right)\\right)A}{\\lambda^{\\frac{\\alpha}{2}}}}}\\\\ {{\\sigma\\gets2\\sigma\\sqrt{N_{1}(\\lambda)}+\\|F_{*}-[F_{\\lambda}]\\|_{L_{2}(\\pi;\\mathcal{V})}\\frac{2A}{\\lambda^{\\frac{\\alpha}{2}}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We bound $\\|F_{*}-\\left[F_{\\lambda}\\right]\\|_{L_{2}(\\pi;\\mathcal{V})}$ using Lemma 10 with $\\gamma=0$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|F_{*}-[F_{\\lambda}]\\|_{L_{2}(\\pi;\\mathcal{Y})}\\le\\omega_{\\rho}\\|F_{*}\\|_{\\beta}\\lambda^{\\frac{\\beta}{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The conclusion is, for all $\\tau\\geq1$ , with probability at least $1-2e^{-\\tau}$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\xi_{i}\\mathbb{I}\\left\\{x_{i}\\in\\Omega_{t}\\right\\}-\\mathbb{E}\\big[\\xi(X,Y)\\mathbb{1}\\{X\\in\\Omega_{t}\\}\\big]\\right\\|_{S_{2}(\\mathcal{H},y)}}\\\\ &{\\leq4\\sqrt{2}\\tau\\left(\\frac{2\\sigma\\sqrt{N_{1}(\\lambda)}+\\|F_{*}-[F_{\\lambda}]\\|_{L_{2}(\\pi;y)}\\frac{2A}{\\lambda^{\\frac{\\alpha}{2}}}}{\\sqrt{n}}+\\frac{2\\big(R+\\chi(t,\\lambda)\\big)A}{n\\lambda^{\\frac{\\alpha}{2}}}\\right)}\\\\ &{\\leq4\\sqrt{2}\\tau\\left(\\frac{2\\sigma}{\\sqrt{n}}\\sqrt{\\mathcal{N}_{1}(\\lambda)}+\\frac{2A\\|F_{*}\\|_{\\beta}\\omega_{\\rho}}{\\sqrt{n}\\lambda^{\\frac{\\alpha-\\beta}{2}}}+\\frac{2\\big(R+t+A\\big)A}{n\\lambda^{\\frac{\\alpha}{2}}}+\\frac{2A^{2}\\operatorname*{max}\\{E,\\omega_{\\rho}\\}\\|F_{*}\\|_{\\beta}}{n\\lambda^{\\alpha-\\frac{\\beta}{2}}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Lemma 14. Suppose that the same assumptions and notations listed in Theorem 7 hold. ", "page_idx": 28}, {"type": "text", "text": "1. Suppose $\\beta+p>\\alpha,$ , and $\\lambda\\asymp n^{-\\frac{1}{\\beta+p}}$ . For any fixed $\\tau\\geq1$ , with probability at least $1-2e^{-\\tau}$ , suppose that the truncation level $t$ satisfies ", "page_idx": 28}, {"type": "equation", "text": "$$\nt\\leq n^{{\\frac{1}{2}}\\left(1+{\\frac{p-\\alpha}{p+\\beta}}\\right)},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "then there exists a constant $c>0$ such that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left\\|\\frac{1}{n}\\sum_{i=1}^{n}\\xi_{i}\\mathbb{1}\\big\\{x_{i}\\in\\Omega_{t}\\big\\}-\\mathbb{E}\\big[\\xi(X,Y)\\mathbb{1}\\{X\\in\\Omega_{t}\\}\\big]\\right\\|_{S_{2}(\\mathcal{H},\\mathcal{Y})}\\leq c\\tau n^{-\\frac{1}{2}\\frac{\\beta}{\\beta+p}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "2. Suppose $\\beta+p\\leq\\alpha$ , and $\\begin{array}{r}{\\lambda\\asymp\\left(\\frac{n}{\\log^{\\theta}(n)}\\right)^{\\frac{1}{\\alpha}}}\\end{array}$ for some $\\theta>1,$ . For any fixed $\\tau\\geq1$ , with probability at least $1-2e^{-\\tau}$ , suppose that the truncation level $t$ satisfies ", "page_idx": 29}, {"type": "equation", "text": "$$\nt\\leq n^{{\\frac{1}{2}}\\left(1-{\\frac{\\beta}{\\alpha}}\\right)},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "then there exists a constant $c>0$ such that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left\\|{\\frac{1}{n}}\\sum_{i=1}^{n}\\xi_{i}\\mathbb{1}\\{x_{i}\\in\\Omega\\}-\\mathbb{E}[\\xi(X,Y)\\mathbb{1}\\{X\\in\\Omega\\}]\\right\\|_{\\mathcal{G}}\\leq c\\tau\\left({\\frac{n}{\\log^{\\theta}(n)}}\\right)^{-{\\frac{\\beta}{2\\alpha}}}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof. Note that Theorem 7 yields the same conclusion as in the scalar-valued case proved in [54, Theorem 13]. The Lemma then follows from the analysis for the scalar-valued case in the proof of [54, Theorem 15]. \u53e3 ", "page_idx": 29}, {"type": "text", "text": "We adapt [55, Theorem 15] to the vector-valued setting. ", "page_idx": 29}, {"type": "text", "text": "Theorem 8. Suppose that Assumptions (EMB), (EVD), (MOM) and (SRC) hold for $0\\,\\leq\\,\\beta\\,\\leq\\,2\\rho_{:}$ , where $\\rho$ is the qualification, and $p\\leq\\alpha\\leq1$ . ", "page_idx": 29}, {"type": "text", "text": "1. In the case of $\\beta+p>\\alpha_{*}$ , choosing $\\begin{array}{r}{\\lambda\\asymp n^{-\\frac{1}{\\beta+p}}}\\end{array}$ , for any fixed $\\tau\\geq\\log(4)$ , when $n$ is sufficiently large, with probability at least $1-4e^{-\\tau}$ , $w e$ have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left\\|\\left(\\left(\\hat{C}_{Y X}-C_{\\lambda}\\hat{C}_{X}\\right)-\\left(C_{Y X}-C_{\\lambda}C_{X}\\right)\\right)C_{X,\\lambda}^{-\\frac{1}{2}}\\right\\|_{S_{2}(\\mathcal{H},y)}\\leq c\\tau n^{-\\frac{1}{2}\\frac{\\beta}{\\beta+p}}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $c$ is a constant independent of $n,\\tau,\\lambda.$ . ", "page_idx": 29}, {"type": "text", "text": "2. In the case of $\\beta+p\\leq\\alpha$ , choosing $\\lambda\\asymp\\left(\\frac{n}{\\log^{\\theta}(n)}\\right)^{-\\frac{1}{\\alpha}}$ for some $\\theta>1$ . We make the additional assumption that there exists some $\\alpha^{\\prime}<\\alpha$ such that Assumption (MOM) is satisfied for $\\alpha^{\\prime}<\\alpha$ . Then, for any fixed $\\tau\\geq\\log(4)$ , when $n$ is sufficiently large, where the hidden index bound depends on $\\alpha-\\alpha^{\\prime}$ , with probability at least $1-4e^{-\\tau}$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left\\|\\big(\\big(\\hat{C}_{Y X}-C_{\\lambda}\\hat{C}_{X}\\big)-\\big(C_{Y X}-C_{\\lambda}C_{X}\\big)\\big)\\,C_{X,\\lambda}^{-\\frac{1}{2}}\\right\\|_{S_{2}(\\mathcal{H},\\mathcal{Y})}\\leq c\\tau\\left(\\frac{n}{\\log^{\\theta}(n)}\\right)^{-\\frac{\\beta}{2\\alpha}}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $c$ is a constant independent of $n,\\tau,\\lambda.$ ", "page_idx": 29}, {"type": "text", "text": "Proof. By assumption (EMB) and Theorem 6, if $\\beta<\\alpha$ , then the inclusion map ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathcal{Z}_{\\pi}^{q_{\\alpha,\\beta}}:[\\mathcal{G}]^{\\beta}\\hookrightarrow L_{q_{\\alpha,\\beta}}(\\pi;\\mathcal{V})\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "is bounded, where $\\begin{array}{r}{q_{\\alpha,\\beta}:=\\frac{2\\alpha}{\\alpha-\\beta}}\\end{array}$ . If $\\beta\\ge\\alpha$ , then by Lemma 11 the inclusion map ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathcal{Z}_{\\pi}^{\\infty}:[\\mathcal{G}]^{\\beta}\\hookrightarrow L_{\\infty}(\\pi;\\mathcal{V})\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "is bounded and therefore $[\\mathcal{G}]^{\\beta}$ is continuously embedded into $L_{q}(\\pi;\\mathcal{V})$ for any $q\\geq1$ . In the rest of the proof, we will use $q$ to denote $q_{\\alpha,\\beta}$ , unless otherwise specified. Furthermore, we will use $c_{q}=\\|\\bar{F}_{*}\\|_{L_{q}(\\pi;\\mathcal{V})}$ . ", "page_idx": 29}, {"type": "text", "text": "We first consider the case $\\beta+p>\\alpha$ . We can easily verify using $\\beta+p>\\alpha$ that the following inequality holds ", "page_idx": 29}, {"type": "equation", "text": "$$\n{\\frac{1}{2}}\\left(1+{\\frac{p-\\alpha}{p+\\beta}}\\right)>{\\frac{1}{2}}\\left({\\frac{p}{p+\\beta}}\\right)>{\\frac{\\alpha-\\beta}{2\\alpha}}={\\frac{1}{q_{\\alpha,\\beta}}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Choose $t=n^{\\tilde{q}^{-1}}$ , where ", "page_idx": 29}, {"type": "equation", "text": "$$\n{\\frac{1}{\\tilde{q}}}={\\frac{1}{2}}\\left({\\frac{1}{2}}\\left(1+{\\frac{p-\\alpha}{p+\\beta}}\\right)+{\\frac{1}{q}}\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We thus have ", "page_idx": 29}, {"type": "equation", "text": "$$\nn^{\\frac{1}{2}\\left(1+\\frac{p-\\alpha}{p+\\beta}\\right)}>t=n^{\\tilde{q}^{-1}}>n^{\\frac{1}{q_{\\alpha,\\beta}}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Thus the assumptions for both Lemma 12 and Lemma 14 are satisfied. ", "page_idx": 29}, {"type": "text", "text": "We then consider the case $\\beta+p\\leq\\alpha$ . We now apply Assumption (EMB) and Theorem 6 to $\\alpha^{\\prime}$ instead of $\\alpha$ . We obtain that the inclusion map $I_{\\pi}^{q_{\\alpha^{\\prime},\\beta}}$ is bounded, where we recall that $q_{\\alpha^{\\prime},\\beta}$ is defined to be \u03b12\u2032\u03b1\u2212\u03b2 . Since x \u21a6 $x\\mapsto{\\frac{2x}{x-\\beta}}$ is monotonically decreasing for $x>\\beta$ , we obtain the inequality ", "page_idx": 30}, {"type": "equation", "text": "$$\n{\\frac{2\\alpha^{\\prime}}{\\alpha^{\\prime}-\\beta}}>{\\frac{2\\alpha}{\\alpha-\\beta}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We choose t = nq\u03b1,\u03b2 . By construction, $t$ satisfies the assumptions in Lemma 14. Furthermore, the assumptions of Lemma 12 are satisfied, with $F_{*}\\in L_{q^{\\prime}}(\\pi,\\mathcal{Y})$ , and t = nq\u03b1,\u03b2 . ", "page_idx": 30}, {"type": "text", "text": "Having established the applicability of Lemma 14 and Lemma 12, let us turn our attention to proving the results of the Theorem. Denote ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\xi(x,y)=\\left(y-C_{\\lambda}\\phi(x)\\right)\\otimes\\left(C_{X,\\lambda}^{-\\frac{1}{2}}\\phi(x)\\right)\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We compute ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\xi(x,y)]=(C_{Y X}-C_{\\lambda}C_{X})C_{X,\\lambda}^{-\\frac{1}{2}}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "1. The $\\beta+p>\\alpha$ case. Have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\left\\|\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\xi_{i}-\\mathbb{E}\\big[\\xi(x,y)\\big]\\right\\|_{S_{2}(\\mathcal{H},y)}\\leq\\left\\|\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\xi_{i}\\mathbb{1}\\big\\{x_{i}\\in\\Omega_{t}\\big\\}-\\mathbb{E}\\big[\\xi(x,y)\\mathbb{1}\\big\\{x\\in\\Omega_{t}\\big\\}\\big]\\right\\|_{S_{2}(\\mathcal{H},y)}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\left\\|\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\xi_{i}\\mathbb{1}\\big\\{x_{i}\\in\\Omega_{t}^{c}\\big\\}\\right\\|_{S_{2}(\\mathcal{H},y)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\left\\|\\mathbb{E}\\big[\\xi(x,y)\\mathbb{1}\\left\\{x\\in\\Omega_{t}^{c}\\right\\}\\big\\|_{S_{2}(\\mathcal{H},y)}}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We can bound the first term with probability at least $1-2e^{-\\tau}$ by $c\\tau\\left({\\frac{n}{\\log^{\\theta}(n)}}\\right)^{-{\\frac{\\beta}{2\\alpha}}}$ , according to Lemma 14. By Lemma 12, with probability at least $1-e^{-\\tau}$ , for sufficiently large $n$ , $x_{i}\\in\\Omega_{t}$ for all $i\\in[n]$ , whereby the second term is zero. It remains to bound the third term, where our bound will be deterministic. Using Jensen\u2019s inequality, we have, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\mathbb{E}[\\xi(x,y)\\mathbb{1}\\left\\{x\\in\\Omega_{t}^{c}\\right\\}]\\right\\|_{S_{2}(\\mathcal{H},y)}\\leq\\mathbb{E}\\left[\\left\\|\\xi(x,y)\\mathbb{1}\\{x\\in\\Omega_{t}^{c}\\}\\right\\|_{S_{2}(\\mathcal{H},y)}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}\\left[\\left\\|(y-C_{\\lambda}\\phi(x))\\mathbb{1}\\{x\\notin\\Omega_{t}\\}\\right\\|y\\cdot\\left\\|C_{X,\\lambda}^{-\\frac{1}{2}}\\phi(x)\\right\\|_{\\varkappa}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq A\\lambda^{-\\frac{\\alpha}{2}}\\mathbb{E}\\left[\\left\\|(y-C_{\\lambda}\\phi(x))\\mathbb{1}\\{x\\notin\\Omega_{t}\\}\\right\\|y\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where in the third line we used Lemma 17. We first split the second term into an approximation error and a noise term using triangle inequality. ", "page_idx": 30}, {"type": "text", "text": "$\\mathfrak{L}\\big[\\|\\big(y-C_{\\lambda}\\phi(x)\\big)\\mathbb{1}\\{x\\,\\mathfrak{L}\\,\\Omega_{t}\\}\\|y\\big]\\leq\\mathbb{E}\\big[\\|\\big(y-F_{*}(x)\\big)\\mathbb{1}\\{x\\,\\mathfrak{L}\\,\\Omega_{t}\\}\\|y\\big]+\\mathbb{E}\\big[\\|\\big(F_{*}(x)-F_{\\lambda}(x)\\big)\\mathbb{1}\\{x\\,\\mathfrak{L}\\,\\Omega_{t}\\}\\|_{\\mathcal{H}}\\big]$ ] We bound the first term using the tower property of conditional expectation, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|\\left(y-C_{\\lambda}\\phi(x)\\right)\\mathbb{1}\\left\\{x\\notin\\Omega_{t}\\right\\}\\right\\|y\\right]\\leq\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\mathbb{E}\\big[\\left\\|y-C_{\\lambda}\\phi(x)\\big\\|y\\mid x\\big]\\mathbb{1}\\left\\{x\\notin\\Omega_{t}\\right\\}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\mathbb{E}\\big[\\left\\|y-C_{\\lambda}\\phi(x)\\right\\|_{\\mathcal{Y}}^{2}\\big|x\\big]^{\\frac{1}{2}}\\mathbb{1}\\left\\{x\\notin\\Omega_{t}\\right\\}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\sigma\\pi(x\\notin\\Omega_{t})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\frac{\\sigma C_{q}^{q}}{t^{q}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where in the third inequality we used Assumption (MOM) with $q=2$ , and in the fourth inequality we used Lemma 12. We bound the second term using Cauchy-Schwarz inequality and Lemma 10 with $\\gamma=0$ , ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\|\\big(F_{*}(x)-F_{\\lambda}(x)\\big)\\mathbb{1}\\{x\\notin\\Omega_{t}\\}\\|y\\right]\\le\\mathbb{P}\\big(x\\notin\\Omega_{t}\\big)^{\\frac{1}{2}}\\|F_{*}\\|_{\\beta}\\omega_{\\rho}\\lambda^{\\frac{\\beta}{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Therefore, using Lemma 12, we have, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\|\\mathbb{E}[\\xi(x,y)\\mathbb{1}\\left\\{x\\in\\Omega_{t}^{c}\\right\\}\\|_{S_{2}(\\mathcal{H},\\mathcal{Y})}\\leq A\\lambda^{-\\frac{\\alpha}{2}}\\left(\\frac{\\sigma c_{q}^{q}}{t^{q}}+\\frac{c_{q}^{\\frac{q}{2}}}{t^{\\frac{q}{2}}}\\|F_{*}\\|_{\\beta}\\omega_{\\rho}\\lambda^{\\frac{\\beta}{2}}\\right).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We now plug in $\\begin{array}{r}{\\lambda\\asymp n^{-\\frac{1}{\\beta+p}}}\\end{array}$ . Recall that by construction $t>n^{\\frac{1}{q}}$ . Thus, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lambda^{-\\frac{\\alpha}{2}}t^{-q}\\lesssim n^{-1}n^{\\frac{\\alpha}{2(\\beta+p)}}<n^{-1}n^{\\frac{\\beta+p}{2(\\beta+p)}}=n^{-\\frac{1}{2}}\\leq n^{-\\frac{1}{2}\\frac{\\beta}{\\beta+p}}}\\\\ &{\\lambda^{\\frac{\\beta-\\alpha}{2}}t^{-\\frac{q}{2}}\\lesssim n^{-\\frac{1}{2}}n^{\\frac{-(\\beta-\\alpha)}{2(\\beta+p)}}<n^{-\\frac{1}{2}}n^{\\frac{p}{2(\\beta+p)}}=n^{-\\frac{1}{2}\\frac{\\beta}{\\beta+p}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We\u2019ve therefore proved inequality (34). ", "page_idx": 31}, {"type": "text", "text": "2. The $\\beta+p\\leq\\alpha$ case. We proceed similarly to the $\\beta+p>\\alpha$ case. We have, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\left\\|\\frac{1}{n}\\sum_{i=1}^{n}\\xi_{i}-\\mathbb{E}\\big[\\xi(x,y)\\big]\\right\\|_{S_{2}(\\mathcal{H},y)}\\leq\\left\\|\\frac{1}{n}\\sum_{i=1}^{n}\\xi_{i}\\mathbb{1}\\big\\{x_{i}\\in\\Omega_{t}\\big\\}-\\mathbb{E}\\big[\\xi(x,y)\\mathbb{1}\\big\\{x\\in\\Omega_{t}\\big\\}\\big]\\right\\|_{S_{2}(\\mathcal{H},y)}}}\\\\ &{\\qquad\\qquad\\qquad+\\left\\|\\frac{1}{n}\\sum_{i=1}^{n}\\xi_{i}\\mathbb{1}\\big\\{x_{i}\\in\\Omega_{t}^{c}\\big\\}\\right\\|_{S_{2}(\\mathcal{H},y)}}\\\\ &{\\qquad\\qquad\\qquad+\\left\\|\\mathbb{E}\\big[\\xi(x,y)\\mathbb{1}\\left\\{x\\in\\Omega_{t}^{c}\\right\\}\\big]\\right\\|_{S_{2}(\\mathcal{H},y)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We can bound the first term with probability at least 1 \u22122e\u2212\u03c4 by c\u03c4 (log\u03b8n(n)) , according to Lemma 14. By Lemma 12, with probability at least $1-e^{-\\tau}$ , for sufficiently large $n$ , $x_{i}\\in\\Omega_{t}$ for all $i\\in[n]$ , whereby the second term is zero. We bound the third term by Eq. (36). We now plug in $\\lambda\\asymp\\left(\\frac{n}{\\log^{\\theta}(n)}\\right)^{-\\frac{1}{\\alpha}}$ . Recall that by construction $t>n^{\\frac{1}{q}}$ . Thus, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lambda^{-\\frac{\\alpha}{2}}t^{-q}\\lesssim n^{-1}\\left(\\cfrac{n}{\\log^{\\theta}(n)}\\right)^{\\frac{1}{2}}<\\left(\\cfrac{n}{\\log^{\\theta}(n)}\\right)^{-\\frac{1}{2}}\\leq\\left(\\cfrac{n}{\\log^{\\theta}(n)}\\right)^{-\\frac{\\beta}{2\\alpha}}}\\\\ &{\\lambda^{\\frac{\\beta-\\alpha}{2}}t^{-\\frac{q}{2}}\\lesssim n^{-\\frac{1}{2}}\\left(\\cfrac{n}{\\log^{\\theta}(n)}\\right)^{\\frac{\\alpha-\\beta}{2\\alpha}}<\\left(\\cfrac{n}{\\log^{\\theta}(n)}\\right)^{\\frac{-\\beta}{2\\alpha}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We have therefore proved inequality (35). ", "page_idx": 31}, {"type": "text", "text": "We adapt [54, Theorem 16] to the vector-valued setting. ", "page_idx": 31}, {"type": "text", "text": "Theorem 9 (Bound of estimation error). Suppose that assumptions (EMB), (EVD), (MOM) and (SRC) hold for $0\\le\\beta\\le2\\rho$ , where $\\rho$ is the qualification, and $p\\leq\\alpha<1$ . For $0\\leq\\gamma\\leq1$ , with $\\gamma\\le\\beta$ , ", "page_idx": 31}, {"type": "text", "text": "1. In the case of $\\beta+p\\,>\\,\\alpha,$ , by choosing $\\lambda\\asymp n^{-\\frac{1}{\\beta+p}}$ , for any fixed $\\tau\\,\\geq\\,\\log(4)$ , when n is sufficiently large, with probability at least $1-4e^{-\\tau}$ , we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\big\\|\\big[\\hat{C}_{\\lambda}-C_{\\lambda}\\big]\\big\\|_{S_{2}([\\mathcal{H}]^{\\gamma},\\mathcal{Y})}^{2}\\leq c\\tau^{2}n^{-\\frac{\\beta-\\gamma}{\\beta+p}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $c$ is a constant independent of $n,\\tau$ . ", "page_idx": 31}, {"type": "text", "text": "2. In the case of $\\beta+p\\leq\\alpha,$ , by choosing $\\lambda\\asymp\\left(\\frac{n}{\\log^{\\theta}(n)}\\right)^{-\\frac{1}{\\alpha}}$ , for any fixed $\\tau\\geq\\log(4)$ , when n is sufficiently large, with probability at least $1-4e^{-\\tau}$ , we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\big\\|\\big[\\hat{C}_{\\lambda}-C_{\\lambda}\\big]\\big\\|_{S_{2}([\\mathcal H]^{\\gamma},\\mathcal P)}^{2}\\leq c\\tau^{2}\\left(\\frac{n}{\\log^{\\theta}(n)}\\right)^{-\\frac{\\beta-\\gamma}{\\alpha}}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where c is a constant independent of $n,\\tau$ . ", "page_idx": 31}, {"type": "text", "text": "Proof. Firstly, we establish the applicability of Lemma 19. ", "page_idx": 31}, {"type": "text", "text": "1. The $\\beta+p>\\alpha$ case. Have $\\lambda\\asymp n^{-\\frac{1}{\\beta+p}}$ , hence ", "page_idx": 31}, {"type": "equation", "text": "$$\nn\\lambda^{\\alpha}\\gtrsim n^{\\frac{\\beta+p-\\alpha}{\\beta+p}}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "whereas using $\\lambda\\leq\\|C_{X}\\|_{\\mathcal{H}\\rightarrow\\mathcal{H}}$ for sufficiently large $n$ , as well as Lemma 16, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\lambda A^{2}\\tau\\log\\left(2e N(\\lambda)\\frac{\\|C_{X}\\|_{\\mathcal{H}\\to\\mathcal{H}}+\\lambda}{\\|C_{X}\\|_{\\mathcal{H}\\to\\mathcal{H}}}\\right)\\le8A^{2}\\tau\\log(4e c_{2,1}\\lambda^{-p})\\lesssim8A^{2}\\tau\\left(\\log(4e c_{2,1})+\\frac{p}{\\beta+p}\\log(n)\\right)\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Therefore, for a fixed $\\tau>0$ , for all sufficiently large $n$ , Eq. (40) in Lemma 19 is satisfied. ", "page_idx": 32}, {"type": "text", "text": "2. The $\\beta+p\\leq\\alpha$ case. Have $\\lambda\\asymp\\left(\\frac{n}{\\log^{\\theta}(n)}\\right)^{-\\frac{1}{\\alpha}}$ for some $\\theta>1$ , hence ", "page_idx": 32}, {"type": "equation", "text": "$$\nn\\lambda^{\\alpha}\\geq\\log^{\\theta}(n)\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "whereas similar to the $\\beta+p>\\alpha$ case, we ahve ", "page_idx": 32}, {"type": "equation", "text": "$$\n8A^{2}\\tau\\log\\left(2e{\\cal N}(\\lambda)\\frac{\\|C_{X}\\|_{\\mathcal{H}\\to\\mathcal{H}}+\\lambda}{\\|C_{X}\\|_{\\mathcal{H}\\to\\mathcal{H}}}\\right)\\lesssim8A^{2}\\tau\\left(\\log(4e c_{2,1})+\\frac{p}{\\alpha}\\log\\left(\\frac{n}{\\log^{\\theta}(n)}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Therefore, for a fixed $\\tau>0$ , for all sufficiently large $n$ , Eq. (40) in Lemma 19 is satisfied. ", "page_idx": 32}, {"type": "text", "text": "We thus conclude for all $\\alpha\\in(0,1]$ , with probability $\\geq1-2e^{-\\tau}$ , Eq. (41) and (42) are satisfied simultaneously. ", "page_idx": 32}, {"type": "text", "text": "We exploit the following decomposition ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\|\\left[\\hat{C}_{\\lambda}-C_{\\lambda}\\right]\\right\\|_{S_{2}([\\mathcal{H}]^{\\gamma},y)}\\leq\\left\\|\\left(\\hat{C}_{\\lambda}-C_{\\lambda}\\right)C_{X}^{\\frac{1-\\gamma}{2}}\\right\\|_{S_{2}(\\mathcal{H},y)}}&{}\\\\ {\\leq\\left\\|\\left(\\hat{C}_{\\lambda}-C_{\\lambda}\\right)\\hat{C}_{X,\\lambda}^{\\frac{1}{2}}\\right\\|_{S_{2}(\\mathcal{H},y)}\\cdot\\left\\|\\hat{C}_{X,\\lambda}^{-\\frac{1}{2}}C_{X,\\lambda}^{\\frac{1}{2}}\\right\\|_{\\mathcal{H}\\to\\mathcal{H}}\\cdot\\left\\|C_{X,\\lambda}^{-\\frac{1}{2}}C_{X}^{\\frac{1-\\gamma}{2}}\\right\\|_{\\mathcal{H}\\to\\mathcal{H}}}\\\\ {\\leq\\left\\|\\left(\\hat{C}_{\\lambda}-C_{\\lambda}\\right)\\hat{C}_{X,\\lambda}^{\\frac{1}{2}}\\right\\|_{S_{2}(\\mathcal{H},y)}\\cdot3\\cdot\\operatorname*{sup}\\frac{\\mu_{i}^{\\frac{1-\\gamma}{2}}}{i\\in\\mathbb{N}}}&{}\\\\ {\\leq\\left\\|\\left(\\hat{C}_{\\lambda}-C_{\\lambda}\\right)\\hat{C}_{X,\\lambda}^{\\frac{1}{2}}\\right\\|_{S_{2}(\\mathcal{H},y)}\\cdot3\\lambda^{-\\frac{\\gamma}{2}},}&{\\mathcal{()}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where in the first inequality we used Lemma 22, in the third inequality we used Eq. (42) and in the last inequality we used Lemma 21. We consider the following decomposition ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{C}_{\\lambda}-C_{\\lambda}=\\hat{C}_{\\lambda}-C_{\\lambda}\\left(\\hat{C}_{X}g_{\\lambda}\\left(\\hat{C}_{X}\\right)+r_{\\lambda}\\left(\\hat{C}_{X}\\right)\\right)}\\\\ &{\\quad\\quad\\quad\\quad=\\left(\\hat{C}_{Y X}-C_{\\lambda}\\hat{C}_{X}\\right)g_{\\lambda}\\left(\\hat{C}_{X}\\right)-C_{\\lambda}r_{\\lambda}\\left(\\hat{C}_{X}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Hence ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\left\\|\\left[\\hat{C}_{\\lambda}-C_{\\lambda}\\right]\\right\\|_{S_{2}([\\mathcal{H}]^{\\gamma},\\mathcal{Y})}^{2}\\le18\\lambda^{-\\gamma}\\left((I)^{2}+(I I)^{2}\\right),\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(I\\right)=\\left\\|\\left(\\hat{C}_{Y X}-C_{\\lambda}\\hat{C}_{X}\\right)\\hat{C}_{X,\\lambda}^{\\frac{1}{2}}g_{\\lambda}\\left(\\hat{C}_{X}\\right)\\right\\|_{S_{2}(\\mathcal{H},\\mathcal{Y})}}\\\\ &{\\left(I I\\right)=\\left\\|C_{\\lambda}r_{\\lambda}\\left(\\hat{C}_{X}\\right)\\hat{C}_{X,\\lambda}^{\\frac{1}{2}}\\right\\|_{S_{2}(\\mathcal{H},\\mathcal{Y})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Term (I). The high level idea is to bound (I) by exploiting the first axiom of the filter function (8), where $g_{\\lambda}(\\hat{C}_{X})$ is intuitively a regularized inverse of ${\\hat{C}}_{X}$ , by grouping it with $\\hat{C}_{X,\\lambda}$ . ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(I)\\leq\\left\\|\\left(\\hat{C}_{Y X}-C_{\\lambda}\\hat{C}_{X}\\right)C_{X,\\lambda}^{-\\frac{1}{2}}\\right\\|_{S_{2}(\\mathcal{H},y)}\\cdot\\left\\|C_{X\\lambda}^{\\frac{1}{2}}\\hat{C}_{X,\\lambda}^{-\\frac{1}{2}}\\right\\|_{\\mathcal{H}\\to\\mathcal{H}}\\cdot\\left\\|\\hat{C}_{X,\\lambda}g_{\\lambda}\\left(\\hat{C}_{X}\\right)\\right\\|_{\\mathcal{H}\\to\\mathcal{H}}}\\\\ &{\\quad\\leq\\left\\|\\left(\\hat{C}_{Y X}-C_{\\lambda}\\hat{C}_{X}\\right)C_{X,\\lambda}^{-\\frac{1}{2}}\\right\\|_{S_{2}(\\mathcal{H},y)}\\cdot\\sqrt{3}\\operatorname*{\\sup}_{t\\in[0,\\kappa^{2}]}(t+\\lambda)g_{\\lambda}(t)}\\\\ &{\\quad\\leq\\left\\|\\left(\\hat{C}_{Y X}-C_{\\lambda}\\hat{C}_{X}\\right)C_{X,\\lambda}^{-\\frac{1}{2}}\\right\\|_{S_{2}(\\mathcal{H},y)}\\cdot2\\sqrt{3}E.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where the second inequality follows from Eq. (42). We consider the following decomposition ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\bigg\\|\\big(\\hat{C}_{Y X}-C_{\\lambda}\\hat{C}_{X}\\big)\\,C_{X,\\lambda}^{-\\frac{1}{2}}\\bigg\\|_{S_{2}(\\mathcal{H},\\mathcal{Y})}^{2}\\leq2\\,\\bigg\\|\\big(\\big(\\hat{C}_{Y X}-C_{\\lambda}\\hat{C}_{X}\\big)-\\big(C_{Y X}-C_{\\lambda}C_{X}\\big)\\big)\\,C_{X,\\lambda}^{-\\frac{1}{2}}\\bigg\\|_{S_{2}(\\mathcal{H},\\mathcal{Y})}^{2}}&\\\\ {+\\,2\\,\\bigg\\|\\big(C_{Y X}-C_{\\lambda}C_{X}\\big)\\,C_{X,\\lambda}^{-\\frac{1}{2}}\\bigg\\|_{S_{2}(\\mathcal{H},\\mathcal{Y})}^{2}}&\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We bound the first term by Theorem 8 and the second term by Lemma 8. This yields, for $\\tau\\geq\\log(4)$ , with probability at least $1-4e^{-\\tau}$ , for some constant $c>0$ which does not depend on $n,\\tau,\\lambda$ , ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\|\\left(\\hat{C}_{Y X}-C_{\\lambda}\\hat{C}_{X}\\right)C_{X,\\lambda}^{-\\frac{1}{2}}\\right\\|_{S_{2}(\\mathcal{H},\\mathcal{Y})}^{2}\\le2\\omega_{\\rho}^{2}\\|F_{*}\\|_{\\beta}^{2}\\lambda^{\\beta}+\\left\\{C\\tau^{2}n^{-\\frac{\\beta}{\\beta+p}}\\right.}&{\\beta+p\\geq\\alpha}\\\\ {\\left.\\left[c\\tau^{2}\\left(\\frac{n}{\\log^{\\theta}(n)}\\right)^{-\\frac{\\beta}{\\alpha}}\\right.}&{\\beta+p<\\alpha}\\\\ {\\left.\\leq\\left\\{\\tau^{2}(c+2\\|F_{*}\\|_{\\beta}^{2}\\omega_{\\rho}^{2}\\lambda^{\\beta})n^{-\\frac{\\beta}{\\beta+p}}\\right.}&{\\beta+p\\geq\\alpha}\\\\ {\\left.\\tau^{2}(c+2\\|F_{*}\\|_{\\beta}^{2}\\omega_{\\rho}^{2}\\lambda^{\\beta})\\left(\\frac{n}{\\log^{\\theta}(n)}\\right)^{-\\frac{\\beta}{\\alpha}}\\right.}&{\\beta+p<\\alpha}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where we used that $\\tau>1$ . So collecting all the relevant constants together, we can write the upper bound of term (I) as follows: with probability at least $1-4e^{-\\tau}$ , for some constant $c^{\\prime}>0$ (different from the $c$ before) which does not depend on $n,\\tau,\\lambda$ , we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n(I)\\leq c^{\\prime}\\tau\\cdot{\\left\\{\\begin{array}{l l}{n^{-{\\frac{1}{2}}{\\frac{\\beta}{\\beta+p}}}}&{\\beta+p\\geq\\alpha}\\\\ {\\left({\\frac{n}{\\log^{\\theta}(n)}}\\right)^{-{\\frac{\\beta}{2\\alpha}}}}&{\\beta+p<\\alpha.}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Term $(\\mathbf{II})$ . Using Lemma 9, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n(I I)\\leq B\\left\\|\\hat{C}_{X,\\lambda}^{\\frac{1}{2}}r_{\\lambda}(\\hat{C}_{X})g_{\\lambda}(C_{X})C_{X}^{\\frac{\\beta+1}{2}}\\right\\|_{\\mathcal{H}\\rightarrow\\mathcal{H}}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "The second term is the same as the scalar-valued case, which is bounded in Step 3 of the proof of [54, Theorem 16]. We define ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\Delta_{1}:=32\\operatorname*{max}\\left\\{\\frac{\\beta-1}{2},1\\right\\}E\\omega_{\\rho}\\kappa^{\\beta-1}\\lambda^{\\frac{1}{2}}n^{-\\frac{\\operatorname*{min}(\\beta,3)-1}{4}}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "By the proof of [54, Theorem 16], we have, with probability at least $1-6e^{-\\tau}$ ", "page_idx": 33}, {"type": "equation", "text": "$$\n(I I)\\leq6B\\omega_{\\rho}E\\lambda^{\\frac{\\beta}{2}}+\\Delta_{1}B\\tau\\mathbb{1}\\{\\beta>2\\}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "1. Case $\\beta+p>\\alpha$ . In this case $\\lambda\\asymp n^{-\\frac{1}{\\beta+p}}$ . We note that for $\\beta>2,\\,\\Delta_{1}$ as a function of $n$ can be written as ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\Delta_{1}\\asymp n^{-\\frac{1}{2(\\beta+p)}-\\frac{\\operatorname*{min}(\\beta,3)-1}{4}}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Note that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\frac{1}{2(\\beta+p)}+\\frac{\\operatorname*{min}(\\beta,3)-1}{4}-\\frac{\\beta}{2(\\beta+p)}=\\frac{1}{2}\\left(\\frac{p}{\\beta+p}+\\frac{\\operatorname*{min}(\\beta,3)-1}{2}\\right)>0\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Hence ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\Delta_{1}\\lesssim n^{-\\frac{\\beta}{2(\\beta+p)}}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Therefore we have shown that there exists some constant $c^{\\prime\\prime}>0$ , independent of $n,\\lambda,\\tau$ , such that with probability at least $1-6e^{-\\tau}$ , for sufficiently large $n$ , ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\left\\|\\left[\\hat{C}_{\\lambda}-C_{\\lambda}\\right]\\right\\|_{S_{2}([\\mathcal{H}]^{\\gamma},\\mathcal{V})}\\le c^{\\prime\\prime}\\tau n^{-\\frac{1}{2}\\frac{\\beta-\\gamma}{\\beta+p}}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "2. Case $\\beta+p\\leq\\alpha$ . In this case $\\beta\\leq\\alpha\\leq1$ , and $\\begin{array}{r}{\\lambda\\asymp\\left(\\frac{n}{\\log^{\\theta}(n)}\\right)^{-\\frac{1}{\\alpha}}}\\end{array}$ . We have also shown that there exists some constant $c^{\\prime\\prime}>0$ , independent of $n,\\lambda,\\tau$ , such that with probability at least $1-6e^{-\\tau}$ , for sufficiently large $n$ , ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\left\\|\\left[\\hat{C}_{\\lambda}-C_{\\lambda}\\right]\\right\\|_{S_{2}([\\mathcal H]\\cap\\mathcal P)}\\le c^{\\prime\\prime}\\tau\\left(\\frac{n}{\\log^{\\theta}(n)}\\right)^{-\\frac{\\beta-\\gamma}{2\\alpha}}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Putting together Lemma 10 and Theorem 9, we have proved Theorem 4. ", "page_idx": 33}, {"type": "text", "text": "D Auxiliary Results ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "D.1 Spectral Calculus, Proof of Proposition 1 and Empirical Solution ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Definition 9 (Spectral Calculus; see 16, Chapter 2.3). Let $H$ be a Hilbert space. Consider $g:\\mathbb{R}\\to\\mathbb{R}$ and a self-adjoint compact operator $A:H\\rightarrow H$ admitting a spectral decomposition written as ", "page_idx": 34}, {"type": "equation", "text": "$$\nA=\\sum_{i\\in I}\\lambda_{i}h_{i}\\otimes h_{i}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We then define $g(A):H\\rightarrow H$ as ", "page_idx": 34}, {"type": "equation", "text": "$$\ng(A):=\\sum_{i\\in I}g(\\lambda_{i})h_{i}\\otimes h_{i}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "whenever this series converges in operator norm. ", "page_idx": 34}, {"type": "text", "text": "Proof of Proposition $^{\\,l}$ . We define the sampling operator $S:\\mathbb{R}^{n}\\rightarrow\\mathcal{H}$ and it dual $S^{*}:{\\mathcal{H}}\\rightarrow\\mathbb{R}^{n}$ , ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S:\\mathbb{R}^{n}\\to\\mathcal{H},}\\\\ &{\\qquad\\alpha\\mapsto\\displaystyle\\sum_{i=1}^{n}\\alpha_{i}\\phi(x_{i})\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad f\\mapsto(f(x_{i}))_{i=1}^{n}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We can verify that $\\hat{C}_{X}=n^{-1}S S^{*}$ and $\\mathbf{K}=S^{*}S$ . Let $\\mathbf{Y}=\\left(y_{i}\\right)_{i=1}^{n}\\in\\mathbb{R}^{n}$ . We have, for all $x\\in\\mathscr{X}$ , ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\hat{F}_{\\lambda}(x)=\\hat{C}_{\\lambda}\\phi(X)}\\\\ &{=\\bigg(\\frac{1}{n}\\sum_{i=1}^{n}y_{i}\\otimes\\phi(x_{i})\\bigg)g_{\\lambda}(\\hat{C}_{X})\\phi(X)}\\\\ &{=\\sum_{y\\in\\mathcal{Y}_{n}}\\bigg(\\phi(x_{i}),\\frac{1}{n}g_{\\lambda}(n^{-1}S^{\\ast})\\phi(X)\\bigg)_{n}}\\\\ &{=\\mathbf{Y}^{T}S^{\\ast}\\left(\\frac{1}{n}g_{\\lambda}(n^{-1}S^{\\ast})\\phi(X)\\right)}\\\\ &{=\\mathbf{Y}^{T}\\frac{1}{n}g_{\\lambda}(n^{-1}S^{\\ast})S^{\\ast}\\phi(X)}\\\\ &{=\\mathbf{Y}^{T}\\frac{1}{n}g_{\\lambda}(n^{-1}S^{\\ast})S^{\\ast}\\phi(X)}\\\\ &{=\\mathbf{Y}^{T}\\frac{1}{n}g_{\\lambda}(n^{-1}\\mathbf{K})S^{\\ast}\\phi(X)}\\\\ &{=\\mathbf{Y}^{T}\\frac{1}{n}g_{\\lambda}(n^{-1}\\mathbf{K})\\mathbf{k}_{X}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "To go from (38) to (39), we make the following observation. Consider the singular value decomposition of the compact operator $S$ , there is $m\\leq n$ such that ", "page_idx": 34}, {"type": "equation", "text": "$$\nS=\\sum_{i=1}^{m}\\sqrt{\\sigma_{i}}f_{i}\\otimes e_{i}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $(e_{i})_{i},(f_{i})_{i}$ are orthonormal sequences in $\\mathbb{R}^{n}$ and $\\mathcal{H}$ respectively. We then have ", "page_idx": 34}, {"type": "equation", "text": "$$\nS S^{*}=\\sum_{i=1}^{m}\\sigma_{i}f_{i}\\otimes f_{i},\\quad S^{*}S=\\sum_{i=1}^{m}\\sigma_{i}e_{i}\\otimes e_{i}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Therefore, we deduce ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{S^{*}g_{\\lambda}\\left(\\frac{S S^{*}}{n}\\right)=\\left(\\displaystyle\\sum_{i}\\sqrt{\\sigma_{i}}e_{i}\\otimes f_{i}\\right)\\left(\\displaystyle\\sum_{j}g_{\\lambda}\\left(\\frac{\\sigma_{j}}{n}\\right)f_{j}\\otimes f_{j}\\right)}&{{}}\\\\ {=\\displaystyle\\sum_{i,j}g_{\\lambda}\\left(\\frac{\\sigma_{j}}{n}\\right)\\sqrt{\\sigma_{i}}e_{i}\\otimes f_{j}\\langle f_{i},f_{j}\\rangle_{\\mathcal{H}}}&{{}}\\\\ {=\\displaystyle\\sum_{i}g_{\\lambda}\\left(\\frac{\\sigma_{i}}{n}\\right)\\sqrt{\\sigma_{i}}e_{i}\\otimes f_{i}}&{{}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Similarly, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\left.g_{\\lambda}\\left(\\frac{S^{*}S}{n}\\right)S^{*}=\\left(\\sum_{j}g_{\\lambda}\\left(\\frac{\\sigma_{j}}{n}\\right)e_{j}\\otimes e_{j}\\right)\\left(\\sum_{i}\\sqrt{\\sigma_{i}}e_{i}\\otimes f_{i}\\right)\\right.}\\\\ {\\displaystyle=\\sum_{i,j}g_{\\lambda}\\left(\\frac{\\sigma_{j}}{n}\\right)\\sqrt{\\sigma_{i}}e_{j}\\otimes f_{i}\\langle e_{i},e_{j}\\rangle_{\\mathbb{R}^{n}}}\\\\ {\\displaystyle=\\sum_{i}g_{\\lambda}\\left(\\frac{\\sigma_{i}}{n}\\right)\\sqrt{\\sigma_{i}}e_{i}\\otimes f_{i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Hence we have proved ", "page_idx": 35}, {"type": "equation", "text": "$$\nS^{*}g_{\\lambda}\\left(\\frac{S S^{*}}{n}\\right)=g_{\\lambda}\\left(\\frac{S^{*}S}{n}\\right)S^{*}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "as desired. ", "page_idx": 35}, {"type": "text", "text": "Proposition 2. Any minimizer $F\\in{\\mathcal{G}}$ of ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathcal{E}_{n}(F):=\\frac{1}{n}\\sum_{i=1}^{n}\\|y_{i}-F(x_{i})\\|_{\\mathcal{V}}^{2}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "on $\\mathcal{G}$ must satisfy ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\hat{C}_{Y X}=\\hat{C}\\hat{C}_{X},\\qquad C\\in S_{2}(\\mathcal{H},\\mathcal{Y}),\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where $F(\\cdot)=C\\phi(\\cdot)$ . ", "page_idx": 35}, {"type": "text", "text": "Proof. By Corollary 1, it is equivalent to solve the following optimization problem on $S_{2}(\\mathcal{H},\\mathcal{V})$ , ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{C\\in S_{2}(\\mathcal H,\\mathcal y)}\\frac1n\\sum_{i=1}^{n}\\|y_{i}-C\\phi(x_{i})\\|_{\\mathcal y}^{2}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Recall for a Hilbert-Schmidt operator $L\\in S_{2}(\\mathcal{H},\\mathcal{Y})$ , we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\langle L,a\\otimes b\\rangle_{S_{2}(\\mathcal{H},\\mathcal{Y})}=\\langle a,L b\\rangle_{\\mathcal{Y}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Using this, we re-write the objective as an inner product in $S_{2}(\\mathcal{H},\\mathcal{V})$ : ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\|y_{i}-C\\phi(x_{i})\\|_{\\mathcal{Y}}^{2}=\\frac{1}{n}\\sum_{i=1}^{n}-2\\langle C,y_{i}\\otimes\\phi(x_{i})\\rangle_{S_{2}(\\mathcal{H},\\mathcal{Y})}+\\langle C,\\left(C\\phi(x_{i})\\right)\\otimes\\phi(x_{i})\\rangle_{S_{2}(\\mathcal{H},\\mathcal{Y})}+\\langle C,\\left(C\\phi(x_{i})\\right)\\rangle_{S_{2}(\\mathcal{H},\\mathcal{Y})}}\\\\ {=-2\\langle C,\\hat{C}_{Y X}\\rangle_{S_{2}(\\mathcal{H},\\mathcal{Y})}+\\langle C,C\\hat{C}_{X}\\rangle_{S_{2}(\\mathcal{H},\\mathcal{Y})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Taking the Fr\u00e9chet derivative with respect to $C$ and setting in to zero, we obtain the following first order condition ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\hat{C}_{Y X}=C\\hat{C}_{X}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "D.2 Properties Related to Assumptions (EMB) and (EVD) ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Lemma 15 (Lemma 13 [17]). Under (EMB), the following inequality is satisfied, for $\\lambda>0$ and $\\pi$ -almost all $x\\in\\mathscr{X}$ , ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\left(C_{X}+\\lambda I d_{\\mathcal{H}}\\right)^{-\\frac{1}{2}}k(x,\\cdot)\\right\\|_{\\mathcal{H}}\\leq A\\lambda^{-\\frac{\\alpha}{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Definition 10 ( $l$ -effective dimension). For $l\\geq1$ , the $l$ -effective dimension $\\mathcal{N}_{l}:(0,\\infty)\\rightarrow[0,\\infty)$ is defined by ", "page_idx": 35}, {"type": "equation", "text": "$$\n{\\mathcal{N}}_{l}(\\lambda):={\\mathrm{Tr}}\\left[C_{X}^{l}C_{X,\\lambda}^{-l}\\right]=\\sum_{i\\geq1}\\left({\\frac{\\mu_{i}}{\\mu_{i}+\\lambda}}\\right)^{l}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "The 1-effective dimension is widely considered in the statistical analysis of kernel ridge regression (see [6], [5], [32], [34], [17]). The following lemma provides upper and lower bounds for the $l$ \u2212effective dimension. ", "page_idx": 35}, {"type": "text", "text": "Lemma 16. Suppose Assumption (EVD) holds with parameter $p\\in(0,1]$ , for any $\\lambda\\in(0,1]$ , there exists a constant $c_{2,l}>0$ independent of $\\lambda$ such that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathcal{N}_{l}(\\lambda)\\leq c_{2,l}\\lambda^{-p}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "If furthermore, Assumption $\\mathrm{(EVD+)}$ holds with parameter $p\\in(0,1)$ , for any $\\lambda\\in(0,1]$ , there exists a constant $c_{1,l}>0$ independent of $\\lambda$ such that ", "page_idx": 36}, {"type": "equation", "text": "$$\nc_{1,l}\\lambda^{-p}\\leq\\mathcal{N}_{l}(\\lambda)\\leq c_{2,l}\\lambda^{-p}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "The proof can be found in [29] (Proposition D.1), but as the proof is incomplete we provide a full proof for completeness. This allows us to detect that the value $p=1$ in Assumption $\\mathrm{(EVD+)}$ is not compatible with the assumption of a bounded kernel (see Remark 7 below). ", "page_idx": 36}, {"type": "text", "text": "Proof. ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathcal{N}_{l}(\\lambda)\\leq\\sum_{i\\geq1}\\left(\\frac{D_{2}}{D_{2}+\\lambda^{i}}\\right)^{l}\\qquad(x\\mapsto\\frac{x}{x+\\lambda}\\operatorname{is~monotonically~increasing})}\\\\ &{\\leq\\displaystyle\\int_{0}^{+\\infty}\\left(\\frac{D_{2}}{D_{2}+\\lambda x^{\\frac{1}{p}}}\\right)^{l}\\,\\mathrm{d}x\\qquad(i\\mapsto\\left(\\frac{D_{2}}{D_{2}+\\lambda^{i}\\lambda^{p}}\\right)^{l}\\mathrm{~is~positive~and~decreasing})}\\\\ &{\\displaystyle=\\int_{0}^{+\\infty}\\left(\\frac{D_{2}}{D_{2}+y^{\\frac{1}{p}}}\\right)^{l}\\frac{\\,\\mathrm{d}y}{\\lambda^{p}}\\qquad(y^{1/p}=\\lambda x^{1/p})}\\\\ &{\\displaystyle\\leq\\lambda^{-p}\\left(1+\\int_{1}^{+\\infty}\\left(\\frac{D_{2}}{D_{2}+y^{\\frac{1}{p}}}\\right)^{l}\\mathrm{d}y\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Let us now consider the integral. Let us first consider $p\\leq1<l$ , ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle{\\int_{1}^{\\infty}\\left(\\frac{D_{2}}{D_{2}+y^{\\frac{1}{p}}}\\right)^{l}\\mathrm{d}y\\leq D_{2}^{l}\\int_{1}^{\\infty}y^{-\\frac{l}{p}}\\mathrm{d}y}}}\\\\ {{\\displaystyle{=D_{2}^{l}\\frac{p}{l-p}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Therefore, using $\\lambda\\leq1$ , we can take $\\begin{array}{r}{c_{2,l}=1+D_{2}^{l}\\frac{p}{l-p}}\\end{array}$ . The remaining edge case $p=1=l$ , is covered by [17, Lemma 11] with $c_{2,1}=\\left\\|C_{X}\\right\\|_{S_{1}(\\mathcal{H})}$ . For the lower bound, we proceed similarly. For $p\\in(0,1)$ (and therefore $p<l$ ), ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathcal{N}_{l}(\\lambda)\\geq\\sum_{i\\geq1}\\left(\\frac{D_{1}}{D_{1}+\\lambda^{i}}\\right)^{l}}&{\\quad\\left(x\\mapsto\\frac{x}{x}\\:\\mathrm{is~monotonically~increasing}\\right)}\\\\ &{\\displaystyle\\qquad\\geq\\int_{1}^{\\infty}\\left(\\frac{D_{1}}{D_{1}+\\lambda^{\\alpha}}\\right)^{l}\\mathrm{d}x}&{\\quad\\left(i\\mapsto\\left(\\frac{D_{1}}{D_{1}+\\lambda^{i/l}}\\right)^{l}\\mathrm{~is~positive~and~decreasing}\\right)}\\\\ &{\\displaystyle\\qquad=\\int_{1}^{\\infty}\\left(\\frac{D_{1}}{D_{1}+y^{\\frac{1}{\\beta}}}\\right)^{l}\\frac{\\mathrm{d}y}{\\lambda^{p}}}&{\\quad\\left(y^{1/p}=\\lambda x^{1/p}\\right)}\\\\ &{\\displaystyle\\qquad\\geq\\lambda^{-p}\\int_{1}^{\\infty}\\left(\\frac{D_{1}}{D_{1}+1}\\right)^{l}y^{-\\frac{1}{p}}\\mathrm{d}y}\\\\ &{\\displaystyle\\qquad=\\lambda^{-p}\\left(\\frac{D_{1}}{D_{1}+1}\\right)^{l}\\frac{p}{l-p}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Therefore, we can take $\\begin{array}{r}{c_{1,l}=\\left(\\frac{D_{1}}{D_{1}+1}\\right)^{l}\\frac{p}{l-p}}\\end{array}$ . ", "page_idx": 36}, {"type": "text", "text": "Remark 7. We note that Assumption $\\mathrm{(EVD+)}$ ) with $p=1$ is not compatible with the assumption that $k$ is bounded (Assumption 3). Indeed, suppose that $\\mu_{i}\\geq D_{1}i^{-1}$ , for all $i\\geq1$ . Recall that $\\{[e_{i}]\\}_{i\\geq1}$ forms an orthonormal set in $L_{2}(\\pi)$ . By Mercer\u2019s theorem, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\kappa^{2}\\geq\\int_{\\mathcal X}k(x,x)\\pi(\\mathrm{d}x)=\\sum_{i\\geq1}\\mu_{i}\\int_{\\mathcal X}e_{i}(x)^{2}\\pi(\\mathrm{d}x)=\\sum_{i\\geq1}\\mu_{i}\\geq D_{1}\\sum_{i\\geq1}i^{-1}=+\\infty,\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "which leads to a contradiction. ", "page_idx": 36}, {"type": "text", "text": "Lemma 17. For any $l\\in[1,2]$ , the following equality holds, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\int_{X}\\left\\|[C_{X,\\lambda}^{-\\frac{l}{2}}k(x,\\cdot)]\\right\\|_{2-l}^{2}d\\pi(x)=\\mathcal{N}_{l}(\\lambda).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "In particular for $l=1$ , ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\int_{\\mathcal{X}}\\left\\|C_{X,\\lambda}^{-\\frac{1}{2}}k(x,\\cdot)\\right\\|_{\\mathcal{H}}^{2}d\\pi(x)=\\mathcal{N}_{1}(\\lambda),\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "and for $l=2$ , ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\int_{X}\\left\\|[C_{X,\\lambda}^{-1}k(x,\\cdot)]\\right\\|_{L_{2}(\\pi)}^{2}d\\pi(x)=\\mathcal{N}_{2}(\\lambda).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof. Fix $x\\in\\mathscr{X}$ . Since $k(x,\\cdot)\\in\\mathcal{H}$ , and $\\left\\{\\mu_{i}^{1/2}e_{i}\\right\\}_{i\\in I}$ is an ONB of $(\\ker I_{\\pi})^{\\perp}$ , we have that $\\pi$ \u2212almost everywhere ", "page_idx": 37}, {"type": "equation", "text": "$$\nk(x,\\cdot)=\\sum_{i\\in I}\\langle k(x,\\cdot),\\mu_{i}^{1/2}e_{i}\\rangle_{\\mathcal{H}}\\mu_{i}^{1/2}e_{i}=\\sum_{i\\in I}\\mu_{i}e_{i}(x)e_{i}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "This is Mercer\u2019s Theorem [51]. On the other hand, $\\pi$ \u2212almost everywhere, ", "page_idx": 37}, {"type": "equation", "text": "$$\nC_{X,\\lambda}^{-l/2}=\\sum_{i\\in I}(\\mu_{i}+\\lambda)^{-l/2}(\\sqrt{\\mu_{i}}e_{i})\\otimes(\\sqrt{\\mu_{i}}e_{i}).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Therefore, ", "page_idx": 37}, {"type": "equation", "text": "$$\n[C_{X,\\lambda}^{-l/2}k(x,\\cdot)]=\\sum_{i\\in I}\\frac{\\mu_{i}}{(\\mu_{i}+\\lambda)^{l/2}}e_{i}(x)[e_{i}],\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "and by Parseval\u2019s identity, using that $\\{\\mu_{i}^{(2-l)/2}[e_{i}]\\}_{i\\in I}$ is an ONB of $[\\mathcal{H}]^{2-l}$ , ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\|[C_{X,\\lambda}^{-l/2}k(x,\\cdot)]\\|_{2-l}^{2}=\\sum_{i\\in I}\\left(\\frac{\\mu_{i}}{\\mu_{i}+\\lambda}\\right)^{l}e_{i}(x)^{2}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Therefore, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\int_{\\mathcal{X}}\\|[C_{X,\\lambda}^{-l/2}k(x,\\cdot)]\\|_{2-l}^{2}\\mathrm{d}\\pi(x)=\\sum_{i\\in I}\\left(\\frac{\\mu_{i}}{\\mu_{i}+\\lambda}\\right)^{l}\\int_{\\mathcal{X}}e_{i}(x)^{2}\\mathrm{d}\\pi(x)=\\mathcal{N}_{l}(\\lambda),\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where we used that $([e_{i}])_{i\\in I}$ forms an orthonormal set in $L_{2}(\\pi)$ . ", "page_idx": 37}, {"type": "text", "text": "D.3 Concentration Inequalities ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "The following Theorem is from [17, Theorem 26]. ", "page_idx": 37}, {"type": "text", "text": "Theorem 10 (Bernstein\u2019s inequality). Let $(\\Omega,{\\cal{B}},P)$ be a probability space, $H$ be a separable Hilbert space, and $\\xi:\\Omega\\rightarrow H$ be a random variable with ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\|\\xi\\|_{H}^{m}]\\leq\\frac{1}{2}m!\\sigma^{2}L^{m-2}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "for all $m\\geq2$ . Then, for $\\tau\\geq1$ and $n\\geq1$ , the following concentration inequality is satisfied ", "page_idx": 37}, {"type": "equation", "text": "$$\nP^{n}\\left(\\left(\\omega_{1},\\ldots,\\omega_{n}\\right)\\in\\Omega^{n}:\\left\\Vert\\frac{1}{n}\\sum_{i=1}^{n}\\xi\\left(\\omega_{i}\\right)-\\mathbb{E}_{P}\\xi\\right\\Vert_{H}^{2}\\geq32\\frac{\\tau^{2}}{n}\\left(\\sigma^{2}+\\frac{L^{2}}{n}\\right)\\right)\\leq2e^{-\\tau}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "In particular, for $\\tau\\geq1$ and $n\\geq1$ , ", "page_idx": 37}, {"type": "equation", "text": "$$\nP^{n}\\left(\\left(\\omega_{1},\\ldots,\\omega_{n}\\right)\\in\\Omega^{n}:\\left\\Vert\\frac{1}{n}\\sum_{i=1}^{n}\\xi\\left(\\omega_{i}\\right)-\\mathbb{E}_{P}\\xi\\right\\Vert_{H}\\geq4\\sqrt{2}\\frac{\\tau}{\\sqrt{n}}\\left(\\sigma+\\frac{L}{\\sqrt{n}}\\right)\\right)\\leq2e^{-\\tau}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Lemma 18. Let $\\tau\\geq\\log(2)$ , with probability at least $1-2e^{-\\tau}$ , for $\\sqrt{n\\lambda}\\geq8\\tau\\kappa\\sqrt{\\operatorname*{max}\\{\\mathcal{N}(\\lambda),1\\}}$ , we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\left\\|\\hat{C}_{X,\\lambda}^{-1}C_{X,\\lambda}\\right\\|_{\\mathcal{H}\\to\\mathcal{H}}\\le2.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof. The proof is identical to [5, Proposition 5.4] with the only difference that in their setting $\\kappa=1$ . ", "page_idx": 37}, {"type": "text", "text": "Proposition 3 (Proposition C.9 29). Let \u03c0 be a probability measure on $\\mathcal{X},f\\in L_{2}(\\pi)$ and $\\|f\\|_{\\infty}\\leq M$ . Suppose we have $x_{1},\\ldots,x_{n}$ sampled i.i.d. from $\\pi$ . Then, for any $\\tau\\geq\\log(2)$ , the following holds with probability at least $1-2e^{-\\tau}$ : ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\|f\\|_{L_{2}(\\pi)}^{2}-\\frac{5\\tau M^{2}}{3n}\\leq\\|f\\|_{2,n}^{2}\\leq\\frac{3}{2}\\|f\\|_{L_{2}(\\pi)}^{2}+\\frac{5\\tau M^{2}}{3n},\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $\\Vert\\cdot\\Vert_{2,n}$ was defined in Definition 7. ", "page_idx": 38}, {"type": "text", "text": "Lemma 19 (Lemma 12 54). Let Assumptions (EMB), (SRC) and (MOM) be satisfied. For $\\tau\\geq1$ , $i f$ $\\lambda$ and $n$ satisfy that ", "page_idx": 38}, {"type": "equation", "text": "$$\nn\\geq8A^{2}\\tau\\lambda^{-\\alpha}\\log\\left(2e\\mathcal{N}(\\lambda)\\frac{\\|C_{X}\\|_{\\mathcal{H}\\to\\mathcal{H}}+\\lambda}{\\|C_{X}\\|_{\\mathcal{H}\\to\\mathcal{H}}}\\right)\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "then the following operator norm bounds are satisfied with probability not less than $1-2e^{-\\tau}$ ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|C_{X,\\lambda}^{-\\frac{1}{2}}\\hat{C}_{X,\\lambda}^{\\frac{1}{2}}\\right\\|_{\\mathcal{H}\\to\\mathcal{H}}^{2}\\le2,}\\\\ {\\left\\|C_{X,\\lambda}^{\\frac{1}{2}}\\hat{C}_{X,\\lambda}^{-\\frac{1}{2}}\\right\\|_{\\mathcal{H}\\to\\mathcal{H}}^{2}\\le3.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "D.4 Miscellaneous results ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Lemma 20 (Cordes inequality [18]). Let $A,B$ be two positive bounded linear operators on a separable Hilbert space $H$ and $s\\in[0,1]$ . Then ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\|A^{s}B^{s}\\|_{H\\rightarrow H}\\leq\\|A\\|_{H\\rightarrow H}^{s}\\|B\\|_{H\\rightarrow H}^{s}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Lemma 21 (Lemma 25 [17]). For $\\lambda>0$ and $s\\in[0,1],$ , we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t\\geq0}{\\frac{t^{s}}{t+\\lambda}}\\leq\\lambda^{s-1}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "We recall the following basic Lemma from [30, Lemma 2]. ", "page_idx": 38}, {"type": "text", "text": "Lemma 22. For $0\\leq\\gamma\\leq1$ and $F\\in{\\mathcal{G}}$ , the inequality ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\|[F]\\|_{\\gamma}\\leq\\left\\|C C_{X}^{\\frac{1-\\gamma}{2}}\\right\\|_{S_{2}(\\mathcal{H},\\mathcal{Y})}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "holds, where $C=\\bar{\\Psi}^{-1}(F)\\in S_{2}(\\mathcal{H},\\mathcal{Y})$ . If, in addition, $\\gamma<1$ or $C\\perp\\mathcal{D}\\otimes\\ker I_{\\pi}$ is satisfied, then the result is an equality. ", "page_idx": 38}, {"type": "text", "text": "Definition 11. Let $\\mathcal{X}\\subseteq\\mathbb{R}^{d}$ be a compact set and $\\theta\\in(0,1]$ . For a function $f:\\mathcal{X}\\to\\mathbb{R}$ , we introduce the H\u00f6lder semi-norm ", "page_idx": 38}, {"type": "equation", "text": "$$\n[f]_{\\theta,\\chi}:=\\operatorname*{sup}_{x,y\\in\\mathcal{X},x\\neq y}\\frac{|f(x)-f(y)|}{\\|x-y\\|^{\\theta}},\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $\\|\\cdot\\|$ represents the usual Euclidean norm. Then, we define the H\u00f6lder space ", "page_idx": 38}, {"type": "equation", "text": "$$\nC^{\\theta}(\\mathcal{X}):=\\{f:\\mathcal{X}\\to\\mathbb{R}\\mid[f]_{\\theta,\\mathcal{X}}<+\\infty\\}\\,,\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "which is equipped with the norm ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\|f\\|_{C^{\\theta}(\\mathcal{X})}:=\\operatorname*{sup}_{x\\in\\mathcal{X}}\\left|f(x)\\right|+[f]_{\\theta,\\mathcal{X}}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "The next lemma is used to prove Lemma 24 below. It appears in [29, Lemma A.3], albeit the use of an erroneous equality in their proof: $\\|k(x,\\cdot)-k(y,\\cdot)\\|_{\\mathcal{H}}^{\\frac{\\cdot}{2}^{.}}\\bar{=}k(x,\\bar{x})k(y,y)-k(x,\\bar{y})^{2}$ . We therefore provide our own proof of this result. ", "page_idx": 38}, {"type": "text", "text": "Lemma 23. Assume that $\\mathcal{H}$ is an RKHS over a compact set $\\mathcal{X}\\subseteq\\mathbb{R}^{d}$ associated with a kernel $k\\in C^{\\theta}(\\mathcal{X}\\times\\mathcal{X})$ for $\\theta\\in(0,1]$ . Then, we have ${\\mathcal{H}}\\subseteq C^{{\\frac{\\theta}{2}}}({\\mathcal{X}})$ and ", "page_idx": 38}, {"type": "equation", "text": "$$\n[f]_{\\frac{\\theta}{2},\\chi}\\leq\\sqrt{2[k]_{\\theta,\\chi\\times\\chi}}\\|f\\|_{\\mathcal H}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Proof. For all $(x,y)\\in\\mathcal{X}$ and $f\\in\\mathcal H$ , by the reproducing property and Cauchy\u2013Schwarz inequality, ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|f(x)-f(y)|=|\\langle k(x,\\cdot)-k(y,\\cdot),f\\rangle_{\\mathcal{H}}|\\leq\\|f\\|_{\\mathcal{H}}\\|k(x,\\cdot)-k(y,\\cdot)\\|_{\\mathcal{H}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Then, using $k\\in C^{\\theta}(\\mathcal{X}\\times\\mathcal{X})$ , we obtain ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|k(x,\\cdot)-k(y,\\cdot)\\|_{\\mathcal{H}}^{2}=k(x,x)+k(y,y)-2k(x,y)\\leq2[k]_{\\theta,\\mathcal{X}\\times\\mathcal{X}}\\|x-y\\|^{\\theta},}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "which concludes the proof. ", "page_idx": 39}, {"type": "text", "text": "We derive as a corollary a quantitative upper bound on the $\\epsilon_{}$ -covering number of the the set of (spectral) regularized kernel basis function with respect to the $\\|\\cdot\\|_{\\infty}$ norm. ", "page_idx": 39}, {"type": "text", "text": "Lemma 24 (Lemma C.10 by 29). Assume that $\\mathcal{H}$ is an RKHS over a compact set $\\mathcal{X}\\subseteq\\mathbb{R}^{d}$ associated with a kernel $k\\in C^{\\theta}(\\mathcal{X}\\times\\dot{\\mathcal{X}})$ for $\\theta\\in(0,1]$ . Assume that $k(x,x)\\leq\\kappa^{2}$ for all $x\\in\\mathscr{X}$ . Then, we have that for all $\\epsilon>0$ , ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathcal{N}(\\mathcal{K}_{\\lambda},\\Vert\\cdot\\Vert_{\\infty},\\epsilon)\\leq c(\\lambda\\epsilon)^{-\\frac{2d}{\\theta}}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathcal{K}_{\\lambda}:=\\left\\{C_{X,\\lambda}^{-1}k(x,\\cdot)\\right\\}_{x\\in\\mathcal{X}^{\\times}}}\\end{array}$ , and $c$ is a positive constant which does not depend on $\\lambda,\\epsilon$ and only depends on $\\kappa$ and $[k]_{\\theta,\\mathcal{X}\\times\\mathcal{X}}$ . $\\bar{\\mathcal{N}}(\\kappa_{\\lambda},\\|\\cdot\\|_{\\infty},\\epsilon)$ denotes the \u03f5\u2212covering number of the set $\\kappa_{\\lambda}$ in the norm $\\|\\cdot\\|_{\\infty}$ (see $I^{49}$ , Definition 6.19] for the definition of covering numbers). ", "page_idx": 39}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: We specifically point out the settings and detailed contributions of our work in both abstract and introduction. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 40}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Justification: In describing our assumptions as well as discussing our theoretical results, we specifically list our limitations. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 40}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: We provide our assumptions in Section 2.3. The complete proof of our works are listed in the Appendix. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 41}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: Our paper does not contain experiments. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 41}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: The paper does not include experiments requiring code. Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 42}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 42}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 42}, {"type": "text", "text": "", "page_idx": 43}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 43}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 43}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: Our paper mainly focuses on investigating the learning efficiency of spectral algorithms. We therefore believe our paper does not have any negative societal impacts. We explain how our theory can improve our understanding of various learning algorithms in the introduction which could be potential positive societal impacts. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 43}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 44}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 44}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 44}, {"type": "text", "text": "", "page_idx": 45}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 45}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 45}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 45}]