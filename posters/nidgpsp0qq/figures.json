[{"figure_path": "NidGPsP0Qq/figures/figures_8_1.jpg", "caption": "Figure 1: Running averaged reward of Algorithm 1 and Algorithm 2 on MNIST. Note that Algorithm 1 uniformly explores in the first 2N = 10000 rounds, and thus its averaged reward at t = 10000 is about 1/K = 0.1.", "description": "This figure shows the average reward over time for both Algorithm 1 (off-policy) and Algorithm 2 (on-policy) on the MNIST dataset.  The x-axis represents the number of iterations, and the y-axis represents the average reward. Algorithm 1 starts with a lower average reward because it uses uniform exploration for the first 10,000 rounds, resulting in an average reward of approximately 0.1 (1/number of classes). Algorithm 2 demonstrates a consistently higher average reward, indicating better performance.", "section": "4 Experiments"}, {"figure_path": "NidGPsP0Qq/figures/figures_9_1.jpg", "caption": "Figure 2: Performance of Algorithm 2 under true (unobserved) rewards and constructed rewards. Left figure: Results on MNIST dataset after the first N uniform exploration rounds. Right figure: Results on our conversational dataset.", "description": "This figure compares the performance of Algorithm 2 using the true (unobserved) rewards and the constructed rewards (a lower bound of the true reward) on two datasets: MNIST and a conversational dataset. The left panel shows the results on MNIST, illustrating the average reward over time.  The right panel presents the same comparison but for the conversational dataset.  Both panels demonstrate the effectiveness of using the constructed reward, which is crucial because the true reward is unavailable during the learning process.", "section": "4 Experiments"}]