[{"type": "text", "text": "Provably Efficient Interaction-Grounded Learning with Personalized Reward ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Mengxiao Zhang University of Iowa mengxiao-zhang@uiowa.edu ", "page_idx": 0}, {"type": "text", "text": "Yuheng Zhang University of Illinois Urbana-Champaign yuhengz2@illinois.edu ", "page_idx": 0}, {"type": "text", "text": "Haipeng Luo University of Southern California haipengl@usc.edu ", "page_idx": 0}, {"type": "text", "text": "Paul Mineiro Microsoft Research pmineiro@microsoft.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Interaction-Grounded Learning (IGL) [Xie et al., 2021] is a powerful framework in which a learner aims at maximizing unobservable rewards through interacting with an environment and observing reward-dependent feedback on the taken actions. To deal with personalized rewards that are ubiquitous in applications such as recommendation systems, Maghakian et al. [2022] study a version of IGL with context-dependent feedback, but their algorithm does not come with theoretical guarantees. In this work, we consider the same problem and provide the first provably efficient algorithms with sublinear regret under realizability. Our analysis reveals that the step-function estimator of prior work can deviate uncontrollably due to finite-sample effects. Our solution is a novel Lipschitz reward estimator which underestimates the true reward and enjoys favorable generalization performances. Building on this estimator, we propose two algorithms, one based on explore-thenexploit and the other based on inverse-gap weighting. We apply IGL to learning from image feedback and learning from text feedback, which are reward-free settings that arise in practice. Experimental results showcase the importance of using our Lipschitz reward estimator and the overall effectiveness of our algorithms. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Traditional bandit problems [Auer et al., 2002] or reinforcement learning problems [Sutton and Barto, 2018] assume that the learner has access to the reward, which is her learning goal. However, such explicit reward information is usually difficult to obtain in many real-world scenarios, including human-computer interface applications [Pantic and Rothkrantz, 2003, Freeman et al., 2017] and recommender systems [Yi et al., 2014, Wu et al., 2017]. Recently, Xie et al. [2021] study a new setting called Interaction-Grounded Learning (IGL), where the learner interacts with the environment and receives some feedback on her actions instead of explicit reward signals. The learner needs to discover the implicit information about the reward in the feedback and maximizes the reward. ", "page_idx": 0}, {"type": "text", "text": "From an information-theoretic perspective, IGL is intractable unless further assumptions are made. Xie et al. [2021] introduce a conditional independence assumption which states that the feedback is conditionally independent of the action and context given the latent reward. However, this assumption is unrealistic in many scenarios. For example, different users interact with recommender systems in different ways even under the same latent reward [Maghakian et al., 2022]. This inspires us to study IGL with personalized rewards, a setting where the feedback depends on the context. Although ", "page_idx": 0}, {"type": "text", "text": "Maghakian et al. [2022] study this for recommender systems, they only provide empirical results of their approach, leaving the following question open: ", "page_idx": 1}, {"type": "text", "text": "Can we design provably efficient algorithms for interaction-grounded learning when the feedback depends on the context given the latent reward? ", "page_idx": 1}, {"type": "text", "text": "Contributions. In this paper, we answer the question in the positive and provide the first provably efficient algorithms with sublinear regret guarantee for IGL with personalized reward. To achieve this, in Section 3.1, we first propose a novel reward estimator via inverse kinematics. Specifically, using the samples collected by applying a uniform policy, we construct a Lipschitz reward, which underestimates the reward for all the policies but approximates the reward for the optimal policy well. With this reward estimator, we propose two algorithms, one based on Explore-then-Exploit (Section 3.2) and the other based on inverse-gap weighting (Section 3.3). Both algorithms achieve ${\\mathcal{O}}(T^{{\\frac{2}{3}}})$ regret. To the best of our knowledge, we are the first to propose provable regret guarantees for IGL with personalized reward. In Section 4, we implement both algorithms and apply them to both an image classification dataset and a conversational dataset. The empirical performance showcases the effectiveness of our algorithm and the importance of using the Lipschitz reward estimator. ", "page_idx": 1}, {"type": "text", "text": "1.1 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Interaction-Grounded Learning (IGL). Xie et al. [2021] is the first work studying IGL and assumes that the feedback is independent of the context and action conditioned on the latent reward. Xie et al. [2022] further relaxes the assumption to an action-inclusive version, where the feedback is independent of context conditioned on both the action and the reward.2 Hu et al. [2024] follows the same setting as Xie et al. [2021] but proposes an information-theoretic approach to enforce the conditional independence assumption. However, as we mentioned before, context-dependent feedback is ubiquitous in real-word applications, so such conditional independence assumptions reduce the generality of the IGL framework. Maghakian et al. [2022] is the closest work to ours which also considers personalized rewards. Nonetheless, their work focuses on the empirical side and does not provide any theoretical guarantees. ", "page_idx": 1}, {"type": "text", "text": "Contextual online learning with partial feedback. Our work is closely related to the recent trend of designing efficient contextual learning algorithms with partial feedback, including contextual bandits [Langford and Zhang, 2007, Agarwal et al., 2012, 2014, Foster and Krishnamurthy, 2018, Foster et al., 2018, Foster and Rakhlin, 2020, Simchi-Levi and Xu, 2021], where the learner receives explicit reward signal of her taken action; contextual bandits with feedback graphs [Zhang et al., 2024a,b], where the learner\u2019s observation of the reward is determined based on a feedback graph; and contextual partial monitoring [Bart\u00f3k and Szepesv\u00e1ri, 2012, Kirschner et al., 2020], where the learner\u2019s observation is defined by a signal matrix or a linear observation operator. We adopt and generalize the ideas for designing contextual bandits algorithms [Foster and Rakhlin, 2020] to design algorithms for IGL. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminary ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Problem setup. Throughout the paper, we use $[N]$ to denote the set $\\{1,2,\\ldots,N\\}$ for a positive integer $N$ . The problem of online Interative-Grounded Learning (IGL) with personalized reward we consider is defined as follows. The interaction between the learner and the environment lasts for $T$ rounds. At each round $t\\in[T]$ , the environment reveals a stochastic context $x_{t}\\in\\mathscr{X}$ i.i.d. drawn from an unknown distribution $\\mathcal{D}$ , and the learner decides an action $a_{t}\\in[K]$ from a finite action set of size $K$ based on this context. Then, different from the classic contextual bandits in which the learner observes the binary reward of her chosen action $r(x_{t},a_{t})\\in\\{0,1\\}$ , she receives feedback $y_{t}\\in\\mathcal{V}$ . ", "page_idx": 1}, {"type": "text", "text": "Feedback dependence assumption. We follow the assumption proposed in [Maghakian et al., 2022] that the feedback is conditionally independent of the action given the context and the realized reward. ", "page_idx": 2}, {"type": "text", "text": "Assumption 1. For arbitrary $(x,a,r,y)$ tuple where the reward $r$ and the feedback y are generated based on the context x and action $a$ , we assume $y$ is conditionally independent of action a given context $x$ and reward $r$ . In other words, we assume that $y\\perp a\\mid r,x$ . ", "page_idx": 2}, {"type": "text", "text": "As mentioned, compared to prior work [Xie et al., 2021, 2022], this better captures many real-world applications such as recommender systems where different users interact with them in different ways [Beel et al., 2013, Shin, 2020, Maghakian et al., 2022]. ", "page_idx": 2}, {"type": "text", "text": "Realizability. We assume that the learner has access to two function classes $\\mathcal{F}\\subseteq(\\mathcal{X}\\times[K]\\mapsto[0,1])$ and $\\Phi\\subseteq(\\bar{x}\\times\\mathcal{Y}\\mapsto\\{0,1\\})$ , where $\\mathcal{F}$ characterizes the mean of the reward for a given contextaction pair, and $\\Phi$ characterizes the realized reward given the context and the received feedback. A policy $\\pi\\,:\\,\\mathcal{X}\\,\\rightarrow\\,\\Delta(K)$ specifies the action probability conditioned on the context. For each $f\\in\\mathcal F$ , we use $\\pi_{f}$ to denote the induced policy which takes action greedily according to $f$ , that is, $\\textstyle{\\bar{\\boldsymbol{\\pi}}}_{f}(a|x)=\\mathbb{1}\\{a\\stackrel{*}{=}\\operatorname{argmax}_{a^{\\prime}\\in[K]}f(x,a^{\\prime})\\},\\forall a^{\\prime}\\in[K]$ . We also use the shorthand $\\pi^{\\star}$ for the optimal policy $\\pi_{f^{\\star}}$ . We then make the following realizability assumption following previous contextual bandits literature [Agarwal et al., 2012, Foster et al., 2018, Foster and Rakhlin, 2020, Simchi-Levi and Xu, 2021]. ", "page_idx": 2}, {"type": "text", "text": "Assumption 2 (Realizability). There exists a regression function $f^{\\star}\\in{\\mathcal{F}}$ such that $\\mathbb{E}[r(x_{t},a)|x_{t}]=$ $f^{\\star}(x_{t},\\bar{a})$ for all $a\\in[K]$ and $t\\in[T]$ . Furthermore, there exists a feedback decoder $\\phi^{\\star}\\in\\Phi$ such that $\\phi^{\\star}(x_{t},y_{t})=r(x_{t},a_{t})$ for all $i\\in[T]$ . ", "page_idx": 2}, {"type": "text", "text": "For simplicity, we also assume that $\\mathcal{F}$ and $\\Phi$ are finite with cardinality $|{\\mathcal{F}}|$ and $|\\Phi|$ . Our results can be further generalized to broader function classes, which will be discussed in later sections. ", "page_idx": 2}, {"type": "text", "text": "Identifiability. As mentioned in [Xie et al., 2021, 2022, Maghakian et al., 2022], it is impossible to learn if we do not break the symmetry between reward being 1 and being 0. Following [Maghakian et al., 2022], we make the following assumption: ", "page_idx": 2}, {"type": "text", "text": "Assumption 3 (Identifiability). For any $x\\,\\in\\,{\\mathcal{X}}$ , $f^{\\star}$ defined in Assumption 2 satisfies that: 1) $\\textstyle\\sum_{a=1}^{K}f^{\\star}(x,a)\\leq\\alpha$ for some $\\begin{array}{r}{0<\\alpha<\\frac{K}{2}}\\end{array}$ ; 2) $\\operatorname*{max}_{a\\in[K]}f^{\\star}(x,a)\\geq\\theta$ where $\\theta>\\frac{\\alpha}{K-\\alpha}$ . ", "page_idx": 2}, {"type": "text", "text": "The first condition says that the sum of the expected reward over actions is less than $\\frac{K}{2}$ given any context $x$ . That is to say, the reward vector is sparse if $f^{\\star}(x,a)\\in\\{0,1\\}$ . The second condition says that for each context $x\\in\\mathscr{X}$ , there exists an action that achieves a large enough expected reward. These two conditions are indeed satisfied by many real-world applications, including the $s$ -multi-label classification problem (with $s<K/2$ ) where $\\alpha=s$ and $\\theta=1$ . ", "page_idx": 2}, {"type": "text", "text": "Regret. The learner\u2019s performance is measured via the notion of regret, which is defined as the expected difference between the learner\u2019s total reward and the one received by the optimal policy: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{Reg}_{\\mathtt{C B}}=\\mathbb{E}\\left[\\sum_{t=1}^{T}f^{\\star}(x_{t},\\pi^{\\star}(x_{t}))-\\sum_{t=1}^{T}f^{\\star}(x_{t},a_{t})\\right],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Other notations. We denote the $(K-1)$ -dimensional simplex by $\\Delta_{K}$ . Let 1 be the all-one vector in an appropriate dimension and $\\textstyle\\pi_{\\mathrm{Unif}}\\,=\\,\\frac{1}{K}\\cdot{\\bf1}\\,\\in\\,\\Delta_{K}$ . For a $d$ -dimensional vector $\\boldsymbol{v}\\in\\mathbb{R}^{d}$ , we denote its $i$ -th entry by $v_{i}.\\ \\mathbb{1}\\{\\cdot\\}$ is the indicator function and $e_{i}$ is the $i$ -th standard basis vector in an appropriate dimension. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we discuss our methodology. In Section 3.1, we start from introducing how we construct a Lipschitz reward estimator based on uniform samples, which serves as the key for our algorithm construction. We prove that this estimator is an underestimator of the reward, and more importantly matches the reward for the optimal policy. Based on this estimator, we design two algorithms for this problem in Section 3.2 and Section 3.3, with the first one based on explore-thenexploit and the second one based on inverse-gap weighting (IGW). ", "page_idx": 2}, {"type": "text", "text": "3.1 Reward Estimator Construction via Uniform Exploration ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We first show how we construct a reward estimator based on feedback collected from a uniform policy, which serves as the most important component in our two algorithms. ", "page_idx": 3}, {"type": "text", "text": "3.1.1 Inverse Kinematics ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "When the reward for each context-action pair is deterministic and binary, Maghakian et al. [2022] show that if the learner uniformly samples an action for any context and is able to accurately predict the posterior probability of her chosen action given the context and the feedback (inverse kinematics), then she is able to infer that the reward is 1 if that posterior probability is above certain threshold. Here, we generalize this thresholding reward estimator to the randomized binary reward case, and further prove that this estimator correctly models the reward for the optimal policy. To see this, we first prove the following lemma showing the exact posterior distribution over actions if the learner selects an action uniformly randomly. This is also proven in [Maghakian et al., 2022, Eq.(2)] as well, and we include it here for completeness. ", "page_idx": 3}, {"type": "text", "text": "Lemma 1. For any context $x\\in\\mathscr{X}$ , suppose that the learner picks a uniformly random action $a\\in[K]$ . Let $r$ and $y$ be its realized reward and the corresponding feedback. Then, under Assumption $^{\\,l}$ and Assumption 2, the posterior distribution of a given the context $x$ and feedback y equals to ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathsf{P r}[a|x,y]=\\frac{f^{\\star}(x,a)\\cdot\\phi^{\\star}(x,y)}{\\sum_{a^{\\prime}=1}^{K}f^{\\star}(x,a^{\\prime})}+\\frac{(1-f^{\\star}(x,a))(1-\\phi^{\\star}(x,y))}{K-\\sum_{a^{\\prime}=1}^{K}f^{\\star}(x,a^{\\prime})},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $f^{\\star}$ and $\\phi^{\\star}$ are the true expected reward and feedback decoder defined in Assumption 2. ", "page_idx": 3}, {"type": "text", "text": "Now we show how to infer the true reward from this inverse kinematics. Specifically, we show: ", "page_idx": 3}, {"type": "text", "text": "Lemma 2. For any context $x\\in\\mathscr{X}$ and action $a\\in[K]$ , let $r(x,a)$ be the realized reward and $y$ be the feedback. Let $h^{\\star}(x,y)\\in\\Delta_{K}$ be such that for each $a\\in[K]$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\nh_{a}^{\\star}(x,y)\\triangleq\\frac{f^{\\star}(x,a)\\cdot\\phi^{\\star}(x,y)}{\\sum_{a^{\\prime}=1}^{K}f^{\\star}(x,a^{\\prime})}+\\frac{(1-f^{\\star}(x,a))(1-\\phi^{\\star}(x,y))}{K-\\sum_{a^{\\prime}=1}^{K}f^{\\star}(x,a^{\\prime})}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Then we have ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\ r(x,a)=\\phi^{\\star}(x,y)\\geq\\mathbb{1}\\{h_{a}^{\\star}(x,y)\\geq\\frac{\\theta}{\\alpha}\\}\\,f o r\\,a l l\\,a\\in[K];}\\\\ &{\\bullet\\ r(x,a)=\\phi^{\\star}(x,y)=\\mathbb{1}\\{h_{a}^{\\star}(x,y)\\geq\\frac{\\theta}{\\alpha}\\}\\,f o r\\,a=\\pi^{\\star}(x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Proof. Note that since $\\phi^{\\star}(x,y)\\in\\{0,1\\}$ , only one term can be non-zero in Eq. (2). If $\\begin{array}{r}{h_{a}^{\\star}(x,y)\\geq\\frac{\\theta}{\\alpha}}\\end{array}$ , where $\\theta$ and $\\alpha$ are defined in Assumption 3, then the reward $\\phi^{\\star}(x,y)$ has to be 1 since otherwise, we have $\\phi^{\\star}(x,y)=0$ and ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{\\theta}{\\alpha}\\leq h_{a}^{\\star}(x,y)=\\frac{1-f^{\\star}(x,a)}{K-\\sum_{a^{\\prime}=1}^{K}f^{\\star}(x,a^{\\prime})}\\leq\\frac{1}{K-\\alpha}<\\frac{\\theta}{\\alpha},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the second and the third inequality are due to Assumption 3. This leads to a contradiction. Therefore, we know that the realized reward is 1 if $h_{a}^{\\star}(x,y)$ is no less than $\\frac{\\theta}{\\alpha}$ . Therefore, $\\mathbb{1}\\{h_{a}^{\\star}(x,y)\\,\\geq\\,{\\frac{\\theta}{\\alpha}}\\}$ can be viewed as an underestimator of $r(x,a)$ . To prove the second property, consider the case in which $a=\\pi^{\\star}(x)$ . Then, we know that when $\\phi^{\\star}(x,y)=1$ , we must also have $\\begin{array}{r}{h_{a}^{\\star}(x,y)\\ge\\frac{\\theta}{\\alpha}}\\end{array}$ since ", "page_idx": 3}, {"type": "equation", "text": "$$\nh_{a}^{\\star}(x,y)=\\frac{f^{\\star}(x,\\pi^{\\star}(x))}{\\sum_{i=1}^{K}f^{\\star}(x,a)}\\geq\\frac{f^{\\star}(x,\\pi^{\\star}(x))}{\\alpha}\\geq\\frac{\\theta}{\\alpha},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the first inequality uses the first property in Assumption 3 and the second inequality uses the second property in Assumption 3. \u53e3 ", "page_idx": 3}, {"type": "text", "text": "Lemma 2 shows that the function $h^{\\star}(x,y)$ defined in Eq. (2) satisfies two important properties. First, $\\mathbb{1}\\{h_{a}^{\\star}(x,y)\\,\\geq\\,{\\frac{\\theta}{\\alpha}}\\}$ serves as a reward underestimator for all the policies. Second, it matches the reward of the optimal policy. Note that this is different from [Maghakian et al., 2022, Eq.(3)], since ", "page_idx": 3}, {"type": "text", "text": "they only consider the deterministic binary reward for each context-action pair, and they do not show that the constructed reward estimator matches the reward of the optimal policy. ", "page_idx": 4}, {"type": "text", "text": "Based on these two properties, we know that if we have access to $h^{\\star}$ , then the policy $\\pi$ that maximizes the surrogate reward $\\begin{array}{r}{\\overbar{\\mathbb{E}}\\left[\\mathbb{1}\\{h_{\\pi(x)}^{\\star}(x,y)\\}\\geq\\frac{\\theta}{\\alpha}\\right]}\\end{array}$ also maximizes the true expected reward. ", "page_idx": 4}, {"type": "text", "text": "3.1.2 Learning the Posterior Distribution via ERM ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Next, we show how to learn this $h^{\\star}$ via uniformly collected samples. Define the function class $\\mathcal{H}$ as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{H}=\\Bigg\\{h:\\mathcal{X}\\times\\mathcal{Y}\\mapsto\\Delta_{K},h_{a}(x,y)=\\frac{f(x,a)\\phi(x,y)}{\\sum_{i=1}^{K}f(x,i)}+\\frac{(1-f(x,a))(1-\\phi(x,y))}{K-\\sum_{i=1}^{K}f(x,i)}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Note that $|{\\mathcal{H}}|=|{\\mathcal{F}}|\\cdot|\\Phi|$ . Since $h^{\\star}$ models the posterior distribution over actions when the learner selects her action uniformly randomly, we collect $N$ tuples of $(x_{t},a_{t},y_{t})$ by sampling $a_{t}$ uniformly from $[K]$ and find the empirical risk minimizer (ERM) $\\widehat{h}\\in\\mathcal{H}$ over these samples using squared loss. In the following lemma, we show that $\\widehat{h}$ enjoys $\\mathcal{O}\\left(\\frac{\\log|\\mathcal{H}|}{N}\\right)$ excess risk with high probability. ", "page_idx": 4}, {"type": "text", "text": "Lemma 3. Let $\\{(x_{i},a_{i},y_{i})\\}_{i=1}^{N}$ be $N$ i.i.d. samples where $x_{i}~\\in~{\\mathcal{D}}$ , $a~\\in~\\pi_{\\mathrm{Unif}},$ , and $y_{i}$ is the corresponding feedback. Leth be the ERM with respect to the squared loss defined as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widehat{h}=\\underset{h\\in\\mathcal{H}}{\\operatorname{argmin}}\\left\\{\\sum_{i=1}^{N}\\|h(x_{i},y_{i})-e_{a_{i}}\\|_{2}^{2}\\right\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Then, with probability at least $1-\\delta$ , we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbb{E}_{x\\sim\\mathcal{D},a\\sim\\pi_{\\mathrm{Unif}},y\\mid x,a}\\left[\\|\\widehat{h}(x,y)-e_{a}\\|_{2}^{2}-\\|h^{\\star}(x,y)-e_{a}\\|_{2}^{2}\\right]\\leq\\mathcal{O}\\left(\\frac{\\log\\frac{|\\mathcal{H}|}{\\delta}}{N}\\right),}\\\\ &{}&{\\mathbb{E}_{x\\sim\\mathcal{D},a^{\\prime}\\sim\\pi_{\\mathrm{Unif}},y\\mid x,a^{\\prime}}\\left[\\|\\widehat{h}(x,y)-h^{\\star}(x,y)\\|_{2}\\right]\\leq\\mathcal{O}\\left(\\sqrt{\\frac{\\log\\frac{|\\mathcal{H}|}{\\delta}}{N}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The full proof is deferred to Appendix A. Different from the classic one-dimensional squared loss, here we consider the $\\ell_{2}$ -loss between two vectors in $\\Delta_{K}$ . Directly applying the generalization bound for each entry leads to a $K$ -factor worse bound. Instead, our proof is based on the observation that the loss function $\\lVert h(x,y)-e_{a}\\rVert_{2}^{2}$ , when seen as a function of $h$ , satisfies the so-called strong 1-central condition [Van Erven et al., 2015, Definition 7]. Moreover, these results can be extended to function classes with infinite size and bounded covering number based on Theorem 7.7 of [Van Erven et al., 2015]. ", "page_idx": 4}, {"type": "text", "text": "3.1.3 Constructing Lipschitz Reward Estimators Based onh ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Now we show how to construct a reward estimator based on the ERM $\\widehat{h}$ . According to Section 3.1.1, an intuitive form of the reward (under)estimator is $\\begin{array}{r}{\\mathbb{1}\\{\\widehat{h}_{a}(x,y)\\geq\\frac{\\theta}{\\alpha}\\}}\\end{array}$ . However, since the indicator function is not Lipschitz, $\\begin{array}{r}{\\mathbb{1}\\{\\widehat{h}_{a}(x,y)\\geq\\frac{\\theta}{\\alpha}\\}}\\end{array}$ can be very different from $\\begin{array}{r}{\\mathbb{1}\\{h_{a}^{\\star}(x,y)\\geq\\frac{\\theta}{\\alpha}\\}}\\end{array}$ even with the generalization bound proven in Lemma 3. To resolve this issue, we propose a Lipschitz variant of the indicator function (defined in Eq. (6)) and show that it also satisfies the two properties shown in Lemma 2. ", "page_idx": 4}, {"type": "text", "text": "Lemma 4. Define $G(v,\\beta,\\sigma)$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\nG(v,\\beta,\\sigma)=\\frac{1}{\\sigma}(v-\\beta)\\mathbb{1}\\{\\beta\\le v<\\beta+\\sigma\\}+\\mathbb{1}\\{v\\ge\\beta+\\sigma\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Then, for any context $x\\in\\mathscr{X}$ , action $a\\in[K]$ , and feedback $y\\in\\mathcal{V}$ generated via context $x$ and the realized reward $r(x,a)$ , we have the following two properties with $\\begin{array}{r}{\\sigma\\triangleq\\frac{1}{2}\\left(\\frac{\\theta}{\\alpha}-\\frac{1}{K-\\alpha}\\right)>0.}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "Input: number of exploration samples $N$ , parameters $\\alpha,\\theta$ and $\\begin{array}{r}{\\sigma=\\frac{1}{2}\\left(\\frac{\\theta}{\\alpha}-\\frac{1}{K-\\alpha}\\right)}\\end{array}$ .   \nfor $t=1,2,\\cdots,2N$ do   \nReceive context $x_{t}$ , sample $a_{t}\\sim\\pi_{\\mathrm{Unif}}$ , and observe signal $y_{t}$ .   \nCalculate the empirical risk minimizer $\\widehat{h}$ as in Eq. (3).   \nConstruct the reward decoder $\\widehat{r}_{\\sigma}(x,y,a)=G(\\widehat{h_{a}}(x,y),\\frac{\\theta}{\\alpha}-\\sigma,\\sigma)$ where $G$ is defined in Eq. (6). Calculate $\\begin{array}{r}{\\widehat{\\pi}=\\operatorname*{argmax}_{\\pi\\in\\Pi}\\left\\{\\sum_{i=N+1}^{2N}\\pi(a_{i}|x_{i})\\cdot\\widehat{r}_{\\sigma}(x_{i},y_{i},a_{i})\\right\\}}\\end{array}$ where $\\Pi=\\{\\pi_{f}:f\\in\\mathcal{F}\\}$ . for $t=2N+1,\\dots,T$ do   \nExecute policy $\\widehat{\\pi}$ . ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\ r(x,a)=\\phi^{\\star}(x,y)\\geq G(h_{a}^{\\star}(x,y),\\frac{\\theta}{\\alpha}-\\sigma,\\sigma),}\\\\ &{\\bullet\\ r(x,a)=\\phi^{\\star}(x,y)=G(h_{a}^{\\star}(x,y),\\frac{\\theta}{\\alpha}-\\sigma,\\sigma)\\ i f a=\\pi^{\\star}(x),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The proof shares a similar spirit to Lemma 2 and is deferred to Appendix A. Notably, the function $G(v,\\bar{\\beta},\\sigma)$ is $\\frac{1}{\\sigma}$ -Lipschitz in $v$ , which is important in order to show concentration between the reward estimator with respect to the true posterior distribution $\\begin{array}{r}{G(h_{a}^{\\star}(x,y),\\frac{\\theta}{\\alpha}-\\sigma,\\sigma)}\\end{array}$ and that constructed via the ERM function $\\widehat{h}$ : $\\begin{array}{r}{G(\\widehat{h}_{a}(x,y),\\frac{\\theta}{\\alpha}-\\sigma,\\sigma)}\\end{array}$ . In the following, we will show how to use the reward estimator $\\begin{array}{r}{G(\\widehat{h}_{a}(x,y),\\frac{\\theta}{\\alpha}-\\sigma,\\sigma)}\\end{array}$ to design algorithms with provable guarantees. ", "page_idx": 5}, {"type": "text", "text": "3.2 Off-Policy Algorithm ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Built on the reward estimator in Section 3.1, we present our off-policy algorithm, summarized in Algorithm 1. Our algorithm follows the explore-then-exploit idea and consists of two phases. In the exploration phase, we perform uniform exploration and collect the dataset $\\{x_{i},a_{i},y_{i}\\}_{i=1}^{2N}$ . The first samples are used to learn the reward estimator and the rest samples are used to learn the policy $\\widehat{\\pi}$ with $\\widehat{r}.$ . For the exploitation phase, we employ  t he learned policy $\\widehat{\\pi}$ in the remaining $T-2N$ iterations. We present the regret bound of Algorithm 1 in the following theorem. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1. Under Assumptions 1-3, Algorithm 1 with $N=T^{2/3}K^{2/3}\\sigma^{-2/3}\\log^{1/3}(|\\mathcal{H}|T)$ guarantees that $\\mathbf{Reg}_{\\mathsf{C B}}\\leq\\mathcal{O}\\left(T^{2/3}K^{2/3}\\sigma^{-2/3}\\log^{1/3}(|\\mathcal{H}|T)\\right)$ . ", "page_idx": 5}, {"type": "text", "text": "We remark that this is the first provably efficient algorithm for IGL with personalized reward. The key of the proof is to show that the learned policy $\\widehat{\\pi}$ is near-optimal under the true reward. This is achieved by using the properties of function $G$ in L  emma 4 and proving that the reward decoder $\\widehat{r}_{\\sigma}$ is close to the ground-truth. The full proof is deferred to Appendix B. Besides the dependence  o n $T,K$ and $\\log\\left|\\mathcal{H}\\right|$ , our regret bound also depends on $\\sigma^{-1}$ , which measures how sparse the reward vector is and characterizes the difficulty of the problem. For example, $\\sigma^{-1}=\\mathcal{O}(1)$ in the multi-class classification problem and $\\sigma^{-1}\\leq\\mathcal{O}(\\overset{\\cdot}{s}^{2})$ in the $s$ -multi-label classification problem. ", "page_idx": 5}, {"type": "text", "text": "3.3 On-Policy Algorithm ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Since on-policy algorithms are more favorable in practice, we further introduce an on-policy algorithm based on the inverse-gap weighting strategy. Following [Foster and Rakhlin, 2020], we assume access to an online regression oracle $\\mathsf{A l g S q}$ : at each round $t\\in[T]$ , the oracle $\\mathsf{A l g S q}$ produces an estimator $\\widehat{f}_{t}$ in the convex hull of $\\mathcal{F}$ , then receives a context-action-reward tuple $(x_{t},a_{t},r_{t})$ . The squared loss of the oracle for this round is defined as $(\\widehat{f_{t}}(x_{t},a_{t})-r_{t})^{2}$ , which is on average assumed to be close to that of the best predictor in $\\mathcal{F}$ . ", "page_idx": 5}, {"type": "text", "text": "Assumption 4 (Bounded squared loss regret). For any sequence $\\{(x_{t},a_{t},r_{t})\\}_{t=1}^{T}$ , the regression oracle $\\mathsf{A l g S q}$ guarantees the following regret bound for some $\\mathbf{Reg}_{\\mathsf{S q}}$ that depends on $T,\\,K$ , and $\\mathcal{F}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\left(\\widehat{f}_{t}(x_{t},a_{t})-r_{t}\\right)^{2}-\\operatorname*{inf}_{f\\in\\mathcal{F}}\\sum_{t=1}^{T}\\left(f(x_{t},a_{t})-r_{t}\\right)^{2}\\leq\\mathbf{Reg}_{\\mathsf{S q}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Input: online regression oracle AlgSq, number of exploration samples $N$ , parameters $\\alpha,\\theta,\\gamma$ and $\\begin{array}{r}{\\sigma=\\frac12\\left(\\frac{\\theta}{\\alpha}-\\frac{1}{K-\\alpha}\\right)}\\end{array}$ . ", "page_idx": 6}, {"type": "text", "text": "for $t=1,2,\\cdot\\cdot\\cdot,N$ do ", "page_idx": 6}, {"type": "text", "text": "Receive context $x_{t}$ , sample $a_{t}\\sim\\pi_{\\mathrm{Unif}}$ , and observe signal $y_{t}$ .   \nCalculate the empirical risk minimizer $\\widehat{h}$ as in Eq. (3).   \nfor $t=N+1,\\ldots,T$ do Receive context $x_{t}$ . Obtain an estimator $\\widehat{f}_{t}$ from the oracle $\\mathsf{A l g S q}$ . Calculate the action  distribution $p_{t}$ as $p_{t,a}=\\left\\{\\begin{array}{l l}{\\frac{1}{K+\\gamma(\\widehat f_{t}(x_{t},\\widehat a)-\\widehat f_{t}(x_{t},a))},}&{a\\neq\\widehat a,}\\\\ {1-\\sum_{a^{\\prime}\\neq\\widehat a}p_{a^{\\prime}},}&{a=\\widehat a}\\end{array},\\right.$ where $\\widehat{a}=\\mathop{\\mathrm{argmax}}_{a\\in[K]}\\widehat{f}_{t}(x_{t},a)$ . Sample $a_{t}$ from $p_{t}$ and re ceive feedback $y_{t}$ . Feed $(x_{t},a_{t},G(\\widehat{h}_{a_{t}}(x_{t},y_{t}),\\frac{\\theta}{\\alpha}-\\sigma,\\sigma))$ to the oracle $\\mathsf{A l g S q}$ where $G$ is defined in Eq. (6). ", "page_idx": 6}, {"type": "text", "text": "Based on the ground-truth inverse kinematics function $h^{\\star}$ , we define a function $\\underline{{f}}^{\\star}(x,a)\\;:=\\;$ $\\begin{array}{r}{\\underline{{\\mathbb{E}}}_{y\\mid x,a}\\left[G(h_{a}^{\\star}(x,y),\\frac{\\theta}{\\alpha}-\\sigma,\\sigma)\\right]}\\end{array}$ , which is always a lower bound on $f^{\\star}(x,a)$ according to Lemma 4. Since we only feed the surrogate reward to the oracle, we make another mild assumption that our regression function class $\\mathcal{F}$ also realizes $\\underline{{f}}^{\\star}$ . ", "page_idx": 6}, {"type": "text", "text": "Assumption 5 (Lower Bound Realizability). We also assume that $\\underline{{f}}^{\\star}\\in\\mathcal{F}$ , where $\\underline{{f}}^{\\star}$ is defined as $\\underline{{f}}^{\\star}(x,a)=\\mathbb{E}_{y|x,a}\\left[G(h_{a}^{\\star}(x,y),\\frac{\\theta}{\\alpha}-\\sigma,\\sigma)\\right]$ ", "page_idx": 6}, {"type": "text", "text": "We now summarize our algorithm in Algorithm 2. After obtaining the inverse kinematics predictor $\\widehat{h}$ in the same manner as Algorithm 1, instead of uniform exploring, we use the estimated reward fro m the oracle and an inverse-gap weighting strategy [Abe and Long, 1999, Foster and Rakhlin, 2020] defined in Eq. (7). Different from the contextual bandit problem where the true reward is given and fed to the oracle, we feed the predicted reward $\\begin{array}{r}{G(\\widehat{h}_{a_{t}}(x_{t},y_{t}),\\frac{\\theta}{\\alpha}-\\sigma,\\sigma)}\\end{array}$ to the oracle. ", "page_idx": 6}, {"type": "text", "text": "One might wonder how such misspecification in rewards would affect the regret bound. Our key observation is that since we use the uniform policy to collect data used to trainh, the generalization error of $\\widehat{h}$ is small for any $a$ due to good coverage of the dataset on each action. Based on this observation, we prove the following theorem for Algorithm 2. ", "page_idx": 6}, {"type": "text", "text": "Theorem 2. Under Assumptions 1-5, Algorithm 2 with certain choice of $N$ and $\\gamma$ guarantees that $\\mathbf{Reg}_{\\mathrm{CB}}={\\mathcal{O}}\\left({\\sqrt{K T\\mathbf{Reg}_{5{\\mathrm{q}}}}}+\\sigma^{-2/3}(K{\\dot{T}})^{2/3}\\log^{1/3}(|{\\mathcal{H}}|T)\\right)$ ", "page_idx": 6}, {"type": "text", "text": "The proof mainly relies on the generalization bounds in Lemma 3 and the property of $G$ in Lemma 4, and is deferred to Appendix C. We observe that Algorithm 2 enjoys the same dependence on $T,K$ , $\\sigma^{-1}$ as Algorithm 1. For finite $\\mathcal{F}$ , we can use Vovk\u2019s aggregation algorithm [Vovk, 1995] as the regression oracle and achieve $\\mathbf{Reg}_{\\mathsf{S q}}=\\mathcal{O}\\left(\\log\\vert\\mathcal{F}\\vert\\right)$ , making the second term in the regret bound negligible.3 While in theory our on-policy algorithm does not seem to be more favorable than the off-policy algorithm (mostly because they both need to uniformly explore for a certain period in order to build $\\widehat{h}$ ), it can still perform better in practice, as shown in our experiments. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we apply IGL to learning from image feedback and learning from text feedback. Specifically, we conduct experiments on the MNIST dataset and a conversational dataset to verify the effectiveness of our algorithms and the Lipschitz reward estimator constructed by Eq. (6). ", "page_idx": 6}, {"type": "table", "img_path": "NidGPsP0Qq/tmp/daf9a92191adc3c7b3aaa3a08c8109d7f43f348204ecf8772b846ee67a8e5ff1.jpg", "table_caption": ["Table 1: Performance of Algorithm 1 and Algorithm 2 on the MNIST dataset. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.1 Experiments on Learning from Image Feedback ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Experiment Setup We first conduct experiments on MNIST dataset and implement both the offpolicy algorithm Algorithm 1 and the on-policy algorithm Algorithm 2. The setup is as follows: at each step $t$ , the learner receives an image $x_{t}$ with ground-truth label $l_{t}$ and picks action $a_{t}$ from $\\{0,\\cdot\\cdot\\cdot,9\\}$ as the predicted label. If the prediction $a_{t}$ is correct, the learner receives as feedback an image $y_{t}$ with digit $(l_{t}+1)$ mod 10; otherwise, the learner receives an image with digit $(l_{t}-1)$ mod 10. Therefore, different from the experimental setup in [Xie et al., 2021], our feedback $y_{t}$ does depend on both the context and the reward. Both function classes $\\mathcal{H}$ and $\\mathcal{F}$ are implemented as two-layer convolutional neural networks. ", "page_idx": 7}, {"type": "text", "text": "We use PyTorch framework [Paszke et al., 2019] and parameter-free optimizers [Orabona and Tommasi, 2017] to learn the reward estimator and the policy. For both algorithms, we set the number of exploration samples $N=5000$ and pick the parameter $\\sigma\\in\\{0,0.05,0.1,0.15,0.2,0.25,0.3\\}$ and $\\textstyle{\\frac{\\theta}{\\alpha}}\\,\\in\\,\\{{\\frac{1}{3}}\\,+\\,{\\frac{\\sigma}{2}},\\,{\\frac{1}{2}}\\,+\\,{\\frac{\\sigma}{2}}\\}$ . For the on-policy algorithm, we use a time-varying exploration parameter $\\gamma_{t}\\,=\\,\\sqrt{K t}$ as suggested by Foster and Krishnamurthy [2021]. We run the experiments on one NVIDIA GeForce RTX 2080 Ti. ", "page_idx": 7}, {"type": "text", "text": "The interaction lasts for $T\\,=\\,60000$ rounds. We use two metrics to evaluate the performance of the algorithm, including the average progressive reward during the interaction process and the test accuracy on a held-out test set containing 10000 samples. When evaluating the on-policy algorithm on the test set, we take actions greedily according to $\\widehat{f}_{T}$ . Since we use the uniform policy to collect data for learning the reward estimator, the progressiv e reward at that phase is counted as $\\ln{\\tt1}$ . ", "page_idx": 7}, {"type": "text", "text": "Results We run the experiments with 4 different random seeds and report the mean value and standard deviation in Table 1. The running averaged reward over the entire $T$ rounds are shown in Figure 1. We can see that both algorithms achieve good performance, despite never observing any true labels. While the theoretical regret guarantees for both algorithms are of the same order, empirically, the on-policy Algorithm 2 performs better than Algorithm 1 with over $90\\%$ test accuracy. This is because the on-policy algorithm uses the inverse-gap weighting strategy to achieve a better trade-off between exploration and exploitation, while the off-policy algorithm learns the policy from uniform exploration. On the other hand, to demonstrate the effectiveness of the Lipschitz reward estimator, we compare the performances of the Lipschitz estimator Eq. (6) with a binary reward sepstaicme.a tTorh $\\begin{array}{r}{\\widehat{r}_{\\mathrm{binary}}(x,y,a)\\,=\\,\\mathbb{1}\\{\\widehat{h}_{a}(x,y)\\,\\geq\\,\\frac{\\theta}{\\alpha}\\}}\\end{array}$ ,s cwhihtze rree twhaer dp aerstaimmeatteorr $\\frac{\\theta}{\\alpha}$ ipsr osevaers cohveedr  tohvee rb itnhaer ys aomnee by a clear margin for both algorithms. This matches our theoretical analysis that highlights the vital role of the Lipschitzness of the reward estimator in obtaining good regret guarantees. We also plot the performance of Algorithm 2 under both true reward and constructed reward in the left figure of Figure 2.4 The figure shows that the constructed reward is indeed a lower bound of the true reward, and the policy can learn from the constructed reward effectively. ", "page_idx": 7}, {"type": "text", "text": "4.2 Experiments on Learning From Text Feedback ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We further consider an application of learning with text feedback. The IGL framework ftis this problem well since this is a natural reward-free setting involving learning from user\u2019s text feedback, which is idiosyncratically related to the quality of the actions taken and the question asked. Specifically, given the input of a question, the learner needs to select one of the $K$ possible answers. Instead of receiving whether the selected answer is correct, the learner only receives user\u2019s text feedback to the chosen answer. The goal of the learner is to choose the best answer only based on such text feedback. ", "page_idx": 7}, {"type": "image", "img_path": "NidGPsP0Qq/tmp/302432063108c724579bf46e634e1fac9ead986a4785ab5114a5fbaa6daa9e2f.jpg", "img_caption": ["Figure 1: Running averaged reward of Algorithm 1 and Algorithm 2 on MNIST. Note that Algorithm 1 uniformly explores in the first $2N=10000$ rounds, and thus its averaged reward at $t=10000$ is about $1/K=\\mathrm{\\bar{0}.1}$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Dataset Construction Our dataset is constructed as follows. Specifically, we construct our question set $S=\\{q_{i}\\}_{i\\in[20000]}$ from Chatbot Arena datasets [Zheng et al., 2024]. Then, for each question $q_{i}\\in S$ we ask a larger language model $\\mathcal{G}$ with a high ELO score on the chatsys leaderboard [Chiang et al., 2024] to generate a \u201cgood\u201d answer $g_{i,0}$ with $r_{i,0}=1$ ; and ask a (much) smaller language model $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ with a (much) lower ELO score to generate 4 \u201cbad\u201d answers $b_{i,j}$ with reward $r_{i,j}\\,=\\,0$ , $j\\in\\{1,2,3,4\\}$ . Specifically, we pick $\\mathcal{G}$ to be \u201cQwen1.5-32B-Chat\u201d with ELO score 1134 and $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ to be \u201cQwen1.5-0.5B-Chat\u201d with ELO score5 less than 804 [Bai et al., 2023].6 For each questionanswer tuple $\\left(q_{i},g_{i,0},b_{i,1},b_{i,2},b_{i,3},b_{i,4}\\right)$ , we ask another large language model $\\mathcal{R}$ to simulate a user response $f_{i,j},\\,j\\,\\in\\,\\{0,1,2,3,4\\}$ to the good (bad) answers under the instruction that the user is satisfied (unsatisfied) with the answer. We pick $\\mathcal{R}$ to be Qwen1.5-32B-Chat as well. This forms our final dataset $S_{\\mathrm{Conv}}=\\{(q_{i},(g_{i,0},f_{i,0},r_{i,0})$ , $\\{(b_{i,j},f_{i,j},r_{i,j})\\}_{j\\in[4]})\\}_{i\\in[20000]}$ . Again, the true reward is never revealed to the learner, and we only use this reward to measure the performance of our algorithm. The prompt we use is deferred to Appendix D.1. We generate our dataset using one A100 GPU for two weeks. ", "page_idx": 8}, {"type": "text", "text": "4.2.1 Algorithm Configurations and Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Given the superior performance of Algorithm 2 over Algorithm 1 on the MNIST dataset shown in Section 4.1, we only test Algorithm 2 on the conversational dataset. We use the first $N=10000$ data points to learn h and the remaining $\\vert S\\vert-N=10000$ data points to learn the optimal policy based on the reward function constructed via $\\widehat{h}$ . We use the same parameter-free optimizer as the one in Section 4.1. Next, we introduce the construction ofh and the regression oracle: ", "page_idx": 8}, {"type": "text", "text": "Inverse kinematic model. The model class $\\mathcal{H}$ we consider is the pretrained language model Llama3-8B-Instruct [AI $@$ Meta, 2024] with an additional multi-class classification head.7 The language model is prompted with a question-answer-feedback tuple $(x,a,y)$ ; see Appendix D.2 for the prompt. To learn the inverse kinematic model, we use parameter efficient fine-tuning [Mangrulkar et al., 2024] with a rank-1 LORA adapter [Hu et al., 2021] and binary cross entropy loss, which is with respect to the indicator of whether-or-not the predicted action corresponds to the action selected in the tuple. ", "page_idx": 8}, {"type": "image", "img_path": "NidGPsP0Qq/tmp/0ae614f9957a80e43e39db9173b5ad62c0c8a59626bab8eb29f9a8a0c72e3384.jpg", "img_caption": ["Figure 2: Performance of Algorithm 2 under true (unobserved) rewards and constructed rewards. Left figure: Results on MNIST dataset after the first $N$ uniform exploration rounds. Right figure: Results on our conversational dataset. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "We successfully learn $\\widehat{h}$ using one A100 GPU within 6 hours. After obtaining $\\widehat{h}$ , we construct the reward estimator $\\begin{array}{r}{G(\\widehat{h}_{a}(x,y),\\frac{\\theta}{\\alpha}-\\sigma,\\sigma)}\\end{array}$ \u2212\u03c3, \u03c3) with \u03c3 = 0.1, \u03b1 = K2 $\\alpha=\\textstyle{\\frac{K}{2}}={\\frac{5}{2}}$ , and $\\theta=1$ . ", "page_idx": 9}, {"type": "text", "text": "Regression oracle. Similar to the inverse kinematic model, the reward prediction function class $\\mathcal{F}$ is again based on the pretrained Llama-3-8B-Instruct model but with an additional regression head. Specifically, the language model is prompted only with a question-answer pair $(x,a)$ using the prompt deferred to Appendix D.3 and predicts a score in $[0,1]$ for this question-answer pair. The regression oracle again applies parameter efficient fine-tuning with a different rank-1 LORA adapter on the regression loss, which measures the error of predicting the output of the reward predictor $G(\\widehat{h}_{a}(x,\\bar{y}),\\frac{\\theta}{\\alpha}-\\sigma,\\sigma)$ . This process is done on one A100 GPU within 3 hours. ", "page_idx": 9}, {"type": "text", "text": "Results. The empirical results on the conversational dataset are shown in Figure 2. We show the running averaged true reward and the running average constructed reward received by Algorithm 2 after the first $N=10000$ rounds of learning $\\widehat{h}$ . The $x$ -axis is the time horizon and $y$ -axis is the value of average reward. Similar to our experiment  results in Section 4.1, the right figure in Figure 2 shows that our constructed reward estimator is a lower bound on the true reward, matching our theoretical results in Lemma 4, and Algorithm 2 is able to learn the reward effectively through the text feedback with the constructed reward estimator. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "HL and MZ were supported by NSF Award IIS-1943607. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Naoki Abe and Philip M Long. Associative reinforcement learning using linear probabilistic concepts. In ICML, pages 3\u201311. Citeseer, 1999. ", "page_idx": 9}, {"type": "text", "text": "Alekh Agarwal, Miroslav Dud\u00edk, Satyen Kale, John Langford, and Robert Schapire. Contextual bandit learning with predictable rewards. In Artificial Intelligence and Statistics, pages 19\u201326. PMLR, 2012.   \nAlekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. Taming the monster: A fast and simple algorithm for contextual bandits. In International Conference on Machine Learning, pages 1638\u20131646. PMLR, 2014.   \nAI@Meta. Llama 3 model card. 2024. URL https://github.com/meta-llama/llama3/blob/ main/MODEL_CARD.md.   \nPeter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit ", "page_idx": 9}, {"type": "text", "text": "problem. Machine learning, 47:235\u2013256, 2002. ", "page_idx": 9}, {"type": "text", "text": "Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.   \nG\u00e1bor Bart\u00f3k and Csaba Szepesv\u00e1ri. Partial monitoring with side information. In International Conference on Algorithmic Learning Theory, pages 305\u2013319. Springer, 2012.   \nJoeran Beel, Stefan Langer, Andreas N\u00fcrnberger, and Marcel Genzmehr. The impact of demographics (age and gender) and other user-characteristics on evaluating recommender systems. In Research and Advanced Technology for Digital Libraries: International Conference on Theory and Practice of Digital Libraries, TPDL 2013, Valletta, Malta, September 22-26, 2013. Proceedings 3, pages 396\u2013400. Springer, 2013.   \nWei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E Gonzalez, et al. Chatbot arena: An open platform for evaluating llms by human preference. arXiv preprint arXiv:2403.04132, 2024.   \nDylan Foster and Alexander Rakhlin. Beyond ucb: Optimal and efficient contextual bandits with regression oracles. In International Conference on Machine Learning, pages 3199\u20133210. PMLR, 2020.   \nDylan Foster, Alekh Agarwal, Miroslav Dudik, Haipeng Luo, and Robert Schapire. Practical contextual bandits with regression oracles. In International Conference on Machine Learning, pages 1539\u20131548. PMLR, 2018.   \nDylan J Foster and Akshay Krishnamurthy. Contextual bandits with surrogate losses: Margin bounds and efficient algorithms. Advances in Neural Information Processing Systems, 31, 2018.   \nDylan J Foster and Akshay Krishnamurthy. Efficient first-order contextual bandits: Prediction, allocation, and triangular discrimination. Advances in Neural Information Processing Systems, 34: 18907\u201318919, 2021.   \nEuan Freeman, Graham Wilson, Dong-Bach Vo, Alex Ng, Ioannis Politis, and Stephen Brewster. Multimodal feedback in hci: haptics, non-speech audio, and their applications. In The Handbook of Multimodal-Multisensor Interfaces: Foundations, User Modeling, and Common Modality Combinations-Volume 1, pages 277\u2013317. 2017.   \nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.   \nXiaoyan Hu, Farzan Farnia, and Ho-fung Leung. An information theoretic approach to interactiongrounded learning. arXiv preprint arXiv:2401.05015, 2024.   \nJohannes Kirschner, Tor Lattimore, and Andreas Krause. Information directed sampling for linear partial monitoring. In Proceedings of Thirty Third Conference on Learning Theory, pages 2328\u2013 2369, 2020.   \nJohn Langford and Tong Zhang. The epoch-greedy algorithm for multi-armed bandits with side information. Advances in neural information processing systems, 20, 2007.   \nJessica Maghakian, Paul Mineiro, Kishan Panaganti, Mark Rucker, Akanksha Saran, and Cheng Tan. Personalized reward learning with interaction-grounded learning (igl). arXiv preprint arXiv:2211.15823, 2022.   \nS Mangrulkar, S. Gugger, L. Debut, Y. Belkada, and Paul S. Peft: State-of-the-art parameter-efficient fine-tuning methods. https://github.com/huggingface/peft, 2024. Accessed: 2024-05-14.   \nFrancesco Orabona and Tatiana Tommasi. Training deep networks without learning rates through coin betting. Advances in Neural Information Processing Systems, 30, 2017. URL https: //github.com/bremen79/parameterfree.   \nMaja Pantic and Leon JM Rothkrantz. Toward an affect-sensitive multimodal human-computer interaction. Proceedings of the IEEE, 91(9):1370\u20131390, 2003.   \nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.   \nDonghee Shin. How do users interact with algorithm recommender systems? the interaction of users, algorithms, and performance. Computers in human behavior, 109:106344, 2020.   \nDavid Simchi-Levi and Yunzong Xu. Bypassing the monster: A faster and simpler optimal algorithm for contextual bandits under realizability. Mathematics of Operations Research, 2021.   \nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.   \nTim Van Erven, Peter Grunwald, Nishant A Mehta, Mark Reid, Robert Williamson, et al. Fast rates in statistical and online learning. Journal of Machine Learning Research, 54(6), 2015.   \nVladimir G Vovk. A game of prediction with expert advice. In Proceedings of the eighth annual conference on Computational learning theory, pages 51\u201360, 1995.   \nQingyun Wu, Hongning Wang, Liangjie Hong, and Yue Shi. Returning is believing: Optimizing longterm user engagement in recommender systems. In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, pages 1927\u20131936, 2017.   \nTengyang Xie, John Langford, Paul Mineiro, and Ida Momennejad. Interaction-grounded learning. In International Conference on Machine Learning, pages 11414\u201311423. PMLR, 2021.   \nTengyang Xie, Akanksha Saran, Dylan J Foster, Lekan Molu, Ida Momennejad, Nan Jiang, Paul Mineiro, and John Langford. Interaction-grounded learning with action-inclusive feedback. Advances in Neural Information Processing Systems, 35:12529\u201312541, 2022.   \nXing Yi, Liangjie Hong, Erheng Zhong, Nanthan Nan Liu, and Suju Rajan. Beyond clicks: dwell time for personalization. In Proceedings of the 8th ACM Conference on Recommender systems, pages 113\u2013120, 2014.   \nMengxiao Zhang, Yuheng Zhang, Haipeng Luo, and Paul Mineiro. Efficient contextual bandits with uninformed feedback graphs. International Conference on Machine Learning, 2024a.   \nMengxiao Zhang, Yuheng Zhang, Olga Vrousgou, Haipeng Luo, and Paul Mineiro. Practical contextual bandits with feedback graphs. Advances in Neural Information Processing Systems, 36, 2024b.   \nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Proofs in Section 3.1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Lemma 1. For any context $x\\in\\mathscr{X}$ , suppose that the learner picks a uniformly random action $a\\in[K]$ . Let $r$ and $y$ be its realized reward and the corresponding feedback. Then, under Assumption $^{l}$ and Assumption 2, the posterior distribution of a given the context $x$ and feedback y equals to ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathsf{P r}[a|x,y]=\\frac{f^{\\star}(x,a)\\cdot\\phi^{\\star}(x,y)}{\\sum_{a^{\\prime}=1}^{K}f^{\\star}(x,a^{\\prime})}+\\frac{(1-f^{\\star}(x,a))(1-\\phi^{\\star}(x,y))}{K-\\sum_{a^{\\prime}=1}^{K}f^{\\star}(x,a^{\\prime})},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $f^{\\star}$ and $\\phi^{\\star}$ are the true expected reward and feedback decoder defined in Assumption 2. ", "page_idx": 12}, {"type": "text", "text": "Proof. ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathsf{P r}[a|x,y]=\\frac{\\mathsf{P r}[a]\\,x\\,\\cdot\\,\\mathsf{P r}[y]\\,x\\,,a]}{\\mathsf{P r}[y]\\,x}}\\qquad}&{}\\\\ &{=\\mathsf{P r}[a]\\,x\\,\\frac{\\mathsf{P r}[r]\\,x\\,=\\,1-\\,\\mathsf{P r}[y]\\,x\\,,a,\\,r=1\\}+\\mathsf{P r}[y]\\,x\\,=\\,0|x\\,,a]\\cdot\\,\\mathsf{P r}[y]\\,x\\,,a,\\,r=0]}\\\\ &{=\\mathsf{P r}[a]\\,x\\,\\frac{\\mathsf{P r}[a,a]\\,\\cdot\\,\\mathsf{P r}[y]\\,x,r=1]+\\mathsf{P r}[a]\\,x\\,\\cdot\\,\\mathsf{P r}[y]\\,x\\,,r=0]}{\\mathsf{P r}[y]\\,x}}\\\\ &{=\\mathsf{P r}[a]\\,x\\,\\frac{\\int^{\\mathsf{P r}}(x,a)\\,\\cdot\\,\\mathsf{P r}[y]\\,x\\,,r=1]+\\mathsf{P r}[a]\\,x\\,\\frac{\\int^{\\mathsf{P r}}(x,a)\\,\\cdot\\,\\mathsf{P r}[y]\\,x\\,,r=0]}{\\mathsf{P r}[y]\\,x}}\\\\ &{=\\mathsf{P r}[a]\\,x\\,\\frac{\\int^{\\mathsf{P r}}(x,a)\\,\\cdot\\,\\mathsf{P r}[r=1]\\,x\\,y}{\\mathsf{P r}[r=1]\\,x}\\,+\\mathsf{P r}[a]\\,x\\,\\frac{\\left(1-\\,r^{\\mathsf{*}}(x,a)\\right)\\,\\cdot\\,\\mathsf{P r}[r=0]\\,x\\,y}{\\mathsf{P r}[r=0]\\,x}}\\\\ &{=\\mathsf{P r}[a]\\,x\\,\\frac{\\int^{\\mathsf{P r}}(x,a)\\,\\cdot\\,\\mathsf{P r}[r=1]\\,x\\,y}{\\mathsf{C r}_{a}^{\\prime}\\,x}}\\\\ &{=\\mathsf{P r}[a]\\,x\\,\\frac{\\int^{\\mathsf{P r}}(x,a)\\,\\cdot\\,\\mathsf{P r}[r=1]\\,\\cdot\\,\\mathsf{P r}[a]\\,x\\,,a^{\\prime}}{\\mathsf{P r}[a]\\,x}+\\mathsf{P r}[a]\\,x\\,\\frac{\\int^{\\mathsf{P r}}(x,a)\\,\\cdot\\,\\mathsf{P r}[a]\\,x\\,-0]x\\,y}{\\mathsf{C r}_{a}^{\\prime}\\,x}}\\\\ &{=\\mathsf{P r}[a]\\,x\\,\\frac{\\int^{\\mathsf{P r}}(x,a)\\,\\cdot\\,\\mathsf{P r}[r=1]\\,x\\,y}{\\mathsf{C r}_{a}^{\\prime}\\,x}+\\mathsf{P r} \n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Recall that \u03c0Unif = 1 is the policy which uniformly samples an action regardless of the context. Under $\\pi_{\\mathrm{Unif}}$ , we know that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathsf{P r}[a|x,y]=\\frac{f^{\\star}(x,a)\\cdot\\phi^{\\star}(x,y)}{\\sum_{a^{\\prime}=1}^{K}f^{\\star}(x,a^{\\prime})}+\\frac{(1-f^{\\star}(x,a))(1-\\phi^{\\star}(x,y))}{K-\\sum_{a^{\\prime}=1}^{K}f^{\\star}(x,a^{\\prime})}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Lemma 3. Let $\\{(x_{i},a_{i},y_{i})\\}_{i=1}^{N}$ be $N$ i.i.d. samples where $x_{i}~\\in~{\\mathcal{D}}$ , $a~\\in~\\pi_{\\mathrm{Unif}}$ , and $y_{i}$ is the corresponding feedback. Let $\\widehat{h}$ be the ERM with respect to the squared loss defined as follows: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\widehat{h}=\\underset{h\\in\\mathcal{H}}{\\operatorname{argmin}}\\left\\{\\sum_{i=1}^{N}\\|h(x_{i},y_{i})-e_{a_{i}}\\|_{2}^{2}\\right\\}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Then, with probability at least $1-\\delta$ , we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbb{E}_{x\\sim\\mathcal{D},a\\sim\\pi_{\\mathrm{UM}},y\\mid x,a}\\left[\\|\\widehat{h}(x,y)-e_{a}\\|_{2}^{2}-\\|h^{\\star}(x,y)-e_{a}\\|_{2}^{2}\\right]\\leq\\mathcal{O}\\left(\\frac{\\log\\frac{|\\mathcal{H}|}{\\delta}}{N}\\right),}\\\\ &{}&{\\mathbb{E}_{x\\sim\\mathcal{D},a^{\\prime}\\sim\\pi_{\\mathrm{UM}},y\\mid x,a^{\\prime}}\\left[\\|\\widehat{h}(x,y)-h^{\\star}(x,y)\\|_{2}\\right]\\leq\\mathcal{O}\\left(\\sqrt{\\frac{\\log\\frac{|\\mathcal{H}|}{\\delta}}{N}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Proof. For notational convenience, we denote $(x,y,a)$ by $Z$ and define $\\ell_{h}(Z)=\\|h(x,y)-e_{a}\\|_{2}^{2}$ . Now we aim to show that $\\ell_{h}(Z)$ satisfies the strong $\\eta$ -central condition for some $\\eta>0$ [Van Erven et al., 2015]. Specifically, we aim to show that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbb{E}_{Z}\\left[\\exp(-\\eta(\\ell_{h}(Z)-\\ell_{h^{\\star}}(Z)))\\right]\\le1.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "To show this, direct calculation shows that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbb{E}_{Z}\\left[\\exp(-\\eta(\\ell_{h}(Z)-\\ell_{h^{\\star}}(Z)))\\right]\n$$", "text_format": "latex", "page_idx": 12}, {"type": "equation", "text": "$$\n=\\mathbb{E}_{x,y}\\left[\\exp(-\\eta\\|h(x,y)-h^{\\star}(x,y)\\|_{2}^{2})\\cdot\\mathbb{E}_{a|x,y}\\left[\\exp\\left(-2\\eta(h(x,y)-h^{\\star}(x,y)\\right)^{\\top}(h^{\\star}(x,y)-e_{a})\\right)\\right]\\right]\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Since $\\mathbb{E}_{a|x,y}[e_{a}]=h^{\\star}(x,y)$ , the random variable $(h(x,y)-h^{\\star}(x,y))^{\\top}(h^{\\star}(x,y)-e_{a})$ given $x$ and $y$ is zero-mean and is within the range $[-2\\|h(x,y)-h^{\\star}(x,y)\\|_{2},2\\|h(x,y)-h^{\\star}(x,y)\\|_{2}]$ . Therefore, we know that $(h(x,y)-h^{\\star}(x,y))^{\\top}(\\bar{h}^{\\star}(x,y)-e_{a})$ is $\\|h(x,y)-h^{\\star}(x,y)\\|_{2}^{2}$ -sub-Gaussian and we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{Z}\\left[\\exp(-\\eta(\\ell_{h}(Z)-\\ell_{h^{\\star}}(Z)))\\right]}\\\\ &{\\leq\\mathbb{E}_{x,y}\\left[\\exp(-\\eta\\|h(x,y)-h^{\\star}(x,y)\\|_{2}^{2})\\exp\\left(\\frac{4\\eta^{2}\\|h(x,y)-h^{\\star}(x,y)\\|_{2}^{2}}{2}\\right)\\right]}\\\\ &{=\\mathbb{E}_{x,y}\\left[\\exp\\left(\\left(2\\eta^{2}-\\eta\\right)\\|h(x,y)-h^{\\star}(x,y)\\|_{2}^{2}\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Picking $\\begin{array}{r}{\\eta\\,=\\,\\frac{1}{2}}\\end{array}$ proves Eq. (8). Noticing the fact that $|\\ell_{h}(Z)|\\,\\leq\\,4$ for all $h\\ \\in\\ \\mathcal H$ and applying Theorem 7.6 in [Van Erven et al., 2015] proves Eq. (4). ", "page_idx": 13}, {"type": "text", "text": "Now we prove Eq. (5). Noticing the fact that $\\mathbb{E}_{a|x,y}[e_{a}]=h^{\\star}(x,y)$ and applying this to Eq. (4), we know that with probability at least $1-\\delta$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x,y}\\left[\\lVert\\widehat{h}(x,y)-h^{\\star}(x,y)\\rVert_{2}^{2}\\right]\\leq\\mathcal{O}\\left(\\frac{\\log\\frac{|\\mathcal{H}|}{\\delta}}{N}\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "This means that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{a^{\\prime}\\sim\\pi_{\\mathrm{Uif}}}\\mathbb{E}_{y\\mid x,a^{\\prime}}\\left[\\Vert\\widehat{h}(x,y)-h^{\\star}(x,y)\\Vert_{2}^{2}\\right]\\le\\mathcal{O}\\left(\\frac{\\log\\frac{|\\mathcal{H}|}{\\delta}}{N}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Further applying Jensen\u2019s inequality shows that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{a^{\\prime}\\sim\\pi_{\\mathrm{UI}}}\\mathbb{E}_{y\\mid x,a^{\\prime}}\\left[\\|\\widehat{h}(x,y)-h^{\\star}(x,y)\\|_{2}\\right]}\\\\ &{=\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{a^{\\prime}\\sim\\pi_{\\mathrm{UI}}}\\mathbb{E}_{y\\mid x,a^{\\prime}}\\left[\\sqrt{\\|\\widehat{h}(x,y)-h^{\\star}(x,y)\\|_{2}^{2}}\\right]}\\\\ &{\\le\\sqrt{\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{a^{\\prime}\\sim\\pi_{\\mathrm{UI}}}\\mathbb{E}_{y\\mid x,a^{\\prime}}\\left[\\|\\widehat{h}(x,y)-h^{\\star}(x,y)\\|_{2}^{2}\\right]}}\\\\ &{\\le\\mathcal{O}\\left(\\sqrt{\\frac{\\log\\frac{|\\mathcal{H}|}{\\delta}}{N}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "(Jensen\u2019s inequality) ", "page_idx": 13}, {"type": "text", "text": "Lemma 4. Define $G(v,\\beta,\\sigma)$ as ", "page_idx": 13}, {"type": "equation", "text": "$$\nG(v,\\beta,\\sigma)=\\frac{1}{\\sigma}(v-\\beta)\\mathbb{1}\\{\\beta\\le v<\\beta+\\sigma\\}+\\mathbb{1}\\{v\\ge\\beta+\\sigma\\}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then, for any context $x\\in\\mathscr{X}$ , action $a\\in[K]$ , and feedback $y\\in\\mathcal{V}$ generated via context $x$ and the realized reward $r(x,a)$ , we have the following two properties with $\\begin{array}{r}{\\sigma\\triangleq\\frac{1}{2}\\left(\\frac{\\theta}{\\alpha}-\\frac{1}{K-\\alpha}\\right)>0.}\\end{array}$ . ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\ r(x,a)=\\phi^{\\star}(x,y)\\geq G(h_{a}^{\\star}(x,y),\\frac{\\theta}{\\alpha}-\\sigma,\\sigma),}\\\\ &{\\bullet\\ r(x,a)=\\phi^{\\star}(x,y)=G(h_{a}^{\\star}(x,y),\\frac{\\theta}{\\alpha}-\\sigma,\\sigma)\\ i f a=\\pi^{\\star}(x),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. To prove the first property, we show $\\phi^{\\star}(x,y)\\,=\\,1\\$ if $\\begin{array}{r}{h_{a}^{\\star}(x,y)\\,\\geq\\,\\frac{\\theta}{\\alpha}\\,-\\,\\sigma}\\end{array}$ . Specifically, if $\\begin{array}{r}{h_{a}^{\\star}(x,y)\\geq\\frac{\\theta}{\\alpha}-\\sigma}\\end{array}$ and $\\phi^{\\star}(x,y)=0$ , then we know that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{\\theta}{\\alpha}-\\sigma\\leq h_{a}^{\\star}(x,y)=\\frac{1-f^{\\star}(x,a)}{K-\\sum_{i=1}^{K}f^{\\star}(x,i)}\\leq\\frac{1}{K-\\alpha},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": ",o nwtrhaicdihc itsi osnu raeclcyo arnd iunpg pteor  tbhoe udnedf inoift $\\sigma$ hseinn $\\begin{array}{r}{h_{a}^{\\star}(x,y)\\geq\\frac{\\theta}{\\alpha}\\!-\\!\\sigma}\\end{array}$ $\\phi^{\\star}(x,y)=1$ $\\begin{array}{r}{G(h_{a}^{\\star}(x,y),\\frac{\\theta}{\\alpha}\\!-\\!\\sigma,\\sigma)}\\end{array}$ $\\begin{array}{r}{G(h_{a}^{\\star}(x,y),\\frac{\\theta}{\\alpha}\\!-\\!\\sigma,\\sigma)\\leq}\\end{array}$ 1. When $\\begin{array}{r}{h_{a}^{\\star}(x,y)\\,<\\,\\frac{\\theta}{\\alpha}\\,-\\,\\sigma}\\end{array}$ , by definition, we know that $\\vec{G}(h_{a}^{\\star}(x,y),\\frac{\\theta}{\\alpha}-\\sigma,\\sigma)\\,=\\,0\\,\\stackrel{\\leftarrow}{\\leq}\\,\\phi^{\\star}(x,y)$ . Combining the two cases finishes the proof for the first property. ", "page_idx": 14}, {"type": "text", "text": "To prove the second property, note that when $\\phi^{\\star}(x,y)=1$ and $a=\\pi^{\\star}(x)$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\nh_{\\pi^{\\star}(x)}^{\\star}(x,y)=\\frac{f^{\\star}(x,\\pi^{\\star}(x))}{\\sum_{i=1}^{K}f^{\\star}(x,i)}\\geq\\frac{\\theta}{\\alpha},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the last inequality is by Assumption 3. This means that $\\begin{array}{r}{G(h_{a}^{\\star}(x,y),\\frac{\\theta}{\\alpha}-\\sigma,\\sigma)=1=\\phi^{\\star}(x,y)}\\end{array}$ . When $\\phi^{\\star}(x,y)=0$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\nh_{\\pi^{\\star}(x)}^{\\star}(x,y)=\\frac{1-f^{\\star}(x,\\pi^{\\star}(x))}{K-\\sum_{i=1}^{K}f^{\\star}(x,i)}\\le\\frac{1}{K-\\alpha}\\le\\frac{\\theta}{\\alpha}-\\sigma,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "meaning that $\\begin{array}{r}{G(h_{a}^{\\star}(x,y),\\frac{\\theta}{\\alpha}-\\sigma,\\sigma)=0}\\end{array}$ . This finishes the proof. ", "page_idx": 14}, {"type": "text", "text": "B Proofs in Section 3.2 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For any policy $\\pi$ and reward estimator $r_{\\sigma}$ , we define the value function $V(\\pi,r_{\\sigma})$ and the empirical value function $\\widehat{V}(\\pi,r_{\\sigma})$ as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{V}(\\pi,r_{\\sigma})=\\displaystyle\\frac{1}{N}\\sum_{i=N+1}^{2N}K\\cdot\\pi(a_{i}|x_{i})\\cdot r_{\\sigma}(x_{i},y_{i},a_{i}),}\\\\ &{V(\\pi,r_{\\sigma})=\\mathbb{E}_{x\\sim{\\mathcal{D}},a\\sim\\pi_{\\mathrm{Unif}},y|x,a}\\left[K\\cdot\\pi(a|x)\\cdot r_{\\sigma}(x,y,a)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\{(x_{i},a_{i},y_{i})\\}_{i=N+1}^{2N}$ is collected with the uniform policy $\\pi_{\\mathrm{Unif}}$ . The true value function $V(\\pi)$ is defined as: ", "page_idx": 14}, {"type": "equation", "text": "$$\nV(\\pi)=\\mathbb{E}_{x\\sim\\mathcal{D}}\\left[\\sum_{a=1}^{K}\\pi(a|x)\\cdot f^{\\star}(x,a)\\right].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Using Lemma 4, we can establish the equivalence between $V(\\pi^{\\star})$ and $V(\\pi^{\\star},r_{\\sigma}^{\\star})$ where $r_{\\sigma}^{\\star}(x,y,a)=$ $\\begin{array}{r}{G(h_{a}^{\\star}(x,y),\\frac{\\theta}{\\alpha}-\\sigma,\\sigma)}\\end{array}$ . ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V(\\pi^{\\star})=\\mathbb{E}_{x\\sim\\mathcal{D},a\\sim\\pi_{\\mathrm{Unif}}}\\left[K\\cdot\\pi^{\\star}(a|x)\\cdot f^{\\star}(x,a)\\right]}\\\\ &{\\qquad\\quad=\\mathbb{E}_{x\\sim\\mathcal{D},a\\sim\\pi_{\\mathrm{Unif}},y|x,a}\\left[K\\cdot\\pi^{\\star}(a|x)\\cdot\\phi^{\\star}(x,y)\\right]}\\\\ &{\\qquad\\quad=\\mathbb{E}_{x\\sim\\mathcal{D},a\\sim\\pi_{\\mathrm{Unif}},y|x,a}\\left[K\\cdot\\pi^{\\star}(a|x)\\cdot G\\left(h_{a}^{\\star}(x,y),\\frac{\\theta}{\\alpha}-\\sigma,\\sigma\\right)\\right]}\\\\ &{\\qquad\\quad=V(\\pi^{\\star},r_{\\sigma}^{\\star}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In the following lemma, we prove that $\\widehat{\\pi}$ is near optimal under the true reward. ", "page_idx": 14}, {"type": "text", "text": "Lemma 5. Under Assumption 2 and Assumption 3, the following holds with probability at least $1-3\\delta$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\nV(\\pi^{\\star})-V(\\widehat\\pi)\\leq\\mathcal{O}\\left(\\frac{K}{\\sigma}\\sqrt{\\frac{\\log\\frac{|\\mathcal{H}|}{\\delta}}{N}}\\right),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\widehat{\\pi}$ is defined in Algorithm 1. ", "page_idx": 14}, {"type": "text", "text": "Proof. We first introduce some high probability events that the following analysis is based on. First, according to Hoeffding\u2019s inequality together with a union bound over $\\pi\\,\\in\\,\\Pi$ (note that $|\\Pi|=|\\mathcal{F}|\\leq|\\bar{\\mathcal{H}}|)$ , with probability at least $1-\\delta$ , we have for any $\\pi\\in\\Pi$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|V(\\pi,r_{\\sigma}^{\\star})-\\widehat{V}(\\pi,r_{\\sigma}^{\\star})\\right|\\leq\\mathcal{O}\\left(K\\cdot\\sqrt{\\frac{\\log\\frac{|\\mathcal{H}|}{\\delta}}{N}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In addition, according to Hoeffding\u2019s inequality with a union bound over $h\\in\\mathcal H$ , with probability at least $1-\\delta$ , we have for any $h\\in\\mathcal H$ , the following inequality holds: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\lvert\\frac1N\\sum_{i=N+1}^{2N}\\lVert h(x_{i},y_{i})-h^{\\star}(x_{i},y_{i})\\rVert_{2}-\\mathbb E_{x\\sim\\mathcal D,a\\sim\\pi_{\\mathrm{Unif}},y|x,a}\\left[\\lVert h(x,y)-h^{\\star}(x,y)\\rVert_{2}\\right]\\right\\rvert}\\\\ &{\\le\\mathcal O\\left(\\sqrt{\\frac{\\log\\frac{|\\mathcal H|}{\\delta}}{N}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Combining Eq. (14) with Eq. (5) in Lemma 3, we know that with probability at least $1-2\\delta$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{N}\\displaystyle\\sum_{i=N+1}^{2N}\\Big|\\widehat{h}_{a_{i}}(x_{i},y_{i})-h_{a_{i}}^{\\star}(x_{i},y_{i})\\Big|\\leq\\frac{1}{N}\\displaystyle\\sum_{i=N+1}^{2N}\\|\\widehat{h}(x_{i},y_{i})-h^{\\star}(x_{i},y_{i})\\|_{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\mathcal{O}\\left(\\sqrt{\\displaystyle\\frac{\\log\\frac{|\\mathcal{H}|}{\\delta}}{N}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The following analysis is based on the condition that Eq. (13) and Eq. (15) hold. ", "page_idx": 15}, {"type": "text", "text": "Bounding $\\left|\\widehat{V}(\\pi,\\widehat{r}_{\\sigma})-\\widehat{V}(\\pi,r_{\\sigma}^{\\star})\\right|$ . We first show that for any policy $\\pi\\in\\Pi$ , the prediction from $\\widehat{r}_{\\sigma}$ is close to $r_{\\sigma}^{\\star}$ in terms of the empirical value function defined in Eq. (10), where $\\widehat{r}_{\\sigma}(x,y,a)\\triangleq$ $\\begin{array}{r}{G(\\widehat{h}_{a}(x,y),\\frac{\\theta}{\\alpha}-\\sigma,\\sigma)}\\end{array}$ . ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\hat{V}(\\pi,\\widetilde{\\gamma}_{r})-\\hat{V}(\\pi,\\pi_{r}^{*})\\right|\\leq\\displaystyle\\frac{1}{N}\\displaystyle\\sum_{i=1}^{2N}K\\cdot\\pi(a_{i}|\\kappa_{i})\\left|\\widehat{\\gamma}_{\\pi}(\\pi_{i},y_{i},a_{i})-r_{\\pi}^{*}(x_{i},y_{i},a_{i})\\right|}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\frac{K}{N}\\displaystyle\\sum_{i=1}^{2N}|\\widehat{\\gamma}_{\\pi}(\\pi_{i},y_{i},a_{i})-r_{\\pi}^{*}(x_{i},y_{i},a_{i})|}\\\\ &{\\qquad=\\displaystyle\\frac{K}{N}\\displaystyle\\sum_{i=1}^{2N}\\left|\\left(\\widehat{\\gamma}_{\\pi_{i}}(\\pi_{i},y_{i})\\displaystyle\\frac{\\theta}{\\pi}-\\sigma,\\sigma\\right)-G(h_{\\pi_{i}}^{*}(x_{i},y_{i}),\\displaystyle\\frac{\\theta}{\\pi}-\\sigma,\\sigma)\\right|}\\\\ &{\\qquad\\leq\\displaystyle\\frac{K}{N\\sigma_{\\pi}}\\displaystyle\\sum_{i=1}^{2N}\\left|\\widehat{h}_{i}(\\pi_{i},y_{i})-h_{\\pi}^{*}(x_{i},y_{i})\\right|}\\\\ &{\\qquad\\leq\\displaystyle\\frac{K}{N\\sigma_{\\pi}}\\displaystyle\\sum_{i=1}^{2N}\\|\\widehat{h}(\\pi_{i},y_{i})-h^{*}(x_{i},y_{i})\\|_{2}}\\\\ &{\\qquad\\qquad\\leq\\sigma\\left(\\frac{K}{\\sigma}\\sqrt{\\frac{|\\alpha_{\\pi}|\\frac{\\mathbb{W}}{\\pi}}{N}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The third inequality is because $G(v,\\beta,\\sigma)$ is $\\frac{1}{\\sigma}$ -Lipschitz in $v$ and the last inequality is from Eq. (15). ", "page_idx": 15}, {"type": "text", "text": "Lower bound $V(\\pi)$ by $\\widehat{V}(\\pi,\\widehat{r}_{\\sigma})$ . Then, we show that for any policy $\\pi\\in\\Pi,V(\\pi)$ is lower bounded by $\\widehat{V}(\\pi,\\widehat{r}_{\\sigma})$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{V(\\pi)=\\mathbb{E}_{x\\sim\\mathcal{D}}\\left[\\displaystyle\\sum_{a=1}^{K}\\pi(a|x)f^{\\star}(x,a)\\right]}&{}\\\\ &{\\qquad=\\mathbb{E}_{x\\sim\\mathcal{D},a\\sim\\pi_{\\mathrm{Unif}}}\\left[K\\pi(a|x)f^{\\star}(x,a)\\right]}\\\\ &{\\qquad=\\mathbb{E}_{x\\sim\\mathcal{D},a\\sim\\pi_{\\mathrm{Unif}}}\\mathbb{E}_{y|x,a}\\left[K\\pi(a|x)\\phi^{\\star}(x,y)\\right]}\\\\ &{\\qquad\\geq\\mathbb{E}_{x\\sim\\mathcal{D},a\\sim\\pi_{\\mathrm{Unif}}}\\mathbb{E}_{y|x,a}\\left[K\\pi(a|x)G(h_{a}^{\\star}(x,y),\\frac{\\theta}{\\alpha}-\\sigma,\\sigma)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "(Assumption 2) ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{=V(\\pi,r_{\\sigma}^{\\star})}\\\\ {=V(\\pi,r_{\\sigma}^{\\star})-\\widehat{V}(\\pi,r_{\\sigma}^{\\star})+\\widehat{V}(\\pi,r_{\\sigma}^{\\star})-\\widehat{V}(\\pi,\\widehat{r}_{\\sigma})+\\widehat{V}(\\pi,\\widehat{r}_{\\sigma})}\\\\ {\\geq\\widehat{V}(\\pi,\\widehat{r}_{\\sigma})-\\mathcal{O}\\left(\\frac{K}{\\sigma}\\sqrt{\\frac{\\log\\frac{|\\mathcal{H}|}{\\delta}}{N}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The first inequality is from Lemma 4 and the second inequality is from Eq. (13) and Eq. (16). Recall that $\\widehat{\\pi}=\\operatorname*{max}_{\\pi\\in\\Pi}\\widehat{V}(\\pi,\\widehat{r}_{\\sigma})$ , we then know that with probability at least $1-3\\delta$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{V(\\pi^{*})-V(\\widehat\\pi)\\leq V(\\pi^{*})-\\widehat V(\\widehat\\pi,\\widehat\\tau_{\\widehat\\pi})+\\mathcal O\\left(\\frac{K}{\\sigma}\\sqrt{\\frac{\\log\\frac{|\\mathcal\\pi|}{\\delta}}{N}}\\right)}}\\\\ &{}&{\\leq V(\\pi^{*})-\\widehat V(\\pi^{*},\\widehat\\tau_{\\sigma})+\\mathcal O\\left(\\frac{K}{\\sigma}\\sqrt{\\frac{\\log\\frac{|\\mathcal\\pi|}{\\delta}}{N}}\\right)}\\\\ &{}&{\\leq V(\\pi^{*})-\\widehat V(\\pi^{*},r_{*}^{*})+\\mathcal O\\left(\\frac{K}{\\sigma}\\sqrt{\\frac{\\log\\frac{|\\mathcal\\pi|}{\\delta}}{N}}\\right)}\\\\ &{}&{\\leq V(\\pi^{*})-V(\\pi^{*},r_{*}^{*})+\\mathcal O\\left(\\frac{K}{\\sigma}\\sqrt{\\frac{\\log\\frac{|\\mathcal\\pi|}{\\delta}}{N}}\\right)}\\\\ &{}&{=\\mathcal O\\left(\\frac{K}{\\sigma}\\sqrt{\\frac{\\log\\frac{|\\mathcal\\pi|}{\\delta}}{N}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which finishes the proof. ", "page_idx": 16}, {"type": "text", "text": "Next, we prove Theorem 1. ", "page_idx": 16}, {"type": "text", "text": "Theorem 1. Under Assumptions 1-3, Algorithm $^{\\,l}$ with $N=T^{2/3}K^{2/3}\\sigma^{-2/3}\\log^{1/3}(|\\mathcal{H}|T)$ guarantees that $\\mathbf{Reg}_{\\mathsf{C B}}\\leq\\mathcal{O}\\left(T^{2/3}K^{2/3}\\sigma^{-2/3}\\log^{1/3}(|\\mathcal{H}|T)\\right)$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. We bound $\\mathbf{Reg}_{\\mathsf{C B}}$ as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{Reg_{CB}}=\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}f^{*}(x_{t},\\pi^{*}(x_{t}))-\\displaystyle\\sum_{t=1}^{T}f^{*}(x_{t},a_{t})\\right]}\\\\ &{\\phantom{\\ x x}\\leq2N+\\mathbb{E}\\left[\\displaystyle\\sum_{t=2N+1}^{T}f^{*}(x_{t},\\pi^{*}(x_{t}))-\\displaystyle\\sum_{t=2N+1}^{T}f^{*}(x_{t},a_{t})\\right]}\\\\ &{\\phantom{\\ x x}=2N+\\mathbb{E}\\left[\\displaystyle\\sum_{t=2N+1}^{T}f^{*}(x_{t},\\pi^{*}(x_{t}))-\\displaystyle\\sum_{t=2N+1}^{T}f^{*}(x_{t},\\widetilde{\\pi}(x_{t}))\\right]}\\\\ &{\\leq2N+T\\cdot\\mathbb{E}\\left[V(\\pi^{*})-V(\\widetilde{\\pi})\\right]}\\\\ &{\\leq\\mathcal{O}\\left(N+\\displaystyle\\frac{T K}{\\sigma}\\sqrt{\\frac{\\log(|\\mathcal{H}|T)}{N}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The last step is from Lemma 5 with $\\begin{array}{r}{\\delta=\\frac{1}{T}}\\end{array}$ . Picking $N=T^{2/3}K^{2/3}\\sigma^{-2/3}\\log^{1/3}(|\\mathcal{H}|T)$ finishes the proof. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "C Proofs in Section 3.3 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Theorem 2. Under Assumptions 1-5, Algorithm 2 with certain choice of $N$ and $\\gamma$ guarantees that $\\mathbf{Reg}_{\\mathrm{CB}}={\\mathcal{O}}\\left({\\sqrt{K T\\mathbf{Reg}_{5{\\mathrm{q}}}}}+\\sigma^{-2/3}(K{\\dot{T}})^{2/3}\\log^{1/3}(|{\\mathcal{H}}|T)\\right)$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. According to Eq. (9), we know that with probability at least $1-\\delta$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{n=1}^{K}\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\mid x,a}\\left(\\widehat{h}_{a}(x,y)-h_{a}^{\\star}(x,y)\\right)^{2}}\\\\ &{\\leq K\\cdot\\mathbb{E}_{x\\sim\\mathcal{D},a\\sim\\pi_{\\mathrm{tot}},y\\mid x,a}\\left[\\left(\\widehat{h}_{a}(x,y)-h_{a}^{\\star}(x,y)\\right)^{2}\\right]}\\\\ &{\\leq K\\cdot\\mathbb{E}_{x\\sim\\mathcal{D},a\\sim\\pi_{\\mathrm{tot}},y\\mid x,a}\\left[\\|\\widehat{h}(x,y)-h^{\\star}(x,y)\\|_{2}^{2}\\right]}\\\\ &{\\leq\\mathcal{O}\\left(\\frac{K\\log\\frac{\\vert\\mathcal{H}\\vert}{\\delta}}{N}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the last inequality uses Eq. (9). Let $a_{t}^{\\star}\\,=\\,\\operatorname{argmax}_{a}\\,f^{\\star}(x_{t},a)$ and recall that $\\underline{{f}}^{\\star}(x,a)\\;:=\\;$ $\\begin{array}{r}{\\mathbb{E}_{y|x,a}\\left[G(h_{a}^{\\star}(x,y),\\frac{\\theta}{\\alpha}-\\sigma,\\sigma)\\right]\\in\\mathcal{F}}\\end{array}$ . We have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbf{Reg}_{\\mathbf{C}\\mathbf{B}}=\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}f^{*}(x_{t},a_{t}^{*})-\\displaystyle\\sum_{t=1}^{T}f^{*}(x_{t},a_{t})\\right]}&{}\\\\ {=\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}_{y\\mid x_{t},a_{t}^{*}}\\big\\lbrack\\phi^{*}(x_{t},y)\\big\\rbrack-\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}_{y^{\\prime}\\mid x_{t},a_{t}}\\big\\lbrack\\phi^{*}(x_{t},y^{\\prime})\\big\\rbrack\\right]}&{}\\\\ {\\leq\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\underline{{f}}^{*}(x_{t},a_{t}^{*})-\\displaystyle\\sum_{t=1}^{T}\\underline{{f}}^{*}(x_{t},a_{t})\\right]}&{}\\\\ {\\leq\\frac{\\gamma}{4}\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}(\\hat{f}_{t}(x_{t},a_{t})-\\underline{{f}}^{*}(x_{t},a_{t}))^{2}\\right]+\\mathcal{O}\\left(\\frac{T K}{\\gamma}\\right)}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The last step is from Foster and Rakhlin [2020, Lemma 3]. Recall that $\\widehat{r}_{\\sigma}(x_{t},y_{t},a_{t})\\;\\;=$ $\\begin{array}{r}{G(\\widehat{h}_{a_{t}}(x_{t},y_{t}),\\frac{\\theta}{\\alpha}-\\sigma,\\sigma)}\\end{array}$ . Direct calculation shows that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbf{Reg}_{\\mathbf{S}_{\\mathbf{q}}}\\ge\\mathbb{E}\\left[\\frac{T}{\\underset{t=1}{\\sum}(\\widehat{f}_{t}(x_{t},a_{t})-\\widehat{f}_{r}(x_{t},y_{t},a_{t}))^{2}-\\sum_{t=1}^{T}(\\underline{{f}}^{*}(x_{t},a_{t})-\\widehat{f}_{r}(x_{t},y_{t},a_{t}))^{2}\\right]}\\\\ {=\\mathbb{E}\\left[\\frac{T}{\\underset{t=1}{\\sum}(\\widehat{f}_{t}(a_{t})-\\int^{*}(x_{t},a_{t}))(\\widehat{f}_{t}(x_{t},a_{t})+\\int^{*}(x_{t},a_{t})-2\\widehat{f}_{r}(x_{t},y_{t},a_{t}))\\right]}\\\\ {=\\mathbb{E}\\left[\\frac{T}{\\underset{t=1}{\\sum}(\\widehat{f}_{t}(x_{t},a_{t})-\\int^{*}(x_{t},a_{t}))^{2}}\\right]}\\\\ &{\\ \\ \\ +2\\mathbb{E}\\left[\\frac{T}{\\underset{t=1}{\\sum}(\\widehat{f}_{t}(x_{t},a_{t})-\\underline{{f}}^{*}(x_{t},a_{t}))(\\underline{{f}}^{*}(x_{t},a_{t})-\\mathbb{E}_{y_{t}}[\\widehat{f}_{\\sigma}(x_{t},y_{t},a_{t})])\\right]}\\\\ {\\ge\\frac{1}{2}\\mathbb{E}\\left[\\frac{T}{\\underset{t=1}{\\sum}(\\widehat{f}_{t}(x_{t},a_{t})-\\int^{*}(x_{t},a_{t}))^{2}}\\right]-2\\mathbb{E}\\left[\\frac{T}{\\underset{t=1}{\\sum}(\\widehat{f}^{*}(x_{t},a_{t})-\\mathbb{E}_{y_{t}}[\\widehat{r}_{\\sigma}(x_{t},y_{t},a_{t})])^{2}}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the last step is by AM-GM inequality. For the second term, we know that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\left(\\underline{{f}}^{\\star}(x_{t},a_{t})-\\mathbb{E}_{y_{t}}[\\widehat{r}_{\\sigma}(x_{t},y_{t},a_{t})]\\right)^{2}\\right]}\\\\ &{=\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\left(\\mathbb{E}_{y_{t}|x_{t},a_{t}}\\left[G\\left(h_{a_{t}}^{\\star}(x_{t},y_{t}),\\frac{\\theta}{\\alpha}-\\sigma,\\sigma\\right)-G\\left(\\widehat{h}_{a_{t}}(x_{t},y_{t}),\\frac{\\theta}{\\alpha}-\\sigma,\\sigma\\right)\\right]\\right)^{2}\\right]}\\\\ &{\\le\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}_{y_{t}|x_{t},a_{t}}\\left[\\left(G\\left(h_{a_{t}}^{\\star}(x_{t},y_{t}),\\frac{\\theta}{\\alpha}-\\sigma,\\sigma\\right)-G\\left(\\widehat{h}_{a_{t}}(x_{t},y_{t}),\\frac{\\theta}{\\alpha}-\\sigma,\\sigma\\right)\\right)^{2}\\right]\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\sum_{a=1}^{K}\\mathbb{E}_{y\\mid x_{t},a}\\left[\\left(G\\left(h_{a}^{\\star}(x_{t},y),\\frac{\\theta}{\\alpha}-\\sigma,\\sigma\\right)-G\\left(\\widehat{h}_{a}(x_{t},y),\\frac{\\theta}{\\alpha}-\\sigma,\\sigma\\right)\\right)^{2}\\right]\\right]}\\\\ &{\\leq\\displaystyle\\frac{1}{\\sigma^{2}}\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\sum_{a=1}^{K}\\mathbb{E}_{y\\mid x_{t},a}\\left[\\left(h_{a}^{\\star}(x_{t},y)-\\widehat{h}_{a}(x_{t},y)\\right)^{2}\\right]\\right]}\\\\ &{\\leq\\mathcal{O}\\left(\\frac{T K\\log(|\\mathcal{H}|T)}{\\sigma^{2}N}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the first inequality is from Jensen\u2019s inequality; the third inequality is from that $G(v,\\beta,\\sigma)$ is $\\frac{1}{\\sigma_{\\!}}$ -Lipschitz in $v$ ; and the last inequality is by Eq. (18) with $\\begin{array}{r}{\\delta=\\frac{1}{T}}\\end{array}$ . Combining Eqs. (19)-(21), we obtain that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbf{Reg}_{\\mathrm{CB}}\\leq\\mathcal{O}\\left(\\gamma\\mathbf{Reg}_{\\mathsf{S q}}+\\frac{T K\\gamma\\log(|\\mathcal{H}|T)}{\\sigma^{2}N}+\\frac{T K}{\\gamma}+N\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Picking $\\begin{array}{r}{N=\\frac{1}{\\sigma}\\sqrt{T K\\gamma\\log(|\\mathcal{H}|T)}}\\end{array}$ and $\\gamma=\\operatorname*{min}\\Big\\{\\sqrt{K T/\\mathbf{Reg}_{5\\mathsf{q}}},\\sigma^{-2/3}(K T)^{2/3}\\log^{-1/3}(|\\mathcal{H}|T)\\Big\\},$ we obtain that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbf{Reg}_{\\mathrm{CB}}\\leq\\mathcal{O}\\left(\\sqrt{K T\\mathbf{Reg}_{5\\mathrm{q}}}+\\sigma^{-2/3}(K T)^{2/3}\\log^{1/3}(|\\mathcal{H}|T)\\right),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which finishes the proof. ", "page_idx": 18}, {"type": "text", "text": "D Omitted Details in Section 4 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "D.1 Prompt Used in Generating Dataset ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The prompt we use in answer generation basically the question itself shown as follows. We replace \u201cquestion\u201d by $x$ in our experiment. ", "page_idx": 18}, {"type": "text", "text": "Prompt ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "{question} ", "page_idx": 18}, {"type": "text", "text": "The prompt we use in feedback generation as follows. We replace \u201cquestion\u201d and \u201canswer\u201d by $x$ and $a$ respectively in our experiments. We replace \u201cmood\u201d by \u201csatisfied\u201d (\u201cnot satisfied\u201d) when the answer is generated by Qwen1.5-32B-Chat (Qwen1.5-0.5B-Chat). ", "page_idx": 18}, {"type": "text", "text": "Prompt ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "System: You are a user simulator. A user has been presented a Question and an Answer. Simulate the user\u2019s next statement. The user is $\\{{\\mathrm{mood}}\\}$ with the Answer to the Question.   \nQuestion: {question}   \nAnswer:{answer} ", "page_idx": 18}, {"type": "text", "text": "D.2 Prompt Used in Learning Inverse Kinematic Model ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The prompt we use in learning the inverse kinematic model is as follows. We replace \u201cquestion\u201d, \u201canswer\u201d, and \u201cfeedback\u201d by $x,\\,a$ , and $y$ respectively in our experiments. ", "page_idx": 18}, {"type": "text", "text": "Prompt ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "System: You are a conversation evaluating agent. Given a User\u2019s Question, an Answer, and the User\u2019s Feedback: determine if the User\u2019s Feedback is consistent with Answer. Respond with Yes or No only.   \nUser\u2019s Question: {question}   \nAnswer:{answer}   \nUser\u2019s Feedback:{feedback} ", "page_idx": 18}, {"type": "text", "text": "D.3 Prompt Used in Learning Policy ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The prompt we use in learning the reward predictor is as follows. We replace \u201cquestion\u201d and \u201canswer\u201d by $x$ and $a$ respectively in our experiments. ", "page_idx": 19}, {"type": "text", "text": "Prompt ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "System: You are an Answer evaluating agent. Given a User\u2019s Question and an Answer: assess if the Answer is good. Respond with Yes or No only.   \nUser\u2019s Question:{question}   \nAnswer:{answer}   \nRespond with Yes or No only. ", "page_idx": 19}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: See abstract and Section 1. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: See Section 1 and Section 2. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: See Assumption 1, Assumption 2, Assumption 3, Assumption 4, Assumption 5, and the appendix. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: See Algorithm 1, Algorithm 2, and detailed descriptions in Section 4 and Appendix D. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: See supplementary materials. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] Justification: See Section 4. ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: See Section 4. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] Justification: See Section 4. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The authors have reviewed the NeurIPS Code of Ethics. The research conducted in this paper conforms with it in every respect. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This work is mostly theoretical, and we do not foresee any negative ethical or societal outcomes. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This paper poses no such risks. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 24}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We properly cite the used dataset and models and respect the licenses. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 24}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 25}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}]