{"references": [{"fullname_first_author": "Bansal, A.", "paper_title": "Universal guidance for diffusion models", "publication_date": "2023", "reason": "This paper is highly relevant to the current work as it introduces a method for universal guidance in diffusion models, which is a crucial aspect of controlling video motion."}, {"fullname_first_author": "Guo, Y.", "paper_title": "Animatediff: Animate your personalized text-to-image diffusion models without specific tuning", "publication_date": "2023", "reason": "This paper presents Animatediff, a text-to-video diffusion model that is directly relevant to the current work and is used in some of the experiments."}, {"fullname_first_author": "Ho, J.", "paper_title": "Video diffusion models", "publication_date": "2022", "reason": "This paper is foundational to the field of video diffusion models, which are at the core of the research."}, {"fullname_first_author": "Tang, L.", "paper_title": "Emergent correspondence from image diffusion", "publication_date": "2024", "reason": "This paper introduces a method for extracting semantic correspondence from image diffusion features, which is highly relevant to the current work's analysis of motion features."}, {"fullname_first_author": "Dhariwal, P.", "paper_title": "Diffusion models beat gans on image synthesis", "publication_date": "2021", "reason": "This is a highly influential work in the field of diffusion models, which provides the fundamental basis for much of the work on video generation."}]}