[{"figure_path": "OU1uqd1vyw/tables/tables_7_1.jpg", "caption": "Table 1: Data selection performance in federated setting on MedicalQA. We bold the highest performance and underline the second highest performance for each row.", "description": "This table presents the results of data selection methods in a federated learning setting for the Medical QA task.  It compares the performance of several methods (Oracle, Perplexity, IFD, DataInf, and CLUES) across two metrics: GPT-4 Scoring and Knowledge Avg, using two different pre-trained models (Mistral-7b and Llama2-7b). The \"Mix-qual Data\" row shows performance on mixed-quality data, while the \"Oracle\" row provides an upper bound representing ideal performance with only high-quality data.  The numbers in parentheses indicate percentage improvement of CLUES over the Mix-qual Data.", "section": "4.1 Experimental Setup"}, {"figure_path": "OU1uqd1vyw/tables/tables_7_2.jpg", "caption": "Table 2: Data selection performance on MMedBench. We bold the highest performance and underline the second highest performance for each row.", "description": "This table presents the results of data selection methods on the MMedBench dataset, comparing the performance of different methods against an oracle (training only on high-quality data) in both federated and model merging settings. The metrics used are GPT-4 scoring and knowledge average, reflecting performance improvements achieved by selecting high-quality data using the proposed CLUES method.", "section": "4.1 Experimental Setup"}, {"figure_path": "OU1uqd1vyw/tables/tables_9_1.jpg", "caption": "Table 3: Data selection performance on FiQA. We bold the highest performance for each row.", "description": "This table presents the performance of three different data selection methods on the FiQA dataset.  The methods are: selecting data by a fixed ratio, selecting data based on a predetermined score, and the authors' proposed method which uses a global threshold determined by anchor data. The evaluation metrics used are Precision, Recall, F1-Score, and Accuracy.  The results demonstrate that the authors' method significantly outperforms the other methods in terms of all four metrics.", "section": "4.2 Main Results"}, {"figure_path": "OU1uqd1vyw/tables/tables_14_1.jpg", "caption": "Table 5: Preliminary results on MMedBench.", "description": "This table presents the preliminary results of the proposed CLUES method on the MMedBench dataset. It compares the performance of two pre-trained models (Mistral-7b and Llama2-7b) under two conditions: using the original raw data and using data after applying the CLUES data selection method. The performance metrics used are GPT-4 Scoring and KnowledgeAvg. The table shows that the CLUES method significantly improves the performance for both models, especially for the GPT-4 scoring metric.", "section": "4.2 Main Results"}, {"figure_path": "OU1uqd1vyw/tables/tables_17_1.jpg", "caption": "Table 1: Data selection performance in federated setting on MedicalQA. We bold the highest performance and underline the second highest performance for each row.", "description": "This table presents the results of data selection methods in a federated learning setting for the MedicalQA task. It compares several methods (Oracle, PPL, IFD, DataInf, and the proposed CLUES method) in terms of their performance on two metrics: GPT-4 scoring and knowledge average.  The best performing method for each metric is bolded, while the second-best is underlined. This helps to illustrate the effectiveness of the CLUES method for selecting high-quality data in this collaborative setting.", "section": "4.2 Main Results"}, {"figure_path": "OU1uqd1vyw/tables/tables_17_2.jpg", "caption": "Table 8: Samples of the output of merged models on FiQA dataset.", "description": "This table shows examples of questions and the model's responses with and without the proposed data selection method.  The responses highlight the improvements in accuracy and completeness after applying the data selection technique on the Financial Question Answering (FiQA) dataset. ", "section": "4.2 Main Results"}]