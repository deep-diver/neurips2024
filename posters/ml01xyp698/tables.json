[{"figure_path": "ml01XyP698/tables/tables_6_1.jpg", "caption": "Table 1: Generalizable Novel View Interpolation results on ScanNet [18]. FreeSplat-fv is trained with our FVT strategy, and the other methods are all trained on specific number of views to form a complete comparison. Time(s) indicates the total time of encoding input images and rendering one image.", "description": "This table compares the performance of FreeSplat with other generalizable novel view synthesis methods on the ScanNet dataset for novel view interpolation tasks.  It shows the PSNR, SSIM, LPIPS, rendering time, and number of Gaussian primitives (#GS) for both 2-view and 3-view settings.  FreeSplat-spec represents FreeSplat trained on a specific number of views, while FreeSplat-fv uses the Free-View Training (FVT) strategy.", "section": "5.2 Results on ScanNet"}, {"figure_path": "ml01XyP698/tables/tables_6_2.jpg", "caption": "Table 2: Long Sequence (10 views) Explicit Reconstruction results on ScanNet. The results of pixelSplat, MVSplat and FreeSplat-spec are given using their 3-views version.", "description": "This table presents the results of reconstructing 3D scenes from long sequences (10 views) of images using different methods, including pixelSplat, MVSplat, and FreeSplat.  It compares the performance in terms of time taken for processing, the number of Gaussian primitives used, and the quality of the reconstructed views in terms of PSNR, SSIM, and LPIPS, both for view interpolation and extrapolation. The results for pixelSplat and MVSplat are based on their 3-view versions for fair comparison.", "section": "5.2 Results on ScanNet"}, {"figure_path": "ml01XyP698/tables/tables_6_3.jpg", "caption": "Table 3: Novel View Depth Rendering results on ScanNet. \u2020: 10-views results of pixelSplat, MVSplat and FreeSplat-spec are given using their 3-views version.", "description": "This table presents a comparison of novel view depth rendering results on the ScanNet dataset for different methods (NeuRay, pixelSplat, MVSplat, FreeSplat-spec, and FreeSplat-fv) and varying numbers of input views (2, 3, and 10).  The metrics used to evaluate performance are Absolute Difference (Abs Diff), Absolute Relative Difference (Abs Rel), and the percentage of points with depth error less than 1.25 (\u03b4 < 1.25). The results show FreeSplat-fv consistently outperforms other methods in depth accuracy, particularly with 10 views.", "section": "5.2 Results on ScanNet"}, {"figure_path": "ml01XyP698/tables/tables_8_1.jpg", "caption": "Table 1: Generalizable Novel View Interpolation results on ScanNet [18]. FreeSplat-fv is trained with our FVT strategy, and the other methods are all trained on specific number of views to form a complete comparison. Time(s) indicates the total time of encoding input images and rendering one image.", "description": "This table compares the performance of FreeSplat with other generalizable novel view synthesis methods on the ScanNet dataset.  The comparison is done for scenarios with 2 and 3 input views.  Metrics include PSNR, SSIM, LPIPS, rendering time, and the number of Gaussians used.  FreeSplat-fv denotes the version trained with the Free-View Training strategy, showing its ability to generalize across different numbers of input views.", "section": "5.2 Results on ScanNet"}, {"figure_path": "ml01XyP698/tables/tables_13_1.jpg", "caption": "Table 6: Comparison on computational cost and whole scene reconstruction (30 input views). We report the required GPU for Train / Test, the Encoding Time, the rendering FPS, and PSNR of novel views. - denotes that we are not able to run pixelSplat inference using 30 input views due to its increasing GPU requirement.", "description": "This table compares the computational cost and performance of different methods for whole scene reconstruction using 30 input views. It shows the GPU memory required for training and testing, encoding time, rendering FPS (frames per second), and PSNR (peak signal-to-noise ratio) for novel views. Note that pixelSplat could not be run with 30 views due to high GPU memory requirements.", "section": "A.3 Additional Experiments"}, {"figure_path": "ml01XyP698/tables/tables_13_2.jpg", "caption": "Table 7: Results on RE10K and ACID with 2 input views. We train our model on RE10K, and report its results on RE10K and ACID. * denotes that our model is trained on our previously downloaded 9,266 scenes instead of 11,075 scenes used by baselines.", "description": "This table presents a comparison of the performance of different methods on the RE10K and ACID datasets using only 2 input views.  The PSNR, SSIM, and LPIPS metrics are used to evaluate the quality of novel view synthesis. The asterisk (*) indicates that the authors' model was trained on a slightly smaller subset of the RE10K dataset (9,266 scenes) compared to the baselines (11,075 scenes).", "section": "5.2 Results on ScanNet"}]