[{"type": "text", "text": "HGDL: Heterogeneous Graph Label Distribution Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yufei $\\mathbf{Jin}^{\\dagger}$ , Heng Lian\u22c4, Yi $\\mathbf{H}\\mathbf{e}^{\\diamond}$ , Xingquan $\\mathbf{Z}\\mathbf{h}\\mathbf{u}^{\\dag}*$ ", "page_idx": 0}, {"type": "text", "text": "\u2020Dept. of Elec. Eng. & Computer Sci., Florida Atlantic University, Boca Raton, FL 33431, USA \u22c4Dept. of Data Science, William & Mary, Williamsburg, VA 23185, USA yjin2021@fau.edu; hlian01@wm.edu; yihe@wm.edu; xzhu3@fau.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Label Distribution Learning (LDL) has been extensively studied in IID data applications such as computer vision, thanks to its more generic setting over single-label and multi-label classification. This paper advances LDL into graph domains and aims to tackle a novel and fundamental heterogeneous graph label distribution learning (HGDL) problem. We argue that the graph heterogeneity reflected on node types, node attributes, and neighborhood structures can impose significant challenges for generalizing LDL onto graphs. To address the challenges, we propose a new learning framework with two key components: 1) proactive graph topology homogenization, and 2) topology and content consistency-aware graph transformer. Specifically, the former learns optimal information aggregation between meta-paths, so that the node heterogeneity can be proactively addressed prior to the succeeding embedding learning; the latter leverages an attention mechanism to learn consistency between meta-path and node attributes, allowing network topology and nodal attributes to be equally emphasized during the label distribution learning. By using KL-divergence and additional constraints, HGDL delivers an end-to-end solution for learning and predicting label distribution for nodes. Both theoretical and empirical studies substantiate the effectiveness of our HGDL approach. Our code and datasets are available at https://github.com/Listener-Watcher/HGDL. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Definite supervision signals are often postulated in learning settings [3, 4]; yet, data generated from the real world tend to present inherent ambiguity, imposing challenges on assertive classifiers that predict instances into single or multiple classes. Label Distribution Learning (LDL) [5, 6, 7, 8, 9] has emerged to navigate label ambiguity by pursuing a mapping from instances to their class distributions. Each distribution quantifies the descriptive degrees of various classes given a specific instance. ", "page_idx": 0}, {"type": "text", "text": "However, the existing LDL studies mainly [10, 6, 7, 11] focus on independent and identically distributed (IID) data, such as images or texts, which do not generalize well on graphs. In fact, the topological structure underlying instances may provide invaluable information for label distribution learning. For example, in the task of urban planning, recent learning models have been employed to predict the point of interests (POIs) of local regions [12, 13, 14, 15]. LDL can further extend this task by providing the regional distributions over all POIs, which lends a finer-granular delineation of urban regional functionality instead of single- or multi-class classification. To wit, for a region that mixes four POIs (classes): housing, healthcare, education and worship, unlike other models assertively classify it into one or multiple POI(s), LDL model can provide insights of the functional degrees of all four POIs in this region, as shown in Figure 1. Nevertheless, existing LDL studies overlook the urban topology, which can be rendered from, e.g., the taxi services across regions [2], missing out critical city traffic patterns that are highly correlated with regional functionalities. For instance, the regions with balanced POI distributions (e.g., $\\mathbf{R}_{2}$ and ${\\bf R}_{3}$ ) are less likely to form connections with other nodes compared to regions heavily skewed towards a single class (e.g., $\\mathrm{{R}_{4}}^{\\prime}$ ), as their residents enjoy fewer needs to travel to other regions for services such as education and healthcare. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this paper, we aim to enable and generalize the label distribution learning paradigm in networked data. Two technical challenges confront our study. First, real-world graphs are mostly heterogeneous, comprising diverse types of nodes for better expressiveness. Graph heterogeneity complicates the message-passing between nodes of a specific type (e.g., residence), as the label distributions of those nodes are influenced by their neighboring nodes that may vary in terms of types, content, and topological features. Simply leveraging node embeddings generated from message-passing for LDL will thus not work well [16, 17]. To aid, although meta-path aggregation [18] is seemingly viable, it necessitates extensive domain knowledge and expertise to craft meta-paths for each node type with respect to their label distributions; given the combinatorial number of possible meta-paths in large heterogeneous graphs, searching for the optimal meta-path for LDL is costly, laborious, and time-demanding. ", "page_idx": 1}, {"type": "text", "text": "Second, graph topology and nodal features may suggest inconsistent label distributions, where nodes sharing similar contents are positioned far apart on the graph topology. The inconsistency is furthered in heterogeneous graphs, where nodes of the same type often connect through other intermediary types, resulting in substantial topological distances between them. Unlike traditional LDL that focuses on instance vectors only, an effective LDL model on graphs require harmonizing nodal contents with topological structures for a unified representation. The impact of distantly positioned nodes within a graph is substantially diminished, consequently steering the LDL model to prioritize individual nodal vectors, leading to compromised node representations in which the informative patterns embedded in their neighborhood structures are overlooked. Such patterns, which may significantly enhance label distribution predictions as illustrated in Figure 1, are neglected, undermining the LDL model effectiveness. ", "page_idx": 1}, {"type": "image", "img_path": "OwguhIAh8R/tmp/468356c9055eda496673ed9dc8a4cc5d941d6d627f860c87f36205934bc057f6.jpg", "img_caption": ["Figure 1: Motivating example of HGDL study, where each node is a local urban region [1] and edges represent taxi services [2] commuting between regions. Heterogeneous node types indicate disparate land use, including residence (R), service (S), leisure (L), and transit (T), among which R nodes are of our interest. Colored R nodes are with ground truth, which delineate their distributions over multiple point-of-interests (POIs), and each POI is deemed as a class/label. Our HGDL problem is to predict the label distribution of uncolored R nodes, enabling a precise delineation of regional urban functionality. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To overcome the challenges, we propose a new learning framework, coined Heterogeneous Graph label Distribution Learning (HGDL). Specifically, to tame the graph heterogeneity, HGDL learns the optimal graph topology of the target nodes from multiple homogeneous structures searched with various meta-path schemes through a tailored attention mechanism. The node embeddings are then generated by harmonizing the nodal features and the learned meta-path graph using a transformer architecture. A joint optimization objective is crafted based on the distance between true and predicted label distributions of the target nodes from their resultant embeddings, which unifies the learning of meta-path graph topology and the feature-topology harmonization function in an end-to-end fashion. ", "page_idx": 1}, {"type": "text", "text": "A key innovation of HGDL is that it changes existing heterogeneous graph learning paradigm from reactive (meaning that aggregation of different meta-paths are done after embedding learning from individual meta-path), to be proactive (meaning that aggregation are done before embedding learning). Combined with attention and transformer mechanisms to adjust individual meta-paths\u2019 interplay, and align with nodal features, HGDL deliver significantly better performance over alternatives. Our theoretical analysis assures that HGDL outperforms that of using an arbitrary meta-path graph, and HGDL\u2019s topology and feature consistency learning sparsifies network connectivity, intermediately encouraging tightens the error-bound, resulting in better model generalization. ", "page_idx": 1}, {"type": "text", "text": "1. This study pioneers the exploration of LDL problem in heterogeneous graphs. The learning problem enjoys practical implications such as for urban functionality delineation (presented in Sec 6.1) and, to our knowledge, has not yet been explored by any contemporary research.   \n2. We propose an end-to-end HGDL learning approach to jointly learn an optimal meta-path graph topology and align it with nodal features for consistent message-passing. Our approach is surprisingly simple yet effective, with its performance evidenced by theoretical underpinnings. Our approach and its analysis are presented in Sections 4 and 5, respectively.   \n3. Empirical study has been carried out over five graph datasets that span domains of biomedicine, scholarly network, business network, and urban planning. Experimental results substantiate the effectiveness of our approach over rival models, documented in Section 6. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Label Distribution Learning (LDL) strives to learn a mapping from input to a distribution that proflies the descriptive degrees of classes associated with it [5, 10, 6, 7, 11]. Existing LDL methods fall into three categories, namely, problem transformation (PT), algorithm adaption (AA), and specialized algorithm (SA). PT methods transform LDL as multiple single-label learning tasks, using with label probabilities [19], and AA approaches revise mainstream learning algorithm to fti the LDL loss. SA algorithms are most commonly used because LDL learning is driven by new algorithm designs. Label correlation has been found to benefti the label distribution learning, where approaches were proposed to encode label correlation to a distance to measure the similarity of any two labels [6]. Later, low-rank approximation is used to construct label correlation matrix to capture the global label correlations [7] Instead of exploring common features for all labels, label-specific features [10] for each label are used to enhance the LDL model. Exploring feature-label and label-label correlation [9] has recently been studied in generalizable label distribution learning for cross domain learning. A Gaussian label distribution learning method [11] employs a parametric model underpinned by an optimization strategy assessing KL-divergence distance between Gaussian distributions, followed by a regression loss to normalize the KL-divergence distance. Noticing the difficulty to obtain ground-truth label distributions, Label Enhancement [20] is commonly used to recover label distributions from logical labels. Our research further push LDL to be generalized onto heterogeneous graphs, which have been overlooked by existing research. Although a recent study [21] explored using LDL in topological spaces, it focused on homogeneous graphs only and cannot work in the setting of more than one node type. Thus, the studied problem in [21] and its challenges differ from ours. ", "page_idx": 2}, {"type": "text", "text": "Heterogeneous Graph Neural Networks have drawn extensive attention in graph learning [16, 17, 22, 18, 23, 24], because the graph heterogeneity imposes considerable challenge to model the interplay among various node types, features, labels, and network topology. Using meta-path to aggregate information from different types of nodes/edges is a common approach for heterogeneous graph learning. HetGNN [17, 18] designs graph neural networks to encode features for each type of neighbors and then aggregates neighbors\u2019 representation with respect to different types. This provides a way for GNN to deal with heterogeneous graph structures and node attributes. HAN [25] introduces attention mechanisms to heterogeneous graph learning, where attentions are applied to embedding features learned from homogeneous networks, each created from a meta-path. By doing so, attentions serve as a weighting mechanism automatically determining the importance of each meta-path for learning. Using transformers for heterogeneous networks has also been investigated recently. For example. HGT [26] designs node- and edge-type dependent parameters to characterize the heterogeneous attention over each edge, allowing this method to learn representations for different types of nodes and edges. SeHGNN [27] proposes a transformer based semantic fusion module, allowing feature fusion from different meta-paths. Our research is fundamentally different from existing work in two aspects: 1) we study LDL learning for heterogeneous networks, and 2) we propose a new way to aggregate and align information for heterogeneous network. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notations. A heterogeneous graph is denoted by $G=\\{V,E,X,Y\\}$ associated with a node type mapping $\\phi:\\,V\\mapsto\\tau^{v}$ and an edge type mapping $\\varphi:\\,E\\mapsto\\mathcal{T}^{e}$ , with $\\mathcal{T}^{v}$ and $\\mathcal{T}^{e}$ the predefined and finite sets of nodes and edges, respectively, and $|\\mathcal{T}^{v}|\\ge2$ . Denote $t_{\\iota}\\in\\mathcal{T}^{v}$ as the node type of our interest, and suppose in total $n$ nodes are of this type. Without loss of generality, we have $\\phi(v_{1})\\,=\\,.\\ldots\\,=\\,\\phi(v_{n})\\,=\\,t_{\\iota}$ , and $V_{t_{\\iota}}\\,=\\,\\{v_{1},\\ldots,v_{n}\\}\\,\\subset\\,V$ . We deem these $n$ nodes as our target nodes, using a feature matrix $X\\in\\mathbb{R}^{n\\times m}$ to describe their nodal contents, where each node contains ", "page_idx": 2}, {"type": "image", "img_path": "OwguhIAh8R/tmp/b886a0760b9ae473a087af276d9bf62cf3b4a956a20be5562ff78ffae72f73ef.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: The proposed HGDL framework. Using $k$ meta-paths, the heterogeneous network in $\\textcircled{\\scriptsize{1}}$ is converted to $k$ homogeneous meta-path graphs in $\\circleddash$ . Topology homogenization in $\\circledast$ proactively aggregates all $k$ meta-path graphs, through learnable weight matrix $W_{0}^{i}\\in\\mathbb{R}^{n\\times f}$ for each graph, and finally obtain attention $\\Theta\\in\\mathbb{R}^{n\\times k}$ across all graphs. Topology and feature consistency-aware graph transformer in $\\textcircled{4}$ harmonizes the local and global consistencies. The objective function in $\\circleddash$ unifies loss and regularization terms to guide nodal label distribution learning. ", "page_idx": 3}, {"type": "text", "text": "an $m$ -dimensional feature vector. A meta-path $\\mathcal{P}$ is defined as a relational sequence in form of $t_{1}\\stackrel{r_{1}}{\\rightarrow}t_{2}\\dots\\stackrel{r_{i}}{\\rightarrow}t_{i}\\stackrel{r_{i+1}}{\\rightarrow}t_{i+1}\\dots\\rightarrow t_{l}$ (abbreviated as $\\mathcal{P}=(t_{1}t_{2}\\ldots t_{i}t_{i+1}\\ldots t_{l})$ , where $(t_{i}t_{i+1})\\in\\mathscr{T}^{e}$ describes the composite relation between a pair of node types. By defining a meta-path $\\mathcal{P}$ with same first and last node type as the target node type, i.e. $t_{1}=t_{l}=t_{\\iota}$ , we can use $\\mathcal{P}$ to convert a heterogeneous network as a meta-path graph concerning only the target node type, which shall be discussed later in Section 4.1. ", "page_idx": 3}, {"type": "text", "text": "Problem Statement. In our HGDL problem, the goal is to learn a predictive mapping $\\hbar\\colon(G,X)\\mapsto$ $Y$ , where $Y\\in[0,1]^{q}$ is a distribution of descriptive labels over $q$ classes. Let $y_{i,j}\\in[0,1]$ be the probability that the node $v_{i}$ belongs to the $j$ -th class, we have $\\textstyle\\sum_{j=1}^{q}y_{i,j}=1$ . In this work, we follow a transductive learning regime [28] to allow the ground-truth label distributions known for a subset of target nodes $V_{t r}\\subset V_{t_{\\iota}}$ during training. Our learned mapping $\\hbar$ is expected to generalize well so can make accurate prediction on the remaining target nodes $V_{t_{\\iota}}\\setminus V_{t r}$ . ", "page_idx": 3}, {"type": "text", "text": "4 HGDL: The Proposed Approach ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Overview. The proposed HGDL approach comprises three key components, as illustrated in Figure 2. First, for the target nodes belonging to the node type $t_{\\iota}$ of interest, HGDL generates multiple homogeneous meta-path graphs based on their original locations on the heterogeneous graph through meta-paths; the optimal graph topology of this node type is then learned from the homogeneous graphs via attention mechanism $(S e c\\,4.l)$ . Second, HGDL learns the embeddings of the target nodes by harmonizing the information sourced from their feature space and the learned optimal topology using a transformer-like neural architecture (Sec 4.2). Third, HGDL minimizes the distance between the predicted and ground-truth label distributions based on the learned node embeddings. We tailor an objective function to unify the three components into one end-to-end optimization problem, in which the optimal graph topology, the harmonization function of the feature and topological information, and the target node label distribution are jointly learned (Sec 4.3). ", "page_idx": 3}, {"type": "text", "text": "4.1 Optimal Graph Topology Homogenization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "For a heterogenous graph, by leveraging meta-path idea, multiple different meta-path homogeneous adjacency matrix can be obtained and they can be treated as multiple sources. Graph learning is about exchanging and updating information from neighbor nodes. A proper neighbor set is therefore important for a target node to learn correct distribution. Given multiple sources, each node will have multiple neighbor sets to choose from for updating. Traditionally, embeddings are learned for all the neighbor sets, and then aggregation over embeddings is learned. Semantics over embeddings are hard to interpret and learn compared with directly learned from different neighbor sets. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "To generate a meta-path graph from the original heterogeneous graph, interactions between the metapath and the heterogeneous graph path are used. Two nodes $v_{i}$ and $v_{j}$ are connected in the meta-path graph, if there exists a path connecting them in the heterogeneous graph, and the path follows the metapath. Given a meta path $\\mathcal{P}=\\left(t_{1}\\ldots t_{i}t_{i+1}\\ldots t_{l}\\right)$ , we say that a path $p=(v_{1},\\ldots,v_{i},v_{i+1},\\ldots,v_{l})$ in graph $G$ follows the meta-path $\\mathcal{P}$ , if $\\forall i$ , $\\phi(v_{i})=t_{i}$ . Take graph in Fig. 1 as an example. Given meta-path $\\mathcal{P}=(r\\ s\\ r)$ , which defines node type $t_{1}=r,t_{2}=s,t_{3}=r$ . Path $\\boldsymbol{p}=\\left(r_{2},s_{1},r_{3}\\right)$ in the heterogeneous graph follows $\\mathcal{P}$ because all nodes in the path $p$ satisfy $\\phi(r_{2})=t_{1}=r,\\phi(s_{1})=t_{2}=$ $s,\\phi(\\bar{r_{3}})=t_{3}\\bar{=}\\;r$ . Because path $\\boldsymbol{p}=\\left(r_{2},s_{1},r_{3}\\right)$ follows the meta-path $\\mathcal{P}$ , an edge is used to connect $r_{2}$ and $r_{3}$ in the homogeneous meta-path graph constructed from $\\mathcal{P}$ . Indeed, each meta-path defines a specific way of information propagation in a heterogeneous network, with resulted meta-path graph capturing unique relationships between target nodes. While defining a single meta-path is relatively easy, there often exists many meta-paths; aggregating a variety of meta-path graphs to support the downstream learning task is non-trivial. ", "page_idx": 4}, {"type": "text", "text": "After searching the meta-paths connecting the nodes of target type $t_{\\iota}$ , we generate a set of graphs $\\mathcal{A}=\\{A_{1},\\ldots\\bar{A}_{k}\\}$ , in which each adjacency $A_{i}\\in\\{0,1\\}^{n\\times n}$ captures the topological structure of the $i$ -th meta-path-based homogeneous graph. Denoted by $A_{i}[p,q]=1$ means that two target nodes $v_{p}$ and $v_{q}$ , with $\\phi(v_{p})=\\phi(v_{q})\\,\\,=t_{\\iota}$ , are connected by a meta-path; otherwise, $A_{i}[p,q]={\\bar{0}}$ . Unlike existing studies [25] that yield target node embeddings through reactive meta-path aggregation, where they aggregate local neighborhood information for each $A_{i}\\in A$ to capture $k$ separate meta-path topologies, our HGDL learns the optimal graph topology from $\\boldsymbol{\\mathcal{A}}$ in a proactive fashion. Intuitively, HGDL learns node-level attention scores for various homogeneous graphs $A_{i}$ , to respect the fact that the neighboring nodes may pass messages with varying importance levels in local neighborhoods, while the meta-paths walking across nodes of types other than the target $t_{\\iota}$ . Revisit the motivating example demonstrated in Fig 1 where the residence nodes are deemed as the target, the meta-path linking through the service nodes dominates, as the residence nodes are more likely to be linked through service nodes instead of Transit and Leisure nodes. Specifically, the attention scores for the nodes in every $A_{i}\\in{\\mathcal{A}}$ are learned in a GAT regime [29], defined as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Theta=\\operatorname{softmax}\\Big(\\big\\|_{A_{i}\\in\\mathcal{A}}\\big\\{\\operatorname{LapNorm}(A_{i})W_{0}^{i}\\big\\}W_{0}^{'}\\Big),\\quad\\tilde{A}=\\sum_{i=1}^{k}\\Theta[;i]\\cdot\\operatorname{LapNorm}(A_{i}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\operatorname{LapNorm}(A_{i})=D_{i}^{-{\\frac{1}{2}}}(A_{i}+I)D_{i}^{-{\\frac{1}{2}}}\\in\\mathbb{R}^{n\\times n}$ denotes Laplacian normalization of $A_{i}$ , with $D_{i}$ being $A_{i}$ \u2019s degree matrix and $I$ is an identity matrix. This term mitigates the imbalanced degree distribution of the meta-path graphs. Denoted by $W_{0}^{i}$ and $W_{0}^{'}$ are the learnable GAT parameters, where $W_{0}^{i}\\in\\mathbb{R}^{n\\times f}$ maps the meta-path topology of $A_{i}$ onto an $f$ -dimensional semantic space. The operator $\\|A_{i}\\!\\in\\!A\\{\\cdot\\}$ concatenates all $k$ resultant node embedding matrices from meta-path graphs $\\boldsymbol{\\mathcal{A}}$ , thereby producing an $\\mathbb{R}^{n\\times k\\cdot f}$ lookup matrix, where each node is associated with a $k{\\cdot}f$ -dimensional embedding representation, and each latent $f$ -dimension captures the local neighborhood structure of this node. Then, $W_{0}^{'}\\in\\mathbb{R}^{k\\cdot f\\times k}$ summarize the $f$ -dimensional latent space into one coefficient through convex combination, resulting in attention logits, which are fed into softmax $\\begin{array}{r}{\\mathrm{(\\cdot)}=\\exp(\\cdot)/\\sum_{i}\\exp(\\cdot)}\\end{array}$ to yield the attention matrix $\\Theta=[0,1]^{n\\times k}$ . Take the $i^{\\th}$ -th column vector of $\\Theta$ , denoted b y $\\Theta[;,i]\\in$ $[0,1]^{n}$ , we have the attention scores of $n$ target nodes for message-passing in the $i$ -th meta-path graph $A_{i}$ . We thus can deem $\\tilde{A}$ in Eq. (1) as the learned optimal graph topology, which is an element-wise linear combination of $k$ meta-path topologies with the attention scores broadcast onto all $n$ nodes. Note, $\\tilde{A}$ is asymmetric, namely $\\tilde{A}[p,\\bar{q}]\\ne\\tilde{A}[q,p]$ , $\\forall p\\neq q$ . This is because that the attention score of information aggregation from node $v_{p}$ to $v_{q}$ may be different from that from $v_{q}$ to $v_{p}$ , as their respective local neighborhood topologies naturally differ. ", "page_idx": 4}, {"type": "text", "text": "4.2 Local Topology and Global Feature Consistency-Aware Graph Transformer ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "After obtaining the optimal topology $\\tilde{A}$ from all meta-path graphs $A_{i}$ , the next question is how to harmonize it with the feature information to better the target node embeddings. The benefit of such harmonization is evident. Revisiting the urban network in Fig 1, we can envision that a pair of residence nodes tend to be associated with similar embedding vectors because they enjoy two types of consistencies: i) local neighborhood topology: their residents tend to travel to similar functional regions for leisure or service purposes, and ii) global feature space: they share similar contents such as house types and number of residing families. These local and global consistencies complement with each other, as the residence nodes having similar contents can be topologically faraway from each other on the urban network, and vice versa. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "To harmonize the local and global consistencies, we are inspired by the recent graph transformers [30] and observe that the feature attention suggests a global adjacency matrix, which can be incorporated into the message-passing process. We define the graph transformer block as follows. ", "page_idx": 5}, {"type": "equation", "text": "$$\nZ=\\mathrm{ReLU}(\\tilde{A}X W_{1}),\\quad\\tilde{A}_{2}=\\mathrm{LapNorm}\\Big(\\mathrm{softmax}\\big((Z Q)(Z K)^{\\top}\\odot\\tilde{A}\\big)\\Big),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $Z\\,\\in\\,\\mathbb{R}^{n\\times h}$ is the node embeddings learned from the optimal graph topology $\\tilde{A}$ through local information aggregation, parameterized by $W_{1}\\in\\mathbb{R}^{m\\times h}$ . Denoted by $\\bar{A}_{2}^{\\bar{\\mathbf{\\alpha}}}\\in[\\bar{0_{,}}1]^{n\\times n}$ is the normalized feature attention adjacency, where $Q$ and $K\\in\\mathbb{R}^{h\\times h}$ map the embedding matrix $Z$ onto the latent query and key spaces, respectively, such that $(Z Q)(Z K)^{\\top}$ calculates an $n\\times n$ node-level attention matrix with respect to the feature space information. Instead of normalizing the attention score by the hidden dimension $h$ , we penalize the feature attention adjacency through an element-wise production $\\odot$ with the optimal meta-path topology A\u02dc. The intuition behind Eq. (2) is that, for each target node, it aggregates information from those neighboring nodes only if their meta-path topology and feature space are both with high attention scores. In addition, Eq. (2) functions similarly to the edge dropout [31]; in lieu of randomly removing edges, we enforce a neighbor-set intersection, where the information is only propagated from the neighbors on which the feature space and meta-path topology both agree. Such an intersection sparsity thus lowers the degree of the resultant attention adjacency, thereby uplifting the learning efficacy, which will be substantiated later in Sec 5. Finally, denoted by $H=\\mathrm{LeakyReLU}(\\tilde{A}_{2}Z W_{2})\\in\\mathbb{R}^{n\\times h}$ are the resultant node embeddings, capturing both local topology and global feature consistencies, which is parameterized by weight $W_{2}\\in\\mathbb{R}^{h\\times h}$ . ", "page_idx": 5}, {"type": "text", "text": "4.3 An End-to-End HGDL Objective Function ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Based on the resultant target node embeddings $H$ , we can predict their label distributions as $\\hat{Y}=$ softmax $(\\tilde{A}_{2}H W_{3})\\in[0,1]^{n\\times q}$ , where $\\hat{Y}_{i}=\\{\\hat{y}_{i,j}\\}_{j=1}^{q}$ is the predicted label distribution of node $v_{i}$ , among which $\\hat{y}_{i,j}$ denotes its predicted probability of belonging to the class $j$ . The unified objective of our HGDL framework is defined as follows. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\operatorname*{min}_{\\{W_{0}^{i}\\}_{i=1}^{k},W_{0}^{\\prime},W_{1},W_{2},W_{3},K,Q}\\ell_{\\mathtt{H G D L}}=D_{\\mathtt{K L}}(Y\\|\\hat{Y})-\\gamma\\cdot\\Omega,}&{}\\\\ {D_{\\mathtt{K L}}(Y\\|\\hat{Y})=\\displaystyle\\sum_{i=1}^{n}\\sum_{j=1}^{q}y_{i,j}\\cdot\\log\\frac{y_{i,j}}{\\hat{y}_{i,j}},\\quad\\Omega=\\displaystyle\\sum_{i=1}^{n}D_{\\mathtt{K L}}(\\Theta[i,:]\\parallel U[1,k]),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the KL-divergence $D_{\\mathrm{KL}}(Y\\Vert\\hat{Y})$ gauges the discrepancy between the predicted and groundtruth label distributions of the target nodes [32]. The regularization term $\\Omega$ gauges the distance between the attention scores of the $i$ -th node across $k$ meta-path typologies (denoted by $\\Theta[i,:])$ and a uniform distribution $U[1,k]$ . We note the minus sign before $\\Omega$ , thus minimizing this term encourages a larger KL-distance, thereby avoiding the trivial uniform attention distribution (meaning that for each node, the learned attention weights from different meta-paths are encouraged to be as different as possible). $\\gamma$ is a tuned parameter to balance the two terms. ", "page_idx": 5}, {"type": "text", "text": "5 Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We follow the PAC-Bayes regime to analyze the theoretical performance of our HGDL algorithm by deriving its generalization error bound. We proceed analysis based on the meta-path graph adjacency matrices $\\dot{\\boldsymbol{A}}=\\{\\boldsymbol{A}_{1},\\dots\\boldsymbol{A}_{k}\\}$ , which are searched from the heterogeneous graph $G$ . Throughout the analysis, we assume the nodal feature representations to be residing in an $\\ell_{2}$ -ball of radius $B$ . We argue this a mild assumption, because in implementation we can leverage the batch-norm layers to normalize the resultant node embeddings, such that $\\|\\mathbf{h}_{i}^{j}\\|_{2}\\le B$ , where $\\mathbf{h}_{i}^{j}$ denotes the $i$ -th node\u2019s embedding resulted from the $j$ -th hidden layer. ", "page_idx": 5}, {"type": "text", "text": "Let $L_{\\mathcal{G}}(\\hbar)$ and $L_{(X,\\tilde{A})}(\\hbar)$ denote the generalization risk over a graph distribution $\\mathcal{G}$ and the empirical $r i s k$ on the target node samples and the learned meta-path topology $(X,{\\tilde{A}})$ , respectively, where $(X,{\\tilde{A}})\\in G\\ {\\overset{\\mathrm{iid}}{\\sim}}\\ g$ . We can define: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{\\mathcal{G}}(\\hbar)=\\mathbb{E}_{(X,\\tilde{A})\\sim\\mathcal{G}}\\mathbb{E}_{\\boldsymbol{y}_{i}\\sim\\boldsymbol{Y}}\\left[\\ell\\big(\\hbar(X,\\tilde{A})[i],\\boldsymbol{y}_{i}\\big)\\right],}\\\\ &{L_{(X,\\tilde{A})}(\\hbar)=\\frac{1}{n}\\sum_{i=1}^{n}\\left[\\ell\\big(\\hbar(X,\\tilde{A})[i],\\boldsymbol{y}_{i}\\big)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\ell(\\cdot,\\cdot)$ is a convex distance metric between two distributions that follows $|\\ell(u,p)-\\ell(u,q)|\\leq$ $(\\sqrt{p}+1)\\|p-q\\|_{2},\\,\\forall u,p,q\\in\\mathbb{R}^{m}$ . Denoted by $\\hbar(X,\\tilde{A})[i]\\in\\mathbb{R}^{q}$ and $y_{i}\\in[0,1]^{q}$ the predicted and ground-truth label distribution of the $i$ -th target node, respectively. Implementing KL-divergence, we have $\\begin{array}{r}{\\ell\\bigl(\\hbar(X,\\tilde{A})[i],y_{i}\\bigr)=\\sum_{j=1}^{q}\\hbar(X,\\tilde{A})[i,\\tilde{j}]\\ln(\\hbar(X,\\tilde{A})[i,j]/\\dot{y}_{i,j})}\\end{array}$ , where the predicted probability that node $i$ belongs to the $j$ -th class is denoted by $\\hbar(X,{\\tilde{A}})[i,j]$ . By analyzing the performance of the learned meta-path graph topology A\u02dc, we find that: ", "page_idx": 6}, {"type": "text", "text": "Theorem 1. Let $\\mathbb{E}[L_{(X,A_{i})}(\\hbar)]$ be the empirical risk of using the $i$ -th meta-path graph $A_{i}\\in{\\mathcal{A}}$ for label distribution prediction. With the SGD step-size $\\eta_{:}$ , we have ", "page_idx": 6}, {"type": "equation", "text": "$$\nL_{(X,\\tilde{A})}(\\hbar)\\leq\\operatorname*{min}_{A_{i}\\in\\mathcal{A}}\\mathbb{E}[L_{(X,A_{i})}(\\hbar)]+\\frac{\\ln k}{\\eta n}+\\frac{\\eta}{8}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Remark 1. Theorem 1 indicates that the empirical risk of HGDL is no larger than the minimum empirical risk incurred by training label distribution learner on the optimal meta-path graph, as the error bound on the RHS reduces to ${\\mathcal{O}}(1/n)$ with constant $k$ and $\\eta$ . With Stochastic Gradient Descent (SGD) optimizer, larger number of target nodes $n$ will lead to more training updates over them, diminishing the ${\\mathcal{O}}(1/n)$ bound faster. This finding substantiates the tightness of our meta-path learning strategy for the optimal graph topology $\\tilde{A}$ . ", "page_idx": 6}, {"type": "text", "text": "Due to page limits, we defer the proof of Theorem 1 and the rest analysis to the Supplement. We then analyze the generalization error bound of HGDL and find that: ", "page_idx": 6}, {"type": "text", "text": "Theorem 2. Let $\\hbar\\in\\mathcal{H}:\\mathcal{X}\\times\\mathcal{G}\\to\\mathbb{R}^{q}$ be an $l$ -layer message-passing neural network with maximum hidden dimension $k$ , of which the $i$ -th layer is parameterized by $W_{i}$ . Then for any $\\delta,\\gamma,B>0$ and $l>1$ , with probability at least $1-\\delta$ we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\cal L}_{\\mathcal{G}}\\left(\\hbar\\right)-{\\cal L}_{\\left(X,\\tilde{A}\\right)}\\left(\\hbar\\right)\\leq\\frac{2\\left(\\sqrt{2q}+\\sqrt{2}\\right)q}{\\sqrt{n}}\\operatorname*{max}_{i\\in\\left[n\\right],j\\in\\left[l\\right]}{\\left\\Vert\\mathbf{h}_{i}^{j}\\right\\Vert_{2}}}\\\\ &{+\\,3b\\sqrt{\\frac{\\log2/\\delta}{2n}}+\\mathcal{O}\\left(\\sqrt{\\frac{B^{2}d^{l-1}l^{2}k\\log\\left(l k\\right)\\mathcal{D}\\left(W_{i}\\right)+\\log\\frac{n l}{\\delta}}{\\gamma^{2}n}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathcal{D}(W_{i})=\\prod_{i=1}^{l}\\|W_{i}\\|_{2}^{2}\\cdot\\sum_{i=1}^{l}\\left(\\|W_{i}\\|_{F}^{2}/\\|W_{i}\\|_{2}^{2}\\right)}\\end{array}$ bounds the hypothesis space and $b$ is $a$ constant. ", "page_idx": 6}, {"type": "text", "text": "Remark 2. We remark several key observations from Theorems 1 and 2. First, the generalization capability of the algorithm is negatively impacted by a higher dimensional label space $q$ . Second, the robustness of HGDL decreases with larger $B$ values, which gauges the magnitude of perturbations thus the inherent high data variance. Third, as the graph neural network architecture becomes deeper (larger $l$ ) or wider (larger $k$ ), the generalization risk increases, suggesting the potential risk of model overfitting. Forth, with larger $n$ , the generalization error bound diminishes, which indicates that the meta-path topology can be better delineated with an increased number of target nodes on the graph. ", "page_idx": 6}, {"type": "text", "text": "Remark 3. By combining Theorems 1 and 2, we observe that the generalization error bound of HGDL using A\u02dc outperforms that of using an arbitrary meta-path graph $A_{i}$ . Further, it is easy to verify that the maximum degree of A\u02dc, denoted by $d$ , is smaller than that of $A_{i}$ , denoted by $d_{i}$ , i.e., $d\\leq d_{i}.\\,\\forall i\\in[k]$ . This rationalizes our graph transformer design in $\\sec4.2$ , where the enforced topology and feature consistency in Eq. (2) sparsifies network connectivity, thereby intermediately encourages better model generalization. ", "page_idx": 6}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "6.1 Experiment Setup ", "text_level": 1, "page_idx": 7}, {"type": "table", "img_path": "OwguhIAh8R/tmp/0bd74af21a74abecd181f61882b87996a60f69e8f39fdd627791634f3783eaaa.jpg", "table_caption": ["Table 1: Summary of dataset statistics. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Benchmark Datasets To our best   \nknowledge, no heterogeneous graph dataset with ground-true label distributions currently exists. To level the comparison study, we prepare five datasets with ground-truth node label distributions using existing heterogeneous graphs, including DBLP [33], ACM [33], YELP, DRUG [34], and URBAN [1]. Table 1 summarizes the data statistics. A detailed description on the dataset creation and preprocessing, as well as their domain and label semantic meanings, has been deferred to the Supplement B due to space limitation. ", "page_idx": 7}, {"type": "text", "text": "Compared Models In total six competitors are identified for comparative study. As no model directly resolving the HGDL problem exists, we employ the state-of-the-art heterogeneous graph neural networks and integrate them KL-divergence loss to learn label distributions of nodes. They include: 1) $\\mathrm{GCN}_{\\mathrm{KL}}$ : A baseline that uses graph constructed from each meta-path to train a vanilla GCN [35], using KL-divergence as loss function, and reports the best meta-path result; 2) $\\mathrm{{HAN}_{K L}}$ : This baseline uses HAN [25] to integrate embedding from different meta-paths; and 3) $\\underline{{\\mathrm{SeHGNN}_{\\mathrm{KL}}}}$ : This baseline uses SeHGNN [27], a transformer based approach, to aggregate meta-paths embedding with KLdivergence loss function. For ablation study, we further include three variants reduced from our proposed HGDL method, which include: 4) $\\mathrm{HGDL}_{\\mathrm{\\negTH}}$ : it removes HGDL\u2019s topology homogenization (Sec 4.1), which learns embedding from each meta-path graph and reports the best meta-path result; 5) HGDL\u00actransformer: it uses GCN instead of the transformer (Sec 4.2) to learn embedding to validate HGDL\u2019s transformer for embedding learning; and 6) $\\mathrm{{HGDLED}}$ : it replaces HGDL\u2019s topology and feature consistence-aware graph transformer (Sec. 4.2) by using a random edge dropout method [31]. ", "page_idx": 7}, {"type": "text", "text": "Evaluation Metrics To measure the discrepancy between two distributions, i.e., the predicted and true label distributions of target nodes, we identify six metrics: Cosine Distance (COD), Canberra Distance (CAD), Chebyshev Distance (CHD), Clark Distance (CLD); Intersection Score (IND), and Kullback-Leibler Divergence (KL). Their definitions and calculations are deferred to Supplement B. ", "page_idx": 7}, {"type": "table", "img_path": "OwguhIAh8R/tmp/c3c13495e9d1b8e36e7b9a2a4ee0aee471f7859767aee4de68abb394f3bdfa68.jpg", "table_caption": [], "table_footnote": ["Table 2: Mean $\\pm$ standard deviation results of seven models on five datasets. Best results are bold, and $\\uparrow$ (or $\\downarrow$ ) indicates the higher (or lower) the better. Results are taken from 5 repeats. The win/tie/loss counts are suggested by the paired $t$ -test at $90\\%$ confidence level. "], "page_idx": 7}, {"type": "image", "img_path": "OwguhIAh8R/tmp/f8d56ecd66bdf5b164d9a89612f8f7ebf125fdbb20241cae7ced34119fff91d9.jpg", "img_caption": ["Figure 3: Comparisons between HGDL vs. results from a single meta-path (CAD and CLD are calculated in natural log for better visualization) for five datasets. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "OwguhIAh8R/tmp/158e2fc8b57dc563bb3cbc0786778127f929dd66a5e64c443d73a540fa735c6b.jpg", "img_caption": ["Figure $4\\!\\!:\\!\\mathrm{~KL~}$ and CLD tradeoff function example. The estimated probability distribution is $[x_{1},x_{2}$ , 0.9] and true probability distribution is [0.05,0.05,0.9], with $x_{1}+x_{2}=0.1$ . Horizontal axis is the $x_{1}$ value and vertical axis is the loss for both CLD and KL divergence. Green dashed lines cover the tradeoff region where the CLD loss monotonically increases and KL-divergence decreases. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "6.2 Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Table 2 summarizes results of all methods. Overall, our HGDL wins in 99 out of 180 settings, among which on average 20 out of 30 settings excel in COD, CHD, IND, and KL metrics, 11 out of 30 in CAD, and 8 out of 30 in CLD. On DURG, ACM, DBLP, and URBAN datasets, HGDL outperforms its competitors in $83\\%$ settings in COD, CHD, IND, and KL metrics $46\\%$ in CAD, and $20\\%$ in CLD. Beyond its overall better comparative performance, we make the following observation on HGDL. First, $\\mathrm{HAN}_{\\mathrm{KL}}$ and $\\mathrm{SeHGNN_{KL}}$ achieve better performance on YELP and DBLP dataset for CAD and CLD metrics, but not on the other datasets and metrics. This shows that existing meta-path based methods cannot learn distribution prediction well. In general, these models show similar performance on YELP dataset. We hypothesize that this is due to the lack of rich feature information on YELP, of which the dimension of nodal features is 19 which is minimum across all datasets. Second, HGDL achieves the best results in KL-divergence by a large margin across all settings. On average, HGDL have a $15\\%$ improvement compared with the second best result across all datasets in KL-divergence. Given that KL-divergence is the loss objective in our framework, we extrapolate that HGDL converges well in terms of minimizing the distribution distance. Same observation can be drawn from the validation loss curve as shown in Supplement C Figures 5, 6, and 7. In addition, on metrics being strongly related to KL divergence including COD, CHD, and IND, our HGDL also enjoys significant performance improvement over other models. Among the metrics, CLD metrics shows a different patterns in terms of KL divergence, we show in Figure 4 that CLD and KL has a tradeoff region in small probability distribution and therefore caused such difference. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Third, the ablation study between HGDL and its variants, i.e., HGDL\u00actransformer, $\\mathrm{HGDL}_{\\mathrm{\\negTH}}$ and HGDLED, demonstrate clear benefits of topology homogenization and consistency-aware graph transformer in aggregating meta-paths and nodal features for LDL learning for heterogeneous graphs (more results are deferred to the Section E.1 in Supplement C; there, we observe that $\\mathrm{{HGDL}_{\\mathrm{{ed}}}}$ has no improvement in KL-divergence with different edge drop rates compared to HGDL\u00actransformer, which is the model with 0 edge drop rate). We observe in Table 2 that HGDL\u00actransformer shows comparable performance on ACM and YELP by tying HGDL in five and six metrics, respectively; however, HGDL outperforms it in all settings in other three datasets. Likewise, $\\mathrm{HGDL_{\\negTH}}$ ties HGDL across all metrics in YELP but is inferior to HGDL in all settings in other four datasets. $\\mathrm{{HGDLED}}$ ties HGDL in six and five settings on YELP and URBAN, respectively, but is outperformed by HGDL for all other three datasets in all settings. The robust performance of HGDL can be attributed to two aspects. On the one hand, the improved results over those ablation variants suggest that our devised model components for proactive meta-path learning and attention modeling are indispensable. On other other hand, it substantiates the usefulness of our design that lets HGDL learn semantic fusion before the embedding learning. This end-to-end learning design provides a larger search space for embedding learning to find optimal solutions, whereas other methods that learn embedding and perform fusion in two independent stages may result in suboptimal node embeddings thus inferior LDL performance. ", "page_idx": 9}, {"type": "text", "text": "Fourth, even though the optimal meta-path choice may vary across different metrics and datasets, our HGDL that proactively learns to aggregate multiple meta-path graphs leads to the best performance in most cases. Figure 3 illustrates the performance from single meta-path graph and we can observe that our method outperforms the single best path results in all five datasets, with a larger improvement when the meta-path results are close (indicating each meta-path has similar information, e.g., ACM dataset in Figure 3 (b)) and a smaller improvement when one meta-path is significantly inferior to others (e.g., DBLP dataset Figure 3 (c) where $p_{1}$ outperforms $p_{2}$ with a large margin). These results validate the tightness of Theorem 1 by demonstrating the optimality of the learned meta-path graph in our HGDL method. ", "page_idx": 9}, {"type": "text", "text": "6.3 Scalability Analysis ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Denote the total number of nodes, hidden dimension size, and number of meta-path by $n,\\,f$ , and $k$ , respectively. The number of learnable parameters is $O(n)$ for graph topology homogenization, because HGDL requires learning an adjacency matrix from all meta-path, which involves $\\bar{k}n f+k^{2}f$ training parameters (i.e. $\\mathcal{O}(n)$ complexity). Inducing adjacency matrix from features, i.e. the 2nd stage, only requires ${\\mathcal O}(1)$ number of learnable parameters, same as vanilla GCN. As a result, HGDL has $\\mathcal{O}(n)$ complexity. The runtime performance is detailed in Appendix H.3. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper explored a novel graph learning setting, namely, heterogeneous graph label distribution learning. Our goal is to predict label distributions of target nodes in a heterogeneous graph, which enables a finer-granular delineation of node properties compared to traditional single- or multiclass node classification. We demonstrated that the topological heterogeneity and inconsistency impose unique challenge for generalizing LDL into networked data, and proposed HGDL to overcome them. Specifically, HGDL proactively aggregates meta-paths to achieve optimal graph topology homogenization through attention mechanism, followed by a transformer-based approach to ensure topology and feature consistency for learning node label distributions. We analyzed the PAC-Bayes error bound of HGDL, and the result suggests the superiority of our design over those models learned from a single meta-path graph. Empirical results on five benchmark datasets validated the tightness of our analysis and substantiate that HGDL significantly outperformed its competitors. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgment ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work has been supported in part by the National Science Foundation (NSF) under Grant Nos.   \nIIS-2236578, IIS-2236579, IIS-2302786, IIS-2441449, IOS-2430224, and IOS-2446522. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] N. L. Houssou, J.-l. Guillaume, and A. Prigent, \u201cA graph based approach for functional urban areas delineation,\u201d in Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing, pp. 652\u2013658, 2019.   \n[2] A. Rossi, G. Barlacchi, M. Bianchini, and B. Lepri, \u201cModelling taxi drivers\u2019 behaviour for the next destination prediction,\u201d IEEE Transactions on Intelligent Transportation Systems, vol. 21, no. 7, pp. 2980\u20132989, 2019.   \n[3] P. Kar and P. Jain, \u201cSupervised learning with similarity functions,\u201d NeurIPS, vol. 25, 2012.   \n[4] A. Hefny, C. Downey, and G. J. Gordon, \u201cSupervised learning for dynamical system learning,\u201d NeurIPS, vol. 28, 2015.   \n[5] X. Geng, \u201cLabel distribution learning,\u201d IEEE Trans. Knowl. Data Eng., vol. 28, no. 7, pp. 1734\u2013 1748, 2016.   \n[6] X. Jia, W. Li, J. Liu, et al., \u201cLabel distribution learning by exploiting label correlations,\u201d in AAAI, 2018.   \n[7] T. Ren, X. Jia, W. Li, and S. Zhao, \u201cLabel distribution learning with label correlations via low-rank approximation,\u201d in IJCAI, p. 3325\u20133331, 2019.   \n[8] J. Wang and X. Geng, \u201cTheoretical analysis of label distribution learning,\u201d in AAAI, vol. 33, pp. 5256\u20135263, 2019.   \n[9] X. Zhao, L. Qi, Y. An, and X. Geng, \u201cGeneralizable label distribution learning,\u201d in Proceedings of the 31st ACM International Conference on Multimedia (MM-23), p. 8932\u20138941, 2023.   \n[10] T. Ren, X. Jia, W. Li, L. Chen, and Z. Li, \u201cLabel distribution learning with label-specific features,\u201d in Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI-19), pp. 3318\u20133324, International Joint Conferences on Artificial Intelligence Organization, 2019.   \n[11] H. Xu, X. Liu, Q. Zhao, Y. Ma, C. Yan, and F. Dai, \u201cGaussian label distribution learning for spherical image object detection,\u201d in CVPR, 2023.   \n[12] Z. Yao, Y. Fu, B. Liu, W. Hu, and H. Xiong, \u201cRepresenting urban functions through zone embedding with human mobility patterns,\u201d in IJCAI, 2018.   \n[13] Y. Luo, F.-l. Chung, and K. Chen, \u201cUrban region profiling via multi-graph representation learning,\u201d in CIKM, pp. 4294\u20134298, 2022.   \n[14] Y. Zheng, Y. Lin, L. Zhao, T. Wu, D. Jin, and Y. Li, \u201cSpatial planning of urban communities via deep reinforcement learning,\u201d Nature Computational Science, vol. 3, no. 9, pp. 748\u2013762, 2023.   \n[15] Y. Liu, J. Ding, Y. Fu, and Y. Li, \u201cUrbankg: An urban knowledge graph system,\u201d ACM Transactions on Intelligent Systems and Technology, vol. 14, no. 4, pp. 1\u201325, 2023.   \n[16] Q. Lv, M. Ding, Q. Liu, Y. Chen, W. Feng, S. He, C. Zhou, J. Jiang, Y. Dong, and J. Tang, \u201cAre we really making much progress? revisiting, benchmarking and refining heterogeneous graph neural networks,\u201d in SIGKDD, pp. 1150\u20131160, 2021.   \n[17] C. Zhang, D. Song, C. Huang, A. Swami, and N. Chawla, \u201cHeterogeneous graph neural network,\u201d in Proc. of KDD, pp. 793\u2013803, 2019.   \n[18] X. Fu, J. Zhang, Z. Meng, and I. King, \u201cMagnn: Metapath aggregated graph neural network for heterogeneous graph embedding,\u201d in WWW, pp. 2331\u20132341, 2020.   \n[19] H. Borchani, G. Varando, C. Bielza, and P. Larranaga, \u201cA survey on multi-output regression,\u201d Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, vol. 5, 07 2015.   \n[20] Y. Wang, Y. Zhou, J. Zhu, X. Liu, W. Yan, and Z. Tian, \u201cContrastive label enhancement,\u201d in Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence (IJCAI-23), pp. 4353\u20134361, 2023.   \n[21] Y. Jin, R. Gao, Y. He, and X. Zhu, \u201cGldl: Graph label distribution learning,\u201d in Proceedings of the 38th Annual AAAI Conference on Artificial Intelligence, 2024.   \n[22] Y. Dong, Z. Hu, K. Wang, Y. Sun, and J. Tang, \u201cHeterogeneous network representation learning,\u201d in Proc. of the 29th International Joint Conf. on Artificial Intelligence (IJCAI-20), pp. 4861\u2013 4867, 7 2020.   \n[23] Y. Jing, Y. Yang, X. Wang, M. Song, and D. Tao, \u201cAmalgamating knowledge from heterogeneous graph neural networks,\u201d in CVPR, pp. 15709\u201315718, 2021.   \n[24] X. Wang, N. Liu, H. Han, and C. Shi, \u201cSelf-supervised heterogeneous graph neural network with co-contrastive learning,\u201d in SIGKDD, pp. 1726\u20131736, 2021.   \n[25] W. Xiao, J. Houye, S. Chuan, W. Bai, C. Peng, Y. P., and Y. Yanfang, \u201cHeterogeneous graph attention network,\u201d WWW, 2019.   \n[26] Z. Hu, Y. Dong, K. Wang, and Y. Sun, \u201cHeterogeneous graph transformer,\u201d in Proceedings of The Web Conference, p. 2704\u20132710, 2020.   \n[27] X. Yang, M. Yan, S. Pan, X. Ye, and D. Fan, \u201cSimple and efficient heterogeneous graph neural network,\u201d Proceedings of the AAAI Conference on Artificial Intelligence, pp. 10816\u201310824, 2023.   \n[28] M. Wan, Y. Ouyang, L. Kaplan, and J. Han, \u201cGraph regularized meta-path based transductive regression in heterogeneous information network,\u201d in SDM, pp. 918\u2013926, 2015.   \n[29] P. Veli\u02c7ckovi\u00b4c, G. Cucurull, A. Casanova, A. Romero, P. Li\u00f2, and Y. Bengio, \u201cGraph attention networks,\u201d in ICLR, 2018.   \n[30] V. P. Dwivedi and X. Bresson, \u201cA generalization of transformer networks to graphs,\u201d 2021.   \n[31] Y. Rong, W. Huang, T. Xu, and J. Huang, \u201cDropedge: Towards deep graph convolutional networks on node classification,\u201d in International Conference on Learning Representations (ICLR), 2020.   \n[32] S. Kullback and R. A. Leibler, \u201cOn information and sufficiency,\u201d The annals of mathematical statistics, vol. 22, no. 1, pp. 79\u201386, 1951.   \n[33] J. Tang, J. Zhang, L. Yao, J. Li, L. Zhang, and Z. Su, \u201cArnetminer: Extraction and mining of academic social networks,\u201d in KDD, pp. 990\u2013998, 2008.   \n[34] Y. Gu, S. Zheng, Q. Yin, R. Jiang, and J. Li, \u201cREDDA: Integrating multiple biological relations to heterogeneous graph neural network for drug-disease association prediction,\u201d Computers in Biology and Medicine, 11 2022.   \n[35] T. N. Kipf and M. Welling, \u201cSemi-supervised classification with graph convolutional networks,\u201d in ICLR, 2017.   \n[36] G. Landrum, P. Tosco, B. Kelley, Ric, D. Cosgrove, sriniker, gedeck, R. Vianello, NadineSchneider, E. Kawashima, G. Jones, D. N, A. Dalke, B. Cole, M. Swain, S. Turk, AlexanderSavelyev, A. Vaucher, M. W\u00f3jcikowski, I. Take, V. F. Scalfani, D. Probst, K. Ujihara, guillaume godin, A. Pahl, R. Walker, J. Lehtivarjo, F. Berenger, jasondbiggs, and strets123, \u201crdkit/rdkit: 2023_09_4 (q3 2023) release,\u201d Jan. 2024.   \n[37] D. S. Himmelstein, \u201cUser-friendly extensions to mesh v1.0,\u201d Feb. 2016.   \n[38] V. D. Blondel, J.-L. Guillaume, R. Lambiotte, and E. Lefebvre, \u201cFast unfolding of communities in large networks,\u201d Journal of statistical mechanics: theory and experiment, vol. 2008, no. 10, p. P10008, 2008.   \n[39] N. Cesa-Bianchi and G. Lugosi, Prediction, learning, and games. Cambridge university press, 2006.   \n[40] P. L. Bartlett, D. J. Foster, and M. J. Telgarsky, \u201cSpectrally-normalized margin bounds for neural networks,\u201d NeurIPS, vol. 30, 2017.   \n[41] B. Neyshabur, S. Bhojanapalli, and N. Srebro, \u201cA pac-bayesian approach to spectrallynormalized margin bounds for neural networks,\u201d in ICLR, 2018.   \n[42] R. Liao, R. Urtasun, and R. Zemel, \u201cA pac-bayesian approach to generalization bounds for graph neural networks,\u201d in ICLR, 2020.   \n[43] J. A. Tropp, \u201cUser-friendly tail bounds for sums of random matrices,\u201d Foundations of computational mathematics, vol. 12, pp. 389\u2013434, 2012.   \n[44] P. L. Bartlett and S. Mendelson, \u201cRademacher and gaussian complexities: Risk bounds and structural results,\u201d Journal of Machine Learning Research, vol. 3, no. Nov, pp. 463\u2013482, 2002.   \n[45] R. E. Schapire and Y. Freund, Foundations of machine learning, pp. 23\u201352. Mit Press, 2012.   \n[46] A. Maurer, \u201cA vector-contraction inequality for rademacher complexities,\u201d in ALT, pp. 3\u201317, Springer, 2016.   \n[47] S. M. Kakade, K. Sridharan, and A. Tewari, \u201cOn the complexity of linear prediction: Risk bounds, margin bounds, and regularization,\u201d NeurIPS, vol. 21, 2008.   \n[48] S.-H. Cha, \u201cComprehensive survey on distance/similarity measures between probability density functions,\u201d City, vol. 1, no. 2, p. 1, 2007. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Algorithm 1 HGDL Algorithm   \nInput: $G=\\{V,E,X,Y\\}$ ; Meta-paths: $\\mathbb{P}=\\{\\mathcal{P}_{1},\\ldots,\\mathcal{P}_{k}\\}$   \nParameter: epochs, $U[1,k]$ is the uniform distribution, $\\gamma$ is a constant   \nOutput: $\\hat{Y}$   \n1: $A\\leftarrow\\emptyset$   \n2: For <each meta-path $\\mathcal{P}_{i}\\in\\mathbb{P}>$   \n3: Meta-path Graph $G_{i}\\leftarrow$ Generate Meta-path graph $(G,\\mathcal{P}_{i})$   \n4: $A_{i}\\leftarrow$ Adjacency Matrix $(G_{i})$   \n5: $A\\leftarrow A\\cup A_{i}$   \n6: EndFor   \n7: $t\\leftarrow0$ .   \n8: while $t\\ \\leq$ epochs do   \n9: $\\begin{array}{r l}&{\\Theta\\leftarrow\\mathrm{Attenition}_{W_{i}^{\\prime}}(A_{i})\\{\\mathrm{Attenition}()\\}\\mathrm{follows~Eq.~(1)}\\}}\\\\ &{\\tilde{A}\\leftarrow\\mathrm{Agrepat}_{W_{i}^{\\prime}}(\\Theta,A_{i})\\{\\mathrm{Agrepat~follows~Eq.~(2)}\\}}\\\\ &{Z\\leftarrow\\mathrm{ReLU}(\\tilde{A}X\\mathrm{1})}\\\\ &{Z_{q}\\leftarrow Z Q}\\\\ &{Z k\\leftarrow(Z K)^{\\top}}\\\\ &{\\tilde{A}_{2}\\leftarrow\\mathrm{LapNom}\\Big(\\mathrm{sottmax}((Z q_{G}Z_{k})\\odot\\tilde{A})\\Big)}\\\\ &{H\\leftarrow\\mathrm{LeabyReLU}(\\tilde{A}_{2}Z W_{2})}\\\\ &{\\tilde{Y}\\leftarrow\\mathrm{sottmax}(\\tilde{A}H W_{3})}\\\\ &{\\ell_{K L}\\leftarrow D_{K L}(\\mathrm{V})[\\tilde{V}]\\{\\}}\\\\ &{\\ell_{K P o p}\\rightarrow\\mathrm{Det}(\\mathrm{Voll})\\{\\mathrm{D}_{K}\\}}\\\\ &{G\\rightarrow\\mathrm{abst~\\mathrm{enit}\\}\\mathrm{min}(\\ell_{K L}\\vert\\}_{K L}-\\gamma,\\ell_{r e a s})}\\end{array}$   \n10:   \n11:   \n12:   \n13:   \n14:   \n15:   \n16:   \n17:   \n18:   \n19:   \n20: Update $W_{1},W_{2},W_{3},Q,K,W_{0}^{i},W_{i}^{'}$ with Gradient.   \n21: $t\\gets t+1$   \n22: end while   \n23: return $\\hat{Y}$ ", "page_idx": 13}, {"type": "text", "text": "A Appendix / supplemental material ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B Roadmap ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A structured guide to navigate the supplement content is organized as follows. ", "page_idx": 13}, {"type": "text", "text": "\u2022 Supplement A: Supplement A presents the pseudo-code of our proposed HGDL algorithm.   \n\u2022 Supplement B: Supplement B reports datasets and baseline methods used for the experimental study.   \n\u2022 Supplement C: Supplement C reports experimental settings and additional results.   \n\u2022 Supplement D: Supplement D elaborates the proof of Theorem 1 as introduced in Section 5 of the main manuscript.   \n\u2022 Supplement E: Supplement E elaborates the proof of Theorem 2 as introduced in Section 5 of the main manuscript.   \n\u2022 Supplement F: Supplement F carries out additional theoretical analysis explaining the rationality of HGDL for label distribution learning. ", "page_idx": 13}, {"type": "text", "text": "C Supplement A: Pseudo-code and Implementation Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Algorithm 1 lists the main steps of the HGDL framework where Lines 1 to 6 generate meta-path graphs and their adjacency matrices. Lines 7 to 10 denote the optimal graph topology homogenization (Sec 4.1 of the main manuscript), and Lines 11 to 15 are local topology and global feature consistencyaware graph transformer (Sec 4.2 of the main manuscript). Lines 16 to 20 denote the loss terms and the model training process. ", "page_idx": 13}, {"type": "table", "img_path": "OwguhIAh8R/tmp/2afd916865b3692886adc4f2ba763a209eac4b693a733f4a6e8aa7975b1c80cf.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "OwguhIAh8R/tmp/c4fcf4ca7e607f4c3c48dc725e664787dff3a19242b5996e8b182a340af4d2e4.jpg", "table_caption": [], "table_footnote": ["Table 4: Meta-paths and label semantics of the benchmark datasets "], "page_idx": 14}, {"type": "text", "text": "D Supplement B: Experiments and Datasets ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "D.1 Dataset Description ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Because no benchmark heterogeneous graph datasets with label distributions currently exist, we create four datasets using existing heterogeneous graphs including DBLP [33], ACM [33], YELP, and DRUG dataset [34]. Table 3 reports number of nodes/edges, number of each node/edge types, and number of labels for each dataset. Table 4 reports meta-paths and label semantic for each datasets. Number of features are refer to the content attribute/features of the target node types. In our experiments, we did not use any content information of non-target nodes (so only target nodes have features). ", "page_idx": 14}, {"type": "text", "text": "\u2022 DBLP is a citation network [33] with four types of nodes (author, paper, conference, and term nodes). Author nodes are our target nodes. To construct a label distribution, we labeled each author with a distribution of conference areas, using the history of each author\u2019s published papers at different conference areas. The author\u2019s node features are a bag of words of all papers published by the author.   \n\u2022 ACM is a citation network from [33] with five types of nodes (author, paper, conference, affliiation, and subject nodes). Author nodes are our target nodes. We first extract one third of the authors and then labeled them based on the subjects of their published papers. Similar to the DBLP dataset, each author\u2019s features are also a bag-of-word representation of the author\u2019s papers.   \n\u2022 YELP is an open-access review network with four types of nodes (business, user, tip, and review nodes). User nodes are our target nodes. We label each user by counting the ratings of the business each user made reviews on and normalize the countings of each rate to a distribution. User\u2019s features are the bag of words of their reviews. ", "page_idx": 14}, {"type": "table", "img_path": "OwguhIAh8R/tmp/278c41687e9d70159765b56ca7eea12ad00308152da1aeee1c23e33304ca02bf.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "OwguhIAh8R/tmp/301ea75098199317c778a48cedef7924152466311f39b8a93dd8c7c8828d77f1.jpg", "table_caption": ["Table 5: Examples of drug name, ID, and drug in SMILES notation (SMILES: Simplified Molecular Input Line Entry System) "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "OwguhIAh8R/tmp/69398823add97ed8fdadf59d12181d55d8ccaba7991c32698e803777a2d62256.jpg", "table_caption": ["Table 6: DRUG label semantics "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Table 7: DBLP dataset average label distributions (mean\u00b1Std) for all nodes w.r.t. different classes. To generate this average, label distributions are grouped by their dominant class. Average label distributions are then calculated within their respective groups. Standard deviations are calculated between each class within each individual group. The table is $q\\times q$ , where $q$ denotes the number of classes. The diagonal values denote the dominant class\u2019s probability value. The lower the diagonal values, the more spread out the class probability is. ", "page_idx": 15}, {"type": "text", "text": "\u2022 DRUG is a drug-disease-protein-gene network from [34], with four types of nodes (drug, disease, protein, and gene nodes). Drug nodes are our target nodes. The label distribution of each drug node is obtained by using associated disease etiology (using top-level categories from Medical Subject Headings, MeSH, disease database). The node features is ", "page_idx": 15}, {"type": "table", "img_path": "OwguhIAh8R/tmp/63e1aa27f3f0823f8a3a1f093e7ba131903c7f347b80a948bee1e9333a3c6001.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Table 8: ACM dataset average label distributions (mean $\\pm$ Std) for all nodes w.r.t. different classes. To generate this average, label distributions are grouped by their dominant class. Average label distributions are then calculated within their respective groups. Standard deviations are calculated between each class within each individual group. The table is $q\\times q$ , where $q$ denotes the number of classes. The diagonal values denote the dominant class\u2019s probability value. The lower the diagonal values, the more spread out the class probability is. Class 7 has 0 label distribution because no samples have Class 7 as the dominate class. ", "page_idx": 16}, {"type": "image", "img_path": "OwguhIAh8R/tmp/a11ad57c2f53eae37273a3b56bf8665ec858981b2e69325e01a4edb4d79e55d6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Table 9: DRUG dataset average label distributions(part 1) (mean $\\pm\\mathrm{Std})$ ) for all nodes w.r.t. different classes. To generate this average, label distributions are grouped by their dominant class. Average label distributions are then calculated within their respective groups. Standard deviations are calculated between each class within each individual group. The table is $q\\times q$ , where $q$ denotes the number of classes. The diagonal values denote the dominant class\u2019s probability value. The lower the diagonal values, the more spread out the class probability is. ", "page_idx": 16}, {"type": "image", "img_path": "OwguhIAh8R/tmp/38275fd197d80155b2e43f5ccb2035a0f567a899359c6af18f1649916401cae3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Table 10: DRUG dataset average label distributions(part 2) (mean\u00b1Std) for all nodes w.r.t. different classes. To generate this average, label distributions are grouped by their dominant class. Average label distributions are then calculated within their respective groups. Standard deviations are calculated between each class within each individual group. The table is $q\\times q$ , where $q$ denotes the number of classes. The diagonal values denote the dominant class\u2019s probability value. The lower the diagonal values, the more spread out the class probability is. Class 21, Class 25, and Class 28 has 0 label distribution because no samples have any one of these three three classes (21, 25, 28) as the dominate class ", "page_idx": 16}, {"type": "text", "text": "created from chemical compound of each drug using their SMILES notations. SMILES (Simplified Molecular Input Line Entry System) is a chemical notation that represents a chemical structure as a string so it can be used by the computer. Table 5 lists examples ", "page_idx": 16}, {"type": "table", "img_path": "OwguhIAh8R/tmp/a3f3b6e1dce75c00cd1ccc1af2785676435da71fb0179fe61965f0bedcd121a3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Table 11: YELP dataset average label distributions (mean $\\pm$ Std) for all nodes w.r.t. different classes. To generate this average, label distributions are grouped by their dominant class. Average label distributions are then calculated within their respective groups. Standard deviations are calculated between each class within each individual group. The table is $q\\times q$ , where $q$ denotes the number of classes. The diagonal values denote the dominant class\u2019s probability value. The lower the diagonal values, the more spread out the class probability is. 0 indicates no samples have specific dominate class. ", "page_idx": 17}, {"type": "table", "img_path": "OwguhIAh8R/tmp/e0bae1aa7a8b49b7cfbb6717e6a5602430be559c0767cf5cc00942e3f5bf279a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Table 12: URBAN dataset average label distributions (mean\u00b1Std) for all nodes w.r.t. different classes. To generate this average, label distributions are grouped by their dominant class. Average label distributions are then calculated within their respective groups. Standard deviations are calculated between each class within each individual group. The table is $q\\times q$ , where $q$ denotes the number of classes. The diagonal values denote the dominant class\u2019s probability value. The lower the diagonal values, the more spread out the class probability is. 0 indicates no samples have specific dominate class. ", "page_idx": 17}, {"type": "text", "text": "of three drugs\u2019 names, IDs, and their SMILES strings. Because SMILES describes drug\u2019s chemical compound, We use RDKit library [36] to convert the chemical compound to 191 measures(scalars) of that chemical compound and use it as drug node (target node) features. To obtain the label, we use the MeSH-tree parsing tool [37] to convert the disease ID to corresponding MeSH prefix code which corresponds to high level disease and obtain in total 28 high-level diseases that drugs could potentially target on. Table 6 lists the 28 labels used in DRUG dataset (including MeSH prefix codes and names). ", "page_idx": 17}, {"type": "text", "text": "\u2022 URBAN is a functional urban area network constructed based on the city of Porto, Portugal, uniformly divided into a grid of 30 by 80 blocks. A dataset comprising 22,208 points of interest (POI) is sourced from the OpenStreetMap platform 2 to generate the label distribution. This dataset includes a large amount of fundamental categories like hotels, banks and churches, which have been further categorized into 10 more general groups, as detailed in Table 4. We count the types and corresponding numbers of POIs to generate the label for each block. After excluding blocks devoid of any POIs, the remaining blocks were primarily divided into four different types of nodes using the Louvain algorithm [38]. These node types are named as Nature Residence, Comprehensive Service, Green Leisure and Transit Junction based on their main functions and nodes from Nature Residence are our target nodes. The edges are constructed by Taxi Service Trajectory 3, which documents the taxi routines (from origins to destinations) in Porto. In our resulting heterogeneous graph, two target nodes are considered neighbors if they have the same recorded destination. ", "page_idx": 17}, {"type": "text", "text": "Tables 7, 8, 9 and 10, and 11 report average label distributions for DBLP, ACM, DRUG, YELP, and URBAN dataset, respectively. To generate average label distributions, for each dataset, nodes are grouped by their dominant class (where dominant class is the class with the largest distribution value among all classes, using ground-truth distributions). Average label distributions are then calculated within respective groups. It is worth noting that some classes may not be a domain class of any node (e.g. Class 7 of the ACM dataset in Table 8, or Classes 21, 25, and 28 in the DRUG dataset in Table 10). In this case, the average label distributions of the class are reported as 0 in the tables. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "D.2 Baselines ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We extend existing heterogeneous graph learning methods to integrate KL-divergence loss, and implement three baselines for comparisons. In addition, three variants of the proposed HGDL method are compared for ablation study purposes: ", "page_idx": 18}, {"type": "text", "text": "\u2022 $\\mathrm{GCN}_{\\mathrm{KL}}$ : A baseline uses graph constructed from each meta-path to train a vanilla GCN, using KL-divergence as loss function, and reports the best meta-path result. This baseline is used to demonstrate whether the proposed HGDL can outperform the best single meta-path result.   \n\u2022 $\\mathrm{HAN}_{\\mathrm{KL}}$ : This baseline is to compare HGDL\u2019s semantic fusion component with HAN [25], a classical heterogeneous network learning semantic fusion method. $\\mathrm{HAN}_{\\mathrm{KL}}$ uses HAN to integrate embedding from meta-path graphs created from different meta-paths, and uses KL-divergence as the loss function for training.   \n\u2022 $\\mathrm{SeHGNN_{KL}}$ This baseline is to compare HGDL\u2019s semantic fusion with SeHGNN [27], a state of the art transformer based heterogeneous semantic fusion learning method. $\\mathrm{SeHGNN_{KL}}$ uses neighbor aggregation proposed by SeHGNN to aggregate meta-paths embedding, with KL-divergence being used as the loss function for training.   \n\u2022 HGDL: The proposed HGDL method.   \n\u2022 $\\mathrm{HGDL_{\\negTH}}$ : This is a variant of the HGDL for ablation study. HGDL\u00ac $\\mathrm{\\nabla{TH}}$ removes the graph topology homogenization module (Sec 4.1). The embedding is trained from each meta-path with topology and feature consistence-aware graph transformer (Sec 4.2) being applied to each meta-path. The result of the best meta-path is reported, and the purpose is to demonstrate HGDL\u2019s topology homogenization effectiveness.   \n\u2022 HGDL\u00actransformer: This is a variant of the HGDL for ablation study. HGDL\u00actransformer uses GCN, instead of the transformer (Sec 4.2), to learn embeddings. This serves as an ablation study to show the advantage of HGDL\u2019s transformer component for embedding learning.   \n\u2022 $\\mathrm{{HGDL}_{\\mathrm{{ED}}}}$ This is a variant of the HGDL for ablation study. HGDLED replaces HGDL\u2019s topology and feature consistence-aware graph transformer (Sec. 4.2) using random edge dropout method [31]. The purposes to demonstrate the advantage of consistent-aware graph transfer, which resembles to learnable edge dropping, compared to random edge dropout. ", "page_idx": 18}, {"type": "text", "text": "D.3 Experiment Setup ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "All results reported in the paper are based on 5 times average for each method. Hyperparameters of all the baselines include the hidden dimension for the embedding learning and the hidden dimension for the semantic fusion learning (if the semantic fusion part exists), and the learning rate. In practice, a 0.005 learning rate is fixed for all the baselines unless it is observed that specific method cannot converge under such learning rate or can converge much faster with a large learning rate. An early stop patience signal is set and a maximum number of epochs is fixed. In practice, we observed convergence before reaching maximum number of epochs in general. For the methods that cannot learn well with all the hyperparameter search, we report the best results we got. For the HDGLED method, the originally learned feature topology for HGDL is replaced by a 0-1 matrix generated by a Bernoulli distribution with a drop rate provided to convert to a 0-1 matrix. The drop rate for the HDGLED baseline in Table 2 in the main manuscript is 0.1. Fig 5 provides the results of different drop rate cases. For HGDL we have additional hyperparameters negative slope of Leaky Relu activation and $\\gamma$ for regularization loss. ", "page_idx": 18}, {"type": "text", "text": "The train:validation:test splits for DRUG, ACM, DBLP, YELP, and URBAN dataset are 8:1:1, 7:1:2, 4:1:5, 8:1:1, 5:3:2, respectively. We used a smaller training set $(40\\%)$ for DBLP because because this dataset has comprehensive nodal features (8,920 features) which are the-bag-of-words of papers published by authors. DBLP also has much less number of labels (4 classes), compared to other datasets. The rich nodal features, combined with less number of classes, provide rather rich information to derive research fields. So we reduce the training set size, compared to the test set, to provide less label information for learning. ", "page_idx": 18}, {"type": "text", "text": "D.4 Evaluation metric ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Given one predicted label distribution $\\hat{y}$ and its corresponding ground truth distribution $y$ , the six metrics used in the Table 2 can be defined as the following: ", "page_idx": 19}, {"type": "text", "text": "\u2022 Cosine Distance(COD): ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{COD}(y,\\hat{y})=1-\\frac{y\\cdot v}{\\Vert y\\Vert\\Vert\\hat{y}\\Vert}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "\u2022 Canberra Distance(CAD):", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{CAD}(y,\\hat{y})=\\sum_{i}\\frac{|y_{i}-\\hat{y}_{i}|}{|y_{i}|+|\\hat{y}_{i}|}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "\u2022 Chebyshev Distance(CHD): ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\mathrm{CHD}}(y,\\hat{y})=\\operatorname*{max}_{i}|y_{i}-\\hat{y}_{i}|\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "\u2022 Clark Distance (CLD): ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{CLD}(y,\\hat{y})=\\sqrt{\\sum_{i}\\frac{(y_{i}-\\hat{y}_{i})^{2}}{(y_{i}+\\hat{y}_{i})^{2}}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "\u2022 Intersection Score(IND): ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{IND}(y,\\hat{y})=\\sum_{i}\\mathrm{min}(y_{i},\\hat{y}_{i})\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "\u2022 Kullback-Leibler Divergence(KL): ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{KL}(y,\\hat{y})=\\sum_{i}y_{i}\\cdot\\log\\frac{y_{i}}{\\hat{y}_{i}},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "E Supplement C: Additional Results and Analysis ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "E.1 Edge Dropping Comparisons ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "HGDL treats the learnt feature attention as another adjacency matrix, and incorporates it into the graph adjacency matrix by a $\\odot$ operator, which provides an intersection of neighbor sets for a node, i.e., the resulting convolution layer is propagated to a neighbor set that both feature and graph topology agrees on. This operation is similar to the recently proposed drop message idea that randomly masks edges for the nodes. Instead of random masking, HGDL selects the neighbors that both feature and graph topology agree on. In other words, Eq. (2) in the main manuscript functions similarly to the edge dropout [31]; in lieu of randomly removing edges, we enforce a neighbor-set intersection, where the information is only propagated from the neighbors on which the feature space and meta-path topology both agree. ", "page_idx": 19}, {"type": "text", "text": "To compare the performance between our approach vs. the edge dropout [31], the variant HGDLED is used to replace our transformer based approach by using random edge dropout. In this case, for $\\mathrm{HDGLED}$ , the originally learned feature topology for HGDL is replaced by a 0-1 matrix generated by a Bernoulli distribution with a drop rate provided to convert to a 0-1 matrix. For comparisions, we vary the edge drop rates and report HGDLED\u2019s performance in Figure 5. The results show that that $\\mathrm{{HGDLED}}$ with random edge dropping doesn\u2019t necessarily help improve the distribution learning, implying that our method works not just by relying on the decreasing of the edge number, but also relying on choosing the proper edges to drop. ", "page_idx": 19}, {"type": "text", "text": "E.2 Validation Loss Comparisons ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Figures 6, 7, and 8 report the training process validation loss (w.r.t. KL-divergence loss) between $\\mathrm{{HAN}_{K L}}$ , $\\mathrm{SeHGNN_{KL}}$ , and HGDL. Because early stop is applied to all methods, they terminate at respective number of epochs. In our experiments, a patience score is used to determine the minimum loss. A method will continue if a smaller validation loss is found within patience number of epochs after the current smallest loss, and terminates otherwise. ", "page_idx": 19}, {"type": "table", "img_path": "OwguhIAh8R/tmp/b319663cb1d0be9a835b044a1617a10b5bdb7a785e470c21d3caf46ed9283353.jpg", "table_caption": [], "table_footnote": ["Table 13: Experimental results of HINormer and GLDL on the Drug and DBLP datasets, where HINormer is a recent heterogeneous graph learning baseline and GLDL is for LDL in homogeneous graphs. "], "page_idx": 20}, {"type": "image", "img_path": "OwguhIAh8R/tmp/5e79abd6227929c2e3ca16058f3daba53a4d4c163e049823f6e5f65c8d28c750.jpg", "img_caption": ["Figure 5 $:\\mathrm{HGDL}_{\\mathrm{ED}}$ baseline with different edge drop rates on DRUG and ACM dataset. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "OwguhIAh8R/tmp/c6ff7531b90f0e0136cc2458a7b0848cf8f8aef1b0926dc026bf4b7b82350ddd.jpg", "img_caption": ["Figure 6: Training process KL-divergence validation loss comparisons on the DRUG dataset. The $x$ -axis denotes epochs, and the $y$ -axis denotes training validation loss. Early stop is applied to all three methods, so methods terminate at different epochs. A method terminates at the minimum loss, if no smaller loss is found after continuing a patience number of epochs. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "OwguhIAh8R/tmp/13376f983567905cc01ab6bcea6b1f2942b7d0dc1774f098c1606bc6c6bb4003.jpg", "img_caption": ["Figure 7: Training process KL-divergence validation loss comparisons on the DBLP dataset. The $x$ -axis denotes epochs, and the $y$ -axis denotes training validation loss. Early stop is applied to all three methods, so methods terminate at different epochs. A method terminates at the minimum loss, if no smaller loss is found after continuing a patience number of epochs. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "OwguhIAh8R/tmp/4c9eb585aaccfc2654927468262c2912f83825b7c4ee52b046eb3b411c85e7dd.jpg", "img_caption": ["Figure 8: Training process KL-divergence validation loss comparisons on the ACM dataset. The $x$ -axis denotes epochs, and the $y$ -axis denotes training validation loss. Early stop is applied to all three methods, so methods terminate at different epochs. A method terminates at the minimum loss, if no smaller loss is found after continuing a patience number of epochs. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "OwguhIAh8R/tmp/e1ddcca3effc3b44768e23c2ed435a910c7b867530ba50669122ba9e600413b0.jpg", "img_caption": ["Figure 9: Sensitive analysis of $\\gamma$ , ranging from 0, 1e-5, 1e-4, 1e-3, 1e-2, and 0.1, converted to the values of $l o g(\\gamma)$ as -6, -5, -4, -3, -2, and $^{-1}$ along the $\\Chi$ -axis. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "The comparisons show that HGDL achieves the smallest KL-divergence loss across all datasets. For the DRUG dataset (Figure 6), HGDL converges at much better optimal point than the other two baselines. This is also evident by the results shown in the Table 2 of the main manuscript (which are validated on the test set nodes). ", "page_idx": 22}, {"type": "text", "text": "E.3 Sensitive Analysis of hyperparameter ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "To study the effect of hyperparameter $\\gamma$ on the model performance, we explore the $\\gamma$ value ranging from [0,1e-5,1e-4,1e-3,1e-2,0.1] and see how the KL divergence metric is affected by different $\\gamma$ values for our five datasets shown in Figure 9. We can observe that optimal $\\gamma$ values vary for different datasets and therefore it is important to search for a proper $\\gamma$ value for each datasets. ", "page_idx": 22}, {"type": "text", "text": "E.4 Comparison against HINormer and GLDL ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "HINormer is a recent heterogenous graph learning baseline and GLDL method is a recent LDL learning method in homogeneous graphs. To make a more comprehensive comparisons, Table 13 provides the results applied to DBLP and Drug dataset with these two additional baselines. We observe that our method is still competitive in most metrics in terms of GLDL and is more superior to HINormer method in general. This further validates the superiority of our design on heterogenous settings. ", "page_idx": 22}, {"type": "text", "text": "F Supplement D: Proof of Theorem 1 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Theorem 3. Let $\\mathbb{E}[L_{(X,A_{i})}(\\hbar)]$ be the empirical risk of using the $i$ -th meta-path graph $A_{i}\\in{\\mathcal{A}}$ for label distribution prediction. With the SGD step-size $\\eta$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\nL_{(X,\\tilde{A})}(\\hbar)\\leq\\operatorname*{min}_{A_{i}\\in\\mathcal{A}}\\mathbb{E}[L_{(X,A_{i})}(\\hbar)]+\\frac{\\ln k}{\\eta n}+\\frac{\\eta}{8}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. We define the cumulative risk $\\begin{array}{r}{L_{(X,A_{i})}(\\hbar)\\ =\\ \\sum_{i=1}^{n}\\left[\\ell\\big(\\hbar(X,A_{i})[i],y_{i}\\big)\\right]}\\end{array}$ over $n$ training iterations, and a quantitative variable $Q_{n}$ as follows: ", "page_idx": 22}, {"type": "equation", "text": "$$\nQ_{n}=\\exp(-\\eta L_{(X,A_{1})}(\\hbar))+.\\,.\\,.\\exp(-\\eta L_{(X,A_{k})}(\\hbar)),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where it is trivial to derive $Q_{1}=k$ , as no risk will be suffered if no target node attends training. By performing stochastic gradient descent (SGD) over $n$ iterations, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\ln\\displaystyle\\frac{Q_{n}}{Q_{1}}=\\ln\\displaystyle\\sum_{A_{i}\\in\\mathcal{A}}\\exp(-\\eta L_{(X,A_{i})}(\\hbar))-\\ln k}\\\\ &{\\geq\\ln\\displaystyle\\operatorname*{max}_{A_{i}\\in\\mathcal{A}}\\big\\{\\exp(-\\eta L_{(X,A_{i})}(\\hbar))\\big\\}-\\ln k}\\\\ &{=-\\,\\eta\\displaystyle\\operatorname*{min}_{A_{i}\\in\\mathcal{A}}\\big\\{L_{(X,A_{i})}(\\hbar)\\big\\}-\\ln k,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ln\\frac{Q_{n}}{Q_{n-1}}=\\ln\\frac{\\sum_{A_{i}\\in A}\\big[\\exp\\left(-\\eta\\{L_{(X,A_{i})}^{(n-1)}(\\hbar)+\\ell_{i}(\\hat{y}_{n},y_{n})\\right)\\big]}}{\\sum_{A_{i}\\in A}\\exp\\left(-\\eta L_{(X,A_{i})}^{(n-1)}(\\hbar)\\right)}}\\\\ &{\\qquad\\qquad=\\ln\\underset{A_{i}\\in A}{\\sum_{\\omega\\in A}}\\mathrm{softmax}\\big(-\\eta L_{(X,A_{i})}^{(n-1)}((\\hbar))\\cdot\\exp(-\\eta\\ell_{i}(\\hat{y}_{n},y_{n})\\big)}\\\\ &{\\qquad\\qquad\\leq\\ln\\underset{A_{i}\\in A}{\\sum_{\\omega\\in A}}\\Theta[;i]\\exp(-\\eta\\ell_{i}(\\hat{y}_{n},y_{n})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\ell_{i}(\\hat{y}_{n},y_{n})\\,=\\,\\ell(\\hbar(X,A_{i})[n],y_{n})$ denotes the immediate loss evaluated at the $n$ -th training iteration based on the $i$ -th meta-path graph topology. The attention matrix $\\Theta$ are with all entries non-negative by its definition. To proceed, we introduce the lemma by [39]. ", "page_idx": 23}, {"type": "text", "text": "Lemma 4 (Hoeffding Inequality). Let $X$ be a random variable with $a\\le X\\le b$ . Then for any $s\\in\\mathbb{R}$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\ln\\mathbb{E}\\big(e^{s X}\\big)\\leq s\\mathbb{E}X+\\frac{s^{2}(b-a)^{2}}{8}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "With normalized immediate loss $\\begin{array}{r}{\\ell_{i}(\\hat{y}_{n},y_{n})\\in[0,1],}\\end{array}$ $\\forall i$ , we can relax Eq. (12) based on the convexity of $\\ell$ and $\\hbar$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ln\\displaystyle\\frac{Q_{n}}{Q_{n-1}}\\leq-\\eta\\big[\\displaystyle\\sum_{A_{i}\\in A}\\Theta[:,i]\\ell_{i}(\\hat{y}_{n},y_{n})\\big]+\\frac{\\eta^{2}}{8}}\\\\ &{\\qquad\\qquad\\leq-\\eta\\big[\\ell(\\hbar(X,\\displaystyle\\sum_{i=1}^{k}\\Theta[:,i]A_{i})[n],y_{n})\\big]+\\frac{\\eta^{2}}{8}}\\\\ &{\\qquad\\qquad\\leq-\\eta\\big[\\ell(\\hbar(X,\\tilde{A})[n],y_{n})\\big]+\\frac{\\eta^{2}}{8}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We can observe that over $n$ training iterations: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\ln\\frac{Q_{n}}{Q_{n-1}}+\\ln\\frac{Q_{n-1}}{Q_{n-2}}+\\dots+\\ln\\frac{Q_{2}}{Q_{1}}}\\\\ &{=\\ln\\left(\\frac{Q_{n}}{Q_{n-1}}\\cdot\\frac{Q_{n-1}}{Q_{n-2}}\\cdot\\dots\\frac{Q_{2}}{Q_{1}}\\right)=\\ln\\frac{Q_{n}}{Q_{1}}}\\\\ &{\\le-\\eta\\sum_{i=1}^{n}\\ell(h(X,\\tilde{A})[i],y_{i})+\\frac{n\\cdot\\eta^{2}}{8}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Chaining Eq. (11) and Eq. (14) we arrive at: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\ell(\\hbar(X,\\tilde{A})[i],y_{i})\\leq\\operatorname*{min}_{A_{i}\\in\\mathcal{A}}\\big\\{L_{(X,A_{i})}(\\hbar)\\big\\}+\\frac{\\ln k}{\\eta}+\\frac{n\\cdot\\eta}{8},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and dividing the both sides by $n$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\nL_{(X,\\tilde{A})}(\\hbar)\\leq\\operatorname*{min}_{A_{i}\\in\\mathcal{A}}\\mathbb{E}[L_{(X,A_{i})}(\\hbar)]+\\frac{\\ln k}{\\eta n}+\\frac{\\eta}{8}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Note that $\\begin{array}{r}{\\mathbb{E}[L_{(X,A_{i})}(\\hbar)]=\\frac{1}{n}L_{(X,A_{i})}(\\hbar)}\\end{array}$ concludes the proof. ", "page_idx": 23}, {"type": "text", "text": "G Supplement E: Proof of Theorem 2 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Theorem 5. Let $\\hbar\\in\\mathcal{H}:\\mathcal{X}\\times\\mathcal{G}\\to\\mathbb{R}^{q}$ be an $l$ -layer message-passing neural network with maximum hidden dimension $k$ , of which the $i$ -th layer is parameterized by $W_{i}$ . Then for any $\\delta,\\gamma,B>0$ and ", "page_idx": 23}, {"type": "text", "text": "$l>1$ , with probability at least $1-\\delta$ we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\cal L}_{\\mathcal{G}}\\left(\\hbar\\right)-{\\cal L}_{\\left(X,\\tilde{A}\\right)}\\left(\\hbar\\right)\\leq\\frac{2\\left(\\sqrt{2q}+\\sqrt{2}\\right)q}{\\sqrt{n}}\\operatorname*{max}_{i\\in\\left[n\\right],j\\in\\left[l\\right]}{\\left\\Vert\\mathbf{h}_{i}^{j}\\right\\Vert_{2}}}\\\\ &{+\\,3b\\sqrt{\\frac{\\log2/\\delta}{2n}}+\\mathcal{O}\\left(\\sqrt{\\frac{B^{2}d^{l-1}l^{2}k\\log\\left(l k\\right)\\mathcal{D}\\left(W_{i}\\right)+\\log\\frac{n l}{\\delta}}{\\gamma^{2}n}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathcal{D}(W_{i})=\\prod_{i=1}^{l}\\|W_{i}\\|_{2}^{2}\\cdot\\sum_{i=1}^{l}\\left(\\|W_{i}\\|_{F}^{2}/\\|W_{i}\\|_{2}^{2}\\right)}\\end{array}$ bounds the hypothesis space and $b$ is $a$ constant. ", "page_idx": 24}, {"type": "text", "text": "Proof We first try to explore the boundary of a GCN without label distribution, and use the multiclass $\\gamma$ -margin loss following [40, 41]. The generalization error and the empirical error are defined as, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{\\mathcal{G}}\\left(f_{w}\\right)=\\underset{z\\sim\\mathcal{D}}{\\mathbb{P}}\\left(f_{w}(X,A)[y]\\leq\\gamma+\\underset{j\\neq y}{\\operatorname*{max}}\\;f_{w}(X,A)[j]\\right),}\\\\ &{L_{(X,A)}\\left(f_{w}\\right)=\\cfrac{1}{n}\\,\\underset{z_{i}\\in S}{\\sum}\\,\\mathbf{1}\\left(f_{w}(X,A)[y]\\leq\\gamma+\\underset{j\\neq y}{\\operatorname*{max}}\\;f_{w}(X,A)[j]\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\gamma>0$ and $f_{w}(X,A)$ is the $l$ -th layer representations, i.e., $H_{l}\\,=\\,f_{w}(X,A)$ . 1 is a all-one vector. Two necessary lemmas are derived from this. First, a margin-based generalization bound is given to guarantee that, as long as the change of the output brought by the perturbations is small with a large probability, the corresponding generalization bound [41] is defined as: ", "page_idx": 24}, {"type": "text", "text": "Lemma 6. Let $f_{w}(x):\\mathcal{X}\\to\\mathbb{R}^{K}$ be any model with parameters $w$ , and let $P$ be any distribution on the parameters that is independent of the training data. For any $w$ , we construct a posterior $Q(w+u)$ by adding any random perturbation u to $w$ , s.t., $\\begin{array}{r}{\\mathbb{P}\\left(\\underset{-}{\\operatorname*{max}}_{x\\in\\mathcal{X}}\\left|f_{w+u}(x)-f_{w}(x)\\right|_{\\infty}<\\frac{\\gamma}{4}\\right)>\\frac{1}{2}}\\end{array}$ . Then, for any $\\gamma,\\delta>0,$ , with probability at least $1-\\delta$ we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\nL_{\\mathcal{G}}\\left(f_{w}\\right)\\leq L_{\\left(X,A\\right)}\\left(f_{w}\\right)+\\sqrt{\\frac{2D_{\\mathrm{KL}}(Q(w+u)\\|P)+\\log\\frac{8n}{\\delta}}{2(n-1)}},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $n$ is the number of instances of training set. Hence, in order to apply Lemma 6 , we must ensure that the change of the output brought by the weight perturbations is small with a large probability. In the following lemma, we bound this change using the product of the spectral norms of learned weights at each layer and a term depending on some statistics of the graph [42]. ", "page_idx": 24}, {"type": "text", "text": "Lemma 7. For any $B>0,l>1_{\\cdot}$ , let $f_{w}\\,\\in\\,\\mathcal{H}\\,:\\,\\mathcal{X}\\,\\times\\,\\mathcal{G}\\,\\to\\,\\mathbb{R}^{K}$ be a $l$ -layer GCN. Then for any $w$ , and $x\\in\\mathcal{X}_{B,h_{0}}$ , and any perturbation $u=\\mathrm{vec}\\left(\\{U_{i}\\}_{i=1}^{l}\\right)$ such that $\\begin{array}{r}{\\forall i\\in\\mathbb{N}_{l}^{+},\\|U_{i}\\|_{2}\\,\\leq\\,\\frac{1}{l}\\,\\|W_{i}\\|_{2}.}\\end{array}$ , the change in the output of $G C N$ is bounded as, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left\\lvert L_{(X,A)}\\left(f_{(w+u)}\\right)-L_{(X,A)}\\left(f_{w}\\right)\\right\\rvert_{2}}\\\\ {\\displaystyle\\leq e B d^{\\frac{l-1}{2}}\\left(\\prod_{i=1}^{l}\\left\\lVert W_{i}\\right\\rVert_{2}\\right)\\sum_{k=1}^{l}\\displaystyle\\frac{\\left\\lVert U_{k}\\right\\rVert_{2}}{\\left\\lVert W_{k}\\right\\rVert_{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then, we can deduce the bound for a GCN without label distribution. First, let $\\begin{array}{r l}{\\beta}&{{}=}\\end{array}$ $\\left(\\prod_{i=1}^{l}\\Vert W_{i}\\Vert_{2}\\right)^{1/t}$ . Weights are normalized as $\\begin{array}{r}{\\tilde{W}_{i}~=~\\frac{\\beta}{\\Vert W_{i}\\Vert_{2}}W_{i}}\\end{array}$ W\u03b2i\u2225 Wi. Due to the homogeneity of ReLU, i.e., $a\\phi(x)~=~\\phi(a x),\\forall a~\\geq~0$ . $\\begin{array}{r}{\\prod_{i=1}^{l}\\left\\|{W_{i}}\\right\\|_{2}\\;=\\;\\prod_{i=1}^{l}\\left\\|{\\tilde{W}_{i}}\\right\\|_{2}}\\end{array}$ and $\\left\\Vert W_{i}\\right\\Vert_{F}/\\left\\Vert W_{i}\\right\\Vert_{2}\\,=$ $\\left\\Vert\\tilde{W}_{i}\\right\\Vert_{F}/\\left\\Vert\\tilde{W}_{i}\\right\\Vert_{2}$ , i.e., the terms present in the bound remain unchanged after normalization. Therefore, w.l.o.g., we conduct the assumption that the norm is equal across all layers, i.e., $\\forall i$ $\\left|i,\\left|\\left|W_{i}\\right|\\right|_{2}=\\beta$ . Consider the prior $P={\\mathcal{N}}\\left(0,\\sigma^{2}I\\right)$ and the random perturbation $u\\sim\\mathcal{N}\\left(0,\\sigma^{2}I\\right)$ . Note that the $\\sigma$ of the prior and the perturbation keep the same and will be set according to $\\beta$ . Specifically, the value of $\\sigma$ is set based on some approximation $\\tilde{\\beta}$ of $\\beta$ since the prior $P$ can not be directly deduced by any learned weights. We select the approximation $\\tilde{\\beta}$ as the cover set that encompasses the possible significant range of $\\beta$ . For the moment, assuming we have a constant $\\tilde{\\beta}$ , we can examine $\\beta$ that fulfills the condition $\\begin{array}{r}{|\\beta-\\tilde{\\beta}|\\leq\\frac{1}{l}\\beta}\\end{array}$ . Note that this also implies ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\beta-\\tilde{\\beta}|\\leq\\displaystyle\\frac{1}{l}\\beta,\\Rightarrow\\left(1-\\displaystyle\\frac{1}{l}\\right)\\beta\\leq\\tilde{\\beta}\\leq\\left(1+\\displaystyle\\frac{1}{l}\\right)\\beta,}\\\\ &{\\qquad\\qquad\\Rightarrow\\left(1-\\displaystyle\\frac{1}{l}\\right)^{l-1}\\beta^{l-1}\\leq\\tilde{\\beta}^{l-1}\\leq\\left(1+\\displaystyle\\frac{1}{l}\\right)^{l-1}\\beta^{l-1},}\\\\ &{\\qquad\\qquad\\Rightarrow\\left(1-\\displaystyle\\frac{1}{l}\\right)^{l}\\beta^{l-1}\\leq\\tilde{\\beta}^{l-1}\\leq\\left(1+\\displaystyle\\frac{1}{l}\\right)^{l}\\beta^{l-1},}\\\\ &{\\qquad\\qquad\\Rightarrow\\frac{1}{e}\\beta^{l-1}\\leq\\tilde{\\beta}^{l-1}\\leq e\\beta^{l-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "According to [43], for random perturbations $U_{i}\\,\\in\\,\\mathbb{R}^{h\\times h}$ which is object to the distribution $U_{i}\\sim$ $\\mathcal{N}\\left(\\mathbf{0},\\sigma^{2}\\breve{I}\\right)$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\|U_{i}\\|_{2}\\geq t\\right)\\leq2k e^{-t^{2}/2k\\sigma^{2}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We consider the perturbations of all layers, thus: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{P}\\left(\\left\\|U_{1}\\right\\|_{2}<t\\&\\cdots\\&\\left\\|U_{l}\\right\\|_{2}<t\\right)=1-\\mathbb{P}\\left(\\exists i,\\left\\|U_{i}\\right\\|_{2}\\ge t\\right),}}\\\\ &{}&{\\ge1-\\displaystyle\\sum_{i=1}^{l}\\mathbb{P}\\left(\\left\\|U_{i}\\right\\|_{2}\\ge t\\right),}\\\\ &{}&{\\ge1-2l k e^{-t^{2}/2k\\sigma^{2}}.\\ \\ \\ \\ }\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Setting $2l k e^{-t^{2}/2k\\sigma^{2}}=\\textstyle{\\frac{1}{2}}$ , we can have $t=\\sigma\\sqrt{2k\\log(4l k)}$ . ", "page_idx": 25}, {"type": "text", "text": "The likelihood that the spectral norm perturbation of any layer not exceeds $\\sigma{\\sqrt{2h\\log(4l k)}}$ is at least $\\frac{1}{2}$ . Plugging this bound into Lemma 7, we have with probability at least $\\frac{1}{2}$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|f_{w+u}(X,A)-f_{w}(X,A)\\right|_{2}\\le e B d^{\\frac{l-1}{2}}\\left(\\displaystyle\\prod_{i=1}^{l}\\|W_{i}\\|_{2}\\right)\\displaystyle\\sum_{i=1}^{l}\\frac{\\|U_{i}\\|_{2}}{\\|W_{i}\\|_{2}},}\\\\ &{\\qquad\\qquad\\qquad\\qquad=e B d^{\\frac{l-1}{2}}\\beta^{l}\\displaystyle\\sum_{i=1}^{l}\\frac{\\|U_{i}\\|_{2}}{\\beta},}\\\\ &{\\qquad\\qquad\\qquad\\le e B d^{\\frac{l-1}{2}}\\beta^{l-1}l\\sigma\\sqrt{2k\\log(4l k)},}\\\\ &{\\qquad\\qquad\\le e^{2}B d^{\\frac{l-1}{2}}\\beta^{l-1}l\\sigma\\sqrt{2k\\log(4l k)},}\\\\ &{\\qquad\\qquad\\le\\frac{\\gamma}{4},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where we can set $\\begin{array}{r}{\\sigma=\\frac{\\gamma}{4e^{2}B d^{\\frac{L-1}{2}}\\tilde{\\beta}^{l-1}l\\sqrt{k\\log(4l k)}}}\\end{array}$ to get a new inequality. Note that Lemma 7 also requires \u2200i \u2208Nl+ , \u2225Ui\u22252 \u2264 l1 \u2225Wi\u22252. The requirement is satisfied if \u03c3 \u2264l\u221a2k l\u03b2og(4lk) which in turn can be satisfied if ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{\\gamma}{4e B d^{\\frac{I-1}{2}}\\beta^{l-1}l\\sqrt{2k\\log(4l k)}}\\leq\\frac{\\beta}{l\\sqrt{2k\\log(4l k)}},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "since the chosen value of $\\sigma$ satisfies $\\begin{array}{r}{\\sigma\\leq\\frac{\\gamma}{4e B d^{\\frac{l-1}{2}}\\beta^{l-1}l\\sqrt{2k\\log(4l k)}}}\\end{array}$ ", "page_idx": 25}, {"type": "text", "text": "Note that Eq. (22) can be rewritten as $\\begin{array}{r}{\\frac{\\gamma}{4e B}d^{\\frac{1-1}{2}}\\leq\\beta^{l}}\\end{array}$ . KL term in the PAC-Bayes bound in Lemma 6 is deduced as: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}(Q\\|P)=\\frac{|w|_{2}^{2}}{2\\sigma^{2}}=\\frac{4e^{2}B^{2}d^{l-1}\\beta^{2l-2}l^{2}k\\log(4l k)}{2\\gamma^{2}}\\underset{i=1}{\\overset{l}{\\sum}}\\,\\|W_{i}\\|_{F}^{2}\\,,}\\\\ &{\\hphantom{\\quad}\\leq\\mathcal{O}\\left(\\frac{B^{2}d^{l-1}\\beta^{2l}l^{2}k\\log(l k)}{\\gamma^{2}}\\underset{i=1}{\\overset{l}{\\sum}}\\,\\frac{\\|W_{i}\\|_{F}^{2}}{\\beta^{2}}\\right),}\\\\ &{\\hphantom{\\quad}\\leq\\mathcal{O}\\left(B^{2}d^{l-1}l^{2}k\\log(l k)\\frac{\\prod_{i=1}^{l}\\|W_{i}\\|_{2}^{2}}{\\gamma^{2}}\\underset{i=1}{\\overset{l}{\\sum}}\\,\\frac{\\|W_{i}\\|_{F}^{2}}{\\|W_{i}\\|_{2}^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "From Lemma 6, fixing any $\\tilde{\\beta}$ , with probability $1-\\delta$ and for all $w$ such that $\\begin{array}{r}{|\\beta-\\tilde{\\beta}|\\leq\\frac{1}{l}\\beta}\\end{array}$ . Let $\\begin{array}{r}{\\mathcal{D}(W_{i})=\\prod_{i=1}^{l}\\|W_{i}\\|_{2}^{2}\\cdot\\sum_{i=1}^{l}\\left(\\|W_{i}\\|_{F}^{2}/\\|W_{i}\\|_{2}^{2}\\right)}\\end{array}$ we can get, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{\\mathcal{G}}\\left(f_{w}\\right)\\leq L_{\\left(X,A\\right)}\\left(f_{w}\\right)}\\\\ &{\\qquad\\qquad+\\mathcal{O}\\left(\\sqrt{\\frac{B^{2}d^{l-1}l^{2}k\\log\\left(l k\\right)\\mathcal{D}\\left(W_{i}\\right)+\\log\\frac{n}{\\delta}}{\\gamma^{2}n}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The choice of $\\tilde{\\beta}$ is important, such that for any $\\beta$ , we can bound the generalization error like Eq. (24). First, the value range of $\\beta$ is decided as the following, ", "page_idx": 26}, {"type": "equation", "text": "$$\n{\\frac{1}{\\sqrt{d}}}\\left({\\frac{\\gamma{\\sqrt{d}}}{2B}}\\right)^{1/l}\\leq\\beta\\leq{\\frac{1}{\\sqrt{d}}}\\left({\\frac{\\gamma{\\sqrt{n d}}}{2B}}\\right)^{1/l},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "since otherwise the bound holds trivially as $L_{\\left(X,A\\right)}\\left(f_{w}\\right)\\leq1$ by definition. Note that the lower bound in Eq. (25) ensures that Eq. (22) holds which in turn justifies the applicability of Lemma 7. If $\\begin{array}{r}{\\beta<\\frac{1}{\\sqrt{d}}\\left(\\frac{\\gamma\\sqrt{d}}{2B}\\right)^{1/l}}\\end{array}$ , then for any $(X,A)$ and any $j\\in\\mathbb{N}_{K}^{+},|f(X,A)[j]|\\leq\\frac{\\gamma}{2}$ . To prove this, we have, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\left\\vert f_{w}(X,A)[j]\\right\\vert\\leq\\left\\vert f_{w}(X,A)\\right\\vert_{2}=\\left\\vert\\frac{1}{n}\\mathbf{1}_{n}\\mathbf{x}^{L-1}W_{l}\\right\\vert_{2},}}\\\\ &{}&{\\leq\\frac{1}{n}\\left\\vert\\mathbf{1}_{n}\\mathbf{x}^{L-1}\\right\\vert_{2}\\left\\vert W_{l}\\right\\vert_{2},}\\\\ &{}&{\\leq\\left\\Vert W_{l}\\right\\Vert_{2}\\underset{i=1}{\\operatorname*{max}}\\left\\vert\\mathbf{x}^{L-1}[i,i]\\right\\vert_{2},}\\\\ &{}&{\\leq B d^{\\frac{L-1}{2}}\\underset{i=1}{\\overset{l}{\\prod}}\\left\\Vert W_{i}\\right\\Vert_{2}=d^{\\frac{L-1}{2}}\\beta^{l}B,}\\\\ &{}&{=d^{\\frac{L-1}{2}}B\\frac{\\gamma}{2B d^{\\frac{L-1}{2}}}\\leq\\frac{\\gamma}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "$L_{(X,A)}\\left(f_{w}\\right)\\;=\\;1$ $\\beta\\ <\\ {\\textstyle\\frac{1}{\\sqrt{d}}}\\left({\\textstyle\\frac{\\gamma\\sqrt{d}}{2B}}\\right)^{1/l}$ . Alternatively, if $\\beta\\,>$ $\\begin{array}{l}{\\displaystyle{\\frac{1}{\\sqrt{d}}\\left(\\frac{2\\sqrt{n d}}{2B}\\right)^{1/l}}}\\end{array}$ , the term inside the big-O notation in Eq. (24) would be, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sqrt{\\frac{B^{2}d^{l-1}l^{2}k\\log(l k)\\mathcal{D}(W_{i})+\\log\\frac{n}{\\delta}}{\\gamma^{2}n}}\\geq\\sqrt{\\frac{l^{2}k\\log(l k)}{4}\\sum_{i=1}^{l}\\frac{\\|W_{i}\\|_{F}^{2}}{\\|W_{i}\\|_{2}^{2}}}}}\\\\ &{}&{\\geq\\sqrt{\\frac{l^{2}k\\log(l k)}{4}}\\geq1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where we set $k\\geq2$ in practice and $l\\geq2$ , while considering the fact $\\|W_{i}\\|_{F}\\geq\\|W_{i}\\|_{2}$ . To account for $\\beta$ within the range defined by Eq. (25), a condition ensuring that $\\begin{array}{r}{|\\beta-\\tilde{\\beta}|\\,\\le\\,\\frac1l\\beta}\\end{array}$ would be $\\begin{array}{r}{|\\beta-\\bar{\\beta}|\\leq\\frac{1}{l\\sqrt{d}}\\left(\\frac{\\gamma\\sqrt{d}}{2B}\\right)^{1/l}}\\end{array}$ . So, if we can identify an overlap of the span in Eq. (25) with a radius of $\\begin{array}{r}{\\frac{1}{l\\sqrt{d}}\\left(\\frac{\\partial\\sqrt{d}}{2B}\\right)^{1/l}}\\end{array}$ and ascertain that bounds akin to Eq. (24) are met when $\\tilde{\\beta}$ assumes any value within that overlap, then a bound that is valid for every $\\beta$ can be established. Clearly, it is only essential to ponder a coverage $C$ of magnitude $\\begin{array}{r}{|C|=\\frac{l}{2}\\left(n^{\\frac{1}{2}}-1\\right)}\\end{array}$ . Thus, representing the event of Eq. (24) when $\\tilde{\\beta}$ adopts the $i$ -th value from the coverage as $E_{i}$ , we deduce: ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(E_{1}\\&\\cdot\\cdot\\cdot\\&E_{|C|}\\right)=1-\\mathbb{P}\\left(\\exists i,\\bar{E}_{i}\\right),}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geq1-\\displaystyle\\sum_{i=1}^{|C|}\\mathbb{P}\\left(\\bar{E}_{i}\\right)\\geq1-|C|\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Note $\\bar{E}_{i}$ denotes the complement of $E_{i}$ . Hence, we now present the PAC-Bayes generalization bound [42] of GCNs as Theorem 8. ", "page_idx": 27}, {"type": "text", "text": "Theorem 8. For any $B>0,l>1$ , let $f_{w}\\in\\mathcal{H}:\\mathcal{X}\\times\\mathcal{G}\\to\\mathbb{R}^{K}$ be a $l$ layer GCN. Then for any $\\delta,\\gamma>0$ , with probability at least $1-\\delta$ over the choice of an i.i.d. size-n training set $S$ according to $\\mathcal{D}$ , for any $w$ , we have, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{\\mathcal{G}}\\left(f_{w}\\right)\\leq L_{(X,A)}\\left(f_{w}\\right)}\\\\ &{\\qquad\\qquad+\\mathcal{O}\\left(\\sqrt{\\frac{B^{2}d^{l-1}l^{2}k\\log\\left(l k\\right)\\mathcal{D}\\left(W_{i}\\right)+\\log\\frac{n\\left|C\\right|}{\\delta}}{\\gamma^{2}n}}\\right),}\\\\ &{\\qquad\\qquad=L_{(X,A)}\\left(f_{w}\\right)}\\\\ &{\\qquad\\qquad+\\mathcal{O}\\left(\\sqrt{\\frac{B^{2}d^{l-1}l^{2}k\\log\\left(l k\\right)\\mathcal{D}\\left(W_{i}\\right)+\\log\\frac{n l}{\\delta}}{\\gamma^{2}n}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then we consider a GCN with label distribution learning, and explore the bound in term of Rademacher complexity. Specifically, let $\\eta(\\cdot)$ be a label distribution function, thus $\\eta\\left(\\mathbf{x}_{i}\\right)\\mathbf{\\Psi}=$ $\\left\\{\\eta_{\\mathbf{x}_{i}}^{y_{1}},\\ldots,\\eta_{\\mathbf{x}_{i}}^{y_{p}}\\right\\}$ . Given a function class $\\mathcal{H}$ and a loss function $\\ell(\\cdot)$ , for function $\\hbar\\in{\\mathcal{H}}$ , its corresponding risk and empirical risk are defined as $L_{\\mathcal{G}}\\left(f_{w}\\right)=\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}[\\ell(\\hbar(\\mathbf{x}),\\eta(\\mathbf{x}))]$ and $L_{\\left(X,A\\right)}\\left(f_{w}\\right)=$ $\\begin{array}{r}{\\frac{1}{n}\\sum_{i=1}^{n}\\ell\\left(\\hbar\\left(\\mathbf{x}_{i}\\right),\\eta\\left(\\mathbf{x}_{i}\\right)\\right)}\\\\ {\\ell,}\\end{array}$ , respectively. Recall the definition of rademacher complexity w.r.t. $S$ and ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{R}}_{n}(\\ell\\circ\\mathcal{H}\\circ S)=\\mathbb{E}_{\\epsilon_{1},\\dots,\\epsilon_{n}}\\left[\\operatorname*{sup}_{\\hbar\\in\\mathcal{H}}\\frac{1}{n}\\sum_{i=1}^{n}\\ell\\left(\\hbar\\left(\\mathbf{x}_{i}\\right),\\eta\\left(\\mathbf{x}_{i}\\right)\\right)\\epsilon_{i}\\right],\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\epsilon_{1},\\ldots,\\epsilon_{n}$ are $n$ independent rademacher random variables with $\\mathbb{P}\\left(\\epsilon_{i}=1\\right)=\\mathbb{P}\\left(\\epsilon_{i}=-1\\right)=$ $1/2$ . ", "page_idx": 27}, {"type": "text", "text": "Then we can derive the following necessary lemma [44] [45], ", "page_idx": 27}, {"type": "text", "text": "Lemma 9. Let $\\mathcal{H}$ be a family of functions. For a loss function $\\ell$ bounded by $\\mu_{\\cdot}$ , then for any $\\delta>0$ , with probability at least $1-\\delta_{i}$ , for all $\\hbar\\in{\\mathcal{H}}$ such that ", "page_idx": 27}, {"type": "equation", "text": "$$\nL_{\\mathcal{D}}(\\hbar)\\leq L_{S}(\\hbar)+2\\hat{\\mathcal{R}}_{n}(\\ell\\circ\\mathcal{H}\\circ S)+3\\mu\\sqrt{\\frac{\\log2/\\delta}{2n}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and the theorem of Rademacher complexity of the method for loss function KL-divergence [8]: ", "page_idx": 27}, {"type": "text", "text": "Theorem 10. Let $\\mathcal{H}$ be a family of functions for multi-output linear regression, and $\\mathcal{H}_{j}$ be a family of functions for the $j$ -th output. Rademacher complexity of ME(Maximum-Entropy) with $K L$ loss satisfies ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{R}}_{n}(\\mathrm{KL}\\circ\\mathrm{SF}\\circ\\mathcal{H}\\circ S)\\leq(\\sqrt{2q}+\\sqrt{2})\\sum_{j=1}^{q}\\hat{\\mathcal{R}}_{n}\\left(\\mathcal{H}_{j}\\circ S\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Note that $\\mathrm{KL}(\\mathbf{u},\\cdot)$ is not $\\rho$ -Lipschitz over $\\mathbb{R}^{m}$ for any $\\rho\\in\\mathbb R$ and $\\mathbf{u}\\in\\mathbb{R}^{m}$ . Define function $\\phi(\\cdot,\\cdot)$ as $\\mathrm{KL}(\\cdot,\\mathrm{SF}(\\cdot))$ . Next we show that $\\phi(\\mathbf{u},\\cdot)$ satisfy Lipschitzness. For $\\mathbf{p},\\mathbf{q}\\in\\mathbb{R}^{m}$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n|\\phi({\\mathbf{u}},{\\mathbf{p}})-\\phi({\\mathbf{u}},{\\mathbf{q}})|=|\\mathrm{KL}({\\mathbf{u}},\\mathrm{SF}({\\mathbf{p}}))-\\mathrm{KL}({\\mathbf{u}},\\mathrm{SF}({\\mathbf{q}}))|,\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which equals ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=1}^{p}u_{i}\\left(\\ln\\frac{\\exp{(p_{i})}}{\\sum_{j=1}^{n}\\exp{(p_{j})}}-\\ln\\frac{\\exp{(q_{i})}}{\\sum_{j=1}^{n}\\exp{(q_{j})}}\\right)\\Bigg\\rvert,}\\\\ &{\\displaystyle\\leq\\sum_{i=1}^{p}\\left\\vert\\ln\\left(1+\\sum_{j\\neq i}\\mathrm{e}^{p_{j}-p_{i}}\\right)-\\ln\\left(1+\\sum_{j\\neq i}\\mathrm{e}^{q_{j}-q_{i}}\\right)\\right\\vert u_{i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Observing that $\\textstyle\\ln\\left(1+\\sum_{j}\\exp v_{i}\\right)$ is 1-Lipschitz for $\\mathbf{v}\\in\\mathbb{R}^{m}$ , thus right-hand side of preceding equation is bounded by ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{i=1}^{p}u_{i}\\left\\|\\mathbf{p}-\\mathbf{1}\\cdot p_{i}-\\mathbf{q}+\\mathbf{1}\\cdot q_{i}\\right\\|_{2},}\\\\ {\\displaystyle\\leq\\left\\|\\mathbf{p}-\\mathbf{q}\\right\\|_{2}+\\sqrt{c}\\displaystyle\\sum_{i=1}^{p}u_{i}\\left|p_{i}-q_{i}\\right|,}\\\\ {\\displaystyle\\leq\\left(\\sqrt{p}+1\\right)\\left\\|\\mathbf{p}-\\mathbf{q}\\right\\|_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "namely, $\\phi$ is $({\\sqrt{p}}+1)$ -Lipschitz. According to [46], we have: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\mathcal{R}}_{n}(\\mathrm{KL}\\circ\\mathrm{SF}\\circ\\mathcal{H}\\circ S)\\leq\\sqrt{2}(\\sqrt{q}+1)\\displaystyle\\sum_{j=1}^{q}\\hat{\\mathcal{R}}_{n}\\left(\\mathcal{H}_{j}\\circ S\\right),\\ }\\\\ {=(\\sqrt{2q}+\\sqrt{2})\\displaystyle\\sum_{j=1}^{q}\\hat{\\mathcal{R}}_{n}\\left(\\mathcal{H}_{j}\\circ S\\right),\\ }\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Although only implementing KL-divergence does not satisfy Lipschitzness, the combination of KL and Softmax(SF) is $(\\sqrt{p}+1)$ -Lipschitz. Define class of functions of $j$ -th output with weight constraints as $\\mathcal{H}_{j}=\\left\\{x\\rightarrow\\mathbf{w}_{j}\\cdot x:\\|\\mathbf{w}_{j}\\|_{2}\\leq1\\right\\}$ . According to [47], rademacher complexity of $\\mathcal{H}_{j}$ satisfies ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{R}}_{n}\\left(\\mathcal{H}_{j}\\circ S\\right)\\leq\\frac{\\operatorname*{max}_{i\\in[n]}\\mid\\|x_{i}\\|_{2}}{\\sqrt{n}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Then right-hand side of Eq. (31) is bounded as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{R}}_{n}(\\mathrm{KL}\\circ\\mathrm{SF}\\circ\\mathcal{H}\\circ S)\\leq\\frac{(\\sqrt{2q}+\\sqrt{2})q}{\\sqrt{n}}\\operatorname*{max}_{i\\in[n]}\\left\\|x_{i}\\right\\|_{2}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "According to Lemma 9, while we have the rademacher complexity of LDL with KL-divergence loss defined in Eq. (32), the bound can be defined as: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{\\mathcal{G}}\\left(f_{w}\\right)-L_{\\left(X,A\\right)}\\left(f_{w}\\right)\\leq2\\frac{\\left(\\sqrt{2q}+\\sqrt{2}\\right)q}{\\sqrt{n}}\\underset{i\\in\\left[n\\right]}{\\operatorname*{max}}\\left\\Vert x_{i}\\right\\Vert_{2}}\\\\ &{\\qquad\\qquad\\qquad+\\left.3\\mu\\sqrt{\\frac{\\log2/\\delta}{2n}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Considering the node representations vary in different layers, Without loss of generality, we replace $\\operatorname*{max}_{i\\in[n]}\\left\\|x_{i}\\right\\|_{2}$ with $\\operatorname*{max}_{i\\in[n],j\\in[l]}\\left\\|\\mathbf{x}_{i}^{j}\\right\\|_{2}^{\\cdot}$ to find the maximum node representation, instead of node feature. Then recall the PAC risk bound defined in Eq. (29), ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{\\mathcal{G}}\\left(f_{w}\\right)-L_{\\left(X,A\\right)}\\left(f_{w}\\right)}\\\\ &{\\qquad\\qquad\\leq\\mathcal{O}\\left(\\sqrt{\\frac{B^{2}d^{l-1}l^{2}k\\log\\left(l k\\right)\\mathcal{D}\\left(W_{i}\\right)+\\log\\frac{n l}{\\delta}}{\\gamma^{2}n}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We use the sum of both above bounds as the final risk bound for a GCN with KL-divergence loss for the label distribution learning problem. ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{\\mathcal{G}}\\left(f_{w}\\right)-L_{\\left(X,A\\right)}\\left(f_{w}\\right)\\leq\\frac{2\\left(\\sqrt{2q}+\\sqrt{2}\\right)q}{\\sqrt{n}}\\underset{i\\in\\left[n\\right],j\\in\\left[l\\right]}{\\operatorname*{max}}\\left\\Vert\\mathbf{x}_{i}^{j}\\right\\Vert_{2}}\\\\ &{+\\left3b\\sqrt{\\frac{\\log{2/\\delta}}{2n}}+\\mathcal{O}\\left(\\sqrt{\\frac{B^{2}d^{l-1}l^{2}k\\log(l k)\\mathcal{D}\\left(W_{i}\\right)+\\log{\\frac{n l}{\\delta}}}{\\gamma^{2}n}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "As [48] suggests that 0 is replaced by a very small value, say $\\gamma\\,>\\,0$ , for division by 0 when implementing $\\mathrm{KL}$ divergence, then for probability distribution $\\mathbf{p},\\mathbf{q}\\in\\mathbb{R}^{m}$ with $p_{i}\\geq\\gamma,q_{i}\\geq\\gamma$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname{KL}(\\mathbf{p},\\mathbf{q})=\\sum_{i=1}^{n}p_{i}\\ln{\\frac{p_{i}}{q_{i}}}\\leq\\sum_{i=1}^{n}p_{i}\\ln{\\frac{1}{\\gamma}}\\leq-\\ln\\gamma,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "thus there exists a constant $b\\geq-\\ln\\gamma$ such that $\\mathrm{KL}(\\cdot,\\cdot)\\leq b$ (e.g., $b=35$ for $\\gamma=1\\times10^{-15}$ ).   \nHence, all of the above concludes the proof. ", "page_idx": 29}, {"type": "text", "text": "H Supplement F: Additional Theoretical Analysis ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "H.1 Uniform Distribution Phenomenon ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "It is observed in the experiment that given a fixed large hidden dimension $f$ , normalized adjacency matrices are more easily to initialize approximate uniform attention than unnormalized adjacency matrices. In the following, we provide a theoretical analysis of the reason behind this phenomenon. ", "page_idx": 29}, {"type": "text", "text": "H.1.1 Normalized adjacency matrix are more likely to initialize uniform attention ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "For simplicity of analysis, we use one node\u2019s attentions from a single meta path graph in the study.   \nSame analysis can be applied to all nodes and all meta-path graphs, without loss of generalizability. ", "page_idx": 29}, {"type": "text", "text": "Denote $A\\in\\mathbf{R}^{n\\times n}$ the adjacency matrix of the meta-path graph from a meta-path $\\mathcal{P}$ , and $W\\in\\mathbb{R}^{n\\times f}$ the learnable weight of the meta-path graph $W$ is indeed $W_{0}^{i}$ in Eq. (1) shown in the main manuscript. We drop $W_{0}^{i}$ \u2019s subscript 0 and superscript $i$ here for simplicity, as our analysis is based on a single meta-path graph). $\\boldsymbol{W}[:,i]\\in\\mathbb{R}^{n\\times1}$ denotes the column vector of $W$ , and $\\b{A}[i,:]\\in\\mathbb{R}^{1\\times n}$ is a row vector recording node $i$ \u2019s connections ( $n$ is the number of nodes in the meta-path graph). ", "page_idx": 29}, {"type": "text", "text": "A uniform weight initialization is with bounds $[-\\frac{1}{\\sqrt{f}},\\frac{1}{\\sqrt{f}}]$ for all the learnable matrices. $d$ denotes the average degree of the graph adjacency matrix. ", "page_idx": 29}, {"type": "text", "text": "Theorem 11. Denote $A$ and $\\tilde{A}$ the adjacency matrix and normalized adjacency matrix of a graph, respectively. For node i in the graph, its attention is proportional to $\\begin{array}{r}{A[i,:]\\cdot W[:,\\dot{i}]\\sim{\\mathcal O}(\\frac{\\dot{d}}{\\sqrt{f}})}\\end{array}$ for the adjacency matrix, but proportional to $\\begin{array}{r}{\\tilde{A}[i,:]\\cdot W[:,i]\\sim\\mathcal{O}(\\frac{1}{\\sqrt{f}})}\\end{array}$ for normalized adjacency matrix. ", "page_idx": 29}, {"type": "text", "text": "Proof. Normalization of $A$ (ignoring the self-loop for simplicity) gives: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\tilde{A}=D^{\\frac{-1}{2}}A D^{\\frac{-1}{2}}}}\\\\ {{\\tilde{A}[i,j]=\\frac{1}{\\sqrt{d_{i}}\\sqrt{d_{j}}}*A[i,j]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Denote $e_{j}\\ \\in\\ \\mathbb{R}^{1\\times n}$ a one-hot encoded vector with only $j^{t h}$ position has value 1 (e.g. $e_{2}=$ $(0,1,0,0,0)$ . Because $\\|e_{i}\\|=1$ , it\u2019s easy to know that $e_{j},\\forall j$ can be used to construct an orthonormal eigenbasis. Denote $E_{i}$ as the index set of edges for node $i$ . For example, if $A[i,:]=(0,1,1,0,0)$ , then $E_{i}=\\{2,3\\}$ , because index position 2 and 3 has 1 (i.e. edge connections) to node $i$ . Then we can decompose $A[i,:]$ and $W[:,i]$ as follows, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{A\\displaystyle[i,:]=\\sum_{j\\in E_{i}}e_{j}}}\\\\ {{\\tilde{A}\\displaystyle[i,:]=\\sum_{j\\in E_{i}}\\frac{1}{\\sqrt{d_{i}}\\sqrt{d_{j}}}e_{j}}}\\\\ {{W[:,i]=\\sum_{k}c_{k}e_{k}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $c_{k}$ is a scalar for basis $e_{k}$ and $c_{k}$ is bounded by $[-\\frac{1}{\\sqrt{f}},\\frac{1}{\\sqrt{f}}]$ . Assume that $c_{k}\\neq0$ for $k\\in E_{i}$ (meaning that the scalar is nonzero when nodes $i$ and $k$ are connected), then we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A[i,:]\\cdot W[:,i]=\\displaystyle\\sum_{j\\in E_{i}}e_{j}*\\sum_{k}c_{k}e_{k}=\\displaystyle\\sum_{l=1}^{d_{i}}c_{l}\\sim O(d/\\sqrt{f})}\\\\ &{\\tilde{A}[i,:]\\cdot W[:,i]=\\displaystyle\\sum_{j\\in E_{i}}\\frac{1}{\\sqrt{d_{i}}\\sqrt{d_{j}}}e_{j}*\\sum_{k}c_{k}e_{k}}\\\\ &{\\qquad\\qquad\\qquad\\sim\\displaystyle\\sum_{l=1}^{d_{i}}\\frac{c_{l}}{\\sqrt{d\\times d}}\\sim O(1/\\sqrt{f})}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $d_{i}$ denotes the degree of node $i$ (i.e. $\\begin{array}{r}{d_{i}=\\sum A[i,:])}\\end{array}$ . Compared to Eq. (42) vs. Eq. (40), it is easy to conclude that $f$ plays more significant role in Eq. (42). Meaning that an unnormalized adjacency matrix affects the final attention with the average degree of the matrix while normalized adjacency matrix is dominated by the square root of hidden dimension $f$ . With a relatively large hidden dimension, it is easy to see that normalized adjacency matrices are more likely to be dominated by the hidden dimension size $f$ and produce uniform distribution of learned attention in the beginning stage. ", "page_idx": 30}, {"type": "text", "text": "H.2 Benefits of proposed attention structure in terms of label distribution learning ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Similar structure of our proposed attention structure is applied when combining GNN and transformer structure, we provide an analysis on why such proposed attention structure is beneficial to graph label distribution learning. To show this, we first leverage the conclusion from Eq. (34) to obtain a bound of graph label distribution learning on normal l GCN layer. ", "page_idx": 30}, {"type": "text", "text": "There are three parts for the bounds, the part O $\\begin{array}{r}{\\mathcal{O}\\Bigg(\\sqrt{\\frac{B^{2}d^{l-1}l^{2}k\\log(l k)\\mathcal{D}(W_{i})+\\log\\frac{n l}{\\delta}}{\\gamma^{2}n}}\\Bigg)}\\end{array}$ is our focus as it is directly related to maximum degree $d$ of graph adjacency matrix and is related to our change in the GNN structure. It is easy to observe that our proposed attention structure follows the same GCN update except the adjacency matrix is now learnable from features and combined with topology space. So we assume that our model is a dynamic graph version and we focus on how the changes in the learnable graph could affect the bound. We show in the following that our method reduces the maximum degree of graph adjacency matrix and therefore tightens the bound for graph label distribution learning. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "Our attention layer learns a feature space topology and intersects its feature neighbor with its topology neighbor to get a consistent neighbor set agreed by both feature and topology. ", "page_idx": 30}, {"type": "text", "text": "Theorem 12 (Equal priority of feature and topology space). Denote original graph structure as A, propagation graph for attention structure as $\\tilde{A}$ , denote $\\delta(A[i,:])$ as the $i^{t h}$ node\u2019s degree. $\\Delta(A)$ as maximum degree of adjacency matrix $A$ and arg $\\mathrm{max}_{i}$ $\\delta(A[i,:])$ as the node index that has the maximum degree: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\Delta(\\tilde{A})\\leq\\Delta(A)\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Tilde{A}_{f}=Z Q(Z K)^{T}}\\\\ &{\\Tilde{A}=S o f t m a x(\\Tilde{A}_{f\\odot A})}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "With $\\odot$ operator, for any node i, it is easy to observe that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\delta(\\tilde{A}[i,:])\\leq\\delta(A[i,:])\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Assume without loss of generality that $A$ has $\\Delta(\\mathbf{A})$ at node $j$ , i.e, $\\arg\\operatorname*{max}_{i}\\delta(A{[i,:]})=j$ . and $\\tilde{A}$ has $\\Delta(\\mathbf{A})$ at node k,i.e, a $\\mathrm{trg}\\,\\mathrm{max}_{i}\\,\\delta(\\tilde{A}[i,:])=k$ . For arbitrary node $i$ in $\\tilde{A}$ , ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta(\\tilde{A}[i,:])\\leq\\delta(A[i,:])\\leq\\delta(A[j,:])}\\\\ &{\\Delta(\\tilde{A})=\\delta(\\tilde{A}[k,:])\\leq\\delta(A[k,:])\\leq\\delta(A[j,:])=\\Delta(A)}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We note that the equality sign only appears iff $k=j$ and $\\delta(\\tilde{A}[k,:])=\\delta(A[k,:])$ which relies on $\\tilde{A}_{f}$ at row $k$ to be exactly the same as $A$ at row $k$ . The probability is related to the consistency of feature similarity and topology structure. ", "page_idx": 31}, {"type": "text", "text": "For a graph with relatively low homophily score $p_{i}$ for a specific node $i$ with $p_{i}$ defined as ", "page_idx": 31}, {"type": "equation", "text": "$$\np_{i}=\\frac{\\sum_{i\\in N e i g h b o r}(L a b e l(N e i g h b o r(i))==L a b e l(i))}{|N e i g h b o r|}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $L a b e l(i)$ for a node i is the dominate class of its distribution and Neighbor $(i)$ is defined as topology neighbor set of node $i$ . $|N e i g h b o r(i)|$ indicates the size of the set. ", "page_idx": 31}, {"type": "text", "text": "Assume that feature similarity strongly relates to label connection among nodes,i.e, features are similar between samples if their label distribution are close. The learnt $\\bar{\\tilde{A}}_{f}$ at row $\\mathbf{k}$ satisfies the equality condition iff $p_{k}=1$ . For practical graphs in general, such homophily score rarely exist and therefore our methods should reduce the maximum degree of the propagation graph in most cases. \u53e3 ", "page_idx": 31}, {"type": "text", "text": "Remark 1. With Theorem 6, we have shown that our proposed design (i $\\cdot e$ using $\\odot$ between learnt feature topology and graph topology) can reduce maximum degree of the propagation graph with high probability. Leveraging derived bound term from Eq. (29) related to maximum degree, we can observe that lower maximum degree allows the model to have a tighter bound for HGDL problem in general. Therefore, our proposed design is naturally beneficial to the heterogeneous graph label distribution learning. ", "page_idx": 31}, {"type": "table", "img_path": "OwguhIAh8R/tmp/84703cbe7f114a14b849ad6a5cfa0cd4b7a614cee4ff69ac92f7124080486348.jpg", "table_caption": [], "table_footnote": ["Table 14: Runtime results for 100 training epochs. The numbers of target nodes are labeled beside their corresponding dataset names. "], "page_idx": 31}, {"type": "text", "text": "H.3 Runtime Discussion ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "To verify the efficiency of our method, Table 14 shows the runtime for 100 training epochs for our baseline methods, Each method has been trained with 100 epochs. We can observe that our proposed HGDL algorithm enjoys good scalability by attaining runtime results that are comparable to the baseline GCNs. Compared to other baseline such as SeHGNN, HGDL is in general much faster. This indicates that our method is more efficient for learning heterogeneous networks. This can be mainly attributed to the automated learning of weights to control individual meta-paths and then integrates them to yield optimal graph topology homogenization for label distribution learning. ", "page_idx": 31}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: Our study aims to generalize Label Distribution Learning from IID data to the non-IID, networked data, and we have made this claim clear in the Abstract and Introduction. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 32}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: We discussed the limited performance gain on CAD and CLD metrics in Section 6.3 and provided Runtime analysis in Supplement F Section H.3. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 32}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We have analyzed the generalization error bound of the proposed algorihtm in a PAC-Bayes regime and provided the complete proof in Supplement D and E. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 33}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 33}, {"type": "text", "text": "[Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: We have provided tuned parameter settings in provided Github link and reported their impact on empirical results in Supplement C. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 33}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We have uploaded the code and benchmark datasets as the supplementary submission in provided Github link. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 34}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We have presented the creation and preprocessing steps of the benchmark datasets and the implementation details of our algorithms in Supplementary B and C, respectively. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 34}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We generated experimental results from repeated experiments (including different train/test split) and leveraged t-test to validate the statistical significance in settings where our algorithm outperform its competitors. We reported its win/tie/loss in Section 6.2 where \u201cwin\u201d indicates that our algorithm outperforms the compared models with confidence level at a level of $90\\%$ . ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 34}, {"type": "text", "text": "\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 35}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We disclosed our computing node setup. Note, our algorithm is not computational demanding and could be generalized onto various single-GPU machines. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 35}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We revised the COE and all agreed on it. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 35}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We demonstrated multiple potential application of our algorithm with the benchmark dataset, including the use case of Urban Functionality Delineation, which has been leveraged as the motivating example of our proposal in Introduction. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 36}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA]   \nustification: NA   \nGuidelines: \u2022 The answer NA means that the paper poses no such risks. \u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. \u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 36}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA]   \nJustification: NA   \nGuidelines: \u2022 The answer NA means that the paper does not use existing assets. \u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 37}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: The artifacts generated from this paper will be published under CC-BY 4.0 license for public access and non-commercial use. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 37}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: NA Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 37}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] Justification: NA Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 38}]