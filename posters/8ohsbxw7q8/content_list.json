[{"type": "text", "text": "Graph Diffusion Policy Optimization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yijing $\\mathbf{Liu^{*1}}$ , Chao $\\mathbf{D}\\mathbf{u}^{*\\dagger2}$ , Tianyu Pang2, Chongxuan $\\mathbf{Li}^{3}$ , Min $\\mathbf{Lin^{2}}$ , Wei Chen\u20201 ", "page_idx": 0}, {"type": "text", "text": "1State Key Lab of CAD&CG, Zhejiang University 2Sea AI Lab, Singapore 3Renmin University of China {liuyj86,chenvis}@zju.edu.cn; {duchao,tianyupang,linmin}@sea.com; chongxuanli@ruc.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent research has made significant progress in optimizing diffusion models for downstream objectives, which is an important pursuit in fields such as graph generation for drug design. However, directly applying these models to graph presents challenges, resulting in suboptimal performance. This paper introduces graph diffusion policy optimization (GDPO), a novel approach to optimize graph diffusion models for arbitrary (e.g., non-differentiable) objectives using reinforcement learning. GDPO is based on an eager policy gradient tailored for graph diffusion models, developed through meticulous analysis and promising improved performance. Experimental results show that GDPO achieves state-of-the-art performance in various graph generation tasks with complex and diverse objectives. Code is available at https://github.com/sail-sg/GDPO. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Graph generation, a key facet of graph learning, has applications in a variety of domains, including drug and material design [54], code completion [8], social network analysis [20], and neural architecture search [64]. Numerous studies have shown significant progress in graph generation with deep generative models [34, 62, 69, 21]. one of The most notable advances in the field is the introduction of graph diffusion probabilistic models (DPMs) [61, 31]. These methods can learn the underlying distribution from graph data samples and produce high-quality novel graph structures. ", "page_idx": 0}, {"type": "text", "text": "In many use cases of graph generation, the primary focus is on achieving specific objectives, such as high drug efficacy [60] or creating novel graphs with special discrete properties [22]. These objectives are often expressed as specific reward signals, such as binding affinity [10] and synthetic accessibility [7], rather than a set of training graph samples. Therefore, a more pertinent goal in such scenarios is to train graph generative models to meet these predefined objectives directly, rather than learning to match a distribution over training data [72]. ", "page_idx": 0}, {"type": "text", "text": "A major challenge in this context is that most signals are non-differentiable w.r.t. graph representations, making it difficult to apply many optimization algorithms. To address this, methods based on property predictors [29, 37] learn parametric models to predict the reward signals, providing gradient guidance for graph generation. However, since reward signals can be highly complex (e.g., results from physical simulations), these predictors often struggle to provide accurate guidance [44]. An alternative direction is to learn graph generative models as policies through reinforcement learning (RL) [72], which enables the integration of exact reward signals into the optimization. However, existing work primarily explores earlier graph generative models and has yet to leverage the superior performance of graph DPMs [9, 68]. On the other hand, several pioneer works have seen significant progress in optimizing continuous-variable (e.g., images) DPMs for downstream objectives [6, 16]. The central idea is to formulate the sampling process as a policy, with the objective serving as a reward, and then learn the model using policy gradient methods. However, when these approaches are directly extended to (discrete-variable) graph DPMs, we empirically observe a substantial failure, which we will illustrate and discuss in Sec. 4. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To close this gap, we present graph diffusion policy optimization (GDPO), a policy gradient method designed to optimize graph DPMs for arbitrary reward signals. Using an RL formulation similar to that introduced by Black et al. [6] and Fan et al. [16] for continuous-variable DPMs, we first adapt the discrete diffusion process of graph DPMs to a Markov decision process (MDP) and formulate the learning problem as policy optimization. Then, to address the observed empirical failure, we introduce a slight modification to the standard policy gradient method REINFORCE [58], dubbed the eager policy gradient and specifically tailored for graph DPMs. Experimental evaluation shows that GDPO proves effective across various scenarios and achieves high sample efficiency. Remarkably, our method achieves a $\\mathbf{41.64\\%}$ to $\\mathbf{81.97\\%}$ average reduction in generation-test distance and a $1.03\\%$ to $\\mathbf{19.31\\%}$ improvement in the rate of generating effective drugs, while only querying a small number of samples (1/25 of the training samples). ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Graph Generative Models. Early work in graph generation employs nonparametric random graph models [15, 26]. To learn complex distributions from graph-structured data, recent research has shifted towards leveraging deep generative models. This includes approaches based on auto-regressive generative models [69, 39], variational autoencoders (VAEs) [34, 41, 23], generative adversarial networks (GANs) [62, 9, 43], and normalizing flows [53, 40, 42]. ", "page_idx": 1}, {"type": "text", "text": "Recently, diffusion probabilistic models (DPMs) [25, 56] have significantly advanced graph generation [70]. Models like EDP-GNN [46] GDSS [31] and DruM [30] construct graph DPMs using continuous diffusion processes [57]. While effective, the use of continuous representations and Gaussian noise can hurt the sparsity of generated graphs. DiGress [61] employs categorical distributions as the Markov transitions in discrete diffusion [2], performing well on complex graph generation tasks. While these works focus on learning graph DPMs from a given dataset, our primary focus in this paper is on learning from arbitrary reward signals. ", "page_idx": 1}, {"type": "text", "text": "Controllable Generation for Graphs. Recent progress in controllable generation has also enabled graph generation to achieve specific objectives or properties. Previous work leverages mature conditional generation techniques from GANs and VAEs [66, 52, 36, 28, 14]. This paradigm has been extended with the introduction of guidance-based conditional generation in DPMs [12]. DiGress [61] and GDSS [31] provide solutions that sample desired graphs with guidance from additional property predictors. MOOD [37] improves these methods by incorporating out-of-distribution control. However, as predicting the properties (e.g., drug efficacy) can be extremely difficult [33, 44], the predictors often struggle to provide accurate guidance. Our work directly performs property optimization on graph DPMs, thus bypassing this challenge. ", "page_idx": 1}, {"type": "text", "text": "Graph Generation using RL. RL techniques find wide application in graph generation to meet downstream objectives. REINVENT [47] and GCPN [68] are representative works, which define graph environments and optimize policy networks with policy gradient methods [59]. For datafree generation modelling, MolDQN [71] replaces the data-related environment with a humandefined graph environmentand and utilizes Q-Learning [24] for policy optimi zation. To generate more realistic molecules, DGAPN [63] and FREED [67] investigate the fragment-based chemical environment, which reduce the search space significantly. Despite the great successes, existing methods exhibit high time complexity and limited policy model capabilities. Our work, based on graph DPMs with enhanced policy optimization, achieves new state-of-the-art performance. ", "page_idx": 1}, {"type": "text", "text": "Aligning DPMs. Several works focus on optimizing generative models to align with human preferences [45, 3]. DPOK [16] and DDPO [6] are representative works that align text-to-image DPMs with black-box reward signals. They formulate the denoising process of DPMs as an MDP and optimize the model using policy gradient methods. For differentiable rewards, such as human preference models [35], AlignProp [50] and DRaFT [11] propose effective approaches to optimize DPMs with direct backpropagation, providing a more accurate gradient estimation than DDPO and DPOK. However, these works are conducted on images. To the best of our knowledge, our work is the first effective method for aligning graph DPMs, filling a notable gap in the literature. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we briefly introduce the background of graph DPMs and policy gradient methods. ", "page_idx": 2}, {"type": "text", "text": "Following Vignac et al. [61], we consider graphs with categorical node and edge attributes, allowing representation of diverse structured data like molecules. Let $\\mathcal{X}$ and $\\mathcal{E}$ be the space of categories for nodes and edges, respectively, with cardinalities $a=|\\mathcal{X}|$ and $b=|\\mathcal{E}|$ . For a graph with $n$ nodes, we denote the attribute of node $i$ by a one-hot encoding vector $\\pmb{x}^{(i)}\\in\\mathbb{R}^{a}$ . Similarly, the attribute of the edge1 from node $i$ to node $j$ is represented as $\\pmb{e}^{(i\\bar{j})}\\in\\mathbb{R}^{b}$ . By grouping these one-hot vectors, the graph can then be represented as a tuple $G\\triangleq(X,E)$ , where $\\b{X}\\in\\mathbb{R}^{n\\times a}$ and $\\pmb{E}\\in\\mathbb{R}^{n\\times n\\times b}$ . ", "page_idx": 2}, {"type": "text", "text": "3.1 Graph Diffusion Probabilistic Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Graph diffusion probabilistic models (DPMs) [61] involve a forward diffusion process $q(G_{1:T}|G_{0})=$ $\\begin{array}{r}{\\prod_{t=1}^{T}q(G_{t}|G_{t-1})}\\end{array}$ , which gradually corrupts a data distribution $q(G_{0})$ into a simple noise distribution $q(\\pmb{G}_{T})$ over a specified number of diffusion steps, denoted as $T$ . The transition distribution $q(G_{t}|G_{t-1})$ can be factorized into a product of categorical distributions for individual nodes and edges, i.e., $q(\\pmb{x}_{t}^{(i)}|\\pmb{x}_{t-1}^{(i)})$ and $q(e_{t}^{(i j)}|e_{t-1}^{(i j)})$ . For simplicity, superscripts are omitted when no ambiguity is caused in the following. The transition distribution for each node is defined as $q(\\mathbf{x}_{t}|\\mathbf{x}_{t-1})=$ $\\operatorname{Cat}(\\pmb{x}_{t};\\pmb{x}_{t-1}\\pmb{Q}_{t})$ , where the transition matrix is chosen as $\\begin{array}{r}{{\\pmb Q}_{t}\\triangleq\\alpha_{t}{\\pmb I}+(1-\\alpha_{t})({\\pmb1}_{a}{\\bf1}_{a}^{\\top})/a}\\end{array}$ , with $\\alpha_{t}$ transitioning from 1 to 0 as $t$ increases [2]. It then follows that $q(\\pmb{x}_{t}|\\pmb{x}_{0})=\\mathrm{Cat}(\\pmb{x}_{t};\\pmb{x}_{0}\\pmb{Q}_{t})$ and $\\begin{array}{r}{q(\\mathbf{x}_{t-1}|\\mathbf{x}_{t},\\mathbf{x}_{0})=\\mathrm{Cat}(\\mathbf{x}_{t-1};\\frac{\\mathbf{x}_{t}Q_{t}^{\\top}\\odot\\mathbf{\\deltax}_{0}\\bar{Q}_{t-1}}{\\mathbf{x}_{0}\\bar{Q}_{t}\\mathbf{x}_{t}^{\\top}})}\\end{array}$ ; xtQxt\u22a4 \u2299Q\u00afxx0 \u22a4Q\u00aft\u22121), where Q\u00aft \u225cQ1Q2 \u00b7 \u00b7 \u00b7 Qt and \u2299denotes elementwise product. The design choice of $Q_{t}$ ensures that $q(\\pmb{x}_{T}|\\pmb{x}_{0})\\approx\\mathrm{Cat}(\\pmb{x}_{T};\\mathbf{1}_{a}/a)$ , i.e., a uniform distribution over $\\mathcal{X}$ . The transition distribution for edges is defined similarly, and we omit it for brevity. ", "page_idx": 2}, {"type": "text", "text": "Given the forward diffusion process, a parametric reverse denoising process $\\begin{array}{r l}{p_{\\theta}(G_{0:T})}&{{}=}\\end{array}$ $\\begin{array}{r}{p(G_{T})\\prod_{t=1}^{T}p_{\\theta}(G_{t-1}|G_{t})}\\end{array}$ is then learned to recover the data distribution from $p(G_{T})\\approx q(G_{T})$ (an approx imate uniform distribution). The reverse transition $p_{\\theta}\\big(G_{t-1}\\big|G_{t}\\big)$ is a product of categorical distributions over nodes and edges, denoted as $p_{\\theta}(\\mathbf{x}_{t-1}|G_{t})$ and $p_{\\theta}(e_{t-1}|G_{t})$ . Notably, in line with the $\\scriptstyle{x_{0}}$ -parameterization used in continuous DPMs [25, 32], $p_{\\theta}(\\mathbf{x}_{t-1}|G_{t})$ is modeled as: ", "page_idx": 2}, {"type": "equation", "text": "$$\np_{\\theta}(\\mathbf{\\boldsymbol{x}}_{t-1}|G_{t})\\triangleq\\sum_{\\widetilde{\\mathbf{\\boldsymbol{x}}}_{0}\\in\\mathcal{X}}q(\\mathbf{\\boldsymbol{x}}_{t-1}|\\mathbf{\\boldsymbol{x}}_{t},\\widetilde{\\mathbf{\\boldsymbol{x}}}_{0})p_{\\theta}(\\widetilde{\\mathbf{\\boldsymbol{x}}}_{0}|G_{t}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $p_{\\theta}(\\widetilde{\\mathbf{x}}_{0}|G_{t})$ is a neural network predicting the posterior probability of $\\pmb{x}_{0}$ given a noisy graph $\\pmb{G}_{t}$ . For ed ges, each definition is analogous and thus omitted. ", "page_idx": 2}, {"type": "text", "text": "The model is learned with a graph dataset $\\mathcal{D}$ by maximizing the following objective [61]: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{I}_{\\mathrm{GDPM}}(\\theta)=\\mathbb{E}_{G_{0},t}\\mathbb{E}_{q(G_{t}|G_{0})}\\left[\\log p_{\\theta}(G_{0}|G_{t})\\right],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\pmb{G}_{0}$ and $t$ follow uniform distributions over $\\mathcal{D}$ and $[1,T]$ , respectively. After learning, graph samples can then be generated by first sampling $G_{T}$ from $p(\\bar{\\pmb{G}}_{T})$ and subsequently sampling $\\pmb{G}_{t}$ from $p_{\\theta}(G_{t-1}|G_{t})$ , resulting in a generation trajectory $(G_{T},G_{T-1},\\hdots,G_{0})$ . ", "page_idx": 2}, {"type": "text", "text": "3.2 Markov Decision Process and Policy Gradient ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Markov decision processes (MDPs) are commonly used to model sequential decision-making problems [17]. An MDP is formally defined by a quintuple $(S,\\mathcal{A},P,r,\\bar{\\rho}_{0})$ , where $\\boldsymbol{S}$ is the state space containing all possible environment states, $\\boldsymbol{\\mathcal{A}}$ is the action space comprising all available potential actions, $P$ is the transition function determining the probabilities of state transitions, $r$ is the reward signal, and $\\rho_{0}$ gives the distribution of the initial state. ", "page_idx": 2}, {"type": "text", "text": "In the context of an MDP, an agent engages with the environment across multiple steps. At each step $t$ , the agent observes a state $s_{t}\\,\\in\\,S$ and selects an action $\\mathbf{\\alpha}_{a_{t}}\\in\\mathcal{A}$ based on its policy distribution ", "page_idx": 2}, {"type": "image", "img_path": "8ohsbxw7q8/tmp/bad19d2c6e8763abb8316c2cff1ddb844e2dafed667a9547f0d47cb998dbe8e3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 1: Overview of GDPO. (1) In each optimization step, GDPO samples multiple generation trajectories from the current Graph DPM and queries the reward function with different $\\pmb{G}_{0}$ . (2) For each trajectory, GDPO accumulates the gradient $\\nabla_{\\theta}\\log p_{\\theta}(G_{0}|G_{t})$ of each $(G_{0},G_{t})$ pair and assigns a weight to the aggregated gradient based on the corresponding reward signal. Finally, GDPO estimates the eager policy gradient by averaging the aggregated gradient from all trajectories. ", "page_idx": 3}, {"type": "text", "text": "$\\pi_{\\boldsymbol{\\theta}}\\big(\\mathbf{a}_{t}|\\mathbf{s}_{t}\\big)$ . Subsequently, the agent receives a reward $r(s_{t},\\mathbf{\\boldsymbol{a}}_{t})$ and transitions to a new state $s_{t+1}$ following the transition function $P(s_{t+1}|s_{t},\\mathbf{a}_{t})$ . As the agent interacts in the MDP (starting from an initial state $s_{0}\\sim\\rho_{0})$ ), it generates a trajectory (i.e., a sequence of states and actions) denoted as $\\tau=(s_{0},a_{0},s_{1},\\pmb{a}_{1},\\dots,\\pmb{s}_{T},\\pmb{a}_{T})$ . The cumulative reward over a trajectory $\\tau$ is given by $R(\\tau)=$ $\\textstyle\\sum_{t=0}^{T}r\\big(\\pmb{s}_{t},\\pmb{a}_{t}\\big)$ . In most scenarios, the objective is to maximize the following expectation: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{J}_{\\mathrm{RL}}(\\theta)=\\mathbb{E}_{\\tau\\sim p(\\tau|\\pi_{\\theta})}\\left[R(\\tau)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Policy gradient methods aim to estimate $\\nabla_{\\theta}\\mathcal{I}_{\\mathrm{RL}}(\\theta)$ and thus solve the problem by gradient descent. An important result is the policy gradient theorem [19], which estimates $\\nabla_{\\theta}\\mathcal{I}_{\\mathrm{RL}}(\\theta)$ as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}\\mathcal{J}_{\\mathrm{RL}}(\\theta)=\\mathbb{E}_{\\tau\\sim p(\\tau\\mid\\pi_{\\theta})}\\!\\left[\\sum_{t=0}^{T}\\nabla_{\\theta}\\log\\pi_{\\theta}(a_{t}\\vert s_{t})R(\\tau)\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The REINFORCE algorithm [58] provides a simple method for estimating the above policy gradient using Monte-Carlo simulation, which will be adopted and discussed in the following section. ", "page_idx": 3}, {"type": "text", "text": "4 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we study the problem of learning graph DPMs from arbitrary reward signals. We first present an MDP formulation of the problem and conduct an analysis on the failure of a direct application of REINFORCE. Based on the analysis, we introduce a substitute termed eager policy gradient, which forms the core of our method Graph Diffusion Policy Optimization (GDPO). ", "page_idx": 3}, {"type": "text", "text": "4.1 A Markov Decision Process Formulation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "A graph DPM defines a sample distribution $p_{\\theta}(G_{0})$ through its reverse denoising process $p_{\\theta}(\\pmb{G}_{0:T})$ . Given a reward signal $r(\\cdot)$ for $\\pmb{G}_{0}$ , we aim to maximize the expected reward (ER) over $p_{\\theta}(G_{0})$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{I}_{\\mathrm{ER}}(\\theta)=\\mathbb{E}_{{G}_{0}\\sim p_{\\theta}({G}_{0})}\\left[r(G_{0})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "However, directly optimizing ${\\mathcal{I}}_{\\mathrm{ER}}(\\theta)$ is challenging since the likelihood $p_{\\theta}(G_{0})$ is unavailable [25] and $r(\\cdot)$ is black-box, hindering the use of typical RL algorithms [6]. Following Fan et al. [16], we ", "page_idx": 3}, {"type": "image", "img_path": "8ohsbxw7q8/tmp/5f285d53449b8e41997d7df3a4d13f41a585aceb12e9339c85ee459bc81b333a.jpg", "img_caption": ["Figure 2: Toy experiment comparing DDPO and GDPO. We generate connected graphs with increasing number of nodes. Node categories are disregarded, and the edge categories are binary, indicating whether two nodes are linked. The graph DPM is initialized randomly as a one-layer graph transformer from DiGress [61]. The diffusion step $T$ is set to 50, and the reward signal $r(G_{0})$ is defined as 1 if $G_{0}$ is connected and 0 otherwise. We use 256 trajectories for gradient estimation in each update. The learning curve illustrates the diminishing performance of DDPO as the number of nodes increases, while GDPO consistently performs well. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "formulate the denoising process as a $T$ -step MDP and obtain an equivalent objective. Using notations in Sec. 3, we define the MDP of graph DPMs as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{s_{t}\\triangleq(G_{T-t},T-t),\\ \\ a_{t}\\triangleq G_{T-t-1},\\ \\ \\pi_{\\theta}(a_{t}|s_{t})\\triangleq p_{\\theta}(G_{T-t-1}|G_{T-t}),}&{}\\\\ {P(s_{t+1}|s_{t},a_{t})\\triangleq(\\delta_{G_{T-t-1}},\\delta_{T-t-1}),\\ \\ \\ r(s_{t},a_{t})\\triangleq r(G_{0})\\ \\mathrm{if}\\ t=T,\\ r(s_{t},a_{t})\\triangleq0\\ \\mathrm{if}\\ t<T,}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the initial state $\\ s_{\\mathrm{0}}$ corresponds to the initial noisy graph $G_{T}$ and the policy corresponds to the reverse transition distribution. As a result, the graph generation trajectory $(G_{T},G_{T-1},\\hdots,G_{0})$ can be considered as a state-action trajectory $\\tau$ produced by an agent acting in the MDP. It then follows that $p(\\tau|\\pi_{\\boldsymbol{\\theta}})\\,=\\,p_{\\boldsymbol{\\theta}}(G_{0:T})$ .2 Moreover, we have $\\begin{array}{r}{R(\\tau)\\,=\\,\\sum_{t=0}^{T}r(s_{t},a_{t})\\,=\\,r(G_{0})}\\end{array}$ . Therefore, the expected cumulative reward of the agent $\\mathcal{T}_{\\mathrm{RL}}(\\theta)\\,=\\,\\mathbb{E}_{p(\\tau|\\pi_{\\theta})}[R(\\tau)]\\,=\\,\\mathbb{E}_{p_{\\theta}(G_{0:T})}[r(G_{0})]$ is equivalent to ${\\mathcal{I}}_{\\mathrm{ER}}(\\theta)$ , and thus ${\\mathcal{I}}_{\\mathrm{ER}}(\\theta)$ can also be optimized with the policy gradient $\\nabla_{\\theta}\\mathcal{I}_{\\mathrm{RL}}(\\theta)$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\nabla_{\\boldsymbol{\\theta}}\\mathcal{J}_{\\mathrm{RL}}(\\boldsymbol{\\theta})=\\mathbb{E}_{\\boldsymbol{\\tau}}\\left[r(G_{0})\\sum_{t=1}^{T}\\nabla_{\\boldsymbol{\\theta}}\\log p_{\\boldsymbol{\\theta}}(G_{t-1}|G_{t})\\right],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the generation trajectory $\\tau$ follows the parametric reverse process $p_{\\theta}(\\pmb{G}_{0:T})$ . ", "page_idx": 4}, {"type": "text", "text": "4.2 Learning Graph DPMs with Policy Gradient ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The policy gradient $\\nabla_{\\theta}\\mathcal{I}_{\\mathrm{RL}}(\\theta)$ in Eq. (7) is generally intractable and an efficient estimation is necessary. In a related setting centered on continuous-variable DPMs for image generation, DDPO [6] estimates the policy gradient $\\nabla_{\\theta}\\mathcal{I}_{\\mathrm{RL}}(\\theta)$ with REINFORCE and achieves great results. This motivates us to also try REINFORCE on graph DPMs, i.e., to approximate Eq. (7) with a Monte Carlo estimation: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\nabla_{\\boldsymbol{\\theta}}\\mathcal{J}_{\\mathrm{RL}}\\approx\\frac{1}{K}\\sum_{k=1}^{K}\\frac{T}{|\\mathcal{T}_{k}|}\\sum_{t\\in\\mathcal{T}_{k}}{r(G_{0}^{(k)})\\nabla_{\\boldsymbol{\\theta}}\\log p_{\\boldsymbol{\\theta}}(G_{t-1}^{(k)}|G_{t}^{(k)})},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "rwahnedroe $\\{G_{0:T}^{(k)}\\}_{k=1}^{K}$ tiarme $K$ tprsa j(ewchtiocrihe as vsoaidm spluemd mfriongm $p_{\\theta}(\\pmb{G}_{0:T})$ aenstde $\\{\\mathcal{T}_{k}\\subset[1,T]\\}_{k=1}^{K}$ haer ee sutinimfaotrimonl)y. ", "page_idx": 4}, {"type": "text", "text": "However, we empirically observe that it rarely converges on graph DPMs. To investigate this, we design a toy experiment, where the reward signal is whether $G_{0}$ is connected. The graph DPMs are randomly initialized and optimized using Eq. (8). We refer to this setting as DDPO. Fig. 2 depicts the learning curves, where the horizontal axis represents the number of queries to the reward signal and the vertical axis represents the average reward. The results demonstrate that DDPO fails to converge to a high reward signal area when generating graphs with more than 4 nodes. Furthermore, as the number of nodes increases, the fluctuation of the learning curves grows significantly. This implies that DDPO is essentially unable to optimize properly on randomly initialized models. We conjecture that the failure is due to the vast space constituted by discrete graph trajectories and the well-known high variance issue of REINFORCE [58]. A straightforward method to reduce variance is to sample more trajectories. However, this is typically expensive in DPMs, as each trajectory requires multiple rounds of model inference. Moreover, evaluating the reward signals of additional trajectories also incurs high computational costs, such as drug simulation [48]. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "This prompts us to delve deeper at a micro level. Since the policy gradient estimation in Eq. (8) is a weighted summation of gradients, we first inspect each summand term $\\nabla_{\\theta}\\log p_{\\theta}(G_{t-1}|\\dot{G_{t}})$ . With the parameterization Eq. (1) described in Sec. 3.1, it has the following form: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}\\log p_{\\theta}(G_{t-1}|G_{t})=\\frac{1}{p_{\\theta}(G_{t-1}|G_{t})}\\sum_{\\widetilde{G}_{0}}\\underbrace{q(G_{t-1}|G_{t},\\widetilde{G}_{0})}_{\\mathrm{weight}}\\underbrace{\\nabla_{\\theta}p_{\\theta}(\\widetilde{G}_{0}|G_{t})}_{\\mathrm{gradient}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where we can view the \u201cweight\u201d term as a weight assigned to the gradient $\\nabla_{\\theta}p_{\\theta}(\\widetilde{G}_{0}|G_{t})$ , and thus $\\nabla_{\\theta}\\log p_{\\theta}(\\mathbf{G}_{t-1}|\\mathbf{G}_{t})$ as a weighted sum of such gradients, with $\\widetilde{G}_{0}$ taken over all possible graphs. Intuitively, the gradient $\\nabla_{\\theta}p_{\\theta}(\\widetilde{G}_{0}|G_{t})$ promotes the probability of predicting $\\widetilde{G}_{0}$ from $G_{t}$ . Note, however, that the weight $q(G_{t-1}|G_{t},\\widetilde{G}_{0})$ is completely independent of $r(\\widetilde{G}_{0})$ and could assign large weight for $\\widetilde{G}_{0}$ that has low reward. Since the weighted sum in Eq. (9) can be dominated by gradient terms with large $q(G_{t-1}|G_{t},\\widetilde{G}_{0})$ , given a particular sampled trajectory, it is fairly possible that $\\nabla_{\\theta}\\log p_{\\theta}(\\mathbf{G}_{t-1}|\\mathbf{G}_{t})$ increases the probabilities of predicting undesired $\\widetilde{G}_{0}$ with low rewards from $\\pmb{G}_{t}$ . This explains why Eq. (8) tends to produce fluctuating and unreliabl e policy gradient estimates when the number of Monte Carlo samples (i.e., $K$ and $|\\mathcal{T}_{k}|)$ is limited. To further analyze why DDPO does not yield satisfactory results, we present additional findings in Appendix A.5. Besides, we discuss the impact of importance sampling techniques in the same section. ", "page_idx": 5}, {"type": "text", "text": "4.3 Graph Diffusion Policy Optimization ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To address the above issues, we suggest a slight modification to Eq. (8) and obtain a new policy gradient denoted as $\\pmb{g}(\\theta)$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pmb{g}(\\theta)\\triangleq\\frac{1}{K}\\sum_{k=1}^{K}\\frac{T}{|\\mathcal{T}_{k}|}\\sum_{t\\in\\mathcal{T}_{k}}{r(\\pmb{G}_{0}^{(k)})\\nabla_{\\theta}\\log p_{\\theta}(\\pmb{G}_{0}^{(k)}|\\pmb{G}_{t}^{(k)})},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "which we refer to as the eager policy gradient. Intuitively, although the number of possible graph trajectories is tremendous, if we partition them into different equivalence classes according to $G_{0}$ , where trajectories with the same $\\pmb{G}_{0}$ are considered equivalent, then the number of these equivalence classes will be much smaller than the number of graph trajectories. The optimization over these equivalence classes will be much easier than optimizing in the entire trajectory space. ", "page_idx": 5}, {"type": "text", "text": "Technically, by replacing the summand gradient term $\\nabla_{\\theta}\\log p_{\\theta}(\\mathbf{G}_{t-1}|\\mathbf{G}_{t})$ with $\\nabla_{\\theta}\\log p_{\\theta}(G_{0}|G_{t})$ in Eq. (8), we skip the weighted sum in Eq. (9) and directly promotes the probability of predicting $\\pmb{G}_{0}$ which has higher reward from $\\pmb{G}_{t}$ at all timestep $t$ . As a result, our estimation does not focus on how $\\pmb{G}_{t}$ changes to $G_{t-1}$ within the trajectory; instead, it aims to force the model\u2019s generated results to be close to the desired $G_{\\mathrm{0}}$ , which can be seen as optimizing in equivalence classes. While being a biased estimator of the policy gradient $\\nabla_{\\theta}\\mathcal{I}_{\\mathrm{RL}}(\\theta)$ , the eager policy gradient consistently leads to more stable learning and better performance than DDPO, as demonstrated in Fig. 2. We present the resulting method in Fig. 1 and Algorithm 1, naming it Graph Diffusion Policy Optimization (GDPO). ", "page_idx": 5}, {"type": "text", "text": "5 Reward Functions for Graph Generation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this work, we study both general graph and molecule reward signals that are crucial in real-world tasks. Below, we elaborate on how we formulate diverse reward signals as numerical functions. ", "page_idx": 5}, {"type": "text", "text": "5.1 Reward Functions for General Graph Generation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Validity. For graph generation, a common objective is to generate a specific type of graph. For instance, one might be interested in graphs that can be drawn without edges crossing each other [43]. ", "page_idx": 5}, {"type": "table", "img_path": "8ohsbxw7q8/tmp/66ca7dabbe8fb864373d09d245c8c2b827ad13842ed546b843d154aeab30e162.jpg", "table_caption": ["Table 1: General graph generation on SBM and Planar datasets. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "For such objectives, the reward function $r_{\\mathrm{val}}(\\cdot)$ is then formulated as binary, with $r_{\\mathrm{val}}(G_{0})\\,\\triangleq\\,1$ indicating that the generated graph $\\pmb{G}_{0}$ conforms to the specified type; otherwise, $r_{\\mathrm{val}}(G_{0})\\triangleq0$ . ", "page_idx": 6}, {"type": "text", "text": "Similarity. In certain scenarios, the objective is to generate graphs that resemble a known set of graphs $\\mathcal{D}$ at the distribution level, based on a pre-defined distance metric $d(\\cdot,\\cdot)$ between sets of graphs. As an example, the $D e g(\\mathcal{G},\\mathcal{D})$ [38] measures the maximum mean discrepancy (MMD) [18] between the degree distributions of a set $\\mathcal{G}$ of generated graphs and the given graphs $\\mathcal{D}$ . Since our method requires a reward for each single generated graph $G_{0}$ , we simply adopt $D e g(\\{G_{0}\\},D)$ as the signal. As the magnitude of reward is critical for policy gradients [58], we define $r_{\\mathrm{deg}}(G_{0})\\triangleq$ $\\exp(-D e g(\\{G_{0}\\},{\\mathcal D})^{2}/\\sigma^{2})$ , where the $\\sigma$ controls the reward distribution, ensuring that the reward lies within the range of 0 to 1. The other two similar distance metrics are $C l u s(\\mathcal{G},\\mathcal{D})$ and $O r b(\\mathcal{G},\\mathcal{D})$ , which respectively measure the distances between two sets of graphs in terms of the distribution of clustering coefficients [55] and the distribution of substructures [1]. Based on the two metrics, we define two reward signals analogous to $r_{\\mathrm{deg}}$ , namely, $r_{\\mathrm{clus}}$ and $r_{\\mathrm{orb}}$ . ", "page_idx": 6}, {"type": "text", "text": "5.2 Reward Functions for Molecular Graph Generation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Novelty. A primary objective of molecular graph generation is to discover novel drugs with desired therapeutic potentials. Due to drug patent restrictions, the novelty of generated molecules has paramount importance. The Tanimoto similarity [4], denoted as $J(\\cdot,\\cdot)$ , measures the chemical similarity between two molecules, defined by the Jaccard index of molecule fingerprint bits. Specifically, $J\\in[0,1]$ , and $J(G_{0},G_{0}^{\\prime})\\,=\\,1$ indicates that two molecules $G_{0}$ and $G_{0}^{\\prime}$ have identical fingerprints. Following Xie et al. [65], we define the novelty of a generated graph $G_{0}$ as $\\begin{array}{r}{N O V(G_{0})\\triangleq1-\\operatorname*{max}_{G_{0}^{\\prime}\\in\\mathcal{D}}J(G_{0},G_{0}^{\\prime})}\\end{array}$ , i.e., the similarity gap between $G_{0}$ and its nearest neighbor in the training dataset $\\mathcal{D}$ , and further define $r_{\\mathrm{Nov}}(G_{0})\\triangleq N O V(G_{0})$ . ", "page_idx": 6}, {"type": "text", "text": "Drug-Likeness. Regarding the efficacy of molecular graph generation in drug design, a critical indicator is the binding affinity between the generated drug candidate and a target protein. The docking score [10], denoted as $D S(\\cdot)$ , estimates the binding energy (in kcal/mol) between the ligand and the target protein through physical simulations in 3D space. Following Lee et al. [37], we clip the docking score in the range $[-20,0]$ and define the reward function as $r_{\\mathrm{DS}}(G_{0})\\triangleq-D S(G_{0})/20$ ", "page_idx": 6}, {"type": "text", "text": "Another metric is the quantitative estimate of drug-likeness $Q E D(\\cdot)$ , which measures the chemical properties to gauge the likelihood of a molecule being a successful drug [5]. As it takes values in the range $[0,1]$ , we adopt $r_{\\mathrm{QED}}(G_{0})\\triangleq\\mathbb{I}[Q E D(G_{0})>0.5]$ . ", "page_idx": 6}, {"type": "text", "text": "Synthetic Accessibility. The synthetic accessibility [7] $S A(\\cdot)$ evaluates the inherent difficulty in synthesizing a chemical compound, with values in the range from 1 to 10. We follow Lee et al. [37] and use a normalized version as the reward function: $r_{\\mathrm{SA}}(G_{0})\\triangleq(10-S A(G_{0}))/9$ . ", "page_idx": 6}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we first examine the performance of GDPO on both general graph generation tasks and molecular graph generation tasks. Then, we conduct several ablation studies to investigate the effectiveness of GDPO\u2019s design. Our code can be found in the supplementary material. ", "page_idx": 6}, {"type": "table", "img_path": "8ohsbxw7q8/tmp/b3a6f1bc2e7e20c266cb6fbc99c9d5b8a8292da9db4d5bd8972b77a28fa0dd25.jpg", "table_caption": ["Table 2: Molecule property optimization results on ZINC250k. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "6.1 General Graph Generation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Datasets and Baselines. Following DiGress [61], we evaluate GDPO on two benchmark datasets: SBM (200 nodes) and Planar (64 nodes), each consisting of 200 graphs. We compare GDPO with GraphRNN [69], SPECTRE [43], GDSS [31], MOOD [37] and DiGress. The first two models are based on RNN and GAN, respectively. The remaining methods are graph DPMs, and MOOD employs an additional property predictor. We also test DDPO [6], i.e., graph DPMs optimized with Eq. (8). ", "page_idx": 7}, {"type": "text", "text": "Implementation. We set $T=1000$ , $|\\mathcal{T}|=200$ , and $N=100$ . The number of trajectory samples $K$ is 64 for SBM and 256 for Planar. We use a DiGress model with 10 layers. More implementation details can be found in Appendix A.1. ", "page_idx": 7}, {"type": "text", "text": "Metrics and Reward Functions. We consider four metrics: $D e g(\\mathcal{G},\\mathcal{D}_{t e s t})$ , $C l u s(\\mathcal{G},\\mathcal{D}_{t e s t})$ , $O r b(\\mathcal{G},\\mathcal{D}_{t e s t})$ , and the $V U.N$ metrics. $V U.N$ measures the proportion of generated graphs that are valid, unique, and novel. The reward function is defined as follows: ", "page_idx": 7}, {"type": "equation", "text": "$$\nr_{\\mathrm{general}}=0.1\\times(r_{\\mathrm{deg}}+r_{\\mathrm{clus}}+r_{\\mathrm{orb}})+0.7\\times r_{\\mathrm{val}},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where we do not explicitly incorporate uniqueness and novelty. All rewards are calculated on the training dataset if a reference graph set is required. All evaluation metrics are calculated on the test dataset. More details about baselines, reward signals, and metrics are in Appendix A.3. ", "page_idx": 7}, {"type": "text", "text": "Results. Table 1 summarizes GDPO\u2019s superior performance in general graph generation, showing notable improvements in Deg and $V U.N$ across both SBM and Planar datasets. On the Planar dataset, GDPO significantly reduces distribution distance, achieving an $\\mathbf{81.97\\%}$ average decrease in metrics of Deg, Clus, and $O r b$ compared to DiGress (the best baseline method). For the SBM dataset, GDPO has a $\\mathbf{41.64\\%}$ average improvement. The low distributional distances to the test dataset suggests that GDPO accurately captures the data distribution with well-designed rewards. Moreover, we observe that our method outperforms DDPO by a large margin, primarily because the graphs in Planar and SBM contain too many nodes, which aligns with the observation in Fig. 2. ", "page_idx": 7}, {"type": "text", "text": "6.2 Molecule Property Optimization ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Datasets and Baselines. Molecule property optimization aims to generate molecules with desired properties. We evaluate our method on two large molecule datasets: ZINC250k [27] and MOSES [49]. The ZINC250k dataset comprises 249,456 molecules, each containing 9 types of atoms, with a maximum node count of 38; the MOSES dataset consists of 1,584,663 molecules, with 8 types of atoms and a maximum node count of 30. We compare GDPO with several leading methods: ", "page_idx": 7}, {"type": "table", "img_path": "8ohsbxw7q8/tmp/9fb5d765bb7a9c23fb36e1ace2d4be391bec9767abbb8aba727ac31de47867c5.jpg", "table_caption": ["Table 3: Molecule property optimization results on MOSES. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "GCPN [68], REINVENT [47], FREED [67] and MOOD [37]. GCPN, REINVENT and FREED are RL methods that search in the chemical environment. MOOD, based on graph DPMs, employs a property predictor for guided sampling. Similar to general graph generation, we also compare our method with DiGress and DDPO. Besides, we show the performance of DiGress with property predictors, termed as DiGress-guidance. ", "page_idx": 8}, {"type": "text", "text": "Implementation. We set $T=500$ , $\\left\\vert\\mathcal{T}\\right\\vert=100$ , $N=100$ , and $K=256$ for both datasets. We use the same model structure with DiGress. See more details in Appendix A.1. ", "page_idx": 8}, {"type": "text", "text": "Metrics and Reward Functions. Following MOOD, we consider two metrics essential for real-world novel drug discovery: Novel hit ratio $(\\%)$ and Novel top $5\\%$ docking score, denoted as Hit Ratio and $D S$ (top $5\\%$ ), respectively. Using the notations from Sec. 5.2, the Hit Ratio is the proportion of unique generated molecules that satisfy: $D S<$ median $D S$ of the known effective molecules, $N O V>$ 0.6, $Q E D>0.5$ , and $S A<5$ . The $D S$ (top $5\\%$ ) is the average $D S$ of the top $5\\%$ molecules (ranked by $D S)$ that satisfy: $N O V>0.6$ , $Q E D>0.5$ , and $S A<5$ . Since calculating ${D S}$ requires specifying a target protein, we set five different protein targets to fully test GDPO: parp1, fa7, 5ht1b, braf, and jak2. The reward function for molecule property optimization is defined as follows: ", "page_idx": 8}, {"type": "equation", "text": "$$\nr_{\\mathrm{molecule}}=0.1\\times\\left(r_{\\mathrm{QED}}+r_{\\mathrm{SA}}\\right)+0.3\\times r_{\\mathrm{Nov}}+0.5\\times r_{\\mathrm{DS}}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "We do not directly use Hit Ratio and $D S$ (top $5\\%$ ) as rewards in consideration of method generality. The reward weights are determined through several rounds of search, and we find that assigning a high weight to $r_{\\mathrm{NOV}}$ leads to training instability, which is discussed in Sec. 6.3. More details about the experiment settings are discussed in Appendix A.4. ", "page_idx": 8}, {"type": "text", "text": "Results. In Table 2, GDPO shows significant improvement on ZINC250k, especially in the Hit Ratio. A higher Hit Ratio means the model is more likely to generate valuable new drugs, and GDPO averagely improves the Hit Ratio by $5.72\\%$ in comparison with other SOTA methods. For $D S$ (top $5\\%$ ), GDPO also has a $1.48\\%$ improvement on average. Discovering new drugs on MOSES is much more challenging than on ZINC250k due to its vast training dataset. In Table 3, GDPO also shows promising results on MOSES. Despite a less favorable Hit Ratio on 5ht1b, GDPO achieves an average improvement of $12.94\\%$ on the other four target proteins. For $D S$ (top $5\\%$ ), GDPO records an average improvement of $5.54\\%$ compared to MOOD, showing a big improvement in drug efficacy. ", "page_idx": 8}, {"type": "text", "text": "6.3 Generalizability, Sample Efficiency, and A Failure Case ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To validate whether GDPO correctly optimizes the model, we test the performance of GDPO on metrics not used in the reward signal. In Table 4, we evaluate the performance on Spectral MMD [43], where the GDPO is optimized by Eq. (11). The results demonstrate that GDPO does ", "page_idx": 8}, {"type": "table", "img_path": "8ohsbxw7q8/tmp/3424f32600ad69d1e34704f8d3d11d0d03525e72afc5d1db4834b87537f5de98.jpg", "table_caption": ["Table 4: Generalizability of GDPO on Spectral MMD. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "not show overftiting; instead, it finds a more powerful model. The results presented in Appendix A.5 further support that GDPO can attain high sample novelty and diversity. ", "page_idx": 9}, {"type": "text", "text": "We then investigate two crucial factors for GDPO: 1) the number of trajectories; 2) the selection of the reward signals. We test our method on ZINC250k and set the target proteins as 5ht1b. In Fig. 3 (a), the results indicate that GDPO exhibits good sampling efficiency, as it achieves a significant improvement in average reward by querying only 10k molecule reward signals, which is much less than the number of molecules contained in ZINC250k. Moreover, the sample efficiency can be further improved by reducing the number of trajectories, but this may lead to training instability. To achieve consistent results, we use 256 trajectories. In Fig. 3 (b), we illustrate a failure case of GDPO when assigning a high weight to $r_{\\mathrm{NOV}}$ . Gen", "page_idx": 9}, {"type": "image", "img_path": "8ohsbxw7q8/tmp/796d7c135d51e24e99f5f86991ed775981c2c79eea557cb0cd58ef1534461f37.jpg", "img_caption": [], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Figure 3: We investigate two key factors of GDPO on ZINC250k, with the target protein being $5h t I b$ . Similarly, the vertical axis represents the total queries, while the horizontal axis represents the average reward.(a) We vary the number of trajectories for gradient estimation. (b) We fix the weight of $r_{\\mathrm{QED}}$ and $r_{\\mathrm{SA}}$ , and change the weight of $r_{\\mathrm{NOV}}$ while ensuring the total weight is 1. ", "page_idx": 9}, {"type": "text", "text": "erating novel samples is challenging. MOOD [37] addresses this challenge by controlling noise in the sampling process, whereas we achieve it by novelty optimization. However, assigning a large weight to $r_{\\mathrm{NOV}}$ can lead the model to rapidly degenerate. One potential solution is to gradually increase the weight and conduct multi-stage optimization. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We introduce GDPO, a novel policy gradient method for learning graph DPMs that effectively addresses the problem of graph generation under given objectives. Evaluation results on both general and molecular graphs indicate that GDPO is compatible with complex multi-objective optimization and achieves state-of-the-art performance on a series of representative graph generation tasks. We discuss some limitations of our work in Appendix A.2. Our future work will investigate the theoretical gap between GDPO and DDPO in order to obtain effective unbiased estimators. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgment ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is supported by the Zhejiang Provincial Natural Science Foundation of China (LD24F020011) and \u201cPioneer and Leading Goose\u201d R&D Program of Zhejiang (2024C01167). Chongxuan Li was supported by Beijing Natural Science Foundation (L247030); Beijing Nova Program (No. 20230484416). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Nesreen K Ahmed, Jennifer Neville, Ryan A Rossi, and Nick Duffield. Efficient graphlet counting for large networks. In 2015 IEEE international conference on data mining, pages 1\u201310. IEEE, 2015.   \n[2] Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Structured denoising diffusion models in discrete state-spaces. ArXiv, abs/2107.03006, 2021.   \n[3] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, T. J. Henighan, Nicholas Joseph, Saurav Kadavath, John Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom B. Brown, Jack Clark, Sam McCandlish, Christopher Olah, Benjamin Mann, and Jared Kaplan. Training a helpful and harmless assistant with reinforcement learning from human feedback. ArXiv, abs/2204.05862, 2022.   \n[4] D\u00e1vid Bajusz, Anita R\u00e1cz, and K\u00e1roly H\u00e9berger. Why is tanimoto index an appropriate choice for fingerprint-based similarity calculations? Journal of Cheminformatics, 7, 2015.   \n[5] G. Richard J. Bickerton, Gaia V. Paolini, J\u00e9r\u00e9my Besnard, Sorel Muresan, and Andrew L. Hopkins. Quantifying the chemical beauty of drugs. Nature chemistry, 4 2:90\u20138, 2012.   \n[6] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. ArXiv, abs/2305.13301, 2023.   \n[7] Krisztina Boda, Thomas Seidel, and Johann Gasteiger. Structure and reaction based evaluation of synthetic accessibility. Journal of Computer-Aided Molecular Design, 21:311\u2013325, 2007.   \n[8] Marc Brockschmidt, Miltiadis Allamanis, Alexander L. Gaunt, and Oleksandr Polozov. Generative code modeling with graphs. ArXiv, abs/1805.08490, 2018.   \n[9] Nicola De Cao and Thomas Kipf. Molgan: An implicit generative model for small molecular graphs. ArXiv, abs/1805.11973, 2018.   \n[10] Tobiasz Ciepli\u00b4nski, Tomasz Danel, Sabina Podlewska, and Stanislaw Jastrzebski. Generative models should at least be able to design molecules that dock well: A new benchmark. Journal of Chemical Information and Modeling, 63:3238 \u2013 3247, 2020.   \n[11] Kevin Clark, Paul Vicol, Kevin Swersky, and David J. Fleet. Directly fine-tuning diffusion models on differentiable rewards. ArXiv, abs/2309.17400, 2023.   \n[12] Prafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image synthesis. ArXiv, abs/2105.05233, 2021.   \n[13] Jerome Eberhardt, Diogo Santos-Martins, Andreas F Tillack, and Stefano Forli. Autodock vina 1.2. 0: New docking methods, expanded force field, and python bindings. Journal of chemical information and modeling, 61(8):3891\u20133898, 2021.   \n[14] Peter Eckmann, Kunyang Sun, Bo Zhao, Mudong Feng, Michael K. Gilson, and Rose Yu. Limo: Latent inceptionism for targeted molecule generation. Proceedings of machine learning research, 162:5777\u20135792, 2022.   \n[15] Paul L. Erdos and Alfr\u00e9d R\u00e9nyi. On the evolution of random graphs. Transactions of the American Mathematical Society, 286:257\u2013257, 1984.   \n[16] Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, P. Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Dpok: Reinforcement learning for fine-tuning text-to-image diffusion models. ArXiv, abs/2305.16381, 2023.   \n[17] Eugene A Feinberg and Adam Shwartz. Handbook of Markov decision processes: methods and applications, volume 40. Springer Science & Business Media, 2012.   \n[18] Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Scholkopf, and Alex Smola. A kernel two-sample test. J. Mach. Learn. Res., 13:723\u2013773, 2012.   \n[19] Ivo Grondman, Lucian Busoniu, Gabriel AD Lopes, and Robert Babuska. A survey of actorcritic reinforcement learning: Standard and natural policy gradients. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), 42(6):1291\u20131307, 2012.   \n[20] Aditya Grover, Aaron Zweig, and Stefano Ermon. Graphite: Iterative generative modeling of graphs. In International Conference on Machine Learning, 2018.   \n[21] Xiaojie Guo and Liang Zhao. A systematic survey on deep generative models for graph generation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45:5370\u20135390, 2020.   \n[22] Frank Harary and C. St. J. A. Nash-Williams. On eulerian and hamiltonian graphs and line graphs. Canadian Mathematical Bulletin, 8:701 \u2013 709, 1965.   \n[23] Arman Hasanzadeh, Ehsan Hajiramezanali, Nick G. Duffield, Krishna R. Narayanan, Mingyuan Zhou, and Xiaoning Qian. Semi-implicit graph variational auto-encoders. ArXiv, abs/1908.07078, 2019.   \n[24] Hado Hasselt. Double q-learning. Advances in neural information processing systems, 23, 2010.   \n[25] Jonathan Ho, Ajay Jain, and P. Abbeel. Denoising diffusion probabilistic models. ArXiv, abs/2006.11239, 2020.   \n[26] Paul Holland, Kathryn B. Laskey, and Samuel Leinhardt. Stochastic blockmodels: First steps. Social Networks, 5:109\u2013137, 1983.   \n[27] John J. Irwin, T. Sterling, Michael M. Mysinger, Erin S. Bolstad, and Ryan G. Coleman. Zinc: A free tool to discover chemistry for biology. Journal of Chemical Information and Modeling, 52:1757 \u2013 1768, 2012.   \n[28] Wengong Jin, Regina Barzilay, and T. Jaakkola. Hierarchical generation of molecular graphs using structural motifs. In International Conference on Machine Learning, 2020.   \n[29] Wengong Jin, Regina Barzilay, and T. Jaakkola. Multi-objective molecule generation using interpretable substructures. In International Conference on Machine Learning, 2020.   \n[30] Jaehyeong Jo, Dongki Kim, and Sung Ju Hwang. Graph generation with diffusion mixture. arXiv preprint arXiv:2302.03596, 2023.   \n[31] Jaehyeong Jo, Seul Lee, and Sung Ju Hwang. Score-based generative modeling of graphs via the system of stochastic differential equations. In International Conference on Machine Learning, 2022.   \n[32] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. ArXiv, abs/2206.00364, 2022.   \n[33] Sarah L. Kinnings, Nina Liu, Peter J. Tonge, Richard M. Jackson, Lei Xie, and Philip E. Bourne. A machine learning-based method to improve docking scoring functions and its application to drug repurposing. Journal of chemical information and modeling, 51 2:408\u201319, 2011.   \n[34] Thomas Kipf and Max Welling. Variational graph auto-encoders. ArXiv, abs/1611.07308, 2016.   \n[35] Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, P. Abbeel, Mohammad Ghavamzadeh, and Shixiang Shane Gu. Aligning text-to-image models using human feedback. ArXiv, abs/2302.12192, 2023.   \n[36] Myeong-Sung Lee and Kyoungmin Min. Mgcvae: Multi-objective inverse design via molecular graph conditional variational autoencoder. Journal of chemical information and modeling, 2022.   \n[37] Seul Lee, Jaehyeong Jo, and Sung Ju Hwang. Exploring chemical space with score-based out-of-distribution generation. ArXiv, abs/2206.07632, 2022.   \n[38] Renjie Liao, Yujia Li, Yang Song, Shenlong Wang, Will Hamilton, David K Duvenaud, Raquel Urtasun, and Richard Zemel. Efficient graph generation with graph recurrent attention networks. Advances in neural information processing systems, 32, 2019.   \n[39] Renjie Liao, Yujia Li, Yang Song, Shenlong Wang, Charlie Nash, William L. Hamilton, David Kristjanson Duvenaud, Raquel Urtasun, and Richard S. Zemel. Efficient graph generation with graph recurrent attention networks. In Neural Information Processing Systems, 2019.   \n[40] Jenny Liu, Aviral Kumar, Jimmy Ba, Jamie Ryan Kiros, and Kevin Swersky. Graph normalizing flows. ArXiv, abs/1905.13177, 2019.   \n[41] Qi Liu, Miltiadis Allamanis, Marc Brockschmidt, and Alexander L. Gaunt. Constrained graph variational autoencoders for molecule design. In Neural Information Processing Systems, 2018.   \n[42] Youzhi Luo, Keqiang Yan, and Shuiwang Ji. Graphdf: A discrete flow model for molecular graph generation. In International Conference on Machine Learning, 2021.   \n[43] Karolis Martinkus, Andreas Loukas, Nathanael Perraudin, and Roger Wattenhofer. Spectre : Spectral conditioning helps to overcome the expressivity limits of one-shot graph generators. In International Conference on Machine Learning, 2022.   \n[44] Duc Duy Nguyen and Guowei Wei. Agl-score: Algebraic graph learning score for proteinligand binding scoring, ranking, docking, and screening. Journal of chemical information and modeling, 2019.   \n[45] Khanh Nguyen, Hal Daum\u00e9, and Jordan L. Boyd-Graber. Reinforcement learning for bandit neural machine translation with simulated human feedback. ArXiv, abs/1707.07402, 2017.   \n[46] Chenhao Niu, Yang Song, Jiaming Song, Shengjia Zhao, Aditya Grover, and Stefano Ermon. Permutation invariant graph generation via score-based generative modeling. In International Conference on Artificial Intelligence and Statistics, 2020.   \n[47] Marcus Olivecrona, Thomas Blaschke, Ola Engkvist, and Hongming Chen. Molecular de-novo design through deep reinforcement learning. Journal of Cheminformatics, 9, 2017.   \n[48] Nataraj Sekhar Pagadala, Khajamohiddin Syed, and Jack Adam Tuszynski. Software for molecular docking: a review. Biophysical Reviews, 9:91 \u2013 102, 2017.   \n[49] Daniil Polykovskiy, Alexander Zhebrak, Benjam\u00edn S\u00e1nchez-Lengeling, Sergey Golovanov, Oktai Tatanov, Stanislav Belyaev, Rauf Kurbanov, Aleksey Anatolievich Artamonov, Vladimir Aladinskiy, Mark Veselov, Artur Kadurin, Sergey I. Nikolenko, Al\u00e1n Aspuru-Guzik, and Alex Zhavoronkov. Molecular sets (moses): A benchmarking platform for molecular generation models. Frontiers in Pharmacology, 11, 2018.   \n[50] Mihir Prabhudesai, Anirudh Goyal, Deepak Pathak, and Katerina Fragkiadaki. Aligning text-toimage diffusion models with reward backpropagation. ArXiv, abs/2310.03739, 2023.   \n[51] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36, 2024.   \n[52] Davide Rigoni, Nicol\u00f3 Navarin, and Alessandro Sperduti. Conditional constrained graph variational autoencoders for molecule design. 2020 IEEE Symposium Series on Computational Intelligence (SSCI), pages 729\u2013736, 2020.   \n[53] Chence Shi, Minkai Xu, Zhaocheng Zhu, Weinan Zhang, Ming Zhang, and Jian Tang. Graphaf: a flow-based autoregressive model for molecular graph generation. ArXiv, abs/2001.09382, 2020.   \n[54] Martin Simonovsky and Nikos Komodakis. Graphvae: Towards generation of small graphs using variational autoencoders. In International Conference on Artificial Neural Networks, 2018.   \n[55] Sara Nadiv Soffer and Alexei Vazquez. Network clustering coefficient without degreecorrelation biases. Physical Review E, 71(5):057101, 2005.   \n[56] Jascha Narain Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. ArXiv, abs/1503.03585, 2015.   \n[57] Yang Song, Jascha Narain Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. ArXiv, abs/2011.13456, 2020.   \n[58] Richard S. Sutton and Andrew G. Barto. Reinforcement learning: An introduction. IEEE Trans. Neural Networks, 9:1054\u20131054, 1998.   \n[59] Richard S. Sutton, David A. McAllester, Satinder Singh, and Y. Mansour. Policy gradient methods for reinforcement learning with function approximation. In Neural Information Processing Systems, 1999.   \n[60] Oleg Trott and Arthur J. Olson. Autodock vina: Improving the speed and accuracy of docking with a new scoring function, efficient optimization, and multithreading. Journal of Computational Chemistry, 31, 2009.   \n[61] Cl\u00e9ment Vignac, Igor Krawczuk, Antoine Siraudin, Bohan Wang, Volkan Cevher, and Pascal Frossard. Digress: Discrete denoising diffusion for graph generation. ArXiv, abs/2209.14734, 2022.   \n[62] Hongwei Wang, Jia Wang, Jialin Wang, Miao Zhao, Weinan Zhang, Fuzheng Zhang, Xing Xie, and Minyi Guo. Graphgan: Graph representation learning with generative adversarial nets. ArXiv, abs/1711.08267, 2017.   \n[63] Yulun Wu, Mikaela Cashman, Nicholas Choma, Erica T Prates, Ver\u00f3nica G Melesse Vergara, Manesh Shah, Andrew Chen, Austin Clyde, Thomas S Brettin, Wibe A de Jong, et al. Spatial graph attention and curiosity-driven policy for antiviral drug discovery. arXiv preprint arXiv:2106.02190, 2021.   \n[64] Saining Xie, Alexander Kirillov, Ross B. Girshick, and Kaiming He. Exploring randomly wired neural networks for image recognition. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 1284\u20131293, 2019.   \n[65] Yutong Xie, Chence Shi, Hao Zhou, Yuwei Yang, Weinan Zhang, Yong Yu, and Lei Li. Mars: Markov molecular sampling for multi-objective drug discovery. ArXiv, abs/2103.10432, 2021.   \n[66] Carl Yang, Peiye Zhuang, Wenhan Shi, Alan Luu, and Pan Li. Conditional structure generation through graph variational generative adversarial nets. In Neural Information Processing Systems, 2019.   \n[67] Soojung Yang, Doyeong Hwang, Seul Lee, Seongok Ryu, and Sung Ju Hwang. Hit and lead discovery with explorative rl and fragment-based molecule generation. ArXiv, abs/2110.01219, 2021.   \n[68] Jiaxuan You, Bowen Liu, Rex Ying, Vijay S. Pande, and Jure Leskovec. Graph convolutional policy network for goal-directed molecular graph generation. In Neural Information Processing Systems, 2018.   \n[69] Jiaxuan You, Rex Ying, Xiang Ren, William L. Hamilton, and Jure Leskovec. Graphrnn: Generating realistic graphs with deep auto-regressive models. In International Conference on Machine Learning, 2018.   \n[70] Mengchun Zhang, Maryam Qamar, Taegoo Kang, Yuna Jung, Chenshuang Zhang, Sung-Ho Bae, and Chaoning Zhang. A survey on graph diffusion models: Generative ai in science for molecule, protein and material. ArXiv, abs/2304.01565, 2023.   \n[71] Zhenpeng Zhou, Steven Kearnes, Li Li, Richard N Zare, and Patrick Riley. Optimization of molecules via deep reinforcement learning. Scientific reports, 9(1):10752, 2019.   \n[72] Zhenpeng Zhou, Steven M. Kearnes, Li Li, Richard N. Zare, and Patrick F. Riley. Optimization of molecules via deep reinforcement learning. Scientific Reports, 9, 2018. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Experimental Details and Additional Results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Implementation Details. ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For all experiments, we use the graph transformer proposed in DiGress [61] as the graph DPMs, and the models are pre-trained on the training dataset before applying GDPO or DDPO. During fine-tuning, we keep all layers fixed except for attention, set the learning rate to 0.00001, and utilize gradient clipping to limit the gradient norm to be less than or equal to 1. In addition, due to significant numerical fluctuations during reward normalization, we follow DDPO [6] in constraining the normalized reward to the range from $[-5,5]$ . This means that gradients resulting from rewards beyond this range will not contribute to model updates. When there is insufficient memory to generate enough trajectories, we use gradient accumulation to increase the number of trajectories used for gradient estimation. We conducted all experiments on a single A100 GPU with 40GB of VRAM and an AMD EPYC 7352 24-core Processor. ", "page_idx": 14}, {"type": "text", "text": "Training time and efficiency. Training DiGress on the ZINC250k dataset using a single A100 GPU typically takes 48-72 hours, whereas fine-tuning with GDPO takes only 10 hours (excluding the time for reward function computation). This high efficiency is in line with the findings in the practice of DDPO, which is different from traditional RL methods. Additionally, as in Fig. 3 and Sec 6.3, GDPO effectively improves the average reward of the model using only 10,000 queries. This sample size is notably small compared to the 250,000 samples present in the ZINC250k dataset, showing the impressive sample efficiency of GDPO. ", "page_idx": 14}, {"type": "text", "text": "A.2 Limitations and Broader Impact. ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Below we list some limitations of the current work: ", "page_idx": 14}, {"type": "text", "text": "\u2022 Potential for overoptimization: As an RL-based approach, a recognized limitation is the risk of overoptimization, where the DPM distribution may collapse or diverge excessively from the original distribution. In Section 6.3, we demonstrated a failure case where, with a high weight on novelty in the reward function, GDPO encounters a sudden drop in reward after a period of optimization. Future research could explore the application of regularization techniques, similar to those utilized in recent works such as DPO [51], to mitigate this risk.   \n\u2022 Inherited limitations of DPMs: Our method inherits certain limitations inherent to diffusion models, particularly concerning their training and inference costs. As we do not modify the underlying model architecture, these constraints persist.   \n\u2022 Scalability to large graphs: The scalability of GDPO to larger graphs (e.g., with 500 or more nodes) remains unexplored. ", "page_idx": 14}, {"type": "text", "text": "For broader impact, this paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. ", "page_idx": 14}, {"type": "text", "text": "A.3 General Graph Generation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Baselines. There are several baseline methods for general graph generation, we summarize them as follows: ", "page_idx": 14}, {"type": "text", "text": "\u2022 GraphRNN: a deep autoregressive model designed to model and generate complex distributions over graphs. It addresses challenges like non-uniqueness and high dimensionality by decomposing the generation process into node and edge formations.   \n\u2022 SPECTRE: a novel GAN for graph generation, approaches the problem spectrally by generating dominant parts of the graph Laplacian spectrum and matching them to eigenvalues and eigenvectors. This method allows for modeling global and local graph structures directly, overcoming issues like expressivity and mode collapse.   \n\u2022 GDSS: A novel score-based generative model for graphs is introduced to tackle the task of capturing permutation invariance and intricate node-edge dependencies in graph data generation. This model employs a continuous-time framework incorporating a novel graph diffusion process, ", "page_idx": 14}, {"type": "image", "img_path": "8ohsbxw7q8/tmp/12a49c4a5c9efaaedae8d7197012f043893d9b9c567481cc3949c5800650a378.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "characterized by stochastic differential equations (SDEs), to simultaneously model distributions of nodes and edges. ", "page_idx": 15}, {"type": "text", "text": "\u2022 DiGress: DiGress is a discrete denoising diffusion model designed for generating graphs with categorical attributes for nodes and edges. It employs a discrete diffusion process to iteratively modify graphs with noise, guided by a graph transformer network. By preserving the distribution of node and edge types and incorporating graph-theoretic features, DiGress achieves state-ofthe-art performance on various datasets. ", "page_idx": 15}, {"type": "text", "text": "\u2022 MOOD: MOOD introduces Molecular Out-Of-distribution Diffusion, which employs out-ofdistribution control in the generative process without added costs. By incorporating gradients from a property predictor, MOOD guides the generation process towards molecules with desired properties, enabling the discovery of novel and valuable compounds surpassing existing methods. ", "page_idx": 15}, {"type": "text", "text": "Metrics. The metrics of general graph generations are all taken from GraphRNN [38]. The reported metrics compare the discrepancy between the distribution of certain metrics on a test set and the distribution of the same metrics on a generated graph. The metrics measured include degree distributions, clustering coefficients, and orbit counts (which measure the distribution of all substructures of size 4). Following DiGress [61], we do not report raw numbers but ratios computed as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\nr=\\mathrm{MMD}(g e n e r a t e d,t e s t)^{2}/\\,\\mathrm{MMD}(t r a i n i n g,t e s t)^{2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Besides, we explain some metrics that are used in the general graph generation: ", "page_idx": 15}, {"type": "text", "text": "\u2022 Clus: the clustering coefficient measures the tendency of nodes to form clusters in a network. Real-world networks, especially social networks, often exhibit tightly knit groups with more ties between nodes than expected by chance. There are two versions of this measure: global, which assesses overall clustering in the network, and local, which evaluates the clustering around individual nodes.   \n\u2022 Orb: Graphlets are induced subgraph isomorphism classes in a graph, where occurrences are isomorphic or non-isomorphic. They differ from network motifs, which are over- or underrepresented graphlets compared to a random graph null model. Orb will count the occurrences of each type of graphlet in a graph. Generally, if two graphs have similar numbers of graphlets, they are considered to be relatively similar. ", "page_idx": 15}, {"type": "text", "text": "A.4 Molecule Property Optimization ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Implementation Details. Following FREED [67], we selected five proteins, PARP-1 (Poly [ADPribose] polymerase-1), FA7 (Coagulation factor VII), 5-HT1B (5-hydroxytryptamine receptor 1B), ", "page_idx": 15}, {"type": "text", "text": "BRAF (Serine/threonine-protein kinase B-raf), and JAK2 (Tyrosine-protein kinase JAK2), which have the highest AUROC scores when the protein-ligand binding affinities for DUD-E ligands are approximated with AutoDock Vina [13], as the target proteins for which the docking scores are calculated. QED and SA scores are computed using the RDKit library. ", "page_idx": 16}, {"type": "text", "text": "Baselines. There are several baseline methods for molecular graph generation under the given objectives, they are diverse in methodology and performance, we summarize them as follows: ", "page_idx": 16}, {"type": "text", "text": "\u2022 GCPN: Graph Convolutional Policy Network (GCPN) is a general graph convolutional networkbased model for goal-directed graph generation using reinforcement learning. The GCPN is trained to optimize domain-specific rewards and adversarial loss through policy gradient, operating within an environment that includes domain-specific rules.   \n\u2022 REINVENT: This method enhances a sequence-based generative model for molecular design by incorporating augmented episodic likelihood, enabling the generation of structures with specified properties. It successfully performs tasks such as generating analogs to a reference molecule and predicting compounds active against a specific biological target.   \n\u2022 HierVAE: a hierarchical graph encoder-decoder for drug discovery, overcoming limitations of previous approaches by using larger and more flexible graph motifs as building blocks. The encoder generates a multi-resolution representation of molecules, while the decoder adds motifs in a coarse-to-fine manner, effectively resolving attachments to the molecule.   \n\u2022 FREED: a novel reinforcement learning (RL) framework for generating effective acceptable molecules with high docking scores, crucial for drug design. FREED addresses challenges in generating realistic molecules and optimizing docking scores through a fragment-based generation method and error-prioritized experience replay (PER).   \n\u2022 MOOD: please refer to Appendix A.3. ", "page_idx": 16}, {"type": "text", "text": "Metrics. There are several metrics for evaluating the molecule properties, we summarize the meaning of these metrics as follows: ", "page_idx": 16}, {"type": "text", "text": "\u2022 Docking Score: Docking simulations aim to find the best binding mode based on scoring functions. Scoring functions in computational chemistry and molecular modeling predict binding affinity between molecules post-docking. They are commonly used for drug-protein interactions, but also for protein-protein or protein-DNA interactions. After defining the score function, we can optimize to find the optimal drug-protein matching positions and obtain the docking score.   \n\u2022 QED: Drug-likeness evaluation in drug discovery often lacks nuance, leading to potential issues with compound quality. We introduce QED, a measure based on desirability, which considers the distribution of molecular properties and allows the ranking of compounds by relative merit. QED is intuitive, transparent, and applicable to various settings. We extend its use to assess molecular target druggability and suggest it may reflect aesthetic considerations in medicinal chemistry.   \n\u2022 SA: a scoring method for rapid evaluation of synthetic accessibility, considering structural complexity, similarity to available starting materials, and strategic bond assessments. These components are combined using an additive scheme, with weights determined via linear regression analysis based on medicinal chemists\u2019 accessibility scores. The calculated synthetic accessibility values align well with chemists\u2019 assessments. ", "page_idx": 16}, {"type": "text", "text": "A.5 Additional Results of the GDPO ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Table 5: General graph generation on SBM and Planar datasets with different reward signals. ", "page_idx": 16}, {"type": "table", "img_path": "8ohsbxw7q8/tmp/c63b8c45203bf12c3fae4aee3882ae0d1efbe900570bc2d07e30502e5a3b1356.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Study of the Reward Signals. In Table. 5, we showcase the performance of GDPO on Planar under different configurations of reward weights. We keep the three weights related to distance the same and adjust the weight of validity while ensuring that the sum of weights is 1. The results indicate that GDPO is not very sensitive to the weights of several reward signals for general graph generation, even though these weight configurations vary significantly, they all achieve good performance. Additionally, we found that GDPO can easily increase $V U.N$ to above 80 while experiencing slight losses in the other three indicators. When applying GDPO in practice, one can make a tradeoff between them based on the specific application requirements. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "table", "img_path": "8ohsbxw7q8/tmp/3b2ddc8a895600c3edb6a9979004814fd7233c303a357f7352a3e4a306d3145c.jpg", "table_caption": ["Table 6: Study of the Important Sampling on ZINC250k. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "The Impact of Important Sampling. The importance sampling technique in DDPO, aims to facilitate multiple steps of optimization using the same batch of trajectories. This is achieved by weighting each item on the trajectory with an importance weight derived from the density ratio estimated using the model parameters from the previous step $\\theta_{\\mathrm{prev}}$ and the current step $\\theta$ (referred to as DDPO-IS): ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}\\mathcal{J}_{\\mathrm{DDPO-IS}}(\\theta)=E_{\\tau}\\left[r(G_{0})\\sum_{t=1}^{T}\\frac{p_{\\theta}(G_{t-1}|G_{t})}{p_{\\theta_{\\mathrm{prev}}}(G_{t-1}|G_{t})}\\nabla_{\\theta}\\log p_{\\theta}(G_{t-1}|G_{t})\\right].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Our eager policy gradient, independently motivated, aims to address the high variance issue of the policy gradient in each step of optimization, as elaborated in Sec. 4.2. Intuitively, the eager policy gradient can be viewed as a biased yet significantly less fluctuating gradient estimation. ", "page_idx": 17}, {"type": "text", "text": "We conducted a series of experiments on $Z{\\tt I N C250k}$ to compare DDPO, DDPO-IS, and GDPO. The experimental setup remains consistent with the description in Section 6.2. Additionally, considering that the importance sampling technique in DDPO and our eager policy gradient appear to be orthogonal, we also explored combining them simultaneously (referred to as GDPO-IS): ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\nabla_{\\boldsymbol{\\theta}}\\mathcal{J}_{\\mathrm{GDPO-IS}}(\\boldsymbol{\\theta})=E_{\\tau}\\left[r(G_{0})\\sum_{t=1}^{T}\\frac{p_{\\boldsymbol{\\theta}}(G_{0}|G_{t})}{p_{\\boldsymbol{\\theta}_{\\mathrm{prev}}}(G_{0}|G_{t})}\\nabla_{\\boldsymbol{\\theta}}\\log p_{\\boldsymbol{\\theta}}(G_{0}|G_{t})\\right].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In Table. 6, while importance sampling enhances the performance of DDPO, consistent with the results reported in the DDPO paper, it does not yield improvements for GDPO-IS over GDPO. We speculate that this discrepancy may be due to the biasness of the eager policy gradient, rendering it incompatible with the importance sampling technique. We intend to investigate the mechanism and address this in our future work. Nevertheless, it is noteworthy that the performance of DDPO-IS remains inferior to GDPO, indicating the superiority of our proposed GDPO method. ", "page_idx": 17}, {"type": "table", "img_path": "8ohsbxw7q8/tmp/2de4b485a9c639f842834cffe88b4825f0fc7c3d7c800db83c0209a4472dcb46.jpg", "table_caption": ["Table 7: Novelty and Diversity on ZINC250k. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Novelty and Diversity of GDPO. To provide further insight into the novelty and diversity of our approach, we introduce two additional metrics: ", "page_idx": 17}, {"type": "text", "text": "\u2022 Intersection over Union (IoU): We compare two sets of molecules: 1) 500 molecules generated by GDPO (denoted as GDPO) and 2) top 500 molecules among 10,000 molecules generated by our base DPM before finetuning (denoted as TopPrior). We then compute $\\mathrm{IoU}{=}100~\\times$ $\\frac{|\\dot{\\mathrm{GDPO}}\\cap\\mathrm{TopPrior}|}{|\\mathrm{GDPO}\\cup\\mathrm{TopPrior}|}\\%$ . We report an average IoU of 5 independent runs. ", "page_idx": 17}, {"type": "text", "text": "In Table. 7, these results show that GDPO has not converged to a trivial solution, wherein it merely selects a subset of molecules generated by the prior diffusion model. Instead, GDPO has learned an effective and distinct denoising strategy from the prior diffusion model. ", "page_idx": 18}, {"type": "text", "text": "The Gap between Image DPMs and Graph DPMs. GDPO is tackling the high variance issue inherent in utilizing policy gradients on graph DPMs, as stated and discussed in Sec. 4.2. To provide clarity on what GDPO tackles, we would like to elaborate more on the high variance issue of policy gradients on graph DPMs. Consider the generation trajectories in image and graph DPMs: ", "page_idx": 18}, {"type": "text", "text": "In image DPMs, the generation process follows a (discretization of) continuous diffusion process $(\\mathbf{x}_{t})_{t\\in[0,T]}$ . The consecutive steps $\\mathbf x_{t-1}$ and $\\mathbf{x}_{t}$ are typically close due to the Gaussian reverse denoising distribution $p(\\mathbf{x}_{t-1}|\\mathbf{x}_{t})$ (typically with a small variance). ", "page_idx": 18}, {"type": "text", "text": "In graph DPMs, the generation process follows a discrete diffusion process $\\left(G_{T},\\dots,G_{0}\\right)$ , where each $G_{t}$ is a concrete sample (i.e., one-hot vectors) from categorical distributions. Therefore, consecutive steps $G_{t-1}$ and $G_{t}$ can be very distant. This makes the trajectory of graph DPMs more fluctuating than images and thus leads to a high variance of the gradient $\\nabla_{\\theta}\\log p(G_{t-1}|G_{t})$ (and the ineffectiveness of DDPO) when evaluated with same number of trajectories as in DDPO. ", "page_idx": 18}, {"type": "text", "text": "Regarding the \u201cdistance\u201d between two consecutive steps $G_{t}$ and $G_{t-1}$ , our intuition stems from the fact that graphs generation trajectories are inherently discontinuous. This means that each two consecutive steps can differ significantly, such as in the type/existence of edges. In contrast, the generation trajectories of images, governed by reverse SDEs, are continuous. This continuity implies that for fine-grained discretization (i.e., large $T$ ), $\\mathbf{x}_{t}$ and $\\mathbf{x}_{t-1}$ can be arbitrarily close to each other (in the limit case of $T\\to\\infty$ ). ", "page_idx": 18}, {"type": "image", "img_path": "8ohsbxw7q8/tmp/26eb5c8a680d07ff00a8c5e04b43da71dd9897fe6adb09dd16d9e588cb661d69.jpg", "img_caption": ["Figure 4: We investigate the L2 distance between two consecutive steps in two types of DPMs. The diffusion step is 1000 for two models. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "To provide quantitative support for this discussion, we conduct an analysis comparing the distances between consecutive steps in both image and graph DPMs. We employ a DDPM [a] pre-trained on CIFAR-10 for image diffusion and DiGress [b] pre-trained on the Planar dataset for graph diffusion, both with a total of $T\\,=\\,1000$ time steps. In these models, graphs are represented with one-hot vectors (as described in Sec. 3) and image pixels are rescaled to the range [0, 1], ensuring their scales are comparable. \u221aWe then directly compare t\u221ahe per-dimension L2 distances in both spaces, denoted as $\\|G_{t}-\\dot{G}_{t-1}\\|_{2}/\\sqrt{D_{G}}$ and $\\|\\mathbf{x}_{t}-\\mathbf{x}_{t-1}\\dot{}\\|_{2}/\\sqrt{D_{I}}$ , where $D_{G}$ and $D_{I}$ are the dimensions of graphs and images, respectively. (Dividing by $\\sqrt{D}$ is to eliminate the influence of different dimensionalities.) We sample 512 trajectories from each DPM and plot the mean and deviation of distances with respect to the time step $t$ . ", "page_idx": 18}, {"type": "text", "text": "In Fig. 4, the results support the explanation of GDPO. While we acknowledge that graphs and images reside in different spaces and typically have different representations, we believe the comparison with L2 distance can provide valuable insights into the differences between graph and image DPMs. ", "page_idx": 18}, {"type": "text", "text": "GDPO on the Synthetic Tree-like Dataset. We first generate a tree and then connect a clique to the nodes of the tree, performing a specified number of rewrite operations as suggested. Based on the number of rewrite steps, graph size, and clique position, we generate multiple datasets, each containing 400 samples. Of these, 256 samples are used for training Graph DPMs, with the remaining samples allocated for validation and testing. In Fig. 5, we present some examples. Fig. 5(a)illustrates a tree structure with a clique of size 4. When the number of rewrite steps is 3, Fig. 5(d) demonstrates that the overall structure of the samples is disrupted. After training the Graph DPMs, we apply GDPO. The model receives a reward of 1 when it generates a tree with a clique; otherwise, the reward is 0. We then ablate the following factors to test the performance of GDPO. ", "page_idx": 18}, {"type": "image", "img_path": "8ohsbxw7q8/tmp/6e6e645005eb0cba4ac385731ae18e97808622265092f21157e4978b44f3464b.jpg", "img_caption": ["Figure 5: Tree with Different Parameters. Node 0 is the root node. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "8ohsbxw7q8/tmp/545e172adfe115b57271b94f4cb3f50e6ac4e0bc8b437898930408fdbf541803.jpg", "img_caption": ["Figure 6: Ablation Study on the Synthetic Tree-like Dataset. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "Rewrite Steps: In Fig. 6(a), we demonstrate GDPO\u2019s performance across different rewrite steps, with four curves representing steps ranging from 0 to 3. Despite a notable decrease in the initial reward as the number of rewrite steps increases, GDPO consistently optimizes the Graph DPMs effectively to generate the desired graph structure. ", "page_idx": 19}, {"type": "text", "text": "Graph Size: In Fig. 6(b), we gradually increase the number of nodes from 16 to 40. The results show that graph size affects the initial reward but does not impact GDPO\u2019s optimization performance. ", "page_idx": 19}, {"type": "text", "text": "Clique Position: We experiment with inserting the clique at different levels of the tree but find no significant difference. We believe this is because the position of the clique does not affect the initial reward of the Graph DPMs, leading to similar optimization results with GDPO. ", "page_idx": 20}, {"type": "text", "text": "Comparison with Baseline: In Fig. 6(c), we compare GDPO with DDPO. The results, consistent with those in Figure 2 of the paper, reveal a clear distinction between GDPO and DDPO in handling challenging data generation tasks. ", "page_idx": 20}, {"type": "text", "text": "A.6 Discussions ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Comparison with the $x_{0}$ -prediction Formulation. Indeed, our eager policy gradient in Eq. 10, compared to the policy gradient of REINFORCE in Eq. 8, resembles the idea of training a denoising network to predict the original uncorrupted graph rather than performing one-step denoising. However, we note that training a denoising network to predict the original data is fundamentally a matter of parametrization of one-step denoising. Specifically, the one-step denoising $p_{\\theta}(x_{t-1}|\\dot{G}_{t})$ is parameterized as a weighted sum of $x_{0}$ -prediction, as described in Eq. 1. Our method in Eq. 8 is motivated differently, focusing on addressing the variance issue as detailed in Sections 4.2 and 4.3. ", "page_idx": 20}, {"type": "text", "text": "Pros and Cons of the RL Approach against Classifier-based and Classifier-free Guidance for Graph DPMs. Compared to graph diffusion models using classifier-based and classifier-free guidance, RL approaches such as GDPO have at least two main advantages: ", "page_idx": 20}, {"type": "text", "text": "\u2022 Compatibility with discrete reward signals and discrete graph representations: As guidance for diffusion models is based on gradients, a differentiable surrogate (e.g., property predictors [65, 37]) is needed for non-differentiable reward signals (e.g., results from physical simulations). RL approaches naturally accommodate arbitrary reward functions without the need for intermediate approximations. \u2022 Better sample efficiency: For graph diffusion models with classifier-based or classifier-free guidance, labeled data are required at the beginning and are independently collected with the graph diffusion models. In contrast, RL approaches like GDPO collect labeled data during model training, thus allowing data collection from the current model distribution, which can be more beneficial. We also empirically observe a significant gap in sample efficiency. ", "page_idx": 20}, {"type": "text", "text": "Analysis on the Bias-variance Trade off. The main bias of GDPO arises from modifying the \"weight\" term in Eq. 9, which shifts the model\u2019s focus more towards the generated results rather than the intermediate process, thereby reducing potential noise. Due to the discrete nature of Graph DPMs, the $x_{0}$ -prediction and $x_{t-1}$ -prediction formulations cannot be related through denoising objectives as in continuous DPMs. This issue also complicates the connection between DDPO and GDPO. We have not yet identified a relevant solution and are still working on it. In our empirical study, we do not observe significant performance variance and tradeoff for GDPO given the current scale of experiments. This may be due to the graph sizes we explored not being sufficiently large. In future implementations, we will incorporate support for sparse graphs to assess GDPO\u2019s performance on larger graph datasets and investigate the tradeoff more thoroughly. ", "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We carefully check the claims, and they align with our evaluation. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: We illustrate the limitations of our work with a failure case. Additionally, we discuss the limitations from various perspectives in the appendix. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: Our paper does not propose any theoretical results. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: All the implementation details are discussed in the appendix Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: In the supplementary materials, we provide the code, dataset, and instructions for reproduction. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: In the experimental setup section, we provide detailed instructions. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: All statistical results are obtained by repeating the experiment five times, and the corresponding standard deviations are provided. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: In the appendix, we provide these contents. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: This work falls under the general machine learning domain, and during the research process, we adhered to the NeurIPS Code of Ethics. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: In the appendix, we discuss the potential impacts of this work in detail. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: This work does not involve any relevant datasets or models. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: All datasets used in this paper are open-source, and there are no copyright issues involved. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 25}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: This work is based on existing datasets. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: This work does not involve any related issues. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: This work does not involve any related issues. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}]