{"references": [{"fullname_first_author": "Tim Brooks", "paper_title": "InstructPix2Pix: Learning to follow image editing instructions", "publication_date": "2023-00-00", "reason": "This paper introduces a foundational text-guided image editing model that is adapted and extended in the target paper for 3D scene editing."}, {"fullname_first_author": "Junzhe Zhu", "paper_title": "HiFA: High-fidelity text-to-3D generation with advanced diffusion guidance", "publication_date": "2023-00-00", "reason": "This paper presents a crucial method for distilling pre-trained text-to-image models to generate high-fidelity 3D objects, a core technique used in the target paper."}, {"fullname_first_author": "Ayaan Haque", "paper_title": "Instruct-NeRF2NeRF: Editing 3D scenes with instructions", "publication_date": "2023-00-00", "reason": "This paper is a closely related work that addresses 3D scene editing using text prompts, providing a strong comparison point for the novel method proposed in the target paper."}, {"fullname_first_author": "Ben Poole", "paper_title": "DreamFusion: Text-to-3D using 2D diffusion", "publication_date": "2023-00-00", "reason": "This is a seminal paper in text-to-3D generation that laid the groundwork for several subsequent works, including the approach used in the target paper."}, {"fullname_first_author": "Ashkan Mirzaei", "paper_title": "Spin-NeRF: Multiview segmentation and perceptual inpainting with neural radiance fields", "publication_date": "2023-00-00", "reason": "This paper provides a relevant method for multi-view consistent scene editing that is directly compared against in the target paper."}]}