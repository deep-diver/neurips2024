[{"type": "text", "text": "Provably Robust Score-Based Diffusion Posterior Sampling for Plug-and-Play Image Reconstruction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xingyu Xu\u2217 Yuejie Chi\u2020 Carnegie Mellon University Carnegie Mellon University ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In a great number of applications, the goal is to infer an unknown image from a small number of noisy measurements collected from a known and possibly nonlinear forward model describing certain sensing or imaging modality, which is often ill-posed. Score-based diffusion models, thanks to their impressive empirical success, have emerged as an appealing candidate of an expressive prior in image reconstruction. In order to accommodate diverse tasks at once, it is of great interest to develop efficient, consistent and robust algorithms that incorporate unconditional score functions of an image prior distribution in conjunction with flexible choices of forward models. This work develops an algorithmic framework for employing score-based diffusion models as an expressive data prior in nonlinear inverse problems with general forward models. Motivated by the plug-and-play framework in the imaging community, we introduce a diffusion plug-and-play method (DPnP) that alternatively calls two samplers, a proximal consistency sampler based solely on the likelihood function of the forward model, and a denoising diffusion sampler based solely on the score functions of the image prior. The key insight is that denoising under white Gaussian noise can be solved rigorously via both stochastic (i.e., DDPM-type) and deterministic (i.e., DDIM-type) samplers using the same set of score functions trained for generation. We establish both asymptotic and nonasymptotic performance guarantees of DPnP, and provide numerical experiments to illustrate its promise in various tasks. To the best of our knowledge, DPnP is the first provably-robust posterior sampling method for nonlinear inverse problems using unconditional diffusion priors. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In a great number of sensing and imaging applications, the paramount goal is to infer an unknown image $x^{\\star}\\in\\mathbb R^{d}$ from a collection of measurements $y\\in\\mathbb{R}^{m}$ that are possibly noisy, incomplete, and even nonlinear. Examples include restoration tasks such as inpainting, super-resolution, denoising, as well as imaging tasks such as magnetic resonance imaging [LDP07], optical imaging $[\\mathrm{SEC^{+}}15]$ , microscopy imaging [HSMC17], radar and sonar imaging [PEPC10], and many more. ", "page_idx": 0}, {"type": "text", "text": "Due to sensing and resource constraints, the problem of image reconstruction is often ill-posed, where the desired resolution of the unknown image overwhelms the set of available observations. Consequently, this necessitates the need of incorporating prior information regarding the unknown image to assist the reconstruction process. Over the years, numerous types of prior information have been considered and adopted, from hand-crafted priors such as subspace or sparsity constraints [Don06, CR12], to data-driven ones prescribed in the form of neural networks [UVL18, BJPD17]. These priors can be regarded as some sort of generative models for the unknown image, which postulate the high-dimensional image admits certain parsimonious representation in a low-dimensional data manifold. It is desirable that the generative models are sufficiently expressive to capture the diversity and structure of the image class of interest, yet nonetheless, still lead to image reconstruction problems that are computationally tractable. ", "page_idx": 0}, {"type": "image", "img_path": "SLnsoaY4u1/tmp/766da35b7000a937c98e9612ee608ab5613ac699a6523722df141cb358368219.jpg", "img_caption": ["Figure 1: Solving linear and nonlinear inverse problems with Diffusion Plug-and-Play (DPnP). "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Score-based diffusion models as an image prior. Recent years have seen tremendous progress on generative artificial intelligence (AI), where it is possible to generate new data samples \u2014 such as images, audio, text \u2014 at unprecedented resolution and scale from a target distribution given training data. Diffusion models, originally proposed by [SDWMG15], are among one of the most successful frameworks, underneath popular content generators such as DALL\u00b7E $[\\bar{\\mathbf{R}}\\mathbf{D}\\mathbf{N}^{+}22]$ , Stable Diffusion $[\\mathbf{RBL}^{+}22]$ , Imagen $[{\\mathbf{S}}{\\mathbf{C}}{\\mathbf{S}}^{+}22]$ , and many others. Roughly speaking, score-based diffusion models convert noise into samples that resemble those from a target data distribution, by forming the reverse Markov diffusion process only using the score functions of the data contaminated at various noise levels [SE19, HJA20, $\\mathrm{SSDK}^{+}\\dot{2}1$ , SME20]. In particular, [SME20] developed a unified framework to interpret score-based diffusion models as reversing certain Stochastic Differential Equations (SDE) using either SDE or probability flow Ordinary Differential Equations (ODE), leading to stochastic (i.e., DDPM-type) and deterministic (i.e., DDIM-type) samplers, respectively. While the DDIM-type sampler is more amenable to acceleration, the DDPM-type sampler tends to generate images of higher quality and diversity when running for a large number of steps [SME20]. ", "page_idx": 1}, {"type": "text", "text": "Thanks to the expressive power of score-based diffusion models in generating complex and finegrained images, they have emerged as a plausible candidate of an expressive prior in image reconstruction [SSXE21, $C\\dot{\\bf K}\\dot{\\bf M}^{+}23$ , $\\operatorname{FSR}^{\\bar{+}}23]$ via the lens of Bayesian posterior sampling. To accommodate diverse applications with various image characteristics and imaging modalities, it is desirable to develop plug-and-play methods that do not require training from scratch or end-to-end training for every new imaging task. Nonetheless, despite a flurry of recent efforts, existing algorithms either are computationally expensive $[\\mathrm{WTN}^{+}23$ , CICM23], inconsistent $[\\mathbf{CKM}^{+}23$ , KEES22, MSKV24], or confined to linear inverse problems [CICM23, DS24]. Therefore, a natural question arises: ", "page_idx": 1}, {"type": "text", "text": "Can we develop a practical, consistent and robust algorithm that incorporates score-based diffusion models as an image prior with general (possibly nonlinear) forward models? ", "page_idx": 1}, {"type": "text", "text": "1.1 Our contribution ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "This paper provides an affirmative answer to this question, by developing an algorithmic framework to sample from the posterior distribution of images, where score-based diffusion models are employed as an expressive image prior in nonlinear inverse problems with general forward models. Specifically, our contributions are as follows. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Diffusion plug-and-play for posterior sampling. Motivated by the plug-and-play [VBW13] framework in the imaging community, we introduce a diffusion plug-and-play method (DPnP) that alternatively calls two samplers, a proximal consistency sampler that aims to generate samples that are more consistent with the measurements, and a denoising diffusion sampler that focuses on sampling from the posterior distribution of an easier problem \u2014 image denoising under white Gaussian noise \u2014 to enforce the prior constraint. Our method is modular, in the sense that the proximal consistency sampler is solely based on the likelihood function of the forward model, and the denoising diffusion sampler is based solely on the score functions of the image prior. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Posterior sampling for image denoising. While the proximal consistency sampler can be borrowed somewhat straightforwardly from existing literature such as the Metropolis-adjusted Langevin algorithm [RR98], the denoising diffusion sampler, on the other hand, has not been addressed in the literature to the best of our knowledge. Our key insight is that this can be solved via both stochastic (i.e., DDPM-type) or deterministic (i.e., DDIM-type) samplers by carefully choosing the forward SDEs and discretizing the resulting reversal SDE or ODE using the exponential integrator [ZC22]. Importantly, the denoising diffusion samplers use the same set of unconditional score functions for generation, making it readily implementable without additional training. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Theoretical guarantees. We establish both asymptotic and non-asymptotic performance guarantees of the proposed DPnP method. Asymptotically, we verify the correctness of our method by proving that $\\bar{\\mathsf{D P}}\\bar{\\mathsf{n P}}$ converges to the conditional distribution of $x^{\\star}$ given measurements $y$ , assuming exact unconditional score estimates of the image prior. We next establish a non-asymptotic convergence theory of DPnP, where its performance degenerates gracefully with respect to the errors of the samplers, due to, e.g., score estimation errors and limited sampling steps. To the best of our knowledge, this provides the first provably-robust method for nonlinear inverse problems using unconditional score-based diffusion priors. ", "page_idx": 2}, {"type": "text", "text": "We further provide numerical experiments to illustrate its promise in solving both linear and nonlinear image reconstruction tasks, such as super-resolution, phase retrieval, and quantized sensing. Due to its plug-and-play nature, we expect it to be of broad interest to a wide variety of inverse problems. ", "page_idx": 2}, {"type": "text", "text": "Related works. Given its interdisciplinary nature, our work sits at the intersection of generative modeling, computational imaging, optimization and sampling. Due to space limits, we postpone the discussion of related works to Appendix A. ", "page_idx": 2}, {"type": "text", "text": "Notation. Let $p_{x}$ denote the probability distribution of $x$ , and $p_{x}(\\cdot|y)$ denotes the conditional distribution of $x$ given $y$ . We use $X~{\\stackrel{\\mathrm{(d)}}{=}}~Y$ to denote random variables $X$ and $Y$ are equivalent in distribution. The matrix $I_{d}$ denotes an identity matrix of dimension $d$ . For two probability distributions with density $p(x)$ and $q(x)$ , the total variation distance between them is $\\bar{\\mathsf{T V}}(p,q):=$ |p(x) \u2212q(x)|dx. The \u03c72-divergence of p to q is \u03c72(p \u2225q) :=  (p(x)q(\u2212xq)(x))2d . ", "page_idx": 2}, {"type": "text", "text": "2 Score-based generative models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we set up the preliminary on diffusion-based generative models, which we will be relying upon to develop our algorithm. The key components consist of a forward process, which diffuses the data distribution $p^{\\star}$ to the standard normal distribution by gradually injecting noise into the samples, and a backward process, which reverses the forward process so that it can transform the standard normal distribution to the data distribution $p^{\\star}$ . To facilitate understanding, it will be convenient to formulate these processes in continuous time. For discrete-time formulation and implementation, please refer to Appendix E. ", "page_idx": 2}, {"type": "text", "text": "2.1 The forward process and score functions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The continuous-time forward diffusion follows the Ornstein-Uhlenbeck (OU) process, defined by the Stochastic Differential Equation (SDE) ${\\mathrm{SSDK}}^{+}21\\$ ]: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{d}X_{\\tau}=-X_{\\tau}\\mathrm{d}\\tau+\\sqrt{2}\\,\\mathrm{d}B_{\\tau},\\quad\\tau\\geq0,\\quad X_{0}\\sim p^{\\star},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $(B_{\\tau})_{\\tau\\geq0}$ is the standard $d$ -dimensional Brownian motion. It can be shown that [Doo42, Eva12] the marginal distribution of $X_{\\tau}$ for $\\tau\\geq0$ is ", "page_idx": 2}, {"type": "equation", "text": "$$\nX_{\\tau}\\overset{\\mathrm{(d)}}{=}\\mathrm{e}^{-\\tau}X_{0}+\\sqrt{1-\\mathrm{e}^{-2\\tau}}\\varepsilon,\\quad X_{0}\\sim p^{\\star},\\,\\varepsilon\\sim\\mathcal{N}(0,I_{d}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "It is then clear that the limiting distribution $X_{\\infty}\\sim{\\mathcal{N}}(0,I_{d})$ as $\\tau\\rightarrow\\infty$ , i.e., the OU process diffuses $X_{0}\\sim p^{\\star}$ to the standard normal distribution. The score function of $X_{\\tau}$ is defined by ", "page_idx": 2}, {"type": "equation", "text": "$$\ns(\\tau,x)=\\nabla\\log p_{X_{\\tau}}(x).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "An enlightening property [Vin11] of the score function is that it can be interpreted as the minimum mean-squared error (MMSE) estimate of $\\varepsilon_{t}$ given $x_{t}=x$ , fueled by Tweedie\u2019s formula: ", "page_idx": 2}, {"type": "equation", "text": "$$\ns(\\tau,x)=-\\frac{1}{\\sqrt{1-\\mathrm{e}^{-2\\tau}}}\\underbrace{\\mathbb{E}_{X_{0}\\sim p^{\\star},\\,\\varepsilon\\sim{\\cal N}(0,I_{d})}\\big(\\varepsilon\\,|\\,\\mathrm{e}^{-\\tau}X_{0}+\\sqrt{1-\\mathrm{e}^{-2\\tau}}\\varepsilon=x\\big)}_{=:\\varepsilon(\\tau,x)}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Consequently, this makes it possible to estimate the score functions via learning to denoise [Hyv05], by estimating the denoising function $\\varepsilon(\\tau,\\cdot)$ , as typically done in practice [HJA20]. ", "page_idx": 2}, {"type": "text", "text": "2.2 The reverse process and sampling ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To enable sampling, one needs to \u201creverse\u201d the forward diffusion process. Fortunately, it is possible to leverage classical theory [And82, AGS05] to reverse the SDE, and apply discretization to the timereversal processes to collect samples. We shall describe two popular approaches below, corresponding to stochastic (i.e., DDPM-type) and deterministic (i.e., DDIM-type) samplers respectively following primarily the framework set forth in $[\\mathrm{SSDK}^{+}21]$ . ", "page_idx": 2}, {"type": "text", "text": "Time-reversed SDEs and probability flow ODEs. Let us begin with the more general theory of reversing SDEs, which will be useful in future sections. Consider a SDE given by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{d}M_{\\tau}=\\alpha M_{\\tau}\\mathrm{d}\\tau+\\sqrt{\\beta}\\mathrm{d}B_{\\tau},\\quad\\tau\\geq0,\\quad M_{0}\\sim p_{M_{0}},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\alpha\\,\\in\\,\\mathbb{R}$ and $\\beta\\,>\\,0$ are constants. For any positive time $\\tau_{\\infty}>0$ , define the reversed time parameter ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\tau^{\\mathsf{r e v}}:=\\tau^{\\mathsf{r e v}}(\\tau)=\\tau_{\\infty}-\\tau.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We are now ready to describe the time-reversed processes. ", "page_idx": 3}, {"type": "text", "text": "1) The time-reversed $S D E$ of (5) on the time interval $[0,\\tau_{\\infty}]$ is defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{d}M_{\\tau^{\\mathrm{ev}}}^{\\mathrm{rev}}=\\left(-\\alpha M_{\\tau^{\\mathrm{ev}}}^{\\mathrm{rev}}+\\beta\\nabla\\log p_{M_{\\tau^{\\mathrm{rev}}}}(M_{\\tau^{\\mathrm{rev}}}^{\\mathrm{rev}})\\right)\\mathrm{d}\\tau+\\sqrt{\\beta}\\mathrm{d}\\tilde{B}_{\\tau},~\\tau\\in[0,\\tau_{\\infty}],~M_{\\tau_{\\infty}}^{\\mathrm{rev}}\\sim p_{M_{\\tau_{\\infty}}},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\tilde{B}$ is an independent copy of $B$ , i.e., another Brownian motion. It is a classical result [And82] that the reversed process $M^{\\mathsf{r e v}}$ shares the same path distribution as $M$ , i.e., $(M_{\\tau}^{\\sf r e v})_{\\tau\\in[0,\\tau_{\\infty}]}\\ \\overset{(\\mathrm{d})}{=}$ $(M_{\\tau})_{\\tau\\in[0,\\tau_{\\infty}]}$ . In other words, the joint distribution of $(M_{\\tau_{1}}^{\\mathsf{r e v}},M_{\\tau_{2}}^{\\mathsf{r e v}},\\cdot\\cdot\\cdot\\,,M_{\\tau_{k}}^{\\mathsf{r e v}})$ for any $0\\leq\\tau_{1}\\leq$ $\\tau_{2}\\leq\\cdot\\cdot\\cdot\\leq\\tau_{k}\\leq\\tau_{\\infty}$ , for any integer $k\\geq1$ , coincides with that of $(M_{\\tau_{1}},M_{\\tau_{2}},\\cdot\\cdot\\cdot\\,,M_{\\tau_{k}})$ . ", "page_idx": 3}, {"type": "text", "text": "2) In place of the reversed SDE in (7), it is possible to consider the following probability flow ODE [AGS05, $\\mathrm{SSDK}^{+}21\\bar{1}$ ]: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{d}M_{\\tau^{\\mathrm{ev}}}^{\\mathrm{rev}}=\\left(-\\alpha M_{\\tau^{\\mathrm{rev}}}^{\\mathrm{rev}}+\\frac{\\beta}{2}\\nabla\\log p_{M_{\\tau^{\\mathrm{rev}}}}(\\tau^{\\mathrm{rev}},M_{\\tau^{\\mathrm{rev}}}^{\\mathrm{rev}})\\right)\\mathrm{d}\\tau,\\;\\tau\\in[0,\\tau_{\\infty}],\\;M_{\\tau_{\\infty}}^{\\mathrm{rev}}\\sim p_{M_{\\tau_{\\infty}}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The reversed ODE satisfies a slightly weaker guarantee than that of the reversed SDE, which nevertheless suffices for most practical purposes [SSDK+21]: M \u03c4rev = $M_{\\tau}^{\\sf r e v}\\overset{\\left(\\mathrm{d}\\right)}{=}M_{\\tau},\\quad\\tau\\in[0,\\tau_{\\infty}].$ . Note that the reversed ODE only guarantees identical marginal distribution for each $M_{\\tau}^{\\tt r e v}$ , whereas the reversed SDE guarantees identical joint distribution. ", "page_idx": 3}, {"type": "text", "text": "Specializing the above to the OU process (1) with proper discretization then leads to popular samplers used for generation, as follows. ", "page_idx": 3}, {"type": "text", "text": "DDPM-type stochastic samplers. Specializing the time-reversed SDE (7) to the OU process gives ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}X_{\\tau^{\\mathrm{rev}}}^{\\mathrm{rev}}=\\big(X_{\\tau^{\\mathrm{rev}}}^{\\mathrm{rev}}+2s(\\tau^{\\mathrm{rev}},X_{\\tau^{\\mathrm{rev}}}^{\\mathrm{rev}})\\big)\\mathrm{d}\\tau+\\sqrt{2}\\mathrm{d}\\tilde{B}_{\\tau},\\quad\\tau\\in[0,\\tau_{\\infty}],\\quad X_{\\tau_{\\infty}}^{\\mathrm{rev}}\\sim p_{X_{\\tau_{\\infty}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "As $\\tau_{\\infty}\\rightarrow\\infty$ , it can be seen from (2) that $p_{X_{\\tau_{\\infty}}}$ converges to $\\mathcal{N}(0,I_{d})$ . Thus the solution of the above SDE can be approximated by initializing $X_{\\tau_{\\infty}}^{\\mathsf{r e v}}\\sim\\mathcal{N}(0,I_{d})$ instead. The DDPM sampler [HJA20] can be viewed as a discretization of this SDE $[\\mathrm{SSDK}^{+}21]$ ]. ", "page_idx": 3}, {"type": "text", "text": "DDIM-type deterministic samplers. On the other hand, the probability flow ODE (8) for the OU process reads as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm d X_{\\tau^{\\mathrm{ev}}}^{\\mathsf{r e v}}=\\big(X_{\\tau^{\\mathrm{rev}}}^{\\mathsf{r e v}}+s(\\tau^{\\mathsf{r e v}},X_{\\tau^{\\mathrm{rev}}}^{\\mathsf{r e v}})\\big)\\mathrm d\\tau,\\quad\\tau\\in[0,\\tau_{\\infty}],\\quad X_{\\tau_{\\infty}}^{\\mathsf{r e v}}\\sim p_{X_{\\tau_{\\infty}}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Again, as $\\tau_{\\infty}\\rightarrow\\infty$ , one may approximate the initialization with $X_{\\tau_{\\infty}}^{\\mathsf{r e v}}\\sim\\mathcal{N}(0,I_{d})$ . It is known that the popular DDIM sampler $[\\mathrm{SSDK}^{+}21$ , SME20] is a discretization of this ODE [ZC22]. The ODE-based deterministic samplers allow more aggressive choice of discretization schedules, as well as fast ODE solvers $[\\mathbf{L}Z\\mathbf{B}^{+}22]$ , enabling significantly accelerated sampling process compared to the SDE-based stochastic samplers. ", "page_idx": 3}, {"type": "text", "text": "3 Posterior sampling via diffusion plug-and-play ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We are interested in solving (possibly nonlinear) inverse problems, where the aim is to infer an unknown image $x^{\\star}\\in\\mathbb{R}^{d}$ from its measurements $y\\in\\mathbb{R}^{m}$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\ny=\\mathcal{A}(x^{\\star})+\\xi,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathcal{A}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{m}$ is the measurement operator underneath the forward model, and $\\xi$ denotes measurement noise. We focus on the Bayesian setting where the prior information of $x^{\\star}$ is provided in the form of some prior distribution $p^{\\star}(\\cdot)$ , i.e., ", "page_idx": 3}, {"type": "equation", "text": "$$\nx^{\\star}\\sim p^{\\star}(x),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The posterior distribution given measurements $y$ is defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\np^{\\star}(x|y)\\propto p^{\\star}(x)\\,p(y|x^{\\star}=x)=p^{\\star}(x)\\,\\mathrm{e}^{\\mathcal{L}(x;y)}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, $\\mathcal{L}(\\cdot;y)$ is the log-likelihood function of the measurements. Notwithstanding, our framework allows flexible choices of the forward model and the noise distributions. In addition, while this formulation is derived from probabilistic interpretations, it also subsumes the \u201creward-guided\u201d or \u201closs-guided\u201d setting $[\\mathrm{SZY}^{+}23]$ , where $\\mathcal{L}$ can be viewed as a reward function or a negative loss function, both of which characterize preference over structural properties of $x^{\\star}$ . ", "page_idx": 4}, {"type": "text", "text": "Assumption on the forward model. Throughout the paper, for simplicity, we make the following mild assumption on $\\mathcal{L}$ , which is applicable to many applications of interest. ", "page_idx": 4}, {"type": "text", "text": "Assumption 1. We assume $\\mathcal{L}(\\cdot\\,;\\,y)$ is differentiable almost everywhere, and $\\operatorname*{sup}_{x\\in\\mathbb{R}^{d}}\\mathcal{L}(x;y)<\\infty$ . ", "page_idx": 4}, {"type": "text", "text": "Goal. Our goal is to sample $\\widehat{x}$ from the posterior distribution $\\widehat{x}\\sim p^{\\star}(\\cdot\\,|\\,y)$ given estimates $\\widehat{s}(\\tau,x)$ (resp. $\\widehat{\\varepsilon}(\\tau,\\bar{x}))$ ) of the unconditional score functions $s(\\tau,x)$ (resp. the noise function $\\varepsilon(\\tau,x),$ ) in (3), assum i ng knowledge of the likelihood function $\\mathcal{L}(\\cdot;y)$ . ", "page_idx": 4}, {"type": "text", "text": "3.1 Key ingredient: score-based denoising posterior sampling ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We begin with an inspection on one of the most fundamental inverse problems: denoising under white Gaussian noise. As shall be elucidated shortly, the denoising diffusion samplers turn out to be an important building block in our algorithm for general inverse problems. ", "page_idx": 4}, {"type": "text", "text": "Image denoising under white Gaussian noise. Suppose that we have access to a noisy version of $x^{\\star}\\sim p^{\\star}$ contaminated by white Gaussian noise, given by ", "page_idx": 4}, {"type": "equation", "text": "$$\nx_{\\mathsf{n o i s y}}=x^{\\star}+\\xi,\\quad\\xi\\sim\\mathcal{N}(0,\\eta^{2}I_{d}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\eta>0$ is the noise intensity assumed to be known. Our goal is to sample from $p^{\\star}(\\cdot\\,|\\,x_{\\mathsf{n o i s y}})$ given the score estimates $\\widehat{s}_{t}(x)$ (resp. the noise estimates $\\widehat{\\varepsilon_{t}}(x)\\}$ ). We will develop our score-based denoising posterior sampl e r, termed DDS, with two varian t s, DDS-DDPM and DDS-DDIM, which can be viewed as analogues of the well-known DDPM and DDIM samplers in unconditional scorebased sampling respectively. Before proceeding, it is worth highlighting that the two variants will be derived from different forward diffusion processes, since we observe the resulting variants empirically lead to more competitive performance. ", "page_idx": 4}, {"type": "text", "text": "A stochastic DDPM-type sampler via heat flow. We begin with a stochastic DDPM-type sampler for denoising, termed DDS-DDPM. We divide our development into the following steps. ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "1) Step 1: introducing the heat flow. Let us introduce a heat flow with initial distribution $p^{\\star}$ , defined by the following SDE: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{d}Y_{\\tau}=\\mathrm{d}B_{\\tau},\\quad\\tau\\geq0,\\quad Y_{0}\\sim p^{\\star},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $(B_{\\tau})_{\\tau\\geq0}$ is the standard $d$ -dimensional Brownian motion. The solution of (13) is simply ", "page_idx": 4}, {"type": "equation", "text": "$$\nY_{\\tau}=Y_{0}+B_{\\tau},\\quad\\tau\\geq0.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Since $B_{\\tau}\\sim\\mathcal{N}(0,\\tau I_{d})$ , it readily follows that $B_{\\eta^{2}}\\stackrel{\\mathrm{(d)}}{=}\\xi$ , which together with $Y_{0}\\sim p^{\\star}$ yield the important observation that $x_{\\mathsf{n o i s y}}=x^{\\star}+\\xi$ can be viewed as an endpoint of the heat flow, in the sense that $x_{\\mathsf{n o i s y}}=x^{\\star}+\\xi\\ {\\overset{\\mathrm{(d)}}{=}}\\ Y_{\\eta^{2}}$ . ", "page_idx": 4}, {"type": "text", "text": "2) Step 2: reversing the heat flow. Following similar reasonings in Section 2, the next step boils down to reverse the heat flow (13). The time-reversal of the heat flow SDE (13) is (cf. (7)) given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{d}Y_{\\eta^{2}-\\tau}^{\\mathsf{r e v}}=\\nabla\\log p_{Y_{\\eta^{2}-\\tau}}(Y_{\\eta^{2}-\\tau}^{\\mathsf{r e v}})\\mathrm{d}\\tau+\\mathrm{d}\\tilde{B}_{\\tau},\\quad\\tau\\in[0,\\eta^{2}],\\quad Y_{\\eta^{2}}^{\\mathsf{r e v}}\\sim p_{Y_{\\eta^{2}}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $(\\tilde{B}_{\\tau})_{\\tau\\geq0}$ is an independent copy of $(B_{\\tau})_{\\tau\\geq0}$ . As introduced earlier, the virtue of the time-reversed SDE (15) is that it produces a process $Y_{\\tau}^{\\tt r e v}$ with the same path distribution as $Y_{\\tau}$ , i.e., $(Y_{\\tau}^{\\mathsf{r e v}})_{\\tau\\in[0,\\eta^{2}]}\\ {\\stackrel{\\mathrm{(d)}}{=}}\\ (Y_{\\tau})_{\\tau\\in[0,\\eta^{2}]}$ . In particular, the joint distribution of $(Y_{0}^{\\mathsf{r e v}},Y_{\\eta^{2}}^{\\mathsf{r e v}})$ is the same as that of $\\left(Y_{0},Y_{\\eta^{2}}\\right)\\ {\\stackrel{\\mathrm{(d)}}{=}}\\ \\left(x^{\\star},x_{\\mathsf{n o i s y}}\\right)$ . This implies that the conditional distribution $p^{\\star}(\\cdot\\,|\\,x_{\\mathsf{n o i s y}})$ is the same as $p_{Y_{0}^{\\mathsf{r e v}}}(\\cdot\\,|\\,Y_{\\eta^{2}}^{\\mathsf{r e v}}\\,=\\,x_{\\mathsf{n o i s y}})$ ). Surprisingly, the latter admits a simple interpretation: $p_{Y_{0}^{\\mathsf{r e v}}}(\\cdot\\,|\\,Y_{\\eta^{2}}^{\\mathsf{r e v}}\\,=\\,x_{\\mathsf{n o i s y}})$ is the distribution of $Y_{0}^{\\mathsf{r e v}}$ when we initialize (15) with $Y_{\\eta^{2}}^{\\mathrm{rev}}\\;=\\;x_{\\mathsf{n o i s y}}!$ Therefore, sampling the posterior $p^{\\star}(\\cdot\\,|\\,x_{\\mathsf{n o i s y}})$ amounts to solving the following simple SDE: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{d}Y_{\\eta^{2}-\\tau}^{\\mathsf{r e v}}=\\nabla\\log p_{Y_{\\eta^{2}-\\tau}}(Y_{\\eta^{2}-\\tau}^{\\mathsf{r e v}})\\mathrm{d}\\tau+\\mathrm{d}\\tilde{B}_{\\tau},\\quad\\tau\\in[0,\\eta^{2}],\\quad Y_{\\eta^{2}}^{\\mathsf{r e v}}=x_{\\mathsf{n o i s y}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3) Step 3: connecting the score functions. It is now immediate to arrive at our proposed stochastic sampler DDS-DDPM by discretization of this SDE (16), which requires knowledge of the score functions $\\nabla\\log p_{Y_{\\tau}}(\\cdot)$ . A key observation is that they can in fact be computed from the score function $s(\\tau,x)$ (cf. (3)), due to the following lemma, whose proof is provided in Appendix D.1. ", "page_idx": 5}, {"type": "text", "text": "Lemma 1 (Score function of $Y_{\\tau}$ ). For $\\tau\\geq0$ , we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\nabla\\log p_{Y_{\\tau}}(x)=\\frac{1}{\\sqrt{1+\\tau}}s\\left(\\frac{1}{2}\\log(1+\\tau),\\,\\frac{x}{\\sqrt{1+\\tau}}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The resulting sampler, DDS-DDPM, is summarized in Algorithm 2 (deferred in the appendix) using a discretization procedure with an exponential integrator [ZC22]. ", "page_idx": 5}, {"type": "text", "text": "A deterministic DDIM-type sampler via OU process. We next develop a deterministic DDIM-type sampler for denoising, termed DDS-DDIM. ", "page_idx": 5}, {"type": "text", "text": "1) Step 1: introducing a posterior-initialized OU process. To sample from the posterior distribution $p^{\\star}(\\cdot|x_{\\mathsf{n o i s y}})$ , we first introduce a random variable $w$ which has (unconditional) distribution ", "page_idx": 5}, {"type": "equation", "text": "$$\np_{w}(x):=p^{\\star}(x^{\\star}=x\\,|\\,x^{\\star}+\\xi=x_{\\mathsf{n o i s y}}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "in the same form of the desired posterior distribution $p^{\\star}(\\cdot|x_{\\mathsf{n o i s y}})$ . Here, since the noisy observation $x_{\\mathsf{n o i s y}}$ is given, we regard it as fixed.We then further introduce $z=w-x_{\\mathsf{n o i s y}}$ , which is a \u201ccentered\u201d version of $w$ , whose distribution is ", "page_idx": 5}, {"type": "equation", "text": "$$\np_{z}(x):=p_{w}(x+x_{\\mathsf{n o i s y}})=p^{\\star}(x^{\\star}=x+x_{\\mathsf{n o i s y}}\\,|\\,x^{\\star}+\\xi=x_{\\mathsf{n o i s y}}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The OU process with initial distribution $p_{z}$ is defined by the SDE: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{d}Z_{\\tau}=-Z_{\\tau}\\mathrm{d}\\tau+\\mathrm{d}B_{\\tau},\\quad\\tau\\geq0,\\quad Z_{0}\\sim p_{z},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $B_{\\tau}$ is the standard $d\\!.$ -dimensional Brownian motion. As in (2), the marginal distribution of $Z_{\\tau}$ is given by ", "page_idx": 5}, {"type": "equation", "text": "$$\nZ_{\\tau}\\stackrel{\\mathrm{(d)}}{=}\\mathrm{e}^{-\\tau}Z_{0}+\\sqrt{1-\\mathrm{e}^{-2\\tau}}\\varepsilon,\\quad Z_{0}\\sim p_{z},\\;\\varepsilon\\sim\\mathcal{N}(0,I_{d}),\\quad\\tau\\geq0.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "2) Step 2: reversing the OU process. Following similar reasonings in Section 2, reversing the OU process (18) will enable us to generate samples $z\\sim p_{z}$ . Then we can set $w=z+x_{\\mathsf{n o i s y}}$ , which, by definition, has distribution $p_{w}$ defined in (17), and is a sample from the desired posterior distribution $p^{\\star}(\\cdot|x_{\\mathsf{n o i s y}})$ . We are thus led to solve the time-reversed probability flow ODE (cf. (8)) of (18), given by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{d}Z_{\\tau^{\\mathrm{ev}}}^{\\mathrm{rev}}=\\left(Z_{\\tau^{\\mathrm{rev}}}^{\\mathrm{rev}}\\!+\\!\\nabla\\log p_{Z_{\\tau^{\\mathrm{rev}}}}(\\tau^{\\mathrm{rev}},Z_{\\tau^{\\mathrm{rev}}}^{\\mathrm{rev}})\\right)\\mathrm{d}\\tau,\\quad\\tau\\in[0,\\tau_{\\infty}],\\quad Z_{\\tau_{\\infty}}^{\\mathrm{rev}}\\sim\\mathcal{N}(0,I_{d}),\\quad\\tau^{\\mathrm{rev}}=\\tau_{\\infty}\\!-\\!\\tau.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "3) Step 3: connecting the score functions. We are now one step away from our proposed deterministic sampler DDS-DDIM, which is derived by discretization of the ODE (20). We need to know the score functions $\\nabla\\log p_{Z_{\\tau}}(\\cdot)$ , which again can be computed from the score function $s(\\tau,x)$ (cf. (3)), as documented by the following lemma, whose proof is provided in Appendix D.2. ", "page_idx": 5}, {"type": "text", "text": "Lemma 2 (Score function of $Z_{\\tau}$ ). For $\\tau\\geq0$ , we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\nabla\\log p_{Z_{\\tau}}(x)=-\\frac{\\mathrm{e}^{2\\tau}x}{\\eta^{2}+\\mathrm{e}^{2\\tau}-1}+\\frac{\\mathrm{e}^{\\tau-\\tilde{\\tau}}\\eta^{2}}{\\eta^{2}+\\mathrm{e}^{2\\tau}-1}s\\left(\\tilde{\\tau},\\,\\mathrm{e}^{-\\tilde{\\tau}}x_{\\mathrm{noisy}}+\\frac{\\mathrm{e}^{\\tau-\\tilde{\\tau}}\\eta^{2}x}{\\eta^{2}+\\mathrm{e}^{2\\tau}-1}\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\tilde{\\tau}:=\\tilde{\\tau}(\\tau)=\\frac{1}{2}\\log\\left(\\frac{\\eta^{2}(\\mathrm{e}^{2\\tau}-1)}{\\eta^{2}+\\mathrm{e}^{2\\tau}-1}+1\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "After plugging this into (20) and solving the ODE for $Z_{\\tau}^{\\mathsf{r e v}}$ , we see that $Z_{0}^{\\mathsf{r e v}}+x_{\\mathsf{n o i s y}}$ is the desired sample from the posterior distribution $p^{\\star}(\\cdot|x_{\\mathsf{n o i s y}})$ , as argued before. Numerically, the ODE (20) is solved by discretization with an exponential integrator [ZC22], resulting in the sampler DDS-DDIM as summarized in Algorithm 3 (deferred in the appendix). ", "page_idx": 5}, {"type": "text", "text": "Input: Measurements $y\\ \\in\\ \\mathbb{R}^{m}$ , log-likelihood function $\\mathcal{L}(\\cdot;y)$ of the forward model, score   \nestimatess, annealing schedule (\u03b7k)0\u2264k\u2264K.   \nInitializa t ion: Sample $\\begin{array}{r}{\\widehat{x}_{0}\\sim\\mathcal{N}(0,\\frac{\\eta_{0}}{4}\\overline{{I}}_{d})}\\end{array}$   \nAlternating sampling: for $k=0,1,2,\\ldots,K-1$ do   \n(1) Proximal consistency sampler: Sample $\\begin{array}{r}{\\widehat{x}_{k+\\frac{1}{2}}\\propto\\exp\\left(\\mathcal{L}(\\cdot\\,;\\,y)-\\frac{1}{2\\eta_{k}^{2}}\\|\\cdot-\\widehat{x}_{k}\\|^{2}\\right)}\\end{array}$ using subroutine $\\mathsf{P C S}(\\widehat{x}_{k},y,\\mathcal{L},\\eta_{k})$ (Alg. 4).   \n(2) Denoising diffusion sampler: Sample $\\begin{array}{r}{\\widehat{x}_{k+1}\\sim\\exp\\left(\\log p^{\\star}(x)-\\frac1{2\\eta_{k}^{2}}\\|x-\\widehat{x}_{k+\\frac12}\\|^{2}\\right)}\\end{array}$ using subroutine DDS-DDPM $(\\widehat{x}_{k+\\frac{1}{2}},\\widehat{s},\\eta_{k})$ (Alg. 2) or DDS-DDIM $(\\widehat{x}_{k+\\frac{1}{2}},\\widehat{s},\\eta_{k})$ (Alg. 3).   \nOutput: ${\\widehat{x}}_{K}$ . ", "page_idx": 6}, {"type": "text", "text": "3.2 Our algorithm: diffusion plug-and-play ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Now we turn to the general setting where the measurement operator $\\boldsymbol{\\mathcal{A}}$ is arbitrary. From the factorization of posterior distribution in (11), one intuitively understands that a posterior sampler must obey two constraints simultaneously: (i) the data prior constraint, corresponding to the first factor $p^{\\star}(x)$ , which imposes that the posterior sampler should be less likely to sample at those points where $p^{\\star}(x)$ is small; (ii) the measurement consistency constraint, corresponding to the second factor $\\mathrm{e}^{\\mathcal{L}(x;y)}$ , which imposes that $\\boldsymbol{\\mathcal{A}}(\\boldsymbol{x})\\approx\\boldsymbol{y}$ . ", "page_idx": 6}, {"type": "text", "text": "Diffusion plug-and-play (DPnP). We will apply the idea of alternatively enforcing these two constraints from a sampling perspective in the same spirit of [VDC19, LST21, BB23]. Our algorithm, dubbed diffusion plug-and-play (DPnP), alternates between two samplers, the denoising diffusion sampler (DDS) and the proximal consistency sampler (PCS), which can be viewed as the substitutes for the proximal operator and the gradient step respectively. Given the iterate ${\\widehat{x}}_{k}$ and the annealing parameter $\\eta_{k}$ at the $k$ -th iteration, DPnP proceeds with the following two step s : ", "page_idx": 6}, {"type": "text", "text": "(i) Proximal consistency sampler to enforce the measurement consistency constraint. DPnP draws a sample $\\widehat{x}_{k+\\frac12}$ from the distribution proportional to $\\begin{array}{r}{\\exp\\Big(\\mathcal{L}(x\\,;\\,y)-\\frac{1}{2\\eta_{k}^{2}}\\|x-\\widehat{x}_{k}\\|^{2}\\Big)}\\end{array}$ to promote the image to be consistent with the measurements. This step, which we denote as the proximal consistency sampler, can be achieved by small modifications of standard algorithms such as Metropolis-Adjusted Langevin Algorithm (MALA) [RR98] given in Algorithm 4 (deferred in the appendix). ", "page_idx": 6}, {"type": "text", "text": "(ii) Denoising diffusion sampler to enforce the data prior constraint. DPnP next draws a sample $\\widehat{x}_{k+1}$ from the distribution proportional to ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\exp\\left(-\\big(-\\log p^{\\star}(x)+\\frac{1}{2\\eta_{k}^{2}}\\|x-\\widehat{x}_{k+\\frac{1}{2}}\\|^{2}\\big)\\right)\\propto p^{\\star}(x^{\\star}=x\\,|\\,x^{\\star}+\\eta_{k}w=\\widehat{x}_{k+\\frac{1}{2}})\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "to promote the image to be consistent with the prior, where $w\\sim\\mathcal{N}(0,I_{d})$ . The last step, which follows from the Bayes\u2019 rule, makes it clear that this step can be precisely achieved by the denoising diffusion sampler (developed in Section 3.1) using solely the unconditional score function. ", "page_idx": 6}, {"type": "text", "text": "Combining both steps lead to the proposed DPnP method described in Algorithm 1. Some comments about the proposed DPnP method are in order. ", "page_idx": 6}, {"type": "text", "text": "\u2022 The proximal consistency sampler PCS can be viewed as a \u201csoft\u201d version of the proximal point method [Dru17]. This can be seen from a first-order approximation: the maximum likelihood of the distribution $\\begin{array}{r l}{\\mathrm{~\\exp{\\big(}}{\\mathcal{L}}(\\cdot;y)-\\frac{1}{2\\eta_{k}^{2}}\\|\\cdot-\\widehat{x}_{k}\\|^{2}{\\big)}}\\end{array}$ is attained at the point $x^{\\prime}\\in\\mathbb R^{d}$ satisfying $\\nabla_{x^{\\prime}}\\mathcal{L}(x^{\\prime};y)-\\frac{1}{\\eta_{k}^{2}}(x^{\\prime}-\\widehat{x}_{k})=0,\\quad\\Longrightarrow\\quad x^{\\prime}=\\widehat{x}_{k}+\\eta_{k}^{2}\\nabla_{x^{\\prime}}\\mathcal{L}(x^{\\prime};y)\\approx\\widehat{x}_{k}+\\eta_{k}^{2}\\nabla_{\\widehat{x}_{k}}\\mathcal{L}(\\widehat{x}_{k};y),$ Therefore, the proximal consistency sampler draws random samples \u201cconcentrated\u201d around $x^{\\prime}$ , which approximates the implicit proximal point update, akin to a gradient step at $\\widehat{x}_{k}$ . \u2022 On the other end, the denoising posterior sampler DDS can be regarded as a \u201csoft\u201d version of the proximal operator. In particular, when $p^{\\star}$ is supported on a low-dimensional manifold $\\mathcal{M}$ , it forces $x$ to reside in $\\mathcal{M}$ , like the proximal map. To see this, note that denoising posterior distribution vanishes outside $\\mathcal{M}$ by (23). ", "page_idx": 6}, {"type": "text", "text": "\u2022 The proximal consistency sampler PCS admits a simple form when the forward model $\\boldsymbol{\\mathcal{A}}$ is linear, i.e. ${\\bar{A}}(x)=A x$ for some matrix $A\\in\\mathbb{R}^{m\\times d}$ , and the measurement noise $\\xi\\sim\\mathcal{N}(0,\\Sigma)$ is Gaussian. In this situation, the proximal consistency sampler PCS can be implemented directly by ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\widehat{x}_{k+\\frac{1}{2}}=\\mathsf{P C S}(\\widehat{x}_{k},y,\\mathcal{L},\\eta_{k})=\\widetilde{x}_{k}+\\widetilde{\\Sigma}_{k}^{1/2}w_{k},\\quad w_{k}\\sim\\mathcal{N}(0,I_{d}),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "4 Theoretical analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we establish both asymptotic and non-asymptotic performance guarantees of DPnP. ", "page_idx": 7}, {"type": "text", "text": "Asymptotic consistency. We begin with the asymptotic consistency of DPnP in the theorem below. ", "page_idx": 7}, {"type": "text", "text": "Theorem 1 (Asymptotic consistency of DPnP). Assume the score function estimate $\\widehat{s}(\\tau,\\cdot)$ is accurate, i.e., $\\widehat{s}(\\tau,x)=s(\\tau,x)$ , and assume the ODE/SDEs in DDS and PCS are solve d  exactly. Let $(\\varepsilon_{l})_{l\\geq0}$ be   a decreasing sequence of positive numbers satisfying $\\begin{array}{r}{\\operatorname*{lim}_{l\\to\\infty}\\varepsilon_{l}=0}\\end{array}$ , and $(k_{l})_{l\\ge0}$ be an increasing sequence of integers with $k_{0}~=~0$ . Set the annealing schedule $\\eta_{k}~=~\\varepsilon_{l}$ , for $k_{l-1}\\leq k<k_{l},\\;l=1,2,\\cdots\\;L e t\\operatorname*{min}_{l^{\\prime}=1,2,\\cdots}|k_{l}$ $|k_{l^{\\prime}}-k_{l^{\\prime}-1}|\\to\\infty$ , the output $\\widehat{x}_{k_{l}}$ of DPnP converges in distribution to the posterior distribution $p^{\\star}(\\cdot|y)$ for $l\\rightarrow\\infty$ . ", "page_idx": 7}, {"type": "text", "text": "In words, Theorem 1 establishes the asymptotic consistency of DPnP under fairly mild assumptions on the forward model (cf. Assumption 1): as long as the sampled distributions of DDS and PCS are exact, then running DPnP with a slowly diminishing annealing schedule of $\\{\\eta_{k}\\}$ will output samples approaching the desired posterior distribution $p^{\\star}(\\cdot|y)$ when the number of iterations $l$ goes to infinity. ", "page_idx": 7}, {"type": "text", "text": "Non-asymptotic error analysis. We now step away from the idealized setting when the sampled distributions of DDS and PCS are exact. In practice, there are many sources of errors that can influence the sampled distributions of DDS and PCS, e.g., the discretization error arising from numerically solving ODE/SDE, and the score estimation error. In effect, these non-idealities will make PCS and DDS inexact. That is, the distribution they generate will slightly deviate from the distribution they ought to sample from. In this paper, we model such deviations by the total variation distance from the distribution generated by PCS (resp. DDS) to the ideal distribution proportional to $\\begin{array}{r}{\\exp(\\mathcal{L}(x;y)-\\frac{1}{2\\eta_{k}^{2}}\\|x-\\widehat{x}_{k}\\|^{2}\\big)}\\end{array}$ (resp. $p^{\\star}(x^{\\star}=x|x^{\\star}+\\eta_{k}\\varepsilon=\\widehat{x}_{k+\\frac{1}{2}})$ ) uniformly over all iterations. Analyzing these errors is out of the scope of this paper, and we point the interested readers to parallel lines of works, e.g., [LWCC23, MV19, $\\mathrm{CLA}^{+}\\bar{2}1\\bar{]}$ , among many others. In our analysis, we will assume a black-box bound for the total variation errors of PCS and DDS, which can be combined with existing analyses of the respective samplers to bound the iteration complexity of DPnP. ", "page_idx": 7}, {"type": "text", "text": "Theorem 2 (Non-asymptotic robustness of DPnP). With the notation in DPnP (Algorithm $^{\\,l}$ ), set $\\eta_{k}\\equiv\\eta>0$ . Under Assumption $^{\\,l}$ , there exists $\\lambda:=\\lambda(p^{\\star},\\mathcal{L},\\eta)\\in(0,1)$ , such that the following holds. Define a stationary distribution $\\pi_{\\eta}$ by $\\pi_{\\eta}(x)\\propto p^{\\star}(x)q_{\\eta}(x)$ , where $q_{\\eta}$ is defined by ", "page_idx": 7}, {"type": "equation", "text": "$$\nq_{\\eta}(x):=\\mathrm{e}^{\\mathcal{L}(\\cdot;y)}\\ast p_{\\eta\\varepsilon}(x)=\\frac{1}{(2\\pi)^{d/2}\\eta^{d}}\\int\\mathrm{e}^{\\mathcal{L}(x^{\\prime};y)-\\frac{1}{2\\eta^{2}}\\|x-x^{\\prime}\\|^{2}}\\mathrm{d}x^{\\prime},\\quad\\varepsilon\\sim\\mathcal{N}(0,I_{d}),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $^*$ denotes convolution. If PCS has error at most \u03b5PCS in total variation and DDS has error at most \u03b5DDS in total variation per iteration, then for any accuracy goal \u03b5acc > 0, with K \u224d log(11\u2212/\u03bb\u03b5acc), we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathsf{T V}(p_{\\widehat{x}_{K}},\\pi_{\\eta})\\lesssim\\varepsilon_{\\mathsf{a c c}}\\sqrt{\\chi^{2}(p_{\\widehat{x}_{1}}\\,\\|\\,\\pi_{\\eta})}+\\frac{1}{1-\\lambda}(\\varepsilon_{\\mathsf{D D S}}+\\varepsilon_{\\mathsf{P C S}})\\log\\left(\\frac{1}{\\varepsilon_{\\mathsf{a c c}}}\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Before interpreting Theorem 2, we observe that $q_{0}(x)=\\mathrm{e}^{\\mathcal{L}(x;y)}$ , thus $\\pi_{0}(x)\\propto p^{\\star}(x)\\mathrm{e}^{\\mathcal{L}(x;y)}$ coincides with the desired posterior distribution $p^{\\star}(\\cdot|y)$ . Thus Theorem 2 tells us that, assuming a constant annealing schedule $\\eta_{k}=\\eta$ , the output of DPnP converges in total variation to the distribution $\\pi_{\\eta}$ , which is a distorted version of the desired posterior distribution up to level $\\eta$ , with sufficiently many iterations. A few remarks are in order. ", "page_idx": 7}, {"type": "text", "text": "Non-diminishing $\\eta$ . It can be seen from Theorem 2 that even with a nonzero $\\eta$ , DPnP already enforces the data prior strictly. On the other hand, the measurement consistency is distorted by an order of $\\eta$ . This is usually tolerable, since the measurements are themselves contaminated by ", "page_idx": 7}, {"type": "text", "text": "Table 1: Samples of different algorithms for phase retrieval, where $\\mathsf{D P n P}$ generate images of higher quality and recover fine details of the image more faithfully than the state-of-the-art DPS $[\\mathsf{C K M}^{\\bar{+}}23]$ and LGD-MC $[\\mathrm{SZY}^{+}23]$ algorithms. Pixel-based ReSample $[{\\mathrm{SKZ}}^{+}23]$ did not generate meaningful images, possibly due to high nonlinearity of phase retrieval. ", "page_idx": 8}, {"type": "image", "img_path": "SLnsoaY4u1/tmp/779b6bf2689e5c02e9d53479d3517a5436663660fbc97cb4848612661aa9b654.jpg", "img_caption": ["Table 2: Samples of different algorithms for quantized sensing. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "SLnsoaY4u1/tmp/1ecc4ce2ec11b25fa023d4c62eeb465919f4248874137aaf9d5b59b336c9a31d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "noise, thus when $\\eta$ is smaller than the noise level, the distortion would be tolerable. In practice, it is beneficial to choose an annealing schedule of $\\{\\eta_{k}\\}$ , which will be elaborated in Section 5. ", "page_idx": 8}, {"type": "text", "text": "Provable robustness. Theorem 2 indicates the performance of $\\mathsf{D P n P}$ degenerates gracefully in the presence of sampling errors. To the best of our knowledge, this is the first provably consistent and robust posterior sampling method for nonlinear inverse problems using score-based diffusion priors. ", "page_idx": 8}, {"type": "text", "text": "5 Numerical experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We provide preliminary numerical evidence to corroborate the promise of $\\mathsf{D P n P}$ in solving both linear and nonlinear image reconstruction tasks. We denote DPnP with the subroutines DDS-DDPM and DDS-DDIM as DPnP-DDPM and DPnP-DDIM respectively. ", "page_idx": 8}, {"type": "text", "text": "Table 3: Evaluation of solving inverse problems on FFHQ $256\\times256$ validation dataset (1k samples). Despite considerable efforts to optimize parameters, pixel-based ReSample did not generate meaningful results for phase retrieval. ", "page_idx": 8}, {"type": "table", "img_path": "SLnsoaY4u1/tmp/5c367f997f41f3076826e41033d07fa9dedd65dd4f2bab0438b4ddb60c4eddbe.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "SLnsoaY4u1/tmp/6e5187902c0f20903fdf5a8631d9fcc5983e503a47fbf7e449b26b2a32d83506.jpg", "table_caption": ["Table 4: Evaluation of solving inverse problems on ImageNet $256\\times256$ validation dataset (1k samples). Despite considerable efforts to optimize parameters, pixel-based ReSample did not generate meaningful results for phase retrieval. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Experimental setups. We compare DPnP with the state-of-the-art algorithms including DPS $[\\boldsymbol{\\mathrm{C}}\\bar{\\boldsymbol{\\mathrm{KM}}}^{+}23]$ , LGD-MC $[\\mathrm{SZY}^{+}23]$ , and pixel-based ReSample $[S\\!\\mathrm{K}Z^{+}23]$ for super-resolution (linear), phase retrieval (nonlinear), and quantized sensing (nonlinear). Definitions of these forward measurement models are in Appendix G.2. The annealing schedule $\\{\\eta_{k}\\}$ of $\\mathsf{D P n P}$ is fixed across all tasks (Appendix H.2), while DPS, LGD-MC, and ReSample are fine-tuned with reasonable effort for best performance. All experiments are run on a single Nvidia L40 GPU. More details and experiments are in Appendix G. ", "page_idx": 9}, {"type": "text", "text": "Sample images. We present the sample images of different algorithms for the most complicated task of phase retrieval. For phase retrieval, Fourier transform is performed to the image with a coded mask, and only the magnitude of the Fourier transform is taken as the measurement $[\\mathrm{SEC}^{+}15]$ . Due to the nonlinearity of the operation of taking magnitude, the forward model is nonlinear. The samples generated by different algorithms are shown in Table 1. ", "page_idx": 9}, {"type": "text", "text": "We also present the sample images for the nonlinear problem of quantized sensing. In quantized sensing, each pixel of the image is randomly dithered and then quantized to one bit per channel. Nonlinearity of quantizing renders this forward model nonlinear. The samples generated by different algorithms are shown in Table 2. ", "page_idx": 9}, {"type": "text", "text": "Evaluation. We evaluate the performance of DPnP on the FFHQ validation dataset [KLA19] and the ImageNet validation dataset $[\\mathrm{RDS}^{+}15]$ . Since DPnP-DDIM has similar performance with DPnPDDPM but admits much faster implementation, only DPnP-DDIM is evaluated. The LPIPS and PSNR are shown in Table 3 and Table 4. These two metrics are arguably the more relevant ones for solving inverse problems. For comparison under other metrics such as FID, SSIM, cf. Appendix G.4. ", "page_idx": 9}, {"type": "text", "text": "It can be seen that, $\\mathsf{D P n P}$ is capable of solving both linear and nonlinear problems, and, in comparison with prior state-of-the-art, performs better in recovering fine and crisper details. ", "page_idx": 9}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper sets forth a rigorous and versatile algorithmic framework called DPnP for solving nonlinear inverse problems via posterior sampling, using image priors prescribed by score-based diffusion models with general forward models. DPnP alternates between two sampling steps implemented by DDS and PCS, to promote consistency with the data prior constraint and the measurement constraint respectively. We provide both asymptotic and non-asymptotic convergence guarantees, establishing DPnP as the first provably consistent and robust score-based diffusion posterior sampling method for general nonlinear inverse problems. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is supported in part by Office of Naval Research under N00014-19-1-2404, and by National Science Foundation under DMS-2134080 and ECCS-2126634. X. Xu is also gratefully supported by the Axel Berny Presidential Graduate Fellowship at Carnegie Mellon University. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[AGS05] L. Ambrosio, N. Gigli, and G. Savar\u00e9. Gradient flows: in metric spaces and in the space of probability measures. Springer Science & Business Media, 2005. ", "page_idx": 9}, {"type": "text", "text": "[And82] B. D. Anderson. Reverse-time diffusion equation models. Stochastic Processes and their Applications, 12(3):313\u2013326, 1982. ", "page_idx": 9}, {"type": "text", "text": "[BCSB18] G. T. Buzzard, S. H. Chan, S. Sreehari, and C. A. Bouman. Plug-and-play unplugged: Optimization-free reconstruction using consensus equilibrium. SIAM Journal on Imaging Sciences, 11(3):2001\u20132020, 2018.   \n[BDBDD24] J. Benton, V. De Bortoli, A. Doucet, and G. Deligiannidis. Nearly $d_{\\cdot}$ -linear convergence bounds for diffusion models via stochastic localization. In The Twelfth International Conference on Learning Representations, 2024. [BJPD17] A. Bora, A. Jalal, E. Price, and A. G. Dimakis. Compressed sensing using generative models. In International conference on machine learning, pages 537\u2013546. PMLR, 2017. [Bou23a] N. Bourbaki. Th\u00e9ories spectrales Chapitres 1 et 2. Springer, 2023. [Bou23b] N. Bourbaki. Th\u00e9ories spectrales: Chapitres 3 \u00e0 5. Springer Nature, 2023.   \n$[\\mathbf{CCL}^{+}22]$ S. Chen, S. Chewi, J. Li, Y. Li, A. Salim, and A. R. Zhang. Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. arXiv preprint arXiv:2209.11215, 2022.   \n$[\\mathrm{CCL}^{+}23]$ S. Chen, S. Chewi, H. Lee, Y. Li, J. Lu, and A. Salim. The probability flow ODE is provably fast. Neural Information Processing Systems, 2023. [CDC23] F. Coeurdoux, N. Dobigeon, and P. Chainais. Plug-and-play split Gibbs sampler: embedding deep generative priors in Bayesian inference. arXiv preprint arXiv:2304.11134, 2023. [CDD23] S. Chen, G. Daras, and A. Dimakis. Restoration-degradation beyond linear diffusions: A non-asymptotic analysis for DDIM-type samplers. In International Conference on Machine Learning, pages 4462\u20134484, 2023.   \n[CICM23] G. Cardoso, Y. J. E. Idrissi, S. L. Corff, and E. Moulines. Monte carlo guided diffusion for Bayesian linear inverse problems. arXiv preprint arXiv:2308.07983, 2023.   \n$[\\mathsf{C K M}^{+}23]$ H. Chung, J. Kim, M. T. Mccann, M. L. Klasky, and J. C. Ye. Diffusion posterior sampling for general noisy inverse problems. In International Conference on Learning Representations, 2023.   \n$[\\mathrm{CLA}^{+}21]$ S. Chewi, C. Lu, K. Ahn, X. Cheng, T. Le Gouic, and P. Rigollet. Optimal dimension dependence of the metropolis-adjusted langevin algorithm. In Conference on Learning Theory, pages 1260\u20131300. PMLR, 2021. [CLL23] H. Chen, H. Lee, and J. Lu. Improved analysis of score-based generative modeling: User-friendly bounds under minimal smoothness assumptions. In International Conference on Machine Learning, pages 4735\u20134763, 2023. [CLS15] E. J. Candes, X. Li, and M. Soltanolkotabi. Phase retrieval via Wirtinger flow: Theory and algorithms. IEEE Transactions on Information Theory, 61(4):1985\u20132007, 2015. [CLY23] H. Chung, S. Lee, and J. C. Ye. Fast diffusion sampler for inverse problems by geometric decomposition. arXiv preprint arXiv:2303.05754, 2023. [CR12] E. Candes and B. Recht. Exact matrix completion via convex optimization. Communications of the ACM, 55(6):111\u2013119, 2012. [Don06] D. L. Donoho. Compressed sensing. IEEE Transactions on Information Theory, 52(4):1289\u20131306, 2006. [Doo42] J. L. Doob. The Brownian movement and stochastic equations. Annals of Mathematics, 43(2):351\u2013369, 1942. [Dru17] D. Drusvyatskiy. The proximal point method revisited. arXiv preprint arXiv:1712.06038, 2017. [DS24] Z. Dou and Y. Song. Diffusion posterior sampling for linear inverse problem solving: A filtering perspective. In International Conference on Learning Representations, 2024. [Efr11] B. Efron. Tweedie\u2019s formula and selection bias. Journal of the American Statistical Association, 106(496):1602\u20131614, 2011. [Eva12] L. C. Evans. An introduction to stochastic differential equations, volume 82. American Mathematical Soc., 2012. [FBS24] Z. Fang, S. Buchanan, and J. Sulam. What\u2019s in a prior? Learned proximal networks for inverse problems. In The Twelfth International Conference on Learning Representations, 2024.   \n$[\\mathrm{FSR}^{+}23]$ B. T. Feng, J. Smith, M. Rubinstein, H. Chang, K. L. Bouman, and W. T. Freeman. Score-based diffusion models as principled priors for inverse imaging. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 10486\u201310497, 2023.   \n$[\\mathrm{GJP^{+}}24]$ S. Gupta, A. Jalal, A. Parulekar, E. Price, and Z. Xun. Diffusion posterior sampling is computationally intractable. arXiv preprint arXiv:2402.12727, 2024. [GL10] K. Gregor and Y. LeCun. Learning fast approximations of sparse coding. In Proceedings of the 27th international conference on international conference on machine learning, pages 399\u2013406, 2010.   \n[GMJS22] A. Graikos, N. Malkin, N. Jojic, and D. Samaras. Diffusion models as plug-and-play priors. Advances in Neural Information Processing Systems, 35:14715\u201314728, 2022. [HJA20] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840\u20136851, 2020.   \n[HSMC17] J. Huang, M. Sun, J. Ma, and Y. Chi. Super-resolution image reconstruction for high-density three-dimensional single-molecule microscopy. IEEE Transactions on Computational Imaging, 3(4):763\u2013773, 2017. [Hyv05] A. Hyv\u00e4rinen. Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research, 6(4), 2005.   \n[KEES22] B. Kawar, M. Elad, S. Ermon, and J. Song. Denoising diffusion restoration models. Advances in Neural Information Processing Systems, 35:23593\u201323606, 2022. [Key81] R. Keys. Cubic convolution interpolation for digital image processing. IEEE transactions on acoustics, speech, and signal processing, 29(6):1153\u20131160, 1981.   \n[KHR23] F. Koehler, A. Heckett, and A. Risteski. Statistical efficiency of score matching: The view from isoperimetry. International Conference on Learning Representations, 2023. [KLA19] T. Karras, S. Laine, and T. Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4401\u20134410, 2019.   \n[KVE21] B. Kawar, G. Vaksman, and M. Elad. Stochastic image denoising by sampling from the posterior distribution. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1866\u20131875, 2021.   \n$[\\mathrm{LBA}^{+}22]$ R. Laumont, V. D. Bortoli, A. Almansa, J. Delon, A. Durmus, and M. Pereyra. Bayesian imaging using plug & play priors: when Langevin meets Tweedie. SIAM Journal on Imaging Sciences, 15(2):701\u2013737, 2022. [LDP07] M. Lustig, D. Donoho, and J. M. Pauly. Sparse MRI: The application of compressed sensing for rapid MR imaging. Magnetic Resonance in Medicine: An Official Journal of the International Society for Magnetic Resonance in Medicine, 58(6):1182\u20131195, 2007.   \n$[\\mathrm{LHE}^{+}24]$ G. Li, Y. Huang, T. Efimov, Y. Wei, Y. Chi, and Y. Chen. Accelerating convergence of score-based diffusion models, provably. arXiv preprint arXiv:2403.03852, 2024.   \n[LHW24] G. Li, Z. Huang, and Y. Wei. Towards a mathematical theory for consistency training in diffusion models. arXiv preprint arXiv:2402.07802, 2024. [LST21] Y. T. Lee, R. Shen, and K. Tian. Structured logconcave sampling with a restricted Gaussian oracle. In Conference on Learning Theory, pages 2993\u20133050. PMLR, 2021.   \n[LWCC23] G. Li, Y. Wei, Y. Chen, and Y. Chi. Towards faster non-asymptotic convergence for diffusion-based generative models. arXiv preprint arXiv:2306.09251, 2023.   \n$[\\mathrm{L}Z\\mathbf{B}^{+}22]$ C. Lu, Y. Zhou, F. Bao, J. Chen, C. Li, and J. Zhu. DPM-Solver: A fast ODE solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:5775\u20135787, 2022.   \n[MLE21] V. Monga, Y. Li, and Y. C. Eldar. Algorithm unrolling: Interpretable, efficient deep learning for signal and image processing. IEEE Signal Processing Magazine, 38(2):18\u2013 44, 2021.   \n[MSKV24] M. Mardani, J. Song, J. Kautz, and A. Vahdat. A variational perspective on solving inverse problems with diffusion models. In The Twelfth International Conference on Learning Representations, 2024. [MV19] O. Mangoubi and N. K. Vishnoi. Nonconvex sampling with the metropolis-adjusted langevin algorithm. In Conference on learning theory, pages 2259\u20132293. PMLR, 2019.   \n[PEPC10] L. C. Potter, E. Ertin, J. T. Parker, and M. Cetin. Sparsity and compressed sensing in radar imaging. Proceedings of the IEEE, 98(6):1006\u20131020, 2010.   \n$[\\mathsf{P R S}^{+}24]$ C. Pabbaraju, D. Rohatgi, A. P. Sevekari, H. Lee, A. Moitra, and A. Risteski. Provable benefits of score matching. Advances in Neural Information Processing Systems, 36, 2024. [PW24] Y. Polyanskiy and Y. Wu. Information theory: From coding to learning. Cambridge university press, 2024.   \n$[\\mathrm{RBL}^{+}22]$ R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684\u201310695, 2022.   \n$[\\mathrm{RDN}^{+}22]$ A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. Hierarchical textconditional image generation with CLIP latents. arXiv preprint arXiv:2204.06125, 2022.   \n$[\\mathrm{RDS^{+}15}]$ O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211\u2013 252, 2015.   \n[REM17] Y. Romano, M. Elad, and P. Milanfar. The little engine that could: Regularization by denoising (RED). SIAM Journal on Imaging Sciences, 10(4):1804\u20131844, 2017. [RR98] G. O. Roberts and J. S. Rosenthal. Optimal scaling of discrete approximations to Langevin diffusions. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 60(1):255\u2013268, 1998. [RS18] E. T. Reehorst and P. Schniter. Regularization by denoising: Clarifications and new interpretations. IEEE transactions on computational imaging, 5(1):52\u201367, 2018. [SC97] L. Saloff-Coste. Lectures on finite Markov chains, pages 301\u2013413. Springer Berlin Heidelberg, Berlin, Heidelberg, 1997. [Sch12] H. Schaefer. Banach Lattices and Positive Operators, volume 215. Springer Science & Business Media, 2012. $[{\\mathrm{SCS}}^{+}22]$ C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes, B. Karagol Ayan, T. Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:36479\u201336494, 2022.   \nSDWMG15] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pages 2256\u20132265, 2015. [SE19] Y. Song and S. Ermon. Generative modeling by estimating gradients of the data distribution. Advances in Neural Information Processing Systems, 32, 2019. $[{\\mathrm{SEC}}^{+}15]$ Y. Shechtman, Y. C. Eldar, O. Cohen, H. N. Chapman, J. Miao, and M. Segev. Phase retrieval with application to optical imaging: a contemporary overview. IEEE signal processing magazine, 32(3):87\u2013109, 2015. $[{\\mathrm{SKZ}}^{+}23]$ B. Song, S. M. Kwon, Z. Zhang, X. Hu, Q. Qu, and L. Shen. Solving inverse problems with latent diffusion models via hard data consistency. arXiv preprint arXiv:2307.08123, 2023. [SME20] J. Song, C. Meng, and S. Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2020.   \n$[\\mathrm{SSDK}^{+}21]$ Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole. Scorebased generative modeling through stochastic differential equations. International Conference on Learning Representations, 2021.   \n[SSXE21] Y. Song, L. Shen, L. Xing, and S. Ermon. Solving inverse problems in medical imaging with score-based generative models. In International Conference on Learning Representations, 2021.   \n[SVMK22] J. Song, A. Vahdat, M. Mardani, and J. Kautz. Pseudoinverse-guided diffusion models for inverse problems. In International Conference on Learning Representations, 2022.   \n[SWC $^{+}23$ ] Y. Sun, Z. Wu, Y. Chen, B. T. Feng, and K. L. Bouman. Provable probabilistic imaging using score-based generative priors. arXiv preprint arXiv:2310.10835, 2023.   \n$[\\mathrm{SZY}^{+}23]$ J. Song, Q. Zhang, H. Yin, M. Mardani, M.-Y. Liu, J. Kautz, Y. Chen, and A. Vahdat. Loss-guided diffusion models for plug-and-play controllable generation. In International Conference on Machine Learning, pages 32483\u201332498. PMLR, 2023. [Tie94] L. Tierney. Markov Chains for Exploring Posterior Distributions. The Annals of Statistics, 22(4):1701 \u2013 1728, 1994.   \n$[\\mathrm{TYT}^{+}22]$ B. L. Trippe, J. Yim, D. Tischer, D. Baker, T. Broderick, R. Barzilay, and T. Jaakkola. Diffusion probabilistic modeling of protein backbones in 3d for the motif-scaffolding problem. arXiv preprint arXiv:2206.04119, 2022. [TZ24] W. Tang and H. Zhao. Contractive diffusion probabilistic models. arXiv preprint arXiv:2401.13115, 2024. [UVL18] D. Ulyanov, A. Vedaldi, and V. Lempitsky. Deep image prior. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 9446\u20139454, 2018. [VBW13] S. V. Venkatakrishnan, C. A. Bouman, and B. Wohlberg. Plug-and-play priors for model based reconstruction. In IEEE Global Conference on Signal and Information Processing, pages 945\u2013948. IEEE, 2013.   \n[VDC19] M. Vono, N. Dobigeon, and P. Chainais. Split-and-augmented Gibbs samplerapplication to large-scale inference problems. IEEE Transactions on Signal Processing, 67(6):1648\u20131661, 2019. [Vin11] P. Vincent. A connection between score matching and denoising autoencoders. Neural computation, 23(7):1661\u20131674, 2011.   \n[Wib18] A. Wibisono. Sampling as optimization in the space of measures: The Langevin dynamics as a composite optimization problem. In Conference on Learning Theory, pages 2093\u20133027. PMLR, 2018.   \nWTN+23] L. Wu, B. Trippe, C. Naesseth, D. Blei, and J. P. Cunningham. Practical and asymptotically exact conditional sampling in diffusion models. Advances in Neural Information Processing Systems, 36, 2023. [ZC22] Q. Zhang and Y. Chen. Fast sampling of diffusion models with exponential integrator. In The Eleventh International Conference on Learning Representations, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Related works ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Algorithmic unrolling and plug-and-play image reconstruction. Composite optimization algorithms, which aim to minimize the sum of a measurement fidelity term and a regularization term promoting desirable solution structures, have been the backbone of inverse problem solvers. To unleash the power of deep learning, [GL10] advocates the perspective of algorithmic unrolling, which turns an iterative algorithm into concatenations of linear and nonlinear layers like in a neural network. [VBW13] recognized that the proximal mapping step in many composite optimization algorithms can be regarded as a denoiser or denoising operator with respect to the given prior, and proposed to \u201cplug in\u201d alternative denoisers, in particular state-of-the-art deep learning denoisers, leading to a class of popular algorithms known as plug-and-play methods [BCSB18]; see [MLE21] for a review. ", "page_idx": 15}, {"type": "text", "text": "Regularization by denoising and score matching. [Vin11] pointed out a connection between score matching and image denoising, which is a consequence of the Tweedie\u2019s formula [Efr11]. The regularization by denoising (RED) framework [REM17] follows the plug-and-play framework to minimize a regularized objective function, where the regularizer is defined based on the plug-in image denoiser; [RS18] later clarified that the RED framework can be interpreted as score matching by denoising using the Tweedie\u2019s formula. [KVE21] developed a stochastic image denoiser for posterior sampling of image denoising using annealed Langevin dynamics. [FBS24] provided a framework to learn exact proximal operators for inverse problems. ", "page_idx": 15}, {"type": "text", "text": "Plug-and-play posterior sampling. Motivated by the need to characterize the uncertainty, tackling image reconstruction as posterior sampling from a Bayesian perspective is another important approach. Our method is inspired by the plug-and-play framework but takes on a sampling perspective, exploiting the connection between optimization and sampling [Wib18]. Along similar lines, $[\\mathbf{LBA}^{+}22$ , BB23] proposed Bayesian counterparts of plug-and-play for posterior sampling, where they leveraged the connection to score matching for sampling from the image prior, but did not consider score-based diffusion models for the image prior, which is a key aspect of ours; see also $[\\mathbf{S}\\mathbf{W}\\mathbf{C}^{+}23]$ . [CDC23] extended the split Gibbs sampler [VDC19] in the plug-and-play framework, and advocated the use of score-based diffusion models such as DDPM [HJA20] for image denoising based on heuristic observations. In contrast, we rigorously derive the denoising diffusion samplers from first principles, unraveling critical gaps from na\u00efve applications of the generative samplers to denoising, and offer theoretical guarantees on the correctness of our approach. ", "page_idx": 15}, {"type": "text", "text": "Score-based diffusion models as image priors. Several representative methods for solving inverse problems using score-based diffusion priors alternates between taking steps along the diffusion process and projecting onto the measurement constraint, e.g., $[\\mathbf{CKM}^{+}2\\bar{3}$ , KEES22, $S\\mathrm{K}Z^{+}23$ , CLY23, GMJS22, SVMK22]. However, these approaches do not possess asymptotic consistency guarantees. $[\\mathrm{SZY}^{+}23]$ proposed to use multiple Monte Carlo samples to reduce bias. On the other hand, [CICM23] developed Monte Carlo guided diffusion methods for Bayesian linear inverse problems which tend to be computationally expensive, and [DS24] recently introduced a filtering perspective and applied particle filtering. Although asymptotically consistent, these approaches are limited to linear inverse problems. $[\\mathrm{TYT}^{+}22$ , $\\mathrm{WTN}^{+}23]$ introduced sequential Monte Carlo (SMC) algorithms for conditional sampling using unconditional diffusion models that are asymptotically exact. [MSKV24] developed a variational perspective that connects to the regularization by denoising framework. $[\\mathrm{GJP^{+}}24]$ showed that the worst-case complexity of diffusion posterior sampling can take super-polynomial time regardless of the algorithm in use. ", "page_idx": 15}, {"type": "text", "text": "Theory of diffusion models and score matching. A number of recent papers have studied the nonasymptotic convergence rates of popular diffusion samplers, including but not limited to stochastic DDPM-type samplers $[\\mathbf{CCL}^{+}22$ , CLL23, LWCC23, BDBDD24, TZ24], deterministic DDIM-type samplers [LWCC23, $\\mathrm{CCL}^{+}23$ , CDD23], and accelerated samplers $[\\mathrm{LHE}^{+}24$ , LHW24]. In addition, the statistical efficiency of score matching has also been investigated [KHR23, $\\mathrm{PRS}^{+}24]$ . ", "page_idx": 15}, {"type": "text", "text": "B Discrete-time formulation of diffusion processes ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The discrete-time forward and backward diffusion processes can be understood as two processes constructed in the following manner: ", "page_idx": 15}, {"type": "text", "text": "1) a forward process ", "page_idx": 15}, {"type": "equation", "text": "$$\nx_{0}\\rightarrow x_{1}\\rightarrow\\dots\\rightarrow x_{T}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "that starts with samples from the target image distribution and diffuses into a noise distribution (e.g., standard Gaussians) by gradually injecting noise into the samples; ", "page_idx": 15}, {"type": "text", "text": "2) a reverse process ", "page_idx": 16}, {"type": "equation", "text": "$$\nx_{T}^{\\mathsf{r e v}}\\to x_{T-1}^{\\mathsf{r e v}}\\to\\cdots\\to x_{0}^{\\mathsf{r e v}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "that starts from pure noise (e.g., standard Gaussians) and converts it into samples whose distribution is close to the target image distribution. ", "page_idx": 16}, {"type": "text", "text": "Consider the forward Markov process in $\\mathbb{R}^{d}$ that starts with a sample from the data distribution $p_{X}$ , and adds noise over the trajectory according to ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x_{0}\\sim p^{\\star},}\\\\ &{x_{t}=\\sqrt{1-\\beta_{t}}\\,x_{t-1}+\\sqrt{\\beta_{t}}\\,w_{t},\\qquad1\\leq t\\leq T,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\{w_{t}\\}_{1\\le t\\le T}$ \u2019s are independent standard Gaussian vectors, i.e., $w_{t}\\overset{\\mathrm{i.i.d.}}{\\sim}\\mathcal{N}(0,I_{d})$ , and $\\{\\beta_{t}\\in$ $(0,1)\\}$ describes the noise-injection rates used in each step. Therefore, we can write $x_{t}$ equivalently as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{x_{t}:=\\sqrt{\\bar{\\alpha}_{t}}\\,x_{0}+\\sqrt{1-\\bar{\\alpha}_{t}}\\,\\varepsilon_{t},\\quad\\varepsilon_{t}\\sim\\mathcal{N}(0,I_{d}),\\quad t=0,1,\\cdots\\,,T.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Here, $(\\bar{\\alpha}_{t})_{t=0,1,\\cdots,T}$ is the schedule of diffusion given by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\alpha_{t}:=1-\\beta_{t},\\qquad\\bar{\\alpha}_{t}:=\\prod_{k=1}^{t}\\alpha_{k},\\qquad1\\le t\\le T.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Clearly, it verifies that $1\\geq\\bar{\\alpha}_{0}>\\bar{\\alpha}_{1}>\\cdot\\cdot>\\bar{\\alpha}_{T}>0$ . As long as $\\bar{\\alpha}_{T}$ is vanishing, it is easy to observe that the distribution of $x_{T}$ approaches $\\mathcal{N}(0,I_{d})$ . ", "page_idx": 16}, {"type": "text", "text": "Score functions. As will be seen, in order to sample from $p^{\\star}$ , it turns out to be sufficient to learn the score functions of $p_{x_{t}}$ at each step of the forward process, defined as ", "page_idx": 16}, {"type": "equation", "text": "$$\ns_{t}^{\\star}(x)=\\nabla\\log p_{x_{t}}(x),\\qquad t=0,1,\\cdots\\,,T.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "As in the continuous time, the score function can be viewed as a MMSE estimate: ", "page_idx": 16}, {"type": "equation", "text": "$$\ns_{t}^{\\star}(x)=-\\frac{1}{\\sqrt{1-\\bar{\\alpha}_{t}}}\\underbrace{\\mathbb{E}_{x_{0}\\sim p^{\\star},\\,\\varepsilon_{t}\\sim\\mathcal{N}(0,I_{d})}(\\varepsilon_{t}\\,|\\,\\sqrt{\\bar{\\alpha}_{t}}x_{0}+\\sqrt{1-\\bar{\\alpha}_{t}}\\varepsilon_{t}=x)}_{=:\\varepsilon_{t}^{\\star}(x)}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Comparing (27) and (2), it can be checked that the discrete-time diffusion process can be embedded into the continuous-time one via the time change ", "page_idx": 16}, {"type": "equation", "text": "$$\nt\\mapsto\\frac{1}{2}\\log\\frac{1}{\\bar{\\alpha}_{t}},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "in the sense that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{x_{t}^{\\star}\\overset{(\\mathrm{d})}{=}X_{\\frac{1}{2}\\log\\frac{1}{\\bar{\\alpha}_{t}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This establishes a correspondence between the continuous-time formulation and the discrete-time formulation. ", "page_idx": 16}, {"type": "text", "text": "Within the discrete-time formulation, we assume the score function estimates are given as $\\widehat{s}_{t}(\\cdot)$ : $\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d}$ , $t=1,\\dots,T$ such that $\\widehat{s}_{t}\\approx s_{t}^{\\star}$ in analogy with the continuous-time counterpart. ", "page_idx": 16}, {"type": "text", "text": "C Details of algorithm subroutines ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "With the discrete-time perspective established in Appendix B, we are now ready to present the detailed description and the implementation of our algorithms DDS-DDPM (Algorithm 2), DDS-DDIM (Algorithm 3), and PCS (Algorithm 4). ", "page_idx": 16}, {"type": "text", "text": "D Score functions of diffusion denoising samplers ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "D.1 Proof of Lemma 1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. The marginal distribution (14) of the heat flow can be written as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Y_{\\tau}\\overset{\\mathrm{(d)}}{=}Y_{0}+\\sqrt{\\tau}\\varepsilon,\\quad Y_{0}\\sim p^{\\star},\\;\\varepsilon\\sim\\mathcal{N}(0,I_{d}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Comparing (2) and (31), it is not hard to check that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Y_{\\tau}\\overset{\\mathrm{(d)}}{=}\\sqrt{1+\\tau}X_{\\frac{1}{2}\\log(1+\\tau)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Algorithm 2 Denoising Diffusion Sampler (stochastic) DDS-DDPM(xnoisy,s, \u03b7) ", "page_idx": 17}, {"type": "text", "text": "Input: noisy data $x_{\\mathsf{n o i s y}}\\in\\mathbb{R}^{d}$ , score estimates $\\widehat{s}:=\\{\\widehat{s}_{t}(\\cdot):\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d},t=1,\\ldots,T\\}$ or noise estimates $\\widehat{\\varepsilon}=\\left\\{\\widehat{\\varepsilon}_{t}(\\cdot):\\mathring{\\mathbb{R}}^{d}\\rightarrow\\mathbb{R}^{d},t=1,\\cdot\\cdot\\cdot,T\\right\\}$ , and noise level $\\eta>0$ .   \nScheduli n g: Co mpute the diffusion schedule $(\\tau_{t})_{0\\leq t\\leq T^{\\prime}}$ by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\tau_{t}=\\bar{\\alpha}_{t}^{-1}-1,\\quad0\\leq t\\leq T^{\\prime},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ", "page_idx": 17}, {"type": "equation", "text": "$$\nT^{\\prime}:=\\operatorname*{max}\\left\\{t:0\\leq t\\leq T,\\,\\bar{\\alpha}_{t}>\\frac{1}{\\eta^{2}+1}\\right\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Initialization: Set $\\widehat{x}_{T^{\\prime}}=x_{\\mathsf{n o i s y}}$ ", "page_idx": 17}, {"type": "text", "text": "Diffusion: for $t=T^{\\prime},T^{\\prime}-1,\\stackrel{.}{\\dots}...,1$ do ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\widehat{x}_{t-1}=\\widehat{x}_{t}-2(\\sqrt{\\tau_{t}}-\\sqrt{\\tau_{t-1}})\\,\\widehat{\\varepsilon}_{t}+\\sqrt{\\tau_{t}-\\tau_{t-1}}\\,w_{t},\\quad w_{t}\\sim\\mathcal{N}(0,I_{d}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\widehat{\\varepsilon}}_{t}:={\\widehat{\\varepsilon}}_{t}(\\sqrt{{\\bar{\\alpha}}_{t}}\\,{\\widehat x}_{t})=-\\frac{1}{\\sqrt{1-{\\bar{\\alpha}}_{t}}}{\\widehat s}_{t}\\left(\\sqrt{{\\bar{\\alpha}}_{t}}\\,{\\widehat x}_{t}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Output: $\\widehat{x}_{0}$ ", "page_idx": 17}, {"type": "text", "text": "Algorithm 3 Denoising Diffusion Sampler (deterministic) DDS-DDIM(xnoisy,s, \u03b7) ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Input: noisy data $x_{\\mathsf{n o i s y}}\\in\\mathbb{R}^{d}$ , score estimates $\\widehat{s}:=\\{\\widehat{s}_{t}(\\cdot):\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d},t=1,\\ldots,T\\}$ or noise estimates $\\widehat{\\varepsilon}=\\left\\{\\widehat{\\varepsilon}_{t}(\\cdot):\\mathring{\\mathbb{R}}^{d}\\rightarrow\\mathbb{R}^{d},t=1,\\cdot\\cdot\\cdot,T\\right\\}$ ,   and noi se level $\\eta>0$ .   \nScheduling: Compute the diffusion schedule $(\\bar{u}_{t})_{0\\leq t\\leq T^{\\prime}}$ by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\bar{u}_{t}=\\frac{(\\eta^{2}+1)\\bar{\\alpha}_{t}-1}{\\eta^{2}+\\bar{\\alpha}_{t}-1},\\quad0\\leq t\\leq T^{\\prime},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ", "page_idx": 17}, {"type": "equation", "text": "$$\nT^{\\prime}:=\\operatorname*{max}\\left\\{t:0\\leq t\\leq T,\\,\\bar{\\alpha}_{t}>\\frac{1}{\\eta^{2}+1}\\right\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Initialization: Draw $z_{T^{\\prime}}\\sim\\mathcal{N}(0,I_{d})$ . Diffusion: for $t=T^{\\prime},T^{\\prime}-1,\\dots,1\\,\\mathrm{~$ do ", "page_idx": 17}, {"type": "equation", "text": "$$\nz_{t-1}=\\frac{\\sqrt{(\\eta^{2}-1)\\bar{u}_{t-1}+1}}{\\sqrt{(\\eta^{2}-1)\\bar{u}_{t}+1}}z_{t}+\\sqrt{(\\eta^{2}-1)\\bar{u}_{t-1}+1}\\cdot\\big(h(\\eta,\\bar{u}_{t-1})-h(\\eta,\\bar{u}_{t})\\big)\\widehat{\\varepsilon}_{t},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ", "text_level": 1, "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h(\\eta,u):=-\\arctan\\frac{\\eta}{\\sqrt{u^{-1}-1}},}\\\\ &{\\qquad\\widehat{\\varepsilon}_{t}:=\\widehat{\\varepsilon}_{t}\\left(\\sqrt{\\widehat{\\alpha}_{t}}x_{\\mathrm{noisy}}+\\frac{\\eta^{2}\\sqrt{\\widehat{u}_{t}\\widehat{\\alpha}_{t}}z_{t}}{(\\eta^{2}-1)\\widehat{u}_{t}+1}\\right)=-\\frac{1}{\\sqrt{1-\\widehat{\\alpha}_{t}}}\\widehat{s}_{t}\\left(\\sqrt{\\widehat{\\alpha}_{t}}x_{\\mathrm{noisy}}+\\frac{\\eta^{2}\\sqrt{\\widehat{u}_{t}\\widehat{\\alpha}_{t}}z_{t}}{(\\eta^{2}-1)\\widehat{u}_{t}+1}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Output: $x_{\\mathsf{n o i s y}}+z_{0}$ . ", "page_idx": 17}, {"type": "text", "text": "Denote $\\begin{array}{r}{\\theta=\\frac{1}{2}\\log(1+\\tau)}\\end{array}$ as a short-hand. We have ", "page_idx": 17}, {"type": "equation", "text": "$$\np_{Y_{\\tau}}(x)=p_{\\sqrt{1+\\tau}X_{\\theta}}(x)\\propto p_{X_{\\theta}}\\left(\\frac{1}{\\sqrt{1+\\tau}}x\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore it follows that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\nabla\\log p_{Y_{\\tau}}(x)=\\nabla_{x}\\log p_{X_{\\theta}}\\left(\\frac{1}{\\sqrt{1+\\tau}}x\\right)=\\frac{1}{\\sqrt{1+\\tau}}s\\left(\\theta,\\frac{1}{\\sqrt{1+\\tau}}x\\right),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we used the definition $s(\\theta,\\cdot)=\\nabla\\log p_{X_{\\theta}}(\\cdot)$ . Plugging the definition $\\begin{array}{r}{\\theta=\\frac{1}{2}\\log(1+\\tau)}\\end{array}$ into the above equation yields the desired result. ", "page_idx": 17}, {"type": "text", "text": "Algorithm 4 Proximal Consistency Sampler $\\mathsf{P C S}(x,y,\\mathcal{L},\\eta)$ (adapted from Metropolis-Adjusted Langevin Algorithm [RR98]) ", "page_idx": 18}, {"type": "text", "text": "Input: starting point $x\\,\\in\\,\\mathbb{R}^{d}$ , measurements $y\\,\\in\\,\\mathbb{R}^{m}$ , log-likelihood function of the forward model $\\mathcal{L}(\\cdot;y)$ , proximal parameter $\\eta>0$ .   \nHyperparameter: Langevin stepsize $\\gamma$ , and the number of iterations $N$ .   \nInitialization: $z_{0}=x$ .   \nUpdate: for $n=0,1,\\cdot\\cdot\\cdot\\,,N-1$ do   \n(1) One step of discretized Langevin: Set $r=\\mathrm{e}^{-\\gamma/\\eta^{2}}$ , and ", "page_idx": 18}, {"type": "equation", "text": "$$\nz_{n+\\frac{1}{2}}=r z_{n}+(1-r)x+\\eta^{2}(1-r)\\nabla_{z_{n}}\\mathcal{L}(z_{n};y)+\\eta\\sqrt{1-r^{2}}w_{n},\\quad w_{n}\\sim\\mathcal{N}(0,I_{d}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This is equivalent to drawing $z_{n+\\frac{1}{2}}$ from a distribution with density $Q(\\cdot;z_{n})$ , where ", "page_idx": 18}, {"type": "equation", "text": "$$\nQ(z^{\\prime};z)=\\frac{1}{(2\\pi(1-r^{2}))^{d/2}}\\exp\\left(-\\frac{\\left\\|z^{\\prime}-\\left(r z+(1-r)x+\\eta^{2}(1-r)\\nabla_{z}\\mathcal{L}(z;y)\\right)\\right\\|^{2}}{2(1-r^{2})}\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "(2) Metropolis adjustment: Compute ", "text_level": 1, "page_idx": 18}, {"type": "equation", "text": "$$\nq=\\frac{\\exp\\left(\\mathcal{L}(z_{n+\\frac{1}{2}};y)-\\frac{1}{2\\eta^{2}}\\|z_{n+\\frac{1}{2}}-x\\|^{2}\\right)}{\\exp\\left(\\mathcal{L}(z_{n};y)-\\frac{1}{2\\eta^{2}}\\|z_{n}-x\\|^{2}\\right)}\\cdot\\frac{Q(z_{n};z_{n+\\frac{1}{2}})}{Q(z_{n+\\frac{1}{2}};z_{n})},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and set ", "page_idx": 18}, {"type": "equation", "text": "$$\nz_{n+1}=\\left\\{{z_{n+\\frac{1}{2}}},\\begin{array}{l l}{\\mathrm{with~probability~min}(1,q),}\\\\ {\\mathrm{with~probability~1-min}(1,q).}\\end{array}}\\right.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Output: $z_{N}$ ", "page_idx": 18}, {"type": "text", "text": "D.2 Proof of Lemma 2 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof. We first compute the probability density function of $z$ . Recall that $z\\,=\\,w\\,-\\,x_{\\mathsf{n o i s y}}$ , thus applying Bayes rule yields ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p_{z}(x)=p_{w}(x+x_{\\mathrm{noisy}})=p^{\\star}(x^{\\star}=x+x_{\\mathrm{noisy}}|x^{\\star}+\\xi=x_{\\mathrm{noisy}})}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\frac{p^{\\star}(x+x_{\\mathrm{noisy}})p_{\\xi}(-x)}{p_{x^{\\star}+\\xi}(x_{\\mathrm{noisy}})}\\propto p^{\\star}(x+x_{\\mathrm{noisy}})p_{\\xi}(-x),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\xi\\sim\\mathcal{N}(0,\\eta^{2}I_{d})$ . It is straightforward to compute ", "page_idx": 18}, {"type": "equation", "text": "$$\np_{\\xi}(-x)=\\frac{1}{(2\\pi)^{d/2}\\eta^{d}}\\mathrm{e}^{-\\frac{1}{2\\eta^{2}}\\|x\\|^{2}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "therefore ", "page_idx": 18}, {"type": "equation", "text": "$$\np_{z}(x)\\propto p^{\\star}(x+x_{\\mathrm{noisy}})\\mathrm{e}^{-\\frac{1}{2\\eta^{2}}\\|x\\|^{2}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We proceed to compute the probability density function of $Z_{\\tau}$ . According to (19), it follows that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle p_{Z_{\\tau}}(x)=p_{\\mathrm{e}^{-\\tau}z}\\ast p_{\\sqrt{1-\\mathrm{e}^{-2\\tau}\\varepsilon}}(x)}\\\\ {\\displaystyle\\qquad\\qquad=\\int p_{\\mathrm{e}^{-\\tau}z}(x^{\\prime})\\,p_{\\sqrt{1-\\mathrm{e}^{-2\\tau}\\varepsilon}}(x-x^{\\prime})\\mathrm{d}x^{\\prime}}\\\\ {\\displaystyle\\qquad\\propto\\int p_{z}(\\mathrm{e}^{\\tau}x^{\\prime})\\exp\\left(-\\frac{1}{2(1-\\mathrm{e}^{-2\\tau})}\\|x-x^{\\prime}\\|^{2}\\right)\\mathrm{d}x^{\\prime}}\\\\ {\\displaystyle\\qquad\\propto\\int p^{\\star}(x_{\\mathrm{noisy}}+\\mathrm{e}^{\\tau}x^{\\prime})\\exp\\left(-\\frac{1}{2\\eta^{2}}\\|\\mathrm{e}^{\\tau}x^{\\prime}\\|^{2}\\right)\\exp\\left(-\\frac{1}{2(1-\\mathrm{e}^{-2\\tau})}\\|x-x^{\\prime}\\|^{2}\\right)\\mathrm{d}x^{\\prime},}\\\\ {\\displaystyle\\qquad\\propto\\int p^{\\star}(x^{\\prime})\\exp\\left(-\\frac{1}{2\\eta^{2}}\\|x^{\\prime}-x_{\\mathrm{noisy}}\\|^{2}\\right)\\exp\\left(-\\frac{1}{2(1-\\mathrm{e}^{-2\\tau})}\\|x-\\mathrm{e}^{-\\tau}(x^{\\prime}-x_{\\mathrm{noisy}})\\|^{2}\\right)\\mathrm{d}x.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $^*$ denotes convolution, the penultimate line follows from (32) and the last line follow from the change of variable $x^{\\prime}\\mapsto\\mathrm{e}^{-\\tau}(x^{\\prime}-x_{\\mathsf{n o i s y}})$ . One may exercise some brute force to verify that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\exp\\left(-\\frac{1}{2\\eta^{2}}\\|x^{\\prime}-x_{\\mathrm{noisy}}\\|^{2}\\right)\\exp\\left(-\\frac{1}{2(1-\\mathrm{e}^{-2\\tau})}\\|x-\\mathrm{e}^{-\\tau}(x^{\\prime}-x_{\\mathrm{noisy}})\\|^{2}\\right)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\exp\\left(-\\displaystyle\\frac{\\mathrm{e}^{2\\tau}\\|x\\|^{2}}{2(\\eta^{2}+\\mathrm{e}^{2\\tau}-1)}\\right)\\exp\\left(-\\displaystyle\\frac{1}{2(1-\\mathrm{e}^{-2\\tilde{\\tau}})}\\Big\\|\\mathrm{e}^{-\\tilde{\\tau}}x_{\\mathrm{noisy}}+\\displaystyle\\frac{\\mathrm{e}^{\\tau-\\tilde{\\tau}}\\eta^{2}x}{\\eta^{2}+\\mathrm{e}^{2\\tau}-1}-\\mathrm{e}^{-\\tilde{\\tau}}x^{\\prime}\\Big\\|^{2}\\right)}\\\\ &{\\propto\\exp\\left(-\\displaystyle\\frac{\\mathrm{e}^{2\\tau}\\|x\\|^{2}}{2(\\eta^{2}+\\mathrm{e}^{2\\tau}-1)}\\right)p_{\\sqrt{1-\\mathrm{e}^{-2\\tilde{\\tau}}}\\varepsilon}\\left(\\mathrm{e}^{-\\tilde{\\tau}}x_{\\mathrm{noisy}}+\\displaystyle\\frac{\\mathrm{e}^{\\tau-\\tilde{\\tau}}\\eta^{2}x}{\\eta^{2}+\\mathrm{e}^{2\\tau}-1}-\\mathrm{e}^{-\\tilde{\\tau}}x^{\\prime}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\tilde{\\tau}$ is as defined in (22). Plug this back into (33), we see ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma_{z_{\\tau}}(x)\\propto\\exp\\left(-\\frac{\\mathrm{e}^{2\\tau}\\|x\\|^{2}}{2(\\eta^{2}+\\mathrm{e}^{2\\tau}-1)}\\right)\\int p^{\\star}(x^{\\prime})p\\sqrt{1-\\mathrm{e}^{-2\\tau}}\\varepsilon\\left(\\mathrm{e}^{-\\tilde{\\tau}}x_{\\mathrm{noisy}}+\\frac{\\mathrm{e}^{\\tau-\\tilde{\\tau}}\\eta^{2}x}{\\eta^{2}+\\mathrm{e}^{2\\tau}-1}-\\mathrm{e}^{-\\tilde{\\tau}}x^{\\prime}\\right)\\mathrm{d}x^{\\prime}}\\\\ &{\\qquad\\propto\\exp\\left(-\\frac{\\mathrm{e}^{2\\tau}\\|x\\|^{2}}{2(\\eta^{2}+\\mathrm{e}^{2\\tau}-1)}\\right)\\int p^{\\star}(\\mathrm{e}^{\\tilde{\\tau}}x^{\\prime})p\\sqrt{1-\\mathrm{e}^{-2\\tilde{\\tau}}}\\varepsilon\\left(\\mathrm{e}^{-\\tilde{\\tau}}x_{\\mathrm{noisy}}+\\frac{\\mathrm{e}^{\\tau-\\tilde{\\tau}}\\eta^{2}x}{\\eta^{2}+\\mathrm{e}^{2\\tau}-1}-x^{\\prime}\\right)\\mathrm{d}x^{\\prime}}\\\\ &{\\qquad\\propto\\exp\\left(-\\frac{\\mathrm{e}^{2\\tau}\\|x\\|^{2}}{2(\\eta^{2}+\\mathrm{e}^{2\\tau}-1)}\\right)p_{\\mathrm{e}^{-\\tau}x_{0}}\\ast p\\sqrt{1-\\mathrm{e}^{-2\\tilde{\\tau}}}\\varepsilon\\left(\\mathrm{e}^{-\\tilde{\\tau}}x_{\\mathrm{noisy}}+\\frac{\\mathrm{e}^{\\tau-\\tilde{\\tau}}\\eta^{2}x}{\\eta^{2}+\\mathrm{e}^{2\\tau}-1}\\right)}\\\\ &{\\qquad\\propto\\exp\\left(-\\frac{\\mathrm{e}^{2\\tau}\\|x\\|^{2}}{2(\\eta^{2}+\\mathrm{e}^{2\\tau}-1)}\\right)p_{X_{\\tau}}\\left(\\mathrm{e}^{-\\tilde{\\tau}}x_{\\mathrm{noisy}}+\\frac{\\mathrm{e}^{\\tau-\\tilde{\\tau}}\\eta^{2}x}{\\eta^{2}+\\mathrm{e}^{2\\tau}-1}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the second line applies the change of variable $x^{\\prime}\\mapsto\\mathrm{e}^{\\tilde{\\tau}}x^{\\prime}$ in the integral, the penultimate line follow\u221as from $p_{\\mathrm{e}^{-\\tilde{\\tau}}x_{0}}(x^{\\prime})\\,\\propto\\,p^{\\star}(\\mathrm{e}^{\\tilde{\\tau}}x^{\\prime})$ (since $x_{0}\\sim p^{\\star}$ ), and the last line follows from $\\boldsymbol{X}_{\\tilde{\\tau}}\\stackrel{\\mathrm{(d)}}{=}$ $\\mathrm{e}^{-\\tilde{\\tau}}x_{0}+\\sqrt{1-\\mathrm{e}^{-2\\tilde{\\tau}}}\\varepsilon$ . ", "page_idx": 19}, {"type": "text", "text": "Finally, from the above formula, we obtain ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla\\log p_{Z_{\\tau}}(x)=\\nabla_{x}\\left(-\\frac{\\mathrm{e}^{2\\tau}\\|x\\|^{2}}{2\\left(\\eta^{2}+\\mathrm{e}^{2\\tau}-1\\right)}\\right)+\\nabla_{x}\\log p_{X_{\\bar{\\tau}}}\\left(\\mathrm{e}^{-\\tilde{\\tau}}x_{\\mathrm{noisy}}+\\frac{\\mathrm{e}^{\\tau-\\tilde{\\tau}}\\eta^{2}x}{\\eta^{2}+\\mathrm{e}^{2\\tau}-1}\\right)}\\\\ &{\\qquad\\qquad=-\\frac{\\mathrm{e}^{2\\tau}x}{\\eta^{2}+\\mathrm{e}^{2\\tau}-1}+\\frac{\\mathrm{e}^{\\tau-\\tilde{\\tau}}\\eta^{2}}{\\eta^{2}+\\mathrm{e}^{2\\tau}-1}s\\left(\\tilde{\\tau},\\,\\mathrm{e}^{-\\tilde{\\tau}}x_{\\mathrm{noisy}}+\\frac{\\mathrm{e}^{\\tau-\\tilde{\\tau}}\\eta^{2}x}{\\eta^{2}+\\mathrm{e}^{2\\tau}-1}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where we used the definition $s(\\tilde{\\tau},\\cdot)=\\nabla\\log p_{X_{\\tilde{\\tau}}}(\\cdot)$ . ", "page_idx": 19}, {"type": "text", "text": "E Discretization via the exponential integrator E.1 General form of the exponential integrator ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Consider a SDE of the form: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{d}M_{\\tau}=\\bigl(v(\\tau)M_{\\tau}+f(\\tau,M_{\\tau})\\bigr)\\mathrm{d}\\tau+\\sqrt{\\beta}\\mathrm{d}B_{\\tau},\\quad\\tau\\in[0,\\tau_{\\infty}],\\quad M_{0}\\sim p_{M_{0}},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $v:[0,\\tau_{\\infty}]\\to\\mathbb{R}$ , $f:[0,\\tau_{\\infty}]\\times\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d}$ are deterministic functions, and $\\beta>0$ is a constant. Given discretization time points $0=\\tau_{0}\\leq\\tau_{1}\\leq\\cdot\\cdot\\leq\\tau_{k}\\leq\\tau_{\\infty}$ , a na\u00efve way to discretize the SDE is ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{M_{\\tau_{i+1}}-M_{\\tau_{i}}\\approx\\left(v(\\tau_{i})M_{\\tau_{i}}+f(\\tau_{i},M_{\\tau_{i}})\\right)\\left(\\tau_{i+1}-\\tau_{i}\\right)+\\sqrt{\\beta}\\sqrt{\\tau_{i+1}-\\tau_{i}}\\varepsilon_{i},\\quad i=0,1,\\cdots,k-1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\varepsilon_{i}\\sim\\mathcal{N}(0,I_{d})$ is a standard $d$ -dimensional Gaussian random vector which is independent of $M_{\\tau_{i}}$ . Although this approach is straightforward, it has the drawback that the linear term $v(\\tau)M_{\\tau}$ is discretized rather crude. For example, for the OU process where $v\\equiv-1$ , $f\\equiv0$ , $\\beta=2$ , the SDE can be solved analytically as in (2), while the above approach still has a discretization error. ", "page_idx": 19}, {"type": "text", "text": "A more accurate discretization, known to significantly improve the quality of score-based generative models, is given by the exponential integrator [ZC22], which preserves the linear term and discretizes the SDE to ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{d}\\widehat{M}_{\\tau}=\\big(v(\\tau)\\widehat{M}_{\\tau}+f(\\tau_{i},\\widehat{M}_{\\tau_{i}})\\big)\\mathrm{d}\\tau+\\sqrt{\\beta}\\mathrm{d}B_{\\tau},\\quad\\tau\\in[\\tau_{i},\\tau_{i+1}],\\quad i=0,1,\\cdots,k,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with initialization $\\widehat{M_{0}}\\sim p_{M_{0}}$ . On each time interval $[\\tau_{i},\\tau_{i+1}]$ , this is simply a linear SDE, which can be explicitly s olved by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\widehat{M}_{\\tau}\\stackrel{\\mathrm{(d)}}{=}\\mathrm{e}^{V(\\tau)-V(\\tau_{i})}\\widehat{M}_{\\tau_{i}}+\\left(\\int_{\\tau_{i}}^{\\tau}\\mathrm{e}^{V(\\tau)-V(\\tilde{\\tau})}\\mathrm{d}\\tilde{\\tau}\\right)f(\\tau_{i},\\widehat{M}_{\\tau_{i}})+\\sqrt{\\beta}\\left(\\int_{\\tau_{i}}^{\\tau}\\mathrm{e}^{2(V(\\tau)-V(\\tilde{\\tau}))}\\mathrm{d}\\tilde{\\tau}\\right)^{1/2}\\varepsilon_{i},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $V$ is the antiderivative of $\\boldsymbol{v}$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\nV(\\tau)=\\int_{0}^{\\tau}v(\\tilde{\\tau})\\mathrm{d}\\tilde{\\tau}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Taking $\\tau=\\tau_{i+1}$ , we obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{M}_{\\tau_{i+1}}\\overset{\\mathrm{(d)}}{=}\\mathrm{e}^{V(\\tau_{i+1})-V(\\tau_{i})}\\widehat{M}_{\\tau_{i}}+\\left(\\int_{\\tau_{i}}^{\\tau_{i+1}}\\mathrm{e}^{V(\\tau_{i+1})-V(\\tilde{\\tau})}\\mathrm{d}\\tilde{\\tau}\\right)f(\\tau_{i},\\widehat{M}_{\\tau_{i}})}\\\\ &{\\qquad\\qquad+\\;\\sqrt{\\beta}\\mathrm{e}^{V(\\tau_{i+1})}\\left(\\displaystyle\\int_{\\tau_{i}}^{\\tau_{i+1}}\\mathrm{e}^{2(V(\\tau_{i+1})-V(\\tilde{\\tau}))}\\mathrm{d}\\tilde{\\tau}\\right)^{1/2}\\varepsilon_{i},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which provides an iterative formula to compute $\\widehat{M}_{\\tau_{i+1}}$ ", "page_idx": 20}, {"type": "text", "text": "E.2 Discretization of DDS-DDPM ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Plug the expression of $\\nabla\\log{p_{Y_{\\tau}}}$ in Lemma 1 into (16), and use the notation $\\tau^{\\mathsf{r e v}}=\\eta^{2}-\\tau$ , we obtain, for $\\bar{\\tau}\\in[0,\\bar{\\eta}^{2}]$ , that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{d}Y_{\\tau^{\\mathrm{rev}}}^{\\mathrm{rev}}=\\frac{1}{\\sqrt{1+\\tau^{\\mathrm{rev}}}}s\\bigg(\\frac{1}{2}\\log(1+\\tau^{\\mathrm{rev}}),\\,\\frac{Y_{\\tau^{\\mathrm{rev}}}^{\\mathrm{rev}}}{\\sqrt{1+\\tau^{\\mathrm{rev}}}}\\bigg)\\,\\mathrm{d}\\tau+\\mathrm{d}\\tilde{B}_{\\tau}}\\\\ &{\\quad\\quad=-\\frac{1}{\\sqrt{\\tau^{\\mathrm{rev}}}}\\varepsilon^{\\mathrm{cont}}\\bigg(\\frac{1}{2}\\log(1+\\tau^{\\mathrm{rev}}),\\,\\frac{Y_{\\tau^{\\mathrm{rev}}}^{\\mathrm{rev}}}{\\sqrt{1+\\tau^{\\mathrm{rev}}}}\\bigg)\\,\\mathrm{d}\\tau+\\mathrm{d}\\tilde{B}_{\\tau}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Choosing discretization time points. To discretize this SDE, we first choose the discretization time points. Recalling (??), it is most reasonable to discretize at those time points $0\\,\\leq\\,\\tau_{0}^{\\mathsf{r e v}}\\,\\leq\\,\\cdots\\,\\leq$ $\\bar{\\tau}_{T^{\\prime}}^{\\mathsf{r e v}}\\leq\\eta^{2}$ which satisfy ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\log(1+\\tau_{t}^{\\mathsf{r e v}})=\\frac{1}{2}\\log\\frac{1}{\\bar{\\alpha}_{t}},\\quad0\\leq t\\leq T^{\\prime}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This solves to ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\tau_{t}^{\\mathsf{r e v}}=\\bar{\\alpha}_{t}^{-1}-1.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The requirement that $\\tau_{t}^{\\mathsf{r e v}}\\leq\\eta^{2}$ translates to $\\begin{array}{r}{\\bar{\\alpha}_{t}\\geq\\frac{1}{1+\\eta^{2}}}\\end{array}$ , which yields the following choice of $T^{\\prime}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\nT^{\\prime}:=\\operatorname*{max}\\left\\{t:0\\leq t\\leq T,\\,\\bar{\\alpha}_{t}>\\frac{1}{\\eta^{2}+1}\\right\\}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Applying the exponential integrator. Now we apply the exponential integrator to discretize the SDE on each time interval $\\tau^{\\mathsf{r e v}}\\in[\\tau_{t-1},\\tau_{t}]$ , $t=1,\\cdot\\cdot\\cdot,T^{\\prime}$ as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{d}\\widehat{Y}_{\\tau^{\\mathrm{ev}}}^{\\mathrm{rev}}=-\\frac{1}{\\sqrt{\\tau^{\\mathrm{rev}}}}\\varepsilon^{\\mathrm{cont}}\\left(\\frac{1}{2}\\log(1+\\tau_{t}^{\\mathrm{rev}}),\\,\\frac{\\widehat{Y}_{\\tau_{t}^{\\mathrm{rev}}}^{\\mathrm{rev}}}{\\sqrt{1+\\tau_{t}^{\\mathrm{rev}}}}\\right)\\mathrm{d}\\tau+\\mathrm{d}\\tilde{B}_{\\tau},}\\\\ &{\\quad\\quad=-\\frac{1}{\\sqrt{\\tau^{\\mathrm{rev}}}}\\varepsilon_{t}^{\\star}\\left(\\frac{\\widehat{Y}_{\\tau_{t}^{\\mathrm{rev}}}^{\\mathrm{rev}}}{\\sqrt{1+\\tau_{t}^{\\mathrm{rev}}}}\\right)\\mathrm{d}\\tau+\\mathrm{d}\\tilde{B}_{\\tau}}\\\\ &{\\quad\\quad=-\\frac{1}{\\sqrt{\\tau^{\\mathrm{rev}}}}\\varepsilon_{t}^{\\star}\\left(\\sqrt{\\bar{\\alpha}_{t}}\\widehat{Y}_{\\tau_{t}^{\\mathrm{rev}}}^{\\mathrm{rev}}\\right)\\mathrm{d}\\tau+\\mathrm{d}\\tilde{B}_{\\tau}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The SDE can be integrated directly on $\\tau^{\\mathsf{r e v}}\\in[\\tau_{t-1},\\tau_{t}]$ (see also (34), with $v\\equiv0$ ), yielding ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{Y}_{\\tau_{t-1}^{\\mathrm{ev}}}^{\\mathrm{rev}}=\\widehat{Y}_{\\tau_{t}^{\\mathrm{rev}}}^{\\mathrm{rev}}-2\\big(\\sqrt{\\tau_{t}^{\\mathrm{rev}}}-\\sqrt{\\tau_{t-1}^{\\mathrm{rev}}}\\big)\\cdot\\varepsilon_{t}^{\\star}\\Big(\\sqrt{\\bar{\\alpha}_{t}}\\widehat{Y}_{\\tau_{t}^{\\mathrm{rev}}}^{\\mathrm{rev}}\\Big)+\\int_{\\eta^{2}-\\tau_{t}}^{\\eta^{2}-\\tau_{t-1}}d\\tilde{B}_{\\tau}\\mathrm{d}\\tau}\\\\ {\\overset{()}{=}\\widehat{Y}_{\\tau_{t}^{\\mathrm{rev}}}^{\\mathrm{rev}}-2\\big(\\sqrt{\\tau_{t}^{\\mathrm{rev}}}-\\sqrt{\\tau_{t-1}^{\\mathrm{rev}}}\\big)\\cdot\\varepsilon_{t}^{\\star}\\Big(\\sqrt{\\bar{\\alpha}_{t}}\\widehat{Y}_{\\tau_{t}^{\\mathrm{rev}}}^{\\mathrm{rev}}\\Big)+\\sqrt{\\tau_{t}^{\\mathrm{rev}}-\\tau_{t-1}^{\\mathrm{rev}}}w_{t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $w_{t}\\sim\\mathcal{N}(0,I_{d})$ is independent of $\\widehat{Y}_{\\tau_{t}^{\\mathrm{rev}}}^{\\mathrm{rev}}$ . Set $\\widehat{x}_{t}=\\widehat{Y}_{\\tau_{t}^{\\mathrm{rev}}}^{\\mathrm{rev}}$ we obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\widehat{x}_{t-1}\\overset{\\mathrm{(d)}}{=}\\widehat{x}_{t}-2(\\sqrt{\\tau_{t}}-\\sqrt{\\tau_{t-1}})\\cdot\\varepsilon_{t}^{\\star}\\big(\\sqrt{\\bar{\\alpha}_{t}}\\widehat{x}_{t}\\big)+\\sqrt{\\tau_{t}-\\tau_{t-1}}w_{t},\\quad w_{t}\\sim\\mathcal{N}(0,I_{d}),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which is exactly the update equation in Algorithm 2, except that $\\varepsilon_{t}^{\\star}$ is replaced by the noise estimate ${\\widehat{\\varepsilon}}_{t}$ . ", "page_idx": 20}, {"type": "text", "text": "E.3 Discretization of DDS-DDIM ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Plug in the expression of $s z$ in Lemma 2 into the probability flow ODE (20), we obtain ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{L}_{\\tau^{\\mathrm{rev}}}^{\\mathrm{rev}}=\\frac{\\eta^{2}-1}{\\eta^{2}+\\mathrm{e}^{2\\tau^{\\mathrm{rev}}}-1}Z_{\\tau^{\\mathrm{rev}}}^{\\mathrm{rev}}\\mathrm{d}\\tau+\\frac{\\mathrm{e}^{\\tau^{\\mathrm{rev}}}-\\bar{\\tau}\\left(\\tau^{\\mathrm{rev}}\\right)\\eta^{2}}{\\eta^{2}+\\mathrm{e}^{2\\tau^{\\mathrm{rev}}}-1}s\\left(\\tilde{\\tau}\\left(\\tau^{\\mathrm{rev}}\\right),\\,\\mathrm{e}^{-\\tilde{\\tau}\\left(\\tau^{\\mathrm{rev}}\\right)}x_{\\mathrm{noisy}}+\\frac{\\mathrm{e}^{\\tau^{\\mathrm{rev}}-\\tilde{\\tau}\\left(\\tau^{\\mathrm{rev}}\\right)}\\eta^{2}x}{\\eta^{2}+\\mathrm{e}^{2\\tau^{\\mathrm{rev}}}-1}\\right)\\mathrm{d}\\tau}\\\\ &{\\qquad=\\frac{\\eta^{2}-1}{\\eta^{2}+\\mathrm{e}^{2\\tau^{\\mathrm{rev}}}-1}Z_{\\tau^{\\mathrm{rev}}}^{\\mathrm{rev}}\\mathrm{d}\\tau-\\frac{\\mathrm{e}^{2\\tau^{\\mathrm{rev}}}}{\\mathrm{e}^{2\\tau^{\\mathrm{rev}}}-1}\\varepsilon^{\\mathrm{cont}}\\Big(\\tilde{\\tau}\\left(\\tau^{\\mathrm{rev}}\\right),\\,\\mathrm{e}^{-\\tilde{\\tau}\\left(\\tau^{\\mathrm{rev}}\\right)}x_{\\mathrm{noisy}}+\\frac{\\mathrm{e}^{\\tau^{\\mathrm{rev}}-\\tilde{\\tau}\\left(\\tau^{\\mathrm{rev}}\\right)}\\eta^{2}x}{\\eta^{2}+\\mathrm{e}^{2\\tau^{\\mathrm{rev}}}-1}\\Big)\\,\\mathrm{d}\\tau,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the second line used the definition (22). ", "page_idx": 21}, {"type": "text", "text": "Choosing discretization time points. Similar to the derivation in Appendix E.2, we discretize at time points $0=\\tau_{0}^{\\mathsf{r e v}}\\leq\\tau_{1}^{\\mathsf{r e v}}\\leq\\dot{\\cdots}\\leq\\tau_{T^{\\prime}}^{\\mathsf{r e v}}\\leq\\eta^{2}$ , which obey ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\tilde{\\tau}(\\tau_{t}^{\\mathsf{r e v}})=\\frac{1}{2}\\log\\frac{1}{\\bar{\\alpha}_{t}},\\quad t=0,1,\\dotsc,T^{\\prime},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which solves to ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\tau_{t}^{\\mathsf{r e v}}=\\frac{1}{2}\\log\\frac{\\eta^{2}+\\bar{\\alpha}_{t}-1}{(\\eta^{2}+1)\\bar{\\alpha}_{t}-1}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "To make this well-defined, we require ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{\\eta^{2}+\\bar{\\alpha}_{t}-1}{(\\eta^{2}+1)\\bar{\\alpha}_{t}-1}>0,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which is equivalent to ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\bar{\\alpha}_{t}>\\frac{1}{1+\\eta^{2}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This leads to the same choice of $T^{\\prime}$ as in (36). We also set ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\tau_{\\infty}=\\tau_{T^{\\prime}}^{\\mathrm{rev}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "It is convenient to introduce a notation for the corresponding discrete schedule of $\\tau_{t}^{\\mathsf{r e v}}$ , denoted by ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\bar{u}_{t}=\\mathrm{e}^{-2\\tau_{t}^{\\mathrm{rev}}}=\\frac{(\\eta^{2}+1)\\bar{\\alpha}_{t}-1}{\\eta^{2}+\\bar{\\alpha}_{t}-1},\\quad t=0,1,\\cdots,T^{\\prime}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Applying the exponential integrator. Now we apply the exponential integrator, which discretizes the ODE on each time interval $\\tau^{\\mathsf{r e v}}\\in[\\tau_{t-1},\\tau_{t}]$ , $t=1,\\cdot\\cdot\\cdot,T^{\\prime}$ , as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{L}_{\\tau^{\\mathrm{rev}}}^{\\mathrm{rev}}=\\frac{\\eta^{2}-1}{\\eta^{2}+\\mathrm{e}^{2\\tau^{\\mathrm{rev}}}-1}\\hat{Z}_{\\tau^{\\mathrm{rev}}}^{\\mathrm{rev}}\\mathrm{d}\\tau-\\frac{\\mathrm{e}^{2\\tau^{\\mathrm{rev}}}}{\\mathrm{e}^{2\\tau^{\\mathrm{rev}}}-1}\\varepsilon^{\\mathrm{cont}}\\Bigg(\\tilde{\\tau}(\\tau_{t}^{\\mathrm{rev}}),\\,\\mathrm{e}^{-\\tilde{\\tau}(\\tau_{t}^{\\mathrm{rev}})}x_{\\mathrm{noisy}}+\\frac{\\mathrm{e}^{\\tau_{t}^{\\mathrm{rev}}-\\tilde{\\tau}(\\tau_{t}^{\\mathrm{rev}})}\\eta^{2}\\widehat{Z}_{\\tau^{\\mathrm{rev}}}^{\\mathrm{rev}}}{\\eta^{2}+\\mathrm{e}^{2\\tau_{t}^{\\mathrm{rev}}}-1}\\Bigg)\\,\\mathrm{d}\\tau}\\\\ &{\\quad=\\frac{\\eta^{2}-1}{\\eta^{2}+\\mathrm{e}^{2\\tau^{\\mathrm{rev}}}-1}\\hat{Z}_{\\tau^{\\mathrm{rev}}}^{\\mathrm{rev}}\\mathrm{d}\\tau-\\frac{\\mathrm{e}^{2\\tau^{\\mathrm{rev}}}}{\\mathrm{e}^{2\\tau^{\\mathrm{rev}}}-1}\\varepsilon_{t}^{\\star}\\Bigg(\\sqrt{\\tilde{\\alpha}_{t}}x_{\\mathrm{noisy}}+\\frac{\\mathrm{e}^{\\tau_{t}^{\\mathrm{rev}}}\\sqrt{\\tilde{\\alpha}_{t}}\\eta^{2}\\widehat{Z}_{\\tau^{\\mathrm{rev}}}^{\\mathrm{rev}}}{\\eta^{2}+\\mathrm{e}^{2\\tau_{t}^{\\mathrm{rev}}}-1}\\Bigg)\\,\\mathrm{d}\\tau,}\\\\ &{\\quad=\\frac{\\eta^{2}-1}{\\eta^{2}+\\mathrm{e}^{2\\tau^{\\mathrm{rev}}}-1}\\hat{Z}_{\\tau^{\\mathrm{rev}}}^{\\mathrm{rev}}\\mathrm{d}\\tau-\\frac{\\mathrm{e}^{2\\tau^{\\mathrm{rev}}}}{\\mathrm{e}^{2\\tau^{\\mathrm{rev}}}-1}\\varepsilon_{t}^{\\star}\\Bigg(\\sqrt{\\tilde{\\alpha}_{t}}x_{\\mathrm{noisy}}+\\frac{\\sqrt{\\tilde{\\alpha}_{t}}\\sqrt{\\\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the second line follows from (38), and the last line follows from dividing both the denominator and the numerator in the fraction inside ${\\widehat{\\varepsilon}}_{t}$ by $\\mathrm{e}^{2\\tau_{t}^{\\mathrm{rev}}}$ . This is a first-order linear ODE on $\\tau^{\\mathsf{r e v}}\\in$ $\\left[{{\\tau}_{t-1}},{{\\tau}_{t}}\\right]$ , which can be solved explicitly ( c f. (34)) by ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\mathsf{Z}}_{\\tau^{\\mathrm{ev}}}^{\\mathsf{r e v}}=\\frac{\\sqrt{(\\eta^{2}-1)\\mathrm{e}^{-2\\tau^{\\mathrm{ev}}}}+1}{\\sqrt{(\\eta^{2}-1)\\bar{u}_{t}+1}}\\hat{Z}_{\\tau_{t}^{\\mathrm{ev}}}^{\\mathsf{r e v}}}\\\\ &{\\qquad\\qquad+\\,\\sqrt{(\\eta^{2}-1)\\mathrm{e}^{-2\\tau^{\\mathrm{ev}}}+1}\\cdot\\left(h(\\eta,\\mathrm{e}^{-2\\tau^{\\mathrm{ev}}})-h(\\eta,\\bar{u}_{t})\\right)\\cdot\\varepsilon_{t}^{\\star}\\!\\left(\\sqrt{\\bar{\\alpha}_{t}}x_{\\mathrm{noisy}}+\\frac{\\sqrt{\\bar{u}_{t}}\\sqrt{\\bar{\\alpha}_{t}}\\eta^{2}\\widehat{Z}_{\\tau^{\\mathrm{ev}}}^{\\mathsf{r e v}}}{(\\eta^{2}-1)\\bar{u}_{t}+1}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for $\\tau^{\\mathsf{r e v}}\\in[\\tau_{t-1},\\tau_{t}]$ , where ", "page_idx": 21}, {"type": "equation", "text": "$$\nh(\\eta,u):=-\\arctan\\frac{\\eta}{\\sqrt{u^{-1}-1}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Plug in $\\tau^{\\mathsf{r e v}}=\\tau_{t-1}$ in the above solution, and set $z_{t}=\\widehat{Z}_{\\tau_{t}^{\\mathrm{rev}}}^{\\mathrm{rev}}$ , we obtain ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{z_{t-1}=\\displaystyle\\frac{\\sqrt{(\\eta^{2}-1)\\bar{u}_{t-1}+1}}{\\sqrt{(\\eta^{2}-1)\\bar{u}_{t}+1}}z_{t}\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad(40\\eta^{2})\\times(0.25\\eta^{2})\\times(0.64\\eta^{2})\\times(0.25\\eta^{2})}\\\\ {+\\displaystyle\\sqrt{(\\eta^{2}-1)\\bar{u}_{t-1}+1}\\cdot\\left(h(\\eta,\\bar{u}_{t-1})-h(\\eta,\\bar{u}_{t})\\right)\\cdot\\varepsilon_{t}^{\\star}\\left(\\sqrt{\\bar{\\alpha}_{t}}x_{\\mathrm{noisy}}+\\frac{\\sqrt{\\bar{u}_{t}}\\sqrt{\\bar{\\alpha}_{t}}\\eta^{2}z_{t}}{(\\eta^{2}-1)\\bar{u}_{t}+1}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The initialization, which should ideally be $z_{T_{\\cdot}^{\\prime}}=\\widehat{Z}_{\\tau_{\\infty}}^{\\mathsf{r e v}}\\sim p_{Z_{\\tau\\infty}}$ , is approximated by $z_{T^{\\prime}}\\sim\\mathcal{N}(0,I_{d})$ . This is exactly the update equation and the initiali zation in Algorithm 3, except that $\\varepsilon_{t}^{\\star}$ is replaced by the noise estimate ${\\widehat{\\varepsilon_{t}}}$ . ", "page_idx": 22}, {"type": "text", "text": "E.4 Discretization of PCS ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We first note that the Metropolis-adjustment step in PCS (cf. Algorithm 4) is standard following the classical form of MALA [RR98]. Therefore, we focus on explaining the Langevin step. Recall the continuous-time Langevin dynamics for sampling from the distribution $\\begin{array}{r l}{\\exp(\\mathcal{L}\\bar{(}\\cdot;y)-\\frac{\\mathbf{\\bar{1}}}{2\\eta^{2}}\\|\\cdot-x\\|^{2})}\\end{array}$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{d}Z_{\\tau}=-\\nabla_{Z_{\\tau}}\\mathcal{L}(Z_{\\tau};y)\\mathrm{d}\\tau+\\frac{1}{\\eta^{2}}(Z_{\\tau}-x)\\mathrm{d}\\tau+\\sqrt{2}\\mathrm{d}B_{\\tau},\\quad\\tau\\geq0,\\quad Z_{0}\\sim\\mathcal{N}(0,I_{d}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The classical form of MALA, as in [RR98], performs one step of a straightforward discretization of (41) as the Langevin step, as follows: ", "page_idx": 22}, {"type": "equation", "text": "$$\nz_{n+\\frac{1}{2}}\\approx z_{n}-\\gamma\\nabla_{z_{n}}\\mathcal{L}(z_{n};y)+\\frac{\\gamma}{\\eta^{2}}(z_{n}-x)+\\sqrt{2\\gamma}w_{n},\\quad w_{n}\\sim\\mathcal{N}(0,I_{d}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "In our setting, due to the presence of the linear drift term $\\begin{array}{r}{\\frac{1}{\\eta^{2}}(Z_{\\tau}-x)}\\end{array}$ , which can be quite large when $\\eta$ is small, we apply the exponential integrator instead. Set the discretization time points $\\tau_{n}=n\\gamma$ , the exponential integrator reads as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{d}Z_{\\tau}=-\\nabla_{Z_{n\\gamma}}\\mathcal{L}(Z_{n\\gamma};y)\\mathrm{d}\\tau+\\frac{1}{\\eta^{2}}(Z_{\\tau}-x)\\mathrm{d}\\tau+\\sqrt{2}\\mathrm{d}B_{\\tau},\\quad n\\gamma\\leq\\tau\\leq(n+1)\\gamma.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Solve this linear SDE on $n\\gamma\\leq\\tau\\leq(n+1)\\gamma$ directly (see also (34)) to obtain ", "page_idx": 22}, {"type": "equation", "text": "$$\nZ_{(n+1)\\gamma}\\stackrel{\\mathrm{(d)}}{=}r Z_{n\\gamma}+(1-r)x+\\eta^{2}(1-r)\\nabla_{Z_{n\\gamma}}\\mathcal{L}(Z_{n\\gamma};y)+\\eta\\sqrt{1-r^{2}}w_{n},\\quad w_{n}\\sim\\mathcal{N}(0,I_{d}),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $r:=\\mathrm{e}^{-\\gamma/\\eta^{2}}$ . This is the same as the update equation for the Langevin step in PCS (cf. Algorithm 4). ", "page_idx": 22}, {"type": "text", "text": "$\\mathbf{F}$ Proof of main theorems ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "F.1 Proof of Theorem 1 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Proof. We first collect the asymptotic correctness of our subroutines PCS and DDS in the following two lemmas. The correctness of PCS is actually well-known, see e.g., [Tie94, Corollary 2]. ", "page_idx": 22}, {"type": "text", "text": "Lemma 3 (Correctness of PCS). Under Assumption $^{\\,l}$ , with notation in Algorithm 4, in the continuous-time limit: $\\gamma\\,\\rightarrow\\,0$ , $N\\to\\infty$ , the algorithm PCS outputs samples with distribution $\\begin{array}{r}{\\propto\\exp(\\mathcal{L}(\\cdot;y)+\\frac{1}{2\\eta}\\|\\cdot-\\dot{x}\\|^{2})}\\end{array}$ . ", "page_idx": 22}, {"type": "text", "text": "The next lemma guarantees the correctness of DDS with exact unconditional score functions. ", "page_idx": 22}, {"type": "text", "text": "Lemma 4 (Correctness of DDS). Assume the score function estimation $\\widehat{s}_{t}$ is accurate, i.e. $\\widehat{s}_{t}=s_{t}^{\\star}$ . In the continuous-time limit: $T\\rightarrow\\infty,\\bar{\\alpha}_{T}\\rightarrow0$ , $\\frac{\\bar{\\alpha}_{t-1}}{\\bar{\\alpha}_{t}}\\rightarrow1$ , unifomly in $t_{:}$ ,  both DDS-DDIM   and DDSDDPM output x obeying the posterior distribution $p^{\\star}(x^{\\star}=x\\,|\\,x^{\\star}+\\eta\\varepsilon=x_{\\mathsf{n o i s y}})$ ), $\\varepsilon\\sim\\mathcal{N}(0,I_{d})$ . ", "page_idx": 22}, {"type": "text", "text": "The proof of Theorem 1 is based on two lemmas on the one-step transition kernel of $\\mathsf{D P n P}$ and the asymptotic behavior of the transition kernel, which we will present soon. First, we set up some notations. Denote ", "page_idx": 22}, {"type": "equation", "text": "$$\np_{\\eta}(x):=p_{x^{\\star}\\sim p^{\\star},\\varepsilon\\sim\\mathcal{N}(0,I_{d})}(x^{\\star}+\\eta\\varepsilon=x)=\\frac{1}{(2\\pi)^{d/2}\\eta^{d}}\\int p^{\\star}(z)\\mathrm{e}^{-\\frac{1}{2\\eta^{2}}\\|x-z\\|^{2}}\\mathrm{d}z.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "From the first equality, it is clear that $p_{\\eta}\\to p^{\\star}$ when $\\eta\\to0^{+}$ . We will also use the notation $q_{\\eta}$ defined in (24), which we recall here: ", "page_idx": 23}, {"type": "equation", "text": "$$\nq_{\\eta}(x):=\\frac{1}{(2\\pi)^{d/2}\\eta^{d}}\\int\\mathrm{e}^{\\mathcal{L}(z;y)-\\frac{1}{2\\eta^{2}}\\|x-z\\|^{2}}\\mathrm{d}z.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In virtue of the Assumption 1, we know that $q_{\\eta}$ is finite for all $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ . ", "page_idx": 23}, {"type": "text", "text": "For convenience, we introduce a notation for application of transition kernels. For a probability distribution $p(x)$ and a probability transition kernel $K(x,x^{\\prime})$ , denote by $p\\circ K$ the probability distribution given by ", "page_idx": 23}, {"type": "equation", "text": "$$\np\\circ K(x^{\\prime})=\\int p(x)K(x,x^{\\prime})\\mathrm{d}x.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The first lemma characterizes the one-step behavior of DPnP in terms of Markov transition kernels. ", "page_idx": 23}, {"type": "text", "text": "Lemma 5. Under the settings of Lemma 4 and Lemma 3, the one-step transition kernel of DPnP with $\\eta_{k}=\\eta$ is given by: ", "page_idx": 23}, {"type": "equation", "text": "$$\nK_{\\mathsf{D P n P},\\eta}(x,x^{\\prime})=\\left(\\int\\frac{q_{0}(z)}{p_{\\eta}(z)}\\mathrm{e}^{-\\frac{1}{2\\eta^{2}}\\|z-x\\|^{2}-\\frac{1}{2\\eta^{2}}\\|z-x^{\\prime}\\|^{2}}\\mathrm{d}z\\right)\\frac{p^{\\star}(x^{\\prime})}{q_{\\eta}(x)}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In other words, $i f\\widehat{x}_{k}$ has distribution $p_{{\\widehat x}_{k}}$ , then the distribution of $\\widehat{x}_{k+1}$ is ", "page_idx": 23}, {"type": "equation", "text": "$$\np_{\\widehat{x}_{k+1}}(x^{\\prime})=p_{\\widehat{x}_{k}}\\circ K_{{\\mathsf{D P n P}},\\eta}(x)=\\int p_{\\widehat{x}_{k}}(x)K_{{\\mathsf{D P n P}},\\eta}(x,x^{\\prime})\\mathrm{d}x.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The proof is postponed to Appendix F.3. The next lemma analyzes the ergodic properties of the Markov chain with transition kernel $K_{\\mathsf{D P n P},\\eta}$ . These properties are known [BB23] but scattered in different literatures, so we will provide a brief proof to be self-contained. ", "page_idx": 23}, {"type": "text", "text": "Lemma 6. The Markov transition kernel $K_{\\mathsf{D P n P},\\eta}$ has the following properties: ", "page_idx": 23}, {"type": "text", "text": "(i) (Stationary distribution.) Let $\\pi_{\\eta}$ be the probability distribution defined by ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\pi_{\\eta}(x)=c_{\\eta}p^{\\star}(x)q_{\\eta}(x),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $c_{\\eta}\\,>\\,0$ is the normalization constant such that $\\textstyle\\int\\pi_{\\eta}(x)\\mathrm{d}x=1$ . Then $K_{\\mathsf{D P n P},\\eta}$ is reversible with stationary distribution \u03c0\u03b7. ", "page_idx": 23}, {"type": "text", "text": "(ii) (Convergence.) For any initial distribution $p_{\\mathrm{:}}$ , the distribution of the Markov chain with kernel $K_{\\mathsf{D P n P},\\eta}$ converges to $\\pi_{\\eta}$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathsf{T V}(p\\circ K_{\\mathsf{D P n P},\\eta}^{(n)},\\,\\pi_{\\eta})\\rightarrow0,\\quad n\\rightarrow\\infty,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "$K_{\\mathsf{D P n P},\\eta}^{(n)}$ $n$ $K_{\\mathsf{D P n P},\\eta}.$ ", "page_idx": 23}, {"type": "text", "text": "The proof is postponed to Appendix F.4. We now show how to prove Theorem 1 with the above two lemmas. With the annealing schedule in Theorem 1, between steps $k_{l-1}\\,\\leq\\,k\\,<\\,k_{l}$ , which consist of consecutive $(k_{l}-k_{l-1})$ steps, the transition kernel of one-step of DPnP is $K_{\\mathsf{D P n P},\\varepsilon_{l}}$ . As $(k_{l}-k_{l-1})\\to\\infty$ , Lemma 6 implies that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathsf{T V}(p_{\\widehat{x}_{k_{l}}},\\,\\pi_{\\varepsilon_{l}})=\\mathsf{T V}(p_{\\widehat{x}_{k_{l-1}}}\\circ K_{\\mathsf{D P n P},\\varepsilon_{l}}^{(k_{l}-k_{l-1})},\\,\\pi_{\\varepsilon_{l}})\\to0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Under the assumption in Theorem 1 that $\\varepsilon_{l}\\rightarrow0$ , we let $l\\rightarrow\\infty$ to see $\\begin{array}{r}{\\operatorname*{lim}_{l\\to\\infty}\\pi_{\\varepsilon_{l}}=c_{0}p^{\\star}(\\cdot)\\mathrm{e}^{\\mathcal{L}(\\cdot;y)}=}\\end{array}$ $p^{\\star}(\\cdot|y)$ , thus $p_{\\widehat{x}_{k_{l}}}\\to p^{\\star}(\\cdot|y)$ , as claimed. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "F.2 Proof of Lemma 4 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Proof. For DDS-DDPM, we note that under the continuous-time limit in Lemma 4, the discretization time points given by (35) verify ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\tau_{0}^{\\mathrm{rev}}=0,\\quad\\operatorname*{sup}_{0\\leq t\\leq T^{\\prime}-1}|\\tau_{t}^{\\mathrm{rev}}-\\tau_{t+1}^{\\mathrm{rev}}|\\rightarrow0,\\quad\\tau_{T^{\\prime}}^{\\mathrm{rev}}\\rightarrow\\left(\\frac{1}{1+\\eta^{2}}\\right)^{-1}-1=\\eta^{2},\\quad T^{\\prime}\\rightarrow\\infty.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, these discretization time points $0=\\tau_{0}^{\\mathsf{r e v}}\\leq\\cdots\\leq\\tau_{T^{\\prime}}^{\\mathsf{r e v}}\\leq\\eta^{2}$ form a partition of $[0,\\eta^{2}]$ , which becomes infinitely fine in the continuous-time limit. Thus the discretized integrator (37) converges to the solution of the SDE (16), which, as we have already argued in Appendix $\\mathrm{D}$ , produces samples obeying the denoising posterior distribution $p^{\\star}(\\cdot|x_{\\mathsf{n o i s y}})$ , as claimed. ", "page_idx": 24}, {"type": "text", "text": "The proof for DDS-DDPM follows similarly, by observing that the discretization time points in (35) form an infinitely fine partition of $[0,\\infty)$ in the continuous-time limit. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "F.3 Proof of Lemma 5 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Proof. The proof is based on computing the transition kernel of the two subroutines. We claim that ", "page_idx": 24}, {"type": "text", "text": "(i) Sampling with probability density proportional to $\\begin{array}{r}{\\exp(\\mathcal{L}(\\cdot;y)-\\frac{1}{2\\eta^{2}}\\|\\cdot-x\\|^{2})}\\end{array}$ is equivalent to applying the following Markov transition kernel ", "page_idx": 24}, {"type": "equation", "text": "$$\nK_{\\mathsf{P C S},\\eta}(x,x^{\\prime})=\\frac{1}{q_{\\eta}(x)}\\mathrm{e}^{\\mathcal{L}(x^{\\prime};y)-\\frac{1}{2\\eta^{2}}\\|x^{\\prime}-x\\|^{2}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "(ii) Sampling with probability $p^{\\star}(x^{\\star}\\,|\\,x^{\\star}+\\eta\\varepsilon\\,=\\,x)$ , where $\\varepsilon\\,\\sim\\mathcal{N}(0,I_{d})$ , is equivalent to applying the following Markov transition kernel: ", "page_idx": 24}, {"type": "equation", "text": "$$\nK_{008,\\eta}(x,x^{\\prime})=\\frac{1}{p_{\\eta}(x)}p^{\\star}(x^{\\prime})\\mathrm{e}^{-\\frac{1}{2\\eta^{2}}\\|x^{\\prime}-x\\|^{2}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "It is then clear that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\overbracket{\\mathrm{V}_{\\mathsf{D}\\mathsf{P}\\mathsf{n},\\eta}}_{\\nabla\\Pi\\mathsf{P},\\eta}(x,x^{\\prime})=\\int K_{\\mathsf{P C S},\\eta}(x,z)K_{\\mathsf{D}\\mathsf{D}\\mathsf{S},\\eta}(z,x^{\\prime})\\mathrm{d}z=\\left(\\int\\frac{q_{0}(z)}{p_{\\eta}(z)}\\mathrm{e}^{-\\frac{1}{2\\eta^{2}}\\|z-x\\|^{2}-\\frac{1}{2\\eta^{2}}\\|z-x^{\\prime}\\|^{2}}\\mathrm{d}z\\right)\\frac{p^{\\star}(x^{\\prime})}{q_{\\eta}(x)}\\mathrm{d}x^{\\prime},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "as desired. We now prove the above two claims. For (i), note that by (23), we know $K_{\\mathsf{D D S},\\eta}(x,\\cdot)\\propto$ $p^{\\star}(\\cdot)\\mathrm{e}^{-\\frac{1}{2\\eta^{2}}\\|\\cdot-x\\|^{2}}$ . Thus it suffices to compute the normalization constant, which is ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\int p^{\\star}(x^{\\prime})\\mathrm{e}^{-\\frac{1}{2\\eta^{2}}\\|x^{\\prime}-x\\|^{2}}\\mathrm{d}x^{\\prime}=p_{\\eta}(x),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "by the definition of $p_{\\eta}$ . Therefore ", "page_idx": 24}, {"type": "equation", "text": "$$\nK_{008,\\eta}(x,x^{\\prime})=\\frac{1}{p_{\\eta}(x)}p^{\\star}(x^{\\prime})\\mathrm{e}^{-\\frac{1}{2\\eta^{2}}\\|x^{\\prime}-x\\|^{2}},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "as claimed. The proof of (ii) follows similarly. ", "page_idx": 24}, {"type": "text", "text": "F.4 Proof of Lemma 6 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Proof. We first introduce a fundamental lemma [PW24], which provides a simple method to bound the total variation between two distributions. ", "page_idx": 24}, {"type": "text", "text": "Lemma 7 (Data-processing inequality). Let $p,q$ be two probability distributions, and $K$ be $a$ probability transition kernel. Then ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathsf{T V}(p\\circ K,q\\circ K)\\leq\\mathsf{T V}(p,q).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We now prove the two items in Lemma 6 separately. ", "page_idx": 24}, {"type": "text", "text": "Proof of (i). We first show that $\\pi_{\\eta}$ is well-defined, i.e., $\\begin{array}{r}{\\int p^{\\star}(x)q_{n}\\eta(x)\\mathrm{d}x<\\infty}\\end{array}$ . This can be seen from Assumption 1, which implies $\\begin{array}{r}{q_{\\eta}(x)\\lesssim\\int\\mathrm{e}^{-\\frac{1}{2\\eta^{2}}\\|x-z\\|^{2}}\\mathrm{d}z\\lesssim1}\\end{array}$ , hence ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\int p^{\\star}(x)q_{\\eta}(x)\\mathrm{d}x\\lesssim\\int p^{\\star}(x)\\mathrm{d}x=1.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "To show that $K_{\\mathsf{D P n P},\\eta}$ is reversible with stationary distribution $\\pi_{\\eta}$ , it suffices to verify ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pi_{\\eta}(x)K_{\\mathsf{D P n P},\\eta}(x,x^{\\prime})=\\pi_{\\eta}(x^{\\prime})K_{\\mathsf{D P n P},\\eta}(x^{\\prime},x),\\quad\\forall x,x^{\\prime}\\in\\mathbb{R}^{d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "However, it is easily checked that both sides are equal to ", "page_idx": 25}, {"type": "equation", "text": "$$\nc_{\\eta}\\left(\\int\\frac{q_{0}(z)}{p_{\\eta}(z)}\\mathrm{e}^{-\\frac{1}{2\\eta^{2}}\\|z-x\\|^{2}-\\frac{1}{2\\eta^{2}}\\|z-x^{\\prime}\\|^{2}}\\mathrm{d}z\\right)p^{\\star}(x^{\\prime})p^{\\star}(x).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof of $(i i)$ . We define an auxiliary Markov transition kernel $K_{\\mathsf{a u x},\\eta}=K_{\\mathsf{D D S},\\eta}\\circ K_{\\mathsf{P C S},\\eta}$ . More explicitly, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\tilde{\\mathrm{\\boldmath~\\psi~}}_{\\mathrm{aux},\\eta}(x,x^{\\prime})=\\int K_{\\sf D D S},\\eta(x,z)K_{\\sf P C S,\\eta}(z,x^{\\prime})\\mathrm{d}z=\\left(\\int\\frac{p^{\\star}(z)}{q_{\\eta}(z)}\\mathrm{e}^{-\\frac{1}{2\\eta^{2}}\\|z-x\\|^{2}-\\frac{1}{2\\eta^{2}}\\|z-x^{\\prime}\\|^{2}}\\mathrm{d}z\\right)\\frac{\\mathrm{e}^{\\mathcal{L}(x^{\\prime};y)}}{p_{\\eta}(x)}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "It is easy to see that ", "page_idx": 25}, {"type": "equation", "text": "$$\np\\circ K_{\\mathsf{D P n P},\\eta}^{(n)}=p\\circ K_{\\mathsf{P C S},\\eta}\\circ K_{\\mathsf{a u x},\\eta}^{(n-1)}\\circ K_{\\mathsf{D D S},\\eta}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Thus we are led to investigate the ergodic properties of $K_{\\mathsf{a u x},\\eta}$ . Similar to the proof of item (i) above, it is not hard to show that $K_{\\mathsf{a u x},\\eta}$ is reversible with respect to the stationary distribution ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mu_{\\eta}(x):=c_{\\eta}p_{\\eta}(x)q_{0}(x)=c_{\\eta}p_{\\eta}(x)\\mathrm{e}^{\\mathcal{L}(x;y)}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Moreover, one may check that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\pi_{\\eta}=\\mu_{\\eta}\\circ K_{00\\mathsf{S},\\eta}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "It is apparent that $\\mu(x^{\\prime})>0$ and $K_{\\mathsf{a u x},\\eta}(x,x^{\\prime})/\\mu_{\\eta}(x^{\\prime})>0$ for all $\\boldsymbol{x},\\boldsymbol{x^{\\prime}}\\in\\mathbb{R}^{d}$ . By [Tie94, Corollary 1], such a Markov transition kernel obeys, for any probability distribution $q$ , that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathsf{T V}(q\\circ K_{\\mathsf{a u x},\\eta}^{(n)},\\;\\mu_{\\eta})\\to0,\\quad n\\to\\infty.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "In view of (44) and (45), we set $q=p\\circ K_{\\mathsf{P C S},\\eta}$ and invoke the data-processing inequality to obtain ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{T V}(p\\circ K_{\\mathsf{D P n P},\\eta}^{(n)},\\,\\pi_{\\eta})=\\mathsf{T V}(q\\circ K_{\\mathsf{a u x},\\eta}^{(n-1)}\\circ K_{\\mathsf{D O S},\\eta},\\,\\mu_{\\eta}\\circ K_{\\mathsf{D O S},\\eta})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\mathsf{T V}(q\\circ K_{\\mathsf{a u x},\\eta}^{(n-1)},\\,\\mu_{\\eta})}\\\\ &{\\qquad\\qquad\\qquad\\to0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "as $n\\to\\infty$ . This completes the proof. ", "page_idx": 25}, {"type": "text", "text": "F.5 Proof of Theorem 2 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Proof. Denote by $\\tilde{K}_{\\mathsf{P C S},\\eta}$ and $\\tilde{K}_{\\mathsf{D D S},\\eta}$ and the transition kernels for PCS and for DDS, respectively. Note that these may deviate from the transition kernels $K_{\\mathsf{P C S},\\eta}$ and $K_{\\mathsf{D D S},\\eta}$ defined for the idealized asymptotic setting in Appendix F. We have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{T V}(p_{\\widehat{x}_{N}},\\pi_{\\eta})=\\mathsf{T V}(p_{\\widehat{x}_{N-\\frac{1}{2}}}\\circ\\tilde{K}_{\\mathsf{D D S},\\eta},\\,\\pi_{\\eta})}\\\\ &{\\qquad\\qquad\\leq\\mathsf{T V}(p_{\\widehat{x}_{N-\\frac{1}{2}}}\\circ K_{\\mathsf{D D S},\\eta},\\,\\pi_{\\eta})+\\mathsf{T V}(p_{\\widehat{x}_{N-\\frac{1}{2}}}\\circ K_{\\mathsf{D D S},\\eta},\\,p_{\\widehat{x}_{N-\\frac{1}{2}}}\\circ\\tilde{K}_{\\mathsf{D D S},\\eta})}\\\\ &{\\qquad\\qquad\\leq\\mathsf{T V}(p_{\\widehat{x}_{N-\\frac{1}{2}}}\\circ K_{\\mathsf{D D S},\\eta},\\,\\pi_{\\eta})+\\varepsilon_{\\mathsf{D D S}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the second line is triangle inequality, and the third line follows from the assumption in Theorem 2 that DDS has error at most $\\varepsilon_{\\mathsf{D D S}}$ in total variation, by taking the input of DDS to be $\\widehat{x}_{N-\\frac{1}{2}}$ ", "page_idx": 25}, {"type": "text", "text": "Similarly, from $p_{\\widehat{x}_{N-\\frac{1}{2}}}=p_{\\widehat{x}_{N-1}}\\circ\\tilde{K}_{\\mathsf{P C S},\\eta}$ and the assumption that PCS has error at most $\\varepsilon_{\\mathsf{P C S}}$ in total variation, we can show ", "page_idx": 25}, {"type": "text", "text": "$\\begin{array}{r}{\\Gamma\\mathsf{V}(p_{\\tilde{x}_{N-\\frac{1}{\\hbar}}}\\mathrm{e}_{\\mathsf{E}\\mathsf{D}\\mathsf{D}\\mathsf{S},\\eta},\\,\\pi_{\\eta})\\le\\mathsf{T V}(p_{\\tilde{x}_{N-1}}\\mathrm{e}_{\\mathsf{E}\\mathsf{P}\\mathsf{C}\\mathsf{S},\\eta}\\circ K_{\\mathsf{D}\\mathsf{D}\\mathsf{S},\\eta},\\,\\pi_{\\eta})\\!+\\!\\varepsilon_{\\mathsf{P C S}}=\\mathsf{T V}(p_{\\tilde{x}_{N-1}}\\mathrm{e}_{\\mathsf{E}\\mathsf{D}\\mathsf{P}\\mathsf{n}\\mathsf{P},\\eta},\\,\\pi_{\\eta})\\!+\\!\\varepsilon_{\\mathsf{P C S}}\\,.}\\end{array}$ S. ", "page_idx": 25}, {"type": "text", "text": "The above two inequalities together imply ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathsf{T V}(p_{\\widehat{x}_{N}},\\pi_{\\eta})\\leq\\mathsf{T V}(p_{\\widehat{x}_{N-1}}\\circ K_{\\mathsf{D P n P},\\eta},\\ \\pi_{\\eta})+\\varepsilon_{\\mathsf{D D S}}+\\varepsilon_{\\mathsf{P C S}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Iterating this process, we obtain ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathsf{T V}(p_{\\widehat{x}_{N}},\\pi_{\\eta})\\leq\\mathsf{T V}(p_{\\widehat{x}_{1}}\\circ K_{\\mathsf{D P n P},\\eta}^{(N-1)},\\,\\pi_{\\eta})+(N-1)(\\varepsilon_{\\mathsf{D D S}}+\\varepsilon_{\\mathsf{P C S}}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "It remains to bound $\\mathsf{T V}(p_{\\widehat{x}_{1}}\\circ K_{\\mathsf{D P n P},\\eta}^{(N-1)},\\;\\pi_{\\eta})$ . For this, we need the following two lemmas. ", "page_idx": 25}, {"type": "text", "text": "Lemma 8 (Comparing TV and $\\chi^{2}$ -divergence, [PW24]). For any two distributions $p,q$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathsf{T V}(p,q)\\leq\\sqrt{\\chi^{2}(p\\parallel q)}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Lemma 9 $\\chi^{2}$ -contractivity of $K_{\\mathsf{D P n P},\\eta},$ ). There exists some $\\lambda:=\\lambda(p^{\\star},\\mathcal{L},\\eta)\\in(0,1),$ , such that for any probability distribution $p(x)$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\chi^{2}(p\\circ K_{\\mathsf{D P n P},\\eta}^{(N)}\\parallel\\pi_{\\eta})\\leq\\lambda^{2N}\\chi^{2}(p\\parallel\\pi_{\\eta}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "A form of Lemma 9 is well-known for Markov chains with countable state spaces, but relatively few sources provide a complete proof for the abstract setting we consider here with continuous state space. For sake of completeness, we prove Lemma 9 in Appendix F.6. ", "page_idx": 26}, {"type": "text", "text": "Combining the above two lemmas, we obtain ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathsf{T V}(p_{\\widehat{x}_{1}}\\circ K_{\\mathsf{D P n P},\\eta}^{(N-1)},\\,\\pi_{\\eta})\\leq\\sqrt{\\chi^{2}(p_{\\widehat{x}_{1}}\\circ K_{\\mathsf{D P n P},\\eta}^{(N-1)}\\parallel\\pi_{\\eta})}\\leq\\lambda^{N-1}\\sqrt{\\chi^{2}(p_{\\widehat{x}_{1}}\\parallel\\pi_{\\eta})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Plug this into (46), we obtain ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathsf{T V}(p_{\\widehat{x}_{N}},\\pi_{\\eta})\\leq\\lambda^{N-1}\\sqrt{\\chi^{2}(p_{\\widehat{x}_{1}}\\parallel\\pi_{\\eta})}+(N-1)(\\varepsilon_{\\mathsf{D D S}}+\\varepsilon_{\\mathsf{P C S}}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "With $\\begin{array}{r}{N\\asymp\\frac{\\log(1/\\varepsilon_{\\mathsf{a c c}})}{1-\\lambda}}\\end{array}$ such that $\\lambda^{N-1}\\le\\exp\\big(-(N-1)(1-\\lambda)\\big)\\le\\varepsilon_{\\mathsf{a c c}}$ , the desired result readily follows. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "F.6 Proof of Lemma 9 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Proof. We need a few fundamental properties of reversible Markov chains, which are collected below. First we set up some notations. Define the Hilbert space $L^{2}(\\pi)$ to be the space of square-integrable functions with respect to measure $\\pi$ , i.e., those functions $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{C}$ such that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\|f\\|_{L^{2}(\\pi)}:=\\left(\\int|f(x)|^{2}\\pi(x)\\mathrm{d}x\\right)^{1/2}<\\infty.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The first well-known property [SC97] offers a way to represent a reversible transition kernel as a self-adjoint operator (infinite-dimensional symmetric matrix). ", "page_idx": 26}, {"type": "text", "text": "Lemma 10 (Self-adjoint representation of reversible Markov operator). Assume $K(x,x^{\\prime})$ is a Markov transition kernel that is reversible with respect to the stationary distribution $\\pi(x)$ . Then the integral operator $K:L^{2}(\\pi)\\rightarrow L^{2}(\\pi)$ defined by ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathcal{K}f(x)=\\int K(x,x^{\\prime})f(x^{\\prime})\\mathrm{d}x^{\\prime}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "is self-adjoint and compact. For any probability distribution $p(x)$ such that $\\begin{array}{r}{\\int\\frac{p^{2}(x)}{\\pi(x)}\\mathrm{d}x<\\infty}\\end{array}$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\int p(x)\\cdot K f(x)\\mathrm{d}x=\\int p\\circ K(x^{\\prime})f(x^{\\prime})\\mathrm{d}x^{\\prime}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Moreover, the eigenvalues of $\\kappa$ are the same as those of $K$ . ", "page_idx": 26}, {"type": "text", "text": "The following theorem is a generalization of the classical Perron-Frobenius theory for finitedimensional transition matrix to strictly positive operators. The form we present here can be found in [Sch12, Theorem V.6.6]; see also [Bou23b, Theorem III.6.7] for a more elementary treatment which can also be adapted to the form we need. ", "page_idx": 26}, {"type": "text", "text": "Theorem 3 (Jentzsch). Let $K(x,x^{\\prime})$ be a Markov transition kernel. If $K(x,x^{\\prime})\\;>\\;0$ for any $x,x^{\\prime}\\in\\mathbb R^{d}$ , then $K$ has a unique stationary distribution $\\pi$ . Moreover, 1 is a simple eigenvalue of $K$ , with $\\pi$ being the only left eigenfunction, and the constant function 1 being the only right eigenfunction. In addition, there exists $\\lambda\\in(0,1)$ such that any other eigenvalue of $K$ has modulus no larger than $\\lambda$ . ", "page_idx": 26}, {"type": "text", "text": "We are now ready to prove Lemma 9. We divide the proof into the following steps. ", "page_idx": 27}, {"type": "text", "text": "Step 1: controlling the eigenvalues of $\\kappa_{\\mathsf{D P n P},\\eta}$ . Recall the auxiliary kernel $K_{\\mathsf{a u x},\\eta}$ defined in (43). It is a standard result in linear algebra or function analysis $[\\mathrm{Bou}23\\mathrm{a}]$ that $K_{\\mathsf{a u x},\\eta}=K_{\\mathsf{P C S},\\eta}\\circ$ $K_{\\mathsf{D D S},\\eta}$ has same eigenvalues as $K_{\\mathsf{D P n P},\\eta}\\,=\\,K_{\\mathsf{D D S},\\eta}\\,\\circ\\,K_{\\mathsf{P C S},\\eta}$ . From (43), it is easy to check $K_{\\mathsf{a u x},\\eta}\\dot{(}x,x^{\\prime})>0$ , thus Theorem 3 implies 1 is a simple eigenvalue of $\\kappa_{\\mathsf{D P n P},\\eta}$ . Moreover, there exists $\\dot{\\lambda}:=\\dot{\\lambda}(p^{\\star},\\mathcal{L},\\eta)\\in(0,1)$ , such that any other eigenvalue of $K_{\\mathsf{a u x},\\eta}$ has modulus no larger than $\\lambda$ . ", "page_idx": 27}, {"type": "text", "text": "Since $K_{\\mathsf{D P n P},\\eta}$ has the same eigenvalues as $K_{\\mathsf{a u x},\\eta}$ , and, by Lemma 10, the operator $\\kappa_{\\mathsf{D P n P},\\eta}$ also has the same eigenvalues as these two, we conclude that $\\kappa_{\\mathsf{D P n P},\\eta}$ is a self-adjoint compact operator on $L^{2}(\\pi_{\\eta})$ , of whom 1 is a simple eigenvalue. Moreover, any other eigenvalue of $\\kappa_{\\mathsf{D P n P},\\eta}$ has modulus no larger than $\\lambda$ . ", "page_idx": 27}, {"type": "text", "text": "Step 2: establishing the contractivity of $\\kappa_{\\mathsf{D P n P},\\eta}$ in $L^{2}(\\pi_{\\eta})$ . It is easy to verify that the constant function 1, which takes value 1 for any $x\\,\\in\\,\\mathbb{R}^{d}$ , is a eigenfunction of $\\kappa_{\\mathsf{D P n P},\\eta}$ associated to the simple eigenvalue 1, thus is the only (up to scaling) eigenfunction associated to that eigenvalue. It is also a unit-length eigenfunction, since $\\begin{array}{r}{\\|\\mathbf{1}\\|_{L^{2}(\\pi_{\\eta})}=(\\int1\\cdot\\pi_{\\eta}(x)d x)^{1/2}=1}\\end{array}$ . Therefore, the operator $\\boldsymbol{\\mathcal{K}}_{\\sf D P n P,\\eta}-\\mathbf{11}^{\\top}$ is a self-adjoint operator whose eigenvalues have modulus no larger than $\\lambda$ , where $\\mathbf{11^{\\top}}$ is the orthogonal projection onto 1 in $L^{2}(\\pi_{\\eta})$ , defined by ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbf{1}\\mathbf{1}^{\\top}f(x)\\equiv\\int f(x^{\\prime})\\pi_{\\eta}(x^{\\prime})\\mathrm{d}x^{\\prime},\\quad\\forall x\\in\\mathbb{R}^{d}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Using the fact that $\\mathcal{K}_{\\mathsf{D P n P},\\eta}\\mathbf{11}^{\\top}\\,=\\,\\mathbf{11}^{\\top}\\mathcal{K}_{\\mathsf{D P n P},\\eta}\\,=\\,\\mathbf{11}^{\\top}$ , one may show $(\\mathcal{K}_{\\sf D P n P,\\eta}-\\mathbf{11}^{\\top})^{N}=$ $\\mathcal{K}_{\\mathsf{D P n P},\\eta}^{(N)}-\\mathbf{11}^{\\top}$ by expanding the product, see e.g. [SC97]. Consequently, $\\mathcal{K}_{\\mathsf{D P n P},\\eta}^{N}-\\mathbf{11}^{\\top}$ is a self-adjoint operator whose eigenvalues have modulus no larger than $\\lambda^{N}$ , i.e., ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathcal{K}_{\\mathsf{D P n P},\\eta}^{N}-\\mathbf{11}^{\\top}\\|_{L^{2}(\\pi_{\\eta})\\to L^{2}(\\pi_{\\eta})}\\le\\lambda^{N},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\parallel\\cdot\\parallel_{L^{2}(\\pi_{\\eta})\\rightarrow L^{2}(\\pi_{\\eta})}$ denotes the operator norm on $L^{2}(\\pi_{\\eta})$ . ", "page_idx": 27}, {"type": "text", "text": "Step 3: bounding the inner product of p \u25e6K(DNPn)P,\u03b7 \u2212\u03c0\u03b7 with any square-integrable function. Note that when , the conclusion is trivially true. For the rest part of the proof, we assume $\\chi^{2}(p\\,\\|\\,\\pi_{\\eta})<\\infty$ . Now, for any $f\\in L^{2}(\\pi_{\\eta})$ , by applying Lemma 10 iteratively, we obtain ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\int p\\circ K_{\\mathsf{D P n P},\\eta}^{(N)}(x)f(x)\\mathrm{d}x=\\int p(x^{\\prime})K_{\\mathsf{D P n P},\\eta}^{N}f(x^{\\prime})\\mathrm{d}x^{\\prime}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\int p(x^{\\prime})\\cdot(K_{\\mathsf{D P n P},\\eta}^{N}-\\mathbf{11}^{\\top})f(x^{\\prime})\\mathrm{d}x^{\\prime}+\\int p(x^{\\prime})\\mathbf{11}^{\\top}f(x^{\\prime})\\mathrm{d}x^{\\prime}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\int p(x^{\\prime})\\cdot(K_{\\mathsf{D P n P},\\eta}^{N}-\\mathbf{11}^{\\top})f(x^{\\prime})\\mathrm{d}x^{\\prime}+\\int f(x^{\\prime})\\pi_{\\eta}(x^{\\prime})\\mathrm{d}x^{\\prime},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the last line follows from the definition of $\\mathbf{11^{\\top}}$ and $\\textstyle\\int p(x^{\\prime})\\mathrm{d}x^{\\prime}=1$ . Rearrange the terms to see ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\int\\left(p\\circ K_{\\mathsf{D P n P},\\eta}^{(N)}(x)-\\pi_{\\eta}(x)\\right)f(x)\\mathrm{d}x=\\int p(x^{\\prime})\\cdot(K_{\\mathsf{D P n P},\\eta}^{N}-\\mathbf{1}\\mathbf{1}^{\\top})f(x^{\\prime})\\mathrm{d}x^{\\prime}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "In particular, taking $p=\\pi_{\\eta}$ yields ", "page_idx": 27}, {"type": "equation", "text": "$$\n0=\\int\\pi_{\\eta}(x^{\\prime})\\cdot(K_{\\mathsf{D P n P},\\eta}^{N}-\\mathbf{11}^{\\top})f(x^{\\prime})\\mathrm{d}x^{\\prime}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Substract (49) from (48), and then take absolute value, we obtain ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\left\\|\\int\\left(p\\circ K_{\\mathsf{D P n P},\\eta}^{(N)}(x)-\\pi_{\\eta}(x)\\right)f(x)\\mathrm{d}x\\right\\|}\\\\ &{=\\displaystyle\\left|\\int\\left(p(x^{\\prime})-\\pi_{\\eta}(x^{\\prime})\\right)\\cdot\\left(K_{\\mathsf{D P n P},\\eta}^{N}-\\mathbf{11}^{\\top}\\right)f(x^{\\prime})\\mathrm{d}x^{\\prime}\\right|}\\\\ &{\\leq\\left(\\displaystyle\\int\\frac{\\left(p(x^{\\prime})-\\pi_{\\eta}(x^{\\prime})\\right)^{2}}{\\pi_{\\eta}(x)}\\mathrm{d}x\\right)^{1/2}\\cdot\\left\\|\\left(K_{\\mathsf{D P n P},\\eta}^{N}-\\mathbf{11}^{\\top}\\right)f(x^{\\prime})\\right\\|_{L^{2}(\\pi_{\\eta})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "equation", "text": "$$\n\\leq\\sqrt{\\chi^{2}(\\boldsymbol{p}\\,\\Vert\\,\\boldsymbol{\\pi}_{\\boldsymbol{\\eta}})}\\cdot\\lambda^{N}\\Vert f\\Vert_{L^{2}(\\pi_{\\boldsymbol{\\eta}})}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Step 4: choosing an appropriate square-integrable function. Now, set ", "text_level": 1, "page_idx": 28}, {"type": "equation", "text": "$$\nf(x)=\\frac{p\\circ K_{\\mathsf{D P n P},\\eta}^{(N)}(x)-\\pi_{\\eta}(x)}{\\pi_{\\eta}(x)}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "It is easily checked that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\int\\big(p\\circ K_{\\mathsf{D P n P},\\eta}^{(N)}(x)-\\pi_{\\eta}(x)\\big)f(x)\\mathrm{d}x=\\chi^{2}(p\\circ K_{\\mathsf{D P n P},\\eta}^{(N)}\\parallel\\pi_{\\eta}),\\quad}\\\\ {\\|f\\|_{L^{2}(\\pi_{\\eta})}=\\sqrt{\\chi^{2}(p\\circ K_{\\mathsf{D P n P},\\eta}^{(N)}\\parallel\\pi_{\\eta})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Plug these equations into (50), we obtain ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\chi^{2}(p\\circ K_{\\mathsf{D P n P},\\eta}^{(N)}\\parallel\\pi_{\\eta})\\leq\\lambda^{2N}\\chi^{2}(p\\parallel\\pi_{\\eta}),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "as claimed. ", "page_idx": 28}, {"type": "text", "text": "G Additional numerical results ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "G.1 Implementation details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Score functions used. We use the same pre-trained score functions as in $[\\mathsf{C K M}^{+}23]$ .3 ", "page_idx": 28}, {"type": "text", "text": "Normalization. All images are normalized in the usual way to fit into the range $[-1,1]$ . ", "page_idx": 28}, {"type": "text", "text": "Parameters of our algorithm. We choose the same annealing schedule across all tasks. Please see Appendix H for a detailed discussion. ", "page_idx": 28}, {"type": "text", "text": "Parameters of comparison methods. We made our best effort to fine-tune the other algorithms within a reasonable amount of time for each task. We list the paramters, following the notations in the original paper $[\\mathbf{CKM}^{+}23$ , $\\mathrm{SZY}^{+}23]$ , as follows. ", "page_idx": 28}, {"type": "text", "text": "\u2022 Super-resolution: For DPS, the learning rate is set to 0.6. For LGD-MC, the MC sampling variance $r_{t}\\,=\\,0.05$ , the loss coefficient $\\lambda=10^{-3}$ , and the learning rate is set to 60.0. For ReSample, the stochastic resampling variance parameter $\\gamma=10$ for FFHQ and $\\gamma=4.0$ for ImageNet.   \n\u2022 Phase retrieval: For DPS, the stepsize is set to 0.8. For LGD-MC, the MC sampling variance $r_{t}=0.05$ , the loss coefficient $\\lambda=10^{-3}$ , and the learning rate is set to 400.0.   \n\u2022 Quantized sensing: For DPS, the stepsize is set to 100.0. For LGD-MC, the MC sampling variance $r_{t}\\,=\\,0.05$ , the loss coefficient $\\lambda=2\\times10^{-5}$ , and the learning rate is set to 500.0. For ReSample, the stochastic resampling variance parameter $\\gamma=4$ for FFHQ and $\\gamma=3.5$ for ImageNet. ", "page_idx": 28}, {"type": "text", "text": "G.2 Forward measurement operators ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Super-resolution. The forward model for super-resolution is the usual bicubic downsampling operator [Key81], which is a linear operator (in fact, a block Hankel matrix). We use a downsampling ratio of 4 in all our experiments. The measurement noise is set to be white Gaussian, with variance 0.2. Note that the noise variance is moderately larger than that in $[\\mathrm{CKM}^{+}23]$ to better reflect the scenario in practical inverse problems. ", "page_idx": 28}, {"type": "text", "text": "Phase retrieval. We consider phase retrieval with a coded mask, which is a classical inverse problem [CLS15]. For a $256\\times256$ image $x$ (for each color channel) in our experiments, we first generate a random mask $M\\in\\mathbb{R}^{256\\times256}$ (which is shared across color channels), then apply Fourier transform $\\mathcal{F}$ to $M\\odot x$ , where $\\odot$ denotes the Hadarmard (entrywise) product, and finally preserve only the magnitudes of the Fourier transform. Formally, the forward measurement operator is ${\\mathcal A}(\\stackrel{.}{x})\\;=$ $\\operatorname*{mag}(\\mathcal{F}(M\\odot x))$ , where $\\operatorname*{mag}(\\cdot)$ computes the entrywise magnitude of a matrix with complex entries. The measurement noise is again set to be white Gaussian, with variance 0.2. ", "page_idx": 28}, {"type": "image", "img_path": "SLnsoaY4u1/tmp/abf6c130f7b0ba0a6422f7b8a1c7dea15d65be402961754f2cd68855e605ec8f.jpg", "img_caption": ["Quantized sensing. In this work, quantized sensing refers to the task of reconstructing an image from its low-bit quantized version. The forward measurement operator is a one-bit per channel, dithered quantization operator. More precisely, the forward measurement operator is an entrywise application of the following stochastic function $Q$ with dithering level $\\theta>0$ : "], "img_footnote": [], "page_idx": 29}, {"type": "equation", "text": "$$\nQ(\\mathfrak{p i x e l})=\\left\\{1,\\begin{array}{l l}{\\mathrm{with~probability~}\\frac{\\mathrm{e}^{\\mathfrak{p i x e l}/\\theta}}{1+\\mathrm{e}^{\\mathfrak{p i x e l}/\\theta}}}\\\\ {-1,}&{\\mathrm{with~probability~}\\frac{1}{1+\\mathrm{e}^{\\mathfrak{p i x e l}/\\theta}},}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\mathsf{p i x e l}\\in[-1,1]$ is the value of each pixel in each channel. The measurements in quantized sensing are therefore one-bit-per-channel images. The dithering level $\\theta$ is set to 0.4 in our experiments. ", "page_idx": 29}, {"type": "text", "text": "G.3 Sample images for other inverse problems ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Super-resolution. The samples generated by different algorithms are shown in Table 5. ", "page_idx": 29}, {"type": "text", "text": "Across different tasks, linear and nonlinear, it can be seen that DPnP has stronger capability of reconstructing the image with higher fidelity to the fine details. ", "page_idx": 29}, {"type": "text", "text": "G.4 Additional performance metrics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Computation time in terms of Neural Function Estimations (NFEs). In additional to the clock time statistics in the main text, we also measure the computational cost per sample of different algorithms in terms of the number of Neural Function Estimations, i.e., the number of calls to score functions. The results are in Table 6. Note that the NFEs for DPnP depends on the initialization, the annealing schedule (and the number of timesteps for DPnP-DDIM). We provide typical numbers of NFEs with the choice of parameters given in Appendix H and with a suitable number of timesteps. ", "page_idx": 29}, {"type": "table", "img_path": "SLnsoaY4u1/tmp/92dabe265b24ebac6c183f3127628865ccdbe1e42b7971eae103cd249da76f3b.jpg", "table_caption": ["Table 6: Number of NFEs for different algorithms. "], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "Frechet Inception Distance (FID) and Structural Similarity Index Measure (SSIM). We also compare the FID and SSIM of different algorithms across different tasks. The results are shown in Table 7 and Table 8. It should be pointed out that FID is arguably not a very relevant notion to measure the quality of solving inverse problems, as accurately solving inverse problems means that the generated distribution is close to the conditional, i.e., posterior distribution of the image, while FID only measure the closeness to the unconditional, i.e., prior distribution of the image. ", "page_idx": 29}, {"type": "table", "img_path": "SLnsoaY4u1/tmp/7d5641bf3df9a0d71e95de91486522c4f164d4e42a38dedf03aaaf62eae3a753.jpg", "table_caption": ["Table 7: FID and SSIM of solving inverse problems on FFHQ $256\\times256$ validation dataset (1k samples). "], "table_footnote": [], "page_idx": 30}, {"type": "table", "img_path": "SLnsoaY4u1/tmp/3f226a9080ce8f3252f1c352611743f657d69e7e4c4b1e24fb6c22f69889681c.jpg", "table_caption": ["Table 8: FID and SSIM of solving inverse problems on ImageNet $256\\times256$ validation dataset (1k samples). "], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "H Ablation studies ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "H.1 Initialization ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "In the Algorithm 1, the initial guess $\\widehat{x}_{0}$ is set to be a properly scaled Gaussian random vector. However, from Theorem 2 it can be infered   that using a heuristic posterior sampler as the initializer could decrease $\\chi^{2}(p_{\\widehat{x}_{1}}\\parallel\\pi_{\\eta})$ , hence potentially improve the convergence speed of DPnP. By using existing algorithms lik e DPS or LGD-MC as initializer, $\\mathsf{D P n P}$ can improve upon the results of existing algorithms towards the correct posterior distribution efficiently and provably. In our experiments, we find it helpful to initialize $\\mathsf{D P}\\bar{\\mathsf{n P}}$ with LGD-MC, which accelerates the algorithm significantly. ", "page_idx": 30}, {"type": "text", "text": "H.2 Annealing schedule ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "We discuss the choice of the annealing schedule $\\eta_{k}$ in DPnP (Algorithm 1). As seen in the theoretical analysis (Theorem 2), if we set all the $\\eta_{k}\\equiv\\eta$ for some constant $\\eta>0$ , then $\\mathsf{D P n P}$ converges to a distribution $\\pi_{\\eta}$ , which can be regarded as a version of the posterior distribution $p^{\\star}(\\cdot|y)$ distorted by an order of $O(\\eta)$ . The smaller $\\eta$ is, the more accurate the final distribution will be. On the other hand, it was also seen that in many cases, the spectral gap is $\\Omega(\\eta)$ , hence the convergence time is $\\begin{array}{r}{O(\\frac{1}{\\eta})}\\end{array}$ . Therefore, smaller $\\eta$ would make it take longer to converge.4 ", "page_idx": 30}, {"type": "text", "text": "To strike a balance between the accuracy and the convergence rate, we find it empirically successful to adapt an gradually decreasing schedule for $\\eta_{k}$ , similar to [BB23]. In the first few iterations, we set $\\eta_{k}$ to be a large constant. After this initial phase, we decrease $\\eta_{k}$ slowly, eventually to $\\eta_{N}$ which is chosen to be a small constant. An example of such an annealing schedule is ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\eta_{k}=\\left(\\eta_{K}/\\eta_{0}\\right)^{\\frac{k-K_{0}}{K-K_{0}}}\\eta_{0},\\quad K_{0}<k\\le K,\\quad\\eta_{K}>0\\;\\mathrm{a\\;small\\;constant},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $K_{0}<K$ is the length of the initial phase, which can be chosen as, e.g. $K_{0}=K/5$ . For all the numerical experiments, we set $\\eta_{0}=0.4$ , $\\eta_{N}=0.15$ , $K_{0}=4$ , $K=20$ . ", "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The abstract and introduction provides explicit description of the contributions and the context. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: We discuss the scope of our work and provide ablation studies ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 31}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We provide clear statement of assumptions for our theory. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: We provide algorithm tables and experimental setups with a reasonable level of detail. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 32}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 32}, {"type": "text", "text": "Answer: [No] ", "page_idx": 33}, {"type": "text", "text": "Justification: The code is still in preparation to meet publication standard. We promise to release it after this work is accepted. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 33}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Experimental settings are given in detail. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 33}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: The computational cost of reporting error bars is too high. Most of the closely related works do not include them. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: All experiments are done on a single Nvidia L40 GPU. The time of execution is reported. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 34}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: We fully honor the NeurIPS Code of Ethics in this work. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 34}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: While our provably robust algorithm may potentially improve the fieldity of diffusion-based methods in solving inverse problems, it awaits future research before anything special can be said about potential societal impacts. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: No data or models is released by this work. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 35}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: All assets are credited properly. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 35}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 36}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 36}]