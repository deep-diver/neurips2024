[{"type": "text", "text": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Coleman Hooper1 Sehoon ${\\bf K i m}^{1}$ Hiva Mohammadzadeh1 Michael W. Mahoney1,2,3 Yakun Sophia Shao1 Kurt Keutzer1 Amir Gholami1,2 ", "page_idx": 0}, {"type": "text", "text": "1University of California, Berkeley 2ICSI 3LBNL {chooper, sehoonkim, hiva, mahoneymw, ysshao, keutzer, amirgh}@berkeley.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "LLMs are seeing growing use for applications which require large context windows, and with these large context windows KV cache activations surface as the dominant contributor to memory consumption during inference. Quantization is a promising approach for compressing KV cache activations; however, existing solutions fail to represent activations accurately in sub-4-bit precision. Our work, KVQuant, facilitates low precision KV cache quantization by incorporating several novel methods: (i) Per-Channel Key Quantization, where we adjust the dimension along which we quantize the Key activations to better match the distribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations before the rotary positional embedding to mitigate its impact on quantization; (iii) Non-Uniform KV Cache Quantization, where we derive per-layer sensitivity-weighted non-uniform datatypes that better represent the distributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we isolate outliers separately for each vector to minimize skews in quantization ranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral models, we achieve $<0.1$ perplexity degradation with 3-bit quantization on both Wikitext-2 and C4, outperforming existing approaches. Our method enables serving LLaMA-7B with a context length of up to 1 million on a single A100-80GB GPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for KVQuant, showing that we can achieve up to ${\\sim}1.7\\times$ speedups, compared to baseline fp16 matrix-vector multiplications, for the LLaMA-7B model. Code is available at https://github.com/SqueezeAILab/KVQuant. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large language models (LLMs) have revolutionized many natural language processing (NLP) tasks. In order to improve the capabilities of LLMs, there is significant interest in increasing the context lengths of LLMs. Longer context lengths enable new applications, including long document summarization, retrieval for answering questions about long documents, extended multi-turn applications [6], and code analysis. To support this pull from applications, there have been significant recent advances in long-context length models in industry [2, 30], as well as in academia [6, 22]. ", "page_idx": 0}, {"type": "text", "text": "Given the importance of LLM workloads, there is strong motivation to improve their inference efficiency. LLM inference with large context lengths can be incredibly resource-intensive; serving LLMs requires high-end GPUs, and the largest LLMs require costly multi-GPU inference setups. When analyzing the computational nature of generative inference with LLMs, it becomes quickly apparent that, for relatively small batch sizes, the computation is memory bound [17]. With the growing divergence between computational speeds and memory speeds, this problem is only going to get worse over time [12]. This makes reducing the memory bottleneck preeminently important. Further analysis shows that the memory bottleneck is strongly related to context size. For short sequence lengths, the dominant contributor to memory consumption is the weight matrices, and therefore the optimal strategy is to minimize the model size in order to reduce memory consumption as well as bandwidth requirements [18, 17]. However, as shown in Figure 1, the main bottleneck for long sequence lengths is the memory requirements for caching Key and Value (KV) activations throughout inference. This challenge is further exacerbated when one considers batched inference. ", "page_idx": 0}, {"type": "image", "img_path": "0LXotew9Du/tmp/f4fb9a4f71384bd166a45b46cf93ea09e8581afdc66d4ae617a960a5ed40101a.jpg", "img_caption": ["Figure 1: Left: Model size versus activation memory size for the LLaMA-7B model with sequence length 512 and 128K. For longer context lengths, the KV cache becomes the dominant memory bottleneck. Memory consumption of model weights and KV cache activations for different LLaMA models with different sequence lengths are provided in Table 7 in Appendix A. Right: Overview of the different components used in KVQuant that result in less than 0.1 perplexity degradation over the fp16 baseline when quantizing the KV cache for the LLaMA-7B model to 3-bit precision. As shown in Table 1, our 3-bit approach results in $4.8\\times$ reduction in cached activation memory footprint. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "It is therefore crucial to develop methods for compressing the KV cache to enable efficient longsequence length inference. Existing approaches lead to unacceptable accuracy degradation due to the outlier structures in KV cache activations as well as suboptimal bit allocation with existing uniform and non-uniform approaches. In this work, we perform an extensive analysis of KV cache activations in recent LLMs, revealing patterns which can be exploited to enable ultra-low precision quantization with minimal accuracy loss. In particular, we make the following contributions (summarized in Figure 1): ", "page_idx": 1}, {"type": "text", "text": "\u2022 We find that the Keys exhibit outliers in specific channels before applying RoPE. However, the outlier channel magnitudes become less consistent after applying RoPE, posing a distinct challenge for low precision quantization. We address this by quantizing Keys per-channel before RoPE is applied (see Section 3.1 and Section 3.2). ", "page_idx": 1}, {"type": "text", "text": "\u2022 We find that existing uniform and non-uniform quantization methods result in sub-optimal quantization signpost placement. Instead, we propose a Non-Uniform Quantization (NUQ) method which considers sensitivity and not just magnitude when quantizing activations. We show that one can apply sensitivity-weighted non-uniform quantization offilne on a calibration set to derive accurate datatypes for KV cache quantization (see Section 3.3).   \n\u2022 Even with the above, we find that outlier values in cached KV activations can significantly degrade quantization resolution. Unlike for weights, it is non-trivial to extract outlier values from activations, given the dynamic nature of activations. However, we find that we can efficiently and accurately identify and compress outlier values in order to store them compactly in a separate sparse representation. We also find that per-vector outlier detection outperforms per-matrix outlier detection with no additional memory overhead. By removing only $1\\%$ of outliers, we can attain under 0.1 perplexity degradation on both Wikitext-2 and C4 for 3-bit KV cache quantization with the LLaMA, Llama-2, Llama-3, and Mistral models, thereby facilitating accurate inference with $4.8\\times$ longer context length.   \n\u2022 We implement custom CUDA kernels to perform activation quantization efficiently during inference, achieving up to ${\\sim}1.7\\times$ speedups for Key and Value matrix-vector multiplications for LLaMA-7B at 4-bit precision relative to the fp16 baseline (see Section 3.7 and 4.4). These results demonstrate how our methodology allows for accurate and efficient low-bit KV cache quantization. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 LLM Inference ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "When inferring a decoder-only LLM, inference proceeds in two distinct phases. In the prefill phase, the model takes in an input prompt, which it processes in parallel. During the generation phase, the model then generates the output sequence autoregressively, meaning that each token generation is dependent on all previously generated tokens. As such, for small batch sizes, the generation phase of LLM inference is typically memory-bandwidth bound, as the only available parallelism is across different sequences in a given batch. Additionally, during generation, the model needs to store intermediate Key and Value activations at each layer in order to condition generations on previously generated output tokens. Otherwise, we would need to recompute all prior Keys and Values at each timestep, which would be prohibitively expensive. These stored activations are referred to as the Key-Value $(K V)$ cache. Throughout this paper, we will capitalize Key and Value to distinguish when we are referring to the KV cache tensors. Assuming a model with $n$ layers and $h$ attention heads with dimension $d$ that is stored using $e$ bytes per element, the KV cache size for batch size $b$ and sequence length $l$ is $2\\cdot n\\cdot h\\cdot d\\cdot e\\cdot b\\cdot l\\,$ , meaning that it grows linearly with both batch size and sequence length. As shown in Table 7, the KV cache becomes the dominant contributor to memory consumption for longer sequence lengths and larger batch sizes. Note that since each sequence in batched inference depends on separate past context, there is no available batch-level parallelism when loading the cached Keys and Values for their respective computations in batched inference. KV cache loading is therefore always memory-bandwidth bound. This motivates pursuing methods to optimally compress the KV cache, even at the expense of a more complex dequantization process. ", "page_idx": 2}, {"type": "text", "text": "2.2 LLM Quantization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "There have been many prior works on LLM quantization. Several have focused on weight-only quantization for LLMs, due to the greater contribution to memory consumption and runtime for small sequence lengths and batch sizes [21, 9, 17]. Prior works have noted the presence of distinct outliers in both weights and activations [7, 9, 17]. One approach that has been developed to address this outlier issue in the context of weight quantization is dense-and-sparse quantization, where each weight matrix is decomposed into a sparse outlier matrix and a dense low-precision matrix [9, 17]. Prior works have also leveraged non-uniform quantization methods to improve accuracy for the same bit precision by allowing for flexible quantization signpost placement [17, 8]. These approaches have either used a fixed non-uniform datatype such as NormalFloat [8], or derived quantization signposts using a sensitivity-weighted $\\boldsymbol{\\mathrm{k}}$ -means approach [17]. Appendix B provides a more detailed overview of related work for outlier-aware LLM quantization and non-uniform LLM quantization. ", "page_idx": 2}, {"type": "text", "text": "There has also been work on quantizing both weights and activations (including KV cache) [41, 33]. However, there is still a significant perplexity degradation when quantizing KV cache activations to low precision; [34, 44] quantized KV cache activations to 4-bits, but required fine-grained grouping for 4-bit quantization, while still observing some perplexity degradation, and [34] observed that 3-bit KV cache quantization with fine-grained grouping leads to unacceptable accuracy loss. Other works quantized KV cache activations to 4-bits but required retraining to maintain performance [24]. One concurrent work also explores low precision KV cache quantization in order to enable larger batch size inference by reducing the KV cache size [26]. ", "page_idx": 2}, {"type": "text", "text": "2.3 KV Cache Compression ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "There have also been several prior works on compressing the KV cache. Some of these methods aim to only store important tokens in the KV cache and to evict less important tokens, thereby maintaining low memory usage [25, 43, 11, 20]. Other methods aim to only retrieve a subset of tokens at each step to achieve memory bandwidth savings [32]. In this work, we explore KV cache quantization as an orthogonal direction for compressing the KV cache in order to enable long context inference. ", "page_idx": 2}, {"type": "image", "img_path": "0LXotew9Du/tmp/c2306bc44511067d37a66fb324cfd456cd8b7c502a35fc8f946a0cbbb68487ea.jpg", "img_caption": ["Figure 2: Example distributions of the activation values for Keys pre-RoPE, Keys post-RoPE, and Values for LLaMA-7B on a sample with 2K sequence length from Wikitext-2. We observe several patterns: (i) Keys pre-RoPE exhibit clear outliers in specific channels across different tokens; (ii) after applying RoPE, the distribution becomes less structured and there are less consistent magnitudes for outlier channels (this is expected, as RoPE applies a rotation operation between pairs of channels); and (iii) Values exhibit no fixed outlier pattern with outlier values across channels and tokens. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 Per-Channel Key Quantization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To inform our approach, we first performed a detailed analysis to understand the KV cache distributions. Figure 2 shows sample distributions for the KV cache activations. We observe that the Key matrices tend to have distinct outlier channels, which have larger average magnitudes than other channels; this corroborates previous observations about outlier channels in LLM activations [7, 41]. The Value matrices exhibit both outlier channels as well as outlier tokens (although these outliers are less extreme than the outlier Key channels). ", "page_idx": 3}, {"type": "text", "text": "Existing KV cache quantization approaches perform per-token quantization (meaning that the scaling factor and zero-point are shared by elements in the same token) [34, 44]. However, due to the differing average magnitudes between channels, the values within a channel are easier to quantize when grouped together than the values across different channels. As such, to better match the distributions, we investigate per-channel KV cache quantization, meaning that the scaling factor and zero-point are shared by elements in the same channel. By sharing the scaling factor and zero-point along the channel dimension, this will naturally group together values with similar magnitudes, thereby mitigating the impacts of outlier channels on other channels when quantizing to low precision. As outlined in Appendix G, we find that per-channel quantization provides significant accuracy benefits for Keys but not for Values. By leveraging per-channel quantization for Keys and pertoken quantization for Values, we observe a 3.82 perplexity improvement on Wikitext-2 for 3-bit LLaMA-7B quantization. Note that this can potentially add runtime overhead since the quantization dimension is now misaligned with the reduction dimension for the Keys when performing matrixvector multiplications. However, we find that we are able to efficiently dequantize Keys and perform the Query-Key matrix-vector multiplication without adding runtime overhead, as shown in Section 4.4. Additionally, as outlined in Section 3.6, per-channel quantization can also be challenging due to the need to recompute scaling factors as tokens are added to the Key cache. We show that we can calibrate offline for scaling factors, thereby avoiding expensive online recomputation. ", "page_idx": 3}, {"type": "text", "text": "Per-channel Key quantization was also explored in another concurrent work [26], which leveraged similar intuition about grouping together large magnitude values in the same channel to minimize quantization error. Their methodology requires fine-grained grouping for per-channel quantization while maintaining a residual subset of the KV cache in fp16. In our work, we demonstrate that by leveraging offilne calibration, we can accurately perform per-channel quantization without grouping. ", "page_idx": 3}, {"type": "text", "text": "3.2 Pre-RoPE Key Quantization", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "One issue when quantizing Keys is handling the rotary positional embedding (RoPE), which is applied to Keys and Queries in most public LLMs, including LLaMA and Llama-2 [35]. Given Query and Key vectors $Q_{m}=W_{q}*x_{m}$ and $K_{n}=W_{k}*x_{n}$ at positions $m$ and $n$ in the sequence, RoPE is applied as position-dependent rotations to each of these vectors to obtain $\\tilde{Q}_{m}=R_{\\theta,m}^{d}\\cdot Q_{m}$ and $\\tilde{K}_{n}=R_{\\theta,n}^{d}\\cdot K_{n}$ . This embeds the relative position between a Query and Key vector as an amount of an angle that is a multiple of its position index. When caching Key vectors, we therefore need to either cache $\\tilde{K}_{n}$ , or else we need to cache $K_{n}$ and apply $R_{\\theta,n}^{d}$ on-the-fly during inference. The challenge with caching Key vectors after applying this rotation is that it leads to mixing pairs of channels by different amounts for different positions in the sequence, as shown in Appendix C (since it jointly rotates pairs of channels by different angles depending on the position in the sequence). The post-RoPE activation distribution is also shown in Figure 2, demonstrating how the rotation between pairs of channels leads to less consistent channel magnitudes. This makes it harder to quantize Key activation channels which would typically have consistent large-magnitude values. This motivated our investigation into whether we could perform pre-RoPE Key quantization (meaning that we quantize $K_{n}$ ) and then efficiently apply the positional embeddings on-the-fly after dequantization. The beneftis of pre-RoPE Key quantization are highlighted in Appendix H, yielding 0.82 perplexity improvement on Wikitext-2 for 3-bit LLaMA-7B quantization. To be able to quantize Keys pre-RoPE, we develop a fused kernel to efficiently apply RoPE post-dequantization (the details of this approach will be discussed in Section 3.7). ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "3.3 nuqX: An X-Bit Per-Layer Sensitivity-Weighted Non-Uniform Datatype ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Uniform quantization is suboptimal for KV cache quantization since the Query and Key activations are non-uniform. Additionally, KV cache loading is memory bandwidth bound, regardless of batch size or sequence length, meaning that the dequantization overhead introduced by non-uniform quantization methods is not problematic (since the added computation does not introduce any additional latency). It is therefore desirable to leverage non-uniform quantization methods for KV cache quantization. In [17], the authors computed non-uniform quantization signposts using a sensitivity-weighted $\\boldsymbol{\\mathrm{k}}$ -means approach. However, this cannot be applied directly to KV cache quantization as the Values are quantized dynamically at runtime, which means that we would need to apply K-means online during inference, and it is also difficult to estimate sensitivity for activations online. We therefore facilitate efficient online non-uniform KV cache quantization by deriving a per-tensor non-uniform datatype offline on calibration data, which is then rescaled per-channel or per-token to accurately represent the key and value distributions. We compute sensitivity-weighted quantization signposts offilne on a calibration set prior to inference, while maintaining compatibility with per-vector quantization by separately normalizing each channel to the range $[-1,1]$ prior to deriving the shared datatype. Using the diagonal Fisher information matrix (derived in Appendix D), along with the quantization error for activation $A$ , we formulate the error minimization objective as follows, where $A$ is flattened to one dimension and where $N$ is the number of elements from all of the samples in our calibration set: ", "page_idx": 4}, {"type": "equation", "text": "$$\nQ(A)^{*}\\simeq\\arg\\operatorname*{min}_{Q}\\sum_{i=1}^{N}\\mathcal{F}_{i i}\\big(A_{i}-Q(A_{i})\\big)^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We modify the objective in Equation 1 as described in Appendix E in order to apply it using the normalized activation values. We then minimize it offline on a calibration set using a $\\mathbf{k}$ -means solver in order to obtain the quantization signposts for the non-uniform datatype for each Key or Value layer. Appendix I compares our non-uniform quantization approach with existing uniform and non-uniform quantization baselines [8], demonstrating how our non-uniform approach provides 0.29 perplexity improvement on Wikitext-2 for LLaMA-7B relative to 3-bit uniform methods. Table 16 in Appendix L shows how computing the required Fisher information for the LLaMA-65B model takes only a few minutes, and how using the k-means solver takes only a few minutes per layer (with the computation for each layer being parallelizable). ", "page_idx": 4}, {"type": "text", "text": "3.4 Per-Vector Dense-and-Sparse Quantization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "As shown in Figure 4 in Appendix F, for both Keys and Values, the majority of elements are contained within a small percentage of the dynamic range. This means that by leveraging dense-and-sparse quantization, as demonstrated in [17], in order to isolate a small percentage of numerical outliers, we can restrict the range that we need to represent, thereby allowing us to represent the remaining elements with greater precision. However, when looking at the Key and Value distributions in Figure 2, different channels and tokens have different average magnitudes. Therefore, an element which counts as an outlier in one channel may not be an outlier in another channel (since that channel may have a greater average magnitude), making naive application of dense-and-sparse quantization suboptimal. It is therefore crucial to directly target the outlier values that skew the dynamic range at the granularity that we are quantizing in order to address the values that are exaggerating the range along that particular dimension. In this work, we leverage per-vector dense-and-sparse quantization, where we use a different outlier threshold per-vector (either a separate threshold per-channel for per-channel quantization, or a separate threshold per-token for per-token quantization), rather than a single outlier threshold for each layer. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Note that computing outlier thresholds for per-vector dense-and-sparse quantization poses potential accuracy and efficiency challenges. However, in Section 3.6, we show that we are able to accurately calibrate for per-channel outlier thresholds offilne and efficiently compute per-token outlier thresholds online. After determining the upper and lower outlier thresholds, the remaining numbers in the vector are normalized to the range $[-1,1]$ , and we then minimize Equation 1 (ignoring outliers) in order to obtain the quantization signposts for the non-uniform datatype for the remaining numbers. Appendix J will demonstrate the benefits of removing a small percentage of numerical outliers and keeping them in full precision, as well as the advantages of per-vector dense-and-sparse quantization over using a single global outlier threshold for each layer. As shown in Figure 1, by removing $1\\%$ of numerical outliers using per-vector outlier thresholds, we achieve an additional 0.19 perplexity improvement on Wikitext-2 for 3-bit LLaMA-7B quantization, which is within 0.07 perplexity of the fp16 baseline. ", "page_idx": 5}, {"type": "text", "text": "3.5 Attention Sink-Aware Quantization ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Prior work has demonstrated that after the first few layers in LLMs, the model tends to allocate a large attention score to the first token [42]. This occurs even when the initial token is not semantically important. This phenomenon happens because the model tends to use the inital token as a \u201csink\u201d. In our work, we demonstrate that due to the Attention Sink phenomenon, the model is disproportionately sensitive to quantization error in the first token. By keeping only the first token in fp16, we can attain perplexity benefits, particularly for 2-bit quantization. A similar observation has also been made in another concurrent work [23]. Note that when retaining the first token in fp16, we account for this during the calibration process as well, meaning that we ignore the first token when deriving the nuqX datatype and when calibrating the scaling factors and zero points offilne for the Keys. As demonstrated in Appendix K, this approach persistently yields performance benefits, particularly with lower bit widths and without dense-and-sparse quantization. ", "page_idx": 5}, {"type": "text", "text": "3.6 Offline Calibration versus Online Computation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "A crucial challenge for activation quantization is that we either need to compute statistics on-the-fly (which is potentially expensive) or else we need to use offilne calibration data (which potentially has negative accuracy implications). The challenges with computing scaling factors (and zero-points) online versus offline for both Keys and Values are shown in Figure 5 in Appendix L. In per-channel quantization, it is challenging to update scaling factors online since the scaling factors corresponding to each incoming channel would potentially need to be updated whenever a new token is added to the KV cache. It is therefore desirable to be able to compute statistics offilne (i.e., using calibration data before running inference). While this can have negative effects on model accuracy, in Appendix L we show that we can effectively calibrate offline for per-channel quantization, obviating the need for online updates of scaling factors for per-channel quantization. For per-token quantization, it is challenging to calibrate for scaling factors offline due to the presence of outlier Value tokens. It is therefore desirable to be able to compute scaling factors and outlier thresholds online for each incoming token. As shown in Appendix L, we can efficiently compute outlier thresholds online per-token by offloading to the CPU. By leveraging custom quantization function implementations for compressing activations, we are able to perform online per-token Value quantization without compromising on performance. ", "page_idx": 5}, {"type": "text", "text": "3.7 Kernel Implementation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In order to efficiently perform activation quantization on-the-fly, we leverage dedicated kernel implementations with our 4-bit quantization method for compressing vectors to reduced precision and extracting the sparse outliers, performing matrix-vector multiplications using the compressed vectors, and performing sparse matrix-dense vector multiplications using the sparse outliers. We store the quantized Key and Value matrices as 4-bit elements which are used as indices into lookup tables to recover the corresponding fp16 values. We store the sparse outlier matrices in either CompressedSparse Row (CSR) or Compressed-Sparse Column (CSC) format (depending on which aligns better with appending new Key and Value tokens). The kernels for the Key matrix-vector operations apply RoPE on-the-fly in order to support pre-RoPE quantization. More kernel implementation details are provided in Appendix R. ", "page_idx": 5}, {"type": "table", "img_path": "0LXotew9Du/tmp/4000bc7d4436e99c4bc2bfec121ffa15941fd61f5830758ff5200dc0b289dcaf.jpg", "table_caption": ["Table 1: Evaluation of our method for different models using the perplexity (PPL) on Wikitext-2. KVQuant results are using pre-RoPE per-channel quantization for Keys. KV cache sizes are estimated assuming a sequence length of 128K (ignoring context length limits for the models). Note that ATOM and FlexGen use 4-bit quantization with group sizes of 128 and 64 with uniform quantization, respectively, and we extend their methods to 3-bit and 2-bit quantization. We leverage Attention Sink-Aware quantization for all bit widths. We used post-RoPE quantization for all baseline methods since it achieves higher accuracy when quantizing Keys per-token as shown in Appendix P. Table 18 in Appendix O demonstrates a full evaluation on all LLaMA, Llama-2, Llama-3, and Mistral models. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "4 Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Main Evaluation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We used the LLaMA-7B/13B/30B/65B, Llama-2-7B/13B/70B, Llama-3-8B/70B, and Mistral-7B models to evaluate our methodology by measuring perplexity on both Wikitext-2 and C4 [36, 37, 1, 16, 27, 31]. Perplexity has been measured using teacher forcing with the output logits of all input tokens. We compared our method against (i) uniform quantization without grouping (intX), (ii) nonuniform quantization using NormalFloat [8] without grouping (nfX), as well as (iii) Atom [44] and FlexGen [34]. Note that Atom and FlexGen use uniform quantization with group sizes of 64 and 128, respectively. All the KVQuant models throughout this experiment section are calibrated using 16 calibration samples of sequence length 2K from the Wikitext-2 training set. See Appendix M for details on our experimental setup, including our methodology for computing KV cache size estimates. ", "page_idx": 6}, {"type": "text", "text": "Table 1 shows the results for LLaMA models for the Wikitext-2 dataset. We compared our method with per-token quantization with and without grouping. The baseline configurations used by Atom and FlexGen are included for reference [44, 34]. We find that our method consistently outperforms baseline approaches by an especially large margin with 3-bit and 2-bit quantization. Once we incorporate outliers, we further push the performance of low-precision quantization, achieving 4-bit quantization with less than 0.02 perplexity degradation, 3-bit quantization with under 0.1 perplexity degradation, and 2-bit quantization with under 0.5 perplexity degradation on Wikitext-2, relative to the fp16 baseline, across all models (while attaining $3.7\\times$ , $4.8\\times$ , and $6.9\\times$ memory savings, respectively). ", "page_idx": 6}, {"type": "image", "img_path": "0LXotew9Du/tmp/8fe448f60b5cbc04f0d805423f6e1d3c6bb9846480337d8c9fddc86bd5f699a9.jpg", "img_caption": ["Figure 3: Perplexity results for the LLaMA-2-7B-32K model $[5J$ as well as the Llama-2-70B-32K LongLoRA model [6] on the Wikitext-2 dataset, evaluated using different sequence lengths. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 2: Passkey retrieval results across different context lengths for the LLaMA-2-7B-32K model (uptrained for long sequence lengths using positional interpolation [5]) as well as the Llama-2- 70B-32K LongLoRA model [6]. The values reported are the success rate for retrieving the passkey, computed over 50 samples. We also include comparisons with KIVI for reference, using the 2-bit configuration with group size of 32 and 128-element fp16 residual [26]. Average bit widths are estimated for each approach assuming $32K$ context length. Note that the open-source code for running KIVI with LLaMA does not support grouped-query attention, so we did not include comparisons with KIVI for Llama-2-70B-32K. ", "page_idx": 7}, {"type": "table", "img_path": "0LXotew9Du/tmp/5a8cfaadf002fba95d1206fc8fea3254c43ecf7e63b298cb3f806f456e545f65.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.2 Long Context Length Evaluation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Perplexity Evaluation. We evaluated long context length performance using the LLaMA-2-7B32K model (uptrained for long sequence lengths using positional interpolation [5]) as well as the Llama-2-70B-32K LongLoRA model [6]. For evaluating performance on longer context lengths, we first evaluated perplexity on Wikitext-2 using larger amounts of input context, as shown in Figure 3 [6, 13]. The results demonstrate how our method maintains accuracy even for longer amounts of input context, thereby enabling efficient and accurate long sequence length inference. ", "page_idx": 7}, {"type": "text", "text": "Passkey Retrieval Evaluation. We also evaluated the performance of our quantization method on passkey retrieval to assess the model\u2019s ability to use its context. Passkey retrieval involves evaluating the model\u2019s capacity to locate specific information in long texts [19], and this can be used to effectively measure the maximum distance over which a token can attend during the inference stage. We used the passkey evaluation framework from [45] (which is based on the methodology from [28]) to evaluate retrieval performance. The passkey retrieval results are provided in Table 2, demonstrating how our method is able to maintain the retrieval performance for long context length models. We also include comparisons with the passkey retrieval when using KIVI for the LLaMA-2-7B-32K model [26], demonstrating that our approach can attain higher retrieval rate for the same compression level. Our improved performance on retrieval tasks can be attributed to our improved representation of all tokens equally. This approach differs from KIVI, which preserves a local window of residual tokens in fp16. Therefore, while KIVI is effective at representing the tail part of the context, it may provide less benefit for tasks requiring the utilization of the full context window. ", "page_idx": 7}, {"type": "text", "text": "LongBench Evaluation. Table 3 shows evaluation on LongBench [3] for the LLaMA-2-7B-32K model. LongBench contains a suite of long-context length evaluation benchmarks including QA tasks, summarization, and few-shot learning [3]. The max input context length is set at 31500, and results using KIVI are also included for reference [26]. Our results demonstrate that our 3-bit ", "page_idx": 7}, {"type": "text", "text": "Table 3: LongBench evaluation for the LLaMA-2-7B-32K model using KVQuant-3bit- $I\\%$ . Comparisons with KIVI are included for reference, using the configuration with group size of 32 and 128-element fp16 residual [26]. Average bit widths are estimated for each approach assuming $I2.2K$ context length, which was the average number of tokens across all tasks. ", "page_idx": 8}, {"type": "table", "img_path": "0LXotew9Du/tmp/07bdeed60ce90ced9019923f74eab4e4fa2011d8a736a2469cce92336d643def.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "0LXotew9Du/tmp/ef2610a853eef804c4cc432745077d5538626bc3bed3c3ca97f49157c1774b84.jpg", "table_caption": ["Table 4: RULER evaluation results for the LLaMA-2-7B-32K model with KVQuant quantization methods. We report accuracy across RULER tasks, comparing our KVQuant configurations to baseline and KIVI approaches. A maximum context length of 32K is used for evaluation. Our results show that our method retains baseline accuracy even with aggressive quantization and pruning. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "model can attain minimal degradation relative to the fp16 baseline, outperforming KIVI for a similar compression level. ", "page_idx": 8}, {"type": "text", "text": "RULER Evaluation. Finally in Table 4, we provide evaluation of KVQuant and KIVI on the RULER benchmark suite [15] using LLaMA-2-7B-32K. As can be seen in the table, 3-bit KVQuant achieves $14\\%$ better score against KIVI with a similar average bit-width. Furthermore, our 2-bit KVQuant achieves similar accuracy to KIVI with $1.5\\times$ smaller bit-width. ", "page_idx": 8}, {"type": "text", "text": "4.3 Joint Weight and KV Cache Quantization ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Table 5 provides results for our KV cache quantization method when the weights are also quantized using the methodology in SqueezeLLM [17]. We observe minimal perplexity degradation when leveraging our KV cache quantization approach, even when weights are also quantized to reduced precision. In particular, we observe small 0.02 and 0.1 perplexity degradation of 4-bit and 3-bit weight-only quantization, respectively, when quantizing the KV cache using nuq4- $\\cdot1\\%$ for the LLaMA7B and LLaMA-13B models. These results demonstrate how our method is compatible with existing weight-only quantization methods. ", "page_idx": 8}, {"type": "text", "text": "4.4 Performance Analysis and Memory Savings ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Table 6 shows kernel benchmarking results using a batch size of 1 for the 4-bit dense-and-sparse compression and matrix-vector kernel implementations. We show results across different sequence lengths to assess the performance of the kernels at different points during generation. We report latency benchmarked on an A6000 GPU. The results show that for the Key and Value multiplications, we can achieve $1.2\u20131.6\\times$ and $1.3\u20131.7\\times$ latency savings, respectively, relative to the baseline. We have integrated these kernels into an end-to-end generation pipeline that is able to compress activations dynamically during inference, thereby achieving significant memory savings and allowing for either larger batch sizes or longer sequence lengths. ", "page_idx": 8}, {"type": "text", "text": "Appendix A highlights the beneftis of KVQuant in supporting longer context lengths through reducing KV cache memory footprint. As shown in Table 8 in Appendix A, our nuq2 method provides $8\\times\\,\\mathrm{KV}$ cache compression and enables serving the quantized LLaMA-7B model with a context length of 1M tokens on a single A100 GPU, as well as enabling serving the LLaMA-7B model with 10M context length on an 8-GPU system. Our results show little degradation compared to baseline fp16 inference ", "page_idx": 8}, {"type": "table", "img_path": "0LXotew9Du/tmp/3ec4755d7cba965c8ad78bcdd206c60bc5c25efaa37ce25e51b19e3462ebcfce.jpg", "table_caption": ["Table 5: KV cache quantization results when KVQuant is applied in conjunction with the weight quantization methodology in SqueezeLLM [17]. w4-s45 and w3-s45 for weights refer to the 4-bit and 3-bit dense-and-sparse weight quantization approaches in [17], respectively. See Appendix M for experimental details. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Table 6: Average latency (in microseconds) for the Key and Value nuq4- $I\\%$ kernels, benchmarked on an A6000 GPU for the LLaMA-2-7B-32K model across different sequence lengths (l). fp16 matrix-vector multiplication latencies are included for reference, and the fp16 Key multiplication time also includes the time to apply RoPE to the newly appended Key vector. Section 3.7 and Appendix R provide additional details for our kernel implementation, Appendix R describes our benchmarking methodology, and Table 22 provides a detailed breakdown of kernel runtime on an A6000 GPU. ", "page_idx": 9}, {"type": "table", "img_path": "0LXotew9Du/tmp/2c7e8bdc86331d6a58a5a072fadcc7e5b57708f77c9a2fddb44a2e8848348042.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "while providing significant compression, demonstrating the benefits of our approach for enabling accurate and efficient long sequence length inference. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "As context lengths in LLMs increase, the KV cache activations surface as the dominant contributor to memory consumption. Quantization is a promising approach to reduce the size of KV cache activations, but prior solutions failed to represent activations accurately in ultra-low precisions, such as sub-4-bit. In contrast, we achieve accurate ultra-low precision KV cache quantization. By quantizing Keys per-channel before applying RoPE, we are able to better match the outlier distribution and mitigate the impacts of RoPE on quantization (due to it mixing pairs of channels which may have different average magnitudes). We use non-uniform quantization to better allocate the small number of quantization signposts at low precision. We observe significant accuracy improvements when employing dense-and-sparse quantization, particularly when detecting outliers at the same granularity as we compute quantization scaling factors. Crucially, we demonstrate that we can perform accurate calibration offline for Keys, as well as efficient online scaling factor and outlier threshold computation for Values. By leveraging these methods, we are able to enable accurate low-precision activation quantization, achieving $4.8\\mathrm{x}$ compression $(\\mathrm{nuq}3{-}1\\%$ outliers) with only 0.1 perplexity degradation across different LLaMA, Llama-2, Llama-3, and Mistral models. Our methodology therefore supports inferring the LLaMA-7B model with a context length of 10M on an 8-GPU serving system. Through our efficient kernel implementation, we are able to show improved latency relative to the fp16 baseline, demonstrating how our method allows for improved latency in addition to the memory savings. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The authors would like to acknowledge Nicholas Lee for helpful discussions and feedback. We acknowledge gracious support from Intel, Furiosa, Apple, Samsung SAIT, and NVIDIA. We also appreciate the support from Microsoft through their Accelerating Foundation Model Research, including great support from Sean Kuno. Furthermore, we appreciate support from Google Cloud, the Google TRC team, and specifically Jonathan Caton, and Prof. David Patterson. Prof. Keutzer\u2019s lab is sponsored by the Intel corporation, Intel One-API, Intel VLAB team, the Intel One-API center of excellence, as well as funding through BDD and BAIR. We appreciate great feedback and support from Ellick Chan, Saurabh Tangri, Andres Rodriguez, and Kittur Ganesh. Sehoon Kim would like to acknowledge the support from the Korea Foundation for Advanced Studies (KFAS). Michael W. Mahoney would also like to acknowledge a J. P. Morgan Chase Faculty Research Award as well as the DOE, NSF, and ONR. Our conclusions do not necessarily reflect the position or the policy of our sponsors, and no official endorsement should be inferred. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] AI@Meta. Llama 3 model card. 2024. [2] Anthropic. Introducing claude 2.1, Nov 2023. [3] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench: A bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023. [4] Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Understanding and overcoming the challenges of efficient transformer quantization. arXiv preprint arXiv:2109.12948, 2021. [5] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023.   \n[6] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language models. arXiv preprint arXiv:2309.12307, 2023. [7] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022. [8] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314, 2023. [9] Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. Spqr: A sparse-quantized representation for near-lossless llm weight compression. arXiv preprint arXiv:2306.03078, 2023.   \n[10] Goran Flegar and Enrique S Quintana-Ort\u00ed. Balanced csr sparse matrix-vector product on graphics processors. In Euro-Par 2017: Parallel Processing: 23rd International Conference on Parallel and Distributed Computing, Santiago de Compostela, Spain, August 28\u2013September 1, 2017, Proceedings 23, pages 697\u2013709. Springer, 2017.   \n[11] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023.   \n[12] Amir Gholami, Zhewei Yao, Sehoon Kim, Michael Mahoney, and Kurt Keutzer. Ai and memory wall. RiseLab Medium Post, 2021.   \n[13] Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. Lm-infinite: Simple on-the-fly length generalization for large language models. arXiv preprint arXiv:2308.16137, 2023.   \n[14] Jung Hwan Heo, Jeonghoon Kim, Beomseok Kwon, Byeongwook Kim, Se Jung Kwon, and Dongsoo Lee. Rethinking channel dimensions to isolate outliers for low-bit weight quantization of large language models. arXiv preprint arXiv:2309.15531, 2023.   \n[15] Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris Ginsburg. Ruler: What\u2019s the real context size of your long-context language models? arXiv preprint arXiv:2404.06654, 2024.   \n[16] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.   \n[17] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization. arXiv preprint arXiv:2306.07629, 2023.   \n[18] Sehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo Kang, Ruohan Yan, Hasan Genc, Grace Dinh, Qijing Huang, Kurt Keutzer, Michael W Mahoney, et al. Full stack optimization of transformer inference: a survey. arXiv preprint arXiv:2302.14017, 2023.   \n[19] Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. How long can context length of open-source llms truly promise? In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023.   \n[20] Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen. Snapkv: Llm knows what you are looking for before generation, 2024.   \n[21] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023.   \n[22] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with blockwise ringattention, 2024.   \n[23] Ruikang Liu, Haoli Bai, Haokun Lin, Yuening Li, Han Gao, Zhengzhuo Xu, Lu Hou, Jun Yao, and Chun Yuan. Intactkv: Improving large language model quantization by keeping pivot tokens intact, 2024.   \n[24] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. Llm-qat: Data-free quantization aware training for large language models. arXiv preprint arXiv:2305.17888, 2023.   \n[25] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time. arXiv preprint arXiv:2305.17118, 2023.   \n[26] Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. Kivi: Plug-and-play 2bit kv cache quantization with streaming asymmetric quantization. 2023.   \n[27] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016.   \n[28] Amirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context length for transformers. arXiv preprint arXiv:2305.16300, 2023.   \n[29] Roman Novak, Yasaman Bahri, Daniel A Abolafia, Jeffrey Pennington, and Jascha SohlDickstein. Sensitivity and generalization in neural networks: an empirical study. arXiv preprint arXiv:1802.08760, 2018.   \n[30] OpenAI. New models and developer products announced at devday 2023, Nov 2023.   \n[31] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv e-prints, 2019.   \n[32] Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake, Carlo Luschi, and Douglas Orr. Sparq attention: Bandwidth-efficient llm inference. arXiv preprint arXiv:2312.04985, 2023.   \n[33] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally calibrated quantization for large language models. arXiv preprint arXiv:2308.13137, 2023.   \n[34] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Christopher R\u00e9, Ion Stoica, and Ce Zhang. Flexgen: High-throughput generative inference of large language models with a single gpu. In International Conference on Machine Learning, pages 31094\u201331116. PMLR, 2023.   \n[35] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.   \n[36] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.   \n[37] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[38] Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, and Xianglong Liu. Outlier suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling. arXiv preprint arXiv:2304.09145, 2023.   \n[39] Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi Zhang, Fengwei Yu, and Xianglong Liu. Outlier suppression: Pushing the limit of low-bit transformer language models. Advances in Neural Information Processing Systems, 35:17402\u201317414, 2022.   \n[40] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-theart natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38\u201345, Online, October 2020. Association for Computational Linguistics.   \n[41] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, pages 38087\u201338099. PMLR, 2023.   \n[42] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks, 2023.   \n[43] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher R\u00e9, Clark Barrett, et al. H _2 o: Heavy-hitter oracle for efficient generative inference of large language models. arXiv preprint arXiv:2306.14048, 2023.   \n[44] Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci. Atom: Low-bit quantization for efficient and accurate llm serving. arXiv preprint arXiv:2310.19102, 2023.   \n[45] Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li. Pose: Efficient context window extension of llms via positional skip-wise training. arXiv preprint arXiv:2309.10400, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Memory Bottlenecks for Long Context Length Inference ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Table 7 shows the model size and KV cache memory requirements for different LLaMA models with different sequence lengths. For short sequence lengths, the model weights are the primary memory bottleneck. However, for longer sequence lengths and larger batch sizes, the KV cache memory is the main bottleneck. This is particularly pronounced when the weights are already quantized to low precision. In our work, we demonstrate that we can help address the KV cache memory bottleneck through low-precision KV cache quantization. By compressing the KV cache to 2-bit precision, we can enable 1M context length inference with the LLaMA-7B model on a single A100-80GB GPU, and we can also enable 10M context length inference with the LLaMA-7B model on an 8-GPU system. ", "page_idx": 13}, {"type": "text", "text": "Table 8 shows the KV cache memory requirements for 128K, 1M, and 10M sequence lengths, with the KV cache stored in fp16 as well as 4-bit, 3-bit, and 2-bit precision with KVQuant. As one can see, our method provides $3.7\\times\\,\\mathrm{KV}$ cache compression $(\\mathrm{nuq}4{-}1\\%)$ ) and enables serving the quantized LLaMA-65B model with a context length of 32K tokens on a single A100-80GB GPU (requiring 30.3GB for the model weights compressed to 4-bit, and 46.5GB for the KV cache when compressed with nuq2- $\\cdot1\\%$ ), and our nuq2 method enables serving the LLaMA-7B model with a context length of 1M tokens on a single A100 GPU (requiring 64GB for the KV cache). Additionally, when considering an 8-GPU serving system, we enable serving the LLaMA-7B model with 10M context length (with nuq2), or the LLaMA-65B model with 1M context length (with nuq3). Our results show little degradation compared to baseline fp16 inference while providing significant compression, demonstrating the benefits of our approach for enabling accurate and efficient long sequence length inference. ", "page_idx": 13}, {"type": "table", "img_path": "0LXotew9Du/tmp/a76bf78636993161e2046ff1373799070d2fdb851df97ec8f9877b4d3b9aa319.jpg", "table_caption": ["Table 7: Model size and activation memory size estimates for different sequence lengths and batch sizes (BS) for different LLaMA models. For long sequence lengths and larger batch sizes, activation memory is the main bottleneck (particularly when weights are already quantized to low precision). By compressing the KV cache to 2-bit precision, we can enable 1M context length inference with the LLaMA-7B model on a single A100-80GB GPU, and we can also enable 10M context length inference with the LLaMA-7B model on an 8-GPU system. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "B Additional Related Works ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 Outlier-Aware LLM Quantization ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "LLMs have been known to have distinct outliers both in weights and activations [7, 9, 17]. SqueezeLLM and $\\operatorname{SpQR}$ both decompose the weight matrix into a sparse matrix containing a small portion of outliers and a dense matrix that can be accurately quantized to low precision (referred to as dense-and-sparse or sparse-quantized representation) [9, 17]. LLM.int8() [7] handled particular outlier channels separately in higher precision, and SmoothQuant [41] migrates quantization difficulty due to outlier channels to weights in order to support joint weight-activation quantization. Other works reconsidered the dimension along which we quantize in order to reduce quantization error (or else added per-channel compensation to improve quantization performance) [4, 14, 39, 38]. In this work, we demonstrate that per-channel pre-RoPE Key quantization provides significant accuracy beneftis given the outlier structure in Keys, and that dense-and-sparse quantization can be efficiently applied for KV cache quantization. ", "page_idx": 13}, {"type": "table", "img_path": "0LXotew9Du/tmp/4535c7b84627c7f23bd8c493dded5056b7f98ebe58d387e53bebdfa3aca40e72.jpg", "table_caption": ["Table 8: Activation memory size estimates (GB) for 128K, 1M, and 10M sequence length (l) for different LLaMA models. By compressing the KV cache to 2-bit precision, we can enable 1M context length inference with the LLaMA-7B model on a single A100-80GB GPU, and we can also enable 10M context length inference with the LLaMA-7B model on an 8-GPU system. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "B.2 Non-uniform LLM Quantization ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Non-uniform quantization has also been applied in the context of LLMs. Non-uniform quantization allows for more flexible quantization signpost placement relative to uniform quantization methods, enabling improved accuracy for the same bit precision [17, 8]. Building on the observation that model parameters tend to be approximately normally-distributed, prior work has proposed the NormalFloat datatype [8]. SqueezeLLM [17] derived per-channel non-uniform quantization signposts using a sensitivity-weighted $\\mathbf{k}$ -means approach. In this work, we show that we can derive accurate per-layer non-uniform datatypes using a sensitivity-weighted $\\boldsymbol{\\mathrm{k}}$ -means approach with KV cache activations. ", "page_idx": 14}, {"type": "text", "text": "C RoPE Equation", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The rotation matrix for RoPE is provided in Equation 2, where c and s are cosine and sine functions, $\\theta_{i}=10000^{-2(i-1)/d}$ , $d$ is the attention head dimension, and $n$ is the current position in the sequence: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left[\\begin{array}{c c c c c}{{\\mathrm{c}}(n\\theta_{1})}&{-{\\mathrm{s}}(n\\theta_{1})}&{\\cdot\\cdot\\cdot}&{0}&{0}\\\\ {{\\mathrm{s}}(n\\theta_{1})}&{{\\mathrm{c}}(n\\theta_{1})}&{\\cdot\\cdot\\cdot}&{0}&{0}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}&{\\vdots}\\\\ {0}&{0}&{\\cdot\\cdot\\cdot}&{{\\mathrm{c}}(n\\theta_{d/2})}&{-{\\mathrm{s}}(n\\theta_{d/2})}\\\\ {0}&{0}&{\\cdot\\cdot\\cdot}&{{\\mathrm{s}}(n\\theta_{d/2})}&{{\\mathrm{c}}(n\\theta_{d/2})}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The Query vectors computed at each iteration will have RoPE applied (to obtain $\\tilde{Q}_{m}=R_{\\theta,m}^{d}*Q_{m})$ . When caching Key vectors, we therefore need to either cache $\\tilde{K}_{n}=R_{\\theta,n}^{d}*K_{n}$ , or else we need to cache $K_{n}$ and apply $R_{\\theta,n}^{d}$ on-the-fly during inference. In order to apply $R_{\\theta,n}^{d}$ efficiently on-the-fly, we leverage the element-wise formulation of RoPE rather than the matrix-multiplication formulation from Equation 2. The element-wise formulation for $R_{\\theta,n}^{d}x$ is as follows, where $\\odot$ is the elementwise multiplication operator (note that the formulation that we use matches the implementation in the Transformers library for LLaMA [40], and it is a different but equivalent formulation to the element-wise expression in [35]): ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left[\\begin{array}{c}{x_{1}}\\\\ {x_{2}}\\\\ {\\vdots}\\\\ {x_{\\frac{d}{2}}}\\\\ {x_{\\frac{d}{2}+1}}\\\\ {\\vdots}\\\\ {x_{d-1}}\\\\ {x_{d}}\\end{array}\\right]\\odot\\left[\\begin{array}{c}{\\mathrm{c}(\\theta_{1}n)}\\\\ {\\mathrm{c}(\\theta_{2}n)}\\\\ {\\vdots}\\\\ {\\mathrm{c}(\\theta_{\\frac{d}{2}n})}\\\\ {\\mathrm{c}(\\theta_{1}^{2}n)}\\\\ {\\vdots}\\\\ {\\mathrm{c}(\\theta_{\\frac{d}{2}-1}n)}\\\\ {\\mathrm{c}(\\theta_{\\frac{d}{2}}n)}\\end{array}\\right]+\\left[\\begin{array}{c}{-x_{\\frac{d}{2}+1}}\\\\ {-x_{\\frac{d}{2}+2}}\\\\ {\\vdots}\\\\ {-x_{d}}\\\\ {x_{1}}\\\\ {\\vdots}\\\\ {x_{\\frac{d}{2}-1}}\\\\ {x_{\\frac{d}{2}}}\\end{array}\\right]\\odot\\left[\\begin{array}{c}{\\mathrm{s}(\\theta_{1}n)}\\\\ {\\mathrm{s}(\\theta_{2}n)}\\\\ {\\vdots}\\\\ {\\mathrm{s}(\\theta_{\\frac{d}{2}n})}\\\\ {\\mathrm{s}(\\theta_{1}^{2}n)}\\\\ {\\vdots}\\\\ {\\mathrm{s}(\\theta_{\\frac{d}{2}-1}n)}\\\\ {\\mathrm{s}(\\theta_{\\frac{d}{2}}n)}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By leveraging this element-wise implementation, we can apply RoPE on-the-fly to the Key activations (after dequantizing the Key activations and before multiplying them with the corresponding elements in the Query vector). ", "page_idx": 15}, {"type": "text", "text": "D Derivation for Sensitivity Analysis ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "To compute the sensitivity analysis we largely follow the derivation in [29], which was originally provided to compute sensitivity of a NN based classifier but can be extended with minor modifications for quantization. ", "page_idx": 15}, {"type": "text", "text": "In particular, the sensitivity measure is based on how much the loss output of the model is perturbed. To compute this we denote activations at a layer before quantization as A, after quantization as $\\mathbf{A_{Q}}$ , quantization perturbation in activation as $\\mathbf{A}-\\mathbf{A}_{\\mathbf{Q}}$ , and the gradient of the Loss function w.r.t. activation as $\\begin{array}{r}{J(A)\\,=\\,\\frac{\\partial\\mathcal{L}}{\\partial A}(A)}\\end{array}$ . By making the assumption that the quantization perturbation of different activations follow a Gaussian distribution with zero mean, we can show that the sensitivity of an activation value is proportional to $\\mathcal{F}_{i i}\\big(A-Q(A)\\big)^{2}$ as was given in Equation 1: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\Delta\\mathbf{A}}\\left[\\vert\\boldsymbol{\\mathcal{L}}\\left(\\mathbf{A}\\right)-\\boldsymbol{\\mathcal{L}}\\left(\\mathbf{A}+\\Delta\\mathbf{A}\\right)\\vert^{2}\\right]\\approx\\mathbb{E}_{\\Delta\\mathbf{A}}\\left[\\left(\\mathbf{J}(\\mathbf{A})^{T}\\Delta\\mathbf{A}\\right)^{2}\\right]=\\mathbb{E}_{\\Delta\\mathbf{A}}\\left[\\left(\\sum_{i}J_{i}\\Delta\\boldsymbol{A}_{i}\\right)^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}_{\\Delta\\mathbf{A}}\\left[\\displaystyle\\sum_{i}J_{i}^{2}\\Delta\\boldsymbol{A}_{i}^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\sum_{i}J_{i}^{2}\\mathbb{E}_{\\Delta\\mathbf{A}}\\left[\\Delta\\boldsymbol{A}_{i}^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Here note that we first assume a first order Taylor series expansion to approximate the perturbation to the loss, and then use the zero mean assumption of the quantization perturbation to derive the second line. To approximate the $\\mathbb{E}_{\\Delta\\mathbf{A}}\\left[\\Delta A_{i}^{2}\\right]$ we use empirical evaluation of the expectation by sampling multiple different inputs and empirically computing the resulting quantization perturbation which results in Equation 1. ", "page_idx": 15}, {"type": "text", "text": "E Derivation for Quantization Error ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In our work, before applying the sensitivity-weighted $\\mathbf{K}$ -means to derive quantization signposts, we normalize each element $A_{i}$ in the flattened activation $A$ . This normalization for $A_{i}$ involves a shift by a zero-point $z_{i}$ followed by rescaling the quantization signposts by a scaling factor $s_{i}$ , where $s_{i}$ and $z_{i}$ are the scaling factor and zeropoint corresponding to element $A_{i}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\nA_{i,n o r m}=\\frac{A_{i}-z_{i}}{s_{i}},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $A_{i}$ and $A_{i,n o r m}$ are element $i$ from activation $A$ before and after normalization, respectively. We then quantize $A_{i,n o r m}$ to $Q(A_{i,n o r m})$ with quantization error $\\Delta A_{i,n o r m}$ . After we dequantized, we rescale each element by $s_{i}$ and add $z_{i}$ to get the recovered quantized activation value $\\bar{Q}(A_{i})$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\nQ(A_{i})=s_{i}\\,Q(A_{i,n o r m})+z_{i}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "As such, if there is quantization error $\\Delta A_{i,n o r m}$ in $A_{i,n o r m}$ , this will be scaled by $s_{i}$ in terms of the error in $A_{i}$ , i.e., $\\Delta A_{i}\\ =\\ s_{i}\\Delta A_{i,n o r m}$ . For activation $A$ which is normalized to $A_{n o r m}$ (with corresponding scaling factors $s_{i}$ for each element $A_{i}$ ), minimizing the sensitivity-weighted quantization error as expressed in Equation 1 gives us the following expression, which we can minimize using the normalized activations across all $N$ elements from the samples in a calibration set: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{Q(A)^{*}\\simeq\\arg\\operatorname*{min}_{Q}\\sum_{i=1}^{N}\\mathcal{F}_{i i}\\big(A_{i}-Q(A_{i})\\big)^{2}}}\\\\ &{=\\arg\\underset{Q}{\\operatorname*{min}}\\sum_{i=1}^{N}\\mathcal{F}_{i i}\\Big(s_{i}^{2}\\big(A_{i,n o r m}-Q(A_{i,n o r m})\\big)^{2}\\Big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "F Key and Value Dynamic Range ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Figure 4 shows the portion of the elements contained within difference percentages of the dynamic range for both Keys and Values. The majority of values $(\\sim99\\%)$ are contained in a small portion of the dynamic range, and a small portion of numerical outliers skew the dynamic range that must be represented. This motivates our dense-and-sparse approach which removes numerical outliers and stores them in a separate sparse matrix, thereby restricting the range that needs to be represented in the dense component. ", "page_idx": 16}, {"type": "image", "img_path": "0LXotew9Du/tmp/c1bce43cc0403f9fd811fa7deab0c509e78b11358c466e0902f69de85250723e.jpg", "img_caption": ["Figure 4: Distribution of the magnitude of elements of Key (Pre-RoPE) and Value activations for different layers of LLaMA-7B, computed on a single sample with sequence length $2K$ from the Wikitext-2 dataset. The normalized magnitude is computed by dividing by the largest magnitude value in that layer. As one can see, for both Key and Value activations, the majority of values lie in a small portion of the dynamic range, with a few numerical outliers skewing the dynamic range (and thereby reducing the fidelity when quantizing to low precision). "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "G Per-Channel Key Quantization Ablations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "As shown in Table 9, per-channel quantization for Keys and per-token quantization for Values outperforms the standard per-token quantization approach for both Keys and Values, yielding an improvement of 3.82 perplexity for the LLaMA-7B model at 3-bit precision. This demonstrates the beneftis of per-channel Key quantization to mitigate the large outlier channels in Keys. Note that for all experiments using per-channel quantization, we use an fp16 zeropoint rather than a low-precision zeropoint that is rounded to the nearest integer value. We do this since for some of the Key channels, all of the elements are positive or all of the elements are negative, meaning that the zeropoint will fall outside of the range that is representable by a low-precision integer (and rounding it to the nearest low-precision value can degrade performance). ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "Additionally, we observe that per-channel quantization for Values actually performs worse than per-token quantization. We hypothesize that this behavior is because per-channel Value quantization leads to greater error accumulation in particular output values (since the result of the attention scores multiplied by one channel of the Values will be localized to a single value in the output vector), which leads to greater quantization error at later model layers. Another concurrent work, KIVI [26], observes similar behavior for per-channel Value quantization, which they attribute to the fact that per-token Value quantization confines the error to each token. Assuming that the output is a weighted sum of only a few important tokens (as only a few attention scores are large), a perturbation in these tokens can lead to significant degradation. Per-token Value quantization therefore ensures that the quantization of unimportant tokens does not adversely impact the important tokens. ", "page_idx": 17}, {"type": "table", "img_path": "0LXotew9Du/tmp/3a3ab2c16ef209ade3b122f59f872a0e010064fd5b8b59c8d510304ad5a72027.jpg", "table_caption": ["Table 9: Ablation Study: Perplexity comparison of per-token and per-channel quantization for KV cache activations for LLaMA-7B. PT refers to per-token quantization, and PC refers to per-channel quantization. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "H Pre-RoPE Key Quantization Ablations", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "As shown in Table 10, pre-RoPE Key quantization achieves higher accuracy than post-RoPE quantization, with an improvement of 0.82 perplexity for 3-bit quantization with the LLaMA-7B model. These results show that the rotary positional embeddings make Key quantization more challenging due to mixing pairs of channels with different magnitudes. Pre-RoPE quantization thereby allows for more accurate quantization at low precision. ", "page_idx": 17}, {"type": "table", "img_path": "0LXotew9Du/tmp/e6026e08f89e1cfec085a074da20a3f09a719571bcab6ef17f688993d0eca0cc.jpg", "table_caption": ["Table 10: Ablation Study: Perplexity comparison of Pre-RoPE and post-RoPE Key quantization for LLaMA-7B (using per-channel Key quantization and per-token Value quantization). Pre-RoPE quantization leads to significant improvement (see Section 3.2 for more details). "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "I Sensitivity-Weighted Non-Uniform Quantization Ablations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Table 11 shows perplexity evaluation results across different LLaMA, Llama-2, and Mistral models on Wikitext-2 for different datatypes, including nuq3, nuq3 without using sensitivity-weighting, as well as nuq3 with sensitivity-weighting but without accounting for the per-channel scaling factors when performing k-means. We observe particularly noticeable gains relative to uniform quantization for 3- bit and 2-bit quantization, where the beneftis of non-uniform quantization are more pronounced due to the reduced precision. These results demonstrate the beneftis of our sensitivity-weighted non-uniform quantization approach relative to NormalFloat quantization [8], as we achieve consistent accuracy improvements of up to 0.33 perplexity across different models. These results also demonstrate the necessity of our sensitivity-weighting approach in order to derive performant non-uniform datatypes using a $\\mathbf{k}$ -means based approach. Additionally, we observe distinct beneftis when also accounting for the per-channel scaling factors when performing $\\mathrm{k}$ -means. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "Table 11: Ablation Study: Ablation of our sensitivity-weighted non-uniform datatype for different models on Wikitext-2. All experiments use pre-RoPE per-channel quantization for Keys and per-token quantization for Values (meaning that all configurations are the same as in KVQuant, except for the datatype). We compare against both uniform (int3) and non-uniform (nf3) [8] approaches, as well as with using \u201cunweighted\u201d $k$ -means (i.e., not sensitivity-weighted) and \u201cFisher-weighted kmeans\u201d (without accounting for per-channel scaling factors) to compute the non-uniform quantization signposts. Note that there is slight variation in average bitwidth across models due to the differing hidden dimensions. Results report perplexity with 2K/4K/8K sequence length for LLaMA, Llama-2, and Mistral, respectively. ", "page_idx": 18}, {"type": "table", "img_path": "0LXotew9Du/tmp/60cbcaf43ae29c5deda91c50765dd17609b162d359dbcfab9b2c2d8341ca03bb.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "J Per-Vector Dense-and-Sparse Quantization Ablations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Table 12 shows the performance improvements we observe when isolating a small portion of outliers and storing them in a sparse format. We provide results both with using a single per-matrix outlier threshold, as well as with applying separate outlier thresholds per-vector. In particular, we see greater improvements by employing outlier detection with a different threshold per-channel for Keys and per-token for Values. This provides additional beneftis since some values which would be considered outliers for the entire matrix are not actually outliers within a particular channel (so they are not hard to quantize). It is therefore better to directly target the outliers that will skew the quantization range for a particular channel. By removing $1\\%$ of outliers using per-vector thresholds, we can achieve an additional 0.19 reduction in perplexity for the LLaMA-7B model at 3 bits, thereby enabling 3-bit quantization with under 0.1 degradation in perplexity. ", "page_idx": 18}, {"type": "text", "text": "Table 12: Ablation Study: Perplexity comparison of different outlier isolation methods for LLaMA7B on Wikitext-2. Per-vector outlier detection allows for significant accuracy improvements relative to per-tensor outlier detection. All experiments use per-token quantization for Values and per-channel quantization for Keys (pre-RoPE). \u201cPV\u201d refers to using per-vector outlier thresholds, and \u201cPM\u201d refers to using a single per-matrix outlier threshold. ", "page_idx": 18}, {"type": "table", "img_path": "0LXotew9Du/tmp/f6a5b41fc4736c20db0076d254aa4c47822f324440e3eb9485f24305a5aab86e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "K Attention Sink-Aware Quantization Ablations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Table 13 provides perplexity with and without Attention Sink-Aware quantization across different LLaMA, Llama-2, and Llama-3 models. For all bit widths (4, 3, and 2-bit) and in both denseonly and dense-and-sparse quantization settings, Attention Sink-Aware quantization consistently yields perplexity improvement. Notably, the perplexity gain is more pronounced at lower bit widths and without sparsity. We observe particularly significant improvements for Llama-3 models, with perplexity improvements of 0.25 and $0.13~\\mathrm{PPL}$ for nuq2- $1\\%$ on Llama-3-8B and Llama-3-70B, respectively. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "table", "img_path": "0LXotew9Du/tmp/7f795a0d81f23d9724de74df0903c3d1f508b5a9a7f61d8547b049312279ebf5.jpg", "table_caption": ["Table 13: Ablation Study: Perplexity with and without Attention Sink-Aware quantization on Wikitext-2 with various models. Attention Sink-Aware quantization consistently improves perplexity across different models and bit widths, particularly with lower bit widths and without sparsity. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "L Calibration Ablations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Figure 5 outlines the accuracy and efficiency challenges which our work addresses in order to enable accurate and efficient KV cache quantization. We demonstrate that we can circumvent the challenges related to efficiently performing online per-channel scaling factor computation by instead calibrating offline for the scaling factor without hurting accuracy. Additionally, we show that we can perform per-token scaling factor and outlier threshold computation efficiently online during inference, thereby enabling accurate per-token quantization without compromising on efficiency. ", "page_idx": 19}, {"type": "image", "img_path": "0LXotew9Du/tmp/4756654181c54adf69a4c9752a3528b03487aa9d0242ba8f22de36d05a45a5c0.jpg", "img_caption": ["Challenge: need to potentially recompute Challenge: Need to compute the the scaling factor every time a new key is scaling factor for each incoming token added if we compute it online "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 5: One typically achieves better performance when the scaling factor/zero point are computed online. However, this is quite challenging to do for per-channel quantization, as these factors will not only need to be recomputed for every new Key appended to the Key cache, but also all the prior cached Keys will need to be updated. As such, we use a calibration set to compute per-channel scaling factors offline. A similar challenge exists for per-token quantization, but online calibration for this does not require updating prior cached Values. In Section 3.6 and Appendix $L,$ we discuss how we are able to efficiently compute outlier thresholds / scaling factors for per-token calibration, thereby enabling online computation. ", "page_idx": 19}, {"type": "text", "text": "Table 14 shows accuracy results when using offline calibration for computing the scaling factors for the Keys. For 3-bit quantization, we observe minor accuracy degradation when not employing outlier extraction methods. However, if we remove a small percentage of outliers, then the accuracy with offline calibration is the same as computing the scaling factors online per-channel during evaluation. This demonstrates that when incorporating outlier extraction methods, we are better able to perform offilne calibration due to reduced sensitivity to outliers (either to outliers during calibration that exaggerate the quantization range, or to outliers during evaluation that cannot be represented accurately if there weren\u2019t large outliers observed during calibration). ", "page_idx": 19}, {"type": "text", "text": "Table 15 shows the runtime for the topk operation for the LLaMA-7B model (which is required for computing outlier thresholds online). It compares the runtime of the topk operation with the runtime for the QKV projections, finding that the topk runtime is $45\\%$ of the matrix-vector operation runtime for a single projection layer. The topk operation can also be performed efficiently on the CPU, so we can actually run this operation in parallel with the subsequent linear layer matrix-vector operations on the GPU (which is possible by computing the Value projection before the Key and Query projections). This allows us to compress the activations dynamically without added runtime overhead, thereby enabling online scaling factor computation for the Value tensors. ", "page_idx": 20}, {"type": "text", "text": "Table 16 shows how both Fisher information computation and calibration (including $\\mathbf{k}\\cdot$ -means) perlayer take only a few minutes for the LLaMA-65B model on a typical server machine. Even if we perform calibration sequentially for each layer, the entire calibration process would take a maximum of 6 hours for the LLaMA-65B model at 4-bit precision. ", "page_idx": 20}, {"type": "text", "text": "Table 14: Ablation Study: Model accuracy when using offilne calibration for Keys with LLaMA-7B. When incorporating outlier detection, offline calibration for Keys is able to perform comparably with online calibration. All nuq3 experiments use per-token quantization for Values and per-channel quantization for Keys (pre-RoPE), and experiments with outliers use per-vector outlier detection. ", "page_idx": 20}, {"type": "table", "img_path": "0LXotew9Du/tmp/4722dbc468cb5eea60f712d24fa0038c246ecdb4b653d1bace99113d421f2d16.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Table 15: topk runtime on a vector of length 4096 for computing outlier thresholds when using $I\\%$ sparsity (compared with the runtime for the QKV matrix multiplications, which are $4096\\!\\times\\!4096$ by 4096 matrix-vector multiplications for the LLaMA-7B model). The runtime is reported on a system with an A6000 GPU and an Intel Xeon Gold 6126 CPU. We find that the runtime for the topk operation is only $45\\%$ of the runtime of each matvec operation. Additionally, the topk operation can be performed efficiently on the CPU; we can therefore run this operation in parallel with subsequent linear layer operations on the GPU to compress the activations dynamically without added overhead. ", "page_idx": 20}, {"type": "table", "img_path": "0LXotew9Du/tmp/162ccbd8a4cb0f24aedaa82a45a7107809003fdf5bfb16ccb1ef8d06046a5ee7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Table 16: Runtime for computing Fisher information as well as for calibration (including $k$ -means) with 16 samples for LLaMA-65B quantization. Runtime for computing Fisher information was computed on an 8-GPU A100-80GB system. Runtime for calibration (including $k$ -means) was performed on an Intel Xeon Gold 6442Y CPU, and is shown for a single layer. Note that calibration is independent for each layer, so it can be easily parallelized. ", "page_idx": 20}, {"type": "table", "img_path": "0LXotew9Du/tmp/d778b10f4a45340be765908d9a92c223dbcd3d9e68506ef8e72ae386b0a23875.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "M Additional Experimental Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "For our empirical evaluation, we use 16 calibration samples of sequence length 2K from the Wikitext2 training set (as well as the corresponding gradients) to derive the per-channel scaling factors and zero-points, and to derive the non-uniform datatypes for both Keys and Values. While we use KVQuant models that are calibrated on the Wikitext-2 dataset for all experiments, in Appendix L, we include an additional experiment that demonstrates the robustness of the calibration process to the choice of data. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "We measured perplexity on both Wikitext-2 and on C4 using a sequence length equal to the maximum context length of the model (2K for LLaMA, 4K for Llama-2, and 8K for Llama-3 and Mistral7B). For generative tasks, when processing the input prompt, the Key/Value matrix multiplications are computed using the fp16 Keys and Values, and then they are separately compressed into low precision. For baseline experiments, we use post-RoPE quantization, both since this is required from an efficiency perspective without a dedicated kernel implementation, and because it provides better accuracy when quantizing Keys per-token as shown in Appendix P. ", "page_idx": 21}, {"type": "text", "text": "We make several assumptions in order to estimate average bit widths and KV cache sizes for different approaches. We compute these estimates assuming a sequence length of 128K (unless otherwise specified). For integer quantization, we assume a low-precision integer offset and a 16-bit scaling factor, whereas for NormalFloat and NUQ we assume that the zero-point and offset are each 16-bit. For the sparse matrices, 32-bit integers are assumed for the per-token indices (since we need to support long sequence lengths), and the elements and per-element indices are assumed to be 16-bit. This means that for CSR, the rows are assumed to be 32-bit and the columns and values are assumed to be 16-bit, whereas for CSC, the columns are assumed to be 32-bit and the rows and values are assumed to be 16-bit. ", "page_idx": 21}, {"type": "text", "text": "N Comparison Between Different Datatypes and Sparsity Levels ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Table 17 shows perplexity across LLaMA, Llama-2, Llama-3, and Mistral models on Wikitext-2 using different datatypes. The results highlight that the NUQ datatype generally outperforms both uniformly quantized INT datatype and non-uniformly quantized NF datatype even after they are incorporated with grouping at the expense of increased bit-widths. This performance can be further improved by extracting $0.1\\%$ to $1.0\\%$ of outliers. Note that NUQ with a sparsity level $1.0\\%$ has a similar memory requirement as INT with a group size of 64, but achieves significant perplexity improvement across all models and bit widths. ", "page_idx": 21}, {"type": "text", "text": "O Full Perplexity Evaluation ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Tables 18 and 19 show perplexity evaluation across all LLaMA, Llama-2, Llama-3, and Mistral models on Wikitext-2 and C4, respectively. These results demonstrate the benefits of our approach for KV cache compression across different models and bit widths. ", "page_idx": 21}, {"type": "table", "img_path": "0LXotew9Du/tmp/e258fe002584eb56a1c0d31ab1c56925cb09bf65f76e42be01380f92a96fb798.jpg", "table_caption": ["Table 17: Comparison of our NUQ datatype with and without sparsity against other data types on different models using the perplexity (PPL) measured on Wikitext-2. Non-uniform quantization (\u201cnuq\u201d) results are using pre-RoPE per-channel quantization for Keys. \u201cgs64/128\u201d refers to baseline experiments using grouping with group size 64/128. Note that there is a slight variation in average bitwidth across models due to the differing hidden dimensions. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Table 18: Full evaluation of our method for different models on all LLaMA, Llama-2, Llama-3, and Mistral models using the perplexity on Wikitext-2. KVQuant results are using pre-RoPE per-channel quantization for Keys. Average bit widths assume a sequence length of 128K (ignoring context length limits for the models). Note that ATOM and FlexGen use 4-bit quantization with group sizes of 128 and 64 with uniform quantization, respectively, and we extend their methods to 3-bit and 2-bit quantization. We leverage Attention-Sink Aware quantization for all bit widths. We used post-RoPE quantization for all baseline methods since it achieves higher accuracy when quantizing Keys per-token as shown in Appendix P. ", "page_idx": 22}, {"type": "table", "img_path": "0LXotew9Du/tmp/3f8ed2e3d51d0cbe4ef89b1ab9d3b1208a95ac4a408dce404587755847871f69.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Table 19: Full evaluation of our method for different models on all LLaMA, Llama-2, Llama-3, and Mistral models using the perplexity on C4. KVQuant results are using pre-RoPE per-channel quantization for Keys. Average bit widths assume a sequence length of 128K (ignoring context length limits for the models). Note that ATOM and FlexGen use 4-bit quantization with group sizes of 128 and 64 with uniform quantization, respectively, and we extend their methods to 3-bit and 2-bit quantization. We leverage Attention-Sink Aware quantization for all bit widths. We used post-RoPE quantization for all baseline methods since it achieves higher accuracy when quantizing Keys per-token as shown in Appendix P. ", "page_idx": 23}, {"type": "table", "img_path": "0LXotew9Du/tmp/b128c15c93a9cb1fc70c97672884e43936eb5a7648324246462b226979c2e9bb.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "P Post-RoPE Per-Token Quantization Ablation", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Table 20 shows perplexity evaluation on Wikitext-2 for the LLaMA-7B model with uniform quantization, with Keys quantized pre-RoPE and post-RoPE. These results show that post-RoPE Key quantization is superior to pre-RoPE Key quantization when quantizing Keys per-token. This is because when rotating an outlier channel with large average magnitude and another channel with smaller average magnitude together, at some positions in the sequence, part of the magnitude from the outlier channel will be shifted to the smaller channel. This partially mitigates the impact of the outlier channel on skewing the quantization range for some of the tokens in the sequence. As such, for our baseline comparisons, we use post-RoPE per-token Key quantization to serve as a stronger baseline. ", "page_idx": 24}, {"type": "table", "img_path": "0LXotew9Du/tmp/e7c2b0d506e4cc1e99b0abf7735fd4a02c0e0476b42a26458c3f4d175c9213a8.jpg", "table_caption": ["Table 20: Model accuracy when using pre-RoPE and post-RoPE quantization for LLaMA-7B with per-token Key quantization. Our experiments demonstrate that post-RoPE quantization is superior when using per-token Key quantization. Therefore, we decided to use these results for baseline comparison with per-token quantization. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "Q Experiments on Calibration Data Robustness ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "To evaluate the robustness of our quantization method to the choice of calibration datasets, we measure the perplexity on Wikitext-2 and C4 using different quantized models calibrated with Wikitext-2 and C4. As shown in Table 21, the resulting perplexity numbers remain similar even when the models are calibrated with out-of-domain examples (e.g., calibrated with Wikitext-2 and evaluated on C4, and vice versa), demonstrating the robustness of the calibration process to the choice of data. ", "page_idx": 24}, {"type": "table", "img_path": "0LXotew9Du/tmp/2665c13ac03919f34e30c4ba677c04ca773486d5612f508abef79465fb4868bb.jpg", "table_caption": ["Table 21: Perplexity (PPL) results on Wikitext-2 and $C4$ using different quantization schemes, calibrated using Wikitext-2 and $C4.$ . "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "R Kernel Implementation Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We implemented 4-bit lookup table-based kernels for matrix-vector multiplication between the Key or Value activations (packed as a lookup table (LUT) plus indices into the LUT per-element) and a full-precision activation vector. These kernels load the compressed Key and Value activations and dequantize them only as needed in order to minimize memory bandwidth utilization. All arithmetic is performed in fp16. The lookup table entries are the values of the sensitivity-weighted non-uniform datatype for that particular layer scaled according to the range of activations that need to be represented [8]. ", "page_idx": 24}, {"type": "text", "text": "When selecting between the Compressed-Sparse Column (CSC format) and the Compressed-Sparse Row (CSR) format for storing the outliers for the Keys and Values, we needed to consider how easy it would be to append new vectors. When using CSC format for the Key matrix, we only need to append a single element to the column vector, as well as one new element to the row and value vectors per nonzero element in that new column. If we used CSR format, we would need to insert the new column and value elements in the middle of the existing column and value vectors, and we would need to recompute the elements of the row vector. When using CSR format for the Value matrix, we only need to append a single element to the row vector, as well as one new element to the column and value vectors per nonzero element in that new row. If we used CSC format, we would need to insert the new row and value elements in the middle of the existing row and value vectors, and we would need to recompute the elements of the column vector. We therefore used the CSC format for the Key matrices and the CSR format for the Value matrices. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "One challenge with efficiently processing the sparse matrix-dense vector operation is that the sparsity distribution may be unbalanced. This poses a challenge for efficiently processing the sparse matrix on a GPU as there can be different numbers of nonzeros to process per thread. We therefore leverage a balanced sparse matrix-dense vector kernel based on [10, 17], which assigns an equal number of nonzeros per thread. This has greater synchronization overhead than assigning a single thread for an entire row or column when processing CSR/CSC matrices, but it leads to a more balanced work assignment between threads. We set the number of threads such that there were 10 nonzero values assigned to each thread. The dense non-uniform kernel and balanced sparse kernels are launched in one call to avoid overhead from summing the output vectors from these separate operations. ", "page_idx": 25}, {"type": "text", "text": "A second potential challenge is that the quantization dimension for the Keys and Values is not aligned with the reduction dimension for the respective matrix-vector operations. If we used per-channel or per-token lookup tables for dequantization, we would need to load from a different lookup table for each element as we iterate along the reduction dimension. However, since our implementation uses a single per-layer datatype that is rescaled per-vector, we can load a single scaling factor and zero-point for each channel / token and broadcast it across all threads (while doing a table lookup from the shared per-layer datatype, which is duplicated such that each thread can access it efficiently in parallel). The use of a single per-layer datatype that is rescaled per-vector therefore allows for much more efficient GPU kernel implementations in the context of per-channel Key / per-token Value quantization. ", "page_idx": 25}, {"type": "text", "text": "Table 22 shows a detailed breakdown of kernel runtime, including how much time is spent packing vectors into the compressed format and how much time is spent on the dense and sparse matrixvector multiplications. For the fp16 baselines, we benchmarked runtime by averaging across 1000 iterations. When benchmarking our dense-and-sparse kernels (both for packing and matrix-vector multiplication), since the runtime of the sparse kernels can vary based on the sparsity pattern, we ran the LLaMA-2-7B-32K model and collected activations at each layer, and we then measured the average runtime across 1000 iterations for each layer in the model. We find that even with $1\\%$ sparsity, we can attain significant speedups of up to $1.7\\times$ relative to the fp16 matrix-vector multiply kernels, demonstrating how our methodology facilitates efficient inference with a low-precision quantized KV cache. ", "page_idx": 25}, {"type": "text", "text": "Table 22: Average latency (in microseconds) for the Key and Value dense nuq4 kernels as well as for the sparse kernels (with $I\\%$ outliers), benchmarked on an A6000 GPU for the LLaMA-2-7B-32K model. Benchmarking results are reported for different sequence lengths (l). fp16 matrix-vector multiplication latencies are included for reference, and the Key multiplication time also includes the time to apply RoPE to the newly appended Key vector. We find that our dense-and-sparse approach (even with $I\\%$ outliers) provides latency beneftis relative to the fp16 baseline, even when accounting for the time to compress activations online. ", "page_idx": 25}, {"type": "table", "img_path": "0LXotew9Du/tmp/06aab34d2a8ab55fccce8f836930959b382fd9d737bcbdc05bf4c59a3ae434dd.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "S Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "While our work enables accurate long-context length inference by reducing the memory requirements, there is significant work required for training long context length models with greater than 100K context length. This work is orthogonal to our efforts, which are constrained to efficient inference with long context length models. Additionally, our latency benchmarking results currently focus on memory-bandwidth bound generation rather than prompt processing (where we need to compress multiple Keys and Values at once). Finally, in the current end-to-end implementation, there are inefficiencies in how memory allocation is handled for updating the sparse matrix (where the data corresponding to the previous tokens have to be copied when concatenating them with the data from the new token). In future work, we plan to optimize this by doing blocked allocation to avoid overheads from reallocating memory. ", "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: The main claims in the abstract and introduction (around accuracy improvements in terms of perplexity reduction across different models, as well as in terms of efficiency improvements) are justified in the evaluation in Section 4 and Section 4.4, respectively. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: Appendix S discusses limitations for our work. The computational efficiency of our quantization method is also discussed in Appendix L. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: For the derivation in Section 3.3, full derivations are provided in Appendices D and E. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: Experimental details are described in Appendix M, and a link to the anonymized code is provided. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We provide a link to the anonymized code, and all datasets and models used are publicly available. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Details for reproducing the main evaluation in the paper are provided in Appendix M. Full kernel benchmarking details are provided in Appendix R. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [No] ", "page_idx": 29}, {"type": "text", "text": "Justification: The runs for the experiments in the paper have low variance. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 29}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 30}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We provide explanation of the computational efficiency for our quantization method in discussed in Appendix L. We also discuss the required GPU memory for supporting different context lengths with and without our KV cache quantization approach in Appendix A. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We do not foresee any ethical consequences for this work. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: Our work is focused on efficient inference with existing Large Language Models. We do not foresee any particular societal impacts from this work. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 31}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: Our work is focused on efficient inference with existing Large Language Models. The models and datasets used in this work were already publicly available, and the methods developed in this work do not require specific safeguards. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: All datasets and models used in this paper are open-source and publicly available. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We provide a link to the anonymized code, which has documentation provided that explains how to run the experiments. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: We did not conduct any crowdsourcing experiments or research with human subjects. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: We did not conduct any research with human subjects. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 33}]