[{"figure_path": "0LXotew9Du/figures/figures_1_1.jpg", "caption": "Figure 1: Left: Model size versus activation memory size for the LLaMA-7B model with sequence length 512 and 128K. For longer context lengths, the KV cache becomes the dominant memory bottleneck. Memory consumption of model weights and KV cache activations for different LLaMA models with different sequence lengths are provided in Table 7 in Appendix A. Right: Overview of the different components used in KVQuant that result in less than 0.1 perplexity degradation over the fp16 baseline when quantizing the KV cache for the LLaMA-7B model to 3-bit precision. As shown in Table 1, our 3-bit approach results in 4.8\u00d7 reduction in cached activation memory footprint.", "description": "The figure shows a comparison of memory usage between weights and KV cache activations for the LLaMA-7B model at different sequence lengths.  It highlights that while weights dominate memory at short sequence lengths, the KV cache becomes the dominant memory bottleneck as sequence length increases. The right-hand side presents an overview of the KVQuant method's components and demonstrates the perplexity improvement achieved by reducing the KV cache memory footprint through 3-bit quantization.", "section": "1 Introduction"}, {"figure_path": "0LXotew9Du/figures/figures_3_1.jpg", "caption": "Figure 2: Example distributions of the activation values for Keys pre-RoPE, Keys post-RoPE, and Values for LLaMA-7B on a sample with 2K sequence length from Wikitext-2. We observe several patterns: (i) Keys pre-RoPE exhibit clear outliers in specific channels across different tokens; (ii) after applying RoPE, the distribution becomes less structured and there are less consistent magnitudes for outlier channels (this is expected, as RoPE applies a rotation operation between pairs of channels); and (iii) Values exhibit no fixed outlier pattern with outlier values across channels and tokens.", "description": "This figure shows example distributions of activation values for Keys (before and after applying Rotary Positional Embedding (RoPE)), and Values in layer 10 of the LLaMA-7B model using a sample of 2000 tokens from the Wikitext-2 dataset.  The key observation is the different outlier patterns in Keys: before RoPE, distinct outliers are present in specific channels; after RoPE, this pattern is less structured.  Values, however, show no consistent outlier pattern across channels and tokens. This analysis is crucial to the paper's approach of quantizing Keys before RoPE to improve accuracy in low-precision quantization.", "section": "3 Method"}, {"figure_path": "0LXotew9Du/figures/figures_7_1.jpg", "caption": "Figure 3: Perplexity results for the LLaMA-2-7B-32K model [5] as well as the Llama-2-70B-32K LongLoRA model [6] on the Wikitext-2 dataset, evaluated using different sequence lengths.", "description": "This figure shows the perplexity results on the Wikitext-2 dataset for two different LLMs, LLaMA-2-7B-32K and Llama-2-70B-32K, using various sequence lengths.  The perplexity, a measure of how well the model predicts the next word, is plotted against the evaluation sequence length.  Different quantization methods (fp16, nuq4-1%, nuq3-1%, nuq2-1%) are compared, demonstrating the impact of quantization on model performance with increasing context length. The right-hand side plots show the memory usage (in GB) of the KV cache for each quantization method.", "section": "4.2 Long Context Length Evaluation"}, {"figure_path": "0LXotew9Du/figures/figures_16_1.jpg", "caption": "Figure 4: Distribution of the magnitude of elements of Key (Pre-RoPE) and Value activations for different layers of LLaMA-7B, computed on a single sample with sequence length 2K from the Wikitext-2 dataset. The normalized magnitude is computed by dividing by the largest magnitude value in that layer. As one can see, for both Key and Value activations, the majority of values lie in a small portion of the dynamic range, with a few numerical outliers skewing the dynamic range (and thereby reducing the fidelity when quantizing to low precision).", "description": "This figure shows the distribution of the magnitude of elements for both Keys (before Rotary Positional Embedding) and Values in different layers of the LLaMA-7B model. The data is from a single sample with a sequence length of 2K from the Wikitext-2 dataset. The y-axis represents the normalized magnitude, calculated by dividing each element's magnitude by the maximum magnitude in its layer.  The x-axis indicates the layer number. The figure highlights that for both Keys and Values, most elements are concentrated in a small portion of the dynamic range. A few outlier elements significantly skew the range, making it challenging to quantize the data to low precision accurately.  The different colors represent different percentage thresholds (e.g., t99 shows the portion of elements within the top 1% of magnitudes).", "section": "3 Method"}, {"figure_path": "0LXotew9Du/figures/figures_19_1.jpg", "caption": "Figure 5: One typically achieves better performance when the scaling factor/zero point are computed online. However, this is quite challenging to do for per-channel quantization, as these factors will not only need to be recomputed for every new Key appended to the Key cache, but also all the prior cached Keys will need to be updated. As such, we use a calibration set to compute per-channel scaling factors offline. A similar challenge exists for per-token quantization, but online calibration for this does not require updating prior cached Values. In Section 3.6 and Appendix L, we discuss how we are able to efficiently compute outlier thresholds / scaling factors for per-token calibration, thereby enabling online computation.", "description": "This figure illustrates the challenges of online vs. offline computation for scaling factors in per-channel and per-token quantization.  Per-channel requires recomputing factors for each new key, while per-token only needs computation for the new token.  The authors choose offline calibration for per-channel to avoid this online overhead, and they use online calibration for per-token.", "section": "3.6 Offline Calibration versus Online Computation"}]