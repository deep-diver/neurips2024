[{"figure_path": "v4dXL3LsGX/figures/figures_2_1.jpg", "caption": "Figure 1: We show the latent space covered by different methods. For either simulated data or human data, the generative agents produced by GAMMA can cover a larger strategy space. Generative models can provide novel agents by interpolating the agents in the simulated population (a). On human data (b), the human proxy model only covers a subset of all human player behavior patterns, while the generative model can capture the diversity in the data. We can also control the latent space sampling (c) to model a target population of agents (e.g., human coordinators).", "description": "This figure displays the latent space visualization using t-SNE for three different scenarios: simulated data, human data, and human-adaptive sampling. The results show that the generative model (GAMMA) can generate a more diverse range of partner strategies than either simulated agents or a behavior cloning approach based on human data.  In the human-adaptive sampling scenario, the generative model is steered towards generating agents similar to human players. This highlights the ability of GAMMA to cover a broader strategy space and to adapt to real-world human strategies.", "section": "4 GAMMA: Generative Agent Modeling for Multi-agent Adaptation"}, {"figure_path": "v4dXL3LsGX/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of the method for GAMMA. The generative model learns a latent distribution over partner strategies from either simulated or human data. Sampling partners from the generative model enables training a robust Cooperator that can coordinate with a variety of different humans.", "description": "This figure illustrates the GAMMA (Generative Agent Modeling for Multi-agent Adaptation) method.  The left side shows how the generative model learns a latent distribution of partner strategies using both simulated agent data and real human interaction data. The latent representation, Z, encodes partner styles. The right side depicts how the trained generative model produces diverse partner agents (\u00b5z) that a Cooperator agent (\u03c0c) then learns to coordinate with using reinforcement learning (RL). The iterative interaction between the generative agents and the Cooperator enables robust, zero-shot cooperation with novel human partners.", "section": "4 GAMMA: Generative Agent Modeling for Multi-agent Adaptation"}, {"figure_path": "v4dXL3LsGX/figures/figures_6_1.jpg", "caption": "Figure 3: The first five layouts Cramped Room, Asymmetric Advantages, Coordination Ring, Forced Coordination, Counter Circuit are originally proposed in Carroll et al. [1]. We create an additional Multi-strategy Counter layout. In this new layout, humans can additionally choose between making onion vs. tomato soup, which makes coordination significantly more challenging.", "description": "This figure shows six different Overcooked game layouts used in the experiments.  The first five are from previous work and are progressively more complex. The sixth layout, \"Multi-strategy Counter\", is a new, more complex layout introduced by the authors, adding additional strategic choices (onion vs. tomato soup) to increase the difficulty and diversity of the possible strategies.", "section": "5 Experiments"}, {"figure_path": "v4dXL3LsGX/figures/figures_7_1.jpg", "caption": "Figure 4: Evaluation of different methods using a human proxy model. Rewards are normalized by the highest reward achieved on each layout. The learning curves in (a) show the average normalized reward across all environments, indicating that GAMMA helps the Cooperator converge to a higher reward. This improvement is also consistent across individual layouts, as illustrated in (b) and (c). We observe the largest performance gap on the \u2018Counter Circuit\u2019 and \u2018Multi-Strategy Counter\u2019 layouts, which are the most complex in terms of the number of valid cooperation strategies.", "description": "This figure compares the performance of different methods for training a cooperative agent in the Overcooked game.  It shows learning curves (average normalized reward over training steps across all layouts), and a comparison of final normalized and original rewards across individual game layouts for different methods.  The key takeaway is that using a generative model (GAMMA) consistently improves the performance of the cooperative agent, especially on more complex layouts.", "section": "6 Results"}, {"figure_path": "v4dXL3LsGX/figures/figures_8_1.jpg", "caption": "Figure 5: Performance of different agents when played with real humans. Error bars [4] use the Standard Error of the Mean (SE) for statistical significance (p < 0.05). Methods trained on human data are shown in green. Whether training with simulated or human data, GAMMA shows consistent, statistically significant advantages over the baselines. GAMMA-HA is able to efficiently use the real human dataset to learn a better sampling of its latent space, achieving the best performance when cooperating with real humans.", "description": "This figure displays the average scores achieved by different cooperative agents when playing against real human partners in two Overcooked layouts: Counter Circuit and Multi-strategy Counter.  Error bars represent the standard error, indicating statistical significance.  Agents trained using human data are highlighted in green.  The results demonstrate that GAMMA consistently outperforms baseline methods, regardless of whether the training data is simulated or human. GAMMA with human-adaptive sampling (GAMMA-HA) shows the best performance, effectively leveraging a small amount of human data to achieve superior cooperation with real humans.", "section": "6 Results"}, {"figure_path": "v4dXL3LsGX/figures/figures_9_1.jpg", "caption": "Figure 1: We show the latent space covered by different methods. For either simulated data or human data, the generative agents produced by GAMMA can cover a larger strategy space. Generative models can provide novel agents by interpolating the agents in the simulated population (a). On human data (b), the human proxy model only covers a subset of all human player behavior patterns, while the generative model can capture the diversity in the data. We can also control the latent space sampling (c) to model a target population of agents (e.g., human coordinators).", "description": "This figure compares the latent space covered by different methods for generating agents in a cooperative game.  Panel (a) shows that using GAMMA with simulated data produces a larger and more diverse strategy space compared to using the simulated agents directly. Panel (b) demonstrates that GAMMA trained on human data captures a wider range of strategies than a human proxy model. Panel (c) highlights the ability of GAMMA to control the latent space sampling and generate agents with a specific strategy profile, such as human coordinators.", "section": "4 GAMMA: Generative Agent Modeling for Multi-agent Adaptation"}, {"figure_path": "v4dXL3LsGX/figures/figures_14_1.jpg", "caption": "Figure 1: We show the latent space covered by different methods. For either simulated data or human data, the generative agents produced by GAMMA can cover a larger strategy space. Generative models can provide novel agents by interpolating the agents in the simulated population (a). On human data (b), the human proxy model only covers a subset of all human player behavior patterns, while the generative model can capture the diversity in the data. We can also control the latent space sampling (c) to model a target population of agents (e.g., human coordinators).", "description": "This figure visualizes the latent space generated by different methods for training cooperative agents.  It compares the latent space coverage of simulated agents, human players, behavior cloning, and generative agents trained on simulated or human data. Panel (a) shows interpolation capabilities of the generative model using simulated data, (b) compares generative model coverage to human proxy model coverage using human data, and (c) demonstrates human-adaptive sampling using generative model.", "section": "2 Related Work"}, {"figure_path": "v4dXL3LsGX/figures/figures_15_1.jpg", "caption": "Figure 8: Human performance improves with the number of trials, indicating that the humans learn, change, and adapt their gameplay during the course of the evaluation.", "description": "This figure shows the average final score achieved by human participants in the Overcooked game across eight gameplay trials. The scores steadily increase from trial one to eight, illustrating that humans continuously adapt their strategies and improve their performance as they gain more experience with the game and its challenges.", "section": "G Additional Human Study Results"}, {"figure_path": "v4dXL3LsGX/figures/figures_17_1.jpg", "caption": "Figure 1: We show the latent space covered by different methods. For either simulated data or human data, the generative agents produced by GAMMA can cover a larger strategy space. Generative models can provide novel agents by interpolating the agents in the simulated population (a). On human data (b), the human proxy model only covers a subset of all human player behavior patterns, while the generative model can capture the diversity in the data. We can also control the latent space sampling (c) to model a target population of agents (e.g., human coordinators).", "description": "This figure shows the latent space visualization of different methods for generating agents' strategies.  (a) demonstrates how GAMMA, using a generative model trained on simulated data, can generate a wider range of strategies compared to simply using the simulated agents directly. (b) shows that, even with human data, GAMMA produces a more diverse range of strategies than a human proxy model based on behavior cloning.  Finally, (c) highlights GAMMA's ability to control the latent space sampling, allowing for targeting a specific population of agents (e.g., focusing on human-like strategies).", "section": "4 GAMMA: Generative Agent Modeling for Multi-agent Adaptation"}, {"figure_path": "v4dXL3LsGX/figures/figures_17_2.jpg", "caption": "Figure 1: We show the latent space covered by different methods. For either simulated data or human data, the generative agents produced by GAMMA can cover a larger strategy space. Generative models can provide novel agents by interpolating the agents in the simulated population (a). On human data (b), the human proxy model only covers a subset of all human player behavior patterns, while the generative model can capture the diversity in the data. We can also control the latent space sampling (c) to model a target population of agents (e.g., human coordinators).", "description": "This figure visualizes the latent space of different methods for generating agents in a cooperative game.  Panel (a) shows how GAMMA, using simulated data, generates agents that span a wider range of strategies than simply using the simulated agents alone. Panel (b) demonstrates that GAMMA, using human data, captures a greater diversity of strategies than a human proxy model.  Panel (c) highlights GAMMA's ability to control the sampling process to focus on a specific type of agent, such as human coordinators.", "section": "4 GAMMA: Generative Agent Modeling for Multi-agent Adaptation"}, {"figure_path": "v4dXL3LsGX/figures/figures_17_3.jpg", "caption": "Figure 1: We show the latent space covered by different methods. For either simulated data or human data, the generative agents produced by GAMMA can cover a larger strategy space. Generative models can provide novel agents by interpolating the agents in the simulated population (a). On human data (b), the human proxy model only covers a subset of all human player behavior patterns, while the generative model can capture the diversity in the data. We can also control the latent space sampling (c) to model a target population of agents (e.g., human coordinators).", "description": "This figure compares the latent space coverage of different methods for generating agents in a cooperative game.  (a) shows that using a generative model with simulated data expands the strategy space beyond the original simulated agents. (b) shows that the generative model on human data captures more diversity than a simple behavior cloning approach. (c) demonstrates the ability to control sampling to target specific agent characteristics (e.g., human-like coordination strategies).", "section": "4 GAMMA: Generative Agent Modeling for Multi-agent Adaptation"}, {"figure_path": "v4dXL3LsGX/figures/figures_17_4.jpg", "caption": "Figure 1: We show the latent space covered by different methods. For either simulated data or human data, the generative agents produced by GAMMA can cover a larger strategy space. Generative models can provide novel agents by interpolating the agents in the simulated population (a). On human data (b), the human proxy model only covers a subset of all human player behavior patterns, while the generative model can capture the diversity in the data. We can also control the latent space sampling (c) to model a target population of agents (e.g., human coordinators).", "description": "This figure shows the latent space visualization of different methods for generating agents' strategies in the Overcooked game.  Panel (a) demonstrates how GAMMA, using simulated data, generates a more diverse range of strategies than the original simulated agent population. Panel (b) compares GAMMA's performance on human data to a behavior cloning approach, highlighting GAMMA's ability to capture the diversity of human strategies. Finally, Panel (c) shows how GAMMA allows for targeted agent generation by controlling the sampling process within the latent space, aiming for human-like coordination.", "section": "4 GAMMA: Generative Agent Modeling for Multi-agent Adaptation"}, {"figure_path": "v4dXL3LsGX/figures/figures_18_1.jpg", "caption": "Figure 12: Learning curves for methods using simulated data across six layouts. Error bars are the Standard Error of the Mean (SE). All methods are evaluated using a held-out human proxy model as the partner player. GAMMA consistently shows better or equal performance on all layouts for both simulated data-generation methods (FCP, CoMeDi, MEP) when evaluated against the human-proxy model.", "description": "This figure compares the performance of different multi-agent reinforcement learning methods on six different Overcooked game layouts. The methods compared are FCP, FCP + GAMMA, CoMeDi, CoMeDi + GAMMA, MEP, and MEP + GAMMA.  The y-axis represents the reward obtained, while the x-axis represents the number of steps in training. Error bars indicate the standard error of the mean.  GAMMA consistently outperforms or matches the performance of the baselines across all six layouts, suggesting that the generative agent model is effective at training more robust cooperative agents.", "section": "5 Experiments"}, {"figure_path": "v4dXL3LsGX/figures/figures_18_2.jpg", "caption": "Figure 13: Learning curves for methods using human data. When evaluated with a held-out human proxy agent (a), human adaptive sampling learns faster on Counter Circuit, but does not reach better final performance since PPO-BC is trained to exploit a human-proxy agent. With simulated self-play partners (b), Human-Adaptive GAMMA with DFT shows better performance.", "description": "This figure shows the learning curves for different methods using human data for two layouts: Counter Circuit and Multi-strategy Counter.  The left panel (a) shows evaluation results against a held-out human proxy agent.  The human adaptive sampling method (GAMMA-HA) learns faster but doesn't outperform the baseline method (PPO + BC) which is trained to specifically exploit the human proxy agent. In contrast, the right panel (b) shows an evaluation against held-out self-play agents where GAMMA-HA with decoder-only fine-tuning (DFT) performs significantly better.  This highlights the ability of GAMMA-HA to generalize well to diverse, unseen human players.", "section": "6.2.2 H2: Training with Human Data"}, {"figure_path": "v4dXL3LsGX/figures/figures_19_1.jpg", "caption": "Figure 14: With larger KL Divergence penalty coefficient (\u03b2 in Eq [1]), the new full-model fine-tuning (FFT) largely improves the original FFT and achieves a comparable performance with the original best decoder-only fine-tuning (DFT) method.", "description": "This figure shows the learning curves for three different methods of fine-tuning a generative model using human data: the original best decoder-only fine-tuning (DFT), the original full fine-tuning (FFT), and a new full fine-tuning method (FFT) with a larger KL divergence penalty coefficient. The new FFT method significantly improves upon the original FFT, achieving performance comparable to the original best DFT method. This suggests that increasing the KL divergence penalty coefficient can help to mitigate the issue of insufficient human data when fine-tuning generative models.", "section": "4.3 Targeted GAMMA using Human-Adaptive Sampling and Fine-tuning"}, {"figure_path": "v4dXL3LsGX/figures/figures_20_1.jpg", "caption": "Figure 15: Performance of z-conditioned Cooperator. The z-conditioned Cooperator reaches a higher reward in the Multi-strategy Counter. The performance decreases after the peak since the z-conditioned policy overfits the encoder.", "description": "This figure shows the learning curves of two different cooperator agents trained on the Overcooked game. One is a standard cooperator and the other is a *z-conditioned* cooperator, meaning its policy is conditioned on a latent variable representing the partner's strategy. The z-conditioned agent initially performs better, especially on the more complex 'Multi-strategy Counter' layout. However, its performance eventually decreases due to overfitting of the latent variable. This suggests a trade-off between quickly adapting to a specific partner and generalizing well to unseen partners.", "section": "Additional simulated results"}]