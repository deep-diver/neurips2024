[{"figure_path": "y6JotynERr/tables/tables_8_1.jpg", "caption": "Table 1: Performance Results for CV task on CIFAR-10 and CIFAR-100. Training data is distributed among S, M, and L device prototypes in a 1:3:6 ratio, subdivided among clients using Dirichlet distribution. Public datasets are CIFAR-100 for CIFAR-10 and ImageNet-100 for CIFAR-100. Client configurations include 100, 20, and 4 clients for S, M, and L, with sampling rates of 0.1, 0.2, and 0.5. Architectures are ResNet-8, ResNet-14, and ResNet-18 for S, M and L, respectively. All models are trained from scratch for 60 rounds. See Appendix D.1 for additional experiments using hetero-family architecture and more details.", "description": "This table presents the performance of different federated learning methods (FedAvg, FedDF, FedET, TAKFL, and TAKFL+Reg) on CIFAR-10 and CIFAR-100 datasets for image classification.  The results are broken down by device prototype size (Small, Medium, Large) and data heterogeneity (low and high).  It shows the top-1 accuracy achieved by each method under different experimental conditions.", "section": "7.2 Main Experimental Results"}, {"figure_path": "y6JotynERr/tables/tables_8_2.jpg", "caption": "Table 2: Performance Results for NLP Task on MNLI and SST-2. Training data distribution is similar to the CV task using only Dir(0.5) here. Public datasets are SNLI [2] for MNLI [52] and Sentiment140 [14] for SST-2 [45]. Client configurations are 8, 4, and 2 clients for S, M, and L, with sample rates of 0.3, 0.5, and 1.0, respectively. Architectures include Bert-Tiny, Bert-Mini, and Bert-Small [47] for S, M, and L, initialized from pre-trained parameters and fine-tuned for 20 communication rounds. See Appendix F.2 for more details.", "description": "This table presents the performance of different federated learning methods on two natural language processing tasks: MNLI and SST-2.  It shows the accuracy achieved by different methods (FedAvg, FedDF, FedET, TAKFL) on three different device prototypes (small, medium, large), which vary in model and data size. The results are averaged over three independent runs.", "section": "7.2 Main Experimental Results"}, {"figure_path": "y6JotynERr/tables/tables_16_1.jpg", "caption": "Table 1: Performance Results for CV task on CIFAR-10 and CIFAR-100. Training data is distributed among S, M, and L device prototypes in a 1:3:6 ratio, subdivided among clients using Dirichlet distribution. Public datasets are CIFAR-100 for CIFAR-10 and ImageNet-100 for CIFAR-100. Client configurations include 100, 20, and 4 clients for S, M, and L, with sampling rates of 0.1, 0.2, and 0.5. Architectures are ResNet-8, ResNet-14, and ResNet-18 for S, M and L, respectively. All models are trained from scratch for 60 rounds. See Appendix D.1 for additional experiments using hetero-family architecture and more details.", "description": "This table presents the performance results for computer vision (CV) tasks using CIFAR-10 and CIFAR-100 datasets.  It shows the top-1 accuracy achieved by different federated learning methods (FedAvg, FedDF, FedET, TAKFL) across three device prototype sizes (small, medium, large) under different levels of data heterogeneity.  The table includes results for both standard and self-regularized TAKFL, showcasing the improvement in accuracy resulting from the addition of the self-regularization component. Note that the training data is non-i.i.d. and the details of model architectures and training parameters are also provided.", "section": "Main Experimental Results"}, {"figure_path": "y6JotynERr/tables/tables_17_1.jpg", "caption": "Table 1: Performance Results for CV task on CIFAR-10 and CIFAR-100. Training data is distributed among S, M, and L device prototypes in a 1:3:6 ratio, subdivided among clients using Dirichlet distribution. Public datasets are CIFAR-100 for CIFAR-10 and ImageNet-100 for CIFAR-100. Client configurations include 100, 20, and 4 clients for S, M, and L, with sampling rates of 0.1, 0.2, and 0.5. Architectures are ResNet-8, ResNet-14, and ResNet-18 for S, M and L, respectively. All models are trained from scratch for 60 rounds. See Appendix D.1 for additional experiments using hetero-family architecture and more details.", "description": "This table presents the performance comparison of different federated learning methods on CIFAR-10 and CIFAR-100 datasets for image classification task.  The methods are compared across three different device prototype sizes (small, medium, and large), each having a different model and dataset size.  The table shows the top-1 accuracy results and considers different data heterogeneity levels using a Dirichlet distribution.  Additional experimental results using a different model architecture are provided in Appendix D.1.", "section": "7.2 Main Experimental Results"}, {"figure_path": "y6JotynERr/tables/tables_24_1.jpg", "caption": "Table 4: Performance Results for CV task on CIFAR-10 and CIFAR-100. Training data is distributed among S, M, and L device prototypes in a 1:3:6 ratio, subdivided among clients using Dirichlet distribution. Public datasets are CIFAR-100 for CIFAR-10 and ImageNet-100 for CIFAR-100. Client configurations include 100, 20, and 4 clients for S, M, and L, with sampling rates of 0.1, 0.2, and 0.5. In homo-family settings, architectures are ResNet8, ResNet14, and ResNet18; in hetero-family settings, they are ViT-S, ResNet14, and VGG-16. All models are trained from scratch for 60 rounds. See Appendix F.1 for more details.", "description": "This table presents the performance results of different federated learning methods (FedAvg, FedDF, FedET, TAKFL, and TAKFL+Reg) on CIFAR-10 and CIFAR-100 datasets. The results are categorized by data heterogeneity (low and high) and architecture setting (homo-family and hetero-family). For each setting, the table shows the average top-1 accuracy and standard deviation across three independent runs for small (S), medium (M), and large (L) device prototypes.", "section": "D.1 Main Experimental Results on Computer Vision (CV) Task"}, {"figure_path": "y6JotynERr/tables/tables_26_1.jpg", "caption": "Table 5: Performance Results for CV task on TinyImageNet, STL-10, and CINIC-10 using pre-trained models.", "description": "This table presents the performance results for computer vision (CV) tasks on three datasets: TinyImageNet, STL-10, and CINIC-10, using pre-trained models.  The results are shown for FedAvg, FedMH, FedET, TAKFL, and TAKFL+Reg (TAKFL with regularization) methods.  The table displays the average accuracy across multiple runs with standard deviations.  The results highlight TAKFL's performance improvement over existing methods, particularly for the larger and more complex models.", "section": "7.2 Main Experimental Results"}, {"figure_path": "y6JotynERr/tables/tables_26_2.jpg", "caption": "Table 2: Performance Results for NLP Task on MNLI and SST-2. Training data distribution is similar to the CV task using only Dir(0.5) here. Public datasets are SNLI [2] for MNLI [52] and Sentiment140 [14] for SST-2 [45]. Client configurations are 8, 4, and 2 clients for S, M, and L, with sample rates of 0.3, 0.5, and 1.0, respectively. Architectures include Bert-Tiny, Bert-Mini, and Bert-Small [47] for S, M, and L, initialized from pre-trained parameters and fine-tuned for 20 communication rounds. See Appendix F.2 for more details.", "description": "This table shows the performance of different federated learning methods on two NLP tasks: MNLI and SST2.  It compares the performance across three different device prototype sizes (Small, Medium, Large), with varying data heterogeneity and model sizes. The table includes results for several baseline methods and TAKFL (with and without regularization), highlighting TAKFL's superior performance.", "section": "Main Experimental Results"}, {"figure_path": "y6JotynERr/tables/tables_27_1.jpg", "caption": "Table 1: Performance Results for CV task on CIFAR-10 and CIFAR-100. Training data is distributed among S, M, and L device prototypes in a 1:3:6 ratio, subdivided among clients using Dirichlet distribution. Public datasets are CIFAR-100 for CIFAR-10 and ImageNet-100 for CIFAR-100. Client configurations include 100, 20, and 4 clients for S, M, and L, with sampling rates of 0.1, 0.2, and 0.5. Architectures are ResNet-8, ResNet-14, and ResNet-18 for S, M and L, respectively. All models are trained from scratch for 60 rounds. See Appendix D.1 for additional experiments using hetero-family architecture and more details.", "description": "This table presents the performance comparison of different federated learning methods (FedAvg, FedDF, FedET, TAKFL) on CIFAR-10 and CIFAR-100 image classification tasks.  The results are broken down by device prototype size (small, medium, large) and data heterogeneity level (Dir(0.3) and Dir(0.1)). It shows the top-1 accuracy achieved by each method for each device prototype, highlighting the impact of different levels of data heterogeneity and model architecture on the performance of the algorithms. The table also includes results with self-regularization (TAKFL+Reg) to demonstrate the effectiveness of the self-regularization technique employed in TAKFL.", "section": "7.2 Main Experimental Results"}, {"figure_path": "y6JotynERr/tables/tables_27_2.jpg", "caption": "Table 7: Scalability Evaluation. Detailed performance results for 7 device prototypes case.", "description": "This table presents the results of scalability experiments performed on a federated learning setting with 7 device prototypes of varying sizes (from extremely small (XXS) to extremely large (XXL)).  It shows the average top-1 accuracy and standard deviation for each prototype across three independent runs.  The baselines (FedAvg, FedDF, FedET) are compared against TAKFL and TAKFL+Reg, demonstrating the ability of TAKFL to scale effectively with diverse device capabilities and achieve state-of-the-art results.", "section": "7.3 Scalability Evaluation"}, {"figure_path": "y6JotynERr/tables/tables_27_3.jpg", "caption": "Table 9: Scalability Evaluation. Detailed performance results for 3 device prototypes case.", "description": "This table presents the scalability evaluation results for three device prototypes (XXS, S, M) using the CINIC-10 dataset.  The average performance across these prototypes is reported, along with the standard deviation, for several methods including FedAvg, FedDF, FedET, TAKFL, and TAKFL+Reg.  It demonstrates the performance of the methods in a more homogeneous setting with smaller device prototypes.", "section": "7.3 Scalability Evaluation"}, {"figure_path": "y6JotynERr/tables/tables_29_1.jpg", "caption": "Table 4: Performance Results for CV task on CIFAR-10 and CIFAR-100. Training data is distributed among S, M, and L device prototypes in a 1:3:6 ratio, subdivided among clients using Dirichlet distribution. Public datasets are CIFAR-100 for CIFAR-10 and ImageNet-100 for CIFAR-100. Client configurations include 100, 20, and 4 clients for S, M, and L, with sampling rates of 0.1, 0.2, and 0.5. In homo-family settings, architectures are ResNet8, ResNet14, and ResNet18; in hetero-family settings, they are ViT-S, ResNet14, and VGG-16. All models are trained from scratch for 60 rounds. See Appendix F.1 for more details.", "description": "This table presents the performance of different federated learning methods (FedAvg, FedDF, FedET, TAKFL) on CIFAR-10 and CIFAR-100 image classification tasks.  It compares performance across different device prototype sizes (Small, Medium, Large) under low and high data heterogeneity.  The results show TAKFL's superior performance compared to existing methods, especially in heterogeneous settings.  The table includes results with and without self-regularization for TAKFL.", "section": "D.1 Main Experimental Results on Computer Vision (CV) Task"}, {"figure_path": "y6JotynERr/tables/tables_30_1.jpg", "caption": "Table 1: Performance Results for CV task on CIFAR-10 and CIFAR-100. Training data is distributed among S, M, and L device prototypes in a 1:3:6 ratio, subdivided among clients using Dirichlet distribution. Public datasets are CIFAR-100 for CIFAR-10 and ImageNet-100 for CIFAR-100. Client configurations include 100, 20, and 4 clients for S, M, and L, with sampling rates of 0.1, 0.2, and 0.5. Architectures are ResNet-8, ResNet-14, and ResNet-18 for S, M and L, respectively. All models are trained from scratch for 60 rounds. See Appendix D.1 for additional experiments using hetero-family architecture and more details.", "description": "This table presents the performance of different federated learning methods (FedAvg, FedDF, FedET, TAKFL) on CIFAR-10 and CIFAR-100 datasets for image classification.  The data is non-i.i.d. distributed among three groups of devices (small, medium, and large) with varying model and dataset sizes.  The table shows the top-1 accuracy for each device type under different data heterogeneity levels and indicates that TAKFL outperforms other methods.", "section": "7.2 Main Experimental Results"}, {"figure_path": "y6JotynERr/tables/tables_34_1.jpg", "caption": "Table 1: Performance Results for CV task on CIFAR-10 and CIFAR-100. Training data is distributed among S, M, and L device prototypes in a 1:3:6 ratio, subdivided among clients using Dirichlet distribution. Public datasets are CIFAR-100 for CIFAR-10 and ImageNet-100 for CIFAR-100. Client configurations include 100, 20, and 4 clients for S, M, and L, with sampling rates of 0.1, 0.2, and 0.5. Architectures are ResNet-8, ResNet-14, and ResNet-18 for S, M and L, respectively. All models are trained from scratch for 60 rounds. See Appendix D.1 for additional experiments using hetero-family architecture and more details.", "description": "This table presents the performance results of different federated learning methods on CIFAR-10 and CIFAR-100 datasets for image classification.  The results are categorized by device prototype size (Small, Medium, Large) and data heterogeneity (low and high) and show the top-1 accuracy for each method.  The table includes results for FedAvg (standard federated averaging), FedDF (Federated Distillation Framework), FedET (Federated Ensemble Transfer), TAKFL (Task Arithmetic Knowledge Federated Learning), and TAKFL+Reg (TAKFL with regularization).  It highlights the performance improvement of TAKFL compared to existing KD-based methods across different scenarios.", "section": "7.2 Main Experimental Results"}, {"figure_path": "y6JotynERr/tables/tables_34_2.jpg", "caption": "Table 1: Performance Results for CV task on CIFAR-10 and CIFAR-100. Training data is distributed among S, M, and L device prototypes in a 1:3:6 ratio, subdivided among clients using Dirichlet distribution. Public datasets are CIFAR-100 for CIFAR-10 and ImageNet-100 for CIFAR-100. Client configurations include 100, 20, and 4 clients for S, M, and L, with sampling rates of 0.1, 0.2, and 0.5. Architectures are ResNet-8, ResNet-14, and ResNet-18 for S, M and L, respectively. All models are trained from scratch for 60 rounds. See Appendix D.1 for additional experiments using hetero-family architecture and more details.", "description": "This table presents the performance of different federated learning methods on CIFAR-10 and CIFAR-100 datasets for image classification.  It compares the performance of FedAvg, FedDF, FedET, and TAKFL across three different device prototypes (small, medium, and large) under varying levels of data heterogeneity.  Results are presented as average top-1 accuracy and standard deviation over three independent runs.  The table shows the impact of different knowledge distillation methods in diverse and heterogeneous device settings. The impact of TAKFL's self-regularization is also shown.", "section": "7.2 Main Experimental Results"}, {"figure_path": "y6JotynERr/tables/tables_35_1.jpg", "caption": "Table 1: Performance Results for CV task on CIFAR-10 and CIFAR-100. Training data is distributed among S, M, and L device prototypes in a 1:3:6 ratio, subdivided among clients using Dirichlet distribution. Public datasets are CIFAR-100 for CIFAR-10 and ImageNet-100 for CIFAR-100. Client configurations include 100, 20, and 4 clients for S, M, and L, with sampling rates of 0.1, 0.2, and 0.5. Architectures are ResNet-8, ResNet-14, and ResNet-18 for S, M and L, respectively. All models are trained from scratch for 60 rounds. See Appendix D.1 for additional experiments using hetero-family architecture and more details.", "description": "This table presents the performance of different federated learning methods on CIFAR-10 and CIFAR-100 datasets for image classification.  The results are broken down by three different device prototype sizes (Small, Medium, Large) which vary in model and dataset size, reflecting real-world heterogeneity.  The table shows the top-1 accuracy for each device type and learning method, with standard deviations across three runs.  The methods compared include FedAvg, FedDF, FedET, and the proposed TAKFL method, with and without self-regularization.  The data distribution across the devices is non-i.i.d. using a Dirichlet distribution.", "section": "Main Experimental Results"}, {"figure_path": "y6JotynERr/tables/tables_35_2.jpg", "caption": "Table 4: Performance Results for CV task on CIFAR-10 and CIFAR-100. Training data is distributed among S, M, and L device prototypes in a 1:3:6 ratio, subdivided among clients using Dirichlet distribution. Public datasets are CIFAR-100 for CIFAR-10 and ImageNet-100 for CIFAR-100. Client configurations include 100, 20, and 4 clients for S, M, and L, with sampling rates of 0.1, 0.2, and 0.5. In homo-family settings, architectures are ResNet8, ResNet14, and ResNet18; in hetero-family settings, they are ViT-S, ResNet14, and VGG-16. All models are trained from scratch for 60 rounds. See Appendix F.1 for more details.", "description": "This table presents the performance of different federated learning methods (FedAvg, FedDF, FedET, TAKFL) on CIFAR-10 and CIFAR-100 datasets for image classification.  It compares the performance across three device prototype sizes (small, medium, large) under both low and high data heterogeneity scenarios. Two architectural settings (homo-family and hetero-family) are included, showing model accuracy with and without self-regularization. The table provides a detailed breakdown of experimental parameters, including dataset portions, number of clients, sampling rates, and architectural specifications.", "section": "D.1 Main Experimental Results on Computer Vision (CV) Task"}, {"figure_path": "y6JotynERr/tables/tables_35_3.jpg", "caption": "Table 5: Performance Results for CV task on TinyImageNet, STL-10, and CINIC-10 using pre-trained models.", "description": "This table presents the performance results for computer vision (CV) tasks using pre-trained models on three datasets: TinyImageNet, STL-10, and CINIC-10.  The results are shown for different device prototypes (small, medium, and large), comparing the performance of FedAvg, FedMH, FedET, TAKFL, and TAKFL+Reg (TAKFL with self-regularization). The table demonstrates the performance gains achieved by TAKFL, particularly in comparison to existing knowledge distillation (KD)-based methods.", "section": "D.2 Additional Experimental Results on CV Task"}, {"figure_path": "y6JotynERr/tables/tables_35_4.jpg", "caption": "Table 16: Details of Architecture parameters for Scalability Section 7.3, and Appendix D.4.", "description": "This table shows the architecture, number of parameters, dataset portion, number of clients, sample rate, and local training epochs used for each device prototype in the scalability experiments. The device prototypes range from extremely small (XXS) to extremely large (XXL), simulating real-world variations in device capabilities.", "section": "D.4 Scalability Evaluation"}, {"figure_path": "y6JotynERr/tables/tables_36_1.jpg", "caption": "Table 1: Performance Results for CV task on CIFAR-10 and CIFAR-100. Training data is distributed among S, M, and L device prototypes in a 1:3:6 ratio, subdivided among clients using Dirichlet distribution. Public datasets are CIFAR-100 for CIFAR-10 and ImageNet-100 for CIFAR-100. Client configurations include 100, 20, and 4 clients for S, M, and L, with sampling rates of 0.1, 0.2, and 0.5. Architectures are ResNet-8, ResNet-14, and ResNet-18 for S, M and L, respectively. All models are trained from scratch for 60 rounds. See Appendix D.1 for additional experiments using hetero-family architecture and more details.", "description": "This table presents the performance results of different federated learning methods on CIFAR-10 and CIFAR-100 image classification tasks.  The results are broken down by device prototype size (small, medium, large) and data heterogeneity level.  It shows the top-1 classification accuracy achieved by different models, including FedAvg, FedDF, FedET, and the proposed TAKFL. The table highlights TAKFL's improved performance over existing methods, especially in handling various device capabilities and data heterogeneity.", "section": "7.2 Main Experimental Results"}, {"figure_path": "y6JotynERr/tables/tables_36_2.jpg", "caption": "Table 1: Performance Results for CV task on CIFAR-10 and CIFAR-100. Training data is distributed among S, M, and L device prototypes in a 1:3:6 ratio, subdivided among clients using Dirichlet distribution. Public datasets are CIFAR-100 for CIFAR-10 and ImageNet-100 for CIFAR-100. Client configurations include 100, 20, and 4 clients for S, M, and L, with sampling rates of 0.1, 0.2, and 0.5. Architectures are ResNet-8, ResNet-14, and ResNet-18 for S, M and L, respectively. All models are trained from scratch for 60 rounds. See Appendix D.1 for additional experiments using hetero-family architecture and more details.", "description": "This table presents the performance of different federated learning methods on CIFAR-10 and CIFAR-100 datasets for image classification.  It compares the performance of FedAvg, FedDF, FedET, and TAKFL across three device prototype sizes (Small, Medium, Large) under different data heterogeneity settings (Dir(0.3) and Dir(0.1)).  The table shows the top-1 accuracy and includes results with and without self-regularization for TAKFL.", "section": "Main Experimental Results"}]