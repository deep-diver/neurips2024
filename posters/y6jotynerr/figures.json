[{"figure_path": "y6JotynERr/figures/figures_2_1.jpg", "caption": "Figure 1: Overview of our approach and its distinction from prior works. (a) This figure illustrates the vanilla ensemble distillation process, where logits from ensembles of various sizes are averaged and used as the distillation target across all prototypes. This approach leads to the dilution of information and suboptimal knowledge transfer (refer to Sections 6 and 7 for details). (b) This figure depicts our approach, TAKFL, which treats knowledge transfer from each prototype's ensemble as a separate task and distills them independently. Additionally, a KD-based self-regularization technique is introduced to mitigate issues related to the noisy and unsupervised ensemble distillation. Finally, the heterogeneously distilled knowledge is strategically integrated using an adaptive task arithmetic operation, allowing for customized knowledge integration based on each student prototype's needs.", "description": "This figure illustrates the difference between the vanilla ensemble distillation method and the proposed TAKFL method.  The left panel (a) shows how the vanilla method averages logits from ensembles of different sizes, leading to information dilution and suboptimal knowledge transfer. The right panel (b) details TAKFL, which treats each device's knowledge transfer as a separate task, independently distilling each to preserve unique contributions and avoid dilution.  TAKFL incorporates self-regularization to handle noisy distillation and uses adaptive task arithmetic to integrate distilled knowledge for optimal performance, customizing the integration process for each student model.", "section": "Related Works"}, {"figure_path": "y6JotynERr/figures/figures_5_1.jpg", "caption": "Figure 1: Overview of our approach and its distinction from prior works. (a) This figure illustrates the vanilla ensemble distillation process, where logits from ensembles of various sizes are averaged and used as the distillation target across all prototypes. This approach leads to the dilution of information and suboptimal knowledge transfer (refer to Sections 6 and 7 for details). (b) This figure depicts our approach, TAKFL, which treats knowledge transfer from each prototype's ensemble as a separate task and distills them independently. Additionally, a KD-based self-regularization technique is introduced to mitigate issues related to the noisy and unsupervised ensemble distillation. Finally, the heterogeneously distilled knowledge is strategically integrated using an adaptive task arithmetic operation, allowing for customized knowledge integration based on each student prototype's needs.", "description": "This figure compares the vanilla ensemble distillation method with the proposed TAKFL method.  (a) shows the traditional method where logits from ensembles of different sizes are averaged and used as a single distillation target. This can dilute information and lead to suboptimal knowledge transfer. (b) illustrates TAKFL, which treats each device prototype's ensemble as a separate task, distilling them independently to avoid information dilution.  TAKFL incorporates self-regularization to handle noise in the distillation process and uses adaptive task arithmetic to integrate the distilled knowledge based on each student prototype's needs.", "section": "Related Works"}, {"figure_path": "y6JotynERr/figures/figures_8_1.jpg", "caption": "Figure 1: Overview of our approach and its distinction from prior works. (a) This figure illustrates the vanilla ensemble distillation process, where logits from ensembles of various sizes are averaged and used as the distillation target across all prototypes. This approach leads to the dilution of information and suboptimal knowledge transfer (refer to Sections 6 and 7 for details). (b) This figure depicts our approach, TAKFL, which treats knowledge transfer from each prototype's ensemble as a separate task and distills them independently. Additionally, a KD-based self-regularization technique is introduced to mitigate issues related to the noisy and unsupervised ensemble distillation. Finally, the heterogeneously distilled knowledge is strategically integrated using an adaptive task arithmetic operation, allowing for customized knowledge integration based on each student prototype's needs.", "description": "This figure compares the vanilla ensemble distillation method with the proposed TAKFL method. (a) shows how the vanilla method averages logits from ensembles of different sizes, leading to information dilution and suboptimal knowledge transfer. (b) illustrates TAKFL's approach, where knowledge transfer from each device prototype is treated as a separate task, distilled independently, and then integrated using adaptive task arithmetic for optimal performance.", "section": "Related Works"}, {"figure_path": "y6JotynERr/figures/figures_14_1.jpg", "caption": "Figure 1: Overview of our approach and its distinction from prior works. (a) This figure illustrates the vanilla ensemble distillation process, where logits from ensembles of various sizes are averaged and used as the distillation target across all prototypes. This approach leads to the dilution of information and suboptimal knowledge transfer (refer to Sections 6 and 7 for details). (b) This figure depicts our approach, TAKFL, which treats knowledge transfer from each prototype's ensemble as a separate task and distills them independently. Additionally, a KD-based self-regularization technique is introduced to mitigate issues related to the noisy and unsupervised ensemble distillation. Finally, the heterogeneously distilled knowledge is strategically integrated using an adaptive task arithmetic operation, allowing for customized knowledge integration based on each student prototype's needs.", "description": "This figure compares vanilla ensemble distillation with the proposed TAKFL method.  Vanilla ensemble distillation averages logits from ensembles of varying sizes, leading to information loss and suboptimal knowledge transfer.  TAKFL, in contrast, treats knowledge transfer from each ensemble as a separate task, distilling them independently to preserve unique contributions.  It also incorporates self-regularization to mitigate noise and uses adaptive task arithmetic for customized knowledge integration.", "section": "Related Works"}, {"figure_path": "y6JotynERr/figures/figures_14_2.jpg", "caption": "Figure 1: Overview of our approach and its distinction from prior works. (a) This figure illustrates the vanilla ensemble distillation process, where logits from ensembles of various sizes are averaged and used as the distillation target across all prototypes. This approach leads to the dilution of information and suboptimal knowledge transfer (refer to Sections 6 and 7 for details). (b) This figure depicts our approach, TAKFL, which treats knowledge transfer from each prototype's ensemble as a separate task and distills them independently. Additionally, a KD-based self-regularization technique is introduced to mitigate issues related to the noisy and unsupervised ensemble distillation. Finally, the heterogeneously distilled knowledge is strategically integrated using an adaptive task arithmetic operation, allowing for customized knowledge integration based on each student prototype's needs.", "description": "This figure compares vanilla ensemble distillation with the proposed TAKFL method.  Vanilla ensemble distillation averages logits from ensembles of different sizes, leading to information loss.  TAKFL, in contrast, treats knowledge transfer as separate tasks for each device, distilling independently to avoid dilution. It then uses adaptive task arithmetic to integrate the knowledge, allowing customization for optimal performance on each student model.", "section": "Related Works"}, {"figure_path": "y6JotynERr/figures/figures_21_1.jpg", "caption": "Figure 1: Overview of our approach and its distinction from prior works. (a) This figure illustrates the vanilla ensemble distillation process, where logits from ensembles of various sizes are averaged and used as the distillation target across all prototypes. This approach leads to the dilution of information and suboptimal knowledge transfer (refer to Sections 6 and 7 for details). (b) This figure depicts our approach, TAKFL, which treats knowledge transfer from each prototype's ensemble as a separate task and distills them independently. Additionally, a KD-based self-regularization technique is introduced to mitigate issues related to the noisy and unsupervised ensemble distillation. Finally, the heterogeneously distilled knowledge is strategically integrated using an adaptive task arithmetic operation, allowing for customized knowledge integration based on each student prototype's needs.", "description": "This figure compares vanilla ensemble distillation with the proposed TAKFL method.  Vanilla ensemble distillation averages logits from ensembles of different sizes, leading to information dilution and suboptimal knowledge transfer.  In contrast, TAKFL treats each device's ensemble as a separate task, distilling independently to avoid dilution. It uses a self-regularization technique to mitigate noise in the distillation process and then adaptively integrates the knowledge using task arithmetic for optimal performance.", "section": "Related Works"}, {"figure_path": "y6JotynERr/figures/figures_29_1.jpg", "caption": "Figure 1: Overview of our approach and its distinction from prior works. (a) This figure illustrates the vanilla ensemble distillation process, where logits from ensembles of various sizes are averaged and used as the distillation target across all prototypes. This approach leads to the dilution of information and suboptimal knowledge transfer (refer to Sections 6 and 7 for details). (b) This figure depicts our approach, TAKFL, which treats knowledge transfer from each prototype's ensemble as a separate task and distills them independently. Additionally, a KD-based self-regularization technique is introduced to mitigate issues related to the noisy and unsupervised ensemble distillation. Finally, the heterogeneously distilled knowledge is strategically integrated using an adaptive task arithmetic operation, allowing for customized knowledge integration based on each student prototype's needs.", "description": "This figure compares the vanilla ensemble distillation (VED) and TAKFL methods for handling knowledge transfer in federated learning with heterogeneous devices.  Panel (a) shows the VED approach, which averages logits from ensembles of different sizes, leading to information loss and suboptimal knowledge transfer. Panel (b) illustrates the TAKFL approach.  TAKFL treats knowledge transfer as separate tasks, distilling each ensemble independently to avoid information dilution. It also uses a self-regularization technique and adaptive task arithmetic knowledge integration to customize the transfer process for each device.", "section": "Related Works"}, {"figure_path": "y6JotynERr/figures/figures_30_1.jpg", "caption": "Figure 1: Overview of our approach and its distinction from prior works. (a) This figure illustrates the vanilla ensemble distillation process, where logits from ensembles of various sizes are averaged and used as the distillation target across all prototypes. This approach leads to the dilution of information and suboptimal knowledge transfer (refer to Sections 6 and 7 for details). (b) This figure depicts our approach, TAKFL, which treats knowledge transfer from each prototype's ensemble as a separate task and distills them independently. Additionally, a KD-based self-regularization technique is introduced to mitigate issues related to the noisy and unsupervised ensemble distillation. Finally, the heterogeneously distilled knowledge is strategically integrated using an adaptive task arithmetic operation, allowing for customized knowledge integration based on each student prototype's needs.", "description": "This figure compares vanilla ensemble distillation (VED) and TAKFL.  (a) shows VED, where logits from ensembles of different sizes are averaged, causing information dilution and suboptimal knowledge transfer.  (b) illustrates TAKFL, which handles each device's ensemble knowledge as a separate task, distills them independently, and integrates them adaptively using task arithmetic for optimal performance.", "section": "Related Works"}]