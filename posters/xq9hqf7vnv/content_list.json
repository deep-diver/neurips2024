[{"type": "text", "text": "Principled Probabilistic Imaging using Diffusion Models as Plug-and-Play Priors ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zihui Wu1 Yu Sun4 Yifan Chen5 Bingliang Zhang1 Yisong Yue1 Katherine L. Bouman1,2,3 ", "page_idx": 0}, {"type": "text", "text": "1Department of Computing and Mathematical Sciences, Caltech 2Department of Electrical Engineering, Caltech 3Department of Astronomy, Caltech 4Department of Electrical and Computer Engineering, Johns Hopkins University 5Courant Institute of Mathematical Sciences, New York University ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Diffusion models (DMs) have recently shown outstanding capabilities in modeling complex image distributions, making them expressive image priors for solving Bayesian inverse problems. However, most existing DM-based methods rely on approximations in the generative process to be generic to different inverse problems, leading to inaccurate sample distributions that deviate from the target posterior defined within the Bayesian framework. To harness the generative power of DMs while avoiding such approximations, we propose a Markov chain Monte Carlo algorithm that performs posterior sampling for general inverse problems by reducing it to sampling the posterior of a Gaussian denoising problem. Crucially, we leverage a general DM formulation as a unified interface that allows for rigorously solving the denoising problem with a range of state-of-the-art DMs. We demonstrate the effectiveness of the proposed method on six inverse problems (three linear and three nonlinear), including a real-world black hole imaging problem. Experimental results indicate that our proposed method offers more accurate reconstructions and posterior estimation compared to existing DM-based imaging inverse methods. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Inverse problems arise in many computational imaging applications, where the goal is to recover an image $\\pmb{x}\\in\\mathbb{R}^{n}$ from a set of sparse and noisy measurements $\\pmb{y}\\in\\mathbb{R}^{m}$ . The relationship between $\\textbf{\\em x}$ and $\\textit{\\textbf{y}}$ can be described by ", "page_idx": 0}, {"type": "equation", "text": "$$\n{\\pmb y}={\\pmb{\\mathcal A}}({\\pmb x})+{\\pmb n},\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $\\mathcal{A}(\\cdot):\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{m}$ is the forward operator (linear or nonlinear) and $\\mathbfit{\\Delta}$ is the random measurement noise in $\\mathbb{R}^{m}$ . Since the sparsity and noisiness of $\\textit{\\textbf{y}}$ often lead to significant uncertainty in $\\textbf{\\em x}$ , it is preferable to sample the posterior distribution $p({\\pmb x}|{\\pmb y})$ over all possible solutions based on some prior distribution $p(x)$ , rather than finding a single deterministic solution. Traditional posterior sampling methods often rely on simple image priors that do not reflect the sophistication of real-world image distributions. On the other hand, diffusion models (DMs) have recently emerged as a powerful tool for modeling highly complex image distributions [33, 62]. Nevertheless, it remains a challenge to turn DMs into reliable imaging inverse solvers, which motivates us to develop a principled Bayesian method that leverages DMs as priors for posterior sampling. ", "page_idx": 0}, {"type": "text", "text": "Diffusion models generate samples from a distribution by reversing a diffusion process from the target distribution to a simple (usually Gaussian) distribution [33, 62]. In particular, it estimates a clean image $\\pmb{x}_{0}$ from a noise image $x_{T}$ by successively denoising noisy images, where $\\pmb{x}_{t}\\sim p_{t}$ is the intermediate noisy image at time $t\\,\\in\\,[0,T]$ . Reversing diffusion requires one to estimate the time-varying gradient log density (score function) $\\nabla\\log p_{t}(\\mathbf{x}_{t})$ along the diffusion process, or $\\nabla\\log p_{t}(\\pmb{x}_{t}|\\pmb{\\dot{y}})$ in the case of sampling the posterior $p(\\pmb{x}|\\pmb{y})$ . ", "page_idx": 0}, {"type": "image", "img_path": "Xq9HQf7VNV/tmp/68e89308a827695c94448c953accc7e3a0beaae86621d94324af4ab83ccf9966.jpg", "img_caption": ["Figure 1: Demonstration of the proposed method, PnP-DM, for posterior sampling using the real data for the M87 black hole from April $6^{\\mathrm{th}}$ , 2017 [21]. The black hole imaging problem is non-convex and highly ill-posed due to severe noise corruption and measurement sparsity. Our method rigorously integrates measurements from a real-world imaging system with an expressive image prior in the form of a diffusion model, which was trained with images from the GRMHD black hole simulation [22] in this case. Besides having high visual quality, our posterior samples accurately capture key features of the M87 black hole such as the bright spot location and ring diameter. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "To design generic DM-based inverse problem solvers, most existing methods attempt to approximate the time-varying gradient log density $\\nabla\\log p_{t}(\\pmb{x}_{t}|\\pmb{y})$ [17, 72, 83, 61, 38, 57, 59, 43, 15, 76, 18, 54, 8]. In particular they first apply Bayes\u2019 rule to separate the forward operator from an unconditional prior over the intermediate noisy image $\\pmb{x}_{t}$ : ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\nabla\\log p_{t}(\\pmb{x}_{t}|\\pmb{y})=\\nabla\\log p_{t}(\\pmb{y}|\\pmb{x}_{t})+\\nabla\\log p_{t}(\\pmb{x}_{t}).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "By instead aiming to evaluate the right hand side, one can leverage the existing pre-trained DMs for the unconditional term $\\nabla\\log p_{t}(\\mathbf{x}_{t})$ . However, the main challenge in this case is that $\\nabla\\log p_{t}(\\pmb{y}|\\pmb{x}_{t})$ is intractable to compute in general, as $p_{t}(\\pmb{y}|\\pmb{x}_{t})$ involves an integral over all possible $\\scriptstyle x_{0}$ \u2019s that could give rise to $\\pmb{x}_{t}$ [17]. Various methods have been proposed to circumvent the intractability and can mostly be categorized into two groups. One group of methods explicitly approximate $\\nabla\\log p_{t}(\\pmb{y}|\\pmb{x}_{t})$ by making simplifying assumptions [61, 17, 59, 8]. However, even for arguably the finest approximation to date proposed in the recent work [8], it is exact only when the prior distribution $p(x)$ is Gaussian. For general prior distributions beyond Gaussian, these methods do not sample the true posterior $p({\\pmb x}|{\\pmb y})$ . The other group of methods do not make explicit approximations but instead substitute $\\nabla\\log p_{t}(\\pmb{y}|\\pmb{x}_{t})$ with empirically designed updates where $\\textit{\\textbf{y}}$ is treated as a guidance signal [72, 83, 38, 57, 43, 15, 76, 18, 54]. Although these methods may have strong empirical performance, they have deviated from the Bayesian formulation and no longer aim to sample the target posterior. In summary, these existing DM-based inverse methods should be best viewed as guidance methods, where the generative process is guided towards the regions where the measurement $\\textit{\\textbf{y}}$ is more likely to be observed, not as posterior sampling methods [8]. We also note that some recent work considered combing DMs with Sequential Monte Carlo to ensure asymptotic consistency in posterior sampling [11, 23], but the investigation has been limited to linear imaging inverse problems. ", "page_idx": 1}, {"type": "text", "text": "Our contributions In this work, we pursue a different path towards posterior sampling with DM priors by proposing a new Markov chain Monte Carlo (MCMC) algorithm, which we call Plug-andPlay Diffusion Models (PnP-DM). It incorporates DMs in a principled way and circumvents the approximation required when taking the approach in (2). The proposed algorithm is based on the Split Gibbs Sampler [70] that alternates between two sampling steps that separately involve the likelihood and prior. While the likelihood step can be tackled with traditional sampling techniques, the prior step involves a Bayesian denoising problem that requires careful design. Importantly, we identify a connection between the Bayesian denoising problem and the unconditional image generation problem under a general formulation of DMs presented in [36] (which is referred to as the EDM formulation hereafter). This connection allows us to perform rigorous posterior sampling for denoising using DMs without approximating the generative process and enables the use of a wide range of pretrained DMs through the unified EDM formulation. We present an analysis on the non-asymptotic behavior of PnP-DM by establishing a stationarity guarantee in terms of the average Fisher information. We further demonstrate the strong empirical performance of PnP-DM by investigating three linear and three nonlinear noisy inverse problems, including a black hole interferometric imaging problem involving real data that is both nonlinear and severely ill-posed (see Figure 1). Overall, PnP-DM outperforms existing baseline methods, achieving higher accuracy in posterior estimation. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Split Gibbs Sampler $(S G S)$ is an MCMC approach developed for Bayesian inference [70]. It is also related to the Proximal Sampler [41, 14, 25, 78] and serves as the backbone for the Generative Plug-and-Play $(G P n P)$ [6] and Diffusion Plug-and-Play $(D P n P)$ [77] frameworks in computational imaging. The goal of SGS is to sample the posterior distribution ", "page_idx": 2}, {"type": "equation", "text": "$$\np({\\pmb x}|{\\pmb y})\\propto p({\\pmb y}|{\\pmb x})p({\\pmb x})=\\exp(-f({\\pmb x};{\\pmb y})-g({\\pmb x}))\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $f(\\mathbf{{x};\\pmb{y}}):=-\\log p(\\pmb{y}|\\pmb{x})$ and $g(\\pmb{x}):=-\\log p(\\pmb{x})$ are the potential functions of the likelihood and prior distribution, respectively. The dual dependence of (3) on both the likelihood and prior makes it nontrivial to directly sample from it in general. Instead, SGS leverages the composite structure of the posterior distribution by adopting a variable-splitting strategy and considers sampling an alternative distribution ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pi(\\pmb{x},z)\\propto\\exp\\left(-f(z;\\pmb{y})-g(\\pmb{x})-\\frac{1}{2\\rho^{2}}\\|\\pmb{x}-z\\|_{2}^{2}\\right)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $z\\in\\mathbb{R}^{n}$ is an augmented variable and $\\rho>0$ is a hyperparameter that controls the strength of the coupling between $\\textbf{\\em x}$ and $_{z}$ . We denote the $\\mathbf{\\nabla}_{\\alpha}.$ - and $_{z}$ -marginal distributions of (4) as $\\pi^{X}({\\bar{\\pmb x}}):=$ $\\textstyle\\int\\pi(x,{\\bar{z}})\\mathrm{d}{\\bar{z}}$ and $\\textstyle\\pi^{Z}(\\pmb{x})\\,:=\\,\\int\\pi(\\pmb{x},z)\\mathrm{d}\\pmb{x}$ , respectively. As $\\rho\\,\\rightarrow\\,0$ , $\\pi^{X}$ converges to the target posterior $p({\\pmb x}|{\\pmb y})$ in terms of total variation distance [70], so one can obtain approximate samples from the target posterior by sampling (4) instead. ", "page_idx": 2}, {"type": "text", "text": "SGS samples (4) via Gibbs sampling. Specifically, SGS starts from an initialization $\\pmb{x}^{(0)}$ and, for iteration $k=0,\\cdot\\cdot\\cdot\\,,K-1$ , alternates between ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\mple~}z^{(k)}\\sim\\pi^{Z|X=x^{(k)}}(z)\\propto\\exp\\left(-f(z;y)-\\frac{1}{2\\rho^{2}}\\|x^{(k)}-z\\|_{2}^{2}\\right)}\\\\ &{x^{(k+1)}\\sim\\pi^{X|Z=z^{(k)}}(x)\\propto\\exp\\left(-g(x)-\\frac{1}{2\\rho^{2}}\\|x-z^{(k)}\\|_{2}^{2}\\right)\\!.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Note that the two conditional distributions separately involve $f(\\cdot;\\pmb{y})$ and $g(\\cdot)$ . The likelihood and prior are decoupled so that these two steps can be designed in a modular way. A similar variablesplitting strategy is also adopted in optimization methods such as the Half-Quadratic Splitting (HQS) method [31] and the Alternating Direction Method of Multipliers (ADMM) [30, 7]. In fact, SGS can be viewed as a sampling analogue of HQS. SGS is a principled approach to posterior sampling if the two sampling steps are rigorously implemented. ", "page_idx": 2}, {"type": "text", "text": "Existing works related to SGS Several works have designed algorithms for solving imaging inverse problems based on SGS [52, 19, 6, 27, 77]. The key distinction among these methods lies in their approaches to the prior step. For instance, the works [52, 6, 27] applied Langevin-based updates for sampling $\\pi^{X|Z=z}$ such that the prior information is encoded by either traditional regularizers or off-the-shelf image denoisers. The work [19] tackled the prior step by heuristically customizing a diffusion model (i.e. DDPM [33]) for sampling $\\pi^{X|Z=z}$ . A concurrent work [77] improved the implementation by devising two diffusion processes that rigorously solve the prior step. Our method differs from [77] by connecting the prior step to the EDM formulation [36]. This connection allows us to seamlessly integrate state-of-the-art DMs as expressive image priors for Bayesian inference through a unified interface, eliminating the need for additional customization for each model and leading to better empirical performance. We also note the recent work [42] that adopted the optimizationbased variable-splitting formulation of HQS and utilized general DMs as image priors. We instead considers the SGS formulation from a Bayesian posterior sampling standpoint. Additionally, while SGS-based methods theoretically accommodate general inverse problems, empirical evidence on real-world nonlinear inverse problems remains scarce in the literature. In this work, we demonstrate our method on three nonlinear inverse problems, including a black hole imaging problem. For a more comprehensive review of related works, see Appendix E. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "A schematic diagram for the proposed method is shown in Figure 2. Our method, dubbed PnP-DM, builds upon the SGS framework with rigorous implementations of the two sampling steps and an annealing schedule for the coupling parameter $\\rho$ . We start with our implementations of the first step for solving both linear and nonlinear inverse problems. ", "page_idx": 2}, {"type": "image", "img_path": "Xq9HQf7VNV/tmp/82da25cc69d6c78025ee8f32d94a9fbfe30aa8c4920d6b42f377ad465f224a51.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: A schematic diagram of our method. Our method alternates between a likelihood step that enforces data consistency and a prior step that solves a denoising posterior sampling problem by leveraging the Split Gibbs Sampler [70]. An annealing schedule controls the strength of the two steps at each iteration to facilitate efficient and accurate sampling. A crucial part of our design is the prior step, where we identify a key connection to a general diffusion model framework called the EDM [36]. This connection allows us to easily incorporate a family of state-of-the-art diffusion models as priors to conduct posterior sampling in a principled way without additional training. Our method demonstrates strong performance on a variety of linear and nonlinear inverse problems. ", "page_idx": 3}, {"type": "text", "text": "3.1 Likelihood step: enforcing data consistency ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "For the likelihood step at iteration $k$ , we sample ", "page_idx": 3}, {"type": "equation", "text": "$$\nz^{(k)}\\sim\\pi^{Z|X=x^{(k)}}(z)\\propto\\exp\\left(-f(z;y)-\\frac{1}{2\\rho^{2}}\\|x^{(k)}-z\\|_{2}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Linear forward model and Gaussian noise We first consider a simple yet common case where the forward model $\\boldsymbol{\\mathcal{A}}$ is linear and the noise distribution is zero-mean Gaussian, i.e. $\\pmb{\\mathscr{A}}:=\\pmb{\\mathscr{A}}\\in\\mathbb{R}^{m\\times n}$ and $\\pmb{n}\\sim\\mathcal{N}(\\mathbf{0},\\pmb{\\Sigma})$ . In this case, the potential function of the likelihood term is $\\begin{array}{r}{f(\\mathbf{\\boldsymbol{x}};\\mathbf{\\boldsymbol{y}})=\\frac{1}{2}\\|\\pmb{\\boldsymbol{y}}-\\pmb{A}\\pmb{x}\\|_{\\Sigma}^{2}}\\end{array}$ (up to an additive constant that does not depend on $\\textbf{\\em x}$ and $\\textit{\\textbf{y}}$ ) where $\\|\\cdot\\|_{\\Sigma}^{2}:=\\langle\\cdot,\\Sigma^{-1}\\cdot\\rangle$ . It is then straightforward to show that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pi^{Z|X=x}=\\mathcal{N}(\\pmb{m}(\\pmb{x}),\\pmb{\\Lambda}^{-1})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathbf{A}:=A^{T}\\Sigma^{-1}A+\\frac{1}{\\rho^{2}}I}\\end{array}$ and $\\begin{array}{r}{m(x):=\\Lambda^{-1}(A^{T}\\Sigma^{-1}y+\\frac{1}{\\rho^{2}}x)}\\end{array}$ . The problem of sampling from Gaussian distributions has been systematically studied [71]. We refer readers to Appendix C.1 for a more detailed discussion. ", "page_idx": 3}, {"type": "text", "text": "General case For general nonlinear inverse problems, the likelihood step is not sampling from a Gaussian distribution anymore. Nevertheless, since we have access to $\\pi^{Z|X=x}$ in closed form up to a multiplicative factor, we can use Monte Carlo methods based on Langevin dynamics to draw samples from it as long as the likelihood potential is differentiable. Specifically, we first set up the following Langevin SDE that admits $\\pi^{Z|X=x}$ as the stationary distribution ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{d}z_{t}=\\nabla\\log\\pi^{Z|X=x}(z_{t})\\mathrm{d}t+\\sqrt{2}\\mathrm{d}w_{t}=\\left[-\\nabla f(z;y)-\\frac{1}{\\rho^{2}}(z-x)\\right]\\mathrm{d}t+\\sqrt{2}\\mathrm{d}w_{t}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We then initialize the SDE at $z_{\\mathrm{0}}=x$ and run it with Euler discretization. The pseudocode is provided in Appendix C.1. ", "page_idx": 3}, {"type": "text", "text": "3.2 Prior step: denoising via the EDM framework ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "For the prior step at iteration $k$ , we sample ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pmb{x}^{(k+1)}\\sim\\pi^{X|Z=z^{(k)}}(\\pmb{x})\\propto\\exp\\left(-g(\\pmb{x})-\\frac{1}{2\\rho^{2}}\\|\\pmb{x}-\\pmb{z}^{(k)}\\|_{2}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "A closer examination of (6) reveals that this prior step is essentially to draw posterior samples for a Gaussian denoising problem, where the \u201cmeasurement\u201d is $z^{(k)}$ , the noise level is $\\rho$ , and the prior distribution is $p(\\pmb{x})\\stackrel{+}{\\propto}\\exp(-g(\\pmb{x}))$ . ", "page_idx": 3}, {"type": "text", "text": "We tackle this denoising posterior sampling problem within SGS using DMs as image priors. In particular, we leverage the EDM framework [36], which was originally proposed to unify various formulations of DMs for unconditional image generation. To see the connection of the EDM framework to (6), consider a family of mollified distributions $p(\\pmb{x};\\pmb{\\sigma})$ given by adding i.i.d Gaussian noise of standard deviation $\\sigma$ to the prior distribution $p(x)$ , i.e. $\\pmb{x}+\\sigma\\pmb{\\epsilon}\\sim p(\\pmb{x};\\sigma)$ . The core idea of the EDM framework is that a variety of state-of-the-art DMs can be unified into the following reverse SDE: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{d}x_{t}=\\left[\\frac{\\dot{s}(t)}{s(t)}x_{t}-2s(t)^{2}\\dot{\\sigma}(t)\\sigma(t)\\nabla\\log p\\left(\\frac{x_{t}}{s(t)};\\sigma(t)\\right)\\right]\\mathrm{d}t+s(t)\\sqrt{2\\dot{\\sigma}(t)\\sigma(t)}\\mathrm{d}\\bar{w}_{t}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\bar{\\pmb{w}}_{t}$ is an $n$ -dimensional Wiener process running backward in time, $\\sigma(t)>0$ is a pre-defined noise level schedule with $\\sigma(0)=0$ , $s(t)$ is a pre-defined scaling schedule, and ${\\dot{\\sigma}}(t),\\,{\\dot{s}}(t)$ are their time derivatives. As shown in [36], the defining property of (7) is that $\\mathbf{x}_{t}/s(t)\\sim p(\\mathbf{x};\\sigma(t))$ for any time $t$ . Therefore, solving this SDE backward in time allows us to travel from any noise level $\\sigma(t)$ to the clean image distribution at $t=0$ . This means that we can use (7) to solve (6) with arbitrary noise level $\\rho$ as long as $\\rho$ is within the range of $\\sigma(t)$ . Indeed, the distribution of $\\scriptstyle x_{0}$ conditioned on $\\pmb{x}_{t}$ is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{l}{p(x_{0}|x_{t})\\propto p(x_{t}|x_{0})p(x_{0})\\propto\\mathcal{N}(s(t)x_{0},s(t)^{2}\\sigma(t)^{2}I)\\exp(-g(x_{0}))}\\\\ {\\propto\\exp\\left(-g(x_{0})-\\frac{1}{2\\sigma(t)^{2}}\\|x_{0}-x_{t}/s(t)\\|_{2}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We highlight that the last line exactly matches (6) when $\\pmb{x}_{t}=s(t)\\pmb{z}^{(k)}$ and $\\sigma(t)=\\rho$ . Therefore, we can naturally design a practical algorithm that samples (6) by following these three steps: (1) find $t^{*}$ such that $\\sigma(t^{*})=\\rho$ , (2) initialize at $\\mathbf{\\bar{\\alpha}}_{t^{*}}=s(t^{*})\\bar{\\mathbf{z}}^{(k)}$ , and (3) solve (7) backward from $t^{*}$ to 0 by choosing the discretization time steps and integration scheme. Through this unified interface, any DMs, once converted to the EDM formulation, can be directly turned into a rigorous solver for (6). ", "page_idx": 4}, {"type": "text", "text": "Leveraging the connection with EDM, our prior step implementation comes with a large design space that encompasses a variety of existing DMs, such as DDPM (or VP-SDE) [33], VE-SDE [62], and iDDPM [51]. In our experiments, we conduct posterior sampling with all these different models within our framework and all of them provide high-quality samples. The pseudocode of our implementation and more details on the EDM formulation for the prior step is given in Appendix C.2. ", "page_idx": 4}, {"type": "text", "text": "3.3 Putting it all together ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The pseudocode of PnP-DM in complete form is presented in Algorithm 1. PnP-DM alternates between the two sampling steps with an annealing schedule $\\{\\rho_{k}\\}$ for the coupling parameter. We find that the annealing schedule on $\\rho$ accelerates the mixing time of the Markov chain and prevents the algorithm from getting stuck in bad local minima for solving highly ill-posed inverse problems. This is a common practice in both Langevin-based [39, 34, 64] and SGS-based [6, 77] MCMC algorithms to improve the empirical performance in solving inverse problems. ", "page_idx": 4}, {"type": "text", "text": "Our work shares some similarities with PnP-SGS [19] but contains three main key differences. First, as demonstrated in our experiments, we investigate three nonlinear inverse problems, while nonlinear inverse problems are beyond the scope of [19]. Our experiments show that $\\mathrm{PnP-SGS}$ struggles with challenging nonlinear inverse problems such as Fourier phase retrieval. Second, we adopt the EDM formulation to ensure that the prior step of PnP-DM is a rigorous mapping from the image manifold with the desired noise level to the clean image manifold, aligning with the theory of SGS. In contrast, the prior step of $\\mathrm{PnP-SGS}$ [19] is heuristic (which is also pointed out by [77]) and not rigorously designed to sample (6). Third, unlike PnP-SGS [19] that uses a constant $\\rho$ , we consider an annealing schedule $\\{\\rho_{k}\\}$ for the coupling parameter, which is important for highly ill-posed inverse problems. ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 Plug-and-Play Diffusion Models (PnP-DM) ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Input: initialization $\\pmb{x}_{0}\\ \\in\\ \\mathbb{R}^{n}$ , total number of iterations $K\\ >\\ 0$ , coupling strength schedule $\\{\\rho_{k}\\,>\\,0\\}_{k=0}^{K-1}$ , likelihood potential $f(\\,\\cdot\\,;\\pmb{y})$ with measurements $\\pmb{y}\\in\\mathbb{R}^{m}$ , pretrained model $D_{\\theta}({}\\cdot{};{}\\cdot{})$ that approximates $\\nabla\\log p\\left(\\mathbf{\\boldsymbol{x}};\\boldsymbol{\\sigma}\\right)$ with $(D_{\\theta}(\\mathbf{\\boldsymbol{x}};\\sigma)-\\mathbf{\\boldsymbol{x}})/\\sigma^{2}$ . ", "page_idx": 4}, {"type": "text", "text": "4: end for ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "5: return x(k+1) ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "3.4 Theoretical insights ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We provide some theoretical insights on the non-asymptotic behavior of $\\mathrm{PnP}$ -DM. We start with the following definitions. For two probability measures $\\mu$ and $\\widetilde{\\mu}$ such that $\\mu\\ll\\widetilde{\\mu}$ , the Kullback\u2013Leibler $(K L)$ divergence an d  Fisher informatio n (or Fisher divergence) of $\\mu$ with respect to $\\widetilde{\\mu}$ are defined, respectively, as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathsf{K L}(\\mu||\\widetilde{\\mu}):=\\int\\mu\\log\\frac{\\mu}{\\widetilde{\\mu}}\\quad\\mathrm{and}\\quad\\mathsf{F l}(\\mu||\\widetilde{\\mu}):=\\int\\mu\\left\\|\\nabla\\log\\frac{\\mu}{\\widetilde{\\mu}}\\right\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Both divergences are equal to zero if and only if $\\mu=\\widetilde{\\mu}$ . KL divergence is a common metric for quantifying the diff e rence of one distribution with respect to another. Fisher information has been used for analyzing the stationarity of sampling algorithms [3, 65]. ", "page_idx": 5}, {"type": "image", "img_path": "Xq9HQf7VNV/tmp/0af358d2e077796dfe322f68eaf7b2db168c96bc1f75dcb429f311798a15e9eb.jpg", "img_caption": ["Figure 3: A conceptual illustration of the non-stationary and stationary time-continuous processes as interpolations of $K$ discretize iterations of $\\mathrm{PnP-DM}$ . "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "We analyze PnP-DM via a continuous-time perspective, leveraging the interpolation techniques introduced for Langevin Monte Carlo [67, 3, 65]. We assume that the likelihood step (5) can be implemented exactly and the prior step (6) involves running the reverse diffusion process (7) with an approximated score function $s_{t}\\approx\\nabla\\log p_{t}:=\\nabla\\log p(\\,\\cdot\\,;\\sigma(t))$ . Let $\\nu_{0}^{X}$ be the distribution of the initialization $\\pmb{x}^{(0)}$ . Let $\\nu_{k}^{Z}$ and $\\nu_{k+1}^{X}$ be the distributions of $z^{(k)}$ and $\\pmb{x}^{(k+1)}$ at the $k^{\\mathrm{th}}$ iteration. Recall that the stationary distributions are $\\pi^{X}$ and $\\pi^{Z}$ . Our analysis is concerned with two continuous-time processes: (1) the non-stationary process from $\\nu_{0}^{X}$ , a non-stationary initialization, to $\\nu_{K}^{X}$ where (7) is run with the approximated score function $\\scriptstyle{s_{t}}$ and (2) the stationary process that alternates between stationary distributions $\\pi^{X}$ and $\\pi^{Z}$ . These two processes are the interpolation PnP-DM in non-stationary and stationary states and define continuous transitions over discrete iterations. A conceptual illustration of the two processes is provided in Figure 3 with the exact formulations in Appendix A. Now we present our main result: ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.1. Consider running $K$ iterations of PnP-DM with $\\rho_{k}\\equiv\\rho>0$ and a score estimate $s_{t}\\approx\\nabla\\log p_{t}:=\\nabla\\log p(\\cdot\\,;\\sigma(\\bar{t}))$ . Let $t^{*}\\,>\\,0$ be such that $\\sigma(t^{*})\\,=\\,\\rho$ and $\\delta:=\\operatorname*{inf}_{t\\in[0,t^{*}]}v(t)$ where $v(t):=s(t)\\sqrt{2\\dot{\\sigma}(t)\\sigma(t)}.$ . Define $\\nu_{\\tau}$ and $\\pi_{\\tau}$ as the distributions at time $\\tau$ of the non-stationary and stationary process, respectively. Then, for over $K$ iterations of $P n P$ -DM, or equivalently over $\\tau\\in[0,T]$ with $T:=K(t^{*}+1)$ , we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\int_{0}^{T}\\mathsf{F l}\\left(\\pi_{\\tau}||\\nu_{\\tau}\\right)\\mathrm{d}\\tau\\le\\frac{4\\mathsf{K L}\\bigl(\\pi^{X}||\\nu_{0}^{X}\\bigr)}{K(t^{\\ast}+1)\\operatorname*{min}(\\rho,\\delta)^{2}}+\\mathstrut\\quad\\frac{4\\epsilon_{s c o r e}}{(t^{\\ast}+1)\\delta^{2}}\\quad,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where we assume that the score estimation error $\\begin{array}{r}{\\epsilon_{s c o r e}:=\\int_{1}^{t^{*}+1}v(\\tau)^{2}\\mathbb{E}_{\\pi_{\\tau}}\\|s_{\\tau}-\\nabla\\log p_{\\tau}\\|_{2}^{2}\\mathrm{d}\\tau<\\infty.}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "The proof is provided in Appendix A. This theorem states that the average distance (measured by Fisher information) of the non-stationary process with respect to the stationary process over $K$ iterations of PnP-DM goes to zero at a rate of $O(1/K)$ under certain conditions up to the score approximation error. Note that our theory only requires $L^{2}$ -accurate score estimate under the measure $\\pi_{\\tau}$ , which is a relatively weaker condition than the common $L^{\\infty}$ -accurate score estimate assumption in prior analysis of sampling methods involving score estimates [5, 65]. This result resembles the first-order stationarity for Langevin Monte Carlo [3]. Unlike the non-asymptotic analysis in [77], we utilize the average Fisher information instead of the total variation distance, enabling us to obtain an explicit convergence rate. Here $\\delta$ is the infimum of the diffusion coefficient along the reverse diffusion in (7); see further discussions on the role of $\\delta$ in Appendix A.3. Our theory shows that the accurate implementations of the two sampling steps lead to a sampler that provably converges to the stationary process that alternates between the two target stationary distributions. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "image", "img_path": "Xq9HQf7VNV/tmp/b42b852f1109880b8237fe5235b6edd8e2af19f2026d086d62bc480181b8f507.jpg", "img_caption": ["Figure 4: Results on a synthetic problem with the ground truth posterior available. PnP-DM can sample it more accurately that DPS [17]. "], "img_footnote": [], "page_idx": 5}, {"type": "table", "img_path": "Xq9HQf7VNV/tmp/c4cb53c09cd61781bb516a8a06fc39c1c0ebcd55d692af6d36bbffdfbbbde9a1.jpg", "table_caption": ["Table 1: Quantitative comparison on three noisy linear inverse problems for 100 FFHQ color test images. Bold: best; Underline: second best. "], "table_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "", "img_caption": ["Figure 5: Visual examples for the motion deblur problem $(\\sigma_{y}=0.05)$ . We visualize one sample generated by each sampling algorithm. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Validation with ground truth posterior ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We first demonstrate the accuracy of $\\mathrm{PnP}$ -DM for posterior sampling on a simulated compressed sensing problem with a Gaussian prior where the posterior distribution can be expressed in a closed form. The mean and per-pixel standard deviation of the prior are visualized on the bottom left of Figure 4. The linear forward model $A\\in\\mathbb{R}^{m\\times n}$ is a Gaussian matrix $(m=n/2)$ ), i.e. $A_{i j}\\sim\\mathcal{N}(0,1)$ . A test image is randomly generated from the prior (see top left of Figure 4), and the measurement is calculated according to (1) with $\\pmb{n}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{\\dot{0}}.01^{2}\\pmb{I})$ . We compare our method with the popular DM-based method DPS [17]. We draw 1,000 samples and visualize the empirical mean and per-pixel standard deviation for both algorithms. Compared with the true posterior (second column), we find that the both methods accurately estimate the mean. However, the standard deviation image estimated by DPS significantly deviates from the ground truth. In contrast, our standard deviation image matches the ground truth in terms of both absolute magnitude and spatial distribution. These results highlight the accuracy of our method over DPS by taking a more principled Bayesian approach. ", "page_idx": 6}, {"type": "text", "text": "4.2 Benchmark experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Dataset and inverse problems We test our proposed algorithm and several baseline methods on 100 images from the validation set of the FFHQ dataset [37] for five inverse problems: (1) Gaussian deblur with kernel size $61\\!\\times\\!61$ and standard deviation 3.0, (2) Motion deblur with kernel size $61\\!\\times\\!61$ and intensity of 0.5, (3) Super-resolution with $4\\times$ downsampling ratio, (4) the coded diffraction patterns (CDP) reconstruction problem (nonlinear) in [10, 50] (phase retrieval with a phase mask), and (5) the Fourier phase retrieval (nonlinear) with $4\\times$ oversampling. We add i.i.d. Gaussian noise to all the simulated measurements $\\textit{\\textbf{y}}$ . In particular, i.e. $\\boldsymbol{n}\\sim\\dot{\\mathcal{N}}(\\mathbf{0},\\bar{\\sigma}_{y}^{2}\\boldsymbol{I})$ . For all problems except for Fourier phase retrieval, the noise standard deviation is set as $\\sigma_{y}^{-}=0.05$ . Due to the severe ill-posedness of Fourier phase retrieval, we consider a smaller noise standard deviation $\\sigma_{\\pmb y}=0.01$ . ", "page_idx": 6}, {"type": "text", "text": "Baselines and comparison protocols We consider four variants of DMs as plug-in priors for our method, namely VP-SDE (VP) [33], VE-SDE (VE) [62], iDDPM [51], and EDM [36]. We compare our method with various baselines, including (1) optimization-based methods: PnP-ADMM [13], DPIR [79]; (2) conditional DMs: DDRM [38], DPS [18]; and (3) SGS-based method: PnP-SGS [19], DPnP [77]. For fair comparison, we use the same pre-trained score function checkpoint for all DM-based methods. Since the pre-trained score function was trained with the DDPM formulation (VP-SDE) [33], we convert it to the EDM formulation by applying the VP preconditioning [36]. We use the Peak Signal-to-Noise Ratio (PSNR), the Structural Similarity Index Measure (SSIM), and the Learned Perceptual Image Patch Similarity (LPIPS) distance for quantitative comparison. For each sampling method, we draw 20 randoms samples, calculate their mean, and report the metrics on the mean image. More experimental details are provided in Appendices B, C, D. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Results: linear problems A quantitative comparison is provided in Table 1. PnP-DM generally outperforms the baseline methods and that the VE and EDM variants consistently outperform the other two variants on these linear problems. Figure 5 contains visual examples for the motion deblur problem (see Appendix F.2 for the other two linear problems). PnP-DM provides high-quality reconstructions that are both sharp and consistent with the ground truth image. We also provide an uncertainty quantification analysis based on pixel-wise statistics in Figure 6. In the left three columns, we visualize the absolute error $(|\\bar{\\boldsymbol{x}}-\\boldsymbol{x}|)$ , standard deviation (std), and absolute ${\\bf Z}$ -score $(|\\bar{\\pmb{x}}-\\pmb{x}|/\\mathsf{s t d})$ . In the third column, red pixels highlight locations where the ground truth pixel values are outliers of the 3-sigma credible interval (CI) under the estimated posterior uncertainty. The fourth column contains scatter plots of $|{\\bar{\\mathbf{x}}}-{\\boldsymbol{x}}|$ versus std for each pixel of the reconstructions, where red boxes show the percentages of outliers (outside of 3-sigma CI) and gray boxes indicate the percentages within the 3-sigma CI. Similar to the ", "page_idx": 7}, {"type": "image", "img_path": "Xq9HQf7VNV/tmp/76f2199093319f813619844068539eacdbef267b717c23f90b7a3d1a63d1d5a7.jpg", "img_caption": ["Figure 6: Comparison of uncertainty quantification (UQ) for the motion deblur. Left 3 columns: absolute error $(|\\bar{\\boldsymbol{{x}}}-\\boldsymbol{{x}}|)$ , standard deviation (std), and absolute z-score $(|\\bar{\\pmb{x}}-\\pmb{x}|/\\mathsf{s t d})$ with the outlier pixels in red. Right column: scatter plot of $|{\\bar{x}}-x|$ versus std. Note that PnP-DM leads to a better UQ performance than the baselines by having the lowest percentage of outliers while avoiding having overestimated per-pixel standard deviations. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "synthetic prior experiment, DPS tends to have larger standard deviation estimations, as shown by the less concentrated distribution of gray points around the origin. Compared with baselines, especially $\\mathrm{PnP-SGS}$ , our approach captures a higher percentage $(97.46\\%)$ of ground truth pixels than the baselines $g6.20\\%$ and $88.77\\%$ ). If the true posterior were truly Gaussian, $99\\%$ of the ground-truth pixels should lie within the 3-sigma CI; however, as the posterior is not Gaussian with a DM-based prior, we do not necessarily expect to reach $99\\%$ coverage. ", "page_idx": 7}, {"type": "text", "text": "Results: nonlinear problems We provide a quantitative comparison in Table 2. For the CDP reconstruction problem, PnP-DM performs on par with DPS but outperforms other SGS-based methods. We then consider the Fourier phase retrieval (FPR) problem, which is known to be a challenging nonlinear inverse problem. One challenge lies in its invariance to $180^{\\circ}$ rotation, so the posterior distribution have two modes, one with upright images and another with $180^{\\circ}$ -rotated images, that equally fti the measurement. To increase the chance of getting properly-oriented reconstructions, we run each algorithm with four different random initializations and report the metrics for the best run, following the practice in [18]. We find that PnP-DM significantly outperforms the baselines on this highly ill-posed inverse problem. As shown in Figure 7 (a), our method can provide high-quality reconstructions for both orientations, while the baseline methods fail to capture at least one of the two modes. We further run our method for a test image with 100 different random initialization and collect reconstructions in both orientations that are above 28dB in PSNR (90 out of 100 runs). The percentage of upright and rotated reconstructions are visualized by the pie chart in Figure 7 (b). With a prior on upright face images, our method generate mostly samples with the upright orientation. Nevertheless, it can also find the other mode that has an equal likelihood, demonstrating its ability to capture multi-modal posterior distributions. ", "page_idx": 7}, {"type": "text", "text": "4.3 Experiments on black hole imaging ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Problem setup We finally validate PnP-DM on a real-world nonlinear imaging inverse problem: black hole imaging (BHI) (see Appendix B for more details). A visual illustration of BHI is provided in Figure 8 (a). This BHI inverse problem is severely ill-posed. Even with an Earth-sized telescope, only a small fraction of the Fourier frequencies of the target black hole can be measured (region within the red box); in reality, this region is further subsampled with a highly sparse pattern (black lines). Additionally, the atmospheric noise causes nonlinearity of this BHI problem that sometimes results in a multi-modal posterior distribution of the reconstructed image [63]. Here we demonstrate the effectiveness of PnP-DM in capturing a multi-modal posterior distribution. For brevity, we restrict our choice of diffusion models in PnP-DM to EDM and use DPS as the baseline. ", "page_idx": 7}, {"type": "table", "img_path": "Xq9HQf7VNV/tmp/b106d49712ec13862c82519dad940a959c72464d9693eee4b43057ac54cdaa35.jpg", "table_caption": ["Table 2: Quantitative evaluation on two noisy nonlinear inverse problems for 100 FFHQ grayscale test images. Bold: best; Underline: second best. "], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "", "img_caption": ["Figure 7: Results of the Fourier phase retrieval problem. (a) PnP-DM provides both upright and rotated reconstructions (two modes given by the invariance of the forward model to $180^{\\circ}$ rotation) with high fidelity, while the baseline methods cannot. (b) We visualize the percentages of upright and rotated reconstructions out of 90 runs for a test image with two samples for each orientation. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Results on simulated data We use the simulated data from [63] where the measurements are generated assuming that the ground-truth black hole image were at the location of the Sagittarius $\\mathbf{A}^{*}$ black hole. Figure 8 (b) visually compares the results obtained by PnP-DM and DPS. We use the t-SNE method [66] to cluster the generated samples (100 for each method) and identify two modes in the samples generated by PnP-DM and three modes in those generated by DPS. We visualize the mean and three samples for each image mode. A metric for quantifying the degree of data mismatch is labeled on the top right corner of each image. As illustrated by both the mean and sample images, PnP-DM successfully captures the two modes previously identified for this dataset [63]. Note that PnP-DM generates high-fidelity samples from both modes with sharp details of the flux ring, and its samples from \u201cMode 1\u201d align well with the ground truth image. In contrast, two out of the three modes sampled by DPS fail to exhibit a meaningful black hole structure and do not correspond with the observed measurements, as indicated by the significantly larger data mismatch values. ", "page_idx": 8}, {"type": "text", "text": "Results on real data Finally, we apply PnP-DM to the real M87 black hole data from April $6^{\\mathrm{th}}$ , 2017 [21], with the results shown in Figure 1. By leveraging an expressive DM-based image prior, PnP-DM generates high-quality samples that are both visually plausible and consistent with the ring diameters observed in the official EHT reconstruction. These results highlight the robustness and effectiveness of our method in tackling a highly ill-posed real-world inverse problem. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We have introduced PnP-DM, a posterior sampling method for solving imaging inverse problems. The backbone of our method is a split Gibbs sampler that iteratively alternates between two steps that separately involve the likelihood and prior. Crucially, we establish a link between the prior step and a general DM framework known as the EDM formulation. By leveraging this connection, we seamlessly integrate a diverse range of state-of-the-art DMs as priors through a unified interface. Experimental results demonstrate that our method outperforms existing DM-based methods across both linear and nonlinear inverse problems, including a nonlinear and severely ill-posed black hole interferometric imaging problem. ", "page_idx": 8}, {"type": "image", "img_path": "Xq9HQf7VNV/tmp/112bc757f7a528a58e29f93c0272b724323db21d3e57efb5fc409857200fd143.jpg", "img_caption": ["Figure 8: Results on a nonlinear and severely ill-posed black hole imaging problem. Our method, PnP-DM, is compared with the conditional diffusion model baseline DPS. A metric quantifying the mismatch with the observed measurements is labeled for each sample, which should be around 2 for ideal measurement fit. Samples generated by PnP-DM exhibit two distinct modes with sharp details and a consistent ring structure, while samples given by DPS display inconsistent ring sizes and sometimes fail to capture the black hole structure entirely with samples having poor measurement fti. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Limitations PnP-DM can be further improved in the following two aspects. First, PnP-DM currently requires evaluating the likelihood and prior steps for the entire image at a time. This potentially poses computational challenges in solving large-scale inverse problems (e.g. 3D imaging) or those with expensive likelihood evaluation (e.g. PDE inverse problems). Second, the current theoretical analysis does not consider the approximation error introduced in the likelihood step for general nonlinear inverse problems when running Langevin MCMC for finite iterations. Explicit incorporation of this error would offer further insights into the empirical performance of PnP-DM. ", "page_idx": 9}, {"type": "text", "text": "Broader impacts We expect this work to make a positive impact in computational imaging and related application domains. For many imaging problems, there is a need to facilitate image reconstruction with expressive image priors and quantify uncertainty, which could lead to better imaging systems that enables further understanding of the imaging target. Nonetheless, as we are introducing DMs as priors into the imaging process, it is inevitable to inherent the potential bias of these models. ", "page_idx": 9}, {"type": "text", "text": "6 Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The authors thank Charles Gammie, Ben Prather, Abhishek Joshi, Vedant Dhruv, and Chi-kwan Chan for providing the black hole simulations. The authors also thank the generous funding from Schmidt Sciences and the Heritage Medical Research Fellowship. Z.W. was supported by an Amazon AI4Science Fellowship. Y.S. was supported by a Computing, Data, and Society Fellowship. B.Z. was supported by a Kortschak Fellowship. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Rizwan Ahmad, Charles A. Bouman, Gregery T. Buzzard, Stanley Chan, Sizhuo Liu, Edward T. Reehorst, and Philip Schniter. Plug-and-play methods for magnetic resonance imaging: Using denoisers for image recovery. IEEE Signal Processing Magazine, 37(1):105\u2013116, 2020.   \n[2] Brian. D. O. Anderson. Reverse-time diffusion equation models. Stochastic Processes and their Applications, 12:313\u2013326, 1982. [3] Krishna Balasubramanian, Sinho Chewi, Murat A Erdogdu, Adil Salim, and Shunshi Zhang. Towards a theory of non-log-concave sampling:first-order stationarity guarantees for langevin monte carlo. In Po-Ling Loh and Maxim Raginsky, editors, Proceedings of Thirty Fifth Conference on Learning Theory, volume 178 of Proceedings of Machine Learning Research, pages 2896\u20132923. PMLR, 02\u201305 Jul 2022.   \n[4] Georgios Batzolis, Jan Stanczuk, Carola-Bibiane Schonlieb, and Christian Etmann. Conditional image generation with score-based diffusion models. ArXiv, abs/2111.13606, 2021. [5] Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion schr\u00f6dinger bridge with applications to score-based generative modeling. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021. [6] Charles A. Bouman and Gregery T. Buzzard. Generative plug and play: Posterior sampling for inverse problems, 2023. [7] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends in Machine Learning, 3:1\u2013122, 01 2011. [8] Benjamin Boys, Mark Girolami, Jakiw Pidstrigach, Sebastian Reich, Alan Mosca, and O. Deniz Akyildiz. Tweedie moment projected diffusions for inverse problems, 2023. [9] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity natural image synthesis. In International Conference on Learning Representations, 2019.   \n[10] Emmanuel Cand\u00e8s, Xiaodong Li, and Mahdi Soltanolkotabi. Phase retrieval from coded diffraction patterns. Applied and Computational Harmonic Analysis, 39, 10 2013.   \n[11] Gabriel Victorino Cardoso, Yazid Janati, Sylvain Le Corff, and \u00c9ric Moulines. Monte carlo guided diffusion for bayesian linear inverse problems. ArXiv, abs/2308.07983, 2023.   \n[12] Andrew A. Chael, Michael D. Johnson, Katherine L. Bouman, Lindy L. Blackburn, Kazunori Akiyama, and Ramesh Narayan. Interferometric imaging directly with closure phases and closure amplitudes. The Astrophysical Journal, 857(1):23, apr 2018.   \n[13] Stanley Chan, Xiran Wang, and Omar Elgendy. Plug-and-play admm for image restoration: Fixed point convergence and applications. IEEE Transactions on Computational Imaging, PP, 05 2016.   \n[14] Yongxin Chen, Sinho Chewi, Adil Salim, and Andre Wibisono. Improved analysis for a proximal algorithm for sampling. In Po-Ling Loh and Maxim Raginsky, editors, Proceedings of Thirty Fifth Conference on Learning Theory, volume 178 of Proceedings of Machine Learning Research, pages 2984\u20133014. PMLR, 02\u201305 Jul 2022.   \n[15] Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. Ilvr: Conditioning method for denoising diffusion probabilistic models. 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 14347\u201314356, 2021.   \n[16] Jooyoung Choi, Jungbeom Lee, Chaehun Shin, Sungwon Kim, Hyunwoo Kim, and Sungroh Yoon. Perception prioritized training of diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11472\u201311481, June 2022.   \n[17] Hyungjin Chung, Jeongsol Kim, Michael Thompson Mccann, Marc Louis Klasky, and Jong Chul Ye. Diffusion posterior sampling for general noisy inverse problems. In The Eleventh International Conference on Learning Representations, 2023.   \n[18] Hyungjin Chung and Jong Chul Ye. Score-based diffusion models for accelerated mri. Medical Image Analysis, page 102479, 2022.   \n[19] Florentin Coeurdoux, Nicolas Dobigeon, and Pierre Chainais. Plug-and-play split gibbs sampler: embedding deep generative priors in bayesian inference, 2023.   \n[20] Regev Cohen, Yochai Blau, Daniel Freedman, and Ehud Rivlin. It has potential: Gradient-driven denoisers for convergent solutions to inverse problems. In Neural Information Processing Systems, 2021.   \n[21] The Event Horizon Telescope Collaboration. First m87 event horizon telescope results. iv. imaging the central supermassive black hole. The Astrophysical Journal Letters, 875(1):L4, apr 2019.   \n[22] The Event Horizon Telescope Collaboration. First m87 event horizon telescope results. v. physical origin of the asymmetric ring. The Astrophysical Journal Letters, 875(1):L5, apr 2019.   \n[23] Zehao Dou and Yang Song. Diffusion posterior sampling for linear inverse problem solving: A filtering perspective. In The Twelfth International Conference on Learning Representations, 2024.   \n[24] Bradley Efron. Tweedie\u2019s formula and selection bias. Journal of the American Statistical Association, 106:1602\u20131614, 12 2011.   \n[25] Jiaojiao Fan, Bo Yuan, and Yongxin Chen. Improved dimension dependence of a proximal algorithm for sampling. In Gergely Neu and Lorenzo Rosasco, editors, Proceedings of Thirty Sixth Conference on Learning Theory, volume 195 of Proceedings of Machine Learning Research, pages 1473\u20131521. PMLR, 12\u201315 Jul 2023.   \n[26] Zhenghan Fang, Sam Buchanan, and Jeremias Sulam. What\u2019s in a prior? learned proximal networks for inverse problems. In The Twelfth International Conference on Learning Representations, 2024.   \n[27] Elhadji C. Faye, Mame Diarra Fall, and Nicolas Dobigeon. Regularization by denoising: Bayesian model and langevin-within-split gibbs sampling, 2024.   \n[28] Berthy T. Feng, Jamie Smith, Michael Rubinstein, Huiwen Chang, Katherine L. Bouman, and William T. Freeman. Score-based diffusion models as principled priors for inverse imaging. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 10520\u201310531, October 2023.   \n[29] James R. Fienup. Phase retrieval algorithms: a comparison. Applied optics, 21 15:2758\u201369, 1982.   \n[30] Daniel Gabay and Bertrand Mercier. A dual algorithm for the solution of nonlinear variational problems via finite element approximation. Computers & Mathematics With Applications, 2:17\u201340, 1976.   \n[31] Donald Geman and Chengda Yang. Nonlinear image recovery with half-quadratic regularization. IEEE transactions on image processing : a publication of the IEEE Signal Processing Society, 4 7:932\u201346, 1995.   \n[32] Alexandros Graikos, Nikolay Malkin, Nebojsa Jojic, and Dimitris Samaras. Diffusion models as plug-andplay priors. ArXiv, abs/2206.09012, 2022.   \n[33] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 6840\u20136851. Curran Associates, Inc., 2020.   \n[34] Ajil Jalal, Marius Arvinte, Giannis Daras, Eric Price, Alexandros G Dimakis, and Jon Tamir. Robust compressed sensing mri with deep generative priors. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 14938\u201314954. Curran Associates, Inc., 2021.   \n[35] Ulugbek Kamilov, Charles Bouman, Gregery Buzzard, and Brendt Wohlberg. Plug-and-play methods for integrating physical and learned models in computational imaging: Theory, algorithms, and applications. IEEE Signal Processing Magazine, 40:85\u201397, 01 2023.   \n[36] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.   \n[37] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4396\u20134405, 2018.   \n[38] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. Denoising diffusion restoration models. In Advances in Neural Information Processing Systems, 2022.   \n[39] Bahjat Kawar, Gregory Vaksman, and Michael Elad. SNIPS: Solving noisy inverse problems stochastically. Advances in Neural Information Processing Systems, 34:21757\u201321769, 2021.   \n[40] R\u00e9mi Laumont, Valentin De Bortoli, Andr\u00e9s Almansa, Julie Delon, Alain Durmus, and Marcelo Pereyra. Bayesian imaging using plug & play priors: When langevin meets tweedie. SIAM Journal on Imaging Sciences, 15(2):701\u2013737, 2022.   \n[41] Yin Tat Lee, Ruoqi Shen, and Kevin Tian. Structured logconcave sampling with a restricted gaussian oracle. In Proceedings of Thirty Fourth Conference on Learning Theory, volume 134 of Proceedings of Machine Learning Research, pages 2993\u20133050. PMLR, 15\u201319 Aug 2021.   \n[42] Xiang Li, Soo Min Kwon, Ismail R. Alkhouri, Saiprasad Ravishankar, and Qing Qu. Decoupled data consistency with diffusion purification for image restoration, 2024.   \n[43] Jiaming Liu, Rushil Anirudh, Jayaraman J. Thiagarajan, Stewart He, K Aditya Mohan, Ulugbek S. Kamilov, and Hyojin Kim. Dolce: A model-based probabilistic diffusion framework for limited-angle ct reconstruction. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 10498\u201310508, October 2023.   \n[44] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV), December 2015.   \n[45] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019.   \n[46] Morteza Mardani, Jiaming Song, Jan Kautz, and Arash Vahdat. A variational perspective on solving inverse problems with diffusion models. In The Twelfth International Conference on Learning Representations, 2024.   \n[47] S\u00e9gol\u00e8ne Martin, Anne Gagneux, Paul Hagemann, and Gabriele Steidl. Pnp-flow: Plug-and-play image restoration with flow matching, 2024.   \n[48] Tim Meinhardt, Michael M\u00f6ller, Caner Hazirbas, and Daniel Cremers. Learning proximal operators: Using denoising networks for regularizing inverse imaging problems. 2017 IEEE International Conference on Computer Vision (ICCV), pages 1799\u20131808, 2017.   \n[49] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In International Conference on Learning Representations, 2021.   \n[50] Christopher A. Metzler, Philip Schniter, Ashok Veeraraghavan, and Richard Baraniuk. prdeep: Robust phase retrieval with a flexible deep network. In International Conference on Machine Learning, 2018.   \n[51] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 8162\u20138171. PMLR, 18\u201324 Jul 2021.   \n[52] Marcelo Pereyra, Luis A. Vargas-Mieles, and Konstantinos C. Zygalakis. The split gibbs sampler revisited: Improvements to its algorithmic structure and augmented target distribution. SIAM Journal on Imaging Sciences, 16(4):2040\u20132071, 2023.   \n[53] B. Remy, F. Lanusse, N. Jeffrey, J. Liu, Jean-Luc Starck, K. Osato, and T. Schrabback. Probabilistic mass-mapping with neural score estimation. Astronomy & Astrophysics, 672, 12 2022.   \n[54] Litu Rout, Negin Raoof, Giannis Daras, Constantine Caramanis, Alex Dimakis, and Sanjay Shakkottai. Solving linear inverse problems provably via posterior sampling with latent diffusion models. In Thirtyseventh Conference on Neural Information Processing Systems, 2023.   \n[55] Ernest K. Ryu, Jialin Liu, Sicheng Wang, Xiaohan Chen, Zhangyang Wang, and Wotao Yin. Plug-and-play methods provably converge with properly trained denoisers. In International Conference on Machine Learning, 2019.   \n[56] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David Fleet, and Mohammad Norouzi. Image super-resolution via iterative refinement. IEEE Transactions on Pattern Analysis and Machine Intelligence, PP:1\u201314, 09 2022.   \n[57] Bowen Song, Soo Min Kwon, Zecheng Zhang, Xinyu Hu, Qing Qu, and Liyue Shen. Solving inverse problems with latent diffusion models via hard data consistency. In The Twelfth International Conference on Learning Representations, 2024.   \n[58] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021.   \n[59] Jiaming Song, Arash Vahdat, Morteza Mardani, and Jan Kautz. Pseudoinverse-guided diffusion models for inverse problems. In International Conference on Learning Representations, 2023.   \n[60] Jiaming Song, Qinsheng Zhang, Hongxu Yin, Morteza Mardani, Ming-Yu Liu, Jan Kautz, Yongxin Chen, and Arash Vahdat. Loss-guided diffusion models for plug-and-play controllable generation. In Proceedings of the 40th International Conference on Machine Learning, ICML\u201923. JMLR.org, 2023.   \n[61] Yang Song, Liyue Shen, Lei Xing, and Stefano Ermon. Solving inverse problems in medical imaging with score-based generative models. In International Conference on Learning Representations, 2022.   \n[62] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021.   \n[63] He Sun and Katherine L. Bouman. Deep probabilistic imaging: Uncertainty quantification and multi-modal solution characterization for computational imaging. In AAAI Conference on Artificial Intelligence (AAAI), 2021.   \n[64] Yu Sun, Brendt Wohlberg, and Ulugbek Kamilov. An online plug-and-play algorithm for regularized image reconstruction. IEEE Transactions on Computational Imaging, PP:1\u20131, 01 2019.   \n[65] Yu Sun, Zihui Wu, Yifan Chen, Berthy T. Feng, and Katherine L. Bouman. Provable probabilistic imaging using score-based generative priors. IEEE Transactions on Computational Imaging, 10:1290\u20131305, 2024.   \n[66] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, 9(86):2579\u20132605, 2008.   \n[67] Santosh S. Vempala and Andre Wibisono. Rapid convergence of the unadjusted langevin algorithm: Isoperimetry suffices. In Neural Information Processing Systems, 2019.   \n[68] Singanallur V. Venkatakrishnan, Charles A. Bouman, and Brendt Wohlberg. Plug-and-play priors for model based reconstruction. In 2013 IEEE Global Conference on Signal and Information Processing, pages 945\u2013948, 2013.   \n[69] Pascal Vincent. A connection between score matching and denoising autoencoders. Neural Computation, 23:1661\u20131674, 2011.   \n[70] Maxime Vono, Nicolas Dobigeon, and Pierre Chainais. Split-and-augmented gibbs sampler\u2014application to large-scale inference problems. IEEE Transactions on Signal Processing, 67(6):1648\u20131661, 2019.   \n[71] Maxime Vono, Nicolas Dobigeon, and Pierre Chainais. High-dimensional gaussian sampling: A review and a unifying approach based on a stochastic proximal point algorithm. SIAM Review, 64(1):3\u201356, 2022.   \n[72] Yinhuai Wang, Jiwen Yu, and Jian Zhang. Zero-shot image restoration using denoising diffusion null-space model. The Eleventh International Conference on Learning Representations, 2023.   \n[73] Luhuan Wu, Brian L Trippe, Christian A. Naesseth, David M Blei, and John P Cunningham. Practical and asymptotically exact conditional sampling in diffusion models. arXiv preprint arXiv:2306.17775, 2023.   \n[74] Zihui Wu, Yu Sun, Jiaming Liu, and Ulugbek Kamilov. Online regularization by denoising with applications to phase retrieval. In 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW), pages 3887\u20133895, 2019.   \n[75] Zihui Wu, Yu Sun, Alex Matlock, Jiaming Liu, Lei Tian, and Ulugbek S. Kamilov. Simba: Scalable inversion in optical tomography using deep denoising priors. IEEE Journal of Selected Topics in Signal Processing, 14(6):1163\u20131175, 2020.   \n[76] Bin Xia, Yulun Zhang, Shiyin Wang, Yitong Wang, Xinglong Wu, Yapeng Tian, Wenming Yang, and Luc Van Gool. Diffir: Efficient diffusion model for image restoration. ICCV, 2023.   \n[77] Xingyu Xu and Yuejie Chi. Provably robust score-based diffusion posterior sampling for plug-and-play image reconstruction, 2024.   \n[78] Bo Yuan, Jiaojiao Fan, Jiaming Liang, Andre Wibisono, and Yongxin Chen. On a class of gibbs sampling over networks. In Gergely Neu and Lorenzo Rosasco, editors, Proceedings of Thirty Sixth Conference on Learning Theory, volume 195 of Proceedings of Machine Learning Research, pages 5754\u20135780. PMLR, 12\u201315 Jul 2023.   \n[79] Kai Zhang, Yawei Li, Wangmeng Zuo, Lei Zhang, Luc Gool, and Radu Timofte. Plug-and-play image restoration with deep denoiser prior. IEEE Transactions on Pattern Analysis and Machine Intelligence, PP:1\u20131, 06 2021.   \n[80] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond a Gaussian denoiser: Residual learning of deep CNN for image denoising. IEEE Transactions on Image Processing, 26(7):3142\u2013 3155, 2017.   \n[81] Kai Zhang, Wangmeng Zuo, Shuhang Gu, and Lei Zhang. Learning deep cnn denoiser prior for image restoration. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2808\u20132817, 2017.   \n[82] Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator. In The Eleventh International Conference on Learning Representations, 2023.   \n[83] Yuanzhi Zhu, Kai Zhang, Jingyun Liang, Jiezhang Cao, Bihan Wen, Radu Timofte, and Luc Van Gool. Denoising diffusion models for plug-and-play image restoration. In IEEE Conference on Computer Vision and Pattern Recognition Workshops (NTIRE), 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Theory ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Interpolation of PnP-DM ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we formally introduce the interpolation of PnP-DM. We consider the case where the coupling strength $\\rho$ is constant, i.e. $\\rho_{k}\\equiv\\rho$ and make the following assumption. ", "page_idx": 15}, {"type": "text", "text": "Assumption A.1. There exists a unique $t^{*}$ such that $\\sigma(t^{*})=\\rho$ ", "page_idx": 15}, {"type": "text", "text": "This assumption is satisfied for common diffusion models. Popular choices of the noise level schedule include $\\sigma(t)=t$ or $\\sigma(t)=\\sqrt{t}$ , which are monotonically increasing functions of $t$ . We first present two propositions showing that the two steps in SGS can be implemented by running two SDEs. ", "page_idx": 15}, {"type": "text", "text": "Proposition A.2 (Brownian bridge for the likelihood step). For iteration $k$ with iterate $\\pmb{x}^{(k)}$ , the likelihood step of SGS is equivalent to solving the following $S D E$ from $t=0$ to $t=1$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{d}\\pmb{x}_{t}=\\rho^{2}\\nabla\\log\\phi_{t}(\\pmb{x}_{t})\\mathrm{d}t+\\rho\\mathrm{d}\\pmb{w}_{t}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\pmb{x}_{0}=\\pmb{x}^{(k)}$ and $\\begin{array}{r}{\\phi_{t}(\\pmb{x}):=\\int\\exp[-f(\\pmb{z};\\pmb{y})-\\frac{1}{2\\rho^{2}(1-t)}\\|\\pmb{x}-\\pmb{z}\\|_{2}^{2}]\\mathrm{d}\\pmb{z}.}\\end{array}$ ", "page_idx": 15}, {"type": "text", "text": "Proof. This proposition is due to the Brownian bridge construction presented in Lemma 4 of [78]. This SDE satisfies that $\\begin{array}{r}{p(\\pmb{x}_{1}|\\pmb{x}_{0})\\propto\\exp\\left(-f(\\pmb{x}_{1};\\pmb{y})-\\frac{1}{2\\rho^{2}}\\|\\pmb{x}_{0}-\\pmb{x}_{1}\\|_{2}^{2}\\right)}\\end{array}$ . Therefore, solving (9) from $t=0$ to $t=1$ is equivalent to taking a likelihood step. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Proposition A.3 (EDM reverse diffusion for the prior step). For iteration $k$ with iterate $z^{(k)}$ , the prior step of SGS is equivalent to solving the following SDE from $t=t^{*}$ to $t=0$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{d}\\pmb{x}_{t}=\\left[u(t)\\pmb{x}_{t}-v(t)^{2}\\nabla\\log p_{t}\\left(\\pmb{x}_{t}\\right)\\right]\\mathrm{d}t+v(t)\\mathrm{d}\\pmb{\\bar{w}}_{t}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\mathbf{\\boldsymbol{x}}_{t^{*}}=s(t^{*})z^{(k)}$ , $\\begin{array}{r}{u(t):=\\frac{\\dot{s}(t)}{s(t)}}\\end{array}$ , $v(t):=s(t)\\sqrt{2\\dot{\\sigma}(t)\\sigma(t)}$ , and $p_{t}$ is the distribution of $s(t){\\mathbf{}x}+{}$ $s(t)\\sigma(t)\\epsilon$ with $\\textbf{\\em x}$ following the prior distribution $p(\\pmb{x})\\propto\\exp(-g(\\pmb{x}))$ and $\\pmb{\\epsilon}\\sim\\mathcal{N}(\\mathbf{0},\\pmb{I})$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. First note that (10) is exactly (7) written in terms of $u(t)$ and $v(t)$ . We know that the (10) is the reverse SDE of the following SDE ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{d}\\pmb{x}_{t}=u(t)\\pmb{x}_{t}\\mathrm{d}t+v(t)\\mathrm{d}\\pmb{w}_{t}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\pmb{x}_{0}\\sim p(\\pmb{x})$ and $p_{t}$ is the marginal distribution of $\\pmb{x}_{t}$ . As we showed in the main text, it holds for (11) that ", "page_idx": 15}, {"type": "equation", "text": "$$\np(\\pmb{x}_{0}|\\pmb{x}_{t})\\propto\\exp\\left(-g(\\pmb{x}_{0})-\\frac{1}{2\\sigma(t)^{2}}\\|\\pmb{x}_{0}-\\pmb{x}_{t}/s(t)\\|_{2}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "As (10) is the time-reversed process of (11), they share the same path distribution and thus the same conditional distribution $p(\\mathbf{\\boldsymbol{x}}_{0}|\\mathbf{\\boldsymbol{x}}_{t})$ . So, if we set $\\mathbf{\\dot{x}}_{t^{*}}=s(t^{*})z^{(k)}$ , we have that ", "page_idx": 15}, {"type": "equation", "text": "$$\np(\\pmb{x}_{0}|\\pmb{x}_{t^{*}})\\propto\\exp\\left(-g(\\pmb{x}_{0})-\\frac{1}{2\\sigma(t^{*})^{2}}\\|\\pmb{x}_{0}-\\pmb{z}^{(k)}\\|_{2}^{2}\\right)\\propto\\exp\\left(-g(\\pmb{x}_{0})-\\frac{1}{2\\rho^{2}}\\|\\pmb{x}_{0}-\\pmb{z}^{(k)}\\|_{2}^{2}\\right),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which is the desired conditional distribution of the prior step. Therefore, solving (10) from $t=t^{*}$ to $t=0$ is equivalent to taking a prior step. ", "page_idx": 15}, {"type": "text", "text": "Due to Proposition A.2 and Proposition A.3, the SDEs (9) and (10) implement the two desired conditional distributions in SGS. In PnP-DM, the prior step involves a network that approximates the score function of the prior distribution, i.e. $\\pmb{s}_{t}\\approx\\nabla\\log{p_{t}}$ , so the continuous-time process for the actual update is ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{d}\\pmb{x}_{t}=\\left[u(t)\\pmb{x}_{t}-v(t)^{2}\\pmb{s}_{t}\\left(\\pmb{x}_{t}\\right)\\right]\\mathrm{d}t+v(t)\\mathrm{d}\\pmb{\\bar{w}}_{t}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We can then interpolate PnP-DM by considering a dynamic that alternates between running (9) and (12). ", "page_idx": 15}, {"type": "text", "text": "Since each likelihood step takes 1 unit of time and each prior step takes $t^{*}$ unit of time, the total time of the interpolating process for $K$ iterations of PnP-DM is $T:=K(t^{*}+1)$ . We use $\\tau$ to denote the time that has elapsed from initializing PnP-DM with $\\pmb{x}^{(0)}$ . We define $\\{\\nu_{\\tau}\\}$ and $\\{\\pi_{\\tau}\\}$ as the distributions at time $\\tau$ of the non-stationary process initialized at $\\pmb{x}^{(0)}\\sim\\nu_{0}^{X}$ (Figure 3 top) and the stationary process initialized at $x^{(0)}\\sim\\pi^{X}$ (Figure 3 bottom), respectively. Therefore, we have ", "page_idx": 15}, {"type": "text", "text": "A.2 Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Before proving our main result, we present a key lemma for our analysis, which quantifies the time-derivative of the KL divergence in terms of the Fisher information along a pair of general diffusion processes. ", "page_idx": 16}, {"type": "text", "text": "Lemma A.4. Given the following pair of diffusion processes ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\mathbf{\\boldsymbol{x}}_{t}=b(\\mathbf{\\boldsymbol{x}}_{t},t)\\mathrm{d}t+c(t)\\mathrm{d}\\mathbf{\\boldsymbol{w}}_{t}}\\\\ {\\mathrm{d}\\widetilde{\\mathbf{\\boldsymbol{x}}}_{t}=\\widetilde{b}(\\widetilde{\\mathbf{\\boldsymbol{x}}}_{t},t)\\mathrm{d}t+c(t)\\mathrm{d}\\mathbf{\\boldsymbol{w}}_{t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $b(\\cdot,\\cdot):\\mathbb{R}^{n}\\times\\mathbb{R}\\to\\mathbb{R}^{n}$ , $\\widetilde{b}(\\cdot,\\cdot):\\mathbb{R}^{n}\\times\\mathbb{R}\\to\\mathbb{R}^{n},$ , and $c(\\cdot):\\mathbb{R}\\rightarrow\\mathbb{R}$ . Let $\\mu_{t}$ be the distribution of $\\pmb{x}_{t}$ initialized with $\\mathbf{\\boldsymbol{x}}_{0}\\sim\\mu_{0}$ f or (13), and let ${\\widetilde{\\mu_{t}}}$ be the distribution of $\\widetilde{\\pmb{x}}_{t}$ initialized with $\\widetilde{\\pmb{x}}_{0}\\sim\\widetilde{\\mu}_{0}$ for (14). Then we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\partial_{t}\\mathsf{K L}(\\mu_{t}||\\widetilde{\\mu}_{t})\\le-\\frac{c(t)^{2}}{4}\\mathsf{F l}\\left(\\mu_{t}||\\widetilde{\\mu}_{t}\\right)+\\frac{1}{c(t)^{2}}\\int\\left\\|\\widetilde{b}_{t}-b_{t}\\right\\|_{2}^{2}\\mu_{t}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof of Lemma A.4. Writing $b(\\cdot,t)$ as $b_{t}$ and $\\widetilde{b}(\\cdot,t)$ as $\\widetilde{b}_{t}$ , by the Fokker-Planck equations of (13) and (14), we have that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\partial_{t}\\mu_{t}=\\ensuremath{\\mathrm{div}}\\left[\\left(\\frac{c(t)^{2}}{2}\\nabla\\log\\mu_{t}-b_{t}\\right)\\mu_{t}\\right]\\quad\\mathrm{and}\\quad\\partial_{t}\\widetilde{\\mu}_{t}=\\ensuremath{\\mathrm{div}}\\left[\\left(\\frac{c(t)^{2}}{2}\\nabla\\log\\widetilde{\\mu}_{t}-\\widetilde{b}_{t}\\right)\\widetilde{\\mu}_{t}\\right].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Defining $\\phi(x):=x\\log x$ and $\\begin{array}{r}{\\phi^{\\prime}(x)=\\frac{\\mathrm{d}}{\\mathrm{d}x}\\phi(x)=\\log x+1}\\end{array}$ , we can calculate ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{I}(\\mathrm{Sre}|\\{x_{j}\\}=\\Delta t)\\,g\\left(\\frac{\\mathrm{Pe}}{\\Delta x}\\right)}\\\\ &{=\\int\\mathrm{Pe}\\left(\\frac{\\mathrm{Pe}}{\\Delta x}\\right)\\left(\\phantom{_{j}}\\right)\\left(\\phantom{_{k}}\\;\\!\\!\\!-\\frac{1}{2}\\rho\\sigma_{\\mathrm{eff}}^{2}\\left(\\frac{\\mathrm{Pe}}{\\Delta x}\\right)\\!\\!\\!-\\frac{1}{2}\\rho\\left(\\frac{\\mathrm{Pe}}{\\Delta x}\\right)\\Delta x\\,\\right.}\\\\ &{=\\int\\mathrm{Pe}\\left(\\frac{\\mathrm{Pe}}{\\Delta x}\\right)\\left(\\phantom{_{j}}\\right)\\left(\\phantom{_{k}}\\!\\!\\!\\!-\\left(\\frac{\\mathrm{Pe}}{\\Delta x}\\!\\!\\!-\\!\\!\\!1_{j}\\sigma_{\\mathrm{eff}}^{2}\\left(\\frac{\\mathrm{Pe}}{\\Delta x}\\right)\\!\\!\\!-\\!\\!\\!1_{j}\\sigma_{\\mathrm{eff}}^{2}\\left(\\frac{\\mathrm{Pe}}{\\Delta x}\\right)\\!\\!\\!-\\!\\!\\!1_{j}\\sigma_{\\mathrm{eff}}^{3}\\left(\\frac{\\mathrm{Pe}}{\\Delta x}\\right)\\!\\!\\!-\\!\\!\\!1_{j}\\sigma_{\\mathrm{eff}}^{4}\\left(\\frac{\\mathrm{Pe}}{\\Delta x}\\right)\\!\\!\\!-\\!\\!\\!1_{j}\\sigma_{\\mathrm{eff}}^{5}\\left(\\frac{\\mathrm{Pe}}{\\Delta x}\\right)\\!\\!\\!-\\!\\!\\!1_{j}\\sigma_{\\mathrm{eff}}^{6}\\left(\\frac{\\mathrm{Pe}}{\\Delta x}\\right)\\right)d x}\\\\ &{\\quad+\\int\\mathrm{Pe}\\left(\\frac{\\mathrm{Pe}}{\\Delta x}\\right)\\left(\\phantom{_{j}}\\right)\\mathrm{Ar}\\left(\\phantom{_{k}}\\;\\!\\!\\!-\\!\\!\\!1_{j}\\sigma_{\\mathrm{eff}}^{2}\\left(\\frac{\\mathrm{Pe}}{\\Delta x}\\right)\\!\\!\\!-\\!\\!\\!1_{j}\\sigma_{\\mathrm{eff}}^{2}\\left(\\frac{\\mathrm{Pe}}{\\Delta x}\\right)\\!\\!\\!-\\!\\!\\!1_{j}\\sigma_{\\mathrm{eff}}^{4}\\left(\\frac{\\mathrm{Pe}}{\\Delta x}\\right)\\!\\!\\!-\\!\\!\\!1_{j}\\sigma_{\\mathrm{eff}}^{2}\\left(\\frac{\\mathrm{Pe}}{\\Delta x}\\right)\\!\\!\\!-\\!\\!\\!1_{j}\\sigma_{\\mathrm{eff}}^ \n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where we used the fact that $\\begin{array}{r}{-\\frac{1}{2}a^{2}-a b\\leq-\\frac{1}{4}a^{2}+b^{2},\\forall a,b\\in\\mathbb{R}}\\end{array}$ for the inequality. ", "page_idx": 16}, {"type": "text", "text": "Now we are ready to prove Theorem 3.1. ", "page_idx": 17}, {"type": "text", "text": "Proof of Theorem 3.1. We first consider the likelihood steps over $K$ iterations of PnP-DM. Applying Lemma 2 of [78] to the likelihood steps (9) of the non-stationary and stationary processes, we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\partial_{\\tau}\\mathsf{K L}(\\pi_{\\tau}||\\nu_{\\tau})=-\\frac{\\rho^{2}}{2}\\mathsf{F l}(\\pi_{\\tau}||\\nu_{\\tau})\\leq-\\frac{\\rho^{2}}{4}\\mathsf{F l}(\\pi_{\\tau}||\\nu_{\\tau}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for $\\tau\\in[k(t^{*}+1),k(t^{*}+1)+1]$ with $k=0,...,K-1$ . Integrating both sides over $\\tau\\in[k(t^{*}+$ $1),k(t^{*}+1)+1]$ , we get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\int_{k(t^{*}+1)}^{k(t^{*}+1)+1}\\mathsf{F l}\\left(\\pi_{\\tau}||\\nu_{\\tau}\\right)\\mathrm{d}\\tau=\\frac{4[\\mathsf{K L}(\\pi^{X}||\\nu_{k}^{X})-\\mathsf{K L}(\\pi^{Z}||\\nu_{k}^{Z})]}{\\rho^{2}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for $k=0,...,K-1$ . ", "page_idx": 17}, {"type": "text", "text": "Then, applying Lemma A.4 to the prior steps (12) with ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{b({\\boldsymbol x}_{t},t):=u(t){\\boldsymbol x}_{t}-{\\boldsymbol v}(t)^{2}\\nabla\\log p_{t}\\left({\\boldsymbol x}_{t}\\right)}\\\\ &{\\widetilde{b}({\\boldsymbol x}_{t},t):=u(t){\\boldsymbol x}_{t}-{\\boldsymbol v}(t)^{2}{\\boldsymbol s}_{t}\\left({\\boldsymbol x}_{t}\\right)}\\\\ &{\\qquad c(t):={\\boldsymbol v}(t)}\\\\ &{\\qquad\\quad\\delta:=\\displaystyle\\operatorname*{inf}_{t\\in[0,t^{*}]}{\\boldsymbol v}(t),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\partial_{\\tau}\\mathsf{K L}(\\pi_{\\tau}||\\nu_{\\tau})\\le-\\frac{v(\\tau)^{2}}{4}\\mathsf{F l}\\left(\\pi_{\\tau}||\\nu_{\\tau}\\right)+\\frac{1}{v(\\tau)^{2}}\\int\\left\\|v(\\tau)^{2}\\left(\\pmb{s}_{\\tau}-\\nabla\\log p_{\\tau}\\right)\\right\\|_{2}^{2}\\pi_{\\tau}}\\\\ {\\displaystyle\\le-\\frac{v(\\tau)^{2}}{4}\\mathsf{F l}\\left(\\pi_{\\tau}||\\nu_{\\tau}\\right)+v(\\tau)^{2}\\int\\left\\|\\pmb{s}_{\\tau}-\\nabla\\log p_{\\tau}\\right\\|_{2}^{2}\\pi_{\\tau}}\\\\ {\\displaystyle\\le-\\frac{\\delta^{2}}{4}\\mathsf{F l}\\left(\\pi_{\\tau}||\\nu_{\\tau}\\right)+v(\\tau)^{2}\\mathbb{E}_{\\pi_{\\tau}}\\left\\|\\pmb{s}_{\\tau}-\\nabla\\log p_{\\tau}\\right\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for $\\tau\\ \\in\\ [k(t^{*}+1)+1,(k+1)(t^{*}+1)]$ with $k\\;=\\;0,...,K\\;-\\;1$ . Integrating both sides over $\\tau\\in[k(t^{*}+1)+1,(k+1)(t^{*}+1)]$ , we get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\int_{k(t^{*}+1)+1}^{(k+1)(t^{*}+1)}\\mathsf{F l}\\left(\\pi_{\\tau}\\|\\nu_{\\tau}\\right)\\mathrm{d}\\tau\\leq\\frac{4[\\mathsf{K L}(\\pi^{Z}||\\nu_{k}^{Z})-\\mathsf{K L}(\\pi^{X}||\\nu_{k+1}^{X})]}{\\delta^{2}}+\\frac{4\\epsilon_{\\mathrm{score}}}{\\delta^{2}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\epsilon_{\\mathrm{score}}:=\\int_{k(t^{*}+1)+1}^{(k+1)(t^{*}+1)}v(\\tau)^{2}\\mathbb{E}_{\\pi_{\\tau}}\\left\\|s_{\\tau}-\\nabla\\log p_{\\tau}\\right\\|_{2}^{2}\\mathrm{d}\\tau=\\int_{1}^{t^{*}+1}v(\\tau)^{2}\\mathbb{E}_{\\pi_{\\tau}}\\left\\|s_{\\tau}-\\nabla\\log p_{\\tau}\\right\\|_{2}^{2}\\mathrm{d}\\tau.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Finally, combining (16) and (17) for $k=0,...,K-1$ , we obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int_{0}^{T}\\mathsf{F l}\\left(\\pi_{\\tau}||\\nu_{\\tau}\\right)\\mathrm{d}\\tau\\leq\\frac{4[\\mathsf{K L}\\left(\\pi^{X}||\\nu_{0}^{X}\\right)-\\mathsf{K L}\\left(\\pi^{X}||\\nu_{K}^{X}\\right)]}{\\operatorname*{min}(\\rho,\\delta)^{2}}+\\frac{4K\\epsilon_{\\mathrm{score}}}{\\delta^{2}}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{4\\mathsf{K L}\\left(\\pi^{X}||\\nu_{0}^{X}\\right)}{\\operatorname*{min}(\\rho,\\delta)^{2}}+\\frac{4K\\epsilon_{\\mathrm{score}}}{\\delta^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The proof is concluded by dividing $T=K(t^{*}+1)$ on both sides. ", "page_idx": 17}, {"type": "text", "text": "A.3 Discussion ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "To facilitate the discussion, we first present the following proposition. ", "page_idx": 17}, {"type": "text", "text": "Proposition A.5. Define a weighting function $\\lambda(\\tau)$ over $\\tau\\in[0,T]$ such that for $k=0,...,K-1$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\lambda(\\tau)=\\binom{\\rho^{2}}{v(\\tau)^{2}}\\quad i f\\tau\\in[k(t^{*}+1),k(t^{*}+1)+1],\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then, under the same settings of Theorem 3.1, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\int_{0}^{T}\\lambda(\\tau)\\mathsf{F l}\\left(\\pi_{\\tau}||\\nu_{\\tau}\\right)\\mathrm{d}\\tau=\\frac{4\\mathsf{K L}(\\pi^{X}||\\nu_{0}^{X})}{K(t^{\\ast}+1)}+\\frac{4\\epsilon_{s c o r e}}{t^{\\ast}+1}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\begin{array}{r}{\\epsilon_{s c o r e}:=\\int_{1}^{t^{*}+1}v(\\tau)^{2}\\mathbb{E}_{\\pi_{\\tau}}\\|s_{\\tau}-\\nabla\\log p_{\\tau}\\|_{2}^{2}\\mathrm{d}\\tau.}\\end{array}$ ", "page_idx": 18}, {"type": "text", "text": "Proof. With the definition of $\\lambda(\\tau)$ , we can apply Lemma 2 of [78] to the likelihood steps and obtain ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\int_{k(t^{*}+1)}^{k(t^{*}+1)+1}\\lambda(\\tau)\\mathsf{F}\\mathsf{I}\\left(\\pi_{\\tau}||\\nu_{\\tau}\\right)\\mathsf{d}\\tau=4[\\mathsf{K L}(\\pi^{X}||\\nu_{k}^{X})-\\mathsf{K L}(\\pi^{Z}||\\nu_{k}^{Z})]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for $k=0,...,K-1$ . Similarly, we can apply Lemma A.4 to the prior steps and obtain ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\int_{k(t^{*}+1)+1}^{(k+1)(t^{*}+1)}\\lambda(\\tau)\\mathsf{F}!\\left(\\pi_{\\tau}||\\nu_{\\tau}\\right)\\mathrm{d}\\tau\\leq4[\\mathsf{K L}(\\pi^{Z}||\\nu_{k}^{Z})-\\mathsf{K L}(\\pi^{X}||\\nu_{k+1}^{X})]+4\\epsilon_{\\mathrm{score}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\epsilon_{\\mathrm{score}}:=\\int_{k(t^{*}+1)+1}^{(k+1)(t^{*}+1)}v(\\tau)^{2}\\mathbb{E}_{\\pi_{\\tau}}\\left\\|s_{\\tau}-\\nabla\\log p_{\\tau}\\right\\|_{2}^{2}\\mathrm{d}\\tau=\\int_{1}^{t^{*}+1}v(\\tau)^{2}\\mathbb{E}_{\\pi_{\\tau}}\\left\\|s_{\\tau}-\\nabla\\log p_{\\tau}\\right\\|_{2}^{2}\\mathrm{d}\\tau.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Together, for $\\tau\\in[0,T]$ . We can then get (18) by combining (19) and (20) for $k=0,...,K-1$ and dividing by $T:=K(t^{*}+1)$ . \u53e3 ", "page_idx": 18}, {"type": "text", "text": "Unlike Theorem 3.1, this proposition calculates the weighted average of the Fisher information along the two processes with the weighting function $\\lambda(\\tau)$ . The bound in Theorem 3.1 on the unweighted average of Fisher information can be obtained by further lower-bounding the left hand side of (18) using the infimum of $\\lambda(\\tau)$ over $\\tau\\,\\in\\,[0,T]$ . Given this observation, we can see the role of $\\delta$ in Theorem 3.1. With a strictly positive $\\delta$ , the weighting function $\\lambda(\\tau)$ is always strictly positive, so the (unweighted) average Fisher information must converge to 0. This is precisely the case for the VP- and VE-SDE [62]. On the other hand, if $\\delta\\,=\\,0$ , the Fisher information F $\\left(\\pi_{\\tau}||\\nu_{\\tau}\\right)$ may be increasingly large as $\\lambda(\\tau)$ gets closer to 0. For iDDPM and EDM, this could happen near $t=0$ in the reverse diffusion at $v(0)=0$ for these diffusion processes. Nevertheless, we can instead consider a slightly adjusted diffusion coefficient $\\tilde{v}(t):=v(t)+\\epsilon$ with $\\epsilon>0$ . Using the relation between scores and diffusions $\\operatorname{div}(p\\nabla\\log p)=\\Delta p$ , we get the following reverse SDE which has the same law as (7) at each $t$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{l}x_{t}=\\left[\\frac{\\dot{s}(t)}{s(t)}x_{t}+\\left(\\frac{\\epsilon^{2}}{2}-2s(t)^{2}\\dot{\\sigma}(t)\\sigma(t)\\right)\\nabla\\log p\\left(\\frac{x_{t}}{s(t)};\\sigma(t)\\right)\\right]\\mathrm{d}t+\\left(s(t)\\sqrt{2\\dot{\\sigma}(t)\\sigma(t)}+\\epsilon\\right)\\mathrm{d}\\bar{w}_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In this case, $\\tilde{v}(t)=s(t)\\sqrt{2\\dot{\\sigma}(t)\\sigma(t)}+\\epsilon$ is strictly positive, so the convergence on the unweighted average Fisher information is also guaranteed. ", "page_idx": 18}, {"type": "text", "text": "B Inverse problem setup ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Data usage We list the data we have used for our experiments: ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "\u2022 For the synthetic prior experiment, we took images from the CelebA dataset [44], turned them into grayscale, rescaled them to $[-1,1]$ , and resized them to $32\\times32$ pixels for efficient computation. We then found the empirical mean and covariance of the images to construct the Gaussian image prior. The test image was randomly drawn from this Gaussian prior. \u2022 For the benchmark experiments, we used the first 100 images (index 00000 to 00099) in the FFHQ dataset [37]. For all linear inverse problems, the test images were in RGB and normalized to range $[-1,1]$ . For all nonlinear problems, the test images were in grayscale and normalized to range $[0,1]$ . \u2022 For the black hole experiments, we used the simulated data used in [63] and the publicly available EHT 2017 data1 that was used to produce the first image of the M87 black hole. ", "page_idx": 18}, {"type": "text", "text": "Gaussian and motion deblur The forward model is defined as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\pmb{y}\\sim\\mathcal{N}(B\\pmb{x},\\sigma_{y}^{2}\\pmb{I})\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $B\\in\\mathbb{R}^{n\\times n}$ is a circulant matrix that effectively implements a convolution with kernel $^k$ under the circular boundary condition. For the Gaussian deblurring problem, we fixed the kernel $^k$ as a Gaussian kernel with standard deviation 3.0 and size $61\\times61$ . For the motion deblurring problem, we randomly generated the kernel $^k$ for each test image using the code2 with intensity of 0.5 and size $61\\times61$ . For fair comparison, the blur kernel for each test image was set the same for all compared methods. ", "page_idx": 19}, {"type": "text", "text": "Super-resolution The forward model is defined as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\pmb{y}\\sim\\mathcal{N}(P_{f}\\pmb{x},\\sigma_{\\pmb{y}}^{2}\\pmb{I})\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $P_{f}\\in\\mathbb{R}^{\\frac{n}{f}\\times n}$ is a matrix that implements a block averaging fliter to downscale the images by a factor of $f$ . Specifically, we set $f=4$ and used the SVD implementation from the code3 of [38]. ", "page_idx": 19}, {"type": "text", "text": "Coded diffraction patterns (CDP) CDP is a measurement model originally proposed in [10]. The target $\\textbf{\\em x}$ is illuminated by a coherent source and modulated by a phase mask $_{D}$ . The light field then undergoes the far-field Fraunhofer diffraction and is measured by a standard camera. Mathematically, the forward model of CDP is defined as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\pmb{y}\\sim\\mathcal{N}(|\\pmb{F}\\pmb{D}\\pmb{x}|,\\sigma_{\\pmb{y}}^{2}\\pmb{I})\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\pmb{F}$ denotes the 2D Fourier transform. We followed [74] to set $_{D}$ as a diagonal matrix with entries drawn randomly from the complex unit circle. ", "page_idx": 19}, {"type": "text", "text": "Fourier phase retrieval We adopted a similar setting as [18]. In particular, the forward model is defined as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\pmb{y}\\sim\\mathcal{N}(|\\pmb{F}\\pmb{P}\\pmb{x}|,\\sigma_{\\pmb{y}}^{2}\\pmb{I}),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $_{P}$ denotes the oversampling matrix that effectively pads $\\textbf{\\em x}$ in 2D matrix form with zeros. We considered a $4\\times$ oversampling ratio for grayscale images of size $256\\times256$ , so $_{P x}$ has a size of $512\\times512$ . ", "page_idx": 19}, {"type": "text", "text": "Black hole imaging We adopted the same BHI setup as in [63, 65]. The relationship between the black hole image and each interferometric measurement, or so-called visibility, is given by ", "page_idx": 19}, {"type": "equation", "text": "$$\nV_{a,b}^{t}=g_{a}^{t}g_{b}^{t}\\cdot e^{-i(\\phi_{a}^{t}-\\phi_{b}^{t})}\\cdot F_{a,b}^{t}(\\pmb{x})+\\eta_{a,b}\\in\\mathbb{C},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $a$ and $b$ denote a pair of telescopes, $t$ represents the time of measurement acquisition, $i$ is the imaginary unit, and ${\\pmb F}_{a,b}^{t}({\\pmb x})$ is the Fourier component of the image $\\textbf{\\em x}$ corresponding to the baseline between telescopes $a$ and $b$ at time $t$ . In practice, there are three main sources of noise in (21): gain error $g_{a}$ and $g_{b}$ at the telescopes, phase error $\\phi_{a}^{t}$ and $\\phi_{b}^{t}$ , and baseline-based additive white Gaussian noise $\\eta_{a,b}$ . The gain and phase errors stem from atmospheric turbulence and instrument miscalibration and often cannot be ignored. To correct for these two errors, multiple noisy visibilities can be combined into data products that are invariant to these errors, which are called closure phase and log closure amplitude measurements [12] ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{y}_{t,(a,b,c)}^{\\mathrm{cph}}=\\angle(V_{a,b}V_{b,c}V_{a,c}):=\\mathcal{A}_{t,(a,b,c)}^{\\mathrm{cph}}(\\pmb{x}),}\\\\ &{\\pmb{y}_{t,(a,b,c,d)}^{\\mathrm{logcamp}}=\\log\\left(\\frac{|V_{a,b}^{t}||V_{c,d}^{t}|}{|V_{a,c}||V_{b,d}^{t}|}\\right):=\\mathcal{A}_{t,(a,b,c,d)}^{\\mathrm{logcamp}}(\\pmb{x}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\angle$ computes the angle of a complex number. Given a total of $M$ telescopes, there are in total $\\frac{(M{-}1)(\\bar{M^{}}{-}2)}{2}$ closure phase and M(M2\u22123)log closure amplitude measurements at time t, after eliminating repetitive measurements. In our experiments, we used a 9-telescope array $M=9,$ ) from the Event Horizon Telescope (EHT) and constructed the data likelihood term based on these nonlinear closure quantities. Additionally, because the closure quantities do not constrain the total flux (i.e. summation of the pixel values) of the underlying black hole image, we added a constraint on the total flux in the likelihood term. The overall potential function of the likelihood is given by ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "equation", "text": "$$\nf(\\boldsymbol{x};\\boldsymbol{y})=\\sum_{t,\\boldsymbol{\\mathsf{c}}}\\frac{\\|\\boldsymbol{A}_{t,\\boldsymbol{\\mathsf{c}}}^{\\mathrm{cph}}(\\boldsymbol{x})-\\boldsymbol{y}_{t,\\boldsymbol{\\mathsf{c}}}^{\\mathrm{cph}}\\|_{2}^{2}}{2\\sigma_{\\mathrm{cph}}^{2}}+\\sum_{t,\\boldsymbol{\\mathsf{d}}}\\frac{\\|\\boldsymbol{A}_{t,\\boldsymbol{\\mathsf{d}}}^{\\mathrm{logcamp}}(\\boldsymbol{x})-\\boldsymbol{y}_{t,\\boldsymbol{\\mathsf{d}}}^{\\mathrm{logcamp}}\\|_{2}^{2}}{2\\sigma_{\\mathrm{logcamp}}^{2}}+\\frac{\\|\\sum_{i}\\boldsymbol{x}_{i}-\\boldsymbol{y}^{\\mathrm{fux}}\\|_{2}^{2}}{2\\sigma_{\\mathrm{fux}}^{2}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In this equation, $y^{\\mathrm{{flux}}}$ is the total flux of the underlying black hole, which can be accurately measured. We use $\\bar{\\pmb{y}}\\;:=\\;(\\bar{\\pmb{y}}^{\\mathsf{c p h}},\\pmb{y}^{\\mathsf{l o g c a m p}},\\pmb{y}^{\\mathsf{f l u x}})$ to denote all the measurements and c, d as the indices for the closure phase and log closure amplitude measurements. Parameters $\\sigma_{\\mathsf{c p h}}$ , \u03c3logcamp were given by the telescope system and $\\sigma_{\\mathrm{flux}}$ was set to $\\sqrt{2}$ in our experiments to constrain the total flux. The data mismatch metric reported in Figure 8 is defined as the sum of the reduced $\\chi^{2}$ values for the closure phase and log closure amplitude measurements, which are calculated using the ehtim.obsdata.Obsdata.chisq function of the ehtim package4. Both $\\chi^{2}$ values should ideally be around 1 for data with high signal-to-noise ratio (SNR). Therefore, a data mismatch value around 2 to 3 is considered as fitting the measurements well. ", "page_idx": 20}, {"type": "text", "text": "C Technical details of PnP-DM ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "C.1 Likelihood step ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Linear forward model and Gaussian noise As we showed in the main paper, in case of linear forward models and Gaussian noise, the likelihood step is ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\pi^{Z|X=x}=\\mathcal{N}(\\pmb{m}(\\pmb{x}),\\pmb{\\Lambda}^{-1})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathbf{A}:=A^{T}\\Sigma^{-1}A+\\frac{1}{\\rho^{2}}I}\\end{array}$ and $\\begin{array}{r}{m(x):={\\mathbf{A}}^{-1}(A^{T}{\\mathbf{\\Sigma}}^{-1}y^{+}\\frac{1}{\\rho^{2}}{\\mathbf{x}})}\\end{array}$ . The bottleneck here is that both the mean and the covariance involve the matrix inverse $\\Lambda^{-1}$ , which can be prohibitive to compute directly for high-dimensional problems. Nevertheless, the computational cost can be significantly alleviated when the noise is i.i.d. Gaussian, i.e. $\\Sigma=\\sigma_{y}^{2}\\pmb{I}$ , and $\\pmb{A}$ can be efficiently decomposed. For example, if one can efficiently calculate the SVD of the forward model $\\pmb{A}$ , i.e. $\\pmb{A}=\\pmb{U}\\pmb{S}\\pmb{V}^{T}$ , one can find the Cholesky decomposition of $\\Lambda^{-1}$ as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\Lambda^{-1}=L L^{T}\\quad\\mathrm{where}\\quad L:=V\\left(\\frac{1}{\\sigma^{2}}S^{2}+\\frac{1}{\\rho^{2}}I\\right)^{-1/2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since $\\boldsymbol{S}$ is a diagonal matrix, the second term can be calculated with only $O(n)$ complexity. Then, leveraging the property of multivariate Gaussian distribution, we can sample $\\pmb{\\eta}\\sim\\mathcal{N}(\\mathbf{0},\\pmb{I})$ and calculate $z=m(x)\\!+\\!L\\eta$ as a sample that exactly follows the target Gaussian distribution $\\mathcal{N}(m({\\pmb x}),{\\pmb\\Lambda}^{-1})$ . An analogous derivation with Fourier transform can be done when $\\pmb{A}$ is a circulant convolution matrix. ", "page_idx": 20}, {"type": "text", "text": "Nonlinear forward model We provide the pseudocode of the LMC algorithm for sampling the likelihood step with general differentiable forward models in Algorithm 2. ", "page_idx": 20}, {"type": "table", "img_path": "Xq9HQf7VNV/tmp/07e00e36986857b8b1cefa8a38f5169e85be6fbea2e0a6ea1b5347d78d891a10.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Table 3 summarizes the hyperparameters we used for solving the nonlinear inverse problems considered in this work. ", "page_idx": 20}, {"type": "table", "img_path": "Xq9HQf7VNV/tmp/e6e278250a837c4fcb97ea53ebd1b8db97623183383ac6f7693d4b9daf2d1fce.jpg", "table_caption": ["Table 3: List of hyperparameters for the likelihood step of PnP-DM "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "C.2 Prior step ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The EDM framework We formally introduce the EDM formulation [36] using our notations. The forward diffusion process is defined as the following linear It\u00f4 SDE ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{d}\\pmb{x}_{t}=u(t)\\pmb{x}_{t}\\mathrm{d}t+v(t)\\mathrm{d}\\pmb{w}_{t},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $u(t):\\mathbb{R}\\rightarrow\\mathbb{R}$ , $v(t):\\mathbb{R}\\rightarrow\\mathbb{R}$ are the drift and diffusion coefficients. The generative process is the time-reversed version of (23). According to [2], it is another It\u00f4 SDE of the form ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{d}\\pmb{x}_{t}=\\left[u(t)\\pmb{x}_{t}-v(t)^{2}\\nabla_{\\pmb{x}_{t}}\\log p_{t}(\\pmb{x}_{t})\\right]\\mathrm{d}t+v(t)\\mathrm{d}\\pmb{\\bar{w}}_{t},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $p_{t}(\\pmb{x}_{t})$ is the marginal distribution of $\\pmb{x}_{t}$ . There also exists a reverse probability flow ODE ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{d}\\pmb{x}_{t}=\\left[u(t)\\pmb{x}_{t}-\\frac{1}{2}v(t)^{2}\\nabla_{\\pmb{x}_{t}}\\log p_{t}(\\pmb{x}_{t})\\right]\\mathrm{d}t,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which shares the same marginal distributions as (24). Based on (23), we have ", "page_idx": 21}, {"type": "equation", "text": "$$\np(\\pmb{x}_{t}|\\pmb{x}_{0})=\\mathcal{N}(s(t)\\pmb{x}_{0},s(t)^{2}\\sigma(t)^{2}\\pmb{I})\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $p(\\mathbf{\\boldsymbol{x}};\\sigma(t))$ is the distribution obtained by adding i.i.d. Gaussian noise of standard deviation $\\sigma(t))$ to the prior data. The idea of the EDM formulation is to write the reverse diffuison directly in terms of the scaling and noise level of $\\pmb{x}_{t}$ with respect to $\\pmb{x}_{0}$ , which are more important than the drift and diffusion coefficients. With the relations between $u(t),v(t),$ $p_{t}$ and $s(t),\\sigma(\\bar{t}),p(\\cdot;\\sigma(t))$ , we can rewrite (24) and (25) as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{d}x_{t}=\\left[\\frac{\\dot{s}(t)}{s(t)}x_{t}-2s(t)^{2}\\dot{\\sigma}(t)\\sigma(t)\\nabla_{x_{t}}\\log p\\left(\\frac{x_{t}}{s(t)};\\sigma(t)\\right)\\right]\\mathrm{d}t+s(t)\\sqrt{2\\dot{\\sigma}(t)\\sigma(t)}\\mathrm{d}\\bar{w}_{t}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{d}\\pmb{x}_{t}=\\left[\\frac{\\dot{s}(t)}{s(t)}\\pmb{x}_{t}-s(t)^{2}\\dot{\\sigma}(t)\\sigma(t)\\nabla_{\\pmb{x}_{t}}\\log p\\left(\\frac{\\pmb{x}_{t}}{s(t)};\\sigma(t)\\right)\\right]\\mathrm{d}t.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Note that (26) is precisely the SDE (7) we considered for the prior step. Finally, due to the Tweedie\u2019s formula [24], we can approximate $\\nabla_{\\mathbf{\\boldsymbol{x}}_{t}}\\log p\\left(\\,\\cdot\\,;\\boldsymbol{\\sigma}(t)\\right)$ by a denoiser $[\\bar{D}_{\\theta}({}\\cdot{};\\sigma({}\\cdot{}))-{}\\cdot{}]/\\sigma(t)^{2}$ trained to minimize the $\\ell_{2}$ error of a denoising objective. Substituting the score function by the approximation with the denoiser and using the chain rule, we can further rewrite (26) and (27) as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{d}x_{t}=\\left[\\left(\\frac{2\\dot{\\sigma}(t)}{\\sigma(t)}+\\frac{\\dot{s}(t)}{s(t)}\\right)x_{t}-\\frac{2\\dot{\\sigma}(t)s(t)}{\\sigma(t)}D_{\\theta}\\left(\\frac{x_{t}}{s(t)};\\sigma(t)\\right)\\right]\\mathrm{d}t+s(t)\\sqrt{2\\dot{\\sigma}(t)\\sigma(t)}\\mathrm{d}\\bar{w}_{t}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\,\\mathrm{d}x_{t}=\\left[\\left({\\frac{\\dot{\\sigma}(t)}{\\sigma(t)}}+{\\frac{\\dot{s}(t)}{s(t)}}\\right)x_{t}-{\\frac{\\dot{\\sigma}(t)s(t)}{\\sigma(t)}}D_{\\theta}\\left({\\frac{x_{t}}{s(t)}};\\sigma(t)\\right)\\right]\\mathrm{d}t.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Pseudocode We provide the pseudocode for our prior step in Algorithm 3. Note that the update rule is precisely the Euler discretization of (28) and (29). The discretization time steps $\\{t_{i}\\}_{i=0}^{N^{\\mathrm{~\\!~}}}$ , scaling schedule $s(\\cdot)$ , and noise schedule $\\sigma(\\cdot)$ , are kept the same as in Table 1 of [36]. For all experiments, we set the total number of time steps to 100, i.e. $N=100$ . We note that this does not imply that each prior step has a number of function evaluations (NFE) equal to 100. Since $\\rho$ is to a small value as the algorithm runs, the number of steps in later iterations of the algorithm are much fewer than 100. The prior step is similar to the image synthesis process in SDEdit [49] that starts from the middle of the reverse diffusion process. We used the pf-ODE solver for the CDP problem and the SDE solver for all other problems. Part of our code implementation is based on the repository5. ", "page_idx": 21}, {"type": "text", "text": "Input: noisy image $z\\in\\mathbb{R}^{n}$ , assumed noise level $\\rho>0$ , pretrained model $D_{\\theta}({}\\cdot{};{}\\cdot{})$ that approximates $\\nabla\\log p\\left(\\mathbf{\\boldsymbol{x}};\\boldsymbol{\\sigma}\\right)$ with $(D_{\\theta}({\\pmb x};\\sigma)-{\\pmb x})/\\sigma_{.}^{2}$   \nHyperparameter: discretization time steps $\\{t_{i}\\}_{i=0}^{N}$ (monotonically decreasing to $t_{N}=0$ ), scaling schedule $s(\\cdot)$ , noise schedule $\\sigma(\\cdot)$ , solver (SDE or pf-ODE) 1: $i^{*}\\gets$ smallest $i$ such that $\\sigma(t_{i})\\leq\\rho$ $\\triangleright$ Find the starting point of the reverse diffusion 2: ${\\pmb v}_{i^{*}}\\gets s(t_{i^{*}}){\\pmb z}$ $\\triangleright$ Initialize at time $t_{i^{*}}$ 3: for $i=i^{*},\\cdot\\cdot\\cdot,N-1$ do 4: $\\lambda\\leftarrow2$ if solver is SDE else 1 5: $\\begin{array}{r}{\\hat{d}_{i}\\gets\\Big(\\frac{\\lambda\\dot{\\sigma}\\left(t_{i}\\right)}{\\sigma\\left(t_{i}\\right)}+\\frac{\\dot{s}\\left(t_{i}\\right)}{s\\left(t_{i}\\right)}\\Big)v_{i}-\\frac{\\lambda\\dot{\\sigma}\\left(t_{i}\\right)s\\left(t_{i}\\right)}{\\sigma\\left(t_{i}\\right)}D_{\\theta}\\left(\\frac{v_{i}}{s\\left(t_{i}\\right)};\\sigma\\left(t_{i}\\right)\\right)}\\end{array}$ 6: $\\boldsymbol{v}_{i+1}\\leftarrow\\boldsymbol{v}_{i}+(t_{i+1}-t_{i})\\boldsymbol{d}_{i}$ \u25b7Drift 7: if $i\\neq N-1$ and solver is SDE then 8: $\\begin{array}{r}{\\dot{v}_{i+1}\\leftarrow v_{i+1}+s(t)\\sqrt{2\\dot{\\sigma}(t)\\sigma(t)(t_{i}-t_{i+1})}\\epsilon_{i}\\quad\\mathrm{where}\\quad\\epsilon_{i}\\sim\\mathcal{N}(\\mathbf{0},I)}\\end{array}$ \u25b7Diffusion 9: end if ", "page_idx": 22}, {"type": "text", "text": "Model checkpoint For experiments with FFHQ color images, we used the pretrained checkpoint from [16] available at the repository6. For experiments with synthetic data, FFHQ grayscale images, and black hole images, we trained our own models using the same repository. The model network is based on the U-Net architecture in [51] with BigGAN [9] residual blocks, multi-resolution attention, and multi-head attention with fixed channels per head. See the appendix of [16] for architecture details. Specifically, we changed the input and output channels to 1 and 2, respectively, to accommodate grayscale inputs, and reduced the number of down-pooling and up-pooling levels in the U-Net for smaller images. We trained all models until convergence using an exponential moving average (EMA) rate of 0.9999, 32-bit precision, and the AdamW optimizer [45]. Here is a list of training data we used for each model: ", "page_idx": 22}, {"type": "text", "text": "\u2022 For the Gaussian prior model, we randomly generated images from the constructed Gaussian prior distribution.   \n\u2022 For the FFHQ grayscale model, we used the images with index 01000 to 69999 in the FFHQ dataset.   \n\u2022 For the black hole model, we used 3068 simulated black hole images from the GRMHD simulation, which stands for general relativistic magnetohydrodynamic simulation [22]. See Figure 9 for some example training images. We applied data augmentation with random flipping and resizing, so that the flux spin rotation and the ring diameter vary from sample to sample. ", "page_idx": 22}, {"type": "image", "img_path": "Xq9HQf7VNV/tmp/0c9f092a7dd0004fd50629d76fe7db38a5aefad38a7d8329737b018c8e6a8c6c.jpg", "img_caption": ["Figure 9: Example images from the dataset for training the black hole diffusion model prior. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Preconditioning Since the checkpoints we used are all trained based on the DDPM (or VP-SDE) formulation [33], we converted them to the denoiser $D_{\\theta}$ under the EDM formulation via the VP preconditioning [36]. Specifically, if we denote the pretrained model as $F_{\\theta}({}\\cdot{};{}\\cdot{})$ , the model we used for Algorithm 3 is ", "page_idx": 22}, {"type": "equation", "text": "$$\nD_{\\theta}({\\pmb x};\\sigma):=c_{\\mathrm{skip}}(\\sigma){\\pmb x}+c_{\\mathrm{out}}(\\sigma)F_{\\theta}\\left(c_{\\mathrm{in}}(\\sigma){\\pmb x};c_{\\mathrm{noise}}(\\sigma)\\right),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "is the inverse of the VP-SDE noise schedule defined as $\\sigma_{\\mathsf{V P}}(t):=\\sqrt{e^{\\frac{1}{2}\\beta_{\\mathsf{d}}t^{2}+\\beta_{\\mathrm{min}}t}-1}$ with $\\beta_{\\mathrm{d}}=19.9$ and $\\beta_{\\mathrm{min}}=0.1$ . This adaption allows us to make a fair comparison with other DM-based methods using the same pretrained models. One can also incorporate DMs trained with other formulations into PnP-DM by properly setting the preconditioning parameters. ", "page_idx": 22}, {"type": "text", "text": "Connection to DDS-DDPM in [77] A concurrent work [77] introduced a rigorous implementation of the prior step, called DDS-DDPM, by converting the DDPM [33] (or VP-SDE [62]) sampler into a reverse diffusion based on the VE-SDE [62]. The diffusion process after the conversion can be used to solve (6) rigorously by properly choosing the starting point. In fact, our formulation admits DDS-DDPM as a special case with the VP preconditioning and reverse diffusion based on the VE-SDE. Here we explicitly show this connection. For the VE-SDE, we have $s_{\\mathsf{V E}}(t)\\,=\\,1$ , $\\sigma_{\\vee\\mathsf E}(t)\\,=\\,\\sqrt{t}$ , $\\boldsymbol{u}\\mathsf{v}\\mathsf{E}(t)=0$ , and $\\boldsymbol{v}_{\\mathsf{V}\\mathsf{E}}(t)=1$ . So (28) becomes ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathrm{d}\\pmb{x}_{t}=\\left[\\frac{1}{t}\\pmb{x}_{t}-\\frac{1}{t}D_{\\theta}\\left(\\pmb{x}_{t};\\sqrt{t}\\right)\\right]\\mathrm{d}t+\\mathrm{d}\\bar{\\pmb{w}}_{t}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Applying the VP preconditioning (30) to (31), we obtain ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathrm{d}\\pmb{x}_{t}=\\left[\\frac{1}{\\sqrt{t}}F_{\\theta}\\left(\\frac{\\pmb{x}_{t}}{\\sqrt{t+1}};999\\sigma_{\\mathsf{V P}}^{-1}(\\sqrt{t})\\right)\\right]\\mathrm{d}t+\\mathrm{d}\\bar{\\pmb{w}}_{t}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We can then rescale the time range from [0, 1] to $[0,1000]$ , discretize (32) backward in time over the time steps $\\{\\tau_{t}\\}$ from [77], and apply the exponential integrator [82] to the drift term, resulting in the following update rule: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\widehat{\\pmb{x}}_{t-1}=\\widehat{\\pmb{x}}_{t}-2(\\sqrt{\\tau_{t}}-\\sqrt{\\tau_{t-1}})F_{\\theta}\\left(\\frac{\\widehat{\\pmb{x}}_{t}}{\\sqrt{\\tau_{t}+1}};\\sigma_{\\mathsf{v P}}^{-1}(\\sqrt{\\tau_{t}})\\right)+\\sqrt{\\tau_{t}-\\tau_{t-1}}\\epsilon\\quad\\mathrm{where~}\\epsilon\\sim\\mathcal{N}(\\mathbf{0},I)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Based on the definition $\\tau_{t}:=\\bar{\\alpha}_{t}^{-1}-1=\\sigma\\mathsf{v p}(t)^{2}$ in DDS-DDPM, we get ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\widehat{\\mathbf{x}}_{t-1}=\\widehat{\\mathbf{x}}_{t}-2(\\sqrt{\\tau_{t}}-\\sqrt{\\tau_{t-1}})F_{\\theta}\\left(\\sqrt{\\bar{\\alpha}_{t}}\\widehat{\\mathbf{x}}_{t};t\\right)+\\sqrt{\\tau_{t}-\\tau_{t-1}}\\epsilon\\quad\\mathrm{where~}\\epsilon\\sim\\mathcal{N}(\\mathbf{0},I)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This is exactly the update rule of DDS-DDPM with $F_{\\theta}({}\\cdot{};t)$ denoting the noise estimate $\\widehat{\\epsilon}_{t}(\\cdot)$ of DDPM. One can also verify that the initialization in DDS-DDPM is equivalent to ours by c h ecking that \u03b1\u00aft \u2265 $\\begin{array}{r}{\\bar{\\alpha}_{t}\\geq\\frac{1}{\\eta^{2}+1}}\\end{array}$ \u03b721+1 is equivalent to \u03c4t = \u03c3VP(t)2 \u2264\u03b72 where \u03b7 \u2261\u03c1 is the assumed noise level in (6). As one can see, DDS-DDPM is equivalent to our prior step by choosing the VP-preconditioning, VE reverse diffusion, and a particular integration scheme. In fact, our prior step allows for more general definitions of diffusion processes and includes both the ODE and SDE solvers. ", "page_idx": 23}, {"type": "text", "text": "C.3 Others ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Annealing schedule for $\\rho$ In this work, we considered an exponential annealing schedule for the coupling strength $\\rho$ . We note that the schedule can be more general than exponential decay and we leave the investigation of other decay schedules in future work. Specifically, we specified a starting level $\\rho_{0}$ , decay rate $\\alpha$ , and a minimum value $\\rho_{\\mathrm{min}}$ . Then we set ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\rho_{k}=\\operatorname*{max}(\\alpha^{k}\\rho_{0},\\rho_{\\operatorname*{min}})\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for $k=0,\\cdot\\cdot\\cdot\\,,K-1$ . Table 4 summarizes the annealing hyperparameters that we used for all the inverse problems considered in this work. ", "page_idx": 23}, {"type": "table", "img_path": "Xq9HQf7VNV/tmp/602f70c9513b3fc47cf7ce77bb66b5b0e4b93fbf6f31fa93043b9ef8ddbd4211.jpg", "table_caption": ["Table 4: List of hyperparameters for the annealing schedule of $\\rho$ in PnP-DM "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Initialization For the linear inverse problems, we used the zero initialization, i.e. ${\\pmb x}^{(0)}\\,=\\,{\\bf0}\\,\\in$ $\\mathbb{R}^{n}$ . For the CDP and Fourier phase retrieval problems, we used the Gaussian initialization, i.e. $\\pmb{x}^{(0)}\\sim\\mathcal{N}(\\mathbf{0},\\pmb{I})$ . For black hole imaging experiments, we used the uniform random initialization between 0 and 1 for each pixel. We found that PnP-DM, as an MCMC algorithm, is insensitive to the initialization. Except for the black hole experiments where we found the negative values would cause problems, any reasonable initialization would lead to comparable results. This observation corroborates our convergence result, which holds for any initialization $\\nu_{0}^{X}$ . ", "page_idx": 23}, {"type": "text", "text": "Number of iterations We ran 500 iterations for the synthetic prior experiments, 200 iterations for the black hole experiments, and 100 for all other experiments. The numbers were chosen so that the algorithm was fully converged. ", "page_idx": 24}, {"type": "text", "text": "Sample collection To collect multiple samples using our method, there are two main approaches: (1) Run a single Markov chain and collect samples after a certain number of iterations, known as the burn-in period, to ensure the chain has converged. (2) Run several independent Markov chains and collect one sample from each chain after convergence. The first approach is more efficient, but the collected samples are not entirely independent and thus may have a small effective sample size. The second approach ensures all samples are fully independent but takes longer to run. In our experiments, we used the first approach for all tests involving $256\\times256$ images to enhance efficiency. Specifically, we set the burn-in period to 40 iterations and collected 20 random samples from the remaining 60 iterations (one every 3 iterations). For other experiments, due to the smaller image sizes, we employed the second approach to obtain fully independent samples. ", "page_idx": 24}, {"type": "text", "text": "Compute All experiments were performed on NVIDIA RTX A6000 and A100 GPUs. The runtime per image depends on several factors, such as the choice of GPU, the total number of iterations and the coupling strength schedule $\\{\\rho_{k}\\}$ (as it takes more network evaluations for larger $\\rho$ for our EDM-based denoiser). In our actual experiments, we ran each image for at least 100 iterations to ensure convergence, which took around 1 minute for a single Markov chain. Here we present a comparison of computational efficiency with the major baselines on a linear super-resolution and a nonlinear coded diffraction patterns problem in Table 5. The clock time in seconds and number of function evaluations (NFE) are calculated for each method to measure its computational efficiency. All hyperparameters are kept the same for each method as those used for Table 1 and Table 2 in the manuscript. As expected, DM-based approaches (DDRM & DPS) generally yield shorter runtimes due to their lower NFEs. Nevertheless, our PnP-DM method significantly outperforms these methods while achieving comparable runtimes with DPS $(\\approx1.5\\times)$ , despite its larger NFEs $(\\approx3\\times)$ . This is primarily due to two factors: 1) PnP-DM avoids running the full diffusion process by adapting the starting noise level to $\\rho_{k}$ at each iteration, and 2) the runtime is further reduced by using an annealing schedule of $\\rho_{k}$ . We also note that the runtime reported for DDRM and DPS below is the time it takes to generate one sample. For the linear inverse problem experiments, where we generated 20 samples for each sampling method, PnP-DM was faster than DPS because we took 20 samples that PnP-DM generated along one Markov chain of batch size 1 (hence same runtime as below, around 50 seconds) but DPS requires running a diffusion process with batch size 20, which was significantly slower (around 330 seconds). ", "page_idx": 24}, {"type": "table", "img_path": "Xq9HQf7VNV/tmp/41b88e73b4e592d84d01259d8ba1d1bb002d08d0e048cbb97401a374b969f4f0.jpg", "table_caption": ["Table 5: Comparison of computational efficiency between $\\mathrm{PnP}$ -DM and other baseline methods "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "D Implementation details of baseline methods ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "PnP-ADMM We set the ADMM penalty parameter as 2 and ran for 500 iterations to ensure convergence. We used the pretrained DnCNN denoiser [80] available at the deepinv library7. ", "page_idx": 24}, {"type": "text", "text": "DPIR We followed the annealing schedule in [79] and ran for 40 iterations. We used the pretrained DRUNet denoiser [81] available at the deepinv library. ", "page_idx": 24}, {"type": "text", "text": "DDRM We ran all the experiments with the default parameters: $\\eta_{B}=1.0$ , $\\eta=0.85$ , and 20 steps for the DDIM sampler [58]. For the Gaussian deblur problem, we used the SVD-based forward model implementation based on separable 1D convolution. We ran it with an additional batch dimension to collect multiple samples. ", "page_idx": 24}, {"type": "text", "text": "DPS We followed the original paper to use a 1000-step DDPM sampler backbone. For the linear inverse problems, we used the step size given in [17], i.e. $\\zeta^{\\prime}=1$ . For the nonlinear inverse problems, we optimized the step size $\\zeta^{\\prime}$ by performing a grid search, which led to $\\zeta^{\\prime}=3$ for CDP and Fourier phase retrieval and $\\zeta^{\\prime}=0.001$ for black hole imaging. For the synthetic prior experiments, we also optimized the step size and used $\\zeta^{\\prime}=0.1$ for compressed sensing and $\\bar{\\zeta^{\\prime}}=1$ for Gaussian deblur. We ran it with an additional batch dimension to collect multiple samples. ", "page_idx": 25}, {"type": "text", "text": "PnP-SGS We performed a grid search for the coupling parameter $\\rho$ and found that $\\rho=0.1$ worked the best for all problems. We followed the practice in [19] to have a burn-in period of 20 iterations during which the reverse diffusion is early-stopped. We ran the algorithm for 100 iterations in total and collect 20 samples in the 80 iterations after the burn-in period. ", "page_idx": 25}, {"type": "text", "text": "DPnP We implemented the DDS-DDPM sampler for the prior step. For fair comparison, we used the same annealing schedule for the coupling strength (denoted as $\\eta_{k}$ in [77]) as PnP-DM. We ran it for the same number of iterations for each inverse problem with the same way of collecting samples as our method. ", "page_idx": 25}, {"type": "text", "text": "HIO We set $\\beta=0.7$ and applied both the non-negative constraint and the finite support constraint. To mitigate the instability of reconstruction depending on initialization, we first repeatedly ran the algorithm with 100 different random initializations and chose the reconstruction that has the best measurement fit. Then we ran another 10,000 iterations with the chosen reconstruction to ensure convergence and report the metrics on the final reconstruction. ", "page_idx": 25}, {"type": "text", "text": "E Additional related works ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Image reconstruction with plug-and-play priors Plug-and-Play priors $(\\mathrm{PnP})$ [68] is an algorithmic framework that leverages off-the-shelf denoisers for solving imaging inverse problems. Recognizing the equivalence between the proximal operator and finding the maximum a posteriori $(M A P)$ solution to a denoising problem, PnP substitutes the proximal update in many optimization algorithms, such as ADMM [13, 55] and HQS [81, 79], with generic denoising algorithms, particularly those based on deep learning [48, 81, 79]. The $\\mathrm{PnP}$ framework enjoys both convergence guarantees [64, 55] and strong empirical performance [75, 1] due to its compatibility with stateof-the-art learning-based denoising priors. Recent works have also proposed learning-based PnP frameworks that have direction interpretations from an optimization perspective [20, 26]. See [35] for a comprehensive review on the theory and practice of PnP. ", "page_idx": 25}, {"type": "text", "text": "Posterior sampling with MCMC and learning priors Learning-based priors have also been considered in the Bayesian context, where one seeks to sample the posterior distribution defined under a learned prior. An important technique is denoising score matching (DSM) [69], which connects image denoising with learning the score function of an image distribution. Based on DSM, prior works have incorporated deep denoising priors into MCMC formulations, particularly focusing the Langevin Monte Carlo and its variants as they involve the score function of the target distribution [39, 34, 40, 65, 53]. Recently, methods based on SGS have also gained increasing popularity [52, 19, 6, 27, 77]. Unlike $\\mathrm{PnP}$ methods based on optimization, these sampling methods possess the ability to generate diverse solutions and quantify the uncertainty of solution space. ", "page_idx": 25}, {"type": "text", "text": "Solving inverse problems with diffusion models The remarkable performance of diffusion models [33, 62] on modelling image distributions makes them desirable choices as images priors for solving inverse problems. One popular approach is to leverage a pretrained unconditional model and modify the reverse diffusion process during inference to enforce data consistency [72, 18, 17, 59, 38, 60, 61, 8, 43, 83, 57]. Despite of the promising performance of these methods, they usually involve approximations and empirically driven designs that are hard to justify theoretically and may lead to inconsistent sample distributions. Another line of work learns task-specific models, which achieves higher accuracy at the cost of re-training models for new problems [4, 56, 43]. Methods based on Particle Filtering and Sequential Monte Carlo are also considered to ensure asymptotic consistency [11, 23, 73]. Diffusion models have also been considered as a prior for variational inference [28, 46] and pluy-and-play image reconstruction [32, 47]. ", "page_idx": 25}, {"type": "text", "text": "F Additional experimental results ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "F.1 Synthetic prior experiment ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In addition to the compressed sensing experiment presented in the main paper, we show another comparison on a Gaussian deblurring problem in Figure 10. Here, the linear forward model $A\\in\\bar{\\mathbb{R}^{m\\times n}}$ is a 2D convolution matrix with a Gaussian blur kernel of size $7\\!\\times\\!7$ and standard deviation 3.0. Similar to the compressed sensing experiment, both methods yield accurate reconstructions of the mean. However, in terms of the posterior standard deviation, DPS exhibits a notable difference from the ground truth, whereas our method achieves a significantly more accurate result. ", "page_idx": 26}, {"type": "text", "text": "F.2 Linear inverse problems ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We provide visual comparisons for the Gaussian deblur and super-resolution problems in Figure 11. ", "page_idx": 26}, {"type": "image", "img_path": "Xq9HQf7VNV/tmp/fd478d81e7617868ec45b666ec2b5e07fcb4990ecc50586564e48f4ac8fea88a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 10: Comparison of our method and DPS [17] on estimating the posterior distribution of a Gaussian deblurring problem under a Gaussian prior. While the mean estimations of the two methods are of roughly the same quality, our approach provides a much more accurate estimation of the posterior per-pixel standard deviation than DPS. ", "page_idx": 26}, {"type": "image", "img_path": "Xq9HQf7VNV/tmp/70e7f0a889e161a2f0b2a0871a74995892fc6376d03ea0c27de73e8e567cf365.jpg", "img_caption": ["Figure 11: Visual comparison between our method and baselines on solving the Gaussian deblurring and super-resolution problems with i.i.d. Gaussian noise $(\\sigma_{y}=0.05)$ ). We visualize one sample generated by each algorithm. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Additional visual examples are provided in Figure 12 (Gaussian deblurring), Figure 13 (motion deblurring), and Figure 14 (super-resolution). ", "page_idx": 26}, {"type": "text", "text": "F.3 Nonlinear inverse problems ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We provide visual comparisons for the CDP reconstruction problem in Figure 15, where we visualize one sample for each method. As shown by the red zoom-in boxes, PnP-DM can recover fine-grained features such as the hair threads that are missing in the reconstructions by the baselines. Additional reconstruction examples are given in Figure 16 for the CDP reconstruction problem. ", "page_idx": 26}, {"type": "text", "text": "We then show some additional reconstruction examples with comparison to DPS in Figure 17. For each method, we visualize the best reconstruction out of four runs for each test image according to the PSNR value. While DPS failed on around half of the test images, our proposed method provided high-fidelity reconstructions on almost all test images. This comparison highlights the better robustness of our method over DPS. ", "page_idx": 26}, {"type": "image", "img_path": "Xq9HQf7VNV/tmp/1357367596834b704c87ebe765381c4dab6351c6e3fbaa84b9bdb407f445bd14.jpg", "img_caption": ["Figure 12: Additional visual examples for the Gaussian deblurring problem. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "Xq9HQf7VNV/tmp/731c7341d34226cbce9dfff5249b9a4dead127912120502bbad555c4441d119f.jpg", "img_caption": ["Figure 13: Additional visual examples for the motion deblurring problem. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "F.4 Black hole imaging ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Finally, we present visual examples from the black hole imaging experiments. Samples generated by PnP-DM (EDM) and DPS using the simulated data are shown in Figure 18, with the data mismatch metric labeled at the top right corner of each sample. Consistent with the results in Figure 8, DPS can only capture one of the two posterior modes. DPS samples from Mode 2 and Mode 3 significantly deviate from the measurements and lack the expected black hole structure. In contrast, PnP-DM successfully samples both posterior modes and consistently produces samples that fit the measurements well. Additionally, Figure 19 presents more samples obtained by applying PnP-DM to the real M87 black hole data. The generated samples are not only diverse but also fti the measurements well with data mismatch values around 2. These samples exhibit a ring diameter consistent with the official EHT reconstruction in Figure 1 and share a common bright spot location at the lower half of the ring. ", "page_idx": 27}, {"type": "image", "img_path": "Xq9HQf7VNV/tmp/e53d1f0f9c7656908c85089c4e46eeaa918aad48890291d2b6fb42326db44113.jpg", "img_caption": ["Figure 15: Visual comparison between our method and baselines on solving the coded diffraction pattern (CDP) reconstruction problems with i.i.d. Gaussian noise $(\\sigma_{y}=0.05)$ . We visualize one sample generated by each algorithm. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "F.5 Further analysis ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Sensitivity analysis on the annealing schedule $\\{\\rho_{k}\\}$ In Figure 20, we present a sensitivity analysis on the annealing schedule $\\{\\rho_{k}\\}$ . In particular, we show the PSNR curves of $\\pmb{x}_{k}$ with different exponential decay rates $\\alpha$ (left) and minimum coupling levels $\\rho_{\\mathrm{min}}$ (right) for one linear (super-resolution) and one nonlinear (coded diffraction patterns) problem. We have the following conclusions based on the results. First, different decay rates lead to different rates of convergence, which corroborates with our theoretical insights that $\\rho$ plays the same role as the step size. The final level of PSNR is not sensitive to different decay rates, as all curves converge to the same level. Second, as $\\rho_{\\mathrm{min}}$ decreases, the final PSNR becomes higher. This is as expected because the stationary distribution of the $\\pmb{x}_{k}$ , $\\pi^{{\\dot{X}}}$ should converge to the true target posterior, $\\bar{p}(\\pmb{x}|\\pmb{y})$ , as $\\rho$ decreases. ", "page_idx": 28}, {"type": "text", "text": "Convergence curves with intermediate visual examples In Figure 21, we show some visual examples of intermediate $\\pmb{x}_{k}$ and $z_{k}$ iterates (left) and convergence plots of PSNR, SSIM, and LPIPS for $\\pmb{x}_{k}$ (right) on the super-resolution problem. As $\\rho_{k}$ decreases, $\\pmb{x}_{k}$ becomes closer to the ground truth and $z_{k}$ gets less noisy. Both the visual quality and metric curves stabilize after the minimum coupling strength is achieved. Despite being run for 100 iterations in total, our method generates good images in around 40 iterations, which is around 30 seconds and 1600 NFEs. ", "page_idx": 28}, {"type": "image", "img_path": "Xq9HQf7VNV/tmp/245f6ca6cf287fe6844a837d3066d6c0deb72beb105f561911ad1539a6797ee3.jpg", "img_caption": ["Figure 16: Additional visual examples for the Fourier phase retrieval problem. "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "Xq9HQf7VNV/tmp/f8870149674d31ae36875fcbb8ab42ee0ae08825017344e6d82299c370948305.jpg", "img_caption": ["Figure 17: Additional visual examples for the Fourier phase retrieval problem. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "G Licenses ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We list the licenses of all the assets we used in this paper: ", "page_idx": 29}, {"type": "text", "text": "\u2022 Data \u2013 CelebA [44]: Unknown ", "page_idx": 29}, {"type": "image", "img_path": "Xq9HQf7VNV/tmp/0e97aac8e21b6ec8246ab73673d4f8397c5cafb2e6bbbbee1074a2e7367ef338.jpg", "img_caption": ["Figure 18: Additional visual examples given by PnP-DM and DPS using the simulated black hole data. ", ""], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "Xq9HQf7VNV/tmp/1779c4f8255bd64e8d9a6f5cf672a672473e7eed63575b2a5a578e86e4cc6a83.jpg", "img_caption": ["Figure 19: Additional visual examples given by PnP-DM using the real M87 black hole data. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "\u2013 FFHQ [37]: Creative Commons BY-NC-SA 4.0 license \u2013 Simulated and real black hole data: Unknown   \n\u2022 Code \u2013 The license for each code repository that we have used is listed in the footnote after the repository link.   \n\u2022 Pretrained model \u2013 FFHQ model by Choi et al. [16]: MIT license ", "page_idx": 30}, {"type": "image", "img_path": "Xq9HQf7VNV/tmp/bc2b012cdd13f5de272dbdc099a3df744757493e8c21d873836952346df44475.jpg", "img_caption": [], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "Figure 20: Sensitivity analysis on the annealing schedule $\\rho_{k}$ with different decay rates $\\alpha$ (left) and minimum coupling strength $\\rho_{\\mathrm{min}}$ (right) for a linear (super-resolution) and a nonlinear (coded diffraction patterns) inverse problem. Recall from Appendix C.3 that $\\rho_{k}:=\\operatorname*{max}(\\alpha^{k}\\rho_{0},\\rho_{\\operatorname*{min}})$ , where we set $\\rho_{0}=10$ for this experiment. ", "page_idx": 31}, {"type": "image", "img_path": "Xq9HQf7VNV/tmp/37d1f511b35a8394bcff5a40a481ac3098735133eaed1946be10aa7b9826ea3e.jpg", "img_caption": ["Figure 21: Visual examples of intermediate $\\pmb{x}_{k}$ and $\\scriptstyle z_{k}$ iterates (left) and convergence plots of PSNR, SSIM, and LPIPS for $\\pmb{x}_{k}$ iterates (right) on the super-resolution problem. The vertical dashed lines show the iterations at which the $\\pmb{x}_{k}$ and $\\scriptstyle z_{k}$ iterates are visualized. "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We have made sure that the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 32}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: We have added a paragraph on the limitations of our method to the conclusion section. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 32}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We have presented our theoretical result in the main paper with the proof in the appendix. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 33}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We have provided all the necessary details to reproduce the main results in our paper. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 33}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We have included the code for reproducing the main experimental results of our work in the supplemental material. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 34}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: We have provided all the training and test details in the appendix. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 34}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: Due to the limited space and constraints of time and computational resources, we were unable to include error bars for our experiments over multiple runs. Alternatively, we evaluated our method and baselines based on multiple samples and various metrics. We conducted a uncertainty quantification analysis where we calculated the per-pixel statistics and confidence intervals. We also extensively included visual examples to better demonstrate the performance of the methods we considered in this paper. All of these efforts aimed to better reflect the significance of our improvement over existing methods. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 34}, {"type": "text", "text": "\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 35}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We have provided details on our compute resources in the appendix. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 35}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We confirm that our research conform with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 35}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We have added a \u201cBroarder Impacts\u201d section in the main manuscript. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 36}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: Our work does not involve models that have a high risk for misuse. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 36}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We have properly credited the owners of all the assets used in the paper. Their licenses are listed in the footnotes and Appendix G. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 37}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We have documented the code we provide in the supplement. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 37}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: Our research does not involve crowdsourcing. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 37}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: Our research does not involve human subjects. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 37}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 38}]