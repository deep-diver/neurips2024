{"references": [{"fullname_first_author": "Jonathan Frankle", "paper_title": "The lottery ticket hypothesis: Finding sparse, trainable neural networks", "publication_date": "2018-00-00", "reason": "This paper introduces the Lottery Ticket Hypothesis, a foundational concept for the field of sparse neural networks which is central to the current work."}, {"fullname_first_author": "Decebal Constantin Mocanu", "paper_title": "Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science", "publication_date": "2018-00-00", "reason": "This paper proposes Dynamic Sparse Training (DST), a key methodology used in the current research to improve training efficiency of sparse networks."}, {"fullname_first_author": "Utku Evci", "paper_title": "Rigging the lottery: Making all tickets winners", "publication_date": "2020-00-00", "reason": "This paper introduces RigL, a significant advancement in DST which addresses challenges in training convergence and is compared against in the current work."}, {"fullname_first_author": "K. Bhatia", "paper_title": "The extreme classification repository: Multi-label datasets and code", "publication_date": "2016-00-00", "reason": "This paper provides a valuable resource of benchmark datasets for Extreme Multi-label Classification (XMC), the domain of focus for the current research."}, {"fullname_first_author": "Rohit Babbar", "paper_title": "Speeding-up one-versus-all training for extreme classification via mean-separating initialization", "publication_date": "2022-00-00", "reason": "This paper presents a related work which improves the efficiency of training for extreme multi-label classification and uses many of the same datasets, allowing for direct comparison."}]}