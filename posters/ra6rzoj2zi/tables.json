[{"figure_path": "RA6rzOJ2zI/tables/tables_5_1.jpg", "caption": "Table 1: Statistics of XMC Datasets with and without Label Features. This table presents a comparison across various datasets, detailing the total number of training instances (N), unique labels (L), number of test instances (N'), average label count per instance (L), and average data points per label (\u00ce).", "description": "This table provides a statistical overview of several extreme multi-label classification (XMC) datasets used in the paper.  It shows the number of training and testing instances, the total number of labels, the average number of labels per instance, and the average number of instances per label. The datasets are categorized into those with and without label features, offering a comparative analysis of their characteristics.", "section": "3.1 Datasets"}, {"figure_path": "RA6rzOJ2zI/tables/tables_6_1.jpg", "caption": "Table 2: Comparison with different methods. Comparing our sparse model against its dense counterpart and with state-of-the-art XMC methods on Wiki10-31K, Wiki-500K, Amazon-670K and Amazon-3M datasets. Mtr (GiB) indicates peak GPU memory consumption during training.", "description": "This table compares the performance of the proposed Dynamic Sparse Training (DST) method against several baseline methods across four different extreme multi-label classification (XMC) datasets.  The baselines include dense models, dense models with a bottleneck layer, and state-of-the-art XMC methods.  The table shows precision at 1, 3, and 5, along with the peak GPU memory consumption during training for each method and dataset.  The sparsity level of the DST method is also indicated.", "section": "3.2 Baselines and evaluation metrics"}, {"figure_path": "RA6rzOJ2zI/tables/tables_6_2.jpg", "caption": "Table 2: Comparison with different methods. Comparing our sparse model against its dense counterpart and with state-of-the-art XMC methods on Wiki10-31K, Wiki-500K, Amazon-670K and Amazon-3M datasets. Mtr (GiB) indicates peak GPU memory consumption during training.", "description": "This table compares the performance of the proposed Dynamic Sparse Training (DST) method against several baselines on four extreme multi-label classification (XMC) datasets.  The baselines include dense models, dense models with a bottleneck layer, and other state-of-the-art XMC methods.  The table shows precision at 1, 3, and 5, as well as the peak GPU memory consumption during training for each method.  The sparsity level is also indicated for the sparse methods.", "section": "3.3 Empirical performance"}, {"figure_path": "RA6rzOJ2zI/tables/tables_8_1.jpg", "caption": "Table 2: Comparison with different methods. Comparing our sparse model against its dense counterpart and with state-of-the-art XMC methods on Wiki10-31K, Wiki-500K, Amazon-670K and Amazon-3M datasets. Mtr (GiB) indicates peak GPU memory consumption during training.", "description": "This table compares the performance of the proposed SPARTEX model against various baselines on four extreme multi-label classification (XMC) datasets.  Baselines include dense models, dense models with a bottleneck layer, and state-of-the-art XMC methods.  The table shows precision at 1, 3, and 5, and peak GPU memory consumption (Mtr) during training, for various sparsity levels.  It highlights the memory efficiency of the proposed method while maintaining competitive performance.", "section": "3.2 Baselines and evaluation metrics"}, {"figure_path": "RA6rzOJ2zI/tables/tables_9_1.jpg", "caption": "Table 5: Comparison of Fan-in (sparsity) effects on model performance and memory usage for Amazon-670K dataset.", "description": "This table shows the impact of different sparsity levels (controlled by the fan-in parameter) and the use of an auxiliary loss on the performance (P@1, P@3, P@5) and memory usage (Mtr) for the Amazon-670K dataset.  It also provides the training time (Epoch Time) and inference time (Inference Time) for each configuration, demonstrating the trade-off between model sparsity, performance, and resource consumption.", "section": "3.7 Impact of Varying Sparsity Levels"}, {"figure_path": "RA6rzOJ2zI/tables/tables_9_2.jpg", "caption": "Table 6: Comparison of Auxiliary loss cut-off epoch effects on model performance for different Fan-in (sparsity) levels.", "description": "This table shows the impact of varying the cut-off epoch for the auxiliary loss on the model's final performance. Two different sparsity levels (83% and 92%) are considered.  The results are presented in terms of Precision@1 (P@1), Precision@3 (P@3), and Precision@5 (P@5).  It shows that there is an optimal cut-off point for the auxiliary loss, beyond which, performance begins to degrade.", "section": "Experiments and discussion"}, {"figure_path": "RA6rzOJ2zI/tables/tables_9_3.jpg", "caption": "Table 7: Performance comparison between fixed CascadeXML [25] embeddings and end-to-end training with DST on Wiki-500K and Amazon-670K datasets.", "description": "This table compares the performance of models using fixed embeddings (CascadeXML) with end-to-end training using Dynamic Sparse Training (DST) on two datasets: Wiki-500K and Amazon-670K.  It shows that end-to-end training with DST leads to consistent improvements over using fixed embeddings across all metrics (Precision@1, Precision@3, Precision@5). The gains are more significant for Precision@1.", "section": "3.9 DST with Fixed Embedding vs End-to-End Training"}, {"figure_path": "RA6rzOJ2zI/tables/tables_16_1.jpg", "caption": "Table 8: Hyperparameters of our approach to facilitate reproducibility. \"LR\" stands for learning rate.", "description": "This table lists the hyperparameters used in the experiments for different datasets.  It specifies the encoder architecture (BERT Base or DistilBERT), batch size, dropout rate, number of training epochs, learning rates for the encoder and classifier, warmup steps, and sequence length.", "section": "3. Experiments and discussion"}, {"figure_path": "RA6rzOJ2zI/tables/tables_17_1.jpg", "caption": "Table 9: DST and other related hyperparameter settings for different datasets.", "description": "This table details the hyperparameters used in the Dynamic Sparse Training (DST) experiments across different datasets.  It shows the fan-in (sparsity level), pruning mode (fraction or threshold), rewiring threshold and fraction, rewiring interval, use of an auxiliary classifier, auxiliary loss cut-off epoch, and whether an intermediate layer was used and its size. These parameters were adjusted for each dataset to optimize training efficiency and performance.", "section": "3.5 Effect of Rewiring Interval"}]