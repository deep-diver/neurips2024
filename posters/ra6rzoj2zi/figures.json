[{"figure_path": "RA6rzOJ2zI/figures/figures_1_1.jpg", "caption": "Figure 1: Model configurations and performance comparisons at various sparsity levels. The left panel illustrates our model configurations: 'S' represents a semi-structured fixed fan-in sparse layer, 'W' denotes an intermediate layer, and 'Aux' refers to an auxiliary head of meta-classifiers. These configurations help maintain performance as the label space size increases from 31K to 670K and beyond. The right panel demonstrates the comparative precision at 1 for our model against other methods across increasing levels of sparsity on the Amazon670K dataset.", "description": "This figure shows the model architectures used in the paper and their performance compared to other methods.  The left panel illustrates three variations of the model, differing in the presence of an intermediate layer and an auxiliary loss function and the size of the output space. The right panel shows a comparison of precision@1 scores on the Amazon670K dataset across varying sparsity levels, for the three model variations and two baseline approaches.", "section": "1 Introduction"}, {"figure_path": "RA6rzOJ2zI/figures/figures_4_1.jpg", "caption": "Figure 2: Gradient Flow of the encoder during training with and without Auxiliary Objective.", "description": "This figure compares the gradient flow of the encoder during training with and without the auxiliary objective. The plot shows that the introduction of an auxiliary objective results in a significantly larger gradient signal during the initial training phase, which speeds up learning. However, because the task associated with the meta-classifiers differs from the final task, while both share the same encoder, maintaining it throughout the entire training process can deteriorate the encoder's representation quality. Therefore, the auxiliary objective's influence is gradually reduced as training progresses.", "section": "3. Experiments and discussion"}, {"figure_path": "RA6rzOJ2zI/figures/figures_7_1.jpg", "caption": "Figure 3: left: Comparison of performance declines as the size of the label space increases, given a fixed sparsity. right: Performance of our model at different epochs, across various sparsity ratios.", "description": "The figure shows a comparison of performance degradation in DST as the size of the label space increases. The left panel shows the performance drop at a fixed sparsity level (83%) across different label space sizes (31K, 131K, 500K, 670K, 3M). The right panel shows the performance of the model at different epochs and sparsity levels (83%, 92%, and 96%) for the Amazon-670K dataset. It highlights the impact of label space size and sparsity on model performance.", "section": "3 Experiments and discussion"}, {"figure_path": "RA6rzOJ2zI/figures/figures_8_1.jpg", "caption": "Figure 1: Model configurations and performance comparisons at various sparsity levels. The left panel illustrates our model configurations: 'S' represents a semi-structured fixed fan-in sparse layer, 'W' denotes an intermediate layer, and 'Aux' refers to an auxiliary head of meta-classifiers. These configurations help maintain performance as the label space size increases from 31K to 670K and beyond. The right panel demonstrates the comparative precision at 1 for our model against other methods across increasing levels of sparsity on the Amazon670K dataset.", "description": "This figure presents a comparison of different model configurations for dynamic sparse training (DST) and their performance across various sparsity levels. The left panel shows the model architectures with a semi-structured sparse layer ('S'), an intermediate layer ('W'), and an auxiliary head ('Aux'). The right panel shows a graph comparing the precision@1 of the proposed method and other baselines as sparsity increases on the Amazon670K dataset.", "section": "2 Dynamic Sparse Training for Extreme Multi-label Classification"}, {"figure_path": "RA6rzOJ2zI/figures/figures_17_1.jpg", "caption": "Figure 1: Model configurations and performance comparisons at various sparsity levels. The left panel illustrates our model configurations: 'S' represents a semi-structured fixed fan-in sparse layer, 'W' denotes an intermediate layer, and 'Aux' refers to an auxiliary head of meta-classifiers. These configurations help maintain performance as the label space size increases from 31K to 670K and beyond. The right panel demonstrates the comparative precision at 1 for our model against other methods across increasing levels of sparsity on the Amazon670K dataset.", "description": "This figure shows the model architecture used in the paper (left panel) and a comparison of its performance against other methods on the Amazon670K dataset (right panel). The architecture uses a combination of semi-structured sparse layer, intermediate layer, and an auxiliary head, which helps improve performance at high sparsity levels and large output spaces.  The comparison shows that the proposed model achieves competitive precision@1 results.", "section": "2 Dynamic Sparse Training for Extreme Multi-label Classification"}, {"figure_path": "RA6rzOJ2zI/figures/figures_18_1.jpg", "caption": "Figure 1: Model configurations and performance comparisons at various sparsity levels. The left panel illustrates our model configurations: 'S' represents a semi-structured fixed fan-in sparse layer, 'W' denotes an intermediate layer, and 'Aux' refers to an auxiliary head of meta-classifiers. These configurations help maintain performance as the label space size increases from 31K to 670K and beyond. The right panel demonstrates the comparative precision at 1 for our model against other methods across increasing levels of sparsity on the Amazon670K dataset.", "description": "This figure shows the model architectures used in the paper for various sparsity levels and label space sizes. The left panel illustrates the different components of the model, including the semi-structured sparse layer (S), intermediate layer (W), and auxiliary head (Aux).  The right panel compares the precision@1 of the proposed model with other methods, demonstrating its ability to maintain performance at higher sparsity levels and larger label spaces.", "section": "Dynamic Sparse Training for Extreme Multi-label Classification"}, {"figure_path": "RA6rzOJ2zI/figures/figures_19_1.jpg", "caption": "Figure 1: Model configurations and performance comparisons at various sparsity levels. The left panel illustrates our model configurations: 'S' represents a semi-structured fixed fan-in sparse layer, 'W' denotes an intermediate layer, and 'Aux' refers to an auxiliary head of meta-classifiers. These configurations help maintain performance as the label space size increases from 31K to 670K and beyond. The right panel demonstrates the comparative precision at 1 for our model against other methods across increasing levels of sparsity on the Amazon670K dataset.", "description": "This figure shows the model architecture and performance comparison.  The left panel illustrates three model variations: a baseline with only a semi-structured sparse layer (S),  a model adding an intermediate layer (W), and a model using an auxiliary classifier (Aux). The right panel compares the precision@1 of the three variations against a dense model and a static sparsity model across a range of sparsity levels, demonstrating the effectiveness of the proposed architecture in maintaining performance, particularly at high sparsity levels and large label spaces.", "section": "1 Introduction"}]