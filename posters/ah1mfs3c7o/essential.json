{"importance": "This paper is important because it presents **InterControl**, a novel method for generating multi-person interactions using only single-person motion data. This addresses a significant limitation of existing methods and opens up exciting new avenues for research in areas such as virtual reality, animation, and robotics.  The zero-shot capability and high precision of joint control offered by InterControl have **significant implications** for applications involving diverse and complex human interactions.  The annotation-free nature using an LLM reduces the workload for data collection and annotation which is significant for the human motion research community.", "summary": "InterControl: Zero-shot multi-person interaction generation by precisely controlling every joint using only single-person data.", "takeaways": ["InterControl achieves zero-shot multi-person interaction generation using only single-person motion data.", "The method employs precise spatial control of every joint in every person at any time, enhancing realism and flexibility.", "InterControl leverages LLMs for annotation-free interaction description and is compatible with physics-based character simulators."], "tldr": "Current text-conditioned motion synthesis excels in single-person animations but struggles with multi-person interactions due to the complexity of modeling interactions and the lack of large-scale multi-person interaction datasets.  Existing methods often rely on training with fixed numbers of characters, limiting their generalization capabilities.  They also fail to precisely control individual character's joints, which is crucial for generating realistic interactions.\nThis paper introduces InterControl, which tackles these issues by representing interactions as pairs of joints with specified contact or distance constraints.  Instead of training a multi-person model, InterControl adapts a single-person motion generation model to precisely control each joint's position using a novel motion controller and inverse kinematics.  The method successfully generates realistic interactions for an arbitrary number of people in a zero-shot manner, demonstrating promising results on HumanML3D and KIT-ML datasets. This significantly reduces the need for massive multi-person datasets.", "affiliation": "Chinese University of Hong Kong", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "AH1mFs3c7o/podcast.wav"}