[{"figure_path": "MuPlJ9fT4b/figures/figures_1_1.jpg", "caption": "Figure 1: Overview of our framework for data-efficient neural operator learning (with our contributions highlighted in red). Stage 1: Unsupervised pretraining only on unlabeled PDE data. Stage 2: Fine-tuning with reduced simulation costs of PDE data. Stage 3: Test-time in-context examples can improve the neural operator\u2019s out-of-distribution performance, without additional training costs.", "description": "This figure illustrates the three stages of the proposed data-efficient neural operator learning framework. Stage 1 involves unsupervised pretraining on unlabeled PDE data using physics-inspired proxy tasks. Stage 2 performs fine-tuning with reduced simulation costs on labeled PDE data. Finally, stage 3 leverages in-context learning during inference to improve out-of-distribution performance without additional training.  The contributions of the authors are highlighted in red.", "section": "1 Introduction"}, {"figure_path": "MuPlJ9fT4b/figures/figures_4_1.jpg", "caption": "Figure 2: Overview: unsupervised pretraining via MAE and super-resolution. During pre-training, in the input unlabeled PDE data, a random subset (e.g., 70%) of spatial locations are masked, followed by a Gaussian blur. After the encoder and decoder, the full set of input is required to be reconstructed.", "description": "This figure illustrates the unsupervised pretraining framework using Masked Autoencoders (MAE) and super-resolution.  The process begins with input unlabeled PDE data. A portion of the data is randomly masked (e.g., 70%), and a Gaussian blur is applied to the masked regions. This modified data is then passed through an encoder and decoder network. The decoder's task is to reconstruct the original input data from the masked and blurred version, forcing the model to learn robust and invariant features despite data corruption. This method helps the model become more resilient to sparse or noisy data, common in scientific applications.", "section": "3.1 Unsupervised Pretraining"}, {"figure_path": "MuPlJ9fT4b/figures/figures_6_1.jpg", "caption": "Figure 3: Pretraining neural operators on unlabeled PDE data improves its performance and data efficiency on Poisson (a), Helmholtz (b), Reaction-Diffusion (c), and Navier-Stokes (d and e, with relative errors at different unrolled steps shown on f). \"random init.\": models are trained from scratch with random initialization. \u201cvision pretrained (SSv2)\": fine-tuning from the publicly available checkpoint for Video-MAE (pretrained on computer vision dataset SSV2 [21] for video understanding). Savings of the number of simulated PDE data (when \"random init.\" achieves the best test error) are shown in red.", "description": "This figure demonstrates the data efficiency of unsupervised pretraining for neural operators on various PDEs.  It compares the performance of models trained from scratch (\"random init.\") against models with unsupervised pretraining and those fine-tuned from a vision-pretrained model (SSv2). The results show that unsupervised pretraining significantly reduces the number of simulated PDE data needed to achieve comparable or better performance than models trained from scratch, highlighting its data efficiency.  The savings in the number of simulated data points are shown in red.", "section": "4 Empirical Results"}, {"figure_path": "MuPlJ9fT4b/figures/figures_7_1.jpg", "caption": "Figure 3: Pretraining neural operators on unlabeled PDE data improves its performance and data efficiency on Poisson (a), Helmholtz (b), Reaction-Diffusion (c), and Navier-Stokes (d and e, with relative errors at different unrolled steps shown on f). \"random init.\": models are trained from scratch with random initialization. \u201cvision pretrained (SSv2)\": fine-tuning from the publicly available checkpoint for Video-MAE (pretrained on computer vision dataset SSV2 [21] for video understanding). Savings of the number of simulated PDE data (when \"random init.\" achieves the best test error) are shown in red.", "description": "This figure compares the performance of three different neural operator training methods across various PDEs: unsupervised pretraining, training from scratch (\"random init.\"), and fine-tuning a vision-pretrained model.  The results show that unsupervised pretraining using unlabeled data significantly improves performance and reduces the need for simulated data, leading to greater data efficiency.  The figure also highlights that, for the same level of performance, the number of training samples required is substantially less when using unsupervised pretraining, resulting in significant savings of simulation costs.", "section": "4.1 Unsupervised Pretraining Enables Data-Efficient Operator Learning"}, {"figure_path": "MuPlJ9fT4b/figures/figures_8_1.jpg", "caption": "Figure 3: Pretraining neural operators on unlabeled PDE data improves its performance and data efficiency on Poisson (a), Helmholtz (b), Reaction-Diffusion (c), and Navier-Stokes (d and e, with relative errors at different unrolled steps shown on f). \"random init.\": models are trained from scratch with random initialization. \u201cvision pretrained (SSv2)\": fine-tuning from the publicly available checkpoint for Video-MAE (pretrained on computer vision dataset SSV2 [21] for video understanding). Savings of the number of simulated PDE data (when \"random init.\" achieves the best test error) are shown in red.", "description": "This figure demonstrates the data efficiency gains achieved by unsupervised pretraining of neural operators on unlabeled PDE data.  It compares the performance of models trained from scratch (\"random init.\") and those fine-tuned from a vision-pretrained model (SSv2) against models using the unsupervised pretraining method.  The plots show relative l2 error versus the number of training samples for various PDEs (Poisson, Helmholtz, Reaction-Diffusion, Navier-Stokes). The red numbers highlight the significant reduction in the number of simulated PDE data required to achieve comparable or better performance with the unsupervised pretraining method.", "section": "4 Empirical Results"}, {"figure_path": "MuPlJ9fT4b/figures/figures_19_1.jpg", "caption": "Figure 7: Visualizations of architectures we studied. Left: FNO [46]. Right: VideoMAE [77].", "description": "This figure shows the architectures of two neural operator models used in the paper: the Fourier Neural Operator (FNO) and the Video-MAE.  The FNO architecture uses a series of Fourier layers, while the Video-MAE uses a transformer-based encoder-decoder structure. Both are designed for processing spatiotemporal data.  The detailed components of each architecture, including linear layers, activation functions (GeLU), and Fourier transforms, are illustrated. This visualization helps clarify the structural differences between these two prominent deep learning models used in the experiments.", "section": "3.1.4 Model Architectures"}, {"figure_path": "MuPlJ9fT4b/figures/figures_19_2.jpg", "caption": "Figure 8: Comparison between our unsupervised pretraining method versus MoCo v2 [8].", "description": "This figure compares the performance of the proposed unsupervised pretraining method against MoCo v2, a popular contrastive learning method, on the ERA5 dataset.  The x-axis represents the number of training samples used, and the y-axis shows the relative l2 error.  The plot demonstrates that the unsupervised pretraining method consistently achieves lower error rates than MoCo v2 across varying training data sizes, highlighting its superior data efficiency.", "section": "4.2 More Comprehensive Experiments on Real-World Data"}, {"figure_path": "MuPlJ9fT4b/figures/figures_20_1.jpg", "caption": "Figure 9: Pretraining neural operators on unlabeled PDE data improves its performance and data efficiency on Poisson (left), Helmholtz (right). \"random init.\": models are trained from scratch with random initialization. \"vision pretrained\": fine-tuning from the checkpoint pretrained on computer vision dataset ImageNet [11].", "description": "This figure compares the performance of three different training methods for neural operators on Poisson and Helmholtz PDEs.  The \"random init.\" method trains the model from scratch with random weights, showing a clear need for substantial training data to achieve good performance.  The \"vision pretrained\" method uses a model pre-trained on ImageNet, a large computer vision dataset, before fine-tuning it on the PDE data.  This approach shows some improvement over the random initialization, but is still not as efficient as the \"unsupervised\" method. The \"unsupervised\" method uses a model pre-trained on unlabeled PDE data before fine-tuning, showing significant performance gains with far less training data than the other methods. This demonstrates the effectiveness of unsupervised pre-training on the specific domain of PDEs.", "section": "4.1 Unsupervised Pretraining Enables Data-Efficient Operator Learning"}, {"figure_path": "MuPlJ9fT4b/figures/figures_20_2.jpg", "caption": "Figure 10: Joint unsupervised pretraining on multiple PDEs (green solid curve) further improves the data efficiency of neural operators when fine-tuning on Poisson (left), Helmholtz (middle), Reaction-Diffusion (right). \"random init.\": models are trained from scratch with random initialization. \"unsupervised\": models are pretrained on a single unsupervised PDE data. \"unsupervised joint\": models are pretrained on a joint of multiple unsupervised PDE datasets. \"NS\": Navier Stokes. \"RD\": Reaction-Diffusion.", "description": "This figure shows the results of an experiment comparing the performance of neural operators trained with different pretraining strategies.  The green line represents a model pretrained on a combination of unlabeled Poisson, Helmholtz, and Navier-Stokes datasets (joint pretraining), while the other lines show results for models pretrained on single unlabeled datasets or trained from scratch (\"random init.\"). The results demonstrate that joint pretraining leads to superior performance and data efficiency during fine-tuning on various PDEs (Poisson, Helmholtz, and Reaction-Diffusion).", "section": "4 Empirical Results"}, {"figure_path": "MuPlJ9fT4b/figures/figures_20_3.jpg", "caption": "Figure 11: Fine-tuning FNO (pretrained on Poisson) on unseen samples from Helmholtz.", "description": "This figure shows the results of fine-tuning a Fourier Neural Operator (FNO) model, pretrained on the Poisson equation, on the Helmholtz equation.  It compares the performance of the model with unsupervised pretraining on the Poisson data to a model trained from scratch ('random init.') and a model fine-tuned from an ImageNet-pretrained checkpoint. The x-axis represents the number of training samples, and the y-axis represents the relative l2 error. The graph demonstrates that unsupervised pretraining on a related task improves the model's performance on a new, unseen task, requiring fewer training samples than the other models.", "section": "H Fine-tuning on Unseen PDEs is Challenging"}, {"figure_path": "MuPlJ9fT4b/figures/figures_21_1.jpg", "caption": "Figure 3: Pretraining neural operators on unlabeled PDE data improves its performance and data efficiency on Poisson (a), Helmholtz (b), Reaction-Diffusion (c), and Navier-Stokes (d and e, with relative errors at different unrolled steps shown on f). \"random init.\": models are trained from scratch with random initialization. \u201cvision pretrained (SSv2)\": fine-tuning from the publicly available checkpoint for Video-MAE (pretrained on computer vision dataset SSV2 [21] for video understanding). Savings of the number of simulated PDE data (when \"random init.\" achieves the best test error) are shown in red.", "description": "This figure demonstrates the data efficiency and improved performance achieved by pretraining neural operators on unlabeled PDE data.  It compares the performance of models trained from scratch (\"random init.\"), models fine-tuned from a vision-pretrained model (SSv2), and models using the proposed unsupervised pretraining method. The results are shown across four different PDEs, highlighting significant data savings with the unsupervised pretraining approach.", "section": "4.1 Unsupervised Pretraining Enables Data-Efficient Operator Learning"}, {"figure_path": "MuPlJ9fT4b/figures/figures_22_1.jpg", "caption": "Figure 13: Visualization of FNO reconstructions of unlabeled PDE data on the Poisson (\"Pois.\"), Helmholtz (\"Helm.\"), 2D Diffusion-Reaction (\"D.R.\"), and 2D incompressible Navier-Stokes (\"N.S.\") equations during MAE pretraining. (Mask ratio: 0.1 for Poisson, Helmholtz, and 2D Diffusion-Reaction equations; 0.7 for incompressible Navier-Stokes.) In masks, only white areas are visible to the model during pretraining.", "description": "This figure visualizes the reconstruction performance of the masked autoencoder (MAE) pretraining method on four different types of partial differential equations (PDEs): Poisson, Helmholtz, Reaction-Diffusion, and Navier-Stokes.  Each row represents a different PDE. The columns show the original \"source\" data, the \"mask\" applied to the data during training (white areas are visible, black areas are masked), the MAE's \"prediction\" of the complete data, and the \"error\" between the prediction and the original data.  The mask ratio, which indicates the proportion of masked data, varies across the PDEs, reflecting different optimal masking strategies depending on data characteristics.", "section": "K Visualization of MAE Pretraining"}, {"figure_path": "MuPlJ9fT4b/figures/figures_23_1.jpg", "caption": "Figure 13: Visualization of FNO reconstructions of unlabeled PDE data on the Poisson (\"Pois.\"), Helmholtz (\"Helm.\"), 2D Diffusion-Reaction (\"D.R.\"), and 2D incompressible Navier-Stokes (\"N.S.\") equations during MAE pretraining. (Mask ratio: 0.1 for Poisson, Helmholtz, and 2D Diffusion-Reaction equations; 0.7 for incompressible Navier-Stokes.) In masks, only white areas are visible to the model during pretraining.", "description": "This figure visualizes the results of masked autoencoder (MAE) pretraining on four different partial differential equations (PDEs).  The leftmost column shows the original data ('source'), the middle column shows the masked data used for training ('mask'), the third column displays the FNO's reconstruction of the original data from the masked data ('prediction'), and the rightmost column shows the difference between the original and reconstructed data ('error').  The masking ratio, which is the percentage of the data masked for training, was varied between PDEs; a more aggressive masking ratio was used for the 2D incompressible Navier-Stokes equation.", "section": "K Visualization of MAE Pretraining"}, {"figure_path": "MuPlJ9fT4b/figures/figures_24_1.jpg", "caption": "Figure 15: We show snapshot examples from ERA5 temperature [30] (a, b) and ScalarFlow [14] (c, d) at different temporal steps; and also an example of Airfoil mask, velocities, and pressure [75] (e-j).", "description": "This figure shows examples of real-world datasets used in the paper. (a, b) show ERA5 temperature data at different timesteps. (c, d) show ScalarFlow density data at different timesteps. (e-j) show the Airfoil dataset, which includes mask, freestream velocity (x and y directions), pressure, and velocity (x and y directions).  These examples showcase the variety of data used in the paper's experiments, representing different physical processes and levels of complexity.", "section": "4.2 More Comprehensive Experiments on Real-World Data"}, {"figure_path": "MuPlJ9fT4b/figures/figures_25_1.jpg", "caption": "Figure 16: Benefits of in-context examples. To analyze the benefit of in-context examples for complicated PDE systems, we decompose the relative MSE error into \\\"Scale\\\" and \\\"Shape\\\". \\\"Scale\\\" indicates the alignment of the range of model outputs with targets (closer to 1 the better), via the slope of a linear regression. \\\"Shape\\\" indicates the alignment of scale-invariant spatial/temporal structures via normalized relative MSE (i.e. model outputs or targets are normalized by their own largest magnitude before MSE). We find that the benefit of in-context examples lies in that the scale of the model\\'s output keeps being calibrated (red line being closer to 1) when adding more demos.", "description": "The figure shows the benefits of using in-context examples in improving the accuracy of PDE prediction.  The relative MSE error is decomposed into 'Scale' (alignment of model output range with targets) and 'Shape' (alignment of scale-invariant structures).  The results indicate that adding more demos improves the calibration of model output scale, leading to better accuracy.", "section": "4.3 In-Context Examples Enable Data-Efficient OOD Generalization"}, {"figure_path": "MuPlJ9fT4b/figures/figures_25_2.jpg", "caption": "Figure 17: Visualizations of mining in-context examples for FNO in OOD testing. Ranges of solutions predicted with in-context examples (min/max of each snapshot, reflected in colorbars) become closer to the target.", "description": "This figure visualizes the impact of using in-context examples on the out-of-distribution (OOD) generalization performance of Fourier Neural Operators (FNOs). It shows that incorporating in-context examples improves the accuracy of predictions, particularly in terms of aligning the range of predicted solutions with the true solutions.  The differences in solution patterns and value ranges between in-distribution and out-of-distribution data highlight the challenges of OOD generalization for neural operators, and the effectiveness of the proposed in-context learning method.", "section": "N Visualizations with In-Context Examples"}]