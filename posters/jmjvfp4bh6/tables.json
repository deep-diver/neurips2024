[{"figure_path": "jMJVFP4BH6/tables/tables_7_1.jpg", "caption": "Table 2: Consistency between the neuron's semantics and the images/captions. Grad., Act. and Ca. denote gradient-, activation- and causality-based methods, respectively. \u2021 and * represent the CLIPScore w.r.t input and output images. We use background highlights the best performance.", "description": "This table presents the quantitative evaluation results of semantic relevance for T-neurons and I-neurons identified by different attribution methods including NAM.  It compares the consistency between the semantics of these neurons and the corresponding input/output images and captions using several metrics: CLIPScore (with respect to input and output images), BERTScore, MoverScore, and BLEURT. The table highlights the best-performing method for each metric and class (T-neurons/I-neurons).", "section": "4.3 RQ2: Properties of T/I-Neurons & Effectiveness of NAM"}, {"figure_path": "jMJVFP4BH6/tables/tables_15_1.jpg", "caption": "Table 1: The semantics of T/I-neurons with the highest attribution scores identified by different attribution methods, when the output of MLLMs containing the targeted semantics. (Ll,Uk) denotes the k-th neuron at layer l. For each method, we report semantics with top-4 probabilities.", "description": "This table presents the top four semantic categories predicted by various neuron attribution methods (Grad, AcT, CE, NAM) for both text (T-neurons) and image (I-neurons) outputs of multi-modal large language models (MLLMs).  The location of the most relevant neurons within the model's layers is also specified.  This helps illustrate the modality-specific semantic knowledge learned by different neurons and the effectiveness of the proposed NAM method.", "section": "4.3 RQ2: Properties of T/I-Neurons & Effectiveness of NAM"}, {"figure_path": "jMJVFP4BH6/tables/tables_16_1.jpg", "caption": "Table 2: Consistency between the neuron's semantics and the images/captions. Grad., Act. and Ca. denote gradient-, activation- and causality-based methods, respectively. \u2021 and * represent the CLIPScore w.r.t input and output images. We use background highlights the best performance.", "description": "This table presents the quantitative evaluation results of the semantic relevance of neurons identified by different methods (NAM and baselines).  The consistency between the semantics of neurons and the input/output images are measured using CLIPScore, BERTScore, MoverScore, and BLEURT. The results show that NAM achieves the highest consistency scores compared to the baselines for both T-neurons and I-neurons across different categories of images and text.", "section": "4.3 RQ2: Properties of T/I-Neurons & Effectiveness of NAM"}, {"figure_path": "jMJVFP4BH6/tables/tables_17_1.jpg", "caption": "Table 1: The semantics of T/I-neurons with the highest attribution scores identified by different attribution methods, when the output of MLLMs containing the targeted semantics. (Ll,Uk) denotes the k-th neuron at layer l. For each method, we report semantics with top-4 probabilities.", "description": "This table presents the top four semantic categories predicted by different neuron attribution methods for both text (T-neurons) and image (I-neurons) outputs of multi-modal large language models (MLLMs).  The methods used are Grad, AcT, CE, and NAM.  The location of the neurons with the highest attribution scores (layer and neuron index) are provided alongside the predicted semantics.", "section": "4.3 RQ2: Properties of T/I-Neurons & Effectiveness of NAM"}, {"figure_path": "jMJVFP4BH6/tables/tables_17_2.jpg", "caption": "Table 1: The semantics of T/I-neurons with the highest attribution scores identified by different attribution methods, when the output of MLLMs containing the targeted semantics. (Ll,Uk) denotes the k-th neuron at layer l. For each method, we report semantics with top-4 probabilities.", "description": "This table shows the top four semantic categories identified by different neuron attribution methods (Grad, AcT, CE, NAM) for both text (T-neurons) and image (I-neurons) outputs of multi-modal large language models (MLLMs).  The location of the neurons (layer and index) are also provided.  The results highlight the semantics that are most strongly associated with specific neurons in MLLMs when generating content with specific target semantics (e.g., a horse or a dog).", "section": "4.3 RQ2: Properties of T/I-Neurons & Effectiveness of NAM"}]