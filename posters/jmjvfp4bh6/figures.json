[{"figure_path": "jMJVFP4BH6/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of neuron attribution methods for interpreting LLMs. (a) The paradigm of GILL; (b) current attribution methods tailored for text-only LLMs; (c) the challenges of extending current attribution methods to MLLMs; (d) the paradigm of our NAM. Best viewed in color.", "description": "This figure illustrates the differences in neuron attribution methods between text-only LLMs and MLLMs. (a) shows the GILL model architecture, a multimodal LLM. (b) depicts the traditional neuron attribution for text-only LLMs, directly linking neurons to text outputs. (c) highlights the challenges of applying this method to MLLMs, such as noise in generated images and the intermingling of modality-specific neurons. (d) presents the proposed NAM method, which addresses these challenges by using image segmentation to reduce noise and identifying modality-specific neurons.", "section": "1 Introduction"}, {"figure_path": "jMJVFP4BH6/figures/figures_2_1.jpg", "caption": "Figure 2: Illustration of neuron attribution methods for interpreting LLMs. (a) The paradigm of current attribution methods tailored for text-only LLMs. (b) The challenges of extending current attribution methods to MLLMs. (c) The paradigm of our NAM.", "description": "This figure illustrates the neuron attribution methods for interpreting LLMs, comparing the traditional approach for text-only LLMs with the challenges and proposed solution for Multimodal LLMs (MLLMs). Panel (a) shows the standard approach for text-only LLMs, focusing on attributing the output text to neurons within the FFN layers.  Panel (b) highlights the challenges of applying this method to MLLMs, such as semantic noise (extraneous elements in generated images), inefficiency (computationally expensive methods), and the intermingling of text and image neurons. Panel (c) illustrates the proposed NAM method (Neuron Attribution for MLLMs) which addresses these challenges by using image segmentation to identify target semantic regions, employing activation-based scores for efficiency, and decoupling the analysis of modality-specific neurons.", "section": "3 Method"}, {"figure_path": "jMJVFP4BH6/figures/figures_6_1.jpg", "caption": "Figure 3: Distribution of (a) I-neurons, (b) T-neurons, and (c) intersection and subset of I-neurons and T-neurons per layer identified by NAM, chosen by different number of neurons with top scores on average. Best viewed in color.", "description": "This figure visualizes the distribution of I-neurons (image-related neurons), T-neurons (text-related neurons), and their overlap across different layers of the multi-modal large language model (MLLM).  Subfigures (a) and (b) show the distributions of I-neurons and T-neurons respectively, highlighting the concentration of these neurons in the middle and higher layers of the model. Subfigure (c) illustrates the intersection and the subset relationships between these two neuron types, indicating that while some neurons contribute to both image and text generation, many are modality-specific.", "section": "4.2 RQ1: Distribution of T/I-Neurons"}, {"figure_path": "jMJVFP4BH6/figures/figures_8_1.jpg", "caption": "Figure 1: Illustration of neuron attribution methods for interpreting LLMs. (a) The paradigm of GILL; (b) current attribution methods tailored for text-only LLMs; (c) the challenges of extending current attribution methods to MLLMs; (d) the paradigm of our NAM. Best viewed in color.", "description": "This figure illustrates the different paradigms of neuron attribution methods for LLMs. (a) shows the architecture of GILL, a multimodal LLM. (b) depicts current methods for text-only LLMs. (c) highlights the challenges in extending those methods to multimodal LLMs. (d) presents the proposed NAM method, which addresses these challenges.", "section": "1 Introduction"}]