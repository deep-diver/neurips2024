[{"type": "text", "text": "Transformers as Game Players: Provable In-context Game-playing Capabilities of Pre-trained Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Chengshuai Shi University of Virginia cs7ync@virginia.edu ", "page_idx": 0}, {"type": "text", "text": "Kun Yang University of Virginia ky9tc@virginia.edu ", "page_idx": 0}, {"type": "text", "text": "Jing Yang The Pennsylvania State University yangjing@psu.edu ", "page_idx": 0}, {"type": "text", "text": "Cong Shen University of Virginia cong@virginia.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The in-context learning (ICL) capability of pre-trained models based on the transformer architecture has received growing interest in recent years. While theoretical understanding has been obtained for ICL in reinforcement learning (RL), the previous results are largely confined to the single-agent setting. This work proposes to further explore the in-context learning capabilities of pre-trained transformer models in competitive multi-agent games, i.e., in-context game-playing (ICGP). Focusing on the classical two-player zero-sum games, theoretical guarantees are provided to demonstrate that pre-trained transformers can provably learn to approximate Nash equilibrium in an in-context manner for both decentralized and centralized learning settings. As a key part of the proof, constructional results are established to demonstrate that the transformer architecture is sufficiently rich to realize celebrated multi-agent game-playing algorithms, in particular, decentralized V-learning and centralized VI-ULCB. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Since proposed in Vaswani et al. [57], the transformer architecture has received significant interest. It has powered many recent breakthroughs in artificial intelligence [11, 12, 21, 23], including the extremely powerful large language models such as GPT [42] and Llama [55, 56]. One of the most striking observations from the research of these transformer-powered models is that they demonstrate remarkable in-context learning (ICL) capabilities. In particular, after appropriate pre-training, the models can handle new tasks when prompted by a few descriptions or demonstrations without any parameter updates, e.g., Brown et al. [11], Chowdhery et al. [15], Liu et al. [39]. ", "page_idx": 0}, {"type": "text", "text": "ICL is practically attractive as it provides strong generalization capabilities across different downstream tasks without requiring further training or a large amount of task-specific data. These appealing properties have motivated many empirical studies to better understand ICL [18, 26, 59]; see the survey by Dong et al. [22] for key findings and results. In addition to the empirical investigations, recent years have witnessed growing efforts in gaining deeper theoretical insights into ICL, e.g., Ahn et al. [2], Akyirek et al. [3], Bai et al. [7], Cheng et al. [14], Li et al. [37], Raventos et al. [44], Wu et al. [64], Xie et al. [66], Zhang et al. [71]. ", "page_idx": 0}, {"type": "text", "text": "Among these empirical and theoretical studies, one emerging direction focuses on the capability of pretrained transformer models to perform in-context reinforcement learning (ICRL) [28, 34, 35, 50, 73]. In particular, the transformer is pre-trained with interaction data from diverse environments, modeling the interaction as a sequential prediction task. During inference, the pre-trained transformer is prompted via the interaction trajectory in the current environment for it to select actions. The work by Lin et al. [38] provides some theoretical understanding of ICRL, including both a general pretraining guarantee and specific constructions of transformers to realize some well-known designs in multi-armed bandits and RL (especially, LinUCB [1], Thompson sampling [54], and UCB-VI [6]). Wang et al. [62] further provides understandings on the capability of transformers learning temporal difference (TD) methods [52] via an in-context fashion. A detailed literature review can be found in Sec.6. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "The insights from Lin et al. [38], Wang et al. [62] are largely confined to the single-agent scenario, i.e., a single-agent multi-armed bandit or Markov decision process (MDP). The power of RL, however, extends to the much broader multi-agent scenario, especially the multi-player competitive games such as GO [49], Starcraft [58], and Dota 2 [10]. To provide a more comprehensive understanding of ICRL, this work targets further studying the in-context game-playing (ICGP) capabilities of transformers in multi-agent competitive settings. To the best of our knowledge, this is the first work providing theoretical analyses and empirical pieces of evidence on theICGPcapabilities of transformers. The contributions of this work are further summarized as follows. ", "page_idx": 1}, {"type": "text", "text": "\u00b7 A general framework is proposed to model in-context game-playing via transformers, where we focus on the representative two-player zero-sum Markov games and target learning Nash equilibrium (NE). Compared with the single-agent scenario [38], the multi-agent setting considered in this work broadens the ICRL research scope while it is also more complicated due to its game-theoretic nature. ", "page_idx": 1}, {"type": "text", "text": "\u00b7 The challenging decentralized learning setting is first studied, where two distinct transformers are trained to learn NE, one for each player, without observing the opponent's actions. A general realizability-conditioned guarantee is first derived that characterizes the generalization error of the pre-trained transformers. Then, the capability of the transformer architecture is demonstrated by providing a concrete construction so that the famous V-learning algorithm [8] can be exactly realized. Lastly, a finite-sample upper bound on the approximation error of NE is proved to establish the ICGP capability of transformers. As a further implication, the result of realizing V-learning demonstrates the capability of pre-trained transformers to perform model-free RL designs, in addition to the model-based ones (e.g., UCB-VI [6] as studied in Lin et al. [38]). ", "page_idx": 1}, {"type": "text", "text": "\u00b7 To obtain a complete understanding, the centralized learning setting is also investigated, where one transformer is pre-trained to control both players? actions. A similar set of results is provided: a general pre-training guarantee, a constructional result to demonstrate realizability, and a finite-sample upper bound on the approximation error of NE. Distinctly, the transformer construction is presented as a specific parameterization to implement the renowned centralized VI-ULCB algorithm [8]. ", "page_idx": 1}, {"type": "text", "text": "\u00b7 Furthermore, experiments are also performed to practically test the ICGP capabilities of the pretrained transformers. The obtained results not only corroborate the derived theoretical claims, but also empirically motivate this and further studies on the interesting direction of pre-trained models in game-theoretic settings. ", "page_idx": 1}, {"type": "text", "text": "2  A Theoretical Framework for In-Context Game Playing ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1  The Basic Setup of Environments ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "To demonstrate the ICGP capability of transformers, we focus on one of the most basic gametheoretic settings: two-player zero-sum Markov games [47], while the discussions provided later conceivably extend to more general games. An illustration of the different settings (i.e., decentralized and centralized) considered in this work (with details explained later) is given in Fig. 1. The overall framework is introduced in the following, which extends Lin et al. [38] from the single-agent decision-making setting to the competitive multi-agent domain. ", "page_idx": 1}, {"type": "text", "text": "Considering a set of two-player zero-sum Markov games denoted as $\\mathcal{M}$ . Each environment $M\\in\\mathcal{M}$ shares the number of episodes $G$ , the number of steps $H$ in each episode, the state space $\\boldsymbol{S}$ (with $|S|=S\\rangle$ 0, the action spaces $\\{\\mathcal{A},\\mathcal{B}\\}$ (with $|{\\mathcal{A}}|=A$ and $|B|=B$ ), and the reward space $\\mathcal{R}$ . Here $\\boldsymbol{\\mathcal{A}}$ and $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ denote the action spaces of two players, respectively, which are referred to as the max-player and the min-player for convenience. ", "page_idx": 1}, {"type": "image", "img_path": "pRQmRaonxf/tmp/37e58aaeade6a010f64d305e3b412d4d1b1023c92a83ce15a6e88c01a1c79d3d.jpg", "img_caption": ["Figure 1: An overall view of the framework, where the in-context game-playing (ICGP) capabilities of transformers are studied in both decentralized and centralized learning settings. The orange arrows denote the supervised pre-training procedure and the blue arrows mark the inference procedure. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Each environment $M=\\{\\mathbb{T}_{M}^{h-1},\\mathbb{R}_{M}^{h}:h\\in[H]\\}$ has its ransition model $\\mathbb{T}_{M}^{h}:S\\times A\\times B\\rightarrow\\Delta(S)$ and reward functions $\\mathbb{R}_{M}^{h}:\\mathcal{A}\\times\\mathcal{B}\\rightarrow\\Delta(\\mathcal{R})$ where $\\mathbb{T}_{M}^{0}(\\cdot)$ dentes teinial statdistribon. Particularly, overall $G$ episodes of $H$ steps happen in each environment $M$ , with each episode starting at $s^{g,1}\\sim\\dot{\\mathbb{T}}_{M}^{0}(\\cdot)$ . If the action pair $(a^{g,h^{\\mathbf{\\alpha}}},b^{g,h^{\\mathbf{\\alpha}}})$ is taken upon state $s^{g,h}$ at step $h$ in episode $g$ , the state is transited to $s^{g,h+1}\\sim\\mathbb{T}_{M}^{h}(\\cdot|s^{g,h},a^{g,h},b^{g,h})$ , and reward $r^{g,h}\\sim\\mathbb{R}_{M}^{h}(s^{g,h},a^{g,h},b^{g,h})$ (respectively, $-r^{g,h})$ is collected by the max-player (respectively, the min-player). For simplicity, we assume that the max-player rewards are bounded in $[0,1]$ and deterministic, i.e., for each $(s,a,b,h)$ , there exists $r\\in[0,1]$ such that $\\mathbb{R}_{M}^{g,h}(r|s,a,b)=1$ Also, the initial state $s^{g,1}$ is assumedtobe a fixed one $s^{1}$ \uff0c i.e., $\\mathbb{T}_{M}^{0}(s^{1})=1$ ", "page_idx": 2}, {"type": "text", "text": "We further leverage the notation $T:=G H$ , while using time $t$ and episode-step pair $(g,h)$ in an interleaving manner with $t:=(g\\mathrm{~-~}1)H\\mathrm{~}+h$ : The partial interaction trajectory up to time $t$ is then denoted as $D^{t}:=\\{(s^{\\tau},a^{\\tau},b^{\\tau},r^{\\tau}):\\tau\\in[t])$ and we use the abbreviated notation $D:=D^{T}$ Individually, for the max-player, we denote her observed interaction trajectory up to time $t$ by $D_{+}^{t}:=\\{(\\dot{s^{\\tau}},a^{\\tau},r^{\\tau}):\\tau\\in\\dot{[t]}\\}$ and write $D_{+}:=D_{+}^{T}$ for short. Similarly, for the min-player, we denote $D_{-}^{t}:=\\{(s^{\\tau},b^{\\tau},r^{\\tau}):\\tau\\in[t])$ and $D_{-}:=D_{-}^{T}$ ", "page_idx": 2}, {"type": "text", "text": "2.2  Game-playing Algorithms and Nash Equilibrium ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "A game-playing algorithm $\\mathtt{A L g}$ can map a partial trajectory $D^{t-1}$ and state $s^{t}$ to a distribution over the actions, i.e., $\\mathtt{A l g}(\\cdot,\\cdot|D^{t-1},s^{t})\\in\\Delta(A\\times B)$ If one algorithm $\\mathtt{A L g}$ is decoupled for the two players (as in the later decentralized setting), we denote it as $\\mathsf{A l g}\\,=\\,\\mathsf{\\overline{{(A1g_{+},A1g_{-})}}}$ , where $\\mathtt{A l g}_{+}(\\cdot|D_{+}^{t-1},s^{t})\\in\\Delta(A)$ and $\\mathtt{A l g\\_}(\\cdot|D_{-}^{t-1},s^{t})\\in\\Delta(\\mathcal{B})$ . Given an environment $M$ and an algorithm ${\\tt A l g}$ , the distribution over a full trajectory $D$ can be expressed as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{P}_{M}^{\\mathtt{A l g}}(D)=\\prod_{t\\in[T]}\\mathbb{T}_{M}^{t-1}(s^{t}|s^{t-1},a^{t-1},b^{t-1})\\cdot\\mathtt{A l g}(a^{t},b^{t}|D^{t-1},s^{t})\\cdot\\mathbb{R}_{M}^{t}(r^{t}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "If further considering an environment prior distribution $\\Lambda\\in\\Delta(\\mathcal{M})$ such that $M\\sim\\Lambda$ , the joint distibution of $(M,D)$ is denoted as $\\mathbb{P}_{\\Lambda}^{\\tt A l g}(D)$ where $M\\sim\\Lambda(\\cdot)$ and $D\\sim\\mathbb{P}_{M}^{\\mathtt{A l g}}(\\cdot)$ ", "page_idx": 2}, {"type": "text", "text": "For environment $M$ and a game-playing algorithm $\\pi$ ,we define its value function over one episode $V_{M}^{\\pi}(s^{1})=\\mathbb{E}_{D^{H}\\sim\\mathbb{P}_{M}^{\\pi}}[\\sum_{t\\in[H]}\\bar{r}^{t}]$ With the marginalized polie of $\\pi$ denoted as $(\\mu,\\nu)$ we define their best responses as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nu_{\\uparrow}(\\mu):=\\arg\\operatorname*{min}_{\\nu^{\\prime}}V_{M}^{\\mu,\\nu^{\\prime}}(s^{1}),\\;\\;\\mu_{\\uparrow}(\\nu):=\\arg\\operatorname*{max}_{\\mu^{\\prime}}V_{M}^{\\mu^{\\prime},\\nu}(s^{1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "whose corresponding values are ", "page_idx": 2}, {"type": "equation", "text": "$$\nV_{M}^{\\mu,\\dagger}(s^{1}):=V_{M}^{\\mu,\\nu_{\\dagger}(\\mu)}(s^{1}),\\;\\;V_{M}^{\\dagger,\\nu}(s^{1}):=V_{M}^{\\mu_{\\dagger}(\\nu),\\nu}(s^{1}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "With the notion of best responses, the following classical definition of approximate Nash equilibrium (NE) can be introduced [8, 32, 40, 47]. ", "page_idx": 2}, {"type": "text", "text": "Definition 2.1 (Approximate Nash equilibrium). A decoupled policy pair $(\\hat{\\mu},\\hat{\\nu})$ is an $\\varepsilon$ -approximate Nash equilibrium for environment $M$ \u00fc $V_{M}^{\\hat{\\mu},\\dagger}(s^{1})+\\varepsilon\\geq V_{M}^{\\hat{\\mu},\\hat{\\nu}}(s^{1})\\geq V_{M}^{\\dagger,\\hat{\\nu}}(s^{1})-\\varepsilon,$ i.e, the Nash equilibrium gap $V_{M}^{\\dagger,\\hat{\\nu}}(s^{1})-V_{M}^{\\hat{\\mu},\\dagger}(s^{1})\\leq2\\varepsilon$ \uff0c ", "page_idx": 2}, {"type": "text", "text": "For each environment, our learning goal is to approximate its NE policy pair. In other words, we target outputting a policy pair $(\\hat{\\mu},\\hat{\\nu})$ that is $\\varepsilon$ -approximate NE with an error $\\varepsilon$ that is as small as possible, after interacting with the environment for an overall $T$ rounds. ", "page_idx": 3}, {"type": "text", "text": "2.3  The Transformer Architecture ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "With the basics of the game-playing environment and the learning target established, we now introduce the transformer architecture [57], which has demonstrated great potential in processing sequential inputs. First, for an input vector $\\pmb{x}\\in\\mathbb{R}^{d}$ ,we denote $\\sigma_{\\mathrm{r}}(\\pmb{x}):=\\hat{\\mathrm{ReLU}}(\\pmb{x}):=\\operatorname*{max}\\{\\pmb{x},\\stackrel{\\cdot}{0}\\}\\ \\hat{\\in}\\ \\mathbb{R}^{d}$ as the entry-wise ReLU activation function and $\\sigma_{s}(\\pmb{x}):=\\mathrm{softmax}(\\pmb{x})\\in\\mathbb{R}^{d}$ as the softmax activation function, while using $\\sigma(\\cdot)$ to refer to a non-specified activation function (i.e., both ReLU and softmax may be used). Then, the masked attention layer and the MLP layer can be defined as follows. ", "page_idx": 3}, {"type": "text", "text": "Definition 2.2 (Masked Attention Layer). A masked attention layer with $M$ heads is denoted as $\\operatorname{Attn}\\!\\theta(\\cdot)$ with parameters $\\pmb{\\theta}\\,=\\,\\{(\\pmb{V_{m}};\\pmb{Q_{m}},\\pmb{K_{m}})\\}_{m\\in[M]}\\,\\subset\\,\\mathbb{R}^{d\\times d}$ On any input sequence ${\\pmb H}\\,=$ $[\\boldsymbol{h}_{1},\\cdot\\cdot\\cdot\\,,\\boldsymbol{h}_{N}]\\in\\mathbb{R}^{d\\times N}$ we have $\\overline{{H}}=\\mathrm{Attn}_{\\theta}(H)=[\\overline{{h}}_{1},\\cdot\\cdot\\cdot\\cdot,\\overline{{h}}_{N}]\\in\\mathbb{R}^{d\\times N}$ where ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\overline{{\\boldsymbol{h}}}_{i}=\\boldsymbol{h}_{i}+\\sum_{m\\in[M]}\\frac{1}{i}\\sum_{j\\in[i]}\\sigma_{r}(\\langle Q_{m}\\boldsymbol{h}_{i},\\boldsymbol{K}_{m}\\boldsymbol{h}_{j}\\rangle)\\cdot V_{m}\\boldsymbol{h}_{j}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Definition 2.3 (MLP Layer). An MLP layer with hidden dimension $d^{\\prime}$ is denoted as $\\mathrm{MLP}_{\\theta}$ with parameters $\\pmb{\\theta}=(\\pmb{W}_{1},\\pmb{W}_{2})\\in\\mathbb{R}^{d^{\\prime}\\times d}\\times\\mathbb{R}^{d\\times d^{\\prime}}$ On any input sequence $\\b{H}=[\\b{h}_{1},\\cdot\\cdot\\cdot\\ ,\\b{h}_{N}]\\in\\mathbb{R}^{d\\times N}$ \uff0c we have $\\begin{array}{r}{\\overline{{H}}=\\mathrm{MLP}_{\\theta}(H)=[\\overline{{h}}_{1},\\cdot\\cdot\\cdot\\cdot,\\overline{{h}}_{N}]\\in\\mathbb{R}^{d\\times N},}\\end{array}$ where ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\overline{{\\pmb{h}}}_{i}=\\pmb{h}_{i}+\\pmb{W}_{2}\\cdot\\pmb{\\sigma}(\\pmb{W}_{1}\\cdot\\pmb{h}_{i}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The combination of masked attention layers and MLP layers leads to the overall decoder-based transformer architecture studied in this work, as defined in the following. ", "page_idx": 3}, {"type": "text", "text": "Definition 2.4 (Decoder-based Transformer). An $L$ -layer decoder-based transformer,denoted as $\\operatorname{TF}_{\\theta}(\\cdot)$ , is a composition of $L$ masked attention layers, each followed by an MLP layer and a clip operation: $\\mathrm{TF}_{\\theta}(H)=H^{(L)}\\in\\mathbb{R}^{d\\times N}$ where $H^{(L)}$ is defined iterativelyby taking $H^{(0)}=H\\in$ $\\mathbb{R}^{d\\times N}$ and for $l\\in[L]$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{H}^{(l)}=\\mathbf{M}\\mathbf{L}\\mathbf{P}_{\\pmb{\\theta}_{\\mathrm{mip}}^{(l)}}\\left(\\mathbf{A}\\mathbf{t}\\mathrm{tn}_{\\pmb{\\theta}_{\\mathrm{matu}}^{(l)}}\\left(\\pmb{H}^{(l-1)}\\right)\\right)\\in\\mathbb{R}^{d\\times N},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where parameter $\\pmb{\\theta}=\\{(\\pmb{\\theta}_{\\mathrm{mattn}}^{(l)},\\pmb{\\theta}_{\\mathrm{mlp}}^{(l)}):l\\in\\{L\\}\\}$ consists of $\\pmb{\\theta}_{\\mathrm{mattn}}^{(l)}\\,=\\,\\{(\\pmb{V}_{m}^{(l)},\\pmb{Q}_{m}^{(l)},\\pmb{K}_{m}^{(l)}):m\\,\\in$ [M]} C Rdxd and 0l $\\pmb{\\theta}_{\\mathrm{mlp}}^{(l)}=\\big(\\pmb{W}_{1}^{(l)},\\dot{\\pmb{W}}_{2}^{(l)}\\big)\\in\\mathbb{R}^{d^{\\prime}\\times d}\\times\\mathbb{R}^{d\\times d^{\\prime}}$ ", "page_idx": 3}, {"type": "text", "text": "$\\Theta_{d,L,M,d^{\\prime},F}:=\\,\\{\\pmb\\theta=(\\pmb\\theta_{\\mathrm{mattn}}^{(1:L)},\\pmb\\theta_{\\mathrm{mlp}}^{(1:L)})$ $\\lVert\\pmb{\\theta}\\rVert\\leq F\\}$ , where the norm of a transformer $\\mathrm{TF}_{\\theta}$ is denoted as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\|\\pmb\\theta\\|:=\\operatorname*{max}_{l\\in[L]}\\left\\{\\operatorname*{max}_{m\\in[M]}\\left\\{\\|Q_{m}^{(l)}\\|_{\\infty},\\|K_{m}^{(l)}\\|_{\\infty}\\right\\}+\\sum_{m\\in[M]}\\|V_{m}^{(l)}\\|_{\\infty}+\\|W_{1}^{(l)}\\|_{\\infty}+\\|W_{2}^{(l)}\\|_{\\infty}\\right\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Other Notations. The total variation distance between two algorithms $\\{\\pi,\\pi^{\\prime}\\}$ upon $D^{t-1}\\cup\\{s\\}$ is denoted as $\\mathrm{TV}(\\pi,\\pi^{\\prime}|D^{t-1},s)\\,:=\\,\\mathrm{TV}(\\pi(\\cdot|D^{t-1},s),\\pi^{\\prime}(\\cdot|D^{t-1},s))$ . Also, the notation $x\\lesssim y$ indicates that $x$ is a lower or equivalent order term compared with $y$ i.e., $x=\\mathcal{O}(y),\\tilde{\\mathcal{O}}(\\cdot)$ hides poly-logarithmic terms in $H,G,S,A,B$ , and $\\mathrm{poly}(\\cdot)$ compactly denotes a polynomial term with respect to the input. ", "page_idx": 3}, {"type": "text", "text": "3 Decentralized Learning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "First, we study the decentralized learning setting, i.e., each player takes actions following her own model independently without observing the opponent's actions, as it better captures the unique game-playing scenario considering in this work. This setting is aligned with the canonical study of normal-form games [20, 53], and has been extended to Markov games in recent years [32, 41, 51]. In the following, we start by introducing the basic setup of supervised pre-training and provide a general performance guarantee relying on a realizability assumption. Then, we provide a constructional result to demonstrate that the algorithms induced by transformers are rich enough to realize the celebrated V-learning algorithm [32]. With these results, we finally establish that with V-learning providing training data, the pre-trained transformer can effectively approximate NE when interacting with different environments in an in-context fashion. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "3.1  Supervised Pre-training Results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "3.1.1  Basic Setups ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Training Dataset. In the supervised pre-training, we use a context algorithm $\\mathtt{A l g}_{\\mathrm{0}}$ to collect the offline trajectories. For the decentralized setting, the context algorithm $\\mathtt{A l g}_{\\mathrm{0}}$ used for data collection is assumed to be consisted of two decoupled algorithms $(\\mathtt{A1g}_{+,0},\\mathtt{A1g}_{-,0})$ for the max- and min-players, respectively. With the context algorithm, we consider $N$ i.i.d. offline trajectories $\\{\\overline{{D}}_{i}:i\\in[N]\\}$ are collected, where $\\overline{{D}}_{i}:=D_{i}\\cup D_{i}^{\\prime}$ with ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{i}:=\\{(s_{i}^{t},a_{i}^{t},b_{i}^{t},r_{i}^{t}):t\\in[T]\\}\\sim\\mathbb{P}_{\\Lambda}^{{\\mathbf{A}}{\\mathbf{B}}_{0}}(\\cdot);}\\\\ &{D_{i}^{\\prime}:=\\{(a_{i,s}^{t},b_{i,s}^{t})\\sim\\mathbb{A}\\mathbf{1}\\mathbf{g}_{0}(\\cdot,\\cdot|D_{i}^{t-1},s):t\\in[T],s\\in S\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "It can be observed that $D_{i}$ is the commonly considered offline interaction trajectory of $\\mathtt{A l g}_{\\mathrm{0}}$ , while $D_{i}^{\\prime}$ is the sampled actions of each state $s$ at each step $t$ with $\\mathtt{A l g}_{\\mathrm{0}}$ . Compared with Lin et al. [38], $D_{i}^{\\prime}$ is an augmented component. We first note that collecting $D_{i}^{\\prime}$ is relatively easy in practical applications, as it only needs to additionally sample from the distribution ${\\tt A l g}_{0}(\\cdot,\\cdot|D_{i}^{t-1},s)$ for each $s~\\in~s$ (i.e., no additional interactions with the environment). Moreover, the reason to incorporate such an augmentation is to provide additional diverse pre-training data due to the unique game-theoretic environment, with further discussions provided after the later Lemma 3.6. It has also been recognized previously [16, 72] that the data requirement for learning Markov games is typically much stronger than that for single-agent RL. ", "page_idx": 4}, {"type": "text", "text": "To facilitate the decentralized training, the overall dataset is further split into two parts: $\\{\\overline{{D}}_{+,i}:i\\in$ $[N]\\}$ and $\\{\\overline{{D}}_{-,i}:i\\in[N]\\}$ where ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overline{{D}}_{+,i}:={D}_{+,i}\\cup{D}_{+,i}^{\\prime},\\ {D}_{+,i}:=\\{(s_{i}^{t},a_{i}^{t},r_{i}^{t}):t\\in[T]\\},\\ {D}_{+,i}^{\\prime}:=\\{a_{i,s}^{t}:t\\in[T],s\\in\\mathcal{S}\\};}\\\\ &{\\overline{{D}}_{-,i}:={D}_{-,i}\\cup{D}_{-,i}^{\\prime},\\ {D}_{-,i}:=\\{(s_{i}^{t},b_{i}^{t},r_{i}^{t}):t\\in[T]\\},\\ {D}_{-,i}^{\\prime}:=\\{b_{i,s}^{t}:t\\in[T],s\\in\\mathcal{S}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In other words, $\\overline{{D}}_{i,+}$ denotes the observations of the max-player, while $\\overline{{D}}_{i,-}$ those of the min-layer.   \nNote that neither player can observe the opponent's actions. ", "page_idx": 4}, {"type": "text", "text": "Algorithm Induced by Transformers. Due to the decentralized nature, two embedding mappings of $d_{+}$ and $d_{-}$ dimensions are considered as $\\mathrm{h}_{+}:S\\cup(A\\times\\mathcal{R})\\rightarrow\\mathbb{R}^{d_{+}}$ and $\\mathrm{h}_{-}:S\\cup(\\mathcal{B}\\stackrel{\\cdot}{\\times}\\mathcal{R})\\stackrel{\\cdot}{\\rightarrow}\\mathbb{R}^{d_{-}}$ together with two transformers $\\mathrm{TF}_{\\theta_{+}}$ and $\\mathrm{TF}_{\\theta_{-}}$ . Taking the max-player's transformer as representative, for trajectory $(D_{+}^{t-1},s^{t})$ , let $\\pmb{H}_{+}=\\mathtt{h}_{+}(D_{+}^{t-1},s^{t})=[\\mathtt{h}_{+}(s^{1}),\\mathtt{h}_{+}(a^{1},r^{1}),\\cdot\\cdot\\cdot\\,,\\mathtt{h}_{+}(s^{t})]$ be the input to $\\mathrm{TF}_{\\theta_{+}}$ , and the obtained output is $\\overline{{\\pmb{H}}}_{+}=\\mathrm{TF}_{\\pmb{\\theta}_{+}}(\\pmb{H}_{+})=[\\overline{{\\pmb{h}}}_{+,1},\\overline{{\\pmb{h}}}_{+,2},\\cdot\\cdot\\cdot\\mathbf{\\nabla},\\overline{{\\pmb{h}}}_{+,-2},\\overline{{\\pmb{h}}}_{+,-1}]$ \uff0c which has the same shape as $H_{+}$ . Similarly, the mapping $\\mathtt{h}_{-}$ is used for the min-player's transformer $\\mathrm{TF}_{\\theta_{-}}$ to embed trajectory $(D_{-}^{t-1},s^{t})$ ", "page_idx": 4}, {"type": "text", "text": "We further assume that two fixed linear extraction mappings, $\\mathbb{A}\\in\\mathbb{R}^{A\\times d_{+}}$ and $\\mathtt{B}\\in\\mathbb{R}^{B\\times d_{-}}$ , are used to induce algorithms ${\\tt A l g}_{\\theta_{+}}$ and $\\mathtt{A l g}_{\\theta_{-}}$ over the action spaces $\\boldsymbol{\\mathcal{A}}$ and $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ of the max- and min-players, respectively, as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathtt{A l g}_{\\theta_{+}}(\\cdot|D_{+}^{t-1},s^{t})=\\mathrm{proj}_{\\Delta}\\left(\\mathtt{A}\\cdot\\mathrm{TF}_{\\theta_{+}}\\left(\\mathtt{h}_{+}(D_{+}^{t-1},s^{t})\\right)_{-1}\\right),}\\\\ {\\mathtt{A l g}_{\\theta_{-}}(\\cdot|D_{-}^{t-1},s^{t})=\\mathrm{proj}_{\\Delta}\\left(\\mathtt{B}\\cdot\\mathrm{TF}_{\\theta_{-}}\\left(\\mathtt{h}_{-}(D_{-}^{t-1},s^{t})\\right)_{-1}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathrm{proj}_{\\Delta}$ denotes the projection to a probability simplex. ", "page_idx": 4}, {"type": "text", "text": "Training Scheme. We consider the standard supervised pre-training to maximize the log-likelihood of observing training datasets $\\overline{{D}}_{+}$ (resp., $\\overline{{D}}_{-}$ ) over algorithms $\\{\\mathsf{A1g}_{\\theta_{+}}:\\theta_{+}\\in\\Theta_{+}\\}$ (resp., $\\{\\mathtt{A l g}_{\\mathtt{-}}:$ $\\pmb{\\theta}_{-}\\in\\Theta_{-}\\}$ \uff09with $\\Theta_{+}:=\\Theta_{d_{+},L_{+},M_{+},d_{+}^{\\prime},F_{+}}$ (resp., $\\Theta_{-}:=\\Theta_{d_{-},L_{-},M_{-},d_{-}^{\\prime},F_{-}})$ . In particular, the pre-training outputs ${\\widehat{\\pmb{\\theta}}}_{+}$ and $\\widehat{\\theta}_{-}$ are determined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widehat{\\theta}_{+}=\\arg\\operatorname*{max}_{\\theta_{+}\\in\\Theta_{+}}\\frac{1}{N}\\sum_{i\\in[N]}\\sum_{t\\in[T]}\\sum_{s\\in S}\\log\\left(\\mathbb{A}\\mathbb{g}_{\\theta_{+}}(a_{i,s}^{t}|D_{+,i}^{t-1},s)\\right);\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widehat{\\theta}_{-}=\\arg\\operatorname*{max}_{\\theta_{-}\\in\\Theta_{-}}\\frac{1}{N}\\sum_{i\\in[N]}\\sum_{t\\in[T]}\\sum_{s\\in S}\\log\\left(\\mathbb{A}\\mathbb{g}_{\\theta_{-}}(b_{i,s}^{t}|D_{-,i}^{t-1},s)\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "3.1.2  Theoretical Guarantees ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we provide a generalization guarantee of the algorithms $^{\\mathtt{A l g}_{\\widehat{\\theta}_{+}}}$ and $\\mathtt{A l g}_{\\widehat{\\theta}_{-}}$ pre-trained following the scheme introduced above. First, the standard definition regarding the covering number and an assumption of approximate realizability are introduced to facilitate the analysis, which are also leveraged in Lin et al. [38]. ", "page_idx": 5}, {"type": "text", "text": "Definition 3.1 (Decentralized Covering Number). For a class of algorithms $\\{{\\tt A}{\\tt L}{\\tt g}_{\\pmb{\\theta}_{+}}:{\\pmb\\theta}_{+}\\in\\Theta_{+}\\},$ we say $\\tilde{\\Theta}_{+}\\subseteq\\Theta_{+}$ is a $\\rho_{+}$ -cover of $\\Theta_{+}$ $i f\\tilde{\\Theta}_{+}$ is a finite set such that for any $\\pmb{\\theta}_{+}\\in\\Theta_{+}$ , there exists $\\tilde{\\theta}_{+}\\in\\tilde{\\Theta}_{+}$ such that forall $D_{+}^{t-1},s\\in S,t\\in[T],$ it holdsthat ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\log\\mathsf{A}\\mathsf{1}\\mathsf{g}_{\\tilde{\\theta}_{+}}(\\cdot|D_{+}^{t-1},s)-\\log\\mathsf{A}\\mathsf{1}\\mathsf{g}_{\\theta_{+}}(\\cdot|D_{+}^{t-1},s)\\right\\|_{\\infty}\\le\\rho_{+}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The covering number $\\mathcal{N}_{\\Theta_{+}}(\\rho_{+})$ is the minimal cardinality of $\\Tilde{\\Theta}_{+}$ such that $\\Tilde{\\Theta}_{+}$ is a $\\rho_{+}$ -cover of $\\Theta_{+}$ Similarly, we can define the $\\rho_{-}$ -cover of $\\Theta_{-}$ and the covering number $\\mathcal{N}_{\\Theta_{-}}(\\rho_{-})$ ", "page_idx": 5}, {"type": "text", "text": "Assumption 3.2 (Decentralized Approximate Realizability). There exist $\\pmb{\\theta}_{+}^{*}\\in\\Theta_{+}$ and $\\varepsilon_{+,\\mathrm{real}}>0$ suchthatforall $t\\in[T],s\\in S,a\\in{\\mathcal{A}},$ itholdsthat ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\log\\left(\\mathbb{E}_{D\\sim\\mathbb{P}_{\\Lambda}^{\\mathbb{A}\\mathbb{g}_{0}}}\\left[\\frac{\\mathtt{A}\\mathtt{l}\\mathrm{g}_{+,0}(a|D_{+}^{t-1},s)}{\\mathtt{A}\\mathtt{l}\\mathrm{g}_{\\theta_{+}^{*}}(a|D_{+}^{t-1},s)}\\right]\\right)\\leq\\varepsilon_{+,\\mathrm{real}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We also similarly assume $\\varepsilon_{-,\\mathrm{real}}$ -approximate realizability of ${\\tt A l g}_{-,0}$ via ${\\tt A l g}_{\\theta_{-}^{*}}$ with $\\pmb{\\theta}_{-}^{*}\\in\\Theta_{-}$ ", "page_idx": 5}, {"type": "text", "text": "Then, we can establish the following generalization guarantee on the TV distance between $(\\mathtt{A l g}_{\\widehat{\\theta}_{+}},\\mathtt{A l g}_{\\widehat{\\theta}_{-}})$ and $\\mathtt{A1g}_{0}=(\\mathtt{A1g}_{0,+},\\mathtt{A1g}_{0,-})$ , capturing their similarities. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.3 (Decentralized Pre-training Guarantee). Let $\\widehat{\\pmb{\\theta}}_{+}$ be the max-player's pre-training output defined in Sec. 3.1.1. Take $\\mathcal{N}_{\\Theta_{+}}=\\bar{\\mathcal{N}_{\\Theta_{+}}}(1/N)$ as in Def. 3.1. Then, under Assumption 3.2, with probability at least $1-\\delta$ it holds that! ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}_{D\\sim\\mathbb{P}_{\\Lambda}^{\\mathrm{Ag}_{0}}}\\left[\\sum_{t\\in[T],s\\in\\mathcal{S}}\\mathrm{TV}\\left(\\mathrm{A}\\mathrm{1}_{\\mathbb{E}_{+,0}},\\mathrm{A}\\mathrm{1}_{\\mathbb{E}_{\\hat{\\theta}_{+}}}|D_{+}^{t-1},s\\right)\\right]\\lesssim T S\\sqrt{\\varepsilon_{+,\\mathrm{real}}}+T S\\sqrt{\\frac{\\log\\left(\\mathcal{N}_{\\Theta_{+}}T S/\\delta\\right)}{N}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "A similar result holds for the min-players\u2019 pre-training output $\\widehat{\\theta}_{-}$ ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.3 dmonstrates that n expctation of the pre-training data distibution,. $\\mathbb{P}_{\\Lambda}^{\\mathtt{A l g}_{0}}(D)$ \uff0c the TV distance between the pre-trained algorithm ${\\tt A l g}_{\\widehat{\\theta}_{+}}$ (resp, ${\\tt A l g}_{\\widehat{\\theta}_{-}}$ ) and the context algorithm ${\\tt A l g}_{+,0}$ (resp, ${\\tt A l g}_{-,0})$ can be bounded via two terms: one from the approximate realizability, i.e., $\\varepsilon_{+,\\mathrm{real}}$ (resp, $\\varepsilon_{-,\\mathrm{real}})$ , and the other from the limited amount of pre-training trajectories, i.e., finite $N$ While we can diminish the second term via a large pre-training dataset (i.e., sufficient pre-training games), the key question is whether the transformer structure is sufficiently expressive to realize the context algorithm, i.e., having a small $\\varepsilon_{+,\\mathrm{real}}$ , which we affrmatively answer via an example of realizing V-learning [32] in the next subsection. ", "page_idx": 5}, {"type": "text", "text": "3.2  Realizing V-learning ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To demonstrate the capability of transformers in the decentralized game-playing setting, we choose to prove that they can realize the renowned V-learning algorithm [32], the first design that breaks the curse of multiple agents in learning Markov games. Particularly, V-learning leverages techniques from adversarial bandits [5] to perform policy updates without observing the opponent's actions. The details of V-learning are provided in Appendix G.1, where its unique output rule is also elaborated. ", "page_idx": 5}, {"type": "text", "text": "In the following theorem, we demonstrate that a transformer can be constructed to exactly perform V-learning with a suitable parameterization. One additional Assumption G.2 on the existence of a transformer parameterized by the class of $\\Theta_{d,L_{D},M_{D},d_{D},F_{D}}$ to perform exact division is adopted for the convenience of the proof, while in Appendix G.2, we further demonstrate that the required division operation can be approximated to any arbitrary precision. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Theorem 3.4. With embedding mapping $h_{+}$ and extraction mapping A defined in Appendix G.3, underAssumption $G.2$ there exists a transformer $\\mathrm{TF}_{\\theta_{+}}$ with ", "page_idx": 6}, {"type": "equation", "text": "$$\nd\\lesssim\\!H S A,\\;\\;L\\lesssim G H L_{D},\\;\\;\\operatorname*{max}_{l\\in[L]}M^{(l)}\\lesssim H S^{2}+H S A+M_{D},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "equation", "text": "$$\nd^{\\prime}\\lesssim G+A+d_{D},\\enspace\\lVert\\pmb\\theta\\rVert\\lesssim G H^{2}S+G^{3}+F_{D},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "which satisies that for all $D_{+}^{t-1},s\\,\\in\\,\\mathcal{S},t\\,\\in\\,[T]$ $\\mathtt{A l g}_{\\theta_{+}}(\\cdot|D_{+}^{t-1},s)\\,=\\,\\mathtt{A l g}_{V\\cdot l e a r n i n g}(\\cdot|D_{+}^{t-1},s)$ A similar construction $\\mathrm{TF}_{\\theta_{-}}$ exists for the min-player's transformer such that forall $D_{-}^{t-1},s\\in S,t\\in$ $[T],\\,\\mathtt{A l g}_{\\pmb{\\theta}_{-}}(\\cdot|D_{-}^{t-1},s)=\\mathtt{A l g}_{V\\cdot l e a r n i n g}(\\cdot|D_{-}^{t-1},s).$ ", "page_idx": 6}, {"type": "text", "text": "The proof of Theorem 3.4 (presented in Appendix G.3) is challenging because V-learning is a modelfree design, while UCB-VI [6] studied in Lin et al. [38] and VI-ULCB [8] later presented in Sec. 4.2 are both model-based ones. We believe this result deepens our understanding of the capability of pre-trained transformers in decision-making, i.e., they can realize both model-based and model-free designs, showcasing their further potentials. ", "page_idx": 6}, {"type": "text", "text": "More specifically, with the embedded trajectory as the input, the model-based philosophy is natural for the masked attention mechanism, i.e., the value computation at each step is directly over all raw inputs in previous steps. Thus, the construction procedure is straightforward as $(\\mathrm{input}_{1}$ ,input2, $...,\\,\\mathrm{input}_{t})\\to(\\mathrm{value}_{1}$ , value2, .., valuet). However, the model-free designs are different, where value computation at one step requires previous values (instead of raw inputs). In other words, the construction procedure is a recursive one as $(\\mathrm{input_{1}}$ , input2, ., input) \u2192 (value1, input2, .,. $\\mathrm{input}_{t})\\to(\\mathrm{value}_{1}$ ,value2,. $.,\\mathrm{input}_{t})\\rightarrow...\\rightarrow(\\mathrm{value}_{1}$ , value2, .., valuet), whose realization requires carefully crafted constructions. ", "page_idx": 6}, {"type": "text", "text": "3.3 The Overall ICGP Capablity ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Finally, built upon the obtained results, the following theorem demonstrates the ICGP capability of pre-trained transformers in the decentralized setting. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.5. Let $\\Theta_{+}$ and $\\Theta_{-}$ be the classes of transformers satisfying the requirements in Theorem 3.4 and $(\\mathtt{A1g}_{+,0},\\mathtt{A1g}_{-,0})$ both be $V.$ learning. Let $(\\hat{\\mu},\\hat{\\nu})$ be the output policies via the output rule of $V\\!\\cdot$ learning. Denoting $\\mathtt{A l g}_{\\widehat{\\theta}}=(\\mathtt{A l g}_{\\widehat{\\theta}_{+}},\\mathtt{A l g}_{\\widehat{\\theta}_{-}})$ and $\\mathcal{N}_{\\Theta}=\\mathcal{N}_{\\Theta_{+}}\\mathcal{N}_{\\Theta_{-}}$ . Then, with probability at least $1-\\delta$ it holdsthat ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}_{D\\sim\\mathbb{P}_{\\Lambda}^{\\mathrm{A}\\bar{g}}\\left(V_{M}^{\\dagger,\\hat{\\nu}}(s^{1})-V_{M}^{\\hat{\\mu},\\dagger}(s^{1})\\right)}\\lesssim\\sqrt{\\frac{H^{5}S(A\\vee B)\\log(S A B T)}{G}}+T H S\\sqrt{\\frac{\\log\\left(T S\\sqrt{\\Theta/\\delta}\\right)}{N}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "With the obtained upper bound on the approximation error of NE, Theorem 3.5 demonstrates the ICGP capability of pre-trained transformers as the algorithms ${\\tt A l g}_{\\hat{\\theta}_{+}}$ and $\\mathtt{A l g}_{\\hat{\\theta}_{-}}$ are fixed during interactions with varying inference games (i.e., no parameter updates). When prompted by the interaction trajectory in the current game, they are capable of deciding the future interaction strategy and finally provide policy pairs that are approximate NE. We further note that during both pre-training and inference, each player's transformer takes inputs of its own observed trajectories, but not the opponent's actions, which refects the decentralized requirement. Moreover, the approximation error in Theorem 3.5 depends on $A\\lor B$ instead of $A B$ as in the later Theorem 4.2, evidencing the benefits of decentralized learning. ", "page_idx": 6}, {"type": "text", "text": "Proof Sketch. The proof of Theorem 3.5 (presented in Appendix H) rely on the following, decomposition, where $\\mathbb{E}_{0}[\\cdot]$ and $\\mathbb{E}_{\\widehat{\\theta}}[\\cdot]$ are with respect to PA1go and IP $\\mathrm{\\mathbb{P}}_{\\Lambda}^{\\tt A l g}\\hat{\\pmb\\theta}$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{\\ell}_{\\widehat{\\theta}}\\left[V_{M}^{\\dagger,\\hat{\\nu}}(s^{1})-V_{M}^{\\hat{\\mu},\\dagger}(s^{1})\\right]=\\mathbb{E}_{0}\\left[V_{M}^{\\dagger,\\hat{\\nu}}(s^{1})-V_{M}^{\\hat{\\mu},\\dagger}(s^{1})\\right]}}\\\\ &{}&{\\quad+\\,\\mathbb{E}_{\\widehat{\\theta}}\\left[V_{M}^{\\dagger,\\hat{\\nu}}(s^{1})\\right]-\\mathbb{E}_{0}\\left[V_{M}^{\\dagger,\\hat{\\nu}}(s^{1})\\right]+\\mathbb{E}_{0}\\left[V_{M}^{\\hat{\\mu},\\dagger}(s^{1})\\right]-\\mathbb{E}_{\\widehat{\\theta}}\\left[V_{M}^{\\hat{\\mu},\\dagger}(s^{1})\\right]\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "It can be observed that the first decomposed term is the performance of the considered ${\\mathrm{V}}.$ learning, which can be obtained following Jin et al. [32] as in Theorem G.1. ", "page_idx": 6}, {"type": "text", "text": "Then, the remaining terms concern the performance of the policy pair $(\\hat{\\mu},\\hat{\\nu})$ learned from $\\mathtt{A l g}_{\\mathrm{0}}$ and ${\\tt A l g}_{\\widehat{\\theta}}$ against their own best responses, respectively. This is drastically different from the consideration in Lin et al. [38], which only bounds the performance of the learned policies, i.e., ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{E}_{0}\\left[V_{M}^{\\hat{\\mu},\\hat{\\nu}}(s^{1})\\right]-\\mathbb{E}_{\\hat{\\pmb{\\theta}}}\\left[V_{M}^{\\hat{\\mu},\\hat{\\nu}}(s^{1})\\right].\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The involvement of best responses complicates the analysis. After careful treatments in Appendix H, we obtain the following lemma to characterize these terms. ", "page_idx": 7}, {"type": "text", "text": "Lemma 3.6. For any two decentralized algorithms $\\mathtt{A l g}_{\\alpha}$ and ${\\tt A l g}_{\\beta}$ we denote their performed policies for episode $g$ are $(\\mu_{\\alpha}^{g},\\nu_{\\alpha}^{g})$ and $(\\mu_{\\beta}^{g},\\nu_{\\beta}^{g})$ . and their final output policies via the output rule of V-learning (see Appendix $G.l$ )are $\\left(\\hat{\\mu}_{\\alpha},\\hat{\\nu}_{\\alpha}\\right)$ and $\\left(\\hat{\\mu}_{\\beta},\\hat{\\nu}_{\\beta}\\right)$ . For $\\{\\hat{\\mu}_{\\alpha},\\hat{\\mu}_{\\beta}\\}$ , it holds that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\alpha}\\left[V_{M}^{\\hat{\\mu}_{\\alpha},\\dagger}(s^{1})\\right]-\\mathbb{E}_{\\beta}\\left[V_{M}^{\\hat{\\mu}_{\\beta},\\dagger}(s^{1})\\right]\\lesssim\\!H\\cdot\\!\\sum_{t\\in[T],s\\in\\mathcal{S}}\\!\\mathbb{E}_{\\alpha}\\left[\\mathrm{TV}\\left(\\mu_{\\alpha}^{t},\\mu_{\\beta}^{t}|D_{+}^{t-1},s\\right)\\right]}\\\\ {+H\\cdot\\sum_{t\\in[T],s\\in\\mathcal{S}}\\!\\mathbb{E}_{\\alpha}\\left[\\mathrm{TV}\\left(\\nu_{\\alpha}^{t},\\nu_{\\beta}^{t}|D_{-}^{t-1},s\\right)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\mathbb{E}_{\\alpha}[\\cdot]$ and $\\mathbb{E}_{\\beta}[\\cdot]$ are wihrespect o $\\mathbb{P}_{\\Lambda}^{\\tt A l g_{\\alpha}}$ and $\\mathbb{P}_{\\Lambda}^{\\mathtt{A l g}_{\\beta}}$ $A$ similaresut holds or $\\{\\hat{\\nu}_{\\alpha},\\hat{\\nu}_{\\beta}\\}$ ", "page_idx": 7}, {"type": "text", "text": "With Lemma 3.6, we can incorporate Theorem 3.3 to upper bound the TV distance between $\\mathtt{A l g}_{\\mathrm{0}}$ and ${\\tt A l g}_{\\widehat{\\theta}}$ which together with Theorem 3.4 establish $\\varepsilon_{\\mathrm{real,+}}=\\varepsilon_{\\mathrm{real,-}}=0$ in this case, leading to the desired performance guarantee in Theorem 3.5. We here further note that the effectiveness of Theorem 3.3 in capturing the bound in Lemma 3.6 over all $s\\in S$ credits to the augmented dataset $D^{\\prime}$ , which provides diverse data of all $s\\in S$ ", "page_idx": 7}, {"type": "text", "text": "4  Centralized Learning ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we discuss the scenario of centralized learning, i.e., training one joint model to control both players' interactions. This is also known as the self-play setting [8, 9, 33, 40, 67]. Following a similar procedure as the decentralized discussions, we first provide supervised pre-training guarantees and then demonstrate that transformers are capable of realizing the renowned VI-ULCB algorithm [8]. It is thus established that in a centralized learning seting, the pre-trained transformer can still effectively perform ICGP and approximate NE. ", "page_idx": 7}, {"type": "text", "text": "4.1  Supervised Pre-training Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The same training dataset $\\{\\overline{{D}}_{i}\\,:\\,i\\,\\in\\,[N]\\}$ as in Section 3.1.1 is considered. As the centralized setting is studied here, no further split of the dataset is needed. Moreover, one $d$ -dimensional mapping $\\mathbf{h}:S\\cup(A\\times B\\times\\mathcal{R})\\rightarrow\\bar{\\mathbb{R}^{d}}$ can be designed to embed the trajectories, and the induced algorithm $\\mathtt{A l g}_{\\theta}(\\cdot,\\cdot|D^{t},s^{t})$ from the transformer $\\mathrm{TF}_{\\theta}$ can be obtained via a fixed linear extraction mapping E similarly as Eqn. (1). Finally, the MLE training is performed with $\\Theta:=\\Theta_{d,L,M,d^{\\prime},F}$ as $\\begin{array}{r}{\\widehat{\\theta}=\\arg\\operatorname*{max}_{\\theta\\in\\Theta}\\frac{1}{N}\\sum_{i\\in[N]}\\sum_{t\\in[T]}\\sum_{s\\in S}\\log\\big(\\mathbb{A}\\mathbb{1}_{\\mathcal{E}_{\\theta}}(a_{i,s}^{t},b_{i,s}^{t}|D_{i}^{t-1},s)\\big).}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "Then, a generalization guarantee of ${\\tt A l g}_{\\widehat{\\theta}}$ can be provided similarly as Theorem 3.3, which is deferred to Theorem C.3. This centralized result also implies that the pre-trained centralized algorithm performs similarly as the context algorithm, with errors caused by the approximate realizability and the finite pre-training data. ", "page_idx": 7}, {"type": "text", "text": "4.2 Realizing VI-ULCB ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The VI-ULCB algorithm [8] is one of the first provably efficient centralized learning designs for Markov games. It extends the key idea of using confidence bounds to incorporate uncertainties from stochastic bandits and MDPs [4, 6] to handle competitive environments, and has further inspired many extensions in Markov games [9, 30, 33, 40, 65]. As VI-ULCB is highly representative, we choose it as the example for realization in the centralized setting to demonstrate the capability of transformers. ", "page_idx": 7}, {"type": "text", "text": "To make VI-ULCB practically implementable, we adopt an approximate CCE solver powered by multiplicative weight update (MWU) in the place of its originally required general-sum NE solver (which is computationally demanding). This modification is demonstrated as provably efficient in later works [9, 40, 65]. Then, the following result illustrates that a transformer can be constructed to exactly perform the MWU-version of VI-ULCB. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Theorem 4.1. With embedding mapping h and extraction mapping E defined in Appendix D.2, there existsatransformer $\\mathrm{TF}_{\\theta}$ With ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d\\lesssim H S^{2}A B,\\qquad L\\lesssim G H S,\\quad\\operatorname*{max}_{l\\in[L]}M^{(l)}\\lesssim H S^{2}A B,}\\\\ &{d^{\\prime}\\lesssim G^{2}H S^{2}A B,\\quad\\|\\pmb\\theta\\|\\lesssim H S^{2}A B+G^{3}+G H,}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "which satisfes that for all $D^{t-1},s\\in S,t\\in[T],\\,{\\tt A l g}_{\\theta}(\\cdot,\\cdot|D^{t-1},s)={\\tt A l g}_{V I.U L C B}(\\cdot,\\cdot|D^{t-1},s).$ ", "page_idx": 8}, {"type": "text", "text": "One observation from the proof of Theorem 4.1 (presented in Appendix D.2) is that transformer layers can perform MWU so that an approximate CCE can be found, which is not reported in Lin et al. [38] and further demonstrates the in-context learning capability of transformers in playing normal-form games (since MWU is one of the most basic designs). ", "page_idx": 8}, {"type": "text", "text": "4.3  The Overall ICGP Capability ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "With Theorem 4.1 showing VI-ULCB can be exactly realized (i.e., $\\varepsilon_{\\mathrm{real}}=0$ in Assumption C.2), we can further prove an overall upper bound of the approximation error of NE by $\\mathtt{A l g}_{\\widehat{\\theta}}$ via the following theorem, demonstrating the ICGP capability of transformers. ", "page_idx": 8}, {"type": "text", "text": "Theorem 4.2. Let $\\Theta$ be the class of transformers satisfying the requirements in Theorem 4.1 and $\\mathtt{A l l g}_{\\mathrm{0}}$ be VI-ULCB. For all $(g,h,s)\\in[G]\\times[H]\\times{\\cal S}$ let $(\\breve{\\mu}^{g,h}(\\cdot|s),\\imath^{g,h}(\\cdot|s))$ be the marginalized policies of $\\mathtt{A l g}_{\\widehat{\\theta}}(\\cdot,\\cdot|D^{t-1},\\dot{s})$ Then,with probability at least $1-\\delta$ ,it holds that ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbb{P}_{\\Lambda}^{\\mathrm{ng}_{\\tilde{\\theta}}},\\hat{\\mu},\\hat{\\nu}}\\left[V_{M}^{\\dagger,\\hat{\\nu}}(s^{1})-V_{M}^{\\hat{\\mu},\\dagger}(s^{1})\\right]\\lesssim\\sqrt{\\frac{H^{4}S^{2}A B\\log(S A B T)}{G}}+T H S\\sqrt{\\frac{\\log(T S\\mathcal{N}_{\\Theta})/\\delta)}{N}},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\hat{\\mu}$ and $\\hat{\\nu}$ are uniformly sampled as $\\hat{\\mu}\\sim U n i f\\{\\mu^{1},\\cdot\\cdot\\cdot,\\mu^{G}\\}$ and $\\hat{\\nu}\\sim U n i f\\{\\nu^{1},\\cdot\\cdot\\cdot\\,,\\nu^{G}\\},$ with $\\mu^{g}:=\\{\\mu^{g,h}(\\cdot|s):(h,s)\\in[H]\\times{\\mathcal{S}}\\}$ and $\\nu^{g}:=\\{\\nu^{g,h}(\\cdot|s):(h,s)\\in[H]\\times{\\cal S}\\}$ ,and $\\mathbb{E}_{\\hat{\\mu},\\hat{\\nu}}$ is with respect to the process of policy sampling. ", "page_idx": 8}, {"type": "text", "text": "This result demonstrates the ICGP capability of pre-trained transformers in the centralized setting, complementing the discussions in the decentralized results. ", "page_idx": 8}, {"type": "text", "text": "5 Empirical Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Experiments are performed on two-player zero-sum normal-form games ( $H=1$ ) and Markov games $H\\,=\\,2)$ , with the decentralized EXP3 [5] (which can be viewed as a one-step V-learning) and the centralized VI-ULCB being the context algorithms as demonstrations, respectively. Additional experimental setups and details can be found in Appendix J. It can be first observed from Fig. 2 that, the transformers pre-trained with $N=20$ games performs better on the inference tasks than the ones pre-trained with $N\\,=\\,10$ games. This observation empirically validates the theoretical result that more pre-training games benefit the final game-playing performance during inference (i.e., the $\\sqrt{1/N}$ -dependencies established in Theorems 3.5 and 4.2). Moreover, when the number of pre-training games is sufficient (i.e., $N=20$ in Fig. 2), the obtained transformers can indeed learn to approximate NE in an in-context manner (i.e., having a gradually decaying NE gap), and also the obtained performance is similar to the context algorithm, i.e., EXP3 or VI-ULCB. These observations provide empirical pieces of evidence to support the ICGP capabilities of pre-trained transformers, motivating and validating the theoretical analyses performed in this work. ", "page_idx": 8}, {"type": "text", "text": "6 Related Works ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In-context Learning. Since GPT-3 [11] demonstrates the ICL capability of pre-trained transformers, growing attention has been paid to this direction. In particular, an emerging line of work targets providing a deeper understanding of the fundamental mechanism behind the ICL capability [3, 24, 29, 31, 37, 44, 59, 60, 64, 66, 68], where many interesting results have been obtained. In particular, transformers have been shown to be capable of performing in-context gradient descent so that varying optimization-based algorithms can be realized [2, 3, 7, 26, 59]. Also, Giannou et al. [27] demonstrates that looped transformers can emulate basic computing blocks, whose combinations can lead tocomplex operations. ", "page_idx": 8}, {"type": "image", "img_path": "pRQmRaonxf/tmp/f3401bd73ac9a3896a1230e6e14fdc751fb07c7f62b5205c277b57f96a37b9ca.jpg", "img_caption": ["Figure 2: Comparisons of Nash equilibrium (NE) gaps over episodes in both decentralized and centralized learning scenarios, averaged over 10 inference games. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "This work is more focused on the in-context reinforcement learning (ICRL) capability of pre-trained transformers, as demonstrated in Grigsby et al. [28], Laskin et al. [34], Lee et al. [35], Wang et al. [62]. The recent work by Lin et al. [38] initiates the theoretical investigation of this topic. In particular, Lin et al. [38] provides generalization guarantees after pre-training in the single-agent RL scenario, and further constructs transformers to realize provably efficient single-agent bandits and RL algorithms (in particular, LinUCB [1], Thompson sampling [54], UCB-VI [6]). This work extends Lin et al. [38] to the domain of competitive multi-agent RL by studying the in-context game-playing setting. A recent concurrent work [36] also touches upon the in-context game-playing capability of pre-trained transformers, while focusing on practical aspects and exploiting different opponents. ", "page_idx": 9}, {"type": "text", "text": "Competitive Multi-agent RL. The study of RL in the competitive multi-agent domain has a long and fruitful history [10, 47, 49, 58, 70]. In recent years, researchers have gained a deeper theoretical understanding of this topic. The centralized setting (also known as self-play) has been investigated in Bai and Jin [8], Bai et al. [9], Cui et al. [17], Huang et al. [30], Jin et al. [33], Liu et al. [40], Wang et al. [63], Xiong et al. [67], Zhang et al. [69], and this work focuses on the representative VI-ULCB design [9]. On the other hand, decentralized learning is more challenging, and the major breakthrough is made by V-learning [9, 32, 41, 51], which is thus adopted as the target algorithm in this work. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work investigated the in-context game-playing (ICGP) capabilities of pre-trained transformers, broadening the research scope of in-context RL from the single-agent scenario to the more challenging multi-agent competitive games. Focusing on the classical two-player zero-sum Markov games, a general learning framework was first introduced, laying down a solid ground for this and later studies. Through concrete theoretical results, this work further demonstrated that in both decentralized and centralized learning settings, properly pre-trained transformers are capable of approximating Nash equilibrium in an in-context manner. As a key part of the proof, concrete sets of parameterization were provided to demonstrate that the transformer architecture can realize two famous designs, decentralized V-learning and centralized VI-ULCB. Empirical experiments further validate the theoretical results (especially that pre-trained transformers can indeed approximate NE in an incontext manner) and motivate future studies on this under-explored research direction. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The work of CSs and KY was supported in part by the US National Science Foundation (NSF) under awards CNS-2002902, ECCS- 2029978, ECCS-2143559, and CNS-2313110, and the Bloomberg Data Science Ph.D. Fellowship. The work of JY was supported in part by the US NSF under awards CNS-1956276 andCNS-2114542. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1]  Abbasi-Yadkori, Y., Pal, D., and Szepesvari, C. (2011). Improved algorithms for linear stochastic bandits. Advances in neural information processing systems, 24.   \n[2] Ahn, K., Cheng, X., Daneshmand, H., and Sra, S. (2023). Transformers learn to implement preconditioned gradient descent for in-context learning. arXiv preprint arXiv:2306.00297.   \n[3] Akyurek, E., Schuurmans, D., Andreas, J., Ma, T., and Zhou, D. (2022). What learning algorithm is in-context learning? investigations with linear models. arXiv preprint arXiv:2211.15661.   \n[4]  Auer, P., Cesa-Bianchi, N., and Fischer, P. (2002a). Finite-time analysis of the multiarmed bandit problem. Machine learning, 47:235-256.   \n[5]  Auer, P., Cesa-Bianchi, N., Freund, Y., and Schapire, R. E. (2002b). The nonstochastic multiarmed bandit problem. SIAM journal on computing, 32(1):48-77.   \n[6]  Azar, M. G., Osband, I., and Munos, R. (2017). Minimax regret bounds for reinforcement learning. In International Conference on Machine Learning, pages 263-272. PMLR.   \n[7] Bai, Y., Chen, F., Wang, H., Xiong, C., and Mei, S. (2023). Transformers as statisticians: Provable in-context learning with in-context algorithm selection. arXiv preprint arXiv:2306.04637.   \n[8]  Bai, Y. and Jin, C. (2020). Provable self-play algorithms for competitive reinforcement learning. In International conference on machine learning, pages 551-560. PMLR.   \n[9] Bai, Y., Jin, C., and Yu, T. (2020). Near-optimal reinforcement learning with self-play. Advances in neural information processing systems, 33:2159-2170.   \n[10] Berner, C., Brockman, G., Chan, B., Cheung, V., Debiak, P., Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse, C., et al. (2019). Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680.   \n[11] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.   \n[12] Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., and Zagoruyko, S. (2020). End-to-end object detection with transformers. In European conference on computer vision, pages 213-229. Springer.   \n[13] Cesa-Bianchi, N. and Lugosi, G. (2006). Prediction, learning, and games. Cambridge university press.   \n[14] Cheng, X., Chen, Y., and Sra, S. (2023). Transformers implement functional gradient descent to learn non-linear functions in context. arXiv preprint arXiv:2312.06528.   \n[15] Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P, Chung, H. W., Sutton, C., Gehrmann, S., et al. (2023). Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1-113.   \n[16] Cui, Q. and Du, S. S. (2022). When are offline two-player zero-sum markov games solvable? Advances in Neural Information Processing Systems, 35:25779-25791.   \n[17] Cui, Q., Zhang, K., and Du, S. (2023). Breaking the curse of multiagents in a large state space: Rl in markov games with independent linear function approximation. In The Thiry Sixth Annual Conference on Learning Theory, pages 2651-2652. PMLR.   \n[18] Dai, D., Sun, Y, Dong, L., Hao, Y., Ma, S., Sui, Z., and Wei, F. (2023). Why can gpt learn in-context? language models secretly perform gradient descent as meta-optimizers. In Findings of the Association for Computational Linguistics: ACL 2023, pages 4005-4019.   \n[19] Daskalakis, C. (2013). On the complexity of approximating a nash equilibrium. ACM Transactions on Algorithms (TALG), 9(3): 1-35.   \n[20] Daskalakis, C., Fishelson, M., and Golowich, N. (2021). Near-optimal n-regret learning in general games. Advances in Neural Information Processing Systems, 34:27604-27616.   \n[21] Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv: 1810.04805.   \n[22] Dong, Q., Li, L., Dai, D., Zheng, C., Wu, Z., Chang, B., Sun, X., Xu, J., and Sui, Z. (2022). A survey for in-context learning. arXiv preprint arXiv:2301.00234.   \n[23] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.   \n[24] Fu, D., Chen, T.-Q., Jia, R., and Sharan, V. (2023). Transformers learn higher-order optimization methods for in-context learning: A study with linear models. arXiv preprint arXiv:2310.17086.   \n[25]  Gao, B. and Pavel, L. (2017). On the properties of the softmax function with application in game theory and reinforcement learning. arXiv preprint arXiv:1704.00805.   \n[26] Garg, S., Tsipras, D., Liang, P. S., and Valiant, G. (2022). What can transformers learn incontext? a case study of simple function classes. Advances in Neural Information Processing Systems, 35:30583-30598.   \n[27] Giannou, A., Rajput, S., Sohn, J.-y., Lee, K., Lee, J. D., and Papailiopoulos, D. (2023). Looped transformers as programmable computers. In International Conference on Machine Learning, pages 11398-11442. PMLR.   \n[28]  Grigsby, J., Fan, L., and Zhu, Y. (2023). Amago: Scalable in-context reinforcement learning for adaptive agents. arXiv preprint arXiv:2310.09971.   \n[29] Guo, T., Hu, W., Mei, S., Wang, H., Xiong, C., Savarese, S., and Bai, Y. (2023). How do transformers learn in-context beyond simple functions? a case study on learning with representations. arXiv preprint arXiv:2310.10616.   \n[30] Huang, B., Lee, J. D., Wang, Z., and Yang, Z. (2021). Towards general function approximation in zero-sum markov games. arXiv preprint arXiv:2107.14702.   \n[31] Huang, Y, Cheng, Y., and Liang, Y. (2023). In-context convergence of transformers. arXiv preprint arXiv:2310.05249.   \n[32] Jin, C., Liu, Q., Wang, Y, and Yu, T. (2023). V-learning--a simple, efficient, decentralized algorithm for multiagent reinforcement learning. Mathematics of Operations Research.   \n[33]  Jin, C., Liu, Q., and Yu, T. (2022). The power of exploiter: Provable multi-agent rl in large state spaces. In International Conference on Machine Learning, pages 10251-10279. PMLR.   \n[34] Laskin, M, Wang, L., Oh, J., Parisotto, E., Spencer, S., Steigerwald, R., Strouse, D., Hansen, S., Filos, A., Brooks, E, et al. (2022). In-context reinforcement learning with algorithm distillation. arXiv preprint arXiv:2210.14215.   \n[35] Lee, J. N., Xie, A., Pacchiano, A., Chandak, Y., Finn, C., Nachum, O., and Brunskill, E. (2023). Supervised pretraining can learn in-context reinforcement learning. arXiv preprint arXiv:2306.14892.   \n[36] Li, S., Yang, C., Zhang, Y., Li, P., Wang, X., Huang, X., Chan, H., and An, B. (2024). In-context exploiter for extensive-form games. arXiv preprint arXiv:2408.05575.   \n[37] Li, Y., Idiz, M. E., Papailiopoulos, D., and Oymak, S. (2023). Transformers as algorithms: Generalization and stabity in in-context learning. In International Conference on Machine Learning, pages 19565-19594. PMLR.   \n[38] Lin, L., Bai, Y., and Mei, S. (2023). Transformers as decision makers: Provable in-context reinforcement learning via supervised pretraining. arXiv preprint arXiv:2310.08566.   \n[39] Liu, J., Shen, D., Zhang, Y, Dolan, B., Carin, L., and Chen, W. (2021a). What makes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804.   \n[40] Liu, Q., Yu, T., Bai, Y., and Jin, C. (2021b). A sharp analysis of model-based reinforcement learning with self-play. In International Conference on Machine Learning, pages 7001-7010. PMLR.   \n[41] Mao, W. and Basar, T. (2023). Provably efficient reinforcement learning in decentralized general-sum markov games. Dynamic Games and Applications, 13(1):165-186.   \n[42]  OpenAI (2023). GPT-4 technical report. arXiv preprint, arXiv:2303.08774.   \n[43] Radford, A, Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. (2019). Language models are unsupervised multitask learners.   \n[44] Raventos, A., Paul, M., Chen, F., and Ganguli, S. (2023). Pretraining task diversity and the emergence of non-bayesian in-context learning for regression. arXiv preprint arXiv:2306.15063.   \n[45]  Reddy, G. (2023). The mechanistic basis of data dependence and abrupt learning in an in-context classification task. In The Twelth International Conference on Learning Representations.   \n[46]  Roughgarden, T. (2010). Algorithmic game theory. Communications of the ACM, 53(7):78-86.   \n[47] Shapley, L. S. (1953). Stochastic games. Proceedings of the national academy of sciences, 39(10):1095-1100.   \n[48] Shoham, Y. and Leyton-Brown, K. (2008). Muliagent systems: Algorithmic, game-theoretic, and logical foundations. Cambridge University Press.   \n[49] Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M., Bolton, A., et al. (2017). Mastering the game of go without human knowledge. nature, 550(7676):354-359.   \n[50] Sini, V., Nikulin, A., Kurenkov, V., Zisman, I., and Kolesnikov, S. (2023). In-context reinforcement learning for variable action spaces. arXiv preprint arXiv:2312.13327.   \n[51] Song, Z., Mei, S., and Bai, Y. (2021). When can we learn general-sum markov games with a large number of players sample-effciently? arXiv preprint arXiv:2110.04184.   \n[52]  Sutton, R. S. (1988). Learning to predict by the methods of temporal differences. Machine learning, 3:9-44.   \n[53] Syrgkanis, V, Agarwal, A., Luo, H, and Schapire, R. E. (2015). Fast convergence of regularized learning in games. Advances in Neural Information Processing Systems, 28.   \n[54]  Thompson, W. R. (1933). On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3/4):285-294.   \n[55] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Roziere, B., Goyal, N., Hambro, E., Azhar, F,et al. (2023a). Llama: Open and effcient foundation language models. arXiv preprint arXiv:2302.13971.   \n[56] Touvron, H, Martin, L., Stone, K., Albert, P, Almahairi, A., Babaei, Y, Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. (2023b). Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.   \n[57] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.   \n[58] Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds, T., Georgiev, P, et al. (2019). Grandmaster level in starcraf i using multi-agent reinforcement learning. Nature, 575(7782):350-354.   \n[59] Von Oswald, J., Niklasson, E., Randazzo, E., Sacramento, J., Mordvintsev, A., Zhmoginov, A., and Vladymyrov, M. (2023a). Transformers learn in-context by gradient descent. In International Conference on Machine Learning, pages 35151-35174. PMLR.   \n[60] Von Oswald, J., Niklasson, E., Schlegel, M., Kobayashi, S., Zucchet, N., Scherrer, N., Miller, N., Sandler, M., Vladymyrov, M., Pascanu, R., et al. (2023b). Uncovering mesa-optimization algorithms in transformers. arXiv preprint arXiv:2309.05858.   \n[61] Wainwright, M. J. (2019). High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cambridge university press.   \n[62] Wang, J., Blaser, E., Daneshmand, H., and Zhang, S. (2024). Transformers learn temporal difference methods for in-context reinforcement learning. arXiv preprint arXiv:2405.13861.   \n[63] Wang, Y, Liu, Q., Bai, Y., and Jin, C. (2023). Breaking the curse of multiagency: Provably efficient decentralized multi-agent rl with function approximation. arXiv preprint arXiv:2302.06606.   \n[64] Wu, J., Zou, D., Chen, Z., Braverman, V., Gu, Q., and Bartlett, P. L. (2023). How many pretraining tasks are needed for in-context learning of linear regression? arXiv preprint arXiv:2310.08391.   \n[65] Xie, Q., Chen, Y., Wang, Z., and Yang, Z. (2020). Learning zero-sum simultaneous-move markov games using function approximation and correlated equilibrium. In Conference on learning theory, pages 3674-3682. PMLR.   \n[66] Xie, S. M., Raghunathan, A., Liang, P., and Ma, T. (2021). An explanation of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080.   \n[67] Xiong, W., Zhong, H., Shi, C., Shen, C., and Zhang, T. (2022). A self-play posterior sampling algorithmforzero-sum markov games. In International Conference onMachine Learning,pages 24496-24523. PMLR.   \n[68] Yun, C., Bhojanapalli, S., Rawat, A. S., Reddi, S. J., and Kumar, S. (2019). Are transformers universal approximators of sequence-to-sequence functions? arXiv preprint arXiv: 1912.10077.   \n[69] Zhang, K., Kakade, S., Basar, T., and Yang, L. (2020). Model-based multi-agent rl in zero-sum markov games with near-optimal sample complexity. Advances in Neural Information Processing Systems, 33:1166-1178.   \n[70] Zhang, K., Yang, Z., and Basar, T. (2021). Multi-agent reinforcement learning: A selective overview of theories and algorithms. Handbook of reinforcement learning and control, pages 321-384.   \n[71] Zhang, R., Frei, S., and Bartlett, P. L. (2023). Trained transformers learn linear models in-context. arXiv preprint arXiv:2306.09927.   \n[72] Zhong, H., Xiong, W., Tan, J., Wang, L., Zhang, T., Wang, Z., and Yang, Z. (2022). Pessimistic minimax value iteration: Provably efficient equilibrium learning from offline datasets. In International Conference on Machine Learning, pages 27117-27142. PMLR.   \n[73] Zisman, I., Kurenkov, V., Nikulin, A., Sini, V., and Kolesnikov, S. (2023). Emergence of in-context reinforcement learning from noise distillation. arXiv preprint arXiv:2312.12275. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A An Overview of the Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, an overview of the appendix is provided. First, additional discussions are presented in Appendix B, which cover broader impacts of this work and our thoughts on the future directions. ", "page_idx": 14}, {"type": "text", "text": "Then, the proof details omitted in this main paper are provided. While the decentralized learning setting is the major focus in the main paper, the discussions and proofs for the centralized learning setting are first provided to facilitate the presentation and understanding as the decentralized learning setting is more challenging. ", "page_idx": 14}, {"type": "text", "text": "\u00b7 The supervised pre-training guarantee (i.e., Theorem C.3) for the centralized learning setting is proved in Appendix C. The details and realization of VI-ULCB (i.e., Theorem 4.1) are presented in Appendix D. The proofs for the overall performance guarantee (i.e., Theorem 4.2) can be found in AppendixE.   \n\u00b7 Subsequently, the proofs for the supervised pre-training guarantee (i.e., Theorem 3.3) in the decentralized learning setting are provided in Appendix F. Appendix G contains the details and realization of V-learning (i.e., Theorem 3.4). The overall performance guarantee (i.e., Theorem 3.5) is proved in Appendix H.   \n\u00b7 A detailed discussion of the covering number is provided in Appendix I. ", "page_idx": 14}, {"type": "text", "text": "Finally, the setups and details of the experiments presented in Sec. 5 are reported in Appendix J. ", "page_idx": 14}, {"type": "text", "text": "B  Additional Discussions ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1   Broader Impacts ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "This work mainly provides a theoretical understanding of the in-context game-playing capabilities of pre-trained transformers, broadening the research scope of in-context reinforcement learning from single-agent settings to multi-agent competitive games. Due to its theoretical nature, we do not foresee major negative societal impacts; however, we still would like to acknowledge the need for responsible usage of the practical implementation of the proposed game-playing transformers due to their high capability in various environments. ", "page_idx": 14}, {"type": "text", "text": "B.2 Limitations and Future Works ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The research direction of in-context game-playing is currently under-explored, and we believe that there are many interesting topics to be further investigated. ", "page_idx": 14}, {"type": "text", "text": "\u00b7 Different game forms and game-solving algorithms. This work mainly studies the classical twoplayer zero-sum Markov games, which can be viewed as the most basic form of competitive games, and has particular focuses on constructing transformers to realize V-learning [32] and VI-ULCB [8]. The cooperative games, on the other hand, are conceptually more similar to the single-agent setting [38]. There are more complicated game forms [46, 48], e.g., the mixed cooperative-competitive games, which requires different game-solving algorithms. We believe the framework built in this work is beneficial to further explore the capabilities of pre-trained models in game-theoretical scenarios. ", "page_idx": 14}, {"type": "text", "text": "\u00b7 Pre-training dataset construction. This work considering the pre-training dataset is collected from the context algorithm with an additional augmentation. First, while the current proofs rely on the augmentation, it will be an interesting topic to understand whether it is necessary. As mentioned Sec. 3.1.1, learning in Markov games typically require more diverse data than learning in singleagent settings; however, the minimum requirement to perform effective pre-training is worth further exploring. Moreover, in the study of single-agent RL [35], it is shown that pre-training with the data from the optimal policy is more efficient, which is further theoretically investigated in Lin et al. [38]. In multi-agent competitive games, it is currently unclear whether similar strategies can be incorporated, e.g., pre-training with data collected by Nash equilibrium policies or best responses for certain other policies. ", "page_idx": 14}, {"type": "text", "text": "\u00b7 Large-scale empirical evaluations. Due to the limited computation resources, the experiments reported in Sec. 5 are relatively small-scale compared with the current size of practically adopted transformers. It would be an important and interesting direction to further evaluate the ICGP capabilities of pre-trained transformers in large-scale experiments and practical game-theoretic applications. Also, the training dynamics are also worth further investigation, e.g., the sudden shifts in learning effectiveness reported by Reddy [45]. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Besides these directions, from the theoretical perspective, we believe it would be valuable to investigate how to extend the current study on the tabular setting to incorporate function approximation, where we conjecture it is sufficient for the pre-training dataset to cover information of certain representative states and actions (e.g., a well-coverage of the feature space) [72]. Another attractive theoretical question is how to learn from a dataset collected by multiple context algorithms. From the practical perspective, a future study on the impact of the practical training recipe (e.g., model structure, training hyperparameters, etc.) would be desirable to bring additional insights. ", "page_idx": 15}, {"type": "text", "text": "C Proofs for the Centralized Supervised Pre-training Guarantees ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "First, the definition of the centralized covering number and an assumption of centralized approximate realizability are introduced to facilitate the analysis, which are also leveraged in Lin et al. [38]. ", "page_idx": 15}, {"type": "text", "text": "Definition C.1 (Centralized Covering Number). For a class of algorithms $\\left\\{{\\tt A l g}_{\\theta}:\\theta\\in\\Theta\\right\\}$ we say $\\tilde{\\Theta}\\subseteq\\Theta$ is a $\\rho$ cover of $\\Theta$ $i f\\tilde{\\Theta}$ is a finite set such that for any $\\pmb\\theta\\in\\Theta$ there exists $\\tilde{\\pmb{\\theta}}\\in\\tilde{\\Theta}$ suchthat for all $D^{t-1},s\\in S,t\\in[T].$ it holds that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left\\|\\log\\tt{A}\\mathtt{{L}}\\mathtt{{g}}_{\\tilde{\\theta}}(\\cdot,\\cdot|D^{t-1},s)-\\log\\tt{A}\\mathtt{{L}}\\mathtt{{g}}_{\\theta}(\\cdot,\\cdot|D^{t-1},s)\\right\\|_{\\infty}\\le\\rho.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The covering number $\\mathcal{N}_{\\Theta}(\\rho)$ is the minimal cardinality of $\\Tilde{\\Theta}$ suchthat $\\Tilde{\\Theta}$ isa $\\rho$ -coverof $\\Theta$ ", "page_idx": 15}, {"type": "text", "text": "Assumption C.2 (Centralized Approximate Realizability). There exist $\\pmb{\\theta}^{*}\\in\\Theta$ and $\\varepsilon_{\\mathrm{real}}>0$ such thatforall $s\\in S,t\\in[T],(a,b)\\in\\mathcal{A}\\times\\mathcal{B},$ itholdsthat ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\log\\left(\\mathbb{E}_{D\\sim\\mathbb{P}_{\\Lambda}^{\\mathbb{A}\\mathbb{g}_{0}}}\\left[\\frac{\\mathtt{A}\\mathtt{l}\\mathrm{g}_{0}(a,b|D^{t-1},s)}{\\mathtt{A}\\mathtt{l}\\mathrm{g}_{\\theta^{*}}(a,b|D^{t-1},s)}\\right]\\right)\\leq\\varepsilon_{\\mathrm{real}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then, the following pre-training guarantee can be established. ", "page_idx": 15}, {"type": "text", "text": "Theorem C.3 (Centralized Pre-training Guarantee). Let 0 be the maximum likelihood pre-training outputTake $\\ensuremath{\\mathcal{N}_{\\Theta}}=\\ensuremath{\\mathcal{N}_{\\Theta}}(1/N)$ as in Definition C.1. Then, under Assumption C.2, with probability at least $1-\\delta_{i}$ itholdsthat ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}_{D\\sim\\mathbb{P}_{\\Lambda}^{\\mathrm{Ag}_{0}}}\\left[\\sum_{t\\in[T],s\\in\\mathcal{S}}\\mathrm{TV}(\\mathtt{A l g}_{0},\\mathtt{A l g}_{\\widehat{\\theta}}|D^{t-1},s)\\right]\\lesssim T S\\sqrt{\\varepsilon_{\\mathrm{real}}}+T S\\sqrt{\\frac{\\log\\left(\\mathcal{N}_{\\Theta}T S/\\delta\\right)}{N}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof of Theorem C.3. This proof extends that of Theorem 6 in Lin et al. [38] to the multi-agent scenario. Let $\\Tilde{\\Theta}$ be a $\\rho$ -coveringsetof $\\Theta$ withcovering number $\\ensuremath{\\mathcal{N}_{\\Theta}}\\,=\\,\\ensuremath{\\mathcal{N}_{\\Theta}}(\\rho)$ as defined in Definition C.1. With Lemma 15 in Lin et al. [38], we can obtain that for any $\\theta\\in\\Theta$ ,thereexists $\\tilde{\\pmb{\\theta}}\\in\\tilde{\\Theta}$ such that for all $D^{t-1}$ \uff0c $t\\in[T]$ and $s\\in S$ \uff0c ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname{TV}\\left(\\mathrm{A}1\\mathrm{g}_{\\tilde{\\theta}},\\mathrm{A}1\\mathrm{g}_{\\theta}|D^{t-1},s)\\right)\\leq\\rho\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For $m\\in[N_{\\Theta}],t\\in[T],i\\in[N],s\\in\\mathcal{S}$ , we define that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\ell_{i,m}^{t}(s):=\\log\\left(\\frac{\\mathrm{A}\\mathrm{L}_{0}\\,\\log\\big(a_{i,s}^{t},b_{i,s}^{t}|D_{i}^{t-1},s\\big)}{\\mathrm{A}\\mathrm{L}_{\\theta_{m}}\\,\\big(a_{i,s}^{t},b_{i,s}^{t}|D_{i}^{t-1},s\\big)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "According to Lemma 14 in Lin et al. [38], with probability at least $1-\\delta$ ,for all $m\\in[\\mathcal{N}_{\\Theta}],t\\in$ $[T],s\\in{\\bar{S}}$ , it holds that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\sum_{i\\in[N]}\\ell_{i,m}^{t}(s)+\\log(\\mathcal{N}_{\\Theta}T S/\\delta)\\geq\\sum_{i\\in[N]}-\\log\\left(\\mathbb{E}\\left[\\exp\\left(-\\frac{\\ell_{i,m}^{t}(s)}{2}\\right)\\right]\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Furthermore, it can be established that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\exp\\left(-\\frac{\\ell_{i,m}^{t}(s)}{2}\\right)|D_{i}^{t-1}\\right]=\\mathbb{E}\\left[\\sqrt{\\frac{\\mathbb{A}\\mathbb{E}_{\\pmb{\\theta}_{m}}\\left(a_{i,s}^{t},b_{i,s}^{t}|D_{i}^{t-1},s\\right)}{\\mathbb{A}\\mathbb{E}_{\\mathbf{1}}\\mathbb{E}_{\\mathbf{0}}\\left(a_{i,s}^{t},b_{i,s}^{t}|D_{i}^{t-1},s\\right)}}|D_{i}^{t-1}\\right]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n=\\sum_{(a,b)\\in{\\cal A}\\times{\\cal B}}\\sqrt{\\mathrm{A}\\mathrm{L}}g_{\\theta_{m}}\\left(a,b|D_{i}^{t-1},s\\right)\\mathrm{A}\\mathrm{L}g_{0}\\left(a,b|D_{i}^{t-1},s\\right)\\!,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which implies that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\exp\\left(-\\frac{\\ell_{i,m}^{t}\\left(s\\right)}{2}\\right)\\right]=1-\\frac{1}{2}\\cdot\\mathbb{E}\\left[\\displaystyle\\sum_{(a,b)\\in A\\times B}\\left(\\sqrt{\\mathtt{A l g}_{\\theta_{m}}\\left(a,b|D_{i}^{t-1},s\\right)}-\\sqrt{\\mathtt{A l g}_{0}\\left(a,b|D_{i}^{t-1},s\\right)}\\right)^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leq1-\\frac{1}{2}\\cdot\\mathbb{E}\\left[\\mathrm{TV}\\left(\\mathtt{A l g}_{\\theta_{m}},\\mathtt{A l g}_{0}|D_{i}^{t-1},s\\right)^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the inequality is from the fact that the Hellinger distance is smaller than the TV distance. Then, we can obtain that for any $\\pmb{\\theta}$ coveredby $\\theta_{m}$ , it holds that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\mathbb{E}_{D}\\left[\\mathrm{TV}\\left(\\mathbb{A}\\mathrm{I}\\mathrm{g}_{0},\\mathbb{A}\\mathrm{I}g_{\\theta}\\big|D^{t-1},s\\right)\\right]\\right)^{2}}\\\\ &{\\le\\left(\\mathbb{E}_{D}\\left[\\mathrm{TV}\\left(\\mathrm{A}\\mathrm{I}\\mathrm{g}_{0},\\mathrm{A}\\mathrm{I}\\mathrm{g}_{\\theta_{m}}\\big|D^{t-1},s\\right)\\right]+\\mathbb{E}_{D}\\left[\\mathrm{TV}\\left(\\mathrm{A}\\mathrm{I}\\mathrm{g}_{\\theta_{m}},\\mathrm{A}\\mathrm{I}\\mathrm{g}_{\\theta}\\big|D^{t-1},s\\right)\\right]\\right)^{2}}\\\\ &{\\le2\\left(\\mathbb{E}_{D}\\left[\\mathrm{TV}\\left(\\mathrm{A}\\mathrm{I}\\mathrm{g}_{0},\\mathrm{A}\\mathrm{I}\\mathrm{g}_{\\theta_{m}}\\big|D^{t-1},s\\right)\\right]\\right)^{2}+2\\left(\\mathbb{E}_{D}\\left[\\mathrm{TV}\\left(\\mathrm{A}\\mathrm{I}\\mathrm{g}_{\\theta_{m}},\\mathrm{A}\\mathrm{I}g_{\\theta}\\big|D^{t-1},s\\right)\\right]\\right)^{2}}\\\\ &{\\le2\\mathbb{E}_{D}\\left[\\mathrm{TV}\\left(\\mathrm{A}\\mathrm{I}\\mathrm{g}_{0},\\mathrm{A}\\mathrm{I}\\mathrm{g}_{\\theta}\\big|D^{t-1},s\\right)^{2}\\right]+2\\rho^{2}}\\\\ &{\\le4-4\\mathbb{E}\\left[\\exp\\left(-\\frac{\\ell_{i}^{\\prime},m}{2}\\binom{s}{2}\\right)\\right]+2\\rho^{2}}\\\\ &{\\le-4\\log\\left(\\mathbb{E}\\left[\\exp\\left(-\\frac{\\ell_{i}^{\\prime},m}{2}\\binom{s}{2}\\right)\\right]\\right)+2\\rho^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which further implies that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{N\\underset{s\\in\\mathcal{S}}{\\sum}\\underset{t\\in\\mathbb{Z}}{\\sum}\\left(\\mathbb{E}_{D}\\left[\\mathbb{T}\\left(\\mathbb{A}\\mathbf{g}_{0},\\mathbb{A}\\mathbf{g}_{\\theta}\\big|D^{t-1},s\\right)\\right]\\right)^{2}}\\\\ &{\\leq-4\\underset{s\\in\\mathcal{S}}{\\sum}\\underset{t\\in\\mathbb{Z}}{\\sum}\\underset{t\\in[N]}{\\sum}\\log\\left(\\mathbb{E}\\left[\\exp\\left(-\\frac{\\ell_{i,m}^{t}\\left(s\\right)}{2}\\right)\\right]\\right)+2N S T\\rho^{2}}\\\\ &{\\leq2\\underset{s\\in\\mathcal{S}}{\\sum}\\underset{t\\in[T]}{\\sum}\\underset{\\epsilon\\in[N]}{\\sum}\\;\\bigg(\\epsilon_{i,m}^{t}(s)+2N S T\\rho^{2}+4S T\\log(N_{\\Theta}T S/\\delta)}\\\\ &{=2\\underset{s\\in\\mathcal{S}}{\\sum}\\underset{t\\in[T]}{\\sum}\\sum\\underset{t\\in[N]}{\\log}\\left(\\frac{\\mathbb{A}\\mathbf{g}_{0}\\left(a_{i,s}^{t},b_{i,s}^{t}|D_{i}^{t-1},s\\right)}{\\mathbb{A}\\mathbf{E}_{\\theta_{0}}\\left(a_{i,s}^{t},b_{i,s}^{t}|D_{i}^{t-1},s\\right)}\\right)+2N S T\\rho^{2}+4S T\\log(N_{\\Theta}T S/\\delta)}\\\\ &{\\leq2\\underset{s\\in\\mathcal{S}}{\\sum}\\underset{t\\in[T]}{\\sum}\\sum\\underset{i\\in[N]}{\\log}\\left(\\frac{\\mathbb{A}\\mathbf{g}_{0}\\left(a_{i,s}^{t},b_{i,s}^{t}|D_{i}^{t-1},s\\right)}{\\mathbb{A}\\mathbf{E}_{\\theta_{0}}\\left(a_{i,s}^{t},b_{i,s}^{t}|D_{i}^{t-1},s\\right)}\\right)+2N S T\\rho^{2}+2N S T\\rho+4S T\\log(N_{\\Theta}T S/\\delta)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus, for the obtained $\\widehat{\\pmb{\\theta}}$ , with probability at least $1-\\delta$ , it holds that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{N\\displaystyle\\sum_{s\\in\\mathcal{S}}\\displaystyle\\sum_{t\\in[T]}\\left(\\mathbb{E}_{D}\\left[\\lceil\\mathrm{TV}\\left(\\mathrm{Alg}_{0},\\boldsymbol{\\mathrm{Alg}_{\\bar{\\theta}}}\\rceil D^{t-1},s\\right)\\right]\\right)^{2}}\\\\ &{\\le2\\displaystyle\\sum_{s\\in\\mathcal{S}}\\displaystyle\\sum_{t\\in[T]}\\sum_{i\\in[N]}\\log\\left(\\frac{\\mathrm{Alg}_{0}\\left(a_{i,s}^{t},b_{i,s}^{t}|D_{i}^{t-1},s\\right)}{\\mathrm{Alg}_{\\bar{\\theta}}\\left(a_{i,s}^{t},b_{i,s}^{t}|D_{i}^{t-1},s\\right)}\\right)+2N S T\\rho^{2}+2N S T\\rho+4S T\\log(N_{\\Theta}T S/\\delta}\\\\ &{\\le2\\displaystyle\\sum_{s\\in\\mathcal{S}}\\displaystyle\\sum_{t\\in[T]}\\sum_{i\\in[N]}\\log\\left(\\frac{\\mathrm{Alg}_{0}\\left(a_{i,s}^{t},b_{i,s}^{t}|D_{i}^{t-1},s\\right)}{\\mathrm{Alg}_{\\Theta}\\left(a_{i,s}^{t},b_{i,s}^{t}|D_{i}^{t-1},s\\right)}\\right)+2N S T\\rho^{2}+2N S T\\rho+4S T\\log(N_{\\Theta}T S/\\delta)}\\\\ &{\\le2\\displaystyle\\sum_{s\\in\\mathcal{S}}\\displaystyle\\sum_{t\\in[T]}\\sum_{i\\in[N]}\\log\\left(\\mathbb{E}\\left[\\frac{\\mathrm{Alg}_{0}\\left(a_{i,s}^{t},b_{i,s}^{t}|D_{i}^{t-1},s\\right)}{\\mathrm{Alg}_{\\Theta}\\left(a_{i,s}^{t},b_{i,s}^{t}|D_{i}^{t-1},s\\right)}\\right]\\right)+S T\\log(T S/\\delta)}\\\\ &{+2N S T\\rho^{2}+2N S T\\rho+4S T\\log(N_{\\Theta}T S/\\delta),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Further by Cauchy-Schwarz inequality, we can obtain that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{s\\in S}\\displaystyle\\sum_{t\\in[T]}\\left(\\mathbb{E}_{D}\\left[\\mathrm{TV}\\left(\\mathbb{A}\\mathbf{1}\\mathbf{g}_{0},\\mathbb{A}\\mathbf{1}\\mathbf{g}_{\\widehat{\\theta}}|D^{t-1},s\\right)\\right]\\right)}\\\\ &{\\displaystyle\\leq\\sqrt{S T\\sum_{s\\in\\mathcal{S}}\\sum_{t\\in[T]}\\left(\\mathbb{E}_{D}\\left[\\mathrm{TV}\\left(\\mathbb{A}\\mathbf{1}\\mathbf{g}_{0},\\mathbb{A}\\mathbf{1}\\mathbf{g}_{\\widehat{\\theta}}|D^{t-1},s\\right)\\right]\\right)^{2}}}\\\\ &{\\displaystyle=O\\left(S T\\sqrt{\\varepsilon_{\\mathrm{real}}}+S T\\sqrt{\\frac{\\log(\\sqrt{\\mathit{N}\\varphi S T})}{N}}+S T\\sqrt{\\rho}+S T\\rho\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Taking $\\rho=1/N$ concludes the proof. ", "page_idx": 17}, {"type": "text", "text": "D  Proofs for Realizing VI-ULCB ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "D.1 Details of MWU VI-ULCB ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We here note one distinction from the VI-ULCB design considered in this work from its vanilla version proposed in Bai and Jin [8], which makes VI-ULCB practically implementable. Especially, Bai and Jin [8] requires an oracle solver that can provide the exact NE policy pair $(\\mu^{*},\\nu^{*})$ from any two general input payoff matrices $(\\overline{{Q}},\\underline{{Q}})\\in\\mathbb{R}^{A\\times B}\\times\\mathbb{R}^{A\\times B}$ However, it is known that approximating such a general-sum NE is computationally hard (specifically, PPAD-complete) [19], which makes this vanilla version impractical. Luckily, later studies [9, 40, 65] have demonstrated that a solver finding one weaker notation of equilibrium, i.e., coarse correlated equilibrium (CCE), is already sufficient. Following these recent results, we replace the NE solver with an approximate CCE solver in VI-ULCB. Moreover, we consider finding such CCEs via no-regret learning.2 In particular, both players virtually run multiplicative weight update (MWU) (which is also known as Hedge), a classical no-regret algorithm, with payoff matrices $(\\overline{{Q}},\\underline{{Q}})$ for several rounds; then, an aggregated policy can be generated as an approximate CCE. The details of the VI-ULCB algorithm are provided in Alg. 1. ", "page_idx": 17}, {"type": "text", "text": "More specifically, we consider that an approximate CCE solver is adopted such that with each pair of inputs $(\\overline{{\\boldsymbol{Q}}}^{h}(s,\\cdot,\\cdot),\\underline{{\\boldsymbol{Q}}}^{h}(s,\\cdot,\\cdot))$ , we can obtain an $\\varepsilon_{\\mathrm{CCE}}$ -approximateCCEpolicy $\\pi^{h}(\\cdot,\\cdot|s)$ Which satisfiesthat ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{(a,b)\\sim\\pi^{h}(\\cdot,\\cdot|s)}\\left[\\overline{{Q}}(s,a,b)\\right]\\geq\\underset{a^{*}\\in\\mathcal{S}}{\\operatorname*{max}}\\mathbb{E}_{(a,b)\\sim\\pi^{h}(\\cdot,\\cdot|s)}\\left[\\overline{{Q}}(s,a^{*},b)\\right]-\\varepsilon_{\\mathrm{CCE}}}\\\\ &{\\mathbb{E}_{(a,b)\\sim\\pi^{h}(\\cdot,\\cdot|s)}\\left[\\underline{{Q}}(s,a,b)\\right]\\leq\\underset{b^{*}\\in\\mathcal{S}}{\\operatorname*{min}}\\mathbb{E}_{(a,b)\\sim\\pi^{h}(\\cdot,\\cdot|s)}\\left[\\underline{{Q}}(s,a,b^{*})\\right]+\\varepsilon_{\\mathrm{CCE}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We also specifically choose to obtain such approximate CCEs by having both players (virtually) perform MWU against each other. The details of MWU are included in Alg. 2, where we use the following notations to denote normalized losses: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\overline{{L}}^{h}(s,a,b):=\\frac{H-\\overline{{Q}}^{h}(s,a,b)}{H},\\:\\:\\underline{{L}}^{h}(s,a,b):=\\frac{H-\\underline{{Q}}^{h}(s,a,b)}{H}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Standard online learning results [13] guarantee that using learning rates $\\eta_{A}=\\sqrt{\\log(A)/N_{\\mathrm{MWU}}}$ and $\\eta_{B}=\\sqrt{\\log(B)/N_{\\mathrm{MWU}}}$ ,after $N_{\\mathrm{MWU}}$ rounds of MwU, the policy ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\pi^{h}(\\cdot,\\cdot|s)=\\frac{1}{N_{\\mathrm{MWU}}}\\sum_{n\\in[N]}\\mu_{n}^{h}(\\cdot|s)\\nu_{n}^{h}(\\cdot|s)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "is an $\\varepsilon_{\\mathrm{CCE}}$ -approximate CCE policy, with ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\varepsilon_{\\mathrm{CCE}}=H\\sqrt{\\frac{\\log(A+B)}{N_{\\mathrm{MWU}}}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "1: Initialize: for any $(s,a,b,h)$ \uff0c $\\overline{{Q}}^{h}(s,a,b)\\ \\gets\\ H$ \uff0c $\\underline{{Q}}^{h}(s,a,b)~\\leftarrow~0$ \uff0c $N^{h}(s,a,b)\\;\\;\\leftarrow\\;\\;0$   \n$N^{h}(s,a,b,s^{\\prime})\\gets0$   \n2: for episode $g=1,\\cdot\\cdot\\cdot,G$ do   \n3:for $(s,a,b)\\in S\\times A\\times B$ do   \n4: Compute $\\begin{array}{r l}&{\\overline{{Q}}^{h}(s,a,b)\\gets\\operatorname*{min}\\left\\{\\hat{r}^{h}(s,a,b)+\\left[\\hat{\\mathbb{P}}^{h}\\overline{{V}}^{h+1}\\right](s,a,b)+c\\sqrt{\\frac{H^{2}S\\iota}{N^{h}(s^{h},a^{h},b^{h})}},H\\right\\}}\\\\ &{\\underline{{Q}}^{h}(s,a,b)\\gets\\operatorname*{max}\\left\\{\\hat{r}^{h}(s,a,b)+\\left[\\hat{\\mathbb{P}}^{h}\\underline{{V}}^{h+1}\\right](s,a,b)-c\\sqrt{\\frac{H^{2}S\\iota}{N^{h}(s^{h},a^{h},b^{h})}},0\\right\\}}\\end{array}$   \n5: Compute   \n6: end for   \n7: for $s\\in S$ do   \n8: Update $\\pi_{h}(\\cdot,\\cdot|s)\\;\\leftarrow\\;\\varepsilon_{N}$ approximate $\\mathrm{CCE}\\left(\\overline{{Q}}^{h}(s,\\cdot,\\cdot),\\underline{{Q}}^{h}(s,\\cdot,\\cdot)\\right)$ solved by $N$ round   \nMWU   \n9: Compute $\\begin{array}{r}{\\overline{{V}}^{h}(s)\\leftarrow\\sum_{a,b}\\pi^{h}(a,b|s)\\overline{{Q}}^{h}(s,a,b)}\\end{array}$   \n10: Compute $\\begin{array}{r}{\\underline{{V}}^{h}(s)\\leftarrow\\sum_{a,b}\\pi^{h}(a,b|s)\\underline{{Q}}^{h}(s,a,b)}\\end{array}$   \n11: end for   \n12: for step $h=1,\\cdots\\,,H$ do   \n13: Take action $(a^{h},b^{h})\\sim\\pi^{h}(\\cdot,\\cdot|s^{h})$   \n14: Observe reward $r^{h}$ and next state $s^{h+1}$   \n15: Update $N_{h}(s^{h},a^{h},b^{h})$ and $N^{h}(s^{h},a^{h},b^{h},s^{h+1})$   \n16: Update $\\hat{\\mathbb{P}}^{h}(\\cdot|s^{h},a^{h},b^{h})$ and $\\hat{r}^{h}(s^{h},a^{h},b^{h})$   \n17: end for   \n18: end for ", "page_idx": 18}, {"type": "text", "text": "Algorithm 2 MWU ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1: Input: learning rates $\\eta_{A}=\\sqrt{\\log(A)/N}$ and $\\eta_{B}=\\sqrt{\\log(B)/N}$ , action sets $\\boldsymbol{\\mathcal{A}}$ and $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ with size   \n$A$ and $B$ , loss matrices $\\overline{{L}}^{h}(s,\\cdot,\\cdot)$ and $\\underline{{L}}^{h}(s,\\cdot,\\cdot)$   \n2: Initialize: cumulative loss $O_{+}\\leftarrow\\mathbf{0}_{A}$ and $O_{-}\\leftarrow\\mathbf{0}_{A}$   \n3: for $n=1,\\cdot\\cdot\\cdot\\,,N$ do   \n4:Compute $\\mu_{n}^{h}(\\cdot|s)\\gets\\sigma_{\\mathrm{s}}(-\\eta_{A}O_{+})\\in\\Delta(A)$   \n5:Compute $\\nu_{n}^{h}(\\cdot|s)\\leftarrow\\sigma_{\\mathrm{s}}(-\\eta_{B}O_{-})\\in\\Delta(\\mathcal{B})$   \n6: Observe vectors $o_{+,n}\\in\\mathbb{R}^{A}$ with $o_{+,n}(a)=\\nu_{n}^{h}(\\cdot|s)\\cdot\\overline{{L}}^{h}(s,a,\\cdot)$   \n7: Observe vectors $o_{-,n}\\in\\mathbb{R}^{B}$ with $o_{-,n}(b)=\\mu_{n}^{h}(\\cdot|s)\\cdot\\underline{{L}}^{h}(s,\\cdot,b)$   \n8: Update $O_{+}=O_{+}+o_{+,n}$ and $O_{-}=O_{-}+o_{-,n}$   \n9: end for   \n10: Output: policy $\\begin{array}{r}{\\sum_{n\\in[N]}\\mu_{n}^{h}(\\cdot|s)\\nu_{n}^{h}(\\cdot|s)/N}\\end{array}$ ", "page_idx": 18}, {"type": "text", "text": "Furthermore, for a certain bounded $\\varepsilon_{\\mathrm{CCE}}$ , Xie et al. [65] demonstrated that the performance degradation can still be controlled. Following the results therein, the following theorem can be easily established. ", "page_idx": 18}, {"type": "text", "text": "Theorem D.1 (Modified from Theorem 2 from Bai and Jin [8]). With probability at least $1-\\delta$ in anyenvironment $M$ theoutputpolicies $\\left\\{(\\mu^{g},\\nu^{g}):g\\in[G]\\right\\}$ from theMWU-version of VI-ULCB satisfythat ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{g\\in[G]}V_{M}^{\\dagger,\\nu^{g}}(s^{1})-V_{M}^{\\mu^{g},\\dagger}(s^{1})=O\\left(\\sqrt{H^{3}S^{2}A B T\\log(S A B T/\\delta)}+T\\varepsilon_{\\mathrm{CCE}}\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "With $\\delta=1/T$ , to have a non-dominant loss caused by the approximate CCE solver, we can choose $N_{\\mathrm{MWU}}=G$ . Then, for any environment $M$ , it holds that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}_{D\\sim\\mathbb{P}_{M}^{\\mathrm{v1:ULCB}}}\\left[\\sum_{g\\in[G]}V_{M}^{\\dagger,\\nu^{g}}(s^{1})-V_{M}^{\\mu^{g},\\dagger}(s^{1})\\right]=O\\left(\\sqrt{H^{3}S^{2}A B T\\log(S A B T)}\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "D.2 Proof of Theorem 4.1: The Realization Construction ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "D.2.1 Embeddings and Extraction Mapping ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We consider each episode of observations to be embedded in $2H$ tokens. In particular, for each $t\\in[T]$ , we construct that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h_{2t-1}=\\mathbf{h}(s^{\\theta,h})=\\left[\\begin{array}{c}{0_{A}}\\\\ {0_{B}}\\\\ {-\\frac{1}{\\theta_{s}^{0}\\bar{h}}-\\left[\\begin{array}{c}{h_{2t-1}^{\\mathrm{pr},a}}\\\\ {h_{2t-1}^{\\mathrm{pr},a}}\\\\ {-\\frac{1}{\\theta_{s}^{0}\\bar{h}}-\\cdot}\\\\ {0_{B}\\mathrm{or}_{-1}^{\\mathrm{pr},a}}\\end{array}\\right]=\\left[\\begin{array}{c}{h_{2t-1}^{\\mathrm{pr},a}}\\\\ {-\\frac{h_{2t-1}^{\\mathrm{pr},a}}{h_{2t-1}^{\\mathrm{pr},b}}}\\\\ {\\cdot\\frac{h_{2t-1}^{\\mathrm{pr},a}}{h_{2t-1}^{\\mathrm{pr},a}}\\cdot}\\\\ {\\cdot\\frac{h_{2t-1}^{\\mathrm{pr},a}}{h_{2t-1}^{\\mathrm{pr},a}}}\\end{array}\\right]\\,,}\\\\ {h_{2t}=\\mathbf{h}(a^{\\theta,h},b^{\\theta,h},r^{\\theta,h})=\\left[\\begin{array}{c}{a^{\\theta,h}}\\\\ {b^{\\theta,h}}\\\\ {\\cdot\\frac{r^{\\theta,h}}{\\theta_{s}^{0}\\bar{h}}}\\\\ {0_{A}^{\\theta,h}}\\\\ {\\cdot\\frac{h_{2t-1}^{\\mathrm{pr},a}}{h_{2t}^{\\mathrm{pr},a}}\\cdot}\\\\ {0_{B}^{\\theta,h}}\\end{array}\\right]=\\left[\\begin{array}{c}{h_{2t-1}^{\\mathrm{pr},a}}\\\\ {\\cdot\\frac{h_{2t-1}^{\\mathrm{pr},a}}{h_{2t-1}^{\\mathrm{pr},b}}\\cdot}\\\\ {\\cdot\\frac{h_{2t-1}^{\\mathrm{pr},a}}{h_{2t}^{\\mathrm{pr},a}}\\cdot}\\\\ {\\cdot\\frac{h_{2t-1}^{\\mathrm{pr},a}}{h_{2t}^{\\mathrm{pr},a}}\\cdot}\\end{array}\\right]\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $s^{g,h},a^{g,h},b^{g,h}$ are represented via one-hot embedding. The positional embedding ${\\bf p o s}_{i}$ is defined as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{pos}_{i}:=\\left[\\begin{array}{c}{g}\\\\ {h}\\\\ {t}\\\\ {e_{h}}\\\\ {v_{i}}\\\\ {i}\\\\ {i^{2}}\\\\ {1}\\end{array}\\right],\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $e_{h}$ is a one-hot vector with the $h$ -th element being 1 and $v_{i}:=\\mathbb{1}\\{h_{i}^{a}=\\mathbf{0}\\}$ denote the tokens that do not embed actions and rewards. ", "page_idx": 19}, {"type": "text", "text": "In summary, for observations $D^{t-1}\\cup\\{s^{t}\\}$ , we obtain the following tokens of length $2t-1$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\pmb H}:=h({\\pmb D}^{t-1},s^{t})=[{\\pmb h}_{1},{\\pmb h}_{2},\\cdot\\cdot\\cdot\\,,{\\pmb h}_{2t-1}]=[{\\pmb h}(s^{1}),{\\pmb h}(a^{1},b^{1},r^{1}),\\cdot\\cdot\\cdot\\,,{\\pmb h}(s^{t})].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "With the above input $\\pmb{H}$ , the transformer outputs $\\overline{{{H}}}\\,=\\,\\mathrm{TF}_{\\pmb{\\theta}_{+}}({\\cal H})$ of the same size as $\\pmb{H}$ . The extraction mapping $\\boldsymbol{\\mathrm E}$ is directly set to satisfy the following ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathtt{E}\\cdot\\overline{{h}}_{-1}=\\mathtt{E}\\cdot\\overline{{h}}_{2t-1}=\\overline{{h}}_{2t-1}^{c}\\in\\mathbb{R}^{A B},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "i.e., the part $c$ of the output tokens is used to store the learned policy. ", "page_idx": 19}, {"type": "text", "text": "D.2.2 An Overview of the Proof ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In the following, for the convenience of notations, we will consider step $t+1$ , i.e., with observations $D^{t}\\cup\\{s^{t+1}\\}$ . Given an input token matrix ", "page_idx": 19}, {"type": "equation", "text": "$$\nH=h(D^{t},s^{t+1})=[h_{1},h_{2},\\cdot\\cdot\\cdot,h_{2t+1}],\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "we construct a transformer to perform the following steps ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left[\\begin{array}{c}{h_{2t+1}^{\\mathrm{pre},\\,\\{a,b,c\\}}}\\\\ {h_{2t+1}^{\\mathrm{pre},\\,b}}\\\\ {h_{2t+1}^{\\mathrm{pre},\\,c}}\\\\ {h_{2t+1}^{\\mathrm{pre},\\,d}}\\end{array}\\right]\\xrightarrow{\\mathrm{step~l}}\\left[\\begin{array}{c}{h_{2t+1}^{\\mathrm{pre},\\,\\{a,b,c\\}}}\\\\ {N^{h}(s,a,b,b)}\\\\ {N^{h}(s,a,b,s^{\\prime})}\\\\ {N^{h}(s,a,b)r^{h}(s,a,b)}\\\\ {\\star}\\\\ {\\mathbf{0}}\\\\ {\\mathbf{p}\\mathfrak{s}_{2t+1}}\\end{array}\\right]\\xrightarrow{\\mathrm{step~2}}\\left[\\begin{array}{c}{h_{2t+1}^{\\mathrm{pre},\\{a,b,c\\}}}\\\\ {\\hat{\\mathbb{P}}^{h}(s^{\\prime}|s,a,b)}\\\\ {\\hat{r}^{h}(s,a,b)}\\\\ {\\star}\\\\ {\\mathbf{0}}\\\\ {\\mathbf{pos}_{2t+1}}\\end{array}\\right]\\xrightarrow{\\mathrm{step~3}}\\left[\\begin{array}{c}{h_{2t+1}^{\\mathrm{pre},\\{a,b,c\\}}}\\\\ {\\overline{{Q}}^{h}(s,a,b)}\\\\ {\\underline{{Q}}^{h}(s,a,b)}\\\\ {\\star}\\\\ {\\mathbf{0}}\\\\ {\\mathbf{pos}_{2t+1}}\\end{array}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\mathrm{sep}\\,4}{\\exp\\,4}\\left[\\begin{array}{c}{h_{2t+1}^{\\mathrm{pre},\\{a,b,c\\}}}\\\\ {\\pi^{h}(a,b|s)}\\\\ {\\star}\\\\ {\\bf{0}}\\\\ {{\\bf p o s}_{2t+1}}\\end{array}\\right]\\xrightarrow{\\mathrm{step}\\,5}\\left[\\begin{array}{c}{h_{2t+1}^{\\mathrm{pre},\\{a,b,c\\}}}\\\\ {\\overline{{V}}^{h}(s)}\\\\ {\\underline{{V}}^{h}(s)}\\\\ {\\star}\\\\ {{\\bf{0}}}\\\\ {{\\bf p o s}_{2t+1}}\\end{array}\\right]\\xrightarrow{\\mathrm{step}\\,6}\\left[\\begin{array}{c}{h_{2t+1}^{\\mathrm{pre},\\{a,b\\}}}\\\\ {\\pi^{h+1}(s)^{h+1}}\\\\ {h_{2t+1}^{\\mathrm{post},d}}\\end{array}\\right]:=\\left[\\begin{array}{c}{h_{2t+1}^{\\mathrm{post},a}}\\\\ {h_{2t+1}^{\\mathrm{post},b}}\\\\ {h_{2t+1}^{\\mathrm{post},c}}\\\\ {h_{2t+1}^{\\mathrm{post},d}}\\end{array}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we use $N^{h}(s,a,b)$ \uff0c $N^{h}(s,a,b,s^{\\prime})$ \uff0c $N^{h}(s,a,b)r^{h}(s,a,b),~~\\hat{\\mathbb{P}}^{h}(s^{\\prime}|s,a,b),~~\\hat{r}^{h}(s,a,b),$ $\\overline{{Q}}^{h}(s,a,b)$ \uff0c $Q^{h}(s,a,b)$ and $\\pi^{h}(a,b|s)$ to denote their entirevectors over $h\\,\\in\\,[H],s\\,\\in\\,{\\mathcal{S}},a\\,\\in$ $A,b\\in B,s^{\\prime}\\in S$ The notation $\\star$ denotes other quantities in $h_{2(t+1)}^{d}$ ", "page_idx": 20}, {"type": "text", "text": "The following provides a sketch of the proof. ", "page_idx": 20}, {"type": "text", "text": "Step 1 There exists an attention-only transformer $\\mathrm{TF}_{\\theta}$ to complete Step 1 with ", "page_idx": 20}, {"type": "equation", "text": "$$\nL=O(1),\\;\\;\\operatorname*{max}_{l\\in[L]}M^{(l)}=O(H S^{2}A B),\\;\\;\\|\\pmb\\theta\\|=O(H G+H S^{2}A B).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Step 2 There exists a transformer $\\mathrm{TF}_{\\theta}$ to complete Step 2 with ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{L=O(1),\\,\\,\\,\\operatorname*{max}_{l\\in[L]}M^{(l)}=O(H S^{2}A B),\\,\\,\\,d^{\\prime}=O(G^{2}H S^{2}A B)}\\\\ &{\\|\\pmb\\theta\\|=O(H S^{2}A B+G^{3}+G H).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Step 3 There exists a transformer $\\mathrm{TF}_{\\theta}$ to complete Step 3 with ", "page_idx": 20}, {"type": "equation", "text": "$$\nL=O(H),\\ \\ \\operatorname*{max}_{l\\in[L]}M^{(l)}=O(S A B),\\,\\ d^{\\prime(l)}=O(S A B),\\,\\ \\|\\theta\\|=O(H+S A B).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Step 4 There exists a transformer $\\mathrm{TF}_{\\theta}$ to complete Step 4 with ", "page_idx": 20}, {"type": "equation", "text": "$$\nL=O(G H S),\\;\\operatorname*{max}_{l\\in[L]}M^{(l)}=O(A B),\\;\\;d^{\\prime(l)}=O(A B),\\;\\;\\|\\theta\\|=O(H+A B).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Step 5 There exists an attention-only transformer $\\mathrm{TF}_{\\theta}$ to complete Step 5 with ", "page_idx": 20}, {"type": "equation", "text": "$$\nL=O(1),\\;\\;\\operatorname*{max}_{l\\in[L]}M^{(l)}=O(H S),\\;\\;\\|\\pmb\\theta\\|=O(H S).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Step 6 There exists an attention-only transformer $\\mathrm{TF}_{\\theta}$ to complete Step 6 with ", "page_idx": 20}, {"type": "equation", "text": "$$\nL=O(1),~~\\operatorname*{max}_{l\\in[L]}M^{(l)}=O(H S),~~\\|\\pmb\\theta\\|=O(H S+G H).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus, the overall transformer $\\mathrm{TF}_{\\theta}$ can be summarized as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{L=O(G H S),\\,\\,\\,\\underset{l\\in[L]}{\\operatorname*{max}}\\,M^{(l)}=O(H S^{2}A B),\\,\\,\\,d^{\\prime}=O(G^{2}H S^{2}A B),}\\\\ {\\|\\theta\\|=O(H S^{2}A B+G^{3}+G H).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Also, from the later construction, we can observe that $\\log(R)=\\tilde{\\mathcal{O}}(1)$ ", "page_idx": 20}, {"type": "text", "text": "D.2.3  Proof of Step 1: Update $N^{h}(s,a,b),N^{h}(s,a,b,s^{\\prime})\\;\\mathbf{and}\\;N^{h}(s,a,b)r^{h}(s,a,b)$ ", "page_idx": 20}, {"type": "text", "text": "This can be similarly completed by an attention-only transformer constructed in Step 1 of realizing UCB-VI in Lin et al. [38]. ", "page_idx": 20}, {"type": "text", "text": "D.2.4  Proof of Step 2: Update $\\hat{\\mathbb{P}}(s^{\\prime}|s,a,b)$ and ${\\hat{r}}(s,a,b)$ ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "This can be similarly completed by a transformer constructed in Step 2 of realizing UCB-VI in Lin et al. [38]. ", "page_idx": 20}, {"type": "text", "text": "D.2.5 Proof of Step 3: Compute $\\overline{{Q}}^{h}(s^{\\prime}|s,a,b)$ and $\\underline{o}^{h}(s^{\\prime}|s,a,b)$ ", "page_idx": 20}, {"type": "text", "text": "The computation of $\\overline{{Q}}^{h}(s^{\\prime}|s,a,b)$ can be similarly completed by a transformer constructed in Step 3 of realizing UCB-VI in Lin et al. [38]. The $Q$ part can also be obtained by modifying a few plus signs to minuses. ", "page_idx": 20}, {"type": "text", "text": "D.2.6 Proof of Step 4: Compute CCE ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "This is the most challenging part of realizing the VI-ULCB design, which distinguishes it from the single-agent algorithms, e.g., UCB-VI [6]. As mentioned in Appendix D.1, we obtain an approximate CCE via virtually playing MWU. In the following, for one tuple $(s,h)$ , we prove that one transformer can be constructed to perform a one-step MwU update with that ", "page_idx": 21}, {"type": "equation", "text": "$$\nL=O(1),\\ \\ \\operatorname*{max}_{l\\in[L]}M^{(l)}=O(A B),\\ \\ d^{\\prime}=O(A B),\\ \\ \\|\\theta\\|=O(H+A B).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "To obtain this result, we construct a transformer to perform the following computation from inputs to output for all $t^{\\prime}\\leq t$ ", "page_idx": 21}, {"type": "equation", "text": "$$\nh_{2t}=\\left[\\begin{array}{c}{{\\bf0}}\\end{array}\\right],\\;\\;h_{2t+1}=\\left[\\begin{array}{c}{{\\overline{{L}}^{h}(s,\\cdot,\\cdot)}}\\\\ {{\\underline{{L}}^{h}(s,\\cdot,\\cdot)}}\\\\ {{\\sum_{\\tau<n}o_{+,\\tau}}}\\\\ {{\\sum_{\\tau<n}o_{-,\\tau}}}\\\\ {{\\mu_{n}(\\cdot|s)}}\\\\ {{\\nu_{n}(\\cdot)s}}\\\\ {{\\bf0}}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\xrightarrow[]{\\mathrm{compute}}\\overline{{h}}_{2t}=[\\begin{array}{l}{\\mathbf{0}}\\end{array}],\\ \\overline{{h}}_{2t+1}=\\left[\\begin{array}{l}{\\begin{array}{r}{\\overline{{L}}^{h}(s,\\cdot,\\cdot)}\\\\ {\\underline{{L}}^{h}(s,\\cdot,\\cdot)}\\\\ {\\sum_{\\tau<n+1}^{\\tau}o_{+,\\tau}}\\\\ {\\sum_{\\tau<n+1}^{\\tau}o_{-,\\tau}}\\\\ {\\mu_{n+1}(\\cdot|s)}\\\\ {\\nu_{n+1}(\\cdot|s)}\\\\ {\\sum_{\\tau\\leq n+1}^{\\tau}\\mu_{\\tau}(\\cdot)}\\\\ {\\mathbf{0}}\\end{array}}\\right].\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Note that hre we againuse the notations $\\begin{array}{r}{\\overline{{L}}^{h}(s,\\cdot,\\cdot)=\\frac{H-\\overline{{Q}}^{h}(s,\\cdot,\\cdot)}{H}}\\end{array}$ and $\\begin{array}{r}{\\underline{{L}}^{h}(s,\\cdot,\\cdot)=\\frac{H-\\underline{{Q}}^{h}(s,\\cdot,\\cdot)}{H}}\\end{array}$ to denote the normalized losses. It can be seen that this computation can be performed via one ReLU MLPlayer. ", "page_idx": 21}, {"type": "text", "text": "Step 4.1: Get $^{O}{+},\\!n$ and $O_{-,n}$ ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "First, we can construct that for all $t^{\\prime}\\leq t$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{Q_{a,1}^{(1)}h_{2t^{\\prime}}=\\left[\\begin{array}{c}{v_{2t^{\\prime}}-1}\\\\ {0}\\\\ {t^{\\prime}}\\\\ {H}\\end{array}\\right],}&{Q_{a,1}^{(1)}h_{2t^{\\prime}+1}=\\left[\\begin{array}{c}{v_{2t^{\\prime}+1}-1}\\\\ {v_{n}(\\cdot|s)}\\\\ {t^{\\prime}+1}\\\\ {H}\\end{array}\\right];}\\\\ &{K_{a,1}^{(1)}h_{2t^{\\prime}}=\\left[\\begin{array}{c}{H}\\\\ {0}\\\\ {-H}\\\\ {t^{\\prime}}\\end{array}\\right],}&{K_{a,1}^{(1)}h_{2t^{\\prime}+1}=\\left[\\begin{array}{c}{H}\\\\ {\\overline{{L}}^{h}(a,\\cdot|s)}\\\\ {-H}\\\\ {t^{\\prime}+1}\\end{array}\\right],}&\\\\ &{V_{a,1}^{(1)}h_{2t^{\\prime}}=2t^{\\prime},\\ \\ V_{a,1}^{(1)}h_{2t^{\\prime}+1}=2t^{\\prime}+1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "WithRais $h_{2t^{\\prime}}^{d}=0$ and $h_{2t^{\\prime}+1}^{d}=$ $o_{+,n}(a)$ . With another $A-1$ paralleling heads, the whole vector $^{O}{+},\\!n$ can be computed. Similarly, with $B$ more paralleling heads, the whole vector $O_{-,n}$ can be computed. ", "page_idx": 21}, {"type": "text", "text": "Then, with one ReLU MLP layer, we can obtain that ", "page_idx": 21}, {"type": "equation", "text": "$$\nh_{2t^{\\prime}}^{d}=\\mathbf{0},\\;\\;h_{2t^{\\prime}+1}^{d}=\\left[\\begin{array}{l}{\\sum_{\\tau<n}o_{+,\\tau}}\\\\ {\\sum_{\\tau<n}o_{-,\\tau}}\\end{array}\\right]+W_{2}^{(1)}\\sigma_{\\tau}\\left(W_{1}^{(1)}h_{2t^{\\prime}+1}\\right)=\\left[\\begin{array}{l}{\\sum_{\\tau<n+1}o_{+,\\tau}}\\\\ {\\sum_{\\tau<n+1}o_{-,\\tau}}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The required transformer can be summarized as ", "page_idx": 21}, {"type": "equation", "text": "$$\nL=1,\\;\\;M^{(1)}=O(A+B),\\;\\;d^{\\prime}=O(A+B),\\;\\;\\|\\theta\\|=O(H).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Step 4.2: Get $\\mu_{n+1}(\\cdot|s)$ and $\\nu_{n+1}(\\cdot|s)$ ", "page_idx": 22}, {"type": "text", "text": "We can construct a softmax MLP layer such that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{W_{1}^{(1)}h_{2t^{\\prime}}={\\bf0}_{A},\\;\\;\\sigma_{s}(W_{1}^{(1)}h_{2t^{\\prime}})=\\displaystyle\\frac{1}{A}\\cdot{\\bf1}_{A}}}\\\\ {{W_{1}^{(1)}h_{2t^{\\prime}+1}=\\left[\\begin{array}{l}{{-\\eta_{A}\\sum_{\\tau<n+1}\\sigma_{+,n}^{t^{\\prime}}}}\\end{array}\\right],\\;\\;\\sigma_{s}(W_{1}^{(1)}h_{2t^{\\prime}+1})=\\left[\\begin{array}{l}{{\\mu_{n+1}(\\cdot|s)}}\\end{array}\\right],}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Wwhere $\\eta_{A}~=~\\sqrt{\\log(A)/G}$ .Thus, $\\mu_{n+1}(\\cdot|s)$ can be provided. Similarly, another MLP layer $\\{W_{1}^{(2)},W_{2}^{(2)}\\}$ Wwithsftmax activation can provide $\\nu_{n+1}(\\cdot|s)$ . The curren output can be exprsed as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\boldsymbol{h}_{2t^{\\prime}}^{d}=\\left[\\begin{array}{c}{\\mathbf{0}}\\\\ {\\frac{1}{A}\\cdot\\mathbf{1}_{A}}\\\\ {\\frac{1}{B}\\cdot\\mathbf{1}_{B}}\\end{array}\\right],\\;\\boldsymbol{h}_{2t^{\\prime}+1}^{d}=\\left[\\begin{array}{c}{\\mu_{n}(\\cdot|s)}\\\\ {\\nu_{n}(\\cdot|s)}\\\\ {\\mu_{n+1}(\\cdot|s)}\\\\ {\\nu_{n+1}(\\cdot|s)}\\end{array}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We can further construct one more attention layer as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{{\\cal{Q}}_{1}^{(4)}h_{2t^{\\prime}}=\\left[\\begin{array}{c}{{1-v_{2t^{\\prime}}}}\\\\ {{t^{\\prime}}}\\\\ {{1}}\\end{array}\\right],\\;\\;{\\cal{Q}}_{1}^{(3)}h_{2t^{\\prime}+1}=\\left[\\begin{array}{c}{{1-v_{2t^{\\prime}+1}}}\\\\ {{t^{\\prime}+1}}\\\\ {{1}}\\end{array}\\right];}}\\\\ {{{\\cal{K}}_{1}^{(3)}h_{2t^{\\prime}}=\\left[\\begin{array}{c}{{1}}\\\\ {{-1}}\\\\ {{t^{\\prime}}}\\end{array}\\right],\\;\\;{\\cal{K}}_{1}^{(3)}h_{2t^{\\prime}+1}=\\left[\\begin{array}{c}{{1}}\\\\ {{-1}}\\\\ {{t^{\\prime}+1}}\\end{array}\\right];}}\\\\ {{{\\cal{V}}_{1}^{(3)}h_{2t^{\\prime}}=2t^{\\prime}\\cdot\\left[\\begin{array}{c}{{-\\frac{1}{4}\\cdot\\mathbf{1}_{A}}}\\\\ {{-\\frac{1}{B}\\cdot\\mathbf{1}_{B}}}\\end{array}\\right],\\;\\;{\\cal{V}}_{1}^{(3)}h_{2t^{\\prime}+1}=\\left(2t^{\\prime}+1\\right)\\cdot\\left[\\begin{array}{c}{{-\\frac{1}{A}\\cdot\\mathbf{1}_{A}}}\\\\ {{-\\frac{1}{B}\\cdot\\mathbf{1}_{B}}}\\end{array}\\right].}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "With ReLU activation, this construction would result in that ", "page_idx": 22}, {"type": "equation", "text": "$$\nh_{2{t}^{\\prime}}={\\left[\\begin{array}{l}{{\\frac{1}{A}}\\cdot\\mathbf{1}_{A}}\\\\ {{\\frac{1}{B}}\\cdot\\mathbf{1}_{B}}\\end{array}\\right]}+{\\left[\\begin{array}{l}{-{\\frac{1}{A}}\\cdot\\mathbf{1}_{A}}\\\\ {-{\\frac{1}{B}}\\cdot\\mathbf{1}_{B}}\\end{array}\\right]}=\\mathbf{0},\\,\\,\\,h_{2{t}^{\\prime}+1}={\\left[\\begin{array}{l}{\\mu_{n+1}(\\cdot|s)}\\\\ {\\nu_{n+1}(\\cdot|s)}\\end{array}\\right]}+\\mathbf{0}={\\left[\\begin{array}{l}{\\mu_{n+1}(\\cdot|s)}\\\\ {\\nu_{n+1}(\\cdot|s)}\\end{array}\\right]}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "At last, one ReLU MLPlayer $\\{W_{1}^{(3)},W_{2}^{(3)}\\}$ can be constructed to replace $\\mu_{n}(\\cdot|s)$ and $\\nu_{n}(\\cdot|s)$ with $\\mu_{n+1}(\\cdot|s)$ and $\\nu_{n+1}(\\cdot|s)$ ", "page_idx": 22}, {"type": "equation", "text": "$$\nW_{2}^{(3)}\\sigma_{\\mathrm{r}}\\left(W_{1}^{(3)}h_{2t^{\\prime}}\\right)=\\mathbf{0},\\;\\,W_{2}^{(3)}\\sigma_{\\mathrm{r}}\\left(W_{1}^{(3)}h_{2t^{\\prime}+1}\\right)=\\left[\\begin{array}{l}{\\mu_{n+1}(\\cdot|s)-\\mu_{n}(\\cdot|s)}\\\\ {\\nu_{n+1}(\\cdot|s)-\\nu_{n}(\\cdot|s)}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The required transformer can be summarized as ", "page_idx": 22}, {"type": "equation", "text": "$$\nL=3,\\;\\;\\operatorname*{max}_{l\\in[L]}M^{(l)}=O(1),\\;\\;d^{\\prime}=O(A+B),\\;\\;\\|\\theta\\|=O(\\sqrt{\\log(A)/G}+\\sqrt{\\log(B)/G}+1).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Step 4.3: Get $\\begin{array}{r}{\\sum_{\\tau\\leq n+1}\\mu_{\\tau}(\\cdot|s)\\nu_{\\tau}(\\cdot|s)/N.}\\end{array}$ ", "page_idx": 22}, {"type": "text", "text": "We can construct ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{{\\pmb Q}_{1}^{(1)}{h}_{2t^{\\prime}}=\\left[\\begin{array}{c}{0}\\\\ {t^{\\prime}}\\\\ {1}\\end{array}\\right],\\;\\;{\\pmb Q}_{1}^{(1)}{h}_{2t^{\\prime}+1}=\\left[\\begin{array}{c}{\\mu_{n+1}(a|s)}\\\\ {t^{\\prime}+1}\\\\ {1}\\end{array}\\right];}\\\\ &{}&{{\\pmb K}_{1}^{(1)}{h}_{2t^{\\prime}}=\\left[\\begin{array}{c}{0}\\\\ {-1}\\\\ {t^{\\prime}}\\end{array}\\right],\\;\\;{\\pmb K}_{1}^{(1)}{h}_{2t^{\\prime}+1}=\\left[\\begin{array}{c}{\\nu_{n+1}(b|s)}\\\\ {-1}\\\\ {t^{\\prime}+1}\\end{array}\\right];}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\nV_{1}^{(5)}h_{2t^{\\prime}}=2t^{\\prime},\\ V_{1}^{(5)}h_{2t^{\\prime}+1}=2t^{\\prime}+1.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "With ReLU activation, this construction can update that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h_{2t^{\\prime}}=0,\\ h_{2t^{\\prime}+1}=\\mu_{n+1}(a|s)\\nu_{n+1}(b|s).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Using an overall $A B$ paralleling heads, we can then obtain $\\mu_{n+1}(\\cdot|s)\\nu_{n+1}(\\cdot|s)$ . Then, with a ReLU MLPlayer $\\{W_{1}^{(5)},W_{2}^{(5)}\\}$ ,we canobtain $\\begin{array}{r}{\\sum_{\\tau\\leq n+1}\\mu_{\\tau}(\\cdot|s)\\nu_{\\tau}(\\cdot|s)/N}\\end{array}$ ", "page_idx": 22}, {"type": "text", "text": "The required transformer can be summarized as ", "page_idx": 22}, {"type": "equation", "text": "$$\nL=1,\\;\\;M^{(1)}=O(A B),\\;\\;d^{\\prime}=O(A B),\\;\\;\\|\\pmb\\theta\\|=O(A B).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Combining all the sub-steps provides proof of a one-step MWU update. The same transformer can be stackedfor $G$ times, which completes the $G$ -stepMwU. ", "page_idx": 22}, {"type": "text", "text": "D.2.7 Proof of Step 5: Compute $\\overline{{V}}^{h}(s)$ and $\\underline{{V}}^{h}(s)$ ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We can construct that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{Q_{1}^{(1)}h_{2t^{\\prime}}=\\left[\\begin{array}{c}{{0}}\\\\ {{t^{\\prime}}}\\\\ {{H}}\\end{array}\\right],\\;\\;Q_{1}^{(1)}h_{2t^{\\prime}+1}=\\left[\\begin{array}{c}{{\\pi^{h}(.,.|s)}}\\\\ {{t^{\\prime}+1}}\\\\ {{H}}\\end{array}\\right];}}\\\\ {{K_{1}^{(1)}h_{2t^{\\prime}}=\\left[\\begin{array}{c}{{0}}\\\\ {{-H}}\\\\ {{t^{\\prime}}}\\end{array}\\right],\\;\\;K_{1}^{(1)}h_{2t^{\\prime}+1}=\\left[\\begin{array}{c}{{\\overline{{{Q}}}^{h}(s,.,.)}}\\\\ {{-H}}\\\\ {{t^{\\prime}+1}}\\end{array}\\right];}}\\\\ {{V_{1}^{(1)}h_{2t^{\\prime}}=2t^{\\prime},\\;\\;V_{1}^{(1)}h_{2t^{\\prime}+1}=2t^{\\prime}+1.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "With ReLU activation, this construction leads to that ", "page_idx": 23}, {"type": "equation", "text": "$$\nh_{2t^{\\prime}}=0,\\ h_{2t^{\\prime}+1}=\\pi^{h}(\\cdot,\\cdot|s)\\cdot\\overline{{Q}}^{h}(s,\\cdot,\\cdot)=\\overline{{V}}^{h}(s).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Thus, with overall $2H S$ paralling heads, the values of $\\{\\overline{{V}}^{h}(s),\\underline{{V}}^{h}(s):h\\in[H],s\\in\\mathcal{S}\\}$ can be computed. ", "page_idx": 23}, {"type": "text", "text": "The required transformer can be summarized as ", "page_idx": 23}, {"type": "equation", "text": "$$\nL=1,\\;\\;M^{(1)}=O(H S),\\;\\;\\|\\pmb\\theta\\|=O(H S).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "D.2.8 Proof of Step 6: Obtain $\\pi^{h+1}(\\cdot,\\cdot|s^{h+1})$ ", "page_idx": 23}, {"type": "text", "text": "We can construct one $H S$ -head transformer that for all $(s,h)\\in{\\mathcal{S}}\\times[H]$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Q_{h,s}^{(1)}h_{2\\ell^{\\prime}}=\\left[\\begin{array}{c}{0}\\\\ {e_{h^{\\prime}}}\\\\ {t^{\\prime}}\\\\ {1}\\\\ {1}\\end{array}\\right],\\ Q_{h,s}^{(1)}h_{2\\ell^{\\prime}+1}=\\left[\\begin{array}{c}{s^{\\prime}+1}\\\\ {e_{h^{\\prime}+1}}\\\\ {t^{\\prime}+1}\\\\ {1}\\end{array}\\right];}\\\\ {K_{h,s}^{(1)}h_{2\\ell^{\\prime}}=\\left[\\begin{array}{c}{e_{s}}\\\\ {e_{h}}\\\\ {-1}\\\\ {t^{\\prime}}\\\\ {-1}\\end{array}\\right],\\ K_{h,s}^{(1)}h_{2\\ell^{\\prime}+1}=\\left[\\begin{array}{c}{e_{s}}\\\\ {e_{h}}\\\\ {t^{\\prime}+1}\\\\ {-1}\\end{array}\\right];}\\\\ {V_{h,s}^{(1)}h_{2\\ell^{\\prime}}=0,\\ V_{h,s}^{(1)}h_{2\\ell^{\\prime}+1}=\\pi^{h}(.,.,|s).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "With ReLU activation, this construction leads to the update that ", "page_idx": 23}, {"type": "equation", "text": "$$\nh_{2t^{\\prime}}=\\mathbf{0},\\ h_{2t^{\\prime}+1}=\\frac{1}{2t^{\\prime}+1}\\cdot\\pi^{h^{\\prime}+1}(\\cdot,\\cdot|s^{t^{\\prime}+1}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then, we can construct that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{{\\cal{Q}}_{1}^{(1)}h_{2t^{\\prime}}=\\left[\\begin{array}{c}{{2t^{\\prime}}}\\\\ {{2G H t^{\\prime}}}\\\\ {{1}}\\end{array}\\right],\\;\\;{\\cal{Q}}_{1}^{(1)}h_{2t^{\\prime}+1}=\\left[\\begin{array}{c}{{2t^{\\prime}+1}}\\\\ {{2G H(t^{\\prime}+1)}}\\\\ {{1}}\\end{array}\\right];}}\\\\ {{{\\cal{K}}_{1}^{(1)}h_{2t^{\\prime}}=\\left[\\begin{array}{c}{{1}}\\\\ {{-1}}\\\\ {{2G H t^{\\prime}}}\\end{array}\\right],\\;\\;{\\cal{K}}_{1}^{(1)}h_{2t^{\\prime}+1}=\\left[\\begin{array}{c}{{1}}\\\\ {{-1}}\\\\ {{2G H(t^{\\prime}+1)}}\\end{array}\\right];}}\\\\ {{{\\cal{V}}_{1}^{(1)}h_{2t^{\\prime}}={\\bf{0}},\\;\\;{\\cal{V}}_{h,s}^{(1)}h_{2t^{\\prime}+1}=\\frac{1}{2t^{\\prime}+1}\\cdot\\pi^{h^{\\prime}+1}(\\cdot,\\cdot|s^{t^{\\prime}+1}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "With ReLU activation, this construction leads to the update that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h_{2t^{\\prime}}=\\mathbf{0},\\;\\;h_{2t^{\\prime}+1}=\\pi^{h^{\\prime}+1}(\\cdot,\\cdot|s^{t^{\\prime}+1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The required transformer can be summarized as ", "page_idx": 23}, {"type": "equation", "text": "$$\nL=2,\\ \\operatorname*{max}_{l\\in[L]}M^{(l)}=O(H S),\\ \\ \\|\\pmb\\theta\\|=O(H S+G H).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "E Proofs for the Centralized Overall Performance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Proof of Theorem 4.2. First, we can obtain the decomposition that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{M\\sim\\Lambda,D\\sim\\mathbb{P}_{M}^{\\mathbf{A}_{R}}}\\left[\\displaystyle\\sum_{g\\in[G]}V_{M}^{\\dagger,\\rho}(s^{1})-V_{M}^{\\mu,\\dagger}(s^{1})\\right]}\\\\ &{=\\mathbb{E}_{M\\sim\\Lambda,D\\sim\\mathbb{P}_{M}^{\\mathbf{A}_{R}}}\\left[\\displaystyle\\sum_{g\\in[G]}V_{M}^{\\dagger,\\rho^{\\prime}}(s^{1})-V_{M}^{\\mu,\\dagger}(s^{1})\\right]}\\\\ &{+\\mathbb{E}_{M\\sim\\Lambda,D\\sim\\mathbb{P}_{M}^{\\mathbf{A}_{R}}}\\left[\\displaystyle\\sum_{g\\in[G]}V_{M}^{\\dagger,\\mu^{\\prime}}(s^{1})\\right]-\\mathbb{E}_{M\\sim\\Lambda,D\\sim\\mathbb{P}_{M}^{\\mathbf{A}_{R}}}\\left[\\displaystyle\\sum_{g\\in[G]}V_{M}^{\\dagger,\\mu^{\\prime}}(s^{1})\\right]}\\\\ &{+\\mathbb{E}_{M\\sim\\Lambda,D\\sim\\mathbb{P}_{M}^{\\mathbf{A}_{R}}}\\left[\\displaystyle\\sum_{g\\in[G]}V_{M}^{\\mu,\\dagger}(s^{1})\\right]-\\mathbb{E}_{M\\sim\\Lambda,D\\sim\\mathbb{P}_{M}^{\\mathbf{A}_{R}}}\\left[\\displaystyle\\sum_{g\\in[G]}V_{M}^{\\mu,\\dagger}(s^{1})\\right]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Via Theorem D.1, it holds that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}_{M\\sim\\Lambda,D\\sim\\mathbb{P}_{M}^{\\mathrm{Ag}_{0}}}\\left[\\sum_{g\\in[G]}V_{M}^{\\dagger,\\nu^{g}}(s^{1})-V_{M}^{\\mu^{g},\\dagger}(s^{1})\\right]=O\\left(\\sqrt{H^{3}S^{2}A B T\\log(S A B T)}\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then, via Lemma E.1 and Theorem C.3, we can obtain that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{M\\sim\\Lambda,D\\sim\\mathbb{P}_{M}^{{\\mathbb{A}}_{R}}}\\left[\\displaystyle\\sum_{g\\in[G]}{V_{M}^{\\mu^{g},\\dagger}(s^{1})}\\right]-\\mathbb{E}_{M\\sim\\Lambda,D\\sim\\mathbb{P}_{M}^{{\\mathbb{A}}_{R}}}\\left[\\sum_{g\\in[G]}{V_{M}^{\\mu^{g},\\dagger}(s^{1})}\\right]}\\\\ &{\\ =\\cal{O}\\left(T\\cdot\\mathbb{E}_{M\\sim\\Lambda,D\\sim\\mathbb{P}_{M}^{{\\mathbb{A}}_{R}}}\\left[\\displaystyle\\sum_{t\\in[T]}\\sum_{s\\in S}\\left[\\mathrm{TV}\\left(\\mathbb{A}1\\mathbb{g}_{0},\\mathbb{A}1\\mathbb{g}_{\\widehat{\\theta}}|D^{t-1},s\\right)\\right]\\right]\\right)}\\\\ &{\\ =\\cal{O}\\left(T^{2}S\\sqrt{\\varepsilon_{\\mathrm{real}}}+T^{2}S\\sqrt{\\frac{\\log{(N\\mathrm{e}T S/\\widehat{\\theta})}}{N}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and similarly, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{M\\sim\\Lambda,D\\sim\\mathbb{P}_{M}^{\\mathrm{Ag}}\\theta}\\left[\\displaystyle\\sum_{g\\in[G]}V_{M}^{\\dagger,\\nu^{g}}(s^{1})\\right]-\\mathbb{E}_{M\\sim\\Lambda,D\\sim\\mathbb{P}_{M}^{\\mathrm{Ag}_{0}}}\\left[\\sum_{g\\in[G]}V_{M}^{\\dagger,\\nu^{g}}(s^{1})\\right]}\\\\ &{=O\\left(T^{2}S\\sqrt{\\varepsilon_{\\mathrm{real}}}+T^{2}S\\sqrt{\\frac{\\log{(\\sqrt{N_{\\Theta}T S/\\delta})}}{N}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "With Theorem 4.1 providing that $\\varepsilon_{\\mathrm{real}}=0$ , combining the above terms completes the proof of the regret bound. The bound on the covering number, i.e., $\\mathrm{\\bar{log}}(N_{\\Theta})$ can be obtained via Lemma I.4. ", "page_idx": 24}, {"type": "text", "text": "Lemma E.1. For any two centralized algorithms $\\mathtt{A l g}_{\\alpha}$ and ${\\tt A l g}_{\\beta}$ , we denote their performed policies for episode $g$ are $(\\pi_{\\alpha}^{g},\\pi_{\\beta}^{g})$ , whose marginal policies are $(\\mu_{\\alpha}^{g},\\nu_{\\alpha}^{g})$ and $(\\mu_{\\beta}^{g},\\nu_{\\beta}^{g})$ .For $\\{\\mu_{\\alpha}^{g},\\nu_{\\beta}^{g}\\}$ it holdsthat ", "page_idx": 24}, {"type": "text", "text": "$\\natural_{\\alpha}\\left[\\sum_{g\\in[G]}V_{M}^{\\mu_{\\alpha}^{g},\\dagger}(s^{1})\\right]-\\mathbb{E}_{\\beta}\\left[\\sum_{g\\in[G]}V_{M}^{\\mu_{\\beta}^{g},\\dagger}(s^{1})\\right]\\lesssim T\\cdot\\mathbb{E}_{\\alpha}\\left[\\sum_{t\\in[T],s\\in S}\\mathrm{TV}\\left(\\pi_{\\alpha}^{t},\\pi_{\\beta}^{t}|D^{t-1},s\\right)\\right],$ where Ea[] and E[] are with respecto Pa and $\\mathbb{P}_{\\Lambda}^{\\mathtt{A l g}_{\\beta}}$ $A$ simila result holds or $\\bigl\\{\\nu_{\\alpha}^{g},\\nu_{\\beta}^{g}\\bigr\\}$ \uff0c ", "page_idx": 24}, {"type": "text", "text": "Proof of Lemma E.1. It holds that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}_{D\\sim\\mathbb{P}_{M}^{{\\texttt A}{\\texttt B}_{\\alpha}}}\\left[\\sum_{g\\in[G]}V_{M}^{\\mu_{\\alpha}^{g},\\dagger}(s^{1})\\right]-\\mathbb{E}_{D\\sim\\mathbb{P}_{M}^{{\\texttt A}{\\texttt B}_{\\beta}}}\\left[\\sum_{g\\in[G]}V_{M}^{\\mu_{\\beta}^{g},\\dagger}(s^{1})\\right]\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\underbrace{\\mathbb{E}_{D\\sim\\mathbb{P}_{M}^{{\\texttt A}\\mathtt{L}_{\\alpha}}}\\left[\\displaystyle\\sum_{g\\in[G]}V_{M}^{\\mu_{g}^{g},\\dagger}(s^{1})-\\displaystyle\\sum_{g\\in[G]}V_{M}^{\\mu_{g}^{g},\\dagger}(s^{1})\\right]}_{:=(\\mathrm{term~I})}}\\\\ &{+\\underbrace{\\mathbb{E}_{D\\sim\\mathbb{P}_{M}^{{\\texttt A}\\mathtt{L}_{\\alpha}}}\\left[\\displaystyle\\sum_{g\\in[G]}V_{M}^{\\mu_{g}^{g},\\dagger}(s^{1})\\right]-\\mathbb{E}_{D\\sim\\mathbb{P}_{M}^{{\\texttt A}\\mathtt{L}_{\\beta}}}\\left[\\displaystyle\\sum_{g\\in[G]}V_{M}^{\\mu_{g}^{g},\\dagger}(s^{1})\\right]}_{:=(\\mathrm{term~I})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Denoting $D^{H}:=D^{(g-1)H+1:g H}$ and $\\begin{array}{r}{f(D^{H})=\\sum_{h\\in[H]}r^{g,h}}\\end{array}$ we ca furthe obtan that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{[V_{k_{1}^{n}}^{n_{1}^{n}-1}(x^{1})-V_{k_{2}^{n}}^{n_{1}^{n}-1}(x^{1})}\\\\ &{\\overset{(a)}{\\leq}V_{k_{1}^{n}}^{n_{1}^{n}-1}(\\nu_{3}^{n_{1}})(x^{1})-V_{k_{1}^{n}}^{n_{1}^{n}-1}(\\nu_{3}^{n_{1}})(x^{1})}\\\\ &{=\\mathbb{E}_{U\\sim\\nu_{n}^{n_{1}^{n}-1}}^{n_{1}}\\alpha_{1}^{-n_{1}^{n}}\\Big\\{\\int(D^{1})^{n}]-\\mathbb{E}_{U\\sim\\nu_{n}^{n_{1}^{n}-1}}^{n_{1}^{n}-1}\\tilde{F}_{k_{1}^{n}}^{n}(\\nu^{n})\\Big\\}\\int(D^{1})}\\\\ &{=\\underset{k_{1}\\leq i}{\\sum}V_{k_{1}^{n}}^{n_{1}}\\int\\underset{0\\leq i\\leq n_{1}^{n}\\leq i\\leq n_{1}^{n}}{\\sum}f_{k_{1}^{n+1}}(x^{1})\\beta_{1}^{-n_{1}^{n}-1}(\\nu_{3}^{n_{1}^{n}-1})}\\\\ &{-\\underset{k_{1}\\leq i\\leq n_{1}^{n}\\leq i\\leq n_{1}^{n}}{\\sum}\\underset{0\\leq i\\leq n_{1}^{n}\\leq i\\leq n_{1}^{n}}{\\sum}\\underset{0\\leq i\\leq n_{1}^{n}\\leq i\\leq n_{1}^{n}}{\\sum}\\Big[\\int(D^{1})\\Big]}\\\\ &{\\underset{k_{1}\\leq i\\leq i\\leq n_{1}^{n}}{\\sum}\\underset{0\\leq i\\leq n_{1}^{n}\\leq i\\leq n_{1}^{n}\\leq i\\leq n_{1}^{n}}{\\sum}\\underset{0\\leq i\\leq n_{1}^{n}\\leq i\\leq n_{1}^{n}}{\\sum}\\Big[\\int\\Big(D^{k_{1}^{n}})^{n}\\Big]}\\\\ &{\\overset{(b)}{\\leq}2H\\underset{k\\leq i\\leq n_{1}^{n}}{\\sum}\\underset{0\\leq i\\leq n_{1}^{n}\\leq i\\leq n_{1}^{n}\\leq i\\leq n_{ \n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where (a) is from the definition of best responses, (b) is from the variational representation of the TV distance, (c) is from the fact that $\\operatorname{TV}(\\mu\\times\\nu,\\mu^{\\prime}\\times\\nu)=\\operatorname{TV}(\\mu,\\mu^{\\prime})$ , and (d) is from the fact that $\\begin{array}{r}{\\mathrm{TV}(\\sum_{b}\\pi(\\cdot,b),\\sum_{b}\\pi^{\\prime}(\\cdot,b))\\le\\mathrm{TV}(\\pi(\\cdot,\\cdot),\\pi^{\\prime}(\\cdot,\\cdot))}\\end{array}$ . The above relationship further leads to that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\displaystyle(\\mathrm{term~I}):=\\mathbb{E}_{D\\sim\\mathbb{P}_{M}^{\\mathrm{Ag}_{\\alpha}}}\\left[\\sum_{g\\in[G]}V_{M}^{\\mu_{\\alpha}^{g},\\dagger}(s^{1})-\\sum_{g\\in[G]}V_{M}^{\\mu_{\\beta}^{g},\\dagger}(s^{1})\\right]}\\\\ &{}&{\\displaystyle\\leq2H\\cdot\\mathbb{E}_{D\\sim\\mathbb{P}_{M}^{\\mathrm{Ag}_{\\alpha}}}\\left[\\sum_{t\\in[T]}\\sum_{s\\in\\mathcal{S}}\\mathrm{TV}\\left(\\pi_{\\alpha}^{t},\\pi_{\\beta}^{t}|D^{t-1},s\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Also, denoting $\\begin{array}{r}{g(D)=\\sum_{g\\in[G]}V^{\\mu_{\\beta}^{g},\\dagger}(s^{1})}\\end{array}$ it holds that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\mathrm{term~II}):=\\mathbb{E}_{D\\sim\\mathbb{F}_{M}^{\\mathrm{AE}_{\\alpha}}}\\left[\\displaystyle\\sum_{g\\in[G]}V^{\\mu_{g}^{a},\\dagger}(s^{1})\\right]-\\mathbb{E}_{D\\sim\\mathbb{F}_{M}^{\\mathrm{AE}_{\\beta}}}\\left[\\sum_{g\\in[G]}V^{\\mu_{g}^{a},\\dagger}(s^{1})\\right]}\\\\ &{\\quad\\quad\\quad=\\displaystyle\\sum_{t\\in[T]}\\mathbb{E}_{D^{t}\\sim\\mathbb{F}_{M}^{\\mathrm{AE}_{\\alpha}},D^{t+1:T}\\sim\\mathbb{F}_{M}^{\\mathrm{AE}_{\\beta}}}\\left[g(D)\\right]-\\displaystyle\\sum_{t\\in[T]}\\mathbb{E}_{D^{t-1}\\sim\\mathbb{F}_{M}^{\\mathrm{AE}_{\\alpha}},D^{t+T}\\sim\\mathbb{F}_{M}^{\\mathrm{AE}_{\\beta}}}\\left[g(D)\\right]}\\\\ &{\\quad\\quad\\quad\\leq2T\\displaystyle\\sum_{t\\in[T]}\\mathbb{E}_{D^{t-1}\\sim\\mathbb{F}_{M}^{\\mathrm{AE}_{\\alpha}},s^{t}}\\left[\\mathrm{TV}\\left(\\mathbb{A}1\\mathbb{g}_{\\alpha},\\mathbb{A}1\\mathbb{g}_{\\beta}|D^{t-1},s^{t}\\right)\\right]}\\\\ &{\\quad\\quad\\quad\\leq2T\\cdot\\mathbb{E}_{D\\sim\\mathbb{F}_{M}^{\\mathrm{AE}_{\\alpha}}}\\left[\\displaystyle\\sum_{t\\in[T]}\\sum_{s\\in S}\\mathrm{TV}\\left(\\pi_{\\alpha}^{t},\\pi_{\\beta}^{t}|D^{t-1},s\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Combining (term I) and (term II) finishes the proof. ", "page_idx": 25}, {"type": "text", "text": "F  Proofs for the Decentralized Supervised Pre-training Guarantees ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Definition F.1 (The Complete Version of Definition 3.1). For a class of algorithms $\\{\\mathsf{A}\\mathsf{1g}_{\\theta_{+}}:\\theta_{+}\\in$ $\\Theta_{+}\\}$ we say $\\tilde{\\Theta}_{+}\\subseteq\\Theta_{+}$ is a $\\rho_{+}$ -cover of $\\Theta_{+}$ $i f\\,\\tilde{\\Theta}_{+}$ is a finite set such that for any $\\pmb{\\theta}_{+}\\in\\Theta_{+}$ , there exists $\\tilde{\\pmb{\\theta}}_{+}\\in\\tilde{\\Theta}_{+}$ such that for all $D_{+}^{t-1},s\\in S,t\\in[T],$ it holds that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left\\|\\log\\mathbb{A}\\mathbb{L}_{\\tilde{\\theta}_{+}}(\\cdot,\\cdot|D_{+}^{t-1},s)-\\log\\mathbb{A}\\mathbb{1}\\mathbf{g}_{\\theta_{+}}(\\cdot|D_{+}^{t-1},s)\\right\\|_{\\infty}\\leq\\rho_{+}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The covering number $\\mathcal{N}_{\\Theta_{+}}(\\rho_{+})$ is the minimal cardinality of $\\Tilde{\\Theta}_{+}$ such that $\\Tilde{\\Theta}_{+}$ is a $\\rho_{+}$ -cover of $\\Theta_{+}$ ", "page_idx": 26}, {"type": "text", "text": "Similarly,for a class of algorithms $\\left\\{{\\sf A l g}_{\\theta_{-}}\\,:\\,\\pmb{\\theta}_{-}\\,\\in\\,\\Theta_{-}\\right\\}$ we say $\\tilde{\\Theta}_{-}\\ \\subseteq\\ \\Theta_{-}$ is a $\\rho_{-}$ -cover of $\\Theta_{-},~i f\\,\\tilde{\\Theta}_{-}$ is afiniteset such that for any $\\theta_{-}\\ \\in\\ \\Theta_{-}$ , there exists $\\tilde{\\pmb{\\theta}}_{-}\\ \\in\\ \\tilde{\\Theta}_{-}$ such that for all $D_{-}^{t-1},s\\in S,t\\in[T],$ it holds that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left\\|\\log\\tt{A}\\mathbb{1}g_{\\tilde{\\theta}_{-}}(\\cdot,\\cdot|D_{-}^{t-1},s)-\\log\\tt{A}\\mathbb{1}g_{\\theta_{-}}(\\cdot|D_{-}^{t-1},s)\\right\\|_{\\infty}\\leq\\rho_{-}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The covering number $\\mathcal{N}_{\\Theta_{-}}(\\rho_{-})$ is the minimal cardinality of $\\Tilde{\\Theta}_{-}$ such that $\\Tilde{\\Theta}_{-}$ is a $\\rho_{-}$ -cover of $\\Theta_{-}$ Assumption F.2 (The Complete Version of Assumption 3.2). There exists ${\\pmb\\theta}_{+}^{*}\\in\\Theta_{+}$ suchthatthere exists $\\varepsilon_{+,\\mathrm{real}}>0$ for all $t\\in[T],s\\in S$ \uff0c $a\\in{\\mathcal{A}}$ itholdsthat ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\log\\left(\\mathbb{E}_{M\\sim\\Lambda,D\\sim\\mathbb{P}_{M}^{\\mathrm{Alg}_{0}}}\\left[\\frac{\\mathtt{A l g}_{+,0}(a|D_{+}^{t-1},s)}{\\mathtt{A l g}_{\\theta_{+}^{*}}(a|D_{+}^{t-1},s)}\\right]\\right)\\leq\\varepsilon_{+,\\mathrm{real}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Similarly, there exists $\\pmb{\\theta}_{-}^{*}\\in\\Theta_{-}$ such that there exists $\\varepsilon_{-,\\mathrm{real}}>0,$ for all $t\\in[T],s\\in{\\mathcal{S}},\\,b\\in{\\mathcal{B}},$ it holds that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\log\\left(\\mathbb{E}_{M\\sim\\Lambda,D\\sim\\mathbb{P}_{M}^{\\mathrm{Alg}_{0}}}\\left[\\frac{\\mathtt{A l g}_{-,0}(b|D_{-}^{t-1},s)}{\\mathtt{A l g}_{\\theta_{-}^{*}}(b|D_{-}^{t-1},s)}\\right]\\right)\\leq\\varepsilon_{-,\\mathrm{real}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Theorem F.3 (The Complete Version of Theorem 3.3). Let $\\widehat{\\pmb{\\theta}}_{+}$ be the max-player's pre-training output defined in Section 3.1.1. Take $\\mathcal{N}_{\\Theta_{+}}=\\mathcal{N}_{\\Theta_{+}}(1/N)$ as in Definition $F.l$ Then, under Assumption $F.2$ with probability at least $1-\\delta$ it holds that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{M\\sim\\Lambda,D\\sim\\mathbb{P}_{M}^{\\mathrm{Alg}_{0}}}\\left[\\sum_{t\\in[T],s\\in\\mathcal{S}}\\mathrm{TV}\\left(\\mathbb{A}\\mathrm{L}_{\\mathbb{Q}_{+,0}},\\mathbb{A}\\mathrm{L}_{\\mathbb{Q}_{+}}|D_{+}^{t-1},s\\right)\\right]}\\\\ {=O\\left(T S\\sqrt{\\varepsilon_{+,\\mathrm{real}}}+T S\\sqrt{\\frac{\\log\\left(N_{\\Theta_{+}}T S/\\delta\\right)}{N}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Let $\\widehat{\\pmb{\\theta}}_{-}$ be themin-player's pre-training output defined inSection 3.1.1.Take $\\smash{\\ensuremath{\\mathcal{N}_{\\Theta_{-}}}=\\ensuremath{\\mathcal{N}_{\\Theta_{-}}}(1/N)}$ as in Definition $F.I$ Then, under Assumption $F.2$ with probability at least $1-\\delta$ , it holds that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{M\\sim\\Lambda,D\\sim\\mathbb{P}_{M}^{\\mathbb{A}\\mathbb{B}_{0}}}\\left[\\sum_{t\\in[T],s\\in\\mathcal{S}}\\mathrm{TV}\\left(\\mathbb{A}\\mathrm{L}_{\\mathbb{g}_{-,0}},\\mathbb{A}\\mathrm{L}_{\\mathcal{G}_{-}}|D_{+}^{t-1},s\\right)\\right]}\\\\ {=O\\left(T S\\sqrt{\\varepsilon_{-,\\mathrm{real}}}+T S\\sqrt{\\frac{\\log\\left(N_{\\Theta_{-}}T S/\\delta\\right)}{N}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof of Theorem F.3. This theorem can be similarly proved as Theorem 3.3. ", "page_idx": 26}, {"type": "text", "text": "G  Proofs for Realizing V-learning ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In the following proof, we focus on the max-player's perspective, which can be easily extended for the min-player. ", "page_idx": 26}, {"type": "text", "text": "G.1  Details of V-learning ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "The details of V-learning [32], discussed in Sec. 3.2, are presented in Alg. 3, where the following notations are adopted ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\alpha_{n}=\\frac{H+1}{H+n},\\,\\,\\,\\beta_{n}=c\\cdot\\sqrt{\\frac{H^{3}A\\log(H S A G/\\delta)}{n}},}\\\\ {\\displaystyle\\gamma_{n}=\\eta_{n}=\\sqrt{\\frac{H\\log(A)}{A n}},\\,\\,\\,\\omega_{n}=\\alpha_{n}\\left(\\prod_{\\tau=2}^{n}(1-\\alpha_{\\tau})\\right)^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "After the learning process, V-learning requires an additional procedure to provide the output policy. We include this procedure in Alg. 4, where the following notations are adopted ", "page_idx": 27}, {"type": "text", "text": "$N^{g,h}(s)=$ the number of times $s$ is visited at step $h$ before episode $g$ \uff0c $g_{i}^{h}(s)=$ the index of the episode $s$ is visited at step $h$ for the $i$ -th time. ", "page_idx": 27}, {"type": "text", "text": "Following the results in Jin et al. [32], we can obtain the following performance guarantee of V-learning. ", "page_idx": 27}, {"type": "text", "text": "Theorem G.1 (Theorem 4 in Jin et al. [32]). With probability at least $1-\\delta$ in anyenvironment $M$ theoutputpolicies $(\\hat{\\mu},\\hat{\\nu})$ from $V{\\mathrm{.}}$ learningsatisfythat ", "page_idx": 27}, {"type": "equation", "text": "$$\nV_{M}^{\\dagger,\\hat{\\nu}}(s^{1})-V_{M}^{\\hat{\\mu},\\dagger}(s^{1})=O\\left(\\sqrt{\\frac{H^{5}S(A+B)\\log(S A B T/\\delta)}{G}}\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus, with $\\delta=1/T$ , we can obtain that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}_{D\\sim\\mathbb{P}_{M}^{\\mathrm{v1eaming}}}\\left[V_{M}^{\\dagger,\\hat{\\nu}}(s^{1})-V_{M}^{\\hat{\\mu},\\dagger}(s^{1})\\right]=O\\left(\\sqrt{\\frac{H^{5}S(A+B)\\log(S A B T)}{G}}\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Algorithm 3 V-learning [32] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1: Initialize: for any $(s,a,h)\\in S\\times A\\times[H],V^{h}(s)\\gets H+1-h,N^{h}(s)\\gets0,\\mu^{h}(a|s)\\gets1/A$   \n2: for episode $g=1,\\cdot\\cdot\\cdot,G$ do   \n3: receive s9,1   \n4: for step $h=1,\\cdots\\,,H$ do   \n5: Take action $a^{g,h}\\sim\\pi^{h}(\\cdot|s^{g,h})$ , observe reward $r^{g,h}$ and next state $s^{g,h+1}$   \n6: Update $n=N^{h}(s^{g,h})\\leftarrow N^{h}(s^{g,h})+1$   \n7: Update $\\tilde{V}^{h}(s^{g,h})\\gets\\left(1-\\alpha_{n}\\right)\\tilde{V}^{h}(s^{g,h})+\\alpha_{n}\\left(r_{\\mathrm{~s~}}^{g,h}+V^{h+1}(s^{g,h+1})+\\beta_{n}\\right)$   \n8: Update $V^{h}(s^{g,h})\\longleftarrow\\operatorname*{min}\\left\\{H+1-h,\\tilde{V}^{h}(s^{g,h})\\right\\}$   \n9: Compute eh(s9,h a) < H-mg,h-vyh+1(s,h+1) \\*(sa=,+)n forall a A   \n10: Update $\\begin{array}{r}{\\mu^{h}(a|s^{g,h})\\propto\\exp\\left(-\\frac{\\eta_{n}}{\\omega_{n}}\\cdot\\sum_{\\tau\\in[n]}\\omega_{\\tau}\\cdot\\tilde{\\ell}_{\\tau}^{h}(s^{g,h},a)\\right)}\\end{array}$ for all a A ", "page_idx": 27}, {"type": "text", "text": "11: end for ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "12: end for ", "page_idx": 27}, {"type": "text", "text": "G.2 An Additional Assumption About Transformers Performing Division ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Before digging into the proof of Theorem 3.4, we first state the following assumption that there exists one transformer that can perform the division operation. ", "page_idx": 27}, {"type": "text", "text": "Assumption G.2. There exists one transformer $\\mathrm{TF}_{\\theta}$ with ", "page_idx": 27}, {"type": "equation", "text": "$$\nL=L_{D},\\;\\;\\operatorname*{max}_{l\\in[L]}M^{(l)}=M_{D},\\;\\;d^{\\prime}=d_{D},\\;\\;\\|\\pmb\\theta\\|=F_{D},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Algorithm 4 V-learning Executing Output Policy $\\hat{\\mu}$ [32] ", "text_level": 1, "page_idx": 28}, {"type": "table", "img_path": "pRQmRaonxf/tmp/1cff1bbb7b88490b8e77ab20ff8606307106a09edb42ceef604e36bb52adefc3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "such that for any $x\\ \\in\\ [0,1],y\\ \\in\\ [\\sqrt{\\log(A)/(A G)},\\sqrt{\\log(A)/A}+1],\\varphi\\,>\\,0$ with input ${\\textbf{\\em H}}=$ $[h_{1},\\cdot\\cdot\\cdot\\,,h_{e},\\cdot\\cdot\\cdot]$ where ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{h}_{i}=\\left[\\begin{array}{c}{0}\\\\ {\\varphi}\\\\ {\\mathbf{0}}\\end{array}\\right],\\;\\;\\forall i\\neq e;\\;\\,h_{e}=\\left[\\begin{array}{c}{x}\\\\ {y}\\\\ {\\mathbf{0}}\\end{array}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "the transformer can provide output $\\overline{{\\pmb{H}}}=[\\overline{{\\pmb{h}}}_{1},\\cdot\\cdot\\cdot\\,,\\overline{{\\pmb{h}}}_{e},\\cdot\\cdot\\cdot]$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overline{{\\boldsymbol{h}}}_{i}=\\left[\\begin{array}{c}{0}\\\\ {\\varphi}\\\\ {0}\\\\ {{\\bf0}}\\end{array}\\right],\\;\\,\\forall i\\neq e;\\;\\,\\overline{{\\boldsymbol{h}}}_{e}=\\left[\\begin{array}{c}{x}\\\\ {y}\\\\ {x/y}\\\\ {\\bf0}\\end{array}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We note that this assumption on performing exact division is only for the convenience of the proof as one can approximate the division to any arbitrary precision only via one MLP layer with ReLU activation. ", "page_idx": 28}, {"type": "text", "text": "In particular, let $\\mathrm{Ball}_{\\infty}^{k}(R)=[-R,R]^{\\infty}$ denote the standard $\\ell_{\\infty}$ ball in $\\mathbb{R}^{k}$ with radius $R>0$ we introduce the following definitions and result. ", "page_idx": 28}, {"type": "text", "text": "Definition G.3 (Approximability by Sum of ReLUs, Definition 12 in Bai et al. [7]). A function $g:\\mathbb{R}^{k}\\rightarrow\\mathbb{R}$ $(\\varepsilon_{\\mathsf{a p p r o x}},R,M,C)$ approxablebysumofUsiterexist $a$ $(M,C)$ sumof ReLUs\"function ", "page_idx": 28}, {"type": "equation", "text": "$$\nf_{M,C}(z)=\\sum_{m=1}^{M}c_{m}\\sigma(\\pmb{a}_{m}^{\\top}[z;1])\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "with ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\sum_{m=1}^{M}|c_{m}|\\leq C,\\,\\,\\,\\operatorname*{max}_{m\\in[M]}\\|a_{m}\\|_{1}\\leq1,\\,\\,\\,a_{m}\\in\\mathbb{R}^{k+1},\\,\\,\\,c_{m}\\in\\mathbb{R}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "such that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{z\\in\\mathrm{Ball}_{\\infty}^{k}(R)}\\left|g(z)-f_{M,C}(z)\\right|\\leq\\varepsilon_{\\mathrm{approx}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Definition G.4 (Sufficiently Smooth $k$ -variable Function, Definition A.1 in Bai et al. [7]). We say $a$ function $g:\\mathbb{R}^{k}\\rightarrow\\mathbb{R}$ is $(R,C_{\\ell})$ -smooth if for $s=\\lceil(k-1)/2\\rceil+2,$ g is a $C^{s}$ function on $\\mathrm{Ball}_{\\infty}^{k}(R)$ and ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{z\\in{\\mathrm{Ball}}_{\\infty}^{k}(R)}\\|\\nabla^{i}g(z)\\|_{\\infty}=\\operatorname*{sup}_{z\\in{\\mathrm{Ball}}_{\\infty}^{k}(R)}\\operatorname*{max}_{j_{1},\\cdots,j_{i}\\in[k]}|\\partial_{z_{j_{1}}\\cdots z_{j_{i}}}g(z)|\\leq L_{i}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "for all $i\\in\\{0,1,\\cdots\\,,s\\}$ with ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{0\\leq i\\leq s}L_{i}R^{i}\\leq C_{\\ell}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proposition G.5 (Approximating Smooth $k$ -variable Functions, Proposition A.1 in Bai et al. [7]). For any $\\varepsilon_{\\mathrm{approx}}>0,R\\geq1,C_{\\ell}>0,$ we have the following: Any $(R,C_{\\ell})$ -smoothfunction $g:\\mathbb{R}^{k}\\rightarrow\\mathbb{R}$ $(\\varepsilon_{\\mathsf{a p p r o x}},R,M,C)$ aproximablbysumof ReLUus wih $M\\leq C(k)C_{\\ell}^{2}\\log(1+C_{\\ell}/\\varepsilon_{a p p r o x})/\\varepsilon_{a p p r o x}^{2}$ and $C\\le C(k)C_{\\ell}$ where $C(k)>0$ is a constant that depends only on $k$ ", "page_idx": 28}, {"type": "text", "text": "Then, we can consider $g(x,y)=(c_{1}+x)/(c_{2}+y)$ with $c_{1}\\geq1$ $c_{2}\\geq1+c_{2}^{\\prime}>1$ $x\\in[-1,1]$ and $y\\in[-1,1]$ . It can be verified that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle|g(x,y)|=\\frac{c_{1}+x}{c_{2}+y}\\leq\\frac{1+c_{1}}{c_{2}^{\\prime}};\\;\\;|\\partial_{x}g(x,y)|=\\frac{1}{c+y}\\leq\\frac{1}{c_{2}^{\\prime}};\\;\\;|\\partial_{y}g(x,y)|=\\frac{c_{1}+x}{(c_{2}+y)^{2}}\\leq\\frac{1+c_{1}}{(c_{2}^{\\prime})^{2}};}\\\\ {\\displaystyle|\\partial_{x}^{2}g(x,y)|=0;\\;\\;|\\partial_{y}^{2}g(x,y)|=\\frac{2(c_{1}+x)}{(c_{2}+y)^{3}}\\leq\\frac{2(1+c_{1})}{(c_{2}^{\\prime})^{3}};\\;\\;|\\partial_{x}\\partial_{y}g(x,y)|=\\frac{1}{(c+y)^{2}}\\leq\\frac{1}{(c_{2}^{\\prime})^{2}};}\\\\ {\\displaystyle|\\partial_{x}^{3}g(x,y)|=0;\\;\\;|\\partial_{y}^{3}g(x,y)|=\\frac{6(c_{1}+x)}{(c_{2}+y)^{4}}\\leq\\frac{6(1+c_{1})}{(c_{2}^{\\prime})^{4}};}\\\\ {\\displaystyle|\\partial_{x}^{2}\\partial_{y}g(x,y)|=0;\\;\\;|\\partial_{x}\\partial_{y}^{2}g(x,y)|=\\frac{2}{(c+y)^{3}}\\leq\\frac{2}{(c_{2}^{\\prime})^{3}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Then, from Definition G.4 there exists one $C_{l}$ such that $g(x,y)$ is $(1,C_{l})$ -smooth. Thus, by Proposition G.5, this function can be approximated by a sum of ReLUs (defined in Definition G.3). With this observation, the division operation required in Assumption G.2 can be approximated. ", "page_idx": 29}, {"type": "text", "text": "G.3 Proof of Theorem 3.4: The Realization Construction ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "G.3.1 Embeddings and Extraction Mappings ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We consider each episode of the max-player's observations to be embedded in $2H$ tokens. In particular, for each $t\\in[T]$ ,weconstruct that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h_{2t-1}=\\mathbf{h}_{+}(s^{g,h})=\\left[\\begin{array}{c}{0_{A}}\\\\ {\\cdots}\\\\ {0_{s^{g},h}}\\\\ {\\cdots}\\\\ {0_{0}}\\\\ {\\vdots}\\\\ {0_{0}s_{2t-1}}\\end{array}\\right]=\\left[\\begin{array}{c}{h_{2t-1}^{\\mathrm{pr},a}}\\\\ {\\cdots}\\\\ {-\\frac{1}{g_{2t-1}^{\\mathrm{pr},b}}}\\\\ {\\vdots}\\\\ {h_{2t-1}^{\\mathrm{pr},a}}\\end{array}\\right],}\\\\ {h_{2t}=\\mathbf{h}_{+}(a^{g,h},r^{g,h})=\\left[\\begin{array}{c}{a^{g,h}}\\\\ {\\cdots}\\\\ {\\frac{0_{s}}{g_{2t}}\\cdots}\\\\ {\\vdots}\\\\ {0_{0}s_{2t}}\\end{array}\\right]=\\left[\\begin{array}{c}{h_{2t}^{\\mathrm{pr},a}}\\\\ {\\cdots}\\\\ {-\\frac{h_{2t}^{\\mathrm{pr},a}}{h_{2t}^{\\mathrm{pr},c}}}\\\\ {\\vdots}\\\\ {h_{2t}^{\\mathrm{pr},d}}\\end{array}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $s^{g,h},a^{g,h}$ are represented via one-hot embedding. The positional embedding $\\mathbf{pos}_{i}$ is defined as ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbf{pos}_{i}:=\\left[\\begin{array}{c}{g}\\\\ {h}\\\\ {t}\\\\ {e_{h}}\\\\ {v_{i}}\\\\ {i}\\\\ {i^{2}}\\\\ {1}\\end{array}\\right],\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $v_{i}:=\\mathbb{1}\\{h_{i}^{a}=\\mathbf{0}\\}$ denote the tokens that do not embed actions and rewards. ", "page_idx": 29}, {"type": "text", "text": "In summary, for observations $D_{+}^{t-1}\\cup\\{s^{t}\\}$ , we obtain the tokens of length $2t-1$ which can be expressed as the following form: ", "page_idx": 29}, {"type": "equation", "text": "$$\nH:=h_{+}(D_{+}^{t-1},s^{t})=[h_{1},h_{2},\\cdot\\cdot\\cdot\\cdot,h_{2t-1}]=[\\mathtt{h.}(s^{1}),\\mathtt{h.}(a^{1},r^{1}),\\cdot\\cdot\\cdot\\cdot,\\mathtt{h.}(s^{t})].\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "With the above input $\\pmb{H}$ . the transformer outputs $\\overline{{{\\cal H}}}\\,=\\,\\mathrm{TF}_{\\theta_{+}}(H)$ of the same size as $\\pmb{H}$ . The extraction mapping $\\mathtt{A}$ is direcly set to satisfy the following ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathtt{A}\\cdot\\overline{{h}}_{-1}=\\mathtt{A}\\cdot\\overline{{h}}_{2t-1}=\\overline{{h}}_{2t-1}^{c}\\in\\mathbb{R}^{A},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "i.e., the part $c$ of the output tokens is used to store the learned policy. ", "page_idx": 29}, {"type": "text", "text": "G.3.2 An Overview of the Proof ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "In the following, for the convenience of notations, we will consider step $t+1$ , i.e., with observations $D_{+}^{t}\\cup\\{s^{t+1}\\}$ Given an input token matrix ", "page_idx": 30}, {"type": "equation", "text": "$$\nH=h_{+}(D_{+}^{t},s^{t+1})=[h_{1},h_{2},\\cdot\\cdot\\cdot,h_{2t+1}],\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "we construct a transformer to perform the following steps ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left[\\begin{array}{c}{h_{2t+1}^{\\mathrm{pre}}}\\\\ {h_{2t+1}^{\\mathrm{pre}}}\\\\ {h_{2t+1}^{\\mathrm{pre}}}\\\\ {h_{2t+1}^{\\mathrm{pre}}}\\end{array}\\right]\\xrightarrow{\\mathrm{step~1}}\\left[\\begin{array}{c}{h_{2t+1}^{\\mathrm{pre}}(a,b,c)}\\\\ {h_{2t+1}^{\\mathrm{pre}}}\\\\ {\\star}\\\\ {0}\\\\ {\\bf p o s_{2t+1}}\\end{array}\\right]\\xrightarrow{\\mathrm{step~2}}\\left[\\begin{array}{c}{h_{2t+1}^{\\mathrm{pre}}(a,b,c)}\\\\ {h_{2t+1}^{\\mathrm{pre}}}\\\\ {V^{t}(s^{t})}\\\\ {\\star}\\\\ {0}\\\\ {{\\bf p o s}_{2t+1}}\\end{array}\\right]\\xrightarrow{\\mathrm{step~3}}\\left[\\begin{array}{c}{h_{2t+1}^{\\mathrm{pre}}(a,b,c)}\\\\ {h_{2t+1}^{\\mathrm{pre}}}\\\\ {\\vdots}\\\\ {\\omega^{t}(\\cdot|s^{t})}\\\\ {\\star}\\\\ {0}\\\\ {{\\bf p o s}_{2t+1}}\\end{array}\\right]}&{}\\\\ {\\xrightarrow{\\mathrm{step~4}}\\left[\\begin{array}{c}{h_{2t+1}^{\\mathrm{pre}}(a,b)}\\\\ {\\mu^{t+1}(\\cdot|s^{t+1})}\\\\ {h_{2t+1}^{\\mathrm{q}}}\\\\ {h_{2t+1}^{\\mathrm{q}}}\\end{array}\\right]:=\\left[\\begin{array}{c}{h_{2t+1}^{\\mathrm{pre}}(a,b)}\\\\ {h_{2t+1}^{\\mathrm{pre}}}\\\\ {h_{2t+1}^{\\mathrm{pre}}}\\\\ {h_{2t+1}^{\\mathrm{pre}}}\\end{array}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $n:=N^{h}(s^{g,h})$ ", "page_idx": 30}, {"type": "text", "text": "To ease the notations, in the proof, we will slightly abuse $\\mathrm{TF}_{\\theta}$ as $\\mathrm{TF}_{\\theta_{+}}$ . The following provides a sketch of the proof. ", "page_idx": 30}, {"type": "text", "text": "Step 1 There exists an attention-only transformer $\\mathrm{TF}_{\\theta}$ to complete Step 1 with ", "page_idx": 30}, {"type": "equation", "text": "$$\nL=O(1),~~\\operatorname*{max}_{l\\in[L]}M^{(l)}=O(1),~~\\lVert\\pmb\\theta\\rVert=O(H G)\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Step 2 There exists a transformer $\\mathrm{TF}_{\\theta}$ to complete Step 2 with ", "page_idx": 30}, {"type": "equation", "text": "$$\nL=O(H G),\\;\\operatorname*{max}_{l\\in[L]}M^{(l)}=O(H S^{2}),\\;\\;d^{\\prime}=O(G),\\;\\;\\|\\theta\\|=O(G^{3}+G H),\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Step 3 There exists a transformer $\\mathrm{TF}_{\\theta}$ to complete Step 3 with ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{L=O(G H L_{D}),\\,\\,\\,\\operatorname*{max}_{l\\in[L]}M^{(l)}=O(H S A+M_{D}),\\,\\,\\,d^{\\prime}=O(d_{D}+A+G),}\\\\ &{{\\|\\theta\\|}=O(G H^{2}S+F_{D}+G^{3}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Step 4 There exists a transformer $\\mathrm{TF}_{\\theta}$ to complete Step 4 with ", "page_idx": 30}, {"type": "equation", "text": "$$\nL=O(1),~~\\operatorname*{max}_{l\\in[L]}M^{(l)}=O(H S),~~\\|\\pmb\\theta\\|=O(H S+G H).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Thus, the overall transformer $\\mathrm{TF}_{\\theta}$ can be summarized as ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{L=O(G H L_{D}),\\,\\,\\,\\underset{l\\in[L]}{\\operatorname*{max}}\\,M^{(l)}=O(H S^{2}+H S A+M_{D}),\\,\\,\\,d^{\\prime}=O(G+A+d_{D}),}\\\\ &{\\|\\theta\\|=O(G H^{2}S+G^{3}+F_{D}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Also, from the later construction, we can observe that $\\log(R)=\\tilde{\\mathcal{O}}(1)$ . The bound on the covering number, i.e., $\\log(N_{\\Theta_{+}})$ can be obtained via Lemma I.4. ", "page_idx": 30}, {"type": "text", "text": "G.3.3 Proof of Step 1: Update $N^{h}(s^{g,h})$ ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "From the proof of Step 1 in realizing UCB-VI in Lin et al. [38], we know that there exists a transformer with 3 heads that for all $t^{\\prime}\\leq t$ can move $s^{t^{\\prime}},(a^{t^{\\prime}},r^{t^{\\prime}})$ from $h_{2t^{\\prime}-1}^{a}$ and $h_{2t^{\\prime}}^{b}$ 10 $h_{2t^{\\prime}+1}^{d}$ while maintaining $h_{2t^{\\prime}}^{d}$ not updated. Thisconstructed transformer is shown in Lin e al [38] to be an attention-only one with ReLU activation and ", "page_idx": 30}, {"type": "equation", "text": "$$\nL=2,\\;\\;\\operatorname*{max}_{l\\in[L]}M^{(l)}\\leq3,\\;\\;\\|\\theta\\|\\leq O(H G).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Then, still following Lin et al. [38], another attention-only transformer can be constructed so that $N^{t}(s^{t})$ canbecomputedin $h_{2t+1}^{d}$ The construted rasormerhasReLU actiationand ", "page_idx": 31}, {"type": "equation", "text": "$$\nL=2,\\ \\operatorname*{max}_{l\\in[L]}M^{(l)}=O(1),\\ \\|\\pmb\\theta\\|=O(H).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "In summary, at the end of Step 1, we can obtain that for all $t^{\\prime}\\leq t$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h_{2t^{\\prime}}^{d}=\\left[\\begin{array}{c}{0}\\\\ {{\\bf p o s}_{2t^{\\prime}}}\\end{array}\\right],\\;\\;h_{2t^{\\prime}+1}^{d}=\\left[\\begin{array}{c}{s^{t^{\\prime}}}\\\\ {a^{t^{\\prime}}}\\\\ {r^{t^{\\prime}}}\\\\ {N^{t^{\\prime}}\\!\\left(s^{t^{\\prime}}\\right)}\\\\ {{\\bf0}}\\\\ {{\\bf p o s}_{2t^{\\prime}+1}}\\end{array}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "with an attention-only transformer that uses ReLU activation and ", "page_idx": 31}, {"type": "equation", "text": "$$\nL=4,\\ \\operatorname*{max}_{l\\in[L]}M^{(l)}=O(1),\\ \\ \\lVert\\pmb\\theta\\rVert=O(H G).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "G.3.4 Proof of Step 2: Compute $\\tilde{V}^{h}(s^{g,h})$ and $V^{h}(s^{g,h})$ ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In Step 2, let $n^{t}=N^{t}(s^{t})$ , the following computations will be performed: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{V}_{\\mathrm{new}}^{t}(s^{t})\\gets(1-\\alpha_{n^{t}})\\tilde{V}_{\\mathrm{old}}^{t}(s^{t})+\\alpha_{n^{t}}\\left(r^{t}+V_{\\mathrm{old}}^{t+1}(s^{t+1})+\\beta_{n^{t}}\\right),}\\\\ &{V_{\\mathrm{new}}^{t}(s^{t})\\gets\\operatorname*{min}\\left\\{H+1-h,\\tilde{V}_{\\mathrm{new}}^{t}(s^{t})\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "First, similar to Step 2 in realizing UCB-VI in Lin et al. [38], we can obtain $\\alpha_{n^{t^{\\prime}}}$ and $\\beta_{n^{t^{\\prime}}}$ in each $h_{2t^{\\prime}+1}^{d}$ via transformer with ReLU ativtion and ", "page_idx": 31}, {"type": "equation", "text": "$$\nL=O(1),\\ \\operatorname*{max}_{l\\in[L]}M^{(l)}=O(1),\\ d^{\\prime}=O(G),\\ \\lVert\\pmb\\theta\\rVert=O(G^{3}).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Then, we can assume that the values of $\\{\\tilde{V}_{\\mathrm{old}}^{h}(s),V_{\\mathrm{old}}^{h}(s):h\\in[H],s\\in\\mathcal{S}\\}$ is already computed in token $h_{2\\tau-1}^{d}$ adp $h_{2\\tau+1}^{d}$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\boldsymbol{h}_{2\\tau-1}^{d}=\\left[\\begin{array}{c}{\\star}\\\\ {\\tilde{V}_{\\mathrm{old}}^{1;H}(\\cdot)}\\\\ {V_{\\mathrm{old}}^{1;H}(\\cdot)}\\\\ {\\mathbf{0}}\\\\ {\\mathbf{pos}_{2\\tau-1}}\\end{array}\\right],\\;\\boldsymbol{h}_{2\\tau}^{d}=\\left[\\begin{array}{c}{\\mathbf{0}}\\\\ {\\mathbf{pos}_{2\\tau}}\\end{array}\\right],\\;\\boldsymbol{h}_{2\\tau+1}^{d}=\\left[\\begin{array}{c}{\\mathbf{0}}\\\\ {\\mathbf{pos}_{2\\tau+1}}\\end{array}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\frac{\\mathrm{compute}}{\\mathrm{conpute}},\\,h_{2\\tau}^{d}=\\left[\\begin{array}{c}{{\\bf0}}\\\\ {{\\bf p o s}_{2\\tau}}\\end{array}\\right],\\,h_{2\\tau+1}^{d}=\\left[\\begin{array}{c}{\\star}\\\\ {\\tilde{V}_{\\mathrm{new}}^{1:H}(\\cdot)}\\\\ {V_{\\mathrm{new}}^{1:H}(\\cdot)}\\\\ {{\\bf0}}\\\\ {{\\bf p o s}_{2\\tau+1}}\\end{array}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "In the following, we will show that this one-step update can be completed via a transformer that requires ", "page_idx": 31}, {"type": "equation", "text": "$$\nL=O(1),\\ \\operatorname*{max}_{l\\in[L]}M^{(l)}=O(H S^{2}),\\ \\,d^{\\prime}=O(G),\\ \\,||\\pmb{\\theta}||=O(G^{3}+G H),\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and the overall updates can be completed via stacking $T$ similar transformers. ", "page_idx": 31}, {"type": "text", "text": "Step 2.1: Obtain $\\tau-|t^{\\prime}-1-\\tau|$ ", "page_idx": 31}, {"type": "text", "text": "First, we obtain an auxiliary value $\\tau\\mathrm{~-~}\\left|t^{\\prime}\\mathrm{~-~}1\\mathrm{~-~}\\tau\\right|$ in each. $h_{2t^{\\prime}-1}^{d}$ and $h_{2t^{\\prime}}^{d}$ for all $t^{\\prime}\\ \\ \\leq\\ \\ t$ In particular, we can construct three MLP layers with ReLU activation, i.e., $\\{W_{1}^{(1)},W_{2}^{(1)},\\bar{W}_{1}^{(2)},W_{2}^{(2)},W_{1}^{(3)},W_{2}^{(3)}\\}$ to sequentil computethat ", "page_idx": 31}, {"type": "equation", "text": "$$\n[\\begin{array}{l}{0}\\end{array}]\\rightarrow\\sigma_{r}\\left([\\begin{array}{l}{t^{\\prime}-1-\\tau}\\end{array}]\\right);\n$$", "text_format": "latex", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sigma_{r}\\left(\\left[\\begin{array}{l}{t^{\\prime}-1-\\tau}\\end{array}\\right]\\right)\\to\\sigma_{r}\\left(\\left[\\begin{array}{l}{t^{\\prime}-1-\\tau}\\end{array}\\right]\\right)+\\sigma_{r}\\left(\\left[\\begin{array}{l}{\\tau-t^{\\prime}+1}\\end{array}\\right]\\right)=\\left[\\begin{array}{l}{|t^{\\prime}-1-\\tau|}\\end{array}\\right]}\\\\ &{\\quad\\quad\\left[\\begin{array}{l}{|t^{\\prime}-1-\\tau|}\\end{array}\\right]\\to\\left[\\begin{array}{l}{\\tau-|t^{\\prime}-1-\\tau|}\\end{array}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "It can be observed that $\\tau-|t^{\\prime}-1-\\tau|$ reaches its maximum $\\tau$ at $t^{\\prime}=\\tau+1$ ", "page_idx": 32}, {"type": "text", "text": "The required transformer can be summarized as ", "page_idx": 32}, {"type": "equation", "text": "$$\nd^{\\prime}=1,\\;\\;\\|\\pmb{\\theta}\\|=O(G H).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Step 2.2: Move $\\tilde{V}_{\\mathbf{old}}^{1:H}(\\cdot)$ and $V_{\\mathbf{old}}^{1:H}(\\cdot)$ ", "page_idx": 32}, {"type": "text", "text": "Then, we move $\\tilde{V}_{\\mathrm{old}}^{1:H}(\\cdot)$ and $V_{\\mathrm{old}}^{1:H}(\\cdot)$ from $h_{2\\tau-1}$ $\\ensuremath{h_{2\\tau+1}}$ .In partiular, we canconstrut tt ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{Q_{1}h_{2\\ell^{\\prime}}=\\left[\\begin{array}{c}{v_{2\\ell^{\\prime}}-1}\\\\ {\\tau-\\left|{\\boldsymbol t^{\\prime}}-1-\\tau\\right|}\\\\ {-1}\\\\ {t^{\\prime}}\\end{array}\\right],\\;\\;Q_{1}h_{2\\ell^{\\prime}+1}=\\left[\\begin{array}{c}{v_{2\\ell^{\\prime}+1}-1}\\\\ {\\tau-\\left|{\\boldsymbol t^{\\prime}}-\\tau\\right|}\\\\ {-1}\\\\ {t^{\\prime}+1}\\\\ {1}\\end{array}\\right],}\\\\ &{}&{K_{1}h_{2\\ell^{\\prime}}=\\left[\\begin{array}{c}{1}\\\\ {1}\\\\ {\\tau}\\\\ {\\tau}\\\\ {\\left|{\\boldsymbol t^{\\prime}}+1\\right|}\\end{array}\\right],\\;\\;K_{1}h_{2\\ell^{\\prime}+1}=\\left[\\begin{array}{c}{1}\\\\ {1}\\\\ {\\tau}\\\\ {\\left|{\\boldsymbol t^{\\prime}}+2\\right|}\\end{array}\\right];}\\\\ &{}&{V_{1}h_{2\\ell^{\\prime}-1}=(2\\tau+1)\\left[\\begin{array}{c}{\\tilde{V}_{1\\ell}^{(1)}(\\cdot)}\\\\ {\\tilde{V}_{2\\ell}^{(1)}(\\cdot)}\\\\ {\\tilde{V}_{3\\ell}^{(1)}(\\cdot)}\\end{array}\\right],\\;\\;V_{1}h_{2\\tau}=0,\\;\\;V_{1}h_{2\\tau+1}=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "With ReLU activation, this construction leads to that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{c c}{{h_{2t^{\\prime}}=h_{2t^{\\prime}}+\\displaystyle\\frac{1}{2t^{\\prime}}\\sum_{i\\leq2t^{\\prime}}\\sigma_{r}\\left(-1-\\left|t^{\\prime}-1-\\tau\\right|-t^{\\prime}+t(i)+1\\right)\\cdot V_{1}h_{i}={\\bf0},\\,\\,\\forall t^{\\prime}\\leq t}}\\\\ {{h_{2t^{\\prime}+1}=h_{2t^{\\prime}+1}+\\displaystyle\\frac{1}{2t^{\\prime}+1}\\sum_{i\\leq2t^{\\prime}+1}\\sigma_{r}\\left(-\\left|t^{\\prime}-\\tau\\right|-t^{\\prime}+t(i)+1\\right)\\cdot V_{1}h_{i}}}\\\\ {{=\\left\\{\\displaystyle\\frac{1}{2t^{\\prime}+1}\\left(V_{1}h_{2t^{\\prime}-1}+V_{1}h_{2t^{\\prime}}+2V_{1}h_{2t^{\\prime}+1}\\right)=\\left[\\begin{array}{l}{{\\tilde{V}_{\\mathrm{old}}^{1;h}(\\cdot)}}\\\\ {{V_{\\mathrm{old}}^{1;H}(\\cdot)}}\\end{array}\\right]}}&{{\\mathrm{if~}t^{\\prime}=\\tau}}\\\\ {{{\\bf0}}}&{{\\mathrm{otherwise}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "The transformer required in Step 2.3 can be summarized as ", "page_idx": 32}, {"type": "equation", "text": "$$\nL=1,\\;\\;M^{(1)}=O(1),\\;\\;\\|\\pmb\\theta\\|=O(G H).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Step 2.3: Compute $\\tilde{V}_{\\mathbf{new}}^{1:H}(\\cdot)$ and $V_{\\mathbf{new}}^{1:H}(\\cdot)$ ", "page_idx": 32}, {"type": "text", "text": "Finally, we get to compute $\\tilde{V}_{\\mathrm{new}}^{1:H}(\\cdot)$ and $V_{\\mathrm{new}}^{1:H}(\\cdot)$ , where ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\tilde{V}_{\\mathrm{new}}^{\\tau}(s^{\\tau})=\\tilde{V}_{\\mathrm{old}}^{\\tau}(s^{\\tau})-\\alpha_{n^{\\tau}}\\tilde{V}_{\\mathrm{old}}^{\\tau}(s^{\\tau})+\\alpha_{n^{\\tau}}\\left(r^{\\tau}+V_{\\mathrm{old}}^{\\tau+1}(s^{\\tau+1})+\\beta_{n^{\\tau}}\\right).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We can have a $H S$ -head transformer that ", "page_idx": 32}, {"type": "equation", "text": "$$\nQ_{h,s}h_{2t^{\\prime}}=\\left[\\begin{array}{c}{v_{2t^{\\prime}}-1}\\\\ {\\tau-|t^{\\prime}-1-\\tau|}\\\\ {-1}\\\\ {t^{\\prime}}\\\\ {1}\\\\ {0}\\\\ {e_{h^{\\prime}}}\\\\ {1}\\\\ {0}\\end{array}\\right],\\ Q_{h,s}h_{2t^{\\prime}+1}=\\left[\\begin{array}{c}{v_{2t^{\\prime}+1}-1}\\\\ {\\tau-|t^{\\prime}-\\tau|}\\\\ {-1}\\\\ {t^{\\prime}+1}\\\\ {1}\\\\ {s^{t^{\\prime}}}\\\\ {e_{h^{\\prime}+1}}\\\\ {1}\\\\ {\\alpha_{n^{\\prime}}}\\end{array}\\right],\n$$", "text_format": "latex", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{K_{h,s}h_{2t^{\\prime}}=\\left[\\begin{array}{c}{1}\\\\ {1}\\\\ {\\tau}\\\\ {-1}\\\\ {t^{\\prime}}\\\\ {e_{s}}\\\\ {e_{h+1}}\\\\ {-2}\\\\ {1}\\end{array}\\right],\\ \\ K_{h,s}h_{2t^{\\prime}+1}=\\left[\\begin{array}{c}{1}\\\\ {1}\\\\ {\\tau}\\\\ {-1}\\\\ {t^{\\prime}+1}\\\\ {e_{s}}\\\\ {e_{h+1}}\\\\ {-2}\\\\ {1}\\end{array}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "equation", "text": "$$\nV_{h,s}h_{2\\tau+1}=-(2\\tau+1)\\cdot V_{\\mathrm{old}}^{h}(s)\\cdot\\left[\\begin{array}{l}{e_{h,s}}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "It holds that for all $t^{\\prime}\\leq t$ ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{c l}{{h_{2t^{\\prime}}=h_{2t^{\\prime}}+\\displaystyle\\frac{1}{2t^{\\prime}}\\sum_{h,s}\\sum_{i\\le2t^{\\prime}}\\sigma_{r}\\left(-1-\\left|t^{\\prime}-1-\\tau\\right|-t^{\\prime}+t(i)+e_{h^{\\prime}}\\cdot e_{h+1}-2\\right)V_{h,s}h_{i}={\\bf0}}}\\\\ {{\\mathrm{}}}\\\\ {{\\displaystyle\\imath_{2t^{\\prime}+1}=h_{2t^{\\prime}+1}+\\displaystyle\\frac{1}{2t^{\\prime}+1}\\sum_{h,s}\\sum_{i\\le2t^{\\prime}+1}\\sigma_{r}\\left(-|t^{\\prime}-\\tau|-t^{\\prime}-1+t(i)+s^{t^{\\prime}}\\cdot e_{s}+e_{h^{\\prime}+1}\\cdot e_{h}+\\alpha_{n^{\\prime}}-2\\right)}}\\\\ {{\\mathrm{}}}\\\\ {{\\displaystyle=\\left\\{V_{\\mathrm{old}}^{1;H}(\\cdot)-\\alpha_{n^{\\prime}}V_{\\mathrm{old}}^{h^{\\prime}}(s^{\\prime})\\cdot e_{h^{\\prime},s^{\\prime\\prime}}\\quad\\mathrm{if~}t^{\\prime}=\\tau\\right.}}\\\\ {{\\displaystyle{\\bf0}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "which completes the computation $(1-\\alpha_{n^{\\tau}})\\tilde{V}_{\\mathrm{old}}^{\\tau}(s^{\\tau})$ ", "page_idx": 33}, {"type": "text", "text": "Two similar $H S$ -head transformers can further perform $(1-\\alpha_{n^{\\tau}})\\tilde{V}_{\\mathrm{old}}^{\\tau}(s^{\\tau})+\\alpha_{n^{\\tau}}r^{\\tau}+\\alpha_{n^{\\tau}}\\beta_{n^{\\tau}}$ . and a similar $H S^{2}$ -head transformer can finalize $\\tilde{V}_{\\mathrm{new}}^{\\tau}(s^{\\tau})$ as $(1-\\alpha_{n^{\\tau}})\\tilde{V}_{\\mathrm{old}}^{\\tau}(s^{\\tau})+\\alpha_{n^{\\tau}}r^{\\tau}+\\alpha_{n^{\\tau}}\\beta_{n^{\\tau}}+$ Qnz Vit+(st+1). ", "page_idx": 33}, {"type": "text", "text": "Finally, from the proof of Step 3 in realizing UCB-VI in Lin et al. [38], one MLP layer can perform ", "page_idx": 33}, {"type": "equation", "text": "$$\nV_{\\mathrm{old}}^{\\tau}(s^{\\tau})=\\operatorname*{min}\\left\\{\\tilde{V}_{\\mathrm{new}}^{\\tau}(s^{\\tau}),H-h+1\\right\\}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "The required transformer in step 2.4 can be summarized as ", "page_idx": 33}, {"type": "equation", "text": "$$\nL=4,\\ \\operatorname*{max}_{l\\in[L]}M^{(l)}=O(H S^{2}),\\,\\ d^{\\prime}=O(1),\\ \\|\\pmb\\theta\\|=O(G H).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Thus, we can see that the update of $\\boldsymbol{h}_{2\\tau+1}$ can be done. Repeating the similar step for each $\\tau\\in[T]$ would complete the overall updates. ", "page_idx": 33}, {"type": "text", "text": "G.3.5 Proof of Step 3: Compute $\\tilde{\\ell}_{n^{t}}(s^{g,h},a)$ and $\\mu^{h}(a|s^{g,h})$ ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "In step 3, let $n^{t}=N^{t}(s^{t})$ , for step $t$ , we update ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\ell}_{n^{t}}^{t}(s^{t},a)\\gets\\frac{H-r^{t}-V^{t+1}\\left(s^{t+1}\\right)}{H}\\cdot\\frac{\\mathbf{1}\\{a=a^{t}\\}}{\\mu_{\\mathrm{old}}^{t}(a^{t}|s^{t})+\\gamma_{n^{t}}};}\\\\ &{\\mu_{\\mathrm{new}}^{t}(a|s^{t})\\propto\\exp\\left(-\\frac{\\eta_{n^{t}}}{\\omega_{n^{t}}}\\cdot\\displaystyle\\sum_{i\\in[n^{t}]}\\omega_{i}\\cdot\\tilde{\\ell}_{i}^{t}(s^{t},a)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "First, similar to Step 2, we can obtain $\\omega_{n^{t^{\\prime}}},\\eta_{n^{t^{\\prime}}}/\\omega_{n^{t^{\\prime}}}$ and $\\gamma_{n^{t^{\\prime}}}$ in each 22t'+1 via a transformer with ReLU activation and ", "page_idx": 33}, {"type": "equation", "text": "$$\nL=O(1),\\ \\operatorname*{max}_{l\\in[L]}M^{(l)}=O(1),\\ d^{\\prime}=O(G),\\ \\lVert\\pmb\\theta\\rVert=O(G^{3}).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Denoting vectors $\\mathcal{L}\\in\\mathbb{R}^{H S A}$ and $\\pmb{\\mu}\\in\\mathbb{R}^{H S A}$ containing the cumulative losses and policies, i., $\\textstyle\\sum_{i\\in[n]}\\omega_{i}{\\tilde{\\ell}}_{i}^{h}(s,a)$ and $\\mu^{h}(a|s)$ . Similar to step 2, we will assume that the values of $\\{\\mathcal{L}_{\\mathrm{old}},\\mu_{\\mathrm{old}}\\}$ ", "page_idx": 33}, {"type": "text", "text": "which are computed via information before time $\\tau$ are aready contained in token $h_{2\\tau-1}^{d}$ and prove viainductintoshwthat thet of newalus canbecomputeditke $h_{2\\tau+1}^{d}$ ie., ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\boldsymbol{h}_{2\\tau-1}^{d}=\\left[\\begin{array}{c}{\\boldsymbol{\\star}}\\\\ {\\mathcal{L}_{\\mathrm{old}}}\\\\ {\\boldsymbol{\\mu}_{\\mathrm{old}}}\\\\ {\\mathbf{0}}\\\\ {\\mathbf{pos}_{2\\tau-1}}\\end{array}\\right],\\;\\boldsymbol{h}_{2\\tau}^{d}=\\left[\\begin{array}{c}{\\mathbf{0}}\\\\ {\\mathbf{pos}_{2\\tau}}\\end{array}\\right],\\;\\boldsymbol{h}_{2\\tau+1}^{d}=\\left[\\begin{array}{c}{\\mathbf{0}}\\\\ {\\mathbf{pos}_{2\\tau+1}}\\end{array}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "equation", "text": "$$\n\\xrightarrow{\\mathrm{compute}}h_{2\\tau}^{d}=\\left[\\begin{array}{c}{\\mathbf{0}}\\\\ {\\mathbf{pos}_{2\\tau}}\\end{array}\\right],\\,h_{2\\tau+1}^{d}=\\left[\\begin{array}{c}{\\star}\\\\ {\\mathcal{L}_{\\mathrm{new}}}\\\\ {\\mu_{\\mathrm{new}}}\\\\ {\\mathbf{0}}\\\\ {\\mathbf{pos}_{2\\tau+1}}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "In the following, we will show that this one-step update can be completed via a transformer that requires ", "page_idx": 34}, {"type": "equation", "text": "$$\nL=O(L_{D}),\\ \\ \\operatorname*{max}_{l\\in[L]}M^{(l)}=O(H S A+M_{D}),\\,\\ d^{\\prime}=O(d_{D}+A),\\ \\ \\|\\theta\\|=O(G H^{2}S+F_{D}),\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and the overall updates can be completed via stacking $T$ similar transformers. ", "page_idx": 34}, {"type": "text", "text": "Step 3.1: Obtain $\\tau-|t^{\\prime}-1-\\tau|$ ", "page_idx": 34}, {"type": "text", "text": "First, similar to step 2.1, we can have an auxiliary value $\\tau-|t^{\\prime}-1-\\tau|$ ineach $h_{2t^{\\prime}-1}^{d}$ and $h_{2t^{\\prime}}^{d}$ for all $t^{\\prime}<t$ via three MLP layers. The required transformer can be summarized as ", "page_idx": 34}, {"type": "equation", "text": "$$\nd^{\\prime}=1,\\;\\;\\|\\pmb{\\theta}\\|=O(G H).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Step 3.2: Compute $\\tilde{\\ell}_{n^{\\tau}}^{\\tau}(s^{\\tau},a)$ ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "First, similar to Step 2.2, we can move ${\\mathcal L}_{\\mathrm{old}}$ and $\\pmb{\\mu}_{\\mathrm{old}}$ from $h_{2\\tau-1}^{d}$ $h_{2\\tau+1}^{d}$ while keeping other tokens unchanged. In particular, we can construct that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{Q_{1}h_{2\\ell^{\\prime}}=\\left[\\begin{array}{c}{v_{2\\ell^{\\prime}}-1}\\\\ {\\tau-|^{t^{\\prime}}-1-\\tau|}\\\\ {-1}\\\\ {t^{\\prime}}\\end{array}\\right],}}&{Q_{1}h_{2\\ell^{\\prime}+1}=\\left[\\begin{array}{c}{v_{2\\ell^{\\prime}+1}-1}\\\\ {\\tau-|^{t^{\\prime}}-\\tau|}\\\\ {-1}\\\\ {t^{\\prime}+1}\\\\ {1}\\end{array}\\right]}}\\\\ &{}&{K_{1}h_{2\\ell^{\\prime}}=\\left[\\begin{array}{c}{1}\\\\ {1}\\\\ {\\tau}\\\\ {\\tau}\\\\ {-1}\\\\ {t^{\\prime}+1}\\end{array}\\right],}&{K_{1}h_{2\\ell^{\\prime}+1}=\\left[\\begin{array}{c}{1}\\\\ {1}\\\\ {\\tau}\\\\ {\\tau-1}\\\\ {t^{\\prime}+2}\\end{array}\\right];}\\\\ &{}&{V_{1}h_{2\\ell^{\\prime}-1}=(2\\tau+1)\\left[\\begin{array}{c}{Z_{0\\ell^{\\prime}}}\\\\ {f_{0\\ell}}\\\\ {\\mu_{0\\ell}}\\end{array}\\right],}&{V_{1}h_{2\\ell^{\\prime}}=0,\\ {\\cal V}_{1}h_{2\\ell^{\\prime}+1}=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "With ReLU activation, this construction leads to that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h_{2t^{\\prime}}=\\mathbf{0},\\,\\,\\,\\forall t^{\\prime}\\leq t;\\,\\,\\,h_{2t^{\\prime}+1}=\\left\\{\\left[\\begin{array}{l}{\\mathcal{L}_{\\mathrm{old}}}\\\\ {\\mu_{\\mathrm{old}}}\\end{array}\\right]\\right.\\,\\,\\,\\mathrm{if}\\,t^{\\prime}=\\tau}\\\\ {\\mathbf{0}\\,\\,\\,\\,\\,\\,\\mathrm{otherwise}}\\end{array}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Then, using similar constructions as Step 2.3, we can specificlly extract $V^{\\tau+1}(s^{\\tau+1})$ and $\\mu_{\\mathrm{old}}^{\\tau}(a^{\\tau}|s^{\\tau})$ tohd $h_{2\\tau+1}^{d}$ while keeping other tokens unchanged. Following the extraction, one MLP layer can compute the value H---v+1(s++1) and ad it to $h_{2\\tau+1}^{d}$ The requiredtransformer canbe summarized as ", "page_idx": 34}, {"type": "equation", "text": "$$\nL=O(1),\\ \\operatorname*{max}_{l\\in[L]}M^{(l)}=O(H S A),\\,\\ d^{\\prime}=1,\\ \\|\\pmb\\theta\\|=O(G H).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "With Assumption G.2, for token $\\ensuremath{h_{2\\tau+1}}$ , we can further have one transformer to compute ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\tilde{\\ell}_{n^{\\tau}}^{\\tau}(s^{\\tau},a^{\\tau})=\\frac{H-r^{\\tau}-V^{\\tau+1}(s^{\\tau+1})}{H}\\cdot\\frac{1}{\\mu_{\\mathrm{old}}^{\\tau}(a^{\\tau}|s^{\\tau})+\\gamma_{n^{\\tau}}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "The overall required transformer can be summarized as ", "page_idx": 35}, {"type": "equation", "text": "$$\nL=O(L_{D}),\\ \\ \\operatorname*{max}_{l\\in[L]}M^{(l)}=O(H S A+M_{D}),\\,\\ d^{\\prime}=O(d_{D}),\\ \\ \\|\\theta\\|=O(G H+F_{D}).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Step 3.3: Update $\\mu^{\\tau}(a|s^{\\tau})$ ", "page_idx": 35}, {"type": "text", "text": "First, one transformer can be constructed toupdate ${\\mathcal{L}}_{\\mathrm{old}}$ $\\mathcal{L}_{\\mathrm{new}}$ .n $h_{2\\tau+1}^{d}$ . In particular, we can have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q_{h,s}h_{2r}=\\left[\\begin{array}{c}{\\tau_{2r}-1}\\\\ {\\tau-\\left|t^{\\prime}-1-\\tau\\right|}\\\\ {\\tau}\\\\ {t^{\\prime}}\\\\ {1}\\\\ {e_{h}}\\\\ {1}\\end{array}\\right],\\ \\ Q_{h,s}h_{2r+1}=\\left[\\begin{array}{c}{\\tau_{2r+1}-1}\\\\ {\\tau-\\left|t^{\\prime}-\\tau\\right|}\\\\ {\\tau+1}\\\\ {1}\\\\ {e_{h}^{\\prime}}\\\\ {e_{h}^{\\prime}}\\\\ {1}\\end{array}\\right],}\\\\ &{K_{h,b,\\tau_{2r}}=\\left[\\begin{array}{c}{1}\\\\ {\\frac{1}{-\\tau}}\\\\ {\\tau}\\\\ {\\frac{-\\tau}{t}}\\\\ {e_{h}^{\\prime}}\\\\ {e_{h}+1}\\end{array}\\right],\\ \\ K_{h,b,a_{T}+1}=\\left[\\begin{array}{c}{1}\\\\ {\\frac{1}{-\\tau}}\\\\ {\\tau-1}\\\\ {\\epsilon+1}\\\\ {\\epsilon+\\frac{\\epsilon}{t}}\\\\ {e_{h}^{\\prime}+1}\\end{array}\\right],}\\\\ &{\\kappa_{b,1}=\\left[\\begin{array}{c}{h_{1}}\\\\ {e_{T}}\\\\ {h_{2}}\\\\ {1}\\end{array}\\right],\\ \\ K_{b,1}h_{2r+1}=-(2r+1)\\cdot\\mathcal{L}_{a b}(h_{s},s)}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "With ReLU activation, this construction leads to that ", "page_idx": 35}, {"type": "equation", "text": "$$\nh_{2t^{\\prime}}=\\mathbf{0},\\;\\;\\forall t^{\\prime}\\leq t;\\;\\;h_{2t^{\\prime}+1}=\\left\\{\\!\\!\\begin{array}{l l}{\\!\\!\\mathscr{L}_{\\mathrm{old}}(h(\\tau),s^{\\tau},\\cdot)}&{\\mathrm{if~}t^{\\prime}=\\tau}\\\\ {\\!\\!\\!0}&{\\mathrm{otherwise}}\\end{array}\\!\\!\\right..\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "With a similar $A$ -head transformer, we can add $\\omega_{n^{\\tau}}\\cdot\\tilde{\\ell}_{n^{\\tau}}^{\\tau}(s^{\\tau},a^{\\tau})$ to $\\mathcal{L}_{\\mathrm{old}}(h(\\tau),s^{\\tau},a^{\\tau})$ and thus obtain the new values $\\mathcal{L}_{\\mathrm{new}}(h(\\tau),s^{\\tau},\\cdot)$ . Furthermore, a one-head transformer can obtain ", "page_idx": 35}, {"type": "equation", "text": "$$\nh_{2\\tau+1}=\\frac{\\eta_{n^{\\tau}}}{\\omega_{n^{\\tau}}}\\cdot\\mathcal{L}_{\\mathrm{new}}(h(\\tau),s^{\\tau},\\cdot)\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Finally, with another softmax MLP layer, we can obtain $\\mu_{\\mathrm{new}}^{\\tau}(\\cdot|s^{\\tau})$ ", "page_idx": 35}, {"type": "text", "text": "The required transformer can be summarized as ", "page_idx": 35}, {"type": "equation", "text": "$$\nL=O(1),\\ \\operatorname*{max}_{l\\in[L]}M^{(l)}=O(H S+A),\\,\\ d^{\\prime}=O(A),\\ \\lVert\\pmb\\theta\\rVert=O(G H^{2}S).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "G.3.6 Proof of Step 4: Obtain $\\mu^{h+1}(\\cdot|s^{h+1})$ ", "page_idx": 35}, {"type": "text", "text": "This step is essentially the same as Step 6 in realizing VI-ULCB, which can be completed with a transformer that ", "page_idx": 35}, {"type": "equation", "text": "$$\nL=1,\\;\\;M^{(1)}=H S,\\;\\;\\|\\pmb\\theta\\|=O(H S).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "H  Proofs for the Decentralized Overall Performance ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Proof of Theorem 3.5. We use the decomposition that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{E}_{M\\sim\\Lambda,D\\sim\\mathbb{P}_{M}^{\\mathrm{at}}\\hat{\\theta}}\\left[V_{M}^{\\dagger,\\hat{\\nu}}(s^{1})-V_{M}^{\\hat{\\mu},\\dagger}(s^{1})\\right]=\\mathbb{E}_{M\\sim\\Lambda,D\\sim\\mathbb{P}_{M}^{\\mathrm{Ag}_{0}}}\\left[V_{M}^{\\dagger,\\hat{\\nu}}(s^{1})-V_{M}^{\\hat{\\mu},\\dagger}(s^{1})\\right]\n$$", "text_format": "latex", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{+\\,\\mathbb{E}_{M\\sim\\Lambda,D\\sim\\mathbb{P}_{M}^{\\mathbb{A}\\mathbb{L}_{\\hat{\\theta}}}}\\left[V_{M}^{\\dagger,\\hat{\\nu}}(s^{1})\\right]-\\mathbb{E}_{M\\sim\\Lambda,D\\sim\\mathbb{P}_{M}^{\\mathbb{A}\\mathbb{L}_{\\ell}0}}\\left[V_{M}^{\\dagger,\\hat{\\nu}}(s^{1})\\right]}\\\\ &{+\\,\\mathbb{E}_{M\\sim\\Lambda,D\\sim\\mathbb{P}_{M}^{\\mathbb{A}\\mathbb{L}_{\\ell}0}}\\left[V_{M}^{\\hat{\\mu},\\dagger}(s^{1})\\right]-\\mathbb{E}_{M\\sim\\Lambda,D\\sim\\mathbb{P}_{M}^{\\mathbb{A}\\mathbb{L}_{\\hat{\\theta}}}}\\left[V_{M}^{\\hat{\\mu},\\dagger}(s^{1})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "First, via Theorem G.1, it holds that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbb{E}_{M\\sim\\Lambda,D\\sim\\mathbb{P}_{M}^{\\mathrm{Ag_{0}}}}\\left[V_{M}^{\\dagger,\\hat{\\nu}}(s^{1})-V_{M}^{\\hat{\\mu},\\dagger}(s^{1})\\right]=O\\left(\\sqrt{\\frac{H^{5}S(A+B)\\log(S A B T)}{G}}\\right).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Then, via Lemma 3.6 and Theorem 3.3, we can obtain that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{^{\\mathfrak{L}_{M\\sim\\Lambda,D\\sim\\mathbb{P}_{M}^{\\mathtt{A g}_{0}}}\\left[V_{M}^{\\hat{\\mu},\\dagger}(s^{1})\\right]-\\mathbb{E}_{M\\sim\\Lambda,D\\sim\\mathbb{P}_{M}^{\\mathtt{A g}_{\\hat{\\theta}}}}\\left[V_{M}^{\\hat{\\mu},\\dagger}(s^{1})\\right]}\\\\ &{=O\\left(H\\cdot\\displaystyle\\sum_{t\\in[T],s\\in S}\\mathbb{E}_{\\alpha}\\left[\\operatorname{TV}\\big(\\mu_{\\alpha}^{t},\\mu_{\\beta}^{t}|D_{+}^{t-1},s\\big)+\\operatorname{TV}\\big(\\nu_{\\alpha}^{t},\\nu_{\\beta}^{t}|D_{-}^{t-1},s\\big)\\right]\\right)}\\\\ &{=O\\left(T H S\\sqrt{\\varepsilon_{+,\\mathrm{real}}}+T H S\\sqrt{\\varepsilon_{-,\\mathrm{real}}}+T H S\\sqrt{\\frac{\\log\\big(N\\!\\varphi_{+}T S/\\delta\\big)}{N}}+T H S\\sqrt{\\frac{\\log\\big(N\\!\\varphi_{-}T S/\\delta\\big)}{N}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "and similarly, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{^{\\mathbb{Z}}_{M\\sim\\Lambda,D\\sim\\mathbb{P}_{M}^{\\mathbb{A}_{\\delta}}\\delta}\\left[V_{M}^{\\dagger,\\hat{\\nu}}(s^{1})\\right]-\\mathbb{E}_{M\\sim\\Lambda,D\\sim\\mathbb{P}_{M}^{\\mathbb{A}_{\\delta}}}\\left[V_{M}^{\\dagger,\\hat{\\nu}}(s^{1})\\right]}\\\\ &{=O\\left(T H S\\sqrt{\\varepsilon_{+,\\mathrm{real}}}+T H S\\sqrt{\\varepsilon_{-,\\mathrm{real}}}+T H S\\sqrt{\\frac{\\log\\left(\\mathcal{N}\\Theta_{+}T S/\\delta\\right)}{N}}+T H S\\sqrt{\\frac{\\log\\left(\\mathcal{N}\\Theta_{-}T S/\\delta\\right)}{N}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "With Theorem 3.4 providing that $\\varepsilon_{+,\\mathrm{real}}=\\varepsilon_{-,\\mathrm{real}}=0$ . combining the above terms completes the proof of the regret bound. The bound on the covering number, i.e., $\\log(N_{\\Theta_{+}})$ and $\\log(N_{\\Theta_{-}})$ can be obtained via Lemma I.4. \u53e3 ", "page_idx": 36}, {"type": "text", "text": "In the following, we provide the proof for the Lemma 3.6, which plays a key role in the above proof of the overall performance. ", "page_idx": 36}, {"type": "text", "text": "Proof of Lemma 3.6. It holds that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{D\\sim\\mathbb{P}_{M}^{\\mathrm{Ag}_{\\alpha}}}\\left[V_{M}^{\\hat{\\mu}_{\\alpha},\\dagger}(s^{1})\\right]-\\mathbb{E}_{D\\sim\\mathbb{P}_{M}^{\\mathrm{Ag}_{\\beta}}}\\left[V_{M}^{\\hat{\\mu}_{\\beta},\\dagger}(s^{1})\\right]}\\\\ &{=\\underbrace{\\mathbb{E}_{D\\sim\\mathbb{P}_{M}^{\\mathrm{Ag}_{\\alpha}}}\\left[V_{M}^{\\hat{\\mu}_{\\alpha},\\dagger}(s^{1})-V_{M}^{\\hat{\\mu}_{\\beta},\\dagger}(s^{1})\\right]}_{:=(\\mathrm{term~I})}+\\underbrace{\\mathbb{E}_{D\\sim\\mathbb{P}_{M}^{\\mathrm{Ag}_{\\alpha}}}\\left[V_{M}^{\\hat{\\mu}_{\\beta},\\dagger}(s^{1})\\right]-\\mathbb{E}_{D\\sim\\mathbb{P}_{M}^{\\mathrm{Ag}_{\\beta}}}\\left[V_{M}^{\\hat{\\mu}_{\\beta},\\dagger}(s^{1})\\right]}_{:=(\\mathrm{term~II})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Denoting $\\begin{array}{r}{f(D^{H})=\\sum_{h\\in[H]}r^{h}}\\end{array}$ we can obtain that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{M}^{\\beta_{\\beta_{i}},\\gamma}(s)^{1}-V_{M}^{\\beta_{\\beta_{i}},\\gamma}(s^{1})}\\\\ &{\\stackrel{(a)}{\\leq}V_{M}^{\\beta_{\\alpha_{i}},\\gamma_{\\beta},(\\beta_{\\beta})}(s^{1})-V_{M}^{\\beta_{\\beta_{i}},\\varepsilon_{\\gamma}(\\hat{\\mu}_{\\beta})}(s^{1})}\\\\ &{\\stackrel{(b)}{=}\\frac{1}{G}\\sum_{g\\in[G]}^{\\varepsilon}\\left[V_{M}^{\\beta_{g^{\\prime}\\mu}(\\hat{\\mu}_{\\beta})}(s^{1})-V_{M}^{\\beta_{g^{\\prime}\\mu}(\\hat{\\mu}_{\\beta})}(s^{1})\\right]}\\\\ &{\\leq\\frac{1}{G}\\sum_{g\\in[G]}^{\\varepsilon}\\sum_{k\\in[H]}^{\\varepsilon}\\Bigg[\\mathbb{E}_{D^{1:1}\\sim\\mathbb{R}_{M}^{\\beta_{g^{\\prime}\\mu}^{\\varepsilon}(\\hat{\\mu}_{\\beta})},D^{b+1:H}\\sim\\mathbb{R}_{M}^{\\beta_{g^{\\prime}\\mu}^{\\varepsilon}(\\hat{\\mu}_{\\beta})}}\\left[f(D^{H})\\right]\\Bigg]}\\\\ &{-\\frac{1}{G}\\sum_{g\\in[G]}^{\\varepsilon}\\sum_{k\\in[H]}^{\\varepsilon}\\left[\\mathbb{E}_{D^{1:1-\\varepsilon}\\sim\\mathbb{R}_{M}^{\\beta_{g^{\\prime}\\mu}^{\\varepsilon}(\\hat{\\mu}_{\\beta})},D^{b+1:H}\\sim\\mathbb{R}_{M}^{\\beta_{g^{\\prime}\\mu}^{\\varepsilon}(\\hat{\\mu}_{\\beta})}}\\left[f(D^{H})\\right]\\right]}\\\\ &{\\stackrel{(c)}{\\leq}\\frac{2H}{G}\\sum_{g\\in[G]}^{\\varepsilon}\\sum_{k\\in[H]}^{\\varepsilon}\\mathbb{E}_{D^{1:1-\\varepsilon}\\sim\\mathbb{R}_{M}^{\\beta_{g^{\\prime}\\mu}^{\\varepsilon}(\\hat{\\mu}_{\\beta})},s^{\\star}}\\left[\\mathbf{Y}\\left(\\hat{\\mu}_{\\alpha}^{\\varepsilon,h}\\times\\nu_{\\uparrow}^{h}(\\hat{\\mu}_{\\beta})(\\cdot,\\cdot|s^{h}),\\hat{\\mu}_{\\beta}^{\\varepsilon,h}\\times\\nu_{\\uparrow}^\n$$", "text_format": "latex", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\frac{2H}{G}\\displaystyle\\sum_{g\\in[\\mathcal{Q}]}\\sum_{h\\in[\\mathcal{Q}]}\\mathbb{E}_{p^{1+\\lfloor h\\rfloor}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\mathbb{E}_{h}^{\\ y_{1},h}\\!\\!\\exp[\\theta_{\\alpha}^{\\theta_{1},h}(\\cdot\\vert s),h^{\\theta_{2},h}(\\cdot\\vert s)]\\Big]}\\\\ &{\\stackrel{(a)}{=}\\frac{2H}{G}\\displaystyle\\sum_{g\\in[\\mathcal{Q}]}\\sum_{h\\in[\\mathcal{H}]}\\sum_{h^{1+\\lfloor\\rfloor}\\sim\\mathcal{P}_{h}^{\\theta_{1},h-\\lfloor\\theta_{2}\\rfloor}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where (a) is from the definition of best responses, (b) uses the notation $\\hat{\\mu}^{g}$ to denote the output policy from Alg. 4 with the initial random sampling result to be $g$ (c) uses the variational representation of the TV distance, (d) uses the abbreviations $n=N^{g,h}(s^{h})$ and $g_{i}\\,=\\,g_{i}^{h}(s^{h})$ , (e) leverages the property of TV distance, and (f) uses the fact that $\\alpha_{n,i}<1$ ", "page_idx": 37}, {"type": "text", "text": "With the above result, we can further obtain that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathrm{term~I}):=\\mathbb{E}_{D\\sim\\mathbb{P}_{M}^{{\\texttt A}_{g}}}\\left[V_{M}^{\\hat{\\mu}_{\\alpha},\\dagger}(s^{1})-V_{M}^{\\hat{\\mu}_{\\beta},\\dagger}(s^{1})\\right]\\leq\\mathbb{E}_{D\\sim\\mathbb{P}_{M}^{{\\texttt A}_{g}}}\\left[2H\\sum_{t\\in[T]}\\sum_{s\\in S}\\mathrm{TV}\\left(\\mu_{\\alpha}^{t}(\\cdot|s),\\mu_{\\beta}^{t}(\\cdot|s)\\right)\\right].\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Also, for term I), denoting $g(D)=V_{M}^{\\hat{\\mu}_{\\beta},\\dagger}(s^{1})$ it holds that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{(term~II):}=\\mathbb{E}_{D\\sim\\mathbb{P}_{M}^{\\mathtt{A g}}}\\left[V_{M}^{\\beta_{\\beta},\\dagger}(s^{1})\\right]-\\mathbb{E}_{D\\sim\\mathbb{P}_{M}^{\\mathtt{A g}}}\\left[V_{M}^{\\beta_{\\beta},\\dagger}(s^{1})\\right]}\\\\ &{\\phantom{\\sum_{\\ell\\in[T]}}=\\sum_{t\\in[T]}^{\\infty}\\mathbb{E}_{D^{t}\\sim\\mathbb{P}_{M}^{\\mathtt{A g}},D^{t+1:T}\\sim\\mathbb{P}_{M}^{\\mathtt{A g}_{\\theta}}}[g(D)]-\\sum_{t\\in[T]}^{\\infty}\\mathbb{E}_{D^{t-1}\\sim\\mathbb{P}_{M}^{\\mathtt{A g}},D^{t+T}\\sim\\mathbb{P}_{M}^{\\mathtt{A g}_{\\theta}}}[g(D)]}\\\\ &{\\phantom{\\sum_{\\ell\\in[T]}}\\le2H\\sum_{\\ell\\in[T]}^{\\infty}\\mathbb{E}_{D^{t-1}\\sim\\mathbb{P}_{M}^{\\mathtt{A g}_{\\theta}}}\\mathrm{,~}s^{t}\\left[\\mathrm{TV}\\left(\\mathtt{A l g}_{\\alpha}(\\cdot,\\cdot|D^{t-1},s^{t}),\\mathtt{A l g}_{\\beta}(\\cdot,\\cdot|D^{t-1},s^{t})\\right)\\right]}\\\\ &{\\le2H\\displaystyle\\sum_{t\\in[T]}^{\\infty}\\sum_{s\\in\\mathcal{S}}\\mathbb{E}_{D^{t-1}\\sim\\mathbb{P}_{M}^{\\mathtt{A g}_{\\theta}}}\\left[\\mathrm{TV}\\left(\\mu_{\\alpha}^{t}(\\cdot|D_{+}^{t-1},s^{t}),\\mu_{\\beta}^{t}(\\cdot,\\cdot|D_{+}^{t-1},s^{h})\\right)\\right]}\\\\ &{\\phantom{\\sum_{\\ell\\in[T]}}+2H\\displaystyle\\sum_{t\\in[T]}^{\\infty}\\sum_{s\\in\\mathcal{S}}\\mathbb{E}_{D^{t-1}\\sim\\mathbb{P}_{M}^{\\mathtt{A g}_{\\theta}}}\\left[\\mathrm{TV}\\left(\\nu_{\\alpha}^{t}(\\cdot|D_{+}^{t-1},s),\\nu_{\\beta}^{t}(\\cdot,\\cdot|D_{+}^{t-1},s)\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Combining (term I) and (term II) concludes the proof. ", "page_idx": 37}, {"type": "text", "text": "1   Discussions on the Covering Number ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "In the following, we characterize the covering number of algorithms induced by transformers parameterizedby $\\pmb{\\theta}\\in\\Theta_{d,L,M,d^{\\prime},F}$ i.e., $\\left\\{\\mathsf{A1g}_{\\theta}:\\theta\\in\\Theta_{d,L,M,d^{\\prime},F}\\right\\}$ . Here we will mainly take the perspective of the centralized setting, while the extension to the decentralized setting is straightforward. ", "page_idx": 37}, {"type": "text", "text": "To facilitate the discussion, we introduce the following clipped transformer, where an additional clip operator is adopted to bound the output of the transformer. ", "page_idx": 37}, {"type": "text", "text": "Definition I.1 (Clipped Decoder-based Transformer). An $L$ -layer clipped decoder-based transformer, denoted as $\\mathrm{TF}_{\\theta}^{R}(\\cdot)$ , is a composition of $L$ masked attention layers, each followed by an MLP layer and a clip operation: $\\mathrm{TF}_{\\theta}^{R}(H)\\,=\\,H^{(L)}\\,\\in\\,\\mathbb{R}^{d\\times N}$ ,where $H^{(L)}$ is defined iteratively by taking $\\begin{array}{r}{H^{(0)}=\\mathrm{clip}_{R}(H)\\in\\mathbb{R}^{d\\times N}}\\end{array}$ and for $l\\in[L]$ ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{H}^{(l)}=\\mathrm{clip}_{R}\\left(\\mathbf{M}\\mathbf{L}\\mathbf{P}_{\\pmb{\\theta}_{\\mathrm{mip}}^{(l)}}\\left(\\mathbf{A}\\mathrm{ttn}_{\\pmb{\\theta}_{\\mathrm{matn}}^{(l)}}\\left(\\pmb{H}^{(l-1)}\\right)\\right)\\right)\\in\\mathbb{R}^{d\\times N},}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $\\operatorname{clip}_{R}(H)=[\\mathrm{proj}_{\\|h\\|_{2}\\leq R}(h_{i}):i\\in[N]]$ ", "page_idx": 37}, {"type": "text", "text": "Furthermore,for $\\zeta\\,\\in\\,(0,1]$ ,we define thefollowing $\\zeta$ biased algorithm induced by transformer $\\mathrm{TF}_{\\pmb{\\theta}}^{R}(\\pmb{H})$ as ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbb{A}\\mathrm{2}\\mathsf{g}_{\\theta}^{\\zeta}(\\cdot,\\cdot|D^{t-1},s^{t})=(1-\\zeta)\\cdot\\mathrm{proj}_{\\Delta}\\left(\\mathsf{E}\\cdot\\mathsf{T F}_{\\theta}^{R}\\left(\\mathtt{h}(D^{t-1},s^{t})\\right)_{-1}\\right)+\\frac{\\zeta}{A B}\\cdot\\mathbf{1}_{A B},\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "with ${\\bf1}_{A B}$ denoting an all-one vector of dimension $A B$ , which introduces a lower bound $\\zeta/A B$ for the probably each pair $(a,b)\\in A\\times B$ to be sampled. ", "page_idx": 38}, {"type": "text", "text": "Finally, for any $\\pmb{H}=[\\pmb{h}_{1},\\cdot\\cdot\\cdot\\mathrm{~,~}\\pmb{h}_{N}]\\in\\mathbb{R}^{d\\times N}$ , we denote ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\|\\pmb{H}\\|_{2,\\infty}:=\\operatorname*{max}_{i\\in[N]}\\|\\pmb{h}_{i}\\|_{2}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Proposition I.2 (Modified from Proposition J.1 in Bai et al. [7]). For any $\\pmb{\\theta}_{1},\\pmb{\\theta}_{2}\\in\\Theta_{d,L,M,d^{\\prime},F},$ we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\|\\mathrm{TF}_{\\theta_{1}}^{R}(H)-\\mathrm{TF}_{\\theta_{2}}^{R}(H)\\|_{2,\\infty}\\leq L F_{H}^{L-1}F_{\\Theta}\\|\\theta_{1}-\\theta_{2}\\|,\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $F_{\\Theta}:=F R(2+F R^{2}+F^{3}R^{2})$ and $F_{H}:=(1+F^{2})(1+F^{2}R^{3})$ ", "page_idx": 38}, {"type": "text", "text": "Proof. This proposition can be obtained similarly as Proposition J.1 in Bai et al. [7] with the following Lemma I.3 in the place of Lemma J.1 in Bai et al. [7]. \u53e3 ", "page_idx": 38}, {"type": "text", "text": "Lemma I3 (Modified from Lemma J.1 in Bai et al. [7]). For a single MLP layer $\\theta_{\\mathrm{mlp}}=(W_{1},W_{2})$ we introduceits norm ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\|\\pmb{\\theta}_{\\mathrm{mlp}}\\|=\\|\\pmb{W}_{1}\\|_{\\mathrm{op}}+\\|\\pmb{W}_{2}\\|_{\\mathrm{op}}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "For any fixed hidden dimension $D^{\\prime}$ weconsider ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\Theta_{\\mathrm{mlp},F}:=\\{\\theta_{\\mathrm{mlp}}:\\|\\theta_{\\mathrm{mlp}}\\|\\leq F\\}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Then, for any $H\\,\\in\\,\\mathcal{H}_{R},\\pmb{\\theta}_{\\mathrm{mlp}}\\,\\in\\,\\Theta_{\\mathrm{mlp},F},$ . it holds that $\\mathrm{MLP}_{\\theta_{\\mathrm{mlp}}}(H)$ . $2F R$ Lipschitz in $\\theta_{\\mathrm{mlp}}$ and $(1+F^{2})$ -Lipschitz in $\\pmb{H}$ ", "page_idx": 38}, {"type": "text", "text": "Proof. It holds that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|{\\mathrm{MLP}}_{\\theta_{\\mathrm{mp}}}(H)-{\\mathrm{MLP}}_{\\theta_{\\mathrm{mp}}^{\\prime}}(H)\\|_{2,\\infty}}\\\\ &{=\\underset{i}{\\mathrm{max}}\\,\\|{W}_{2}\\sigma({W}_{1}h_{i})-{W}_{2}^{\\prime}\\sigma({W}_{1}^{\\prime}h_{i})\\|_{2}}\\\\ &{\\le\\underset{i}{\\mathrm{max}}\\,\\|{W}_{2}-{W}_{2}^{\\prime}\\|_{\\mathrm{op}}\\|\\sigma({W}_{1}^{\\prime}h_{i})\\|_{2}+\\|{W}_{2}^{\\prime}\\|_{\\mathrm{op}}\\|\\sigma({W}_{1}h_{i})-\\sigma({W}_{1}^{\\prime}h_{i})\\|_{2}}\\\\ &{\\overset{(a)}{\\le}\\underset{i}{\\mathrm{max}}\\,\\|{W}_{2}-{W}_{2}^{\\prime}\\|_{\\mathrm{op}}\\operatorname*{max}\\{1,\\|{W}_{1}h_{i}\\|_{2}\\}+\\|{W}_{2}^{\\prime}\\|_{\\mathrm{op}}\\|{W}_{1}h_{i}-{W}_{1}^{\\prime}h_{i}\\|_{2}}\\\\ &{\\le(1+F R)\\|{W}_{2}-{W}_{2}^{\\prime}\\|_{\\mathrm{op}}+F R\\|{W}_{1}-{W}_{1}^{\\prime}\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Inequality (a) is from that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\|\\sigma_{\\mathrm{{r}}}(x)\\|_{2}\\leq\\|x\\|_{2},\\ \\ \\|\\sigma_{\\mathrm{{s}}}(x)\\|_{2}\\leq1,\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "and ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\|\\sigma_{\\mathrm{{r}}}(x)-\\sigma_{\\mathrm{{r}}}(y)\\|_{2}\\le\\|x-y\\|_{2},\\ \\ \\|\\sigma_{\\mathrm{{s}}}(x)-\\sigma_{\\mathrm{{s}}}(y)\\|_{2}\\le\\|x-y\\|_{2},\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where the last result on the Lipschitzness of softmax is adopted from Gao and Pavel [25]. Similarly, we can obtain that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathrm{MLP}_{\\theta_{\\mathrm{mip}}}(H)-\\mathrm{MLP}_{\\theta_{\\mathrm{mip}}}(H^{\\prime})\\|_{2,\\infty}}\\\\ &{=\\underset{i}{\\mathrm{max}}\\,\\|h_{i}+W_{1}\\sigma(W_{2}h_{i})-h_{i}^{\\prime}-W_{1}\\sigma(W_{2}h_{i}^{\\prime})\\|}\\\\ &{\\le\\|H-H^{\\prime}\\|_{2,\\infty}+\\|W_{1}\\|_{\\infty}\\underset{i}{\\mathrm{max}}\\,\\|\\sigma(W_{2}h_{i})-\\sigma(W_{2}h_{i}^{\\prime})\\|_{2}}\\\\ &{\\le\\|H-H^{\\prime}\\|_{2,\\infty}+\\|W_{1}\\|_{\\infty}\\underset{i}{\\mathrm{max}}\\,\\|W_{2}h_{i}-W_{2}h_{i}^{\\prime}\\|_{2}}\\\\ &{\\le\\|H-H^{\\prime}\\|_{2,\\infty}+F^{2}\\|H-H^{\\prime}\\|_{2,\\infty},}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "which concludes the proof. ", "page_idx": 38}, {"type": "text", "text": "Lemma I.4 (Modified from Lemma 16 in Lin et al. [38]). For the space of transformers $\\{\\mathrm{TF}_{\\pmb{\\theta}}^{R}:\\pmb{\\theta}\\in$ $\\Theta_{d,L,M,d^{\\prime},F}\\}$ , the covering number of the induced algorithms $\\left\\{\\mathsf{A1g}_{\\theta}^{\\zeta}:\\theta\\in\\Theta_{d,L,M,d^{\\prime},F}\\right\\}$ satisfies that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\log\\mathcal{N}_{\\Theta_{d,L,M,d^{\\prime},F}}(\\rho)=O\\left(L^{2}d(M d+d^{\\prime})\\log\\left(1+\\frac{\\operatorname*{max}\\{F,R,L\\}}{\\zeta\\rho}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Proof. First, similar to Lemma 16 in Lin et al. [38], we can use Example 5.8 in Wainwright [61] to obtain that the $\\delta$ -covering number of the ball $B_{\\|\\cdot\\|}(F)$ with radius $F$ under norm $\\parallel\\cdot\\parallel$ , i.e., $B_{\\parallel\\cdot\\parallel}(F)=\\{\\pmb\\theta:\\|\\pmb\\theta\\|\\leq F\\}$ , can be bounded as ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\log N(\\delta;B_{\\|\\cdot\\|}(F),\\|\\cdot\\|)\\leq L(3M d^{2}+2d d^{\\prime})\\log(1+2F/\\delta).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Recall that the projection operation to a probability simplex is Lipschitz continuous, i.e. ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathrm{proj}_{\\Delta}({\\pmb x})-\\mathrm{proj}_{\\Delta}({\\pmb y})\\|_{2}\\leq\\|{\\pmb x}-{\\pmb y}\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Then, we can see that there exists a subset $\\Tilde{\\Theta}\\subset\\Theta_{d,L,M,d^{\\prime},F}$ with size ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\log|\\Theta_{d,L,M,d^{\\prime},F}|\\leq L(3M d^{2}+2d d^{\\prime})\\log(1+2F/\\delta)\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "such that for any $\\pmb{\\theta}\\in\\Theta_{d,L,M,d^{\\prime},F}$ , there exists $\\tilde{\\pmb{\\theta}}\\in\\tilde{\\Theta}$ with ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\log\\mathrm{{Al}}g_{\\theta}^{\\zeta}(\\cdot,\\cdot|D^{t-1},s)-\\log\\mathrm{{Al}}g_{\\theta}^{\\zeta}(\\cdot,\\cdot|D^{t-1},s)\\right\\|_{\\infty}}\\\\ &{\\le\\displaystyle\\frac{A B}{\\zeta}\\left\\|\\mathrm{{Al}}g_{\\theta}^{\\zeta}(\\cdot,\\cdot|D^{t-1},s)-\\mathrm{{Al}}g_{\\theta}^{\\zeta}(\\cdot,\\cdot|D^{t-1},s)\\right\\|_{\\infty}}\\\\ &{\\le\\displaystyle\\frac{A B}{\\zeta}\\left\\|\\mathrm{{Tr}}_{\\theta}^{R}(H)-{\\mathrm{Tr}}_{\\theta}^{R}(H)\\right\\|_{2,\\infty}}\\\\ &{\\le\\displaystyle\\frac{A B}{\\zeta}\\cdot L F_{H}^{L-1}F_{\\Theta}\\cdot\\|\\theta-\\tilde{\\theta}\\|}\\\\ &{\\le\\displaystyle\\frac{A B}{\\zeta}L F_{H}^{L-1}F_{\\Theta}\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Let $\\begin{array}{r}{\\delta=\\frac{\\zeta\\rho}{A B L F_{H}^{L-1}F_{\\Theta}}}\\end{array}$ P,wecanobtangA(ID,s)logAg(,, \u2264pwhih proves that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\cot(\\mathcal{N}_{\\Theta_{d,L,M,d^{\\prime},F}}(\\rho))}\\\\ &{\\le L(3M d^{2}+2d d^{\\prime})\\log(1+2F/\\delta)}\\\\ &{=L(3M d^{2}+2d d^{\\prime})\\log\\left(1+\\frac{A B L F_{H}^{L-1}F_{\\Theta}}{\\zeta\\rho}\\right)}\\\\ &{=O\\left(L(3M d^{2}+2d d^{\\prime})\\log\\left(1+\\frac{A B L(1+F^{2})^{L-1}(1+F^{2}R^{3})^{L-1}F R(2+F R^{2}+F^{3}R^{2})}{\\zeta\\rho}\\right)\\right)}\\\\ &{=O\\left(L^{2}(M d^{2}+d d^{\\prime})\\log\\left(1+\\frac{\\operatorname*{max}\\{A,B,F,R,L\\}}{\\zeta\\rho}\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "which concludes the proof. ", "page_idx": 39}, {"type": "text", "text": "From the transformer construction in the proofs for Theorems 3.4 and 4.1, we can observe that it is sufficient to specify one $R$ With $\\log(R)={\\tilde{O}}(1)$ without impacting the transformers\u2019 operations. Also, in Theorems 3.3 and C.3, $\\rho$ is taken to be $1/N$ ", "page_idx": 39}, {"type": "text", "text": "Finally, for the introduced $\\zeta$ parameter, it can be recognized that the induced algorithms discussed in the main paper, i.e. ${\\tt A l g}_{\\theta}$ , can be interpreted as $\\zeta\\,=\\,0$ , which does not lead to a meaningful $\\log(\\mathcal{N}_{\\Theta_{d,L,M,d^{\\prime},F}}(\\rho))$ providediLmma4Hweve anon-ze $\\zeta$ can tackle thistuationby oly introducing an additional realization error. Especially, assuming Assumption C.2 can be achieved with $\\varepsilon_{\\mathrm{real}}=0$ , i.e., exactly realizing the context algorithm (as in Theorem 4.1), we can obtain that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\log\\left(\\mathbb{E}_{D\\sim\\mathbb{P}_{\\Lambda}^{\\mathrm{Alg}_{0}}}\\left[\\frac{\\mathrm{A}\\mathrm{1g}_{0}(a,b|D^{t-1},s)}{\\mathrm{A}\\mathrm{1g}_{\\theta^{*}}^{\\zeta}(a,b|D^{t-1},s)}\\right]\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\log\\left(\\mathbb E_{D\\sim\\mathbb P_{\\Lambda}^{\\mathrm{ag}}}\\left[\\frac{\\mathbb A1\\mathbb g_{0}(a,b|D^{t-1},s)}{\\mathbb A1\\mathbb g_{\\theta^{*}}(a,b|D^{t-1},s)}\\cdot\\frac{\\mathbb A1\\mathbb g_{\\theta^{*}}(a,b|D^{t-1},s)}{\\mathbb A1\\mathbb g_{\\theta^{*}}^{\\theta}(a,b|D^{t-1},s)}\\right]\\right)}\\\\ &{=\\log\\left(\\mathbb E_{D\\sim\\mathbb P_{\\Lambda}^{\\mathrm{ag}}}\\left[\\frac{\\mathbb A1\\mathbb g_{\\theta^{*}}(a,b|D^{t-1},s)}{\\left(1-\\zeta\\right)\\cdot\\mathbb A1\\mathbb g_{\\theta^{*}}(a,b|D^{t-1},s)+\\zeta/(A B)}\\right]\\right)}\\\\ &{\\leq\\log\\left(\\frac{1}{(1-\\zeta)+\\zeta/(A B)}\\right)}\\\\ &{\\leq\\log\\left(\\frac{1}{1-\\zeta}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "With $\\zeta~~=~{\\cal O}(1/N)$ , an additional realization error $\\varepsilon_{\\mathrm{real}}~=~O(1/N)$ occurs, whose impact on the overall performance bound (i.e., Theorems 3.5 and 4.2) is non-dominating. As a result, for the parameterization provided in Theorem 4.1, a covering number satisfies $\\log(\\mathcal{N}_{\\Theta})=$ $\\tilde{O}(\\mathrm{poly}(G H S A B)\\log(N))$ can be obtained. Similarly, the results can be extended to the decentralized setting, where for the parameterization provided in Theorem 3.4, it holds that $\\log(\\mathcal{N}_{\\Theta_{+}})=$ $\\tilde{O}(\\mathrm{poly}(G H S A L_{D}M_{D}d_{D})\\log(N F_{D})).$ ", "page_idx": 40}, {"type": "text", "text": "J  Details of Experiments ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "J.1  Detail of Games ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "\u25cf The normal-form games for the decentralized setting. The normal-form games (i.e., matrix games) used in decentralized experiments of Sec. 5 have $A=B=5$ actions for both players and $G=3000$ episodes (i.e., a horizon of $T=3000$ with $H=1$ ), which can be interpreted as having $S=1$ state. ", "page_idx": 40}, {"type": "text", "text": ". The Markov games for the centralized setting. The Markov games used in centralized experiments of Sec. 5 have $A=B=5$ actions for both players, $S=4$ states, $H\\,=\\,2$ steps in each episode and $G=300$ episodes (i.e., a horizon of $T=G H=600)$ 0. The transitions are fixed for different games: when both players take the same actions, the state transits to the next one (i.e., $1\\to2,\\cdots\\,,4\\to1)$ ; otherwise, the state stays the same. ", "page_idx": 40}, {"type": "text", "text": "At the start of each game (during both training and inference), a $A\\times B$ reward matrix $R_{h}(s,\\cdot,\\cdot)$ is generated for each step $h\\in[H]$ and state $s\\in S$ with its elements independently sampled from a standard Gaussian distribution truncated on $[0,1]$ . Then, the interactions proceed as follows: at each time step $h\\in[H]$ , the players select action $a$ and $b$ on state $s$ based on their computed policy distributions. After selecting their actions, the players receive rewards $R_{h}(s,a,b)$ and $\\bar{-R}_{h}(\\bar{s},a,\\bar{b})$ \uff0c respectively, while the state transits to the new one. ", "page_idx": 40}, {"type": "text", "text": "J.2  Collection of Pre-training Data ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "\u00b7 The EXP3 algorithm for the decentralized setting. In the decentralized setting, both players are equipped with the EXP3 algorithm [5] to collect pre-training data. Up to time step. $t$ the trajectory of the max-player is recorded as $^{\\star}a_{1},r_{1},\\cdot\\cdot\\cdot\\,,a_{t},r_{t}^{\\,^{\\,^{\\,*}}}$ , and that of the min-player as $^{\\bullet\\ast}b_{1},1-$ $r_{1},\\cdot\\cdot\\cdot\\;,b_{t},1-r_{t}^{\\;\\;^{\\,}}$ ", "page_idx": 40}, {"type": "text", "text": "\u00b7 The VI-ULCB algorithm for the centralized setting. In the centralized setting, both players jointly follow the VI-ULCB algorithm [8] to collect pre-training data. Up to time step $t$ the trajectory is recorded as $^{*}s_{1},a_{1},b_{1},r_{1},1-r_{1},\\cdot\\cdot\\cdot\\,,s_{t},a_{t},b_{t},r_{t},1-r_{t}^{\\,,}$ ", "page_idx": 40}, {"type": "text", "text": "We note that the decimal digits of the rewards are limited to two to facilitate tokenization, while $1-r_{i}$ insteadof $-r_{i}$ is adopted for the min-player to avoid the additional complexity of negative numbers. ", "page_idx": 40}, {"type": "text", "text": "J.3 Transformer Structure and Training ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "The transformer architecture employed in our experiments is primarily based on the well-known GPT-2 model [43], and our implementation follows the miniGPT realization? for simplicity. The numbers of transformer layers and attention heads have been modified to make the entire transformer much smaller. In particular, we utilize a transformer with 2 layers and 4 heads. Given that the transformer is used to compute the policy, we modify the output layer of the transformer to make it aligned with the action dimension. We focus solely on the last output from this layer to determine the action according to the computed transformer policy. ", "page_idx": 40}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "For the training procedure, we use one Nvidia 6000 Ada to train the transformer with a batch size of 32, trained for 100 epochs, and we set the learning rate as $5\\times10^{-4}$ . The experimental codes are available at https: //github . com/ShenGroup/ICGP. ", "page_idx": 41}, {"type": "text", "text": "J.4 Performance Measurement ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "To test the performance of the pre-trained transformer (in particular, how well it approximates EXP3 and VI-ULCB), we adopt the measurement of Nash equilibrium gap. In particular, for either the transformer induced policy or EXP3, we denote the max-player's policy at time step $t$ as $\\mu^{t}$ , and the min-player's policy at time step $t$ as $\\nu^{t}$ . Furthermore, the average policy is computed as ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\bar{\\mu}^{t}=\\frac{1}{t}\\sum_{\\tau\\in[t]}\\mu^{t},\\qquad\\bar{\\nu}^{t}=\\frac{1}{t}\\sum_{\\tau\\in[t]}\\nu^{t}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "The NE gap at step $t$ is computed as ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{a\\in\\mathcal{A}}R\\bar{\\nu}^{t}-\\operatorname*{min}_{b\\in\\mathcal{B}}(\\bar{\\mu}^{t})^{\\top}R.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "For VI-ULCB, the process is similar except that $\\mu^{t}$ and $\\nu^{t}$ are taken as the marginalized policies of the joint policy learned at step $t$ while the NE gap is cumulated over one episode. The NE gaps averaged over 10 randomly realized games at each step are plotted in Fig. 2. ", "page_idx": 41}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: This work investigates the in-context game-playing (ICGP) capabilities of pretrained transformers, where both the decentralized and centralized settings are considered. The abstract and introduction (i.e., Sec. 1) accurately reflect the considered problem and the obtained main results, highlighting our contributions in providing a comprehensive understanding of ICGP. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 42}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: The major limitations of this work in our mind are illustrated in Appendix B.2. In particular, it is emphasized that this work mainly focuses on the two-player zero-sum Markov games and two specific game-solving algorithms. Also, the construction of the pre-trained dataset and the need of further large-scale experiments are discussed. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should refect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 42}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Justification: The main paper (particularly, Sec. 3.1.1 and the beginning of Sec. 4.1) provides the main setups and assumptions considered in this work. Additional Assumptions C.2, F.2, and G.2 are deferred to the appendix due to the page limits while being noted in the main paper with corresponding pointers. The complete proofs are provided in the appendix (see Appendix A for an overview), which have been carefully checked multiple times and are correct to the best of our knowledge. A proof sketch is included in Sec. 4.3 to facilitate the readers'understanding. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 43}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Justification: A detailed description of the experimental setups and details is provided in Appendix J. The complete set of codes for the experiments are available at https : //github. com/ShenGroup/ICGP. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b)If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n() If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct thedataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 43}, {"type": "text", "text": "", "page_idx": 44}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: The complete set of codes can be found at https://github.com/ ShenGroup/ICGP. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 44}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Justification: A detailed description of the experimental setups and details is provided in Appendix J. The experimental codes are available at https : //github . com/ShenGroup/ ICGP. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 44}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: Error bars have been provided in Fig. 2, which is the main experimental results in thiswork. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 45}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: The computer resources used to perform the experiments reported in this work are described in Appendix J. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 45}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: All the authors have reviewed the NeurIPs Code of Ethics and practiced it during this submission. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 45}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Justification: Discussions on the broader impacts of this work are provided in Appendix B.1.   \nDue to the theoretical focus of this work, we do not foresee major negative social impacts. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 46}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 46}, {"type": "text", "text": "Justification: The transformers trained in this work are only for the purpose of validating the theoretical claims, which we do not foresee risks of misuse. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to acces the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 46}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: We have included the URL of the adopted transformer model in Appendix J, which is under a standard MIT license. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 47}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: No new assets have been released with this work. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetis used.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 47}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: No crowdsourcing and research with human subjects has been performed during this work. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 47}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 47}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 48}, {"type": "text", "text": "Justification: No crowdsourcing and research with human subjects has been performed during this work. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 48}]