[{"figure_path": "juJl2uSq4D/tables/tables_22_1.jpg", "caption": "Table 1: The first step generates one of four possible beliefs of a history. Measured elements in the table denote the expected rewards of actions at t = 2 following a behavioral policy. We can see that for all m \u2208 [M], a \u2208 A it holds that Pm(s2, a2) > 0 for all m \u2208 [M], a2 \u2208 A; yet, we cannot estimate E[r2] given some histories.", "description": "This table illustrates a counter-example where single latent-state coverability is insufficient for off-policy evaluation guarantees in LMDPs.  It shows four possible initial histories (combinations of action a1 and reward r1) that could result from one of three possible latent contexts (m). For each history, the table shows the expected reward (E[r2]) for actions a2=1 and a2=2 at the next time step. The table demonstrates that even when all state-action pairs are covered under all contexts, it is still impossible to estimate the expected reward E[r2] from some histories, thus highlighting the necessity of using multi-step coverability in the analysis.", "section": "D.5 A Counter Example for Remark 4.3"}]