[{"type": "text", "text": "RL in Latent MDPs is Tractable: Online Guarantees via Off-Policy Evaluation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jeongyeol Kwon University of Wisconsin-Madison jeongyeol.kwon@wisc.edu ", "page_idx": 0}, {"type": "text", "text": "Shie Mannor Technion / NVIDIA AI shie@ee.technion.ac.il ", "page_idx": 0}, {"type": "text", "text": "Constantine Caramanis University of Texas at Austin constantine@utexas.edu ", "page_idx": 0}, {"type": "text", "text": "Yonathan Efroni Meta AI jonathan.efroni@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In many real-world decision problems there is partially observed, hidden or latent information that remains fixed throughout an interaction. Such decision problems can be modeled as Latent Markov Decision Processes (LMDPs), where a latent variable is selected at the beginning of an interaction and is not disclosed to the agent. In the last decade, there has been significant progress in solving LMDPs under different structural assumptions. However, for general LMDPs, even in the tabular case, no algorithm is known to provably match the existing lower bound [41]. We introduce the first sample-efifcient algorithm for LMDPs without any additional distributional assumptions. Our result builds of fa new perspective on the role of of-f policy evaluation guarantees and coverage coefifcients in LMDPs, a perspective, that has been overlooked in the context of exploration in partially observed environments. Specifically, we establish a novel of-fpolicy evaluation lemma and introduce a new coverage coefifcient for LMDPs. Then, we show how these can be used to derive near-optimal guarantees of an optimistic exploration algorithm. These results, we believe, can be valuable for a wide range of interactive learning problems beyond LMDPs, and especially, for partially observed environments. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In Reinforcement Learning (RL) [54], an agent aims to maximize the long-term cumulative rewards through interactions within an unknown environment. Markov Decision Processes (MDPs) are perhaps the most well-studied and popular framework for this goal. As the name suggests, MDPs heavily rely on the Markovian assumption that requires the state to be fully observable. However, many real-world decision problems involve critical partially observed or latent information, such as sensitive or unknown preference information of users in recommendation systems [28], undiagnosed illness in medical treatments [62, 53], and adaptation to uninformed tasks in robotics [67, 48]. Even when such latent factors remain fixed throughout a period of interactions the fundamental Markovian property of MDPs is no longer valid. ", "page_idx": 0}, {"type": "text", "text": "A line of work has proposed efifcient RL algorithms in the presence of latent contexts [12, 24, 11, 21, 41, 37] within the framework that we here collectively refer to as Latent Markov Decision Processes (LMDP) following [41]. In LMDPs, nature selects an MDP from a finite set of $M$ candidate MDP models at the beginning of a period of interactions (a.k.a. episode), and an agent interacts with the chosen MDP for $H$ time steps of an episode (the horizon). However, the identity of the chosen MDP is not given to the agent. We call this unknown identity the latent context. ", "page_idx": 0}, {"type": "text", "text": "Most prior work on LMDPs has relied on strict separation assumptions (e.g., [11, 24, 37]). The applicability of these approaches is limited to scenarios where the horizon is sufifciently large and identification of the latent model can be guaranteed, i.e., $H\\gg\\Omega(S A)$ [24], where $S$ and $A$ are the state and action spaces size. Without these explicit horizon requirements, as far we know, all existing algorithms suffer the curse of horizon, requiring sample complexity $\\Omega(A^{H})$ \u2013 which frequently arises in the more general framework of Partially Observed MDPs (POMDPs) [52, 39]. Without the ability to identify the underlying latent model, it remains unclear how to address the curse of horizon inherent in partially observed systems [39]. ", "page_idx": 1}, {"type": "text", "text": "Recently, a series of works [40, 42, 43] proposed sample efifcient algorithms without separation assumptions when $M=O(1)$ , assuming the transition dynamics of models with different latent context is similar. While this is still a substantial contribution, their results cannot be easily extended to the general LMDP setting with different transition dynamics (see Section 1.1). Consequently, to date, the following question has remained open: ", "page_idx": 1}, {"type": "text", "text": "Can we break the curse of horizon in LMDPs if $M=O(1)$ without any assumptions? ", "page_idx": 1}, {"type": "text", "text": "In this work we provide the first sample-efifcient exploration algorithm for LMDPs without any assumptions. Throughout the paper, we assume that $H>2M$ , and focus on whether we can improve the trivial upper bound that incurs complexity $\\Omega(A^{H})$ . Since a $\\Omega(S A)^{M}$ lower bound for LMDPs has been established [41], our goal is to achieve an upper bound of $\\mathrm{poly}({\\cal S},A)^{M}$ without any assumptions, namely, to get a matching upper bound up to polynomial factors. ", "page_idx": 1}, {"type": "text", "text": "1.1 Technical Challenges ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Many online RL algorithms follow a similar pattern. They make use of a confidence set \u2013 a set of candidate models (hypothesis) that can explain the observed data with high probability \u2013 and execute a policy that will shrink the volume of the confidence set is produced and executed [6, 38, 27, 7, 33, 36]. The entirety of the statistical problem is to analyze the decaying rate of confidence sets under proper model class assumptions [49, 34, 19]. ", "page_idx": 1}, {"type": "text", "text": "Challenge 1: Limitation of Existing POMDP Algorithms. Existing approaches for online exploration in partially observed systems largely fall into the category of Optimistic Maximum Likelihood Estimation (OMLE) [45, 46]. This class of algorithms often requires an assumption that allows the construction of shrinking confidence sets. These algorithms also assume access to a set of special policies \u2013 called core-tests \u2013 to be executed to generate trajectories [5, 8, 16, 17, 45, 57, 13, 22, 46, 26]. Without specifying the proper core-tests, the volume of confidence sets may not decay in a desired rate, leading to the curse of horizon $\\Omega(A^{H})$ [39, 15]. Further, existing POMDP approaches require an ability to recover the belief of the underlying model from observations, e.g., by assuming the distribution of observations when executing the core-tests is invertible to the belief over hidden states. Consequently, existing literature on POMDPs has two limitations: (i) it requires to specify a priori a set of core-tests policies, and (ii) it assumes the full-rankness of the state-observation emission matrix when the core-tests are being executed. ", "page_idx": 1}, {"type": "text", "text": "While LMDPs are a special class of POMDPs, neither the existence of a set of core-tests is known a priori, nor it is possible to recover the belief over latent contexts from distribution of trajectories (see Section B for details). This creates a fundamental challenge for existing approaches when applied to LMDPs. Further, little is understood on learning a near-optimal policy among \u201cdoubly-exponential\u201d number of candidate history-dependent policies without either the visibility of contexts or core-tests. This calls for a new perspective on the question of efifcient exploration in LMDPs. ", "page_idx": 1}, {"type": "text", "text": "Challenge 2: Limitation of Existing LMDP Algorithms. The work of [43] suggested an alternative strategy to learn a near-optimal policy in LMDPs: the moment-matching approach for exploration in LMDPs. When all contexts share the same state-transition dynamics, the notion of moments can be defined as the joint distribution of rewards under a fixed prior at a tuple of at most $d:=2M-1$ state-action pairs $\\dot{\\pmb{x}}=\\left((s_{[1]},a_{[1]}),...,(s_{[d]},a_{[d]})\\right)$ . This in turn suggests that the exploration algorithm must learn how to visit these length- $d$ state-action tuples simultaneously, i.e., find a policy that ensures that $\\textbf{\\em x}$ appears as a subsequence of the entire trajectory with high enough probability. ", "page_idx": 1}, {"type": "text", "text": "When the transition dynamics of different latent contexts is similar, reaching optimally to $d$ state-action pairs is a minor challenge; e.g., we can first learn the shared transition kernel with any reward-free exploration scheme for MDPs [33], and then execute the policy that maximizes the probability of reaching the $d$ state-action pairs. However, for general LMDP, when the transition dynamics of different latent contexts may differ, this approach is no longer available since the latent transition dynamics may not be learnable in general. Furthermore, to follow the notion of moments suggested in [43], the data collected for estimating the correlation tensor must be collected under the same prior (belief) over all latent contexts. Unfortunately, ensuring this for general LMDPs, when the transition dynamics of different latent contexts are not equal, is impossible, since even if we obtain the samples of correlations, different policies may result in different and unknown priors over contexts. These challenges hint we need an alternative approach to solve general LMDPs, when the transition dynamics vary between latent contexts. ", "page_idx": 1}, {"type": "image", "img_path": "juJl2uSq4D/tmp/7d947d65e829024fd75f2aab01c949a3446f276a1c5b7b486b48edf8bd56a07d.jpg", "img_caption": ["Figure 1: Highlevel description of LMDP-OMLE. In the online phase, we find a new test policy under which models in the confidence set do not agree. Then the exploration policy is constructed with our new notion of segmentation of policies within $\\Psi_{\\tt t e s t}$ that are executed throughout. In the oflifne phase, we add the batched sample trajectories to dataset and update the confidence set of models. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Challenge 3: Limitations of Existing Complexity Measures in RL. Numerous studies have examined complexity measures for RL with function approximation or in the rich-observation settings [30, 34, 19]. These studies are based on the Markovian assumption, which does not hold in the LMDP setting where the entire history may be needed to decode the latent state. When defining the effective state as the entire history at each time step, it is unclear how to analyze the complexity measures from these studies without resorting to exponential guarantees in the horizon. ", "page_idx": 2}, {"type": "text", "text": "1.2 Overview of Our Contribution ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Recent studies have found some fundamental connections between off-policy evaluation (OPE) and online exploration in RL [59, 2, 29, 9, 55, 3, 4]. In this work, we offer a fresh viewpoint, which deviates from existing works, on the connection between OPE and online exploration. This perspective, together with new analysis tools, allows us to provide a sample-efifcient algorithm for the LMDP setting. This further showcases the usefulness of OPE for online exploration in POMDPs. ", "page_idx": 2}, {"type": "text", "text": "Arguably, the fundamental question in OPE is the following: how much does a behavioral policy $\\psi$ tell about a target policy $\\pi?$ The simplest form of the OPE guarantee in MDPs relies on the notion of coverage coefficient given by: ", "page_idx": 2}, {"type": "equation", "text": "$$\nC(\\psi;\\pi)=\\operatorname*{max}_{s,a,t}\\frac{\\mathbb{P}^{\\pi}(s_{t}=s,a_{t}=a)}{\\mathbb{P}^{\\psi}(s_{t}=s,a_{t}=a)}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "How would this quantity be related to online exploration? A key observation to start developing intuition is the following: an unbounded coverage coefifcient, i.e., $C(\\psi;\\pi)=\\infty$ implies there exists a state-action pair, at some time-steps, that cannot be reached under $\\psi$ , but can be reached with $\\pi$ . ", "page_idx": 2}, {"type": "text", "text": "The algorithmic framework we develop in this work builds of fOMLE [46]. In Section 3, we consider the MDP setting to provide intuition of our analysis. There, OMLE iteratively tests new policies on models from the confidence set which predict different outcomes, until the trajectory distribution of all policies is reliably estimated. Since the number of new state-action pairs is bounded for MDPs, the number of times the coverage coefifcient can be large must be bounded during an interaction. We provide new analysis for the MDP setting based on OPE tools. ", "page_idx": 2}, {"type": "text", "text": "To apply this approach for LMDPs, we are required to develop a new notion of coverage coefifcient and new OPE tools. We propose a coverage coefifcient that can be informally described as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\nC(\\psi;\\pi)=\\operatorname*{max}_{(\\mathcal{E},\\mathbb{Z})}\\operatorname*{max}_{m}\\frac{\\mathbb{P}^{\\pi}(\\mathcal{T}\\in\\mathcal{E}\\mid m)}{\\mathbb{P}^{\\psi}(\\mathcal{T}\\in\\mathcal{E}\\mid m,\\ \\mathbf{do}\\,\\mathcal{T})}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $m$ is the unobserved latent context, $\\mathcal{T}=(s_{t},a_{t},r_{t})_{t\\in[H]}$ is a sampled trajectory, $\\mathcal{E}$ is an event of interest, e.g., visiting length of at most $d$ tuples of states and actions within an episode, and $\\mathcal{T}$ is an intervention of interest, e.g., force an action $a$ at the $t^{t h}$ time step regardless of $\\psi$ (for the formal definition, see Definition 4.1). Note that the coverage coefifcient cannot be measured explicitly, since $m$ is a latent variable; nevertheless, this concept is central to our analysis and our ability to analyze the sample complexity of the proposed algorithm. Its usefulness lies in an OPE guarantee we develop (see Lemma 4.2): ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Upsilon\\nabla\\big(\\mathbb{P}_{\\theta^{*}}^{\\pi},\\mathbb{P}_{\\theta}^{\\pi}\\big)(\\mathcal{T})\\lesssim C(\\psi;\\pi)\\cdot\\sum_{\\mathcal{Z}}\\Upsilon\\nabla\\big(\\mathbb{P}_{\\theta^{*}}^{\\psi},\\mathbb{P}_{\\theta}^{\\psi}\\big)(\\mathcal{T}\\mid\\mathbf{do}\\mathcal{Z}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathrm{TV}(\\mathbb{P}_{1},\\mathbb{P}_{2})(\\cdot)$ is the total-variation (TV) distance between two probability measures $\\mathbb{P}_{1},\\mathbb{P}_{2}$ . ", "page_idx": 3}, {"type": "text", "text": "With these tools at hand, we design an iterative online exploration algorithm for the LMDP setting, and prove its sample complexity matches the lower bound, up to polynomial factors. The algorithm, we refer as LMDP-OMLE (see Figure 1 for highlevel illustration), repeats the following: $(i)$ find a policy for which the trajectory distributions between models in the confidence set is large or terminate, or $(i i)$ collect new data with exploration policies constructed with a set of (obtained) test policies and interventions, an exploration strategy for LMDPs that we introduce. ", "page_idx": 3}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We consider an episodic RL with time-horizon $H$ in LMDPs defined as follows: ", "page_idx": 3}, {"type": "text", "text": "Definition 2.1 (Latent Markov Decision Process (LMDP)) An LMDP $\\mathcal{M}$ consists of a tuple $(S,{\\mathcal{A}},{\\mathcal{R}},\\theta,H)$ with a state space $\\boldsymbol{S}$ ; action space $\\boldsymbol{\\mathcal{A}}$ ; reward space $\\mathcal{R}$ , and a finite-time horizon $H$ . $\\theta$ is a model parameter consisting of multiple MDPs in the model $\\boldsymbol{\\theta}:=(\\{w_{m},T_{m},R_{m}\\})_{m=1}^{M}$ . $I n$ each $m^{t h}$ MDP, $T_{m}:S\\times A\\times S\\to[0,1]$ maps a state-action pair and a next state to a probability; $R_{m}:S\\times A\\times\\mathcal{R}\\to[0,1]$ is a probability of rewards; $\\lbrace w_{m}\\rbrace_{m=1}^{M^{\\mathrm{~\\large~\\cdot~}}}$ are the mixing weights such that at the beginning of every episode the $m^{t h}$ model is chosen with probability $w_{m}$ . ", "page_idx": 3}, {"type": "text", "text": "Without loss of generality, we assume that there exists a null state that represents the starting and terminal state $s_{0}=s_{H+1}=\\emptyset$ , and a null action at the beginning of an episode $a_{0}=\\emptyset$ , even though actual policies do not take any action at the beginning. $T_{m}(\\cdot|s_{0},a_{0})$ is the initial state distribution of the $m^{t h}$ MDP. We assume that the number of latent contexts is constant $M=O(1)$ , and the time-horizon is larger than the number of contexts $H>2M$ . Further, we assume the reward values are finite and bounded: ", "page_idx": 3}, {"type": "text", "text": "Assumption 2.2 (Finite and Bounded Reward) The reward distribution has finite support with (arbitrarily large) cardinality, and each reward is bounded: $|r|\\leq1$ for all $r\\in\\mathcal{R}$ . ", "page_idx": 3}, {"type": "text", "text": "We also note that this concept can be easily generalized to instantaneous observations that include rewards, and thus, we do not lose much generality due to Assumption 2.2. We consider a policy class $\\Pi$ which contains all history-dependent policies $\\pi:\\Xi\\times(S,{\\mathcal{A}},{\\mathcal{R}})^{*}\\times(S\\times[H])\\rightarrow\\Delta(A)$ , where $\\Xi$ is the space of independent variables decided at the beginning of execution. As a special case, we consider the class of memoryless policies: $\\Pi_{\\mathrm{mls}}:(S\\bar{\\times}[H])\\bar{\\to}\\Delta(A)$ We are interested in finding an optimal history dependent policy $\\pi\\in\\Pi$ that maximizes the expected reward: ${V}_{\\theta^{\\ast}}^{\\ast}:=$ $\\begin{array}{r}{\\operatorname*{max}_{\\pi\\in\\Pi}\\mathbb{E}_{\\theta^{*}}^{\\pi}\\left[\\sum_{t=1}^{H}r_{t}\\right]}\\end{array}$ , where $\\theta^{*}\\in\\Theta$ is the true model parameter and $\\mathbb{E}_{\\theta^{*}}^{\\pi}[\\cdot]$ is expectation taken over the true LMDP model $\\mathcal{M}^{\\ast}$ when policy $\\pi$ is executed. ", "page_idx": 3}, {"type": "text", "text": "Notation We use $[n]:=\\{1,\\ldots,n\\}$ and $[n]_{+}:=\\{0\\}\\cup[n]$ . We define $d:=2M-1$ and assume $H\\,>\\,2M$ . Let $\\mathtt{S u b S e q}(H,d)$ be the set of subsequences of $(1,2,...,H)$ with length less than or equal to $d$ , i.e., SubSeq $\\lfloor(\\dot{H},d):=\\{(\\tau_{1},\\tau_{2},...,\\tau_{q})|q\\in[d],1\\leq\\tau_{1}<...<\\tau_{q}\\leq H\\}$ . We often denote a state-action pair $(s,a)$ as one symbol $x=(s,a)\\in\\mathcal{X}=(S\\times\\mathcal{A})$ , and an reward-next state pair $(\\boldsymbol r,\\boldsymbol s^{\\prime})$ as one symbol $y^{'}\\!=(r,s^{\\prime})\\,\\'\\in\\mathcal{V}=(\\mathcal{R}\\times\\mathcal{S})$ . We often express the next state at time step $t$ as either $s_{t+1}$ or $s_{t}^{\\prime}$ , and the pair of instantaneous observation and next state as $y_{t}=(r_{t},s_{t+1})=(\\bar{r_{t}},s_{t}^{\\prime})$ . For any segment of a sequence $\\left(z_{1},z_{2},...,z_{H}\\right)$ from $t_{1}$ to $t_{2}$ , we often simplify the notation as $z_{t_{1}:t_{2}}$ . We denote the entire trajectory as $\\tau:=(s,a,r)_{1:H}$ , and $T_{1:t}\\,=\\,((s,a,r)_{1:t-1},s_{t})$ for a history of length $t$ . For any set $\\boldsymbol{S}$ , we define $S^{\\otimes k}$ as a short-hand for the $k$ -times Cartesian power of $\\boldsymbol{S}$ . ", "page_idx": 3}, {"type": "text", "text": "1: Input: $n_{\\tt t e s t}\\in\\mathbb{N},\\beta,\\epsilon_{\\mathrm{TV}},\\eta>0,\\mathcal{C}^{0}=\\Theta$   \n2: Initialize $k=0$   \n3: while there exists $\\pi^{k}\\in\\Pi_{\\mathrm{mls}}$ , and $\\theta_{1},\\theta_{2}\\in\\mathcal{C}^{k}$ such that TV $\\left(\\mathbb{P}_{\\theta_{1}}^{\\pi^{k}},\\mathbb{P}_{\\theta_{2}}^{\\pi^{k}}\\right)(T)>4\\epsilon_{\\mathtt{T V}}\\,\\mathbf{do}$   \n4: Generate data $\\{T_{j}^{k}\\}_{j=1}^{n_{\\mathrm{test}}}$ by executing $\\pi^{k}$ , update $\\mathcal{D}^{k}\\leftarrow\\mathcal{D}^{k-1}\\cup\\{(\\mathcal{T}_{j}^{k},\\pi^{k})\\}_{j=1}^{n_{\\mathrm{test}}}$   \n5: Refine the confidence set with the dataset: $\\begin{array}{r}{\\mathcal{C}^{k+1}=\\Big\\{\\theta\\in\\Theta\\Big\\vert\\sum_{(\\mathcal{T},\\pi)\\in\\mathcal{D}^{k}}\\log\\mathbb{P}_{\\theta}^{\\pi}(\\mathcal{T})\\geq\\arg\\operatorname*{max}_{\\theta\\in\\Theta}\\sum_{(\\mathcal{T},\\pi)\\in\\mathcal{D}^{k}}\\log\\mathbb{P}_{\\theta}^{\\pi}(\\mathcal{T})-\\beta\\Big\\}}\\end{array}$ ( $k\\gets k+1$   \n6: end while   \n7: Pick any $\\theta\\in{\\mathcal{C}}^{k}$ and return the optimal policy of $\\mathcal{M}:=(S,\\mathcal{A},\\mathcal{O},\\theta)$ . ", "page_idx": 4}, {"type": "text", "text": "We define $\\mathtt{S u b T r a j(7,7)}\\subseteq(\\mathcal{X}\\times\\mathcal{y})\\otimes|\\tau|$ as a valid subsequence of trajectories at time-steps $\\tau\\in\\mathsf{S u b S e q}(H,d)$ , i.e., if $(x_{\\tau},y_{\\tau})\\in\\mathtt{S u b T r a j}(T,\\tau)$ , for any $i$ such that $\\tau_{i}=\\tau_{i+1}$ , $y_{\\tau_{i}}=(r_{\\tau_{i}},s_{\\tau_{i}}^{\\prime})$ and $x_{\\tau_{i+1}}=(s_{\\tau_{i+1}},a_{\\tau_{i+1}})$ must have $s_{\\tau_{i}}^{\\prime}=s_{\\tau_{i+1}}$ . ", "page_idx": 4}, {"type": "text", "text": "For a tuple of state-action pairs (or states) of length $q$ , we denote $\\pmb{x}~=~(x_{[1]},...,x_{[q]})$ (or $\\textbf{\\textit{s}}=$ $(s_{[1]},...,s_{[q]})$ with bracketed indices for each element to distinguish from time steps. We use $|x|$ for the length of sequence $\\textbf{\\em x}$ . We denote the cardinality of the state and action space as $S:=|S|$ and $A:={\\bar{|}}A|$ . For any two models $\\theta_{1},\\theta_{2}$ , we often denote $\\mathbb{P}_{1}(\\cdot):=\\mathbb{P}_{\\theta_{1}}(\\cdot)$ and $\\mathbb{P}_{2}(\\cdot):=\\mathbb{P}_{\\theta_{2}}(\\cdot)$ whenever the context is clear. We denote $P_{m}(\\cdot)$ for a probability measured conditioned on the context $m\\in[M]$ over the ground-truth model $\\theta_{1}$ when we compare $\\theta_{1}$ and $\\theta_{2}$ ). We denote Unif $(\\mathcal{A})$ as the uniform distribution over a set $\\boldsymbol{\\mathcal{A}}$ . Let $\\mathrm{TV}(\\mathbb{P}_{1},\\mathbb{P}_{2})(X)$ be the total-variation distance between two probability measures $\\mathbb{P}_{1}(\\cdot),\\mathbb{P}_{2}(\\cdot)$ over a random variable $X$ . ", "page_idx": 4}, {"type": "text", "text": "3 New Perspective on OMLE: Online Guarantees via Off-Policy Evaluation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we present our new approach for analyzing the OMLE algorithm, and, for establishing intuition in the Markovian setting. Differently than prior analysis [45, 46] which is based on the generalized eluder-type condition assumption (see [46], Condition 3.2), we show that a certain type of an OPE guarantee can be used to study the performance of OMLE. This alternative perspective is instrumental in designing a sample-efifcient algorithm for the LMDP class. ", "page_idx": 4}, {"type": "text", "text": "Consider MDP-OMLE depicted in Algorithm 1. MDP-OMLE is an adaptation of OMLE for the MDP setting with the goal of learning a near-optimal policy. The algorithm iteratively refines the confidence set, i.e., the set of statistically valid models, until it terminates. Specifically, it iteratively repeats the two steps: (i) find a policy for which the TV distance between trajectory distributions of models in the confidence set is sufifciently large, and (ii) collect data with that policy, and use the data to refine the confidence set. To bound the sample complexity of the algorithm we attempt to upper bound the number of iterations, namely, to bound the number of times the TV distance between trajectory distributions can be sufifciently large. ", "page_idx": 4}, {"type": "text", "text": "The following OPE lemma is a tool that allows us to bound the number of iterations of MDP-OMLE.   \nBefore discussing its application, we present the result. ", "page_idx": 4}, {"type": "text", "text": "Lemma 3.1 (TV Bound via OPE for MDPs) For any behavioral and target policies $\\psi,\\pi\\in\\Pi_{i}$ , let the coverage coefficient be defined by: ", "page_idx": 4}, {"type": "equation", "text": "$$\nC(\\psi;\\pi)=\\operatorname*{max}_{t\\in[H]}\\operatorname*{max}_{x\\in\\mathcal{X}}\\frac{\\mathbb{P}_{\\theta^{*}}^{\\pi}(x_{t}=x)}{\\mathbb{P}_{\\theta^{*}}^{\\psi}(x_{t}=x)}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For any two models $\\theta,\\theta^{*}\\in\\Theta$ , the TV distance between trajectory distributions following a target policy $\\pi\\in\\Pi$ is bounded as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T V(\\mathbb{P}_{\\theta^{*}}^{\\pi},\\mathbb{P}_{\\theta}^{\\pi})(\\mathcal{T})\\leq2C(\\psi;\\pi)\\sum_{t\\in[H]}T V(\\mathbb{P}_{\\theta^{*}}^{\\psi},\\mathbb{P}_{\\theta}^{\\psi})(x_{t},y_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "How can we use this result to bound the number of iterations of MDP-OMLE? Consider the infinite sample regime, when MDP-OMLE collects infinite data at each iteration by executing a policy $\\pi^{k}$ on the $k$ th iteration, i.e., $n_{\\tt t e s t}=\\infty$ . Further, assume the algorithm is at the beginning of its $k+1$ iteration. In the infinite sample regime all models in the confidence set must have matching event distribution relatively to the underlying model measured when policy $\\pi^{k}$ is tested. Specifically, for all $\\theta\\in{\\mathcal{C}}^{k}$ and $t\\in[H]$ it holds that $\\mathrm{TV}(\\mathbb{P}_{\\theta^{*}}^{\\pi^{k}},\\mathbb{P}_{\\theta}^{\\pi^{k}})(x_{t},y_{t})=0$ . Then Lemma 3.1 implies the following: for all policies $\\pi$ for which $C(\\pi^{k};\\pi)<\\infty$ it also holds that $\\mathrm{TV}(\\mathbb{P}_{\\theta^{*}}^{\\pi},\\mathbb{P}_{\\theta}^{\\pi})({\\mathcal T})=0$ . Conversely, assume the condition of the while loop at the beginning of the $k+1$ iteration holds true, namely, there exists a policy $\\bar{\\pi}$ for which $\\mathrm{TV}(\\mathbb{P}_{\\theta^{*}}^{\\bar{\\pi}},\\mathbb{P}_{\\theta}^{\\bar{\\pi}})(\\mathcal{T})\\stackrel{}{>}0$ . Then Lemma 3.1 also implies that ${\\cal C}(\\pi^{k};\\bar{\\pi})=\\infty$ , namely, there exists an $x\\in\\mathscr{X}$ and $t\\in[H]$ such that $\\mathbb{P}_{\\theta^{*}}^{\\bar{\\pi}}(x_{t}=x)>0$ whereas $\\mathbb{P}_{\\theta^{*}}^{\\pi^{k}}(x_{t}=x)=0$ . Next, recall that MDP-OMLE sets the data collection policy at the $k+1$ iteration to be $\\pi^{k+1}=\\bar{\\pi}$ . Hence, the data collection policy at the $k+1$ iteration will visit some state-action pair at some time step $\\pi^{k}$ did not visit. Hence, in the infinite sample regime, MDP-OMLE halts after at most $H S A$ iterations, as there are at most $H S A$ different state-action pairs in different time steps. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "The intuition presented above is robust to sampling error, $i.e$ , when $n_{\\tt t e s t}<\\infty$ . To simplify the discussion, let us temporarily assume that $\\mathbb{P}_{\\theta^{*}}^{\\pi}(x_{t}=x)>\\gamma$ for all $\\pi\\in\\Pi$ and $x\\in{\\mathcal{S}}\\times{\\mathcal{A}}$ (we do not require this assumption in our final result by analyzing a perturbed MDP). The key intuition on which the finite sample analysis builds upon is formalized in the following lemma: ", "page_idx": 5}, {"type": "text", "text": "Lemma 3.2 (Coverage Multiplicative Increase) For all $k>0$ in Algorithm $^{\\,l}$ , there exists at least one $t\\in[H]$ and $x\\in\\mathscr{X}$ such that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\theta^{*}}^{\\pi^{k}}(x_{t}=x)\\geq c\\cdot\\frac{\\epsilon_{T V}}{H}\\sqrt{\\frac{n_{t e s t}}{(H S A)\\beta}}\\cdot\\operatorname*{max}_{j<k}\\mathbb{P}_{\\theta^{*}}^{\\pi^{j}}(x_{t}=x).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with some absolute constant $c>0$ . ", "page_idx": 5}, {"type": "text", "text": "Therefore, by setting the number of samples to be $n_{\\sf t e s t}\\,\\geq\\,(4H^{2}S A\\beta)/(c\\epsilon_{\\mathrm{TV}})^{2}$ , we ensure that in every iteration MDP-OMLE doubles the coverage of at least one state-action pair at a certain time step. Therefore, the algorithm terminates within at most $K=O(H S A\\cdot\\log(1/\\gamma))$ iterations with high probability . After termination, we are guaranteed that any two models in the confidence set are $\\epsilon_{\\mathrm{TV}}$ -close in TV-distance for any policy, hence we can obtain $\\epsilon\\,=\\,(H\\epsilon_{\\mathrm{{TV}}})$ -optimal policy. To summarize, we state the following theorem: ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.3 Let $K=O(H S A)\\log(H S A/\\epsilon)$ and $\\beta=\\log(K|\\Theta|/\\eta)$ . Then, with probability at least, $1-\\delta$ , MDP-OMLE terminates after $K$ iterations with at most $N$ episodes being generated, where ", "page_idx": 5}, {"type": "equation", "text": "$$\nN\\geq O(H^{6}S^{2}A^{2})\\cdot\\log(H S A/\\epsilon)\\log(K|\\Theta|/\\eta)/\\epsilon^{2},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and outputs an $\\epsilon$ -optimal policy with probability at least $1-\\eta$ . ", "page_idx": 5}, {"type": "text", "text": "In a typical tabular MDP setting, we take $O(\\log|\\Theta|)=\\tilde{O}(S A)$ , by discretizing the class of MDPs. Hence the sample complexity of MDP-OMLE is $N\\,=\\,\\tilde{O}(H^{6}S^{3}A^{3}/\\epsilon^{2})$ . While this upper bound is suboptimal compared to the minimax rate [7], the appeal of this type of analysis is its ability to bypass the need for analyzing the decaying volume of the constructed confidence sets (Section 1.1, Challenge 1). ", "page_idx": 5}, {"type": "text", "text": "4 Efifcient Exploration in LMDPs ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In previous section we presented a new approach to analyze the OMLE algorithm for MDPs. Next, we develop an analogous technique for the LMDP setting and design the LMDP-OMLE algorithm. Central to its design and analysis is an OPE lemma and a new coverage coefifcient which we now present. ", "page_idx": 5}, {"type": "text", "text": "Intuition from moment-exploration algorithm in [43]. Before we dive into our key results, let us provide our intuition on how we construct the OPE lemma for LMDPs. Our construction is inspired by the moment-exploration algorithm proposed in [43]: when state-transition dynamics are identical across latent contexts, i.e., $T_{1}\\,=\\,T_{2}\\,=\\,...\\,=\\,T_{M}$ , we can first learn the transition dynamics with any reward-free type exploration scheme for MDPs [33], and then set the exploration policy that sufifciently visits some tuples of state-actions $\\textbf{\\em x}$ of length at most $d$ . Specifically, they set a memorlyess exploration policy $\\psi\\in\\Pi_{\\mathrm{m1s}}$ which sets $\\mathbb{P}^{\\psi}(x_{\\tau}=\\pmb{x})$ sufifciently large for some $\\dot{\\tau}\\in\\mathsf{S u b S e q}(\\dot{H},d)$ ", "page_idx": 5}, {"type": "text", "text": "1: Input: $K,d,n_{\\tt t e s t}~\\in~\\mathbb{N}$ , $\\epsilon_{\\tt t e s t},\\eta~>~0$ , $\\beta~=~\\log(K|\\Theta|/\\eta)$ , $\\Psi_{\\bf t e s t}^{0}~=~\\{{\\tt U n i f}(A)\\}$ , $\\mathcal{D}^{0}\\ =$   \n{(Tj, Unif(A))}jnt=es1t where each $\\mathcal{T}_{j}$ is generated by executing Unif $(\\mathcal{A})$ , $\\mathcal{C}^{1}$ as defined in (1)   \n2: Initialize $k=1$   \n3: while there exists $\\pi^{k}\\in\\Pi_{\\mathrm{mls}}$ , and $\\theta_{1},\\theta_{2}\\in\\mathcal{C}^{k}$ such that TV $\\left(\\mathbb{P}_{\\theta_{1}}^{\\pi^{k}},\\mathbb{P}_{\\theta_{2}}^{\\pi^{k}}\\right)(T)>4\\epsilon_{\\tt t e s t}\\;{\\bf d o}$   \n4: $\\Psi_{\\mathrm{test}}^{k}\\leftarrow\\Psi_{\\mathrm{test}}^{k-1}\\cup\\{\\pi^{k}\\}$ , initialize $\\mathcal{D}^{k}=\\mathcal{D}^{k-1}$   \n5: for all $\\psi=(\\psi_{0},\\dot{\\psi_{1}},...,\\psi_{d})\\in\\Psi_{\\mathrm{test}}^{k}\\times...\\times\\Psi_{\\mathrm{test}}^{k}\\times\\{\\mathrm{Unif}\\,({\\mathcal{A}})\\}$ with at least one $i\\in[d-1]_{+}$   \nbeing $\\psi_{i}=\\pi^{k}$ , and $\\tau\\in\\mathsf{S u b S e q}(H,d)$ , $z\\in\\{0,1\\}^{|\\tau|}$ do   \n6: Generate data $\\{\\mathcal{T}_{j}\\}_{j=1}^{n_{\\mathrm{test}}}$ by executing $\\nu(\\psi;\\tau,z)$   \n7: Update $\\mathcal{D}^{k}\\leftarrow\\mathcal{D}^{k}\\cup\\{(\\mathcal{T}_{j},\\nu(\\psi;\\tau,z)\\}_{j=1}^{n_{\\mathrm{test}}}$   \n8: end for   \n9: Update the confidence set $\\mathcal{C}^{k+1}$ according to equation (1)   \n10: $k\\bar{\\leftarrow}\\,k+1$   \n11: end while   \n12: Pick any $\\theta\\in{\\mathcal{C}}^{k}$ and return the optimal policy of $\\mathcal{M}:=(\\mathcal{S},\\mathcal{A},\\mathcal{R},\\theta)$ . ", "page_idx": 6}, {"type": "text", "text": "and $\\pmb{x}\\in\\mathcal{X}^{\\otimes|\\tau|}$ . We note that the same moment-exploration strategy cannot be applied to general LMDPs with different state-transition dynamics since learning the transition dynamics itself involves latent contexts. Nevertheless, the intuition from [43] suggests that our key statistics are this visitation probabilities to all tuples of state-actions within a trajectory. ", "page_idx": 6}, {"type": "text", "text": "4.1 Off-Policy Evaluation in LMDPs ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The OPE lemma we derive in this section makes use of a behavior policy of a special form which we refer to as a segmented policy, inspired by the notion of moment-exploration in [43]. Let us formally define the key quantities to establish our OPE lemma. A segmented policy, which we denote by $\\nu(\\psi;\\tau,z)$ , takes as an input a sequence of history-dependent policies, $\\pmb{\\psi}=(\\psi_{0},...,\\psi_{d})$ , a sequence of time steps, we call checkpoints, $\\pmb{\\tau}\\,=\\,(\\tau_{1},...,\\tau_{|\\tau|})\\,\\in\\,\\mathtt{S u b S e q}(H,d)$ , and a sequence of binary numbers $\\pmb{z}=(z_{1},...,z_{|\\pmb{\\tau}|})\\in\\{0,1\\}^{|\\pmb{\\tau}|}$ where $|\\tau|\\leq d$ , and returns a history-dependent policy. ", "page_idx": 6}, {"type": "text", "text": "The segmented policy $\\nu(\\psi;\\tau,z)$ switches sequentially between different policies in $\\psi$ . The time steps in which the switch occurs are determined by $\\tau$ : starting from time step $\\tau_{i}+1$ policy $\\psi_{i}$ will be executed. Finally, the sequence $_{\\textit{z}}$ determines whether an intervention with a random action will occur at the $\\tau_{i}$ time-step. If $z_{i}=1$ the executed action at time step $\\tau_{i}$ is the uniform action, Unif $(\\mathcal{A})$ , and, otherwise, the policy $\\psi_{i-1}$ is executed. The segmented policy is also denoted by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\nu(\\psi;\\tau,z):=\\psi_{0}\\underset{(\\tau_{1},z_{1})}{\\circ}\\psi_{1}\\underset{(\\tau_{2},z_{2})}{\\circ}\\cdots\\underset{(\\tau_{|\\tau|},z_{|\\tau|})}{\\circ}\\psi_{|\\tau|},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\stackrel{\\ast\\cdot}{\\pi}_{a}\\tiny{\\Pi}_{(t,z)}^{\\mathrm{~o~}}\\pi_{b}^{\\mathrm{~,~}\\rangle}$ means switch to policy $\\pi_{b}$ at starting from time step $t+1$ , and at time step $t$ take random action if $z=1$ and otherwise execute $\\pi_{a}$ . ", "page_idx": 6}, {"type": "text", "text": "We are now ready to define a coverage coefifcient for the LMDP class of models. This new coverage coefifcient is central to the analysis and design of LMDP-OMLE. ", "page_idx": 6}, {"type": "text", "text": "Definition 4.1 (LMDP Coverage Coefifcient) The LMDP coverage coefficient of a sequence of policies $\\pmb{\\psi}\\in\\Pi^{\\otimes(d+1)}$ with respect to a target policy $\\pi\\in\\Pi$ in is given by: ", "page_idx": 6}, {"type": "equation", "text": "$$\nC(\\psi;\\pi):=\\operatorname*{max}_{\\substack{\\tau\\in s u b s e_{q}(H,d)\\,z\\in\\{0,1\\}\\,\\otimes\\,|\\tau|\\,(x,y)\\in s u b\\,T r a j(T,\\tau)\\,m\\in[M]}}\\frac{P_{m}^{\\pi}(x_{\\tau}=x,y_{\\tau}=y)}{P_{m}^{\\nu(\\psi;\\tau,z)}(x_{\\tau}=x,y_{\\tau}=y)}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The LMDP coverage coefifcient $C(\\psi;\\pi)$ between a sequence of policies, $\\psi$ , and a target, historydependent, policy $\\pi$ , depends on the worst-case way to generate a segmented policy, $\\nu(\\psi;\\tau,z)$ from $\\psi$ . Further, it is a worst-case ratio of the probability of a sequence of observations within $|\\tau|=d$ different time steps, namely, $x_{\\tau},y_{\\tau}$ . This is different than the standard coverage coefifcient (see equation (2)), that depends on observation from a single time. Fortunately, $C(\\psi;\\pi)$ requires only a partial set of observations, instead of using full trajectories. This is crucial towards developing sample complexity guarantees that are not exponential in $H$ . Lastly, observe that the LMDP coverage coefifcient depends on the latent context $m$ , and thus, we cannot measure $C(\\psi;\\pi)$ from samples. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "We are now ready to provide the key OPE lemma, which makes use of the LMDP coverage coefifcient. ", "page_idx": 7}, {"type": "text", "text": "Lemma 4.2 (TV Bound via OPE for LMDPs) Let $d=2M-1$ . For any two models $\\theta,\\theta^{*}\\in\\Theta$ , and for any $\\pi\\in\\Pi$ and $\\pmb{\\psi}\\in\\Pi^{\\otimes(d+1)}$ , let $C(\\psi;\\pi)$ be defined as (4) over $\\theta^{*}$ . Then the following holds: ", "page_idx": 7}, {"type": "equation", "text": "$$\nT V(\\mathbb{P}_{\\theta^{*}}^{\\pi},\\mathbb{P}_{\\theta}^{\\pi})(\\mathcal{T})\\leq M\\cdot C(\\psi;\\pi)\\sum_{\\substack{\\tau\\in S u b s e\\mathfrak{q}(H,d)\\,z\\in\\{0,1\\}\\otimes\\,|\\tau|}}T V\\left(\\mathbb{P}_{\\theta^{*}}^{\\nu(\\psi;\\tau,z)},\\mathbb{P}_{\\theta}^{\\nu(\\psi;\\tau,z)}\\right)(x_{\\tau},y_{\\tau}).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "This result is analgous to the OPE result for MDPs (see Lemma 3.1). It is a tool that allows us to bound the TV distance between trajectory distributions of a history-dependent policy $\\pi$ by a term that depends on a segmented policy $\\nu(\\psi;\\tau,z)$ and an LMDP coverage coefifcient. Importantly, the term on the RHS that depends on the segmented policy, $\\nu(\\psi;\\tau,z)$ , is a sum of distributions of partial trajectories of size $|\\tau|\\leq d$ , which is independent of the horizon length, $H$ . ", "page_idx": 7}, {"type": "text", "text": "Remark 4.3 (Why is single latent-state coverability coefifcient not enough?) One may wonder why it is not sufficient to consider a single latent-state coverability analogous to Lemma 3.1, namely an analogous to (2) defined as: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{t\\in[H]}\\operatorname*{max}_{x\\in\\mathcal{X}}\\operatorname*{max}_{m\\in[M]}\\frac{P_{m}^{\\pi}(x_{t}=x)}{P_{m}^{\\psi}(x_{t}=x)}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "In Appendix D.5 we provide a counter-example where such single latent-state coverage coefficient is finite, and yet, off-policy evaluation guarantee cannot be established. ", "page_idx": 7}, {"type": "text", "text": "4.2 Coverage Doubling via Sufifciency of Memoryless Polices ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To convert the OPE guarantee to an online exploration algorithm, we aim to use the coverage-doubling argument. Ideally, we could apply the coverage-doubling argument with the general policy class similarly to the MDP case as presented in Section 3. However, in its current form, Lemma 4.2 requires the behavior policy to be a segmented policy, and is not valid for any general behavioral policy. Hence, it is not obvious on which probabilistic events we can apply the coverage doubling argument. We leave it as future work whether we can obtain an of-fpolicy evaluation lemma with general history-dependent behavioral policies, and its clearer conversion to online guarantees. ", "page_idx": 7}, {"type": "text", "text": "In this work, we present an alternative plan to the above issue: we reduce the search space from history-dependent policies to memoryless policies. This allows us to track quantities on a segmentwise coverage. Specifically, we first note that the LMDP coverage coefifcient can be bounded (after maximizing over the sequence $_{\\textit{z}}$ ) by: ", "page_idx": 7}, {"type": "equation", "text": "$$\nC(\\psi;\\pi)\\leq\\operatorname*{max}_{\\tau\\in\\mathrm{Subseq}(H,d)}\\operatorname*{max}_{x\\in\\mathcal{X}^{\\otimes\\mid\\tau\\mid}}\\operatorname*{max}_{m\\in[M]}\\prod_{i=0}^{d-1}\\frac{\\operatorname*{max}_{T_{1:\\tau_{i}}}P_{m}^{\\pi}(s_{\\tau_{i+1}}=s_{[i+1]}|s_{\\tau_{i}}^{\\prime}=s_{[i]}^{\\prime},T_{1:\\tau_{i}})}{(1/A)\\cdot P_{m}^{\\nu(\\psi_{i};\\tau_{i})}(s_{\\tau_{i+1}}=s_{[i+1]}|s_{\\tau_{i}}^{\\prime}=s_{[i]}^{\\prime})},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\nu(\\psi_{i};\\tau_{i})$ denotes a segmented policy executing $\\psi_{i}$ after the $\\tau_{i}^{t h}$ time-step with memory reset, hence ignoring the history up to $\\tau_{i}$ (the conditioning event $s_{\\tau_{0}}^{\\prime}=s_{[0]}^{\\prime}$ s[\u20320] at i = 0 can be ignored). Inspired by the form in denominator, we aim to double the following probability defined over a context-segment pair: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\psi\\in\\Psi_{\\mathrm{test}}}P_{m}^{\\nu(\\psi;t_{1})}\\big(s_{t_{2}}=s|s_{t_{1}}^{\\prime}=s^{\\prime}\\big),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "for at least one $m\\in[M],s,s^{\\prime}\\in{\\cal S}$ and $t_{1}<t_{2}$ . However, another challenge remains: the RHS in equation (5) consists of the maximum over all possible histories in the numerator, whereas in the denominator we force the data collection policy to reset the memory at checkpoints. We still have to side-step this discrepancy to apply the coverage doubling argument. ", "page_idx": 7}, {"type": "text", "text": "The restriction to the class of memoryless policies allows us to resolve these issues since ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{T_{1:t_{1}}}P_{m}^{\\pi}\\big(s_{t_{2}}=s|s_{t_{1}}^{\\prime}=s^{\\prime},\\mathcal{T}_{1:t_{1}}\\big)=P_{m}^{\\pi}\\big(s_{t_{2}}=s|s_{t_{1}}^{\\prime}=s^{\\prime}\\big),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "equation", "text": "$P_{m}^{\\nu(\\psi;t_{1})}(s_{t_{2}}=s|s_{t_{1}}^{\\prime}=s^{\\prime})=P_{m}^{\\psi}(s_{t_{2}}=s|s_{t_{1}}^{\\prime}=s^{\\prime}),$ ", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "if $\\pi,\\psi\\in\\Pi_{\\mathrm{mls}}$ since $P_{m}$ represents the latent Markovian transition dynamics. Thus, we can aim to double up the quantity in equation (6). To apply this argument, we establish our second key lemma, a crucial building block for the coverage doubling argument: ", "page_idx": 8}, {"type": "text", "text": "Lemma 4.4 (Sufifciency of Memoryless Polices for LMDPs) Suppose the following holds: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pi_{\\mathrm{mls}}\\in\\Pi_{\\mathrm{mls}}}T V(\\mathbb{P}_{\\theta^{*}}^{\\pi_{\\mathrm{mls}}},\\mathbb{P}_{\\theta}^{\\pi_{\\mathrm{mls}}})(\\mathcal{T})\\leq\\epsilon_{t e s t}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Then for any history-dependent policies $\\pi\\in\\Pi$ , the following holds: ", "page_idx": 8}, {"type": "equation", "text": "$$\nT V(\\mathbb{P}_{1}^{\\pi},\\mathbb{P}_{2}^{\\pi})(\\mathcal{T})\\le M(2H^{2})^{d}\\cdot(M S A)^{d}\\cdot\\epsilon_{t e s t}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Therefore, we reduced our goal of learning an optimal policy to finding a set of models which satisfies equation (7) with respect to all memoryless policies. Importantly, upon estimating the trajectory distribution up to accuracy $\\epsilon_{\\tt t e s t}\\,>\\,0$ for memoryless policies, we have a bounded TV distance between the trajectory distribution of all history-dependent policies, includes the optimal policy. ", "page_idx": 8}, {"type": "text", "text": "4.3 The LMDP-OMLE Algorithm ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Once the search space is reduced to memoryless policies, we aim to match trajectory distributions for all memoryless policies. At a high-level, LMDP-OMLE follows similar recipe to MDP-OMLE, and is summarized in Algorithm 2. It can be described as follows: ", "page_idx": 8}, {"type": "text", "text": "1. Find a memoryless policy $\\pi^{k}\\in\\Pi_{\\mathrm{mls}}$ whose prediction on trajectory distributions does not match between two models in the confidence set $\\mathcal{C}^{k}$ . Add $\\pi^{k}$ to the collection of test policies $\\Psi_{\\mathtt{t e s t}}^{k}$ , that forms each segment of (segmented) exploration policies. 2. Collect new sample trajectories following the new set of segmented policies for exploration, generated by different combinations of collected test policies and switching operations. 3. Update the confidence set $\\mathcal{C}^{k}$ with Maximum Likelihood Estimation (MLE) on the updated dataset $\\mathcal{D}^{k}$ by equation (1). ", "page_idx": 8}, {"type": "text", "text": "The data collection policy of LMDP-OMLE (second step above) is inspired and leverages Lemma 4.2 to give upper bounds on the TV distance of untested policies. Next, by Lemma 4.4, we know that when the while loop terminates, any optimal policy of a model contained in the confidence set is a near-optimal policy of the underlying LMDP. We conclude this section with our main theorem on the sample complexity of learning the optimal policy in Latent MDPs: ", "page_idx": 8}, {"type": "text", "text": "Theorem 4.5 (Sample Complexity of LMDP-OMLE) Let $d=2M-1$ and assume $H>2M$ . After at most $K=O(M\\bar{S}^{2}H)\\cdot\\mathrm{log}(M\\bar{S}A H/\\epsilon)$ iterations, LMDP-OMLE (Algorithm 2) terminates with at most $N$ episodes being generated where ", "page_idx": 8}, {"type": "equation", "text": "$$\nN\\gtrsim\\big(M^{4}S^{6}A^{4}H^{7}\\cdot\\log(M S A H/\\epsilon)\\big)^{d}\\cdot M^{4}H^{2}\\cdot\\log(K|\\Theta|/\\eta)/\\epsilon^{2},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "and outputs an \u03f5-optimal policy with probability at least $1-\\eta$ . ", "page_idx": 8}, {"type": "text", "text": "Note that in tabular LMDPs with finite support rewards, we have $\\log(|\\Theta|)=O(S^{2}A|\\mathcal{R}|\\log(1/\\epsilon))$ . The appeal of $\\log(|\\Theta|)$ dependence is an flexible extension of the same result to parameterized reward distributions. In Section D.1 and D.4 we provide a proof overview and a full proof of Theorem 4.5. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion and Future Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this work, we presented the first sample-efifcient algorithm for LMDPs, resolving an open question of efifcient exploration with latent contexts. While our result is specialized to LMDPs, we believe our new perspectives and techniques on deriving online guarantees through the lens of OPE can be useful for a broader range of interactive learning, and, especially, partially observed problems. While resolving the open problem, there are a few remaining questions for the LMDP setting. ", "page_idx": 8}, {"type": "text", "text": "Tightness of the Result. The upper bound in Theorem 4.5 scales with $\\tilde{O}\\left(M S A H\\cdot\\log(1/\\epsilon)\\right)^{O(M)}$ , while the existing lower bound is $\\bar{\\Omega}(S A)^{M}$ . Closing this polynomial gap in the exponent, and having a matching upper and lower bounds can be valuable for a deeper understanding of LMDPs and possibly for POMDPs in general. ", "page_idx": 9}, {"type": "text", "text": "General OPE lemma and Regret Guarantees for LMDPs. The OPE lemma derived in this work (Lemma 4.2) assumes the behavior policy is a segmented policy with intervention at different checkpoints. While this result allows us to provide guarantees on LMDP-OMLE and prove it learns a near-optimal policy, this result is restrictive, in that it does not provide general guarantees for OPE nor makes it possible to derive regret guarantees. In particular, can we evaluate $\\pi\\,\\in\\,\\Pi$ without policy-switching or intervention when the behavioral policy is a ge\u221aneric history-dependent policy $\\psi\\in\\Pi?$ Further, is there an algorithm with provable poly $^{\\prime}(S,A)^{M}\\cdot{\\sqrt{T}}$ regret for the general LMDP setting? ", "page_idx": 9}, {"type": "text", "text": "Towards Practical Settings. Our result gives a worst-case guarantee. Yet, practical instances may be much simpler under different set of assumptions e.g., with provided side-information [66, 44] or additional structural assumptions [41, 63, 14]. Deriving new conditions can be of great importance for real-world applications, e.g., there could be more practical notion of separation, or the set of instances that allows the notion of coverage-coefifcient with a (significantly) shorter length $d=o(M)$ of state-action tuples. Further, developing practical RL methodologies for the LMDP setting remains an unexplored challenge with significant importance for numerous applications. These are remained to be explored in future works. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgment ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research was partially funded by AFOSR/AFRL grant no. FA9550-18-1-0166, NSF Grants 2019844 and 2112471, and Israel Science Foundation Grant No. 2199/20. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] A. Agarwal, S. Kakade, A. Krishnamurthy, and W. Sun. FLAMBE: Structural complexity and representation learning of low rank MDPs. Advances in neural information processing systems, 33:20095\u201320107, 2020.   \n[2] A. Al-Marjani, A. Tirinzoni, and E. Kaufmann. Active coverage for PAC reinforcement learning. In The Thirty Sixth Annual Conference on Learning Theory, pages 5044\u20135109. PMLR, 2023.   \n[3] P. Amortila, D. J. Foster, N. Jiang, A. Sekhari, and T. Xie. Harnessing density ratios for online reinforcement learning. In The Twelfth International Conference on Learning Representations, 2023.   \n[4] P. Amortila, D. J. Foster, and A. Krishnamurthy. Scalable online exploration via coverability. arXiv preprint arXiv:2403.06571, 2024.   \n[5] A. Anandkumar, R. Ge, D. Hsu, S. M. Kakade, and M. Telgarsky. Tensor decompositions for learning latent variable models. The Journal of Machine Learning Research, 15(1):2773\u20132832, 2014.   \n[6] P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire. The nonstochastic multiarmed bandit problem. SIAM journal on computing, 32(1):48\u201377, 2002.   \n[7] M. G. Azar, I. Osband, and R. Munos. Minimax regret bounds for reinforcement learning. In International Conference on Machine Learning, pages 263\u2013272. PMLR, 2017.   \n[8] K. Azizzadenesheli, A. Lazaric, and A. Anandkumar. Reinforcement learning of POMDPs using spectral methods. In Conference on Learning Theory, pages 193\u2013256, 2016.   \n[9] P. J. Ball, L. Smith, I. Kostrikov, and S. Levine. Efifcient online reinforcement learning with ofilfne data. In International Conference on Machine Learning, pages 1577\u20131594. PMLR, 2023.   \n[10] A. Bennett and N. Kallus. Proximal reinforcement learning: Efifcient off-policy evaluation in partially observed markov decision processes. Operations Research, 2023.   \n[11] E. Brunskill and L. Li. Sample complexity of multi-task reinforcement learning. In Uncertainty in Artificial Intelligence, page 122. Citeseer, 2013.   \n[12] I. Chad\u00e8s, J. Carwardine, T. Martin, S. Nicol, R. Sabbadin, and O. Buffet. MOMDPs: a solution for modelling adaptive management problems. In Twenty-Sixth AAAI Conference on Artificial Intelligence (AAAI-12), 2012.   \n[13] F. Chen, Y. Bai, and S. Mei. Partially observable RL with B-stability: Unified structural condition and sharp sample-efifcient algorithms. In The Eleventh International Conference on Learning Representations, 2022.   \n[14] F. Chen, C. Daskalakis, N. Golowich, and A. Rakhlin. Near-optimal learning and planning in separated latent MDPs. arXiv preprint arXiv:2406.07920, 2024.   \n[15] F. Chen, H. Wang, C. Xiong, S. Mei, and Y. Bai. Lower bounds for learning in revealing POMDPs. In International Conference on Machine Learning, pages 5104\u20135161. PMLR, 2023.   \n[16] C. Dann, N. Jiang, A. Krishnamurthy, A. Agarwal, J. Langford, and R. E. Schapire. On oracleefifcient PAC RL with rich observations. In Advances in neural information processing systems, pages 1422\u20131432, 2018.   \n[17] Y. Efroni, C. Jin, A. Krishnamurthy, and S. Miryoosef.i Provable reinforcement learning with a short-term memory. In International Conference on Machine Learning, pages 5832\u20135850. PMLR, 2022.   \n[18] Y. Efroni, D. Misra, A. Krishnamurthy, A. Agarwal, and J. Langford. Provably flitering exogenous distractors using multistep inverse dynamics. In International Conference on Learning Representations, 2021.   \n[19] D. J. Foster, S. M. Kakade, J. Qian, and A. Rakhlin. The statistical complexity of interactive decision making. arXiv preprint arXiv:2112.13487, 2021.   \n[20] S. A. Geer. Empirical Processes in M-estimation, volume 6. Cambridge university press, 2000.   \n[21] C. Gentile, S. Li, P. Kar, A. Karatzoglou, G. Zappella, and E. Etrue. On context-dependent clustering of bandits. In International Conference on Machine Learning, pages 1253\u20131262. PMLR, 2017.   \n[22] N. Golowich, A. Moitra, and D. Rohatgi. Learning in observable POMDPs, without computationally intractable oracles. Advances in neural information processing systems, 35:1458\u20131473, 2022.   \n[23] Z. D. Guo, S. Doroudi, and E. Brunskill. A PAC RL algorithm for episodic POMDPs. In Artificial Intelligence and Statistics, pages 510\u2013518, 2016.   \n[24] A. Hallak, D. Di Castro, and S. Mannor. Contextual markov decision processes. arXiv preprint arXiv:1502.02259, 2015.   \n[25] D. Hsu, S. M. Kakade, and T. Zhang. A spectral algorithm for learning hidden markov models. Journal of Computer and System Sciences, 78(5):1460\u20131480, 2012.   \n[26] R. Huang, Y. Liang, and J. Yang. Provably efifcient UCB-type algorithms for learning predictive state representations. In The Twelfth International Conference on Learning Representations, 2023.   \n[27] T. Jaksch, R. Ortner, and P. Auer. Near-optimal regret bounds for reinforcement learning. Journal of Machine Learning Research, 11:1563\u20131600, 2010.   \n[28] A. J. Jeckmans, M. Beye, Z. Erkin, P. Hartel, R. L. Lagendijk, and Q. Tang. Privacy in recommender systems. Social media retrieval, pages 263\u2013281, 2013.   \n[29] Z. Jia, G. Li, A. Rakhlin, A. Sekhari, and N. Srebro. When is agnostic reinforcement learning statistically tractable? Advances in Neural Information Processing Systems, 36, 2023.   \n[30] N. Jiang, A. Krishnamurthy, A. Agarwal, J. Langford, and R. E. Schapire. Contextual decision processes with low bellman rank are PAC-learnable. In International Conference on Machine Learning, pages 1704\u20131713. PMLR, 2017.   \n[31] N. Jiang and L. Li. Doubly robust off-policy value evaluation for reinforcement learning. In International conference on machine learning, pages 652\u2013661. PMLR, 2016.   \n[32] C. Jin, S. M. Kakade, A. Krishnamurthy, and Q. Liu. Sample-efifcient reinforcement learning of undercomplete POMDPs. arXiv preprint arXiv:2006.12484, 2020.   \n[33] C. Jin, A. Krishnamurthy, M. Simchowitz, and T. Yu. Reward-free exploration for reinforcement learning. In International Conference on Machine Learning, pages 4870\u20134879. PMLR, 2020.   \n[34] C. Jin, Q. Liu, and S. Miryoosef.i Bellman eluder dimension: New rich classes of RL problems, and sample-efifcient algorithms. Advances in neural information processing systems, 34:13406\u2013 13418, 2021.   \n[35] S. Kakade and J. Langford. Approximately optimal approximate reinforcement learning. In Proceedings of the Nineteenth International Conference on Machine Learning, pages 267\u2013274, 2002.   \n[36] E. Kaufmann, P. M\u00e9nard, O. D. Domingues, A. Jonsson, E. Leurent, and M. Valko. Adaptive reward-free exploration. In Algorithmic Learning Theory, pages 865\u2013891. PMLR, 2021.   \n[37] C. Kausik, K. Tan, and A. Tewari. Learning mixtures of markov chains and MDPs. In International Conference on Machine Learning, pages 15970\u201316017. PMLR, 2023.   \n[38] M. Kearns and S. Singh. Near-optimal reinforcement learning in polynomial time. Machine learning, 49:209\u2013232, 2002.   \n[39] A. Krishnamurthy, A. Agarwal, and J. Langford. PAC reinforcement learning with rich observations. In Advances in Neural Information Processing Systems, pages 1840\u20131848, 2016.   \n[40] J. Kwon, Y. Efroni, C. Caramanis, and S. Mannor. Reinforcement learning in reward-mixing MDPs. Advances in Neural Information Processing Systems, 34, 2021.   \n[41] J. Kwon, Y. Efroni, C. Caramanis, and S. Mannor. RL for latent MDPs: Regret guarantees and a lower bound. Advances in Neural Information Processing Systems, 34, 2021.   \n[42] J. Kwon, Y. Efroni, C. Caramanis, and S. Mannor. Tractable optimality in episodic latent MABs. Advances in Neural Information Processing Systems, 35:23634\u201323645, 2022.   \n[43] J. Kwon, Y. Efroni, C. Caramanis, and S. Mannor. Reward-mixing MDPs with few latent contexts are learnable. In International Conference on Machine Learning, pages 18057\u201318082. PMLR, 2023.   \n[44] J. Kwon, Y. Efroni, S. Mannor, and C. Caramanis. Prospective side information for latent MDPs. arXiv preprint arXiv:2310.07596, 2023.   \n[45] Q. Liu, A. Chung, C. Szepesv\u00e1ri, and C. Jin. When is partially observable reinforcement learning not scary? In Conference on Learning Theory, pages 5175\u20135220. PMLR, 2022.   \n[46] Q. Liu, P. Netrapalli, C. Szepesvari, and C. Jin. Optimistic MLE: A generic model-based algorithm for partially observable sequential decision making. In Proceedings of the 55th Annual ACM Symposium on Theory of Computing, pages 363\u2013376, 2023.   \n[47] H. Namkoong, R. Keramati, S. Yadlowsky, and E. Brunskill. Off-policy policy evaluation for sequential decisions under unobserved confounding. Advances in Neural Information Processing Systems, 33:18819\u201318831, 2020.   \n[48] Z. Rimon, A. Tamar, and G. Adler. Meta reinforcement learning with finite training tasks-a density estimation approach. Advances in Neural Information Processing Systems, 35:13640\u2013 13653, 2022.   \n[49] D. Russo and B. Van Roy. Eluder dimension and the sample complexity of optimistic exploration. Advances in Neural Information Processing Systems, 26, 2013.   \n[50] C. Shi, M. Uehara, J. Huang, and N. Jiang. A minimax learning approach to of-fpolicy evaluation in confounded partially observable markov decision processes. In International Conference on Machine Learning, pages 20057\u201320094. PMLR, 2022.   \n[51] S. Singh, M. R. James, and M. R. Rudary. Predictive state representations: a new theory for modeling dynamical systems. In Proceedings of the 20th conference on Uncertainty in artificial intelligence, pages 512\u2013519, 2004.   \n[52] R. D. Smallwood and E. J. Sondik. The optimal control of partially observable markov processes over a finite horizon. Operations research, 21(5):1071\u20131088, 1973.   \n[53] L. N. Steimle, D. L. Kaufman, and B. T. Denton. Multi-model markov decision processes. Optimization Online URL http://www. optimization-online. org/DB_FILE/2018/01/6434. pdf, 2018.   \n[54] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT press, 2018.   \n[55] K. Tan and Z. Xu. A natural extension to online algorithms for hybrid RL with limited coverage. arXiv preprint arXiv:2403.09701, 2024.   \n[56] G. Tennenholtz, U. Shalit, and S. Mannor. Off-policy evaluation in partially observable environments. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 10276\u201310283, 2020.   \n[57] M. Uehara, A. Sekhari, J. D. Lee, N. Kallus, and W. Sun. Provably efifcient reinforcement learning in partially observable dynamical systems. Advances in Neural Information Processing Systems, 35:578\u2013592, 2022.   \n[58] M. Uehara, C. Shi, and N. Kallus. A review of of-fpolicy evaluation in reinforcement learning. arXiv preprint arXiv:2212.06355, 2022.   \n[59] T. Xie, D. J. Foster, Y. Bai, N. Jiang, and S. M. Kakade. The role of coverage in online reinforcement learning. In The Eleventh International Conference on Learning Representations, 2022.   \n[60] T. Xie, Y. Ma, and Y.-X. Wang. Towards optimal of-fpolicy evaluation for reinforcement learning with marginalized importance sampling. Advances in neural information processing systems, 32, 2019.   \n[61] Y. Xu, J. Zhu, C. Shi, S. Luo, and R. Song. An instrumental variable approach to confounded off-policy evaluation. In International Conference on Machine Learning, pages 38848\u201338880. PMLR, 2023.   \n[62] G. Yauney and P. Shah. Reinforcement learning with action-derived rewards for chemotherapy and clinical trial dosing regimen selection. In Machine Learning for Healthcare Conference, pages 161\u2013226. PMLR, 2018.   \n[63] W. Zhan, M. Uehara, W. Sun, and J. D. Lee. PAC reinforcement learning for predictive state representations. arXiv preprint arXiv:2207.05738, 2022.   \n[64] T. Zhang. From $\\varepsilon$ -entropy to KL-entropy: Analysis of minimum information complexity density estimation. Annals of Statistics, 34(5):2180\u20132210, 2006.   \n[65] Y. Zhang and N. Jiang. On the curses of future and history in future-dependent value functions for off-policy evaluation. arXiv preprint arXiv:2402.14703, 2024.   \n[66] R. Zhou, R. Wang, and S. S. Du. Horizon-free and variance-dependent reinforcement learning for latent markov decision processes. In International Conference on Machine Learning, pages 42698\u201342723. PMLR, 2023.   \n[67] L. Zintgraf, K. Shiarlis, M. Igl, S. Schulze, Y. Gal, K. Hofmann, and S. Whiteson. VariBAD: A very good method for bayes-adaptive deep RL via meta-learning. In International Conference on Learning Representations, 2019. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix A Additional Preliminaries ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The difference between the values of any policy $\\pi\\,\\in\\,\\Pi$ measured on two models $\\theta_{1},\\theta_{2}\\in\\Theta$ are bounded by the total-variation (TV) distance between trajectory distributions, that is, ", "page_idx": 13}, {"type": "equation", "text": "$$\n|V_{1}^{\\pi}-V_{2}^{\\pi}|\\leq H\\cdot\\mathsf{T V}(\\mathbb{P}_{1}^{\\pi},\\mathbb{P}_{2}^{\\pi})({\\mathcal T}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "since the maximum reward that can be obtained in an episode is bounded by $H$ due to Assumption 2.2. Hence, if we can show that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{TV}(\\mathbb{P}_{\\theta^{*}}^{\\pi},\\mathbb{P}_{\\theta}^{\\pi})(\\mathcal{T})\\leq\\epsilon/H=:\\epsilon_{\\mathrm{TV}},\\qquad\\forall\\pi\\in\\Pi,}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "then an optimal policy $\\hat{\\pi}^{*}$ of the empirical model $\\hat{\\theta}$ is guaranteed to be $2\\epsilon$ -optimal in the true model $\\theta^{*}$ . Henceforth, we focus on finding an empirical model $\\hat{\\theta}$ that satisfies (8). ", "page_idx": 13}, {"type": "text", "text": "To bound the TV-distance between trajectory distributions for all history-dependent policies $\\pi\\in\\Pi$ between any two LMDP models, we start by unfolding the expression of statistical distance ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{V}(\\mathbb{P}_{1}^{\\pi},\\mathbb{P}_{2}^{\\pi})(\\mathcal{T})=\\sum_{(x_{t},r_{t})_{t\\in[H]}}|\\mathbb{P}_{1}^{\\pi}((x_{t},r_{t})_{t\\in[H]})-\\mathbb{P}_{2}^{\\pi}((x_{t},r_{t})_{t\\in[H]})|}\\\\ {\\displaystyle=\\sum_{x_{1},H}\\prod_{t=1}^{H}\\pi(a_{t}|T_{1:t})\\times\\left|\\sum_{m=1}^{M}w_{m}^{1}\\prod_{t=0}^{H}T_{m}^{1}(s_{t+1}|x_{t})R_{m}^{1}(r_{t}|x_{t})-\\sum_{m=1}^{M}w_{m}^{2}\\prod_{t=0}^{H}(s_{t+1}|x_{t})R_{m}^{2}(r_{t}|x_{t})\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "When the context is clear, we compare trajectory distributions between any two given model parameters $\\theta_{1},\\theta_{2}\\in\\Theta$ , and denote the probability measure from each model following $\\pi$ as $\\mathbb{P}_{1}^{\\pi}(\\cdot),\\bar{\\mathbb{P}_{2}^{\\pi}}(\\cdot)$ . ", "page_idx": 13}, {"type": "text", "text": "A.1 Additional Notation ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "To reduce the notation overload, we use $P_{m}(y_{t}|x_{t})=T_{m}(s_{t+1}|x_{t})R_{m}(r_{t}|x_{t})$ . When the context is clear, we often use a shorthand $\\pi_{t}=\\pi(a_{t}|\\mathcal{T}_{1:t})$ , and denote $\\pi_{t_{1}:t_{2}}$ as a shorthand of the product of a time-consecutive sequence from $t_{1}$ to $t_{2}$ , i.e., $\\begin{array}{r}{\\pi_{t_{1}:t_{2}}=\\prod_{t=t_{1}}^{t_{2}}\\pi_{t}}\\end{array}$ . When we sum over both $x_{t}$ and $y_{t}$ , we implicitly mean that the $s_{t}^{\\prime}$ part of $y_{t}$ , which we denote as $s^{\\prime}(y_{t})$ , must match to the $s_{t+1}$ part of $x_{t+1}$ , which we denote as $s(x_{t+1})$ . Using the notation, we rewrite the unfolded TV-distance equation as the following: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{x_{1:H}}\\sum_{y_{1:H}}\\pi_{1:H}\\left|\\sum_{m=1}^{M}w_{m}^{1}\\prod_{t=0}^{H}P_{m}^{1}(y_{t}|x_{t})-\\sum_{m=1}^{M}w_{m}^{2}\\prod_{t=0}^{H}P_{m}^{2}(y_{t}|x_{t})\\right|.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We use a shorthand $\\delta_{\\pi}(X)$ for $\\begin{array}{r}{|\\mathbb{P}_{1}^{\\pi}(X)-\\mathbb{P}_{2}^{\\pi}(X)|=|\\sum_{m=1}^{M}w_{m}^{1}P_{m}^{1,\\pi}(X)-\\sum_{m=1}^{M}w_{m}^{2}P_{m}^{2,\\pi}(X)|.}\\end{array}$ and thus $\\begin{array}{r}{\\sum_{X}\\delta_{\\pi}(X)=\\mathrm{T}\\dot{\\mathrm{V}}(\\mathbb{P}_{1}^{\\pi};\\mathbb{P}_{2}^{\\pi})(X)}\\end{array}$ where the summation is over all possible realizations of a random  variable $X$ . Finally, we denote $d=2M-1$ . ", "page_idx": 13}, {"type": "text", "text": "A.2 Preliminaries for Lemma 4.2 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Here we define a few more quantities that will be crucial for the proofs for Section 4. In bounding the total variation distance in terms of tested policies without exponential blow-up in $H$ , the key is to marginalize events across time steps. Let us first fix the checkpoint time-steps $\\pmb{\\tau}=(\\tau_{1},...,\\tau_{q})$ for $q\\in[d]$ , and a sequence of executable policies $\\psi=(\\psi_{0},\\psi_{1},...,\\psi_{d})$ . Each $i^{t h}$ segment policy will be executed within time interval $(\\tau_{i},\\tau_{i+1}]$ for $i\\geq0$ . ", "page_idx": 13}, {"type": "text", "text": "To proceed, for the initial policy $\\psi_{0}$ , let $l^{0}$ be the smallest quantity, among the contexts, of the ratio between the state visitation probabilities in consecutive steps when $\\psi_{0}$ is executed (recall that we denote $\\boldsymbol{x}_{t}=\\left(\\boldsymbol{s}_{t},\\boldsymbol{a}_{t}\\right)$ , $y_{t}=\\left(r_{t},s_{t+1}\\right)$ ): ", "page_idx": 13}, {"type": "equation", "text": "$$\nl^{0}(x_{t},r_{t};s_{t+1}):=\\operatorname*{min}_{n\\in\\{1,2\\}}\\left(\\operatorname*{min}_{m\\in[M_{n}]}\\frac{P_{m}^{n,\\psi_{0}}(s_{t})}{P_{m}^{n,\\psi_{0}}(s_{t+1})}P_{m}^{n}(r_{t},s_{t+1}|x_{t})\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "This quantity can be understood as the minimum (over latent contexts) of the \u201cpseudo-posterior\u201d probabilities of 1-step event given the future state if $\\psi_{0}$ is memoryless, since ", "page_idx": 13}, {"type": "equation", "text": "$$\nl^{0}(x_{t},r_{t};s_{t+1})\\cdot\\psi_{0}(a_{t}|s_{t})=\\operatorname*{min}_{n\\in\\{1,2\\}}\\operatorname*{min}_{m\\in[M]}P_{m}^{\\psi_{0}}(x_{t},r_{t}|s_{t+1}),\\;\\mathrm{if}\\;\\psi_{0}\\in\\Pi_{\\mathrm{mls}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Henceforth we use a shorthand $l_{t}^{0}:=l^{0}(x_{t},r_{t};s_{t+1})$ . We recursively define a sequence of the above quantity. ", "page_idx": 14}, {"type": "text", "text": "Next, we fix the event $(x_{\\tau},y_{\\tau})$ at the event-log time-steps. For all $i\\geq0$ and $\\tau_{i}<t\\leq\\tau_{i+1}$ , we define $l_{t}^{i}(x_{\\tau_{1:i}},y_{\\tau_{1:i}})$ and $p_{m}^{n,i}(x_{\\tau_{1:i}},y_{\\tau_{1:i}})$ recursively as the following: ", "page_idx": 14}, {"type": "equation", "text": "$$\nl_{t}^{i}(x_{\\tau_{1:i}},y_{\\tau_{1:i}}):=\\operatorname*{min}_{n\\in\\{1,2\\}}\\left(\\operatorname*{min}_{m\\in[M]:p_{m}^{n,i}(x_{\\tau_{1:i}},y_{\\tau_{1:i}})>0}\\frac{P_{m}^{n,\\nu(\\psi_{i};\\tau_{i})}(s_{t}|s_{\\tau_{i}+1})P_{m}^{n}(r_{t},s_{t+1}|x_{t})}{P_{m}^{n,\\nu(\\psi_{i};\\tau_{i})}(s_{t+1}|s_{\\tau_{i}+1})}\\right),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p_{m}^{n,i+1}(x_{\\tau_{1:i+1}},y_{\\tau_{1:i+1}})=p_{m}^{n,i}(x_{\\tau_{1:i}},y_{\\tau_{1:i}})\\times\\Big(P_{m}^{n,\\nu(\\psi_{i};\\tau_{i})}(s_{\\tau_{i+1}}|s_{\\tau_{i}+1})P_{m}^{n}(y_{\\tau_{i+1}}|x_{\\tau_{i+1}})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad-P_{m}^{n,\\nu(\\psi_{i};\\tau_{i})}\\left(s^{\\prime}(y_{\\tau_{i+1}})|s_{\\tau_{i}+1}\\right)l_{\\tau_{i+1}+1}^{i}(x_{\\tau_{1:i}},y_{\\tau_{1:i}})\\Big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Here, we define $t_{[0]}\\equiv-1$ , where we recall that $\\nu(\\pi;t)$ means the memory reset of a policy $\\pi$ at time step $t+1$ . $x_{\\tau_{1:0}},y_{\\tau_{1:0}}\\equiv\\phi$ , and $l_{0}^{0}\\equiv1$ and $p_{m}^{n,0}\\equiv w_{m}^{n}$ for $n=1,2$ . The key point here is that in this recursive construction, as $i$ increase, we have at least one $i$ such that either 1,i+1= 0 or p2,i $p_{m}^{2,i+1}=0$ i.e., at least one context is removed from consideration at each checkpoint. ", "page_idx": 14}, {"type": "text", "text": "In the subsequent steps in our proof, we often omit the dependence on $(x_{\\tau_{1:i}},y_{\\tau_{1:i}})$ , as well as $\\pi_{\\tau_{0:i}}$ in $l_{t}^{i}$ and $p_{m}^{n,i+1}$ when the context is clear. Finally, we define ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta(x_{\\tau},y_{\\tau}):=\\left|\\sum_{m=1}^{M}p_{m}^{1,|\\tau|}-\\sum_{m=1}^{M}p_{m}^{2,|\\tau|}\\right|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Now we are ready to state our key intermediate lemma: ", "page_idx": 14}, {"type": "text", "text": "Lemma A.1 For any target policy $\\pi\\in\\Pi$ and a sequence of segment policies $\\psi=(\\psi_{0},\\psi_{1},...,\\psi_{d})$ , the following holds: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{x_{1:H}}\\sum_{y_{1:H}}\\pi_{1:H}\\left|\\sum_{m=1}^{M}w_{m}^{1}\\prod_{t=0}^{H}P_{m}^{1}(y_{t}|x_{t})-\\sum_{m=1}^{M}w_{m}^{2}\\prod_{t=0}^{H}P_{m}^{2}(y_{t}|x_{t})\\right|}\\\\ &{\\le\\displaystyle\\sum_{\\tau\\in S u b s e q(H,d)}\\sum_{x_{\\tau},y_{\\tau}}\\Delta(x_{\\tau},y_{\\tau})\\times\\left(\\frac{P_{m}^{\\pi}(x_{\\tau},y_{\\tau})}{\\prod_{i=0}^{|\\tau|-1}P_{m}^{\\nu(\\psi_{i},\\tau_{i})}(s_{\\tau_{i+1}}|s_{\\tau_{i}+1})P_{m(x_{\\tau},y_{\\tau})}(y_{\\tau_{i+1}}|x_{\\tau_{i+1}})}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $m(x_{\\tau},y_{\\tau})$ is the smallest $m\\in[M]$ such that $p_{m}^{1,|\\tau|}>0$ ", "page_idx": 14}, {"type": "text", "text": "A.3 Preliminaries for Lemma 4.4 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We present additional tools that are useful for proving Lemma 4.4. We first define the notion of (latent) segment coverage coefifcient of the set of test policies $\\Psi_{\\tt t e s t}$ with respect to $\\pi$ as the following: ", "page_idx": 14}, {"type": "text", "text": "Definition A.2 (LMDP Segment Coverage Coefifcient) The coverage of $\\pi\\in\\Pi$ with respect to a set of test policies $\\Psi_{t e s t}\\subseteq\\Pi$ is defined as: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\rho(\\Psi_{t e s t};\\pi):=\\operatorname*{max}_{t_{1}<t_{2}}\\operatorname*{max}_{s,s^{\\prime}}\\operatorname*{max}_{m}\\frac{\\operatorname*{max}_{h:|h|=t_{1}}P_{m}^{\\pi}(s_{t_{2}}=s|s_{t_{1}}^{\\prime}=s^{\\prime},h)}{\\operatorname*{max}_{\\psi\\in\\Psi_{t e s t}}P_{m}^{\\nu(\\psi;t_{1})}(s_{t_{2}}=s|s_{t_{1}}^{\\prime}=s^{\\prime})},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The key lemma is to bound the coverage coefifcient of segmented policies consisting of mixture policies defined as the following: ", "page_idx": 14}, {"type": "text", "text": "Lemma A.3 Let $\\psi_{\\xi}\\in\\Pi$ be a mixture of a subset of behavioral policies for the following set with a fixed $t_{0}\\in[H]$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Psi_{\\xi}=\\left\\{\\arg\\operatorname*{max}_{\\psi\\in\\Psi_{t e s t}}P_{m}^{\\nu(\\psi;t_{0})}\\big(s_{t_{0}+t}=s|s_{t_{0}}^{\\prime}=s^{\\prime}\\big),\\;\\forall m,s,s^{\\prime},t\\right\\}\\subseteq\\Psi_{t e s t}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Let $n=|\\Psi_{\\xi}|$ . We define the mixture policy as $\\begin{array}{r}{\\psi_{\\xi}:=\\frac1n\\sum_{\\psi\\in\\Psi_{\\xi}}\\psi}\\end{array}$ , i.e., given the time interval $l$ until the next checkpoint time, $\\psi_{\\xi}$ first uniformly randomly picks one policy from $\\Psi_{\\xi}$ and executes the picked policy afterwards. Let $\\psi_{\\xi}:=(\\psi_{\\xi},\\psi_{\\xi},...,\\psi_{\\xi})$ (of length $d+1,$ ). Then the following holds: ", "page_idx": 14}, {"type": "equation", "text": "$$\nC(\\psi_{\\xi};\\pi)\\leq(n A\\cdot\\rho(\\Psi_{t e s t};\\pi))^{d}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A.4 Auxiliary Concentration Lemmas ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The following lemmas are the standard concentration of log-likelihood values of the models within the confidence set. The proofs are standard and can also be in e.g., [1, 45, 44] and [20, 64]. We let $\\mathcal{D}^{k}$ be the dataset at the beginning of the $k^{t h}$ iteration in Algorithm 2. We denote $\\beta:=\\log(K|\\Theta|/\\eta)$ . ", "page_idx": 15}, {"type": "text", "text": "Lemma A.4 (Uniform Bound on the Likelihood Ratios) With probability $1-\\eta$ for any $\\eta>0$ , for all $k\\in[K]$ and for any $\\theta\\in\\Theta$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{(\\mathcal T,\\pi)\\in\\mathcal D^{k}}\\log(\\mathbb{P}_{\\theta}^{\\pi}(\\mathcal T))-\\beta\\leq\\sum_{(\\mathcal T,\\pi)\\in\\mathcal D^{k}}\\log(\\mathbb{P}_{\\theta^{*}}^{\\pi}(\\mathcal T)).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Lemma A.5 (Concentration of Maximum Likelihood Estimators) With probability $1-\\eta,$ , for all $k\\in[K],\\,t\\in[H]$ and $\\theta\\in\\Theta$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{\\left(T,\\pi\\right)\\in{\\mathcal{D}}^{k}}T V^{2}\\left(\\mathbb{P}_{\\theta}^{\\pi},\\mathbb{P}_{\\theta^{*}}^{\\pi}\\right)({\\mathcal{T}})\\leq\\sum_{\\left(T,\\pi\\right)\\in{\\mathcal{D}}^{k}}\\log\\left({\\frac{\\mathbb{P}_{\\theta^{*}}^{\\pi}({\\mathcal{T}})}{\\mathbb{P}_{\\theta}^{\\pi}({\\mathcal{T}})}}\\right)+3\\beta.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Appendix B Additional Related Work ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The literature on reinforcement learning theory is fast growing. While learning in fully observable systems has been extensively studied in the past decades, relatively little has been understood for online exploration in partially observable systems until recently. We review recent theoretical advances in reinforcement learning with partial observations that are closely related to us. ", "page_idx": 15}, {"type": "text", "text": "Exploration in POMDPs Learning a near-optimal policy in POMDPs is a notoriously hard problem [52] due to its full generality. In particular, the statistical complexity of online exploration in a general POMDP fundamentally suffers from the curse of horizon [39]. A earlier breakthrough involved introducing structural assumptions on system dynamics, which enable the recovery of underlying POMDPs model under the uniform ergodicity assumption [25, 5, 8, 23]. ", "page_idx": 15}, {"type": "text", "text": "Recent theoretical breakthrough concerns the exploration problem in POMDPs without the ergodicity assumption [32, 45], initiating a remarkable progress in understanding the statistical complexity of reinforcement learning in POMDPs under proper structural assumptions. To this date, well-studied tractable POMDP classes (that overcome the curse of horizon) can be considered largely as a system with a \u201cshort-window\u201d for efifcient exploration [16, 1, 18, 17, 22, 45, 13]. The crux of the shortwindow assumption is the prior knowledge that the a short consecutive execution of purely random actions is enough to obtain sufifcient statistics of histories, i.e., the full-rankness of latent state-future observation emission matrices. A short sequence of uniformly random actions become the set of core tests of the system [51]. In such systems, learning the optimal policy only incurs poly $(S,A)\\cdot A^{L}$ complexity [46] (with window-size $L$ ), breaking the curse of horizon. ", "page_idx": 15}, {"type": "text", "text": "Unfortunately, the same story does not apply in LMDPs, as there is no such \u201cshort-window\u201d assumption that allows us to learn the sufifcient statistics of histories. This calls for a set of new ideas and concepts, which could be of independent interest. ", "page_idx": 15}, {"type": "text", "text": "Of-fPolicy Evaluation in POMDPs Along with the fast progress in online reinforcement learning with partial observations, there is a growing interest in of-fpolicy evaluation with partial observations [47, 56, 61]. While the sample complexity of OPE has been extensively studied in MDPs under various model class assumptions [31, 60, 58], most existing OPE results in POMDPs are asymptotic in nature, and often suffers the curse of horizon due to their use of importance-weight sampling. ", "page_idx": 15}, {"type": "text", "text": "Recently, several recent works have proposed an alternative measure of coverage in the latent space, breaking the curse of horizon [50, 10, 65]. However, their results rely on the weakly-revealing assumptions that is often made in tractable POMDP classes [45], and can only evaluate within the class of memoryless policies. Our results are developed for the of-fpolicy evaluation in LMDPs with several new concepts, which can also be of independent interest to of-fpolicy evaluation problems in partially observed systems. ", "page_idx": 15}, {"type": "text", "text": "B.1 Additional Details on the Full-Rankness Assumption ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We give a more detailed explanation of the full-rankness assumption that has become popular in the POMDP literature [45]. As mentioned, the statistical sufifciency of core-tests, which is represented as the minimum singular value of the \u201clatent state-future trajectory\u201d emission matrix $L$ . Such an assumption has been exploited in the earlier work of LMDPs in [41], where the matrix $L$ is defined in the following form: ", "page_idx": 16}, {"type": "equation", "text": "$$\nL_{s}[m,(\\psi_{\\sf t e s t},\\mathcal{T}_{t:H})]=P_{m}^{\\nu(\\psi_{\\sf t e s t};t)}(\\mathcal{T}_{t:H}|s),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\psi_{\\tt t e s t}\\in\\Psi_{\\tt t e s t}$ is a test policy, and $\\mathcal{T}_{t:H}$ is the future trajectory after time step $t$ . Therefore, direct application of POMDP approaches such as OMLE require the prior knowledge of $\\Psi_{\\tt t e s t}$ and $\\sigma_{\\mathrm{min}}(L_{s})>0$ for all $s\\in S$ . The rationale behind such assumptions is to ensure that a distribution of future trajectories can be converted to a belief of latent contexts, hence a distribution of future observations can serve as an alternative to a belief state. However, we are not given the set of core-tests $\\Psi_{\\tt t e s t}$ , or even the existence of $\\Psi_{\\tt t e s t}$ that ensures $\\sigma_{\\mathrm{min}}(L_{s})>0$ for general Latent MDPs. ", "page_idx": 16}, {"type": "text", "text": "With Separation. In a recent work by Chen et al. [14], a polynomial upper bound in $M$ has been established under a notion of strong-separation between contexts with a sufifciently long time horizon $H$ . In essence, their assumptions guarantee that $\\sigma_{\\mathrm{min}}(L_{s})>0$ holds for most of the states with $a$ priori given test policy, along with additional analysis for the tail of episodes. It is of great importance to identify such practical assumptions that lead to fully polynomial upper bounds, especially for instances with some proper notion of separations even when no prior knowledge of test policies is provided. ", "page_idx": 16}, {"type": "text", "text": "Appendix C Proofs for Section 3 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "C.1 Proof of Lemma 3.1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "This base case corresponds to Lemma 4.2 with $M_{1}=M_{2}=1$ . For convenience, we let $\\theta_{1}=\\theta^{*}$ and $\\theta_{2}=\\theta$ , and thus, $\\mathbb{P}_{1}=\\mathbb{P}_{\\theta^{*}}$ \u2217and $\\mathbb{P}_{2}=\\mathbb{P}_{\\theta}$ . We can show the inequality recursively: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{x_{1:H}}\\sum_{y_{1:H}}\\pi_{1:H}\\left|\\prod_{t=0}^{H}\\mathbb{P}_{1}(y_{t}|x_{t})-\\prod_{t=0}^{H}\\mathbb{P}_{2}(y_{t}|x_{t})\\right|}\\\\ &{\\le\\displaystyle\\sum_{x_{1:H}}\\sum_{y_{1:H-1}}\\pi_{1:H}\\left|\\prod_{t=0}^{H-1}\\mathbb{P}_{1}(y_{t}|x_{t})-\\prod_{t=0}^{H-1}\\mathbb{P}_{2}(y_{t}|x_{t})\\right|\\sum_{y_{H}}\\mathbb{P}_{2}(y_{H}|x_{H})}\\\\ &{\\quad+\\displaystyle\\sum_{x_{1:H}}\\sum_{y_{1:H-1}}\\pi_{1:H}\\prod_{t=0}^{H-1}\\mathbb{P}_{1}(y_{t}|x_{t})\\sum_{y_{H}}|\\mathbb{P}_{1}(y_{H}|x_{H})-\\mathbb{P}_{2}(y_{H}|x_{H})|\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Note that $\\begin{array}{r}{\\sum_{y_{H}}\\mathbb{P}_{2}(y_{H}|x_{H})=1}\\end{array}$ and ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sum_{x_{1:H-1}}\\sum_{y_{1:H-1}}\\pi_{1:H-1}\\prod_{t=0}^{H-1}\\mathbb{P}_{1}(y_{t}|x_{t})=\\mathbb{P}_{1}^{\\pi}(s_{H}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "since we implicitly sum over $s_{H}=s^{\\prime}\\big(y_{H-1}\\big)$ as we described in Appendix A.1. Thus, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{1:H}\\sum_{y_{1}:H}\\pi_{1:H}\\left|\\prod_{t=0}^{H}\\mathbb{P}_{1}(y_{t}|x_{t})-\\prod_{t=0}^{H}\\mathbb{P}_{2}(y_{t}|x_{t})\\right|\\le\\sum_{x_{1}:H}\\sum_{y_{1}:H-1}\\pi_{1:H}\\left|\\prod_{t=0}^{H-1}\\mathbb{P}_{1}(y_{t}|x_{t})-\\prod_{t=0}^{H-1}\\mathbb{P}_{2}(y_{t}|x_{t})\\right|}\\\\ {+\\sum_{x_{H},y_{H}}\\mathbb{P}_{1}^{\\pi}(x_{H})\\left|\\mathbb{P}_{1}(y_{H}|x_{H})-\\mathbb{P}_{2}(y_{H}|x_{H})\\right|.\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then we can show that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{\\tau,y,H}\\mathbb{P}_{1}^{\\pi}(x_{H})\\left|\\mathbb{P}_{1}(y_{H}|x_{H})-\\mathbb{P}_{2}(y_{H}|x_{H})\\right|=\\sum_{x_{H},y_{H}}\\left(\\frac{\\mathbb{P}_{1}^{\\pi}(x_{H})}{\\mathbb{P}_{1}^{\\psi}(x_{H})}\\right)\\mathbb{P}_{1}^{\\psi}(x_{H})\\left|\\mathbb{P}_{1}(y_{H}|x_{H})-\\mathbb{P}_{2}(y_{H}|x_{H})\\right|}}\\\\ &{\\leq C(\\psi;\\pi)\\left(\\sum_{x_{H},y_{H}}|\\mathbb{P}_{1}^{\\psi}(x_{H},y_{H})-\\mathbb{P}_{2}^{\\pi}(x_{H},y_{H})|+\\sum_{x_{H},y_{H}}|\\mathbb{P}_{1}^{\\psi}(x_{H})-\\mathbb{P}_{2}^{\\pi}(x_{H})|\\mathbb{P}_{2}(y_{H}|x_{H})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\leq2C(\\psi;\\pi)\\mathrm{TV}(\\mathbb{P}_{1}^{\\psi},\\mathbb{P}_{2}^{\\psi})(x_{H},y_{H}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Applying the same step inductively from $t=H$ to $t=1$ , we get the lemma. ", "page_idx": 17}, {"type": "text", "text": "C.2 Proof of Lemma 3.2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Let $\\begin{array}{r}{\\Psi_{\\xi}:=\\left\\{\\pi^{j^{*}(x,t)},\\forall x,t\\;|\\;j^{*}(x,t):=\\arg\\operatorname*{max}_{j\\in0,1,\\dots,k-1}\\mathbb{P}^{\\pi^{j}}(x_{t}=x)\\right\\}}\\end{array}$ . Then, let $\\psi_{\\xi}\\in\\Pi$ be a policy that can adapt to the predetermined checkpoint $l$ , such that $\\begin{array}{r}{\\psi_{\\xi}=\\frac{1}{|\\Psi_{\\xi}|}\\sum_{\\psi\\in\\Psi_{\\xi}}\\psi,i.e.}\\end{array}$ , a mixture of policies in $\\Psi_{\\xi}$ . Note that $|\\Psi_{\\xi}|\\leq H S A$ . Lemma 3.1 tells us that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{T V}(\\mathbb{P}_{1}^{\\pi},\\mathbb{P}_{2}^{\\pi})(\\mathcal T)\\leq2\\displaystyle\\sum_{t\\in[H]}C(\\psi_{\\xi};\\pi)\\cdot\\mathsf{T V}(\\mathbb{P}_{1}^{\\psi_{\\xi}},\\mathbb{P}_{2}^{\\psi_{\\xi}})(\\mathcal T)}\\\\ &{\\phantom{\\quad\\quad}\\leq2\\displaystyle\\sum_{t\\in[H]}\\sum_{\\psi\\in\\Psi_{\\xi}}\\frac{C(\\psi_{\\xi};\\pi)}{|\\Psi_{\\xi}|}\\cdot\\mathsf{T V}(\\mathbb{P}_{1}^{\\psi},\\mathbb{P}_{2}^{\\psi})(\\mathcal T)}\\\\ &{\\phantom{\\quad\\quad\\quad}\\leq2H\\cdot\\left(\\frac{C(\\psi_{\\xi};\\pi)}{\\sqrt{|\\Psi_{\\xi}|}}\\right)\\sqrt{\\sum_{j=0}^{k-1}\\mathsf{T V}^{2}(\\mathbb{P}_{1}^{\\pi^{j}},\\mathbb{P}_{2}^{\\pi^{j}})(\\mathcal T)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then we apply Lemma A.4 and Lemma A.5 to deduce that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{j=0}^{k-1}\\mathrm{T}\\mathbb{V}^{2}(\\mathbb{P}_{1}^{\\pi^{j}},\\mathbb{P}_{2}^{\\pi^{j}})(\\mathcal{T})\\leq\\frac{16\\beta}{n_{\\sf t e s t}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "On the other hand, note that ", "page_idx": 17}, {"type": "equation", "text": "$$\nC(\\psi_{\\xi};\\pi^{k})=\\operatorname*{max}_{t\\in[H]}\\operatorname*{max}_{x\\in\\mathcal{X}}\\frac{\\mathbb{P}^{\\pi^{k}}(x_{t}=x)}{\\mathbb{P}^{\\psi_{\\xi}}(x_{t}=x)}\\leq\\operatorname*{max}_{x\\in S\\times A}\\frac{|\\Psi_{\\xi}|\\cdot\\mathbb{P}^{\\pi^{k}}(x_{t}=x)}{\\operatorname*{max}_{j<k}\\mathbb{P}^{\\pi^{j}}(x_{t}=x)}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now using the while loop condition, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\epsilon_{\\mathtt{T V}}<\\mathsf{T V}\\big(\\mathbb{P}_{1}^{\\pi^{k}},\\mathbb{P}_{2}^{\\pi^{k}}\\big)(\\mathcal{T})\\leq8H\\cdot\\underset{t\\in[H]}{\\operatorname*{max}}\\frac{C\\big(\\psi_{\\xi};\\pi^{k}\\big)}{\\sqrt{\\lvert\\Psi_{\\xi}\\rvert}}\\sqrt{n_{\\mathsf{t e s t}}\\beta}}\\\\ &{\\qquad\\leq8H\\cdot\\sqrt{\\frac{H S A\\beta}{n_{\\mathsf{t e s t}}}}\\underset{t\\in[H]}{\\operatorname*{max}}\\operatorname*{max}_{\\substack{\\operatorname*{max}}}\\frac{\\mathbb{P}^{\\pi^{k}}\\left(x_{t}=x\\right)}{\\operatorname*{max}_{j<k}\\mathbb{P}^{\\pi^{j}}\\left(x_{t}=x\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Rearranging the inequality, implies that there exists a $t\\in[H]$ and an $x\\in\\mathscr{X}$ such that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{j<k}\\mathbb{P}^{\\pi^{j}}(x_{t}=x)\\leq\\frac{8H}{\\epsilon_{\\sf T V}}\\sqrt{\\frac{H S A\\beta}{n_{\\sf t e s t}}}\\cdot{\\mathbb{P}^{\\pi^{k}}}(x_{t}=x).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "C.3 Proof of Theorem 3.3 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We first show that Algorithm 1 terminates after $K=H S A\\log(1/\\gamma)$ iterations where $\\gamma=\\epsilon_{\\mathrm{test}}^{2}/H^{2}$ . We consider a perturbed model $\\hat{\\theta}^{*}=(w,\\hat{T},R)$ where ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{T}(\\cdot|s,a)=(1-\\gamma)T^{*}(\\cdot|s,a)+\\gamma\\mathbb{1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By simulation lemma [38], for any $\\pi\\in\\Pi$ , note that $\\mathrm{TV}(\\mathbb{P}_{1}^{\\pi},\\mathbb{P}_{2}^{\\pi})(y|x)\\leq2\\gamma S$ for all $x,y$ , and thus ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{T}\\nabla(\\mathbb{P}_{\\hat{\\theta}^{*}}^{\\pi},\\mathbb{P}_{\\theta^{*}}^{\\pi})(\\mathcal{T})=\\displaystyle\\sum_{x_{1}:H}\\sum_{y_{1}:H}\\pi_{1:H}\\left|\\prod_{t=0}^{H}\\mathbb{P}_{1}(y_{t}|x_{t})-\\prod_{t=0}^{H}\\mathbb{P}_{2}(y_{t}|x_{t})\\right|}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\displaystyle\\sum_{x_{1}:H-1}\\sum_{y_{1}:H-1}\\pi_{1:H-1}\\left|\\prod_{t=0}^{H-1}\\mathbb{P}_{1}(y_{t}|x_{t})-\\prod_{t=0}^{H-1}\\mathbb{P}_{2}(y_{t}|x_{t})\\right|}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad+\\displaystyle\\sum_{x_{H}}\\sum_{y_{H}}\\mathbb{P}_{1}^{\\pi}(x_{H})\\left|\\mathbb{P}_{1}(y_{H}|x_{H})-\\mathbb{P}_{2}(y_{H}|x_{H})\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\le\\sum_{x_{1:H-1}}\\sum_{y_{1:H-1}}\\pi_{1:H-1}\\left|\\prod_{t=0}^{H-1}\\mathbb{P}_{1}(y_{t}|x_{t})-\\prod_{t=0}^{H-1}\\mathbb{P}_{2}(y_{t}|x_{t})\\right|+2\\gamma S}}\\\\ &{\\le\\ldots\\le2S\\gamma H.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For an arbitrary $k$ iteration, we check whether the coverage doubling argument (Lemma D.1) still holds. To see this, first note that we can define $\\hat{\\Psi}_{\\xi},\\,\\hat{\\psi}_{\\xi}$ and $\\hat{C}(\\hat{\\psi}_{\\xi};\\pi)$ as in Lemma 3.2 in terms of $\\hat{\\theta}^{*}$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\hat{\\Psi}_{\\xi}:=\\left\\{\\pi^{j^{*}(x,t)},\\forall x,t\\mid j^{*}(x,t):=\\arg\\operatorname*{max}_{j\\in0,1,\\ldots,k-1}{\\mathbb P}_{\\hat{\\theta}^{*}}^{\\pi^{j}}(x_{t}=x),\\right\\},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and $\\hat{\\psi}_{\\xi}$ is a checkpoint-dependent policy where $\\hat{\\psi}_{\\xi}$ is a mixture of $\\hat{\\Psi}_{\\xi}$ , and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\hat{C}(\\psi;\\pi):=\\operatorname*{max}_{t\\in[H]}\\operatorname*{max}_{x\\in\\mathcal{X}}\\frac{\\mathbb{P}_{\\hat{\\theta}^{\\ast}}^{\\pi}(x_{t}=x)}{\\mathbb{P}_{\\hat{\\theta}^{\\ast}}^{\\psi}(x_{t}=x)}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now we invoke Lemma A.4 and Lemma A.5 to show that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{TV}\\left(\\mathbb{P}_{\\hat{\\theta}^{*}}^{\\hat{\\psi}_{\\xi}},\\mathbb{P}_{\\theta}^{\\hat{\\psi}_{\\xi}}\\right)(\\mathcal{T})\\leq\\mathrm{TV}\\left(\\mathbb{P}_{\\hat{\\theta}^{*}}^{\\hat{\\psi}_{\\xi}},\\mathbb{P}_{\\theta^{*}}^{\\hat{\\psi}_{\\xi}}\\right)(\\mathcal{T})+\\mathrm{TV}\\left(\\mathbb{P}_{\\theta^{*}}^{\\hat{\\psi}_{\\xi}},\\mathbb{P}_{\\theta}^{\\hat{\\psi}_{\\xi}}\\right)(\\mathcal{T})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\leq2S H\\gamma+\\displaystyle\\frac{1}{|\\hat{\\Psi}_{\\xi}|}\\sum_{\\psi\\in\\hat{\\Psi}_{\\xi}}\\mathrm{TV}\\left(\\mathbb{P}_{\\theta^{*}}^{\\psi},\\mathbb{P}_{\\theta}^{\\psi}\\right)(\\mathcal{T}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for all $\\theta\\in{\\mathcal{C}}^{k}$ using the triangle inequality for TV distance and $(a+b)^{2}\\leq2(a^{2}+b^{2})$ . Thus, we can derive equation (16) in terms of $\\hat{\\theta}^{*}$ as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{T}\\nabla(\\mathbb{P}_{\\hat{\\theta}^{*}}^{\\pi},\\mathbb{P}_{\\theta}^{\\pi})(\\mathcal{T})\\le2H\\cdot\\frac{\\hat{C}(\\hat{\\psi}_{\\xi};\\pi)}{\\sqrt{\\lvert\\hat{\\Psi}_{\\xi}\\rvert}}\\sqrt{\\frac{16\\beta}{n_{\\mathrm{test}}}+(2S H)^{2}\\gamma^{2}}}\\\\ &{\\qquad\\qquad\\qquad\\le2H\\sqrt{H S A}\\sqrt{\\frac{16\\beta}{n_{\\mathrm{test}}}+(2S H)^{2}\\gamma^{2}}\\cdot\\underset{t\\in[H]}{\\operatorname*{max}}\\underset{x\\in\\mathcal{X}}{\\operatorname*{max}}\\frac{\\mathbb{P}_{\\hat{\\theta}^{*}}^{\\pi^{k}}(x_{t}=x)}{\\operatorname*{max}_{j<k}\\mathbb{P}_{\\hat{\\theta}^{*}}^{\\psi}(x_{t}=x)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "On the other hand, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\left(\\mathrm{T}\\nabla(\\mathbb{P}_{\\hat{\\theta}^{*}}^{\\pi_{k}},\\mathbb{P}_{\\theta_{1}}^{\\pi_{k}})(\\mathcal{T}),\\mathrm{T}\\nabla(\\mathbb{P}_{\\hat{\\theta}^{*}}^{\\pi_{k}},\\mathbb{P}_{\\theta_{2}}^{\\pi_{k}})(\\mathcal{T})\\right)>2\\boldsymbol{\\epsilon}_{\\mathrm{T}}-2S H\\gamma>1.5\\boldsymbol{\\epsilon}_{\\mathrm{T}\\mathrm{V}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "by setting $\\gamma=\\epsilon_{\\mathrm{test}}/(4S A H)^{4}$ . Let $n_{\\tt t e s t}=64\\beta(H^{3}S A)/\\epsilon_{\\tt t e s t}^{2}$ , and we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n2\\epsilon_{\\mathrm{{T}\\mathrm{{V}}}}<\\epsilon_{\\mathsf{t e s t}}\\cdot\\operatorname*{max}_{t\\in[H]}\\operatorname*{max}_{x\\in\\mathcal{X}}\\frac{\\mathbb{P}_{\\hat{\\theta}^{*}}^{\\pi}(x_{t}=x)}{\\operatorname*{max}_{j<k}\\mathbb{P}_{\\hat{\\theta}^{*}}^{\\psi}(x_{t}=x)}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Hence the same doubling argument holds, and MDP-OMLE (Algorithm 1) will terminate after at most ", "page_idx": 18}, {"type": "equation", "text": "$$\nK=O(H S A\\log(H S A/\\epsilon_{\\sf t e s t}))\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "iterations. We note that all inequalities hold for all $K$ iterations with probability at least $1-\\eta$ . Finally, by setting $\\epsilon_{\\tt t e s t}=\\epsilon_{\\tt T V}$ and $\\epsilon_{\\mathrm{{T}V}}=\\epsilon/H$ , we can conclude that the total number of trajectories that was generated by MDP-OMLE is bounded by ", "page_idx": 18}, {"type": "equation", "text": "$$\nO(1)\\cdot H^{6}S^{2}A^{2}\\beta\\log(H S A/\\epsilon)/\\epsilon^{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Appendix D Proofs for Section 4 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "D.1 Proof Sketch ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we provide an overview of the proof for Theorem 4.5. Compared to Algorithm 1, the main differences in LMDPs from the MDP cases are two-fold: ", "page_idx": 18}, {"type": "text", "text": "(a) Our goal is to find the optimal history-dependent (non-Markovian) policy. ", "page_idx": 18}, {"type": "text", "text": "(b) We cannot observe any context-segment pair that previous behavioral policies could not cover. ", "page_idx": 19}, {"type": "text", "text": "For the first point, (a), we already have reduced the problem from matching all history-dependent policies to a set of behavioral policies generated by the concatenation of memoryless policies in Lemma 4.4. For the second point, (b), even though we cannot observe the latent context $m$ under which each segment is covered, we can improve the coverage of each context-segment pair given the test set $\\Psi_{\\mathtt{t e s t}}^{k}$ every iteration: ", "page_idx": 19}, {"type": "text", "text": "Lemma D.1 Let $n_{t e s t}\\geq3\\beta M^{2}(8H^{2})^{d}(M S^{2}A^{2})^{d}/\\epsilon_{t e s t}^{2}$ . Then with probability at least $1-\\eta,$ , at every $k^{t h}$ iteration in Algorithm 2, there must exist at least one $(m,t_{1},t_{2},s,s^{\\prime}),$ , such that ", "page_idx": 19}, {"type": "equation", "text": "$$\nP_{m}^{\\pi^{k}}(s_{t_{2}}=s|s_{t_{1}}^{\\prime}=s^{\\prime})>2\\cdot\\operatorname*{max}_{\\psi\\in\\Psi_{t e s t}^{k-1}}P_{m}^{\\psi}(s_{t_{2}}=s|s_{t_{1}}^{\\prime}=s^{\\prime}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "That is, we can ensure that the coverage of at least one context-segment pair is being exponentially improved despite the unobservability of latent contexts $m$ . ", "page_idx": 19}, {"type": "text", "text": "For a moment, to simplify the discussion, we first assume that the uniformly random policy Unif $(A)$ has a non-zero $\\gamma>0$ probability for covering all segments in all contexts: ", "page_idx": 19}, {"type": "equation", "text": "$$\nP_{m}^{\\mathrm{Unif}(A)}\\big(s_{t_{2}}=s_{2}\\big|s_{t_{1}}=s_{1}\\big)\\geq\\gamma,\\qquad\\forall t_{1}<t_{2},m,s_{1},s_{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We note that this assumption will be eventually removed in our final result. Thus, if we start from the initial coverage $\\gamma>0$ over all context-segment pairs, then since every probability is in the range of $[0,1]$ , this doubling-up event can happen at most $\\log(1/\\gamma)$ times for every context-segment pair. Therefore, Algorithm 2 must terminate after at most $K\\overset{\\cdot}{=}\\overset{\\cdot}{M}S^{2}H\\log(1/\\gamma)$ iterations. ", "page_idx": 19}, {"type": "text", "text": "Separately from the coverage improvement in Lemma D.1, the standard concentration of the confidence set on the generated trajectory data is given by the maximum-likelihood estimation: ", "page_idx": 19}, {"type": "text", "text": "Lemma D.2 With probability at least $1\\,-\\,\\eta,$ for all $k^{t h}$ iterations in Algorithm 2, let $\\Psi_{\\xi}\\ =$ $\\{\\psi_{1},...,\\psi_{n}\\}\\subseteq\\Psi_{\\substack{t e s t}}^{k-1}$ be any subset of candidate test policies, and let $\\psi_{\\xi}=(\\psi_{\\xi},...,\\psi_{\\xi},\\,U n i f(A))$ where $\\begin{array}{r}{\\psi_{\\xi}:=\\frac{1}{n}\\sum_{i\\in[n]}\\psi_{i}}\\end{array}$ is a mixture of policies in $\\Psi_{\\xi}$ . Then for any model in the confidence set $\\theta\\in{\\mathcal{C}}$ , the following holds: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{\\substack{T\\in S u b\\,S e q(H,d)\\;z\\subseteq\\{0,1\\}\\otimes\\,|\\tau|}}T V^{2}\\left(\\mathbb{P}_{\\theta^{*}}^{\\nu(\\psi_{\\xi};\\tau,z)},\\mathbb{P}_{\\theta}^{\\nu(\\psi_{\\xi};\\tau,z)}\\right)(x_{\\tau},y_{\\tau})\\leq\\frac{4\\beta}{n^{d}\\cdot n_{t e s t}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Equipped with Lemma D.2 With probability at least $1-\\eta$ , we terminate the while-loop after at most $\\\"K=H S^{2}M\\log(1/\\gamma)$ iterations, and each while-loop generates $K^{d-1}\\cdot n_{\\tt t e s t}$ new trajectories, leading to a total ", "page_idx": 19}, {"type": "equation", "text": "$$\nK^{d}\\cdot n_{\\sf t e s t}=(8M^{2}S^{4}H^{3}A^{2}\\log(1/\\gamma))^{d}\\cdot O(M^{2}\\beta/\\epsilon_{\\sf t e s t}^{2})\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "sample complexity. The near-optimality guarantee for the final empirical model is given by Lemma 4.4 where we set $\\dot{\\epsilon}_{\\sf t e s t}=M^{-1}\\(2H^{2}M\\dot{S}\\dot{A})^{-d}\\cdot\\epsilon_{\\mathrm{TV}}$ to obtain an $(H\\epsilon_{\\mathrm{TV}})$ -optimal policy. ", "page_idx": 19}, {"type": "text", "text": "Without Initial Coverage. Now we remove the initial $\\gamma>0$ coverage assumption (17). To do so, consider a virtual LMDP model $\\hat{\\theta}=(\\{w_{m}^{*},\\hat{T}_{m},R_{m}^{*}\\})_{m=1}^{M}$ with perturbed transition models, i.e., $\\hat{T}_{m}(\\cdot|s,a)=(1-\\gamma)T_{m}^{*}(\\cdot|s,a)+\\gamma\\mathbb{1}$ for all $(s,a)\\in S\\times A$ . For $\\hat{\\theta}$ , it is easy to see that for all $\\pi\\in\\Pi$ , we have $\\mathrm{TV}(\\mathbb{P}_{\\hat{\\theta}}^{\\pi},\\mathbb{P}_{\\theta^{*}}^{\\pi})(\\tau)\\leq2H S\\gamma.$ . Thus, now we can shift our arguments to this perturbed model with sufifciently small $\\gamma$ , and it is straightforward that the perturbed model has the $\\gamma>0$ segment coverage for any policy, which concludes Theorem 4.5. ", "page_idx": 19}, {"type": "text", "text": "D.2 Proof of Lemma 4.2 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We start from equation (12) in Lemma A.1. We refer the reader to Appendix A.2 for the used notation here, with additional notation we define here: for a subset of indices $I\\subseteq[|\\tau|]$ , we write $\\tau_{I}:=(\\tau_{i})_{i\\in I}$ to refer to a subsequence of $\\tau$ at positions $I$ , and $\\tau_{/I}$ a subsequence at positions outside of $I$ . ", "page_idx": 19}, {"type": "text", "text": "We can first bound $\\Delta(x_{\\tau},y_{\\tau})$ as the following: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta(x_{\\tau},y_{\\tau})\\leq\\displaystyle\\sum_{I\\leq\\lfloor\\lceil\\tau\\rfloor\\rceil}\\left(\\prod_{i\\in I}^{l+1}(x_{\\tau_{i}\\wedge-1},y_{\\tau_{i}\\wedge-1})\\right)\\times}\\\\ &{\\bigg|\\sum_{m}w_{m}^{1}\\left(\\prod_{i\\in I}P_{m}^{1,\\nu(\\psi_{i-1};\\tau_{i}-1)}(s_{\\tau_{i}+1}|s_{\\tau_{i-1}+1})\\right)\\left(\\prod_{i\\in\\lfloor\\rceil\\tau\\rfloor/I}P_{m}^{1,\\nu(\\psi_{i-1};\\tau_{i}-1)}(s_{\\tau_{i}}|s_{\\tau_{i-1}+1})P_{m}^{1}(y_{\\tau_{i}},\\psi_{i-1};\\psi_{i-1})\\right)}\\\\ &{\\quad-\\sum_{m}w_{m}^{2}\\left(\\prod_{i\\in I}P_{m}^{2,\\nu(\\psi_{i-1};\\tau_{i}-1)}(s_{\\tau_{i}+1}|s_{\\tau_{i-1}+1})\\right)\\left(\\prod_{i\\in\\lfloor\\rceil\\tau\\rfloor/I}P_{m}^{2,\\nu(\\psi_{i-1};\\tau_{i}-1)}(s_{\\tau_{i}}|s_{\\tau_{i-1}+1})P_{m}^{2}(y_{\\tau_{i}},\\psi_{i-1};\\psi_{i-1})\\right)}\\\\ &{=\\displaystyle\\sum_{I\\leq\\lfloor\\tau\\rfloor\\rfloor}\\frac{\\prod_{i\\in I}l^{\\frac{l}{\\tau_{i}}-1}(x_{\\tau_{i+1}-1},y_{\\tau_{i+1}-1})}{\\prod_{i\\in\\lfloor\\tau\\rfloor/I}(1/A)}\\delta_{\\nu(\\psi;\\tau,z(I;\\tau))}(s_{\\tau_{i}^{\\prime}}^{\\prime},x_{\\tau_{I}/\\tau_{i}},y_{\\tau_{I}/\\tau}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $z(I;\\tau)$ satisfies $z(I;\\tau)_{j}=0$ if $j\\in I$ and 1 otherwise. Then we can observe that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\prod_{i\\in I}l_{\\tau_{i}}^{i-1}(x_{\\tau_{1:i-1}},y_{\\tau_{1:i-1}})}{\\prod_{i\\in[q]/I}(1/A)}\\cdot\\frac{1}{\\prod_{i=0}^{q-1}P_{m(x_{\\tau},y_{\\tau})}^{\\nu(\\psi_{i};\\tau_{i})}(s_{\\tau_{i+1}}|s_{\\tau_{i}+1})P_{m(x_{\\tau},y_{\\tau})}(y_{\\tau_{i+1}}|x_{\\tau_{i+1}})}}\\\\ &{\\leq\\frac{1}{\\prod_{i\\in I}P_{m(x_{\\tau},y_{\\tau})}^{\\nu(\\psi_{i-1};\\tau_{i-1})}(s_{\\tau_{i}+1}|s_{\\tau_{i-1}+1})\\cdot\\prod_{i\\in[q]/I}P_{m(x_{\\tau},y_{\\tau})}^{\\nu(\\psi_{i-1};\\tau_{i-1})}(x_{\\tau_{i}}|s_{\\tau_{i-1}+1})P_{m(x_{\\tau},y_{\\tau})}(y_{\\tau_{i}}|x_{\\tau_{i}})}}\\\\ &{=\\frac{1}{P_{m(x_{\\tau},y_{\\tau})}^{\\nu(\\psi;\\tau,z)}(x_{\\tau_{I}}^{\\prime},x_{\\tau_{I}/1},y_{\\tau_{I}/l})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "using inequality that can be derived by definition of $l_{t}^{i}$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\nl_{\\tau_{i}}^{i-1}\\leq\\frac{P_{m(x_{\\tau},y_{\\tau})}^{\\nu(\\psi_{i-1};\\tau_{i-1})}(s_{\\tau_{i}}|s_{\\tau_{i-1}+1})}{P_{m(x_{\\tau},y_{\\tau})}^{\\nu(\\psi_{i-1};\\tau_{i-1})}(s_{\\tau_{i}+1}|s_{\\tau_{i-1}+1})}P_{m(x_{\\tau},y_{\\tau})}(y_{\\tau_{i}}|x_{\\tau_{i}}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We are now ready to use this inequality to bound the TV distance between trajectory distribution via equation (12). To do so, we rearrange the summation orders as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{r_{m+1}}{\\sum\\sum}\\frac{\\pi_{1,l}}{r_{m+1}}\\bigg|\\sum_{m=1}^{M_{1}}w_{m}^{1}\\prod_{t=0}^{H}P_{m}^{1}(y_{l}|x_{t})-\\sum_{m=1}^{M_{2}}w_{m}^{2}\\prod_{t=0}^{H}P_{m}^{2}(y_{l}|x_{t})\\bigg|}\\\\ &{\\leq\\underset{r_{m+1}}{\\sum}\\sum_{m=1}^{M_{1}}\\sum_{\\substack{\\nu,\\nu,\\nu,m^{\\prime}(x,\\nu,\\nu)=1}}\\Delta(x_{r},y_{r})\\times\\left(\\frac{P_{m}^{\\prime}(x_{r},y_{r})}{\\prod_{t=0}^{M_{1}}P_{m}^{1}(\\nu,x_{t+1}|y_{m})\\prod_{t=0}^{\\nu}\\left(y_{r+1}|x_{r_{t+1}}|x_{r_{t+1}}\\right)}\\right)}\\\\ &{\\leq\\underset{r\\leq1\\mu}{\\sum}\\frac{\\sum}{\\sum_{m=1}^{M_{1}}\\sum_{\\substack{\\nu,\\nu,\\nu,m^{\\prime}(x,\\nu,\\nu)=1}}}\\sum_{\\substack{\\nu=1}}^{\\infty}\\left(\\frac{P_{m}^{\\prime}(x_{r},y_{r})-\\delta_{r(\\nu,\\tau,\\tau,\\tau,\\tau(\\nu))}\\left(x_{r}^{\\prime},x_{r_{t}},y_{r_{t+1}}\\right)}{P_{m}^{\\prime}(\\nu,x_{r},x(r+1/r))\\left(x_{r}^{\\prime},x_{r_{t+1}},y_{r_{t}}\\right)}\\right)}\\\\ &{\\leq\\underset{r\\leq1\\mu}{\\sum}\\frac{\\sum}{\\sum_{m=1}^{M_{1}}\\sum_{\\substack{\\nu=1}^{M_{1}}}\\sum_{\\substack{\\nu=1}^{M_{2}}}\\sum_{\\substack{\\nu=1}^{M_{2}}}\\left(\\frac{P_{m}^{\\prime}(x_{r}^{\\prime},y_{r_{t+1}},y_{r_{t+1}})-\\delta_{r(\\nu,\\tau,\\tau,\\tau(\\nu))}\\left(x_{r_{t}^{\\prime}}^{\\prime},x_{r_{t}},y_{r_{t}}\\right)}{P_{m}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "proving Lemma 4.2. ", "page_idx": 20}, {"type": "text", "text": "D.3 Proof of Lemma 4.4 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Before proceeding to the proof, we refer the reader to Appendix A.3 for additional preliminaries. Recall the definition of $\\Psi_{\\xi}$ in Lemma A.3: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\Psi_{\\xi}=\\left\\{\\arg\\operatorname*{max}_{\\psi\\in\\Psi_{\\mathrm{test}}}P_{m}^{\\nu(\\psi;t_{0})}\\big(s_{t_{0}+t}=s|s_{t_{0}}^{\\prime}=s^{\\prime}\\big),\\ \\forall m,s,s^{\\prime},t\\right\\}\\subseteq\\Psi_{\\mathrm{test}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that the above definition is invariant to $t_{0}$ . ", "page_idx": 21}, {"type": "text", "text": "Now consider $\\Psi_{\\mathrm{test}}~=~\\Pi_{\\mathrm{mls}}$ . Then, $\\Psi_{\\xi}$ is the set of policies that maximize the probability $P_{m}^{\\nu(\\psi;t_{0})}(s_{t_{0}+t}\\,=\\,s|s_{t_{0}}^{\\prime}\\,=\\,s^{\\prime})$ s,t ai.rtei.,n ga  stmaetem r).y leTshse rpeofloircey,  twhea tc aainm fsir stto  irnedaucche $s$ haaftt $t$ $s^{\\prime}$ $|\\Psi_{\\xi}|\\,\\le\\,M H S$ Then, the Markovian policy maximizing the probability to reach a certain state under a fixed context $m$ is also best within the history-dependent policy class $\\Pi$ , and therefore ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\rho(\\Pi_{\\mathrm{mls}};\\pi):=\\underset{t,t_{0}}{\\mathrm{max}}\\,\\underset{m,s,s^{\\prime}}{\\mathrm{max}}\\,\\frac{\\operatorname*{max}_{\\mathcal{T}_{1:t_{0}}}P_{m}^{\\pi}(s_{t+t_{0}}=s|s_{t_{0}}^{\\prime}=s^{\\prime},\\mathcal{T}_{1:t_{0}})}{\\operatorname*{max}_{\\psi\\in\\Psi_{\\xi}}P_{m}^{\\nu(\\psi;t_{0})}(s_{t+t_{0}}=s|s_{t_{0}}^{\\prime}=s^{\\prime})}\\leq1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We can now invoke Lemma 4.2 and Lemma A.3, and noting that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{TV}(\\mathbb{P}_{1}^{\\pi},\\mathbb{P}_{2}^{\\pi})(s_{\\tau_{I}}^{\\prime},x_{\\tau_{/I}},y_{\\tau_{/I}})\\leq\\mathrm{TV}(\\mathbb{P}_{1}^{\\pi},\\mathbb{P}_{2}^{\\pi})(\\mathcal{T}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Let $\\psi_{\\xi}$ be as defined in Lemma A.3, and for any $\\pi\\in\\Pi$ , we can conclude that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Psi(\\mathbb{P}_{1}^{\\pi},\\mathbb{P}_{2}^{\\pi})(\\mathcal{T})\\leq M\\cdot C(\\psi_{\\xi};\\pi)\\displaystyle\\sum_{\\substack{\\tau\\in\\mathrm{Subseq}(H,d)\\,I\\in[\\![\\tau]\\!]}}\\mathrm{T}\\nabla\\left(\\mathbb{P}_{1}^{\\nu(\\psi_{\\xi};\\tau,z(I;\\tau))},\\mathbb{P}_{2}^{\\nu(\\psi_{\\xi};\\tau,z(I;\\tau))}\\right)\\left(s_{\\tau_{I}}^{\\prime},x_{\\tau_{I}/\\tau}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\leq M\\cdot(M H S A)^{d}\\displaystyle\\sum_{\\tau\\in\\mathrm{Subseq}(H,d)\\,z\\in\\{0,1\\}^{\\lvert\\tau\\rvert}}\\mathrm{T}\\nabla\\left(\\mathbb{P}_{1}^{\\nu(\\psi_{\\xi};\\tau,z)},\\mathbb{P}_{2}^{\\nu(\\psi_{\\xi};\\tau,z)}\\right)(\\mathcal{T}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Finally, it only remains to bound the total variation distance with $\\nu(\\psi_{\\xi};\\tau,z)$ . To see this, for a given $q\\leq d,\\,\\tau\\in\\mathsf{S u b s e q}(H,d)$ , and $z\\in\\{0,1\\}^{|\\tau|}$ , it is easy to check that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{TV}\\left(\\mathrm{P}_{1}^{\\nu(\\psi_{\\xi},\\tau,z)},\\mathrm{P}_{2}^{\\nu(\\psi_{\\xi};\\tau,z)}\\right)(T)\\leq\\underset{\\psi\\in\\Pi_{\\mathrm{mis}}^{\\otimes(q+1)}}{\\operatorname*{max}}\\mathrm{TV}\\left(\\mathrm{P}_{1}^{\\nu(\\psi;\\tau,z)},\\mathrm{P}_{2}^{\\nu(\\psi;\\tau,z)}\\right)(T)\\leq\\epsilon_{\\mathrm{test}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Noting that $\\begin{array}{r}{\\sum_{q\\leq d}{\\binom{H}{q}}\\leq\\operatorname*{min}(H^{d},2^{H})}\\end{array}$ , assuming we are in the regime $H\\gg d$ , we conclude that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathtt{T V}(\\mathbb{P}_{1}^{\\pi},\\mathbb{P}_{2}^{\\pi})(\\mathcal{T})\\le M(M S A)^{d}\\cdot(2H^{2})^{d}\\cdot\\epsilon_{\\mathsf{t e s t}},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "concluding the proof. ", "page_idx": 21}, {"type": "text", "text": "D.4 Proof of Theorem 4.5 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We continue from the conclusion of Lemma D.1, and the remaining step is to ensure that LMDP-OMLE terminates after $K=M S^{2}H\\log(1/\\gamma)$ iterations where $\\gamma=\\epsilon_{\\mathrm{test}}^{2}\\breve{/}(H^{\\dot{2}d})$ without the initial coverage assumption (17). We consider a perturbed model $\\hat{\\theta}^{*}=(\\{w_{m}^{*},\\hat{T}_{m},R_{m}^{*}\\}_{m=1}^{M})$ where ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\hat{T}_{m}(\\cdot|s,a)=(1-\\gamma)T_{m}^{*}(\\cdot|s,a)+\\gamma\\mathbb{1},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for all $m$ . By the performance difference lemma [35], for any $\\pi\\in\\Pi$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{TV}(\\mathbb{P}_{\\hat{\\theta}^{*}}^{\\pi},\\mathbb{P}_{\\theta^{*}}^{\\pi})(\\mathcal{T})\\leq\\sum_{m}w_{m}^{*}\\cdot\\mathrm{TV}(\\mathbb{P}_{\\hat{\\theta}^{*}}^{\\pi},\\mathbb{P}_{\\theta^{*}}^{\\pi})(\\mathcal{T}|m)\\leq2S\\gamma H.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For every $k^{t h}$ iteration, we check whether the coverage doubling argument (Lemma D.1) still holds. To see this, first note that we can define $\\hat{\\rho}(\\Psi_{\\tt t e s t}^{k};\\pi)$ and $\\hat{\\Psi}_{\\xi},\\,\\hat{\\psi_{\\xi}}\\,$ and $\\hat{C}(\\psi_{\\xi};\\pi)$ as in Lemma A.3 in terms of ${\\hat{\\theta}}^{*}$ . Then Lemma D.2 can be modified to guarantee that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{\\tau\\in\\mathrm{Subseq}(H,d)}\\sum_{z\\in\\{0,1\\}^{|\\tau|}}\\mathrm{T}\\mathbb{V}^{2}\\left(\\mathrm{P}_{\\hat{\\theta}^{*}}^{\\nu(\\hat{\\psi}_{\\xi};\\tau,z)},\\mathbb{P}_{\\theta}^{\\nu(\\hat{\\psi}_{\\xi};\\tau,z)}\\right)(T)\\leq\\frac{16\\beta}{n^{d}\\cdot n_{\\mathrm{test}}}+2(2H)^{d}(2S H)^{2}\\gamma^{2},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for all $\\theta\\in{\\mathcal{C}}^{k}$ using the triangle inequality for TV distance and $(a+b)^{2}\\leq2(a^{2}+b^{2})$ . Thus, we can derive (19) in terms of $\\hat{\\theta}^{*}$ as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{TV}(\\mathbb{P}_{\\hat{\\theta}^{*}}^{\\pi},\\mathbb{P}_{\\theta}^{\\pi})(\\mathcal{T})\\leq8M(n A\\hat{\\rho}(\\hat{\\Psi}_{\\xi};\\pi))^{d}\\sqrt{\\frac{(2H)^{d}\\beta}{n^{d}\\cdot n_{\\mathrm{test}}}+(2H)^{d}(4S H)^{2}\\gamma^{2}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "table", "img_path": "juJl2uSq4D/tmp/2fbfdc3bd1d9f12daca8f4b520a77207e873e9d695ba84976c9c0c938e60030c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Table 1: The first step generates one of four possible beliefs of a history. Measured elements in the table denote the expected rewards of actions at $t=2$ following a behavioral policy. We can see that for all $m\\in[M],a\\in{\\mathcal{A}}$ it holds that $\\mathbb{P}_{m}\\!\\left(s_{2},a_{2}\\right)>0$ for all $m\\in[M]$ , $a_{2}\\in A$ ; yet, we cannot estimate $\\mathbb{E}[r_{2}]$ given some histories. ", "page_idx": 22}, {"type": "text", "text": "On the other hand, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\left(\\mathrm{T}\\nabla(\\mathbb{P}_{\\hat{\\theta}^{*}}^{\\pi_{k}},\\mathbb{P}_{\\theta_{1}}^{\\pi_{k}})(\\mathcal{T}),\\mathrm{T}\\nabla(\\mathbb{P}_{\\hat{\\theta}^{*}}^{\\pi_{k}},\\mathbb{P}_{\\theta_{2}}^{\\pi_{k}})(\\mathcal{T})\\right)>2\\boldsymbol{\\epsilon}_{\\mathrm{T}}-2S H\\gamma>1.5\\boldsymbol{\\epsilon}_{\\mathrm{T}\\mathrm{V}},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Now we set $\\gamma\\;=\\;\\epsilon_{\\sf t e s t}^{2}(8H n A)^{-2d}(4M S H)^{-2}$ with $n_{\\sf t e s t}\\:=\\:64M^{2}\\beta(H n A^{2})^{d}/\\epsilon_{\\sf t e s t}^{2}$ and $n\\,=$ $M H S^{2}$ , and we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n2\\epsilon_{\\mathrm{TV}}^{2}<4^{-d}\\cdot\\hat{\\rho}(\\Psi_{\\tt t e s t}^{k-1};\\pi^{k})^{d}\\epsilon_{\\tt t e s t}^{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Hence the same doubling argument holds, and Algorithm 2 will terminate after at most ", "page_idx": 22}, {"type": "equation", "text": "$$\nK=O(M d S^{2}H\\log(M H S A/\\epsilon_{\\sf t e s t}))\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "iterations. We note that all inequalities hold for all $K$ iterations with probability at least $1-\\eta$ . Finally, we invoke Lemma 4.4 with $\\bar{\\epsilon_{\\sf t e s t}}=\\epsilon_{\\sf T V}\\cdot\\mathrm{poly}(H,M,S,A)^{-d},\\epsilon_{\\sf T V}=\\bar{\\epsilon}/(4M S\\bar{A}H)^{d}$ and $d=2M-1$ , we can conclude that the total number of trajectories we generated is bounded by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{poly}(M,H,S,A,\\log(M H S A/\\epsilon))^{d}/\\epsilon^{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "D.5 A Counter Example for Remark 4.3 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "One may wonder why it is not sufifcient to consider a single latent-state coverability analogous to Lemma 3.1, analogous to equation (2), defined as the following: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{t\\in[H]}\\operatorname*{max}_{x\\in\\mathcal{X}}\\operatorname*{max}_{m\\in[M]}\\frac{P_{m}^{\\pi}(x_{t}=x)}{P_{m}^{\\psi}(x_{t}=x)}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Here we present a counter example where the multi-step events must be considered even in the latent state space: the LMDP consists of $M=3$ MDPs with $\\mathcal{S}=\\{1,2\\},\\mathcal{A}=\\{1,2\\},\\mathcal{R}=\\{-1,0,1\\}$ , and $H\\,=\\,2$ . All MDP starts from $s_{1}~=~1$ and with a transition kernel $T_{m}(s_{2}\\,=\\,2|s_{1},a)\\,=\\,1$ for all $m\\ \\in\\ [M]$ and $a\\ \\in\\ A$ . Reward models are given by $R_{1}(r\\,=\\,-1|s\\,=\\,1,a\\,=\\,1)\\,=\\,1$ , $R_{2}(r\\,=\\,1|s\\,=\\,1,a\\,=\\,2)\\,=\\,1$ , and $R_{m}(r|s=1,a=1)\\,=\\,0.5$ for $r\\,\\in\\,\\{0,1\\}$ , $m\\in\\{2,3\\}$ , and $R_{m}(r|s=1,a=2)=0.5$ for $r\\in\\{-1,0\\}$ , $m\\in\\{1,3\\}$ . ", "page_idx": 22}, {"type": "text", "text": "Now we target to measure the expected rewards of actions executed by a behavioral policy at $s_{2}=2$ as in Table 1. In this example, all actions are covered under all contexts following the behavior policy, i.e., $P_{m}(s_{2},a_{2})>0$ . Yet, we cannot estimate the expected reward of action $a_{2}=1$ under context $m=1$ . Therefore, the speculated single latent-state coverage coefifcient is finite for this problem, however, off-policy evaluation guarantee cannot be established only with the single latent-state coverability. ", "page_idx": 22}, {"type": "text", "text": "Appendix E Deferred Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "E.1 Proof of Lemma D.1 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "For any fixed checkpoint $t_{0}$ , recall the definition $\\Psi_{\\xi}$ as defined in equation (14): ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\Psi_{\\xi}=\\left\\{\\operatorname*{arg}\\operatorname*{max}_{\\psi\\in\\Pi_{\\mathrm{test}}^{k-1}}P_{m}^{\\nu(\\psi;t_{0})}(s_{t+t_{0}}=s|s_{t_{0}}^{\\prime}=s^{\\prime}),\\,\\forall m,s,s^{\\prime},t\\right\\}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Note that $|\\Psi_{\\xi}|\\leq M S^{2}H$ and invariant to $t_{0}$ since $\\Psi_{\\tt t e s t}^{k-1}\\subset\\Pi_{\\mathrm{mls}}$ . Now for any memoryless policy $\\pi\\in\\Pi_{\\mathrm{mls}}$ , we recall Lemma 4.2 with $\\pmb{\\psi}_{\\xi}=(\\psi_{\\xi},...,\\psi_{\\xi}$ , Unif $(\\mathcal{A})$ ), where $\\psi_{\\xi}\\in\\Pi$ is a mixture policy of $\\Psi_{\\xi}$ . We have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{TV}(\\mathbb{P}_{\\theta^{*}}^{\\pi},\\mathbb{P}_{\\theta}^{\\pi})(\\mathcal T)\\le M C(\\psi_{\\xi};\\pi)\\sum_{\\tau\\in\\mathrm{subseq}(H,d)}\\sum_{z\\in\\{0,1\\}:|\\tau|}\\mathrm{TV}\\left(\\mathbb{P}_{\\theta^{*}}^{\\nu(\\psi;\\tau,z)},\\mathbb{P}_{\\theta}^{\\nu(\\psi;\\tau,z)}\\right)(\\mathcal T)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\leq M C(\\psi_{\\xi};\\pi)\\sqrt{(2H)^{d}}\\cdot\\sqrt{\\underset{\\tau\\in\\mathrm{Subseq}(H,d)}{\\sum}\\underset{z\\in\\{0,1\\}^{d}\\}{\\sum}\\underset{\\tau}{\\sum}\\Upsilon^{2}\\left(\\mathrm{P}_{\\theta^{*}}^{\\nu(\\psi;\\tau,z)},\\mathrm{P}_{\\theta}^{\\nu(\\psi;\\tau,z)}\\right)(\\mathcal{T})}}\\\\ &{\\leq4M(n A\\rho(\\Psi_{\\mathrm{test}}^{k-1};\\pi))^{d}\\sqrt{\\underset{n^{d}\\cdot n_{\\mathrm{test}}}{\\sum}\\underset{\\tau\\in\\mathrm{Subst}}{\\sum}},}&{(19)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the last inequality follows by Lemma D.2. By the while-loop condition, for $\\pi_{k}\\in\\Pi_{\\mathrm{mls}}$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\left(\\mathrm{TV}(\\mathbb{P}_{\\theta^{*}}^{\\pi_{k}},\\mathbb{P}_{\\theta_{1}}^{\\pi_{k}})(\\mathcal{T}),\\mathrm{TV}(\\mathbb{P}_{\\theta^{*}}^{\\pi_{k}},\\mathbb{P}_{\\theta_{2}}^{\\pi_{k}})(\\mathcal{T})\\right)>2\\epsilon_{\\mathrm{TV}},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "by the triangle inequality for TV distance. Therefore, we can conclude that ", "page_idx": 23}, {"type": "equation", "text": "$$\n4\\epsilon_{\\tt T V}^{2}<M^{2}(A\\rho({\\Psi}_{\\tt t e s t}^{k-1};{\\pi}^{k}))^{2d}\\cdot\\frac{16(2n H)^{d}\\beta}{n_{\\tt t e s t}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Plugging $n_{\\sf t e s t}=64M^{2}\\beta(8H n A^{2})^{d}/\\epsilon_{\\sf t e s t}^{2}$ with $n=M H S^{2}$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n4^{d}<\\rho(\\Psi_{\\mathrm{test}}^{k-1};\\pi^{k})^{2d}=\\rho(\\Psi_{\\mathrm{test}}^{k-1};\\pi^{k})^{2d}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Thus, $\\rho(\\Psi_{\\mathrm{test}}^{k-1};\\pi^{k})>2$ , which in turn implies Lemma D.1. ", "page_idx": 23}, {"type": "text", "text": "E.2 Proof of Lemma D.2 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "By the construction of confidence set in equation (1) and Lemma A.5, for all $k\\in[K]$ and $\\theta\\in{\\mathcal{C}}_{k}$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{(T,\\pi)\\in\\mathcal{D}_{k}}\\Upsilon\\mathsf{V}^{2}(\\mathbb{P}_{\\theta^{*}}^{\\pi},\\mathbb{P}_{\\theta}^{\\pi})(T)\\le3\\beta,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\beta~~=~\\log(K|\\Theta|/\\eta)$ . Let $\\psi_{\\xi}$ be a mixture of a subset of candidate policies $\\begin{array}{r l}{\\Psi_{\\xi}}&{{}=}\\end{array}$ $\\left\\{\\psi_{1},\\psi_{2},...,\\psi_{n}\\right\\}\\quad\\subseteq\\quad\\Psi_{\\mathrm{test}}^{k-1}$ . With $\\begin{array}{r l r}{\\psi_{\\xi}}&{{}=}&{(\\psi_{\\xi},\\psi_{\\xi},...,\\psi_{\\xi},\\mathrm{Unif}(\\mathcal{A}))}\\end{array}$ and for every $\\tau\\ \\in$ Subseq $(H,d),z\\in\\{0,1\\}^{|\\tau|}$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{TV}\\left(\\mathbb{P}_{\\theta^{*}}^{\\nu(\\psi_{\\xi};\\tau,z)},\\mathbb{P}_{\\theta}^{\\nu(\\psi_{\\xi};\\tau,z)}\\right)({\\mathcal T})}\\\\ &{\\le\\frac{1}{n^{d}}\\sum_{\\substack{i_{1},i_{2},\\ldots,i_{d}\\in[n]}}\\mathrm{TV}\\left(\\mathbb{P}_{\\theta^{*}}^{\\nu((\\psi_{i_{1}},\\ldots,\\psi_{i_{d}},\\mathfrak{u}_{\\mathrm{id}}\\mathfrak{f}(A));\\tau,z)},\\mathbb{P}_{\\theta}^{\\nu((\\psi_{i_{1}},\\ldots,\\psi_{i_{d}},\\mathfrak{u}_{\\mathrm{id}}\\mathfrak{f}(A));\\tau,z)}\\right)({\\mathcal T})}\\\\ &{\\le\\sqrt{\\frac{1}{n^{d}}}\\sqrt{\\sum_{\\substack{i_{1},i_{2},\\ldots,i_{d}\\in[n]}}\\mathrm{TV}^{2}\\left(\\mathbb{P}_{\\theta^{*}}^{\\nu((\\psi_{i_{1}},\\ldots,\\psi_{i_{d}},\\mathfrak{u}_{\\mathrm{id}}\\mathfrak{f}(A));\\tau,z)},\\mathbb{P}_{\\theta}^{(\\nu(\\psi_{i_{1}},\\ldots,\\psi_{i_{d}},\\mathfrak{u}_{\\mathrm{id}}\\mathfrak{f}(A));\\tau,z)}\\right)({\\mathcal T})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{r\\in\\mathrm{Subseq}(H,d)}\\sum_{I\\subseteq[\\![t]\\!]}\\mathrm{TV}^{2}\\left(\\mathrm{P}_{\\theta^{*}}^{\\nu(\\psi_{\\xi};\\tau,z(I;\\tau))},\\mathrm{P}_{\\theta}^{\\nu(\\psi_{\\xi};\\tau,z(I;\\tau))}\\right)(s_{\\tau_{I}}^{\\prime},x_{\\tau_{I}},y_{\\tau_{I}})}\\\\ &{\\le\\displaystyle\\frac{1}{n^{d}}\\sum_{\\tau\\in\\mathrm{Subseq}(H,d)}\\sum_{z\\in\\{0,1\\}^{\\lvert\\tau\\rvert}}\\sum_{i_{1},i_{2},\\ldots,i_{d}\\in[n]}\\mathrm{TV}^{2}\\left(\\mathrm{P}_{\\theta^{*}}^{\\nu((\\psi_{i_{1}},\\ldots,\\psi_{i_{d}},\\Uparrow\\mathrm{i}\\mathrm{f}(A));\\tau,z)},\\mathrm{P}_{\\theta}^{\\nu((\\psi_{i_{1}},\\ldots,\\psi_{i_{d}},\\Uparrow\\mathrm{i}\\mathrm{f}(A));\\tau,z)}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the last inequality is due to the construction of our dataset $\\mathcal{D}^{k}$ and the concentration guarantee for the confidence set $\\bar{c}^{k}$ . ", "page_idx": 23}, {"type": "text", "text": "E.3 Proof of Lemma A.1 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "First note that we can rewrite an atomic probability $P_{m}^{n}(y_{t}|x_{t})=P_{m}^{n}\\big(r_{t},s_{t+1}|x_{t}\\big)$ as ", "page_idx": 23}, {"type": "equation", "text": "$$\n^{2n}_{m}(r_{t},s_{t+1}|x_{t})=\\frac{P_{m}^{n,\\psi_{0}}(s_{t+1})}{P_{m}^{n,\\psi_{0}}(s_{t})}\\left(\\frac{P_{m}^{n,\\psi_{0}}(s_{t})}{P_{m}^{n,\\psi_{0}}(s_{t+1})}P_{m}^{n}(r_{t},s_{t+1}|x_{t})-l^{0}(x_{t},r_{t};s_{t+1})+l^{0}(x_{t},r_{t};s_{t+1})\\right)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In turn, moving from conditioning on the event $\\left({{x}_{t}},{{y}_{t}}\\right)$ , we view the LMDP model after induction as if there are at most $M_{1}-1$ contexts in the first LMDP model (or $M_{2}-1$ in the second LMDP model if $l^{0}(x_{t},r_{t};s_{t})$ is attained with $n=2$ ). We often use a shorthand $l^{0}(x_{t},y_{t})=l^{0}(x_{t},r_{t};s_{t+1})$ to reduce the burden on the notation. ", "page_idx": 23}, {"type": "text", "text": "Proof. The basic strategy is to apply iteratively the triangle inequality. To illustrate, we first expand the equation at $t=1$ : ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{t=n}{\\sum}\\underset{w_{1},w_{2}}{\\sum}\\pi_{1:}\\left\\vert\\sum_{m_{1}=1}^{M_{1}}\\boldsymbol{v}_{m_{1}}^{1}\\prod_{i=0}^{H}P_{m_{1}}^{1}(\\boldsymbol{y}_{1}|\\boldsymbol{x}_{2})-\\sum_{m_{1}=1}^{M_{1}}w_{m_{1}}^{2}\\prod_{i=0}^{H}P_{m_{1}}^{2}(\\boldsymbol{y}_{1}|\\boldsymbol{x}_{2})\\right\\vert}\\\\ &{=\\underset{t=n}{\\sum}\\underset{\\frac{1}{\\sqrt{n_{1}\\omega_{1}}}}{\\sum}\\pi_{1:}\\mu_{1}\\Bigg[\\sum_{m_{1}=1}^{M_{1}}w_{m_{1}}^{1}P_{m_{1}}^{1,\\omega_{0}}(\\boldsymbol{y}_{2})\\left(\\frac{P_{m_{1}}^{\\star\\star\\alpha_{1}}(\\boldsymbol{y}_{1})}{P_{m_{1}}^{\\star\\star\\alpha_{1}}}\\boldsymbol{p}_{m_{1}}^{1}(\\boldsymbol{y}_{1}|\\boldsymbol{x}_{1})-\\boldsymbol{l}^{0}(\\boldsymbol{x}_{1},\\boldsymbol{y}_{1})+P^{\\alpha}(\\boldsymbol{x}_{1},\\boldsymbol{y}_{1})\\right)\\prod_{i=2}^{H}P_{m_{1}}^{1}(\\boldsymbol{y}_{1}|\\boldsymbol{x}_{2})}\\\\ &{\\qquad-\\sum_{m_{1}=1}^{M_{1}}w_{m_{1}}^{2}P_{m_{1}}^{2}(\\boldsymbol{y}_{2})\\left(\\frac{P_{m_{1}}^{\\star\\alpha_{1}}(\\boldsymbol{y}_{1})}{P_{m_{1}}^{\\star\\star\\alpha_{1}}}\\boldsymbol{p}_{m_{1}}^{2}(\\boldsymbol{y}_{1}|\\boldsymbol{x}_{1})-\\boldsymbol{l}^{0}(\\boldsymbol{x}_{1},\\boldsymbol{y}_{1})+P^{\\alpha}(\\boldsymbol{x}_{1},\\boldsymbol{y}_{1})\\right)\\prod_{i=2}^{H}P_{m_{1}}^{2}(\\boldsymbol{y}_{1}|\\boldsymbol{x}_{2})\\Bigg]}\\\\ &{\\leq\\underset{t=n}{\\sum}\\underset{\\frac{1}{\\sqrt\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We can continue applying triangle inequalities to all possible first event-logging time step, and we can start with the following inequality: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{\\tau_{1},\\eta_{1},H}\\pi_{1:H}\\left|\\sum_{m=1}^{M}w_{m}^{1}\\prod_{t=0}^{H}P_{m}^{1}(y_{t}|x_{t})-\\sum_{m=1}^{M}w_{m}^{2}\\prod_{t=0}^{H}P_{m}^{2}(y_{t}|x_{t})\\right|}\\\\ &{\\le\\displaystyle\\sum_{\\tau_{1}\\in[H]}\\sum_{x_{1:\\tau_{1}},y_{1:\\tau_{1}}}\\pi_{1:\\tau_{1}}l_{1:\\tau_{1}-1}^{0}\\times}\\\\ &{\\displaystyle\\sum_{\\tau_{1}=1:H}\\pi_{\\tau_{1}+1:H}\\left|\\sum_{m=1}^{M}p_{m}^{1,1}(x_{\\tau_{1}},y_{\\tau_{1}})\\prod_{t=\\tau_{1}+1}^{H}P_{m}^{1}(y_{t}|x_{t})-\\sum_{m=1}^{M}p_{m}^{2,1}(x_{\\tau_{1}},y_{\\tau_{1}})\\prod_{t=\\tau_{1}+1}^{H}P_{m}^{2}(y_{t}|x_{t})\\right|}\\\\ &{~~~~\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Recall that at least one p1m,1 or $p_{m}^{2,1}$ is $_0$ , that is, one of the contexts in either of the two systems is eliminated. ", "page_idx": 24}, {"type": "text", "text": "Now for $(i)$ , we can pick the next event-logging time step $\\tau_{2}>\\tau_{1}$ , and apply the triangle inequality similarly, and repeat such event-logging until all contexts are exhausted. Since there are at most $2M$ contexts, we cannot repeat this process more than $d=2M-1$ times. Hence, we arrive to the following inequality: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{x_{1:H}}\\sum_{y_{1:H}}\\pi_{1:H}\\left|\\sum_{m=1}^{M}w_{m}^{1}\\prod_{t=0}^{H}P_{m}^{1}(y_{t}|x_{t})-\\sum_{m=1}^{M}w_{m}^{2}\\prod_{t=0}^{H}P_{m}^{2}(y_{t}|x_{t})\\right|}\\\\ &{\\leq\\displaystyle\\sum_{\\tau\\in\\mathrm{Subseq}(H,d)}\\sum_{x_{\\tau},y_{\\tau}}\\Delta(x_{\\tau},y_{\\tau})\\times\\sum_{x_{0:\\tau_{1}-1}}\\cdots\\sum_{x_{\\tau_{|\\tau|}+1:H}}\\prod_{i=0}^{|\\tau|}\\left(\\pi_{\\tau_{i}+1:\\tau_{i+1}}\\cdot l_{\\tau_{i}+1:\\tau_{i+1}-1}^{i}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where we set $\\tau_{|\\tau|+1}\\equiv H+1$ . To proceed from here, we first observe that for any $m\\in[M]$ and $i\\geq0$ such that $p_{m}^{1,i}>0$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pi_{\\tau_{i}+1:\\tau_{i+1}}\\cdot l_{\\tau_{i}+1:\\tau_{i+1}-1}^{i}\\leq\\pi_{\\tau_{i}+1:\\tau_{i+1}}\\frac{\\prod_{t=\\tau_{i}+1}^{\\tau_{i+1}-1}P_{m}(y_{t}|x_{t})}{P_{m}^{\\nu(\\psi_{i};\\tau_{i})}\\left(s_{\\tau_{i+1}}\\vert s_{\\tau_{i}+1}\\right)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\pi(a_{\\tau_{i+1}}\\vert h_{\\tau_{i+1}})\\cdot\\frac{\\prod_{t=\\tau_{i}+1}^{\\tau_{i+1}-1}\\pi(a_{t}\\vert h_{t})P_{m}(y_{t}\\vert x_{t})}{P_{m}^{\\nu(\\psi_{i};\\tau_{i})}\\left(s_{\\tau_{i+1}}\\vert s_{\\tau_{i+1}}\\right)}}\\\\ &{\\qquad\\qquad\\qquad=\\frac{P_{m}^{\\pi}\\left(x_{\\tau_{i}+1:\\tau_{i+1}},y_{\\tau_{i}+1:\\tau_{i+1}-1}\\vert h_{\\tau_{i}+1}\\right)}{P_{m}^{\\nu(\\psi_{i};\\tau_{i})}\\left(s_{\\tau_{i+1}}\\vert s_{\\tau_{i+1}}\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Thus, we can summarize that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{\\tau_{1:H}}\\sum_{y_{1:H}}\\pi_{1:H}\\left|\\sum_{m=1}^{M}w_{m}^{1}\\prod_{t=0}^{H}P_{m}^{1}(y_{t}|x_{t})-\\sum_{m=1}^{M_{2}}w_{m}^{2}\\prod_{t=0}^{H}P_{m}^{2}(y_{t}|x_{t})\\right|}\\\\ &{\\le\\displaystyle\\sum_{\\tau\\in\\mathrm{Subseq}(H,d)}\\sum_{x_{\\tau},y_{\\tau}}\\Delta(x_{\\tau},y_{\\tau})\\times\\sum_{x_{0:\\tau_{1}-1}}\\cdots\\sum_{x_{\\tau_{|\\tau|}+1:H}}\\prod_{i=0}^{|\\tau|}\\left(\\frac{P_{m}^{\\pi}(x_{\\tau},y_{\\tau})\\left(x_{\\tau_{i}+1:\\tau_{i+1}},y_{\\tau_{i}+1:\\tau_{i+1}-1}\\right)h_{\\tau_{i}+1:H}}{P_{m}^{\\nu(\\psi_{i},\\tau_{i})}\\left(s_{\\tau_{i+1}}\\vert s_{\\tau_{i}+1},\\phi\\right)}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We note that each term in the product sequence is equivalent to ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{P_{m(x_{\\tau},y_{\\tau})}^{\\pi}(h_{\\tau_{i+1}+1}|h_{\\tau_{i}+1})}{P_{m(x_{\\tau},y_{\\tau})}^{\\nu(\\psi_{i};\\tau_{i})}(s_{\\tau_{i+1}}|s_{\\tau_{i}+1})P_{m(x_{\\tau},y_{\\tau})}(y_{\\tau_{i+1}}|x_{\\tau_{i+1}})},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and thus ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\prod_{i=0}^{|\\tau|}\\left(\\frac{P_{m(x_{\\tau},y_{\\tau})}^{\\pi}(x_{\\tau_{i}+1:\\tau_{i+1}},y_{\\tau_{i}+1:\\tau_{i+1}-1}|h_{\\tau_{i}+1})}{P_{m(x_{\\tau},y_{\\tau})}^{\\nu(\\psi_{i};\\tau_{i})}(s_{\\tau_{i+1}}|s_{\\tau_{i}+1})}\\right)}\\\\ &{=\\left(\\frac{P_{m(x_{\\tau},y_{\\tau})}^{\\pi}(x_{1:H},y_{1:H})}{\\prod_{i=0}^{q}P_{m(x_{\\tau},y_{\\tau})}^{\\nu(\\psi_{i};\\tau_{i})}(s_{\\tau_{i+1}}|s_{\\tau_{i}+1})P_{m(x_{\\tau},y_{\\tau})}(y_{\\tau_{i+1}}|x_{\\tau_{i+1}})}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Here we set $\\tau_{q+1}\\,=\\,H+1$ and $P_{m}^{\\pi}(s_{H+1}|\\cdot)=1$ for any $m,\\,\\pi$ and conditional event. Since the denominator does not depend on events outside of event-logging time-steps $\\tau$ , we can marginalize the probabilities in the inner summation and conclude the lemma. \u25a1 ", "page_idx": 25}, {"type": "text", "text": "E.4 Proof of Lemma A.3 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Let us slightly extend the lemma such that we consider different sets of behavioral policies for different checkpoint time-steps. ", "page_idx": 25}, {"type": "text", "text": "Proof. Note that for all $m,s^{\\prime},s,t_{1},t_{2}$ , by the construction of $\\psi_{\\xi}$ , it follows that ", "page_idx": 25}, {"type": "equation", "text": "$$\nP_{m}^{\\nu(\\psi_{\\xi};t_{1})}(s_{t_{2}}=s|s_{t_{1}}^{\\prime}=s^{\\prime})\\geq\\operatorname*{max}_{\\psi\\in\\Psi_{\\xi}}\\frac{P_{m}^{\\nu(\\psi;t_{1})}(s_{t_{2}}=s|s_{t_{1}}^{\\prime}=s^{\\prime})}{n}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We can observe that for any $m$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\gamma_{m}^{\\nu(\\psi_{\\xi};\\tau,z(l;\\tau))}(s_{\\tau_{l}}^{\\prime},x_{\\tau_{/l}},y_{\\tau_{/l}})=\\prod_{i\\in I}P_{m}^{\\nu(\\psi_{\\xi};\\tau_{i-1})}(s_{\\tau_{i}+1}|s_{\\tau_{i-1}}^{\\prime})\\cdot\\prod_{i\\in[q]/l}\\frac{1}{A}\\cdot P_{m}^{\\nu(\\psi_{\\xi};\\tau_{i-1})}(s_{\\tau_{i}}|s_{\\tau_{i-1}}^{\\prime})P_{m}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "On the other hand, for any $\\pi\\in\\Pi,\\tau\\subseteq[H],I\\in[|\\tau|],x_{\\tau},y_{\\tau},$ ", "page_idx": 25}, {"type": "equation", "text": "$$\ns_{\\tau_{I}}^{\\prime},x_{\\tau_{I}},y_{\\tau_{I}})\\leq\\prod_{i\\in I}\\operatorname*{max}_{\\tau_{1:\\tau_{i-1}}}P_{m}^{\\pi}(s_{\\tau_{i}+1}|s_{\\tau_{i-1}}^{\\prime},\\mathcal{T}_{1:\\tau_{i-1}})\\cdot\\prod_{i\\in[q]/I}\\operatorname*{max}_{\\tau_{i}}P_{m}^{\\pi}(x_{\\tau_{i}}|s_{\\tau_{i-1}}^{\\prime},\\mathcal{T}_{1:\\tau_{i-1}})P_{m}(y_{\\tau_{i}}|s_{\\tau_{i-1}}^{\\prime},\\mathcal{T}_{1:\\tau_{i-1}})\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Applying the inequality with the definition of $\\rho(\\Psi_{\\xi};\\pi)$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma(\\psi_{\\xi};\\pi)=\\underset{\\tau\\leq[H],|\\tau|\\leq d}{\\operatorname*{max}}\\underset{t\\leq\\ell\\leq|\\tau|}{\\operatorname*{max}}\\underset{(x,y)\\in[\\mathcal{S}^{\\infty}\\otimes\\tau^{|I|}]}{\\operatorname*{max}}\\ \\underset{m\\in[M]}{\\operatorname*{max}}\\ \\frac{P_{m}^{m}(s_{\\tau_{I}}^{\\prime}=s^{\\prime},x_{\\tau_{I}}=x,y_{\\tau_{I}}=y)}{P_{m}^{\\nu(\\psi_{\\xi},\\tau,s(I-1))}(s_{\\tau_{I}}^{\\prime}=s^{\\prime},x_{\\tau_{I}}=x,y_{\\tau_{I}}=x,y_{\\tau_{I}}=0)}}\\\\ &{\\leq\\underset{\\tau\\leq[H],|\\tau|\\leq d}{\\operatorname*{max}}\\ \\underset{(x,y)\\in(\\mathbb{R}^{2}\\otimes\\tau^{|I|})}{\\operatorname*{max}}\\ \\underset{m\\in[M]}{\\operatorname*{max}}\\ \\underset{i\\in I}{\\operatorname*{max}}\\underset{i\\in I}{\\prod}\\frac{\\operatorname*{max}_{T_{1:\\tau_{i-1}}}P_{m}^{m}(s_{\\tau_{i+1}}|s_{\\tau_{i-1}}^{\\prime},T_{1:\\tau_{i-1}})}{P_{m}^{\\nu(\\psi_{\\xi};\\tau_{i-1})}(s_{\\tau_{i+1}}|s_{\\tau_{i-1}}^{\\prime})}}\\\\ &{\\qquad\\quad\\times\\underset{i\\in[q]/I}{\\prod}\\ A\\cdot\\frac{\\operatorname*{max}_{T_{1:\\tau_{i-1}}}P_{m}^{\\psi}(s_{\\tau_{i}}|s_{\\tau_{i-1}}^{\\prime},T_{1:\\tau_{i-1}})}{P_{m}^{\\nu(\\psi_{\\xi};\\tau_{i-1})}(s_{\\tau_{i}}|s_{\\tau_{i-1}}^{\\prime})}}\\\\ &{\\leq(n A\\cdot\\rho(\\Psi_{\\xi},\\tau))^{d},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "concluding Lemma A.3. ", "page_idx": 25}, {"type": "text", "text": "E.5 Proof of Lemma A.4 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "This is by now a standard MLE technique for constructing confidence sets in RL [1]. ", "page_idx": 26}, {"type": "text", "text": "Proof. The proof follows a Chernof fbound type of technique: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}_{\\theta^{*}}\\left(\\displaystyle\\sum_{(\\tau,\\pi)\\in\\mathcal{D}^{k}}\\log\\left(\\frac{\\mathbb{P}_{\\theta}^{\\pi}(\\tau)}{\\mathbb{P}_{\\theta^{*}}^{\\pi}(\\tau)}\\right)\\geq\\mathbb{E}_{\\theta^{*}}\\left[\\displaystyle\\sum_{(\\tau,\\pi)\\in\\mathcal{D}^{k}}\\log\\left(\\frac{\\mathbb{P}_{\\theta}^{\\pi}(\\tau)}{\\mathbb{P}_{\\theta^{*}}^{\\pi}(\\tau)}\\right)\\right]+\\beta\\right)}\\\\ &{\\quad\\le\\mathbb{P}_{\\theta^{*}}\\left(\\exp\\left(\\displaystyle\\sum_{(\\tau,\\pi)\\in\\mathcal{D}^{k}}\\log\\left(\\frac{\\mathbb{P}_{\\theta}^{\\pi}(\\tau)}{\\mathbb{P}_{\\theta^{*}}^{\\pi}(\\tau)}\\right)\\right)\\ge\\exp\\left(\\beta\\right)\\right)}\\\\ &{\\quad\\le\\mathbb{E}_{\\theta^{*}}\\left[\\exp\\left(\\displaystyle\\sum_{(\\tau,\\pi)\\in\\mathcal{D}^{k}}\\log\\left(\\frac{\\mathbb{P}_{\\theta}^{\\pi}(\\tau)}{\\mathbb{P}_{\\theta^{*}}^{\\pi}(\\tau)}\\right)\\right)\\right]\\exp(-\\beta).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The last inequality is by Markov\u2019s inequality. Note that random variables are $(\\tau,\\pi)$ in the trajectory dataset $\\mathcal{D}$ , and ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\theta^{*}}\\left[\\sum_{(\\tau,\\pi)\\in\\mathcal{D}^{k}}\\log\\left(\\frac{\\mathbb{P}_{\\theta}^{\\pi}(\\tau)}{\\mathbb{P}_{\\theta^{*}}^{\\pi}(\\tau)}\\right)\\right]=-\\mathrm{KL}(\\mathbb{P}_{\\theta^{*}}(\\mathcal{D}^{k})||\\mathbb{P}_{\\theta}(\\mathcal{D}^{k}))\\le0.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Therefore, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\theta^{*}}\\left[\\exp\\left(\\sum_{(\\tau,\\pi)\\in\\mathcal{D}^{k}}\\log\\left(\\frac{\\mathbb{P}_{\\theta}^{\\pi}(\\tau)}{\\mathbb{P}_{\\theta^{*}}^{\\pi}(\\tau)}\\right)\\right)\\right]=\\mathbb{E}_{\\theta^{*}}\\left[\\Pi_{(\\tau,\\pi)\\in\\mathcal{D}^{k}}\\frac{\\mathbb{P}_{\\theta}^{\\pi}(\\tau)}{\\mathbb{P}_{\\theta^{*}}^{\\pi}(\\tau)}\\right]=\\sum_{\\mathcal{D}^{k}}\\mathbb{P}_{\\theta}(\\mathcal{D}^{k})=1.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Combining the above, taking a union bound over $k\\in[K]$ rounds and $\\theta\\in\\Theta$ , letting $\\beta=\\log(K|\\Theta|/\\eta),$ , with probability $1-\\eta$ , the inequality in Lemma A.4 holds. ", "page_idx": 26}, {"type": "text", "text": "E.6 Proof of Lemma A.5 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Proof. By the TV-distance and Hellinger distance relation, for any $\\iota,\\tau,\\pi$ and $t\\in[H]$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{T}\\mathbb{V}^{2}\\left(\\mathrm{P}_{\\theta}^{\\pi}(\\tau),\\mathbb{P}_{\\theta^{*}}^{\\pi}(\\tau)\\right)\\leq2\\mathrm{H}^{2}\\left(\\mathrm{P}_{\\theta}^{\\pi}(\\tau),\\mathbb{P}_{\\theta^{*}}^{\\pi}(\\tau)\\right)}\\\\ &{\\phantom{\\quad}=2\\left(1-\\mathbb{E}_{\\tau\\sim\\mathrm{P}_{\\theta^{*}}^{\\pi}}\\left[\\sqrt{\\frac{\\mathrm{P}_{\\theta}^{\\pi}(\\tau)}{\\mathrm{P}_{\\theta^{*}}^{\\pi}(\\tau)}}\\right]\\right)\\leq-2\\log\\left(\\mathbb{E}_{\\tau\\sim\\mathrm{P}_{\\theta^{*}}^{\\pi}}\\left[\\sqrt{\\frac{\\mathrm{P}_{\\theta}^{\\pi}(\\tau)}{\\mathrm{P}_{\\theta^{*}}^{\\pi}(\\tau)}}\\right]\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "To bound the summation over samples, we start from ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{(\\tau,\\pi)\\in\\mathcal{D}^{k}}\\mathtt{H}^{2}\\left(\\mathbb{P}_{\\theta}^{\\pi}(\\tau),\\mathbb{P}_{\\theta^{*}}^{\\pi}(\\tau)\\right)\\leq-\\sum_{(\\tau,\\pi)\\in\\mathcal{D}^{k}}\\log\\left(\\mathbb{E}_{\\tau\\sim\\mathbb{P}_{\\theta^{*}}^{\\pi}}\\left[\\sqrt{\\frac{\\mathbb{P}_{\\theta}^{\\pi}(\\tau)}{\\mathbb{P}_{\\theta^{*}}^{\\pi}(\\tau)}}\\right]\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "On the other hand, by the Chernof fbound, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}_{\\theta^{*}}\\left(\\sum_{(\\tau,\\pi)\\in\\mathcal{D}^{k}}\\log\\left(\\sqrt{\\frac{\\mathbb{P}_{\\theta}^{\\pi}(\\tau)}{\\mathbb{P}_{\\theta^{*}}^{\\pi}(\\tau)}}\\right)\\geq\\sum_{(\\tau,\\pi)\\in\\mathcal{D}^{k}}\\log\\mathbb{E}_{\\tau\\sim\\mathbb{P}_{\\theta^{*}}^{\\pi}}\\left[\\sqrt{\\frac{\\mathbb{P}_{\\theta}^{\\pi}(\\tau)}{\\mathbb{P}_{\\theta^{*}}^{\\pi}(\\tau)}}\\right]+\\beta\\right)}\\\\ &{\\quad\\quad\\leq\\mathbb{E}_{\\theta^{*}}\\left[\\frac{\\exp\\Big(\\sum_{(\\tau,\\pi)\\in\\mathcal{D}^{k}}\\log\\Big(\\sqrt{\\frac{\\mathbb{P}_{\\theta}^{\\pi}(\\tau)}{\\mathbb{P}_{\\theta^{*}}^{\\pi}(\\tau)}}\\Big)\\Big)}{\\exp\\big(\\sum_{(\\tau,\\pi)\\in\\mathcal{D}^{k}}\\log\\mathbb{E}_{\\tau\\sim\\mathbb{P}_{\\theta^{*}}^{\\pi}}\\Big[\\sqrt{\\frac{\\mathbb{P}_{\\theta}^{\\pi}(\\tau)}{\\mathbb{P}_{\\theta^{*}}^{\\pi}(\\tau)}}\\big]\\big)}\\right]\\exp(-\\beta)}\\\\ &{\\quad\\quad=\\mathbb{E}_{\\theta^{*}}\\left[\\frac{\\Pi_{(\\tau,\\pi)\\in\\mathcal{D}^{k}}\\sqrt{\\frac{\\mathbb{P}_{\\theta}^{\\pi}(\\tau)}{\\mathbb{P}_{\\theta^{*}}^{\\pi}(\\tau)}}}{\\Pi_{(\\tau,\\pi)\\in\\mathcal{D}^{k}}\\mathbb{E}_{\\tau\\sim\\mathbb{P}_{\\theta^{*}}^{\\pi}}\\Big[\\sqrt{\\frac{\\mathbb{P}_{\\theta}^{\\pi}(\\tau)}{\\mathbb{P}_{\\theta^{*}}^{\\pi}(\\tau)}}\\Big]}\\right]\\exp(-\\beta)}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\mathbb{E}_{\\theta^{*}}\\left[\\mathbb{E}_{\\theta^{*}}\\left[\\frac{\\Pi_{(\\tau,\\pi)\\in\\mathcal{D}^{k-1}}\\sqrt{\\frac{\\mathbb{P}_{\\theta}^{\\pi}(\\tau)}{\\mathbb{P}_{\\theta^{*}}^{\\pi}(\\tau)}}}{\\Pi_{(\\tau,\\pi)\\in\\mathcal{D}^{k-1}}\\mathbb{E}_{\\tau\\sim\\mathbb{P}_{\\theta^{*}}^{\\pi}}\\left[\\sqrt{\\frac{\\mathbb{P}_{\\theta}^{\\pi}(\\iota,\\tau)}{\\mathbb{P}_{\\theta^{*}}^{\\pi}(\\iota,\\tau)}}\\right]}\\mathcal{D}^{k-1}\\right]\\right]\\exp(-\\beta)}\\\\ &{=\\mathbb{E}_{\\theta^{*}}\\left[\\frac{\\Pi_{(\\tau,\\pi)\\in\\mathcal{D}^{k-1}}\\sqrt{\\frac{\\mathbb{P}_{\\theta}^{\\pi}(\\tau)}{\\mathbb{P}_{\\theta^{*}}^{\\pi}(\\tau)}}}{\\Pi_{(\\tau,\\pi)\\in\\mathcal{D}^{k-1}}\\mathbb{E}_{\\tau\\sim\\mathbb{P}_{\\theta^{*}}^{\\pi}}\\left[\\sqrt{\\frac{\\mathbb{P}_{\\theta}^{\\pi}(\\iota,\\tau)}{\\mathbb{P}_{\\theta^{*}}^{\\pi}(\\iota,\\tau)}}\\right]}\\right]\\exp(-\\beta)=\\hdots=\\exp(-\\beta),}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where in the last line, we used the tower property of expectation. Thus, again by setting $\\beta\\,=$ $\\log(K|\\Theta|/\\eta)$ , with probability at least $1-\\eta$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{(\\tau,\\pi)\\in{\\mathcal D}^{k}}\\mathbb{H}^{2}(\\mathbb{P}_{\\theta}^{\\pi}(\\tau),{\\mathbb P}_{\\theta^{*}}^{\\pi}(\\tau))\\leq-\\frac{1}{2}\\sum_{(\\tau,\\pi)\\in{\\mathcal D}^{k}}\\log\\left(\\frac{\\mathbb{P}_{\\theta}^{\\pi}(\\tau)}{\\mathbb{P}_{\\theta^{*}}^{\\pi}(\\tau)}\\right)+\\beta}\\\\ &{\\quad\\quad\\quad=-\\frac{1}{2}\\displaystyle\\sum_{(\\tau,\\pi)\\in{\\mathcal D}^{k}}\\log\\left(\\frac{\\mathbb{P}_{\\theta}^{\\pi}(\\tau)}{\\mathbb{P}_{\\theta^{*}}^{\\pi}(\\tau)}\\right)+\\frac{1}{2}\\displaystyle\\sum_{(\\tau,\\pi)\\in{\\mathcal D}^{k}}\\log\\left(\\frac{\\mathbb{P}_{\\theta}^{\\pi}(\\tau)}{\\mathbb{P}_{\\theta^{*}}^{\\pi}(\\tau)}\\right)+\\beta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "for all $k\\in[K]$ and $\\theta\\in\\Theta$ . Now we can apply Lemma A.4, and finally have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{(\\tau,\\pi)\\in\\mathcal{D}^{k}}\\mathrm{H}^{2}(\\mathbb{P}_{\\theta}^{\\pi}(\\tau),\\mathbb{P}_{\\theta^{*}}^{\\pi}(\\tau))\\le-\\frac{1}{2}\\sum_{(\\tau,\\pi)\\in\\mathcal{D}^{k}}\\log\\left(\\frac{\\mathbb{P}_{\\theta}^{\\pi}(\\tau)}{\\mathbb{P}_{\\theta^{*}}^{\\pi}(\\tau)}\\right)+\\frac{3}{2}\\beta.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Since $\\mathrm{{TV}^{2}\\leq2\\mathrm{{H}^{2}}}$ , we get the lemma. ", "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The abstract present the problem we consider in this work \u2013 designing an efifcient learning algorithm for the RL setting \u2013 and elaborate, in a highlevel, on our new approach for solving this open problem. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: In the conclusion part we highlight the limitations of our work the main ones are: (i) our algorithm is optimal only up to polynomial factors and closing this gap is left as future work, (ii) designing computationally and sample efifcient algorithm, under some oracle assumptions, is an open problem which is left for future work. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The complete proof is given in the Appendix. Further, we made substantial effort to provide intuition for the proof in the main paper: by providing analysis for the simpler MDP problem (and complementary analysis in the Appendix), as well as by connecting the analysis of this simpler setting to the analysis of the LMDP setting. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: There are no experimental results in this paper, but a resolution of an algorithmic open problem. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufifcient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: There are no experimental results in this paper, but a resolution of an algorithmic open problem. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: There are no experimental results in this paper, but a resolution of an algorithmic open problem. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: There are no experimental results in this paper, but a resolution of an algorithmic open problem. ", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufifcient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: There are no experimental results in this paper, but a resolution of an algorithmic open problem. ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: There are no potential harms caused by the research process or required mitigation measures that should have been taken for this work. Further, this work focuses on a mathematical framework with no immediate societal impact or potential harmful consequences, to our opinion. ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our work focuses on establishing the learnability of the LMDP setting, was not established by previous works. The LMDP setting has been investigated in the past, as we mentioned in the introduction section, by both empirical and theoretical communities. We do not see immediate societal impacts of our work between the new results we derived, and the promise for improving algorithms for the LMDP, and POMDP settings in future works. ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer:[NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: There are no experimental results in this paper, but a resolution of an algorithmic open problem. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: There are no experimental results that make use of data or models in this paper, but a resolution of an algorithmic open problem. ", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: There are no experimental results in this paper, but a resolution of an algorithmic open problem. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] Justification: This work does not involve crowdsourcing nor research with human subjects. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 30}]